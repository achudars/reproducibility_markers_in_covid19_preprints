---
title: "Reproducibility of COVID-19 pre-prints in 2020"
subtitle: "Markers of open code or data not found in 81 per cent of pre-prints"
author: 
  - Rohan Alexander^[University of Toronto.]
  - Annie Collins^[University of Toronto.]
thanks: "Collins thanks CANSSI Ontario for financial support. Code and data are available at: https://github.com/anniecollins/reproducibility."
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: "We create a dataset of all the pre-prints published on arXiv, bioRxiv, and medRxiv between 28 January 2020 and 31 January 2021. We extract the text from these pre-prints and parse them looking for keyword markers signalling the availability of the data and code underpinning the pre-print. We are unable to find markers of either open data or open code for 81 per cent of the pre-prints on medRxiv  in our sample, and X per cent of those on arXiv in our sample. Our paper demonstrates the need to have authors categorize the degree of openness of their pre-print as part of the pre-print submissions process, and more broadly, the need to better integrate open science training into a wide range of fields."
output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE, message = FALSE, warning = FALSE)
```

```{r loadpackages, include=FALSE}
library(here)
library(knitr)
library(lubridate)
library(tidyverse)
library(stargazer)
library(wesanderson)
library(kableExtra)
```


```{r loaddata, include=FALSE}
med_open_data_results <- 
  read_csv(here::here("outputs/data/med_open_data_results.csv"))
med_results <- 
  read_csv(here::here("outputs/data/med_results.csv")) # All COVID medRxiv papers (12,060)
med_2019_open_data_results <- 
  read_csv(here::here("outputs/control-data/med_2019_open_data_results.csv")) # All pre-prints posted to medRxiv in 2019

bio_open_data_results <- 
  read_csv(here::here("outputs/data/bio_open_data_results.csv"))
bio_results <- 
  read_csv(here::here("outputs/data/bio_results.csv")) # All COVID bioRxiv papers (3,714)
bio_2019_open_data_results <- 
  read_csv(here::here("outputs/control-data/bio_2019_open_data_results.csv"))
bio_2019_results <- 
  read_csv(here::here("outputs/control-data/bio_data_2019.csv"))

arxiv_open_data_results <- 
  read_csv(here::here("outputs/data/arxiv_open_data_results.csv"))
all_arxiv_results <- 
  read_csv(here::here("outputs/data/arxiv_results.csv")) # All COVID arXiv papers (3,812)

socarxiv_open_data_results <- 
  read_csv(here::here("outputs/data/socarxiv_open_data_results.csv"))
socarxiv_results <- 
  read_csv(here::here("outputs/data/socarxiv_results.csv")) # All COVID medRxiv papers (444). Same as socarxiv_sample.  

all_arxiv_results <- 
  all_arxiv_results %>% 
  mutate(submitted = as_date(submitted),
         updated = as_date(updated))
```


# Introduction

Scientists use open repositories of papers to more quickly disseminate their research than is possible in traditional journals or conference proceedings. These repositories, such as arXiv, bioRxiv, medRxiv, and SocArXiv, are a critical component of scientific communication and many results build on the work published there. Thus it is important that the results that are published are credible. Pre-print repositories have drawn unprecedented attention in the context of the 2019 novel coronavirus (COVID-19) pandemic and the changes it has imposed on the scientific community [@else2020]. These repositories are not peer-reviewed, and, in general, anyone with appropriate academic credentials can submit a pre-print 

Neither peer-review nor credentials are a panacea nor a guarantee of quality. And the gate-keeping and slow publication times of traditional journals mean pre-print repositories have an important role to play. But given the importance of these repositories, it is important that scientists impose on themselves various standards for their results. Following @weissgerber2021automated we examine papers about COVID-19 published to  arXiv, bioRxiv, medRxiv, and SocArXiv from January 2020, through to May 1, 2021. We search for markers of open science as indicators of reproducibility, specifically open data and open code. 

We find that approximately 75 per cent of papers sampled from arXiv, 68 percent of papers sampled from bioRxiv, 80 per cent of papers sampled from medRxiv, and 84 per cent of papers sampled from socArXiv contain neither open data nor open code markers. Examining trends over time, we find that the proportion of pre-prints containing open data or code markers has fluctuated but shown no obvious increasing or decreasing trend throughout the pandemic. We also find that the presence of open data or open code markers has little influence on a pre-print's future publication, while the subset of sampled pre-prints that has been published contains an overall lower proportion of papers with these markers. Finally, we briefly examine the presence of open data and code markers in specific types of pre-prints, namely modelling, simulation, and machine learning research.

The remainder of this paper is structured as follows: in Section \@ref(data-and-methodology) we discuss the process of constructing our dataset through retrieving pre-prints from the arXiv, bioRxiv, medRxiv, and socArXiv repositories and mining them for open data and open code markers. In Sections \@ref(model) and \@ref(results) we explore some key findings in this raw data and explore some logistic regression models that support these findings. Finally, in Section \@ref(discussion) we discuss the implications of these findings in the broader context of reproducibility and science during the COVID-19 pandemic, as well as next steps to expand on our findings and questions raised in the research process.


# Data and Methodology

## Overview 

Our primary dataset consists of information extracted from the arXiv, bioRxiv, medRxiv, and SocArXiv pre-print repositories. We downloaded pre-prints that contained various keywords related to COVID-19 (a full list of these is available in Appendix \@ref(keywords)), and then identified the availability of open code and open data using the Open Data Detection in Publications (ODDPub) text mining algorithm [@citePDDPub] in the `oddpub` package [@citeODDPubpackage]. 

We constructed this dataset by first creating a local copy of each repository using the repository API. We identified papers related to the COVID-19 pandemic through several keyword searches to create our dataset of papers and associated metadata. This metadata varies by repository, but generally includes the following information: title, abstract, author(s), date posted, research field, DOI, version number, corresponding author, corresponding author's institutional affiliation, and published DOI (if the pre-prints has since been published in a peer-reviewed journal). In the case of SocArXiv there are additionally fields that are self-reported by the submitting author about the availability of code and data.

We then checked these pre-prints for open data and code markers using the ODDPub algorithm. This required downloading each pre-print as a PDF and then converting the PDFs to text files. Some papers from SocArxiv were only available in .docx format and were downloaded as such and then converted to text format. We then conducted the open data and open code detection procedure. This involved searching for keywords and other markers of open data and open code such as references to GitHub repositories and similar. A full listing of these markers is included in Appendix \@ref(markers). This was conducted using the `open_data_search()` function from the `oddpub` package.

The result of this process is a dataset indicating the presence of open data or open code markers in each pre-print (with a logical vector for each marker, followed by the relevant open data or open code statements when applicable). Our final dataset was formed by joining this dataframe with the original sample dataframe, typically using the DOI or the unique filename, to form a dataset including all original, qualitative metadata for each pre-print alongside its open data and open code status and markers. 

```{r monthly-papers-total, fig.cap = "Number of pre-prints related to COVID-19", out.width="90%", fig.align = "center"}
just_arxiv <- 
  all_arxiv_results  %>% 
  select(submitted) %>% 
  mutate(type = "arXiv") %>% 
  rename(date = submitted)

just_medRxiv <- 
  med_results  %>% 
  select(date) %>% 
  mutate(type = "medRxiv")

just_bioRxiv <- 
  bio_results  %>% 
  select(date) %>% 
  mutate(type = "bioRxiv")

just_socarxiv <- 
  socarxiv_results  %>% 
  select(date_created) %>%
  rename(date = "date_created") %>%
  mutate(type = "socArXiv")

all <- rbind(just_arxiv, just_medRxiv, just_bioRxiv, just_socarxiv)

all %>% 
  mutate(month = floor_date(date, "month")) %>% 
  filter(year(date) >= 2020) %>% 
  ggplot(aes(x = month)) + 
  geom_histogram(aes(fill = type), stat="count", position = "dodge") + 
  labs(title = "COVID-19-related pre-prints posted per month",
       subtitle = "Total of 3,896 relevant pre-prints on arXiv, 3,714 on bioRxiv, 12,060 on medRxiv, 410 on socArXiv",
       x = "Month",
       y = "Number of pre-prints",
       fill = "Repository") +
  theme_minimal() +
  scale_fill_manual(values = wes_palette("Darjeeling1"))
```



## Summary statistics

The number of pre-prints posted per month increased dramatically in the first half of 2020 across all repositories, reaching its maximum sometime between April and June (depending on repository) and subsequently decreasing. The number of pre-prints posted monthly since August 2020 has remained reasonably steady, with the exception of medRxiv which experienced a spike up to nearly 1000 pre-prints posted in March 2021 (Figure \@ref(fig:monthly-papers-total)). For context, COVID-19 was declared a pandemic by the World Health Organization (WHO) on March 11, 2020, at which point the number of cases globally had just surpassed 118,000 (primarily in east Asia) and the virus had been reported in 114 countries [@citeWHOtimeline].

Broadly, we are unable to find markers of either open data or open code for around 68 percent of the sampled bioRxiv pre-prints, 80 per cent of the sampled medRxiv pre-prints, 76 per cent of the sampled arXiv pre-prints, and 84 per cent of all COVID-19 related socArXiv pre-prints. The distribution of the remaining portion of pre-prints varies by repository (Table \@ref(tab:summarycounts-medRxiv)). Notably, 28 per cent of sampled pre-prints from bioRxiv contained open data markers, the highest of proportion of any repository. Similarly, 20 per cent of sampled arXiv pre-prints contained markers of open code, the highest of any of the repositories in question.

```{r summarycounts-medRxiv, }
# open data/open code count summary table

just_arxiv <-
  arxiv_open_data_results  %>%
  select(submitted, is_open_data, is_open_code) %>%
  mutate(submitted = as.Date(submitted)) %>%
  mutate(type = "arXiv") %>%
  rename(date = submitted)

just_medRxiv <-
  med_open_data_results  %>%
  mutate(date = as.Date(date))  %>%
  select(date, is_open_data, is_open_code) %>%
  mutate(type = "medRxiv")

just_bioRxiv <-
  bio_open_data_results  %>%
  select(date, is_open_data, is_open_code) %>%
  mutate(date = as.Date(date)) %>%
  mutate(type = "bioRxiv")

just_socarxiv <-
  socarxiv_open_data_results  %>%
  select(date_created, is_open_data, is_open_code) %>%
  rename(date = "date_created") %>%
  mutate(type = "socArXiv")

all <- rbind(just_arxiv, just_medRxiv, just_bioRxiv, just_socarxiv)

all %>%
  group_by(type) %>%
  count(is_open_data, is_open_code) %>%
  mutate(prop_total = n / sum(n)) %>%
  mutate(is_open_data = ifelse(is_open_data == 0, "No", "Yes"),
         is_open_code = ifelse(is_open_code == 0, "No", "Yes")) %>%
  ungroup() %>%
  select(is_open_data, is_open_code, n, prop_total) %>%
  kable(col.names = c("Open data markers", "Open code markers", "Count", "Proportion of total"),
        caption = "Counts and proportions of open data and code markers in medRxiv sample",
        booktabs = TRUE,
        digits = 2) %>%
  group_rows(index = c("arXiv" = 4, "bioRxiv" = 4, "medRxiv" = 4, "socArXiv" = 4))
```

The distribution of total sampled pre-prints and sampled pre-prints with open data or code markers roughly follows that of COVID-19-related pre-prints posted in general (Figure \@ref(fig:monthly-papers-condition-stack)). The proportion of pre-prints with open data or code has fluctuated over time but shows no consistent overall increase or decrease throughout the course of the pandemic, nor in conjunction with increases or decreases in the total number of pre-prints posted to any given repository (Figure \@ref(fig:monthly-proportion-of-open)). In our datasets, very few (if any) pre-prints were sampled for the month of January 2020. None of these pre-prints contained open data or open code markers, thus the 0 per cent rate of open data and code for this month across all repositories should be considered an outlier.

```{r monthly-papers-condition-stack, fig.cap = "Number of pre-prints related to COVID-19 sampled from medRxiv, distinguished by presence of open data or code markers", out.width="90%", fig.align = "center"}
# Number of papers for each open data/code status per month
open_status_med <- 
  med_open_data_results %>% 
  select(title, date, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "medRxiv")

open_status_bio <- 
  bio_open_data_results %>% 
  select(title, date, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "bioRxiv", date = as.character(date))

open_status_arxiv <- 
  arxiv_open_data_results %>% 
  select(title, submitted, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "arXiv", date = as.character(submitted) %>% str_sub(end = 10)) %>%
  select(title, date, is_open_code, is_open_data, condition, repo)

open_status_socarxiv <- 
  socarxiv_open_data_results %>% 
  select(title, date_created, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "socArXiv", date = as.character(date_created)) %>%
  select(title, date, is_open_code, is_open_data, condition, repo)

open_status <- rbind(open_status_med, open_status_bio, open_status_arxiv, open_status_socarxiv)

open_status$date <- as.Date(open_status$date)
open_status$condition[open_status$condition == 2] <- "Both"
open_status$condition[open_status$condition == 0] <- "Neither"
open_status$condition[open_status$condition == 1 & open_status$is_open_code == 1] <- "Open Code"
open_status$condition[open_status$condition == 1 & open_status$is_open_data == 1] <- "Open Data"

open_status %>% 
  ggplot() + 
  geom_bar(aes(x = factor(floor_date(date, "month")), 
              fill=factor(condition, levels = c("Neither", "Open Code", "Open Data", "Both")), 
              position = "stack")) +
  labs(fill = "Type of marker") +
  scale_x_discrete(breaks=c("2020-01-01","2020-07-01", "2021-01-01"), 
                   labels=c("Jan 2020", "Jul 2020", "Jan 2021")) +
  ylab("Number of pre-prints") +
  xlab("Month") +
  labs(title="Sampled pre-prints by month", subtitle="medRxiv, Total = 1,200") +
  theme_minimal() +
  scale_fill_manual(values = wes_palette("Darjeeling1")) +
  facet_wrap(~repo)
```

```{r monthly-proportion-of-open, fig.cap= "Proportion of pre-prints in sample with open data or open code markers by month", out.width="90%", fig.align = "center"}
# Proportion of papers posted with open data or open code each month
med_month_sum_cov <- 
  med_open_data_results %>% 
  mutate(date = as.Date(date)) %>%
  group_by(month = floor_date(date, "month")) %>% 
  summarise(count = n(), "Open Code" = mean(is_open_code), "Open Data" = mean(is_open_data)) %>% 
  mutate(daily_rate = count/days_in_month(month), repo = "medRxiv")

bio_month_sum_cov <- 
  bio_open_data_results %>% 
  mutate(date = as.Date(date)) %>%
  group_by(month = floor_date(date, "month")) %>% 
  summarise(count = n(), "Open Code" = mean(is_open_code), "Open Data" = mean(is_open_data)) %>% 
  mutate(daily_rate = count/days_in_month(month), repo = "bioRxiv")

arxiv_month_sum_cov <- 
  arxiv_open_data_results %>% 
  group_by(month = floor_date(submitted, "month")) %>% 
  summarise(count = n(), "Open Code" = mean(is_open_code), "Open Data" = mean(is_open_data)) %>% 
  mutate(daily_rate = count/days_in_month(month), repo = "arXiv")

socarxiv_month_sum_cov <- 
  socarxiv_open_data_results %>% 
  group_by(month = floor_date(date_created, "month")) %>% 
  summarise(count = n(), "Open Code" = mean(is_open_code), "Open Data" = mean(is_open_data)) %>% 
  mutate(daily_rate = count/days_in_month(month), repo = "socArXiv")

month_sum_cov <- rbind(med_month_sum_cov, bio_month_sum_cov, arxiv_month_sum_cov, socarxiv_month_sum_cov) %>%
  pivot_longer(cols = c("Open Data", "Open Code")) %>%
  filter(value != 1) # Removes outliers where small number of papers in a month was sampled and both had open data/code

# TODO: Fix date labels, facet_wrap labels
month_sum_cov %>%
  ggplot() +
  geom_line(data = month_sum_cov, aes(month, value, color = repo)) +
  ggtitle("Proportion of sampled pre-prints with open data or code markers") +
  labs(colour = "Repository",
       x = "Month",
       y = "Proportion of pre-prints") + 
  theme_minimal() +
  scale_color_manual(values = wes_palette("Darjeeling1")) +
  facet_wrap(~name)

```

## Publication status

Our metadata also contains an indication of whether or not a pre-print has been published in the form of a DOI linking to the corresponding published paper. The proportion of pre-prints that have been published varies by repository (Table \@ref(tab:published-sample-summary)). Notably, of all COVID-19-related pre-prints created during 2020 and 2021, approximately one third of those posted to bioRxiv and nearly 30 percent of those posted to medRxiv were eventually published in peer reviewed journals. This is high in comparison to the proportions from arXiv and socArXiv, which may be indication of a bias towards pre-print servers showcasing research from the biomedical and life sciences.

```{r published-sample-summary}
med_published <- 
  med_results %>% 
  select(title, date, published) %>% 
  mutate(repo = "medRxiv", published = as.numeric(!is.na(published)))

bio_published <- 
  bio_results %>% 
  select(title, date, published) %>% 
  mutate(repo = "bioRxiv", published = as.numeric(!is.na(published)))

arxiv_published <- 
  all_arxiv_results %>% 
  select(title, submitted, doi) %>%
  mutate(doi = as.numeric(!is.na(doi)), repo = "arXiv") %>%
  rename(published = doi, date = submitted)

socarxiv_published <- 
  socarxiv_results %>% 
  select(title, date_created, links.doi) %>%
  mutate(links.doi = as.numeric(!is.na(links.doi)), repo = "socArXiv") %>%
  rename(published = links.doi, date = date_created)


published <- rbind(med_published, bio_published, arxiv_published, socarxiv_published)

published %>% 
  group_by(repo) %>% 
  summarise(`Published pre-prints` = sum(published), `Proportion published` = mean(published) %>% round(2)) %>%
  kable(caption = "Counts and proportion of published COVID-19 pre-prints in each repository",
        col.names = c("Repositiory", "Published pre-prints", "Proportion published"))
```


In Table \@ref(tab:open-published-summary) we disaggregate sampled pre-prints by whether there is an indication that it was published. We find that the proportion of pre-prints with open data or code markers among those that have been published is approximately the same as pre-prints that have not been published, differing by one percent only within the bioRxiv and arXiv samples, and three percent in the socArxiv sample (Table \@ref(tab:open-published-summary)).

```{r open-published-summary}
# Maybe find a way to use the data frames constructed above instead of constructing new ones?
# TODO: rethink this section, the proportions are nearly identical between those that are published and those that aren't

# ArXiv variables: doi (looks like best); link_doi, journal_ref seem to offer supplenmentary information
# SocArXiv variables: links.doi

med_published_open_status <- 
  med_open_data_results %>% 
  select(title, date, published, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "medRxiv")

bio_published_open_status <- 
  bio_open_data_results %>% 
  select(title, date, published, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "bioRxiv")

arxiv_published_open_status <- 
  arxiv_open_data_results %>% 
  select(title, submitted, doi, is_open_code, is_open_data) %>%
  mutate(doi = as.numeric(!is.na(doi)), condition = is_open_data + is_open_code, repo = "arXiv") %>%
  rename(published = doi, date = submitted)

socarxiv_published_open_status <- 
  socarxiv_open_data_results %>% 
  select(title, date_created, links.doi, is_open_code, is_open_data) %>%
  mutate(links.doi = as.numeric(!is.na(links.doi)), condition = is_open_data + is_open_code, repo = "socArXiv") %>%
  rename(published = links.doi, date = date_created)


published_open_status <- rbind(med_published_open_status, bio_published_open_status, arxiv_published_open_status, socarxiv_published_open_status)


published_open_status$condition[published_open_status$condition == 2] <- "Both"
published_open_status$condition[published_open_status$condition == 0] <- "Neither"
published_open_status$condition[published_open_status$condition == 1 & published_open_status$is_open_code == 1] <- "Open code"
published_open_status$condition[published_open_status$condition == 1 & published_open_status$is_open_data == 1] <- "Open data"

published_open_status %>%
  count(repo, published, condition) %>% 
  pivot_wider(names_from = condition, values_from = n, values_fill = 0) %>%
  mutate(Published = ifelse(published == 0, "No", "Yes"), `Proportion with neither`= round((`Neither`)/(`Open data` + `Open code` + Both + Neither), 2)) %>%
  select(Published, Both, Neither, `Open code`, `Open data`, `Proportion with neither`) %>%
  kable(caption = "Counts and proportions of open data markers by whether the pre-print was published",
        booktabs = TRUE) %>%
  group_rows(index = c("bioRxiv" = 2, "medRxiv" = 2, "arXiv" = 2, "socArXiv" = 2))
```

```{r open-published-stack, fig.cap= "Number of pre-prints in sample with open data or open code markers by publication status", out.width="90%", fig.align = "center"}

# TODO: are these graphs even necessary?

# published_open_status$published[published_open_status$published == 0] <- "Published"
# published_open_status$published[published_open_status$published == 1] <- "Unpublished"

published_open_status %>% 
  mutate(published = ifelse(published == 0, "Unpublished", "Published")) %>%
  ggplot() + 
  geom_bar(aes(x = published, fill=factor(condition, levels = c("Neither", "Open code", "Open data", "Both")), position = "stack")) +
  labs(fill = "Type of marker",
       y = "Number of pre-prints",
       x = "Publication status") +
  ggtitle("Pre-prints by publication status") +
  theme_minimal() +
  facet_wrap(~repo) + 
  scale_fill_manual(values = wes_palette("Darjeeling1"))
  
```

It is important to note that our dataset imperfectly characterizes publication. In particular, it likely does not have the publication details for some papers that were published. And even if it were a perfect record, there is a publication lag (estimated at an average of around 60 days) that may skew the results for the papers in the latter portion of our sample [@kwon2020].
## Type of pre-print

There are a variety of different types of pre-prints on medRxiv, including those focused on machine learning, modelling, and simulation. It may be that there are differences in open science practices between these. For instance, in papers related to simulation there is less likely to be issues around sharing code.

To characterize the pre-prints by type, we performed keyword searches within the titles and abstracts of our dataset to examine potential differences in the prevalence of open data or code for specific types of papers. There were 21 sampled pre-prints that included the term 'machine learning'. Of those, the proportion of pre-prints with open data or code markers rose to 38 per cent, double that of the sample overall (Table \@ref(tab:summary-by-type)). Similarly, keywords relating to simulation appeared in 80 sampled pre-prints and keywords related to modelling appeared in over one-third of our sampled pre-prints. The latter two categories did not contain pre-prints with open data or code markers at a rate much higher than the general sample, with 22 and 24 per cent of each type of paper containing either open data or open code markers compared to 19 per cent overall. However, due to the significant role that modelling has played in the context of COVID-19, it is likely that a simple keyword search does not have the resolution needed to properly characterize this subset, it is it possible that it only contains pre-prints presenting their own, novel, models.

```{r stack-by-type, fig.cap= "Number of pre-prints in sample with open data or open code markers by type", out.width="90%", fig.align = "center"}

# Number of open data/code papers by keyword for different types
med_paper_type_keywords <- 
  med_open_data_results %>% select(title, abstract, date, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "medRxiv")

bio_paper_type_keywords <- 
  bio_open_data_results %>% select(title, abstract, date, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "bioRxiv")

arxiv_paper_type_keywords <- 
  arxiv_open_data_results %>% select(title, abstract, submitted, is_open_code, is_open_data) %>% 
  rename(date = submitted) %>%
  mutate(condition = is_open_data + is_open_code, repo = "arXiv")

socarxiv_paper_type_keywords <- 
  socarxiv_open_data_results %>% select(title, description, date_created, is_open_code, is_open_data) %>% 
  rename(date = date_created, abstract = description) %>%
  mutate(condition = is_open_data + is_open_code, repo = "socArXiv")

paper_type_keywords <- rbind(med_paper_type_keywords, bio_paper_type_keywords, arxiv_paper_type_keywords, socarxiv_paper_type_keywords)
  
paper_type_keywords$condition[paper_type_keywords$condition == 2] <- "Both"
paper_type_keywords$condition[paper_type_keywords$condition == 0] <- "Neither"
paper_type_keywords$condition[paper_type_keywords$condition == 1 & paper_type_keywords$is_open_code == 1] <- "Open code"
paper_type_keywords$condition[paper_type_keywords$condition == 1 & paper_type_keywords$is_open_data == 1] <- "Open data"

paper_type_keywords$`Machine Learning` <- str_detect( paper_type_keywords$title, "machine learning") | 
                                          str_detect( paper_type_keywords$abstract, "machine learning")
paper_type_keywords$Simulation <- str_detect( paper_type_keywords$title, "simulate|simulation|simulating") | 
                                    str_detect( paper_type_keywords$abstract, "simulate|simulation|simulating|simulated")
paper_type_keywords$modelling <- str_detect( paper_type_keywords$title, "modelling|model|models|modeling") | 
                                str_detect( paper_type_keywords$abstract, "modelling|model|models|modeling")
paper_type_keywords <- paper_type_keywords %>% 
  pivot_longer(cols = c("Machine Learning", "Simulation", "modelling"), names_to = "Type", values_to = "Value") %>%
  filter(Value == TRUE)

# # Plot
paper_type_keywords %>% ggplot() +
  geom_bar(aes(x = Type, fill=factor(condition, levels = c("Neither", "Open code", "Open data", "Both")), position = "stack")) +
  labs(fill = "Type of marker") +
  scale_x_discrete(labels=c("Machine learning", "Modelling", "Simulation")) +
  ylab("Number of pre-prints") +
  xlab("Month") +
  ggtitle("Sampled pre-prints by type") +
  theme_minimal() +
  scale_fill_manual(values = wes_palette("Darjeeling1")) +
  facet_wrap(~repo)

```


```{r summary-by-type}
paper_type_keywords %>% 
  count(repo, Type, condition) %>% 
  pivot_wider(names_from = condition, values_from = n, values_fill = 0) %>% 
  mutate(Type = if_else(Type == "modelling", "Modelling", Type),
         Type = if_else(Type == "Machine Learning", "Machine learning", Type),
         ) %>% 
  mutate(`Proportion with either/both`= round((`Open data` + `Open code` + Both)/(`Open data` + `Open code` + Both + Neither), 2)) %>%
  select(!repo) %>%
  kable(caption = "Counts and proportions of open data markers by the type of pre-print",
        booktabs = TRUE) %>%
  group_rows(index = c("arxiv" = 3, "bioRxiv" = 3, "medRxiv" = 3, "socArXiv" = 3))
```

## Pre-pandemic Pre-prints

In order to examine the influence of the COVID-19 pandemic on open science practices in during the pandemic, we analyzed pre-prints posted between January and December 2019 from each of the four repositories using the same process as was done for the COVID-19-related pre-prints. Since medRxiv was founded in June 2019, all pre-prints unrelated to COVID-19 posted from the latter half of 2019 were analyzed (a total of 913). For bioRxiv, a random sample of 1,200 was taken from all non-COVID-19-related pre-prints posted in the relevant date range.

```{r 2019-results}
bio_2019 <- bio_2019_results %>%
  select(date) %>%
  mutate(type = "bioRxiv")

med_2019 <- med_2019_open_data_results %>%
  select(date) %>%
  mutate(type = "medRxiv")

all_2019 <- rbind(bio_2019, med_2019)

# TODO: position = "dodge" not working
all_2019 %>% 
  mutate(month = floor_date(date, "month")) %>% 
  # filter(year(date) >= 2020) %>% 
  ggplot(aes(x = month, fill = type)) + 
  geom_histogram(stat="count", position = "dodge") +
  labs(title = "Pre-prints posted per month, 2019",
       subtitle = "Total of 31,752 pre-prints on bioRxiv, 913 on medRxiv",
       x = "Month",
       y = "Number of pre-prints",
       fill = "Repository") +
  theme_minimal() +
  scale_fill_manual(values = wes_palette("Darjeeling1"))
```

```{r 2019-open-results}
just_2019_medRxiv <-
  med_2019_open_data_results  %>%
  mutate(date = as.Date(date))  %>%
  select(date, is_open_data, is_open_code) %>%
  mutate(type = "medRxiv")

just_2019_bioRxiv <-
  bio_2019_open_data_results  %>%
  select(date, is_open_data, is_open_code) %>%
  mutate(date = as.Date(date)) %>%
  mutate(type = "bioRxiv")

all_2019 <- rbind(just_2019_medRxiv, just_2019_bioRxiv)

all_2019 %>%
  group_by(type) %>%
  count(is_open_data, is_open_code) %>%
  mutate(prop_total = n / sum(n)) %>%
  mutate(is_open_data = ifelse(is_open_data == 0, "No", "Yes"),
         is_open_code = ifelse(is_open_code == 0, "No", "Yes")) %>%
  ungroup() %>%
  select(is_open_data, is_open_code, n, prop_total) %>%
  kable(col.names = c("Open data markers", "Open code markers", "Count", "Proportion of total"),
        caption = "Counts and proportions of open data and code markers in 2019 medRxiv and bioRxiv samples",
        booktabs = TRUE,
        digits = 2) %>%
  group_rows(index = c("bioRxiv" = 4, "medRxiv" = 4))
```

Between June and December 2019, the number of pre-prints posted to medRxiv monthly saw an overall increase which may be expected as the repository gained recognition and popularity in the medical research community. The number of papers posted monthly to bioRxiv also saw an overall increase throughout 2019. It is important to note here that due to its relative immaturity at the beginning of the COVID-19 pandemic, a significant portion of medRxiv's overall usage has been dedicated to COVID-19-related research. In total, 19,638 pre-prints have been posted to medRxiv since June 2019, 12,060 (approximately 61 per cent) of which relate to COVID-19.

Of the analyzed pre-prints from 2019, 63 percent of those posted to bioRxiv and 75 per cent of those posted to medRxiv showed no indication of open data or open code. Both of these proportions are approximately five per cent lower than the corresponding proportion of COVID-19-related pre-prints, meaning that the analyzed pre-prints from 2019 contained an overall higher prevalence of open data or code markers than pre-prints concerning COVID-19.

```{r 2019-published}
med_published_2019 <- 
  med_2019_open_data_results %>% 
  select(title, date, published) %>% 
  mutate(repo = "medRxiv", published = as.numeric(published))

bio_published_2019 <- 
  bio_2019_results %>% 
  select(title, date, published) %>% 
  mutate(repo = "bioRxiv", published = as.numeric(!is.na(published)))

published_2019 <- rbind(med_published_2019, bio_published_2019)

published_2019 %>% 
  group_by(repo) %>% 
  summarise(`Published pre-prints` = sum(published), `Proportion published` = mean(published) %>% round(2)) %>%
  kable(caption = "Counts and proportion of published COVID-19 pre-prints in each repository, 2019",
        col.names = c("Repositiory", "Published pre-prints", "Proportion published"))
```



```{r, 2019-open-publishing}
med_2019_published_open_status <- 
  med_2019_open_data_results %>% 
  select(title, date, published, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "medRxiv")

bio_2019_published_open_status <- 
  bio_2019_open_data_results %>% 
  select(title, date, published, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "bioRxiv")


published_2019_open_status <- rbind(med_2019_published_open_status, bio_2019_published_open_status)


published_2019_open_status$condition[published_2019_open_status$condition == 2] <- "Both"
published_2019_open_status$condition[published_2019_open_status$condition == 0] <- "Neither"
published_2019_open_status$condition[published_2019_open_status$condition == 1 & published_2019_open_status$is_open_code == 1] <- "Open code"
published_2019_open_status$condition[published_2019_open_status$condition == 1 & published_2019_open_status$is_open_data == 1] <- "Open data"

published_2019_open_status %>%
  count(repo, published, condition) %>% 
  pivot_wider(names_from = condition, values_from = n, values_fill = 0) %>%
  mutate(Published = ifelse(published == 0, "No", "Yes"), `Proportion with neither`= round((`Neither`)/(`Open data` + `Open code` + Both + Neither), 2)) %>%
  select(Published, Both, Neither, `Open code`, `Open data`, `Proportion with neither`) %>%
  kable(caption = "Counts and proportions of open data markers by whether the pre-print was published, 2019",
        booktabs = TRUE) %>%
  group_rows(index = c("bioRxiv" = 2, "medRxiv" = 2))
```

Examining publication rates for pre-pandemic papers, we observe that 64 per cent of pre-prints posted to bioRxiv and 61 per cent of pre-prints posted to medRxiv during 2019 were eventually peer reviewed and published (Table \@ref(tab:2019-published). These rates are approximately double those of the COVID-19-related pre-prints posted to servers during 2020 and 2021 (33 per cent and 31 per cent from bioRxiv and medRxiv respectively). When disaggregated by open data and code status, we find that published and unpublished pre-prints contain open data and code markers with similar prevelance (Table \@ref(tab:2019-open-publishing)). This mimics the behaviour of published and unpublished COVID-19-related pre-prints.

# Model

We are interested in understanding firstly whether open data and open code became more likely over time, and secondly whether the likelihood that at pre-print ends up published is related to provision of open data and open code. To examine these questions we estimate Equation \@ref(eq:first) by fitting two separate generalized linear models with open data and open code markers as binary variables (representing their presence or absence in a pre-print) that are explained as functions of the date at which a pre-print was posted. 

\begin{equation} 
\mbox{Open data/code} = \beta_0 + \beta_1 \mbox{date} (\#eq:first)
\end{equation} 

We then estimate Equation \@ref(eq:second), again by fitting separate generalized linear models, looking at whether a pre-print is published, based on whether it had open code/data. 

\begin{equation} 
\mbox{Open data/code} = \beta_0 + \beta_1 \mbox{date} (\#eq:second)
\end{equation} 

We estimate our models in the statistical programming language `R` [@citeR], using the function `glm()`.

Our estimates (Table \@ref(tab:datatimemodel)) indicate, in greater clarity, the negligible role time appears to play in the proportion of pre-prints with open data or open code posted to medRxiv. Both models output negligibly small, non-significant coefficients on the date term, indicating limited influence of the date a pre-print was posted on its likelihood of containing open data or code markers.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results = 'asis'}
data_on_date <- glm(is_open_data ~ date, family = binomial, data = med_open_data_results)
code_on_date <- glm(is_open_code ~ date, family = binomial, data = med_open_data_results)

published_on_data <- glm(published ~ as.factor(is_open_data), family = binomial, data = med_open_data_results)
published_on_code <- glm(published ~ as.factor(is_open_code), family = binomial, data = med_open_data_results)

stargazer(data_on_date, code_on_date, published_on_data, published_on_code,
          header = FALSE,
          title = 'Estimation results',
          dep.var.labels = c('Open data', 'Open code', 'Published'),
          covariate.labels=c('Date', 'Has open data', 'Has open code', 'Constant'),
          report=('vc*s'),
          no.space = TRUE,
          digits=2,
          digits.extra=1,
          notes.append = TRUE,
          notes.align = "l",
          ci = FALSE,
          ci.level=0.95,
          label = "tab:datatimemodel"
          )
```

We get similar results for publication status as a function of open data or open code presence; however we do see a significant (p < 0.05) coefficient indicating that the presence of open code markers may have a negative influence on a pre-print's likelihood of eventual publication. It is unrealistic to assume that this means pre-prints with open code are less likely to be published on the basis of their code alone. There may be common cause factors involved, such as a disinclination of journals to publish purely computational work as has been the case with pre-print repositories like bioRxiv [@kwon2020].


# Results

## Summary of all papers, then differences by repository + reasons
- Overall, 76 percent of sampled papers did not have open data or code
- Of repositories, bioRxiv had least with no open data/code (68%) and socArXiv had the
Overall, approximately 76 percent of pre-prints analyzed across all four repositories (arXiv, bioRxiv, medRxiv, and socArXiv) contained no markers of open data or code. Of the repositories in question, bioRxiv had the lowest proportion of pre-prints with neither open data nor code markers at 68 per cent. BioRxiv also had the highest proportion of pre-prints with open data markers at 28 per cent (TODO: why?). 83 per cent of all socArXiv pre-prints related to COVID-19 contained no open data or code markers, the lowest proportion of any of the repositories. This may be expected given the nature of pre-prints posted to socArXiv (social sciences and humanities papers which may less reliant on computational results requiring code).

## Time and posting rate influence on open data/code - pre-pandemic and during pandemic as well
- Open data or code does not appear to have increased in availability throughout the pandemic
- Open data or code availability does not appear to have suffered in times of high publication like April through June 2020
- On medRxiv, open data became less available during the pandemic than prior to the pandemic
  - Could have something to do with how popular medRxiv was during the pandemic and how new it was during 2019
- On bioRxiv, there was not significant evidence to suggest that the pandemic has decreased the availability of open data or code, although there was a noticable difference in the rates

We find that time has little influence over the likelihood of a pre-print to have open data or code markers. In the context of the COVID-19 pandemic, this means that the prevalence of open data and code markers has not consistently increased nor decreased over the course of the pandemic, and also appears to be unimpacted by the monthly rate of pre-prints posted to medRxiv (which increased dramatically between January and May 2020 and has fluctuated since). On one hand, it is encouraging to note that open data and code does not appear to have suffered in times of high publication rates, namely April through July 2020. On the other hand, one might hope that more data has become openly available throughout the course of the pandemic, or that the rate of availability of open data or code has improved as the scientific community has recognized the importance of global collaboration in combating the pandemic, neither of which appear to be the case.

## Publication - why is there little to no difference between published vs non published pre-prints? Publication bias? Insert chi-test results
- We find little to no difference between the availability of data and code in published versus unpublished pre-prints
- Good to know that, as a whole, data and code are not becoming less available as results are published (although we do not endeavor to make direct comparisons between the published and pre-print versions of individual papers)

The presence of open data or code markers does not appear to have significant influence over whether or not a pre-print posted to medRxiv has subsequently been published in a peer reviewed journal, however the proportion of published pre-prints with open data or code markers appears to be slightly lower than those that have not been published (15 per cent versus 20 per cent). This is not necessarily a significant result given the size of our sample and the potential inaccuracies in the medRxiv API's attempts to identify publication, however it does provide a potential area of further study.

## Type of paper - leave for now? Incorporate some into first paragraph
The proportion of pre-prints with open data or code markers is significantly higher among pre-prints with keywords indicating machine learning-based research. This is a somewhat anticipated result, as machine learning tends to rely more heavily on computational and code-focused methodology, however the subset of sampled pre-prints with keywords relating to simulation is very small. On the other hand, the prevalence of pre-prints with open data or code markers only saw a slight increase in the subset of pre-prints with either modelling or simulation keywords. Since the pandemic has seen a rise in scientific and public focus on epidemiological modelling, it is likely that not all papers showing keywords for modelling actually propose their own models which would account for the minimal increase of open data and code markers in this context.

## Pre-pandemic vs during pandemic - chi tests, publication



# Discussion

- Add reference somewhere to that paper about the numbers: https://link.springer.com/article/10.1007/s11192-020-03675-3
- Add reference somewhere to that paper about keyword search and then analysis. Was it a Gary King one?



## On the role of transparency and reproducibility

Transparency and reproducibility are hallmarks of quality scientific research. Open data and open code contribute greatly to both in allowing the scientific community to more easily verify the authenticity of purported scientific discovery and its supporting evidence, which is especially important in cases where scientific research may quickly and directly impact clinical practice or policy such as during the COVID-19 pandemic. Among a myriad of other impacts on biomedical research, COVID-19 has dramatically increased the popularity of pre-prints from both a production and consumption stand-point. The number of COVID-19 pre-prints posted to medRxiv increased dramatically in the early stages of the pandemic while non-COVID-19 pre-print rates remained consistent. The same trends were apparent in abstracts accessed by medRxiv users, where COVID-19 pre-print abstracts were viewed over 15 times the rate of non-COVID-19 pre-print abstracts [@citeFraser]. For these reasons, it is more important now than ever to examine open science standards and reproducibility within pre-print repositories.

Open data is generally accepted to be beneficial to the scientific process, and to a paper's reproducibility potential, hence it is concerning that over 75 per cent of pre-prints in our sample contained no open data markers. This concern is slightly mitigated by recognition of challenges in working with biomedical data compared with data in other fields, notably privacy and ethics concerns when working with personal data [@Floca2014]. The COVID-19 pandemic has seen an uptake in open science practices globally, as evidenced by the creation of open data repositories such as the dashboard maintained by the Center for Systems Science and Engineering at Johns Hopkins University [@citeCSSE] or the large number of publishers who have removed paywalls from published COVID-19 research [@gill2020]. (MENTION/REFERENCE https://wellcome.org/coronavirus-covid-19/open-data ?) 
- Insert discussion on difference from 2019 data

Open code as an open science marker is much more context and field-dependent, as not all biomedical research papers will rely on computational methods for their analyses. However, in pre-prints where code comprises a large portion of the methodology or results, posting it openly to repositories like GitHub contributes greatly to a pre-print's potential reproducibility. This gains importance as computational methods become increasingly popular in the rush to form predictions about emerging situations with limited data or laboratory research, which was the case for modelling studies in the early days of the COVID-19 pandemic (CITE). We also see growing concern over the quality and consequences of this sort of research, with bioRxiv no longer allowing purely computational work [@kwon2020].

## The role of of pre-print repositories

Many concerns have arisen from the unprecedented rate at which COVID-19 research has been posted and consumed via pre-print repositories, particularly in the early stages of the pandemic [@raynaud2021]. Any rushed scientific research has potential to skip (or at least place less precedence on) quality open science practices, thus it may be reasonable to expect a decrease of open data or code markers in the pre-prints posted during times of increased overall posting to medRxiv. In our analysis, we found little relationship between time posted and likelihood of having open data or code markers with the proportion of papers containing these markers fluctuating greatly from month to month and no apparent decrease during periods of increased publication. This suggests that open science practices are more highly influenced by other factors, perhaps training, publication bias, or the nature of the paper itself. On the other hand, we do not see an overall long-term increase in either open data or open code markers throughout our year of analysis which we may expect in the context of the aforementioned open science movements the pandemic has fostered. Although not pre-print specific, @else2020 found that overall research output has fluctuated between different fields and topics (namely modelling disease spread, public health, diagnostics and testing, mental health, and hospital mortality) throughout different stages of the pandemic which may account for some of the fluctuation and overall lack of noticeable trend over the course of the year.

To emphasize the ongoing need for open data and code in modelling a pandemic, we consider two high profile epidemiological models that emerged in early 2020. Modelling was conducted by Imperial College London (ICL) [@ferguson2020] and the Institute for Health Metrics and Evaluation (IHME) at the University of Washington [@murray2020], and both papers were initially posted to pre-print repositories. The ICL model went on to become the most cited pre-print as of December 2020 [@else2020], and both had significant influence over policy and public health decisions worldwide [@adam2020]. An independent review of these two models by @jin2020 found that while code and data were openly available for both, only the ICL model was reproducible due to limited transparency on the underlying methodology of the IHME model. The open-source nature of these papers was fundamental to reproduction attempts and is a great example of the need for open data and code in reproducing COVID-19 research, particularly in evaluating pre-prints as they begin to influence public decision-making. 

In the context of the above factors, it was disheartening in our analysis to find that the proportion of modelling- and simulation-related pre-prints with open data or code increased only marginally from that of the entire sample. One might hope that modelling papers should universally be subject to the same analysis as conducted by @jin2020 as for the examples above, which is made possible by the availability of relevant code and data. Although our analysis was not particularly robust, this shows a need for future investigation and potential overall improvement in open science standards for these types of pre-prints (of course subject to the data and code considerations already discussed). This need is again emphasized by the new-found speed at which pre-print articles may gain public, media, and political attention in the context of the pandemic.

## The importance of open data and open code

Beyond pre-prints, COVID-19 has had great influence over publication and peer review processes as well, with expedited review timelines for COVID-19 papers at the expense of longer waits for other scientific research [@else2020]. Needless to say, it is important that open data and code standards be maintained in published work as well. Our findings in this regard were two-fold: that open data or code markers do not appear to directly influence a pre-print's eventual publication, and that a lower proportion of published papers contain either open data or code markers than the general sample (TODO: no longer valid; cite new McGuinness/SHephard paper). Both of these raise concerns over publication bias, the potential that journals have favored novel yet less transparent or reproducible papers over those with null results but a high standard of open science practices. Concerns have already been raised through systemic reviews of COVID-19 publications [@raynaud2021], and oversights in data accessibility have led to high profile retractions of publications in the past, for example two papers from The Lancet and the New England Journal of Medicine that were withdrawn due to concerns over the private nature of their underlying dataset [@ledford2020].

In all fields of science, increasing access to data and code used for pre-printed or published research is a step in the direction of more transparent, reproducible, and reliable research. The ongoing COVID-19 pandemic has created a novel, constantly changing scientific culture that should be navigated with the utmost care so as to uphold standards of scientific practice for both the research community and the safety of the general public. Our analysis shows that there is much improvement to be made in the areas of open data and code availability within COVID-19 pre-print papers on arXiv, bioRxiv, medRxiv, and socArXiv.

Demand for timely research / results in high frequency as epidemic is rapidly evolving. Preprints are more efficient at doing this because there is no time spent on peer review. However, they also allow lesser-known researchers to better disperse their research, because of the possibility that fast-tracked peer review may be biased towards established researchers. While there is a clear need for pre-prints, the point remains that they do not go through the peer review process. This question of quality and validity is particularly pertinent in the COVID-19 context because poorly validated results and false information may spread quickly, and have real effects. We are not saying that peer review implies that a paper is of a high-quality, we are instead saying that the provision of code and data alongside the preprint goes some way to allowing others to trust the findings of pre-prints even though they have not been peer-reviewed. One way this could be encouraged would be for pre-print repositories to have authors characterize the extent to which they have adopted open science practices as part of their submission. Although those papers that do not adopt these practices should not be rejected from pre-print repositories, greater clarity around this would be useful and might move the state-of-the-art forward.



## Weaknesses and next steps

In this paper we consider only pre-prints from medRxiv, but this analysis can and should be extended to other pre-print repositories including bioRxiv and arXiv. The `medrxivr` package [@citemedrxivr] can be used with the bioRxiv API as well, and our code can be modified to conduct the same scraping and text mining procedure as was used for medRxiv data.

We also wish to expand our analysis to consider the geographic distribution of research and the potential influence of different practices and policies concerning open science as pre-prints vary by location. This is pertinent to our current paper as the epicenter of the virus spread (and thus of scientific output) has shifted throughout the pandemic which has implications for our time-based analysis. 

An important weakness to note is the potential presence of false negatives in indicators of publication in our dataset. @abdill2019 estimate that the false-negative rate may be as high as 37.5 per cent for data pulled from the bioRxiv API, meaning analysis of published papers may represent only a fraction of those that have actually been published. It is unclear to what extent this is the case for medRxiv or what bias may exist in the subset of pre-prints for which publication was detected, as it is likely that this process relies on title-based text matching [@abdill2019]. It is also likely that some of our more recent sampled pre-prints will be published in future which we could not account for at the time of our data collection.

We also recognize that this analysis relies heavily on text-based analysis which was not verified directly in most cases and may lead to higher levels of uncertainty. In future, we wish to take smaller sub-samples to validate factors like publication status or paper topic beyond simple keyword searches or API output.

\newpage

# (APPENDIX) Appendix {-} 

# Keywords

Insert details on keyword search used to identify COVID-19 papers.



# Markers

Insert details on what markers the `oddpub::open_data_search()` uses. Include info here directly from package documentation (i.e. keywords/phrases used for text parsing)?


# Classification

Insert details on keyword search used for machine learning/modelling/simulation



\newpage


# References


