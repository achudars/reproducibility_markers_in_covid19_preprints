---
title: "Reproducibility of COVID-19 research papers on medRxiv"
subtitle: "TBD"
author: "Annie Collins^[University of Toronto.]"
thanks: "We thank CANSSI... Code and data are available at: https://github.com/anniecollins/reproducibility."
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: "We create a dataset of all the papers published on bioRxiv and medRxiv between X and Y. We extract the text from these papers and parse them for keywords to do with the availability of data and scripts underpinning the paper. We find that X per cent of papers have X. Our paper demonstrates the need for Y."
output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(knitr)
library(here)

knitr::opts_chunk$set(echo = FALSE, error = FALSE, message = FALSE, warning = FALSE)

```

# Introduction

Scientists use open repositories of papers to more quickly disseminate their research than is possible in traditional journals. These repositories, such as arxiv, bioRxiv, and medRxiv, are a critical component of science and many results build on the work published there. So it is important that the results that are published are credible. These repositories are not peer-reviewed, and, in general, anyone with appropriate academic credentials can submit a paper. 

While neither peer-review nor credentials are a panacea nor a guarantee of quality, given the importance of these repositories, it is important that scientists impose on themselves various standards for their results. Following @weissgerber2021automated we examine papers about COVID-19 published to bioRxiv and medRxiv during 2020. We search for markers of open science and reproducibility, such as X, Y, and Z. 

We find that A, B, and C.

The remainder of this paper is structured as follows...




# Data & Methodology

Our primary data set consists of information extracted from the medRxiv repository combined with output from running a sample of COVID-19-related pre-prints through the Open Data Detection in Publications (ODDPub) text mining algorithm. 

We constructed this data set by first creating a local copy of the medRxiv repository via the medRxiv API and then filtering for papers related to the COVID-19 pandemic through several keyword searches to create our sampling frame (n = 9,929, including only the most recent version of any given pre-print). This data includes the following variables for each pre-print in the repository: title, abstract, author(s), date posted, research field, DOI, version number, corresponding author, corresponding author's institutional affiliation, and published DOI (if the paper has since been published in a peer reviewed journal). 

We then selected a random sample of these papers (n = 1,200) to check for open data and code markers using the ODDPub algorithm. This required downloading each paper as a PDF, converting the PDFs to text files, and conducting the open data and code detecting procedure to produce a results table indicating the presence of open data or open code markers in each paper (with a value of TRUE or FALSE for each marker and the relevant open data or open code statements when applicable).

Our final data set was formed by joining these two tables together via DOI to form a data set including all original, qualitative information for each pre-print alongside its open data and open code status and markers.

# Sample Summaries
```{r summary-table}
# open data/open code count summary table
med_open_data_results <- read_csv(here("outputs/data/med_open_data_results.csv"))
med_open_data_results %>% 
                    count(is_open_data, is_open_code) %>%
                    kable(col.names = c("Contains Open Data Markers", "Contains Open Code Markers", "Count"))
```

```{r open-data-published-summary}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_data) %>% 
  pivot_wider(names_from = is_open_data, values_from = n) %>%
  rename("Open Data Markers" = `TRUE`, "No Open Data Markers" = `FALSE`) %>%
  kable()
```

```{r open-code-published-summary}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_code) %>% 
  pivot_wider(names_from = is_open_code, values_from = n) %>%
  rename("No Open Code Markers" = `0`, "Open Code Markers" = `1`) %>%
  kable()
```


```{r monthly-papers-total}
include_graphics(here("outputs/figures/papers_posted_total.pdf"))
```

```{r monthly-papers-sample}
include_graphics(here("outputs/figures/papers_posted_sample.pdf"))
```

```{r}
include_graphics(here("outputs/figures/prop_open_per_month.pdf"))
```







# Model

We run our analysis in `R` [@citeR]. 

In this analysis we distinguish between *posting* (the event that a pre-print is uploaded to medRxiv) and *publishing* (the event that the pre-print has subsequently been published in a peer reviewed journal).

```{r}
# logit model: posting rate vs. open data, publication rate vs. open code
```

```{r}
# logit model: open data/code vs. publication status
# pub_model <- glm(published ~ is_open_data + is_open_code, family = binomial, data = med_open_data_results)
```



# Results

# Discussion

There are many factors that impact the ability of an author to post make their data available

## First discussion point - Time
- How has open data/code shifted over the course of the pandemic?
- How has open data/code changed with publication rate?


## Second discussion point - Eventual Publication
- See weaknesses

## Third discussion point - Type of Paper
- A significantly higher proportion if

## Weaknesses and next steps

- No open data/code was verified manually, all dependent on algorithm
- Publication info has high rate of false negatives (i.e. medRxiv data seems to miss a lot of papers that go on to be published)
- Want to look at geographic distribution and prevalence of open data/code - influence on open data policies and timing throughout pandemic


\newpage

# Appendix {-}

Include info here directly from package documentation (i.e. keywords/phrases used for text parsing)?

\newpage


# References


