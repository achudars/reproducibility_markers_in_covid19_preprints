---
title: "Reproducibility of COVID-19 pre-prints"
author: 
  - Annie Collins^[University of Toronto.]
  - Rohan Alexander^[University of Toronto, rohan.alexander@utoronto.ca.]
thanks: "**Preprint:** This present manuscript is a version of a preprint by the authors [@collins2021reproducibility]. The text, tables and figures of this manuscript overlap with those of the preprint version by the authors. **Acknowledgements:** We thank Amy Farrow, Jessica Gronsbell, Monica Alexander, and Thomas William Rosenthal for helpful comments. Two anonymous reviewers and the editor provided extensive comments that substantially improved this paper and we are grateful to them. **Funding statement:** Collins thanks CANSSI Ontario and Monica Alexander for financial support. **Data accessibility statement:** Code and data are available at: https://github.com/anniecollins/reproducibility_markers_in_covid19_preprints. **Author contributions:** Collins had the original idea and wrote the first draft of the paper. Collins and Alexander obtained and created the datasets. Collins and Alexander analysed the data. All authors interpreted the data and contributed writing the paper and approved the final version."
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: "To examine the reproducibility of COVID-19 research, we create a dataset of pre-prints posted to arXiv, bioRxiv, and medRxiv between 28 January 2020 and 30 June 2021 that are related to COVID-19. We extract the text from these pre-prints and parse them looking for keyword markers signalling the availability of the data and code underpinning the pre-print. For the pre-prints that are in our sample, we are unable to find markers of either open data or open code for 75 per cent of those on arXiv, 67 per cent of those on bioRxiv, and 79 per cent of those on medRxiv. We conclude that there may be value in having authors categorize the degree of openness of their pre-print as part of the pre-print submissions process, and more broadly, there is a need to better integrate open science training into a wide range of fields."
output:
  bookdown::pdf_document2:
    keep_tex:  true
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE, message = FALSE, warning = FALSE)
```

```{r loadpackages, include=FALSE}
library(here)
library(knitr)
library(lubridate)
library(tidyverse)
library(stargazer)
# library(wesanderson)
library(kableExtra)
library(DiagrammeR)
library(patchwork)
```


```{r loaddata, include=FALSE}
med_open_data_results <- 
  read_csv(here::here("outputs/data/med_open_data_results.csv"))
med_results <- 
  read_csv(here::here("outputs/data/med_results.csv")) # All COVID medRxiv papers (12,060)
med_2019_open_data_results <- 
  read_csv(here::here("outputs/control-data/med_2019_open_data_results.csv")) # All pre-prints posted to medRxiv in 2019

bio_open_data_results <- 
  read_csv(here::here("outputs/data/bio_open_data_results.csv"))
bio_results <- 
  read_csv(here::here("outputs/data/bio_results.csv")) # All COVID bioRxiv papers (3,714)
bio_2019_open_data_results <- 
  read_csv(here::here("outputs/control-data/bio_2019_open_data_results.csv"))
bio_2019_results <- 
  read_csv(here::here("outputs/control-data/bio_data_2019.csv"))

arxiv_open_data_results <- 
  read_csv(here::here("outputs/data/arxiv_open_data_results.csv"))
all_arxiv_results <- 
  read_csv(here::here("outputs/data/arxiv_results.csv")) # All COVID arXiv papers (3,812)
arxiv_2019_open_data_results <- 
  read_csv(here::here("outputs/control-data/arxiv_2019_open_data_results.csv"))
all_arxiv_2019_results <- 
  read_csv(here::here("outputs/control-data/arxiv_data_2019.csv"))

socarxiv_open_data_results <-
  read_csv(here::here("outputs/data/socarxiv_open_data_results.csv"))
socarxiv_2019_open_data_results <-
  read_csv(here::here("outputs/control-data/socarxiv_2019_open_data_results.csv"))
socarxiv_metadata_2019 <-
  read_csv(here::here("outputs/control-data/socarxiv_metadata_2019.csv"))

all_arxiv_results <- 
  all_arxiv_results %>% 
  mutate(submitted = as_date(submitted),
         updated = as_date(updated))

# Colour palette for all vizes
cbPalette <- c("#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

```


# Introduction

Scientists use open repositories of papers to disseminate their research more quickly than is possible in traditional journals or conference proceedings, and also to obtain feedback on their work prior to publication. These repositories, such as arXiv, bioRxiv, and medRxiv, are a critical component of scientific communication and a lot of research builds on the pre-prints posted there. Pre-print repositories have been especially important during the 2019 novel coronavirus (COVID-19) pandemic and the changes it has imposed on the scientific community [@else2020]. The centrality of pre-prints to science means that it is important that the results that are posted are credible. These repositories are not peer-reviewed, and, in general, anyone with appropriate academic credentials can submit a pre-print.

Neither peer-review nor credentials are a panacea nor a guarantee of quality. And the gate-keeping and slow publication times of traditional journals mean pre-print repositories are important. But it is important that scientists impose standards on themselves, and arguably repositories have a role to play here. Following @weissgerber2021automated, we examine pre-prints about COVID-19 posted to arXiv, bioRxiv, and medRxiv from 28 January 2020 through to 30 June 2021. By way of background, each of these three repositories has a different focus: arXiv is general although it has especially high rates of usage from fields like mathematics, physics, and computer science, bioRxiv focuses on biological sciences, and medRxiv focuses on health sciences.

We search for markers of open science as indicators of reproducibility, specifically open data and open code. The definition of reproducibility tends to vary by context and academic field [@barba2018terminologies]. For the purposes of this paper, we define reproducibility to mean the ability for different researchers to achieve the same results given the same data and computational methods as the original source. This contrasts replicability, which we define as the ability for different researchers to achieve consistent results by conducting the full data collection and analysis process in lieu of reusing original data. These definitions match that of @NAP25303 and @cacioppo2015social.

We find that of the papers sampled, approximately 75 per cent of papers from arXiv, 67 per cent of papers from bioRxiv, and 79 per cent of papers from medRxiv contain neither open data nor open code markers. A summary of our main results is contained in Figure \@ref(fig:flowchart). Examining trends over time, we find that the proportion of pre-prints containing open data or code markers has fluctuated but shown no obvious trend throughout the pandemic. We also find that the presence of open data or open code markers seems to have little association with a pre-print's subsequent publication, and the subset of sampled pre-prints that have been published contains approximately the same proportion of papers with these markers. 

```{r flowchart, fig.cap = "Summary of process and main results", out.width="90%", fig.align = "center"}
DiagrammeR::grViz("digraph {

graph [layout = dot, rankdir = TB]

# define the global styles of the nodes. We can override these in box if we wish
node [shape = rectangle]

all [label =  'All pre-prints posted between 28 January 2020 and 30 June 2021']
onlycovid [label =  'Those about COVID-19']
arxivcovidcount [label =  'arXiv: 4,250 pre-prints']
biorxivcovidcount [label =  'bioRxiv: 4,146 pre-prints']
medrxivcovidcount [label =  'medRxiv: 13,194 pre-prints']
# socarxivcovidcount [label =  'SocArXiv: 478 pre-prints']
randomsample [label =  'Random sampling stratified by repository']
arxivsamplecount [label =  'arXiv: 1,000 pre-prints']
biorxivsamplecount [label =  'bioRxiv: 1,000 pre-prints']
medrxivsamplecount [label =  'medRxiv: 1,500 pre-prints']
# socarxivsamplecount [label =  'SocArXiv: 478 pre-prints']
opencodeanddata [label =  'Those with markers of open code or open data']
arxivopencount [label =  'arXiv: 248 pre-prints']
biorxivopencount [label =  'bioRxiv: 326 pre-prints']
medrxivopencount [label =  'medRxiv: 320 pre-prints']
# socarxivopencount [label =  'SocArXiv: 73 pre-prints']

# edge definitions with the node IDs
all -> onlycovid -> {arxivcovidcount biorxivcovidcount medrxivcovidcount} -> randomsample -> {arxivsamplecount biorxivsamplecount medrxivsamplecount} -> opencodeanddata -> {arxivopencount biorxivopencount medrxivopencount}
}")
```

The remainder of this paper is structured as follows: in Section \@ref(methodology) we discuss the process of constructing our dataset through retrieving pre-prints from the arXiv, bioRxiv, and medRxiv repositories and mining them for open data and open code markers. In Section \@ref(results), we present the results and key findings of this process. Finally, in Section \@ref(discussion) we discuss the implications of these findings in the broader context of reproducibility and science during the COVID-19 pandemic, as well as next steps to expand on our findings and questions raised in the research process.


# Methodology

## Pre-print metadata

Our primary dataset consists of pre-print metadata extracted from the arXiv, bioRxiv, and medRxiv pre-print repositories via their respective Application Programming Interfaces (APIs). This metadata varies by repository, but generally includes the following information: title, abstract, author(s), date created, research field, DOI, version number, corresponding author, corresponding author's institutional affiliation, published DOI (if the pre-print has since been published in a peer-reviewed journal), and download link. The data collection process was conducted separately for COVID-19 and pre-COVID-19 papers. 

For COVID-19-related pre-prints, we first created a local copy of each repository containing all metadata for pre-prints posted between 1 January, 2020, and 30 June, 2021. 
<!-- In the case of SocArXiv there are fields that are self-reported by the submitting author about the availability of code and data.  -->
We classified individual pre-prints as "COVID-19-related" based on whether they contained one or more of the following terms in their title or abstract (case insensitive): "COVID-19", "COVID 19", "corona virus", "coronavirus", "coronavirus-2", "SARS-CoV-2", "SARSCoV-2", and "2019-nCoV". We then randomly sampled pre-prints for further analysis.
<!-- , except for SocArXiv, for which we analyzed all pre-prints that matched the above criteria, because there were only 478. -->

For pre-COVID-19 pre-prints, we created a local copy of each repository containing all metadata for pre-prints posted between 1 January, 2019, and December 31, 2019. Since medRxiv was launched in June 2019, we used all pre-print data from the latter half of 2019. We then randomly sampled 1,200 pre-prints from each repository's dataset for analysis, except for medRxiv for which only 913 pre-prints were available over this time.


## Open data and code detection

We checked our sampled pre-prints for open data and code markers using the Open Data Detection in Publications (ODDPub) text mining algorithm [@citePDDPub] within the `oddpub` R package [@citeODDPubpackage] (RRID:SCR_018385) . This required downloading each pre-print as a PDF and then converting the PDFs to text files. 
<!-- Some papers from SocArXiv were only available in Word document format (.docx) and were downloaded as such and then converted to text format.  -->
We then conducted the open data and open code detection procedure, which involved searching for keywords and other markers of open data and open code availability. This was conducted using the `open_data_search()` function from the `oddpub` package. Under the validation procedure from the original authors, the ODDPub algorithm performed with a sensitivity of 0.73 and a specificity of 1.00 for open code detection and a sensitivity of 0.73 and a specificity of 0.97 for open data detection compared with manual screening [@citePDDPub]. Since the ODDPub algorithm was developed specifically for biomedical publications, we conducted our own validation process for its performance on arXiv pre-prints. We found that the ODDPub algorithm performed with a sensitivity of 0.60 and a specificity of 0.98 for open code detection and a sensitivity of 0.67 and a specificity of 0.98 for open data detection compared with manual screening. Details of our validation procedure are contained in Appendix \@ref(oddpub-algorithm-performance-on-arxiv-pre-prints).

What constitutes open code or open data is complicated and discipline specific. The details of the `oddpub` approach are available in @citePDDPub. The general criteria are that specific mention should be made of where the data and code are located, and that data should be as close to raw as possible. Data and code must also be freely accessible to anyone (no request, application, registration process, or affiliation required). Our work was conducted using the statistical programming language R [@citeR] (RRID:SCR_001905).

The result of this process is a dataset indicating the presence of open data or open code markers in each pre-print (with a logical vector for each marker, followed by the relevant open data or open code statements where applicable). Our final dataset was formed by joining this output with the original sample metadata, typically using the DOI or the unique file name, to form a dataset including all original metadata for each pre-print alongside its open data and open code status and markers. 

<!-- Although SocArXiv's metadata contained author-reported information concerning data and code availability for each pre-print, we elected to use the results of the ODDPub algorithm in our analysis to ensure comparability of results between pre-prints from all repositories.  -->
<!-- Notable discrepancies between the ODDPub algorithm and authors' self-reported data and code availability are documented in Appendix \@ref(socarxivissues). -->

```{r monthly-papers-total, fig.cap = "Number of pre-prints related to COVID-19 posted to arXiv (N = 4,250), bioRxiv (N = 4,146), and medRxiv (N = 13,194) per month", out.width="90%", fig.align = "center", fig.pos="H"}
just_arxiv <- 
  all_arxiv_results  %>% 
  select(submitted) %>% 
  mutate(type = "arXiv") %>% 
  rename(date = submitted)

just_medRxiv <- 
  med_results  %>% 
  select(date) %>% 
  mutate(type = "medRxiv")

just_bioRxiv <- 
  bio_results  %>% 
  select(date) %>% 
  mutate(type = "bioRxiv")

# just_socarxiv <- 
#   socarxiv_open_data_results  %>% 
#   select(date_created) %>%
#   rename(date = "date_created") %>%
#   mutate(date = as.Date(date), type = "SocArXiv")

all <- rbind(just_arxiv, 
             just_medRxiv, 
             just_bioRxiv
             # , 
             # just_socarxiv
             )

all %>% 
  mutate(month = floor_date(date, "month")) %>% 
  filter(year(date) >= 2020) %>%
  filter(date < "2021-06-30") %>% 
  ggplot(aes(x = month)) + 
  geom_histogram(aes(fill = type), stat="count", position = "dodge") + 
  labs(title = "COVID-19-related pre-prints posted per month",
       x = "Month",
       y = "Number of pre-prints",
       fill = "Repository") +
  scale_x_date(breaks=as.Date(c("2020-01-01","2020-07-01", "2021-01-01")),
                   labels=c("Jan 2020", "Jul 2020", "Jan 2021")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45)) +
  scale_fill_manual(values = cbPalette) +
  facet_wrap(~type,
             scales = "free_y")
```


# Results

## Pre-pandemic pre-prints

To examine the influence of the COVID-19 pandemic on open science practices during the pandemic, we analyzed pre-prints posted between January and December 2019 from each of the four repositories in question. Since medRxiv was founded in June 2019, all pre-prints posted in the latter half of 2019 were analyzed (a total of 913). For all other repositories, a random sample of 1,200 was taken from all non-COVID-19-related pre-prints posted in the relevant date range.

```{r 2019-results-med-arxiv, out.width="90%", fig.align='center', fig.cap = "Number of pre-prints posted per month in 2019 to arXiv (N = 150,018), bioRxiv (N = 31,752), medRxiv (N = 913)", fig.pos="H"}
bio_2019 <- bio_2019_results %>%
  select(date) %>%
  mutate(type = "bioRxiv")

med_2019 <- med_2019_open_data_results %>%
  select(date) %>%
  mutate(type = "medRxiv")

# socarxiv_2019 <- socarxiv_metadata_2019 %>%
#   select(date_created) %>%
#   mutate(type = "socArXiv") %>%
#   rename(date = date_created)

arxiv_2019 <- all_arxiv_2019_results %>%
  select(submitted) %>%
  mutate(type = "arXiv") %>%
  rename(date = submitted)

all_2019 <- rbind(arxiv_2019, bio_2019, med_2019
                  # , socarxiv_2019
                  )

all_2019 %>% 
  mutate(month = floor_date(date, "month")) %>% 
  # filter(year(date) >= 2020) %>% 
  ggplot(aes(x = month, fill = type)) + 
  geom_histogram(stat="count", position = "dodge2") +
  labs(title = "Pre-prints posted per month, 2019",
       x = "Month",
       y = "Number of pre-prints",
       fill = "Repository") +
  theme_minimal() +
  scale_fill_manual(values = cbPalette) +
  facet_wrap(~type,
             scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```


Between June and December 2019, the number of pre-prints posted to medRxiv monthly saw an overall increase, which may be expected as the repository gained recognition and popularity in the medical research community (Figure \@ref(fig:2019-results-med-arxiv)). The number of pre-prints posted monthly to bioRxiv also saw a slight overall increase throughout 2019, while the number of those posted to arXiv fluctuated throughout the year (Figure \@ref(fig:2019-results-med-arxiv)). Due to its relative immaturity at the beginning of the COVID-19 pandemic, a significant portion of medRxiv's overall usage has been dedicated to COVID-19-related research. In total, 21,647 pre-prints were posted to medRxiv between June 2019 and 30 June, 2021, 13,194 of which (approximately 61 per cent) relate to COVID-19.

Of the analyzed pre-prints from 2019, 93 per cent of those posted to arXiv, 63 per cent of those posted to bioRxiv, and 75 per cent of those posted to medRxiv showed no indication of open data or open code. 

```{r monthly-papers-condition-stack-2019, fig.cap = "Number of sampled pre-prints posted in 2019 from arXiv (n = 1,200), bioRxiv (n = 1,200), and medRxiv (n = 913) distinguished by presence of open data or code markers", out.width="90%", fig.align = "center"}
# Number of papers for each open data/code status per month
open_status_med_2019 <-
  med_2019_open_data_results %>%
  select(title, date, is_open_code, is_open_data) %>%
  mutate(condition = is_open_data + is_open_code, repo = "medRxiv")

open_status_bio_2019 <-
  bio_2019_open_data_results %>%
  select(title, date, is_open_code, is_open_data) %>%
  mutate(condition = is_open_data + is_open_code, repo = "bioRxiv", date = as.character(date))

open_status_arxiv_2019 <-
  arxiv_2019_open_data_results %>%
  select(title, submitted, is_open_code, is_open_data) %>%
  mutate(condition = is_open_data + is_open_code, repo = "arXiv", date = as.character(submitted) %>% str_sub(end = 10)) %>%
  select(title, date, is_open_code, is_open_data, condition, repo)

# open_status_socarxiv_2019 <-
#   socarxiv_2019_open_data_results %>%
#   select(title, date_created, is_open_code, is_open_data) %>%
#   mutate(condition = is_open_data + is_open_code, repo = "SocArXiv", date = as.character(date_created)) %>%
#   select(title, date, is_open_code, is_open_data, condition, repo)

open_status <- rbind(open_status_med_2019, open_status_bio_2019, open_status_arxiv_2019
                     # , open_status_socarxiv_2019
                     )

open_status$date <- as.Date(open_status$date)
open_status$condition[open_status$condition == 2] <- "Both"
open_status$condition[open_status$condition == 0] <- "Neither"
open_status$condition[open_status$condition == 1 & open_status$is_open_code == 1] <- "Open Code"
open_status$condition[open_status$condition == 1 & open_status$is_open_data == 1] <- "Open Data"

figufour <- 
open_status %>%
  ggplot() +
  geom_bar(aes(x = factor(floor_date(date, "month")),
              fill=factor(condition, levels = c("Neither", "Open Code", "Open Data", "Both")),
              position = "stack")) +
  labs(fill = "Type of marker") +
  scale_x_discrete(breaks=c("2019-01-01","2019-07-01", "2019-12-01"),
                   labels=c("Jan 2019", "Jul 2019", "Dec 2019")) +
  ylab("Number of pre-prints") +
  xlab("Month") +
  labs(title="Sampled pre-prints by month in 2019", subtitle="Note the different y-axes.") +
  theme_minimal() +
  scale_fill_manual(values = cbPalette) +
  facet_wrap(~repo,
             scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```


```{r 2019-published, fig.pos="H"}
med_published_2019 <- 
  med_2019_open_data_results %>% 
  select(title, date, published) %>% 
  mutate(repo = "medRxiv", published = as.numeric(published))

bio_published_2019 <- 
  bio_2019_results %>% 
  select(title, date, published) %>% 
  mutate(repo = "bioRxiv", published = as.numeric(!is.na(published)))

# socarxiv_published_2019 <- 
#   socarxiv_metadata_2019 %>% 
#   select(title, date_created, links.doi) %>% 
#   mutate(repo = "socArXiv", published = as.numeric(!is.na(links.doi))) %>%
#   rename(date = date_created) %>%
#   select(title, date, published, repo)

arxiv_published_2019 <-
  all_arxiv_2019_results %>%
  select(title, submitted, doi) %>%
  mutate(repo = "arXiv", published = as.numeric(!is.na(doi))) %>%
  rename(date = submitted) %>%
  select(title, date, published, repo)

published_2019 <- rbind(med_published_2019, bio_published_2019, 
                        # socarxiv_published_2019, 
                        arxiv_published_2019)

published_2019 %>% 
  group_by(repo) %>% 
  summarise(`Published pre-prints` = sum(published), `Proportion published` = mean(published) %>% round(2)) %>%
  kable(caption = "Counts and proportion of published COVID-19 pre-prints in each repository, 2019",
        col.names = c("Repositiory", "Published pre-prints", "Proportion published"),
        format.args = list(big.mark = ","))
```



```{r, 2019-open-publishing}
med_2019_published_open_status <- 
  med_2019_open_data_results %>% 
  select(title, date, published, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "medRxiv")

bio_2019_published_open_status <- 
  bio_2019_open_data_results %>% 
  select(title, date, published, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "bioRxiv")

# socarxiv_2019_published_open_status <- 
#   socarxiv_2019_open_data_results %>%
#   mutate(published = !is.na(links.doi) %>% as.numeric()) %>%
#   select(title, date_created, published, is_open_code, is_open_data) %>%
#   rename(date = date_created) %>%
#   mutate(condition = is_open_data + is_open_code, repo = "socArXiv")

arxiv_2019_published_open_status <- 
  arxiv_2019_open_data_results %>%
  mutate(published = !is.na(doi) %>% as.numeric()) %>%
  select(title, submitted, published, is_open_code, is_open_data) %>%
  rename(date = submitted) %>%
  mutate(condition = is_open_data + is_open_code, repo = "arXiv")

published_2019_open_status <- rbind(med_2019_published_open_status, bio_2019_published_open_status, 
                                    # socarxiv_2019_published_open_status, 
                                    arxiv_2019_published_open_status)

published_2019_open_status$condition[published_2019_open_status$condition == 2] <- "Both"
published_2019_open_status$condition[published_2019_open_status$condition == 0] <- "Neither"
published_2019_open_status$condition[published_2019_open_status$condition == 1 & published_2019_open_status$is_open_code == 1] <- "Open code"
published_2019_open_status$condition[published_2019_open_status$condition == 1 & published_2019_open_status$is_open_data == 1] <- "Open data"

published_2019_open_status %>%
  count(repo, published, condition) %>% 
  pivot_wider(names_from = condition, values_from = n, values_fill = 0) %>%
  mutate(Status = ifelse(published == 0, "Unpublished", "Published"), `Proportion with neither`= round((`Neither`)/(`Open data` + `Open code` + Both + Neither), 2)) %>%
  select(Status, Both, Neither, `Open code`, `Open data`, `Proportion with neither`) %>%
  kable(caption = "Counts and proportions of open data markers by whether the pre-print was published, 2019",
        booktabs = TRUE,
        format.args = list(big.mark = ",")) %>%
  group_rows(index = c("arXiv" = 2, "bioRxiv" = 2, "medRxiv" = 2))
```

Examining publication rates for pre-pandemic papers, we observe that 41 per cent of pre-prints posted to arXiv, 64 per cent of pre-prints posted to bioRxiv, and 61 per cent of pre-prints posted to medRxiv during 2019 were eventually peer reviewed and published (Table \@ref(tab:2019-published)). When disaggregated by open data and code status, we find that published and unpublished pre-prints contain open data and code markers in similar proportions (Table \@ref(tab:2019-open-publishing)).

<!-- MOVE TO AFTER COVID PAPERS

These rates are at least double those of the COVID-19-related pre-prints posted in each repository during 2020 and 2021. When disaggregated by open data and code status, we find that published and unpublished pre-prints contain open data and code markers in similar proportions (Table \@ref(tab:2019-open-publishing)). This mimics the behaviour of published and unpublished COVID-19-related pre-prints. -->

## All pre-prints related to COVID-19

The number of pre-prints posted per month increased in the first half of 2020 across all repositories, reaching a maximum sometime between April and June (depending on repository) and subsequently decreasing. The number of pre-prints posted monthly since August 2020 has remained reasonably steady, with the exception of medRxiv, which experienced an increase to nearly 1,000 pre-prints posted in March 2021 (Figure \@ref(fig:monthly-papers-total)). For context, COVID-19 was declared a pandemic by the World Health Organization (WHO) on March 11, 2020, at which point the number of cases globally had just surpassed 118,000 (primarily in east Asia) and the virus had been reported in 114 countries [@citeWHOtimeline].


## Open data and code

From the collection of all pre-prints related to COVID-19, we randomly sampled 3,910 pre-prints to analyze, stratified by repository. This sample is broken down as follows: 1,500 from medRxiv, 1,000 from arXiv, and 1,000 from bioRxiv. Broadly, we are unable to find markers of either open data or open code for 3,011 pre-prints or approximately 76 per cent of our sample (Appendix \@ref(tables-with-specific-values-displayed-in-the-graphs) Table \@ref(tab:summarycounts-repositories)). Of the remaining pre-prints, 7 per cent contained open code markers only, 10 per cent contained open data markers only, and 8 per cent included markers of both open data and open code.

When differentiated by repository, we observe that open data and code markers were absent from 75 per cent of the sampled arXiv pre-prints, 67 per cent of the sampled bioRxiv pre-prints, 79 per cent of the sampled medRxiv pre-prints. The distribution of the remaining portion of pre-prints also varies by repository (Appendix \@ref(tables-with-specific-values-displayed-in-the-graphs) Table \@ref(tab:summarycounts-repositories)). Notably, 28 per cent of sampled pre-prints from bioRxiv contained open data markers and 22 per cent of sampled arXiv pre-prints contained markers of open code, the highest proportions of any repository for each type of marker. Our results are similar to @mcguinness2021descriptive, who focus on medRxiv and find that 23 per cent describe open data.

```{r, summarycounts-overall}
just_arxiv <-
  arxiv_open_data_results  %>%
  select(submitted, is_open_data, is_open_code) %>%
  mutate(submitted = as.Date(submitted)) %>%
  mutate(type = "arXiv") %>%
  rename(date = submitted)

just_medRxiv <-
  med_open_data_results  %>%
  mutate(date = as.Date(date))  %>%
  select(date, is_open_data, is_open_code) %>%
  mutate(type = "medRxiv")

just_bioRxiv <-
  bio_open_data_results  %>%
  select(date, is_open_data, is_open_code) %>%
  mutate(date = as.Date(date)) %>%
  mutate(type = "bioRxiv")

# just_socarxiv <-
#   socarxiv_open_data_results  %>%
#   select(date_created, is_open_data, is_open_code) %>%
#   rename(date = "date_created") %>%
#   mutate(type = "SocArXiv")

all <- rbind(just_arxiv, just_medRxiv, just_bioRxiv
             # , just_socarxiv
             )


all_total <- 
  all %>%
  count(is_open_data, is_open_code) %>%
  mutate(prop_total = n / sum(n)) %>%
  # mutate(is_open_data = ifelse(is_open_data == 0, "No", "Yes"),
  #        is_open_code = ifelse(is_open_code == 0, "No", "Yes")) %>%
  select(is_open_data, is_open_code, n, prop_total)
```

The distribution of total sampled pre-prints and sampled pre-prints with open data or code markers roughly follows that of COVID-19-related pre-prints posted in general (Figure \@ref(fig:arxiv-and-monthly-papers-condition-stack)). The proportion of pre-prints with open data or code has fluctuated over time but shows no consistent overall increase or decrease throughout the course of the pandemic, nor in conjunction with increases or decreases in the total number of pre-prints posted to any given repository. In our datasets, very few (if any) pre-prints were sampled for the month of January 2020. None of these pre-prints contained open data or open code markers, thus the 0 per cent rate of open data and code for this month across all repositories should be considered an outlier. 

It is also important to note that pre-prints posted during the early months of the pandemic were likely using, and reusing, publicly available data sources due to an inability to collect original data within a short timeframe. Additionally, `oddpub` does not consider '[t]he reuse of data/code previously published by other researchers' [@citePDDPub]. A different definition of open data could enable pre-prints that reuse publicly available data to be considered as having their data available for reproducibility purposes.

The proportion of bioRxiv and medRxiv pre-prints lacking both open data and open code are approximately four per cent higher than the corresponding proportions of 2019 pre-prints, suggesting that the analyzed pre-prints from 2019 may contain an overall higher prevalence of open data and code markers than pre-prints concerning COVID-19. Specifically, we found that open data availability in medRxiv pre-prints was significantly associated with a pre-pandemic registration date ($\chi$^2^ = 4.8508, p < 0.005), as was open code availability for bioRxiv pre-prints ($\chi$^2^ = 14.491, p < 0.005). This would suggest that open data and code practices may have suffered in the context of COVID-19, or that it may be something that is backfilled after posting.

On the other hand, the analyzed arXiv COVID-19-related pre-prints contain a higher proportion of open data and code markers overall than their 2019 counterparts, with a increase of 18 per cent in sampled arXiv papers. For these repositories, the presence of both open data and open code markers was significantly associated with registration during the pandemic, suggesting that pre-prints related to COVID-19 in arXiv may have more consistently adhered to open science practices than their pre-pandemic counterparts ($\chi$^2^ = 93.124, arXiv open data;  $\chi$^2^ = 106.88, arXiv open code; $\chi$^2^ = 12.303).


```{r monthly-papers-condition-stack, fig.cap = "Number of sampled pre-prints related to COVID-19 from arXiv (n = 1,000), bioRxiv (n = 1,000), and medRxiv (n = 1,500) distinguished by presence of open data or code markers", out.width="90%", fig.align = "center"}
# Number of papers for each open data/code status per month
open_status_med <- 
  med_open_data_results %>% 
  select(title, date, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "medRxiv")

open_status_bio <- 
  bio_open_data_results %>% 
  select(title, date, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "bioRxiv", date = as.character(date))

open_status_arxiv <- 
  arxiv_open_data_results %>% 
  select(title, submitted, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "arXiv", date = as.character(submitted) %>% str_sub(end = 10)) %>%
  select(title, date, is_open_code, is_open_data, condition, repo)

# open_status_socarxiv <- 
#   socarxiv_open_data_results %>% 
#   select(title, date_created, is_open_code, is_open_data) %>% 
#   mutate(condition = is_open_data + is_open_code, repo = "SocArXiv", date = as.character(date_created)) %>%
#   select(title, date, is_open_code, is_open_data, condition, repo)

open_status <- rbind(open_status_med, open_status_bio, open_status_arxiv
                     # , open_status_socarxiv
                     )

open_status$date <- as.Date(open_status$date)
open_status$condition[open_status$condition == 2] <- "Both"
open_status$condition[open_status$condition == 0] <- "Neither"
open_status$condition[open_status$condition == 1 & open_status$is_open_code == 1] <- "Open Code"
open_status$condition[open_status$condition == 1 & open_status$is_open_data == 1] <- "Open Data"

figfive <- 
open_status %>% 
  ggplot() + 
  geom_bar(aes(x = factor(floor_date(date, "month")), 
              fill=factor(condition, levels = c("Neither", "Open Code", "Open Data", "Both")), 
              position = "stack")) +
  labs(fill = "Type of marker") +
  scale_x_discrete(breaks=c("2020-01-01","2020-07-01", "2021-01-01"), 
                   labels=c("Jan 2020", "Jul 2020", "Jan 2021")) +
  ylab("Number of pre-prints") +
  xlab("Month") +
  labs(title="Sampled pre-prints by month related to COVID-19", subtitle="Note the different y-axes.") +
  theme_minimal() +
  scale_fill_manual(values = cbPalette) +
  facet_wrap(~repo,
             scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r arxiv-and-monthly-papers-condition-stack, fig.cap = "Number of sampled pre-prints posted in 2019, compared with those related to COVID-19: from arXiv (n in 2019 = 1,000; n related to COVID = 1,200), bioRxiv (n in 2019 = 1,000; n related to COVID n = 1,200), and medRxiv (n in 2019 = 1,500; n related to COVID n = 913) distinguished by presence of open data or code markers", out.width="90%", fig.align = "center"}

figufour / figfive + plot_layout(guides = 'collect')
```





```{r monthly-proportion-of-open, fig.cap= "Proportion of pre-prints in sample with open data or open code markers by month", out.width="90%", fig.align = "center"}
# Proportion of papers posted with open data or open code each month
med_month_sum_cov <- 
  med_open_data_results %>% 
  mutate(date = as.Date(date)) %>%
  group_by(month = floor_date(date, "month")) %>% 
  summarise(count = n(), "Open Code" = mean(is_open_code), "Open Data" = mean(is_open_data)) %>% 
  mutate(daily_rate = count/days_in_month(month), repo = "medRxiv")

bio_month_sum_cov <- 
  bio_open_data_results %>% 
  mutate(date = as.Date(date)) %>%
  group_by(month = floor_date(date, "month")) %>% 
  summarise(count = n(), "Open Code" = mean(is_open_code), "Open Data" = mean(is_open_data)) %>% 
  mutate(daily_rate = count/days_in_month(month), repo = "bioRxiv")

arxiv_month_sum_cov <- 
  arxiv_open_data_results %>% 
  group_by(month = floor_date(submitted, "month")) %>% 
  summarise(count = n(), "Open Code" = mean(is_open_code), "Open Data" = mean(is_open_data)) %>% 
  mutate(daily_rate = count/days_in_month(month), repo = "arXiv")

# socarxiv_month_sum_cov <- 
#   socarxiv_open_data_results %>% 
#   group_by(month = floor_date(as.Date(date_created), "month")) %>% 
#   summarise(count = n(), "Open Code" = mean(is_open_code), "Open Data" = mean(is_open_data)) %>% 
#   mutate(daily_rate = count/days_in_month(month), repo = "SocArXiv")

month_sum_cov <- rbind(med_month_sum_cov, bio_month_sum_cov, arxiv_month_sum_cov) %>%
  pivot_longer(cols = c("Open Data", "Open Code")) %>%
  filter(value != 1) # Removes outliers where small number of papers in a month was sampled and both had open data/code

# TODO: Fix date labels, facet_wrap labels
# month_sum_cov %>%
#   ggplot() +
#   geom_line(data = month_sum_cov, aes(month, value, color = repo)) +
#   ggtitle("Proportion of sampled pre-prints with open data or code markers") +
#   labs(colour = "Repository",
#        x = "Month",
#        y = "Proportion of pre-prints") + 
#   theme_minimal() +
#   scale_color_manual(values = wes_palette("Darjeeling1")) +
#   facet_wrap(~name)
```

## Publication status

The proportion of pre-prints that have been published varies by repository (Table \@ref(tab:published-sample-summary)). Notably, of all COVID-19-related pre-prints in our dataset, approximately 30 per cent of those posted to bioRxiv and nearly one third of those posted to medRxiv were published. This is high in comparison to the proportion from arXiv, and although this might suggest that COVID-19-related pre-prints in biomedical fields have received greater attention overall than pre-prints from other fields, our results in Section \@ref(pre-pandemic-pre-prints) suggest that this pattern pre-dates the pandemic. 

<!-- These rates are also low in comparison to 2019 pre-prints (approximately half), which is likely a result of publication delays and the high volume of research output related to COVID-19 posted as pre-prints. -->

<!-- Does this make sense? Should find source, check citeFraser paper -->

```{r published-sample-summary}
med_published <- 
  med_results %>% 
  select(title, date, published) %>% 
  mutate(repo = "medRxiv", published = as.numeric(!is.na(published)))

bio_published <- 
  bio_results %>% 
  select(title, date, published) %>% 
  mutate(repo = "bioRxiv", published = as.numeric(!is.na(published)))

arxiv_published <- 
  all_arxiv_results %>% 
  select(title, submitted, doi) %>%
  mutate(doi = as.numeric(!is.na(doi)), repo = "arXiv") %>%
  rename(published = doi, date = submitted)

# socarxiv_published <- 
#   socarxiv_open_data_results %>% 
#   select(title, date_created, links.doi) %>%
#   mutate(links.doi = as.numeric(!is.na(links.doi)), repo = "SocArXiv") %>%
#   rename(published = links.doi, date = date_created)


published <- rbind(med_published, bio_published, arxiv_published
                   # , socarxiv_published
                   )

published %>% 
  group_by(repo) %>% 
  summarise(`Published pre-prints` = sum(published), 
            `Proportion published` = mean(published) %>% round(2)) %>%
  kable(caption = "Counts and proportion of published COVID-19 pre-prints in each repository",
        col.names = c("Repository", "Published pre-prints", "Proportion published"),
        format.args = list(big.mark = ","))
```

In Table \@ref(tab:open-published-summary) we disaggregate sampled pre-prints by whether there is an indication of publication. We find that the proportion of pre-prints with open data or code markers among those that have been published is roughly the same as pre-prints that have not been published, differing by only a few percentage points. 

There is limited literature examining the relationship between data and code availability in manuscripts between the pre-print and publication stages. @mcguinness2021descriptive examine differences in data availability statements between medRxiv pre-prints and their published counterparts. They find that data availability was maintained for the vast majority of their sample, varying by journal data sharing policy with greater improvements in openness among manuscripts published in journals mandating data sharing. While limited to medRxiv, the results of @mcguinness2021descriptive align with our own work and provide initial evidence to suggest that data availability is generally maintained or improved between the pre-print and publication stages.

```{r open-published-summary}
# Maybe find a way to use the data frames constructed above instead of constructing new ones?

# ArXiv variables: doi
# SocArXiv variables: links.doi

med_published_open_status <- 
  med_open_data_results %>% 
  select(title, date, published, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "medRxiv")

bio_published_open_status <- 
  bio_open_data_results %>% 
  select(title, date, published, is_open_code, is_open_data) %>% 
  mutate(condition = is_open_data + is_open_code, repo = "bioRxiv")

arxiv_published_open_status <- 
  arxiv_open_data_results %>% 
  select(title, submitted, doi, is_open_code, is_open_data) %>%
  mutate(doi = as.numeric(!is.na(doi)), condition = is_open_data + is_open_code, repo = "arXiv") %>%
  rename(published = doi, date = submitted)

# socarxiv_published_open_status <- 
#   socarxiv_open_data_results %>% 
#   select(title, date_created, links.doi, is_open_code, is_open_data) %>%
#   mutate(links.doi = as.numeric(!is.na(links.doi)), condition = is_open_data + is_open_code, repo = "SocArXiv") %>%
#   rename(published = links.doi, date = date_created)


published_open_status <- rbind(med_published_open_status, 
                               bio_published_open_status, 
                               arxiv_published_open_status
                               # , socarxiv_published_open_status
                               )


published_open_status$condition[published_open_status$condition == 2] <- "Both"
published_open_status$condition[published_open_status$condition == 0] <- "Neither"
published_open_status$condition[published_open_status$condition == 1 & published_open_status$is_open_code == 1] <- "Open code"
published_open_status$condition[published_open_status$condition == 1 & published_open_status$is_open_data == 1] <- "Open data"

published_open_status %>%
  count(repo, published, condition) %>% 
  pivot_wider(names_from = condition, values_from = n, values_fill = 0) %>%
  mutate(Status = ifelse(published == 0, "Unpublished", "Published"), `Proportion with neither`= round((`Neither`)/(`Open data` + `Open code` + Both + Neither), 2)) %>%
  select(Status, Both, Neither, `Open code`, `Open data`, `Proportion with neither`) %>%
  kable(caption = "Counts and proportions of open data markers by whether the pre-print was published",
        booktabs = TRUE,
        format.args = list(big.mark = ",")) %>%
  group_rows(index = c("arXiv" = 2, "bioRxiv" = 2, "medRxiv" = 2))
```

Our dataset likely imperfectly characterizes publication and does not have the publication details for some papers that were published. And even if it were a perfect record, there is a publication lag (estimated at an average of around 60 days for COVID-19-related pre-prints, although that varies by discipline) that may especially skew the results for pre-prints in the latter portion of our sample [@kwon2020].


# Discussion


## On the role of transparency and reproducibility

Transparency and reproducibility are hallmarks of quality scientific research due to their relationship with independent verification [@Stodden2020Theme]. Open data and open code contribute to both by allowing the scientific community to more easily verify the authenticity of purported scientific discoveries and their supporting evidence. Data sharing also allows others to reuse other researchers' data sets for further analysis or to supplement their own data, contributing to new insights within their field of study. 

These factors are especially important in cases where scientific research may quickly and directly impact clinical practice or public policy, such as research on the COVID-19 pandemic. Among many other impacts on the research landscape, COVID-19 has increased the popularity of pre-prints from both a production and consumption standpoint. The number of COVID-19 pre-prints posted to medRxiv increased in the early stages of the pandemic, while non-COVID-19 pre-print numbers were largely as expected. The same trends were apparent in abstracts accessed by medRxiv users, where COVID-19 pre-print abstracts were viewed over 15 times more than non-COVID-19 pre-print abstracts [@citeFraser]. For these reasons, it is important to examine open science standards and reproducibility within pre-print repositories.

Open data is generally accepted to be beneficial to the scientific process and to a paper's reproducibility potential, hence it is concerning that around 75 per cent of pre-prints in our sample contained no open data markers. This concern is slightly mitigated by recognition of challenges in working with biomedical data compared with data in other fields, notably privacy and ethics concerns when working with personal data [@Floca2014]. The COVID-19 pandemic has seen open science initiatives, as evidenced by the creation of open data repositories such as the dashboard maintained by the Center for Systems Science and Engineering at Johns Hopkins University [@citeCSSE] or the large number of publishers who removed paywalls from published COVID-19 research [@gill2020]. While the intention at the start of the pandemic was that there would be 'clear statements regarding the availability of underlying data' [@citeWellcome] some retractions of work have been based on 'unreliable or nonexistent data' [@da2021optimizing]. 
<!-- SocArXiv asks submitters to make explicit the degree of openness in the data, and this is something that could be adopted by other pre-print repositories. -->

Open code as an open science marker is context and field-dependent, as not all biomedical research papers will rely on computational methods for their analyses. However, in pre-prints where code comprises a large portion of the methodology or results, posting it openly to repositories like GitHub contributes to a pre-print's potential reproducibility. This is important when computational methods are used to form predictions about emerging situations with limited data or laboratory research, which was the case for modelling studies in the early days of the COVID-19 pandemic. We also see growing concern over the quality and consequences of this sort of research, with bioRxiv no longer allowing purely computational work [@kwon2020].

The other concern is the adverse selection issue caused by meeting the open science aims of sharing code and data. Authors that share their data and code open their work up to criticism. If authors who make their data and code available make similar mistakes to authors who choose to not publish their data and code, it is more likely that the mistake would not be noticed in the case where data and code were not published. The current system is biased against those who follow best practice. @mcguinness2021descriptive advocate for '(s)trict editorial policies that mandate data sharing', and other changed norms are needed.


## The role of pre-print repositories

There has been a large amount of research on COVID-19 [@da2021publishing]. Many concerns have arisen from the rate at which COVID-19 research has been posted and consumed via pre-print repositories, particularly in the early stages of the pandemic [@raynaud2021]. Rushed scientific research has potential to skip (or at least place less precedence on) open science practices, thus it may be reasonable to expect a decrease in open data or code markers in the initial few months of the pandemic. We found little relationship between date posted and likelihood of having open data or code markers, with the proportion of pre-prints containing these markers fluctuating from month to month. This suggests that open science practices are more influenced by other factors, perhaps including training, publication bias, or the nature of the pre-print itself. On the other hand, we do not see an overall long-term increase in either open data or open code markers throughout our period of analysis, which we may have expected in the context of the open science movements the pandemic has fostered. Although not pre-print specific, @else2020 found that overall research output has fluctuated between different fields and topics (namely modelling disease spread, public health, diagnostics and testing, mental health, and hospital mortality) throughout different stages of the pandemic, which may account for some of the fluctuation and overall lack of noticeable trend in our sample.

To emphasize the ongoing need for open data and code in modelling a pandemic, we consider two high profile epidemiological models that emerged in early 2020. Modelling was conducted by Imperial College London (ICL) [@ferguson2020] and the Institute for Health Metrics and Evaluation (IHME) at the University of Washington [@murray2020], and both were initially posted to pre-print repositories. The ICL model went on to become the most cited pre-print as of December 2020 [@else2020], and both had significant influence over policy and public health decisions worldwide [@adam2020]. An independent review of these two models by @jin2020 found that while code and data were openly available for both, only the ICL model was reproducible due to limited transparency on the underlying methodology of the IHME model. The open-source nature of these models was fundamental to reproduction attempts and is an example of the need for open data and code in COVID-19 research, particularly as pre-prints influence public decision-making. 

In the context of the above factors, it was encouraging to find in our analysis that the proportion of pre-prints with open data or code posted to arXiv increased from 7 per cent pre-pandemic to 25 per cent for COVID-19-related pre-prints. This pattern, however, was not observed among the analyzed bioRxiv and medRxiv pre-prints, and may just reflect the nature of COVID-19 pre-prints. With many pre-prints from these repositories still pertaining to epidemiological modelling, one might hope that they should universally be subject to the same analysis as conducted by @jin2020 as for the examples above, which is made possible by the availability of relevant code and data. Our analysis suggests a need for future investigation and potential overall improvement in open science standards for these types of pre-prints (subject to the data and code considerations already discussed). This need is again emphasized by the new-found speed at which pre-prints may gain public, media, and political attention in the context of the pandemic, particularly those from medRxiv and bioRxiv. One further concern is raised by @da2020silently, who shows that there are pre-prints on those two pre-print servers---medRxiv and bioRxiv---that were withdrawn or retracted with relatively little information about the underlying reason, after gaining substantial media attention.

## The importance of open data and open code

Beyond pre-prints, COVID-19 has influenced publication and peer review processes, with timelines for COVID-19 papers being expedited at the expense of longer waits for other scientific research [@else2020]. It is important that open data and code standards be maintained in published work as well. In our sample, published pre-prints contain open data or code markers in similar proportions to their unpublished counterparts, a pattern that was present for pre-prints related to COVID-19 and those posted in 2019. This appears initially to alleviate some concerns over the relationship between open data and publication bias, that is, the potential that journals have favoured novel yet less transparent or reproducible papers over those with null results but a high standard of open science practices. However, publication bias is complex and this result should be approached with caution. Concerns have already been raised through systemic reviews of COVID-19 publications [@raynaud2021], and oversights in data accessibility have led to high profile retractions of publications in the past; for example, papers from *The Lancet* and the *New England Journal of Medicine* which were withdrawn due to concerns over the private nature of their underlying dataset [@ledford2020]. Further, @cabanac2021day show that not all pre-prints are linked to their subsequent peer-reviewed publication, which may further bias our results. Further, there is the potential for bias due to older pre-prints having had more time to be published than newer pre-prints. And @oikonomidi2020changes and @Beroe051821 show that differences between updated versions of the same pre-print can be substantial; again this is something that we do not account for and could bias our results.

In all fields of science, increasing access to data and code used for pre-printed or published research is a step in the direction of more transparent, reproducible, and reliable research. The ongoing COVID-19 pandemic has created a novel, constantly changing scientific culture that should be navigated with care to uphold standards of scientific practice for both the research community and the safety of the public. Our analysis shows that there is room for improvement in the areas of open data and code availability within COVID-19 pre-print papers on arXiv, bioRxiv, and medRxiv
<!-- , and SocArXiv. -->

There is demand for timely research and high frequency results because the pandemic rapidly evolves. Pre-prints are efficient in this role because there is no time spent on peer review. They also allow lesser-known researchers to better disperse their research because of the possibility that fast-tracked peer review may be biased towards established researchers. While there is a clear need for pre-prints, the point remains that they do not go through the peer review process. This question of quality and validity is particularly pertinent in the COVID-19 context because poorly validated results and false information may spread quickly and have real effects. We are not saying that peer review implies that a paper is of a high-quality; we are instead saying that the provision of code and data alongside the pre-print goes some way to allowing others to trust the findings of pre-prints, even though they have not been peer-reviewed. One way this could be encouraged would be for all pre-print repositories to have authors characterize the extent to which they have adopted open science practices as part of their submission, in the same way that is done in SocArXiv. Although those pre-prints that do not adopt these practices should not be rejected from pre-print repositories, greater clarity around this would be useful and might move the state-of-the-art forward.



## Weaknesses and next steps

<!-- In this paper we consider only pre-prints from medRxiv, but this analysis can and should be extended to other pre-print repositories including bioRxiv and arXiv. The `medrxivr` package [@citemedrxivr] can be used with the bioRxiv API as well, and our code can be modified to conduct the same scraping and text mining procedure as was used for medRxiv data. -->

We wish to expand our analysis to consider the geographic distribution of research and the potential influence of different practices and policies concerning open science as pre-prints vary by location. This is important because the epicenter of the pandemic changed throughout the pandemic, which has implications for our time-based analysis.

A logical next step would be to extend this analysis to additional pre-print servers. An initial iteration of our analyses included samples of pre-pandemic and COVID-19-related pre-prints posted to SocArXiv, a social sciences pre-print server hosted by the Center for Open Science. We validated the ODDPub algorithm against the presence of data links provided by pre-print authors upon submission (available in the pre-print metadata drawn from the Open Science Framework API) and found that the algorithm performed with 52 per cent sensitivity on the 2019 sample and 29 per cent sensitivity for COVID-19-related pre-prints. The high rate of false negatives for open data detection is concerning, and it was decided that the ODDPub algorithm is not suitable for use on pre-prints from this server. A more generalized (or perhaps field specific) algorithm would be necessary for analysis of open data and code availability in SocArXiv and other more specialized servers. Details on validation are available in Appendix \@ref(oddpub-algorithm-performance-on-socarxiv-pre-prints).

We recognize that factors beyond open data and code play a large role in the reproducibility of scientific research. Not all pre-prints providing open data or code will be reproducible. Factors such as data documentation, methodological reporting, software choice, and many others all play a role in the reproduction process and should be regarded with just as much gravity when disseminating results.

An important weakness is the potential presence of false negatives in indicators of publication in our dataset. @abdill2019 estimate that the false-negative rate may be as high as 37.5 per cent for data pulled from the bioRxiv API, meaning analysis of published papers may represent only a fraction of those that have been published. It is unclear to what extent this is the case for other repositories or what bias may exist in the subset of pre-prints for which publication was detected, because it is likely that this process relies on title-based text matching [@abdill2019]. It is also likely that some of our more recent sampled pre-prints will be published in the future which we could not account for at the time of our data collection.

Our paper depends on search responses from the various repositories, which are based on our selection of keywords. Our selection of keywords is not exhaustive, for instance, perhaps 'the pandemic' could result in additional papers. Future work could make this keyword approach more systematic, for instance following @king2017computer.

We also recognize that this analysis relies heavily on text-based analysis which was not verified directly in most cases and may lead to higher levels of uncertainty. The `oddpub` package was built to analyze biomedical publications and it may be that some of the differences that we find between repositories are due to this. We also note that the ODDPub algorithm is relatively narrow in its definition of "open", excluding data that is available via registration or in some other restricted form. Adopting a broader definition of openness, either through using a less restrictive algorithm or through manual verification, would likely produce different results particularly for pre-prints using clinical data. In future, we wish to take smaller sub-samples to validate factors like publication status, paper topic, and open code and data status, beyond the approaches that we used here. 


\newpage

# (APPENDIX) Appendix {-}


# Tables with specific values displayed in the graphs

```{r 2019-open-results, fig.pos="H"}
just_2019_medRxiv <-
  med_2019_open_data_results  %>%
  mutate(date = as.Date(date), type = "medRxiv")  %>%
  select(date, is_open_data, is_open_code, type)

just_2019_bioRxiv <-
  bio_2019_open_data_results  %>%
  select(date, is_open_data, is_open_code) %>%
  mutate(date = as.Date(date), type = "bioRxiv")

# just_2019_socarxiv <- 
#   socarxiv_2019_open_data_results %>%
#   select(date_created, is_open_data, is_open_code) %>%
#   mutate(date = as.Date(date_created), type = "socArXiv") %>%
#   select(date, is_open_data, is_open_code, type)

just_2019_arxiv <-
  arxiv_2019_open_data_results %>%
  select(submitted, is_open_data, is_open_code) %>%
  mutate(date = as.Date(submitted), type = "arXiv") %>%
  select(date, is_open_data, is_open_code, type)

all_2019 <- rbind(just_2019_medRxiv, just_2019_bioRxiv, 
                  # just_2019_socarxiv, 
                  just_2019_arxiv)

all_2019_total <- 
  all_2019 %>%
  count(is_open_data, is_open_code) %>%
  mutate(prop_total = n / sum(n)) %>%
  # mutate(is_open_data = ifelse(is_open_data == 0, "No", "Yes"),
  #        is_open_code = ifelse(is_open_code == 0, "No", "Yes")) %>%
  select(is_open_data, is_open_code, n, prop_total)

all_2019_by_type <- 
  all_2019 %>%
  group_by(type) %>%
  count(is_open_data, is_open_code) %>%
  mutate(prop_total = n / sum(n)) %>%
  # mutate(is_open_data = ifelse(is_open_data == 0, "No", "Yes"),
  #        is_open_code = ifelse(is_open_code == 0, "No", "Yes")) %>%
  ungroup() %>%
  select(is_open_data, is_open_code, n, prop_total)

rbind(all_2019_total, all_2019_by_type) %>% 
  mutate(markers = case_when(is_open_data == 0 & is_open_code == 0 ~ "Neither",
                             is_open_data == 0 & is_open_code == 1 ~ "Open code",
                             is_open_data == 1 & is_open_code == 0 ~ "Open data",
                             TRUE ~ "Both")) %>%
  select(markers, n, prop_total) %>%
  kable(col.names = c("Markers", "Count", "Proportion of total"),
        caption = "Count and proportions of open data and code markers by pre-print repository in 2019 sample",
        booktabs = TRUE,
        digits = 2,
        format.args = list(big.mark = ",")) %>%
  group_rows(index = c("Total" = 4, "arXiv" = 4, "bioRxiv" = 4, "medRxiv" = 4))  %>%
  kableExtra::kable_styling(latex_options = "hold_position")

```

```{r summarycounts-repositories}
# open data/open code count summary table

all_by_type <- 
  all %>%
  group_by(type) %>%
  count(is_open_data, is_open_code) %>%
  mutate(prop_total = n / sum(n)) %>%
  # mutate(is_open_data = ifelse(is_open_data == 0, "No", "Yes"),
  #        is_open_code = ifelse(is_open_code == 0, "No", "Yes")) %>%
  ungroup() %>%
  select(is_open_data, is_open_code, n, prop_total)

rbind(all_total, all_by_type) %>%
  mutate(markers = case_when(is_open_data == 0 & is_open_code == 0 ~ "Neither",
                             is_open_data == 0 & is_open_code == 1 ~ "Open code",
                             is_open_data == 1 & is_open_code == 0 ~ "Open data",
                             TRUE ~ "Both")) %>%
  select(markers, n, prop_total) %>%
  kable(col.names = c("Markers", "Count", "Proportion of total"),
        caption = "Count and proportions of open data and code markers by pre-print repository",
        booktabs = TRUE,
        digits = 2,
        format.args = list(big.mark = ",")) %>%
  group_rows(index = c("Total" = 4, "arXiv" = 4, "bioRxiv" = 4, "medRxiv" = 4))  %>%
  kableExtra::kable_styling(latex_options = "hold_position")
```

# Table of proportion of all preprints in each category, for each preprint server

```{r propbyserverandcategory, fig.cap = "Proportion of all preprints in each category, for each preprint server", fig.align = "center"}
# Number of papers for each open data/code status per month
open_status %>%
  group_by(repo, condition) %>%
  count() %>%
  ungroup() %>%
  group_by(repo) %>%
  mutate(proportion = n/(sum(n))) %>%
  ungroup() %>%
  select(condition, n, proportion) %>%
   # kable(caption = "Number and proportion of all preprints in each category, for each preprint server",
   #      col.names = c("Repository", "Condition", "Number", "Proportion"),
   #      digits = 2,
   #      booktabs = TRUE,
   #      linesep = ""
   #      )
  kable(col.names = c("Markers", "Count", "Proportion of total"),
        caption = "Number and proportion of all preprints in each category, for each preprint server",
        booktabs = TRUE,
        digits = 2,
        format.args = list(big.mark = ",")) %>%
  group_rows(index = c("arXiv" = 4, "bioRxiv" = 4, "medRxiv" = 4))  %>%
  kableExtra::kable_styling(latex_options = "hold_position")

```

# ODDPub algorithm performance on arXiv pre-prints 

We verified the accuracy of the ODDPub algorithm on a subset of our analyzed pre-prints from 2019 from arXiv. We took a simple random sample of 100 papers. In the original validation process, the annotators stratified by detection status prior to sampling to ensure relatively high representation of papers where open data or code was detected. Since the major concern for our manual verification is potential false negatives, this skewed representation was unnecessary. Open data and code status were verified first via the "Code & Data" tab on each pre-print's page on the arXiv website, then by checking for an explicit data availability section within the pre-print PDF, and finally by manually checking the body of the paper using keyword searches. Results were recorded manually in Excel. This mimics the procedure outlined for the original validation of ODDPub [@citePDDPub].

Many of the pre-prints in arXiv did not use data or code, namely those from pure mathematics and physics. There were also several that reused other publicly or privately available data sets, and regardless of whether or not they were shared alongside the paper, these do not count as open data according to the standards outlined by the original authors of the ODDPub algorithm [@citePDDPub]. Algorithmic performance is specified in Table \@ref(tab:arxiv-accuracy-data-2019) and Table \@ref(tab:arxiv-accuracy-code-2019).

```{r arxiv-confusion-data-2019}
arxiv_2019_manual_results <- read.csv(here::here("validation", "arxiv_2019_validation_sample_COMPLETE.csv"))

arxiv_2019_data_check <- arxiv_2019_manual_results %>%
  count(is_open_data, manual_open_data) %>%
  pivot_wider(names_from = manual_open_data, values_from = n) %>%
  mutate(is_open_data = case_when(is_open_data == 1 ~ "Open data detected", TRUE ~ "No open data detected")) %>%
  select(is_open_data, `1`, `0`) %>%
  rename(Predicted = is_open_data, `Data available` = `1`, `No data available` = `0`) %>%
  arrange(desc(Predicted))

arxiv_2019_data_check %>% kable(caption = "ODDPub predictions for open data compared with manual check, arXiv sample")

```

```{r arxiv-accuracy-data-2019}
arc_2019_accuracy <- (as.numeric(arxiv_2019_data_check[1,2] + arxiv_2019_data_check[2,3])/nrow(arxiv_2019_manual_results)) %>% round(2)
arc_2019_sensitivity <- (as.numeric(arxiv_2019_data_check[1,2])/sum(arxiv_2019_data_check$`Data available`)) %>% round(2)
arc_2019_specificity <- (as.numeric(arxiv_2019_data_check[2,3])/sum(arxiv_2019_data_check$`No data available`)) %>% round(2)

tibble(Metric = c("Accuracy", "Sensitivity", "Specificity"),
      Value = c(arc_2019_accuracy, arc_2019_sensitivity, arc_2019_specificity)) %>% kable(caption = "ODDPub prediction accuracy, open data, arXiv sample")
```

```{r arxiv-confusion-code-2019}

arxiv_2019_code_check <- arxiv_2019_manual_results %>%
  count(is_open_code, manual_open_code) %>%
  pivot_wider(names_from = manual_open_code, values_from = n) %>%
  mutate(is_open_code = case_when(is_open_code == 1 ~ "Open code detected", TRUE ~ "No open code detected")) %>%
  select(is_open_code, `1`, `0`) %>%
  rename(Predicted = is_open_code, `Code available` = `1`, `No code available` = `0`) %>%
  arrange(desc(Predicted))

arxiv_2019_code_check %>% kable(caption = "ODDPub predictions for open code compared with manual check, arXiv sample")
```

```{r arxiv-accuracy-code-2019}
arc_2019_code_accuracy <- (as.numeric(arxiv_2019_code_check[1,2] + arxiv_2019_code_check[2,3])/nrow(arxiv_2019_manual_results)) %>% round(2)
arc_2019_code_sensitivity <- (as.numeric(arxiv_2019_code_check[1,2])/sum(arxiv_2019_code_check$`Code available`)) %>% round(2)
arc_2019_code_specificity <- (as.numeric(arxiv_2019_code_check[2,3])/sum(arxiv_2019_code_check$`No code available`)) %>% round(2)

tibble(Metric = c("Accuracy", "Sensitivity", "Specificity"),
      Value = c(arc_2019_code_accuracy, arc_2019_code_sensitivity, arc_2019_code_specificity)) %>% kable(caption = "ODDPub prediction accuracy, open code, arXiv sample")
```

# ODDPub algorithm performance on SocArXiv pre-prints

SocArXiv allows authors to input a link to their data source/repository upon submission of a pre-print. This link can then be accessed via the API metadata. The presence of a data link was used as an indicator that a pre-print provides open data for the purposes of validating the ODDPub algorithm. When available, a data link is stored under the variable name "attributes.data_links". The data was manipulated using functions from the R package tidyverse [@citetidyverse] to create a binary variable indicating data availability or lack thereof. We assume "attributes.data_links" to indicate the true availability of data for the purposes of validating the ODDPub algorithm. It is possible, however, that some authors failed to indicate their data availability in the proper field upon posting to SocArXiv, and thus some of the false positive may in fact be true positives.

Against the data availability indicated by pre-print authors in our 2019 sample, the ODDPub algorithm performed with an accuracy of 93 percent, a sensitivity of 52 percent, and a specificity of 94 percent. In our 2020 and 2021 sample, the algorithm performed with an accuracy of 79 percent, a sensitivity of 29 percent, and a specificity of 92 percent. Specific predictions are broken down in Table \@ref(tab:confusion-matrix-2019) and Table \@ref(tab:confusion-matrix-2020).

It is unclear the precise inclusion criteria for data submitted to the data link field. It is possible that some of the links provided lead to data sets that are publicly available for reuse, which would not constitute "open data" by the ODDPub algorithm's definition, in which case the accuracy could potentially be higher in reality than 93 percent and 79 percent in the samples considered.


```{r, confusion-matrix-2019}
# SocArXiv 2019 Confusion Matrix
soc_2019_check <- socarxiv_2019_open_data_results %>%
  mutate(data_link_available = case_when(is.na(attributes.data_links) ~ "No data linked", TRUE ~ "Data linked")) %>%
  count(data_link_available, is_open_data) %>%
  pivot_wider(names_from = data_link_available, values_from = n) %>%
  mutate(is_open_data = case_when(is_open_data == 1 ~ "Open data detected", TRUE ~ "No open data detected")) %>%
  rename(`ODDPub algorithm`= is_open_data) %>%
  arrange(desc(`ODDPub algorithm`))
soc_2019_check %>% kable(caption = "ODDPub predictions for open data compared with data links provided by authors, 2019 SocArXiv sample")
```

```{r, accuracy-2019}
soc_2019_accuracy <- (as.numeric(soc_2019_check[1,2] + soc_2019_check[2,3])/nrow(socarxiv_2019_open_data_results)) %>% round(2)
soc_2019_sensitivity <- (as.numeric(soc_2019_check[1,2])/sum(soc_2019_check$`Data linked`)) %>% round(2)
soc_2019_specificity <- (as.numeric(soc_2019_check[2,3])/sum(soc_2019_check$`No data linked`)) %>% round(2)

tibble(Metric = c("Accuracy", "Sensitivity", "Specificity"),
      Value = c(soc_2019_accuracy, soc_2019_sensitivity, soc_2019_specificity)) %>% kable(caption = "ODDPub prediction accuracy, open data, 2019 SocArXiv sample")
```

```{r confusion-matrix-2020}
# SocArXiv 2019 Confusion Matrix
soc_2020_check <- socarxiv_open_data_results %>%
  mutate(data_link_available = case_when(is.na(attributes.data_links) ~ "No data linked", TRUE ~ "Data linked")) %>%
  count(data_link_available, is_open_data) %>%
  pivot_wider(names_from = data_link_available, values_from = n) %>%
  mutate(is_open_data = case_when(is_open_data == 1 ~ "Open data detected", TRUE ~ "No open data detected")) %>%
  rename(`ODDPub algorithm`= is_open_data) %>%
  arrange(desc(`ODDPub algorithm`))
soc_2020_check %>% kable(caption = "ODDPub predictions for open data compared with data links provided by authors, COVID-19-related SocArXiv pre-prints sample")
```

```{r accuracy-2020}
soc_2020_accuracy <- (as.numeric(soc_2020_check[1,2] + soc_2020_check[2,3])/nrow(socarxiv_open_data_results)) %>% round(2)
soc_2020_sensitivity <- (as.numeric(soc_2020_check[1,2])/sum(soc_2020_check$`Data linked`)) %>% round(2)
soc_2020_specificity <- (as.numeric(soc_2020_check[2,3])/sum(soc_2020_check$`No data linked`)) %>% round(2)

tibble(Metric = c("Accuracy", "Sensitivity", "Specificity"),
      Value = c(soc_2020_accuracy, soc_2020_sensitivity, soc_2020_specificity)) %>% kable(caption = "ODDPub prediction accuracy, open data, COVID-19-related SocArXiv pre-prints sample")
```

<!-- # Markers -->

<!-- Insert details on what markers the `oddpub::open_data_search()` uses. Include info here directly from package documentation (i.e. keywords/phrases used for text parsing)? -->

<!-- # SocArXiv differences {#socarxivissues} -->

<!-- Insert details... -->



\newpage

# References


