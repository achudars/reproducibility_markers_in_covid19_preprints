---
title: "Reproducibility of COVID-19 research papers on medRxiv"
subtitle: "TBD"
author: "Annie Collins^[University of Toronto.]"
thanks: "We thank CANSSI... Code and data are available at: https://github.com/anniecollins/reproducibility."
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: "We create a dataset of all the papers published on bioRxiv and medRxiv between X and Y. We extract the text from these papers and parse them for keywords to do with the availability of data and scripts underpinning the paper. We find that X per cent of papers have X. Our paper demonstrates the need for Y."
output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(knitr)
library(here)

knitr::opts_chunk$set(echo = FALSE, error = FALSE, message = FALSE, warning = FALSE)

```

# Introduction

Scientists use open repositories of papers to more quickly disseminate their research than is possible in traditional journals. These repositories, such as arxiv, bioRxiv, and medRxiv, are a critical component of science and many results build on the work published there. So it is important that the results that are published are credible. These repositories are not peer-reviewed, and, in general, anyone with appropriate academic credentials can submit a paper. 

While neither peer-review nor credentials are a panacea nor a guarantee of quality, given the importance of these repositories, it is important that scientists impose on themselves various standards for their results. Following @weissgerber2021automated we examine papers about COVID-19 published to bioRxiv and medRxiv during 2020. We search for markers of open science and reproducibility, such as X, Y, and Z. 

We find that A, B, and C.

The remainder of this paper is structured as follows...




# Data & Methodology

Our primary data set consists of information extracted from the medRxiv repository combined with output from running a sample of COVID-19-related pre-prints through the Open Data Detection in Publications (ODDPub) text mining algorithm. 

We constructed this data set by first creating a local copy of the medRxiv repository via the medRxiv API and then filtering for papers related to the COVID-19 pandemic through several keyword searches to create our sampling frame (n = 9,929, including only the most recent version of any given pre-print). This data includes the following variables for each pre-print in the repository: title, abstract, author(s), date posted, research field, DOI, version number, corresponding author, corresponding author's institutional affiliation, and published DOI (if the paper has since been published in a peer reviewed journal). 

We then selected a random sample of these papers (n = 1,200) to check for open data and code markers using the ODDPub algorithm. This required downloading each paper as a PDF, converting the PDFs to text files, and conducting the open data and code detecting procedure to produce a results table indicating the presence of open data or open code markers in each paper (with a value of TRUE or FALSE for each marker and the relevant open data or open code statements when applicable).

Our final data set was formed by joining these two tables together via DOI to form a data set including all original, qualitative information for each pre-print alongside its open data and open code status and markers.

# Sample Summaries
```{r summary-table}
# open data/open code count summary table
med_open_data_results <- read_csv(here("outputs/data/med_open_data_results.csv"))
med_open_data_results %>% 
                    count(is_open_data, is_open_code) %>%
                    mutate(prop_total = n/1200) %>%
                    kable(col.names = c("Contains Open Data Markers", "Contains Open Code Markers", "Count", "Porportion of Total"))
```

```{r open-data-published-summary}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_data) %>% 
  pivot_wider(names_from = is_open_data, values_from = n) %>%
  rename("Open Data Markers" = `1`, "No Open Data Markers" = `0`) %>%
  kable()
```

```{r open-code-published-summary}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_code) %>% 
  pivot_wider(names_from = is_open_code, values_from = n) %>%
  rename("No Open Code Markers" = `0`, "Open Code Markers" = `1`) %>%
  kable()
```


```{r monthly-papers-total}
# Number of COVID-related papers posted each month in total
include_graphics(here("outputs/figures/papers_posted_total.pdf"))
```

```{r monthly-papers-by-condition}
# Number of papers posted each month from the sample
# include_graphics(here("outputs/figures/papers_posted_sample.pdf"))
# Likely don't need this anymore, see monthly-papers-sample-2
```

```{r proportion-of-open-data}
# Proportion of papers posted with open data or open code each month
include_graphics(here("outputs/figures/prop_open_per_month.pdf"))
```

```{r monthly-papers-by-condition}
# Number of papers for each open data/code status per month
include_graphics(here("outputs/figures/stack_conditions_plot.pdf"))
```
- Proportion of open data/code has fluctuated over time but shows no consistent overall increase or decrease over the course of the pandemic
- Proportion of open data seems to be relatively unimpacted by the number of papers posted to medRxiv in any given month
- Open code dipped in summer months when posting rate to medRxiv was high, bit questionable
- Open code follows similar patterns of increase/decrease as data but ultimately is less prevelant in the repository

# Publication
```{r}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_data) %>% 
  pivot_wider(names_from = is_open_data, values_from = n) %>%
  rename("Open Data Markers" = `1`, "No Data Code Markers" = `0`) %>% kable()
```

```{r}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_code) %>% 
  pivot_wider(names_from = is_open_code, values_from = n) %>%
  rename("No Open Code Markers" = `0`, "Open Code Markers" = `1`) %>% kable()
```


# Type of Paper
```{r}
# Number of open data/code papers by keyword for different types
include_graphics(here("outputs/figures/paper_type_keywords_plot.pdf"))
```
- Once data is restricted to certain types of papers - those including modeling, simulation, or machine learning - the availability of open data and code becomes much higher than the data set overall
- Relatively small sample size, but still important to note?
- Also an expected result



# Model

We run our analysis in `R` [@citeR]. 

In this analysis we distinguish between *posting* (the event that a pre-print is uploaded to medRxiv) and *publishing* (the event that the pre-print has subsequently been published in a peer reviewed journal).

```{r}
# logit model: posting rate vs. open data, publication rate vs. open code
```

```{r}
# logit model: open data/code vs. publication status
# pub_model <- glm(published ~ is_open_data + is_open_code, family = binomial, data = med_open_data_results)
```



# Results

# Discussion

There are many factors that impact the ability of an author to post make their data available

## Open Data/Code in broader context
- Discussion on open data/code in general and in life/medical sciences
- Are these a good indicator of reproducibility? How much do these contribute to reproducibility?

## First discussion point - Time
- Important to note that time plays a role in geographic focus in this context - early cases/research in China, then east and Southeast Asia, differences in different local data practices/policies
- On one hand good that there did not appear to be a drop in proportion of papers with open data/code as posting rates have risen, but also interesting to note that there has not been an overall increase
- 

## Second discussion point - Eventual Publication
- See weaknesses

## Third discussion point - Type of Paper
- Public health modeling/simulation papers more important near beginning of pandemic when less was understood about COVID as a disease specifically
- Perhaps contributed to the fact that the rate of open data/code from beginning of pandemic has been consistent throughout despite what we would hope is an increase in availability of COVID-19-related data?


## Weaknesses and next steps

- No open data/code was verified manually, all dependent on algorithm
- Publication info has high rate of false negatives (i.e. medRxiv data seems to miss a lot of papers that go on to be published)
- Want to look at geographic distribution and prevalence of open data/code - influence on open data policies and timing throughout pandemic
- Extend research to other indicators of reproducibility


\newpage

# Appendix {-}

Include info here directly from package documentation (i.e. keywords/phrases used for text parsing)?

\newpage


# References


