---
title: "Reproducibility of COVID-19 pre-prints on medRxiv"
subtitle: "Markers of open code or data not identified in 81 per cent of pre-prints"
author: "Annie Collins^[University of Toronto.]"
thanks: "Paper presented at Toronto Workshop on Reproducibility, 26 Februrary 2021. Collins thanks CANSSI Ontario for financial support. Code and data are available at: https://github.com/anniecollins/reproducibility."
date: "`r format(Sys.time(), '%d %B %Y')`"
abstract: "We create a dataset of all the pre-prints published on medRxiv between between 28 January 2020 and 31 January 2021. We extract the text from these pre-prints and parse them looking for for keyword markers signalling the availability of the data and code underpinning the pre-print. We are unable to find markers of either open data or open code for 81 per cent of the pre-prints in our sample. Our paper demonstrates the need to have authors categorize the degree of openness of their pre-print as part of the medRxiv submissions process, and more broadly, the need to better integrate open science training into a wide range of fields."
output:
  bookdown::pdf_document2:
toc: FALSE
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error = FALSE, message = FALSE, warning = FALSE)
```

```{r loadpackages, include=FALSE}
library(here)
library(knitr)
library(lubridate)
library(tidyverse)
```


```{r loaddata, include=FALSE}
med_open_data_results <- read_csv(here("outputs/data/med_open_data_results.csv"))
```


# Introduction

Scientists use open repositories of papers to more quickly disseminate their research than is possible in traditional journals. These repositories, such as arxiv, bioRxiv, and medRxiv, are a critical component of science and many results build on the work published there. So it is important that the results that are published are credible. These repositories are not peer-reviewed, and, in general, anyone with appropriate academic credentials can submit a paper. 

While neither peer-review nor credentials are a panacea nor a guarantee of quality, given the importance of these repositories, it is important that scientists impose on themselves various standards for their results. Following @weissgerber2021automated we examine papers about COVID-19 published to bioRxiv and medRxiv during 2020. We search for markers of open science and reproducibility, such as X, Y, and Z. 

We find that A, B, and C.

The remainder of this paper is structured as follows...




# Data & Methodology

Our primary data set consists of information extracted from the medRxiv repository combined with output from running a sample of COVID-19-related pre-prints through the Open Data Detection in Publications (ODDPub) text mining algorithm [@citePDDPub], using the `oddpub` package [@citeODDPubpackage]. 

We constructed this data set by first creating a local copy of the medRxiv repository via the medRxiv API and then filtering for papers related to the COVID-19 pandemic through several keyword searches to create our sampling frame (n = 9,929, including only the most recent version of any given pre-print). This data includes the following variables for each pre-print in the repository: title, abstract, author(s), date posted, research field, DOI, version number, corresponding author, corresponding author's institutional affiliation, and published DOI (if the paper has since been published in a peer reviewed journal). 

We then selected a random sample of these papers (n = 1,200) to check for open data and code markers using the ODDPub algorithm. This required downloading each paper as a PDF, converting the PDFs to text files, and conducting the open data and code detecting procedure to produce a results table indicating the presence of open data or open code markers in each paper (with a value of TRUE or FALSE for each marker and the relevant open data or open code statements when applicable). These PDFs were downloaded and converted to txt files using the `medrxivr` package [@citemedrxivr].

Our final data set was formed by joining these two tables together via DOI to form a data set including all original, qualitative information for each pre-print alongside its open data and open code status and markers. Broadly, we are unable to find markers of either open data or open code for around 81 per cent of pre-prints (Table \@ref(tab:summarycounts)).

```{r summarycounts}
# open data/open code count summary table
med_open_data_results %>% 
  count(is_open_data, is_open_code) %>%
  mutate(prop_total = n / sum(n)) %>%
  mutate(is_open_data = ifelse(is_open_data == 0, "No", "Yes"),
         is_open_code = ifelse(is_open_code == 0, "No", "Yes")) %>% 
  kable(col.names = c("Contains Open Data Markers", "Contains Open Code Markers", "Count", "Proportion of Total"),
        caption = "Counts and proportions of open data and code markers in our sample",
        booktabs = TRUE,
        digits = 2)
```

We do not see a substantial difference in the presence of markers of open data or code between whether a pre-print ended up being published. There was a low number of papers that had markers of either open data or code in both those that were published and those that were not published (Tables \@ref(tab:open-data-published-summary) and \@ref(tab:open-code-published-summary)). It is important to note here that our dataset imperfectly characterises publication. In particular, it does not have the publication details for some papers that were published. And even if it were a perfect record, there is a publication lag that may skew the results.

```{r open-data-published-summary}
med_open_data_results %>% 
  group_by(published) %>% 
  count(is_open_data) %>% 
  pivot_wider(names_from = is_open_data, values_from = n) %>%
  mutate(published = ifelse(published == 0, "No", "Yes")) %>%
  rename("Open Data Markers" = `1`, 
         "No Open Data Markers" = `0`,
         Published = published) %>%
  kable(caption = "Counts and proportions of open data markers by whether the pre-print was published",
        booktabs = TRUE)
```

```{r open-code-published-summary}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_code) %>% 
  pivot_wider(names_from = is_open_code, values_from = n) %>%
  rename("No Open Code Markers" = `0`, 
         "Open Code Markers" = `1`,
         Published = published) %>%
  kable(caption = "Counts and proportions of open code markers by whether the pre-print was published",
        booktabs = TRUE) 
```

The number of pre-prints posted by month reached its maximum in May 2020, and has remained reasonably steady at around 750 per month since around August 2020 (Figure \@ref(fig:monthly-papers-total)).

```{r monthly-papers-total, fig.cap = "Number of pre-prints related to COVID-19 posted to medRxiv"}
# Number of COVID-related papers posted each month in total
med_results <- read_csv(here::here("outputs/data/med_results.csv")) # All COVID medRxiv papers (9,929)

med_results %>% 
  group_by(month = floor_date(date, "month")) %>% 
  summarise(count=n()) %>% 
  ggplot(aes(month, count)) + 
  # geom_line(stat="identity") +
  geom_col() + 
  labs(x = "Month",
       y = "Number of pre-prints") +
  theme_minimal()
```

```{r monthly-papers-by-condition}
# Number of papers posted each month from the sample
# include_graphics(here("outputs/figures/papers_posted_sample.pdf"))
# Likely don't need this anymore, see monthly-papers-sample-2
```

```{r proportion-of-open-data}
# Proportion of papers posted with open data or open code each month
include_graphics(here("outputs/figures/prop_open_per_month.pdf"))
```

```{r monthly-papers-by-condition-stack}
# Number of papers for each open data/code status per month
include_graphics(here("outputs/figures/stack_conditions_plot.pdf"))
```
- Proportion of open data/code has fluctuated over time but shows no consistent overall increase or decrease over the course of the pandemic
- Proportion of open data seems to be relatively unimpacted by the number of papers posted to medRxiv in any given month
- Open code dipped in summer months when posting rate to medRxiv was high, bit questionable
- Open code follows similar patterns of increase/decrease as data but ultimately is less prevelant in the repository

# Publication



```{r}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_data) %>% 
  pivot_wider(names_from = is_open_data, values_from = n) %>%
  rename("Open Data Markers" = `1`, "No Data Code Markers" = `0`) %>% kable()
```

```{r}
med_open_data_results %>% 
  group_by(published) %>% count(is_open_code) %>% 
  pivot_wider(names_from = is_open_code, values_from = n) %>%
  rename("No Open Code Markers" = `0`, "Open Code Markers" = `1`) %>% kable()
```


# Type of Paper

```{r}
# Number of open data/code papers by keyword for different types
include_graphics(here("outputs/figures/paper_type_keywords_plot.pdf"))
```

- Once data is restricted to certain types of papers - those including modeling, simulation, or machine learning - the availability of open data and code becomes much higher than the data set overall
- Relatively small sample size, but still important to note?
- Also an expected result



# Model

We run our analysis in `R` [@citeR]. 

In this analysis we distinguish between *posting* (the event that a pre-print is uploaded to medRxiv) and *publishing* (the event that the pre-print has subsequently been published in a peer reviewed journal).

```{r}
# logit model: posting rate vs. open data, publication rate vs. open code
```

```{r}
# logit model: open data/code vs. publication status
# pub_model <- glm(published ~ is_open_data + is_open_code, family = binomial, data = med_open_data_results)
```



# Results

# Discussion

There are many factors that impact the ability of an author to post make their data available

## Open Data/Code in broader context
- Discussion on open data/code in general and in life/medical sciences
- Are these a good indicator of reproducibility? How much do these contribute to reproducibility?

## First discussion point - Time
- Important to note that time plays a role in geographic focus in this context - early cases/research in China, then east and Southeast Asia, differences in different local data practices/policies
- On one hand good that there did not appear to be a drop in proportion of papers with open data/code as posting rates have risen, but also interesting to note that there has not been an overall increase
- 

## Second discussion point - Eventual Publication
- See weaknesses

## Third discussion point - Type of Paper
- Public health modeling/simulation papers more important near beginning of pandemic when less was understood about COVID as a disease specifically
- Perhaps contributed to the fact that the rate of open data/code from beginning of pandemic has been consistent throughout despite what we would hope is an increase in availability of COVID-19-related data?


## Weaknesses and next steps

- No open data/code was verified manually, all dependent on algorithm
- Publication info has high rate of false negatives (i.e. medRxiv data seems to miss a lot of papers that go on to be published)
- Want to look at geographic distribution and prevalence of open data/code - influence on open data policies and timing throughout pandemic
- Extend research to other indicators of reproducibility


\newpage

# Appendix {-}

Include info here directly from package documentation (i.e. keywords/phrases used for text parsing)?

\newpage


# References


