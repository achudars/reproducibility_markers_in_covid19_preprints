A note on tools for prediction under uncertainty and identifiability of
SIR-like dynamical systems for epidemiology
Chiara Piazzolaa , Lorenzo Tamellinia,∗, Raúl Temponeb,c
a Consiglio

arXiv:2008.01400v3 [stat.ME] 9 Nov 2020

Nazionale delle Ricerche - Istituto di Matematica Applicata e Tecnologie Informatiche “E. Magenes”
(CNR-IMATI), Via Ferrata 5/A, 27100 Pavia, Italy
b Alexander von Humboldt Professor in Mathematics for Uncertainty Quantification, RWTH Aachen University, Pontdriesch
14-16, 52062, Aachen, Germany
c King Abdullah University of Science and Technology (KAUST) - Computer, Electrical and Mathematical Sciences &
Engineering Division (CEMSE), Thuwal, 23955-6900, Saudi Arabia.

Abstract
We provide an overview of the methods that can be used for prediction under uncertainty and data fitting
of dynamical systems, and of the fundamental challenges that arise in this context. The focus is on SIR-like
models, that are being commonly used when attempting to predict the trend of the COVID-19 pandemic.
In particular, we raise a warning flag about identifiability of the parameters of SIR-like models; often, it
might be hard to infer the correct values of the parameters from data, even for very simple models, making
it non-trivial to use these models for meaningful predictions. Most of the points that we touch upon are
actually generally valid for inverse problems in more general setups.
Keywords: Dynamical Systems, Mathematical Epidemiology, Uncertainty Quantification, Model
Identifiability, Bayesian Inversion, Fisher Approximation

1. Introduction
This work provides an overview of the methods that can be used for prediction under uncertainty (also
known as Uncertainty Quantification) and data fitting of dynamical systems, and of the fundamental challenges that arise in this context. While this work can be easily connected with the usage of SIR-like models
for the COVID-19 pandemic, the discussion presented here is actually valid for compartmental models in
epidemiology and for dynamical systems in general; most points would actually be valid also in the context
of inverse problems with spatial inhomogeneities. We put particular emphasis on the issue of identifiability,
whose possible lack might cause serious issues when attempting long-term forecasts. To make our case clearer,
in this work we use synthetic data only, which gives us full control on the errors generated by the numerical
identifiability procedure.
For the sake of compactness, we have chosen to not provide many technical details on the topics that we
touch, but rather point the reader to the relevant bibliography. For the same reason, most of the bibliography
for further reading is provided at the end of each section, rather than during the discussion. We chose,
however, to keep a rather concrete register, therefore each section comes with one or two short examples.
We use for this purpose simple models, with the understanding that the points raised by the examples will
be even more valid for more complicated models. For a more bird’s eye view on data-informed modeling
and identifiability, and their ramifications in the general society, see e.g. [1, 2]. For readers’ convenience, we
report here the topic of each section and list the examples:
Section 2: SIR-like models in epidemiology
Section 3: Forward Uncertainty Quantification (UQ): tools to make predictions under uncertainty
∗ Corresponding

author
Email addresses: chiara.piazzola@imati.cnr.it (Chiara Piazzola), tamellini@imati.cnr.it (Lorenzo Tamellini),
tempone@uq.rwth-aachen.de, raul.tempone@kaust.edu.sa (Raúl Tempone)

Preprint submitted to Elsevier

November 10, 2020

Section 4: Sensitivity analysis: pinpointing what parameters we need to get right, and preliminary assessment of feasibility of inversion
Section 5: Inverse UQ (data fitting) as a preliminary step to tune the pdf of the parameters to the data
Section 6: An ideal UQ workflow, from data to predictions under uncertainty
Section 7: Structural identifiability
Section 8: Practical identifiability
Section 9: Discussion and conclusions: a revisited UQ workflow
Example
Example
Example
Example
Example
Example
Example
Example
Example

1:
2:
3:
4:
5:
6:
7:
8:
9:

Forward UQ of a SIR model
Computing the Sobol indices for a SIR model
Inverse and posterior-based forward UQ of a SIR model
Incorporating prior information on parameters in inverse UQ
Inverse UQ when different data types have different noise levels
Structural identifiability of a SIR model by differential algebra
Structural identifiability of a SIR model by mapping approach
Practical non-identifiability of a SIR model with unknown under-reporting factor
Structural and practical identifiability of a SEIRD model

All the numerical results in the examples have been obtained with Matlab, and the source code is available
at https://sites.google.com/view/sparse-grids-kit. Some of the examples rely on the functionalities
of the Sparse Grids Matlab Kit, which is developed by some of the authors of this manuscript and can be
downloaded from the same website.
2. SIR-like models in epidemiology
The recent COVID-19 pandemic has triggered an unprecedented effort among researchers worldwide1 . In
the field of applied mathematics, a large share of this effort has been focusing on devising tools to forecast
the trends of the epidemics.
The most widely used tools to this end are compartmental models, where individuals of a population are
categorized in compartments (Infected, Recovered, Dead, etc.) and can transition from one compartment to
another according to some “transition rates”. The origin of these models can be traced back to the work of
Kermack and McKendrick [3]. The actual model in that paper was a system of integro-differential equations.
Some simplifications allow to rewrite those equations as a non-linear system of ordinary differential equations
(ODEs), whose simplest form is the SIR model:

β


IS
Ṡ = −


Npop

β
(1)
I˙ =
IS − rI


N
pop



Ṙ = rI,
which describes the time-evolution of three compartments: individuals (S)usceptible to the disease, individuals (I)nfected with the disease, and finally individuals (R)emoved from the disease dynamics (either
because they recovered, assuming immunity after having contracted the disease, or died). The total number
of individuals in the population Npop = S + I + R is supposed constant, and individuals transition from
one compartment to the next one with certain transition rates β, r. Besides the ODE, the Kermack and
McKendrick integro-differential equations can also be rewritten as a stochastic differential equation whose
limit is the ODE equation; see [4].
Of course, a simple SIR model is insufficient to capture the dynamics of the COVID-19 disease, due to its
biological peculiarities, such as the incubation time and the presence of asymptomatic carriers of the disease,
as well as human interventions such as individuals in quarantine (hence with limited transmissivity) and
hospitalized. Therefore, many works in the COVID-19 literature consider more complex variations of the
1 On

July 14th: 1600+ preprints on arxiv.org, 5100+ preprints on medrxiv.org, 1400+ preprints on biorxiv.org

2

simple SIR model (1) with, for example, more compartments, time-dependent coefficients, or by introducing
network models, in an attempt to better describe the dynamics of the pandemic and provide reliable forecasts
of its evolution. Of course, one should always keep in mind that while more complex models have potentially
a greater predictive power, they are also more complex to analyze and tune, so that one should ideally look
for the model with the optimal trade-off between these two aspects.
Bibliography and further reading.
• For a survey of SIR-like models “pre-COVID-19” for diseases such as Zika, Dengue, Ebola, H1N1, see
e.g. [5, 4, 6, 7, 8, 9, 10, 11].
• For some examples of SIR-like models for COVID-19, see e.g. [12, 13, 14, 15, 16, 17, 18].
• A somewhat different approach is proposed in [19], where the underlying model is a simple SIR, with
a more complex model for the probability distribution of the delays between infection and the observed
events (hospitalization, recovery, death).
• Control strategies for SIR-like systems are also an important topic, see e.g. [20].
3. Forward Uncertainty Quantification (UQ): tools to make predictions under uncertainty
In general, SIR-like models can be written as ODE systems for a state vector X with Nstates components.
The evolution of the system depends on Ncoef coefficients p = [p1 , . . . , pNcoef ] and on the Nstates initial
conditions q = [q1 , . . . , qNstates ]. Moreover, we might be interested in monitoring not only the states of the
system but also some related quantities Y (Quantities of Interest, say we have Nqoi of them), which can
be derived from X by an observation operator G, that in turn might depend on Nhyp hyper-parameters
h = [h1 , . . . , hNhyp ]:


Ẋ = f (X, p)
(2)
X(t0 ) = q


Y (t) = G(X(t), h),
where ∀t ∈ [0, T ] we have X ∈ RNstates , Y ∈ RNqoi , and f (·, p) : RNstates → RNstates , G(·, h) : RNstates →
RNqoi , p ∈ RNcoef , q ∈ RNstates , h ∈ RNhyp .
We collect coefficients and initial conditions in a vector ϑ = [p, q] with Nϑ = Ncoef + Nstates components.
Throughout the manuscript, we refer to ϑ as parameters, and we will write X(ϑ), Y (ϑ, h) to emphasize the
dependence of the states and quantities of interest on parameters and hyper-parameters. For SIR, p = [β, r],
and Y might be for instance:
• the prevalence data, i.e., the number of infected individuals at a specific time, G(X(t)) = I(t);
• the incidence data, i.e., the number of new cases over the reference time period, G(X(t)) =

β
Npop I(t)S(t);

• the peak-time of number of infected persons: G(X) = arg maxt∈[0,T ] I(t);
• the peak-time of the cumulative incidence data i.e., the new infected persons in a time-window of length
R t+∆ β
∆: G(X, ∆) = arg maxt t
Npop S(s)I(s)ds.
Another important scenario is under-reporting, where we assume that due to insufficient measurements, we
1
I(t) (K being possibly unknown);
observe only a fraction K of the total number of infected, G(X(t), K) = K
similar under-reporting scenarios could be of course conceived also for other quantities. Observe that at this
point of the manuscript we are not concerned whether these quantities are reasonably easy to measure and
obtain: for instance, for most infections data available are typically incidence rather than prevalence. At this
level, we are just giving examples of the mathematical setup of the problem, in an ideal scenario where we
have access to all sort of measurements.
Typically, most of the parameters (and possibly the hyper-parameters as well) are not known exactly and
they are either taken from literature or calibrated from data. We can then assume that these parameters are
random variables with a certain probability density function (pdf): for instance, uniform random variables
3

1

12

0.8

10
8

0.6

S
I
R

0.4
0.2

6

0

50

100

150

8

2

2
0.5

4
3

0.2

2

6
4

0

pdf of S, sparse grids
pdf of R, sparse grids
pdf of S, MC
pdf of R, MC

10

4

0

0

0.3

12
pdf of S, sparse grids
pdf of I, sparse grids
pdf of R, sparse grids
pdf of S, MC
pdf of I, MC
pdf of R, MC

1

0

0.1
1

0
0

0.5

1

0

0

50

100

150

0

0.5

1

Figure 1: Left: SIR dynamics. The colored lines represent the SIR dynamics obtained by Monte Carlo samples of β, r, and the
thick black lines represent the average computed by sparse grids. The other panels represent pdfs of quantities of interest: SIR
states at T = 30, 100 (by Monte Carlo and sparse grids), peak-time for I and peak-value for I (sparse grids only).

over a variability range, or Gaussian random variables centered around a most likely value.2 Then, a natural
question is: how does the variability of the parameters impact the quantities of interest Y of the SIR-like
model at hand? Or otherwise, what is the variability range of Y as the parameters range over their values?
This kind of analysis is known as forward UQ in computational science and engineering. The most
straightforward way to accomplish this task is by sampling methods, i.e., by generating M samples of the
parameters ϑ1 , ϑ2 , . . . ϑM according to their probability distribution, solving the SIR-like system for each
ϑi , and estimating statistics such as mean, standard deviation, confidence bands, and the probability density
function of Y from the corresponding values Y (ϑ1 , h), Y (ϑ2 , h), . . . Y (ϑM , h). The easiest sampling scheme
is Monte Carlo, but more advanced sampling techniques can be used (Latin Hypercube Sampling, Stratified
Sampling, Quasi-Monte-Carlo, Sparse grids, among others; see the bibliography at the end of this section).
Within sampling schemes, quantities such as mean, standard deviations and higher moments with respect to
the parameters can be computed by averaging over the M samples of the model results:
Eϑ [Y (ϑ, h, t)] ≈

M
X

ωi Y (ϑi , h, t),

(3)

i=1

where the parameter samples ϑi and the weights ωi depend on the specific sampling method used. For
1
. The probability density function of the quantity of
instance, Monte Carlo employs random ϑi and ωi = M
interest can be approximated by, for example, histograms or kernel density estimates, see e.g. [21, 22].
Example 1 (Forward UQ of a SIR model). Consider a SIR model with initial conditions S(0) = 0.95, I(0) =
0.05, R(0) = 0. The survey on the literature performed by [16] suggests these ranges for the parameters:
β ∈ [0.25, 0.35], r ∈ [0.06, 0.18]. We assume that a-priori we have no knowledge that any value of β, r is more
plausible than others; therefore, we assume that β, r are uniform independent random variables. We solve
the SIR system with Matlab’s ode45 up to final time T = 150.
Figure 1-left shows the SIR dynamics obtained by 100 Monte Carlo samples. The black lines are the
average trajectories of SIR obtained by sampling values of β and r (65 samples with sparse grids sampling).
The remaining panels show pdfs of quantities of interest of SIR: SIR states at T = 30 (after the average peak
position) and T = 100 (when the dynamics is over), again computed both with sparse grids (solid line) and
Monte Carlo (circle markers); peak time and peak intensity (we only show the pdf obtained by sparse grids).
Figure 2 shows the so-called response surface, i.e., a plot showing how the quantity of interest changes as β
and r vary in their range (we report only those obtained by sparse grids). Response surfaces are useful to
derive information on the general trends of the system, and to quickly approximate the value of a quantity
of interest without evaluating the full model, i.e., they act as a surrogate model for the dynamical system;
the pdfs of the quantities of interest obtained by sparse grids have actually been obtained by querying these
response surfaces rather than the full model.
Bibliography and further reading.
2 Technically, most parameters of compartmental models must be positive, so a Gaussian random variable is not suitable
and one should consider other random variables, e.g. Beta or log-normal. However, here we are keeping things on a simple/introductory level, and in any case a Gaussian random variable can be truncated to ensure positivity if needed.

4

22
0.6
0.55

21
0.4

0.8

0.5

0.5

20
0.75

0.45

0.35

0.4

0.7

19

0.65

18

0.4

0.3

0.35

0.3
0.25

0.6

0.2

0.55

0.3

17
0.2

0.25

16
0.2
0.5
0.15

0.1

15

0.15

0.45

0.1

0.1

14
0.25

0.4

0
0.2

0.25

0.15
0.35
0.1

r

0.3
0.05

0.05
0.05

0.3
0.1

0.05
0.05

0.35
0.35
0.2

0.3

0.25

0.3

0.1

0.2

0.15
0.15

0.25

r

0.15

0.1
0.2

0.35

r

0.05

0.25

0.35

r

0.3

0.15

0.1
0.05

r

0.2

0.35

Figure 2: Sparse grids response surfaces for the SIR quantities of interest. From left to right: S at T = 30, I at T = 30, R at
T = 30, peak time, and peak value.

• For random sampling methods (Monte Carlo, Latin Hypercube Sampling, Stratified Sampling), see [23,
24, 25]. Random sampling methods are robust and easy to implement, but have a poor accuracy (typically
proportional to M −1/2 , with M denoting the number of samples).
• For sparse grids sampling methods, see [26, 27, 28, 29]. These are deterministic (i.e., non-random)
sampling schemes that generalize tensor (cartesian) grid sampling when the parameter space is highdimensional, in which case a cartesian grid sampling scheme would be too expensive. They are less
straightforward than random sampling methods, but guarantee greater accuracy, at least for problems
up to a few tens of parameters. These tools have been developed in the context of UQ for models that
are expensive to evaluate, whereas evaluating a SIR-like model is typically very fast. Therefore, their
use is not as crucial in the context of COVID-19, and random sampling methods might be favored for
their straightforwardness. Sparse grids have still an advantage over random sampling for sensitivity
analysis, see the next section.
• A somewhat intermediate possibility are Quasi Monte Carlo sampling methods, such as Sobol or Halton
sequences. These are also deterministic sampling schemes as well, that aim at covering the space
of parameters in the “most uniform way” (space filling) [23, 24, 30]. They typically have accuracy
proportional to M −1 , with M denoting the number of samples.
4. Sensitivity analysis: pinpointing what parameters we need to get right, and preliminary
assessment of feasibility of inversion
Sensitivity analysis aims at assessing which parameters have the largest impact on the quantities of interest. This information is crucial to determine which parameters should be subjected to further investigations
to reduce their variability. The sensitivity analysis can be local or global :
• local sensitivity analysis is usually based on the derivatives of the quantities of interest with respect to
ϑ, upon fixing each ϑi at some representative value (average, median, mode).3
• global sensitivity analysis considers the total variability of a quantity of interest and decomposes such
variability into elementary components, each due to ϑi individually or to mixed effects such as ϑi ϑj ,
ϑi ϑj ϑk , . . .: the larger the component, the more sensitive the quantity of interest to ϑi is.
In these short notes we focus on the Sobol indices for global sensitivity analysis, which are variance-based
indices, i.e. the variability of the quantity of interest is measured as its variance [31]. The total variance
is decomposed as follows: one term due to each ϑi ; one term due to each mixed effect composed of two
parameters, ϑi ϑj ; one term for each mixed effect composed of three parameters ϑi ϑj ϑk , and so forth. The
3 Of course here we are assuming that the quantity of interest depends smoothly on ϑ, so that it is possible to compute the
derivatives. This is not always obvious and should be checked.

5

1
Sobol indices, S

1

Sobol indices, I

1

0.9

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.6
0.5

0.5

0.4

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0.1

0
20

40

60

80

100

Beta
r

0.5

0.4

0.3

0

0.6

S
I
R

0.6
Beta
r

0.4

0

0.8

0.7

0.6
Beta
r

Sobol indices, R

1

0.2
0

0
0

20

40

60

80

100

0.4

0

20

40

60

80

100

0

50

100

150

Figure 3: Time-evolution of Sobol indices for the SIR compartments, and SIR trajectories obtained by reducing the range of
variability of r. The principal indices are represented by the solid line, while the total indices are represented by the dashed line.

sum is then normalized to one, and each quantity thus obtained is called Sobol index:
1=

Nϑ
X

si +

i=1

Nϑ
X

sij +

i,j=1,i6=j

Nϑ
X

sijk + . . .

(4)

i,j,k=1,i6=j6=k

The Sobol index of each parameter ϑi per se, si , is usually reported as an indicator of the importance of each
parameter, and is called the principal Sobol index. Another relevant quantity is the total Sobol index of a
parameter, sTi , which is obtained by adding to the principal Sobol index of ϑi all the Sobol indices of mixed
effects of which ϑi is part, e.g. for ϑ1 :
sT1 = s1 +

Nϑ
X

s1j +

j=1,j6=1

Nϑ
X

s1jk + . . .

j,k=1,j6=1,k6=1

PNϑ
PNϑ T
Note that in general i=1
si < 1 and i=1
si > 1. This approach bears many similarities with the ANOVA
decomposition in statistics. An important observation is that, with an eye to parameter identification, we can
expect that if the Sobol index of a parameter is small, it will be hard to recover its value from measurements
of the quantity of interest. We also remark that as a general rule of thumb, the larger the range of values of
a parameter, the larger its corresponding Sobol indices.
Example 2 (Computing the Sobol indices for a SIR model). Consider again the SIR example of the previous
section. The Sobol decomposition of any quantity of interest Y reads 1 = sβ + sr + sβr and the total Sobol
indices can be computed as sTβ = sβ +sβr , sTr = sr +sβr . Figure 3 shows the time-evolution of the Sobol indices
(principal and total) for the SIR states: the principal indices are represented by the solid line, while the total
indices are represented by the dashed line. The principal and total indices behave very similarly, indicating
that the interaction between the two parameters is quite limited. Note that the Sobol indices are not constant
in time and behave differently for the different compartments. More specifically, the asymptotic regime is
mostly dictated by r for all the compartments, while β impacts more in the transient regime, especially in the
case of the compartment R. This has an impact on the inversion procedure. In particular, severe difficulties
in the estimation of β can be encountered if the data of R are missing or too noisy. Moreover, note that the
influence of r is larger, in general. Further evidence of this is shown in the right-most panel, where we show
the variability of the trajectories if the range of r is reduced to [0.06, 0.1]. The overall variability is greatly
reduced, as expected.
Bibliography and further reading.
• Classical books on sensitivity are e.g. [32, 33].
• Sobol indices can be computed either by Monte Carlo sampling, see e.g. [33], or perhaps more conveniently by polynomial expansions or sparse grids sampling, see e.g. [34].
• Sobol indices analyses for SIR-like problems can be found in [35, 5].
• An alternative to Sobol indices are the Morris indices [36].
• Sobol indices can also be computed with respect to measures of variability other than variance, see e.g.
[37].
6

5. Inverse UQ (data fitting) as a preliminary step to tune the pdf of the parameters to the
data
The sections above discussed some general elementary tools to perform predictions under uncertainty once
the pdfs for the parameters have been chosen. This section discusses the preliminary step to the UQ process,
i.e. how to construct pdfs for the parameters, and in particular how to do so by merging prior information
on the parameters and the available data. Upon deriving these data-informed pdfs, we will use them to carry
out the UQ analysis. In literature, data-informed pdfs are often called posterior pdfs, ρpost , as opposed to
prior pdfs, ρprior , before the data are available.
Computing the posterior pdfs of the parameters can be done by means of the Bayes theorem on conditional
probabilities. This procedure is quite general and includes as a special case the least-squares approach for
data fitting (this connection will be made clearer later). For ease of exposition, we exemplify the procedure
over a specific example - extension to other problems is relatively straightforward (see e.g. Examples 4, 5
later on). For now we assume that:
• We have at our disposal Nmeas measurements of the I state and Nmeas measurements of the R
state, at equispaced times ti = i∆t, i = 1, 2, 3, . . . Nmeas . In total we have 2Nmeas data, D =
{Iˆ1 , Iˆ2 , . . . , R̂1 , R̂2 , . . .}. As stated previously, please note that we are not hinting in any way that
having measurements of both I(t) (i.e., prevalence data) and R(t) is what happens in real scenarios: we
make this (quite restrictive in practice) assumption to put ourselves in the easiest scenario to illustrate
the mathematical procedure. Using more realistic data would add mathematical technicalities without
giving any further insight;
• These data correspond to some values ϑtrue of coefficients and initial conditions of the system (2);
• The prior pdfs for ϑ are uniform (see Example 4 for an example with Gaussian priors);
• Our measurements are under-reported by a factor K, i.e., we are able to measure only a fraction of the
actual compartments;
• Data are noisy, i.e. affected by some random errors I,i , R,i , that are modeled by independent random
variables with zero mean and standard deviation σ;
• the standard deviation σ is identical for I and R (see Example 5 for the generalization to the case
where the two compartments have different σ);
• I,i , R,i are Gaussian random variables N (0, σ 2 ). See discussion at the end of the section for bibliography on more general models;
• K, σ are hyper-parameters constant in time. We assume for the moment that K is known and σ is
unknown (see Examples 6, 7, 8 for a discussion on how to determine K in case it is assumed unknown
as well).
In formulas our data model is the following:

Iˆi = G(I(ϑtrue , ti ), K) + I,i =

1
K I(ϑtrue , ti )

R̂ = G(R(ϑ
i
true , ti ), K) + R,i =

+ I,i ,

1
K R(ϑtrue , ti )

i = 1, 2, . . . Nmeas
(5)

+ R,i ,

i = 1, 2, . . . Nmeas .

We also introduce the 2Nmeas misfits M = {MI,1 , MI,2 , . . . , MR,1 , MR,2 , . . .} between the data and the model
predictions, obtained upon fixing the parameters at some estimate ϑguess of ϑtrue :
(
1
MI,i (ϑguess ) = Iˆi − K
I(ϑguess , ti ), i = 1, 2, . . . Nmeas
(6)
1
MR,i (ϑguess ) = R̂i − K
R(ϑguess , ti ), i = 1, 2, . . . Nmeas .

7

5.1. Bayes Theorem and posterior distributions
The Bayes theorem provides us with a practical formula to compute the posterior pdf of the parameters
ϑ, i.e., with a means of adjusting the prior pdf to the data at hand. An informal writing of the Bayes formula
is
1
pdf(ϑ given M) = pdf(M given ϑ) × pdf(ϑ) ×
(7)
pdf(M)
where “pdf(ϑ given M)” is the pdf of the parameters when given the misfits, hence given the data (i.e.,
the posterior pdf that we aim at computing), while “pdf(ϑ)” is the pdf of the parameters based only on
a-priori information. The “pdf(M)” can be simply considered to be the normalization constant such that
the posterior pdf is actually a pdf (i.e., its integral is equal to 1). Therefore, to make the computation of the
posterior pdf practical we only need to know the expression of the “pdf(M given ϑ)”, which is the so-called
likelihood function; we will denote this quantity as L(ϑ).
Deriving an expression for L(ϑ) is quite straightforward. If ϑguess were the true values, then the probability that the misfits M have certain values is the probability that the measurement errors I,i , R,i have
those values (cf. Equations (5) and (6)). By assumption, we know that I,i , R,i are independent Gaussian
random variables with zero mean and standard deviation σ, therefore,
L(ϑ) =

NY
meas
i=1

√

1

−1

2πσ 2

ˆ

1

2

e 2σ2 ( K I(ϑ,ti )−Ii )

NY
meas

√

i=1

1

−1

2πσ 2

2

1

e 2σ2 ( K R(ϑ,ti )−R̂i )

(8)

so that the posterior pdf of the parameters reads
ρpost (ϑ|D) ∝ L(ϑ)ρprior (ϑ) =

NY
meas
i=1

√

1
2πσ 2

e

−1
2σ 2

1
(K
I(ϑ,ti )−Iˆi )2

NY
meas
i=1

√

1
2πσ 2

−1

1

2

e 2σ2 ( K R(ϑ,ti )−R̂i ) ρprior (ϑ) (9)

where the ∝ symbol is used to signify that we have omitted the normalization constant.
5.2. Computational challenges of working with the posterior pdf
Equipped with (9), we would then only need to proceed as in Section 3 and perform the UQ analysis.
Although conceptually straightforward, this approach can be practically challenging, because it is not easy
to obtain samples of the random parameters ϑ distributed according to the posterior pdf (9). The classical
computational tool to this end is the so-called Markov-Chain Monte Carlo - MCMC [38], which generates
a sequence of proposed values of ϑ that are asymptotically distributed according to ρpost . A nice feature of
MCMC algorithms is that they do not require knowledge of the normalization constant. The use of MCMC
for forward UQ has however some drawbacks:
1. the likelihood function has to be evaluated at every proposed ϑ, which requires evaluating the SIR-like
model. Even if evaluating SIR-like models for a single choice of ϑ is quite cheap, this procedure can
be overall expensive, bearing in mind that until the sequence of generated ϑ enters in the asymptotic
regime, the values generated have to be discarded because they are not distributed according to ρpost .
2. most MCMC algorithms proceed by acceptance-rejection criteria, where a new value of ϑ is generated
and then rejected if doesn’t agree with certain criteria; this leads to a further increase in the number
of model evaluations;
3. the forward UQ analysis based on the MCMC samples is a Monte Carlo analysis, which needs many
samples of ϑ to provide an accurate estimate (the accuracy being proportional to the inverse of the
square root of the number of samples as already discussed – or more precisely, the inverse of the square
root of the number of accepted samples upon having entered the asymptotic regime).
4. the design of an efficient MCMC algorithm (effective proposal strategies with low rejection rate, quick
to enter the asymptotic regime) might be non-trivial.

8

5.3. Gaussian approximation of the posterior: Maximum Likelihood Estimate (MLE) and Fisher approximation
Instead of using an MCMC approach, the strategy we employ here is to approximate ρpost with a multivariate Gaussian distribution with mean µG and covariance matrix ΣG ; this is also called Fisher approximation. This approximation, provided that the available data are sufficient to determine the parameters,
is in general more and more accurate as more data become available, i.e., as Nmeas → ∞, and it has the
advantage that upon doing so, it is much easier to perform the UQ analysis, because obtaining samples from
Gaussian random variables is a standard task. It has, however, some disadvantages that will be made clearer
in the later sections, when discussing identifiability of the system: in a nutshell, we can already reveal that
the problem is that the Fisher approximation assumes identifiability of the system, but this is not always true
in practice and whether the system is identifiable or not should be checked beforehand. MCMC instead does
not assume identifiability, and can in principle be used even when the system is not identifiable: dealing with
a non-identifiable system is, however, intrinsically difficult and care needs to be taken also when tackling it
using MCMC methods, as will be made clearer later on.
The Gaussian approximation is centered at the point of maximum of the posterior pdf (maximum aposteriori estimate, MAP). Since we have further made the assumption that the prior pdf for ϑ is uniform
(see Example 4 for the extension to the case of non-uniform prior), this is equivalent to computing the value
ϑ where the likelihood function is maximized; this point is generally known as the Maximum Likelihood
Estimate (MLE) for ϑ:
µG = ϑM LE = arg max LD (ϑ).
ϑ

In practice, it is numerically more convenient to work with the logarithm of the likelihood, and to recast the
problem as a minimization problem, i.e., to compute µG as
N LL(ϑ) = −2 log(LD (ϑ)).

ϑM LE = arg min N LL(ϑ),

(10)

ϑ

The function N LL(ϑ) is called negative log-likelihood, and in the particular case where the noise affecting
the data is assumed to be Gaussian random variables (such as in our case), this problem is equivalent the
least-squares estimate of ϑ. Indeed, it is straightforward to combine (8) and (10) to obtain4
"N
2 NX
2 #
meas 
meas 
X
1
1
1
I(ϑ, ti ) − Iˆi +
R(ϑ, ti ) − R̂i
.
(11)
ϑM LE = arg min 2
σ
K
K
ϑ
i=1
i=1
Observe that this formulation does not require prior information on the value of the noise variance σ 2 : if σ 2
is unknown, it can be recovered as the sample variance of the misfits at ϑM LE
"N
2 NX
2 #
meas 
meas 
X
1
1
1
2
I(ϑM LE , ti ) − Iˆi +
I(ϑM LE , ti ) − Iˆi
σ 2 ≈ σM
.
(12)
LE =
2Nmeas i=1
K
K
i=1
The covariance matrix ΣG of the Gaussian approximation can be chosen as the inverse of the Hessian of the
NLL at the MLE of the parameters ϑM LE , see e.g. [39]:
ΣG = H −1 ,

Hi,j =

∂2
N LL(ϑ)
∂ϑi ∂ϑj

.

(13)

ϑ=ϑM LE

The matrix H is also called the Fisher Information Matrix. The diagonal entries of ΣG are the variances of
the posterior pdfs of the Gaussian approximations of the parameters, i.e.,
ϑi ∼ N ([ϑM LE ]i , [ΣG ]i,i ).

(14)

This formula quantifies the intuitive fact that the precision of the MLE is related to how narrow the minimum
of the NLL at ϑM LE is. A deep, narrow minimum means that moving even slightly from ϑM LE will change
4 the full NLL includes additional terms in log(σ) and log(2π) that we can however drop in the minimization process, since
they do not depend on ϑ.

9

consistently the value of NLL; therefore, we have a significant evidence that the estimate is precise. Conversely,
a shallow minimum means that the MLE is not very reliable. At ϑM LE the Hessian is positive definite, with
large eigenvalues if the minimum is narrow; therefore, its inverse has small eigenvalues, and in general small
diagonal entries, that can be used as variances of the parameters (the opposite is true for a shallow minimum:
the Hessian has small eigenvalues, which means that the diagonal entries of its inverse will be large, and
consequently the variances of the parameters will be large). As already mentioned, approximating the true
posterior with equation (14) is in general more and more valid as more data become available, provided that
the system is identifiable, as we will make clear below.5 Given the expression of the likelihood in equation
(8), we can derive an expression for H as follows:
NX
meas





1 ∂
∂
1
∂2
1
ˆm
I
(ϑ
)
I
(ϑ
)
+
I
(ϑ
)
+
I
(ϑ
)
−
I
m
M
LE
m
M
LE
m
M
LE
m
M
LE
Kσ 2 K ∂yi
∂yj
K
∂ϑi ,ϑj
m=1




NX
meas
1
1 ∂
∂
1
∂2
Rm (ϑM LE )
Rm (ϑM LE ) +
Rm (ϑM LE ) − R̂m
Rm (ϑM LE ) ,
Kσ 2 K ∂yi
∂yj
K
∂ϑi ,ϑj
m=1

Hi,j =

(15)

with Im (ϑM LE ) := I(ϑM LE , tm ) and Rm (ϑM LE ) := R(ϑM LE , tm ). Usually, the terms involving the second
derivatives of I and R are dropped because they are smaller than the other terms: this is because either the
misfits at ϑ = ϑM LE are small or because of near-linearity of the models Im (ϑ), Rm (ϑ) close to the solution,
i.e., ∂ϑi ,ϑj Im (ϑM LE ) and ∂ϑi ,ϑj Rm (ϑM LE ) are small [41, Chap. 10]. Collecting all the derivatives of the
model predictions with respect to the parameters in the Jacobian matrix JIR , we can write in compact form6
Hi,j ≈

1
K 2 σ2

JIR (ϑM LE )T JIR (ϑM LE ),

(16)

∂
Im (ϑM LE ), m = 1, 2, . . . , Nmeas ,
∂ϑi
∂
Rm (ϑM LE ), m = 1, 2, . . . , Nmeas .
[JIR ]m+Nmeas ,i =
∂ϑi

[JIR ]m,i =

Finally, we make an important remark: minimizing the NLL to compute ϑM LE requires repeatedly
evaluating the SIR-like model for the various parameters ϑ proposed by the optimizer. The minimization
procedure should be repeated several times with different starting guesses, to avoid local mimima.
Example 3 (Inverse and posterior-based forward UQ of a SIR model). In this example we show the results
of the inversion procedure using artificial/synthetic data, by fixing the values of the parameters to ϑtrue =
[0.29, 0.09], adding numerical Gaussian noise with σ = 0.025, discount factor to K = 3, considering data
collected at t = 1, 2, . . . , 30, and verifying the results of the inversion procedure. We then perform the forward
UQ based on the posterior pdf.
Regarding the results of the inversion procedure, we expect to see that ρpost is centered close to the true
value of the parameters with a reasonably small variance, i.e. ϑM LE ≈ ϑtrue and [ΣG ]i,i such that the support
2
of ρpost (ϑi ) is smaller than the support of ρprior (ϑi ). We also expect σM
LE to be a reasonable approximation
2
of the true σ . Regarding the subsequent forward UQ, we expect to see that the uncertainty in the prediction
is smaller than what would be obtained by using the prior information only, and the expected values of the
quantities of interest is closer to the true values when using the posterior pdf than when using the prior. The
resulting estimates for the parameters obtained from the inverse UQ are
β = 0.2848,

r = 0.0861,

σ = 0.02791.

The estimated covariances computed using the full Hessian (that we can compute directly by centered finite
differences in this simple test) and with Equation (16) (where the Jacobian entries are also computed by
5 The statistical interpretation of this fact is that the MLE is asymptotically Gaussian distributed with covariance matrix
equal to the inverse of the Fisher Information Matrix. This means that it is an efficient estimator, because it reaches the
Cramer-Rao lower bound on the variance of estimators [40].
6 Sometimes the term Fisher Information Matrix is used to indicate this approximation rather than the full Hessian.

10

1

0.5

50

0.8

0.4

S true sol
I true sol
R true sol
I data
R data
S MLE curve
I MLE curve
R MLE curve

0.6

0.4

0.2

I true
R true
I MLE curve
R MLE curve
I data
R data

0.3

0

40
30

0

20

40

60

80

100

20

0.1

10

20

0
0

10

20

r, prior pdf
r, posterior pdf
r true
r MLE

60
40

0.2

0

-0.2

80
, prior pdf
, posterior pdf
true
MLE

0.25

30

0.3

0
0.06 0.08 0.1 0.12 0.14 0.16 0.18

0.35

57

10
14

0.16

MLE for [ ,r]
True values

4000

10

3000

3000

2000

2000

1000

1000

8

0.14

r

4000

12

6

0.12

4
2

0.1

0
0.2

0
0.2

0.08
0.06
0.25

0.3

0.35

0.1

0.35

0
0.2

0.15

0.15

0.15
0.35

0.1

0.05

0.35

0.1

0.3

0.3

r

r
0.25

0.05

0.3

r
0.25

0.05

0.25

Figure 4: Result of the inverse UQ analysis for SIR. Top row, from left to right: trajectories corresponding to ϑ = ϑtrue and
ϑ = ϑM LE , and synthetic data obtained dividing the trajectories for ϑ = ϑtrue by the under-reporting factor K and adding the
Gaussian noise; zoom on the data, and trajectories for ϑ = ϑtrue and ϑ = ϑM LE rescaled by K; prior and posterior pdfs for β
and r, as well as the true and MLE values of the parameters. Bottom row, from left to right: isolines of N LL; surface-plot of
the full likelihood; surface-plot of the likelihood after the Fisher approximation, cf. equation (15); surface-plot of the likelihood
after having further dropped the second derivatives of I, R in the definition of H, cf. equation (16).

centered finite differences) are respectively


0.7995 0.1064
ΣG,Jac = 10−4 ×
,
0.1064 0.2609

ΣG,Hessian = 10−4 ×



0.8073
0.1206

0.1206
0.2616


.

Figure 4 provides more details on the results. The top row shows on the left the true trajectories from
which the data were generated (dotted thick lines), the noisy data (circles with thin line) and the trajectory
obtained by fixing the parameters as ϑ = ϑM LE (solid line). The next panel provides a zoom on the data.
We have also rescaled both the true trajectory and the MLE trajectory by K, to emphasize the match with the
data. The match between true and MLE trajectories is very good, although not perfect (the distance between
the trajectories would further reduce for smaller standard deviations σ of the noise). The last two panels of the
row compare the prior and (Gaussian approximation of ) the posterior pdfs of the parameters. It can be seen
that the posterior are centered close to the true value, and the Gaussian pdf is quite concentrated in comparison
to the prior interval. The bottom row provides details about the minimization procedure. More specifically,
the leftmost panel shows the contour of the NLL function, the true values of the parameters (yellow dot) and
the MLE (red dot). The presence of noise prevents a perfect match between the true values and the MLE,
but the match is nonetheless good and the isolines are nicely rounded, which suggest a unique, narrow (hence
trustworthy) minimum. The next panel shows the corresponding likelihood function, taken by exponentiating
the NLL (remember that since we have assumed uniform prior, the posterior is proportional to the likelihood),
which shows a clear Gaussian profile. The next panel shows the Gaussian approximation where the covariance
approximation has been computed by inverting the true Hessian of the NLL, while the approximation obtained
by using the Jacobian matrix only is shown in the right-most panel, cf. equation (15) and (16). The three
latter plots match well (other than by the rescaling factor), indicating that the approximation steps are not
introducing significant errors. The MLE has been computed with the fminsearch algorithm in Matlab, which
implements the derivative-free Nelder–Mead (simplex) algorithm.
Finally, upon calibrating the pdfs of the parameters to the data, the forward UQ analysis can be performed,
using the sampling methods discussed in Section 3 to compute the mean of the quantities of interest (S,I,R
compartments, location and intensity of the peak), and their pdfs. Results are shown in Figure 5, where we
compare the results obtained with prior and posterior pdfs, to appreciate the improvement in the quality of
the predictions if data are provided. The top row compares the true and the expected SIR trajectories after
the forward UQ analysis based on prior and posterior pdfs. The thick solid lines are the expected values (we
11

pdf of S,I,R at time t=30, prior

8

pdf of peak time, prior

0.25
S
I
R

6

0.2

pdf of peak value, prior

4

peak time for I, pdf
true peak time

peak value for I, pdf
true peak value

3

0.15
4

2
0.1

2
0

1

0.05
0

0.2

0.4

0.6

0.8

0

1

pdf of S,I,R at time t=30, posterior

25

S
I
R

20

0

20

40

60

80

0

100

pdf of peak time, posterior

0.8

peak time for I, pdf
true peak time

0.6

0

0.2

0.4

0.6

0.8

1

pdf of peak value, posterior

20

peak value for I, pdf
true peak value

15

15
0.4

10

0.2

5

10
5
0

0

0.2

0.4

0.6

0

0.8

0

20

40

60

80

0

100

0

0.2

0.4

0.6

0.8

1

Figure 5: Forward UQ analysis for SIR based on either the prior or the posterior pdfs of the parameters.

compute these with sparse grids sampling), the thick dotted lines are the true trajectories and the colored
lines are Monte Carlo trajectories based on the prior/posterior distribution. The results clearly show that
the posterior forward UQ is more centered around the true trajectories, and the uncertainty in the prediction
is smaller. Similar conclusions can be obtained by looking at the pdfs of the quantities of interest computed
based on either the prior or the posterior pdfs, where we have marked with vertical dotted lines the true values
(mid-row: prior-based, bottom row: posterior based).
Example 4 (Incorporating prior information on parameters in the inverse UQ). Suppose that we provide
prior information about the parameters ϑtrue different from the uniform distribution. For instance, we could
for simplicity assume that each ϑi is a Gaussian random variable with mean ϑ̄i and standard deviation si ,
and these variables are all independent (in this way we are allowing ϑi to assume negative values with a
non-zero probability; we will fix this issue later in the example). Then, the posterior distribution (9) becomes
ρpost (ϑ|D) ∝ L(ϑ)ρprior (ϑ) =

NY
meas

√

1

−1

ˆ

1

2

e 2σ2 ( K I(ϑ,ti )−Ii )

NY
meas

√

1

−1

1

2

e 2σ2 ( K R(ϑ,ti )−R̂i )

Nϑ
Y

1

−1
2

(ϑi −ϑ̄i )2

e 2si
2
2
2
2πs
2πσ
2πσ
i
i=1
i=1
i=1
(17)
and the Fisher approximation should now be centered at the maximum of the posterior pdf (we referred to
this value as MAP)
µM AP = arg min [−2 log(LD (ϑ)ρprior )] ,
p

ϑ

which in practice recovers a form of Tikhonov regularization of the least-squares problem, with penalization
2
parameters σs2 :
i

#
2
2 X
Nϑ
Nmeas 
Nmeas 
1 X
1
1 X
1
1
2
ˆ
= arg min 2
I(ϑ, ti ) − Ii + 2
R(ϑ, ti ) − R̂i +
(ϑi − ϑ̄i ) .
σ i=1
K
σ i=1
K
s2
ϑ
i=1 i
"

µM AP

2
The computation of the sample variance estimator σM
LE and of the Hessian H should be of course updated
accordingly. Coming back to the non-positivity issue of the Gaussian prior pdf, a possible workaround is e.g.
to assume a log-normal prior for the parameters and then work with the log of the parameters in the model.
The expression derived in equation (17) for the posterior pdf would still be valid, the change being “hidden”
in the mappings ϑ → R(ϑ, t), ϑ → I(ϑ, t).

12

Example 5 (Inverse UQ when different data types have different noise levels). Let us consider the data model
(5) and assume that the noises I,i and R,i are independent Gaussian random variables distributed according
2
to N (0, σI2 ) and N (0, σR
), respectively. The posterior distribution of the parameters can be computed with a
slight modification of the procedure explained above to account for the different variances. We detail the new
procedure here below, following closely [39, 42].
σ2
We begin by assuming that the ratio λ between the variances of the two sets of data λ := σ2I is known. In
R
this case minimizing the NLL (10) is equivalent to minimizing the following quantity
T (ϑ) =

NX
meas
i=1



1
I(ϑ, ti ) − Iˆi
K

2
+λ

NX
meas



i=1

1
R(ϑ, ti ) − R̂i
K

2
,

which is obtained combining (8), opportunely modified to incorporate the different standard deviations, and
(10). The parameter λ weighs the sum of squared residuals of I and R, highlighting a different level of trust in
the first or second set of data. If λ is small, the residuals of I condition more the quantity T , whereas the data
for R are more influential if λ is large. Finally, if λ = 1 the data sets are equally weighted and we recover
the least-squares formula (11). Hence, minimizing the quantity T can be seen as a weighted least-squares
criterion.
As λ is however unknown in general, we vary iteratively λ within an appropriate range of values, and
minimize T for each value of λ to find the corresponding ϑM LE . Among the considered λ, we then select the
one that realizes the minimum of N LL. The corresponding value of T is also needed, and we denote it by
Tmin . Indeed, the values of the empirical variances of the two data sets are then recovered in the following
way
q
p
σI = Tmin /2Ndata , σR = σI2 /λ.
Note that value of λ can be selected also by means of criteria other than the minimum of the NLL. In
particular, in [42] the Kayshap Information Criteria has been employed, that penalizes values of λ that result
in shallow NLL.
Finally, we illustrate this minimization procedure with the help of an example. We consider the SIR
model, and generate 41 equispaced synthetic data in the time interval [0, 20] with ϑtrue = [0.29, 0.09], adding
numerical Gaussian noises with σI = 0.2 and σR = 0.05, where λtrue = 16. We consider integer values of
λ in the range [1, 90], and for each of them we minimize T . In Figure 6 on the left we plot the minimum
value of the NLL for each considered λ. We observe that the value of λ resulting in the minimum NLL can
be approximately correctly identified (λmin ≈ 21), resulting in the following estimates of the parameters
ϑM LE = [0.2917, 0.0950],

σI = 0.2330

σR = 0.0508.

The figure in the middle displays the posterior distribution of the parameters corresponding to λmin and to
the two extremes of the considered range of λ. The figures on the right show 200 Monte Carlo trajectories
based on the posterior distribution of the parameters. The value λ = λmin (left-most panel) gives a narrow
bundle of trajectories matching the true ones. The other bundles (λ = 1, 90 i.e., the smallest and largest
values of λ tested) are more spread and in the case of λ = 90 they are also far from being centered around
the true trajectories.
Bibliography and further reading.
• MCMC methods for deriving the posterior pdfs of parameters of SIR-like models for COVID-19 have
been used in e.g. [18, 17].
• Several variations of Gaussian approximation of the posterior pdfs can be conceived, depending on the
choice of the mean (e.g., MLE, MAP, expected value of the posterior) and of the covariance matrix, see
e.g. [43, Result 8, p.224].
• For further approaches to the Bayesian inversion problem, see Approximate Bayesian Computation
(ABC, see e.g. [44]) and the Integrated Nested Laplace Approximation (INLA, [45]). ABC is used in
cases when it is difficult to evaluate (or even define) the likelihood function. INLA is instead useful
when certain conditional posteriors can be reasonably approximated by Gaussian distributions.
13

-25
-30

pdf

60

pdf r

35

min
=1
= 90

true

30

min

50

-35
25

-40
40

-45
20

-50

30
15

-55
20

-60

10

-65
10

5

-70
1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86
0

0
0

0.2

0.4

0.6

0

0.05

0.1

0.15

0.2

Figure 6: Left: values of N LL across the range of tested values of λ for the test in which the data for I and R have different
standard deviations. The smallest NLL is reached for λmin close to the exact one λtrue ; middle: posterior distribution of β and
r for λmin , λ = 1, and λ = 90; right: Monte Carlo trajectories based on the posterior distribution of the parameters, for λmin ,
λ = 1, and λ = 90, respectively. The black lines are the true trajectories.

• In this section we have assumed a simple error model with additive Gaussian errors. See e.g. [6, 5] for
more general error models. Furthermore, one can model under-reporting and additional features of the
measurements, such as the fact that these kind of data are intrinsically integer numbers, by choosing
more suitable likelihood functions than in equation (8), such as negative binomial or quasi-Poisson
likelihooods, see e.g. [46, 47, 19].
• In this section we have assumed that ρprior are either uniform or Gaussian random variables; other
choices with good properties are possible, e.g. the Jeffreys non-informative priors [48].
• Using sparse grids for UQ with exact posterior pdf in equation (9) is not straightforward, because the
functional shape of ρpost does not fall into classical families of pdfs (e.g. uniform, Gaussian, gamma)
for which these methods are developed. A possible remedy would be to compute ad-hoc polynomials, in
the spirit of [49, 50]. Another possibility is to keep sampling according to the prior pdf, see e.g. [51],
but this is possibly suboptimal if the prior and the posterior are significantly different (e.g., a uniform
prior and a very peaked posterior).
• Conversely, sparse grids sampling for Gaussian random variables has been discussed multiple times in
literature, see e.g. [52]; therefore, sparse grids computation are easy to use upon having performed the
Fisher approximation.
• Replacing the evaluation of the full-model with a response surface either in the MCMC sampling of
posterior pdf or in the minimization of NLL is possible (this operation is routine in computationallyheavy inverse problems, see e.g. [53]), but in this context this operation does not dramatically speed-up
the computational time because SIR-like models are rather cheap to evaluate.
• We could extend the error model for I , R to account for the various sources of error (model error,
response surface error if used, numerical discretization errors), see e.g. [54].
• Minimizing the NLL function is an example of non-linear least-squares optimization problems, for which
ad-hoc algorithms exist, like Gauss–Newton, Levenberg-Marquardt, VarPro, Trust-region reflective, see
e.g. [41, 55].
• Inverse UQ / data fitting methodologies can also be applied to stochastic models, where the dynamics of
the system are influenced by a “process noise”, and measurements are further corrupted by “observation
noise” (i.e., the errors I,i , R,i in equation (5)). The methodology for data fitting described above of
course needs to be adjusted accordingly, see e.g. [56, 57, 58].
6. Summary: an ideal UQ workflow, from data to prediction
Summarizing the discussion so far, the ideal UQ workflow for predictions under uncertainty would consist
of the steps reported in Algorithm 1. This ideal workflow however is missing one step, i.e., the identifiability
14

analysis, which is a crucial preliminary analysis to perform. We discuss it in details in the next sections.
The adjusted ideal UQ workflow will then be presented in the final Section 9 (Discussion and conclusion),
see Algorithm 2.
Algorithm 1: Ideal UQ workflow
1
2

3
4

Choose a model and the prior distributions for its parameter (literature, expert opinion);
Compute Sobol indices to assess which parameters are more influential and can be inferred from data.
Fix the remaining parameters to some reasonable value;
Perform the inverse UQ analysis to adjust the prior distribution to the data evidence;
Perform the forward UQ analysis based on the posterior distribution to obtain statistical information
about the quantities of interest of the model (e.g. expected value, variance, full pdf of the outputs);

7. Structural identifiability
The fundamental assumption underlying the inversion approach proposed in Section 5 is that there exists
a certain set of parameters and hyper-parameters [ϑtrue , htrue ] that generated the observed data from the
system (2), as expressed in Equation (5). We then embrace the fact that the data are noisy, and that
this noise might prevent us from correctly determining the values [ϑtrue , htrue ]; we therefore give up on
giving a “one-shot” estimate of [ϑtrue , htrue ], and rather content ourselves with computing a posterior pdf,
which quantifies our degree of belief on each possible value of [ϑtrue , htrue ]. The Fisher approximation then
further assumes that the NLL has a unique, well-shaped minimum, which means that the posterior pdf of the
parameters is sufficiently well-approximated by a Gaussian pdf centered at [ϑM LE , hM LE ], whose variance
gets smaller as we acquire more data. If instead we believe that there exists a certain set [ϑtrue , htrue ] but
for some reason we think that our data do not support the assumption that the posterior is Gaussian (for
instance, because we have only limited data), we could consider the “full” posterior given by equation (9)
instead (using e.g. MCMC as computational tool), and ideally three scenarios might then occur:
1. the posterior pdf is actually close to Gaussian;
2. the posterior pdf is unimodal but it departs from Gaussian in that it might show “heavy tails” and/or
some degree of skewness. This would indicate that we might be introducing a bias that leads to
over/underestimates;
3. the posterior pdf is multi-modal. This would mean that the inversion procedure is suggesting a few
“likely” combinations of parameters ϑ, each corresponding to one peak of the posterior pdf: in this
case the heights of the peaks represent our belief on the plausibility that such ϑ is the “true one”.
In any case, the crucial point that one has to address is: can we guarantee that there is a unique set
[ϑtrue , htrue ] that generates the observed outputs? Or, equivalently, is the inverse problem well-posed? If
not, the Fisher approximation is bound to fail (for instance, item 3 in the list above) and the MCMC
approach also needs to be handled with care. In the field of mathematical epidemiology (and more generally
of dynamical systems/systems control), this question falls into the study of the so-called system identifiability,
which can be divided in two consecutive steps:
Structural identifiability: studying from a theoretical point of view the well-posedness of the identifiability
(inverse) problem, assuming that perfect information is available, i.e., that infinitely many noise-free
observations of the outputs are available. It is an intrinsic property of the system and is the topic of
this section. This topic is well-studied in the epidemiological literature: a list of references is available
at the end of the section.
Practical identifiability: addressing the identifiability of the system given limited and noisy observations
of the outputs. It depends not only on the properties of the system but also on the quality of the data.
In other words, the structural identifiability is a necessary but not sufficient condition for the practical
identifiability of the system. The practical identifiability is the topic of the next Section 8.
15

Mathematically, a system is structurally identifiable if the model map, i.e. the function [ϑ, h] 7→ Y (ϑ, h)
mapping each realization of ϑ, h to the corresponding values of the outputs / quantities of interest is injective.
Of course, numerical estimates of parameters obtained by UQ techniques for structurally non-identifiable
systems are not reliable and might lead to very wrong predictions. We will show some results on this in
Example 8. In the following we discuss two approaches to structural identifiability, one based on differential
algebra and a mapping approach. Other methods are available, see the bibliography at the end of the section.
7.1. Differential algebra
The differential algebra approach to the structural identifiability problem is based on deriving a set of
differential equations for the model outputs Y of the form P(Y, Ẏ , Ÿ , . . . , ϑ, h) = 0, where P is a monic
differential polynomial including only Y , their derivatives and the model parameters/hyperparameters; see
e.g. Example 6. These equations are known as input-output equations and are an implicit form of the model
map, as they generate the same output as the original model. The coefficients of the input-output equations
give indication on the identifiability of the system: if the map from [ϑ, h] to the coefficients of the inputoutput equation is injective, the system is structurally identifiable; conversely, if there are multiple values of
[ϑ, h] that generate the same input-output equations, the system is structurally non-identifiable.
One approach to obtain such equations is by ad-hoc substitution and differentiation to eliminate the
unwanted quantities, starting from the original system (2) – of course, one must be careful not to remove/introduce additional solutions e.g. by canceling/multiplying every term by Y . For systems with
many compartments, the manual ad-hoc substitution method might be impractical, and a more algorithmic
approach is needed: one possibility consists in generating the input-output equations as part of the so-called
characteristic set of the algebraic ideal generated by the polynomials defining the model, see [59]. Finally,
note that the calculations required to derive the input-output equations can be done also using symbolic
calculus software, e.g. Mathematica and Maple.
We refer to [60] for the theoretical background underlying this procedure: in particular, in that work
it is shown that the identifiability result does not depend on the particular method employed to derive the
input-output equations, as long as a certain property, called mutual reduction, holds true – see Example 6.
This property is always true in the case of a single output quantity, whereas it needs to be enforced in the case
of multiple output quantities. The fact that the identifiability result does not depend on the specific form of
the input-output equation stems from the fact that by definition all the input-output equations generate the
same output trajectory as the original model. From this, it follows that all forms of such equations contain
the same identifiability information of the original system.
Another (quite technical) preliminary condition for the differential algebra approach to be valid is the socalled solvability condition [61], which can be safely be assumed to hold for a wide class of practical problems,
and we do this in this work as well. We refer again to [60] for a discussion on why this condition can be
“safely assumed”, and to [62] for an ad-hoc example where this condition is not valid and the differential
algebra approach fails.
Example 6 (Structural identifiability of a SIR model by differential algebra). In this example, we focus on
1
I. Our argument here is similar to those in [6, 9]; as
the case of the SIR model (1) with output Y = K
already mentioned, having prevalence data is somehow unrealistic, yet it is a scenario already rich enough for
our “didactical” purposes. We consider the following system

β


Ṡ = −
IS


Npop


β
IS − rI
I˙ =

Npop




Y = 1 I.
K
Note that we have neglected the equation for R in (1) as it does not influence the dynamics of the system.
Combining the differential equation for I and Y yields
Ẏ =

β
Y S − rY,
Npop

16

which can be solved for S; from the latter, an expression for Ṡ can then be derived. Then, replacing these
expressions for S and Ṡ in the differential equation for S of the SIR model, we obtain the following equation
Npop Ÿ Y − Npop Ẏ 2 + Kβ Ẏ Y 2 + KrβY 3 = 0.
We then divide all the terms by Npop to obtain the following monic polynomial, i.e. a polynomial with the
coefficient of the highest order term equal to 1:
Ÿ Y − Ẏ 2 + K

β
β
Ẏ Y 2 + Kr
Y 3 = 0.
Npop
Npop

β
β
, Kr Npop
] is injective, i.e., the system
Assuming Npop and K known, the map [β, r] 7→ [K Npop


β


C1 = K N ,
pop

β

C2 = Kr
,
Npop
can be solved for β, r. This means that it is possible to uniquely identify β and r, and the model is structurally
identifiable. Conversely, if K is unknown, only r and the combination Kβ can be uniquely estimated, i.e.,
the model is structurally non-identifiable.
1
If instead we consider the case of having also a second quantity of interest Z = K
R, all the parameters
of the SIR model result to be structurally identifiable, as we show in the following. Since we have data of I
and R, we drop the differential equation for the compartment S of the SIR model and consider the following
equivalent system

I˙ = β I(Npop − I − R) − rI



Npop




Ṙ
=
rI

1


Y = I


K




Z = 1 R.
K
The practice of eliminating a compartment is possible as we assume that Npop = S +I +R for all times, which
means that the dynamics of the compartment that we eliminate is completely determined by the remaining
ones. In particular, if we let S = Npop − I − R, we can immediately see that
β
β
Ṡ = −I˙ − Ṙ = −
I(Npop − I − R) + rI − rI = −
IS,
Npop
Npop
i.e., we recover the initial equation for S. In this second example we have two outputs, therefore we have to
derive two input-output equations. We rewrite the differential equations for I and R in terms of Y and Z
and obtain
β
K Ẏ −
(Npop − KY − KZ)KY + rKY = 0
and
K Ż − KrY = 0.
Npop
To make the polynomials monic we have to introduce a ranking among the variables.7 A common choice for
the ranking is Y < Z < Ẏ < Ż (any other ranking would lead to the same results as already mentioned, of
course after different computations), from which it follows that the leading monomials of the input-output
equations are Ẏ and Ż, respectively. We then divide all the terms of the equations above by the coefficient of
the corresponding leading term and obtain the following monic input-output equations
Ẏ − βY +

7 i.e.,

βK 2
βK
Y +
Y Z + rY = 0
Npop
Npop

and

Ż − rY = 0.

a total ordering of the variables and of their derivatives. For a formal definition, see [63].

17

Hp

x2

y2

X(t,p)

λ

Hp
Y(t,p)=Y(t,p)
(Hp)-1

X(t,p)

x1

y1

Figure 7: The mapping approach for structural identifiability.

Since we have more than one output, before concluding on the identifiability of the system we have to check
that the two equations are mutually reduced; if not, the analysis could lead to spurious results, as pointed
out in [60]. The concept of reduction is again based on the chosen ranking of the variables: a polynomial Pi
is reduced with respect to the polynomial Pj if it does not contain neither the leading monomial of Pj with
equal or greater degree nor its derivatives. In our case it can be easily seen that the input-output equations
are mutually reduced, and by looking at their coefficients we conclude that β, r and K can be simultaneously
identified. Hence, the SIR model is structurally identifiable from prevalence data of I and R.
We close this example mentioning that [6] discusses structural identifiability of SIR (with no underreporting factor) in the more realistic scenario where cumulative incidence data rather than prevalence are
available. The result obtained is that SIR is structurally identifiable even from such kind of data (but other
problems that we will discuss in Example 8 occur, so that one should restrain from attempting parameter
identification of SIR from cumulative incidence data).
7.2. Mapping approach
This approach is discussed in [64, 65]; we refer the reader interested to the theoretical background to these
two references and only sketch the main idea and the “practical recipe” here. Example 7 gives an example of
the application of this method to the SIR model. For ease of notation, in this discussion the vector p ∈ Rp
collects all the uncertain elements of (2), i.e., not just the coefficients, but also the initial conditions and the
hyper-parameters, i.e. p = Ncoef +Nstates +Nhyp . Moreover, we change the notation in (2) to a more compact
form and write f p (X) instead of f (X, p) and similarly for G, and let Nstates = n, Nqoi = m. Summarizing
the new notation, (2) becomes

Ẋ(t, p) = f p (X(t, p))

(18)
X(0, p) = X0 (p)


p
Y (t, p) = G (X(t, p)),
with X ∈ Rn , Y ∈ Rm , f p : Rn → Rn , Gp : Rn → Rm , ∀t ∈ [0, tmax ], and X0 : Rp → Rn .
We explain the method with the support of Figure 7. If the system (18) is not identifiable, then there are
two different sets of parameters, say p and p̄, such that the trajectories X(t, p) and X(t, p̄) are different but
the corresponding outputs are identical, Y (t, p) = Y (t, p̄) ∀t ∈ [0, tmax ]. If these conditions hold true, then
it is possible to rework the equality Y (t, p) = Y (t, p̄) to explicitly construct a map λ : Rn → Rn that maps
the trajectories X(t, p) and X(t, p̄) to one another: λ(X(p̄)) = X(p) (we will come back to this point with
more details later on).
Since the map λ has been derived by enforcing equality between the outputs Y (t, p) and Y (t, p̄), without
taking into account the dynamics of the system, one has to further check that λ(X(p̄)) still solves the

18

dynamical system. Taking time derivatives of both sides of λ(X(p̄)) = X(p) and using (18) one gets (removing
the dependence on t for ease of notation):
λ(X(p̄)) = X(p)
∇λ

X(p̄)
p̄

Ẋ(p̄) = Ẋ(p)

∇λ

X(p̄)

f (X(p̄)) = f p (X(p))

∇λ

X(p̄)

f p̄ (X(p̄)) = f p (λ(X(p̄))).

(19)

Therefore, one has to check that the last equation, i.e. equation (19) is valid for the proposed λ. This will
result in a set of conditions for the components of p and p̄: if the resulting conditions are p = p̄ the system
is identifiable; otherwise, there will be non-trivial conditions between some of the components of p and some
of the components of p̄, (see e.g. Example 7), which means that the system is not identifiable.
We now come back to the issue of constructing the map λ. As already mentioned, the idea is to construct
λ from the condition Y (p̄) = Y (p), i.e. from
Gp̄ (X(p̄)) = Gp (X(p)) = Gp (λ(X(p̄)).
In principle, it would be enough to solve for λ in the latter, i.e. λ(X) = (Gp )−1 [Gp̄ (X)], but the system
might be underdetermined if we have m < n observables. Then, the idea is to complement the observables
with additional equations and to create an “augmented” observables vector H p : Rn → Rn , as follows, by
taking into account the directional derivatives of the outputs Yj along the direction f p (X):
H1p = Y1 = Gp1 (X(p))
H2p = Y2 = Gp2 (X(p))
...
p
Hm
= Ym = Gpm (X(p))
p
Hm+1
= ∇X Y1 · f p (X)
p
Hm+2
= ∇X Y2 · f p (X)

...
Hnp = ∇X Yn−m · f p (X).
The next step is to verify that H p is a bijective map (this condition is named Observability Rank Condition,
ORC), i.e. that the Jacobian of H p with respect to X is non-singular in the domain of definition of the
trajectories X for every admissible p. Finally, the map λ can be computed by solving for λ the system of
equations:
H p̄ (X(p̄)), p̄) = H p (λ(X(p̄)), p),
leading to (cf. Figure 7)
λ(X) = (H p )−1 H p̄ (X),
where the well-posedness of (H p )−1 is guaranteed by the ORC. We close this discussion with a couple of
remarks on the construction of H p :
• In case the number of observables m < n/2, adding the directional derivatives ∇X Yj ·f p (X), j = 1, . . . m
will not be enough to reach n observables. In this case, one should also add derivatives of higher order.
• In the opposite case, in which 2m > n, one can choose among multiple directional derivatives ∇X Yj ·
f p (X): after having dropped the choices that make the mapping H p singular in the domain of definition
of the trajectories X, one should check all remaining combinations (the fact that one choice results in
identifiability in this case does not rule out the possibility that another choice might result in nonidentifiability). If m > n, then there are more observables than states, and the same principle applies.

19

Example 7 (Structural identifiability of a SIR model by mapping approach). Let us consider the SIR model
1
with output y = K
I as in Example 6, and rewrite it replacing S, I as X = [x1 , x2 ]:

β




x1 x2
x˙1 = −

β

N

pop
x
x
−

1
2


β
Npop
.
⇒ f p (x1 , x2 ) = 
x1 x2 − rx2
x˙2 =


β

Npop

x
x
−
rx

1
2
2

Npop

y = 1 x 2 .
K
Our goal is to recover the structural identifiability results already obtained in Example 6 with the differential
algebra approach. The first step is to build the two maps H p and λ, where p = [β, r, K, Npop ]. As for
H p = (H1p (x1 , x2 ), H2p (x1 , x2 ))T , we need to augment the observable y with a directional derivative:

1
p

H1 = y = x2 ,
K
1 β
1

H2p = ∇X y · f p (X) =
x1 x2 − rx2 .
K Npop
K
The Jacobian of this mapping has non-zero determinant whenever x2 6= 0, which is a value never attained by
the trajectories X unless the initial condition is x2 (0) = 0 (in which case the trajectory is the uninteresting
case X = [0, 0]T ). Therefore, we can apply the methodology. The mapping λ = (λ1 (x1 , x2 ), λ2 (x1 , x2 ))T is
obtained by solving the equation H p (λ(X)) = H p̄ (X), i.e.




1
1
Npop
β̄




x
,
λ
=
2
K 2
λ1 =
x1 − r̄ + r ,
K̄
β
N̄pop
⇒
1 β
1 β̄
1
1




x1 x2 − r̄x2 ,
λ1 λ2 − rλ2 =

λ2 = K x2 ,
K Npop
K
K̄ N̄pop
K̄
K̄
whose Jacobian is



Npop β̄
 N̄ β
pop
∇x λ = 

0


0 
.
K 

K̄
Then, enforcing condition (19) results in the following equations:



Npop β̄ β̄
β Npop
β̄
K


x2 (p̄) ,
x1 (p̄) − r̄ + r

− N̄ β N̄ x1 (p̄)x2 (p̄) = − N
β
N̄
K̄
pop
pop
pop
pop





β̄
K
β
β Npop
K
K


x1 (p̄)x2 (p̄) − r̄x2 (p̄) =
x1 (p) − r̄ + r
x2 (p̄) − r x2 (p̄) .

Npop β
K̄ N̄pop
N̄pop
K̄
K̄
The second equation is identically verified. Conversely, the first one holds true for every x1 , x2 if
r = r̄,

K̄ β̄
Kβ
=
,
Npop
N̄pop

i.e. we obtain the same non-trivial condition previously obtained in Example 6: at most one parameter out
of K, β, Npop can be identified. If both observations of I and R are available instead, then we have two
observables

1

y1 = x2 ,
K

y = 1 (N − x − x ) ,
2
pop
1
2
K
and the analysis must be repeated. In particular, the map H p can now be constructed without using directional
derivatives, as

1
p

 H1 = x 2 ,
K

H p = 1 (N − x − x ) .
pop
1
2
2
K
20

This map is linear, therefore it is bijective for every X and we can carry on with the analysis. The mapping
λ is obtained by solving the equations H p (λ(X)) = H p̄ (X), resulting in



K
K


,
λ1 = x1 + Npop 1 −
K̄
K̄

K

λ2 = x2 ,
K̄
whose Jacobian is K
times the identity matrix. Then, enforcing condition (19) results in the following
K̄
equations:




K
β
K
K
K β̄

−
x1 (p̄)x2 (p̄) = −
x1 (p) + Npop 1 −
x2 (p̄),

N
K̄ N̄pop
K̄
K̄
pop K̄






K
β̄
β
K
K
K
K

−
x1 (p̄)x2 (p̄) − r̄x2 (p̄) =
x1 (p) + Npop 1 −
x2 (p̄) − r x2 (p),
Npop K̄
K̄ N̄pop
K̄
K̄
K̄
β
β̄
=
, i.e., the system is now structurally identifiable
Npop
N̄pop
to be known, as already discussed in Example 7.

which result in the conditions r = r̄, K = K̄,
if we assume Npop

7.3. Sensitivity analysis
As already mentioned in Section 4, if an output / quantity of interest Y is only weakly influenced by
a parameter, such parameter might be non-identifiable from Y : therefore, computing the sensitivity of a
quantity to a parameter gives an indication about the structural identifiability of the parameter. As explained
in Section 4, sensitivity analysis can be local or global. In the first case, a small gradient of the quantity of
interest value indicates that the quantity of interest is not very sensible to small variations in the parameter,
which might then be non-identifiable. The global sensitivity analysis can be performed by means of Sobol
indices: parameters with low Sobol index do not strongly influence the quantity of interest Y , and might be
non-identifiable from observations of Y .
Bibliography and further reading.
• The concept of structural identifiability was first introduced in [66].
• Some theoretical background on the differential algebra approach is provided in [63, 60].
• Many methods we have not directly mentioned here (e.g. Taylor series approach, generating series
approach, and methods based on the implicit function theorem) are explained in [63].
• See [6] for the detailed discussion of structural identifiability of a SEIR model by differential algebra.
As already mentioned, [6] also shows that the SIR model is structurally identifiable also in the case of
cumulative incidence data.
• Further discussion on the solvability condition for the differential algebra approach can be found in
[67, 68].
• Sensitivity analysis is discussed in, e.g., [5] and [63], where several methods based on the Jacobian
matrix J and on the Fisher matrix H are discussed, see equations (13) and (16).
8. Practical identifiability
Upon having assessed the structural identifiability of a system, it is still not obvious that the system can
be identified from limited, noisy data, that possibly cover only a fraction of the time-span of the dynamics
(e.g., when one measures the initial part of a trajectory and wants to assess the parameters for long-term
forecast). With reference to Figure 7, if the output trajectories Y (p), Y (p̄) are distinct but close, they might
become indistinguishable from one another if we only have at our disposal a noisy cloud of points around
them rather than the entire exact trajectories. The study of this setting is called practical identifiability
21

analysis, and is typically performed on synthetic data to see under which conditions the inversion procedure
obtains results “close enough” to the true values of the parameters. In the following, we give a short outlook
on the main tools; see the bibliography at the end of the section for further readings on each method.
Monte Carlo simulations/bootstrap: generate M sets of synthetic data and for each data set, compute
the MLE ϑ(k) , k = 1, . . . , M . Then, compute dispersion indices for the M MLE of each parameter,
such as their sample variance or their average relative error, which is defined as
ARE(ϑi ) =

M
(k)
1 X |ϑi − ϑtrue,i |
.
M
ϑtrue,i
k=1

Repeat the procedure for synthetic data with increasing levels of noise and observe the trend of the
dispersion indices as the noise increases. If the ARE of the estimates is e.g. higher than the noise level,
the parameters are not-identifiable (other criteria in the same spirit might be used as well).
The same procedure should be repeated by considering each time a different number of parameters to
be jointly estimated, until all the parameters are included. In this way, it is possible to detect the
influence of each parameter on the quality of the joint estimate.
Finally, note that the amount and the time-span of the available data might be relevant for practical
identifiability, and the analysis should ideally be repeated while varying these settings. For instance, [6]
discusses the bootstraps analysis for the SIR model computing the ARE of parameter estimates using
data sets of increasing time-span: the results indicate that the model with unknown β, r is practically
identifiable from data on the I compartment only after that the epidemic peak is reached (despite the
fact the model is structurally identifiable regardless of the time-span of the data, see Example 6).
Fisher Information Matrix: as already explained in Section 5.3, the Fisher Information Matrix is the
Hessian of the NLL at the maximum log-likelihood estimate (or its approximation in Equation (16)).
If the Fisher Information Matrix is positive definite with large eigenvalues, the NLL has a narrow
minimum and we can conclude local practical identifiability of the system, i.e. identifiability in a neighborhood of the maximum likelihood estimate; conversely, small eigenvalues are symptom of practical
non-identifiability, since the minimum of the NLL occurs in a shallow region. Numerically, the optimization procedure might even end up in a critical point where the Fisher Information Matrix is not
even positive definite, e.g. it could be non-definite or it could have rank smaller than the number of
parameters. In that case, the rank of Fisher Information Matrix gives an indication on the maximum
number of parameters that can be simultaneously inferred, see e.g. [69].
Correlation matrix C: The inverse of the Fisher Information Matrix H gives an approximation of the
covariance matrix of the parameters, ΣG , see Equation (13). From ΣG , it is possible to compute the
correlation matrix of the parameters, rescaling each entry as
Cij =

ΣG,ij
.
ΣG,ii ΣG,jj

If two parameters have correlation close to 1, they are linearly dependent, and cannot be estimated
separately. Hence, they are practically non-identifiable.
Optimization with multiple restart: As already mentioned in Section 5.3, one should repeat several
times the optimization to determine the MLE for different initial guesses and check where the minimization ends. If the algorithm ends always at the same point, there is empirical evidence that the
NLL has a unique minimum and the system is globally practically identifiable (the minimum might be
different from the nominal value of the parameter, i.e. the noise might introduce a bias in the estimate,
see e.g. Example 3). If several local minima or a manifold of minima are detected we conclude that the
system is not practically identifiable: in particular, in the case of a manifold of minima there are many
sets of parameters that fit equally well the data, denoting a possible case of structural non-identifiability.
For instance, in Example 6 when only data of I are considered, we expect that the region Kβ = const
will be a manifold of minimum points for the NLL.

22

Profile log-likelihood for each parameter: The previous discussion, pointing to the possibility that the
NLL might have a manifold of minima, allows us to introduce the last tool to assess practical identifiability, i.e., the profile log-likelihood; this tool is actually “in between”, and could also be used for
structural identifiability, as it will be made clearer later on.
Whenever ϑ is practically identifiable, the NLL should have a global minimum at ϑ = ϑtrue . Therefore,
a visual inspection of the NLL can immediately tell whether the system is identifiable or not. Of course,
this is not a viable solution for problems with more than two parameters. In this case, one can resort to
the profile likelihood, which is a mono-dimensional slice of the log-likelihood function in the direction
of the considered parameter ϑi . The profile likelihood can be obtained by changing the parameter
ϑi iteratively in a certain range of values around its MLE value ϑM LE,i , while reoptimizing all other
parameters. Thus, the profile log-likelihood is defined as
P Li (ϑ) =

min

{y:yi =ϑM LE,i }

N LL(y).

If the profile log-likelihood has a unique minimum, the parameter is practically identifiable, whereas
more complex shapes, with shallow regions and multiple minima indicate that the parameter is practically non-identifiable. In particular, a flat profile means that the NLL has a manifold of minima that fit
the data equally well and we can again conclude that the system is structurally non-identifiable. For a
schematic illustration we refer to Figure 8. Crucially, if the NLL has a manifold of minima, the Fisher
approximation of the posterior will be completely wrong, since it is based on the assumption that NLL
has a unique minimum; the case of finitely many local minima could instead be fixed by acquiring more
data, which should hopefully make the “spurious peaks” become smaller and smaller.
In summary, the analysis of the profile log-likelihood is based on checking the “flatness”/“shallowness”
of such function. To make this criterion more quantitative and discriminate between different levels of
shallowness one can provide confidence thresholds of the profile log-likelihood CIPL using a χ21 test:
CIPL (yi ) = {y|PLi (y) ≤ PL(ϑMLE,i ) + ∆α χ21 },
where ∆α χ21 is the α quantile of the χ21 distribution. If the likelihood profile of a parameter exceeds
the confidence threshold ∆α χ21 on both sides of ϑM LE,i the parameter is practically identifiable (the
threshold is indicated by the black dotted lines in Figure 8). This is an alternative approach to determine
a posterior pdf for the MLE, consisting of a uniform pdf instead of the Gaussian Fisher approximation
discussed in Section 5.3. Such uniform estimate might be more robust for limited datasets [70].
Finally, observe that if the data are generated in a noise-less way by sampling the trajectories of
ϑ = ϑtrue , and they are sufficiently many, we end up in the scenario of structural identifiability. The
profile log-likelihood can still be constructed and evaluated also in this case, as the NLL is just the sum
of square misfits (without the σ12 factor), and therefore it can be used as a computational tool to verify
structural identifiability, i.e., for the existence of a manifold of minima.
In case the system is structurally or practically non-identifiable, a possible workaround is to learn some
of the parameters from independent studies, thus reducing the number of parameters to be simultaneously
identified. Sometimes even a hierarchical optimization approach might be effective: in this approach, one
does a first round of optimization to obtain the values of the entire set of parameters but retains the values
obtained for the identifiable parameters only. Upon fixing these parameters to the values just obtained, the
optimization of the remaining parameters can be repeated. An alternative is to reparametrize the model,
replacing the original parameters with the combination that can be identified, see e.g. [11]. Finally, one
could resort to so-called marginalization techniques, see e.g. [71, 72, 73].
Example 8 (Practical non-identifiability of a SIR model with unknown under-reporting factor). In the
scenario of COVID-19, it has been often pointed out that data of infected and dead persons have been underreported (even significantly), but the exact value of the under-reporting factor K is not known. Some discussion on this aspect is provided in [13, 17]. In this example, we consider a SIR model, generate synthetic
data and investigate the practical identifiability of β, r, K when measurements of I and R are considered,
by computing the profile likelihood for K; we already know from Example 6 that in this scenario (somehow
23

0.9

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0.1

0

0

0

-0.1
-1

0

-0.1
-1

1

-0.1
0

1

0

0.5

1

Figure 8: Three cases of profile likelihood (blue line), with nominal value of the parameter (red star), and confidence threshold
∆α χ21 (dashed line ). Left: the parameter is practically identifiable; center: the parameter is not structurally identifiable; right:
the parameter is practically non-identifiable.

-124

-150

-86
-126
-86.5

-160
-128

-87

-130

-87.5
-88

-132

-88.5

-134

-170

-180

-89

-136

-190

1 2 3 4 5

1 2 3 4 5

1 2 3 4 5

K

K

K

Figure 9: Profile Likelihood for K for the three different scenarios, i.e. data collection ending: before peak of I (left), around
the peak of I (center), after the peak of I (right).

unrealistic, yet already rich enough for exposition purposes) the parameters are structurally identifiable. We
repeat practical identifiability analysis in three settings, that differ by the time-span covered by the data: until
past the peak of I (T = 20), up to the peak of I (T = 30), and before the peak of I (T = 40).
We fix ϑ = [0.28, 0.11], σ = 0.025, and the discount factor to K = 3. The results are reported in Figure
9 and suggest that for data before peak the profile likelihood of K is shallow (look at the scale on the vertical
axis) and even has a minimum at the wrong value K = 2, denoting practical non-identifiability of K. For
longer collection times instead, the profile likelihood shows an increasingly deep minimum around the correct
value K = 3. The fact that time might be important in determining whether a system is practically identifiable
was already discussed in [5, 8, 6]. Fixing K to the wrong value to perform the forward UQ analysis of course
leads to predictions that are far from the true behavior of the system, see Figure 10-top-left. An important
disclaimer to do here is that it would be hard to say that the set of parameters obtained for K = 2 fits the data
worse than those obtained for K = 3, see Figure 10-top and bottom-second figure. Even using quantitative
criteria to evaluate the goodness of fit of the two fittings, such as
r
2
PNmeas 
1
F
(ϑ
)
−
F̂
, for F = I, R
Root Mean Square Error (RMSE) : Nmeas
m
M LE
m
m=1
Mean Absolute Error (MAE) :

1
Nmeas

PNmeas
m=1

|Fm (ϑM LE ) − F̂m |, for F = I, R

24

0.14

0.14
I Data
R Data
I Pred.
R Pred.

0.1

0.14
Bisector

I Prediction

0.08
0.06
0.04

Bisector

0.12

0.12

0.1

0.1

R Prediction

0.12

0.08
0.06
0.04

2
0
-2

0.08

Bisector

-3

0.06

-2

-1

0

1

2

-2

-1

0

1

2

-2

-1

0

1

2

-2

-1

0

1

2

3

0.04

2
0.02

0.02

0

0

0

-0.02

-0.02

-0.02
0

5

10

15

20

0.02

0

days

0.05

0.1

0

Bisector

0.12

0

0.1

R Prediction

I Prediction

0.04

3

2

0.12

0.1

0.06

-3

0.14
Bisector

0.08

Bisector

0.1

R Data

0.14
I Data
R Data
I Pred.
R Pred.

0.1

0.05

I Data

0.14
0.12

0
-2

0.08
0.06
0.04

-2

0.08

Bisector

-3

0.06

3

0.04

2
0.02

0.02

0

0

0

-0.02

-0.02

-0.02

0

5

10

15

20

0.02

0

days

0.05

0.1

0
-2
0

0.05

I Data

Bisector

0.1

R Data

-3

3

Figure 10: Comparison of quality of predictions and data fitting obtained for K = 2 (top) and K = 3 (bottom) in the scenario
of data available only before the peak (T = 20). From left to right: forward UQ based on the posterior pdf; the fitting of the
data in the two cases; scatterplot of data vs predictions; qqplot of misfits.

Contour NLL

Contour NLL

0.18

Contour NLL

0.18
MLE for [ ,r]
Nominal values

0.16

0.18
MLE for [ ,r]
Nominal values

0.16

0.12

0.1

0.1

r

0.14

0.12

r

0.14

0.12

r

0.14

0.1

0.08

0.08

0.08

0.06

0.06

0.06

0.04

0.04

0.02
0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

0.02
0.1

MLE for [ ,r]
Nominal values

0.16

0.04

0.15

0.2

pdf of

0.25

0.3

0.35

0.4

0.45

0.5

0.02
0.1

0.15

100

100

50

50

50

0

0
0.2

0.3

0.4

0.5

0.2

0.3

0.4

0.5

0.35

0.1

0.2

pdf of r
100

100

50

50

50

0.15

0.2

0
0.05

0.4

0.45

0.5

0.1

0.3

0.4

0.5

pdf of r

100

0.1

0.3

0
0.1

pdf of r

0
0.05

0.25

pdf of

100

0.1

0.2

pdf of

0.15

0.2

0
0.05

0.1

0.15

0.2

Figure 11: Results of inversion for the three cases T = 20, 30, 40 for K = 3. Top row: contours of the NLL in the three cases
(left to right); the true value of the parameters is the yellow dot, the MLE is the red dot. The NLL contour lines suggest that
the NLL has a unique minimum at the MLE, which is always close to the true value of the parameters. Bottom rows: Gaussian
approximation of the posterior pdfs of β, r for the three cases T = 20, 30, 40 for K = 3.

25

Mean Absolute Percentage Error (MAPE) :

1
Nmeas

PNmeas 
m=1


Fm (ϑM LE ) − F̂m /F̂m , for F = I, R.

would actually tell that the fitting of the case K = 2 is slightly better than the case K = 3 (numbers not
reported for brevity). The scatterplots of predictions vs data are qualitatively identical, and reasonably aligned
with the bisector, see Figure 10-top and bottom third and fourth panel. The empirical distribution of the
misfits in both cases are qualitatively identical and close to a Gaussian (see the quantile-quantile plots in
Figure 10-top and bottom-right panels). In summary, all of these diagnostic tools give little-to-no evidence
that K = 2 is the wrong choice of the under-reporting parameter. We close this example with some remarks:
• the quality of the fitting of the cases K = 2, 3 cannot be assessed by the R2 coefficient, which is not
well-defined for non-linear least-squares problems [74].
• if instead K is known, the SIR system is practically identifiable regardless of time, since the NLL in
the β − r plan has always a unique minimum close to the true value, in all of the three scenarios, see
Figure 11-top. Of course, the time span of data collection still has an impact on the quality of the
results. Indeed, if time increases the minimum of the NLL is less and less shallow, which implies that
the uncertainty on the parameters is smaller and smaller. This is visible in Figure 11-bottom, where
we show the Gaussian approximation of the posterior pdfs of β, r. As the data time-span increases,
the posterior pdfs are more and more concentrated and, equivalently, the system is more and more
practically identifiable (cf. Fisher Information Matrix criterion for practical identifiability).
• As already mentioned in Example 6, [6] discusses whether the SIR model without under-reporting factor
is identifiable from cumulative incidence data, rather than prevalence data; since cumulative incidence
data are most typically reported in an outbreak, this question is very relevant for practical purposes.
The finding is that while SIR is indeed structurally identifiable from cumulative incidence data, it is
not practically identifiable from this kind of data, and advocates for a broader diffusion of prevalence
data, from which SIR is practically identifiable, as just discussed in this example.
Example 9 (Structural and practical identifiability of a SEIRD model). In this example, we discuss the
identifiability of a slightly more complex model with incubation period (compartment “E”, exposed), where we
distinguish between recovered and dead persons (compartments “R” and “D”, respectively). We also assume
that at T = Tlock the parameter β changes, due to some restriction measure being enforced (lockdown).8 We
call this model SEIRDz:

β(t)


Ṡ = −
IS


Npop





β(t)



Ė = N IS − iE
pop
˙ = iE − dI − rI

I






Ṙ = rI





Ḋ = dI
where β(t) = β1 for t ≤ Tlock and β(t) = β1 − z for t > Tlock .9 We consider these ranges for the parameters
(that we consider as uniform random variables):
β1 ∈ [0.25, 0.35]

r ∈ [0.06, 0.18]

d ∈ [0.01, 0.02]

i ∈ [0.14, 0.33]

z ∈ [0.1, 0.2].

In this setting the rate r is the recovery rate (inverse of the average days of sickness), the rate d is the
mortality rate and the rate i is the inverse of the incubation time. With these intervals we are assuming
that the average number of days of sickness is roughly between 5 and 16, while the average incubation time
is roughly between 3 and 7 days. Moreover, upon introducing the fatality ratio d/(d + r), i.e. the proportion
of infected that eventually die, this example considers a very lethal infection, with a fatality ratio roughly
8A

change in β of 90% was observed on real-data e.g. in [18].
option would be to consider β(t) = β1 (1 − z) after Tlock , i.e., to use z as the percentage decrease of β1 instead of
the absolute decrease. The treatment of the two models would be identical.
9 Another

26

Sobol indices, S

0.7

Sobol indices, E

1

0.6

0.8

0.8

1

0.9

r
d
i
z

Sobol indices, D

0.9

1

0.9

r
d
i
z

Sobol indices, R

1

1

0.9

0.5

Sobol indices, I

1

1

r
d
i
z

r
d
i
z

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0.1

1

r
d
i
z

0.8
0.7
0.6

0.4

0.5
0.4

0.3

0.3
0.2

0.2

0.1

0

0
0

50

100

0
0

50

100

0.1

0
0

50

100

0
0

50

100

0

50

100

Figure 12: Prior-based forward UQ for the SEIRDz model. Left-most panel: Monte Carlo realizations and expected value of
the compartments. Remaining panels: time-evolution of the Sobol indices.

between 5% and 25%. We consider Tlock = 15 and run the simulation until T = 100. We set the initial
conditions to S(0) = 0.95, E(0) = 0.04, I(0) = 0.01, R(0) = 0, D(0) = 0, and we assume of having at disposal
measurements of the compartments I, R, D.
If we assume that β is constant in time, we can show that the system is structurally identifiable, by means
of the differential algebra approach. The computations are shown in details in Appendix A. Since we have
assumed that we know the time Tlock where the change in β happens, we can apply the structural analysis
results to both the time intervals t ≤ Tlock and t > Tlock separately, and conclude that the model SEIRDz is
structurally identifiable.
Figure 12 shows some results for the preliminary prior-based forward UQ analysis. The left-most panel
shows 100 Monte Carlo trajectories, and the expected value of the compartments, computed with a sparse
grid (2433 model evaluations10 ). It is clearly visible that the trajectories are significantly scattered, and that
the asymptotic values of the compartments vary considerably. The remaining panels show the time-evolution
of the Sobol indices, from which we can derive some information about the identifiability of the model. We
can see that not all parameters impact equally the variability of the solution, and we expect in particular that
it will be difficult to recover by the inversion procedure the value of those parameters that have the smallest
impact (i, z, β1 ). Even more so given that we only measure the compartments I, R, D (while β1 would be
best recovered from the S compartment), and that we will only measure them up to a certain time, and the
Sobol indices are not constant in time (for instance, β1 has a significant impact on R but only at late times,
say T ≥ 30, while we measure essentially early times). Thus, while the system is structurally identifiable, it
might be practically non-identifiable. Next, we perform the inversion and check for practical identifiability.
We fix the parameters as
β1,true = 0.28,

rtrue = 0.11,

dtrue = 0.018,

itrue = 0.18,

ztrue = 0.18,

K = 3,

σ = 0.01;

and measure data until T = 40 (after peak). We repeat the minimization procedure 20 times with different
initial guesses for the parameters, to investigate the presence of local minima of the NLL, and select as MLE
the results that led to the smallest NLL. The results of this procedure are shown in Figure 13-top, where we
report the initial and final values of the NLL, as well as the initial and final values of the parameters. We can
conclude that while the final values of the NLL are all close (yet not identical), the values of the parameters
show a significant variability, denoting the fact that the NLL has a multiple minima with similar NLL value.
Moreover, most of the parameters are not correctly identified. This further suggests that the model might be
practically non-identifiable, at least for the value of σ tested here. Therefore, the trajectories corresponding
to the computed values of the parameters are quite far from the true one, see Figure 13-bottom.
To confirm our diagnosis of non-identifiability, we compute the profile likelihood of the problem. Results
are shown in Figure 14. The only parameter with a deep, narrow minimum is r (compare the vertical scales)
while the other ones are in rather shallow regions. Moreover, the profile likelihoods for i, β1 and z are very
noisy (as expected, due to the fact that their Sobol indices are rather small). This confirms that the only
10 the number of sparse grids points is larger than in Example 1, where we considered a SIR model. This is because now we have
to sample a 5-dimensional parameters space, and sparse grids suffer to a certain degree the so-called “curse of dimensionality”
[75], i.e. loosely speaking, the number of sampling points grows rapidly (more than linearly) with the number of dimensions.
A sampling method that fully suffers from this problem is cartesian sampling, where the number of points grows exponentially
with the number of dimensions.

27

0.35

500
0

0.18

0.022

0.4

0.16

0.02

0.35

0.14

0.018

0.12

0.016

0.3

-500

0.25

0.2

0.3

-1000
0

10

20 0.25

0.15
0.25

-586.5
-587

0.1

0.014

0.08

0.012

0.2

-587.5

0.15

-588
0

10

0.06
0

20

10

20

0.01
0

10

20

1

0.1

0.2
0.15
0

10

20

20

40

0.05
0

10

20

60

80

100

0

10

20

0.1
S
E
I
R
D

0.8

0.08

0.6

0.06

0.4

0.04

0.2

0.02

0

0
0

20

40

60

80

100

0

Figure 13: Results for inversion of SEIRDz. Top row, from left to right: the initial and final value of NLL for the 20 trials (blue
and red line, respectively) and a zoom on the final values (the value with the smallest NLL is marked with a black square); the
initial and final values of the parameters for the 20 trial (blue and red line, respectively), the correct values (black dash lines),
and the values of the parameters that yield the smallest NLL (black square marker). Bottom row: the trajectories of the 20
MLEs and a zoom on the E, I, D compartments. The colored trajectories are those obtained by the values of the parameters
obtained by the 20 optimization trials. The MLE trajectories are reported in full black line, while the true ones in dashed black
lines.

parameter that can be easily identified are r and to a certain extent d (given that the shape of the profile
likelihood is not noisy, although shallow), and overall the system is practically non-identifiable.
As mentioned, a possible workaround which might help in this situation is to learn some of the parameters
from independent studies, and thus reduce the number of parameters to be simultaneously identified. For
instance, medical studies might give us estimates of the incubation time, recovery time and death rate, so
that we are left to identify only the contact probabilities β1 and z. One has to pay attention to the fact that
setting the influential parameters to the wrong values can be however detrimental to the procedure. Here, we
fix the values of i, r, and d to their exact values and repeat the identification procedure. This is of course an
over-optimistic scenario. Another possibility would be to use, for example, r, d obtained from the inversion
procedure (whose profile likelihood is “well-shaped”, even though the one for d is quite shallow; we named this
procedure as “hierarchical optimization” in the previous discussion). The results obtained by fixing i, r, and
d to their exact values are reported in Figure 15, which shows the same information of Figure 13, i.e. initial
and final values of NLL and parameters, and trajectories of the model. The presence of local minima in the
NLL is greatly reduced, and the identification of β1 and z is more robust and closer to the true values (this
might not always be the case though, depending on the noise level and the quality of the data). As a result, the
trajectories corresponding to this new set of parameters are closer to the true ones than the previous results,
as shown in the right-most panel.
Bibliography and further reading.
• For a general survey on practical identifiability, we refer the reader to [63, 6, 7].
• Bootstrap approaches in the context of epidemiological models are considered e.g. in [6, 7]. The analysis
in [6] considers a criterion based on ARE to conclude on identifiability, whereas in [7] a combination
of the size of the confidence interval and mean squared error of the parameters is employed.
• An extensive explanation on the use of the likelihood profiles is given in [70] and further references are
[9, 11].
28

-586.8

-586

-460
-480

-587

-587.5

-587.2

-587.55

-587.3

-587.6

-586.5
-500

-587.4

-587.2
-587.65
-587.5

-520
-587

-587.4

-587.7
-587.6

-540
-587.75
-587.6
-560

-587.7
-587.8

-587.5
-587.8

-580

-588
0.1

-600
0.2

0.3

-587.8

-587.85

0

0.1

0.2

-588
0.01

0.02

0.03

-587.9

-587.9
0.2

0.4

0

0.6

0.1

0.2

Figure 14: Profile likelihood for the SEIRDz identifiability problem, centered around the MLEs of the parameters (red circle
markers). Observe that the red markers do not always coincide with the profile likelihood. This is because the optimization
for the remaining parameters might land at different local optima (despite running the optimizer multiple times with random
initial points).

-200

0.35

0.2

0.1
0.09

-400

0.18

0.08
0.07

-600
0

10

0.16

20

0.06

0.3

0.05

-585.8

0.14

0.04
0.03

-586

0.12

0.02
0.01

0.25

-586.2
0

10

20

0.1
0

10

20

0

10

20

0
0

20

40

60

80

100

Figure 15: Results for inversion of SEIRDz upon blocking the parameters r, d, i. Left panel: the initial and final value of NLL
for the 20 trials (blue and red line, respectively) and a zoom on the final values (the value with the smallest NLL is marked
with a black square). Mid panel: the initial and final values of β1 , z for the 20 trial (blue and red line, respectively), the correct
values (black dash lines), and the values of the parameters that yield the smallest NLL (black square marker). Right panel: the
true trajectories (black lines), the MLE when trying to identify all parameters (colored dotted lines) and the MLE when trying
to identify β1 , z only (full colored lines).

29

• [8] provides a quite comprehensive step-by-step guide on fitting compartmental models for epidemiology,
with an eye to identifiability.
• In the context of COVID-19, some discussion about identifiability is provided by [76].
• [9] provides an interesting example of the consequences of identifiability in the context of a SEIRbased model for a Dengue outbreak: the system is practically non-identifiable, resulting in two sets of
parameters that show an excellent fit of the data but provide dramatically different predictions when
used to test a possible non-medical remediation strategy (removal of mosquitos).
• Optimal design of experiments can be used to improve the quality of the data to minimize the impact of
practical identifiability issues, see e.g. [77].
9. Discussion and conclusions: a revisited UQ workflow
In these notes we have reviewed some computational tools for prediction under uncertainty and parameter
identification for dynamical systems, which we referred to as forward and inverse UQ analyses, respectively.
Combined together, these tools provide a powerful framework for reliable predictions of the outputs of a
dynamical system, and complement the punctual predictions with confidence estimates with solid ground
in probability/statistics theory. However, investigators should always carefully check whether their model
is actually identifiable, both structurally and practically. Dynamical systems might indeed be not fully
identifiable, and blindly using the prediction tools in this case can be harmful, see e.g. Example 8 or the
above-mentioned case study on Dengue reported in [9].
If the system is not identifiable, the Fisher approach to the inversion problem is bound to fail, because it
intrinsically assumes identifiablity of the system, or in other words because it assumes that the NLL has a
unique minimum where the Gaussian approximation of the posterior should be centered, whereas in case of
structural non-identifiability the NLL has a manifold of minima. The case of NLL with a finite number of local
minima (symptom of practical non-identifiability, see Section 8) could instead hopefully be fixed by acquiring
more data of the right kind, that should hopefully rule out the “wrong minima”. Conversely, MCMC methods
make no assumptions on the shape of the NLL and therefore might be a partially safer technique, However,
MCMC algorithms come with a much larger computational cost and are not entirely safe either, since they
typically implement some sort of adaptive sampling, where most of the samples are collected in regions of
large likelihood, i.e., they cannot entirely escape the problem of computing the maxima of the likelihood.
Therefore, unless they are properly designed and tuned, they could fail to realize that there might be an
entire manifold of minima. A compromise solution could be to use the Fisher approximation not plainly as
the posterior pdf of the parameters, but only in the context of an importance sampling strategy, where one
still generates samples from the Fisher approximation of the posterior but then rescales them suitably to
remove any bias [78, 79]. In summary, the ideal UQ workflow sketched in Section 6 (cf. Algorithm 1) can be
adjusted as detailed in Algorithm 2.
Of course, the underlying assumption here is that we know what is the exact model that generated the
data, and we are dealing with parametric identification only. Discussing how model mis-specification affects
identifiability further adds to the complexity of the problem and it is out of the scope of this work.
Our theoretical discussion has been complemented with a number of small examples. None of these
consider the initial conditions of the system as unknown, but doing so would not pose any conceptual
challenge from a numerical point of view. As already hinted in Section 7, the structural identifiability of
parameters connected to the initial conditions can be investigated e.g. by the mapping approach.
An important point that we did not discuss is the issue of model-selection. In the case when multiple
models are available (quite common in the case of epidemics modeling and, in particular, of COVID-19), is
there any way to tell which one has the largest statistical evidence? A large body of work is available on
this topic in the statistical literature, where several criteria have been developed to select the “best model”.
The underlying principle is that adding more parameters might lead to a better fit of the data, but the more
parameters, the larger the chances that the model is overfitted, i.e., that it adjusts to the noise and gets
limited predicting power. Thus, one should restrain from blindly adding more parameters.11 Criteria that
11 this

is a mathematical formulation of the Occam’s razor.

30

Algorithm 2: Ideal UQ workflow
1
2

3
4

5

6
7
8
9

10
11
12

Choose a model and the prior distributions for its parameter (literature, expert opinion);
Determine whether the system is structurally identifiable (Sobol indices, profile likelihood,
differential algebra, mapping approach, etc.);
if the model is structurally identifiable then
while the model is not practically identifiable (bootstrap, profile likelihood, multiple restart, etc.)
do
Acquire more data / get information on some parameters from independent studies / perform a
hierarchical optimization;
end
choose Fisher approximation as inversion method;
else
the likelihood has a manifold of minima: choose an appropriate MCMC algorithm as inversion
method;
end
perform the inverse UQ analysis;
perform the forward UQ analysis based on the posterior distribution to obtain statistical information
about the quantities of interest of the model (e.g. expected value, variance, full pdf of the outputs);

try to identify the optimal model among a pool of possible ones include, for example, the Akaike Information
Criterion (AIC), the Bayes Information Criterion (BIC), and the Kayshap Information Criterion (KIC). We
refer the interested reader e.g. to [80, 81, 82] for a more thorough discussion, as well as to e.g. [83] for a
discussion on a model selection strategy in the case when some models are not identifiable.
Acknowledgments
The authors acknowledge the many fruitful discussions with several colleagues, and in particular the
colleagues at CNR-IMATI that participated in the COVID-19 modeling study group.
Declarations of interest
None.
Funding
Lorenzo Tamellini and Chiara Piazzola have been supported by the PRIN 2017 project 201752HKH8
“Numerical Analysis for Full and Reduced Order Methods for the efficient and accurate solution of complex systems governed by Partial Differential Equations (NA-FROM-PDEs)”. Lorenzo Tamellini also acknowledges the support of GNCS-INdAM (Gruppo Nazionale Calcolo Scientifico - Istituto Nazionale di Alta
Matematica). This work was supported by the KAUST Office of Sponsored Research (OSR) under Award
No. URF/1/2584-01-01 and the Alexander von Humboldt foundation. Raúl Tempone is a member of the
KAUST SRI Center for Uncertainty Quantification in Computational Science and Engineering.
Appendix A. Structural identifiability of a SEIRD model by differential algebra
In this section we consider the SEIRD model, which is a simplified version of the SEIRDz model considered
in Example 9 with β constant in time. We show by means of the differential algebra technique explained in
Section 7 that it is structurally identifiable from prevalence data of I, R and D.

31

Let us consider the following system

β


IS
Ṡ = −


Npop






I˙ = i(Npop − S − I − R − D) − dI − rI







Ṙ = rI




Ḋ = dI



1



Y = I


K



1


Z= R



K




W = 1 D,
K
where we have removed the equation for the compartment E, as it holds that Npop = S + E + I + R + D (the
same argument was used in Example 6, when we removed the equation for S while discussing identifiability
of SIR with data of I and R). We then rewrite the differential equations in terms of the observed variables.
In particular, we derive the following explicit expression for S from the second equation:
S=−

K
i+d+r
Ẏ + Npop − K
Y − KW − KZ.
i
i

From this, we compute Ṡ, insert both formulas in the first differential equation above, and obtain the first
input-output equation:
−

K
i+d+r
βK 2
βK 2
Ÿ − K
Ẏ − K Ẇ − K Ż −
Y Ẏ + βKY −
(i + d + r)Y 2 = 0.
i
i
iNpop
iNpop

The other two input-output equations follow from the third and fourth differential equation above and are:
K Ż − rKY = 0

and

K Ẇ − dKY = 0.

The set of these three input-output equations is not mutually reduced with respect to the ranking Y < Z <
W < Ẏ < Ż < Ẇ < Ÿ < Z̈ < Ẅ (other ranking would lead to the same results). The third equation is not
reduced with respect to the first one, as its leader monomial Ẇ appears also in the first one. Similarly, the
second equation is not reduced with respect to the first one. By doing some further substitutions to eliminate
Ż and Ẇ in the first equation we finally get a set of mutually reduced equations:

K
i+d+r
βK 2
βK 2


−
Ÿ
−
K
Ẏ
−
KdY
−
KrY
−
Y
Ẏ
+
βKY
−
(i + d + r)Y 2 = 0


 i
i
iNpop
iNpop
K Ż − rKY = 0




K Ẇ − dKY = 0.
The last step is to make the polynomials monic with respect to their leaders, which are Ÿ , Ż, and Ẇ ,
respectively. It then follows that all the coefficients can be uniquely determined; hence, the SEIRD model is
structurally identifiable from data of I, R, and D.
References
[1] A. Alahmadi, S. Belet, A. Black, D. Cromer, J. Flegg, T. House, P. Jayasundara, J. Keith, J. McCaw,
M. R., J. Ross, F. Shearer, S. Thein Than Tun, W. J., L. White, J. Whyte, A. Yan, A. Zarebski,
Influencing public health policy with data-informed mathematical models of infectious diseases: Recent
developments and new challenges, Epidemics 32 (2020) 100393.
32

[2] J. H. Guillaume, J. D. Jakeman, S. Marsili-Libelli, M. Asher, P. Brunner, B. Croke, M. C. Hill, A. J.
Jakeman, K. J. Keesman, S. Razavi, J. D. Stigter, Introductory overview of identifiability analysis: A
guide to evaluating whether you have the right type of data for your modeling purpose, Environmental
Modelling & Software 119 (2019) 418 – 432.
[3] W. O. Kermack, A. G. McKendrick, A contribution to the mathematical theory of epidemics, Proceedings
of the Royal Society A. 115 (772) (1927) 700––721.
[4] M. A. Capistrán, J. A. Christen, J. X. Velasco-Hernández, Towards uncertainty quantification and
inference in the stochastic SIR epidemic model, Mathematical Biosciences 240 (2) (2012) 250–259.
[5] A. Capaldi, S. Behrend, B. Berman, J. Smith, J. Wright, A. L. Lloyd, Parameter estimation and uncertainty quantification for an epidemic model, Mathematical Biosciences and Engineering 9 (3) (2012)
553–576.
[6] N. Tuncer, T. T. Le, Structural and practical identifiability analysis of outbreak models, Mathematical
Biosciences 299 (2018) 1–18.
[7] K. Roosa, G. Chowell, Assessing parameter identifiability in compartmental dynamic models using a
computational approach: application to infectious disease transmission models, Theoretical Biology and
Medical Modelling 16 (1) (2019).
[8] G. Chowell, Fitting dynamic models to epidemic outbreaks with quantified uncertainty: A primer for
parameter uncertainty, identifiability, and forecasts, Infectious Disease Modelling 2 (3) (2017) 379–398.
[9] Y.-H. Kao, M. C. Eisenberg, Practical unidentifiability of a simple vector-borne disease model: Implications for parameter estimation and intervention assessment, Epidemics 25 (2018) 89–100.
[10] M. G. Roberts, Epidemic models with uncertainty in the reproduction number, Journal of Mathematical
Biology 66 (7) (2013) 1463–1474.
[11] C. Tönsing, J. Timmer, C. Kreutz, Profile likelihood-based analyses of infectious disease models, Statistical Methods in Medical Research 27 (7) (2018) 1979–1998.
[12] G. Giordano, F. Blanchini, R. Bruno, P. Colaneri, A. Di Filippo, A. Di Matteo, M. Colaneri, Modelling
the COVID-19 epidemic and implementation of population-wide interventions in Italy, Nature Medicine
26 (2020) 855–860.
[13] C. Anastassopoulou, L. Russo, A. Tsakris, C. Siettos, Data-based analysis, modelling and forecasting of
the COVID-19 outbreak, PLOS ONE 15 (3) (2020) 1–21.
[14] L. Peng, W. Yang, D. Zhang, C. Zhuge, L. Hong, Epidemic analysis of COVID-19 in China by dynamical
modeling, arXiv preprints (2002.06563) (2020).
[15] H. Wang, Z. Wang, Y. Dong, R. Chang, C. Xu, X. Yu, S. Zhang, L. Tsamlag, M. Shang, J. Huang,
Y. Wang, G. Xu, T. Shen, X. Zhang, Y. Cai, Phase-adjusted estimation of the number of Coronavirus
Disease 2019 cases in Wuhan, China, Cell Discovery 6 (2020).
[16] F. Della Rossa, D. Salzano, A. Di Meglio, F. De Lellis, M. Coraggio, C. Calabrese, A. Guarino, R. Cardona, P. DeLellis, D. Liuzza, F. Lo Iudice, G. Russo, M. Di Bernardo, A network model of Italy shows
that intermittent regional strategies can alleviate the COVID-19 epidemic, Nature Communications 11
(2020) 5106.
[17] M. Gatto, E. Bertuzzo, L. Mari, S. Miccoli, L. Carraro, R. Casagrandi, A. Rinaldo, Spread and dynamics
of the COVID-19 epidemic in Italy: Effects of emergency containment measures, Proceedings of the
National Academy of Sciences 117 (19) (2020) 10484–10491.

33

[18] E. Lavezzo, E. Franchin, C. Ciavarella, G. Cuomo-Dannenburg, L. Barzon, C. Del Vecchio, L. Rossi,
R. Manganelli, A. Loregian, N. Navarin, D. Abate, M. Sciro, S. Merigliano, E. De Canale, M. C. Vanuzzo,
V. Besutti, F. Saluzzo, F. Onelia, M. Pacenti, S. G. Parisi, G. Carretta, D. Donato, L. Flor, S. Cocchio,
G. Masi, A. Sperduti, L. Cattarino, R. Salvador, M. Nicoletti, F. Caldart, G. Castelli, E. Nieddu, B. Labella, L. Fava, M. Drigo, K. A. M. Gaythorpe, K. E. C. Ainslie, M. Baguelin, S. Bhatt, A. Boonyasiri,
O. Boyd, L. Cattarino, C. Ciavarella, H. L. Coupland, Z. Cucunubá, G. Cuomo-Dannenburg, B. A.
Djafaara, C. A. Donnelly, I. Dorigatti, S. L. van Elsland, R. FitzJohn, S. Flaxman, K. A. M. Gaythorpe,
W. D. Green, T. Hallett, A. Hamlet, D. Haw, N. Imai, B. Jeffrey, E. Knock, D. J. Laydon, T. Mellan,
S. Mishra, G. Nedjati-Gilani, P. Nouvellet, L. C. Okell, K. V. Parag, S. Riley, H. A. Thompson, H. J. T.
Unwin, R. Verity, M. A. C. Vollmer, P. G. T. Walker, C. E. Walters, H. Wang, Y. Wang, O. J. Watson,
C. Whittaker, L. K. Whittles, X. Xi, N. M. Ferguson, A. R. Brazzale, S. Toppo, M. Trevisan, V. Baldo,
C. A. Donnelly, N. M. Ferguson, I. Dorigatti, A. Crisanti, Suppression of a SARS-CoV-2 outbreak in
the Italian municipality of Vo’, Nature 584 (2020) 425–429.
[19] A. Pugliese, S. Sottile, Inferring the COVID-19 infection curve in Italy, arXiv preprints (2004.09404)
(2020).
[20] G. Albi, L. Pareschi, M. Zanella, Control with uncertain data of socially structured compartmental
epidemic models, arXiv preprints (2004.13067) (2020).
[21] M. Rosenblatt, Remarks on some nonparametric estimates of a density function, The Annals of Mathematical Statistics 27 (3) (1956) 832–837.
[22] E. Parzen, On estimation of a probability density function and mode, The Annals of Mathematical
Statistics 33 (3) (1962) 1065–1076.
[23] R. E. Caflisch, Monte Carlo and quasi-Monte Carlo methods, in: Acta numerica, 1998, Vol. 7 of Acta
Numer., Cambridge Univ. Press, Cambridge, 1998, pp. 1–49.
[24] H. Niederreiter, Random number generation and quasi-Monte Carlo methods, CBMS-NSF regional conference series in applied mathematics, SIAM, 1992.
[25] M. D. McKay, R. J. Beckman, W. J. Conover, Comparison of three methods for selecting values of input
variables in the analysis of output from a computer code, Technometrics 21 (2) (1979) 239–245.
[26] I. Babuška, F. Nobile, R. Tempone, A stochastic collocation method for elliptic partial differential
equations with random input data, SIAM Review 52 (2) (2010) 317–355.
[27] D. Xiu, J. Hesthaven, High-order collocation methods for differential equations with random inputs,
SIAM J. Sci. Comput. 27 (3) (2005) 1118–1139.
[28] A. Cohen, R. DeVore, Approximation of high-dimensional parametric PDEs, Acta Numerica 24 (2015)
1–159.
[29] M. D. Gunzburger, C. G. Webster, G. Zhang, Stochastic finite element methods for partial differential
equations with random input data, Acta Numerica 23 (2014) 521–650.
[30] I. H. Sloan, H. Woźniakowski, When are quasi-Monte Carlo algorithms efficient for high-dimensional
integrals?, J. Complexity 14 (1) (1998) 1–33.
[31] I. M. Sobol’, Sensitivity estimates for nonlinear mathematical models, Math. Modeling Comput. Experiment 1 (4) (1993) 407–414 (1995).
[32] D. Cacuci, Sensitivity & Uncertainty Analysis, Volume 1: Theory, no. v. 1, CRC Press, 2003.
[33] A. Saltelli, M. Ratto, T. Andres, F. Campolongo, J. Cariboni, D. Gatelli, M. Saisana, S. Tarantola,
Global Sensitivity Analysis: The Primer, Wiley, 2008.
[34] L. Formaggia, A. Guadagnini, I. Imperiali, V. Lever, G. Porta, M. Riva, A. Scotti, L. Tamellini, Global
sensitivity analysis through polynomial chaos expansion of a basin-scale geochemical compaction model,
Computational Geosciences 17(1) (2013) 25–42.
34

[35] E. Borgonovo, X. Lu, Is Time to Intervention in the COVID-19 Outbreak Really Important? A Global
Sensitivity Analysis Approach, arXiv preprints (2005.01833) (2020).
[36] M. D. Morris, Factorial sampling plans for preliminary computational experiments, Technometrics 33 (2)
(1991) 161–174.
[37] A. Dell’Oca, M. Riva, A. Guadagnini, Moment-based metrics for global sensitivity analysis of hydrological systems, Hydrology and Earth System Sciences 21 (12) (2017) 6219–6234.
[38] A. M. Stuart, Inverse problems: A bayesian perspective, Acta Numerica 19 (2010) 451–559.
[39] J. Carrera, S. P. Neuman, Estimation of aquifer parameters under transient and steady state conditions:
1. maximum likelihood method incorporating prior information, Water Resources Research 22 (2) (1986)
199–210.
[40] S. M. Kay, Fundamentals of statistical signal processing, Prentice Hall PTR, 1993.
[41] J. Nocedal, S. Wright, Numerical Optimization, 1st Edition, Springer-Verlag New York, 1999.
[42] G. Porta, L. Tamellini, V. Lever, M. Riva, Inverse modeling of geochemical and mechanical compaction
in sedimentary basins through polynomial chaos expansion, Water Resources Research 50 (12) (2014).
[43] J. Berger, Statistical Decision Theory and Bayesian Analysis, Springer Series in Statistics, Springer,
1985.
[44] C. C. Drovandi, Approximate Bayesian Computation, American Cancer Society, 2017, pp. 1–9.
[45] H. Rue, S. Martino, N. Chopin, Approximate Bayesian inference for latent Gaussian models by using
integrated nested Laplace approximations, Journal of the Royal Statistical Society: Series B (Statistical
Methodology) 71 (2) (2009) 319–392.
[46] J. M. Ver Hoef, P. L. Boveng, Quasi-poisson vs. negative binomial regression: How should we model
overdispersed count data?, Ecology 88 (11) (2007) 2766–2772.
[47] A. Lindén, S. Mäntyniemi, Using the negative binomial distribution to model overdispersion in ecological
count data, Ecology 92 (7) (2011) 1414–1421.
[48] D. Firth, Bias reduction of maximum likelihood estimates, Biometrika 80 (1) (1993) 27–38.
[49] J. Witteveen, S. Sarkar, H. Bijl, Modeling physical uncertainties in dynamic stall induced fluid–structure
interaction of turbine blades using arbitrary polynomial chaos, Computers and Structures 85 (11) (2007)
866–878.
[50] S. Oladyshkin, W. Nowak, Data-driven uncertainty quantification using the arbitrary polynomial chaos
expansion, Reliability Engineering and System Safety 106 (0) (2012) 179 – 190.
[51] C. Schillings, C. Schwab, Sparse, adaptive Smolyak quadratures for Bayesian inverse problems, Inverse
Problems 29 (6) (2013).
[52] O. G. Ernst, B. Sprungk, L. Tamellini, On Expansions and Nodes for Sparse Grid Collocation of Lognormal Elliptic PDEs, Arxiv e-prints (1906.01252), accepted. Also available as IMATI report 19-02
(2019).
[53] Y. Marzouk, D. Xiu, A stochastic collocation approach to bayesian inference in inverse problems, in:
Communications in computational physics 6, 2009, pp. 826–847.
[54] A. Manzoni, S. Pagani, T. Lassila, Accurate solution of bayesian inverse uncertainty quantification problems combining reduced basis methods and reduction error models, SIAM/ASA Journal on Uncertainty
Quantification 4 (1) (2016) 380–412.
[55] G. Golub, V. Pereyra, Separable nonlinear least squares: the variable projection method and its applications, Inverse Problems 19 (2) (2003) R1–R26.
35

[56] P. De Valpine, A. Hastings, Fitting population models incorporating process noise and observation error,
Ecological Monographs 72 (1) (2002) 57–76.
[57] D. He, E. L. Ionides, A. A. King, Plug-and-play inference for disease dynamics: measles in large and
small populations as a case study, Journal of The Royal Society Interface 7 (43) (2010) 271–283.
[58] L. Martı́n-Fernández, G. Gilioli, E. Lanzarone, J. Mı́guez, S. Pasquali, F. Ruggeri, D. P. Ruiz, A raoblackwellized particle filter for joint parameter estimation and biomass tracking in a stochastic predatorprey system, Mathematical Biosciences and Engineering 11 (1551-0018 2014 3 573) (2014) 573.
[59] J. Ritt, Differential Algebra, Colloquium publications, American Mathematical Society, 1950.
[60] M. Eisenberg, Input-output equivalence and identifiability: some simple generalizations of the differential
algebra approach, arXiv preprints (1302.5484v2) (Feb. 2019).
[61] M. P. Saccomani, S. Audoly, L. D’Angiò, Parameter identifiability of nonlinear systems: the role of
initial conditions, Automatica 39 (4) (2003) 619–632.
[62] H. Hong, A. Ovchinnikov, G. Pogudin, C. Yap, Global identifiability of differential models, Communications on Pure and Applied Mathematics 73 (9) (2020) 1831–1879. doi:10.1002/cpa.21921.
[63] H. Miao, X. Xia, A. S. Perelson, H. Wu, On identifiability of nonlinear ODE models and applications in
viral dynamics, SIAM Review 53 (1) (2011) 3–39.
[64] N. Evans, M. Chapman, M. Chappell, K. Godfrey, The structural identifiability of a general epidemic
(SIR) model with seasonal forcing, IFAC Proceedings Volumes 35 (1) (2002) 109–114, 15th IFAC World
Congress.
[65] N. D. Evans, L. J. White, M. J. Chapman, K. R. Godfrey, M. J. Chappell, The structural identifiability
of the susceptible infected recovered model with seasonal forcing, Mathematical Biosciences 194 (2)
(2005) 175–197.
[66] R. Bellman, K. Åström, On structural identifiability, Mathematical Biosciences 7 (3) (1970) 329–339.
[67] A. Ovchinnikov, G. Pogudin, P. Thompson, Input-output equations and identifiability of linear ode
models, arXiv preprints (1910.03960) (2020).
[68] A. Ovchinnikov, A. Pillay, G. Pogudin, T. Scanlon, Computing all identifiable functions for ode models,
arXiv preprints (2004.07774) (2020).
[69] M. C. Eisenberg, M. A. Hayashi, Determining identifiable parameter combinations using subset profiling,
Mathematical Biosciences 256 (2014) 116–126.
[70] A. Raue, C. Kreutz, T. Maiwald, J. Bachmann, M. Schilling, U. Klingmüller, J. Timmer, Structural
and practical identifiability analysis of partially observed dynamical models by exploiting the profile
likelihood, Bioinformatics 25 (15) (2009) 1923–1929.
[71] M. Iglesias, Z. Sawlan, M. Scavino, R. Tempone, C. Wood, Bayesian inferences of the thermal properties
of a wall using temperature and heat flux measurements, International Journal of Heat and Mass Transfer
116 (2018) 417–431.
[72] V. Kolehmainen, T. Tarvainen, S. Arridge, J. Kaipio, Marginalization of uninteresting distributed parameters in inverse problems-application to diffuse optical tomography, International Journal for Uncertainty
Quantification 1 (1) (2011) 1–17.
[73] F. Ruggeri, Z. Sawlan, M. Scavino, R. Tempone, A hierarchical bayesian setting for an inverse problem
in linear parabolic pdes with noisy boundary conditions, Bayesian Anal. 12 (2) (2017) 407–433.
[74] A.-N. Spiess, N. Neumeyer, An evaluation of R2 as an inadequate measure for nonlinear models in
pharmacological and biochemical research: a Monte Carlo approach, BMC Pharmacology 10 (1) (2010).

36

[75] G. Wasilkowski, H. Wozniakowski, Explicit cost bounds of algorithms for multivariate tensor product
problems, Journal of Complexity 11 (1) (1995) 1 – 56.
[76] W. C. Roda, M. B. Varughese, D. Han, M. Y. Li, Why is it difficult to accurately predict the COVID-19
epidemic?, Infectious Disease Modelling 5 (2020) 271–281.
[77] Q. Long, M. Scavino, R. Tempone, S. Wang, A Laplace method for under-determined bayesian optimal
experimental designs, Computer Methods in Applied Mechanics and Engineering 285 (2015) 849–876.
[78] J. Beck, B. M. Dia, L. F. Espath, Q. Long, R. Tempone, Fast bayesian experimental design: Laplacebased importance sampling for the expected information gain, Computer Methods in Applied Mechanics
and Engineering 334 (2018) 523–553.
[79] J. Beck, B. Mansour Dia, L. Espath, R. Tempone, Multilevel double loop Monte Carlo and stochastic
collocation methods with importance sampling for bayesian optimal experimental design, International
Journal for Numerical Methods in Engineering 121 (15) (2020) 3482–3503.
[80] A. Schöniger, T. Wöhling, L. Samaniego, W. Nowak, Model selection on solid ground: Rigorous comparison of nine ways to evaluate bayesian model evidence, Water Resources Research 50 (12) (2014)
9484–9513.
[81] G. Claeskens, N. L. Hjort, Model Selection and Model Averaging, no. 9780521852258 in Cambridge
Books, Cambridge University Press, 2008.
[82] K. P. Burnham, D. R. Anderson, Model selection and inference: a practical information-theoretical
approach, New-York: Springel-Verlag (1998).
[83] M. Drton, M. Plummer, A bayesian information criterion for singular models, Journal of the Royal
Statistical Society: Series B (Statistical Methodology) 79 (2) (2017) 323–380.

37

