arXiv:2010.09958v2 [stat.AP] 25 Jan 2021

Forecasting unemployment using Internet search
data via PRISM
Dingdong Yi ∗ , Shaoyang Ning ∗ , Chia-Jung Chang ∗ , S. C. Kou *
January 2021

Abstract
Big data generated from the Internet offer great potential for predictive analysis.
Here we focus on using online users’ Internet search data to forecast unemployment
initial claims weeks into the future, which provides timely insights into the direction
of the economy. To this end, we present a novel method PRISM (Penalized Regression with Inferred Seasonality Module), which uses publicly available online search
data from Google. PRISM is a semi-parametric method, motivated by a general statespace formulation, and employs nonparametric seasonal decomposition and penalized regression. For forecasting unemployment initial claims, PRISM outperforms
all previously available methods, including forecasting during the 2008-2009 financial crisis period and near-future forecasting during the COVID-19 pandemic period,
when unemployment initial claims both rose rapidly. The timely and accurate unemployment forecasts by PRISM could aid government agencies and financial institutions to assess the economic trend and make well-informed decisions, especially in
the face of economic turbulence.
Key Words: state-space model; seasonality; exogenous variable; big data; penalized
regression; unemployment forecast
* Dingdong

Yi is a Quantitative Researcher at Citadal Americas LLC; email: yidingdong@gmail.com.
Shaoyang Ning is Assistant Professor of Statistics, Williams College; email: sn9@williams.edu. Chia-Jung
Chang is Associate Professor, Department of Statistics and Applied Probability, National University of
Singapore, Singapore; email: stacc@nus.edu.sg. S. C. Kou is Professor, Department of Statistics, Harvard
University, Cambridge, MA 02138; email: kou@stat.harvard.edu.

1

1.

Introduction

Driven by the growth and wide availability of Internet and online platforms, big data are
generated with an unprecedented speed nowadays. They offer the potential to inform
and transform decision making in industry, business, social policy and public health
(Manyika et al., 2011; McAfee and Brynjolfsson, 2012; Chen et al., 2012; Khoury and
Ioannidis, 2014; Kim et al., 2014; Murdoch and Detsky, 2013). Big data can be used for
developing predictive models for systems that would have been challenging to predict
with traditional small-sample-based approaches (Einav and Levin, 2014; Siegel, 2013).
For instance, numerous studies have demonstrated the potential of using Internet search
data in tracking influenza outbreaks (Ginsberg et al., 2009; Yang et al., 2015; Ning et al.,
2019; ?), dengue fever outbreaks (Yang et al., 2017), financial market returns (Preis et al.,
2013; Risteski and Davcev, 2014), consumer behaviors (Goel et al., 2010), unemployment
(Ettredge et al., 2005; Choi and Varian, 2012; Li, 2016) and housing prices (Wu and
Brynjolfsson, 2015).
We consider here using Internet users’ Google search to forecast US unemployment
initial claims weeks into the future. Unemployment initial claims measure the number
of jobless claims filed by individuals seeking to receive state jobless benefits. It is closely
watched by the government and the financial market, as it provides timely insights into
the direction of the economy. A sustained increase of initial claims would indicate rising
unemployment and a challenging economy, whereas a steady decrease of initial claims
would signal recovery of labor market. During the great financial crises of 2008 and
the COVID-19 pandemic, these unemployment data have been a key focus for government agencies when making fiscal and monetary policy decisions under unprecedented
pressure.
Weekly unemployment initial claim is the (unadjusted) total number of actual initial
claims filed under the Federal-State Unemployment Insurance Program in each week
ending on Saturday. The Employment and Training Administration (ETA) of the US
2

Department of Labor collects the weekly unemployment insurance claims reported by
each state’s unemployment insurance program office, and releases the data to the public
at 8:30 A.M. (eastern time) on the following Thursday. Thus, the weekly unemployment
initial claim data are reported with a one-week delay: the number reported on Thursday
of a given week is actually the unemployment initial claim number of the preceding
week. For accessing the general economic trend, it is, therefore, highly desirable for
government agencies and financial institutions to predict the unemployment situation
of the current week, which is known as nowcasting (Giannone et al., 2008), as well as
weeks into the future. In this article, we use the general phrase forecasting to cover both
nowcasting (the current week) and predicting into future weeks.
In contrast to the one-week delayed unemployment reports by the Department of Labor, Internet users’ online search of unemployment-related query terms provides highly
informative and real-time information for the current unemployment situation. For instance, a surge of Internet search of “unemployment office”, “unemployment benefits”,
“unemployment extension”, etc. in a given week could indicate an increase of unemployment of that week, as presumably more people unemployed are searching for information of getting unemployment aid. Internet search data, offering a real-time “peek”
of the current week, thus, augments the delayed official time-series unemployment data.
There are several challenges in developing an effective method to forecast weekly
unemployment initial claims with Internet search data. First, the volatile seasonality
pattern accounts for most of the variation of the target time series. Figure 1 (bottom)
plots the weekly unemployment initial claims from 2007 to 2016; the seasonal spikes
are particularly noteworthy. A prediction method should address and utilize the strong
seasonality in order to achieve good prediction performance. Second, the method needs
to effectively incorporate the most up-to-date Internet search data into the modeling of
target time series. Third, as people’s search pattern and the search engine both evolve
over time, the method should be able to accommodate this dynamic change.
Most time series models rely on state-space models to deal with seasonality, where
3

cumulative absolute error
unemployment initial claims

BATS

1.5e+07
TBATS
PRISM w/o X
BSTS
PRISM

1.0e+07
5.0e+06
0.0e+00

8e+05
6e+05
4e+05
2e+05
2008

2010

2012
Time

2014

2016

Figure 1: (Top) The cumulative absolute error of nowcasting of different methods. (Bottom) The unemployment initial claims for the same period of 2007 − 2016.
the latent components capture the trend and seasonality (Aoki, 1987; Harvey, 1989).
Among the time series models, structural time series models and innovation state-space
models are two main frameworks (Harvey and Koopman, 1993; Durbin and Koopman,
2012; Hyndman et al., 2008), both of which have various extensions of seasonal pattern
modeling and can incorporate exogenous signals as regression components (see Supplementary Material A2 for more discussion). For nowcasting time series with seasonal
pattern, Scott and Varian (2013, 2014) developed a Bayesian method based on the structural time series model, using a spike-and-slab prior for variable selection, and applied it
to nowcast unemployment initial claims with Google search data by treating the search
data as regressors. Alternative to this regression formulation, Banbura et al. (2013) proposed a nowcasting method using a factor model, in which target time series and related
exogenous time series are driven by common factors.
Here we introduce a novel prediction method PRISM, which stands for Penalized Re-

4

gression with Inferred Seasonality Module, for forecasting times series with seasonality,
and use it to forecast unemployment initial claims. Our method is semi-parametric in
nature, and takes advantage of both the state-space formulation for time series forecasting and penalized regression. With the semi-parametric method PRISM, we significantly
expand the range of time series models for forecasting, going beyond the traditional approaches, which are often tailored for individual parametric models. PRISM offers a
robust and more accurate forecasting alternative to traditional parametric approaches
and effectively addresses the three aforementioned challenges in forecasting time series
with strong seasonality. First, our method accommodates various nonparametric and
model-based seasonal decomposition tools, and effectively incorporates the estimated
seasonal components into predictive modeling. It thus can robustly handle complex seasonal patterns. Second, different from the traditional regression formulation, our joint
modeling of the target time series and the exogenous variables accommodates the potential causal relationship between them — people do online Google search in response of
becoming unemployed. Third, PRISM uses dynamic forecasting — training its predictive
equation each week for the forecasting — and utilizes rolling window and exponential
weighting to account for the time-varying relationship between the target time series and
the exogenous variables. For forecasting unemployment initial claims, PRISM delivers
superior performance over all existing forecasting methods for the entire time period of
2007 − 2019, and is exceptionally robust to the ups and downs of the general economic
environment, including the huge volatility caused by the 2008 financial crisis.
While the forecasting target here is the unemployment initial claims, we want to
highlight that PRISM applies to forecasting other time series with complex seasonal
patterns.

5

2.

Data and Method

2.1.

Initial Claims Data and Internet Search Data from Google

The weekly (non-seasonally adjusted) initial claims are our target time series. The initial
claims for the preceding week are released every Thursday. The time series of the initial claims from 1967 to present are available at https://fred.stlouisfed.org/series/
ICNSA.
The real-time Internet search data we used were obtained from Google Trends (www.
google.com/trends) with Python package pytrends. The Google Trends website, which
is publicly accessible, provides weekly (relative) search volume of search query terms
specified by a user. Specifically, for a user-specified query term, Google Trends provides
integer-valued weekly times series (after 2004); each number in the time series, ranging
from 0 to 100, represents the search volume of that search query term in a given week
divided by the total online search volume of that week; and the number is normalized
to integer values from 0 to 100, where 100 corresponds to the maximum weekly search
within the time period (specified by the user). Figure A1 in the Supplementary Material
illustrates the Google Trend time series of several search query terms in a 5-year span.
The search query terms in our study were also identified from the Google Trends tool.
One feature of Google Trends is that, in addition to the time series of a specific term (or
a general topic), it also returns the top query terms that are most highly correlated with
the specific term. In our study, we used a list of top 25 Google search terms that are most
highly correlated with the term “unemployment”. Table A1 of the Supplementary Material lists these 25 terms, which were generated by Google Trends on January 11, 2018;
they included 12 general unemployment related query terms, such as “unemployment
office”, “unemployment benefits” and “unemployment extension”, as well as 13 query
terms that were combinations of state names and “unemployment”, such as “California
unemployment” and “unemployment Florida”.

6

2.2.

Overview of PRISM

PRISM employs a two-stage estimation procedure for forecasting time series yt using its
lagged values and the available exogenous variables xt . The derivation and rationale of
each step will be described subsequently.
Input: Target time series {y1:(t−1) } and exogenous time series {xt0 :t }. In the forecasting of unemployment initial claims, {y1:(t−1) } is the official weekly unemployment
initial claim data reported with one-week delay, and {xt0 :t } is the multivariate Google
Trends data starting from 2004.
Stage 1 of PRISM: Seasonal decomposition. Decompose {yt }, the univariate time
series of interest, into the seasonal component {γt }, and the seasonally adjusted component zt = yt − γt . In particular, with a fixed rolling window length M, Stage
1 nonparametrically decomposes {y(t− M):(t−1) } into estimated seasonal component

{γ̂i,t }i=(t− M),...,(t−1) and estimated seasonally adjusted component {ẑi,t }i=(t− M),...,(t−1)
using data available at time t.
Stage 2 of PRISM: Penalized regression. Forecast target time series using:
ŷt+l =

(l )
µy

K

+∑

j =1

(l )
α j ẑt− j,t

K

+∑

j =1

(l )
δj γ̂t− j,t

p

+ ∑ β i xi,t
(l )

(2.1)

i =1

where the coefficients are estimated by a rolling-window L1 penalized linear regression using historical data for each forecasting horizon: l = 0 corresponds to nowcast;
l ≥ 1 corresponds to forecasting future weeks. Note that for notational ease, we will
suppress “(l )” in the superscripts of the coefficients in the subsequent discussion.

2.3.

Derivation of PRISM

PRISM is motivated by a general state-space formulation for univariate time series with
seasonal pattern. We postulate that the seasonal and seasonally adjusted component
7

{γt } and {zt } each evolve according to a linear state-space model with state vectors {st }
and {ht } respectively:
y t = z t + γt
(

z t = w 0 ht + et

(2.2b)

ht = F ht −1 + ηt

(2.2c)

(2.2a)
(

γt = v 0 s t + ζ t

(2.2d)

st = P st −1 + ωt

(2.2e)

iid

where (et , ζ t , ηt0 , ωt0 )0 ∼ N (0, H ), and θ = (w, F , v, P , H ) are the parameters.
Our state-space formulation contains a variety of widely used time series models,
including structural time series models (Harvey, 1989) and additive innovation statespace models (Aoki, 1987; Ord et al., 1997; Hyndman et al., 2008). Under the general
formulation (2.2), a specific parametric model can be obtained by specifying the state
vectors {ht } and {st } along with the dependence structure H. We highlight a few
special cases of model (2.2) in Supplementary Material.
PRISM also models the contemporaneous information from exogenous variables. Let
xt = ( x1,t , x2,t , . . . , x p,t )0 be the vector of the exogenous variables at time t. We postulate
a state-space model for xt on top of yt , instead of adding them as regressors as in traditional models. In particular, at each time t, we assume a multivariate normal distribution
for xt conditional on the level of unemployment initial claims yt ,
xt |yt ∼ N p (µx + yt β, Q)

(2.3)

where β = ( β 1 , . . . , β p )0 , µx = (µ x1 , . . . , µ x p )0 , and Q is the covariance matrix. xt is
assumed to be independent of {yl , xl : l < t} conditional on yt . For {yt } following the
general state-space model (2.2), our joint model for yt and xt can be diagrammed as:

· · · → (st , ht ) → (st +1 , ht +1 ) → · · ·
↓
↓
yt
y t +1
↓
↓
xt
xt +1
8

To forecast yt+l under above model assumptions at time t, we consider the predictive
distribution of yt+l by conditioning on the historical data {y1:(t−1) } and contemporaneous exogenous time series {xt0 :t } as well as the latent seasonal component {γ1:(t−1) }.
z1:(t−1) is known given y1:(t−1) and γ1:(t−1) . We can derive a universal representation
of the predictive distribution p(yt+l | z1:(t−1) , γ1:(t−1) , xt0 :t ), which is normal with mean
linear in z1:(t−1) , γ1:(t−1) and xt as in (2.1) (see Supplementary Material for the proof).
This observation leads to our two-stage semi-parametric estimation procedure PRISM
for nowcasting yt and forecasting yt+l (l ≥ 1) using all available information at time t.

2.4.

Stage 1 of PRISM: seasonal decomposition

PRISM estimates the unobserved seasonal components γ1:(t−1) in the first stage. For this
purpose, various seasonal decomposition methods can be used here, including nonparametric methods such as the classical additive seasonal decomposition (Stuart et al., 1963)
and parametric methods based on innovation state-space models. We use the method
of Seasonal and Trend decomposition using Loess (STL) (Cleveland et al., 1990) as the
default choice. The STL method is nonparametric. It is widely used and robust for
decomposing time series with few assumptions owing to its nonparametric nature.
At every time t for forecasting, we apply the seasonal decomposition method (such
as the default STL) to historical initial claims observations y(t− M):(t−1) with M being a
large number. For each rolling window from t − M to t − 1, the univariate time series
y(t− M):(t−1) is decomposed into three components: seasonal, trend and the remainder.
Denote γ̂i,t and ẑi,t as the estimates of γi and zi using data available at time t. Then, at
each t the seasonal decomposition generates estimated seasonal component time series

{γ̂i,t }i=(t− M),...,(t−1) and seasonally adjusted time series {ẑi,t }i=(t− M),...,(t−1) ; the latter is
the sum of trend component and remainder component. In our forecasting of unemployment initial claims, we took M = 700. We describe the basic procedure of the default
STL in the Supplementary Material.

9

2.5.

Stage 2 of PRISM: penalized linear regression

For each fixed forecasting horizon l (≥ 0), we estimate yt+l by the linear predictive
equation:
K

K

p

j =1

j =1

i =1

ŷt+l = µy + ∑ α j ẑt− j,t + ∑ δj γ̂t− j,t + ∑ β i xi,t ,

(2.4)

where for notational ease we have suppressed l in the coefficients and used the generic
notations µy , α j , δj with the understanding that there is a separate set of {µy , α =

(α1 , . . . , αK ), δ = (δ1 , . . . , δK ), β = ( β 1 , . . . , β p )} for each l. At each time t and for each
forecasting horizon l, the regression coefficients µy , α, δ and β are obtained by minimizing
1
N

t − l −1

∑

w

t−τ



τ =t−l − N
p

K

K

j =1

j =1

yτ +l − µy − ∑ α j ẑτ − j,τ − ∑ δj γ̂τ − j,τ

− ∑ β i xi,τ

2

+ λ1 (kαk1 + kδ k1 ) + λ2 kβ k1 ,

(2.5)

i =1

where N is the length of a rolling window, w is a discount factor, and λ1 and λ2 are
nonnegative regularization parameters.

2.6.

Features of PRISM

There are several distinct features of our estimation procedure. First, a rolling window of
length N is employed. This is to address the fact that the parameters in the predictive Eq.
(2.4) can vary with time t. In our case, people’s search pattern and the search engine tend
to evolve over time, and it is quite likely that the same search phrases would contribute
in different ways over time to the response variable. Correspondingly, the coefficients
in (2.4) need to be estimated dynamically each week, and the more recent observations
should be considered more relevant than the distant historical observations for inferring
the predictive equations of current time. The rolling window of observations and the
exponentially decreasing weights are utilized for such purpose. Our use of exponential
10

weighting is related to the weighted least square formulation that is usually referred to
as discounted weighted regression in the econometrics literature (Ameen and Harrison,
1984; Taylor, 2010).
Second, since the number of unknown coefficient in (2.4) tends to be quite large
compared to the number of observations within the rolling window, we applied L1 regularization in our rolling-window estimation (Tibshirani, 1996), which gives robust and
sparse estimate of the coefficients. Up to two L1 penalties are applied: on (α, δ ) and on
β, as they represent two sources of information: information from time series components {ẑt } and {γ̂t }, and information from the exogenous variables {xt }.
Third, PRISM is a semi-parametric method. The predictive Eq. (2.4) is motivated and
derived from our state-space formulation (2.2). However, the estimation is not parametric in that (i) the seasonal and seasonally adjusted components are learned nonparametrically in Stage 1, and (ii) the coefficients in (2.4) are dynamically estimated each
week in Stage 2. Combined together, the two stages of PRISM give us a simple and
robust estimation procedure. This approach is novel and different from the typical approaches for linear state-space models, which often estimate unknown parameters using
specific parametrization and select a model based on information criteria (Hyndman
et al., 2008).

2.7.

Using PRISM without exogenous variables

In the case when exogenous time series {xt } are not available, PRISM estimates yt+l
according to the following linear predictive equation:
K

K

j =1

j =1

ŷt+l = µy + ∑ α j ẑt− j,t + ∑ δj γ̂t− j,t ,

(2.6)

which is a degenerated special case of the predictive Eq. (2.4). Under the same estimation
procedure as in (2.5) except that β and xt are dropped, predictive Eq. (2.6) can be used
to forecast univariate time series with seasonal patterns without exogenous time series.
11

2.8.

Constructing point-wise predictive intervals for PRISM estimate

The semi-parametric nature of PRISM makes it more difficult to construct predictive
intervals on PRISM forecasts, as we cannot rely on parametric specifications, such as
posterior distributions, for predictive interval construction. However, the fact that we are
forecasting time series suggests a (non-parametric) method for us to construct predictive
intervals based on the historical performances of PRISM.
For nowcasting at time t, given the historical data available up to time t − 1, we can
evaluate the root mean square error of nowcasting for the last L time periods as
ˆt=
se

1/2
1 t −1
(ŷτ − yτ )2
,
∑
L τ =t− L

where ŷτ is the real time PRISM estimate for yτ generated at time τ. Under the asˆ t would be an estisumption of local stationarity and normality of the residual, se
mate for the standard error of ŷt . We can thus use it to construct predictive interval
for the current PRISM estimate. An 1 − α point-wise predictive interval is given by
ˆ t ), where zα/2 is the 1 − α/2 quantile of the standard normal
ˆ t , ŷt + zα/2 se
(ŷt − zα/2 se
distribution. Supplementary Material A10 shows that the empirical residuals are approximately normal, supporting our construction. The point-wise predictive intervals
for forecasting into future weeks can be constructed similarly. In practice, we set L = 52,
ˆ t based on the forecasts of the most recent one-year window.
estimating the se

2.9.

Training PRISM for forecasting unemployment initial claims

In our forecasting of unemployment initial claims, we applied a 3-year rolling window
of historical data to estimate the parameters in (2.5), i.e. N = 156 (weeks). The choice of
3-year rolling window is recommended in the literature (D’Amuri and Marcucci, 2017)
as well as supported by our empirical studies (Supplementary A7). In addition, since the
Google Trends data are only available since 2004, with the 3-year rolling window we are
able to test the performance of PRISM in 2007-2009, the entire span of the financial crisis,
12

which serves as an important test of the capability of the various prediction methods. We
took K = 52 (weeks) to employ the most recent 1-year estimated seasonal and seasonally
adjusted components, and p = 25 (Google search terms) according to the list of top 25
nationwide query terms related to “unemployment” in Table A1.
The weekly search volume data from Google Trends are limited up to a 5-year span
per download. The subsequent normalization of the search volumes done by Google
is based on the search query term and the specified date range, which implies that the
absolute values of search volumes are normalized differently between different 5-year
ranges. However, within the same set of 5-year data the relative scale of variables is consistent (as they are normalized by the same constant). Therefore, to avoid the variability
from different normalization across different 5-year spans, and to ensure the coherence
of the model, for each week, we used the same set of 5-year-span data downloaded for
both the training data and the prediction.
For the choice of the discount factor, we took w = 0.985 as the default choice. This
follows the suggestion by Lindoff (1997) that setting the discount factor between 0.95
and 0.995 works in most applications. We further conducted the experiments for w ∈

[0.95, 0.995] (see Supplementary Material A6) and found the performance of PRISM is
quite robust for w ∈ [0.95, 0.995] while w = 0.985 gives optimal accuracy for our insample nowcasting.
For the regularization parameters λ1 and λ2 in (2.5), we used cross-validation to
minimize the mean squared predictive errors (i.e., the average of prediction errors from
each validation set of data). We found empirically that the extra flexibility of having two
separate λ1 and λ2 does not give improvement over fixing λ1 = λ2 . In particular, we
found that for every forecasting horizon l = 0, 1, 2, 3, in the cross-validation process of
setting (λ1 , λ2 ) for separate L1 penalty, over 80% of the weeks showed that the smallest
cross-validation mean squared error when restricting λ1 = λ2 is within 1 standard error
of the global smallest cross-validation mean squared error. For model simplicity, we thus
chose to further restrict λ1 = λ2 when forecasting unemployment initial claims.
13

2.10.

Accuracy metrics

We used root-mean-squared error (RMSE) and mean absolute error (MAE) to evaluate
the performance of different methods. For an estimator {ŷt } and horizon l, the RMSE
and MAE are defined, respectively, as RMSEl (ŷ, y ) = [ (n
and MAEl (ŷ, y ) =

1
( n2 − n1 − l +1)

1

2 − n1 − l +1)

n

∑t=2 n1 +l (ŷt − yt )2 ]1/2

n
∑t=2 n1 +l |ŷt − yt |, where n1 + l and n2 are respectively the

start and end of the forecasting period for each l.

3.

Results

3.1.

Retrospective Forecasting for 2007-2016

We applied PRISM to produce forecasts of weekly unemployment initial claims for the
time period of 2007 to 2016 for four time horizons: real-time, one, two, and three weeks
ahead of the current time. We compared the forecasts to the ground truth — the unemployment initial claims released by the Department of Labor one-week behind real-time
— by measuring the RMSE and MAE.
For comparison, we calculated the RMSE and MAE of four alternative forecasting
methods: (a) Bayesian Structural Time Series (BSTS) (Scott and Varian, 2014); (b) and
(c), two forecasting methods using exponential smoothing: BATS and TBATS (De Livera
et al., 2011); and (d) the naive method, which without any modeling effort simply uses
the last available weekly unemployment initial claims number (which is of the prior
week) as the prediction for the current week, one, two, and three week(s) later. The
naive method serves as a baseline. Both BATS and TBATS are based on innovation
state-space model; BATS is an acronym for key features of the model: Box-Cox transformation, ARMA errors, Trend, and Seasonal components; TBATS extends BATS to
handle complex seasonal patterns with trigonometric representations, and the initial T
connotes “trigonometric”. BSTS only produces nowcasting; it does not produce numbers
for forecasting into future weeks.
14

As PRISM uses both historical unemployment initial claims data and Google search
information, to quantify the contribution of the two resources, we also applied PRISM
but without the Google search information. We denoted this method as “PRISM w/o
xt ”.
For fair comparison, in generating retrospective estimates of unemployment initial
claims, we reran all methods each week using only the information available up to that
week, i.e., we obtained the retrospective estimation as if we had relived the testing period
of 2007 − 2016. The two exponential smoothing methods (b) BATS and (c) TBATS only
use historical initial claims data and do not offer the option of including exogenous
variable in their forecasting, while the method (a) BSTS allows the inclusion of exogenous
variables. Thus, for forecasting at each week t, BSTS takes the historical initial claim data
and Google Trends data as input, whereas BATS and TBATS use historical initial claim
data only. PRISM was applied twice: with and without Google search information. The
results of BSTS, BATS and TBATS were produced by their respective R packages under
their default settings.
Table 1 presents the performance of forecasting (including nowcasting) unemployment initial claims over the entire period of 2007 − 2016 for the four forecasting horizons. The RMSE and MAE numbers reported here are relative to the naive method, i.e.,
the number reported in each cell is the ratio of the error of a given method to that of
the naive method. The absolute error of the naive method is reported in the parentheses. BSTS does not produce numbers for forecasting into future weeks, as its R package
outputs prediction of the target time series only with exogenous variables inputted.
Table 1 reveals the following. First, PRISM uniformly outperforms all the other methods for the entire period of 2007 − 2016 under all forecasting horizons. Second, the
real-time Google Trends data are very helpful for nowcasting, as PRISM and BSTS have
better nowcasting results than the other methods that use only historical initial claim
data. Third, the contribution of contemporaneous Google Trends data becomes less significant in forecasting future weeks, as evidenced by the shrinking performance gap
15

between PRISM and “PRISM w/o xt ” from nowcasting to forecasting. Fourth, among
the three methods that only use historical initial claim data, the predictive method based
on PRISM without Google information outperforms the exponential smoothing methods
BATS and TBATS.
Following the suggestion of a referee, we further compared the performance of
PRISM to the other methods with an additional metric: Cumulative Sum of Squared forecast Error Differences (CSSED) (Welch and Goyal, 2008), which calculates the cumulative
difference in mean-squared error (MSE) between PRISM and the alternative. The CSSED
at time T for an alternative method m is defined as CSSEDm,PRISM = ∑tT=1 (e2t,m − e2t,PRISM ),
where et,m and et,PRISM are the prediction errors at time t for method m and PRISM respectively. The detailed comparison results are given in Supplementary Material A13,
which again shows that the advantage of PRISM over alternatives is consistent over the
whole evaluation period.
real-time

1 week

2 weeks

3 weeks

RMSE
PRISM
PRISM w/o xt
BSTS
BATS
TBATS
naive

0.493
0.647
0.588
1.002
0.711
1 (50551)

0.483
0.532
0.897
0.559
1 (62227)

0.461
0.507
0.848
0.544
1 (69747)

0.470
0.524
0.832
0.528
1 (73527)

MAE
PRISM
PRISM w/o xt
BSTS
BATS
TBATS
naive

0.539
0.659
0.612
0.992
0.750
1 (33637)

0.517
0.559
0.898
0.599
1 (41121)

0.476
0.510
0.825
0.570
1 (47902)

0.460
0.496
0.781
0.525
1 (52794)

Table 1: Performance of different methods over 2007 − 2016 for four forecasting horizons:
real-time, 1 week, 2 weeks and 3 weeks ahead. RMSE and MAE here are relative to the
error of naive method; that is, the number reported is the ratio of the error of a given
method to that of the naive method; the absolute RMSE and MAE of the naive method
are reported in the parentheses. The boldface indicates the best performer for each
forecasting horizon and each accuracy metric.

16

To assess the statistical significance of the improved prediction power of PRISM compared to the alternatives, we conducted Diebold-Mariano test (Diebold and Mariano,
1995), which is a nonparametric test for comparing the prediction accuracy between two
time-series forecasting methods. Table 2 reports the p-values of the Diebold-Mariano test
(the null hypothesis being that PRISM and the alternative method in comparison have
the same prediction accuracy in RMSE). With all the p-values smaller than 2.1%, Table 2
shows that the improved prediction accuracy of PRISM over BSTS, BATS, and TBATS is
statistically significant in all of the forecasting horizons evaluated. Further comparison
with two additional methods is presented in Supplementary Material A12, where PRISM
continues to show significant advantage in prediction accuracy over seasonal AR model
and the method of D’Amuri and Marcucci (2017).
PRISM w/o
BSTS
BATS
TBATS

real-time
7.86 × 10−5
7.14 × 10−3
9.17 × 10−8
5.20 × 10−9

1 week
2.09 × 10−2
3.60 × 10−8
7.24 × 10−3

2 weeks
2.10 × 10−2
1.05 × 10−9
2.59 × 10−3

3 weeks
3.95 × 10−3
1.88 × 10−9
1.86 × 10−2

Table 2: P-values of the Diebold-Mariano test for prediction accuracy comparison between PRISM and the alternatives over 2007–2016 for four forecasting horizons: realtime, 1 week, 2 weeks and 3 weeks ahead. The null hypothesis of the test is that PRISM
and the alternative method in comparison have the same prediction accuracy in RMSE.

Figure 2 shows the RMSE of the yearly nowcasting results of the different methods;
here the RMSE is measured relative to the error of the naive method. It is seen that
PRISM gives consistent relative RMSE throughout the 2007 − 2016 period. It is noteworthy that PRISM outperforms all other methods in 2008 and 2009 when the financial
crisis caused significant instability in the US economy. For predictions into future weeks,
PRISM also gives the leading performance for the 2007 − 2016 period (detailed year-byyear plots for the forecasting performance of the different methods for each forecasting
horizon are provided in Supplementary Material A14).
For a closer look of the performance of different methods, Figure 1 (top) shows how
17

the absolute errors of nowcasting accumulate through 2007 to 2016. The cumulative
absolute error of PRISM rises at the slowest rate among all methods. As shown in Figure 1 (bottom), the 2008 financial crisis caused significantly more unemployment initial
claims. PRISM handles the financial crisis period well, as the accumulation of error is
rather smooth for the financial crisis period. Other methods all accumulate loss in a considerably higher rate during the financial crisis. Furthermore, PRISM handles the strong
seasonality of initial claim data well, since the accumulation of error is smooth within
each year. Among all the methods considered, BATS is bumpy in handling seasonality,
as the accumulation jumps when the initial claim data spikes.
BSTS

PRISM w/o X

TBATS

BATS

0.8
0.0

0.4

RMSE

1.2

PRISM

2007

2008

2009

2010

2011

2012

2013

2014

2015

2016

Year

Figure 2: Yearly nowcasting performance of different methods from 2007 to 2016. RMSE
is measured relative to the error of the naive method; a value above 1 indicates that the
method performs worse than the naive method in that time period.
We further constructed point-wise predictive intervals for the PRISM estimates. Figure 3 shows the point estimates and 95% predictive intervals by PRISM for the nowcasting during 2008 − 2016 in comparison to the true unemployment initial claims officially
revealed a week later (in red). For 2008 − 2016, the actual coverage of the predictive
interval is 96.6%, which is slightly higher than the nominal 95%. For longer forecasting horizons, the predictive intervals by PRISM also give coverage close to the nominal
95%: for the one-week-ahead, two-week-ahead and three-week-ahead forecasts, the ac18

tual coverage levels of the PRISM predictive intervals are, respectively, 93.9%, 95.4% and
94.7% (detailed plots of the PRISM predictive intervals are provided in Supplementary
Material A15).

unemployment initial claims

1000000

750000
band

500000

actual
nowcast

250000
2008

2010

2012

2014

2016

Figure 3: Predictive Interval of PRISM from 2008 to 2016. The shaded area corresponds to
the 95% point-wise predictive interval of PRISM nowcasting. The blue curve is the point
estimate of PRISM nowcasting. The red curve is the true unemployment initial claims.
The actual coverage of the 95% PRISM predictive interval is 96.6% in 2008 − 2016.

3.2.

Out-of-Sample Performance 2017-2019

To further assess PRISM’s performance, we applied PRISM to produce out-of-sample
forecasts of weekly unemployment initial claims for the period of 2017-2019. Note that
the PRISM methodology, including all the model specifications, was frozen at the end of
2016, so this evaluation is completely out of sample.
Table 3 summarizes the prediction errors of PRISM (both with and without Google
data) in both RMSE and MAE in comparison with other benchmark methods. PRISM
again shows consistent advantage over other benchmark methods in out-of-sample predictions. PRISM with Google data is leading across the board (except for the MAE of
3-week-ahead prediction where it virtually ties with PRISM without Google information). Notably, the relative errors compared with the naive method are quite stable over
19

RMSE
PRISM
PRISM w/o xt
BSTS
BATS
TBATS
naive
MAE
PRISM
PRISM w/o xt
BSTS
BATS
TBATS
naive

real-time

1 week

2 weeks

3 weeks

0.497
0.550
0.921
1.064
0.699
1(27941)

0.442
0.454
0.982
0.581
1(34936)

0.376
0.387
0.907
0.512
1(41651)

0.343
0.349
0.832
0.465
1(46749)

0.550
0.617
0.941
1.155
0.694
1(19686)

0.484
0.502
0.972
0.587
1(24630)

0.411
0.422
0.907
0.505
1(29971)

0.368
0.365
0.816
0.435
1(35016)

Table 3: Performance of different methods over 2017 − 2019 for four forecasting horizons:
real-time, 1 week, 2 weeks and 3 weeks ahead. RMSE and MAE here are relative to the
error of naive method; that is, the number reported is the ratio of the error of a given
method to that of the naive method; the absolute RMSE and MAE of the naive method
are reported in the parentheses. The boldface indicates the best performer for each
forecasting horizon and each accuracy metric.

PRISM w/o xt
BSTS
BATS
TBATS

real-time
2.74 × 10−9
2.64 × 10−5
3.80 × 10−7
1.04 × 10−3

1 week
4.26 × 10−5
3.09 × 10−5
2.79 × 10−3

2 weeks
1.25 × 10−5
2.40 × 10−5
7.46 × 10−3

3 weeks
2.70 × 10−2
4.59 × 10−6
2.48 × 10−3

Table 4: P-values of the Diebold-Mariano test for prediction accuracy comparison between PRISM and the alternatives over 2017–2019 for four forecasting horizons: realtime, 1 week, 2 weeks and 3 weeks ahead. The null hypothesis of the test is that PRISM
and the alternative method in comparison have the same prediction accuracy in RMSE.

the years, similar to the results in 2007-2016. Breaking into each year (as shown in Figure
4), PRISM uniformly outperforms other methods in comparison and gives rather stable
error reduction from the naive method over the years, both in- and out-of-sample. This
figure together with Figure 2 shows that PRISM reduced around 50% error from the native method (in RMSE) year across year from 2007 to 2019. The statistical significance of
PRISM’s improved prediction power is also verified by the Diebold-Mariano test in Ta20

BSTS

PRISM w/o X

TBATS

BATS

0.8
0.0

0.4

RMSE

1.2

PRISM

2017

2018

2019

Year

Figure 4: Yearly nowcasting performance of different methods from 2017 to 2019. RMSE
is measured relative to the error of the naive method; a value above 1 indicates that the
method performs worse than the naive method in that time period.
ble 4, where all the p-values are smaller than 3%. The consistent performance of PRISM
both in the retrospective testing of 2007-2016 and the out-of-sample testing of 2017-2019
indicates the robustness and accuracy of PRISM over changes in economic environments
and trends.
We also constructed point-wise predictive intervals based on the out-of-sample nowcasts in 2017-2019 (Figure 5). Compared with the actual unemployment data released
one weak later by the Department of Labor, the intervals by PRISM capture the actual
numbers of unemployment initial claims in 97.1% of the weeks in 2017 − 2019, which is
higher than the nominal 95% level. This again underscores the stability of the PRISM
methodology.

3.3.

Out-of-Sample Performance During COVID-19 Pandemic Period

The global shutdown due to the COVID-19 pandemic has heavily impacted the US economy and job market. In particular, the numbers of unemployment claims in the US have
skyrocketed to record-breaking levels with more than 40 millions people in total filing
for initial claims since the start of the pandemic. This phenomenon has attracted signif21

unemployment initial claims

4e+05

band
3e+05
actual
nowcast
2e+05

2018−01

2018−07

2019−01

2019−07

2020−01

Figure 5: Predictive Interval of PRISM from 2017 to 2019. The shaded area corresponds to
the 95% point-wise predictive interval of PRISM nowcasting. The blue curve is the point
estimate of PRISM nowcasting. The red curve is the true unemployment initial claims.
The actual coverage of the 95% PRISM predictive interval is 97.1% in 2017 − 2019.
icant attention from major news media and the general public (Cohen, 2020a,b; Casselman, 2020). As the weekly unemployment claims remain ”stubbornly high” (Casselman,
2020), concerns for significant layoffs and severe economic downturn persist. Accurate
and reliable forecasting of near-future unemployment claims would thus provide very
valuable insights into the trend of the general economy during such trying times. In light
of this, we further applied PRISM to the out-of-sample data of the COVID-19 pandemic
period to evaluate its performance and robustness to such unusual economic shock.
Table 5 and Figure 6 summarize the performance of PRISM’s real-time nowcasting
of the weekly unemployment claims in comparison with the other methods during the
COVID pandemic period from March 21, 2020 to July 18, 2020. For close examination of
the different methods, we break down the entire period into 3-week windows. During
the first three weeks when the COVID-19 shutdown triggered the sudden and drastic
rise in unemployment claims, both PRISM and BSTS picked up the signal rather quickly
due to the input from the real-time Google Trends data that track the search of unem22

ployment related query terms. Since April, PRISM began to show its advantage over the
other methods as it adapted the predictive model to the “new regime”, leading the chart
in 5 out of 6 evaluation windows. It is worth pointing out that forecasting unemployment initial claims during this period is a very challenging task as the unprecedented
huge jump of unemployment claims drastically altered the pattern in the data (including the time-series pattern): we noted that PRISM is the only method that consistently
outperforms the naive method throughout this COVID-19 period (the time-series based
methods often performed worse than the naive method). This out-of-sample forecasting performance thus indicates the robustness of PRISM to unusual economic shocks
and events, giving us more evidence of the model’s reliability and accuracy. Further
evaluation of PRISM and the benchmarks for longer-horizon predictions is presented in
Supplementary Material A16, where PRISM also shows advantage in near-future predictions.
RMSE
PRISM
BSTS
BATS
TBATS
naive
MAE
PRISM
BSTS
BATS
TBATS
naive

Mar 21-Apr 4

Apr 11-Apr 25

May 2-May 16

May 23-Jun 6

Jun 13-Jun 27

Jul 4-Jul 18

0.684
0.543
1.701
1.862
1 (2362454)

0.257
0.838
1.498
0.432
1 (932295)

0.607
0.910
0.505
0.425
1 (488192)

0.538
0.749
1.104
1.497
1 (232074)

0.574
1.009
2.617
4.496
1 (59761)

0.629
1.327
2.640
1.817
1 (105392)

0.784
0.539
1.952
2.136
1 (1986663)

0.215
0.863
1.514
0.363
1 (898656)

0.462
0.879
0.553
0.453
1 (444600)

0.445
0.700
0.896
1.651
1 (206791)

0.711
1.092
3.236
5.940
1 (44883)

0.488
1.460
2.762
1.818
1 (95054)

Table 5: Performance of PRISM and benchmark methods during the COVID-19 pandemic period for real-time nowcasting. Evaluation period is broken down to 3-week
windows. RMSE and MAE here are relative to the error of naive method; that is, the
number reported is the ratio of the error of a given method to that of the naive method;
the absolute RMSE and MAE of the naive method are reported in the parentheses. The
boldface indicates the best performer for each forecasting horizon and each accuracy
metric.

23

BSTS

TBATS

BATS

2
0

1

RMSE

3

4

PRISM

Mar 21 − Apr 4

Apr 11 − Apr 25

May 2 − May 16

May 23 − Jun 6

Jun 13 − Jun 27

Jul 4 − Jul 18

Time

Figure 6: Nowcasting performance of different methods during the COVID-19 pandemic
period (March 21, 2020 to July 18, 2020). RMSE is measured relative to the error of the
naive method and evaluated in three-week windows; a value above 1 indicates that the
method performs worse than the naive method in that time period.

4.

Discussion

The wide availability of data generated from the Internet offers great potential for predictive analysis and decision making. Our study on using Internet search data to forecast unemployment initial claims illustrates one such potential. The arrival of new
data (sometimes in new forms) requires new methodology to analyze and utilize them.
PRISM is an example where traditional statistical models are brought together with more
recent statistical tools, such as L1 regularization and dynamic training.
In this article we focus on using Internet search data to forecast unemployment initial
claims weeks into the future. We introduced a novel method PRISM for forecasting time
series with strong seasonality. PRISM is semi-parametric and can be generally applied
with or without exogenous variables. PRISM is motivated from a general state-space
formulation that contains a variety of widely used time series models as special cases.
The two stages of PRISM are easy to implement. The numerical evaluation shows that
PRISM outperforms all alternatives in forecasting unemployment initial claim data for
24

the time period of 2007 − 2019. We believe that the accurate and robust forecasts by
PRISM would greatly benefit the public and the private sectors to assess and gauge the
economic trends.
PRISM also demonstrates stable adaptability to unusual economic shocks such as
the 2008-2009 financial crisis and the 2020 COVID-19 pandemic shutdown. The outperformance of PRISM relative to other methods are robust during long periods of economic expansion and during short periods of economic recession. In particular, during
the 2008-2009 financial crisis and the COVID-19 pandemic period, we found that the
real-time data from Google enables PRISM to quickly pick up the signal and the changes
in data patterns and to provide insight on real-time and near-future economic trends.
This gives us confidence that the unemployment forecasts given by PRISM would provide government agencies with much-needed information to react promptly and make
well-informed decisions in the face of future economic and financial shocks.
The predictive power and advantage of PRISM mainly come from the following features: (1) dynamic model training based on a rolling window to account for changes
in people’s search pattern and changes in the relationship between Google search information and the targeted economic activity/index; (2) utilization of L1 penalty to select the most relevant predictors and to filter out noisy and redundant information; (3)
combination of non-parametric seasonality decomposition and penalized regression for
greater flexibility and adaptability; (4) incorporation of real-time Google search information from multiple related query terms to enhance prediction accuracy and robustness.
Although this article focuses on forecasting unemployment initial claims, PRISM can
be generally used to forecast time series with complex seasonal patterns. The semiparametric approach of PRISM covers a wider range of time series models than traditional methods, as PRISM transforms the inference of a complicated class of state-space
models into penalized regression of linear predictive models. Furthermore, dynamically fitting the predictive equations of PRISM addresses the time-varying relationship
between the exogenous variables and the underlying time series. One interesting ques25

tion for future study is to explore if we can extend PRISM to forecasting unemployment
indicators in more specified industries such as construction, manufacturing, transportation, finance, and government or to forecasting other unemployment indicators such as
non-farm payrolls. Another direction for future study is to extend PRISM to predict
unemployment indicators for different ethnic or demographic groups. Furthermore, it
would also be of great future interests to see if PRISM can contribute to forecasting
future breaks and macro-economic cycles.
We conclude this article with a few remarks on the detailed implementation of the
PRISM method. We used real-time Internet search data from Google Trends, which provides publicly available data through subsampling and renormalization: a data set undergoes subsampling (Google draws a small sample from its raw search data for a search
query term) and renormalization (after the sampling, Google normalizes and rounds up
the search volumes so that they become integers between 0 and 100 for each search query
term) when downloaded. Due to the subsampling and renormalization, the search term
volumes are noisy and variable (Yang et al., 2015). The L1 regularization adopted in
PRISM has shown advantage in extracting the signals and reducing redundant information from Google search data (see Supplementary Material A9 and A11). Furthermore,
the dynamic training with rolling window accounts for changes in search engine algorithms, people’s search patterns, economic trends and other patterns that change over
time (Burkom et al., 2007; Ning et al., 2019). This is also evident in Figure A11, where
each of the 25 candidate search terms has distinct patterns coming in and out of the
dynamically fitted model throughout the time. The discount factor adopted also gives
more weights on more recent data to capture more recent economic trends and Google
search changes, which is similar to the data tapering idea proposed by (Dahlhaus, 1988;
Dahlhaus et al., 1997) for improved performance in locally stationary time series. Our
empirical analysis supports the effectiveness of the rolling window and discount factor
(Supplementary Material A6 and A7). Alternative frameworks for inferring time series
models with state-space structures include the dynamic linear model (DLM) (Shumway
26

and Stoffer, 2017).
One limitation with PRISM arises from the data availability from Google Trends.
The public available tool only provides up to 5-year range of weekly search data per
download. Access to data in higher resolution and longer time span requires Google’s
permission to use its nonpublic API. Furthermore, in each downloaded batch, the search
volume data are normalized by Google to the scale from 0 to 100 based on the queried
search term and specific data range of that download. Thus, to avoid the variability
due to the normalization and to ensure the consistency of results, we kept both the
model training and the corresponding prediction within the same (downloaded) set of
5-year data. Furthermore, Google search data may not reflect the entire population
of unemployed people, especially those who would not search online for employment
information. Therefore, what we utilized is essentially the association between the search
volume of related search terms and our target of unemployment claims for PRISM’s
prediction.
The R package PRISM.forecast that implements the PRISM method is available on
CRAN at https://CRAN.R-project.org/package=PRISM.forecast. We also made the
code available at https://github.com/ryanddyi/prism.

Acknowledgment
S.C.K.’s research is supported in part by NSF grant DMS-1810914.

References
Ameen, J., and Harrison, P. (1984), “Discount weighted estimation,” Journal of Forecasting,
3(3), 285–296.
Aoki, M. (1987), State space modeling of time series Springer Science & Business Media.
Banbura, M., Giannone, D., Modugno, M., and Reichlin, L. (2013), “Now-casting and
the real-time data flow,” in Handbook of Economic Forecasting. Vol. 2, eds. G. Elliott, and
A. Timmermann Elsevier, pp. 195–237.

27

Burkom, H. S., Murphy, S. P., and Shmueli, G. (2007), “Automated time series forecasting
for biosurveillance,” Statistics in Medicine, 26(22), 4202–4218.
Casselman, B. (2020), “Rise in Unemployment Claims Signals an Economic Reversal,”
The New York Times, .
URL:
https://www.nytimes.com/2020/07/23/business/economy/unemployment-economycoronavirus.html
Chen, H., Chiang, R. H., and Storey, V. C. (2012), “Business intelligence and analytics:
From big data to big impact.,” MIS Quarterly, 36(4), 1165–1188.
Choi, H., and Varian, H. (2012), “Predicting the present with Google Trends,” Economic
Record, 88(s1), 2–9.
Cleveland, R. B., Cleveland, W. S., McRae, J. E., and Terpenning, I. (1990), “STL: A
seasonal-trend decomposition,” Journal of Official Statistics, 6(1), 3–73.
Cohen, P. (2020a), “Rise in Unemployment Claims Signals an Economic Reversal,” The
New York Times, .
URL: https://www.nytimes.com/2020/05/28/business/economy/coronavirus-unemploymentclaims.html
Cohen, P. (2020b), “‘Still Catching Up’: Jobless Numbers May Not Tell Full Story,” The
New York Times, .
URL: https://www.nytimes.com/2020/05/28/business/economy/coronavirus-unemploymentclaims.html
Dahlhaus, R. (1988), “Small sample effects in time series analysis: A new asymptotic
theory and a new estimate,” The Annals of Statistics, pp. 808–841.
Dahlhaus, R. et al. (1997), “Fitting time series models to nonstationary processes,” The
Annals of Statistics, 25(1), 1–37.
D’Amuri, F., and Marcucci, J. (2017), “The predictive power of Google searches in forecasting US unemployment,” International Journal of Forecasting, 33(4), 801–816.
De Livera, A. M., Hyndman, R. J., and Snyder, R. D. (2011), “Forecasting time series
with complex seasonal patterns using exponential smoothing,” Journal of the American
Statistical Association, 106(496), 1513–1527.
Diebold, F. X., and Mariano, R. S. (1995), “Comparing Predictive Accuracy,” Journal of
Business & Economic Statistics, 13(3), 253–263.
Durbin, J., and Koopman, S. J. (2012), Time series analysis by state space methods Oxford
University Press.
Einav, L., and Levin, J. (2014), “The data revolution and economic analysis,” Innovation
Policy and the Economy, 14(1), 1–24.
Ettredge, M., Gerdes, J., and Karuga, G. (2005), “Using web-based search data to predict
macroeconomic statistics,” Communications of the ACM, 48(11), 87–92.
Gardner Jr, E. S., and McKenzie, E. (1985), “Forecasting trends in time series,” Management Science, 31(10), 1237–1246.
Giannone, D., Reichlin, L., and Small, D. (2008), “Nowcasting: The real-time informational content of macroeconomic data,” Journal of Monetary Economics, 55(4), 665–676.
Ginsberg, J., Mohebbi, M. H., Patel, R. S., Brammer, L., Smolinski, M. S., and Brilliant,
28

L. (2009), “Detecting influenza epidemics using search engine query data,” Nature,
457(7232), 1012–1014.
Goel, S., Hofman, J. M., Lahaie, S., Pennock, D. M., and Watts, D. J. (2010), “Predicting
consumer behavior with Web search,” Proceedings of the National Academy of Sciences,
107(41), 17486–17490.
Gould, P. G., Koehler, A. B., Ord, J. K., Snyder, R. D., Hyndman, R. J., and Vahid-Araghi,
F. (2008), “Forecasting time series with multiple seasonal patterns,” European Journal of
Operational Research, 191(1), 207–222.
Harvey, A. C. (1989), Forecasting, structural time series models and the Kalman filter Cambridge University Press.
Harvey, A., and Koopman, S. J. (1993), “Forecasting hourly electricity demand using
time-varying splines,” Journal of the American Statistical Association, 88(424), 1228–1236.
Holt, C. (1957), Forecasting trends and seasonals by exponentially weighted averages,,
Technical report, Carnegie Institute of Technology.
Hyndman, R. J., Khandakar, Y. et al. (2007), Automatic time series for forecasting: the forecast package for R, number 6/07 Monash University, Department of Econometrics and
Business Statistics.
Hyndman, R., Koehler, A. B., Ord, J. K., and Snyder, R. D. (2008), Forecasting with exponential smoothing: the state space approach Springer Science & Business Media.
Khoury, M. J., and Ioannidis, J. P. (2014), “Big data meets public health,” Science,
346(6213), 1054–1055.
Kim, G.-H., Trimi, S., and Chung, J.-H. (2014), “Big-data applications in the government
sector,” Communications of the ACM, 57(3), 78–85.
Li, X. (2016), “Nowcasting with big data: is Google useful in presence of other information,” Policy Research Working Paper, .
Lindoff, B. (1997), On the optimal choice of the forgetting factor in the recursive least
squares estimator,, Technical report, Lund Institute of Technology.
Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., and Byers, A. H.
(2011), Big data: The next frontier for innovation, competition, and productivity McKinsey
& Company.
McAfee, A., and Brynjolfsson, E. (2012), “Big data: The management revolution,” Harvard Business Review, 90(10), 60–68.
Murdoch, T. B., and Detsky, A. S. (2013), “The inevitable application of big data to health
care,” Journal of the American Medical Association, 309(13), 1351–1352.
Ning, S., Yang, S., and Kou, S. (2019), “Accurate regional influenza epidemics tracking
using Internet search data,” Scientific Reports, 9(1), 5238.
Ord, J. K., Koehler, A. B., and Snyder, R. D. (1997), “Estimation and prediction for a class
of dynamic nonlinear statistical models,” Journal of the American Statistical Association,
92(440), 1621–1629.
Preis, T., Moat, H. S., and Stanley, H. E. (2013), “Quantifying trading behavior in financial
markets using Google Trends,” Scientific Reports, 3, 1684.
29

Risteski, D., and Davcev, D. (2014), Can we use daily Internet search query data to
improve predicting power of EGARCH models for financial time series volatility,, in
Proceedings of the International Conference on Computer Science and Information Systems
(ICSIS’2014), October 17–18, 2014, Dubai (United Arab Emirates).
Scott, S. L., and Varian, H. R. (2013), Bayesian variable selection for nowcasting economic
time series,, Technical report, National Bureau of Economic Research.
Scott, S. L., and Varian, H. R. (2014), “Predicting the present with bayesian structural
time series,” International Journal of Mathematical Modelling and Numerical Optimisation,
5(1-2), 4–23.
Shumway, R. H., and Stoffer, D. S. (2017), Time series analysis and its applications: with R
examples Springer.
Siegel, E. (2013), Predictive analytics: The power to predict who will click, buy, lie, or die John
Wiley & Sons.
Stuart, A., Kendall, M. G. et al. (1963), The advanced theory of statistics Griffin.
Taylor, J. W. (2010), “Exponentially weighted methods for forecasting intraday time series
with multiple seasonal cycles,” International Journal of Forecasting, 26(4), 627–646.
Tibshirani, R. (1996), “Regression shrinkage and selection via the lasso,” Journal of the
Royal Statistical Society. Series B (Methodological), 58(1), 267–288.
Welch, I., and Goyal, A. (2008), “A comprehensive look at the empirical performance of
equity premium prediction,” The Review of Financial Studies, 21(4), 1455–1508.
Winters, P. R. (1960), “Forecasting sales by exponentially weighted moving averages,”
Management Science, 6(3), 324–342.
Wu, L., and Brynjolfsson, E. (2015), “The future of prediction: How Google searches
foreshadow housing prices and sales,” in Economic Analysis of the Digital Economy University of Chicago Press, pp. 89–118.
Yang, S., Kou, S. C., Lu, F., Brownstein, J. S., Brooke, N., and Santillana, M. (2017),
“Advances in using Internet searches to track dengue,” PLoS Computational Biology,
13(7), e1005607.
Yang, S., Santillana, M., and Kou, S. C. (2015), “Accurate estimation of influenza epidemics using Google search data via ARGO,” Proceedings of the National Academy of
Sciences, 112(47), 14473–14478.
Zou, H., and Hastie, T. (2005), “Regularization and variable selection via the elastic net,”
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301–320.

30

Supplementary Material
Details of the methodology, derivation and performance of PRISM are presented as
follows. First, the exact search query terms used in our study, which were identified from
Google Trends, are presented. Second, the general state-space formulation that motivates
PRISM is presented together with a few widely used special cases. Third, the predictive
distribution for PRISM forecasting, together with the mathematical proof, is described
in detail. Fourth, the robustness of PRISM to the choice of the seasonal decomposition
method, to the choice of the discount factor, to the choice of the length of the training
window, and to the choice of the number of observations for the seasonal decomposition
is presented. Fifth, the effect of regularization in PRISM is studied. Sixth, we investigate
the normality of the residuals, which supports PRISM’s predictive interval construction.
Seventh, the fitted coefficients of PRISM are discussed. Eighth, we further compare
PRISM with two additional benchmark methods. Ninth, one additional performance
metric, CSSED, is studied for comparing PRISM with the alternative methods. Tenth,
year-by-year forecasting performance of different methods in longer forecasting horizons
(1-week, 2-week, and 3-week ahead) are reported. Eleventh, the predictive intervals of
PRISM for longer horizon forecasting (1-week, 2-week, and 3-week ahead) are presented.
Twelfth, we report the results of different methods for forecasting longer horizons during
the COVID-19 pandemic period.

A1.

Internet Search Data from Google

The real-time Internet search data we used were from Google Trends (www.google.com/
trends). The search query terms that we used in our study were also identified from
the Google Trends tool. One feature of Google Trends is that, in addition to the time
series of a specific term (or a general topic), it also returns the top query terms that are
most highly correlated with the specific term. In our study, we used a list of top 25
Google search terms that are the most highly correlated with the term “unemployment”.
Table A1 lists these 25 terms, which were generated by Google Trends on January 11,
2018. Figure A1, the upper panel, illustrates the Google Trend time series of several
search query terms in a 5-year span. Comparing these time series to the lower panel
of Figure A1, which shows the unemployment initial claims in the same time period, it
is evident that the former provides noisy signal about the latter. On the Google Trends
site, the weekly data are available for at most a 5-year span in a query, and it would be
automatically transformed to monthly data if one asks for more than 5 years. To model
and forecast the weekly unemployment claims for the entire period of 2007-2016, we
downloaded separate weekly data sets from Google Trends, covering 2004-2008, 20062010, 2008-2012, 2010-2014 and 2012-2016, respectively.
To avoid the variability due to the normalization and to ensure the coherence of the
model, for each week, we kept both the training data and the data used for prediction
within the same 5-year span of data downloaded. For each search term, we downloaded
Google Trends data based on the same 5-year range and the multivariate xt . Then we
1

trained the model and made predictions based on a rolling window of 3 years. So for the
same set of data downloaded with the range of 2004-2008, we are able to make predictions for weeks in 2007-2008; similarly, the data covering 2006-2010 will give predictions
for weeks in 2009-2010, etc. See Figure A2 for an illustration.
Table A1: Top 25 nationwide search query terms associated with the term “unemployment” generated by Google Trends as of January 11, 2018.
unemployment
unemployment office
ny unemployment
unemployment florida
nj unemployment
unemployment insurance
unemployment oregon
unemployment washington
unemployment login

A2.

unemployment benefits
pa unemployment
nys unemployment
unemployment extension
unemployment number
california unemployment
new york unemployment
unemployment wisconsin

unemployment rate
claim unemployment
ohio unemployment
texas unemployment
file unemployment
unemployed
indiana unemployment
unemployment online

Our State-Space Formulation and Special Cases

Our state-space model, Eq. (2.2) in the main text, contains a variety of widely used time
series models, including structural time series models and additive innovation statespace models. Under this general formulation, a specific parametric model can be obtained by specifying the state-space models for zt and γt along with their dependence
structure H.

A2.1.

Special case 1: AR model with seasonal pattern

The following AR model with seasonal pattern is a special case: modeling zt as an
autoregressive process with lag N and assuming a dummy variable formulation with
period S for the seasonal component γt :
y t = z t + γt ,
N

z t = µ z + ∑ α j z t − j + ηt ,

iid

ηt ∼ N (0, ση2 )

j =1

S −1

γt = −

∑ γt − j + ω t ,

(A2.1)

iid

ωt ∼ N (0, σω2 )

j =1

The dummy variable model for the seasonal component implies that sum of the seasonal
components over the S periods, ∑Sj=−01 γt− j , has mean zero and variance σω2 . In the model
(A2.1), each time series block of {z(t− N +1):t }t≥ N and {γ(t−S+2):t }t≥(S−1) evolves as a
Markov Chain. Under our general state-space model, Eq. (2.2) in the main text, if we set
2

100
60
40
7e+05
5e+05
3e+05

unemployment initial claims

9e+05

20

Google Trends

80

file unemployment
unemployment office
california unemployment
unemployment florida

Jan 07
2006

Jul 01
2006

Jan 06
2007

Jul 07
2007

Jan 05
2008

Jul 05
2008

Jan 03
2009

Jul 04
2009

Jan 02
2010

Jul 03
2010

Dec 25
2010

Figure A1: The upper panel shows the Google Trends data of four unemployment related
search queries in 2006-2010. The lower panel shows the weekly unemployment initial
claims data in the same time period.

3

2004

2005

2006

2007

2008

2009

2010

2011

04 - 08 data
Training

Prediction

06 - 10 data
Training

Prediction

……
Figure A2: Illustration of the rolling-window prediction scheme based on 5-year span of
Google Trends data.
ht = (1, zt , zt−1 , . . . , zt− N +1 ) and st = (γt , γt−1 , . . . , γt−S+2 ), then it reduces to the model
(A2.1).

A2.2.

Special case 2: structural time series models

The basic structural model assumes that a univariate time series is the sum of trend,
seasonal and irregular components, each of which follows an independent stochastic
process (Harvey 1989). The model is
iid

et ∼ N (0, σe2 ),

y t = µ t + γt + e t ,

(A2.2)

where µt is the trend component, and γt and et are the seasonal and irregular components, respectively. The trend is often specified by a local level model
iid

ηt ∼ N (0, ση2 ),

µt = µt−1 + δt + ηt ,
iid

ζ t ∼ N (0, σζ2 ),

δt = δt−1 + ζ t ,

(A2.3a)
(A2.3b)

where µt is the level and δt is the slope. ηt and ζ t are assumed mutually independent.
For time series with S periods, the seasonal component can be specified through the

4

seasonal dummy variable model
S −1

γt = −

∑ γt − j + ω t ,

iid

ωt ∼ N (0, σω2 ).

(A2.4)

j =1

which is the same as the seasonal component in the seasonal AR model (A2.1). Alternatively, the seasonal pattern can be modeled by a set of trigonometric terms at seasonal
frequencies λ j = 2πj/S (Harvey 1989):
[S/2]

γt =

∑

γ j,t ,

(A2.5a)

j =1

γ j,t
γ∗j,t

!

cos λ j sin λ j
− sin λ j cos λ j

=

!

γ j,t−1
γ∗j,t−1

!

+

ω j,t
ω ∗j,t

!
,

(A2.5b)

where ω j,t and ω ∗j,t , j = 1, . . . , [S/2], are independent and normally distributed with
common variance σω2 .
Under our general state-space model, Eq. (2.2) of the main text, if we take zt = µt + et
and ht = (µt , δt ), then it specializes to structural time series models. In particular,
for the dummy variable seasonality of (A2.4), st in the general model corresponds to
st = (γt , γt−1 , . . . , γt−S+2 ); and for the trigonometric seasonality of (A2.5), st in the
∗ , . . . , γ∗
general model corresponds to st = (γ1,t , . . . , γ[S/2],t , γ1,t
[S/2],t ).

A2.3.

Special case 3: additive innovations state-space models

An alternative to structural time series models, which have multiple sources of error,
innovation state-space model (Aoki 1987), where the same error term appears in each
equation, is also popular. These innovation state-space models underlie exponential
smoothing methods, which are widely used in time series forecasting and have been
proven optimal under many specifications of the innovation state-space model (Ord et al.
1997; Hyndman et al. 2008). Among exponential smoothing methods, Holt-Winters’
method (Holt 1957; Winters 1960) is developed to capture both trend and seasonality, and
it postulates a model specification similar to the basic structural model (A2.2)- (A2.4). In
particular, Holt-Winters’ additive method is
yt
µt
δt
γt

=
=
=
=

µt−1 + δt−1 + γt−S + et ,
µt−1 + δt−1 + αet ,
δt−1 + βet ,
γt−S + ωet ,

5

(A2.6a)
(A2.6b)
(A2.6c)
(A2.6d)

where the components µt , δt and γt represent level, slope and seasonal components
iid

of time series, and et ∼ N (0, σ2 ) is the only source of error. Since Eq. (A2.6a) can be
rewritten as
y t = µ t + γt + ( 1 − α − ω ) e t ,
we observe that model (A2.6) is special case of our general formulation with zt =
µt + (1 − α − ω )et , ht = (µt , δt ) and st = (γt , γt−1 , . . . , γt−S+1 ) in Eq. (2.2) of the main
text. The Holt-Winters model is among a collection of innovation state-space models
that are summarized in Hyndman et al. (2008) using the triplet (E, T, S), which represents model specification for the three components: error, trend and seasonality. For
instance, (A2.6) is also referred to as local additive seasonal model or ETS(A,A,A), where
A stands for additive. Our general state-space formulation, Eq. (2.2) in the main text,
also incorporates many useful model extensions as special cases, including the damped
trend (Gardner Jr and McKenzie 1985) and multiple seasonal patterns (Gould et al. 2008;
De Livera et al. 2011). For example, the damped trend double seasonal model extends
model (A2.6) to include a factor φ ∈ [0, 1) and a second seasonal component as follows:
(1)

(2)

yt = µt−1 + φδt−1 + γt−S1 + γt−S2 + et ,
µt = µt−1 + φδt−1 + αet ,
δt = φδt−1 + βet ,

(A2.7)

(1)

= γ t − S1 + ω 1 e t ,

(2)

= γ t − S2 + ω 2 e t .

γt

γt

(1)

(1)

Our general model contains this extended model, where zt = µt + (1 − α − ω1 − ω2 )et ,
(1)
(2)
(1)
(1)
(2)
(2)
γt = γt + γt , ht = (µt , δt ) and st = (γt , . . . , γt−S1 +1 , γt , . . . , γt−S2 +1 ).

A2.4.

Motivation of the general formulation

The motivation of our general state-space formulation is to collectively consider all possible models under it and to semi-parametrically obtain the prediction under this large
class of models. In comparison, traditional time series studies often rely on parameter
estimation of specified models such as those highlighted in the previous subsections.
For instance, exponential smoothing is tailored for computing the likelihood and obtaining maximum likelihood estimates of the innovation state-space models. For other
parametric models with multiple sources of error, their likelihood might be evaluated
by the Kalman filter, but the parameter estimation can be difficult in many cases. In
the traditional parametric times series model setting, model selections are often applied
by optimizing certain selection criteria (e.g. AIC or BIC), but when the class of models
under consideration become really large such as Eq. (2.2) of the main text, traditional
model selection methods encounter serious challenges (as they lack scalability) to operate on such a wide range of models. As a consequence, traditional parametric time
6

series models often consider a much smaller collection of models compared to Eq. (2.2)
of the main text. The cost of focusing on a small class of models is that the forecasting
accuracy can substantially suffer as the risk of model misspecification is high.
To relieve these challenges and improve the performance of forecasting, we use our
general state-space formulation to motivate a semi-parametric method for forecasting
time series. We derive and study a linear predictive model that is coherent with all
possible models under Eq. (2.2) of the main text. With forecasting as our main goal,
we essentially transform the question from the inference of a complicated class of statespace models into penalized regression and forecasting based on a linear prediction
formulation.

A3.

Predictive Distributions for Forecasting

Under our general state-space model – Eq. (2.2) and Eq. (2.3) of the main text – given the
historical data {y1:(t−1) } and contemporaneous exogenous time series {xt0 :t }, the predictive distribution for forecasting yt+l (l ≥ 0) at time t would be p(yt+l | y1:(t−1) , xt0 :t ). In
PRISM, we consider the predictive distribution of yt by further conditioning on the latent
seasonal component {γt }:
p(yt+l | y1:(t−1) , γ1:(t−1) , xt0 :t ).

(A3.1)

Note that since zt = yt − γt for all t, z1:(t−1) is known given y1:(t−1) and γ1:(t−1) . The
advantage of working on (A3.1) is that we can establish a universal representation of the
predictive distribution as given by the next proposition.
Proposition 1. Under our model — Eq. (2.2) and (2.3) of the main text — yt+l (l ≥ 0)
conditioning on {z1:(t−1) , γ1:(t−1) , xt0 :t } follows a normal distribution with the conditional mean
E(yt+l | z1:(t−1) , γ1:(t−1) , xt0 :t ) linear in z1:(t−1) , γ1:(t−1) and xt .
As a partial result that leads to Proposition 1, we have, without the exogenous variables, the conditional distribution of yt+l | z1:(t−1) , γ1:(t−1) is normal with mean linear in
z1:(t−1) and γ1:(t−1) .


Based on Proposition 1, we can represent p yt+l | z1:(t−1) , γ1:(t−1) , xt0 :t as
yt+l =

(l )
µt

t −1

+∑

j =1

(l )

(l )

(l )

(l )
α j,t zt− j

t −1

+∑

j =1

(l )
δj,t γt− j

p

+ ∑ β i,t xi,t + et ,
(l )

iid

2
et ∼ N (0, σt,l
),

(A3.2)

i =1

(l )

2 are fixed but unknown constants that are determined
where µt , α j,t , δj,t , β i,t and σt,l
by original parameters θ and the initial values of the state vectors.

7

A4.

Proof of Proposition 1

We first prove the case for l = 0. Let rt = (zt , γt )0 . Since yt = zt + γt for all t, yt |
y1:(t−1) , γ1:(t−1) is equivalent to zt + γt | z1:(t−1) , γ1:(t−1) . By treating rt as a 2-dimensional
observable and αt = (h0t , s0t )0 as the state vector, we can rewrite Eq. (2.2) of the main text
as
rt = Φ 0 αt + ξt
(A4.1)
αt = Λαt−1 + τt
"
#
"
#
w 0
F O
iid
where Φ =
,Λ=
and (ξt0 , τt0 )0 ∼ N (0, H ).
0 v
O P
0
0
0
Denote r1:t = (r1 , . . . , rt ) and α1:t = (α10 , . . . , α0t )0 . According to the property of
Gaussian linear state-space model, r1:t and α1:t jointly follows a multivariate normal
distribution. Therefore, the sub-vector r1:t is also normal, and rt | r1:(t−1) follows a
bivariate normal distribution with mean linear in r1:(t−1) , i.e.
rt | r1:(t−1) ∼ N (Γt r1:(t−1) , Σt ),

(A4.2)

where Γt and Σt are determined by Φ, Λ and H. For any given parameters, the above
distribution can be numerically evaluated through Kalman filter. Here, we focus only on
the general analytical formulation. Following (A4.2) and yt = 10 rt , we have
yt | r1:(t−1) ∼ N (10 Γt r1:(t−1) , 10 Σt 1).
Thus, given r1:(t−1) , or equivalently z1:(t−1) and γ1:(t−1) , yt has a univariate normal distribution with mean linear in z1:(t−1) and γ1:(t−1) .
When taking the exogenous variable xt into account, we have p(yt | r1:(t−1) , xt0 :t ) ∝
p(xt | yt ) p(yt | r1:(t−1) ). Since



1
0 −1
p(xt | yt ) ∝ exp − (xt − µx − yt β ) Q (xt − µx − yt β ) ,
2

(A4.3)

it follows that
p(yt | r1:(t−1) , xt0 :t )


1 0
1
0 −1
−1
0
2
∝ exp − (xt − µx − yt β ) Q (xt − µx − yt β ) − (1 Σt 1) (yt − 1 Γt r1:(t−1) ) .
2
2
Hence, yt | r1:(t−1) , xt0 :t is normal, since the above equation is an exponential function of

8

a quadratic form of yt . By reorganizing the terms, we have




E yt | r1:(t−1) , xt0 :t =



0

1 Σt 1

 −1

0

+β Q

−1

β

 −1 

0

1 Σt 1

 −1

0

0

1 Γt r1:(t−1) + β Q

and




Var yt | r1:(t−1) , xt0 :t =



0

1 Σt 1

 −1

0

+β Q

−1

β

 −1

−1



(xt − µ x )

.

Therefore, yt | z1:(t−1) , γ1:(t−1) , xt0 :t has a normal distribution with mean linear in z1:(t−1) ,
γ1:(t−1) and xt .
Next, we prove the case for l ≥ 1. We consider the following predictive distribution
Z 



p yt+l | xt0 :t , r1:(t−1)
∝
p yt+l , rt | xt0 :t , r1:(t−1) drt
Z


∝
p (yt+l | xt0 :t , r1:t ) p rt | xt0 :t , r1:(t−1) drt .

Since xt0 :t is independent of yt+l conditional on y1:t , p(yt+l | xt0 :t , r1:t ) = p(yt+l | r1:t ).
Similarly, xt0 :(t−1) is independent of rt conditional on r1:(t−1) , which implies p(rt |


xt0 :t , r1:(t−1) ) = p(rt | xt , r1:(t−1) ). Note that p rt | xt , r1:(t−1) ∝ p(xt | rt ) p(rt |
r1:(t−1) ). Thus, we have


p yt+l , rt | xt0 :t , r1:(t−1)





∝ p (yt+l | r1:t ) p rt | xt , r1:(t−1)



∝ p (yt+l | r1:t ) p(xt | rt ) p(rt | r1:(t−1) ).
In the first part of the proof, we have learned that r1:(t+l ) is multivariate normal. Similar
to (A4.2), we can write rt+l | r1:t as
rt+l | r1:t ∼ N (Γt,l r1:t , Σt,l ),

(A4.4)

where Γt,l and Σt,l are determined by Φ, Λ and H. Hence, yt+l | r1:t ∼ N (10 Γt,l r1:t , 10 Σt,l 1).
Combining the above results with (A4.3), we have


2 1
1
p yt+l , rt | xt0 :t , r1:(t−1) ∝ exp − (10 Σt,l 1)−1 yt+l − 10 Γt,l r1:t − (xt − µx − β10 rt )0 Q−1
2
2

1
0
0 −1
(xt − µx − β1 rt ) − (rt − Γt−1,1 r1:(t−1) ) Σt−1,1 (rt − Γt−1,1 r1:(t−1) ) ,
2
(A4.5)




whose right hand side is an exponential function of a quadratic form of yt+l and rt .
9

Hence, yt+l , rt | xt0 :t , r1:(t−1) is multivariate normal. Consequently, the marginal distribution of yt+l | xt0 :t , r1:(t−1) is univariate normal. Moreover, the conditional expectation
is


E yt+l | xt0 :t , r1:(t−1)


= E E (yt+l | r1:t ) | xt0 :t , r1:(t−1)


0
0 0
= E Γt,l (r1:
,
r
)
|
x
,
r
t
:t
1:(t−1)
0
( t −1) t
0

0
0
= Γt,l r1:(t−1) , E(rt | xt , r1:(t−1) ) ,
where E(rt | xt , r1:(t−1) ) is linear in xt and r1:(t−1) . Therefore, yt+l | xt0 :t , r1:(t−1) is
univariate normal with mean linear in xt and r1:(t−1) .

A5.

Robustness to the Choice of Seasonal Decomposition
Method

We compare the performance of PRISM with two seasonal decomposition methods: STL
and the classic additive decomposition. Both methods decompose target time series yt
into the trend component Tt , the seasonal component St and the irregular component
Rt :
yt = Tt + St + Rt .
In the classic additive decomposition, the estimated trend component T̂t is calculated
from the moving average of {yt }. The seasonal component St for a certain week is
assumed to be the same in each period, and is estimated by the average of the detrended
value, yt − T̂t for the specific week. For example, assuming 52 weeks in a year, the
seasonal index for the fifth week is the average of all the detrended fifth week values in
the data.
In contrast, STL relies on a sequence of applications of loess smoother to generate
the seasonal and trend components. The implementation of STL involves two loops. In
the outer loop, robustness weights are assigned to each data point to reduce the effects
of outliers, while the inner loop iteratively updates the trend and seasonal components.
In the inner loop iteration, the seasonal components are updated using detrended time
series similar to classic additive decomposition, and the trend components are calculated
by loess smoothing of the deseasonalized time series.
Both STL and the classic additive seasonal decomposition are options in the R package of PRISM with STL being the default setting. Table A2 describes the performance of
PRISM with the two different methods. The numerical result of PRISM is quite robust
to the choice of the seasonal decomposition method with STL providing slightly better
overall result.
10

Table A2: The performance of PRISM with two different seasonal decomposition methods: STL and additive decomposition over 2007 − 2016. RMSE and MAE are measured
relative to the respective error of the naive method. The boldface highlights the better
performance for each metric and forecasting horizon.
RMSE
additive decomposition
STL decomposition
MAE
additive decomposition
STL decomposition

A6.

real-time

forecast 1 wk

forecast 2 wk

forecast 3 wk

0.496
0.493

0.486
0.483

0.464
0.461

0.467
0.470

0.541
0.539

0.523
0.517

0.479
0.476

0.458
0.460

Effect of the Discount Factor

We tested the effect of the discount factor w. Lindoff (1997) suggested setting w between
0.95 and 0.995 for practical applications. Table A3 shows the performance of PRISM with
different w ∈ [0.95, 1] (note that w = 1 corresponds to no discount). The performance of
PRISM is seen to be quite robust for the different choices of w between [0.95, 0.995]. The
discount factor w is an option in our R package of PRISM, and the default value is set to
be 0.985. We chose w = 0.985 as it gives the optimal real-time prediction accuracy and
close to optimal prediction accuracy in longer forecasting horizons, as reported in Table
A3. We can also see that comparing with no discounting (w = 1), our default choice of
discount factor can provide 1% to 5% more error reduction in terms of the relative errors
to the naive method. This motivates us to incorporate the discount factor in our model.

A7.

The Length of Training Window

In this section we conducted an empirical analysis on the choice of the rolling window
length in training PRISM. With the limited, 5-year availability of weekly Google search
data (more details in Section 2.1 and Supplementary Material A1), we varied the rolling
window lengths between 2 and 4 years to keep the fitting and prediction within the
same set of downloaded data. Table A4 reports the result for various training window
length. We observe that the default 3-year window gives the leading performance. It
appears that the 3-year window provides a good trade-off between the timeliness of
short-term training data and the statistical efficiency from long-term, large-size data.
The 3-year rolling window choice is also consistent with the choice in D’Amuri and
Marcucci (2017). In addition, since Google data only starts in 2004, the choice of 3-year
training window enables us to have forecasts from 2007 onward, which is important for
evaluating the performance of PRISM (and other methods) during the entire period of
the 2008-2009 financial crisis.

11

Table A3: The performance of PRISM for the discount factor w ∈ [0.95, 1] over 2007 −
2016. RMSE and MAE are measured relative to the respective error of the naive method.
The boldface highlights the best performance for each metric and forecasting horizon.
RMSE
w=1
w = 0.995
w = 0.99
w = 0.985
w = 0.98
w = 0.975
w = 0.97
w = 0.965
w = 0.96
w = 0.955
w = 0.95
MAE
w=1
w = 0.995
w = 0.99
w = 0.985
w = 0.98
w = 0.975
w = 0.97
w = 0.965
w = 0.96
w = 0.955
w = 0.95

real-time

forecast 1 wk

forecast 2 wk

forecast 3 wk

0.515
0.512
0.495
0.493
0.506
0.500
0.513
0.516
0.519
0.525
0.540

0.513
0.501
0.491
0.483
0.489
0.485
0.488
0.488
0.492
0.502
0.484

0.470
0.463
0.457
0.461
0.471
0.471
0.475
0.485
0.486
0.502
0.482

0.465
0.462
0.461
0.470
0.472
0.468
0.495
0.479
0.483
0.497
0.491

0.571
0.556
0.545
0.539
0.543
0.540
0.549
0.555
0.552
0.563
0.572

0.568
0.546
0.532
0.517
0.517
0.516
0.519
0.523
0.524
0.533
0.521

0.507
0.487
0.478
0.476
0.484
0.481
0.488
0.495
0.496
0.511
0.496

0.476
0.467
0.461
0.460
0.460
0.461
0.482
0.465
0.468
0.477
0.474

Table A4: The RMSE of PRISM under different rolling windows for training over 2007 −
2016. RMSE is measured relative to the respective error of the naive method. The
boldface highlights the best performance for each forecasting horizon.
PRISM (default 3-year window)
PRISM with 2-year window
PRISM with 4-year window
naive

A8.

real-time
0.493
0.540
0.502
1 (50551)

1 week
0.484
0.512
0.494
1 (62227)

2 weeks
0.461
0.489
0.467
1 (69747)

3 weeks
0.470
0.494
0.476
1 (73527)

Number of Observations for Seasonal Decomposition
in Stage 1 of PRISM

For the seasonal decomposition in Stage 1 of PRISM, we used M = 700 historical initial
claim observations as the default choice. A relatively large number of M (for example,
more than 10 years of data, M ≥ 520) is preferred to ensure stability of the decompo12

sition. We also conducted an experiment on the sensitivity of the choice of M. Table
A5 reports the result, which shows that the performance of PRISM is quite robust to the
choice of M, and that the default choice of M = 700 gives rather favorable performance.
Table A5: The performance of PRISM under different choices of M, the number of historic initial claim observations for seasonal decomposition in Stage 1 over 2007 − 2016.
RMSE is measured relative to the respective error of the naive method. The boldface
highlights the best performance for each forecasting horizon.
PRISM (default M = 700)
PRISM with M = 600
PRISM with M = 800
naive

A9.

real-time
0.539
0.544
0.544
1 (50551)

1 week
0.518
0.521
0.521
1 (62227)

2 weeks
0.477
0.479
0.479
1 (69747)

3 weeks
0.460
0.457
0.464
1 (73527)

Effect of Regularization

We now assess different types of regularization in fitting the coefficients of PRISM, examining the effects of L1 , L2 and the elastic net (Zou and Hastie 2005) penalty. For L2
regularization, we replaced the L1 -norm penalty in (2.5) with L2 norms (i.e., using the
Ridge regression). For the elastic net , the objective function is
1
N

t − l −1

∑

τ =t−l − N

w

t−τ



K

K

p

j =1

j =1

i =1

yτ +l − µy − ∑ α j ẑτ − j,τ − ∑ δj γ̂τ − j,τ − ∑ β i xi,τ

2

+ aλ (kαk1 + kδ k1 + kβ k1 ) + (1 − a)λ (kαk2 + kδ k2 + kβ k2 ) ,
where a is the weight between L1 and L2 penalization. Cross-validation is used for
selecting both tuning parameters λ and a.
The results of using different types of regularization are given in Table A6. We can
see the clear advantage of L1 penalty in error reduction, especially in longer horizon
forecasts. Unlike the L1 regularization, the L2 penalty will not produce zeros in the
fitted coefficients. That is, for those noisy search terms that are not relevant in the
prediction, they will remain in the model with small, non-zero coefficients, which may
lead to more variability in the prediction. In contrast, with the L1 penalty, the noise and
less relevant information from these search terms tend to be eliminated while only the
most predictive search terms are selected to remain in the model. Similarly, although
the elastic net in theory is more flexible with both L1 and L2 penalties, in practice it
is not doing as well as the L1 penalization due to the noisy search terms. The results
here thus show that the noise reduction by L1 penalty is quite effective in improving the
performance.

13

Table A6: The RMSE of PRISM under different regularization over 2007 − 2016. RMSE
is measured relative to the respective error of the naive method. The boldface highlights
the best performance for each forecasting horizon.
PRISM with L2
PRISM with Elastic Net
PRISM
naive

A10.

real-time
0.543
0.497
0.493
1 (50551)

forecast 1 week
0.551
0.490
0.484
1 (62227)

forecast 2 week
0.533
0.472
0.461
1 (69747)

forecast 3 week
0.543
0.483
0.470
1 (73527)

Normality of Residuals
●

●

●

50
● ●

Residuals

0

● ●

●

● ●●

●

●●

●
●●●●
●●●●
●●●●●●●
●●●●●●
●●●●●●●●
●●●●●●
●●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●
●●●●●●●●●●
●
●
●
●●●●
●●●●●
●●●
●●●●
●●●●●

●●

●

●

●

●
●

−50
●

●

−100
−3

−2

−1

0

1

2

3

theoretical

Figure A3: The normal Q-Q plot of fitted residuals by PRISM in 2008 − 2016.
Here we provide empirical evidence on the normality of the residuals for constructing
the predictive intervals. Figure A3 shows the normal Q-Q plot of the fitted residuals by
PRISM from 2008-2016. Except for a few points, the vast majority of the fitted residuals
fall along with the normal distribution, which supports the construction of the predictive
intervals by PRISM in Section 2.8 of the main text.

A11.

Fitted Coefficients of PRISM

We report the dynamically fitted coefficients by PRISM from each week’s prediction in
2008-2016 in Figure A4. Focusing on the coefficients of Google search terms (the bottom
section of the heatmap), we can see the sparsity of coefficients from the L1 penalty. On
14

Negative coefficient

< −10000

Zero coefficient

−5000

0

Positive coefficient

5000

> 10000

(Intercept)
z.lag_52
z.lag_51
z.lag_50
z.lag_49
z.lag_48
z.lag_47
z.lag_46
z.lag_45
z.lag_44
z.lag_43
z.lag_42
z.lag_41
z.lag_40
z.lag_39
z.lag_38
z.lag_37
z.lag_36
z.lag_35
z.lag_34
z.lag_33
z.lag_32
z.lag_31
z.lag_30
z.lag_29
z.lag_28
z.lag_27
z.lag_26
z.lag_25
z.lag_24
z.lag_23
z.lag_22
z.lag_21
z.lag_20
z.lag_19
z.lag_18
z.lag_17
z.lag_16
z.lag_15
z.lag_14
z.lag_13
z.lag_12
z.lag_11
z.lag_10
z.lag_9
z.lag_8
z.lag_7
z.lag_6
z.lag_5
z.lag_4
z.lag_3
z.lag_2
z.lag_1
s.lag_52
s.lag_51
s.lag_50
s.lag_49
s.lag_48
s.lag_47
s.lag_46
s.lag_45
s.lag_44
s.lag_43
s.lag_42
s.lag_41
s.lag_40
s.lag_39
s.lag_38
s.lag_37
s.lag_36
s.lag_35
s.lag_34
s.lag_33
s.lag_32
s.lag_31
s.lag_30
s.lag_29
s.lag_28
s.lag_27
s.lag_26
s.lag_25
s.lag_24
s.lag_23
s.lag_22
s.lag_21
s.lag_20
s.lag_19
s.lag_18
s.lag_17
s.lag_16
s.lag_15
s.lag_14
s.lag_13
s.lag_12
s.lag_11
s.lag_10
s.lag_9
s.lag_8
s.lag_7
s.lag_6
s.lag_5
s.lag_4
s.lag_3
s.lag_2
s.lag_1
california.unemployment
claim.unemployment
file.unemployment
indiana.unemployment
new.york.unemployment
nj.unemployment
nys.unemployment
ny.unemployment
ohio.unemployment
pa.unemployment
texas.unemployment
unemployed
unemployment.benefits
unemployment
unemployment.extension
unemployment.florida
unemployment.insurance
unemployment.login
unemployment.number
unemployment.office
unemployment.online
unemployment.oregon
unemployment.rate
unemployment.washington
unemployment.wisconsin

2008

2010

2012

2014

2016

Figure A4: The heatmap illustrating the dynamically fitted coefficients by PRISM for
each week of forecasting in 2008 − 2016. Red color represents positive coefficients, blue
color represents negative coefficients, and white color represents zero.
average, 6.20 out of the 25 search terms are selected and included in the each week’s
model by PRISM during 2008-2016. Note that all 25 terms have been included at some
15

point. In addition, we can also see different search terms come in and out of the model
over time, indicating the dynamic movement for each term in its contribution to the final
forecasting.

A12.

Comparison with Additional Benchmarks

RMSE
PRISM
Seasonal AR
D’Amuri and Marcucci (2017)
naive
MAE
PRISM
Seasonal AR
D’Amuri and Marcucci (2017)
naive

real-time

1 week

2 weeks

3 weeks

0.493
0.998
1.281
1 (50551)

0.483
0.854
1.089
1 (62227)

0.461
0.816
1.001
1 (69747)

0.470
0.832
0.971
1 (73527)

0.539
1.051
1.074
1 (33637)

0.517
1.083
1.022
1 (41121)

0.476
1.062
0.990
1 (47902)

0.460
1.082
0.915
1 (52794)

Table A7: Performance of PRISM and additional methods over 2007 − 2016 for four
forecasting horizons: real-time, 1 week, 2 weeks and 3 weeks ahead. RMSE and MAE
here are relative to the error of naive method; that is, the number reported is the ratio of
the error of a given method to that of the naive method; the absolute RMSE and MAE
of the naive method are reported in the parentheses. The boldface indicates the best
performer for each forecasting horizon and each accuracy metric.

Seasonal AR
D’Amuri and Marcucci (2017)

real-time
1.11 × 10−11
8.58 × 10−3

1 week
5.43 × 10−13
7.64 × 10−3

2 weeks
2.82 × 10−14
8.24 × 10−6

3 weeks
4.66 × 10−17
4.89 × 10−4

Table A8: P-values of the Diebold-Mariano test for prediction accuracy comparison between PRISM and the alternatives over 2007–2016 for four forecasting horizons: realtime, 1 week, 2 weeks and 3 weeks ahead. The null hypothesis of the test is that PRISM
and the alternative method in comparison have the same prediction accuracy in RMSE.
In this section, we compare PRISM with two additional methods: the seasonal AR
model and an AR model with a single Google search term as the exogenous variable
(D’Amuri and Marcucci 2017). We fitted the seasonal AR with R package forecast
(Hyndman et al. 2007) where the order for AR component is selected by BIC. For the
method of D’Amuri and Marcucci (2017), following the procedure specified in the paper,
16

the Google search volume of “jobs” was included as the exogenous variable. Table A7
summarizes the comparison result. It shows that PRISM leads both methods by a large
margin. To assess the statistical significance of the improved prediction power of PRISM
over the alternative methods, the p-values of the Diebold-Mariano test (Diebold and
Mariano 1995) are reported in Table A8 (the null hypothesis being that PRISM and the
alternative method in comparison have the same prediction accuracy in RMSE). With all
the p-values smaller than 0.008, Table A8 shows that the improved prediction accuracy
of PRISM over the seasonal AR model and the method of D’Amuri and Marcucci (2017)
is statistically significant in all of the forecasting horizons evaluated.

A13.

Comparing PRISM with Benchmarks in CSSED

In this section, we compare PRISM with the benchmark methods according to an additional metric: Cumulative Sum of Squared forecast Error Differences (CSSED) Welch and
Goyal (2008), which was recommended by a referee. The CSSED at time T for benchmark
method m is defined by CSSEDm,PRISM = ∑tT=1 (e2t,m − e2t,PRISM ), where et,m and et,PRISM
are the prediction errors at time t for the benchmark method m and PRISM respectively.
A positive CSSED indicates the benchmark is less accurate than PRISM. Figures A5 - A8
showcase the CSSED between different benchmarks and PRISM over the period of 20072016. Consistent with the RMSE and MAE reported in Table 1 of the main text, PRISM is
the leading method. We also see a major gain in prediction accuracy for PRISM in 2009
during the financial crisis. This also indicates the robustness of PRISM by incorporating
Google search information in forecasting.

17

D&M 2017

CSSED

1.5e+12

BATS
naive
Seasonal AR

1.0e+12

5.0e+11
TBATS
PRISM w/o X

unemployment initial claims

BSTS

0.0e+00

8e+05
6e+05
4e+05
2e+05
2008

2010

2012
Time

2014

2016

Figure A5: (Top) The Cumulative Sum of Squared forecast Error Differences (CSSED) of
nowcasting between different benchmarks and PRISM. “D&M 2017” denotes the method
of D’Amuri and Marcucci (2017). (Bottom) The unemployment initial claims for the same
period of 2007 − 2016.

18

2.0e+12
D&M 2017

naive

1.5e+12

CSSED

BATS

1.0e+12

Seasonal AR

5.0e+11

unemployment initial claims

TBATS
PRISM w/o X

0.0e+00

8e+05
6e+05
4e+05
2e+05
2008

2010

2012
Time

2014

2016

Figure A6: (Top) The Cumulative Sum of Squared forecast Error Differences (CSSED)
of 1-week ahead forecasting between different benchmarks and PRISM. “D&M 2017”
denotes the method of D’Amuri and Marcucci (2017). (Bottom) The unemployment
initial claims for the same period of 2007 − 2016.

19

D&M 2017
naive

2.0e+12

1.5e+12

CSSED

BATS
Seasonal AR

1.0e+12

5.0e+11

unemployment initial claims

TBATS
PRISM w/o X

0.0e+00

8e+05
6e+05
4e+05
2e+05
2008

2010

2012
Time

2014

2016

Figure A7: (Top) The Cumulative Sum of Squared forecast Error Differences (CSSED)
of 2-week ahead forecasting between different benchmarks and PRISM. “D&M 2017”
denotes the method of D’Amuri and Marcucci (2017). (Bottom) The unemployment
initial claims for the same period of 2007 − 2016.

20

naive
D&M 2017

2.0e+12

1.5e+12
CSSED

BATS
Seasonal AR

1.0e+12

5.0e+11

unemployment initial claims

TBATS
PRISM w/o X

0.0e+00

8e+05
6e+05
4e+05
2e+05
2008

2010

2012
Time

2014

2016

Figure A8: (Top) The Cumulative Sum of Squared forecast Error Differences (CSSED)
of 3-week ahead forecasting between different benchmarks and PRISM. “D&M 2017”
denotes the method of D’Amuri and Marcucci (2017). (Bottom) The unemployment
initial claims for the same period of 2007 − 2016.

21

A14.

Year-by-year Forecasting Performance of Different Methods

In this section, we report the bar charts comparing the yearly RMSE of PRISM with
other methods in longer horizon forecasting from 2007 to 2019 in Figures A9-A11. Note
that BSTS is not in the plots as it only provides nowcasting, not forecasting (of future
weeks). Similar to the nowcasting performance in Figures 2 and 4, PRISM gives leading
performance in most of the years for longer horizon forecasting.
PRISM w/o X

TBATS

BATS

0.8
0.0

0.4

RMSE

1.2

PRISM

2007

2008

2009

2010

2011

2012

2013

2014

2015

2016

2017

2018

2019

Year

Figure A9: Year-by-year 1-week ahead forecasting performance of different methods
from 2007 to 2019. RMSE is measured relative to the error of the naive method; a value
above 1 indicates that the method performs worse than the naive method in that time
period.

PRISM w/o X

TBATS

BATS

0.8
0.0

0.4

RMSE

1.2

PRISM

2007

2008

2009

2010

2011

2012

2013

2014

2015

2016

2017

2018

2019

Year

Figure A10: Year-by-year 2-week ahead forecasting performance of different methods
from 2007 to 2019. RMSE is measured relative to the error of the naive method; a value
above 1 indicates that the method performs worse than the naive method in that time
period.

22

PRISM w/o X

TBATS

BATS

0.8
0.0

0.4

RMSE

1.2

PRISM

2007

2008

2009

2010

2011

2012

2013

2014

2015

2016

2017

2018

2019

Year

Figure A11: Year-by-year 3-week ahead forecasting performance of different methods
from 2007 to 2019. RMSE is measured relative to the error of the naive method; a value
above 1 indicates that the method performs worse than the naive method in that time
period.

A15.

Predictive Intervals of PRISM for Longer Horizon
Forecasting

Figures A12-A14 plots the predictive intervals of PRISM in longer horizon predictions
and reports the acutal coverage of the intervals. As the prediction horizon increases, the
uncertainty in point prediction grows, and thereby we see the increase in the width of
predictive intervals. The actual coverage of PRISM’s intervals remains stable and close
to the nominal 95%: the actual coverages for 1-, 2-, 3-week ahead prediction are 93.9%,
95.4%, and 94.7% respectively.

23

unemployment initial claims

750000
band
500000

actual
nowcast

250000

2010

2015

2020

unemployment initial claims

Figure A12: Predictive Interval of PRISM from 2007 to 2019 for 1-week ahead forecasting. The shaded area corresponds to the 95% point-wise predictive interval of PRISM
nowcasting. The blue curve is the point estimate of PRISM nowcasting. The red curve is
the true unemployment initial claims. The actual coverage of the 95% PRISM predictive
interval is 93.9% in 2007 − 2019.

750000
band
500000

actual
nowcast

250000

2010

2015

2020

Figure A13: Predictive Interval of PRISM from 2007 to 2019 for 2-week ahead forecasting.. The shaded area corresponds to the 95% point-wise predictive interval of PRISM
nowcasting. The blue curve is the point estimate of PRISM nowcasting. The red curve is
the true unemployment initial claims. The actual coverage of the 95% PRISM predictive
interval is 95.4% in 2007 − 2019.
24

unemployment initial claims

1000000

750000
band
500000
actual
nowcast

250000

2010

2015

2020

Figure A14: Predictive Interval of PRISM from 2007 to 2019 for 2-week ahead forecasting.. The shaded area corresponds to the 95% point-wise predictive interval of PRISM
nowcasting. The blue curve is the point estimate of PRISM nowcasting. The red curve is
the true unemployment initial claims. The actual coverage of the 95% PRISM predictive
interval is 94.7% in 2007 − 2019.

25

A16.

COVID-19 Period: forecasting longer horizons

Table A9 compares the performance of PRISM and the alternative methods for longerhorizon forecasting (namely, forecasting 1-week, 2-week and 3-week ahead) during the
COVID-19 pandemic period. The evaluation period is from April 11, 2020 to July 18,
2020, the time period where the signal from the unprecedented jump of unemployment
initial claims due to the COVID-19 pandemic trickled in and challenged longer horizon
forecasting. With input from Google search information, PRISM shows clear advantage
over the alternative methods in 1-week ahead and 2-week ahead (i.e., near-future) predictions. As we are looking further into the future, the information from Google search
data becomes less powerful for prediction, and the performance gap between PRISM
and the other methods shrinks. This observation is consistent with what we observed in
Table 1 of the main text. For 3-week ahead forecasting, PRISM is the second best, trailing
TBATS (although TBATS performs quite poorly in nowcasting – often much worse than
the naive method – as shown in Table 5 and Figure 6 of the main text).
RMSE
PRISM
BATS
TBATS
Seasonal AR
D’Amuri and Marcucci (2017)
naive
MAE
PRISM
BATS
TBATS
Seasonal AR
D’Amuri and Marcucci (2017)
naive

1 week

2 weeks

3 weeks

0.489
1.094
0.580
1.994
5.036
1 (873455)

0.530
0.848
0.614
1.522
1.792
1 (1328192)

0.997
1.008
0.908
1.100
1.609
1 (1971750)

0.427
0.990
0.690
1.384
4.027
1 (633992)

0.521
0.733
0.667
1.033
1.457
1 (1003857)

0.917
0.966
0.816
1.046
1.603
1 (1462434)

Table A9: Performance of PRISM and benchmark methods during COVID-19 pandemic
period (April 11, 2020 to July 18, 2020) for three forecasting horizons: 1 week, 2 weeks,
and 3 weeks. RMSE and MAE here are relative to the error of naive method; that is, the
number reported is the ratio of the error of a given method to that of the naive method;
the absolute RMSE and MAE of the naive method are reported in the parentheses. The
boldface indicates the best performer for each forecasting horizon and each accuracy
metric.

26

References
Ameen, J., and Harrison, P. (1984), “Discount weighted estimation,” Journal of Forecasting,
3(3), 285–296.
Aoki, M. (1987), State space modeling of time series Springer Science & Business Media.
Banbura, M., Giannone, D., Modugno, M., and Reichlin, L. (2013), “Now-casting and
the real-time data flow,” in Handbook of Economic Forecasting. Vol. 2, eds. G. Elliott, and
A. Timmermann Elsevier, pp. 195–237.
Burkom, H. S., Murphy, S. P., and Shmueli, G. (2007), “Automated time series forecasting
for biosurveillance,” Statistics in Medicine, 26(22), 4202–4218.
Casselman, B. (2020), “Rise in Unemployment Claims Signals an Economic Reversal,”
The New York Times, .
URL:
https://www.nytimes.com/2020/07/23/business/economy/unemployment-economycoronavirus.html
Chen, H., Chiang, R. H., and Storey, V. C. (2012), “Business intelligence and analytics:
From big data to big impact.,” MIS Quarterly, 36(4), 1165–1188.
Choi, H., and Varian, H. (2012), “Predicting the present with Google Trends,” Economic
Record, 88(s1), 2–9.
Cleveland, R. B., Cleveland, W. S., McRae, J. E., and Terpenning, I. (1990), “STL: A
seasonal-trend decomposition,” Journal of Official Statistics, 6(1), 3–73.
Cohen, P. (2020a), “Rise in Unemployment Claims Signals an Economic Reversal,” The
New York Times, .
URL: https://www.nytimes.com/2020/05/28/business/economy/coronavirus-unemploymentclaims.html
Cohen, P. (2020b), “‘Still Catching Up’: Jobless Numbers May Not Tell Full Story,” The
New York Times, .
URL: https://www.nytimes.com/2020/05/28/business/economy/coronavirus-unemploymentclaims.html
Dahlhaus, R. (1988), “Small sample effects in time series analysis: A new asymptotic
theory and a new estimate,” The Annals of Statistics, pp. 808–841.
Dahlhaus, R. et al. (1997), “Fitting time series models to nonstationary processes,” The
Annals of Statistics, 25(1), 1–37.
D’Amuri, F., and Marcucci, J. (2017), “The predictive power of Google searches in forecasting US unemployment,” International Journal of Forecasting, 33(4), 801–816.
De Livera, A. M., Hyndman, R. J., and Snyder, R. D. (2011), “Forecasting time series
with complex seasonal patterns using exponential smoothing,” Journal of the American
Statistical Association, 106(496), 1513–1527.
Diebold, F. X., and Mariano, R. S. (1995), “Comparing Predictive Accuracy,” Journal of
Business & Economic Statistics, 13(3), 253–263.
Durbin, J., and Koopman, S. J. (2012), Time series analysis by state space methods Oxford
University Press.
27

Einav, L., and Levin, J. (2014), “The data revolution and economic analysis,” Innovation
Policy and the Economy, 14(1), 1–24.
Ettredge, M., Gerdes, J., and Karuga, G. (2005), “Using web-based search data to predict
macroeconomic statistics,” Communications of the ACM, 48(11), 87–92.
Gardner Jr, E. S., and McKenzie, E. (1985), “Forecasting trends in time series,” Management Science, 31(10), 1237–1246.
Giannone, D., Reichlin, L., and Small, D. (2008), “Nowcasting: The real-time informational content of macroeconomic data,” Journal of Monetary Economics, 55(4), 665–676.
Ginsberg, J., Mohebbi, M. H., Patel, R. S., Brammer, L., Smolinski, M. S., and Brilliant,
L. (2009), “Detecting influenza epidemics using search engine query data,” Nature,
457(7232), 1012–1014.
Goel, S., Hofman, J. M., Lahaie, S., Pennock, D. M., and Watts, D. J. (2010), “Predicting
consumer behavior with Web search,” Proceedings of the National Academy of Sciences,
107(41), 17486–17490.
Gould, P. G., Koehler, A. B., Ord, J. K., Snyder, R. D., Hyndman, R. J., and Vahid-Araghi,
F. (2008), “Forecasting time series with multiple seasonal patterns,” European Journal of
Operational Research, 191(1), 207–222.
Harvey, A. C. (1989), Forecasting, structural time series models and the Kalman filter Cambridge University Press.
Harvey, A., and Koopman, S. J. (1993), “Forecasting hourly electricity demand using
time-varying splines,” Journal of the American Statistical Association, 88(424), 1228–1236.
Holt, C. (1957), Forecasting trends and seasonals by exponentially weighted averages,,
Technical report, Carnegie Institute of Technology.
Hyndman, R. J., Khandakar, Y. et al. (2007), Automatic time series for forecasting: the forecast package for R, number 6/07 Monash University, Department of Econometrics and
Business Statistics.
Hyndman, R., Koehler, A. B., Ord, J. K., and Snyder, R. D. (2008), Forecasting with exponential smoothing: the state space approach Springer Science & Business Media.
Khoury, M. J., and Ioannidis, J. P. (2014), “Big data meets public health,” Science,
346(6213), 1054–1055.
Kim, G.-H., Trimi, S., and Chung, J.-H. (2014), “Big-data applications in the government
sector,” Communications of the ACM, 57(3), 78–85.
Li, X. (2016), “Nowcasting with big data: is Google useful in presence of other information,” Policy Research Working Paper, .
Lindoff, B. (1997), On the optimal choice of the forgetting factor in the recursive least
squares estimator,, Technical report, Lund Institute of Technology.
Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., and Byers, A. H.
(2011), Big data: The next frontier for innovation, competition, and productivity McKinsey
& Company.
McAfee, A., and Brynjolfsson, E. (2012), “Big data: The management revolution,” Harvard Business Review, 90(10), 60–68.
28

Murdoch, T. B., and Detsky, A. S. (2013), “The inevitable application of big data to health
care,” Journal of the American Medical Association, 309(13), 1351–1352.
Ning, S., Yang, S., and Kou, S. (2019), “Accurate regional influenza epidemics tracking
using Internet search data,” Scientific Reports, 9(1), 5238.
Ord, J. K., Koehler, A. B., and Snyder, R. D. (1997), “Estimation and prediction for a class
of dynamic nonlinear statistical models,” Journal of the American Statistical Association,
92(440), 1621–1629.
Preis, T., Moat, H. S., and Stanley, H. E. (2013), “Quantifying trading behavior in financial
markets using Google Trends,” Scientific Reports, 3, 1684.
Risteski, D., and Davcev, D. (2014), Can we use daily Internet search query data to
improve predicting power of EGARCH models for financial time series volatility,, in
Proceedings of the International Conference on Computer Science and Information Systems
(ICSIS’2014), October 17–18, 2014, Dubai (United Arab Emirates).
Scott, S. L., and Varian, H. R. (2013), Bayesian variable selection for nowcasting economic
time series,, Technical report, National Bureau of Economic Research.
Scott, S. L., and Varian, H. R. (2014), “Predicting the present with bayesian structural
time series,” International Journal of Mathematical Modelling and Numerical Optimisation,
5(1-2), 4–23.
Shumway, R. H., and Stoffer, D. S. (2017), Time series analysis and its applications: with R
examples Springer.
Siegel, E. (2013), Predictive analytics: The power to predict who will click, buy, lie, or die John
Wiley & Sons.
Stuart, A., Kendall, M. G. et al. (1963), The advanced theory of statistics Griffin.
Taylor, J. W. (2010), “Exponentially weighted methods for forecasting intraday time series
with multiple seasonal cycles,” International Journal of Forecasting, 26(4), 627–646.
Tibshirani, R. (1996), “Regression shrinkage and selection via the lasso,” Journal of the
Royal Statistical Society. Series B (Methodological), 58(1), 267–288.
Welch, I., and Goyal, A. (2008), “A comprehensive look at the empirical performance of
equity premium prediction,” The Review of Financial Studies, 21(4), 1455–1508.
Winters, P. R. (1960), “Forecasting sales by exponentially weighted moving averages,”
Management Science, 6(3), 324–342.
Wu, L., and Brynjolfsson, E. (2015), “The future of prediction: How Google searches
foreshadow housing prices and sales,” in Economic Analysis of the Digital Economy University of Chicago Press, pp. 89–118.
Yang, S., Kou, S. C., Lu, F., Brownstein, J. S., Brooke, N., and Santillana, M. (2017),
“Advances in using Internet searches to track dengue,” PLoS Computational Biology,
13(7), e1005607.
Yang, S., Santillana, M., and Kou, S. C. (2015), “Accurate estimation of influenza epidemics using Google search data via ARGO,” Proceedings of the National Academy of
Sciences, 112(47), 14473–14478.
Zou, H., and Hastie, T. (2005), “Regularization and variable selection via the elastic net,”
29

Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301–320.

30

