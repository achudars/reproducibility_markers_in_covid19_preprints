TweetsCOV19 - A Knowledge Base of Semantically Annotated
Tweets about the COVID-19 Pandemic
Dimitar Dimitrov1 , Erdal Baran1 , Pavlos Fafalios2 , Ran Yu1 , Xiaofei Zhu3 , Matthäus Zloch1 , and
Stefan Dietze1,4,5
1 GESIS

- Leibniz Institute for the Social Sciences, Cologne, Germany
of Computer Science, FORTH-ICS, Heraklion, Greece
3 Chongqing University of Technology, Chongqing, China
4 Heinrich-Heine-University Düsseldorf, Germany
5 L3S Research Center, Hannover, Germany
{dimitar.dimitrov,erdal.baran,ran.yu,matthaeus.zloch,stefan.dietze}@gesis.org
fafalios@ics.forth.gr
zxf@cqut.edu.cn

arXiv:2006.14492v4 [cs.SI] 15 Aug 2020

2 Institute

ABSTRACT
Publicly available social media archives facilitate research in the
social sciences and provide corpora for training and testing a wide
range of machine learning and natural language processing methods. With respect to the recent outbreak of the Coronavirus disease 2019 (COVID-19), online discourse on Twitter reflects public
opinion and perception related to the pandemic itself as well as
mitigating measures and their societal impact. Understanding such
discourse, its evolution, and interdependencies with real-world
events or (mis)information can foster valuable insights. On the
other hand, such corpora are crucial facilitators for computational
methods addressing tasks such as sentiment analysis, event detection, or entity recognition. However, obtaining, archiving, and
semantically annotating large amounts of tweets is costly. In this
paper, we describe TweetsCOV19, a publicly available knowledge
base of currently more than 8 million tweets, spanning October
2019 - April 2020. Metadata about the tweets as well as extracted
entities, hashtags, user mentions, sentiments, and URLs are exposed
using established RDF/S vocabularies, providing an unprecedented
knowledge base for a range of knowledge discovery tasks. Next
to a description of the dataset and its extraction and annotation
process, we present an initial analysis and use cases of the corpus.

CCS CONCEPTS
• Information systems → Digital libraries and archives; Social
networks; Resource Description Framework (RDF); Sentiment analysis.

KEYWORDS
Twitter; RDF; Entity Linking; Sentiment Analysis; Social Media
Archives; COVID-19; Coronavirus
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CIKM ’20, October 19–23, 2020, Virtual Event, Ireland
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6859-9/20/10. . . $15.00
https://doi.org/10.1145/3340531.3412765

ACM Reference Format:
Dimitar Dimitrov1 , Erdal Baran1 , Pavlos Fafalios2 , Ran Yu1 , Xiaofei Zhu3 ,
Matthäus Zloch1 , and Stefan Dietze1, 4, 5 . 2020. TweetsCOV19 - A Knowledge
Base of Semantically Annotated Tweets about the COVID-19 Pandemic.
In Proceedings of the 29th ACM International Conference on Information
and Knowledge Management (CIKM ’20), October 19–23, 2020, Virtual Event,
Ireland. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3340531.
3412765

1

INTRODUCTION

Social web platforms have emerged as a primary forum for online
discourse. Such user-generated content can be seen as a comprehensive documentation of societal discourse of immense historical
value for future generations [5] and as an important resource for
contemporary research. On the one hand, research in the computational social sciences relies on social media data to gain novel
insights, for instance, about the spreading pattern of false claims on
Twitter [34] or prevalent biases observable in online discourse [6].
On the other hand, computational methods at the intersection of
natural language processing (NLP) and machine learning (ML) such
as sentiment analysis [23], classification of news web pages, users
or posts [25], or fake news detection [33] rely on social web corpora
for training and evaluation.
Twitter specifically has been recognized as an important data
source, facilitating research focused on insights or methods related
to online discourse. In particular, during the recent COVID-19 pandemic, online discourse on Twitter has proved crucial to facilitate
an understanding of the impact of the pandemic, implemented measures, societal attitudes and perceptions in this context and, most
importantly, the interdependencies between public opinion and
relevant political actions, policies, media events or scientific discoveries. Recent corpora include a multilingual dataset of COVID-19TweetIDs [7] consisting of more than 129 million tweet IDs, or a
tweet corpus with sentiment annotations released by Lamsal [21].
Next to datasets focused on COVID-19 as a whole, datasets on other
related topics have been created, for instance, about vaccines [24]
covering sentiment-annotated tweets since June 2017 mentioning
vaccine-related keywords.
However, given the legal and computational challenges involved
in processing, reusing and publishing data crawled from Twitter,
existing corpora usually consist of either raw metadata (such as

tweet IDs, usernames, publishing dates) [16] or very limited and
only partially precomputed features, such as georeferences [26]. In
addition, corpora tend to be tailored towards a technical audience,
limiting reuse by non-technical research disciplines lacking the
skills and infrastructure for large-scale data processing.
To facilitate a variety of multi-aspect data exploration scenarios,
in prior work, we introduced TweetsKB1 —a pipeline for semantic
annotation of tweets and a knowledge base of RDF data for more
than 1.5 billion tweets spanning almost five years exposed using
established vocabularies [10]. Whereas entity-centric access and
exploration methods are crucial to facilitate exploration of large
Twitter archives, TweetsKB consistently applies W3C data sharing standards to publish a long-term Twitter archive in the form
of an extensible and easy to access knowledge graph including
metadata about tweets, user mentions, disambiguated entities, and
sentiments. Building on the prior release of TweetsKB in 2018, this
work provides the following contributions:
• Extension of TweetsKB. Building on a continuous Twitter crawl and a parellelised annotation pipeline, we expand
TweetsKB with data from April 2018 up to now, including
additional metadata of about 486 million tweets, adding up
to an unprecedented corpus of more than 63 billion triples
describing more than 2 billion tweets starting from February
2013. To the best of our knowledge, TweetsKB is the largest
publicly available Twitter archive and the only dataset consistently providing a knowledge graph of tweet metadata
and precomputed features about entities and sentiments.
Next to adding additional data based on our enrichment and
data lifting pipeline (cf. Section 2), we also extend both the
applied schema and enrichment pipeline in order to include
additional features (shared URLs).
• Extraction and publishing of TweetsCOV19, a knowledge graph of COVID-19-related online discourse. Taking advantage of TweetsKB and related infrastructure, we
extract TweetsCOV19, a unique corpus of COVID-19-related
online discourse. By applying a well-designed seed list (cf.
Section 3.1), we extract a TweetsKB subset spanning the period October 2019 - April 2020 and apply the same feature
extraction and data publishing methods as for TweetsKB.
This results in a dataset containing more than 270 million
triples describing metadata for about 8.1 million tweets from
3.6 million Twitter users. Data is accessible as downloadable
dumps following the N3 format and can be queried online
through a dedicated, HTTP-accessible SPARQL endpoint.
An easy to process tsv file is provided in addition.
• Initial descriptive data analysis, use cases, tasks and
reuse. Next to providing basic statistics about TweetsKB in
general, we provide initial analysis and exploration of the
TweetsCOV19 data (cf. Section 3.2) in order to facilitate an
understanding and reuse of the dataset. In order to facilitate
and document reuse and impact of the data, we introduce
a number of use cases, discuss prior use (cf. Section 4) of
the data, for instance, to facilitate research in the social sciences, as well as additional machine learning and NLP tasks
facilitated by TweetsCOV19. Among others, these include
1 https://data.gesis.org/tweetskb

the task of predicting tweet virality, posed as a computation
challenge (cf. Section 4.2).
Given the fact that all Twitter corpora are prohibited from republishing actual tweet texts, precomputed features that reflect content
and semantics of individual tweets, such as mentioned entities,
hashtags, or URLs, together with expressed sentiments provide a
unique foundation for studying online discourse and its evolution
over time. To the best of our knowledge, TweetsCOV19 is the only
COVID-19-related dataset available as public knowledge graph of
tweets metadata and semantic annotations following established
vocabularies and Web data sharing standards.

2

CONSTRUCTING A KNOWLEDGE BASE OF
TWITTER DISCOURSE

Whereas the processing of TweetsCOV19, described in Section 3,
builds on TweetsKB, here, we describe the construction process
of TweetsKB as a general, large-scale knowledge base of Twitter
discourse. Note that, next to updating the corpus with crawled
data after the previous release, improvements were made to the
processing pipeline for this release compared to the extraction
process described in [10].
TweetsKB is a public RDF corpus containing a unique collection of more than 2 billion semantically-annotated tweets spanning
more than 7 years (February 2013 - April 2020). Metadata about
the tweets as well as extracted entities, sentiments, hashtags and
user mentions are exposed using established RDF/S vocabularies,
forming a large knowledge graph of tweet-related data and allowing the expression of structured (SPARQL) queries that satisfy
complex/analytical information needs (cf. Section 4). TweetsKB is
generated through the following steps: (i) harvesting, (ii) filtering,
(iii) cleaning, (iv) semantic annotation and metadata extraction, (vi)
data lifting (using a dedicated RDF/S model). Below we describe
these steps, the availability of the extension of the TweetsKB dataset,
and provide some descriptive statistics.
Harvesting, filtering, cleaning. Tweets are continuously harvested through the public Twitter streaming API since January
2013, accumulating more than 9.5 billion tweets up to now (May
2020). While all data is being archived locally and on restricted
servers, TweetsKB is based on the cleaned-up English-language
subset. As part of the filtering step, we eliminate retweets and nonEnglish tweets, reducing the number of tweets to about 2.3 billion
tweets. In addition, we remove spam through a Multinomial Naive
Bayes (MNB) classifier, trained on the HSpam dataset which has
94% precision on spam labels [29] removing an additional 10% of
tweets.
Semantic annotation and metadata extraction. Adhering to
the Twitter license terms, the text of each tweet is not republished
itself but only tweet IDs which may be rehydrated for specific
purposes. In addition, full-text is exploited for extracting and disambiguating mentioned entities (entity linking), as well as for extracting the magnitude of the expressed positive and negative sentiments
(sentiment analysis). Relying on the experimental motivation and
prior work in [10], for entity linking, we exploit Yahoo’s Fast Entity
Linker (FEL) [3]. FEL has shown particularly cost-efficient performance on the task of linking entities from short texts to Wikipedia
and is fast and lightweight, being well-suited to run over billions

Table 1: Descriptive statistics of TweetsKB and TweetsCOV19.
feature
hashtags
mentions
entities
non-neutral sentiment

total
739,642,147
1,072,723,250
2,575,861,358
1,047,840,159

TweetsKB
ratio of tweets with
unique at least one feature
52,244,423
116,499,222
1,919,083
-

of tweets in a distributed manner. We trained the FEL model using
a Wikipedia dump of April 2020 and we set a confidence threshold
of -3 which has been shown empirically to provide annotations of
good quality (favoring precision). Using a current Wikipedia dump
allows linking to COVID-19-related entities (cf. Section 3). We also
store the confidence score of each extracted entity to facilitate data
consumers to set confidence scores which suit their use cases and
requirements when working with our precomputed annotations.
The quality of the entity annotations produced by FEL over tweets
was evaluated in [10], demonstrating high precision (86%) and an
overall satisfactory performance (F1 = 54%).
For sentiment analysis, we used SentiStrength [32], a robust
and efficient tool for sentiment strength detection on social web
data. SentiStrength assigns both a positive and a negative score to
a short text, to account for both types of sentiments that can be
expressed at the same time. The value of a positive (negative) sentiment ranges from +1 (-1) for no positive (no negative) to +5 (-5) for
extremely positive (extremely negative). We provide an evaluation
of the quality of sentiment annotations produced by SentiStrength
over tweets in [10], demonstrating a reasonable performance, in
particular in distinguishing stronger sentiments.
Entity and sentiment annotations are accompanied by the following metadata extracted from the tweets: tweet id, post date, username
(user who posted the tweet), favourite and retweet count (at the time
of fetching the tweet), hashtags (words starting with #), and user
mentions (words starting with @). Starting from April 2018, we also
extract the URLs included in the tweets. For ensuring data privacy,
we anonymize usernames to ensure that tweets for particular users
can be aggregated but users not identified.
Data lifting. We generate RDF triples in the N3 format using the
data model described in [10], which exploits terms from established
vocabularies, in particular SIOC core ontology [4], ONYX [28],
schema.org [13], and Open NEE Model [9]. The selection of vocabularies was based on the following objectives: (i) avoiding schema
violations, (ii) enabling data interoperability through term reuse,
(iii) having dereferenceable URIs, (iv) extensibility. During lifting,
we normalize sentiment scores in the range [0, 1] using the formula: score = (|sentimentV alue | − 1)/4). For this release, we extended the data model described in [10] with one additional property (schema:citation) which refers to a URL mentioned in the tweet.
Given that roughly 21% of tweets contain URLs, providing means
to analyze shared URLs and Pay-Level-Domains (PLDs)2 provides
additional opportunities for a range of research questions and tasks,
for instance, with respect to the spreading of misinformation.

0.19
0.35
0.58
0.54

TweetsCOV19
ratio of tweets with
unique at least one feature

total
3,653,928
5,363,449
11,537,537
4,478,603

566,308
1,251,963
331,307
-

0.30
0.40
0.70
0.55

Descriptive statistics and data availability. TweetsKB currently
contains approximately 62.23 billion triples describing online discourse on Twitter. Table 1 summarizes descriptive statistics of the
dataset (February 2013 - April 2020). About 19% of the tweets contain at least one hashtag and 35% at least one user mention. FEL
extracted at least one entity for 58% of the tweets, while the average
number of entities per tweet is 1.26. About 46% of the tweets have
no sentiment, i.e., the score is zero for both the positive and the
negative sentiment. Finally, we report a statistic calculated only for
the TweetsKB extension starting from April 2018 for which we also
extracted shared URLs, i.e., 21% of the tweets from April 2018 to
April 2020 contain at least one URL.
The full TweetsKB is available as N3 files (split by month) through
the Zenodo data repository (DOI: 10.5281/zenodo.573852),3 under
a Creative Commons Attribution 4.0 license. For demonstration purposes, we have also set up a public SPARQL endpoint, currently
containing a subset of about 5% of the dataset4 . Example queries and
more information are available through TweetsKB’s home page.5
The source code used for triplifying the data is available as open
source on GitHub6 .

3

THE TWEETSCOV19 DATASET

In this section, we describe the extraction of the TweetsCOV19
dataset7 — a subset of TweetKB containing tweets related to COVID19, which captures online discourse about various aspects of the
pandemic and its societal impact. Applications and use of the data
are described in greater detail in Section 4. We discuss the extensibility, sustainability, and maintenance of the dataset in Section 5
before providing a thorough comparison of TweetsCOV19 and related datasets in Section 6.

3.1

Extraction Procedure & Availability

To extract the dataset, we compiled a seed list of 268 COVID-19related keywords8 . The seed list is an extension of the seed list9 of
Chen et al. [7] and allows a broader view on the societal discourse
on COVID-19 in Twitter. We conducted full text filtering on the
cleaned full-text of English tweets (cf. Section 2) and retain all tweets
containing at least one of the keywords in the seed list. We consider
only original tweets and no retweets. Overall, our corpus contains
16,266,285 occurrences of matching seed keywords. We applied the
same process to extract relevant metadata and semantically enrich
3 https://zenodo.org/record/573852

4 https://data.gesis.org/tweetskb/sparql
5 https://data.gesis.org/tweetskb

(Graph IRI: http://data.gesis.org/tweetskb)

6 https://github.com/iosifidisvasileios/AnnotatedTweets2RDF

2

The PLD is a sub-domain of a public top-level domain (like .com), for which users
usually pay for. For example, the PLD for www.example.com would be example.com.

7 https://data.gesis.org/tweetscov19

8 https://data.gesis.org/tweetscov19/keywords.txt

9 https://github.com/echen102/COVID-19-TweetIDs/blob/master/keywords.txt

Table 2: Top five matching keywords, user mentions, hashtags, and pay-level-domains of TweetsCOV19.
keywords
frequency
ppe
3,368,192
coronavirus 2,363,080
covid
2,308,054
corona
1,513,195
covid19
1,498,386

user mentions
frequency
realdonaldtrump
41,839
narendramodi
13,039
pmoindia
12,701
jaketapper
9,836
who
9,776

each tweet as described in Section 2. To simplify analysis of the
posted URLs, we resolved all shortened URLs.
The current state of the full dataset is available in two formats: (i)
as a text file with tabular separated values (tsv) and (ii) as RDF triples
in N3 format (cf. Section 2). The N3 version of the dataset consists
of 274, 451, 101 RDF triples accessible through a dedicated SPARQLendpoint10 and as downloadable dumps11 . The source code used
for triplifying the data is available as open source on GitHub12 . All
data is available under a Creative Commons Attribution 4.0 license.
New data will be incrementally added to the corpus in the future.

3.2

Initial Data Analysis

In this section, we present a preliminary and non-exhaustive analysis of the TweetsCOV19 dataset in order to facilitate an understanding of the data and captured features. The TweetsCOV19 dataset
consists of 8,151,524 original tweets posted by 3,664,518 users captured during October 2019 - April 2020. Table 1 shows descriptive
statistics of TweetsCOV19. Given the applied FEL confidence threshold (-3 cf. Section 2), we note a high percentage of tweets containing
entities (70%). The relatively low number of unique entities, on the
other hand, is most probably due to the topic-specific nature of
the corpus. Comparing the ratio of tweets with at least one feature
across TweetsKB and TweetsCOV19, we observe constantly higher
numbers (at least 5%) for all features, with non-natural sentiment being the sole exception (about 1%). Table 2 shows the top five matching keywords, user mentions, hashtags, and PLDs in TweetsCOV19.
The most frequently matching keyword—ppe—is the acronym for
personal protective equipment such as face masks, eye protection,
and gloves. Politicians, journalists, and health organizations are the
most frequent user mentions, with @realdonaldtrump being by far
the most frequently mentioned Twitter user, while the most used
hashtags are #covid19 and #coronavirus. Apart from URLs to Twitter
and other social media platforms, URLs from PLDs of major news
outlets appear to be most frequently shared.
The TweetsCOV19 dataset contains 2,148,490 URLs from 1,645,394
distinct pay-level-domains. We observe that about 25% of the tweets
in TweetsCOV19 contain at least one URL, compared to 21% in
TweetsKB (cf. Section 2). The higher proportion of URLs seems
intuitive given that for emerging topics such as COVID-19, sharing
informational resources is one of the primary motivations.
Although the TweetsCOV19 dataset contains data from October
2019 to April 2020, our next analysis concentrates on the period
from January to April 2020, where the topic starts dominating
social media. Figure 1 presents a comparison of hashtag popularity
over time for #coronavirus vs. #covid19 and #hydroxychloroquine vs.

hashtags
covid19
coronavirus
covid_19
stayhome
china

frequency
160,585
148.317
27,049
26,542
23,602

PLDs
twitter.com
youtube.com
instagram.com
nytimes.com
theguardian.com

frequency
251,839
99,505
50,846
30,892
26,737

#vaccine. The hashtag #coronavirus is present for the whole period
and shows a small initial peak just before the emergence of #covid19
in the beginning of February 2020. While #vaccine is a topic that has
been receiving attention on social media even before the COVID-19
crisis, #hydroxychloroquine gained first popularity as a possible
drug for treating COVID-19 patients. Nevertheless, mentions of
both terms seem to be strongly correlated. As desired, COVID-19
is present not only with respect to hashtags but also with respect
to the semantic annotations of the data. Table 3 shows the top
five most frequently recognized entities per month from January
to April 2020. The entity Coronavirus_disease_2019 experiences
a drastic boost in March and April 2020 and is overall the most
frequent in TweetsCOV19.
In the context of the pandemic, Figure 2 illustrates the sentiments of tweets containing relevant user mentions and URLs to
news media outlets. For all tweets on a given day, the figure shows
their average positive (red), average negative (blue) sentiment, and
the sum of positive and negative sentiments for a tweet averaged
overall tweet for the day (green). While a systematic detection and
interpretation of events is out of the scope of this paper, the fluctuation of the sentiment towards Donald Trump (cf. Figure 2(a)) may
be better understood in the context of an excerpt from the timeline
of major events as compiled by the Washington Post13 . The timeline highlights Trump’s mention of the COVID-19 outbreak in his
state of the union address (February 4, 2020) and the United States
Department of Health and Human Services (HHS) announcing its
first efforts to rapidly develop a COVID-19 vaccine in cooperation
with pharmaceutical industry representatives (February 18, 2020).
Another view on these events is provided by the sentiment of
tweets containing URLs to Breitbart—a politically far-right-wing
associated news media (cf. Figure 2(c)) and CNN—a left-wing associated media (cf. Figure 2(d)). We observe the strongest fluctuations in
the sentiment, with the biggest divergence of sentiment to tweets
sharing links to Breitbart articles in the week of Trump’s State
of the Union address (February 4, 2020). The strongly diverging
spikes, both positivity and negativity increase at the same time,
suggest a possible controversiality and polarization in Breitbart’s
coverage of topics related to the address. We observe the opposite
converging pattern around February 22, 2020, for the sentiment
of tweets mentioning the World Health Organization (WHO) (cf.
Figure 2(b)). The aspects associated with those sentiments patterns
represent a promising direction for future investigations.

4

USE CASES & IMPACT

This section describes usage and use cases of TweetsCOV19.

10 https://data.gesis.org/tweetscov19/sparql

(Graph IRI: http://data.gesis.org/
tweetscov19)
11 https://zenodo.org/record/3871753
12 https://github.com/iosifidisvasileios/AnnotatedTweets2RDF

13 https://www.washingtonpost.com/politics/2020/04/20/what-trump-did-about-

coronavirus-february

Table 3: Entities over time. The table shows the top five entities (confidence score -2) and their frequency per month in the
TweetsCOV19 dataset since the beginning of 2020. COVID-19* is used as shortcut for the entity Coronavirus_disease_2019.
January 2020
entity
frequency
Wuhan
10,147
Iran
5,905
BTS
5,014
What’s_Happening!!
4,899
Twitter
4,105

February 2020
entity
frequency
Wuhan
10,494
COVID-19*
4,999
BTS
4,513
What’s_Happening!!
3,431
Twitter
3,351
#coronavirus
#covid19

12500
10000

Frequency

Frequency

March 2020
entity
frequency
COVID-19*
178,396
Social_distancing
66,176
Italy
22,164
Wuhan
16,804
India
15,822

7500
5000
2500

300
250
200
150
100
50
0

April 2020
entity
frequency
COVID-19*
200,342
Social_distancing
52,323
India
18,992
Hydroxychloroquine
15,820
Wuhan
14,478

#hydroxychloroquine
#vaccine

0
020 020 020 020 020 020 020 020 020 020 020 020 020 020 020 020 020
2
07 14 2 21 2 28 2 04 2 11 2 18 2 25 2 03 2 10 2 17 2 24 2 31 2 07 2 14 2 21 2 28 2
Jan Jan Jan Jan Feb Feb Feb Feb Mar Mar Mar Mar Mar Apr Apr Apr Apr

020 020 020 020 020 020 020 020 020 020 020 020 020 020 020 020 020
07 2 14 2 21 2 28 2 04 2 11 2 18 2 25 2 03 2 10 2 17 2 24 2 31 2 07 2 14 2 21 2 28 2
Jan Jan Jan Jan Feb Feb Feb Feb Mar Mar Mar Mar Mar Apr Apr Apr Apr

(a) #coronavirus vs. #covid19

(b) #hydroxychloroquine vs. #vaccine

Figure 1: Hashtag usage over time. The figure shows a comparison of hashtag popularity over time for (a) the two most popular
hashtags #coronavirus and #covid19, and for (b) #hydroxychloroquine vs. #vaccine.
@who
4

2

2

0
positive
negative
average

2
4

Sentiment

Sentiment

@realDonaldTrump
4

0
positive
negative
average

2
4

020 020 020 020 020 020 020 020 020 020 020 020 020 020 020 020 020
07 2 14 2 21 2 28 2 04 2 11 2 18 2 25 2 03 2 10 2 17 2 24 2 31 2 07 2 14 2 21 2 28 2
Jan Jan Jan Jan Feb Feb Feb Feb Mar Mar Mar Mar Mar Apr Apr Apr Apr

020 020 020 020 020 020 020 020 020 020 020 020 020 020 020 020 020
07 2 14 2 21 2 28 2 04 2 11 2 18 2 25 2 03 2 10 2 17 2 24 2 31 2 07 2 14 2 21 2 28 2
Jan Jan Jan Jan Feb Feb Feb Feb Mar Mar Mar Mar Mar Apr Apr Apr Apr

(a) Donald Trump

(b) WHO

cnn
4

2

2

0
positive
negative
average

2
4

Sentiment

Sentiment

breitbart
4

0
positive
negative
average

2
4

20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20
7 20 4 20 1 20 8 20 4 20 1 20 8 20 5 20 3 20 0 20 7 20 4 20 1 20 7 20 4 20 1 20 8 20
aJ n 0 Jan 1 Jan 2 Jan 2 Feb 0 Feb 1 Feb 1 Feb 2 Mar 0 Mar 1 Mar 1 Mar 2 Mar 3 Apr 0 Apr 1 Apr 2 Apr 2

20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20
7 20 4 20 1 20 8 20 4 20 1 20 8 20 5 20 3 20 0 20 7 20 4 20 1 20 7 20 4 20 1 20 8 20
aJ n 0 Jan 1 Jan 2 Jan 2 Feb 0 Feb 1 Feb 1 Feb 2 Mar 0 Mar 1 Mar 1 Mar 2 Mar 3 Apr 0 Apr 1 Apr 2 Apr 2

(c) Breitbart

(d) CNN

Figure 2: Sentiment over time. The figure shows the sentiment of tweets mentioning (a) Donald Trump and (b) WHO, and containing URLs to (c) Breitbart—a politically far-right-wing associated news media—and (d) CNN—a left-wing associated media.

4.1

Exploitation Scenarios & Example Queries

Next to downloading the dumps, one can directly explore the full
dataset or develop applications that make use the data through
HTTP requests and the SPARQL endpoint, e.g., for retrieving specific data of interest, or for offering a user friendly interface on
top of the endpoint, e.g., one similar to [12].14 In this section, we
introduce a few basic queries to illustrate the use of the endpoint.
14 Offering

such a user-friendly interface is beyond the scope of this paper but considered future work.

Consider, for instance, that a user wants to investigate online
discussions around the president of the United States in a specific time period, e.g., April 2020. The SPARQL query in Figure 3
retrieves the top five entities co-occurring with the entity Donald Trump in tweets of April 2020 (‘top’ in terms of number of
tweets mentioning the entities). The query returns the following
five entities: China (1,103 tweets), Coronavirus_disease_2019 (1,042
tweets), Hydroxychloroquine (703 tweets), Disinfectant (436 tweets),
President_of_the_United_States (369 tweets). Individual entities of

1 select ?otherEntity (count(?tweet) as ?count) where {
2 ?tweet schema:mentions ?entity ; dc:created ?date
3
FILTER(year(?date) = 2020 and month(?date) = 4)
4 ?entity a nee:Entity ; nee:hasMatchedURI dbr:Donald_Trump .
5 ?tweet schema:mentions ?entity2 .
6 ?entity2 a nee:Entity ; nee:hasMatchedURI ?otherEntity
7
FILTER (?otherEntity != dbr:Donald_Trump)
8 } group by ?otherEntity order by desc(?count) LIMIT 5

.

Figure 3: Top entities co-occurring with the entity Donald_Trump during April 2020.
1 select (day(?date) as ?day) (count(?tweet) as ?count) where {
2 ?tweet schema:mentions ?entity ; dc:created ?date
3
FILTER(year(?date) = 2020 and month(?date) = 4) .
4 ?entity a nee:Entity ; nee:hasMatchedURI dbr:Hydroxychloroquine
5 } group by day(?date) order by day(?date)

.

Figure 4: Number of tweets per day in April 2020 mentioning
the entity Hydroxychloroquine.
1 select ?url (count(?tweet) as ?count) where {
2 ?tweet schema:mentions ?entity ; dc:created ?date
3
FILTER(year(?date)=2020 and month(?date)=4 and (day(?date)=6 ||
4 ?entity a nee:Entity ; nee:hasMatchedURI dbr:Hydroxychloroquine .
5 ?tweet schema:citation ?url
6 } group by ?url order by desc(?count)

day(?date)=7)) .

Figure 5: Top URLs mentioned in tweets of April 6, 2020 together with the entity Hydroxychloroquine.
interest may be explored further. For instance, the SPARQL query
in Figure 4 retrieves the number of tweets per day in April 2020
mentioning the entity Hydroxychloroquine. The results show a very
high increment on April 6 and April 7, 2020 (from around 500 tweets
before April 6 to 2,324 in April 6 and 2,105 in April 7), suggesting
that some significant event related to this entity took place during
this period. To explore this further, the SPARQL query in Figure 5
retrieves the top URLs included in tweets from April 6 and April
7, 2020, that mention the entity Hydroxychloroquine. Headlines of
the top three URLs are: “Detroit rep says hydroxychloroquine, Trump
helped save her life amid COVID-19 fight” 15 (54 tweets), “Trump’s Aggressive Advocacy of Malaria Drug for Treating Coronavirus Divides
Medical Community” 16 (53 tweets), and “Scoop: Inside the epic White
House fight over hydroxychloroquine” 17 (30 tweets). These news
articles provide indicators about the event during April 6 - April 7,
2020, triggering a spike in popularity for Hydroxychloroquine.

4.2

Ground Truth Data for ML and NLP Models

From a methodological perspective, the corpus can serve as training/testing data for building and evaluating ML and NLP models.
TweetsCOV19 can foster NLP research in different ways. For example, for each tweet, the dataset offers posted URLs and can thus
provide ground truth data for building citation discovery models,
i.e., predicting the URLs (PLDs) shared in a tweet. In cases where
the ground truth is not directly provided by TweetsCOV19, the data
can still be useful, e.g., tweets containing user mentions and entities
can be easily identified for further annotation as part of crowd
15 https://www.freep.com/story/news/local/michigan/detroit/2020/04/06/democrat-

karen-whitsett-coronavirus-hydroxychloroquine-trump/2955430001
16 https://www.nytimes.com/2020/04/06/us/politics/coronavirus-trump-malaria-drug.
html
17 https://www.axios.com/coronavirus-hydroxychloroquine-white-house-013062860bbc-4042-9bfe-890413c6220d.html

sourcing-based ground truth creation. In this way, the dataset can
support building NLP models for aspect-based sentiment analysis,
stance detection for political claims or fake news identification.
Current work, for instance, is concerned with computing stances of
tweets towards claims, such as the ones public in ClaimsKG 18 [31]
and explicitly capture stances as metadata.
Furthermore, the dataset is a resource for building and evaluating
machine learning models concerned with human online behavior.
Predicting tweet virality is a concrete example where TweetsCOV19
is currently in use and provides the data for the COVID-19 Retweet
Prediction Challenge19 part of the CIKM 2020 AnalytiCup20 . The
goal of this challenge is to predict COVID-19-related tweets’ virality
in terms of the number of their retweets. Retweeting—re-posting
original content without any change—is a popular function in online social networks and amplifies the spread of original messages.
Understanding retweet behavior is useful and has many practical
applications, e.g. (political) audience design and marketing [18, 30],
tracking (fake) news and misinformation [22, 34], social event detection [14]. In particular, when designing campaigns of high societal impact and relevance, when handling communication through
emergencies such as hurricane warnings [19] and health-related
campaigns about breast cancer screening [8], being able to predict
future popularity of tweets is crucial. TweetsCOV19 can shape the
understanding of such processes through the tweets metadata and
semantic annotations it offers.

4.3

Interdisciplinary Usage & Impact

The TweetsKB and TweetsCOV19 datasets are currently used to
support interdisciplinary research in various fields. TweetsKB is
currently used to shape the understanding of solidarity discourse
in the context of migration, e.g., as part of the SOLDISK project21 .
In addition, ongoing joint work with media and communication
studies researchers22 uses TweetsKB to investigate the societal
impact of the ongoing COVID-19 pandemic and most importantly,
acceptance and trust for mitigating measures, the individual risk
assessment and the impact of specific media events or information
campaigns on related discourse and solidarity within society. In this
context, in particular the impact of misinformation on solidarity
and attitudes is being explored, taking advantage of the provided
metadata together with additional metadata such as shared URLs
and claims conveyed as part of these. Additional use cases are the
joint exploration of means to extract statistically representative
data for federal statistical agencies such as DESTATIS23 as a way to
complement traditional data gathering instruments, such as survey
programmes, which are not well-suited to capture societal discourse
or dynamic interdependencies.
Among the lessons learned so far is that, despite all data preprocessing and enrichment aimed at simplifying reuse and interpretation of the data, data consumers tend to depend on support from
computer and data scientists to handle and analyze the data. While
18 https://data.gesis.org/claimskg

19 http://data.gesis.org/covid19challenge
20 https://cikm2020.org/analyticup

21 https://www.uni-hildesheim.de/soldisk/en/project-description

22 https://www.phil-fak.uni-duesseldorf.de/en/kmw/professur-i-prof-dr-frank-

marcinkowski/research-areas
23 https://www.destatis.de/EN

in some cases, the critical issue is handling data at such a scale,
in other cases, interpreting serialization formats (such as JSON or
N3) or vocabularies poses challenges for users. Additionally, data
quality problems related to the underlying raw data and preprocessed features call for highly collaborative projects where expertise
in data characteristics and computational methods contributes to
addressing higher-level research questions.

5

SUSTAINABILITY, MAINTENANCE &
EXTENSIBILITY

With respect to ensuring long-term sustainability, two aspects are
of crucial importance: (i) maintenance and sustainability of the
corpus and enrichment pipeline, and (ii) maintenance of a user base
and network. In order to ensure long-term sustainability, GESIS
as research data infrastructure organisation exploits its technical
expertise in hosting robust research data services has taken over the
TweetsKB corpus with this recent update and hosts and maintains
both TweetsKB and TweetsCOV19. Maintenance of the corpus will
be facilitated through the continuous process of crawling 1% of
all tweets (running since January 2013) through the public Twitter
API. In order to cater for downtimes and ensure that historic data is
available for all time periods, redundant crawlers have been set up
since March 2019. Storage of raw API output is currently handled
through both, secure local GESIS storage services as well as the
HDFS cluster at L3S Research Center.
The annotation and triplification process (cf. Section 2) will be
periodically repeated in order to incrementally expand the TweetsKB corpus and ensure its currentness, one of the requirements
for many of the envisaged use cases of the dataset. While this will
permanently increase the population of the dataset, the schema
itself is extensible and facilitates the enrichment of tweets with
additional information. For instance, information about the users
involved in particular interactions (retweets, likes) or additional
information about involved entities or references/URLs can be included. Next to facilitating the reuse of TweetsKB itself, we also
publish the source code used to semantically annotate and triplify
the data (cf. Footnote 12), to enable third parties to establish and
share similar corpora, e.g., focused topic-specific Twitter crawls. By
following established W3C principles for data sharing and through
the use of persistent URIs, both the schema as well as the corpus
itself can be extended and linked. TweetsCOV19, in particular, will
be updated continuously with the next release scheduled together
with the submission deadline of the COVID-19 Retweet Prediction
Challenge. This allows us to utilize data from the period since this
release as testing data for challenge participants.
A user base emerged gradually throughout the past years, most
importantly, through enabling non-computer scientists (e.g. social
scientists) to interact and analyze the data (cf. Section 4). Additionally, the corpus will be further advertised through interdisciplinary networks like the Web Science Trust24 . Whereas the use
of Zenodo for depositing the dataset, as well as its registration at
datahub.ckan.io, makes it citable and findable, we are currently
exploring additional means, e.g., GESIS-hosted research data portals and registries to further publish and disseminate the dataset or
particular subsets.
24 http://www.webscience.org

6

RELATED WORK

Several Twitter-related datasets similar to TweetsKB have emerged
to support different fields such as machine learning and natural
language processing. Compared to TweetsKB, some datasets contain
only information filtered from raw Twitter stream data, e.g., to
extract subsets of relevance to particular events25 . Others include
annotations, such as mentioned entities [10], or manually curated
labels, e.g., sentiments26 .
Since the pandemic started around January 2020, several datasets
capturing COVID-19 discussions on Twitter have been released for
academic use, including a new COVID-19-dedicated stream API
by Twitter. The COVID-19 streaming API27 returns tweets filtered
based on 590 COVID-19-related keywords and hashtags (snapshot
of terms on May 13, 2020) in the legacy enriched native response
format28 . We provide a summary of all relevant COVID-19-related
datasets we found by the time of this study (i.e., May 20, 2020)
at the TweetsCOV19 home page (cf. Footnote 7). For the most of
these datasets, tweets are harvested and filtered from the Twitter stream based on mentions of COVID-19-related keywords and
hashtags [2, 16, 17, 24, 24, 26]. The number of keywords and hashtags range from 3 [17] to 800 [26]. Some of datasets further apply
language [1, 11, 21, 27] or geo-location filters [20]. Other datasets
focus on specific languages. For example, ArCOV-19 [15] contains
tweets returned by the Twitter standard search API29 when using COVID-19-related keywords (e.g., corona) as queries and written in Arabic, whereas others include only tweets in Turkish [27].
Most of the datasets are being updated regularly. The number of
tweets contained varies per dataset and range from 747,599 [15]
to over 524 million [26] by the time of this study (i.e., May 20,
2020). The filtering criteria (e.g., keywords and selected user accounts) of all datasets are transparent. All datasets are available
as csv, tsv, json or plain text files for downloading. Each dataset
provides the tweet IDs that can be used to rehydrate tweets, i.e., to
acquire actual tweet content of the tweets. Some datasets also contain further information of tweets such as the publishing time [2],
usernames [11, 16, 26, 27], geo-location [20, 24, 26] and retweet
information [27]. A few datasets offer automatic annotations such
as frequent used terms [2], sentiment scores per tweet [21], geolocation inferred from tweets [16] or places mentioned [24, 26].
The starting date of data collection varies, with the earliest available dataset providing data starting from January 1, 2020 [1]. The
Vaccine Sentiment Tracking dataset [24] which is intended for sentiment analysis on vaccine-related topics even dates back to June
29, 2017.
TweetsCOV19 differs from existing datasets as: (i) it is extracted
from a permanent crawl (TweetsKB) spanning more than seven
years – facilitating to trace keywords and topics over extended
periods of time, (ii) it has rich semantic annotations – entities,
sentiment scores, and URLs mentioned in tweets, (iii) the data is
published following FAIR/W3C standards and can be accessed in
25 https://digital.library.unt.edu/ark:/67531/metadc1259406
26 https://data.world/crowdflower/weather-sentiment

27 https://developer.twitter.com/en/docs/labs/covid19-stream

28 https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-

object
29 https://developer.twitter.com/en/docs/tweets/search/api-reference/get-searchtweets

various ways – downloadable data dump as tab separated text files,
RDF triples in N3 format, and a live SPARQL-endpoint. In particular,
given that legal constraints prevent the republication of actual tweet
text, precomputed features that reflect the semantics of tweets are
both a distinctive feature of our dataset and a crucial requirement
for efficiently analyzing online discourse.

7

CONCLUSIONS

As part of this work, we have performed a significant update of
the TweetsKB dataset and pipeline. The most important part of the
pipeline update is the usage of the April 2020 Wikipedia dump to
perform entity linking, i.e., to enable linking to entities related
to the ongoing COVID-19 pandemic. We have also introduced
TweetsCOV19, a knowledge graph of Twitter discourse on COVID19 and its societal impact. The corpus facilitates the exploration
and analysis of online discourse even without costly feature computation or rehydration of tweets. Additionally, we have introduced
several use cases from various disciplines, currently exploiting the
corpus to derive insights and evaluate computational methods for
various tasks.
Future work will take advantage of the extensible knowledge
graph nature of the corpus to incrementally add further contextual
information. TweetsKB and TweetsCOV19 can be extended by computing stances towards claims taking advantage of ClaimsKG [31],
and by classifying tweets or users based on their shared URLs and
claims allowing to detect misinformation.

ACKNOWLEDGMENTS
We want to thank our colleagues at L3S Research Center, Hanover,
Germany, and Humboldt University, Berlin, Germany, who initialized and are currently running the long-term Twitter crawl underlying TweetsKB and TweetsCOV19. We also thank the reviewers
for their valuable comments and suggestions.

REFERENCES
[1] Sarah Alqurashi, Ahmad Alhindi, and Eisa Alanazi. 2020. Large arabic twitter
dataset on covid-19. arXivpreprintarXiv:2004.04315
[2] Juan M. Banda, Ramya Tekumalla, Guanyu Wang, Jingyuan Yu, Tuo Liu, Yuning
Ding, Katya Artemova, Elena Tutubalina, and Gerardo Chowell. 2020. A largescale COVID-19 Twitter chatter dataset for open scientific research - an international
collaboration. https://doi.org/10.5281/zenodo.3831406
[3] Roi Blanco, Giuseppe Ottaviano, and Edgar Meij. 2015. Fast and space-efficient
entity linking for queries. In Eighth International Conference on Web Search and
Data Mining. ACM, 179–188.
[4] John G Breslin, Stefan Decker, Andreas Harth, and Uldis Bojars. 2006. SIOC: an
approach to connect web-based communities. International Journal of Web Based
Communities 2, 2 (2006).
[5] Axel Bruns and Katrin Weller. 2016. Twitter as a first draft of the present: and the
challenges of preserving it for the future. In 8th ACM Conference on Web Science.
ACM, 183âĂŞ189.
[6] Abhijnan Chakraborty, Johnnatan Messias, Fabricio Benevenuto, Saptarshi Ghosh,
Niloy Ganguly, and Krishna Gummadi. 2017. Who Makes Trends? Understanding
Demographic Biases in Crowdsourced Recommendations.
[7] Emily Chen, Kristina Lerman, and Emilio Ferrara. 2020. Tracking Social Media
Discourse About the COVID-19 Pandemic: Development of a Public Coronavirus
Twitter Data Set. JMIR Public Health Surveill 6, 2 (2020). https://doi.org/10.2196/
19273
[8] Jae Eun Chung. 2017. Retweeting in health promotion: Analysis of tweets about
Breast Cancer Awareness Month. Computers in Human Behavior 74 (2017), 112–
119.
[9] Pavlos Fafalios, Manolis Baritakis, and Yannis Tzitzikas. 2015. Exploiting linked
data for open and configurable named entity extraction. International Journal on

Artificial Intelligence Tools 24, 2 (2015), 1540012.
[10] Pavlos Fafalios, Vasileios Iosifidis, Eirini Ntoutsi, and Stefan Dietze. 2018. Tweetskb: A public and large-scale rdf corpus of annotated tweets. In European Semantic
Web Conference. Springer, 177–190.
[11] Zhiwei Gao, Shuntaro Yada, Shoko Wakamiya, and Eiji Aramaki. 2020.
NAIST COVID: Multilingual COVID-19 Twitter and Weibo Dataset. (2020).
arXiv:2004.08145
[12] Malo Gasquet, Darlene Brechtel, Matthaus Zloch, Andon Tchechmedjiev, Katarina
Boland, Pavlos Fafalios, Stefan Dietze, and Konstantin Todorov. 2019. Exploring
Fact-checked Claims and their Descriptive Statistics. In ISWC 2019 Satellite Tracks18th International Semantic Web Conference.
[13] Ramanathan V Guha, Dan Brickley, and Steve Macbeth. 2016. Schema.org: evolution of structured data on the web. Commun. ACM 59, 2 (2016), 44–51.
[14] Manish Gupta, Jing Gao, ChengXiang Zhai, and Jiawei Han. 2012. Predicting
future popularity trend of events in microblogging platforms. Proceedings of the
American Society for Information Science and Technology 49, 1 (2012), 1–10.
[15] Fatima Haouari, Maram Hasanain, Reem Suwaileh, and Tamer Elsayed. 2020.
ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation Networks. arXiv preprint arXiv:2004.05861 (2020).
[16] Xiaolei Huang, Amelia Jamison, David Broniatowski, Sandra Quinn, and Mark
Dredze. 2020. Coronavirus Twitter Data: A collection of COVID-19 tweets with
automated annotations. https://doi.org/10.5281/zenodo.3735015
[17] Daniel Kerchner and Laura Wrubel. 2020. Coronavirus Tweet Ids. https://doi.org/
10.7910/DVN/LW0BTB
[18] Eunice Kim, Yongjun Sung, and Hamsu Kang. 2014. Brand followersâĂŹ retweeting behavior on Twitter: How brand relationships influence brand electronic
word-of-mouth. Computers in Human Behavior 37 (2014), 18–25.
[19] Marina Kogan, Leysia Palen, and Kenneth M Anderson. 2015. Think local, retweet
global: Retweeting by the geographically-vulnerable during Hurricane Sandy. In
18th Conference on Computer Supported Cooperative Work & Social Computing.
ACM, 981–993.
[20] Rabindra Lamsal. 2020. Coronavirus (COVID-19) Geo-tagged Tweets Dataset.
https://doi.org/10.21227/fpsb-jz61
[21] Rabindra Lamsal. 2020. Coronavirus (COVID-19) Tweets Dataset. https://doi.org/
10.21227/781w-ef42
[22] Cristian Lumezanu, Nick Feamster, and Hans Klein. 2012. #bias: Measuring the
Tweeting Behavior of Propagandists.
[23] Abhilash Mittal and Sanjay Patidar. 2019. Sentiment Analysis on Twitter Data:
A Survey. In 7th International Conference on Computer and Communications
Management. ACM, 91âĂŞ95.
[24] Martin M Müller and Marcel Salathé. 2019. Crowdbreaks: Tracking health trends
using public social media data and crowdsourcing. Frontiers in public health 7
(2019).
[25] Kashyap Popat, Subhabrata Mukherjee, Jannik Strötgen, and Gerhard Weikum.
2017. Where the truth lies: Explaining the credibility of emerging claims on
the web and social media. In 26th International Conference on World Wide Web
Companion. International World Wide Web Conferences Steering Committee,
1003–1012.
[26] Umair Qazi, Muhammad Imran, and Ferda Ofli. 2020. GeoCoV19: A Dataset of
Hundreds of Millions of Multilingual COVID-19 Tweets with Location Information. ACM SIGSPATIAL Special 12, 1 (2020).
[27] Ibrahim Sabuncu and Zyenep Yurek. 2020. Corona Virus (COVID-19) Turkish
Tweets Dataset. https://doi.org/10.21227/0wf0-0792
[28] J Fernando Sánchez-Rada and Carlos A Iglesias. 2016. Onyx: A linked data
approach to emotion representation. Information Processing & Management 52, 1
(2016), 99–114.
[29] Surendra Sedhai and Aixin Sun. 2015. Hspam14: A collection of 14 million tweets
for hashtag-oriented spam research. In 38th International Conference on Research
and Development in Information Retrieval. ACM, 223–232.
[30] Stefan Stieglitz and Linh Dang-Xuan. 2012. Political communication and influence
through microblogging–An empirical analysis of sentiment in Twitter messages
and retweet behavior. In 45th Hawaii International Conference on System Sciences.
IEEE, 3500–3509.
[31] A. Tchechmedjiev, P. Fafalios, K. Boland, M. Gasquet, M. Zloch, B. Zapilko, S.
Dietze, and K. Todorov. 2019. ClaimsKG: A Live Knowledge Graph of FactChecked Claims. In 18th International Semantic Web Conference. Springer, 309–
324.
[32] Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. 2012. Sentiment strength
detection for the social web. Journal of the American Society for Information
Science and Technology 63, 1 (2012), 163–173.
[33] Sebastian Tschiatschek, Adish Singla, Manuel Gomez Rodriguez, Arpit Merchant,
and Andreas Krause. 2018. Fake News Detection in Social Networks via Crowd
Signals. In Companion Proceedings of the The Web Conference 2018. International
World Wide Web Conferences Steering Committee, 517âĂŞ524.
[34] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false
news online. Science 359, 6380 (2018), 1146–1151.

