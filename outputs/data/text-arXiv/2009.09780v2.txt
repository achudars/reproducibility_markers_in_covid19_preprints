I MPACT OF LUNG SEGMENTATION ON THE DIAGNOSIS AND
EXPLANATION OF COVID-19 IN CHEST X- RAY IMAGES
A P REPRINT

arXiv:2009.09780v2 [eess.IV] 30 Jan 2021

Lucas Teixeira
Universidade Estadual de Maringá
Luiz S. Oliveira
Universidade Federal do Paraná

Rodolfo M. Pereira
Instituto Federal do Paraná

Diego Bertolini
Universidade Tecnológica Federal do Paraná

Loris Nanni
Università Degli Studi di Padova

George D. C. Cavalcanti
Universidade Federal de Pernambuco

Yandre M. G. Costa
Universidade Estadual de Maringá

February 2, 2021

A BSTRACT
The COVID-19 pandemic is undoubtedly one of the biggest public health crises our society has ever
faced in recent history. One of the main complications caused by COVID-19 is pneumonia, which is
diagnosed using imaging exams, such as chest X-ray (CXR) and computed tomography (CT) scan.
The CT scan is more precise than the CXR. However, CXR is suitable in particular situations because
it is cheaper, faster, more widespread, and exposes the patient to less radiation. This study aims to
demonstrate the impact of lung segmentation in COVID-19 identification using CXR images and
evaluate which contents of the image decisively contribute to its identification. We performed the
lung segmentation using a U-Net CNN architecture, and the classification using three well-known
CNN architectures: VGG, ResNet, and Inception. To estimate the impact of lung segmentation, we
applied some Explainable Artificial Intelligence (XAI) techniques, specifically LIME and Grad-CAM.
To empirically evaluate our approach, we composed a database with three classes: lung opacity
(pneumonia), COVID-19, and normal. The segmentation achieved a Jaccard distance of 0.034 and
a Dice coefficient of 0.982. The classification using segmented lung achieved an F1-Score of 0.88
for the multi-class setup and 0.83 for COVID-19 identification. Further testing and XAI techniques
suggest that segmented CXR images represent a much more realistic and less biased performance.
To the best of our knowledge, no other work tried to estimate the impact of lung segmentation in
COVID-19 identification using comprehensive XAI techniques.
Keywords COVID-19 · chest X-ray · semantic segmentation · explainable artificial intelligence

1

Introduction

The Coronavirus disease 2019 (COVID-19) pandemic, caused by the virus named Severe Acute Respiratory Syndrome
Coronavirus 2 (SARS-CoV-2), has become the most significant public health crisis our society has faced recently 1 .
COVID-19 affects mainly the respiratory system and, in extreme cases, causes a massive inflammatory response that
reduces the total lung capacity [1]. COVID-19 high transmissibility, lack of general population immunization, and high
incubation period [2] makes it a dangerous and lethal disease. In these circumstances, artificial intelligence (AI) based
solutions are being used in various contexts, from diagnostic support to vaccine development [3].
The standard imaging tests for pneumonia, and consequently COVID-19, are chest X-ray (CXR) and computed
tomography or computerized X-ray imaging (CT) scan. The CT scan is the gold standard for lung disease diagnosis
1

https://covid19.who.int/

A PREPRINT - F EBRUARY 2, 2021

since it generates very detailed images. However, CXR is still very useful in particular scenarios, since they are
cheaper, generate the resulting images faster, expose the patient to much less radiation, and it is more widespread in the
emergency care units [4].
After the COVID-19 outbreak, several studies were proposed to investigate its diagnostic based on the use of images
taken from the lungs [5, 6, 7, 8]. Despite the impressive advances, there is a lack of more critical analysis regarding the
content captured in those images that contribute to consistent results [9, 10, 11].
Our main objective is to evaluate the impact of lung segmentation in identifying pneumonia caused by different
microorganisms using CXR images obtained from various sources (i.e., Cohen, RSNA pneumonia detection challenge,
among others). We have primarily focused on CXR images due to their smaller cost and high availability in the
emergency care units, especially those located in less economically developed regions. Moreover, we emphasize
COVID-19, aiming to provide solutions that can be useful in the current pandemic context. To support that objective,
we used an U-Net Convolutional Neural Network (CNN) for lung segmentation, and three popular CNN models for
COVID-19 identification: VGG16 [12], ResNet50V2 [13] and InceptionV3 [14].
We first improved our previously created COVID-19 database (i.e., RYDLS-20 [5]), now called RYDLS-20-v2, adding
more image sources. Then, we set up the problem as a multi-class classification problem with three classes: lung opacity,
COVID-19, and normal lungs (i.e., no-pneumonia), in which lung opacity means pneumonia caused by any previously
known pathogen. To segment lung images, we applied a deep learning approach using a U-Net CNN architecture [15].
Over the last few years, the area known as Explainable Artificial Intelligence (XAI) has attracted many researchers
in the artificial intelligence (AI) field. The main interest of XAI is to research and develop approaches to explain the
individual predictions of modern machine learning (ML) based solutions. In medical applications based on images, we
understand that a proper explanation regarding the obtained decision is fundamental. In an ideal scenario, the decision
support system should be able to suggest the diagnosis and justify, as better as possible, which contents of the image
have decisively contributed to achieving a particular decision.
To assess the impact of lung segmentation on the identification of COVID-19, we used two XAI approaches: Local
Interpretable Model-agnostic Explanations (LIME) [16] and Gradient-weighted Class Activation Mapping (Grad-CAM)
[17]. LIME works by finding features, superpixels (i.e., particular zones of the image), that increases the probability
of the predicted class, i.e., regions that support the current model prediction. Such regions can be seen as important
regions because the model actively uses them to make predictions. Grad-CAM focuses on the gradients flowing into the
last convolutional layer of a given CNN for a specific input image and label. We can then visually inspect the activation
mapping (AM) to verify if the model is focusing on the appropriate portion of the input image. Both techniques are
somewhat complementary, and by exploring them, we can provide a more complete report of the lung segmentation
impact on COVID-19 identification.
Our results indicated that when the whole image is considered, the model may learn to use other features besides
lung opacities, or even from outside the lungs region. In such cases, the model is not learning to identify pneumonia
or COVID-19, but something else. Thus, we can infer that the model is not reliable even though it achieves a good
classification performance. Using lung segmentation, we would supposedly remove a meaningful part of noise and
background information, forcing the model to take into account only data from the lung area, i.e., desired information
in this specific context. Thus, the classification performance in models using segmented CXR images tends to be more
realistic, closer to human performance, and better reasoned.
The remaining of this paper is organized as follows: Section 2 introduces our proposed methodology and experimental
setup. Section 2 shows details about our methodology, experimental setup, including database, algorithms, and
parameters. Section 3 presents the obtained results. Later, Section 4 discusses the obtained results. Finally, Section 5
presents our conclusions and possibilities for future works.

2

Material and Methods

We focused on exploring data from CXR images for reliable identification of COVID-19 among pneumonia caused by
other micro-organisms. Hence, we proposed a specific method that allowed us to assess lung segmentation’s impact on
COVID-19 identification.
To better understand the proposal of this work, Figure 1 shows a general overview of the classification approach adopted,
containing: lung segmentation (Phase 1), classification (Phase 2), and XAI (Phase 3). Phase 1 is skipped entirely for the
classification of non-segmented CXR images.
In the following subsections, we describe each one of the Phases described in Figure 1.
2

A PREPRINT - F EBRUARY 2, 2021

Figure 1: Proposed methodology.
2.1

Lung Segmentation (Phase 1)

The first phase in our method is the lung segmentation, aiming to remove all background and retain only the lung
area. We expect it to reduce noise that can interfere with the model prediction. Figure 2 presents an example of lung
segmentation.

(c) Segmented lungs.
(a) CXR image.

(b) Binary mask.

Figure 2: Lungs segmentation on CXR image.
Specifically, in deep models, any extra information can lead to model overfitting. This is especially important in CXR
since many images contain burned-in annotations about the machine, operator, hospital, or patient. Figure 3 presents an
example of CXR images with burned in information.
We expect that the models using segmented images rely on information in the lung area rather than background
information, i.e., an increase in the model reliability and prediction quality in a real-world scenario. For example, if a
model is trained to predict lung opacity, it must use lung area information. Otherwise, it is not identifying opacity but
something else.
In order to perform lung segmentation, we applied a CNN approach using the U-Net architecture [15]. The U-Net input
is the CXR image, and the output is a binary mask that indicates the region of interest (ROI). Thus, the training requires
a previously set of binary masks.
3

A PREPRINT - F EBRUARY 2, 2021

Figure 3: CXR with burned in annotations.

The COVID-19 dataset used does not have manually created binary masks for all images. Thus, we adopted a semiautomated approach to creating binary masks for all CXR images. First, we used three additional CXR datasets with
binary masks to increase the training sample size and some binary masks provided by v7labs2 . We then trained the
U-Net model and used it to predict the binary masks for all images in our dataset. After that, we reviewed all predicted
binary masks and manually created masks for those CXR images that the model was unable to generalize well. We
repeated this process until we judged the result satisfactory and achieved a good intersection between target and obtained
regions.
2.1.1

Lung Segmentation Database

Table 1 presents the main characteristics of the database used to perform experimentation on lung segmentation. It
comprises 1,645 CXR images, with a 95/5 percentage train/test split. We also create a third set for training evaluation,
called validation set, containing 5 percent of the training data.
Table 1: Lung segmentation database.
Characteristic

Samples

Train
Validation
Test

1,483
79
83

Total

1,645

Table 2 presents the samples distribution for each source.
Table 2: Lung segmentation database composition.
Source

Samples
3

Cohen v7labs
Montgomery
Shenzhen
JSRT
Manually created

2

https://github.com/v7labs/COVID-19-xray-dataset

4

489
138
566
247
205

A PREPRINT - F EBRUARY 2, 2021

2.1.2

U-Net

The U-Net CNN architecture is a fully convolutional network (FCN) that has two main components: a contraction path,
also called an encoder, which captures the image information; and the expansion path, also called decoder, which uses
the encoded information to create the segmentation output [15].
We used the U-Net CNN architecture with some small changes: we included dropout and batch normalization layers in
each contracting and expanding block. These additions aim to improve training time and reduce overfitting. Figure 4
presents our adapted U-Net architecture.

Figure 4: Custom U-Net architecture
Furthermore, since our dataset is not standardized, the first step was to resize all images to 400px × 400px, because it
presented a good balance between computational requirements and classification performance. We also experimented
with smaller and larger dimensions with no significant improvement.
In this model, we achieve a much better result without using transfer learning and training the network weights from
scratch. Table 3 reports the parameters used in U-Net training.
Table 3: U-Net parameters.
Parameter

Value

Epochs
Batch size
Learning rate

100
16
0.001

After the segmentation, we applied a morphological opening with 5 pixels to remove small brights spots, which usually
happened outside the lung region. We also applied a morphological dilation with 5 pixels to increase and smooth the
predicted mask boundary. Finally, we also cropped all images to keep only the ROI indicated by the mask. After crop
the images were also resized to 300px × 300px. Figure 2 shows an example of this process.
Besides, we also applied data augmentation techniques extensively to further expand our training data. Details regarding
the usage and parameters will be discussed in Section 2.2.2.
5

A PREPRINT - F EBRUARY 2, 2021

2.2

Classification (Phase 2)

We chose a simple and straightforward approach with three of the most popular CNN architectures: VGG16,
ResNet50V2 InceptionV3. For all, we applied transfer learning by loading pre-trained weights from ImageNet
only for the convolutional layers [18]. We then added three fully-connected (FC) layers together, followed by dropout
and batch normalization layers containing 1024, 1024, and 512 units. We performed the classification using full and
segmented CXR images independently.
In the literature, many papers employ complex classification approaches. However, a complex model does not
necessarily mean better performance whatsoever. Even very simple deep architectures tend to overfit very quickly [19].
There must be a solid argument to justify applying a complicated approach to a low sample size problem. Additionally,
CXR images are not the gold standard for pneumonia diagnosis because it has low sensitivity [20, 4]. Thus, human
performance in this problem is usually not very high [21]. That makes us wonder how realistic are some approaches
presented in the literature, in which they achieve a very high classification accuracy.
Table 4 reports the parameters used in the CNN training. We also used a Keras callback to reduce the learning rate by
half once learning stagnates for three consecutive epochs.
Table 4: CNN parameters.

2.2.1

Parameter

Value

Warm-up epochs
Fine-tuning epochs
Batch size
Warm-up learning rate
Fine-tuning learning rate

50
100
40
0.001
0.0001

COVID-19 Database (RYDLS-20-v2)

Table 5 presents some details of the proposed database, which was named RYDLS-20-v2. The database comprises
2,678 CXR images, with an 80/20 percentage train/test split following a holdout validation split.
Therefore, we performed the split considering some crucial aspects: i) multiple CXR images from the same patient are
always kept in the same fold, ii) images from the same source are evenly distributed in the train and test split, and iii)
each class is balanced as much as possible while complying with the two previous restrictions. We also created a third
set for training evaluation, called validation set, containing 20 percent of the training data randomly.
In this context, given the considerations mentioned above, simple random cross-validation would not suffice since it
might not correctly separate the train and test split to avoid data leakage, and it could reduce robustness instead of
increasing it. In this context, the holdout validation is a more comfortable option to ensure a fair and proper separation
of train and test data. The test set was created to represent an independent test set in which we can validate our
classification performance and evaluate the segmentation impact in a less biased context.
Table 5: RYDLS-20-v2 main characteristics.
Class

Train

Validation

Test

Lung opacity (other than COVID-19)
COVID-19
Normal

739
315
673

189
93
150

231
95
193

Total

1727

432

519

We built our database by further expanding our previous work RYDLS-20 [5] and adopting some guidelines and images
provided by the COVIDx dataset [8]. Moreover, we set up the problem with three classes: lung opacity (pneumonia
other than COVID-19), COVID-19, and normal. We also experimented with expanding the number of classes to
represent a more specific pathogen, such as bacteria, fungi, viruses, COVID-19, and normal. However, in all cases, the
trained models did not differentiate between bacteria, fungi, and viruses very well, possibly due to the reduced sample
size. Thus, we decided to take a more general approach to create a more reliable classification schema while retaining
the focus on developing a more realistic approach.
The CXR images were obtained from eight different sources. Table 6 presents the samples distribution for each source.
6

A PREPRINT - F EBRUARY 2, 2021

Table 6: Sources used in RYDLS-20-v2 database.
Source

Lung opacity

COVID-19

Normal

Dr. Joseph Cohen GitHub Repository [22]
Kaggle RSNA Pneumonia Detection Challenge4
Actualmed COVID-19 Chest X-ray Dataset
Initiative5
Figure 1 COVID-19 Chest X-ray Dataset
Initiative6
Radiopedia encyclopedia7
Euroad8
Hamimi’s Dataset[23]
Bontrager and Lampignano’s Dataset [24]

140
1000

418
-

16
1000

-

51

-

-

34

-

7
1
7
4

-

-

We considered posteroanterior (PA) and anteroposterior (AP) projections with the patient erect, sitting, or supine on
the bed. We disregarded CXR with a lateral view because they are usually used only to complement a PA or AP view
[25]. Additionally, we also considered CXR taken from portable machines, which usually happens when the patient
cannot move (e.g., ICU admitted patients). This is an essential detail since there are differences between regular X-ray
machines and portable X-ray machines regarding the image quality; we found most portable CXR images in the classes
COVID-19 and lung opacity. We removed images with low resolution and overall low quality to avoid any issues when
resizing the images.
Finally, we have no further details about the X-ray machines, protocols, hospitals, or operators, and these details impact
the resulting CXR image. All CXR images are de-identified9 , and for some of them, there is demographic information
available, such as age, gender, and comorbidities.
Figure 5 presents image examples for each class retrieved from the RYDLS-20-v2 database.

(a) Lung opacity.

(b) COVID-19.

(c) Normal.

Figure 5: RYDLS-20-v2 image samples.
2.2.2

Data Augmentation

We extensively used data augmentation during training in segmentation and classification to virtually increase our
training sample size [26]. Table 7 presents the transformations used during training along with their parameters. The
probability of applying each transformation was kept at the default value of 50%. We used the library albumentations10
to perform all transformations [27]. Figure 6 displays some examples of the transformations applied.
2.3

XAI (Phase 3)

Depending on the perspective, most machine learning models can be seen as a black-box classifier, it receives input and
somehow computes an output [28]. It might happen both with deep and shallow learning, with some exceptions like
9
10

Aiming at attending to data privacy policies.
https://github.com/albumentations-team/albumentations

7

A PREPRINT - F EBRUARY 2, 2021

Table 7: Data augmentation parameters.
Transformation

Segmentation

Classification

Horizontal flip

–

–

Shift scale rotate

Shift limit = 0.0625
Scale limit = 0.1
Rotate limit = 45

Shift limit = 0.05
Scale limit = 0.05
Rotate limit = 15

Elastic transform

Alpha = 1
Sigma = 50
Alpha affine = 50

Alpha = 1
Sigma = 20
Alpha affine = 20

Random brightness

Limit = 0.2

Limit = 0.2

Random contrast

Limit = 0.2

Limit = 0.2

Random gamma

Limit = (80, 120)

Limit = (80, 120)

Figure 6: Data augmentation examples.

decision trees. Even though we can measure our model’s performance using a set of metrics, it is nearly impossible to
make sure that the model focuses on the correct portion of the test image for prediction.
Specifically, in our use case, we want the model to focus exclusively on the lung area and not somewhere else. If the
model uses information from other regions, even if very high accuracy is achieved, there can be some limitations to its
application, since it is not learning to identify COVID-19 but something else.
Here, we aim to demonstrate that by using segmented images, the model prediction uses primarily the lung area, which
is not often the case when we use full CXR images. To do so, we applied two XAI approaches: LIME and Grad-CAM.
Despite having the same main objective, they differ in how they find the important regions. Figures 7 and 8 shows
examples of important regions highlighted by LIME and Grad-CAM, respectively. In section 3, we will show that
models trained using segmented lungs focus primarily on the lung area, while models trained using full CXR images
frequently focus elsewhere.
The reason for not using handcrafted feature extraction algorithms here is that it is usually not straightforward to rebuild
the reverse path, i.e., from prediction to the raw image. Sometimes, the handcrafted algorithm creates global features,
eliminating the possibility of identifying the image regions that resulted in a specific feature.
For each image in the test set, we used LIME and Grad-CAM to find the most important regions used for the predicted
class, i.e., regions that support the given prediction. We then summarized all those regions in a heatmap to show the
most common regions that the model uses for prediction. Thus, we have one heatmap per classifier per class per XAI
approach.
8

A PREPRINT - F EBRUARY 2, 2021

(b) Segmented CXR image.

(a) Full CXR image.

Figure 7: LIME example.

(a) Full CXR image.

(b) Segmented CXR image.

Figure 8: Grad-CAM example.

Table 8 presents the parameters used in LIME. Grad-CAM has a single configurable parameter, which is the convolutional
layer to be used, and, in our case, we used the standard approach.

Table 8: LIME parameters.
Parameter

Value

Superpixels identification
Quickshift kernel size
Distance metric
Number of samples per image
Number of superpixels in explanation per image
Filter only positive superpixels

Quickshift segmentation
4
Cosine
1000
5
True

9

A PREPRINT - F EBRUARY 2, 2021

3

Results

This section presents an overview of our experimental findings and a preliminary analysis of each contribution
individually.
3.1

Lung Segmentation Results

Table 9 shows the overall U-Net segmentation performance for the test set for each source we used to compose the lung
segmentation database considering the Jaccard distance and the Dice coefficient metrics.
Table 9: Lung segmentation results.
Database

Jaccard distance

Dice coefficient

Cohen v7labs
Montgomery
Shenzhen
JSRT
Manually created masks

0.041 ± 0.027
0.019 ± 0.007
0.017 ± 0.008
0.018 ± 0.011
0.071 ± 0.021

0.979 ± 0.014
0.991 ± 0.003
0.991 ± 0.004
0.991 ± 0.006
0.964 ± 0.011

Test set

0.035 ± 0.027

0.982 ± 0.014

As we expected, our manually created masks underperformed when compared to the other sources’ results, this may
have happened because our masks were not made by professional radiologists. Following that, the Cohen v7labs set also
presented a somewhat lower performance. Our manual inspection showed that the model did not include the overlapping
region between the lung and heart, and the masks in Cohen v7labs included that region, hence the difference. The
performance of the remaining databases is outstanding.
3.2

Classification Results

Table 10 presents F1-Score results for our multi-class scenario. The models using non-segmented CXR images presented
better results than the models that used segmented images when we consider raw performance for COVID-19 and lung
opacity. Both settings were on par in the normal class.
Table 10: F1-Score results.
Class

COVID-19

Lung opacity

Normal

Macro-avg

Segmented - VGG16
Segmented - ResNet50V2
Segmented - InceptionV3

0.83
0.78
0.83

0.88
0.87
0.89

0.9
0.91
0.92

0.87
0.85
0.88

Non-segmented - VGG16
Non-segmented - ResNet50V2
Non-segmented - InceptionV3

0.94
0.91
0.86

0.91
0.9
0.9

0.91
0.92
0.91

0.92
0.91
0.9

In all cases, the models using segmented images performed worse, considering the selected metric. That result alone
might discourage the usage of segmentation in practice. However, in Section 3.3, we will show that it is still worth to
take into account the segmentation strategy. Even though the use of segmentation does not lead to improvements in the
F1-Score rates, the resulting models may present a more realistic performance.
3.3

XAI Results

Figures 9 and 10 present the LIME and Grad-CAM heatmaps for our multi-class scenario. We can notice that the
models created using segmented CXR images focused primarily in the lung area. The lung shape is discernible in all
heatmaps. The only small exception is the VGG16 Lung Opacity class. Despite having the visible lung shape, it also
focused a lot in other regions. In contrast, the models that used full CXR images are more chaotic. We can see, for
instance, that for both InceptionV3 and VGG16, the Lung Opacity and Normal class heatmaps almost did not focus on
the lung area at all.
Even though the models that used full CXR images performed better, considering the F1-Score, they used information
outside the lung area to predict the output class. Thus, they did not necessarily learn to identify lung opacity or
10

A PREPRINT - F EBRUARY 2, 2021

(a) VGG16.

(b) ResNet50V2.

(c) InceptionV3.

Figure 9: LIME heatmaps.

(a) VGG16.

(b) ResNet50V2.

Figure 10: Grad-CAM heatmaps.

11

(c) InceptionV3.

A PREPRINT - F EBRUARY 2, 2021

COVID-19, but something else. Hence, we can say that even though they perform better, considering the classification
metric, they are worse and not reliable for real-world applications.

4

Discussions

In general, models using full CXR images performed significantly better, which is an exciting result since we expected
otherwise. We applied a Wilcoxon signed-rank test, which indicated that the models using segmented CXR images
have a significantly lower F1-Score than the models using non-segmented CXR images (p = 0.019). Additionally, a
Bayesian t-test also indicated that using segmented CXR images reduces the F1-Score with a Bayes Factor of 2.1. The
Bayesian framework for hypothesis testing is very robust even for a low sample size [29].
Such a result was the main reason we decided to apply XAI techniques to explain individual predictions. Our rationale is
that a CXR image contains a lot of noise and background data, which might trick the classification model into focusing
on the wrong portions of the image during training. Figure 11 presents some examples of the Grad-CAM showing that
the model is actively using burned in annotations for the prediction. The LIME heatmaps presented in Figure 9 show
the same behavior for the classes Lung opacity and Normal in the non-segmented models, i.e., the model learned to
identify the annotations and not lung opacities. The Grad-CAM heatmaps in Figure 10 also show the focus on the
annotations for all classes in the non-segmented models.

(a) Example 1.

(b) Example 2.

Figure 11: Grad-CAM showing a large gradient on CXR annotations.
The most affected class by lung segmentation is the COVID-19, followed by Lung opacity. The Normal class had
a minimal impact. The best F1-Scores for COVID-19 and Lung opacity using full CXR images are 0.94 and 0.91,
respectively, and after the segmentation, they are 0.83 and 0.89, respectively. We conjecture that such an impact comes
from the fact that many CXR images are from patients with severe clinical conditions who cannot walk or stand. Thus
the medical practitioners must use a portable X-ray machine that produces images with the “AP Portable” annotation.
That impact also means that the classification models had trouble identifying COVID-19.
Considering specifically the models using segmented CXR images, InceptionV3 performed better in all classes. Figure
12 provides a visual representation of the F1-Score achieved in the experimental results stratified by the model used and
lung segmentation. Figure 13 shows the confusion matrix for the InceptionV3 using segmented CXR images. Overall
the classifier presented a remarkable performance in all labels. The largest misclassification happened with the class
Lung opacity being predicted as Normal, followed by the class COVID-19 being predicted as Lung opacity. However,
there are reasonable explanations for both: i) Most examples from the classes Lung opacity and Normal came from the
RSNA database; thus, we believe that the data source biased the classification marginally; ii) pneumonia caused by
COVID-19 could have been confused with pneumonia caused by another pathogen. A solution for both issues would be
to increase the number of images in the database, including more data sources.
Our XAI approach is novel in the sense that we explored a more general explanation instead of focusing on single
examples. In the literature, there are many papers exploring LIME and Grad-CAM for a couple of handpicked examples.
The main problem with such approaches is that the examples might have been eventually chosen according to the
specific purposes intended. In this paper, we applied the XAI techniques to each image in the test set individually and
12

A PREPRINT - F EBRUARY 2, 2021

Figure 12: F1-Score results boxplot stratified by segmentation and model.

Figure 13: Segmented InceptionV3 Confusion Matrix.

13

A PREPRINT - F EBRUARY 2, 2021

created a heatmap aggregating all individual results to represent a broader context, which indicates which portions
of the CXR image the models have focused on for prediction. Figures 9 and 10 demonstrate that the models using
full CXR images are misleading because they focus a lot on the left and right uppermost regions, which is usually the
location of burned-in annotations.
In a real-world application, especially in medical practice, we must be cautious and thorough when designing systems
aimed at diagnostic support because they directly affect people’s lives. A misdiagnosis can have severe consequences
for the health and further treatment of a patient. Furthermore, in the COVID-19 pandemic, such consequences can also
affect other people since it is a highly infectious disease. Even though the current pandemic attracted much attention
from the research community in general, few works focused on a more critical evaluation of the solutions proposed. So,
we believe now it is time to devote efforts in this direction.
Ultimately, we demonstrated here that lung segmentation is important for COVID-19 identification in CXR images
through a comprehensive and straightforward application of deep models coupled with XAI techniques. In fact, in our
previous work [5], we have addressed the task of pneumonia identification as a whole, stating that maybe the patterns of
the injuries caused by the different pathogens (virus, bacteria, and fungus) are different, so we were able to classify the
CXR images with machine learning techniques. Even though the experimental results of that work have shown that
it may be possible, it is challenging to be sure that other patterns did not bias the results in the images that were not
related to the lungs.
Furthermore, we still believe that even after lung segmentation, the database bias still potentially influenced the
classification model. Thus, more aspects regarding the CXR images and the classification model must be further
evaluated to design a proper COVID-19 diagnosis system using CXR images.

5

Conclusion

The application of pattern recognition techniques has proven to be very useful in many situations in the real world.
Several papers propose using machine learning methods to identify pneumonia and COVID-19 in CXR images with
encouraging results. However, very few proposed to use lung segmentation to avoid any data leak or overfitting, and
only focused on the classification performance.
Considering a real-world application, segmentation is an important step since it removes background information,
reduces the chance of data leak, and forces the model to focus only on important image areas. Segmentation might not
improve the classification performance, but as it forces the model to use only the lung area information, it increases the
model’s reliability and quality.
The classification using segmented lungs achieved an F1-Score of 0.88 for the multi-class setup and 0.83 for COVID-19
identification. Using non-segmented CXR images, the classification achieved an F1-Score of 0.92 and 0.94 for the
multi-class setup and COVID-19 identification, respectively.
It is unfair to make direct comparisons of identification rates from different works, as they usually use different databases
under different circumstances. Nevertheless, to the best of our knowledge, we achieved the best identification rate of
COVID-19 among other types of pneumonia using segmented CXR images. Additionally, we must highlight our novel
approach to demonstrate the importance of lung segmentation in CXR image classification.
We do not claim state-of-the-art classification results at this time for a couple of reasons: i) there are some initiatives to
build a comprehensive COVID-19 CXR database still ongoing; however, we still do not have a reliable database that
can be used as a definitive benchmark; ii) in clinical practice, a small difference in the classification performance is
hardly noticeable, and the model reliability and quality are more important than the classification performance [30]; and,
iii) the CXR is not the gold standard for diagnosis, even experienced medical practitioners sometimes face doubts when
examining a CXR image [4]; thus we should be very cautious at papers claiming very high classification performance
when the human performance is much lower.
Our segmentation approach achieved a Jaccard distance of 0.034 and a Dice coefficient of 0.982, which represents
a robust performance considering two factors: i) we did not aim to surpass the state-of-the-art performance of lung
segmentation in CXR images; instead, we focused on creating a general segmentation model capable of producing
binary lung masks for CXR images in our COVID-19 database; ii) the lung segmentation database was composed
of multiple sources, some masks were even manually created. Nevertheless, our approach was on par with current
state-of-the-art lung segmentation in CXR images [31, 32, 33].
Furthermore, we applied LIME and Grad-CAM to demonstrate that using segmented CXR images, the models focused
primarily on information in the lung area to classify the CXR images. Thus, despite lowering the F1-Score, segmentation
improves the prediction quality as it forces the model to use only relevant information.
14

A PREPRINT - F EBRUARY 2, 2021

As future work, we aim to keep improving our database to increase our classification performance and provide more
robust estimates by using more CNN architectures for segmentation and classification. Furthermore, we want to apply
more sophisticated segmentation techniques to isolate specific lung opacities caused by COVID-19. Likewise, we also
want to explore more approaches to evaluate the model predictions, such as SHAP [34].

References
[1] Matthew Zirui Tay, Chek Meng Poh, Laurent Rénia, Paul A MacAry, and Lisa FP Ng. The trinity of COVID-19:
immunity, inflammation and intervention. Nature Reviews Immunology, pages 1–12, 2020.
[2] Stephen A Lauer, Kyra H Grantz, Qifang Bi, Forrest K Jones, Qulu Zheng, Hannah R Meredith, Andrew S Azman,
Nicholas G Reich, and Justin Lessler. The incubation period of coronavirus disease 2019 (COVID-19) from
publicly reported confirmed cases: estimation and application. Annals of Internal Medicine, 172(9):577–582,
2020.
[3] Ahmad Alimadadi, Sachin Aryal, Ishan Manandhar, Patricia B Munroe, Bina Joe, and Xi Cheng. Artificial
intelligence and machine learning to fight COVID-19, 2020.
[4] Wesley H Self, D Mark Courtney, Candace D McNaughton, Richard G Wunderink, and Jeffrey A Kline. High
discordance of chest x-ray and computed tomography for detection of pulmonary opacities in ed patients:
implications for diagnosing pneumonia. The American Journal of Emergency Medicine, 31(2):401–405, 2013.
[5] Rodolfo M Pereira, Diego Bertolini, Lucas O Teixeira, Carlos N Silla Jr, and Yandre M G Costa. COVID-19
identification in chest x-ray images on flat and hierarchical classification scenarios. Computer Methods and
Programs in Biomedicine, 194:105532, 2020.
[6] Xiao Qi, Lloyd G Brown, David J Foran, John Nosher, and Ilker Hacihaliloglu. Chest x-ray image phase features
for improved diagnosis of covid-19 using convolutional neural network. International journal of computer assisted
radiology and surgery, pages 1–10, 2020.
[7] Parisa Gifani, Ahmad Shalbaf, and Majid Vafaeezadeh. Automated detection of covid-19 using ensemble of
transfer learning with deep convolutional neural network based on ct scans. International journal of computer
assisted radiology and surgery, pages 1–9, 2020.
[8] Linda Wang and Alexander Wong. COVID-net: A tailored deep convolutional neural network design for detection
of COVID-19 cases from chest x-ray images. arXiv preprint arXiv:2003.09871, 2020.
[9] Beatriz Garcia Santa Cruz, Jan Sölter, Matias Nicolas Bossa, and Andreas Dominik Husch. On the composition
and limitations of publicly available covid-19 x-ray imaging datasets. arXiv preprint arXiv:2008.11572, 2020.
[10] Gianluca Maguolo and Loris Nanni. A critic evaluation of methods for covid-19 automatic detection from x-ray
images. arXiv preprint arXiv:2004.12823, 2020.
[11] Enzo Tartaglione, Carlo Alberto Barbano, Claudio Berzovini, Marco Calandri, and Marco Grangetto. Unveiling covid-19 from chest x-ray with deep learning: a hurdles race with small data. International Journal of
Environmental Research and Public Health, 17(18):6933, 2020.
[12] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
Proceedings of the International Conference on Learning Representations, pages 1–14, 2015.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In
Proceedings of the European Conference on Computer Vision, pages 630–645. Springer, 2016.
[14] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception
architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2818–2826, 2016.
[15] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In Proceedings of the International Conference on Medical Image Computing and computer-assisted
intervention, pages 234–241. Springer, 2015.
[16] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “why should i trust you?” explaining the predictions of
any classifier. In Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining,
pages 1135–1144, 2016.
[17] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv
Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the
IEEE International Conference on Computer Vision, pages 618–626, 2017.
[18] Antonio Gulli and Sujit Pal. Deep learning with Keras. Packt Publishing Ltd, 2017.
15

A PREPRINT - F EBRUARY 2, 2021

[19] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929–1958,
2014.
[20] Jared T Hagaman, Ralph J Panos, Gregory W Rouan, and Ralph T Shipley. Admission chest radiograph lacks
sensitivity in the diagnosis of community-acquired pneumonia. The American journal of the medical sciences,
337(4):236–240, 2009.
[21] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti
Bagul, Curtis Langlotz, Katie Shpanskaya, Matthew P Lungren, and Andrew Y Ng. Chexnet: Radiologist-level
pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225, 2017.
[22] Joseph Paul Cohen, Paul Morrison, Lan Dao, Karsten Roth, Tim Q Duong, and Marzyeh Ghassemi. COVID-19
image data collection: Prospective predictions are the future. arXiv preprint arXiv:2006.11988, 2020.
[23] Ahmed Hamimi. Mers-cov: Middle east respiratory syndrome corona virus: Can radiology be of help? initial
single center experience. The Egyptian Journal of Radiology and Nuclear Medicine, 47(1):95–106, 2016.
[24] Amr M Ajlan, Brendan Quiney, Savvas Nicolaou, and Nestor L Muller. Swine-origin influenza a (h1n1) viral
infection: radiographic and ct findings. American Journal of Roentgenology, 193(6):1494–1499, 2009.
[25] Kenneth L Bontrager and John Lampignano. Textbook of radiographic positioning and related Anatomy-E-Book.
Elsevier Health Sciences, 2013.
[26] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of
Big Data, 6(1):60, 2019.
[27] Alexander Buslaev, Vladimir I Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A
Kalinin. Albumentations: fast and flexible image augmentations. Information, 11(2):125, 2020.
[28] Josua Krause, Adam Perer, and Kenney Ng. Interacting with predictions: Visual inspection of black-box machine
learning models. In Proceedings of the Conference on Human Factors in Computing Systems, pages 5686–5697,
2016.
[29] Felix D Schönbrodt, Eric-Jan Wagenmakers, Michael Zehetleitner, and Marco Perugini. Sequential hypothesis
testing with bayes factors: Efficiently testing mean differences. Psychological methods, 22(2):322, 2017.
[30] Po-Hsuan Cameron Chen, Yun Liu, and Lily Peng. How to develop machine learning models for healthcare.
Nature Materials, 18(5):410, 2019.
[31] Cheng Chen, Qi Dou, Hao Chen, and Pheng-Ann Heng. Semantic-aware generative adversarial nets for unsupervised domain adaptation in chest x-ray segmentation. In International workshop on machine learning in medical
imaging, pages 143–151. Springer, 2018.
[32] Youbao Tang, Yuxing Tang, Jing Xiao, and Ronald M Summers. Xlsor: A robust and accurate lung segmentor on
chest x-rays using criss-cross attention and customized radiorealistic abnormalities generation. arXiv preprint
arXiv:1904.09229, 2019.
[33] Jyoti Islam and Yanqing Zhang. Towards robust lung segmentation in chest radiographs with deep learning. arXiv
preprint arXiv:1811.12638, 2018.
[34] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances in Neural
Information Processing Systems, pages 4765–4774, 2017.

16

