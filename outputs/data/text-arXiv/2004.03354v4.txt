Inexpensive Domain Adaptation of Pretrained Language Models:
Case Studies on Biomedical NER and Covid-19 QA
Nina Poerner∗† and Ulli Waltinger† and Hinrich Schütze∗
∗
Center for Information and Language Processing, LMU Munich, Germany
†
Corporate Technology Machine Intelligence (MIC-DE), Siemens AG Munich, Germany
poerner@cis.uni-muenchen.de | inquiries@cislmu.org

arXiv:2004.03354v4 [cs.CL] 27 Jun 2020

Abstract
Domain adaptation of Pretrained Language
Models (PTLMs) is typically achieved by unsupervised pretraining on target-domain text.
While successful, this approach is expensive
in terms of hardware, runtime and CO2 emissions. Here, we propose a cheaper alternative: We train Word2Vec on target-domain text
and align the resulting word vectors with the
wordpiece vectors of a general-domain PTLM.
We evaluate on eight biomedical Named Entity
Recognition (NER) tasks and compare against
the recently proposed BioBERT model. We
cover over 60% of the BioBERT – BERT
F1 delta, at 5% of BioBERT’s CO2 footprint
and 2% of its cloud compute cost. We also
show how to quickly adapt an existing generaldomain Question Answering (QA) model to an
emerging domain: the Covid-19 pandemic.1

1

Introduction

Pretrained Language Models (PTLMs) such as
BERT (Devlin et al., 2019) have spearheaded advances on many NLP tasks. Usually, PTLMs
are pretrained on unlabeled general-domain and/or
mixed-domain text, such as Wikipedia, digital
books or the Common Crawl corpus.
When applying PTLMs to specific domains, it
can be useful to domain-adapt them. Domain adaptation of PTLMs has typically been achieved by pretraining on target-domain text. One such model is
BioBERT (Lee et al., 2020), which was initialized
from general-domain BERT and then pretrained
on biomedical scientific publications. The domain
adaptation is shown to be helpful for target-domain
tasks such as biomedical Named Entity Recognition (NER) or Question Answering (QA). On the
downside, the computational cost of pretraining can
be considerable: BioBERTv1.0 was adapted for ten
1

www.github.com/npoe/covid-qa

days on eight large GPUs (see Table 1), which is
expensive, environmentally unfriendly, prohibitive
for small research labs and students, and may delay
prototyping on emerging domains.
We therefore propose a fast, CPU-only domainadaptation method for PTLMs: We train
Word2Vec (Mikolov et al., 2013a) on target-domain
text and align the resulting word vectors with the
wordpiece vectors of an existing general-domain
PTLM. The PTLM thus gains domain-specific lexical knowledge in the form of additional word vectors, but its deeper layers remain unchanged. Since
Word2Vec and the vector space alignment are efficient models, the process requires a fraction of the
resources associated with pretraining the PTLM
itself, and it can be done on CPU.
In Section 4, we use the proposed method to
domain-adapt BERT on PubMed+PMC (the data
used for BioBERTv1.0) and/or CORD-19 (Covid19 Open Research Dataset). We improve over
general-domain BERT on eight out of eight biomedical NER tasks, using a fraction of the compute cost
associated with BioBERT. In Section 5, we show
how to quickly adapt an existing Question Answering model to text about the Covid-19 pandemic,
without any target-domain Language Model pretraining or finetuning.

2
2.1

Related work
The BERT PTLM

For our purpose, a PTLM consists of three parts:
A tokenizer TLM : L+ → L+
LM , a wordpiece embedding lookup function ELM : LLM → RdLM
and an encoder function FLM . LLM is a limited vocabulary of wordpieces. All words from
the natural language L+ that are not in LLM
are tokenized into sequences of shorter wordpieces, e.g., dementia becomes dem ##ent ##ia.
Given a sentence S = [w1 , . . . , wT ], tokenized

BioBERTv1.0
BioBERTv1.1
GreenBioBERT (Section 4)
GreenCovidSQuADBERT (Section 5)

size

Domain adaptation hardware

base
base
base
large

8 NVIDIA v100 GPUs (32GB)
8 NVIDIA v100 GPUs (32GB)
12 Intel Xeon E7-8857 CPUs, 30GB RAM
12 Intel Xeon E7-8857 CPUs, 40GB RAM

Power(W)

Time(h)

CO2 (lbs)

Google Cloud $

1505
1505
1560
1560

240
552
12
24

544
1252
28
56

1421 – 4762
3268 – 10952
16 –
76
32 – 152

Table 1: Domain adaptation cost. CO2 emissions are calculated according to Strubell et al. (2019). Since our
hardware configuration is not available on Google Cloud, we take an m1-ultramem-40 instance (40 vCPUs, 961GB
RAM) to estimate an upper bound on our Google Cloud cost.

as TLM (S) = [TLM (w1 ); . . . ; TLM (wT )], ELM embeds every wordpiece in TLM (S) into a real-valued,
trainable wordpiece vector. The wordpiece vectors of the entire sequence are stacked and fed into
FLM . Note that we consider position and segment
embeddings to be a part of FLM rather than ELM .
In the case of BERT, FLM is a Transformer
(Vaswani et al., 2017), followed by a final FeedForward Net. During pretraining, the FeedForward Net predicts the identity of masked wordpieces. When finetuning on a supervised task, it is
usually replaced with a randomly initialized layer.
2.2

Domain-adapted PTLMs

Domain adaptation of PTLMs is typically achieved
by pretraining on unlabeled target-domain text.
Some examples of such models are BioBERT
(Lee et al., 2020), which was pretrained on the
PubMed and/or PubMed Central (PMC) corpora,
SciBERT (Beltagy et al., 2019), which was pretrained on papers from SemanticScholar, ClinicalBERT (Alsentzer et al., 2019; Huang et al., 2019a)
and ClinicalXLNet (Huang et al., 2019b), which
were pretrained on clinical patient notes, and AdaptaBERT (Han and Eisenstein, 2019), which was
pretrained on Early Modern English text. In most
cases, a domain-adapted PTLM is initialized from
a general-domain PTLM (e.g., standard BERT),
though Beltagy et al. (2019) report better results
with a model that was pretrained from scratch with
a custom wordpiece vocabulary. In this paper, we
focus on BioBERT, as its domain adaptation corpora are publicly available.
2.3

Word vectors

Word vectors are distributed representations of
words that are trained on unlabeled text. Contrary to PTLMs, word vectors are non-contextual,
i.e., a word type is always assigned the same vector, regardless of context. In this paper, we use
Word2Vec (Mikolov et al., 2013a) to train word
vectors. We will denote the Word2Vec lookup function as EW2V : LW2V → RdW2V .

2.4

Word vector space alignment

Word vector space alignment has most frequently
been explored in the context of cross-lingual word
embeddings. For instance, Mikolov et al. (2013b)
align English and Spanish Word2Vec spaces by a
simple linear transformation. Wang et al. (2019)
use a related method to align cross-lingual word
vectors and multilingual BERT wordpiece vectors.

3

Method

In the following, we assume access to a generaldomain PTLM, as described in Section 2.1, and a
corpus of unlabeled target-domain text.
3.1

Creating new input vectors

In a first step, we train Word2Vec on the targetdomain corpus. In a second step, we take the intersection of LLM and LW2V . In practice, the intersection mostly contains wordpieces from LLM
that correspond to standalone words. It also contains single characters and other noise, however, we
found that filtering them does not improve alignment quality. In a third step, we use the intersection to fit an unconstrained linear transformation
W ∈ RdLM ×dW2V via least squares:
X
argmin
||WEW2V (x) − ELM (x)||22
W

x∈LLM ∩LW2V

Intuitively, W makes Word2Vec vectors “look
like” the PTLM’s native wordpiece vectors, just
like cross-lingual alignment makes word vectors
from one language “look like” word vectors from
another language. In Table 2 (top), we show examples of within-space and cross-space nearest
neighbors after alignment.
3.2

Updating the wordpiece embedding layer

Next, we redefine the wordpiece embedding layer
of the PTLM. The most radical strategy would be to
replace the entire layer with the aligned Word2Vec
vectors:
ÊLM : LW2V → RdLM ; ÊLM (x) = WEW2V (x)

query ∈ LW2V ∩ LLM
Boldface: Training vector pairs

query ∈ LW2V − LLM

Biomedical NER task

Query

NNs of query in ELM [LLM ]

NNs of query in WEW2V [LW2V ]

surgeon
surgeon
depression
depression

physician, psychiatrist, surgery
surgeon, physician, researcher
Depression, recession, depressed
depression, anxiety, anxiousness

surgeon, urologist, neurosurgeon
neurosurgeon, urologist, radiologist
depression, Depression, hopelessness
depressive, insomnia, Depression

ventricular
suppressants
anesthesiologist
nephrotoxicity

cardiac, pulmonary, mitochondrial
medications, medicines, medication
surgeon, technician, psychiatrist
toxicity, inflammation, contamination

atrial, ventricle, RV
suppressant, prokinetics, painkillers
anesthetist, anaesthesiologist, anaesthetist
hepatotoxicity, ototoxicity, cardiotoxicity

(NER task ID)

BC5CDR-disease (Li et al., 2016)
NCBI-disease (Doğan et al., 2014)
BC5CDR-chem (Li et al., 2016)
BC4CHEMD (Krallinger et al., 2015)
BC2GM (Smith et al., 2008)
JNLPBA (Kim et al., 2004)
LINNAEUS (Gerner et al., 2010)
Species-800 (Pafilis et al., 2013)

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

BERT (ref)
(Lee et al., 2020)

BioBERTv1.0 (ref)
(Lee et al., 2020)

BioBERTv1.1 (ref)
(Lee et al., 2020)

GreenBioBERT
(with standard error of the mean)

81.97 / 82.48 / 82.41
84.12 / 87.19 / 85.63
90.94 / 91.38 / 91.16
91.19 / 88.92 / 90.04
81.17 / 82.42 / 81.79
69.57 / 81.20 / 74.94
91.17 / 84.30 / 87.60
69.35 / 74.05 / 71.63

85.86 / 87.27 / 86.56
89.04 / 89.69 / 89.36
93.27 / 93.61 / 93.44
92.23 / 90.61 / 91.41
85.16 / 83.65 / 84.40
72.68 / 83.21 / 77.59
93.84 / 86.11 / 89.81
72.84 / 77.97 / 75.31

86.47 / 87.84 / 87.15
88.22 / 91.25 / 89.71
93.68 / 93.26 / 93.47
92.80 / 91.92 / 92.36
84.32 / 85.12 / 84.72
72.24 / 83.56 / 77.49
90.77 / 85.83 / 88.24
72.80 / 75.36 / 74.06

84.88 (.07) / 85.29 (.12) / 85.08 (.08)
85.49 (.23) / 86.41 (.15) / 85.94 (.16)
93.82 (.11) / 92.35 (.17) / 93.08 (.07)
92.80 (.04) / 89.78 (.07) / 91.26 (.04)
83.34 (.15) / 83.58 (.09) / 83.45 (.10)
71.93 (.12) / 82.58 (.12) / 76.89 (.10)
92.50 (.17) / 84.54 (.26) / 88.34 (.18)
73.19 (.26) / 75.47 (.33) / 74.31 (.24)

Table 2: Top: Examples of within-space and cross-space nearest neighbors (NNs) by cosine similarity in GreenBioBERT’s wordpiece embedding layer. Blue: Original wordpiece space. Green: Aligned Word2Vec space. Bottom: Biomedical NER test set precision / recall / F1 (%). “(ref)”: Reference scores from Lee et al. (2020).
Boldface: Best model in row. Underlined: Best model without target-domain LM pretraining.

In initial experiments, this strategy led to a
drop in performance, presumably because function words are not well represented by Word2Vec,
and replacing them disrupts BERT’s syntactic abilities. To prevent this problem, we leave existing
wordpiece vectors intact and only add new ones:
ÊLM : LLM ∪ LW2V → RdLM ;
(
ELM (x)
if x ∈ LLM
ÊLM (x) =
WEW2V (x) otherwise
3.3

(1)

Updating the tokenizer

In a final step, we update the tokenizer to account
for the added words. Let TLM be the standard
BERT tokenizer, and let T̂LM be the tokenizer that
treats all words in LLM ∪ LW2V as one-wordpiece
tokens, while tokenizing any other words as usual.
In practice, a given word may or may not benefit
from being tokenized by T̂LM instead of TLM . To
give a concrete example, 82% of the words in the
BC5CDR NER dataset that end in the suffix -ia are
part of a disease entity (e.g., dementia). TLM tokenizes this word as dem ##ent ##ia, thereby exposing this strong orthographic cue to the model. As
a result, TLM improves recall on -ia diseases. But
there are many cases where wordpiece tokenization is meaningless or misleading. For instance
euthymia (not a disease) is tokenized by TLM as e
##uth ##ym ##ia, making it likely to be classified
as a disease. By contrast, T̂LM gives euthymia a
one-wordpiece representation that depends only on
distributional semantics. We find that using T̂LM
improves precision on -ia diseases.

To combine these complementary strengths, we
use a 50/50 mixture of TLM -tokenization and T̂LM tokenization when finetuning the PTLM on a task.
At test time, we use both tokenizers and mean-pool
the outputs. Let o(T (S)) be some output of interest
(e.g., a logit), given sentence S tokenized by T . We
predict: 12 [o(TLM (S)) + o(T̂LM (S))]

4

Experiment 1: Biomedical NER

In this section, we use the proposed method to
create GreenBioBERT, an inexpensive and environmentally friendly alternative to BioBERT. Recall that BioBERTv1.0 (biobert v1.0 pubmed pmc)
was initialized from general-domain BERT (bertbase-cased) and then pretrained on PubMed+PMC.
4.1

Domain adaptation

We train Word2Vec with vector size dW2V =
dLM = 768 on PubMed+PMC (see Appendix for
details). Then, we update the wordpiece embedding layer and tokenizer of general-domain BERT
(bert-base-cased) as described in Section 3.
4.2

Finetuning

We finetune GreenBioBERT on the eight publicly
available NER tasks used in Lee et al. (2020). We
also do reproduction experiments with generaldomain BERT and BioBERTv1.0, using the same
setup as our model. See Appendix for details on
preprocessing and hyperparameters. Since some of
the datasets are sensitive to the random seed, we
report mean and standard error over eight runs.

NER task ID

BERT (ref)
BERT (repro)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
0.00 0.25

BioBERTv1.0 (ref)
BioBERTv1.0 (repro)

BioBERTv1.1 (ref)
GreenBioBERT

0.50 0.75 1.00 1.25
Test set F1 shifted and scaled

1.50

Results and discussion

Experiment 2: Covid-19 QA

In this section, we use the proposed method to
quickly adapt an existing general-domain QA
model to an emerging target domain: Covid-19.
Our baseline model is SQuADBERT,2 an existing BERT model that was finetuned on generaldomain SQuAD (Rajpurkar et al., 2016). We evaluate on Deepset-AI Covid-QA (Möller et al., 2020),
a SQuAD-style dataset with 2019 questions about
147 papers from CORD-19 (Covid-19 Open Research Dataset). We assume that there is no labeled
target-domain data for finetuning, which is a realistic setup for a new domain.
5.1

(2)

(3)

(4)

(5)

(6)

(7)

(8)

-4.88
-4.33

-3.50
-3.60

-4.13
-3.19

-3.34
-3.19

-2.34
-1.92

-0.56
-0.50

-0.84
-0.84

-4.63
-3.58

EM

F1

substr

domain adaptation corpus

Table 2 (bottom) shows entity-level precision, recall and F1, as measured by the CoNLL NER scorer.
For ease of visualization, Figure 1 shows what portion of the BioBERT – BERT F1 delta is covered.
On average, we cover between 61% and 70% of
the F1 delta (61% for BioBERTv1.0 (ref), 70%
for BioBERTv1.1 (ref), and 61% if we take our
reproduction experiments as reference).
To test whether the improvements over generaldomain BERT are due to the aligned Word2Vec vectors, or just to the availability of additional vectors
in general, we perform an ablation study where we
replace the aligned vectors with their non-aligned
counterparts (by setting W = 1 in Eq. 1) or with
randomly initialized vectors. Table 3 (top) shows
that dev set F1 drops under these circumstances,
i.e., vector space alignment is important.

5

(1)

non-aligned
random init

SQuADBERT

Figure 1:
NER test set F1, transformed as
(x − BERT(ref) )/(BioBERTv1.0(ref) − BERT(ref) ).
“(ref)”: Reference scores from Lee et al. (2020). “(repro)”: Results of our reproduction experiments. Error
bars: Standard error of the mean.

4.3

NER task ID

Domain adaptation

We train Word2Vec with vector size dW2V =
dLM = 1024 on CORD-19 and/or PubMed+PMC.
2
www.huggingface.co/bert-large-uncasedwhole-word-masking-finetuned-squad

——–

size

33.04 58.24 65.87

GreenCovid- CORD-19 only
2GB 34.62 60.09 68.20
SQuADBERT CORD-19+PubMed+PMC 94GB 34.32 60.23 68.00

Table 3: Top: NER ablation study. Drop in dev set F1
(w.r.t. GreenBioBERT) when using non-aligned or randomly initialized word vectors instead of aligned word
vectors. Bottom: Results (%) on Deepset-AI CovidQA. EM (exact match) and F1 are evaluated with the
SQuAD scorer. “substr”: Predictions that are a substring of the gold answer. Much higher than EM, because many gold answers are not minimal answer spans
(see Appendix, “Notes on Covid-QA”, for an example).

The process takes less than an hour on CORD19 and about one day on the combined corpus,
again without the need for a GPU. Then, we update
SQuADBERT’s wordpiece embedding layer and
tokenizer, as described in Section 3. We refer to
the resulting model as GreenCovidSQuADBERT.
5.2

Results and discussion

Table 3 (bottom) shows that GreenCovidSQuADBERT outperforms general-domain SQuADBERT
on all measures.
Interestingly, the small
CORD-19 corpus is enough to achieve this result (compare “CORD-19 only” and “CORD19+PubMed+PMC”), presumably because it is specific to the target domain and contains the CovidQA context papers.

6

Conclusion

As a reaction to the trend towards high-resource
models, we have proposed an inexpensive, CPUonly method for domain-adapting Pretrained Language Models: We train Word2Vec vectors on
target-domain data and align them with the wordpiece vector space of a general-domain PTLM.
On eight biomedical NER tasks, we cover over
60% of the BioBERT – BERT F1 delta, at 5%
of BioBERT’s domain adaptation CO2 footprint
and 2% of its cloud compute cost. We have also
shown how to rapidly adapt an existing BERT QA
model to an emerging domain – the Covid-19 pandemic – without the need for target-domain Language Model pretraining or finetuning.
We hope that our approach will benefit practitioners with limited time or resources, and that it
will encourage environmentally friendlier NLP.

References
Emily Alsentzer, John Murphy, William Boag, WeiHung Weng, Di Jindi, Tristan Naumann, and
Matthew McDermott. 2019. Publicly available clinical BERT embeddings. In 2nd Clinical Natural
Language Processing Workshop, pages 72–78, Minneapolis, USA.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text.
In EMNLP-IJCNLP, pages 3606–3611, Hong Kong,
China.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171–4186, Minneapolis, USA.
Jesse Dodge, Suchin Gururangan, Dallas Card, Roy
Schwartz, and Noah A Smith. 2019. Show your
work: Improved reporting of experimental results.
In EMNLP-IJCNLP, pages 2185–2194, Hong Kong,
China.

Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2020.
BioBERT: A pretrained biomedical language representation model
for biomedical text mining.
Bioinformatics,
36(4):1234–1240.
Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter
Davis, Carolyn J Mattingly, Thomas C Wiegers, and
Zhiyong Lu. 2016. BioCreative V CDR task corpus:
a resource for chemical disease relation extraction.
Database, 2016.
Ilya Loshchilov and Frank Hutter. 2018. Fixing weight
decay regularization in Adam.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168.

Rezarta Islamaj Doğan, Robert Leaman, and Zhiyong
Lu. 2014. NCBI disease corpus: a resource for disease name recognition and concept normalization.
Journal of biomedical informatics, 47:1–10.

Timo Möller, Anthony Reina, Raghavan Jayakumar,
and Malte Pietsch. 2020. Covid-qa: A question &
answer dataset for covid-19.

Martin Gerner, Goran Nenadic, and Casey M Bergman.
2010. LINNAEUS: a species name identification
system for biomedical literature. BMC bioinformatics, 11(1):85.

Evangelos Pafilis, Sune P Frankild, Lucia Fanini,
Sarah Faulwetter, Christina Pavloudi, Aikaterini
Vasileiadou, Christos Arvanitidis, and Lars Juhl
Jensen. 2013. The SPECIES and ORGANISMS resources for fast and accurate identification of taxonomic names in text. PloS one, 8(6).

Xiaochuang Han and Jacob Eisenstein. 2019. Unsupervised domain adaptation of contextualized embeddings for sequence labeling. In EMNLP-IJCNLP,
pages 4229–4239, Hong Kong, China.
Kexin Huang, Jaan Altosaar, and Rajesh Ranganath.
2019a. ClinicalBERT: Modeling clinical notes and
predicting hospital readmission. arXiv preprint
arXiv:1904.05342.
Kexin Huang, Abhishek Singh, Sitong Chen, Edward T Moseley, Chih-ying Deng, Naomi George,
and Charlotta Lindvall. 2019b. Clinical XLNet:
Modeling sequential clinical notes and predicting
prolonged mechanical ventilation. arXiv preprint
arXiv:1912.11975.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction to the bio-entity recognition task at JNLPBA.
In International Joint Workshop on Natural Language Processing in Biomedicine and its Applications, pages 70–75.
Martin Krallinger, Obdulia Rabal, Florian Leitner,
Miguel Vazquez, David Salgado, Zhiyong Lu,
Robert Leaman, Yanan Lu, Donghong Ji, Daniel M
Lowe, et al. 2015. The CHEMDNER corpus of
chemicals and drugs and its annotation principles.
Journal of cheminformatics, 7(1):1–17.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In EMNLP, pages
2383–2392, Austin, USA.
Larry Smith, Lorraine K Tanabe, Rie Johnson nee
Ando, Cheng-Ju Kuo, I-Fang Chung, Chun-Nan
Hsu, Yu-Shi Lin, Roman Klinger, Christoph M
Friedrich, Kuzman Ganchev, et al. 2008. Overview
of BioCreative II gene mention recognition.
Genome biology, 9(2):S2.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for
deep learning in NLP. In ACL, pages 3645–3650,
Florence, Italy.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. In NeurIPS, pages 5998–6008, Long
Beach, USA.
Hai Wang, Dian Yu, Kai Sun, Janshu Chen, and Dong
Yu. 2019. Improving pre-trained multilingual models with vocabulary expansion. In CoNLL, pages
316–327, Hong Kong, China.

Inexpensive Domain Adaptation of
Pretrained Language Models (Appendix)
Word2Vec training
We downloaded the PubMed, PMC and CORD-19
corpora from:
• https://ftp.ncbi.nlm.nih.gov/pub/
pmc/oa_bulk/ [20 January 2020, 68GB raw text]
• https://ftp.ncbi.nlm.nih.gov/pubmed/
baseline/ [20 January 2020, 24GB raw text]
• https://pages.semanticscholar.org/
coronavirus-research [17 April 2020, 2GB
raw text]

We extract all abstracts and text bodies and apply
the BERT basic tokenizer (a rule-based word tokenizer that standard BERT uses before wordpiece
tokenization). Then, we train CBOW Word2Vec3
with negative sampling. We use default parameters except for the vector size (which we set to
dW2V = dLM ).
Experiment 1: Biomedical NER
Pretrained models
General-domain BERT and BioBERTv1.0 were
downloaded from:
• www.storage.googleapis.com/bert_
models/2018_10_18/cased_L-12_H768_A-12.zip
• www.github.com/naver/biobertpretrained

Data
We downloaded the NER datasets by following instructions on www.github.com/dmis-lab/
biobert#Datasets. For detailed dataset statistics,
see Lee et al. (2020).
Preprocessing
We use Lee et al. (2020)’s preprocessing strategy:
We cut all sentences into chunks of 30 or fewer
whitespace-tokenized words (without splitting inside labeled spans). Then, we tokenize every chunk
S with T = TLM or T = T̂LM and add special
tokens:
X = [CLS] T (S) [SEP]
Word-initial wordpieces in T (S) are labeled as
B(egin), I(nside) or O(utside), while non-wordinitial wordpieces are labeled as X(ignore).
3

www.github.com/tmikolov/word2vec

Modeling, training and inference
We follow Lee et al. (2020)’s implementation
(www.github.com/dmis-lab/biobert): We add
a randomly initialized softmax classifier on top
of the last BERT layer to predict the labels. We
finetune the entire model to minimize negative log
likelihood, with the AdamW optimizer (Loshchilov
and Hutter, 2018) and a linear learning rate scheduler (10% warmup). All finetuning runs were done
on a GeForce Titan X GPU (12GB).
At inference time, we gather the output logits
of word-initial wordpieces only. Since the number
of word-initial wordpieces is the same for TLM (S)
and T̂LM (S), this makes mean-pooling the logits
straightforward.
Hyperparameters
We tune the batch size and peak learning rate on
the development set (metric: F1), using the same
hyperparameter space as Lee et al. (2020):
Batch size: [10, 16, 32, 64]4
Learning rate: [1 · 10−5 , 3 · 10−5 , 5 · 10−5 ]
We train for 100 epochs, which is the upper end
of the 50–100 range recommended by the original
authors. After selecting the best configuration for
every task and model (see Table 4), we train the
final model on the concatenation of training and
development set, as was done by Lee et al. (2020).
See Figure 2 for expected maximum development
set F1 as a function of the number of evaluated hyperparameter configurations (Dodge et al., 2019).
Experiment 2: Covid-19 QA
Pretrained model
We downloaded the SQuADBERT baseline from:
• www.huggingface.co/bert-largeuncased-whole-word-maskingfinetuned-squad

Data
We downloaded the Deepset-AI Covid-QA dataset
from:
• www.github.com/deepset-ai/COVIDQA/blob/master/data/questionanswering/COVID-QA.json [24 June 2020]
4

Since LINNAEUS and BC4CHEM have longer maximum
tokenized chunk lengths than the other datasets, our hardware
was insufficient to evaluate batch size 64 on them.

At the time of writing, the dataset contains 2019
questions and gold answer spans.5 Every question is associated with one of 147 research papers
(contexts) from CORD-19.6 Since we do not do
target-domain finetuning, we treat the entire dataset
as a test set.
Preprocessing
We tokenize every question-context pair (Q, C)
with T = TLM or T = T̂LM , which yields
(T (Q), T (C)). Since T (C) is usually too long
to be digested in a single forward pass, we define a sliding window with width and stride N =
(Q)|
floor( 509−|T
). At step n, the “active” win2
(l)

(r)

dow is between an = (n − 1)N + 1 and an =
min(|C|, nN ). The input is defined as:
X (n) = [CLS] T (Q) [SEP]
T (C)a(l) −p(l) :a(r) +p(r) [SEP]
n

(l)

n

n

n

(r)

pn and pn are chosen such that |X (n) | = 512,
and such that the active window is in the center of
the input (if possible).
Modeling and inference
Feeding X (n) into the QA model yields start log(n)
its h0 (start,n) ∈ R|X | and end logits h0 (end,n) ∈
(n)
R|X | . We extract and concatenate the slices that
correspond to the active windows of all steps:
h(∗) ∈ R|T (C)|
h(∗) = [h

0(∗,1)
(l)

(r)

a1 :a1

;...;h

0(∗,n)
(l)

(r)

an :an

; . . .]

Next, we map the logits from the wordpiece level
to the word level. This allows us to mean-pool the
outputs of TLM and T̂LM even when |TLM (C)| 6=
|T̂LM (C)|.
Let ci be a word in C and let T (C)j:j+|T (ci )| be
the corresponding wordpieces. The start and end
logits of ci are:
(∗)

oi

(∗)

= maxj≤j 0 ≤j+|T (ci )| [hj 0 ]

Finally, we return the answer span Ck:k0 that
(start)
(end)
maximizes ok
+ ok0 , subject to the con0
straints that k does not precede k and the answer
contains no more than 500 characters.
5

In an earlier version of the paper, we reported results
on a preliminary version of Deepset-AI Covid-QA, which
contained 1380 questions.
6
www.github.com/deepset-ai/COVIDQA/issues/103

Notes on Covid-QA
There are some important differences between
Covid-QA and SQuAD, which make the task challenging:
• The Covid-QA contexts are full documents
rather than single paragraphs. Thus, the correct answer may appear several times, often
with slightly different wordings. But only a
single occurrence is annotated as correct, e.g.:
Question: What was the prevalence of Coronavirus OC43 in community samples in
Ilorin, Nigeria?
Correct: 13.3% (95% CI 6.9-23.6%) # from
main text
Predicted: 13.3%, 10/75 # from abstract
• SQuAD gold answers are defined as the
“shortest span in the paragraph that answered
the question” (Rajpurkar et al., 2016, p. 4),
but many Covid-QA gold answers are longer
and contain non-essential context, e.g.:
Question: When was the Middle East Respiratory Syndrome Coronavirus isolated
first?
Correct: (MERS-CoV) was first isolated in
2012, in a 60-year-old man who died in
Jeddah, KSA due to severe acute pneumonia and multiple organ failure
Predicted: 2012
These differences are part of the reason why the
exact match score is lower than the word-level F1
score and the substring score (see Table 3, bottom,
main paper).

Biomedical NER task
BC5CDR-disease
NCBI-disease
BC5CDR-chem
BC4CHEMD
BC2GM
JNLPBA
LINNAEUS
Species-800

(ID)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)

BERT (repro)
hyperparams dev set F1

BioBERTv1.0 (repro)
hyperparams dev set F1

GreenBioBERT
hyperparams dev set F1

32, 3 · 10−5
32, 3 · 10−5
64, 3 · 10−5
16, 1 · 10−5
32, 1 · 10−5
32, 5 · 10−5
16, 1 · 10−5
32, 1 · 10−5

10, 1 · 10−5
32, 1 · 10−5
32, 1 · 10−5
32, 1 · 10−5
64, 3 · 10−5
32, 5 · 10−5
32, 1 · 10−5
32, 1 · 10−5

32, 1 · 10−5
10, 3 · 10−5
10, 1 · 10−5
16, 1 · 10−5
64, 3 · 10−5
10, 3 · 10−5
10, 1 · 10−5
16, 1 · 10−5

82.12
87.52
91.00
88.02
83.91
85.18
96.67
72.70

85.15
87.99
93.36
89.35
85.54
85.30
97.22
77.34

83.90
88.43
92.59
88.53
84.25
85.10
96.49
75.93

Expected max dev set F1

Table 4: Best hyperparameters (batch size, peak learning rate) and best dev set F1 per NER task and model. BERT
(repro) and BioBERTv1.0 (repro) refer to our reproduction experiments.

1.0
84
0.8
82
0.6 3
0.4
85
0.2
84
0.0
0.0 3

(1)

88
87

6 9 12
(5)

(2)

93
92
91

3 6 9 12
85

(6)

(3)

89
88
87

3 6 9 12
97
96
95

(7)

(4)
3

76
74
72

6

9

(8)

84
6 9 120.2
3 6 0.4
9 12
3 6 9 0.8 3 6 9 12
0.6
1.0
Number of hyperparameter configurations evaluated
BERT (repro)
BioBERTv1.0 (repro)
GreenBioBERT

Figure 2: Expected maximum F1 on NER development sets as a function of the number of evaluated hyperparameter configurations. Numbers in brackets are NER task IDs (see Table 4).

