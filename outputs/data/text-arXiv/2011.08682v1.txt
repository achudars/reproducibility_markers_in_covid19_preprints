SeekNet : Improved Human Instance Segmentation via
Reinforcement Learning Based Optimized Robot Relocation

arXiv:2011.08682v1 [cs.CV] 17 Nov 2020

Venkatraman Narayanan, Bala Murali Manoghar, Rama Prashanth RV and Aniket Bera
University of Maryland, College Park, USA

Abstract
Amodal recognition is the ability of the system to detect
occluded objects. Most state-of-the-art Visual Recognition
systems lack the ability to perform amodal recognition. Few
studies have achieved amodal recognition through passive
prediction or embodied recognition approaches. However,
these approaches suffer from challenges in real-world applications, such as dynamic objects. We propose SeekNet
, an improved optimization method for amodal recognition
through embodied visual recognition. Additionally, we implement SeekNet for social robots, where there are multiple
interactions with crowded humans. Hence, we focus on occluded human detection & tracking and showcase the superiority of our algorithm over other baselines. We also experiment with SeekNet to improve the confidence of COVID-19
symptoms pre-screening algorithms using our efficient embodied recognition system.

Figure 1: SeekNet

the shortcomings of passively detecting occluded objects.
The algorithms suffer from the following challenges:
• They only work on a single target object at a time and
expect only one instance of the target object within the
searchable area.

1. Introduction
Recent technologies in the field of robotics and AI have
made remarkable advancements in the field of autonomous
driving, mobile robots, social robots, etc. Most systems rely
on a robust visual recognition system. Many recent work
have improved Visual Recognition tasks such as Object
recognition [28, 34, 39], Semantic Segmentation [6, 22, 35].
Very few efforts have focused on amodal object recognition [40] and segmentation [12, 27, 42]. Amodal Visual
Recognition is the ability of the system to perceive occluded
objects [26].
Some attempts were made to solve amodal recognition
tasks by modeling it as an embodied recognition problem [7, 40]. These methods utilize the locomotive ability
of a mobile robot to solve amodal recognition, rather than
passively attempting to predict the occluded object. Such a
system works specifically well for social robots since most
of the environment is occluded from the robots’ FOV.
The works [7, 40] have enabled a method to overcome

• They lack the ability to track a dynamic object.
We propose SeekNet to overcome such shortcomings
with an ability to track dynamic objects and achieve embodied recognition tasks. Since object detection is a vast topic
and targeted algorithms are required to address the many
sub-groups (or classes) under object detection, we mainly
focus on embodied recognition for social robots. We test
our algorithm on a social robot, and since social robots primarily interact with dynamic humans in the environment,
we designed an embodied recognition system that targets
humans in the environment. To this end, our main contributions are:
• We present a novel approach to perform the amodal
segmentation of humans in a crowded environment and
track them further.
• We provide an improved robot navigation system,
1

based on policy networks, that explores a predefined
environment to track the humans in it.

Though there have been many advancements in segmentation, only a few of them achieve frame rates that are sufficient for the navigation domain. The Box2Pix [37] is twice
as fast as any other existing approach. Their approach represents a balanced fusion of object and pixel knowledge,
which produces accurate instance segmentation with an efficient single FCN forward pass and a single image pass
post-processing.

• We demonstrate an application with our SeekNet
to improve the pre-screening algorithms aimed at
COVID-19 detection, which is currently a global pandemic.
COVID-19 or Coronavirus cases have spiked across the
world. To slow the spread of COVID-19, the CDC (Centers for Disease Control and Prevention) in the US and
WHO (World Health Organization) are encouraging people
to practice self-quarantine if they symptoms of COVID-19
to slow down the outbreak to reduce the chance of infection among high-risk populations to reduce the burden on
the health care system. Even with an aggressive testing process, it is not practical to frequently get tested at a required
rate. Hence, it will be highly helpful if fast pre-screening
becomes available to identify potential COVID-19 carriers.
Many contactless methods use a variety of sensors for
pre-screening COVID-19 symptoms. We employ social
robots in indoor environment to achieve higher confidence
during the COVID-19 screening process, ensure better coverage and higher screening frequency. Furthermore, rather
than relying on one modality, we fuse multiple modalities to
simultaneously measure the vital signs, like body temperature, respiratory rate, heart rate, etc., to improve the screening accuracy.
The paper is organized as follows: Section 2 presents related work, section 3 gives an overview of of our pipeline,
section 4 describes each stage of our pipeline in detail, and
finally in section 5 we evaluate theoretical and practical results of our work.

2.2. Human Detection
For social navigation, the problem of occlusion is more
pronounced because of frequent human-human interaction,
even in a sparse crowd [8, 23]. For pedestrians, detection in
such complex scenarios is usually achieved by using pose
estimation techniques [3, 5]. These networks are two-stage
networks where the first stage extracts the skeleton information. The second stage combines the pixel classification
and poses information to generate a pixel map of individual humans. The approaches by [36, 41] generate accurate
masks even with heavy occlusion. Though the results are
very promising, these networks are computationally intensive, and the execution time exponentially increases with
the number of humans in the scene. Thus these methods are
not suitable for integrating with a navigation scheme where
real-time execution is necessary. To this end, our algorithm,
SeekNet uses simpler instance segmentation methods and
leverages the movement capability of robots to achieve occlusion free masks.

2.3. Embodied Segmentation
To better understand the shape of an object in case of
occlusion, Lu Qi et al. [27] trained a model to estimate the
hidden region. Though they produce good results when the
shape of the object is complex, they are far from humanlevel performance. In order to accurately determine the
shape of an object, Yang et al. [40] imitate the human ability to move and control the view angle actively. They introduced the task of Embodied amodal segmentation and
addressed the problem using Embodied Mask-RCNN. This
approach is trained for static objects, but moving humans
are the prime targets in the case of social navigation. To
this end, our approach SeekNet is trained for such dynamic
environments and can maintain a constant distance with the
target.

2. Related Work
2.1. Instance Segmentation
From a social robot navigation perspective, we have
to treat humans and other objects differently. Also, we
have to draw boundaries between different humans, and
thus instance segmentation becomes an integral part of our
pipeline. Broadly speaking, there are three approaches for
instance segmentation. The first approach generates a pixel
map of separate objects using the output from the object detection task. Good results have been achieved using MaskRCNN [13], and there have been several improvements over
this model to improve execution speeds. Another approach
is to use individual networks to collect high-level object information and low level per pixel information. The results
of the two networks are combined to form a pixel map of
individual objects. The third method uses a single FCN.
The features generated by this network are post-processed
to obtain both object level and pixel-level information.

2.4. AI models for COVID Prognosis and Diagnosis
There are numerous AI models available to classify between an infectious person and a non-infectious person. The
improvement in the accuracy of these models is also fastpaced. [9,31,38] explore different AI approaches related to
COVID-19 detection. However, these models do not scale
well, and there is a high risk of bias that raises concerns to
be used in daily practice. To mitigate this, [32], use multiple
2

sensors to improve accuracy. Various methods to fuse different algorithms have also been explored in [24,25]. These
methods use late fusion due to unavailability or sparse availability of multi-modal data such as temperature, heart rate,
respiratory rate, cough signature, etc., for the same human
subject. Uniformly all these models need a specific environment setting to get the best accuracy and are far from
deployment in public places. Our model SeekNet could be
leveraged for deploying these contactless diagnosis models
in public places as it can seclude a single person in a group
and keep him in focus for the entire course of measurement
and access the state of health of a person.

2.5. Multimodal Behavior Analysis in Social
Robotics

Figure 2: Overview: We present SeekNet, an improved Human Instance Segmentation via Reinforcement Learning Based Optimized
Robot Relocation. We use our algorithm to improve COVID-19 detection symptoms in the wild.

In order to coexist with humans, social robots need to
understand their emotional state and incorporate socially
acceptable behavior. Emotion recognition from features
such as facial expressions, gestures, and walks has been addressed in the literature surveyed in [1,2,4,29]. Multimodal
and context-aware affect recognition models are also available for that purpose as well [19–21, 33].

pipeline. We build on [37] and adapt their network architecture to generate three types of outputs: instance segmentation, object classification, object classification confidence.
We retain the [37] modifications from the GoogLeNet’s
inception module. The additional inception modules added
to the backbone achieve a larger receptive field that helps in
identifying humans near the robot. To predict the actual box
parameters and box classes, we add 1×1 convolutions from
different levels of backbone layers and recursively compute
the receptive field theoretically as used in [37].

3. Overview and Methodology
The primary goal for SeekNet is to learn an optimal solution that improves the weakly learned detectors. Specifically, our navigation pipeline aims at improving Visual
Recognition algorithms in an Embodied Recognition setup.
Our SeekNet relies on an RGB camera onboard a mobile
robot with other components necessary for robot perception
and navigation. Our Visual Perception system consists of
Amodal Recognition and Amodal Segmentation to identify
potentially occluded objects, refined by our novel navigation system to improve the detection confidence (or accuracy) by maneuvering the robot to a more advantageous position. Our system relies on a mobile robot’s ability to reposition itself to better complete the Visual Recognition task
at hand. Figure 2 provides a brief overview of our SeekNet
system.
The following subsections will describe our approach in
detail. We discuss the details of the datasets used to train
our perception and policy network, along with other processing techniques (if any) used. We also provide details on
our Amodal Recognition and Segmentation routine, where
we also briefly discuss our human detection and segmentation routine from an RGB camera. Finally, we discuss our
navigation system.

RFout = (RFin − 1) × s + k

where RF is the input and output receptive field, s is
the stride of the corresponding layer, and k is the kernel
size. We maintain the theoretical receptive fields to be twice
as that of the maximum value of height or width of prior
boxes while assigning it to a specific layer. This is done to
compensate for the reduction in the receptive field during
training, as suggested by [18].
As used by [37] semantic class and center offset class
is predicted using skip connections from inception modules
of corresponding layers. It consists of 1 × 1 convolutions
with element-wise addition, deconvolutions to upscale the
low-resolution feature maps sequentially.
3.1.2

3.1. Segmentation
3.1.1

(1)

Loss Formulation

We use a hybrid loss (equation 2) to train our network. The
hybrid loss is a weighted combination of losses for each
of the sub-tasks (semantic, offsets, bounding box, classification) performed by our network using the approach presented by Kendall et al. [15] to learn task uncertainties σ

Network Structure

Since we are deploying SeekNet on a robot, the model has
to achieve frame rates sufficient to use in the navigation
3

3.2. Embodied Recognition
Ltotal =
+
+

1
2
σsem

1
2
σof
f

1

We train a policy network to achieve the embodied
recognition task. Based on our amodal recognition’s human detection confidence (section 3.1.1), we identify potential goal points to pursue and refine using our embodied recognition system. Our identification process is based
on weak detection confidence below a threshold (λ). We
build our policy network upon [11, 40]. Our policy network receives the LiDAR scans and each human segmentation masks from the amodal recognition system and outputs
probabilities over the action space considered for the navigation task.
Action Space: The action space is a set of permissible
robot velocities in continuous space. The action velocities
consists of translational and rotational velocity. We set the
bounds on translational velocity, v ∈ [0.0, 1.0] and rotational velocity, w ∈ [−1.0, 1.0] to accomodate the robot
kinematics. We sample actions at step t using equation 6

· Lsem + log σsem
· Lof f + log σof f
(2)
· Lbbox + log σbbox

2
σbbox
1
+ 2 · Lcls + log σcls
σcls

We use a standard cross-entropy loss for semantic segmentation, Lsem . L2 regression loss for center offset vectors Lof f sets . Both semantic segmentation loss and center
offset loss are normalized over the number of valid pixels.
Lbbox is the L2 regression loss for bounding box parameters (xmin, ymin, xmax, ymax). For classification Lcls , we
use Focal Loss [17] to counter the imbalance between foreground and background classes.
3.1.3

at = π(l0 , l1 , l2 , h0 , h1 , ..., hi )

SSD Adaptation and Instance Segmentation

where l0 , l1 , l2 represent the three consecutive processed LiDAR scan frames and h0 , h1 , ..ht represent the historical
and human segmentation masks concatenated together.
Policy Network: The policy network has four components {fhuman , flidar , fact }. fhuman is for encoding
the human segmentation masks. We resize the masks to
244 × 244, and pass them to fhuman , which consists of
four 5 × 5 Conv, BatchNorm, ReLU. Each Conv block is
followed by a 2 × 2 MaxPool blocks, producing an encoded
human segmentation mask ztimg = ftraj ([h0 , h1 , hi ])
We process the three consecutive lidar frames by passed
them through two 1 × 1 Conv, followed by a 256D fullyconnected (FC) layer. The lidar frames are encoded as
ztlidar = fenc ([l0 , l1 , l2 ]). The fact is a multi-layer perceptron (MLP) network, with 1 128D FC hidden layer and
finally produces the action velocities. fact takes in the
encoded human trajectories, ztimg , lidar encodings, ztlidar ,
previous velocity, vt−1 , goal position, sg , and current robot
position, st , to predict the robot velocities at time t, given
by equation 6.

The OCHuman dataset has heavy occlusions, and for a
heavily occluded human, the bounding box will be small.
To detect such small objects, the IoU of the prior boxes
has to be reduced, resulting in bad object detection performance. So to mitigate the problem of low coverage, we
use the relative box parameter instead of IoU as suggested
by [37]. This also helps in the training process since we
match the loss (based on corner offsets) and generation metric in a common space. The relative change between bp rior
of size (xmin , ymin , xmax , ymax ) and annotated ground
truth box bGT is given by equation 3
s
dchange =

2
2
∆ytl
∆x2tl
∆ybr
∆x2br
+
+
+
hGT
wGT
hGT
wGT

(5)

(3)

where,
wGT = xmax − xmin
hGT = ymax − ymin
(4)
∆x and ∆y are the absolute difference in the two boxes’
x and y parameters, and tl, br represent top-right and
bottom-left positions.
In order to densely cover both small and large objects,
we use 21 prior boxes. The dimensions of prior boxes are
found by clustering, as suggested by [30].
We combine the output from three outputs: semantic class, center offset vectors, and object detection with
bounding boxes to generate instance segmentation output
as proposed in [37].

vt = fact ([ztimg , ztlidar , vt−1 , sg , st ])

(6)

vt is then sent to a linear layer with softmax to derive the
probability distribution over the action space, from which
the action is sampled. We learn {fhuman , flidar , fact } via
reinforcement learning.
Rewards: Our reward function for the policy network is
inspired from [11]. We aim to arrive at an optimal strategy
to avoid collisions during navigation while ensuring that we
improve the targeted object’s detection confidence (human).
4

The reward function to achieve the mentioned goals is given
in equation 7
t
rt = rct + rw
+ rht

encourage development of algorithms more suited for practical and real life situations.
JTA Dataset: JTA (Joint Track Auto) [10] dataset is a
massive collection of pedestrian pose estimation and tracking in urban scenarios. The data is created by exploiting
the highly photorealistic video game Grand Theft Auto V
developed by Rockstar North. The dataset contains 512
video clips from several scenarios in urban environments.
The dataset covers variation in illumination and a variety of
view angles. It also covers the indoor and outdoor scenarios
with natural actions lime sitting, running, chatting, etc., in a
typical crowded environment. The clips are precisely annotated with values of visible and occluded body parts, people
tracking with 2D and 3D coordinates.

(7)

The reward r at time t is a combination of reward for avoid
collisions, rc , reward for smooth movement, rw and reward
for improving the detection confidence, rh .
The penalty for colliding with obstacles is given by equation 8.

r
collision , if robot collides.
rct =
(8)
0,
Otherwise.
For ensuring smooth navigation, the penalty for large rotational velocities is given by equation 9.

w |wt |, if |wt | > 0.7.
w
t
rw
=
(9)
0,
Otherwise.

4.3. Implementation Details

In our implementation, we use rarrival = 15, wg =
2.5, rcollision = −15, ww = −0.1, rp = 2.5, rp =
−0.5, ξ = 0.1

Amodal Recognition: We train our pipeline on dataset
described in section (4.2) with a train-validation split of
90%-10%. We use ADAM [16] optimizer, with decay parameters of (β1 = 0.9 and β2 = 0.999) to train our networks. We set the initial learning rate as 0.009 with 10%
decay every 250 epochs. The models were trained with the
hybrid loss detailed in section 3.2. We used 2 Nvidia RTX
2080 Ti GPUs having 11GB of GPU memory each and 64
GB of RAM to perform our experiments.
Embodied Recognition: We train our embodied recognition policy network on a simulation environment generated using Stage Mobile Robot Simulator. We generate
multiple scenarios (see in figure 3) with obstacles to train
our policy network. We use RMSProp [14] for training our
policy network with learning rate 0.00004 and  = 0.00005.

4. Experiment and Results

4.4. Analysis on Amodal Recognition

4.1. Metrics

We tabulate the results from our experiments in Table 1.
We use scenarios 5 and 6 from figure 3 to perform our comparison studies. We report the metrics mentioned in section
4.1 for all the baselines described [40]. It can be seen that
our implementation has the best change in classification accuracy across all our experiments.

To ensure that we progressively reposition the robot to improve detection confidence on the targeted object, we reward the system based on 10. The penalty is only applied
when the robot is actively pursuing a target.

r , if phi > phi .
p
t
t−1
(10)
ret =
rn , Otherwise.

We evaluate our amodal recognition efficiency based on
classification accuracy (Acccls ), and segmentation accuracy
as mean Intersection-over-Union (IoU) on the first frame of
detection. We also report the tracking accuracy (Acctr ) to
evaluate our amodal recognition system. We evaluate the
embodied recognition system in terms of change in classification accuracy (∆hacc ).

4.5. Using SeekNet for COVID-19 screening
As mentioned earlier, we use SeekNet for improving
the existing algorithms for pre-screening COVID-19 symptoms. We perform the experiment similar to our Embodied
Human Detection & Segmentation routine explained above,
with the exception that we focus on the COVID-19 symptom detection confidence instead of the human detection
confidence. As explained in section 1, we use an ensemble
of algorithms that screens for COVID-19 symptoms from a
multitude of sensors. In table 2, we report the learning from
our experiment. We see that similar to our embodied human
detection experiment, we see significant boost COVID-19

4.2. Datasets
In this work, we use datasets specially designed for detecting humans with heavy occlusions.
OCHuman:OCHuman [41] is a massive dataset designed for all three tasks: detection, pose estimation and
instance segmentation that are most important human related tasks. This dataset captures severe occlusion between
human bodies is often encountered in life. This dataset contains 8110 detailed annotated human instances within 4731
images. This dataset primarily emphasises on occlusions to
5

Moving Path

Amodal Recognition

Training

Testing

Acccls

mIoU

Acctr

Passive
ShortestPath
ShortestPath
ShortestPath
ShortestPath
SeekNet (ours)

Passive
Passive
RandomPath
ShortestPath
SeekNet (ours)
SeekNet (ours)

87.5
87.4
87.5
87.7
87.6
87.6

72.4
72.5
72.6
72.5
72.6
72.5

71.5
71.6
71.5
71.4
71.6
21.6

Embodied Recognition
∆hacc after ms
80
160
320
87.6
87.8
87.9
88.2
88.3

88.3
88.2
88.1
89.1
89.7

90.1
89.1
89.8
90.3
90.5

Table 1: Comparison of Amodal Human Segmentation and Tracking.

5. Conclusion, Limitations and Future Work
Metric

Passive/Passive

SeekNet /SeekNet (ours)

Acccls
mIoU
Acctr
AccC19
∆hacc after 80 ms
∆hacc after 160 ms
∆hacc after 320 ms

87.4
72.5
71.6
67.2
-

87.9
72.9
72.7
71.3
88.1
88.5
89.1

We propose SeekNet to overcome shortcomings of passively detecting occluded objects with an ability to track
dynamic objects and achieve embodied recognition tasks.
Since object detection is a vast topic and targeted algorithms
are required to address the many sub-groups (or classes)
under object detection, we mainly focused on embodied
recognition for social robots. We tested our algorithm on a
social robot, and since social robots primarily interact with
dynamic humans in the environment, we designed an embodied recognition system that targets humans in the environment.

Table 2: Comparison of Amodal Human Segmentation and Tracking.

References
screening confidence when using our algorithm. It is important to note that we still use amodal human detection
pipeline to detect and target human for COVID-19 symptom screening.

[1] Abhishek Banerjee, Uttaran Bhattacharya, and Aniket Bera.
Learning unseen emotions from gestures via semanticallyconditioned zero-shot perception with adversarial autoencoders. arXiv preprint arXiv:2009.08906, 2020. 3
[2] Aniket Bera, Tanmay Randhavane, Rohan Prinja, Kyra Kapsaskis, Austin Wang, Kurt Gray, and Dinesh Manocha. How
are you feeling? multimodal emotion learning for sociallyassistive robot navigation. In 2020 15th IEEE International
Conference on Automatic Face and Gesture Recognition (FG
2020)(FG), pages 894–901. 3
[3] Uttaran Bhattacharya, Trisha Mittal, Rohan Chandra, Tanmay Randhavane, Aniket Bera, and Dinesh Manocha. Step:
Spatial temporal graph convolutional networks for emotion
perception from gaits. In AAAI, pages 1342–1350, 2020. 2
[4] Uttaran Bhattacharya, Nicholas Rewkowski, Pooja Guhan,
Niall L Williams, Trisha Mittal, Aniket Bera, and Dinesh
Manocha. Generating emotive gaits for virtual agents using
affect-based autoregression. ISMAR, 2020. 3
[5] Uttaran Bhattacharya, Christian Roncal, Trisha Mittal, Rohan Chandra, Aniket Bera, and Dinesh Manocha. Take an
emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping. 2019. 2
[6] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,
Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.

Figure 3: Training Navigation Policy

6

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

Panoptic-deeplab: A simple, strong, and fast baseline for
bottom-up panoptic segmentation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 12475–12485, 2020. 1
Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee,
Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 2054–2063,
2018. 1
Vishnu Sashank Dorbala, Arjun Srinivasan, and Aniket Bera.
Can a robot trust you? a drl-based approach to trust-driven
human-guided navigation. arXiv preprint arXiv:2011.00554,
2020. 2
Andre Esteva, Alexandre Robicquet, Bharath Ramsundar,
Volodymyr Kuleshov, Mark DePristo, Katherine Chou,
Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean.
A guide to deep learning in healthcare. Nature medicine,
25(1):24–29, 2019. 2
Matteo Fabbri, Fabio Lanzi, Simone Calderara, Andrea
Palazzi, Roberto Vezzani, and Rita Cucchiara. Learning to
detect and track visible and occluded body joints in a virtual world. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 430–446, 2018. 5
Tingxiang Fan, Xinjing Cheng, Jia Pan, Dinesh Manocha,
and Ruigang Yang.
Crowdmove: Autonomous mapless navigation in crowded scenarios.
arXiv preprint
arXiv:1807.07870, 2018. 4
Patrick Follmann, Rebecca Kö Nig, Philipp Hä Rtinger,
Michael Klostermann, and Tobias Bö Ttger. Learning to see
the invisible: End-to-end trainable amodal instance segmentation. In 2019 IEEE Winter Conference on Applications of
Computer Vision (WACV), pages 1328–1336. IEEE, 2019. 1
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision, pages 2961–2969, 2017. 2
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky.
Neural networks for machine learning lecture 6a overview
of mini-batch gradient descent. Cited on, 14(8), 2012. 5
Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task
learning using uncertainty to weigh losses for scene geometry and semantics. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 7482–7491,
2018. 3
Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 5
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer
vision, pages 2980–2988, 2017. 4
Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel.
Understanding the effective receptive field in deep convolutional neural networks. In Advances in neural information
processing systems, pages 4898–4906, 2016. 3
Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket
Bera, and Dinesh Manocha. Emotions don’t lie: An audiovisual deepfake detection method using affective cues. In

[20]

[21]

[22]

[23]

[24]

[25]

[26]
[27]

[28]

[29]

[30]

[31]

[32]

7

Proceedings of the 28th ACM International Conference on
Multimedia, pages 2823–2832, 2020. 3
Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket
Bera, and Dinesh Manocha. M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech
cues. In AAAI, pages 1359–1367, 2020. 3
Trisha Mittal, Pooja Guhan, Uttaran Bhattacharya, Rohan
Chandra, Aniket Bera, and Dinesh Manocha. Emoticon:
Context-aware multimodal emotion recognition using frege’s
principle. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 14234–
14243, 2020. 3
Rohit Mohan and Abhinav Valada. Efficientps: Efficient
panoptic segmentation. arXiv preprint arXiv:2004.02307,
2020. 1
Venkatraman Narayanan,
Bala Murali Manoghar,
Vishnu Sashank Dorbala, Dinesh Manocha, and Aniket
Bera. Proxemo: Gait-based emotion learning and multiview proxemic fusion for socially-aware robot navigation,
2020. 2
Toshiaki Negishi, Shigeto Abe, Takemi Matsui, He Liu,
Masaki Kurosawa, Tetsuo Kirimoto, and Guanghao Sun.
Contactless vital signs measurement system using rgbthermal image sensors and its clinical screening test on patients with seasonal influenza. Sensors, 20(8):2171, 2020.
3
T. Negishi, G. Sun, S. Sato, H. Liu, T. Matsui, S. Abe, H.
Nishimura, and T. Kirimoto. Infection screening system using thermography and ccd camera with good stability and
swiftness for non-contact vital-signs measurement by feature
matching and music algorithm. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC), pages 3183–3186, 2019. 3
Stephen E Palmer. Vision science: Photons to phenomenology. MIT press, 1999. 1
Lu Qi, Li Jiang, Shu Liu, Xiaoyong Shen, and Jiaya Jia.
Amodal instance segmentation with kins dataset. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 3014–3023, 2019. 1, 2
Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors:
Detecting objects with recursive feature pyramid and switchable atrous convolution. arXiv preprint arXiv:2006.02334,
2020. 1
Tanmay V Randhavane, Aniket Bera, Emily Kubin, Kurt
Gray, and Dinesh Manocha. Modeling data-driven dominance traits for virtual characters using gait analysis.
IEEE Transactions on Visualization and Computer Graphics, 2019. 3
Joseph Redmon and Ali Farhadi. Yolo9000: better, faster,
stronger. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 7263–7271, 2017. 4
KC Santosh. Ai-driven tools for coronavirus outbreak: need
of active learning and cross-population train/test models on
multitudinal/multimodal data. Journal of medical systems,
44(5):1–5, 2020. 2
Armote Somboonkaew, Sirajit Vuttivong, Panintorn Prempree, Ratthasart Amarit, Sataporn Chanhorm, Kosom

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

Chaitavon, Supanit Porntheeraphat, and Sarun Sumriddetchkajorn. Temperature-compensated infrared-based lowcost mobile platform module for mass human temperature
screening. Appl. Opt., 59(17):E112–E117, Jun 2020. 2
Kyu-Seob Song, Young-Hoon Nho, Ju-Hwan Seo, and
Dong-soo Kwon. Decision-level fusion method for emotion
recognition using multimodal emotion recognition information. In 2018 15th International Conference on Ubiquitous
Robots (UR), pages 472–476. IEEE, 2018. 3
Mingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet:
Scalable and efficient object detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10781–10790, 2020. 1
Andrew Tao, Karan Sapra, and Bryan Catanzaro. Hierarchical multi-scale attention for semantic segmentation. arXiv
preprint arXiv:2005.10821, 2020. 1
Subarna Tripathi, Maxwell Collins, Matthew Brown,
and Serge Belongie.
Pose2instance: Harnessing keypoints for person instance segmentation. arXiv preprint
arXiv:1704.01152, 2017. 2
Jonas Uhrig, Eike Rehder, Björn Fröhlich, Uwe Franke, and
Thomas Brox. Box2pix: Single-shot instance segmentation
by assigning pixels to object boxes. In 2018 IEEE Intelligent
Vehicles Symposium (IV), pages 292–299. IEEE, 2018. 2, 3,
4
Anwaar Ulhaq, Asim Khan, Douglas Gomes, and Manoranjan Pau. Computer vision for covid-19 control: A survey.
arXiv preprint arXiv:2004.09420, 2020. 2
Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,
Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet:
A new backbone that can enhance learning capability of cnn.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops, pages 390–391,
2020. 1
Jianwei Yang, Zhile Ren, Mingze Xu, Xinlei Chen, David J
Crandall, Devi Parikh, and Dhruv Batra. Embodied amodal
recognition: Learning to move to perceive objects. In Proceedings of the IEEE International Conference on Computer
Vision, pages 2040–2050, 2019. 1, 2, 4, 5
Song-Hai Zhang, Ruilong Li, Xin Dong, Paul Rosin, Zixi
Cai, Xi Han, Dingcheng Yang, Haozhi Huang, and Shi-Min
Hu. Pose2seg: Detection free human instance segmentation.
In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 889–898, 2019. 2, 5
Ziheng Zhang, Anpei Chen, Ling Xie, Jingyi Yu, and
Shenghua Gao. Learning semantics-aware distance map with
semantics layering network for amodal instance segmentation. In Proceedings of the 27th ACM International Conference on Multimedia, pages 2124–2132, 2019. 1

8

