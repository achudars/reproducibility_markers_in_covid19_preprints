Dual-Consistency Semi-Supervised Learning
with Uncertainty Quantification for COVID-19
Lesion Segmentation from CT Images

arXiv:2104.03225v1 [eess.IV] 7 Apr 2021

Yanwen Li*1 , Luyang Luo*2 ,
Huangjing Lin1,2 , Hao Chen3 , Pheng-Ann Heng2,4
1

Imsight AI Research Lab, Shenzhen, China
Department of Computer Science and Engineering,
The Chinese University of Hong Kong, Hong Kong, China
3
Department of Computer Science and Engineering,
The Hong Kong University of Science and Technology, Hong Kong, China.
4
Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine
Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese
Academy of Sciences, China
2

Abstract. The novel coronavirus disease 2019 (COVID-19) characterized by atypical pneumonia has caused millions of deaths worldwide. Automatically segmenting lesions from chest Computed Tomography (CT)
is a promising way to assist doctors in COVID-19 screening, treatment
planning, and follow-up monitoring. However, voxel-wise annotations are
extremely expert-demanding and scarce, especially when it comes to
novel diseases, while an abundance of unlabeled data could be available.
To tackle the challenge of limited annotations, in this paper, we propose
an uncertainty-guided dual-consistency learning network (UDC-Net) for
semi-supervised COVID-19 lesion segmentation from CT images. Specifically, we present a dual-consistency learning scheme that simultaneously
imposes image transformation equivalence and feature perturbation invariance to effectively harness the knowledge from unlabeled data. We
then quantify both the epistemic uncertainty and the aleatoric uncertainty and employ them together to guide the consistency regularization
for more reliable unsupervised learning. Extensive experiments showed
that our proposed UDC-Net improves the fully supervised method by
6.3% in Dice and outperforms other competitive semi-supervised approaches by significant margins, demonstrating high potential in realworld clinical practice.
Keywords: COVID-19 · Semi-supervised learning · Uncertainty · Segmentation.

1

Introduction

By the end of 2020, the coronavirus disease 2019 (COVID-19) [36] characterized
by atypical pneumonia has spread over 220 countries and areas, infected more
1

The first two authors contributed equally.

2

Yanwen Li, Luyang Luo, et al.

than 81 million people, and caused near 1.8 million losses of lives1 . For early
screening of the COVID-19, chest computed tomography (CT) plays an vital role
as a noninvasive and fast technique, which is reported to have high sensitivity
for detecting COVID-19-related abnormal findings [6,1,13,7]. To improve the
screening efficiency and alleviate radiologists’ reading burden, various automatic
COVID-19 chest CT analysis methods have been proposed from whole-volume
classification and triaging [20,28,8,17,4], weakly-supervised lesion localization
[16,31], to accurate segmentation of lesion regions [5,27]. Among previous studies,
segmentation of COVID-19 often provides more accurate descriptions of the
lesions, which has significant potential in assisting doctors with the diagnosis,
treatment planning, and follow-up monitoring.
Currently, advanced segmentation methods are often fully supervised and require a large amount of pixel-wise or voxel-wise annotations. For medical images,
especially novel diseases like COVID-19, acquiring such annotations is extremely
expertise-demanded and time-consuming, while unlabeled data are often abundant with increasing positive cases. Therefore, semi-supervised learning (SSL)
that utilizes both labeled and unlabeled data is of great value to develop robust
and accurate COVID-19 lesion segmentation algorithms. Thus far, many SSLbased segmentation approaches have been developed and successfully applied to
various tasks [25]. A wide range of works [23,19,2,9,14] adopts the smoothness assumption that two data samples that are close in the input space share the same
label. This assumption is further expanded to the feature space of deep neural
networks, where similarities of deep feature maps are used for cluster assignment [29,26,21]. Despite the achievement, these approaches do not ensure robust
learning from samples with high confidence and low uncertainty. To reduce the
influence of uncertain samples, uncertainty estimation has been introduced into
the literature of SSL [34,33,30,15]. Nevertheless, semi-supervised segmentation
of COVID-19 lesions remains a challenging task, of which the annotations are
extremely scarce, and the lesions often have irregular and ambiguous contours.
To tackle the above challenges, we propose a novel deep neural network with
a dual-consistency learning scheme in both the image level and feature level
for COVID-19 lesion segmentation from chest CT scan volumes. Specifically, we
impose image-level transformation equivalence out of the observation that the
prediction of a sample should obtain the same transformation of the input. Meanwhile, we adopt feature-level perturbation invariance to a multi-decoder V-Net,
where auxiliary decoder paths take perturbated features as inputs and form output consistency with a main decoder. Dual-consistency comprehensively enforces
smoothness assumption into the SSL model from both input space and feature
space, and hence the network could learn more invariant representations to diverse input or feature variants. Moreover, deep neural networks could memorize
and easily overfit to noisy and uncertain contour points of COVID-19 lesions [35],
which leads to poor generalization in real-world clinical practice. Hence, we further introduce a novel uncertainty guidance to the consistency learning process.
Particularly, we quantify the epistemic uncertainty by computing the entropy of
1

https://covid19.who.int

UDC-Net: Uncertainty-guided Dual-Consistency Learning

3

Fig. 1: Overview of UDC-Net. Feature-level consistency (green region) is formed
by the main decoder’s prediction pU and auxiliary decoders’ predictions
{q1U , · · · , qkU }. Image-level consistency (blue region) is formed by pU and the
prediction p̃U of transformed image. The predictions from multiple decoders are
used to quantify the epistemic uncertainty ue and aleatoric uncertainty ua . The
two uncertainties are used together to guide the consistency learning process
(red region). A supervised loss is also used for the labeled data (orange region).

the mean predictions from the multiple decoders and the aleatoric uncertainty by
estimating the variance among the decoders. The two uncertainties are then used
together in an indicator function to filter out uncertain samples during training.
The proposed uncertainty-guided dual-consistency network (UDC-Net) is evaluated on a large-scale COVID-19 dataset with 852 whole-volume chest CT scans.
Extensive experiments show that our approach outperforms other competitive
SSL-based segmentation approaches, yielding state-of-the-art performance on
semi-supervised COVID-19 lesion segmentation.

2

Method

As illustrated in Fig. 1, our UDC-Net consists of a modified 3D multi-decoder VNet [18] as its backbone. Apart from the supervised loss, our method makes full
use of the unlabeled data by a feature-level consistency module and an image-

4

Yanwen Li, Luyang Luo, et al.

level consistency module. Moreover, both epistemic uncertainty and aleatoric
uncertainty are estimated to guide more robust consistency learning.
2.1

Dual-consistency Learning for Semi-supervised Segmentation

Image-level Consistency Learning via transformation equivalence of deep
segmentation models fseg indicates that while a transformation T (·) is applied to
an input image x, there should be fseg (T (x)) = T (fseg (x)) [32]. We conduct random transformation on the images to get the perturbated version T (x) as the input to our network. Subsequently, we have the corresponding prediction f (T (x))
given by the V-Net and the inverse transformation to the output T −1 (f (T (x))),
which should be consistent to the output of input data without transformation
f (x). Following the notations set before, let p = f (x) and p̃ = f (T (x)) ,we
introduce an image-level consistency regularization by minimizing the L2 loss
between the two versions of output:
LIC =

N
1 X
kpi − [T −1 (p̃)]i k22
N i=1

(1)

where i and N are the index and the total number of voxels, respectively.
Feature-level Consistency Learning via perturbation invariance can also
enrich the learned representation of the model [19]. Particularly, different perturbated versions of the same feature maps should maintain the same predictions.
Following [21], we append several auxiliary decoders to the V-Net and inject
shared encoder’s outputs with various types of perturbations. Each auxiliary decoder receives a different version of the perturbated feature map, while the main
decoder receives the un-perturbated feature map. Denoting the prediction from
the main decoder as p, the prediction from the k-th auxiliary decoder as q k , the
feature-level consistency is achieved by regularizing p and each q k as follows:
LFC =

N K
1 XX
kpi − qik k22
N · K i=1

(2)

k=1

where K is the total number of extra decoders. Particularly, each extra decoder
receives one type of perturbated features and is required to generate consistent
prediction with the main decoder.
2.2

Dual Uncertainty Quantification for Robust Learning

The perturbation of the hidden representations during the consistency learning
process could amplify the feature noises and uncertainty caused by the difficulty of accurately delineating the lesion contours of COVID-19. To this end, we
propose to simultaneously quantify the epistemic uncertainty and aleatoric uncertainty to estimate the multi-decoder confidence and consensus, respectively,

UDC-Net: Uncertainty-guided Dual-Consistency Learning

5

to guide more robust unsupervised learning.
Epistemic Uncertainty measures the uncertainty in the model when training
data is insufficient [10]. Previous works [34,15] used the entropy of the mean
prediction of multiple perturbated inputs from self-ensembling models to estimate the model uncertainty. In our case, the epistemic uncertainty can be easily
quantified with the main decoder and the K auxiliary decoders as below:
1
µi =
K +1

"

K
X

!
qik

#
+ pi

and uei = −µi logµi

(3)

k=1

where i indicates the voxel index, K is the total number of auxiliary decoders,
µ is the mean prediction, and ue is the estimated uncertainty.
Aleatoric Uncertainty is determined by the quality of the dataset such as
the data noise or inaccurate annotations [10]. Derived from the smoothness assumption, we deem the predictions of target classes should be less sensitive to
perturbation than that of noises [3,9]. Therefore, we quantify the aleatoric uncertainty ua as the standard deviation over predictions from different decoders
as follows:
v"
#
u K
u X
1
a
t
k
2
ui =
(qi − µi ) + (pi − µi )2
(4)
K +1
k=1

Supposing the average prediction of a suspicious infection area is high but the
outputs from different branches vary severely, this means that the area is sensitive to perturbation, which highly suggests a noisy sample. Therefore, ua essentially indicates the consensus among different decoders, which is complementary
with the epistemic uncertainty that measures the confidence of the model.
2.3

Uncertainty-guided Dual-consistency Learning for Segmentation

The quantified uncertainties are used to filter out uncertain samples and consequently guide the model to learn from more reliable unlabeled data. Denoting i
as the index of voxels on the prediction volume, the reliable voxels are selected
from the set Ω = {i|uia < τ a & uie < τ e }, where τ a and τ e are two thresholds.
The cross consistency loss among decoder paths is then back-propagated from
those voxels:
LUFC =

K X
X

kpi − qik k22

(5)

k=1 i∈Ω

Here, the uncertainty guidance is applied onto feature-level consistency learning
as the uncertainties are generated with feature perturbations. Thus, the total
loss for our uncertainty-guided dual-consistency learning UDC-Net for semisupervised lesion segmentation is as follows:

6

Yanwen Li, Luyang Luo, et al.

L = LS + αLIC + βLUFC

(6)

where LS is the supervised loss consists of a Dice loss and a cross entropy loss, α
and β are two hyper-parameters weighing the contributions from different losses.
During training, we first trained a supervised V-Net and then added the
extra decoders for finetuning with uncertainty-guided consistency learning. The
training process was terminated if the Dice coefficient on the validation dataset
stagnated. Adam [11] was used as the optimizer with an initial learning rate
of 0.001 and a learning decay rate of 0.95 per epoch. As widely adopted by
SSL works [24,21], α and β were set to be two sigmoid-shape monotonically
functions of the training steps with maximum of 1. The threshold τ e and τ a were
set to 0.34 and 0.12 after tuning on the validation set. For testing, we carried
out sliding window inference and took only the main decoder’s prediction. All
implementation was done with Pytorch [22] on an NVIDIA TITAN X GPU.

3

Experiments

3.1

Datasets and Evaluation Metrics

Datasets. In total, 852 chest CT volumes acquired from December 2019 to
April 2020 were collected and enrolled in this study, among which 144 were
voxel-annotated by four experienced radiologists. The labeled data were divided
into: (1) 65 cases as labeled training dataset; (2) 9 cases as the validation set;
and (3) 70 cases as the test dataset. Remained 708 chest CT scans were used as
the unlabeled training data.
Evaluation Metrics. We adopted three evaluation criteria to evaluate the segmentation performance: Dice Score(DSC), Jaccard similarity cofficient(Jaccard),
Average Symmetric Surface Distance(ASD).

Table 1: Ablation studies of different components in UDC-Net. All results are reported as validation/testing datasets. (FC: feature-level consistency; IC: imagelevel consistency; UE: epistemic uncertainty; UA: aleatoric uncertainty)
IC

Components
FC
UE
UA

X
X
X
X

X
X
X
X
X

X
X
X

X
X

DSC[%] ↑
70.0 / 71.1
70.3 / 73.5
71.4 / 75.6
71.9 / 76.7
72.2 / 76.9
72.2 / 77.0
72.7 / 77.4

Evaluation Metrics
Jaccard[%] ↑
ASD[mm] ↓
56.5 / 56.8
12.1 / 12.1
56.7 / 59.3
12.2 / 8.4
58.4 / 62.3
12.1 / 6.1
58.9 / 63.7
11.7 / 5.8
59.4 / 63.9
11.4 / 5.1
59.0 / 64.0
11.3 / 3.2
59.9 / 64.5
10.9 / 3.9

UDC-Net: Uncertainty-guided Dual-Consistency Learning

3.2

7

Ablation Study on Different Components

We conduct ablation studies to analyze the contributions of our proposed methods by removing certain components of UDC-Net. The quantitative results can
be seen in Table 1. Regarding the performance on testing set, with image-level
consistency (IC), the result shows 2.4% improvement in DSC, 2.5% improvement
in Jaccard, and 3.7 improvement in ASD (row 2 vs. row1) comparing to 3D VNet. Meanwhile, feature-level consistency (FC) regularization leads to a large
improvement of 4.5% in DSC, 5.5% in Jaccard, and 6.0 in ASD (row 3 vs. row1).
Unifying the dual consistencies further improves DSC and Jaccard with about
1% (row 4 vs. row 3), which indicates its necessity in semi-supervised learning
from the unlabeled data. Moreover, introducing uncertainty guidance (UE and
UA) to FC result in gains of 1.3% in DSC, 1.6% in Jaccard, and 1.0 in ASD (row
5 vs. row 3), which indicates the importance of uncertainty estimation. At the
last two rows, we show the method with dual-form uncertainty (row 6) achieves
better DSC and Jaccard with a comparable ASD to a single-uncertainty model
(row 7), further demonstrating that dual uncertainties are complementary for
guiding more robust learning.
3.3

Comparison with State-of-the-art Methods

We compare our method against other state-of-the-art semi-supervised segmentation approaches to demonstrate its efficacy above others. Several recent proposed models were implemented, including Mean-Teacher (MT) [24], Uncertaintyaware mean teacher [34], Transformation-consistent Self-ensembling Model (TCSM)
[12], and Cross Consistency Training (CCT) [21].

Table 2: Quantitative comparison with other semi-supervised methods.
Evaluation Metrics
DSC[%] ↑ Jaccard[%] ↑ ASD[mm] ↓
V-Net [18]
71.1
56.8
12.1
Mean Teacher [24]
72.5
58.2
11.3
UA-MT [34]
74.0
60.1
9.2
72.9
58.9
9.1
TCSM [12]
CCT [21]
75.6
62.3
6.1
UDC-Net (ours)
77.4
64.5
3.9
Methods

Quantitative comparison results are reported in Table 2. For a fair comparison, we implemented all methods with the 3D V-Net as backbone. As observed,
UDC-Net outperforms all other methods with at least 1.8% in Dice, 2.2% in
Jaccard, and 2.2 in ASD, showing outstanding unsupervised learning efficacy.
Qualitative comparison is illustrated by visualizing the segmentation results
in Figure 2. As demonstrated, Our UDC-Net delineates more accurate lesion

8

Yanwen Li, Luyang Luo, et al.
MT

UAMT

TCSM

CCT

Ours

Case1

Case2

Case3

Fig. 2: Qualitative comparison. Green and orange curves delineate the model
prediction and ground truth, respectively. Best viewed in color.

contours than other methods regarding diverse shapes and sizes of lesion. Visualization of the two uncertainties can be found in the supplementary.
3.4

Analysis on Efficacy of Leveraging Unlabeled Data

To further validate our UDC-Net’s effectiveness on leveraging the unlabeled data,
we evaluate its segmentation results with different ratios of training data. As reported in Table 3, the proposed UDC-Net consistently improves the baseline
V-Net with significant margins in both DSC, Jaccard, and ASD, whenever 32
or 65 labeled scans are provided. Moreover, the proposed approach also consistently outperforms CCT [21] (the best model among those compared with ours)
under all different scenarios. Notably, when less data are given, UDC-Net shows
comparable or even better results than CCT. For instance, UDC-net achieves
75.0% DSC, 61.5% Jaccard, and 4.8 ASD with 32 labeled scans and 140 unlabeled scans (3rd row), which is comparable to the performance of CCT with
double labeled scans (7th row). With 65 labeled scans and 140 unlabeled scans,
UDC-Net (8th row) shows superior performance than CCT with 5 times unlabeled data (9th row). These findings demonstrate that our method enables more
efficient unsupervised learning, suggesting a high potential for clinical usage.

4

Conclusions

In this paper, we present an uncertainty-guided dual-consistency learning method
for semi-supervised COVID-19 lesion segmentation from chest CT scans. Both
image-level transformation equivalence and feature-level perturbation invariance

UDC-Net: Uncertainty-guided Dual-Consistency Learning

9

Table 3: Quantitative performance comparison under different numbers of training labeled/unlabeled data.
Method
V-Net [18]
CCT [21]
UDC-Net (ours)
CCT [21]
UDC-Net (ours)
V-Net
CCT [21]
UDC-Net (ours)
CCT [21]
UDC-Net (ours)

# scans used
Evaluation Metrics
Labeled Unlabeled DSC[%] ↑ Jaccard[%] ↑ ASD[mm] ↓
32
0
70.4
56.0
4.3
32
140
74.4
60.7
5.8
32
140
75.0
61.5
4.8
32
708
75.2
61.6
7.0
32
708
76.9
64.0
4.4
65
0
71.1
56.8
12.1
65
140
75.1
61.8
5.8
65
140
76.6
63.5
5.4
65
708
75.6
62.3
6.0
65
708
77.4
64.5
3.9

are introduced to form dual consistency learning from unlabeled data. In addition, the dual-form uncertainty mechanism further improves the learning process with more reliable and robust guidance. Extensive experiments on a large
COVID-19 dataset demonstrate the efficiency and potential of our method in
real-world scenarios with limited annotations. Future work will include improving the method with more robust knowledge distillation and generalizing to other
semi-supervised learning tasks.
Acknowledgement. This work was supported by Key-Area Research and Development Program of Guangdong Province, China (2020B010165004), Hong
Kong Innovation and Technology Fund (Project No. ITS/311/18FP and Project
No. ITS/426/17FP.), and National Natural Science Foundation of China with
Project No. U1813204.

10

Yanwen Li, Luyang Luo, et al.

References
1. Ai, T., Yang, Z., Hou, H., Zhan, C., Chen, C., Lv, W., Tao, Q., Sun, Z., Xia, L.:
Correlation of chest ct and rt-pcr testing in coronavirus disease 2019 (covid-19) in
china: a report of 1014 cases. Radiology p. 200642 (2020)
2. Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., Raffel, C.A.:
Mixmatch: A holistic approach to semi-supervised learning. In: NeurIPS. pp. 5049–
5059 (2019)
3. Chapelle, O., Scholkopf, B., Zien, A.: Semi-supervised learning. IEEE TNNLS
20(3), 542–542 (2009)
4. Di, D., Shi, F., Yan, F., Xia, L., Mo, Z., Ding, Z., Shan, F., Song, B., Li, S., Wei, Y.,
et al.: Hypergraph learning for identification of covid-19 with ct imaging. MedIA
p. 101910 (2020)
5. Fan, D.P., Zhou, T., Ji, G.P., Zhou, Y., Chen, G., Fu, H., Shen, J., Shao, L.: Infnet: Automatic covid-19 lung infection segmentation from ct images. IEEE TMI
(2020)
6. Fang, Y., Zhang, H., Xie, J., Lin, M., Ying, L., Pang, P., Ji, W.: Sensitivity of
chest ct for covid-19: comparison to rt-pcr. Radiology p. 200432 (2020)
7. Huang, C., Wang, Y., Li, X., Ren, L., Zhao, J., Hu, Y., Zhang, L., Fan, G., Xu, J.,
Gu, X., et al.: Clinical features of patients infected with 2019 novel coronavirus in
wuhan, china. The Lancet 395(10223), 497–506 (2020)
8. Jin, C., Chen, W., Cao, Y., Xu, Z., Tan, Z., Zhang, X., Deng, L., Zheng, C., Zhou,
J., Shi, H., et al.: Development and evaluation of an artificial intelligence system
for covid-19 diagnosis. Nat. Commun 11(1), 1–14 (2020)
9. Ke, Z., Wang, D., Yan, Q., Ren, J., Lau, R.W.: Dual student: Breaking the limits
of the teacher in semi-supervised learning. In: ICCV. pp. 6728–6736 (2019)
10. Kendall, A., Gal, Y.: What uncertainties do we need in bayesian deep learning for
computer vision? In: NeurIPS. pp. 5574–5584 (2017)
11. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR
(Poster) (2015)
12. Li, X., Yu, L., Chen, H., Fu, C.W., Xing, L., Heng, P.A.: Transformationconsistent self-ensembling model for semisupervised medical image segmentation.
IEEE TNNLS (2020)
13. Liang, W., Yao, J., Chen, A., Lv, Q., Zanin, M., Liu, J., Wong, S., Li, Y., Lu, J.,
Liang, H., et al.: Early triage of critically ill covid-19 patients using deep learning.
Nat. Commun 11(1), 1–7 (2020)
14. Liu, Q., Yu, L., Luo, L., Dou, Q., Heng, P.A.: Semi-supervised medical image
classification with relation-driven self-ensembling model. IEEE TMI (2020)
15. Luo, L., Yu, L., Chen, H., Liu, Q., Wang, X., Xu, J., Heng, P.A.: Deep mining
external imperfect data for chest x-ray disease screening. IEEE TMI 39(11), 3583–
3594 (2020)
16. Ma, J., Nie, Z., Wang, C., Dong, G., Zhu, Q., He, J., Gui, L., Yang, X.: Active
contour regularized semi-supervised learning for covid-19 ct infection segmentation
with limited annotations. Physics in Medicine & Biology (2020)
17. Mei, X., Lee, H.C., Diao, K.y., Huang, M., Lin, B., Liu, C., Xie, Z., Ma, Y., Robson,
P.M., Chung, M., et al.: Artificial intelligence–enabled rapid diagnosis of patients
with covid-19. Nat. Med pp. 1–5 (2020)
18. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks
for volumetric medical image segmentation. In: 3DV. pp. 565–571. IEEE (2016)

UDC-Net: Uncertainty-guided Dual-Consistency Learning

11

19. Miyato, T., Maeda, S.i., Koyama, M., Ishii, S.: Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE TPAMI
41(8), 1979–1993 (2018)
20. Oh, Y., Park, S., Ye, J.C.: Deep learning covid-19 features on cxr using limited
training data sets. IEEE TMI (2020)
21. Ouali, Y., Hudelot, C., Tami, M.: Semi-supervised semantic segmentation with
cross-consistency training. In: CVPR. pp. 12674–12684 (2020)
22. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, highperformance deep learning library. In: NeurIPS. pp. 8026–8037 (2019)
23. Qiao, S., Shen, W., Zhang, Z., Wang, B., Yuille, A.: Deep co-training for semisupervised image recognition. In: ECCV. pp. 135–152 (2018)
24. Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged
consistency targets improve semi-supervised deep learning results. NeurIPS 30,
1195–1204 (2017)
25. Van Engelen, J.E., Hoos, H.H.: A survey on semi-supervised learning. Mach Learn
109(2), 373–440 (2020)
26. Wang, D., Zhang, Y., Zhang, K., Wang, L.: Focalmix: Semi-supervised learning for
3d medical image detection. In: CVPR. pp. 3951–3960 (2020)
27. Wang, G., Liu, X., Li, C., Xu, Z., Ruan, J., Zhu, H., Meng, T., Li, K., Huang,
N., Zhang, S.: A noise-robust framework for automatic segmentation of covid-19
pneumonia lesions from ct images. IEEE TMI 39(8), 2653–2663 (2020)
28. Wang, L., Lin, Z.Q., Wong, A.: Covid-net: A tailored deep convolutional neural
network design for detection of covid-19 cases from chest x-ray images. Sci. Rep
10(1), 1–12 (2020)
29. Wang, X., Chen, H., Ran, A.R., Luo, L., Chan, P.P., Tham, C.C., Chang, R.T.,
Mannil, S.S., Cheung, C.Y., Heng, P.A.: Towards multi-center glaucoma oct image
screening with semi-supervised joint structure and function multi-task learning.
MedIA 63, 101695 (2020)
30. Wang, X., Tang, F., Chen, H., Luo, L., Tang, Z., Ran, A.R., Cheung, C.Y., Heng,
P.A.: Ud-mil: Uncertainty-driven deep multiple instance learning for oct image
classification. IEEE JBHI (2020)
31. Wang, X., Deng, X., Fu, Q., Zhou, Q., Feng, J., Ma, H., Liu, W., Zheng, C.:
A weakly-supervised framework for covid-19 classification and lesion localization
from chest ct. IEEE TMI (2020)
32. Worrall, D.E., Garbin, S.J., Turmukhambetov, D., Brostow, G.J.: Harmonic networks: Deep translation and rotation equivariance. In: CVPR. pp. 5028–5037
(2017)
33. Xia, Y., Yang, D., Yu, Z., Liu, F., Cai, J., Yu, L., Zhu, Z., Xu, D., Yuille, A., Roth,
H.: Uncertainty-aware multi-view co-training for semi-supervised medical image
segmentation and domain adaptation. MedIA 65, 101766 (2020)
34. Yu, L., Wang, S., Li, X., Fu, C.W., Heng, P.A.: Uncertainty-aware self-ensembling
model for semi-supervised 3d left atrium segmentation. In: MICCAI. pp. 605–613.
Springer (2019)
35. Zhang, C., Bengio, S., Hardt, M., Recht, B., Vinyals, O.: Understanding deep
learning requires rethinking generalization. In: ICLR (2017)
36. Zhu, N., Zhang, D., Wang, W., Li, X., Yang, B., Song, J., Zhao, X., Huang, B.,
Shi, W., Lu, R., et al.: A novel coronavirus from patients with pneumonia in china,
2019. New England Journal of Medicine (2020)

12

Yanwen Li, Luyang Luo, et al.

5

Supplementary Materials

5.1

Visualization of Epistemic Uncertainty Map and Aleatoric
Uncertainty Map.

Fig. 3: Visualization of input images, ground truth masks, epistemic uncertainty
map, and aleatoric uncertainty map. The visualization results demonstrates that
our proposed epistemic and aleatoric uncertainties are complementary.

UDC-Net: Uncertainty-guided Dual-Consistency Learning

5.2

13

Perturbations of Auxiliary Decoders

Table 4: List of perturbations used in feature-level consistency learning.
Perturbations[21] Description
Feature Noise
A noise tensor N is applied to the output of encoder z to get z̃ = z ∗ N + z.
Generating a randomly dropout mask Mdrop to obtain perturbated
Feature Dropout
z̃ = z ∗ Mdrop .
Generating a object mask Mobj using the output of main decoder
Object Masking
to get z̃ = z ∗ Mobj .
Context Masking Generating a context mask Mcon = 1 − Mobj to obtain z̃ = z ∗ Mcon .
Zero-out a random crop within each object’s bounding box from
Guided cutout
the corresponding feature map z.
Using VAT to push the distribution to be isotropically smooth. Finding
Intermediate VAT the adversarial perturbation radv alter its prediction the most and
injected into z to obtain z̃ = radv + z.[19]
Random dropout Spacial dropout applied to z as a random perturbation

5.3

Monitoring the lesion development in follow-up process.

Fig. 4: An example of monitoring the lesion development of a patient from mild
infection (stage 1), to common infection (stage 2), severe infection (stage 3), and
finally to recovery stage (stage 4).

