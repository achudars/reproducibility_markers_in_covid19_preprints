1

Relational Modeling for Robust and Efficient
Pulmonary Lobe Segmentation in CT Scans

arXiv:2004.07443v4 [eess.IV] 12 May 2020

Weiyi Xie, Colin Jacobs, Jean-Paul Charbonnier, Bram van Ginneken

Abstract—Pulmonary lobe segmentation in computed tomography scans is essential for regional assessment of pulmonary
diseases. Recent works based on convolution neural networks
have achieved good performance for this task. However, they
are still limited in capturing structured relationships due to the
nature of convolution. The shape of the pulmonary lobes affect
each other and their borders relate to the appearance of other
structures, such as vessels, airways, and the pleural wall. We
argue that such structural relationships play a critical role in
the accurate delineation of pulmonary lobes when the lungs are
affected by diseases such as COVID-19 or COPD.
In this paper, we propose a relational approach (RTSU-Net)
that leverages structured relationships by introducing a novel
non-local neural network module. The proposed module learns
both visual and geometric relationships among all convolution
features to produce self-attention weights.
With a limited amount of training data available from COVID19 subjects, we initially train and validate RTSU-Net on a
cohort of 5000 subjects from the COPDGene study (4000 for
training and 1000 for evaluation). Using models pre-trained on
COPDGene, we apply transfer learning to retrain and evaluate
RTSU-Net on 470 COVID-19 suspects (370 for retraining and
100 for evaluation). Experimental results show that RTSU-Net
outperforms three baselines and performs robustly on cases with
severe lung infection due to COVID-19.
Index Terms—Pulmonary Lobe, Segmentation, Computed Tomography, COVID-19, COPD, Convolution Neural Network,
Non-local Neural Networks.

I. I NTRODUCTION

T

HE human lungs consist of five disjoint pulmonary lobes.
The right lung is composed of an upper, middle, and
lower lobe, while the left lung only has an upper and a lower
lobe. The lobes are separated by the pulmonary fissures, a
double-fold of visceral pleura visible as a thin line on CT
images. The lobes are functionally independent units because
each has its own vascular and bronchial supply. As a result,
the extent of the disease often varies substantially across lobes,
This work was supported by the Dutch Lung Foundation. We acknowledge
the COPDGene Study (ancillary study ANC-398) for providing the data used.
COPDGene is funded by Award Number U01 HL089897 and Award Number
U01 HL089856 from the National Heart, Lung, and Blood Institute. The
content is solely the responsibility of the authors. It does not necessarily
represent the official views of the National Heart, Lung, and Blood Institute
of the National Institutes of Health. The COPD Foundation also supports
the COPDGene project through contributions made to an Industry Advisory
Board comprised of AstraZeneca, Boehringer Ingelheim, GlaxoSmithKline,
Novartis, Pfizer, Siemens, and Sunovion. (Corresponding author: Weiyi Xie.
Contacting email: Weiyi.Xie@radboudumc.nl).
W. Xie, C. Jacobs, B. van Ginneken are with the Diagnostic Image Analysis
Group, Radboudumc, Nijmegen, the Netherlands.
Jean-Paul Charbonnier is with Thirona, Nijmegen, the Netherlands.

and lobe-wise assessment of pulmonary disorders is of clinical
importance.
Computed Tomography (CT) is the best way to image the
lungs in vivo. COVID-19, the pandemic disease caused by the
SARS-Cov2 virus is straining healthcare systems worldwide.
A CT severity score can summarize the severity of the disease
where each lobe is scored visually by radiologists on a scale
from 0 to 5. The summation of these scores quantifies lung
involvement on a scale from 0 to 25 [1]. The score provides a
tool to assess disease severity and progression, which further
benefits clinical decision making. To automate the CT severity
score, lobe segmentation in COVID-19 scans is needed. CT
scans of COVID-19 patients are affected by extensive patchy
ground-glass region and consolidations and may even show
lobes or complete lungs filled with pleural fluid. Automated
lobe segmentation is highly challenging in scans with such
extensive pathological changes.
Many automatic lobe segmentation approaches focused on
finding visible fissures, assuming that the detected fissures
equivalent to find the lobe segmentation by interpolation.
Both early fissure enhancement filters [2]–[5] and more robust
supervised learning methods [6] relied heavily on hand-crafted
features, thus hard to generalize. Moreover, because incomplete fissures are very common [7], interpolation of boundaries
based on visible fissures may not suffice to find the lobe
borders reliably. Instead of finding fissures alone, anatomical
relations between lobes and nearby airways, vessels, and the
lung borders were exploited to account for incomplete fissures
and damaged lung due to pathology [8]–[11].
Recent advances in convolution neural networks (CNN) provide a data-driven approach for more robust feature extraction
in an end-to-end optimization process. Many works have successfully adopted CNNs in their lobe segmentation framework
[12]–[15]. In [12], deep supervision was extensively used in
the up-sampling path based on their V-Net design [16] along
with the multi-tasking that segments lobe and lobe borders
at the same time. [13] uses a relatively deeper architecture
based on Dense-Net [17] to ensure a sufficient receptive field
of extracted features. Global Geometric features were explored
[14] as additional input channels to a convolution layer.
The use of multi-resolution input in a two-stage cascading
CNNs to extract both global and local features has been proposed for lobe segmentation in CT by Gerard et al. [15]. Their
first stage network was trained on low-resolution images to
learn global features from the entire scan. The global features
were added to the second stage network to provide contextual
guidance, while the second stage network was designed to
focus on capturing local details at a high resolution. Their

2

framework has also been successfully applied for pulmonary
fissure and lung segmentation tasks [18], [19]. In this work,
we also employ a two-stage approach, that is trained in an
end-to-end fashion.
Although existing CNN approaches have achieved superior
performance in lobe segmentation comparing to the feature
engineering approaches, they may still be inefficient and
limited in relational reasoning, such as capturing the interlobar
relationships and other long-range relationships between lobes
and other structures in the CT image. CNN approaches assumed that such relationships between objects and object parts
in semantic segmentation could be implicitly learned directly
from the CNN training process.
However, as [20], [21] have pointed out, the hierarchical
feature representation computed using a sequence of stacked
convolutional layers can be highly inefficient in inferring relations between convolution features. As higher-level features
in CNNs commonly represent objects and object parts, instead
of aggregating these features based on their semantic interactions, convolutional filters act as templates, where features are
aggregated depending on the filter weights. This may cause
inefficiency in capturing relations between features because
filters weights are not invariant to permutations of features. In
addition, convolution filters are limited to capture long-range
relations due to the use of local kernels.
CT findings in patients of a COVID-19 infection [22], [23]
often include multiple regions with focal pathological changes,
ranging from ground-glass to consolidations to organizing
pneumonia. These changes occur more often in the lower
lobes. Here the lobar boundaries can be deformed substantially. In these cases, information from other regions in the
CT image may be crucial for locating and delineating a target
lobe. Therefore, in this paper, we introduce a novel nonlocal neural network module to model the global structured
relationships for pulmonary lobe segmentation. The proposed
non-local neural network module computes a feature response
at one location using both appearance and geometric features
from all other positions at the scan-level. We call this approach
a Relational two-stage U-net, or RTSU-Net, for short.
The main contributions of this paper are as follows:
• We propose a novel non-local neural network module
that can capture the global structured relationships between object and object parts in terms of their visual
and geometric features for the lobe segmentation. The
proposed RTSU-Net is robust and produces accurate lobe
segmentations even for scans with severe pathology.
• We used a multi-resolution framework similar to [15],
[18], [19], however, we train both stages in an end-to-end
fashion. This gives the RTSU-Net the ability to capture
the global object relationships at the full scan level from
the first stage network while extracting local details at
the second stage simultaneously in the same optimization
process.
• RTSU-Net is fast and memory-efficient, considering it
consists of a cascade of two CNNs. RTSU-Net requires
only a standard GPU with 12GB memory to train and
takes around 30 seconds to produce lung and lobe segmentations for a full thoracic CT scan at test time. The

time consumption includes the CNN inference time, preprocessing, and post-processing, excluding the time spent
on IO.
A. Related Work
Although convolutional neural networks (CNNs) have
achieved superior performance in a wide range of medical
imaging segmentation tasks [16], [24], they are still limited
in modeling object relationships, especially the long-range
interactions. Several techniques have been proposed to account
for the missing capability of relational reasoning in CNNs.
Poudel et al. [25] introduced recurrent neural networks to
aggregate features across the axial slices for cardiac segmentation in multi-slice MRI. A known issue with recurrent network
networks is that they suffer from vanishing gradients [26]
and therefore are hard to train. The object relations could
also be explicitly defined using Graph Models such as dense
conditional random fields (CRF) [27]. However, due to their
heavy computational demands, dense CRFs are often only used
as the post-processing steps and optimized separately on a
heuristic basis, making it hard for this approach to scale well.
Attention is widely used for various tasks such as machine
translation, image and video classification, object detection,
and semantic segmentation. Self-attention methods [28], [29]
capture contextual dependencies between words by computing
the embedding at one word by a weighted summation of embeddings at all words in sentences. As one of the self-attention
applications, a non-local neural network was proposed for semantic segmentation [30] by computing a global self-attention
map for each feature based on all the other features in an input
CNN feature map. The attention weights were determined by
predefined similarity measurements between pairwise features
in a linear-projected subspace, as an efficient way of modeling
their conceptual relationships.
There are several recent extensions of this non-local method
in semantic segmentation. CCNet [31] was proposed to employ
a simple criss-cross trick, which reduces the space and time
complexity of the non-local module from O((H × W ) × (H ×
W )) to O((H×W )×(H+W −1)) in two-dimensional images.
Hu et al. [20] aggregated features based on both visual and
geometric correspondence in a locally connected aggregation
graph, thus lacking long-range relationships. A dynamical
aggregation graph was proposed in [21] to capture both short
and long-range relationships, but no geometric correspondence
between features was used.
Our approach is motivated by the above works. Our selfattention module uses the criss-cross trick to collect global
structured relationships between object and object parts in
terms of their visual and geometric correspondence in the
feature representation.
II. DATA
CT scans used in this study were obtained from two sources.
We refer to the first set as the COPD set and the second set
as the COVID-19 set.
A large set of scans from subjects with COPD, ranging
from mild to very severe, was obtained from the COPDGene

3

TABLE I: Characteristics of the two data sets used in this
study. (a) lists the distribution of GOLD stages and other
classes, see [32] in the COPD data set. (b) gives the distribution of CO-RADS scores [1] across the training and
test sets. CO-RADS score 1-6 indicates the level of suspicion for COVID-19 positive disease, ranging from very low,
low, equivocal, high, very high, and confirmed positive from
the reverse-transcription polymerase chain reaction (RT-PCR)
tests, respectively.
(a) COPD set GOLD stages
GOLD stages

#subjects for training

#subjects for testing

GOLD0
GOLD1
GOLD2
GOLD3
GOLD4
Non Spirometry
Non Smoking
PRISm
Total

1709
319
734
441
226
30
45
496
4000

433
80
184
110
57
2
11
123
1000

(b) COVID-19 set CO-RADS
CORADS

#subjects for training

#subjects for testing

1
2
3
4
5
6
Total

158
46
47
30
65
24
370

23
9
20
16
24
8
100

study [32]. This is a clinical trial with data from 21 imaging
centers in the United States. In total, COPDGene enrolled
10,000 subjects. Each subject underwent both inspiration and
expiration chest CT. Image reconstruction uses sub-millimeter
slice thickness and in-plane resolution, with edge-enhancing
and smooth algorithms. Data from COPDGene is publicly
available and can be retrieved after submitting an ancillary
study proposal (ANC-398 was used for this work).
We randomly selected 5000 subjects and used only Phase
I inspiration CT scans (one scan per subject). Subjects were
randomly grouped into a training set (n = 4000) and a test
set (n = 1000). Slice thickness ranged from 0.625-0.9mm and
pixel spacing from 0.478-1.0mm. Most scans were performed
using 200mAs a tube voltage of 120kVp and B31f and B35f
reconstruction kernels. The CT protocols are detailed in [32].
The other data set was obtained from Radboud University
Medical Center, Nijmegen, the Netherlands. On March 18,
2020, this institution implemented a low-dose non-contrast
CT protocol and all patients who arrived at the hospital
with suspicion of COVID-19 disease and inpatients for whom
COVID-19 was considered a possibility underwent CT. In
accordance with local guidelines, we only included scans
from subjects who did not object to the use of their scans
for research purposes and we worked with anonymized data.
Permission for research use was obtained from our review
board (file number CMO 2016-3045, Project 20027). It is the
intention to share these scans via a national Dutch COVID-19
database.

We randomly selected 470 subjects and used one scan per
subject by selecting the CT scan of the smallest slice thickness
in a study. Scans have a pixel spacing between 0.5mm to
0.9mm and a slice thickness of 0.5 mm. Scans were performed
using X-ray tube current ranging from 10mA to 493mA and a
tube voltage of either 100 or 120kVp. Convolution kernels in
reconstruction were lung kernels (FC83, FC86). 370 of these
scans were used for training and the other 100 for testing.
See Table I for the distribution of GOLD stages in the
training and the test set for the COPD set and the distribution
of CO-RADS scores [1] from the COVID-19 set. The CORADS scores defined the level of suspicion COVID-19 and
were extracted from the radiology reports. Complete individual
results of reverse-transcription polymerase chain reaction (RTPCR) tests were not available at the time of anonymization of
the data, but it is known that the majority of the test cases
were positive for COVID-19 (these test cases overlap with the
data used in [1]).
From the two training data sets, we selected 100 scans as
the validation set for the COPD set, and we selected 50 scans
for validation from the COVID-19 set for retraining all the
models.
A. Reference Standard
Lobe segmentation references were obtained from Thirona,
a company that specializes in chest CT analysis. First, automated segmentation of the left and right lung was generated using a commercialized software (LungQ, Thirona,
Nijmegen, NL), followed by manual refinement if needed.
Second, automatic algorithms [6], [9], [33] were used to
extract the lobar boundary with possible interpolation for
incomplete fissures using information from nearby airways and
vessels. Next, the automatically found lobar boundaries were
manually corrected separately for the left and the right lung, by
trained analysts with at least one year experience in annotating
pulmonary structures on CT. Analysts repeatedly adjusted the
control points on the auto-generated lobar boundaries until the
updated lobar boundaries were satisfactory. All analysts have
a medical background and have received extensive training in
lung anatomy and segmenting lobes in CT imaging. In case
of doubt, radiologists could be consulted.
III. M ETHODS
We define the lobe segmentation problem as a voxel-wise
classification problem. Given a scan I, the goal is to predict
the voxel label lˆi for every spatial location i, where lˆi ∈ the
label set L = {0, 1, 2, 3, 4, 5} representing the background,
left upper, left lower, right upper, right lower, and the right
middle lobe, respectively.
In this paper, we use a multi-resolution approach with two
cascaded CNNs to capture both global context and local details
for the lobe segmentation, as proposed in [15]. Our framework
is depicted in Fig. 1.
Besides the use of a multi-resolution framework, we introduce a novel non-local module to capture structured relationships and our efficient network design allows end-to-end
training of our multi-resolution framework. For each CNN, we

4

Fig. 1: The overview of our lobe segmentation framework with a cascade of two CNNs. At each stage, a CNN (RU-Net) uses
the proposed non-local module to capture the structured relationships between objects and object parts. The output from the
RU-Net I is concatenated with the cropped 3D patches as the input for RU-Net II.

place our proposed non-local module to aggregate relational
information for the features at the coarsest resolution as these
features commonly represent high-level semantics such as
objects and objects parts [34]. The proposed non-local module
computes visual and geometric correspondence between these
features, naturally modeling relationships between objects and
object parts. The use of geometric information is inspired by
[14]. Also, the proposed non-local module can enlarge the
receptive fields of these features because the computation of
one non-local response involves all features in the feature map.
We refer the CNN with the proposed non-local module at
each stage to as relational U-Net (RU-Net), and the details
are explained later in this section.

A. Cascading relational U-Nets
The first RU-Net reads an input scan at a down-sampled
resolution to coarsely segment the lobes and lobe borders.
These coarse outputs are subsequently upsampled to a higher
resolution by trilinear interpolation. The high-resolution input
scan and the output of the first RU-Net are concatenated and
cropped into 3D patches as the input to train the second RUNet to precisely segment lobes and lobe borders. The cascade
of two relational U-Nets is trained end-to-end, allowing both
local details and scan-level context to be learned in the same
optimization process. Furthermore, we use the errors found in
the predictions of the first RU-Net to optimally sample 3D
patches for training the second stage, which encourages the
second RU-Net to focus on the regions where the first RUNet fails. This technique can be seen as a form of online hard
example mining. [35].

B. Relational U-Net
The relational U-Net architecture (RU-Net) is a 3D U-Net
architecture [24] with a smaller number of convolution filters
and an additional non-local module. The RU-Net has three
down-sampling layers in the encoding path, and each layer
consists of two convolutions and a max-pooling operation.
Following the down-sampling path, two more convolutions are
used to double the number of convolution filters. We then place
the non-local module before up-sampling. In the up-sampling
path, three layers are used to reconstruct the resolution, and
each contains one tri-linear interpolation, followed by two
convolutions to reduce the interpolation artifacts. In the end,
features are reshaped via a single 1x1x1 convolution in two
parallel output branches, and each corresponds to a different
learning objective; one produces 6-channel softmax probabilities for segmenting the background and the five lobes. The
other provides a single channel probability map by sigmoid
function for predicting the lobe border. Features from 3x3x3
convolutions are batch normalized and activated via a rectifier
linear unit (ReLU). No dropout is used.
The first RU-Net uses padded convolutions, whereas the
second uses valid convolutions. The details regarding RU-Net
network architecture on both stages are provided in Table II,
where the names of the down-sampling layers are prefixed with
’Down’, and the name of up-sampling layers are prefixed with
’Up’. The numbers listed are based on the execution order.

C. The Non-Local module
The original non-local neural network [30] for semantic
segmentation computes the feature response at a position as

5

TABLE II: Architectures for the first and the second stage of
the relational U-Nets. The convolution filters are named by
the kernel sizes K and number of filters N as K × K × K, N
(stride 1 for all). Non-local linear embedding parameters are
defined in Eqs. (3) and (5). k denotes the operation performed
in dual paths.
Layer
Down1

Down2

Down3
Bridge
Non-local

Up1

Up2

Up3
Output
MAC
#Parameter

RU-Net I
RU-Net II
3x3x3,1-16
3x3x3,8-24
3x3x3,16-24
3x3x3,24-48
2x2x2 max pool, stride 2
3x3x3,24-24
3x3x3,48-48
3x3x3,24-48
3x3x3,48-96
2x2x2 max pool, stride 2
3x3x3,48-64
3x3x3,96-96
3x3x3,64-128
3x3x3,96-192
2x2x2 max pool, stride 2
3x3x3,128-128
3x3x3,192-192
3x3x3,128-256
3x3x3,192-384
Wθ ,Wφ ∈R256×32
Wθ ,Wφ ∈R384×32
Wω ,Wρ ∈R3×32
Wω ,Wρ ∈R3×32
Wr ∈R32×256
Wr ∈R32×384
3x3x3,384-128
3x3x3,576-192
3x3x3,128-128
3x3x3,192-192
trilinear interpolation x2
3x3x3,176-48
3x3x3,288-96
3x3x3,48-48
3x3x3,96-96
trilinear interpolation x2
3x3x3,72-24
3x3x3,144-48
3x3x3,24-24
3x3x3,48-48
trilinear interpolation x2
1x1x1,6 k 1x1x1,1
1x1x1,6 k 1x1x1,1
5.71 G
8.79 G
3.85M
9.24 M

yi =

1 X
f (xi , xj )g(xj ),
ζ(x)

1 X
(f (xi , xj ) + τ (µi , µj ))g(xj ),
ζ(x, µ)

(2)

∀j

A similar reparameterization can be applied using the softmax
function row-wise under linear projections to reformulate
Equation 2 into matrix multiplications:

a weighted sum of the features at all locations in the input
feature maps as
yi =

However, the original non-local module disregards the geometric correspondence between features, while [14] shows that
introducing geometric coordinates improves the performance
on lobe segmentation. Hence, we propose to compute nonlocal responses with a geometric term. Here, we denote µi , µj
as geometric coordinates for the position i and j. µi is the
center coordinate of the receptive field of the feature at position
i with respect to the original input image and rescaled to
[0 ∼ 1] range by the size of the original input image. We
note that if the feature map is produced from a cropped
input, the center coordinate of the receptive field is then
shifted according to the 3D patch offset to the original input
image. The rescaled geometric coordinates are then shifted
by 0.5 to have zero mean. τ (µi , µj ) is the pairwise function
for measuring correlations. Then, the non-local response with
geometric terms is defined as:

(1)

∀j

where yi at location i is computed as a weighted sum using the
correspondence between the feature xi at the location i and all
features indexed by j in the input feature map x. The feature
correspondence between feature xi and xj is also called the
self-attention in this context, computed by the pairwise function f , which is used to weigh the feature embedding g(xj )
before normalizing by ζ(x). For simplicity, g is set to a linear
projection: g(xj ) = Wg xj , and the pairwise function f can
be the embedded Gaussian function using linear embeddings
T
defined as f (xi , xj ) = P
e(Wθ xi ) (Wφ xj ) . We set the normalizing factor as ζ(x) =
∀j f (xi , xj ). Then y becomes the
softmax computation along the dimension j written, in matrix
multiplication form, as y = sof tmax(xT WθT Wφ x)g(x). To
make the input and output of the non-local module the same
size, the yi is reshaped to have the same dimensions as
the input xi by applying the linear reconstruction function
r, r(yi ) = Wr yi . Therefore, the non-local response at location
j can be written as zi = Wr yi + xi .
The feature response zi automatically achieves a global
receptive field with respect to the input. The computed selfattention map f (xi , xj ) captures the feature correlations, as
relevant features would have higher attention responses.

y = sof tmax(xT WθT Wφ x + max(0, µT WωT Wρ µ))g(x),
(3)
where f (xi , xj ) is parameterized as a dot product in a subspace projected using the linear transformation matrix Wθ and
Wφ . Similarly, Wω and Wρ are linear transformations that
are used to project the geometric features µ into a subspace
where their correspondence is measured by the pairwise kernel
function τ , τ (µi , µj ) = max(0, µT WωT Wρ µ). Such correspondence is then trimmed at 0, to restrict geometric relations
within a certain threshold.
The Equation 3 however, has high computational cost because the self-attention map requires computing xT WθT Wφ x
and µT WωT Wρ µ on all pairs of locations. Each term has
complexity in time and space of O(C × W 2 × H 2 × D2 )
where C is the dimension of linear projected subspace and
W, H, D denotes the width, height, and depth of a 3D feature
map. To reduce computational complexity, we adopt the crisscross trick [31], which has a time and space complexity of
O((C × W × H × D) × (H + W + D − 2)). In CCNet,
Equation 2 is modified to:
yi =

X
1
(f (xi , xj ) + τ (µi , µj ))g(xj ),
ζ(x, µ)

(4)

j∈Ωj

where Ωj indicates the neighboring voxels with respect to
j under criss-cross connectivity, such sparse connectivity requires having three recurrent criss-cross modules to cover all
spatial locations in computation.
Given the input feature xi , the non-local response zit for a
feature location i at each t-th recurrent criss-cross module can
be written as follows:
(
xi
if t = 0
t
zi =
t−1
t−1
(5)
Wr yi + zi
if t = 1, 2, . . . , T
P
yit = ζ(z1t ,µ) j∈Ωj (f (zit , zjt ) + τ (µi , µj ))g(zjt )

6

At each recurrent step, the non-local response zit is used as
the input feature for computing the non-local response for the
next recurrent step. For the size of scans used in this work,
full global context can be achieved with three recurrent steps
for a 3D input feature map. Therefore, we set T = 3.
D. Online Hard Example Mining
As shown in Fig. 1 using the red dashed lines, we compute
the mean square errors (MSE) between the lobe-wise softmax
probabilities of the first RU-Net and the lobe reference standard. We then go through all sliding window 3D patches, and
find K patches with the highest integral of MSE and use them
for training the second RU-Net.
K is set to 1.0 such that all patches are used to train at the
beginning and continuously reduced until it reaches a coverage
of only approximately 20% of the scan volume at the end of
the training process. The proposed online hard example mining
does not introduce extra forward and backward passes on the
network, therefore the additional computational cost is trivial.
E. Learning Objectives
There are two learning objectives for each RU-Net: lobe
segmentation and lobe border segmentation, inspired by [12],
[15]. Therefore, the final loss function is a summation of four
terms, and each is the generalized Dice loss [36]. The lobe
border reference is pre-computed from the lobe reference by
detecting object boundaries.
Let r be the segmentation reference with n-th voxel values
rln for the class label l and r̂ln be the predicted probabilistic
map for the label l over n-th image voxel, then the generalized
Dice loss is defined as:
P
P
wl n rln r̂ln
l
P
,
GLD = 1 − 2 P
l wl
n rln + r̂ln
PN
with wl = 1/( n l rln )2 , where Nl the in total number of
voxels for the class label l in the segmentation reference. wl is
to re-balance learning against the variance in object volumes.
IV. E XPERIMENTS
As the COVID-19 pandemic emerged only recently, it was
not possible to obtain a large amount of CT scans with
annotations of COVID-19 patients. Therefore, we used a
transfer learning approach in our experiments. For training of
the models on the COVID-19 data, the models were initialized
with the trained weights from our models developed on the
COPD data set.
A. Training details
Training, validation, and testing of each experiment were
carried out on a machine with a NVidia TitanX GPU with 12
GB memory. The methods were implemented using Python
3.6, Pytorch 1.1.0 library [37]. The trainable parameters of
each method were initialized using Kaiming He initialization
when training from scratch [38] and were optimized using
stochastic gradient descent with a momentum of 0.9, and
the initial learning rate set to 10e-6. The initial models were

trained using CT scans from the COPD data set. Therefore,
these models may not be familiar with the visual patterns
in COVID-19 scans. For efficiently training on new visual
patterns, all models were retrained using a combined loss
between the generalized Dice loss (as we used to train the
initial models) and top-K cross-entropy loss where K is set
to 30% of all voxels in the input. The top-K cross-entropy
loss was implemented simply as the voxel-wise cross-entropy
loss but selecting only K voxels with the largest cross-entropy
to back-propagate.
B. Comparison with previous work
We compared our approach with three baselines, the wellknown 3D U-Net and two recently published methods for lobe
segmentation in CT. .
1) 3D U-Net: We implemented 3D U-Net following the
original paper [24]. The input is a mini-batch of two 132 ×
132 × 132 3D patches randomly cropped from the preprocessed scan (refer to IV-D). As a result of using valid
convolutions, the output of this network is 44×44×44 voxels.
During test time, the softmax probabilities of all 3D patches
are tiled together by sliding over the entire scan without
overlaps to build up a scan-level probability map. The final
prediction is then made by assigning each voxel to the label
with the highest probability.
2) FRV-Net and PDV-Net: We compare the proposed
method with two existing end-to-end lobe segmentation methods. FRV-Net [12] follows the design of the V-Net [16] and
extensively uses the idea of deep supervision at almost all
scales in the up-sampling pathways. PDV-Net [13] uses dense
connections, following the DenseNet [17], to design their
network with a considerably large receptive field to capture
contextual information. PDV-Net takes the entire CT scan
as the input, thus potentially capable of learning the global
information. Note that these two works have specific preprocessing and post-processing strategies. The input scan in
FRV-Net is resized into a fixed size of 128 × 256 × 256 and
intensities are clipped into the range [−1000 ∼ 400] HU. In
PDV-Net, the input scan is resized into 128 × 512 × 512.
We implemented both architectures following the paper at our
best efforts.
C. Ablation studies
To assess the contribution of the proposed non-local module
in RTSU-Net, we performed several ablation studies. During
these experiments, the models were trained from scratch
using the COPD training set and retrained on COVID-19 and
performance is measured on the COPD test set of 1000 cases
and 100 COVID-19 cases. The performance of our proposed
model was assessed without the geometric features in the
non-local module, and without the non-local module in the
relational two stage U-Net framework.
D. Pre-processing and post-processing
All training and test scans were standardized by clamping
intensity values to the [−1200 ∼ 400] range before re-scaling

7

into [0 ∼ 1]. Then all scans were down-sampled using trilinear
interpolation to have a 256 × 256 in-plane resolution while zspacing is adjusted to make the scan isotropic.
The input size of the second CNN for our proposed method
consisted of two 116 × 116 × 116 sized 3D patches. The
pre-processed scan was down-sampled by a factor of 2 using
trilinear interpolation as the input for the first stage (padding
with zeros are needed if the size on z axial is not divisible
by 16). The softmax probability outputs of all 3D patches at
the second stage were tiled together by sliding over the entire
scan without overlaps to produce a scan-level probability map,
which is used to generate the final prediction by assigning each
voxel to the label with the highest probability.
As a post-processing step, the predictions were then upsampled by nearest neighbor interpolation to match the original resolution of the scans. All evaluations are performed by
using predictions and reference segmentations at the original
resolution.
E. Evaluation Metrics
The Intersection over Union (IOU), and average symmetric
surface distance (ASSD) between predictions and segmentation references were used for quantitative evaluation of segmentation performance. The IOU between two binary masks
X, Y is defined as:
IOU (X, Y ) =

|X ∩ Y |
,
|X ∪ Y |

Denote two surfaces as SX ,SY from the masks X, Y , and coordinate indices on the surface as x,y. The average symmetric
surface distance (ASSD) is defined as:
P

ASSD(X, Y ) =

x∈SX

P
miny∈SY d(x,y)+ y∈S minx∈SX d(y,x)
Y
|SX |+|SY |

with d(·) being the Euclidean distance, and |SX | and |SY | the
surface area for SX and SY , respectively.
Besides the lobe-based measurements, we also evaluated the
performance of all models in the lung segmentation task by
taking the union of all lobes as the lung. Furthermore, we
add a metric to measure fissure alignment by computing the
average symmetric surface distance in the interlobar borders
between the predictions and the segmentation references.
The overall performance of the method was evaluated by
computing the average of the per-lobe metrics. A Wilcoxon
signed-rank test was employed to assess whether the performance difference was statistically significant (p < 0.01 with
Bonferroni correction).
Also, we computed the number of Multi-Adds operations
(MAC) and the number of parameters to assess computational
efficiency. We also provide a comparison with independent
human readers on a subset of 100 subjects from the COPD
data only.
V. R ESULTS
A. Quantitative results
Table III reports the quantitative results on both data sets.
The proposed method significantly outperformed the baseline

Fig. 2: Box and whisker plots of IOU per-lobe for different
methods on the COPD data set (top) and the COVID-19 data
set (bottom).

methods and two published end-to-end lobe segmentation
methods on both data set (p < 0.01 with Bonferroni correction) consistently in all measurements. Our model also exhibits
more robust performance, considering the smaller standard
deviations.
Box and whisker plots are provided in Fig. 2. These plots
show that for both the COVID-19 and the COPD cases, the
right middle lobe is the most difficult to segment, which is
not surprising given its known high variance in shape and the
fact that the minor fissure is often incomplete or even absent.
RTSU-Net clearly outperforms the other methods on both data
sets. It can be also observed that there are less outliers with
low IOU, indicating RTSU-Net is more robust.
In terms of computational efficiency, the proposed method
consumes even less memory than the baseline approach, with
only a slight increase in the Multi-Adds operations (MAC).
Hence, we conclude that the proposed method outperforms the
other methods without introducing a substantial computational
overhead. The proposed method processes a single scan at test
time in 30 seconds on average, of which around 20 seconds
are spent on model inference and the remainder on pre- and

8

TABLE III: Quantitative results on the COPD and COVID-19 test sets. IOU and ASSD (in mm) metrics are given in mean ±
standard deviation. Boldface denotes the result significantly better than others (p < 0.01 with Bonferroni correction).
(a) COPD results
Method

MAC

#Param

3DU-Net [24]

10.5G

16.32M

FRV-Net [12]

7.2G

15.5M

PDV-Net [13]

7.2G

15.5M

14.5G

13.1M

RTSU-Net (ours)

Metric

Overall

Lungs

LUL

LLL

RUL

RLL

RML

interlobar

IOU
ASSD
IOU
ASSD
IOU
ASSD
IOU
ASSD

0.915±0.037
1.214±0.948
0.918±0.038
1.408±1.190
0.912±0.049
3.027±5.544
0.949±0.026
0.607±0.537

0.965±0.007
0.514±0.202
0.965±0.014
0.602±0.393
0.951±0.031
1.665±2.982
0.976±0.010
0.326±0.166

0.944±0.033
0.766±0.839
0.950±0.038
0.818±1.535
0.937±0.032
1.802±2.926
0.962±0.020
0.482±0.534

0.937±0.007
0.951±1.017
0.932±0.050
1.030±1.481
0.926±0.044
2.772±6.286
0.959±0.023
0.465±0.446

0.918±0.043
1.264±1.050
0.917±0.050
1.557±1.779
0.912±0.066
2.885±6.600
0.952±0.030
0.668±1.020

0.937±0.032
0.936±1.069
0.942±0.033
0.957±1.414
0.926±0.050
3.109±7.520
0.960±0.010
0.534±0.518

0.840±0.032
2.153±2.738
0.848±0.103
2.680±3.441
0.854±0.109
4.540±7.800
0.912±0.080
0.885±1.412

N/A
2.054±1.691
N/A
2.292±2.218
N/A
2.541±3.460
N/A
0.947±0.800

(b) COVID-19 results
Method

MAC

#Param

3DU-Net [24]

10.5G

16.3M

FRV-Net [12]

9.3G

15.5M

PDV-Net [13]

9.3G

15.5M

14.5G

13.1M

RTSU-Net (ours)

Metric

Overall

Lungs

LUL

LLL

RUL

RLL

RML

interlobar

IOU
ASSD
IOU
ASSD
IOU
ASSD
IOU
ASSD

0.904±0.051
1.388±1.055
0.905±0.049
1.236±1.058
0.891±0.051
1.908±1.727
0.922±0.040
0.866±0.729

0.946±0.030
0.840±0.666
0.952±0.029
0.711±0.742
0.943±0.030
0.877±0.771
0.956±0.020
0.581±0.425

0.936±0.031
0.894±0.910
0.936±0.029
0.796±0.695
0.927±0.035
1.379±2.158
0.944±0.020
0.603±0.310

0.890±0.104
1.491±1.560
0.907±0.075
1.248±1.607
0.885±0.086
1.582±1.596
0.922±0.041
0.793±0.586

0.911±0.068
1.392±1.753
0.909±0.065
1.346±1.569
0.896±0.075
3.451±5.194
0.927±0.061
0.969±1.391

0.914±0.075
1.454±2.215
0.914±0.083
1.250±2.484
0.903±0.082
1.425±2.409
0.924±0.061
0.917±1.348

0.870±0.093
1.710±1.806
0.862±0.096
1.541±1.500
0.844±0.097
1.705±1.464
0.893±0.082
1.049±1.284

N/A
2.213±2.025
N/A
1.950±1.934
N/A
2.718±2.343
N/A
1.226±1.508

RUL, RML, RLL, LUL, LLL: Right upper, Right middle, Right lower, Left upper, Left lower lobes. Overall: per-lobe mean.

TABLE IV: Ablation study on the both data set for the nonlocal module (Non-local) and the geometric features (Geometric) into the two-stage cascading framework. Boldface denotes
that a result is significantly better than others in the same
column (p < 0.01 with Bonferroni correction).
(a) COPD results
Method

Two- Non- GeoASSD
stage local metric

IOU

only two-stage
w/o geometric
RTSU-Net

X
X
X

0.940±0.031
0.942±0.031
0.949±0.026

X
X

X

1.122±1.315
0.956±1.395
0.607±0.537

(b) COVID-19 results
Method

Two- Non- GeoASSD
stage local metric

IOU

only two-stage
w/o geometric
RTSU-Net

X
X
X

0.916±0.045
0.918±0.037
0.922±0.04

X
X

X

1.025±0.893
1.370±1.582
0.866±0.729

post-processing.
B. Ablation study
Table IV shows the results of the ablation study, where we
compare the two-stage cascading framework without non-local
modules, the framework with non-local modules without the
geometric term, and the RTSU-Net. The results on both the
COPD and COVID-19 data demonstrate the added value of
the non-local module and show that the introduction of the
geometric features increases the performance over the nonlocal module alone. This effect is most pronounced for the
surface distance metric.
C. Effect of the Non-Local module
In theory, the proposed non-local module can achieve a
global receptive field in an efficient way instead of using

Fig. 3: Effective Receptive Field (ERF) before the non-local
module (2nd row) and after (3rd row) by running forward pass
for the first RU-Net on a CT scan from the COVID-19 test
set. The green area indicates non-zero gradients (with respect
to the input scan) of a feature at a location in the input scan
corresponding to the red square (1st row).

aggressively down-sampled input or relying on much deeper
CNN architectures. To measure the effective receptive field
(ERF) size before and after the non-local operation, we
computed the gradients ∂F
∂I i of the feature at the location
i in the feature map F to the input image I. We run a forward
pass for the first RU-Net on a CT scan from the COVID-19
test set. The ERF of the features at the same corresponding
location before and after the non-local operation are visualized
in Fig. 3 for three orthogonal slices.

9

lung is present. We can also clearly see the attention weights
follows the lobe borders. Figure V-C (b) shows a case with
multiple ground-glass lesions, where the interactions between
the feature representing the region nearby the right middle
lobe and features presenting other regions in the entire lung.
Interestingly, we note that by introducing the geometric term
in the non-local module, attention weights also correspond to
the lung bounding box.
D. Qualitative Results

(a) intra-lobe dependency

Fig. 5 shows results for the 3D U-Net, PDV-Net [13], FRVNet [12], and the RTSU-Net from top to bottom. For comparison, reference segmentations are provided in the bottom row.
We selected three COPD (4-6 column) and three COVID-19
cases (1-3 column) with various levels of pathological and
anatomical variations. We observed that all methods usually
do not produce oversegmentation of the lungs. By capturing
feature dependencies, we see that the proposed method generates generally smoother lobe borders and is even able to infer
the approximately correct lobe shapes when the lung is filled
with fluid (1st column).
E. Comparison with human readers

(b) long-range dependency
Fig. 4: The Self-Attention weights (2nd row) from the proposed non-local module for the feature whose location is
shown using the green spot in the original input scan (1st
row). We use color map jet [39] for this plot. Two scans from
the COVID-19 test set are shown. (a) demonstrates mostly
the feature dependencies within the lobe in the clear lung. (b)
indicates long-range dependencies are required when the target
lobe is affected by disease.

The figure renders non-zero gradients in green and indicates
the center of the ERF with a red square. The center is a mapped
coordinate from the chosen feature in the feature map to the
input image via up-sampling. Thus a slight shift may occur.
The left image shows the ERF before the non-local operation is
contained in a square due to the nature of stacked convolutions.
However, the ERF after non-local on the right side shows a
non-square distribution, reaching the other side of the lung.
We, therefore, conclude that the non-local module can enlarge
the effective receptive field dramatically.
To study the structured relationships between features, we
visualize the self-attention weights for the feature at location
i given the feature map x and geometric features µ. We
run a forward pass for the first stage RU-Net on two CT
scans from the COVID-19 test set. The attention weights are
the i-th row vector in the self-attention matrix corresponding
to f (xi , xj ) + τ (µi , µj ) from the equation 2. Figure V-C
(a) shows the feature at the location i (green dot) mostly
depends on the information within the lobe when the health

To evaluate human performance, we asked two independent
human readers (analysts) to manually segment the lobes from
scratch, given segmentation of the lung. Their results are
evaluated on a random set of 100 scans from the COPD
test set. The human readers achieved 0.953 ± 0.017 IOU
and 0.501 ± 0.193 ASSD (in mm) on average, while the
RTSU-Net achieved 0.953 ± 0.015 IOU and 0.541 ± 0.231
ASSD. The human readers and the RTSU-Net method are both
significantly better than the other methods.
In terms of lung segmentation, the analysts reached 0.974±
0.015 IOU and 0.34 ± 0.214 ASSD on average, while the
RTSU-Net achieved 0.977±0.009 IOU and 0.325±0.2 ASSD
on average.
Regarding the fissure alignment, the analysts reached
0.686±0.361 ASSD on average while the RTSU-Net achieved
0.835 ± 0.398 ASSD on average. We conclude that the RTSUNet method performs comparably to humans for segmenting
the lung and the lobes.
F. Validation on LOLA11
We
have
applied
our
method
to
the
55
scans of the LOLA11 challenge, available on
https://lola11.grand-challenge.org/.
This
is an independent test set in which approximately half of
these scans are very difficult to segment due to the presence
of gross pathology. Lobar borders are completely invisible in
some of these scans.
Our method (submission date May 3, 2020) reaches a
mean IOU of 0.9197 for the lobe segmentation and 0.9706
in lung segmentation. This score is comparable to the other
top participants and ranks #2 for automatic lobe segmentation
methods, after submission of a not yet published variant of
LobeNet (submission date November 20, 2019).

10

Fig. 5: Qualitative comparison of segmentation results for six representative test cases. The left three columns show COVID-19
cases, the right three columns show COPD cases. From top to bottom: input image, 3DU-Net baseline, PDV-Net, FRV-Net,
the proposed RTSU-Net, and the segmentation reference. right upper, right middle, right lower, left upper, left lower
lobes.

11

VI. D ISCUSSION AND C ONCLUSION
We have presented a novel method using relational twostage convolution neural networks for segmenting pulmonary
lobes in CT images. The proposed method is capable of
capturing visual and geometric correspondence between highlevel convolution features, which may represent the relationships between objects and object parts. This proposed nonlocal module can also be used to effectively and efficiently
enlarge the receptive field of convolution features. This module
can be easily used as a common neural network layer in
other computer vision tasks such as object detection and
classification.
We show in our results that learning feature dependencies
improves the lobe segmentation performance significantly on
the COPD and the COVID-19 data set. The average symmetric
distance metric in the ablation study shows that using geometric features is effective for generating more precise object
boundaries. This can also be observed from the qualitative
results, where the lobe boundaries from the proposed method
are more consistent with the reference lobe shapes. Without
depending on prior lung segmentation, our approach serves as
an end-to-end lobe segmentation framework that can be used
for lung segmentation as well, by taking the union of lobes
per lung.
In terms of computational efficiency, our method maintains the same level of Multi-Adds operations (MAC) as
the standard 3D U-Net and two other approaches previously
proposed for pulmonary lobe segmentation. It requires even
fewer trainable parameters compared to the standard 3D UNet. Our method can be trained and tested on a consumerlevel GPU with 12 GB memory, and speed at test time is
around 30 seconds for a full resolution CT scan (20 seconds
for deep learning inference and 10 for pre-processing and postprocessing).
For segmenting the lobes in scans of COPD patients, the
previously published LobeNet method [15] reported excellent
performance on 1076 scans from COPDGene, with an ASSD
of 0.138 mm, well below the voxel resolution and well below
what RTSU-Net and independent human analysts achieved in
a set of 100 COPDGene scans in this study (Sect. V-E). These
metrics are not directly comparable as [15] used a different set
of scans and a reference partly provided by a software package.
For future studies, it would be interesting to directly compare
both approaches. On LOLA11, LobeNet outperformed RTSUNet by a very small margin. We noticed failures of RTSU-Net
on scans with abnormalities distinct from what occurred in the
COPD and COVID-19 training data.
Segmentation of lobes in scans of patients with severe
pneumonia due to COVID-19 is not an easy task. In this work,
we used only 370 COVID-19 CT scans for training. Thanks
to pre-training on 4000 COPD scans, we still obtained good
results with a small training set, and we were able to provide
lobe segmentations robust to the presence of ground-glass,
consolidations, and crazy paving.
Lobe segmentation is an important prerequisite for accurate quantification of lung damage in COVID-19 CT scans.
Fig. 5 shows that the standard 3D U-Net (2nd row), PDV-

Net (3rd row), and FRV-Net (4th row) may miss areas of
consolidation (3rd column) while the RTSU-Net found the
lobes accurately. RTSU-Net also performs reasonably well
when this lobe is completely filled with pleural fluid (first
column). Nevertheless, we also see that sometimes the border
of the segmentations of the proposed method is incorrect (3rd
column, the right upper lobe shows a slight oversegmentation
across the lobar borders towards the shoulder),
We hypothesize that a larger training set would further
improve performance, especially for cases with gross pathological changes that are not yet well represented in the current
training scans. Nevertheless, the results presented here are
sufficient for further analysis, and we believe that they will
prove useful in automated per-lobe severity scoring. This is a
topic for future research.
We freely share our segmentation algorithm on
https://grand-challenge.org/algorithms/
and
provide results for public data such as the scans from
https://coronacases.org/.
R EFERENCES
[1] M. Prokop, W. van Everdingen, T. van Rees Vellinga, J. Quarles van
Ufford, L. Stöger, L. Beenen, B. Geurts, H. Gietema, J. Krdzalic,
C. Schaefer-Prokop, B. van Ginneken, M. Brink, and The “COVID-19
Standardized Reporting” Working Group of the Dutch Radiological
Society, “CO-RADS – a categorical CT assessment scheme for patients
with suspected COVID-19: definition and evaluation,” Radiology,
p. 201473, 2020. [Online]. Available: https://pubs.rsna.org/doi/abs/10.
1148/radiol.2020201473
[2] M. Kubo, N. Niki, S. Nakagawa, K. Eguchi, M. Kaneko, N. Moriyama,
H. Omatsu, R. Kakinuma, and N. Yamaguchi, “Extraction algorithm of
pulmonary fissures from thin-section CT images based on linear feature
detector method,” IEEE Trans Nucl Sci, vol. 46, no. 6, pp. 2128–2133,
1999.
[3] M. Kubo, N. Niki, K. Eguchi, M. Kaneko, M. Kusumoto, N. Moriyama,
H. Omatsu, R. Kakinuma, H. Nishiyama, K. Mori et al., “Extraction of
pulmonary fissures from thin-section CT images using calculation of
surface-curvatures and morphology filters,” in ICIP, vol. 2, 2000, pp.
637–640.
[4] J. Wang, M. Betke, and J. P. Ko, “Pulmonary fissure segmentation on
CT,” Med Image Anal, vol. 10, no. 4, pp. 530–547, 2006.
[5] J. Pu, J. K. Leader, B. Zheng, F. Knollmann, C. Fuhrman, F. C.
Sciurba, and D. Gur, “A computational geometry approach to automated
pulmonary fissure segmentation in CT examinations,” IEEE Trans Med
Imaging, vol. 28, no. 5, pp. 710–719, 2008.
[6] E. M. van Rikxoort, B. van Ginneken, M. Klik, and M. Prokop,
“Supervised enhancement filters: application to fissure detection in chest
CT scans,” IEEE Trans Med Imaging, vol. 27, no. 1, pp. 1–10, 2007.
[7] B. Raasch, E. Carsky, E. Lane, J. O’callaghan, and E. Heitzman, “Radiographic anatomy of the interlobar fissures: a study of 100 specimens,”
AJR Am J Roentgenol, vol. 138, no. 6, pp. 1043–1049, 1982.
[8] J.-M. Kuhnigk, V. Dicken, S. Zidowitz, L. Bornemann, B. Kuemmerlen,
S. Krass, H.-O. Peitgen, S. Yuval, H.-H. Jend, W. S. Rau et al., “New
tools for computer assistance in thoracic CT. part 1. functional analysis
of lungs, lung lobes, and bronchopulmonary segments,” Radiographics,
vol. 25, no. 2, pp. 525–536, 2005.
[9] E. M. Van Rikxoort, M. Prokop, B. de Hoop, M. A. Viergever, J. P.
Pluim, and B. van Ginneken, “Automatic segmentation of pulmonary
lobes robust against incomplete fissures,” IEEE Trans Med Imaging,
vol. 29, no. 6, pp. 1286–1296, 2010.
[10] B. Lassen, E. M. van Rikxoort, M. Schmidt, S. Kerkstra, B. van Ginneken, and J.-M. Kuhnigk, “Automatic segmentation of the pulmonary
lobes from chest CT scans based on fissures, vessels, and bronchi,” IEEE
Trans Med Imaging, vol. 32, no. 2, pp. 210–222, 2012.
[11] F. J. Bragman, J. R. McClelland, J. Jacob, J. R. Hurst, and D. J.
Hawkes, “Pulmonary lobe segmentation with probabilistic segmentation
of the fissures and a groupwise fissure prior,” IEEE Trans Med Imaging,
vol. 36, no. 8, pp. 1650–1663, 2017.

12

[12] F. T. Ferreira, P. Sousa, A. Galdran, M. R. Sousa, and A. Campilho,
“End-to-end supervised lung lobe segmentation,” in IJCNN, 2018, pp.
1–8.
[13] A.-A.-Z. Imran, A. Hatamizadeh, S. P. Ananth, X. Ding, N. Tajbakhsh,
and D. Terzopoulos, “Fast and automatic segmentation of pulmonary
lobes from chest CT using a progressive dense V-network,” Comput
Methods Biomech Biomed Eng Imaging Vis, pp. 1–10, 2019.
[14] W. Wang, J. Chen, J. Zhao, Y. Chi, X. Xie, L. Zhang, and X. Hua,
“Automated segmentation of pulmonary lobes using coordination-guided
deep neural networks,” in ISBI. IEEE, 2019, pp. 1353–1357.
[15] S. E. Gerard and J. M. Reinhardt, “Pulmonary lobe segmentation using
a sequence of convolutional neural networks for marginal learning,” in
ISBI, 2019, pp. 1207–1211.
[16] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
neural networks for volumetric medical image segmentation,” in 3DV.
IEEE, 2016, pp. 565–571.
[17] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in CVPR, 2017, pp. 4700–4708.
[18] S. E. Gerard, T. J. Patton, G. E. Christensen, J. E. Bayouth, and J. M.
Reinhardt, “Fissurenet: A deep learning approach for pulmonary fissure
detection in CT images,” IEEE Trans Med Imaging, vol. 38, no. 1, pp.
156–166, 2018.
[19] S. E. Gerard, J. Herrmann, D. W. Kaczka, G. Musch, A. FernandezBustamante, and J. M. Reinhardt, “Multi-resolution convolutional neural
networks for fully automated segmentation of acutely injured lungs in
multiple species,” Med Imag Anal, vol. 60, p. 101592, 2020.
[20] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image
recognition,” in ICCV, 2019, pp. 3464–3473.
[21] L. Zhang, D. Xu, A. Arnab, and P. H. Torr, “Dynamic graph message
passing networks,” arXiv preprint arXiv:1908.06955, 2019.
[22] A. Bernheim, X. Mei, M. Huang, Y. Yang, Z. A. Fayad, N. Zhang,
K. Diao, B. Lin, X. Zhu, K. Li, S. Li, H. Shan, A. Jacobi, and M. Chung,
“Chest CT findings in coronavirus disease-19 (COVID-19): relationship
to duration of infection,” Radiology, p. 200463, 2020.
[23] H. Shi, X. Han, N. Jiang, Y. Cao, O. Alwalid, J. Gu, Y. Fan, and
C. Zheng, “Radiological findings from 81 patients with COVID-19
pneumonia in Wuhan, China: a descriptive study,” Lancet Infect Dis,
2020.
[24] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
“3D U-Net: learning dense volumetric segmentation from sparse annotation,” in MICCAI, 2016, pp. 424–432.
[25] R. P. Poudel, P. Lamata, and G. Montana, “Recurrent fully convolutional
neural networks for multi-slice mri cardiac segmentation,” in Reconstruction, segmentation, and analysis of medical images, 2016, pp. 83–
94.
[26] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difficulty of training
recurrent neural networks,” in ICML, 2013, pp. 1310–1318.
[27] K. Kamnitsas, C. Ledig, V. F. Newcombe, J. P. Simpson, A. D. Kane,
D. K. Menon, D. Rueckert, and B. Glocker, “Efficient multi-scale 3D
CNN with fully connected CRF for accurate brain lesion segmentation,”
Med Imag Anal, vol. 36, pp. 61–78, 2017.
[28] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” arXiv:1409.0473, 2014.
[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in NIPS, 2017,
pp. 5998–6008.
[30] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,” in CVPR, 2018, pp. 7794–7803.
[31] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, “Ccnet:
Criss-cross attention for semantic segmentation,” arXiv:1811.11721,
2018.
[32] E. A. Regan, J. E. Hokanson, J. R. Murphy, B. Make, D. A. Lynch,
T. H. Beaty, D. Curran-Everett, E. K. Silverman, and J. D. Crapo,
“Genetic epidemiology of COPD (COPDGene) study design,” COPD,
vol. 7, no. 1, pp. 32–43, 2011.
[33] E. M. van Rikxoort, M. Prokop, B. de Hoop, M. A. Viergever, J. P.
Pluim, and B. van Ginneken, “Automatic segmentation of the pulmonary
lobes from fissures, airways, and lung borders: evaluation of robustness
against missing data,” in MICCAI, 2009, pp. 263–271.
[34] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
in CVPR, 2014, pp. 580–587.
[35] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based object
detectors with online hard example mining,” in CVPR, 2016, pp. 761–
769.

[36] C. H. Sudre, W. Li, T. Vercauteren, S. Ourselin, and M. J. Cardoso,
“Generalised Dice overlap as a deep learning loss function for highly
unbalanced segmentations,” in DLMIA, 2017, pp. 240–248.
[37] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in
PyTorch,” in NIPS Autodiff Workshop, 2017.
[38] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification,” in
ICCV, 2015, pp. 1026–1034.
[39] L. Zhou and C. D. Hansen, “A survey of colormaps in visualization,”
IEEE Trans Vis Comput Graph, vol. 22, no. 8, pp. 2051–2069, 2015.

