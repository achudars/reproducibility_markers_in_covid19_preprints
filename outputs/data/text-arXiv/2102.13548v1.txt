arXiv:2102.13548v1 [stat.ME] 26 Feb 2021

1

Variational Full Bayes Lasso:
Knots Selection in Regression Splines
Larissa Alves1 , Ronaldo Dias2 , Helio S. Migons3
March 1, 2021

Abstract
We develop a fully automatic Bayesian Lasso via variational inference. This is a scalable procedure for
approximating the posterior distribution. Special attention is driven to the knot selection in regression spline.
In order to carry through our proposal, a full automatic variational Bayesian Lasso, a Jefferey’s prior is proposed
for the hyperparameters and a decision theoretical approach is introduced to decide if a knot is selected or
not. Extensive simulation studies were developed to ensure the effectiveness of the proposed algorithms. The
performance of the algorithms were also tested in some real data sets, including data from the world pandemic
Covid-19. Again, the algorithms showed a very good performance in capturing the data structure.

1

Introduction
In the recent literature one finds many alternative proposals for modeling and estimating a smooth function.

In this article we focus on variants of smoothing splines, called penalized regression splines (Montoya et al. [2014],
Eilers and Marx [1996]). This is an attractive approach for modeling the nonlinear smoothing effect of covariates.
This work discusses the selection of knots given a fixed maximum number of knots. A roughness penalty is
introduced to control the selection of knots and consequently to balance the two conflicting goals, goodness of
fit and smoothness. Our approach will be through a full Bayesian Lasso with variational inference. It is related
to the work of Osborne et al. [1998] where an efficient algorithm to calculate the classical Lasso estimator was
presented. Our contribution, therefore, includes the application of the mean field variational inference (Blei et al.
[2017], Ormerod and Wand [2010]) for the complete Bayesian lasso penalty (Park and Casella [2008] and Mallick
and Yi [2014])). Choosing the ideal number of knots and their position is a difficult problem. We propose a
two-step procedure related to the work of Ruppert [2002]. For regularization and model selection, the proposed
procedure starts with a fixed maximum number of knots and then uses a full Bayesian lasso, which combines
characteristics of shrinkage and variable selection, to obtain the most significant knots to recover the unknown
smooth function. The number of knots is chosen based on an approximation of the predictive distribution in a
grid of knots values.
The original formulation of the Bayesian Lasso is based on a hierarchical representation of the Laplace distribution, as a mixture of scale normal based on exponential (Park and Casella [2008]) and more recently as a
mixture of uniform with exponential (Mallick and Yi [2014]).
∗ The

authors,(LA)1 ENCE, (RD)2 IMECC-UNICAMP and (HSM)3 IM-UFRJ, contributed equally to the design and implementation
of the research, to the analysis of the results and to the writing of the manuscript. Corresponding author. E-mail addresses:
migon@im.ufrj.br

1

Alternative procedures for selecting the effective number of knots involving least squares and penalized splines
regression has been proposed in the recent literature, see (Spiriti et al. [2013], Montoya et al. [2014]).
It is well known that MCMC often takes a great deal of computational time and is not scalable. Therefore,
our proposal is to use variational inference (VI) integrated with a decision theoretical approach to knot selection
in regression splines. Both are discussed in detail and have shown to be comparatively better than the alternatives
presented in the current literature.
The remainder of the paper is organized as follows. In Section 2 presents a review the Bayesian linear model,
the variational inference and the hierarchical formulation of the Laplace distribution. In Section 3, shows the
full Bayesian Lasso, including the Jeffrey’s prior for the hyperparameters and the Bayes factor criterion for knots
selection. Section 4 states the knot selection procedure for regression spline in an almost fully automatic algorithm.
Section 5 shows a comparative numerical simulation of the performance of the proposed method and other existing
approaches in the literature. A data analysis of real datasets is presented in Section 6.

2

A review of the Methodology
In order to set the notation to be used later, this section presents a brief summary of Bayesian regression

models and variational inference techniques and also establishes the framework for our proposal to select knots in
regression spline models to be developed in Section 4.
We summarize the conjugate Bayesian analysis of a linear model. In addition, we present an introduction
to variational inference and the hierarchical representation of the Laplace distribution. For more details see,
Drugowitsch [2019], Denison et al. [1998], Berry et al. [2002], Goepp et al. [2018] and Lang and Brezger [2004].

2.1

Bayesian estimation in linear models

Following the notation of Migon et al. [2015] let the linear model be
y | β, φ ∼ N (Xβ, φ−1 In )
where y is n-vector of observed quantities, X is a known n × p matrix, β is a p-vector of parameters and φ is
the precision associated with each one of the independent observations. The conjugate prior, a Normal-Gamma,
is defined as:
β|φ ∼
φ ∼

N (m0 , φ−1 C0−1 )
Ga(a0 , b0 )

where m0 and (φ C0 )−1 are, respectively, the prior mean and covariance matrix and a0 , b0 are the parameters of
the precision prior distribution. The posterior distribution is
β|φ, y, X
φ|y, X
m1
a1

∼ N (m1 , φ−1 C1−1 )
∼ Ga(a1 , b1 )
= C1−1 (C0 m0 + X T y) and C1 = C0 + X T X
n
1
= a0 +
and b1 = b0 + [(y − Xm1 )T y + (m0 − m1 )T C0 m0 ]
2
2

A very useful extension of the above regression model is the Bayesian hierarchical regression models, which
2

will be extensively used latter. It was proposed in the seminal paper of Lindley and Smith [1972] and a dynamic
version was introduced in Gamerman and Migon [1993].

2.2

Variational Inference - main aspects

It is well known that Bayesian inference regarding unknown quantities is entirely based on their probabilistic
description. Therefore, variational inference (VI), a method to deal with the approximation of probability densities
is very useful for Bayesian inference. In fact, these techniques can be traced back to the field of machine
learning (Jordan et al. [1999]). Loosely speaking, they basically exchange sampling, as in MCMC procedures, for
optimization. By choosing a flexible family of approximate densities, an attempt is made to find a member of
this family, which minimizes some optimal criterion, for example Kulback-Leibner divergence (KL). Variational
inference is useful for quickly comparing alternative models and also for dealing with large data sets. Blei et al.
[2017] pointed out that the accuracy of variational inference has not yet been thoroughly studied and many open
questions are still there to be answered.
The basic ideas about variational inference can be easily followed in Blei et al. [2017] and in Ormerod and
Wand [2010]. Many examples are presented in the Bishop [2006] book. Let y be a vector of n independent
identically distributed observations and z a vector including latent variables and the parameters as well. The log
marginal data distribution, also known as evidence integral, is denoted by p(y). Evidence integrals that are often
unavailable in closed form require exponential time to be evaluated and present difficulties in making the inference
for a model such as this. To avoid calculating the evidence integral, one tries to find a lower bound, which is
known as ELBO(q) - Evidence Lower Bound and will be denoted by L(q). It is easy to verify that:
log p(y) = L(q) + KL(q||p),
where L(q) =

R

q(z) log

p(y,z)
q(z) dz

and KL(q||p) = −

R

q(z) log

p(z|y)
q(z) dz,

since p(y) =

p(y,z)/q(z)
p(z|y)/q(z) .

It is clear that maxq L(q) ' minq KL(q||p) and also that KL(q||p) ≥ 0 with equality if and only if p(z|y) =
q(z). In general, it is difficult to obtain this posterior distribution, therefore, the approach is to choose a family
of tractable densities. Let’s assume the following:
q(z) =

m
Y

ql (zl )

l=1

where a partition of the z into m disjoint groups is denoted as zl . It is worth pointing out that there is no
restriction on the functional forms of the variational densities ql (zl ).
The central idea is to maximize each factor (blocks of z’s) of q(z) in turn. We keep ql6=h fixed and maximize
L(q). Note that:
L(q)

=

Z Y
m

ql (zl )[log p(y, z) − log ql (zl )]dz

l=1

Z
=

Z
Z
Y
qh (zh ) [ log p(y, z)
ql (zl ) dzl ] dzh − qh (zh ) log qh (zh ) dzh + const
l6=h

Z
=

Z
qh (zh ) log p̃(y, zh ) dzh −

qh (zh ) log qh (zh ) dzh + const

(1)

where p̃(y, zh ) = El6=h log p(y, z) + const. The L(q) will be presented for the specific case of Lasso in subsection

3

3.3. Note that it depends on the variational parameters.
Worth emphasizing that the problem of approximating the posterior distribution for the parameters of interest
was replaced for a maximization problem. The algorithm to solve the optimization problem was introduced by
Bishop [2006] and denoted by CAVI - coordinate ascent variational inference. The CAVI optimizes one factor of
the mean field variational density at a time.
Since (1) is equal to −KL(·||·), maximizing it is equivalent to minimizing KL. Therefore, the optimal solution
is:
log q ∗ (zl ) = El6=h [log p(y, z)] + const
As one can see, q ∗ (zl ) depends on the full conditional distributions, as usually denoted in the MCMC literature
(Casella and George [1992]). Therefore, there is a natural link with Gibbs Sampling but the proposed approach
leads to tractable solutions involving only local operations.

2.3

Hierarchical representation of the Laplace distribution

It is well known that the original Lasso formulation (Tibshirani [1996]) is related to the Laplace distribution
which can be represented as hierarchical mixture of distributions and is relevant to a hierarchical modeling, which
in turn is important for the implementation of Gibbs Sampling.
One of these forms of representation is a scale mixture of a Normal distribution with an Exponential distribution
(West [1987]) and the other is a mixture of a Uniform distribution with a Gamma distribution (Mallick and Yi
[2014]).
Specifically,following
Andrews [1974], it is easy to verify that the hierarchical representation: β|τ ∼ N [0, τ ]

λ2
and τ |λ ∼ exp 2 leads, by marginalizing on τ , to the standard Laplace distribution, whose density is:

λ
exp(−λ |β|) =
2

Z

∞



0


 2 
  2
τ −1/2
λ
λ
β2
√
exp −
exp − τ
dτ.
2τ
2
2
2π

(2)

The above hierarchical representation of the Laplace distribution is important to introduce the full Bayesian
Lasso. The penalty term in the classical Lasso can be interpreted as independent Laplace prior distribution over
the regression parameters. Moreover, the posterior mode can be seen as the Lasso estimates.

3

The full Bayesian Lasso
Following the hierarchical representation for the Laplace distributions in Subsection 2.3, Park and Casella

[2008] shows a Bayesian formulation of the Lasso regression model. The hierarchical model is defined as:
y|X, β, φ ∼ N [Xβ, φ−1 In ]
β|φ, τ
τj |λ

∼ N [0, φ−1 Dτ ]
∼ Exp(λ)

with j = 1, . . . , p

where Dτ = diag(τ1 , . . . , τp ) and τj |λ are conditionally independent for all j. The model can be completed
with the hyperparameters of the priors φ ∼ Ga(a0 , b0 ) and λ ∼ Ga(g0 , h0 ). In Subsection 3.1 we propose an
independent Jeffreys prior for φ and λ to automate the Lasso, and this implies supposing a0 , b0 , g0 and h0 tending
to zero.

4

Let θ = (β, φ, τ , λ) be the vector of the parameters and the latent variables of the model. The posterior distribution is obtained as proportional to the model distribution times the prior distribution for the latent component
and the parameters:
p(θ|y, X) ∝ p(y|X, β, φ) p(β|φ, τ ) p(τ |λ) p(φ) p(λ).
For instance, the above joint posterior is often intractable. An almost obvious numerical approach, since the
breakthrough paper of Gelfand and Smith [1990], is to use stochastic simulation.

3.1

Jeffreys prior using Fisher decomposition

In order to develop an automatic Bayesian Lasso procedure it is worth to introduce non informative priors
for the hyperparameters involved. Following Fonseca et al. [2019] and exploring the conditional independence
involved in the Lasso model, the Fisher information decomposition for Lasso follows as:
Iy (λ) = Iτ (λ) − Ey [Iβ ,τ (λ|y)],

(3)

where Iβ ,τ (λ|y) is the information obtained from the full conditional distribution p(β, τ |y, λ). We also are using
the conditional independence described by the graph that represents the Bayesian Lasso model.
We will develop, in turn, each of the components in the expression (3). The quantity Iτ (λ) is based on the
independent marginal distribution of τj , leading directly to Iτ (λ) = λp2 .
In order to obtain Iβ ,τ (λ|y), we take advantage of the known full conditional distribution of (β, τ |λ, y) (see
(4)). Since (β|τ , λ, y) does not depend on λ, then it is easy to obtain Ey [Iβ ,τ (λ|y)] = λp .
Then substituting in (3), it follows Iy (λ) =

p
λ2

+

p
λ2

and so the prior for λ is p(λ) ∝ λ−1 . This result is

similar to the one reported in Fonseca et al. [2019], using the Uniform Gamma mixture.
It is well known that the Jeffrey’s prior of φ is proportional of φ−1 .

3.2

The MCMC formulation

Considering the model and the prior distribution already specified, we know that the posterior distribution in
this case has an unknown form. Therefore, we can use the MCMC to obtain a sample of the posterior distribution
through the complete conditional distributions (Gibbs Sampler). Calculations of complete conditionals are as
follows.


1 T
−1 T
−1 −1
(X
X
+
D
)
(X T X + D−1
)
X
y,
τ
τ
φ


1
GIG
, 2λ, β 2j φ
2


n p
1
Ga
+ + a0 , b0 + [(y − Xβ)T (y − Xβ) + β T D−1
β]
τ
2
2
2


p
X
Ga g0 + p, h0 +
τj 


(β|y, θ −β ) ∼
(τj |y, θ −τj ) ∼
(φ|y, θ −φ ) ∼
(λ|y, θ −λ ) ∼

N

(4)

j=1

where θ − stands for the entire vector θ without the parameter followed by symbol ” ”, and GIG denotes the
generalized inverse Gaussian distribution. See appendix.

5

3.3

The variational approximation applied to Lasso

In order to obtain a scalable inference procedure, we introduce an alternative methodology.
To make the notation consistent, the vector including latent variables and parameters denoted by z in the
subsection 2.2 is represented in this section by the vector θ. Let the independent Jeffrey’s prior be p(φ) ∝
p(λ) ∝

1
λ.

1
φ

and

The joint distribution of the observations, latent components and parameters can easily be followed

from the Figure 1 which in turn summarizes the model.

λ

τ𝑗

1: 𝑝

1: 𝑛

β𝑗

𝑦𝑖

𝜙

𝑥𝑖

Figure 1: Directed acyclic graph
It is worth remembering the expression of the mean field posterior approximation for the latent components
and parameters:
log(q(θ)) = log(q1 (β, φ)) + log(q2 (τ |λ)) + log(q3 (λ))
After quoting Blei et al. [2017] the optimal ql (θ l ) is proportional to the exponential of the log of the complete
conditional distribution that is calculated in (4)
ql∗ (θ l ) ∝ exp{E−l [log p(θ l |θ −l , y)]}, l = 1, 2, 3.
In the first step, the variational posterior for β and φ, that maximizes the variational bound L(q) while holding
q2 (τ |λ) and q3 (λ) fixed, is given by
log q1∗ (β, φ)

=

log(p(y|β, φ)) + Eτ [log(p(β, φ|τ ))] + const

=

log N (β|mβ , φ−1 Cβ ) × Ga(φ|aφ , bφ )

It is easy to see that this is a normal-gamma distribution with parameters:

6

T
Cβ−1 = Eτ (D−1
τ ) + X X,

and

aφ = a0 + n/2,

and

mβ = Cβ X T y,
1
bφ = b0 + (yT y − mTβ Cβ−1 mβ ).
2

Next, the variational distribution of τ , that maximizes the variational bound L(q) while holding q3 (λ) fixed is
given by

log q2∗ (τj )

= Eλ [log(p(τj |λ))] + Eβ,φ [log(p(βj , φ|τj ))] + const
log GIG(τj |cτ , dτ , fτj )

=

with GIG being generalized inverse Gaussian distribution, where
cτ =

1
; dτ = 2Eλ [λ] ; fτj = Eβ,φ [φβj2 ].
2

Therefore,
log q2∗ (τ ) = log

p
Y

GIG(τj |cτ , dτ , fτj ).

j=1

Finally, we will identify the variational distribution of λ:

log q3∗ (λ)

=

log(p(λ)) + Eτ [log(p(τ |λ))] + const

=

log Ga(λ|gλ , hλ )

which is a gamma distribution with parameters
gλ = g0 + p ; hλ = h0 +

p
X

Eτ (τj ).

j=1

The expected values involved in the definition of the above variational distributions are computed as follows
(see the appendix for details and Jørgensen [1982]).

p
fτj κcτ +1 ( dτ fτj )
p
√
,
dτ κcτ ( dτ fτj )

p
fτj Kcτ +2 ( dτ fτj )

p
−
dτ
Kcτ ( dτ fτj )
p

Eτ (τj )

=

(5)
!2 
p
Kcτ +1 ( dτ fτj )
,
p
Kcτ ( dτ fτj )

V arτ (τj )

=

Eτ [D−1
τ ]

p
√
dτ κcτ +1 ( dτ fτj ) 2cτ
p
−
= diag(Eτ (τ1−1 ), . . . , Eτ (τp−1 )), where Eτ (τj−1 ) = p
,
f τj
fτj κcτ ( dτ fτj )

Eβ,φ [φβj2 ]

=

Eλ (λ)

=

m2βj aφ /bφ + (Cβ )jj ,
gλ
hλ

7

(6)

where κp (·) is the Bessel modified function of the second kind.
The evidence lower bound (ELBO) for this model consists of:
L(q)

= Eβ,φ (log p(y|X, β, φ)) + Eβ,φ,τ (log p(β, φ|τ )) + Eτ,λ (log p(τ |λ)) +
+Eλ (log p(λ)) − Eβ,φ (log q1 (β, φ)) − Eτ,λ (log q2 (τ |λ)) − Eλ (log q3 (λ))

Each of the above terms are evaluated as function of the variational parameters, as follows:

Eβ,φ (log p(y|X, β, φ))

Eβ,φ,τ (log p(β, φ|τ ))

Eτ,λ (log p(τ |λ))
Eλ (log p(λ))

n
(ψ(aφ ) − log bφ − log 2π) +
2 

1 aφ
−
(y − Xmβ )T (y − Xmβ ) + tr(X T XCβ )
2 bφ
aφ
p
(ψ(aφ ) − log bφ − log 2π) + (a0 − 1)(ψ(aφ ) − log bφ ) − b0
+
=
2
bφ

 
p
p 
1X
1X
aφ
1
+
Eτ (log τj ) −
mβj
+ (Cβ )jj Eτ
2 j=1
2 j=1
bφ
τj

=

p
gλ X
= p(ψ(gλ ) − log hλ ) −
Eτ (τj )
hλ j=1

= g0 log h0 − log Γ(g0 ) + (g0 − 1)(ψ(gλ ) − log hλ ) − h0

gλ
hλ

p
1
(ψ(aφ ) − log bφ − log 2π) − log |Cβ | + aφ log bφ − log Γ(aφ ) +
2
2
+(aφ − 1)(ψ(aφ ) − log bφ ) − aφ
p 
q
X
dτ
cτ
log
Eτ,λ (log q2 (τ |λ)) =
− log 2 − log Kcτ ( dτ fτj ) + (cτ − 1)Eτ (log τj ) +
2
f τj
j=1

 
fτj
dτ
1
−
Eτ (τj ) −
Eτ
2
2
τj
Eλ (log q3 (λ)) = − log Γ(gλ ) + (gλ − 1)ψ(gλ ) + log hλ − gλ

Eβ,φ (log q1 (β, φ))

=

The second order Taylor expansion for log τj at E(τj ) is used to obtain the approximation for its expected
value: E(log τj ) ≈ log E(τj ) −

V ar(τj )
2E 2 (τj )

where the mean and the variance of τj are in equations (5) and (6).

Note that the variational bound depends on the quantities mβ , Cβ , bφ , dτ , fτj e hλ . The algorithm updates
these quantities in each iteration. The ELBO is maximized and hence L(q) reaches a plateau with stabilization
of those quantities. The algorithm consists of the following steps:
Algorithm 1 Variational Inference
Step 1. Initialize the variational hyperparameters: mβ , Cβ , aφ , bφ , gλ , hλ , cτ , dτ , fτj .
Step 2.
while ELBO does not reach convergence do
for l = 1, 2, 3 do
Compute ql∗ (θ l ) ∝ exp{E−l [log p(θ l |θ −l , y)]}
Step 3. Update the variational hyperparameters based on the expected values.
Calculate ELBO
end for
end while

8

Convergence can be achieved by analyzing changes to ELBO in consecutive iterations or by analyzing the
quantities on which it depends.
We end this section by showing the predictive distribution. Let y o e y p be the observed and the predicted
vectors, respectively. Finally, let p(β, φ|y o ) be its variational component. Then, after some algebraic calculations,
we have a Student’s t-distribution (St) as follows:
p

o

p

p(y |y , X )

Z Z
=

p

o

Z Z

p(y |β, φ)p(β, φ|y )dβdφ ≈


= St y p |X T mβ , (1 + X T Cβ X)

bφ
(aφ − 1)

p(y p |β, φ)q1 (β, φ)dβdφ

, 2aφ ,

where q1 (β, φ) is the variational approximation of the posterior distribution, a normal-gamma distribution.

3.4

Variable selection

We will discuss, from a Bayesian point of view, three alternative procedures for selecting knots (variables) in
penalized regression splines (linear regression). In this work, we propose a new decision criterion based on the
Bayes factor. This proposed criterion is fully described below in 3.4.1
3.4.1

Bayes Factor decision criteria

In general, the selection of predictors/knots in a penalized regression/penalized regression splines () can be
seen as a decision problem. Consider the general case where it is necessary to decide for one of the following models
M0 : θ ∈ Θ0 or M1 : θ ∈ Θ1 based on some observations (D). An optimal decision will be based on the posterior
probabilities, p(M0 |D) and p(M1 |D) and, also on the cost of the wrong decisions. Denote by a the cost associate
for the choice of the model M0 when, in fact, the true model is M1 and let b be the cost of choosing model M1
when the true model is M0 . Therefore, if b p(M0 |D) > a p(M1 |D) then M0 should be chosen as the most
plausible model for θ. By Bayes’ theorem, the posterior odds are given by the product of the prior odds times the
R
Bayes factor, F B(M0 , M1 ) = p(D|M0 )/p(D|M1 ), where p(D|Mi ) = Θi p(D|θ, Mi )p(θ|Mi )dθ, i = 0, 1.
Hereafter, we will assume that the prior odds is equal one.
Particularly, we consider two alternatives: M0 : βj = 0 and M1 : βj = δ, where δ 6= 0 is a constant
to be defined. Under M0 , let us assume that βj |D ∼ N (0, s2j ) and under M1 we have βj |D ∼ N (δ, s2j ),
where s2j = var(βj |D). Hence, it is straightforward to get log(F B(M0 , M1 )) = log(exp{− 21 βj2 }/ exp{− 12 (βj −
δ)2 }) = 21 δ 2 − βj δ.
Assuming that at least a moderate evidence against M1 corresponds to F B(M0 , M1 ) ≥ 3 and β is considered
to be significantly distant from the M0 , if and only if it is greater or equal to the third quartile of the standard
normal distribution, that is β0.75 = 0.67, then a quadratic equation must be solved whose root is δ = 2.3. Thus,
our proposal to select knots in spline regression is described in the following algorithm:

9

Algorithm 2 Bayes Factor decision criteria
Step 1. Take βj∗ = mj /sj , a standardized point estimate of βj ,
Step 2. Compute the Bayes factor at βbj? : BF (M0 , M1 ) =

mj = E[βj |D] and s2j
b? )2 }
exp{− 1
(β
j
2
b? −δ)2 } , δ = 2.3.
exp{− 1 (β
2

= var(βj |D).

j

Step 3. Compute π ? = BF (M0 , M1 )/(1 + BF (M0 , M1 )).
Step 4. Define BF evidence
Choose two positive numbers a and b, (with a = 1 and b = 3, corresponding to a Bayes factor equal to 3)
a
if π ? < a+b
then
M0 is rejected and j th predictor is excluded from the model
else
the j th predictor is not excluded from the model.
end if

3.4.2

Other criteria

One procedure, due to (Li and Lin [2010]), is based on a 50% credible interval. That is, if the credible
interval of a given coefficient contains zero, the explanatory variable associated with it must be removed from the
model. A second criterion, named scaled neighborhood (Li and Lin [2010]) corresponds to evaluate the posterior
probability of [−sj , sj ], where s2j = var(βj |y) and decide the exclusion of a predictor if this probability exceeds a
certain threshold, for instance Li and Lin [2010] suggests 1/2 as this limit.

4

Knots Selection in Regression Spline
We start with the standard setup for nonparametric regression models. Let’s suppose we have a collection of

observations (yi , xi ) for i = 1, . . . , n such that
yi = f (xi ) + i ,

(7)

where f (xi ) = E[yi |xi ] are the values obtained by a unknown smooth function f that takes values on the interval
[a, b] ⊂ R and i is a sequence of random variables that are uncorrelated with mean zero and unknown precision
φ. A possible approach to estimate f is to assume that the regression curve f can be well approximated by
a spline function. See details in Dias [1999]. That this, given a sequence of knots κ = (κ1 , . . . , κK ) so that
κ1 < κ2 . . . < κK−1 < κK , a spline regression model can be written as:
f (x, β) = β0 +

p
X

βj x j +

j=1

K
X

βp+k (x − κk )p+ ,

(8)

k=1

where p is the degree of the polynomial spline and β is the vector of coefficients of dimension K + p + 1. The
functions (u)+ are the well known truncated power basis, (u)p+ = max(0, up ). Note that, for a fixed K, the
vector of knots κ and the set of basis functions {1, x, x2 , . . . , xp , (x − κ1 )p+ , . . . (x − κK )p+ }, an estimate of f ,
say fˆ, can be obtained by estimating the vector β. Such that,
b = β̂0 +
fˆ = f (x, β)

p
X

β̂j xj +

j=1

K
X

β̂p+k (x − κk )p+ .

k=1

It’s well known (Dias [1998], Dias and Gamerman [2002], Dias and Garcia [2007], Kooperberg and Stone [1991])
when K increases the bias gets smaller causing over-fitting but at the same time substantially increases the
10

variance. On the other hand, if K goes to zero then variance might drastically be reduced causing under-fitting
and considerably increases bias. Thus, K acts as the smoothing parameter in regression spline fit and hence it
balances the trade-off between over-fitting and under-fitting. A good procedure should not only provide an ideal
number of knots (or basis functions) but also quantify the uncertainty of adding or removing them. Figure 2
shows the effect of different values of K for a spline regression model.

4

Effect of K

K=6

K=60

3

K=3

2

*

*

*

*

*

1

ydat

*

*

*

*
*
*

**

*

0

*

*

* *

*
*

*

*

*

*

*

*

−1
−2

*
0.0

**

*

*
*

*

*

*

*

*

*

*

*
*

*

**

*

*

*

**

*
*

*

*
*

*

**

*

**

* *

***
*
**

*

*

**
*

*

*

*
* *

*

* *

*

*
*

*

*
*

*

*

*

*

*
*

**
*

*

*

*

0.2

*

0.4

0.6

0.8

1.0

dat

Figure 2: Large values of K causes over-fitting.
There are other basis functions that can represent a regression function such as B-splines, wavelets radial basis
etc. For all, it is still necessary to balance between under-fitting and over-fitting. Even in the case of smoothing
splines, the regularization parameter needs to be obtained. Specifically, in this work we are dealing with the
b
following optimization problem: Find β(λ)
the minimizer of
n
K
X
X
(yi − f (xi , β))2 + λ
|βp+k |,
i=1

(9)

k=1

where λ is the smoothing parameter. For large values of λ the solution of this optimization problem tends to the
polynomial regression fit, that is over-fitting. Note that the penalty term involves only the coefficients associated
to the knots sequence κ1 < κ2 . . . < κK . Consequently, selecting knots is equivalent to selecting the coefficients
that contribute most to the fitting. Under the Bayesian point of view, this work presents a novel and scalable
procedure for selecting knots.

4.1

The variational inference for knots selection

Following the idea of the Lasso procedure for variable selection, under the Bayesian point of view, the selection
of knots in the spline regression models can be made by assuming an independent Laplace prior distribution for the


T
T T
coefficients associated with the knots. We will denote β = β (1) , β (2)
where β (1) = (β0 , β1 , . . . , βp )T is
the polynomial coefficients with dimension p + 1 and β (2) = (βp+1 , . . . , βp+K )T of dimension K is the penalised
coefficients. The hierarchical structure presented in the subsection 2.3 is maintained and in this way we complete
the model defined by the equations (7) and (8):

11

β (1)
β

(2) T

T

∼ N (m0 , C0 )

|φ, τ

∼ N (0, φ−1 Dτ )

τj |λ

∼ Exp(λ),

φ

∼ Ga(a0 , b0 )

λ

∼ Ga(g0 , h0 )

j = 1, . . . , K

(10)

The Bayesian inference procedure in this case must be carried out with caution since the vector of coefficients
contains the coefficients of the polynomial, which will not be penalized, and the coefficients of the basis functions to
which we assume a prior Lasso distribution for knots selection. The design matrix is then partitioned X = (X1 , X2 )
with X1 of dimension (n × p + 1) and X2 (n × K). Then, the i-th row of the matrix X is given by:
Xi = {1, xi , x2i , . . . , xpi , (xi − κ1 )p+ , . . . (xi − κK )p+ }.
|
{z
} |
{z
}
X1i

X2i

and Xβ = X1 β (1) + X2 β (2) .
In this context, we have a prior distribution of the vector θ = (β (1) , β (2) , φ, τ, λ) and the posterior distribution
is given by:
p(θ|y) ∝ p(y|X, β, φ) p(β (1) ) p(β (2) |φ, τ ) p(τ |λ) p(φ) p(λ).
Slight adaptations need to be made in the variational inference method and the main one refers to the fact that
we now have 4 partitions of the parametric vector giving rise to the following variational densities:
log(q(θ)) = log(q1 (β (2) , φ)) + log(q2 (τ )) + log(q3 (λ)) + log(q4 (β (1) ))
where log q4 (β (1) ) = log N (β (1) |mβ1 , Cβ1 ). The calculations for this and other variational densities are similar to
those developed for the regression model in subsection 3.3 and can be found in the appendix.

4.2

Algorithm for automatic knot selection

An alternative approach to determine the maximum number of knots K is to consider it as an unknown
parameter and estimate it. However, as in this work, K is not a parameter of direct interest since the selection of
knots is carried out through the Lasso scheme. Despite this, in order to have a good fit, it is important to properly
define the number of knots and their positions. In section 5 presents some exercises involving selecting knots and
shows the importance of the correct specification of the value of K. Thus, an algorithm will be proposed for the
automatic choice of the number of knots K that is based both on the VB algorithm for estimating the Lasso
model and on the criterion for selecting variables (knots).
The algorithm starts by proposing a grid of possible values of K the maximum number knots. Naturally,
the grid of values is an issue to be discussed. Note that it is not necessary to propose a grid that covers all
the natural numbers, since computational time can be excessively high. Moreover, given a maximum number of
knots, the most significant knots will be selected. For instance, start the grid with the maximum number of knots
K = 20. By using Lasso and the selection criteria, it is possible to have 6 among these 20 knots as the most
significant ones. On the other hand, simulations show that starting with a very large maximum number of knots

12

may cause problems in the selection, since the penalty criteria acts more severely when the number of knots is
extremely big for the size of the data set. See numerical simulations in Section 5. Therefore, our proposal is to
provide a grid that increases 10 units at a time, so that the knots are placed in the quantiles or evenly spaced in
the explanatory variable domain. This spacing allows us to position knots in different locations before and after
selecting significant knots. In the simulated exercises presented in Section 5, the grid starts with K = 10 knots.
In summary, ELBO is taken as a stopping criterion and the objective is to maximize it. The algorithm consists
of: for fixed grid values, start with the lowest value. The model is adjusted via VB together with the selection
criteria and then calculates the ELBO. Move to the next grid value and repeat the procedure. As long as the
ELBO increases with the grid values, the algorithm continues. The detailed algorithm is given below:
Algorithm 3 Maximum Number of Knots
Step 1. j ← 1. Initialize Kj = 10.
Step 2. Fit model via VB algorithm.
Step 3. Compute ELBO.
Step 4. Apply BF to select the most significant knots.
Step 5. Kj+1 = Kj + 10 and repeat steps 2, 3 and 4.
if ELBO(Kj+1 ) ≥ ELBO(Kj ) then
Set Kj ← Kj+1 . Repeat steps 2 to 5.
end if
if ELBO(Kj+1 ) < ELBO(Kj ) then
Stop and deliver ELBO and the most significant knots
end if

5

Simulations Studies
This section proposes 5 exercises with artificial data. The first three based on Lasso for linear regression

models and the last two focused on the use of Lasso to select knots in spline regression.
The inference procedure assumes, for all exercises, the following prior distributions: φ ∼ Ga(0.1, 0.1), λ ∼
Ga(0.1, 0.1) e β 1 ∼ N (1, 100) (in case of regression splines). For MCMC, 15,000 iterations were necessary to
achieve convergence. The first 5,000 iterations were discarded as the burn in process and one observation was
taken for every ten observations to remove autocorrelation, ending with a sample size of 10000. These quantities
were obtained by using the criterion found in Lewis and Raftery [1997], that provides the number of iterations
needed to guarantee the convergence in the Gibbs Sampler. The VB algorithm is repeated until the changes in
mβ , Cβ , bφ , dτ , fτj and hλ between two consecutive iterations are less than 0.01%. When applied, the classic
procedure was implemented using the R software glmnet package, which in turn applies 5-fold cross-validation to
estimate the penalty parameter λ.

5.1

Variable Selection for Linear Regression

The goal of these exercises applied to the linear regression models is twofold. Firstly to compare the estimation
methods VB and MCMC (eventually we also use the classic Lasso in the comparison). Secondly, we wish to
compare the CI, SN and BF selection criteria. Moreover, different sparsity scenarios, variations in the sample
size, different correlations between explanatory variables and different values for the accuracy of the model are
considered.
Specifically, exercise 1 aims to estimate Lasso hyperparameters via VB and MCMC. Only one data set is
simulated from which the real values of all parameters and hyperparameters of the model are known. VB presents

13

results similar to MCMC and computational time 14 times shorter. Exercise 2 is based on a simulation study with
100 replicates that presents a lesser sparsity structure. Variations in the sample size and in the correlation among
the explanatory variables are considered. Again, VB and MCMC present similar and superior results to the classic
Lasso. When the CI, SN and BF selection criteria are compared, BF gives the best results, with high proportions
of exclusion for coefficients that are zero and low exclusion proportions for coefficients that are different from
zero. Exercise 3 is designed for scenarios with 100 replicates and with greater sparsity when compared to exercise
2. This exercise takes into account cases where n < p and different values for the model’s precision. The results
are similar of those obtained in simulation 2
5.1.1

Exercise 1: MCMC vs. VB

The purpose of simulation 1 is to compare the MCMC and VB methods to curve fitting and computational
time. For this study we considered n = 100, p = 10 and each column of the matrix X was generated from a
distribution N (0, In ). For the parameters, were taken φ = 0.4, λ = 5 and τj |λ ∼ Exp(λ), ∀j. The regression
coefficients and observations were generated considering the Lasso regression model.
Table 1 shows us a posterior summary of the model parameters. There, one can see the mean and the standard
deviation of the approximate posterior obtained by using VB. Also, the posterior mean, the posterior standard
deviation via MCMC and the true value of the parameters. Note that the point estimates obtained by the VB are
close to those obtained by the MCMC. In addition, for both methods, the results are close to the real values with
small standard deviation. This same conclusion can be seen in Figure 3. In fact, Figure 3 exhibits a graphical
comparison between MCMC and VB. The histogram represents the sample of the posterior distribution obtained
via MCMC and the curve in red the approximate posterior density obtained by the VB. The green dot indicates
the true value of the parameters. Note that the curves approximated by the VB are close to the histograms and
both centered on the actual values. The remaining parameters τj show similar results.

14

Table 1: Posterior summary.
Parameters
β1
β2
β3
β4
β5
β6
β7
β8
β9
β10
φ
τ1
τ2
τ3
τ4
τ5
τ6
τ7
τ8
τ9
τ10
λ

Real
0.463
0.116
-1.251
0.250
-0.319
0.826
-0.036
0.144
0.064
-0.298
0.4
0.340
0.088
0.148
0.470
0.048
0.162
0.120
0.069
0.027
0.275
5

Mean VB
0.557
-0.046
-1.316
0.396
-0.078
0.844
0.091
0.074
-0.090
-0.370
0.473
0.233
0.137
0.401
0.201
0.140
0.296
0.142
0.139
0.140
0.194
4.745

Sd VB
0.132
0.136
0.153
0.161
0.137
0.140
0.142
0.136
0.131
0.150
0.066
0.188
0.159
0.231
0.179
0.160
0.205
0.161
0.160
0.161
0.177
1.493

Mean MCMC
0.558
-0.048
-1.315
0.383
-0.079
0.845
0.096
0.081
-0.081
-0.369
0.473
0.265
0.162
0.453
0.234
0.152
0.345
0.164
0.173
0.148
0.212
5.600

Sd MCMC
0.139
0.138
0.160
0.171
0.142
0.148
0.149
0.143
0.134
0.163
0.070
0.326
0.250
0.398
0.359
0.209
0.333
0.266
0.365
0.244
0.278
3.776

Since MCMC and VB present similar results, it is worth to point out the main difference between these
estimation methods, which is computational time. For exercise 1, the computational time of the VB was 0.72
seconds while that of the MCMC was 10.15 seconds. In the following exercises these computational times become
even more discrepant as we will be dealing with simulations with replicates.
5.1.2

Exercise 2: High correlation

In this exercise a simulation was developed based on 100 replicates p = 8, β = (3, 1.5, 0, 0, 2, 0, 0, 0)T and the
design matrix is generated from a multivariate normal distribution with zero mean, variance 1 and two different
correlation structures between xi e xj : 0 e 0.7|i−j| , ∀i e j. Let’s consider φ = 1/9 and 3 nested scenarios varying
the sample size with {nT , nV } = {20, 10}, {100, 50} e {200, 100}, where nT e nV denote the size of the training
set and the size of the validation set, respectively. Therefore, we have a total of 6 different scenarios. Note that
the explanatory variables are standardized to have mean 0 and variance 1
A Tabela 2 summarize the results of exercise 2.
Table 2: Simulation 2 with 100 replicates, p = 8 explanatory variables and the vector of coefficients β =
(3, 1.5, 0, 0, 2, 0, 0, 0)T .
Simulation
S2.1
S2.2
S2.3
S2.4
S2.5
S2.6

nT
20
100
200
20
100
200

nV
10
50
100
10
50
100

cov(Xi , Xj )
0
−
−
0.7|i−j|
−
−

The comparison of the MCMC and VB methods is our main objective in this simulation, however, frequentist
Lasso is also considered through the glmnet package of the R software. For the frequentist Lasso, a 5-fold cross-

15

0.6

2.5

3.0
−0.4

−0.2

0.0

0.2

0.4

2.0
0.5
0.0
−0.6

−0.4

−0.2

0.0

0.2

0.4

0.4

1.2

density

2.0

2.0
1.5

density

1.0

1.0

0.0

0.5

0.5
0.0
0.0

0.2

0.4

0.6

0.8

−0.4

−0.2

0.0

0.2

0.4

0.6

−0.4

−0.2

0.0

β7

0.2

0.4

3

4

0.6

β8

0.4

−0.6

−0.4

−0.2

0.0

4
0

1

β10

2

3

0

φ

0.6

0.7

5

1.5

0

10

20

30

40

0

0.0

1

0.5

1.0

density

density

2.0

4

2.5

0.20
0.15

density

0.10
0.05
0.00

0.5

2

τ2

6
5
4
3

0.4

1

τ1

2
1
0
0.3

3
0

1
−0.8

3

0.2

β9

2

0.0

0

0.0
−0.2

density

2
1

0.5

0.5
0.0

−0.4

2

density

1.5
1.0

1.5

density

2.0

3

2.0

2.5

5

4

2.5

3.0

6

β4

1.0

density

1.0

2.5

2.5
2.0
1.5

density

1.0
0.5
0.0

−1.0

β3

density

0.8

β6

2.5

2.5
2.0
1.5

density

1.0
0.5

−1.4

0.6

β5

3.0

β2

0.0
−1.8

1.5

density

1.0

1.5

density

0.8

β1

1.5

0.4

0.0

0.0

0.5

0.5

1.0

1.0

1.5

density

2.0

2.0

2.5

2.5

3.0

3.0
2.5
2.0
1.5

density

1.0
0.5
0.0

0.2

0

1

2

3

4

0.0

0.5

τ3

λ

1.0

1.5

2.0

2.5

τ4

Figure 3: Comparison MCMC (histogram) versus VB (solid line). The dot marks the actual value of the parameter
used to generate the data.
.
validation is used to select the parameter λ. In addition, different variable selection criteria will be compared as
described in 5: credible interval (CI), scaled neighborhood (SN) and Bayes factor (BF).
In order to compare Lasso’s predictive power from the different estimation techniques, MCMC, VB and
frequentist Lasso, the mean absolute error (MAE) was calculated for each replicate of the validation set using the
following expression:

16

M AE =

nV
1 X
|y P − yiV |
nV i=1 i

(11)

where yiP are the predicted values in the validation set, obtained from the fitted model after the selection of the
coefficients. yiV are the observed values in the validation set and nV is the size of the validation set. Note that
MCMC generates a sample of the predictive distribution from each iteration of the method. Then, yiP is obtained
as follows:
p(yiP |y) =

AM
1 X
p(yiP |θ (j) ),
AM j=1

where AM is MCMC number of iterations and θ is the vector of coefficients.
Figure 4 shows the box-plots of the mean absolute errors for each of the six proposed scenarios. As the sample
size increases, we observe a smaller difference between the three estimation methods. When the sample is small,
similar results are obtained between MCMC and VB. These have the median MAE and the lowest dispersion when
compared to the frequentist Lasso. Next, we will detail the performance of the selection criteria for each βj .

Lasso

MCMC

VB

4
0

2

MAE

4
MAE
2
0

0

2

MAE

4

6

S2.3

6

S2.2

6

S2.1

Lasso

VB

VB

VB

6
0

2

MAE

4

6
4
2
0
MCMC

MCMC

S2.6

MAE

4
MAE
2
0

Lasso

Lasso

S2.5

6

S2.4

MCMC

Lasso

MCMC

VB

Lasso

MCMC

VB

Figure 4: Mean absolute error (MAE) using (5.1.2) for the 6 scenarios of the simulation 2. Estimation methods:
MCMC, VB and frequentist Lasso.

17

The Table 3 shows the frequency of times that the predictor xj , j = 1, . . . , 8 was excluded in the 100
replicates, considering the three variables selection methods and all six scenarios built in Simulation 2. We present
the proportions only for the VB because so far its results are similar to those of the MCMC. Note that for this
simulation exercise the BF presents the best results in all scenarios, with a greater proportion of exclusion when
the actual values of βj are zero and a small proportion when the β ’s are different from zero. In addition, it is
noted that as the sample size increases, the three criteria tend to correctly choose coefficients that are zero and
the coefficients that are different from zero. From exercises 1 and 2, one may notice that the approximations of
the VB are as good as the results obtained by the MCMC. Nevertheless, the gain in computational time provided
by VB is far superior than MCMC. In addition, we saw that BF is a variable selection criterion that presents
superior results when compared with CI and SN. In the following subsection we show the performance of the VB
estimation method and the BF selection criterion for a more complex numerical experiment with greater sparsity.
Table 3: Comparison of the three methods on variable selection accuracy using VB for the six scenarios (the
frequency of exclusions for the predictor xj , j = 1, . . . , 8) with β = (3, 1.5, 0, 0, 2, 0, 0, 0)T .

5.1.3

Simulation

Method

β1

β2

β3

β4

β5

β6

β7

β8

S2.1

VB + CI
VB + SN
VB + BF

0.01
0.02
0.00

0.09
0.15
0.09

0.64
0.73
0.88

0.57
0.71
0.70

0.13
0.20
0.13

0.66
0.77
0.82

0.65
0.75
0.75

0.62
0.71
0.92

S2.2

VB + CI
VB + SN
VB + BF

0.00
0.00
0.00

0.00
0.00
0.00

0.47
0.70
0.73

0.51
0.62
0.78

0.00
0.00
0.00

0.57
0.71
0.73

0.67
0.78
0.84

0.56
0.72
0.72

S2.3

VB + CI
VB + SN
VB + BF

0.00
0.00
0.00

0.00
0.00
0.00

0.50
0.71
0.72

0.47
0.62
0.73

0.00
0.00
0.00

0.67
0.71
0.77

0.60
0.73
0.80

0.58
0.76
0.77

S2.4

VB + CI
VB + SN
VB + BF

0.02
0.06
0.02

0.09
0.11
0.09

0.53
0.70
0.65

0.53
0.62
0.80

0.19
0.24
0.18

0.71
0.75
0.85

0.60
0.73
0.81

0.64
0.74
0.88

S2.5

VB + CI
VB + SN
VB + BF

0.00
0.00
0.00

0.00
0.00
0.00

0.57
0.73
0.78

0.49
0.68
0.77

0.00
0.00
0.00

0.60
0.75
0.76

0.51
0.70
0.80

0.57
0.75
0.82

S2.6

VB + CI
VB + SN
VB + BF

0.00
0.00
0.00

0.00
0.00
0.00

0.55
0.70
0.77

0.47
0.66
0.75

0.00
0.00
0.00

0.60
0.77
0.79

0.46
0.64
0.72

0.57
0.75
0.77

Exercise 3: High sparsity with small n and large p

In this exercise we consider a situation with sparsity given by p = 40 e β = (0T , 3T , 0T , 3T )T , where 0 e 3
are vectors of dimension 10 and each of their entries are 0 and 3 respectively. The design matrix X is generated
from a multivariate normal distribution with mean zero, variance 1 and the correlation between the columns xi e
xj is equal to 0.5, ∀i 6= j. We analyze 4 different scenarios by varying the sample size and the precision parameter
φ. The simulated data were analyzed as follows, {nT , nV } = {20, 10} e {200, 100} where nT e nV are the size
of the training set and the size of the validation set respectively. In addition, we set the precision parameter as
φ = 1/9 and φ = 1/225. For each scenario we consider 100 replicates. Table 4 summarizes all the scenarios
considered in this simulation exercise 3. It is worth mentioning that in scenarios S3.1 and S3.3 we have n < p
Similarly to exercise 2, the MAE was calculated for each replicate as a predictive measure. Figure 5 shows the
box-plots of each scenario for MCMC, VB and Lasso. One may see that the MCMC and VB present similar and

18

Table 4: Scenarios in Simulation 3
Simulation
S3.1
S3.2
S3.3
S3.4

nT
20
200
20
200

nV
10
100
10
100

φ
1/9
1/225
-

superior results to the Lasso when the sample size is small. As the sample increases the results become similar in
the 3 approaches.

MAE
10
5

5

MAE
10

15

S3.2

15

S3.1

Lasso

MCMC

VB

Lasso

VB

MAE
20
15

15

MAE
20

25

S3.4

25

S3.3

MCMC

Lasso

MCMC

VB

Lasso

MCMC

VB

Figure 5: Mean absolute error (MAE) obtained by using (5.1.2) for the 4 scenarios in exercise 3, comparing the
estimation methods MCMC, VB and Lasso.
Figure 6 shows the proportions of exclusions (gray bars) and selections (black bars) for each of the 40
coefficients in the 100 replicates, when comparing the estimation methods, VB and Lasso. MCMC was omitted
for presenting results similar to VB. In the Bayesian context, the selection criterion used in all scenarios was the
BF. It is expected that the black bars will be larger when the true coefficients are different from zero and that the
gray bars will be large when the true coefficients are equal to zero. The proportions of the errors are represented

19

by the black bars when the coefficients are zero (type I error) and by the gray bars when the coefficients are
different from zero (type II error Thus, it can be seen for n < p, both VB and Lasso do not have a good selection
and exclusion performance, with a slight advantage of VB. On the other hand, as the sample increases, the VB
presents good results, better than those presented by Lasso. Also note that when n > p both VB and Lasso have
the same type II error. However, for all coefficients, the type I error is considerably less in VB than in MCMC

5.2

Knots selection

As the VB presents results similar to the MCMC, but with considerably less computational time, therefore, in
the two exercises applied to the spline regression models, only the VB is used. The goal is to define the maximum
number of knots from a grid of values and, in turn, to select the most significant knots and their positions.
Exercises 4 and 5 consist of a simulation study of the penalized spline regression model defined in equations (7),
(8) and (10). What differs in the two simulation studies consists of the number of bumps in the smooth function
f . In exercise 4, shows an example with 1 bump, while exercise 5 analyzes 2 bumps.
In both exercises we have 100 replicates with n = 100, variance equals to 0.3 ( φ = 1/0.3) and xi taking
equally spaced values in the interval [0, 1]. In addition, Cubic Splines (p = 3) are used, the interior knots of the
truncated power basis are positioned in the quantiles of the variable xi . The maximum number of knots varies in
the grid K = 10, 20, 30, 40 and 50.
The ELBO is used to indicate the maximum number of knots and for the both exercises we have an optimal
initial guess of K = 30 knots. The BF and CI selection criteria indicate that around 8 of these 30 initial knots
have a higher frequency of being selected as the most significant ones.
5.2.1

Exercise 4: Single structure/One bump

In exercise 4 we use the smooth function f called ”Bump” given by f (x) = x + 2 exp{−(16 ∗ (x − 0.5))2 }.
Figures 7, 8 and 9 show some results for K = 10, 30 and 50 knots, respectively. In the first line of graphics
there are the plots of the data generated for one of the replicates (dots), the true curve (solid line) and the
average of the fittings of the 100 replicates for the three selection criteria (dashed lines). The second line shows
the proportion of excluded knots, also for the three selection criteria CI, SN and BF.
Observe that the positions of the knots most selected as significant are in the rise and fall of the bump. In
addition, it should also be noted that as K increases, the three selection criteria tend to be more rigorous in the
penalty, hence, excluding more knots. This occurs more severely in the SN criterion, which presents an average
of fittings worse when K = 50. The results of the CI and BF criteria are similar in all cases. The results for
exercises where the maximum number of knots are 20 and 40 have been omitted as they are similar, for K = 10
and K = 50, respectively
Figure 10 exhibits the 100 fitted models for each of the replicates considering K = 10, 30 and 50, and the
BF selection criterion. One can see that the variability increases as the value of K increases. The same occurs
when considering the other selection criteria.
Figure 11 shows the frequency of the number of knots selected in the 100 replicates for each of the selection
criteria (CI, SN and BF) and maximum number of knots (K = 10, 30 and 50). Note that CI and BF criteria,
when K = 10, select between 5 and 6 knots as the most frequent ones. On the other hand, when K = 30 knots
7 knots among them are selected more frequently. In the case where the maximum number of knots is K = 50
we have a bimodal behavior for the frequency of the number of selected knots and one can see that the penalty

20

is more severe as K grows.
ELBO as a model comparison measure can be used to propose the maximum number of knots. It is worth
mentioning that the larger the ELBO the more that model is preferable. From the Table 5, which presents the
average ELBO for each value of K, we have that the model proposed with K = 30 knots which selects more
frequently 7 of these knots as significant, is preferable.
Table 5: Average ELBO
Criterion
FB
IC
SN

k = 10
-32.24
-32.24
-37.15

k = 20
-16.10
-16.37
-39.14

21

k = 30
-2.55
-2.55
-69.16

k = 40
-19.59
-20.91
-244.80

k = 50
-22.23
-46.14
-390.80

0.5
0
0.5
1

1

0.5

0

0.5

1

Lasso − S3.1

1

VB − S3.1

β

0−−−−−−−−03−−−−−−−−30−−−−−−−−03−−−−−−−−3

0−−−−−−−−03−−−−−−−−30−−−−−−−−03−−−−−−−−3

Regression Coefficients

VB − S3.2

Lasso − S3.2

0.5
0
0.5
1

1

0.5

0

0.5

1

Regression Coefficients

1

β

β

0−−−−−−−−03−−−−−−−−30−−−−−−−−03−−−−−−−−3

0−−−−−−−−03−−−−−−−−30−−−−−−−−03−−−−−−−−3

Regression Coefficients

VB − S3.3

Lasso − S3.3

0.5
0
0.5
1

1

0.5

0

0.5

1

Regression Coefficients

1

β

β

0−−−−−−−−03−−−−−−−−30−−−−−−−−03−−−−−−−−3

0−−−−−−−−03−−−−−−−−30−−−−−−−−03−−−−−−−−3

Regression Coefficients

VB − S3.4

Lasso − S3.4

0.5
0
0.5
1

1

0.5

0

0.5

1

Regression Coefficients

1

β

β

β

0−−−−−−−−03−−−−−−−−30−−−−−−−−03−−−−−−−−3

Regression Coefficients

0−−−−−−−−03−−−−−−−−30−−−−−−−−03−−−−−−−−3

Regression Coefficients

Figure 6: Proportion of selected (black) and excluded coefficients (gray) for the 4 scenarios in exercise 3 with the
estimation methods VB (left column ) and Lasso (right column).

22

0.0

0.4
x

0.8

1 2 3 4 5 6 7 8 910
knots

0.0

0.4
x

0.8

Excluded proportion
0.0
0.4
0.8

0.8

Excluded proportion
0.0
0.4
0.8

0.4
x

Excluded proportion
0.0
0.4
0.8

0.0

0

y
1

0

y
1

0

y
1

2

BF

2

SN

2

CI

1 2 3 4 5 6 7 8 910
knots

1 2 3 4 5 6 7 8 910
knots

Figure 7: Proportion of excluded knots and the average of the fittings (dashed line), K=10.

23

0.4
x

0.8

0.0

0.4
x

0.8

1 5 10 15 20 25 30
knots

0.0

0.4
x

0.8

Excluded proportion
0.0
0.4
0.8

Excluded proportion
0.0
0.4
0.8

Excluded proportion
0.0
0.4
0.8

0.0

0

y
1

0

y
1

0

y
1

2

BF

2

SN

2

CI

1 5 10 15 20 25 30
knots

1 5 10 15 20 25 30
knots

Figure 8: Proportion of excluded knots and the average of the fittings (dashed line), K=30.

24

0.0

0.4
x

0.8

1 10 20 30 40 50
knots

0.0

0.4
x

0.8

Excluded proportion
0.0
0.4
0.8

0.8

Excluded proportion
0.0
0.4
0.8

0.4
x

Excluded proportion
0.0
0.4
0.8

0.0

0

y
1

0

y
1

0

y
1

2

BF

2

SN

2

CI

1 10 20 30 40 50
knots

1 10 20 30 40 50
knots

Figure 9: Proportion of excluded knots and the average of the fittings (dashed line), K=50.

0.0

0.4
x

0.8

0

y
1

y
1
0

0

y
1

2

k = 50

2

k = 30

2

k = 10

0.0

0.4
x

0.8

0.0

0.4
x

0.8

Figure 10: Fittings for each of the 100 replicates, for different numbers of knots and according to the selection
criteria BF.

25

SN, k = 10

BF, k = 10

20
10
0

5

Frequency

30

CI, k = 10

3 4 5 6 7 8 9

3 4 5 6 7 8 9

3 4 5 6 7 8 9

number of knots

number of knots

CI, k = 30

SN, k = 30

BF, k = 30

20
10
0

5

Frequency

30

number of knots

4

6

8 10

0

2

4

6

8

4

6

8 10

number of knots

number of knots

CI, k = 50

SN, k = 50

BF, k = 50

20
10
0

5

Frequency

30

number of knots

0

5

10

15

number of knots

0

2

4

6

8

number of knots

0

5

10

15

number of knots

Figure 11: Frequency of the number of selected knots in the 100 replicates.

26

5.2.2

Exercise 5: Double structure/Two bumps

Similar to exercise 4, we proposed exercise 5, however, now considering a curve with 2 bumps. The curve f
is a mixing two normal distributions:
f (x) = 0.3N (x|0.4, 0.01) + 0.7N (x|0.8, 0.01),
where N (x|a, b) is a normal distribution with mean a and variance b. In this exercise we assume n = 300 and the
other conditions of simulation 4 were kept. That is, 100 replicates, φ = 1/0.3, xi equally space in [0, 1], p = 3,
knots placed at the xi quantiles and maximum number of knots K = 10, 20, 30, 40, 50. We omitted the graphics
for K = 20 and K = 40, as they are similar.
The results obtained for the case of 1 bump are similar to those obtained here. Once again, it is possible to
see in the Figures 12, 13 and 14 the average of the fittings of the 100 replicates for the three selection criteria in
the dashed lines (first row of plots) and the proportion of excluded knots (second row of plots) for different values
of K. In the three figures one may notice that the knots that are most selected as significant are positioned in the
ups and downs of the bumps. For the case of 2 bumps, the higher the value of K, the more severe the exclusion
of knots is. The SN criterion does not show good results as the maximum number of knots increases and the
performance of the CI and BF criteria are similar in all cases.

2
0.0

0.4
x

0.8

1 2 3 4 5 6 7 8 910
knots

0.0

0.4
x

0.8

Excluded proportion
0.0
0.4
0.8

0.8

Excluded proportion
0.0
0.4
0.8

0.4
x

Excluded proportion
0.0
0.4
0.8

0.0

−1

0

y
1

2
−1

0

y
1

−1

0

y
1

2

3

BF

3

SN

3

CI

1 2 3 4 5 6 7 8 910
knots

1 2 3 4 5 6 7 8 910
knots

Figure 12: Proportion of excluded knots and the average of the fittings - K=10.
In Figure 15 shows the fittings of 100 replicates according to the BF criterion for different numbers of knots.

27

2
0.0

0.4
x

0.8

1 5 10 15 20 25 30
knots

0.0

0.4
x

0.8

Excluded proportion
0.0
0.4
0.8

0.8

Excluded proportion
0.0
0.4
0.8

0.4
x

Excluded proportion
0.0
0.4
0.8

0.0

−1

0

y
1

2
−1

0

y
1

−1

0

y
1

2

3

BF

3

SN

3

CI

1 5 10 15 20 25 30
knots

1 5 10 15 20 25 30
knots

Figure 13: Proportion of excluded knots and the average of the fittings - K=30.
It can be seen that there is less variability than in the case of 1 bump, possibly due to the increase in the sample
size to n = 300 . Nonetheless, it is possible to see that the when the maximum number of knots increases , the
variability between the fitted models also increases.
Our analysis shows that the SN criterion does not provide a good fit for K = 50 and in Figure 16 we see that
this criterion tends to underestimate the number of significant knots as K increases. We will analyze in more
detail the frequency of the number of knots selected in the 100 replicates using the CI and BF criteria. Note that
both criteria present similar results. In Figure 16 we observed that when the maximum number of knots is 10,
both the CI and BF criteria indicate more frequently that 7 out of 10 knots are significant. When K = 30 or
K = 50, the criteria CI and BF most frequently indicate 8 knots as significant.
In order to define the maximum number of knot, the average ELBO was calculated for each value of K in a
fixed grid. For the FB criterion we have that the average ELBO is -74.36 when K = 10. This value increases
(-68.67 when K = 20) until it reaches the maximum value of -67.15 when K = 30. The average ELBO value
for K = 40 is -70.44. Thus, for BF criterion K = 30 is the initial guess for the maximum number of knots.The
same can be seen in the CI criterion. This result coincides with that obtained in exercise 4.

28

2
0.0

0.4
x

0.8

1 10 20 30 40 50
knots

0.0

0.4
x

0.8

Excluded proportion
0.0
0.4
0.8

0.8

Excluded proportion
0.0
0.4
0.8

0.4
x

Excluded proportion
0.0
0.4
0.8

0.0

−1

0

y
1

2
−1

0

y
1

−1

0

y
1

2

3

BF

3

SN

3

CI

1 10 20 30 40 50
knots

1 10 20 30 40 50
knots

Figure 14: Proportion of excluded knots and the average of the fitted models - K=50 nós.

0.0

0.4
x

0.8

−1

0

y
1 2

y
1 2
0
−1

−1

0

y
1 2

3

k = 50

3

k = 30

3

k = 10

0.0

0.4
x

0.8

0.0

Figure 15: Fit of 100 replicates for different values of knots.

29

0.4
x

0.8

SN, k = 10

BF, k = 10

30
20
10
0

Frequency

40

CI, k = 10

5

6

7

8

9

4

5

6

7

8

9

5

6

7

8

9

number of knots

number of knots

CI, k = 30

SN, k = 30

BF, k = 30

20
10
0

5

Frequency

30

number of knots

6

8

10

12

14

2

4

6

8

6

8

10

12

14

number of knots

number of knots

CI, k = 50

SN, k = 50

BF, k = 50

20
10
5
0

Frequency

30

number of knots

4

6

8

10

number of knots

0

1

2

3

4

5

number of knots

5 6 7 8 9

Figure 16: Frequency of selected knots in 100 replicates.

30

11

number of knots

6

Applications to real data
In this section, two applications with real data will be analyzed. The first is considered more usual in the

literature in the area and the second addresses a current issue related to the world pandemic of Covid-19. In
both cases, the Penalized Spline Regression model with polynomials of degrees 2 and 3 is fiited to the data. In
addition, the maximum number varies between 10, 20 and 30. It is noteworthy that in the first example the knots
are equally spaced positioned. Lasso, through variational inference (VB), is used in both applications together
with the Bayes Factor (BF) criterion for the selection of the most significant knots. ELBO will be the measure
considered for comparing models

6.1

Age and Income data

The first data set considers the income and age of 205 Canadians (Ullah and Zinde-Walsh [1985]. These data
have been widely used in applications of non-parametric regression models. See for example Ruppert [2002]. A
logarithmic transformation was applied to income, as can be seen in the data represented by black dots in the
two plot in Figure 17. Table 6 shows the ELBO measure computed for each fitted model by varying the degree
of the polynomial (p) and the maximum number of knots (k). For both p = 2 and p = 3, ELBO achieves its
maximum value at K = 10. Comparing these two quantities, the largest ELBO occurs for p = 3 and K = 10.
The graph on the left of Figure 17 shows the fitting of the penalized spline regression model (solid black line) and
its credible interval of 95% (gray shaded area). Note that the credible interval covers a large part of the observed
points. At the bottom of the graph we have the symbol ”x” representing the 10 knots positioned and in black
the only knot considered significant among the 10, according to the FB criterion. At the level of comparison, the
graph on the right of Figure 17 shows the fitted model obtained with the R function ”smooth.spline” (solid line
in red) and the fitting of the proposed model (black solid line). In this application, the results of these two fitted
models are similar.
Table 6: ELBO - Age and income data
p=2
p=3

6.1.1

k = 10
-221.49
-220.77

k = 20
-225.45
-227.03

k = 30
-237.22
-223.93

Covid-19 data

In order to observe the trend of the daily cases of Covid-19 in the USA and Brazil, the penalized spline
regression model was fitted to the data (logarithmic scale). United States data ranges from March 1, 2020 to
November 30, 2020, while data from Brazil ranges from March 10, 2020 to November 30, 2020.
Figure 18 shows the number of daily cases of Covid-19 in the USA (left) and Brazil (right) on the original
scale. The black dots in the Figures 19 and 20 show the same data set in logarithmic scale.
In this example, analyzing real data, we consider models with splines of degrees 2 and 3. The maximum
number of knots varies every 10 knots. To find the maximum number of knots, we use the ELBO measure by
starting the grid with K = 10 knots. After obtaining the optimal K value BF criterion is applied to select which
knots are the most significant and their positions.
The inference procedure was performed from the Bayesian point of view through variational inference. The
prior distribution remains the same as for studies with artificial data.
31

15
11

12

log income
13

14

15
14
log income
13
12
11
20

30

40

50

60

20

30

40

age

50

60

age

Cases Covid−19 US
0 50000
150000

Cases Covid−19 Brazil
0 20000
50000

Figure 17: Left: The fit of the spline regression model (black solid line) with p = 3 and one significant knot among
K = 10 knots and the log-income and age data (black dots) with the credible interval of 95% (shaded area).
Right: The fit of the proposed model (solid black line) and a fitted model using the R function ”smooth.spline”
(solid red line) to the log-income and age data (dots).

0

50

100

150

200

250

0

time

50

100

150

200

250

time

Figure 18: Daily cases of Covid-19 in US (left) and in Brazil (right).

Table 7 exhibits the results of ELBO for different values of p and K, for data from the USA and Brazil.
Marked in bold are the cases in which ELBO achieves the highest values for p = 2 and p = 3. In the North
American case, ELBO is maximum when p = 3 and K = 20. It is worth mentioning that among the 20 knots,
9 were significant according to the BF criterion. Considering the Brazil data, one can see that the largest ELBO
occurs when p = 2 and K = 10, and only 6 of these 10 knots are significant, according to the BF. The following
results are presented only for models with the largest ELBO.

32

Table 7: ELBO - US and Brazil Covid-19 data.

p=2
p=3

k = 10
57.25
18.89

US
k = 20
108.96
116.10

k = 30
95.25
24.53

k = 10
-145.32
-181.52

Brazil
k = 20
-168.23
-179.88

k = 30
-164.55
-197.66

The plot to the left of the Figure 19 shows the fit (solid green line) of the penalized spline regression model
with 3 polynomial of degree 3 and 9 significant knots(out of a total of 20 knots) for the US Covid-19 data. The
shaded area represents the 95% credible interval and it contains most of the observed data (black dots). Note
that significant knots (black asterisks) are located such that they cover the bumps of the curve. The ”x” in red
are the excluded knots that were not considered in the fit. The plot on the right compares the fits of the proposed
model (solid green line) with the R function ”smooth .spline” (red solid line). Note that the second fit captures,

12
log cases Covid−19 US
4
6
8
10
2

2

log cases Covid−19 US
4
6
8
10

12

in addition to the signal, the noise contained in the data.

0

50

100

150

200

250

0

time

50

100

150

200

250

time

Figure 19: Left: The fit of the penalized spline regression model (solid green line) with p = 3 and 9 significant
knots (black asterisks) out of K = 20 knots (x red) with the 95% credible interval (shaded area) and the fit for
the logarithm of the number of daily cases of Covid-19 in the USA (black dots). Right: The fit of the proposed
model (solid green line) and the fit of a model using the R function ”smooth.spline” (solid red line) to the log
data of the number of daily cases of Covid-19 in the USA (dots).

Figure 20 shows the fit (solid green line) of the proposed model with p = 2 and 6 significant knots (black
asterisks at the bottom of the graph). The excluded knots are represented by ”x” in red. The 95% credible
interval (gray shadow) covers much of the observed data points. The plot on the right makes a comparison
between the fit of the proposed model and the R function ”smooth.spline” (red solid line). As in the case of the
U.S. data, we observed that the fit by using ”smooth.spline” does not smooth the data as the proposed model
and follows the series random noise more closely.

33

log cases Covid−19 Brazil
4
6
8
10
2

log cases Covid−19 Brazil
4
6
8
10
2
0

50

100

150

200

250

0

time

50

100

150

200

250

time

Figure 20: Left: the fit of the penalized regression spline model (solid green line) with p = 2 and 6 significant
knots (black asterisks) out of K = 10 knots (x in red) with the 95% credible interval (shaded area) and the fit
of the logarithm of the number of daily cases of Covid-19 in Brazil (black dots). Right: The curve fitting of the
proposed model (solid green line) and the fit using R function ”smooth.spline” (solid red line) to the logarithm
of the number of daily cases of Covid-19 in Brazil (dot).

7

Conclusions
This article proposes a new scalable procedure for selecting the number of knots in regression splines: A

fully automatic Bayesian Lasso through variational inference. Simulation studies have shown effectiveness of this
procedure in modeling different types of data sets. In addition, the numerical exercises show that this approach
is much faster than the traditional one that is based on MCMC type algorithms. In real data sets the procedure
was able to capture the trend existing in them. Thus providing a better understanding of the data dynamic.
Acknowledgments.

This paper was partially supported by Fapesp Grants (RD) 2018/04654, (RD and HSM)

2019/10800-0, (RD) 2019/00787-7.

34

Appendix 1: The variational distributions for Lasso
The variational posterior for β and φ while holding q2 (τ |λ) and q3 (λ) fixed, is given by

log q1∗ (β, φ)

=

log(p(y|β, φ)) + Eτ [log(p(β, φ|τ ))] + const



1
−n/2 −1
−1/2
T
= log (2π)
|φ In |
exp − (y − Xβ) (φIn )(y − Xβ) +
2
 


1 T
−p/2 −1
−1/2
−1
+Eτ log (2π)
|φ Dτ |
exp − β φDτ β
+
2
+Eτ {log[φa0 −1 exp{−φb0 }]} + const
n p

φ
T
T
T
+ + a0 − 1 log φ − {β T [Eτ (D−1
τ ) + X X]β + y y − 2y Xβ + 2b0 } + const
2
2
2
log N (β|mβ , φ−1 Cβ ) × Ga(φ|aφ , bφ )

=
=

It is easy to see that this is a normal-gamma distribution with parameters:

T
Cβ−1 = Eτ (D−1
τ ) + X X,

and

aφ = a0 + n/2,

and

mβ = Cβ X T y,
1
bφ = b0 + (yT y − mTβ Cβ−1 mβ ).
2

The variational distribution of τ while holding q3 (λ) fixed is given by

log q2∗ (τj )

= Eλ [log(p(τj |λ))] + Eβ,φ [log(p(βj , φ|τj ))] + const
 


φ 2
−1
−1/2
= Eλ {log[exp{−λτj }]} + Eβ,φ log (φ τj )
exp −
β
+ const
2τj j


1
1
1
= − log τj −
2Eλ [λ]τj + Eβ,φ [φβj2 ] + const
2
2
τj
= log GIG(τj |cτ , dτ , fτj )

with GIG being generalized inverse Gaussian distribution, where
cτ =

1
; dτ = 2Eλ [λ] ; fτj = Eβ,φ [φβj2 ].
2

Therefore,
log q2∗ (τ )

= log

p
Y

GIG(τj |cτ , dτ , fτj ).

j=1

The variational distribution of λ is:

35

log q3∗ (λ)

=
=

log(p(λ)) + Eτ [log(p(τ |λ))] + const
 

p
Y
log[λg0 −1 exp{−h0 λ}] + Eτ log 
λ exp{−τj λ} + const
j=1

=

(g0 + p − 1) log λ − λ[h0 +

p
X

Eτ (τj )] + const

j=1

=

log Ga(λ|gλ , hλ )

which is a gamma distribution with parameters
gλ = g0 + p ; hλ = h0 +

p
X

Eτ (τj ).

j=1

The expected values can be computed as follows:
It is worth pointing out that if X ∼ GIG(p, a, b), then its density is
p

( ab ) 2
√
xp−1 exp{−(ax + b/x)/2}, x > 0,
f (x|p, a, b) =
2κp ( ab)
where κp (·) is a modified Bessel function of the second kind, with

√
√
 
b  κp+2 ( ab)
b κp+1 ( ab)
√
√
and V ar[X] =
−
a κp ( ab)
a
κp ( ab)
√
r
a κp+1 ( ab) 2p
√
−
b κp ( ab)
b
r

E[X]

=

E[X −1 ]

=

√ !2 
κp+1 ( ab) 
√
κp ( ab)

Therefore,
−1
−1
Eτ [D−1
τ ] = diag(Eτ (τ1 ), . . . , Eτ (τp )),

√

Eτ (τj−1 )

p
dτ κcτ +1 ( dτ fτj ) 2cτ
p
= p
−
,
fτj
fτj κcτ ( dτ fτj )
p

Eτ (τj ) =

p
fτj κcτ +1 ( dτ fτj )
p
√
,
dτ κcτ ( dτ fτj )

Eλ (λ) =

gλ
.
hλ

For the calculus of Eβ,φ [φβj2 ], let x|y ∼ N (µx , y −1 σx ) and y ∼ Ga(a, b), so

E[X 2 Y ] = E[E(X 2 Y |Y )] = E[Y E(X 2 |Y )] = E[Y (E 2 (X|Y ) + V ar(X|Y ))] = µ2x E(Y ) + σx =
Thus,
36

aµ2x
+ σx .
b

Eβ,φ [φβj2 ] = m2βj aφ /bφ + (Cβ )jj .

37

Appendix 2: The variational distributions for Regression Spline
The variational posterior for β (2) and φ while holding q2 (τ |λ), q3 (λ) and q4 (β (1) ) fixed, is given by

log q1∗ (β (2) , φ)

= Eβ (1) [log(p(y|β, φ))] + Eτ [log(p(β (2) , φ|τ ))] + const
n h
= Eβ (1) log (2π)−n/2 |φ−1 In |−1/2 ×


1
(1)
(2) T
(1)
(2)
× exp − (y − X1 β − X2 β ) (φIn )(y − X1 β − X2 β )
+
2
 


T
1
(2)
+Eτ log (2π)−K/2 |φ−1 Dτ |−1/2 exp − β (2) φD−1
+
τ β
2
+ log[φa0 −1 exp{−φb0 }] + const


n K
=
+
+ a0 − 1 log φ +
2
2
o
φ n (2) T
(1)
(2)
(2) T
(1)
T
T
T
[X
[Eτ (D−1
)
+
X
X
]β
−
2β
y
−
X
X
E
β
(β
)]
+
−
2
1
τ
2
2
2
β
2

T
T
1
−φ b0 + [yT y − 2Eβ (1) (β (1) )X1T y + Eβ (1) (β (1) X1T X1 β (1) )] + const
2
=

log N (β (2) |mβ (2) , φ−1 Cβ (2) ) × Ga(φ|aφ , bφ )

It is easy to see that this is a normal-gamma distribution with parameters:

−1
T
Cβ−1
(2) = Eτ (Dτ ) + X2 X2 ,

(1)

mβ (2) = Cβ (2) [X2T y − X2T X1 Eβ (β (1) )],

and

aφ = a0 + n/2

bφ =

and

T
T
1
b0 + [yT y − 2Eβ (1) (β (1) )X1T y + Eβ (1) (β (1) X1T X1 β (1) )] − mTβ(2) Cβ−1
(2) mβ (2)
2


.

The variational distribution of τ while holding q3 (λ) fixed is given by

log q2∗ (τj )

= Eλ [log(p(τj |λ))] + Eβ,φ [log(p(βj , φ|τj ))] + const

 

φ 2
= Eλ {log[exp{−λτj }]} + Eβ,φ log (φ−1 τj )−1/2 exp −
βj
+ const
2τj


1
1
1
2Eλ [λ]τj + Eβ,φ [φβj2 ] + const
= − log τj −
2
2
τj
= log GIG(τj |cτ , dτ , fτj )

with GIG being generalized inverse Gaussian distribution, where
cτ =

1
; dτ = 2Eλ [λ] ; fτj = Eβ,φ [φβj2 ].
2

Therefore,
log q2∗ (τ ) = log

p
Y

GIG(τj |cτ , dτ , fτj ).

j=1

38

The variational distribution of λ is:

log q3∗ (λ)

=
=

log(p(λ)) + Eτ [log(p(τ |λ))] + const
 

p
Y
log[λg0 −1 exp{−h0 λ}] + Eτ log 
λ exp{−τj λ} + const
j=1

=

(g0 + p − 1) log λ − λ[h0 +

p
X

Eτ (τj )] + const

j=1

=

log Ga(λ|gλ , hλ )

which is a gamma distribution with parameters
gλ = g0 + p ; hλ = h0 +

p
X

Eτ (τj ).

j=1

The expected values can be computed as follow:
It is worth pointing out that if X ∼ GIG(p, a, b), then its density is
p

f (x|p, a, b) =

( ab ) 2
√
xp−1 exp{−(ax + b/x)/2}, x > 0,
2κp ( ab)

where κp (·) is a modified Bessel function of the second kind, with

√
√
 
b κp+1 ( ab)
b  κp+2 ( ab)
√
√
and V ar[X] =
−
a κp ( ab)
a
κp ( ab)
√
r
a κp+1 ( ab) 2p
√
−
b κp ( ab)
b
r

E[X]

=

E[X −1 ]

=

√ !2 
κp+1 ( ab) 
√
κp ( ab)

Therefore,
−1
−1
Eτ [D−1
τ ] = diag(Eτ (τ1 ), . . . , Eτ (τp )),

√

Eτ (τj−1 )

p
dτ κcτ +1 ( dτ fτj ) 2cτ
p
= p
−
,
fτj
fτj κcτ ( dτ fτj )
p

Eτ (τj ) =

p
fτ κc +1 ( dτ fτj )
√j τ p
,
dτ κcτ ( dτ fτj )

Eλ (λ) =

gλ
.
hλ

For the calculus of Eβ,φ [φβj2 ], let x|y ∼ N (µx , y −1 σx ) and y ∼ Ga(a, b), so

E[X 2 Y ] = E[E(X 2 Y |Y )] = E[Y E(X 2 |Y )] = E[Y (E 2 (X|Y ) + V ar(X|Y ))] = µ2x E(Y ) + σx =

39

aµ2x
+ σx .
b

Thus,
Eβ,φ [φβj2 ] = m2βj aφ /bφ + (Cβ )jj .

40

References
D. F. Andrews. A robust method for multiple linear regression. Technometrics, 16(4):523–531, 1974. doi:
10.1080/00401706.1974.10489233.

URL https://www.tandfonline.com/doi/abs/10.1080/00401706.

1974.10489233.
S. M. Berry, R. J. Carroll, and D. Ruppert. Bayesian smoothing and regression splines for measurement error
problems. Journal of the American Statistical Association, 97(457):160–169, 2002. ISSN 01621459. URL
http://www.jstor.org/stable/3085771.
C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: a review for statisticians. J. Amer.
Statist. Assoc., 112(518):859–877, 2017. ISSN 0162-1459. doi: 10.1080/01621459.2017.1285773. URL
https://doi.org/10.1080/01621459.2017.1285773.
G. Casella and E. I. George. Explaining the gibbs sampler. The American Statistician, 46(3):167–174, 1992. ISSN
00031305. URL http://www.jstor.org/stable/2685208.
D. G. T. Denison, B. K. Mallick, and A. F. M. Smith. Automatic bayesian curve fitting. Journal of the Royal
Statistical Society B, 60:363–377, 1998.
R. Dias. Density estimation via hybrid splines. J. Statist. Comput. Simul., 60(4):277–294, 1998.
R. Dias. Sequential adaptive non parametric regression via H-splines. Communications in Statistics: Computations
and Simulations, 28:501–515, 1999.
R. Dias and D. Gamerman. A Bayesian approach to hybrid splines nonparametric regression. Journal of Statistical
Computation and Simulation., 72(4):285–297, 2002.
R. Dias and N. L. Garcia. Consitent estimator for basis selection based on a proxy of the kullbakc-leibler distance.
Journal of Econometrics, 141(1):167–178, 2007.
J. Drugowitsch. Vblinlogit: Variational bayesian linear and logistic regression. J. Open Source Softw., 4(38):1359,
2019. doi: 10.21105/joss.01359. URL https://doi.org/10.21105/joss.01359.
P. H. C. Eilers and B. D. Marx. Flexible smoothing with B-splines and penalties. Statist. Sci., 11(2):89–121,
1996. ISSN 0883-4237. With comments and a rejoinder by the authors.
T. C. O. Fonseca, H. S. Migon, and H. Mirandola. Reference bayesian analysis for hierarchical models, 2019.
D. Gamerman and H. S. Migon. Dynamic hierarchical models. Journal of the Royal Statistical Society. Series B
(Methodological), 55(3):629–642, 1993. ISSN 00359246. URL http://www.jstor.org/stable/2345875.
A. E. Gelfand and A. F. M. Smith. Sampling-based approaches to calculating marginal densities. Journal of the
American Statistical Association, 85(410):398–409, 1990. ISSN 01621459. URL http://www.jstor.org/
stable/2289776.
V. Goepp, O. Bouaziz, and G. Nuel. Spline Regression with Automatic Knot Selection. working paper or preprint,
Aug. 2018. URL https://hal.archives-ouvertes.fr/hal-01853459.

41

M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for
graphical models. Machine Learning, 37(2):183–233, 1999. doi: 10.1023/A:1007665907178. URL https:
//doi.org/10.1023/A:1007665907178.
B. Jørgensen. Statistical properties of the generalized inverse Gaussian distribution, volume 9 of Lecture Notes
in Statistics. Springer-Verlag, New York-Berlin, 1982. ISBN 0-387-90665-7.
C. Kooperberg and C. J. Stone. A study of logspline density estimation. Comp. Stat. Data Anal., 12:327–347,
1991.
S. Lang and A. Brezger. Bayesian p-splines. Journal of Computational and Graphical Statistics, 13(1):183–212,
2004. doi: 10.1198/1061860043010. URL https://doi.org/10.1198/1061860043010.
S. M. Lewis and A. E. Raftery. Estimating Bayes factors via posterior simulation with the Laplace-Metropolis
estimator. J. Amer. Statist. Assoc., 92(438):648–655, 1997. ISSN 0162-1459. doi: 10.2307/2965712. URL
https://doi.org/10.2307/2965712.
Q. Li and N. Lin. The Bayesian elastic net. Bayesian Anal., 5(1):151–170, 2010. ISSN 1936-0975. doi:
10.1214/10-BA506. URL https://doi.org/10.1214/10-BA506.
D. V. Lindley and A. F. M. Smith. Bayes estimates for the linear model. J. Roy. Statist. Soc. Ser. B, 34:1–41,
1972. ISSN 0035-9246. URL http://links.jstor.org/sici?sici=0035-9246(1972)34:1<1:BEFTLM>2.
0.CO;2-#&origin=MSN.
H. Mallick and N. Yi. A new Bayesian lasso. Stat. Interface, 7(4):571–582, 2014. ISSN 1938-7989. doi:
10.4310/SII.2014.v7.n4.a12. URL https://doi.org/10.4310/SII.2014.v7.n4.a12.
H. S. Migon, D. Gamerman, and F. Louzada. Statistical inference. Chapman & Hall/CRC Texts in Statistical
Science Series. CRC Press, Boca Raton, FL, second edition, 2015. ISBN 978-1-4398-7880-4. An integrated
approach.
E. Montoya, N. Ulloa, and V. Miller. A simulation study comparing knot selection methods with equally spaced
knots in a penalized regression spline. International Journal of Statistics and Probability, 3(3):96–110, 2014.
J. T. Ormerod and M. P. Wand. Explaining variational approximations. The American Statistician, 64(2):140–153,
2010. doi: 10.1198/tast.2010.09058. URL https://doi.org/10.1198/tast.2010.09058.
M. R. Osborne, B. Presnell, and B. A. Turlach. Knot selection for regression splines via the lasso. Computing
Science and Statistics, pages 44–49, 1998.
T. Park and G. Casella.
681–686, June 2008.

The Bayesian Lasso.

Journal of the American Statistical Association, 103(482):

ISSN 0162-1459, 1537-274X.

doi: 10.1198/016214508000000337.

URL http:

//www.tandfonline.com/doi/abs/10.1198/016214508000000337.
D. Ruppert. Selecting the number of knots for penalized splines. Journal of Computational and Graphical Statistics,
11(4):735–757, 2002. doi: 10.1198/106186002853. URL https://doi.org/10.1198/106186002853.
S. Spiriti, R. Eubank, P. W. Smith, and D. Young. Knot selection for least-squares and penalized splines. Journal
of Statistical Computation and Simulation, 83(6):1020–1036, 2013. doi: 10.1080/00949655.2011.647317. URL
https://doi.org/10.1080/00949655.2011.647317.
42

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series
B (Methodological), 58(1):267–288, 1996. doi: https://doi.org/10.1111/j.2517-6161.1996.tb02080.x. URL
https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x.
A. Ullah and V. Zinde-Walsh. Estimation and testing in a regression model with spherically symmetric errors.
Econom. Lett., 17(1-2):127–132, 1985. ISSN 0165-1765. doi: 10.1016/0165-1765(85)90142-9. URL https:
//doi.org/10.1016/0165-1765(85)90142-9.
M. West. On scale mixtures of normal distributions. Biometrika, 74:646–648, 1987. URL http://www.stat.
duke.edu/mw/MWextrapubs/West1987a.pdf.

43

