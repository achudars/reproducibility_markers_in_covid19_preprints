COVID-19 and Computer Audition: An Overview on What
Speech & Sound Analysis Could Contribute in the SARS-CoV-2 Corona Crisis
Björn W. Schuller1,2,3, Dagmar M. Schuller3, Kun Qian4 , Juan Liu5 , Huaiyuan Zheng6 , Xiao Li7
1

arXiv:2003.11117v1 [cs.SD] 24 Mar 2020

2

GLAM – Group on Language, Audio & Music, Imperial College London, UK
EIHW – Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany
3
audEERING GmbH, Gilching, Germany
4
Educational Physiology Laboratory, The University of Tokyo, Tokyo 113-0033, Japan
5
Department of Plastic Surgery, The Central Hospital of Wuhan, Tongji Medical College,
Huazhong University of Science and Technology, Wuhan 430014, P. R. China
6
Department of Hand Surgery, Wuhan Union Hospital, Tongji Medical College,
Huazhong University of Science and Technology, Wuhan 430022, P. R. China
7
Department of Neurology, Children’s Hospital of Chongqing Medical University,
Chongqing Medical University, Chongqing 400014, P. R. China
schuller@IEEE.org

Abstract
At the time of writing, the world population is suffering from
more than 10 000 registered COVID-19 disease epidemic induced deaths since the outbreak of the Corona virus more than
three months ago now officially known as SARS-CoV-2. Since,
tremendous efforts have been made worldwide to counter-steer
and control the epidemic by now labelled as pandemic. In this
contribution, we provide an overview on the potential for computer audition (CA), i. e., the usage of speech and sound analysis by artificial intelligence to help in this scenario. We first survey which types of related or contextually significant phenomena can be automatically assessed from speech or sound. These
include the automatic recognition and monitoring of breathing, dry and wet coughing or sneezing sounds, speech under
cold, eating behaviour, sleepiness, or pain to name but a few.
Then, we consider potential use-cases for exploitation. These
include risk assessment and diagnosis based on symptom histograms and their development over time, as well as monitoring
of spread, social distancing and its effects, treatment and recovery, and patient wellbeing. We quickly guide further through
challenges that need to be faced for real-life usage. We come
to the conclusion that CA appears ready for implementation of
(pre-)diagnosis and monitoring tools, and more generally provides rich and significant, yet so far untapped potential in the
fight against COVID-19 spread.
Index Terms: Corona virus, SARS-CoV-2, COVID-19, Computer Audition, Machine Listening, Computational Paralinguistics, Coughing, Sneezing, Speech Under Cold, Snoring, Breathing, Speech under Mask, Elderly Speech

1. Introduction
The World Health Organisation’s (WHO) office in China was
first made aware of the previously unknown SARS-CoV-2
‘Corona’ virus on the last day(s) of the last year, i. e., roughly
three and a half month ago. On 11 March 2020, the WHO declared the disease triggered by the virus – COVID-19 – as pandemic. The spread of the disease induced by the SARS-CoV-2
or ‘Corona’ virus is assumed to underlie an exponential growth.
Johns Hopkins University’s Center for Systems Science and Engineering (CSSE) gathers via several sources the reported offi-

cial Corona virus COVID-19 Global Cases1 which at the time
of writing were assumed to account for 245 484 with 10 031
deaths, and 86 035 reported as recovered. However, whether
there are long-term effects after recovery is yet to be researched.
In the light of this dramatic spread, one is currently internationally witnessing drastic counter-measures that have not been
seen in this form over decades in many countries. These include significant public ‘shut-down’ measures to foster ‘social
distancing’ in order to slow-down and control further spread.
As research globally is making massive efforts to contribute
to better understand and fight the phenomenon from a medical
and interdisciplinary point of view, also computer science and
engineering in terms of ‘Digital Health’ solutions aim at maximum exploitation of available and realisable means. In particular, in combination with Artificial Intelligence (AI), one can
exploit a powerful tool, which so far has largely been tapped for
prediction of COVID-19 spread (cf. e. g., [1]) and Computer
Vision (CV) approaches in the Corona context such as for automatic screening for COVID-19 on CT images [2, 3]. There is,
however, broader belief that also other signals including such
from sensors on a smartphone could help even in the diagnosis
of COVID-19 [4].
In the following, we aim to provide an overview on what
Computer Audition (CA), i. e., the application of computing
for audio processing including ‘Machine Listening’, ‘Computational Paralinguistics’, and more general speech and sound
analysis, but also synthesis, could contribute in this situation.
To the best of the authors’ knowledge, this resource is so far not
used despite offering a plethora of opportunities in this context.
The remainder of this overview is structured as follows: We
first summarise phenomena more and less closely related to the
case of COVID-19 that have already been targeted by CA and
would be readily available. Examples include automatic recognition of speakers suffering from a cold or wearing a mask,
breathing, coughing and sneezing sounds, or recognition of audio in spatial proximity. We then shift to the introduction of
concrete use-cases how CA could benefit the ongoing global
fight against the Corona crisis. Subsequently, we introduce
challenges and entry barriers from a technical as well as ethical and societal point of view before concluding this overview.
1 assessed

on 20 March 2020, 11:13:39 AM server time

Speech analysis by computational means is highly related to
the field of Computational Paralinguistics [5]. The field has
several related recognition tasks on offer. These are often well
documented in the framework of competitive challenge events
such as the Interspeech Computational Paralinguistics Challenge (ComParE). The latter has – perhaps closest related to
the COVID-19 case – in its 2017 edition featured the automatic recognition of speech under cold [6], i. .e., automatically
recognising speakers affected by a cold from the acoustics of
their voice. In the ongoing challenge of this year, the continuous assessment of breathing patterns from the speech signal
appears relevant [7], e. g., as basis to recognise COVID-19’s
often witnessed symptoms of short-breathiness and breathing
difficulties. The current ComParE challenge further targets the
recognition of speech under mask, i. e., the automatic recognition whether a speaker is wearing a facial protective mask, and
the recognition of emotion of elderly, which may become interesting in monitoring the aftermath of social isolation of elderly,
as currently discussed, e.ġ., in the U. K. for three months. On
the age scale’s opposite end, toddlers’ crying sounds seems to
be the only indicator to understand if they are suffering from
COVID-19 symptoms. In the ComParE challenge series, infant
crying was investigated in 2018 [8], and the valence, i. e., positivity of baby sounds in 2019 [9]. As symptoms of COVID-19
can also include lack of appetite, it seems further interesting
to reference to the EAT challenge [10]: In this event, it was
demonstrated that one can infer from audio whether speech under eating and eating sounds indicate eating difficulty and “likability” related to whether one enjoys eating. The assessment
of sleepiness – a further symptom of COVID-19 – was first featured in ComParE in 2011 [11] as binary task, and as continuous sleepiness assessment on the Karolinska sleepiness scale
in 2019 [9]. Also pain such as headache or bodily pain can accompany COVID-19; speech under pain has also been shown
to be automatically accessibly [12, 13]. When it comes to individual risk assessment and monitoring, speaker traits may be
of interest. High mortality risk groups include the elderly, and
(slightly) higher mortality rate was so far seen in male individuals [14]. Age and gender were also shown in the context of
ComParE, and can be automatically determined reliably given
sufficient speech material [15]. A history of health issue can
further indicate high risk. A number of health-related speaker
state information relevant in this context has been shown feasible such as individuals suffering from asthma [16], head-andneck cancer [17], or smoking habits [18, 19].
Speaker diarisation, i. e., determining who is speaking
when, and speaker counting [20] can become of interest in the
ongoing social distancing. When it comes to counter measures
such as quarantine, or risk assessment of individuals, one could
also consider the usage of automatic recognition of deceptive
speech when people are questioned about their recent contacts
or whereabouts, as their personal work and life interests may
interfere with the perspective of being sent to quarantine. Deception and sincerity were targeted in ComParE in 2016 [21].
Monitoring wellbeing of individuals during social distancing
and quarantine can further find interest in depression and fear

Risk Assessment

2.1. Speech Analysis

CA Task

Monitoring

In the following, we set out by show-casing what CA has already successfully targeted as audio cases for recognition, and
appears related to the task of interest in this contribution – fighting the ongoing COVID-19 spread.

Table 1: Interdependence of Computer Audition (CA) tasks and
potential use-cases in the context of the Corona crisis. SLP:
Spoken Language Processing. Health state encompasses a
wider range of health factors such as asthma, head and neck
cancer, or smoking habits that can be inferred from speech audio.

Diagnosis

2. Computer Audition: Related Phenomena

Speech Analysis
Age & Gender
Breathing
Cold
Crying (Infants)
Deception & Sincerity
Depression
Emotion (incl. of Elderly)
Health State
Lung Sounds
Mask
Pain
Sleepiness
SLP
Speaker Count

√
√
√
√

√
√
√
√
√
√

√
√
√
√
√
√
√
√
√
√
√

Sound Analysis
Coughing (dry, wet, productive)
Cardiovascular Disease
Diarisation
Localisation
Proximity
Sneezing
Snoring
Source Separation
Swallowing
Throat Clearing

√
√
√
√
√
√
√
√
√
√

√
√
√
√
√
√
√
√
√
√

recognition. Both were shown feasibly to be assessed from
speech in the Audio/Visual Emotion Challenge (AVEC) challenge event series [22] including from speech only at reasonable
deviation on a continuous scale.
Generally speaking, speech audio also includes textual
cues. Broadening up to Spoken Language Processing (SLP),
this can also be of help to gather and analyse information from
spoken conversations available in individual communications,
news or social media. For textual cues, this has already been
considered [23]. From a speech analysis perspective, this includes Automatic Speech Recognition (ASR) and Natural Language Processing (NLP).
2.2. Sound Analysis
From a sound analysis perspective, one may first consider such
of interest for COVID-19 use-cases that are produced by the
human body. In the context of COVID-19, this includes fore
mostly the automatic recognition of coughs [24, 25, 26] including dry vs wet coughing [27] and dry vs productive cough-

ing [28] and sneeze [26], swallowing and throat clearing [25]
sounds – all showcased at high recognition rates. As severe
COVID-19 symptoms are mostly linked to developing a pneumonia which is the cause of most deaths of COVID-19, it further appears of interest that different breathing patterns, respiratory sounds and lung sounds of patients with pneumonia can
be observed through CA [29], even with mass devices such as
smart-phones [30]. Of potential relevance could also be the already possible monitoring of different types of snoring sounds
[31] including their excitation pattern in the vocal tract and their
potential change over time to gain insight on symptoms also
during sleep. Further, highest risk of mortality from COVID-19
has been seen for such suffering from cardiovascular disease
followed by chronic respiratory disease. In ComParE 2018,
heart beats were successfully targeted from audio for three types
heart status, namely, normal, mild, and moderate/severe abnormality. Hearing local proximity from ambient audio further appears possible [32], and could be used to monitor individuals
potentially too close to each other in the ‘social distancing’ protective counter-measure scenarios. 3D audio localisation [33]
and diarisation further allows for locating previously recognised
sounds and attributing them to to sources. This could further
help in the monitoring of public spaces or providing warnings
to users as related to individuals potentially being locally too
close with directional pointers. Audio source separation and denoising [34] of stethoscope sounds and audio [35] for clinicians
and further processing can additionally serve as tool.

3. Potential Use-Cases
Let us next elaborate on use-cases we envision as promising for
CA in the context of COVID-19. A coarse visual overview on
the dependence of CA tasks and these use-cases is provided in
Table 1. Check-marks indicate that the already available automatic audio analysis tasks listed in the left column appear of
interest in the three major use-case groups listed in the rightmost three columns. Note that these are indicative in nature.
3.1. Risk Assessment
A first use-case targets the prevention of COVID-19 spread by
individual risk assessment. As shown above, speaker traits like
age, gender, or health state can be assessed automatically from
the voice to provide an estimate on the individual mortality risk
level. In addition, one can monitor if oneself or others around
are wearing a mask when speaking, count speakers around oneself and locate these and their distance to provide a real-time
ambient risk assessment and informative warning.
3.2. Diagnosis
While the standard for diagnosis of COVID-19 is currently a
nasopharyngeal swab, several other possibilities exist including
chest CT-based analysis as very reliable resource as outlined
above. Here, we consider whether an audio-based diagnosis
could be possible. While it seems clear that such an analysis
will not be suited to compete with the state-of-the-art in professional testing previously named, its non-invasive and ubiquitously available nature would allow for individual pre-screening
‘anywhere’, ‘anytime’, in real-time, and available more or less
to ‘anyone’. To the best of the authors’ knowledge, no study has
yet investigated audio from COVID-19 patients vs highly varied control group data including such suffering from influenza
or cold and healthy individuals. Unfortunately, coughing and
sneezing of COVID-19 patients does not differ significantly to

human perception from ‘normal’ patients. This includes lung
and breathing sounds. However, [36] assume that abnormal respiratory patterns can be a clue for diagnosis. Overall, by that,
it seems unclear if diagnosis from short audio samples of patients could be directly possible, given that most speech or body
sounds are likely not to show significant differences for closely
related phenomena such as influenza or cold.
Rather, we believe that a histogram of symptoms over time
in combination with their onset appears highly promising. Table 2 visualises this concept in a qualitative manner by coarse
ternary quantification of each symptom or ‘feature’ from a machine learning perspective2 . Each of the symptoms in the table
can – as outlined above – (already) be assessed automatically
from an intelligent audio sensor. In a suited personal application
such as on a smart phone or smart watch, smart home device
with audio abilities, or via a telephone service, etc., one could
collect frequency of symptoms over time and from the resulting
histogram differentiate with presumably high success rate between COVID-19, influenza, and cold. By suited means of AI,
a probability could be given to users how likely their symptoms
speak for COVID-19. Of particular interest thereby is also the
“Onset Gradient” feature in Table 2. It alludes to whether the
onset of symptoms over time is gradual (i. e., over the span of
up to two weeks or more) or rather abrupt (i. e., within hours or
a few days only), which can be well observed by AI analysis in
a histogram sequence updated over time. Collecting such information from many users, this estimate for histogram-based diagnosis of COVID-19 can be improved in precision over time if
users “donate their data”. In addition, clinicians could be given
access to the histogram or be pointed to typical audio examples in a targeted manner remotely that have been collected over
longer time to speed up the decision whether the users should
go for other more reliable forms of testing. This could help to
highly efficiently pre-select individuals for screening.
3.3. Monitoring of Spread
Beyond the idea of using smartphone based surveys and AI
methods to monitor the spread of the virus [37], one could use
CA for audio analysis via telephone or other spoken conversation. An AI could monitor the spoken conversations and screen
for speech under cold or other symptoms as shown in Table 2.
Together with GPS coordinates from smart phones or knowledge of the origin of the call from the cell, one could establish
real-time spread maps.
3.4. Monitoring of Social Distancing and Effects
Social distancing and – in already diagnosed cases of COVID19 or direct contact isolation of individuals – might lead to different negative side effects. People who have less social connection might suffer from even a weaker immune system, responding less well to pathogens [38]. Especially, the high-risk
target group of elderly could even encounter suicidal thoughts
and develop depression or other clinical conditions in isolation
[39]. CA might provide indications about social interaction,
exemplary speaking time during the day via phone or other devices, as well as measure emotions of the patient throughout the
day or detecting symptoms of depression and suicidal risk [40].
In addition, the public obedience and discipline in socialdistancing could be monitored with the aid of CA. AI allows
to count speakers, locate them and their potential symptoms as
2 based on https://www.qld.gov.au, https://www.medicinenet.com/,
https://www.medicalnewstoday.com, all assessed on 20 March 2020.

COVID-19

Influenza

Cold

Table 2: Qualitative behaviour of symptoms of COVID-19 vs
cold and influenza (flu): Tentative histogram by symptom (‘feature’/‘variable’) in ternary quantification (from no/low (‘+’) to
frequent/high (“+++’)). Shown is also the symptom gradient
onset behaviour. Further frequently related variables include
diarrhea, fever, or watery eyes, which could partially be assessed also by audio – the latter two rater by physiological
and visual sensors, respectively. Assembled from diverse references. The table is indicative in nature on purpose, and more
fine-grained quantification could apply.

+++
+++

++
++

+
+

+
+

++
+

+++
+++

Coughing

dry ++

dry ++

+

Sneezing

+

+

+++

Sore Throat

+

++

+++

+
++

+++
+++

++
+

mild ++

+++

+

Appetite Loss

+

+++

+

Onset Gradient

+

+++

+

Symptom
Breathing: Dypnea (Shortness)
Breathing: Difficulty
Rhinorrhea (Runny Nose)
Nasal Congestion

Pain: Body
Pain: Head (Headache)
Fatigue, Tiredness

reflecting in the audio signal (cf. Table 2), and ‘diarise’ the audio sources, i. e., attribute which symptoms came from which
(human) individual. Likewise, public spaces could be empowered by AI that detects potentially risky settings, which are overcrowded, under-spaced in terms of distance between individuals, and spot potentially COVID-19 affected subjects among a
crowd, and whether these and others are wearing a protective
mask while speaking.
3.5. Monitoring of Treatment and Recovery
During hospitalisation or other forms of treatment and recovery, CA can monitor the progress, e. g., by updating histograms
of symptoms. In addition, the wellbeing of patients could be
monitored similarly to the case of individual monitoring in social distancing situations as described above. This could include
listening to their emotions, eating habits, fatigue, or pain, etc.
3.6. Generation of Speech and Sound
While we have focused entirely on the analysis of audio up to
this point, it remains to state that there may be also use-cases for
the generation of audio by AI in a COVID-19 scenario. Speech
conversion and synthesis could, e.ġ., help those suffering from
COVID-19 symptoms to ease their conversation with others. In
such a setting, an AI algorithm can fill in the gaps arising from
coughing sounds, enhance a voice suffering from pain or fatigue and further more, e. g., by generative adversarial networks
[41]. In addition, alarm signals could be rendered which are
mnemonic and re-recognisable, but adapt to the ambient sound
to be particularly audible.

4. Challenges
4.1. Time
The fight against COVID-19 has been marked by a race to prevent too rapid spread that could lead to peak infection rates
that overburden the national health systems and availability of
beds in the intensive care units leading to high morality rates.
Further, at presence, it cannot be clearly stated whether or not
COVID-19 will persistently stay as disease. However, recent research and findings [42] as well as model calculations indicate
that COVID-19 will heavily spread over the next 6-9 months in
different areas of the world. Enhanced social distancing might
delay the spreading. Additionally, at the moment there is no
solid research available to prove persistent immunity against the
virus after an infection with COVID-19. Therefore, the need to
apply measures of enhanced risk assessment, diagnosis, monitoring, and treatment is urgently necessary to support the current
medical system as well as to get COVID-19 under control.
4.2. Collecting COVID-19 Patient Data
Machine learning essentially needs data to learn from. Accordingly, for any kind of CA application targeting speech or
sounds from patients suffering from COVID-19 infection, we
will need collected and annotated data. At present, such data
is not publicly available for research purposes, but urgently
needed. Hence, a crucial step in the first place will be to collect
audio data from diagnosed patients and ideally control subjects
under equal conditions and demographic characteristics.
4.3. Model Sharing
In order to accelerate the adaptation of machine learning models
of CA for COVID-19, exchange of data will be crucial. As
such data is usually highly private and sensitive in nature, the
recent advances in federated machine learning [43] can benefit
the exchange of personal model parameters rather than audio
to everyone’s benefit. Likewise, user of according services can
‘donate their data’ in a safe and private manner.
4.4. Real-world Audio Processing
Most of the tasks and use-cases listed above require processing
of audio under more or lass constrained ‘in-the-wild’ conditions
such as audio recording over telephone, VoIP, or audio takes
at home, in public spaces, or in hospitals. These are usually
marked by noise, reverberation, transmissions with potential
loss, and further disturbances. In addition, given the pandemic
character of the SARS-CoV-2 Corona crisis, one will ideally
need to be robust against multilingual, multicultural, or local
speech and sound variability.
4.5. Green Processing
Green processing summarises here the idea of efficiency in
computing. This will be a crucial factor for mobile applications, but also for big(ger) data speech analysis [44] such as in
the case of telephone audio data analysis. It includes conservative consumption of energy such as on mobile devices, efficient transmission of data such as in the above named federated
machine learning in order not to burden network transmission,
memory efficiency, model update efficiency, and many further
factors.

4.6. Trustability of Results
Machine Learning and Pattern Recognition methods as used in
CA are usually statistical learning paradigms and hence prone
to error. The probability of error needs to be a) estimated,
known as confidence measure estimation, and b) communicated
to users of CA services in the COVID-19 context to assure
trustability of these methods. One step further, results should
ideally be explainable. However, eXplainable AI (XAI) itself
is at this time a young discipline, but provides an increasing
method inventory allowing for interpretation of results [45].
4.7. Ethics
Many of the above suggested use-cases come at massive ethical responsibility and burden which can often only be justified in times of global crisis as the current one. This includes
fore mostly many of the above sketched applications of CA for
monitoring. Assuring privacy at all times will be crucial to benefit only the goal of fighting COVID-19 spread without opening
doors for massive miss-use. Further concerns in this context
will touch upon legal and societal implications. All of these
cannot be discussed here – rather, we can provide pointers for
the interested reader as starting points [46, 47, 48].

5. Concluding Remarks
In this short overview, we provided pointers towards what Computer Audition (CA) could potentially contribute to the ongoing
fight against the world-wide spread of the SARS-CoV-2 virus
known as ‘Corona crisis’ and the COVID-19 infection triggered
by it. We have summarised a number of potentially useful audio
analysis tasks by means of AI that have already been demonstrated feasible. We further elaborated use-cases how these
could benefit in this battle, and shown challenges arising from
real-life usage. The envisioned use-cases included automated
audio-based individual risk assessment, audio-only-based diagnosis by symptom frequency and symptom development histograms over time in combination with machine learning, and
several contributions to monitoring of COVID-19 and its effects
including spread, social distancing, and treatment and recovery
besides use-cases for audio generation. At the time of writing,
it seems that what matters most is a rapid exploitation of this
largely untapped potential. Obviously, in this short overview,
not all possibilities could be included, and many further potential use-cases may exist. Further, the authorship is formed
by experts on CA, digital health, and clinicians having worked
with COVID-19 infected patients over the last months – further
insights from other disciplines will be highly valuable to add.
The Corona crisis demands for common efforts on all ends –
we truly hope computer audition can add a significant share to
an accelerated success of the crisis’ defeat.

6. Acknowledgements
We express our deepest sorrow for those who left us due to
COVID-19; they are not numbers, they are lives. We further
express our highest gratitude and respect to the clinicians and
scientists, and anyone else these days helping to fight against
COVID-19, and at the same time help us maintain our daily
lives. We acknowledge funding from the EU’s HORIZON
2020 Grant No. 115902 (RADAR CNS). This work was further
partially supported by the Zhejiang Lab’s International Talent
Fund for Young Professionals (Project HANAMI), P. R. China,
the JSPS Postdoctoral Fellowship for Research in Japan (ID

No. P19081) from the Japan Society for the Promotion of Science (JSPS), Japan, and the Grants-in-Aid for Scientific Research (No. 19F19081 and No. 17H00878) from the Ministry of
Education, Culture, Sports, Science and Technology (MEXT),
Japan.

7. References
[1] Z. Hu, Q. Ge, L. Jin, and M. Xiong, “Artificial intelligence forecasting of covid-19 in China,” arXiv preprint arXiv:2002.07112,
2020.
[2] O. Gozes, M. Frid-Adar, H. Greenspan, P. D. Browning,
H. Zhang, W. Ji, A. Bernheim, and E. Siegel, “Rapid ai development cycle for the coronavirus (covid-19) pandemic: Initial results for automated detection & patient monitoring using deep
learning ct image analysis,” arXiv preprint arXiv:2003.05037,
2020.
[3] S. Wang, B. Kang, J. Ma, X. Zeng, M. Xiao, J. Guo, M. Cai,
J. Yang, Y. Li, X. Meng et al., “A deep learning algorithm using
ct images to screen for corona virus disease (covid-19),” medRxiv,
2020.
[4] H. S. Maghdid, K. Z. Ghafoor, A. S. Sadiq, K. Curran, and K. Rabie, “A novel ai-enabled framework to diagnose coronavirus covid
19 using smartphone embedded sensors: Design study,” arXiv
preprint arXiv:2003.07434, 2020.
[5] B. Schuller and A. Batliner, Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing.
Wiley, 2013.
[6] B. Schuller, S. Steidl, A. Batliner, E. Bergelson, J. Krajewski,
C. Janott, A. Amatuni, M. Casillas, A. Seidl, M. Soderstrom,
A. Warlaumont, G. Hidalgo, S. Schnieder, C. Heiser, W. Hohenhorst, M. Herzog, M. Schmitt, K. Qian, Y. Zhang, G. Trigeorgis,
P. Tzirakis, and S. Zafeiriou, “The INTERSPEECH 2017 Computational Paralinguistics Challenge: Addressee, Cold & Snoring,”
in Proceedings INTERSPEECH. Stockholm, Sweden: ISCA,
2017, pp. 3442–3446.
[7] B. W. Schuller, A. Batliner, C. Bergler, E.-M. Messner, A. Hamilton, S. Amiriparian, A. Baird, G. Rizos, M. Schmitt, L. Stappen,
H. Baumeister, A. D. MacIntyre, and S. Hantke, “The INTERSPEECH 2020 Computational Paralinguistics Challenge: Elderly
Emotion, Breathing & Masks,” in Proceedings INTERSPEECH.
Shanghai, China: ISCA, 2020, 5 pages, to appear.
[8] B. W. Schuller, S. Steidl, A. Batliner, P. B. Marschik, H. Baumeister, F. Dong, S. Hantke, F. Pokorny, E.-M. Rathner, K. D. BartlPokorny, C. Einspieler, D. Zhang, A. Baird, S. Amiriparian,
K. Qian, Z. Ren, M. Schmitt, P. Tzirakis, and S. Zafeiriou, “The
INTERSPEECH 2018 Computational Paralinguistics Challenge:
Atypical & Self-Assessed Affect, Crying & Heart Beats,” in Proceedings INTERSPEECH. Hyderabad, India: ISCA, 2018, pp.
122–126.
[9] B. W. Schuller, A. Batliner, C. Bergler, F. Pokorny, J. Krajewski,
M. Cychosz, R. Vollmann, S.-D. Roelen, S. Schnieder, E. Bergelson, A. Cristià, A. Seidl, L. Yankowitz, E. Nöth, S. Amiriparian,
S. Hantke, and M. Schmitt, “The INTERSPEECH 2019 Computational Paralinguistics Challenge: Styrian Dialects, Continuous
Sleepiness, Baby Sounds & Orca Activity,” in Proceedings INTERSPEECH. Graz, Austria: ISCA, 2019, pp. 2378–2382.
[10] D. Schuller and B. Schuller, “The Challenge of Automatic Eating
Behaviour Analysis and Tracking,” in Recent Advances in Intelligent Assistive Technologies: Paradigms and Applications, ser. Intelligent Systems Reference Library, H. N. Costin, B. W. Schuller,
and A. M. Florea, Eds. Springer, 2020, pp. 187–204.
[11] B. Schuller, S. Steidl, A. Batliner, F. Schiel, J. Krajewski,
F. Weninger, and F. Eyben, “Medium-Term Speaker States –
A Review on Intoxication, Sleepiness and the First Challenge,”
Computer Speech and Language, vol. 28, no. 2, pp. 346–374,
2014.

[12] Y. Oshrat, A. Bloch, A. Lerner, A. Cohen, M. Avigal, and
G. Zeilig, “Speech prosody as a biosignal for physical pain detection,” in Proceedings 8th Speech Prosody, Boston, USA, 2016,
pp. 420–424.
[13] Z. Ren, N. Cummins, J. Han, S. Schnieder, J. Krajewski, and
B. Schuller, “Evaluation of the Pain Level from Speech: Introducing a Novel Pain Database and Benchmarks,” in Proceedings 13th
ITG Conference on Speech Communication, ser. ITG-Fachbericht,
vol. 282. Oldenburg, Germany: ITG/VDE, 2018, pp. 56–60.
[14] F. Caramelo, N. Ferreira, and B. Oliveiros, “Estimation of risk factors for covid-19 mortality-preliminary results,” medRxiv, 2020.
[15] F. Weninger, E. Marchi, and B. Schuller, “Improving Recognition of Speaker States and Traits by Cumulative Evidence: Intoxication, Sleepiness, Age and Gender,” in Proceedings INTERSPEECH. Portland, USA: ISCA, 2012, pp. 1159–1162.
[16] I. Mazić, M. Bonković, and B. Džaja, “Two-level coarse-to-fine
classification algorithm for asthma wheezing recognition in children’s respiratory sounds,” Biomedical Signal Processing and
Control, vol. 21, pp. 105–118, 2015.
[17] A. Maier, T. Haderlein, F. Stelzle, E. Nöth, E. Nkenke,
F. Rosanowski, A. Schützenberger, and M. Schuster, “Automatic
speech recognition systems for the evaluation of voice and speech
disorders in head and neck cancer,” EURASIP Journal on Audio,
Speech, and Music Processing, vol. 2010, no. 1, p. 926951, 2009.
[18] A. H. Poorjam, M. H. Bahari et al., “Multitask speaker profiling for estimating age, height, weight and smoking habits from
spontaneous telephone speech signals,” in Proceedings 4th International Conference on Computer and Knowledge Engineering
(ICCKE). Masshad, Iran: IEEE, 2014, pp. 7–12.
[19] H. Satori, O. Zealouk, K. Satori, and F. Elhaoussi, “Voice comparison between smokers and non-smokers using hmm speech
recognition system,” International Journal of Speech Technology,
vol. 20, no. 4, pp. 771–777, 2017.
[20] C. Xu, S. Li, G. Liu, Y. Zhang, E. Miluzzo, Y.-F. Chen, J. Li,
and B. Firner, “Crowd++ unsupervised speaker count with smartphones,” in Proceedings ACM International Joint Conference
on Pervasive and Ubiquitous Computing (UbiComp, Zurich,
Switzerland, 2013, pp. 43–52.
[21] B. Schuller, S. Steidl, A. Batliner, J. Hirschberg, J. K. Burgoon,
A. Baird, A. Elkins, Y. Zhang, E. Coutinho, and K. Evanini, “The
INTERSPEECH 2016 Computational Paralinguistics Challenge:
Deception, Sincerity & Native Language,” in Proceedings INTERSPEECH 2016. San Francisco, USA: ISCA, 2016, pp. 2001–
2005.
[22] M. Valstar, J. Gratch, B. Schuller, F. Ringeval, R. Cowie, and
M. Pantic, “Summary for AVEC 2016: Depression, Mood, and
Emotion Recognition Workshop and Challenge,” in Proceedings
24th ACM International Conference on Multimedia (MM). Amsterdam, The Netherlands: ACM, 2016, pp. 1483–1484, (acceptance rate short paper: 30 %).
[23] R. Pandey, V. Gautam, K. Bhagat, and T. Sethi, “A machine learning application for raising wash awareness in the times of covid19 pandemic,” arXiv preprint arXiv:2003.07074, 2020.
[24] S. Matos, S. S. Birring, I. D. Pavord, and H. Evans, “Detection
of cough signals in continuous audio recordings using hidden
markov models,” IEEE Transactions on Biomedical Engineering,
vol. 53, no. 6, pp. 1078–1083, 2006.
[25] T. Olubanjo and M. Ghovanloo, “Tracheal activity recognition
based on acoustic signals,” in Proceedings 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). Chicago, USA: IEEE, 2014, pp. 1436–
1439.
[26] S. Amiriparian, S. Pugachevskiy, N. Cummins, S. Hantke, J. Pohjalainen, G. Keren, and B. Schuller, “CAST a database: Rapid targeted large-scale big data acquisition via small-world modelling
of social media platforms,” in Proceedings 7th biannual Conference on Affective Computing and Intelligent Interaction (ACII).
San Antionio, USA: IEEE, 2017, pp. 340–345.

[27] P. Moradshahi, H. Chatrzarrin, and R. Goubran, “Improving the
performance of cough sound discriminator in reverberant environments using microphone array,” in Proceedings International Instrumentation and Measurement Technology Conference
(I2MTC). Graz, Austria: IEEE, 2012, pp. 20–23.
[28] J. Schröder, J. Anemiiller, and S. Goetze, “Classification of human cough signals using spectro-temporal gabor filterbank features,” in Proceedings International Conference on Acoustics,
Speech and Signal Processing (ICASSP).
Shanghai, China:
IEEE, 2016, pp. 6455–6459.
[29] R. L. Murphy, A. Vyshedskiy, V.-A. Power-Charnitsky, D. S.
Bana, P. M. Marinelli, A. Wong-Tse, and R. Paciej, “Automated
lung sound analysis in patients with pneumonia,” Respiratory
care, vol. 49, no. 12, pp. 1490–1497, 2004.
[30] I. Song, “Diagnosis of pneumonia from sounds collected using
low cost cell phones,” in Proceedings International Joint Conference on Neural Networks (IJCNN). IEEE, 2015, pp. 1–8.
[31] C. Janott, M. Schmitt, Y. Zhang, K. Qian, V. Pandit, Z. Zhang,
C. Heiser, W. Hohenhorst, M. Herzog, W. Hemmert, and
B. Schuller, “Snoring Classified: The Munich Passau Snore
Sound Corpus,” Computers in Biology and Medicine, vol. 94,
no. 1, pp. 106–118, 2018.
[32] F. B. Pokorny, M. Fiser, F. Graf, P. B. Marschik, and B. W.
Schuller, “Sound and the city: Current perspectives on acoustic geo-sensing in urban environment,” Acta Acustica united with
Acustica, vol. 105, no. 5, pp. 766–778, 2019.
[33] S. Delikaris-Manias, D. Pavlidi, V. Pulkki, and A. Mouchtaris,
“3d localization of multiple audio sources utilizing 2d doa histograms,” in Proceedings 24th European Signal Processing Conference (EUSIPCO). Budapest, Hungary: IEEE, 2016, pp. 1473–
1477.
[34] S. Liu, G. Keren, and B. W. Schuller, “N-HANS: Introducing the Augsburg Neuro-Holistic Audio-eNhancement System,”
arXiv preprint arXiv:1911.07062, 2019.
[35] K. Yang, Z. He, W. Yang, Q. Tang, D. Li, Z. Wang, Q. Lin,
and W. Jia, “Heart sound denoising using computational auditory scene analysis for a wearable stethoscope,” in Proceedings
56th International Midwest Symposium on Circuits and Systems
(MWSCAS). Columbus, USA: IEEE, 2013, pp. 1220–1223.
[36] Y. Wang, M. Hu, Q. Li, X.-P. Zhang, G. Zhai, and N. Yao, “Abnormal respiratory patterns classifier may contribute to large-scale
screening of people infected with covid-19 in an accurate and unobtrusive manner,” arXiv preprint arXiv:2002.05534, 2020.
[37] A. S. S. Rao and J. A. Vazquez, “Identification of covid-19 can be
quicker through artificial intelligence framework using a mobile
phone-based survey in the populations when cities/towns are under quarantine,” Infection Control & Hospital Epidemiology, pp.
1–18, 2020.
[38] S. W. Cole, M. E. Levine, J. M. Arevalo, J. Ma, D. R. Weir,
and E. M. Crimmins, “Loneliness, eudaimonia, and the human
conserved transcriptional response to adversity,” Psychoneuroendocrinology, vol. 62, pp. 11–17, 2015.
[39] Y. Luo, L. C. Hawkley, L. J. Waite, and J. T. Cacioppo, “Loneliness, health, and mortality in old age: A national longitudinal
study,” Social science & medicine, vol. 74, no. 6, pp. 907–914,
2012.
[40] N. Cummins, S. Scherer, J. Krajewski, S. Schnieder, J. Epps, and
T. F. Quatieri, “A review of depression and suicide risk assessment
using speech analysis,” Speech Communication, vol. 71, pp. 10–
49, 2015.
[41] S. Pascual, A. Bonafonte, and J. Serra, “Segan: Speech
enhancement generative adversarial network,” arXiv preprint
arXiv:1703.09452, 2017.
[42] Z. Wu and J. M. McGoogan, “Characteristics of and important
lessons from the Coronavirus disease 2019 (COVID-19) outbreak
in China: summary of a report of 72314 cases from the Chinese
Center for Disease Control and Prevention,” JAMA, 2020.

[43] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept and applications,” ACM Transactions on Intelligent
Systems and Technology (TIST), vol. 10, no. 2, pp. 1–19, 2019.
[44] J. P. Verma, S. Agrawal, B. Patel, and A. Patel, “Big data analytics: Challenges and applications for text, audio, video, and social
media data,” International Journal on Soft Computing, Artificial
Intelligence and Applications (IJSCAI), vol. 5, no. 1, pp. 41–51,
2016.
[45] A. Adadi and M. Berrada, “Peeking inside the black-box: A
survey on explainable artificial intelligence (xai),” IEEE Access,
vol. 6, pp. 52 138–52 160, 2018.
[46] A. Batliner and B. Schuller, “More Than Fifty Years of Speech
Processing – The Rise of Computational Paralinguistics and Ethical Demands,” in Proceedings ETHICOMP.
Paris, France:
CERNA, 2014.
[47] D. Greene, A. L. Hoffmann, and L. Stark, “Better, nicer, clearer,
fairer: A critical assessment of the movement for ethical artificial intelligence and machine learning,” in Proceedings 52nd
Hawaii International Conference on System Sciences (HICSS),
Kauai, USA, 2019, pp. 2122–2131.
[48] C. Nebeker, J. Torous, and R. J. B. Ellis, “Building the case for
actionable ethics in digital health research supported by artificial
intelligence,” BMC medicine, vol. 17, no. 1, p. 137, 2019.

