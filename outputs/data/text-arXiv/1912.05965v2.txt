arXiv:1912.05965v2 [stat.AP] 18 Nov 2020

A Powerful Modelling Framework for
Nowcasting and Forecasting COVID-19 and
Other Diseases
Oliver Stoner∗ , Theo Economou and Alba Halliday
Department of Mathematics, University of Exeter, UK
November 19, 2020

Abstract
The COVID-19 pandemic has highlighted delayed reporting as a significant impediment to effective disease surveillance and decision-making. In the absence of timely
data, statistical models which account for delays can be adopted to nowcast and
forecast cases or deaths. We discuss the four key sources of systematic and random
variability in available data for COVID-19 and other diseases, and critically evaluate
current state-of-the-art methods with respect to appropriately separating and capturing this variability. We present a general spatio-temporal hierarchical framework
for correcting delayed reporting and demonstrate its application to daily English hospital deaths from COVID-19 and Severe Acute Respiratory Infection cases in Brazil.
We compare our approach to competing models with respect to theoretical flexibility
and quantitative metrics from a rolling nowcasting experiment imitating a realistic
operational scenario. Based on consistent and compelling leads in nowcasting accuracy, bias, and precision, we demonstrate that our approach represents the current
best-practice for correcting delayed reporting.

Keywords: Bayesian, censoring, coronavirus, Generalized Dirichlet, notification delay.
∗

This work was supported by an EPSRC Doctoral Training Partnership studentship awarded to Alba
Halliday.

1

1

Introduction

The coronavirus disease or COVID-19 is an infectious disease caused by the severe acute
respiratory syndrome coronavirus 2 (SARS-Cov-2) virus. Like many infectious diseases,
data on COVID-19 cases and deaths are typically subject to delayed reporting, otherwise
known as ‘notification delay’. This is when available count data are, for a time, an underrepresentation of the truth, owing to flaws or ‘lags’ in the data collection mechanism. In
disease surveillance, delays – e.g. ones that occur during the transfer of information from
local clinics to national surveillance centres – mean that complete and informative counts
of new cases or deaths are not immediately available. Often these delays are substantial,
so that it can take several weeks or even months for the available data to reach a total
reported count.
Delay Structure of COVID−19 Data
Hospital Deaths in East of England

Day of Death

t−4

Figure 1: Bar plot of reported COVID-19 hospital deaths in the East of England region, for
the days leading up to and including day t, the
4th of May 2020. The grey bars represent the
total (as yet unobserved) number of reported
deaths, while the different coloured bars show
the number of deaths reported after each day
of delay.

Delay

t−3

1
2

t−2

3
4

t−1

5
t
0

10

20

30

40

50

Reported Deaths

To visualise the data challenge, Figure 1 shows reported COVID-19 hospital deaths in
the East of England in the days leading up to and including day t, the 4th of May 2020.
The coloured bars show the partially reported data available at the end of day t, while the
grey bars show the as-of-yet unknown total number of deaths for each date. For day t,
we have only observed the portion of deaths reported within the latest 24 hour reporting
2

period (here referred to as ‘within the first delay’ and shown in green). For t − 1, we have
data reported within the first delay (green) as well as ones reported within the second delay
(orange). The number of available ‘delayed’ counts therefore increases by one for each day
we go back into the past.
Significant changes in the delay mechanism (e.g. in the proportion of deaths reported in
the first delay) make it challenging to draw conclusions about the total counts in a timely
manner. For example, a similar number of deaths were reported within the first delay for
days t and t − 2 (Figure 1), while the total number of deaths (grey) was much higher for
t − 2. For a practitioner analysing the data at the end of day t, there is no clear way of
knowing from the available reported counts that the total for day t is comparatively low –
especially given the increased death count reported so far occurring on day t − 1 compared
to day t − 2. In disease surveillance, therefore, delayed reporting can make it difficult to
confidently detect an outbreak within a time frame during which interventions are most
effective. A particular issue during the COVID-19 pandemic is the need to confidently
detect any local outbreaks so that interventions like increased social distancing measures
can be considered and implemented. Here, failure to act in a timely manner carries the
risk of loss of life, while unnecessary interventions can also be costly for the local economy
or other aspects of population well-being.
From a statistical perspective, tackling delayed reporting is a prediction problem. We
would like to predict (nowcast) the present-day total count (e.g. the number of deaths), as
well as forecast future counts, based on any available partial counts and potentially on any
historical total counts which have now been fully observed. In this article we propose and
evaluate a compelling spatio-temporal hierarchical approach to correcting delayed reporting
in COVID-19 data and other disease surveillance applications. The article is structured

3

as follows: in Section 2, we discuss the need to consider different sources of variability
in COVID-19 data suffering from delayed reporting and use this as a principled basis
for comparing existing approaches; in Section 3, we present our general framework for
correcting delayed reporting in COVID-19 data, alongside a general discussion of spatial,
temporal, and spatio-temporal structures which may be included in the model; in Section
4, we apply this framework to counts of hospital deaths from COVID-19 in regions of
England and present a rolling prediction experiment to illustrate the model’s operational
effectiveness in comparison with other approaches. Finally, we conclude with a critical
discussion of our approach and avenues for future research in Section 5.
Accompanying the article is a substantial appendix, which is structured as follows: in
Appendix A we apply our approach to Severe Acute Respiratory Infection (SARI) data from
Brazil, to both illustrate the framework’s applicability to general disease surveillance data
suffering from delayed reporting and also to compare independent time series models for
each region with a joint model sharing the same spatio-temporal structure as the COVID-19
application; in Appendix B we present the mathematical formulation of competing models
appearing in section 4; and finally, in Appendix C we discuss how our framework could be
adjusted to take into account under-reporting in the final reported death/case counts.

2

Background

We begin by introducing some notation. Let y be the total count, e.g. the number of
COVID-19 deaths or cases occurring on a given day, and let zd be the portion of y observed
P
within d = 1, . . . , D delays, so that D
d=1 zd = y. To better understand existing modelling
approaches, it is instructive to appreciate the different sources of variability which might

4

be present in data relating to COVID-19 but also other diseases. Figure 2 shows the
total number of COVID-19 in-hospital deaths that were identified on each day in England,
which we call the ‘announced’ deaths. These are deaths that were confirmed on that day,
but may have occurred days ago – shown here in order to evidence the variability in the
reporting delay. Also plotted are the ‘actual’ deaths (y) confirmed to have occurred on
that day (which is unknown for a time due to delayed reporting). The period in concern
(April-May) is broadly after the first peak in the UK, and there is a clear downward trend
in both the actual and announced deaths. This downward trend illustrated by the solid
line is what we call the ‘systematic variability’ in y, which will vary regionally e.g. due
to different population sizes, population densities or time since disease took hold of the
region. The day-to-day fluctuation of the actual death count y about the smooth curve, is
what we call the ‘random variability’ of y.
Hospital Deaths from COVID−19 in England
Monday
Tuesday
Wednesday

750

Deaths

Thursday
Friday
500

Saturday
Sunday

250
Actual
Announced

0
Apr 15

May 01

May 15

Trend

Date

Figure 2: Scatter plot of daily hospital deaths in England. Dashed line and points: the
number of deaths reported on each day (announced deaths). Different shapes and colours
represent the day of the week. Dotted line: the number of actual deaths on each day (y).
Solid line: smooth trend of the actual deaths.

5

In addition to the variability in y, we must also consider variability in the reporting
delay, which can also be decomposed into random and systematic. Notice for instance the
clear ‘weekly cycle’ in the announced deaths (Figure 2) – also referred to as the ‘weekend
effect’ – where significantly fewer deaths are announced at weekends. The weekend effect
for this data can be explained by lower levels of administrative staffing at many hospitals
trusts on Saturday and Sunday. This further highlights the issue in using such data as a raw
indicator of the progression of the epidemic, especially when flaws like delayed-reporting
aren’t always communicated transparently. From a modelling perspective, failure to take
into account this kind of systematic variability in the reporting delay (which in conjunction
with variability in y makes up the variability in the delayed counts zd ) means ignoring
crucial information when it comes to nowcasting and forecasting.
In addition to the weekly cycle, we would also expect systematic between-region variability in the reporting delay, e.g. resource inequality between regions; as well as systematic
temporal variability, e.g. if reporting efficiency improves over time. In summary, attempts
to correct for delayed reporting of COVID-19 should carefully consider the following four
sources of variability in the available data:
(a) Systematic variability in the total count y (e.g. exponential growth/decay, seasonal
patterns, regional variation).
(b) Random variability in y (e.g. day-to-day variation in death count).
(c) Systematic variability in the reporting delay (e.g. weekly cycles, improvements in
reporting efficiency over time, between-region differences).
(d) Random variability in the reporting delay (e.g. day-to-day variation).

6

The available data at any given time comprise historical (fully) reported counts y and
partial counts zd corresponding both to historic y and to more recent unobserved y. These
are the sources of information to be utilised for nowcasting and forecasting and as explained
in the subsequent section, the appropriate handling of their respective variability will result
in more optimal predictions of current and future counts y.

2.1

Review of existing approaches

Stoner and Economou (2019) present an overview of the well established biostatistical literature on correcting reporting delay. Here we revisit some of that but with a particular
focus on utility to COVID-19 applications: Höhle and an der Heiden (2014) and Salmon
et al. (2015) both propose approaches which combine a Poisson/Negative-Binomial model
to describe y with a Multinomial model for the partial counts zd |y, to describe variability
in the delayed reporting. These models are applied to Shiga toxin-producing Escherichia
coli (STEC) (Höhle and an der Heiden, 2014) and Salmonella (Salmon et al., 2015) data,
respectively. The main strength of these approaches is the intuitive separation of variability (random and systematic) in the total count y (a & b) from variability in the reporting
delay (c & d). In particular, Höhle and an der Heiden (2014) present two separate options:
1) the Multinomial probabilities are realisations from the Generalized-Dirichlet distribution for each time step, and 2) the Multinomial probabilities are modelled with a logistic
transformation of potentially informative covariates. The first option offers considerable
flexibility to capture different amounts of random variability in the reporting delay, but
lacks the capability of capturing systematic variability like the weekly-cycle in reporting
performance discussed previously. Meanwhile, the second option allows such systematic
variability to be captured, at the expense of model fit and non-optimal predictions in the
7

(very common) situations where delayed counts zd |y are over-dispersed with respect to the
Multinomial (Stoner and Economou, 2019).
Epidemiological applications (including disease surveillance) often have a spatial dimension (Cabrera and Taylor, 2019) and this is certainly true for COVID-19, where data are
often grouped into geographical units like regions or health authorities. Two existing approaches which deal with spato-temporal data are Bastos et al. (2019) and Rotejanaprasert
et al. (2020). In both cases, the partial counts zd are assumed Negative-Binomial in a
Bayesian hierarchical framework, where E[zd ] = µd depends on covariates and random effects intended to capture systematic variability in the total count (a) – albeit indirectly
P
through y = d zd – and in the reporting delay (c). This approach, applied to spatiotemporal SARI data from Brazil (Bastos et al., 2019) and to dengue fever data from Thailand (Rotejanaprasert et al., 2020), is a generalisation of older chain-ladder approaches (e.g.
Mack (1993)) and is quite flexible, as it can potentially incorporate a wide variety of temporal, spatial and spatio-temporal structures. However, the total counts are not explicitly
modelled, while the partial counts are assumed independent given covariates and random
effects. As such, random variability in the total counts (b) is not necessarily captured
well in addition to the added risk of excessive predictive uncertainty when nowcasting and
forecasting (Stoner and Economou, 2019). This is in part due to the lack of separation
between systematic variability in the total count (a) and the reporting delay (c). A similar
approach which partly addresses this separation issue is given by McGough et al. (2020),
P
where the mean of zd is defined as µd = βd λ. Parameter βd ∈ (0, 1), where d βd = 1, is the
proportion expected to be reported with delay d, while λ = E[y] is effectively the mean of
the total count. The proportions β = {βd } are fixed, while λ is modelled by random effects
at the log-scale. To account for systematic variation (over time) in the reporting delay

8

(c), the model is applied over a sliding temporal window of fixed length. As such, βd is
representative of reporting behaviour in more recent data. Although this allows flexibility
to capture structured temporal variability in the delay, it may result in over-smoothing of
the delay distribution if the window size is too big relative to significant short-term structured variability in reporting performance (like those exhibited by UK COVID-19 data, as
illustrated later in Figure 5).
Finally, Stoner and Economou (2019) propose and assess a general framework for correcting delayed reporting, which utilises a Negative-Binomial model for y and a GeneralizedDirichlet-Multinomial (GDM) model for zd |y. Covariates and random effects can be included in the parameters of the GDM, to account for systematic variability in the mean
and variance of the reporting delay. The benefit of this approach is that all four sources
of variability are accounted for separately using flexible distributions, leading to enhanced
interpretability of the model design along with improved prediction performance when
nowcasting and forecasting (Stoner and Economou, 2019). In the following sections we will
detail how a spatio-temporal extension of this framework can be used to correct delayed
reporting in COVID-19 (Section 4) and other disease surveillance data (Appendix A).
The various approaches can be broadly classified in two groups: one where the delayed
counts zd are modelled marginally without explicitly modelling/using historical information
on the totals y, e.g. Bastos et al. (2019) and McGough et al. (2020); and another which
models the delay counts jointly but also conditionally on y, i.e. z|y, in conjunction with
a separate model for y, e.g. Stoner and Economou (2019) and Höhle and an der Heiden
(2014). We argue that the latter approach is better able to explicitly capture (a) and (b)
in the model for y, as well as (c) and (d) in the model for z|y, especially if the model is
sufficiently flexible to allow for overdispersion relative to the multinomial – like the GDM.

9

Emphasising that the predictand of interest is the total count y, we note that the GDM
framework implemented in the Bayesian framework produces the predictive distribution

p y (unseen) |y (obs) , z (obs) , thus utilising all available information. The ‘marginal’ approaches
P
predict y indirectly as d zd , thus needlessly predicting unseen zd ’s whose uncertainty will
propagate in the predicted y and potentially failing to appropriately capture the random
variability of y, due to the lack of an explicit model for historical y.
In Section 4.5 we apply models based on our GDM approach, Bastos et al. (2019) and
McGough et al. (2020) to COVID-19 death data from the UK, and make comparisons
based on nowcasting and forecasting performance, as well as interpretability. We compare
these three approaches as, in our opinion, they are the main three contenders (in terms of
flexibility and operational applicability) for operational COVID-19 delay correction.

3

Modelling framework

Extending the GDM framework in Stoner and Economou (2019) to include a spatial dimension s ∈ S (e.g. districts, regions, countries) results in the following mathematical
formulation of the model:
yt,s | λt,s , θs ∼ Negative-Binomial(λt,s , θs );
zt,s ∼ GDM(νt,s , φt,s , yt,s ).

log(λt,s ) = f (t, s);

(1)
(2)

Systematic spatio-temporal variability in the total counts yt,s is captured by the general
function f (t, s), which may include an offset (e.g. population), covariates or random effects.
Variability in the delay mechanism is modelled by the Generalized-Dirichlet-Multinomial
(GDM) distribution, a Multinomial mixture whose vector of probabilities has a Generalized10

Dirichlet distribution (Wong, 1998). The use of the GDM for modelling the partial counts,
instead of the more conventional Multinomial, affords a great deal of extra flexibility in
accounting for over-dispersion in the random variability of the reporting delay (d) – which
improves nowcasting efforts – and in capturing unusual covariance structures in the partial
counts (Stoner and Economou, 2019). Here we choose to parametrize the GDM in terms
of νt,s and φt,s . These are respectively the mean and dispersion parameters of the BetaBinomial conditional model for each partial count:
zt,s,d | zt,s,−d , yt,s ∼ Beta-Binomial(νt,s,d , φt,s,d , nt,s,d = yt,s −

X

zt,s,j ).

(3)

j<d

Parameter νt,s,d (the relative mean) is therefore the proportion of the yet-to-be unreported
part of yt,s which is expected to be reported at delay d. In Stoner and Economou (2019),
two options were suggested for modelling the relative means νt,s,d . In the first (named the
Hazard variant) they are modelled directly with a logit link, so that log (νt,s,d /(1 − νt,s,d )) =
g(t, s, d), for some general function g(t, s, d). In the second (the Survivor variant) a model
is first constructed for St,s,d , the expected cumulative proportion reported after delay d:
probit(St,s,d ) = g(t, s, d).

(4)

The relative means are then easily derived as νt,s,d = (St,s,d − St,s,d−1 )/(1 − St,s,d−1 ). Stoner
and Economou (2019) argue that it is more intuitive to consider models for the cumulative
proportion of yt,s reported by delay d, than to consider models for the expected proportion
of yt,s reported at delay d out of those not already reported by delay d − 1, and so advocate
adoption of the Survivor variant over the Hazard variant. The operational characteristics
and performance of both options have been studied extensively in Stoner and Economou
11

(2019). In the next subsection we discuss how general functions f (t, s) and g(t, s, d) may
be appropriately specified in a spatio-temporal context, specifically for the Survivor variant
of the GDM framework.

3.1

Spatio-temporal variation

In epidemiological applications like surveillance of COVID-19, we often seek to take into
account both structured and unstructured spatial variability in what we consider here to
be the total counts (i.e. COVID-19 cases or deaths). For example, COVID-19 cases and
deaths may be correlated across neighbouring regions due to population movement. We
may also need to consider how spatial and temporal variability might interact. For example,
a COVID-19 outbreak might be concentrated in a cluster of regions initially, but spread
to more regions as time progresses. With data plagued by reporting delays, we should
also account for spatio-temporal variability in the delay structure. For example, data from
some regions may be available more quickly than data from others, and structured temporal
variability in the delay mechanism may vary with space, e.g. due to changes over time in
surveillance resourcing within individual regions.
To motivate these efforts, it is worth considering the reasons for which joint modelling
of the data across regions is needed, as opposed to applying a time series model to each
region separately. One possible situation is a disease surveillance system where the sum of
cases/deaths over a number of regions is important, e.g. for planning resource allocation
(e.g. tests, ventilators) on a larger geographical scale. Even if the joint (i.e. spatiotemporal) and independent (i.e. individual time series) models perform equally well at
capturing the variance of the total counts in each region, the risk of not capturing similarities across multiple regions (e.g. in their temporal trends) is that the variance of any sum
12

V (S 0 ) = Var

P

s∈S 0

 P
P
yt,s = i∈S 0 j∈S 0 Cov[yt,i , yt,j ] for some S 0 ⊆ S, may not be captured

well – as we illustrate in Appendix A. A well-specified joint model is potentially able to
explicitly quantify the covariance of yt,s across regions (at least at the mean level which
may indeed be sufficient), so that V (S 0 ) is captured better. Meanwhile, for applications
with a high spatial resolution (e.g. local authorities), incorporating spatio-temporal structures may enable a better understanding of how the disease is spreading, to allow resources
to be allocated to areas which are likely to be affected in the near future. Finally, cases
where missing information arises from sources other than reporting delays, e.g. data loss
or national holidays, jointly modelling the regions is important so that regions with less
data can potentially borrow information from the others.
The selection of appropriate ways of capturing spatio-temporal variability is well-established
in the field of epidemiology. In disease data, space is often defined by areal units (e.g. regions, counties etc.) although sometimes data occur as points in space (e.g. counts of cases
at individual clinics or hospitals). In either case, a sensible starting point in defining the
model for the mean incidence rate, f (t, s), is to consider separable functions of the form:
f (t, s) = f1 (t) + f2 (s) + f3 (t, s).

(5)

Here f1 (t) allows for any common temporal or seasonal variation across the regions; f2 (s)
(which may include offsets such as population) allows for any overall differences in the
mean of yt,s between regions; and f3 (t, s) allows for spatio-temporal interactions.
For the purposes of this article we need not restrict ourselves to any specific models for
capturing spatio-temporal variability, among the many that are available (see for instance
Banerjee et al. (2014)), particularly as this choice is often application dependent. Instead
we will briefly give some illustrative examples and then discuss in more detail an approach
13

based on nested spline structures, in our applications to COVID-19 and SARI data.
Formulating the model for the expected cumulative proportion through g(t, s, d) is
slightly more complex due to the extra dimension, but can be decomposed in a similar
manner, noting that in practice, d is discrete and bounded from above. Here we consider
separable functions of the form:
g(t, s, d) = g1 (t) + g2 (s) + g3 (d) + g4 (t, s) + g5 (t, d) + g6 (s, d) + g7 (t, s, d).

(6)

This allows for spatio-temporal variability in the delay mechanism, e.g. by including tensor
product smoothing terms of time and delay which vary with space (Wood, 2006).
It is important to appreciate the need for structures involving interactions between
space, time and delay, and to consider whether these make sense within the context of the
application and the data. For instance, the delay mechanism may not, in some applications,
vary across space s where this is defined at a small spatial scale (e.g. individual clinics or
hospitals), but it may well vary across s at a larger scale (e.g. counties/regions). In the
application to COVID-19 mortality data in Section 4, we opt for choices of f (·) and g(·)
that are flexible yet practically feasible and appropriate to the application.

4

Application to COVID-19 deaths

The National Health Service for England (NHS England) publishes daily count data of
deaths occurring in hospitals in England of patients who had either tested positive for
COVID-19 or where COVID-19 was mentioned on their death certificate. Each daily file
presents deaths reported in the time period from 4pm 2 days prior to publication until 4pm
on the day before publication, grouped in time by date of death and in space by region (e.g.
14

South West England) but also higher resolutions like NHS Trusts. Assembling published
files for each day over a period of time then provides information on the reporting delay by
constructing, for each date of death and geographic region, the number of deaths reported
on each day. For more straight-forward statistical analysis and compatibility with existing
modelling approaches, the number of deaths reported on each day can be organised by the
number of days of delay relative to the date of death, taking the first delay (d = 1) to be
deaths which occurred and were reported within the latest reporting period.
As with other approaches (e.g. McGough et al. (2020)), the total counts must be assumed fully reported after a specified cut-off, which we denote Dmax . Resulting predictions
of y can then be interpreted as the number of cases/deaths which will be reported after
Dmax days from the actual day of death. If only a low proportion (e.g. < 50% of y tends
to be reported after Dmax , then nowcasts and forecasts will not offer a complete picture of
ongoing or upcoming outbreaks to decision-makers. If Dmax is needlessly high, then more
data on totals y will be unknown and thus require sampling during model fitting, increasing
the complexity of the model and potentially making the model impractical for real-time
(e.g. daily) prediction. Ideally, Dmax is chosen to be sufficiently high that on average most
of y (e.g. 90%) are reported. The choice of Dmax is therefore very application-dependent
but not daunting, because in many applications most of y is reported in the first few delays
(i.e. d < 10), with less and less reported afterwards. For this dataset, over 90% of all deaths
reported within 28 days – in the time period from 2nd of April 2020 to 24th of July 2020
– were reported within 7 days and over 95% were reported within 14 days. Here we opt
for Dmax = 14 days. If no value of Dmax is specified, then all total counts y are unknown
and the model is non-identifiable without additional information (e.g. informative prior
distributions), similar to the case of correcting under-reporting (Stoner et al., 019a).

15

4.1

Nested spline model

Stoner and Economou (2019) present a model for a time series of dengue fever data in Rio de
Janeiro, Brazil, where the incidence of the total recorded dengue counts is modelled by the
combination of an intercept term, a temporal effect and a seasonal effect: f (t) = ι + αt + ηt .
The temporal (αt ) and seasonal (ηt ) effects were defined using penalized cubic splines, and
set up using the jagam function from the mgcv package for the R programming language
(Wood, 2016). This was shown to be a very flexible model in capturing smooth temporal
and seasonal variation, so we also consider it here to describe the time series of COVID-19
deaths counts for any individual region, though dropping the seasonal component (as we
have only a few months of data). To capture spatio-temporal variability, we extend this to
include spatially-varying intercept and temporal effects:
f (t, s) = ιs + δt,s ,

(7)

with ιs assigned a non-informative Normal(0, 102 ) prior distribution and δt,s characterised as
(δ)

penalized cubic splines of time for each region, defined by δt,s = Xt κs . Here Xt is a model
(δ)

matrix of the basis functions evaluated at each time point, and κs is a vector of coefficients.
To penalize the splines for over-fitting, the coefficients are assigned a Multivariate-Normal
(δ)

(δ)

prior with mean zero and precision matrix Ωs = τs M (δ) . Matrix M (δ) is a known non(δ)

diagonal matrix, scaled by a smoothing (penalty) parameter τs
larger values of

(δ)
τs

(Wood, 2016), so that

result in a smoother δt,s for each s.

As the regions in the data are geographically very large, we are more concerned with
accounting for similarity in trends between regions – in both the fatality rate and in the
reporting delay over time – than explicitly modelling any space-time interactions. To

16

achieve this, we can re-introduce the temporal effect αt and make its (basis function)
coefficients the mean of the coefficients for the regional effects δt,s , i.e.
αt = Xt κ(α) ;

(8)

κ(α) ∼ Multivariate-Normal(0, Ω(α) = τ (α) M (α) );
κ(δ)
∼ Multivariate-Normal(κ(α) , Ω(δ)
s
s ).

(9)
(10)

The function αt therefore captures common temporal variation across all regions (and so
can be interpreted as the overall trend in the fatality rate for the whole of England), while
the δt,s capture regional deviations from these overall trends. The parameter τ (α) therefore
(δ)

penalizes the overall (England) effect for smoothness, while the τs penalize the smoothness
of the regional deviations from the overall effects. The main advantage of this structure
– which can be efficiently implemented using Markov Chain Monte Carlo (MCMC) owing
to the conjugate relationship between the Multivariate-Normal priors for the overall effects
and the regional deviations – is that αt can capture temporal covariation between regions,
an important feature for some applications as discussed in Section 3.1. This approach to
pooling information, while allowing for individual variability, was shown to be very effective
in modelling global trends in polluting cooking-fuel usage, so that countries with little data
could borrow information from regional trends (Stoner et al., 019b).
We adopt the same approach when extending the relatively simple (Survivor) model
used in Stoner and Economou (2019) for the expected cumulative proportion reported at
each delay, g(t, d) = ψd + βt , first to include spatial variability and second to account for

17

any weekly cycles (see Figure 2) in the reporting delay:
g(t, s, d) = ψs,d + γt,s + ξt,s .

(11)

Here fixed delay effects ψs,d capture the overall curve of the cumulative proportion reported
after each delay and are independent across regions. They are assigned non-informative
first order random walk prior distributions, i.e. ψs,d ∼ Normal(ψs,d−1 , 102 ), but truncated
such that ψs,d > ψs,d−1 (to respect the fact that the cumulative proportion should increase
with d). As for the model for f (t, s), temporal effects γt,s are penalized cubic splines centred
on an overall temporal trend βt (as in (8)-(10)). Finally, ξt,s are penalized cubic splines with
a cyclic (periodic) basis over the days of the week - to account for systematic variability
such as the ‘weekend-effect’ - centred around an overall weekly effect ηt .

4.2

Prior distributions and implementation

Prior distributions for other parameters were chosen to constrain the parameter space
to reasonable values (relative to the data) but without being overly informative: For the
Negative-Binomial dispersion parameters θs we specified independent Gamma(2,0.02) prior
distributions, where the 95% credible interval [12.1, 279] covers high levels of over-dispersion
(e.g. θs = 20), while more extreme levels (e.g. θs = 10) are less likely a-priori. We also
specified Gamma(2,0.02) priors for the Beta-Binomial dispersion parameters φs,d , following the same reasoning. Finally, it can be more interpretable to parametrize the spline
√ (δ)
(δ)
(δ)
precision penalties (e.g. τs ) as standard-deviation penalties (i.e. σs = 1/ τ s ), so that
(δ)

smaller values for σs

correspond to a stricter penalty. For these we specified positive

Half-Normal(0,1) prior distributions, meaning smoother functions are more likely a-priori.

18

As discussed in Stoner and Economou (2019), instead of explicitly modelling all available
partial counts zt,s,d , we can reduce computational complexity by choosing to only explicitly
model counts for d ≤ D0 ≤ Dmax . We achieve this by only including the conditional BetaP
Binomial models for zt,s,d up to D0 , so that the remainder rt,s = yt,s − D
d=D0 +1 zt,s,d is
modelled implicitly. The trade-off associated with this choice is that predictive precision
for yt,s,d is reduced, but generally only for past weeks t ≤ t0 −D0 . Hence selecting a small D0
may be considered pragmatic where optimally precise predictions are not needed far into
the past. In this experiment we opt for D0 = 6, which we consider sensible in a situation
where optimally precise predictions are not needed for six weeks or more into the past.
All code was written and executed in the R programming language (R Core Team, 2019).
The model was implemented using the nimble package (de Valpine et al., 2017), which
facilities highly flexible implementation of Bayesian models using MCMC. Four MCMC
chains were run from different randomly generated initial values and with different random
number generator seeds. We ran the chains for 200k iterations, discarding 100k as burn-in
and then thinning by 10. Convergence of the MCMC chains was assessed by computing
the potential scale reduction factor (PSRF) (Brooks and Gelman, 1998) for samples of
each λt,s , and of each θs (i.e. the parameters associated with nowcasting and forecasting).
By convention, starting multiple chains from different initial values, with different random
number generator seeds, and obtaining a PSRF close to or less than 1.05 for a given
parameter is taken to indicate convergence. Here, all of the PSRFs for the θs were less
than 1.05, as well as virtually all of the λt,s (>93%). The model was not fitted to the
counts for the whole of England, but predictions for England are achieved by summation
of Monte Carlo samples from each region’s predictive distribution.

19

4.3

Results for the 4th of May 2020

To illustrate our approach as a tool for real-time decision-making, we first look at estimates
and predictions from the model imagining we are fitting it at the end of the 4th of May 2020,
with only data that would have been available then. In the following subsection, we then
present a rolling prediction experiment to assess nowcasting and forecasting performance
when the model is employed systematically over a longer period of time.
Temporal Trends
Daily Death Rate

Cumulative Proportion Reported

1.0
0.25
0.5
0.0
−0.5
−1.0

Apr 01

0.00
East of England
London
Midlands
North East and Yorkshire
North West
South East
South West
Apr 15

−0.25
−0.50

May 01

Apr 01

Apr 15

May 01

Figure 3: Posterior median effects of time on the daily COVID-19 fatality rate (left) and
the cumulative proportion reported (right), for each region. The dashed lines show the
overall effects for England.
The left panel of Figure 3 shows the posterior median temporal splines δt,s in the mean
fatality rate λt,s . The dashed line shows the overall effect for England, αt . All regions
show a peak around the first week of April, before decreasing at varying rates. Meanwhile,
the right panel of Figure 3 shows the posterior median temporal splines in the probit
model for the cumulative proportion reported. Most regions (and indeed the overall effect
for England) show an increase in the first two weeks of April, after which they are fairly
constant, equating to a general speeding up of reporting in the East of England, London,
20

the Midlands, the North West and the South East.

Cumulative Proportion Reported

Mean Delay Distribution

75%

East of England
London
Midlands
North East and Yorkshire
North West
South East
South West

50%
25%

2

4

Figure 4: Posterior median expected cumulative proportion of deaths reported after each
day of delay, for each region.

6

Delay (days)

Similarly, Figure 4 shows the posterior median of the overall mean delay distribution, as
quantified by effects ψt,d and plotted in terms of the cumulative proportion reported. We
can interpret this plot as the expected delay distribution when temporal (γt,s ) and weekly
(ξt,s ) effects are equal to zero. Recall from Section 4.1 that these effects are estimated
independently for each region. With that in mind, the delay curves are remarkably similar
across regions, with around a quarter of deaths reported within the first reporting period
and with most deaths reported within 2 days of occurrence.
Combining all of the effects in Figures 3 and 4 along with the weekly cycle (ξt,s ), Figure
5 shows the posterior median expected proportion reported within 2 days (St,s,2 ) depending
on which day of the week they occurred. Here, the temporal effects γt,s have been frozen at
their values on the 8th of April (left) and the 22nd of April (right). In both panels, we can
see clear weekly cycles for most regions (the East of England, London, the Midlands, the
North West, and the South East), where a noticeably lower proportion of deaths occurring
towards the end of the week are reported within 2 days.
Looking across panels, the same regions exhibiting clear weekly cycles demonstrated a
21

Proportion of deaths reported within 2 days of occurrence
Weekly cycle on 8th April

Weekly cycle on 22nd April

80.0%

East of England
London

70.0%

Midlands
60.0%

North East and Yorkshire
North West

50.0%

South East
South West

40.0%

Mon

Tue

Wed

Thu

Fri

Sat

Sun Mon

Tue

Wed

Thu

Fri

Sat

Sun

Figure 5: Posterior median (with 95% credible intervals) of the expected proportion of
COVID-19 deaths reported within 2 days of occurrence (St,s,2 ), for each region and as a
function of which day of the week they occurred. The temporal effects are fixed at their
values for the 8th of April 2020 (left panel) and the 22nd of April 2020 (right panel).
significant increase in the proportion reported within 2 days, from around 50% to around
70%, in just 2 weeks. This substantial change over a short period of just 14 days may not be
captured well by approaches relying solely on a moving-window to account for systematic
temporal variability in the delay distribution (i.e. McGough et al. (2020)), which has
implications for nowcasting and forecasting accuracy, as we investigate in Section 4.5. The
remaining two regions, the North East and Yorkshire and the South West, had consistently
high reporting rates throughout the week and throughout the time period of study.
Finally, Figure 6 shows nowcasting and forecasting predictions based on data available
up to the 5th of May 2020. With hindsight, we can compare predictions to the now
fully reported counts to assess performance, plotted as points. Generally the nowcasting
predictions are good; forecasted trends are convincing, and uncertainty appears reasonable
for decision-making purposes. To more thoroughly assess this, in the following subsection

22

Predicted Daily Hospital Deaths from COVID−19
East of England

London

Midlands

200

100

150

90
150
60

80
100

100

60

50

50

30

40
20

0
06 Apr

20 Apr

04 May

06 Apr

North West

20 Apr

04 May

06 Apr

South East

20 Apr

04 May

06 Apr

South West
40

50

20 Apr

04 May

England
800

75

100

North East and Yorkshire

30

600

20

400

50
25
06 Apr

20 Apr

04 May

10
06 Apr

20 Apr

0

04 May

200
06 Apr

20 Apr

04 May

06 Apr

20 Apr

04 May

Date of Death

Figure 6: Posterior median nowcasting and forecasting predictions of the total daily deaths
yt,s (lines) with up to 95% prediction intervals (shaded areas) for each region, using only
data available on the 5th of May 2020 (vertical lines). Points show the total daily deaths
reported within 14 days of occurrence (only available with hindsight).
we present a rolling prediction experiment.

4.4

Operational characteristics

To quantitatively assess nowcasting and forecasting performance when the model is systematically employed over a period of time, we carried out a rolling prediction experiment
which emulates use of the model every day for 20 days. The experiment proceeds as follows:
Step 1: Select a present-day t0 .
Step 2: Hold back all partial counts which would be unavailable at t = t0 .

23

Step 3: Fit the model, predicting any partially observed and completely unobserved deaths.
Step 4: Set t0 = t0 + 1 and repeat steps 2-4.
The prediction experiment starts on the 15th of April 2020, and ends on the 4th of
May 2020, totalling 20 model runs. We then arrange predictions by the difference, in days,
between the date the prediction is made for and the emulated date of fitting. For example,
a difference of 0 days corresponds to all the predictions made for the same day the model
was fitted. Moreover, a negative difference corresponds to a prediction made for a day in
the past relative to the model fit (i.e. the count is at least partly observed) and a positive
difference corresponds to forecasting. Figure 7 presents posterior median predictions of the
total daily deaths (with 95% prediction intervals) arranged in this manner. Also plotted
are the 95% prediction interval coverage values (the proportion of observations that lie
within the 95% prediction intervals).
If the model is working as intended, we should see more accurate and less uncertain
predictions for negative differences, because the counts have been more completely observed
the further one predicts into the past and vice versa for forecasting. Meanwhile, we would
hope to see high coverage values (90+%) regardless of when we are making predictions for.
First, looking at Figure 7 we do not see any evidence of systematic biases in the predictions
(e.g. over- or under-predicting overall or for any particular region). The coverage values do
appear to be a bit too low (meaning the model is over-confident) when making predictions
a few days into the past, but the accuracy is so high that we doubt this would pose any
practical problems. When nowcasting (i.e. a difference of 0 or perhaps -1 days) and when
forecasting, the coverage is high, while the uncertainty is not unreasonable even when
forecasting almost a week ahead in a fast-changing epidemic.

24

Predictions of Hospital Deaths from COVID−19 in England
by Time from Day Model Fitted
−3 Days

−2 Days

−1 Days

0 Days

1 Days

1000

0.84

0.91

0.94

0.95

0.95

Predicted Deaths

100

East of England
London

10

Midlands

2 Days

3 Days

4 Days

5 Days

6 Days

1000

0.95

0.93

0.93

0.93

North East and Yorkshire
North West
South East

0.94

South West

100

England
10
10

100 1000

10

100 1000

10

100 1000

10

100 1000

10

100 1000

Observed Deaths

Figure 7: Posterior median predictions (with 95% prediction intervals) of daily hospital
deaths in England, from the rolling prediction experiment, arranged by the time from the
model fit day. Also shown are 95% prediction interval coverage values.

4.5

Comparison with competing approaches

We have made a number of theoretical arguments in favour of the GDM over competing
approaches (e.g. Bastos et al. (2019), McGough et al. (2020)), particularly the separation
of the four main sources of variability in disease surveillance data suffering from delayed
reporting and the flexibility of the GDM as a model for random variability in the delay.
However, it remains to be seen if these arguments translate into meaningful improvements
in nowcasting performance when applied to real COVID-19 data in an operational setting.
To investigate this, we seek to quantitatively compare 5 competing models – based on the
rolling prediction experiment (detailed in Section 4.4), focussing on same-day nowcasting
predictions – defined as predictions for the present day in a scenario where the model is
fitted each day using the latest available data. Full details for each competitor model are
25

provided in Appendix B, but they can be summarised as follows:
1) GDM The full GDM as described in Section 4.1.
2) NB A Negative-Binomial approximation to the marginal model of the GDM for the
delayed counts z, where the models for systematic variability in the total count and
in the delay are appropriately separated and are identical to those in Section 4.1.
3) INLA A “direct” Negative-Binomial model for the delayed counts z, based on the
framework proposed by Bastos et al. (2019) and where the seasonal component is
replaced by a weekly cycle to capture systematic weekly variability in the delay.
4) NobBS A model for the delayed counts z based on the framework proposed in McGough et al. (2020) and implemented using the NobBS package for R.
5) NobBS-14 A second model based on McGough et al. (2020), where a moving window
of 14 days is specified to capture systematic temporal variation in the reporting delay.
Including a ‘marginal’ version our GDM model (NB) based on the same systematic model
for mean total count (deaths) and the same ‘Survivor’ model for the expected cumulative
proportion reported at each day should illuminate to what degree differences between the
GDM and the other models (i.e. INLA and NobBS) are attributable solely to the use
of the full GDM conditional model to appropriately capture variability in the reporting
delay. So that the comparison can focus primarily on the performance of each modelling
framework, rather than any specific spatio-temporal structures, all models are implemented
as independent time series models for each of the 7 regions.
To quantify differences in nowcasting performance, we calculate 4 metrics for each
model. The first is the root-mean squared error of the posterior median predicted number
26

√

Region
East of England
London
Midlands
N.E. and Yorks.
North West
South East
South West
Overall

East of England
London
Midlands
N.E. and Yorks.
North West
South East
South West
Overall

GDM
10
18
13
8
17
13
6
13
Mean
GDM
46
59
62
44
58
45
21
48

Mean Squared Error
NB INLA NobBS NobBS-14
9
14
15
14
17
28
32
30
12
17
20
21
8
8
7
8
16
18
19
19
12
16
20
18
5
5
5
5
12
17
19
18
95% Prediction Interval Width
NB INLA NobBS NobBS-14
52
67
69
62
61
99
108
88
79
123
133
105
53
61
56
60
77
100
102
81
60
78
87
70
22
27
25
24
58
79
83
70

GDM
3
7
2
1
6
-1
-1
2
95%
GDM
1
0.9
1
1
0.9
0.9
1
0.96

Bias
NB
INLA NobBS NobBS-14
3
10
13
11
7
18
25
20
2
10
15
13
2
5
5
5
6
11
14
11
2
11
16
12
0
1
2
1
3
10
13
10
Prediction Interval Coverage
NB
INLA NobBS NobBS-14
1
1
0.95
0.95
0.95 0.95
0.95
0.95
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0.95
1
0.95
0.95
0.95
0.99 0.99
0.98
0.97

Table 1: Nowcasting performance of competing models when applied to COVID-19 data.
of deaths on each day, which quantifies how accurate point estimates are. The second is the
bias, defined as the mean difference between the median predicted deaths and the observed
deaths, which quantifies any systematic over- or under-prediction when nowcasting. The
third is the mean 95% prediction interval width for the total number of deaths on each day,
which quantifies how precise/uncertain predictions are. The fourth and final metric is the
95% prediction interval coverage, which checks whether uncertainty is reliably quantified
by the model. In particular, coverage values much less than 0.95 might suggest too few
data points are captured by the 95% intervals and the model is over-confident. Conversely,
higher coverage values could suggest the predictions display excessive uncertainty.
Table 1 presents these metrics separately for each region and overall. Note that the
rolling prediction experiment covers 20 days, therefore resulting in 20 same-day predictions
27

Same−Day Nowcasts of COVID−19 Deaths
Observed Deaths

GDM

NB

INLA

NobBS

NobBS−14

150
100
50
0

0

50

100

150

0

50

100

150

0

50

100

150

0

50

100

150

0

50

100

150

Predicted Deaths

Figure 8: Scatter plots of posterior median predicted total deaths (across all regions) versus
observed total deaths, from each competing model.
for each region. This means a difference in a regional coverage value of 0.05 equates to one
data point being captured or not captured. In every region predictions from the GDM were
the most precise (as quantified by lower prediction interval widths), while point estimates
from the GDM and the marginal NB (which share the same systematic model) were the
most accurate in 5 out of 7 regions and equally accurate in the remaining 2. Predictions
from the GDM and the marginal NB did not display the persistent over-prediction seen in
the INLA and NobBS approaches – illustrated in Figure 8 which compares posterior median
predicted deaths with observed deaths – which we believe is likely due to the use of splines
to capture the downward trends in the total deaths rather than the first order random walk
(RW1) models in the INLA and NobBS models. Overall, the GDM outperformed the INLA
(based on Bastos et al. (2019)) and NobBS models (based on McGough et al. (2020)) by
wide margins across the board. All models have overall coverage values above 0.95, with
the GDM offering the closest at 0.96.
Figure 9 shows 95% prediction interval widths as a time series for each day. Here,
consistent leads in the precision of nowcasting predictions from the GDM compared to
competing frameworks (INLA, NobBS) are stark, particularly earlier on in the pandemic.
28

Same−Day Nowcasts of COVID−19 Deaths
East of England

London

Midlands

160

200

95% Prediction Interval Width

100
120

80
60

80

40

40

North West

North East and Yorkshire
80
70

150

60
100

50
40

50

South East

30
Apr 20

South West

150

40

INLA
30

90

75

60

50

30

25
Apr 20

Apr 27

May 04

May 04

GDM

100

120

Apr 27

NB
NobBS

20

NobBS−14
Apr 20

Apr 27

May 04

Apr 20

Apr 27

May 04

Figure 9: Time series of central 95% prediction interval widths for same-day nowcasts from
the 5 competing models, for each region.
Meanwhile, the marginal (NB) approximation to our GDM model also outperforms a) the
INLA model, likely due to the appropriate separation of systematic variability in the total
count and in the delay; and b) the NobBS models, probably due to the inclusion of a
weekly cycle and due to splines being a more flexible way of capturing systematic temporal
variation in the delay compared to a fixed moving window. However, the importance of
appropriately quantifying delay variability using a flexible model like the GDM is illustrated
by the non-trivial difference between the marginal model (NB) and the GDM – noting that
the marginal (NB) model still relies on a full MCMC implementation due to the nonlinearity of explicitly modelling the cumulative proportion reported at each delay.
All code and data required to reproduce these results are provided as supplementary
material.

29

5

Discussion

The COVID-19 pandemic has highlighted the need to optimally correct delays in disease
data, for timely mitigation actions. Here, we have critically reviewed the three mainstream
approaches to correcting delays, and quantified their respective performance when applied
to COVID-19 mortality data. We have argued that our multivariate approach based on the
Generalized-Dirichlet-Multinomial is theoretically the most advanced in explicitly capturing
the different sources of variability in the data. Furthermore, of the three it is the only
one which readily provides predictions of total counts y conditional upon all available
data, i.e. historic y and partial counts z. Indeed, when applying the GDM and four
other competing models representing the current best-practice in addressing the delayed
reporting to the COVID-19 deaths data, the GDM approach is by far the most capable
in terms of nowcasting accuracy, bias, and precision – in a realistic operational scenario.
Compared to competitors, the GDM exhibited around a 25-30% smaller root-mean squared
error and around a 30-40% smaller mean 95% prediction interval width, whilst still offering
95% prediction interval coverage values in the region of 0.95. This certainly establishes the
GDM as the current best-practice in nowcasting COVID-19 and other diseases, while also
allowing for reliable forecasting.
To account for spatio-temporal variability, we have introduced a novel spatial extension
to the GDM framework – allowing for a wide variety of spatial, temporal, and spatiotemporal structures to be included in both the model for the total reported counts after
any delays have passed, and in the model for the delay mechanism itself. Within this
we have demonstrated one specific model based on nested spline structures. Though the
spatio-temporal models presented here for COVID-19 and SARI data are quite intuitive
and demonstrably better than independent time series models when aggregating predictions
30

to a super-regional level (as illustrated in Appendix A.3), they may be overly simplistic:
Firstly, while our nested spline approach enabled the model to capture the distribution of
the total counts across all regions well, both the model for the total deaths/cases and the
delay model lack any explicit spatial structure (i.e. the model assumes equivalent similarity
between all regions). A more sophisticated approach to space-time interactions may be
needed for applications with a fine spatial resolution, e.g. to potentially capture the spread
of a disease over time. Furthermore, in cases where some regions have a lot of missing
data, models with explicit spatial structure may allow for more precise predictions in those
regions. A lack of data may also warrant including spatial structure in the dispersion
parameters, which is possible by modelling them as log-linear (Stoner and Economou,
2019). Applications intended for operational use might also benefit from considering more
complicated mean delay models with delay-time interactions, which are of course possible
within the framework proposed here, e.g. using tensor product smooths (Wood, 2006).
The model we presented for correcting delayed of COVID-19 deaths takes around 10-20
minutes to compile and run (as implemented in Section 4.2). We believe this is reasonable
in a daily operational setting, allowing for potential errors and any need to run the MCMC
for more iterations for convergence. Indeed, a model based on Stoner and Economou (2019)
for nowcasting daily COVID-19 deaths by age and region in England (Seaman et al., 2020)
has been implemented operationally and currently feeds into the UK Scientific Pandemic
Influenza Group on Modelling (SPI-M) on a weekly basis (MRC Biostatistics Unit, 2020).
However, the greater dimensions of the SARI data (more time points and spatial regions)
results in a run-time in the region of 12 hours. Seaman et al. (2020) improved on our
implementation of the GDM by not sampling any missing partial counts z, leading to better
mixing of MCMC chains, which would go some way in reducing overall run-times. However,

31

for applications with even greater dimensions (e.g. COVID-19 data at the level of individual
cities, hospitals, or trusts), there is a clear need for either a method of implementing the
GDM in a more efficient manner or a new framework altogether which offers comparable
predictive performance to the GDM and improved computational feasibility.
Using the framework presented here it is possible to comprehensively address the problem of delayed reporting in COVID-19 and other disease surveillance applications. This
ignores, however, the possible problem of substantial under-reporting in the final reported
counts, e.g. due to a lack of testing capacity, as described in Appendix C. Therefore
any predictions from models discussed here, e.g. of disease cases, may lead to an undersized response to the true magnitude of an outbreak. Although we have explained how
under-reporting can be taken into account within the modelling framework we propose,
this approach has so far only been applied in a purely hypothetical scenario (see Stoner
(2019)). Further research should therefore be directed at applications where effective disease surveillance is inhibited by both delayed reporting and under-reporting.

SUPPLEMENTARY MATERIAL
All supplementary files are contained within an archive which is available to download as
a single file.
COVID Master: Master script for reproducing the COVID-19 application (R script).
COVID Data: Data for the COVID-19 application (Excel spreadsheet).
SARI Master: Master script for reproducing the SARI application (R script).
SARI Data: File containing data for the SARI application (RData file).
32

References
Banerjee, S., B. P. Carlin, and A. E. Gelfand (2014). Hierarchical Modeling and Analysis
for Spatial Data. CRC Press.
Bastos, L. S., T. Economou, M. F. C. Gomes, D. A. M. Villela, F. C. Coelho, O. G. Cruz,
O. Stoner, T. Bailey, and C. T. Codeço (2019). A modelling approach for correcting
reporting delays in disease surveillance data. Statistics in Medicine 38(22), 4363–4377.
Brooks, S. P. and A. Gelman (1998). General methods for monitoring convergence of
iterative simulations. Journal of Computational and Graphical Statistics 7(4), 434–455.
Cabrera, M. and G. Taylor (2019). Modelling spatio-temporal data of dengue fever using
generalized additive mixed models. Spatial and Spatio-temporal Epidemiology 28, 1 –
13.
Codeço, C. T., J. d. S. Cordeiro, A. W. d. S. Lima, R. A. Colpo, O. G. Cruz, F. C. Coelho,
P. M. Luz, C. J. Struchiner, and F. R. d. Barros (2012). The epidemic wave of influenza
A (H1N1) in Brazil, 2009. Cadernos de saúde pública 28(7), 1325–1336.
de Valpine, P., D. Turek, C. J. Paciorek, C. Anderson-Bergman, D. T. Lang, and R. Bodik
(2017). Programming with models: Writing statistical algorithms for general model
structures with NIMBLE. Journal of Computational and Graphical Statistics 26(2),
403–413.
Gelman, A., J. Carlin, H. Stern, D. Dunson, A. Vehtari, and D. Rubin (2014, November).
Bayesian Data Analysis, Third Edition (Chapman and Hall/CRC Texts in Statistical
Science) (Third ed.). London: Chapman and Hall/CRC.
33

Höhle, M. and M. an der Heiden (2014, 6). Bayesian nowcasting during the STEC O104:H4
outbreak in Germany, 2011. Biometrics 70(4), 993–1002.
Lindgren, F. and H. Rue (2015). Bayesian spatial modelling with r-inla. Journal of
Statistical Software, Articles 63(19), 1–25.
Mack, T. (1993). Distribution-free calculation of the standard error of chain ladder reserve
estimates. ASTIN Bulletin 23(2), 213–225.
McGough, S. F., M. A. Johansson, M. Lipsitch, and N. A. Menzies (2020, 04). Nowcasting
by bayesian smoothing: A flexible, generalizable model for real-time epidemic tracking.
PLOS Computational Biology 16(4), 1–20.
MRC

Biostatistics

COVID-19.

Unit

(2020,

Nov).

Nowcasting

and

forecasting

of

https://www.mrc-bsu.cam.ac.uk/tackling-covid-19/

nowcasting-and-forecasting-of-covid-19/. Accessed: 2020-11-02.
Plummer, M. (2003). Jags: A program for analysis of bayesian graphical models using
gibbs sampling.
R Core Team (2019). R: A Language and Environment for Statistical Computing. Vienna,
Austria: R Foundation for Statistical Computing.
Rotejanaprasert, C., N. Ekapirat, D. Areechokchai, and R. Maude (2020, 12). Bayesian
spatiotemporal modeling with sliding windows to correct reporting delays for real-time
dengue surveillance in thailand. International Journal of Health Geographics 19.
Salmon, M., D. Schumacher, K. Stark, and M. Höhle (2015). Bayesian outbreak detection
in the presence of reporting delays. Biometrical Journal 57(6), 1051–1067.
34

Seaman, S., P. Samartsidis, M. Kall, and D. De Angelis (2020). Nowcasting CoVID-19
deaths in england by age and region. medRxiv.
Stoner, O. (2019).

Bayesian Hierarchical Modelling Frameworks for Flawed Data in

Environment and Health. Ph. D. thesis, University of Exeter.
Stoner, O. and T. Economou (2019). Multivariate hierarchical frameworks for modelling
delayed reporting in count data. Biometrics.
Stoner, O., T. Economou, and G. Drummond Marques da Silva (2019a). A hierarchical framework for correcting under-reporting in count data. Journal of the American
Statistical Association.
Stoner, O., G. Shaddick, T. Economou, S. Gumy, J. Lewis, I. Lucio, G. Ruggeri, and
H. Adair-Rohani (2019b). Global household energy model: A multivariate hierarchical
approach to estimating trends in the use of polluting and clean fuels for cooking.
Wong, T.-T. (1998). Generalized Dirichlet distribution in Bayesian analysis. Applied
Mathematics and Computation 97(2), 165 – 181.
Wood, S. (2016). Just another Gibbs additive modeler: Interfacing jags and mgcv. Journal
of Statistical Software, Articles 75(7), 1–15.
Wood, S. N. (2006). Low-rank scale-invariant tensor product smooths for generalized additive mixed models. Biometrics 62(4), 1025–1036.
World Health Oganization (2014).

WHO surveillance case definitions for ILI

and SARI. https://www.who.int/influenza/surveillance_monitoring/ili_sari_
surveillance_case_definition/en/. Accessed 2019-09-25.
35

A

Severe Acute Respiratory Infection Data

The World Health Organization defines a severe acute respiratory infection (SARI) as an
acute respiratory infection where the patient suffers from both a fever measured above
38◦ C and coughing, where hospitalization is necessary and where the onset of the infection
was within the last 10 days (World Health Oganization, 2014). One reason for this classification is to standardise surveillance of influenza-like illnesses, so that seasonal patterns
in respiratory virus circulation can be studied and to inform prevention policies (Bastos
et al., 2019). As explained in Bastos et al. (2019), lags in data assimilation (from hospitals
to local authorities, then to state and national levels) introduce delays in the information
on SARI available to public health decision-makers, potentially inhibiting response to influenza outbreaks. In this section we apply our proposed framework to SARI data from
Brazil, to illustrate the generality of our approach beyond COVID-19, as well as to compare
a joint (spatio-temporal) model to independent time series models.

A.1

Data

We use data from the Brazilian state of Paraná, which was severely affected by the 2009
H1N1 epidemic compared to other states (Codeço et al., 2012) and continues to have one
of the highest rates of SARI incidence (Bastos et al., 2019). Here we consider a much
longer time period of 230 weeks (from the start of January 2013 to the end of May 2017),
compared to 66 weeks in Bastos et al. (2019), to enable us to draw meaningful conclusions
about seasonal variation. The state is divided into s = {1, . . . , 22} health regions and
we consider the total count to be fully observed 6 months after occurrence (Dmax = 27).
The dimension of the total counts yt,s is therefore 230x22 and the dimension of the partial

36

counts zt,s,d is 230x22x27 (corresponding to over 100k observations). For this application,
we imagine that the present-day week, t0 , is week 224 (mid April 2017). We then seek to
make predictions for t0 = 224, for previous weeks where the total count is still partially
unobserved (t = {t0 − Dmax + 2, . . . , t0 − 1} = {199, . . . , 223}) and for the next 6 weeks
(t = {t0 + 1, . . . , t0 + 6} = {225, . . . , 230}).
Recorded Severe Acute Respiratory Infection (SARI) Cases
Weekly Total per 100,000 People, State of Paraná, Brazil
6

4

2

0
2013

2014

2015

2016

2017

Figure 10: Area plot of total recorded SARI cases per 100,000 people, with a different
colour for each of the 22 health regions. The vertical line shows the present week for this
experiment t0 = 224.
Figure 10 shows weekly total recorded SARI cases per 100,000 people by region. The
plot shows a clear seasonal cycle across all regions, with outbreaks reaching their peak in
April-July, as well as considerable year-to-year variability. There is also some evidence of
regional variation in the overall rate – for example, the brightest green region tends to have
quite a low rate of cases per 100,000 people, compared to some other regions – as well as
regional variation in the seasonal timing of outbreaks. At “present day” t0 = 224, shown by
the vertical line, we are in the early stages of the annual influenza outbreak, so forecasting
predictions should ideally show an increasing trend in the number of SARI cases.

37

A.2

Model for SARI cases

In Section 4.1, we used a nested spline structure to add a spatial dimension to the time
series model for dengue fever cases presented in Stoner and Economou (2019). To model the
incidence of SARI cases we adopt a near identical approach, making use of spatially-varying
intercept, temporal and seasonal effects:
f (t, s) = ιs + δt,s + ξt,s ;

(12)

Effects δt,s and ξt,s are temporal and seasonal (cyclic) splines, respectively, which vary by
region. Figure 10, however, suggests that a large portion of temporal and seasonal variation
may be common across all regions. Once again, we can introduce temporal and seasonal
effects αt and ηt , and make their (basis function) coefficients the mean of the coefficients for
the regional effects δt,s and ξt,s . Similarly, the model for the expected cumulative proportion
reported after each delay is characterised by the addition of fixed delay effects ψs,d which
are independent across regions and temporal spline effects γt,s :
g(t, s, d) = ψs,d + γt,s .

(13)

Prior distributions and implementation are the equivalent to the COVID-19 model, detailed
in Section 4.2.

A.3

Results

Figure 11 shows median predicted temporal (δt,s , left) and seasonal (ξt,s , centre) effects
on SARI incidence, as well as the temporal effect on the cumulative proportion reported

38

(γt,s , right). A different colour is used for each region and the dashed black lines show the
median predicted overall effects, αt , ηt and βt , respectively. The estimated effects on SARI
incidence follow the overall trends quite closely, with only a few deviating substantially.
For example, there are noticeable increases in the temporal effect on SARI incidence for
almost all regions around mid 2013 and around mid 2016, corresponding to the two largest
outbreaks seen in Figure 10. Similarly, all the seasonal effects reflect the increase in SARI
incidence leading up to Brazil’s winter seen in Figure 10. The effects on the cumulative
proportion reported are substantially more variable, suggesting that the delay mechanism
may be driven more by local factors compared to SARI incidence.
Temporal Effect

Seasonal Effect

Temporal Effect

on SARI Incidence

on SARI Incidence

on Cumulative Proportion Reported

2
2
2

1
1
0

0

−2

2013

2014

2015

2016

2017

0

−1

−1

−2

−2
Jan

Mar

May

Jul

Sep

Nov

2013

2014

2015

2016

2017

Figure 11: Median predicted temporal and seasonal effects on SARI incidence (left and
centre) and median predicted temporal effect on the cumulative proportion reported (right).
Summarising, the 22 health regions of Paraná have a lot in common, in terms of temporal
and seasonal variation in SARI incidence. It is worth examining, however, whether anything
tangible was gained from modelling the regions simultaneously as opposed to using 22
independent time series models. In Section 3.1, we argued that modelling the regions
independently could impede the model’s ability to capture the variance of the total number
of reported cases across all regions. To assess this, we applied 22 independent models where
f (t) = ι + δt + ξt and g(t, d) = ψd + γt for each region. We then used posterior predictive
39

checking (Gelman et al., 2014) to see which approach captures the variance of the total
better.
Replicates of Total Reported SARI Cases
Across All 22 Health Regions

Sample Mean

Independent
Joint

Figure 12: Posterior replicates of the sample mean (left) and sample variance (right)
of fully observed (t ≤ t0 − Dmax + 1) total
reported number of SARI cases across all
22 health regions. Vertical lines show the
corresponding observed statistics.

Sample Variance
0.0015

0.3

Density

Model

0.2

0.0010

0.1

0.0005

0.0
62.5

65.0

67.5

70.0

0.0000
3000 3500 4000 4500 5000

Figure 12 shows posterior replicates of the sample mean and sample variance of fully
observed (t ≤ t0 −Dmax +1) total reported number of SARI cases across all 22 health regions
(i.e. the total for the whole state). Both models perform equally well at capturing the mean
number of cases, but notably the joint model is overwhelmingly better at capturing the
sample variance compared to the independent models – the observed sample variance is
firmly in the centre of the replicate distribution from the joint model. This suggests that
the joint model is more appropriate in applications where predictions are also important
at a larger geographical scale.
Finally, we can examine the model’s ability to nowcast and forecast in this application.
Figure 13 shows predicted total reported SARI cases in the three most populous regions
of Paraná: Curitiba (left), Londrina (centre) and Maringá (right). Among these three
regions, the forecasting predictions when fitted at present week t0 = 224 appear most
precise for Curitiba and Londrina, while somewhat over-predicting the number of cases in
Maringá. This over-prediction is common across many of the regions, likely owing to the
reduced magnitude of the outbreak compared to the previous year, which the model may
detect when fitted further into the outbreak. That said, virtually all of the observations
40

are within the 95% prediction intervals, with precise nowcasting predictions for the present
week (shown by the vertical line), which are all within the 50% prediction intervals.
For this experiment, the overall 95% prediction interval coverage was 0.99 for predictions
of the total reported count corresponding to previous weeks (t < t0 ), 1 when forecasting
(t > t0 ) and 1 when nowcasting (t = t0 ).
Predicted Weekly Total SARI Cases
Curitiba

Londrina

100

Maringá

●

50

●

40

75
●
●

50

30
●●

●
●

●
●

●

●
●

●

●●

20

●
●

60
●
●

●

●

●

●

●

●

●
●
●

40

●

●

●

●
●
●●
●
●
●
●
●
●● ●●
●
●●●●
● ● ●
●
●
●
● ● ● ● ● ● ● ●● ●
●
●
●
●●●● ● ● ● ●
●
● ●● ● ● ● ●
● ●
●
●●
●
●
● ●●
●●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ● ● ●
●
● ●●
●
●
● ●
●● ●●
● ● ●●
●●● ●
●
●
● ●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●
● ● ●●●● ●●●●●
● ●●●
● ●●●●●
●
●●●●●●●
● ●● ● ● ● ●
●
●
●
●

●

●
25
●

20

10
0

0

Dec

Feb

Apr

Jun

●
●

●
●

●●
●

●

●
●

0

Dec

Feb

Apr

Jun

Dec

Feb

Apr

Jun

Figure 13: Predicted (median, 50%, 65%, 80%, and 95% prediction intervals) total reported
SARI cases for the three most populous health regions. The vertical lines show the present
week t0 = 224.
All code and data required to reproduce these results are provided as supplementary
material.

B

Details of Competing Models for COVID-19 Deaths

In Section 4.5 we compared the GDM against four competing models for COVID-19 deaths
in a rolling nowcasting experiment. Here we provide details on the specification and implementation of each of these models.

41

B.1

Marginal Negative-Binomial Model (NB)

The first competing model is a Negative-Binomial model intended to approximate the (asyet) unknown marginal model for the delayed counts zt,s,d obtained from the GDM when
integrating out the total counts yt,s . The key feature of this model is that the systematic
models for the total count and the reporting delay are otherwise identical to those presented
in Section 4.1:
zt,s,d | µt,s,d , λt,s , θs ∼ Negative-Binomial(µt,s,d λt,s , θs );
log(λt,s ) = ιs + δt,s ;

(14)
(15)

where µt,s,d is the mean proportion at each delay, again defined by a probit model for the
cumulative proportion reported:
µt,s,d = St,s,d − St,s,d−1 ;

(16)

probit(St,s,d ) = ψs,d + γt,s + ξt,s ,

(17)

and λt,s is the mean of yt,s . Like in the GDM model for COVID-19 deaths, we include
models for the first D0 = 6 delayed counts zt,s,d , but also an extra (identical) model for the
P 0
remainder rt,s = yt,s − D
d=1 zt,s,d – which is unobserved when yt,s is unobserved (see Stoner
and Economou (2019) for the rationale behind modelling the remainder). The model is
implemented using MCMC following the specification in Section 4.2 for prior distributions
and implementation.

42

B.2

Negative-Binomial Model Based on Bastos et al. (INLA)

The second model is a Negative-Binomial model for the delay counts zt,s,d based on the
framework and implementation detailed in Bastos et al. (2019):
zt,s,d | λt,s , θs ∼ Negative-Binomial(λt,s , θs );

(18)

log(λt,s ) = ιs + δt,s + βd,s + γt,s,d + ξt,s .

(19)

Independently for each region, ιs are the intercepts, δt,s are first order random walks (RW1)
(to capture the epidemic curve), βd,s are RW1 effects to capture the mean delay distribution,
γt,s,d are RW1 temporal effects for each delay to capture temporal variation in the delay
distribution, and ξt,s are cyclic RW2 effects of time of day, to capture weekly cycles in
the reporting delay. The main difference between this model and the above marginal
approximation to the GDM is the lack of separation of systematic variability in the total
count and the reporting delay. The model covers each of the d = 1, . . . , 14 delays and is
implemented using the Integrated Nested Laplace Approximation method and the r-inla
package (Lindgren and Rue, 2015), using code adapted from Bastos et al. (2019).

B.3

Models based on McGough et al. (NobBS and NobBS-14)

The third and fourth competitor models are based on the framework for the delayed counts
zt,s,d proposed by McGough et al. (2020) and implemented using the NobBS package for R.
The package allows for both Poisson and Negative-Binomial models, and we opted for the

43

latter to account for over-dispersion:
zt,s,d | λt,s,d , θs ∼ Negative-Binomial(λt,s,d , θs );
log(λt,s,d ) = αt,s + log(βd,s );

(20)
(21)

where αt,s is an RW1 effect to capture the epidemic curve and βd,s are vectors of proportions
P max
such that D
d=1 βd,s = 1 (recall the maximum delay Dmax = 14). Effects βd,s therefore
capture the expected proportion reported at each delay and are modelled as fixed in time.
To account for temporal heterogeneity in the delay distribution, McGough et al. (2020)
propose fitting the model to data in a moving window of fixed length, so that the estimated
βd,s are more appropriate for recent data. We fit two models using this approach, one with
no window (NobBS) – the model is fitted to all of the data – and one with a window of
14 days (NobBS-14), so that the estimated mean delay distribution is representative of the
last two weeks. In both cases, we used the default weakly-informative prior distributions
from the NobBS package, which implements the model using the JAGS software facility for
MCMC (Plummer, 2003).

C

Under-reporting

Where count data are affected by delayed-reporting, the total reported count, yt,s,d , is
often still a substantial under-representation of the true count, termed here xt,s,d . Reports
of COVID-19 cases may, for instance, be affected by under-reporting due to a lack of
testing availability, false negative test results, or a lack of symptoms. Similarly, some
deaths due to COVID-19 may be missed if the patient was not tested and COVID-19 was
not specified on the death certificate. To take this into account, Stoner and Economou
44

(2019) present a comprehensive framework for simultaneously modelling under-reporting
and delayed-reporting. Extended here to include a spatial dimension, this is achieved by
replacing Equation (1) in Section 3 of the main text with:
xt,s | λt,s , θs ∼ Negative-Binomial(λt,s , θs );
yt,s | xt,s , πt,s ∼ Binomial(πt,s , yt,s );


πt,s
= i(t, s),
log
1 − πt,s

(22)
(23)
(24)

for yt,s ≤ xt,s and where i(t, s) is a general function which may include covariates or
random effects, e.g. covariates representing access to COVID-19 tests. The likelihood for
yt,s is non-identifiable between a high λt,s and a low πt,s , or vice-versa, so in the case where
all available counts are assumed potentially under-reported (i.e. xt,s is always unobserved),
identifiability can be achieved using prior information (Stoner et al., 019a).

45

