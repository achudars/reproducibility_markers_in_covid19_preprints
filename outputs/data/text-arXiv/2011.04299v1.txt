COVID-19 PATIENT DETECTION FROM TELEPHONE QUALITY SPEECH DATA
Kotra Venkata Sai Ritwik, Shareef Babu Kalluri, Deepu Vijayasenan

arXiv:2011.04299v1 [cs.SD] 9 Nov 2020

Department of Electronics and Communication Engineering,
National Institute of Technology Karnataka - Surathkal, Mangalore, India
ABSTRACT
In this paper, we try to investigate the presence of cues about
the COVID-19 disease in the speech data. We use an approach
that is similar to speaker recognition. Each sentence is represented as super vectors of short term Mel filter bank features
for each phoneme. These features are used to learn a twoclass classifier to separate the COVID-19 speech from normal. Experiments on a small dataset collected from YouTube
videos show that an SVM classifier on this dataset is able
to achieve an accuracy of 88.6% and an F1-Score of 92.7%.
Further investigation reveals that some phone classes, such as
nasals, stops, and mid vowels can distinguish the two classes
better than the others.
Index Terms— COVID-19, Telephone speech, super vectors, SVM
1. INTRODUCTION
The pandemic, due to the novel coronavirus (COVID-19), has
spread around the world. In such a situation, the identification
of people infected with COVID-19 is challenging for health
organizations as well as individuals. Major symptoms such
as a rise in body temperature, cough, difficulty in breathing are observed; however, totally asymptomatic cases are
also possible[1]. The clinical protocols in identifying whether
the individual is infected with coronavirus include swab test
[2], CT scans [3], chest X-Ray Images [4] etc. In this context, leveraging bio-markers like speech and audio signals in
screening the COVID-19 will help in the assessment of the viral infection. Different studies and pathological investigations
have proved that COVID-19 infected individuals exhibit difficulty while breathing and speaking. Furthermore, the changes
in speech might not be identifiable by human perception, although the person is infected, but the Computer auditions
(CA) [5, 6] can. As a token of contribution to the society
in this pandemic situation, we tried to investigate on detection of the COVID-19 infection from speech sounds alone. In
this process, we also focus on telephone speech so that such a
system could be integrated to assess the risk of the pandemic
at a region by telephone operators.
There are a few attempts in the literature in identifying pathological conditions such as bronchitis and pertussis

using mainly cough patterns [7]. These research efforts
perform machine learning techniques on speech samples collected from smartphone recordings. From the studies on the
COVID-19 patients, it was noticed that the respiration activity would be more rapid for the infected people than other
patients with flu and common cold [8]. Other studies show
that the respiratory parameters are also have impact on the
persons mood/emotion[9], stress levels [10] and physiological state of individuals [11]. In the context of the Interspeech
Computational Paralinguistics Challenge (ComParE), different research efforts have attempted to estimate the behavioral
state of humans using speech data in conditions like cold,
cough, pain, sleepiness, and infant cries. Schuller et al. has
suggested a set of computer audition tasks using speech analysis and sound analysis for COVID-19 risk assessment using
machine learning techniques [6]. However, in this work, we
look only at the speech signal and not at breathing or cough
patterns that may be difficult to acquire through a noisy telephone channel. Also, in this case, the user of the system
does not have to force a cough or create breathing patterns.
To the best of the authors’ knowledge, this is the first work
on COVID-19 screening using only telephone quality speech
data.
The organization of the paper is as follows, Section 2
details the feature extraction and statistical representation of
each phoneme as well as sentence. It also details the classification methods. In Section 3, the details of the collected
dataset are briefed. Later the experiments and results are detailed in Section 4. Finally, the conclusions of the reported
work and future directions of the proposed approach are presented in Section 5.
2. APPROACH
Many attempts have been made to automate the disease identification using the respiratory patterns [8], cough patterns [7]
and also COVID-19 [5]. These attempts make use of the biomarkers from speech such as fundamental frequency, shortterm cepstrum, cepstral peak prominence, Harmonics to noise
ratio, Glottal open quotient, etc. Higher-level feature derived
features from such features were employed to screen the people for COVID-19 [5]. In another attempt, a pool of such
features across different sounds was used to identify the pres-

Traning
Speech
Data

Filter Bank
Features

Feature
Representation

utterance level feature. We had to resort to this primitive approach as there was no large scale development data available
in this domain for learning more advanced feature representations such as i-vectors or x-vectors.

SVM
Traning

ASpIRE
Model

Testing
Speech
Data

Filter Bank
Features

Feature
Representation

SVM
Prediction

Positive Class
Negative Class

Fig. 1. Block diagram for detection of COVID-19 positive or
negative class using speech data
ence of COVID-19 infection [12]. In this work, as well, we
use short term mel-spectrum as the low-level features. We
follow an approach similar to super vectors that were originally used in speaker recognition [13] to extract utterance
level features. An SVM trained with this feature input is used
to predict COVID-19 from speech data. The block diagram of
the approach is shown in Figure 1. We could not use the more
advanced version of super-vectors such as the i-vectors [14]
or x-vectors [15] due to limited amount of training data.
2.1. Super vector Features
Traditionally super vectors are extracted using the posterior
probabilities of a background Gaussian Mixture Model. However, the dataset we worked on was from YouTube videos
and was very noisy. Therefore we decided to use a deep
neural network model as the phoneme posterior probability
estimator rather than a GMM. We used the ASpIRE chain
model [16].
The model is a Time-Delayed Neural Network(TDNN)
that is trained on the Fisher English dataset. Data augmentation by means of different room impulse responses and
noises are incorporated to increase the robustness [17]. The
model outputs the frame-level posteriors of different contextdependent phonemes.
Consider an input sequence of short term mel-spectral features from a speech utterance {x, x2 , . . . , xT }. The Aspire
model takes the short term mel-cepstrum as the input and predicts the frame level posteriors {p1 , . . . , pT } where:
 1
pj
 p2j 
 
pj =  . 
(1)
 .. 
pM
j
with pij denoting the posterior probability of phoneme i at
the frame index j. Once the posterior probabilities are calculated, we compute the normalized first order statistics of each
phoneme as :
1 X i
pj xj
(2)
fi = P i
j pj j
We then concatenate all frames of f i of each phoneme to obtain a super vector Fi = [f1 , f2 , . . . , fj ] which represents the

2.2. Classification
The computed statistics (Fi ) are used as features for the learning of a support vector machine(SVM) model for the task. We
have generated multiple training examples from the same sentence by splitting it into many overlapping sentences. A radial
basis kernel is used to train the SVM model. Cross-validation
experiments in the training data are performed to determine
the hyper-parameters, and the results are reported in an independent test set.
3. DATASET
The dataset consists of audio clips extracted from YouTube
videos of TV interviews (often a video call) of COVID-19
positive patients 1 . The dataset is made of two distinct classes:
The COVID-19 Positive speakers class and the COVID-19
Negative speakers class. The video recordings of the COVID19 Positive speakers are either from the hospital environment
or home isolated/quarantine environment. Most of the audio,
therefore, has a lot of background noise. The COVID-19 Negative speech recordings are also taken from a controlled studio
environment while broadcasting over TV channels/YouTube.
The speech part of the audio was extracted at the 44.1kHz
sampling rate. The audio samples were converted into a single channel (mono) wav format and passed through a 300Hz
– 3.4 kHz band-pass filter. This is performed to simulate telephone quality speech. The audio is finally down-sampled
to 8 kHz range. Audio corresponding to different speakers
are segmented manually, and each utterance was annotated as
positive and negative class. COVID-19 positive speakers are
annotated as the positive class, and host/other audio speakers were annotated as the negative class. Speech non-speech
separation was performed on each utterance.
We have collected the speech data from nineteen speakers, of which ten speakers are COVID-19 Positive, and nine
speakers are COVID-19 Negative2 . The composition of male
and female speakers in each class are tabulated in Table 1
along with the number of utterances per each class and gender.
4. EXPERIMENTS AND RESULTS
We divided the collected data into training (12 speakers) and
testing (7 speakers) parts. There are 501 utterances for train1 A sample YouTube link used:

https://www.youtube.com/watch?v=yr2GnZo2KiA
dataset used for this work is available in the following link
https://github.com/shareefbabu/covid data telephone band
2 The

Table 1. Details of collected dataset of Male (M) and Female
(F) Speakers
Class
Speakers # Sentences
F
M
F
M
Positive 4
6
168
296
Negative 3
6
44
194
ing and 201 utterances for testing. There is no overlap speakers in train and test splits.
The performance of the system is assessed with, accuracy
and F1-score metrics. The F1-score metric computation is
given by the following equation
F 1score =

2P R
(P + R)

(3)

where the Precision (P) and Recall (R) are given by:
TP
(T P + F P )
TP
R=
(T P + F N )

P =

Where TP denotes the true positives, FP the false positives
and FN the false negatives.
We windowed the speech into the usual 25ms long, 10
ms shifted frames, and extracted 40-dimensional mel filter
bank coefficients. In addition, the frame-level phone posteriors are computed from the ASpIRE chain model. Even
though the model provides context-dependent phoneme probabilities, we sum over the context-dependent phonemes to
get context-independent phoneme posteriors. Thus we get a
39-dimensional posterior vector corresponding to 39 TIMIT
phonemes. The silence phoneme is discarded. The mel-filter
bank features and phoneme posteriors are used in the computation of first-order statistics of every phoneme ( Eqtn. 2 ).
Each phoneme first-order statistics is a 40 dimensional vector.
These are concatenated to form a 40×39 = 1560 dimensional
super vector. We first performed a set of cross-validation experiments in the training data to optimize certain parameters
and then performed an actual evaluation on the test data.

To compensate for the same, we computed utterance level features using 3s segments from the training data. The 3s segments were chosen with a shift of 0.1 seconds. This allowed
us to increase the number of training examples. However,
testing examples were kept as such.
The SVM kernel and the regularization parameter were
selected based on the maximum accuracy of the crossvalidation experiment. The Radial Basis Function kernel
was found to be the optimal kernel for the SVM. The best
weighted average accuracy and F1 score over all the folds in
cross-validation are 70.5% and 77.0%, respectively.
To get an understanding of the results, we also conducted
an experiment to assess the contribution of different phoneme
classes in COVID-19 detection. This experiment is conducted
by using different feature subsets that correspond to different
speech classes.
We performed the same set of cross-validation experiments by considering only a subset of phoneme posteriors.
We considered eight different classes of phonemes: nasals,
back-vowels, front-vowels, mid-vowels, semi-vowels, stops,
fricatives, and diphthongs. Each time only posteriors that
fall in the sound class were used to compute the first-order
statistics(refer Eqtn: 2). For example, to run the experiments
only for nasal phoneme class we will only use three posteriors
(corresponding to /m/, /n/ and /ng/ ). Therefore the utterance
lever feature will be only 3 × 40 = 120 dimensional. In
order to avoid sentences that do not have these phonemes, we
put a threshold value on the sum of the posteriors across all
frames and within the phone class. This threshold value was
chosen as 30 based on cross-validation experiments. Only if
the value is greater than the threshold, the sentence was considered for testing. If we assume that the DNN was predicting
all labels correctly, this is equivalent to saying that we expect
at least 30 frames within the phoneme class (say nasals) in
the sentence.

4.1. Cross Validation Experiments
Initially, we performed cross-validation experiments to assess
the performance, as well as tune some hyper-parameters of
the SVM classifier. The training dataset of 12 speakers is
divided into six subsets(folds) of 2 speakers each for crossvalidation. The training split has six speakers, each in positive
(307 utterances) and negative (194 utterances) classes. There
are no overlapping speakers across different subsets. Each
time, one subset was kept for testing, and the remaining five
subsets were used for training the SVM model.
However, the training data was not sufficient in this setting, and we have observed an over-fit problem with the SVM.

Fig. 2. Accuracy with confidence interval of 95% marked for
specific phonemes on cross-validation data
Fig.2 shows the accuracy of each class with confidence
interval of 95% on cross-validation data. F1 scores for each
of the phoneme class are shown in Fig.3.

Fig. 3. F1 scores for specific phonemes on cross-validation
data

Fig. 4. Accuracy with confidence interval of 95% marked for
specific phonemes on test data

Table 2. Sensitivity and Specificity values on the cross validation data
Specificity Sensitivity
Full Dataset
0.55
0.81
Nasals
0.80
0.85
Front-Vowels
0.73
0.70
0.81
0.73
Mid-Vowels
Back-Vowels
0.70
0.71
Semi-Vowels
0.74
0.58
Stops
0.67
0.87
Fricatives
0.67
0.67
Diphthongs
0.62
0.71

Nasals, Stops, and Mid-Vowels are the top-3 performing
classes on cross-validation with accuracies of 82.9%, 77.52%,
and 77.25%, respectively, over all the folds. The corresponding F1-scores for Nasals, Stops, and Mid-Vowels are 84.06%,
80.23%, and 73.91%, respectively. The specificity and sensitivity scores on the cross-validation dataset for phonemes as
well as full dataset is given in Table.2.
4.2. Evaluation data
After fixing all hyper-parameters of the system, we evaluate
the system performance on independent test data. The SVM
is trained using the entire training data using optimized parameters from the cross-validation experiment. The system is
evaluated on the 7 speakers test dataset (201 utterances) out
of which 4 speakers are in positive class (157 utterances) and
3 speakers are in negative class (44 utterances).
The accuracy and F1 score over the testing data were
found to be 88.6% and 92.7%, respectively. It may be noted
that the system is able to generalize the performance of the
cross-validation experiment to the test set. The ROC curve on
the evaluation data is shown in Fig 6. A very recent coughbased system [18] has reported accuracy of 88.9% and an
F-Score of 90.3% on identifying 4 different pathological con-

Fig. 5. F1 scores for specific phonemes on test data

ditions. However, the system relies on the cough signal, that
is not as easily available as the speech signal. Besides, our
system has the advantage of telephone quality speech data
that can be acquired remotely.
We also tested the system using only phoneme subclasses. The highest performing sound classes are the same
– nasals, stops, and mid-vowels with accuracies 91.8%,
90.1%, and 91.1% and the corresponding F1-Scores are
92.6%, 92.7% and 93.1% respectively. The specificity and
sensitivity scores on the evaluation dataset for phonemes is
given in Table.3. The corresponding ROC curves are shown
in Figure 7. This setup shows a better performance compared
to using all phoneme classes together as feature input in terms
of accuracy, F1 score, and area under the curve (AUC). Thus
it could be concluded that these phoneme classes carry some
bio-markers about the COVID-19 infection. However, it may
be noted that the test dataset in phoneme specific classes is
only a subset of the total subset because all the phoneme subclasses do not occur in every sentence. We noticed that only
around 40% of the total test set had each of these phoneme
classes(nasals, mid-vowels, and stops).

Table 3. Sensitivity and Specificity values on the Test data
Specificity Sensitivity
Full Dataset
0.73
0.93
Nasals
0.85
0.94
0.89
0.92
Mid-Vowels
Stops
0.82
0.90

Fig. 7. ROC Curve for COVID detection with features for
three top performing phoneme classes

Fig. 6. ROC Curve for COVID detection with features from
all phonemes (full sentence)
5. CONCLUSION
We created a dataset of speech from COVID-19 positive
speakers. We used primitive super-vector like features as utterance level features due to data size limitations. SVM based
machine learning systems achieved an accuracy of 88.6%
with an F-score of 92.7%. Testing on individual phoneme
sub classes shows that Nasals, Stops, and Mid-Vowels alone
can predict the presence of COVID-19 from speech data.
The entire study depends only on telephonic quality speech.
In addition to remote screening, such a system could help
predict the pandemic risk in a region by sampling voice calls.
The study is currently limited to 19 for speakers only. The
authors would like to acquire more data. This would enable
us to validate our results on the phoneme sub-class level quite
extensively. We hope this will enable us to design a proper
telephone-based COVID-screening test in the future.
6. REFERENCES
[1] World Health Organization,
Coronavirus disease
(COVID-19) outbreak situation, 2020.
[2] Centers for Disease Control and Prevention, “Coronavirus disease (COVID-19) Test for Current Infection,”
2020.
[3] Ophir Gozes, Maayan Frid-Adar, Hayit Greenspan,

Patrick D Browning, Huangqi Zhang, Wenbin Ji, Adam
Bernheim, and Eliot Siegel, “Rapid AI development cycle for the coronavirus (covid-19) pandemic: Initial results for automated detection & patient monitoring using deep learning ct image analysis,” arXiv preprint
arXiv:2003.05037, 2020.
[4] Linda Wang and Alexander Wong, “Covid-net: A
tailored deep convolutional neural network design for
detection of covid-19 cases from chest x-ray images,”
arXiv preprint arXiv:2003.09871, 2020.
[5] Thomas F Quatieri, Tanya Talkar, and Jeffrey S Palmer,
“A framework for biomarkers of covid-19 based on coordination of speech-production subsystems,” IEEE
Open Journal of Engineering in Medicine and Biology,
vol. 1, pp. 203–206, 2020.
[6] Björn W Schuller, Dagmar M Schuller, Kun Qian, Juan
Liu, Huaiyuan Zheng, and Xiao Li, “Covid-19 and computer audition: An overview on what speech & sound
analysis could contribute in the sars-cov-2 corona crisis,” arXiv preprint arXiv:2003.11117, 2020.
[7] Anthony Windmon, Mona Minakshi, Sriram Chellappan, Ponrathi Athilingam, Marcia Johansson, and
Bradlee A Jenkins, “On detecting chronic obstructive
pulmonary disease (copd) cough using audio signals
recorded from smart-phones.,” in HEALTHINF, 2018,
pp. 329–338.
[8] Yunlu Wang, Menghan Hu, Qingli Li, Xiao-Ping Zhang,
Guangtao Zhai, and Nan Yao, “Abnormal respiratory
patterns classifier may contribute to large-scale screening of people infected with covid-19 in an accurate and
unobtrusive manner,” arXiv preprint arXiv:2002.05534,
2020.

[9] Rabab A Hameed, Mohannad K Sabir, Mohammed A
Fadhel, Omran Al-Shamma, and Laith Alzubaidi, “Human emotion classification based on respiration signal,”
in Proceedings of the International Conference on Information and Communication Technology, 2019, pp. 239–
245.
[10] Valentina Perciavalle, Marta Blandini, Paola Fecarotta,
Andrea Buscemi, Donatella Di Corrado, Luana Bertolo,
Fulvia Fichera, and Marinella Coco, “The role of deep
breathing on stress,” Neurological Sciences, vol. 38, no.
3, pp. 451–458, 2017.
[11] Mark JD Griffiths, Danny Francis McAuley, Gavin D
Perkins, Nicholas Barrett, Bronagh Blackwood, Andrew
Boyle, Nigel Chee, Bronwen Connolly, Paul Dark, Simon Finney, et al., “Guidelines on the management of
acute respiratory distress syndrome,” BMJ open respiratory research, vol. 6, no. 1, pp. e000420, 2019.
[12] Neeraj Sharma, Prashant Krishnan, Rohit Kumar,
Shreyas Ramoji, Srikanth Raj Chetupalli, Prasanta Kumar Ghosh, Sriram Ganapathy, et al., “Coswara–a
database of breathing, cough, and voice sounds for
covid-19 diagnosis,” arXiv preprint arXiv:2005.10548,
2020.
[13] Tomi Kinnunen and Haizhou Li, “An overview of textindependent speaker recognition: From features to supervectors,” Speech communication, vol. 52, no. 1, pp.
12–40, 2010.
[14] Najim Dehak, Patrick J Kenny, Réda Dehak, Pierre Dumouchel, and Pierre Ouellet, “Front-end factor analysis
for speaker verification,” IEEE Transactions on Audio,
Speech, and Language Processing, vol. 19, no. 4, pp.
788–798, 2010.
[15] David Snyder, Daniel Garcia-Romero, Gregory Sell,
Daniel Povey, and Sanjeev Khudanpur, “X-vectors: Robust dnn embeddings for speaker recognition,” in 2018
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP). IEEE, 2018, pp.
5329–5333.
[16] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur, “Audio augmentation for speech recognition,” in Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[17] Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L
Seltzer, and Sanjeev Khudanpur, “A study on data
augmentation of reverberant speech for robust speech
recognition,” in 2017 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2017, pp. 5220–5224.

[18] Ali Imran, Iryna Posokhova, Haneya N Qureshi, Usama Masood, Sajid Riaz, Kamran Ali, Charles N John,
Iftikhar Hussain, and Muhammad Nabeel, “Ai4covid19: Ai enabled preliminary diagnosis for covid-19 from
cough samples via an app,” Informatics in Medicine Unlocked, p. 100378, 2020.

