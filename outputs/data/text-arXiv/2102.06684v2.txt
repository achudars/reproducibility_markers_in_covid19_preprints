arXiv:2102.06684v2 [cs.LG] 15 Feb 2021

DeepGLEAM: a hybrid mechanistic and deep
learning model for COVID-19 forecasting
Authors:
Dongxia (Allen) Wu ‚Ä† , Liyao Gao, Xinyue Xiong ? ,
Matteo Chinazzi? , Alessandro Vespignani? , Yian Ma‚Ä† , Rose Yu‚Ä†
Institution:
‚Ä†
University of California, San Diego, ? Northeastern University.
Email:
dowu@ucsd.edu, marsgao@uw.edu, xiong.xin@northeastern.edu,
m.chinazzi@northeastern.edu, a.vespignani@northeastern.edu,
yianma@ucsd.edu, roseyu@eng.ucsd.edu
Abstract: We introduce DeepGLEAM, a hybrid model for COVID-19 forecasting. DeepGLEAM combines a mechanistic stochastic simulation model GLEAM with deep learning. It uses
deep learning to learn the correction terms from GLEAM, which leads to improved performance.
We further integrate various uncertainty quantification methods to generate confidence intervals.
We demonstrate DeepGLEAM on real-world COVID-19 mortality forecasting tasks.

1

Introduction

Pandemic, hazards, and disasters are only increasing in frequency, magnitude and complexity,
leading to difficult fiscal, social, cultural, and environmental consequences for the nation and its
communities. Accurately modeling of the pandemics dynamics is the key to our ability to predict
the spreading of diseases, planning for adverse events and reducing disaster losses. However,
the current machine learning (ML) methods for time series predictions, e.g., COVID-19 case,
hospitalizations, and fatalities, still have many limitations.
Data-driven methods such as autoregressive models assume stationary distribution. ARIMA
[5] model assumes linearity and Gaussian noises, which are too restrictive for the timescales in
COVID-19. Deep learning models potentially can alleviate this difficulty but require significant
amounts of training data. On their own, deep learning (DL) allows flexible modeling of complex,
unknown phenomena, but does not integrate physical laws or common sense reasoning. Besides,
DL models are often difficult to interpret, and their predictions inaccessible to domain experts [31].
On the other hand, traditional epidemic models are mostly mechanistic models. These models
are derived from first principles, can encode the natural history of the disease, and are easy to
interpret. For instance, large scale stochastic epidemic models that combine real-world high resolution data on populations and human mobility such as the Global Epidemic and Mobility model
(GLEAM) [2, 3, 45, 7] characterize complex epidemic dynamics based on meta-population agestructured compartmental models. Due to the difficulty of generating stochastic epidemic profiles
for thousands of different subpopulations, these models while providing realistic dynamics of the
spatial and temporal spread of an emerging disease, are often slow to simulate at fine resolutions.
Obtaining high-fidelity, high-resolution epidemic models are limited by the high computational
1

cost of generating thousands of multiple embarrassingly parallel model runs to account not only for
the uncertainty surrounding the characteristics of the disease outbreak (initial conditions, starting
time of the epidemic, etiology of the disease) that might require the exploration of vast parameter
space, but also consider the effects of the inherent stochastic behavior of the model. Prior works
apply this idea using an ensemble averaging over 50 models for point estimation and uncertainty
quantification [36, 8]. Lastly, being the models derived from first principles, they are necessarily making simplifying assumptions and simplifications that can be relaxed by employing a deep
learning approach.
We therefore combine both approaches in the current work. We use the forecasts on COVID-19
daily deaths generated by GLEAM [7, 9], an individual-based, stochastic, and spatial epidemic
model, to train a diffusion convolution recurrent neural network (DCRNN) that learns stationary
features in the time series to improve the alignment between the model predictions and the observed
ground-truth data. This combination, named DeepGLEAM, enhances the prediction performance
of the stand-alone mechanistic model by allowing to reduce the bias that exists between the model
predictions and the observed noisy surveillance data.
In our experiments, we observe that the DeepGLEAM model significantly outperforms other
machine learning models such as the vector auto-regressive (VAR) model and a pure deep learning
model in one to four weeks ahead prediction. It also outperforms the GLEAM model in one to
three weeks ahead short term predictions. We additionally quantify the uncertainty of our predictions. We compare different uncertainty quantification methods that are commonly considered,
including bootstrap, quantile regression, posterior sampling, and approximate Bayesian inference.
DeepGLEAM demonstrated appealing performances both in accuracy and confidence intervals.

2

Methodology

2.1

Problem Definition

SpatialDependence
Dependence
Spatial

Given multivariate time series X = (X1 , ¬∑ ¬∑ ¬∑ , Xt ) of t time step, with each Xt ‚àà RP √óD indicating
D features from P locations. In the context of COVID-19 forecasting, locations can be different
states or counties. Features can correspond to incident death, and infected cases. In addition to the
time series, we also have spatial correlations represented as a graph, corresponding to an adjacency

Figure 1: Two types of spatial dependency. left:Epidemic
pixels distributed
on the regular
grid; right:
= a diÔ¨Äusion
process
‚Ä¢
Epidemic
=
a
diÔ¨Äusion
process
Image
data
=
a
regular
grid
‚Ä¢ diffuses
epidemic
a irregular
grid. grid
Image on
data
= a regular

‚Ä¢

‚Ä¢

on an irregular grid (graph)

with convolutional
‚Ä¢ ‚Ä¢Model
Model with convolutional
neural networks

neural networks

‚Ä¢2

on an irregular grid (graph)

Generalize convolutional
convolutional
‚Ä¢ Generalize
operation
to directed
graphs

Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting

operation to directed graphs

matrix A. The spatial correlation can be calculated based on travel data among different locations.
Spatiotemporal forecasting aims learns a function f such that:
f : (X ; A) ‚Üí (Xt+1 , ¬∑ ¬∑ ¬∑ , Xt+H ; A)

(1)

where H is the forecasting horizon. The function f approximate the epidemic dynamics and can
either be a mechanistic model or a deep learning model.

2.2
2.2.1

DeepGLEAM
Global Epidemic and Mobility Model.

The Global Epidemic and Mobility model (GLEAM) is a stochastic spatial epidemic model in
which the world is divided into over 3,200 geographic subpopulations constructed using a Voronoi
tessellation of the Earth‚Äôs surface. Subpopulations are centered around major transportation hubs
(e.g. airports) and consist of cells with a resolution of approximately 25 x 25 kilometers [2, 3, 41,
48, 7, 9]. High resolution population density data are used to define the number of individuals in
each cell [1], while a data-driven approach is employed to generate subpopulation specific agestructured contact patterns [32].
GLEAM integrates a human mobility layer - represented as a network - that uses both shortrange (i.e. commuting) and long-range (i.e. flights) mobility data from the Offices of Statistics
for 30 countries on 5 continents as well as the Official Aviation Guide (OAG) and IATA databases
(updated in 2020). The air travel network consists of the daily passenger flows between airport
pairs (origin and destination) worldwide mapped to the corresponding subpopulations. Where
information is not available, the short-range mobility layer is generated synthetically by relying on
the ‚Äúgravity law‚Äù or the more recent ‚Äúradiation law‚Äù both calibrated using real data [39].
The model is calibrated to realistically describe the evolution of the COVID-19 pandemic as
detailed in [7, 9]. Lastly, GLEAM is stochastic and produces an ensemble of possible epidemic
outcomes for each set of initial conditions. To account for the potentially different reporting levels
of the states, a free parameter Infection Fatality Rate (IFR) multiplier is added to each model. To
calibrate and select the most reasonable outcomes, we filter the models by the latest hospitalization
trends and confirmed cases trends, and then we select and weight the filtered models using Akaike
Information Criterion [47]. The forecast of the evolution of the epidemic is formed by the final
ensemble of the selected models.
2.2.2

Deep Learning Model: Diffusion Convolutional RNN

When the locations are evenly spaced on a regular grid, popular deep forecasting models include
Convolutional LSTM (ConvLSTM) [44] and PredRNN [42]. The dynamics is approximated by a
recurrent neural network (RNN),
ht+1 = œÉ(W h ¬∑ ht + W x ¬∑ Xt )

(2)

where ht are the time-varying hidden states. The convolution operator extracts spatial features and
uses an RNN to model the temporal dynamics, which is the key idea behind ConvLSTM [44].
3












 
 





Figure 2: System architecture of DeepGLEAM. A deep learning model (DCRNN) is trained with
the residual between reported incident death and predictions from GLEAM.

When the locations are distributed as a graph which is the case for COVID-19 forecasting, a
natural extension is to generalize convolution to graph convolution. This type of models include
Spatiotemporal Graph CNN [46], Diffusion Convolutional RNN (DCRNN) [26] and other variants.
Mathematically speaking, that means to replace the matrix multiplication operator in an RNN
with a graph convolution operator, leading to the following design of DCRNN [26]:
ht+1 = œÉ(W ‚àóg ht + W ‚àóg Xt )

(3)

Where ‚àóg stands for graph convolution. There are many variations for graph convolution or graph
embedding. One example is
W ‚àóg Xt = W ¬∑ (D‚àí1 A) ¬∑ Xt
(4)
Here D ‚àà RP √óP contains the diagonal element of A. One can also replace the random walk
matrix D‚àí1 A with the normalized Laplacian matrix I ‚àí D1/2 (D ‚àí A)D1/2 as in [22]. The entire
network is trained by maximizing the likelihood of generating the target future time series using
backpropagation through time. DCRNN is able to capture spatiotemporal dependencies in time
series without imposing strong modeling assumptions on the dynamics.
2.2.3

DeepGLEAM: combining GLEAM with Deep Learning

We use the residual between daily death number and GLEAM predictions as input and output of
the DCRNN model and name it DeepGLEAM model. We use an encoder-decoder sequence to
sequence learning framework in the DCRNN structure. The encoder reads as input a 7 √ó 50 √ó 4
tensor that comprises the daily residuals between the observed death number and the GLEAM
forecasts for the 50 US states over 4 different prediction horizons. It encodes the information in 7
hidden layers. The decoder produces forecasts for each state for the following 4 weeks. We perform
4

Diffusion Convolutional
Recurrent Layer

Diffusion Convolutional
Recurrent Layer

Diffusion Convolutional
Recurrent Layer

Diffusion Convolutional
Recurrent Layer

Predictions

Input Graph
Signals

...
......

...
......

<GO>

......
...

...
......

¬∑

Encoder

Copy States

Time
Delay =1

Decoder

Figure 3: System architecture for the Diffusion Convolutional Recurrent Neural Network designed
for spatiotemporal traffic forecasting. The historical time series are fed into an encoder whose final
states are used to initialize the decoder. The decoder makes predictions based on either previous
ground truth or the model output.
autoregressive weekly death predictions (from one week ahead to four weeks ahead). To this end,
we build a hybrid DeepGLEAM model. The model architecture of DeepGLEAM is shown in
Figure 2. The default DeepGLEAM model is for point estimation. Next, we discuss probabilistic
forecasts by combing DeepGLEAM with Frequentist or Bayesian uncertainty quantification (UQ)
methods, including bootstrap, quantile regression (Quantile), spline quantile regression (SQ), MIS
regression (MAE-MIS), MC dropout, and SG-MCMC.

2.3

Uncertainty Quantification

In this subsection, we introduce various methods for uncertainty quantification that can be seamlessly integrated into DeepGLEAM.
Bootstrap. The (generalized) bootstrap method [13] randomly generates in each round a weight
vector over the index set of the data. The data are then resampled according to the weight vector.
With every resampled dataset, we retrain our model and generate predictions. Using the predictions
from different retrained models, we inference the (1 ‚àí œÅ) confidence interval using (1 ‚àí œÅ2 ) and œÅ2
quantiles. The quantiles can be estimated using the order statistics of Monte Carlo samples.
Mean Interval Score (MIS) and Quantile Regression For a fixed confidence level œÅ, we can
directly minimize Mean Interval Score (MIS) to obtain estimates of the confidence intervals. MIS
is defined for evaluation of estimated upper and lower confidence bounds. For a random vector
Z ‚àº PZ , if the estimated upper and lower confidence bounds are u and l, where u and l are the

5

(1 ‚àí œÅ2 ) and

œÅ
2

quantiles for the (1 ‚àí œÅ) confidence interval, MIS is defined using samples zi ‚àº PZ


N 
2
2
1 X
(u ‚àí l) + (zi ‚àí u)1 {zi > u} + (l ‚àí zi )1 {zi < l} .
M ISN (u, l) =
N i=1
œÅ
œÅ

(5)

Specifically, to use MIS as a loss function for deep neural networks, we use a multi-headed
model to jointly output the upper bound u(x), lower bound l(x), and the prediction f (x) for a
given input x, and minimize the neural network parameter Œ∏:
n
2
LMIS (y, u(x), l(x), f (x); Œ∏, œÅ) = min E(x,y)‚àºD [(u(x) ‚àí l(x)) + (y ‚àí u(x))1 {y > u(x)}
Œ∏
œÅ
(6)
o
2
+ (l(x) ‚àí y)1 {y < l(x)} + |y ‚àí f (x)|]
œÅ
Here 1 {} is an indicator function, which can be implemented using the identity operator over
the larger element in Pytorch.
For quantile regression [24, 25], we can use the one-sided quantile loss function to generate
predictions for a fixed confidence level œÅ. Given an input x, and the output f (x) of a neural
network, parameterized by Œ∏, quantile loss is defined as follows:
n
o
LQuantile (y, f (x); Œ∏, œÅ) = min E(x,y)‚àºD [(y ‚àí f (x))(œÅ ‚àí 1 {y < f (x)}]
(7)
Œ∏

Quantile regression behaves similarly as the MIS regression method. Both methods generate
one confidence interval per time. In addition, [23, 40, 34] have explored similar ideas of directly
optimizing the prediction interval using different variations of quantile loss.
One caveat of these methods is that different predicted quantiles can cross each other due to
variations given finite data. This will cause a strange phenomenon when the size of the data set and
the model capacity is limited: the higher confidence interval does not contain the interval of lower
confidence level or even the point estimate. One remedy for this issue is to add variations in both
the data and the parameters to increase the effective data size and model capacity. In particular,
during training, we can use different subsets of data and repeat random initialization from a prior
distribution to form an ensemble of models. In this way, our modified MIS and quantile regression
methods have integrated across different models to quantify the prediction uncertainty and have
taken advantage of the Bayesian philosophies.
Another solution to alleviate quantile crossing and unify different confidence levels is to minimize CRPS by assuming the quantile function to be a piecewise linear spline with monotonicity
[16], a method we call Spline Quantile regression (SQ). In the experiment, we also included this
method for comparison.
Stochastic Gradient MCMC (SG-MCMC) In Bayesian‚Äôs perspective, the posterior distribution
contains belief about the parameter based on observed data. Therefore, we could construct confidence interval via samplings from posterior distribution. To estimate expectations or quantiles
6

according to the posterior distribution over the parameter space, we use SG-MCMC [43, 28]. We
find in the experiments that the stochastic gradient thermostat method (SGNHT) [10, 38] is particularly useful in controlling the stochastic gradient noise. This is consistent with the observation
in [20] where stochastic gradient thermostat method is applied to an i.i.d. image classification task.
Approximate Bayesian Inference SG-MCMC can be computationally expensive. There are
also approximate Bayesian inference methods introduced to accelerate the inference procedures [29,
12]. In particular, the Monte Carlo (MC) drop out method sets some of the network weights to zero
according to a prior distribution [14, 15]. This method serves as a simple alternative to variational
Bayes methods which approximate the posterior [4, 18, 27, 37, 35]. We examine the popular MC
dropout method in the experiments for comparison.

3

Experiments

We evaluate the performance of both Frequentist and Bayesian UQ methods on the time-series
dataset for COVID-19 incident deaths. This is highly challenging as the data is very small, highly
noisy, and pertain complex spatial dependency. The experiments are implemented using pytorch
[33]. Based on our observations, Bayesian methods typically reach lower error in their mean
predictions while Frequentist methods, especially quantile and MIS regression, prevail in providing
reasonable confidence intervals and achieve the best performance in MIS.

3.1

Datasets and Experiment Setup

Datasets. The COVID-19 dataset contains reported death from Johns Hopkins University [11]
and the death predictions from a mechanistic, stochastic, and spatial metapopulation epidemic
model called Global Epidemic and Mobility Model (GLEAM) [2, 3, 41, 48, 7]. Both data are
recorded for the 50 US states during the time period from May 24th to Sep 12th 2020. We use the
residual between the reported death and the corresponding GLEAM predictions to train the model
(http://covid19.gleamproject.org/).
We form the spatial graph‚Äîadjacency matrix A in Eqn equation 1‚Äîusing air travel flows between different states. The mobility data is obtained from the Official Aviation Guide (OAG) and
the International Air Transportation Association (IATA) databases (updated in 2020). Each directed weighted edge of the graph represents the average number of passengers traveling between
two states on a daily basis.
Evaluation Metrics. We evaluate our model using the metrics. For point estimation, we define
our metrics using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Given multivariate time series X[1,t] = (X1 , ¬∑ ¬∑ ¬∑ , Xt ) of t time step, with each Xt ‚àà RP √óD indicating D
features from P locations. Suppose X[t+1,t+h] = {Xt+1 , . . . , Xt+h } is the ground truth data and
XÃÇ[t+1,t+h] = {XÃÇt+1 , . . . , XÃÇt+h } is the prediction. To evaluate how well the prediction XÃÇ is, we

7

Figure 4: Original flight network connecting US airports. Data is aggregated at the State level to
construct the network State-to-State graph.

could define our metrics as below:
h

X
1
M AE(X[t+1,t+h] , XÃÇ[t+1,t+h] ) =
||X[t+1,t+h] ‚àí XÃÇ[t+1,t+h] ||1,1 .
D ¬∑ P ¬∑ h i=1

(8)

h

X
1
||X[t+1,t+h] ‚àí XÃÇ[t+1,t+h] ||22,2 ,
RM SE(X[t+1,t+h] , XÃÇ[t+1,t+h] ) =
D ¬∑ P ¬∑ h i=1
where is a matrix norm defined by ||A||m,n =

(9)

 mn  n1
m
.
i=1 |aij |


PD PP
i=1

For uncertainty quantification, we define our metric via MIS as defined in equation 5. MIS
rewards narrower confidence or credible intervals and penalizes intervals that do not include the
observations. In our case, MIS is preferred over other scoring functions such as the Brier score [6],
Continuous Ranked Probability Score (CRPS) [30, 19, 17] as it is intuitive and easy to compute.
Experiment Setup for DCRNN in various contexts. For point estimation using DCRNN, we
apply one hidden layer of 8 RNN units and Laplacian filters. The optimization process is via Adam
optimizer [21] learning rate 1e‚àí2 .
For frequentist uncertainty quantification techniques, Bootstrap method specifically is leaveone-out bootstrap due to the limited sample size. For Quantile regression, we apply pinball loss
function [24, 25] to train three different quantiles for the corresponding confidence interval (CI),
e.g., (0.025, 0.5, 0.975) for 95%. For SQ regression, we use spline quantile function to approximate
a quantile function and use the CRPS as the loss function. For MIS regression, We combine MAE
with MIS and directly minimize this loss function.
For Bayesian uncertainty quantification methods, considering MC Dropout, we apply random
dropout through the testing process with 5% drop rate and iterate 300 times to achieve a stable
prediction. The result of SG-MCMC is averaged from 25 posterior samples, and we selected a
Gaussian prior N (0, 2.0) with random initialization as N (0, 0.05).

3.2

Performance of DeepGLEAM and other baseline methods

To validate the benefit of hybrid modeling, we compare with the pure deep learning model (Deep)
for point estimation, the pure mechanistic GLEAM model, the DeepGLEAM model, and the SGMCMC model. To justify the use of deep learning, we also compare with the classic time series
8

Truth
GLEAM
DeepGLEAM
Deep

weekly deaths

1,000
800
600
400
200

23

24

25

26

27

28

29

30

31

week number

32

33

34

35

36

37

Figure 5: One week ahead COVID-19 forecasts visualization in California. Comparison shown for
GEAM, DeepGLEAM and Deep models.
Horizon H
1W
2W
3W
4W

VAR
159.29
305.99
456.35
697.27

Deep
239.94
213.31
189.49
161.88

GLEAM
73.59
65.46
73.75
70.16

DeepGLEAM
66.03
57.67
70.12
70.75

SG-MCMC
58.30
46.61
59.27
70.57

Table 1: RMSE comparison of different approaches for COVID-19 mortality forecasting
model: Vector Auto-regressive (VAR) model. In Table 1, we can see that VAR clearly underperforms other methods. In general, shallow models (VAR) fails in this task due to the complex
dynamic of COVID-19 prediction.
In Figure 5, we visualize an example in California to compare the forecasts of different models.
The input data (for training or validation) are from weeks before 34, and we start the prediction from week 34, labeled by the vertical dash line. It can be observed that Deep model fails
to predict the dynamics of COVID-19 evolution. We quantitively compare the accuracy among
DeepGLEAM, SG-MCMC, Deep, and GLEAM models using RMSE. DeepGLEAM and Deep
are deterministic models while SG-MCMC is the statistical model with the best accuracy. Table
1 shows both DeepGLEAM and SG-MCMC outperform GLEAM while the Deep model is much
worse than GLEAM. By calculation, there is a 6.6% improvement for DeepGLEAM and 17%
improvement for SG-MCMC on average from GLEAM.
One reason for this phenomenon is the distribution shifts in the epidemiology dynamics. Without
background knowledge, deep neural networks do not have enough inductive bias to guide their
predictions. On the other hand, GLEAM, DeepGLEAM, and SG-MCMC leverage the mechanistic
knowledge about disease transmission dynamics to infer underlying dynamic change.

3.3

Comparison of uncertainty quantification methods

Table 2 compares 6 different UQ methods for 1-4 weeks ahead forecasting of COVID-19 mortality.
We observe that SG-MCMC performs the best in RMSE and MAE. Quantile and MIS regressions
achieve the best performance in MIS. SQ regression suffers from a poor MIS due to the small
9

number of knots for linear spline quantile function construction. For MC dropout, its prediction
accuracy is relatively robust but its MIS is bad. The interval in Table 2 shows Bootstrap, SQ, MC
dropout, and SG-MCMC methods tend to make overconfident predictions with shorter intervals
and larger coverage compared with Quantile, and MIS methods.
We visualize the predictions of quantile regression and SG-MCMC in Figure 6. We find mean
predictions from SG-MCMC are closer to the ground truth while quantile regression provides
better confidence bounds at the state level. For example, the US-TX subplot shows that the
SG-MCMC‚Äôs mean prediction is closer to the ground truth compared with quantile prediction.
Meanwhile, the US-GA and US-NY subplots show that the overconfident credible interval of
SG-MCMC fails to cover the ground truth. In these cases, quantile regression can provide safer
confidence bounds and achieve better MIS. For country-level prediction, as shown in the US subplot, SG-MCMC outperforms quantile regression in both mean prediction accuracy and confidence
bounds.
Truth
US

US-TX

US-CA

2000

12500

1250

10000

1500

1000

7500

1000

750

weekly death

5000

500

500

2500
23

27

32

37

US-GA

0

250
23

27

37

US-IL

800

1000

32

23

600

600

600

400

400

200

200

200
0

23

27

32

37

0

23

27

32

37

0

32

37

23

27

US-NJ

800

800
400

27

1500
1250
1000
750
500
250
0

Quantile

US-FL

32

37

US-NY

week number

37

0

32

37

32

37

US-MA

400

200
32

27

600

400

27

23

800

600

23

1200
1000
800
600
400
200
0

SG-MCMC
US-AZ

200
23

27

32

37

0

23

27

Figure 6: One week ahead COVID-19 prediction in the country level and 9 states with largest death
from week 23 to week 37.

3.4

Predictions of the rest 41 states in the U.S.

Figure 7 shows the predictions of the rest 41 states in the U.S.. The data before week 32 has
been seen during training or evaluation, therefore we focus on the result to the right of the vertical
dashed line.

10

Horizon H
1W

2W

3W

4W

Metric

Bootstrap Quantile

SQ

MIS

MC Dropout SG-MCMC

RMSE
MAE
MIS (95% CI)
Interval
MIS (90% CI)
Interval
MIS (80% CI)
Interval

64.63
32.57
856.87
32.48
444.68
32.48
252.39
24.28

70.42
36.63
413.37
190.98
302.77
129.92
218.73
73.23

71.72
74.07
34.42
39.03
1049.69 427.53
23.73 227.19
541.61 285.99
22.62 141.24
286.35 200.32
20.31
84.45

66.82
34.27
790.24
47.79
443.13
40.51
254.42
31.94

58.30
29.72
563.77
47.05
365.6
43.04
212.97
35.96

RMSE
MAE
MIS (95% CI)
Interval
MIS (90% CI)
Interval
MIS (80% CI)
Interval

54.21
32.38
762.55
36.25
399.40
36.25
235.51
27.69

61.35
36.81
363.14
219.06
276.32
150.54
196.56
85.69

63.32
64.42
34.03
36.35
1010.97 379.27
24.46 260.24
522.99 270.41
23.35
161.9
277.86 185.53
21.00
97.11

57.63
33.64
686.43
55.93
397.52
47.53
236.7
37.58

46.61
27.65
599.14
45.45
332.00
42.19
197.59
35.75

RMSE
MAE
MIS (95% CI)
Interval
MIS (90% CI)
Interval
MIS (80% CI)
Interval

67.70
40.33
1028.63
39.95
534.29
39.95
307.14
29.87

72.92
44.29
411.05
242.47
315.96
170.12
237.90
96.07

72.52
41.24
1292.98
24.16
664.50
23.09
349.00
20.81

73.65
43.15
402.46
291.96
304.12
184.03
220.60
111.79

70.03
41.26
905.66
62.39
515.15
53.10
300.09
42.03

59.27
34.62
821.71
46.16
443.94
43.07
254.54
36.63

RMSE
MAE
MIS (95% CI)
Interval
MIS (90% CI)
Interval
MIS (80% CI)
Interval

68.63
41.71
1035.26
43.61
539.43
43.61
315.05
32.32

73.92
46.20
455.27
262.09
359.69
190.23
262.76
105.6

69.94
41.79
1303.02
23.85
669.76
22.79
351.96
20.58

72.44
44.45
428.82
316.13
343.83
206.27
252.51
128.51

70.60
42.28
891.45
67.52
512.72
57.50
302.66
45.56

70.57
40.66
852.26
47.58
458.94
44.65
261.32
38.03

Table 2: Performance comparison of different approaches on Autoregressive DeepGLEAM Model
for COVID-19 mortality forecasting.

4

Conclusion

In this paper, we introduce DeepGLEAM, a hybrid model for COVID-19 forecasting. DeepGLEAM combines a mechanistic stochastic simulation model, GLEAM, with deep learning. It
uses deep learning to learn the correction terms from GLEAM, which leads to improved performance. By integrating various uncertainty quantification methods, we demonstrate DeepGLEAM
on real-world COVID-19 mortality forecasting tasks in both point estimation and uncertainty quan11

Truth
US-PA
400

400

200

200

0

23

27

32

37

US-VA

400

weekly death

27

23

27

32

37

0

32

37

32

37

0

32

37

37

37

150
100
50
23

27

32

37

0
125
100
75
50
25
0

0

37

32

37

0

32

37

23

27

32

37

80
40
20
37

0

27

32

37

0

23

27

32

37

125
100
75
50
25
0

125
100
75
50
25
0

0
250
200
150
100
50
0

27

32

37

US-NV

23

27

32

37

23

32

27

37

32

37

US-WY

23

27

300

200

200

0

27

200

150

150

100

32

37

0
125
100
75
50
25
0
125
100
75
50
25
0

32

37

US-WA

27

32

37

0

32

37

0

32

37

23

27

100

0

23

27

37

32

37

0
125
100
75
50
25
0

32

37

US-CT

100
27

32

37

0

23

27

32

37

US-OR

32

37

0

23

27

US-SD

32

37

32

37

0

37

23

27

32

37

32

37

32

37

US-UT

0

23

27

US-MT
100

50
27

32

US-AR

US-ND
100

23

27

50

50
27

23

100

100

23

0

250
200
150
100
50
0
150

150

50
32

50
0

200

100

200

23

100

27

200

300

150

23

300

400

200

100

37

37

US-TN

300

US-RI

US-WV

32

32

US-MO

100
27

150

27

27

200

23

US-NH

23

23

300

50
23

0
400

SG-MCMC

US-MD

100
23

US-NM

50
27

300

US-OK

100

23

400

250
200
150
100
50
0

Quantile

US-AL

400

100
23

US-KY

US-NE

US-AK

32

23

US-KS

60

27

0

50
27

US-ME

23

37

100

23

50
27

32

US-MN

200

100

23

27

150

150

100
32

US-HI

32

50
32

100

0

27

100

27

100
23

100
23

150

23

200

200

200

200

27

0

0

US-MS

300

300

US-CO

US-DE

50

37

100
27

US-ID
150

32

200

23

50

50

23

27

300

100

100

0

23

US-MI

150

27

0

US-NC

400

200

US-WI

150

23

400

400

US-IA

200

300

US-IN

100

100

US-OH
600

100
23

200

200

0

0

US-LA
400
200

300

300

0

US-SC

600

600

50
23

27

32

37

0

23

27

US-VT

23

27

week number

Figure 7: One week ahead COV-19 prediction for the rest 41 states.
tification. The experimental results show that DeepGLEAM outperforms baseline methods including Vector Autoregressive model, deep learning model, and GLEAM respectively.

12

References
[1] Socioeconomic Data and Applications
http://sedac.ciesin.columbia.edu/gpw.

Center

(SEDAC),

Columbia

University

[2] Duygu Balcan, Vittoria Colizza, Bruno GoncÃßalves, Hao Hu, JoseÃÅ J Ramasco, and Alessandro
Vespignani. Multiscale mobility networks and the spatial spreading of infectious diseases.
Proceedings of the National Academy of Sciences, 106(51):21484‚Äì21489, 2009.
[3] Duygu Balcan, Bruno GoncÃßalves, Hao Hu, JoseÃÅ J Ramasco, Vittoria Colizza, and Alessandro
Vespignani. Modeling the spatial spread of infectious diseases: The GLobal Epidemic and
Mobility computational model. Journal of computational science, 1(3):132‚Äì145, 2010.
[4] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In International Conference on Machine Learning, pages 1613‚Äì
1622, 2015.
[5] George EP Box, Gwilym M Jenkins, Gregory C Reinsel, and Greta M Ljung. Time series
analysis: forecasting and control. John Wiley & Sons, 2015.
[6] Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather
review, 78(1):1‚Äì3, 1950.
[7] Matteo Chinazzi, Jessica T Davis, Marco Ajelli, Corrado Gioannini, Maria Litvinova, Stefano
Merler, Ana Pastore y Piontti, Kunpeng Mu, Luca Rossi, Kaiyuan Sun, et al. The effect of
travel restrictions on the spread of the 2019 novel coronavirus (COVID-19) outbreak. Science,
2020.
[8] Estee Y Cramer, Evan L Ray, Velma K Lopez, Johannes Bracher, Andrea Brennen, Alvaro
J Castro Rivadeneira, Aaron Gerding, Tilmann Gneiting, Katie H House, Yuxin Huang, et al.
Evaluation of individual and ensemble probabilistic forecasts of covid-19 mortality in the us.
medRxiv, 2021.
[9] Jessica T Davis, Matteo Chinazzi, Nicola Perra, Kunpeng Mu, Ana Pastore y Piontti, Marco
Ajelli, Natalie E Dean, Corrado Gioannini, Maria Litvinova, Stefano Merler, et al. Estimating
the establishment of local transmission and the cryptic phase of the covid-19 pandemic in the
usa. medRxiv, 2020.
[10] Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert D Skeel, and Hartmut
Neven. Bayesian sampling using stochastic gradient thermostats. In Advances in neural
information processing systems, pages 3203‚Äì3211, 2014.
[11] Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard to
track covid-19 in real time. The Lancet infectious diseases, 20(5):533‚Äì534, 2020.
[12] M. Dusenberry, G. Jerfel, Y. Wen, Y.-A. Ma, J. Snoek, K. Heller, B. Lakshminarayanan, and
D. Tran. Efficient and scalable Bayesian neural nets with rank-1 factors. In Proceedings of
the 37th International Conference on Machine Learning, pages 9823‚Äì9833. 2020.
13

[13] Bradley Efron and Trevor Hastie. Computer Age Statistical Inference, volume 5. Cambridge
University Press, 2016.
[14] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing
model uncertainty in deep learning. In international conference on machine learning, pages
1050‚Äì1059, 2016.
[15] Yarin Gal, Jiri Hron, and Alex Kendall. Concrete dropout. In Advances in Neural Information
Processing Systems, pages 3581‚Äì3590, 2017.
[16] Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas, Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting with spline quantile
function RNNs. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 1901‚Äì1910, 2019.
[17] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359‚Äì378, 2007.
[18] Alex Graves. Practical variational inference for neural networks. In Advances in neural
information processing systems, pages 2348‚Äì2356, 2011.
[19] Thomas M Hamill and Daniel S Wilks. A probabilistic forecast contest and the difficulty in
assessing short-range forecast uncertainty. Weather and Forecasting, 10(3):620‚Äì631, 1995.
[20] Jonathan Heek and Nal Kalchbrenner. Bayesian inference for large scale image classification.
arXiv:1908.03491, 2019.
[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980, 2014.
[22] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. 2016.
[23] Danijel Kivaranovic, Kory D Johnson, and Hannes Leeb. Adaptive, distribution-free prediction intervals for deep networks. In International Conference on Artificial Intelligence and
Statistics, pages 4346‚Äì4356. PMLR, 2020.
[24] R. Koenker and G. Bassett Jr. Regression quantiles. Econometrica: Journal of the Econometric Society, pages 33‚Äì50, 1978.
[25] Roger Koenker. Quantile Regression. Econometric Society Monographs. Cambridge University Press, 2005.
[26] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural
network: Data-driven traffic forecasting. In International Conference on Learning Representations (ICLR), 2018.
[27] Christos Louizos and Max Welling. Structured and efficient variational deep learning with
matrix gaussian posteriors. In International Conference on Machine Learning, pages 1708‚Äì
1716, 2016.
14

[28] Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient mcmc. In
Advances in Neural Information Processing Systems, pages 2917‚Äì2925, 2015.
[29] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon
Wilson. A simple baseline for Bayesian uncertainty in deep learning. In Advances in Neural
Information Processing Systems, pages 13153‚Äì13164, 2019.
[30] James E Matheson and Robert L Winkler. Scoring rules for continuous probability distributions. Management science, 22(10):1087‚Äì1096, 1976.
[31] Andrew C. Miller, Nicholas J. Foti, Joseph A. Lewnard, Nicholas P. Jewell, Carlos Guestrin,
and Emily B. Fox. Mobility trends provide a leading indicator of changes in sars-cov-2
transmission. medRxiv 2020.05.07.20094441, 2020.
[32] Dina Mistry, Maria Litvinova, Matteo Chinazzi, Laura Fumanelli, Marcelo FC Gomes,
Syed A Haque, Quan-Hui Liu, Kunpeng Mu, Xinyue Xiong, M Elizabeth Halloran, et al.
Inferring high-resolution human mixing patterns for disease modeling. Nature Communications, 12(1):323, Jan 2021.
[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. In Advances in neural information processing
systems, pages 8026‚Äì8037, 2019.
[34] Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy Neely. High-quality prediction
intervals for deep learning: A distribution-free, ensembled approach. In International Conference on Machine Learning, pages 4075‚Äì4084. PMLR, 2018.
[35] Xin Qiu, Elliot Meyerson, and Risto Miikkulainen. Quantifying point-prediction uncertainty
in neural networks via residual estimation with an i/o kernel. In International Conference on
Learning Representations, 2019.
[36] Evan L Ray, Nutcha Wattanachit, Jarad Niemi, Abdul Hannan Kanji, Katie House, Estee Y
Cramer, Johannes Bracher, Andrew Zheng, Teresa K Yamana, Xinyue Xiong, et al. Ensemble
forecasts of coronavirus disease 2019 (covid-19) in the us. MedRXiv, 2020.
[37] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
[38] Xiaocheng Shang, Zhanxing Zhu, Benedict Leimkuhler, and Amos J Storkey. Covariancecontrolled adaptive langevin thermostat for large-scale bayesian sampling. In C. Cortes, N. D.
Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information
Processing Systems 28, pages 37‚Äì45. 2015.
[39] Filippo Simini, Marta C GonzaÃÅlez, Amos Maritan, and Albert-LaÃÅszloÃÅ BarabaÃÅsi. A universal
model for mobility and migration patterns. Nature, 484(7392):96‚Äì100, 2012.
[40] Natasa Tagasovska and David Lopez-Paz. Single-model uncertainties for deep learning. In
Advances in Neural Information Processing Systems, pages 6417‚Äì6428, 2019.
15

[41] Michele Tizzoni, Paolo Bajardi, Chiara Poletto, JoseÃÅ J Ramasco, Duygu Balcan, Bruno
GoncÃßalves, Nicola Perra, Vittoria Colizza, and Alessandro Vespignani. Real-time numerical forecast of global epidemic spreading: case study of 2009 a/h1n1pdm. BMC medicine,
10(1):165, 2012.
[42] Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng Gao, and S Yu Philip. Predrnn:
Recurrent neural networks for predictive learning using spatiotemporal lstms. In Advances in
Neural Information Processing Systems, pages 879‚Äì888, 2017.
[43] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics.
In Proceedings of the 28th international conference on machine learning (ICML-11), pages
681‚Äì688, 2011.
[44] Shi Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun
Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances in neural information processing systems, pages 802‚Äì810, 2015.
[45] Ana Pastore y Piontti, Nicola Perra, Luca Rossi, Nicole Samay, and Alessandro Vespignani.
Charting the Next Pandemic: Modeling Infectious Disease Spreading in the Data Science
Age. Springer, 2018.
[46] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks:
a deep learning framework for traffic forecasting. In Proceedings of the 27th International
Joint Conference on Artificial Intelligence, pages 3634‚Äì3640, 2018.
[47] Qian Zhang, Nicola Perra, Daniela Perrotta, Michele Tizzoni, Daniela Paolotti, and Alessandro Vespignani. Forecasting seasonal influenza fusing digital indicators and a mechanistic
disease model. In Proceedings of the 26th international conference on world wide web,
pages 311‚Äì319, 2017.
[48] Qian Zhang, Kaiyuan Sun, Matteo Chinazzi, Ana Pastore y Piontti, Natalie E Dean, Diana Patricia Rojas, Stefano Merler, Dina Mistry, Piero Poletti, Luca Rossi, et al. Spread of Zika virus
in the Americas. Proceedings of the National Academy of Sciences, 114(22):E4334‚ÄìE4343,
2017.
[49] Lingxue Zhu and Nikolay Laptev. Deep and confident prediction for time series at uber. In
2017 IEEE International Conference on Data Mining Workshops (ICDMW), pages 103‚Äì110.
IEEE, 2017.

16

A

Comparison among methods
Method

Parallel computing resource

Bootstrap
Quantile
MIS
MC Dropout
SG-MCMC

25
1
1
1
25

Small sample

Asymptotic consistency

Accuracy

X

X
X
X
X
XX

X

X

Uncertainty
X
XX
X

Table 3: Comparison of different deep uncertainty quantification methods for forecasts. Double
check marks represent robustly highest performance in experiments.

B
B.1

COVID-19 forecasting experiment
Global Epidemic and Mobility Model.

The Global Epidemic and Mobility model (GLEAM) is a stochastic spatial epidemic model in
which the world is divided into over 3,200 geographic subpopulations constructed using a Voronoi
tessellation of the Earth‚Äôs surface. Subpopulations are centered around major transportation hubs
(e.g. airports) and consist of cells with a resolution of approximately 25 x 25 kilometers [2, 3, 41,
48, 7, 9]. High resolution data are used to define the population of each cell [1]. Other attributes
of individual subpopulations, such as age-specific contact patterns, health infrastructure, etc., are
added according to available data [32].
GLEAM integrates a human mobility layer - represented as a network - that uses both shortrange (i.e. commuting) and long-range (i.e. flights) mobility data from the Offices of Statistics
for 30 countries on 5 continents as well as the Official Aviation Guide (OAG) and IATA databases
(updated in 2020). The air travel network consists of the daily passenger flows between airport
pairs (origin and destination) worldwide mapped to the corresponding subpopulations. Where
information is not available, the short-range mobility layer is generated synthetically by relying on
the ‚Äúgravity law‚Äù or the more recent ‚Äúradiation law‚Äù both calibrated using real data [39].
The model is calibrated to realistically describe the evolution of the COVID-19 pandemic as
detailed in [7, 9]. Lastly, GLEAM is stochastic and produces an ensemble of possible epidemic
outcomes for each set of initial conditions. To account for the potentially different reporting levels
of the states, a free parameter Infection Fatality Rate (IFR) multiplier is added to each model. To
calibrate and select the most reasonable outcomes, we filter the models by the latest hospitalization
trends and confirmed cases trends, and then we select and weight the filtered models using Akaike
Information Criterion [47]. The forecast of the evolution of the epidemic is formed by the final
ensemble of the selected models.
DCRNN setup The model has one hidden layer of RNN with 8 units to overcome the overfitting
problem. The filter type is Laplacian and the diffusion step is 1. The base learning rate of DCRNN
17

is 1e‚àí2 and decay to 1e‚àí3 at epoch 13 with Adam optimizer [21]. We have a strict early stopping
policy to deal with the overfitting problem. The training stops as the validation error does not
improve for three epochs after epoch 13.
Bootstrap For Bootstrap method, due to the small sample we have (typically 25), we only randomly dropped 1 training data while keeping the original validation and testing data. We obtain 25
samples for constructing mean prediction and confidence interval.
Quantile regression We apply pinball loss function [24, 25] to train three different quantiles
for the corresponding confidence interval (CI), e.g., (0.025, 0.5, 0.975) for 95%. The model and
learning rate setup is the same as DCRNN. The result for comparison averages the performance of
10 trails.
SQ regression We use spline quantile function to approximate a quantile function and use the
CRPS as the loss function. For every point prediction, there are 11 trained parameters to construct
the quantile function. The 1st parameter is the intercept term. The next 5 can be transformed to
the slopes of 5 line segments. The last 5 can be transformed to a vector of the 5 knots‚Äô positions.
The model and learning rate setup is the same as DCRNN. The result for comparison averages the
performance of 10 trails.
MIS regression We combine MAE with MIS and directly minimize this loss function. The
model and learning rate setup is the same as DCRNN. The result for comparison averages the
performance of 10 trails.
MC Dropout The training process of MC Dropout is the same as DCRNN. We implement the
algorithm provided by [49] and simplify the model by only considering the model uncertainty.
We apply random dropout through the testing process with 5% drop rate and iterate 300 times to
achieve a stable prediction. The result for comparison averages the performance of 10 trails.
SG-MCMC To generate samples of model parameters Œ∏ (with a slight abuse of notation) according to SG-MCMC. In this task of complex time-series, the temperature of system is hard to
be tuned. Stochastic Gradient Nose-Hoover Thermostats (SGNHT) could avoid this problem in
training. We first denote the loss function (or the negative log-likelihood) over a minibatch of data
e
as L(Œ∏).
We then introduce hyper-parameters including the diffusion coefficients A and the learning rate h and make use of auxiliary variables p ‚àà Rd and Œæ ‚àà R in the algorithm. We randomly
initialize Œ∏, p, and Œæ and update according to the following update rule.
Ô£±
Ô£¥
Ô£≤ Œ∏k+1 = Œ∏k + pk h
e
pk+1 = pk ‚àí ‚àáL(Œ∏)h
‚àíŒæk pk h + N (0, 2Ah)
(10)
T
Ô£¥
Ô£≥ Œæk+1 = Œæk + pk pk ‚àí 1 h.
d

18

Upon convergence of the above algorithm at K-th step, Œ∏K follows the distribution of the posterior.
We run parallel chains to generate different samples according to the posterior and quantify the
predictions uncertainty.
The learning rate of SGNHT is 5e‚àí4 , and we selected a Gaussian prior N (0, 2.0) with random
initialization as N (0, 0.05). We apply training for 800 epochs, and it early stops as long as the
validation error does not improve for 50 epochs. Our result is averaged from 25 posterior samples.

C

Performance of Ensemble DeepGLEAM model

In addition to autoregressive forecasting, we also build an ensemble DeepGLEAM model by training based on the ensembled data along the forecasting horizon as the output. We use the ensemble
DeepGLEAM model which trains four different models separately and predicts the future four
weeks death prediction respectively. Table 4 shows the performance of the UQ methods. We note
here the autoregressive DeepGLEAM outperforms Ensemble DeepGLEAM in both Frequentist
and Bayesian UQ methods in the first three weeks. For longer time-series predictions, the accumulated bias from autoregressive model may negatively affect the performance compared to the
separately trained ensemble model. In the context of COVID-19 prediction, autoregressive DeepGLEAM is a better choice.
T
1W
2W
3W
4W

Metric
RMSE
MIS (95% CI)
RMSE
MIS (95% CI)
RMSE
MIS (95% CI)
RMSE
MIS (95% CI)

Bootstrap Quantile
70.73
1216.63
61.69
1135.01
73.27
1464.52
72.41
1482.08

70.26
430.72
60.94
331.94
71.72
392.66
73.44
418.77

SQ

MAE-MIS

73.82
1299.78
65.44
1272.82
73.88
1559.62
70.44
1586.95

76.32
424.49
65.39
368.20
74.95
416.23
75.16
468.49

MC Drpoout SG-MCMC
68.58
831.86
59.27
796.54
70.32
997.65
72.24
1034.68

62.42
715.40
51.21
727.27
66.72
930.55
68.40
971.55

Table 4: Performance comparison of different approaches on Ensemble DeepGLEAM Model for
COVID-19 mortality forecasting.

19

