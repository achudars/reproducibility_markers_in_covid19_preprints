Chest X-ray lung and heart segmentation based on minimal training
sets
Balázs Maga *

arXiv:2101.08309v1 [eess.IV] 20 Jan 2021

January 22, 2021

Abstract
As the COVID-19 pandemic aggravated the excessive workload of doctors globally, the demand for computer
aided methods in medical imaging analysis increased even further. Such tools can result in more robust diagnostic
pipelines which are less prone to human errors. In our paper, we present a deep neural network to which we refer
to as Attention BCDU-Net, and apply it to the task of lung and heart segmentation from chest X-ray (CXR) images,
a basic but ardous step in the diagnostic pipeline, for instance for the detection of cardiomegaly. We show that
the fine-tuned model exceeds previous state-of-the-art results, reaching 98.1 ± 0.1% Dice score and 95.2 ± 0.1%
IoU score on the dataset of Japanese Society of Radiological Technology (JSRT). Besides that, we demonstrate the
relative simplicity of the task by attaining surprisingly strong results with training sets of size 10 and 20: in terms of
Dice score, 97.0 ± 0.8% and 97.3 ± 0.5, respectively, while in terms of IoU score, 92.2 ± 1.2% and 93.3 ± 0.4%,
respectively. To achieve these scores, we capitalize on the mixup augmentation technique, which yields a remarkable
gain above 4% IoU score in the size 10 setup.

1

Introduction

All around the world, a plethora of radiographic examinations are performed day to day, producing images using
different imaging modalities such as X-ray, computed tomography (CT), diagnostic ultrasound and magnetic resonance
imaging (MRI). According to the publicly available, official data of the National Health Service ([13]), in the period
from February 2017 to February 2018, the count of imaging activity was about 41 million only in England. The
thorough examination of the vast quantity of these images imposes a huge workload on radiologists, which increases
the number of the avoidable human mistakes. Consequently, automated methods aiding the diagnostic processes are
sought-after.
The examination of medical images customarily includes various segmentation tasks, in which detecting and pixelwise annotating different tissues and certain anomalies are vital. Common examples include lung nodule segmentation
in the diagnosis of lung cancer, lung and heart segmentation in the diagnosis of cardiomegaly, or plaque segmentation in the diagnosis of thrombosis. Even in the case of 2-dimensional modalities, such segmentation tasks can be
extremely time-demanding, and the situation gets even worse in three dimension. Taking into consideration that these
tasks are easier to formalize as a standard computer vision exercise than the identification of a particular disease, it is
not surprising that they sparked much activity in the field of automated medical imaging analysis.
Semantic segmentation – that is assigning a pre-defined class to each pixel of an image – requires a high level
of visual understanding, in which state-of-the-art performance is attained by methods utilizing Fully Convolutional
Networks (FCN) [4]. An additional challenge of the field is posed by the strongly limited quantity of training data
on which one train machine learning models, as annotating medical imaging requires specialists in contrast to “reallife” images. To overcome this difficulty, the so-called U-Net architecture was proposed: its capability to being
efficiently trained on small datasets has been demonstrated in [5]. Over the past few years several modifications and
improvements have been proposed on the original architecture, some of which involved different attention mechanisms
designed to help the network to detect the important parts of the images.
* Eötvös

Loránd University, Pázmány Péter sétány 1/C, Budapest, H-1117 Hungary, email: mbalazs0701@gmail.com

1

In the present paper we introduce a new network primarily based on the ideas of [12] and [8], to which we refer to
as Attention BCDU-Net. We optimize its performance through hyperparameter tests on the depth of the architecture
and the loss function, and we demonstrate the superiority of the resulting model compared to the state-of-the-art
network presented in [15] in the task of lung and heart segmentation on chest X-rays. Besides that, we will also give
an insight into two interesting phenomena arising during our research which might be interesting for the AI medical
imaging community: one of them is the very small data requirement of this particular task, while the other one is the
peculiar evolution of the loss curves over the training.

2
2.1

Deep learning approach
Related work

As already mentioned in Section 1, [5] introducing U-Nets is of paramount importance in the field. Since then UNets have been used to cope with diverse medical segmentation tasks, and numerous papers aimed to design U-Net
variants and mechanisms such that the resulting models tackles better the problem considered. Some of these paid
primary attention to the structure of the encoder and the decoder – that is the downsampling and the upsampling
path – of the original architecture. For example in [18], the authors developed a network (CoLe-CNN) with multiple
decoder branches and Inception-v4 inspired encoder to achieve state-of-the-art results in 2-dimensional lung nodule
segmentation. In [10] and [14], the authors introduced U-Net++, a network equipped with intermediate upsampling
paths and additional convolutional layers, leading to essentially an efficient ensemble of U-Nets of varying depths, and
demonstrated its superiority compared to the standard U-Net in many image domains. Other works put emphasis on the
design of skip connections and the way the higher resolution semantic information joins the features coming through
the upsampling branch. In [12], the authors proposed the architecture BCDU-Net, in which instead of the simple
concatenation of the corresponding filters, the features of different levels are fused using a bidirectional ConvLSTM
layer, which introduces nonlinearity into the model at this point and makes more precise segmentations available.
In [8] it has been shown that for medical image analysis tasks the integration of so-called Attention Gates (AGs)
improved the accuracy of the segmentation models, while preserving computational efficiency. In [15], this network
was enhanced by a critic network in a GAN-like scheme following [9], and achieved state-of-the-art results in the task
of lung and heart segmentation. Other attention mechanisms were introduced in [17] and in [16].

2.2

Our proposal

The network architecture Attention BCDU-Net we propose is a modification of the Attention U-Net, shown at Figure
1.

Figure 1: Schematic architecture of Attention U-Net [8].

2

Figure 2: Schematic figure of the attention gate used in Attention U-Net [8], the tensor addition to alter is highlighted
by an arrow.
In [12], the authors demonstrated that it is beneficial to use bidirectional ConvLSTM layers to introduce nonlinearity in the step of merging semantic information gained through skip connections and the features arriving through
the decoder. This inspired us to modify the attention gates (see Figure 2) in a similar manner, in which these pieces
of information are merged via tensor addition, that is a linear operation as well. This addition is replaced by a bidirectional ConvLSTM layer, to which the output of Wg and Wx – the processed features and the semantic information,
respectively – is fed in this order. We note that to our best knowledge, there is a slight ambiguity about the structure of
the resampling steps in the attention gate: while the official implementation is in accordance with the figure, there are
widely utilized implementations in which the output of Wg is upsampled instead of downsampling the output of Wx
in order to fit their shape. We tested both solutions and did not experience a measurable difference in the performance.
We also experimented with the usage of additional spatial and channel attention layers as proposed by [17], however,
we found that it does not improve the performance of our model.
The depth of the network is to be determined by hyperparameter testing. Our tests confirmed that four downsampling steps results in the best performance, however, the differences are minuscule.

2.3

Loss function

A standard score to compare segmentations is the Intersection over Union (IoU): given two sets of pixels X, Y , their
IoU is
|X ∩ Y |
.
IoU (X, Y ) =
|X ∪ Y |
In the field of medical imaging, Dice Score Coefficient (DSC) is probably the most widespread and simple way to
measure the overlap ratio of the masks and the ground truth, and hence to compare and evaluate segmentations. It is a
slight modification of IoU: given two sets of pixels X, Y , their DSC is
DSC(X, Y ) =

2|X ∩ Y |
.
|X| + |Y |

If Y is in fact the result of a test about which pixels are in X, we can rewrite it with the usual notation true/false
positive (TP/FP), false negative (FN) to be
DSC(X, Y ) =

2T P
.
2T P + F N + F P

We would like to use this concept in our setup. The class c we would like to segment corresponds to a set, but it is
more appropriate to consider its indicator function g, that is gi,c ∈ {0, 1} equals 1 if and only if the ith pixel belongs
to the object. On the other hand, our prediction is a probability for each pixel denoted by pi,c ∈ [0, 1]. Then the Dice
Score of the prediction in the spirit of the above description is defined to be
PN
pi,c gi,c + ε
DSC = PN i=1
,
i=1 (pi,c + gi,c ) + ε
where N is the total number of pixels, and ε is introduced for the sake of numerical stability and to avoid divison by 0.
The IoU of the prediction can be calculated in a similar manner. The linear Dice Loss (DL) of the multiclass prediction
is then
X
DL =
(1 − DSCc ) .
c

3

A deficiency of Dice Loss is that it penalizes false negative and false positive predictions equally, which results in high
precision but low recall. For example practice shows that if the region of interests (ROI) are small, false negative pixels
need to have a higher weight than false positive ones. Mathematically this obstacle is easily overcome by introducing
weights α, β as tuneable parameters, resulting in the definition of Tversky similarity index [1]:
N
X

T Ic =

N
X
i=1

pi,c gi,c + α

i=1
N
X

pi,c gi,c + ε
pi,c gi,c + β

N
X

,
pi,c gi,c + ε

i=1

i=1

where pi,c = 1 − pi,c and gi,c = 1 − gi,c , that is the overline simply stands for describing the complement of the class.
Tversky Loss is obtained from Tversky index as Dice Loss was obtained from Dice Score Coefficient:
X
TL =
(1 − T Ic ) .
c

Another issue with the Dice Loss is that it struggles to segment small ROIs as they do not contribute to the loss
significantly. This difficulty was addressed in [11], where the authors introduced the quantity Focal Tversky Loss in
order to improve the performance of their lesion segmentation model:
X
γ −1
FTL =
(1 − T Ic )
,
c

where γ ∈ [1, 3]. In practice, if a pixel with is misclassified with a high Tversky index, the Focal Tversky Loss is
unaffected. However, if the Tversky index is small and the pixel is misclassified, the Focal Tversky Loss will decrease
significantly.
In our work we use multiclass DSC and IoU to evaluate segmentation performance. As our initial tests demonstrated that training our network with Focal Tversky loss results in better scores, we will use this loss function. The
optimal α, β, γ parameters should be determined by extensive hyperparameter testing and grid search. We worked
below with α = 0.6, β = 0.4, γ1 = 0.675.

2.4

Dataset and preprocessing

For training- and validation data, we used the public Japanese Society of Radiological Technology (JSRT) dataset
([3]), available at [2]. The JSRT dataset contains a total of 247 images, all of them are in 2048 × 2048 resolution, and
have 12-bit grayscale levels. Both lung and heart segmentation masks are available for this dataset.
In terms of preprocessing, similarly to [15], the images were resized to the resolution 512 × 512 first. As X-rays
are grayscale images with typically low contrast, which makes their analysis a difficult task. This obstacle might be
overcome by using some sort of histogram equalization technique. The idea of standard histogram equalization is
spreading out the the most frequent intensity values to a higher range of the intensity domain by modifying the intensities so that their cumulative distribution function (CDF) on the complete modified image is as close to the CDF of the
uniform distribution as possible. Improvements might be made by using adaptive histogram equalization, in which the
above method is not utilized globally, but separately on pieces of the image, in order to enhance local contrasts. However, this technique might overamplify noise in near-constant regions, hence our choice was to use Contrast Limited
Adaptive Histogram Equalization (CLAHE), which counteracts this effect by clipping the histogram at a predefined
value before calculating the CDF, and redistribute this part of the image equally among all the histogram bins.

2.5

Data augmentation

Concerning data augmentation, we follow [7], in which the method mixup was used to improve glioma segmentation
on brain MRI’s. This slightly counter-intuitive augmentation technique was introduced by [6]: training data samples
4

are obtained by taking random convex combinations of original image-mask pairs. That is, for (x1 , y1 ) and (x2 , y2 )
image-mask pairs, we create a random mixed up pair x = λx1 +(1−λ)x2 , y = λy1 +(1−λ)y2 , where λ is chosen from
the beta distribution B(δ, δ) for some δ ∈ (0, ∞). In each epoch, the original samples are paired randomly, hence
during the course of the training, a multitude of training samples are fed to the network. (From the mathematical
point of view, as the coefficient λ is chosen independently in each case from a continuous probability distribution,
the network will encounter pairwise distinct mixed up training samples with probability 1, modulo floating point
inaccuracy.) In [6], the authors argue that generating training samples via this method encourages the network to
behave linearly in-between training examples, which reduces the amount of undesirable oscillations when predicting
outside the training examples.
The choice of δ should be determined by hyperparameter testing for any network and task considered. In [6],
δ ∈ [0.1, 0.4] is proposed, while in [7] δ = 0.4 is applied.

3
3.1

Experiments
Training schedule

In our main tests, the JSRT dataset was randomly split so that 85% of it was used for training and the rest for validation
and testing. This split was carried out independently in each case, enhancing the robustness of our results. Besides
that, we also experimented with small dataset training, in which rather modest sets of 10 and 20 X-rays was utilized
as training set. (The test set remained the same.) It enabled us to measure the benefits of mixup more transparently.
In each of these cases, we trained our network with Adam optimizer: in the former case, for 50 epochs, while in the
latter cases for 1000 and 500 epochs, respectively. As these epoch numbers are approximately inversely proportional
to the size of the training sets, these choices correspond to each other in terms of training steps.

3.2

Results

Table 1 summarizes the numerical results we obtained during the testing of Attention BCDU-Net with different train
sizes and choices of δ, while Figure 3-5 display visual results. Note that the highest DSC scores slightly exceed the
ones attained by the state-of-the-art, adversarially enhanced Attention U-Net introduced in [15] (97.6 ± 0.5%) and
admit higher stability. The effect of augmentation is the most striking in the case of training on an X-ray set of size
10, when the choice δ = 0.2 results in a 5% increase of IoU compared to the no mixup case. In general, we found this
case particularly interesting: it was surprising that we could achieve IoU and DSC scores of this magnitude using such
a small training set. Nevertheless the predictions have some imperfections, displayed by Figure 3: the contours of the
segmentation are less clear and both the heart and the lung segmentation tend to contain small spots far from the ground
truth. However, such conspicuous faults are unlikely to occur in the case of the best models for 20 train X-rays (Figure
4), which is still remarkable. The sufficiency of such small training sets is probably due to the relative simplicity of
the task. Notably, lung and heart regions admit large similarity across a set of chest X-rays, and they are strongly
correlated with simple intensity thresholds. Consequently, even small datasets have high representing potential. We
note that as δ gets smaller, the probability density function of B(δ, δ) gets more strongly skewed towards the endpoints
of the interval [0, 1], which results in mixed up samples being closer to original samples in general. The perceived
optimality of δ = 0.2 in the small dataset cases show that a considerable augmentation is beneficial and desirable, yet
it is unadvised to use too wildly modified samples.
The benevolent effect of mixup gets more obscure as we increase the size of the training set. Notably, the results
of different augmentation setups are almost indistinguishable from each other. We interpret this phenomena as another
consequence of the similarity of masks from different samples, which inherently drives the network towards simpler
representations in the case of a sufficiently broad training set, even without using mixup.
We also note that in the case of 10 training samples, while the IoU differences between the no mixup and the
mixup regime are striking, the gain in DSC is less remarkable. It hints that it is unadvised to rely merely on DSC when
evaluating and comparing segmentation models.

5

Figure 3: Ground truth (left) compared to the prediction of the Attention BCDU-Net (right), train size: 10.

Figure 4: Ground truth (left) compared to the prediction of the Attention BCDU-Net (right), train size: 20.

Figure 5: Ground truth (left) compared to the prediction of the Attention BCDU-Net (right), train size: complete.

No mixup
δ = 0.1
δ = 0.2
δ = 0.3
δ = 0.4

Train size: 10
IoU
DSC
87.2 ± 1.9% 94.8 ± 1.1%
91.9 ± 1.3% 96.8 ± 0.9%
92.2 ± 1.2% 97.0 ± 0.8%
91.3 ± 1.2% 96.5 ± 1.0%
91.3 ± 1.4% 96.4 ± 1.0%

Train set: 20
IoU
DSC
91.9 ± 0.6% 96.9 ± 0.5%
92.5 ± 0.5% 97.1 ± 0.5%
93.3 ± 0.4% 97.3 ± 0.5%
92.9 ± 0.5% 97.2 ± 0.5%
93.0 ± 0.5% 97.2 ± 0.4%

Train size: 209 (Complete)
IoU
DSC
94.9 ± 0.2% 98.0 ± 0.1%
95.2 ± 0.2% 98.1 ± 0.1%
95.0 ± 0.1% 98.0 ± 0.1%
94.9 ± 0.1% 98.0 ± 0.1%
94.8 ± 0.1% 97.9 ± 0.1%

Table 1: Dice scores and IoU scores of Attention BCDU-Net with different mixup parameters
We would also like to draw attention to the peculiar loss curves we primarily encountered during the small dataset
trainings, as displayed in Figure 6. Notably, the curve of the validation DSC flattens far below the also flattening curve
of the train DSC, strongly inciting the usage of early stopping. (Train DSC reaches essentially 1 in fact, which is
unsurprising with such a small training set.) However, in the later stages the validation DSC catches up, even though
the train DSC does not have any room for further improvement. We were especially puzzled by this behaviour in the
10-sized training setup, in which both the train and validation DSC seems completely stabilized after from epoch 50 to
epoch 400, yet validation DSC skyrockets in the later stages in a very short amount of time. The same behaviour was
experienced during each test run. We have yet to give the intuitive or theoretical explanation for this phenomenon that

6

how the generalizing ability of the model can improve further when it seems to be in a perfect state from the training
perspective. We note that these observations naturally led us to experiment with even longer trainings, but to no avail.

Figure 6: From left to right: the evolution of the train DSC (blue) and the validation DSC (orange) with 10 training
samples, 20 training samples, and the complete training dataset, respectively. The IoU curves admit similar patterns.

4

Conclusion

In the present work, we addressed the problem of automated lung and heart segmentation on chest X-rays. We
introduced a new model, Attention BCDU-Net, a variant of Attention U-Net equipped with modified attention gates,
and surpassed previous state-of-the-art results. We also demonstrated its ability to attain surprisingly reasonable results
with strongly limited training sets. Performance in these cases was enhanced using the mixup augmentation technique,
resulting in highly notable contribution in the IoU score.
Concerning future work, a natural extension of this work would be adding a structure correcting adversarial network to the training scheme, similarly to [9] and [15], and measuring its effect on the performance, especially in the
setup of limited training sets. We would also like to give some kind of explanation to the phenomenon of peculiar loss
curves.

Acknowledgements
The project was supported by the grant EFOP-3.6.3-VEKOP-16-2017-00002.

References
[1]

Amos Tversky. “Features of similarity.” In: Psychological review 84.4 (1977), p. 327.

[2]

http://db.jsrt.or.jp/eng.php. 2000.

[3]

Junji Shiraishi et al. “Development of a digital image database for chest radiographs with and without a lung
nodule: receiver operating characteristic analysis of radiologists’ detection of pulmonary nodules”. In: American
Journal of Roentgenology 174.1 (2000), pp. 71–74.

[4]

Jonathan Long, Evan Shelhamer, and Trevor Darrell. “Fully convolutional networks for semantic segmentation”.
In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2015, pp. 3431–3440.

[5]

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. “U-Net: Convolutional Networks for Biomedical Image Segmentation”. In: Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015: 18th
International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III. Springer International
Publishing, 2015, pp. 234–241. ISBN: 978-3-319-24574-4. DOI: 10.1007/978- 3- 319- 24574- 4_28.
URL: https://doi.org/10.1007/978-3-319-24574-4_28.

[6]

Hongyi Zhang et al. “mixup: Beyond Empirical Risk Minimization”. In: (Oct. 2017).

[7]

Zach Eaton-Rosen et al. “Improving Data Augmentation for Medical Image Segmentation”. In: 2018.
7

[8]

Ozan Oktay et al. “Attention U-Net: Learning where to look for the pancreas”. In: arXiv:1804.03999 (2018).

[9]

Nanqing Dong Wei Dai B et al. “SCAN: Structure Correcting Adversarial Network for Organ Segmentation in
Chest X-Rays”. In: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision
Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in
Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings. Vol. 11045. Springer.
2018, p. 263.

[10]

Zongwei Zhou et al. “UNet++: A Nested U-Net Architecture for Medical Image Segmentation”. In: July 2018.

[11]

Nabila Abraham and Naimul Mefraz Khan. “A novel focal Tversky loss function with improved attention UNet for lesion segmentation”. In: 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019).
IEEE. 2019, pp. 683–687.

[12]

Reza Azad et al. “Bi-Directional ConvLSTM U-Net with Densley Connected Convolutions”. In: 2019 IEEE/CVF
International Conference on Computer Vision Workshop (ICCVW) (2019), pp. 406–415.

[13]

NHS England and NHS Improvement. “Diagnostic imaging dataset statistical release”. In: (2019).

[14]

Zongwei Zhou et al. “UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation”. In: IEEE transactions on medical imaging (Dec. 2019). DOI: 10.1109/TMI.2019.2959609.

[15]

Gusztáv Gaál, Balázs Maga, and András Lukács. “Attention U-Net Based Adversarial Architectures for Chest
X-ray Lung Segmentation”. In: arXiv:2003.10304 (Mar. 2020).

[16]

Trinh Khanh et al. “Enhancing U-Net with Spatial-Channel Attention Gate for Abnormal Tissue Segmentation
in Medical Imaging”. In: Applied Sciences 10 (Aug. 2020). DOI: 10.3390/app10175729.

[17]

Peng Zhao et al. “SCAU-Net: Spatial-Channel Attention U-Net for Gland Segmentation”. In: Frontiers in Bioengineering and Biotechnology 8 (2020), p. 670. ISSN: 2296-4185. DOI: 10.3389/fbioe.2020.00670.
URL: https://www.frontiersin.org/article/10.3389/fbioe.2020.00670.

[18]

Giuseppe Pezzano, Vicent Ribas Ripoll, and Petia Radeva. “CoLe-CNN: Context-learning convolutional neural network with adaptive loss function for lung nodule segmentation”. In: Computer Methods and Programs
in Biomedicine 198 (2021), p. 105792. ISSN: 0169-2607. DOI: https : / / doi . org / 10 . 1016 / j .
cmpb . 2020 . 105792. URL: http : / / www . sciencedirect . com / science / article / pii /
S0169260720316254.

8

