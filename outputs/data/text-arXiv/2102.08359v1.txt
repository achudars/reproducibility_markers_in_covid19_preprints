End-2-End COVID-19 Detection from Breath & Cough Audio
Harry Coppock*1 , Alexander Gaskell*1 , Panagiotis Tzirakis1 , Alice Baird2 , Lyn Jones3 , Björn W.
Schuller1,2
1

GLAM – Group on Language, Audio, & Music, Imperial College London, London, UK
Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany
3
Radiology Department, North Bristol NHS Trust, Bristol, UK

arXiv:2102.08359v1 [cs.SD] 7 Jan 2021

2

Keywords: COVID-19, Computer Audition, Digital Health,
Deep Learning, Audio

SUMMARY BOX
Our main contributions are as follows:
• We demonstrate the first attempt to diagnose COVID19 using end-to-end deep learning from a crowd-sourced
dataset of audio samples, achieving ROC-AUC of 0.846
• Our model, the COVID-19 Identification ResNet,
(CIdeR), has potential for rapid scalability, minimal cost,
and improving performance as more data becomes available. This could enable regular COVID-19 testing at a
population scale
• We introduce a novel modelling strategy using a custom
deep neural network to diagnose COVID-19 from a joint
breath and cough representation
• We release our four stratified folds for cross parameter
optimisation and validation on a standard public corpus
and details on the models for reproducibility and future
reference

INTRODUCTION
The Coronavirus disease 2019 (COVID-19), caused by
the severe-acute-respiratory-syndrome-coronavirus 2 (SARSCoV-2), is the first global pandemic of the 21st century. Since
its emergence in December 2019, it has led to over 75 million
confirmed cases and more than 1.6 million deaths in over 200
countries (WHO) 1 . SARS-CoV-2 causes either asymptomatic

infection or clinical disease, which ranges from mild to lifethreatening [1]. Developing a swift and accurate test, able to
identify both symptomatic and asymptomatic cases, is therefore essential for pandemic control.
Vocal biomarkers of SARS-CoV-2 infection have been described, thought to relate to the clinical and subclinical effects
of the virus on the lower respiratory tract, neuro-muscular
function, senses of taste and smell and on proprioceptive feedback. Together, these produce a reduction in complexity of
the co-ordination of respiratory and laryngeal motion in both
symptomatic and asymptomatic individuals [2].
Recently, several audio applications have been released that
capture the breath or cough of individuals. Examples include
the ‘Coughvid’ [3], ‘Breath for Science’ 2 , ‘Coswara’ [4],
and ‘CoughAgainstCovid’ [5]. With the release of these
datasets, several studies have been published that leverage
breath and/or cough signals alongside machine learning to detect the virus [6, 7, 8, 9, 10, 11]. However, these approaches
try to compute representations of the breath and cough signals
separately. In contrast, our approach computes a joint representation using a single model.
We postulate that end-to-end deep learning using convolutional neural networks (CNNs) could be successfully applied to this assessment task. This article describes a proof
of concept study of automatic symptomatic and asymptomatic
COVID-19 recognition using combined breathing and coughing information from audio recordings using an end-to-end
CNN design. The code for our experiments and all details for
reproduction of findings can be found at https://github.com/
glam-imperial/cider.

METHODS
The objective is supervised learning binary classification for
diagnosing COVID-19 as positive or negative using audio sig-

*Equal contribution
1 https://www.who.int/emergencies/diseases/novel-coronavirus-2019

2 https://www.breatheforscience.com

Harry Coppock et al.

1

Figure 1 A schematic of the COVID-19 Identification ResNet, (CIdeR). The figure shows a blow-up of a residual block, consisting
of convolutional, batch normalisation, and Rectified Linear Unit (ReLU) layers.

nals. Our implementation, displayed in Figure 1, has two distinct stages which are outlined below:
1. Spectrogram extraction As shown in Figure 1, each
participant in the study carried out by the University of Cambridge [6] could submit waveform audio (WAV) files including a breath sample and a cough sample3 . We first compute the
spectrogram of each of these WAV files to obtain a visual representation of the spectrum of audio frequencies against time.
Next, we perform a log transformation, converting the spectrogram from an amplitude representation to a decibel representation. These transformations are implemented using the
librosa [12] python package.
Each WAV file lasts between one and fourty-eight seconds
with a mean of ten seconds. As uniform duration is required
for CNN input, we chunk the whole WAV file into s-second
segments, using right padding for files shorter than s-seconds.
This creates an image of size {F, W }, where F ∝ f f tn and
W ∝ sr ∗ s and f f tn and sr are parameters used when computing the spectrogram. During model training, we only process one WAV segment (sampled uniformly). At inference
time, we perform majority voting, whereby each chunk is processed in parallel, and the output label becomes the modal
classification from each of the chunks4 .
2. Convolutional Neural Network CIdeR is based on
ResNets [13], a variant of the CNN architecture, which uses
residual blocks. As shown in Figure 1, a residual block consists of two convolutions, batch normalisation [14], and a Rectified Linear Unit (ReLU) non-linearity. These blocks use
“skip” connections which add the output from these operations
to the input activations for this layer. This alleviates the vanishing gradient problem, facilitating deeper architectures with
3 Please

see below and at https://www.covid-19-sounds.org/en/app/ for further
details.
4 The mean of the output logits is taken in the case of a tied vote.

2

more layers, thereby permitting richer hierarchical learnt representations. The number of convolutional channels for each
of CIdeR’s nine layers are annotated in Figure 1.
We concatenate the log spectrograms of the breath and
cough samples depth-wise, creating an {F, W, 2} matrix as
the model input. The CNN outputs a single logit which is
then passed through a sigmoid layer to obtain a (0, 1) score,
representing the probability of a COVID-positive sample. A
weighted binary-cross entropy loss function [15] is used during training to address the class imbalance in the dataset.
Training strategy Prior work [6] used “10-fold-like” cross
validation during training (see the paper for details). In contrast, we implement a stratified 3-fold cross optimisation and
additional validation partitioning using 2 / 1 (rotating development + train) / 1 (always held out fixed test) folds, respectively. This is to best optimise parameters independently of
the test set with a small dataset while ensuring that the test set
remains a) fixed for easier comparison with other work, and b)
truly blind, eliminating the possibility of CIdeR overfitting to
the test set. Our stratified sampling methodology ensures that
our folds represent disjoint sets of participants and each of the
strata (next section) are approximately uniformly distributed
across each fold. To enable reproducibility, the folds are fully
released in the accompanying code.
Baseline Our approach is not directly comparable with the
study from [6] as they do not explicitly provide their folds
and discard some audio samples. To this purpose, to create a
performance reference for CIdeR, we implement a linear kernel Support Vector Machine (SVM) [16] baseline. We extract
openSMILE features [17] for each wavefile following the Interspeech 2016 ComParE challenge format [18] and perform
Principal Component Analysis (PCA) [19], selecting the top
100 components by highest explained variance. We follow the
cross optimisation procedure outlined above, using the develHarry Coppock et al.

opment set to optimise the complexity parameter 5 and reporting final results using the held-out test set.

DATASET
The dataset used in this work consists of 517 crowdsourced
coughing and breathing audio recordings from 355 participants, of which 62 participants had tested positive for COVID19 within 14 days of the recording6 . The samples were collected via android and web apps developed by [6] and can be
found at https://www.covid-19-sounds.org. To be classified as
COVID-negative, participants had to meet a number of stringent criteria described in [6]. These participants were then
divided into 3 categories: those with no cough (healthy-nosymptoms), those with a cough (healthy-with-cough) and those
who had asthma (asthma-with-cough). The COVID-positive
class is constituted of the 62 COVID-positive participants and
is further divided into the sub classes COVID-no-cough, and
COVID-cough representing 39 COVID positive participants
without a cough and 23 participants with a cough, respectively.

EXPERIMENTS & RESULTS
As indicated above, we perform a 3-fold cross optimisation
using the rotating development plus train folds. Recall that the
test set is fixed and always held out during optimisation. For
evaluation metrics, we utilise the Area Under Curve of the Receiver Operating Characteristics curve (AUC-ROC), and Unweighted Average Recall (UAR), both of which are robust to
imbalanced datasets. AUC-ROC maps the relationship between sensitivity and the false positive rate as the classification threshold is varied, and UAR computes the mean recall
per class. The models’ performance is sensitive to initialisation parameters, so we report the mean and standard deviation
from three training runs. Table 1 details our hyperparameter
search and optimal values used for the final model.
Our model performs the three tasks described in the dataset
publication [6], and an additional fourth task. Tasks 1-4 are as
follows:
Task 1 Distinguishing between COVID-positive and the
strata healthy-no-symptoms (62 vs 245 participants).
Task 2 Distinguishing between COVID-positive participants with a cough (COVID-cough) and the strata healthywith-cough (23 vs 30 participants).
between 1e−5 and 1 on a logarithmic scale.
dataset used in this study is a small subset of the full dataset that has
been collected by the University of Cambridge, which has yet to be made
fully public. As of July 2020 the full dataset totalled 10 000 samples from
roughly 7 000 participants.

5 Values
6 The

Harry Coppock et al.

Table 1 Overview of the hyperparameter search detailing the
interval, step size, and optimal parameters (used to obtain the
reported figures in this article – for details cf. the above
named GitHub repository). Hyperparameters were optimised
for task 4, and subsequently used on all tasks. ∗ Interval
constructed using a logarithmic scale. Adam [20] was used for
optimisation.
Parameter

Min.

Max.

Step

Optimal

Learning rate
Batch size
Audio segment length [s]
Spectral bands (fftn ) [#]
Sample rate sr [kHz]

5e−5
8
1
512
24

5e−4
32
8
2 048
48

5e−5
2∗
2∗
2∗
2∗

1e−4
16
8
1 024
24

Task 3 Distinguishing between COVID-positive participants with a cough (COVID-cough) and the strata asthmawith-cough (23 vs 19 participants).
Task 4 Distinguishing between COVID-positive and
COVID-negative (62 vs 293 participants).
Note that the number of participants deviates from [6], as we
also use those audio clips shorter than two seconds resulting
in partially more participants considered.
Results obtained for each task are shown in Table 2, alongside the baseline. CIdeR outperforms on all tasks bar task 2
with high margin on both metrics. The results for tasks 1, 3,
and 4 are statistically significant with a level of significance of
0.01 in a two-sided two sample t-test for difference in sample
means.

DISCUSSION
The results in Table 2 demonstrate two key points: 1) it is
possible to diagnose COVID-19 using a CNN-based model
trained on crowdsourced data; 2) CIdeR obtains a high AUCROC of 0.846 on task 4, the task which uses the entire sample, and so represents the most pertinent task. These suggest
that jointly processing breath and cough audio signals using
a CNN-based classifier could act as an effective and scalable
method for COVID-19 diagnosis.
The only task where CIdeR fails to outperform the baseline
in our experiments is task 2. We posit this is jointly due to the
small number of samples and the similarity of audio patterns
of healthy participants with a cough and those with COVID19, creating a challenging task. We leave further analysis for
future work.
A key limitation of this study is the size and demographics of the publicly available dataset [6]. We are limited to
62 COVID-positive participants, limiting the breadth of any
3

Table 2 Results of the models on tasks 1-4 for 3-fold
optimisation of the number of training epochs based on the
rotated development sets using the frozen optimal model
parameters from Table 1. [Train+development / test] sample
counts are displayed alongside the task. Testing is performed
on the held out test fold, each. The mean Area Under Curve
of the Receiver Operating Characteristics curve (AUC(-ROC))
and the Unweighted Average Recall (UAR) are displayed. A
95% confidence interval is also shown following [21] and the
normal approximation method for AUC-ROC and UAR
respectively. Scores in bold indicate significant results with
α = 0.05 using a 2-sample t-test for no difference in means
between the baseline and CIdeR based on the standard
deviation from the 3-fold cross validation.
Task
1
2
3
4

[688 / 238]
[146 / 28]*
[118 / 32]*
[684 / 350]

CIdeR
AUC

UAR

.827±.051
.570±.216
.909±.130
.846±.040

.770±.053
.535±.185
.774±.145
.765±.044

Baseline
AUC
UAR
.697±.066
.628±.208
.559±.220
.721±.053

.677±.059
.583±.183
.506±.173
.654±.050

*It is questionable whether the normality assumption holds at these small
sample sizes. The confidence interval estimates should therefore be taken
lightly.

conclusions we draw. Our control group, COVID-free participants, is not a random sample as participation required the subject lived in a country with low COVID-19 rates, among other
criteria. How representative these audio biomarker features
are of the wider population is therefore still an open question.
Importantly, before a such a technology can be deployed, evaluation on a larger, more representative dataset is necessary. As
alluded to in [22], pandemics have historically led to breakthroughs in healthcare. If AI-driven screening is to be one of
these breakthroughs from the 2020 COVID-19 pandemic, a
more comprehensive dataset is required.

CONCLUSION
Wholesale testing of the population is a promising avenue for
identifying and controlling the spread of COVID-19. A digital audio-collection and diagnostic system could be deployed
to the majority of the population and performed daily at minimial cost, e. g., for pre-selection for more reliable diagnoses
or monitoring of spread, and therefore holds great potential.
This study introduced the COVID-19 Identification ResNet,
(CIdeR), which demonstrated a strong proof-of-concept for
applying end-to-end deep learning to jointly learning representations from breath and cough audio samples. This was despite a small dataset; given more samples, it seems likely that
CIdeR’s diagnostic capabilities would significantly increase.
4

ACKNOWLEDGEMENTS
The authors give their thanks to the help provided by their
colleagues Mina A. Nessiem and Mostafa M. Mohamed.
The University of Cambridge does not bear any responsibility for the analysis or interpretation of the data used herein,
which represents the own view of the authors of this communication.

REFERENCES
[1] Rafael Polidoro, Robert Hagan, Roberta de Santis Santiago, and Nathan Schmidt. Overview: Systemic Inflammatory Response Derived From Lung Injury Caused
by SARS-CoV-2 Infection Explains Severe Outcomes in
COVID-19. Frontiers in Immunology, 11:1626, 2020.
[2] Thomas Quatieri, Tanya Talkar, and Jeffrey Palmer. A
Framework for Biomarkers of COVID-19 Based on Coordination of Speech-Production Subsystems. IEEE
Open Journal of Engineering in Medicine and Biology,
1:203–206, 2020.
[3] Lara Orlandic, Tomás Teijeiro, and David Atienza. The
COUGHVID crowdsourcing dataset: A corpus for the
study of large-scale cough analysis algorithms. arXiv,
(2009.11644), 2020. 11 pages.
[4] Neeraj Sharma, Prashant Krishnan, Rohit Kumar,
Shreyas Ramoji, Srikanth Raj Chetupalli, Nirmala R.,
Prasanta Kumar Ghosh, and Sriram Ganapathy. Coswara
– A Database of Breathing, Cough, and Voice Sounds
for COVID-19 Diagnosis. In Proc. Interspeech, pages
4811–4815, Shanghai, China, 2020.
[5] Piyush Bagad, Aman Dalmia, Jigar Doshi, Arsha Nagrani, Parag Bhamare, Amrita Mahale, Saurabh Rane,
Neeraj Agarwal, and Rahul Panicker. Cough Against
COVID: Evidence of COVID-19 Signature in Cough
Sounds. arXiv, (2009.08790), 2020. 12 pages.
[6] Chloë Brown, Jagmohan Chauhan, Andreas Grammenos, Jing Han, Apinan Hasthanasombat, Dimitris
Spathis, Tong Xia, Pietro Cicuta, and Cecilia Mascolo. Exploring Automatic Diagnosis of COVID-19
from Crowdsourced Respiratory Sound Data. In Proc.
Knowledge Discovery and Data Mining, pages 3474–
3484, 2020.
[7] Katrin D. Bartl-Pokorny, Florian B. Pokorny, Anton
Batliner, Shahin Amiriparian, Anastasia Semertzidou,
Florian Eyben, Elena Kramer, Florian Schmidt, Rainer
Schönweiler, Markus Wehler, and Björn W. Schuller.
Harry Coppock et al.

The voice of COVID-19: Acoustic correlates of infection, 2020. 8 pages.

national Conference Multimedia, pages 1459–1462, Florence, Italy, 2010.

[8] Kotra Venkata Sai Ritwik, Shareef Babu Kalluri, and
Deepu Vijayasenan. COVID-19 Patient Detection from
Telephone Quality Speech Data. arXiv, (2011.04299),
2020. 6 pages.

[18] Björn Schuller, Stefan Steidl, Anton Batliner, Julia
Hirschberg, Judee K. Burgoon, Alice Baird, Aaron
Elkins, Yue Zhang, Eduardo Coutinho, and Keelan
Evanini. The INTERSPEECH 2016 Computational Paralinguistics Challenge: Deception, Sincerity & Native
Language. In Proc. Interspeech 2016, pages 2001–2005,
San Francisco, CA, 2016.

[9] Jordi Laguarta, Ferran Hueto, and Brian Subirana.
COVID-19 Artificial Intelligence Diagnosis using only
Cough Recordings. IEEE Open Journal of Engineering
in Medicine and Biology, pages 1–1, 2020.
[10] Gadi Pinkas, Yarden Karny, Aviad Malachi, Galia
Barkai, Gideon Bachar, and Vered Aharonson. SARSCoV-2 Detection From Voice. IEEE Open Journal of
Engineering in Medicine and Biology, 1:268–274, 2020.
[11] Ali Imran, Iryna Posokhova, Haneya Naeem Qureshi,
Usama Masood, Sajid Riaz, Kamran Ali, Charles N.
John, and Muhammad Nabeel. AI4COVID-19: AI Enabled Preliminary Diagnosis for COVID-19 from Cough
Samples via an App. arXiv, (2004.01275), 2020. 27
pages.
[12] Brian McFee, Colin Raffel, Dawen Liang, Daniel Ellis,
Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa:
Audio and music signal analysis in python. In Proc.
Python in Science Conference, volume 8, pages 18–25,
Austin, TX, 2015.

[19] Christopher Bishop. Pattern Recognition and Machine
Learning (Information Science and Statistics). SpringerVerlag, Berlin, Heidelberg, 2006.
[20] Diederik Kingma and Jimmy Ba. Adam: A method
for stochastic optimization. In Proc. International Conference on Learning Representations, San Diego, CA,
2015.
[21] James A. Hanley and Barbara J. McNeil. The meaning
and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1):29–36, 1982.
[22] Ashley McKimm. Call to action for the BMJ Innovations
community after COVID-19. BMJ Innovations, 7(1):1–
2, 2021.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep Residual Learning for Image Recognition.
In Proc. Conference on Computer Vision and Pattern
Recognition, pages 770–778, Las Vegas, NV, 2016.
[14] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift. In Proc. International Conference on Machine Learning, volume 37, pages 448–456,
Lille, France, 2015.
[15] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and
Serge J. Belongie. Class-Balanced Loss Based on Effective Number of Samples. arXiv, (1901.05555), 2019.
11 pages.
[16] Nello Cristianini and John Shawe-Taylor. An introduction to support vector machines and other kernel-based
learning methods. Cambridge university press, 2000.
[17] Florian Eyben, Martin Wöllmer, and Björn Schuller.
openSMILE – The Munich Versatile and Fast OpenSource Audio Feature Extractor. In Proc. ACM InterHarry Coppock et al.

5

