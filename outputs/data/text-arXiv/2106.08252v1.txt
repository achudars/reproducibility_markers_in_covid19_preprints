arXiv:2106.08252v1 [cs.IR] 15 Jun 2021

I NTERPRETABLE S ELF - SUPERVISED M ULTI - TASK L EARNING
FOR COVID-19 I NFORMATION R ETRIEVAL AND E XTRACTION

Nima Ebadi
Department of Electrical and Computer Engineering
University of Texas at San Antonio
San Antonio, TX 78249
nima.ebadi@utsa.edu

Peyman Najafirad
Department of Information Systems and Security
University of Texas at San Antonio
San Antonio, TX 78249
peyman.najafirad@utsa.edu

June 16, 2021

A BSTRACT
The rapidly evolving literature of COVID-19 related articles makes it challenging for NLP models to
be effectively trained for information retrieval and extraction with the corresponding labeled data
that follows the current distribution of the pandemic. On the other hand, due to the uncertainty of the
situation, human experts’ supervision would always be required to double check the decision making
of these models highlighting the importance of interpretability. In the light of these challenges, this
study proposes an interpretable self-supervised multi-task learning model to jointly and effectively
tackle the tasks of information retrieval (IR) and extraction (IE) during the current emergency health
crisis situation. Our results show that our model effectively leverage the multi-task and self-supervised
learning to improve generalization, data efficiency and robustness to the ongoing dataset shift problem.
Our model outperforms baselines in IE and IR tasks, respectively by micro-f score of 0.08 (LCA-F
score of 0.05), and MAP of 0.05 on average. In IE the zero- and few-shot learning performances are
on average 0.32 and 0.19 micro-f score higher than those of the baselines.

1

Introduction and Background

The pandemic has resulted in a rapidly evolving literature of scientific publications regarding the novel Coronavirus
which has caused an information crisis [1]. Therefore, researchers, healthcare practitioners, policy makers, and other
individuals fighting against COVID-19 require specialized information retrieval and extraction systems to keep up with
the literature [2, 3]. In this regard, many models are proposed based on state-of-the-art natural language processing
(NLP) techniques, including neural ranking models for document retrieval [4, 2, 5], and pre-trained transformers for
automated annotation and information extraction [6, 7, 8].
However, the limited amount of labeled data and drastic changes of COVID-19 landscape [9] has made it challenging
for such models to effectively scale up to the domain-specific environment of the current pandemic. Moreover, due
to these challenges, such informatic tools cannot manage the pandemic situation on their own, and human experts’
supervision is required to double check their decision making. Therefore, it is significantly important for these models
to provide textual evidence from the raw input such that human experts can interpret causes of a certain decision making
[10, 11].
Although these challenges are inevitable in every health crisis situation and may be confronted again in near future,
very few studies have mainly focused on addressing these specific challenges of the pandemic. In the light of this, we
propose a specilized information retrieval and extraction approach which provides interpretability, data efficiency and
robustness to the dataset shift problem thereby suitable for the context of the pandemic. Our approach is based on
transformers encoders with global-local attention mechanism [12], thus providing interpretability both at document
level and sub-word level. We train the encoder in a multi-task fashion alongside a self-supervised learning task which
have shown promising results in improving data efficiency, generalization and robustness to dataset shift problems
[13, 14].

A PREPRINT - J UNE 16, 2021

Inspired by [15] and to exploit the landscape of transfer learning in an optimum way, we design a unified retriever-ranker
framework to simultaneously and effectively train our model on a masked language modeling (MLM) self-supervised
learning (SSL) task 1 along with two supervised tasks: i) information retrieval: retrieval of related articles to a given
question, and ii) information extraction: extraction of semantic indexes of PubMed articles which are manually indexed
by human experts. Using an input/output transformation module, we translate the tasks of masked language modeling
and semantic indexing to the domain of retriever-ranker. For every task, our model gets an input of a given query, a
COVID-19 article or question, and initially, retrieves a set of candidate related articles. Next, a transformer-based
ranker assign scores to the candidate articles using the attention mechanism proposed by [12], both within and across
articles. The final output is computed by transformation of these scores to their corresponding domain. The attention
mechanisms within/across the input query and candidate articles enables the model to associate textual evidence from
the input to the output decision, thus providing interpretability both on document and sub-word level.
• We propose an interpretable, self-supervised, multi-task learning methodology to effectively tackle the tasks of
information retrieval and extraction in the context of pandemic.
• Our study sheds light on the importance of interpretability and transfer learning–specifically multi-task and
self-supervised learning–in addressing the intrinsic challenges of a health crisis situation like the ongoing
pandemic; i.e. dataset/covariate shift, lack of unlabeled data and uncertainty.
• We devise a novel mechanism to simultaneously train a unified retriever-ranker on a self-supervised task
masked language modeling (MLM) and an information extraction (IE) task semantic indexing along with an
information retrieval (IR) task of retrieving relevant articles to a given question. This enables inter-document
representation learning and interpretability which we show is necessary for the context of the pandemic.

2

Methodology

In our methodology, we attempt to address the three tasks of self-supervised learning (henceforward we refer to as SSL,
or task_0), information extraction (henceforward we refer to as IE, or task_1) and information retrieval (henceforward
we refer to as IR, or task_2) using a unified retreival-ranker framework. Our unified framework includes the following
three steps which are identical for each task: i) an initial retrieval step which works as a weak classifier[10] to retrieve
candidate articles, ii) an I/O transformation technique to map different formats of data into a unified I/O format suitable
for next step, and iii) a ranker which works based on transformer encodings of the candidate retrieved articles.
In the context of our retriever-ranker framework, the three aforementioned tasks are defined as follows:
Task_0: (SSL) A self-supervised pre-text task similar to Masked Language Modeling in [16] is performed to introduce
knowledge about the context of pandemic. However, in this task, the masked article is treated as a query and masked
tokens are selected from a list of covid-19 related terms2 . The model attempts to detect similar articles which include
the masked term/s rather than the term itself (order is not important). As such, not only the pre-text task is consistent
with the downstream tasks, i.e. the general architecture of our multi-task learning model, but also it effectively learns
context matching using both intra- and inter-document information [17].
Task_1: (IE) Extraction of semantic indexes assigned to a given article 3 . The given article is treated as a query and
outputs are the semantic indexes.
Task_2: (IR) For a given question as a query a list of related articles are retrieved which include the target answer.
2.1

Initial Retrieval

To reduce the number of negative examples, we use an initial retrieval system to retrieve a subset of related articles
along with their task specific annotations (for example for extraction of semantic indexes, those that have manually been
annotated are considered, and their annotation as candidates for the given task). Our initial retrieval system includes
a document-level embedding model of SPECTER [17], and Bag-of-Words representation fused with TF-IDF/BM25
following the schema of [10] and [2]. SPECTER is initialized with SciBERT [18] and trained on a bipartite graph of
1

We introduce a self-supervised task of masked language modeling which is consistent with our retriever-ranker general framework
and enables the model to acquire knowledge about the current state of the pandemic.
2
We have used the list of related terms published by NLM https://www.nlm.nih.gov/pubs/techbull/nd20/nd20_mesh_
covid_terms.html
3
Semantic indexing is the information extraction task we address in this study. We use the terms "semantic extraction," "extraction
of semantic indexes" and "semantic indexing" interchangeably. In this project, we deal with PubMed articles whose indexes are
called Medical Subject Headings (MeSH)

2

A PREPRINT - J UNE 16, 2021

citations to capture document-level relatedness and minimize a triplet loss between related articles and maximize over
unrelated ones. We pre-train SPECTER on PubMed articles and fine-tune it on COVID-19 dataset exclusively.
In addition, we use TF-IDF/BM25 weighted sum of article tokens to compute a keyword-based representation as well.
As such every article is represented as follows:
Pn
TF − IDF/BM25(wi , d) × vwi
Pn
d = i=1
i=1 TF − IDF/BM25(wi , d)

(1)

where, wi is the ith word in article d, and vwi is the token embeddings from the pre-trained model.
We concatenate both representations for every article and query in our database. Next using cosine similarity scores
between the input query and other ones, we find the K relevant articles. As for the extraction of semantic indexes (IE
task), and masked language modeling, we further retrieve the candidate indexes and mask terms respectively. In this
regard, we use the summation of their IDF weights as the scoring scheme to rank and retrieve the candidate indexes and
terms. The top M are considered as candidates and passed to the next stage.
2.2

I/O Transformation

In our multi-task learning methodology, the data for every task is fed to a transformer-based ranking system, i.e. our
ranker, which is identical for different tasks. Therefore, we require an I/O transformation module to map every task’s
data format to the unique format of the ranker. The input and output to our ranker is as follows:
Input: given question or article as a query and a list of candidate articles
Output: likelihood scores corresponding to every candidate article
As for the IR task, the input and outputs are consistent with our ranking algorithm. The input is a question along with a
list of candidate related articles, and the output for every candidate article is the likelihood score of including the target
answer.
In our self-supervised task, the input is the masked articles and a list of candidate articles which include the masked
terms withing their contents. Similarly for the information extraction task, the input is a given article and a list of
candidate articles which are manually annotated in PubMed.
In our SSL task, the goal is to predict the masked tokens based on likelihood scores of the candidate terms. In our
IE task, also, the goal is to extract the semantic indexes of a given article based on likelihood scores of the candidate
indexes. However, the output of our ranker assign likelihood scores to the candidate articles instead. Therefore, to train
our model for SSL and IE tasks, we need to transform the likelihood scores of candidate terms and indexes, respectively,
to those which correspond to candidate documents:
YDc = T × YMc , T ∈ RK×M

(2)

where YMc are the likelihood scores of the candidate terms or indexes Mc . T is the transformation matrix whose every
row is bag-of-word representation of the corresponding document based on the candidate terms. YDc are the scores
assigned to candidate documents. While choosing candidate documents, we make sure the matrix T is reversible so that
the likelihood scores of the terms can be extracted and fed to the loss function. In the task of extraction of semantic
indexes task, during inference, the inverse of transformation matrix T −1 is used to compute the indexes’ likelihood
scores which are then passed through thresholds to output the final indexes. Thresholds are optimized to maximize
micro-f1 score [19].
2.3

Transformer-based Ranking

Similar to other studies that apply BERT to biomedical [20] or scientific articles [18], query and the candidate articles
are tokenized into frequent sub-words using WordPiece unsupervised algorithm [21] and the original vocabulary of
BERT. The sub-word tokenization can also mitigate the problem of out-of-vocabulary words which happens more often
during the pandemic.
Query and the paired articles are separated by [SEP] token and [CLS] token is added to the beginning of every article
which is leveraged as the vector representing the corresponding article specified by the query and other relative articles.
The same embedding scheme of BERT is used to compute the representation of every input token–summing the token,
segment4 and position embedding.
4

an additional embedding is used to differentiate tokens of the query with those of the candidate articles

3

A PREPRINT - J UNE 16, 2021

Every candidate document Di n−1 is encoded along with the paired query Qn−1 through multiple layers of a pre-trained
transformer [22].
Every candidate article’s tokens are encoded, Di n , through a self-attention between its own tokens and a crossattention between them and the tokens of the query. The [CLS] token of other articles also attend the [CLS] token to
enable capturing the inter-document information. Since the length of the input sequence is high in the setting of our
methodology, we use global-local attention mechanism [23] to mitigate the computational limitation of transformers in
dealing with long sequences. As for the article tokens, we use windowed dilated attention mechanism proposed by [12]
as a local attention. The attention between [CLS] tokens are also windowed dilated since it is not necessary that all
candidate articles attend to every article in order to capture the inter-document relationship. As for the query tokens,
however, we use global attention mechanism introduced by [24]–in which a global memory is utilized that enables
every token in the query to attend to every token of candidate articles–to bring in task-specific flexibility.
The query also gets encoded, Qn , using a full self-attention mechanism within its own tokens and a cross-attention
between them and the [CLS] tokens of the candidate articles.

3

Experiment
Model
Medical Text Indexer (MTI) (default)
MTI (first line indes)
average top score
R+TR (full attention)
R+TR (increasing w) (from 32-512)
R+TR (fixed w) (=230)
R+TR (decreasing w) (from 512-32)
R+TR (increasing w) (dilation on 2 heads)
R+TR (global + dilated sliding window*)
R+TR (base) (w/o multi-task)
R+TR (base) (w/ multi-task)
R+TR (large) (w/o multi-task)
R+TR (large) (w/ multi-task)

MiF
0.6578
0.6491
0.7143
0.5532
0.6277
0.6142
0.6002
0.6334
0.6602
0.6602
0.6671
0.6980
0.7054

Model
average top score
R+TR (full attention)
R+TR (increasing w) (from 32-512)
R+TR (fixed w) (=230)
R+TR (decreasing w) (from 512-32)
R+TR (increasing w) (dilation on 2 heads)
R+TR (global + dilated sliding window*)
R+TR (base) (w/o multi-task)
R+TR (base) (w/ multi-task)
R+TR (large) (w/o multi-task)
R+TR (large) (w/ multi-task)

MAP
0.4638
0.1911
0.2933
0.2805
0.2581
0.3032
0.3276
0.3276
0.3441
0.3550
0.4097

(b)

(a)

Table 1: Information extraction (a) and retrieval (b) performances of our models along with the baselines (best
performing models of BioASQ Task 8a for IE, and Task 8b Phase A for IR). The baseline scores are the average of their
provided Micro-F1 and MAP (respectively for IE and IR) scores across all test batches. We also evaluate our models on
all the provided test batches. * dilated sliding window uses increasing window from 32 to 512 and dilation on 2 heads.

Hyperparameter
|V |
K
M
w
dilation
dilation heads
dorpout
batch size
output vector size
w.e. size
hidden size
#layers
learning rate

Value(s)
20M, 30M
128, 256 , 512, 1024
128, 256, 512, 1024
32,..., 512, inc[32 : 512], dec[32 : 512]
0, 1, 2, 3, inc[0 : 3]
1, 2, 3
0.1, 0.2, 0.3, 0.4∗
8, 16, 32, 64 (gpu memory limit)
512, 1024, 2048
128, 256, 512∗
128, 256, 512∗
4, 5, 6∗ , 7, 8
0.001, 0.0005, 0.00025, 0.0001

Table 2: Hyperparameters value. w.e.: embedding size for initial retrieval step. We use bold text for the optimal ones
among all tried values. ∗ refer to those for large ranker. Best dilation size is achieved by increasing it by 1 from first
layer to the last.

4

A PREPRINT - J UNE 16, 2021

Model

LCA-F

MiF

MaF

Accu.

MTI (default)
MTI (first line indes)
attention mesh
R+TR (base) (w/o ssl & mt)
R+TR (base) (w/ ssl)
R+TR (base) (w/ ssl & mt)
R+TR (large) (w/o ssl & mt)
R+TR (large) (w/ ssl)
R+TR (large) (w/ ssl & mt)

0.5630
0.5534
0.5788
0.5396
0.5521
0.5632
0.5616
0.5974
0.6115

0.7299
0.7220
0.7645
0.7002
0.7282
0.7555
0.7419
0.7767
0.8102

0.5058
0.5011
0.5298
0.4924
0.5060
0.5111
0.5023
0.5319
0.5587

0.4913
0.5066
0.5576
0.4855
0.5099
0.5234
0.5225
0.5688
0.5864

MiF
(0%)
0.2216
0.2185
0.2710
0.3066
0.3804
0.4852
0.3631
0.4902
0.5644

MiF
(5%)
0.3322
0.3092
0.3965
0.4334
0.4855
0.5920
0.4743
0.6191
0.6759

MiF
(10%)
0.4592
0.4617
0.5041
0.5038
0.6160
0.6762
0.5594
0.6978
0.7411

MiF
(20%)
0.5637
0.5774
0.6186
0.5910
0.6634
0.7241
0.5952
0.7330
0.7888

Table 3: Information extraction performance of our proposed models in comparison with baselines. ssl: self-supervised
learing; mt: multi-task learning. The second half of the table shows the Micro F1 score based on the size of the
COVID-19 training dataset ranging from 0% (zero-shot) to 20% (few-shot).

3.1

Dataset

Prior to training on COVID-19 datasets, the models are initially train on general IE and IR datasets. For IE, we use
BioASQ’s [25] Task 8a dataset which includes almost 15 million article abstracts and titles manually annotated in
PubMed. We select 8M recent articles published from 2007-2019. For IR, we use BioASQ’s Task 8b dataset which
includes 3,243 question paired with related article abstracts. Test sets of each task is used for hyperparameter tuning.
We also make sure there is no overlap between these general sets and their corresponding COVID-19 ones.
After the initial training, The models are trained and evaluated on the following three COVID-19 datasets corresponding
to the three tasks in hand. As for our self-supervised task, we use CORD-19 dataset [26] which includes 200K research
articles about Coronavirus published in peer-reviewed venues and archival services such as bioRxiv5 and medRxiv6 .
We select CORD-19 articles whose MeSH indexes are manually annotated in PubMed for our semantic indexing task.
We crawl and append their MeSH indexes. Consequently, our semantic indexing dataset contains 17K articles which we
chronologically sort and split to 13.6K for training (the oldest 80%) and 3.4K for testing (the latest 20%).
Note: for self-supervised and semantic indexing, we only use the title and abstracts of the articles.
As for IR in the context of COVID-19, we choose TREC-COVID[1] dataset which is an information retrieval dataset
for question answering similar to BioASQ QA task phase b. It includes 50 topics, as queries, represented by tuples of
(concept, question, narrative). In also includes a dataset of 191K candidate documents from CORD-19. The relevance
of 69,317 topic-document pairs are manually evaluated by experts and annotated with three labels of unrelated, partially
related, related.
3.2

Evaluation Metrics

To evaluate the performance of IE we use two sets of evaluation measures: i) flat measures such as micro- and macro-f1
scores, and ii) hierarchical: for which we leverage BioASQ suggested algorithm7 Lowest Common Ancestor F-measure
(LCA-F) [27].
For evaluation of IR we leverage trec_eval, the evaluation metrics and algorithms provided by TREC-COVID 8 . The
evaluation metrics includes normalized discounted cumulative gain (nDCG@N), P@N, Mean Average Precision (MAP),
and Binary preference (Bpref) (see [2] for more details on these metrics).
3.3

Analysis

In this section, we analyze various elements of our proposed methodology architecture which is essentially retriever +
transformer-based ranker (henceforward we refer to as R+TR). In this regard, we only use the general datasets, and do
not use any sample from COVID-19 datasets for our design decisions or hyperparametes tuning. Table 2 shows the
hyperparameters we have tried and selected for our methodology.
5

https://www.biorxiv.org
https://www.medrxiv.org
7
https://github.com/BioASQ/Evaluation-Measures/tree/master/hierarchical
8
https://trec.nist.gov/trec_eval/
6

5

A PREPRINT - J UNE 16, 2021

model
top score
ranke#1 in nCDG@20
ranke#1 in P@20
ranke#1 in Bpref
ranke#1 in MAP
R+TR (base) (w/o ssl & mt)
R+TR (base) (w/ ssl)
R+TR (base) (w/ ssl & mt)
R+TR (large) (w/o ssl & mt)
R+TR (large) (w/ ssl)
R+TR (large) (w/ ssl & mt)
R+TR (base) (w/ ssl & mt) (w/ f.t.)
R+TR (large) (w/ ssl & mt) (w/ f.t.)

nDCG@20
0.8496
0.8496
0.8496
0.8490
0.8490
0.7915
0.8198
0.8571
0.8047
0.8302
0.893
0.8986
0.9237

P@20
0.8760
0.8760
0.8760
0.8690
0.8690
0.8383
0.8556
0.8707
0.8489
0.8607
0.8911
0.9152
0.9459

Bpref
0.6378
0.6372
0.6372
0.6378
0.6378
0.6021
0.6259
0.6420
0.6204
0.6331
0.6573
0.6635
0.6904

MAP
0.4731
0.4718
0.4718
0.4731
0.4731
0.4498
0.4685
0.4713
0.4566
0.4752
0.4911
0.5056
0.5230

Table 4: Information retrieval performance of our model with and without pre-training on self-supervised and semantic
extraction tasks. The performance of SSL models are improved which shows the models learn the current time period
context of medicine literature. During the IE task training the model’s attention gets well trained to extract the relative
semantic indexes from articles which is basically the main part of the IR procedure in bio-medicine articles.

We start with the retriever step which has two hyperparameters for IE task; the number of retrieved candidates K, and
0
M the number of candidate MeSH terms; and one for IR task; the number of retrieved candidates K . We use different
values and choose those that yields the highest recalls. Since this step is not trainable we can choose different values for
0
K and K . It would only change the size of input and output, not their formats. As shown in Table 2, for IE we choose
K = M = 512 because not only it gives the highest recall but also we need K = M so that the T matrix in Equation 2
is reversible and likelihood scores of documents can be mapped to those of MeSH indexes during inference. For IR we
0
choose K = 1024.
Ablation Study Moreover, we analyze several design decision for our transformer-based ranking system, the effect of
multi-task learning on the general datasets, and experimentally compare our use of different attention mechanisms. The
ablation test results for IE and IR are reported in Table 1a and Table 1b respectively.
We use two versions of our ranker, a base version (4 layers, 256 hidden size, 8 heads) and a large version (6 layers,
512 hidden size, 8 heads). We evaluate different attention mechanism on the base model. The implementation of full
attention mechanism requires truncating the input documents resulting in poor performance. However, the combination
of global and dilated sliding window with increasing window size shows better performance than other combinations in
both IE and IR. The multi-task learning improves the performance of IR without affecting the IE’s performance. Such
improvement is expected not only because of the effect of transfer learning but also because IE task is design to improve
retrieval and reinforce the latent space to be closer to those of the semantic indexes which human experts believed to
be a better representation. Such improvement is relatively higher for larger version of our ranker, showing that the
knowledge transfer capability increases with the size of the transformer model. Note: the proposed self-supervised task
is exclusively for COVID-19 datasets and disregarded in our ablation analysis. However, its effect is analysed in the
following sections.
3.4

Information Extraction on COVID-19

In this section, we discuss about the information extraction performance of our methodology exclusively on COVID-19.
Table 3 shows the IE performance of our models and baselines on COVID-19 IE testing set. Results on the left side
shows the performance of the models once trained on COVID-19 IE training set. The baselines are similarly fine-tuned
with the training data. Our proposed R+TR perform similar to the state-of-the-art baselines without but the inferior
without leveraging the proposed self-supervised task and multi-task learning with IR. However, each of these transfer
learning techniques significantly improves the IE performance. Leveraging the self-supervised learning task contributes
more since R+TR models get acquainted with the context of novel pandemic and it’s distributions. Our large model
with both self-supervised and multi-task learning achieves higher scores than the baselines by 0.05 LCA-F score and
0.08 Micro F1 score.
The right side shows the performances based on the size of the training data. We chronologically sort the data and
train the IE models with the a proportion of the from the beginning. As shown in Table 3 the partitions include: 0%
which represents the zero shot learning ability, 5%, 10%, and 20% denoting the few-shot learning. Our large R+TR’s
6

A PREPRINT - J UNE 16, 2021

Figure 1: Illustration of attention weights between the input query and candidate articles along with the extracted
outputs. The intensity and the color of the highlights denotes attention weights’ values which is averaged and set to
three scalar between the highly correlated terms.

zero-shor micro-f1 score is significantly higher than the baselines, by 0.32 on average. It achieves 97% of its optimum
performance by using only 20% of the training data.
3.5

Information Retrieval on COVID-19

Table 4 shows the information retrieval performance of our models evaluated on TREC-COVID round 5 dataset. The
result for other benchmarks are announced scores for round 5 9 . Our models which are only trained on BioASQ QA
task shows inferior performance which is due to the inconsistencies between two tasks, e.g. different distributions of
related/unrelated documents for every query. However, leveraging SSL and multi-task learning, our base model beats
the top nDCG@20 and Bpref scores. This shows how the proposed transfer learning framework improves model’s
ability to scale up to a new domain. Our large R+RT achieves significantly superior performance in every metric score.
Furthermore, to analyze the zero- and few-shot learning ability of our model, we fine-tune our SSL multi-task learning
models with TREC-COVID dataset. In this regard, we choose round 3 dataset for training which has 40 topics identical
to the first 40 topics in round 5. This is because the competition stated from 30 topics in round 1 and every time added 5
topics for the next round. We leave the last 10 topics of round 5 for evaluation.
We also expand the 40 samples of (topic , set of candidate documents) to 1,530 samples by randomly selecting a subset
of 128 candidate documents for every given topic rather than 1024. As shown in the bottom of Table 4, our base and
large model are able to effectively leverage such fine-tuning and achieve significantly better scores than the top ones, by
0.05 MAP score. Note that in TREC-COVID challenge also participants could use results from previous rounds.
3.6

Interpretability

As mentioned in Section 1, the COVID-19 infodemic has entangled automatic information retrieval and extraction
models, so much so that human expers’ supervision would always be required. The interpretability can help human
experts comprehend the decision making of a model and what has caused a mistaken output. As shown in Figure 1, the
local-global attention of our model can assist human experts even when it makes an error by providing evidence for the
mistaken output and suggesting other alternatives. The model extract the semantic index of SARS-CoV-2 while the
manual annotator believes the article is about the general SARS viruses rather than a specific variant. The highlights
shows the global attention between the related articles and the query article, and the local attention withing the query
article. The weights are averaged and set to three scalar values, following [28], to make the visualization simple [29].
As depicted by Figure 1, the extraction of SARS-CoV-2 is because of the highly matched context about COVID-19
(the top related article) and the last sentence. However, the global attention provide another related article along with
suggestions for the correct index. Knowing these, one can quickly identify and fix the error. The interpretability can
also help to understand the performance of the model in mitigating the challenges of COVID-19 infodemic. As a case
9

https://ir.nist.gov/covidSubmit/archive.html

7

A PREPRINT - J UNE 16, 2021

Figure 2: Attention weights of terms attending to COVID-19 and SARS-CoV-2 over different time frames. These
weights are normalized for visualization purpose, following [30].

study, to analyze the performance of our model in handling the shift in the topics and terminologies of COVID-19
related literature, we take a look at attention weights between the various stigmatized and standard terms for the novel
Coronavirus over the time. The stigmatized terms include those which have been used prior to the provisional standard
term "2019-nCoV", such as "Wuhan Coronavirus," "Chinese Virus," "Wuhan Novel Pneumonia" to name a few [31].
We use the aggregated weights10 when these terms attend to or get attended by the standard ones (i.e. COVID-19 and
SARS-CoV-2). We use the chronologically sorted dataset and looked at the weights as the model gets trained over the
different time frames.
As shown in Figure 2, as the distribution of terminologies changes over time, the attention mechanism learns to relate to
the well-established terms mitigating the effect of dataset shift. In the beginning, the model shows high attention weights
toward SARS-CoV as it is another variant of Coronavirus which has also originated from Chine; thereby substantially
matches with the new context. Then it quickly starts to related stigmatized terms even prior to the introduction of their
standard terms. With the advent of the standard terms, the model pays less attention to stigmatized and provisional
terms. The attention over SARS-CoV-1 and other related variants decreases as the model dissolves the confusion
between them.

4

Conclusion

In this study, we have unified the tasks of information retrieval for question answering and extraction of semantic
indexes along with a self-supervised pre-text task to leverage the advantages of transfer learning in addressing the data
efficiency, generalization and dataset shift issues confronting these application in the current pandemic. In comparison to
benchmarks, our model learns with fewer number of labeled data and shows substantially higher zero-shot performance.
Furthermore, the performance of the tasks which have scarce amount of labeled data improves dramatically once
simultaneously trained with other tasks for which relatively decent amount of labeled data is available.
In addition, the global-local attention mechanism, which is the perfect fit for retrieval and ranking models, provided our
model with not only robustness to the drastic changes of the literature, but also interpretibility both on document and
token level. We have shown the potentials of such interpretability in understanding the decision making of the model as
well as lightening the load on human experts.
Fututre Works: Our study brings focus towards state-of-the-art remedies to the current challenges of the pandemic
which opens up new doors to a more systematic analysis of each of these challenges and more sophisticated algorithms
in that regard. In future, we would like to combine more information retrieval and extraction tasks as more data is being
annotated and prepared for domain-specific environment of the pandemic. To better evaluate the performance of the
global-local interpretability, we plan to perform qualitative analysis by providing this tool to a human expert and analyze
their time efficiency and performance. Furthermore, we believe that such unifying mechanism would contribute to more
10

Summed and averaged over all sample queries and candidate articles, using both local and global attentions.

8

A PREPRINT - J UNE 16, 2021

applications and situations other than the current pandemic. We aim to identify other situations which cause drastic
changes and growth in the literature and examine how such models would perform in mitigating those challenges.

FUNDING
The authors gratefully acknowledge the use of the services of Jetstream cloud, funded by National Science Foundation
(NSF) awards 1445604, and the Cloud Technology Endowed Professorship.

References
[1] Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen Voorhees,
Lucy Lu Wang, and William R Hersh. Trec-covid: Rationale and structure of an information retrieval shared task
for covid-19. Journal of the American Medical Informatics Association, 2020.
[2] Andre Esteva, Anuprit Kale, Romain Paulus, Kazuma Hashimoto, Wenpeng Yin, Dragomir Radev, and Richard
Socher. Co-search: Covid-19 information retrieval with semantic search, question answering, and abstractive
summarization. arXiv preprint arXiv:2006.09595, 2020.
[3] Jingqi Wang, Huy Anh, Frank Manion, Masoud Rouhizadeh, and Yaoyun Zhang. Covid-19 signsym–a fast
adaptation of general clinical nlp tools to identify and normalize covid-19 signs and symptoms to omop common
data model. ArXiv.
[4] Edwin Zhang, Nikhil Gupta, Raphael Tang, Xiao Han, Ronak Pradeep, Kuang Lu, Yue Zhang, Rodrigo Nogueira,
Kyunghyun Cho, Hui Fang, et al. Covidex: Neural ranking models and keyword search infrastructure for the
covid-19 open research dataset. arXiv preprint arXiv:2007.07846, 2020.
[5] Abdullatif Köksal, Hilal Dönmez, Rıza Özçelik, Elif Ozkirimli, and Arzucan Özgür. Vapur: A search engine to
find related protein–compound pairs in covid-19 literature. arXiv preprint arXiv:2009.02526, 2020.
[6] Nico Colic, Lenz Furrer, and Fabio Rinaldi. Annotating the pandemic: Named entity recognition and normalisation
in covid-19 literature. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, 2020.
[7] Panagiotis Lymperopoulos, Haoling Qiu, and Bonan Min. Concept wikification for covid-19. In Proceedings of
the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, 2020.
[8] Alexander Spangher, Nanyun Peng, Jonathan May, and Emilio Ferrara. Enabling low-resource transfer learning
across covid-19 corpora by combining event-extraction and co-training. In Proceedings of the 1st Workshop on
NLP for COVID-19 at ACL 2020, 2020.
[9] Farhad Shokraneh and Tony Russell-Rose. Lessons from covid-19 to future evidence synthesis efforts: first
living search strategy and out of date scientific publishing and indexing industry (submitted). Journal of Clinical
Epidemiology, 2020.
[10] Qiao Jin, Bhuwan Dhingra, William Cohen, and Xinghua Lu. Attentionmesh: Simple, effective and interpretable
automatic mesh indexer. In Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical
semantic indexing and question answering, pages 47–56, 2018.
[11] Guangxu Xun, Kishlay Jha, Ye Yuan, Yaqing Wang, and Aidong Zhang. Meshprobenet: a self-attentive probe net
for mesh indexing. Bioinformatics, 35(19):3794–3802, 2019.
[12] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint
arXiv:2004.05150, 2020.
[13] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve
model robustness and uncertainty. arXiv preprint arXiv:1906.12340, 2019.
[14] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A Efros, and Moritz Hardt. Test-time training for
out-of-distribution generalization. 2019.
[15] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint
arXiv:1910.10683, 2019.
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
9

A PREPRINT - J UNE 16, 2021

[17] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. Specter: Document-level
representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pages 2270–2282, 2020.
[18] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint
arXiv:1903.10676, 2019.
[19] Ignazio Pillai, Giorgio Fumera, and Fabio Roli. Threshold optimisation for multi-label classifiers. Pattern
Recognition, 46(7):2055–2065, 2013.
[20] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,
36(4):1234–1240, 2020.
[21] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap
between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
[22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages
5998–6008, 2017.
[23] Anirudh Ravula, Chris Alberti, Joshua Ainslie, Li Yang, Philip Minh Pham, Qifan Wang, Santiago Ontanon,
Sumit Kumar Sanghai, Vaclav Cvicek, and Zach Fisher. Etc: Encoding long and structured inputs in transformers.
2020.
[24] Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers. arXiv preprint
arXiv:2006.03274, 2020.
[25] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R
Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis,
John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artieres, Axel Ngonga, Norman Heino, Eric
Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. An overview
of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC Bioinformatics,
16:138, 2015.
[26] Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn Funk,
Rodney Michael Kinney, Ziyang Liu, William. Merrill, Paul Mooney, Dewey A. Murdick, Devvret Rishi, Jerry
Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Christopher Wilhelm, Boya Xie,
Douglas M. Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. Cord-19: The covid-19 open
research dataset. ArXiv, 2020.
[27] Aris Kosmopoulos, Ioannis Partalas, Eric Gaussier, Georgios Paliouras, and Ion Androutsopoulos. Evaluation
measures for hierarchical classification: a unified view and novel approaches. Data Mining and Knowledge
Discovery, 29(3):820–865, 2015.
[28] Abeed Sarker, Ari Z Klein, Janet Mee, Polina Harik, and Graciela Gonzalez-Hernandez. An interpretable natural
language processing system for written medical examination assessment. Journal of biomedical informatics,
98:103268, 2019.
[29] Tao Lei et al. Interpretable neural models for natural language processing. PhD thesis, Massachusetts Institute of
Technology, 2017.
[30] Toan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention.
arXiv preprint arXiv:1910.05895, 2019.
[31] Zhiwen Hu, Zhongliang Yang, Qi Li, and An Zhang. The covid-19 infodemic: Infodemiology study analyzing
stigmatizing search terms. Journal of medical Internet research, 22(11):e22639, 2020.

10

