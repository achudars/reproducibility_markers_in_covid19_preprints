SLEDGE: A Simple Yet Effective Baseline for
COVID-19 Scientific Knowledge Search
Sean MacAvaney†

Arman Cohan‡

Nazli Goharian†

†

Information Retrieval Lab, Georgetown University, Washington DC
‡
Allen Institute for AI, Seattle WA
{sean,nazli}@ir.cs.georgetown.edu, armanc@allenai.org

arXiv:2005.02365v3 [cs.IR] 3 Aug 2020

Abstract
With worldwide concerns surrounding the
Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly
growing body of literature on the virus. Clinicians, researchers, and policy-makers need a
way to effectively search these articles. In
this work, we present a search system called
S LEDGE, which utilizes SciBERT to effectively re-rank articles. We train the model on
a general-domain answer ranking dataset, and
transfer the relevance signals to SARS-CoV2 for evaluation. We observe S LEDGE’s effectiveness as a strong baseline on the TRECCOVID challenge (topping the learderboard
with an nDCG@10 of 0.6844). Insights provided by a detailed analysis provide some potential future directions to explore, including
the importance of filtering by date and the potential of neural methods that rely more heavily on count signals. We release the code to
facilitate future work on this critical task.1

1

Introduction

The emergence of the Severe Acute Respiratory
Syndrome Coronavirus 2 (SARS-CoV-2) prompted
a worldwide research response. In the first 100
days of 2020, over 5,000 research articles were
published related to SARS-CoV-2 or COVID-19.
Together with articles about similar viruses researched before 2020, the body of research exceeds
50,000 articles. This results in a considerable burden for those seeking information about various
facets of the virus, including researchers, clinicians,
and policy-makers.
In the interest of establishing a strong baseline
for retrieving scientific literature related to COVID19, we introduce S LEDGE: a simple yet effective
baseline for coronavirus Scientific knowLEDGE
1

https://github.com/Georgetown-IR-Lab/
covid-neural-ir

search. Our baseline utilizes a combination of
state-of-the-art techniques for neural information
retrieval.
Recent work in neural information retrieval
shows the effectiveness of pretrained language
models in document ranking. (MacAvaney et al.,
2019; Nogueira and Cho, 2019; Hofstätter et al.,
2020; Dai and Callan, 2019b; Nogueira et al.,
2020b). Building upon success of these models,
S LEDGE is comprised of a re-ranker based on SciBERT (Beltagy et al., 2019), a pretrained language
model optimized for scientific text. Since at the
time of writing there is no available training data
for COVID-19 related search, we additionally use
a domain transfer approach by training S LEDGE
on MS-MARCO (Campos et al., 2016), a generaldomain passage ranking dataset, and apply it to
COVID-19 literature search in zero-shot setting.
We show that S LEDGE achieves strong results
in the task of scientific literature search related to
COVID-19. In particular, S LEDGE tops the leaderboard in Round 1 of the TREC-COVID Information
Retrieval shared task (Roberts et al., 2020),2 a new
test bed for evaluating effectiveness of search methods for COVID-19. We also provide an analysis
into the hyperparameter tuning conducted, the effect of various query and document fields, and possible shortcomings of the approach. Insights from
the analysis highlight the importance of a date filter
for improving precision, and the possible benefit of
utilizing models that include count-based signals in
future work. We hope that better natural language
processing and search tools can contribute to the
fight against the current global crisis.

2

https://ir.nist.gov/covidSubmit/

2

Related Work

Retrieval of scientific literature has been longstudied (Lawrence et al., 1999; Lalmas and
Tombros, 2007; Hersh and Voorhees, 2009; Lin,
2008; Medlar et al., 2016; Sorkhei et al., 2017;
Huang et al., 2019). Most recent work for scientific
literature retrieval has focused on tasks such as collaborative filtering (Chen and Lee, 2018), citation
recommendation (Nogueira et al., 2020a), and clinical decision support (Soldaini et al., 2017), rather
than ad-hoc retrieval.
Pre-trained neural language models (such as
BERT (Devlin et al., 2019)) have recently shown
to be effective when fine-tuned for ad-hoc ranking. Nogueira and Cho (2019) demonstrate that
these networks can be fine-tuned for passage ranking tasks. Others later observed effectiveness at
document ranking tasks, showing that these models
can handle natural-language questions better than
prior approaches (Dai and Callan, 2019b) and that
they can be incorporated into prior neutral ranking
techniques (MacAvaney et al., 2019). Although
computationally expensive, researches have shown
that this can be mitigated to an extent by employing more efficient modeling choices (Hofstätter
et al., 2020; MacAvaney et al., 2020c), caching intermediate representations (Khattab and Zaharia,
2020; MacAvaney et al., 2020b; Gao et al., 2020),
or by modifying the index with new terms or
weights (Nogueira et al., 2019; Dai and Callan,
2019a; Nogueira, 2019). These models also facilitate effective relevance signal transfer; Yilmaz
et al. (2019) demonstrate that the relevance signals
learned from BERT can easily transfer across collections (reducing the chance of overfitting a particular collection). In this work, we utilize relevance
signal transfer from an open-domain question answering dataset to the collection of COVID-19 scientific literature.
In terms of biomedical-related ranking, MacAvaney et al. (2020a) observed the importance
of using a domain-tuned language model (SciBERT (Beltagy et al., 2019)) when ranking in the
biomedical domain (albeit working with clinical
text rather than scientific literature). Some work
already investigates document ranking and Question Answering (QA) about COVID-19. Zhang
et al. (2020) chronicled their efforts of building and
deploying a search engine for COVID-19 articles,
utilizing a variety of available tools ranking techniques. In this work, we find that our approach out-

performs this system in terms of ranking effectiveness. Tang et al. (2020) provide a QA dataset consisting of 124 COVID-19 question-answer pairs.

3

S LEDGE

This section describes the details of S LEDGE, our
method for searching scientific literature related
to COVID-19. We utilize a standard two-stage reranking pipeline for retrieving and ranking COVID19 articles. The articles are curated from the
CORD19 dataset (Wang et al., 2020) and provided
by the task organizers. The first stage employs
an inexpensive ranking model (namely, BM25) to
generate a high-recall collection of candidate documents. The second stage re-ranks the candidate
documents using an expensive but high-precision
SciBERT-based (Beltagy et al., 2019) neural ranking model.
3.1

First-Stage Retrieval

We first index the document collection using standard pre-processing methods: English stopword
removal and Porter stemming. For the text, we
use a concatenation of the title, abstract, and fulltext paragraphs and fulltext headings. The fulltext
gives more opportunities for the first-stage ranker
to match potentially relevant documents than the
title alone would provide. When both the PDF and
PubMed XML versions are available, we use the
text extracted from the PubMed XML because it
is generally cleaner. We then query the index for
each topic with BM25. In this system, we used a
fixed re-ranking threshold of 500; thus only the top
500 BM25 results are retrieved. In our experiments,
we found that there was little recall gained beyond
500.
3.2

Neural Re-Ranking

To best capture the domain-specific language related to scientific text we use the SciBERT (Beltagy et al., 2019) pretrained language model as
the basis of a second-stage supervised re-ranker.
This model is akin to the Vanilla BERT ranker
from (MacAvaney et al., 2019), but utilizing the
SciBERT model base (which is trained on scientific
literature) instead. The query and document text
are encoded sequentially, and relevance prediction
is calculated based on the [CLS] token’s representation (which was used for next sentence prediction
during pre-training). Documents longer than the
maximum length imposed by the positional embed-

dings are split into arbitrary equal-sized passages.
We refer the reader to (MacAvaney et al., 2019) for
more details about Vanilla BERT.
At the time of writing there is no training data
available for the COVID-19 related search and collecting such data is expensive. To mitigate this
challenge, we utilize a domain transfer approach
and apply the learned model to the new domain in
a zero-shot setting. This approach also has the advantage of avoiding overfitting on the target dataset.
Specifically, we train our model using the standard training sequence of the MS-MARCO passage ranking dataset (Campos et al., 2016). This
dataset consists of over 800,000 query-document
pairs in the general domain with a shallow labeling scheme (typically fewer than two positive relevance labels per query; non-relevance assumed
from unlabeled passages). During model training,
we employ the following cross-entropy loss function from Nogueira and Cho (2019):
L(q, d+ , d− ) = − log(R(q, d+ )) − log(R(q, d− ))

(1)

where q is the query, d + and d − are the relevant
and non-relevant training documents, and R(q, d)
is the relevance score.

4

Experimental setup

We now explore the ranking effectiveness of our approach. We evaluate the performance of S LEDGE
using the TREC-COVID Information Retrieval
Challenge dataset (round 1) (Roberts et al., 2020).
TREC-COVID uses the CORD-19 document collection (Wang et al., 2020) (2020-04-10 version,
51,045 articles), with a set of 30 topics related to
COVID-19. These topics include natural queries
such as: Coronavirus response to weather changes
and Coronavirus social distancing impact. The top
articles of participating systems (56 teams) were
judged by expert assessors, who rated each article non-relevant (0), partially-relevant (1), or fullyrelevant (2) to the topic. In total, 8,691 relevance
judgments were collected, with 74% non-relevant,
13% partially relevant, and 14% fully-relevant.
Since the relevance judgments in this dataset are
shallow (avg. 290 per query), we measure effectiveness of each system using normalized Discounted
Cumulative Gain with a cutoff of 10 (nDCG@10),
Precision at 5 of partially and fully-relevant documents (P@5), and Precision at 5 of only fully
relevant documents (P@5 (Rel.)). Both nDCG@10
and P@5 are official task metrics; we include the

P@5 filtered to only fully-relevance documents
because it exposed some interesting trends in our
analysis. Since not all submissions contributed to
the judgment pool, we also report the percentage
of the top 5 documents for each query that have
relevance judgments (judged@5). These settings
represent a high-precision evaluation; we leave it to
future work to evaluate techniques for maximizing
system recall, which may require special considerations (Grossman et al., 2015).
Our work utilizes a variety of existing opensource tools, including OpenNIR (MacAvaney,
2020), Anserini (Yang et al., 2017), and the HuggingFace Transformers library (Wolf et al., 2019).
Our experiments were conducted with a Quadro
RTX 8000 GPU, and a learning rate of 2 × 10−5 .
Note on manual vs automatic runs TRECCOVID makes the distinction between manual and
automatic runs. We adhere to the broad definition
of manual runs, as specified by the task guidelines:
“Automatic construction is when there is no human
involvement of any sort in the query construction
process; manual construction is everything else...
If you make any change to your retrieval system
based on the content of the TREC-COVID topics
(say add words to a dictionary or modify a routine
after looking at retrieved results), then your runs
are manual runs.”3 In short, making any change to
the system on the basis of observations of the query
and/or results qualify as a manual run.

5

Results

In this section we discuss our results in two evaluation settings. In the first setting we apply light
hyperparmeter tuning on the pipeline which still
counts as a manual run as discussed in §4. In the
second setting we do not perform any tuning of any
sort and thus this setting is an automatic run.
5.1

Ranking with light hyperparmeter tuning

Recall that the first stage of S LEDGE is based on
an initial BM25 ranker, topics in the TREC Covid
dataset include 3 different fields: query, question
and narrative, and the documents have title, abstract
and full-text. Choices of the BM25 parameters and
which fields to include in the pipeline can affect the
final performance. Therefore, in the first setting,
we lightly tune these hyperparmeters using minimal human judgments on a subset of the topics.
3

https://ir.nist.gov/covidSubmit/round1.
html

System

nDCG@10

P@5

P@5 (Rel.)

judged@5

0.6844
0.6689
0.6513
0.6082
* 0.5966
* 0.5887
* 0.5790
* 0.5571
* 0.5479
* 0.4649

0.7933
0.8200
0.8333
0.7133
0.7000
0.7200
0.7267
0.7067
* 0.6400
* 0.5867

0.6533
0.5600
0.5933
0.5333
* 0.5200
0.5667
* 0.5333
* 0.4933
* 0.5267
* 0.4733

100%
100%
100%
100%
100%
96%
93%
93%
86%
100%

SLEDGE (ours, “run1”)
BBGhelani2
xj4wang run1
UIUC DMG setrank ret
OHSU RUN2
cu dbmi bm25 1
sheikh bm25 manual
crowd1
CSIROmed RF
dmis-rnd1-run3

Human intervention
Hyperparameter tuning on subset of queries
Human-in-loop active learning
Human-in-loop active learning
unspecified
Query reformulation & hyperparameter tuning
Query reformulation
Query reformulation
Manual relevance feedback
Manual relevance feedback
Query reformulation

Table 1: Top results using any human intervention (manual runs). * indicates our system exhibits a statistically
significant improvement (paired t-test, p < 0.05).
System
sab20.1.meta.docs
SLEDGE (ours, “run2”)
IRIT marked base
CSIROmedNIR*
base.unipd.it
udel fang run3
uogTrDPH QE
UP-rrf5rnd1
BioinfoUA-emb
UIowaS Run3

nDCG@10

P@5

P@5 (Rel.)

judged@5

0.6080
0.6032
0.5880
0.5875
0.5720
0.5370
0.5338
0.5316
0.5298
0.5286

0.7800
0.6867
0.7200
0.6600
0.7267
0.6333
0.6400
0.6800
0.6333
0.6467

0.4867
0.5667
0.5400
0.5867
0.5200
0.4267
0.4667
0.4800
0.4733
0.4733

100%
88%
100%
76%
95%
98%
100%
100%
100%
100%

Methodology
VSM, Multi-Index, Lnu.ltu weighting
BM25 + SciBERT
BM25+RM3 + BERT-base
CovidBert-nli + Inv. Index
Elastic search + boolean queries
F2EXP + CombSUM
Terrier + Query Exp.
unsupervised reciprocal rank fusion
BM25 + DeepRank traind on BioAsk
BM25 + filtering

Table 2: Top results without using any human intervention (automatic runs). No results exhibit a statistically significant difference compared to our system (paired t-test, p < 0.05). ”*” indicates that some sort of manually specified
filtering was used which may contradict the definition of an automatic run by TREC (see note in Section 4).

Specifically, we use shallow relevance judgments
from 15 out of 30 topics assessed by non-experts.4
Unlike manual runs that require human intervention for query reformulation, active learning, or
relevance feedback, we expect our system to be
able to generalize to unseen queries in the domain
because we use manual relevance signals only for
hyperparameter tuning. By tuning the hyperparmeters of the initial retrieval method, the fields of the
topic (query, question, narrative) and document text
(title, abstract, full text), and a date filter, we found
the following pipeline to be most effective based
on our non-expert annotations (run tag: run1):
1. Initial retrieval using BM25 tuned for recall
using a grid search (k1 = 3.9, b = 0.55), utilizing the keyword query field over the full
text of the article. Articles from before January 1, 2020 are disregarded.
2. Re-ranking using a Vanilla SciBERT model
trained on MS-MARCO. The topic’s question
4

Topics 1, 2, 6, 8, 9, 11, 13, 17, 18, 20, 21, 24, 27, 29, 30. 849
judgments were made in total. We found that our non-expert
annotations did not align well with the officially released
expert annotations §5.3.

field is scored over the article’s title and abstract.
We report the performance of the top system
from the top 10 teams (among manual runs) for
TREC-COVID in Table 1. Since the utilization
of humans-in-the-loop vary considerably, we also
indicate for each run the reported human intervention. We find that S LEDGE outperforms all the
other manual runs in terms of nDCG@10 and P@5
(relevant only). Of the top 10 systems that report
their technique for human intervention, ours is also
the only one that relies on human judgments solely
for hyperparameter tuning. This is particularly impressive because the next best systems (BBGhelani2 and xj4wang run1) involves human-in-theloop active learning to rank documents based on
the manual assessor’s relevance. In terms of statistical significance (paired t-test, p < 0.05), our
approach is on par with these active learning runs,
and better than most other submissions in terms of
nDCG@10 and P@5 (relevant).
5.2

Ranking without hyperparameter tuning

We now evaluate our system in an environment that
does not utilize human intervention, hyperparam-

(c)

recall@100
0.360

k1

0.345

0.330

0.315
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00

0.1
0.4
0.7
1.0
1.3
1.6
1.9
2.2
2.5
2.8
3.1
3.4
3.7
4.0
4.3
4.6
4.9
5.2
5.5
5.8

b

0.1
0.4
0.7
1.0
1.3
1.6
1.9
2.2
2.5
2.8
3.1
3.4
3.7
4.0
4.3
4.6
4.9
5.2
5.5
5.8

recall@100
0.405
0.390
0.375
0.360
0.345

0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00

(b)

k1

(a)

b

Figure 1: Comparison of grid search heatmaps for BM25 using the topic’s query over article full text with (a) our
relevance judgments, (b) the full set of official judgments, and (c) the set of official relevance judgments filtered
to only the topics we assessed. The x-axis sweeps b ∈ [0, 1] and the y-axis sweeps k1 ∈ [0.1, 6.0], and each cell
represents the recall@100.

eter tuning, or relevance judgements of any sort.
This represents a full domain transfer setting. Our
pipeline consists of (run tag: run2):
1. Initial retrieval using untuned BM25 (default
parameters, k1 = 0.9, b = 0.4), utilizing the
question text over the title and abstract of a
article. (No date filtering.)
2. Re-ranking using a Vanilla SciBERT model
trained on a medical-related subset of MSMARCO training data. The topic’s question
field is scored over the article’s title and abstract.

P@5 for fully-relevant articles and the difference
between the result are not statistically significant.
Furthermore, due to the 88% and 76% judged@5
of S LEDGE and CSIROmedNIR, the actual P@5
scores for these systems may very well be higher.
Curiously, however, other neural approaches that
are generally high-performing (e.g., those used
by Zhang et al. (2020)) did not rank in the top
10 runs. We do observe that other traditional approaches, such as those that perform query expansion (e.g., udel fang run3, and uogTrDPH QE)
also perform competitively in the automatic setting.
5.3

The purpose of leveraging the medical-related subset of MS-MARCO is to reduce the risk of domain
shift. To produce this subset, we use the MedSyn
lexicon (Yates and Goharian, 2013), which includes
layperson terminology for various medical conditions. Only queries that contain terms from the lexicon are considered in this dataset, leaving 78,895
of the original 808,531 training queries (9.7%).5 A
list of the query IDs corresponding to this filtered
set is available.6
We observe that our automatic S LEDGE run
performs highly competitively among other automatic submissions to the TREC-COVID shared
task. Although the highest-scoring system in terms
of nDCG@10 utilizes a traditional method, we observe that it falls short of neural (e.g., S LEDGE,
IRIT marked base, CSIROmedNIR) in terms of
5

Several common terms were manually excluded to increase
the precision of the filter, such as gas, card, bing, died, map,
and fall. This does not qualify as manual tuning because
these decisions were made only in consideration of the MSMARCO training queries, not any TREC-COVID topics.
6
https://github.com/Georgetown-IR-Lab/
covid-neural-ir/med-msmarco-train.txt

Analysis

Initial retrieval parameters We now evaluate
the hyperparameter tuning process conducted. We
first test the following first-stage ranking functions and tune for recall@100 using our judgments:
BM25 (k1 ∈ [0.1, 6.0] by 0.1, b ∈ [0, 1] by 0.05),
RM3 query expansion (Jaleel et al., 2004) over default BM25 parameters (feedback terms and feedback docs ∈ [1, 20] by 1), QL Sequential Dependency Model (SDM (Metzler and Croft, 2005),
term, ordered, and un-ordered weights by 0.05).
Each of these models is tested using with the query
or question topic field, and over the article full text,
or just the title and abstract. We find that using
BM25 with k1 = 3.9 and b = 0.55, the topic’s
query field, and the article’s full text to yield the
highest recall. We compare the heatmap of this
setting using our judgments, the full set of official
judgments, and the set of official judgments filtered
to only the topics we judged in Figure 1. Although
the precise values for the optimal parameter settings differ, the shapes are similar suggesting that
the hyperparameter choices generalize.

First-Stage

Re-Rank

Query

Document

Query

Document

Question
Query
Query
Query
Question
Query
Query
Query

Full text
Full text
Full text
Full text
Full text
Full text
Full text
Full text

Question
Query
Question
Narrative
Question
Query
Question
Narrative

Title+abstract
Title+abstract
Title+abstract
Title+abstract
Title+abstract
Title+abstract
Title+abstract
Title+abstract

Filter
2020

X
X
X
X

nDCG@10

P@5

P@5 (Rel.)

judged@5

0.7333
0.4190
0.6244
0.6133
0.7733
0.5131
0.6844
0.4898

0.6142
0.5067
0.7333
0.5089
0.6774
0.6267
0.7933
0.5867

0.5467
0.3867
0.5667
0.4600
0.6333
0.4933
0.6533
0.4733

90%
70%
94%
82%
91%
77%
100%
70%

*

Topic fields and date filtering Important hyperparmeters of our system include which topic field
(question, query, or narrative) to use in which stage,
and whether to perform date filtering. We present a
study of the effects of these parameters in Table 3.
First, we observe that the filtering of articles to
only those published after January 1, 2020 always
improves the ranking effectiveness (as compared
to models that retrieved from all articles). Indeed,
we find that only 19% of judged documents from
prior to 2020 were considered relevant (with only
7% fully relevant). Meanwhile, 32% of judged documents after 2020 were considered relevant (19%
fully relevant). We note that although this filter
seems to be effective, it will ultimately limit the
recall of the system. This observation underscores
the value of including a user-configurable date filter
in COVID-19 search engines.
We also observe in Table 3 that both first-stage
ranking and re-ranking based on the question field
may be more effective than using the query field for
first-stage ranking and the question for re-ranking.
Considering that the nDCG@10 already outperforms the performance of our official submission,
and P@5 (fully relevant only) is not far behind with
only 91% of the top documents judged, we can expect that this is likely a better setting going forward.
It also simplifies the pipeline and reflects a more realistic search environment in which the user simply
enters a natural language question. However, this
approach underperforms at identifying partially relevant documents, given by its much lower P@5. In
an environment in which recall is important (such
as systematic review), the hybrid query-question
approach may be preferable. Interestingly, we find
that the narrative field usually reduces ranking effectiveness compared to the other settings. This
may be due to a large distance between the naturallanguage questions seen during training and the

official labels
2
1
0

Table 3: Performance of our system using various sources for the first-stage query text, re-ranking query text, and
date filtering. Our official submission is marked with *.

202

29

30

57

36

47

40

40

121

0

1
2
our labels

Figure 2: Confusion matrix between our non-expert annotations and the official expert TREC labels.

longer-form text seen at evaluation time.
Non-expert judgements We found that our nonexpert relevance labels did not align well with the
official labels; there was only a 60% agreement
rate among the overlapping labels. In 18% of cases,
our labels rated the document as more relevant than
the official label; in 23% of cases ours was rated
less relevant. A full confusion matrix is shown in
Figure 2.
Despite the low agreement rates, the use of domain transfer, and only leveraging the non-expert
labels for hyperparameter tuning suggest that it
would be difficult to overfit to the test collection.
We further investigate whether the subset of queries
we evaluated gained a substantial advantage. To
this end, we plot the difference in the evaluation
metrics between our system and an untuned BM25
ranker in Figure 3. As demonstrated by the figure,
there was no strong preference of our model towards queries that were annotated (marked with *
and in blue). In fact, 9 of the 15 highest-performing
queries were not in the annotated set (in terms of ∆
nDCG@10). This suggests that our approach did
not overfit to signals provided by the non-expert assessments, and that our trained ranker is generally
applicable.
Failure cases Although our system generally outperforms BM25 ranking, it substantially underper-

∆ nDCG@10

1.0
0.5
0.0

−0.5
−1.0

23

15

14

7

20* 30* 17* 27* 18*

23

7

15

14

17* 18*

23

7

14

24*

10

1*

24*

9*

11*

12 28 4
Query ID

29*

3

13*

6*

2*

19

21*

22

26

25

16

5

8*

22

20* 24* 27* 30*

10

28

12

1* 6* 21* 11*
Query ID

4

26

29*

19

25

3

5

9*

2*

8*

16

13*

22

30* 27*

20* 11* 18*

28

26 17* 13*
Query ID

6*

16

25

9*

21*

12

10

19

2*

5

29*

8*

1.0
∆ P@5

0.5
0.0

−0.5
−1.0

∆ P@5 (Rel.)

1.0
0.5
0.0

−0.5
−1.0

4

3

1*

15

Figure 3: Difference in ranking effectiveness between our system and an untuned BM25 model by query for
nDCG@10, P@5, and P@5 (fully relevant only). Queries in blue and marked with * were annotated by nonexperts and used for hyperparameter tuning.

forms for Query 23 (coronavirus hypertension).
When observing the failure cases, we found that
the BM25 model successfully exploited term repetition to identify its top documents as relevant.
Meanwhile, our system ranked documents with incidental mentions of hypertension highly. This suggests that more effective utilization of approaches
that include a count-based component in the ranking score (such as TK (Hofstätter et al., 2020) or
CEDR-KNRM (MacAvaney et al., 2019)) could
yield improvements.

that recent articles (i.e., those published in 2020)
tend to exhibit higher relevance, suggesting the importance of filtering by date for high-precision retrieval. We also find that our non-expert annotation
phase helped converge on good hyperparameters,
while not likely contributing to substantial overfitting to the test set. Finally, through failure case
analysis, we find that count-based approaches may
be a good direction to explore in subsequent rounds
of the shared task.

6

This work was partially supported by the ARCS
Foundation.

Conclusion

In this work we present S LEDGE, a baseline for
literature search related to COVID-19. S LEDGE is
a two stage approach consisting of an initial BM25
ranker followed by SciBERT-based reranker, a domain specific pretrained language model. S LEDGE
is trained on the general domain MS-MARCO
passage ranking dataset and evaluated on TREC
COIVD-search benchmark in a zero-shot transfer
setting. S LEDGE tops the leaderboard among the
initial round submissions from 55 teams to TRECCOVID Search shared task, demonstrating its effectiveness.
Through our analysis we find that the parameters
for initial retrieval are fairly robust. We also find

Acknowledgments

References
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text.
In EMNLP, pages 3615–3620.
Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg,
Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, and Li Deng. 2016. MS MARCO: A
human generated machine reading comprehension
dataset. ArXiv, abs/1611.09268.
Tsung Teng Chen and Maria R. Lee. 2018. Research
paper recommender systems on big scholarly data.
In PKAW.

Zhuyun Dai and James P. Callan. 2019a. Contextaware sentence/passage term importance estimation
for first stage retrieval. ArXiv, abs/1910.10687.
Zhuyun Dai and Jamie Callan. 2019b. Deeper text understanding for ir with contextual neural language
modeling. Proceedings of the 42nd International
ACM SIGIR Conference on Research and Development in Information Retrieval.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understanding. ArXiv, abs/1810.04805.
Luyu Gao, Zhuyun Dai, and James P. Callan. 2020.
EARL: Speedup transformer-based rankers with precomputed representation. ArXiv, abs/2004.13313.
Maura R. Grossman, Gordon V. Cormack, and Adam
Roegiest. 2015. TREC 2016 total recall track
overview. In TREC.
William Hersh and Ellen Voorhees. 2009. TREC genomics special issue overview.
Sebastian Hofstätter, Markus Zlabinger, and Allan
Hanbury. 2020.
Interpretable & time-budgetconstrained contextualization for re-ranking. In
ECAI.
Chien-yu Huang, Arlene Casey, Dorota Głowacka, and
Alan Medlar. 2019. Holes in the outline: Subjectdependent abstract quality and its implications for
scientific literature search. In Proceedings of the
2019 Conference on Human Information Interaction
and Retrieval, pages 289–293.
Nasreen Abdul Jaleel, James Allan, W. Bruce Croft,
Fernando Diaz, Leah S. Larkey, Xiaoyan Li, Mark D.
Smucker, and Courtney Wade. 2004. UMass at
TREC 2004: Novelty and HARD. In TREC.
Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and effective passage search via contextualized late interaction over bert. In SIGIR.
Mounia Lalmas and Anastasios Tombros. 2007. INEX
2002 - 2006: Understanding xml retrieval evaluation.
In DELOS.
Steve Lawrence, Kurt D. Bollacker, and C. Lee Giles.
1999. Indexing and retrieval of scientific literature.
In CIKM ’99.
Jimmy Lin. 2008. Is searching full text more effective than searching abstracts? BMC Bioinformatics,
10:46 – 46.
Sean MacAvaney. 2020. OpenNIR: A complete neural ad-hoc ranking pipeline. In Proceedings of the
Thirteenth ACM International Conference on Web
Search and Data Mining, pages 845–848.
Sean MacAvaney, Arman Cohan, Nazli Goharian, and
Ross Filice. 2020a. Ranking significant discrepancies in clinical reports. In ECIR.

Sean MacAvaney, Franco Maria Nardini, Raffaele
Perego, Nicola Tonellotto, Nazli Goharian, and
Ophir Frieder. 2020b.
Efficient document reranking for transformers by precomputing term representations. In SIGIR.
Sean MacAvaney, Franco Maria Nardini, Raffaele
Perego, Nicola Tonellotto, Nazli Goharian, and
Ophir Frieder. 2020c. Expansion via prediction of
importance with contextualization. In SIGIR.
Sean MacAvaney, Andrew Yates, Arman Cohan, and
Nazli Goharian. 2019. CEDR: Contextualized embeddings for document ranking. Proceedings of the
42nd International ACM SIGIR Conference on Research and Development in Information Retrieval.
Alan Medlar, Kalle Ilves, Ping Wang, Wray Buntine,
and Dorota Glowacka. 2016. Pulp: A system for exploratory search of scientific literature. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information
Retrieval, pages 1133–1136.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In SIGIR
’05.
Rodrigo Nogueira. 2019.
docTTTTTquery.

From doc2query to

Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage
re-ranking with bert. ArXiv, abs/1901.04085.
Rodrigo Nogueira, Zhiying Jiang, Kyunghyun Cho,
and Jimmy Lin. 2020a. Evaluating pretrained transformer models for citation recommendation. In
BIR@ECIR.
Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin.
2020b.
Document ranking with a pretrained
sequence-to-sequence model.
arXiv preprint
arXiv:2003.06713.
Rodrigo Nogueira, Wei Yang, Jimmy Lin, and
Kyunghyun Cho. 2019. Document expansion by
query prediction. ArXiv, abs/1904.08375.
Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina
Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen
Voorhees, Lucy Lu Wang, and William R Hersh.
2020. TREC-COVID: Rationale and Structure of
an Information Retrieval Shared Task for COVID19. Journal of the American Medical Informatics
Association. Ocaa091.
Luca Soldaini, Andrew Yates, and Nazli Goharian.
2017. Learning to reformulate long queries for clinical decision support. J. Assoc. Inf. Sci. Technol.,
68:2602–2619.
Amin Sorkhei, Kalle Ilves, and Dorota Glowacka. 2017.
Exploring scientific literature search through topic
models. In Proceedings of the 2017 ACM Workshop
on Exploratory Search and Interactive Data Analytics, pages 65–68.

Raphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil
Gupta, Phuong Cam, Kyunghyun Cho, and Jimmy
Lin. 2020. Rapidly bootstrapping a question answering dataset for COVID-19. ArXiv, abs/2004.11339.
Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,
Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn
Funk, Rodney Kinney, Ziyang Liu, William. Merrill, Paul Mooney, Dewey A. Murdick, Devvret
Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Christopher Wilhelm, Boya Xie, Douglas M. Raymond, Daniel S.
Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020.
CORD-19: The COVID-19 open research dataset.
ArXiv, abs/2004.10706.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.
Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:
Enabling the use of lucene for information retrieval
research. In SIGIR.
Andrew Yates and Nazli Goharian. 2013. ADRTrace:
Detecting expected and unexpected adverse drug reactions from user reviews on social media sites. In
ECIR.
Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian
Zhang, and Jimmy Lin. 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In EMNLP/IJCNLP.
Edwin M. Zhang, Nikhil Gupta, Rodrigo Nogueira,
Kyunghyun Cho, and Jimmy Lin. 2020. Rapidly
deploying a neural search engine for the COVID19 open research dataset: Preliminary thoughts and
lessons learned. ArXiv, abs/2004.05125.

