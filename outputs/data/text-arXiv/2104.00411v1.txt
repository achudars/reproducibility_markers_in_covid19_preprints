Explaining COVID-19 and Thoracic Pathology
Model Predictions by Identifying Informative
Input Features

arXiv:2104.00411v1 [eess.IV] 1 Apr 2021

Ashkan Khakzar1,? , Yang Zhang1,? , Wejdene Mansour1 , Yuezhi Cai1 , Yawei
Li1 , Yucheng Zhang1 , Seong Tae Kim2,â€  , Nassir Navab1,3
1

Technical University of Munich
2
Kyung Hee University
3
Johns Hopkins University

Abstract. Neural networks have demonstrated remarkable performance
in classification and regression tasks on chest X-rays. In order to establish
trust in the clinical routine, the networksâ€™ prediction mechanism needs to
be interpretable. One principal approach to interpretation is feature attribution. Feature attribution methods identify the importance of input
features for the output prediction. Building on Information Bottleneck
Attribution (IBA) method, for each prediction we identify the chest Xray regions that have high mutual information with the networkâ€™s output.
Original IBA identifies input regions that have sufficient predictive information. We propose Inverse IBA to identify all informative regions.
Thus all predictive cues for pathologies are highlighted on the X-rays, a
desirable property for chest X-ray diagnosis. Moreover, we propose Regression IBA for explaining regression models. Using Regression IBA we
observe that a model trained on cumulative severity score labels implicitly learns the severity of different X-ray regions. Finally, we propose
Multi-layer IBA to generate higher resolution and more detailed attribution/saliency maps. We evaluate our methods using both human-centric
(ground-truth-based) interpretability metrics, and human-independent
feature importance metrics on NIH Chest X-ray8 and BrixIA datasets.
The Code1 is publicly available.
Keywords: Explainable AI Â· Feature Attribution Â· Chest X-rays Â· Covid

1

Introduction

Deep Neural Network models are the de facto standard in solving classification
and regression problems in medical imaging research. Their prominence is specifically more pronounced in chest X-ray diagnosis problems, due to the availability
of large public chest X-ray datasets [28,5,18,6]. Chest X-ray is an economical,
fast, portable, and accessible diagnostic modality. A modality with the aforementioned properties is specifically advantageous in worldwide pandemic situations
?
1

denotes joint first author, and â€  denotes corresponding author (st.kim@khu.ac.kr)
https://github.com/CAMP-eXplain-AI/CheXplain-IBA

2

such as COVID-19 where access to other modalities such as Computed Tomography (CT) is limited [22,15,17]. Therefore, diagnostic chest X-ray neural network
models can be of great value in large-scale screening of patients worldwide.
However, the black-box nature of these models is of concern. It is crucial for
their adoption to know whether the model is relying on features relevant to the
medical condition. In pursuit of interpretability of chest X-ray models, a class of
works focuses on instilling interpretability into the models during optimization
[27,8,24], another class pursues optimization semi-supervised with localization
[12], and another class of works provides post-hoc explanations [28,18,7]. Posthoc explanations have the advantage that they can be applied to any model
without changing the objective function.
One principal method for post-hoc explanation is input feature attribution
(aka saliency methods), i.e. identifying the importance/relevance of input features for the output prediction [23,26,10,3,21,20]. Feature attribution problem
remains largely open to this date, however, many branches of solutions are proposed. The question is which attribution solution to use. Attributions are evaluated from several perspectives, and one crucial and necessary aspect is to evaluate whether the attributed features are indeed important for model prediction,
which is done by feature importance metrics [19,2,16]. One desirable property is
human interpretability of the results, i.e. if the attribution is interpretable for
the user. For example, Class Activation Maps (CAM, GradCAM) [30,21] being
a solid method that is adopted by many chest X-ray model interpretation works,
satisfies feature importance metrics. However, it generates attributions that are
of low resolution, and while accurately highlighting the important features, they
do not highlight these regions with precision. Such precision is of more importance in chest X-rays where features are subtle. On the other hand, some other
methods (e.g. Guided BackPropagation [25], Î±1Î²0 [3], Excitation Backprop [29])
while being pixel-level resolute and human-interpretable, do not satisfy feature
important metrics and some do not explain model behavior [1,4,11,9,14].
Information Bottleneck Attribution (IBA) [20] is a recent method proposed
in neural networks literature that satisfies feature importance metrics, is more
human-interpretable than established methods such as CAMs [30], and is of
solid theoretical grounding. The method also visualizes the amount of information each image region provides for the output in terms of bits/pixels, thus its
attribution maps (saliency maps) of different inputs are comparable in terms of
quantity of the information (bits/pixels). Such properties make IBA a promising
candidate for chest X-ray model interpretation.
In this work, we build upon IBA and propose extended methodologies that
benefit chest X-ray model interpretations.
1.1

Contribution Statement

Inverse IBA: The original IBA method finds input regions that have sufficient
predictive information. In the presence of these features, the rest of the features do not provide predictive information for the output. However, if sufficient
features are removed, some other features can have predictive information. We

3

propose Inverse IBA to find any region that can have predictive information.
Regression IBA: IBA (and many other methods such as CAMs) is only proposed for classification. We propose Regression IBA and by using it we observe
that a model trained on cumulative severity score labels implicitly learns the
severity of different X-ray regions.
Multi-layer IBA: We investigate approaches to use the information in layers
of all resolutions, to generate high-resolution saliency maps that precisely highlight informative regions. Using Multi-layer IBA, for instance, we can precisely
highlight subtle regions such as Mass, or we observe that the model is using
corner regions to classify Cardiomegaly.
Effect of balanced training: We also observe that considering data imbalance
during training results in learned features being aligned with the pathologies.

2

Methodology

Information Bottleneck for Attribution (IBA) [20] inserts a bottleneck
into an existing network to restrict the flow of information during inference given
an input. The bottleneck is constructed by adding noise into the activations of
a layer, thus reducing the amount of information in that layer. Let F denote the
feature map representation at a layer l, the bottleneck is represented by
Z = Î»F + (1 âˆ’ Î»)

(1)

where  is the noise, Î» âˆˆ [0, 1] has the same dimension as F and controls the
amount of noise added to the signal by interpolation. For input, X, the noise
is optimized such that mutual information between the noise-injected activation
maps Z and the input X is minimized, while the mutual information between
these activations Z and the model objective Y is maximized:
maxI(Y, Z) âˆ’ Î²I(X, Z)

(2)

Thus only activations that have predictive information to output are kept. Î»
determines which areas within the feature map have predictive information. The
optimization is performed over Î» and for each input sample X.
The I(Y, Z) term in Eq. 2 in classification corresponds to cross-entropy loss
LCE . The term I(X, Z) is intractable, thus it is (variational) approximated [20]:
I(X, Z) â‰ˆ LI = ER [DKL (P (Z|R)||Q(Z))]

(3)

where Q(Z) = N (ÂµR , ÏƒR ), thus using Eq. 2 the objective becomes
L = LCE + Î²LI
2.1

(4)

Inverse IBA

IBA formulation searches for the smallest regions (by adding as much noise as
possible) that have predictive information. The identified regions have sufficient

4

predictive information for the output. However, there might exist other regions
that have predictive information in the absence of these sufficient regions. For
example, in a dog classifier, the sufficient regions could be the ears and the snout,
and these features are sufficient for dog classification, however, it does not mean
that the tail cannot have predictive information.
To find all regions that have predictive information we change the formulation
of IBA such that it aims for replacing all regions that can have predictive information with the least possible noise. Thus no region with predictive information
remains. For a more intuitive formulation, we change Eq. 1 to
Z = Î» + (1 âˆ’ Î»)F

(5)

in this case, Î» = 1 blocks the flow of information from input. The search for
Î» that adds the least possible noise while minimizing the mutual information
between Z and the objective (i.e. removing all predictive regions) becomes:
L = âˆ’LCE + Î²LI

(6)

For Inverse IBA, the resulting Î» represents the regions that need to be blocked
with high values and non-informative regions with zeros.
2.2

Regression IBA

Original IBA is proposed for classification setting. In this section, we discuss
several variations of IBA for the regression case. We discuss three different regression objectives: 1) MSE Loss defined as LM SE = (Î¦(Z) âˆ’ y)2 . MES loss
has the property that if the target score is small, it identifies regions with small
brixIA score as informative. Because in this case, the objective is trying to find
regions that have information for output to be zero. 2) Regression Maximization
(RM) Loss is simply defined as LRM = Î¦(Z)2 . This loss has the property that
it favors regions with high scores as informative. 3) Deviation loss defined as
LDV = (Î¦(Z) âˆ’ X)2 . We subtract the score of the noisy feature map from the
score of the original image. Similar to IBA for classification, this formulation
identifies regions with sufficient information for the prediction.
2.3

Multi-layer IBA

For original IBA, the bottleneck is inserted in one of the later convolutional layers. As we move towards earlier layers, the variational approximation becomes
less accurate. Thus the optimization in Eq. 4 highlights extra regions that do
not have predictive information in addition to highlighting the sufficient regions.
However, as the resolution of feature maps in earlier layers are higher, the highlighted regions are crisper and more interpretable. In order to derive regions that
are crips and have high predictive information we compute IBA for several layers
and combine their results, thus introducing Multi-layer IBA:
T (IBAL1 ) âˆ© T (IBAL2 )... âˆ© T (IBALL )
where T denotes a thresholding operation to binarize the IBA maps.

(7)

5

Fig. 1. Inverse IBA: Inverse IBA compared with IBA on a sample from the NIH
Chest X-ray8 (left) and a sample from BrixIA (right). (left): Inverse IBA is identifying
both sides of Cardiomegaly as informative. (Right): IBA is identifying two regions with
a severity score of 3 as sufficient for predicting the score of 3, however, Inverse IBA is
identifying all regions with a severity score of 3.

2.4

Chest X-ray Models

Classification model: We denote a neural network function by Î¦Î˜ (x) :
RHÃ—W â†’ RC where C is the number of output classes. For a dataset X =
{x(1) , ..., x(N ) }, and their labels Y = {y(1) , ..., y(N ) }, where y = [yj ]C , and
yj âˆˆ {0, 1}. Chest X-rays can have multiple pathologies. We use Binary Cross
Entropy (BCE) loss on each output for multilabel prediction.
X
LBCE = (yÌ‚, y) = âˆ’
Î²yj log(yÌ‚j ) + (1 âˆ’ yj ) log(1 âˆ’ yÌ‚j )
(8)
j

where Î² is a weighting factor to balance the positive labels.
Regression model: Consider a neural network fÎ˜ (x) : RHÃ—W â†’ R and a
dataset X = {x(1) , ..., x(N ) } of N X-ray images, and their corresponding labels
Y = {y(1) , ..., y(N ) }, where yj âˆˆ 0, ..., 18 is the cumulative severity score on each
image. We model the regression problem with a MSE loss:
1 X
LM SE =
(Î¦Î˜ (x)(n) âˆ’ y(n) )2
(9)
N

3

Experiments and Results

Implementation Details We use three models: 1) NIH ChestX-ray8 classification: Network with 8 outputs for the 8 pathologies. 2) BrixIA regression:
Network with one output and predicts the total severity score (sum of severity
scores of 6 regions) 3) BrixIA classifier: 3 outputs detecting whether a severity
score of 3, 2, and 0/1 exists in the X-rays. We use Densenet 121, and insert the
IBA bottleneck on the output of DenseBlock 3. For Multi-layer IBA we insert it
on the outputs of DenseBlock 1,2 and 3.
3.1

Feature Importance (Human-Agnostic) Evaluations

Experiments in this section evaluate whether an attribution method is identifying important features for the model prediction.

6

Fig. 2. Insertion/Deletion metric: Comparison of different attribution methods in
terms of feature importance. Method with high Insertion AUC and low Deletion AUC
is the best (top left corner is the best).

Fig. 3. Regression IBA: a) Regression IBA (with LDV ) applied on a regression model
that predicts the total severity score. Using Regression IBA we see that the model has
identified the severity scores of different regions implicitly. b) Sensitivity N metric for
evaluating feature importance of different Regression IBA losses

Insertion/Deletion [19,16] Insertion: given a baseline image (we use the
blurred image) features of the original image are added to the baseline image
starting from the most important feature and the output is observed. If the attribution method is correct, after inserting a few features the prediction changes
significantly, thus the AUC of output is high. Deletion: deleting important features first. The lower the AUC the better. Results are presented in Fig. 2.
Sensitivity-N [2] We adapt this metric to regression case (not trivial with
Insertion/Deletion) and use it to evaluate Regression-IBA. In Sensitivity-n we
mask the input randomly and observe the correlation between the output change
and the values of attribution map overlapping with the mask. The higher the
correlation the more accurate the attribution map. Results in Fig. 3b.
3.2

Ground Truth based (Human-centric) Evaluations

Experiments in this section evaluate the attribution maps in terms of the human notion of interpretability, i.e. the alignment between what we understand

7
Table 1. Mean IOU on NIH ChestX-ray8 (all pathologies) for various methods
GradCAM[21] InteGrad[26] LRP[13] Gradients[23] IBA Inverse IBA Multi-layer IBA
0.077
0.076
0.025
0.114
0.114 0.088
0.189
Table 2. Mean IOU on BrixIA for each detector and for various attribution methods
GradCAM
Detector 0/1 0.11
Detector 2 0.0
Detector 3 0.011

InteGrad
0.176
0.13
0.14

LRP
0.0
0.0
0.0

Gradients
0.04
0.019
0.052

IBA
0.145
0.13
0.222

Inverse IBA Multi-layer IBA
0.194
0.171
0.245
0.257
0.243
0.257

and the map. Moreover, they measure how fine-grained the attribution maps are.
Localization For NIH ChestX-ray8 dataset the bounding boxes are available.
To generate boxes for BrixIA score regions, we use a lung segmentation network.
We divide each lung into 3 regions. We threshold the attribution maps and compute their IoU with these divided regions (over dataset).
Correlation Analysis (Regression Models) For the BrixIA dataset, we evaluate the performance of regression models by measuring the correlation between
the attribution scores and the severity scores. For each image, we first assign
each pixel with its severity score, obtaining a severity score map. We then flatten both the attribution and severity score maps and compute their Pearson
correlation coefficient (PCC). The PCC results are as follows: for LM SE , 0.4766,
for LRM , 0.4766, for LM SE , 0.4404, and for random heatmaps, 0.0004.
Table 3. Mean IOU on NIH ChestX-ray8 dataset for BCE and Weighted BCE models,
reported for all the pathologies in NIH Chest X-ray8 using Inverse IBA
Atelec. Cardio. Effusion Infiltrate. Mass Nodule Pneumo. Pn. thorax Mean
BCE
0.016 0.071 0.004
0.001
0.102 0.011 0.0
0.003
0.024
W. BCE 0.073 0.131 0.032
0.058
0.097 0.02
0.066
0.016
0.065

4

Discussion

Inverse IBA: We observe that (Fig. 1) Inverse IBA highlights all regions with
predictive information. On BrixIA sample, IBA only identifies two regions with
a score of 3 as being predictive, while Inverse IBA identifies all regions with a
score of 3. On NIH sample, if we remove the highlighted areas of both methods
(equally remove) from the image, the output change caused by the removal of
Inverse IBA regions is higher. This is also quantitatively validated across dataset
in the Deletion experiment (Fig. 2).
Regression IBA: Using Regression IBA we observe that (Fig. 3) a regression

8

Fig. 4. Multi-layer IBA: Multi-layer IBA generates more fine-grained maps compared to IBA. (Left): Multi-layer IBA precisely highlights subtle features such as Mass
(Right) Using Multi-layer we observe that for Cardiomegaly in this X-ray the corner
regions of Cardiomegaly are used. IBA highlights the entire region.

model which only predicts one cumulative severity score (0-18) for each X-ray
implicitly identifies the severity scores of different regions.
Multi-layer IBA: We use Multi-layer IBA for obtaining fine-grained attributions. In Fig. 4 we see that such fine-grained attributions allow for identifying
subtle features such as Mass. Moreover, Multi-layer IBA also uncovers some hidden insights regarding what features the model is using for the Cardiomegaly
example. While IBA highlights the entire region, Multi-layer IBA shows precisely
the regions to which IBA is pointing.
Imbalanced loss: We observe in Tab. 3 that using weighted BCE results in an
increased IoU with the pathologies. This signifies that the contributing features
of the weighted BCE model are more aligned with the pathology annotations.
The observation is more significant when we consider the AUC of ROC and
the Average Precision (AP) of these models. The AUCs (BCE=0.790, Weighted
BCE=0.788) and APs (BCE=0.243, Weighted BCE=0.236) are approximately
equivalent. The BCE even archives marginally higher scores in terms of AUC of
ROC and AP but its learned features are less relevant to the pathologies.

5

Conclusion

In this work, we build on IBA feature attribution method and come up with
different approaches for identifying input regions that have predictive information. Contrary to IBA, our Inverse IBA method identifies all regions that can
have predictive information. Thus all predictive cues from the pathologies in the
X-rays are highlighted. Moreover, we propose Regression IBA for attribution
on regression models. In addition, we propose Multi-layer IBA, an approach for
obtaining fine-grained attributions which can identify subtle features.

Acknowledgement The authors acknowledge the support of the Munich Center for Machine Learning (MCML).

9

References
1. Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., Kim, B.: Sanity
checks for saliency maps. arXiv preprint arXiv:1810.03292 (2018)
2. Ancona, M., Ceolini, E., OÌˆztireli, C., Gross, M.: Towards better understanding
of gradient-based attribution methods for deep neural networks. arXiv preprint
arXiv:1711.06104 (2017)
3. Bach, S., Binder, A., Montavon, G., Klauschen, F., MuÌˆller, K.R., Samek, W.: On
pixel-wise explanations for non-linear classifier decisions by layer-wise relevance
propagation. PloS one 10(7), e0130140 (2015)
4. Hooker, S., Erhan, D., Kindermans, P.J., Kim, B.: A benchmark for interpretability
methods in deep neural networks. In: Advances in Neural Information Processing
Systems. vol. 32. Curran Associates, Inc. (2019), https://proceedings.neurips.
cc/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf
5. Irvin, J., Rajpurkar, P., Ko, M., Yu, Y., Ciurea-Ilcus, S., Chute, C., Marklund, H.,
Haghgoo, B., Ball, R., Shpanskaya, K., et al.: Chexpert: A large chest radiograph
dataset with uncertainty labels and expert comparison. In: Proceedings of the
AAAI Conference on Artificial Intelligence. vol. 33, pp. 590â€“597 (2019)
6. Johnson, A.E., Pollard, T.J., Greenbaum, N.R., Lungren, M.P., Deng, C.y., Peng,
Y., Lu, Z., Mark, R.G., Berkowitz, S.J., Horng, S.: Mimic-cxr-jpg, a large publicly
available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042
(2019)
7. Karim, M., DoÌˆhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., Beyan,
O., et al.: Deepcovidexplainer: Explainable covid-19 predictions based on chest
x-ray images. arXiv preprint arXiv:2004.04582 (2020)
8. Khakzar, A., Albarqouni, S., Navab, N.: Learning interpretable features via adversarially robust optimization. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention. pp. 793â€“800. Springer (2019)
9. Khakzar, A., Baselizadeh, S., Khanduja, S., Kim, S.T., Navab, N.: Explaining neural networks via perturbing important learned features. arXiv preprint
arXiv:1911.11081 (2019)
10. Khakzar, A., Baselizadeh, S., Khanduja, S., Rupprecht, C., Kim, S.T., Navab, N.:
Neural response interpretation through the lens of critical pathways (2021)
11. Khakzar, A., Baselizadeh, S., Navab, N.: Rethinking positive aggregation and
propagation of gradients in gradient-based saliency methods. arXiv preprint
arXiv:2012.00362 (2020)
12. Li, Z., Wang, C., Han, M., Xue, Y., Wei, W., Li, L.J., Fei-Fei, L.: Thoracic disease
identification and localization with limited supervision. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. pp. 8290â€“8299 (2018)
13. Montavon, G., Lapuschkin, S., Binder, A., Samek, W., MuÌˆller, K.R.: Explaining
nonlinear classification decisions with deep taylor decomposition. Pattern Recognition 65, 211â€“222 (2017)
14. Nie, W., Zhang, Y., Patel, A.: A theoretical explanation for perplexing behaviors
of backpropagation-based visualizations. In: International Conference on Machine
Learning. pp. 3809â€“3818. PMLR (2018)
15. Oh, Y., Park, S., Ye, J.C.: Deep learning covid-19 features on cxr using limited
training data sets. IEEE Transactions on Medical Imaging 39(8), 2688â€“2700 (2020)
16. Petsiuk, V., Das, A., Saenko, K.: Rise: Randomized input sampling for explanation
of black-box models. arXiv preprint arXiv:1806.07421 (2018)

10
17. Punn, N.S., Agarwal, S.: Automated diagnosis of covid-19 with limited posteroanterior chest x-ray images using fine-tuned deep neural networks. Applied Intelligence
pp. 1â€“14 (2020)
18. Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D., Bagul,
A., Langlotz, C., Shpanskaya, K., et al.: Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225 (2017)
19. Samek, W., Binder, A., Montavon, G., Lapuschkin, S., MuÌˆller, K.R.: Evaluating
the visualization of what a deep neural network has learned. IEEE transactions on
neural networks and learning systems 28(11), 2660â€“2673 (2016)
20. Schulz, K., Sixt, L., Tombari, F., Landgraf, T.: Restricting the flow: Information
bottlenecks for attribution. arXiv preprint arXiv:2001.00396 (2020)
21. Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Gradcam: Visual explanations from deep networks via gradient-based localization. In:
Proceedings of the IEEE international conference on computer vision. pp. 618â€“626
(2017)
22. Signoroni, A., Savardi, M., Benini, S., Adami, N., Leonardi, R., Gibellini, P.,
Vaccher, F., Ravanelli, M., Borghesi, A., Maroldi, R., et al.: End-to-end learning for semiquantitative rating of covid-19 severity on chest x-rays. arXiv preprint
arXiv:2006.04603 (2020)
23. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint
arXiv:1312.6034 (2013)
24. Singh, R.K., Pandey, R., Babu, R.N.: Covidscreen: Explainable deep learning
framework for differential diagnosis of covid-19 using chest x-rays. Neural Computing and Applications pp. 1â€“22 (2021)
25. Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M.: Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806 (2014)
26. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. In:
International Conference on Machine Learning. pp. 3319â€“3328. PMLR (2017)
27. Taghanaki, S.A., Havaei, M., Berthier, T., Dutil, F., Di Jorio, L., Hamarneh, G.,
Bengio, Y.: Infomask: Masked variational latent representation to localize chest
disease. In: International Conference on Medical Image Computing and ComputerAssisted Intervention. pp. 739â€“747. Springer (2019)
28. Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Summers, R.M.: Chestx-ray8:
Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 2097â€“2106 (2017)
29. Zhang, J., Bargal, S.A., Lin, Z., Brandt, J., Shen, X., Sclaroff, S.: Top-down neural attention by excitation backprop. International Journal of Computer Vision
126(10), 1084â€“1102 (2018)
30. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features
for discriminative localization. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 2921â€“2929 (2016)

