Conformalized Survival Analysis
Emmanuel J. Candès1,2 , Lihua Lei1 , and Zhimei Ren1

arXiv:2103.09763v1 [stat.ME] 17 Mar 2021

1

2

Department of Statistics, Stanford University, Stanford, CA 94305
Department of Mathematics, Stanford University, Stanford, CA 94305
March 18, 2021
Abstract

Existing survival analysis techniques heavily rely on strong modelling assumptions and are,
therefore, prone to model misspecification errors. In this paper, we develop an inferential method
based on ideas from conformal prediction, which can wrap around any survival prediction algorithm to produce calibrated, covariate-dependent lower predictive bounds on survival times.
In the Type I right-censoring setting, when the censoring times are completely exogenous, the
lower predictive bounds have guaranteed coverage in finite samples without any assumptions
other than that of operating on independent and identically distributed data points. Under
a more general conditionally independent censoring assumption, the bounds satisfy a doubly
robust property which states the following: marginal coverage is approximately guaranteed if
either the censoring mechanism or the conditional survival function is estimated well. Further,
we demonstrate that the lower predictive bounds remain valid and informative for other types
of censoring. The validity and efficiency of our procedure are demonstrated on synthetic data
and real COVID-19 data from the UK Biobank.
Keywords. Censoring, survival time, prediction interval, weighted conformal inference, distribution
boosting, random forests.

1

Introduction

The COVID-19 pandemic has placed extraordinary demands on health systems (e.g., Ranney et al.,
2020). In turn, these demands create an unavoidable need for medical resource allocation and, in
response, several groups of researchers have communicated clinical ethics recommendations (e.g.,
Emanuel et al., 2020; Vergano et al., 2020). By and large, these recommendations require a reliable
benefit assessment of receiving specific types of medical resources; see Table 2 of Emanuel et al.
(2020). Clearly, one benefit measure of interest might be the survival time, the time lapse between
the confirmation of COVID-19 and an event such as death or reaching a critical state, should this
ever occur.

1.1

Survival analysis

Survival times are not always observed due to censoring (Leung et al., 1997). A main goal of survival
analysis is to infer the survival function—the probability that a patient will survive beyond any
specified time—from censored data. The Kaplan-Meier curve (Kaplan and Meier, 1958) produces
such an inference when the population under study is a group of patients with certain characteristics.
On the positive side, the Kaplan-Meier curve does not make any assumption on the distribution of
1

survival times. On the negative side, it can only be applied to a handful of subpopulations because it
requires sufficiently many events in each subgroup (Kalbfleisch and Prentice, 2011). More often than
not, the scientist has available multiple categorical and continuous covariates, and it thus becomes
of interest to understand heterogeneity by studying the conditional survival function; that is, the
dependence on the available factors. In the conditional setting, however, distribution-free inference
for the conditional survival function gets to be challenging. Standard approaches make parametric
or nonparametric assumptions about the distribution of the covariates and that of the survival times
conditional on covariate values. A well-known example is of course the celebrated Cox model which
posits a proportional hazards model in which an unspecified nonparametric base line is modified via
a parametric model describing how the hazard varies in response to explanatory covariates (Cox,
1972; Breslow, 1975). Other popular models, such as accelerated failure time (AFT) (Cox, 1972;
Wei, 1992) and proportional odds models (Murphy et al., 1997; Harrell Jr, 2015), also combine
nonparametric and parametric model specifications.
As medical technologies produce ever larger and more complex clinical datasets, we have witnessed a rapid development of machine learning methods adapted to high-dimensional and heterogeneous survival data (e.g., Verweij and Van Houwelingen, 1993; Faraggi and Simon, 1995; Tibshirani,
1997; Gui and Li, 2005; Hothorn et al., 2006; Zhang and Lu, 2007; Ishwaran et al., 2008; Witten and
Tibshirani, 2010; Goeman, 2010; Simon et al., 2011; Katzman et al., 2016; Lao et al., 2017; Wang
et al., 2019; Li and Bradic, 2020). An appealing feature of these methods is that they typically do
not make modeling assumptions.1 The downside is that it is often challenging to quantify the uncertainty for these methods. To be sure, blind application of off-the-shelf uncertainty quantification
tools, such as the bootstrap (Efron, 1979; Efron and Tibshirani, 1994), can yield unreliable results
since their validity 1) rests on implicit modeling assumptions, and 2) holds only asymptotically (e.g.,
Lei and Candès, 2020; Ratkovic and Tingley, 2021).

1.2

Prediction intervals

For decision-making in sensitive and uncertain environments—think of the COVID-19 pandemic—
it is preferable to produce prediction intervals for the uncensored survival time with guaranteed
coverage rather than point predictions. In this regard, the use of (1 − α) prediction intervals is an
effective way of summarizing what can be learned from the available data; wide intervals reveal a
lack of knowledge and keep overconfidence at arm’s length. Here and below, an interval is said to be
a (1 − α) prediction interval if it has the property that it contains the true label, here, the survival
time, at least 100(1 − α)% of the time (a formal definition is in Section 2). Prediction intervals
have been widely studied in statistics (e.g., Wilks, 1941; Wald, 1943; Aitchison and Dunsmore, 1980;
Stine, 1985; Geisser, 1993; Vovk et al., 2005; Krishnamoorthy and Mathew, 2009) and much research
has been concerned with the construction of covariate-dependent intervals.
Of special interest is the subject of conformal inference, a generic procedure that can be used
in conjunction with sophisticated machine learning prediction algorithms to produce prediction
intervals with valid marginal coverage without making any distributional assumption whatsoever
(e.g., Saunders et al., 1999; Vovk, 2002; Vovk et al., 2005; Lei and Wasserman, 2014; Tibshirani et al.,
2019). While coverage is only guaranteed in a marginal sense, it has been empirically observed that
some conformal prediction methods can also achieve near conditional coverage—that is, coverage
assuming a fixed value of the covariates—when some key parameters of the underlying conditional
distribution can be estimated reasonably well (e.g., Sesia and Candès, 2020; Lei and Candès, 2020).

1.3

Our contribution

Standard conformal inference requires fully observed outcomes and is not directly applicable to samples with censored outcomes. In this paper, we extend conformal inference to handle right-censored
1 To

quote from Efron (2020): “ Neither surface nor noise is required as input to randomForest, gbm, or their kin.

2

outcomes in the setting of Type-I censoring (e.g., Leung et al., 1997). This setting assumes that the
censoring time is observed for every unit while the outcome is only observed for uncensored units.
In particular, we generate a covariate-dependent lower predictive bound (LPB) on the uncensored
survival time, which can be regarded as a one-sided (1 − α)-prediction interval. As we just argued,
the LPB is a conservative assessment of the survival time, which is particularly desirable for highstakes decision-making. A low LPB value suggests either a high risk for the patient, or a high degree
of uncertainty for similar patients due to data scarcity. Either way, the signal to a decision-maker
is that the patient deserves some attention.
Under the completely independent censoring assumption defined below, which states that the
censoring time is independent of both the outcome and covariates, our LPB provably yields a (1 − α)
prediction interval. This property holds in finite samples without any assumption other than that of
operating on i.i.d. samples. Under the more general conditionally independent censoring assumption
introduced later, our LPB satisfies a doubly robust property which states the following: marginal
coverage is approximately guaranteed if either the censoring mechanism or the conditional survival
function is estimated well. In the latter case, the LPB even has approximately guaranteed conditional
coverage.
Readers familiar with conformal inference would notice that the above guarantees can be achieved
by simply applying conformal inference to the censored outcomes, i.e., by constructing an LPB on the
censored outcome treated as the response. This unsophisticated approach is conservative. Instead,
we will see how to provide tighter bounds and sharper inference by applying conformal inference on
a subpopulation with large censoring times; that is, on which censored outcomes are closer to actual
outcomes. To achieve this, we shall see how to carefully combine the selection of a subpopulation
with ideas from weighted conformal inference (Tibshirani et al., 2019).
Lastly, while we focus on clinical examples, it will be clear from our exposition that our methods
can be applied to other time-to-event outcomes in a variety of other disciplines, such as industrial
life testing (Bain, 2017), sociology (Allison, 1984), and economics (Powell, 1986; Hong and Tamer,
2003; Sant’Anna, 2016).

2
2.1

Prediction intervals for survival times
Problem setup

Let Xi , Ci , Ti , i = 1, . . . , n, be respectively the vector of covariates, the censoring time, and the
survival time of the i-th unit/patient. Throughout the paper, we assume that (Xi , Ci , Ti ) are
i.i.d. copies of the random vector (X, C, T ). We consider the Type I right-censoring setting, where
the observables for the i-th unit include Xi , Ci , and the censored survival time Tei , defined as the
minimum of the survival and censoring time:
Tei = min(Ti , Ci ).

T

|=

For instance, if Ti measures the time lapse between the admission into the hospital and death, and
Ci measures the time lapse between the admission into the hospital and the day data analysis is
conducted, then Tei = Ti if the i-th patient died before the day of data analysis and Tei = Ci if she
survives beyond that day.
The censoring time C partially masks information from the inferential target T . As discussed by
Leung et al. (1997), it is necessary to impose constraints on the dependence structure between T and
C to enable meaningful inference. In particular, we make the following conditionally independent
censoring assumption (e.g., Kalbfleisch and Prentice, 2011):
C | X.

3

(1)

(T, X)

|=

This assumes away any unmeasured confounder affecting both the survival and censoring time;
please see immediately below for an example. In some cases, we also consider the completely
independent censoring assumption, which is stronger in the sense that it implies the former:
C.

(2)

For instance, in a randomized clinical trial, the end-of-study censoring time C is defined as the time
lapse between the recruitment and the end of the study. For single-site trials, C is often modelled
as a draw from an exogenous stochastic process (e.g., Carter, 2004; Gajewski et al., 2008) and thus
obeys (2). For multicentral trials, C is often assumed to depend on the site location only (e.g.,
Carter et al., 2005; Anisimov and Fedorov, 2007; Barnard et al., 2010), and thus (1) holds as soon
as the vector of covariates includes the site of the trial.
Although (1) is a strong assumption, it is a widely used starting point to study survival analysis
methods (Kalbfleisch and Prentice, 2011). We leave the investigation of informative censoring (e.g.,
Lagakos, 1979; Wu and Carroll, 1988; Scharfstein and Robins, 2002) to future research. Additionally,
whereas the setting of Type I censoring appears to be restrictive, we will show in Section 2.4 that
an LPB in this setting can still be informative for other censoring types.

2.2

Naive lower predictive bounds

Our ultimate goal is to generate a covariate-dependent LPB as a conservative assessment of the
uncensored survival time T . Denote by L̂(·) a generic LPB estimated from the observed data
(Xi , Ci , Tei )ni=1 . We say an LPB is calibrated if it satisfies the following coverage criterion:


P T ≥ L̂(X) ≥ 1 − α,
(3)
where α is a pre-specified level (e.g., 0.1), and the probability is computed over both L̂(·) and a
future unit (X, C, T ) that is independent of (Xi , Ci , Ti )ni=1 .
Since Te ≤ T , any calibrated LPB on the censored survival time Te is also a calibrated LPB on
the uncensored survival time T . Consequently, a naive approach is to discard the censoring time
Ci ’s and construct an LPB on Te directly. Since the samples (Xi , Tei ) are i.i.d., a distribution-free
calibrated LPB on Te can be obtained via standard techniques from conformal inference (e.g., Vovk
et al., 2005; Lei et al., 2018; Romano et al., 2019b). Our first result is somewhat negative: indeed,
it states that all distribution-free calibrated LPBs on T must be LPBs on Te.
Theorem 1. Take X ∈ Rp and C ≥ 0, T ≥ 0. Assume that L̂(·) is a calibrated LPB on T for all
joint distributions of (X, C, T ) obeying the conditionally independent censoring assumption with X
being continuous and (T, C) being continuous or discrete.2 Then for any such distribution,
P(Te ≥ L̂(X)) ≥ 1 − α.
An LPB constructed by taking Te as the response may be calibrated but also overly conservative
because of the censoring mechanism. To see this, note that the oracle LPB on Te is, by definition,
the α-th conditional quantile of Te | X, denoted by q̃α (X). Similarly, let qα (X) be the oracle LPB
on T . Under the conditionally independent censoring assumption,
P(T ≥ qα (x) | X = x) = 1 − α = P(Te ≥ q̃α (x) | X = x)
= P(T ≥ q̃α (x) | X = x)P(C ≥ q̃α (x) | X = x).
2 Our proof can be extended to include the case where either C or T or both are mixtures of discrete and continuous
distributions but we do not consider such extensions here.

4

If the censoring times are small, the gap between q̃α (x) and qα (x) can be large. For illustration,
assume that X, C, and T are mutually independent, and T ∼ Exp(1), C ∼ Exp(b). It is easy to
show that qα (X) = − log(1 − α) and q̃α (X) = − log(1 − α)/(1 + b). Thus, a naive approach taking
Te as a target of inference can be arbitrarily conservative.
In sum, Theorem 1 implies that any calibrated LPB on T must be a calibrated LPB on Te,
under the conditionally independent censoring assumption only. This is why to make progress and
overcome the limitations of the naive approach, we shall need additional distributional assumptions.

2.3

Leveraging the censoring mechanism

We have just seen that the conservativeness of the naive approach is driven by small censoring
times. A heuristic way to mitigate this issue is to discard units with small values of C. Consider a
threshold c0 , and extract the subpopulation on which C ≥ c0 . One immediate issue with this is that
the selection induces a distributional shift between the subpopulation and the whole population,
namely,
d

(X, C, T ) 6= (X, C, T ) | C ≥ c0 .
For instance, the patients with larger censoring times tend to be healthier than the remaining ones.
To examine the distributional shift in details, note that the joint distribution of (X, Te) on the whole
population is PX × PTe|X while that on the subpopulation is
P(X,Te)|C≥c0 = PX|C≥c0 × PTe|X,C≥c0 .

|=

|=

Next, observe that PTe|X,C≥c0 6= PTe|X even under the completely independent censoring assumption because (T, X) C does not imply Te C | X in general. For example, as in Section 2.2, if
i.i.d.
X, C, and T are mutually independent and T, C ∼ Exp(1), then P(Te ≥ a, C ≥ a) = P(Te ≥ a) >
P(Te ≥ a)P(C ≥ a), for any a > 0. As a result, both the covariate distribution and the conditional
distribution of Te given X differ in the two populations.
Now consider a secondary censored outcome Te ∧ c0 , where a ∧ b = min{a, b}. We have
(a)

P(X,Te∧c0 )|C≥c0 = PX|C≥c0 × PTe∧c0 |X,C≥c0 = PX|C≥c0 × PT ∧c0 |X,C≥c0
(b)

= PX|C≥c0 × PT ∧c0 |X ,

(4)

where (a) uses the fact that
T ∧ c0 = Te ∧ c0 , if C ≥ c0 ,
and (b) follows from the conditionally independent censoring assumption. On the other hand, the
joint distribution of (X, T ∧ c0 ) on the whole population is
P(X,T ∧c0 ) = PX × PT ∧c0 |X .

(5)

Contrasting (4) with (5), we observe that there is only a covariate shift between the subpopulation
and the whole population.
The likelihood ratio between the two covariate distributions is
dPX
P(C ≥ c0 )
(x) =
.
dPX|C≥c0
P(C ≥ c0 | X = x)

(6)

Applying the one-sided version of weighted conformal inference (Tibshirani et al., 2019), discussed
in the next section, gives a calibrated LPB on T ∧ c0 , and thus a calibrated LPB on T . With
sufficiently many units with large values of C, we can choose a large threshold c0 to reduce the loss
5

of power caused by censoring. We emphasize that there is no contradiction with Theorem 1 because,
as shown in Section 3, weighted conformal inference requires P(C ≥ c0 | X) to be (approximately)
known.
We refer to the denominator P(C ≥ c0 | X = x) in (6) as the censoring mechanism, denoted by
c(x; c0 ). We write it as c(x) for brevity when no confusion can arise. This is the conditional survival
function of C evaluated at c0 . Under a censoring of Type I, the Ci ’s are fully observed while the
Ti ’s are only partially observed. Thus, P(C | X) is typically far easier to estimate than P(T | X).
Practically, the censoring mechanism is usually far better understood than the conditional survival
function of T ; for example, as mentioned in Section 2.1, in randomized clinical trials, C often solely
depends on the site location.
Under the completely independent censoring assumption, the covariate shift even disappears
since PX = PX|C≥c0 . In this case, we can apply a one-sided version of conformal inference to obtain
a calibrated LPB on T ∧ c0 , and hence a calibrated LPB on T (e.g., Vovk et al., 2005; Lei et al.,
2018; Romano et al., 2019b). With infinite samples, as c0 → ∞, the method is tight in the sense that
the censoring issue disappears. Again, this result does not contradict Theorem 1, which requires
the LPB to be calibrated under the weaker condition (1). With finite samples, there is a tradeoff
between the choice of the threshold c0 and the size of the induced subpopulation.

2.4

Beyond Type-I censoring

|=

In practice, censoring can be driven by multiple factors. As discussed in Leung et al. (1997), the
two most common types of right censoring in a clinical study are the end-of-study censoring caused
by the trial termination and the loss-to-follow-up censoring caused by unexpected attrition. Let
Cend denote the former and Closs the latter. By definition, Cend is observable for every patient, as
long as the entry times are accurately recorded. When the event is not death, Closs is observable
for everyone as well. However, when the event is death, Closs can only be observed for surviving
patients. This is because for dead patients, it is impossible to know when they would have been lost
to follow-up, had they survived.
In survival analysis without loss-to-follow-up censoring, or time-to-event analysis with non-death
events, the setting of Type I censoring considered in this paper is plausible. However, it is found
that both the end-of-study and loss-to-follow-up censoring are involved in many applications (Leung
et al., 1997). In these cases, the effective censoring time C is the minimum of Cend and Closs , and is
only observable for surviving patients, namely the patients with T > C. This situation prevents us
from applying Algorithm 1 below because the subpopulation with C ≥ c0 is not fully observed. If we
use the subpopulation whose C is 1) observed and 2) larger than or equal to a threshold c0 instead,
then the joint distribution of (X, T ) becomes PX|C≥c0 ,T >C × PT |X,C≥c0 ,T >C . The extra conditioning
event T > C induces a shift of the conditional distribution, since PT |X,C≥c0 ,T >C 6= PT |X,C≥c0 in
general, rendering the weighted split conformal inference invalid.
Our method can nevertheless be adapted to yield meaningful inference under an additional
assumption:
Cend | X.
(7)
(T, Closs )

|=

The assumption (7) is often plausible since the randomness of the end-of-study censoring time often
comes from the entry time of a patient, which is arguably exogenous to the survival time and attrition,
at least when conditioning on a few demographic variables. Let T 0 = T ∧ Closs , the survival time
censored merely by the loss to follow-up. Then the censored survival time T̃ = T ∧C = T 0 ∧Cend , and
(7) implies that T 0 Cend | X, an analog of the conditionally independent censoring assumption (1).
Since Cend is observed for every patient, Algorithm 1 can be applied to produce an LPB L̂(·) such
that
P(T 0 ≥ L̂(X)) ≥ 1 − α =⇒ P(T ≥ L̂(X)) ≥ 1 − α.
An observation in conjunction with this line of reasoning is that unlike most survival analysis
techniques, our method distinguishes two sources of censoring and takes advantage of the censoring
6

mechanism. It can be regarded as a building block to remove the adverse effect of Cend . It remains
an interesting question whether the censoring issue induced by Closs can be resolved or alleviated.

3

Conformal inference for censored outcomes

3.1

Weighted conformal inference

Returning to (4) and (5), the goal is to construct an LPB L̂(·) on T ∧ c0 from training samples
(Xi , T̃i ∧ c0 )Ci ≥c0 = (Xi , Ti ∧ c0 )Ci ≥c0 such that
P(T ∧ c0 ≥ L̂(X)) ≥ 1 − α.
Since T ∧ c0 ≤ T , L̂(·) is a calibrated LPB on T . We consider c0 to be a fixed threshold in Section
3.1 and 3.2, and discuss a data-adaptive approach to choosing this threshold in Section 3.3.
To deal with covariate shifts, Tibshirani et al. (2019) introduced weighted conformal inference,
which extends standard conformal inference (e.g., Vovk et al., 2005; Shafer and Vovk, 2008; Lei and
Wasserman, 2014; Barber et al., 2019a,b; Sadinle et al., 2019; Romano et al., 2020; Cauchois et al.,
2020)). Imagine we have i.i.d. training samples (Xi , Yi )ni=1 drawn from a distribution PX ×PY |X and
wish to construct predictive intervals for test points drawn from the target distribution QX × PY |X
(in standard conformal inference, PX = QX ). Assuming w(x) = dQX (x)/dPX (x) is known, then
weighted conformal inference produces predictive intervals Ĉ(·) with the property


P(X,Y )∼QX ×PY |X Y ∈ Ĉ(X) ≥ 1 − α.
Above, the probability is computed over both the training set and the test point (X, Y ). In our
case, the outcome is T ∧ c0 and the covariate shift w(x) = P(C ≥ c0 )/c(x), as shown in (6).
Operationally, the ‘split version’ of weighted conformal inference sketched in Algorithm 1 has
three main steps:
1. split the data into a training and a calibration fold;
2. apply any prediction algorithm on the training fold to generate a conformity score indicating
how atypical a value of the outcome is given observed covariate values;3
3. calibrate the predicted outcome by the distribution of conformity scores on the calibration
fold. In the calibration step from Algorithm 1, Quantile(1 − α; Q) is the (1 − α) quantile of
the distribution Q defined as
Quantile(1 − α; Q) = sup{z : Q(Z ≤ z) < 1 − α}.
3 Here,

we generate a conformity score such that a large value indicates a lack of conformity to training data.

7

Algorithm 1: (one-sided) weighted split conformal inference
Input: level α; data Z = (Xi , Yi )i∈I ; testing point x;
function V (x, y; D) to compute the conformity score between (x, y) and data D;
function ŵ(x; D) to fit the weight function at x using D as data.
Procedure:
1. Split Z into a training fold Ztr , (Xi , Yi )i∈Itr and a calibration fold Zca , (Xi , Yi )i∈Ica .
2. For each i ∈ Ica , compute the conformity score Vi = V (Xi , Yi ; Ztr ).
3. For each i ∈ Ica , compute the weight Wi = ŵ(Xi ; Ztr ) ∈ [0, ∞).
Wi
tr )
and p̂∞ (x) = P ŵ(x;Z
4. Compute the weights p̂i (x) = P
Wi +ŵ(x;Ztr ) .
i∈Ica Wi +ŵ(x;Ztr )
i∈I
ca

P
5. Compute η(x) = Quantile 1 − α; i∈Ica p̂i (x)δVi + p̂∞ (x)δ∞ .
Output: L̂(x) = inf{y : V (x, y; Ztr ) ≤ η(x)}
A few comments regarding Algorithm 1 are in order. First, when the covariate shift w(x) is
unknown, it can be estimated using the training fold. Second, note that in step 4, if ŵ(x; Ztr ) = ∞,
then p̂i (x) = 0 (i ∈ Zca ) and p̂∞ (x) = 1. In this case, step 5 gives L̂(x) = −∞. Third, the
requirement that Wi ∈ [0, ∞) is natural because Xi ∼ PX and w(X) ∈ [0, ∞) almost surely under
PX even if QX is not absolutely continuous with respect to PX . Fourth, it is worth mentioning in
passing that η(x) is invariant to positive rescalings of ŵ(x). Thus, we can set w(x) = 1/ĉ(x; c0 ) in
our case where ĉ(x; c0 ) is an estimate of c(x; c0 ).
In the algorithm, the conformity score function V (x, y; D) can be arbitrary and we discuss three
popular choices from the literature:
• Conformalized mean regression (CMR) scores are defined via V (x, y; Ztr ) = m̂(x) − y, where
m̂(·) is an estimate of the conditional mean of Y given X. The resulting LPB is then m̂(x) −
η(x). This is the one-sided version of the conformity score used in Vovk et al. (2005) and Lei
and Wasserman (2014).
• Conformalized quantile regression (CQR) scores are defined via V (x, y; Ztr ) = q̂α (x)−y, where
q̂α (·) is an estimate of the conditional α-th quantile of Y given X. The resulting LPB is then
q̂α (x) − η(x). This score was proposed by Romano et al. (2019b); it is more adaptive than
CMR and usually has better conditional coverage.
• Conformalized distribution regression (CDR) scores are defined via V (x, y; Ztr ) = α−F̂Y |X=x (y),
where F̂Y |X=x (·) is an estimate of the conditional distribution of Y given X. The resulting
LPB is then F̂Y−1
|X=x (α − η(x)), or equivalently, the (α − η(x))-th quantile of the estimated
conditional distribution. This score was proposed by Chernozhukov et al. (2019). It is particularly suitable to our problem because most survival analysis methods estimate the whole
conditional distribution.
Under the completely independent censoring assumption, P(C ≥ c0 | X) = P(C ≥ c0 ) almost
surely. As a consequence, we can set ŵ(x) = w(x) ≡ 1 and obtain a calibrated LPB without any
distributional assumption.
Proposition 1. [Corollary 1 of Tibshirani et al. (2019)] Let c0 be any threshold independent of
Zca . Consider Algorithm 1 with Yi = Ti ∧ c0 and ŵ(x; D) ≡ 1. Under the completely independent
censoring assumption, L̂(X) is calibrated.

3.2

Doubly robust lower predictive bounds

Under the more general conditionally independent censoring assumption, the censoring mechanism
needs to be estimated. We can apply any distributional regression techniques such as the kernel
8

method or the newly invented distribution boosting (Friedman, 2020) to estimate c(x) = P(C ≥ c0 |
X = x). For two-sided weighted split-CQR, Lei and Candès (2020) prove that the intervals satisfy
a doubly robust property which states the following: the average coverage is guaranteed if either
the covariate shift or the conditional quantiles are estimated well, and the conditional coverage
is approximately controlled if the latter is true. In Appendix B, we generalize their results to a
broad class of conformity scores proposed by Gupta et al. (2019), including the CMR-, CQR- and
CDR-based scores.
In this section, we first present a version tailored to the CQR-LPB for simplicity.
Theorem 2. Let N = |Ztr |, n = |Zca |, c0 be any threshold independent of Zca , and qα (x; c0 ) denote
the α-th conditional quantile of T ∧ c0 given X = x. Further, let ĉ(x) and q̂(x; c0 ) be estimates of
c(x) and q(x; c0 ) respectively
 using Ztr , and L̂(x) be the corresponding CQR-LPB. Assume that there
exists δ > 0 such that E 1/ĉ(X)1+δ < ∞ and E[1/c(X)1+δ ] < ∞. Suppose that either A1 or A2
(or both) holds:
A1 lim E |1/ĉ(X) − 1/c(X)| = 0.
N →∞

A2

(i) There exists b2 > b1 > 0 and r > 0 such that, for any x and ε ∈ [0, r],
P(T ∧ c0 ≥ qα (x; c0 ) + ε | X = x) ∈ [1 − α − b2 ε, 1 − α − b1 ε]
(ii) lim E [E(X)/ĉ(X)] = lim E [E(X)/c(X)] = 0, where E(x) = |q̂α (x; c0 ) − qα (x; c0 )|.
N →∞

N →∞

Then


lim P T ∧ c0 ≥ L̂(X) ≥ 1 − α.

N,n→∞

Furthermore, under A2, for any ε > 0,
  


lim P E 1 T ∧ c0 ≥ L̂(X) | X > 1 − α − ε = 1.
N,n→∞

Intuitively, if ĉ(x) ≈ c(x), then the procedure approximates the oracle version of weighted splitCQR with the true weights, and the LPBs should be approximately calibrated. On the other hand,
if q̂α (x; c0 ) ≈ qα (x; c0 ), then Vi ≈ qα (Xi ; c0 ) − Ti ∧ c0 . As a result,
P(Vi ≤ 0 | Xi ) ≈ P(Ti ∧ c0 ≤ qα (Xi ; c0 ) | Xi ) = α.
Thus, the (1 − α)-th quantile of the Vi ’s conditional on Ztr is approximately
0. To keep on going,
P
recall that η(x) is the (1 − α)-th quantile of the random distribution i∈Zca p̂i (x)δVi + p̂∞ (x)δ∞ ,
and set G to be the cumulative distribution function of this random distribution. Then,
X
X
G(0) ≈ E[G(0) | Ztr ] =
p̂i (x)P(Vi ≤ 0 | Ztr ) ≈
p̂i (x)(1 − α) ≈ 1 − α,
i∈Zca

i∈Zca

implying that η(x) ≈ 0. Therefore, L̂(x) ≈ qα (x; c0 ), which approximately achieves the desired
conditional coverage.
With the same intuition, we can establish a similar result for the CDR-LPB with a slightly more
complicated version of Assumption A2.
Theorem 3. Let F (· | x) denote the conditional distribution of T ∧ c0 given X = x. With the
same settings and assumptions as in Theorem 2, the same conclusions hold if A2 is replaced by the
following conditions:

9

(i) there exists r > 0 such that, for any x and ε ∈ [0, r],
P(T ∧ c0 ≥ qα+ε (x; c0 ) | X = x) = 1 − α − ε.
(ii) lim E [E(X)/ĉ(X)] = lim E [E(X)/c(X)] = 0, where
N →∞

N →∞

E(x) =

sup

|F (q̂s (x; c0 ) | x) − F (qs (x; c0 ) | x)|.

s∈[α,α+r]

The double robustness of weighted split conformal inference has some appeal; indeed, the researcher can leverage knowledge about both the conditional survival function and the censoring
mechanism without any concern for which is more accurate. Suppose the Cox model is adequate in
a randomized clinical trial; then it can be used to produce q̂α (x; c0 ) in conjunction with the known
censoring mechanism. If the model is indeed correctly specified, the LPB is conditionally calibrated,
as are classical predictive intervals derived from the Cox model (Kalbfleisch and Prentice, 2011); if
the model is misspecified, however, the LPB is still calibrated.

3.3

Choice of threshold

The threshold c0 induces an estimation-censoring tradeoff: a larger c0 mitigates the censoring effect,
closing the gap between the target outcome T and the operating outcome T ∧ c0 , but reduces the
sample size to estimate the censoring mechanism and the conditional survival function. It is thus
important to pinpoint the optimal value of c0 to maximize efficiency.
To avoid double-dipping, we choose c0 on the training fold Ztr . In this way, c0 is independent
of the calibration fold Zca and we are not using the same data twice. In particular, Proposition
1, Theorem 2 and 3 all apply. Concretely, we (1) set a grid of values for c0 , (2) randomly sample
a holdout set from Ztr , (3) apply Algorithm 1 on the rest of Ztr for each value of c0 to generate
LPBs for each unit in the holdout set, and (4) select c0 which maximizes the average LPBs on the
holdout set. One way to see all of this is to pretend that the training fold is the whole dataset and
measure efficiency as the average realized LPBs. In practice, we choose 25% units from Ztr as the
holdout set. The procedure is convenient to implement, though it is by no means the most powerful
approach. We leave the investigation of more principled selection procedures to future research.

4

Simulation studies

In this section, we design simulation studies to evaluate the performance of our method. Specifically,
we run four sets of experiments detailed in Table 1. In each experiment, we compare the CQR- and
CDR-LPB with the following alternatives:
• Cox model: we generate the LPB as the α-th quantile from an estimated Cox model. The
method is implemented via the survival R-package (Therneau, 2020).
• Accelerated failure time (AFT) model: we generate the LPB as the α-th quantile from an
estimated AFT model with Weibull noise. The method is implemented in the survival R
package.
• Censored quantile regression: we consider three variants of quantile regression methods, proposed by Powell (1986), Portnoy (2003), and Peng and Huang (2008), respectively. All three
procedures are implemented in the quantreg R package (Koenker, 2020).
• Censored quantile regression forest (Li and Bradic, 2020): this is a variant of quantile random
forest (Athey et al., 2019) designed to handle time-to-event outcomes. We reimplement the
method based on the code provided in https://github.com/AlexanderYogurt/censored_
ExtremelyRandomForest.
10

• Naive CQR: we apply split-CQR (Romano et al., 2019b) naively to (Xi , Tei )ni=1 , where the
quantiles are estimated by the quantreg R package.
For the CQR-LPB, the conditional quantiles are estimated via censored quantile regression forest
or distribution boosting (Friedman, 2020); for the CDR-LPB, the conditional survival function is
estimated via distribution boosting, which is implemented in the R package conTree (Friedman and
Narasimhan, 2020).
In each experiment, we generate 200 independent datasets, each containing a training set of size
n = 3000, and a test set of size n = 3000. For conformal methods, 50% of the data is used for
fitting the predictive model, and the
50% is reserved for calibration.4 We then evaluate
Premaining
ntest
the coverage of LPBs as (1/ntest ) i=1 1{Ti ≥ L̂(Xi )}. All the results in this section can be
replicated with the code available at https://github.com/zhimeir/cfsurv_paper. In addition,
the proposed CQR- and CDR-LPB are implemented in the R package cfsurvival, available at https:
//github.com/zhimeir/cfsurvival.
The covariate vector X ∈ Rp is generated from PX . The survival time T is generated from an
AFT model with Gaussian noise, i.e.


log T | X ∼ N µ(X), σ 2 (X) .
We consider 2 × 2 settings with univariate or multivariate covariates plus homoscedastic or heteroscedastic errors.5 The choice of the parameters in each setting is specified in Table 1.
Finally, we apply all the methods with target coverage level 1 − α = 90%. In each experiment,
we estimate c(x) by distribution boosting.
dimension p
Uvt. + Homosc.
Uvt. + Heterosc.
Mvt. + Homosc.
Mvt. + Heterosc.

1
1
100
100

PX
U(0, 4)
U(0, 4)
U([−1, 1]p )
U([−1, 1]p )

PC|X

µ(x)

σ(x)

E(0.4)
E(0.4)
E(0.4)
E(0.4)

√
2 + 0.37 x
√
2 + 0.37 x
log 2 + 1 + 0.55(x21 − x3 x5 )
log 2 + 1 + 0.55(x21 − x3 x5 )

1.5
1 + x/5
1
|x10 | + 1

Table 1: Parameters used in the simulation study. “Homosc.” and “Heterosc.” are short for homoscedastic and heteroscedastic; “Uvt.” and “Mvt.” are short for univariate and multivariate. U(a, b)
denotes the uniform distribution supported on [a, b]; E(λ) denotes the exponential distribution with
rate λ.
Figure 1 presents the empirical coverage of the LPBs on uncensored survival times. Censored
random forests, the Cox model, the AFT model, and the three quantile regression methods fail to
achieve the target coverage in most cases. On the other hand, the naive CQR attains the desired
coverage but at the price of being overly conservative. In contrast, both the CQR- and CDR-LPB
achieve near-exact marginal coverage, as predicted by our theory.
Next, we investigate the conditional coverage and efficiency of these methods. In Figure 2(a),
we plot the empirical conditional coverage as a function of the conditional variance of T on X.
In particular, we stratify the data into 10 groups based on equispaced percentiles of Var(T | X)
and plot the average coverage within each stratum along with a 90% confidence band obtained via
repeated sampling. (Note that in either the homoscedastic or the heteroscedastic case, Var(T | X)
is varying with X.) Not surprisingly, the naive CQR is conditionally conservative. In the univariate
case, both the CQR- and CDR-LPB approximately achieve desired conditional coverage; in the
4 The splitting ratio between the training set and the test set is slightly different from the recommendation from Sesia
and Candès (2020), where they suggest using 75% of the data for training and 25% for calibration. We reserve more
data for calibration to ensure there are still enough samples in the calibration set after the selection and to decrease
the variability of the LPBs.
5 Here the term “homoscedastic” or “heteroscedastic” is applied to log T .

11

Figure 1: Empirical 90% coverage of the uncensored survival time T . “CQR-cRF” is short for the
CQR-LPB with censored quantile regression forest; “CQR-conTree” and “CDR-conTree” are short
for the CQR- and CDR-LPB with distribution boosting. The other abbreviations are the same as
in Table 1.
multivariate case, the conditional coverage is slightly uneven, though still concentrating around the
target line. Figure 2(b) presents the ratio between the LPBs and the true α-th conditional quantile
as a function of Var(T | X). This is a measure of efficiency since the true conditional quantile is
the oracle LPB. Here, we observe that naive CQR-LPBs are close to zero, confirming that they are
overly conservative, while the CQR- and CDR-LPBs are fairly close to the oracle LPB, implying
that both methods are relatively efficient.

5

Application to UK Biobank COVID-19 data

We apply our method to the UK Biobank COVID-19 dataset to demonstrate robustness and practicability. UK Biobank (Bycroft et al., 2018) is a large-scale biomedical database and research resource,
containing in-depth genetic and health information from half a million UK participants. In April
2020, UK Biobank started to release COVID-19 testing data, and has since continued to regularly
provide updates. This gives researchers access to a cohort of COVID-19 patients, along with their
date of confirmation, survival status, pre-existing conditions, and other demographic covariates.
We include in our analysis all individuals in UK Biobank who received a positive COVID-19
test result before January 21st, 2021. This results in a dataset of size n = 14,861 with 484 events,
defined as a COVID-related death. We extract eight covariate features, namely, age, gender, body
mass index (BMI), waist size, cardiovascular disease status, diabetes status, hypothyroidism status,
and respiratory disease status. As in Section 2, the censoring time is the time lapse between the
date of a positive test and January 21st, 2021. The survival time is the time lapse between the date
of a positive test and the event (which may have yet to occur).
We wish to harness this data to produce an LPB on the survival time of each COVID-19 patient.
To apply the CQR- or CDR-LPB, we set the threshold c0 to be 14 days. Since survival time
assessment likely informs high-stakes decision-making, we set the target level to 99% for reliability.

5.1

Semi-synthetic examples

To demonstrate robustness, we start our analysis with two semi-synthetic examples so that the
ground truth is known and calibration can be assessed (results on real outcomes are presented next).
We keep the covariate matrix X from the UK Biobank COVID-19 data. In the first simulation study,
we substitute the censoring time with a synthetic C. In the second, each survival time, observed or
not, is substituted with a synthetic version. Details follow:
12

(a)

(b)

Figure 2: Results from the experiments detailed in Table 1: (a) empirical 90% conditional coverage
and (b) ratio between the LPB and the theoretical quantile as a function of Var(T | X). The blue
curves correspond to the mean coverage in (a) and the median ratio in (b). The gray confidence bands
correspond to the 95% and 5% quantiles of the estimates over repeated sampling. The abbreviations
are the same as in Figure 1.
• Synthetic C: we take the censored survival time Te as the uncensored survival time and generate
the censoring time Csyn as
Csyn ∼ E(0.001 · age + 0.01 · gender).
In this setting, the observables are (X, Csyn , Te ∧ Csyn ), and we wish to construct LPBs on Te.
• Synthetic T : we keep the real censoring time C, and generate a survival time Tsyn as:
log Tsyn | X ∼ N (2 + 0.05 · age + 0.1 · gender, 1).
In this setting, the observables are (X, C, Tsyn ∧ C), and we wish to construct LPBs on Tsyn .
Figure 3 shows the histograms of the survival time, censoring time, and censored survival time
from the two simulated datasets. We apply the CDR-LPB (with c0 = 14) to both. For comparison,
we also apply the AFT and naive CQR. To evaluate the LPBs, we randomly split the data into a
training set with 75% of the data and a holdout set with the remaining 25%. Each method is applied
to the training set, and the resulting LPBs are evaluated on the holdout set. We repeat the above
procedure 100 times to create 100 pairs of training and test data sets.
To visualize conditional calibration, we fit a Cox model on the data to generate a predicted risk
score for each unit and stratify all units into 10 subgroups defined by deciles of the predicted risk.
The results for synthetic C and T are plotted in Figures 4 and 5, respectively. As in the simulation
studies from Section 4, we see that the naive CQR is overly conservative. Notably, although the
AFT-LPB is well calibrated in the synthetic-C setting, this method is overly conservative in the
synthetic-T setting, even though the model is correctly specified. In contrast, the CDR-LPB is
calibrated in both examples. From the middle panels of Figures 4 and 5, we also observe that the
13

Figure 3: Histograms of the survival time, censoring time, and censored survival time defined as the
minimum between the two, in each simulation setting.
CDR-LPB is approximately conditionally calibrated. Finally, the right panels show that CDR-LPB
nearly preserves the rank of the predicted risk given by the Cox model. The flat portion of the
LPB towards the left end corresponds to the threshold, implying that at least 99% of people with
predicted risk scores lower than 0.5 can survive beyond 14 days.

Figure 4: Results for synthetic censoring times across 100 replications: empirical coverage (left),
empirical conditional coverage of the CDR-LPB (middle), and CDR-LPB (right) as a function of
the percentile of the predicted risk. The target coverage level is 99%. The blue curves correspond to
the mean coverage in the middle panel and the median LPB in the right panel; the gray confidence
bands correspond to the 5% and 95% quantiles of the estimates across 100 independent replications.

5.2

Real data analysis

We now turn attention to actual COVID-19 responses. Again, we randomly split the data into a
training set including 75% of data and a holdout set including the remaining 25%. Then we run
the CDR on the training set and validate the LPBs on the holdout set. The issue is that the actual
survival time is only partially observed, and thus, the coverage of a given LPB cannot be assessed
accurately (this is precisely why we generated semi-synthetic responses in the previous section.)
Nevertheless, we note that



βlo := P Te ≥ L̂(X) ≤ P T ≥ L̂(X) ≤ 1 − P Te < L̂(X), T ≤ C =: βhi ,
where both βlo and βhi are estimable from the data. This says that we can assess the marginal
coverage of the LPBs by evaluating a lower and upper bound on the coverage. Of course, this
extends to conditional coverage.
To assess the stability, we evaluate our method on 100 independent sample splits. Figure 6
presents the empirical lower and upper bound of the marginal coverage and those of the conditional
14

Figure 5: Results for synthetic survival times: everything else is as in Figure 4.
coverage as functions of the predicted risk (as in the semi-synthetic examples), together with their
variability across 100 sample splits. The left panel shows that the upper bound is very close to
the lower bound, and both concentrate around the target level. Thus we can be assured that the
CDR-LPB is well calibrated. Similarly, the other panels show that the CDR-LPB is approximately
conditionally calibrated. We conclude this section by showing in Figure 7 the LPBs as functions of
the percentiles of the predicted risk, age, and BMI, respectively.

(a)

(b)

(c)

Figure 6: Analysis of the UK Biobank COVID-19 data: (a) lower and upper bounds of the empirical
coverage; (b) lower and (c) upper bounds of empirical coverage as a function of the predicted risk.
The target coverage level is 99%. The blue curves correspond to the mean coverage, and the gray
confidence bands correspond to the 5% and 95% quantiles of the estimates across 100 sample splits.

6
6.1

Discussion and extensions
Sharper coverage criteria

It is more desirable to achieve a stronger conditional coverage criterion:


P T ≥ L̂(X) | X = x ≥ 1 − α,

(8)

which states that L̂(X) is a conditionally calibrated LPB. Clearly, (8) implies valid marginal coverage. Theorem 2 and 3 show that the CQR- and CDR-LPB are approximately conditionally calibrated
if the conditional quantiles are estimated well. However, without distributional assumptions, we can
show that (8) can only be achieved by trivial LPBs.
Theorem 4. Assume that X ∈ Rp and C ≥ 0, T ≥ 0. Let P(X,C) be any given distribution of
(X, C). If L̂(·) satisfies (8) uniformly for all joint distributions of (X, C, T ) with (X, C) ∼ P(X,C) ,
then for all such distributions,
P(L̂(x) = 0) ≥ 1 − α,
15

Figure 7: Analysis of the UK Biobank COVID-19 data: LPBs on the survival time of COVID-19
patients as a function of the percentiles of predicted risk (left), age (middle) and BMI (right). The
target coverage level is 99%. The blue curves correspond to the median LPB across 100 sample
splits.
at almost surely all points x aside from the atoms of PX .
Theorem 4 implies that no nontrivial LPB exists even if the distribution of (X, C) is known.
Put another way, it is impossible to achieve desired conditional coverage while being agnostic to the
conditional survival function. This impossibility result is inspired by previous works on uncensored
outcomes and two-sided intervals (Vovk, 2012; Barber et al., 2019a).
It is valuable to find other achievable coverage criteria which are sharper than the marginal
coverage criterion (3). Without censoring and covariate shift, Vovk et al. (2003) introduced Mondrian
conformal inference to achieve desired marginal coverage over multiple subpopulations. The idea
is further developed from different perspectives (Vovk, 2012; Lei et al., 2013; Guan, 2019; Barber
et al., 2019a; Romano et al., 2019a). Given a partition of the covariate space {X1 , . . . , XK }, Mondrian
conformal inference guarantees that6
P(Y ∈ Ĉ(X) | X ∈ Xk ) ≥ 1 − α,

k = 1, . . . , K.

Following their techniques, we can extend Mondrian conformal inference to our case by modifying
the calibration term η(x) (in Algorithm 1):


X
η(x) = Quantile 1 − α;
p̂i (x)δVi + p̂∞ (x)δ∞  , ∀x ∈ Xk .
(9)
i∈Ica ,Xi ∈Xk

Suppose X1 and X2 correspond to male and female subpopulations. Then η(x) is a function of both
the testing point x and the gender. That said, estimation of censoring mechanisms and conditional
survival functions can still depend on the whole training fold Ztr as joint training may be more
powerful than separate training on each subpopulation (Romano et al., 2019a).
When the censoring mechanism is known, we can prove that
P(T ∧ c0 ≥ L̂(X) | X ∈ Xk ) ≥ 1 − α,

k = 1, . . . , K.

(10)

By the conditionally independent censoring assumption, the target distribution in the localized
criterion (10) for a given k can be rewritten as
(X, T ∧ c0 ) | C ≥ c0 , X ∈ Xk ∼ PX|C≥c0 ,X∈Xk × PT ∧c0 |X .
6 Mondrian

conformal inference allows the subgroups to also depend on the outcome; see Vovk et al. (2005), which
refers to the rule of forming subgroups as a “taxonomy.” Besides, the subgroups can also be overlapping; see Barber
et al. (2019a).

16

The covariate shift between the observed and target distributions is
wk (x) =

dPX|C≥c0 ,X∈Xk
I(x ∈ Xk )
(x) ∝
.
dPX
P(C ≥ c0 | X = x)

This justifies the calibration term (9) in the weighted Mondrian conformal inference. Since the
weighted Mondrian conformal inference is a special case of Algorithm 1, it also enjoys the double
robustness property, implied by Theorem B.1 in Appendix B.

6.2

Survival counterfactual prediction

The proposed method in this paper is designed for a single cohort. In practice, patients are often
exposed to multiple conditions, and the goal is to predict the counterfactual survival times had
the cohort been exposed to a different condition. For example, a clinical study typically involves a
treatment group and a control group. For a new patient, it is of interest to predict her survival time
had she been assigned the treatment. For uncensored outcomes, Lei and Candès (2020) proposed
a method based on weighted conformal inference for counterfactual prediction under the potential
outcome framework (Neyman, 1990; Rubin, 1974). We can extend their strategy to handle censored
outcomes and apply it to the survival counterfactual prediction.
Suppose each patient has a pair of potential survival times (T (1), T (0)), where T (1) (resp. T (0))
denotes the survival time had the patient been assigned into the treatment (resp. control) group.
Our goal is to construct a calibrated LPB on T (1), given i.i.d. observations (Xi , Wi , Ci , Ti )ni=1 with
Wi denoting the treatment assignment and

Ti (1), Wi = 1,
Ti =
Ti (0), Wi = 0.
Without further assumptions on the correlation structures between T (1) and T (0), it is natural to
conduct inference based on the observed treated group since the control group contains no information about T (1). The joint distribution of (X, T (1) ∧ c0 ) on this group becomes
(X, T (1) ∧ c0 ) | C ≥ c0 , W = 1 ∼ PX|C≥c0 ,W =1 × PT (1)∧c0 |X,C≥c0 ,W =1 .
|=

Under the assumption that (T (1), T (0)) (W, C) | X, the conditional distribution of T (1) ∧ c0
matches the target:
PT (1)∧c0 |X,C≥c0 ,W =1 = PT (1)∧c0 |X .
The assumption is a combination of the strong ignorability assumption (Rubin, 1978), a widely
accepted starting point in causal inference, and the conditionally independent censoring assumption.
The density ratio of the two covariate distributions can be characterized by
dPX|C≥c0 ,W =1
1
(x) ∝
.
dPX
P(C ≥ c0 , W = 1 | X = x)

In many applications, it is plausible to further assume that C

|=

w(x) =

W | X. In this case,

P(C ≥ c0 , W = 1 | X = x) = P(C ≥ c0 | X = x)P(W = 1 | X = x),
where the first term is the censoring mechanism and the second term is the propensity score (Rosenbaum and Rubin, 1983). Therefore, we can obtain calibrated LPBs on counterfactual survival times
if both the censoring mechanism and the propensity score are known. This assumption is often
plausible for randomized clinical trials. Furthermore, it has a doubly robust guarantee of coverage
that is similar to Theorems 2 and 3.

17

Acknowledgment
E. C. was supported by Office of Naval Research grant N00014-20-12157, by the National Science
Foundation grants OAC 1934578 and DMS 2032014, by the Army Research Office (ARO) under
grant W911NF-17-1-0304, and by the Simons Foundation under award 814641. L. L. and Z. R. were
supported by the same NSF OAC grant and by the Discovery Innovation Fund for Biomedical Data
Sciences. L.L. was also supported by NIH grant R01MH113078. The authors are grateful to Ying
Jin, Yan Min, Chiara Sabatti, Matteo Sesia, Lu Tian and Steve Yadlowsky for their constructive
feedback.

References
Aitchison, J. and Dunsmore, I. R. (1980). Statistical prediction analysis. CUP Archive.
Allison, P. D. (1984). Event history analysis: Regression for longitudinal event data. Number 46.
Sage.
Anisimov, V. V. and Fedorov, V. V. (2007). Modelling, prediction and adaptive adjustment of
recruitment in multicentre trials. Statistics in medicine, 26(27):4958–4975.
Athey, S., Tibshirani, J., and Wager, S. (2019). Generalized random forests. The Annals of Statistics,
47(2):1148–1178.
Bain, L. (2017). Statistical analysis of reliability and life-testing models: theory and methods. Routledge.
Barber, R. F., Candès, E. J., Ramdas, A., and Tibshirani, R. J. (2019a). The limits of distributionfree conditional predictive inference. Information and Inference: A Journal of the IMA.
Barber, R. F., Candes, E. J., Ramdas, A., and Tibshirani, R. J. (2019b). Predictive inference with
the jackknife+. arXiv preprint arXiv:1905.02928.
Barnard, K. D., Dent, L., and Cook, A. (2010). A systematic review of models to predict recruitment
to multicentre clinical trials. BMC medical research methodology, 10(1):1–8.
Breslow, N. E. (1975). Analysis of survival data under the proportional hazards model. International
Statistical Review/Revue Internationale de Statistique, pages 45–57.
Bycroft, C., Freeman, C., Petkova, D., Band, G., Elliott, L. T., Sharp, K., Motyer, A., Vukcevic,
D., Delaneau, O., O’Connell, J., et al. (2018). The uk biobank resource with deep phenotyping
and genomic data. Nature, 562(7726):203–209.
Carter, R. E. (2004). Application of stochastic processes to participant recruitment in clinical trials.
Controlled clinical trials, 25(5):429–436.
Carter, R. E., Sonne, S. C., and Brady, K. T. (2005). Practical considerations for estimating clinical
trial accrual periods: application to a multi-center effectiveness study. BMC medical research
methodology, 5(1):1–5.
Cauchois, M., Gupta, S., and Duchi, J. (2020). Knowing what you know: valid confidence sets in
multiclass and multilabel prediction. arXiv preprint arXiv:2004.10181.
Chernozhukov, V., Wüthrich, K., and Zhu, Y. (2019). Distributional conformal prediction. arXiv
preprint arXiv:1909.07889.

18

Cox, D. R. (1972). Regression models and life-tables. Journal of the Royal Statistical Society: Series
B (Methodological), 34(2):187–202.
Efron, B. (1979). Bootstrap methods: Another look at the jackknife. The Annals of Statistics, pages
1–26.
Efron, B. (2020). Prediction, estimation, and attribution. International Statistical Review, 88:S28–
S59.
Efron, B. and Tibshirani, R. J. (1994). An introduction to the bootstrap. CRC press.
Emanuel, E., Persad, G., Upshur, R., Thome, B., Parker, M., Glickman, A., Zhang, C., Boyle,
C., Smith, M., and Phillips, J. (2020). Fair allocation of scarce medical resources in the time of
covid-19. The New England Journal of Medicine, 382.
Faraggi, D. and Simon, R. (1995). A neural network model for survival data. Statistics in medicine,
14(1):73–82.
Friedman, J. and Narasimhan, B. (2020). conTree: Contrast Trees and Boosting. R package version
0.2-8.
Friedman, J. H. (2020). Contrast trees and distribution boosting. Proceedings of the National
Academy of Sciences, 117(35):21175–21184.
Gajewski, B. J., Simon, S. D., and Carlson, S. E. (2008). Predicting accrual in clinical trials with
bayesian posterior predictive distributions. Statistics in medicine, 27(13):2328–2340.
Geisser, S. (1993). Predictive inference, volume 55. CRC press.
Goeman, J. J. (2010). L1 penalized estimation in the cox proportional hazards model. Biometrical
journal, 52(1):70–84.
Guan, L. (2019). Conformal prediction with localization. arXiv preprint arXiv:1908.08558.
Gui, J. and Li, H. (2005). Penalized cox regression analysis in the high-dimensional and low-sample
size settings, with applications to microarray gene expression data. Bioinformatics, 21(13):3001–
3008.
Gupta, C., Kuchibhotla, A. K., and Ramdas, A. K. (2019). Nested conformal prediction and quantile
out-of-bag ensemble methods. arXiv preprint arXiv:1910.10562.
Harrell Jr, F. E. (2015). Regression modeling strategies: with applications to linear models, logistic
and ordinal regression, and survival analysis. Springer.
Hong, H. and Tamer, E. (2003). Inference in censored models with endogenous regressors. Econometrica, 71(3):905–932.
Hothorn, T., Bühlmann, P., Dudoit, S., Molinaro, A., and Van Der Laan, M. J. (2006). Survival
ensembles. Biostatistics, 7(3):355–373.
Ishwaran, H., Kogalur, U. B., Blackstone, E. H., Lauer, M. S., et al. (2008). Random survival forests.
Annals of Applied Statistics, 2(3):841–860.
Kalbfleisch, J. D. and Prentice, R. L. (2011). The statistical analysis of failure time data, volume
360. John Wiley & Sons.
Kaplan, E. L. and Meier, P. (1958). Nonparametric estimation from incomplete observations. Journal
of the American statistical association, 53(282):457–481.
19

Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., and Kluger, Y. (2016). Deep
survival: A deep cox proportional hazards network. stat, 1050(2).
Koenker, R. (2020). quantreg: Quantile Regression. R package version 5.75.
Krishnamoorthy, K. and Mathew, T. (2009). Statistical tolerance regions: theory, applications, and
computation, volume 744. John Wiley & Sons.
Lagakos, S. W. (1979). General right censoring and its impact on the analysis of survival data.
Biometrics, pages 139–156.
Lao, J., Chen, Y., Li, Z.-C., Li, Q., Zhang, J., Liu, J., and Zhai, G. (2017). A deep learning-based
radiomics model for prediction of survival in glioblastoma multiforme. Scientific reports, 7(1):1–8.
Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and Wasserman, L. (2018). Distribution-free
predictive inference for regression. Journal of the American Statistical Association, 113(523):1094–
1111.
Lei, J., Robins, J., and Wasserman, L. (2013). Distribution-free prediction sets. Journal of the
American Statistical Association, 108(501):278–287.
Lei, J. and Wasserman, L. (2014). Distribution-free prediction bands for non-parametric regression.
Journal of the Royal Statistical Society: Series B: Statistical Methodology, pages 71–96.
Lei, L. and Candès, E. J. (2020). Conformal inference of counterfactuals and individual treatment
effects. arXiv preprint arXiv:2006.06138.
Leung, K.-M., Elashoff, R. M., and Afifi, A. A. (1997). Censoring issues in survival analysis. Annual
review of public health, 18(1):83–104.
Li, A. H. and Bradic, J. (2020). Censored quantile regression forest. In International Conference on
Artificial Intelligence and Statistics, pages 2109–2119. PMLR.
Murphy, S., Rossini, A., and van der Vaart, A. W. (1997). Maximum likelihood estimation in the
proportional odds model. Journal of the American Statistical Association, 92(439):968–976.
Neyman, J. (1923/1990). On the application of probability theory to agricultural experiments. Essay
on principles. Section 9. Statistical Science, 5:465–472. Translated and edited by D. M. Dabrowska
and T. P. Speed from the Polish original, which appeared in Roczniki Nauk Rolniczyc, Tom X
(1923): 1–51 (Annals of Agricultural Sciences).
Peng, L. and Huang, Y. (2008). Survival analysis with quantile regression models. Journal of the
American Statistical Association, 103(482):637–649.
Portnoy, S. (2003). Censored regression quantiles. Journal of the American Statistical Association,
98(464):1001–1012.
Powell, J. L. (1986). Censored regression quantiles. Journal of econometrics, 32(1):143–155.
Ranney, M. L., Griffeth, V., and Jha, A. K. (2020). Critical supply shortages—the need for ventilators and personal protective equipment during the covid-19 pandemic. New England Journal of
Medicine, 382(18):e41.
Ratkovic, M. and Tingley, D. (2021). Estimation and inference on nonlinear and heterogeneous
effects. Technical report.
Romano, Y., Barber, R. F., Sabatti, C., and Candès, E. J. (2019a). With malice towards none:
Assessing uncertainty via equalized coverage. arXiv preprint arXiv:1908.05428.
20

Romano, Y., Patterson, E., and Candes, E. (2019b). Conformalized quantile regression. In Advances
in Neural Information Processing Systems, pages 3543–3553.
Romano, Y., Sesia, M., and Candès, E. J. (2020). Classification with valid and adaptive coverage.
arXiv preprint arXiv:2006.02544.
Rosenbaum, P. R. and Rubin, D. B. (1983). The central role of the propensity score in observational
studies for causal effects. Biometrika, 70(1):41–55.
Rubin, D. B. (1974). Estimating causal effects of treatments in randomized and nonrandomized
studies. Journal of educational Psychology, 66(5):688.
Rubin, D. B. (1978). Bayesian inference for causal effects: The role of randomization. The Annals
of statistics, pages 34–58.
Sadinle, M., Lei, J., and Wasserman, L. (2019). Least ambiguous set-valued classifiers with bounded
error levels. Journal of the American Statistical Association, 114(525):223–234.
Sant’Anna, P. H. (2016).
arXiv:1604.02642.

Program evaluation with right-censored data.

arXiv preprint

Saunders, C., Gammerman, A., and Vovk, V. (1999). Transduction with confidence and credibility.
In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages
722–726.
Scharfstein, D. O. and Robins, J. M. (2002). Estimation of the failure time distribution in the
presence of informative censoring. Biometrika, 89(3):617–634.
Sesia, M. and Candès, E. J. (2020). A comparison of some conformal quantile regression methods.
Stat, 9(1):e261.
Shafer, G. and Vovk, V. (2008). A tutorial on conformal prediction. Journal of Machine Learning
Research, 9(Mar):371–421.
Shah, R. D., Peters, J., et al. (2020). The hardness of conditional independence testing and the
generalised covariance measure. Annals of Statistics, 48(3):1514–1538.
Simon, N., Friedman, J., Hastie, T., and Tibshirani, R. (2011). Regularization paths for cox’s
proportional hazards model via coordinate descent. Journal of statistical software, 39(5):1.
Stine, R. A. (1985). Bootstrap prediction intervals for regression. Journal of the American Statistical
Association, 80(392):1026–1031.
Therneau, T. M. (2020). A Package for Survival Analysis in R. R package version 3.2-7.
Tibshirani, R. (1997). The lasso method for variable selection in the cox model. Statistics in
medicine, 16(4):385–395.
Tibshirani, R. J., Foygel Barber, R., Candes, E., and Ramdas, A. (2019). Conformal prediction
under covariate shift. Advances in Neural Information Processing Systems, 32:2530–2540.
Tsybakov, A. B. (2008). Introduction to nonparametric estimation. Springer Science & Business
Media.
Vergano, M., Bertolini, G., Giannini, A., Gristina, G., Livigni, S., Mistraletti, G., Riccioni, L.,
and Petrini, F. (2020). Clinical ethics recommendations for the allocation of intensive care treatments in exceptional, resource-limited circumstances: the italian perspective during the covid-19
epidemic. Critical Care, 24.
21

Verweij, P. J. and Van Houwelingen, H. C. (1993). Cross-validation in survival analysis. Statistics
in medicine, 12(24):2305–2314.
von Bahr, B., Esseen, C.-G., et al. (1965). Inequalities for the r-th absolute moment of a sum of
random variables, 1 ≤ r ≤ 2. The Annals of Mathematical Statistics, 36(1):299–303.
Vovk, V. (2002). On-line confidence machines are well-calibrated. In The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings., pages 187–196. IEEE.
Vovk, V. (2012). Conditional validity of inductive conformal predictors. In Asian conference on
machine learning, pages 475–490.
Vovk, V., Gammerman, A., and Shafer, G. (2005). Algorithmic learning in a random world. Springer
Science & Business Media.
Vovk, V., Lindsay, D., Nouretdinov, I., and Gammerman, A. (2003). Mondrian confidence machine.
Technical Report.
Wald, A. (1943). An extension of wilks’ method for setting tolerance limits. The Annals of Mathematical Statistics, 14(1):45–55.
Wang, P., Li, Y., and Reddy, C. K. (2019). Machine learning for survival analysis: A survey. ACM
Computing Surveys (CSUR), 51(6):1–36.
Wei, L.-J. (1992). The accelerated failure time model: a useful alternative to the cox regression
model in survival analysis. Statistics in medicine, 11(14-15):1871–1879.
Wilks, S. S. (1941). Determination of sample sizes for setting tolerance limits. The Annals of
Mathematical Statistics, 12(1):91–96.
Witten, D. M. and Tibshirani, R. (2010). Survival analysis with high-dimensional covariates. Statistical methods in medical research, 19(1):29–51.
Wu, M. C. and Carroll, R. J. (1988). Estimation and comparison of changes in the presence of
informative right censoring by modeling the censoring process. Biometrics, pages 175–188.
Zhang, H. H. and Lu, W. (2007). Adaptive lasso for cox’s proportional hazards model. Biometrika,
94(3):691–703.

A
A.1

Proofs of impossibility results
Proof of Theorem 1
i.i.d.

Let (Xi , Ci , Ti )n+1
∼ (X, C, T ). For notational convenience, we put Zi = (Xi , Ci , Ti ). To avoid
i=1
confusion, we expand L̂(x) into L̂(Z1 , . . . , Zn ; x). Note that L̂ depends on (Z1 , . . . , Zn ) through
(Xi , Ci , Tei )ni=1 . Let
φ(Z1 , . . . , Zn+1 ) = I(Tn+1 < L̂n (Z1 , . . . , Zn ; Xn+1 )).
Since L̂n satisfies (3) under the conditionally independent censoring assumption (1), we have that
P (φ(Z1 , . . . , Zn+1 ) = 1) ≤ α.
22

|=

As a result, if we treat T C | X as a null hypothesis, φ(Z1 , . . . , Zn+1 ) is an α-level test. Note that
X is continuous, and (T, C) are continuous or discrete. By Theorem 2 and Remark 4 of Shah et al.
(2020), for any joint distribution Q of Z with the same continuity conditions on (X, C, T ),
P

(φ(Z1 , . . . , Zn+1 ) = 1) ≤ α.

i.i.d.

Zi ∼ Q

(A.1)

Let Z̃i = (Xi , Ci , Tei ) and Q denote its distribution. Then Tei ∧ Ci = Ti ∧ Ci and thus
L̂n (Z̃1 , . . . , Z̃n ; x) = L̂n (Z1 , . . . , Zn ; x).
Clearly, X is absolutely continuous with respect to the Lebesgue measure and Te, C are absolutely
continuous with respect to the Lebesgue measure or the counting measure. By (A.1) and the
definition of φ, we have


P T̃n+1 ≥ L̂n (Z1 , . . . , Zn ; Xn+1 ) ≥ 1 − α.
The proof is then completed by replacing (Tn+1 , Xn+1 ) with (T, X).

A.2

Proof of Theorem 4

We prove the theorem by modifying the proof of Proposition 4 from Vovk (2012). To avoid confusion,
we expand L̂(x) into L̂(Z1 , . . . , Zn ; x) where Zi = (Xi , Ci , Tei ). Fix any distribution P with (X, C) ∼
P(X,C) and C ≥ 0, T ≥ 0 almost surely. Suppose there exists a set V of PX -non-atom x such that
PX (V) > 0, and for any x ∈ V,
P

i.i.d.

Zi ∼ P

(L̂(Z1 , . . . , Zn , x) > 0) > α.

Since V only includes non-atom x’s, there exists t0 > 0 and δ > 0 such that
P

i.i.d.

Zi ∼ P

(L̂(x) > t0 ) > α + δ,

∀x ∈ V.

(A.2)

We can further shrink V so that
p
2 − 2(1 − PX (V))n ≤ δ/2.

(A.3)

Fix any t1 ∈ (0, t0 ). Define a new probability distribution Q on (X, C, T ) with (X, C) ∼ P(X,C) and
the regular conditional probability

P (T ∈ A | X = x, C = c), x 6∈ V,
Q(T ∈ A | X = x, C = c) =
δt1 (A),
x ∈ V,
where δt1 defines the point mass on t1 . Let dTV denote the total-variation distance. Then,
dTV (P, Q) =

sup

|P (X ∈ AX , C ∈ AC , T ∈ AT ) − Q(X ∈ AX , C ∈ AC , T ∈ AT )|

AX ,AC ,AT

=

sup

|P (X ∈ AX ∩ V, C ∈ AC , T ∈ AT ) − Q(X ∈ AX ∩ V, C ∈ AC , T ∈ AT )|

AX ,AC ,AT

≤

sup

max{P (X ∈ AX ∩ V, C ∈ AC , T ∈ AT ), Q(X ∈ AX ∩ V, C ∈ AC , T ∈ AT )}

AX ,AC ,AT

≤ sup max{P (X ∈ AX ∩ V), Q(X ∈ AX ∩ V)}
AX

= sup PX (AX ∩ V) ≤ PX (V).
AX

23

Using the tensorization inequality for the total-variation distance (see e.g., Tsybakov (2008), Section
2.4) and (A.3), we obtain that
p
dTV (P n , Qn ) ≤ 2 − 2(1 − dTV (P, Q))n ≤ δ/2.
Together with (A.2), this implies that
P

i.i.d.

Zi ∼ Q

(L̂(x) > t0 ) > α + δ/2,

∀x ∈ V.

Let Z = (X, C, T ) be an independent draw from Q. The above inequality can be reformulated as
P

i.i.d.

Zi ,Z ∼ Q

(L̂(X) > t0 | X = x) > α + δ/2,

∀x ∈ V.

Marginalizing over x ∈ V, it implies that
P

i.i.d.

Zi ,Z ∼ Q

(L̂(X) > t0 , X ∈ V) > (α + δ/2)QX (V).

(A.4)

By definition of Q, T = t1 < t0 almost surely conditional on X ∈ V. Thus,
P

i.i.d.

Zi ,Z ∼ Q

(T < L̂(X), X ∈ V) > (α + δ/2)QX (V).

On the other hand, since Q is a distribution with the same marginal distribution of (X, C) and
T ≥ 0 almost surely, for any x,
P

i.i.d.

Zi ,Z ∼ Q

(T < L̂(X) | X = x) ≤ α.

Marginalizing over x ∈ V, it implies that
P

i.i.d.

Zi ,Z ∼ Q

(T < L̂(X), X ∈ V) ≤ αQX (V).

This contradicts (A.4) since QX (V) = PX (V) > 0. The theorem is proved by contradiction.

B

Double robustness of weighted conformal inference

B.1

conformity score via nested sets

Gupta et al. (2019) introduced a broad class of conformity scores characterized by nested sets.
Suppose we have a totally ordered index set S (e.g., R) and a sequence of nested sets {Fs (x; D) :
s ∈ S} in the sense that Fs1 (x; D) ⊂ Fs2 (x; D) for any s1 ≤ s2 ∈ S. Define a score as the index of
the minimal set that includes y, i.e.
V (x, y; D) = inf{s ∈ S : y ∈ Fs (x; D)}.
Without loss of generality, we assume throughout that Finf S (x) is the empty set and Fsup S (x) is
the full domain of Y . The CMR-, CQR-, and CDR-based scores are instances of this:
- CMR score: Fs (x; D) = [m̂(x) − s, ∞).
- CQR score: Fs (x; D) = [q̂α (x) − s, ∞).
- CDR score: Fs (x; D) = [q̂α−s (x), ∞).
We refer the readers to Table 1 of Gupta et al. (2019) for a list of other conformity scores.
24

B.2

A general double robustness property

Throughout the rest of this section, for any event A and random variable X we write P(A | X) for
the conditional expectation E[1A | X]; clearly, P(A | X) is a function of X.
i.i.d.

Theorem B.1. Let (Xi , Yi ) ∼ (X, Y ) ∼ PX × PY |X and let QX be another distribution on the
domain of X. Set N = |Ztr | and n = |Zca |. Further, let {Fs (x; D) : s ∈ S} be any sequence of
nested sets, ŵ(x) = ŵ(x; Ztr ) be an estimate of w(x) = (dQX /dPX )(x), and Ĉ(x) be the resulting
conformal interval from Algorithm 1. Assume that E [ŵ(X) | Ztr ] = 1 and E[w(X)] = 1. Assume
that either B1 or B2 (or both) holds:
B1 lim E |ŵ(X) − w(X)| = 0;
N →∞



B2 (a) PX∼QX (w(X) < ∞) = 1, and there exists δ > 0 such that limsup E ŵ(X)1+δ < ∞;
N →∞

(b) There exists r > 0, s0 ∈ S, b2 > b1 > 0, and a sequence of oracle nested sets {Os (x)}s∈S ,
such that
(i) for any ε ∈ [0, r],
1 − α − b2 ε ≤ P (Y ∈ Os0 −ε (X) | X) ≤ 1 − α − b1 ε,

almost surely;

(ii) lim E [ŵ(X)∆(X)] = lim E [w(X)∆(X)] = 0, where ∆(x) = sups∈[s0 −r,s0 ] ∆s (x),
N →∞

N →∞

∆s (x) = inf {∆ ≥ 0 : s − ∆ ∈ S, Fs−∆ (x; Ztr ) ⊂ Os (x) and Os−∆ (x) ⊂ Fs (x; Ztr )} .
Then


lim P(X,Y )∼QX ×PY |X Y ∈ Ĉ(X) ≥ 1 − α.

N,n→∞

Furthermore, under B2, for any ε > 0,
 


lim PX∼QX P Y ∈ Ĉ(X) | X ≤ 1 − α − ε = 0.
N,n→∞

(B.1)

(B.2)

The proof of Theorem B.1 is a generalization of Theorem A.1 of Lei and Candès (2020). Here
we present a self-contained proof for completeness. We start with three lemmas.
Lemma B.1 (Equation (2) in Lemma 1 from Tibshirani et al. (2019)). Let v1 , . . . , vn+1 ∈ R and
(p1 , . . . , pn+1 ) ∈ R be non-negative reals summing to 1. Then for any β ∈ [0, 1] and
!
!
n+1
n
X
X
vn+1 ≤ Quantile β;
pi δvi ⇐⇒ vn+1 ≤ Quantile β;
pi δvi + pn+1 δ∞ .
i=1

i=1

Lemma B.2 (Equation (10) from Tibshirani et al. (2019)). Let dTV (Q1X , Q2X ) denote the totalvariation distance between Q1X and Q2X . Then

dTV Q1X × PY |X , Q2X × PY |X = dTV (Q1X , Q2X )
Lemma B.3 (Theorem 2 of von Bahr et al. (1965)). Let Zi be independent mean-zero random
variables. Then for any δ ∈ [0, 1),
E

n
X

1+δ

Zi

≤2

i=1

n
X

E|Zi |1+δ .

i=1
i.i.d.

Throughout the proof, we let (X1 , Y1 ), . . . , (Xn , Yn ) ∼ PX × PY |X be the calibration set and let
(Xn+1 , Yn+1 ) ∼ QX × PY |X be a test point. Further, let Zi = (Xi , Yi ), Z = (Z1 , . . . , Zn+1 ), and V =
?
?
(V1 , . . . , Vn+1 ). For any permutation π on {1, . . . , n+1} and v ? ∈ Rn+1 , let vπ? = (vπ(1)
, . . . , vπ(n+1)
).
Additionally, we define E(v) to be the unordered set of v (where repetition of elements is allowed).
25

Proof of Theorem B.1 under assumption B1 We first consider the case where QX is absolutely
continuous with respect to PX , i.e.
PX∼QX (w(X) < ∞) = 1.
In this case, for any measurable function f ,
EX∼QX [f (X)] = EX∼PX [w(X)f (X)].

(B.3)

On the other hand, it always holds that PX∼PX (w(X) < ∞) = 1. In addition, the assumption
EX∼PX [ŵ(X) | Ztr ] < ∞ implies that PX∼PX (ŵ(X) < ∞) = 1. By (B.3),
PX∼QX (ŵ(X) < ∞) = 1 − EX∼PX [w(X)I(ŵ(X) = ∞)].
Since the integrand is non-negative,
EX∼PX [w(X)I(ŵ(X) = ∞)]
= lim EX∼PX [w(X)I(w(X) ≤ K, ŵ(X) = ∞)] ≤ lim KPX∼PX (ŵ(X) = ∞) = 0.
K→∞

K→∞

Thus, we also have
PX∼QX (ŵ(X) < ∞) = 1.
Next, observe that
d

?
(V | E(Z) = E(z ? ), Ztr ) = vΠ
,

where Π is a random permutation on {1, . . . , n + 1} with
P (Π = π | E(Z) = E(z ? ), Ztr ) = P

?
?
P(Z1 = zπ(1)
, . . . , Zn+1 = zπ(n+1)
)

π0

=

P(Z1 = zπ? 0 (1) , . . . , Zn+1 = zπ? 0 (n+1) )

w(x?π(n+1) )
.
Pn+1
n! i=1 w(x?i )

Note that this conditional probability is well-defined because w(X) < ∞ almost surely under both
PX and QX . Consequently, for any i ∈ {1, . . . , n + 1},

w(x?j )
P (Π(n + 1) = j | E(Z) = E(z ? ), Ztr ) = Pn+1
= pj x?n+1 ,
?
i=1 w(xi )
where pn+1 refers to p∞ for notational convenience. As a result,
d

(Vn+1 | E(Z) = E(z ? ), Ztr ) =

n+1
X

pj (x?n+1 )δvj? .

j=1

e X be a new measure with
Let Q
e X (x) = ŵ(x)dPX (x).
dQ
Since EX∼PX [ŵ(X)] = 1, PX∼PX (ŵ(X) < ∞) = 1. As a result, Q̃X is a probability measure. Let
en+1 = (X
en+1 , Yen+1 ) ∼ Q
e X × PY |X . Further let Ze = (Z1 , . . . , Zn , Z
en+1 ) and Ve = (Ve1 , . . . , Ven+1 )
Z
e Using the above result, we have that
denote the conformity score computed with Z.


 n+1
X
d
e Ztr =
en+1 )δ e .
Ven+1 | E(Z),
p̂j (X
Vj
j=1

26

Note that each p̂i (X̃n+1 ), i = 1, . . . , n + 1 is well-defined since ŵ(Xi ) is almost surely finite under
both PX and QX . As a consequence,


en+1 ) | Ztr
P Yen+1 ∈ Ĉ(X
!
!
n
X
en+1 )δ e + p̂n+1 (Ven+1 )δ∞ | Ztr
= P Ven+1 ≤ Quantile 1 − α;
p̂i (X
Vi

i=1

"
= E P Ven+1 ≤ Quantile 1 − α;

n
X

!
en+1 )δ e + p̂n+1 (Ven+1 )δ∞
p̂i (X
Vi

!
e Ztr
| E(Z),

#
| Ztr

i=1
(1)

"

= E P Ven+1 ≤ Quantile 1 − α;

n+1
X

!
en+1 )δ e
p̂i (X
Vi

!
e Ztr
| E(Z),

#
| Ztr

i=1
(2)

≥ 1 − α,
where step (1) is due to Lemma B.1, and step (2) follows from the definition of the quantile function.
On the other hand,




en+1 ) | Zca , Ztr − P Yn+1 ∈ Ĉ(Xn+1 ) | Zca , Ztr
P Yen+1 ∈ Ĉ(X




e X × PY |X , QX × PY |X = dTV Q
e X , QX ,
≤ dTV Q
where the last equality is a result of Lemma B.2. Rearranging the above inequality and taking
expectation w.r.t. Zca , we obtain that






en+1 ) | Ztr − dTV Q
e X , QX
P Yn+1 ∈ Ĉ(Xn+1 ) | Ztr ≥ P Yen+1 ∈ Ĉ(X


e X , QX .
≥ 1 − α − dTV Q
Note that
Z
Z


e X − dQX | = 1 |ŵ(x) − w(x)|dPX (x) = 1 EX∼P |ŵ(X) − w(X)|.
e X , QX = 1 |dQ
dTV Q
X
2
2
2
With assumption B1, we reach the conclusion


lim P(X,Y )∼QX ×PY |X Y ∈ Ĉ(X) =
N,n→∞



lim P Yn+1 ∈ Ĉ(Xn+1 ) ≥ 1 − α.

N,n→∞

(B.4)

Next, we extend the result to the case where PX∼QX (w(X) < ∞) < 1. If PX∼PX (ŵ(X) <
∞) < 1, it is clear that EX∼PX |ŵ(X) − w(X)| = ∞, conflicting with the assumption B1. Thus,
PX∼PX (ŵ(X) < ∞) = 1.
Let Q0X denote the distribution QX conditional on the event V∞ , {x : w(x) < ∞}; that is,
dQ0X (x) =

I(x ∈ V∞ )dQX (x)
.
PX∼QX (V∞ )

Further, set w0 (x) = dQ0X (x)/dPX (x) and ŵ0 (x) = ŵ(x)I(w ∈ V∞ )/PX∼QX (V∞ ). Note that Ĉ(x)
remains the same on V∞ when ŵ is replaced by ŵ0 and QX is replaced by Q0X , because weighted
split conformal inference is invariant with respect to rescalings of the covariate shift estimate. Since
PX∼Q0X (w(X) < ∞) = 1, (B.4) implies that


1
P(X,Y )∼Q0X ×PY |X Y ∈ Ĉ(X) ≥ 1 − α − EX∼PX |ŵ0 (X) − w0 (X)|.
2
27

It can be reformulated as


P Yn+1 ∈ Ĉ(Xn+1 ) | w(Xn+1 ) < ∞ ≥ 1 − α −

1
EX∼PX |ŵ(X) − w(X)|.
2PX∼QX (V∞ )

On the other hand, when w(Xn+1 ) = ∞, η(Xn+1 ) = ∞ implying that L̂(Xn+1 ) = −∞. As a result,


P Yn+1 ≥ L̂(Xn+1 ) | w(Xn+1 ) = ∞ = 1.
Putting the two pieces together, we conclude that


P Yn+1 ≥ L̂(Xn+1 )


= P Yn+1 ≥ L̂(Xn+1 ) | w(Xn+1 ) < ∞ P (w(Xn+1 ) < ∞)


+ P Yn+1 ≥ L̂(Xn+1 ) | w(Xn+1 ) = ∞ P (w(Xn+1 ) = ∞)
1
c
≥ (1 − α)PX∼QX (V∞ ) + PX∼QX (V∞
) − EX∼PX |ŵ(X) − w(X)|
2
1
≥ 1 − α − EX∼PX |ŵ(X) − w(X)|
2
→ 1 − α.
Proof of Theorem B.1 under Assumption B2 Note that Assumption B2 (b) (i) implies that
w(X) is almost surely finite under QX and ŵ(X) is almost surely finite under PX . By the same
reasoning as in the last subsection, w(X) is almost surely finite under PX and ŵ(X) is almost surely
finite under QX .
e Ye ) ∼ QX × PY |X . Under Assumption B2, our first claim is that the calibration term
Let (X,
e
η(X) will be larger than s0 asymptotically. This statement is formalized in Lemma B.4.
Lemma B.4. Under Assumption B2, for any 0 < ε < r/3,


e
lim PX∼Q
η(
X)
≥
s
−
ε
= 1.
0
e
X
N,n→∞

We defer the proof of Lemma B.4 to the end of this section. For now, assuming that Lemma B.4
holds, we have that for any 0 < ε < r/3,


e | X,
e Ztr
P Ye ∈ Ĉ(X)


e Ye ; Zca ) ≤ η(X)
e | X,
e Ztr
= P V (X,


e
e
≥ P Ye ∈ Fη(X)
e (X; Ztr ) | X, Ztr




e Zca ) | X,
e Ztr − P η(X)
e < s0 − ε | X,
e Ztr
≥ P Ye ∈ Fs −ε (X;
0

(1)






e
e
e
e
≥ P Ye ∈ Os0 −2ε−∆(X)
e (X) | X, Ztr − P η(X) < s0 − ε | X, Ztr




e | X,
e Ztr − 1{∆(X)
e > ε} − P η(X)
e < s0 − ε | X,
e Ztr
≥ P Ye ∈ O
(X)
e
e
s0 −2ε−∆(X)1{∆(X)≤ε}

(2)





e
e ≤ ε} − 1{∆(X)
e > ε} − P η(X)
e < s0 − ε | X,
e Ztr
≥ 1 − α − b2 2ε + ∆(X)1{∆(
X)


e > ε} − P η(X)
e < s0 − ε | X,
e Ztr .
≥ 1 − α − 3b2 ε − 1{∆(X)
28

Above, step (1) is due to the definition of ∆(x), and step (2) follows from condition (b) (i) in
e and Ztr ,
Assumption B2. Taking expectation w.r.t. X






e ≥ 1 − α − 3b2 ε − P ∆(X)
e > ε − P η(X)
e < s0 − ε
P Ye ∈ Ĉ(X)
h
i


(1)
e /ε − P η(X)
e < s0 − ε
≥ 1 − α − 3b2 ε − E ∆(X)


e < s0 − ε ,
= 1 − α − 3b2 ε − EX∼PX [∆(X)w(X)] /ε − P η(X)
where step (1) is implied by Markov’s inequality. By Assumption B2, the second term vanishes
asymptotically. By Lemma B.4,




e ≥ 1 − α − 3b2 ε.
lim P(X,Y )∼QX ×PY |X Y ∈ Ĉ(X) = lim P Ye ∈ Ĉ(X)
N,n→∞

N,n→∞

Since the above holds for arbitrary ε > 0, we complete the proof of (B.1).
To see (B.2), note that
 


e | X,
e Ztr ≤ 1 − α − 4b2 ε
P P Ye ∈ Ĉ(X)




e > ε} − P η(X)
e < s0 − ε | X,
e Ztr ≤ 1 − α − 4b2 ε
≤ P 1 − α − 3b2 ε − 1{∆(X)




e > ε} + P η(X)
e < s0 − ε | X,
e Ztr ≥ b2 ε
≤ P 1{∆(X)


 


e > ε + P P η(X)
e < s0 − ε | X,
e Ztr > b2 ε
≤ P ∆(X)


e < s0 − ε /(b2 ε),
≤ E [∆(X)w(X)] /ε + P η(X)
where the last step applies (B.3) and Markov’s inequality. By condition (b) (ii) in assumption
B2 and Lemma B.4,
 


e | X,
e Ztr ≤ 1 − α − 4b2 ε = 0.
lim P P Ye ∈ Ĉ(X)
N,n→∞

Replacing 4b2 ε with ε yields the desired result.
Finally we present a proof of Lemma B.4.
e G(t) be the (random) CDF of Pn p̂i (X)δ
e V +
Proof of Lemma B.4 Let D = {Ztr , (Xi )ni=1 , X},
i
i=1
e V and G∗ (t) = E[G(t) | D]. To begin with, note that
p̂∞ (X)δ
∞


e < s0 − ε ≤ P (G(s0 − ε) ≥ 1 − α) .
P η(X)
It then suffices to show that

lim P(G(s0 − ε) ≥ 1 − α) = 0 for all 0 < ε ≤ r/3. By definition,

N,n→∞

G(s0 − ε) − G∗ (s0 − ε) =

n
X

e [1{Vi ≤ s0 − ε} − P(Vi ≤ s0 − ε | D)] .
p̂i (X)

i=1

Conditional on D, G(s0 − ε) − G? (s0 − ε) is σ 2 -sub-Gaussian, where
!
n
n
X
X
2
2 e
e
e
e
σ =
p̂i (X) ≤ max p̂i (X)
p̂i (X) ≤ max p̂i (X).
i=1

1≤i≤n

i=1

29

1≤i≤n

Then by Hoeffding’s inequality,
t2

∗

P (G(s0 − ε) − G (s0 − ε) ≥ t | D) ≤ exp −

!
(B.5)

e
2 maxi p̂i (X)

On the other hand, when ε < r/3,
G∗ (s0 − ε) =
=
(1)

≤

n
X
i=1
n
X

e (Vi ≤ s0 − ε | D)
p̂i (X)P
e [P (Vi ≤ s0 − ε, ∆(Xi ) > ε/2 | D) + P (Vi ≤ s0 − ε, ∆(Xi ) ≤ ε/2 | D)]
p̂i (X)

i=1
n
X

e [1 {∆(Xi ) > ε/2} + P (Vi ≤ s0 − ε/2 − ∆(Xi ), ∆(Xi ) ≤ ε/2 | D)]
p̂i (X)

i=1

≤
(2)

≤

n
X



e 1 {∆(Xi ) > ε/2} + P Yi ∈ Fs −ε/4−∆(X ) (Xi ; Ztr ) | D
p̂i (X)
0
i

i=1
n
X



e 1 {∆(Xi ) > ε/2} + P Yi ∈ Os −ε/8 (Xi ) | D
p̂i (X)
0

i=1
(3)

≤

n
X

e {∆(Xi ) > ε/2} + 1 − α − b1 ε
p̂i (X)1
8
i=1
n

≤

2X
b1 ε
e
p̂i (X)∆(X
,
i) + 1 − α −
ε i=1
8

(B.6)

where step (1) holds because ∆(Xi ) is deterministic conditional on D, step (2) follows from the
definition of ∆(Xi ), and step (3) follows from the condition (b) (i). Combining (B.5) and (B.6), we
have
P(G(s0 − ε) ≥ 1 − α)
!


n
2
X
b
ε
b
ε
1
1
e
≤ P G(s0 − ε) − G∗ (s0 − ε) ≥
+P
p̂i (X)∆(X
i) ≥
16
32
i=1
!#
"
!
n
X
b21 ε2
b1 ε2
e
≤ E exp −
+P
p̂i (X)∆(X
i) ≥
e
32
512 maxi p̂i (X)
i=1
!
 2 2



n
2
X
b1 ε
1
b
ε
1
e ≥
e
≤ exp −
log n + P max p̂i (X)
+P
p̂i (X)∆(X
(B.7)
i) ≥
i
512
log n
32
i=1
e
By definition, for any value of X,
e ≤ Pnŵ(Xi ) .
p̂i (X)
j=1 ŵ(Xj )
Consequently,


e ≥ 1
P max p̂i (X)
i
log n


≤P

n
X

n
ŵ(Xi ) ≤
2
i=1

30

!


+ P max ŵ(Xi ) ≥
i

n
2 log n


.

Assume without loss of generality that 0 < δ < 1. Since E[ŵ(Xi ) | Ztr ] = 1, Markov’s inequality
and Lemma B.3 give


!
!
1+δ
n
n
n
X
X
n
n
21+δ  X

ŵ(Xi ) − 1 ≥
ŵ(Xi ) − 1
P
ŵ(Xi ) ≤
≤P
≤ 1+δ E
2
2
n
i=1
i=1
i=1
≤

n
 22+2δ
 22+δ 



22+δ X 
1+δ
1+δ
E
|
ŵ(X
)
−
1|
≤
E
|
ŵ(X
)
−
1|
=
E ŵ(X1 )1+δ + 1 . (B.8)
1
i
n1+δ i=1
nδ
nδ

Similarly,


1+δ 
21+δ (log n)1+δ
n
E
max
ŵ(X
)
≤
i
i
i
2 log n
n1+δ
" n
#
X
21+δ (log n)1+δ
21+δ (log n)1+δ
≤
E
ŵ(Xi )1+δ =
E[ŵ(Xi )1+δ ].
1+δ
δ
n
n
i=1


P max ŵ(Xi ) ≥

(B.9)

Combining (B.8) and (B.9) yields

e ≥
lim P max p̂i (X)

N,n→∞

i

1
log n


= 0.

(B.10)

Similarly,
!
!
!
n
n
2
2
X
X
n
b
nε
b
ε
1
1
e
≤P
ŵ(Xi ) ≤
+P
ŵ(Xi )∆(Xi ) ≥
P
p̂i (X)∆(X
i) ≥
32
2
64
i=1
i=1
i=1
!
n
X
n
64
N,n→∞
≤P
ŵ(Xi ) ≤
+
E [ŵ(Xi )∆(Xi )] −→ 0.
(B.11)
2
b1 ε
i=1
n
X

Together, (B.7), (B.10) and (B.11) imply


e < s0 − ε = 0.
lim P η(X)

N,n→∞

B.3

Proof of Theorem 2

Recall that Yi = Ti ∧ c0 , w(x) = 1/c(x) and ŵ(x) = 1/ĉ(x). First we show that Assumption A1 of
Theorem 2 implies Assumption B1 of Theorem B.1. Since E[1/ĉ(X) | Ztr ] < ∞ almost surely and
E[1/c(X)] < ∞, we set
ŵN (x) =

1/ĉ(x)
dPX (x)
1/c(x)
, w(x) =
=
E[1/ĉ(X) | Ztr ]
dPX|T =1 (x)
E[1/c(X)]

and observe
E[ŵN (X) | Ztr ] = 1 = E[w(X)].
Thus, Assumption B1 reduces to
lim E

N →∞

1/ĉ(X)
1/c(X)
−
= 0.
E[1/ĉ(X) | Ztr ] E[1/c(X)]

31

In fact,
1/ĉ(X)
1/c(X)
−
E[1/ĉ(X) | Ztr ] E[1/c(X)]


1
1
1
1
1
1
≤ lim sup
+ lim sup E
E
−
E
−
ĉ(X) c(X)
c(X)
E[1/ĉ(X) | Ztr ] E[1/c(X)]
N →∞ E[1/ĉ(X) | Ztr ]
N →∞






(1)
1
1
1
1
1
+ lim sup E
≤ lim sup E
−
EE
| Ztr − E
ĉ(X) c(X)
c(X)
ĉ(X)
c(X)
N →∞
N →∞


  
1
1
1
1
1
≤ lim sup E
+ lim sup E
| Ztr
−
E E
−
ĉ(X) c(X)
c(X)
ĉ(X) c(X)
N →∞
N →∞



1
1
1
(2)
= 1+E
= 0,
lim sup E
−
c(X)
ĉ(X)
c(X)
N →∞
lim E

N →∞

where step (1) uses the fact that c(x), ĉ(x) ∈ [0, 1], and step (2) uses Assumption A1 and the
condition E[1/c(X)] < ∞.
Next we show that Assumption A2 implies Assumption B2. Remark that Assumption B2 (a) is
satisfied since E[1/ĉ(X)1+δ ] < ∞). It remains to prove that A2 =⇒ B2 (b). Let
Os (x) = [qα (x; c0 ) − s, ∞).
Clearly, Assumption A2 (i) implies Assumption B2 (b) (i) with s0 = 0. Moreover,
∆s (x) = inf {∆ : q̂α (x; c0 ) − s + ∆ ≥ qα (x; c0 ) − s and qα (x; c0 ) − s + ∆ ≥ q̂α (x; c0 ) − s}
= inf {∆ : q̂α (x; c0 ) + ∆ ≥ qα (x; c0 ) and qα (x; c0 ) + ∆ ≥ q̂α (x; c0 )}
= |q̂α (x; c0 ) − qα (x; c0 )| = E(x).
Thus, ∆(x) = sups∈[s0 −r,s0 ] ∆s (x) = E(x) and Assumption B2 (b) (ii) holds.

B.4

Proof of Theorem 3

Using the same argument as in the proof of Theorem 2, it suffices to show that A2 from Theorem 3
implies B2 (b) from Theorem B.1. Let
Os (x) = [qα−s (x; c0 ), ∞),
Clearly, Assumption A2 (i) implies Assumption B2 (b) (i) with s0 = 0.
To compute ∆(x), we first replace r by 2r. Assumption A2 (i) with ε = 0 implies that
F (qα−s (x; c0 ) | X = x) = 1 − α + s for any s ∈ [s0 − 2r, s0 ] = [−2r, 0]. Then for any s ∈ [−r, 0],
∆s (x) = inf {∆ : qα−s+∆ (x; c0 ) ≥ q̂α−s (x; c0 ) and q̂α−s+∆ (x; c0 ) ≥ qα−s (x; c0 )} .
Let V denote the event that E(X) ≤ r. Then on V, α − s + E(X) ∈ [α, α + 2r], and by A2 (i),
α − s + E(X) = F (qα−s (X; c0 ) | X) + E(X) and F (qα−s+E(x) (X; c0 ) | X) = α − s + E(X)
=⇒ α − s + E(X) ≥ F (q̂α−s (X; c0 ) | X) and F (q̂α−s+E(X) (X; c0 ) | X) ≥ α − s
=⇒ qα−s+E(X) (X; c0 ) ≥ q̂α−s (X; c0 ) and q̂α−s+E(X) (X; c0 ) ≥ qα−s (X; c0 )
=⇒ ∆s (X) ≤ E(X).
Thus on V,
∆(X) =

∆s (X) ≤ E(X).

sup
s∈[s0 −r,s0 ]

32

On the other hand, ∆(X) ≤ 1 almost surely. Since c(X) ≤ 1 almost surely, A2 (ii) implies that
E[∆(X)] → 0. By Markov’s inequality,
P(V) → 1.
As a result,
lim E[∆(X)/ĉ(X)] ≤ lim sup E[∆(X)IV /ĉ(X)] + E[∆(X)IV c /ĉ(X)]

N →∞

N →∞

≤ lim sup E[E(X)/ĉ(X)] + αE[IV c /ĉ(X)]
N →∞

≤ lim sup E[E(X)/ĉ(X)] + αP(V c )1+1/δ E[1/ĉ(X)1+δ ] = 0,
N →∞

where the second last step follows from Hölder’s inequality. Similarly, we have
lim E[∆(X)/c(X)] = 0.

N →∞

Since E[1/ĉ(X)], E[1/c(X)] ≥ 1, we conclude that
lim E[ŵ(X)∆(X)] = lim E[w(X)∆(X)] = 0.

N →∞

N →∞

In conclusion, A2 implies B2 (b) (ii). This completes the proof of Theorem 3.

B.5

When is CMR-LPB doubly robust?

For CMR, a natural oracle nested set is given by
Os (x) = [m(x; c0 ) − s, ∞),

where m(x; c0 ) = E[T ∧ c0 | X = x].

However, Assumption B2 (b) (i) with ε = 0 requires the existence of s0 such that
P(T ∧ c0 ∈ Os0 (X) | X) = 1 − α,

almost surely.

This implies that for some s0 ,
P(T ∧ c0 ≥ m(X; c0 ) − s0 | X) = 1 − α,

almost surely.

The above equality does not hold in general. One exception is the additive case with homoscedastic
errors:
T ∧ c0 = m(X; c0 ) + ν, Var[ν | X] = Var[ν].
In this case, we can derive the double robustness of the CMR-LPB based on Theorem B.1.

33

