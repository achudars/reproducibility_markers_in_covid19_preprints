Intra-model Variability in COVID-19 Classification Using Chest X-ray Images
Brian D Goodwin∗ Corey Jaskolski∗ Can Zhong∗ Herick Asmani∗
∗ Synthetaic

arXiv:2005.02167v1 [eess.IV] 30 Apr 2020

{brian,corey,can,herick}@synthetaic.com
Abstract—X-ray and computed tomography (CT) scanning
technologies for COVID-19 screening have gained significant
traction in AI research since the start of the coronavirus
pandemic. Despite these continuous advancements for COVID19 screening, many concerns remain about model reliability
when used in a clinical setting. Much has been published,
but with limited transparency in expected model performance.
We set out to address this limitation through a set of
experiments to quantify baseline performance metrics and
variability for COVID-19 detection in chest x-ray for 12
common deep learning architectures. Specifically, we adopted
an experimental paradigm controlling for train-validation-test
split and model architecture where the source of prediction
variability originates from model weight initialization, random
data augmentation transformations, and batch shuffling. Each
model architecture was trained 5 separate times on identical
train-validation-test splits of a publicly available x-ray image
dataset provided by Cohen et al. (2020). Results indicate that
even within model architectures, model behavior varies in
a meaningful way between trained models. Best performing
models achieve a false negative rate of 3 out of 20 for detecting
COVID-19 in a hold-out set. While these results show promise
in using AI for COVID-19 screening, they further support the
urgent need for diverse medical imaging datasets for model
training in a way that yields consistent prediction outcomes.
It is our hope that these modeling results accelerate work in
building a more robust dataset and a viable screening tool for
COVID-19.
Keywords-Deep Learning; COVID-19; X-ray; Screening;

I. I NTRODUCTION
The spread of the novel coronavirus, which causes
COVID-19, has caught most of the world off-guard resulting
in severely limited testing capabilities. For example, as of
April 15, 2020 almost 3 months since the first case in the
US, only about 3.3 million tests have been administered [1],
which equates to approximately 1% of the US population.
Reverse transcription-polymerase chain reaction (RT-PCR)
is an assay commonly used to test for COVID-19, but is
available in extremely limited capacity [2], [3]. In an effort
to offer a minimally invasive, low-cost COVID-19 screen via
x-ray imaging, AI engineers and data scientists have begun
to collect datasets [4] and utilize computer vision and deep
learning algorithms [5]. All these efforts seek to leverage an
available medical imaging modality for both diagnosis and,
in the future, predicting case outcome. Clinical observations
have largely propelled AI research in computer vision for
screening COVID-19, and these reports cite differentiable
lung abnormalities of COVID-19 patients from chest CT [6],

x-ray [7], [8], and even ultrasound [9]. Current research also
shows that COVID-19 is correlated with specific biomarkers
in x-ray [10].
Though these recent efforts are valuable in that they
will lay the foundation for future work in this area, there
are significant flaws in the methodology as well as in the
behavior of the resultant models. Much of the initial work
on COVID-19 prediction from chest x-ray used a training
set that included a little over 100 images with 10 test images
(that were, in fact, identical to the validation set). Though
such small test data sets do not allow for declaring sweeping
diagnostic value statements, unfortunately the popular media
articles effectively hype the value of these models with
hopeful titles like ”Coronavirus Neural Network can help
spot COVID-19 in Chest X-rays” [11], ”How AI Is Helping
in the Fight Against COVID-19” [12], and ”A.I. could help
spot telltale signs of coronavirus in lung X-rays” [13].
Network weights from these publications are not publicly
available for these published models. We have responded
to this shortcoming by providing pre-trained weights for
many of the most common deep learning architectures for
computer vision, and we have made the code for pre-training
freely available. To our knowledge, this repository of pretrained model weights is the first of its kind in response to
the current crisis and the first to report prediction results
across multiple architectures on a test set that is held out
from the validation and training sets.
Our goal is to facilitate advancement of screening technology for COVID-19 and highlight the need for larger, more
diverse datasets. The urgency for a clinical methodology to
provide COVID-19 screens cannot be understated [14]. Our
hope is twofold: 1) that the community advances computer
vision for COVID-19 detection via x-ray before recommending use in a clinical setting and 2) that pre-trained model
weights will help accelerate ongoing development in AI to
augment the decision-making process for clinicians during
a time where healthcare workers are under a severe amount
of stress.
II. M ETHODS
We carried out a series of experiments to quantify baseline
machine-learning performance in detecting COVID-19 from
chest x-ray images using a series of common, openly available neural network architectures. Computational benchmarking was outside of our experimental approach since

it has been extensively studied [15]. In this study, we
focused on quantifying the expected variability in prediction
outcomes and sought to quantify the reliability of predictions
with respect to the chest x-ray data that is currently available to the public and contains COVID-19 positive scans.
All computational experiments were executed on Lambda
Blade (lambdalabs.com) hardware (8x RTX 8000 + NVLink
GPUs with 48GB GDDR6 RAM, 2x Intel Xeon Gold 5218
processor with 512 GB RAM) using the Pytorch framework
[16].
A. Experimental Design
An identical train-validation-test split (TVTS) was employed for all experiments, and each model architecture
was trained 5 separate times (creating 5 separate models,
each). We controlled for TVTS and model architecture
while allowing randomness in weight initialization and data
augmentation during batch training for each experiment. We
designed our approach with the aim to elucidate expected
variability in model behavior for COVID-19 detection in
chest x-ray.
B. Data
Since COVID-19 datasets are becoming more abundant,
existing sets are constantly growing and evolving, and it has
become important to cite the data source and the day it was
acquired. Specifically, this study uses data from the Cohen et
al. [4] chest x-ray dataset that was acquired on 2020-04-17.
This dataset contains three classes: 1) healthy, 2) community
acquired pneumonia (CAP), and 3) COVID-19 (examples in
Figure 1). We employed a modified 80-10-10 TVTS (Table I)
training paradigm. It was modified to maximize the number
of COVID-19 training samples and double the size of the test
hold out set used in previous work [5]. The dataset version
at the time of this study was large enough to accommodate
the addition of a COVID-19 validation set and test hold out
set 2x larger than previous work (from 10 hold out images
to 20) [5].
Table I: Number of x-ray images in each split for training,
validation, and hold-out testing.

Train
Validation
Test

COVID-19
175
20
20

CAP
4836
605
604

Normal
7081
885
885

C. Architectures
We elected to generate baseline results for the following
commonly used architectures: Resnet-18, -50, -101, -152
[17], WideResnet-50, -101 [18], ResNeXt-50, -101 [19],
MobileNet-v1 [20], Densenet-121, -169, -201 [21].
ADAM optimization was used during training on only
the last fully connected layer in each network using a batch

size of 128 resulting in a mean compute time of 156.7+/50.7 sec/epoch across all architectures. Since all models
have been pre-trained on ImageNet, we elected to freeze
the convolutional layers to retain the higher-level learned
features [22]; i.e., all weights were frozen but for the final
layer in each network. Models were trained on chest x-ray
images of size 3 x 512 x 512 px. All images were assumed to
be RGB channels despite their inherent grayscale property.
Therefore, no manipulations were made to the networks to
uniquely accommodate single channel x-ray imaging data.
All models were trained for 100 epochs with stopping
criteria, and weights from lowest validation loss were saved
out; COVID-19 recall was not considered during training.
All experiments were carried out using weighted cross
entropy loss (wCEL) where the contribution to the loss
from a given class is weighted based on its representative
proportion in the total dataset.

P

loss(x, class) = weight[class] −x[class] + log
j exp(x[j])

Our decision to use wCEL (as opposed to CEL) is based on
the objective to achieve a high recall in the underrepresented
class (the COVID-19 class) since the AI task is straightforward: detect COVID-19. Stopping criteria was based on
plateau of the validation wCEL. Performance metrics were
then calculated using only the test holdout set. Our first
iteration of testing (not reported in this paper) used CEL, and
performance metrics were found to have improved dramatically when models were trained using wCEL (as reported
in this paper). Validation losses followed a common trend
across models during training with a distribution illustrated
in Figure 2).

D. Performance Indices
Each training experiment was carried out on an identical
TVTS of the dataset. For each architecture, 5 training runs
were carried out to gather a small distribution of models (and
prediction outcomes). The class prediction (and therefore
COVID-19 detection) was based solely on the maximum
value output from the softmax layer (3 total classes). Sources
of variability across all experiments included image augmentation transformations that are based on random draws from
a binomial probability distribution, shuffling for batch allocation (i.e., each batch ID does not contain identical images
across all experiments), and random weight initialization
(last layer only).
Datasets were prepared in a manner consistent with Wang
et al. [5] (see github.com/lindawangg/COVID-Net), and a
data augmentation protocol was implemented. Given the
consistent format of a chest x-ray, only modest translational
and rotational augmentations were applied with brightness
jitter and a possibility of horizontal flip (Table II).

Figure 1: Two example images from each class in the dataset.

Table II: Data augmentation composition.

mobilenet_v2
1.0

WCE Loss

resnet152
0.8

resnext101_32x8d

0.6

Augmentation
Horizontal Flip
Brightness Jitter
Random Rotation
Random Translation

Range
[0.9, 1.1]*value
[-10, 10] degrees
[-0.1, 0.1]*size

Probability
0.5
0.5
0.5
0.5

0.4

0

20

40

60

80

Training Epoch
Figure 2: Validation losses from trained epochs for all models with
a select few shown by their respective line color.

E. Statistical Analysis
Multiple measures of accuracy and uncertainty were calculated to quantify baseline performance expectations for
each network. We report statistical tests, model performance
characteristics, and common accuracy metrics with the aim
to quantify the expected performance and the variability
of model performance given the size of current COVID-19
chest x-ray datasets. Specifically, we report Type I (false pos-

itive; FP) and Type II (false negative; FN) error frequencies
in the test set as well as model consistency via McNemars
chi-squared test (with a continuity correction), which is a
common way to test for similarity between models [23],
[24]. This statistical test was carried out using the method
described in Dietterich [23] for a binary classification task
(COVID-19 v. non-COVID-19). All statistical analyses were
carried out using R [25], and figures were built using ggplot2
[26].
III. R ESULTS
Among all tested models, Densenet-169 was found to have
the highest false negative rate (FNR), Densenet-121 had
the highest FNR variance, and Resnet-18 had the highest
mean false positive rate (FPR) (Figure 3). All plot labels
for model architectures are organized by number of tunable

parameters increasing from left to right (order reversed
in Figure 5B). The lowest FNR (0.15) was achieved by
Densenet-201 and ResneXt-101. Overall, Type I and II error
rates varied across models and varied modestly within model
architecture (Figure 3).
FNR (Type II)
0.4
0.3
0.2
0.1
0.0
FPR (Type I)

0.03
0.02
0.01
0.00
v2 21 t18 69 01 t50 t50 01 52 t50 01 01
et_ et1 ne et1 et2 ex ne et1 et1 ne xt1 et1
en sen res sen sen resn res resn resn eres sne resn
l
i
b
n
n
n
re ide
de de
mo de
wid
w

Figure 3: Type I (FPR) and II (FNR) error rates for all tested
architectures across all experiments. Each bar represents a single
experiment (5 for each architecture).

Networks had consistently lower softmax output probabilities for COVID-19 in the event of Type I error (FP) while
TP probability distributions consistently extended well into
those from FP outputs (Figure 4). No significant difference
across architectures was found between softmax probabilities in the event of a more serious Type II error (FN; failing
to correctly detect COVID-19). For the binary COVID-19
detection task, mean softmax outputs were 0.601+/-0.161
and 0.213+/-0.135 for FP and FN, respectively. Interestingly,
16.5% of all TP predictions had softmax probability outputs
below 0.667.
Prediction behavior was often inconsistent between models and those with identical architectures to a significant
degree based on McNemars test (Figure 5). P-values of the
McNemars chi-squared test statistic were used to estimate
the prediction behavior consistency (in the test set only)
between all models for the binary classification task of
detecting COVID-19 versus non-COVID-19. With the null
hypothesis, a low p-value suggests that the two models in
question have inconsistent prediction behaviors. A large portion of model comparisons show low p-values and therefore
high prediction inconsistency (Figure 5B). Similarly, low p-

values were common when comparing models having identical architectures (Figure 5C). If prediction behavior were
largely consistent between models, distributions shown in
Figures 5B and C would have large spikes near 1.0, instead
the distribution of p-values is more uniform than expected.
Given that our experimental design controlled for the TVTS
and model architecture across all experiments, it is expected
that a high quality dataset would produce a distribution of
p-values indicating similar behavior between models with
identical architectures (Figure 5C). McNamers test suggests
that several architectures have reliably inconsistent behavior. The most apparent differences are between Mobilenetv2 v. Wideresnet-101, Resnet-152 v. Wideresnet-101, and
Resnet-152 v. Wideresent-50 (Figure 5A). Mobilenet-v2 was
found to have the least inter- and intra-model differences,
which could be explained by its relatively small number
of tunable parameters making a more ”general” model
fit. Predictions from Densenet-121 models had the most
consistency, on average, with all other trained models including those with identical architecture. Models sharing
the Wideresnet-101 architecture had the most intra-model
differences followed by ResneXt-101. Comparisons between
Mobilenet-v2, densenet-121, and Resnet-18 indicate that
these architectures had the most similar prediction behavior
(Figure 5B), and their false negative rate (FNR) values fall
in the middle of the pack. Conversely, models with deeper
architectures were responsible for the highest FNR values
(Table III).
Table III: Average performance metrics by model architecture.
Note that all metrics except ACC, which is multiclass accuracy,
are for COVID-19 detection only (i.e., binary classification). TPR:
true positive rate (or recall); FPR: false positive rate; FNR: false
negative rate; PPV: positive predictive value (or precision); F1: F1score; ACC: overall accuracy (TP+TN)/n.
Architecture
mobilenet v2
densenet121
resnet18
densenet169
densenet201
resnext50
resnet50
resnet101
resnet152
wideresnet50
resnext101
wideresnet101
All Combined

TPR
0.75
0.71
0.77
0.65
0.79
0.80
0.71
0.73
0.63
0.78
0.80
0.78
0.80

FPR
0.022
0.009
0.026
0.019
0.010
0.012
0.017
0.021
0.014
0.015
0.019
0.021
0.009

FNR
0.25
0.29
0.23
0.35
0.21
0.20
0.29
0.27
0.37
0.22
0.20
0.22
0.20

PPV
0.334
0.539
0.296
0.323
0.519
0.481
0.384
0.335
0.391
0.418
0.369
0.336
0.533

F1
0.455
0.606
0.422
0.430
0.625
0.596
0.489
0.456
0.481
0.544
0.505
0.468
0.640

ACC
0.869
0.878
0.877
0.871
0.884
0.868
0.871
0.879
0.878
0.865
0.881
0.874
0.894

Using the ensemble of models trained during this study,
the FPR and FNR become 0.009 and 0.20, respectively. The
ensemble predictions were carried out by simply summing
the output from the last layer (softmax) of each network for
a given image in the test hold-out set. The model ensemble
offers no improvement over best performing models in FNR

Output Probability COVID−19

1.00

●

●
●
●

●

●
●

●
●
●
●
●
●

●
●

●
●
●
●
●

0.75
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●

●
●
●
●

0.50

●
●
●
●
●
●

●

●
●

●

●
●
●
●
●
●

●

●
●
●
●
●
●

●
●
●
●

●

●
●

●
●
●
●

●
●
●

●
●

●

FN
FP
TP

●
●

●
●
●

●
●

0.25

0.00
21

2
t_v

ne

e
bil

mo

t1
ne

de

e
ns

69
01
01
52
01
01
t18
t50
t50
t50
ex
xt1
ne
ne
et1
et2
et1
et1
ne
et1
sn
ne
es
sn
sn
es
sn
en
en
res
r
r
e
s
s
s
e
e
e
r
r
r
e
r
n
n
re
e
de
de
wid
wid

Figure 4: Box-plot of output probability distributions from the softmax layer for true positive (TP), false positive (FP), and false negative
(FN) cases.

or FPR but improves the F1 score and multiclass accuracy
to 0.640 (from 0.625) and 0.894 (from 0.884), respectively.
IV. D ISCUSSION
The AI community is responding to the COVID-19 crisis
and releasing publications at a rapid pace. Studies show
promise in using AI in a clinical setting as a screening tool
for diseases like COVID-19. While these results suggest
that machine-learning techniques should only be used for
COVID-19 screening (not diagnostic purposes), our intent
with this study was to accelerate existing work for clinical
augmentation purposes and provide insight into model selection for COVID-19 use cases. The use of AI in diagnostic
procedures should be limited for use as a decision support
tool during screening, and diagnoses should not rely on AI
results alone.
Given the length of the incubation period and the variability in the symptom onset latency from infection, it is
difficult to control for the time at which the image was
acquired relative to the time of infection. However, efforts
have been made to include an offset in COVID-19 imaging
datasets by accounting for the number of days since the start
of symptoms or hospitalization [4]. Those who curate these
datasets also must deal with inherent ambiguity in medical
records such as image acquisition ”after a few days” of
symptoms (for example, Cohen et al. [4] assume 5 days).
With the currently available datasets, AI engineers rely
completely on clinical diagnoses and therefore assume that
no false positive images exist; i.e., it is assumed that patients
that tested positive for COVID-19 are indeed COVID-19
positive. Given an estimated false negative rate for COVID19 tests are high ( 10%) [27], perhaps those testing positive
can be safely assumed that they are indeed infected with
the virus (currently, false positive RT-qPCR tests are not
reported).

Deep learning architectures like those in this study have
been translated for use in upper body CT images [28],
and current studies demonstrate more reliable predictions
than those from chest x-rays [29], [30]. Reasons for this
performance relate to the image detail that can be obtained
from a chest CT as well as the size of the dataset used by
Li et al. [29], which contained images from 1296 COVID19 patients allowing for a larger hold-out test size than
what is currently available in x-ray (127 images at the
time of this study) [29]. While CT scans provide enhanced
screens, they are less accessible, more expensive, and less
efficient than the chest x-ray due to the preparation time
and scan time required. Consequently, CT falls short as
a COVID-19 screening tool due to limitations including
the complex mechanics and calibrations required for 3D
geometrical renderings. Furthermore, only 25 CT scanners
exist in the United States per million population (approx.)
[31], and COVID-19 screens place an increased demand on
top of existing demand for CT scans. Not only that, but an
increased infection risk is an unfortunate corollary for nonCOVID-19 patients requiring CT scans [3], especially due
to the amount of CT essential equipment that interfaces with
the patient compared to that for chest x-ray: not to mention
the increased exposure for clinicians and technicians. If
COVID-19 screens are needed en masse, the chest x-ray is
a promising, low-cost service that requires no moving parts
and could be modified to meet a spike in demand.
While this study suffers from several limitations, accuracy metrics on a few tested architectures achieved results
either consistent with or better than the current state-ofthe-art in both x-ray and CT studies [5], [29], [32]–[35].
The results of this study, though limited to specific neural
network architectures, suggest that transfer learning provides
an efficient means to achieve high accuracy in detecting
COVID-19. However, models remain highly dependent upon

Count

200

100

1.00

0.50

0

C

30

20

Count

w
w id
w id e
w ide ereres
id r s ne
e e n
r re sn e t1
r es sn e t1 01
r es ne e t1 01 _4
re esn nex xt1 t10 01_ _3
w r s
1
w id es n ex t1 01 _ 2
w id e n ex t1 01 _ 1
w id e re ex t1 01 _ 5
w ide ereres sne t10 01_ _3 4
id r s ne t5 1 2
e e n
_
r re sn e t5 0_ 1
r es sn e t5 0_ 5
re esn ne et5 t50 0_3 4
re sn e t15 0 _2
t
r s e 1
_
r es ne t1 5 2_ 1
r es ne t1 5 2_ 5
r es ne t1 5 2_ 4
re esn ne t10 52 2_2 3
t
re sn e 10 1 _1
t
r sn et 10 1 _5
r es et 10 1 _4
r es ne 10 1 _3
re esn ne t50 1_ _2
r re sn e t5 _ 1
r es sn e t5 0_ 5
re esn ne et t50 0_3 4
de r res sn ex xt5 50_ _2
e n e t 0 1
d
d e ns sn e xt 50 _5
d e ns en e xt 50 _4
d e ns en e xt 50 _3
de en nse ene et2 t20 50_ _2
s n
1 1
d
d e ns en e t2 01 _5
d e ns en e t2 01 _4
de en nse ene et1 t20 01_ _3
s
1
n
2
ns e e t1 69 _
n t 6
1
r en et 16 9 _5
r es et 16 9 _4
re esn ne 169 9_ _3
t
de
r s e 1
2
d
r es n t 8 _1
d e ns es n et 18 _5
d e ns en n et 18 _4
d e ns e e et 18 _
m en nse en net t12 18_ _2 3
e 1 1
m o s
m o bi e ne t1 21 _ 1
m o bi le ne t1 21 _ 5
m ob bile len net t12 21_ _3 4
ob ile n et _v 1_ 2
ile ne et_ _v 2_ 1
ne t_ v2 2_ 5
t_ v2 _3 4
v2 _
_1 2

B

300

_2 3
v2 _
t_ v2 _4 5
ne t_ v2 2_ 1
ile ne et_ _v 1_ 2
ob ile n et 2 1_ 3
m ob bile len et1 t12 21_ _4
m o bi en e t1 21 _5
m o s n
1
m n se ne t1 2
de en nse ene et1 _1
d e ns en 8 _2
d e ns t1 8 _3
1
d e ne t1 8 _4
5 9_ 2
d s e t1
re esn ne t18 8_ t16 69_ _3
r es ne t1 e t1 69 _4
r es ne en e t1 69 _5
r es ns en e t1 69 _1
r
1 2
s n
de en nse ene et1 t20 01_ _3
d e ns en e t2 01 _4
d e ns en e t2 01 _5
d e ns en e t2 01
d
s n
de en nse ene et20_1 2
d e ns en t5 0_ 3
d e ns x t5 0_ 4
d e ne x t5 0_ 5
d s e
_
re esn nex xt5 t50 _1
r es ne x 0 _2
r es ne t5 0 _3
r es ne t5 0 _4
5
r s e t5
re esn ne t50 0_ 1_1 2
r es ne t5 0 1_ 3
r es ne t1 0 1_ 4
r es ne t1 0 1_ 5
_
r s e t1
re esn ne t10 01 2_1 2
r es ne t1 5 2_ 3
r es ne t1 5 2_ 4
r es ne t1 5 2_ 5 _1 2
_ 0
r s e t1
re esn ne t15 52 et5 50_ _3 4
r es ne t1 sn et 50 _ 5
r es ne re sn et 50 _
r es e re sn et 50
r id
n t 1
w ide ereres sne 01_ _2
w id e re t1 01 _3
w id e x t1 01 _4
1
w id ne x t1 01 _5 1_ _2
w s e
3
1 0
re esn nex xt1 t10 et1 101 1_ _4
r es ne x sn et 10 1 _5
r es ne re sn et 10 1
r es e re sn et 10
r id
n t
w ide ereres sne
w id e re
w id e
w id
w

A

10

0.00
0

0.00

0.25

0.50

0.75

1.00

McNemar's Test p−value

0.00

0.25

0.50

0.75

McNemar's Test p−value

1.00

Figure 5: A representation of p-values from all two-sample McNemars chi-squared tests is shown in 3 plots. The classification task was
COVID-19 v. non-COVID-19 (binary). Tests were carried out between each model (5 for each architecture) and results are represented
via histograms (B, C) and similarity matrix (A). Histogram (B) represents the count of all p-values over their respective values, and
histogram (C) represents the count of p-values from tests where the two models in question have identical architectures. Colors in each
tile within the lower triangle of the similarity matrix in (A) represent the p-value from the paired test. P-values near zero indicate that
model behavior is different while those near 1 indicate similar behavior.

model architecture and vary depending on initial conditions
and data augmentation steps. To better quantify the reliability of AI predictions in this context, our next goal is
to implement segmentation techniques [36] and carry out
a cross-validation protocol for each architecture and use a
richer dataset.
Model accuracy metrics indicate that more advancements
are necessary before using AI for COVID-19 screening
via x-ray. In our opinion, clinicians should not rely on
solutions derived from architectures that have high (and
high variability) FNR. Furthermore, inconsistent behavior
in predictions between models with identical architectures
injects doubt into its efficacy, which is unlikely to resolve
until dataset limitations are worked out. Without a more
abundant dataset, we do not expect deep learning approaches
for COVID-19 screening to gain the reliability needed for
clinical implementation.
Finally, our aim is to encourage a focus on advancing the quality of x-ray screens for COVID-19 due to
its efficiency over other means and to accelerate workflows that seek to leverage AI. We have made pre-trained
model weights and the code for training freely available
to the community through our github repository under a

Creative Commons Attribution-NonCommercial 4.0 License
(covidresearch.ai/datasets; github.com/synthetaic). We expect these model weights to provide significant improvements in model training efficiency as these public datasets
continue to grow and evolve. We believe that AI results have
the potential to achieve a degree of reliability that alleviates
skepticism within the medical community regarding the use
of chest x-ray and computer vision to screen COVID-19.
V. F UTURE D IRECTIONS
Models trained for this study had the task of detecting
COVID-19 versus community acquired pneumonia and nonCOVID-19 (healthy cases), which limits feature space to
which the models are exposed. Future work should include
many more image classifications to enable the network to
learn features specific to a given pathology, which could
provide the means to elucidate differentiable features of
COVID-19 in chest x-ray. We aim to solve the data limitation
problem in future work through numerical methods and data
collection.
R EFERENCES
[1] “The covid tracking project (covidtracking.com).”
Available: https://covidtracking.com/

[Online].

[2] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, Q. Tao, Z. Sun,
[20] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam, “Mobilenets: Efficient
and L. Xia, “Correlation of chest ct and rt-pcr testing in coronavirus
disease 2019 (covid-19) in china: a report of 1014 cases,” Radiology,
convolutional neural networks for mobile vision applications,” arXiv
p. 200642, 2020.
preprint arXiv:1704.04861, 2017.
[21] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,
[3] F. Shi, J. Wang, J. Shi, Z. Wu, Q. Wang, Z. Tang, K. He, Y. Shi, and
D. Shen, “Review of artificial intelligence techniques in imaging data
“Densely connected convolutional networks,” in Proceedings of the
acquisition, segmentation and diagnosis for covid-19,” arXiv preprint
IEEE conference on computer vision and pattern recognition, 2017,
pp. 4700–4708.
arXiv:2004.02731, 2020.
[4] J. P. Cohen, P. Morrison, and L. Dao, “COVID-19 Image Data
[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“Imagenet: A large-scale hierarchical image database,” in 2009 IEEE
Collection,” arXiv:2003.11597 [cs, eess, q-bio], Mar. 2020, arXiv:
2003.11597. [Online]. Available: http://arxiv.org/abs/2003.11597
conference on computer vision and pattern recognition. Ieee, 2009,
[5] L. Wang and A. Wong, “COVID-net: A tailored deep convolutional
pp. 248–255.
[23] T. G. Dietterich, “Approximate statistical tests for comparing
neural network design for detection of COVID-19 cases from chest
radiography images.” [Online]. Available: http://arxiv.org/abs/2003.
supervised classification learning algorithms,” vol. 10, no. 7, pp.
09871
1895–1923. [Online]. Available: http://www.mitpressjournals.org/
doi/10.1162/089976698300017197
[6] Q. Yan, B. Wang, D. Gong, C. Luo, W. Zhao, J. Shen, Q. Shi, S. Jin,
L. Zhang, and Z. You, “Covid-19 chest ct image segmentation – a
[24] B. Everitt. London: Chapman and Hall, 1977.
deep convolutional neural network solution,” 2020.
[25] R Core Team, R: A Language and Environment for Statistical
Computing, R Foundation for Statistical Computing, Vienna, Austria,
[7] C. Huang, Y. Wang, X. Li, L. Ren, J. Zhao, Y. Hu, L. Zhang,
G. Fan, J. Xu, X. Gu, Z. Cheng, T. Yu, J. Xia, Y. Wei, W. Wu,
2020. [Online]. Available: https://www.R-project.org/
[26] H. Wickham, ggplot2: Elegant Graphics for Data Analysis.
X. Xie, W. Yin, H. Li, M. Liu, Y. Xiao, H. Gao, L. Guo, J. Xie,
G. Wang, R. Jiang, Z. Gao, Q. Jin, J. Wang, and B. Cao, “Clinical
Springer-Verlag New York, 2016. [Online]. Available: https:
features of patients infected with 2019 novel coronavirus in wuhan,
//ggplot2.tidyverse.org
[27] I. Yelin, N. Aharony, E. Shaer-Tamar, A. Argoetti, E. Messer,
china,” vol. 395, no. 10223, pp. 497–506. [Online]. Available:
https://linkinghub.elsevier.com/retrieve/pii/S0140673620301835
D. Berenbaum, E. Shafran, A. Kuzli, N. Gandali, T. Hashimshony,
[8] M.-Y. Ng, E. Y. Lee, J. Yang, F. Yang, X. Li, H. Wang, M. M.Y. Mandel-Gutfreund, M. Halberthal, Y. Geffen, M. SzwarcwortCohen, and R. Kishony, “Evaluation of COVID-19 RT-qPCR test
s. Lui, C. S.-Y. Lo, B. Leung, P.-L. Khong et al., “Imaging profile
of the covid-19 infection: radiologic findings and literature review,”
in multi-sample pools,” Infectious Diseases (except HIV/AIDS),
preprint, Mar. 2020. [Online]. Available: http://medrxiv.org/lookup/
Radiology: Cardiothoracic Imaging, vol. 2, no. 1, p. e200034, 2020.
[9] J. Born, G. Brndle, M. Cossio, M. Disdier, J. Goulet, J. Roulin, and
doi/10.1101/2020.03.26.20039438
N. Wiedemann, “Pocovid-net: Automatic detection of covid-19 from
[28] O. Gozes, M. Frid-Adar, N. Sagie, H. Zhang, W. Ji, and
H. Greenspan, “Coronavirus detection and analysis on chest ct with
a new lung ultrasound imaging dataset (pocus),” 2020.
[10] I. D. Apostolopoulos, S. Aznaouridis, and M. Tzani, “Extracting posdeep learning,” 2020.
sibly representative covid-19 biomarkers from x-ray images with deep
[29] L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, J. Bai, Y. Lu, Z. Fang,
Q. Song, K. Cao, D. Liu, G. Wang, Q. Xu, X. Fang, S. Zhang, J. Xia,
learning approach and image data related to pulmonary diseases,”
2020.
and J. Xia, “Artificial intelligence distinguishes COVID-19 from
community acquired pneumonia on chest CT,” p. 200905. [Online].
[11] W.
D.
Heaven,
“A
neural
network
can
help
spot
covid-19
in
chest
x-rays,”
2020.
[Online].
Available: http://pubs.rsna.org/doi/10.1148/radiol.2020200905
Available: https://www.technologyreview.com/2020/03/24/950356/
[30] S. H. Kassani, P. H. Kassasni, M. J. Wesolowski, K. A. Schneider,
coronavirus-neural-network-can-help-spot-covid-19-in-chest-x-ray-pneumonia/ and R. Deters, “Automatic detection of coronavirus disease (covid[12] B. Dickson, “How ai is helping in the fight against covid19) in x-ray and ct images: A machine learning-based approach,”
19,” 2020. [Online]. Available: https://www.pcmag.com/news/
2020.
[31] M. Castillo, “The industry of ct scanning,” American Journal
how-ai-is-helping-in-the-fight-against-covid-19
[13] L. Dormehl, “A.i. could help spot telltale signs of coronavirus in
of Neuroradiology, vol. 33, no. 4, pp. 583–585, 2012. [Online].
lung x-rays,” 2020. [Online]. Available: https://www.digitaltrends.
Available: http://www.ajnr.org/content/33/4/583
[32] X. Xu, X. Jiang, C. Ma, P. Du, X. Li, S. Lv, L. Yu, Y. Chen, J. Su,
com/cool-tech/using-ai-to-spot-coronavirus-lung-damage/
[14] J. P. Kanne, B. P. Little, J. H. Chung, B. M. Elicker, and L. H. Ketai,
G. Lang et al., “Deep learning system to screen coronavirus disease
2019 pneumonia,” arXiv preprint arXiv:2002.09334, 2020.
“Essentials for radiologists on covid-19: an updateradiology scientific
expert panel,” 2020.
[33] F. Shi, L. Xia, F. Shan, D. Wu, Y. Wei, H. Yuan, H. Jiang, Y. Gao,
[15] S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Benchmark
H. Sui, and D. Shen, “Large-scale screening of covid-19 from community acquired pneumonia using infection size-aware classification,”
analysis of representative deep neural network architectures,” vol. 6,
pp. 64 270–64 277. [Online]. Available: http://arxiv.org/abs/1810.
arXiv preprint arXiv:2003.09860, 2020.
00736
[34] J. Chen, L. Wu, J. Zhang, L. Zhang, D. Gong, Y. Zhao, S. Hu,
Y. Wang, X. Hu, B. Zheng et al., “Deep learning-based model
[16] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,
for detecting 2019 novel coronavirus pneumonia on high-resolution
computed tomography: a prospective study,” medRxiv, 2020.
A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative
[35] C. Jin, W. Chen, Y. Cao, Z. Xu, X. Zhang, L. Deng, C. Zheng,
style, high-performance deep learning library,” in Advances in Neural
J. Zhou, H. Shi, and J. Feng, “Development and evaluation of an ai
system for covid-19 diagnosis,” medRxiv, 2020.
Information Processing Systems 32, H. Wallach, H. Larochelle,
A. Beygelzimer, F. dAlche Buc, E. Fox, and R. Garnett, Eds. Curran
[36] Y. Qiu, Y. Liu, and J. Xu, “Miniseg: An extremely minimum network
Associates, Inc., 2019, pp. 8024–8035.
for efficient covid-19 segmentation,” 2020.
[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 770–778.
[18] S. Zagoruyko and N. Komodakis, “Wide residual networks,”
CoRR, vol. abs/1605.07146, 2016. [Online]. Available: http:
//arxiv.org/abs/1605.07146
[19] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2017,
pp. 1492–1500.

