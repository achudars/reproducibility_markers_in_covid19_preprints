Group testing and PCR : a tale of charge value
Emilien Joly∗

Bastien Mallein†

December 17, 2020

arXiv:2012.09096v1 [stat.AP] 16 Dec 2020

Abstract
The original problem of group testing consists in the identification of defective items in a collection,
by applying tests on groups of items that detect the presence of at least one defective item in the group.
The aim is then to identify all defective items of the collection with as few tests as possible. This problem
is relevant in several fields, among which biology and computer sciences. It recently gained attraction as
a potential tool to solve a shortage of COVID-19 test kits, in particular for RT-qPCR.
However, the problem of group testing is not an exact match to this implementation. Indeed, contrarily
to the original problem, PCR testing employed for the detection of COVID-19 returns more than a simple
binary contaminated/non-contaminated value when applied to a group of samples collected on different
individuals. It gives a real value representing the viral load in the sample instead. We study here the
use that can be made of this extra piece of information to construct a one-stage pool testing algorithms
on an idealize version of this model. We show that under the right conditions, the total number of tests
needed to detect contaminated samples diminishes drastically.

1

Introduction

The group testing problem consists in identifying a subset of defective items among a larger set by using tests
on pools of items answering the question “Does this pool contains at least one defective item?”. This problem
has a long history, and appeared several times in different fields of medical biology [Dor43, Tho62, TM06,
FHG+ 12] and computer sciences [MTT08, IKWO18, AJS19]. It has also been the subject of an important
mathematical literature, which studied optimal algorithms for the detection of defective with minimal use of
tests, which are considered a limiting resource. Those algorithms can be divided into two main categories:
adaptive testing: in which the choice of a pool is influenced by the previous results of tests applied to the
group;
non-adaptive testing: in which the choice of a pool does not depend on the results of previous tests.
The aim of a pool testing algorithm is to assess, as precisely as possible, the status (defective or not) of each
item, through the tests made on pools of individuals, while using as few tests as possible.
It should be clear that the more permissive adaptive testing option allows for more flexibility, a more
parsimonious use of tests can thus be archived in this setting. At one extreme, a search tree can be used to
detect defective items with a maximal economy of tests [CJBJ17]. In contrast, non-adaptive testing allows
the possibility to massively parallelize the procedure. As all pools can be constructed before any result is
known, all tests can be performed simultaneously, which can decrease the time needed to obtain the result.
Moreover, in the context of biological testing, non-adaptive schemes decrease the risk of contamination or of
decay of samples during their treatment.
It might be noted that several types of adaptive testing allow some level of parallelizing. For example,
two- or three-stages algorithms can be considered. In this situation, a first set of pools is constructed without
prior information. Using the result of testing on these pools, a second set of pools is constructed. With the
tests made on this second set of pools, the status of each item is assessed in a two-stage algorithm, or a third
set of pools is constructed and tested in a third stage algorithm, before assessing the status of the items.
One of the first pool testing algorithm to be describe was introduced by Dorfman [Dor43], as a method
to detect syphilis in recruited US soldiers. This algorithm is the following: samples taken from individuals
are pooled together in a group, which is then tested for syphilis. If the pool turns negative, all individuals
are declared non-contaminated, while if the pool turns positive, then each individual of the pool is tested.
Note that this is a two-stage algorithm, which we refer to as Dorfman’s algorithm.
∗ CIMAT,

Guanajuato, Mexico, emilien.joly@cimat.mx.
Sorbonne Paris Nord, LAGA, UMR 7539, F-93430, Villetaneuse, France. Member of the MODCOV19 plateform
and the GROUPOOL initiative, mallein@math.univ-paris13.fr.
† Université

1

Several adaptive and non-adaptive pool testing algorithms have been described over the years, such as
matrix testing [CCK+ 99], smart testing [TM06], and testing based on risk estimation for items [ABB19,
BBC+ 20]. We refer to [AJS19] for a recent survey on this topic. To compare these algorithms, it is necessary
to specify more precisely the context in which they are used, such as the relative number of defective and
non-defective items, the authorized false positive and false negative rates, etc.
Prevalence and efficiency of pooling procedures. As stated above, the objective of pool testing is the
reduction of the number of tests used on a population of N items in order to identify the defective ones. If
an algorithm uses a total of T tests, we measure its resource-based efficiency by the quantity
E=

T
.
N

This ratio measures the average number of tests used per item in this pool testing algorithm to detect the
defective ones. Therefore, the lower this ratio is, the more parsimonious the algorithm.
Observe that any reasonable algorithm of pool testing should verify E < 1, as otherwise the testing of
any individual separately represents a more efficient use of resources. In the present article, we assume that
a known proportion p of items is defective. It is worth noting that in that situation, a lower bound on the
efficiency of a reasonable non-adaptive pool testing algorithm is E(p) ≥ p. Indeed, there are approximately
pN defective items among N , so if one makes less than pN tests, there is no possibility to detect the defective
items if all pools contains at least one defective. One is interested in the optimal dependency of E in the
parameter p.
The optimal efficiency of the Dorfman algorithm previously described is obtained by choosing the size of
the pool depending on the value of p in such a way that it is minimal. It can be computed as follows
E D (p) = min
n∈N

1 + n(1 − (1 − p)n )
∼ 2p1/2
n

as p → 0.

(1.1)

Indeed, if one creates pools of n individuals, one test is required for the pool to detect if defective items are
present or not, and if the pool is positive (which happens with probability 1 − (1 − p)n ), an additional one
is needed per item. The equivalent is obtained by choosing n ≈ p−1/2 as p → 0.
Mézard et al. [MTT08] constructed asymptotically optimal non-adaptive and two-steps pool testing
algorithms, which detects asymptotically all defective items, while keeping an efficiency of
E ∗ (p) ∼ C∗ p| log p| as p → 0,

(1.2)

for some C∗ > 0. This algorithm is based on the construction of random pools of size n ≈ c p1 of items, such
that each item belongs to L ≈ C| log p| pools. An item is declared non-defective if it belongs to at least one
pool tested negative, is declared defective if it belongs to at least one positive pool with all the other items
being declared non-defective, and is declared ambiguous otherwise. In that situation, depending of the value
of c, C, either with high probability each defective item will belong to at least one pool of non-defective items,
and thus be identified as non-defective, or with high probability, the number of ambiguous items after the
first stage is small enough that they can be tested individually.
Pooling in the context of the COVID-19 epidemics. In the context of COVID-19, pool testing has
been massively proposed and implemented as a method to diminish the marginal cost of a test as well as
to answer local shortages of test kits, see for example [BBC+ 20, GRK+ 20, SNGYL20, BAKS+ 20, HSP20,
SAKH20, LSF20, GG20, MNB+ 20, TAN20, LPBG+ 20] among many others. The necessity of early detection
of contaminated individuals has been underlined many times, in particular due to the large number of
presymptomatic, asymptomatic and mildly symptomatic individuals that remain contagious and can carry
the diseases to vulnerable people. As a result, the demand for effective and quick testing has skyrocketed,
with the offer being limited by the number of test kits and trained medical professionals for the sampling.
The question of optimization of pool testing thus has practical consequences, as improving on the efficiency
of a testing algorithm can increase the number of individuals that can be tested with the same number of
kits.
A typical test used for the detection of contamination to SARS-COV-2 is the RT-qPCR test (or PCR test
for short), the reverse-transcriptase quantitative polymerase chain reaction. This test allows the measurement
of the number of RNA segments typical of the virus that are present in a given sample. As the name suggest,
the measure is quantitative, thus returns more than binary response (which would be akin to a defective/not
defective result in the classical pooling literature). As such, it seems that this additional piece of information
could be used to improve on the existing group testing strategies to reduce the the number of tests needed
for detection.
2

However, let us underline a couple of important caveats. First, the quantity measured by the PCR test
is related to the logarithm of the viral load carried in the sample, rather than the viral load itself, with
some noise on the measure [BMR20]. Therefore the exact viral load is not known, but rather its order of
magnitude. Secondly, the viral load in defective individuals spans a large spectrum of orders of magnitude
[JMV+ 20]. Therefore, if two defective individuals with viral loads c1 and c2 are tested in the same pool, the
result of the measure will be
log(c1 + c2 ) ≈ max(log c1 , log c2 ),
(1.3)
as c1 and c2 will typically be of different orders of magnitude.
The aim of this article is to propose and study an algorithm that uses the viral load of an individual
to improve its efficiency. We construct this algorithm on an idealized version the the situation described
above. We discuss in Section 8 the adaptation of the algorithm to the COVID scenario, pointing some of its
limitations.
Defective items with load. We consider in this article some theoretical aspects of pooling strategies that
can be employed for the detection of defective items with load, in order to adapt to the PCR testing scenario
previously described. We assume here that each defective item u is associated to a positive value xu that we
call its load. A non-defective individual will have a load of 0. The test of a pool A of individuals has the
effect of measuring the value maxu∈A xu , i.e. the largest load among all individuals in the set A.
Observe that if the load of individuals belongs to {0, 1}, then we are in the settings of the classical pool
testing, and a test only detects the presence of at least a contaminated individual. However, if this load can
takes more values, we show that the results of several tests can be crossed to extract additional information
on the individuals. The load xu can be thought of as the logarithm of the viral load of an individual in PCR
settings, and the choice of measuring the maximal load of a set comes from (1.3).
We denote by p the prevalence of defective items (i.e. the proportion of defective items in the set to be
tested). We assume here that p is known (or at least adequately estimated), so it can be used to choose
the size and number of pools to be made. The load associated to each individual can then be written as
xu = ξu Zu , where ξu is a Bernoulli random variable with parameter p representing the fact that individual
u is defective or not, and Zu is an independent [0, 1]-valued random variable. In this article we will consider
Zu uniformly distributed either on [0, 1] or on {1/K, 2/K, . . . , 1} for some K ∈ N.
The quantity K described above can be interpreted as the level of precision of the measure. The larger
K is, the easier it is to distinguish the level of two defective individuals with similar loads. As a result, the
efficiency attained by our algorithm will decay as K increases, to attain optimal efficiency when K = ∞,
which corresponds to Zu uniformly distributed on [0, 1].
Organization of the paper. We study here a one-step (non-adaptive) algorithm for the detection of
defectives, sometimes under the assumption that the same sample cannot be placed in more than L pools.
We aim for asymptotically efficient algorithms as p → 0 that remains simple to implement and evaluate. We
describe in the next section the general form of the algorithm we study. We then show how to optimize this
algorithm assuming that each sample can only be part of a finite number of pools in Section 5, and optimal
efficiency that can be obtained by this algorithm in Section 6. We then provide some numerical simulations
to compare these asymptotic results to their finite value counterpart in Section 7.

2

The Grid Pool Testing algorithm

In this work, we focus on a simple one-step non-adaptive algorithm. In this algorithm items are organized on
a grid, and the pools are made on lines, columns and diagonals. The algorithm mainly focus on reconstructing
the status of items from the measures made on these diagonals. The parameters of the algorithms to optimize
are the size of the grid the number of pools each item belongs to.
Defining the grid. Before describing the algorithm in more details, we introduce some notation. We
assume the number of items to test to be sufficiently large that it is possible to divide them into batches of
n2 items. We describe the algorithm on a given batch.
The items are dispatched on a grid n×n, with each item being identified by its position (i, j) ∈ {1, . . . , n}2 .
We write ξi,j = 1 if (i, j) is defective and ξi,j = 0 otherwise. Moreover, if ξi,j = 1, we denote by Xi,j the
load of each item (which is 0 if the individual is non-defective, or a number in (0, 1] otherwise). With the
modelling of the previous section, we note that (ξi,j , 1 ≤ i, j ≤ n) are i.i.d. B(p) random variables, with
p the proportion of defective. Conditionally on ξ, (Xi,j , 1 ≤ i, j ≤ n) are independent random variables,
with Xi,j = 0 if ξi,j = 0 and Xi,j uniformly distributed on (0, 1] or on {1/K, 2/K . . . , 1}, depending on the
context.
3

Defining the pools. The pools used can loosely be described as the diagonals of the grid. More precisely,
we introduce the following sets of n items to construct the pools of the algorithm:
• the lines Li = {(i, k), 1 ≤ k ≤ n}, for 1 ≤ i ≤ n.
• the columns Cj = {(k, j), 1 ≤ k ≤ n}, for 1 ≤ j ≤ n.
• the diagonals with various slopes Dba = {(k, ak + b mod(n)), 1 ≤ k ≤ n} for 1 ≤ b ≤ n, where
a ∈ {1, . . . , n − 1}.
In an algorithm constructed such that each item is part of L pools, the pools will be taken as families of
lines, columns and diagonals with slopes smaller than L − 2. In the rest of the article we will assume this
family of pools will form a N (n2 , n, L) multipool, in the terminology of [Tä20]. In other words, we need our
pools to satisfy the following three properties:
1. each pool contains exactly n items;
2. each item belongs to exactly L pools;
3. two items (i, j) and (k, l) share at most one pool in common.
While the first two properties are straightforward from the definition, the third one is not, and only holds
under some assumptions on n and L.
Lemma 2.1. The family {Lk , Ck , Dka , 1 ≤ k ≤ n, a ≤ L − 2} is a N (n2 , n, L) multipool if and only if L − 2
is smaller than the smallest prime divisor of n.
Proof. We first note that two line never cross, and that a line crosses with a column or a diagonal at exactly
one point. Therefore, to verify that {Lk , Ck , Dka , 1 ≤ k ≤ n, a ≤ L − 2} is a multipool, it is enough to check
that no too diagonal cross at more than one place (treating columns as diagonals of line 0).
Observe that for a 6= b, two diagonals Dka and D`b cross at a point (i, j) such that k + ai ≡ ` + bi mod n,
i.e. such that (b − a)i ≡ ` − k mod n. By algebra’s fundamental theorem, there exists a unique i ∈ [1, n]
satisfying this property if and only if (b − a) is prime with n. As |b − a| ≤ L − 2 is smaller than the smallest
prime factor of n, we deduce this is indeed the case, proving that any two pools cross at either 0 (if they have
the same slope) or 1 point.
Remark 2.2. More generally we could prove that selecting families of lines, columns and diagonals in the
n × n grid, it is possible to create a N (n2 , n, L) multipool if and only if L − 2 is smaller than the smallest
prime divisor of n.
In the rest of the article, we enumerate the pools as the family {Pj , j ≤ nL}, with P1 , . . . Pn corresponding
to the lines, Pn+1 , . . . P2n to the columns and the rest to the diagonals, in the increasing order of their slope.
For each ` ≤ nL, the effect of probing the pool P` corresponds to the action of discovering the value
V` := max(i,j)∈P` Xi,j , the largest load among all defective individuals belonging to the pool. Finally, for
convenience, we denote Pi,j the set of pools associated to the individual (i, j),
Pi,j = {` : (i, j) ∈ P` }.
Computation of the positives. The final step of the algorithm consists in a reconstruction of the load
of each individual via the information contained in the family {V` , ` ≤ nL}. We observe immediately that if
V` = 0, then all individuals in the pool are non-defective, and if V` = x 6= 0, then there exists at least one
individual in the pool with load equal to x.
To reconstruct the load of each individual, we employ the following procedure.
1. For every item (i, j), let Vi,j = min`:(i,j)∈P` V` .
2. If Vi,j = 0, the item (i, j) is declared negative.
3. Otherwise, we count the number of apparitions of the value Vi,j inside of the pools containing (i, j) :
Ii,j = |{` : (i, j) ∈ P` and V` = Vi,j }|.
(a) If Ii,j ≥ 2, meaning that at least two tests containing item (i, j) measured it with the same value,
the item (i, j) is declared positive.
(b) Otherwise, the item (i, j) is declared negative.

4

Here is the reason behind this definition. By assumption of the test, Vi,j is an upper bound for the
load Xi,j of the individual. In particular, if Vi,j = 0, we label the individual as non-defective. However,
if Vi,j > 0 it might be that the individual has been, by chance, mixed with defective items in all the tests
that were made on it. The fact that level Vi,j is attained at least twice is a much stronger indication of the
defectiveness of (i, j), as a false positive in that case would mean that it has been by chance mixed in two
pools with different defective individuals sharing exactly the same load, and that in all other pools, there
was at least one individual with a larger load. In the asymptotic we will consider, this will not occur with
large probability, and similarly if Ii,j = 1, with high probability the item will be negative.
Remark 2.3. Observe the procedure we describe here to assess the load and status of each individual is
not the most accurate. With extra care, one could gain more precision of the reconstruction, for example
by checking that each measured load in the pools has been associated to at least one item. However, the
procedure described here has the advantage of simplicity and locality: to give the status of an item, one has
only to consider the results of the tests related to this item. This makes the forthcoming computation of the
probability that an item is wrongfully characterized significantly easier, and it remains efficient enough in
the range of parameters we consider.
We sum up the complete procedure inside Algorithm 1 and a concrete toy example in Figure 1.
Algorithm 1: Grid Pool Testing
Parameters: n,L,
Inputs: X = (X1 , . . . , Xn2 )
Store X inside the grid (Xi,j )1≤i,j≤n line by line;
Define P1 , . . . , Pn as the lines, Pn+1,...,P2n as the columns and P2n+1 , . . . , PnL as the diagonals;
Initialize a matrix S = (Si,j )i,j of empty lists;
for ` = 1, . . . , nL do
Compute V` = max(i,j)∈P` Xi,j ;
Append V` to every Si,j with (i, j) ∈ P` ;
end
Initialize a matrix R = (Ri,j )i,j of zeros;
for 1 ≤ i, j ≤ n do
Compute Vi,j = P
mins∈Si,j s;
Compute Ii,j = s∈Si,j 1{s=Vi,j } ;
if Vi,j 6= 0 and Ii,j ≥ 2 then
Set Ri,j = 1;
end
end
Store the matrix R line by line into a vector (R1 , . . . , Rn2 );
Result: (R1 , . . . , Rn2 )

Efficiency and optimization. It is worth noting that the algorithmic complexity of Algorithm 1 is O(n2 L).
In terms of test usage, it is easy to compute the efficiency of this algorithm as there are a total of nL pools
of n items that are tested, in an effort to detect defective elements among n2 items. The corresponding
efficiency is then
nL
L
E= 2 = .
n
n
To complete the study of this algorithm, one then need to compute its false positives (when Ri,j = 1 implying
detection as defective while Xi,j = 0 so the item is non-defective) and false negatives (when Ri,j = 0 whereas
Xi,j > 0) rates.
We denote by F P R (respectively F N R) the expected number of false positives and false negatives returned
by this algorithm, divided by the total number of contaminated individuals. These two quantities depend
on the four parameters p, K, n and L in an intricate fashion. However, note that while n and L are integer
parameter of the algorithms we can choose, p ∈ [0, 1] and K ∈ N are modelling parameters of the problem,
representing respectively the proportion of defective items and the accuracy of the test. Therefore the main
goal of this study is to optimize the efficiency E of this algorithm by choosing the optimal n(p, K) and
L(p, K) in a way that insures that F P R and F N R both stay below fixed quantities ε and δ. Our main
results are considered under the asymptotic p → 0 of a small proportion of defective items. But as most of
the computations made are explicit before taking limits, computing the optimal value of n and L for given
values p, K remains straightforward.
5

Remark 2.4. Note that an item is falsely labelled as negative if it is part of at most one pool in which it
is the item with the largest load. It corresponds to items at position (i, j) such that Ii,j = 1 in the above
algorithm. Therefore, items such that Ii,j could be labelled as inconclusive and tested again in a sepate batch
in a two-steps algorithm.

Figure 1: A grid with L = 3, n = 6, K = 4 and N = 36. There are 3 defective individuals of respective levels 0.25, 0.5 and 0.75.
For the sake of clarity, we only showed the test involving the bottom leftmost individual for the diagonal of slope 1/3 (hence
five more test are not represented here). The blue circle represent the healthy individuals whereas the black crosses represent
the defective individuals for whom the level of defectiveness is specified.

3
3.1

Computation of the false negative and false positive rates
Exact calculations

In this section, we give explicit upper bounds on the false negative and false positive rates of the algorithm.
This does not take into account a possible defective measurement of the loads of the pools. The false negative
rate of the algorithm is expressed as the probability that a contaminated individual is not detected at the
end of the algorithm whereas the false positive rate is the probability that a non-defective individual is
detected as defective. In Algorithm 1, it is straightforward to compute the false negative/positive rates, as
the reconstructed status of an individual only depend on the status of individuals sharing a pool with it. This
algorithm being unchanged by changing the coordinates of the grid, as on a torus, these rates do not depend
on the position (i, j) of the individual in the grid. We thus only compute the false negative probability of
item (1, 1), given its load. For all x ∈ (0, 1], we set
F N (x) = P((1, 1) is declared negative |X1,1 = x),
and

F P (x) = P((1, 1) is declared positive with load x|X1,1 = 0).

The false negative rate F N R and the false positive rate F P R are then given by
F N R(n, L; p, K) = pE (F N (dKU e/K))
F P R(n, L; p, K) = (1 − p)

K
X

F P (k/K)

k=1

with U a uniform random variable on [0, 1] and the convention that d∞U e/∞ = U and F P R(n, L; p, ∞) = 0.
We used here that a given item is defective with probability p and non-defective with probability 1 − p. The
following Proposition gives two upper bounds on the false positive/negative rates.
6

Proposition 3.1. Let x = k/K and let gn,p (x) = (1 − p(1 − x))n−1 . Then it holds that
F N (x) ≤ L (1 − gn,p (x))

L−1

(3.1)

and that

L(L − 1)  np 2
L−2
gn,p (x)2 (1 − gn,p (x))
.
2
K
In particular, when K → ∞, the false positive rate F P R(n, L; p, K) tends to 0.
F P (x) ≤

(3.2)

Proof. We observe that the probability to wrongfully declare an item as negative depends on K (in the
discrete case) only through the fact that x takes its values in {1/K, 2/K . . . , 1}. This allows us to give a
unified expression for the upper bound of F N .
Given x the load of the item (1, 1), we note this item will be wrongfully declared negative in Algorithm 1
if and only if I1,1 = 1 (as V1,1 ≥ x > 0 a.s.). Decomposing according to the test in P1,1 measuring the lowest
viral load, we have
X
P(V` < min
V`0 |X1,1 = x) = LE(ϕ(V` )|X1,1 = x),
F N (x) = P(I1,1 = 1|X1,1 = x) =
0
` 6=`

`∈P1,1

where ϕ(y) = P(min`0 6=` V`0 > y|X1,1 = x) and `0 is a fixed element of P1,1 . Using that the (V` , ` ∈ P1,1 )
remain i.i.d. conditionally on X1,1 = x, thanks to Lemma 2.1, we have ϕ(y) = P(V`0 > y|X1,1 = x)L−1 .
Then, using that ϕ(y) ≤ ϕ(x) for all y ≥ x, we obtain
F N (x) ≤ LP(V`0 > x|X1,1 = x)L−1 .
As in our setting there is a proportion x of positive individuals with load smaller than x, we obtain
P(V` ≤ x|X1,1 = x) = (1 − p + px)n−1 = (1 − p(1 − x))n−1 = gn,p (x),
which leads to the following upper bound for the false negative rate of an item with load x,
F N (x) ≤ L (1 − gn,p (x))

L−1

.

We can similarly compute the false positive rate of the algorithm by computing the probability that
conditionally on (1, 1) being non-contaminated, this item is determined to be contaminated. This would
happen if and only if (1, 1) is only part of contaminated pools, and that the two pools with the lowest
measured load have the same value, that we write x. Noticing that false positive results never occur in the
infinite precision setting K = ∞, we assume here that K < ∞. Using again that we have a multipool and
that the measure of each test is independent conditionally on the value of X1,1 we obtain
F P (x) = P(I1,1 ≥ 2, V1,1 = x|X1,1 = 0)
= P(∃`1 , `2 ∈ P1,1 , V`1 = V`2 = x, ∀` ∈ P1,1 \{`1 , `2 }, V` ≥ x)
X
Y
≤
P(V`1 = x|X1,1 = 0)P(V`2 = x|X1,1 = 0)
=

P(V`0 ≥ x|X1,1 = 0)

`0 ∈P1,1 \{`1 ,`2 }

{`1 ,`2 }:`1 6=`2

L(L − 1)
P(V`0 = x|X1,1 = 0)2 P(V`0 ≥ x|X1,1 = 0)L−2 ,
2

with `0 a fixed element of P1,1 . Let F (x) be the distribution function of V`0 conditionally on X1,1 = 0. Then,
we can use the upper bound
P(V`0 = x|X1,1 = 0) = F (x) − F (x − 1/K) ≤
We finally get
F P (x) ≤

sup[x− K1 ,x] F 0 (u)
K

F (x) ≤

np
(1 − p(1 − x))n−1
K

L(L − 1)  np 2
L−2
gn,p (x)2 (1 − gn,p (x))
.
2
K

In the rest of the article, we compute the optimal efficiency under different constraints, based on the above
constructed pools, in different situations. We first consider non-adaptive strategies for detection of defective
items based on the measure of lines and columns only, then adding eventually item tests for items whose
status cannot be deduced by the first step algorithm. We then aim at optimal testing efficiency, assuming
that samples can be infinitely divided, and recover results consistent with Mézard et al [MTT08]. In the third
section, we compare our asymptotic estimates with simulated experiments, and obtain the false positive/false
negative rates and efficiency that can be archived in real testing conditions.
7

4

Asymptotics of the false discovery rates at L fixed

In this section, we derive equivalent expressions for the upper bound of the false discovery rates when the
values of K and L remain fixed. We consider two asymptotic cases, when np → 0 and when np → λ > 0. It
is implicitly assumed that n → ∞ and p → 0. In the second case, the calculations are based on the Poisson
approximation of the number of defective items in a specific pool and are consequently more accurate than
the rates in the first case.
Case np → 0. In this case, gn,p (x) can be lower bounded by gn,p (0) since it is a increasing function and
gn,p (0) ∼ e−np . Then
F N R ≤ LpE(1 − gn,p (U ))L−1 ≤ Lp(1 − gn,p (0))L−1 ∼ Lp(np)L−1 .
The false discovery rate is then upper bounded by
FPR ≤ K

L(L − 1)
L(L − 1)  np 2
L−2
(1 − gn,p (0))
∼
(np)L
2
K
2K

In particular, we see that in this regime (as K and L remain fixed), the false negative rate is small with
respect to the false positive rate.
Case np → λ > 0. In this situation, with L being fixed, we immediately obtain that the number of defective
items in each pool converges to a Poisson(λ) random variable. We consider the asymptotic behaviour of the
false positive and false negative rates obtained in this situation. We write
F N R(λ, L; K) = lim p−1 F N R(n, L; λ/n, K)
n→∞

and F P R(λ, L; K) = lim F P R(n, L; λ/n, K),
n→∞

as a function of L and the precision K.
Proposition 4.1. Under the condition np → λ > 0, we have that
F N R(λ, L; ∞) = (1 + (L − 1)e−λ )(1 − e−λ )L−1 ,
and, for all K < ∞,
F N R(λ, L; K) ≤ L(1 − e−λ )L−1

and

F P R(λ, L; K) ≤

1.0

1.4

0.8

1.2

L(L − 1)
(1 − e−λ )L−2
2K

1.0
0.6

0.8
0.6

0.4

0.4

0.2

0.2
0.0

0.0
2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

Figure 2: False negative and false positive rates as a function of L for λ = log 2 and K = 1 (in blue), K = 2 (orange), K = 5
(green), K = 10 (red) and K = ∞ (purple).

Proof. We first compute the false negative rate. From the properties of Poisson processes, we note that
the number of items in a pool with load between x and y is distributed as a Poisson random variable with
parameter λ(y − x), independently of the number of items in this pool with load smaller than x or larger
than y. In particular, for any given test `, for any y ≥ x we have
P(V` > y|X1,1 = x) = (1 − e−λ(1−y) ).

8

Using this fact, with the same computations as in Proposition 3.1, we can compute the false negative rate
of an item of load x as
X
P(V`0 = y|X1,1 = x)P(V`0 > y|X1,1 = x)L−1
F N (λ, L; K) = L
y≥x

= Le

−λ(1−x)

(1 − e

)

−λ(1−x) L−1

K
X

+

e−λ(1−y) (1 − e−λ/K )(1 − e−λ(1−y) )L−1

y=x+1

≤ L(1 − e−λ(1−x) )L−1 .
In the case K = ∞ the computations can be made explicit as in that case
Z 1
F N (λ, L; ∞) = Le−λ(1−x) (1 − e−λ(1−x) )L−1 + L
λe−λ(1−y) (1 − e−λ(1−y) )L−1 dy
x

= Le−λ(1−x) (1 − e−λ(1−x) )L−1 + (1 − e−λ(1−x) )L = (1 + (L − 1)e−λ(1−x) )(1 − e−λ(1−x) )L−1 .
In particular, we also obtain F N (λ, L; ∞) ≤ L(1 − e−λ(1−x) )L−1 .
Similarly, we can compute the false positive rate of a negative item in this regime. An item is falsely
identified as a false positive if it does not belong to any pool with a negative item, and that if the smallest
non-null value observed among the pools is attained at least twice. Using similar bounds as in the previous
sections, we obtain
K
L(L − 1) X
F P R(L; K) ≤
(1 − e−λk/K )L−2 .
2K 2
k=1

Bounding the above quantity by

L(L−1)
2K (1

−e

)

−λ L−2

, we obtain the result.

From this formula, the false positive rate can be written explicitly as the probability that among L
independent copies with the above distribution, the first and second running minimum are equal. We plot
once again this function in L for different values of K.

5

Optimizing one-step testing with L fixed

In this section, we look to optimize Algorithm 1 while assuming that the number L of tests that can be made
on each item is finite. This regime is relevant in particular if a test destroys or damages a sample of the
item, so that limiting the number of tests made on each item becomes relevant. In the rest of the section L
is a fixed constant, and n is a number with no prime factor smaller than L − 1. In that situation Lemma 2.1
holds and we are working with multipools, so that the formulas (3.1) and (3.2) both hold.
Recall that the efficiency of the algorithm is E = L
n , therefore to improve the efficiency of the algorithm,
one has to increase the size of the grid. However, augmenting the value of n has the effect of increasing the
false positive and false negative rates. Therefore, to find the optimal efficiency of Algorithm 1, we fix ε > 0
and η > 0 as maximal values for the proportion of positive and negative items wrongfully labelled as negative
and positive respectively, and we choose n as large as possible such that
and F P R(n, L; p, K) ≤ (1 − p)η.

F N R(n, L; p, K) ≤ pε

As the average number of false negatives found by the algorithm is of the order n2 pε, we will also consider
bounds in the regime when ε → 0 as n → ∞.
A choice of n for ε and η fixed. In this case, one has to choose n accordingly to have L(np)L−1 ≤ pε
L
and L(L−1)
2K (np) ≤ η. The largest n that satisfies the first condition is
n1 = p−1

 ε 1/(L−1)
L

and the largest n that satisfies the second condition is

1/L
2Kη
n2 = p−1
.
L(L − 1)
We recommend to choose n as
n ∼ p−1 min



ε 1/(L−1) 2Kη 1/L
;
L
L2
9



(5.1)

This choice of n gives a efficiency of the algorithm given by
 L

L 1/(L−1) LL+2 1/L
Eε,η (p) = p max
;
.
ε
2Kη

(5.2)

Note that these choices of values for n are driven by the results of Proposition 3.2 and hence are quite
conservative, so the efficiency obtained here is an upper bound of the true optimal efficiency of Algorithm 1.
Note that in a high precision setting (when K is large) the false positive are a minority inside the false
discovery of the algorithm, and the efficiency will depend only on p, L and ε.
We observe that the number of tests to use per item to detect defective ones with fixed false negative/positive rate becomes proportional to p the proportion of true negative. In other words, the total
number of tests this algorithm need to detect defective items becomes ultimately proportional to the number
of defective items when the number of non-defective items is large. This is a notable improvement on pool
testing with {0, 1} response, in which the known optimal asymptotic efficiency is proportional to the product
of the number of defective items and the log of the number of non-defective ones.
A choice of n for vanishing ε and η. Observe as well that in the settings we discuss, the expected
number of false negatives in a given grid will grow as εpn2 and the number of false positive as ηn2 . It may
be inconvenient to let the expected number of false discovery to grow as n becomes large. To avoid a positive
proportion of items on the grid being false negatives, one could instead consider a maximal false positive rate
of ε = α/(pn2 ) as n → ∞ and η = β/n2 .
In this situation, with similar computations as above, we obtain optimal choices of
n1 =



α
LpL

1/(L+1)

and n2 =



2Kβ
L(L − 1)pL

1/(L+2)

and the associated efficiency to L and n = min(n1 , n2 ) is


LL+2 1/(L+1) L/(L+2) L(L − 1) 1/(L+2)
;p
Eα,η (p) = max pL/(L+1)
α
2Kβ
This efficiency is much larger than Eε,η (p), as expected from the lower tolerance to false negatives. It behaves
as a power of p as p → 0. Remark that for L = 2, we have Eα,η (p) ∼ Cα p2/3 as p → 0, so even with these
settings, the algorithm becomes more efficient than Dorfman’s method for p small enough, to the cost of
a fixed proportion η of false positive. For L = 3, the algorithm becomes more efficient than Dorfman’s
algorithm even with a vanishingly small number of false positive.

6

Optimal choice of L as a function of p

In this section, we relax the assumption that L has to be kept fixed, and aim at choosing an optimal couple
n, L so that the efficiency of the algorithm E = L/n is as small as possible, while controlling the false positive
and false negative rates. While it is not mentioned explicitly, it is assumed everywhere in this section that
L − 2 is smaller than the smallest prime factor of n, so that Lemma 2.1 can be applied. Using the growth
rate of primordial numbers and the fact that an optimal choice of L will remain finite, there will always be
a couple (n, L) satisfying the assumption of Lemma 2.1 close enough to the optimal theoretical choice, so
this condition won’t play a role in the asymptotic behaviour of the obtained efficiency.We first investigate
the case when the precision of the loads K is infinite so that the choices of n and L are only driven by the
reduction of the false negatives in the algorithm.
We take interest in the quantity

Eε∗ (p) := min L
(6.1)
n , n, L ∈ N : F N R(n, L; p, ∞) ≤ εp .
In this new context, L no longer fixed, and its choice might depend on p and ε.
We recall that by Proposition 4.1 and the computations above, if np → λ ≥ 0, we have
p−1 F N R(p, L; p, ∞) . L(np)L−1 .
As a result we are lead to choose n and L such that L(np)L−1 ≈ ε, while minimizing the efficiency of the
1/(L−1)
algorithm E = L
. We thus obtain that the efficiency of the algorithm is optimal when L is
n ≈ p(ε/L)
taken to minimize
L 7→ (ε/L)1/(L−1) ,
10

and n as (ε/L)1/(L−1) /p. Therefore, the optimum is attained by choosing np → λ > 0.
To precise the computations in that situation, we use the formula given in Proposition 4.1. We observe
that as np → λ, we have
F N R(p, L; p, ∞) → F N R(λ, L; ∞) = (1 + (L − 1)e−λ )(1 − e−λ )L−1 .
Moreover, we have E(p)/p =

L
np

→ L/λ. As a result, we have


Eε∗ (p)
= min L
λ , L ∈ N, λ > 0 : F N R(λ, L; ∞) ≤ ε .
p→0
p
lim

As ε → 0, the optimal is attained for λ, L such that L log(1 − e−λ ) = log(ε)(1 + o(1)), yielding
Eε∗ (p)
− log ε(1 + o(1))
=
.
p→0
p
−λ log(1 − e−λ )
lim

This quantity is minimal for λ = log 2.
log ε
As a consequence, as p → 0, we recommand using n ≈ (log 2)/p, and as ε → 0 L ≈ − log
2 , to obtain an
optimal efficiency behaving as
Eε∗ (p) .

p(− log ε)
(log 2)2

as p → 0 then ε → 0.

Note in particular that n is chosen depending on the value of p, while L is chosen as a function of ε in this
asymptotic regime.
Remark 6.1. As noted in Remark 2.4, Algorithm 1 could be adapted as a two-step algorithm in which every
inconclusive item is tested again individually. Note that the upper bound we use for the false negative rate
is exactly the rate of inconclusive elements (as we bound F N (x) by F N (0)). In this two-step algorithm, the
efficiency would therefore be E (2) (p) = Eε∗ (p)+ε . −p log(ε)(log 2)−2 +ε, which is minimal for ε = (log 2)−2 p.
This two-step algorithm would thus have an efficiency asymptotically bounded by (log 2)−2 p log(p) as p → 0.
Remark 6.2. We also observe that choosing ε = n−1−α for some α > 0, the efficiency becomes
Eε∗ (p) ≈ 2.08(1 + α)p(− log p)

as p → 0.

With this choice of ε, the probability of observing one false negative in the grid decay to 0 as εn2 p ≈ n−α . In
that situation, we obtain an efficiency with a similar order of magnitude of the optimal results of [MTT08],
with a simpler (non-random) construction of the algorithm. Actually, the efficiency obtained here attains
the lower bound of the optimal efficiency predicted in [MT11] for a two-steps binary pool testing algorithm.
Therefore, the non-adaptive Algorithm 1, making use of the load value of items, archives the same efficiency
as an optimal two-stage algorithm.

7

Comparison of the different algorithms

In this section, we illustrate the behavior of our proposed algorithm versus the two-steps Dorfman’s algorithm
and Mézard’s optimal algorithm. We describe the choices of the parameters in the following.
• The simulations took the set {3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47} of the odd prime numbers
under 50 as the possible values of n.
• The algorithm is allowed to perform tests in up to Lmax = 14 directions. In the case when n ≤ 14, we
obviously restrict the number of directions to Lmax = n − 1.
• The prevalence parameter p varies between 0.05 and 0.2 with a constant increment of 0.05.
• We made vary K inside the set {2, 5, 10, 30, 200, 500} to illustrate its influence.
• For each choice of the parameters (n, L, p, K) above, we run 200 copies Algorithm 1.
Consequently, for each choice of the set of parameters, we observe 200 copies of the output of the algorithm.
Afterwards, the matrix of results is compared to the matrix of the true matrix containing the information
of the true positive and negative individuals. Thanks to that, we compute the mean number (over the 200
copies) of false positive and false negative discovered by the algorithm. Thus, we end with an estimation of
the number of false negative F N (n, L, p, K) and the number of false positive F P (n, L, p, K). The next step
is to compute the optimal value of the efficiency E as a function of p. Then, for any couple (p, K) fixed, we
did the following concrete inclusions of the conditions of Section 5.
11

1. We fixed η = 0.01 and we discarded all the pairs n, L such that F P (n, L, p, K) ≥ η(1 − p)n2 .
2. For each value of ε we considered, we discarded the pairs such that F N (n, L, p, K) ≥ εpn2 .
3. Then, from all the remaining values of the pairs (n, L), we minimized the quotient E(p) = L/n.
This value E(p) as a function of p (for different values of K) is the one that we drew in the following
illustrations.

7.1

Comparing the Efficiency with well known algorithms

1.0

Comparing the different algorithms for K= 30

1.0

Comparing the different algorithms for K= 5

+

+

+

●

●

+

Dorfman
Mézard, al.
Grid eps=0.02
Grid eps=0.08
Grid eps=0.20

+

+
●

●

0.8

0.8

Dorfman
Mézard, al.
Grid eps=0.02
Grid eps=0.08
Grid eps=0.20

●

●

+

+
+

●

●

●

+
+
●

+

+

+

●

●

●

●

0.6

0.6

+
+
●
●

●

●

●

●

●
+

+
+

●

●
+

+

+

●

●

●

+

●

+

●

+

●

●

+

0.2

0.2

●

●

●

●

+
●

●

+
●
●

+

+
+

●

●

●

+
●

+

●

+
+

+
●

●

●

+

●
+

+

+
●

0.4

0.4

●

+

+

E(p)

E(p)

+

●

+

●

+

●

●

●

0.0

●

0.0

●

0.00

0.05

0.10

0.15

0.20

0.00

values of p

0.05

0.10

0.15

0.20

values of p

Figure 3: The effect of the value of K on the slope of E(p) for different values of ε

Figure 3 shows for the two different values K = 5 and K = 30, the behavior of our Efficiency curve versus
Dorfman theoretical efficiency and a simulated Mézard, al. efficiency. We drew the resulting points of E(p)
in three different colors (blue,purple,black) that correspond to the choices of ε given by (0.02, 0.08, 0.2). For
each of these ensembles of points, we also drew a simple regression line. It has to be seen that the dependence
of E on p is clearly linear and that the slope of the line is dependent on the choice of the parameter ε, as
expected. It is also interesting to see that the effect of ε is less clear when K is small since the number of
false positives is higher and then is more limitent than when K is large.

7.2

Showing the choices of L and n

The next three plots (in Figure 4) show the choices of the parameter L during the optimization of E for fixed
values of p and K. As before we let ε vary in between the different plots. Besides been a little unstable in
the choice of L along p, we observe that the optimal L remains bounded (hence is fairly independent from
the choice of p) and does change with a change of ε as suggested by the calculations in Section 6.
The last three plots (in Figure 5) are the analogs of the previous plots with the slight difference that
the displayed numbers correspond to the chosen values of n. In this case, we observe that, now, ε has no
more effect on the chosen values of n. As expected, n depends on p in a decreasing manner and validate the
calculation of Section 6. Indeed, we showed that the best choices of n allow to keep the product np more or
less constant which is the case in the simulations.

8

Application to the COVID-19 pandemic and open questions

The application of the present algorithm to PCR testing in the context of the COVID-19 pandemic requires
some adaptation and presents a couple of challenges. It should first be noted that the simplifications we
made in our modeling were quite important. We thus begin by discussing in more details the discrepancies
between the real-world problem and our idealized model.

12

The optimal choice of L for K= 30 and Eps= 0.05

The optimal choice of L for K= 30 and Eps= 0.01

6 ●
6
●

0.8
0.6

12
●
9
●
8
●
11
●

0.4

10
●
9
●

7
●
5
●
7
●
6
●
8
●
6
●
5
●
4
●
3
●

10
●

E(p)

E(p)

8
7●
●

0.4

66
●●
7
●
5
●
10
●

12
●

7
●
10
●
7
12
●●

0.6

0.6

9
●

0.2

11
●

0.05

0.10

0.15

0.20

9
●
7
●
5
●
4
●

10
●

0.2

E(p)

6
7●
10
●●

0.4

10
●

12
9●
●

7
●
88
●●

6
●

11
●

10
●

5
●
9
●

0.2

12
●

6
11
●●

0.8

0.8

9
●
44 ●
4
●●

0.00

The optimal choice of L for K= 30 and Eps= 0.001

10
●

8
●
5
●

0.00

0.05

0.10

values of p

0.15

0.20

0.00

0.05

0.10

values of p

0.15

0.20

values of p

Figure 4: Efficiency with respect to p and the associated optimal choice of the parameter L. The number displayed inside the
blue bubbles correspond to the chosen value of L in the optimization.

The optimal choice of n for K= 30 and Eps= 0.05

The optimal choice of n for K= 30 and Eps= 0.01

7 ●
7
●

0.8

0.8

●
17
●19
29
●
29
●

23
●
19
●
29
●
29
●
47
●
43
●
47
●
47
●
47
●

17
●

E(p)

13
23
●●

0.4

29
●

0.6

17
●

E(p)

13
13
●●
17
●
13
●

17
●

11
●

0.6

0.6
E(p)

●
●11
19
●13

7
●

13
●

●
13
●17

11
●
13
13
●●

0.4

13
●

13
●

7
●

13
●

23
●
19
●
19
●
29
●
29
●

43
●
43
●
41
●
47
●

0.05

0.10
values of p

0.15

0.20

0.00

43
●

0.2

43
●

0.2

0.2

13
●

7
13
●●

55 ●
5
●●

0.4

0.8

11
●

0.00

The optimal choice of n for K= 30 and Eps= 0.001

11
●

47
●
47
●

0.05

0.10
values of p

0.15

0.20

0.00

0.05

0.10

0.15

0.20

values of p

Figure 5: Efficiency with respect to p and the associated optimal choice of the parameter n. The number displayed inside the
blue bubbles correspond to the chosen value of n in the optimization.

Finite size of samples. In COVID-19 pool testing, the items that are tested are samples taken from
subjects, via nasal swab, saliva sample or other method. If there seems to be usually enough matter to split
the sample into several tests, it will not be possible to make an arbitrary large number of tests on each
sample. Therefore, optimal computations made in Section 5 might be somewhat more relevant. Additionally,
it is worth noting that combining several samples have the effect of creating a composition with the average
viral load rather than the maximal, although the fact that this viral load is spread over several orders of
magnitudes negates partially this problem as discussed in the introduction.
Noisiness in the measure. We chose to represent a lack of accuracy of the measure by replacing the
continuously distributed viral load by a discrete distribution, as if contaminated tests could be arranged into
“classes” of similarly measured viral load. In practice, the quantity returned is a real number, which is a
measure on which a Gaussian noise is expected to be applied. More precisely, several steps of the process have
the effect of creating uncertainty on the measure, and the variance that has to be expected from a pooled
test might be rather large. There is first the collection of the portions of samples used to make the group
testing, whose volume and associated viral load may vary with regards to the attended equal contributions.
Next, the reverse transcriptase step that converts RNA samples of the virus into DNA might introduce
additional noise depending on its rate of conversion. Finally, the PCR measurement itself products a noisy
value, partially corrected by the fact that classically, two different DNA sequences for the virus are measured
separately. It would then require some adaptation of our algorithm to adapt the “finite precision algorithm”
to noisy Gaussian measure of the viral load in each pool, although classical likelihood ratio estimates might
be successfully used here.
Distribution of the viral load among contaminated. Concerning the distribution of the viral load
of the samples, we made here the choice of uniform distribution, which is the most favorable for this type
of algorithm. Although this is quite far from what is effectively observed [JMV+ 20, CRP+ 20], the viral
load observed among large groups of people is usually successfully approached by a mixture of two to three
Gaussian variables with standard deviations between 3 and 6, spanning over the interval [20, 40] (c.f. [BMR20,
13

Appendix B]). The viral loads may be considered sufficiently spread over the interval so that the algorithm
discussed above might still be relevant. The nice and explicit calculations on the choices of the parameters
would need to be adapted. They might also need to be tuned from day to day, depending of the expected
prevalence of samples on a given day, which might vary over time.
Precision limits of the PCR. As it was first noted in [Fur18], theoretical aspects of pool testing usually
assume that the quality of the test does not depend on the size of the pool. However, this is rarely the case
in real-world applications, and it is indeed not the case for the present application. In particular, we show
in [BMR20] that pooling has an impact on the measurement of samples with small viral loads. In our toy
model, this could be taken into account by specifying that in pools of size n, items with load smaller than
cn are treated as non-defective items, for some increasing function cn . This has the effect of decreasing the
value of the optimal choice for n, in order to detect enough contaminated individuals with small viral load.
However, the computations in this case being very dependent on the function cn , we choose not to include it
in the present work.
Random outcome of a pooled test. Finally, we assumed that each test of the pools is performed with
no other error than the one inherent to the PCR itself. It is probably an oversimplification in this case, as
pool testing implies important manipulations of the samples, with possible additional errors involved. For
example, forgetting to collect one individual in a pool, contaminating a pool with a sample that should
not belong to it, etc. Those human errors would create noise on the measures of the pools and so would
deteriorate the information given to our algorithm. Therefore, it would need to be adapted to this situation,
in order not to characterize as negative a sample measured at high values in all but one pool, for example.
Potential extensions. The algorithm presented here has the advantage of being simple to implement
and easy to solve, even by hand. However, more precise algorithms might be employed with the help of
automation for the creation of samples and measure of results. It would therefore be interesting to create
more precise algorithms for PCR-type pool testing. A relevant generalization could be to collect and use
additional information on the subjects. We can imagine that, throughout interviews, some individuals might
be identified as being more likely to be contaminated, while others could be simply routinely tested. It is
probably more efficient to tests the former in smaller pools and the latter in larger ones.
An other project of interest might be the deconvolution of pools created by Dorfman’s algorithm. More
precisely, in the algorithm, instead of testing individually every member of a group detected as contaminated,
it might be interesting to test several samples from different positive pools in a two-stage deconvolution that
might represent a further economy of tests on Dorfman’s algorithm. Choosing the right pools to pair together,
as well as the number of positive pools to be de-convoluted at the same time might be an interesting expansion
on the current work.
Acknowledgements.
We wish to thank members of MODCOV-19 platform of the CNRS for support, in particular Françoise Praz
and Florence Débarre who gave us numerous helpful comments, in particular on the biological aspects of
PCR. We also thank the members of the Groupool initiative for helpful discussions at the earlier stages of
this project on group testing.

References
[ABB19]

H. Aprahamian, D. R. Bish, and E. K Bish. Optimal Risk-Based Group Testing. Management
Science, 65(9):4365–4384, sep 2019.

[AJS19]

M. Aldridge, O. Johnson, and J. Scarlett. Group Testing: An Information Theory Perspective.
Foundations and Trends in Communications and Information Theory, 15(3-4):196–392, 2019.

[BAKS+ 20] Roni Ben-Ami, Agnes Klochendler, Matan Seidel, Tal Sido, Ori Gurel-Gurevich, Moran Yassour, Eran Meshorer, Gil Benedek, Irit Fogel, Esther Oiknine-Djian, Asaf Gertler, Zeev Rotstein, Bruno Lavi, Yuval Dor, Dana G Wolf, Maayan Salton, and Yotam Drier. Large-scale
implementation of pooled RNA-extraction and RT-PCR for SARS-CoV-2 detection. medRxiv,
2020.
[BBC+ 20]

M. Beunardeau, É Brier, N Cartier, A. Connolly, N. Courant, R. Géraud-Stewart, D. Naccache,
and O. Yifrach-Stav. Optimal Covid-19 Pool Testing with a priori Information. arXiv:2005.02940,
2020.
14

[BMR20]

Vincent Brault, Bastien Mallein, and Jean-Francois Rupprecht. Group testing as a strategy for
the epidemiologic monitoring of COVID-19. arXiv:2005.06776, 2020.

[CCK+ 99]

M. A. Chateauneuf, C. J. Colbourn, D. L. Kreher, E. R. Lamken, and D. C. Torney. Pooling,
lattice square, and union jack designs. Annals of Combinatorics, 3(1):27–35, March 1999.

[CJBJ17]

S. Cai, M. Jahangoshahi, M. Bakshi, and S. Jaggi. Efficient algorithms for noisy group testing.
IEEE Transactions on Information Theory, 63(4):2113–2136, 2017.

[CRP+ 20]

Jorge J Cabrera, Sonia Rey, Sonia Perez, Lucia Martinez-Lamas, Olaia Cores-Calvo, Julio Torres,
Jacobo Porteiro, Julio Garcia-Comesana, and Benito J Regueiro. Pooling For SARS-COV-2
Control In Care Institutions. medRxiv, 2020.

[Dor43]

Robert Dorfman. The Detection of Defective Members of Large Populations. The Annals of
Mathematical Statistics, 1943.

[FHG+ 12]

Sasan R. Fereidouni, Timm C. Harder, Nicolas Gaidet, Mario Ziller, Bernd Hoffmann, Saliha
Hammoumi, Anja Globig, and Elke Starick. Saving resources: Avian influenza surveillance using
pooled swab samples and reduced reaction volumes in real-time RT-PCR. Journal of Virological
Methods, 186(1):119–125, 2012.

[Fur18]

T. Furon. The illusion of group testing. [Research Report] RR-9164, Inria Rennes Bretagne
Atlantique, pages 1–19, 2018.

[GG20]

Christian Gollier and Olivier Gossner. Group Testing against Covid-19. Covid Economics,
(2):32–42, 2020.

[GRK+ 20]

Sabyasachi Ghosh, Ajit Rajwade, Srikar Krishna, Nikhil Gopalkrishnan, Thomas E. Schaus,
Anirudh Chakravarthy, Sriram Varahan, Vidhya Appu, Raunak Ramakrishnan, Shashank Ch,
Mohit Jindal, Vadhir Bhupathi, Aditya Gupta, Abhinav Jain, Rishi Agarwal, Shreya Pathak,
Mohammed Ali Rehan, Sarthak Consul, Yash Gupta, Nimay Gupta, Pratyush Agarwal, Ritika
Goyal, Vinay Sagar, Uma Ramakrishnan, Sandeep Krishna, Peng Yin, Dasaradhi Palakodeti,
and Manoj Gopalkrishnan. Tapestry: A Single-Round Smart Pooling Technique for COVID-19
Testing. medRxiv, 2020.

[HSP20]

Catherine A. Hogan, Malaya K. Sahoo, and Benjamin A. Pinsky. Sample Pooling as a Strategy
to Detect Community Transmission of SARS-CoV-2. JAMA, 323(19):1967, may 2020.

[IKWO18]

H. A. Inan, P. Kairouz, M. Wootters, and A. Ozgur. On the Optimality of the Kautz-Singleton
Construction in Probabilistic Group Testing, 2018.

[JMV+ 20]

Terry C Jones, Barbara Mühlemann, Talitha Veith, Guido Biele, Marta Zuchowski, Jörg Hoffmann, Angela Stein, Anke Edelmann, Victor Max Corman, and Christian Drosten. An analysis
of SARS-CoV-2 viral load by patient age. medRxiv, page 2020.06.08.20125484, 2020.

[LPBG+ 20] Stefan Lohse, Thorsten Pfuhl, Barbara Berkó-Göttel, Jürgen Rissland, Tobias Geißler, Barbara
Gärtner, Sören L Becker, Sophie Schneitler, and Sigrun Smola. Pooling of samples for testing
for SARS-CoV-2 in asymptomatic people. The Lancet Infectious Diseases, 3099(20), apr 2020.
[LSF20]

Marc Lipsitch, David L. Swerdlow, and Lyn Finelli. Sample Pooling as a Strategy to Detect
Community Transmission of SARS-CoV-2. New England Journal of Medicine, 382(13):1194–
1196, mar 2020.

[MNB+ 20] Leon Mutesa, Pacifique Ndishimye, Yvan Butera, Annette Uwineza, Robert Rutayisire, Emile
Musoni, Nadine Rujeni, Thierry Nyatanyi, Edouard Ntagwabira, Muhammed Semakula, Clarisse
Musanabaganwa, Daniel Nyamwasa, Maurice Ndashimye, Eva Ujeneza, Ivan Emile Mwikarago,
Claude Mambo Muvunyi, Jean Baptiste Mazarati, Sabin Nsanzimana, Neil Turok, and Wilfred
Ndifon. A strategy for finding people infected with SARS-CoV-2: optimizing pooled testing at
low prevalence. arXiv, April 2020.
[MT11]

M. Mézard and C. Toninelli. Group Testing With Random Pools: Optimal Two-Stage Algorithms. IEEE Transactions on Information Theory, 57(3):1736 – 1745, 2011.

[MTT08]

M. Mézard, M. Tarzia, and C. Toninelli. Group testing with random pools: Phase transitions
and optimal strategy. Journal of Statistical Physics, 131(5):783–801, 2008.
15

[SAKH20]

Nasa Sinnott-Armstrong, Daniel Klein, and Brendan Hickey. Evaluation of Group Testing for
SARS-CoV-2 RNA. medRxiv, 2020.

[SNGYL20] Haran Shani-Narkiss, Omri David Gilday, Nadav Yayon, and Itamar Daniel Landau. Efficient
and Practical Sample Pooling for High-Throughput PCR Diagnosis of COVID-19. medRxiv,
2020.
[TAN20]

Ignacio Torres, Eliseo Albert, and David Navarro. Pooling of Nasopharyngeal Swab Specimens
for SARS-CoV-2 detection by RT-PCR. Journal of Medical Virology, page 25971, may 2020.

[Tho62]

Keith H. Thompson. Estimation of the Proportion of Vectors in a Natural Population of Insects.
Biometrics, 18(4):568, dec 1962.

[TM06]

N. Thierry-Mieg. Pooling in systems biology becomes smart. Nature Methods, 3(3):161–162, mar
2006.

[Tä20]

Matthias Täufer. Rapid, large-scale, and effective detection of COVID-19 via non-adaptive
testing. Journal of Theoretical Biology, 506:110450, 2020.

16

