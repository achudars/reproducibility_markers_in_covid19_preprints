Synthesis of COVID-19 Chest X-rays using Unpaired Image-to-Image Translation
Hasib Zunair and A. Ben Hamza
Concordia Institute for Information Systems Engineering
Concordia University, Montreal, QC, Canada

arXiv:2010.10266v1 [eess.IV] 20 Oct 2020

Abstract

chest X-ray provides a two-dimensional (2D) image, while a
CT scan has the ability to form three-dimensional (3D) images
of the chest. However, chest CT based screening is more expensive, not always available at small or rural hospitals, and often
yields a high false-positive rate. Therefore, the need to develop
computational approaches for detecting COVID-19 via chest radiography imaging not only can save healthcare a tremendous
amount of time and money, but more importantly, it can save
more lives [3]. By leveraging deep learning, several approaches
for the detection of COVID-19 cases from chest radiography
images have been recently proposed, including tailored convolutional neural network (CNN) architectures [4, 5] and transfer
learning based methods [6–9].
While promising, the predictive performance of these deep
learning based approaches depends heavily on the availability of
large amounts of data. However, there is a significant shortage
of chest radiology imaging data for COVID-19 positive patients,
due largely to several factors, including the rare nature of the radiological finding, legal, privacy, technical, and data-ownership
challenges. Moreover, most of the data are not accessible to the
global research community.
In recent years, there have been several efforts to build largescale annotated datasets for chest X-rays and make them publicly available to the global research community [10–14]. At
the time of writing, there exists, however, only one annotated
COVID-19 X-ray image dataset [15], which is a curated collection of X-ray images of patients who are positive or suspected of COVID-19 or other viral and bacterial pneumonias.
This COVID-19 image data collection has been used as a primary source for positive cases of COVID-19 [4–7], where the
detection of COVID-19 is formulated as a classification problem. While the COVID-19 image data collection contains positive examples of COVID-19, the negative examples were acquired from publicly available sources [13] and merged together
for data-driven analytics. This fusion of multiple datasets results in predominantly negative examples with only a small percentage of positive ones, giving rise to a class imbalance problem [10–14]. This in turn becomes a challenge of its own, as
the merged data becomes highly imbalanced. In the context of
a classifier training, the class imbalance problem in the training
data distribution yields sub-optimal performance on the minority class (i.e. positive class for COVID-19).
In order to overcome the aforementioned issues, we present a
domain adaptation framework by leveraging the inter-class variation of the data distribution for the task of conditional image
synthesis by learning the inter-class mapping and synthesizing

Motivated by the lack of publicly available datasets of chest radiographs of positive patients with Coronavirus disease 2019
(COVID-19), we build the first-of-its-kind open dataset of synthetic COVID-19 chest X-ray images of high fidelity using an
unsupervised domain adaptation approach by leveraging class
conditioning and adversarial training. Our contributions are
twofold. First, we show considerable performance improvements on COVID-19 detection using various deep learning
architectures when employing synthetic images as additional
training set. Second, we show how our image synthesis method
can serve as a data anonymization tool by achieving comparable
detection performance when trained only on synthetic data. In
addition, the proposed data generation framework offers a viable
solution to the COVID-19 detection in particular, and to medical image classification tasks in general. Our publicly available benchmark dataset1 consists of 21,295 synthetic COVID-19
chest X-ray images. The insights gleaned from this dataset can
be used for preventive actions in the fight against the COVID-19
pandemic.
Keywords: Chest X-rays, COVID-19, image synthesis, deep
learning, image classification, imbalanced data.

1

Introduction

The World Health Organization (WHO) has declared COVID19, the infectious respiratory disease caused by the novel coronavirus, a global pandemic due to the rapid increase in infections
worldwide. This virus has spread across the globe, sending billions of people into lockdown, as many countries rush to implement strict measures in an effort to slow COVID-19 spread
and flatten the epidemiological curve. Although most people
with COVID-19 have mild to moderate symptoms, the disease
can cause severe lung complications such as viral pneumonia,
which is frequently diagnosed using chest radiography.
Recent studies have shown that chest radiography images
such as chest X-rays (CXR) or computed tomography (CT)
scans performed on patients with COVID-19 when they arrive at
the emergency room can help doctors determine who is at higher
risk of severe illness and intubation [1, 2]. These X-rays and
CT scans show small patchy translucent white patches (called
ground-glass opacities) in the lungs of COVID-19 patients. A
1 https://github.com/hasibzunair/synthetic-covid-cxr-dataset

1

under-represented class samples from the over-represented ones
using unpaired image-to-image translation [16]. The proposed
framework combines class-conditioning and adversarial training in a bid to synthesize realistic looking COVID-19 CXR images. The generated synthetic dataset contains 21,295 synthetic
images of chest X-rays for COVID-19 positive cases. Figure 1
shows how our approach learns to automatically translate an image from one category to another, and more specifically from
normal to COVID-19, and pneumonia to COVID-19.
In addition to demonstrating improved COVID-19 detection
performance through the use of various deep convolutional neural network architectures on the synthetic data to boost training, we show how the proposed data generation and evaluation
pipeline can serve as a viable data-driven solution to medical
image analysis problems, and make our dataset publicly available, which is currently comprised of 21,295 synthetic images
of chest X-rays for COVID-19 positive cases. The main contributions of this paper can be summarized as follows:
• We present an integrated deep learning based framework,
which couples adversarial training and transfer learning to
jointly address inter-class variation and class imbalance.
Figure 1: Given any two unordered image collections, the generative algorithm learns to automatically translate an image from
one category to another. Top: Normal (left) to COVID-19
(right). Bottom: Pneumonia (left) to COVID-19 (right). Subtle visual changes can be observed in the translated images due
to low inter-class variation.

• We synthesize chest X-ray images of COVID-19 to adjust
the skew in training sets by over-sampling positive cases to
mitigate the class imbalance problem, while training classifiers.
• We demonstrate how the data generation procedure can
serve as an anonymization tool by achieving comparable
detection performance when trained only on synthetic data
versus real data in an effort to alleviate privacy concerns.

With the scarcity of annotated medical image datasets, there
has been a surge of interest in developing efficient approaches
for the generation of synthetic medical images. While several
existing generative methods have addressed the translation between multiple imaging modalities CT-PET, CS-MRI, MR-CT,
XCAT-CT [19–22] based on distribution matching, other approaches have focused on the scarcity of labeled data in the
medical field due in large part to the acquisition, privacy and
health safety issues. Conditional and unconditional image synthesis procedures, built on top of these generative models, have
been proposed in retinal images [23,24] and MRI scans [25–27].
These models involve the training of paired data in both source
and target domains to synthesize realistic, high-resolution images in order to aid in medical image classification and segmentation tasks.

The rest of this paper is organized as follows. In Section 2, we
provide a brief overview of generative approaches for medical
image synthesis. In Section 3, we present a generative framework, which couples adversarial training and transfer learning to
jointly address inter-class variation and class imbalance. In Section 4, we present experimental results to demonstrate improved
COVID-19 detection performance through the use of various
deep convolutional neural network architectures on the generated data. Finally, we conclude in Section 5 and point out future
work directions.

2

Related Work

Image synthesis methodologies have also been proposed in
the context of chest X-rays [28]. Our work is significantly different in the sense that we are specifically interested in synthesizing a particular class, whereas in [28] X-rays are generated from surface geometry for landmark detection tasks. While
some generative methods only require paired data in the source
domain with target domain consisting of unlabelled examples,
Cohen et al. [29] have demonstrated that the phenomenon of
hallucinating features (e.g. adding or removing tumors leading
to a semantic change) leads to a high bias in these domain adaptation techniques. To overcome this issue, we have recently proposed a domain adaptation technique based on cycle-consistent

The advent of generative adversarial networks (GANs) [17]
has accelerated research in generative modeling and distribution learning. With the ability to replicate data distributions and
synthesize images with high fidelity, GANs have bridged the
gap between supervised learning and image generation. These
synthetic images can then be used as input to improve the performance of various deep learning algorithms for downstream
tasks, such as image classification and segmentation. GANs
have not only been used in natural images’ settings, but have
also been extensively employed in medical image analysis [18],
where labels are usually scarce or almost non-existent.
2

adversarial networks in order synthesize high fidelity positive
examples to improve detection performance of melanoma from
skin lesion images [30].

The overall objective function is defined as
L(GA , GB , DB , DA ) = LGAN (GA , DB , A, B)
+ LGAN (GB , DA , B, A)

(1)

+ λ Lcyc (GA , GB ),

3

which consists of two adversarial losses and a cycle consistent
loss regularized by a hyper-parameter λ [16]. The first adversarial loss is given by

Proposed Method

In this section, we present the main building blocks of our
proposed image synthesis framework using image-to-image
translation, which is an increasingly popular machine learning
paradigm that has shown great promise in a wide range of applications, including computer graphics, style transfer, satellite
imagery, object transfiguration, character animation, and photo
enhancement. In an typical image-to-image translation problem,
the objective is to learn a mapping that translates an image in one
domain to a corresponding image in another domain using approaches that leverage paired or unpaired training samples. The
latter is the focus of our work. While paired image-to-image
translation methods use pairs of corresponding images in different domains, the paired training samples are, however, not always available. By contrast, the unpaired image-to-image translation problem, in which training samples are readily available,
is more common and practical, but it is highly under-constrained
and fraught with challenges. In our work, we build upon the idea
that there exist no paired training samples showing how an image from one domain can be translated to a corresponding image
in another domain. The task is to generate COVID-19 chest Xrays from chest X-ray images to address COVID-19 class imbalance problem. More specifically, our goal is to learn a mapping
function between Non-COVID-19 images and COVID-19 in order to generate COVID-19 chest X-rays without paired training
samples in an unsupervised fashion.

LGAN (GA , DB , A, B) = Eb∼pdata (b) [log DB (b)]
+ Ea∼pdata (a) [log(1 − DB (Ga (a)))],

(2)

where the generator GA tries to generate images GA (a) that
look similar to COVID-19 images, while DB aims to distinguish
between generated samples GA (a) and real samples b. During
the training, as GA generates a COVID-19 image, DB verifies
if the translated image is actually a real COVID-19 image or
a synthetic one. The data distributions of Non-COVID-19 and
COVID-19 are pdata (a) and pdata (b), respectively.
Similarly, the second adversarial loss is given by
LGAN (GB , DA , B, A) = Ea∼pdata (a) [log DA (a)]
+ Eb∼pdata (b) [log(1 − DA (GB (b)))],

(3)

where GB takes a COVID-19 image b from B as input, and
tries to generate a realistic image GB (b) in A that tricks the
discriminator DB . Hence, the goal of GB is to generate a NonCOVID-19 chest X-ray such that it fools the discriminator DA
to label it as a real Non-COVID-19 image.
The third loss term is to enforce cycle consistency an is given by
Lcyc (GA , GB ) = Ea∼pdata (a) [kGB (GA (a)) − ak1 ]
+ Eb∼pdata (b) [kGA (Gb (b)) − bk1 ],

(4)

which computes the difference between the input image and the
generated one using the `1 -norm.

3.1

Chest X-ray image synthesis
3.2

We formulate the detection of COVID-19 as a binary classification problem. For the Normal vs. COVID-19 and Pneumonia
vs. COVID-19 tasks, we train two translation models and synthesize COVID-19 images for each task in order to adjust the
skew in the training data by over-sampling the minority class.
For the sake of clarity and unless otherwise expressly indicated,
we refer to the source domain of the two tasks as Non-COVID19 instead of Normal and Pneumonia separately.

Model optimization

The idea of the cycle consistency loss it to add a constraint such
that GB (GA (a)) ≈ a and GA (GB (b)) ≈ b. In other words, the
objective is to learn two bijective generator mappings by solving
the following optimization problem:
G∗A , G∗B = arg min

max L(GB , GB , DB , DA ).

GA ,GB DA ,DB

(5)

For the generators GA and GB , the architecture is based on fully
convolutional network (FCN). The discriminators DB and DA
consists of a CNN classifier which verifies whether the image is
real or synthetic.

We adopt our unsupervised domain adaptation technique introduced in [30] to translate Non-COVID-19 images for each
case (i.e. normal or pneumonia) to COVID-19. Given two image domains A and B denoting Non-COVID-19 and COVID-19,
respectively, the goal is to learn to translate images of one type
to another using two generators GA : A → B and GB : B → A,
and two discriminators DB and DA , as illustrated in Figure 2.

3.3

Training procedure

The training for the generators and discriminators are carried out
in the same way as in [30]. First, we balance the inter-class data
samples by performing undersampling. Then, we train CycleGAN to learn a function of the interclass variation between

The generator GA (resp. GB ) translates images from NonCOVID-19 to COVID-19 (i.e. A → B), while the discriminator
DB (resp. DA ) verifies how real an image of B (resp. A) looks.
3

the two groups, i.e. we learn a transformation between NonCOVID-19 and COVID-19 radiographs. We apply CycleGAN
to the over-represented class samples in order to synthesize the
target class samples (i.e. under-represented class).
After training, we apply the generators GA and GB on the
training datasets of Normal vs. COVID-19 and Pneumonia vs.
COVID-19. We apply GA on the majority class of Normal vs.
COVID-19, which consists of normal images in order to synthesize 16,537 COVID-19 images. We denote this synthesized
dataset as GN C , which consists of generated images by performing image-to-image translation from normal to COVID-19.
Similarly, for Pneumonia vs. COVID-19, we synthesize
4,758 COVID-19 images by applying GB on the majority class
consisting of pneumonia images and we denote the synthesized dataset as GP C , which is comprised of generated images
by performing image-to-image translation from pneumonia to
COVID-19. It is worth pointing out that for the sake of clarity,
the discriminator DA is not depicted to Figure 2, as our main is
to generate COVID-19 images from Non COVID-19 images.

such as Radiopaedia.org, Italian Society of Medical and Interventional Radiology2 , and Figure1.com3 . From this dataset, we
discard the CT images and retain the 226 images positive for
COVID-19 and their corresponding labels.
RSNA Pneumonia Detection Challenge. This dataset originated from a Kaggle challenge4 and consists of publicly available data from [13]. It is composed of 26684 images, and each
image was annotated by a radiologist for the presence of lung
opacity; thereby providing a label for two classes. This label is
included as both lung opacity and pneumonia.

4.2

We partition the three classes from COVID-19 Image Data Collection and RSNA Pneumonia Detection Challenge into two
sets, namely “Normal vs. COVID-19” and “Pneumonia vs
COVID-19”. A patient level split is then applied using 80%
as training set and the remaining 20% as test set to assess algorithm performance, and we follow the same evaluation protocol
laid out in [25, 30]. We define the skew ratio as follows:

Cycle Consistency

Non COVID-19

Generated COVID-19

GA

Non COVID-19 / COVID-19?

Reconstructed Non COVID-19

Skew =

DB

Malignant Images

4.3

(6)

Baselines

Since our aim is to provide a dataset to be used as a training set for the minority class, we test the effectiveness of several deep CNN architectures, including VGG-16 [31], ResNet50 [32] and DenseNet-102 [33], on the detection of the minority class. These pretrained networks were trained on more than
a million images from the ImageNet database5 . More specifically, we investigate the contribution of the synthetic datasets
GN C and GP C , which consist of COVID-19 CXR images, to
the overall performance of these deep learning models. The last
layer of each of these models consists of a global average pooling (GAP) layer, which computes the average output of each
feature map in the previous layer and helps minimize overfitting
by reducing the total number of parameters in the model. The

Experiments

In this section, we conduct extensive experiments to evaluate
the performance of the proposed data generation framework on
COVID-19 detection.

4.1

Negative Examples
,
Positive Examples

where Skew = 1 represents a fully balanced dataset, Skew > 1
shows that the negative samples are the majority, and Skew < 1
represents positive sample dominance in the distribution.
The data distributions of Normal vs. COVID-19 and Pneumonia vs. COVID-19 are displayed in Figure 3, which illustrates
the class imbalance in the training dataset. For Pneumonia vs.
COVID-19, the skew ratio is around 22.9, while the skew for
Normal vs. COVID-19 is almost four times larger, indicating
high imbalance in the classes.
We also resize all images to 256 × 256 pixels, and scale the
pixel values to [0, 1] for the training of classifiers. It is important
to mention that when we use the term synthetic data, we refer to
COVID-19 CXR images only.

GB

Figure 2: Illustration of the generative adversarial training process for unpaired image-to-image translation. Chest X-ray images are translated from Non-COVID-19 (i.e. Normal or Pneumonia) to COVID-19 and then back to Non-COVID-19 to ensure cycle consistency in the forward pass. The same procedure is applied in the backward pass from COVID-19 to NonCOVID-19.

4

Dataset splits and preprocessing

Datasets

We use two publicly available datasets of chest X-rays:
COVID-19 Image Data Collection. This dataset comprises
226 images of pneumonia cases with chest X-ray or CT images, specifically COVID-19 cases as well as MERS, SARS,
and ARDS. Data are scraped from publications and websites

2 https://www.sirm.org/category/senza-categoria/covid-19/
3 https://www.figure1.com/covid-19-clinical-cases
4 https://www.kaggle.com/c/rsna-pneumonia-detection-challenge
5 http://www.image-net.org

4

Number of Samples in thousands

Number of Samples in thousands

Normal vs COVID-19 data distribution

16537

Train
Test

3K

centage of positive instances correctly classified and is defined
as
TP
Sensitivity =
,
(7)
TP + FN
where TP, FP, TN and FN denote true positives, false positives,
true negatives and false negatives, respectively. TP is the number of correctly predicted malignant lesions, while TN is the
number of correctly predicted benign lesions. A classifier that
reduces FN (ruling COVID-19 out in cases that do have it) and
FP (wrongly diagnosing COVID-19 where there is none) indicates a better performance. A false negative COVID-19 result
can be a serious problem due to the fact that we lose the benefits
of early intervention. A false positive result can also cause significant issues for both an individual and the community. Even
from an epidemiologicial perspective, a high number of false
positives can lead to a wrong understanding of the spread of
COVID-19 in the community. Sensitivity, also known as recall
or true positive rate, indicates how often a classifier misses a
positive prediction. It is one of the most common measures to
evaluate a classifier in medical image classification tasks [36].
A larger value of Sensitivity indicates a better performance of
the classification model.

2K

4.5

16K
14K
12K
10K
8K
6K
4K
2K
0

5K

4135
180

46

Normal
COVID-19
Pneumonia vs COVID-19 data distribution
4758

Train
Test

4K

1K
0

1257
180

Pneumonia

All experiments are performed on a Linux Workstation (CPU:
AMD 2nd Gen Ryzen Threadripper 2950X, 16-Core, 64Thread, 4.4GHz Max Boost; Memory: 64GB high-performance
RAM; GPU: NVIDIA GeForce RTX 2080 Ti). We perform
training/testing on both COVID-19 Image Data Collection and
RSNA Pneumonia Detection Challenge. For training the models, we use the Adadelta optimization alogrithm [37] to minimize the binary cross-entropy loss function with a learning rate
of 0.001 and batch size of 16. We initialize the weights using
ImageNet and train all layers until the loss stagnates using an
early stopping mechanism.
For each dataset, we follow the same evaluation protocol laid
out in [25, 30] for testing the contribution of newly added data.
In this evaluation protocol, both training and test sets are used.
The training set varies, as new data are added to each configuration. The deep CNN classifiers are trained on this data and
evaluated on the held-out test set. For fair evaluation and comparison purposes, the size of the test set remains constant. It is
important to mention that the test set does not contain any synthetic examples. Moreover, the hyper-parameters are not tuned
and hence do not require a separate validation set.

46

COVID-19

Figure 3: Data distributions of Normal vs. COVID-19 (top) and
Pneumonia vs. COVID-19 (bottom) with skew ratios of 91.87
and 22.9, respectively.
GAP layer turns a feature map into a single number by taking
the average of the numbers in that feature map. Similar to maxpooling layers, GAP layers have no trainable parameters and are
used to reduce the spatial dimensions of a 3D tensor. The GAP
layer is followed by a single fully connected (FC) layer with a
softmax function (i.e. a dense softmax layer of two units for the
binary classification case), which yields the predicted classes’
probabilities that sum to one.

4.4

Implementation details

Evaluation metrics
4.6

Due to high class imbalance in the datasets, the choice of evaluation metrics plays a vital role in the comparison of classifiers. Threshold metrics such as accuracy and rank metrics (e.g.
area under the ROC curve) may lead to a false sense of superiority and mask poor performance [34], thereby introducing
bias. Since we are interested in the detection of the minority
class (COVID-19), we follow the recommendations provided
in [34,35] and perform quantitative evaluations using sensitivity
and false negatives in the same vein as [6]. Sensitivity is the per-

Over-sampling with synthetic data

We demonstrate the effectiveness of the synthetic sets GN C
and GP C in Tables 1 and 2 using four deep learning models,
namely VGG-16 [31], ResNet-50 [32], DenseNet-102 [33], and
DenseNet-121 with a bagging tree classifier (DenseNet121 +
BGT) [6]. For each task, we can observe that when GN C is
added, there is a significant increase in performance. While
the addition of GP C also results in an increase in performance,
such an increase is not quite large compared to adding GN C in
5

Normal vs COVID-19

Probability

1.0

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0

0.0

0.0

0

10

30

40

0

10

20

1.0

30

40

Number of COVID cases

0.0

Figure 4: Confidence scores for the VGG-16 model on unseen
test set of COVID-19 for the two binary classification tasks of
Normal vs. COVID-19 (left) and Pneumonia vs. COVID-19
(right) using original data only.

training set, we observe that not only the number of correctly
detected instances of COVID-19 increases, but also the predictions tend to improve, as demonstrated by the high probability
scores.
Normal vs G1

Pneumonia vs G2

1.0

1.0

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0

0.0

Probability

Probability

Training on anonymized synthetic data

We also evaluate the performance when the classifiers are
trained on only synthetic COVID-19 images, as shown in Tables 1 and 2 for each dataset. Sub-optimial performance is
achieved for both tasks for different CNNs, except for VGG-16
which shows performance improvement compared to when using the original COVID-19 examples. Since a new data sample
is not attributed to an individual patient, but it is rather an instance which is conditioned on the training data, it does not entirely reflect the original data. This suggests that synthetic data
alone cannot to used to achieve optimal performance. In other
words, the synthetic data can be used as a form of pre-training,
which often requires a small amount of real data to achieve comparable performance. In addition, the relatively large margin
between the evaluation scores suggests that the observed difference between the models is actually real, and not due to a
statistical chance.

4.8

20

Number of COVID cases

1.0

4.7

Pneumonia vs COVID-19

1.0

Probability

1.0

some cases. We hypothesize that this is due to the number of
COVID-19 examples in GN C (16,537), which enables the models to learn better representations for COVID-19, whereas GP C
is comprised of only 4,758 COVID-19 examples. Further, an increase in performance using both metrics is observed when the
skew in the training dataset decreases. The relative improvement seems to drop as the model complexity increases, which
is in line with the findings in [38] due to the problem of overparametrization. When synthetic data are used as additional
training set, the detection performance significantly increases.
However, the relative improvement drops when the architectural
complexity of the model increases. Note that despite its simplicity, the VGG-16 network outperforms all the other baseline
methods, while the “DenseNet121 + BGT” model yields the second best performance. For less complex models, we can see
that using only synthetic dataset performs better than the original data. Moreover, Table 2 shows that with the exception of
VGG-16, all models achieve sub-optimal performance when using synthetic data only.

0.0

0

10

20

30

Number of COVID cases

40

0

10

20

1.0

30

40

Number of COVID cases

0.0

Figure 5: Confidence scores for the VGG-16 model on unseen
test set of COVID-19 for the two binary classification tasks of
Normal vs. COVID-19 (left) and Pneumonia vs. COVID-19
(right) using synthetic data without the original examples. Notice that synthetic data increase the confidence scores.

Figures 6 and 7 show improved detection performance when
the synthetic data are used as additional training set. A similar trend was observed with the ResNet-50 and DenseNet-102
models.

Detecting target class with high confidence

Normal vs COVID-19 + G1

Probability

Pneumonia vs COVID-19 + G2

1.0

1.0

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0

0.0

0.0

Probability

1.0

The output of the softmax function describes the probability (or
confidence) of the learning model that a particular sample belongs to a certain class. The softmax layer takes the raw values (logits) of the last FC layer and maps them into probability
scores by taking the exponents of each output and then normalize each number by the sum of those exponents so that all probabilities sum to one. Figure 4 shows the probability scores for the
VGG-16 model on unseen test set of COVID-19 for the two binary classification tasks of Normal vs. COVID-19 and Pneumonia vs. COVID-19 using original data only. The red dashed line
depicts the 0.5 probability threshold. Notice that Figure 4(left)
shows low confidence scores, while Figure 4(right) shows suboptimal performance for COVID-19 detection when using original training data only.
Figure 5 shows that synthetic data can be used without the
original examples. When using synthetic data as additional

0

10

20

30

Number of COVID cases

40

0

10

20

30

Number of COVID cases

40

1.0

0.0

Figure 6: Confidence scores for the VGG-16 model on unseen
test set of COVID-19 for the two binary classification tasks
of Normal vs. COVID-19 (left) and Pneumonia vs. COVID19 (right) with synthetic data as additional training set. Left:
adding 16,537 COVID-19 examples of GN C to the original
COVID-19 dataset. Right: adding 4,758 COVID-19 examples
of GP C to the original COVID-19 dataset.

6

Table 1: COVID-19 detection performance results on Normal vs. COVID-19 test set when trained on real data; real + synthetic
data; and only synthetic data (i.e. only GN C is used for positive class examples in training each model). SEN is short for Sensitivity.
Boldface numbers indicate the best performance.
Real + GN C

Real

Real + GN C + GP C

Only Synthetic

SEN (%) ↑ FN ↓ Skew↓ SEN (%) ↑ FN ↓ Skew↓ SEN (%) ↑ FN ↓ Skew↓ SEN (%) ↑ FN ↓

Model
VGG-16
ResNet-50
DenseNet-102
DenseNet-121 + BGT

19.56
32.61
26.08
36.95

37
31
34
29

91.87
91.87
91.87
91.87

54.34
41.30
28.27
45.65

21
27
33
25

0.98
0.98
0.98
0.98

63.04
43.47
34.73
52.17

17
26
30
22

0.79
0.79
0.79
0.79

50.00
10.86
8.69
21.73

23
41
42
36

Table 2: COVID-19 detection performance results on Pneumonia vs. COVID-19 test set when trained on real data; real + synthetic
data; and only synthetic data (i.e. only GP C is used for positive class examples in training each model). Boldface numbers indicate
the best performance.
Real + GP C

Real
VGG-16
ResNet-50
DenseNet-102
DenseNet-121 + BGT

42
36
44
31

22.9
22.9
22.9
22.9

29.50
36.95
21.74
41.30

Pneumonia vs COVID-19 + G1 + G2

1.0

1.0

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0.0

0.0

0.0

Probability

Probability

8.69
21.73
4.34
32.60

Normal vs COVID-19 + G1 + G2

0

10

20

30

Number of COVID cases

40

0

10

20

30

Number of COVID cases

40

24
29
36
27

1.0

0.95
0.95
0.95
0.95

52.17
41.30
32.43
47.82

22
27
32
24

0.19
0.19
0.19
0.19

39.13
13.04
6.52
32.60

28
40
43
31

in the feature space, enabling a decision boundary between the
classes. The original examples in Figure 8(a) exhibit low interclass variation and consist of outliers. In Figure 8(b), we can see
that the synthetic examples of the GN C dataset are in a different
distribution in the feature space. While the UMAP embeddings
may not be interpreted as a justification that the synthetic examples actually consist of COVID-19 symptoms from a clinical
perspective, it is, however, important to note that the distribution of the synthetic images is significantly different than that
of normal images; thereby enabling a proper decision boundary. A similar trend can be observed in Figures 8(d), (e) and
(f). The overlapping features for Pneumonia vs. COVID-19 can
be explained by the fact that the findings of X-ray imaging in
COVID-19 are not specific, and tend to overlap with other infections such as Pneumonia in this case.

0.0

Figure 7: Confidence scores for the VGG-16 model on unseen
test set of COVID-19 for the two binary classification tasks of
Normal vs. COVID-19 (left) and Pneumonia vs. COVID-19
(right) with synthetic data as additional training set. Both GN C
and GP C are added to the original COVID-19 dataset.

4.9

Only Synthetic

SEN (%) ↑ FN ↓ Skew↓ SEN (%) ↑ FN ↓ Skew↓ SEN(%) ↑ FN ↓ Skew↓ SEN (%) ↑ FN ↓

Model

1.0

Real + GP C + GN C

Generating anonymized synthetic images
with variation

4.10

Data visualization based on dimension reduction plays an important role in data analysis and interpretation. The objective
of dimension reduction is to map high-dimensional data into
a low-dimensional space (usually 2D or 3D), while preserving
the overall structure of the data as much as possible. A commonly used dimension reduction method is the Uniform Manifold Approximation and Projection (UMAP) algorithm, which
is non-linear technique based on manifold learning and topological data analysis. UMAP is capable of preserving both local
and most of the global structure of the data when an appropriate
initialization of the embedding is used. The two-dimensional
UMAP embeddings of the features are shown in Figure 8 to visualize the difference between the original and synthetic data.
Notice that the synthetic samples are in a different distribution

Discussion

Since the generative and classification models are trained to
learn representations in the training data distribution, it is likely
that a bias might occur toward that data. In light of the class
imbalance problem, the generator is trained by under-sampling
the majority class. This under-sampling process often leaves a
relatively small number of data points (180 samples for each domain) to learn from. While a boost in performance is achieved
when using the synthetic datasets, it is not conclusive enough to
confirm whether our approach can be generalized across other
COVID-19 datasets due largely to the lack of such benchmarks.
While the improvements we have achieved using our proposed
framework are encouraging, it is important to mention that a key
objective of this work is not to claim state-of-the-art results, but
7

mance results, that as the amount of synthetic data is increased,
sensitivity improves considerably and the number of false negatives decreases. We believe that the performance can be further
improved by applying more application-specific preprocessing
and exhaustive hyper-parameter tuning, as well as by leveraging
ensemble methods, which we leave for future work.

(a)

6

(b)

Acknowledgements

This work was supported in part by and Natural Sciences and
Engineering Research Council of Canada (NSERC) Discovery
Grant Number N00929. This research was enabled in part by
advanced computing resources provided by Compute Canada.

References
(c)

[1] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, Q. Tao,
Z. Sun, and L. Xia, “Correlation of chest CT and RT-PCR
testing in coronavirus disease 2019 (COVID-19) in China:
a report of 1014 cases,” Radiology, 2020.

(d)

[2] C. Huang, Y. Wang, X. Li, L. Ren, J. Zhao, Y. Hu,
L. Zhang, G. Fan, J. Xu, X. Gu, et al., “Clinical features of
patients infected with 2019 novel coronavirus in Wuhan,
China,” The Lancet, vol. 395, no. 10223, pp. 497–506,
2020.
(e)

(f)

[3] M.-Y. Ng, E. Y. Lee, J. Yang, F. Yang, X. Li, H. Wang,
M. M.-s. Lui, C. S.-Y. Lo, B. Leung, P.-L. Khong, et al.,
“Imaging profile of the COVID-19 infection: radiologic
findings and literature review,” Radiology: Cardiothoracic
Imaging, vol. 2, no. 1, 2020.

Figure 8: Two-dimensional UMAP embeddings: (a) Normal vs.
COVID-19; (b) Normal vs. COVID-19 + GN C ; (c) Normal vs.
COVID-19 + GN C + GP C ; (d) Pneumonia vs. COVID-19; (e)
Pneumonia vs COVID-19 + GN C ; (f) Pneumonia vs. COVID19 + GN C + GP C . Here, G1 and G2 denote GN C and GP C ,
respectively

[4] M. Karim, T. Döhmen, D. Rebholz-Schuhmann,
S. Decker, M. Cochez, O. Beyan, et al., “DeepCOVIDExplainer: Explainable COVID-19 predictions based on
chest X-ray images,” arXiv preprint arXiv:2004.04582,
2020.

rather to release an open source dataset to the research community in an effort to further improve COVID-19 detection.

5

[5] L. Wang and A. Wong, “COVID-Net: A tailored deep convolutional neural network design for detection of COVID19 cases from chest radiography images,” arXiv preprint
arXiv:2003.09871, 2020.

Conclusion

In this paper, we presented an unsupervised domain adaptation
approach by leveraging class conditioning and adversarial training to build an open database of synthetic COVID-19 chest Xray images of high fidelity. This publicly available database
comprises 21,295 synthetic images of chest X-rays for COVID19 positive cases. The insights generated from applying recent
deep learning approaches on this database can be used for preventive actions against the global COVID-19 pandemic, in the
hope of containing the virus. We also demonstrated how the
data generation procedure can serve as an anonymization tool
by achieving comparable detection performance when trained
only on synthetic data versus real data in an effort to alleviate
data privacy concerns. Our experiments reveal that synthetic
data can significantly improve the COVID-19 detection perfor-

[6] S. H. Kassani, P. H. Kassasni, M. J. Wesolowski, K. A.
Schneider, and R. Deters, “Automatic detection of coronavirus disease (COVID-19) in X-ray and CT images:
A machine learning-based approach,” arXiv preprint
arXiv:2004.10641, 2020.
[7] A. Narin, C. Kaya, and Z. Pamuk, “Automatic detection
of coronavirus disease (COVID-19) using X-ray images
and deep convolutional neural networks,” arXiv preprint
arXiv:2003.10849, 2020.
[8] X. Li, C. Li, and D. Zhu, “COVID-MobileXpert: Ondevice COVID-19 screening using snapshots of chest XRay,” arXiv preprint arXiv:2004.03042, 2020.
8

[9] M. Farooq and A. Hafeez, “COVID-ResNet: A deep learning framework for screening of COVID19 from radiographs,” arXiv preprint arXiv:2003.14395, 2020.

[20] G. Yang, S. Yu, H. Dong, G. Slabaugh, P. L. Dragotti,
X. Ye, F. Liu, S. Arridge, J. Keegan, et al., “DAGAN:
Deep de-aliasing generative adversarial networks for fast
compressed sensing MRI reconstruction,” IEEE Transactions on Medical Imaging, vol. 37, no. 6, pp. 1310–1321,
2017.

[10] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E.
Shooshan, L. Rodriguez, S. Antani, G. R. Thoma, and
C. J. McDonald, “Preparing a collection of radiology examinations for distribution and retrieval,” Journal of the
American Medical Informatics Association, vol. 23, no. 2,
pp. 304–310, 2016.

[21] J. M. Wolterink, A. M. Dinkla, M. H. Savenije, P. R.
Seevinck, C. A. van den Berg, and I. Išgum, “Deep MR to
CT synthesis using unpaired data,” in Proc. International
Workshop on Simulation and Synthesis in Medical Imaging, pp. 14–23, 2017.

[11] A. E. W. Johnson, T. J. Pollard, N. R. Greenbaum, M. P.
Lungren, C. ying Deng, Y. Peng, Z. Lu, R. G. Mark, S. J.
Berkowitz, and S. Horng, “MIMIC-CXR-JPG: A large
publicly available database of labeled chest radiographs,”
arXiv preprint arXiv:1901.07042, 2019.

[22] T. Russ, S. Goerttler, A.-K. Schnurr, D. F. Bauer,
S. Hatamikia, L. R. Schad, F. G. Zöllner, and K. Chung,
“Synthesis of CT images from digital body phantoms using CycleGAN,” International Journal of Computer Assisted Radiology and Surgery, vol. 14, no. 10, pp. 1741–
1750, 2019.

[12] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus,
C. Chute, H. Marklund, B. Haghgoo, R. Ball, K. Shpanskaya, et al., “CheXpert: A large chest radiograph
dataset with uncertainty labels and expert comparison,” in
Proc. AAAI Conference on Artificial Intelligence, vol. 33,
pp. 590–597, 2019.

[23] P. Costa, A. Galdran, M. I. Meyer, M. D. Abràmoff,
M. Niemeijer, A. M. Mendonça, and A. Campilho, “Towards adversarial retinal image synthesis,” arXiv preprint
arXiv:1701.08974, 2017.

[13] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and
R. M. Summers, “ChestX-ray8: Hospital-scale chest Xray database and benchmarks on weakly-supervised classification and localization of common thorax diseases,” in
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2097–2106, 2017.

[24] S. U. Dar, M. Yurt, L. Karacan, A. Erdem, E. Erdem, and
T. Çukur, “Image synthesis in multi-contrast MRI with
conditional generative adversarial networks,” IEEE Transactions on Medical Imaging, vol. 38, no. 10, pp. 2375–
2388, 2019.

[14] A. Bustos, A. Pertusa, J.-M. Salinas, and M. de la
Iglesia-Vayá, “PadChest: A large chest X-ray image
dataset with multi-label annotated reports,” arXiv preprint
arXiv:1901.07441, 2019.

[25] H.-C. Shin, N. A. Tenenholtz, J. K. Rogers, C. G. Schwarz,
M. L. Senjem, J. L. Gunter, K. P. Andriole, and M. Michalski, “Medical image synthesis for data augmentation and
anonymization using generative adversarial networks,” in
Proc. International Workshop on Simulation and Synthesis
in Medical Imaging, pp. 1–11, 2018.

[15] J. P. Cohen, P. Morrison, and L. Dao, “COVID-19 image
data collection,” arXiv preprint arXiv:2003.11597, 2020.

[26] J. T. Guibas, T. S. Virdi, and P. S. Li, “Synthetic medical
images from dual generative adversarial networks,” arXiv
preprint arXiv:1709.01872, 2017.

[16] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired
image-to-image translation using cycle-consistent adversarial networks,” in Proc. IEEE International Conference
on Computer Vision, pp. 2223–2232, 2017.

[27] D. Korkinof, T. Rijken, M. O’Neill, J. Yearsley, H. Harvey,
and B. Glocker, “High-resolution mammogram synthesis
using progressive generative adversarial networks,” arXiv
preprint arXiv:1807.03401, 2018.

[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,
“Generative adversarial nets,” in Advances in Neural Information Processing Systems, pp. 2672–2680, 2014.
[18] S. Kazeminia, C. Baur, A. Kuijper, B. van Ginneken, N. Navab, S. Albarqouni, and A. Mukhopadhyay, “GANs for medical image analysis,” arXiv preprint
arXiv:1809.06222, 2018.

[28] B. Teixeira, V. Singh, T. Chen, K. Ma, B. Tamersoy, Y. Wu,
E. Balashova, and D. Comaniciu, “Generating synthetic Xray images of a person from the surface geometry,” in Proc.
IEEE Conference on Computer Vision and Pattern Recognition, pp. 9059–9067, 2018.

[19] A. Ben-Cohen, E. Klang, S. P. Raskin, M. M. Amitai, and
H. Greenspan, “Virtual PET images from CT data using
deep convolutional networks: initial results,” in Proc. International Workshop on Simulation and Synthesis in Medical Imaging, pp. 49–57, 2017.

[29] J. P. Cohen, M. Luck, and S. Honari, “Distribution matching losses can hallucinate features in medical image translation,” in Proc. International Conference on Medical
Image Computing and Computer-Assisted Intervention,
pp. 529–536, 2018.
9

[30] H. Zunair and A. B. Hamza, “Melanoma detection using
adversarial training and deep transfer learning,” Physics in
Medicine & Biology, vol. 65, 2020.
[31] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv
preprint arXiv:1409.1556, 2014.
[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pp. 770–778,
2016.
[33] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional networks,” in
Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4700–4708, 2017.
[34] L. A. Jeni, J. F. Cohn, and F. De La Torre, “Facing imbalanced data–recommendations for the use of performance
metrics,” in Proc. Humaine Association Conference on Affective Computing and Intelligent Interaction, pp. 245–
251, 2013.
[35] J. Brabec and L. Machlica, “Bad practices in evaluation methodology relevant to class-imbalanced problems,”
arXiv preprint arXiv:1812.01388, 2018.
[36] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter,
H. M. Blau, and S. Thrun, “Dermatologist-level classification of skin cancer with deep neural networks,” Nature,
vol. 542, no. 7639, p. 115, 2017.
[37] M. D. Zeiler, “ADADELTA: an adaptive learning rate
method,” arXiv preprint arXiv:1212.5701, 2012.
[38] M. Raghu, C. Zhang, J. Kleinberg, and S. Bengio, “Transfusion: Understanding transfer learning for medical imaging,” in Advances in Neural Information Processing Systems, pp. 3342–3352, 2019.

10

