Identifying COVID-19 Fake News in Social
Media

arXiv:2101.11954v2 [cs.CL] 1 Feb 2021

Tathagata Raha, Vijayasaradhi Indurthi, Aayush Upadhyaya, Jeevesh Kataria,
Pramud Bommakanti, Vikram Keswani, and Vasudeva Varma
Information Retrieval and Extraction Lab (iREL)
International Institute of Information Technology, Hyderabad
{tathagata.raha, vijayasaradhi.i}@research.iiit.ac.in,{aayush.upadhyaya,
jeevesh.kataria, pramud.bommakanti, vikram.keswani}@students.iiit.ac.in,
vv@iiit.ac.in

Abstract. The evolution of social media platforms have empowered everyone to access information easily. Social media users can easily share
information with the rest of the world. This may sometimes encourage
spread of fake news, which can result in undesirable consequences. In
this work, we train models which can identify health news related to
COVID-19 pandemic as real or fake. Our models achieve a high F1-score
of 98.64%. Our models achieve second place on the leaderboard, tailing
the ﬁrst position with a very narrow margin 0.05% points.
Keywords: fake-news · COVID-19 · social media.

1

Introduction

Fake news is ubiquitous and is impacting all spheres of life. The impact of fake
news is more felt especially when the fake news is related to the health of people,
specifically relating to the COVID-19 pandemic. Myths, rumours, unsolicited
tips and unverified claims and advises related to COVID-19 can sometimes lead
to loss of human life. Factually incorrect advises can sometimes create false sense
of health and might delay in getting the required medical help often aggravating
the condition. Uninformed people can easily become victims of propaganda and
has a huge impact on the society. Needless to say, identifying fake COVID health
news is very important as it can save valuable human life.
NLP has made a significant progress in recent times. Transfer learning has
been playing an important role in the areas of NLP research. With the introduction of novel architectures like Transformer, the field of NLP has been revolutionized. We use RoBERTa, a improved variation of BERT for identifying if
the COVID health news is fake or real.
In this task, at first we have used different simple baseline models like naive
bayes, linear classifier, boosting, bagging and SVM models to classify a tweet as
fake or not. For getting the tweet embeddings, we have used tf-idf and word2vec.
As our advanced models, we have experimented with different kinds of transformers models like bert, roberta, electra, etc.

2

2

Raha et al., 2021

Background

The task aims at identifying fake news related to COVID-19 in English language.
Given a social media post, we need to classify it as a fake or a real categories. The
task here was to train machine learning models which can automatically identify
posts related to COVID-19 pandemic as fake or real. These posts include posts
from various social media platforms like Twitter, Facebook and Instagram. The
task deals with these posts in English language, and specifically those posts
which are related to the COVID-19 pandemic. For this task, training data has
been provided. More details about this dataset has been given in the following
sections. Dhoju et. al [4] do a structural analysis and extract relevant features
to train models which can classify health news as fake and real. They achieve a
high F1-score of 0.96

3

Related Work

The study of fake news related to health has not received much attention. With
the COVID-19 pandemic, there has been an increased focus in identifying fake
health news. We list some of the recent related work here.
Dai et al. [2] constructed a comprehensive repository, FakeHealth, which includes news contents with rich features, news reviews with detailed explanations,
social engagements and a user to user social network. They also do exploratory
analysis to understand the characteristics of the datasets and analyse useful
patterns. Waszak et al. [16] analyze top news shared on the social media to
identify leading fake medical information miseducating the society. They curate
top health weblinks in the Polish language social media between 2012 and 2017
and provide detailed analysis.

4

System overview

In this shared task[11], we formulate the problem of identifying a social media
post as fake or not as a text classification problem. At first, we have implemented
a few simple baseline models like linear classifier and boosting models. Then, we
use the transformer architecture and fine-tune different pretrained transformer
models like Roberta, bert and electra on the COVID-19 training dataset. We do
not do explicit preprocessing because we want the model to learn the patterns
of input, like the presence of too many hashtags, too many mentions etc. to help
identify the fake news.
We use Huggingface’s transformers library [17] for finetuning the pretrained
transformer models.

5

Dataset

We use the dataset collected by Patwa et. al [12]. The dataset consisted of tweets
and posts related to COVID-19 obtained from different social-media sites like

Identifying COVID-19 Fake News in Social Media

3

Twitter, Facebook, Instagram and for each tweet there was a label corresponding
to a tweet. The labels were as follows:
1. Fake: This denotes if a post is falsely claimed or fake in nature.
Example: Politically Correct Woman (Almost) Uses Pandemic as Excuse
Not to Reuse Plastic Bag https://t.co/thF8GuNFPe #coronavirus #nashville
2. Real: This denotes a verified post or a post which is true.
Example: The CDC currently reports 99031 deaths. In general the discrepancies in death counts between different sources are small and explicable. The
death toll stands at roughly 100000 people today.
Below in Table 1. we can see the distribution of fake and real labels in training
set, validation set and test set respectively. As we can see that the dataset is
Split
#Samples #Fake #Real
Train
6420
3060
3360
Validation
2140
1020
1120
Test
2140
1021
1120
Table 1. Results on validation set for COVID-19 Fake news identiﬁcation task for
English language

fully balanced, hence there was no necessity to perform steps to make the dataset
balanced. For preprocessing the dataset, we have taken the following measures:
– Lowercasing the words
– Replacing irrevelant symbols with spaces
– Removing stopwords
Below in Table 2., we have provided more dataset statistics like the average,
maximum and minimum number of words in the posts of training, testing and
validation dataset.
Split
Average Maximum Minimum
Train
27.0
1456
3
Validation
26.8
304
3
Test
27.5
1484
4
Table 2. Dataset statistics showing the number of words in diﬀerent splits of the
dataset

6

Baseline models

We have implemented different simple baseline models on the COVID dataset.

4

Raha et al., 2021

Word embeddings: The first step is to represent each post as a vector.We
have chosen two different word embeddings for getting vector representations for
our posts and sentences: Word2Vec [10] and tf-idf [18]. For the Word2Vec,
we find embeddings for each word and take the mean of embeddings of each to
get a 300-dimension vector representation for a text.
Models: After getting the word embeddings, we performed experiments with
the six following classifiers:
– Naive Bayes: This classifier is a probabilistic classifier that uses Bayes
Theorem. On the basis of an event that has occurred previously, it calculates
the probability of the current event. [13]
– Logistic regression: Logistic regression is a statistical model that is used
to estimate the probability of a response based on predictor variables. [6]
– Bagging models (Random Forests): An ensemble of Decision Trees that
uses a tree-like model for predicting the labels. For the final output, it considers the outputs of all the decision trees that it created. [9]
– Boosting models (XGBoost): Boosting is a general ensemble method
where at first a lot of weak classifiers are created and then building a strong
classifier by building a model from the training data, then creating a second
model that attempts to correct the errors from the first model. XGBoost
is a decision-tree-based ensemble Machine Learning algorithm that uses a
gradient boosting framework. [7]
– Support Vector Machines: SVM is a non-probabilistic classifier which
constructs a set of hyperplanes in a high-dimensional space separating the
data into classes. [5]

7

Transformer models

For our more advanced models we explored different transformer models. Vaswani
et al. [15] proposed the transformer architecture. They follow the non-recurrent
architecture with stacked self-attention and fully connected layers for both the
encoder and decoder. Transformer uses concepts like self attention, multi-head
attention, positional embeddings, residual connections and masked attention. We
used the following pre-trained transformer models from HuggingFace repository
and fine-tuned it to our classification task:
– bert-base-uncased: 12-layer, 768-hidden, 12-heads, 110M parameters. The
model has been pretrained on Book Corpus and the Wikipedia data using
the Masked Language Model(MLM) and the Next Sentence Prediction(NSP)
objectives. [3]
– distilbert-base-uncased: 6-layer, 768-hidden, 12-heads, 66M parameters. It is
a smaller model than BERT which is a lot cheaper and faster to train than
BERT. [14]
– roberta-base: 12-layer, 768-hidden, 12-heads, 125M parameters.RoBERTa [8]
is a Robust BERT approach which has been trained on a much more larger
dataset and for much larger number of iterations with a larger batch size of
8k. RoBERTa also removes the NSP objective from the pretraining.

Identifying COVID-19 Fake News in Social Media

5

– google/electra-base: 12-layer, 768-hidden, 12-heads, 110M parameters. ELECTRA models are trained to distinguish ”real” input tokens vs ”fake” input
tokens generated by another neural network, similar to the discriminator of
a GAN. [1]
– xlnet-base-cased: 12-layer, 768-hidden, 12-heads, 110M parameters. It is similar to BERT but it learns bidirectional context alongwith autoregressive
formulation. [19]

8

Experimental Setup

We combine both the dev and training dataset and then split them into train
and validation in the ratio of 90:10. We train on the training split and evaluate
on the validation split.
We do not do any explicit pre-processing like removing the mentions or removing the hashtags because we want the model to learn these patterns.
We use Huggingface’s transformers library [17] for all our experiments.
The primary evaluation metric for the shared task is the F1 score. It is defined
as a is the harmonic mean of the precision and recall. An F1 score reaches its
best value at 1 and worst score at 0. In addition, we report the accuracy metric
also.

Method
Accuracy F1-score
Naive Bayes Model(tf-idf)
0.887
0.885
Linear Classiﬁer(tf-idf)
0.901
0.893
Bagging Model(tf-idf)
0.926
0.921
Boosting Model(tf-idf)
0.914
0.913
SVM Model(tf-idf)
0.941
0.941
Linear Classiﬁer(word2vec)
0.883
0.879
Bagging Model(word2vec)
0.915
0.912
Boosting Model(word2vec)
0.914
0.912
SVM Model(word2vec)
0.909
0.905
bert-base-uncased
0.962
0.960
distilbert-base-uncased
0.957
0.955
roberta-base
0.982
0.982
electra-base
0.981
0.981
xlnet-base-cased
0.948
0.944
Table 3. Results on validation set for COVID-19 Fake news identiﬁcation task for English language. The ﬁrst section denotes the baseline models on tf-idf. Te second section
denotes the baseline models on word2vec. The third section refers to the transformers
models.

6

Raha et al., 2021

Method
Accuracy F1-score
SVM Model(tf-idf)
0.939
0.938
Bagging Model(word2vec)
0.910
0.910
Boosting Model(word2vec)
0.927
0.926
roberta-base
0.9864
0.9864
electra-base
0.9827
0.9827
Table 4. Results on the oﬃcial test set for COVID-19 Fake news identiﬁcation task
for English language. The ﬁrst section denotes the baseline models on tf-idf. Te second section denotes the baseline models on word2vec. The third section refers to the
transformers models.

9

Results

Table 3 shows the results of our model on the validation dataset. We see that
the RoBERTa model gives an F1-score of 0.982 with an accuracy of 0.982 on the
validation set. Our Electra model achieves an F1-score of 0.981 and an accuracy
of 0.981 on the validation set. We submit these two models for final evaluation
on the official test set.
Table 4 shows the official results of our models on the official test set. We
see that the RoBERTa model gives an F1-score of 0.9864 with an accuracy of
0.9864 on the official test set.
Our RoBERTa model achieves 2nd position on the official leader board, 0.05
percentage points less than the best F1 score.
Our Electra model achieves an F1-score of 0.9827 with an accuracy of 0.9827
on the official test set, comparable with the top performing models on the leader
board

10

Conclusion

Identifying fake COVID-19 news is challenging and going forward it would be
useful not only to classify if a social media post is fake or not, but also to give
interpretation on why the news is fake or not. We would like to explore on the
interpretability of the models.

References
1. Clark, K., Luong, M.T., Le, Q.V., Manning, C.D.: Electra: Pre-training text
encoders as discriminators rather than generators (2020)
2. Dai, E., Sun, Y., Wang, S.: Ginger cannot cure cancer: Battling fake health news
with a comprehensive data repository. In: Proceedings of the International AAAI
Conference on Web and Social Media. vol. 14, pp. 853–862 (2020)
3. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint
arXiv:1810.04805 (2018)

Identifying COVID-19 Fake News in Social Media

7

4. Dhoju, S., Main Uddin Rony, M., Ashad Kabir, M., Hassan, N.: Diﬀerences in
health news from reliable and unreliable media. In: Companion Proceedings of
The 2019 World Wide Web Conference. pp. 981–987 (2019)
5. Hassan, S., Raﬁ, M., Shaikh, M.S.: Comparing svm and naive bayes classiﬁers for
text categorization with wikitology as knowledge enrichment. 2011 IEEE 14th
International Multitopic Conference (Dec 2011).
https://doi.org/10.1109/inmic.2011.6151495,
http://dx.doi.org/10.1109/INMIC.2011.6151495
6. Kowsari, Meimandi, J., Heidarysafa, Mendu, Barnes, Brown: Text classiﬁcation
algorithms: A survey. Information 10(4), 150 (Apr 2019).
https://doi.org/10.3390/info10040150,
http://dx.doi.org/10.3390/info10040150
7. Li, M., Xiao, P., Zhang, J.: Text classiﬁcation based on ensemble extreme
learning machine (2018)
8. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 (2019)
9. Markel, J., Bayless, A.J.: Using random forest machine learning algorithms in
binary supernovae classiﬁcation (2020)
10. Mikolov, T., Chen, K., Corrado, G., Dean, J.: Eﬃcient estimation of word
representations in vector space (2013)
11. Patwa, P., Bhardwaj, M., Guptha, V., Kumari, G., Sharma, S., PYKL, S., Das,
A., Ekbal, A., Akhtar, S., Chakraborty, T.: Overview of constraint 2021 shared
tasks: Detecting english covid-19 fake news and hindi hostile posts. In:
Proceedings of the First Workshop on Combating Online Hostile Posts in
Regional Languages during Emergency Situation (CONSTRAINT). Springer
(2021)
12. Patwa, P., Sharma, S., PYKL, S., Guptha, V., Kumari, G., Akhtar, M.S., Ekbal,
A., Das, A., Chakraborty, T.: Fighting an infodemic: Covid-19 fake news dataset.
arXiv preprint arXiv:2011.03327 (2020)
13. Raschka, S.: Naive bayes and text classiﬁcation i - introduction and theory (2017)
14. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter (2020)
15. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, L., Polosukhin, I.: Attention is all you need. In: NIPS. pp. 5998–6008
(2017)
16. Waszak, P.M., Kasprzycka-Waszak, W., Kubanek, A.: The spread of medical fake
news in social media–the pilot quantitative study. Health policy and technology
7(2), 115–118 (2018)
17. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
Rault, T., Louf, R., Funtowicz, M., et al.: Huggingface’s transformers:
State-of-the-art natural language processing. ArXiv pp. arXiv–1910 (2019)
18. Wu, H., Yuan, N.: An improved tf-idf algorithm based on word frequency
distribution information and category distribution information. In: Proceedings
of the 3rd International Conference on Intelligent Information Processing. p.
211–215. ICIIP ’18, Association for Computing Machinery, New York, NY, USA
(2018). https://doi.org/10.1145/3232116.3232152,
https://doi.org/10.1145/3232116.3232152
19. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q.V.: Xlnet:
Generalized autoregressive pretraining for language understanding (2020)

