Distributed Computing in a Pandemic: A Review of
Technologies available for Tackling COVID-19

arXiv:2010.04700v2 [cs.DC] 3 Nov 2020

A Preprint

Jamie J. Alnasir
Department of Computing
Imperial College
London, UK
j.alnasir@imperial.ac.uk

8 October, 2020

Abstract
The current COVID-19 global pandemic caused by the SARS-CoV-2 betacoronavirus has resulted
in over a million deaths and is having a grave socio-economic impact, hence there is an urgency to
find solutions to key research challenges. Some important areas of focus are: developing a vaccine,
designing or repurposing existing pharmacological agents for treatment by identifying druggable
targets, predicting and diagnosing the disease, and tracking and reducing the spread. Much of this
COVID-19 research depends on distributed computing.
In this article, I review distributed architectures — various types of clusters, grids and clouds — that
can be leveraged to perform these tasks at scale, at high-throughput, with a high degree of parallelism,
and which can also be used to work collaboratively.
High-performance computing (HPC) clusters, which aggregate their compute nodes using highbandwidth networking and support a high-degree of inter-process communication, are ubiquitous
across scientific research — they will be used to carry out much of this work. Several bigdata
processing tasks used in reducing the spread of SARS-CoV-2 require high-throughput approaches,
and a variety of tools, which Hadoop and Spark offer, even using commodity hardware.
Extremely large-scale COVID-19 research has also utilised some of the world’s fastest supercomputers,
such as IBM’s SUMMIT — for ensemble docking high-throughput screening against SARS-CoV-2
targets for drug-repurposing, and high-throughput gene analysis — and Sentinel, an XPE-Cray
based system used to explore natural products. Grid computing has facilitated the formation of the
world’s first Exascale grid computer. This has accelerated COVID-19 research in molecular dynamics
simulations of SARS-CoV-2 spike protein interactions through massively-parallel computation and
was performed with over 1 million volunteer computing devices using the Folding@home platform.
Grids and clouds both can also be used for international collaboration by enabling access to
important datasets and providing services that allow researchers to focus on research rather than on
time-consuming data-management tasks.
Keywords SARS-CoV-2 · COVID-19 · distributed · HPC · supercomputing · grid · cloud · cluster

A preprint - 8 October, 2020

1

Introduction

A novel betacoronavirus named SARS-CoV-2 (Severe Acute Respiratory Syndome coronavirus 2) is the cause of
the clinical disease COVID-19 — its spread is responsible for the current coronavirus pandemic and the resulting
global catastrophe [1]. The initial outbreak of the disease was first detected in December 2019 in Wuhan (Hubei
province, China) manifesting as cases of pneumonia, initially of unknown aetiology. On the 10th of January 2020,
Zhang et al. released the initial genome of the virus [2]. Shortly after, it was identified — by deep sequencing analysis
of lower respiratory tract samples — as a novel betacoronavirus and provisionally named 2019 novel coronavirus
(2019-nCoV) [3, 4]. By the 30th of January 2020, the WHO (World Health Organisation) declared the outbreak a
Public Health Emergency of International Concern [5], and a global pandemic on the 11th of March [6]. At the time of
writing (November 2020) there now are over 47 million reported cases of COVID-19 globally and more than 1,208,000
deaths have occurred as a result of the disease [7]. In addition to the casualties, the pandemic is also having a grave
socio-economic impact; it is a global crisis to which researchers will typically apply a variety computational techniques
and technologies to several key areas of focus [8, 9]. These include, but are not limited to, developing a vaccine,
designing or repurposing existing pharmacological agents for treatment by identifying druggable targets, predicting
and diagnosing the disease, e.g. clinical decision support, and tracking and reducing the spread [10, 11, 12]. Many
of the tasks involved can leverage a variety of distributed computing approaches which can be applied at scale, at
high-throughput, and with a high degree of parallelism — they often also need to be performed collaboratively.
The classification of SARS-CoV-2 as a betacoronavirus, and the release of its genome earlier this January, has enabled
research to focus on specific strategies. It is known from the previous 2003 SARS outbreak that ACE2 (Angiotensin
Converting Enzyme) is the main entry point the virus targets to infect its host [13, 14]. To this end, for drug repurposing
or development, COVID-19 research is focused on modelling the interaction between the coronavirus spike protein
(S-protein) and ACE2, and in understanding the structure of the S-protein as an epitope for vaccine development. Other
important targets are the virus’s proteome and the Papain-like and Main proteases – PL-pro and ML-pro, respectively [?].
Given the urgency to reduce mortality, significant efforts are being made to re-purpose medicines that are appropriate
and already approved. Whilst a WHO scientific briefing refers to this practice — off-label prescribing — in the clinical
setting, much of the initial work to predict potential drug candidates will be carried out computationally via in-silico
screening [15, 16]. Furthermore, the scale of the pandemic and the global production of bigdata, particularly whilst a
vaccine has yet to be developed, will rely on bigdata analytics to model the spread of the disease, and inform government
policy and reduce the death rate.
This review paper explores distributed and parallel computing technologies that can be applied to the research effort
to tackle COVID-19. We will discuss relevant distributed architectures and the types of tasks they are suited for, and
where examples exist, research projects that are currently implemented using them. This review will not cover GPU
computing, which although is highly-parallel, is not necessarily distributed; extensive use of GPUs in machine-learning
and deep-learning, which are highly relevant to several COVID-19 research areas, warrants review in its own right.
1.1

The Distributed Computing Model

Distributed computing is a model in which data, software and computational resources are distributed amongst networked
computers termed compute nodes. There are a variety of ways in which compute nodes can be organised in a distributed
system. In this review, we will generally not cover client-server or n-tier architectures — where the focus is on
delineating resources — except where they are used within grids or clouds, but rather we are interested in those
distributed architectures in which compute nodes are grouped-together to improve performance through concurrency. In
these systems, the collectively grouped nodes are utilised as a single resource and are referred to as a cluster [17].
Performance increases are achieved by vertical scaling — increasing the number of CPU cores in individual compute
nodes of the system — or horizontal scaling — increasing the number of compute nodes in the system. Compute nodes
in clusters communicate with each other by passing messages across the network in order to coordinate their activity to
2

A preprint - 8 October, 2020

achieve common computational goals. Distributed systems are also characterised by their lack of a global clock, and are
typically independent of the failure of individual components. In the parallel computing model, however, a global clock
is a requirement because tasks are split into sub-tasks that can be processed independently and simultaneously during
each clock-cycle. The results are then combined upon completion.
Distributed computing architectures exist in several different topologies — although there is some overlap in the scale
and focus of these architectures (depicted in Figure 1) — they can be fundamentally categorised as clusters, grids and
clouds [18], and will be covered in the next sections.

Figure 1: The relationship between different distributed computing architectures. Cluster computing (section 2) and
Supercomputing (section 2.2) focus on traditional non-service applications. Web 2.0 covers almost the whole spectrum
of service-oriented applications, where Cloud computing (section 5) lies at the large-scale side. Grid computing (section
3) overlaps with all of these fields, where it generally has a lesser scale than clouds, with the exception of Volunteering
computing Grids. The five-sided polygon depicted for Grid computing accounts for the Exa-scale Grid supercomputer
achieved using the Folding@home Grid platform (section 3.2). Based on original diagram by Foster et al. [19]

2

Cluster Computing

The compute nodes that clusters comprise are generally interconnected through high-speed, low-latency dedicated
switches and, as they typically use the TCP/IP protocol, are allocated to a subnet. Clusters may be composed of a
collection of homogenous compute nodes, which is preferable for optimal performance in Hadoop based platforms [20],
or may be a heterogeneous collection of compute nodes each having a specialist purpose, for example dedicated storage,
increased CPU cores or GPU capabilities.
Dongarra et al. point out that there is a problem in the current use of the category of cluster for classification as the term
has come to represent "all parallel computers derived through the integration of replicated components" [21]. Because
of the variety of ways in which clusters can be constructed, and taking into account their increasingly frequent assembly
using commodity hardware — Beowulf clusters — the term cluster is often used too broadly without delineating between
the various sub-categories of clusters. A distinction that has important consequences for the way in which they function
and are programmed for. They, therefore, propose a classification that is based on: the clustering topology of the nodes,
3

A preprint - 8 October, 2020

the scope of namespaces, e.g. for process IDs and resources such as ports, the dominant mode of parallelism, and
latency/locality management.
Given this important issue in appropriately classifying clusters is most probably not widely known amongst researchers,
and has not yet been resolved, for the sake of simplicity, we will refer to cluster architectures by their current commonly
used terminology.

2.1

High-performance Computing and MPI

High-performance computing (HPC) is a key enabling technology for scientific and industrial research [22]; HPC
systems are ubiquitous across scientific and academic research institutions. Most of the computational research projects
investigating the structure, function and genome of SARS-CoV-2 will be performed on HPC. This will be predominantly
in-house, but in some cases will be via access to external HPC resources, e.g. in collaboration between institutions. HPC
clusters aggregate their compute nodes using high-bandwidth networking interconnects, such as Infiniband, that allow
them to communicate at high speed to process and store high-throughput data [23]. HPC systems are therefore suited to
solve large and computationally complex problems, such as those involved in docking simulations and mathematical
modelling, that are too large for a standalone workstation, or which would take too long to be feasible. To achieve this,
the compute nodes in HPC clusters are often reliant on a high degree of inter-process communication, for instance,
Message Passing Interface (MPI), in which software that exploits this architecture is implemented.
Shipman et al review the scalability of Infiniband in two different MPI implementations that support it - MVAPICH
which is the most widely used MPI distribution for Infiniband, and OpenMPI. They found that in using OpenMPI,
latency in the communication of small messages was improved by up to 10% in medium/large jobs and that as much as
300% decrease in memory usage can be achieved [24].
2.1.1

Job Submission and Workload Management on HPC

On HPC systems, computational jobs are typically submitted to a job-scheduler. The job-scheduler is cluster management
software that monitors the cluster load and resources, manages the workload of computational jobs, and distributes
jobs across the cluster allocating resources to them accordingly [25]. The process is also known as batch-scheduled
computing, and clusters employing job-schedulers are often referred to as batch clusters (see Figure 2) because jobs are
submitted in a batch to the job scheduler which adds them to a queue — the type of queue is chosen by the user and a
default queue is used if one is not specified. Different types of queue are defined in the job-scheduler’s configuration.
Separate queues are defined for similar job types, for specific purposes, for example: to define the priority of job
execution, to offer different levels of CPU resources and physical RAM, to set a maximum number of jobs per user per
queue, to predetermine the maximum duration of job execution, and to allow jobs access to specialised compute node
resources, such as GPUs. It is, therefore, common to see job queues such as short, medium, long — which refer to the
maximum wall-time (total execution time), bigmem — for jobs requiring large amounts of memory — and GPU.
As jobs are often submitted in batches, they can include various dependent jobs (which must be executed first), and can
also be aggregated into a master job with multiple jobs of the same type each operating on a different input, i.e. as in an
Array Job. Computational jobs can either be normal user programs that run on compute nodes — as if they were run on
a single machine, but with multiple instances running across multiple compute nodes — or more typically, particularly
on HPC, especially written distributed programs. These comprise components that run on multiple compute nodes
which communicate by passing messages, for example using Message Passing Interface (MPI).
Common job-schedulers on HPC systems include LSF, Slurm, PBS/Torque, SGE, and LoadLeveler. YARN is the
job-scheduler and resource manage for the Apache Hadoop platform (and is often also used with Apache Spark).
Job-schedulers provide commands usually remotely accessed via a terminal (although they are sometimes accessed via a
web-interface) to perform common user and job related tasks that are typical to batch-scheduled computing, for example:
4

A preprint - 8 October, 2020

• User tasks: job submission, job deletion, retrieve job status, hold/pause job, list queues, list nodes and their
status, cluster status and load.
• Job tasks, specifying: the queue, CPU requirement, CPU/task, task/node, Wall time limit, Std I/O and error
files, inherit environment, email report on job completion, memory requirements, job dependencies, job arrays,
account to charge.

Figure 2: Generalised architecture of a HPC batch cluster. Users submit jobs to a batch-server that typically has a
job-scheduler as the main component. The job scheduler maintains a set of queues in which jobs are placed. Jobs are
executed according to a scheduling policy and the load (CPU and RAM utilisation) on the target slave compute node.
The jobs are “sent” to run by the remote executor, that is a process communicates with the slave node to ensure the job
is executed on that machine, typically via a daemon. NB: It’s also possible for the master compute node, on which
the batch-server runs, to run jobs. Frequently though in practice the master node is set to provide fewer resources for
running such jobs or is disabled from doing so to prevent the master node, which co-ordinates activity on the cluster,
from being overloaded and to allow it to be more responsive. Daemons running on each of the nodes constantly monitor
the load on their node and communicate this on interrogation by the batch-server.

2.1.2

Ensemble Docking

Molecular docking — in-silico simulation of the electrostatic interactions between a ligand and its target — is used to
score ligands according to their affinity to the target, and is a key task in identifying potential pharmacological agents
to target SARS-CoV-2 [26, 27]. The complex computational process is extensively used in drug development and
repurposing and is often time-consuming and expensive [28, 29]. The protein and enzyme targets that are docked against
are not static, but are constantly moving in ways which are dependent on several factors such as temperature, electrostatic
attractions and repulsions with nearby molecules, solvation (interaction with the solvent environment) etc. These factors
cause atoms in the molecules, within the constraints of the types of bonds the bind them, to adopt spatial arrangements
— termed conformations — that correspond to local energy minima on the energy surface. Molecular Dynamics (MD)
uses in-silico computation to simulate this process, the outcome of which is typically clusters ("ensembles") of the most
probable conformations for docking, i.e. ensemble docking [30].
AutoDock Vina is a widely-used software tool for performing both molecular docking and virtual screening. It has
been optimised to be two orders of magnitude faster than its previous versions by implementing parallelism using
multithreading on multicore machines. [31]. However, as discussed in the Shipman et al. review mentioned above,
5

A preprint - 8 October, 2020

further gains can be made by developing or re-implementing existing code for fine-grained parallelism offered by
MPI, and at the same time, leveraging the scale at which HPC systems can operate. Given the urgency of providing
solutions to the current COVID-19 pandemic, and where high-throughput is required, performance gains can be made
by researchers and developers implementing software in ways which exploit these features and capacity for parallelism
at multiple levels.
In previous work, Zhang et al. have further modified the AutoDock Vina source to implement a mixed MPI and
multi-threaded parallel version called VinaLC. They have demonstrated this works efficiently at a very large-scale —
15K CPUs — with an overhead of only 3.94%. Using the DUD dataset (Database of Useful Docking Decoys), they
performed 17 million flexible compound docking calculations which were completed on 15,408 CPUs within 24 h. with
70% of the targets in the DUD data set recovered using VinaLC. Projects such as this can be repurposed and applied to
identifying potential leads for binding to the SARS-CoV-2 S-protein or the S-protein:Human ACE2 interface, either
through the repurposing or the identification of putative ligands.
2.2
2.2.1

Supercomputers and COVID-19
Drug Repurposing

In recent COVID-19 focused research, Smith et al. have utilised IBM’s SUMMIT supercomputer — the world’s fastest
between November 2018 and June 2020 — to perform ensemble docking virtual high-throughput screening against both
the SARS-CoV-2 S-protein and the S-protein:Human ACE2 interface [32, 33].
SUMMIT, launched by ORNL (Oak Ridge National Laboratory) and based at it’s Oak Ridge Leadership Computing
Facility, comprises 4,608 compute nodes, each with two IBM POWER9 CPUs (containing nine cores each), and six
Nvidia Tesla Volta GPUs for a total of 9,216 CPUs and 27,648 GPUs [34]. Nodes each have 600 GB of memory,
addressable by all CPUs and GPUs, with an additional 800 GB of non-volatile RAM that can be used as a burst buffer or
as extended memory. SUMMIT implements a heterogeneous computing model — in each node the two POWER9
CPUs and Nvidia Volta GPUs are connected using Nvidia’s high-speed NVLink. The interconnect between the nodes
consist of 200 Gb/s Mellanox EDR Infiniband for both storage and inter-process messaging and supports embedded
in-network acceleration for MPI and SHMEM/PGAS.
For a source of ligands, they used the SWEETLEAD dataset which is a highly-curated in-silico database of 9,127
chemical structures representing approved drugs, chemical isolates from traditional medicinal herbs, and regulated
chemicals, including their stereoisomers[35]. The work involved three phases of computation: structural modelling,
molecular dynamics simulations (ensemble building), and in-silico docking. Since the 3D structure of the SARS-CoV-2
S-protein was not yet available during the initial phase of this research, the first phase (structural modelling) was
required and a 3D model was built with SWISSMODEL [36] using the sequences for the COVID-19 S-protein (NCBI
Ref. Seq: YP_009724390.1) and the crystal structure of SARS-CoV S-protein as a template to generate the model
of the SARS-CoV-2 S-protein:ACE2 complex. In the second phase, molecular dynamics simulations were carried
out using GROMACS (compiled on ORNL SUMMIT and run with CHARMM36 force-field [37, 38]) to generate an
ensemble of highest probability, lowest energy conformations of the complex which were selected via clustering of the
conformations. In the final in-silico docking phase, AutoDock Vina was run in parallel using an MPI wrapper.
This work has identified 47 hits for the S-protein:ACE2 interface, with 21 of these having US FDA regulatory approval
and 30 hits for the S-protein alone, with 3 of the top hits having regulatory approval.
2.2.2

High-throughput and Gene Analysis

Another research project by Garvin et al., that has also been undertaken using SUMMIT, focused on the role of
bradykinin and the RAAS (Renin Angiotensin Aldosterone System) in severe, life-threatining COVID-19 symptoms by
analysing 40,000 genes using sequencing data from 17,000 bronchoalveolar lavage (BAL) fluid samples [39, 40]. RAAS
6

A preprint - 8 October, 2020

regulates blood pressure and fluid volume through the hormones renin, angiotensin and aldosterone. Key enzymes in
this system are ACE (Angiotensin Converting Enzyme), and ACE2 which work in antagonistic ways to maintain the
levels of bradykinin, a nine-amino acid peptide that regulates the permeability of the veins and arterioles in the vascular
system. Bradykinin induces hypotension (lowering of blood pressure) by stimulating the dilation of aerterioles and the
constriction of veins, resulting in leakage of fluid into capillary beds. It has been hypothesised that dysregulated of
bradykinin signaling is responsible for the respiratory complications seen in COVID-19 — the bradykinin storm [41].
Their analysis appears to confirm dysregulation of RAAS, as they found decreased expression of ACE together with
increased expression of ACE2, renin, angeiotensin, key RAAS receptors, and both bradykinin receptors. They also
observed increased expression of kininogen and a number of kallikrein enzymes that are kininogen activating — the
activated form, kinins, are polypeptides that are involved in vasodilation, inflammatory regulation, and blood coagulation.
As they point out, atypical expression levels for genes encoding these enzymes and hormones are predicted to elevate
bradykinin levels in multiple tissues and organ systems, and explain many of the symptoms observed in COVID-19.

2.2.3

Exploring Natural Products for Treatment

So far, we have discussed some examples of the use of in-silico docking and screening that have utilised HPC to identify
existing medicines that could potentially be re-purposed for treating COVID-19. However, another strategy — one that
is used to develop new therapeutics — explores the chemistry of natural products, i.e. chemical compounds produced by
living organisms. To this end, in another research project that also performs in-silico docking and screening using a
supercomputer, Sentinel, Baudry et al. focus on natural products [42]. They point out that, natural products, owing to
the long periods of natural selection they are subjected to, perform highly selective functions. Their work, therefore,
aims to identify pharmacophores (spatial arrangement of chemical functional groups that interact with a specific receptor
or target molecular structure) that can be used to develop rationally designed novel medicines to treat COVID-19. In
addition to simulating the interaction with the S-protein RBD, they also included those with the SARS-2 proteome (the
sum of the protein products transcribed from its genome), specifically the main protease and the papain-like protease
enzymes. These are also important targets as they are highly conserved in viruses and are part of the replication
apparatus.
Sentinel, the cluster used for this research, is a Cray XC50, a 48-node, single-cabinet supercomputer featuring a
massively parallel multiprocesser architecture and is based in the Microsoft Azure public cloud data centre. It has
1,920 physical Intel Skylake cores operating at 2.4GHz with Hyperthreading (HT) / Simultaneous Multi-Tasking (SMT)
enabled, therefore providing 3,840 hyperthreaded CPU cores. Each node has 192 GB RAM and they are connected by
an Aries interconnect28 in a Dragonfly topology. A Cray HPE ClusterStor-based parallel file system is used, providing
612 TB of shared storage that is mounted on every node.
The development of medicines from natural products is challenging for several reasons: supply of the plant and marine
organisms, seasonal variation in the organism, extinction of organism sources, and natural products often occur as
mixtures of structurally related compounds, even after fractionation, only some of which are active. Contamination,
stability, solubility of the compounds, culturing source microorganisms, and cases where synergistic activities require
two constituents to be present to display full activity can also present difficulties [43]. Baudry et al., therefore, performed
their screening using a curated database of 423,706 natural products, COCONUT (COlleCtion of Open NatUral
producTs) [44]. COCONUT has been compiled from 117 existing natural product databases for which citations in
literature since 2000 exist.
Using molecular dynamics simulation coordinate files for the structures of the S-protein, main protease and the
papain-like protease enzymes — generated with GROMACS [38] and made available by Oak Ridge National Laboratory
— they generated an ensemble using the ten most populated confirmation for each, for in-silico docking. As AutoDock
was used, its codebase was compiled optimised for Sentinel, with some optimisations for Skylake CPU and memory set
in the Makefile compiler options.
7

A preprint - 8 October, 2020

They performed pharmacophore analysis of the top 500 unique natural product conformations for each target (S-protein,
PL-pro, M-pro). Filtering was applied to the list of putative ligands such that there were no duplicate instances of the
same compound, they only occurred in the set for a single target, and were deemed to be drug-like using the MOE
(Molecular Operating Environment) descriptor [45] from the COCONUT dataset. This resulted in 232, 204, and 164
compounds for the S-protein, PL-pro, M-pro, respectively. Of these, the top 100 natural products were superimposed
onto their respective predicted binding locations on their binding proteins and those that correctly bind to the correct
region (i.e. active site) were subjected to for pharmacophoric analysis. For the S-protein, two clusters of 24 and 73
compounds were found to bind to either side of a loop that interacts with ACE2. For PL-pro, the papain-like protease,
again two clusters of 40 and 60 compounds were found to bind to either side of a beta-sheet. Finally, for ML-pro, the
main protease, five clusters of binding compounds were found, one cluster in in close proximity to the proteases catalytic
site.
The common pharmacophores partaking in these interactions were assessed from the relevant clusters, resulting in a
greater understanding of the structure-activity relationship of compounds likely to be inhibitory to the SARS-CoV-2
S-protein, PL-pro, and ML-pro proteases. As a result, several natural product leads have been suggested which could
undergo further testing and development, and the pharmacophore knowledge could be used to refine existing leads and
guide rational drug design for medicines to treat COVID-19.
2.3

Hadoop and Spark

The Apache Hadoop project is an open-source software “ecosystem”, that is a collection of interrelated, interacting
projects forming a common technological platform and software framework for processing bigdata [46, 47]. Bigdata —
which we will discuss in more detail with respect to the COVID-19 pandemic — is characterised as data possessing large
volume, velocity, variety, value and veracity — known as the v’s of bigdata [48, 49]. As Hadoop has been developed for
bigdata processing and distributed storage it is therefore typically installed on a Linux compute cluster, notwithstanding
that it can be installed on a single, standalone machine, usually only for the purposes of study or prototyping.
2.3.1

Fault-tolerance and Data-locality

Central to the Hadoop platform are both fault-tolerance and data-locality [50]. Fault-tolerance provides built-in
redundancy for mitigating against the increased probability of failures that can occur as clusters are scaled-up to add
more machines and disks. Data-locality reduces network traffic during processing thereby avoiding network bottlenecks
[51]. This is especially important and means Hadoop clusters can be built using commodity hardware and off-the-shelf
networking. To achieve data-locality, the framework uses a distributed file system HDFS [52], which not only provides
fault-tolerance, but also facilitates the execution of code on the same node, or at least the same rack of the cluster, as
where the input data resides. Cluster management, in particular resource allocation and job scheduling, is handled by
Hadoop’s resource scheduler YARN (Yet Another Resource Negotiator) [53].
Hadoop programs are written using the MapReduce formalism, typically implemented in Java, which allows programmatic
access to the distributed data stored on HDFS. Data is stored and processed (i.e. passed between functions) as sets of
key-value pairs (i.e. tuples) on which Map and Reduce operations are carried out [54]. The sets of key-value pairs are
known as splits — automatically partitioned chunks of data — that enable Hadoop to distribute functions of programs
(i.e. map and reduce) to the nodes ensuring parallel execution. Each split is processed by an instance of a distributed
function across one or more nodes. Even if a Hadoop MapReduce job is running on a single node, multiple instances of
the function will be operating on the split data in a single container — a container, therefore, is a bundle of CPU and
memory resources allocated by the scheduler YARN, according to the number of map and reduce instances to be run
and the cluster configuration (where overall available resources are declared).
Hadoop provides high-throughput data processing and previously achieved the record of sorting 100 TB of data at a rate
of 0.578 TB/minute in the 2009 Gray sort competition [55]. This benchmark was performed using Yahoo’s Hammer
8

A preprint - 8 October, 2020

cluster, which at the time rather modestly comprised 3,452 nodes, each with 2x quad-core Xeon CPUs (2.5 GHz), 8 GB
of RAM and 4x SATA hard disks. Each node was connected using 1 gigabit Ethernet.
Apache Spark is a distributed computing framework which can be used standalone or can utilise the Hadoop platform’s
distributed file system (HDFS), and a resource scheduler — typically Apache YARN. Spark, therefore, can be configured
to use the fault-tolerance and data-locality inherent in Hadoop [56].

2.3.2

Spark

Spark, which can run MapReduce programs (written in Python, Java, or Scala), has been designed to overcome the
constraints of Hadoop’s acyclic data flow model. The introduction of a distributed data structure — the Resilient
Distributed Dataset (RDD) — facilitates the re-usability of intermediate data between operations. The RDD is an
immutable (read-only) abstraction for distributed memory that can encapsulate objects from the different programming
languages available on Spark, and provides fault-tolerant, in-memory computation across large clusters. As an RDD
is partitioned across multiple compute nodes and cached in their memory, this offers significant performance gains
over Hadoop and, if for some reason a partition is lost, it can be rebuilt [57]. Spark further increases performance
by optimising the execution of jobs, constructing an execution plan which minimises the linearity of execution that
occurs with Hadoop MapReduce jobs. The execution plan — essentially a directed acyclic graph (DAG) — is built from
the dependencies of job processes and contains the MapReduce operations to be performed on the job’s RDDs. To
make this possible, processes in Spark programs are designated as either action events, for instance, a reduce step, and
others as transformations, for instance, a map step. Spark’s lazy evaluation of the execution plan ensures that only
action events load data into memory, and transformations are only executed when an action has a dependency on that
transformation [58]. This enables cluster resources to be acquired and released on an ad-hoc basis throughout a complex
job and therefore offers improved cluster utilisation as compared to Hadoop.
Hadoop and Spark have also been employed for application to bioinformatics — such as in processing Next-generation
sequencing data, e.g. SNP genotyping, de novo assembly, read alignment, reviewed in [59] and structural biology, e.g.
in-silico molecular docking, structural alignment / clustering of protein-ligand complexes, and protein analysis reviewed
in [50].

2.4

COVID-19 Bigdata Analytics

As briefly mentioned earlier, bigdata refers to data sets that, by virtue of their massive size or complexity, cannot
be processed or analysed by traditional data-processing methods, and, therefore, usually require the application of
distributed, high-throughput computing. Bigdata analytics — the collection of computational methods that are applied
for gaining valuable insight from bigdata — employs highly specialised platforms and software frameworks, such as
Hadoop or Spark. In a paper that focused on AI for bigdata analytics in infectious diseases, which was written over a
year before the current COVID-19 pandemic, Wong et al. point out that, in our current technological age, a variety
of sources of epidemiological transmission data exist, such as sentinel reporting systems, disease centres, genome
databases, transport systems, social media data, outbreak reports, and vaccinology related data [60]. In the absence of a
vaccine, compounded by the difficulty of scaling national testing efforts, this data is crucial for contact tracing, and for
building models to understand and predict the spread of the disease [61].
Furthermore, given the current COVID-19 pandemic has rapidly reached a global scale, the amount of data produced
and the variety of sources is even greater than before. Such data is, in most cases, semi-structured or unstructured and,
therefore, requires pre-processing [62]. The size and rate in which this data is being produced during this pandemic,
particularly in light of the urgency, necessitates bigdata analytics to realise the potential it has to aid in finding solutions
to arrest the spread of the disease by, for example, breaking the chain of transmission (i.e. via track-and-trace systems),
and informing government policy [63].
9

A preprint - 8 October, 2020

The Apache Hadoop ecosystem has a several projects ideally suited to processing COVID-19 big data, and by virtue
of them all utilising Hadoop’s cluster infrastructure and distributed file system, they gain from the scalability and
fault-tolerance inherent in the framework, as discussed earlier. For example, for pre-processing bigdata — often referred
to as cleaning dirty data — Pig is a high-level data-flow language that can compile scripts into sequences of MapReduce
steps for execution on Hadoop [64]. Apache Spark, owing to its in-memory caching and execution optimisations
discussed earlier, offers at least two orders of magnitude faster execution than Hadoop alone and, though centred around
MapReduce programming, is less constrained to it. Hive [65] is a data-warehousing framework which has an SQL type
query language, HBase [66] a distributed scalable database, and Mahout [67] can be used for machine-learning and
clustering of data.

3

Grid Computing

A grid uses the internet as a medium for pooling computing resources which are interconnected in a mesh network
topology, whereby each compute node co-operates in distributing network data, and for which there is no centralised
point of control. The collection of compute nodes in a grid is heterogeneous and geographically dispersed. Foster et al.
have produced a check-list of what constitutes a grid, and point out that in order to broker resources, grids use standard,
open, discoverable protocols and interfaces to facilitate dynamic resource-sharing with interested parties [19].
3.1

Large-scale Parallel-processing Using Grids

The grid architecture allows for massive parallel computing capacity by the horizontal scaling of heterogeneous
compute nodes, and the exploitation of underutilised resources through methods such as idle CPU-cycle scavenging [68].
Distributed, parallel processing using grids is ideally suited for batch tasks that can be executed remotely without any
significant overhead. In order for software to scale in this way, it must be implemented (or re-factored) so as to minimise
several limitations to scalability, such as: how many sub-tasks the job can be divided into, job inter-dependencies,
contention to a single shared resource (e.g. single file, database, etc), and reduce operations which can create latencies,
such as in: inter-process communication, I/O access to devices, network bandwidth, and synchronisation [69]. An
interesting paradigm of grid computing that leverages this scalability, particularly for applications in scientific computing,
is known as volunteering distributed computing and had evolved during the growth of the internet from the 2000s
onwards. This involves allocating work to volunteering users on the internet (commonly referred to as the @home
projects) with tasks typically executed while the user’s machine is idle [70].
3.2

The World’s First Exascale Computer Assembled Using Grid Volunteer Computing

A recent project, that focused on simulating the conformations adopted by the SARS-CoV-2 S-protein, culminated in the
creation of the first Exascale grid computer. This was achieved by enabling over a million citizen scientists to volunteer
their computers to the Folding@home grid computing platform, which was first founded in 2000 to understand protein
dynamics in function and dysfunction [71, 72]. The accomplishment of surmounting the Exascale barrier by this work
is based on a conservative estimate that the peak performance of 1.01 exaFLOPS on the Folding@home platform was
achieved at a point when 280,000 GPUs and 4.8 million CPU cores were performing simulations. The estimate counts
the number of GPUs and CPUs that participated during a three-day window, and makes the conservative assumption
about the computational performance of each device. Namely, that each GPU/CPU participating has worse performance
than a card released before 2015.
In addition to understanding how the structure of the SARS-CoV-2 S-protein dictates its function, simulating the
ensemble of conformations that it adopts allows characterisation of its interactions. These interactions with the ACE2
target, the host system antibodies, as well as glycans on the virus surface, are key to understanding the behaviour of
the virus. However, as pointed out in this work, datasets generated by MD simulations typically consist of only a few
microseconds of simulation — at most millisecond timescales — for a single protein. An unprecedented scale of
10

A preprint - 8 October, 2020

resources are therefore required to perform MD simulations for all of the SARS-CoV-2 proteins. The Folding@home grid
platform has enabled this, generating a run of 0.1 s of simulation data that illuminates the movement and conformations
adopted by these proteins over a biologically relevant time-scale.

3.3

Grid Resources, Services and Middleware

Not only can processing be distributed over a grid, at scale, but so can storage. In the same manner in which distributed
file systems are mounted on cluster systems (i.e. Hadoop’s HDFS), the use of replicated storage on grids allows for both
data-locality — allowing computational tasks that are to be executed access to the data locally — and fault-tolerance —
through redundancy, by providing distributed backup copies of data. Although many grid implementations will make
use of these features, those grids in which the provision of storage and distributed databases is the main purpose are
known as data-grids.
Furthermore, middleware — software providing services and capabilities to applications above the operating system
such as APIs, authentication, and data management — coupled with resources that can be contributed by multiple
organisations in the grid, enables a grid to provide a variety of resources in addition to distributed processing and
storage. For example, software, data services (i.e. through restful APIs), licences, and access to specialist equipment,
which, as mentioned above, are brokered by open and discoverable protocols; Globus Toolkit, gLite, and UNICORE are
major implementations of grid middleware [69]. DisoveryNet was a previous middleware Toolkit, a pilot designed and
developed at Imperial College London and funded by the UK e-Science Programme. An early development, it provided
computational workflow services, such as an XML based workflow language and the ability to couple workflow process
to datasources, as part of an e-Science platform to facilitate the extraction of knowledge from data (KDD) [73]. During
the previous 2002-4 SARS-CoV-1 outbreak, DisoveryNet enabled a collaboration between its team and researchers
from SCBIT (Shanghai Centre for Bioinformation Technology) to analyse the evolution of virus strains from individuals
of different countries [74].

3.4

International Collaboration Through Grids

The features and services that grids provide make them ideal for hosting large-scale international collaboration.
Some notable large-scale grids participating in COVID-19 research are: the World Community Grid launched by
IBM [75], the WLCG (Worldwide LHC Computing Grid) at CERN [76], Berkeley Open Infrastructure for Network
Computing (BOINC) [77], the European Grid Infrastructure (EGI) [78], the Open Science Grid (OSG) [79], and Globus.
Interestingly, grids can be constructed from other grids — for example, BOINC is part of IBM WCG, and CERN’s
WLCG is based on two main grids, the EGI and OSG, which is based in the US.
Previously, in discussing middleware the Globus Toolkit was briefly mentioned — it has become a de facto standard
software for grids, and was produced by the Global Alliance, an international consortium established in 2003 to
advance the open-source grid software. The work of Globus Alliance has been pioneering in supporting international
collaboration in science and industry. The alliance emerged out of the Globus project which was founded as early as
1995 by U.S. Argonne National Laboratory, the University of Southern California’s Information Sciences Institute (ISI)
and the University of Chicago (UofC). The toolkit, which was designed to facilitate global sharing of computational
resources, databases and software tools securely across corporate and institutions, has been deployed for scientific
and industrial applications. Whilst development of the Globus Toolkit ended in 2018 due to a lack of funding, the
service remains under a freemium model, and Globus’s work in enabling worldwide collaboration continue through their
current platform which employs cloud computing to provide services. Globus’s focus is now on providing services for
large-scale reliable and secure file-sharing between researchers, data-management and a curated repository of scientific
datasets, all of which will accelerate COVID-19 research — we shall discuss this when we cover cloud computing.
11

A preprint - 8 October, 2020

3.5

COVID-19 Research on Genomics England Grid

Genomics England, founded in 2014 by the UK government and owned by the Department of Health & Social Care, has
been tasked with delivering the 100,000 genomes project which aims to study the genomes of patients with cancer
or rare diseases [80, 81]. It was conceived at a time when several government and research institutions worldwide
announced large-scale sequencing projects — akin to an arms race of sequencing for patient-centric precision medicine
research. In establishing the project, the UK government and Illumina decided to secure sequencing services for the
project from Illumina [82]. Sequencing of the 100,000 genomes has resulted in 21 PB of data and involved 70,000 UK
patients and family members, 13 genomic medicines centres across 85 recruiting NHS trusts, 1,500 NHS staff, and
2,500 researchers and trainees globally [83].
In 2018, after sequencing of the 100,000 genomes was completed, the UK government announced the significant
expansion of the project — to sequence up to five million genomes over five years [84]. At the time, the Network
Attached Storage (NAS) held 21 PB of data and had reached its node-scaling limit and so a solution that could scale to
hundreds of Petabytes was needed — after consultation with Nephos Technologies, a more scalable storage system
comprising a high-performance parallel file system from WekaIO, Mellanox® high-speed networking, and Quantum
ActiveScale object storage was implemented [85]. Genomics England’s Helix cluster, recently commissioned in 2020,
has 60 compute nodes each with 36 cores (providing 2,160 cores) and approximately 768 GB RAM. It has a dedicated
GPU node with 2x nVidia Tesla V100 GPUs installed [?].
GenOMICC (Genetics Of Mortality In Critical Care) is a collaborative project, first established in 2016, to understand
and treat critical illness such as sepsis and emerging infections (e.g. SARS/MERS/Flu) is now also focusing on
the COVID-19 pandemic. The collaboration involves Genomics England, ISARIC (The International Severe Acute
Respiratory and Emerging Infection Consortium), InFACT (The International Federation of Acute Care Triallists),
Asia-Pacific Extra-Corporeal Life Support Organisation (AP ELSO) and the Intensive Care Society. The aim is to
recruit 15,000 participants for genome sequencing, who have experienced only mild symptoms, i.e. who have tested
positive for COVID-19, but have not been hospitalised. The rationale is that in addition to co-morbidities, there are
genetic factors that determine whether a patient will suffer mild or severe, potentially life-threatenting illness — this
would also explain why some young people, who are fit and healthy have suffered severely and others who are old and
frail did not. Furthermore, since many people who have suffered severe illness from COVID-19 were elderly or from
ethnic minorities, the aim is to recruit participants that are from these backgrounds who suffered from mild symptoms of
COVID-19. To this end, the project will carry out GWAS (Genome Wide Association Studies) to identify associations
between genetic regions (loci) and increased susceptibility to COVID-19 [86].

3.6

Other COVID-19 Research on Grids

In work that utilises various grid resources, including EGI and OSG, together with the European Open Science Cloud
(EOSC) [87], Hassan et al. have performed an in-silico docking comparison between human COVID-19 patient antibody
(B38) and RTA-PAP fusion protein (ricin a chain-pokeweed antiviral protein) against targets (S-protein RBD, Spike
trimer, and membrane-protein) in SARS-CoV-2 [88]. RTA-PAP, plant-derived N-glycosidase ribosomal-inactivating
proteins (RIPs), is a fusion of ricin a chain — isolated from Ricinus communis — and pokeweed antiviral protein —
isolated from Phytolacca Americana, which the same researchers had demonstrated to be anti-infective against Hepatitis
B in prior work [89]. They also utilised a grid based service called WeNMR, which provides computational workflows
for NMR (Nucleic Magnetic Resonance)/SAX (Small-angle X-ray scattering) via easy-to-use web interfaces [90], and
the CoDockPP protein-protein software to perform the docking [91]. They found favourable binding affinities (low
binding energies) for the putative fusion protein RTA-PAP binding with both the SARS-CoV-2 S-protein trimer and
membrane protein, which can be further explored for development as antivirals for use against COVID-19.
12

A preprint - 8 October, 2020

Criteria

Grid

Infrastructure

Decentralised,
Centralised, across several
geographically distributed,
data centres, homogenous
heterogeneous

Provider
Network
Domains
Architecture
Access/interoperability via
Computational focus
Job processing
Funding

Cloud

Research institutes,
universities
Institutional, internet
Multiple
Distributed, federated
Grid middleware
i.e. Open Grid Forum
Standards
Large-scale computation,
storage
Batch
Institutional (local,
national, international)

Large individual company
(service provider)
Internet
Single
Client-server
Standard protocols
(HTTP(S), SOAP, REST API)
On-demand
Interactive and batch
Provider purchases infrastructure,
end-user leases

Table 1: Comparison of general characteristics of grid and cloud computing

4

Grid vs Cloud

Both grids and clouds offer a platform for distributed computing that can be accessed via the internet. However, there
are several key differences in their attributes, for example, in the way their infrastructures are geographically distributed,
the networks they primarily use, their computational architectures, interoperability, computational focus and in who
typically provides and funds them. Table 1 provides a comparison of the general characteristics of grids and clouds.

5

Cloud Computing

A cloud provides access to computing resources via the internet through a service provider and with minimal human
interaction between the user and service provider. Resources are accessed on-demand, generically as a service, without
regard for physical location or specific low-level hardware and in some cases without software configuration [92]. This
has been made possible by the developments in virtualisation technologies such as Xen, and Windows Azure Hypervisor
(WAH) [93, 94]. Services are purchased on-demand in a metered fashion, often to augment local resources and aid in
completion of large or time-critical computing tasks.
Mell et al. at NIST (National Institute of Standards and Technology) have provided a detailed definition of a cloud,
which is based on the following characteristic properties:
• On-demand self-service: computational resources can be provisioned by the user automatically without
requiring human interaction with the service provider.
• Broad network access: resources are accessible over the network, e.g. internet, using standard methods that
allow heterogeneity clients.
• Resource pooling: resources offered by the service provider are pooled to serve multiple consumers. The
service provider’s infrastructure is thus multi-tenanted and physical and virtual resources are dynamically
assigned and reassigned according to consumer demand.
• Rapid elasticity: resources can be elastically provisioned and released, usually automatically. Resources
appear to the end-user as unlimited and are available for provisioning in any quantity.
13

A preprint - 8 October, 2020

• Metered service: resources are automatically controlled to optimise resource use and are measured by a
metering system that is appropriate for the resource type (e.g. storage, processing, bandwidth etc.) Resource
usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer
of the utilized service.
Furthermore, in addition to the above definition of a cloud, they also outline that various service models that can
be provided using cloud computing [95]. In the Software as a Service (SaaS) model, the service provider manages
(automatically) the hardware, network and low-level software configuration required to provide resources as services for example, web servers, and databases. In the Platform as a Service (PaaS) model programming languages, libraries,
services, and tools supported by the provider can be used to deploy user-created applications or acquired applications.
The Infrastructure as a Service (IaaS) model allows provisioning of more fundamental resources such as processing,
storage, networks and other services that allow the user, for example, to install operating systems and user-created
software and libraries.
5.1

International Collaboration through Clouds

As discussed in earlier (section 3.4), the Globus service has evolved from the Globus Alliance grid consortium’s work
on the standardisation and provision of grid services. Currently, Globus is a cloud-enabled platform that facilitates
collaborative research through the provision of services which focus primarily on data management. Globus is used
extensively by the global research community, and at the time of writing, there are over 120,000 users across more than
1,500 institutions registered and connected by more than 30,000 global endpoints. Endpoints are logical file transfer
locations (source or destination) that are registered with the service and represent a resource (e.g. a server, cluster,
storage system, laptop, etc.) between which files can be securely transferred by authorised users. Globus has enabled
the transfer of more than 800 PB of data to-date, and presently more than 500 TB are transferred daily. Some of the
services it provides are given in Table 2 below.
Feature
Identity management

Description
Authentication and authorization interactions are brokered between end-users, identity providers, resource
servers (services), and clients
File transfers
Can be performed securely, either by request or automated via script
File sharing
Allows sharing between users, groups, and setting
access permissions
Workflow automation
Automate workflow steps into pipelines
Dataset assembly
Researchers can develop and deposit datasets, and describe their attributes using domain-specific metadata.
Publication repository
Curators review, approve and publish data
Collaboration
Collaborators can access shared files via Globus login
— no local account is required — and then download
Dataset discovery
Peers and collaborators can search and discover datasets
Table 2: Some important services Globus provides

Another cloud project to enable research collaboration, currently still in development, is the European Open Science
Cloud (EOSC), which was proposed in 2016 by the European Commission with a vision to enable Open Science [87]. It
aims to provide seamless cloud-services for storage, data management and analysis and facilitate re-use of research data
by federating existing scientific infrastructures dispersed across EU member states. After an extensive consultation
period with scientific and institutional stakeholders, the outcome is a road-map of project milestones, published in 2018,
these are: i) Architecture, ii) Data, iii) Services, iv) Access and Interface, and v) Rules and Governance — and are
anticipated to be completed by 2021.

14

A preprint - 8 October, 2020

Ref. / Name

Platform

Scale

[32] Smith et al.

IBM Summit
supercomputer

up to 4,608 nodes,
9,216 CPUs,
27,648 GPUs

Research task

Tools

Outcome

in-silico ensemble docking &
screening of existing medicines
for repurposing

GROMACS,
CHARMM32,
AutoDock Vina

Identified 47 hits for the S-protein:ACE2
interface, with 21 of these having US
FDA regulatory approval. 30 hits for the
S-protein alone, with 3 of the top hits
having regulatory approval.

[39] Garvin et al.

IBM Summit
supercomputer

up to 4,608 nodes,
9,216 CPUs,
27,648 GPUs

large-scale gene analysis

AutoDock

Observed atypical expression levels for
genes in RAAS pointing to bradykinin
dysregulation and storm hypothesis.

[42] Baudry et al.

Cray Sentinel
supercomputer

up to 48 nodes,
1,920 physical cores
3,840 HT/SMT cores

in-silico docking

AutoDock

Pharmacophore analysis of natural product compounds likely to be inhibitory
to the SARS-CoV-2 S-protein, PL-pro,
and ML-pro proteases.

Folding@home grid

4.8 million CPU cores
∼280,000 GPUs

MD simulations

GROMACS,
CHARMM36,
AMBER03

Generated an unprecedented 0.1 s of
MD simulation data.

[88] Hassan et al.

EGI, OSD grids
& EOSC cloud

unspecified

in-silico docking

weNMR,
CoDockPP

Demonstrated high in-silico binding
affinities of fusion protein RTA-PAP putative ligand with both the SARS-CoV-2
S-protein trimer and membrane protein.

[86] Pairo et al.

Genomics England
grid, Helix cluster
up to 60 nodes
(2,160 cores),
2x V100 GPUs

unspecified

GWAS

Not yet specified

Recruitment of 15,000 participants is
ongoing.

[71] Zimmerman
et al.

Table 3: Comparison of COVID-19 research exploiting large-scale distributed computing

6

Discussion and Conclusion

There are a variety of distributed architectures that can be employed to perform efficient, large-scale, and highly-parallel
computation requisite for several important areas of COVID-19 research. Some of the large-scale COVID-19 research
projects we have discussed that utilise these technologies are summarised in Table 3 — these have focused on in-silico
docking, MD simulation and gene-analysis.
High-performance computing (HPC) clusters are ubiquitous across scientific research institute and aggregate their
compute nodes using high-bandwidth networking interconnects. They use communications protocols, such as Message
Passing Interface (MPI), allowing software to achieve a high degree of inter-process communication. Hadoop and Spark
facilitate high-throughput processing suited for the bigdata tasks in COVID-19 research. Even when Hadoop/Spark
clusters are built using commodity hardware, their ecosystem of related software projects can make use of the
fault-tolerant, scalable Hadoop framework i.e. HDFS distributed file system — features that are usually found in more
expensive HPC systems. Although not widely adopted, nor a common use, Hadoop and Spark have also been employed
for applications in bioinformatics (e.g. processing sequencing data) and structural biology (e.g. performing docking,
clustering of protein-ligand conformations).
COVID-19 research has utilised some of the world’s fastest supercomputers, such as IBM’s SUMMIT — to perform
ensemble docking virtual high-throughput screening against SARS-CoV-2 targets for drug-repurposing, and highthroughput gene analysis — and Sentinel, an XPE-Cray based system used to explore natural products. Grid computing
has also come to the fore during the pandemic by enabling the formation of an Exascale grid computer allowing
massively-parallel computation to be performed through volunteer computing using the Folding@home platform.
Both grids and clouds support international collaboration, Services such as Globus provide a variety of services,
for example, reliable file transfer, workflow automation, identity management, publication repositories, and dataset
discovery, thereby allowing researchers to focus on research rather than on time-consuming data-management tasks.
15

A preprint - 8 October, 2020

7

Acknowledgements

The author wishes to thank Eszter Ábrahám for proofreading the manuscript.

8

Conflicts of Interest Statement

The author declares no conflicts of interest.

References
[1] Mary A Lake. What we know so far: Covid-19 current clinical knowledge and research. Clinical Medicine,
20(2):124, 2020.
[2] YZ Zhang. Initial genome release of novel coronavirus, 2020.
[3] Hongzhou Lu, Charles W Stratton, and Yi-Wei Tang. Outbreak of pneumonia of unknown etiology in wuhan,
china: The mystery and the miracle. Journal of medical virology, 92(4):401–402, 2020.
[4] Chaolin Huang, Yeming Wang, Xingwang Li, Lili Ren, Jianping Zhao, Yi Hu, Li Zhang, Guohui Fan, Jiuyang Xu,
Xiaoying Gu, et al. Clinical features of patients infected with 2019 novel coronavirus in wuhan, china. The lancet,
395(10223):497–506, 2020.
[5] WHO. Statement on the second meeting of the international health regulations (2005) emergency committee
regarding the outbreak of novel coronavirus (2019-ncov). 2020.
[6] World Health Organization et al. Who director-general’s opening remarks at the media briefing on covid-19-11
march 2020, 2020.
[7] Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard to track covid-19 in real
time. The Lancet infectious diseases, 20(5):533–534, 2020.
[8] Dayong Zhang, Min Hu, and Qiang Ji. Financial markets under the global pandemic of covid-19. Finance Research
Letters, page 101528, 2020.
[9] Maria Nicola, Zaid Alsafi, Catrin Sohrabi, Ahmed Kerwan, Ahmed Al-Jabir, Christos Iosifidis, Maliha Agha, and
Riaz Agha. The socio-economic implications of the coronavirus pandemic (covid-19): A review. International
journal of surgery (London, England), 78:185, 2020.
[10] Luca Ferretti, Chris Wymant, Michelle Kendall, Lele Zhao, Anel Nurtay, Lucie Abeler-Dörner, Michael Parker,
David Bonsall, and Christophe Fraser. Quantifying sars-cov-2 transmission suggests epidemic control with digital
contact tracing. Science, 368(6491), 2020.
[11] Stephen M Kissler, Christine Tedĳanto, Edward Goldstein, Yonatan H Grad, and Marc Lipsitch. Projecting the
transmission dynamics of sars-cov-2 through the postpandemic period. Science, 368(6493):860–868, 2020.
[12] Guillermo I Perez Perez and Amin Talebi Bezmin Abadi. Ongoing challenges faced in the global control of
covid-19 pandemic. Archives of Medical Research, 2020.
[13] Wenhui Li, Michael J Moore, Natalya Vasilieva, Jianhua Sui, Swee Kee Wong, Michael A Berne, Mohan
Somasundaran, John L Sullivan, Katherine Luzuriaga, Thomas C Greenough, et al. Angiotensin-converting
enzyme 2 is a functional receptor for the sars coronavirus. Nature, 426(6965):450–454, 2003.
[14] Keĳi Kuba, Yumiko Imai, Shuan Rao, Hong Gao, Feng Guo, Bin Guan, Yi Huan, Peng Yang, Yanli Zhang, Wei
Deng, et al. A crucial role of angiotensin converting enzyme 2 (ace2) in sars coronavirus–induced lung injury.
Nature medicine, 11(8):875–879, 2005.
[15] Andre C Kalil. Treating covid-19—off-label drug use, compassionate use, and randomized clinical trials during
pandemics. Jama, 323(19):1897–1898, 2020.
16

A preprint - 8 October, 2020

[16] Off-label use of medicines for COVID-19.
https://www.who.int/publications/i/item/
off-label-use-of-medicines-for-covid-19-scientific-brief, 2020.
[Online; accessed 11September-2020].
[17] George F Coulouris, Jean Dollimore, and Tim Kindberg. Distributed systems: concepts and design. pearson
education, 2005.
[18] Hameed Hussain, Saif Ur Rehman Malik, Abdul Hameed, Samee Ullah Khan, Gage Bickler, Nasro Min-Allah,
Muhammad Bilal Qureshi, Limin Zhang, Wang Yongji, Nasir Ghani, et al. A survey on resource allocation in high
performance distributed computing systems. Parallel Computing, 39(11):709–736, 2013.
[19] Ian Foster, Yong Zhao, Ioan Raicu, and Shiyong Lu. Cloud computing and grid computing 360-degree compared.
In Grid Computing Environments Workshop, 2008. GCE’08, pages 1–10. Ieee, 2008.
[20] B Thirumala Rao, NV Sridevi, V Krishna Reddy, and LSS Reddy. Performance issues of heterogeneous hadoop
clusters in cloud computing. arXiv preprint arXiv:1207.0894, 2012.
[21] Jack Dongarra, Thomas Sterling, Horst Simon, and Erich Strohmaier. High-performance computing: clusters,
constellations, mpps, and future directions. Computing in Science & Engineering, 7(2):51–59, 2005.
[22] EPSRC. An analysis of the impacts and outputs of investment in national HPC. https://epsrc.ukri.org/
newsevents/pubs/impactofnationalhpc/, 2016. [Online; accessed 07-September-2020].
[23] Mark Donald Hill, Norman Paul Jouppi, and Gurindar Sohi. Readings in computer architecture. Gulf Professional
Publishing, 2000.
[24] Galen M Shipman, Timothy S Woodall, Richard L Graham, Arthur B Maccabe, and Patrick G Bridges. Infiniband
scalability in open mpi. In Parallel and Distributed Processing Symposium, 2006. IPDPS 2006. 20th International,
pages 10–pp. IEEE, 2006.
[25] Joseph Kaplan and Michael Nelson. A Comparison of Queueing, Cluster and Distributed Computing Systems.
NASA Technical Memorandum: 109025, January 1993.
[26] Garrett M Morris and Marguerita Lim-Wilby. Molecular docking. Molecular modeling of proteins, pages 365–382,
2008.
[27] Xuan-Yu Meng, Hong-Xing Zhang, Mihaly Mezei, and Meng Cui. Molecular docking: a powerful approach for
structure-based drug discovery. Current computer-aided drug design, 7(2):146–157, 2011.
[28] Hamilton Moses, E Ray Dorsey, David HM Matheson, and Samuel O Thier. Financial anatomy of biomedical
research. Jama, 294(11):1333–1342, 2005.
[29] Michael D Rawlins. Cutting the cost of drug development? Nature reviews Drug discovery, 3(4):360–364, 2004.
[30] Rommie E Amaro, Jerome Baudry, John Chodera, Özlem Demir, J Andrew McCammon, Yinglong Miao, and
Jeremy C Smith. Ensemble docking in drug discovery. Biophysical journal, 114(10):2271–2278, 2018.
[31] Oleg Trott and Arthur J Olson. AutoDock Vina: improving the speed and accuracy of docking with a new scoring
function, efficient optimization, and multithreading. Journal of computational chemistry, 31(2):455–61, jan 2010.
[32] Micholas Smith and Jeremy C Smith. Repurposing therapeutics for covid-19: supercomputer-based docking to the
sars-cov-2 viral spike protein and viral spike protein-human ace2 interface. 2020.
[33] Kerner, S.M. IBM Unveils Summit, the World’s Fastest Supercomputer (For Now). https://www.serverwatch.
com/server-news/ibm-unveils-summit-the-worlds-faster-supercomputer-for-now.html, 2018.
[Online; accessed 07-September-2020].
[34] Sudharshan S Vazhkudai, Bronis R de Supinski, Arthur S Bland, Al Geist, James Sexton, Jim Kahle, Christopher J
Zimmer, Scott Atchley, Sarp Oral, Don E Maxwell, et al. The design, deployment, and evaluation of the coral
pre-exascale systems. In SC18: International Conference for High Performance Computing, Networking, Storage
and Analysis, pages 661–672. IEEE, 2018.
17

A preprint - 8 October, 2020

[35] Paul A Novick, Oscar F Ortiz, Jared Poelman, Amir Y Abdulhay, and Vĳay S Pande. Sweetlead: an in silico
database of approved drugs, regulated chemicals, and herbal isolates for computer-aided drug discovery. PLoS
One, 8(11):e79568, 2013.
[36] Torsten Schwede, Jurgen Kopp, Nicolas Guex, and Manuel C Peitsch. Swiss-model: an automated protein
homology-modeling server. Nucleic acids research, 31(13):3381–3385, 2003.
[37] John Ossyra, Ada Sedova, Arnold Tharrington, Frank Noé, Cecilia Clementi, and Jeremy C Smith. Porting
adaptive ensemble molecular dynamics workflows to the summit supercomputer. In International Conference on
High Performance Computing, pages 397–417. Springer, 2019.
[38] Mark James Abraham, Teemu Murtola, Roland Schulz, Szilárd Páll, Jeremy C Smith, Berk Hess, and Erik
Lindahl. Gromacs: High performance molecular simulations through multi-level parallelism from laptops to
supercomputers. SoftwareX, 1:19–25, 2015.
[39] Michael R Garvin, Christiane Alvarez, J Izaak Miller, Erica T Prates, Angelica M Walker, B Kirtley Amos, Alan E
Mast, Amy Justice, Bruce Aronow, and Daniel Jacobson. A mechanistic model and therapeutic interventions for
covid-19 involving a ras-mediated bradykinin storm. Elife, 9:e59177, 2020.
T.
IA
Supercomputer
Analyzed
Covid-19
—
and
an
Inter[40] Smith,
esting
New
Theory
Has
Emerged.
https://elemental.medium.com/
a-supercomputer-analyzed-covid-19-and-an-interesting-new-theory-has-emerged-31cb8eba9d63,
2020. [Online; accessed 09-September-2020].
[41] Joseph A Roche and Renuka Roche. A hypothesized role for dysregulated bradykinin signaling in covid-19
respiratory complications. The FASEB Journal, 2020.
[42] Kendall Byler, Joseph Landman, and Jerome Baudry. High performance computing prediction of potential natural
product inhibitors of sars-cov-2 key targets. 2020.
[43] Jesse W-H Li and John C Vederas. Drug discovery and natural products: end of an era or an endless frontier?
Science, 325(5937):161–165, 2009.
[44] Sorokina, M.; Steinbeck, C. COlleCtion of Open NatUral producTs. http://doi.org/10.5281/zenodo.
3778405, 2020. [Online; accessed 11-September-2020].
[45] Santiago Vilar, Giorgio Cozza, and Stefano Moro. Medicinal chemistry and the molecular operating environment
(moe): application of qsar and molecular docking to drug discovery. Current topics in medicinal chemistry,
8(18):1555–1572, 2008.
[46] David G Messerschmitt, Clemens Szyperski, et al. Software ecosystem: understanding an indispensable technology
and industry. MIT Press Books, 1, 2005.
[47] JV Joshua, DO Alao, SO Okolie, and O Awodele. Software ecosystem: Features, benefits and challenges. 2013.
[48] Douglas Laney. 3D data management: Controlling data volume, velocity, and variety. Technical report, META
Group, February 2001.
[49] Christine L Borgman. Big Data, little data, no data: Scholarship in the networked world. Mit Press, 2015.
[50] Jamie J Alnasir and Hugh P Shanahan. The application of hadoop in structural bioinformatics. Briefings in
bioinformatics, 21(1):96–105, 2020.
[51] Zhenhua Guo, Geoffrey Fox, and Mo Zhou. Investigation of data locality in mapreduce. In Proceedings of the
2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012), pages
419–426. IEEE Computer Society, 2012.
[52] Apache Software Foundation. HDFS architecture documentation. http://hadoop.apache.org/docs/
current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html, 2016. [Online; accessed 10-Jan-2019].
18

A preprint - 8 October, 2020

[53] Vinod Kumar Vavilapalli, Arun C Murthy, Chris Douglas, Sharad Agarwal, Mahadev Konar, Robert Evans,
Thomas Graves, Jason Lowe, Hitesh Shah, Siddharth Seth, et al. Apache hadoop yarn: Yet another resource
negotiator. In Proceedings of the 4th annual Symposium on Cloud Computing, page 5. ACM, 2013.
[54] Benjamin Fish, Jeremy Kun, Adám D Lelkes, Lev Reyzin, and György Turán. On the computational complexity of
mapreduce. In International Symposium on Distributed Computing, pages 1–15. Springer, 2015.
[55] Owen O’ Malley and Arun C Murthy. Winning a 60 second dash with a yellow elephant, 2009.
[56] James G Shanahan and Laing Dai. Large scale distributed data science using apache spark. In Proceedings of
the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2323–2324.
ACM, 2015.
[57] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J
Franklin, Scott Shenker, and Ion Stoica. Resilient distributed datasets: A fault-tolerant abstraction for in-memory
cluster computing. In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,
pages 2–2. USENIX Association, 2012.
[58] Apache Software Foundation. Spark 2.6 documentation. http://spark.apache.org/docs/latest/
programming-guide.html, 2014. [Online; accessed 26-Jan-2019].
[59] Ronald C Taylor. An overview of the hadoop/mapreduce/hbase framework and its current applications in
bioinformatics. BMC bioinformatics, 11(Suppl 12):S1, 2010.
[60] Zoie SY Wong, Jiaqi Zhou, and Qingpeng Zhang. Artificial intelligence for infectious disease big data analytics.
Infection, disease & health, 24(1):44–48, 2019.
[61] Kaiyuan Sun, Jenny Chen, and Cécile Viboud. Early epidemiological analysis of the coronavirus disease 2019
outbreak based on crowdsourced data: a population-level observational study. The Lancet Digital Health, 2020.
[62] Israel Edem Agbehadji, Bankole Osita Awuzie, Alfred Beati Ngowi, and Richard C Millham. Review of
big data analytics, artificial intelligence and nature-inspired computing models towards accurate detection of
covid-19 pandemic cases and contact tracing. International journal of environmental research and public health,
17(15):5330, 2020.
[63] Nicola Luigi Bragazzi, Haĳiang Dai, Giovanni Damiani, Masoud Behzadifar, Mariano Martini, and Jianhong Wu.
How big data and artificial intelligence can help better manage the covid-19 pandemic. International Journal of
Environmental Research and Public Health, 17(9):3176, 2020.
[64] Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. Pig latin: a
not-so-foreign language for data processing. In Proceedings of the 2008 ACM SIGMOD international conference
on Management of data, pages 1099–1110. ACM, 2008.
[65] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, Zheng Shao, Prasad Chakka, Suresh Anthony, Hao Liu, Pete
Wyckoff, and Raghotham Murthy. Hive: a warehousing solution over a map-reduce framework. Proceedings of
the VLDB Endowment, 2(2):1626–1629, 2009.
[66] Lars George. HBase: The Definitive Guide: Random Access to Your Planet-Size Data. " O’Reilly Media, Inc.",
2011.
[67] Dmitriy Lyubimov and Andrew Palumbo. Apache Mahout: Beyond MapReduce. CreateSpace Independent
Publishing Platform, 2016.
[68] Madhuri D Bhavsar and Shrikant N Pradhan. Scavenging idle cpu cycles for creation of inexpensive supercomputing
power. International Journal of Computer Theory and Engineering, 1(5):602, 2009.
[69] Luis Ferreira, Viktors Berstis, Jonathan Armstrong, Mike Kendzierski, Andreas Neukoetter, Masanobu Takagi,
Richard Bing-Wo, Adeeb Amir, Ryo Murakawa, Olegario Hernandez, et al. Introduction to grid computing with
globus. IBM redbooks, 9, 2003.
19

A preprint - 8 October, 2020

[70] Elmar Krieger and Gert Vriend. Models@ home: distributed computing in bioinformatics using a screensaver
based approach. Bioinformatics, 18(2):315–318, 2002.
[71] Maxwell I Zimmerman, Justin R Porter, Michael D Ward, Sukrit Singh, Neha Vithani, Artur Meller, Upasana L
Mallimadugula, Catherine E Kuhn, Jonathan H Borowsky, Rafal P Wiewiora, et al. Citizen scientists create an
exascale computer to combat covid-19. BioRxiv.
[72] Adam L Beberg, Daniel L Ensign, Guha Jayachandran, Siraj Khaliq, and Vĳay S Pande. Folding@ home: Lessons
from eight years of volunteer distributed computing. In Parallel & Distributed Processing, 2009. IPDPS 2009.
IEEE International Symposium on, pages 1–8. IEEE, 2009.
[73] Anthony Rowe, Dimitrios Kalaitzopoulos, Michelle Osmond, Moustafa Ghanem, and Yike Guo. The discovery
net system for high throughput bioinformatics. Bioinformatics, 19(suppl_1):i225–i231, 2003.
[74] AKTP Au, V Curcin, M Ghanem, N Giannadakis, Y Guo, MA Jafri, M Osmond, A Oleynikov, AS Rowe, J Syed,
et al. Why grid-based data mining matters? fighting natural disasters on the grid: from sars to land slides. In UK
e-science all-hands meeting (AHM 2004), Nottingham, UK, pages 121–126, 2004.
[75] IBM World Community Grid - about page.
https://www.worldcommunitygrid.org/about_us/
viewAboutUs.do, 2020. [Online; accessed 16-September-2020].
[76] A Sciaba, S Campana, M Litmaath, F Donno, JT Moscicki, N Magini, H Renshall, and J Andreeva. Computing at
the petabyte scale with the wlcg. Technical report, 2010.
[77] David P Anderson. Boinc: A system for public-resource computing and storage. In Fifth IEEE/ACM international
workshop on grid computing, pages 4–10. IEEE, 2004.
[78] Fabrizio Gagliardi. The egee european grid infrastructure project. In International Conference on High Performance
Computing for Computational Science, pages 194–203. Springer, 2004.
[79] Ruth Pordes, Don Petravick, Bill Kramer, Doug Olson, Miron Livny, Alain Roy, Paul Avery, Kent Blackburn, Torre
Wenaus, Frank Würthwein, et al. The open science grid. In Journal of Physics: Conference Series, volume 78,
page 012057. IOP Publishing, 2007.
[80] Nayanah Siva. Uk gears up to decode 100 000 genomes from nhs patients. The Lancet, 385(9963):103–104, 2015.
[81] James Gallagher, BBC. DNA project ’to make UK world genetic research leader’. http://www.bbc.co.uk/
news/health-28488313, 2014. [Online; accessed 21-January-2019].
[82] Vivien Marx. The dna of a nation. Nature, 524(7566):503–505, 2015.
[83] Genomics England. 100,000 Genomes project by numbers. https://www.genomicsengland.co.uk/
the-100000-genomes-project-by-numbers/, 2014. [Online; accessed 24-November-2019].
[84] Genomics England.
Secretary of State for Health and Social Care announces ambition
to sequence 5 million genomes within five years.
https://www.genomicsengland.co.uk/
matt-hancock-announces-5-million-genomes-within-five-years/, 2018. [Online; accessed 30October-2020].
[85] HPC
wire.
Genomics
England
Scales
Up
Genomic
Sequencing
with
Quantum
ActiveScale
Object
Storage.
https://www.hpcwire.com/off-the-wire/
genomics-england-scales-up-genomic-sequencing-with-quantum-activescale-object-storage/,
2020. [Online; accessed 22-September-2020].
[86] Erola Pairo-Castineira, Sara Clohisey, Lucĳa Klaric, Andrew Bretherick, Konrad Rawlik, Nicholas Parkinson,
Dorota Pasko, Susan Walker, Anne Richmond, Max Head Fourman, et al. Genetic mechanisms of critical illness
in covid-19. medRxiv, 2020.
20

A preprint - 8 October, 2020

[87] Paul Ayris, Jean-Yves Berthou, Rachel Bruce, Stefanie Lindstaedt, Anna Monreale, Barend Mons, Yasuhiro
Murayama, Caj Södergård, Klaus Tochtermann, and Ross Wilkinson. Realising the european open science cloud.
2016.
[88] Yasser Hassan, Sherry Ogg, and Hui Ge. Novel anti-sars-cov-2 mechanisms of fusion broad range anti-infective
protein ricin a chain mutant-pokeweed antiviral protein 1 (rtam-pap1) in silico. 2020.
[89] Yasser Hassan, Sherry Ogg, and Hui Ge. Expression of novel fusion antiviral proteins ricin a chain-pokeweed
antiviral proteins (rta-paps) in escherichia coli and their inhibition of protein synthesis and of hepatitis b virus in
vitro. BMC biotechnology, 18(1):47, 2018.
[90] Tsjerk A Wassenaar, Marc Van Dĳk, Nuno Loureiro-Ferreira, Gĳs Van Der Schot, Sjoerd J De Vries, Christophe
Schmitz, Johan Van Der Zwan, Rolf Boelens, Andrea Giachetti, Lucio Ferella, et al. Wenmr: structural biology on
the grid. Journal of Grid Computing, 10(4):743–767, 2012.
[91] Ren Kong, Feng Wang, Jian Zhang, Fengfei Wang, and Shan Chang. Codockpp: A multistage approach for global
and site-specific protein–protein docking. Journal of chemical information and modeling, 59(8):3556–3564, 2019.
[92] James E Smith and Ravi Nair. The architecture of virtual machines. Computer, 38(5):32–38, 2005.
[93] Andrew J Younge, Robert Henschel, James T Brown, Gregor Von Laszewski, Judy Qiu, and Geoffrey C Fox.
Analysis of virtualization technologies for high performance computing environments. In Cloud Computing
(CLOUD), 2011 IEEE International Conference on, pages 9–16. IEEE, 2011.
[94] Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, and
Andrew Warfield. Xen and the art of virtualization. In ACM SIGOPS operating systems review, volume 37, pages
164–177. ACM, 2003.
[95] Peter Mell, Tim Grance, et al. The nist definition of cloud computing. 2011.

21

