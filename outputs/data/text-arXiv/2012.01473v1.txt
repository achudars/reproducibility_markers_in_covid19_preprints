1

CovSegNet: A Multi Encoder-Decoder Architecture
for Improved Lesion Segmentation of COVID-19
Chest CT Scans

arXiv:2012.01473v1 [eess.IV] 2 Dec 2020

Tanvir Mahmud, Student Member, IEEE, Md Awsafur Rahman, Student Member, IEEE, Shaikh Anowarul Fattah,
Senior Member, IEEE, and Sun-Yuan Kung, Life Fellow, IEEE

Abstract—Automatic lung lesions segmentation of chest CT
scans is considered a pivotal stage towards accurate diagnosis and severity measurement of COVID-19. Traditional Ushaped encoder-decoder architecture and its variants suffer from
diminutions of contextual information in pooling/upsampling
operations with increased semantic gaps among encoded and
decoded feature maps as well as instigate vanishing gradient
problems for its sequential gradient propagation that result
in sub-optimal performance. Moreover, operating with 3D CTvolume poses further limitations due to the exponential increase
of computational complexity making the optimization difficult.
In this paper, an automated COVID-19 lesion segmentation
scheme is proposed utilizing a highly efficient neural network
architecture, namely CovSegNet, to overcome these limitations.
Additionally, a two-phase training scheme is introduced where a
deeper 2D-network is employed for generating ROI-enhanced
CT-volume followed by a shallower 3D-network for further
enhancement with more contextual information without increasing computational burden. Along with the traditional vertical
expansion of Unet, we have introduced horizontal expansion
with multi-stage encoder-decoder modules for achieving optimum performance. Additionally, multi-scale feature maps are
integrated into the scale transition process to overcome the
loss of contextual information. Moreover, a multi-scale fusion
module is introduced with a pyramid fusion scheme to reduce
the semantic gaps between subsequent encoder/decoder modules
while facilitating the parallel optimization for efficient gradient
propagation. Outstanding performances have been achieved in
three publicly available datasets that largely outperform other
state-of-the-art approaches. The proposed scheme can be easily
extended for achieving optimum segmentation performances in
a wide variety of applications.
Impact Statement—With lower sensitivity (60-70%), elongated
testing time, and a dire shortage of testing kits, traditional RTPCR based COVID-19 diagnostic scheme heavily relies on postCT based manual inspection for further investigation. Hence,
automating the process of infected lesions extraction from chestCT volumes will be major progress for faster accurate diagnosis
of COVID-19. However, in challenging conditions with diffused,
blurred, and varying shaped edges of COVID-19 lesions, conventional approaches fail to provide precise segmentation of lesions
that can be deleterious for false estimation and loss of information. The proposed scheme incorporating an efficient neural
network architecture (CovSegNet) overcomes the limitations of
traditional approaches that provide significant improvement of
performance (8.4% in averaged dice measurement scale) over two
T. Mahmud, M. A. Rahman, and S. A. Fattah are with the Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and
Technology, Dhaka-1000, Bangladesh e-mail: (tanvirmahmud@eee.buet.ac.bd,
mdawsafurrahman@ug.eee.buet.ac.bd, and fattah@eee.buet.ac.bd).
S-Y. Kung is with the Department of Electrical Engineering, Princeton
University, USA e-mail: kung@princeton.edu
This paragraph will include the Associate Editor who handled your paper.

datasets. Therefore, this scheme can be an effective, economical
tool for the physicians for faster infection analysis to greatly
reduce the spread and massive death toll of this deadly virus
through mass-screening.
Index Terms—Artificial intelligence, Biomedical Imaging, Image segmentation, Computer aided analysis, Neural networks.

I. I NTRODUCTION

W

ITH the recent outbreak of Coronavirus disease-2019
(COVID-19), the world has experienced an unprecedented number of deaths with a major collapse in the healthcare system throughout the world [1], [2]. Early diagnosis is
the primary concern to control this global pandemic at this
stage for its extreme infectious nature [3]. Though Reverse
transcription-polymerase chain reaction (RT-PCR) is considered as the gold standard for diagnosing COVID-19, its longer
time requirement, lower sensitivity with a massive shortage
of test-kits have already engendered the extreme urgency of
alternative automated diagnostic schemes [4], [5]. Due to
the wide applicability of the artificial intelligence (AI) tools
in numerous clinical diagnostic measures, it has enormous
potential to expedite the diagnostic process of COVID-19
through automated analysis and interpretation of the clinical
record [6], [7].
Chest radiography has already been proven to be an effective
source for COVID diagnostics due to its major implications
relating to various levels of lung infections [8]. Computer
tomography (CT) scan and chest X-ray have been extensively
explored in the literature to establish an automated AI-based
COVID diagnostic scheme [9]–[11]. Despite the easier access
to chest X-ray, CT scans are more widely accepted due to
its finer details leveraging the accurate diagnosis of COVID
infections. Precise segmentation of lung lesions in chest CT
scans is one of the most demanding and challenging aspects
for faster diagnosis of COVID-19 due to the shortage of
annotated data, diverse levels of infections, and novel types
and characteristics of the infections [12].
Processing 3D CT volume at a whole increases computational complexity exponentially that makes the optimization
and convergence more difficult limiting the architectural diversity of the network. The most widely used alternative of
3D-processing is to operate separately on 2D-slices extracted
from the CT-volume [12]–[16]. However, such slice-based
processing loses inter-slice contextual information that results
in sub-optimal performance. In [17]–[20], smaller sub-volumes

“This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.”

2

TANVIR MAHMUD et al.: COVSEGNET ARCHITECTURE

are extracted from the original 3D volumes to minimize the
computational burden as well as to utilize 3D contextual
information. However, such methods suffer from inter-volume
contextual information loss by considering a smaller portion
of the whole set at a time as well as increases complexity to
process sub-volume level prediction into the final result.
A wide variety of approaches have been introduced in
recent years for segmenting the region-of-interest in diverse
applications. In [21], a fully connected network (FCN) is
introduced that produces multiple scales of encoded feature
maps and reconstructs the segmentation mask utilizing these
encoded representations. In [22], Unet architecture is introduced by integrating an inverted decoder module following the
encoder module to gradually reconstruct the mask that gains
much popularity over the years. However, several architectural
limitations of Unet are identified that provides suboptimal
performance.
•

•

•

•

The skip connection introduced in Unet generates semantic gap between corresponding feature scale of encoderdecoder modules, which mainly arises from the direct concatenation of two semantically dissimilar feature
maps. As the encoder module encodes the input image
gradually into more generalized feature representation,
it contains richer details compared to the corresponding
decoded feature map which contains more information for
the reconstruction of the final segmentation mask. These
existing semantic gaps between corresponding encoder
and decoder feature maps make the optimization process
more difficult to converge for such direct concatenations
through skip connections.
Contextual information loss occurs in traditional pooling/strided convolution-based downsampling operations
that become more eminent with deeper architecture. Such
downsampling operations are mainly carried out for generating more generalized, sparser feature representation
with increased channels and reduced spatial resolution
of the feature map. However, these operations also lead
to loss of contextual information that greatly rises with
the increase of vertical depth of the network. Similarly,
the traditional upsampling operations fail to properly
incorporate contextual information.
The vanishing gradient problem rises in a deeper structure
for sequential optimization of multi-scale features. This
problem mainly arises from the difficulty of gradient
propagation through the deep stack of convolutional
layers. Along with the incorporation of additional levels
in the encoder and decoder stacks to make the network deeper, it becomes increasingly difficult to backpropagate the gradients through these levels for propagating through longer sequential paths that make the
optimization of the deeper layers more difficult. Hence,
this problem reduces the effective contributions of the
deeper layers of the encoder and decoder modules for
improper optimization.
Simplistic sequential convolutional layers are integrated
into each level of encoder/decoder modules that lack
enough architectural diversity to extract features from

a broader spectrum, which is mainly caused by the
linear propagation of gradients that reduces the impact of
prior convolutional layers at each level for diminishing
gradients. It lacks opportunity for the proper reuse of
extracted features in the successive convolutions and
lacks parallelism among convolutional layers required for
better optimization, which lower the diversity of features
generated at different levels of the network.
Different architectural modifications have been explored in
recent years to overcome some of these limitations. To increase
the diversity of operations at each scale of feature maps,
numerous established network building blocks are integrated
in encoder/decoder module, e.g. residual block [23], dense
block [24], inception block [25], dilated residual block [26],
and multi-res block [27]. To reduce the semantic gap between
a particular scale of encoder and decoder, a residual path is
proposed in MultiResUnet architecture instead of a direct skip
connection of Unet [27]. However, the semantic gap generated
between multi-scale feature maps of encoder and decoder
modules still persists. In Unet++ [28], a nested stack of
convolutional layers is introduced to reduce the semantic gaps.
But, it increases computational complexity considerably which
makes convergence difficult. In [19], Vnet is proposed that
utilizes residual building blocks in Unet architecture, while
in [20], cascaded-Vnet is presented for performance improvement that utilizes a dual-stack of the cascaded encoder-decoder
module. Nevertheless, with existing numerous architectural
limitations of traditional U-shaped architecture in each stage, it
increases semantic gaps with the additional encoding-decoding
stage as well as increases vanishing gradient issues with
contextual information loss that open up opportunities for
further optimization.
In this paper, an improved, automated scheme is proposed
for precise lesion segmentation of COVID-19 chest CT volumes by overcoming the limitations of traditional approaches
with a novel deep neural network architecture, named as CovSegNet. The major contributions of this work are summarized
below:
1) Along with the opportunity of vertical expansion, a horizontal expansion strategy is introduced in the CovSegNet architecture. In the vertical expansion mechanism,
the encoder and decoder modules are deepened, while in
horizontal expansion, several encoding-decoding stages
are integrated. As discussed earlier, loss of contextual
information occurs when the network is vertically expanded through subsequent downsampling operations,
though it provides the opportunity for improved generalization through incorporating features from higher
levels. Whereas, the horizontal expansion mechanism
assists to integrate more detailed features at each level
for finer reconstruction that helps to recover the loss
of contextual information. As a result, it provides the
opportunity to increase generalization while exploiting
the available contextual information through an optimal
combination of horizontal and vertical stages.
2) For further replenishing the loss of contextual information in traditional pooling/upsampling operations, a scale

3

Fig. 1. Workflow of the proposed scheme for segmenting lung lesions of COVID-19 in CT volume. In phase-1, deeper CovSegNet2D is trained
and optimized with CT-slices. In phase-2, further joint optimization is carried out where pre-trained CovSegNet2D is fine-tuned for generating the
ROI-enhanced CT volume while shallower form of CovSegNet3D is trained for more precise volumetric segmentation.

3)

4)

5)

6)

transition scheme is introduced in the encoder/decoder
module by incorporating multi-scale feature maps from
preceding levels. This scale transition scheme also improves the gradient flow across different feature scales
of a particular encoder/decoder module.
For reducing semantic gaps among corresponding feature scales of the encoder-decoder modules, a multiscale fusion module is introduced in between successive encoder-decoder modules. This module fuses
multi-scale feature representations, generated at preceding encoder/decoder modules through pyramid fusion
scheme, to generate representational features with reduced semantic gap and improved contextual information for the following decoder/encoder module, instead
of directly connecting corresponding feature scales like
Unet. Moreover, this module establishes parallel linkage
among multi-scale feature maps of subsequent encoderdecoder modules that greatly improve the gradient flow
across the network and helps to reduce the vanishing
gradient problem.
A multi-phase training approach is introduced for integrating the advantages of both the 2D and 3D data
processing scheme to reach the optimum performance.
2D processing provides faster processing with lower
memory consumption while losing inter-slice contextual
information. Whereas, 3D processing exploits both the
intra-slice and inter-slice contextual information while
increasing the computational burden. The proposed
multi-phase training solves this problem by integrating
a deeper variant of CovSegNet2D followed by a much
shallower variant of CovSegNet3D for exploiting all
possible contextual information while limiting the computational burden.
The proposed CovSegNet architecture is designed in a
modular and structured way that can be adapted to its
lightweight, shallow form to reduce complicacy with
considerable performance as well as can be made very
deep to increase diversity for incorporating finer details.
This generic design provides more flexibility for tuning
the design parameters in a wide variety of applications.
Extensive experimentations have been carried out to

validate the effectiveness of the proposed scheme on two
publicly available datasets containing chest CT scans
from COVID-19 patients. Moreover, to validate the wide
applicability of the proposed architecture, experimental
results on a challenging, non-clinical, semantic segmentation dataset are also provided.
II. M ETHODOLOGY
The proposed scheme splits the segmentation of CT volumes into two subsequent phases to reduce the computational
complexity of 3D convolution as well as to take the advantages
of multi-scale 2D convolutions (Fig. 1). In the first phase of
the training, 2D slice-based optimization process is carried out
where a 2D variant of the proposed CovSegNet architecture
(i.e. CovSegNet2D) is employed to extract the segmentation
mask of the infected lesions in CT slices. After optimization,
a thresholding scheme is employed to convert the predicted
probability mask into a binary mask. Hence, after completion
of the phase-1 of training and optimization, this network
is capable of extracting slice-based lesion mask efficiently
and effectively. However, slice-based processing of input CT
volumes will lead to loss of inter-slice contextual information
resulting in sub-optimal performance. To introduce further optimization and processing utilizing the inter-slice information,
phase-2 of the training stage is incorporated. Several 2Dslices are extracted from input CT volumes and the pre-trained
CovSegNet2D is utilized to extract the probability masks
of the lung lesions. As CovSegNet2D is heavily optimized
in phase-1 for 2D-slice based segmentation, it will provide
the effective probability mask of the region-of-interest (ROI)
in the CT slices. These masks are used for enhancing the
ROIs of the CT slices while suppressing the redundant parts,
and these are aggregated later to generate the ROI enhanced
CT volume where most of the redundant parts are removed.
Afterwards, the 3D variant of the proposed CovSegNet (i.e.
CovSegNet3D) is employed into operation for further processing of the ROI enhanced CT volume considering both
intra-slice and inter-slice contextual features. At the phase2 of training, this CovSegNet3D is trained and optimized for
generating the 3D-volumetric probability mask to introduce
the inter-slice processing for improving performance while the

4

TANVIR MAHMUD et al.: COVSEGNET ARCHITECTURE

Fig. 2. Schematic representation of the two-stage implementation of the proposed CovSegNet architecture where two sequential encoder-decoder
operational stages are employed with L subsequent levels. Three multi-scale fusion (MSF) modules are integrated in between subsequent modules.
Generated feature maps from two decoder modules are optimized using the joint optimizer. These encoder, decoder and MSF modules are composed
of several operational unit cells.

pre-trained CovSegNet2D obtained from phase-1 is supposed
to be fine-tuned for generating ROI-enhanced slices. Both
these networks pass through a joint-optimization process for
achieving optimum performance. Moreover, a deeper variant
of CovSegNet2D is used for exploiting the advantages of
less expensive 2D operations while a shallower variant of
CovSegNet3D is used to reduce the computational burden
of 3D processing. As considerably precise performance can
be achieved from the slice-based operations utilizing the
CovSegNet2D only, it minimizes the need for deeper 3Doperations in phase-2 of training. Hence, the proposed hybrid
networking scheme is capable of exploiting the advantages of
both the efficient, lighter 2D convolutions along with the 3D
contextual information that provides optimal performance.
A. Problem Formulation
Let consider the set of CT volumes as X, and their corresponding ground truths as Y, such that Xi ∈ Rh×w×s×c ,
Yi ∈ Rh×w×s×c , and i = {1, 2, 3, . . . , N }, where (h, w, s, c)
denote height, width, number of slices, and channels per slice,
respectively, of a particular CT volume from total N number of
CT volumes. Moreover, let consider xi,j ∈ Rh×w×c as the ith
slice from total S slices of jth CT volume and yi,j ∈ Rh×w×c
as its corresponding mask, such that i = {1, 2, . . . , S}, and
j = {1, 2, . . . , N }. In the first phase of training, the objective
function for slice-based optimization of CovSegNet2D is
Phase 1: argmin L2D (θ, yp , y)

(1)

θ

where, θ denotes the network parameter of CovSegNet2D,
x, yp , y denote the input 2D-slice, predicted probability mask,
and corresponding ground truth mask.

In the phase-2 of training, the pre-trained CovSegNet2D
network obtained from phase-1 is employed to generate ROI
enhanced CT volume X0 , and thus
x0 = x

yp ; ∀ x0 ∈ X0 , x ∈ X, yp ∈ Yp

(2)

where
denotes element-wise multiplication and x denotes
2D-CT slice, x0 denotes ROI-enhanced CT-slice, and yp
denotes the predicted probability mask.
Afterwards, optimization of the CovSegNet3D is carried
out utlizing ROI-enhanced CT-volume, while CovSegNet2D is
fine-tuned to generate more accurate probability masks from
2D-slices, and the joint optimization objective function F can
be formulated as
Phase 2: argmin F{L2D (Θ1 , yp , y), L3D (Θ2 , Yp , Y)}
Θ1 ,Θ2

(3)
where Θ1 denotes the network parameters of CovSegNet2D, Θ2 denotes the network parameters of CovSegNet3D,
X0 , Yp , Y denote the ROI enhanced CT volume, predicted
3D mask, and corresponding 3D ground truth.
B. Proposed CovSegNet architecture
The proposed CovSegNet architecture is a generic representation of a network with a wide range of flexibility for
increasing its applicability in different challenging conditions.
This architecture can be designed for efficient operations
in both 2D and 3D domains. Moreover, it can be made
deeper/lighter according to the requirement of the applications.
In CovSegNet architecture, multiple stages of sequential
encoding and decoding operations are carried out along with
a fusion scheme of multi-scale features in between subsequent

5

TABLE I
A RCHITECTURAL AND O PERATIONAL D ETAILS OF THE E NCODER , D ECODER , AND M ULTI -S CALE F USION M ODULES OF THE P ROPOSED
C OV S EG N ET 2D FOR O PTIMUM P ERFORMANCE IN I NDEPENDENT S INGLE -N ETWORK I MPLEMENTATION
Unit
E-1

Encoder Module
Ingredients
(Conv 1×1, Conv 3×3) × 4

Output
512×512×16

Unit
D-5

Decoder Module
Ingredients
(Conv 1×1, Conv 3×3) × 4

DT-1

Conv 2×2, Stride 2

256×256×32

UT-4

Deconv 2×2, Stride 2

E-2

(Conv 1×1, Conv 3×3) × 4
Maxpool 2×2
Conv 2×2, Stride 2
(Conv 1×1, Conv 3×3) × 4
Maxpool (2×2, 4×4)
Conv 2×2, Stride 2
(Conv 1×1, Conv 3×3) × 4
Maxpool (2×2, 4×4, 8×8)
Conv 2×2, Stride 2

256×256×32

D-4

128×128×64

UT-3

(Conv 1×1, Conv 3×3) × 4
Upsample 2×2
Deconv 2×2, Stride 2
(Conv 1×1, Conv 3×3) × 4
Upsample (2×2, 4×4)
Deconv 2×2, Stride 2
(Conv 1×1, Conv 3×3) × 4
Maxpool (2×2, 4×4, 8×8)
Deconv 2×2, Stride 2

(Conv 1×1, Conv 3×3) × 4

DT-2
E-3
DT-3
E-4
DT-4
E-5

128×128×64

D-3

64×64×128

UT-2

64×64×128

D-2

32×32×256

UT-1

32×32×256

D-1

(Conv 1×1, Conv 3×3) × 4

Output
32×32×256

Unit
MSF-1

64×64×128
64×64×128

MSF-2

128×128×64
128×128×64

MSF-3

256×256×32
256×256×32

MSF-4

512×512×16
512×512×16

MSF-5

Multi-Scale Fusion Module
Ingredients
Upsample(2×2,4×4,8×8,16×16)
Maxpool(2×2,4×4)
Conv 1×1, Conv 3×3
Maxpool (2×2,4×4)
Upsample(2×2,4×4,8×8)
Conv 1×1, Conv 3×3
Maxpool (2×2,4×4)
Upsample(2×2,4×)
Conv 1×1, Conv 3×3
Maxpool (2×2,4×4,8×8)
Upsample(2×2,4×4)
Conv 1×1, Conv 3×3
Maxpool(2×2,4×4,8×8,16×16)
Upsample(2×2, 4×4)
Conv 1×1, Conv 3×3

Output
512×512×16
256×256×32

128×128×64

64×64×128

32×32×256

TABLE II
A RCHITECTURAL AND O PERATIONAL D ETAILS OF THE E NCODER , D ECODER , AND M ULTI -S CALE F USION M ODULES OF THE P ROPOSED
C OV S EG N ET 3D FOR O PTIMUM P ERFORMANCE IN I NDEPENDENT S INGLE -N ETWORK I MPLEMENTATION
Unit
E-1

Encoder Module
Ingredients
(Conv 1×1×1, Conv 3×3×3) × 2

Output
512×512×32×16

Unit
D-4

DT-1

Conv 2×2×2, Stride 2

256×256×16×32

UT-3

E-2

(Conv 1×1×1, Conv 3×3×3) × 2
Maxpool 2×2×2
Conv 2×2×2, Stride 2
(Conv 1×1×1, Conv 3×3×3) × 2
Maxpool 2×2×2
Conv 2×2×2, Stride 2

256×256×16×32

D-3

128×128×8×64

UT-2

(Conv 1×1×1, Conv 3×3×3) × 2

DT-2
E-3
DT-3
E-4

128×128×8×64

D-2

64×64×4×128

UT-1

64×64×4×128

D-1

Decoder Module
Ingredients
(Conv 1×1×1, Conv 3×3×3) × 2
Upsample (2×2×2, 4×4×4)
Deconv 2×2×2, Stride 2
(Conv 1×1×1, Conv 3×3×3) × 2
Maxpool (2×2×2, 4×4×4, 8×8×8)
Deconv 2×2×2, Stride 2
(Conv 1×1×1, Conv 3×3×3) × 2
Maxpool (2×2×2, 4×4×4, 8×8×8)
Deconv 2×2×2, Stride 2
(Conv 1×1×1, Conv 3×3×3) × 2

encoder/decoder module. Each stage of the network consists
of an encoder module and a corresponding decoder module.
Hence, the network, N , can be represented as
N = Dm (Em . . . (D1 (E1 (θE1 ), θD1 ), . . . , θEm ), θDm ) (4)
where Ei , Di represents the encoder and decoder modules,
respectively, of ith stage from total m stages, and θEi , θDi
represents their respective parameters. Two-stage implementation of this architecture is schematically presented in Fig. 2.
This network can be extended from level-1 to level-L
to produce a deeper variant. The encoder/decoder module
constitutes of several unit cells operating at each level of
the network. To generate a deeper network, additional unit
cells are integrated in each of the encoder/decoder module to
increase number of levels. Here, Ei,j , Di,j represent the ith
unit cell of jth stage of encoder and decoder, respectively,
where i = {1, 2, . . . , L}, and j = {1, 2, . . . , m}. Hence, L
number of different scales of representative feature maps are
obtained from each encoder/decoder module. Moreover, scale
transition of feature maps is carried out in between succeeding
encoder/decoder unit cells, and effective transformation on
each scale of feature maps are integrated utilizing the generalized unit cell structure in encoder/decoder module.
In between successive encoder/decoder modules, a multiscale fusion (MSF) module is introduced to reduce the semantic gap with preceding stages as well as to improve
the gradient propagation through parallel linkage of multiscale features. Similar to encoder/decoder module, each MSF
module consists of several operational unit cells operating
at different levels. Let consider, Fi represents the ith MSF
module, Fi,j represents the ith unit cell of jth MSF module,

Output
64×64×4×128

Unit
MSF-1

128×128×8×64
128×128×8×64

MSF-2

256×256×16×32
256×256×16×32

MSF-3

512×512×32×16
512×512×32×16

MSF-4

Multi-Scale Fusion Module
Ingredients
Output
Maxpool(2×2×2,4×4×4)
512×512×32×16
Upsample(2×2×2, 4×4×4,8x8x8)
Conv 1×1×1, Conv 3×3×3
Maxpool (2×2×2, 4×4×4)
256×256×16×32
Upsample(2×2×2,4×4×4)
Conv 1×1×1, Conv 3×3×3
Maxpool (2×2×2, 4×4×4)
128×128×8×64
Upsample(2×2×2,4×4×4)
Conv 1×1×1, Conv 3×3×3
Maxpool (2×2×2,4×4×4)
Upsample(2×2×2,4×4×4, 8×8×8)
64×64x4×128
Conv 1×1×1, Conv 3×3×3

such that i = {1, 2, . . . , L}, j = {1, 2, . . . , 2m − 1}, and
Fi,j ∈ Fi .
Each MSF module takes all scales of feature representations as input from all preceding encoder/decoder stages,
and generates L number of different feature maps for the
following encoder/decoder stage through deep fusion of multiscale features obtained from preceding stages. In each unit cell
of MSF module, multi-scale feature aggregation and pyramid
fusion scheme is employed, which can be represented as
Fi,j = F (E1 , E2 , . . . , Ed j e , D1 , D2 , . . . , Db j c )
2

(5)

2

∀ i = {1, 2, . . . , L}, j = {1, 2, . . . , 2m − 1}
where F (.) represents the functional operations in the MSF
unit cell considering L scale of representations from each of
the preceding encoder/decoder module.
From final level of the sequential decoder modules, several
decoded feature representations are obtained which are processed together in the joint optimizer unit (J ) to produce the
final segmentation mask, and it can be given by,
J = F(D1,1 , D1,2 , . . . , D1,m )

(6)

where F(.) represents the joint optimizer function.
All the basic building blocks of the CovSegNet architecture
are generic and can be designed and optimized for both 2D
and 3D operations. In the following discussions, different
building blocks of the CovSegNet architecture are presented
in detail. For the ease of discussion, mainly 2D operational
blocks are focused. However, for 3D operations, all the convolutional kernels, pooling/upsampling windows are shifted in
dimension for operating with 3D voxels instead of 2D pixels.
Architectural details for the most optimized implementations

6

TANVIR MAHMUD et al.: COVSEGNET ARCHITECTURE

(a) Encoder Module

(b) Decoder Module

Fig. 3. Schematic representations of the proposed encoder and decoder
modules in five-level implementation having five unit blocks along with
associated Down Transition (DT)/ Up Transition (UT) units in between
subsequent unit blocks. Here, (h, w, c) is used to denote the height, width,
and channel of the feature maps at different phase.
(a) Down Transition Unit (DT-3)

(a) Encoder Unit Cell

(b) Decoder Unit Cell

Fig. 4.
Structure of the Encoder/Decoder Unit cells. Four densely
interconnected convolutional stages are employed in each unit. Here, ‘c’
denotes the channelwise concatenation of feature maps extracted from
transition unit and multi-scale fusion (MSF) unit. Ei,j /Di,j denote the
unit blocks of ith level in jth module.

of CovSegNet2D and CovSegNet3D are presented in Table I
and in Table II, respectively.
C. Proposed Encoder/Decoder Structure
The encoder and decoder modules are structurally similar
that are successively used in the sequential stages of CovSegNet. Encoder/decoder modules are schematically presented
in Fig. 3. These encoder/decoder modules are composed
of several operational unit cells with transitional dense interconnections. The operations of encoder/decoder modules
can be divided into two categories: unit cell operations and
transitional operation.
1) Encoder/Decoder Unit Cell operation: In Fig. 4, the unit
cell structure of the encoder/decoder module is presented. In
each unit cell, two input feature map is entered, one from
the transitional unit and the other from the preceding MSF
unit while the output feature map is passed through following
transitional and multi-scale fusion operations. Moreover, each
unit cell consists of four densely interconnected convolutional
layers, where each convolutional layer provides two sequential
convolutional filtering with (1 × 1) and (3 × 3) kernels. Such
dense interconnection between convolutional operations has
been proven to be effective in numerous applications. No
dimensional scaling has been carried out in each of this unit

(b) Up Transition Unit (UT-2)
Fig. 5. Schematic representations of the down transition unit (operating
between level-3 and 4) and the up transition unit (operating between level(L − 2) and (L − 3)). All the feature maps generated from preceding unit
blocks are made uniform and integrated in the transition process.

cell as it is employed for introducing adequate transformation
in the feature space to encode/decode effective representation.
Hence, this unit cell operations can be functionally represented
as, E, D : Rh×w×c → Rh×w×c , where (h, w, c) represents the
height, width and channel of the feature map.
2) Encoder Down-transitional Operation: During down
transitional operations between subsequent unit cells of the
encoder module, the spatial dimension of the feature map is
reduced for generalizing the feature map, whereas the channel
depth is increased to incorporate more filtering operations in
subsequent levels for generating more sparser features. It can
be functionally presented as, f : Rh×w×c → Rh/2×w/2×2c ,
where spatial resolution is downscaled by 2 and channel

7

Fig. 8. Schematic of the joint optimizer module optimizing the decoded
feature maps generated from two decoding stages.

Fig. 6. Schematic representation of the proposed Multi-Scale Fusion
module. Detailed operations performed in an MSF unit cell are particularly focused, and similar operations are carried out in other unit cells
of the MSF module.

transition unit is schematically presented. In each of such down
transition unit, encoded feature representations generated from
all higher levels of unit cells are considered for generating
the down-scaled feature map. Hence, contextual information
lost in each transitional operation can be recovered from very
deep stack of unit cells as feature representations from all
preceding cells are considered during transition. To converge
multi-scale feature maps from preceding levels, firstly, pooling
operations with different kernels are carried out to make
their spatial dimension uniform and subsequently, channelwise
feature aggregation is carried out. The aggregated feature map,
Fagg,DT , generated at ith level can be represented as
i
Fagg,DT
= E i ⊕ P (2×2) (E i−1 ) · · · ⊕ P (2

Fig. 7. Proposed pyramid fusion scheme utilizing diverse windows of
frequent up-sampling and downsampling operations for fusing multi-scale
features.

depth is increased by 2 from the input feature map obtained
from the previous level. However, traditional downsampling
operations using pooling/strided convolutions results in loss of
contextual information. Moreover, it can be more prominent
while incorporating a deep stack of unit cells in the encoder
module. To mitigate the loss of contextual information in down
transitional operation, a higher level of dense interconnection
is proposed among multi-scale feature maps generated from
different unit cells. In Fig. 5a, the structure of such a down

i−1

×2i−1 )

(E 1 ) (7)

where ⊕ indicates the feature concatenation, P (2×2) represents
pooling operation with (2 × 2) window, E i represents the
output of ith unit cell of the encoder.
Finally, a convolutional operation with (2 × 2) kernel is carried out with a stride of (2 × 2) for generating the downscaled
feature map by filtering the aggregated feature vector.
3) Decoder Up-transitional Operation: On the contrary, up
transitional operations are carried out in between successive
decoder unit cells to provide the dimensional shifting towards
the reconstruction of the final segmentation mask. In each of
such up-transition operations, spatial resolution is upscaled
by 2 while channel depth is reduced by 2 to get closer to
the final reconstruction mask and it can be represented as,
f 0 : Rh×w×c → R2h×2w×c/2 . Similar to the down-transitional
operation in Encoder, all the preceding representations of
multi-scale decoded feature maps generated from different
unit cells are taken into consideration in the up-transition
operation to gather more contextual information (Fig. 5b).
Firstly, spatially uniform feature maps are created through
bi-linear interpolation upsampling with different windows,
and feature aggregation is carried out to generate aggregated
feature vector, Fagg,U T , which is given by
i
i
(2×2)
Fagg,U
(Di+1 ) · · · ⊕ U (2
T = D ⊕U

i−1

×2i−1 )

(DL ) (8)

where U (2×2) represents bilinear upsampling operation with
(2 × 2) window, Di represents the output of ith unit cell of
the decoder.
Finally, the aggregated feature map is processed using a
deconvolution operation with (2 × 2) kernel to incorporate the
necessary dimensional up-scaling for further processing in the
following unit cell.

8

TANVIR MAHMUD et al.: COVSEGNET ARCHITECTURE

Fig. 9. Visual representations of the segmentation performances of different state-of-the-art networks on the CT images from Database-1 and
Database-2. Here, ‘yellow’ represents the true positive (TP) regions, ‘red’ represents the false negative (FN) regions, and ‘blue’ represents the false
positive (FP) regions.

D. Proposed Multi-Scale Fusion (MSF) Module with Pyramid
Fusion scheme
During sequential encoding-decoding operations, a semantic
gap is generated between a similar scale of encoded and
decoded feature maps. Moreover, in traditional architecture,
the gradient has to propagate sequentially that sometimes gives
rise to vanishing gradient problems for deeper encoder/decoder
module particularly. As multiple stages of encoding and decoding operations are integrated into the CovSegNet, this problem
is supposed to be more prominent if all the encoder and
decoder modules are sequentially connected. To overcome
these limitations, a multi-scale fusion module is proposed
that develops parallel interconnection among different scales
of feature maps of the encoder/decoder modules utilizing a
pyramid fusion scheme.
As shown in Fig. 6, each MSF module consists of several MSF-unit cells where each cell considers multi-scale
feature maps generated from different levels of preceding
encoder/decoder modules and generates feature map for the
unit cell of the following encoder/decoder module. Here,
similar scale of feature representations generated from different levels of the preceding encoder/decoder modules are
concatenated, firstly, to produce L number of multi-scale
feature maps. Afterward, all the L scales of feature maps are

made spatially equivalent in dimension through pooling and
bi-linear upsampling with different windows, and channelwise
feature concatenation is carried out to generate the aggregated
feature vector. This can be represented as
(i,j)

Fagg,M SF = P (2

i−1

×2i−1 )

(f1 ) ⊕ · · · ⊕ P (2×2) (fi−1 )⊕
L−i

⊕fi ⊕ U (2×2) (fi+1 ) ⊕ · · · ⊕ U (2

×2L−i )

(fL ) (9)

fi = E(1,j) ⊕ . . . E(i,j) ⊕ D(1,j) ⊕ · · · ⊕ D(i−1,j) (10)
(i,j)

where Fagg,M SF is the aggregated feature vector generated in
the ith level of jth MSF module, and fi represents the ith
concatenated feature map.
Afterward, the aggregated feature vector is passed through
a pyramid fusion scheme to generate the output feature vector
that will be fed to the corresponding encoder/decoder unit cell
of the following module. Hence, the generated output feature
map from each MSF unit cell contains information from all
preceding modules and thus, establishes a parallel flow of
optimization for efficient gradient propagation.
E. Proposed Pyramid Fusion (PF) Module
The pyramid fusion (PF) module incorporates pyramid fusion scheme into the aggregated feature map of MSF unit cell
(Fagg,M SF ) utilizing the combinations of sequential multiwindow pooling and upsampling operations (shown in Fig. 7).

9

TABLE III
A BLATION S TUDY OF THE E FFECT OF D IFFERENT M ODULES IN THE P ERFORMANCE (M EAN±S TANDARD D EVIATION ) OF THE P ROPOSED
C OV S EG N ET 2D A RCHITECTURE
Network
Baseline (V1)
Baseline+ DT (V2)
Baseline+ UT (V3)
Baseline+ DT+UT (V4)
Baseline+(MSF-w/o PF) (V5)
Baseline+ MSF (V6)
CovSegNet2D (V7)

Sen.(%)
82.7± 0.49
83.8±0.29
83.1±0.25
84.9±0.41
86.9±0.15
88.4±0.28
90.8±0.32

Spec.(%)
97.4± 0.09
97.8±0.12
97.7±0.08
98.1±0.11
98.3±0.07
98.7±0.08
99.1±0.13

Dataset-1
Dice(%)
84.1±0.29
85.8±0.36
85.4±0.16
86.7±0.27
87.3±0.28
89.2±0.32
91.1±0.25

IoU(%)
79.8±0.21
81.1±0.08
80.9±0.13
82.3±0.32
82.9±0.26
84.1±0.21
86.9±0.09

p-Value
0.0033
0.0017
0.0021
0.0019
0.0041
0.0011

Sen.(%)
71.7±0.12
73.6±0.31
73.1±0.55
74.6±0.17
76.2±0.27
78.8±0.25
81.5±0.22

Spec.(%)
95.8±0.18
96.5±0.15
96.3±0.18
97.1±0.12
97.9±0.16
98.4±0.11
98.9±0.13

Dataset-2
Dice(%)
71.9±0.33
73.4±0.14
73.1±0.19
74.8±0.34
77.2±0.29
79.5±0.21
82.7±0.08

IoU(%)
65.8±0.27
67.6±0.21
67.2±0.35
69.4±0.18
72.8±0.24
74.1±0.25
77.5±0.14

p-Value
0.0023
0.0044
0.0012
0.0034
0.0048
0.0009

TABLE IV
A BLATION S TUDY OF THE E FFECT OF D IFFERENT M ODULES IN THE P ERFORMANCE (M EAN±S TANDARD D EVIATION ) OF THE P ROPOSED
C OV S EG N ET 3D A RCHITECTURE IN DATASET-1
Network
Baseline3D (V13D )
Baseline3D + DT (V23D )
Baseline3D + UT (V33D )
Baseline3D + DT+UT (V43D )
Baseline3D+(MSF-w/o PF) (V53D )
Baseline3D+ MSF (V63D )
CovSegNet3D

Sensitivity(%)
84.5±0.21
85.7±0.31
85.2±0.18
86.7±0.22
87.4±0.25
89.6±0.19
91.1±0.26

Specificity(%)
97.9±0.12
98.2±0.19
98.1±0.08
98.7±0.14
97.9±0.11
98.4±0.15
99.3±0.09

Dataset-1
Dice Score(%)
85.2±0.23
86.1±0.25
85.9±0.18
88.3±0.28
88.2±0.21
89.9±0.17
92.3±0.15

IoU(%)
80.8±0.32
82.3±0.29
82.0±0.21
83.5±0.27
83.8±0.31
85.1±0.19
87.7±0.23

p-Value
0.0011
0.0008
0.0017
0.0032
0.0021
0.0025

TABLE V
P ERFORMANCE C OMPARISON (M EAN±S TANDARD D EVIATION ) OF THE P ROPOSED C OVSEG N ET 2D A RCHITECTURE WITH OTHER
S TATE - OF - THE -A RT A PPROACHES ON 2D-CT SLICES
Network
Unet [22]
Unet++ [28]
MultiResUnet [27]
Attention-Unet-2D [29]
CPF-Net [30]
Semi-Inf-Net [12]
CovSegNet2D(Ours)

Sen.(%)
75.9±0.34
78.6±0.17
77.2±0.33
81.1±0.29
78.9±0.27
82.7±0.26
90.8±0.32

Spec.(%)
88.9±0.12
91.1±0.18
90.3±0.24
92.2±0.11
91.7±0.14
94.8±0.21
99.1±0.13

Dataset-1
Dice(%)
82.3±0.26
84.1±0.23
83.7±0.28
85.1±0.14
84.4±0.25
86.9±0.34
91.1±0.25

IoU(%)
77.1±0.18
78.9±0.21
78.4±0.15
79.6±0.28
79.3±0.25
81.1±0.18
86.9±0.09

TABLE VI
P ERFORMANCE C OMPARISON (M EAN±S TANDARD D EVIATION ) OF
THE C OV S EG N ET 3D A RCHITECTURE WITH OTHER
S TATE - OF - THE -A RT N ETWORKS ON 3D-CT VOLUMES OF DATASET-1
Network
Unet-3D [22]
Unet++-3D [28]
MultiResUnet-3D [27]
Attention-Unet-3D [29]
CPF-Net-3D [30]
VNet-3D [19]
CovSegNet3D(Ours)
CovSegNet-Hybrid(Ours)

Sen.(%)
77.1±0.22
79.2±0.17
78.7±0.27
82.5±0.26
80.1±0.23
84.3±0.29
91.1±0.26
92.6±0.25

Spec.(%)
89.8±0.18
91.7±0.25
90.9±0.16
93.1±0.31
92.6±0.23
93.9±0.17
99.3±0.09
99.5±0.07

Dice(%)
84.2±0.27
85.1±0.29
84.5±0.31
85.9±0.24
85.2±0.18
85.7±0.31
92.3±0.15
94.1±0.19

IoU(%)
79.4±0.24
80.2±0.26
78.9±0.18
81.4±0.29
80.8±0.34
81.3±0.19
87.7±0.23
90.2±0.27

p-Value
0.0024
0.0011

p-Value
0.0008

Sen.(%)
52.9±0.29
57.7±0.32
56.9±0.27
60.8±0.25
62.2±0.14
72.9±0.44
81.5±0.22

Dataset-2
Dice(%)
43.3±0.34
52.3±0.31
50.8±0.28
57.7±0.36
60.4±0.25
74.1±0.24
82.7±0.08

Spec.(%)
86.2±0.09
89.2±0.11
86.9±0.15
88.4±0.12
91.1±0.14
95.8±0.19
98.9±0.13

IoU(%)
38.8±0.32
48.1±0.37
45.2±0.22
51.9±0.26
56.1±0.21
68.1±0.32
77.5±0.14

p-Value
0.0013

TABLE VII
E FFECT OF V ERTICAL E XPANSIONS (L EVELS ) AND H ORIZONTAL
E XPANSIONS (S TAGES ) ON THE D ICE S CORE (M EAN±S TANDARD
D EVIATION ) IN DATASET-1
Level
2
3
4
5
6

1-stage
49.9±0.37
64.8±0.23
75.2±0.32
83.5±0.19
86.7±0.27

CovSegNet2D
2-stage
75.3±0.13
85.8±0.32
89.6±0.27
91.1±0.25
90.9±0.21

3-stage
78.12±0.21
88.5±0.15
90.8±0.22
89.9±0.12
89.1±0.11

1-stage
57.3±0.18
69.3±0.35
79.8±0.29
84.5±0.43
89.3±0.21

CovSegNet3D
2-stage
79.8±0.18
89.2±0.26
92.3±0.15
90.2±0.34
89.8±0.41

3-stage
82.1±0.19
90.2±0.25
91.8±0.17
89.7±0.28
87.9±0.36

diverse feature domains, which can be represented by
Firstly, the depth of the aggregated vector, Fagg,M SF , is
reduced through a pointwise convolution (kernel, 1 × 1) to
generate feature vector fa , and thus, Fagg,M SF 7→ fa , where
fa ∈ Rh×w×c .
Afterwards, the generated vector, fa , passes through multiple spatial scaling-vertical scaling-inverse spatial scaling
operations in parallel with different scaling factors. Spatial
scaling operation is carried out utilizing pair of pooling and
upsampling operations with different kernel windows, while
vertical scaling is employed utilizing convolutional filtering
(kernel, 3 × 3) to reduce the channel depth by one-fourth of
the initial depth. Initial reduction followed by expansion of
the feature map assists in gathering the more general feature
representation, while initial expansion followed by reduction
of the feature map gathers the more detailed information from
a sparser domain. These operations pave the way to extract
the most generalized representations through analyzing from

Pr :Rh×w×c → Rh∗r×w∗r×c → Rh∗r×w∗r×c/4 → Rh×w×c/4
∀r = {0.25, 0.5, 2, 4}

(11)

where Pr denotes one of the parallel operational paths in the
PF module with a spatial scaling factor of r.
Afterwards, feature aggregation operation is carried out
utilizing different representations generated at multiple paths
along with the input representation to generate the aggregated
vector Fagg,P F , where Fagg,P F ∈ Rh×w×2c . Finally, a final
pointwise convolution (kernel, 1 × 1) is carried out to generate
the output feature map fout,P F , where fout,P F ∈ Rh×w×c .
F. Structure of the Joint Optimizer
The decoded feature maps generated from the top of decoder
modules are considered for final reconstruction through a joint
optimization process. This process is schematically shown in
Fig. 8. Initially, an aggregated feature vector Fagg,J , is created

10

TANVIR MAHMUD et al.: COVSEGNET ARCHITECTURE

considering all the output feature maps from different decoder
modules which can be given by
Fagg,J = D1,1 ⊕ D1,2 ⊕ · · · ⊕ D1,S

(12)

where S denotes total number of stages.
Afterward, pyramid fusion scheme is employed on aggregated vector to obtain the more generalized representation
utilizing multi-scale decoded representations. Finally, another
convolutional filtering (kernel, 3 × 3) is carried out to generate
the final segmentation mask fmask , utilizing binary activation
function, and these can be represented as
fmask = σ(Conv(P F (Fagg,J ))

(13)

where σ(.) denotes the non-linear activation.
G. Loss Function
Tversky Index is introduced in [31] for better generalization
of the the dice index by balancing out false positives and false
negatives, which is given by
PP
i=1 p1i g1i + 
T I = PP
PP
PP
i=1 p1i g1i + α
i=1 p0i g1i + β
i=1 p1i g0i + 
(14)
where g0i , p0i indicate the ground truth and prediction probability of pixel i being in a normal region, while g1i , p1i
indicate the ground truth and prediction probability of pixel i
being in an abnormal region, P is the total number of pixels on
a certain image, α, β are used to shift emphasize for balancing
class imbalance such that α + β = 1, and (10−8 ) is used to
avoid division-by-zero as safety factor.
To put more emphasis on hard training examples, a Focal
Tversky loss function is introduced in [32] utilizing the
Tversky Index, which is given by
X
1
(15)
L=
(1 − T Ic ) γ
c

where γ is used to emphasize the challenging less accurate
predictions. Due to the better generalization over a large
number of datasets according to [32], α = 0.7, β = 0.3, γ = 43
are used for all experimentations in this study.
If y, yp denote slice-wise mask ground truth and corresponding probability prediction, respectively, while Y, Yp
denote volumetric mask ground truth and corresponding probability prediction, respectively, the objective loss functions for
separately optimizing CovSegNet2D and CovSegNet3D can
be represented as
L2D = L(y, yp ); y, yp ∈ Rh×w×c

(16)

L3D = L(Y, Yp ); Y, Yp ∈ Rh×w×s×c

(17)

The joint optimization objective function used in phase-2
combining slice-wise and volumetric operations is given by
F = λ(

S
1X i
L ) + L3D
S i=1 2D

(18)

where λ denotes the scaling factor of 2D-loss term, and s
denotes total number of 2D-slices per volume. Here, λ = 0.2
is used for optimization to provide more emphasis on CovSegNet3D in phase-2 as CovSegNet2D is pre-trained in phase-1
and is supposed to be fine-tuned in phase-2.

Fig. 10.
Visual representations of the segementation performances
obtained using single phase training (CovSegNet2D and CovSegNet3D)
and multi-phase training (with hybrid 2D-3D networks) in Dataset-1.
Here, ‘yellow’ represents the true positive (TP) regions, ‘red’ represents
the false negative (FN) regions, and ‘blue’ represents the false positive
(FP) regions.

III. R ESULTS AND D ISCUSSIONS
Experimentations have been carried out on three publicly
available datasets to validate the effectiveness of the proposed
scheme on numerous segmentation tasks. Performances of
CovSegNet2D and CovSegNet3D have been separately studied
along with the proposed hybrid scheme of joint optimization
combining CovSegNet2D and CovSegNet3D.
A. Dataset Description
Dataset-1 contains 20 CT volumes with 1800+ slices annotated by expert radiologist panel [33]. All the slices have
annotations for both lung and infection regions. Each slices are
of resolution (630 × 630) which are resized to (512 × 512).
Dataset-2 is the “COVID-19 CT Segmentation dataset” that
contains 110 axial CT images collected by the Italian Society
of Medical and Interventional Radiology from 40 different
COVID-patients [34]. All the images are of resolution (512 ×
512). Each slice contains multi-class annotations of infections.
Dataset-3 is the “Semantic Drone Dataset” where the semantic
understanding of urban scenes is mainly focused to increase
the safety of drone flight and landing procedures [35]. This
dataset consists of 400 images with pixel-wise annotation for
20 different classes having resolutions of 6000 × 4000 and all
of these images are resized to (512 × 512). Experimentations
on Dataset-3 is mainly integrated to investigate the effectiveness of the proposed CovSegNet architecture on other domains
with challenging operating conditions.

11

TABLE VIII
C OMPARISON OF P ERFORMANCES (M EAN±S TANDARD D EVIATION ) ON D IFFERENT T YPES OF I NFECTIONS (G ROUND G LASS O PACITY AND
C ONSOLIDATION ) IN D IFFERENT CT- SLICES OF DATASET-2
Network
Unet [22]
Unet++ [28]
MultiResUnet [27]
Attention-Unet-2D [29]
CPF-Net [30]
Semi-Inf-Net [12]
CovSegNet2D(Ours)

Sen.(%)
41.1±0.26
48.8±0.23
46.6±0.28
44.8±0.19
49.9±0.18
50.9±0.22
63.8±0.17

Spec.(%)
96.2±0.12
97.8±0.16
97.1±0.14
96.8±0.08
97.4±0.15
96.7±0.11
98.4±0.09

Consolidation
Dice(%)
40.3±0.28
42.6±0.26
42.1±0.19
44.5±0.25
44.1±0.23
45.8±0.31
56.8±0.24

IoU(%)
35.5±0.28
38.2±0.19
37.6±0.27
40.1±0.33
39.9±0.29
41.4±0.18
51.9±0.25

TABLE IX
C OMPARISON OF P ERFORMANCES (M EAN±S TANDARD D EVIATION )
ON M ULTI -C LASS S EMANTIC S EGMENTATION TASK OF DATASET-3
Network
Unet [22]
Unet++ [28]
Attention-Unet-2D [29]
CPF-Net [30]
Semi-Inf-Net [12]
CovSegNet(ours)

Sen.(%)
56.9±0.19
57.3±0.25
58.7±0.22
61.5±0.28
64.9±0.31
76.4±0.18

Spec.(%)
68.6±0.23
70.4±0.31
71.8±0.29
73.1±0.17
76.3±0.27
87.7±0.16

Dataset-3
Dice(%)
42.2±0.35
44.8±0.29
48.5+0.42
51.4±0.38
50.9±0.27
64.6±0.21

IoU(%)
37.7±0.28
40.1±0.33
43.9±0.25
47.7±0.34
46.4±0.26
59.5±0.29

p-Value
6e-5

p-Value
0.0017

Sen.(%)
35.1±0.27
41.2±0.32
44.5±0.28
55.3±0.31
53.5±0.22
62.2±0.34
73.3±0.25

Ground-Glass Opacity
Spec.(%)
Dice(%)
IoU(%)
98.2±0.09
44.1±0.27
39.8±0.25
96.6±0.14
49.9±0.22
45.7±0.27
97.3±0.11
47.7±0.18
43.1±0.28
95.4±0.08
52.9±0.17
47.6±0.35
96.9±0.13
56.9±0.26
51.1±0.34
96.1±0.18
62.7±0.22
58.4±0.23
98.9±0.12
70.9±0.31
66.1±0.19

p-Value
0.0028

TABLE X
E FFECT OF D IFFERENT L OSS F UNCTIONS ON THE P ERFORMANCE
(D ICE S CORE (%)) OF C OV S EG N ET ON DATASET-1
Loss Function
Dice Loss
Dice Loss+ BCE loss
Focal Tversky loss

CovSegNet2D
90.2±0.13
90.4±0.11
91.1±0.25

CovSegNet3D
91.1±0.21
91.5±0.17
92.3±0.15

CovSegNet-Hybrid
93.3±0.09
93.6±0.15
94.1±0.19

improvement of performance is achieved using the proposed
scheme over the other existing best performing approaches.

B. Experimental Setup
Different hyper-parameters of the network are chosen
through experimentation for better performance. Adam optimizer is employed for optimization of the network during
the training phase with an initial learning rate of 10−5 . The
learning rate is decayed after ever 10 epochs with a decaying
rate of 0.99. Intel® Xeon® D − 1653N CPU @2.80GHz with
12M Cache and 8 cores along with 24 GB RAM is used for
experimentation. For hardware acceleration, 2× NVIDIA RTX
2080 Ti GPU having with 4608 CUDA cores running 1770
MHz with 24 GB GDDR6 memory is deployed. The network
is trained for 1000 epochs on each dataset. Batch size is chosen
to be 32 for processing 2D-CT slices, while it is chosen to be
2 for processing 3D-CT volume.
A number of traditional evaluation metrics are used for the
evaluation of performance. These are given by
TP
TP + FP + FN
2T P
Dice Score =
2T P + F P + F N
TP
Specif icity =
TP + FP
TP
Sensitivity =
TP + FN
IoU =

(19)
(20)
(21)
(22)

where T P , F P , F N denote true positive, false positive,
and false-negative predictions, respectively. A five-fold crossvalidation scheme is carried out separately on these databases
for evaluation of the proposed scheme. Mean and standard
deviations of the evaluation metrics obtained from different test folds are reported. For binary thresholding of the
predicted probability mask, a threshold of 0.5 is used in
general. The Wilcoxon rank-sum test is used for statistical
analysis of the performance improvement obtained from the
proposed scheme. The performances of the proposed schemes
are statistically analyzed and the statistical significance level
is set to α = 0.01. The null hypothesis is that no significant

C. Ablation study
To analyze the effectiveness of different modules of the
proposed CovSegNet architecture, an ablation study is carried
out. The baseline model is defined as the two-stage implementations with encoder and decoder modules only excluding
the down-transition (DT) units, up-transition units (UT), and
multi-scale fusion modules. The statistical significance test is
carried out to validate the improvement of dice-scores over
the baseline model.
1) Effects of the transition unit: Instead of proposed downtransition units and up-transition units, traditional max-pooling
and upsampling operations are used, respectively, in the
baseline model according to the conventions of traditional
Unet architecture. Performances with different combinations
of transition units are provided in (V2-V4) of Table III for 2D
analysis. The inclusion of down-transition unit (V2) in encoder
modules provides 1.7% improvement and 1.5% improvement
of dice scores in Database-1 and 2, respectively, over the
baseline. Moreover, the inclusion of up-transition unit (V3)
in decoder modules provides 1.3% and 1.2% improvements
of dice scores, while the inclusion of both of the transition
units (V4) provide 2.6% and 2.9% improvements of dice
scores in Database-1 and 2, respectively. Hence, both of the
up-transition units and down-transition units are contributing
considerable improvements over the baseline performance.
Similar improvements can be noticeable for 3D variants of
the transition units also (from V 23D to V 43D ) that are
summarized in Table IV. All the improvements are found to
be statistically significant (p < 0.01).
2) Effects of the multi-scale fusion (MSF) module: The
MSF modules are proposed in place of the traditional direct
skip connection scheme of Unet architecture to reduce the
semantic gaps between subsequent encoder and decoder modules. In the baseline model, direct skip connections are used
between succeeding modules instead of the MSF module. In
Table III, the change of performance with the inclusion of

12

TANVIR MAHMUD et al.: COVSEGNET ARCHITECTURE

Fig. 11. Visual representations of the segmented multi-class lesions of the CT images from Database-2 obtained using different state-of-the-art
networks. Here, ‘red’ represents the ‘Ground Glass Opacity (GGO)’ regions and ‘yellow’ represents the ‘Consolidation’ regions.

the MSF module in the 2D-baseline model is provided in
V6. It should be noticed that 5.1% improvement of dicescore, 4.3% improvement of IoU score have been achieved
in Database-1, while 7.6% improvement of dice-score, 8.3%
improvement of IoU score have been achieved in Database2. Similar performance improvements can be noticed for the
incorporation of MSF module in the 3D-baseline model (V 63D
in Table IV). These improvements are found to be statistically
significant (p < 0.01).
3) Effects of the pyramid fusion scheme in MSF module:
Pyramid fusion (PF) modules are integrated into the MSF
modules to operate on the aggregated multi-scale feature
vector in the MSF module. Instead of the PF module, a pointwise convolution with (1 × 1) kernel can be performed to
reduce and transform the aggregated vector into the output
vector. The performance of the 2D-baseline model including
this simplified version of the MSF module is reported in V5
of Table III. It is to be noted that 2.3% improvement of
dice score is achieved in Database-1 and 3.4% improvement
is achieved in Database-2 over the baseline model using
these simplified MSF modules, and these improvements are
statistically significant (p < 0.01). However, 3.2% and 5.3%
reduction of dice scores can be noticed in Database-1 and
2, respectively, from the baseline model with original MSF
modules (V6) incorporating PF scheme. Similarly, considerable improvement is also achieved for the incorporation of
3D-pyramid fusion scheme in the 3D variants of MSF module
which can be noticed from V 53D and V 63D in Table IV. It

justifies the effectiveness of the pyramid fusion scheme in the
MSF module.
4) Effects of vertical and horizontal scaling: The proposed
CovSegNet architecture is designed in a modular way with
the opportunity for both vertical and horizontal expansions
for integrating more number of levels and stages, respectively.
In Table VII, the performances of the CovSegNet architecture
with different numbers of levels and stages are provided. It
should be noticed that the optimum dice score of 91.1% is
obtained for CovSegNet2D with 5-levels and 2-stages. The
best performance on single stage implementation is found
to be 86.7%, which is 4.4% lower than the best of the 2stage implementation. Similar analyses have been carried out
on CovSegNet3D using volumetric data where the highest
dice score of 92.3% is achieved with 3-levels and 2-stages
implementation. Moreover, when more stages are included,
comparably higher performances are obtained in a lower
number of levels, e.g. best dice score of 90.8% in the 3stage setup of CovsegNet2D has been achieved with 4-levels.
With the horizontal expansion, the model gathers more amount
of contextual information in a lower number of stages that
result in higher performances. However, more expansion in
both directions starts to increase the complexity that causes a
decrease in performance due to overfitting issues.
5) Effects of the hybrid 2D-3D joint optimization scheme
with two-phase training: The proposed 2-phase training
scheme exploits the advantages of both the slice-based optimization and volumetric optimization. Quantitative performances obtained using CovSegNet2D, CovSegNet3D, and

13

the hybrid scheme are provided in Table V and VI. Slice
based processing provides the advantages of employing deeper
networks for lighter 2D-convolutions, while loses the contextual information from z-axis. On the other hand, volumetric
analysis increases the computational burden for optimization
for 3D-kernels processing while providing more contextual
information. The best variant of CovSegNet3D provides 1.2%
higher dice score, and 0.8% higher IoU score over the best
variant of CovSegNet2D. Thus, the performances of the
proposed CovSegNet architectures are quite comparable in
both 2D and 3D processing with minor variations. It should
be noted that by combining the advantages of both these
schemes in the proposed multi-phase training approach, 3%
and 1.8% higher dice scores are achieved compared to the best
performing CovSegNet2D and CovSegNet3D architectures,
respectively. Moreover, to reduce the computational burden
of 3D-data processing in the hybrid scheme, only 2-level,
dual-stage implementation of the CovSegNet3D is employed
accompanied by the 4-level, dual-stage implementation of the
CovSegNet2D that provides the optimal performance with
minimal complexity. This improvement signifies the effectiveness of the hybrid networking scheme in multi-phase training
(p < 0.01). Moreover, qualitative analysis of the performances
of the individual networks and hybrid networks are presented
in Fig. 10 with different levels of infection. It should be
noticed that both of the false positive and false negative regions
are reduced in the segmented mask for the hybrid scheme
compared to the individual networks.
6) Effects of the loss functions: In Table X, effect of
different loss functions are summarized on the performance of
the CovSegNet. For optimizing the hybrid network, joint optimization objective function (Eqn. 18) is defined incorporating
losses of the CovSegNet2D and CovSegNet3D networks. It
should be noticed that focal Tversky loss function provides
0.9% improvement of dice score over traditional dice loss
function and 0.7% improvement over the the aggregated dice
loss and binary cross entropy loss function. Similar improvement is also achieved for CovSegNet3D and CovSegNethybrid network. However, the proposed CovSegNet architecture mostly provides stable performance over different
traditional loss functions, though the optimum performance
is achieved with the focal Tversky loss for higher emphasis
on the hard training examples.
D. Comparison with Other Existing Approaches
To compare the performances of the proposed CovSegNet
architecture, several state-of-the-art networks are considered.
To compare on a fair platform, most of these networks are
implemented using their open-source implementation, and
similar train-test folds are used for performance evaluation.
Infection segmentation performances using slice-based 2Doperations and volumetric 3D-operations are summarized in
Table V and VI, respectively. CovSegNet2D provides a 4.2%
higher dice score in Database-1, and an 8.6% improvement
in dice score in Database-2 compared to the second-highest
score (Semi-Inf-Net). Hence, consistent improvements in performances have been achieved in 2D-slice based analysis

Fig. 12. Visual representations of the semantic segmentation of drone images from Database-3 obtained using different state-of-the-art networks.

using CovSegNet2D. Moreover, in the volumetric analysis
approach, CovSegNet3D provides an 8.4% higher dice score
and 9.4% higher IoU score compared to the next-best performing model (VNet). Thus, the 3D-variant of CovSegNet
provides consistent improvements over other 3D-counterparts
of existing networks. It should be noticed that the proposed
hybrid scheme combining CovSegNet2D and CovSegNet3D
provides the most optimum performance with a dice score
of 94.1% and IoU score of 90.2%. Some of the qualitative
visualizations of performances obtained in different challenging conditions are shown in Fig. 9. For having the volumetric
information of the Database-1, the proposed hybrid scheme is
employed here, while only 2D-slice based analysis is carried
out in Database-2 using CovSegNet2D. It should be noted that
the proposed scheme performs consistently better compared
to other networks in segmenting most of the challenging
diffused, blurred, and varying shaped edges of COVID lesions.
Moreover, quantitative performances on challenging multiclass lesion segmentation, including separate ground-glass
opacity (GGO) and consolidation regions, are summarized
in Table VIII, where 8.2% improvement in dice score is
obtained in GGO segmentation and 11% improvement in
consolidation segmentation using CovSegNet architecture over
the other best-performing approaches. Additionally, from the
visual analysis of the performances shown in Fig. 11, it can be
easily noted that the proposed network considerably reduces
the false predictions even in these challenging conditions
compared to other state-of-the-art approaches.
Furthermore, quantitative results obtained from non-clinical
Database-3 are summarized in Table IX which shows the significant performance improvement with 22.4% improvement
in dice score, and 21.8% improvement in mean IoU compared
to the Unet architecture. Weighted mean performances over all
20 classes are taken for better estimation. In Fig. 12, visual
representations of some of the sample images are shown for
different networks in Database-3, which more conspicuously
signifies the better performance of the proposed architecture.
Since Database-3 is very complicated with a huge number
of classes, the performance differences between the proposed

14

TANVIR MAHMUD et al.: COVSEGNET ARCHITECTURE

TABLE XI
C OMPUTATIONAL E FFICIENCY A NALYSIS OF N UMEROUS A RCHITECTURES ALONG WITH THE P ERFORMANCES O BTAINED ON DATASET-1

Unet [22]
Semi-Inf-Net [12]
Unet++ [28]
CPF-Net [30]
CovSegNet2D-v1 (ours)
CovSegNet2D-v2 (ours)
CovSegNet2D-v3 (ours)

2-Dimensional Analysis
Total
GPU
Parameters(M)
Usage(GB)
31.0
2.1
33.3
6.8
27.0
6.5
32.4
2.3
L-2, S-2
0.37
1.1
L-3, S-2
1.60
1.8
L-4, S-2
6.70
3.3

CovSegNet2D-v4 (ours)

L-5, S-2

Architecture

Details

27.0

Inference
Time(s)
0.10
0.18
0.17
0.12
0.05
0.07
0.11

Mean
Dice(%)
82.3
86.9
84.1
84.4
75.3
85.8
89.6

0.20

91.1

7.0

CovSegNet and other existing networks are more prominent as
this dataset demands effective exploitation of minute, complex,
and scattered features of diversified classes.
E. Computational Efficiency Analysis of Numerous Approaches
The proposed CovSegNet architecture ensures the proper
optimization of all the network parameters through improved
parallelization that enhances efficient gradient propagation
in the whole network resulting in the effective exploitation
of the contextual information with consistently good performance. However, this improved parallelism also poses some
computational burden for the effective exploitation of the
network parameters. In Table XI, the computational efficiency
of different networks are summarized, where performances
of different variants of CovSegNet is summarized based on
the number of levels (L) and stages (S). For analyzing with
2D data, it is noticeable that the number of parameters of
CovSegNet2D are considerably lower compared to other networks while providing a large improvement of performance.
For example, CovSegNet-v2 provides a 94.8% reduction in
parameter counts of Unet architecture, while providing 1.43x
higher inference speed with a 3.5% higher dice score. With
increasing levels, more precise estimation is achievable in
the cost of speed and memory consumption. Moreover, GPUmemory usage for training with batch size-1 are summarized
for different networks. A similar observation can be carried
out for 3D analysis with CovSegNet3D. It should be noticed
that CovSegNet-Hybrid provides the best achievable dice score
(94.1%) while consisting of 0.09x parameters of Unet3D with
0.08s reduction of inference time. This significant reduction
in parameter counts with the obtained highest performance
is mainly achieved by the joint integration of the efficient
2D processing with effective inter-slice contextual information
exploration using a lighter 3D variant of CovSegNet. Therefore, this hybrid scheme provides considerable advantages over
other existing 3D variants in terms of parameters, and dice
scores with comparable processing speed.
F. Discussions, Limitations, and Future Studies
In summary, numerous architectural renovations assist in
achieving state-of-the-art performance on COVID lesion segmentation. The horizontal and vertical expansion mechanisms
provide the opportunity to incorporate more detailed features
as well as more generalized features, which improved the

Architecture
Unet3D [22]
Vnet3D [19]
MultiResUnet3D [27]
Attention Unet3D [29]
CovSegNet3D-v1 (ours)
CovSegNet3D-v2 (ours)
CovSegNet3D-v3 (ours)
CovSegNet
Hybrid (ours)

3-Dimensional Analysis
Total
GPU
Parameters(M)
Usage(GB)
90.3
13.2
45.1
15.1
18.1
12.9
103.5
20.9
L-2, S-2
1.1
7.0
L-3, S-2
4.6
13.7
L-4, S-2
19.0
22.2
2D(L-4, S-2)
7.8
10.5
3D(L-2,S-2)
Details

Inference
Time(s)
1.22
1.16
1.15
1.13
1.02
1.21
1.85

Mean
Dice(%)
84.2
85.7
84.5
85.9
79.8
89.2
92.3

1.14

94.1

feature quality considerably that is particularly effective in distinguishing multi-class, scattered COVID lesions with widely
varied shapes. Moreover, the improved gradient flow throughout the network, achieved with the introduction of multiscale fusion module and scale transition modules, have greatly
reduced the contextual information loss in the generalization
process and have also ensured the best optimization of all
network parameters that particularly contribute to recover and
distinguish the blurry, diffused edges of COVID lesions as well
as the very minute instances of abnormalities. Furthermore,
the integration of a hybrid 2D-3D networking scheme exploits both the intra-slice and inter-slice contextual information
without increasing computational burden that results in more
precise, finer segmentation performance mostly in challenging
conditions.
Although consistent performances have been achieved in
both the datasets for COVID lesion segmentation, this study
should be carried on larger datasets consisting of wide variations of subjects. However, in the current conditions of
the pandemic, it is difficult to gather a considerably higher
amount of data. The proposed study will be extended with the
incorporation of diversified datasets including patient-based
study considering age, sex, health conditions, and geographical
locations of the patients. Due to the novel characteristics of
the COVID infections, it is difficult to predict the risk and
vulnerability among diverse subjects that can be effective
for reducing the spread and better prevention. An in-depth,
closer, patient-specific study should be carried out for better understandings of the nature of the infection. Moreover,
generative adversarial network-based optimization can be carried out to generate more amount of realistic, synthetic data
to overcome the limitations of available data. Additionally,
this scheme is supposed to be extended for incorporating
automated segmentation-classification joint optimization along
with the severity prediction scheme of COVID infections.
IV. C ONCLUSION
In this study, an automated scheme is proposed with an efficient neural network architecture (CovSegNet) for very precise
lung lesion segmentation of COVID CT scans that provides
outstanding performances with 8.4% average improvement of
dice score over two datasets. The introduced scale transition
operations are found to be very effective for replenishing contextual information loss through repeated integration of generated multi-scale features in both upscaling and downscaling
operations. It is found that horizontal expansion mechanism
with multi-stage encoder-decoder modules assists in further

15

improvements for gathering more multi-scale contextual information when coupled with the traditional vertical expansion
mechanism. Moreover, the multi-scale fusion module with
a pyramid fusion scheme not only substantially reduced the
semantic gaps between subsequent encoder-decoder modules
but also introduced parallel inter-linking among multi-scale
features that greatly mitigates the vanishing gradient issues for
better optimization. Furthermore, the two-phase optimization
scheme with hybrid 2D-3D processing provides considerable
improvement over traditional single domain approaches for
introducing more contextual information to gather finer details.
It is shown that the proposed scheme is capable of segmenting
infected regions along with multi-class COVID-19 lesions
with unprecedented precision even in challenging conditions
with blurred, diffused, and scattered edges. Moreover, it is
found that the proposed network is not only effective in
COVID lesion segmentation but also provides state-of-theart performance on a non-clinical, challenging, multi-class
semantic segmentation task that proves the wide applicability
of the proposed scheme. Therefore, the proposed scheme can
be easily optimized on numerous applications that can be an
effective alternative to other state-of-the-art approaches.
R EFERENCES
[1] V. Surveillances, “The epidemiological characteristics of an outbreak
of 2019 novel coronavirus diseases (COVID-19)—China, 2020,” China
CDC Weekly, vol. 2, no. 8, pp. 113–122, 2020.
[2] C. Wang, P. W. Horby, F. G. Hayden, and G. F. Gao, “A novel
coronavirus outbreak of global health concern,” The Lancet, vol. 395,
no. 10223, pp. 470–473, 2020.
[3] W.-j. Guan, Z.-y. Ni, Y. Hu, W.-h. Liang, C.-q. Ou, J.-x. He, L. Liu,
H. Shan, C.-l. Lei, D. S. Hui et al., “Clinical characteristics of coronavirus disease 2019 in china,” New England journal of medicine, vol.
382, no. 18, pp. 1708–1720, 2020.
[4] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, Q. Tao, Z. Sun, and
L. Xia, “Correlation of chest CT and RT-PCR testing in coronavirus
disease 2019 (COVID-19) in china: a report of 1014 cases,” Radiology,
p. 200642, 2020.
[5] H. Kim, H. Hong, and S. H. Yoon, “Diagnostic performance of CT and
reverse transcriptase-polymerase chain reaction for coronavirus disease
2019: a meta-analysis,” Radiology, p. 201343, 2020.
[6] L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, J. Bai, Y. Lu, Z. Fang,
Q. Song et al., “Artificial intelligence distinguishes COVID-19 from
community acquired pneumonia on chest CT,” Radiology, 2020.
[7] R. Vaishya, M. Javaid, I. H. Khan, and A. Haleem, “Artificial intelligence
(AI) applications for COVID-19 pandemic,” Diabetes & Metabolic
Syndrome: Clinical Research & Reviews, 2020.
[8] P. Huang, T. Liu, L. Huang, H. Liu, M. Lei, W. Xu, X. Hu, J. Chen, and
B. Liu, “Use of chest CT in combination with negative RT-PCR assay
for the 2019 novel coronavirus but high clinical suspicion,” Radiology,
vol. 295, no. 1, pp. 22–23, 2020.
[9] T. Mahmud, M. A. Rahman, and S. A. Fattah, “CovXNet: A multidilation convolutional neural network for automatic COVID-19 and other
pneumonia detection from chest x-ray images with transferable multireceptive feature optimization,” Computers in Biology and Medicine, p.
103869, 2020.
[10] Y. Oh, S. Park, and J. C. Ye, “Deep learning COVID-19 features on
CXR using limited training data sets,” IEEE Transactions on Medical
Imaging, 2020.
[11] H. Kang, L. Xia, F. Yan, Z. Wan, F. Shi, H. Yuan, H. Jiang, D. Wu,
H. Sui, C. Zhang et al., “Diagnosis of coronavirus disease 2019 (COVID19) with structured latent multi-view representation learning,” IEEE
Transactions on Medical Imaging, 2020.
[12] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, and
L. Shao, “Inf-net: Automatic COVID-19 lung infection segmentation
from CT images,” IEEE Transactions on Medical Imaging, 2020.
[13] N. Saeedizadeh, S. Minaee, R. Kafieh, S. Yazdani, and M. Sonka,
“COVID TV-UNet: Segmenting COVID-19 chest CT images using
connectivity imposed u-net,” arXiv preprint arXiv:2007.12303, 2020.

[14] T. Zhou, S. Canu, and S. Ruan, “An automatic COVID-19 CT segmentation network using spatial and channel attention mechanism,” arXiv
preprint arXiv:2004.06673, 2020.
[15] Q. Yan, B. Wang, D. Gong, C. Luo, W. Zhao, J. Shen, Q. Shi,
S. Jin, L. Zhang, and Z. You, “COVID-19 chest CT image
segmentation–a deep convolutional neural network solution,” arXiv
preprint arXiv:2004.10987, 2020.
[16] Y. Qiu, Y. Liu, and J. Xu, “Miniseg: An extremely minimum network
for efficient COVID-19 segmentation,” arXiv preprint arXiv:2004.09750,
2020.
[17] J. Ma, Y. Wang, X. An, C. Ge, Z. Yu, J. Chen, Q. Zhu, G. Dong, J. He,
Z. He et al., “Towards efficient COVID-19 CT annotation: A benchmark
for lung and infection segmentation,” arXiv preprint arXiv:2004.12537,
2020.
[18] D. Müller, I. S. Rey, and F. Kramer, “Automated chest CT image
segmentation of COVID-19 lung infection based on 3d U-Net,” arXiv
preprint arXiv:2007.04774, 2020.
[19] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
neural networks for volumetric medical image segmentation,” in 2016
fourth international conference on 3D vision (3DV). IEEE, 2016, pp.
565–571.
[20] L. Zhang, J. Zhang, P. Shen, G. Zhu, P. Li, X. Lu, H. Zhang, S. A. Shah,
and M. Bennamoun, “Block level skip connections across cascaded
V-Net for multi-organ segmentation,” IEEE Transactions on Medical
Imaging, 2020.
[21] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2015, pp. 3431–3440.
[22] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference on
Medical image computing and computer-assisted intervention. Springer,
2015, pp. 234–241.
[23] F. I. Diakogiannis, F. Waldner, P. Caccetta, and C. Wu, “Resunet-a: a
deep learning framework for semantic segmentation of remotely sensed
data,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 162,
pp. 94–114, 2020.
[24] M. Al Ghamdi, M. Abdel-Mottaleb, and F. Collado-Mesa, “DU-Net:
Convolutional network for the detection of arterial calcifications in
mammograms,” IEEE Transactions on Medical Imaging, 2020.
[25] Z. Zeng, W. Xie, Y. Zhang, and Y. Lu, “RIC-Unet: An improved neural
network based on Unet for nuclei segmentation in histology images,”
IEEE Access, vol. 7, pp. 21 420–21 428, 2019.
[26] L. Mou, L. Chen, J. Cheng, Z. Gu, Y. Zhao, and J. Liu, “Dense dilated
network with probability regularized walk for vessel detection,” IEEE
Transactions on Medical Imaging, vol. 39, no. 5, pp. 1392–1403, 2019.
[27] N. Ibtehaz and M. S. Rahman, “MultiResUNet: Rethinking the u-net
architecture for multimodal biomedical image segmentation,” Neural
Networks, vol. 121, pp. 74–87, 2020.
[28] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:
Redesigning skip connections to exploit multiscale features in image
segmentation,” IEEE Transactions on Medical Imaging, vol. 39, no. 6,
pp. 1856–1867, 2019.
[29] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,
K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz et al., “Attention U-net: Learning where to look for the pancreas,” arXiv preprint
arXiv:1804.03999, 2018.
[30] S. Feng, H. Zhao, F. Shi, X. Cheng, M. Wang, Y. Ma, D. Xiang, W. Zhu,
and X. Chen, “CPFNet: Context pyramid fusion network for medical
image segmentation,” IEEE Transactions on Medical Imaging, 2020.
[31] S. S. M. Salehi, D. Erdogmus, and A. Gholipour, “Tversky loss function
for image segmentation using 3d fully convolutional deep networks,”
in International Workshop on Machine Learning in Medical Imaging.
Springer, 2017, pp. 379–387.
[32] N. Abraham and N. M. Khan, “A novel focal tversky loss function with
improved attention u-net for lesion segmentation,” in 2019 IEEE 16th
International Symposium on Biomedical Imaging (ISBI 2019). IEEE,
2019, pp. 683–687.
[33] “COVID-19 CT lung and infection segmentation dataset,” 2020, [online].
Available: https://doi.org/10.5281/zenodo.3757476.
[34] “COVID-19 CT segmentation dataset,” 2020, [online]. Available: https:
//medicalsegmentation.com/covid19/.
[35] “Semantic drone dataset,” 2019, [online]. Available: https://www.tugraz.
at/index.php?id=22387/.

