Robust Stochastic Stability with Applications to Social Distancing in a Pandemic

arXiv:2103.13475v1 [cs.MA] 24 Mar 2021

Brandon C. Collins, Lisa Hines, Gia Barboza, and Philip N. Brown

Abstract— The theory of learning in games has extensively
studied situations where agents respond dynamically to each
other in light of a fixed utility function. However, in many
settings of interest, agent utility functions themselves vary
as a result of past agent choices. The ongoing COVID-19
pandemic has highlighted the need to formulate and analyze
such models which feature game-environment feedback. For
instance, a highly prevalent virus may incentivize individuals
to wear masks, but extensive adoption of mask-wearing reduces
virus prevalence which in turn reduces individual incentives for
mask-wearing. What is the interplay between epidemic severity
and the behaviors of a victim population? For initial answers,
we develop a general framework using probabilistic coupling
methods that can be used to derive the stochastically stable
states of log-linear learning in certain games which feature such
game-environment feedback. We then apply this framework to
a simple dynamic game-theoretic model of social precautions
in an epidemic and give conditions under which maximallycautious social behavior in this model is stochastically stable.

I. I NTRODUCTION
In social systems and distributed engineered systems,
collective behavior is the result of many individuals making
intertwined self-interested choices. In many cases, the value
of a particular choice depends not only on the current choices
being made by others, but also on the history of past choices.
The recent COVID-19 pandemic exemplifies this. In particular, many social conventions such as wearing masks
and social distancing are known to be effective at reducing
the contagiousness of the disease [1], [2]. To effectively
deploy mitigation policies, it is critical to understand how
a population’s willingness to adopt preventative conventions
interacts dynamically with the severity of an epidemic. For
example, in the absence of an epidemic a population may
prefer not to practice social distancing, but rising case counts
may incentivize individuals to change their behavior to avoid
contracting the disease. However, an epidemic’s severity at
any given time is not only a function of its own dynamics,
but also the behavioral history of the victim population. The
resulting feedback loop may lead to challenges in predicting
the effectiveness of mitigation policies.
In principle, these socio-environmental feedback loops
can be analyzed using techniques from game theory, which
has a long history of analyzing the society-scale effects of
self-interested behavior. For instance, game theory has long
been used to study the spread of social conventions [3]
using models such as the graphical coordination game [4]
This work was supported by the National Science Foundation under
Grants #DEB-2032465 and #ECCS-2013779.
The authors are with the University of Colorado at Colorado
Springs, CO 80918, USA {bcollin3,lhines,gbarboza,

philip.brown}@uccs.edu

with the stochastic learning algorithm log-linear learning [5].
However, traditional analysis techniques almost uniformly
assume that the game’s utility functions are fixed for all time,
so that the agents’ choices over time can be described by a
stationary Markov process. However, such analysis fails or
becomes unwieldy when utility functions themselves depend
on the history of play.
Analysis techniques for history-dependent games have
broad potential applications; for instance, game theoretic
methods are frequently proposed in the area of distributed
control of multiagent systems [6]–[9]. However, in a distributed control application, agents’ actions may directly
modify the strategic environment; for instance if a searchand-rescue UAV identifies a disaster victim, that victim may
be removed from the list of other UAVs’ objectives. Other
applications that can be modeled by history-dependent games
are in machine learning [10]–[12] and biology [13], [14].
Owing in part to the challenges of modeling the complex
game-environment feedback inherent to history-dependent
games, general results on these games are elusive. Recent
work has focused on specific learning algorithms and strategic environments, such as zero-sum games under replicator
dynamics [15]. In [16] the authors characterize an oscillating
tragedy of the commons effect under certain environmental
feedback scenarios.
In this paper, we develop a general framework for analyzing the long-run behavior of binary-action history-dependent
games. In particular, we study the stochastically stable states
of the popular log-linear learning algorithm in such settings.
We show that if the utility functions of the history-dependent
game can be appropriately referenced to the utility functions
of a corresponding exact potential game, then the historydependent game of interest inherits the stochastically stable
states of the reference potential game. To accomplish this we
apply techniques from the theory of probabilistic couplings,
and derive a monotone coupling that relates play in the
history-dependent game with that in the reference potential
game. To showcase an application of the framework, we
present an epidemic model that intertwines the compartmental SIS disease model with the graphical coordination game
convention model. Using our analysis framework we provide
conditions under which the stochastically stable states may
be fully characterized, despite their history-dependence.
A. A Motivating Epidemic Example
The novel COVID-19 epidemic has led to a surge in
interest in understanding social responses to epidemics [17]–
[19]. To model such a scenario we adopt a game theoretic
model of behavior intertwined with a compartmental disease

|N |

1 action profile as ~1 = (1)i=1 and similarly for the all zero
profile, ~0. Further, let ∆(A) denote the standard probability
simplex over A.
Let Ui : A → R be player i’s utility function. We denote
U = {U }i∈N as the collection of all players utility function.
Thus we may denote a game using tuple (N, A, U ), and let
G be the set of all such tuples.
A game g ∈ G is an exact potential game if there exists a
potential function φ such that
Ui (a0i , a−1 ) − Ui (ai , a−i ) = φ(a0i , a−1 ) − φ(ai , a−i ) (1)

Fig. 1. Two runs of the SISGCG model. The full description can be found in
Section IV however a brief description of the parameters is as follows. The
infectiousness of an agent using no precautions is β0 and using precautions
is β1 . The curing parameter, determining how fast agents recover, is γ.

model. Specifically, we couple a graphical coordination game
with a compartmental SIS epidemic model. The coordination
game is a model for how social conventions spread in society
[3], using the assumption that one gains intrinsic value to
using the same conventions as others around them. In this
model, which we call SISGCG, agents choose whether or
not they take precautions by considering both a desire to
coordinate on a set of social distancing conventions and
a desire to practice safe conventions, influenced by the
severity of the epidemic. The dynamics and thus severity of
the epidemic are then in turn impacted by agent’s decision
to adopt precautions. We present the complete details of
the SISGCG model in Section IV. The upper subplot of
Figure 1 showcases how SISGCG captures the complex
interplay between epidemic dynamics and social response.
In particular, epidemic peaks in this model occur due to
recent history of relatively many agents choosing not to
take precautions. In addition, both social conventions and the
epidemic take time to spread in SISGCG, so SISGCG models
a complex relationship between delayed game theorectic
social response, reactive epidemic dynamics, and the curing
time of the disease.
The lower subplot of Figure 1 showcases a scenario in
which our analysis framework can be used to characterize
agent behaviour using stochastic stability. Note that the lower
subplot depicts the case that social distancing is desirable
even when the disease is at a lower-prevalence state and thus
would also be desirable at any higher disease state.
II. M ODEL
A. Game Formulation
In this work we consider binary action games. Letting
N = {1, 2, 3, . . . , |N |} denote the player set, in a binary
action game player i ∈ N has action set Ai = {0, 1}.
The joint action space is then given by A = {0, 1}|N | . We
denote an action profile as a ∈ A and use ai to denote
player i’s action and the actions of all other players by
a−i = (a1 , a2 , . . . , ai−1 , ai+1 , . . . , a|N | ). We refer to the all

for any a ∈ A, and ai , a0i ∈ Ai .
However, games g ∈ G cannot be used to model scenarios
where play interacts with a dynamic environment. That is,
the payoffs of such games cannot depend on the history of
play that may impact the players’ strategic environment. In
this work we generalize G by allowing each history of play
to have a unique utility function. We write AT to denote the
set of joint action histories of length T ∈ N, and the set of
all histories as A = ∪T ∈N AT . We typically use α ∈ AT to
refer to a path and use superscripts to refer to time indices.
For example, α1 ∈ A is the first action profile in the history
and αT is the last, noting we use the convention that T is
the last time index even when α ∈ A. We also define A, AT
as partially ordered sets by first defining partial order ≥A ,
where a0 ≥A a whenever a, a0 ∈ A and a0i ≥ ai for all i ∈
N , recalling that a0i , ai ∈ {0, 1}. Using this we define partial
order ≥AT as ᾱ ≥ α whenever α, ᾱ ∈ AT and ᾱt ≥A αt
for all t ∈ {1, 2, ..., T }.
Using these notions we generalize the previously given
game formulation G to allow for the utility functions to
depend on the history of play. Similar to before, let Uiα :
A → R, where this utility function is not only specific
player i but also to the history α. As before, let U α =
α
(U1α , U2α , ..., U|N
| ) denote each player’s utility function given
history α and let U A = {U α | α ∈ A} be the set of utility
functions across all paths. We denote a history-dependent
game as tuple (N, A, U A ) and let G A be the set of all
such tuples. We now present a class of games that combines
potential games and history dependence.
Definition 1: We call a tuple g = (N, A, U A ) ∈ G A
an aligned history-dependent game if there exists an exact
potential game ĝ = (N, A, Û ) ∈ G with potential function φ̂
such that:
1) {~1} = arg maxz∈A φ̂(z)
T
2) Uiα (1, α−i
) ≥ Ûi (1, a−i )
T
3) Ûi (0, a−i ) ≥ Uiα (0, α−i
)
T
for any α, ∈ A, a, a0 ∈ A, T ∈ N such that α−i
≥A−i a−i
0
and a, a vary by only a unilateral deviation. For convenience
we denote ordering ≥A−i over A−1 = {0, 1}|N |−1 equivalently to ≥A .

B. Learning in Games
In this work we focus on the learning algorithm log-linear
learning, which is a discrete time asynchronous learning
algorithm [5], [20]. That is, for game g ∈ G A at each time

step log-linear learning will select a single agent to update
their action to ai with probability
We define the probability that agent i selects action ai
given history α ∈ AT as
α
T
1
e τ Ui (ai ,α−i )
.
(2)
Pα
(a
)
=
i
P
1
T
α
i
τ Ui (ai ,α−i )
a0i ∈Ai e
where τ , called the temperature is a parameter which governs
the rationality of agents. As τ → 0 agents will best respond
with high probability, and as τ → ∞ agents will choose
actions uniformly at random. Note that we take the last action
profile in the history αT as the behavior of the other agents.
We define the probability α ∈ AT transitions to a0 ∈ A
under log-linear
in a single transition as
 1learning
P
T
0
0
 |N | j∈N Pα
j (aj ) α = a

α 0
1
α
0
T
T
P (a ) = |N | Pi (ai )
αi 6= a0i , α−i
= a0−i (3)


0
else.
This can be interpreted as the probability that given history
α the next action profile αT +1 = a0 .
We say a ∈ A is strictly stochastically stable if the
following  definition [21] holds. For any  > 0 there exists
T > 0, T < ∞ such that such that
Pr(s(t; τ, π, g) = ~1) > 1 −  whenever t > T, τ < T . (4)
where s(·) is a random variable representing the action
profile at time t under log-linear learning, given temperature
τ , initial distribution π ∈ ∆(A) and game g.
Traditionally, exact potential games under log-linear learning may be analyzed using a theory of resistance trees [3],
[5], [20], [22] to relate potential function maximizers to
stochastic stability. However, this analysis depends on the
fact that log-linear learning induces an ergodic markov chain
from any exact potential game. It is easy to show that when
log-linear learning is applied to game g ∈ G A the induced
random process is not Markovian in general. In particular,
the Markov property requires that at any time determining
the next state does not depend on the past, however the
underlying utility functions of g explicitly depend on history
α, and therefore the associated transition given in (2) violates
the Markov property. Thus the traditional technique for
showing stochastic stability in this context fails.
III. M AIN C ONTRIBUTION
We now present our main result, extending stochastic
stability to the class of games we term aligned historydependent games. In particular, we show that for such games
the ~1 state is strictly stochastically stable.
Theorem 1: If g ∈ G A is an aligned history-dependent
game then ~1 is strictly stochastically stable in g under loglinear learning.
The proof of Theorem 1 proceeds using Lemma 1, which
we present here and prove in Section V. The interpretation
of this lemma is that for an aligned history dependent game
g, the probability at any time step that g is in the ~1 action
profile is lower bounded by the probability its associated
exact potential game ĝ is in the ~1 profile.
Lemma 1: If g ∈ G A is an aligned history-dependent
game with associated exact potential game ĝ ∈ G then

Pr(s(T ; τ, π, g) = ~1) ≥ Pr(s(T ; τ, π, ĝ) = ~1) for any
temperature τ > 0, π ∈ ∆(A).
The proof of Lemma 1 is technically involved and depends on our novel monotone coupling framework which
we present in Section V. Using this result we now present a
straightforward proof of Theorem 1.
Proof of Theorem 1: Let g ∈ G A be an aligned historydependent game and ĝ be its associated exact potential game.
It is well-known [20] that in an exact potential game a ∈ A
is a stochastically stable state under log-linear learning if
a ∈ arg max φ(a0 ).
(5)
a0 ∈A

Therefore, because ~1 is the lone maximizer of φ̂, it is strictly
stochastically stable. We apply Lemma 1 directly to the
definition of strict stochastic stability in (4). For any  > 0
there exists T > 0, T < ∞ such that such that
Pr(s(t; τ, π, g)) ≥ Pr(s(t; τ, π, ĝ)) > 1 −  ∀t > T, τ < T
(6)
yielding stochastic stability of ~1 in game g.

IV. A PPLICATION TO S OCIAL D ISTANCING IN A
PANDEMIC
A. SIS Preliminaries
It is appropriate to model epidemics that do not grant
long term immunity with the SIS compartmentalized model.
Here, we assume that every member of the population is in
one of two states, susceptible (S) or infected (I). Individuals
transition from susceptible to infected with rate β > 0, and
infected to susceptible with rate γ > 0. The fraction s(t)
of the population that is susceptible at time t evolves as a
function of time according to the one-dimensional nonlinear
ODE
ṡ = −βs(1 − s) + γ(1 − s).
(7)
The solutions to (7) when s(0) ∈ [0, 1] are fully characterized; in particular, it is known that when β/γ > 1,
there exists an asymptotically stable equilibrium (called the
endemic equilibrium) s∗ = γ/β, and that every solution
to (7) with s(0) < 1 converges to s∗ . For more information,
the reader may reference the survey [23].
B. The Graphical Coordination Game
One model for the spread of conventions in society is the
Graphical Coordination Game (GCG) [4]. In this networked
game, each agent plays the following two-player coordination game with each of her neighbors:
1
0
(8)
u(ai , aj ) = 1 q + c, q + c 0, 0
0
0, 0
1, 1
where ai ∈ {0, 1} is taken to be the row player’s action
and aj is the column player’s action, constant q ∈ (0, 1]
is a base coordination incentive and c ≥ 0 is the payoff
gain, indicating the benefit of coordinating on 1 over 0. The
graphical coordination game is played between |N | players
on an undirected graph G = (N, E) where E is the edge set.
We define the neighbor set of any agent i as Ni = {j ∈ N |
(i, j) ∈ E, i 6= j}. In the GCG an agent’s utility function for
action profile a ∈ A is given by

X

ui (a) =

u(ai , aj ).

(9)

j∈Ni

When c is a constant it is well understood that the GCG
in this formulation denotes an exact potential game. For
convience let E1 (a) ⊆ E denote the set of edges whose
agents are coordinated on action 1, and likewise for E0 (a).
The potential function is then given by
φ(a) = (1 + c)|E1 (a)| + |E0 (a)|.
(10)
C. A Socially Aware Epidemic Model
This model which we call SISGCG is defined by the
following nonlinear hybrid dynamical system:
ṡ = (1 − s)(γ − β(t)s).
(11)
Here, β(t) is piecewise-constant and defined as follows: Let
{tk }k∈N ∈ [0, ∞) be an unbounded set of agent revision
opportunities. At each time tk , an agent is selected uniformly
at random to update her action, which she does according to
log-linear learning with probability (2) with utilities associated with the following graphical coordination game , where
we write I(t) := 1 − s(t), and write player i’s action at time
t as ati :
uSISGCG (ati , atj ) = u(ati , atj ) with c = I(t)

(12)

where q ∈ (0, 1] and represents how willing a population is
to practice safe conventions in the absence of an epidemic.
The action 1 represents a “safe convention” action in which
a player is acting to reduce contagion; the action 0 represents
conventions ignoring the pandemic. These actions are associated with infection coefficients 0 < β1 < β0 , respectively.
Note in the SISGCG model we define the coordination game
with log-linear learning in the same way as previously stated,
but the 2-player coordination game is played with uSISGCG
in place of u. Finally, β(t) is simply the average infection
rate of all individuals, given their choices:
1 X t
ai β1 + (1 − ati )β0 .
(13)
β(t) =
N
i∈N

Proposition 1: If s(0) ∈ [0, 1), then if s(t) is a solution
of (11) with β(t) given by (13), there exists a t̄ such that
s(t) ≤ γ/β1 for all t ≥ t̄ almost surely.
Proof: We write s∗1 := γ/β1 . Note that if s(t) ≥ s∗1 ,
then because β(t) ≥ β1 , we have that ṡ ≤ 0 by (11), and
that this inequality is strict whenever s(t) > s∗1 . Thus, the
set [0, s∗1 ] is positively invariant for the hybrid nonlinear
dynamics given in (11).
To see that s(t) eventually enters [0, s∗1 ] almost surely,
consider the event that s(t) > s∗1 for all t. Since s∗1 is
asymptotically stable when β(t) ≡ β1 and for any action
profile a 6= ~1 that its associated β(t) > β1 , it follows that
the event that β(t) ≡ β1 for all t is the same event as
s(0) > s∗1 and s(t) > s∗1 for all t. However, it can be seen
that the log-linear learning (3) action update probabilities
define a stochastic process which visits every action profile
in A infinitely often. That is, the probability that β(t) ≡ β1
is 0, and thus there must exist a t̄ such that s(t) ≤ s∗1 for all
t ≥ t̄ almost surely.
Immediately, it can be seen that this model exhibits
some desirable intuitions about how populations behave both

during and in the absence of a pandemic. Consider the case
with no pandemic present, that is I(0) = 0 which is a fixed
point of the SIS dynamical system. In this scenario it can
be seen that (12) is a constant payoff matrix, therefore this
instance of SISGCG denotes an exact potential game. It is
easy to see that ~0 uniquely maximizes the potential function
and therefore is strictly stochastically stable.
However, as a consequence of Proposition 1, if I(0) > 0
then I(t) > 0 for all t. It can be seen from (12) that SISGCG
can be represented by a history-dependent game, as the utility
function depends on the history of play. Therefore, agent
decision-making inherits this dependence on the history of
play, leading to non-Markovian behavior which complicates
traditional analysis techniques. However, our Theorem 1
allows us to reference SISGCG to a related exact potential
game and deduce conditions guaranteeing that ~1 is strictly
stochastically stable.
Proposition 2: Let g S be an instance of SISGCG. If
β1 /γ > 1, q + γ/β1 > 1 and I(0) > 0 then we have ~1
is stochastically stable in g.
Proof: To show stochastic stability of ~1 in the SISGCG
model we must:
1) show SISGCG is a history-dependent game,
2) show a corresponding fixed game ĝ exists, and
3) apply Theorem 1.
Let a SISGCG g S be played on graph G = (V, E) with q +
γ/β1 > 1 and I(0) > 0, and we consider g S as played after
time t̄ that exists almost surely as shown in Proposition 1.
It is easy to show SISGCG is a history-dependent game,
g S = (N, A, Ū ). Note that trivially both the player set N and
action set A match the history-dependent framework. To see
that the utility functions fit into the framework note that the
utility function, (12), depends on I(t) which represents the
proportion of agents who are infected. To find this constant
for some t ∈ N, it can be seen from (11) we require the
history of agent up until that point. Thus we have at time t
the utility functions Ū α depend on α ∈ At , which admits
readily into the history-depend framework. Thus we have
g S = (N, A, Ū ) ∈ G A
Now we let ĝ S = (N, A, Û S ) ∈ G be a GCG played on
graph G. We define the pairwise coordination game payoff
matrix by (8) with c = γ/β1 inducing that ĝ S denotes an
exact potential game.
We now use ĝ S to show g S is an aligned history-dependent
game. Because q + γ/β1 > 1 its easy to see that ~1 is the
lone maximizer of the the potential function. Now we verify
T
T
Uiα (1, α−i
) ≥ ÛiS (1, a−1 ) anytime α−i
≥A−i a−i , t > t̄.
This can be rewritten
for
t
>
t̄
as
X
X
q + I(t) ≥
q + γ/β1
(14)
j∈Ni1 (αT
−i )

j∈Ni1 (a−i )

where Ni1 (a−i ) denotes the neighbors of i who are playing
T
1 given profile a. This expressions holds because α−i
≥A−i
T
a−i ⇒ |Ni1 (α−i
)| ≥ |Ni1 (a−i )| and by Proposition 1 I(t) ≥
γ/β1 for all t > t̄. An argument with the same structure
T
holds for Uiα (0, α−i
) ≤ ÛiS (0, a−1 ). Thus g S is an aligned
history-dependent game and we apply Theorem 1 to discover
stochastic stability of ~1 as desired.

V. P ROOF VIA M ONOTONE C OUPLINGS
A. A Primer on Monotone Couplings
We begin with the definition of a monotone coupling, the
core analytical device for our paper. Using this we present
the definition of monotone coupling:
Definition 2: Let X be a countable set with partial ordering ≤X and p1 , p2 be probability measures on measure
space (X, F). Then a monotone coupling of p1 , p2 is a
probability measure p on (X 2 , F 2 ) satisfying the following
for all
x, y ∈ X
X
X
p(x0 , y) = p1 (x0 ).
p(x, y 0 ) = p2 (y 0 ) and
y≥X x0

x≤X y 0

(15)
A monotone coupling is a useful tool for analysis of the
component probability measures p1 and p2 . In particular the
following property holds in general for monotone couplings.
Proposition 3 (Paarporn et al., [24]): Let p1 , p2 be probability measures on (X, F). If p is a monotone coupling of
p1 , p2 then for any increasing random variable Z : X → Z+
we have
∞
X
Ep1 (Z) − Ep2 (Z) =
p(Zηc , Zη )
(16)
η=0

where Zη = {a | Z(a) > η}.
Where we denote a complement set of Z ⊂ X as Z c . The
proof is given in [24, Proposition 1].
B. Notation Required for Proofs
Taking ĝ ∈ G, we give equations analogous to (2), (3)
that give the transition probabilities for ĝ under log-linear
learning. In particular, if agent i is selected to update her
action then she will do so according to:
1
e τ Ui (ai ,a−i )
(17)
P̂ai (ai ) = P
1
0
τ Ui (ai ,a−i )
a0i ∈Ai e
Building on (17), we define the probability that action profile
a transitions to a0 under log-linear learning in a single
transition as
P
 |N1 | j∈N P̂aj (aj ) a = a0

P̂ a (a0 ) = |N1 | P̂ai (a0i )
ai 6= a0i , a−i = a0−i (18)


0
else
0
for some i ∈ N and a, a ∈ A. Additionally, we define the
probability that path α ∈ AT occurs with initial distribution
π ∈ ∆(A) as
TY
−1
t
P̂π (α) = π(α1 )
P̂ α (αt+1 )
(19)
t=1

noting that π(α1 ) denotes the probability of α1 in initial
distribution π.
Corrospondingly, the probability that path α ∈ AT occurs
with initial distribution π ∈ ∆(A) on g ∈ G A is
TY
−1
≤t
Pπ (α) = π(α1 )
P α (αt+1 )
(20)
t=1

where we use α≤t ∈ At to mean history α until time t ∈
{1, 2, 3, . . . , T }.
We now present a result connecting the utility conditions
of aligned history-varying potential games to (17) and (2).

Lemma 2: Let g = (N, A, U A ) ∈ G, ĝ = (N, A, Û ) ∈ G
T
and let i ∈ N, a ∈ A, α ∈ A such that α−i
≥A−i a−i . If
α
T
T
Ui (1, α−i ) ≥ Ûi (1, a−i ) and Ûi (0, a−i ) ≥ Uiα (0, α−i
) then
a
Pα
(1)
≥
P̂
(1).
i
i
Proof: Let g = (N, A, U A ) ∈ G, ĝ = (N, A, Û ) ∈
T
G and let i ∈ N, a ∈ A, α ∈ A be such that α−i
≥A−i
T
α
a−i . Further let Ui (1, α−i ) ≥ Ûi (1, a−i ) and Ûi (0, a−i ) ≥
T
Uiα (0, α−i
). Recalling τ > 0, we begin by considering Pα
i
e τ Ui

(1,αT
−i )

α (1,αT )
−i

+ e τ Ui

1

Pα
i (1)

=

1

e τ Ui

α

1

α (0,αT )
−i

(21)

1

≥

e τ Ûi (1,a−i )
1

1

e τ Ûi (1,a−i ) + e τ Ûi (0,a−i )

=

P̂ai (1).

To see the inequality, it suffices to apply the hypothesis to
x
the fact that ex and l(x) = exe+c are both increasing in x for
a
c > 0. Thus Pα
i (1) ≥ P̂i (1) holds as desired.
Our framework requires a careful partitioning of the action
space into several sets corresponding to different types of
agent action deviations. We use these formalizations for the
various cases of the monotone coupling given in Theorem 2
as seen in (23).
Let f : A → 2A be defined as f (a) = {a0 ∈ A | ai 6=
0
ai , a−i = a0−i for i ∈ N } be the set of action profiles
reachable from a via exactly one unilateral deviation. For
a, a0 ∈ A let


ai 6= a0i
i
(22)
g(a, a0 ) = 0
a = a0


0
−1 ai 6= ai , aj 6= aj , i 6= j
indicate which agent unilaterally deviated their action between action profiles a, a0 , and be −1 if multiple agents have
deviated.
Now, let a, a0 ∈ A where a0 ≥A a. We denote several
disjoint subsets of f (a):
1) r(a) = {z ∈ f (a) | ag(a,z) = 1},
2) q(a, a0 ) = {z ∈ f (a) | z ≤A a0 } \ r(a), and
3) s(a, a0 ) = f (a) \ (q(a, a0 ) ∪ r(a)).
These sets can be interpreted in the following way. The set
r(a) is the set of action profiles which decreased with respect
to ≥A and q(·), s(·) both increased. Between q(·) and s(·),
q(·)’s action profiles remain less than a0 and s(·)’s profiles
are greater then or incomparable to a0 . We now present three
more analogous sets that are disjoint subsets of f (a0 ):
1) R(a0 ) = {z ∈ f (a0 ) | a0g(a0 ,z) = 0},
2) Q(a, a0 ) = {z ∈ f (a0 ) | z ≥A a} \ R(a0 ), and
3) S(a, a0 ) = f (a0 ) \ (Q(a, a0 ) ∪ R(a)).
The interpretation of these sets are flipped relative to r(·),
q(·) and s(·).
We now highlight some useful features of these sets. By
their definitions it is evident that q(·), r(·), s(·) are a disjoint
partition of f (a), and that Q(·), R(·), S(·) are a disjoint
partition of f (a0 ). For any a, a0 , a0 ≥A a we relate these
0
0
sets by a function ba,a : f (a) → f (a0 ). To evaluate ba,a (ā),
identify the agent who deviated their action between a, ā and
0
then deviate that agent’s action in a0 . Formally, ba,a (ā) =

(¬a0g(a,ā) , a0−g(a,ā) ) where for convenience we define ¬ai ∈
{0, 1}\{ai } for ai ∈ Ai = {0, 1}. In particular, this function
relates the disjoint subsets of f (a), f (a0 ) according to the
following lemma.
Lemma 3: If a, a0 ∈ A and a ≤A a0 , then the following
statements hold:
0
1) ba,a : r(a) → S(a, a0 ) is a bijection,
0
2) ba,a : s(a, a0 ) → R(a0 ) is a bijection, and
0
3) ba,a : q(a, a0 ) → Q(a, a0 ) is a bijection.
Lemma 3 is proved in the Appendix.
C. The One-Step Couplings
To prove Lemma 1 and obtain Theorem 1, we construct
a monotone coupling νπĝ between measures Pπ , P̂π . Noting
that νπĝ is a coupling of measures over histories, we first
construct a family of monotone couplings for each one-step
transition (Theorem 2), and subsequently use these couplings
to build the desired coupling over histories (Theorem 3).
Theorem 2: Let g ∈ G A denote an aligned historydependent game and ĝ ∈ G be its associated exact potential
game. Then a monotone coupling exists between P̂ a and P α
for any α ∈ A, a ∈ A whenever a ≤A αT . This monotone
coupling ν a,α : A2 → [0, 1] is given in (23) in Figure 2.
Proof: Let a ∈ A, α ∈ A such that a ≤A αT and let
g ∈ G A be a aligned history-dependent game where ĝ ∈ G
is its associated exact potential game. To verify ν a,α is a
monotone coupling we must show the following conditions
from Definition 2 for any ā, ā0 ∈ A:
1) ν a,α
a well-defined probability measure,
P is a,α
2)
ν (ā, z 0 ) = P̂ a (ā), and
z 0P
≥A ā
3)
ν a,α (z, ā0 ) = P α (ā0 ).
z≤A ā0

We begin by verifying Condition 2. We consider cases
ā ∈
/ (f (a) ∪ {a}), ā ∈ q, ā ∈ r, ā ∈ s and ā = a separately.
We use the notational convention that q, s, Q, S are assumed
to take arguments (a, αT ) and r, R take the argument a, αT
respectively. The first case represents any ā that cannot be
achieved in a single unilateral deviation from a. Trivially, this
gives that P̂ a (ā) = 0, and thus all pairs of ā, z 0 must satisfy
ν a,α (ā, z 0 ) = 0. This holds as all parts of (23) require ā ∈
(f (a) ∪ {a}) except (23h), which has the desired property.
We now consider the second case that ā ∈ q. Note that
only (23d) satisfies this condition, so
X
ν a,α (ā, z 0 ) = ν a,α (ā, αT )
z 0 ≥A ā0
(24)
= P̂ag(a,ā) (āg(a,ā) )/|N | = P̂ a (ā)
as desired.
Next we consider ā ∈ r which satisfies (23c), (23f)
T
uniquely since ba,α is a bijection by Lemma 3. Thus
X
1
P̂ag(a,ā) (0)
ν a,α (ā, z 0 ) =
|N
|
z 0 ≥A ā0

α
(25)
− Pα
g(a,ā) (0) + Pg(αT ,ā0 ) (0)
1 a
=
P̂
(0) = P̂ a (ā)
|N | g(a,ā)

where the second equality follows as g(a, ā) = g(αT , ā0 ) by
T
definition of ba,α . The third equality follows as ā ∈ r =⇒
āg(a,ā) = 0.
Considering
ā ∈ s, we find only
applies,
thus for
 (23e)

X
a,α
0
a,α
a,αT
ā, b
ν (ā, z ) = ν
(ā)
z 0 ≥A ā0

(26)
1 a
P̂g(a,ā) (1) = P̂ a (ā)
|N |
where ā ∈ s =⇒ āg(a,ā) = 1 or else ā would be in q.
The final case for Condition 2 is ā = a. we find cases
(23a), (23b), and (23g) apply
 yielding:
X
X
1
ν a,α (ā, z 0 ) =
|N | −
P̂ag(a,z) (zg(a,z) )
|N |
0
0
z∈q∪r
z ≥A ā

X
P̂ag(αT ,z0 ) (1)
−
=

z 0 ∈R


1 X 
=
1 − P̂ag(a,z) (zg(a,z) )
|N |
z∈f (a)

1 X a
=
P̂i (ai ) = P̂ a (ā)
|N |
i∈N

(27)
where the first equality follows as sums over Q ∪ R are
equivalent to the sums over Q and R as Q, R are disjoint,
0
and that z 0 ∈ R ⇔ zg(α
T ,z 0 ) = 1 by definition of R. The
second equality follows as the R sum is equivalent to one
T
over s by bijection ba,α , and then we may combine it with
the sum over q ∪ r, to a sum over f (a) and |f (a)| = |N |.
This concludes all cases for Condition 2. We omit arguments
for Condition 3 as they run parallel to Condition 2.
To verify Condition 1, we consider each case of (23)
separately. Equations (23b), (23d), (23e), (23f), and (23h) are
trivial as these probabilities are well defined by definition.
Lemma 2 provides:
a
a
α
Pα
(28)
i (1) ≥ P̂i (1) ⇔ P̂i (0) ≥ Pi (0)
where the right hand side follows from Pi (1, a0 , w) +
Pi (0, a0 , w) = 1 = Pi (1, a0 , w0 ) + Pi (0, a0 , w0 ). Equation (23a) follows directly from the hypothesis and (23c)
holds from the right side of the equivalence.
The lone remaining case is (23g), for which we define
sets Nq = {g(a, z) | z ∈ q}, NQ = {g(αT , z) | z ∈ Q}
and so on for r, s, R, S. For convenience we denote unions
of these sets as Nqr := Nq ∪ Nr , NQR := NQ ∪ NR
and so on for other combinations of q, r, s and Q, R, S.
Recalling q, r, Q, R are partitions over states that a, αT may
transition to, similarly, Nqr , NQR are partitions of agents
whose unilateral deviations result in such transitions. These
sets are subsets of N and enable us to expand (23g)
ν a,α (ā, ā0 ) =

1
|N |

X

(1 − P̂ai (¬ai )

i∈Nqr ∩NQR

−

T
Pα
i (¬αi ))

+

X

(1 − P̂ai (¬ai ))

i∈Nqr \NQR

+

X
i∈NQR \Nqr

T
(1 − Pα
i (¬αi ).

(29)



1  α
a


(1)
P
(1)
−
P̂
0
0
T
0

g(a ,ā )

|N | g(α ,ā )




1 α


Pg(αT ,ā0 ) (ā0g(αT ,ā0 ) )


|N
|





1  a



(0)
P̂g(a,ā) (0) − Pα

g(a,ā)

|N |




 1 a
(ā
)
P̂
a,α
0
ν (ā, ā ) = |N | g(a,ā) g(a,ā)


1 a



P̂
(1)


|N
| g(a,ā)




 1 α

P T 0 (0)


 |N | g(α ,ā )





P a
P

1
α
0

|N
|
−
P̂
(z
)
−
P
(z
)

T
0
T
0
g(a,z)
g(a,z)
g(α ,z ) g(α ,z )
|N |


z∈q∪r
z 0 ∈Q∪R


0

ā = a, ā0 ∈ R

(23a)

ā = a, ā0 ∈ Q

(23b)

ā ∈ r, ā0 = αT

(23c)

ā ∈ q, αT = ā0

(23d)

T

ā = ba,α (ā0 ), ā0 ∈ R
T

(23e)

ā ∈ r, ā0 = ba,α (ā)

(23f)

a = ā, αT = ā0

(23g)

otherwise.

(23h)

Fig. 2. The full specification of the one-step monotone coupling for Theorem 2. We adopt the notational convention that q, s, Q, S are assumed to take
arguments a, a0 and r, R take the argument a, a0 respectively.

This expansion takes advantage of |N | = |f (a)| which
allows |N | to enter the sums as 1. It now suffices to show
that the summand of each sum is a well defined probability,
then the sum of |N | well defined probabilities divided by
|N | must also be a well defined probability. The last two
terms in (29) are clearly well-defined probabilities, leaving
only the first term.
We begin by investigating i ∈ Nqr ∩ NQR . In particular,
T
we have Nq = NQ , Ns = NR , Nr = NS due to ba,α and
its bijectiveness due to Lemmas 3. By disjointness of q, r
we have Nqr = NQS which we apply to Nqr ∩ NQR =
NQS ∩ NQR = NQ = Nq . Applying definitions of q, Q we
find i ∈ Nq =⇒ ¬ai = 1, ¬αiT = 0. Thus the summand of
the first sum for i ∈ Nq is given by
α
α
1 − P̂ai (1) − Pα
(30)
i (0) ≥ 1 − Pi (1) − Pi (0) = 0
where in the inequality is by (28), giving that the summands
in the first term of (29) are themselves well defined probabilities. As all conditions have been met, ν a,α is a monotone
coupling as desired.

must exist some t ∈ {1, 2, 3, . . . , T − 1} such that αt ≤A ᾱt
but αt+1 A ᾱt+1 , and let t be the minimal such value. In
t
≤t
t
≤t
this case we have ν α ,ᾱ (αt+1 , ᾱt+1 ) = 0 because ν α ,α̂
is a well defined monotone coupling by Theorem 2, yielding
νπĝ (α, ᾱ) = 0 as desired. It also follows that νπĝ will always
yield a well defined probability as it is either 0 or a product
of well defined probabilities. Thus we only need to show
that the marginal probabilities are preserved given by (15).
We begin by showing the left equation of (15), that is:
X
νπĝ (α, z) = P̂π (α) for each z ∈ AT
(32)
α≤AT z

and omit the proof for the right hand equation as it proceeds
identically. By inspecting (31), we only need to consider z
such that z 1 = α1 and z features at most a single unilateral
deviation between any t, t + 1. With these two conditions we
rewrite
TY
−1
X
X
t ≤t
νπĝ (α, z) =
π(α1 )
ν α ,z (αt+1 , z t+1 )
α≤AT z

1

= π(α )

D. A monotone coupling over histories
We now present coupling νπĝ which is constructed using
the one-step coupling. Using this coupling we then go on to
prove Lemma 1. Note, we define an indicator functions as
1(a = a0 ) to be 1 if a = a0 and 0 else for a, a0 ∈ A.
Theorem 3: Let g ∈ G be an aligned history-dependent
game and ĝ be its corresponding exact potential game. Then
νπĝ : A2T → [0, 1] is a monotone coupling between P̂π , Pπ .
This coupling is given by
TY
−1
t
≤t
νπĝ (α, ᾱ) = π(α1 )1(α1 = ᾱ1 )
ν α ,ᾱ (αt+1 , ᾱt+1 )
t=1

(31)
where α, ᾱ ∈ AT , π ∈ ∆(A).
Proof: Let α, ᾱ ∈ AT and let ĝ be a fixed corresponding fixed game to g ∈ G. We begin by showing that
if α AT ᾱ, then νπĝ (α, ᾱ) = 0. Immediately, we have
νπĝ (α, ᾱ) = 0 if α1 6= ᾱ1 , so we need only consider cases
where α1 = ᾱ1 . Inductively we find that if α AT ᾱ there

t=1

α≤AT z

X

να

1

,z ≤1

(α2 , z 2 ) . . .

α 2 ≤A z 2

X

ν

αT −1 ,z ≤T −1

(αT , z T ).

αT ≤A z T

(33)
as the combinatorial form. Critically, this allows us to to apt ≤t
ply the marginal sum properties of ν α ,z from Theorem 2
for each t ∈ {1, 2, .., T }. First, considering the rightmost
sum in (33), it holds that
X
T −1 ≤T −1
T −1
ν α ,z
(αT , z T ) = P̂ α (αT ).
(34)
α T ≤A z T

Because this has no dependence on z we may factor out
T −1
P̂ α (αT ) and repeat the process on the new rightmost
sum. After performing this process recursively on all sums,
we have
TY
−1
X
t
νπĝ (α, z) = π(α1 )
P̂ α (αt+1 ) = P̂π (α) (35)
α≤AT z

t=1

as desired, noting we accounted for the indicator functions
in νπĝ . This concludes the proof of Theorem 3.
Now that the necessary results have been developed we
proceed with the proof of Lemma 1.
Proof of Lemma 1: Let g ∈ G A be an aligned historydependent game and I ⊂ AT be an upper set. Define
1I (a) := 1(a ∈ I) as an indicator function. Consider
probability measures Pπ , P̂π coupled by νπĝ in Theorem 3,
we have
Pπ (I) − P̂π (I) = EPπ (1I ) − EP̂π (1I )
(36)
= νπĝ (I C , I) ≥ 0.
where the second equality follows by Proposition 3 as 1I is
increasing in AT . Note (36) runs parallel to the proof of [24,
Corollary 3]. That is, for any upper set I ⊂ AT we have
Pπ (I) ≥ P̂π (I)
(37)
−1 ~
meaning we have stochastic dominance. Let ((~0)Tt=1
, 1) ∈ I.
This induces I such that it includes every path such that
at time T the ~1 state is played. This yields the following
interpretation
Pπ (I) = Pr(s(T ; τ, π, g) = ~1)
(38)
representing the probability that at time T game g is in
the ~1 action profile given initial distribution π ∈ ∆(A)
and learning temperature parameter τ . Noting a parallel
interpretation to (38) holds for P̂π , ĝ, we apply these to (37)
to obtain
Pr(s(T ; τ, π, g) = ~1) ≥ Pr(s(T ; τ, π, ĝ) = ~1)
(39)
as desired.

R EFERENCES
[1] J. Howard, A. Huang, Z. Li, Z. Tufekci, V. Zdimal, H.-M. van der
Westhuizen, A. von Delft, A. Price, L. Fridman, L.-H. Tang, V. Tang,
G. L. Watson, C. E. Bax, R. Shaikh, F. Questier, D. Hernandez, L. F.
Chu, C. M. Ramirez, and A. W. Rimoin, “An evidence review of
face masks against covid-19,” Proceedings of the National Academy
of Sciences, vol. 118, no. 4, 2021.
[2] L. Matrajt and T. Leung, “Evaluating the effectiveness of social
distancing interventions to delay or flatten the epidemic curve of
coronavirus disease,” Emerging infectious diseases, vol. 26, no. 8,
p. 1740, 2020.
[3] H. P. Young, “The evolution of conventions,” Econometrica: Journal
of the Econometric Society, pp. 57–84, 1993.
[4] M. Kearns, M. L. Littman, and S. Singh, “Graphical models for game
theory,” arXiv preprint arXiv:1301.2281, 2013.
[5] J. R. Marden and J. S. Shamma, “Revisiting log-linear learning:
Asynchrony, completeness and payoff-based implementation,” Games
and Economic Behavior, vol. 75, no. 2, pp. 788–808, 2012.
[6] R. Chandan, D. Paccagnan, and J. R. Marden, “When Smoothness
is Not Enough: Toward Exact Quantification and Optimization of the
Price-of-Anarchy,” in 58th IEEE Conference on Decision and Control,
pp. 4041–4046, 2019.
[7] K. Paarporn, B. Canty, P. N. Brown, M. Alizadeh, and J. R. Marden,
“The Impact of Complex and Informed Adversarial Behavior in
Graphical Coordination Games,” IEEE Transactions on Control of
Network Systems, vol. 8, no. 1, pp. 200–211, 2020.
[8] J. R. Marden and A. Wierman, “Distributed welfare games,” Operations Research, vol. 61, no. 1, pp. 155–168, 2013.
[9] A. Kanakia, B. Touri, and N. Correll, “Modeling multi-robot task allocation with limited information as global game,” Swarm Intelligence,
vol. 10, no. 2, pp. 147–160, 2016.
[10] C. Wang, C. Xu, X. Yao, and D. Tao, “Evolutionary generative adversarial networks,” IEEE Transactions on Evolutionary Computation,
vol. 23, no. 6, pp. 921–934, 2019.
[11] U. Garciarena, R. Santana, and A. Mendiburu, “Evolved gans for
generating pareto set approximations,” in Proceedings of the Genetic
and Evolutionary Computation Conference, pp. 434–441, 2018.

[12] V. Costa, N. Lourenço, J. Correia, and P. Machado, “Coegan: evaluating the coevolution effect in generative adversarial networks,” in
Proceedings of the Genetic and Evolutionary Computation Conference, pp. 374–382, 2019.
[13] A. R. Tilman, J. R. Watson, and S. Levin, “Maintaining cooperation
in social-ecological systems,” Theoretical Ecology, vol. 10, no. 2,
pp. 155–165, 2017.
[14] A. R. Tilman, J. B. Plotkin, and E. Akçay, “Evolutionary games with
environmental feedbacks,” Nature communications, vol. 11, no. 1,
pp. 1–11, 2020.
[15] S. Skoulakis, T. Fiez, R. Sim, G. Piliouras, and L. Ratliff, “Evolutionary game theory squared: Evolving agents in endogenously evolving
zero-sum games,” arXiv preprint arXiv:2012.08382, 2020.
[16] J. S. Weitz, C. Eksin, K. Paarporn, S. P. Brown, and W. C. Ratcliff,
“An oscillating tragedy of the commons in replicator dynamics with
game-environment feedback,” Proceedings of the National Academy
of Sciences, vol. 113, no. 47, pp. E7518–E7525, 2016.
[17] J. A. Weill, M. Stigler, O. Deschenes, and M. R. Springborn, “Social
distancing responses to covid-19 emergency declarations strongly
differentiated by income,” Proceedings of the National Academy of
Sciences, vol. 117, no. 33, pp. 19658–19660, 2020.
[18] B. Jeffrey, C. E. Walters, K. E. Ainslie, O. Eales, C. Ciavarella,
S. Bhatia, S. Hayes, M. Baguelin, A. Boonyasiri, N. F. Brazeau, et al.,
“Anonymised and aggregated crowd level mobility data from mobile
phones suggests that initial compliance with covid-19 social distancing
interventions was high and geographically consistent across the uk,”
Wellcome Open Research, vol. 5, 2020.
[19] J. Jay, J. Bor, E. O. Nsoesie, S. K. Lipson, D. K. Jones, S. Galea, and
J. Raifman, “Neighbourhood income and physical distancing during
the covid-19 pandemic in the united states,” Nature human behaviour,
vol. 4, no. 12, pp. 1294–1302, 2020.
[20] C. Alós-Ferrer and N. Netzer, “The logit-response dynamics,” Games
and Economic Behavior, vol. 68, no. 2, pp. 413–427, 2010.
[21] P. N. Brown, H. P. Borowski, and J. R. Marden, “Security against
impersonation attacks in distributed systems,” IEEE Transactions on
Control of Network Systems, vol. 6, no. 1, pp. 440–450, 2019.
[22] B. S. Pradelski and H. P. Young, “Learning efficient nash equilibria in
distributed systems,” Games and Economic behavior, vol. 75, no. 2,
pp. 882–897, 2012.
[23] C. Nowzari, V. M. Preciado, and G. J. Pappas, “Analysis and Control of
Epidemics: A Survey of Spreading Processes on Complex Networks,”
IEEE Control Systems, vol. 36, no. 1, pp. 26–46, 2016.
[24] K. Paarporn, C. Eksin, J. S. Weitz, and J. S. Shamma, “Networked
SIS Epidemics with Awareness,” IEEE Transactions on Computational
Social Systems, vol. 4, no. 3, pp. 93–103, 2017.

A PPENDIX
Proof of Lemma 3: Let a, a0 ∈ A such that a0 ≥A a. We
0
proceed by proving ba,a : r(a) → S(a0 ) is a bijection; the
other bijection statements are proved similarly.
0
We begin by proving injectiveness, that is ba,a (z) =
0
ba,a (z 0 ) =⇒ z = z 0 for z, z 0 ∈ r(a). Observe g(a, z) =
0
0
g(a0 , ba,a (z)) = g(a0 , ba,a (z 0 )) = g(a, z 0 ) where the first
0
and third inequalities follow by definition of ba,a and the
middle by hypothesis. Injectiveness follows from g(a, z) =
g(a, z 0 ) meaning a, z and a, z 0 differ by the same agent’s
unilateral deviation. In that context, the possible actions
agent g(a, z) is given by Ag(a,z) \ {ag(a,z) } which is a
singleton by the binary action property, leaving only one
possible state a could transition to in r(a) via a unilateral
deviation. Thus z = z 0 as desired.
Next we show surjection, that is for any z 0 ∈ S(a, a0 ) there
0
exists a z ∈ r(a) such that ba,a (z) = z 0 , for a, a0 ∈ A and
a ≤A a0 . By definition of S(a, a0 ), z 0  a0 , but as z 0 ∈ f (a0 )
z 0 , a0 differ by only a single unilateral deviation by some
agent i. By partial ordering ≤A we may infer a0i = 1, zi0 = 0
else z 0  a0 would be violated. Further, we may infer a = 1
as suppose a = 0, then z 0 ∈ Q(a, a0 ), giving a contradiction

to the definition of z 0 . It is easy to see by definition of r(a)
that ai = 1 =⇒ z ∈ r(a) satisfying g(a, z) = g(a0 , z 0 ) as
zi 6= ai but z−i = a−i by z ∈ f (a). Note g(a, z) = g(a0 , z 0 )
0
is always satisfied when ba,a (z) = z 0 by definition of the
function.


