Anomaly Detection in Stationary Settings: A
Permutation-Based Higher Criticism Approach
arXiv:2009.03117v1 [stat.ME] 7 Sep 2020

Ivo V. Stoepker1 , Rui M. Castro1 , Ery Arias-Castro2 , and Edwin van den
Heuvel1
1
2

Technische Universiteit Eindhoven
University of California, San Diego

Abstract
Anomaly detection when observing a large number of data streams is essential in a
variety of applications, ranging from epidemiological studies to monitoring of complex
systems. High-dimensional scenarios are usually tackled with scan-statistics and related
methods, requiring stringent modeling assumptions for proper calibration. In this
work we take a non-parametric stance, and propose a permutation-based variant of the
higher criticism statistic not requiring knowledge of the null distribution. This results
in an exact test in finite samples which is asymptotically optimal in the wide class of
exponential models. We demonstrate the power loss in finite samples is minimal with
respect to the oracle test. Furthermore, since the proposed statistic does not rely on
asymptotic approximations it typically performs better than popular variants of higher
criticism that rely on such approximations. We include recommendations such that the
test can be readily applied in practice, and demonstrate its applicability in monitoring
the daily number of COVID-19 cases in the Netherlands.
Keywords: permutation test; anomaly detection; higher criticism; minimax hypothesis testing

1

Introduction

We study the problem of anomaly detection when a large number of data streams is observed.
The streams themselves are not structured in any special way. In particular, there is no
spatial or other proximity measure between streams, unlike in some areas like syndromic
surveillance (Kulldorff et al., 2005). We work under the following assumptions:
• Entirety. An anomalous stream is affected in its entirety. This assumption focuses
the discussion on the unstructured nature of the setting. Although simplistic, this
assumption is common in the literature (Aldosari and Moura, 2004; Lexa et al., 2004;
Mei, 2008; Patwari and Hero, 2003; Thomopoulos et al., 1989; Yu et al., 2006).

2
• Sparsity. An anomaly affects only a small fraction of the streams. This assumption
focuses the discussion on the most compelling regimes.
Formally we consider n streams indexed by [n] ≡ {1, . . . , n}, that are observed over a time
period [t] ≡ {1, . . . , t}. Specifically we observe X ≡ (Xij : i ∈ [n], j ∈ [t]), where Xij is the
value from stream i ∈ [n] collected at time j ∈ [t]. Stream i ∈ [n] corresponds to a vector
denoted by Xi ≡ (Xij : j ∈ [t]).
Following standard practice, we place ourselves in a statistical decision theory framework
and cast detection as a hypothesis testing problem. For simplicity, we assume the variables
to be real valued and independent.
In this paper, we consider a stationary setting where the distribution of the observations
remain unchanged over time. In detail, under the null hypothesis all Xij are independent
and identically distributed (i.i.d.) with common distribution function denoted by F0 . The
alternative hypothesis is similar, but there is a subset of anomalous streams S ⊂ [n] such that
for each i ∈ S the distribution of Xij , denoted by Fi , stochastically dominates F0 . In words,
larger-than-usual values are observed in the streams indexed by S. The subset S is unknown
and has no particular structure. Note that we are still assuming all the random variables Xij
are independent. The sparsity assumption means the number of affected streams is believed
to be small in comparison with the total number of streams, i.e. |S|  n. Succinctly, our
hypothesis testing problem is therefore:
i.i.d.

H0 :

∀i∈[n] Xij ∼ F0

H1 :

∃S⊂[n] : ∀i∈S Xij ∼ Fi and ∀i∈S
/ Xij ∼ F0 ,

(1)
i.i.d.

i.i.d.

where for all i ∈ S, distribution Fi stochastically dominates F0 .
Despite its apparent simplicity, the above formulation and ensuing hypothesis test is quite
suitable in a number of scenarios. For instance, in industrial settings the stream observations
Xi , i ∈ [n] might correspond to a failure metric measured from n machines. Under nominal
conditions we expect these to be i.i.d. samples from some distribution, while under the
anomalous conditions some of these machines will occasionally display higher-than-normal
values for the failure metric. In more complex settings the observations might correspond to
residual values, obtained by fitting a model to the streams in order to ensure these are in a
common referential. For instance, in gene expression studies the streams might correspond
to the normalized gene expression level of a number of replicates, so that under nominal
conditions the expression levels are “equalized”, and one is trying to determine if genes are
differentially expressed under a specific environmental change. Obviously in this and similar
settings some pre-processing of the data is needed to ensure all the streams are comparable.
In Section 5.6 we consider yet a different application, where the proposed methodology can be
applied directly to the daily counts of diagnosed COVID-19 cases in different municipalities
in the Netherlands.
Contributions: Although a thorough characterization of the above testing scenario is far
from trivial, it has been addressed quite extensively when the distribution F0 is assumed

3
known, and in particular when this is a normal distribution (Donoho and Jin, 2004; Ingster, 1997). Knowledge of that distribution is required to calibrate the proposed tests (e.g.,
by Monte-Carlo simulation) and ensure the resulting p-value computation is done properly.
When F0 is unknown there are various ways one can proceed. A natural nonparametric
approach is calibration by permutation. This approach is, in fact, quite standard in syndromic surveillance (Huang et al., 2007; Kulldorff et al., 2005, 2009). It is also common
in neuroimaging (Nichols and Holmes, 2002) and other contexts (Flenner and Hewer, 2011;
Walther, 2010).
In this work we propose a variant of the higher criticism statistic that relies on permutations
in both its definition and its calibration, leading to an exact test. Furthermore, we show that,
in the context of a one-parameter exponential family this test is asymptotically optimal, in
the sense that it has essentially the same asymptotic performance as the best possible test
with oracle knowledge of the model parameters. We provide practical recommendations so
that the test is readily applicable in practice and demonstrate it has good performance in
finite sample scenarios. In terms of testing power our procedure compares quite favorably
with other proposed variants of higher criticism, particularly when streams are short (i.e.,
small t), since in its definition in does not rely on asymptotic approximations.
Related work: Early work on distribution-agnostic applications of the higher criticism
can be found in Delaigle and Hall (2009). The setting is different from ours, as the authors
consider testing if a vector of observations, with unknown distribution, has the same mean
as that of auxiliary data. Different approximation methods are discussed for the null distribution based on proximity measures or normal approximations, as well as the possibility of
relaxing independence assumptions. Furthermore, a classification problem is discussed, in
which (vector) samples from two populations are used to classify unlabeled vector samples
to one of the corresponding classes. In a setting closer to ours, Delaigle et al. (2011) use the
higher criticism test with studentized stream means and obtain a p-value by bootstrapping.
Asymptotic results for the statistic are obtained, but controlling the type I error in finite
samples is not discussed. The guarantees obtained are under a significantly different regime
than the one we consider here, namely, the authors only assume that the null distribution
has enough finite moments, but in turn require the stream length t to scale polynomially
with the number of streams n, while our results hold for much shorter streams whose length
scale poly-logarithmically.
The standard higher criticism statistic requires itself knowledge of the null distribution. In
the absence of such knowledge several authors proposed the use of asymptotic approximations. In contrast, we use permutation-based estimates to construct the statistic itself, which
seems to yield higher power in finite sample scenarios in comparison with approximationbased approaches, in addition to the strong theoretical guarantees we provide. Furthermore,
computing the statistic using permutations comes at virtually no extra cost in comparison
to the use of asymptotic approximations, when the statistic itself is already calibrated by
permutation. Wu et al. (2014) consider a different setting than our, but with some important
points-of-contact: in an association setting in genomics the authors aim to use the higher
criticism to test if some covariates (gene SNP’s) have non-null association to the outcome

4
variables (traits), where a rare-weak assumption on the number and strength of the association is made. Calibration by permutation of the statistic is not the author’s main interest,
but is mentioned in the numerical section. The higher criticism statistic itself is computed
based on asymptotic approximations. This type of methodology has been previously used
in applied settings, for example in Sabatti et al. (2009). In Donoho and Jin (2015) other
works in this domain are reviewed, and the possibility of calibration by permutation is also
mentioned, but their power properties are not established.
In Arias-Castro and Wang (2017) the authors consider the problem of detecting sparse
heterogeneous mixtures from a nonparametric perspective, when the null distribution is
assumed to be symmetric and the anomalous observations have a shift in mean. In Zou
et al. (2017) the setting is similar to ours, but instead of hypothesis testing, their goal
is to identify the set of anomalous streams. The authors then bound the probability of
misidentifying the set of anomalous sequences. In Arias-Castro et al. (2018), a permutationbased scan statistic is proposed for the detection of structured anomalies (e.g., an interval)
in a single stream, and shown to match to first order the performance of the scan statistic
calibrated with (oracle) knowledge of the null distribution. Recently, Kurt et al. (2020)
focuses on nonparametric change-point detection in a real-time setting and assumes there is
a period of time guaranteed without anomalies. The authors show their decision statistics
are asymptotically bounded under the null. Power properties are only shown empirically.
Organization: Section 1 introduces and motivates the problem. Section 2 briefly reviews
known results for the normal model and motivates the use of a simple quantized variant of
the higher criticism statistic. In Section 3 we introduce the class of distributional models
we use as a benchmark for our theoretical power analysis and provide lower bounds on
the anomalous signal strength for any test to be asymptotically powerful in the context
of any one-parameter exponential family. Section 4 proposes a novel permutation higher
criticism test, and we show that it is essentially asymptotically optimal in the context of the
one-parameter exponential family. We also propose a permutation max test and establish its
power properties. In Section 5 we examine the finite-sample performance of our methodology
on simulated data, and also apply that methodology to a real data example. All the proofs
are deferred to Section 7. Some supporting results are provided in the Appendix to ensure
the manuscript is self-contained.
Notation: Throughout the paper we use standard asymptotic notation. Let n → ∞, then
an = O(bn ) when |an /bn | is bounded, an = o(bn ) when an /bn → 0, and an = ω(bn ) when
bn = o(an ). We also use probabilistic versions: an = OP (bn ) when |an /bn | is stochastically
bounded1 and an = oP (bn ) when an /bn converges to 0 in probability. Unless otherwise stated,
we consider asymptotic behavior with respect to n → ∞.
1

That is, for any ε > 0 there is a Cε and nε such that ∀n > nε P (|an /bn | > Cε ) < ε.

5

2

An important case: the normal location model

This section considers a specific location model, which serves both as a benchmark and
provides important insights needed for generalizations. These are well known results, but
serve as a stepping stone to propose our methodology in the coming sections. Consider the
setting where F0 is the standard normal distribution with zero mean and unit variance. Under
the alternative, there is a set S ⊂ [n] indexing the anomalous streams, and the corresponding
observations have an elevated mean µ > 0 corresponding to the signal strength. Specifically,
consider the following hypothesis test:
i.i.d.

H0 :

∀i∈[n]

Xij ∼ N (0, 1) .

H1 :

∃S⊂[n] : ∀i∈S

(2)

i.i.d.

Xij ∼ N (µ, 1), and ∀i∈S
/

i.i.d.

Xij ∼ N (0, 1) .

A natural question to ask is how large does µ need to be to ensure one can distinguish the
two hypotheses. In this model it is obvious that the collection of stream means
1X
Xij
(3)
Yi (X) ≡
t
j∈[t]

are jointly sufficient. The setting then reduces to that of the normal means model (Donoho
and Jin, 2004; Ingster, 1997). Note that in those works the alternative hypothesis is a bit
different, and often Xij are assumed to be i.i.d. samples from a sparse normal mixture.
Nevertheless the asymptotic characterization of the optimal tests can be easily translated to
the setting we are considering.
We take an asymptotic point of view. Let ψn (X) : Rnt → {0, 1} be a sequence of tests,
where the outcome 1 indicates rejection of the null hypothesis. The risk of a test is simply
the sum of type I and worst-case type II error, namely
R(ψn ) = P∅ (ψn (X) 6= 0) + max PS (ψn (X) 6= 1) ,
S:|S|=s

where we denote PS the probability under the alternative, with anomalous set S having
signal strength µ. A sequence of tests is said to be asymptotically powerful if R(ψn ) → 0 as
n → ∞, and it is said to be asymptotically powerless if R(ψn ) → 1.
To present an asymptotic characterization of the above hypothesis testing problem it is
convenient to introduce the following parameterization. Let β ∈ [0, 1] be arbitrary (but
fixed) and suppose S has size
|S| = dn1−β e ,
(4)
where dze denotes the smallest integer larger than z. With this parametrization, we can
identify two main regimes:
P
• The dense regime: 0 ≤ β ≤ 1/2. In this scenario a simple test based on i∈[n] Yi (X)
is optimal. As stated in the beginning our interest lies in settings where the anomaly
affects only a reduced number of streams, and so we do not focus much on this regime.
Whenever appropriate we make comments on how our results and methods extend to
the dense regime.

6
P
• The sparse regime: 1/2 < β < 1. In this case a test based on i∈[n] Yi (X) will be
asymptotically powerless. Let r > 0 be fixed and parameterize µ as
r
2r
log(n) .
(5)
µ=
t
Define the detection boundary
(
β − 1/2 ,
1/2 < β ≤ 3/4 ,
ρ∗ (β) ≡
√
2
(1 − 1 − β) , 3/4 < β < 1 .
It can be shown that, if r < ρ∗ (β) all tests are asymptotically powerless (see Ingster
(1997) and Theorem 1). Furthermore, if r > ρ∗ (β) there are tests (introduced below)
that are asymptotically powerful. Note that there are actually two sub-regimes within
the sparse regime, namely:
 The moderately sparse regime: 1/2 < β ≤ 3/4. Here Donoho and Jin (2004) show
that the higher criticism test, based on the maximum of a normalized empirical
process of the (Yi (X), i ∈ [n]) is asymptotically optimal. In fact, this is so in all
regimes. Define the stream p-values as

√
t Yi (X) ,
(6)
pi = 1 − Φ
where Φ denotes the cumulative distribution function of a standard normal random variable. Note that, under the null hypothesis these are i.i.d. samples from a
uniform distribution in [0, 1]. One variant of the higher criticism rejects the null
hypothesis for large values of
max
i∈[bα0 nc]

√

i
− p(i)
np n
,
p(i) (1 − p(i) )

(7)

where α0 ∈ (0, 1) and p(1) ≤ · · · ≤ p(n) are the ordered p-values.
 The very sparse regime: 3/4 < β < 1. Here Donoho and Jin (2004) show that the
test that rejects for large values of maxi∈[n] Yi (X) achieves the detection boundary.
This is not the case in the moderately sparse regime, as shown in (Arias-Castro
et al., 2011). Expressed in terms of p-values, the max test is entirely equivalent
to multiple testing with Bonferroni correction, rejecting the null hypothesis for
small values of p(1) = mini∈[n] pi .
Remark 1. Note that the parameterizations in Equations (4) and (5) can be presented in
a more general way using asymptotic notation. In particular, we can simply assume |S| =
n1−β+o(1) as n → ∞. For the sparse regime, it suffices to assume
r
2(r + o(1))
µ=
log(n) .
t

7

2.1

A quantized higher criticism statistic

In this section we show that a quantized higher criticism statistic is also optimal in the
normal location model. Quantizing the higher criticism statistic has been done before in
different ways - for example, in Arias-Castro et al. (2011), Wu et al. (2014) and others. We
discuss a quantization because the adaption and analysis of the higher criticism statistic,
when using calibration by permutation, is easier for the quantized form introduced here.
In contrast, adaptation to the permutation setting of the original higher criticism statistic
in (7) is not straightforward as the computation of the test statistic itself requires knowledge
of the null distribution - through the computation of the p-values in Equation (6).
We quantize a different variant of higher criticism. This variant was already introduced for
analytical purposes in Donoho and Jin (2004). Let q ≥ 0 and define
)
(
r
X
2q
log(n) ,
Nq (X) ≡
1 Yi (X) ≥
t
i∈[n]

p
This is simply the number of streams whose average is larger or equal to 2(q/t) log(n).
We refer to the latter value as the q-threshold. Define also pq , the probability that a stream
mean from the null exceeds that same threshold:

p
2q log(n) .
(8)
pq ≡ 1 − Φ
Clearly, under the null hypothesis Nq (X) is binomial with parameters n and pq , therefore we
can standardize Nq (X) and define the statistic
Nq (X) − npq
Vq (X) = p
.
npq (1 − pq )

(9)

The Vq (X) statistic, when maximized over q ∈ [0, ∞), is closely related to the higher criticism
statistic (see Remark 4 below).
The following result summarizes several analytical steps in Donoho and Jin (2004).
Proposition 1 (Donoho and Jin (2004)). Let q ∈ [0, ∞) be fixed but arbitrary, and
hn → ∞. Then P∅ (Vq (X) ≥ hn ) ≤ h12 → 0. Furthermore, for S as in (4), when 1/2 < β < 1
np
(sparse regime), hn = no(1) , and µ = 2(r/t) log(n) we have:
• If r > (1 −

√

1 − β)2 then PS (V1 (X) ≥ hn ) → 1.

• If r < 1/4 and r > β − 1/2 then PS (V4r (X) ≥ hn ) → 1.
The proof of Theorem 3, our main result, hinges crucially in showing that important quantities inside the proof of Proposition 1 have the same asymptotic characterization when
considering calibration by permutation. The heart of the proof of Proposition 1 is the realization that, under the alternative, Nq (X) is the sum of two independent binomial random

8
variables; Nq (X) ∼ Bin(n − s, pq ) + Bin(s, vq ), where
 √ √ 2
−( q− r) +o(1)

n
pq = n−q+o(1)
and
vq = 21
√ √ 2


1 − n−( r− q) +o(1)

if r < q
if r = q .
if r > q

We will show a similar result in our setting, implying the result in this proposition can be
used to complete the proof of Theorem 3. For completeness, a proof of Proposition 1 is also
provided in Appendix A.1.
Remark 2. In the dense regime, it is also possible to show that, when µ = √1t nr−1/2 with
r > β, we have PS (V0 (X) ≥ hn ) → 1. This implies the higher criticism statistic is also
optimal in that regime under the normal model.
Proposition 1 shows that, depending on the combination of sparsity |S| and signal strength
µ we might want to consider a test based on Vq (X) for different values of q. Interestingly,
the value q = 1 suffices for the very sparse regime. For the moderately sparse regime the
situation is more intricate, and it turns out the choice q = 4r is essentially optimal (unless
r ≥ 1/4, as then q = 1 suffices). Obviously we do not know r, so the only way we can
capitalize on this knowledge is to scan over a suitable selection of values for q, that hopefully
include (or approximate) the optimal value. To keep the analysis simple and avoid the use
of advanced tools in empirical processes we scan only over a coarse subset of values for q.
The following result is an almost immediate consequence of Proposition 1.
Proposition 2 (Quantized HC). Let Q ⊆ [0, 1] be the set


1 2
Q = 0, , , . . . , 1 ,
kn kn
where kn → ∞ and kn = no(1) . Define the test statistic
T (X) = max Vq (X) .
q∈Q

√
Finally, let hn = ω( kn ) and hn = no(1) , and consider the test that rejects the null hypothesis
p when T (X) ≥ hn . This test has vanishing type I error. ∗When β > 1/2 and
µ = 2(r/t) log(n) this test is asymptotically powerful provided r > ρ (β).
The proof follows almost immediately by using a union of events bound over the grid elements. Under the alternative, the only intricate case is the moderately sparse regime.
However, since the grid is becoming denser as n grows it will contain a value q that is close
enough to the optimal value 4r. A proof is provided in Appendix A.2 for completeness.
Remark 3. For the dense regime, one can prove that, if µ = √1t nr−1/2 this test is asymptotically powerful provided r > β. This follows immediately using the results from Proposition 1,
since the optimal value 0 is guaranteed to be in the grid.
Remark 4. The Vq (X) statistic is closely related to the more familiar higher criticism statistic
as in (7) as
(
)
i
√
−
p
(i)
sup {Vq (X)} = max
np n
,
i∈[i
]
+
p(i) (1 − p(i) )
q∈[0,∞)
where i+ is the largest value of i for which p(i) < 1/2. See Appendix A.3 and Lemma 7.

9

3

Generalization to exponential families

The results of the previous section were specific for the normal location model. Nevertheless
one can envision somewhat natural extensions of the approach and test statistics to the
more general context of the exponential family. When studying distribution-free tests, it
is customary to compare them with parametric tests (Hettmansperger, 1984). As in AriasCastro et al. (2011) and Arias-Castro et al. (2018), we consider one-parameter exponential
models (in natural form) as parametric models, as these play an important role in the related
literature (Donoho and Jin, 2004).
One-parameter exponential models in natural form: Let F0 be a probability distribution on the real line, such that all moments are finite. Let µ0 and σ02 denote respectively
the mean and variance of F0 . The distribution might be continuous, discrete or a combination of the two. In the exponential model there is a parameter θi associated with each
i ∈ [n], and the distribution Fi ≡ Fθi is defined through its density fθi Rwith respect to
F0 : for θ ∈ [0, θ? ), define fθ (x) = exp (θx − log ϕ0 (θ)), where ϕ0 (θ) = eθx dF0 (x) and
θ? = sup{θ > 0 : ϕ0 (θ) < ∞}, assumed to be strictly positive (and possibly infinite). In
other words, fθi denotes the Radon-Nykodym derivative of Fθi with respect to F0 . Since a
natural exponential family has the monotone likelihood ratio property it follows that Fθ is
stochastically increasing in θ (Lehmann and Romano, 2005, Lem 3.4.2). Particular cases of
this model are used in a variety of settings. For example, in many signal and image processing applications, a model with Fθ corresponding to normal distribution with mean θ and a
fixed variance is common. In syndromic surveillance (Kulldorff et al., 2005), a model with
Fθ corresponding to a Poisson distribution is popular. Bernoulli models (Walther, 2010) are
also a particular case of this class, with Fθ corresponding to a Bernoulli distribution.
Let Xij be independent observations, where all observations within stream i are i.i.d. with
distribution function Fθi , as defined above. Under the null hypothesis we have θi = 0 for
all i. Under the alternative, there is a subset of streams for which θi > 0. Our hypothesis
testing problem is therefore given by:
∀i∈[n] θi = 0 ,
∃S⊂[n] : ∀i∈S θi > 0 and ∀i∈S
/

H0 :
H1 :

(10)
θi = 0 .

Keeping the same minimax stance as before, in the remainder our analysis assumes the
anomalous observations have the same distribution so θi = θ > 0 for i ∈ S. We keep the
parametrization of the anomalous
set as in (4), namely |S| = bn1−β c for β ∈ (0, 1], and use
q
the parameterization θ = σ2r2 t log n.
0

Like it was the case for the normal model, a detection boundary defines the region in the
(r, β) plane where any test will be asymptotically powerless. In fact, when the stream length
t is large enough the detection boundary for the exponential family is identical to ρ∗ (β)
defined earlier, as the following result shows.
Theorem 1. Refer to the hypothesis testing problem in (10) and the parameterization |S| =

10
n1−β with β ∈ (1/2, 1) and θ =

q

2r
σ02 t

log n with r > 0. Provided t = ω(log3 (n)) any test will

be asymptotically powerless when r < ρ∗ (β).
In the next section, we develop a permutation-based test that is able to attain optimal
asymptotic performance, giving a converse to the statement of Theorem 1.

4

Calibration by permutation

The quantized higher criticism statistic proposed in Section 2 is asymptotically optimal in
the normal location model, but to compute it and calibrate it (i.e. compute a test p-value)
one needs to assume knowledge of the null distribution. With that knowledge, a simple
way to proceed would be to calibrate the test by Monte-Carlo simulation. The goal in
this section is to construct a test (and test statistic) that can be computed and properly
calibrated without the knowledge of the null distribution. A natural and appealing idea is
to calibrate these tests by permutation. The idea of using permutations is not new in the
context of comparing multiple groups (e.g, consider the classical Kruskal–Wallis test or any
test based on runs). However, such approaches cannot deal with sparse alternatives, as in
the setting we are considering (particularly when β > 1/2).

4.1

Permutation-based max test

As can be expected, using calibration by permutation to calibrate a test based on the value
of the maximum stream-mean yields a test that cannot be optimal for all values of β.
Nevertheless it is insightful to consider this option for its simplicity.
The max test rejects the null hypothesis for large values of
max Yi (X) , where Yi (X) =
i∈[n]

1X
Xij .
t
j∈[t]

Let Π denote the set of permutations of {(i, j) : i ∈ [n], j ∈ [t]}. Note that there are
(nt)! distinct permutations. Let π ∈ Π and Xπ ≡ (Xπ(i,j) , i ∈ [n], j ∈ [t]). In words, Xπ
corresponds to the permuted data over both streams and time. Calibration by permutation
amounts to computing a p-value as
o
1 n
π
π ∈ Π : max {Yi (X )} ≥ max {Yi (X)} .
(11)
Pmax-perm (X) =
i
i
(nt)!
In words, this is just the proportion of times the value of the test statistic computed with
permuted data is at least as large as the value of the test statistic computed on the original
data. Although the computation of the p-value might seem computationally prohibitive, we
can approximate it very accurately by considering instead a uniform sample from the set
of permutations. We have the following characterization of the statistical properties of this
test:

11
Theorem 2 (Permutation max test).
q Let α ∈ (0, 1) be arbitrary and refer to the hypothesis testing problem in (10). Let θ = σ2r2 t log(n) with r > 0. Consider the test that rejects
0

3
H0 if Pmax-perm (X) ≤ α. This test has level at most α. Furthermore
√ let 2t = ω(log (n)) and
β > 0. This test has power converging to one provided r > (1 − 1 − β) .

This theorem shows the max test calibrated by permutation is only guaranteed to be optimal
when β ≥ 3/4, as expected. It is important to note the stream length t starts playing a
more prominent role when considering calibration by permutation. Clearly, if t = 1 it is
impossible to calibrate such a test by permutation, as the test statistic is actually invariant
under permutations in that case. One expects that, for very small values of t there might
therefore be a loss of power in comparison to an oracle test calibrated with knowledge of
the null distribution. The requirement for t in the above theorem is needed to ensure the
stream means have approximately Gaussian tails. For exponential tails this scaling seems
necessary. We conjecture that if the tail of F0 is sub-Gaussian the requirement can be relaxed
to t = ω(log2 (n)) and if it has bounded support t = ω(log n) will suffice. It might seem the
above proposition should follow directly from the results in Arias-Castro et al. (2018) as one
can interpret the max test as a scan test over intervals of length t within nt observations, but
a more careful analysis is needed to get a sharp characterization of the detection boundary.

4.2

Permutation-based higher criticism test

Although the simplicity of the max test is appealing, the test is far from optimal in the
moderately sparse regime (or the dense regime for that matter). Therefore we turn our
attention to the possibility of calibrating the higher criticism statistic by permutation. Note
that the statistic in Theorem 2 critically depends on the specification of the parameters in
our normal setting. Now, since the mean and variance of the null distribution are unknown
this needs to be taken into account. Define
)
(
r
2
X
2σX
q
log(n) ,
N̂q (X) ≡
1 Yi (X) − X ≥
t
i∈[n]

where Yi (X) is as before, while
1 X
X≡
nt

i∈[n],j∈[t]

Xij ,

2
≡
σX

1
nt

X

(Xij − X̄)2 .

(12)

i∈[n],j∈[t]

We must also adapt the normalizing term pq accordingly. In principle, the computation of
this probability requires knowledge of F0 . Instead, we obtain a p-value by permutation as
follows
(
)
r
2
1
2σ
q
X
P̂q (X) ≡
π ∈ Π : Y1 (Xπ ) − X ≥
log(n)
.
(13)
(nt)!
t
Alternatively one might consider an asymptotic normal approximation to compute P̂q instead, as used in Sabatti et al. (2009) and Wu et al. (2014). However, our numerical experiments indicate the permutation-based approach in (13) leads to tests with higher power.

12
Finally, define the counterpart of Vq (X):
N̂q (X) − nP̂q (X)
,
V̂q (X) ≡ q
nP̂q (X)(1 − P̂q (X))

(14)

where we convention that 0/0 = 0. We are now ready to state the main result.
Theorem 3 (Permutation-based higher criticism test). Let Q ⊆ [0, 1] be the set


1 2
Q = 0, , , . . . , 1 ,
(15)
kn kn
with kn → ∞ and |Q| = kn = no(1) . Define the test statistic:
T̂ (X) ≡ max V̂q (X) .
q∈Q

(16)

Finally, define the higher criticism permutation p-value as
o
1 n
π ∈ Π : T̂ (Xπ ) ≥ T̂ (X)
.
Pperm-hc (X) =
(nt)!
Let α ∈ (0, 1) be arbitrary and refer to the hypothesis testing problem in (10). Consider
the test that rejects
q H0 if Pperm-hc (X) ≤ α. This test has level at most α. Furthermore, let
β > 1/2 and θ = σ2r2 t log(n) with r > 0 and t = ω(log3 (n)). This test has power converging
0

to one provided r > ρ∗ (β).
This result shows that, provided the stream length t is large enough this higher criticism
test calibrated without knowledge of the null distribution is asymptotically as powerful as
any oracle test with that knowledge. This comes at a higher computational cost, but it is
not prohibitive; see Section 5.1. Just as with the permutation max test, the requirement on
t is driven by the tail behavior of the null distribution.
σ0 r−1/2
n
with r > 0
Remark 5. For the dense case (i.e. β ≤ 1/2) one can show that, if θ = √
t
and t/n = Ω(1), then the permutation higher criticism test has power converging to one
provided r > β.

5

Experimental results

This section thoroughly explores the potential of the proposed procedure. We begin by
making some important computational remarks, followed by results on simulated data, and
concluding with the application of the methodology to a real dataset.

13

5.1

Computational complexity

The p-value of the permutation test in Theorem 3 is calculated by considering all possible
data permutations. Naturally, this becomes prohibitive even for moderately large datasets,
and instead a sample of the permutation distribution can be used to estimate Pperm-hc (X).
At first glance computation and calibration of this test statistic might still seem prohibitive:
for each permutation π taken from Π, one computes T (Xπ ), which requires us to compute
Pq (Xπ ). Naturally, we can estimate this quantity in a similar fashion using a random set of
permutations. However, this would mean that every permutation sample π used to compute
T (Xπ ) requires us to take a new set of permutation samples, which is disastrous computationally. However, note that Pq (X) is invariant under permutations, namely Pq (X) = Pq (Xπ ).
Thus we can estimate Pq (X) using a random sample of permutations, and then use these
estimates to compute T (Xπ ). Moreover, numerical experiments indicate we can use the same
set of permutations to both estimate Pq (X) and compute T (X), without loss of performance
or raising the type I error. This approach has therefore been used in the ensuing numerical
experiments.

5.2

Simulation setting

For our experimental results, we consider two different models: the normal location model,
discussed in Section 2 and a very standard benchmark, and a model where the underlying
distributions are exponential. In the second model, the null distribution F0 is exponential
with mean 1/λ0 (where we chose λ0 = 1.5). Following the parametrization set up in Section 3
the observations in the anomalous streams are exponentially distributed with mean 1/(λ0 −θ),
where θ ∈ [0, θ∗ ) where θ∗ = λ0 . This second setting allows us to explore the influence
of heavier tails. In fact, the exponential tails are the heaviest tails for which we provide
asymptotic guarantees.
In all simulations considered we assume the observations in the anomalous streams have
distribution Fθ , where
s
2ρ∗ (β)
log(n) ,
(17)
θτ = τ
σ02 t
with τ ≥ 0. In words, we consider a signal strength that is a multiple of the asymptotically
minimal signal strength ρ∗ (β) needed for detection. Asymptotically we know that when
τ < 1 all tests are powerless, and when τ > 1 the permutation higher criticism test is
powerful. In all simulations, test are calibrated at a significance level of 5%.

5.3

Grid choice

Although the grid Q proposed in Theorem 3 is valid asymptotically, it can leave out valuable
information in finite samples, where the global maximum of Vq (X) can be attained for q > 1.
To safeguard the approach against this issue one can instead use a data-dependent grid, such

14
that the grid contains a sufficiently large point q ∗ for which N̂q∗ (X) = 0. A specific choice is


M 2t
2M 2 t
M 2t
maxi,j {Xij } − X
Q = 0,
,
,...,
, with M ≡
.
(18)
2kn log(n) 2kn log(n)
2 log(n)
σX
(Recall the definitions given in (12).) The statement in Theorem 3 still holds for this grid,
as long as the number of elements in the grid kn is not larger than no(1) , and the grid
Q approximates the range [0, 1] with increasing accuracy as n increases. To satisfy both
requirements, a suitable choice for the number of elements kn is given by:
M 2t
kn =
dn ,
2 log(n)

(19)

where dn is a slowly diverging function. Note that 1/dn is the spacing between consecutive
grid points. We show in this section that dn = log n is a suitable default choice. Nevertheless,
provided dn → ∞ and dn = no(1) , Lemma 4 ensures that kn = no(1) with high probability.
It is then possible to adapt the proof of Theorem 3 to get the same guarantees for this grid
choice as explained, but this clutters the proof with undesirable technicalities.
It has been suggested in related works (Arias-Castro et al., 2011; Wu et al., 2014) that scanning over a discrete grid for this type of statistic is merely a proof artifact, and the results are
also valid when q in (16) is optimized over the continuous interval [0, ∞). We have conducted
extensive numerical experiments regarding the choice of grid and observed no deteriorating
performance for extremely fine grid choices. However, since we use permutation-based estimates P̂q (Xπ ) the number of specific values of q we need to consider to optimize Vq over
[0, ∞) becomes significantly larger than n, in principle as large as nB where B is the number of permutations considered. So, from a practical standpoint it is still rather useful to
consider the proposed quantization.
We thus focus on the finite choice as in (18) in all experiments. To recommend the reader
a choice of dn , we simulate data from both models outlined in Section 5.2 and compare the
performance of the tests for different choices of dn as in (19). We use n = 103 streams
of length t = d(log n)2 e = 48. For the experiments shown we took |S| = 12, (i.e. β ≈
0.64, moderately sparse regime), but through extensive experimentation we observed our
conclusions are not sensitive to the sparsity. The results are given in Figure 1. We observe
that a spacing of 1/dn = 1/ log(n) is a reasonable choice. In all remaining experiments, we
therefore extend the grid as in (18) and (19) with dn = log(n).

5.4

Comparison with an oracle test

We now compare our methodology with the oracle test making full use of the null distribution.
The oracle test statistic is defined in (9) for the normal model. For the exponential model the
stream averages are Gamma-distributed, and so one uses the quantiles of that distribution
to compute pq instead. The oracle-test is then calibrated by Monte-Carlo simulation. For
this calibration, 104 simulations were used. In the presentation we use HC-permutation and
HC-oracle to refer to the two tests.
We compare the oracle test with our methodology with respect to two aspects:

15

1

1
d n = log(n) 1.5

0.8

d n = log(n)

d n = log(n)

0.8

0.5

0.6

Power

Power

d n = log(n) 1.5

d n = log(n)

0.6

0.4

0.4

0.2

0.2

0

d n = log(n) 0.5

0
0

0.5

1

1.5

(a) Normal setting.

2

2.5

0

0.5

1

1.5

2

2.5

(b) Exponential setting.

Figure 1: Comparison of power of our permutation-based higher criticism test under different
choices of dn as in (19). The power of the test is depicted as a function of a τ in the
parameterization in (17). We took n = 103 streams with length t = 48 and set |S| = 12. For
each test 103 permutations were used. Each level was repeated 103 times, resulting in the
95% confidence bars depicted.
• Stream lengths. Given that calibration by permutation is meaningless for t = 1 we
expect the performance gap between an oracle and the proposed test to be large for
small streams. However, we observe the finite-sample performance is already quite
comparable to the oracle test even for very small streams.
• Performance loss. Our test makes no use of knowledge of the null distribution. While
asymptotically equivalent to first order, we expect a performance loss in finite samples
compared to the oracle.
We use n = 103 streams in our simulations. To study the influence of stream length we
considered |S| = 12 (i.e. β ≈ 0.65, moderately sparse regime) but the results are qualitatively identical for other choices of sparsity. The signal strength was chosen slightly above
the asymptotic detectability threshold, but ensuring the difference in performance was highlighted. For the exponential model very short streams would lead to a value θ ≥ λ0 in the
parameterization in (17), so the shortest stream considered is t = 4. Figure 2 show that
the performance is comparable with respect to the oracle tests even at very short lengths,
suggesting that finite-sample performance is much more forgiving than what our theoretical
characterization might suggest, as there is only a very small loss of power in comparison
to an oracle. Naturally, the permutation test has no power for streams of length t = 1.
Note that the strength of the anomalous signal is decreasing in t, such that in the normal
model the oracle test performs identically for any value of t. In contrast, for the exponential
model, the difference in null and alternative means is θ/(λ0 (λ0 − θ)) and is larger for smaller
streams, so all tests perform worse at larger values of t.
We now turn our attention to the power characteristics of the proposed tests. We fix n = 103

16

0.6

1
HC-permutation
HC-oracle

0.9

0.5

0.8
0.7

Power

Power

0.4
0.3

0.6
0.5

0.2

0.4
0.1
HC-permutation
HC-oracle

0
100

101

102

0.3
0.2

101

t

(a) Normal setting: τ = 1.5.

102

103

t

(b) Exponential setting: τ = 1.25.

Figure 2: Comparison of power of our permutation-based higher criticism test and the oracle
test, as a function of a stream length t. We used the parameterization in (17) with a fixed
level for τ . Note that the strength of the anomalous signal is decreasing in t as discussed
in the text. For the exponential model, the parametrization of the alternative is undefined
for t ≤ 3, so the t-axis starts at t = 4. We took n = 103 streams and set |S| = 12. For
the permutation tests, 103 permutations were used. For the oracle test, the Monte-Carlo
calibration is based on 104 samples. Each level was repeated 103 times, resulting in the 95%
confidence bars depicted.
and t = d(log n)2 e = 48 and consider |S| = 12 (i.e. β ≈ 0.64, moderately sparse regime),
and |S| = 3 (i.e. β ≈ 0.84, very sparse regime). The results are plotted in Figure 3, and we
see that the proposed permutation higher criticism test has only minute loss of power when
compared to the oracle test.

5.5

Comparison with approximation methods

As mentioned earlier, several authors have considered practical variants of higher criticism
where the permutation stream means are assumed to be approximately normal, and therefore
the test statistic is computed based on a normal approximation (e.g., see Sabatti et al.
(2009); Wu et al. (2014) and references therein). In essence, instead of computing (or, more
appropriately, estimating) P̂q (X) as we do in Equation (13), one could instead compute:
p
pΦ
q = 1 − Φ( 2q log(n)) ,
which arises from the assumption that the standardized permutation stream means Yi (Xπ )
are approximately standard normal. The statistic can then be computed like before, and
calibrated by permutation. We refer to this methodology as HC-approximation in what
follows. We also present results of the max test.
To compare the two approaches, we consider the exponential setting outlined in Section 5.2
with n = 100 streams and t = 4 and t = 6 as stream length. The reason to take a smaller

17

1

1
HC-permutation
HC-oracle

0.8

0.8

0.6

0.6

Power

Power

HC-permutation
HC-oracle

0.4

0.4

0.2

0.2

0

0
0

0.5

1

1.5

2

2.5

0

(a) Normal setting: |S| = 12.

1

1.5

2

2.5

(b) Exponential setting: |S| = 12.

1

1
HC-permutation
HC-oracle

HC-permutation
HC-oracle

0.8

0.8

0.6

0.6

Power

Power

0.5

0.4

0.4

0.2

0.2

0

0
0

0.5

1

1.5

2

(c) Normal setting: |S| = 3.

2.5

0

0.5

1

1.5

2

2.5

(d) Exponential setting: |S| = 3.

Figure 3: Comparison of power of our permutation-based higher criticism test and the
oracle test, as a function of a multiplicative factor τ in the parameterization in (17). We
took n = 103 streams with length t = 48. For the permutation tests, 103 permutations were
used. For the oracle test, the Monte-Carlo calibration is based on 104 samples. Each level
was repeated 103 times, resulting in the 95% confidence bars depicted.

18

1

1
HC-permutation
HC-approximation
Max test

0.8

0.6

Power

Power

0.8

HC-permutation
HC-approximation
Max test

0.6

0.4

0.4

0.2

0.2

0

0
0

0.5

1

(a) Exponential setting: t = 4.

1.5

0

0.5

1

1.5

2

(b) Exponential setting: t = 6.

Figure 4: Comparison of power of our permutation-based higher criticism test and a variant
using asymptotic approximations. The power of the test is depicted as a function of a τ in
the parameterization in (17). We took n = 102 streams and set |S| = 12. For each test 103
permutations were used. Each level was repeated 103 times, giving rise to the 95% confidence
bars depicted.
number of streams than before is to clearly emphasize the differences between the normal
approximation and permutation approaches. We set |S| = 12 (i.e. β ≈ 0.64), but similar
results can be obtained in the other regimes. Figure 4 depicts the results.
We see that there is a clear improvement in finite-sample performance when computing P̂q (X)
by permutation, as opposed to the normal approximation. As expected, the difference in
power decreases as the streams get longer, since the normal approximation becomes more
accurate. In any case, the approximation pΦ
q underestimates the true value of pq when
q is large, as the tail of a Gamma distribution is much heavier than that of a normal
distribution. Since P̂q (X) does not rely on asymptotic approximations it is able to more
accurately estimate pq , resulting on a more powerful test. Extensive experimentation showed
that the permutation-based approach appears to be always superior when the underlying
model is not normal.

5.6

An analysis of daily COVID-19 diagnoses across municipalities
in the Netherlands

After an initial explosive spread of COVID-19 several countries are entering a more stable
regime. When in such a regime, it is of high importance to policy makers to detect signs of
a new impending outbreak: for instance, when a few municipalities of the country reporting
higher-than-usual number of cases.
When the virus is circulating in a reasonably controlled fashion, the dynamics of the disease
transmission are homogeneous across different municipalities and a relatively short time-

19
frame is considered, it is not unreasonable to model the daily diagnoses per capita of each
municipality as originating i.i.d. from some unknown distribution. However, when the virus
is spreading fast in a few municipalities those will display a slightly higher-than-usual number
of daily-cases. As such, the framework we have introduced in this paper can be useful here
to detect signs of local outbreaks, as these would lead to rejection of our null hypothesis (1).
We consider data from the Netherlands. In this country, it is not unreasonable to assume
municipalities are sufficiently comparable: the country is very small and population density
is not too different among municipalities. We consider a timeframe of 5 days. Note that the
incubation period of COVID-19 is believed to be between two and fourteen days – see, for
example Lauer et al. (2020).
We use data on new COVID-19 cases per 100.000 inhabitants from 13th of March up until
10th of August 2020, for each of the 355 municipalities of the Netherlands. This data was
processed from two sources; the data on the number of diagnoses (uncorrected for municipality population) was retrieved from the Dutch national institute for public health and the
environment (RIVM) at https://data.rivm.nl2 , and the data on municipality population was
retrieved from the Dutch central agency for statistics (CBS) at https://opendata.cbs.nl3 .
These were combined to obtain the number of COVID-19 cases per 100.000 inhabitants.
When tests are done on the raw data we often reject the null hypothesis due to one or a few
very large observations. While these cases are important in the context of the application,
one needs no powerful test to mark them as anomalous as their abnormality is so clear.
Especially in the context of COVID-19, these “clear” outliers will be investigated regardless.
A more interesting question is then if, after removing “clear” outliers, the other municipalities
still contain anomalous signals or not.
To remove “clear” outliers in a rigorous way, we use the max test as described in Theorem 2.
We obtain the 95% quantile of the permutation maximum stream mean distribution, and
mark all observations exceeding this threshold as “clear” anomalies. Then, we apply our
permutation higher criticism test as in Theorem 3, as well as the higher criticism test using
a normal approximation as in Section 5.5 on the remaining data to detect possible signals.
We apply our test sequentially over a sliding window of five days across the time domain,
and obtain results for each window. The sequential p-values obtained in this fashion are
plotted in Figure 5, along with the virus’ nationwide progression. Obviously, the obtained
p-values are dependent and care must be taken when interpreting them - the figure is given
merely to aid the presentation.
As can be seen in Figure 5, our test results in much smaller p-values than the approximation
method at each window. There are cases where our test indicates anomalies in seemingly
stable periods. In these periods, while hard to see in aggregated data, we thus have some
evidence that some municipalities have larger-than-usual values. This does not necessarily
lead to a nationwide outbreak, since local measures, such as a restricting access to specific
2

The specific hyperlink to this data is https://data.rivm.nl/geonetwork/srv/dut/catalog.search#
/metadata/1c0fcd57-1102-4620-9cfa-441e93ea5604
3
The specific hyperlink to this data is https://opendata.cbs.nl/statline/?dl=2096B#/CBS/nl/dataset/
70072NED/table

20

3000
HC-permutation
HC-approximation
Number of daily diagnoses

0.9
0.8

p-value

0.7

2500

2000

0.6
0.5

1500

0.4
1000

0.3
0.2

Number of daily diagnoses

1

500

0.1
0
0

20

40

60

80

100

120

140

0
160

Time in days since 13th of March

Figure 5: The p-values of our permutation higher criticism test and the higher criticms test
using normal approximations for a sliding window of the previous five days, along with the
total number of daily diagnoses in the Netherlands. For each test 103 permutations were
used.
nursery homes, might have been taken to prevent further spread. As our data is aggregated
per municipality and local measures are very hard to identify, we are not able to take this
into account in our analysis.
Nonetheless, it would be interesting to identify the anomalous municipalities contributing to
the low p-value of our test. A natural, but perhaps naive approach here would be to compute
p-values of each stream by permutation, and apply suitable multiple-testing methods to
control the family-wise error rate or the false discovery rate. Note, however, that such
permutation p-values will be dependent, so standard methods as Benjamini and Hochberg
(1995) might lead to misleading results. Furthermore, note that the signal picked up by our
test may very well be too faint to be reliably estimated, but large enough to be detectable.
Donoho and Jin (2004) include a discussion on this in the normal model we consider. This
remains an interesting area for future research. Work related to this problem can be found,
for example, in Bartroff and Song (2016); Romano et al. (2008); Romano and Wolf (2007).

21

6

Discussion

This paper considered the problem of anomaly detection when one observes multiple streams
of data. This problem is closely related to the detection of sparse mixtures, where it is known
that a test based on a higher criticism statistics is asymptotically optimal. Computation
of this test statistics and calibration of the ensuing test requires the knowledge of the null
(nominal) distribution. In this work we propose a distribution-free version of the statistic
that is based on permutation ideas, and we show that it is also asymptotically optimal in
the context of one-parameter exponential families. The proposed test is exact, and appears
to be superior to alternative proposals that make use of asymptotic approximations. The
numerical results in Section 5.4 show that the calibration by permutation results in nearly
no loss of power in finite samples compared to an oracle test constructed and calibrated
with full distribution knowledge. Its usefulness on real statistical problem is demonstrated
in Section 5.6.
Our asymptotic results are proven for the class of exponential models. In Delaigle et al. (2011)
a less restrictive class of models is considered, for which the proposed testing methodology
attains a similar detection boundary when streams are of length t = n. We conjecture
that, when we consider a similar class of models (i.e., a location family where the base
distribution satisfies some tail-decaying condition) the detection boundary ρ∗ (β) is attained
as well, provided the streams are long enough. Specifically, when the tails of the base
distribution are decaying exponentially, a stream length t = ω(log3 n) suffices, as shown in
our work. For Pareto tails, we conjecture that at least t = ω(n) is needed to attain the ρ∗ (β)
detection boundary, using large and moderate deviations theory results from Mikosch and
Nagaev (1998). One should keep in mind, however, that in an arbitrary location family the
stream means are in general not a sufficient statistic for the location parameter, therefore the
optimal detection boundary might not be characterized in the same way as for the normal
model. For example, if Fθ is the uniform distribution in [θ, θ + 1] a test based on the stream
means will surely be suboptimal.
Further research could discuss the possibility of adapting the Berk-Jones statistic (Berk
and Jones, 1979) in a similar fashion. Alternatively, one could investigate the possibility of
replacing the observations by their ranks. This has numerical advantages and in some cases
leads only to a minor loss of power (Arias-Castro et al., 2018). A thorough analysis of this
proposal is challenging as one must carefully account for the dependencies induced by the
use of ranks, and remains an interesting avenue for future work.

Acknowledgments
Ivo Stoepker would like to thank Richard A.J. Post for interesting discussions on applications
of our methods. Rui Castro and Ery Arias-Castro would like to thank Ervin Tánczos and
Meng Wang for helpful discussions in the early stages of this project.

22

7

Proofs

In this section we provide the proofs of our main results. We begin with a simple technical
lemma that greatly facilitates the presentation. The proof is a trivial consequence of standard
tail-bounds for the normal approximation (Feller, 1968, Section 7.1, Lemma 2).
Lemma 1. Let Φ be the cumulative distribution function of the standard normal distribution
and x ≥ 0. Then

p
2(x + o(1)) log(n) = n−x+o(1) as n → ∞ .
1−Φ
Next, we argue that for the proofs of our main results, we can assume F0 has zero mean and
unit variance.

7.1

Simplifying assumption

To prove the results in this paper it suffices to consider the case where the nominal distribution F0 has zero mean and unit variance. To see this suppose F0 has arbitrary mean and
variance µ0 and σ02 . Define F̃0 (x) = F0 (µ0 + σ0 x). It is easy to see the distribution F̃0 has
zero mean and unit variance. Using this we can easily re-parameterize the hypothesis test
in (10).
Let X be a random variable with distribution Fθ for some θ ∈ [0, θ∗ ) and define X̃ =
R
X−µ0
. Define also ϕ̃0 (θ̃) = eθ̃x dF̃0 (x), the moment generating function of F̃0 . It is easy to
σ0


check that X̃ has density with respect to F̃0 given by exp θ̃x − log(ϕ̃(θ̃)) where θ̃ = σ0 θ
(equivalently θ = σ10 θ̃). Therefore, statements in θ̃ pertaining a zero mean and unit variance
distribution can be translated to a general distribution by simple multiplication by a factor
1/σ0 .

7.2

Proof of Theorem 1

Proof. Without loss of generality and as explained in Section 7.1 we assume that F0 has
mean zero and variance one, as this makes the arguments easier and less cluttered.
Let ψ(X) : Rnt → {0, 1} denote an arbitrary test function. We begin by bounding the worst
case risk of this test by the average risk, namely
R(ψ) = P∅ (ψ(X) 6= 0) + max PS (ψ(X) 6= 1)
S:|S|=s

≥ P∅ (ψ(X) 6= 0) +

1 X

PS (ψ(X) 6= 1) .
n
s

S:|S|=s

The average risk can naturally be interpreted as the risk of testing the simple null hypothesis
against a simple alternative, where S is chosen uniformly at random over the class of all

23
subsets of [n] with cardinality s. Since we are doing a test between two simple hypotheses
the optimal test (i.e., the test minimizing the average risk) is given by the Neyman-Pearson
lemma, namely ψ(X) = 1 {L ≥ 1} where L is the likelihood ratio given by
L≡

X
1 X

exp
(θX
−
ts
log(ϕ
(θ)))
with
X
≡
Xij .
S
0
S
n
s

S:|S|=s

i∈S,j∈[t]

The risk of this test can be easily expressed as 1 − 21 E (|L − 1|), where the expectation is
with respect to the null hypothesis (so all Xij are i.i.d. with distribution F0 ). To proceed we
need to get an upper bound on E (|L − 1|). A simple, but often useful way to proceed is to
use Jensen’s inequality to get
p
p
E (|L − 1|) ≤ E ((L − 1)2 ) = E (L2 ) − 1 ,
where the equality above follows since E (L) = 1. This approach is generally referred to as
the second moment method. To show any test is asymptotically powerless it suffices therefore
to show that E (L2 ) converges to one as n → ∞.
To simplify the presentation let S and S 0 denote two independent random variables, and both
independent from X. Both S and S 0 are sampled uniformly from the set {S ⊂ [n] : |S| = s}.
Then clearly L = E (exp (θXS − ts log(ϕ0 (θ))) |X) and therefore

E L2 = E (exp (θXS − ts log(ϕ0 (θ))) exp (θXS 0 − ts log(ϕ0 (θ))))
= E (exp (θ(XS + XS 0 ) − 2ts log(ϕ0 (θ))))
= E (exp (t|S ∩ S 0 |(log ϕ0 (2θ) − 2 log ϕ0 (θ)))) .
For the last equality we used the fact that for all S and S 0 we have XS +XS 0 = 2XS∩S 0 +XS4S 0
(in the previous expression 4 denotes the symmetric set difference).
The beauty of the above result is that is reduces quantification of the risk to a statement
about the moment generating function of the random variable K ≡ |S ∩ S 0 |. Given the distribution S and S 0 we conclude that K has an hypergeometric distribution with parameters
(n, s, s), and therefore K is stochastically bounded from above by the binomial distribus
). Using the well-know expression for the moment generating
tion with parameters (s, n−s
function of a binomial distribution we conclude that

s
s
s
E L2 ≤ 1 − n−s
+ n−s
κ(θ)t ,
where κ(θ) ≡ ϕ0 (2θ)/ϕ0 (θ)2 . Therefore E (L2 ) → 1 provided
s2
(κ(θ)t − 1) → 0 .
n−s
Consider now the specific parameterizations of s and θ in the theorem statement. Note that
when t = ω(log3 n) then necessarily θ → 0, so we can conveniently use a Taylor expansion
of the moment generating function ϕ0 (θ) around θ = 0:
ϕ0 (θ) = ϕ0 (0) + θϕ00 (0) +

θ2 00
ϕ (0) + O(θ3 ) ,
2 0

(20)

24
as θ → 0. Using the fact that F0 has mean zero and unit variance we get ϕ0 (θ) = 1 +
1 2
θ + O(θ3 ) as θ → 0. Simple asymptotic algebra yields that κ(θ) = 1 + θ2 + O(θ3 ). Since
2
1 + x ≤ ex we conclude that κ(θ) ≤ exp (θ2 + O(θ3 )).
When β > 1/2 we conclude that
s2
(κ(θ)t − 1) = (1 + o(1))n1−2β (κ(θ)t − 1)
n−s


≤ (1 + o(1))n1−2β exp tθ2 + O(tθ3 ) − 1
= (1 + o(1))exp ((1 − 2β) log n) (exp (2r log n + o(1)) − 1) .
The last expression converges to 0 provided 1 − 2β + 2r < 0 meaning that when r < β − 1/2
any test is asymptotically powerless. This lower bound is tight when β ∈ (1/2, 3/4] (the
moderately sparse regime) but it is a bit loose for the very sparse regime. However, a
modification of the above argument allows us to get a tight lower bound when β > 3/4.
The very sparse regime: the main limitation of the second moment method as presented
above has to do with the fact that the likelihood ratio statistic L might take rather large
values. Although this might be a rare occurrence, it can be enough to ensure the second
moment is much larger than the first moment. A way to mitigate this issue is to consider a
so-called truncated second moment method. Let Ω denote an arbitrary event and define the
truncated likelihood ratio L̃ ≡ L1 {Ω}. Clearly L̃ ≤ L and therefore


E (|L − 1|) = E |L − L̃ + L̃ − 1|


 
≤ E |L̃ − 1| + 1 − E L̃
r  
 
 
≤ E L̃2 − 2E L̃ + 1 + 1 − E L̃ ,
where we used the triangle inequality and the fact that E (L) = 1, followed by Jensen’s

inequality. Therefore, to show a test is powerless it suffices to show that both E L̃ and
 
E L̃2 converge to one as n → ∞. The choice of event Ω is therefore quite crucial. In the
present context we are going to consider the event




r



2(1 + η) log n 
Ω = max Yi <
,
(21)


i∈[n]
t


|
{z
}


≡τ (η)

where η > 0 must be carefully chosen.
 
Truncated first moment: Note first that E L̃ = E (L1 {Ω}) is the probability of Ω
under the alternative hypothesis (where there is a set S of anomalous streams and S is chosen
uniformly at random over the subsets of [n] with cardinality s). Given the symmetry of the

25
 
definition of Ω we see that E L̃ = PS (Ω) where S is an arbitrary set with cardinality s.
Without loss of generality let S = [s]. Then
 
E L̃ = PS (Ω)


= 1 − PS max Yi ≥ τ (η)
i∈[n]

= 1 − sPS (Y1 ≥ τ (η)) − (n − s)P∅ (Y1 ≥ τ (η)) .
using the union bound in the last line. Using Lemma 6 we conclude that
P∅ (Y1 ≥ τ (η)) = n−1+η+o(1) ,
and provided r ≤ 1 + η

√

PS (Y1 ≥ τ (η)) = n−(

√
1+η− r)2 +o(1)

.

Therefore, when r ≤ 1 + η
 
√
√ 2
E L̃ = 1 − n1−β n−( 1+η− r) +o(1) − (n − s)n−1+η+o(1)
√

√

2

= 1 − n1−β−( 1+η− r) +o(1) − O(1) → 1 ,
√
√
provided r < ( 1 + η − 1 − β)2√
. This means the first truncated moment converges to one
for any η > 0, provided r < (1 − 1 − β)2 .
Truncated second moment: bounding this term requires significantly more work. Begin
by noting that
 
E L̃2 = E (exp (2θXS∩S 0 + θXS4S 0 − ts log(ϕ0 (θ))) 1 {Ω})


Y
1 {Yi < τ (η)}
= ϕ0 (θ)−st E exp (2θXS∩S 0 ) exp (θXS4S 0 )
i∈[n]

!

≤ ϕ0 (θ)−st E exp (2θXS∩S 0 ) exp (θXS4S 0 )

Y

1 {Yi < τ (η)}

i∈S∪S 0

≤ ϕ0 (θ)−st E (exp (t|S ∩ S 0 |(log ϕ̃0 (2θ)) − t|S4S 0 |(log ϕ̃0 (θ))) ,
where ϕ̃0 (θ)t ≡ E (exp (θtY1 ) 1 {Y1 < τ (η)}). The steps above mimic the derivation for the
regular second moment, and the main difference is that we now need to consider the moment
generating function of a truncated distribution, instead of the original distribution. Clearly
ϕ̃0 (θ) ≤ ϕ0 (θ) and so we conclude that
 
E L̃2 ≤ E (exp (t|S ∩ S 0 |(log ϕ̃0 (2θ) − 2 log ϕ0 (θ)))) .
Define κ̃(θ) ≡ ϕ̃0 (2θ)/ϕ0 (θ)2 . As before, to show the truncated moment converges to zero it
suffices to show that
s2
(κ̃(θ)t − 1) → 0 .
n−s

26
Note that, the argument based on the untruncated second moment method indicates all tests
are powerless if r < β − 1/4. Since we are considering the case β ≥ 3/4 this means that it
suffices to treat only the case where r ≥ 3/4 − 1/2 = 1/4. To get an upper bound on ϕ̃0 (2θ)t
we make use of the following technical result.
Lemma 2. Let X be a real-valued random variable and let f : R → [0, ∞) be one-to-one
increasing and differentiable. Then, for any τ ∈ R,
Z τ
P(X > x)f 0 (x)dx .
(22)
E (f (X)1 {X ≤ τ }) =
−∞

To use this lemma we must get a good upper bound on P∅ (Y1 > x) for x ≤ τ (η). When
x ≤ 0 we trivially bound this probability by one, and for 0 ≤ x < θ∗ we make use of a simple
Chernoff bound. In a similar fashion to the proof of Lemma 6 we have


X
P∅ (Y1 > x) ≤ P∅ 
X1j ≥ xt
j∈[t]

"

#!

≤ exp −t

sup {λx − log(ϕ0 (λ))}
λ∈[0,θ∗ )

"

#!
sup {λx − log(1 + λ2 /2 + O(λ3 ))}

= exp −t

λ∈[0,θ∗ )


≤ exp −t x2 − log(1 + x2 /2 + O(x3 ))}

≤ exp −t(x2 /2 + O(x3 )) .


Let n be large enough so that τ (η) < θ∗ . Applying the above result and Lemma 2 we get
Z τ (η)
t
P (Y1 > x) 2θtexp (2θtx) dx
ϕ̃0 (2θ) =
−∞
0

Z
≤

Z
2θtexp (2θtx) dx +

−∞

τ (η)


exp −t(x2 /2 + O(x3 )) 2θtexp (2θtx) dx

0

Z

τ (η)


exp −t(x2 /2 + O(x3 )) exp (2θtx) dx
0
Z

 τ (η)

2
3
= 1 + 2θtexp 2θ t exp O(tτ (η))
exp −t(x − 2θ)2 /2) dx

≤ 1 + 2θt

0

√

√

√

 (τ (η)−2θ) t 1
2
√
= 1 + 8π(1 + o(1))(θ t)exp 2θ2 t
exp
−y
/2
dy
√
2π
−2θ t
√
√
√

≤ 1 + 8π(1 + o(1))(θ t)exp 2θ2 t Φ((τ (η) − 2θ) t) ,
Z

where in the second to last step we used the fact that t = ω(log3 n). At this point note that
r 
2
p
√
√
(τ (η) − 2θ) t = − 2
4r − 1 + η log n < 0

27
when r ≥ 1/4, provided we choose η > 0 sufficiently small. Therefore using Lemma 1 we
conclude that
√
p
√
√
2
ϕ̃0 (2θ)t ≤ 1 + 8π(1 + o(1)) 2r log nn4r n−( 4r− 1+η) +o(1)
√
p
√
√
2
= 1 + 8π(1 + o(1)) 2r log n n4r−( 4r− 1+η) +o(1)
√

= n4r−(

√
4r− 1+η)2 +o(1)

.

With an analogous argument to the one used for ϕ̃(2θ)t one can show that (ϕ0 (θ))2t =
n2r+o(1) . Therefore
√
√
s2
2
(κ̃(θ)t − 1) = o(1) + n1−2β+2r−( 4r− 1+η) +o(1) .
n−s
√
√
2
The above converges to zero when
√ is the 2case η
√ 1 −22β + 2r − ( 4r − 1 + η) < 0. This
is small enough and r < (1 − 1 − β) , since in that case 1 − 2β + 2r − ( 4r − 1) < 0,
completing the proof.

7.3

Proof of Theorem 2

Proof. By the arguments in Section 7.1, the proof continues under the assumption that F0
has zero mean and unit variance.
Under the null and for a given (but arbitrary) permutation π ∈ Π it is clear that Xπ and
X have exactly the same distribution. Therefore maxi {Yi (X)} is uniformly distributed on
the set {maxi {Yi (Xπ }, π ∈ Π} (with multiplicities) conditionally on the order statistics of
X. So, for a given α > 0


π
P∅ (Pmax-perm (X) ≤ α) = P∅ {π ∈ Π : max{Yi (X )} ≥ max{Yi (X)}} ≤ α(nt)!
i

≤

i

bα(nt)!c
≤α,
(nt)!

If there are no ties, the first inequality above is an equality, but with ties present the test
becomes slightly more conservative. This argument is completely standard and for more
details on permutation tests the reader is referred to (Lehmann and Romano, 2005).
What remains to be proven is the behavior of the test under the alternative. Namely we
must show that, provided r is large enough (as stated in the proposition) then for any α > 0
P∅ (Pmax-perm (X) > α) −→ 0 .
For convenience, let π be a uniformly distributed permutation of Π and let this be independent from X. We can rewrite our permutation p-value as a conditional probability:


Pmax-perm (X) = P max Yiπ ≥ max Yi X .
i

i

To get a good upper-bound on the permutation p-value we use the following concentration
inequality (see Shorack and Wellner (1986) and Arias-Castro et al. (2018), for instance).

28
Lemma 3 (Bernstein bound for sampling without replacement). Let (Z1 , . . . , Zm )
beP
sampled withoutPreplacement from the
Pn set {z1 , . .2. , zn }. Define zmax = maxj {zj }, z =
n
n
1
1
1
2
j=1 zj , Z = m
j=1 Zj and σz = n
j=1 (zj − z) . Then, for all τ ≥ 0:
n

P Z ≥ z + τ ≤ exp −


mτ 2
2σz2 + 23 (zmax − z)τ


.

Using this lemma, we find that


π
Pmax-perm (X) = P max Yi (X ) ≥ max Yi (X) X
i
i

X 
π
≤
P Yk (X ) ≥ max Yi (X) X
i

k∈[n]


=

X
k∈[n]

≤

X

P


1X
t



π
Xk,j
≥ X + max Yi (X) − X



i

X

j∈[t]

t maxi Yi (X) − X

exp −

2

!


2
2σX
+ 23 (maxi,j Xij − X) maxi Yi (X) − X
!
2
t maxi Yi (X) − X
 .
= n · exp − 2
2σX + 23 (maxi,j Xij − X) maxi Yi (X) − X
k∈[n]

The first inequality is a consequence of a simple union bound, and we used Lemma 3 in the
second inequality.
At this point it is clear that, to control the p-value of our test we need to characterize
2
, maxi,j Xij and maxi Yi (X) under the alternative hypothesis. Since
the behavior of X, σX
|S| = o(n) most of the elements of X are samples from the null distribution. Therefore we
2
should be good estimators for the mean and variance of F0 .
intuitively expect that X and σX
The behavior of the term maxi,j Xij is a bit more delicate, but one can see that for the given
parameterization of the null the dominant contribution is still given by the null distribution.
In contrast, the term maxi Yi (X) − X really depends on the alternative - the largest stream
mean is surely driven by the anomalous observations. Formally, we can show the following
result.
p
Lemma 4. Let β ∈ ( 12 , 1), θ =
2r(log n)/t with r > 0 and consider the alternative
hypothesis in (10). Assume F0 has zero mean and variance one and t = ω(log(n)). Then
(i) X = OP



√1
nt



2
and σX
= 1 + OP



√1
nt



(ii) Let c ∈ (0, θ∗ − θ). Then,


3
P max Xij − X ≤ log(nt) → 1 as n → ∞ .
i,j
c

29
√
√
(iii) Assume further that t = ω(log3 (n)) and let ε > 0. Provided r > ( 1 + ε − 1 − β)2
we have
!
r
2(1 + ε)
log(n) → 1
P max Yi (X) − X ≥
i
t
as n → ∞.
The first result of the lemma provides a rate at which the bound on our sample variance
can decrease. For the analysis of the max test a much simpler result (already proved in
Arias-Castro et al. (2018)) suffices: for any ε > 0 (i) implies that

2
≤ (1 + ε/2) → 1 .
PS σ X
Note also that the bound in Lemma 3 is monotonically decreasing in τ . This ensures
√ that
for
ε
>
0
and
with
probability
tending
to
one
under
the
alternative,
provided
r
>
(
1 + ε−
√
2
1 − β) , the overall p-value of test satisfies


2(1 + ε) log(n)
 .
q
Pmax-perm (X) ≤ n · exp −
2
1
2(1 + ε/2) + 2ε + c log(nt) 2(1 + ε) t log(n)
Written differently, with probability tending to 1 under the alternative:


1+ε
 .
q
log Pmax-perm (X) ≤ log(n) 1 −
log(n)
1
1 + ε/2 + c (log(n) + log(t)) 2(1 + ε) t
To ensure Pmax-perm (X) → 0, or equivalently, log Pmax-perm (X) → −∞ it suffices to ensure
r
1
log(n)
(log(n) + log(t)) 2(1 + ε)
< ε/2 .
c
t
However, since we assume t = ω(log3 (n)) this is immediately satisfied, since the l.h.s. converges to zero.
√
√
We have just proved that, for ε > 0 and r > ( 1 + ε − 1 − β)2 , Pmax-perm (X) → 0 as
n → ∞. Since ε > 0 is arbitrary this implies the result in the theorem, concluding the
proof.

7.4

Proof of Theorem 3

Proof. By the arguments in Section 7.1, the proof continues under the assumption that F0
has zero mean and unit variance. Furthermore the conservativeness of this test follows by
the standard argument already presented in the proof of Theorem 2.
For the rest of the proof consider alternative hypothesis. We must show that
PS (Pperm-hc (X) < α) → 1

30
as n → ∞. Like before, we can write our permutation p-value as a conditional probability
as follows:


π
Pperm-hc (X) = PS T̂ (X ) ≥ T̂ (X) X ,
where π is independent from X and uniformly distributed over Π. The first step is to
understand and simplify the role of π in the above expression. This mirrors the analysis
under the null hypothesis in the proof of Theorem 2, as we need to show that the values
of the test statistic computed with permuted data are a good surrogate for the values of
the test statistic under the null. However, the argument becomes more complex due to the
dependencies introduced by the permutation. Like before, we will use the union bound for
the max-operator in the permuted statistic, inducing a multiplicity by the grid-size:

 X 

π
π
Pperm-hc (X) = PS max V̂q (X ) ≥ T̂ (X) X ≤
PS V̂q (X ) ≥ T̂ (X) X .
(23)
q∈Q

q∈Q

To proceed recall that quantifying V̂q (Xπ ) requires the quantification of two terms: N̂q (Xπ )
and P̂q (Xπ ). Note, however, that P̂q (X) is invariant under permutations of X, and therefore
P̂q (Xπ ) = P̂q (X), as explained before. As such the only random quantity
inside the proba

bility symbol above (conditionally on X) is N̂q (Xπ ). Noting that E N̂q (Xπ ) X = nPq (X)
we have

q


X 
π
π
PS N̂q (X ) − E N̂q (X ) ≥ T̂ (X) nP̂q (X)(1 − P̂q (X)) X . (24)
Pperm-hc (X) ≤
q∈Q

To apply Chebyshev’s inequality we need the right-hand-side of the inequality inside the
probability to be positive, and T̂ (X) might be negative. In the latter case we simply bound
the probability by one. Therefore, using Chebyshev’s inequality we get
n
o


n
o 1 T̂ (X) > 0 X Var N̂q (Xπ ) X
Pperm-hc (X) ≤ 1 T̂ (X) ≤ 0 +
,
(25)
T̂ (X)2
n
P̂
(X)(1
−
P̂
(X))
q
q
q∈Q
where we convention that 0/0 = 0. To continue, we must quantify the conditional variance
of N̂q (Xπ ). The permutation on X causes dependencies, but these are benign when realizing
the conditional permuted stream means are negatively associated conditional on the data.
Using Theorem 2.11 and the properties P6 and P4 in Joag-Dev and Proschan (1983), we find
that Yi (Xπ )|X and Yj (Xπ )|X are negatively associated if i 6= j. For ease of notation, define
r
2
2qσX
zq = X +
log(n) .
t
Then,

 X
π
Var N̂ (q) X =
Var (1 {Yi (Xπ ) ≥ zq } | X)
i∈[n]

+

XX

Cov (1 {Yi (Xπ ) ≥ zq } , 1 {Yj (Xπ ) ≥ zq } | X)

i∈[n] j6=i

≤ nP̂q (X)(1 − P̂q (X)) ,

31
where we used the definition of negative association (Definition 2.1 from Joag-Dev and
Proschan (1983)). In conclusion we get the following simple bound for the p-value:
!
n
o
n
o
X
1
Pperm-hc (X) ≤
1 1 T̂ (X) > 0 + 1 T̂ (X) ≤ 0
T̂ (X)2 q∈Q
o
n
o
kn + 1 n
1
=
T̂ (X) > 0 + 1 T̂ (X) ≤ 0 .
(26)
T̂ (X)2
To continue the proof we must show that T̂ (X) is of order larger then kn = no(1) . This mimics
the approach in Proposition 1 and Theorem 2 under the alternative. Recall that T̂ (X) =
maxq∈Q V̂q (X), so it suffices to get a good lower bound for V̂q (X). This requires controlling
both P̂q (X) (which is random), and the counting term N̂q (X) that is more complicated than
before, as it involves estimates of the mean and variance of F0 . Furthermore, we are no longer
assuming normality. At this point we consider separately the cases β > 1/2 and β ≤ 1/2:
p
Let θ = 2r(log n)/t with r > 0. Provided t is large enough, the following lemma gives us
a high-probability upper bound for P̂q (X).
Lemma 5. Consider the setting of Lemma 4 and let q > 0 and t = ω(log3 (n)). There exists
sequence gn → 0 such that under both the null and alternative hypothesis


P P̂q (X) ≤ n−q+gn → 1 .
Let p̄q = n−q+gn where gn is the sequence in the lemma above. Since V̂q (X) is decreasing in
P̂q (X) (see (14)) we find that, under the alternative, with probability converging to 1
N̂q (X) − np̄q
V̂q (X) ≥ p
.
np̄q (1 − p̄q )
The next step is to get a high-probability lower-bound for the counting term. Our first aim
is to bound the effect of the sample statistics. For q > 0 define


v 

u
p




u 2 1 + log n/(nt) q

X 
p
t
Ñq (X) ≡
1 Yi (X) ≥ log n/(nt) +
log(n) .


t

i∈[n] 


p
√ 
2
By Lemma 4, we have X = OP 1/ nt and σX
= 1 + OP ( 1/nt). Note that we could
p
p
replace the terms log n/(nt) with g(n)/(nt), where g(n)/(nt) = o(1), but we chose this
for concreteness.
Therefore, we conclude that N̂q (X) ≥ Ñq (X) with probability tending to 1. Thus, under the
alternative, we have the following high-probability lower bound for V̂q (X):
N̂q (X) − nP̂q (X)
Ñq (X) − np̄q
≥p
≡ Ṽq (X) ,
V̂q (X) = q
np̄q (1 − p̄q )
nP̂q (X)(1 − P̂q (X))

32
where the inequality holds with probability tending to 1.
All the remains to be show is that Ṽq (X) is sufficiently large for an adequate value of q. It
is clear that Ñq (X) is a sum of two independent binomial random variables. Note that


v 

u
p
u 2 1 + log n/(nt) q
p


t

log(n)
wi,q ≡ P Yi (X) ≥ log n/(nt) +

t
r

2q log n
t

r

2q(1 + o(1)) log n
t

= P Yi (X) ≥
= P Yi (X) ≥



1
√ +
qn

!
q
p
1 + log(n)/nt
!
.

Note that this probability is easily characterized by Lemma 6. Let p̃q ≡ wi,q if i ∈
/ S and
ṽq ≡ wi,q if i ∈ S, so that under the alternative
Ñq (X) ∼ Bin(n − s, p̃q ) + Bin(s, ṽq ),
Note that our probabilities p̃q and ṽq resemble the probabilities pq and vq considered in the
proof of Proposition 1. Using Lemma 6 we immediately conclude that p̃q = n−q+o(1) and
 −(√q−√r)2 +o(1)
if r < q
n
.
ṽq =
o(1)
n
if r ≥ q
Therefore p̄q , p̃q and ṽq all have the same asymptotic characterization as in the normal case,
so we can continue in an analogous fashion as in the proof of Proposition 1 and Theorem 2
and conclude that when r > ρ∗ (β)


PS max Ṽq (X) ≥ kn + 1 → 1 ,
q∈Q

since kn = no(1) . We have now all the pieces needed to bound the p-value in (26). The
result just shown trivially implies that T̂ (X) → 1 and (kn + 1)/T̂ 2 (X) → 0 with probability
tending to one, and therefore
PS (Pperm-hc (X) ≤ α) → 1 ,
completing the proof.

33

7.5

Proof of Lemma 2

Proof. We have
E (f (X)1 {X ≤ τ }) =

Z

∞

P(f (X)1 {X ≤ τ } > t)dt

0

Z

f (τ )

P(f (X) > t)dt

=
0

Z

f (τ )

=
Z0 τ
=

P(X > f −1 (t))dt
P(X > x)f 0 (x)dx,

−∞

where in the last line we changed the integration variable to x := f −1 (t).

7.6

Proof of Lemma 4

Proof. Let us start by characterizing the overall sample mean X. By Chebyshev’s inequality
we have



1
X = E X + OP √
,
nt
as n → ∞. Now note that, under the alternative hypothesis
Z
 |S|
|S|
xexp (θx)
E (X) =
dF0 (x) ,
E X =
n
n
ϕ(θ)
where X ∼ Fθ . A Taylor expansion of the function inside the integral around θ = 0 yields
Z
Z
xexp (θx)
dF0 (x) = x + x2 θ + O(θ2 )dF0 (x)
ϕ(θ)
= θ + O(θ2 )
p

Finally θ = O
log(n)/t → 0 since t = ω(log(n)). Putting all this together yield the first
result stated in (i).
For the second result in (i) note that
1 X
1 X 2
2
(Xij − X)2 =
Xij − X
nt i,j
nt i,j
!
!
X


1 X
1
2
=
E Xij2
+
Xij2 − E Xij2
−X
nt i,j
nt i,j

2
σX
=

34
For the first term we see that

1 X
1 X
1 X
E Xij2 =
Var (Xij ) +
Var (Xij ) + E (Xij )2
nt i,j
nt
nt
i∈S,j∈[t]
/

i∈S,j∈[t]

n − |S| |S|
+
(1 + O(θ))
n
nr
!
log
n
,
= 1 + O n−β
t

=

as n → ∞. In the above the variance of the anomalous streams was characterized with a
Taylor expansion of θ around 0, similarly to what was done for the average term.
For the second term note first that all the moments of F0 are finite, in particular the fourth
moment. Therefore by Chebyshev’s inequality we have


1 X 2
1
X − E (Xij ) = OP √
.
nt i,j ij
nt
2

Finally, we know that X = OP (1/nt) when β > 1/2. Putting everything together yields the
second result stated in (i).
The argument needed to prove (ii) is the same already used in Arias-Castro et al. (2018),
and presented here for completeness. Letting x > 0, a union bound gives






P max Xij > x ≤ P max Xij > x + P max Xij > x
i,j

i∈S,j∈[t]

i∈S,j∈[t]
/

≤ |S|t(1 − Fθ (x)) + (n − |S|)t(1 − F0 (x)) .

(27)

Now, let c ∈ (0, θ∗ − θ). We have that:
Z ∞
1
1 − Fθ (x) =
exp (θu) dF0 (u)
ϕ0 (θ) x
Z ∞
1
=
exp ((θ + c)u) exp (−cu) dF0 (u)
ϕ0 (θ) x
Z ∞
1
≤
exp (−cx)
exp ((θ + c)u) dF0 (u)
ϕ0 (θ)
x
ϕ0 (θ + c)
≤
exp (−cx) .
ϕ0 (θ)
Therefore, with considerable slack, we can take x = 2c log(nt) guarantee that both terms
in (27) converge to zero. Together with the characterization of X in (i) we conclude that


3
P max Xij − X ≤ log(nt) → 1 .
i,j
c
To show part (iii) very different argument is needed as this is a lower-bound on the tail probability, rather than an upper bound. The following lemma gives a precise characterization
of the tail probability.

35
Lemma 6. Consider the setting of Lemma 4. Let i ∈ S and let qn → q > r as n → ∞ and
t = ω(log3 n). Then
!
r
√ √ 2
2qn log n
P Yi (X) ≥
= n−( q− r) +o(1) .
t

If q ≤ r we have P Yi (X) ≥

q

2qn log n
t



= no(1) .

To show (iii) begin by noting that
!
2(1 + ε)
P max Yi (X) ≥
log(n)
i∈[n]
t
!
r
2(1 + ε)
≥ P max Yi (X) ≥
log(n)
i∈S
t
!!|S|
r
2(1 + ε)
log(n)
= 1 − 1 − P Yi (X) ≥
t
r

(28)

√
√
Consider the case where ( 1 + ε − 1 − β)2 < r < 1 + ε. Note that, in that case, we can
use Lemma 6 with (28) which gives:
!
r

|S|
2
√
√
2(1 + ε)σ0
−( 1+ε− r)2 +o(1)
log(n) = 1 − 1 − n
P max Yi (X) ≥
i∈[n]
t



√
√
1−β
−( 1+ε− r)2 +o(1)
= 1 − exp n
log 1 − n


√
√ 2
≥ 1 − exp −n1−β n−( 1+ε− r) +o(1) ,
where
in last
we simply used the fact that log(1 + √
x) ≥ x. √
Finally, provided
√
√ inequality
2
( 1 + ε − 1 − β) < r < 1 + ε we guarantee that 1 − β − ( 1 + ε − r)2 + o(1) > 0.
The statement for r > 1 + ε follows immediately since this probability is monotonically
increasing in r. To get the statement in (iii) we just need to use this result together with
the characterization of X, concluding the proof.

36

7.7

Proof of Lemma 5

Proof. We begin by using Lemma 3 to obtain an upper-bound on P̂q (X)
!
r
2
2σX q
log(n) X
P̂q (X) = P Y1 (Xπ ) − X ≥
t


2
2σX q log(n)

q
≤ exp −
2 q
2σX
2
2
log(n)
2σX + 3 (maxi,j Xij − X)
t


q log(n)
 .
q
= exp −
2q
1
1 + 3 (maxi,j Xij − X) tσ2 log(n)
X

2
Now, part (i) of Lemma 4 implies that σX
≤ 1 + ε for ε > 0, with probability tending to 1.
Using part (ii) of the same lemma we get


q log(n)

q
P̂q (X) ≤ exp −
2q
1
1 + c log(nt) t(1+ε) log(n)

with probability converging to 1. Therefore, provided t = ω(log3 (n)) there is a function
gn → 0 such that
P̂q (X) ≤ exp (−(q + gn ) log(n)) .

7.8

Proof of Lemma 6

To streamline the presentation let W1 , . . . , Wt be i.i.d. with distribution
Fθ and denote by
p
ϕθ (x) the moment generating function of Fθ . Define also τ = (2qn /t) log n. We start by
getting an upper bound for the said probability when r < q. A simple Chernoff bounding
argument yields


"
#!
X
1
P
Wj ≥ τ  ≤ exp −t
sup {λτ − log(ϕθ (λ))}
.
(29)
t
λ∈[0,θ∗ −θ)
j∈[t]

We must now characterize ϕθ (λ). First note that ϕθ (λ) = ϕ0 (λ + θ)/ϕ0 (θ). Now, we develop
a Taylor expansion of ϕ0 (λ) around λ = 0, as we did in Equation (20). Note that F0 has
zero mean and unit variance. We obtain:
λ2
ϕ0 (λ) = 1 +
+ O(λ3 ) ,
(30)
2
as λ → 0. A similar expansion can be developed for ϕ0 (λ + θ) around λ + θ = 0. Combining
all this yields
ϕθ (λ) =

1 + 1/2(λ + θ)2 + O((λ + θ)3 )
λ2
=
1
+
θλ
+
+ O((λ + θ)3 ) ,
1 + θ2 /2 + O(θ3 )
2

37
as both λ, θ → 0, where we used a Taylor expansion for the fraction around θ2 /2+O(θ3 ) = 0.
This suggests the choice λ∗ = τ − θ, which is positive provided n is large enough since r < q.
This choice yields the bound
{λτ − log(ϕθ (λ))} ≥ τ 2 − log(ϕθ (λ))

sup

λ∈[0,θ∗ −θ)


1
≥ (τ − θ)2 + O τ 3 ,
2
since τ > θ and we used the basic inequality log(1 + x) ≤ x. When t = ω(log3 n) the first
term dominates, and therefore we conclude that


X

√
1
√
P
Wj ≥ τ  ≤ exp −( qn − r)2 (log n) + o(1)
t
j∈[t]

√

= n−(

√
q− r)2 +o(1)

.

To lower-bound the probability in (29) we use a tilting argument. Let θτ be such that Fθτ
has mean τ . Such a choice exists for n large enough and necessarily θτ > θ for large n, since
r < q. Define W̃1 , . . . , W̃t to be i.i.d. with distribution Fθτ . Then

P


1X
t

Z

Z
Wj ≥ τ  =

j∈[t]

1


1 X

1


1 X
t

j∈[t]

wj ≥ τ




dFθ (w1 ) · · · dFθ (wt )




t
Y

wj ≥ τ
exp (θwj − log ϕ0 (θ)) dF0 (w1 ) · · · dF0 (wt )
t

j=1
j∈[t]





Z  X
t
Y
ϕ0 (θ)
1
wj ≥ τ
exp (θ − θτ )wj − log
dFθτ (w1 ) · · · dFθτ (wt )
= 1

t
log ϕ0 (θτ )
j=1
j∈[t]

 



t


X
X
ϕ0 (θτ )
1
=
E 1
W̃j ≥ τ exp −(θτ − θ)
W̃j  .
t

ϕ0 (θ)
=

j∈[t]

j∈[t]

With this change of measure we can conveniently use the central limit theorem to get a
meaningful bound. Begin by noting that


1X
P
Wj ≥ τ 
t
j∈[t]

 



t


X
X
ϕ0 (θτ )
1
W̃j − τ
≥
E 1 0 ≤ √
≤ 1 exp −(θτ − θ)
W̃j 


ϕ0 (θ)
t j∈[t] σθτ
j∈[t]



t


X
√
ϕ0 (θτ )
1
W̃j − τ
≥
exp −(θτ − θ)(tτ + tσθτ ) P 0 ≤ √
≤ 1 ,
ϕ0 (θ)
σθτ
t
j∈[t]

38
where σθ2τ denotes the variance of Fθτ . By the central limit theorem we know that
1 X W̃j − τ
√
t j∈[t] σθτ
converges in distribution to a standard normal distribution and therefore the probability in
the expression above converges to Φ(1) − Φ(0) ≈ 0.34 > 1/4. Therefore we conclude that,
for n large enough



t


X
√
1
1
ϕ
(θ
)
0
τ
P
Wj ≥ τ  ≥
exp −(θτ − θ)(tτ + tσθτ ) .
t
4 ϕ0 (θ)
j∈[t]

To control the remaining terms recall that ϕ0 (λ) = 1 + λ2 /2 + O(λ3 ) as λ → 0 (see Equation (30)). Note also that τ = θτ + O(θτ2 ), which implies (after some manipulation) that
θτ = τ + O(τ 2 ). Finally, note that both τ and θ have the same order of magnitude. Putting
all this together we conclude that

t !

ϕ0 (θτ )
t 2
log
=
τ − θ2 + O(θ3 ) .
ϕ0 (θ)
2
√
For the other term note that σθτ = 1 + o(1), and therefore σθτ / t = o(θ). This implies that
√

−(θτ − θ)(tτ + tσθτ ) = −t τ (τ − θ) + O(θ3 ) .
In conclusion


P



1X
t

j∈[t]




t
1
2
3

Wj ≥ τ ≥ exp − (τ − θ) + O(θ )
.
4
2

When t = ω(log3 n) the term (τ − θ)2 term dominates, and we see we get the asymptotic
behavior as in the upper bound, concluding the proof.

 P
For the case r ≥ q we see that necessarily P 1t j∈[t] Wj ≥ τ ≥ no(1) , so the tail probability
cannot be extremely small. In fact, when r > q this probability will be lower bounded by a
constant.

39

A
A.1

Appendix
Proof of Proposition 1

Proof. For convenience, define |S| = s. The first statement in the proposition is a simple
consequence of Chebyshev’s inequality. Under the null hypothesis Nq (X) ∼ Binomial(n, pq ),
therefore
!
Nq (X) − npq
≥ hn
P∅ (Vq (X) ≥ hn ) = P∅ p
npq (1 − pq )


q
= P Nq (X) − npq ≥ hn npq (1 − pq )


q
≤ P |Nq (X) − npq | ≥ hn npq (1 − pq )
≤

1
.
h2n

Since hn → ∞ the statement of the proposition follows.
Consider first the sparse regime 1/2 < β < 1. Under the alternative hypothesis Nq (X) is
a sum of two independent binomial random variables, that is, Nq (X) ∼ Bin(n − s, pq ) +
Bin(s, vq ). Each of the variables corresponds respectively
to the counts of the null and
p
anomalous stream means exceeding the threshold 2q log(n), where
p
√ 
√
2 log(n)( q − r) .
vq = 1 − Φ
In words, vq is the probability an anomalous stream mean exceeds the threshold
For convenience of presentation define also
q
an ≡ s(vq − pq ) − hn npq (1 − pq ) .

p
2q log(n).

Using this definition we see that


q
PS (Vq (X) < hn ) = PS Nq (X) ≤ hn npq (1 − pq ) + npq


q
= PS Nq (X) − E (Nq (X)) ≤ hn npq (1 − pq ) + s(pq − vq )
= PS (Nq (X) − E (Nq (X)) ≤ −an )
= PS (− (Nq (X) − E (Nq (X))) ≥ an )
≤ PS (|Nq (X) − E (Nq (X))| ≥ an ) .

(31)

When an > 0 we can easily bound the above expression using Chebyshev’s inequality. To
make the presentation and derivations clear and simple we make use of asymptotic notation.

40
All the statements below are taken when n → ∞. Using Lemma 1
 √ √ 2
−( q− r) +o(1)

n
pq = n−q+o(1)
and
vq = 21
√ √ 2


1 − n−( r− q) +o(1)
Now, since hn = no(1) , we have that an is given by:
(
√ √ 2
1−q
n1−β−( q− r) +o(1) − nmax{ 2 ,1−β−q}+o(1)
√ √ 2
an =
1−q
n1−β+o(1) − nmax{ 2 ,1−β−q,1−β−( q− r) }+o(1)

we see that
if r < q
if r = q .
if r > q

(32)

if r < q
.
if r ≥ q

(33)

We can now easily retrieve the conditions under which an > 0 for large enough n, namely
the exponent of the positive terms must be larger than the exponent of the negative terms in
the right-hand-side of Equation (33). As an differs depending on whether r < q or r ≥ q we
obtain a different set of conditions of each of these cases. However, many of such conditions
are satisfied trivially, and for each case we obtain only one single nontrivial condition:
√
1−q
√
;
If r < q then an → ∞ provided 1 − β − ( q − r)2 >
2
1−q
.
If r ≥ q then an → ∞ provided 1 − β >
2

(34)
(35)

Under these conditions we can continue from (31) via Chebyshev’s inequality and obtain
PS (Vq (X) < hn ) ≤

Var (Nq (X))
.
a2n

To analyze the bound, first observe that
Var (Nq (X)) = (n − s)pq (1 − pq ) + svq (1 − vq ) ≤ npq + svq (1 − vq ).
Note that vq and 1 − vq are equal for r < q and r > q respectively, and thus vq (1 − vq ) is
equal for both cases asymptotically.
Moreover, the√case
r = q can be merged with r 6= q
√ 2
√ √ 2
by observing that 41 n1−β = 14 n1−β−( q− r) = n1−β−( q− r) +o(1) . We therefore obtain for all
r > 0:
√ √ 2
Var (Nq (X)) = nmax{1−q,1−β−( q− r) }+o(1) .
Consequently
(
√ √ 2
√ √ 2
nmax{1−q,1−β−( q− r) }−2(1−β−( q− r) )+o(1)
√ √ 2
PS (Vq (X) < hn ) ≤
nmax{1−q,1−β−( q− r) }−2(1−β)+o(1)

if r < q,
if r ≥ q.

All that needs to be done at this point is to identify under which conditions the above bound
converges to zero. Clearly, when r ≥ q this is the case. If r ≤ q we require that:
√
√
1 − q − 2(1 − β − ( q − r)2 ) < 0 and
(36)
√ 2
√
1 − β − ( q − r) > 0 .
(37)

41
Therefore, for the results in the proposition to hold we require that conditions (34), (35), (36)
and (37) are simultaneously satisfied. We can easily see that (34) and (36) are equivalent.
After rewriting condition (36) slightly, we are then left with the following three conditions:
1−q
.
2
√
1−q
√
if r < q, then 1 − β − ( q − r)2 >
,
2
√
√
if r < q, then 1 − β − ( q − r)2 > 0.

if r ≥ q, then 1 − β >

(38)
(39)
(40)

At this point it is a matter of algebra to check these conditions are satisfied for the statements
in the proposition. Note that we do not provide results for β = 1, as this would require taking
q > 1.

A.2

Proof of Proposition 2

Proof. Consider first the analysis under the null hypothesis. A simple application of the
result in Proposition 1 and a union of events bound yields
X
P∅ (T (X) ≥ hn ) =
P∅ (Vq (X) ≥ hn )
q∈Q

≤

kn + 1
→0,
h2n

where the last statement follows from the assumptions on hn and kn .
Now we turn our attention to the alternative hypothesis, and must show that
PS (T (X) ≥ hn ) → 1 ,
as n → ∞, under the conditions in the theorem for r and β. For the regimes where the
choices q = 0 or q = 1 are optimal it is straightforward to ensure that PS (Vq ≥ hn ) → 1, so
the theorem result is obviously true. For instance,


PS max Vq (X) ≥ hn ≥ PS (V1 ≥ hn ) → 1
q∈Q

p
√
provided µ = 2r log(n) and r > (1 − 1 − β)2 . Likewise a similar argument holds in the
dense case, where the choice q = 0 is the best one.
The only situation that is slightly more intricate is that when we must consider the choice
q = 4r, as in general that value is not in the grid Q. However, since kn →
p ∞ we will be able
to find a value in Q that is close enough to 4r. Consider the case µ = 2r log(n), r < 1/4
and r > β −1/2. Let q ∗ = minq∈Q |4r −q|. Clearly |q ∗ −4r| ≤ k1n so q ∗ = 4r +o(1). Therefore
pq∗ = n−4r+o(1) and vq∗ = n−r+o(1) and we can follow exactly the same steps as in the proof
of Proposition 1. Therefore


PS max Vq (X) ≥ hn ≥ PS (Vq∗ ≥ hn ) → 1 ,
q∈Q

which is what we wanted to show.

42

A.3

Statement and proof regarding Remark 4

The following lemma formalized the equivalence between the traditional higher-criticism
statistic as in (7) and the Vq (X) statistic introduced in (9).
Lemma 7.

)
(
i
√
−
p
(i)
np n
,
sup {Vq (X)} = max
i∈[i+ ]
p(i) (1 − p(i) )
q∈[0,∞)

(41)

where i+ is the largest value of i for which p(i) < 1/2.
The proof follows by noting that the supremum of Vq (X) is attained at a finite number of
points. This result is essentially stated in Donoho and Jin (2004) but without a complete
proof. Note that the right-hand-side is not entirely equal to the higher criticism statistic, but
it is very similar (particularly when taking α0 = 1/2, which is a common recommendation).
Taking α0 means that essentially almost all streams that have p-values larger than 1/2 will
be ignored.
Proof. We show our assertion (41) in two steps. First, we will show that the supremum of
Vq (X) is attained at a finite number of points. We will then show that the maximum of these
points equals the HC statistic, if α0 is chosen appropriately. For ease of notation, define:
hi (p) =

√

i
−p
np n
,
p(1 − p)

such that we can succinctly write our classical higher-criticism statistic as the maximum over
hi (p(i) ):
)
(
i

√
−
p
(i)
max
np n
= max hi (p(i) )
i∈[n]
i∈[n]
p(i) (1 − p(i) )
To start, define:
2

tX i
.
qi =
2 log(n)
for all Yi > 0. Denote X (i) the ordered stream means, with X (1) being the largest. Note that
qi is monotone in Yi > 0, and thus the ordering is preserved.
Note that Nq (X) is a stepwise, nonincreasing function in q, constant on intervals (q(i) , q(i+1) ].
Next, note that Vq (X) is decreasing in pq , and pq is decreasing in q. Therefore, as Nq (X) is
constant on (q(i) , q(i+1) ], Vq (X) is increasing on (q(i) , q(i+1) ].
Finally, as Nq (X) is nonincreasing, Vq(i) (X) ≥ Vq(i) +δ (X). Therefore, the supremum of Vq (X)
must be attained at the points qi .
Now, note that Nq(i) (X) = i and pq(i) = p(i) , and thus Vq(i) = hi (p(i) ). However, we only have
these q(i) defined for Yi > 0, so:

sup {Vq (X)} = max{Vq(i) } = max hi (p(i) ) ,
q∈[0,∞)

where i+ = arg mini∈[n] {Yi : Yi > 0}.

i∈[i+ ]

i∈[i+ ]

43

References
Aldosari, S. A. and J. M. F. Moura (2004). Detection in decentralized sensor networks. In
2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, Volume 2, pp. ii–277.
Arias-Castro, E., E. J. Candès, and A. Durand (2011). Detection of an anomalous cluster
in a network. Annals of Statistics 39 (1), 278–304.
Arias-Castro, E., E. J. Candès, and Y. Plan (2011). Global testing under sparse alternatives:
Anova, multiple comparisons and the higher criticism. Annals of Statistics 39 (5), 2533–
2556.
Arias-Castro, E., R. M. Castro, E. Tánczos, and M. Wang (2018). Distribution-free detection
of structured anomalies: Permutation and rank-based scans. Journal of the American
Statistical Association 113 (522), 789–801.
Arias-Castro, E. and M. Wang (2017). Distribution-free tests for sparse heterogeneous mixtures. TEST 26 (1), 71–94.
Bartroff, J. and J. Song (2016, mar). A rejection principle for sequential tests of multiple
hypotheses controlling familywise error rates. Scandinavian Journal of Statistics 43 (1),
3–19.
Benjamini, Y. and Y. Hochberg (1995). Controlling the false discovery rate: A practical and
powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B
(Methodological) 57 (1), 289–300.
Berk, R. H. and D. H. Jones (1979, jan). Goodness-of-fit test statistics that dominate
the kolmogorov statistics. Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete 47 (1), 47–59.
Delaigle, A. and P. Hall (2009). Higher criticism in the context of unknown distribution, nonindependence and classification. In Perspectives in mathematical sciences I: Probability and
statistics, pp. 109–138. World Scientific.
Delaigle, A., P. Hall, and J. Jin (2011). Robustness and accuracy of methods for high
dimensional data analysis based on student’s t-statistic. Journal of the Royal Statistical
Society. Series B: Statistical Methodology 73 (3), 283–301.
Donoho, D. and J. Jin (2004). Higher criticism for detecting sparse heterogeneous mixtures.
Annals of Statistics 32 (3), 962–994.
Donoho, D. and J. Jin (2015). Higher criticism for large-scale inference, especially for rare
and weak effects. Statistical Science 30 (1), 1–25.
Feller, W. (1968). An introduction to probability theory and its applications (3rd ed.), Volume 1. John Wiley & Sons.
Flenner, A. and G. Hewer (2011). A Helmholtz principle approach to parameter-free change
detection and coherent motion using exchangeable random variables. SIAM Journal on
Imaging Sciences 4 (1), 243–276.
Hettmansperger, T. P. (1984). Statistical inference based on ranks. Wiley Series in Probability
and Mathematical Statistics: Probability and Mathematical Statistics. New York: John
Wiley & Sons, Inc.
Huang, L., M. Kulldorff, and D. Gregorio (2007). A spatial scan statistic for survival data.
Biometrics 63 (1), 109–118.
Ingster, Y. I. (1997). Some problems of hypothesis testing leading to infinitely divisible

44
distributions. Mathematical Methods of Statistics 6 (1), 47–69.
Joag-Dev, K. and F. Proschan (1983). Negative association of random variables with applications. Annals of Statistics 11 (1), 286–295.
Kulldorff, M., R. Heffernan, J. Hartman, R. Assuncao, and F. Mostashari (2005). A spacetime permutation scan statistic for disease outbreak detection. PLOS Medicine 2 (3),
216.
Kulldorff, M., L. Huang, and K. Konty (2009). A scan statistic for continuous data based
on the normal probability model. International journal of health geographics 8 (1), 1–9.
Kurt, M. N., Y. Yilmaz, and X. Wang (2020, jan). Real-time nonparametric anomaly detection in high-dimensional settings. IEEE Transactions on Pattern Analysis and Machine
Intelligence, (online).
Lauer, S. A., K. H. Grantz, Q. Bi, F. K. Jones, Q. Zheng, H. R. Meredith, A. S. Azman,
N. G. Reich, and J. Lessler (2020, may). The incubation period of coronavirus disease
2019 (COVID-19) from publicly reported confirmed cases: Estimation and application.
Annals of Internal Medicine 172 (9), 577–582.
Lehmann, E. L. and J. P. Romano (2005). Testing statistical hypotheses (3rd ed.). Springer
Texts in Statistics. New York: Springer.
Lexa, M. A., C. J. Rozell, S. Sinanovic, and D. H. Johnson (2004, may). To cooperate or
not to cooperate: detection strategies in sensor networks. Acoustics, Speech, and Signal
Processing, 2004. Proceedings. (ICASSP ’04). IEEE International Conference on 3, 841–
844.
Mei, Y. (2008). Asymptotic optimality theory for decentralized sequential hypothesis testing
in sensor networks. IEEE Transactions on Information Theory 54 (5), 2072–2089.
Mikosch, T. and A. V. Nagaev (1998). Large deviations of heavy-tailed sums with applications in insurance. Extremes 1 (1), 81–110.
Nichols, T. E. and A. P. Holmes (2002). Nonparametric permutation tests for functional
neuroimaging: a primer with examples. Human Brain Mapping 15 (1), 1–25.
Patwari, N. and A. O. Hero (2003, apr). Hierarchical censoring for distributed detection
in wireless sensor networks. Acoustics, Speech, and Signal Processing, 2003. Proceedings.
(ICASSP ’03). 2003 IEEE International Conference on 4, 848–851.
Romano, J., A. Shaikh, and M. Wolf (2008, 12). Control of the false discovery rate under
dependence using the bootstrap and subsampling. TEST 17, 417–442.
Romano, J. P. and M. Wolf (2007, aug). Control of generalized error rates in multiple testing.
Annals of Statistics 35 (4), 1378–1408.
Sabatti, C., S. K. Service, A. L. Hartikainen, A. Pouta, S. Ripatti, J. Brodsky, C. G. Jones,
N. A. Zaitlen, T. Varilo, M. Kaakinen, U. Sovio, A. Ruokonen, J. Laitinen, E. Jakkula,
L. Coin, C. Hoggart, A. Collins, H. Turunen, S. Gabriel, P. Elliot, M. I. McCarthy, M. J.
Daly, M. R. Järvelin, N. B. Freimer, and L. Peltonen (2009, jan). Genome-wide association analysis of metabolic traits in a birth cohort from a founder population. Nature
Genetics 41 (1), 35–46.
Shorack, G. R. and J. A. Wellner (1986). Empirical processes with applications to statistics.
Wiley Series in Probability and Mathematical Statistics: Probability and Mathematical
Statistics. New York: John Wiley & Sons Inc.
Thomopoulos, S. C. A., R. Viswanathan, and D. K. Bougoulias (1989, sep). Optimal distributed decision fusion. Aerospace and Electronic Systems, IEEE Transactions on 25 (5),

45
761–765.
Walther, G. (2010). Optimal and fast detection of spatial clusters with scan statistics. Annals
of Statistics 38 (2), 1010–1033.
Wu, Z., Y. Sun, S. He, J. Cho, H. Zhao, and J. Jin (2014). Detection boundary and higher
criticism approach for rare and weak genetic effects. Annals of Applied Statistics 8 (2),
824–851.
Yu, L., L. Yuan, G. Qu, and A. Ephremides (2006, apr). Energy-driven detection scheme
with guaranteed accuracy. Information Processing in Sensor Networks, 2006. IPSN 2006.
The Fifth International Conference on, 284–291.
Zou, S., Y. Liang, H. V. Poor, and X. Shi (2017). Nonparametric detection of anomalous
data streams. IEEE Transactions on Signal Processing 65 (21), 5785–5797.

