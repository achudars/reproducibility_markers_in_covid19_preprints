Probabilistic K-mean with local alignment for
clustering and motif discovery in functional data
Marzia A. Cremona∗
Dept. of Statistics, The Pennsylvania State University, University Park, USA

arXiv:1808.04773v2 [stat.ME] 7 Jul 2020

Dept. of Operations and Decision Systems, Universit Laval, Canada
CHU de Qubec Universit Laval Research Center, Canada

Francesca Chiaromonte†
Dept. of Statistics, The Pennsylvania State University, University Park, USA
Inst. of Economics and EMbeDS, SantAnna School of Advanced Studies, Pisa, Italy

July 8, 2020

Abstract
We develop a new method to locally cluster curves and discover functional motifs,
i.e. typical “shapes” that may recur several times along and across the curves capturing
important local characteristics. In order to identify these shared curve portions, our
method leverages ideas from functional data analysis (joint clustering and alignment of
curves), bioinformatics (local alignment through the extension of high similarity seeds)
and fuzzy clustering (curves belonging to more than one cluster, if they contain more
than one typical “shape”). It can employ various dissimilarity measures and incorporate derivatives in the discovery process, thus exploiting complex facets of shapes.
We demonstrate the performance of our method with an extensive simulation study,
and show how it generalizes other clustering methods for functional data. Finally, we
provide real data applications to Berkeley growth data, Italian Covid-19 death curves
and “Omics” data related to mutagenesis.

Keywords: Functional data analysis; Clustering; Motif discovery; Local alignments.

1

Introduction

Given a set of curves, we consider the problem of discovering functional motifs inside them,
i.e. typical “shapes” that may recur within each curve, and across several curves in the
set. Some of these motifs may be present in most of the curves, but in different positions.
Conversely, other motifs may characterize subgroups of curves – and thus differentiate among
∗
†

marzia.cremona@fsa.ulaval.ca
fxc11@psu.edu

1

them based on local shape similarities. We provide a novel method for functional motif
discovery that aligns curves locally to identify their shared portions, employing different
definitions of (dis)similarity. Importantly, neither the motifs nor their number, lengths or
radii need to be known in advance; in addition, lengths and radii are specific to each motif.
During the last two decades, the analysis of curves has received increasing attention and
interest in the statistical literature. Indeed, several functional data analysis (FDA) methods
have been developed and applied in many fields (see, e.g., Ramsay and Silverman, 2005; Ferraty and Vieu, 2006; Horváth and Kokoszka, 2012). Several algorithms have been proposed
to cluster aligned functional data (reviewed in Jacques and Preda, 2014). Since functional
data are very often misaligned, algorithms have also been proposed to simultaneously cluster
and align curves (Liu and Yang, 2009; Sangalli et al., 2010; Park and Ahn, 2017). All these
methods consider the curves globally, over their entire domain of definition. However, in
many applications, separation in groups may occur only on a portion of the domain; this
type of clustering structure might be missed by methods that consider curves in their entirety. The multivariate counterpart of this “domain selection” problem is usually referred
to as feature selection, and has been widely studied (see, e.g., Friedman and Meulman, 2004;
Witten and Tibshirani, 2010). In the FDA framework, Fraiman et al. (2016) and Floriello
and Vitelli (2017) proposed methods to cluster curves while performing feature (i.e. domain)
selection. More recently, Vitelli (2019) integrated curve alignment in the sparse clustering
procedure.
The problem of functional motif discovery we tackle here is more general – and to the
best of our knowledge, it has never been studied in the statistical literature. In order to
identify motifs, we define clusters locally on portions of the misaligned curves, and allow
each cluster to contain multiple portions of the same curve (i.e. multiple instances of the
same functional motif). In addition, we allow each curve to belong to multiple clusters
(i.e. to comprise multiple functional motifs). This problem is the continuous version of
sequence motif discovery, which is ubiquitous in bioinformatics and “Omics” sciences (see,
e.g., Bailey et al., 2006) and consists of searching for highly similar patterns in a set of
DNA or protein sequences. While these are discrete sequences of symbols (4 nucleotides,
or 20 amino acids), we consider curves that can attain any real values – and can in fact be
multivariate (i.e. take values in Rd ). A similar problem for time series has been addressed
by the data mining community (Lin et al., 2002; Mueen et al., 2009; Yeh et al., 2016, 2018)
defining a motif as a pattern repeated multiple times within a single time series. Available
tools generally employ the Euclidean distance or the correlation between portions of the
time series. They usually require as input the length and the number of motifs to be found,
although Linardi et al. (2018) recently introduced an algorithm that finds all motifs in a
given range of lengths. Importantly, these tools require a user-specified minimum distance
within which two portions of the time series are considered the same motif (i.e. a “motif
radius”), and this distance is the same across motifs.
We embed the problem of functional motif discovery in a full-blown FDA framework,
which allows us to capture complex shape characteristics by incorporating derivatives in
the discovery process. The FDA framework also allows us to rigorously define variability
within each motif, and to naturally reduce noise in the curves through smoothing. Our
novel method, probabilistic K-mean with local alignment (probKMA), leverages ideas from
FDA, bioinformatics and fuzzy clustering in order to identify K shared curve portions, which
represent K candidate functional motifs in the set of curves under consideration. Indeed,
similar to the K-mean with (global) alignment of Sangalli et al. (2010), we simultaneously
2

perform clustering and alignment of curves. However, we employ local alignment in place of
their global alignment. Also, similar to BLAST-type algorithms in bioinformatics (Altschul
et al., 1990), we perform local alignments through the extension of high similarity “seeds”.
Finally, similar to fuzzy clustering in which points can belong to multiple clusters (Bezdek,
1981; Bezdek et al., 1984), our curves can be associated with zero, one, or more than one
cluster (if they contain more than one typical “shape”).
The article is organized as follows. In Section 2 we present the theoretical setting of
probKMA, formulate it as an optimization problem, derive necessary conditions for its solution and describe its algorithmic implementation. In Section 3 we discuss evaluation of the
clusters produced by the algorithm and identification of the motifs discovered. In Section 4
we provide an extensive simulation study to evaluate probKMA and compare it to other
approaches. Finally, we present real data applications in Section 5 and provide concluding
remarks in Section 6.

2

Probabilistic K-mean with local alignment

We consider a set of N (d-dimensional) curves xi : R −→ Rd , i = 1, . . . , N . Our goal is
to identify K (d-dimensional) cluster centers vk : (0, ck ) −→ Rd , k = 1, . . . , K of unknown
lengths c1 , . . . , ck ∈ [cmin , cmax ]. These are “patterns” to which the curves are, locally, highly
similar with respect to a distance d(·, ·). Since we are interested in local similarity (i.e. similarity between portions of curves) and we define each cluster center only in the interval (0, ck ),
we allow each curve to be aligned to each cluster center as to minimize their distance. Alignment to each cluster center vk is performed composing each curve xi with a warping function
hk,i : R −→ R from a class W . Here we consider shifts W = {h : t 7−→ t + s; s ∈ R}, but
our method can be generalized to other warping functions commonly employed in the FDA
literature (see, e.g., Ramsay and Silverman, 2005).
Because of the focus on local similarity, a curve can belong to more than one cluster; that
is, different portions of a curve can be similar to portions of other curves. Hence, mimicking
fuzzy clustering (see, e.g., Bezdek, 1981; Bezdek et al., 1984), we assign to each curve xi a
probability pk,i to be a member of each cluster k. In particular, we define a membership
function pk : {x1 , . . . , xN } −→ [0, 1] for each k = 1, . . . , K, with pk (xi ) = pk,i , requiring
PN
P
that K
i=1 pk,i > 0 for all k = 1, . . . , K. Each
k=1 pk,i = 1 for all i = 1, . . . , N , and that
membership probability pk,i corresponds to a particular shift sk,i of the curve xi ; namely,
the one that minimizes the distance between xi and vk given all constraints. We summarize
these shifts in a matrix S = [sk,i ] and the membership probabilities in a matrix P = [pk,i ].

2.1

Optimization problem and necessary conditions

Consider the cluster center lengths c1 , . . . , ck as fixed (identification of c1 , . . . , ck ∈ [cmin , cmax ]
is discussed in the next Subsection). ProbKMA can be formulated as the following optimization problem: find K cluster centers v1 , . . . , vK , membership probabilities P and shifts
S that minimize the generalized least-squares functional
Jm (P, S, v1 , . . . , vK ) =

N X
K
X
i=1 k=1

3

(pk,i )m d2 x̃i,sk,i , vk



(1)

P
PN
under the constraints pk,i ∈ [0, 1], ∀i, k; K
k=1 pk,i = 1, ∀i; and
i=1 pk,i > 0, ∀k. Here m > 1
is a fixed weighting parameter controlling the degree of “fuzziness”, and x̃i,sk,i = x ◦ hk,i are
the shifted curves. Necessary conditions for (P̂, Ŝ, v̂1 , . . . , v̂K ) to be a (local) minimizer of
(1) are that each of P̂, Ŝ and v̂1 , . . . , v̂K minimizes (1) fixing all the other variables. We
prove two key results (see Supplementary Material). The first provides an explicit solution
for the optimal membership probabilities P̂ given shifts and centers. Importantly, this result
holds for any distance d(·, ·) and does not rely on any regularity assumption on the curves
or the cluster centers.
Proposition
1. Fix a shift matrix Ŝ and K cluster centers v̂1 , . . . , v̂K . Consider the set

R = i ∈ {1, . . . , N } | d(x̃i,ŝk,i , v̂k ) > 0 for all k and suppose that |R| ≥ K.
Then P̂ = [p̂k,i ] is a global minimizer of the functional
Jm (·, Ŝ, v̂1 , . . . , v̂K ) : [0, 1]K,N −→ R,
P
PN
under the constraints K
k=1 pk,i = 1, ∀i and
i=1 p̂k,i > 0, ∀k if and only if
−1

1
 ! m−1
K
2
X
d x̃i,ŝk,i , v̂k


k = 1, . . . , K
p̂k,i = 
2 x̃
d
,
v̂
i,ŝ
l
l,i
l=1

(2)

(3)

for all i ∈ R and
p̂k,i
with

PK

k=1

(
0,
=
∈ [0, 1],

k : d(x̃i,ŝk,i , v̂k ) > 0
k : d(x̃i,ŝk,i , v̂k ) = 0

(4)

p̂k,i = 1, for all i ∈
/ R.

If the i-th curve has positive distance from all cluster centers, (3) states that its probability of belonging to cluster k is inversely proportional to its distance from the k-th cluster
center. Equation (4) tackles the extreme case of a curve with distance 0 from one or more
cluster centers; in this case the functional minimum is attained by setting the membership
probabilities to 0 for all clusters from which the curve has positive distance. Note that if a
curve has distance 0 to more than one cluster (4) defines an uncountable set of minimizers.
However, the non-uniqueness is trivial.
The second result concerns updates of the cluster centers and depends on the distance
employed. This provides an explicit solution for optimal centers given shifts and probabilities
for any distance dα (·, ·) defined as
Z
Z
d
X
2
2 i
wj h 1 − α c (j)
α c 0(j)
(j)
2
dα (x, v) =
x (t) − v (t) dt +
x (t) − v 0(j) (t) dt , (5)
d
c
c 0
0
j=1
where wj > 0 is the weight of the j-th component of a d-dimensional curve, indicated by
(j) 0
, indicates the weak derivative, and α ∈ [0, 1] is a parameter that defines the relative
weight of the curve’s levels and derivatives. Selecting α = 0 we obtain an L2 -like distance
d0 (·, ·) that focuses exclusively on the levels. In this case we require xi ∈ L2 (R, Rd ) and
vk ∈ L2 ((0, ck ), Rd ). The choice of α = 1 leads to an L2 -like pseudo-distance d1 (·, ·) that
uses only weak derivative information, hence focusing on curve variations (their slopes or
trends). Finally, α ∈ (0, 1) properly defines a Sobolev-like distance dα (·, ·) that allows one
to highlight more complex features of curve shapes, taking into account both levels and
variations. When α > 0, we require xi ∈ H 1 (R, Rd ) and vk ∈ H 1 ((0, ck ), Rd ).
4

Proposition 2. Fix a membership matrix P̂ and a shift matrix Ŝ. Consider the distance
dα (·, ·) with α ∈ [0, 1]. For α = 0, assume that xi ∈ L2 (R, Rd ) and vk ∈ L2 ((0, ck ), Rd )
for k = 1, . . . , K. For α > 0, assume that xi ∈ H 1 (R, Rd ) and vk ∈ H 1 ((0, ck ), Rd ) for
k = 1, . . . , K. Then (v̂1 , . . . , v̂K ) is the (unique) global minimizer of the functional
Jm (P̂, Ŝ, ·) : V1 × · · · × VK −→ R

(6)

if and only if
N
X
(p̂k,i )m x̃i,ŝk,i

v̂k =

i=1

a.e. in (0, ck ), ∀k.

N
X
(p̂k,i )m

(7)

i=1

For α = 1, v̂k is defined by (7) up to an additive constant.
Equation (7) defines the k-th cluster center has a weighted average of the shifted curves in
the interval (0, ck ). Weights are determined by membership probabilities: the contribution of
a curve to the computation of the k-th cluster center is directlt proportional to its probability
of belonging to cluster k.

2.2

Algorithm

Propositions 1 and 2 suggest to numerically minimize (1) through an iterative procedure that
alternates: i. identification of cluster centers with equation (7), ii. curve alignment (warping
function selection), and iii. computation of membership probabilities using equations (3)-(4).
We propose the following algorithm for probKMA.
Initialization Fix the number of clusters K and the cluster
lengths c1 ,P
. . . , cK . ConPK center
(0)
(0)
(0)
sider an initial membership matrix P such that k=1 pk,i = 1, ∀i and N
i=1 pk,i > 0,
∀k (non-degenerate clusters), and an initial shift matrix S(0) ;
Iteration For it = 1, 2, . . . , iterate the following three steps until convergence:
(it)
i. Identification of cluster centers. For each k, compute the k-th cluster center vk
(it−1)
(it−1)
with equation (7), using the shift sk,i and membership probabilities pk,i ;
ii. Curve alignment. For each i and k, align the curve xi to the new cluster center
(it)
(it)
(it)
vk , selecting the shift sk,i that minimizes their distance d(x̃i,s , vk );
iii. Computation of membership probabilities. Compute the membership matrix P(it)
(it)
(it)
with equations (3)-(4), using vk and the shifts sk,i .
Stopping criterion At each iteration, the convergence of the probabilistic clustering is
evaluated by means of the Bhattacharyya distance between the membership matrices
P(it) and P(it−1) . In particular, for each cluster k we compute
!
N q
X
(it) (it−1)
BCk = − log
pk,i pk,i
.
(8)
i=1

The Bhattacharyya distance is computed as the maximum, mean, or order q quantile
of (8) for all clusters k. Steps i-iii are repeated until the global Bhattacharyya distance
reaches a given tolerance.
5

Remark 1. We observe that steps i and iii are analogous to the steps of a fuzzy K-mean
algorithm (Bezdek et al., 1984), or to the steps of an EM algorithm for mixture models
(Dempster et al., 1977). Also notably, steps i and ii correspond to the functional K-mean
with (global) alignment algorithm (Sangalli et al., 2010).
Every iteration can be written in functional form as




(it)
(it)
(it−1)
(it−1)
(it)
(it)
(it−1)
(it−1)
P , S , v1 , . . . , vK ∈ Tm P
,S
, v1
, . . . , vK
where Tm : Y −→ Y is the point-to-set P
map defined by i-iii, P
and Y the subset of
N
[0, 1]K,N × RK,N × V1 × . . . × VK that satisfies K
p
=
1,
∀i
and
k=1 k,i
i=1 pk,i > 0, ∀k. For
each initialization, the algorithm generates a sequence of iterations
n

o
(0)
(0)
Tm(it) P(0) , S(0) , v1 , . . . , vK
.
(9)
it=1,2,...

Below we show that the functional (1) is continuous and descends along (9). This is an
important result, which mimics the one in Hathaway et al. (1987) for fuzzy K-mean (its
proof is provided in the Supplementary Material).
Lemma 3. The functional Jm : Y −→ R is continuous.
Theorem 4. Consider y(it−1) ∈ Y . Then for every y(it) ∈ Tm (y(it−1) ) we have


Jm y(it) ≤ Jm y(it−1) ,

(10)

i.e. Jm is a descent functional for Tm . Moreover, Jm descends strictly along the iterations
if y(it−1) ∈
/ Ω, where Ω ⊆ Y is the solution set of ŷ = (P̂, Ŝ, v̂1 , . . . , v̂K ) ∈ Y such that




Jm P̂, Ŝ, v̂1 , . . . , v̂K ≤ Jm P, Ŝ, v̂1 , . . . , v̂K
∀ P ∈ [0, 1]K,N
(11)
PK
k=1 pk,i = 1
PN
i=1 pk,i > 0;




∀ S ∈ RK,N ;
Jm P̂, Ŝ, v̂1 , . . . , v̂K ≤ Jm P̂, S, v̂1 , . . . , v̂K

(12)





Jm P̂, Ŝ, v̂1 , . . . , v̂K < Jm P̂, Ŝ, v1 , . . . , vK
∀ vk ∈ Vk

(13)

vk 6= v̂k .
Remark 2. Although the previous result does not guarantee that every sequence of iterations
(9) converges to a minimizer of the functional Jm , it is a necessary condition for convergence,
and a desirable property for the algorithm.
Cluster center lengths identification. In the previous theoretical results and algorithm, the lengths c1 , . . . , cK ∈ [cmin , cmax ] of the cluster centers remain fixed along iterations.
However, we seek to identify local similarities even when the lengths of the “matching” curve
portions are not known a priori. This problem has been already tackled by local sequence
alignment methods in bioinformatics, whose goal is to find similar stretches of unknown
6

lengths within a collection of nucleotides or amino acids sequences. In this context, one of
the most widely used algorithms is BLAST (Altschul et al., 1990). BLAST starts by finding
short stretches shared by the sequences, and uses them as seeds. It then extends the seeds on
both sides in order to construct larger local alignments, stopping when the similarity score
drops below a given threshold. Borrowing this logic, we add a center elongation step to our
algorithm. This step is performed only when the algorithm is reaching convergence, to guarantee that we do not extend low-quality cluster centers. We attempt elongation on both the
left and the right of each center, generating the elongated center with equation (7) applied to
the correspondingly elongated set of curves. For elongation to be
we requirethat
PNacceptable,
m 2
the corresponding objective function Jm,k (P, S, v1 , . . . , vK ) = i=1 (pk,i ) d x̃i,sk,i , vk decreases or that it increases less than a given threshold ∆Jm,k .

3

Cluster evaluation and functional motif discovery

To evaluate a clustering produced by probKMA, we develop a generalized silhouette index,
similar to the one used in classic clustering (Rousseeuw, 1987). Our index, which is defined
for portions of curves, measures how well each portion fits in the cluster it was assigned to.
First, from every curve xi , we extract the portions that correspond to the clusters (after
dicotomizing the membership probabilities P into 0 and 1, see Supplementary Material).
Next, we compute the average distance dj (k) of each extracted portion j = 1, . . . , J from
cluster k as the mean of the distances between j itself and all the portions attributed to
cluster k. We define the intra-cluster distance aj as the average distance of portion j from
the cluster kj it belongs to, i.e. aj = dj (kj ), and the inter-cluster distance bj as the minimum
of the average distances of portion j from all the other clusters, i.e. bj = mink6=kj dj (k). The
generalized silhouette index for portion j is a number in [−1, 1] defined as
sj =

b j − aj
.
max(bj , aj )

(14)

Large values of sj indicate that j is appropriately assigned to its cluster, while low values
indicate bad assignments. In particular, negative values signify that portion j is closer to a
cluster different from the one it was assigned to. For each cluster k, we then compute the
cluster average silhouette index Sk as the average silhouette index across all the portions
assigned to k. This measures the compactness of the cluster and hence its quality. Finally,
the overall average silhouette index S, i.e. the average of all Sk ’s, measures the overall quality
of the clustering. Similar to classic clustering, silhouettes for portions, clusters and overall
clustering are visualized in a silhouette plot that facilitates their interpretation.
Likewise other K-mean algorithms, probKMA finds a local minimum of the functional
Jm and its output depends heavily on initialization. If the goal is to cluster the curves in K
groups based on local similarity, we repeat the algorithm using different initializations (and
possibly different initial lengths) and we select the solution with the lowest value of Jm . If
the goal is functional motif discovery, we run the algorithm multiple times with different
initializations, cluster number and motif initial lengths, and form the set of candidate motifs
taking the union of the resulting solutions. We “clean up” this set of candidate motifs using
generalized silhouette indices and motif number of occurrences. We then merge candidate
motifs that are very similar to each other, as they may in fact correspond to the same “true
motif” identified by multiple runs of probKMA. Finally, given the resulting set of discovered
7

motifs, we utilize a motif search algorithm to locate all their instances in the input curves.
For each motif, we map all portions of the curves with distance lower than a given radius R
from the motif. Full details are provided in the Supplementary Material.

4

Simulations

Generating curves comprising functional motifs in a non-trivial task, since we require motifs
to be smoothly embedded in curves while allowing them to occur with noise. In order to
do this, we take advantage of the flexibility provided by B-splines. We consider a B-spline
basis {Φl }Ll=1 of order n, with equally spaced knots t1 , . . . , tL−n+2 , and define each curve as
x(t) =

L
X

cl Φl (t),

(15)

l=1

where cl ∈ R, l = 1, . . . , L are coefficients to be chosen. The order n controls smoothness
and complexity of x (x is a curve of class C n−2 and a piece-wise polynomial of degree n − 1).
Higher orders provide more degrees of freedom, allowing one to generate curves with more
complex shapes, and smoother at the knots. Each basis function Φl has compact support;
specifically, it is 0 outside an interval of length nT , where T is the distance between two
subsequent knots. As a consequence, we can define a functional motif of length T fixing the
values of n coefficients cm,i , . . . , cm,i+n−1 and repeating them multiple times within the same
curve or across different curves. Longer motifs of length 2T, 3T, . . . – that may result in
more complex shapes – can be created in a similar way, fixing the values of n + 1, n + 2, . . .
subsequent coefficients. Since a single curve can embed more than one functional motif, as
well as more than one occurrence of the same motif, we require motifs to be separated by at
least one sub-interval (ti , ti+1 ) as not to be artificially merged (i.e. we require at least n background coefficients between them). Motif occurrences that are “the same”, both in shape and
level, are generated adding Gaussian noise to the corresponding coefficients: c̃m,j = cm,j + j ,
iid
j ∼ N (0, σ 2 ). Motif occurrences that are “the same” in shape but have different levels are
obtained adding a constant dm to all the coefficients that define a single occurrence, choosiid
ing different constants for different occurrences: c̃m,j = cm,j + dm + j , j ∼ N (0, σ 2 ). Backiid
ground coefficients cbg ∈ [a, b] are randomly generated as (cbg − a) /b ∼ Beta(0.45, 0.45).
The Beta distribution allows us to create reasonably different backgrounds for both the
curve x(t) and its derivative x0 (t). With this flexible model we can generate functional
data in several complex scenarios, varying curve and motif lengths, as well as variability,
frequencies and positions of the occurrences of each motif.

4.1

Functional motif discovery: varying curve length and noise in
motifs

The aim of this simulation study is to demonstrate the performance of probKMA in discovering functional motifs embedded in a set of curves, and to examine the effects of increasing
curve length and the noise level comprised in motif occurrences. We consider two different
scenarios, with sets of curves embedding (1) motifs that share both shapes and levels; or (2)
motifs that share shapes but have different levels.
8

Motif 1, σ=1
10

Motif
Occurrences

0
−10

0

x(t)

10

Motif
Occurrences

−10

x(t)

Motif 2, σ=1

0

10

20

30

40

50

60

0

10

20

30

t

40

50

60

t

(a)

(b)

0
−10

x(t)

10

Curves with motifs, σ=1

0

50

100

150

200

t

(c)

Figure 1: Simulation scenario (1) with l = 200 and σ = 1. (a), (b) Two functional motifs
(black solid lines) and 12 aligned occurrences of each (red and blue dashed lines); (c) 20
curves embedding occurrences of the two motifs (red and blue portions, respectively).
In scenario (1), we consider a set of 20 curves embedding two functional motifs – each with
12 occurrences (see Fig. 1 and Supplementary Material). In particular, 12 curves contain
only one occurrence of a motif (6 curves for each of the two motifs), 4 curves contain two
occurrences of a motif (2 curves for each of the two motifs), 2 curves contain one occurrence
of each of the two motifs, and 2 curves contain no motif occurrences at all. We generate data
using a B-spline basis of order 3, knots at distance 10 and motifs of length 60. Coefficients
defining the two motifs are randomly generated from a Beta(0.45, 0.45) distribution rescaled
to [−15, 15]. We consider four different curve lengths l = 200, 300, 400, 500 and four levels
of noise σ = 0.1, 0.5, 1, 2 – a total of 16 simulated datasets. In order to maximize the
consistency among these datasets and thus highlight the effects of different l and σ values,
we place motif occurrences within the leftmost sub-interval of length 200 of each curve –
that is common to all datasets – utilizing the same motif positions and background in all 16
cases. Curves are sampled on a grid of points at distance 1, so that each motif corresponds
to 61 points. For each combination of l and σ, we run our probKMA-based functional
motif discovery with Sobolev-like distance d0.5 . We evaluate the number of motifs found,
the distance between true and estimated motifs, the estimated lengths of motifs, and the
number of true and false positives. ProbKMA is run for K = 2, 3, minimum motif lengths
c = 40, 50, 60 and 20 random initializations for each (K, c) pair (the maximum motif length
is set to 70; see Supplementary Material for other parameters). The same initializations
are employed for all l and σ combinations. Simulation results for l = 200 can be found in
Fig. 2 and show very good performance for our method. As expected, performance slightly
declines when more noise is introduced in the motif instances: some occurrences can be
missed, and/or false positives can be included. However, results remain satisfactory even

9

10

Motif 1

−10

x(t)
0

True motif
Estimated, σ=0.1
Estimated, σ=0.5
Estimated, σ=1
Estimated, σ=2

0

20

40

60

t

8

●

8

50

0.20

False positives
10 12

True positives
10 12

Length
70

Distance

6
4
2
σ

2

1

0.5

0
2

1

0.1

σ

0.5

0.1

2

1

0.1

2

1

0.5

0.1

σ

0.5

●
●
●

0

0 10

●

0.00

2

4

0.10

30

6

●
●

σ

(a)

10

Motif 2

−10

x(t)
0

True motif
Estimated, σ=0.1
Estimated, σ=0.5
Estimated, σ=1
Estimated, σ=2

0

20

40

60

t

●
●

8

8

50

●

2

1

0.5

0.1

4
2
σ

2

1

0.5

0.1

σ

●

0

4
2
2

1

0.5

σ

2

1

0.5

0.1

●

0.1

●

●

●

0

0 10

0.10

30

6

●

6

0.20

●

●

0.00

False positives
10 12

True positives
10 12

Length
●

70

Distance

σ

(b)

Figure 2: Functional motif discovery results for simulation scenario (1) with l = 200 and
various levels of σ. (a) Motif 1; (b) Motif 2. The boxplots in the lower half of the panels
are obtained from 10 replications at each σ value. They show the distance between true
and estimated motifs (stepwise line: distance between the true motif and the average of all
motif occurrences), the estimated length of motifs, the number of true positives and that of
false positives. For all the considered noise levels, exactly 2 motifs are found.

10

when σ = 2. Results for other curve lengths are shown in the Supplementary Material.
They suggest the same behavior as the noise level increases and, interestingly, they appear
rather robust across lengths. The only effect of increasing the ratio between “background”
curve portions and curve portions occupied by motifs is a slight increase in false positives –
which occurs exclusively when also the noise level is high.
In scenario (2) we consider exactly the same curves and motifs as in scenario (1), but
allow motif occurrences to have different levels (see Supplementary Material). In particular,
a random value dm ∼ U (−10, 10) is added to the coefficients defining an individual motif
occurrence. For each combination of l and σ, we run our probKMA-based discovery with
the L2 -like pseudo-distance d1 (this lets us focus on curve variation). Parameters are the
same as in scenario (1), and Figures detailing the results are provided in the Supplementary
Material. We find again that our method has good performance – and that this performance
is affected (as expected) by the noise level, but not much by the length of the curves. In
some cases, especially when the curves are very long, we actually “discover” motifs that were
not embedded in the simulated data. Note that, strictly speaking, these motifs are not all
together “false”. As one elongates the background portions of the curves, it is possible to
generate by chance a few patterns that recur often enough to be identified by our algorithm.
In our experiments, these additional motifs are noisier and have fewer occurrences than the
two motifs originally embedded in the data.
To validate the results described above, we repeat simulations in both scenarios 10 times,
considering 10 different randomly generated pairs of motifs and re-generating the curves’
background (see Supplementary Material). In all cases, our method shows good performance
and similar behaviors as curve length and noise level change. This is evidence that its
effectiveness does not depend on on the specific shapes of the motifs embedded in the curves.
Finally, we consider the 10 simulations for the first scenario and examine how results change
with the number of random initializations used to run probKMA for each (K, c) pair. To do
this, we subsample the probKMA runs from the analysis already conducted, which employed
20 random initializations – using only 5, 10 or 15 initializations for each (K, c) pair. We
re-run the post-processing steps and compare results with those previously obtained with all
20 initializations. Reassuringly, our method is pretty robust to the number of initializations
employed, and retains its good performance even when probKMA is run only 5 times for
each choice of (K, c) (see Supplementary Material).

4.2

Comparison with time series motif discovery

Next, we compare our probKMA-based functional motif discovery to time series motif discovery. As noted in the Introduction, the problem of motif discovery in a time series is similar
to that of functional motif discovery. However, some important differences exist. First, the
definition of motif is different. While our functional motifs recur across a set of curves,
and possibly within individual curves in the set, time series motifs are defined as recurring
within a single time series – usually starting from pairs of highly similar subseries (see Supplementary Material). Second, time series motif similarity is based on Euclidean distance or
correlation, while functional motif similarity is more general, based on a distance between
functions that can incorporate derivatives. Third, and perhaps most importantly, available
time series motif discovery tools are not statistical in nature and they do not estimate the
level of noise associated with each motif. The user needs to provide one, and usually only
one, motif radius as input to the discovery procedure. On the contrary, probKMA-based
11

Table 1: Comparison of probKMA-based functional motif discovery and Matrix Profile on
simulation scenario (1) (TP: true positives; FP: false positives). For probKMA, we report
median results (and median absolute deviations) across 10 repeated simulations.
probKMA

l = 200
σ = 0.1

l = 500
σ=2

Matrix Profile

Radius
TP
Motif 1
FP

—
12 (0)
0 (0)

1
2
0

10
6
0

20
8
0

30
12
0

40
12
0

50
12
0

70
12
0

90
12
0

110
12
0

130
12
0

150
12
0

Motif 2

TP
FP

12 (0)
0 (0)

2
0

7
0

10
0

12
0

12
0

12
0

12
0

12
0

12
0

12
0

12
0

Motif 1

TP
FP

11 (0.7)
2 (1.5)

0
2

0
5

0
8

0
8

0
9

0
10

0
13

1
17

2
19

2
24

2
27

Motif 2

TP
FP

12 (0)
1 (1.5)

2
0

2
1

2
2

12
8

12
16

12
23

12
34

12
51

10
71

12
82

12
88

discovery “learns” an appropriate radius for each motif from the data.
Among existing tools for time series motif discovery, we consider the recent Matrix Profile
(Yeh et al., 2016, 2018, see Supplementary Material). This tool discovers the top motif pairs
in a time series and, for each of these pairs, provides all the “neighboring” subseries, i.e. all
the subseries with distance less than R from the motif pair. The same radius R is used
for all the top motif pairs. We consider again the two simulation scenarios introduced in
Subsection 4.1, focusing on two specifications: the simple case of short curves and low noise
level (l = 200 and σ = 0.1), and the complex case of long curves and high noise level
(l = 500 and σ = 2). For probKMA-based discovery, we use the same parameters as in
Subsection 4.1. We run Matrix Profile on one “time series” obtained by concatenating the
20 curves one after the other, and using different choices of radius (from R = 1 to R = 150).
Since the tool requires also the motif length as input, we use the true motif length c = 60
(a newer implementation of Matrix Profile, introduced in Linardi et al., 2018, can find all
motif pairs in a given range of lengths). Table 1 reports results for simulation scenario
(1). Both probKMA-based motif discovery and Matrix Profile discover the two motifs in
the simple case (l = 200, σ = 0.1). However, when curves are longer and motifs noisier
(complex case, l = 500, σ = 2), Matrix Profile fails to find Motif 1, and includes many
false positives in Motif 2. When the radius is large (R > 100), it does correctly identify
a small number of occurrences of Motif 1, but it also reports a very large number of false
positives (for both motifs). On the contrary, even in this complex case, probKMA-based
motif discovery remains robust to noise level and curve length, and is able to identify both
motifs with a very small number of false positives. Similar observations can be made for
simulation scenario (2) (see Supplementary Material).

4.3

Comparison with non-sparse and sparse functional clustering

Here we perform simulation experiments to compare probKMA, meant as a clustering
method and separate from its motif discovery purpose, to other functional clustering methods. In particular, we include in the comparison the standard functional K-means (Tarpey
and Kinateder, 2003) and K-mean with (global) alignment (KMA Sangalli et al., 2010) –
which are both non-sparse methods – and a recent sparse clustering technique (Floriello and
12

Vitelli, 2017).

0

x(t)

−10

0
−10

x(t)

10

Misligned motifs, σ=2

10

Aligned motifs, σ=2

0

10

20

30

40

50

60

0

20

40

t

60

80

t

(a)

(b)

0
−10

x(t)

10

Curves with aligned motifs, σ=2

0

50

100

150

200

150

200

t

(c)

0
−10

x(t)

10

Curves with misaligned motifs, σ=2

0

50

100
t

(d)

Figure 3: Simulated data for the comparison of functional clustering methods, σ = 2.
(a) Aligned motifs; (b) Misaligned motifs; (c) Curves with aligned motifs within them; (d)
Curves with misaligned motifs within them. When the curves are broader than the motifs
defining the two clusters, the motifs are shown as red and blue lines and the reminder of the
curves as black lines.
We consider 2 clusters and generate 9 curves for each cluster, in the four different scenarios depicted in Fig. 3: (a) curves in the two clusters are aligned, and they differ on the
entire domain; (b) curves in the two clusters are misaligned, and they differ on the entire
domain; (c) curves in the two clusters differ on a portion of the domain, and this portion is
aligned; (d) curves in the two clusters differ on a portion of the domain, and this portion is
misaligned. These four scenarios can be seen as special cases of the more general functional
motif discovery problem, in which each curve contains exactly one motif and (a) curves are
themselves the entire aligned motifs; (b) curves are themselves the entire misaligned motifs;
(c) curves contain aligned motifs; (d) curves contain misaligned motifs. In each scenario, we
run all four clustering methods with Euclidean distance and K = 2. For the sparse clustering
method, we also take the sparsity parameter (i.e. the minimum length of the unselected part
13

Table 2: Comparison of probKMA with non-sparse and sparse functional clustering methods
in four simulation scenarios. We report means (and standard deviations) of classification
error rates across 10 repetitions.
Scenario

K-means

KMA

sparse

probKMA

σ = 0.1

(a)
(b)
(c)
(d)

0 (0)
0.26 (0.13)
0.29 (0.22)
0.49 (0.05)

0 (0)
0.12 (0.19)
0.44 (0.08)
0.49 (0.06)

0 (0)
0.08 (0.18)
0.05 (0.17)
0.52 (0)

0
0
0.04
0.01

(0)
(0)
(0.07)
(0.04)

σ=2

(a)
(b)
(c)
(d)

0 (0)
0.26 (0.18)
0.28 (0.23)
0.44 (0.07)

0 (0)
0.16 (0.21)
0.38 (0.17)
0.49 (0.05)

0 (0)
0.42 (0)
0.11 (0.22)
0.53 (0.01)

0
0
0.04
0.06

(0)
(0)
(0.10)
(0.08)

of the domain) as known, setting it to the curve length minus the motif length. We set the
motif length parameter in probKMA in the same way. In KMA and probKMA, we consider
only shift alignments. We then evaluate clustering results by means of a classification error
rate (1 minus the Rand index; Rand, 1971) that is equal to 0 if every curve is correctly
classified and (since K = 2) is equal to 0.5 if the classification is as good as random. Since
probKMA produces a probabilistic clustering, we compute the classification error rate after
assigning each curve to the cluster with highest membership probability. Results (Table 2)
show that all four methods correctly classify all curves in scenario (a). K-means only works
in this scenario, while KMA performs well in scenarios (a) and (b), and sparse clustering
performs well in scenarios (a) and (c). Interestingly, when the noise level is small, sparse
clustering also achieves a good performance in scenario (b). ProbKMA performs very well
in all four scenarios.

5
5.1

Real data applications
Berkeley growth curves

The Berkeley Growth Study dataset (provided within the R package fda) consists of the
heights of 39 boys and 54 girls recorded from age 1 to 18. We estimate the curves using
monotone B-spline smoothing (with order 6, knots
√ at observed ages, roughness penalty on
third derivative and smoothing parameter λ = 1/ 10, Ramsay et al., 2009).
First, we perform a global probabilistic K-means running probKMA with K = 2 and L2 like pseudo-distance d1 between the entire curves (no alignment permitted). Assigning each
curve to the cluster with highest membership probability, we obtain clusters that differ on the
main pubertal growth spurt timing and roughly corresponds to boys and girls groups (boys
grow later), with 11 misclassified children (2 boys and 9 girls, classification error rate 0.21).
Notably, K-means (with K = 2 and Euclidean distance between curve derivatives) produces
exactly the same clusters. However, our probabilistic approach permit us to visualize curves
whose membership is uncertain and to check the probabilistic memberships of misclassified
children (see Supplementary Material).
Second, we cluster curves locally using probKMA with K = 2 and L2 -like pseudo-distance
d1 between portions of curves of length 8.5 years, hence allowing for a maximum shift of 8.5
years. Dicotomizing the membership probabilities by setting them to 1 when the distance
14

ProbKMA d1 c=8.5 years

6

8

10

Cluster 1
Cluster 2

0

2

4

Growth (cm/year)

10 15 20 25 30

Cluster 1
Cluster 2

5

Growth (cm/year)

12

ProbKMA d1 c=8.5 years

5

10

15

5

Age (years)

10

15

Age (years)

(a)

(b)

Figure 4: Results of local clustering with probKMA (K = 2). (a) Growth velocities with
clustered portions of curves color-coded based on the probabilistic membership to Cluster 1
(going from red when it is 0 to blue when it is 1); (b) Cluster centers (black) with aligned
clustered portions of curves.
between the portion of curve and the cluster center is lower than the median distance, we
obtain two clusters of 50 and 43 portions, respectively (Fig. 4 and Supplementary Material).
Interestingly, 18 curves belong to both clusters (i.e. they comprise both motifs in different
parts of their domains), while 18 curves do not belong to any cluster. Cluster 2 captures
a particular shape for the pubertal growth spurt, while Cluster 1 captures the decrease in
growth velocity that is typical in children between 2 and 3 years of age.

5.2

Italian Covid-19 excess mortality curves

Italy was the first European country to be hit by the Covid-19 pandemic, with the first
confirmed cases around mid-February. Italian regions were hit at different times and with
different strength, and local authorities implemented different responses – especially in the
initial stages of the epidemic. Comparing its evolution across regions can therefore provide
important insights on the role of underlying factors and different containment measures. We
estimate excess mortality due to Covid-19 in Italy using the mortality data (due to all causes)
from the Italian Institute of Statistics (ISTAT). The dataset contains the daily number of
deaths for 7.270 municipalities (covering about 93.5% of the Italian population) from January
1st to April 30th, for the years 2015-2020. We aggregate data by region and we compute
the excess mortality rate curves as the daily difference between 2020 deaths and average
deaths in the period 2015-2019, divided by the population of the considered municipalities
(see Supplementary Material). In order to focus on the Covid-19 period, we only consider
data starting from February 16th. To reduce noise, we smooth the 20 curves using B-spline
smoothing (cubic splines, knots at each day, roughness penalty on second derivative and
smoothing parameter chosen by average Generalized Cross-Validation). Smoothed curves
are shown in Fig. 5(a).
Next, we cluster the 20 Italian regions according to their excess mortality rate curves,
to assess whether there are regions sharing similar epidemic patterns. We are interested
15

2

4

6

Abruzzo
Basilicata
Calabria
Campania
Emilia−Romagna
Friuli Venezia Giulia
Lazio
Liguria
Lombardia
Marche
Molise
Piemonte
Puglia
Sardegna
Sicilia
Toscana
Trento/Bolzano
Umbria
Valle d'Aosta
Veneto

16 Feb
19 Feb
22 Feb
25 Feb
28 Feb
2 Mar
5 Mar
8 Mar
11 Mar
14 Mar
17 Mar
20 Mar
23 Mar
26 Mar
29 Mar
1 Apr
4 Apr
7 Apr
10 Apr
13 Apr
16 Apr
19 Apr
22 Apr
25 Apr
28 Apr

0

Excess mortality rate ( × 10−5)

8

Covid−19 excess mortality

(a)

10

8

Alignment

6
2

2

4

4

Start

8

6

Cluster 1
Cluster 2

0

0

Excess mortality rate ( × 10−5)

ProbKMA d0 c=65 days

o a a ia a o e ia a a a a o a ia a e e o a
zz at bri an iuli azi olisugl egn icilican bri netagngur ardi rch ontzan ost
p G L M P rd S s m Ve m Li b a m ol 'A
ru silicalam
b
m MPie /B e d
A Ba C a zia
To U Ro
Sa
C e
Lo
to ll
−
n
e
en Va
ilia
V
Tr
m
i
l
E
iu
Fr

(b)

(c)

Figure 5: Italian Covid-19 excess mortality rate curves and probKMA results. (a) Smoothed
curves. Vertical black lines represent national lock down (March 9th) and closure of all nonessential economic activities (March 23rd); (b) Cluster centers (black) with aligned clustered
portions of curves; (c) Alignment between portions of curves within each cluster (start day
of each portion).

16

in the entirety of the curves (possibly excluding the extremes of their domains), but we
allow shifts in their alignment to take into consideration possible differences in the time
when the (shared) patterns begun in each region. In particular, we employ probKMA as a
clustering method, with L2 -like distance d0 and cluster centers of length c = 65 days (hence
allowing for a maximum shift of 10 days). Fig. 5(b)-(c) show probKMA results for K = 2,
when assigning each curve to the cluster with highest membership probability. Cluster 2
contains the regions (mainly located in the north of Italy) where Covid-19 hit the hardest.
Lombardia is the region with earliest Covid-19 related deaths, followed by Emilia Romagna,
Marche, Liguria, Piemonte and Trento/Bolzano, and last Valle dAosta (with a delay of 7
days). Cluster 1 contains the regions with milder epidemic patterns. Interestingly, Veneto
is placed in Cluster 1 despite being the first region – together with Lombardia – to officially
report Covid-19 cases. This suggests that Veneto successfully managed to “flatten the
curve” with its early mass testing and contact tracing response (Mugnai and Bilato, 2020).
In contrast, the pattern in Lombardia is so stark that it does not seem to fit properly even
in Cluster 1 – with a large distance from the cluster center (see Supplementary Material).
Indeed, repeating the probKMA clustering with K = 3, Lombardia is placed in a cluster
of its own, while the other two clusters and the alignments within them do not change (see
Supplementary Material).

5.3

Mutagenesis data

To fully illustrate the proposed method in its motif discovery purpose, we apply it to a mutagenesis dataset adapted from Kuruppumullage Don et al. (2013). Mutagenesis comprises
all the processes by which mutations are generated in DNA, it is one of major evolutionary forces, and is central to causing many human diseases (e.g. cancer). Understanding
mutagenesis and how it is influenced by the genomic landscape is key to shedding light
on genome dynamics (Makova and Hardison, 2015). Kuruppumullage Don et al. (2013)
estimated different types of neutral (i.e. not affected by selection) mutation rates in nonoverlapping windows along the human genome comparing it with primates, and employed
Hidden Markov Models to define six divergence states and segment the genome accordingly.
One of the states is of particular interest: it comprises “hot regions” with very high rates
for substitutions, small insertions and deletions, which are associated with high GC content, early replication timing and open chromatin. Since these results were obtained at a
rather large scale (1-Mb windows), investigating rates at finer resolution within the hot regions may reveal more specific trends and patterns of variation. Estimating high-resolution
mutation rates in 1-kb windows within each hot region (with the same pipeline as in Kuruppumullage Don et al., 2013, see Supplementary Material) we generate a dataset of 43
curves, varying in length from 1 Mb (corresponding to a grid of 1 000 points) to 22 Mb
(22 000 points). The curves are very noisy, and contain several missing or inaccurate values
– due to the fact that in many 1-kb windows the information needed to estimate rates is
scarce. In particular, substitution rates can be reliably estimated only in 60% of the 1-kb
windows. After pre-processing with stochastic regression imputation and local smoothing,
missing values are reduced to 17% of the windows (see Supplementary Material). In this
application we do not consider insertion and deletion rates, because their estimates are yet
noisier and less accurate than for substitution rates.
We employ our probKMA-based functional motif discovery on the 43 curves using the
Sobolev-like distance d˜0.5 (the generalized version which can accommodate large gaps; see
17

Table 3: ProbKMA-based functional motif discovery in substitution rate curves. For
each motif found, we report number of occurrences in the data and mean distance of the
occurrences from the motif.
Motif
Number
Mean dist

1

2

3

4

5

6

7

8

9

10

11

12

13

19
1.9

12
1.9

27
3.5

37
5.1

63
6.5

12
3.0

72
8.7

47
7.4

14
5.2

11
3.0

9
5.5

8
18.5

6
17.0

Supplementary Material). We look for motifs with minimum lengths c = 40, 50, 60, 70
(maximum length cmax = 150), and we run probKMA for K = 2, 3, 4, 5 using 10 random
initialization for each (K, c) pair. We employ our generalized silhouette index to evaluate
each probKMA run and to filter the set of candidate motifs, and we select motif-specific
radii based on probKMA results (see Supplementary Material). We identify 13 functional
motifs that differ substantially in length (40 to 104 kb), levels and shapes (see Fig. 6(a)). The
motifs also differ substantially in frequency (i.e. number of occurrences in the data) and level
of variability (see Table 3). This highlights the advantage of employing a motif discovery
methodology able to learn motif-specific lengths, frequencies and variabilities based on the
data.
At least four of the motifs found are of biological interest: Motif 12 corresponds to eight
long sub-regions (about 100 kb) with extremely high substitution rates – an elevation of
10% to 20% relative to the mean level across all hot regions, which is already elevated in

DNase
RNA polymerase II
CTCF
H2AFZ
H3K27ac
H4K20me1
H3K36me3
H3K4me1
H3K4me2
H3K4me3
H3K79me2
H3K9ac
H3K9me3
H3K27me3
5−hydroxymethylcytosine
Sperm hypomethylation
Most conserved elements
CpG islands
Exons
GC content
G Quadruplexes
A−phased repeats
Direct repeats
Inverted repeats
Mirror repeats
Z DNA motifs
Mononucleotides
DNA transposons
Alu
MIR
LTR elements
Gene expression
Replication origin
Recombination hotspots

10
0
−10
−20

(a)

Motif 13

Motif 12

Motif 11

Motif 9

Motif 10

Motif 8

Motif 11
Motif 12
Motif 13

Motif 7

Motif 8
Motif 9
Motif 10

100
Motif 6

Motif 5
Motif 6
Motif 7

80

Motif 5

60

Motif 4

Motif 1
Motif 2
Motif 3
Motif 4

40

Motif 3

20

Motif 2

0

Motif 1

−30

Percentage change (mean)

20

Substitution rate motifs

(b)

Figure 6: ProbKMA-based functional motif discovery in substitution rate curves. (a)
Motifs found, plotted as percent changes with respect to the mean substitution rate across
all hot regions; (b) Genomic landscape of the motifs, with color intensity proportional to the
significance (− log10 (p)) of a mean difference two-sided test contrasting motif occurrences
and hot regions at large – red, blue and white represent positive, negative and non significant
(p > 0.1) differences, respectively.
18

comparison to the genome at large. Motif 4 and Motif 8 also present very high substitution
rates, and opposite patterns. In Motif 4, rates are about 10% above the overall hot regions
mean for the initial ∼20 kb’s, and then decrease. In Motif 8 rates increase and then stabilize
at about 10% above the mean for ∼20 kb’s. The two motifs have similar variability and
are both very frequent (37 and 47 occurrences, respectively). Finally, Motif 13 corresponds
to six long sub-regions with a substitution rate 20-30% below the mean. These portions of
the hot regions are in fact not hot; substitutions rates are similar to those of the rest of
the genome. To investigate the genomic landscape of the motifs found, we consider a set of
35 genomic features – measured in each of the 1-kb windows constituting the hot regions.
These features represent biological contexts that have an interplay with mutagenesis, such
as DNA conformation, DNA sequence, replication, recombination, chromatin openness and
modifications (see Supplementary Material). We then compare, independently for each
genomic feature and each motif, the mean of the measurements in motif occurrences with
the mean across all hot regions. In particular, we perform a simulation-based two-sided test
for mean difference, where the empirical null distribution is obtained from 1000 datasets
generated by randomly relocating motif occurrences within the set of curves. Fig. 6(b)
shows the results of this analysis. We observe that each motif has a characteristic genomic
landscape, which helps in its biological interpretation. For example, occurrences of Motif
13 are enriched in exons and conserved elements compared to hot regions in general – their
lowered substitution rates may indeed correlate with such enrichments.

6

Discussion

This article, for the first time to the best of our knowledge, tackles the problem of functional motif discovery from a statistical perspective. We proposed probKMA for discovering
candidate motifs in a set of curves, incorporating ideas from functional data analysis, bioinformatics and fuzzy clustering. In addition, we proposed a generalized silhouette index to
evaluate probKMA results, and implemented a post-processing stage for merging candidate
motifs and searching motif occurrences along the curves. Although many alternative strategies can be employed in post-processing, each with pros and cons, results on simulated and
real data suggest that our implementation is effective in a range of scenarios.
ProbKMA employs a flexible definition of curve similarity, which incorporates both levels
and derivatives. In addition, similarity is defined locally, in a way that tolerates large gaps in
the curves. This broadens the application scope of our methodology. ProbKMA can also be
applied to multivariate curves and, importantly, does not require the user to specify motif
lengths or variability levels at the outset. These are learned from the data, substantially
improving performance with respect to approaches where lengths and/or radii are fixed.
In our experience, motif discovery with probKMA can fail when motifs are too similar
to one another or when they are too similar to background portions of the curves. This can
happen by chance when motifs are very noisy. Relatedly, simulations show that, when motifs
are very noisy and/or dispersed in very long curves, our method can identify motifs that
were not intentionally introduced in the data – but rather randomly created when generating
background portions of the curves. In a way, these additional motifs may be considered as
“unintentional” and yet true (as opposed to false) positives; they do recur in the curves in a
way that is detectable by the algorithm. Nevertheless, in our simulations they are noisier and
have fewer occurrences. This observation underscores the need for further work addressing
19

the statistical significance of motifs found by probKMA. Notably, the flexible model based
on B-splines that we introduced to generate simulation data may play an important role in
this context, providing a way to estimate the likelihood of discovering motifs in background
curves.
Separately from its motif discovery purpose, probKMA can also be employed for probabilistic clustering of misaligned functional data based on local similarities. In this respect,
it also represents a generalization of sparse clustering procedures recently proposed in functional data analysis (Fraiman et al., 2016; Floriello and Vitelli, 2017). In the limit, when the
minimum motif length is close to the length of the curves under consideration, probKMA
becomes a probabilistic version of K-mean with (global) alignment (Sangalli et al., 2010).
Code implementing our methodology in R is available upon request. An R package is in
preparation.

Supplementary material
The online Supplement contains proofs and additional details on post-processing, simulations
and real-data analyses.

Acknowledgments
We thank Matthew Reimherr and Piercesare Secchi for discussions about functional data
methodology; Kateryna D. Makova and Di (Bruce) Chen for help with the mutagenesis
application; Valeria Vitelli and Davide Floriello for their sparse functional clustering code.

Fundings
This work was partially funded by the Eberly College of Science, the Institute for Cyberscience and the Huck Institutes of the Life Sciences (Penn State University); NSF award
DMS-1407639; and Tobacco Settlement and CURE funds of the PA Department of Health.
M.A. Cremona acknowledge the support of the NSERC.

References
Altschul, S. F., W. Gish, W. Miller, E. W. Myers, and D. J. Lipman (1990). Basic local alignment
search tool. Journal of Molecular Biology 215 (3), 403–410.
Bailey, T. L., N. Williams, C. Misleh, and W. W. Li (2006). MEME: discovering and analyzing
dna and protein sequence motifs. Nucleic Acids Research 34 (suppl 2), W369–W373.
Bezdek, J. C. (1981). Pattern recognition with fuzzy objective function algorithms. Springer.
Bezdek, J. C., R. Ehrlich, and W. Full (1984). FCM: the fuzzy c-means clustering algorithm.
Computers & Geosciences 10 (2), 191–203.
Dempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society. Series B 39 (1), 1–38.

20

Ferraty, F. and P. Vieu (2006). Nonparametric functional data analysis: theory and practice.
Springer Science & Business Media.
Floriello, D. and V. Vitelli (2017). Sparse clustering of functional data. Journal of Multivariate
Analysis 154, 1–18.
Fraiman, R., Y. Gimenez, and M. Svarc (2016). Feature selection for functional data. Journal of
Multivariate Analysis 146, 191–208.
Friedman, J. H. and J. J. Meulman (2004). Clustering objects on subsets of attributes. Journal of
the Royal Statistical Society. Series B 66 (4), 815–849.
Hathaway, R., J. Bezdek, and W. Tucker (1987). An improved convergence theory for the fuzzy
isodata clustering algorithms. Analysis of fuzzy information 3, 123–132.
Horváth, L. and P. Kokoszka (2012). Inference for functional data with applications, Volume 200.
Springer Science & Business Media.
Jacques, J. and C. Preda (2014). Functional data clustering: a survey. Advances in Data Analysis
and Classification 8 (3), 231–255.
Kuruppumullage Don, P., G. Ananda, F. Chiaromonte, and K. D. Makova (2013). Segmenting
the human genome based on states of neutral genetic divergence. Proceedings of the National
Academy of Sciences 110 (36), 14699–14704.
Lin, J., E. Keogh, S. Lonardi, and P. Patel (2002). Finding motifs in time series. In 8th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, Edmonton, Alberta, Canada.
Linardi, M., Y. Zhu, T. Palpanas, and E. Keogh (2018). Matrix profile X: VALMOD - scalable
discovery of variable-length motifs in data series. In ACM SIGMOD/PODS International Conference on Management of Data / Principles of Database Systems, Houston, Texas, USA.
Liu, X. and M. C. Yang (2009). Simultaneous curve registration and clustering for functional data.
Computational Statistics & Data Analysis 53 (4), 1361–1376.
Makova, K. D. and R. C. Hardison (2015). The effects of chromatin organization on variation in
mutation rates in the genome. Nature Reviews Genetics 16 (4), 213.
Mueen, A., E. Keogh, Q. Zhu, S. Cash, and B. Westover (2009). Exact discovery of time series
motifs. In SIAM International Conference on Data Mining, Sparks, Nevada, USA.
Mugnai, G. and C. Bilato (2020). Covid-19 in italy: lesson from the Veneto region. European
Journal of Internal Medicine.
Park, J. and J. Ahn (2017). Clustering multivariate functional data with phase variation. Biometrics 73 (1), 324–333.
Ramsay, J. O., G. Hooker, and S. Graves (2009). Functional data analysis with R and MATLAB.
Springer.
Ramsay, J. O. and B. W. Silverman (2005). Functional data analysis (2 ed.). Springer.
Rand, W. M. (1971). Objective criteria for the evaluation of clustering methods. Journal of the
American Statistical association 66 (336), 846–850.

21

Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster
analysis. Journal of Computational and Applied Mathematics 20, 53–65.
Sangalli, L. M., P. Secchi, S. Vantini, and V. Vitelli (2010). K-mean alignment for curve clustering.
Computational Statistics & Data Analysis 54 (5), 1219–1233.
Tarpey, T. and K. K. Kinateder (2003). Clustering functional data. Journal of Classification 20 (1),
093–114.
Vitelli, V. (2019). A novel framework for joint sparse clustering and alignment of functional data.
arXiv , 1912.00687.
Witten, D. M. and R. Tibshirani (2010). A framework for feature selection in clustering. Journal
of the American Statistical Association 105 (490), 713–726.
Yeh, C.-C. M., Y. Zhu, L. Ulanova, N. Begum, Y. Ding, H. A. Dau, Z. Zimmerman, D. F. Silva,
A. Mueen, and E. Keogh (2018). Time series joins, motifs, discords and shapelets: a unifying
view that exploits the matrix profile. Data Mining and Knowledge Discovery 32 (1), 83–123.
Yeh, C. M., Y. Zhu, L. Ulanova, N. Begum, Y. Ding, H. A. Dau, D. F. Silva, A. Mueen, and
E. Keogh (2016). Matrix profile I: all pairs similarity joins for time series: A unifying view that
includes motifs, discords and shapelets. In IEEE 16th International Conference on Data Mining,
Barcelona, Spain.

22

