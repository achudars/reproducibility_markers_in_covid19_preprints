Deep Transparent Prediction through Latent
Representation Analysis
D. Kollias1 , N. Bouas1 , Y. Vlaxos1 , V. Brillakis1 , M. Seferis1 , I. Kollia1 ,
L. Sukissian2 , J. Wingate3 , and S. Kollias1,2,3

arXiv:2009.07044v2 [cs.LG] 20 Sep 2020

1

School of Electrical and Computer Engineering, National Technical University of
Athens, Athens, Greece
2
GRNET National Infrastructures for Research and Technology, Athens, Greece
3
School of Computer Science, University of Lincoln, Lincoln, UK

Abstract. The paper presents a novel deep learning approach, which
extracts latent information from trained Deep Neural Networks (DNNs)
and derives concise representations that are analyzed in an effective, unified way for prediction purposes. It is well known that DNNs are capable
of analyzing complex data; however, they lack transparency in their decision making, in the sense that it is not straightforward to justify their
prediction, or to visualize the features on which the decision was based.
Moreover, they generally require large amounts of data in order to learn
and become able to adapt to different environments. This makes their
use difficult in healthcare, where trust and personalization are key issues.
Transparency combined with high prediction accuracy are the targeted
goals of the proposed approach. It includes both supervised DNN training
and unsupervised learning of latent variables extracted from the trained
DNNs. Domain Adaptation from multiple sources is also presented as an
extension, where the extracted latent variable representations are used to
generate predictions in other, non-annotated, environments. Successful
application is illustrated through a large experimental study in various
fields: prediction of Parkinson’s disease from MRI and DaTScans; prediction of COVID-19 and pneumonia from CT scans and X-rays; optical
character verification in retail food packaging.
Keywords: deep neural networks · latent variable extraction · transparency · efficiency · prediction · domain adaptation · healthcare

1

Introduction

Over the last few years, Deep Learning (DL) and Deep Neural Networks (DNNs)
have been successfully applied to numerous applications and domains due to the
availability of large amounts of labeled data, including healthcare prediction,
visual analysis and recognition [9,16,6].
Transfer learning (TL) [30] has been the main approach to train Deep Neural
Networks when only small amounts of annotated data are available. TL uses
networks previously trained with large datasets (even of generic patterns) and
fine-tunes the whole, or parts of them, using the small training datasets. A

2

Kollias et al.

serious problem is related to TL: as the DNN learns to make predictions in the
new dataset, it tends to forget the old data that are not used in the retraining
procedure; this is known as ‘catastrophic forgetting’. Moreover, when deploying
a pre-trained model to a real-life application, the assumption is that both the
source (training set) and the target (application-specific) one are drawn from
the same distribution. When this assumption is violated, the DL model trained
on the source domain will not generalize well on the target domain due to the
distribution differences between the two domains; this is known as domain shift.
Learning a discriminative model in the presence of domain shift between target
datasets is known as Domain Adaptation (DA) [21] and is targeted when dealing
with non-annotated data.
Recent research has focused on extracting trained DNN representations and
using them for classification purposes [4], either by an auto-encoder methodology,
or by monitoring neuron outputs in the convolutional or/and fully connected
network layers [32,15]. Such developments are exploited in this paper, proposing
a novel approach that is able to generate unified prediction models, providing
transparency and visualization of their decision making process in a variety
of application domains. At first, we extract appropriate internal features, say
features v, from a DNN model trained with some dataset of interest. Using
a clustering methodology, we generate concise representations, say c, of these
features. Using these representations and the nearest neighbour criterion, we
can then predict, in an efficient and transparent way, the class of new data.
Combining DNN training and clustering has been a topic of recent research.
Surveys, focusing on different clustering methodologies and different combination ways can be found in [1,22]; specific combinations can also be found in
[33,34,13]. Here, our aim is to derive the unified latent representation and prediction framework, illustrating its successful use, especially in medical applications.
The framework of interweaving DNNs and clustering has also been examined in
our former publication [17].
Next, we present a new methodology that alleviates the ‘catastrophic forgetting’ problem by generating a unified model over different datasets. According to
this methodology, we apply the originally trained DNN to a new dataset deriving a corresponding set of representations, through which we train a new DNN.
From the latter DNN, we extract a new set of features, say v0 and a concise
representation c0 . A unified prediction model is then produced by merging the
c and c0 representation sets. Having achieved high precision and recall metrics
in the derivation of each one of these representations ensures that the generated
unified model provides high prediction accuracy in the derived representation
space. This alternative prediction is of great significance in the case of new nonannotated data, since it provides a transparent way for prediction; it is shown
that it can also create richer representations of the prediction problem. We then
use the extracted latent variable representations from trained DNNs in multiple
sources, so as to generate predictions in other environments.
The proposed methodology is applied to a variety of applications, focusing
on medical imaging for healthcare, but also on other applications where image

Deep Transparent Prediction

3

analysis is used for anomaly prediction. In the latter case we focus on quality
control in retail food packaging, based on real images provided by large supermarkets [24]. In the former case we focus: a) on prediction of Parkinsons, based
on datasets of MRI and DaTScans, either created in collaboration with the
Georgios Gennimatas Hospital (GGH) in Athens [29], or provided by the PPMI
study sponsored by M. J. Fox for Parkinsons Research [20], b) on prediction of
COVID-19, based on CT chest scans and x-rays, either public, or aggregated by
GRNET in collaboration with the Hellenic Ministry of Health.
The novel contributions of the paper are the following: i) we develop a novel
unsupervised learning approach, extracting latent variables from trained DNNs,
which, after appropriate clustering, provide unified concise representations that
can be analyzed in an efficient and transparent way for prediction; ii) each concise representation set is linked to the respective input data (i.e., medical images,
or scans, or other information); we are, therefore, able to show - to the (medical)
experts and users/patients - which were the main (similar) cases on which the
provided prediction/diagnosis was based. It is then up to the experts/users to
decide whether they trust (this basis of) the diagnosis, or not; iii) we present
a DA framework, from multiple sources, which uses DNNs and extracted representations from annotated datasets, so as to generate predictions in other,
non-annotated data, collected in different environments; ii) we apply the proposed approach to the following application domains: i) for unified Parkinson’s
disease prediction, over different datasets, based on medical imaging, ii) for effective prediction of COVID-19 from CT chest scan series, or from x-rays, iii)
for optical character verification on food retail packaging, based on DA among
different datasets.

2

Related work

Related research in DNN representations’ generation includes Generative Adversarial Network (GAN) formulations [10] - to provide data augmentation and
improve generalization of the process - and capsules [26,23] to include feedback
and model structuring in the generated models. Moreover, the derived DL framework can be expressed in a multi-objective optimization framework, so that the
approach is coupled with a robust analysis methodology [12].
In recent years, many single source domain adaptation methods have been
proposed. Discrepancy-, Adversarial- and Reconstruction- based approaches are
the three primary domain adaptation approaches currently being applied to address the distribution shift [21]. Discrepancy-based approaches rely on aligning
the distributions in order to minimize the divergence between them. The most
commonly used discrepancy based methods are Maximum Mean Discrepancy
(MMD) [19] and Correlation Alignment (CORAL) [27]. Adversarial-based approaches minimize the distance between the source and the target distributions
using domain confusion, an adversarial method used in GANs. Another class of
approaches known as Reconstruction-based approaches create a shared representation between the source and target domains whilst preserving the individual characteristics of each domain. Rather than minimizing the divergence, [7]

4

Kollias et al.

learns joint representations that classify the labeled source data and at the same
time reconstruct the target domain. Moreover, multi-source domain adaptation
techniques need to handle both domain alignment between source and target
domains, along with alignment between multiple available sources.
In this paper we briefly present a DA approach that we will use in an extended pipeline, following the derivation of the unified representation framework.
In this, we minimize the feature discrepancy through implementing a new loss
function that includes both MMD and CORAL losses for improved generalization. Additionally, a Class Activation Mapping (CAM) component [35] adds an
extra step to the algorithm that provides a visualization of which areas in the
data contributed the most to the decision-making process. CAM adds insight
into the process of CNN interpretability and explainability by overlaying a heat
map over the data to demonstrate where the model is paying more attention in
its decision-making process.

3
3.1

The Proposed Methodology
The Extracted Features from Deep Neural Networks

Our approach starts by training a CNN or CNN-RNN, to predict the status of
data samples. Let us assume that we perform analysis of medical images, e.g.,
MRI and scans, collected in a specific medical centre, or hospital. As in [16]
we consider a CNN part that has a well-known structure, such as ResNet-50,
generally composed of convolutional and pooling layers, followed by one, or two
fully-connected layers. ReLU neuron models are used in this part. In the case
of convolutional and recurrent network, hidden layers with Long Short Term
Memory (LSTM) neuron models, or Gated Recurrent Units (GRU) are used on
top of the CNN part, providing the final classification, or prediction, outputs.
In our approach we select to extract and further analyse the, say M , outputs
of the last fully connected layer, or last hidden layer of the trained CNN, or CNNRNN respectively. This is due to the fact that these outputs constitute high level,
semantic extracts, based on which the trained DNN provides its final predictions.
Other choices involve features extracted, not only from high level, but also from
mid and lower level layers. In the following we present the extraction of concise
semantic information from these representations, using unsupervised analysis.
Let us assume that a dataset S, including medical input images has been
collected and used for training a DNN to predict the healthy, or not healthy,
status of subjects. Let also T denote the respective test set used to evaluate the
performance of the trained network. We train the DNN using the data in S, with
cardinality Ns and, for each input k, we collect the M values of the outputs of
neurons in the selected DNN fully connected or hidden layer, generating a vector
vs (k). A similar vector vt (k) is generated when applying the trained DNN to
each input k of the Nt test set :

Vs = (vs (k), k = 1, . . . , Ns

(1)

Deep Transparent Prediction

5

Fig. 1. Dataset S is the DNN input; clustering of Vs produces representation C

and

Vt = (vt (k), k = 1, . . . , Nt

(2)

In the following we derive a concise representation of these v vectors, by
using a clustering procedure, such as the k-means++ algorithm [2] to generate,
say, L clusters Q = {q1 , . . . , qL } through minimization of the following function:
b k-means = arg min
Q
Q

L X
X

vs − µi

2

(3)

i=1 vs ∈Vs

in which µi denotes the mean of v values belonging to cluster i. For each cluster
i, we then compute the corresponding cluster center c(i), thus defining the set
of cluster centers C, which forms a concise representation and prediction model
for medical diagnosis.

C = (c(i), i = 1, . . . , L

(4)

The procedure, of using dataset S to generate the set of cluster centers C is
illustrated in Fig. 1. Since the derived representation consists of a small number
of cluster centers, medical experts can examine and annotate the respective
DaTscans and MRI images with relevant textual information. This information
can include the subject’s status, as well as other metrics. Let us now focus on
using the set C for diagnosis in new subject cases, e.g., those included in the test
dataset T . For each input in T , we compute the vs value. We then calculate the
euclidean distance of this value from each cluster center in C and classify it to
the category of the closest cluster center. As a result, we classify each test input
to a respective category, thus predicting the subject’s status.
It should be mentioned that, using this approach, we can predict a new subject’s status in a rather efficient and transparent way. At first, only L distances
between M -dimensional vectors have to be computed and the minimum of them
be selected. Then, the subject can be informed of why the specific diagnosis
was made, through visualization of the medical images and presentation of the
medical annotations corresponding to the selected cluster center.
3.2

The Unified Prediction Model

Following the above described approach: a) we design a DNN (as shown in
Fig.1) and extensively train it for predicting a disease, based on image data

6

Kollias et al.

Fig. 2. S 0 is DNN input; Vs is DNN’ input; clustering V 0 s produces representation C 0

provided by a specific hospital, or available database, b) we generate a concise
representation (set C) composed of the derived cluster center representation that
can be used to predict the disease in an efficient way. This information, i.e., the
DNN weights and the set C, represent, in the proposed unified approach, the
knowledge obtained through the analysis of the respective database S.
Let us now consider another medical environment, where another database
related to the disease has been generated. Let us assume that it can be, similarly,
described through the respective S and T sets. We will show how the proposed
approach can alleviate the ‘catastrophic forgetting’ problem. Fig. 2 shows the
procedure we follow to achieve such a model. According to it, we present all
inputs of the new training dataset S 0 to the available DNN that we have already
trained with the original dataset S; we compute the Vs representations, similarly
to (3), named as Vs,in in Fig. 2. These representations, which were generated
using the knowledge obtained from the original dataset, form the input to a new
DNN, named DNN’ in Fig. 2; this network is trained to use these inputs so as
to predict the PD/NPD status of the subjects whose data are in set S 0 .

C 0 = (c’(i), i = 1, . . . , L0

(5)

Fig. 3. Sets T , T 0 are DNN-DNN’ inputs; V 0 s classified through C and C 0

In a similar way, as in Eqs. (3)-(5), we compute the new set of representations,
named Vs0 and through clustering the new set of cluster centers C 0 : The next
step is to merge the sets C and C 0 , creating the unified prediction model. Using
the two network structures (DNN and DNN’ in Fig. 2), in a testing formulation,
and the nearest neighbor criterion with respect to the union of C and C 0 , we

Deep Transparent Prediction

7

can predict the PD/NPD status of all subjects in both test sets T and T’, as
shown in Fig. 3. The resulting representation, consisting of the C and C 0 sets,
can predict a new subject’s status, using the knowledge acquired by DNN and
DNN’ networks trained on both datasets, in an efficient and transparent way.
In summary, we have trained the original DNN over a well defined input data
space (i.e., large number of medical images, balanced data categories, etc). Then
we do not train the new DNN with the respective (new) medical image data
set, but with the concise representations - thus achieving lower dimensionality
and lower risk of overfitting. Moreover, we take advantage of the knowledge of
the originally trained DNN (which provides the input to DNN). In this way,
knowledge is scaled and interweaved between the two networks; it is not simply
accumulated at their outputs.

4

Domain Adaptation from multiple sources

In this Section we describe the potential extension of the unified prediction
framework generated with the annotated medical datasets, to other non-annotated
datasets. We are currently implementing a pipeline which includes generation of
the latent representation sets for COVID-19 diagnosis and uses DA to extend
this framework to new datasets, which are not annotated. Since this pipeline is
currently under implementation, we briefly sketch how we will implement DA
from multiple sources in this framework, using optical character verification in
retail food packaging [25] as an example.
In particular, we will adopt the Domain Adaptation approach developed in
[31] for using labeled source data from, say, N environments, so as to provide
annotation of unlabeled data in a target environment.
The model comprises a feature extractor and a classification part. The feature extractor part learns useful representations from all environments/domains
- such as the latent variables representations derived above, whereas its subnetwork learns features specific to each source-target domain pairs. The classification part of the model learns domain-specific attributes for each target image
and provides, say, N categorization results. It aligns with domain-specific classifiers, as the class boundaries are highly likely to be misclassified, because they
are learned from different classifiers.
The model aims at minimizing: (i) the feature discrepancy, for learning domain invariant representations, (ii) the class boundary discrepancy, for minimizing the mismatch among all classifiers, (iii) the classification loss, improving
source data classification; consequently, leading to improved generalization on
the target data set. The sum of these three Losses is the overall objective function
of architecture training. A Class Activation Mapping (CAM) module is included
to visualize the DNNs interpretation when making predictions. As is well-known,
CAM highlights which areas of the input images contribute the most to the DNN
decision making process.
In this approach, feature discrepancy is reduced by minimizing both MMD
and CORAL loss. MMD defines the distance between the two data distributions.

8

Kollias et al.

CORAL loss is also used to minimize the discrepancy between source and target
data by reducing the (covariance) distance between the source and target feature
representations.
Classifiers are likely to misclassify the target samples near the class boundary
as they are trained using different source domains, each having different target
prediction. Therefore it is aimed to minimize discrepancy among, say, N classifiers by making their probabilistic outputs similar. The network also reduces the
discrepancy among classifiers by minimizing the classification loss. The network
is trained with labeled source data and minimizes the cross-entropy loss.
The total loss is made up of classification loss, feature discrepancy loss and
class discrepancy loss. By minimizing this, the network can classify source domain data more accurately and reduce dataset bias and discrepancy among classifiers.

5

Experimental Study

In the following, we apply the proposed prediction approach, based on latent information extraction, to two different Parkinsons datasets (one created in collaboration with the Georgios Gennimatas Hospital in Athens and the PPMI study
sponsored by M. J. Fox for Parkinsons Research) including MRI and DaTScans
for Parkinsons prediction. Next, we apply the proposed approach for automatically detecting COVID-19 using chest CT scan series and chest x-rays. The
most challenging part of this task is to detect the small nodules and lesions in
the early stage of the COVID-19 and differentiate it from normal cases or pneumonia. The datasets of RSNA pneumonia detection and LUNA detection are
used as source datasets, enriched by smaller datasets that exist for COVID-19.
Finally, the proposed approach including domain adaptation is applied to the
optical verification of end-by date on food retail packaging. In all cases the proposed approach improves the current state-of-the-art, providing a transparent
and easy to implement procedure for prediction of diseases, or anomalies, based
on analysis of real images aggregated in the related environments.
5.1

Prediction of Parkinson’s based on MRI and DaTscans

In [16], DNNs were trained with an augmented dataset based on the Greek
database, achieving very good performance on this database. The convolutional
part of the network was applied to each image component, i.e., to an RGB
DaTscan image and to three (gray-scale) MRIs, using the same pretrained ResNet50 structure. The outputs of these two ResNet structures were concatenated and
fed to the Fully Connected (FC) layer of the CNN part of the network.
This structure has been able to analyse the spatial characteristics of the
DaTscans and MRIs, achieving a high accuracy in the database test set, of 94%,
as shown in Table 1. The complete CNN-RNN architecture included two hidden
layers on top of the CNN part, each containing 128 GRU neurons, as shown
in the Table. This has been able to also analyse the temporal evolution of the
MRI data, achieving an improved performance of 98 % over the test data. We

Deep Transparent Prediction

9

Table 1. The accuracy obtained by CNN and CNN-RNN architectures
Structure

FC Hidden Units in FC Units in Hidden Accuracy (%)
Layers Layers
Layers
Layers
CNN
2
2622-1500
94
CNN-RNN
1
2
1500
128-128
98

trained this CNN-RNN network so as to classify the DaTscans and MRIs to the
correct PD/NPD category, using a batch size of 10, a fixed learning rate of 0.001
and a dropout probability of 50 %. The clustering process, using k-means was
then applied to the Vs vectors. Based on best precision/recall over the generated
clusters, we extracted five clusters, two of which correspond to control subjects,
i.e. NPD ones, with three clusters corresponding to patients. These constitute
the extracted concise representation C set; consequently, C is composed of five
128-dimensional vectors.

Fig. 4. The DaTscans of the 5 selected cluster centers: c1 and c2 correspond to NPD
cases, whilst c3 - c5 to progressing stages of Parkinson’s

The DaTscans corresponding to the extracted cluster centers are shown in
Fig. 4. Through the assistance of medical experts we were able to verify that
the three DaTscans corresponding to patient cases represent different stages
of Parkinson’s disease. In particular: the first of them (c3 ) represents an early
occurrence, between stage 1 and stage 2; the second (c4 ) shows a pathological
case, at stage 2; the third (c5 ) represents a case that has reached stage 3 of
Parkinson’s. In the case of controls, there are differences between the first (c1 ),
which is a clear NPD case and the second (c2 ), which is a more obscure case.
Following the above annotations, it can be said that the derived representations convey more information about the subjects’ status than trained DNN
outputs. This information can be used by medical experts to evaluate the predictions made by the original DNN when new subjects’ data have to be analysed.

10

Kollias et al.

The computed Vs representations in the new cases can be efficiently classified to
the category of the nearest cluster center of C; the cluster center’s Datscan, MRIs
and annotations will then be used to justify, in a transparent way, the provided
prediction. In case of new data, retraining of the deep neural network would
be required, so as to retain the old knowledge and include the new one; this
would be computationally intensive and possibly unfeasible. On the contrary,
the proposed approach would only require extension of the C set with one, or
more, cluster centers, corresponding to the new information; as a consequence,
this would be done in a very efficient way.
Next, we examine the ability of the procedure shown in Fig. 2, using the
trained DNN (CNN-RNN) architecture, to be successfully applied to the PPMI
database [20], for PD/NPD prediction. Since the DaTscans were the basic source
of the DNN’s discriminating ability, we focus our new developments on the
DaTscans included in the PPMI database. For this reason, we have retained
609 subjects from the PPMI database, excluding some patients for which we
were not able to extract DaTscans of good quality. In total we selected 1481
DaTscans, which we combined with MRI triplets from the respective subjects,
generating a dataset of 7700 inputs; each input was composed, of one (gray-scale)
DaTscan and a triplet of MRI images.
At first, for comparison purposes, we trained CNN and CNN-RNN networks,
similar to the ones presented in the previous subsection, from scratch, on the selected PPMI training set (6656 inputs). We used the validation set (1584 inputs)
to test the obtained accuracy in the end of each training epoch. We then tested
the performance of the networks on the test set (2028 inputs). The obtained
accuracy was in the range of 96-97 %, similar to the accuracy achieved by the
state-of-the-art techniques on PPMI. We also used TL on networks generated in
the first subsection of our experimental study, to initialise the re-training of the
new networks. Similar results were obtained in this case as well.
We then applied the procedure shown in Fig. 2, to train DNN’ with the Vs
vectors extracted from the last hidden layer of the DNN that had been trained on
[29,28]. The performance of the network was very high, classifying in the correct
PD/NPD category 99.76 % of the inputs. By then implementing the clustering
procedure shown in Fig. 2, we were able to extract five new clusters, three
of which represent NPD subjects’ cases and two of which represent PD cases.
These cluster centers are 32-dimensional vectors. Fig. 5 shows the DaTscans
corresponding to the cluster centers c’1 - c’5 . Since the patients in the PPMI
Database generally belong to early stages of Parkinson’s (stage 1 to stage 2), it
can be seen that two cluster centers, i.e., c’4 and c’5 were enough to represent
these cases. Variations in the appearance of the non-Parkinson’s cases can be
seen in c’1 - c’3 DaTscans.
We then applied the merging of sets C and C 0 . It should be mentioned that
the 5 centers in set C were 128-dimensional, whilst the 5 centers on set C 0 were
32-dimensional. To produce a unified representation, we made an ablation study,
through PCA analysis, on the classification performance achieved in dataset
[29], if we represented the five cluster centers in C through only 32 principal

Deep Transparent Prediction

11

Fig. 5. DaTscans of the 5 cluster centers in C 0 , with NPD at top and PD at bottom

Fig. 6. The obtained ten cluster centers in 3-D: 5 of them (squares with red/rose color,
& plus (+) symbols with green color) depict patients; 5 of them (stars with blue color
& circles with black/grey color) depict non-patients

components. We were able to achieve a classification performance of 97.92 %,
which is very close to the 98 % performance in Table 1. Consequently, we were
able to generate a unified model consisting of 10 32-dimensional cluster centers.
Fig. 6 shows a 3-D projection of the ten cluster centers. The three (red/rose)
squares denote the patient cases in the dataset [29] and the two (green) plus
(+) symbols represent the patient cases in the PPMI dataset. The two (blue)
stars represent the normal cases in the Greek dataset and the three (black/grey)
circles represent the normal cases in the PPMI dataset. It can be seen that the
PD centers are distinguishable from the NPD ones.
This has been verified by testing the ability of the unified prediction model
to correctly classify all input data in test sets T and T 0 , i.e., the data from both
datasets. There was no effect on the performance of the prediction achieved by
each prediction model, i.e., C and C 0 when applied, separately, to their respective
datasets. This shows that the unified representation set, composed of the union
of C and C 0 , has been able to provide exactly the same prediction results, as the
original representation sets.

12

Kollias et al.

Let us further discuss the significance of the derived cluster centers, for generating trustworthy DNN decision making in healthcare. Whenever a PD/NPD
prediction is provided to the medical expert for a specific subject, it will also
show the subject’s DaTscan, together with the DaTscan of the center of the
selected cluster. The latter will indicate what type of data were used by the system to generate its prediction. In this way, the medical expert, and the subject,
could decide by themselves whether to trust, or not, the suggested decision.
5.2

Prediction of COVID-19 based on CT chest scans and x-rays

We applied the proposed procedure for detection of COVID-19, based on two
medical image types, i.e., CT chest scans and chest x-rays.
The COVID19-CT dataset [11], is an open source public dataset, containing
349 CT scans of 143 patients positive for COVID-19 and 397 CT scans of people
that are negative for COVID-19 (subjects that are normal, or have other types
of diseases). The images of positive patients are collected from medRxiv and
bioRxiv papers about COVID-19. The authors selected CTs containing COVID19 abnormalities by reading the figure captions in the papers and also manually
removed artifacts in the original images. The CTs of people negative for COVID19 are selected from the PubMed Central search engine and from the MedPix,
a publicly-open online medical image database that contains CT scans with
various diseases. The COVID19-CT dataset is split into training, validation and
test sets which contain 191, 60, 98 CT scans positive for COVID-19 and 234, 58,
105 CT scans negative for COVID-19, respectively.
In [11] the EfficientNet-b0, pretrained on Imagenet (including 5,288,548 parameters), was retrained and tested on this dataset, providing an F1 score of
0.78. Data augmentation, based on random cropping, horizontal flip, color jittering with random contrast and random brightness was used. We applied the
proposed procedure for latent variable extraction, using the same EfficientNet-b0
network, adding an extra hidden layer of 32 units, so as to extract 32-dimensional
representations and using vertical and horizontal flipping for data augmentation.
5 clusters were generated, with their respective centroids shown in Fig. 7. The
obtained F1 score was much higher 0.842 (0.855 for the non-COVID case and
0.828 for the COVID one), whilst providing the CT scans that are the (cluster
center) attractors for new predictions. It should be mentioned that in [11] another much more complex network, DenseNet-169 (with 14,149,480 parameters;
trained with additional images from the Lung Nodule Analysis dataset) achieved
an F1 score of 0.85.
In the case of chest x-rays, we used two different datasets, i.e., the COVID
ChestXray dataset [5] and the Kaggle RSNA pneumonia dataset. The first
dataset consists of data compiled from websites such as Radiopaedia.org, the
Italian Society of Medical and Interventional Radiology and Figure1.com. The
second dataset is part of the RSNA Pneumonia Detection Challenge dataset.
In total, the merged dataset consisted of images of subjects with pneumonia,
subjects with COVID-19 and normal ones. The merged dataset was split into
training and test parts, which contained 8629 and 955 pneumonia instances, 128
and 14 COVID-19 instances and 9766 and 885 normal instances, respectively.

Deep Transparent Prediction

13

Fig. 7. CT Scans of the 5 cluster centers: non-COVID at top, COVID at bottom

In a work4 , the COVID-Next network, which is a ResNeXt50-32x4d, was
trained on this dataset. Data augmentation, based on vertical and horizontal
flip, affine transformations (translation, scaling, shearing) and color jittering
was used. COVID-Next achieved an F1 score of 0.93. We applied the proposed
procedure, using the same network and transforms, adding an extra layer with
32 units. Five clusters of the derived representations were generated, with the
respective centroids being shown in Fig. 8. The achieved F1 score was 0.96, much
higher than the state-of-the-art with the same network.
We have further unified the extracted representations from CT scans and
x-rays, verifying the improved classification achieved through the proposed approach.

Fig. 8. Chest x-rays of the 5 cluster centers

5.3

Optical Character Verification in Retail Food Packaging

The Food Packaging Image dataset used in this study consisted of more than
30,000 balanced images (OK vs NOT-OK) from six different locations in UK,
and the target was to automatically verify the quality of use-by dates printed
on them. Initially, three people manually annotated this dataset, with two more
annotators further sampling and verifying the manual annotations for quality
control, hence keeping those 30,000 images that both of them were in full agreement. The main challenges of these datasets were unavailability of labeled data
4

https://github.com/velebit-ai/COVID-Next-Pytorch

14

Kollias et al.

and high variability across the datasets, such as heavy distortion, varying background, illumination/blur, date format, angle and orientation of the label. Training was carried out on a 70 % of the samples with another 10 % used for validation. Finally, the remaining 20 % of the images were used for evaluating and
testing across the six locations, in order to automatically verify the quality of
printed use-by dates, hence detecting images of very low quality.
Experiments were conducted [31], using the multi-source DA approach, i.e.,
using two labeled source datasets for adaptation to a single unlabeled target
domain. The obtained results were compared to the baseline single source adaptation experiment conducted initially. The labeled sources and the unlabeled
target images have been fed through the model where the discrepancy between
the pair of datasets was minimized by jointly reducing feature discrepancy, class
discrepancy and classification losses. The multi-source DA approach significantly
outperformed single-source DA, with an average classification accuracy improvement by more than 6 %. In particular, the achieved performance through multisource DA was very high, reaching an accuracy of 90.53 %, compared to an
accuracy of 84.14 % in the single source DA case.
As was above-mentioned, our current work includes application of the multisource DA approach to COVID-19 prediction.

6

Conclusions and Future Work

In this paper we have developed a new approach for deriving efficient and transparent prediction models and used them for prediction of Parkinson’s, of COVID19 and for detection of anomalies in optical character verification. A crucial issue
in the above described approach is related to the uncertainty introduced in DA,
since labels are not known when dealing with the new data. We are currently extending the developed DNN architectures using Bayesian formulations to provide
self-training capabilities. We are using a sample-wise weighting scheme during
training that places a weight on each training sample, according to the estimated
uncertainty over predicted label, so that we incrementally encourage the model
to assign more weight to uncertain label samples as training progresses [24].
We are also working on representing the DA procedure as a multi-objective
optimization problem [12]. We are investigating the use of capsules in the developed algorithms [26]. In the future we will combine the data driven DNN representations with knowledge-based ontological representation and visual attention
mechanisms [8,14,18,23,3] so as to formally explain the derived predictions.
GRNET is the main consultant of the Greek Ministry of Digital Governance
(DG). Through their Harmoni project with the Ministry of Health, all radiological exams in public hospitals are directly transferred and stored in GRNET data
centers. The Harmoni database does not include annotated medical exams. We
will infer annotations and generate an enriched version of Harmoni. This will be
an issue to discuss through the collaboration with CLAIRE Network that is supported by the Greek Ministry of DG and GRNET and with TAILOR Network
to which NTUA and GRNET participate.

Deep Transparent Prediction

15

References
1. Aljalbout, E., Golkov, V., Siddiqui, Y., Strobel, M., Cremers, D.: Clustering
with deep learning: Taxonomy and new methods. arXiv preprint arXiv:1801.07648
(2018)
2. Arthur, D., Vassilvitskii, S.: k-means++: The advantages of careful seeding. Tech.
rep., Stanford (2006)
3. Avrithis, Y., Tsapatsoulis, N., Kollias, S.: Broadcast news parsing using visual
cues: A robust face detection approach. In: 2000 IEEE International Conference
on Multimedia and Expo. ICME2000. Proceedings. Latest Advances in the Fast
Changing World of Multimedia (Cat. No. 00TH8532). vol. 3, pp. 1469–1472. IEEE
(2000)
4. Azizi, S., et al.: Detection and grading of prostate cancer using temporal enhanced
ultrasound: combining deep neural networks and tissue mimicking simulations.
International journal of computer assisted radiology and surgery 12 (06 2017)
5. Cohen, J.P., Morrison, P., Dao, L., Roth, K., Duong, T.Q., Ghassemi, M.: Covid19 image data collection: Prospective predictions are the future. arXiv preprint
arXiv:2006.11988 (2020)
6. Esteva A., Robicquet A., e.a.: A guide to deep learning in healthcare. Nature
medicine 25, 24–29 (1 2019)
7. Ghifary, M., Kleijn, W.B., Zhang, M., Balduzzi, D., Li, W.: Deep reconstructionclassification networks for unsupervised domain adaptation. In: European Conference on Computer Vision. pp. 597–613. Springer (2016)
8. Glimm, B., Kazakov, Y., Kollia, I., Stamou, G.: Lower and upper bounds for
sparql queries over owl ontologies. In: Twenty-Ninth AAAI Conference on Artificial
Intelligence (2015)
9. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press (2016)
10. Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Information Processing Systems 27, pp. 2672–2680. Curran Associates, Inc. (2014)
11. He, X., Yang, X., Zhang, S., Zhao, J., Zhang, Y., Xing, E., Xie, P.: Sample-efficient
deep learning for covid-19 diagnosis based on ct scans. medRxiv (2020)
12. Jiang, S., Kaiser, M., Yang, S., Kollias, S., Krasnogor, N.: A scalable test suite for
continuous dynamic multiobjective optimization. IEEE transactions on cybernetics
50(6), 2814–2826 (2019)
13. Kappeler, A., et al.: Combining deep learning and unsupervised clustering to improve scene recognition performance. In: 2015 IEEE 17th International Workshop
on Multimedia Signal Processing (MMSP). pp. 1–6. IEEE (2015)
14. Kollia, I., Simou, N., Stafylopatis, A., Kollias, S.: Semantic image analysis using a
symbolic neural architecture. Image Analysis & Stereology 29(3), 159–172 (2010)
15. Kollia, I., Stafylopatis, A.G., Kollias, S.: Predicting parkinsons disease using latent information extracted from deep neural networks. In: 2019 International Joint
Conference on Neural Networks (IJCNN). pp. 1–8. IEEE (2019)
16. Kollias, D., Tagaris, A., Stafylopatis, A., Kollias, S., Tagaris, G.: Deep neural
architectures for prediction in healthcare. Complex & Intelligent Systems 4(2),
119–131 (2018)
17. Kollias, D., Yu, M., Tagaris, A., Leontidis, G., Stafylopatis, A., Kollias, S.: Adaptation and contextualization of deep neural network models. In: 2017 IEEE symposium series on computational intelligence (SSCI). pp. 1–8. IEEE (2017)

16

Kollias et al.

18. Kollias, D., Marandianos, G., Raouzaiou, A., Stafylopatis, A.G.: Interweaving deep
learning and semantic techniques for emotion analysis in human-machine interaction. In: 2015 10th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP). pp. 1–6. IEEE (2015)
19. Long, M., Cao, Y., Wang, J., Jordan, M.: Learning transferable features with
deep adaptation networks. In: Proceedings of the 32nd International Conference
on Machine Learning. vol. 37, pp. 97–105. PMLR (07–09 Jul 2015)
20. Marek, K., et al.: The parkinson progression marker initiative (ppmi). Progress in
Neurobiology 95, 629–635 (12 2011)
21. Mei, W., Deng, W.: Deep visual domain adaptation: A survey. Neurocomputing
(02 2018)
22. Min, E., Guo, X., Liu, Q., Zhang, G., Cui, J., Long, J.: A survey of clustering
with deep learning: From the perspective of network architecture. IEEE Access 6,
39501–39514 (2018)
23. Rapantzikos, K., Tsapatsoulis, N., Avrithis, Y., Kollias, S.: Bottom-up spatiotemporal visual attention model for video analysis. IET Image Processing 1(2), 237–248
(2007)
24. Ribeiro, F.D.S., Calivá, F., Swainson, M., Gudmundsson, K., Leontidis, G., Kollias, S.: Deep bayesian self-training. Neural Computing and Applications pp. 1–17
(2019)
25. Ribeiro, F.D.S., Gong, L., Calivá, F., Swainson, M., Gudmundsson, K., Yu, M.,
Leontidis, G., Ye, X., Kollias, S.: An end-to-end deep neural architecture for optical
character verification and recognition in retail food packaging. In: 2018 25th IEEE
International Conference on Image Processing (ICIP). pp. 2376–2380. IEEE (2018)
26. Ribeiro, F.D.S., Leontidis, G., Kollias, S.D.: Capsule routing via variational bayes.
In: AAAI. pp. 3749–3756 (2020)
27. Sun, B., Saenko, K.: Deep coral: Correlation alignment for deep domain adaptation.
In: European conference on computer vision. pp. 443–450. Springer (2016)
28. Tagaris, A., Kollias, D., Stafylopatis, A.: Assessment of parkinsons disease based
on deep neural networks. In: International Conference on Engineering Applications
of Neural Networks. pp. 391–403. Springer (2017)
29. Tagaris, A., Kollias, D., Stafylopatis, A., Tagaris, G., Kollias, S.: Machine learning
for neurodegenerative disorder diagnosissurvey of practices and launch of benchmark dataset. International Journal on Artificial Intelligence Tools 27(03), 1850011
(2018)
30. Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., Liu, C.: A Survey on Deep
Transfer Learning: 27th International Conference on Artificial Neural Networks,
2018, Proceedings, Part III, pp. 270–279 (10 2018)
31. Thota, M., Kollias, S., Swainson, M., Leontidis, G.: Multi-source deep domain adaptation for quality control in retail food packaging. arXiv preprint
arXiv:2001.10335 (2020)
32. Wingate, J., Kollia, I., Bidaut, L., Kollias, S.: A unified deep learning approach
for prediction of parkinson’s disease. arXiv preprint arXiv:1911.10653 (2019)
33. Xie, J., Girshick, R., Farhadi, A.: Unsupervised deep embedding for clustering
analysis. In: International conference on machine learning. pp. 478–487 (2016)
34. Yang, B., Fu, X., Sidiropoulos, N.D., Hong, M.: Towards k-means-friendly spaces:
Simultaneous deep learning and clustering. In: international conference on machine
learning. pp. 3861–3870 (2017)
35. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features
for discriminative localization. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 2921–2929 (2016)

