Subset Node Representation Learning over Large Dynamic
Graphs
xingzguo@cs.stonybrook.edu
Stony Brook University
Stony Brook, USA

Baojian Zhou∗

arXiv:2106.01570v1 [cs.SI] 3 Jun 2021

ABSTRACT
Dynamic graph representation learning is a task to learn node
embeddings over dynamic networks, and has many important applications, including knowledge graphs, citation networks to social
networks. Graphs of this type are usually large-scale but only a
small subset of vertices are related in downstream tasks. Current
methods are too expensive to this setting as the complexity is at
best linear-dependent on both the number of nodes and edges.
In this paper, we propose a new method, namely Dynamic Personalized PageRank Embedding (DynamicPPE) for learning a target
subset of node representations over large-scale dynamic networks.
Based on recent advances in local node embedding and a novel
computation of dynamic personalized PageRank vector (PPV), DynamicPPE has two key ingredients: 1) the per-PPV complexity is
¯ where 𝑚, 𝑑,
¯ and 𝜖 are the number of edges received, averO (𝑚𝑑/𝜖)
age degree, global precision error respectively. Thus, the per-edge
event update of a single node is only dependent on 𝑑¯ in average;
and 2) by using these high quality PPVs and hash kernels, the
learned embeddings have properties of both locality and global
consistency. These two make it possible to capture the evolution of
graph structure effectively.
Experimental results demonstrate both the effectiveness and efficiency of the proposed method over large-scale dynamic networks.
We apply DynamicPPE to capture the embedding change of Chinese
cities in the Wikipedia graph during this ongoing COVID-19 pandemic 1 . Our results show that these representations successfully
encode the dynamics of the Wikipedia graph.

Dynamic graph embedding; Representation learning; Personalized
PageRank; Knowledge evolution
ACM Reference Format:
Xingzhi Guo, Baojian Zhou, and Steven Skiena. 2021. Subset Node Representation Learning over Large Dynamic Graphs. In Proceedings of the 27th
ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD
authors contributed equally to this research

1 https://en.wikipedia.org/wiki/COVID-19_pandemic

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
KDD ’21, August 14–18, 2021, Virtual Event, Singapore.
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8332-5/21/08. . . $15.00
https://doi.org/10.1145/XXXXXXXXXXXXX

skiena@cs.stonybrook.edu
Stony Brook University
Stony Brook, USA
4

G t−1
v2

v1

∆G = {e1 , e2 , e3 , e4 }
G

t

e3
e1

v2

v1

e4

e2

×10−5

(a) Dynamic graph model

Joe Biden
Donald Trump

Election
Inauguration

3
2
1
0
12

{wvt 1 , wvt 2 , . . .} = DynamicPPE (G t , {v1 , v2 , . . .})

13

14

15

16

17

Year

18

19

20

(b) An application of DynamicPPE

Figure 1: (a) The model of dynamic network in two consecutive snapshots. (b) An application of DynamicPPE to keep
track embedding movements of interesting Wikipedia articles (vertices). We learn embeddings of two presidents of
the United States on the whole English Wikipedia graph
from 2012 monthly, which cumulatively involves 6.2M articles (nodes) and 170M internal links (edges). The embedding movement between two time points is defined as 1 −
cos(𝒘 𝑣𝑡 , 𝒘 𝑣𝑡 +1 ) where cos(·, ·) is the cosine similarity. The significant embedding movements may reflect big social status
changes of Donald_Trump and Joe_Biden 2 in this dynamic
Wikipedia graph.
’21), August 14–18, 2021, Virtual Event, Singapore. ACM, New York, NY, USA,
11 pages. https://doi.org/10.1145/XXXXXXXXXXXXX

1
KEYWORDS

∗ Both

Steven Skiena

baojian.zhou@cs.stonybrook.edu
Stony Brook University
Stony Brook, USA

Embedding movement

Xingzhi Guo∗

INTRODUCTION

Graph node representation learning aims to represent nodes from
graph structure data into lower dimensional vectors and has received much attention in recent years [12, 15, 16, 23, 29, 31, 37]. Effective methods have been successfully applied to many real-world
applications where graphs are large-scale and static [43]. However,
networks such as social networks [4], knowledge graphs [20], and
citation networks [7] are usually time-evolving where edges and
nodes are inserted or deleted over time. Computing representations
of all vertices over time is prohibitively expensive because only a
small subset of nodes may be interesting in a particular application.
Therefore, it is important and technical challenging to efficiently
learn dynamic embeddings for these large-scale dynamic networks
under this typical use case.
Specifically, we study the following dynamic embedding problem:
We are given a subset 𝑆 = {𝑣 1, 𝑣 2, . . . , 𝑣𝑘 } and an initial graph G𝑡
with 𝑡 = 0. Between time 𝑡 and 𝑡 + 1, there are edge events of
insertions and/or deletions. The task is to design an algorithm to
2 Two

English Wikipedia articles are accessible at https://en.wikipedia.org/wiki/
Donald_Trump and https://en.wikipedia.org/wiki/Joe_Biden.

learn embeddings for 𝑘 nodes with time complexity independent
on the number of nodes 𝑛 per time 𝑡 where 𝑘 ≪ 𝑛. This problem
setting is both technically challenging and practically important.
For example, in the English Wikipedia graph, one need focus only
on embedding movements of articles related to political leaders,
a tiny portion of whole Wikipedia. Current dynamic embedding
methods [10, 28, 47, 49, 50] are not applicable to this large-scale
problem setting due to the lack of efficiency. More specifically,
current methods have the dependence issue where one must learn all
embedding vectors. This dependence issue leads to per-embedding
update is linear-dependent on 𝑛, which is inefficient when graphs
are large-scale. This obstacle motivates us to develop a new method.
In this paper, we propose a dynamic personalized PageRank
embedding (DynamicPPE) method for learning a subset of node
representations over large-sale dynamic networks. DynamicPPE
is based on an effective approach to compute dynamic PPVs [45].
There are two challenges of using Zhang et al. [45] directly: 1) the
quality of dynamic PPVs depend critically on precision parameter
𝜖, which unfortunately is unknown under the dynamic setting; and
2) The update of per-edge event strategy is not suitable for batch
update between graph snapshots. To resolve these two difficulties,
first, we adaptively update 𝜖 so that the estimation error is independent of 𝑛, 𝑚, thus obtaining high quality PPVs. Yet previous work
does not give an estimation error guarantee. We prove that the
¯ Second, we incorporate
time complexity is only dependent on 𝑑.
a batch update strategy inspired from [13] to avoid frequent peredge update. Therefore, the total run time to keep track of 𝑘 nodes
¯ Since real-world graphs have the
for given snapshots is O (𝑘𝑑𝑚).
¯
sparsity property 𝑑 ≪ 𝑛, it significantly improves the efficiency
compared with previous methods. Inspired by InstantEmbedding
[30] for static graph, we use hash kernels to project dynamic PPVs
into embedding space. Figure 1 shows an example of successfully
applying DynamicPPE to study the dynamics of social status in the
English Wikipedia graph. To summarize, our contributions are:
(1) We propose a new algorithm DynamicPPE, which is based
on the recent advances of local network embedding on static
graph and a novel computation of dynamic PPVs. DynamicPPE
effectively learns PPVs and then projects them into embedding
space using hash kernels.
(2) DynamicPPE adaptively updates the precision parameter 𝜖 so
that PPVs always have a provable estimation error guarantee.
In our subset problem setting, we prove that the time and space
complexity are all linear to the number of edges 𝑚 but independent on the number of nodes 𝑛, thus significantly improve the
efficiency.
(3) Node classification results demonstrate the effectiveness and efficiency of the proposed. We compile three large-scale datasets
to validate our method. As an application, we showcase that
learned embeddings can be used to detect the changes of Chinese cities during this ongoing COVID-19 pandemic articles on
a large-scale English Wikipedia.
The rest of this paper is organized as follows: In Section 2, we
give the overview of current dynamic embedding methods. The
problem definition and preliminaries are in Section 3. We present
our proposed method in Section 4. Experimental results are reported
in Section 5. The discussion and conclusion will be presented in

Section 6. Our code and created datasets are accessible at https:
//github.com/zjlxgxz/DynamicPPE.

2

RELATED WORK

There are two main categories of works for learning embeddings
from the dynamic graph structure data. The first type is focusing
on capturing the evolution of dynamics of graph structure [49].
The second type is focusing on both dynamics of graph structure
and features lie in these graph data [38]. In this paper, we focus
on the first type and give the overview of related works. Due to
the large mount of works in this area, some related works may not
be included, one can find more related works in a survey [22] and
references therein.
Dynamic latent space models The dynamic embedding models
had been initially explored by using latent space model [18]. The
dynamic latent space model of a network makes an assumption
that each node is associated with an 𝑑-dimensional vector and
distance between two vectors should be small if there is an edge
between these two nodes [32, 33]. Works of these assume that the
distance between two consecutive embeddings should be small. The
proposed dynamic models were applied to different applications
[17, 34]. Their methods are not scalable from the fact that the time
complexity of initial position estimation is at least O (𝑛 2 ) even if
the per-node update is log(𝑛).
Incremental SVD and random walk based methods Zhang
et al. [48] proposed TIMERS that is an incremental SVD-based
method. To prevent the error accumulation, TIMERS properly set
the restart time so that the accumulated error can be reduced.
Nguyen et al. [28] proposed continuous-time dynamic network embeddings, namely CTDNE. The key idea of CTNDE is that instead
of using general random walks as DeepWalk [29], it uses temporal
random walks contain a sequence of edges in order. Similarly, the
work of Du et al. [10] was also based on the idea of DeepWalk. These
methods have time complexity dependent on 𝑛 for per-snapshot
update. Zhou et al. [49] proposed to learn dynamic embeddings by
modeling the triadic closure to capture the dynamics.
Graph neural network methods Trivedi et al. [38] designed a
dynamic node representation model, namely DyRep, as modeling
a latent mediation process where it leverages the changes of node
between the node’s social interactions and its neighborhoods. More
specifically, DyRep contains a temporal attention layer to capture
the interactions of neighbors. Zang and Wang [44] proposed a neural network model to learn embeddings by solving a differential
equation with ReLU as its activation function. [24] presents a dynamic embedding, a recurrent neural network method, to learn
the interactions between users and items. However, these methods
either need to have features as input or cannot be applied to largescale dynamic graph. Kumar et al. [24] proposed an algorithm to
learn the trajectory of the dynamic embedding for temporal interaction networks. Since the learning task is different from ours, one
can find more details in their paper.

3

NOTATIONS AND PRELIMINARIES

Notations We use [𝑛] to denote a ground set [𝑛] := {0, 1, . . . , 𝑛 −
1}. The graph snapshot at time 𝑡 is denoted as G𝑡 V𝑡 , E𝑡 . The
degree of a node 𝑣 is 𝑑 (𝑣)𝑡 . In the rest of this paper, the average

degree at time 𝑡 is 𝑑¯𝑡 and the subset of target nodes is 𝑆 ⊆ V𝑡 . Bold
capitals, e.g. 𝑨, 𝑾 are matrices and bold lower letters are vectors
𝒘, 𝒙. More specifically, the embedding vector for node 𝑣 at time
𝑡 denoted as 𝒘 𝑣𝑡 ∈ R𝑑 and 𝑑 is the embedding dimension. The 𝑖th entry of 𝒘 𝑣𝑡 is 𝑤 𝑣𝑡 (𝑖) ∈ R. The embedding of node 𝑣 for all 𝑇
snapshots is written as 𝑾𝑣 = [𝒘 𝑣1, 𝒘 𝑣2, . . . , 𝒘𝑇𝑣 ] ⊤ . We use 𝑛𝑡 and 𝑚𝑡
as the number of nodes and edges in G𝑡 which we simply use 𝑛
and 𝑚 if time 𝑡 is clear in the context.
Given the graph snapshot G𝑡 and a specific node 𝑣, the personalized PageRank vector (PPV) is an 𝑛-dimensional vector 𝝅𝑣𝑡 ∈ R𝑛
and the corresponding 𝑖-th entry is 𝜋 𝑣𝑡 (𝑖). We use 𝒑𝑡𝑣 ∈ R𝑛 to stand
for a calculated PPV obtained from a specific algorithm. Similarly,
the corresponding 𝑖-th entry is 𝑝 𝑡𝑣 (𝑖). The teleport probability of the
PageRank is denoted as 𝛼. The estimation error of an embedding vector is the difference between true embedding 𝒘 𝑣𝑡 and the estimated
Í
^ 𝑣𝑡 (𝑖) .
embedding 𝒘^𝑣𝑡 is measure by ∥ · ∥ 1 := 𝑛𝑖=1 𝑤 𝑣𝑡 (𝑖) − 𝑤

3.1

Given any initial graph (could be an empty graph), the corresponding dynamic graph model describes how the graph structure evolves
over time. We first define the dynamic graph model, which is based
on Kazemi and Goel [22].
Definition 1 (Simple dynamic graph model [22]). A simple dynamic graph model is defined as an ordered of snapshots G 0, G 1 ,
G 2, . . . , G𝑇 where G 0 is the initial graph. The difference of graph
G𝑡 at time 𝑡 = 1, 2, . . . ,𝑇 is Δ𝐺 𝑡 (ΔV𝑡 , ΔE𝑡 ) := 𝐺 𝑡 \𝐺 𝑡 −1 with
ΔV𝑡 := V𝑡 \V𝑡 −1 and ΔE𝑡 := E𝑡 \E𝑡 −1 . Equivalently, ΔG𝑡 corresponds to a sequence of edge events as the following

𝑡
ΔG𝑡 = 𝑒 1𝑡 , 𝑒 2𝑡 , . . . , 𝑒𝑚
(1)
′ ,
where each edge event 𝑒𝑖𝑡 has two types: insertion or deletion, i.e,
𝑒𝑖𝑡 = (𝑢, 𝑣, event) where event ∈ {Insert, Delete} 3 .
The above model captures evolution of a real-world graph naturally where the structure evolution can be treated as a sequence
of edge events occurred in this graph. To simplify our analysis, we
assume that the graph is undirected. Based on this, we define the
subset dynamic representation problem as the following.
Definition 2 (Subset dynamic
 network embedding problem). Given
a dynamic network model G 0, G 1, G 2, . . . , G𝑇 define in Definition
1 and a subset of target nodes 𝑆 = {𝑣 1, 𝑣 2, . . . , 𝑣𝑘 }, the subset dynamic
network embedding problem is to learn dynamic embeddings of 𝑇
snapshots for all 𝑘 nodes 𝑆 where 𝑘 ≪ 𝑛. That is, given any node
𝑣 ∈ 𝑆, the goal is to learn embedding matrix for each node 𝑣 ∈ 𝑆, i.e.

3.2

(2)

Personalized PageRank

Given any node 𝑣 at time 𝑡, the personalized PageRank vector for
graph G𝑡 is defined as the following
Definition 3 (Personalized PageRank (PPR)). Given normalized
adjacency matrix 𝑾𝑡 = 𝑫𝑡−1 𝑨𝑡 where 𝑫𝑡 is a diagonal matrix with
𝐷𝑡 (𝑖, 𝑖) = 𝑑 (𝑖)𝑡 and 𝑨𝑡 is the adjacency matrix, the PageRank vector
3 The node insertion can be treated as inserting a new edge and then delete it and node

deletion is a sequence of deleting its edges.

where 1𝑠 is the unit vector with 1𝑠 (𝑣) = 1 when 𝑣 = 𝑠, 0 otherwise.
There are several works on computing PPVs for static graph
[1, 2, 5]. The idea is based a local push operation proposed in [2].
Interestingly, Zhang et al. [45] extends this idea and proposes a
novel updating strategy for calculating dynamic PPVs. We use a
modified version of it as presented in Algorithm 1.
Algorithm 1 ForwardPush [45]
1:
2:
3:
4:
5:

Dynamic graph model and its embedding

𝑾𝑣 := [𝒘 𝑣1, 𝒘 𝑣2, . . . , 𝒘𝑇𝑣 ] ⊤ where 𝒘 𝑣𝑡 ∈ R𝑑 and 𝑣 ∈ 𝑆.

𝝅𝑠𝑡 with respect to a source node 𝑠 is the solution of the following
equation
𝝅𝑠𝑡 = 𝛼 ∗ 1𝑠 + (1 − 𝛼)𝝅𝑠𝑡 𝑾 𝑡 ,
(3)

6:
7:
8:
9:
10:
11:

Input: 𝒑𝑠 , 𝒓𝑠 , G, 𝜖, 𝛽 = 0
while ∃𝑢, 𝑟𝑠 (𝑢) > 𝜖𝑑 (𝑢) do
Push(𝑢)
while ∃𝑢, 𝑟𝑠 (𝑢) < −𝜖𝑑 (𝑢) do
Push(𝑢)
return (𝒑𝑠 , 𝒓𝑠 )
procedure Push(𝑢)
𝑝𝑠 (𝑢) += 𝛼𝑟𝑠 (𝑢)
for 𝑣 ∈ Nei(𝑢) do
𝑟𝑠 (𝑣) += (1 − 𝛼)𝑟𝑠 (𝑢)(1 − 𝛽)/𝑑 (𝑢)
𝑟𝑠 (𝑢) = (1 − 𝛼)𝑟𝑠 (𝑢)𝛽

Algorithm 1 is a generalization from Andersen et al. [2] and there
are several variants of forward push [2, 5, 26], which are dependent
on how 𝛽 is chosen (we assume 𝛽 = 0). The essential idea of forward
push is that, at each Push step, the frontier node 𝑢 transforms her 𝛼
residual probability 𝑟𝑠 (𝑢) into estimation probability 𝑝𝑠 (𝑢) and then
pushes the rest residual to its neighbors. The algorithm repeats this
push operation until all residuals are small enough 4 . Methods based
on local push operations have the following invariant property.
Lemma 4 (Invariant property [19]). ForwardPush has the following invariant property
∑︁
𝜋𝑠 (𝑢) = 𝑝𝑠 (𝑢) +
𝑟𝑠 (𝑣)𝜋 𝑣 (𝑢), ∀𝑢 ∈ V.
(4)
𝑣 ∈𝑉

The local push algorithm can guarantee that the each entry of
the estimation vector 𝑝𝑠 (𝑣) is very close to the true value 𝜋𝑠 (𝑣).
We state this property as in the following
Lemma 5 ([2, 45]). Given any graph G(V, E) with 𝒑𝑠 = 0, 𝒓𝑠 = 1𝑠
and a constant 𝜖, the run time for ForwardLocalPush is at most
1− ∥𝒓𝑠 ∥ 1
and the estimation error of 𝜋𝑠 (𝑣) for each node 𝑣 is at most
𝛼𝜖
𝜖, i.e. |𝑝𝑠 (𝑣) − 𝜋𝑠 (𝑣)|/𝑑 (𝑣)| ≤ 𝜖
The main challenge to directly use forward push algorithm to
obtain high quality PPVs in our setting is that: 1) the quality 𝒑𝑠
return by forward push algorithm will critically depend on the
precision parameter 𝜖 which unfortunately is unknown under our
dynamic problem setting. Furthermore, the original update of peredge event strategy proposed in [45] is not suitable for batch update
4 There

are two implementation of forward push depends on how frontier is selected.
One is to use a first-in-first-out (FIFO) queue [11] to maintain the frontiers while the
other one maintains nodes using a priority queue is used [5] so that the operation cost
is O ( 1/𝜖𝛼) instead of O ( log 𝑛/𝜖𝛼) .

between graph snapshots. Guo et al. [13] propose to use a batch
strategy, which is more practical in real-world scenario where there
is a sequence of edge events between two consecutive snapshots.
This motivates us to develop a new algorithm for dynamic PPVs
and then use these PPVs to obtain high quality dynamic embedding
vectors.

4

PROPOSED METHOD

To obtain dynamic embedding vectors, the general idea is to obtain
dynamic PPVs and then project these PPVs into embedding space
by using two kernel functions [30, 42]. In this section, we present
our proposed method DynamicPPE where it contains two main
components: 1) an adaptive precision strategy to control the estimation error of PPVs. We then prove that the time complexity of
this dynamic strategy is still independent on 𝑛. With this quality
guarantee, learned PPVs will be used as proximity vectors and be
"projected" into lower dimensional space based on ideas of Verse
[39] and InstantEmbedding [30]. We first show how can we get high
quality PPVs and then present how use PPVs to obtain dynamic
embeddings. Finally, we give the complexity analysis.

4.1

Dynamic graph embedding for single batch

For each batch update Δ𝐺 𝑡 , the key idea is to dynamically maintain
PPVs where the algorithm updates the estimate from 𝒑𝑡𝑣−1 to 𝒑𝑡𝑣 and
its residuals from 𝒓 𝑣𝑡 −1 to 𝒓 𝑣𝑡 . Our method is inspired from Guo et al.
[13] where they proposed to update a personalized contribution
vector by using the local reverse push 5 . The proposed dynamic
single node embedding, DynamicSNE is illustrated in Algorithm 2.
It takes an update batch ΔG𝑡 (a sequence of edge events), a target
node 𝑠 with a precision 𝜖 𝑡 , estimation vector of 𝑠 and residual vector
as inputs. It then obtains an updated embedding vector of 𝑠 by the
following three steps: 1) It first updates the estimate vector 𝒑𝑠𝑡 and 𝒓𝑠𝑡
from Line 2 to Line 9; 2) It then calls the forward local push method
to obtain the updated estimations, 𝒑𝑠𝑡 ; 3) We then use the hash
kernel projection step to get an updated embedding. This projection
step is from InstantEmbedding where two universal hash functions
are defined as ℎ𝑑 : N → [𝑑] and ℎ sgn : N → {±1} 6 . Then the hash
kernel based on these two hash functions is defined as 𝐻ℎsgn ,ℎ𝑑 (𝒙) :
Í
R𝑛 → R𝑑 where each entity 𝑖 is 𝑗 ∈ℎ −1 (𝑖) 𝑥 𝑗 ℎ sgn ( 𝑗). Different
𝑑
from random projection used in RandNE [47] and FastRP [6], hash
functions has O (1) memory cost while random projection based
method has O (𝑑𝑛) if the Gaussian matrix is used. Furthermore,
hash kernel keeps unbiased estimator for the inner product [42].
In the rest of this section, we show that the time complexity
¯ in average and the estimation error of learned PPVs
is O (𝑚𝑑/𝜖)
measure by ∥ · ∥ 1 can also be bounded. Our proof is based on the
following lemma which follows from Guo et al. [13], Zhang et al.
[45].
Lemma 6. Given current graph G𝑡 and an update batch ΔG𝑡 , the
total run time of the dynamic single node embedding, DynamicSNE
5 One

should notice that, for undirected graph, PPVs can be calculated by using the
invariant property from the contribution vectors. However, the invariant property
does not hold for directed graph. It means that one cannot use reverse local push to get
a personalized PageRank vector directly. In this sense, using forward push algorithm
is more general for our problem setting.
6 For example, in our implementation, we use MurmurHash https://github.com/
aappleby/smhasher

for obtaining embedding 𝒘𝑠𝑡 is bounded by the following
𝑇𝑡 ≤

∑︁ 2 − 𝛼
∥𝒓𝑠𝑡 −1 ∥ 1 − ∥𝒓𝑠𝑡 ∥ 1
+
𝛼𝜖 𝑡
𝛼
𝑡
𝑢 ∈ΔG

𝑝𝑠𝑡 −1 (𝑢)
𝑑 (𝑢)

(5)

Proof. We first make an assumption that there is only one edge
update (𝑢, 𝑣, event) in ΔG𝑡 , then based Lemma 14 of [46], the run
time of per-edge update is at most:
∥𝒓𝑠𝑡 −1 ∥ 1 − ∥𝒓𝑠𝑡 −1 ∥ 1 Δ𝑡 (𝑠)
+
,
𝛼𝜖 𝑡
𝛼𝜖 𝑡

(6)

𝑝 𝑡 −1 (𝑢)

𝑠
where Δ𝑡 (𝑠) = 2−𝛼
𝛼
𝑑 (𝑢) . Suppose there are 𝑘 edge events in
𝑡
ΔG . We still obtain a similar bound, by the fact that forward push
algorithm has monotonicity property: the entries of estimates 𝑝𝑠𝑡 (𝑣)
only increase when it pushes positive residuals (Line 2 and 3 of Algorithm 1). Similarly, estimates 𝑝𝑠𝑡 (𝑣) only decrease when it pushes
negative residuals (Line 4 and 5 of Algorithm 1). In other words, the
amount of work for per-edge update is not less than the amount of
work for per-batch update. Similar idea is also used in [13].
□

Algorithm 2 DynamicSNE(G𝑡 , ΔG𝑡 , 𝑠, 𝒑𝑠𝑡 −1, 𝒓𝑠𝑡 −1, 𝜖 𝑡 , 𝛼)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

Input: graph G𝑡 , ΔG𝑡 , target node 𝑠, precision 𝜖, teleport 𝛼
for (𝑢, 𝑣, op) ∈ Δ𝐺 𝑡 do
if op == Insert(𝑢, 𝑣) then
Δ𝑝 = 𝑝𝑠𝑡 −1 (𝑢)/(𝑑 (𝑢)𝑡 − 1)
if op == Delete(𝑢, 𝑣) then
Δ𝑝 = −𝑝𝑠𝑡 −1 (𝑢)/(𝑑 (𝑢)𝑡 + 1)
𝑝𝑠𝑡 −1 (𝑢) ← 𝑝𝑠𝑡 −1 (𝑢) + Δ𝑝
𝑟𝑠𝑡 −1 (𝑢) ← 𝑟𝑠𝑡 −1 (𝑢) − Δ𝑝 /𝛼
𝑟𝑠𝑡 −1 (𝑣) ← 𝑟𝑠𝑡 −1 (𝑣) + Δ𝑝 /𝛼 − Δ𝑝
𝒑𝑠𝑡 = ForwardPush(𝒑𝑠𝑡 −1, 𝒓𝑠𝑡 −1, G𝑡 , 𝜖 𝑡 , 𝛼)
𝒘𝑠𝑡 = 0
for 𝑖 ∈ {𝑣 : 𝑝𝑠𝑡 (𝑣) ≠ 0, 𝑣 ∈ V𝑡 } do
 
𝑤𝑠𝑡 (ℎ𝑑 (𝑖)) += ℎ sgn (𝑖) max log 𝑝𝑠𝑡 (𝑖)𝑛𝑡 , 0

Theorem 7. Given any graph snapshot G𝑡 and an update batch Δ𝐺 𝑡
where there are 𝑚𝑡 edge events and suppose the precision parameter
is 𝜖 𝑡 and teleport probability is 𝛼, DynamicSNE runs in O (𝑚𝑡 /𝛼 2 +
𝑚𝑡 𝑑¯𝑡 /(𝜖𝛼 2 ) + 𝑚𝑡 /(𝜖𝛼)) with 𝜖 𝑡 = 𝜖/𝑚𝑡
Proof. Based lemma 6, the proof directly follows from Theorem
12 of [46].
□
The above theorem has an important difference from the previous one [45]. We require that the precision parameter will be
small enough so that ∥𝒑𝑠𝑡 − 𝝅𝑠𝑡 ∥ 1 can be bound ( will be discussed
later). As we did the experiments in Figure 2, the fixed epsilon will
make the embedding vector bad. We propose to use a dynamic
precision parameter, where 𝜖 𝑡 ∼ O (𝜖 ′ /𝑚𝑡 ), so that the ℓ1 -norm can
be properly bounded. Interestingly, this high precision parameter
strategy will not case too high time complexity cost from O (𝑚/𝛼 2 )
2 )). The bounded estimation error is presented in the
¯
to O (𝑚𝑑/(𝜖𝛼
following theorem.

Theorem 8 (Estimation error). Given any node 𝑠, define the estimation error of PPVs learned from DynamicSNE at time 𝑡 as ∥𝒑𝑠𝑡 − 𝝅𝑠𝑡 ∥ 1 ,
if we are given the precision parameter 𝜖 𝑡 = 𝜖/𝑚𝑡 , the estimation
error can be bounded by the following
∥𝒑𝑠𝑡 − 𝝅𝑠𝑡 ∥ 1 ≤ 𝜖,

(7)

where we require 𝜖 ≤ 2 7 and 𝜖 is a global precision parameter of
DynamicPPE independent on 𝑚𝑡 and 𝑛𝑡 .
Proof. Notice that for any node 𝑢, by Lemma 5, we have the
following inequality
|𝜋𝑠𝑡 (𝑢) − 𝜋𝑠𝑡 (𝑢)| ≤ 𝜖𝑑 𝑡 (𝑢).

2:
3:

𝑢 ∈V𝑡

4:

𝜖
𝜖 𝑡 𝑑 𝑡 (𝑢) = 𝜖 𝑡 𝑚𝑡 =
𝑚𝑡 = 𝜖.
𝑚
𝑡
𝑡

∑︁

5:

𝑢 ∈V

□

6:
7:
8:
9:

0.6

10:

0.7

DynamicPPE

Algorithm 3 DynamicPPE(G0, 𝑆, 𝜖, 𝛼)
1:

Summing all these inequalities over all nodes 𝑢, we have
∑︁
∥𝒑𝑠𝑡 − 𝝅𝑠𝑡 ∥ 1 =
𝑝𝑠𝑡 (𝑢) − 𝜋𝑠𝑡 (𝑢)
≤

4.2

Our finally algorithm DynamicPPE is presented in Algorithm 3.
At every beginning, estimators 𝒑𝑠𝑡 are set to be zero vectors and
residual vectors 𝒓𝑠𝑡 are set to be unit vectors with mass all on one
entry (Line 4). The algorithm then call the procedure DynamicSNE
with an empty batch as input to get initial PPVs for all target nodes
8 (Line 5). From Line 6 to Line 9, at each snapshot 𝑡, it gets an
update batch ΔG𝑡 at Line 7 and then calls DynamicSNE to obtain
the updated embeddings for every node 𝑣.

Input: initial graph G 0 , target set 𝑆, global precision 𝜖, teleport
probability 𝛼
𝑡 =0
for 𝑠 ∈ 𝑆 := {𝑣 1, 𝑣 2, . . . , 𝑣𝑘 } do
𝒑𝑠𝑡 = 0, 𝒓𝑠𝑡 = 1𝑠
DynamicSNE(G 0, ∅, 𝑠, 𝒑𝑠𝑡 , 𝒓𝑠𝑡 , 1/𝑚 0, 𝛼)
for 𝑡 ∈ {1, 2, . . . ,𝑇 } do
read a sequence of edge events ΔG𝑡 := G𝑡 \G𝑡 −1
for 𝑠 ∈ 𝑆 := {𝑣 1, 𝑣 2, . . . , 𝑣𝑘 } do
𝒘𝑠𝑡 = DynamicSNE(G𝑡 , ΔG𝑡 , 𝑠, 𝒑𝑠𝑡 −1, 𝒓𝑠𝑡 −1, 𝜖/𝑚𝑡 , 𝛼)
return 𝑾𝑠𝑡 = [𝒘𝑠1, 𝒘𝑠2, . . . , 𝒘𝑠𝑇 ], ∀𝑠 ∈ 𝑆.

0.5

0.3
0.2

 = 10−6
 = 10−7
 = 10−8
 = 10−9
01 03 05 07 09 11 13 15 17 19
Year

Micro-F1

Macro-F1

0.6
0.4

0.5
0.4
0.3

 = 10−6
 = 10−7
 = 10−8
 = 10−9
01 03 05 07 09 11 13 15 17 19
Year

Figure 2: 𝜖 as a function of year for the task of node classification on the English Wikipedia graph. Each line corresponds to a fixed precision strategy of DynamicSNE. Clearly,
when the precision parameter 𝜖 decreases, the performance
of node classification improves.
The above theorem gives estimation error guarantee of 𝒑𝑠𝑡 , which
is critically important for learning high quality embeddings. First
of all, the dynamic precision strategy is inevitable because the precision is unknown parameter for dynamic graph where the number of nodes and edges could increase dramatically over time. To
demonstrate this issue, we conduct an experiments on the English
Wikipedia graph where we learn embeddings over years and validate these embeddings by using node classification task. As shown
in Figure 2, when we use the fixed parameter, the performance
of node classification is getting worse when the graph is getting
bigger. This is mainly due to the lower quality of PPVs. Fortunately,
the adaptive precision parameter 𝜖/𝑚𝑡 does not make the run time
increase dramatically. It only dependents on the average degree 𝑑¯𝑡 .
In practice, we found 𝜖 = 0.1 are sufficient for learning effective
embeddings.
7 By

noticing that ∥𝒑𝑠𝑡 − 𝝅𝑠𝑡 ∥ 1 ≤ ∥𝒑𝑠𝑡 ∥ 1 + ∥𝝅𝑠𝑡 ∥ 1 ≤ 2, any precision parameter
larger than 2 will be meaningless.

Based on our analysis, DynamicPPE is an dynamic version of
InstantEmbedding. Therefore, DynamicPPE has two key properties
observed in [30]: locality and global consistency. The embedding
quality is guaranteed from the fact that InstantEmbedding implicitly
factorizes the proximity matrix based on PPVs [39].

4.3

Complexity analysis

Time complexity The overall time complexity of DynamicPPE
is the 𝑘 times of the run time of DynamicSNE. We summarize the
time complexity of DynamicPPE as in the following theorem
Theorem 9. The time complexity of DynamicPPE for learning a
¯𝑡
𝑚 })
𝑡
subset of 𝑘 nodes is O (𝑘 𝑚
+ 𝑘 𝑚𝛼𝑡 2𝑑 + 𝑚𝜖𝑡 + 𝑘𝑇 min{𝑛, 𝜖𝛼
𝛼2
Proof. We follow Theorem 7 and summarize all run time together to get the final time complexity.
□
Space complexity The overall space complexity has two parts:
1) O (𝑚) to store the graph structure information; and 2) the storage
of keeping nonzeros of 𝒑𝑠𝑡 and 𝒓𝑠𝑡 . From the fact that local push oper1
ation [2], the number of nonzeros in 𝒑𝑠𝑡 is at most 𝜖𝛼
. Thus, the total
𝑚 }). Therefore,
storage for saving these vectors are O (𝑘 min{𝑛, 𝜖𝛼
𝑚 )).
the total space complexity is O (𝑚 + 𝑘 min(𝑛, 𝜖𝛼
Implementation Since learning the dynamic node embedding
for any node 𝑣 is independent with each other, DynamicPPE is are
easy to parallel. Our current implementation can take advantage of
multi-cores and compute the embeddings of 𝑆 in parallel.
8 For

the situation that some nodes of 𝑆 has not appeared in G𝑡 yet, it checks every
batch update until all nodes are initialized.

5

EXPERIMENTS

To demonstrate the effectiveness and efficiency of DynamicPPE,
in this section, we first conduct experiments on several small and
large scale real-world dynamic graphs on the task of node classification, followed by a case study about changes of Chinese cities in
Wikipedia graph during the COVID-19 pandemic.

5.1

Datasets

We compile the following three real-world dynamic graphs, more
details can be found in Appendix C.
Enwiki20 English Wikipedia Network We collect the internal
Wikipedia Links (WikiLinks) of English Wikipedia from the beginning of Wikipedia, January 11th, 2001, to December 31, 2020 9 . The
internal links are extracted using a regular expression proposed in
[8]. During the entire period, we collection 6,151,779 valid articles10 .
We generated the WikiLink graphs only containing edge insertion
events. We keep all the edges existing before Dec. 31 2020, and sort
the edge insertion order by the creation time. There are 6,216,199
nodes and 177,862,656 edges during the entire period. Each node
either has one label (Food, Person, Place,...) or no label.
Patent (US Patent Graph) The citation network of US patent[14]
contains 2,738,011 nodes with 13,960,811 citations range from year
1963 to 1999. For any two patents 𝑢 and 𝑣, there is an edge (𝑢, 𝑣)
if the patent 𝑢 cites patent 𝑣. Each patent belongs to six different
types of patent categories. We extract a small weakly-connected
component of 46,753 nodes and 425,732 edges with timestamp,
called Patent-small.
Coauthor We extracted the co-authorship network from the
Microsoft Academic Graph [35] dumped on September 21, 2019. We
collect the papers with less than six coauthors, keeping those who
has more than 10 publications, then build undirected edges between
each coauthor with a timestamp of the publication date. In addition,
we gather temporal label (e.g.: Computer Science, Art, ... ) of authors
based on their publication’s field of study. Specifically we assign
the label of an author to be the field of study where s/he published
the most up to that date. The original graph contains 4,894,639
authors and 26,894,397 edges ranging from year 1800 to 2019. We
also sampled a small connected component containing 49,767 nodes
and 755,446 edges with timestamp, called Coauthor-small.
Academic The co-authorship network is from the academic network [36, 49] where it contains 51,060 nodes and 794,552 edges.
The nodes are generated from 1980 to 2015. According to the node
classification setting in Zhou et al. [49], each node has either one
binary label or no label.

5.2

Node Classification Task

Experimental settings We evaluate embedding quality on binary classification for Academic graph (as same as in [49]), while
using multi-class classification for other tasks. We use balanced
logistic regression with ℓ2 penalty in on-vs-rest setting, and report
the Macro-F1 and Macro-AUC (ROC) from 5 repeated trials with
10% training ratio on the labeled nodes in each snapshot, excluding
dangling nodes. Between each snapshot, we insert new edges and
keep the previous edges.
9 We

collect the data from the dump https://dumps.wikimedia.org/enwiki/20210101/
10 A valid Wikipedia article must be in the 0 namespace

We conduct the experiments on the aforementioned small and
large scale graph. In small scale graphs, we calculate the embeddings of all nodes and compare our proposed method (DynPPE.)
against other state-of-the-art models from three categories11 . 1)
Random walk based static graph embeddings (Deepwalk12 [29],
Node2Vec13 [12]); 2) Random Projection-based embedding method
which supports online update: RandNE14 [47]; 3) Dynamic graph
embedding method: DynamicTriad (DynTri.) 15 [49] which models the dynamics of the graph and optimized on link prediction.
Table 1: Node classification task on the Academic, Patent
Small, Coauthor Small graph on the final snapshot

Static
method

Node2Vec
Deepwalk
DynTri.
Dynamic RandNE
method DynPPE.
Static
method

Node2Vec
Deepwalk
DynTri.
Dynamic RandNE
method DynPPE.

Academic
Patent Small
F1
AUC F1
AUC
𝑑 = 128
0.833 0.975 0.648 0.917
0.834 0.975 0.650 0.919
0.817 0.969 0.560 0.866
0.587 0.867 0.428 0.756
0.808 0.962 0.630 0.911
𝑑 = 512
0.841 0.975 0.677 0.931
0.842 0.975 0.680 0.931
0.811 0.965 0.659 0.915
0.722 0.942 0.560 0.858
0.842 0.973 0.682 0.934

Coauthor Small
F1
AUC
0.477
0.476
0.435
0.337
0.448

0.955
0.950
0.943
0.830
0.951

0.486
0.495
0.492
0.493
0.509

0.955
0.955
0.952
0.895
0.958

Table 2: Total CPU time for small graphs (in second).
RandNE-I is with orthogonal projection, the performance
is slightly better, but the running time is significantly increased. RandNE-II is without orthogonal projection.

Deepwalk16
Node2vec
DynTri.
RandNE-I
RandNE-II
DynPPE.

Academic
498211.75
4584618.79
247237.55
12732.64
1583.08
18419.10

Patent Small
181865.56
2031090.75
117993.36
9637.15
9208.03
3651.59

Coauthor Small
211684.86
1660984.42
108279.4
8436.79
177.89
21323.74

Table 1 shows the classification results on the final snapshot.
When we restrict the dimension to be 128, we observe that static
methods outperform all dynamic methods. However, the static
methods independently model each snapshot and the running time
grows with number of snapshots, regardless of the number of edge
changes. In addition, our proposed method (DynPPE.) outperforms
other dynamic baselines, except that in the academic graph, where
DynTri. is slightly better. However, their CPU time is 13 times
11 Appendix

D shows the hyper-parameter settings of baseline methods

12 https://pypi.org/project/deepwalk/

13 https://github.com/aditya-grover/node2vec

14 https://github.com/ZW-ZHANG/RandNE/tree/master/Python
15 https://github.com/luckiezhou/DynamicTriad

16 We ran static graph embedding methods over a set of sampled snapshots and estimate

the total CPU time for all snapshots.

0.6

0.4
1980

1980 1985 1990 1995 2000 2005 2010 2015

1985

1990

1995

Coauthor-small

0.50
0.45
0.40
0.35
0.30
0.25

2000

DynamicPPE
DynamicTriad
RandNE
Node2Vec
DeepWalk
1985 1990 1995 2000 2005 2010 2015

Patent
DynamicPPE
Commute
RandNE

0.3

Coauthor

0.6

Macro-F1

Macro-F1

EnWiki20
0.4

DynamicPPE
DynamicTriad
RandNE
Node2Vec
DeepWalk

0.5

0.5

DynamicPPE
Commute
RandNE

0.4
0.3
0.2

0.2
2006 2008 2010 2012 2014 2016 2018 2020

DynamicPPE
Commute
RandNE

0.30

Macro-F1

0.4

Macro-F1

Macro-F1

DynamicPPE
DynamicTriad
RandNE
Node2Vec
DeepWalk

0.6

Patent-small

0.7

Macro-F1

Academic
0.8

0.25
0.20
0.15
0.10

1986 1988 1990 1992 1994 1996 1998

1990

1995

2000

2005

2010

2015

2020

Figure 3: Macro-F1 scores of node classification as a function of time. The results of the small and large graphs are on the first
and second row respectively (dim=512). Our proposed methods achieves the best performance on the last snapshot when all
edges arrived and performance curve matches to the static methods as the graph becomes more and more complete.
more than ours as shown in Table 2. According to the JohnsonLindenstrauss lemma[9, 21], we suspect that the poor result of
RandNE is partially caused by the dimension size. As we increase
the dimension to 512, we see a great performance improvement
of RandNE. Also, our proposed method takes the same advantage,
and outperform all others baselines in F1-score. Specifically, the
increase of dimension makes hash projection step (Line 12-13 in
Algorithm 2) retain more information and yield better embeddings.
We attach the visualization of embeddings in Appendix.E.
The first row in the Figure 3 shows the F1-scores in each snapshot when the dimension is 512. We observe that in the earlier
snapshots where many edges are not arrived, the performance of
DynamicTriad [49] is better. One possible reason is that it models
the dynamics of the graph, thus the node feature is more robust
to the "missing" of the future links. While other methods, including ours, focus on an online feature updates incurred by the edge
changes without explicitly modeling the temporal dynamics of a
graph. Meanwhile, the performance curve of our method matches
to the static methods, demonstrating the embedding quality of the
intermediate snapshots is comparable to the state-of-the-art.
Table 2 shows the CPU time of each method (Dim=512). As
we expected, static methods is very expensive as they calculate
embeddings for each snapshot individually. Although our method
is not blazingly fast compared to RandNE, it has a good trade-off
between running time and embedding quality, especially without
much hyper-parameter tuning. Most importantly, it can be easily
parallelized by distributing the calculation of each node to clusters.
We also conduct experiment on large scale graphs (EnWiki20,
Patent, Coauthor). We keep track of the vertices in a subset containing |𝑆 | = 3, 000 nodes randomly selected from the first snapshot in
each dataset, and similarly evaluate the quality of the embeddings
of each snapshot on the node classification task. Due to scale of the
graph, we compare our method against RandNE [47] and an fast
heuristic Algorithm 4. Our method can calculate the embeddings
for a subset of useful nodes only, while other methods have to calculate the embeddings of all nodes, which is not necessary under

our scenario detailed in Sec. 5.3. The second row in Figure 3 shows
that our proposed method has the best performance.
Table 3: Total CPU time for large graphs (in second)

Commute
RandNE-II
DynPPE.
DynPPE (Per-node)

Enwiki20
6702.1
47992.81
1538215.88
512.73

Patent
639.94
6524.04
139222.01
46.407

Coauthor
1340.74
20771.19
411708.9
137.236

Table 3 shows the total CPU time of each method (Dim=512).
Although total CPU time of our proposed method seems to be the
greatest, the average CPU time for one node (as shown in row 4) is
significantly smaller. This benefits a lot of downstream applications
where only a subset of nodes are interesting to people in a large
dynamic graph. For example, if we want to monitor the weekly
embedding changes of a single node (e.g., the city of Wuhan, China)
in English Wikipedia network from year 2020 to 2021, we can
have the results in roughly 8.5 minutes. Meanwhile, other baselines
have to calculate the embeddings of all nodes, and this expensive
calculation may become the bottleneck in a downstream application
with real-time constraints. To demonstrate the usefulness of our
method, we conduct a case study in the following subsection.

5.3

Change Detection

Thanks to the contributors timely maintaining Wikipedia, we believe that the evolution of the Wikipedia graph reflects how the
real world is going on. We assume that when the structure of a
node greatly changes, there must be underlying interesting events
or anomalies. Since the structural changes can be well-captured
by graph embedding, we use our proposed method to investigate whether anomalies happened to the Chinese cities during
the COVID-19 pandemic (from Jan. 2020 to Dec. 2020).
Changes of major Chinese Cities We target nine Chinese major cities (Shanghai, Guangzhou, Nanjing, Beijing, Tianjin, Wuhan,
Shenzhen, Chengdu, Chongqing) and keep track of the embeddings

Table 4: The top cities ranked by the z-score along time. The corresponding news titles are from the news in each time period.
𝑑 (𝑣) is node degree, Δ𝑑 (𝑣) is the degree changes from the previous timestamp.
Date
1/22/20
2/2/20
2/13/20

City
Wuhan
Wuhan
Hohhot

𝑑 (𝑣)
2890
2937
631

Δ𝑑 (𝑣)
54
47
20

Z-score
2.210
1.928
1.370

2/24/20
3/6/20
3/17/20
3/28/20
4/8/20
4/19/20
4/30/20
5/11/20

Wuhan
Wuhan
Wuhan
Zhangjiakou
Wuhan
Luohe
Zunhua
Shulan

3012
3095
3173
517
3314
106
52
88

38
83
78
15
47
15
17
46

2.063
1.723
1.690
1.217
2.118
2.640
2.449
2.449

Top News Title
NBC: "New virus prompts U.S. to screen passengers from Wuhan, China"
WSJ: "U.S. Sets Evacuation Plan From Coronavirus-Infected Wuhan"
Poeple.cn: "26 people in Hohhot were notified of dereliction of duty for prevention and control, and the
director of the Health Commission was removed" (Translated from Chinese).
USA Today: "Coronavirus 20 times more lethal than the flu? Death toll passes 2,000"
Reuters: "Infelctions may drop to zero by end-March in Wuhan: Chinese government expert"
NYT: "Politicians Use of ’Wuhan Virus’ Starts a Debate Health Experets Wanted to Avoid"
"Logo revealed for Freestyle Ski and Snowboard World Championships in Zhangjiakou"
CNN:"China lifts 76-day lockdown on Wuhan as city reemerges from conronavirus crisis"
Forbes: "The Chinese Billionaire Whose Company Owns Troubled Pork Processor Smithfield Foods"
XINHUA: "Export companies resume production in Zunhua, Hebei"
CGTN: "NE China’s Shulan City to reimpose community lockdown in ’wartime’ battle against COVID-19"

Degree Changes

in a 10-day time window. From our prior knowledge, we expect
that Wuhan should greatly change since it is the first reported place
of the COVID-19 outbreak in early Jan. 2020.
Wuhan
Chengdu

80

from 𝑡 − 1 to 𝑡.
𝐷𝑖𝑠𝑡 (𝑤𝑢𝑡 , 𝑤𝑢𝑡 −1 ) − 𝜇
𝜎
√︄
∑︁
1 ∑︁
1
𝜇=
𝐷𝑖𝑠𝑡 (𝑤𝑢𝑡 ′ , 𝑤𝑢𝑡 −′ 1 ), 𝜎 =
(𝐷𝑖𝑠𝑡 (𝑤𝑢𝑡 ′ 𝑤𝑢𝑡 −′ 1 ) − 𝜇) 2
|𝑆 | ′
|𝑆 | ′
𝑍𝑡 (𝑢 |𝑢 ∈ 𝑆) =

𝑢 ∈𝑆

𝑢 ∈𝑆

60
40
20
0

/20 /20 /20 /20 /20 /20 /20
1/11 3/6 4/30 6/24 8/18 10/12 12/6

Figure 4: The changes of major Chinese cities in 2020.
Left:Changes in Node Degree. Right: Changes in Cosine distance
Figure.4(a) shows the node degree changes of each city every 10
days. The curve is quite noisy, but we can still see several major
peaks from Wuhan around 3/6/20 and Chengdu around 8/18/20.
When using embedding changes 17 as the measurement, Figure.4
(b) provides a more clear view of the changed cities. We observe
strong signals from the curve of Wuhan, correlating to the initial
COVID-19 outbreak and the declaration of pandemic 18 . In addition,
we observed an peak from the curve of Chengdu around 8/18/20
when U.S. closed the consulate in Chengdu, China, reflecting the
U.S.-China diplomatic tension19 .
Top changed city along time We keep track of the embedding
movement of 533 Chinese cities from year 2020 to 2021 in a 10day time window, and filter out the inactive records by setting the
threshold of degree changes (e.g. greater than 10). The final results
are 51 Chinese cities from the major ones listed above and less
famous cities like Zhangjiakou, Hohhot, Shulan, ....
Furthermore, we define the z-score of a target node 𝑢 as 𝑍𝑡 (𝑢)
based on the embedding changes within a specific period of time
17 Again,

the embedding movement Dist ( ·, ·) is defined as 1 − cos ( ·, ·)
18 COIVD-19: https://www.who.int/news/item/27-04-2020-who-timeline---covid-19
19 US Consulate: https://china.usembassy-china.org.cn/embassy-consulates/chengdu/

In Table 4, we list the highest ranked cities by the z-score from
Jan.22 to May 11, 2020. In addition, we also attach the top news
titles corresponding to the city within each specific time period.
We found that Wuhan generally appears more frequently as the
situation of the pandemic kept changing. Meanwhile, we found the
appearance of Hohhot and Shulan reflects the time when COVID-19
outbreak happened in those cities. We also discovered cities unrelated to the pandemic. For example, Luohe, on 4/19/20, turns out
to be the city where the headquarter of the organization which
acquired Smithfield Foods (as mentioned in the news). In addition,
Zhangjiakou, on 3/28/20, is the city, which will host World Snowboard competition, released the Logo of that competition.

6

DISCUSSION AND CONCLUSION

In this paper, we propose a new method to learn dynamic node
embeddings over large-scale dynamic networks for a subset of
interesting nodes. Our proposed method has time complexity that
is linear-dependent on the number of edges 𝑚 but independent
on the number of nodes 𝑛. This makes our method applicable to
applications of subset representations on very large-scale dynamic
graphs. For the future work, as shown in Trivedi et al. [38], there
are two dynamics on dynamic graph data, structure evolution and
dynamics of node interaction. It would be interesting to study how
one can incorporate dynamic of node interaction into our model. It
is also worth to study how different version of local push operation
affect the performance of our method.

ACKNOWLEDGMENTS
This work was partially supported by NSF grants IIS-1926781, IIS1927227, IIS-1546113 and OAC-1919752.

REFERENCES
[1] Reid Andersen, Christian Borgs, Jennifer Chayes, John Hopcraft, Vahab S Mirrokni, and Shang-Hua Teng. 2007. Local computation of PageRank contributions.
In International Workshop on Algorithms and Models for the Web-Graph. Springer,
150–165.
[2] Reid Andersen, Fan Chung, and Kevin Lang. 2006. Local graph partitioning
using pagerank vectors. In 2006 47th Annual IEEE Symposium on Foundations of
Computer Science (FOCS’06). IEEE, 475–486.
[3] Reid Andersen, Fan Chung, and Kevin Lang. 2007. Using pagerank to locally
partition a graph. Internet Mathematics 4, 1 (2007), 35–64.
[4] Tanya Y Berger-Wolf and Jared Saia. 2006. A framework for analysis of dynamic
social networks. In Proceedings of the 12th ACM SIGKDD international conference
on Knowledge discovery and data mining. 523–528.
[5] Pavel Berkhin. 2006. Bookmark-coloring algorithm for personalized pagerank
computing. Internet Mathematics 3, 1 (2006), 41–62.
[6] Haochen Chen, Syed Fahad Sultan, Yingtao Tian, Muhao Chen, and Steven
Skiena. 2019. Fast and Accurate Network Embeddings via Very Sparse Random
Projection. In Proceedings of the 28th ACM International Conference on Information
and Knowledge Management. 399–408.
[7] Colin B Clement, Matthew Bierbaum, Kevin P O’Keeffe, and Alexander A Alemi.
2019. On the Use of ArXiv as a Dataset. arXiv preprint arXiv:1905.00075 (2019).
[8] Cristian Consonni, David Laniado, and Alberto Montresor. 2019. WikiLinkGraphs:
A complete, longitudinal and multi-language dataset of the Wikipedia link networks. In Proceedings of the International AAAI Conference on Web and Social
Media, Vol. 13. 598–607.
[9] Sanjoy Dasgupta and Anupam Gupta. 1999. An elementary proof of the JohnsonLindenstrauss lemma. International Computer Science Institute, Technical Report
22, 1 (1999), 1–5.
[10] Lun Du, Yun Wang, Guojie Song, Zhicong Lu, and Junshan Wang. 2018. Dynamic
Network Embedding: An Extended Approach for Skip-gram based Network
Embedding.. In IJCAI, Vol. 2018. 2086–2092.
[11] David F Gleich. 2015. PageRank beyond the Web. Siam Review 57, 3 (2015),
321–363.
[12] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD international conference on
Knowledge discovery and data mining. 855–864.
[13] Wentian Guo, Yuchen Li, Mo Sha, and Kian-Lee Tan. 2017. Parallel personalized
pagerank on dynamic graphs. Proceedings of the VLDB Endowment 11, 1 (2017),
93–106.
[14] Bronwyn H Hall, Adam B Jaffe, and Manuel Trajtenberg. 2001. The NBER patent
citation data file: Lessons, insights and methodological tools. Technical Report.
National Bureau of Economic Research.
[15] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation
learning on large graphs. In Advances in neural information processing systems.
1024–1034.
[16] William L Hamilton, Rex Ying, and Jure Leskovec. 2017. Representation learning
on graphs: Methods and applications. arXiv preprint arXiv:1709.05584 (2017).
[17] Peter Hoff. 2007. Modeling homophily and stochastic equivalence in symmetric
relational data. Advances in neural information processing systems 20 (2007),
657–664.
[18] Peter D Hoff, Adrian E Raftery, and Mark S Handcock. 2002. Latent space approaches to social network analysis. Journal of the american Statistical association
97, 460 (2002), 1090–1098.
[19] Sungryong Hong, Bruno C Coutinho, Arjun Dey, Albert-L Barabási, Mark Vogelsberger, Lars Hernquist, and Karl Gebhardt. 2016. Discriminating topology
in galaxy distributions using network analysis. Monthly Notices of the Royal
Astronomical Society 459, 3 (2016), 2690–2700.
[20] Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Knowledge
graph embedding via dynamic mapping matrix. In Proceedings of the 53rd annual
meeting of the association for computational linguistics and the 7th international
joint conference on natural language processing (volume 1: Long papers). 687–696.
[21] William B Johnson and Joram Lindenstrauss. 1984. Extensions of Lipschitz
mappings into a Hilbert space. Contemporary mathematics 26, 189-206 (1984), 1.
[22] Seyed Mehran Kazemi and Rishab Goel. 2020. Representation Learning for
Dynamic Graphs: A Survey. Journal of Machine Learning Research 21, 70 (2020),
1–73.
[23] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In International Conference on Learning Representations (ICLR).
[24] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic embedding trajectory in temporal interaction networks. In Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining.
1269–1278.
[25] TG Lei, CW Woo, JZ Liu, and F Zhang. 2003. On the Schur complements of
diagonally dominant matrices.
[26] Peter Lofgren. 2015. Efficient Algorithms for Personalized PageRank. Ph.D. Dissertation. Stanford University.

[27] Peter Lofgren, Siddhartha Banerjee, and Ashish Goel. 2015. Bidirectional PageRank Estimation: From Average-Case to Worst-Case. In Proceedings of the 12th
International Workshop on Algorithms and Models for the Web Graph-Volume 9479.
164–176.
[28] Giang Hoang Nguyen, John Boaz Lee, Ryan A Rossi, Nesreen K Ahmed, Eunyee
Koh, and Sungchul Kim. 2018. Continuous-time dynamic network embeddings.
In Companion Proceedings of the The Web Conference 2018. 969–976.
[29] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning
of social representations. In Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining. 701–710.
[30] Ştefan Postăvaru, Anton Tsitsulin, Filipe Miguel Gonçalves de Almeida, Yingtao
Tian, Silvio Lattanzi, and Bryan Perozzi. 2020. InstantEmbedding: Efficient Local
Node Representations. arXiv preprint arXiv:2010.06992 (2020).
[31] Ryan A Rossi, Brian Gallagher, Jennifer Neville, and Keith Henderson. 2013.
Modeling dynamic behavior in large evolving graphs. In Proceedings of the sixth
ACM international conference on Web search and data mining. 667–676.
[32] Purnamrita Sarkar and Andrew W Moore. 2005. Dynamic social network analysis
using latent space models. Acm Sigkdd Explorations Newsletter 7, 2 (2005), 31–40.
[33] Purnamrita Sarkar and Andrew W Moore. 2005. Dynamic social network analysis
using latent space models. In Proceedings of the 18th International Conference on
Neural Information Processing Systems. 1145–1152.
[34] Purnamrita Sarkar, Sajid M Siddiqi, and Geogrey J Gordon. 2007. A latent space
approach to dynamic embedding of co-occurrence data. In Artificial Intelligence
and Statistics. 420–427.
[35] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and
Kuansan Wang. 2015. An overview of microsoft academic service (mas) and
applications. In Proceedings of the 24th international conference on world wide web.
243–246.
[36] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In Proceedings of
the 14th ACM SIGKDD international conference on Knowledge discovery and data
mining. 990–998.
[37] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha.
2019. DyRep: Learning Representations over Dynamic Graphs. In International
Conference on Learning Representations.
[38] Rakshit Trivedi, Mehrdad Farajtabar, Prasenjeet Biswal, and Hongyuan Zha.
2019. DyRep: Learning Representations over Dynamic Graphs. In International
Conference on Learning Representations.
[39] Anton Tsitsulin, Davide Mottin, Panagiotis Karras, and Emmanuel Müller. 2018.
Verse: Versatile graph embeddings from similarity measures. In Proceedings of
the 2018 World Wide Web Conference. 539–548.
[40] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, 11 (2008).
[41] Ulrike Von Luxburg, Agnes Radl, and Matthias Hein. 2014. Hitting and commute
times in large random neighborhood graphs. The Journal of Machine Learning
Research 15, 1 (2014), 1751–1798.
[42] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh
Attenberg. 2009. Feature hashing for large scale multitask learning. In Proceedings
of the 26th annual international conference on machine learning. 1113–1120.
[43] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,
and Jure Leskovec. 2018. Graph convolutional neural networks for web-scale
recommender systems. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining. 974–983.
[44] Chengxi Zang and Fei Wang. 2020. Neural dynamics on complex networks.
In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining. 892–902.
[45] Hongyang Zhang, Peter Lofgren, and Ashish Goel. 2016. Approximate personalized pagerank on dynamic graphs. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 1315–1324.
[46] Hongyang Zhang, Peter Lofgren, and Ashish Goel. 2016. Approximate Personalized PageRank on Dynamic Graphs. arXiv preprint arXiv:1603.07796 (2016).
[47] Ziwei Zhang, Peng Cui, Haoyang Li, Xiao Wang, and Wenwu Zhu. 2018. Billionscale network embedding with iterative random projection. In 2018 IEEE International Conference on Data Mining (ICDM). IEEE, 787–796.
[48] Ziwei Zhang, Peng Cui, Jian Pei, Xiao Wang, and Wenwu Zhu. 2018. TIMERS:
Error-Bounded SVD Restart on Dynamic Networks. In Proceedings of the 32nd
AAAI Conference on Artificial Intelligence. AAAI.
[49] Lekui Zhou, Yang Yang, Xiang Ren, Fei Wu, and Yueting Zhuang. 2018. Dynamic
network embedding by modeling triadic closure process. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 32.
[50] Linhong Zhu, Dong Guo, Junming Yin, Greg Ver Steeg, and Aram Galstyan. 2016.
Scalable temporal latent space inference for link prediction in dynamic social
networks. IEEE Transactions on Knowledge and Data Engineering 28, 10 (2016),
2765–2777.

A

B

PROOF OF LEMMAS

To prove Lemma 4, we first introduce the property of uniqueness
of PPR 𝝅𝑠 for any 𝑠.
Proposition 10 (Uniqueness of PPR [3]). For any starting vector
1𝑠 , and any constant 𝛼 ∈ (0, 1], there is a unique vector 𝝅𝑠 satisfying
(3).
Proof. Recall the PPR equation
𝝅𝑠 = 𝛼1𝑠 + (1 − 𝛼)𝑫 −1 𝑨𝝅𝑠 .
We can rewrite it as (𝑰 − (1−𝛼)𝑫 −1 𝑨)𝝅𝑠 = 𝛼1𝑠 . Notice the fact that
matrix 𝑰 − (1 − 𝛼)𝑫 −1 𝑨 is strictly diagonally dominant matrix. To
Í
see this, for each 𝑖 ∈ V, we have 1−(1−𝛼) 𝑗≠𝑖 |1/𝑑 (𝑖)| = 𝛼 > 0. By
[25], strictly diagonally dominant matrix is always invertible. □
Proposition 11 (Symmetry property [27]). Given any undirected
graph G, for any 𝛼 ∈ (0, 1) and for any node pair (𝑢, 𝑣), we have
𝑑 (𝑢)𝜋𝑢 (𝑣) = 𝑑 (𝑣)𝜋 𝑣 (𝑢).

(8)

Algorithm 4 Commute
1:
2:
3:
4:
5:
6:
7:
8:

Proof of Lemma 7. Assume there are 𝑇 iterations. For each forward push operation 𝑡 = 1, 2, . . . 𝑇 , we assume the frontier node
is 𝑢𝑡 , the run time of one push operation is then 𝑑 (𝑢𝑡 ). For total
Í
𝑇 push operations, the total run time is 𝑇𝑖=1 𝑑 (𝑢𝑖 ). Notice that
during each push operation, the amount of ∥𝒓𝑠𝑡 −1 ∥ 1 is reduced at
least 𝜖𝛼𝑑 (𝑢𝑡 ), then we always have the following inequality
𝜖𝛼𝑑 (𝑢𝑡 ) < ∥𝒓𝑠𝑡 −1 ∥ 1 − ∥𝒓𝑠𝑡 ∥ 1
Apply the above inequality from 𝑡 = 1, 2, to 𝑇 , we will have
𝜖𝛼

𝑇
∑︁

𝑑 (𝑢𝑡 ) ≤ ∥𝒓𝑠0 ∥ − ∥𝒓𝑠𝑇 ∥ 1 = 1 − ∥𝒓𝑠 ∥ 1,

(9)

𝑡 =1

where 𝒓𝑠 is the final residual vector. The total time is then O (1/𝜖𝛼).
To show the estimation error, we follow the idea of [26]. Notice
that, the forward local push algorithm always has the invariant
property by Lemma 4, that is
∑︁
𝜋𝑠 (𝑢) = 𝑝𝑠 (𝑢) +
𝑟𝑠 (𝑣)𝜋 𝑣 (𝑢), ∀𝑢 ∈ V.
(10)

HEURISTIC METHOD: COMMUTE

We update the embeddings by their pairwise relationship (resistance
distance). The commute distance (i.e. resistance distance) 𝐶𝑢𝑣 =
𝐻𝑢𝑣 + 𝐻 𝑣𝑢 , where rescaled hitting time 𝐻𝑢𝑣 converges to 1/𝑑 (𝑣).
As proved in [41], when the number of nodes in the graph is large
enough, we can show that the commute distance tends to 1/𝑑 𝑣 +
1/𝑑𝑢 .

9:
10:
11:

Input: An graph G 0 (V 0, E 0 ) and embedding 𝑾 0 , dimension 𝑑.
Output: 𝑾 𝑇 
for 𝑒 𝑡 (𝑢, 𝑣, 𝑡) ∈ 𝑒 1 (𝑢 1, 𝑣 1, 𝑡 1 ), . . . , 𝑒𝑇 (𝑢𝑇 , 𝑣𝑇 , 𝑡𝑇 ) do
Add 𝑒 𝑡 to G𝑡
If 𝑢 ∉ 𝑉 𝑡 −1 then
generate 𝒘𝑢𝑡 = N (0, 0.1 · 𝑰𝑑 ) or U (−0.5, 0.5)/𝑑
If 𝑣 ∉ 𝑉 𝑡 −1 then
generate 𝒘 𝑣𝑡 = N (0, 0.1 · 𝑰𝑑 ) or U (−0.5, 0.5)/𝑑
𝑑 (𝑢)
1
𝑡
𝒘 𝑣𝑡
𝒘𝑢 = 𝑑 (𝑢)+1 𝒘𝑢𝑡 −1 + 𝑑 (𝑢)
𝑑 (𝑣)

1
𝒘 𝑣𝑡 = 𝑑 (𝑣)+1 𝒘 𝑣𝑡 −1 + 𝑑 (𝑣)
𝒘𝑢𝑡
Return 𝑾𝑇

One can treat the Commute method, i.e. Algorithm 4, as the firstorder approximation of RandNE [47]. The embedding generated by
RandNE is given as the following


𝑼 = 𝛼 0 𝑰 + 𝛼 1 𝑨 + 𝛼 2 𝑨2 + . . . + 𝛼𝑞 𝑨𝑞 𝑹,
(11)
where 𝑨 is the normalized adjacency matrix and 𝑰 is the identity
matrix. At any time 𝑡, the dynamic embedding of node 𝑖 of Commute
is given by
𝑑 (𝑢)
1 𝑡
𝒘 𝑡 −1 +
𝒘
𝑑 (𝑢) + 1 𝑖
𝑑 (𝑢) 𝑣
∑︁
1 𝑡
1
1
𝒘𝑖0 +
𝒘
=
𝑑 (𝑢) + 1
|N (𝑖)|
𝑑 (𝑢) 𝑣

𝒘𝑖𝑡 =

𝑣 ∈𝑉

𝑗 ∈N (𝑖)

By proposition 11, we have
∑︁
𝜋𝑠 (𝑢) = 𝑝𝑠 (𝑢) +
𝑟𝑠 (𝑣)𝜋 𝑣 (𝑢), ∀𝑢 ∈ V

C

𝑣 ∈𝑉

= 𝑝𝑠 (𝑢) +

∑︁
𝑣 ∈𝑉

≤ 𝑝𝑠 (𝑢) +

∑︁

𝑑 (𝑢)
𝑟𝑠 (𝑣)
𝜋𝑢 (𝑣), ∀𝑢 ∈ V
𝑑 (𝑣)
𝜖𝑑 (𝑢)𝜋𝑢 (𝑣), ∀𝑢 ∈ V = 𝑝𝑠 (𝑢) + 𝜖𝑑 (𝑢),

𝑣 ∈𝑉

where the first inequality by the fact that 𝑟𝑠 (𝑣) ≤ 𝜖𝑑 (𝑣) and the
last equality is due to ∥𝜋𝑢 ∥ 1 = 1.
□
Proposition 12 ([45]). Let G = (𝑉 , 𝐸) be undirected and let 𝑡 be a
Í
𝜋 (𝑡 )
vertex of 𝑉 , then 𝑥 ∈𝑉 𝑑𝑥(𝑡 ) ≤ 1.
Proof. By using Proposition 11, we have
∑︁ 𝜋𝑥 (𝑡) ∑︁ 𝜋𝑡 (𝑥)
∑︁
=
≤
𝜋𝑡 (𝑥) = 1.
𝑑 (𝑡)
𝑑 (𝑥)
𝑥 ∈𝑉

𝑥 ∈𝑉

𝑥 ∈𝑉

□

DETAILS OF DATA PREPROCESSING

In this section, we describe the preprocessing steps of three datasets.
Enwiki20: In Enwiki20 graph, the edge stream is divided into
6 snapshots, containing edges before 2005, 2005-2008, ..., 20172020. The sampled nodes in the first snapshot fall into 5 categories.
Patent: In full patent graph, we divide edge stream into 4 snapshots, containing patents citation links before 1985, 1985-1990,...,
1995-1999. In node classification tasks, we sampled 3,000 nodes
in the first snapshot, which fall into 6 categories. In patent small
graph, we divide into 13 snapshots with a 2-year window. All the
nodes in each snapshot fall into 6 categories.Coauthor graph: In
full Coauthor graph, we divide edge stream into 7 snapshots (before
1990, 1990-1995, ..., 2015-2019). The sampled nodes in the first snapshot fall into 9 categories. In Coauthor small graph, the edge stream
is divided into 9 snapshots (before 1978, 1978-1983,..., 2013-2017).
All the nodes in each snapshot have 14 labels in common.

T-SNE: DeepWalk

T-SNE: Node2vec

60

D

60

40

40

History
Geology
Economics
Geography
Chemistry
Philosophy
Sociology
Materials science
Mathematics
Biology
Computer science
Political science
Engineering
Psychology
Environmental science
Business
Physics
Medicine
Art

20
0
20
40
60
40

20

0

20

40

20
0
20
40
60

60

60

T-SNE: DynamicTriad

20
40
60

40

20

0

20

40

20

0

20

40

T-SNE: RandNE

40

History
Geology
Economics
Geography
Chemistry
Philosophy
Sociology
Materials science
Mathematics
Biology
Computer science
Political science
Engineering
Psychology
Environmental science
Business
Physics
Medicine
Art

0

40

60

40
20

History
Geology
Economics
Geography
Chemistry
Philosophy
Sociology
Materials science
Mathematics
Biology
Computer science
Political science
Engineering
Psychology
Environmental science
Business
Physics
Medicine
Art

History
Geology
Economics
Geography
Chemistry
Philosophy
Sociology
Materials science
Mathematics
Biology
Computer science
Political science
Engineering
Psychology
Environmental science
Business
Physics
Medicine
Art

20
0
20
40
60

60

80

60

40

20

0

20

40

60

80

T-SNE: DynamicPPE

60
40

History
Geology
Economics
Geography
Chemistry
Philosophy
Sociology
Materials science
Mathematics
Biology
Computer science
Political science
Engineering
Psychology
Environmental science
Business
Physics
Medicine
Art

20
0
20
40
60
60

40

20

0

20

40

DETAILS OF PARAMETER SETTINGS

Deepwalk: number-walks=40, walk-length=40, window-size=5
Node2Vec: Same as Deepwalk, p = 0.5, q = 0.5
DynamicTriad: iteration=10, beta-smooth=10, beta-triad=10. Each
input snapshot contains the previous existing edges and newly arrived edges.
RandNE: q=3, default weight for node classification [1, 1𝑒2, 1𝑒4, 1𝑒5],
input is the transition matrix, the output feature is normalized (l-2
norm) row-wise.
DynamicPPE: 𝛼 = 0.15, 𝜖 = 0.1, projection method=hash. our
method is relatively insensitive to the hyper-parameters.

E

VISUALIZATIONS OF EMBEDDINGS

We visualize the embeddings of small scale graphs using T-SNE[40]
in Fig.5,6,7.

60

T-SNE: DeepWalk

Figure 6: We randomly select 2,000 nodes from Coauthorsmall graph and visualize their embeddings using T-SNE[40]
.
T-SNE: DeepWalk

60
40
20
0
20
40
60
80
100

50

25

0

25

50

50

0

50

T-SNE: RandNE
0
1

0
1

40

20

0

20

0

20

40

20

40

60

60

40

20

T-SNE: DynamicTriad

40
20

0

20

20

40

60

40

Chemical
Computers & Communications
Drugs & Medical
Electrical & Electronic
Mechanical
Others

60
40
20
0
20
40
60

20

40

0

T-SNE: RandNE

Chemical
Computers & Communications
Drugs & Medical
Electrical & Electronic
Mechanical
Others

60

60

40

20

0

20

40

60

T-SNE: DynamicPPE
Chemical
Computers & Communications
Drugs & Medical
Electrical & Electronic
Mechanical
Others

0
60

40

20

0

20

40

60

T-SNE: DynamicPPE

20
40
60

0
1

60

40
0

20

60
40

20

40

40

40

40

20

60

20

20

60

60

20

0

40

0
100

T-SNE: DynamicTriad
40

0

Chemical
Computers & Communications
Drugs & Medical
Electrical & Electronic
Mechanical
Others

20

150

Chemical
Computers & Communications
Drugs & Medical
Electrical & Electronic
Mechanical
Others

20

0
20

40

100
75

40

60

0

0
1

20

0
1

50

125 100

60

60

T-SNE: Node2vec
50

T-SNE: Node2vec

40

60

40

20

0

20

40

40
20
0
20
40
40

20

0

20

40

60

80

Figure 7: We select all labeled nodes from Academic graph
and visualize their embeddings using T-SNE[40]
.

Figure 5: We randomly select 2,000 nodes from patent-small
and visualize their embeddings using T-SNE[40]
.

