E PIDEMI O PTIM :

arXiv:2010.04452v1 [cs.LG] 9 Oct 2020

A TOOLBOX FOR THE OPTIMIZATION
OF CONTROL POLICIES IN EPIDEMIOLOGICAL MODELS
Cédric Colas
Inria - Flowers team
Univ. de Bordeaux
cedric.colas@inria.fr

Boris Hejblum
Inria - SISTM
Inserm U1219 Bordeaux Population Health
Univ. de Bordeaux

Sébastien Rouillon
GREThA UMR 5113
Univ. de Bordeaux

Rodolphe Thiébaut
Inria - SISTM
Inserm U1219 Bordeaux Population Health
Univ. de Bordeaux

Pierre-Yves Oudeyer, Clément Moulin-Frier
Inria - Flowers team
Univ. de Bordeaux
ENSTA ParisTech

Mélanie Prague
Inria - SISTM
Inserm U1219 Bordeaux Population Health
Univ. de Bordeaux

A BSTRACT
Epidemiologists model the dynamics of epidemics in order to propose control
strategies based on pharmaceutical and non-pharmaceutical interventions (contact
limitation, lock down, vaccination, etc). Hand-designing such strategies is not
trivial because of the number of possible interventions and the difficulty to predict long-term effects. This task can be cast as an optimization problem where
state-of-the-art machine learning algorithms such as deep reinforcement learning, might bring significant value. However, the specificity of each domain –
epidemic modelling or solving optimization problem – requires strong collaborations between researchers from different fields of expertise. This is why we
introduce EpidemiOptim, a Python toolbox that facilitates collaborations between
researchers in epidemiology and optimization. EpidemiOptim turns epidemiological models and cost functions into optimization problems via a standard interface commonly used by optimization practitioners (OpenAI Gym). Reinforcement
learning algorithms based on Q-Learning with deep neural networks (DQN) and
evolutionary algorithms (NSGA-II) are already implemented. We illustrate the use
of EpidemiOptim to find optimal policies for dynamical on-off lock-down control
under the optimization of death toll and economic recess using a SusceptibleExposed-Infectious-Removed (SEIR) model for COVID-19. Using EpidemiOptim and its interactive visualization platform in Jupyter notebooks, epidemiologists, optimization practitioners and others (e.g. economists) can easily compare
epidemiological models, costs functions and optimization algorithms to address
important choices to be made by health decision-makers.

1

I NTRODUCTION

The recent COVID-19 pandemic highlights the destructive potential of infectious diseases in our
societies, especially on our health, but also on our economy. To mitigate their impact, scientific
understanding of their spreading dynamics coupled with methods quantifying the impact of intervention strategies along with their associated uncertainty, are key to support and optimize informed
policy making. For example, in the COVID-19 context, large scale population lock-downs were
enforced based on analyses and predictions from mathematical epidemiological models (Ferguson
et al., 2005; 2006; Cauchemez et al., 2019; Ferguson et al., 2020). In practice, researchers often consider a small number of relatively coarse and pre-defined intervention strategies, and run calibrated
1

epidemiological models to predict their impact (Ferguson et al., 2020). This is a difficult problem
for several reasons: 1) the space of potential strategies can be large, heterogeneous and multi-scale
(Halloran et al., 2008); 2) their impact on the epidemic is often difficult to predict; 3) the problem
is multi-objective by essence: it often involves public health objectives like the minimization of the
death toll or the saturation of intensive care units, but also societal and economic sustainability. For
these reasons, pre-defined strategies are bound to be suboptimal. Thus, a major challenge consists
in leveraging more sophisticated and adaptive approaches to identify optimal strategies.
Machine learning can be used for the optimization of such control policies, with methods ranging
from deep reinforcement learning to multi-objective evolutionary algorithms. In other domains,
they have proven efficient at finding robust control policies, especially in high-dimensional nonstationary environments with uncertainty and partial observation of the state of the system (Deb
et al., 2007; Mnih et al., 2015; Silver et al., 2017; Haarnoja et al., 2018; Kalashnikov et al., 2018;
Hafner et al., 2019). Yet, researchers in epidemiology, in public-health, in economics, and in machine learning evolve in communities that rarely cross, and often use different tools, formalizations
and terminologies. We believe that tackling the major societal challenge of epidemic mitigation requires interdisciplinary collaborations organized around operational scientific tools and goals, that
can be used and contributed by researchers of these various disciplines. To this end, we introduce
EpidemiOptim, a Python toolbox that provides a framework to facilitate collaborations between
researchers in epidemiology, economics and machine learning.
EpidemiOptim turns epidemiological models and cost functions into optimization problems via the
standard OpenAI Gym (Brockman et al., 2016) interface that is commonly used by optimization
practitioners. Conversely, it provides epidemiologists and economists with an easy-to-use access to
a variety of deep reinforcement learning and evolutionary algorithms, capable of handling different
forms of multi-objective optimization under constraints. Thus, EpidemiOptim facilitates the independent update of models by specialists of each topic, while enabling others to leverage implemented
models to conduct experimental evaluations. We illustrate the use of EpidemiOptim to find optimal
policies for dynamical on-off lock-down control under the optimization of death toll and economic
recess using an extended Susceptible-Exposed-Infectious-Removed (SEIR) model for COVID-19
from Prague et al. (2020).

Related Work. We can distinguish two main lines of contributions concerning the optimization
of intervention strategies for epidemic response. On the one hand, several contributions focus on
providing guidelines and identifying the range of methods available to solve the problem. For example, Yáñez et al. (2019) framed the problem of finding optimal intervention strategies for a disease
spread as a reinforcement learning problem; Alamo et al. (2020) provided a road-map that goes from
the access to data sources to the final decision-making step; and Shearer et al. (2020) highlighted
that a decision model for epidemic response cannot capture all of the social, political, and ethical
considerations that these decisions impact. These contributions reveal a major challenge for the community: developing tools that can be easily used, configured and interpreted by decision-makers. On
the other hand, computational contributions proposed actual concrete implementations of such optimization processes. These contributions mostly differ by their definition of epidemiological models
(e.g. SEIR (Yaesoubi et al., 2020) or agent-based models (Chandak et al., 2020)), of optimization
methods (e.g. deterministic rules (Tarrataca et al., 2020), Bayesian optimization (Chandak et al.,
2020), Deep RL (Arango & Pelov, 2020) or evolutionary optimization (Miikkulainen et al., 2020)),
of cost functions (e.g. fixed weighted sum of health and economical costs (Arango & Pelov, 2020),
possibly adding constraints on the school closure budget (Libin et al., 2020), or multi-objective optimization (Miikkulainen et al., 2020)), of state and action spaces (e.g. using the entire observed
epidemic history (Yaesoubi et al., 2020) or an image of the disease outbreak to capture the spatial relationships between locations (Probert et al., 2019)), as well as methods for representing the
model decisions in a format suitable to decision-makers (e.g simple summary state representation
(Probert et al., 2019) or real-time surveillance data with decision rules (Yaesoubi & Cohen, 2016)).
See Appendix A.2 for a detailed description of the aforementioned papers.
Given this high diversity of potential methods in the field, our approach aims at providing a standard
toolbox facilitating the comparison of different configurations along the aforementioned dimensions
in order to assist decision-makers in the evaluation of the range of possible intervention strategies.
2

Contributions. This paper makes three contributions. First, we formalize the coupling of epidemiological models and optimization algorithms with a particular focus on the multi-objective aspect
of such problems (Section 2). Second, based on this formalization, we introduce the EpidemiOptim
library, a toolbox that integrates epidemiological models, cost functions, optimization algorithms
and experimental tools to easily develop, study and compare epidemic control strategies (Section 3).
Third, we demonstrate the utility of the EpidemiOptim library by conducting a case study on the optimization of lock-down policies for the COVID-19 epidemic (Section 4). We use a recent epidemiological model grounded on real data, cost functions based on a standard economical model of GDP
loss, as well as state-of-the-art optimization algorithms, all included in the EpidemiOptim library.
This is, to our knowledge, the first contribution that provides a comparison of different optimization
algorithm performances for the control of intervention strategies on the same epidemiological model.
The user of the EpidemiOptim library can interact with the trained policies via Jupyter notebook, exploring the space of cost functions (health cost x economic cost) for a variety of algorithms. The code
is made available anonymously at https://github.com/flowersteam/EpidemiOptim.

2

T HE EPIDEMIC CONTROL PROBLEM AS AN OPTIMIZATION PROBLEM

In an epidemic control problem, the objective is to find an optimal control strategy to
minimize some cost (e.g. health and/or economical cost) related to the evolution of an
epidemic. We take the approach of reinforcement learning (RL) (Sutton & Barto, 2018):
the control policy is seen as a learning agent,
that interacts with an epidemiological model
that is seen as its learning environment, see
Figure 1. Each run of the epidemic is a learning episode, where the agent alternatively interacts with the epidemic through actions and
observes the resulting states of the environment and their associated costs. To define
this optimization problem, we define three elements: 1) the state and action spaces; 2) the
epidemiological model; 3) the cost function.

Epidemiological
model

Policy

traj.
Learning
algorithm

Trajectory
collector

Agent

Cost
functions

Environment

Figure 1: Epidemic control as an optimization
problem. s, a, ci refer to environment states, control actions, and the ith cost, while traj. is their collection over an episode. Dashed arrows match the
input (→) and output (←) of the OpenAI Gym step
function.

State and action spaces. The state st describes the information the control policy can use (e.g.
epidemiological model states, cumulative costs up to timestep t, etc.) We also need to define which
actions a can be exerted on the epidemic, and how they affect its parameters. For instance, enforcing
a lock-down can reduce the transmission rate of the epidemic and slow it down.
The epidemiological model. The epidemiological model is akin to a transition function T : it
governs the evolution of the epidemic. From the current state st of the epidemic and the current
action at exerted by the agent, it generates the next state st+1 : T : S, A → S where S, A are the
state and action spaces respectively. Note that this mapping can be stochastic.
Cost functions and constraints. The design of cost functions is central to the creation of optimization problems. Epidemic control problems can often be framed as multi-objective problems,
where algorithms need to optimize for a set of cost functions instead of a unique one. Costs can be
defined at the level of timesteps C(s, a) or at the level of an episode Ce (trajectory). This cumulative
cost measure can be obtained by summing timestep-based costs over an episode. For mathematical
convenience, RL practitioners often
P use a discounted sum of costs, where future costs are discounted
exponentially: Ce (trajectory) = t γ t c(st , at ) with discount factor γ (e.g. it avoids infinite sums
when the horizon is infinite). When faced with multiple cost functions Ci |i∈[1..Nc ] , a simple approach is to aggregate them intoPa unique cost function C, computed as the convex combination of
the individual costs: C̄(s, a) = i βi Ci (s, a). Another approach is to optimize for the Pareto front,
i.e. the set of non-dominated solutions. A solution is said to be non-dominated if no other solution
performs better on all costs and strictly better on at least one. In addition, one might want to define
constraints on the problem (e.g. a maximum death toll over a year).
3

The epidemic control problem as a Markov Decision Process. Just like traditional RL problems, our epidemic control problem can be framed as a Markov Decision Process: M :
{S, A, T , ρ0 , C, γ}, where ρ0 is the distribution of initial states, see Yáñez et al. (2019) for a similar
formulation.
Handling time limits. Epidemics are not episodic phenomena, or at least it is difficult to predict
their end in advance. Optimization algorithms, however, require episodes of finite lengths. This
enables the algorithm to regularly obtain fresh data by trying new policies in the environment. This
can be a problem, as we do not want the control policy to act as if the end of the episode was the end
of the epidemic: if the epidemic is growing rapidly right before the end, we want the control policy
to care and react. Evolution algorithms (EAs) cannot handle such behavior as they use episodic data
(θe , Ce ), where θe are the policy parameters used for episode e. RL algorithms, however, can be
made time unaware. To this end, the agent should not know the current timestep, and should not be
made aware of the last timestep (termination signal). Default RL implementations often ignore this
fact, see a discussion of time limits in Pardo et al. (2018).

3
3.1

T HE E PIDEMI O PTIM TOOLBOX
T OOLBOX DESIDERATA

The EpidemiOptim toolbox aims at facilitating collaborations between the different fields interested
in the control of epidemics. Such a library should contain state-of-the-art epidemiological models,
optimization algorithms and relevant cost functions. It should enable optimization practitioners to
bring their newly-designed optimization algorithm, to plug it to interesting environments based on
advanced models grounded in epidemiological knowledge and data, and to reliably compare it to
other state-of-the-art optimization algorithms. On the other hand, it should allow epidemiologists
to bring their new epidemiological model, reuse existing cost functions to form a complete epidemic control problem and use state-of-the-art optimization algorithms to solve it. This requires a
modular approach where algorithms, epidemiological models and cost functions are implemented
independently with a standardized interface. The toolbox should contain additional tools to facilitate
experiment management (result tracking, configuration saving, logging). Good visualization tools
would also help the interpretation of results, especially in the context of multi-objective problems.
Ideally, users should be able to interact with the trained policies, and to observe the new optimal
strategy after the modification of the relative importance of costs in real-time. Last, the toolbox
should enable reproducibility (seed management) and facilitate statistical comparisons of results.
3.2

T OOLBOX ORGANIZATION

Overview. The EpidemiOptim toolbox is centered around two modules: the environment and the
optimization algorithm, see Figure 1. To run an experiment, the user can define its own or select the
ones contained in the library. Appendix Figure 4 presents the main interface of the EpidemiOptim
library. The following sections delve into the environment and algorithm modules.
Environment: epidemiological models. The Model module wraps around epidemiological models, and allows to sample new model parameters from a distribution of models, to sample initial
conditions (ρ0 ) and to run the model for n steps. Currently, the EpidemiOptim library contains
a Python implementation of an extended Susceptible-Exposed-Infectious-Removed (SEIR) model
fitted to French data, from Prague et al. (2020).
Environment: cost functions and constraints. In the EpidemiOptim toolbox, each cost function
is a separate class that can compute ci (s, a), normalize the cost to [0, 1] and generate constraints on
its cumulative value. The Multi-Cost module integrates several cost functions. It computes the list
of costs ci (s, a)|i∈[1..Nc ] and an aggregated cost c̄ parameterized by the mixing weights βi |i∈[1..Nc ] .
The current version of EpidemiOptim contains two costs: 1) a health cost computed as the death
toll of the epidemic; 2) an economic cost computed as the opportunity loss of GDP resulting from
either the epidemic itself (illness and death reduce the workforce), or the control strategies (lockdown also reduces the workforce). In addition, we define two constraints as maximal values that the
cumulative measures of the two costs described above can take.
4

Environment: OpenAI Gym interface as a universal interface. The learning environment defines the optimization problem. We use a framework that has become standard in the Reinforcement
Learning community in recent years: the OpenAI Gym environment (Brockman et al., 2016). Gym
environments have the following interface: First, they have a reset function that resets the environment state, effectively starting a new simulation or episode. Second, they have a step function that
takes as input the next action from the control policy, updates the internal state of the model and
returns the new environment state, the cost associated to the transition and a Boolean signal indicating whether the episode is terminated, see Figure 1. This standardized interface allows optimization
practitioners (RL researchers especially) to easily plug any algorithm, facilitating the comparison of
algorithms on diverse benchmark environments. In our library, this class wraps around an epidemiological model and a set of cost functions. It also defines the state and action spaces S, A. So far, it
contains the EpidemicDiscrete-v0 environment, a Gym-like environment based on the epidemiological model and the bi-objective cost function described above. Agents decide every week whether to
enforce a partial lock-down (binary action) that results in a decreased transmission rate.
Optimization algorithms. The optimization algorithm, or algorithm for short, trains learning
agents to minimize cumulative functions of costs. Learning agents are characterized by their control
policy: a function that takes the current state of the environment as input and produces the next
action to perform: Πθ : S → A where Πθ is parameterized by the vector θ. The objective of the
algorithm is, thus, to find the optimal set of parameters θ∗ that minimizes the cost. These algorithms
collect data by letting the learning agent interact with the environment. Currently, the EpidemiOptim toolbox integrates several RL algorithms based on the famous Deep Q-Network algorithm (Mnih
et al., 2015) as well as the state-of-the-art multi-objective EA NSGA-II Deb et al. (2002).
Utils. We provide additional tools for experimentation: experience management, logging and
plots. Algorithms trained on the same environment can be easily compared, e.g. by plotting their
Pareto front on the same graph. We also enable users to visualize and interact with trained policies
via Jupyter notebooks. Users can select solutions from the Pareto front, modify the balance between
different cost functions and modify the value of constraints set to the agent via sliders. They visualize a run of the resulting policy in real-time. We are also committed to reproducibility and robustness
of the results. For this reason, we integrate to our framework a library for statistical comparisons
designed for RL experiments (Colas et al., 2019).

4

C ASE STUDY: LOCK - DOWN ON - OFF POLICY OPTIMIZATION FOR THE
COVID -19 EPIDEMIC

The SARS-CoV-2 virus was identified on January, 7th 2020 as the cause of a new viral disease
named COVID-19. As it rapidly spread globally, the World Health Organization (2020) declared
a pandemic on March 11th 2020. Exceptional measures have been implemented across the globe
as an attempt to mitigate the pandemic (Kraemer et al., 2020). In many countries, the government
response was a total lock-down: self-isolation with social distancing, schools and workplaces closures, cancellation of public events, large and small gatherings prohibition, and travel limitation.
In France, lock-down lasted 55 days from March, 17th to May, 11th . It led to a decrease in gross
domestic product of, at least, 10.1% (Mandel & Veetil, 2020) but allowed to reduce the death toll
by an estimated 690,000 [570,000; 820,000] people (Flaxman et al., 2020). In this case study, we
investigate alternative strategies optimized by RL and EA algorithms over a period of 1 year.
4.1

E NVIRONMENT

The epidemiological model. We use a mathematical structural model to understand the large-scale
dynamics of the COVID-19 epidemic. We focus on the extended Susceptible-Exposed-InfectiousRemoved (SEIR) model from Prague et al. (2020), which was fitted to French data. The estimation
in this model was region-specific. Here, we use their estimated parameters for the epidemic dynamics in Île-de-France (Paris and its surrounding region), presented in Appendix Table 1. The
model explains the dynamics between Susceptibles (S), Exposed (E), Ascertained Infectious (I),
Removed (R), Non-Ascertained Asymptomatic (A) and Hospitalized (H). The underlying system
of differential equations and a schematic view of the mechanistic model can be found in Appendix
5

Section A.3. To account for uncertainty in the model parameters, we create a distribution of models
resulting from parameter distributions, see details in Appendix Section A.3 and a visualization of 10
sampled models in Appendix Section A.3.2.
State and action spaces. The state of the agent is composed of the epidemiological states
(SEIRAH) normalized by the population size (N = 12, 278, 210), booleans indicating whether
the previous and current state are under lock-down, the two cumulative costs and the level of the
transmission rate (from 1 to 4). The two cumulative costs are normalized by N and 150B respectively, 150B being the approximate maximal cost due to a full year of lockdown. The action occurs
weekly and is binary (1 for lock-down, 0 otherwise). Consecutive weeks of lockdown decrease the
b in four stages (see model description in Appendix Figure 5), while stopping lockdown follows the
opposite path (stair-case increase of b).
Cost functions. In this case-study, an optimal policy should minimize two costs: a health cost
and an economic cost. The health cost is computed as the death toll, it can be evaluated for each
environment transition (weekly) or over a full-episode (e.g. annually): Ch (t) = D(t) = 0.005R(t).
The economic cost is measured in euros as the GDP opportunity cost resulting from diseased or dead
workers, as well as people unemployed due to the lock-down policy. At any time t, the GDP can be
expressed as a Cobb-Douglas function F (L(t)) = AK0γk L(t)1−γk , where K0 is the capital stock of
economy before the outbreak that we will suppose constant during the pandemics, L(t) is the active
population employed, γk is the capital elasticity and A is the exogenous technical progress (Douglas,
1976). Let L0 = λN be the initial employed population, where λ is the activity/employment rate.
The lock-down leads to partial unemployment and decreases the size of the population able to work
(illness, isolation, death), thus L(t) = (1 − u(t))λ(N − G(t)), where u(t) is the level of partial
unemployment due to the lock-down and G(t) = I(t) + H(t) + 0.005R(t) is the size of the ill,
isolated or dead population as defined in our SEIRAH model. The economic cost is defined as the
difference between the GDP before (Y0 ) and after the pandemic, Y0 − F (L(t)) and is given by:
Ceco (t) = Y0 − AK0γk ((1 − u(t))λ(N − G(t))1−γk
Parameters for values of parameters in the economic model are given in Table 1. We also consider
the use of constraints on the maximum value of cumulative costs.
4.2

O PTIMIZATION A LGORITHMS

We consider three algorithms: 1) a vanilla implementation of Deep Q-Network (DQN) (Mnih et al.,
2015); 2) a goal-conditioned version of DQN inspired from (Schaul et al., 2015); 3) NSGA-II, a
state-of-the-art multi-objective evolutionary algorithm (Deb et al., 2002). The trained policy is the
same for all algorithms: a neural network with one hidden-layer of size 64 and ReLU activations.
Further background about these algorithm is provided in Appendix SectionA.3.
Optimizing convex combinations of costs: DQN and Goal-DQN. DQN only targets one cost
function: c̄ = (1 − β) ch + β ceco . Here the health and economic costs are normalized to have
similar ranges (scaled by 1/(65 × 1e3) and 1/1e9 respectively). We train independent policies for
each values of β in [0., 0.05, .., 1]. This consists in training a Q-network to estimate the value of
performing any given action a in any given state s, where the value is defined as the cumulative
negative cost −c̄ expected in the future. Goal-DQN, however, trains one policy Π(s, β) to target any
convex combination parameterized by β. We use the method presented in (Badia et al., 2020): we
train two Q-networks, one for each of the two costs. The optimal action is then selected as the one
maximizing the convex combination of the two Q-values:
Π(s, β) = argmaxa (1 − β) Qs (s, a) + β Qeco (s, a).

(1)

By disentangling the two costs, this method facilitates the representation of values that can have
different scales and enables automatic transfer for any value of β, see justifications in Badia et al.
(2020). During training, agents sample the targeted β uniformly in [0, 1].

6

Adding constraints with Goal DQN-C. We design and train a variant of Goal DQN to handle
constraints on maximal values for the cumulative costs: Goal-DQN-C. We train a Q-network for
each constraints with a cost of 1 each time the constraint is violated, 0 otherwise (Qch , Qceco ). With a
discount factor γ = 1, this network estimates the number of transitions that are expected to violate
the constraints in the future. The action is selected according to Eq. 1 among the actions that are
not expected to lead to constraints violations (Qc (s, a) < 1). If all actions are expected to lead to
violations, the agent selects the action that minimizes that violation. During training, agents sample
β uniformly, and 50% of the time samples uniformly one of the two constraints in [1000, 62000] for
the maximum death toll and [20, 160] for the maximum economic cost, in billions.
EAs to optimize a Pareto front of solutions. We also use NSGA-II, a state-of-the-art multiobjective EA algorithm that trains a population of solutions to optimize a Pareto front of nondominated solutions (Deb et al., 2002). As others EAs, NSGA-II uses episodic data (θe , Ce ). To
obtain reliable measures of Ce in our stochastic environment, we average costs over 30 evaluations.
For this reason, NSGA-II requires more samples than traditional gradient-based algorithms. For
fair comparison, we train two NSGA-II algorithm: one with the same budget of samples than DQN
variants (1e6 environment steps), and the other with 15x as much (after convergence).
4.3

R ESULTS

Pareto fronts. We aim at providing decision tools to decision-maker and, for this reason, cannot
decide ourselves the right value for the mixing parameter β. Instead, we want to present Pareto
fronts of Pareto-optimal solutions. Only NSGA-II aims at producing an optimal Pareto front as its
result while DQN and Goal DQN optimize a unique policy. However, one can build Pareto fronts
even with DQN variants. We can build population of policies by aggregating independenlty trained
DQN policies, or evaluating a single Goal DQN policy on a set of Npareto goals sampled uniformly
(100 here). The resulting population can then be filtered to produce the Pareto front. Figure 2a
presents the resulting Pareto fronts for all variants. The Pareto fronts made of several DQN policies
(yellow) perform better in the regime of low health cost (< 103 deaths) where it leads to save around
20 B euros over NSGA-II and the Goal DQN without constraint. Over 25 × 103 deaths, NSGA-II
becomes the best algorithm, as it allows to save 10 to 15 B euros in average, the health cost being
kept constant.
DQN
NSGA-II (same budget)

1.0

150

costs (% max)

GDP cost (B)

Goal DQN
NSGA-II (budget x15)
GOAL DQN-C

100

50

00

10

20

30

40

# Deaths (×103)

50

(a)

Economic cost

0.8
0.6
0.4
0.2
0.0
0.0

60

Health cost

0.2

0.4

β

0.6

0.8

1.0

(b)

Figure 2: Left: Pareto fronts for various optimization algorithms. The closer to the origin point the
better. Right: Evolution of the costs when Goal DQN agents are evaluated on various β. We plot the
mean and standard error of the mean (left) and standard deviations (right) over 10 seeds.
In the low health cost regime, the DQN policy attempts to break the first wave of the epidemic, then
maintains it low with periodic lock-down (1 or 2 weeks period) until it is sufficiently low. If a second
wave appears, it goes back to that strategy, see Figure 3a for an example. Goal DQN with constraints
shows a similar behavior. NSGA-II enters a cyclic 1-week period of lock-down but does not stop it
even when the number of new cases appears null (Appendix Section A.4). In the low economic cost
regime (below 15 B), NSGA-II waits for the first wave to grow and breaks it by a lock-down of a few
weeks (Figure 3b). DQN-based algorithms sometimes find such policies. Policies in the range of
health cost between 103 and 50 × 103 deaths are usually policy that perform low or high health costs
7

depending on the particular epidemic and its initial state. Appendix Section 4.3 presents comparison
statistics and delves into the different strategies and interesting aspects of the four algorithms.

0 100 200 300

0.2

0 100 200 300

60

0.4

0

0 100 200 300

1

20

0 100 200 300

0.0
2

40

0.2
0.0

0.4

Transmission rate

New GDP Loss (B)

0

Total Deaths

100

6000
4000
2000
0

Total GDP Loss (B)

New Deaths

200

Aggregated Cost

Eco cost: 59.52 B, Death Cost: 6023, Aggregated Cost: 37.17

0 100 200 300

days

0 0 100 200 300

(a)

3

0 100 200 300

0.4

0

1

0 100 200 300

0 100 200 300

0 100 200 300

1

5
0

0
2

10

0.2
0.0

2

20000

Total GDP Loss (B)

New GDP Loss (B)

0

Total Deaths

New Deaths

40000

1000

Transmission rate Aggregated Cost

Eco cost: 13.09 B, Death Cost: 41216, Aggregated Cost: 38.25

0 100 200 300

days

0 0 100 200 300

(b)

Figure 3: Evolution of cost metrics and strategy over a year of simulated epidemic with a DQN agent
(top) in the low health cost regime and a NSGA-II agent in the low economic cost regime (bottom).
Optimizing convex combinations. When β is low (respectively high), the cost function is closer
to the health (respectively economic) cost (see Eq 1). Figure 2b presents the evolution of the two
costs when a trained Goal DQN policy (without constraints) is evaluated on different β. As expected,
the higher β the lower the health cost and vice-versa. We see that, outside of the [0.2, 0.8] range, the
policy falls in extreme regimes: either it always locks-down (low β) or it never does (high β).
Interactive decision making. The graphs presented above help make sense of the general performance of the algorithms, in average over multiple runs. However, it is also relevant to look at
particular examples to understand a phenomenon. For this purpose, we include visualization Jupyter
Notebooks in the EpidemiOptim toolbox. There are three notebooks for DQN, Goal DQNs and
NSGA-II respectively. The two first one help making sense of DQN and Goal DQN algorithms
(with or without constraints). After loading a pre-trained model, the user can interactively act on
the cost function parameters (mixing weight β or constraints on the maximum values of the cumulative costs). As the cost function changes, the policy automatically adapts and the evolution of the
8

epidemic and control strategy are displayed. The third notebook helps visualizing a Pareto front of
the NSGA-II algorithm. After loading a pre-trained model, the user can click on solutions shown
on the Pareto front, which automatically runs the corresponding model and displays the evolution of
the epidemic and control strategy. The user can explore these spaces and visualize the strategies that
correspond to the different regimes of the algorithm.
Discussion. This case study proposed to use a set of state-of-the-art algorithms to target the problem of lock-down policy optimization in the context of the COVID-19 epidemic (RL and EA algorithms). DQN appears to work better in the regime of low health cost, but requires to train multiple
policies independently. As it trains only one policy, Goal DQN is faster, but does not bring any performance advantage. NSGA-II, which is designed for the purpose of multi-objective optimization,
seems to perform better in the regime of low economic cost.

5

D ISCUSSION & C ONCLUSION

On the use of automatic optimization for decision-making. We do not think that optimization
algorithms should ever replace decision-makers, especially in contexts involving the life and death
of individuals such as epidemics. However, we think optimization algorithms can provide decisionmakers with useful insights by taking into account long-term effects of control policies. To this end,
we believe it is important to consider a wide range of models and optimization algorithms to target
a same control problem. This fosters a diversity of methods and reduces the negative impacts of
model- and algorithm-induced biases. The EpidemiOptim toolbox is designed for this purpose: it
provides a one-stop shop centralizing both the modeling and the optimization in one place.
The future of EpidemiOptim. This toolbox is designed as a collaborative toolbox that will be
extended in the future. To this day, it contains one population-based SEIR epidemiological models
and three optimization algorithms. In the future, we plan on extending this library via collaborations
with other research labs in epidemiology and machine learning. We plan on integrating agent-based
epidemiological models that would allow a broader range of action modalities, and to add a variety
of multi-objective algorithms from EAs and RL. We could finally imagine the organization of challenges, where optimization practitioners would compete on a set of epidemic control environments.
Although this toolbox focuses on the use of optimization algorithm to solve epidemic control policies, similar tools could be used in other domains. Indeed, just like epidemics, a large diversity of
dynamical systems can be modeled models by ordinary differential equations (e.g. in chemistry,
biology, economics, etc.). One could imagine spin-offs of EpidemiOptim for applications to other
domains. Furthermore, Chen et al. (2018) recently designed a differentiable ODE solver. Such
solvers could be integrated to EpidemiOptim to allow differential learning through the model itself.
Conclusion. This paper introduces EpidemiOptim, a toolbox that facilitates collaboration between
epidemiologists, optimization practitioners and others in the study of epidemic control policies to
help decision-making. Of course, others have studied the use of optimization algorithms for the
control of epidemics. We see this as a strength, as it shows the relevance of the approach and the
importance of centralizing this research in a common framework that allows easy and reproducible
comparisons and visualizations. Articulating our understanding of epidemics evolution with the societal impacts of potential control strategies on the population (death toll, physical and psychological
traumas, etc) and on the economy will help improve the control of future epidemics.
ACKNOWLEDGMENTS
Cédric Colas is partly funded by the French Ministère des Armées - Direction Générale de
l’Armement. Numerical computations were in part carried out using the PlaFRIM experimental
testbed, supported by Inria, CNRS (LABRI and IMB), Université de Bordeaux, Bordeaux INP and
Conseil Régional d’Aquitaine (see https://www.plafrim.fr/). This work is supported in
part by Inria Mission COVID19, project GESTEPID.
9

R EFERENCES
Teodoro Alamo, Daniel G Reina, and Pablo Millán. Data-driven methods to monitor, model, forecast
and control covid-19 pandemic: Leveraging data science, epidemiology and control theory. arXiv
preprint arXiv:2006.01731, 2020.
Mauricio Arango and Lyudmil Pelov. Covid-19 pandemic cyclic lockdown optimization using reinforcement learning. arXiv preprint arXiv:2009.04647, 2020.
Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,
Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark. arXiv
preprint arXiv:2003.13350, 2020.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Simon Cauchemez, Nathanaël Hoze, Anthony Cousien, Birgit Nikolay, et al. How modelling can
enhance the analysis of imperfect epidemic data. Trends in parasitology, 35(5):369–379, 2019.
Amit Chandak, Debojyoti Dey, Bhaskar Mukhoty, and Purushottam Kar. Epidemiologically and
socio-economically optimal policies via bayesian optimization. arXiv preprint arXiv:2005.11257,
2020.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pp. 6571–6583,
2018.
Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. A hitchhiker’s guide to statistical comparisons of reinforcement learning algorithms. arXiv preprint arXiv:1904.06979, 2019.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and TAMT Meyarivan. A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE transactions on evolutionary computation, 6(2):
182–197, 2002.
Kalyanmoy Deb, S Karthik, et al. Dynamic multi-objective optimization and decision-making using
modified nsga-ii: a case study on hydro-thermal power scheduling. In International conference
on evolutionary multi-criterion optimization, pp. 803–817. Springer, 2007.
Paul H. Douglas. The cobb-douglas production function once again: Its history, its testing, and some
new empirical values. Journal of Political Economy, 84(5):903–915, 1976. doi: 10.1086/260489.
URL https://doi.org/10.1086/260489.
Neil Ferguson, Daniel Laydon, Gemma Nedjati Gilani, Natsuko Imai, Kylie Ainslie, Marc Baguelin,
Sangeeta Bhatia, Adhiratha Boonyasiri, ZULMA Cucunuba Perez, Gina Cuomo-Dannenburg,
et al. Report 9: Impact of non-pharmaceutical interventions (npis) to reduce covid19 mortality
and healthcare demand. 2020.
Neil M Ferguson, Derek AT Cummings, Simon Cauchemez, Christophe Fraser, Steven Riley, Aronrag Meeyai, Sopon Iamsirithaworn, and Donald S Burke. Strategies for containing an emerging
influenza pandemic in southeast asia. Nature, 437(7056):209–214, 2005.
Neil M Ferguson, Derek AT Cummings, Christophe Fraser, James C Cajka, Philip C Cooley, and
Donald S Burke. Strategies for mitigating an influenza pandemic. Nature, 442(7101):448–452,
2006.
Seth Flaxman, Swapnil Mishra, Axel Gandy, H Juliette T Unwin, Thomas A Mellan, Helen Coupland, Charles Whittaker, Harrison Zhu, Tresnia Berah, Jeffrey W Eaton, et al. Estimating the
effects of non-pharmaceutical interventions on covid-19 in europe. Nature, 584(7820):257–261,
2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of ICML,
2018.
10

Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. In Proceedings of ICLR, 2019.
M Elizabeth Halloran, Neil M Ferguson, Stephen Eubank, Ira M Longini, Derek AT Cummings,
Bryan Lewis, Shufu Xu, Christophe Fraser, Anil Vullikanti, Timothy C Germann, et al. Modeling
targeted layered containment of an influenza pandemic in the united states. Proceedings of the
National Academy of Sciences, 105(12):4639–4644, 2008.
Karel Havik, Kieran Mc Morrow, Fabrice Orlandi, Christophe Planas, Rafal Raciborski, Werner
Röger, Alessandro Rossi, Anna Thum-Thysen, Valerie Vandermeulen, et al. The production
function methodology for calculating potential growth rates & output gaps. Technical report,
Directorate General Economic and Financial Affairs (DG ECFIN), European . . . , 2014.
Insee. Insee website. https://www.insee.fr/, 2020.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning, pp. 651–
673, 2018.
Moritz U. G. Kraemer, Chia-Hung Yang, Bernardo Gutierrez, Chieh-Hsi Wu, Brennan Klein,
David M. Pigott, Louis du Plessis, Nuno R. Faria, Ruoran Li, William P. Hanage, John S. Brownstein, Maylis Layan, Alessandro Vespignani, Huaiyu Tian, Christopher Dye, Oliver G. Pybus,
and Samuel V. Scarpino. The effect of human mobility and control measures on the COVID-19
epidemic in China. Science, 368(6490):493–497, 2020. doi: 10.1126/science.abb4218.
Pieter Libin, Arno Moonens, Timothy Verstraeten, Fabian Perez-Sanjines, Niel Hens, Philippe
Lemey, and Ann Nowé. Deep reinforcement learning for large-scale epidemic control. arXiv
preprint arXiv:2003.13676, 2020.
Antoine Mandel and Vipin P. Veetil. The economic cost of covid lockdowns: An out-of-equilibrium
analysis. Economics of Disasters and Climate Change, 2020. doi: 10.2139/ssrn.3588421.
Risto Miikkulainen, Olivier Francon, Elliot Meyerson, Xin Qiu, Elisa Canzani, and Babak Hodjat.
From prediction to prescription: Ai-based optimization of non-pharmaceutical interventions for
the covid-19 pandemic. arXiv preprint arXiv:2005.13766, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015.
Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement
learning. In International Conference on Machine Learning, pp. 4045–4054, 2018.
Mélanie Prague, Linda Wittkop, Quentin Clairon, Dan Dutartre, Rodolphe Thiébaut, and
Boris Pierre Hejblum. Population modeling of early covid-19 epidemic dynamics in french regions and estimation of the lockdown impact on infection rate. medRxiv, 2020.
W. J. M. Probert, S. Lakkur, C. J. Fonnesbeck, K. Shea, M. C. Runge, M. J. Tildesley, and M. J. Ferrari. Context matters: using reinforcement learning to develop human-readable, state-dependent
outbreak response policies. Philosophical Transactions of the Royal Society B: Biological Sciences, 374(1776):20180277, 2019. doi: 10.1098/rstb.2018.0277.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International conference on machine learning, pp. 1312–1320, 2015.
Freya M Shearer, Robert Moss, Jodie McVernon, Joshua V Ross, and James M McCaw. Infectious
disease pandemic planning and response: Incorporating decision analysis. PLoS medicine, 17(1):
e1003018, 2020.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354–359, 2017.
11

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Luı́s Tarrataca, CM Dias, Diego Barreto Haddad, and EF Arruda. Flattening the curves: on-off
lock-down strategies for covid-19 with an application to brazi. arXiv preprint arXiv:2004.06916,
2020.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.
World Health Organization. Who director-general’s opening remarks at the media briefing on
COVID-19 – 11 march 2020, 2020. Accessed: 2020-04-02.
Reza Yaesoubi and Ted Cohen. Identifying cost-effective dynamic policies to control epidemics.
Statistics in medicine, 35(28):5189–5209, 2016.
Reza Yaesoubi, Joshua Havumaki, Melanie H Chitwood, Nicolas A Menzies, Gregg Gonsalves,
Joshua Salomon, A David Paltiel, and Ted Cohen. Adaptive policies for use of physical distancing
interventions during the covid-19 pandemic. medRxiv, 2020.
Andrea Yáñez, Conor Hayes, and Frank Glavin. Towards the control of epidemic spread: Designing
reinforcement learning environments. In AICS, pp. 188–199, 2019.

12

A
A.1

A PPENDIX
E PIDEMI O PTIM INTERFACE

Figure 4 presents the main interface of the EpidemiOptim library.

Figure 4: Main running script of the EpidemiOptim library

A.2

E XTENDED RELATED WORKS

Prior to the current COVID-19 pandemics, Yáñez et al. (2019) framed the problem of finding optimal
intervention strategies for a disease spread as a reinforcement learning problem, focusing on how
to design environments in terms of disease model, intervention strategy, reward function, and state
representations. In Alamo et al. (2020), the CONCO-Team (CONtrol COvid-19 Team) provides a
detailed SWOT analysis (Strengths, Weaknesses, Opportunities, Threats) and a roadmap that goes
from the access to data sources to the final decision-making step, highlighting the interest of standard
optimization methods such as Optimal Control Theory, Model Predictive Control, Multi-objective
control and Reinforcement Learning. However, as argued in Shearer et al. (2020), a decision model
for pandemic response cannot capture all of the social, political, and ethical considerations that
impact decision-making. It is is therefore a central challenge to propose tools that can be easily
used, configured and interpreted by decision-makers.
The aforementioned contributions provide valuable analyses on the range of methods and challenges
involved in applying optimization methods to assist decision making during pandemics events. They
do not, however, propose concrete implementations of such systems. Several recent contributions
have proposed such implementations. Yaesoubi et al. (2020) applied an approximate policy iteration
algorithm of their own (Yaesoubi & Cohen, 2016) on a SEIR model calibrated on the 1918 Influenza
Pandemic in San Francisco. An interesting contribution of the paper was the development of a
pragmatic decision tool to characterize adaptive policies that combined real-time surveillance data
with clear decision rules to guide when to trigger, continue, or stop physical distancing interventions.
Arango & Pelov (2020) applied a more standard RL algorithm (Double Deep Q-Network) on a SEIR
model of the COVID-19 spread to optimize cyclic lock-down timings. The reward function was a
fixed combination of a health objective (minimization of overshoots of ICU bed usage above an ICU
bed threshold) and an economic objective (minimization the time spent under lock-downs). They
compared the action policies optimized by the RL algorithm with a baseline on/off feedback control
(fixed-rule), and with different parameters (reproductive number) of the SEIR model. Libin et al.
(2020) used a more complex SEIR model with coupling between different districts and age groups.
They applied the Proximal Policy Optimization algorithm to learn a joint policy that control the
districts using a reward function quantifying the negative loss in susceptibles over one simulated
week, with a constraint on the school closure budget. Probert et al. (2019) used Deep Q-Networks
(DQN) and Monte-Carlo control on a stochastic, individual-based model of the 2001 foot-and-mouth
13

disease outbreak in the UK. An original aspect of their approach was to define the state at time t
using an image of the disease outbreak to capture the spatial relationships between farm locations,
allowing the use of convolutional neural networks in DQN. Other contributions applied non-RL
optimization methods such as deterministic rules (Tarrataca et al., 2020), stochastic approximation
algorithms (Yaesoubi et al., 2020) or Bayesian optimization (Chandak et al., 2020). This latter paper
also proposes a stochastic agent-based model called VIPER (Virus-Individual-Policy-EnviRonment)
allowing to compare the optimization results on variations of the demographics and geographical
distribution of population. Finally, Miikkulainen et al. (2020) proposed an original approach using
Evolutionary Surrogate-assisted Prescription (ESP). In this approach, a recurrent neural network (the
Predictor) was trained with publicly available data on infections and NPIs in a number of countries
and applied to predicting how the pandemic will unfold in them in the future. Using the Predictor, an
evolutionary algorithm generated a large number of candidate strategies (the Prescriptors) in a multiobjective setting to discover a Pareto front that represents different tradeoffs between minimizing the
number of COVID-19 cases, as well as the number and stringency of NPIs (representing economic
impact).
As we have seen, existing contributions widely differ in their definition of epidemiological models,
optimization methods, cost functions, state and action spaces, as well as methods for representing the
model decisions in a format suitable to decision-makers. Our approach aims at providing a standard
toolbox facilitating the comparison of different configurations along these dimensions in order to
assist decision-makers in the evaluation and the interpretation of the range of possible intervention
strategies.
A.3
A.3.1

A DDITIONAL METHODS
O PTIMIZATION ALGORITHMS

The Deep Q-Network algorithm. DQN was introduced in (Mnih et al., 2015) as an extension
of the Q-learning algorithm (Watkins & Dayan, 1992) for policies implemented by deep neural
networks. The objective is to train a Q-function to approximate the value of a given state-action pair
(s, a). This value can be understood as the cumulative measure of reward that the agent can expect
to collect in the future by performing action at now and following an optimal policy afterwards. In
the Q-learning algorithm, the Q-function is trained by:
Q(st , at ) ← Q(st , at ) + α[rt+1 + γ maxa Q(st+1 , a) − Q(st , at )].
Here, α is a learning rate, [rt+1 + γ maxa Q(st+1 , a) − Q(st , at )] is called the temporal difference
(TD) error with rt+1 +γ maxa Q(st+1 , a) the target. Indeed, the value of the current state-action pair
Q(st , at ) should be equal to the immediate reward rt+1 plus the value of the next state Q(st+1 , at+1 )
discounted by the discount factor γ. Because we assume we behave optimally after the first action,
action at+1 should be the one that maximizes Q(st+1 , a). Once this function is trained, the agent
simply needs to take the action that maximize the Q-value in its current state: a∗ = maxa Q(st , a).
Deep Q-Network only brings a few mechanisms to enable the use of large policies based on deep
neural network see Mnih et al. (2015) for details.
Goal-parameterized DQNs. Schaul et al. (2015) introduced Universal Value Function Approximators: the idea that Deep RL agents could be trained to optimize a whole space of cost functions
instead of a unique one. In their work, cost functions are parameterized by latent codes called goals.
An agent in a maze can decide to target any goal position within that maze. The cost function is
then a function of that goal. To this end, the agent is provided with its current goal. In the traditional
setting, the goal was concatenated to the state of the agent. In a recent work, Badia et al. (2020) used
a cost function decomposed into two sub-costs. Instead of mixing the two costs at the cost/reward
level and learning Q-functions from this signal, they proposed to learn separate Q-functions, one for
each cost, and to merge them afterwards: Q(s, a) = (1 − β) Q1 (s, a) + β Q2 (s, a). We use this
same technique for our multi-objective problems.
Using constraints. In RL, constraints are usually implemented by adding a strong cost whenever
the constraint is violated. However, it might be difficult for the Q-function to learn how to model
these sudden jumps in the cost signal. Instead, we take inspiration from the work of Badia et al.
(2020). We train a separate Q-function for each cost, where the associated reward is -1 whenever
14

the constraint is violated. This Q-function evaluates how many times the constraint is expected to
be violated in the future when following an optimal policy. Once this is trained, we can use it to
filtrate the set of admissible actions: if Qconstraint (s, a) > 1, then we expect action a to lead to at least
1 constraint violation. Once these actions are filtered, the action is selected according to the usual
maximization of the traditional Q-function.
NSGA-II. Multi-objective algorithms do not require the computation of an aggregated cost but use
all costs. The objective of such algorithms is to obtain the best Pareto front. NSGA-II (Deb et al.,
2002) is a state-of-the-art multi-objective algorithm based on EAs (here a genetic algorithm). Starting from a parent population of solutions, parents are selected by a tournament selection involving
the rank of the Pareto front they belong to (fitness) and a crowding measure (novelty). Offspring are
then obtained by cross-over of the parent solutions and mutations of the resulting parameters. The
offspring population is then evaluated and sorted in Pareto fronts. A new parent population is finally
obtained by selecting offspring from the higher-ranked Pareto fronts, prioritizing novel solutions as
a second metric (low crowding measures).
A.3.2

E PIDEMIOLOGICAL MODEL

Figure 5 presents the schematic view of the mechanistic model behind the SEIRAH model from
Prague et al. (2020) (left), and its underlying system of differential equations (right). See details in
the original paper (Prague et al., 2020).

Figure 5: Description of the SEIRAH epidemiological model. Adapted from Prague et al. (2020)
To account for uncertainty in the model parameters, we use a distribution of models. At each episode,
the epidemiological model is sampled from that distribution. The transition function is thus conditioned on a latent code (the model parameters) that is unknown to the agent. This effectively results
in a stochastic transition from the viewpoint of the agent. To build the distribution of models, we
assume a normal distributions for each of the model parameters, using either the standard deviation
from the model inversion for parameters estimated in Prague et al. (2020) or 10% of the mean value
for other parameters. Those values are available in appendix Figure 6 features for reproducibility of
the experiment. We further add a uniformly distributed delay between the epidemic onset and the
start of the learning episode (uniform in [0, 21 days]). This delay models the reaction time of the
decision authorities. This results in the distribution of models shown in Figure 7.
Epidemiological model and economic cost function parameters. Table 1 lists the parameters
of the epidemiological model (from Prague et al. (2020)) and the economic cost function (from the
National Institute of Statistics and Economical Analysis Insee (2020) and Havik et al. (2014)).
Epidemiological model and cost function parameters. Table 1 presents the list of parameters
used in the epidemiological model (top) and the economic cost function (bottom).

15

Param.
b0
r
α
De
DI
Dq
Dh
N
β
S0
E0
I0
R0
A0
H0
D0
K0
L0
λ
Y0
A
u(t)
γk

Interpretation
Transmission rate of ascertained cases before lock-down
Ascertainement rate
Ratio of transmission between A and I
Latent (incubation) period (days)
Infectious period (days)
Duration from I onset to H (days)
Hospitalization period (days)
Population size
Lock-down effects first (βw1 ), second (βw2 ),
third (βw3 ) and thereafter weeks (βw0 )
Initial number of susceptible
Initial number of exposed
Initial number of ascertained
Initial number of removed
Initial number of non ascertained
Initial number of hospitalized
Initial number of death
Initial capital stock in millions euros
Number of employed individuals in millions
Activity/employment rate (%)
Initial GDP in million euros / year
Exogenous technical progress
Level of partial unemployment during lock-down (%)
Capital elasticity

Value ”Ile-de-France”
2.23
0.043
0.55
5.1
2.3
0.36
30
12,278,210
(-0.11, -0.50,
-1.36, -1.46)
N − E0 − I0 − R0 − A0 − H0
5004
16
0
356
4
0
1,388,912
4.58
37.4
424,474
867
50
0.37

Table 1: Parameter values for the Île-de-France region for the SEIRAH epidemiological model and
the economy cost function.

Figure 6: Variance-covariance matrix for parameters of the epidemiological model.

16

A.4

A DDITIONAL RESULTS

A.4.1

A DISTRIBUTION OF MODELS

Figure 7 presents the evolution of the SEIRAH model states for a few models sampled from the
distribution of models described in 4.1. Here, no lockdown is enforced.

2

0

100 200 300

0

0

100 200 300

2

0

100 200 300

0

100 200 300

0

100 200 300

2.3

1

0

0

Transmission rate

20000

Total GDP Loss (B)

New GDP Loss (B)

0.015
0.010
0.005
0.000

4

40000

1000
0

Total Deaths

New Deaths

60000

2000

Aggregated Cost

Eco cost: 1.85 B, Death Cost: 57649, Aggregated Cost: 45.27

2.2

0

100 200 300

days

Figure 7: 10 models sampled from the distribution of epidemiological models used in the case-study.
Now, let us illustrate in details the results of the case study.
A.5

PARETO FRONTS COMPARISONS

Comparing multi-objective algorithms is not an easy task. The representation of the results itself
is not trivial. Figure 2a, for example, presents the standard error of the mean in the GDP cost
dimension, but cannot simultaneously represent the error in the other dimension. For this purpose,
we can rotate the graph and present the standard error of the mean in the health cost dimension (see
Figures 8a and 8b).
DQN
NSGA-II (same budget)

80

# Deaths (×103)

GDP cost (B)

200

Goal DQN
NSGA-II (budget x15)
GOAL DQN-C

150
100
50
00

10

20

30

40

# Deaths (×103)

50

(a)

DQN
NSGA-II (same budget)

60

40

20

00

60

Goal DQN
NSGA-II (budget x15)
GOAL DQN-C

25

50

75

100

125

GDP cost (B)

150

175

(b)

Figure 8: Pareto fronts for various optimization algorithms. Left: means and standard errors of
the mean are computed in the economic cost dimension. right: they are computed in the health
cost dimension (over 10 seeds). Stars indicate significantly higher performance w.r.t. the DQN
condition while circles indicate significantly lower performance. We use a Welch’s t-test with level
of significance α = 0.05.
Statistical comparisons. We run each algorithm for 10 seeds to take into account the stochasticity
of the learning process and assess the robustness of the algorithms. Rigorous algorithm comparison
requires to compute statistical tests to assess the significance of differences in the mean performance
17

(e.g. t-test, see Colas et al. (2019) for a discussion). Here, there are two dimensions resulting from
the two costs. One way of handling this is to compare algorithms on one dimension, keeping the
other constant. Figures 8a and 8b illustrate this process. In Figure 8a, the circles and stars evaluate
the significance of differences (negative and positive w.r.t. the DQN performance respectively) in
the economic cost dimension while the health cost is kept constant. In Figure 8b, we do the reverse
and compare health costs, keeping economic costs constant.
Areas under the curve. One way to compare multi-objective algorithms is to compare the area
under their normalized Pareto fronts (lower is better). We normalize all Pareto front w.r.t the minimum and maximum values on the two dimensions across all points from all experiments. We can
then perform Welch’s t-tests to evaluate the significance of the difference between the mean areas
metrics of each algorithm.

DQN
Goal DQN
Goal DQN-C
NSGA-II (same)
NSGA-II (x15)

DQN

Goal DQN

Goal DQN-C

NSGA-II (same)

NSGA-II (x15)

N/A

0.069
N/A

0.55
0.046
N/A

0.018
0.84
0.21
N/A

0.045
0.0057
0.35
3.2 × 104
N/A

Table 2: Comparing areas under the Pareto fronts. The number in row i, column j compares algorithm i and j by providing the p-value of the Welch’s t-test assessing the significance of their difference. Bold colored numbers indicate significant differences, green indicate positive differences
(column algorithm performs better than row algorithm), while red indicate negative differences (column algorithm performs worse).
Now that we compared algorithms on average metrics, let us delve into each algorithm and look at
particular runs to illustrate their respective strategies.

18

A.5.1

DQN

We first present strategies found by DQN.

1220

1160
80

0

100 200 300

8
6
4
2
0

100 200 300

R (×104)

40
20
0

100 200 300

0

100 200 300

0

100 200 300

1.0

A (×104)

60

0

H (×104)

1180

400
300
200
100
0

I

E (×104)

S (×104)

1200

20
15
10
5
0

0.5

0

100 200 300

0.0

days

0

100 200 300

0.4276
0

100 200 300

Aggregated Cost

2000

0.4278

0.4274

0.4

0

150
100
50
0

0.2

0

100 200 300

0.0

0

100 200 300

00

100 200 300

Transmission rate

New GDP Loss (B)

New Deaths

150
100
50
0

Total GDP Loss (B) Total Deaths

Eco cost: 155.16 B, Death Cost: 3513, Aggregated Cost: 5.40

2
1

0

100 200 300

days

Figure 9: DQN trained with β = 0.. For one run, this figure shows the evolution of model states
(above) and states relevant for optimization (below). The aggregated cost is shown for various
values of β in [0, 0.25, 0.5, 0.75, 1]. For β = 0, the agent only cares about the health cost, it always
locks-down.

19

60

2

40

I

1

10

0

A (×104)

0.5
0.0

100 200 300

0

100 200 300

1.0

5
00

0

100 200 300

20
0.20
0.15
0.10
0.05
0.00

0

100 200 300

0

100 200 300

H (×104)

0

R (×104)

3

E (×104)

S (×104)

1225.0
1222.5
1220.0
1217.5

0

100 200 300

days

0

100

200

300

0.4
0.2
0.0

0.2

200

0

100

200

300

0

60
40
20
0

0

100

200

300

0.0

0

100

200

300

00

100

200

300

Transmission rate

New GDP Loss (B)

0

0.4

400

10

Aggregated Cost

600

20

Total GDP Loss (B) Total Deaths

New Deaths

Eco cost: 62.81 B, Death Cost: 556, Aggregated Cost: 31.84

2
1

0

100

200

days

300

Figure 10: DQN trained with β = 0.5. Here the strategy is cyclical with a one or two weeks
period. For one run, this figure shows the evolution of model states (above) and states relevant for
optimization (below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

20

300
1220

1180

0

100 200 300

100
0

A (×104)

R (×104)

2

20
0

0

100 200 300

0

100 200 300

4

40

0

5

0.8
0.6
0.4
0.2
0.0

0

100 200 300

0

100 200 300

H (×104)

60

0

200

I

E (×104)

S (×104)

10

1200

0

100 200 300

days

100 200 300

0

0

100 200 300

60

100 200 300

0

100 200 300

00

100 200 300

1

20

0

0

2

40

0.2

0.0

Transmission rate

0

Total GDP Loss (B)

New Deaths

New GDP Loss (B)

0.2

1000

0.4

0.0

0.4

2000

50
0

Total Deaths

3000

100

Aggregated Cost

Eco cost: 59.41 B, Death Cost: 2804, Aggregated Cost: 34.86

0

100 200 300

days

Figure 11: DQN trained with β = 0.55. Here the agent first lock-down to stop the first wave, but
then stops the cyclical lockdown early which induces a second wave later, where the agent also
reacts by a lock-down. For one run, this figure shows the evolution of model states (above) and
states relevant for optimization (below). The aggregated cost is shown for various values of β in
[0, 0.25, 0.5, 0.75, 1].

21

0

4000

100

2000

0

100 200 300

I
0

H (×104)

A (×104)

0

100 200 300

0

100 200 300

10

25

100 200 300

0

15

50

0

0

100 200 300

75

R (×104)

1000
750
500
250
0

200

E (×104)

S (×104)

1250
1000
750
500
250

0

5
0

100 200 300

days

40000

100 200 300

0.005
0

100 200 300

0
1.5
1.0
0.5
0.0

1

0

100 200 300

0

Transmission rate

0

0.010
0.000

2

20000

Total GDP Loss (B)

New GDP Loss (B)

1000
0

3

Total Deaths

New Deaths

2000

Aggregated Cost

Eco cost: 1.74 B, Death Cost: 56621, Aggregated Cost: 31.62

0

100 200 300

0

100 200 300

2.3
2.2

0

100 200 300

days

Figure 12: DQN trained with β = 0.65. Here the agent mostly cares about the economic cost, which
results in a no-lockdown policy. For one run, this figure shows the evolution of model states (above)
and states relevant for optimization (below). The aggregated cost is shown for various values of β
in [0, 0.25, 0.5, 0.75, 1].

22

A.5.2

G OAL -DQN WITHOUT CONSTRAINTS

Now we present a few strategies found by Goal-DQN without constraints.

1220

E (×104)

S (×104)

6

0

100

200

0

300

0

100

200

300

3

20

100

200

0

300

200

300

0

100

200

300

0.4

1

0

100

0.6

2

10

0

H (×104)

R (×104)

30

2

A (×104)

1200

I

4

1210

200
150
100
50
0

0.2

0

100

200

days

300

0.0

0

100

200

300

0

0

100

200

300

100

0.2
0

100

200

300

0.0

0

100

200

300

00

100

200

300

2

50
0

Aggregated Cost

0.2

500

0.4

0.0

0.4

1000

Transmission rate

New GDP Loss (B)

New Deaths

60
40
20
0

Total GDP Loss (B) Total Deaths

Eco cost: 92.74 B, Death Cost: 1350, Aggregated Cost: 20.21

1

0

100

200

days

300

Figure 13: Goal-DQN without constraints evaluated in β = 0.2. Here the agent starts with a lasting
lockdown, then pursues with cyclical lock-downs, which ensures the absence of second wave and,
thus, results in low health cost but high economic cost. For one run, this figure shows the evolution
of model states (above) and states relevant for optimization (below). The aggregated cost is shown
for various values of β in [0, 0.25, 0.5, 0.75, 1].

23

100

200

300

100

I

1210
1200 0

150

E (×104)

S (×104)

1220

8
6
4
2
0

50
0

100

200

3

00

200

0

300

200

300

0

100

200

300

0.2

1

100

100

H (×104)

R (×104)

2

10

0

0.4

A (×104)

20

0

300

0

100

200

days

300

0.0

200

300

0.2
0

100

200

300

0
60
40
20
0

Aggregated Cost

100

0.2

500
0

100

200

300

0.0

0

100

200

300

00

100

200

300

Transmission rate

0

0.4

0.0

0.4

1000

Total GDP Loss (B)

New GDP Loss (B)

New Deaths

60
40
20
0

Total Deaths

Eco cost: 59.84 B, Death Cost: 1301, Aggregated Cost: 28.03

2
1

0

100

200

days

300

Figure 14: Goal-DQN without constraints evaluated in β = 0.65. Here we find a cyclical strategy
equivalent to the one of shown with a DQN agent trained with β = 0.5. For one run, this figure shows the evolution of model states (above) and states relevant for optimization (below). The
aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

24

2000

I

100

1000

0

100 200 300

0

80
60
40
20
0

500
0

100 200 300

0

100 200 300

0

100 200 300

15

A (×104)

1000

0

100 200 300

H (×104)

500
0

R (×104)

3000

E (×104)

S (×104)

1000

0

4000

200

10

0

5
0

100 200 300

days

60000

100 200 300

0

1

0

100 200 300

0

100 200 300

100 200 300

0

100 200 300

2.1

2
0

0

2.2

4

0.2

0

Transmission rate

0

Total GDP Loss (B)

New GDP Loss (B)

2

20000

0.4

0.0

3

40000

1000
0

Total Deaths

New Deaths

2000

Aggregated Cost

Eco cost: 4.66 B, Death Cost: 55507, Aggregated Cost: 28.88

2.0

0

100 200 300

days

Figure 15: Goal-DQN without constraints evaluated in β = 0.7. Here the agent only use a lockdown at the very beginning of the epidemic. It is unclear whether this has an impact on the health
cost at all. For one run, this figure shows the evolution of model states (above) and states relevant for
optimization (below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

25

A.5.3

G OAL -DQN WITH CONSTRAINTS

Now we present a few strategies found by Goal-DQN with constraints, and study how it reacts to
health and economic constraints.

1.0

20

I

1224

0.5

0

100 200 300

6

0.0
0.8

10
0

0.0

0

100 200 300

H (×104)

A (×104)

R (×104)

100 200 300

100 200 300

0.05

0.2

0

0

0.10

0.4

2

0

100 200 300

0.6

4

0

30

E (×104)

S (×104)

1226

1222

40

1.5

0

100 200 300

0.00

days

New GDP Loss (B)

10
0

0.2

100

0

100

200

300

0.4
0.2
0.0

0.4

200

5

0

100

200

300

0

75
50
25
0

Transmission rate Aggregated Cost

300

Total GDP Loss (B) Total Deaths

New Deaths

Eco cost: 80.75 B, Death Cost: 297, Aggregated Cost: 24.55

0

0

100

100

200

200

days

300

300

0.0

0

100

200

300

1.5
1.0
0.5
0.0 0

100

200

300

Figure 16: Goal-DQN with constraints evaluated in β = 0.3, Meconomic = 160B, Mhealth =
62000 deaths. This boils down to no constraints as they are maximal values. This leads to a cyclical
policy. For one run, this figure shows the evolution of model states (above) and states relevant for
optimization (below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

26

1250

150

E (×104)
0

750
500
250
0

I

50
0

100 200 300

0

0

80
60
40
20
0

100 200 300

4000
3000
2000
1000
0

100 200 300

15

0

100 200 300

0

100 200 300

10

H (×104)

500

1000

R (×104)

100

750

A (×104)

S (×104)

1000

0

5
0

100 200 300

days

100 200 300

0

1

0

100 200 300

40

0.4

0

100 200 300

0

0

0

100 200 300

00

100 200 300

2

20

0.2
0.0

2

20000

0

Transmission rate Aggregated Cost

40000

Total GDP Loss (B)

New GDP Loss (B)

New Deaths

1500
1000
500
0

Total Deaths

Eco cost: 42.74 B, Death Cost: 45278, Aggregated Cost: 61.58

1

0

100 200 300

days

Figure 17: Goal-DQN with constraints evaluated in β = 0.3, Meconomic = 55B, Mhealth =
62000 deaths. This is the same β as Figure 16 (previous page). Now there is no constraint on
the number of deaths but a strong constraint on the economic cost. The strategy is not cyclical anymore, as the resulting economic cost would be too high. This strategy stays below the economic
constraint but still tries to minimize the health cost. For one run, this figure shows the evolution of
model states (above) and states relevant for optimization (below). The aggregated cost is shown for
various values of β in [0, 0.25, 0.5, 0.75, 1].

27

80
60
40
20
0

R (×104)

0

100 200 300

I

4000
3000
2000
1000
0

0

100 200 300

0

100 200 300

0

100 200 300

15

H (×104)

100 200 300

A (×104)

0
1000
750
500
250
0

200
150
100
50
0

E (×104)

S (×104)

1250
1000
750
500
250

10

0

5
0

100 200 300

days

2

20000

100 200 300

0.010
0.005
0.000 0

100 200 300

0
1.5
1.0
0.5
0.0

1

0

100 200 300

0

Transmission rate

0

Aggregated Cost

40000

Total GDP Loss (B)

New GDP Loss (B)

1500
1000
500
0

Total Deaths

New Deaths

Eco cost: 1.73 B, Death Cost: 54604, Aggregated Cost: 26.41

0

100 200 300

0

100 200 300

2.3
2.2

0

100 200 300

days

Figure 18: Goal-DQN with constraints evaluated in β = 0.7, Meconomic = 160B, Mhealth =
62000 deaths. There is no constraint. As the balance favorizes the economic cost, the strategy does
not implement any lock-down. For one run, this figure shows the evolution of model states (above)
and states relevant for optimization (below). The aggregated cost is shown for various values of β
in [0, 0.25, 0.5, 0.75, 1].

28

40
30
20
10
0

800
0

100

200

300

400
200

0

100

200

100

200

200

300

0

100

200

300

2

5
0

300

100

H (×104)

A (×104)

R (×104)

0

0

3

10

200

0

300

15

400

0

600

I

1000

600

800

E (×104)

S (×104)

1200

1

0

100

200

days

0

300

0.4

20000

0.2

0

100 200 300

0.2
0

100 200 300

0
75
50
25
0

0

100 200 300

0.0

Transmission rate

10000

0.4

0.0

Total Deaths

30000

Total GDP Loss (B)

New GDP Loss (B)

New Deaths

300
200
100
0

Aggregated Cost

Eco cost: 78.37 B, Death Cost: 27214, Aggregated Cost: 67.42

0

100 200 300

0

100 200 300

2
1

0

100 200 300

days

Figure 19: Goal-DQN with constraints evaluated in β = 0.7, Meconomic = 160B, Mhealth =
30500 deaths. Now we have the same setup as the previous page (Figure 18), except that we have
a strong constraint on the number of deaths. The resulting strategy respect the constraint while
attempting to minimize the economic cost. For one run, this figure shows the evolution of model
states (above) and states relevant for optimization (below). The aggregated cost is shown for various
values of β in [0, 0.25, 0.5, 0.75, 1].

29

15

E (×104)

S (×104)

1200

10

1000
0

5
0

100 200 300

300

200

I

1100

100
0

100 200 300

6

0

100 200 300

0

100 200 300

0

100 200 300

1.0

2

0

2.0

H (×104)

4

100

0
1.5

A (×104)

R (×104)

200

0

300

0.5

0

100 200 300

0.0

days

0.4
0.2
0.0

0

100 200 300

60
40
20
0

0.4
0.2

0

100 200 300

0.0

0

100 200 300

00

100 200 300

Transmission rate

100 200 300

Total Deaths

0

15000
10000
5000
0

Total GDP Loss (B)

New GDP Loss (B)

New Deaths

150
100
50
0

Aggregated Cost

Eco cost: 68.56 B, Death Cost: 15114, Aggregated Cost: 928.43

2
1

0

100 200 300

days

Figure 20: Goal-DQN with constraints evaluated in β = 0.3, Meconomic = 55B, Mhealth =
15000 deaths. Here we have strong constraints on both economic costs. In that case, there is no
good solution. This strategy respects the health constraint but violates the economic constraint. For
one run, this figure shows the evolution of model states (above) and states relevant for optimization
(below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

30

A.5.4

NSGA-II

Now we present a few strategies found by NSGA-II.

40
30
20
10
0

2

E (×104)

S (×104)

R (×104)

10.0
7.5
5.0
2.5
0.0 0

0

100 200 300

0.8
0.6
0.4
0.2
0.0

0

100 200 300

100 200 300

0

100 200 300

0

100 200 300

0.10

A (×104)

0

I

1

H (×104)

1227.5
1225.0
1222.5
1220.0
1217.5

0.05

0

0.00

100 200 300

days

0

100

200

300

0.2
0

100

200

300

0

75
50
25
0

Aggregated Cost

0.2

200

0.4

0.0

0.4

400

0

100

200

300

0.0

0

100

200

300

00

100

200

300

Transmission rate

New GDP Loss (B)

15
10
5
0

Total GDP Loss (B) Total Deaths

New Deaths

Eco cost: 80.76 B, Death Cost: 538, Aggregated Cost: 40.79

2
1

0

100

200

days

300

Figure 21: NSGA-II, the closest point to [8000 deaths, 60B] in the Pareto front. In this low health
cost regime, NSGA-II finds a cyclical strategy with a period of 2 weeks. For one run, this figure shows the evolution of model states (above) and states relevant for optimization (below). The
aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

31

3000
100

I

2000

50

1000

0

100 200 300

0

60

H (×104)

20
0

100 200 300

0

100 200 300

0

100 200 300

10

40

0

0

100 200 300

A (×104)

0

R (×104)

1000
750
500
250
0

E (×104)

S (×104)

1250
1000
750
500
250

0

5
0

100 200 300

days

0

100 200 300

0.4

0

0

100 200 300

0

100 200 300

5
0

0

0

100 200 300

0

100 200 300

2

10

0.2
0.0

1

20000

Total GDP Loss (B)

New GDP Loss (B)

0

2

40000

500

Transmission rate Aggregated Cost

1000

Total Deaths

New Deaths

Eco cost: 13.53 B, Death Cost: 53414, Aggregated Cost: 47.85

1

0

100 200 300

days

Figure 22: NSGA-II, the closest point to [30000 deaths, 40B] in the Pareto front. Here we find that
NSGA-II use alternative strategies depending on the epidemiological models it faces. The average
of these strategies ends up close to 30000, although the two strategies either find high health costs
(>50000) or low ones (<1000). These plots show the first alternative, where the strategy aims at
breaking the first wave of the epidemic. See the other alternative in Figure 23. For one run, this
figure shows the evolution of model states (above) and states relevant for optimization (below). The
aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

32

3000
100

I

2000

50

1000

0

100 200 300

0

60

H (×104)

20
0

100 200 300

0

100 200 300

0

100 200 300

10

40

0

0

100 200 300

A (×104)

0

R (×104)

1000
750
500
250
0

E (×104)

S (×104)

1250
1000
750
500
250

0

5
0

100 200 300

days

0

100 200 300

0.4

0

0

100 200 300

0

100 200 300

5
0

0

0

100 200 300

0

100 200 300

2

10

0.2
0.0

1

20000

Total GDP Loss (B)

New GDP Loss (B)

0

2

40000

500

Transmission rate Aggregated Cost

1000

Total Deaths

New Deaths

Eco cost: 13.53 B, Death Cost: 53414, Aggregated Cost: 47.85

1

0

100 200 300

days

Figure 23: NSGA-II, the closest point to [30000 deaths, 40B] in the Pareto front. Here we find that
NSGA-II use alternative strategies depending on the epidemiological models it faces. The average
of these strategies ends up close to 30000, although the two strategies either find high health costs
(>50000) or low ones (<1000). These plots show the second alternative, where the strategy is
cyclical and achieved low health costs. See Figure 22 for the first alternative. For one run, this
figure shows the evolution of model states (above) and states relevant for optimization (below). The
aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

33

100 200 300

I
0

80
60
40
20
0

100 200 300

A (×104)

R (×104)

500
250
0

100 200 300

4000
3000
2000
1000
0
15

H (×104)

0

750

0

200
150
100
50
0

E (×104)

S (×104)

1200
1000
800
600
400

0

100 200 300

0

100 200 300

10

0

5
0

100 200 300

days

100 200 300

0.4

0

1

0

100 200 300

0

100 200 300

0

100 200 300

00

100 200 300

1

5
0

0
2

10

0.2
0.0

2

20000

0

Transmission rate Aggregated Cost

40000

Total GDP Loss (B)

New GDP Loss (B)

New Deaths

1500
1000
500
0

Total Deaths

Eco cost: 13.26 B, Death Cost: 43612, Aggregated Cost: 40.18

0

100 200 300

days

Figure 24: NSGA-II, the closest point to [45000 deaths, 20B] in the Pareto front. NSGA-II seem to
find a robust strategy that consists in a single lock-down of a few weeks to break the first wave. For
one run, this figure shows the evolution of model states (above) and states relevant for optimization
(below). The aggregated cost is shown for various values of β in [0, 0.25, 0.5, 0.75, 1].

34

