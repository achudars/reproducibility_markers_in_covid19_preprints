Stochastic Optimization for Vaccine and Testing Kit Allocation for the
COVID-19 Pandemic
Lawrence Thula,∗, Warren Powellb
a Department

arXiv:2101.01204v1 [cs.AI] 4 Jan 2021

b Department

of Electrical Engineering, Princeton University, Princeton, New Jersey,USA

of Operations Research and Financial Engineering, Princeton University, Princeton, New Jersey,USA

Abstract
The pandemic caused by the SARS-CoV-2 virus has exposed many flaws in the decision-making strategies
used to distribute resources to combat global health crises. In this paper, we leverage reinforcement learning
and optimization to improve upon the allocation strategies for various resources. In particular, we consider a
problem where a central controller must decide where to send testing kits to learn about the uncertain states
of the world (active learning); then, use the new information to construct beliefs about the states and decide
where to allocate resources. We propose a general model coupled with a tunable lookahead policy for making
vaccine allocation decisions without perfect knowledge about the state of the world. The lookahead policy is
compared to a population-based myopic policy which is more likely to be similar to the present strategies in
practice. Each vaccine allocation policy works in conjunction with a testing kit allocation policy to perform
active learning. Our simulation results demonstrate that an optimization-based lookahead decision making
strategy will outperform the presented myopic policy.
Keywords: Decision Processes, Uncertainty Modeling, Reinforcement Learning

1. Introduction

During the early months of 2020, it became evident that the SARS-CoV-2 virus was spreading through
the global population at an alarming rate. As the pandemic devastated global economies and supply chains,
it was obvious that the strategies in place were not sufficient to handle a crisis at this scale. Some of the
obvious challenges presented by this crisis were the distribution of personal protective equipment, testing kits,
and vaccine distribution under extreme shortages, as well as testing accuracy. At the time of this writing, the
world is waiting for a vaccine to suppress the virus in the population and distributing it will present a new
series of logistical issues. This problem is extremely rich with potential to improve distribution processes using
? This

work is supported by the Air Force Office of Scientific Research (Award Number FA9550-19-1-0203).
author
Email address: lathul@princeton.edu (Lawrence Thul)

∗ Corresponding

Preprint submitted to European Journal of Operational Research

January 6, 2021

quantitative strategies; this work will choose a subset of these problems and utilize reinforcement learning and
approximate dynamic programming strategies to improve them.
In this paper, it is assumed that a country/region is in the midst of a pandemic and a decision-maker
is tasked with allocating a limited supply of vaccines and testing kits to various local zones in order to try
to mitigate the spread of the disease. These zones could be states, counties, or some other partition of the
region. A key challenge in this setting is the uncertain number of infected individuals, which can only be
learned by making observations through testing. The data collected through testing would then be used to
create a probabilistic belief about the true underlying state, which gives vaccine distributors useful information
about where to send vaccines to slow the spread. The combination of making resource allocation decisions
under uncertainty, while also performing information collection decisions to improve through active learning,
represents a unique modeling and algorithmic challenge.
The heterogeneous characteristics of population densities, health characteristics and behavior will influence
transmission rates and virus mobility within local regions; hence, intelligently allocated vaccines will impact
the number of infections. The testing decisions are critical to making the best allocation decisions because
the controller needs to have knowledge about the state of the pandemic. The observations made through
testing will reflect biases in the samples caused by factors such as asymptomatic carriers, refusal to get tested,
and false positives/negative errors. It is important to consider the risks caused by the biases because skewed
testing data could eventually lead to poor allocation decisions. The best decisions can be made when each of
these characteristics are properly modeled.
This paper makes the following contributions:
• It provides the first formal model for a high dimensional pandemic resource allocation problem that captures passive information processes, and active learning.. This model is able to capture the uncertainty
in the knowledge about the elements of the SIR model through the belief state, make vaccine allocation
decisions under uncertainty, and perform active learning to improve the quality of the belief state.
• It proposes a vaccine allocation policy which combines a direct lookahead model with a parameterized
approximation of the cost function in order to adjust for risk. The tunable lookahead policy in conjunction with an active learning policy is capable of outperforming a population-based myopic policy.
We demonstrate this through multiple pandemics simulations on toy networks and simulation models of
states in the USA.
• We propose an observation function model in order to capture the biases introduced from asymptomatic
carriers, refusal to get tested, and false positives/negatives. The risk-adjusted stochastic lookahead implementation policy proposed in section 5.1.2 can be tuned to reduce the risks posed by biased sampling.
• We present a robust modeling framework which is capable of working with environment models of

2

increasing complexity. As the epidemiological models used to create a simulator improve, the modeling
framework has the flexibility to adapt to the new data.

The paper is organized as follows. Section 2 summarizes the literature about vaccine distribution, POMDP
modeling, and the unified framework. Section 3 describes the mathematical model for the environment agent
using the unified framework. Section 4 describes the mathematical model for the controlling agent using the
five components of the unified framework. Section 5 describes implementation policies selected from the two
general strategies for designing policies: policy search and lookahead approximations. Section 5 describes
the active learning policy chosen for allocating testing kits. Section 6 describes the results of the model
implemented on an epidemic toy problem and the simulated SARS-CoV-2 pandemic model in the United
States. Section 7 summarizes conclusions from the research.

2. Literature Review

In section 2.1, we give a brief overview of the literature about vaccine distribution strategies during an
epidemic. In section 2.2, we give a brief review of partially observable Markov decision processes (POMDP’s)
and their limitations. In section 2.3, we give an overview about modeling frameworks and their relationship
to the unified framework in this paper.

2.1. Epidemic Planning and Resource Allocation
Planning during an epidemic requires making decisions in many different areas such as enacting social
distancing measures, contact tracing, vaccine distribution, personal protective equipment distribution, and
the allocation of various other essential resources. Making these decisions can be modeled as sequential
decision problems with the common goal of limiting the number of infected individuals. The literature is rich
with policies and algorithms across various different fields to make these decisions intelligently.
The resource distribution problem is multi-faceted. The supply chain to deliver resources from the manufacturing facilities to the recipient has many different steps. For example, some of the decisions which need
to be made are where to send resources, who recieves the resources (i.e. who gets vaccinated at a clinic), and
how to transport the resources to these locations.
The implementation decisions in this paper are concerned with where to send vaccines to have the greatest
reduction in cases. Any sequential decision problem requires a dynamic model of the problem and a policy
for making the decisions using the model. One of the most common ways to model a pandemic is to use
compartmental models, and these are especially useful because they can be formulated as Markov chains. In
1927, Kermack and McKendrick (1927) created the SIR model which is the most basic compartmental model
consisting of three groups within a population: those susceptible (S) to the disease, those infected (I) with
3

the disease, and those removed (R) from the population (from death, recovery, or immunity). Since then,
the compartmental model literature has expanded past the SIR model to include many different extensions.
Some extensions of the SIR model include: the SEIR (Susceptible-Exposed-Infected-Removed) model, SIRS
(Suscepible-Infected-Removed-Susceptible) model, stochastic SIR model, and a spatial SIR model. The SEIR
model is used for diseases with an incubation period before an individual becomes infected. The SIRS model
is used for diseases that do not have permanent immunity. The stochastic SIR model is used for diseases
which have stochastic transmission rates over time. The spatial SIR formulation is used to model a disease
across spatially distributed subpopulations which may interact with one another. In this paper, we focus on
the SIR model with stochastic and spatial extensions; however, there could always be additional extensions
which could make it marginally more realistic.
Most models for planning during epidemics include compartmental models in their logic. Brandeau et al.
(2003) utilizes an SI compartmental model in conjunction with optimization techniques to mitigate the spread
of an infectious disease by allocating resources which suppress transmission rates. Martin et al. (1993) uses a
spatial SEIR model in conjunction with multiple vaccine allocation strategies to mitigate a measles outbreak
amongst dorms on a college campus. Ding et al. (2007), Asano et al. (2008)), Neilan and Lenhart (2011),
and Zakary et al. (2017) use deterministic spatial SIR models and extensions to derive policies from optimal
control theory using variations of the Hamilton-Jacobi-Bellman equations to decide where to allocate vaccines.
Zhang and Prakash (2014) formulates a graphical model of a pandemic with uncertainty in the transmission
rates reflected through the edges in the graph. They propose and compare algorithms to allocate a limited
number of vaccines across the network to minimize the resulting footprint.
Another decision is who receives a set of allocated resources. Patel et al. (2005) uses a stochastic influenza
model along with a genetic algorithm to decide which age groups should receive a limited supply of vaccines.
Medlock and Galvani (2009) uses models of past flu pandemics to compare policies for administering a limited
supply of vaccines among various age groups. Ekici et al. (2008) uses a complex spatial SEIR model in
conjunction with an age-based component to decide who to feed during a flu pandemic in Georgia by setting
up food distribution centers.
There are various other decisions studied among the literature to optimize a vaccine supply chain. For
example, Lin et al. (2020) models a problem to decide whether a distributor will transport vaccines through a
cold chain or a non-cold chain to ensure that they are still viable at administration. Dai et al. (2016) models
an influenza supply chain in the U.S. consisting of healthcare providers, manufacturers, and distributors to
ensure on-time delivery to each provider.

2.2. The POMDP model
This paper will assume that the true pandemic has a compartmental model; however, we assume that the
controller does not have access to this model and must make decisions about where to test to learn about the
4

true state of the pandemic (active learning). If the true state is only partially observable, then the controller
must model the uncertainty to make decisions. There are various modeling strategies to handle decisions under
uncertainty. The next section discusses the modeling framework for partially observable Markov decision
processes (POMDP), and its limitations in this problem.
A partially-observable Markov Decision Process (POMDP) is a system which includes a controlling agent
making decisions in an environment where the underlying state transitions are Markov; however, the true
underlying state of the environment can only be observed partially by the controlling agent. The finite
horizon POMDP problem is modeled using the tuple (S, X , P, R, W, Pobs , T ).
• S - the state space of the environment (e.g. number of infected people in the population).
• X - the decision space (e.g. number of vaccines sent to each zone).
• P(s0 |s, x) - the probability transition matrix for the next environment state given a state-action pair.
• R(s, x) - the reward function for a given state-action pair.
• W - the observation space (e.g. number of new infections measured through testing).
• Pobs (w|s, x) - the probability of observing w ∈ W given a state action pair.
• T - the time horizon.
For the POMDP model, let st be the state variable for the environment agent. The goal of a POMDP is the
same goal as an MDP: to find a policy, X π , which maximizes (or minimizes) the expected sum of cumulative
rewards,
(
π

max E
π

T
X

)
R(st , Xtπ (st ))|s0

.

(1)

t=0

The expectation over π suggests that the stochastic processes in the problem are impacted by the decisions
made by the policy. In a POMDP, the true state variable, st , is unknown to the controlling agent at time t,
hence the value of the state cannot be computed. If the true state variable is unknown then the state transitions
are non-Markovian (from the controlling agents perspective); hence, the entire history of observations and
actions would be required to make decisions. If the entire history of observations had to be stored this would
grow extremely quickly. The rapid growth of the history of actions and observations is known as the curse of
history.
To circumvent the curse of history, the controlling agent stores a belief state variable, bt (s) ∀s ∈ S. bt (s)
P
is a probability distribution over the (presumably discrete) state space, hence s∈S bt (s) = 1. Astrom (1965)
shows that the belief state is a sufficient statistic for the history and describes a distribution over the possible

5

states of the environment at time t derived from the previous observations, actions, and states. It is defined
by,
bt (s) = P(st |Wt , st−1 , xt−1 , Wt−1 , ..., s0 , x0 ) = P(st |Wt , bt−1 (s)).
When new observations arrive the agent must perform a Bayesian update to compute this new belief state
using only the new observations and the previous belief state. Using the terms in the tuple, we can compute
it with,
P
Pobs (Wt+1 = w|s0 , xt ) s∈S P(s0 |s, xt )bt (s)
P
.
obs (W
0
0
t+1 = w|s , xt )
s0 ∈S P
s∈S P(s |s, xt )bt (s)

(2)

bt+1 (s) = P

In equation (2), the state space must be summed over; if it is a vector then |S| grows exponentially. Additionally, computing the transition matrix P(s0 |s, x) and observation matrix, P(w|s0 , x), when the observations and
decisions are vectors will also be intractable. Powell (2011) discusses the reasons for the intractability of equation (2) known as the three curses of dimensionality. In theory, the solution to the belief MDP can then be used
P
to solve for the optimal policy using Bellman’s Equation Ross et al. (2008). Let r(bt , xt ) = s∈S R(s, xt )bt (s),
then Bellman’s optimality equation for a POMDP is given by,
!
∗

π = arg max r(bt , xt ) +
xt

X

0

P(Wt+1 = w|s

∗
, xt )Vt+1
(bt+1 (s0 ))

.

(3)

w∈W

It is important to note that bt+1 (s0 ) must be calculated using equation (2). For finite horizon problems the
exact solution is only tractable when the state space, action space, and observation space are small. The
belief state is an uncountable continuous set, therefore trying to find a mapping for each element of this set
using backward dynamic programming is not possible. However, it is a well known result from the POMDP
literature that the value function for belief states is still piecewise linear and convex (e.g. Pineau et al. (2006)),
which implies that an optimal solution exists. The ability to find the exact optimal solution is almost never
possible for real world problems, in fact, the finite horizon POMDP is PSPACE-complete (e.g. Pineau et al.
(2006)). Pineau et al. (2006) gives a set of solutions using a point-based value iteration approach and compares
it to other POMDP solvers. Ross et al. (2008) derives online planning algorithms for the POMDP problem.
Hoey and Poupart (2005) formulates strategies for solving POMDPs with continuous and multi-dimensional
observations spaces. Roy et al. (2005) attempts to circumvent the curse of dimensionality by finding a low
dimensional belief space embedded in the high dimensional belief state to project into. There have been many
algorithms developed for solving POMDPs to make the solutions tractable, but the computational complexities
suggest that they would only work for problems with small state, action, and observation spaces compared to
the size of the problem discussed in this paper.
The POMDP modeling approach is widely used for problems with unobservable parameters or quantities,
but it suffers from severe computational limitations (it probably cannot be applied to a problem in this paper
with more than 3 or 4 zones). Often overlooked, however, are subtle modeling assumptions that would not
apply for our COVID setting. In particular, the policy in equation (3) uses the one-step transition matrix
6

P(s0 |s, x) which, aside from being computationally intractable, implicitly assumes that the transition function
is known to the controller. This means that the controller actually knows the dynamics of how the disease is
communicated, which is not the case with COVID-19.

2.3. The Unified Framework
The traditional POMDP formulation is not only computationally intractable for all but toy problems, it
does not properly model the fact that the people making decisions about testing and vaccine distribution do
not even know the dynamics of the problem. The POMDP model is a more generalized version of the Markov
Decision Process (MDP) framework which forms the roots for much of the reinforcement learning literature.
It is not the only way to model a sequential decision problem in the literature.
Powell (2019b) compares and contrasts reinforcement learning in the MDP framework with the modeling
framework in the stochastic optimal control literature. The discussion by Powell (2019b) leads to formulation
of the unified framework which ties together the various styles of modeling a sequential decision problem into a
consolidated framework which can easily be translated into software. This framework leads to an optimization
problem to search over policies, which discriminates it from other modeling frameworks. The unified framework
consists of five core components: the state variable, the decision variable, the exogenous information process,
the transition function, and the objective function. After the five components are modeled, a policy can be
chosen from four classes of policies which are listed below:
• Policy function approximation (PFA) - analytical functions directly mapping states to actions, e.g.
lookup tables, parametric function,
• Cost function approximation (CFA) - an optimization problem involving a parameterized approximation
of the cost function, e.g. upper confidence bounding,
• Value function approximation (VFA) - approximating the value of a state which can then be utilized in
Bellman’s equation, e.g. forward approximate dynamic programming,
• Direct lookahead approximation (DLA) - developing an approximate model of the future to optimize the
best decisions through simulation, e.g. Monte Carlo Tree Search.
Powell (2014), Powell (2016), Powell (2019a), and Powell (2020) discuss the formulation of problems using
the unified framework in detail. The unified framework has proven to work in various application domains
with high dimensional state and action spaces. Durante et al. (2020), Al-Kanj et al. (Submitted), and Han
and Powell (2020) utilize the unified framework for problems in application spaces such as energy storage,
transportation, and dynamic pricing, respectively. This paper will view the controller and environment as two
separate agents and model each of them with the unified framework. The interactions between the agents are
then modeled as appendages to their respective exogenous information processes.
7

3. Environment Agent Model

From the perspective of the controlling agent, the environment agent is a black box which can be queried
for observations and impacted by decisions. The real world will almost always be more complex than any
simulator, and a controlling agent would have to approximate the real world to the best of its ability. Therefore,
the environment model was designed to be sufficiently complex to include factors the controlling agent cannot
anticipate. The point is to create a robust model that allows the controlling agent to make effective decisions
with an imperfect model. The model presented in section 4 would function with any increasingly complex
environment model. The simulator designed in this section is to make the environment model as realistic as
possible by including more stochasticity and complexity than the controlling agent model; hence, we recognize
that the simulator constructed for the environment model can be more complex.
Section 2.1 discusses the stochastic spatial SIR model which motivated the design of the environment
agent model. For example, as time passes, there are stochastic interactions between the populations of each
zone. Naturally, zones in close proximity or connected by public transportation are more likely to interact.
Additionally, the system will have an observation function which impacts the samples drawn from the testing
centers. Due to factors caused by inherent characteristics of the disease, human behavior, and the properties
of the test there could be biases shifting the distribution of the observation function. The biases could be
caused by factors such as the likelihood of showing symptoms with the disease, the likelihood of going to get
tested while showing symptoms, and the errors within the testing kits. A common issue with some diseases
is the lack of symptoms in some portion of the positive cases, also known as an asymptomatic carrier. The
presence of symptoms will cause issues with testing because it decreases the likelihood of a person going to get
a test. To make the testing model as realistic as possible, these factors are included in the sampling oracle.
The errors within the tests will also be a major concern because they are not always accurate. There
are two types of errors that are possible: false positives and false negatives. Each type of error will have a
probability of occurring given that a test was administered. Due to limited testing capacity, the number of
tests available may exceed the number of people that try to get access to tests. The model will assume that
as the number of tests available increases within a zone, then the likelihood of going to get a test will also
increase.
In summary, the environment agent model used to construct the simulator in this paper includes many
factors such as spatial correlations between infected populations, biases in testing procedures, stochastically
evolving transition rates, and vaccine success rates. Some of these factors would be extremely difficult to
evaluate online as the controlling agent makes decisions, but they impact the results of the simulator. Hence,
we claim that the simulator presented in this section is sufficiently complex to measure the performance of the
policies designed in section 5 and the quality of the modeling in section 4.

8

3.1. Environment State Variables
The state variables for the environment agent base model include the variables needed to model the system
from time t and onward. The importance of modeling the simulator and the controller as separate entities is
to capture the fact that the controller is not always able to observe the state of the environment perfectly.
Hence, the dynamic state variable for the simulator is given by,
Stenv = (St , It , Rt , αt ),

(4)

where,
St

=

vector of the number of susceptible individuals in each zone,

It

=

vector of the number of infected individuals in each zone,

Rt

=

vector of the number of recovered individuals in each zone,

αt

=

mobility rate at time t.

Additionally, the initial environment state variable consists of all variables which do not change dynamically,
hence it is given by S0env = (N, γ, ξ, (β̄z )z∈Z , (σzβ )z∈Z , S0 , I0 , R0 ) where,
N

=

vector of zone populations,

γ

=

recovery rate of disease,

ξ

=

vaccine success rate,

β̄z

=

mean transmission rate of zone z,

σzβ

=

variance in transmission rate noise,

S0

=

vector of initial susceptibles,

I0

=

vector of initial infected,

R0

=

vector of initial recovered.

In this model of the environment agent, we reiterate that this simulator does not include dynamically changing
mean transmission rates, recovery rates, or vaccine success rates. In the real world, these factors are likely
changing dynamically as local laws change and better drugs are developed.

3.2. Environment Exogenous Information
The exogenous information for the environment agent model refer to streams of information that occur
exogenously, which we model as stochastic processes. These variables include:
β
env
Wt+1
= (xvac
t , M̂t+1 , ε̂t+1 , α̂t+1 )

9

(5)

where,
xvac
t

=

vector of number of vaccines allocated to each zone,

M̂t+1

=

random matrix of inter-zone mobility,

ε̂βt+1

=

additive noise for transmission rate evolution,

α̂t+1

=

mobility rate at time t + 1,

n̂vac
t+1

=

number of vaccines available at time t + 1.

3.3. Environment Transition Functions
The simulator transition function describes how each simulator state variable evolves between time t and
t+1. The spatial SIR model can be modified to allow vaccines (which are managed by the controller) to impact
vaccines pass through the environment transition functions and
the environment agent. At time t, the xvac
t
can impact the true state. The function of the vaccine is to directly move people from the susceptible group
into the recovered/immune group. The dynamics are captured through the following transition equations,
Sxt

=

+
(St − ξxvac
t )

St+1

= Sxt − rt

It+1

=

Rt+1

(6)

Sxt − αt Sxt

M̂t+1 rt

Sxt + αt Sxt

(1 − γ)It + rt

(7)

M̂t+1 rt

(8)

= Rt + γIt + ξxvac
t

where ξ is the success rate of the vaccine, rt =

h

β̂t+1,z Itz
Nz

i

(9)

, and M̂t+1 is the mobility probability matrix. The

mobility rate and number of vaccines available are assumed to be totally exogenous based on the current laws
and vaccine manufacturing process, respectively. The equation for the transition rate at t + 1 is given by,
β̂t+1,z = (β̄z + ε̂βt+1 )+

(10)

where, ε̂βt+1 ∼ N (0, σzβ ).
Since there are no decisions made within the environment there is no objective function.

3.4. Environment Observation Function
Another important component of the simulator is to model the impact of testing within each zone. To
make the testing procedure as realistic as possible, the sampling oracle was designed to contain implicit biases.
The following definitions are used to outline the important parts of the observation function.
Definition 3.1. Assume an individual is randomly drawn from the population of a given zone z at time
t. This individual has characteristics related to the possible disease present in the population. Ωtest =
10

{E pos , E symp , E test , E tpos } represents elements of the set of possible outcomes for this individual. Each element of Ωtest represents the following outcomes:
E pos
E symp

= {The person is positive with the disease},
= {The person is showing symptoms of the disease},

E test

= {The person goes to get tested for the disease between t and t + 1},

E tpos

= {The person tests positive for the disease}.

The complement of each of the outcomes listed are E neg , E asymp , E not , E tneg , respectively.
Definition 3.2. The probability that a person drawn from the population is truly positive with the disease at
time t is given by
pos
pinf
]=
tz = P[E

Itz
.
Nz

(11)

When the controlling agent attempts to query a sample from a zone, then the sampled positive cases may
not reflect the true percentage in the population, on average. The controlling agent may not be able to see
this bias; however, the environment agent will have access to the information causing the skew in the data.
Definition 3.3. Assume a person is drawn from the population and the probability that they are showing
symptoms of the disease while carrying it is given by a constant probability a, and the probability that they
are showing symptoms of the disease while they are not carriers is given by the constant probability b. Hence,
P[E symp |E pos ] = a, and P[E symp |E neg ] = b.
Definition 3.4. If a person is not showing symptoms while positive with the disease, then they have a lower
chance of going to wait in the test queue between time t and t + 1. Assume that as the testing capacity of a
zone is increased, then the likelihood of getting tested increases through the following equations,
test
c(xtest
|E symp , xtest
tz ) = P[E
tz ]

=

test
d(xtest
|E asymp , xtest
tz ) = P[E
tz ]

=

xtest
tz
(1 − c0 ),
Nz
xtest
d0 + tz (1 − d0 ),
Nz
c0 +

(12)
(13)

where c0 and d0 are the initial probability of attempting to get a test while there are none allocated to zone z.
The simulator is able to access each probability, but the controller would not be able to observe this
information directly. It is easier to think about the probabilities a, b, c and d as follows: a is the probability
of being symptomatic while carrying the disease, b is the probability of being symptomatic without carrying
the disease, c is the probability of trying to get tested while showing symptoms, and d is the probability of
trying to get tested without showing symptoms. The probabilities c and d are functions of how many tests
are available, while c0 and d0 are the base probabilities of trying to get tested given that there are no tests
11

available. From the controllers perspective, these values cannot be seen however they are reflected in the
sample created after administering the test.
test
Lemma 3.1. Let fp and fn be the probability of false positive and false negative, respectively. a, b, c(xtest
tz ), d(xtz )

are from definitions 3.3 and 3.4. ptest
represents the probability of testing positive if the person gets tested
tz
given by,
ptest
tz

=

(1 − fn )P[E pos |E test ] + fp (1 − P[E pos |E test ]),

where,
P[E pos |E test , xtest
tz ] =

(ac + (1 − a)d) pinf
tz


.
inf
(c − d) (a − b)ptz + b + d

The variables c and d are functions of the number of tests allocated to a zone.

Proof. See Appendix.

The adjusted probability allows the factors affecting the observation distribution to be reflected in the
probability of testing positive. Therefore, the number of positive test results after administering the limited
ˆ
ˆ
number of tests, xtest
tz , is given by the random variable, It+1,z . The random variable It+1,z has a binomial
test
distribution with parameters (n, p) = (xtest
tz , ptz ). Lemma 3.1 implies the parameter of the binomial distribu-

tion for the observation function does not reflect the true percentage of infected individuals in the population
on average, but a shifted distribution with a bias unknown to the controller.

4. Controlling Agent Model

This section proposes a model for the controlling agent using the five components of the unified framework.
The controller does not have access to the environment agent’s state variable, Stenv , or the transition functions
env
describing how they evolve, S M,env (Stenv , Wt+1
). Due to the lack of this necessary information, the controller

must make assumptions about the components of the environment agent’s model. We are going to start by
outlining the belief model, before giving the five elements of the controller model.

4.1. Belief Model
In a sequential decision problem with imperfectly known states and state transitions, the controller must
maintain a belief state and have a belief about the transition functions in the environment mode, which are
likely unknown. This section contains the three major components of the belief model:

1) the belief state,
12

2) the belief about the transition functions in the environment model,
3) the updating equations for the belief state.

4.1.1. Belief State
The belief state contains the parameters of a probability distribution over the environment agent’s state
space. In the SIR model, there are three physical state variables describing the population of susceptible (S),
infected (I), and removed individuals (R). The S,I, and R subpopulations are elements of the environment
agent state, and are not known perfectly to the controlling agent. Since the controlling agent cannot observe
the environment state perfectly at time t it must maintain a probability distribution over the entire state
space. The controlling agent can iteratively update the belief state as it queries observations from the testing
centers.
Definition 4.1. The SIR model assumes that the total population remains constant. Therefore, the belief
about the true percentage of each subpopulation in a zone has the following property,
rec
p̄susc
+ p̄inf
tz + p̄tz = 1.
tz

The property implies that the belief about the subpopulation of each zone has a multinomial distribution with
inf
rec
parameters (Stz , Itz , Rtz ) ∼ M ult(Nz , p̄susc
tz , p̄tz , p̄tz ). Therefore, the belief state will contain the dynamically

changing parameters of the multinomial distribution for each zone.

4.1.2. Environment Transition Function Belief
The controlling agent assumes a functional form for the transition functions in the environment model.
The transition functions in the environment model are used to update the state variable known perfectly to
the environment agent. Therefore, the functions in this section would describe the set of equations to update
the state variables if there was no uncertainty in the state at time t.
Let (St , It , Rt ) be vectors containing the number of susceptible, infected and removed individuals in each
zone z, respectively. The vectors are the physical state variables for the environment agent. If the controlling agent had access to the true physical state variables and made the decision xvac
t , then it believes the
environment will transition to the next state according to the following set of equations:
St+1

env
= f susc (St , It , xvac
t |S0 ),

(14)

It+1

env
= f inf (St , It , xvac
t |S0 ),

(15)

Rt+1

env
= f rec (Rt , It , xvac
t |S0 ).

(16)

Equations (14 - 16) describe the belief about how each environment state variable evolves. Since the true state
variable is not fully observable each element of (St , It , Rt ) are random variables; hence, the function cannot be
13

evaluated by the controlling agent because it only has a belief state describing the random variables. However,
the equations are important because they must be used to derive the updating equations for the belief state
variables.

4.1.3. Belief State Update
The controlling agent will decide to allocate xtest
tz testing kits to zone z at time t. Between time t and t + 1
the testing kits are administered and produce a random response, Iˆt+1,z , which represents the total number
of positive cases measured in zone z between t and t + 1. Assume the random variable Iˆt+1,z has a binomial
test
distribution with parameters (n, p) = (xtest
tz , ptz ). The response would be analogous to the output of the

observation function defined in the POMDP model and the specifics are outlined in section 3.4.
The probability, ptest
tz , is the probability of randomly selecting an infected individual out of the population
of individuals who received a test at time t in zone z. This probability is affected by factors such as the
likelihood of going to get a test while showing symptoms, the likelihood of showing symptoms while positive
is
and the probability of false positives and false negatives. The exact impact those factors will have on ptest
tz
not known to the controller, but the policies we discuss later will try to make decisions which can mitigate
the risks imposed by the sampling biases.
The belief state updating equations take the observations, Iˆt+1,z , queried from the testing centers and use
inf
rec
them to estimate the new belief state at t + 1. Each of the three parameters p̄susc
t+1,z , p̄t+1,z , and p̄t+1,z in each

zone z will need an updating equation. Note that the infected population is being observed directly with Iˆt+1 ,
but the susceptible and removed populations are not.
prior
prior
Let αtz
and βtz
be parameters of a beta distribution which is constructed to encode the prior infor-

mation about the number of infected individuals in the population of zone z. The controlling agent has prior
knowledge about the infected population prior to the observations at t + 1 because of the assumed functions
in section 4.1.2. The prior information can be encoded at t + 1 by taking the expectation of equation 15 given
by,
prior
αtz

= E[It+1,z |Stcont ],

(17)

prior
βtz

= Nz − E[It+1,z |Stcont ].

(18)

test
test
The observations are assumed to be drawn from a binomial distribution with parameters (xtest
tz , ptz ). ptz

is unknown to the controller, but a sample was just observed from its distribution Iˆt+1,z . The beta prior is
a conjugate prior to the binomial distribution, and through Bayes’ theorem the updated distribution will be
beta-binomial. The resulting distribution can be thought of as the binomial parameter ptest
tz being drawn from
prior
prior
the beta distribution with parameters αtz
and βtz
.
prior
prior
Lemma 4.1. Let αtz
and βtz
be parameters of a beta distribution and Iˆt+1,z be a sample drawn from

14

a binomial distribution with xtest
trials. The compound distribution produced by Bayes’ Theorem is a betatz
binomial distribution. The estimator for the probability of an infection, p̄inf
t+1,z is given by,
p̄inf
t+1,z =

Iˆt+1,z + αzprior
prior
xtest
+ βzprior
tz + αz

.

(19)

Proof. See Appendix.

After the tests have been administered into the population, it is possible to get an estimate of the number
of infected individuals; however, there are two other groups in the population: susceptible and removed. Since
we only have observations of the number of infected individuals at time t + 1, then it would not be possible
to estimate the other two groups without using the belief about the transition functions in section 4.1.2. To
estimate the other two subpopulations within a zone, the belief about the environment transition functions
must be used to update the belief states for susceptible and removed individuals.
After the observations have streamed into the system from the testing centers, the parameters of the
beta distribution are computed using the belief about the transition equations and used to get the posterior
parameter of the beta-binomial distribution given in equation (19). The posterior distribution for the infected
number of individuals is used to update the parameters of the multinomial distributions representing the
susceptible and removed subpopulations within each zone.
Definition 4.2. Let Π∆z be the projection operator for the set defined by,
n
o
inf
rec
2
susc
rec
∆z = (p̄susc
t+1,z , p̄t+1,z ) ∈ [0, 1] : p̄t+1,z + p̄t+1,z = 1 − p̄t+1,z .

The expected susceptible and removed subpopulations of each zone is computed by taking the expectation
of equation (14) and (16), respectively. If the terms are not in the set ∆z in definition 4.2, then they must be
projected back to the nearest point. This set of equations is given by,
S̄t+1,z

= E[St+1,z |Stcont , xvac
t ],

= E[Rt+1,z |Stcont , xvac
t ],


S̄
R̄
t+1,z
t+1,z
rec
(p̄susc
,
p̄
)
=
Π
,
.
∆
t+1,z t+1,z
z
Nz
Nz
R̄t+1,z

(20)
(21)
(22)

In summary, the controlling agent updates the parameters in the belief state through the following process:
1) Make observations Iˆt+1,z for all z ∈ Z,
2) Compute the estimator of p̄inf
t+1,z in equation (19),
rec
3) Use equations (20 - 22) to update p̄susc
t+1,z and p̄t+1,z .

15

rec
It is important to emphasize the beliefs p̄susc
t+1,z and p̄t+1,z would not be possible to update through observations

of Iˆt+1 without the belief about the transition equations presented in section 4.1.2. The belief about the
transition equations may not match the true model (as we demonstrate in this paper) because it is usually
an approximation of the real world, which is usually not possible to know perfectly. After the belief model is
fully defined, then the five components of the unified framework naturally follow.

4.2. State Variables
The state variables include the information which is needed to compute the transition functions, objective
function, and policy for making decisions at time t. Any information which is not changing dynamically
remains a latent variable defined in the initial state. The state variable for the base model is defined as,
rec susc
vac
test
Stcont = ((p̄inf
)
tz , p̄tz , p̄tz )z∈Z , nt , nt

(23)

where,
p̄inf
tz

=

the estimated infected population % at time t in zone z,

p̄rec
tz

=

the estimated recovered population % at time t in zone z,

p̄susc
tz

=

the estimated susceptible population % at time t in zone z,

nvac
t

=

the number of vaccines available at time t,

ntest
t

=

the number of tests available at time t.

Let the state space be denoted by S cont . The initial state contains the variables which are not changing
dynamically. The initial state for this model is given by,
rec susc
vac
test
S0cont = ((βz , γz , p̄inf
0z , p̄0z , p̄0z )z∈Z , n0 , n0 )

where,
βz

=

the transmission rate of each zone z,

γz

=

the recovery rate of each zone z,

p̄inf
0z

=

initial belief of the infected % in zone z,

p̄rec
0z

=

initial belief of the recovered% in zone z,

p̄susc
0z

=

initial belief of the susceptible % in zone z,

nvac
0

=

the number of vaccines available at time t = 0,

ntest
0

=

the number of tests available at time t = 0.

16

4.3. Decision Variables
There are two sets of decisions at each time step: the information collection decision of how many tests to
allocate and the implementation decision of how many vaccinations to allocate to each zone. The decision to
allocate vaccines to each zone is given by, xvac
tz , which is constrained by the total number of vaccines available,
test
nvac
t . The decision to allocate testing kits to each zone is given by, xtz , which is constrained by the total

number of tests available, ntest
. Hence, the vaccine decision variable is constrained by,
t
X

xvac
tz

≤ nvac
t .

(24)

xtest
tz

≤

ntest
.
t

(25)

z∈Z

The testing decision variable is constrained by,
X
z∈Z

test
The set of decisions can be characterized by xt = (xvac
).
t , xt

4.4. Exogenous Information
cont
The exogenous information, Wt+1
, represents all the infromation that arrives between time t and t + 1.

It is given by,
cont
test
Wt+1
= ((Iˆt+1,z )z∈Z , nvac
t+1 , nt+1 )

(26)

where,
Iˆt+1,z

=

number of positive tests sampled,

nvac
t+1

=

the number of vaccines available at t + 1,

ntest
t+1

=

the number of tests available at t + 1.

The number of tests and vaccines available at t+1 is a process evolving completely exogenously from the model.
These values could be determined by the bottlenecks in the manufacturing processes from the distributors. It
is assumed that at the beginning of each time step this number is revealed to the system; however, it does not
attempt to model the stochastics behind it.

4.5. Transition Function
The transition function is the set of equations which describe how each of the state variables evolve between
time t and t + 1. The state variable is given in Equation (23). A simple belief about the transition function

17

would assume the basic SIR model where each zone is evolving independently. Equations (14 - 16) from section
4.1.2 are defined as,
Sxt
St+1

= Sxt − b

It+1

=

Rt+1
where b =

h

βz
Nz

i

and

+
(St − xvac
t ) ,

=

Sxt

(27)
It ,

(1 − γ)It + b

Sxt

(28)
It ,

(29)

= Rt + γIt + min(St , xvac
t ),

(30)

is the elementwise vector multiplication operator. The variables in equations (27

z∈Z

- 30) are not known to the controller, hence they must be estimated statistically.
Lemma 4.2. The belief state for each zone is represented by a multinomial distribution with parameters
susc rec
(Nz , p̄inf
tz , p̄tz , p̄tz ). The expectation of equations (27 - 30) do not have a closed form. If the populations of

the zones are fairly large (Nz ' 100), then these expectations are approximated by assuming each subpopulation
of a zone is normally distributed and independent. We use bars to denote an approximated expectation (e.g.
S̄ ≈ E[S]). Through this assumption, the estimates of the expectation of equations (27 - 30) are given by,





S̄tz − xvac
S̄tz − xvac
tz
tz
x
susc
S̄tz
=
(S̄tz − xvac
)Φ
+
σ
φ
,
(31)
tz
tz
susc
susc
σtz
σtz


x
S̄t+1,z =
1 − βz p̄inf
S̄tz
,
(32)
tz
I¯t+1,z
R̄t+1,z

x
(1 − γ)I¯tz + βz p̄inf
tz S̄tz ,

(33)

x
= R̄tz + γ I¯tz + S̄tz − S̄tz
,

(34)

=

where,
S̄tz
susc
σtz

I¯tz
R̄tz

= Nz p̄susc
tz ,
p
susc
=
Nz p̄susc
tz (1 − p̄tz ),
=

Nz p̄inf
tz ,

= Nz p̄rec
tz ,

and Φ is the standard normal cdf and φ is the standard normal pdf.

Proof. See Appendix.
prior
prior
If αtz
= I¯t+1,z and βtz
= Nz − I¯t+1,z , then Lemma 4.1 gives the updating procedure for p̄inf
t+1,z .
susc
The other belief state variables, p̄rec
t+1,z and p̄t+1,z , are computed using equations (20 - 22) and the result

of Lemma 4.2. Hence,
rec
(p̄susc
t+1,z , p̄t+1,z )


=

Π∆z

18

S̄t+1,z R̄t+1,z
,
Nz
Nz


.

(35)

4.6. Objective Function
The objective function measures the performance of the system. It will measure how well the vaccine
and testing allocation decisions impact the state of the epidemic. The goal of the problem is to come up
with a decision making strategy, or policy, to minimize the total number of infected people in the population
throughout the entire time horizon. Therefore, the cumulative reward objective can be posed as the following
optimization problem,
min E

(T −1
X

π∈Π

)
C̄(Stcont , X π (Stcont ))|S0cont

.

(36)

t=0

The function C̄(Stcont , X π (Stcont ), Wt+1 ) is a one period cost function describing the metric which is evaluated
by the controller at each time period. The goal of mitigating the spread of disease is to minimize the number
of infected people, but it is not possible to perfectly observe the true number of infected people at t + 1. If
the controller had access to the true state and transition equations, then the goal would be to minimize the
true one-step cost given by,
env
C env (Stenv , xt , Wt+1
)=

X

It+1,z (xvac
tz ).

(37)

z∈Z

Equation (37) is not observable, hence the controller will minimize the expected number of individuals infected
with the disease, given the current belief state. The belief state contains the parameters of the probability
distribution over the environment state variables, hence the controllers cost function is given by,
X
X
cont
C̄(Stcont , xt ) =
E[It+1,z (xvac
, xvac
I¯t+1,z (xtz ).
tz )|St
tz ] =
z∈Z

(38)

z∈Z

To evaluate this cost function, the belief about the transition function for the infected number of individuals
must be used to predict what the impact of the decision will be, hence equation (33) must be used to evaluate
the right hand side of equation (38).

5. Designing Policies
The policy is a mapping from the state space to the decision space. At time t there are implementation
decisions (the number of vaccines to allocate to each zone) and learning decisions (the number of testing kits
to allocate to each zone). In section 5.1, we illustrate two types of implementation policies: one from the PFA
class and one from the DLA class. In section 5.2, we present a CFA learning policy for deciding the zones
with the most valuable information to collect through testing.

5.1. Implementation Policies
The implementation decision in this problem setting is to choose how many vaccines to send to each zone
in the nation, xvac
tz . The decision space for the next set of policies is given by,
Xtvac = {xvac
∈ N|Z| : 1T xvac
≤ nvac
t
t
t }.
19

(39)

The optimal policy for making a decision is given by Bellman’s equation defined in equation (3),
Xt∗ (Stcont ) = arg

min

xvac
∈Xtvac
t

∗
cont
C̄(Stcont , xt ) + EWt+1 [Vt+1
(St+1
)|Stcont ]



(40)

∗
where Vt+1
(·) is the optimal value function at t+1 if we knew the true state of the environment. The dimension

of the state variable defined in section 4.2 is 3|Z| + 2, therefore as |Z| grows it becomes computationally
intractable to solve for the optimal policy due to the dimensionality of the state variable. When the optimal
policy is not possible to achieve, then the goal of the problem becomes finding the best approximation to it.
When designing the approximation, there are four possible classes of policies to choose from, which are defined
in section 2.3.

5.1.1. PFA: Population-based allocations
The PFA class of policies is an analytical function which maps states to actions, and usually includes
tunable parameters. These policies can be used to characterize a simple rule-based method, which is why
these are most commonly used in practice. Some examples of very simple PFA policies could be to allocate
the set of vaccines evenly among the zones, proportionally by population, or proportionally by the belief of
the number of infected individuals. In this paper, the PFA we chose to analyze consists of a sigmoid function,
PFA
= (θ0P F A , θ1P F A ). The sigmoid function for each
which is a function of p̄susc
tz , with tunable parameters θ

zone is given by,
gz (Stcont |θP F A )


=

1
P F A p̄susc
tz

1 + e−θ1

θ0P F A
.

Then, to proportionally allocate each set of responses and ensure that the solution remains in the decision set,
the policy becomes:
X

PFA

(Stcont |θP F A )

j vac

nt gz (Stcont |θP F A ) k
= P
.
cont |θ P F A )
z∈Z gz (St
z∈Z

(41)

This policy operates quickly online, but it requires a time consuming offline grid search to find the best
set of hyperparameters. The policy will proportionally allocate the vaccines based on the magnitude of the
response caused by the belief about the percentage of the population that is susceptible to the disease.

5.1.2. DLA: Parameterized Two-Step Lookahead Approximation
In a direct lookahead approximation, an approximate model of the future is created to make the best
decisions at t by looking at the impact of decisions in the future. The lookahead model will approximate the
base model by choosing the θrisk -percentile of the distribution represented by the distribution of the belief
state, and using this value as the lookahead state variable. The lookahead model will simulate the future by
using the belief about the transition functions in equations (27 - 30). This is a reasonable approximation of
the base model; however, it omits performing active learning. To create a proper lookahead model the five
20

components of a sequential decision model defined by the unified framework in section (2.3) must be defined.
A lookahead variable will have a tilde and two time indices, where the first is the time within the base model
and the second index is the time within the lookahead model.

Lookahead State Variable. The distribution of each subpopulation of each county is approximated by a normal
θ
distribution defined in Lemma 4.2; hence the θrisk -percentile of the susceptible distribution is given by S̃tt
=
susc
S̄tz − z θ σtz
. The number of infected individuals will be approximated by the mean of the distribution
θ
θ
I˜ttz = I¯tz , and the number of removed individuals will be R̃ttz
= Nz − S̃ttz
− I˜ttz . The model at time t does

not have access to a forecast of the number of vaccines available in the future, hence the lookahead model will
assume that the number of vaccines available at each t0 in the future, where t0 ≥ t, will be the same. Hence,
the lookahead state for t0 in the future is given by,


cont risk
θ
θ
˜
S̃tt
(θ
) = S̃tt
0
0 z , Itt0 z , R̃tt0 z

z∈Z

Lookahead Decisions. The lookahead model will not attempt to learn in the future, hence the testing decision
will not be modeled. Therefore, the only lookahead decision is the decision about where to allocate vaccines,
x̃tt0 . Each of these decision vectors is constrained by 1T x̃tt0 ≤ nvac
t .

Lookahead Exogenous Information. The lookahead model is using point estimates derived from the belief state,
and the belief transition functions for a point estimate are deterministic, hence there is not an exogenous
information variable.

Lookahead Transition Functions. The lookahead transition functions are given by Equations (27-30), which
are deterministic functions. The lookahead transition equations allow the future to be approximated as long
cont
as the initial lookahead state, S̃tt
, is in the feasible state space.

Lookahead Objective Function. The lookahead objective function is given by
cont
C̃(S̃tt
, x̃tt0 ) =
0

X

I˜t,t+1,z (x̃tt0 z ).

(42)

z∈Z

The optimization problem for a two-step lookahead approximation is given by,
XtDLA,2 = arg

min

x̃tt ,x̃t,t+1

cont
cont
C̃(S̃tt
, x̃tt ) + C̃(S̃t,t+1
, x̃t,t+1 )

(43)

The optimization problem in equation (43) reduces to a problem with a nonconvex quadratic objective
function with linear constraints. For problems where |Z| is not too large the policy can be solved with bilinear
solvers. The details describing the solution to equation (43) can be found in the Appendix.

21

5.2. Learning Policies
The second type of decision is to select how many samples to observe from each zone through testing. At
time t the controlling agent must decide which zones to send the ntest
kits after the implementation policy
t
has already been made. The learning decision will impact the distribution of the random variable for positive
tests, Iˆt+1 , and consequently affect the future decisions about where to send vaccines.
It is important to have estimates of each environment state variable, and this decision will change those
estimates. The testing policy we propose consists of a tradeoff between proportionally allocating a percentage
of the testing kits, and trying to minimize the variance in the belief state at t + 1. The logic behind this design
is get close to an estimate with respect to mean-squared error globally in the region, but also ensure that each
zone is being sampled.
Equation (19) updates the belief about pinf
t+1 by assuming the posterior has a beta-binomial distribution.
If the estimator in equation (19) has a beta-binomial distribution, then its variance is given by,
cont
V ar[p̄inf
]=
t+1,z |St

prior prior
prior
prior
βtz (αtz
+ βtz
+ xtest
xtest
tz )
tz αtz
prior
prior 2
prior
prior
(αtz
+ βtz
) (αtz
+ βtz
+ 1)

.

(44)

Let the sum of variances in equation (44) become the objective function. The testing policy is induced by the
optimization problem when the sum of variances is the objective function, given by
min
test

x

X

test
V ar[p̄inf
)|Stcont ],
t+1,z (x

(45)

z∈Z

subject to 1T xtest ≤ ntest
.
t
Lemma 5.1. Let equation (45) be the optimization problem used to produce the learning decisions. The
optimization problem reduces to a quadratic program with linear constraints given by,
min (xtest )T Dxtest + dT xtest ,

(46)

xtest

subject to 1T xtest ≤ ntest
.
t
where,
"

prior prior
αtz
βtz
D = diag
2
Nz (Nz + 1)
"
#
prior prior
αtz
βtz
d =
.
Nz (Nz + 1)

#

!

z∈Z

z∈Z

This is an integer quadratic program with linear constraints.

The problem with the policy induced by equation (46) is it will greedily choose which zones to send kits
to and ignore some zones with lower variance for too long. We can circumvent this problem by ensuring that
22

a certain portion of the testing kits, ρtest , are distributed proportionally to the population of each zone, and
the remaining 1 − ρtest testing kits are chosen greedily. This policy falls into the CFA class of policies because
it has an embedded optimization problem, and the parameter ρtest must be tuned to find the best proportion
on average. The greedy policy and the proportional policy are given by
X greed (Stcont |ρtest ) = min
(xtest )T Dxtest + dT xtest ,
test
x

(47)

subject to 1T xtest ≤ ntest
− bρtest ntest
c.
t
t
and,

N
P z
X prop (Stcont |ρtest ) = bρtest ntest
t
z∈Z


Nz

c

,
z∈Z

respectively. Hence, the resulting testing policy becomes,
X test (Stcont |ρtest ) = X greed (Stcont |ρtest ) + X prop (Stcont |ρtest ).

(48)

The best ρtest in this function class can be tuned through policy evaluation using the simulator defined in
section 3.

6. Results

The environment agent model from section 3 was used to construct a simulator in python. There were
two simulators constructed: one for a toy problem with 25 randomly generated zones in a region discussed in
section 6.1, and one for a simulation of the states in the United States discussed in section 6.2. Considering a
vaccine would not be developed until a country is in the midst of a pandemic, the initial state of the infected
population is randomly generated from a uniform distribution between 5% and 15% at t = 0. The testing and
vaccine manufacturing process is assumed to be increasing (on average) to simulate the reality that there will
not be a large number of doses at the onset of its distribution into the population.

6.1. Toy Problem Simulator
The toy problem simulator was constructed by randomly generating 25 sample locations, which would be
considered nodes of a graph, and then weighting the mobility transition matrix, M̂t+1,zz0 , by the distance
between the locations. Hence, individuals from nearby zones will have a higher probability of interacting with
one another. Each zone has its own transmission rate and the removal rate for the disease remains constant
for each zone. The graph of the simulated region is shown in Figure 1. The weights of the edges of the graph
represent the probability of individuals from each zone interacting with one another.
Each of the policies in section 5 were implemented and the results of the cumulative total of infected
individuals in each zone can be seen in figure 1. The null policy shows the amount of infected individuals
23

Figure 1: The graph of the simulated region with weighted edges proportional to average transmission rates between zones

if there was no vaccine available with constant transmission rates. After tuning the parameters of the PFA
using a discretized grid search over 50 simulations for each value, the best values were θP F A = (50, 10). After
tuning the risk parameter in the DLA, the best value after 50 simulations for each was θrisk = 0.15. The active
learning policy was tuned through policy evaluation and ρtest = 0.7 after 50 simulations for both policies.

(a) Cumulative number of infected individuals for each policy after (b) New infections each week for 30 weeks accompanied by the weekly
number of vaccines available

30 weeks

Figure 2: Infection totals for each of the policies averaged over 100 sample paths

It is clear that each of the policies offers a significant improvement over the 140,000 people who would be
infected if there was no vaccine being distributed in this region. The tuned PFA offers a 62.2% reduction in
cases over the null policy and the DLA offers a 69% reduction in cases over the null policy.

6.2. USA COVID-19 Simulator
In the model of the United States, the zones in the country are partitioned at the state level. The population
data for each state was provided by the US Census Bureau. Each state in the country has a known population,
Nz . The probability matrix, M̂t+1,zz0 , for each state was weighted by the distance between each state because
neighboring states are more likely to infect each other. The beta values in each state are created by taking
24

the logarithm of the population density and rescaling it between a fixed range of (βmin , βmax ). The policies
discussed in section 5 were implemented with |Z| = 51 zones (including Washington, DC). The initial infection
rate was generated by the record day of daily new infection data in each state to demonstrate that this strategy
could work at the peak of a pandemic.

Figure 3: The cumulative infections for the 50 states (plus Washington, DC) for both of the policies

In the USA simulator, we assume that the vaccines are being manufactured and the initial vaccine total
is about 1% of the population. As the manufacturing capabilities increase, the total vaccines available for
distribution will increase stochastically. Under the null policy with the assumed βz values, the simulator
predicts about 18 million people would be infected with the disease. The PFA policy improves on the null
policy by 45% and the DLA improves on the null policy by 54%. The DLA policy will also prevent about 1.8
million more infections than to the PFA policy over the given time horizon.
When the problem is scaled up to 51 zones and the populations grow to be on the order of millions of
people, then the number of infected individuals has the potential to grow significantly larger. The number of
susceptible people determines the rate at which the infected group will grow, so it is important to allocate
vaccines strategically in areas with large populations of susceptible individuals. In the plot in figure 4, we
show a breakdown of the country as the pandemic spreads with three very different scenarios. In the first
row, there are no vaccines so the infection can spread uncontrollably. In the second row, the PFA strategy is
more likely to spread the vaccine evenly among states if there is a high percentage of susceptible individuals;
hence the states with lower populations and transmission rates are targeted with more vaccines. The third
row shows the DLA policy which is much more aggressive towards states with higher population density. It is
obvious that states like New York, California, and Illinois, which have large cities, will spread the virus within
their own state at a higher rate than states like Wyoming or Montana. Furthermore, many individuals travel
within those high population states, so when the DLA targets those states with vaccines it will have higher
order effects. Hence, the DLA is capable of mitigating those effects.

25

Figure 4: New infected individuals per state for three policies: the null policy, PFA, and DLA

It is intuitive that early action will always produce the fewest number of infections in the population,
which is also why an aggressive DLA has better results. If we assume that the death rate due to the disease
is fixed throughout the time horizon, then it is just a linear function of the number of infected individuals.
Therefore, every infection prevented will be proportional to the number of deaths prevented from the disease.
The risk-adjusted DLA policy prevented 1.8 million more infections than the PFA policy. Hypothetically, if a
1% death rate is assumed, then the DLA policy would save about 180,000 more lives than the myopic PFA.

7. Conclusion

This work presented a formal model for managing vaccines and testing kits during a pandemic using
the unified framework. During a pandemic, the controller would not know the true state of the number of
susceptible, infected and removed individuals; hence, the belief modeling in this paper captures the belief
states and belief transitions. The multiagent modeling strategy in this paper is used to distinctly separate
the modeling of the environment from the modeling of the controller. The strategy allows the environment
to be considered a blackbox and the controller can only query observations and make decisions which impact
it, which is why we focus on a tunable policy which can be optimized to mitigate the risks of an uncertain
environment. Hence, if a more advanced environment model is applied to the problem, then the controller
would still be able to robustly be applied.
There were two types of decisions in this paper: the implementation decision and the learning decisions.
This paper compared two implementation policies in the PFA class and the DLA class while using a CFA
active learning policy for the testing decisions. The results demonstrate that the DLA class with a tuned
26

risk adjustment parameter will perform the best in the toy problem and the US states problem. There are
many more complex policies which could be applied to this problem, but we selected two that would illustrate
the concepts most clearly. The PFA policy is intended to illustrate a very basic policy which is more likely
to be used in practice than the DLA. The DLA policy is obviously more complex because it uses lookahead
modeling and optimization, while the PFA is an analytical function. Therefore, we demonstrate that a well
designed DLA policy can significantly outperform the basic policies most likely to be used in practice by
myopic decision-makers.

8. Bibliography

References
Al-Kanj, L., Powell, W.B., Bouzaeine-Ayari, B., Submitted. The information-collecting vehicle routing problem: Stochastic optimization for emergency storm response. Submitted to Transportation Science. Arxiv
Preprint, arxiv:1605.05711, 2017 URL: http://arxiv.org/pdf/1605.05711v1.pdf.
Asano, E., Gross, L., Lenhart, S., Real, L., 2008. Optimal control of vaccine distribution in a rabies metapopulation model. Mathematical Biosciences and Engineering 5, 219. doi:http://dx.doi.org/10.3934/mbe.
2008.5.219.
Astrom, K., 1965. Optimal control of markov decision processes with incomplete state estimation. Mathematical Analysis and Applications 10, 174–205.
Brandeau, M.L., Zaric, G.S., Richter, A., 2003. Resource allocation for control of infectious diseases in multiple
independent populations: beyond cost-effectiveness analysis. Journal of Health Economics 22, 575–598.
Dai, T., Cho, S.H., Zhang, F., 2016. Contracting for on-time delivery in the u.s. influenza vaccine supply
chain. Manufacturing & Service Operations Management 18, 332–346. doi:10.1287/msom.2015.0574.
Ding, W., Gross, L., Langston, K., Lenhart, S., Real, L., 2007. Rabies in raccoons: optimal control for
a discrete time model on a spatial grid. Journal of Biological Dynamics doi:https://doi.org/10.1080/
17513750701605515.
Durante, J.L., Nascimento, J., Powell, W.B., 2020. Backward approximate dynamic programming with hidden
semi-markov stochastic models in energy storage optimization. Arxiv Preprint doi:arXiv:1710.03914.
Ekici, A., Keskinocak, P., Swann, J.L., 2008. Pandemic influenza response, in: 2008 Winter Simulation
Conference, IEEE. pp. 1592–1600.
Han, W., Powell, W.B., 2020. Optimal online learning for nonlinear belief models using discrete priors.
Operations Research 68, 1538–1556. doi:10.1287/opre.2019.1921.

27

Hoey, J., Poupart, P., 2005. Solving pomdps with continuous or large discrete observation spaces. IJCAI ,
1332–1338.
Kermack, W.O., McKendrick, A.G., 1927. A contribution to the mathematical theory of epidemics. Proceedings of the Royal Society of London 115, 700–721. doi:doi:10.1098/rspa.1927.0118.
Lin, Q., Zhao, Q., Lev, B., 2020. Cold chain transportation decision in the vaccine supply chain. European Journal of Operational Research 283, 182 – 195. URL: http://www.sciencedirect.com/science/
article/pii/S0377221719309075, doi:https://doi.org/10.1016/j.ejor.2019.11.005.
Martin, C., Allen, L., Stamp, M., Jones, M., Carpio, R., 1993. A model for the optimal control of a measles
epidemic. Computation and Control III: Proceedings of the Third Bozeman Conference, Bozeman, Montana,
August 5–11, 1992 , 265–283doi:10.1007/978-1-4612-0321-6_20.
Medlock, J., Galvani, A.P., 2009. Optimizing influenza vaccine distribution. Science 325, 1705–1708. URL:
https://science.sciencemag.org/content/325/5948/1705, doi:10.1126/science.1175570.
Neilan, R.M., Lenhart, S., 2011. Optimal vaccine distribution in a spatiotemporal epidemic model with
an application to rabies and raccoons. Journal of Mathematical Analysis and Applications 378, 603 –
619. URL: http://www.sciencedirect.com/science/article/pii/S0022247X10010528, doi:https://
doi.org/10.1016/j.jmaa.2010.12.035.
Patel, R., Longini, I.M., Halloran, E., 2005. Finding optimal vaccination strategies for pandemic influenza
using genetic algorithms. Journal of Theoretical Biology 234, 201 – 212. URL: http://www.sciencedirect.
com/science/article/pii/S0022519304005703, doi:https://doi.org/10.1016/j.jtbi.2004.11.032.
Pineau, J., Gordon, G., Thrun, S., 2006. Anytime point-based approximations for large pomdps. Journal of
Artificial Intelligence Research 27, 335–380. doi:10.1613/jair.2078.
Powell, W., 2019a. A unified framework for stochastic optimization. European Journal of Operational Research
275, 795–821. doi:https://doi.org/10.1016/j.ejor.2018.07.014.
Powell, W.B., 2011. Approximate Dynamic Programming: Solving the Curses of Dimensionality. Draft.
Powell, W.B., 2014. Clearing the jungle of stochastic optimization. INFORMS Tutorials in Operations
Research: Bridging Data and Decisions , 109–137doi:http://dx.doi.org/10.1287/educ.2014.0128.
Powell, W.B., 2016. A unified framework for optimization under uncertainty. Informs TutORials , 54–83.
Powell, W.B., 2019b. From reinforcement learning to optimal control: A unified framework for sequential
decisions. Book chapter for edited volume: Handbook for Reinforcement Learning and Control doi:http:
//arxiv.org/abs/1912.03513.
Powell, W.B., 2020. Stochastic Optimization and Learning: A unified framework. Draft.

28

Ross, S., Pineau, J., Paquet, S., Chaib-draa, B., 2008. Online planning algorithms for pomdps. Journal of
Artificial Intelligence Research 32, 663–704. doi:10.1613/jair.2567.
Roy, N., Gordon, G., Thrun, S., 2005. Finding approximate pomdp solutions through belief compression.
Journal of Artificial Intelligence Research 23. doi:https://doi.org/10.1613/jair.1496.
Zakary, O., Rachik, M., Elmouki, I., 2017. On the analysis of a multi-regions discrete sir epidemic model: an
optimal control approach. International Journal of Dynamic Control 5, 917–930. doi:https://doi.org/
10.1007/s40435-016-0233-2.
Zhan, D., Xing, H., 2020. Expected improvement for expensive optimization: a review. Journal of Global
Optimization 78, 507–544. doi:https://doi.org/10.1007/s10898-020-00923-x.
Zhang, Y., Prakash, B.A., 2014. Scalable vaccine distribution in large graphs given uncertain data. Proceedings
of the 23rd ACM International Conference on Information and Knowledge Management , 1719–1728URL:
https://doi.org/10.1145/2661829.2662088, doi:10.1145/2661829.2662088.

29

9. Appendix

Lemma 3.1.
Proof. This proof uses definitions 3.1-3.4. Since each of the probabilities are impacted by the number tests
allocated there is an implicit assumption that xtest
tz is fixed. Hence, the probability of getting a test given that
the person is positive to be written as,
P[E test |E pos ]

= P[E test |E symp , E pos ]P[E symp |E pos ] + P[E test |E asymp , E pos ]P[E asymp |E pos ]
= aP[E test |E symp , E pos ] + (1 − a)P[E test |E asymp , E pos ]

To find those probabilities we can use the assumptions given by the definitions implicit in the environment,
hence
P[E test |E symp ]

= P[E test |E symp , E pos ]P[E pos |E symp ] + P[E test |E symp , E neg ]P[E neg |E symp ]
= P[E test |E symp , E pos ](P[E pos |E symp ] + P[E neg |E symp ])
= P[E test |E symp , E pos ] = c,

P[E test |E asymp ]

= P[E test |E asymp , E pos ]P[E pos |E asymp ] + P[E test |E asymp , E neg ]P[E neg |E asymp ]
= P[E test |E asymp , E pos ](P[E pos |E asymp ] + P[E neg |E asymp ])
= P[E test |E asymp , E pos ] = d.

Now, those values can be substituted back into the original equation for the probability of getting a test given
that the person is positive,
P[E test |E pos ]

= aP[E test |E symp , E pos ] + (1 − a)P[E test |E asymp , E pos ]
= ac + (1 − a)d.

The same logic is followed for the probability of getting a test given that the person is negative,
P[E test |E neg ]

= P[E test |E symp , E neg ]P[E symp |E neg ] + P[E test |E asymp , E neg ]P[E asymp |E neg ]
= bP[E test |E symp , E neg ] + (1 − b)P[E test |E asymp , E neg ]
= bP[E test |E symp , E pos ] + (1 − b)P[E test |E asymp , E pos ]
= bc + (1 − b)d

Furthermore, the marginal probability of getting a test is given by,
P[E test ]

=

P[E test |E pos ]P[E pos ] + P[E test |E neg ]P[E neg ]

=

inf
(ac + (1 − a)d)pinf
tz + (bc + (1 − b)d)(1 − ptz ).

30

ptest
is defined as P[E tpos |E test , xtest
tz
tz ]. Through Bayes Theorem, the probability of truly being positive given
that a person goes to get tested is given by,
P[E pos |E test ]

=
=

P[E test |E pos ]P[E pos ]
P[E test ]
(ac + (1 − a)d)pinf
tz
inf
(ac + (1 − a)d)pinf
tz + (bc + (1 − b)d)(1 − ptz )

.

These terms can be rearranged to arrive at,
P[E pos |E test ] =

(ac + (1 − a)d) pinf
tz


.
(c − d) (a − b)pinf
+
b +d
tz

Let fp and fn be the probability of false positive and false negative, respectively. The following equation
computes the probability of testing positive given that the person goes to get tested,
ptest
tz

pos
neg
= P[E tpos |E test , E pos , xtest
|E test ] + P[E tpos |E test , E neg , xtest
|E test ]
tz ]P[E
tz ]P[E

=

(1 − fn )P[E pos |E test ] + fp (1 − P[E pos |E test ]).

Lemma 4.1. Let α and β be parameters of a beta distribution. Assume a sample Iˆ from a binomial distribution after n trials and each trail has probability p. The pdf of the prior beta distribution is given by,
π(x|α, β) =

1
xα−1 (1 − x)β−1 ,
B(α, β)

where,
B(α, β) =

Γ(α)Γ(β)
.
Γ(α + β)

The likelihood of observing Iˆt+1 is given by,


P Iˆt+1

 

n
= k|n, p = L(p|k) =   pk (1 − p)n−k .
k

Therefore, the Bayesian update is given by,

P[p|α, β]

=

=

 
n
  xk (1 − x)n−k 1 xα−1 (1 − x)β−1
B(α,β)
k
L(p = x|k)π(x|α, β)
 
=
R1
L(p = x|k)π(x|α, β)dx
R1 n
0
  xk (1 − x)n−k 1 xα−1 (1 − x)β−1 dx
B(α,β)
0
k
xk (1 − x)n−k xα−1 (1 − x)β−1
R1
0

xk (1 − x)n−k xα−1 (1 − x)β−1 dx

= R1
0

xk+α−1 (1 − x)n+β−k−1
xk+α−1 (1 − x)n+β−k−1 dx

The integral in the denominator is given by,
Z 1
xk+α−1 (1 − x)n+β−k−1 dx = B(α + k, n + β − k).
0

31

.

Hence,
xk+α−1 (1 − x)n+β−k−1
,
B(α + k, n + β − k)
which is a beta distribution with parameters α + k and n + β − k. If π(x|α, β) was the prior distribution for the
value of p, then after the observation Iˆ = k the posterior will also have a beta distribution with parameters
α + k and n + β − k. Therefore, the mean of p given by the posterior distribution is
p̂ =

Iˆ + α
.
n+α+β

If X is a random variable with a beta distribution, X ∼ π(x|α, β). Then its expected value is given by,
E[X] =

α
α+β .

Any samples drawn from this distribution can be thought of as being drawn from a binomial

distribution with parameter p being drawn from the prior beta distribution with parameters α and β. The
terms α and β can be thought of as ”pseudo - observations” of the successes and failures. For Lemma 4.1
prior
prior
ˆ
= α, βtz
= β, and Iˆt+1,z = I.
assume n = xtest
tz , αtz

inf
rec
Lemma 4.2. Assume that (Stz , Itz , Rtz ) has a multinomial distribution with parameters (Nz , p̄susc
tz , p̄tz , p̄tz ).

The subpopulations of each zone are not truly independent; however, when the populations of the zones are
large it is a reasonable assumption to make. Let Ytz = min(Stz , xvac
tz ). The percent error introduced into the
model by assuming there are no correlations would be given by,


=
=

cont
vac
cont
cont
|E[min(Stz , xvac
, xvac
, xvac
, xvac
tz )Itz |St
t ] − E[min(Stz , xtz )|St
t ]E[Itz |St
t ]|
vac
cont
vac
E[min(Stz , xtz )Itz |St , xt ]
cont
|Cov[min(Stz , xvac
),
I
|S
, xvac
tz t
tz
t ]|
.
vac
cont
vac
E[min(Stz , xtz )Itz |St , xt ]

Let 0 be the percent error by assuming that Stz and Itz are independent random variables given by,
0

=

inf
|Cov[Stz , Itz |Stcont , xvac
Nz p̄susc
1
t ]|
tz p̄tz
=
=
.
susc p̄inf
E[Stz Itz |Stcont , xvac
]
N
−1
z
N
(N
−
1)p̄
t
z
z
tz
tz

If the errors, 0 , are equal to
Consider the case Stz <

xvac
tz ,

1
Nz −1 ,

then when the population is large the percent error will get very small.

this implies that Ytz = Stz ; hence,  = 0 . The other case is when Stz ≥ xvac
tz , this

vac
cont
vac
cont
implies that Ytz = xvac
, xvac
, xvac
tz which is deterministic; hence,  = E[xtz Itz |St
t ] − xtz E[Itz |St
t ] = 0.

This implies that  ≤ 0 =

1
Nz −1 .

Therefore, the percent errors are less than or equal to

1
Nz −1

which become

very small as Nz gets large when independence is assumed; hence it is a reasonable assumption.
The belief about the environment agent transition functions for time t + 1 is given by,
Sxt

=

+
(St − xvac
t ) ,

St+1

= Sxt − b

It+1

=

Rt+1

Sxt

(1 − γ)It + b

It ,
Sxt

It ,

= Rt + γIt + min(St , xvac
t ).
32

If we assume that the size of the population is fairly large, then through the central limit theorem we claim
that each of the random variables Stz , Itz , and Rtz can be approximated by independent normal distributions

inf
susc
rec
susc 2
with parameters (S̄tz , σtz
), (I¯tz , σtz
) and (R̄tz , σtz
), respectively. Assume Stz ∼ N S̄tz , (σtz
) , Itz ∼


inf 2
N I¯tz , (σtz
) , and independence, then
+
cont
E[(Stz − xvac
, xvac
tz ) Itz |St
t ]

=

x ¯
S̄tz
Itz .

Through the identity in Zhan and Xing (2020), the term in the previous equation is given by,





S̄t,z − xvac
S̄t,z − xvac
tz
tz
vac + cont
vac
vac
susc
+ σtz φ
.
E[(Stz − xtz ) |St , xtz ] =
(S̄t,z − xtz )Φ
susc
susc
σtz
σtz
+
vac
Furthermore, consider the identity (Stz − xvac
tz ) = Stz − min(Stz , xtz ). We can use this identity to estimate
cont
E[min(Stz , xvac
, xvac
tz )|St
tz ] through the linearity of expectation; hence,
cont
x
E[min(Stz , xvac
, xvac
tz )|St
tz ] = S̄tz − S̄tz .

Finally, through the weak law of large numbers, as Nz 7→ ∞ then S̄tz 7→ E[Stz |Stcont , xvac
tz ] because it has finite
variance. The other belief state variables have the same result. Therefore the estimates of the belief state
variables when they are assumed to have independent normal distributions are given by,
S̄tz

susc
= E[Stz |Stcont , xvac
tz ] = Nz p̄tz ,

I¯tz

inf
= E[Itz |Stcont , xvac
tz ] = Nz p̄tz ,

R̄tz
susc
σtz

rec
= E[Rtz |Stcont , xvac
tz ] = Nz p̄tz ,
q
p
susc
=
V ar[Stz |Stcont , xvac
Nz p̄susc
tz ] =
tz (1 − p̄tz ).

We estimate each expectation by assuming the random variables are normally distributed. Hence,
S̄t+1,z

I¯t+1,z

βz
+
cont
(Stz − xvac
, xvac
tz ) Itz |St
t ],
Nz
βz
+ cont
+
cont
E[(Stz − xvac
, xvac
= E[(Stz − xvac
, xvac
tz ) |St
t ]−
tz ) Itz |St
t ],
Nz


x
=
1 − βz p̄inf
S̄tz
.
tz
+
= E[(Stz − xvac
tz ) −

= E[(1 − γ)Itz +
=

R̄t+1,z

βz
+
cont
(Stz − xvac
, xvac
tz ) Itz |St
t ],
Nz

x
(1 − γ)I¯tz + βz p̄inf
tz S̄tz .

cont
= E[Rtz + γItz + min(Stz , xvac
, xvac
tz )|St
t ],
cont
vac
cont
= E[Rtz |Stcont , xvac
, xvac
, xvac
t ] + E[γItz |St
t ] + E[min(Stz , xtz )|St
t ]

=

x
R̄tz + γ I¯tz + S̄tz − S̄tz
.

Lemma 5.1.

33

Proof. The optimization problem for the learning policy is given by,
min
test

x

X

test
V ar[p̄inf
)|Stcont ],
t+1,z (x

z∈Z

subject to 1T xtest ≤ ntest
.
t
The objective function for the learning policy is given by equation (44). The objective function can be rewritten
as,
X

cont
V ar[p̄inf
]=
t+1,z |St

z∈Z

X xtest αprior β prior (αprior + β prior + xtest )
tz
tz
tz
tz
tz
tz
prior
prior 2
prior
prior
(αtz
+ βtz
) (αtz
+ βtz
+ 1)

z∈Z

.

prior
prior
Recall that αtz
+ βtz
= Nz for all zones therefore it is a constant. Which means that,

X

cont
V ar[p̄inf
]
t+1,z |St

=

z∈Z

X xtest αprior β prior (αprior + β prior + xtest )
tz
tz
tz
tz
tz
tz
z∈Z

=

prior
prior 2
prior
prior
(αtz
+ βtz
) (αtz
+ βtz
+ 1)

,

X xtest αprior β prior (Nz + xtest )
tz
tz
tz
tz
,
(Nz )2 (Nz + 1)

z∈Z
dT xtest
t

where d =

h

prior
αprior
βtz
tz
Nz (Nz +1)

i
z∈Z

=
+ (xtest
)T Dxtest
,
t
t
h

i
prior
αprior
βtz
tz
and D = diag
. The constraint does not change.
N 2 (Nz +1)
z

z∈Z

9.1. DLA policy solution


x̃tt



 be a decision vector where the first |Z| entries are the lookahead decision
Lemma 9.1. Let x̃t = 
x̃t,t+1
vectors for time t and the last |Z| entries are the lookahead decision vectors for time t + 1. The solution to
the two-step lookahead problem in Equation (43) becomes a quadratic program if 1T S̃θtt > nvac
t , otherwise the
solution is xDLA
= S̃θtt . The quadratic program for the non-trivial solution is given by,
t
arg max x̃Tt Qx̃t + qT x̃t
x̃t

subject to K x̃t ≤ h

34

(49)

where,
Q =

q

=

K

=

h

=



diag([b u w) diag([ 21 b u])

,
diag([ 12 b u])
0


(2 − γ)u + b (u z + w v)

,
−b v


I|Z|
0

,
−w I|Z|
 
S̃θ
 tt  ,
z

h
i
h
i
h
h i
βz ˜
βz ˜
βz
I
I
−
1
,
u
=
−
,
w
=
,
v
=
(1 − γ)I˜ttz +
where each term is b = N
ttz
ttz
Nz
Nz
z∈Z
z∈Z
h
i z z∈Z
βz θ ˜
θ
−N
z = S̃ttz
S̃ttz Ittz
, and I|Z| is the identity matrix.
z

βz θ ˜
Nz S̃ttz Ittz

i
z∈Z

z∈Z

θ
θ
cont
) be the lookahead state variable for time t at time t. The lookahead state
, I˜ttz , R̃ttz
= (S̃ttz
Proof. Let S̃tt

variable is a point estimate of the distribution represented by the belief state at a tunable quantile. Equations
(27 - 30) describe how each of the lookahead state variables will transition. The objective function in equation
(43) becomes
cont
cont
C̃(S̃tt
, x̃tt ) + C̃(S̃t,t+1
, x̃t,t+1 ) = I˜t,t+1,z + I˜t,t+2,z ,

=

X

I˜t,t+1,z + (1 − γ)

z∈Z

=

(2 − γ)

X

I˜t,t+1,z +

z∈Z

X
z∈Z

I˜t,t+1,z

X βz
I˜t,t+1,z (S̃t,t+1,z − x̃t,t+1,z )+
Nz

z∈Z

X βz
+
I˜t,t+1,z (S̃t,t+1,z − x̃t,t+1,z )+ .
Nz
z∈Z

We know from equation (29) that the transition equation for I˜t,t+1,z is given by,
βz ˜
θ
Ittz (S̃ttz
I˜t,t+1,z = (1 − γ)I˜ttz +
− x̃ttz )+ .
Nz

(50)

Since these the lookahead state variables are deterministic point estimates, then including the constraint
S̃ttz > x̃ttz will not change the results of the optimization problem in equation (43). The constraint implies
(S̃ttz − x̃ttz )+ = S̃ttz − x̃ttz . Assume the following vectors are defined by,


βz
,
b =
Nz z∈Z


βz ˜
u =
−
Ittz
,
Nz
z∈Z


βz ˜
w =
Ittz − 1
,
Nz
z∈Z


βz θ ˜
v =
(1 − γ)I˜ttz +
S̃ttz Ittz
,
Nz
z∈Z


βz θ ˜
θ
z =
S̃ttz −
S̃ Ittz
.
Nz ttz
z∈Z
35

,

The sum of the terms in equation (50) can be rewritten as,
X

I˜t,t+1,z = 1T v + uT x̃tt .

z∈Z

The objective function can then be rewritten as,
=

(2 − γ)(1T v + uT x̃tt ) + (b

(v + u

where we also assume the constrain z + w

x̃tt ))T (z + w

x̃tt ) − (b

(v + u

x̃tt ))T (x̃t,t+1 ),

x̃tt > x̃t,t+1 . The terms that do not depend on x̃t,t+1 or x̃tt can

be removed from the objective function; hence,
=

(2 − γ)(uT x̃tt ) + (b

(v + u

x̃tt ))T (z + w

x̃tt ) − (b

(v + u

x̃tt ))T (x̃t,t+1 ),

(2 − γ)(uT x̃tt ) + (b (z u + v w))T x̃tt + x̃Ttt diag(b u w)x̃tt − (b (v + u x̃tt ))T (x̃t,t+1 ).


x̃tt
 be a decision vector where the first |Z| entries are the lookahead decision vectors for time t
Let x̃t = 
x̃t,t+1
and the last |Z| entries are the lookahead decision vectors for time t+1. Then the objective function rearranges
=

to the following equation quadratic function,
arg max x̃Tt Qx̃t + qT x̃t
x̃t

subject to Kx̃t ≤ h
where,
Q =

q

=

K

=

h

=



diag([b u w) diag([ 21 b u])
,

diag([ 12 b u])
0


(2 − γ)u + b (u z + w v)

,
−b v


I|Z|
0

,
−w I|Z|
 
S̃θ
 tt  .
z

36

