COVID-19 Contact Tracing: Eight Privacy Questions Explored

arXiv:2005.11416v1 [cs.CY] 22 May 2020

A reply to de Montjoye et al.

HUGH LAWSON-TANCRED, Birbeck, University of London, UK
HENRY C. W. PRICE, Imperial College, UK
ALESSANDRO PROVETTI, Birbeck, University of London, UK
We respond to a recent short paper by de Motjoye et el. on privacy issues with Covid-19 tracking. Their paper, which we discuss here,
is structured around three “toy protocols” for the design of an app which can maximise the utility of contact tracing information while
minimising the more general risk to privacy. On this basis, the paper proceeds to introduce eight questions against which they should
be assessed. The questions raised and the protocols proposed eﬀectively amount to the creation of a game with diﬀerent categories
of players able to make diﬀerent moves. It is therefore possible to analyse the model in terms of optimal game design.

1 INTRODUCTION
The current literature on Covid-19 seems unanimous in recognizing that, as no treatment is yet available, the only
tools that we can currently deploy to stop the epidemic are contact tracing, social distancing, and quarantine. In this
work we discuss contact tracing from an Ethics of Computing point of view. In fact, work on Covid-19 contact tracing,
e.g., [Ferretti et al. 2020] is mostly focussed on “algorithmic” instantaneous contact tracing assisted by a mobile phone
application. The DP-3T project by [Troncoso et al. 2020] is an example of research and implementation eﬀort that
addresses this urgent need.
Recently, a short paper by [de Montjoye et al. 2020] has introduced and examined three toy protocols that summarise
the privacy questions surrounding the Covid-19 tracing. They describe the following 3 toy protocols for discussion:
Toy protocol 1: using location
Each app only records its own location. As a result,
• When a user reports as infected, they send their trajectory (location and time) to the authority.
• The authority, e.g., the NHS in Britain, shares the pseudonymous trajectories of all infected users with every
user.
• Users can then check if they were in close contact with an infected individual.
Toy protocol 2: using Bluetooth.
Each app broadcasts a unique identiﬁer assigned by the authority through Bluetooth. So
• When two phones are near to one another, they exchange these identiﬁers.
• When a user reports as infected, they send all the identiﬁers they encountered to the authority.
• The authority will contacts all the users whose identiﬁer was encountered by an infected user.
Authors’ addresses: Hugh Lawson-Tancred, , Birbeck, University of London, Institute for Data Analytics, London, WC1E 7HX, UK; Henry C. W. Price, ,
Imperial College, Centre for Complexity Science and Theoretical Physics Group, London, SW7 2AZ, UK; Alessandro Provetti, ale@dcs.bbk.ac.uk, Birbeck,
University of London, Department of Computer Science and Information Systems, London, WC1E 7HX, UK.

1

2

Lawson-Tancred et al.

Toy protocol 3: again using Bluetooth.
In this version each app broadcasts a unique identiﬁer using Bluetooth, assigned by the authority. This unique identiﬁer
is reset every hour. Thanks to temporary indentiﬁers,
• When two phones are near to one another, they exchange these identiﬁers.
• When a user reports as infected, they send all the identiﬁers that they have used (one per hour) to the authority.
• The authority shares the identiﬁers of all infected users with every user. Users can then check if they encountered
one of these identiﬁers recently.
1.1

The Questions

[de Montjoye et al. 2020] discuss the following privacy questions, all of them related to the introduction of Covid-19
algorithmic contact tracing.
(1) How do you limit the personal data gathered by the authority?
(2) How do you protect the anonymity of every user?
(3) Does your system reveal to the authority the identity of users who are at risk?
(4) Could your system be used by users to learn who is infected or at risk, even in their social circle?
(5) Does your system allow users to learn any personal information about other users?
(6) Could external parties exploit your system to track users or infer whether they are infected?
(7) Do you put in place additional measures to protect the personal data of infected and at risk users?
(8) How can we verify that the system does what it says?
2 BACKGROUND
The overall purpose of the game is to maximise (pure) contact information while at the same time minimising (noncontact) personal information. As a typical user, Alice should know that she has been in contact with somebody (in
fact Bob) without knowing that her contact is indeed Bob or indeed knowing anything else at all about Bob. Her
ignorance of the latter two facts is as important on privacy grounds as her knowledge of the ﬁrst on grounds of safety
[Radaelli et al. 2018].
Contact information, however, can connect with personal information in two ways. The ﬁrst way is that (i) personal
information is needed to establish contact information [Nat 2020], and the second is that (ii) personal information can
relatively easily be derived from contact information [Gong et al. 2014]. The ideally-designed game will enable the
exchange of contact information without such information having to be established on the basis of any more general
personal information or being the possible source of a derivation of such more general personal information.
Contact tracing should not involve any sharing with either other users or the authority of trajectory/social graph
information (from which identiﬁcation is possible either by the authority or by the adversary). Given these overall
parameters, it seems possible that both the protocols and the questions/answers in the paper could be advantageously
altered.
Secondly and more importantly, there is considerable scope for changing the proposed protocols in order to enable
optimisation of the data ﬂow objectives. On this basis, the ﬁrst protocol needs to be amended so that the authority
does not reveal the entire trajectory of an infected user to all non-infected users.
To avoid such undesirable disclosure, the authority has to know the trajectories of all users, whether infected or not.
This arrangement, by which all information is accumulated with the authority and the minimum possible disclosed to

COVID-19 Contact Tracing: Eight Privacy Questions Explored

3

users, could be protocol “1a.” It seems to be a limitation of trajectory-based apps (and therefore a reason for preferring
identiﬁer-based apps) that with them it is not possible to avoid disclosure of entire trajectories to either other users
or the authority (or both). A decision therefore has to be taken, on this point, between dispersion and aggregation of
information.
This is presumably at least part of the motivation for considering identiﬁer-based protocols; however the diﬀerences
between the second and third protocols could also be clariﬁed further. There are two such diﬀerences. The ﬁrst is that
protocol 2 has a ﬁxed identiﬁer, whereas protocol 3 has a variable identiﬁer (to use a suboptimal term). This is the more
conspicuous diﬀerence, and it plays a larger part in the response to the questions [The National Cyber Security Centre
2020]. The second diﬀerence, however, is that protocol 2 also sends its full history of identiﬁer encounters to the
authority, whereas article 3 only sends its identiﬁer change record. In the case of protocol 3, the authority is not able
to ﬁgure out for itself the now at risk users. So it sends the variable identiﬁers whose status has changed to infect.
This seems to decrease the privacy of the infected users while increasing that of the non-infected users (both at risk
and risk-free), see, e.g., [Sharad and Danezis 2014] for early results on de-anonymisation. This diﬀerence also seems
to have a material eﬀect on the privacy vulnerability. For example, does it reduce or increase the knowledge of the
authority about trajectories/social graphs of either group? If the authority is able to connect the varying identiﬁers,
then it acquires a ﬁner grained level information about the relevant users. Protocol 3 is, therefore, in eﬀect a bet on
the inability of the authority to spot the continuities in series of variable identiﬁers.
There seems to be a further assumption built into protocol 3. Any information available either to the authority or
to any or all users is in principle also potentially available to the adversary. The existing game model thus makes the
further assumption that identiﬁer information on mobile phones is more open to hacking than trajectory information.
However, such an assumption is not immune to challenge. It needs to be clariﬁed what are the relative strengths and
weaknesses of the protocols with respect to the adversary.
Thirdly, the answers to the speciﬁc questions also need to be reviewed (partly in the light of the queries about the
protocols). We now look in turn at some of the ways in which the answers to the eight questions could be changed.

3 A RESPONSE TO THE QUESTIONS
Question 1.
Protocol 1 obviously discloses the whole trajectory of infected users to the authority. The disclosure also to non-infected
users can be avoided, but only at the cost of non-infected users also revealing their entire trajectories to the authority.
This would be the move from protocol 1 to protocol 1a. If there is perceived to be an inverse connection between threat
status and privacy entitlement, then it would seem that this change of trajectory-based protocol would be unfair. A
larger number of users who do not constitute a threat would see their privacy eroded in order to protect the privacy of
the smaller number who have become infected. (It should be noted that this objection applies irrespective of whether
or not any form of blame is to be attributed to the change to infected status (e.g. by disregarding social distancing etc).
In terms of the identiﬁer-based protocols, the improvement provided by protocol 3 over protocol 2 depends on the
authority not being able to reconstruct the pseudonymous social graph across the changes of identiﬁer. As discussed
in connection with the protocols, however, it is not obvious that it will not be possible for the authority to do that.

4

Lawson-Tancred et al.

Question 2.
The same objection to the greater innocuousness of protocol 3 over protocol 1 and protocol 2 arises as with question 1.
Presumably if the game is to rely on "special measures" to "limit the risk", then those special measures should be applied
at the level of the authority not the users (where they can more easily be circumvented and less easily monitored). It
could also be argued that re-identiﬁcation by either the authority or other users automatically raises the risk of reidentiﬁcation by the adversary.

Question 3.
Protocol 1 does indeed give the right answer on this question, but only at the cost of giving too much information to
non-infected users. Again, the diﬀerence between protocol 2 and protocol 3 is not clear. It is a reasonable supposition
that the identities of the infected group are more sensitive than those of the non-infected. The former are (presumably)
less numerous, but they are more vulnerable to potential stigmatisation/vigilantism. If that assumption is right, it
would form a strong objection to protocol 1. What this suggests is that a more nuanced distinction needs to be drawn
between the type of threat posed by the authority and that posed by other users.

Question 4.
Protocol 1 obviously fails this test in its existing form, but that can be prevented by letting the authority know the
trajectories of non-infected users (protocol 1a again). Protocol 3 seems to be deﬁnitely worse than protocol 2 on this
question, however it is tweaked. This reinforces the suspicion that protocol 3 is not preferable to protocol 2 in any
respect. Given the presumably greater practical diﬃculty of deploying protocol 3, this would seem to be a very solid
grounds for rejection of protocol 3 in general.

Question 5.
This seems to be the crucial question for the overall objective of the game as outlined at the start. It is not obvious how
the protocols can be structured to enable either users or the authority to gain only and exclusively speciﬁc contact
information without either supporting it with more general personal information or creating a situation in which
wider personal information can be triangulated from the contact information. This highlights exactly what any privacysecure contact tracing must achieve: the Holy Grail is pure contact information, uncontaminated by any non-contact
personal information.

Question 6.
This seems to raise again the question whether concentrating information with users or with the authority constitutes the greater security risk. Most recent major hacks have focused on concentrations of data, suggesting the more
disaggregation the better. Hacks from large numbers of dispersed users have been less eﬀective, so far as can be known.

Question 7.
It is not clear what additional protections could be made available. One possibility would be some kind of time limitation of the information or preventions on its further disclosure. Presumably this would in practice be a question

COVID-19 Contact Tracing: Eight Privacy Questions Explored

5

about encryption rather than game design. The other obvious way to develop such protections would be through legal/regulatory constraints, but they would also clearly fall outside the scope of the intrinsic app-modelling game under
consideration.
Question 8.
As in many other areas of data protection, the may be a trade-oﬀ between the transparency of the system and its
privacy-protection and/or security [de Montjoye et al. 2013]. It may not be possible simultaneously to optimise all
three parameters. On the other hand, a possible way of at least partially squaring this circle is that some form of
blockchain might be deployed here.
4 CONCLUSIONS
The game-design approach of [de Montjoye et al. 2020] seems to oﬀer, perhaps with some modiﬁcation, a good basis for
a design intended to optimise the protection of the relevant moral assets. Non-infected users have an interest in learning
about vulnerability-increasing contacts, but have no right to any other information about infected users. Infected users
have a duty to maximise knowledge about their contacts, but the right not to have any further information about them
disclosed. (As we have seen, the right of the infected might outweigh that of the non-infected on the grounds of the
risk of vigilantism, whereas it might also be thought to be outweighed because of the possible culpability of infection
given the level of public knowledge [Rosand et al. 2020]. This is clearly a value, not a pure design, issue.) The authority
has the right (and possibly duty) to be as informed as possible about the pattern of spread of the epidemic, but it should
be prevented from acquiring (and indeed retaining) any more than the essential information about the users under its
jurisdiction. The adversary has no rights in this context and is simply a threat to be minimised.
The most signiﬁcant improvement to the approach proposed, in our opinion, would be to replace protocol 1 with
our protocol 1a. We are agnostic as to the general preference for trajectory/identiﬁer approaches, but we suspect that
with the latter protocol 3 on balance creates a greater privacy risk than protocol 2.
ACKNOWLEDGMENTS
This article is based upon work from COST Action DigForAsp CA17124, supported by COST (European Cooperation
in Science and Technology): www.cost.eu.
REFERENCES
Yves-Alexandre de Montjoye, César A. Hidalgo, Michel Verleysen, and Vincent D. Blondel. 2013. Unique in the Crowd: The privacy bounds of human
mobility. Scientiﬁc Reports 3, Article 1376 (March 2013), 1376 pages. https://doi.org/10.1038/srep01376
Yves-Alexandre de Montjoye, Florimond Houssiau, Andrea Gadotti, and Florent Guepin. 2020.
Evaluating COVID-19 contact tracing
apps? Here are 8 privacy questions we think you should ask.
Technical Report. Imperial College, Computational Privacy Group.
https://cpg.doc.ic.ac.uk/blog/evaluating-contact-tracing-apps-here-are-8-privacy-questions-we-think-you-should-ask/
Luca Ferretti, Chris Wymant, Michelle Kendall, Lele Zhao, Anel Nurtay, Lucie Abeler-Dörner, Michael Parker, David Bonsall, and Christophe
Fraser. 2020.
Quantifying SARS-CoV-2 transmission suggests epidemic control with digital contact tracing.
Science 368, 6491 (2020).
https://doi.org/10.1126/science.abb6936 arXiv:https://science.sciencemag.org/content/368/6491/eabb6936.full.pdf
Lei Gong, Takayuki Morikawa, Toshiyuki Yamamoto, and Hitomi Sato. 2014. Deriving Personal Trip Data from GPS Data: A Literature Review on
the Existing Methodologies. Procedia - Social and Behavioral Sciences 138 (2014), 557 – 565. https://doi.org/10.1016/j.sbspro.2014.07.239 The 9th
International Conference on Traﬃc and Transportation Studies (ICTTS 2014).
Nat. 2020. Show evidence that apps for COVID-19 contact-tracing are secure and. Nature 580 (apr 2020). https://doi.org/10.1038/d41586-020-01264-1
Laura Radaelli, Piotr Sapiezynski, Florimond Houssiau, Erez Shmueli, and Yves-Alexandre de Montjoye. 2018. Quantifying Surveillance in the Networked
Age: Node-based Intrusions and Group Privacy. arXiv e-prints, Article arXiv:1803.09007 (March 2018), arXiv:1803.09007 pages. arXiv:cs.CY/1803.09007

6

Lawson-Tancred et al.

Eric Rosand, Khalid Koser, and Lilla Schumicky-Logan. 2020.
Preventing violent extremism during and after the COVID-19 pandemic.
https://www.brookings.edu/blog/order-from-chaos/2020/04/28/preventing-violent-extremism-during-and-after-the-covid-19-pandemic/
Kumar Sharad and George Danezis. 2014. An Automated Social Graph De-Anonymization Technique. In Proceedings of the 13th Workshop on Privacy in
the Electronic Society (WPES 14). Association for Computing Machinery, New York, NY, USA, 47–58. https://doi.org/10.1145/2665943.2665960
The National Cyber Security Centre. 2020. The security behind the NHS contact tracing app. https://www.ncsc.gov.uk/blog-post/security-behind-nhs-contact-tracing-app
Carmela Troncoso, Mathias Payer, Jean-Pierre Hubaux, Marcel SalathÃľ, James Larus, Edouard Bugnion, Wouter Lueks, Theresa Stadler, Apostolos Pyrgelis, Daniele Antonioli, Ludovic Barman, Sylvain Chatel, Kenneth Paterson, Srdjan Capkun, David Basin, Jan Beutel, Dennis Jackson, Bart Preneel, Nigel
Smart, Dave Singelee, Aysajan Abidin, Seda GÃĳrses, Michael Veale, Cas Cremers, Michael Backes, Reuben Binns, Ciro Cattuto, Giuseppe Persiano,
Dario Fiore, Manuel Barbosa, and Dan Boneh. 2020. DP-3T: Decentralized Privacy-Preserving Proximity Tracing. https://github.com/DP-3T/documents

