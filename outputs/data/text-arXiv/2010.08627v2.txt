Minimax Quasi-Bayesian estimation in sparse canonical correlation
analysis via a Rayleigh quotient function
Qiuyun Zhu∗1 and Yves Atchade†1
1

Department of Mathematics and Statistics, Boston University

arXiv:2010.08627v2 [stat.ML] 12 Feb 2021

Abstract
Canonical correlation analysis (CCA) is a popular statistical technique for exploring the
relationship between datasets. The estimation of sparse canonical correlation vectors has
emerged in recent years as an important but challenging variation of the CCA problem, with
widespread applications. Currently available rate-optimal estimators for sparse canonical correlation vectors are expensive to compute. We propose a quasi-Bayesian estimation procedure
that achieves the minimax estimation rate, and yet is easy to compute by Markov Chain Monte
Carlo (MCMC). The method builds on ([37]) and uses a re-scaled Rayleigh quotient function
as a quasi-log-likelihood. However unlike these authors, we adopt a Bayesian framework that
combines this quasi-log-likelihood with a spike-and-slab prior that serves to regularize the inference and promote sparsity. We investigated the empirical behavior of the proposed method
on both continuous and truncated data, and we noted that it outperforms several state-ofthe-art methods. As an application, we use the methodology to maximally correlate clinical
variables and proteomic data for a better understanding of covid-19 disease.

1

Introduction

Canonical correlation analysis is a statistical technique –dating back at least to [18] – that
is used to maximally correlate multiple datasets for joint analysis. The technique has become
a fundamental tool in biomedical research where technological advances have made it possible
to observe fundamental biological phenomena from multiple viewpoints, the so-called multi-omic
datasets ([42]; [26]; [29]). Over the past two decades, limited sample sizes, growing dimensionality,
and the search for meaningful biological interpretations, have led to the development of sparse
canonical correlation analysis ([41, 42, 28, 40, 17]), where a sparsity assumption is imposed on the
canonical correlation vectors.
Statistically optimal estimation of sparse CCA has been considered in several works. ([13])
derived the minimax rate of estimation of sparse CCA, and proposed a two-stage estimation
procedure that achieves the rate. ([37]) uses a generalized Rayleigh quotient approach to propose
a two-stage estimator that also achieves the minimax rate. The main limitation of these rateoptimal estimation procedures is their high computational cost. Specifically, in both approaches,
∗
†

rachyun@bu.edu
atchade@bu.edu

1

each iteration of the first-stage optimization problem has a computational cost of O(p3 ), where
p is the joint number of variables in the datasets. Furthermore, the two-stage nature of these
estimators can also be a problem in practice, since it can be hard to set the required stopping
criterion of the first-stage solver that guarantees a good behavior of the final estimator.
We address these issues by proposing in this paper a conceptually simple, yet rate-optimal
quasi-Bayesian estimator for sparse CCA. More specifically, building on ([37]), we propose a
quasi-Bayesian or (PAC-Bayesian) approach that employs a re-scaled version of the Rayleigh
quotient function as a quasi-log-likelihood function together with a spike and slab prior to obtain
a quasi-posterior distribution. Our sparse CCA estimator is then defined as a posterior mean
of the quasi-posterior distribution. The method is agnostic to the covariance matrix estimators
used in constructing the Rayleigh quotient function. For example, we observe in our experiments
that both the sample covariance matrix estimator and the Kendall’s-tau-based covariance matrix
estimator ([43]) work well, even when they are singular. In fact, although we do not pursue this
here, one can straightforwardly extend our method to solve other generalized eigenvalue problems
in the same spirit as [37].
We analyze the proposed estimator and derive its convergence rate (see Theorem 4). In the
particular case where sample covariance matrices are used to estimate the Rayleigh quotient, we
show that the estimator achieves the minimax rate for sparse CCA estimation, under some modest
sample size condition.
The idea of using a Bayesian framework to produce frequentist estimators is of course wellknown in statistical decision theory. The extension of the idea to allow for pseudo-likelihood
functions is more recent in the statistics and econometrics literature ([10]), but has a longer
history in machine learning under the name of PAC-Bayesian learning ([23, 9]).
We propose a Markov Chain Monte Carlo algorithm based on simulated tempering to sample
from the quasi-posterior distribution, and compute the estimator. In stationarity, the proposed
algorithm has a per-iteration cost of O(s2? p), where s? is the true sparsity level of the signal.
Furthermore, we show empirically that for sufficiently large sample size, the mixing time of the
algorithm scales linearly in p. Overall, our estimator has a more favorable computational cost
than the Rifle estimator of ([37]).
The remaining of the paper is organized as follows. In Section 2 we introduce our estimation
procedure and derive its convergence rate. In Section 3 we detail a simulated tempering algorithm
to sample from the resulting quasi-posterior distribution. In Section 4, we study the numerical
behavior of the proposed method on both continuous and truncated data, and compare it with
other methods. In Section 5, we apply the method in a real data analysis that aims to correlate
clinical and proteomic data from covid-19 patients, for a better understanding of the disease.
Our analysis identifies Alpha-1-acid glycoprotein 1 (AGP 1) as playing an important role in the
progression of Covid-19 into a severe illness.

2

2

Quasi-Bayesian sparse canonical correlation analysis using a
Rayleigh quotient function

Let (X, Y ) ∈ Rpx × Rpy be a pair of high-dimensional zero-mean random vectors with joint
def
def
def
distribution f , with covariance matrices Σx = E(XX T ), Σy = E(Y Y T ) and Σxy = E(XY T ). Let
(vx? , vy? ) ∈ Rpx × Rpy be a pair of principal canonical correlation vectors for f . That is, the vector
(vx? , vy? ) ∈ Rpx × Rpy solves the following optimization problem:
max

vx ∈Rpx , vy ∈Rpy

vxT Σxy vy s.t.

vxT Σx vx = vyT Σy vy = 1.

(1)

def

T
T T
, vy?
) which is our main parameter of interest. Without any
It what follows we set θ? = (vx?
loss of generality we assume that kθ? k2 = 1. Clearly, θ? is estimable only up to a sign. If we set
def
p = px + py , and define the matrices
#
"
#
"
0 Σxy
Σx 0
def
def
def
, and Σ = A + B,
(2)
A =
,
B =
T
Σxy
0
0 Σy

then it is easily seen that problem (1) is equivalent to the following generalized eigenvalue problem
(GEP):
max
θT Aθ s.t. θT Bθ = 2.
(3)
√

θ=(vxT ,vyT )T ∈Rp

This is because max{ xy : x ≥ 0, y ≥ 0, x + y = 2} is achieved by taking x = y = 1. Clearly,
finding a solution of (3), is equivalent to finding a solution of
def

max

θ=(vxT ,vyT )T ∈Rp

R(θ) =

θT Aθ
,
θT Bθ

(4)

where we convene that 0/0 = 0. The objective function R in (4) is known as the (generalized)
Rayleigh quotient of A and B. To the best of our knowledge the idea of estimating the sparse
canonical correlation vectors by directly targeting the Rayleigh quotient was first developed by
([37]). Note that solving (4) in practice requires specifying matrices A and B, which are typically
unknown. In practice, one often constructs estimators of A and B, denoted by Â and B̂, respecdef
tively, based on n i.i.d. samples Z = {(X1 , Y1 ), . . . , (Xn , Yn )} from f . Specifically, given Z, one
first constructs estimators of Σx , Σy , and Σxy , denoted by Σ̂x , Σ̂y , and Σ̂xy , respectively, and then
constructs Â and B̂ from Σ̂x , Σ̂y , and Σ̂xy , in the same fashion as in (2) – In Section 4, we will
provide some examples of how to construct the estimators Σ̂x , Σ̂y , and Σ̂xy . One then replaces
the Rayleigh quotient by its sample version defined as
def

Rn (θ; Z) =

θT Âθ

, θ ∈ Rpx +py .

θ B̂θ
It is worth mentioning that in the high-dimensional regime, direct maximization of Rn poses
problems (the constructed estimators Σ̂x and Σ̂y , e.g. sample covariance matrices, are usually
singular, and inconsistent). [37] addressed these issues by maximixing Rn (·; Z) under a sparsity
contraint. The authors show that this maximization problem can be solved provided that a good
initial value is provided. However, finding a good initial value is very costly. Furthermore, given
that the Rayleigh quotient typically admits several local maxima (and possibly minima), and
given the need to encode a prior sparsity information, we argue that a Bayesian framework is a
more effective approach for leveraging the Rayleigh quotient for sparse CCA.
T

3

2.1

A Quasi-Bayesian approach

We propose to use the quasi-Bayesian framework to turn the Rayleigh quotient CCA estimator
of ([37]) into a Bayesian procedure. More precisely, we shall infer θ? by using the function
θ 7→ nRn (θ; Z)

(5)

as a log-likelihood in a Bayesian inference for θ? . For the prior distribution on θ, we choose the
spike-and-slab distribution, a common choice for Bayesian sparse modeling ([15]). Given a variable
def
selection parameter δ ∈ ∆ = {0, 1}p , let the conditional distribution of θ given δ be
(
p
Y
N(0, ρ−1
if δj = 1
1 ),
π(θ|δ) =
π(θj |δ), where θj |δ = θj |δj ∼
,
(6)
−1
N(0, ρ0 ),
if δj = 0
j=1
and ρ0 > ρ1 > 0 are precision parameters. In addition, the prior distribution of δ is taken as
an independent product of Bernoulli distributions with odd-ratio p−u for some parameter u > 1,
constraint to be s-sparse, for some sparsity parameter 1 ≤ s ≤ p:
 kδk0
1
def
π(δ) ∝
(7)
1∆s (δ), where ∆s = {δ ∈ ∆ : kδk0 ≤ s}.
pu
If we combine the spike-and-slab prior with the quasi-log-likelihood in (5), we then obtain the
quasi-posterior distribution


ρ0
ρ1
(8)
Π(δ, dθ|Z) ∝ 1∆s (δ) exp akδk0 − kθδ k22 − kθ − θδ k22 exp (nRn (θδ ; Z)) dθ,
2
2
where θδ is the entry-wise product of θ and δ, k · k2 the Euclidean norm, and
 r 
1 ρ1
def
a = log
.
pu ρ0
Given the quasi-posterior distribution Π(·|Z) we propose to estimate the projector θ? θ?T associated to θ? by
b def
P
=

Z
∆s ×Rp

θδ θδT
Π(dδ, dθ|Z).
kθδ k22

(9)

The spike-and-slab prior specified in (6) and (7) is fairly standard, and goes back at least to
([15]). However the way it is combined with the pseudo-likelihood to give (8) is nonstandard, and
follows ([2]). The approach can be viewed as an approximation of the point-mass spike-and-slab
prior ([25]), using the pseudo-prior device of ([8]). The appealing feature of this approach is that
in (8) the parameter θ enters the likelihood only through its sparsified form θδ . This has the
effect of decoupling the active components (those corresponding to δj = 1) and the non-active
components, and is particularly attractive from the computational standpoint. The parameter
ρ0 – the precision parameter of the non-selected components – has no effect on the statistical
recovery of the selected components of θ, but can adversely impact the MCMC mixing if a value
too large is used. We suggest setting ρ0 = n, in order to match the posterior variance of selected
components that are actually 0. The posterior distribution Π is very robust to the choice of ρ1
and u, and we find that choosing ρ1 ≈ 1, and u ∈ (1, 2] works well. We discuss the choice of s in
the next sub-section.
4

2.1.1

Existence of Π(·|Z)

Clearly Π(·|Z) is not a posterior distribution in the standard sense, since nRn is not a proper
log-likelihood. As such it may not be well-defined. The next proposition gives a simple condition
under which Π(·|Z) is well-defined.
Proposition 1. Suppose that Z is such that
for all θ ∈ Rp such that 0 < kθk0 ≤ s.

θT B̂θ > 0,

(10)

Then Π(·|Z) is a well-defined probability measure on ∆ × Rp .
Proof. Indeed, under the stated assumption the normalizing constant of Π(·|Z) is bounded from
above by
Z
λ
(Â,s)
λ
(Â,s)
X
n max
n max
π(θ|δ)e λmin (B̂,s) dθ ≤ e λmin (B̂,s) < ∞,
π(δ)
Rp

δ∈∆s

where λmax (A, s) (resp. λmin (A, s))) denotes the largest (resp. smallest) eigenvalues of submatrices obtained by taking at most s row and columns of A (see below for precise definition).
Remark 2. Condition (10) is typically not stringent. For instance, if B̂ is formed from the sample
covariance matrices of X and Y , and X and Y are sub-Gaussian random vectors, then with high
probability (10) holds as soon as the sample size satisfies n ≥ Cs log(px ∨ py ), where C is some
absolute constant that depends only on Σx , Σy and the sub-Gaussian norm of X, Y .
Proposition 1 sheds light on the sparsity parameter s. Proposition 1 shows that the sparsity
parameter s should be chosen such that λmin (B̂, s) > 0. In view of the last remark the choice
s=

cn
,
log(p)

for some constant

c≈1

seems natural. When s? = kθ? k0 is much smaller than n/ log(p), the MCMC sampler can actually
be safely run without explicitly specifying a value for s, that is without the need to enforce the
constraint δ ∈ ∆s . Indeed by posterior contraction, the MCMC sampler in those cases never drift
toward big models. In all our simulations we have implemented the algorithm without specifying
s.

2.2

Rate of convergence

b of θ? θT introduced in (9) is
We show in this section that the quasi-Bayesian estimator P
?
rate-optimal – in a frequentist sense. For M, N ∈ Rq×q , for some q ≥ 1, we define
q
def
def
def
sup
kM uk2 .
hM, N iF = Tr(M T N ), kM kF = hM, M iF , and kM kop =
u∈Rq : kuk2 =1

For J ⊆ {1, . . . , q}, MJ,J denotes the submatrix (Mij )i,j∈J . Given an integer k ≥ 1, we set
def

λmin (M, k) =

min

u∈Rq , kuk2 =1,kuk0 ≤k

uT M u,

def

and λmax (M, k) =

max

u∈Rq , kuk2 =1,kuk0 ≤k

Given integer α ≥ 1, we set
(α)

def

λmax (M, s) =

max

J⊆[1:q], kJk0 =s

max
A∈Rs×s : kAkF =1

Rank(A)≤α

5

|hMJ,J , Ai| ,

uT M u.

(1)

where [1 : q] is a short for {1, . . . , q}. We note that λmax (M, k) = λmax (M, k). We first make the
following basic assumption without which the sparse CCA problem would not be well defined.
H 1. (X T , Y T ) ∼ f with positive definite covariance matrices Σx , Σy , and Σ, and a principal
def
T
T T
canonical vector θ? = (vx?
, vy?
) , such that s? = kθ? k0 ≤ s. Furthermore, the difference between
def

the largest and second largest eigenvalue of S = B −1/2 ΣB −1/2 (denoted by gap), is positive.
Our main assumption on the data generation process is the following.
def

H 2. Z = {(X1T , Y1T )T , . . . , (XnT , YnT )T } is a sequence of n i.i.d. random variables from f , and
there exist absolute constants 0 < κ ≤ κ̄ such that the following holds.
1.


min λmin (Σ̂x , s + s? ), λmin (Σ̂y , s + s? ), λmin (Σ̂, s + s? ) ≥ κ,


max λmax (Σ̂x , s + s? ), λmax (Σ̂y , s + s? ), λmax (Σ̂, s + s? ), ≤ κ̄.
2. For some constant r1 (depending possibly on n, p)


(2)
(2)
(2)
max λmax (Σ̂x − Σx , s + s? ), λmax (Σ̂y − Σy , s + s? ), λmax (Σ̂ − Σ, s + s? ) ≤ r1 .
Remark 3. We first remark that if H2 holds, then for all θ ∈ Rp , with kθk0 ≤ s,
!
Σ̂
0
x
θT B̂θ = θT
θ ≥ κkθk22 .
0
Σ̂y
Meaning that H2 implies that the quasi-posterior distribution Π(·|Z) is well-defined, by Proposition 1.
Theorem 4. Assume H1, and H2, and choose u > 1 such that pu−1 > 2, and pu ≥ s? exp(1). Set
def

 =

r1
.
gap

(11)

There exists some absolute constant C0 that depends only on κ and κ̄, such that the following
holds. For all M > C0 such that
M 2  κ 2 2
nr1 ≥ (s? + 1)(u + 1) log (p ∨ (c0 n)),
8gap κ̄

(12)

b in (9) satisfies
where c0 is some absolute constant that depends only on κ and κ̄, the estimator P
Z
M2 κ 2 2
θδ θδT
− 8gap
( κ̄ ) nr1 .
T
T
b
P − θ? θ? ≤
−
θ
θ
Π
(dδ,
dθ|Z)
≤
M

+
2e
?
?
2
F
∆s ×Rp kθδ k2
F
Proof. See Section A.
2

2

κ
−M
nr2
Since the term 2e 8gap ( κ̄ ) 1 is much smaller than M  (under (12)), the main conclusion of
b as a frequentist estimator, estimates θ? θ?T at the rate
the theorem is that the posterior mean P,
M . In the particular case where Σ̂x , Σ̂y and Σ̂ are covariances matrices of sub-Gaussian random

6

p
vectors, we show in Proposition 5 (see below) that r1 = C0 s log(p)/n, under the sample size
condition n ≥ C0 s log(p). In that case the condition in (12) becomes
M 2  κ 2 2
C0 s log(p) ≥ (s? + 1)(u + 1) log (p ∨ (c0 n)),
8gap κ̄
which is easily satisfied. Hence the convergence rate of the estimator is
r
1
s log(p)
=
,
gap
n

(13)

which achieves the minimax rate of the CCA problem, as derived in ([13]). Note that our result
does not contradict ([14]), which conjectures that n ≥ C0 s2 log(p) is a necessary sample size
condition for estimating θ? at the rate , with a polynomial complexity estimator, because Theorem
b In practice we compute
4 does not make any claim about the computational complexity of P.
b by Markov Chain Monte Carlo, and interestingly we notice that the mixing of our proposed
P
MCMC sampler depends on the sample size in a way that confirms the conjecture of ([14]) (see
Section 4.2.).
2.2.1

On Assumption H2

It is well-known that Assumption H2-(1) holds true in the particular case of covariance matrices
of sub-Gaussian random vectors, provided that the sample size satisfies n ≥ c0 s log(p), for some
absolute constant c0 . See for instance [30] Theorem 1, or [14] Lemma 6.5 for the Gaussian case,
and [33] Theorem 3.2 for more general sub-Gaussian distributions. Under roughly the same sample
size conditions, H2-(2) is also known to hold as we show below.
def

Proposition 5. Suppose that Zi = (XiT , YiT )T are i.i.d. random vectors from a mean-zero subdef
Gaussian distribution f , with sub-Gaussian norm K = sup{k hZ, ui kψ2 , u ∈ Rp , kuk2 = 1},
P
where k · kψ2 refers to the sub-Gaussian norm of a random variable. Let Σ̂x = n−1 ni=1 Xi XiT ,
P
P
Σ̂y = n−1 ni=1 Yi YiT , and Σ̂ = n−1 ni=1 Zi ZiT . There exist an absolute constants c0 , C > 1, such
that for n ≥ 4c0 s log(p),
r


c0 αs log(p)
(α)
(α)
(α)
2
max λmax (Σ̂x − Σx , s), λmax (Σ̂y − Σy , s), λmax (Σ̂ − Σ, s) ≤ CK λmax (Σ, s)
,
n
with probability 1 − 2p−(c0 −1)s .
Proof. We present the details of this claim for Σ̂, the argument being similar for the other two
covariance matrices. For any J ⊂ [1 : p] of size s, we have
n


1X
1/2
−1/2
−1/2
1/2
1/2
T
UiJ UiJ
− Is kop ,
kΣ̂J,J − ΣJ,J kop = kΣJ,J ΣJ,J Σ̂J,J ΣJ,J − Is ΣJ,J kop ≤ kΣJ,J k2op × k
n
i=1

def

−1/2

where UiJ = ΣJ,J ZiJ , where ZiJ = (Zij )j∈J , is mean zero and isotropic. By Theorem 4.6.1
(Equation 4.22) of ([38]), provided that n ≥ 4c0 s log(p) for some absolute constant c0 > 1, we
have
r
c0 s log(p)
1/2 2
2
,
kΣ̂J,J − ΣJ,J kop ≤ CK kΣJ,J kop
n
7

with probability at least 1 − 2p−c0 s . Therefore, for any matrix A ∈ Rs×s , with kAkF = 1, and
Rank(A) ≤ α, using the singular value decomposition of A, we have
r
E
D
√
c0 αs log(p)
2
max
Σ̂J,J − ΣJ,J , A ≤ αkΣ̂J,J − ΣJ,J kop ≤ CK λmax (Σ, s)
.
s×s
n
A∈R
: kAkF =1
Rank(A)≤α

Since the number of subsets of [1 : p] of size s is smaller than ps , we conclude with a union bound
argument that
r
c0 αs log(p)
(α)
2
λmax (Σ̂ − Σ, s) ≤ CK λmax (Σ, s)
,
n
with probability 1 − 2p−(c0 −1)s .
2.2.2

Bayesian inference

In this paper we have used the posterior mean of a quasi-posterior distribution as a frequentist
estimator. The idea of using a Bayesian framework to produce a frequentist estimator is of course a
very old one. In fact it is well known in classical statistical that all “good” estimators are Bayesian.
The extension of the idea to allow for pseudo-likelihood functions is more recent in the statistics
and econometrics literature ([10]), but the approach has a long history in machine learning under
the name of PAC-Bayesian learning ([23, 9]). A natural question here is whether one can use
the full quasi-posterior distirbution Π(·|Z) to carry inference on θ? , for instance through credible
sets. The difficulty is the lack of calibration of the quasi-likelihood function (we could easily have
used 2n instead n as a scaling factor in the Rayleigh quotient). These issues have been explored
in the low-dimensional setting ([6, 35]), but remain largely open in the high-dimensional setting.
We leave this question as a possible future research. Currently we do not advocate the use of our
quasi-posterior distribution for Bayesian inference on θ? .

3

Markov Chain Monte Carlo

It is straightforward to design an MCMC sampler to sample from (8). However we face the
challenge that the distribution has possibly multiple modes, with a complicated landscape. Indeed,
any generalized eigenvector of (Â, B̂) is a stationary points of the Rayleigh quotient Rn (·; Z). In
addition, if (u, v) maximizes Rn (·; Z), so does (−u, −v). To deal with this challenge, we leverage
simulated tempering (see e.g., [16]), a MCMC sampling technique that has shown promising
performance in sampling from multi-modal distributions.
Given K temperatures 1 = t1 < t2 < . . . < tK , and K positive weights c1 , . . . , cK , we introduce
def
an extended distribution on X = ∆ × Rp × {1, . . . , K}, which is






a
ρ1
ρ0
n
1
2
2
exp
kδk0 exp −
kθδ k2 −
kθ − θδ k2 exp
Rn (θδ ; Z) dθ. (14)
Π̄(δ, dθ, k|Z) ∝
ck
tk
2tk
2tk
tk
We recover the distribution (8) as the conditional distribution of (δ, θ) given k = 1 in (14). To
sample from (14), we use a Metropolis-within-Gibbs strategy summarized in Algorithm 1.
First, we update δ by applying a Gibbs sampler to the conditional distribution of δ given k and
def
θ. Note that the conditional distribution of δj given k, θ and δ−j , where δ−j = (δ1 , . . . , δj−1 , δj+1 , . . . , δp ),
8

Algorithm 1 Simulated tempering for sparse canonical correlation analysis
Model Input: Matrices Â, B̂, prior parameters ρ0 , ρ1 , q.
MCMC Input: Number of iterations N , batch size J, temperatures 1 = t1 < . . . , < tK ,
weights (c1 , . . . , cK ), and step-sizes (η1 , . . . , ηK ).
(0) i.i.d.
Initialization: Set k (0) = 1. Draw δj ∼ Ber(0.5), ∀j = 1, . . . , p, and independently θ(0) ∼
N(0, Ip ).
for t = 0 to N − 1, given (k (t) , δ (t) , θ(t) ) = (k, δ, θ) do
1. Update δ: Uniformly randomly select a subset J from {1, . . . , p} of size J without re(J)
placement, and draw δ̄ ∼ Qk,θ (δ, ·), where the transition kernel described in (17).
2. Update θ: Draw the components of [θ̄]δ̄c independently from N(0, ρ−1
0 tk ). Draw [θ̄]δ̄ ∼
Pηk ,k,δ̄ ([θ]δ̄ , ·), where Pη,k,δ denotes the transition kernel of the MALA with step-size η and
invariant distribution given by Wk,δ , whose density is proportional to (18).
3. Update k: Draw k̄ ∼ Tδ̄,θ̄ (k, ·), where Tδ,θ is the transition kernel of the MetropolisHastings on {1, . . . , K} with invariant distribution given by (19) and random walk proposal
that has reflection at the boundaries.
4. New MCMC state: Set (δ (t+1) , θ(t+1) , k (t+1) ) = (δ̄, θ̄, k̄).
end for
Output: {(δ (t) , θ(t) , k (t) ) : 0 ≤ t ≤ N s.t. k (t) = 1}

is the Bernoulli distribution Ber(qj ), with probability of success given by
def

qj =






−1
a
1
n
n
1 + exp − +
(ρ1 − ρ0 )θj2 exp
Rn (θδ(j,0) ; Z) − Rn (θδ(j,1) ; Z)
,
tk
2tk
tk
tk

where
(j,0)

δi


0
def
=
δ

i

i=j

(j,1)

,

δi

i 6= j


1
def
=
δ

i

i=j

.

(15)

(16)

i 6= j

(j)

Given k, θ and j, let Qk,θ denote the transition kernel on ∆ which, given δ, leaves δi unchanged
for all i 6= j, and draws δj ∼ Ber(qj ). We update δ as follows: randomly draw a subset J =
{J1 , . . . , JJ } of size J from {1, . . . , p}, and update δ using the transition kernel on ∆ given by
(J) def

(J )

(J )

(J )

Qk,θ = Qk,θ1 Qk,θ2 · · · Qk,θJ .

(17)

Second, given k and δ, we update θ. We let [θ]δ to denote the δ-selected component of θ listed
def
def
in their original order: [θ]δ = (θj : j ∈ {1 ≤ k ≤ p : δk = 1}), and [θ]δc = (θj : j ∈ {1 ≤ k ≤ p :
δk = 0}). We utilize the fact that the selected components [θ]δ and the unselected components
[θ]δc of θ are independent conditional on k and δ to update θ. In addition, given k and δ, the
components of [θ]δc are i.i.d. N(0, tk ρ−1
0 ) and the distribution of [θ]δ , denoted by Wk,δ , has density
kδk
0
on R
proportional to


n
ρ1
2
kuk2 + Rn ((u, 0)δ ; Z) ,
(18)
u 7→ exp −
2tk
tk
9

where the notation (u, 0)δ denotes the vector in Rp such that [(u, 0)δ ]δ = u and [(u, 0)δ ]δc = 0.
Hence we update [θ]δ using a standard Metropolis adjusted Langevin algorithm (MALA) with
target distribution Wk,δ , and step-size ηk . For details on MALA, see e.g., [31]. For convenience,
let us denote the transition kernel of this Markov chain on Rkδk0 as Pηk ,k,δ .
Third, given δ and θ, we update k using a standard Metropolis-Hastings algorithm with a
random walk proposal that has reflection at the boundaries. Specifically, at k we propose with
equal probability either k − 1 or k + 1, except at 1, where we only propose 2, and at K, where we
only propose K −1. Let Tδ,θ denote the transition kernel on {1, . . . , K} of this Metropolis-Hastings
algorithm with invariant distribution


a
ρ1
ρ0
n
1
2
2
kδk0 −
kθδ k2 −
kθ − θδ k2 + Rn (θδ ; Z) .
(19)
i 7→ exp
ci
ti
2ti
2ti
ti
Lastly, we collect samples by retaining the values of (δ, θ) at iterations at which k = 1. In
stationarity these samples have distribution (8).

3.1

Parameter choices and adaptive tuning

Algorithm 1 as outlined above depends on the user-defined parameters J, K, (t1 , . . . , tK ),
(c1 , . . . , cK ), and (η1 , . . . , ηK ). The parameter J (the Gibbs sampling batch size) does not greatly
impact performance, and setting J = 100 works well in most settings. Efficient selection and
tuning of temperatures in simulated tempering has received some attention ([16, 3]), and despite
some progress ([24]), to the best of our knowledge, there is no practical and scalable algorithm to
do so. In our implementation we use variations of the geometric scaling. We refer the reader to
Section 4 for specific choices.
We tune the step-sizes η = (η1 , . . . , ηK ) of MALA and the weights (c1 , . . . , cK ) of simulated
tempering using adaptive MCMC methods , see e.g., [1]. To tune ηk , we follow the algorithm
proposed in [4], with a targeted acceptance probability of 30%. For simulated tempering to visit
all temperature levels frequently , the weights (c1 , . . . , cK ) need to be adequately tuned. We refer
the reader to [16] for an extensive discussion of the issue. This problem can be efficiently solved
using the Wang-Landau algorithm for simulated tempering as developed in [5]. We follow this
approach here. The fully adaptive MCMC sampler is presented in Algorithm 2 in the appendix.

3.2

MCMC output processing

Given the output {(δ (t) , θ(t) , k (t) ) : 0 ≤ t ≤ N s.t. k (t) = 1} from Algorithms 1 or 2,
we use three quarters of the total iterations as burn-in. Then we treat the remaining output at
def
iteration t ∈ T = {t : 43 N ≤ t ≤ N s.t. k (t) = 1} as samples from the distribution of interest
in (8). At iteration t ∈ T, we estimated the principal canonical correlation pair by v (t) where
def
(t) def
(t)
v (t) = [θ(t) ]δ(t) is the entry-wise product of δ (t) and θ(t) . We denote vx = (vi : 1 ≤ i ≤ px ) and
(t) def
(t)
(t)
(t)
vy = (vi : px + 1 ≤ i ≤ p). Then we normalize vx and vy to have unit Euclidean norm as the
estimators of vx? and vy? at iteration t.

10

4

Numerical studies

We extensively test the proposed approach. First we test the sample size condition for our
algorithm. Since the success of the method is predicated on a good behavior of the MCMC sampler,
we investigate empirically the mixing time of Algorithm 1 as the dimension p increases. Then we
perform a simulation study that compares our approach to the frequentist method Rifle in [37].
The results clearly show the advantage of using the Raleigh quotient in a Bayesian setting. We also
investigate the behavior of the proposed method in settings where one of the datasets is subject to
truncation, and we compare the results to the method specifically proposed in [43] to handle such
mixed datasets. Again our method outperforms the competition. Our method’s implementation
is available from the author’s github page https://github.com/rachelwho/Sparse-CCA.

4.1

Simulated data generation

We simulate the datasets using a model from [37]. For simplicity, we let px = py = p/2. Then
we consider two (p/2)-dimensional random vectors X and Y with joint distribution (X, Y ) ∼
N(0, Σ). Here we assume that
!
T
λ1 Σx vx? vy?
Σy
Σx Σxy
p
,
Σ=
,
Σ
=
p
xy
T
T
T
vx? Σx vx? vy? Σy vy?
Σxy Σy
where 0 < λ1 < 1 is the largest generalized eigenvalue, and vx? and vy? are the principal canonical
correlation vectors. Clearly, vx? and vy? are maximizer of the Rayleigh quotient and λ1 is the
maximum value.
We consider the case when Σx and Σy are block diagonal matrix with five blocks, each of
0
dimension p/10 × p/10, where the (j, j 0 )-th element of each block takes value 0.8|j−j | . We let
√
λ1 = 0.9, (vx? )j = (vy? )j = 1/ 3 for j ∈ {1, 6, 11}, and (vx? )j = (vy? )j = 0 otherwise. Then we
generate n samples (xi , yi ), i = 1, . . . , n from N(0, Σ).

4.2

Sample size condition

As mentioned above, it was conjectured by ([14]) that it is not possible to construct an estimator of θ? that is computable in polynomial time and estimates θ? at the rate  obtained in (13)
in the regime n = o(s2 log(p)). The authors made a compelling argument for this conjecture by
showing that any such estimator for the sparse CCA problem can be used to solve the planted
clique problem in a regime where it is widely believed to be computationally intractable. Since our
estimator achieves the rate  under the weaker condition n ≥ C0 s log(p), we have the opportunity
to test empirically [14]’s conjecture. This boils down to the mixing time of the proposed simulated
tempering algorithm.
We generate data from the data model described in Section 4.1 for each p ∈ {500, 2000, 5000},
2.5
with sample size n = ds1.5
? log(p)e and n = ds? log(p)e, where we recall that s? = 6. We use the
sample covariance matrices as estimators of Σx , Σy , and Σxy to construct the extended posterior
distribution Π̄ in (14) that we sample using Algorithm 2, and temperatures {1, 1/0.9, 1/0.8, 1/0.7}.
Since in this particular data model the largest value of the (population) Rayleigh quotient is
λ1 = 0.9, we compute the value of the sample Rayleigh quotient Rn (·; Z) along the MCMC
iterations and use its proximity to λ1 as empirical indication of mixing.
11

We run each MCMC sampler for N = 10, 000 iterations, and we repeat each MCMC simulation
30 times (each time with a new dataset). At each iteration time of the MCMC sampler, we
average the values of Rn (·; Z) across the 30 replications. Fig. 1 shows the plot of the averaged
sample Rayleigh quotient along with iterations. The results seem to suggest that the condition
n ≥ C0 s2 log(p) is indeed needed for the simulated tempering sampler to mix well, which appears
to confirm the conjecture by ([14]).

p = 500

p = 2000

1

1

0.95

0.95

0.9

0.9

0.9

0.85

0.85

0.85

0.95

0.8

0.8

0.8

0.75

0.75

0.75

0.7

0.7

0.7

0.65

0.65

0.65

0.6

n = s1.5 log(p)
2.5

n = s log(p)
target 1 = 0.9

0.55
0.5
0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

p = 5000

1

0.6

0.6

0.55

0.55

0.5

0.5
0

1000

2000

3000

4000

5000

6000

7000

8000

9000

10000

0

1000

2000

3000

4000

5000

6000

7000

8000

9000

Figure 1: Average values of sample Rayleigh quotient along the MCMC iterations, averaged over
30 data replications.

4.3

Empirical mixing time of Algorithm 1

The simulation results above suggest that the simulated tempering sampler has good mixing
properties when the sample size n is large enough. Here we investigate more carefully the mixing
time of Algorithm 1 as function of the dimension p, using the coupled chain approach of ([7, 19]).
We focus on the case where n = p/2.
We start with a brief description of the method. Let {X (t) , t ≥ 0} be the Markov chain
generated by Algorithm 1, where X (t) = (δ (t) , θ(t) , k (t) ) ∈ X. Let P denote the transition kernel
of the Markov chain {X (t) , t ≥ 0}. The basic idea of the method is to construct a coupling
P̌ of P with itself: that is, a transition kernel on X × X such that P̌ ((x, y), A × X) = P (x, A),
P̌ ((x, y), X × B) = P (y, B), for all x, y ∈ X, and all measurable sets A, B. The coupling P̌ is
def
constructed in such a way that P̌ ((x, y), D) > 0, where D = {(x, x) : x ∈ X} and P̌ ((x, x), D) =
1. The method then proceeds as follows. Fix a lag L ≥ 1. Draw X (0) ∼ Π̄(0) , Y (0) ∼ Π̄(0)
(where Π̄(0) is the initial distribution as given in the initialization step in Algorithm 1). Draw
X (k) |(X (0) , Y (0) ) ∼ P k (X (0) , ·), for all 1 ≤ k ≤ L. Then for any k ≥ 1, draw,
(XL+k , Yk )| {(XL+k−1 , Yk−1 ), . . . , (XL , Y0 )} ∼ P̌ ((XL+k−1 , Yk−1 ), ·) ,
12

k ≥ 1.

10000

Setting
def

τ (L) = inf {k > L : Xk = Yk−L } ,
it then holds under some ergodicity assumptions on P (see [7]) that
'!#
"
&
τ (L) − L − t
(t)
kΠ̄ − Π̄ktv ≤ E max 0,
,
L

(20)

where dxe denote the smallest integer above x. The implication of (20) is that we can empirically
upper bound the left hand side of (20) by simulating multiple copies of the joint chain as described
above and then approximating the expectation on the right hand side of (20) by Monte Carlo.
We refer the reader to Appendix B for a description of the specific coupled chain that we employ.
Although the empirical mixing time estimation method described above only applies to Markov
chains, we have applied it here to the adaptively tuned Algorithm 2. We conjecture that the
methodology remains approximately valid for well-constructed adaptive MCMC samplers.
Now we describe the implementation details. We generate datasets from the data model
described in Section 4.1 for each p ∈ {100, 200, . . . , 5000} with sample size n = p/2. We use
sample covariance matrices as estimators of Σx , Σy , and Σxy to construct the extended posterior
distribution Π̄ in (14) with temperatures {1, 1/0.9, 1/0.8, 1/0.7, 1/0.6}. We set the lag L = p and
a maximum iterations of 10 × p + 1000. For each value of p, we repeat the simulations 50 times to
estimate the distribution of the meeting time τ (L) of the chain. More precisely, using ε = 0.1, we
estimate the mixing time of the chain as the first iteration t for which the Monte Carlo estimate
of the right hand side of (20) is less than ε. Fig. 2 below shows the plot of the meeting times
and the estimated mixing times (both divided by p), as functions of p. The results suggest that
Algorithm 2 has a mixing time that scales roughly linearly in the dimension p.

Median meeting times

20

Estimated mixing times

18
16

meeting times / p

14
12
10
8
6
4

5000

4900

4800

4700

4600

4500

4400

4300

4200

4100

4000

3900

3800

3700

3600

3500

3400

3300

3200

3100

3000

2900

2800

2700

2600

2500

2400

2300

2200

2100

2000

1900

1800

1700

1600

1500

1400

1300

1200

1100

900

800

1000

700

600

500

400

300

200

100

2

Dimension (p = p x +p y )

Figure 2: Boxplots of the distributions of meeting times (divided by p) for each value of p. The
median meeting time (solid line) as well as the estimated meeting time (dashed line) are also
shown.

13

4.4

Continuous data example and comparison with [37]

We now compare our method to Rifle(k) in [37]. Rifle(k) is a two-stage algorithm. It first
solves a convex relaxation of (1). Then using the solution as the initial value, Rifle(k) iteratively
perform a gradient ascent step on Rn , and a truncation step that preserves only the top k entries,
setting the remaining entries to 0.
We generate the data using the model in Section 4.1, with sample size n = 200 and dimension
p = 500. As above we use the sample covariance matrices as estimators of Σx , Σy , and Σxy .
Even at this modest scale the computational cost of the first stage of Rifle is high (the running
time is about 20 mins using the R-package rifle, compared to about 2 mins for our simulated
tempering sampler to converge),
For the MCMC sampling we consider two implementations of Algorithm 2: one without simulated tempering (by only using one temperature, i.e. k = 1), and one with simulated tempering by
setting the set of temperatures to be {1, 1/0.9, 1/0.8, 1/0.7, 1/0.6}. We run the MCMC sampler
without tempering for number of N = 2000 iterations, and N = 10000 iterations for simulated
(t)
tempering. In both case, we process the output as in Section 3.2, then obtaining unit vectors vx
(t)
and vy as estimators of vx? , vy? at iteration t ∈ T. In the simulated data experiments where the
true values of vx? , vy? are known, we assess the quality of the inference by computing the posterior
\x ) and mse(v
\y ), where
mean square errors mse(v
\x ) def
mse(v
=

1 X
mse(vx(t) ),
|T|

and



def
mse(vx ) = min kvx − vx? k22 , kvx + vx? k22 , (21)

t∈T

\y ) is computed similarly.
and mse(v
For the comparison we use Rifle(6) (that is the method is provided with the exact number of
nonzero components). Because of the high computational cost of Rifle we explored two possibilities.
In the first experiment, the initial estimate in Rifle(6) is generated from mean zero standard
Gaussian noise, the same as our simulated tempering sampler. In the second experiment, we
initialize Rifle(6) from the ground truth vx? , vy? perturbed with mean zero Gaussian noise with
standard deviation 0.2. In such initialization, the elements of the initial value at indices i where
√
θ?i 6= 0 will be greater than 1/ 3 − 0.2 ∗ 1.5 ≈ 0.3 with probability over 93%, and the elements of
the initial value at indices i where θ?i = 0 will be less than 0.2 ∗ 1.5 = 0.3 with probability over
93%. Hence after truncation, the support of the initial value is the true support of θ? with high
probability, which assures the goodness of the initial estimate in the second experiment. As with
our method we report the mean square errors of Rifle (see Equation (21)). These mean square
errors estimates are then averaged over 100 data simulations. The results are shown in Table 1.
We can see from the results that Rifle(6) typically requires a good initialization and performs
very poorly otherwise. However, even with good initialization the method still significantly underperforms our Bayesian method. The results also illustrate the advantage of using simulated
tempering. In roughly one out of four datasets the plain MCMC sampler – without tempering –
gets trapped in a local mode. This explains its relatively poor performance.

14

Rifle(6) with random Init.

Rifle(6) with good Init.

MCMC

Simulated Temp.

1.38 (0.31)
1.40 (0.31)

0.32 (0.25)
0.31 (0.27)

0.50 (0.80)
0.48 (0.81)

0.06 (0.28)
0.06 (0.28)

mse(vx )
mse(vy )

Table 1: Mean square errors of Rifle(6) and proposed Bayesian method. The algorithm Rifle(6)
is initialized either from random noise (first column), or from the truth plus a small noise (2nd
column). The numbers in parenthesis are standard errors obtained from 100 data replications.

4.5

Application to mixed data type and comparison with [43]

In this section, we investigate the performance of our method when the data generating process
deviates from the multivariate Gaussian generator described in Section 4.1. We consider the
truncated latent Gaussian copula model in [43]. The truncated latent Gaussian copula model is
useful in modeling truncated and non-Gaussian data, which are commonly encountered in biomedical applications. Our proposed framework remains easily applicable to this model.
Definition 6 (Gaussian copula model). A random vector Z = (Z1 , . . . , Zp )T is a realization of
the Gaussian copula model, if there exists a transformation h : Rp → Rp such that h(Z) =
(h1 (Z1 ), . . . , hp (Zp ))T ∼ N(0, Σ) and for each j = 1, . . . , p, transformation hj : R → R is monotonically increasing. We write this as Z ∼ NPN(0, Σ, h).
Definition 7 (Truncated Gaussian copula model). A random vector Z = ((Z (1) )T , (Z (2) )T )T ,
where Z (1) ∈ Rpx and Z (2) ∈ Rpy , is a realization of a latent Gaussian copula model with truncation
(2)
if there exists a random vector U ∈ Rpy such that (Z (1) , U ) ∼ NPN(0, Σ, h) and Zj = I(Uj >
Cj )Uj for all j = 1, . . . , py , where C = (C1 , . . . , Cpy ) is a truncation parameter. We write Z ∼
TNPN(0, Σ, h, C).
In this model the parameter of interest is the principal canonical correlation vectors of h(Z (1) , U ) ∼
N(0, Σ). However the challenge is that we can only observe Z. Building on the work by ([22, 11])
it is shown by [43] that a Kendall’s-tau-based estimator of the covariance matrix Σ can be derived from n realizations
of Z ∼ TNPN(0, Σ, h, C). We refer the reader to [43] for details. Let
!
Σ̂x Σ̂xy
Σ̂ =
be the resulting estimator, where Σ̂x and Σ̂xy are estimators for covariance
Σ̂yx Σ̂y
matrix of X and Y and Σ̂y is the estimator between X and Y . Based on Σ̂x , Σ̂xy and Σ̂y , our
methodology readily applies, leading to the quasi-posterior distribution (8) and a similar estimab We compare our approach to the estimator proposed by [43] that estimates the principal
tor P.
canonical correlation pair by solving
max vxT Σ̂xy vy − λ1 kvx k1 − λ2 kvy k1 ,
vx ,vy

s.t. vxT Σ̂x vx ≤ 1, vyT Σ̂y vy ≤ 1.

(22)

For the comparison we set sample size n = 200 and dimensions px , py = 100, and generate the
dataset from TNPN(0, Σ, h, C), where Σ is constructed the same way in Section 4.1. As shown
in [43], since the Kendall’s-tau-based estimator Σ̂ is invariant to h, without any loss of generality,
we set h to be identity function. For simplicity, we only test for C = c1py , where c = −2, −1, 0.
The resulting truncation level (percentage of zero elements across the variables) is in the range of
2 − 50%.
15

Truncation

Simulated tempering
\x )
\y )
c truncation level mse(v
mse(v
-2
-1
0

0.02
0.16
0.50

mixedcca(BIC1)
mse(vy )

mixedcca(BIC2)

mse(vx )

mse(vy )

mse(vx )

0.02 (0.00) 0.02 (0.01) 0.04 ( 0.03 ) 0.04 ( 0.05 ) 0.04 ( 0.03 ) 0.04 ( 0.04 )
0.02 (0.01) 0.03 (0.01) 0.04 ( 0.03 ) 0.04 ( 0.04 ) 0.04 ( 0.03 ) 0.05 ( 0.04 )
0.03 (0.01) 0.11 (0.03) 0.07 ( 0.05 ) 0.11 ( 0.13 ) 0.08 ( 0.05 ) 0.12 ( 0.12 )
d
(a) m
se

Truncation

Simulated tempering
\x ) TPR(v
\y )
c truncation level TPR(v
-2
-1
0

0.02
0.16
0.50

mixedcca(BIC1)
TPR(vx )

TPR(vy )

mixedcca(BIC2)
TPR(vx )

TPR(vy )

1.00 (0.00) 1.00 (0.01) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
1.00 (0.00) 1.00 (0.02) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
1.00 (0.01) 0.96 (0.10) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00) 1.00 (0.00)
d
(b) TPR

Truncation

Simulated tempering
\x ) TNR(v
\y )
c truncation level TNR(v
-2
-1
0

0.02
0.16
0.50

mixedcca(BIC1)
TNR(vx )

TNR(vy )

mixedcca(BIC2)
TNR(vx )

TNR(vy )

1.00 (0.00) 1.00 (0.00) 0.98 (0.01) 0.98 (0.01) 0.97 (0.02) 0.96 (0.02)
1.00 (0.00) 1.00 (0.00) 0.98 (0.01) 0.98 (0.01) 0.96 (0.02) 0.96 (0.02)
1.00 (0.00) 1.00 (0.00) 0.98 (0.01) 0.97 (0.02) 0.94 (0.03) 0.93 (0.05)
[
(c) TNR

d and TNR
[ of proposed method and mse, TPR
d
Table 2: Mean (and standard deviation) of m
se, TPR
and TNR of mixedCCA for different truncation values c when h is identity function.
We implement Algorithm 2 with the number of iterations N = 10000 and the set of temperatures {1, 1/0.9, 1/0.8, 1/0.7}. We evaluate the quality of the inference by computing the posterior
\x ) and mse(v
\y ) as in (21). We also assess the variable selection performean square errors mse(v
mance at iteration t ∈ T by computing true-positive rate (TPR) and true-negative rate (TNR)
defined as
def
TPR(vx(t) ) =
(t)

(t)

(t)

#{j : (vx )j 6= 0, (vx? )j 6= 0}
def #{j : (vx )j = 0, (vx? )j = 0}
, TNR(vx(t) ) =
,
#{j : (vx? )j 6= 0}
#{j : (vx? )j = 0}
(t)

(23)

and TPR(vy ), TNR(vy ) is computed similarly. We assess the algorithm performance by averaging
d and TNR.
[
TPR and TNR over t ∈ T, denoted TPR
For the comparison we compute the estimator (22) using the algorithm mixedcca in the R package mixedCCA of [43], using BIC1 and BIC2 criteria respectively for selecting the regularization
parameters λ1 and λ2 . We average these performance measures over 50 data replications, and the
results are presented in Table 2. We can see from Table 2a that overall our approach performs
better than mixedcca in terms of accuracy, in particular with smaller standard deviation.

16

5

Principal canonical correlation of clinical and proteomic data
in covid-19 patients

COVID-19 is an infectious disease that is rapidly sweeping through the world. The disease
is caused by a severe acute respiratory syndrome coronavirus (SARS-CoV-2). There is currently
an intense global effort to better understand the virus and find cures and vaccines. We use our
methodology to re-analysis a data set produced by [36] that aims to identify biomarkers for early
detection of severely ill Covid-19 patients1 . To that end, the study enrolled 86 patients (some
non-Covid-19 patients, and among the Covid-19 patients, some that developed mild symptoms,
and some that became severely ill). The exact protocol for recruiting these patients is unclear.
For each patient they measured three (3) physical characteristics (sex, age, and body mass index),
twelve (12) clinical variables as routinely measured from blood samples (white blood cells count,
lymphocytes count, C-reactive protein, etc...). Furthermore, the serum of each patient is analyzed
by liquid mass spectrometry-based proteomics to quantify their proteome and metabolome. In
[36], the data is used to build a statistical model to predict whether or not a Covid-19 patient
will progress to a severe state of illness. The dataset of [36] is freely available from the journal
website.
We use canonical correlation analysis to re-analyze the data. A common working assumption
is that SARS-CoV-2 induces patterns of molecular changes that can be detected in the sera of
patients. Canonical correlation analysis may help identify these patterns. To do this we focus
on the proteomic data, and we estimate the principal sparse canonical correlation between the
physical and clinical variables on one hand and the proteomic variables on the other. See for
instance [32] for a similar analysis on tuberculosis and malaria.
We pre-process the data by removing all the proteins for which 50% or more values are missing,
leading to a total of py = 513 proteins, and px = 15 clinical and physical variables. The sample
size is n = 86 patients. Liquid mass spectrometry-based proteomics typically produces a large
quantity of missing values ([20, 27]). We make the assumption here that the missing values are
driven mainly by detection limit truncation ([20]), which makes the truncated latent Gaussian
copula model described above appropriate for this example. We apply simulated tempering with
temperatures {1, 1/0.9, 1/0.8, 1/0.7} that we run for 100,000 iterations.
Our estimate of the principal canonical vector of first dataset (vx? ) has only one selected
component (corresponding to C-reactive protein) with estimated inclusion probability of Π(δj =
1|Z) = 0.99. All other physical and clinical variables have inclusion probabilities smaller than 0.1.
We found also that the principal canonical vector of the proteomic data is also driven by a single
protein (P02763, also known as Alpha-1-acid glycoprotein 1 or AGP 1), with estimated inclusion
probability of Π(δj = 1|Z) = 0.89. All other proteins have inclusion probability smaller than 0.1.
Fig. 3 shows the boxplot of the distribution of sample Rayleigh quotient (this corresponds to the
estimated correlation ρ̂ between the two data set), as well as the boxplot and autocorrelation
function of the MCMC output of the coefficients of CRP and AGP 1 in the quasi-posterior
distribution. The fast decay of the autocorrelation functions show a good mixing of the MCMC
sampler.
1

For reasons that are still poorly understood, about 80% of patients infected by SARS-CoV-2 experience mild
to no symptoms, whereas in about 20% of the cases, patients become severely ill.

17

The highly sparse nature of the estimated canonical correlation vectors is striking. Several
studies have observed the predictive power of C-reative protein (CRP) in the progression of Covid19 into a severe illness (see for instance [34] for a meta-analysis). This suggests that the correlation
detected in our analysis between the two datasets is indeed driven by the progression of Covid-19
into a severe illness. Therefore, our analysis suggests that protein AGP 1 may also be playing an
important role in the progression of Covid-19 into a severe illness. We learn from Uniprot2 , that
AGP 1 functions as transport protein in the blood stream, and appears to function in modulating
the activity of the immune system during the acute-phase reaction. Furthermore, AGP 1 appears
on the list of differentially expressed proteins in the sera of severely ill Covid-19 patients designed
by [36], and also appeared in the literature as playing a role in the immune system’s response to
malaria ([12]).

Autocorrelation Function CRP
Sample Autocorrelation

0.85
0.84
0.83
0.82
0.81

Boxplot CRP

1

Autocorrelation Function AGP

10

0.8
0.6

9

0.4
8
0.2
7

0
-0.2

0

1

50

100

9

0.6

8

0.4

7

0.2

6

0

5
-0.2

6

10

0.8

0

50

1

Lag

Boxplot AGP

1

Sample Autocorrelation

Boxplot of \hat\rho
0.86

100
1

Lag

Figure 3: Boxplot and autocorrelation plots from MCMC output.

Boxplot of CRP by group of patients

Boxplot of AGP by group of patients

300

2.4
2.2

250
2
1.8

200

1.6
150

1.4
1.2

100
1
0.8

50

0.6
0

0.4
Non-Covid

Non-severe

Severe

Non-Covid

Non-severe

Severe

Figure 4: Distribution of CRP and AGP by group of patients.

6

Conclusion

In this work, we have developed a minimax optimal estimation procedure for sparse canonical
correlation analysis within a quasi-Bayesian framework. Our method can be further extended
to capture more than one canonical correlation vector, either by deflation, or by reformulating
2

https://www.uniprot.org

18

the problem as a higher dimensional canonical correlation analysis estimation problem as in [37].
Also, one can straightforwardly extend our method to solve other generalized eigenvalue problems
that arise in other statistical problems, as for instance in Fisher discriminant analysis. Finally
whether it is possible to use the full quasi-posterior distribution for a Bayesian inference of θ? in
the high-dimensional setting is an important problem that we leave for possible future research.

7

Acknowledgements

The authors are grateful to Roger Zoh for very helpful discussions. This work is partially
supported by the NSF grant DMS 2015485.

References
[1] Christophe Andrieu and Johannes Thoms. A tutorial on adaptive mcmc. Statistics and
Computing, 18(4):343–373, 2008.
[2] Yves Atchadé and Anwesha Bhattacharyya. An approach to large-scale quasi-bayesian inference with spike-and-slab priors, 2019.
[3] Yves Atchadé, Gareth Roberts, and Jeffrey Rosenthal. Towards optimal scaling of metropoliscoupled markov chain monte carlo. Statistics and Computing, 21:555–568, 10 2011.
[4] Yves Atchadé and Jeffrey Rosenthal. On adaptive markov chain monte carlo algorithms.
Bernoulli, 11, 10 2005.
[5] Yves F. Atchadé and Jun S. Liu. The wang-landau algorithm in general state spaces: Applications and convergence analysis. Statistica Sinica, 20(1):209–233, 2010.
[6] P. G. Bissiri, C. C. Holmes, and S. G. Walker. A general framework for updating belief
distributions. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
78(5):1103–1130, 2016.
[7] Niloy Biswas, Pierre E. Jacob, and Paul Vanetti. Estimating convergence of markov chains
with l-lag couplings, 2019.
[8] B. P. Carlin and S. Chib. Bayesian model choice via markov chain monte carlo methods. J.
Roy. Stat. Soc. B, 57(3):473–484, 1995.
[9] Olivier Catoni. Statistical learning theory and stochastic optimization, volume 1851 of Lecture
Notes in Mathematics. Springer-Verlag, Berlin, 2004. Lecture notes from the 31st Summer
School on Probability Theory held in Saint-Flour, July 8–25, 2001.
19

[10] Victor Chernozhukov and Han Hong. An MCMC approach to classical estimation. J. Econometrics, 115(2):293–346, 2003.
[11] Jianqing Fan, Han Liu, Yang Ning, and Hui Zou. High dimensional semiparametric latent
graphical model for mixed data. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 79(2):405–421, 2017.
[12] M J Friedman. Control of malaria virulence by alpha 1-acid glycoprotein (orosomucoid),
an acute-phase (inflammatory) reactant. Proceedings of the National Academy of Sciences,
80(17):5421–5424, 1983.
[13] Chao Gao, Zongming Ma, Zhao Ren, and Harrison H. Zhou. Minimax estimation in sparse
canonical correlation analysis. Ann. Statist., 43(5):2168–2197, 10 2015.
[14] Chao Gao, Zongming Ma, and Harrison H. Zhou. Sparse cca: Adaptive estimation and
computational barriers. Ann. Statist., 45(5):2074–2101, 10 2017.
[15] Edward I. George and Robert E. McCulloch. Approaches for bayesian variable selection.
Statistica Sinica, 7(2):339–373, 1997.
[16] Charles J. Geyer and Elizabeth A. Thompson. Annealing markov chain monte carlo
with applications to ancestral inference. Journal of the American Statistical Association,
90(431):909–920, 1995.
[17] David Hardoon and John Shawe-Taylor. Sparse canonical correlation analysis. Machine
Learning, 83:331–353, 06 2011.
[18] H Hotelling. Relations between two sets of variates. Biometrika, 1936.
[19] Pierre Jacob, John O’Leary, and Yves Atchadé. Unbiased markov chain monte carlo with
couplings. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 08
2017.
[20] Yuliya V. Karpievitch, Ashoka D. Polpitiya, Gordon A. Anderson, Richard D. Smith, and
Alan R. Dabney. Liquid chromatography mass spectrometry-based proteomics: Biological
and technological aspects. Ann. Appl. Stat., 4(4):1797–1823, 12 2010.
[21] Shengqiao Li. Concise formulas for the area and volume of a hyperspherical cap. Asian
Journal of Mathematics and Statistics, 4(1):66–70, 2011.
[22] Han Liu, John Lafferty, and Larry Wasserman. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. Journal of Machine Learning Research, 10,
04 2009.
[23] David A. McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355–363, 1999.
[24] Btażej Miasojedow, Eric Moulines, and Matti Vihola. An adaptive parallel tempering algorithm. Journal of Computational and Graphical Statistics, 22(3):649–664, 2013.

20

[25] T. J. Mitchell and J. J. Beauchamp. Bayesian variable selection in linear regression. JASA,
83(404):1023–1032, 1988.
[26] Qianxing Mo, Ronglai Shen, Cui Guo, Marina Vannucci, Keith S Chan, and Susan G Hilsenbeck. A fully bayesian latent variable model for integrative clustering analysis of multi-type
omics data. Biostatistics, 19(1):71–86, 2017.
[27] Jonathon J. O’Brien, Harsha P. Gunawardena, Joao A. Paulo, Xian Chen, Joseph G. Ibrahim,
Steven P. Gygi, and Bahjat F. Qaqish. The effects of nonignorable missing data on label-free
mass spectrometry proteomics experiments. Ann. Appl. Stat., 12(4):2075–2095, 12 2018.
[28] Elena Parkhomenko, David Tritchler, and Joseph Beyene. Sparse canonical correlation analysis with application to genomic data integration. Statistical applications in genetics and
molecular biology, 8:Article 1, 2009.
[29] Nimrod Rappoport and Ron Shamir. Multi-omic and multi-view clustering algorithms: review
and cancer benchmark. Nucleic acids research, 46(20):10546–10562, 2018.
[30] Garvesh Raskutti, Martin J. Wainwright, and Bin Yu. Restricted eigenvalue properties for
correlated gaussian designs. J. Mach. Learn. Res., 11:2241–2259, 2010.
[31] Gareth O. Roberts and Richard L. Tweedie. Exponential convergence of langevin distributions
and their discrete approximations. Bernoulli, 2(4):341–363, 1996.
[32] Juho Rousu, Daniel D. Agranoff, Olugbemiro Sodeinde, John Shawe-Taylor, and Delmiro
Fernandez-Reyes. Biomarker discovery by sparse canonical correlation analysis of complex
clinical phenotypes of tuberculosis and malaria. PLOS Computational Biology, 9(4):1–10, 04
2013.
[33] Mark Rudelson and Shuheng Zhou. Reconstruction from anisotropic random measurements.
IEEE Trans. Inf. Theor., 59(6):3434–3447, June 2013.
[34] Bikash R. Sahu, Raj Kishor Kampa, Archana Padhi, and Aditya K. Panda. C-reactive
protein: A promising biomarker for poor prognosis in covid-19 infection. Clinica Chimica
Acta, 509:91 – 94, 2020.
[35] Benjamin A. Shaby. The open-faced sandwich adjustment for mcmc using estimating functions. Journal of Computational and Graphical Statistics, 23(3):853–876, 10 2014.
[36] Bo Shen, Xiao Yi, Yaoting Sun, Xiaojie Bi, Juping Du, Chao Zhang, Sheng Quan, Fangfei
Zhang, Rui Sun, Liujia Qian, Weigang Ge, Wei Liu, Shuang Liang, Hao Chen, Ying Zhang,
Jun Li, Jiaqin Xu, Zebao He, Baofu Chen, Jing Wang, Haixi Yan, Yufen Zheng, Donglian
Wang, Jiansheng Zhu, Ziqing Kong, Zhouyang Kang, Xiao Liang, Xuan Ding, Guan Ruan,
Nan Xiang, Xue Cai, Huanhuan Gao, Lu Li, Sainan Li, Qi Xiao, Tian Lu, Yi Zhu, Huafen Liu,
Haixiao Chen, and Tiannan Guo. Proteomic and metabolomic characterization of covid-19
patient sera. Cell, 182(1):59 – 72.e15, 2020.

21

[37] Kean Ming Tan, Zhaoran Wang, Han Liu, and Tong Zhang. Sparse generalized eigenvalue
problem: Optimal statistical rates via truncated rayleigh flow. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 80(5):1057–1086, 2018.
[38] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data
Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018.
[39] Vincent Q. Vu and Jing Lei. Minimax sparse principal subspace estimation in high dimensions.
Ann. Statist., 41(6):2905–2947, 12 2013.
[40] Sandra Waaijenborg and Aeilko Zwinderman. Sparse canonical correlation analysis for identifying, connecting and completing gene-expression networks. BMC bioinformatics, 10:315,
09 2009.
[41] Ami Wiesel, Mark Kliger, and Alfred Hero. A greedy approach to sparse canonical correlation
analysis. 02 2008.
[42] Daniela M Witten and Robert J Tibshirani. Extensions of sparse canonical correlation analysis with applications to genomic data. Statistical applications in genetics and molecular
biology, 8(1):1–27, 2009.
[43] Grace Yoon, Raymond J. Carroll, and Irina Gaynanova. Sparse semiparametric canonical
correlation analysis for data of mixed types. arXiv: Methodology, 2018.

A

Proof of Theorem 4

Throughout the proof c0 denotes a generic absolute constant that depends only on κ and κ̄ in
H2, but whose actual value or expression may change during the text.
For δ ∈ ∆s , let


θδ θδT
def
p
T
Bδ = θ ∈ R : M  <
− θ? θ?
,
kθδ k22
F
and
def

B = ∪δ∈∆s {δ} × Bδ .
By splitting the integral over B, Bc and the remaining space we obtain
Z
θδ θδT
− θ? θ? Π (dδ, dθ|Z) ≤ M  + Π(B|Z).
T
∆s ×Rp θδ θδ
F

(24)

We then proceed to bound Π(B|Z). From the definition of Π(·|Z) and by integrating out the
non-selected component θ − θδ , we have

1Bδ (θ) exp − ρ21 kθδ k22 − ρ20 kθ − θδ k22 + nRn (θδ ; Z) dθ

R
P
Π(B|Z) =
ρ1
ρ0
2
2
akδk0
δ∈∆s e
Rp exp − 2 kθδ k2 − 2 kθ − θδ k2 + nRn (θδ ; Z) dθ
 q kδk0 R

P
ρ1
ρ1
1
2
δ∈∆s pu
2π
Rkδk0 1Bδ ((u, 0)δ ) exp − 2 kuk2 + nR̄n ((u, 0)δ ; Z) du
=
, (25)
 q kδk0 R

P
ρ1
ρ1
1
2 + nR̄ ((u, 0) ; Z) du
exp
−
kuk
n
δ
2
δ∈∆s pu
2π
2
Rkδk0
P

δ∈∆s

eakδk0

R

Rp

22

where
def

R̄n (θ; Z) = Rn (θ; Z) − Rn (θ? ; Z).
We show in Lemma 8 that the denominator on the right hand side of (25) is bound from below
by e−(s? +1)(u+1) log(p∨(c0 n)) , for some absolute constant c0 . This implies that
Π(B|Z) ≤ e(s? +1)(u+1) log(p∨(c0 n))
r kδk0 Z
 ρ

X1
ρ1
1
2
1
((u,
0)
)
exp
−
kuk
+
n
R̄
((u,
0)
;
Z)
du. (26)
×
n
B
δ
δ
2
δ
pu 2π
2
Rkδk0
δ∈∆s

We show in Lemma 11 that any θ ∈ Rp , such that kθk0 ≤ s,
gap  κ 2
kθθT − θ? θ?T k2F + c0 r1 kθθT − θ? θ?T kF ,
2 κ̄
 2
κ̄
0 r1
for some absolute constant c0 that depends only on κ and κ̄. Therefore, for 4cgap
≤ kθθT −
κ
θ? θ?T kF , we have
gap  κ 2
kθθT − θ? θ?T k2F .
Rn (θ; Z) − Rn (θ? ; Z) ≤ −
4 κ̄
 2
Therefore, for M ≥ 4c0 κ̄κ , we have
Rn (θ; Z) − Rn (θ? ; Z) ≤ −

Π(B|Z) ≤ e(s? +1)(u+1) log(p∨(c0 n))
 r kδk0 Z
 ρ

2
2
2 X
1
ρ1
1
n
− M 4gap ( κ
2
)
κ̄
kuk
1
((u,
0)
)
exp
−
×e
B
δ
2 du
δ
pu 2π
2
Rkδk0
δ∈∆s
r kδk0
2
X1

κ 2
ρ1
−1 kδk0 /2
(s? +1)(u+1) log(p∨(c0 n)) − M 4gap ( κ̄ ) n2
≤e
e
2πρ
1
pu 2π
δ∈∆s

≤ 2e(s? +1)(u+1) log(p∨(c0 n)) e−

M 2 gap
4

( κκ̄ )

2

n2

2

≤ 2e

2

M
− 8gap
( κ̄κ ) nr21

,

under the sample size condition (12), where the third inequality follows from the assumptions
u > 1, and pu−1 > 2.
The theorem follows by collecting the terms.

We derive here a lower bound on the normalizing constant of the quasi-posterior distribution.
Lemma 8. Under Assumption H2, and assuming that pu ≥ e1 s? , we have
r kδk0 Z
 ρ

X1
ρ1
1
2
exp − kuk2 + nR̄n ((u, 0)δ ; Z) du ≥ e−(s? +1)(u+1) log(p∨(c0 n)) , (27)
kδk
pu 2π
2
R 0

δ∈∆s

for some absolute constant c0 .
Proof. Clearly, the left hand side of (27) is bounded from below by
 r  s? Z
 ρ

1
ρ1
1
2
exp − kuk2 + nR̄n ((u, 0)δ? ; Z) du.
pu 2π
2
Rs?

23

For any θ ∈ Rp that has the same support as θ? , we have
θT Âθ

R̄n (θ; Z) =

θT B̂θ

−

θ?T Âθ?
θ?T B̂θ?

θT Σ̂θ

θT Σ̂θ?
− ?
θT B̂θ θ?T B̂θ?


θT Σ̂θ θ?T B̂θ? − θT B̂θ

=

=

(θ?T B̂θ? )(θT B̂θ)

+

1



θ?T B̂θ?


θT Σ̂θ − θ?T Σ̂θ? .

Since Rn (·; Z) is invariant to rescaling, we can assume without any loss of generality that kθk2 =
kθ? k2 = 1. Therefore for Z satisfying H2-(1),we have from Lemma 10
 2
 !
κ̄
κ̄
2
kθθT − θ? θ?T kF
+2
κ
κ

≤

R̄n (θ; Z)

 2
κ̄
4
kθθT − θ? θ?T kF . (28)
κ

≤

It follows from the above observations that for Z satisfying H2 the left hand side of (27) is bounded
from below by


1
pu

r

ρ1
2π

 s? Z
Rs?




ρ1
C
2
T
T
exp − kuk2 − nkuu − θ? θ? kF du
2
2
 r  s?
Z
ρ
1
ρ1
− 21 kuk22
−Cη 2 n
e
du,
≥
e
pu 2π
S0
def

where C = 8(κ̄/κ)2 , η ∈ (0, 1) and S0 = {u ∈ Rs? : kuuT − θ? θ?T kF ≤ 2η 2 }. Note that the integral
R
ρ
− 21 kuk22
du is invariant to change of variables by orthogonal matrices. Hence in that integral
S0 e
we can replace θ? by the unit vector e = (0, . . . , 0, 1) ∈ Rs? . Using this and switching to polar
coordinates, we write the integral as
Z +∞
Z

ρ1 2
ρ1
2
e− 2 r rs? −1 dr × ν θ ∈ S s? −1 : | sin(θ)| ≤ η ,
e− 2 kuk2 du =
S0

0

where ν is the surface measure on the unit sphere S s? −1 = {u ∈ Rs? : kuk2 = 1}, and sin(θ) is

the sine of the angle between θ and e. The measure ν θ ∈ S s? −1 : | sin(θ)| ≤ η is equal to twice
the spherical cap around the pole e defined by η. We use the formula of the spherical cap from
([21]) to write
s? −1

ν θ∈S

s? −1

4π 2

: | sin(θ)| ≤ η =
Γ s?2−1


Z

arcsin(η)

sins? −2 (θ)dθ

0
s? −1

4π 2

=
Γ s?2−1
Whereas,
+∞

Z

−

e

ρ1 2
r
2

r

0

s? −1

1
dr =
2



2
ρ1

e
S0

−

ρ1
kuk22
2

du ≥

2
√

s? π
24



0

 s?

It follows that
Z

Z

2π
ρ1

2

Γ
 s?
2

η

s? −1

xs? −2
4π 2
η s? −1

√
dx ≥
.
Γ s?2−1 s? − 1
1 − x2

s 
?

2

η s? −1 .

.

We conclude that for Z satisfying H2, and any η ∈ (0, 1), the left hand side of (27) is bounded
from below by
 s?
2
1
2
√
e−(s? −1) log(1/η) e−Cη n ≥ e−(u+1)(1+s? ) log(p∨Cn) ,
u
πs? p
√
by taking η = 1/ Cn, and assuming that pu ≥ e1 s? . This concludes the proof.
We make use of the following version of the Davis-Kahan sin Θ theorem taken from [39] Lemma
4.2.
Lemma 9. Let A be a p×p symmetric semipositive definite matrix and suppose that its eigenvalues
satisfies λ1 (A) > λ2 (A) ≥ . . . ≥ λp (A). If a unit vector u is an eigenvector of A associated to the
largest eigenvalue λ1 (A), for all v ∈ Rp , kvk2 = 1 it holds
A, uu0 − vv 0 ≥

1
(λ1 (A) − λ2 (A)) kuu0 − vv 0 k2F .
2

We will need the following technical result.
Lemma 10. For any unit vectors u, v and square matrix B with matching dimensions, we have
|hB, uuT − vv T i| ≤ 2kBkop kuuT − vv T kF ,

(29)

Proof. Indeed, we have
|hB, uuT − vv T i| = |(u − v)T Bu + v T B(u − v)| ≤ 2kBkop ku − vk2 .
Similarly, we have |hB, uuT − vv T i| ≤ 2kBkop ku + vk2 . Hence
|hB, uuT − vv T i| ≤ 2kBkop min (ku − vk2 , ku + vk2 ) .
The result follows by noting that
kuuT − vv T kF ≥ min (ku − vk2 , ku + vk2 ) .
(30)
p
√
To see this, note that kuuT − vv T kF = ku − vk2 ku + vk2 / 2 = ku − vk2 2 − ku − vk22 /2. Hence,
if ku − vk22 ≤ 2, then we have kuuT − vv T kF ≥ ku − vk2 . But if ku − vk22 > 2 then kuuT − vv T kF >
ku + vk2 . Hence the result.
The next result describes the behavior of the Rayleigh quotient function that yields the posterior contraction result.
Lemma 11. Assume H2. For any θ ∈ Rp such that kθk0 ≤ s, we have
gap  κ 2
Rn (θ; Z) − Rn (θ? ; Z) ≤ −
kθθT − θ? θ?T k2F + c0 r1 kθθT − θ? θ?T kF .
2 κ̄

(31)

Proof. Fix θ ∈ Rp such that kθk0 ≤ s. Since the Rayleigh quotient is invariant under rescaling we
can assume without any loss of generality that kθk2 = 1. We have
θT Σθ
θ?T Σθ?
−
θT Bθ θ?T Bθ?
θT B̂θ θ?T B̂θ?
*
#+



 "
θ? θ?T
θθT
θθT
θ? θ?T
θ? θ?T
θθT
+ Σ̂ − Σ, T
−
+ Σ̂,
−
−
−
. (32)
θ Bθ θ?T Bθ?
θT B̂θ θT Bθ
θ?T B̂θ? θ?T Bθ?

R̄n (θ; Z) = Rn (θ; Z) − Rn (θ? ; Z) =

θT Σ̂θ

−

θ?T Σ̂θ?

25

=

def

Set S = B −1/2 ΣB −1/2 , w = B 1/2 θ/kB 1/2 θk2 , w? = B 1/2 θ? /kB 1/2 θ? k2 , and note that w? is an
eigenvector of S associated to the largest eigenvalue of S. Hence by the curvature lemma (Lemma
9) we have
gap
θ?T Σθ?
θT Σθ
= hS, wwT − w? w?T i ≤ −
−
kwwT − w? w?T k2F .
θT Bθ θ?T Bθ?
2
Let I ⊆ {1, . . . , p} be the joint support of θ and θ? (hence kIk0 ≤ s + s? ). Then we can express


T
θI θIT
θ?I θ?I
T
T
1/2
kww − w? w? kF = (BI,I )
(BI,I )1/2 .
− T
θIT (BI,I )θI θ?I
(BI,I )θ?I
F
We recall that for any square matrix A and invertible matrix B,
kAkF = kB −1/2 B 1/2 AB 1/2 B −1/2 kF ≤ kB −1/2 k2op kB 1/2 AB 1/2 kF ,
where kM kop denotes the operator norm of M . With these observations in mind, we get
kwwT − w? w?T kF ≥

T
θI θIT
θ?I θ?I
1
−
T
T
k(BI,I )−1/2 k2op θI (BI,I )θI θ?I (BI,I )θ?I

F

≥κ

T
θ?I θ?I
θI θIT
− T
θI (BI,I )θI θ?I (BI,I )θ?I

.

T

F

We note also that for any unit vectors u, v and symmetric invertible matrix B with matching
dimension,
vv T
uuT
− T
T
u Bu v Bv

2

=
F

kuuT − vv T k2F
kuuT − vv T k2F
(uT Bu − v T Bv)2
+
≥
.
(uT Bu)2 (v T Bv)2 (uT Bu)(v T Bv)
(uT Bu)(v T Bv)

(33)

Hence, under H2,
 κ 2

kwwT − w? w?T k2F ≥

κ̄

T 2
kF =
kθI θIT − θ?I θ?I

 κ 2
κ̄

kθθT − θ? θ?T k2F .

In conclusion we have
θT Σθ
θ?T Σθ?
gap  κ 2
−
≤
−
kθθT − θ? θ?T k2F .
θT Bθ θ?T Bθ?
2 κ̄

(34)

The second term from (32) can be written as


θθT
θ? θ T
Σ̂ − Σ, T
− T ?
θ Bθ θ? Bθ?
≤

T
+
θI θIT
θ?I θ?I
−
T
T
θ Bθ
θ? Bθ?
T
θI θIT
θ?I θ?I
− θT Bθ?
θT Bθ
?
F

*


=

(Σ̂)I,I − ΣI,I ,

D

max

M ∈RI×I : kM kF =1, Rank(M )≤2

(Σ̂)I,I − ΣI,I , M

T
θI θIT
θ?I θ?I
−
θT Bθ θ?T Bθ?

E

×

F

T
θI θIT
θ?I θ?I
−
θT Bθ θ?T Bθ?

.
F

And we note from (33) and Lemma 10 that for Z satisfying H2,
T
θI θIT
θ?I θ?I
−
θT Bθ θ?T Bθ?

2

≤
F

1
T
BII , θI θIT − θ?,I θ?,I
κ4

2

+

26

1
2
T
θI θIT − θ?,I θ?,I
2
F
κ
 2 !
1
κ̄
≤ 2 1+2
kθθT − θ? θ?T k2F . (35)
κ
κ

Therefore for Z satisfying H2,


θθT
θ? θ?T
Σ̂ − Σ, T
−
≤ c0 r1 kθθT − θ? θ?T kF .
θ Bθ θ?T Bθ?

(36)

We process the last term in (32) as follows.
*

#+
 "
θθT
θ? θ?T
θ? θ?T
Σ̂,
−
−
−
θT B̂θ θT Bθ
θ?T B̂θ? θ?T Bθ?




θT Σ̂θ?
θ? θ T
θT Σ̂θ
θθT
− ?
B − B̂, T ?
=
B − B̂, T
θ Bθ
θ? Bθ?
θT B̂θ
θ?T B̂θ?
!



θ?T Σ̂θ?
θT Σ̂θ
θ?T Σ̂θ?
θθT
θθT
θ? θ?T
+
. (37)
=
−
B − B̂, T
B − B̂, T
−
θ Bθ
θ Bθ θ?T Bθ?
θT B̂θ θ?T B̂θ?
θ?T B̂θ?


θθT

Hence for Z satisfying H2, the first term in the last display can be bounded, similar to (28), as
θT Σ̂θ
θT B̂θ

−

θ?T Σ̂θ?
θ?T B̂θ?

!

θθT
B − B̂, T
θ Bθ



"  
#
1
κ̄ 2
≤ λmax (B̂ − B, s) 4
kθθT − θ? θ?T kF
κ
κ
4
≤ λmax (B̂ − B, s)
κ

 2
κ̄
kθθT − θ? θ?T kF .
κ

The rightmost of (37) is similar to (36):
θ?T Σ̂θ?
θ?T B̂θ?



θ? θ T
θθT
− T ?
B − B̂, T
θ Bθ θ? Bθ?



≤ c0 r1 kθθT − θ? θ?T kF .

In conclusion the last term in (32) is bounded from above by
#+
* 
 "
θ? θ?T
θθT
θ? θ?T
θθT
−
−
−
≤ c0 r1 kθθT − θ? θ?T kF .
Σ̂,
θT B̂θ θT Bθ
θ?T B̂θ? θ?T Bθ?

(38)

We conclude from (34-38) that for Z satisfying H2
Rn (θ; Z) − Rn (θ? ; Z) ≤ −

gap  κ 2
kθθT − θ? θ?T k2F + c0 r1 kθθT − θ? θ?T kF .
2 κ̄

This ends the proof.

B

Description of the coupled chain for mixing time estimation

Here we introduce the specific coupled chain that we employ. We refer the reader to [7] and [19]
for more details on the construction of such coupled kernels. The striking point of their method
is that even in problems as involved in the one considered in this work, these coupled kernels are
easy to construct and simulate.
We modify Algorithm 1 to construct the coupled kernel P̌ . Let (δ (1,t) , θ(1,t) , k (1,t) ) and let
(δ (2,t) , θ(2,t) , k (2,t) ) denote the states of the two chains at time t. At some iteration t ≥ 1, given
27

(δ (1,L+t) , θ(1,L+t) , k (1,L+t) ) = (δ (1) , θ(1) , k (1) ) and (δ (2,t) , θ(2,t) , k (2,t) ) = (δ (2) , θ(2) , k (2) ), we now describe how to generate the next state of the coupled chain.
In step 1, to update δ (1) and δ (2) , we first make use of the same randomly drawn subset J.
(J)
(i)
(i)
For i = 1, 2, drawing δ̄ (i) ∼ Qk,θ (δ (i) , ·) is equivalent to let δ̄−J = δ−J , and for any j ∈ J, draw
(i)

δ̄j

(i)

∼ Ber(qj ) which we implement in the following way. We first draw a common uniform
(i)

(i)

number uj ∼ Uniform(0, 1), then we obtain δ̄j = 1{qj ≤ uj } for i = 1, 2.
In step 2, to update θ(1) and θ(2) , we partition the indices {1, . . . , p} into four groups: Gab = {j :
(2)
(1)
(2)
(1)
δ̄j = a, δ̄j = b} for a, b = 0, 1. To update the components of θG00 and θG00 , for any j ∈ G00 we
(i)

first draw a common standard normal random variables Zj , and then obatin θ̄j = tk(i) ρ−1
0 Zj for
(1)

(2)

i = 1, 2. To update the components of θG01 and θG01 , for any j ∈ G01 we again first draw a common
(1)

= tk(1) ρ−1
0 Zj , and simultaneously
p
(2)
(2)
(2)
using MALA with proposal θj + ηk(2) ∇ log π(θj ) + 2ηk(2) Zj , where π(θj ) is the

standard normal random variables Zj , and then obtain θ̄j
(2)

draw θ̄j

(2)

marginal posterior distribution of θj . Notice that the joint distribution of [θ(2) ]δ̄(2) is given by
Wk(2) ,δ̄(2) , whose density is proportional to (18). A similar update procedure is used for updating
(1)

(2)

(1)

(2)

the components of θG10 and θG10 . To update the components of θG11 and θG11 , we draw reflection(1)

(2)

coupled MALA proposals in [7], and then for the acceptance step, θG11 and θG11 share the same
uniform random variables.
In step 3, to update k (1) and k (2) , we first draw two common uniform numbers w, u ∼
Uniform(0, 1) to couple two chains in the Metropolis-Hastings algorithm approach. In the random walk proposal step, if w ≤ 0.5, for both two chains we propose the left neighbor except at
the boundaries. In the acceptance step, for both chains, we accept the proposal if the acceptance
probability is greater than u, otherwise we reject the proposal.

C

An adaptive version of simulated tempering for Canonical correlation analysis

28

Algorithm 2 Adaptive version of simulated tempering for Canonical correlation analysis
Model Input: Matrices Â, B̂, prior parameters ρ0 , ρ1 , q.
MCMC Input: Number of iterations N , Batch size J, temperatures 1 = t1 < . . . , < tK .
Adaptive MCMC Input: a = 10 and w ∈ (0, 1).
(0) i.i.d.
∼ Ber(0.5), ∀j = 1, . . . , p, and indepenMCMC Initialization: Set k (0) = 1. Draw δj
(0)
dently θ ∼ N(0, Ip ).
Adaptation Parameters Initialization : . Set τ (0) = 0 ∈ RK , v (0) = (0, . . . , 0) ∈ RK , and
choose c(0) ∈ (0, ∞)K .
for t = 1 to N − 1, given (k (t) , δ (t) , θ(t) ) = (k, δ, θ), τ (t) = τ , c(t) = c, and v (t) = v do
1. Update δ: Uniformly randomly select a subset J from {1, . . . , p} of size J without re(J)
placement, and draw δ̄ ∼ Qk,θ (δ, ·), where the transition kernel described in (17).
2. Update θ and τ : Draw the components of [θ̄]δ̄c independently from N(0, ρ−1
0 tk ). Draw
τ
[θ̄]δ̄ ∼ Pη,k,δ̄ ([θ]δ̄ , ·), where η = e k and Pη,k,δ denotes the transition kernel of the MALA
with step-size η and invariant distribution given by Wk,δ , whose density is proportional to
(18). Denote the acceptance probability of the MALA update. Set
τ̄k = τk + vk−0.6 (α − 0.3),
and for i 6= k, set τ̄i = τi .
3. Update k, c and v: Draw k̄ ∼ Tδ̄,θ̄ (k, ·), where Tδ,θ is the transition kernel of the
Metropolis-Hastings on {1, . . . , K} with invariant distribution given by (19) and random
walk proposal that has reflection at the boundaries. We then set
c̄k̄ = ck̄ ea , v̄k̄ = vk̄ + 1,
and for i 6= k̄, c̄i = ci , and v̄i = vi .
P
K
4. Update a and v: If kv̄/( K
k=1 v̄k ) − 1/Kk∞ ≤ w/K, then set a = a/2, v̄ = 0 ∈ R .
5. New MCMC state: Set (δ (t+1) , θ(t+1) , k (t+1) ) = (δ̄, θ̄, k̄), τ (t+1) = τ̄ , c(t+1) = c̄, and
v (t+1) = v̄.
end for
Output: {(δ (t) , θ(t) , k (t) ) : 0 ≤ t ≤ N s.t. k (t) = 1}

29

