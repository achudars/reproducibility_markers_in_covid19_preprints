arXiv:2012.03827v1 [eess.IV] 7 Dec 2020

T HE ROLE OF R EGULARIZATION IN S HAPING W EIGHT AND
N ODE P RUNING D EPENDENCY AND DYNAMICS

Yael Ben-Guigui
The School of Electrical
and Computer Engineering
Ben-Gurion University

Jacob Goldberger
Engineering Faculty
Bar-Ilan University

Tammy Riklin-Raviv
The School of Electrical
and Computer Engineering
Ben-Gurion University

A BSTRACT
The pressing need to reduce the capacity of deep neural networks has stimulated the development
of network dilution methods and their analysis. While the ability of L1 and L0 regularization
to encourage sparsity is often mentioned, L2 regularization is seldom discussed in this context.
We present a novel framework for weight pruning by sampling from a probability function that
favors the zeroing of smaller weights. In addition, we examine the contribution of L1 and L2
regularization to the dynamics of node pruning while optimizing for weight pruning. We then
demonstrate the effectiveness of the proposed stochastic framework when used together with a weight
decay regularizer on popular classification models in removing 50% of the nodes in an MLP for
MNIST classification, 60% of the filters in VGG-16 for CIFAR10 classification, and on medical
image models in removing 60% of the channels in a U-Net for instance segmentation and 50% of
the channels in CNN model for COVID-19 detection. For these node-pruned networks, we also
present competitive weight pruning results that are only slightly less accurate than the original, dense
networks.

1

Introduction

or node pruning while training. In [12] the pruning phase
was applied to parameters of an already trained and optimized neural network in an attempt to maintain similar
The advent of deeper and wider neural networks has made accuracy. Most current methods follow this strategy since
the need to reduce their storage and computational cost pruning from scratch was shown to be inferior [43, 26].
critical, especially for certain real-time applications. In
Since random pruning may reduce network accuracy [4],
particular, features such as model size, memory footprint,
a pruning decision is usually based on some scoring crithe number of floating point operations (FLOPs) and power
terion, e.g., absolute values or contributions to network
usage which are directly related to the number of network
activation or gradients. In this context it is worth noting
parameters must be considered in resource-constrained
that the optimal pruning criterion is debatable. In [17] it
setups such as mobile phones and wearable devices. In
was suggested that a weight’s magnitude is indicative of
recent years there has been a significant effort to address
its usefulness in reducing the error, while other approaches
the apparent trade-off between inference accuracy and rehave suggested more sophisticated "importance" criteria,
liability which requires sufficient network capacity and
and utilized the Hessian of the loss function [22, 14] or a
parameters’ frugality, see [4] and references therein. HowTaylor expansion to approximate its changes due to prunever, having adequate network capacity and maintaining
ing [30]. However, the latter approaches involve additional
network compactness are not necessarily contradictory
computational cost during training. The other main source
goals. It has been shown that neural networks are usuof variability across different pruning algorithms are the
ally overparametrized and therefore can be pruned without
timing and the way the pruning is carried out. Although sethardly loss in accuracy [11, 38, 29]. Moreover, overpating thresholds below which nodes or edges are removed is
rameterized neural networks can easily overfit and even
useful [12] a stochastic approach that defines the probabilmemorize random patterns in the data [43], and thus lack
ity for pruning is more elegant. However, some stochastic
the ability to generalize at inference time.
pruning algorithms involve thresholding as well, either
One straightforward approach to reducing the number of because the weight zeroing is done temporarily - as in
neural network’ parameters consists of systematic weight

Dropout based approaches [9, 29] or because the probabilistic gating mechanism is not binary [45]. Obviously,
setting a threshold as well as the timing and frequency
of its application requires determining the right balance
between the pruning ratio and accuracy. All these hyperparameters are network-dependent and if the threshold
is applied as part of a post-process, it cannot be learned
during training.

and CIFAR-10 classification (respectively) and the U-Net
for segmentation of cell microscopy data. The results show
that our method is on a par with weight pruning algorithms
and with node pruning algorithms. Moreover, the pruning
of both weights and nodes has the clear advantage of reducing higher ratios of the parameters, thus saving storage
space while more effectively reducing the computational
cost. The code of the proposed pruning method, which can
be easily adapted to neural networks of different architecA network can be pruned at different levels. While weight
tures, will be made available upon the acceptance of this
pruning is highly prevalent [11, 38, 29], node pruning,
manuscript.
i.e., the removal of a node along with all of its in-going
and out-going connections is considered to be more effective, reduces computational cost and is more suitable 2 Stochastic weight pruning and
for hardware and software optimization [3, 41, 26, 31].
regularization
In the same manner, pruning can be applied to redundant
channels [16] and kernels or filters [23, 28, 18, 15] to simplify convolutional neural networks or to directly reduce Given a network architecture and a training dataset we aim
the FLOPS [1]. The level of pruning can be changed in to learn a sparse parameter set θ by optimizing a suitable
any stage of the optimization, depending on the pruning objective function. Our explicit goal is weight pruning but
objective [33]. However, to the best of our knowledge, we show that if a weight regularization function is added
simultaneous pruning of different levels with a single prun- to the cost function, pruning of the network nodes also
ing objective has never been addressed. Moreover, most takes place. Our pruning method is based on zeroing each
weight pruning algorithms achieve poor node pruning re- edge weight in a stochastic manner where the probability
sults, especially when weight pruning is done at the end of of a network parameter to be retained is proportional to its
a training process based on some predefined criterion, e.g., magnitude.
the probability that all weights connected to a particular Let h(·, θ) define a non-linear function parametrized by θ
node will be zeroed is exponentially low.
that represents a neural network. We define the network’s
In a recent line of works, regularization was employed to loss function that includes the regularisation terms L(·)
n
encourage sparsity. In [27] a differentiable approximation with respect to a given set of input-output pairs {xi , yi }i=1
and
a
set
of
weights
θ
=
(w
,
...,
w
)
as
follows:
1
m
to L0 regularization via the hard concrete distribution was
used. Other methods have utilized the L1 norm as an
n
1X
alternative approximation to L0 , e.g., [25, 19]. However,
l(h(xi ; θ), yi ) + λR(θ)
(1)
L(θ) =
n i=1
while L2 regularization is occasionally incorporated as
part of a network’s loss function [12, 11], its effect on the
dynamics of the sparsity-constrained training process has where R(θ) = ||θ||pp is the Lp norm regularization function of the weights, λ is a non-negative hyperparameter
not been examined.
and l(·) corresponds to the model loss function. In [27] L0
The key contribution of the proposed study lies in utiliz- regularization was suggested to encourage weight sparsity.
ing weight decay regularization to facilitate pruning and Here, in contrast, we focus on either L or L regulariza2
understanding its role in manipulating pruning dynamics. tion. While the L norm is often used 1as a differentiable
1
Rather than using a fixed pruning criterion, we define a approximation of L , L norm which encourages weight
0
2
probability function and a gating mechanism such that decay [13, 17] has not
been used explicitly to encourage
weights with lower absolute values are more likely to be network sparsity. In the following, we show that once a
removed. Regularization then plays a triple role. Beyond stochastic weight gating function is applied that favors
its clear advantage in reducing overfitting, it decreases zeroing weights with smaller magnitudes, L and L regu2
1
weight’ values, thus increasing their pruning probability. larizations are advantageous.
Moreover, it turns out that while optimizing for weight
m
P
pruning, node pruning is increased as well. Our experRecall that for L2 , i.e., when R(θ) ≡ ||θ||22 =
wj2 the
iments show that an application of a weight regularizer
j=1
suppresses the tendency of edges to compensate for pruned update step of a weight wj by gradient decent optimizer is
edges that are connected to the same nodes.
the following:
The proposed method, dubbed Weight to Node Pruning
wjt = (1 − λ0 )wjt−1 − (∇wj l(θt−1 ))
(2)
(WtoNP), is applied to several standard network architectures, for both classification and segmentation and is where t counts the update steps,  is the learning rate and
compared to state-of-the-art weight and node pruning al- λ0 = λ. Following [13, 17] we set 0 < λ0 < 1. When
gorithms. Specifically we demonstrate its strength using a the gradient of l(θ) with respect to wj is negligible, wj
multilayer perceptron (MLP) and a VGG-16 for MNIST shrinks in each step by a factor of 1 − λ0 .
2

Similarly, the update step in the presence of L1 regulariza- Algorithm 1 Weight to Node Pruning (WtoNP) Algorithm
tion is as follows:
Training dataset: input-output pairs (x, y) ≡
t−1
t−1
t
0
t−1
{xi , yi }ni=1
wj = wj − λ sign(wj ) − (∇wj l(θ )) (3)
Parameters: Trainable network weights θ =
which means that whether positive or negative, if
(w1 , ..., wm )
∇wj l(θt−1 ) is negligible, wj is “pushed" to zero by λ0
Trained neural network: h(·, θ) with respect to some
in each gradient descent step.
loss function l(y, h(x, θ)) and some optimizer scheme
g(·)
Our goal is to stochastically prune weights based on their
magnitude, where weights with smaller magnitudes have
for each minibatch: do
a higher probability to be pruned. Each weight can be
Compute gradients
with respect to the loss:
Pn
zero with a different probability, according to Bernoulli
1
l(h(x
L(θ)
=
i ; θ), yi ) + λkθkp , p ∈ {1, 2}.
i=1
n
distribution (∼ Bern(·)), depends on the weight magniUpdate network parameters:
tude. Therefore we use a function for mapping the weight
for j = 1 to m do

magnitude to a probability. Let ϕ : [0, ∞) → [0, 1] denote
wj ← wj − g ∇wj L(θ)
a monotonically increasing function and let zj denote a
end for
binary gating variable zj ∼ Bern (ϕ(|wj |)) . We propose
Stochastically prune weights based on their magnia modified weight update during the parameter learning
tude:
process as follows:
for j = 1 to m do

Zj ∼ Bern(ϕ(|wj |))
w̃jt = wjt−1 − g ∇wj L(θt−1 )
j = 1, ..., m (4)

set wj ← wj · zj
zjt ∼ Bern ϕ(|w̃jt |)
(5)
end for
end for
wjt = w̃jt · zjt
(6)
where g() is an optimizer scheme. Several even functions can be considered to represent ϕ, e.g., a Gaussian shaped function 1 − exp(− 12 aw2 ), We can also use edges) to be pruned together. In our scheme, a zeroed edge
1 − 4σ(aw)(1 − σ(aw)) such that σ() is the sigmoid func- can move away from zero but since the updated weight
is still small there is a good chance that it will be zeroed
tion and a is a parameter that controls its slope.
again and so on. In that case we can expect that the weights
The Weight to Node Pruning (WtoNP) is summarized
of the neighboring edges will become larger to compensate
in Algorithm 1 and the main concept is illustrated in
for the removed edge. However, if the network paramefigure 1. It is applied to a trained network performing an
ters are regularized, the rate of growth of edges that are
additional training session that enforces sparse solutions
neighbors of the zeroed edge is suppressed. This in turn
by stochastically selecting a set of weights to be dropped.
causes the entire node to be less informative for the global
The proposed strategy of random zeroing of network comnetwork decision such that gradually the magnitudes of
ponents resembles the Dropout algorithm [37]. The main
its edges will decrease as well until finally they will be
difference is that dropout temporarily eliminates different
zeroed. We demonstrate this dynamics for various network
fractions of the weights in each forward-backward session
architectures in the experiment section.
of a single instance and then restores the original weights’
values. In our approach when a weight w is zeroed its
previous value is discarded. In [9], on the other hand, the
Dropout is applied to reduce model dependency on the
unimportant edges, thereby potentially reducing the performance degradation as a result of post-processing pruning
(via threshold application). In contrast, the proposed joint
edge pruning and model optimization explicitly targets
both the performance and the sparsity of the learned network. Louizos et al. use an adaptation of concrete dropout
and Molchanov et al. [29] use variational dropout, both
with trainable drop rates to the weights of the network. In
these two works, in contrast to our method the number of Figure 1: Illustration of the WtoNP algorithm main
concept. The WtoNP algorithm is applied to a trained
trainable parameters was doubled.
network by sampling weights and pruning them based on
In general, there is no direct connection between weight their magnitude. If a weight regularization function is
pruning and node pruning and there must be a different added to the cost function, pruning of the network nodes
explicit loss function for each of the two pruning goals. In also takes place.
other words, weight pruning algorithms do not encourage
other edges that enter the same node (a.k.a. neighbouring
3

3

Pruning performances, comparisons and
analysis

To validate the effectiveness of our method, we applied it
to two different classification tasks and instance segmentation. We employed several regularization terms: L1 , L2
and elastic-net, as well as no regularization. For all tasks,
we initially reached a baseline model by training a full-size
network until convergence. We then applied our pruning
strategy to the trained network by performing a combined
training and pruning session.
3.1

Implementation details

We tested the proposed pruning algorithm on four different
network architectures performing four different tasks. The
networks were trained using Adam optimizer [20] with the
following (default) hyper-parameters: learning rate=0.001,
β1 =0.9 and β2 =0.999. The only exception was the learning
rate of the VGG-like network which was set to 0.0005. The
proposed pruning technique is implemented as a Keras
callback or fastai callback and therefore can easily be
incorporated in the training of any network regardless of
its architecture. We define the stochastic function ϕ as
follows:
ϕ(w) = 1 − 4σ(aw)(1 − σ(aw)),

this experiment we used the serial section Transmission
Electron Microscopy (ssTEM) dataset of the Drosophila
first instar larva ventral nerve cord (VNC) [2]. An example
of a microscopy image from the tested dataset is presented
in figure 2. To demonstrate the quality of the segmentation results, a manual segmentation (middle) of the cell
image on the left is presented along with the U-Net prediction (right) after the pruning of 95% of its weights. The
training data include 30 images (512 × 512 pixels) randomly partitioned into patches (128×128 pixels), where
each training example is paired with a manual (ground
truth) segmentation. For the test dataset which contains
30 images as well, evaluation was performed by the challenge organizers since the ground truth annotations were
confidential. The U-Net was trained for 3000 epochs each
session using cross entropy loss and data augmentation as
in [34]. For the pruning sessions, we set the coefficient of
all regularizations terms to λ = 1e − 4.
Cell image

Manual seg.

Predicted seg.

(7)

where σ denotes the standard sigmoid function. The
hyperparameter a which controls the slope of the sigmoid,
was tuned using the validation set. In our experiments a
Figure 2: A microscopy image along with its segmentatook values in the range of [1e − 2, 1e − 5].
tions. A cell microscopy image (left) along with its manual
segmentation (middle) and its segmentation prediction by
MLP-300-100 for MNIST classification: The MLP-300- the U-Net following pruning of 95% of its weights.
100 [22] is a fully connected network with two hidden
layers, with 300 and 100 neurons each with 267K param- DarkCovidNet Model for automatic COVID-19 deteceters. The baselines were trained for 200 epochs and the tion using raw chest X-ray images: The DarkCovidNet
networks were diluted for another 200 epochs. We set architecture introduced in [32] and inspired by the DarkNet
λ = 1e − 4 for all regularizations terms in the pruning architecture. This model contains 17 convolution layers
sessions.
and uses for multi-class classification (COVID vs. NoFindings vs. Pneumonia). We used the dataset with which
the model was trained in the original paper. This dataset
contains 125 X-ray images diagnosed with COVID-19,
500 no-findings and 500 pneumonia class frontal chest
X-ray images from two different sources [5, 40]. Figure
3 present example for chest X-ray images with COVID19 case (left) Pneumonia case (middle) and No-Findings.
80% of the X-ray images are used for training and 20% for
test. The model has been trained using the code published
by [32] for 100 epochs. For the pruning sessions, we used
λ = 5e − 4 and λ = 1e − 4 for L2 and L1 regularization,
U-Net for cell segmentation in microscopy images: The respectively.
U-Net architecture which was initially introduced in [34]
is a symmetrical fully convolutional network with skip connections, comprising a contracting path and an expansive 3.2 Comparison of the different pruning techniques
path. The tested cell segmentation task and the dataset used
are part of the ISBI 2012 segmentation challenge1 . For We compared the WtoNP to the state-of-the-art weight and
node pruning methods as shown in Table 1 and Table 2,
1
http://brainiac2.mit.edu/isbi_challenge/
respectively. The methods were evaluated based on their
4

VGG-like for CIFAR-10 classification: The VGG-like
network was inspired by the original VGG-16 [36]. It is a
standard convolution neural network with 13 convolutional
layers followed by two fully connected layers with 512
and 10 neurons, respectively. The total number of trainable
parameters is 15M. We used the CIFAR-10 dataset [21]
to evaluate this model. The network was trained for 250
epochs with L2 regularization. For the pruning session
we used λ = 5e − 5 and λ = 1e − 5 for L2 and L1
regularization, respectively.

Table 1: Comparison to weight pruning algorithms
Model

Method

MLP-300-100
(MNIST)

LWC [12]
L-OBS [8]
Zhang et al [44]
SWS [38]
DNS [10]
GSM [7]
Autoprune [42]
WtoNP
BLIND [35]
Zhu et al [47]
Huang et al [18]
DCP [48]
SparseVD [29]
WtoNP

VGG-like
(CIFAR-10)

Error rate[%] ↓

% Pruned Weights ↑

1.64 → 1.58
1.76 → 1.82
1.60 → 1.60
1.89 → 1.94
2.28 → 1.99
1.81 → 1.82
1.72 → 1.78
1.53 → 1.71
6.75 → 6.59
6.49 → 6.69
7.23 → 10.63
6.01 → 5.43
7.55 → 7.55
6.46 → 6.69

91.77
93.00
95.63
95.70
98.21
98.34
98.75
98.45
86.10
88.24
92.80
93.58
98.46
93.86

Table 2: Comparison to node pruning methods
Model

Method

Error rate[%] ↓

% Pruned Nodes ↑

% Pruned Weights ↑

MLP-300-100
(MNIST)

SparseVD [29]
BC [26]
L0 [27]
NeST [6]
Autoprune [42]
WtoNP
Li et al [23]
StructuredBP [31]
Liu et al [24]
VCNN [45]
Wang et al [39]
WtoNP

1.60 → 1.80
1.60 → 1.80
1.60 → 1.80
1.29
1.60 → 1.82
1.40 → 1.73
6.75 → 6.60
7.20 → 7.50
7.53 → 8.25
6.75 → 6.82
6.87 → 6.85
6.46 → 7.71

41.05
67.14
67.30
16.13∗
69.01
49.00
37.12
75.17
47.35
62.00
59.10
60.34

97.80
89.20
−
97.05
−
98.30
64.00
−
36.00
73.34
91.80
83.53

VGG-like
(CIFAR-10)

Medical image networks pruned with WtoNP
Model

segmentation score (rand)↑

% Pruned Nodes ↑

% Pruned Weights ↑

U-Net

96.37 → 96.43

98.20 → 98.22

60.60

96.94

Model↑

precision↑

recall↑

f1-score↑

accuracy↑

% Pruned Nodes↑

% Pruned Weights↑

DarkCovidNet

89 → 89

83 → 82

85 → 85

84 → 85

48

97

COVID-19

Pneumonia

segmentation score (MI)↑

No-Findings.

Table 3: Pruning 90% of VGG-like weights on CIFAR-10
Regularization

L2
L1

Figure 3: Chest X-ray images of COVID-19 case (left)
Pneumonia case (middle) and No-Findings.

without

classification errors before and after the pruning procedure
and the percentages of pruned weights or nodes. For MLP300-100, we calculated the node percentage including the
input layer, marking with * pruning methods that did not
consider it. For the MLP-300-100 we obtained a weight
pruning percentage of more than 98%, while maintaining
higher accuracy than other pruning methods with similar
pruning ratios. For the VGG comparison, ‘pruned nodes’

Method

WtoNP
SKeras[46]
TD[9]
WtoNP
SKeras[46]
TD [9]
WtoNP
SKeras[46]
TD [9]

Zero node Ratio↑

Error rate[%]↓

26.9 ± 3.0
30.3 ± 5.8
0
64.1 ± 2.3
53.0 ± 8.9
0
1.7 ± 0.5
0.1 ± 0.1
0

9.0 ± 0.2
10.4 ± 0.6
9.6 ± 0.4
9.8 ± 0.1
8.3 ± 0.6
10.0 ± 0.2
8.2 ± 0.3
17.7 ± 0.8
10.5 ± 0.3

indicate the kernels’ prune percentage. Note the relatively
low error rate (6.69) with the high percentage of weight
pruning (93.86%) that was obtained using our WtoNP. In
addition, our method presented competitive nodes pruning
performances, by pruning 60% of the VGG network kernels. Pruning results of networks for medical application
5

are barely reported in the literature. Bear in mind that
though less than 4% of the U-Net weights remained, the
segmentation scores were improved.For the DarkCovidNet
Model we obtained 97% weight pruning similar classification reports, with higher accuracy and minor drop on the
recall. The results reported in Tables 1-2 were obtained
using L2 regularization.

any regularization (right). While the number of removed
connections (black) seems to be similar in all the plots, the
patterns of the remaining connections for the regularized
pruning appear to be much more coherent, which means
that entire rows and columns (nodes) were removed. On
the other hand, when no regularization was applied, the
pattern of remaining connections was random. The second
row presents the weights (color coded) of the edges connecting the second hidden layer (100 nodes) to the output
layer (10 nodes) visually. Note that for the regularized
pruning, very few nodes were required to represent each of
the output digits. In addition, as expected, the magnitudes
of the remaining connections for the pruning with L2 regularization were much lower. The weights’ color map for
unregularized pruning was much denser, with many more
active nodes (columns) in general. Note however (see also
first row in table 5 and figure 5) that the average number
of nodes require to represent each digit is similar to one
obtained using pruning with L2 regularization. The maps
presented in the figure correspond to the results shown in
Table 4 when pruning 97% of the weights.

We next tested the impact of L1 and L2 regularizations
on pruning performance for the VGG-like network using
the WtoNP and two other pruning approaches that originally did not include a regularization term. Specifically,
we used a standard Keras implementation of magnitudebased weight pruning [46] and an implementation of the
targeted Dropout algorithm [9] with a TensorFlow backend
2
. Table 3 presents the comparisons for pruning 90% of
the weights. It shows that the presence of a regularization term facilitated node pruning for the magnitude-based
weight pruning method in [46] as well. On the other hand,
regularization did not influence the targeted Dropout pruning method since the pruning in that case is done as a
post-processing step.
3.3

In the next experiment we examined the average number
of nodes required to represent an MNIST digit. Note that
in the original unpruned network, most of the 100 nodes
in FC2 were connected to all 10 nodes in the output layer,
where each node represents a single digit. Figure 5 and
Table 5 present the average number of nodes in FC2 that
remained connected to each of the nodes in the output
layer for different regularization schemes. These average
numbers are presented for the case of 95% pruned weights
(Table 5) and as a function of the percentage of pruned
weights (Figure 5). The last row in Table 5 presents the
percentages of pruned nodes in FC2 when 95% of the
pruned weights were removed. Note that we ran each
experiment 4 to 6 times so that the tables include standard
deviation values, shown as error-bars in the figures.

Analysis of pruning structure for different
regularizations

In this section, we presents further pruning results obtained
using L1 , L2 and elastic net regularizations for the VGG,
MLP, U-Net and DaekCovidNet networks and investigate
the pruning structure in particular layers for the MLP-300100 and the U-Net architectures.
Table 4 presents further pruning results obtained using
L1 , L2 and the elastic-net regularizations with respect to
no regularization at all for the different neural network
architectures. For fair comparison, in all experiments we
kept the percentage of pruned weights fixed and present
the respective accuracy and percentage of pruned nodes.
As expected, there is a trade-off between these measures,
where L1 is preferable for the VGG-like network when the
main objective is to maximize the percentage of pruned
nodes, whereas L2 works the best in all respects for the UNet. We stress that in all experiments the pruning process
was applied directly to the weights and not to the nodes
(kernels), while tuning λ and a to achieve the desired
percentage of pruned weights.

U-Net for cell segmentation in microscopy images: Figure 6 presents the U-Net architecture (lower panel) along
with the respective bar plot (upper panel) listing the percentages of pruned kernels in each layer. Recall that the
U-Net was diluted by direct pruning of its weights. In
the presence of L1 (blue), L2 (red) and elastic-net (yellow) regularizations throughout the weight pruning process, complete U-Net’s kernels were pruned as well. As
expected, the node pruning percentages were much lower
when no regularization was applied (purple bars). Table 6
presents the respective percentages of pruned weights and
kernels in the entire network along with the segmentation
scores for the test data.

MLP-300-100 for MNIST classification: The MLP-300100 is composed of an input layer with 784 nodes, two fully
connected hidden layers, termed FC1 and FC2 containing
300 and 100 nodes, respectively, and an output layer with
10 nodes, where each node represents a digit. Figure 4
presents a visualization of the weights of the edges connecting between the consecutive MLP layers. The first
row in the figure presents binary maps of the remaining
connections between the input layer (784 nodes) and the
first hidden layer (300 nodes) after pruning with either the
L1 (left) or L2 (middle) regularization terms or without
2

DarkCovidNet Model for automatic COVID-19 detection using raw chest X-ray images:
Figure 7 presents the DarkCovidNet architecture (lower
panel) along with the respective bar plot (upper panel)
listing the percentages of pruned kernels in each layer for
weights pruning percentage of 97%. In the presence of
L1 (blue), L2 (red) regularizations throughout the weight

https://pypi.org/project/keras-targeted-dropout/

6

Table 4: Pruning Results for Different Regularizations
Model

Reg

Top-1 Error[%]

% Pruned nodes

MLP-300-100

L1
L2
Elastic-net
No-regularization
L1
L2
Elastic-net
No-regularization
L1
L2
No-regularization
L1
L2
No-regularization

2.57 ± 0.2
1.66 ± 0.1
2.28 ± 0.2
1.71 ± 0.1
2.28 ± 0.1
1.76 ± 0.1
1.98 ± 0.1
2.18 ± 0.3
9.79 ± 0.1
8.97 ± 0.2
8.17 ± 0.3
10.10 ± 0.2
8.86 ± 0.2
8.29 ± 0.2

42.30 ± 0.8
34.80 ± 0.7
41.40 ± 0.7
13.30 ± 0.4
45.90 ± 0.7
47.40 ± 0.7
45.80 ± 0.7
16.07 ± 2.2
64.07 ± 2.3
26.96 ± 3.1
1.68 ± 0.5
69.86 ± 1.0
17.10 ± 4.3
1.86 ± 0.3

VGG-like

% Pruned weights

92

97

90
95

Model

Reg

seg score (rand)↑

segm score (MI)

% Pruned Nodes

% Pruned Weights

U-Net

L1
L2
Elastic-net
No-regularization

94.48 ± 0.7
95.81 ± 0.7
94.25 ± 0.6
94.28 ± 0.8

97.43 ± 0.1
98.12 ± 0.2
97.39 ± 0.7
97.72 ± 0.1

1.45 ± 0.2
68.05 ± 5.9
3.46 ± 2.5
0

95

Model↑

Reg

% Pruned Weights↑

DarkCovidNet

L1
L2
No-regularization

precision↑

recall↑

f1-score↑

accuracy↑

% Pruned Nodes↑

88.67 ± 0.4
88.28 ± 0.5
88.89 ± 1.6

82.33 ± 1.4
81.72 ± 1.3
83.00 ± 1.4

84.50 ± 0.7
84.17 ± 0.9
85.11 ± 1.1

84.00 ± 0.1
84.00 ± 0.7
84.33 ± 2.0

68.87 ± 0.6
40.07 ± 8.4
0±0

97

Table 5: The average number of nodes in FC2 that re- Table 7: Node and weight pruning percentages in MLPmained connected to a node in the output layer (nodes 300-100 when L2 regularization was applied for different
per digits) and the percentage of pruned nodes in FC2 for values of λ. The results refer to Figure 8 in the main paper.
MLP-300-100 with different regularization schemes. ↓
2e − 4
6e − 4
12e − 4
lower is sparser, ↑ higher is sparser.
% pruned weights 97.7 ± 0.1 98.5 ± 0.1 98.9 ± 0.1
Regularization

# nodes per digit

% pruned nodes in FC2

L1
L2
elastic-net
w/o reg.

11.8 ± 1.0
24.4 ± 2.7
11.8 ± 0.9
21.7 ± 1.5

84.7 ± 1.4
64.8 ± 3.1
80.4 ± 1.9
3.5 ± 2.1

% pruned nodes

4

L1

L2

Elastic-net

w/o reg

75.4
99.9
97.6
94.9

82.0
99.7
97.7
95.1

82.3
99.8
97.3
93.9

36.5
99.8
97.5
95.0

53.9 ± 0.7

57.7 ± 0.9

Analysis of pruning dynamics

In this section, we investigate the influence of different
regularization terms on the dynamics and the structure of
weight and node pruning for different network architectures. To highlight the robustness of the pruning dynamics
we ran each of the experiments 4 to 6 times for different
sets of initial weights. Specifically, we initialized the first
training sessions (prior to the pruning) using different sets
of random weights. As a result, the initial weights of the
pruning sessions were different for each run. The plots are
therefore shown with error bars.
Regularization strength: In Figure 8 we present the dynamics of weight and node pruning for the MLP-300-100
using three different values of L2 regularization coefficients λ. As expected, the percentage of pruned weights
increased as λ took on higher value since the regularization was stronger. However, the change in the percentage
of pruned nodes as a function of λ was higher. Table 7
presents the final pruning results.
Regularization terms: In the next experiment we compared the effects of L1 , L2 and elastic-net regularizations

Table 6: Percentages of the pruned kernels and weights
in the entire network along with the segmentation (seg.)
scores in the presence of L1 , L2 , or elastic-net regularization terms or without any regularization.
% pruned kernels
% pruned weights
seg. score (MI)
seg. score (Rand)

48.2 ± 1.1

pruning process, complete kernels of the deeper layers
were pruned. This indicates that there is redundancy, especially in the deep layers. When no regularization was
applied, the node pruning percentage is zero.
7

50

50

50

100

100

100

150

150

150

200

200

200

250

250

250

300

300
100

200

300

400

500

600

700

300
100

200

300

400

500

600

700

100

60

80

500

600

700
4

2

0

-2

-4

-4

-4

40

400

0
1
2 2
3
4
0
5
6
-2 7
8
9

0
1
2 2
3
4
0
5
6
-2 7
8
9

20

300

4

4

0
1
2
3
4
5
6
7
8
9

200

20

100

40

L1

60

80

L2

100

20

40

60

80

100

w/o regularization

Figure 4: Visualization of the remaining connections between consecutive layers in MLP-300-1000 after pruning with
L1 and L2 (left, middle) and without (right) regularization. The first row in the figure presents binary maps of the
remaining connections between the input layer (784 nodes) and the first hidden layer (300 nodes). The second row is a
visual presentation of the weights (color coded) of the edges connecting the second hidden layer (100 nodes) to the
output layer (10 nodes).
The plots presented for the MLP-300-100 show that L1
(blue), L2 (red) and the elastic-net (yellow) regularization
terms had similar influence on node and weight pruning
dynamic and percentages. Different patterns of node pruning dynamics of the VGG and the U-Net architectures
were observed for the different regularization terms. It
appeared that L1 regularization was preferable for VGG
node pruning, yet for weight pruning L2 was preferable.
Overall using L2 regularization for U-Net provided better
segmentation performances, yet better node and weight
pruning ratios were obtained by either L1 or elastic-net
regularizations.

40

Nodes per digit

35

L 1 reg

Elastic reg

L 2 reg

w\o reg

30
25
20
15
10
5
86

88

90

92

94

96

98

Weight compensation and weight decay: Finally, we
tested our hypothesis that when no regularization is applied, the magnitudes of edges that are neighbors of zero
edges (connected to the same nodes) increase to compensate for the missing edges while in the presence of a
regularization term, this compensation mechanism is suppressed and gradually entire nodes are zeroed. The results
of this experiment are presented in Figure 10. To assess
our assumptions, we chose a specific layer for each of the
four architectures. We then calculated, for each epoch,
the mean ratio of pruned weights per node (magenta) and
the mean magnitude of non-zero weights per node (blue).
The size of the error bars presents the average variance
per node. The first row in Figure 10 presents the results
obtained without regularization. Since a very high percentage of the weights were pruned, the mean ratio of pruned
weights per node (magenta plot) almost reached one. Yet,
the nodes themselves were not zeroed as the magnitudes of

100

Weight pruning percentage
Figure 5: The average number of nodes representing an
MNIST digit as a function of the weight pruning percentage for MLP-300-100. Results are shown for pruning with
L1 , L2 and elastic-net as well as without regularization.

with respect to training with no regularization - on the
dynamics of weight and node pruning for four different architectures. The plots presented in Figure 9 shows the
weight/node pruning percentages as a function of the
number of epochs. In the absence of a regularization
term (purple plots) node pruning percentage was either
low (MLP) or negligible (VGG, U-Net, DarkCovidNet)
whereas weight pruning percentage was relatively high.
8

% of pruned kernels

100
80
60
40

L1

Elastic-Net

L2

w/o

20
0
a.64 b.64 c.128 d.128 e.256 f.256 g.512 h.512 i.256 j.256 k.256 l.128 m.128 n.128 o.64 p.64 q.64

# of kernels for each layer
a.

b.

o.
c.

d.

l.
e.

64

64

128

128

256

f.

256

g.

h.

512

512

i.

j.

k.

256

256

256

m.

p.

q.

n.

Copy
Conv
Conv+
Max-pool
Upconv
128

128

128

64 64 64

Figure 6: The U-Net architecture along with its respective bar plot presenting the percentage of pruned kernels
in each of its layers. Lower panel: visualization of the U-Net architecture. The U-Net is composed of convolutional
encoder layers of decreasing size followed by convolutional decoder layers of increasing size. In addition, corresponding
encoder-decoder layers are connected with skip connections. The number of kernels in each layer is indicated underneath
the bar that represents it.
Upper panel: percentage of pruned kernels in each layer for the U-Net architecture for L1 , L2 , elastic-net regularization
and no regularization. (top image).
Modifies U-net architecture were each box corresponds to a multi-channel feature map, with the number of channels
indicated below the box. Arrows in different colors denote different operations. The gray arrows correspond to copied
feature maps. (bottom image).

% of pruned kernels

100
L1
L2
w/o

80
60
40
20
0

a.8

b.16

c.32

d.32

e.32

f.64

g.64

h.64 i.128 j.128 k.128 l.256 m.256 n.256 0.128 p.256

q.3

# of kernels for each kernel
a.
b.
c.

d.

Conv
Conv+Max-pool

e.
f.

8

16

32

32

32

64

g.

64

h.

64

i.

j.

k.

128

128

128

l.

m.

n.

256

256

256

o.

128

q.

p.

256

3

Figure 7: The DarkCovidNet architecture along with its respective bar plot presenting the percentage of pruned
kernels in each of its layers. Lower panel: visualization of the DarkCovidNet architecture. TheDarkCovidNet is
composed of convolutional layers of decreasing size. The number of kernels in each layer is indicated underneath the
bar that represents it.
Upper panel: percentage of pruned kernels in each layer for the DarkCovidNet architecture for L1 , L2 and no
regularization. (top image).
DarkCovidNet architecture were each box corresponds to a multi-channel feature map, with the number of channels
indicated below the box. Arrows in different colors denote different operations. (bottom image).
9

pruning results were on a par with both best the weight and
node pruning algorithms for different widely used image
classification and segmentation architectures.

References
Figure 8: Pruning dynamics for MLP-300-100 applied
to the MNIST classification. Weight (left) and node
(right) pruning percentages as a function of the number of epochs. Three sets of experiments were conducted using L2 regularization with different values λ =
2e−4, 6e − 4, 12e − 4 in blue, red and yellow, respectively.

the remaining weights gradually increased to compensate
for the missing weights. The second and the third rows of
Figure 10 present pruning dynamic results when L2 regularization is applied. The plots in the second row present
the results for nodes that were not zeroed throughout the
entire pruning process. Similar to the experiments with no
regularization (first row) the magnitudes of the remaining
weights in the none-zero nodes increased to compensate
for the zeroed weights. However, the increment in the
mean magnitude was much larger and the variance in magnitude was higher, probably to compensate for the large
number of zeroed nodes. The plots in the third row of Figure 10 present the mean results for nodes that eventually
decreased to zero. For these nodes the regularization suppressed the tendency of the non-zero weights to grow in
order to compensate for their neighboring zeroed weights.
Eventually, these weights were also zeroed, as shown in
the blue plots. Note that the corresponding average ratio
of pruned weights per nodes (magenta) increased to 1.

5

Conclusions

[1] Aflalo, Y., Noy, A., Lin, M., Friedman, I., Zelnik,
L., 2020. Knapsack pruning with inner distillation.
arXiv preprint arXiv:2002.08258 .
[2] Arganda-Carreras, I., Turaga, S.C., Berger, D.R.,
Cireşan, D., Giusti, A., Gambardella, L.M., Schmidhuber, J., Laptev, D., Dwivedi, S., Buhmann, J.M.,
et al., 2015. Crowdsourcing the creation of image
segmentation algorithms for connectomics. Frontiers
in neuroanatomy 9, 142.
[3] Augasta, M.G., Kathirvalavakumar, T., 2013. Pruning algorithms of neural networks—a comparative
study. Central European Journal of Computer Science 3, 105–115.
[4] Blalock, D., Ortiz, J.J.G., Frankle, J., Guttag, J., 2020.
What is the state of neural network pruning? arXiv
preprint arXiv:2003.03033 .
[5] Cohen, J.P., Morrison, P., Dao, L., 2020. Covid19 image data collection.
arXiv 2003.11597
URL:
https://github.com/ieee8023/
covid-chestxray-dataset.
[6] Dai, X., Yin, H., Jha, N.K., 2019. Nest: A neural
network synthesis tool based on a grow-and-prune
paradigm. IEEE Transactions on Computers 68,
1487–1497.
[7] Ding, X., Zhou, X., Guo, Y., Han, J., Liu, J., et al.,
2019. Global sparse momentum sgd for pruning
very deep neural networks, in: Advances in Neural
Information Processing Systems, pp. 6379–6391.

[8]
We presented a general stochastic approach for neural network pruning which is indifferent to the network architecture, its training regime and its loss function. The method
is shown to be effective for concurrent weight and node
[9]
pruning. This is accomplished by utilizing weight decay
regularization to facilitate pruning and understanding its
role in manipulating pruning dynamics. Rather than using
a fixed pruning criterion, we defined a probability function [10]
and a gating mechanism such that weights with lower absolute values were more likely to be removed. Regularization
then played a triple role. Beyond its clear advantage in
[11]
reducing overfitting, it decreased the weights magnitudes
thus increasing their pruning probability. Moreover, while
optimizing for weight pruning, node pruning was increased
as well. We empirically studied the mechanism for this
effect. Specifically, we showed that the L1 and L2 reg- [12]
ularization term suppressed the ‘tendency’ of non-zero
weights to compensate for neighboring pruned edges associated with the same nodes. Finally, we showed that the
10

Dong, X., Chen, S., Pan, S., 2017. Learning to prune
deep neural networks via layer-wise optimal brain
surgeon, in: Advances in Neural Information Processing Systems, pp. 4857–4867.
Gomez, A.N., Zhang, I., Swersky, K., Gal, Y., Hinton,
G.E., 2019. Learning sparse networks using targeted
dropout. arXiv preprint arXiv:1905.13678 .
Guo, Y., Yao, A., Chen, Y., 2016. Dynamic network
surgery for efficient dnns, in: Advances in neural
information processing systems, pp. 1379–1387.
Han, S., Mao, H., Dally, W.J., 2015a. Deep compression: Compressing deep neural networks with
pruning, trained quantization and huffman coding.
arXiv preprint arXiv:1510.00149 .
Han, S., Pool, J., Tran, J., Dally, W., 2015b. Learning
both weights and connections for efficient neural network, in: Advances in neural information processing
systems, pp. 1135–1143.

VGG-like

90
85
50

100

150

50

0

200

0

50

40
30
20
10
100

200

0

250

0

1000

150

80
60
40
20
0
0

200

50

100

150

3000

4000

5000

50

0
0

6000

20

200

50

0
0

250

1000

2000

3000

4000

40

60

80

100

80

100

Number of epochs

100

5000

100

50

0
0

6000

20

40

60

Number of epochs

Number of epochs

Number of epochs

Number of epochs

2000

100

Number of epochs

% of pruned kernels

% of pruned kernels

% of pruned nodes

pruned nodes

50

50

150

50

Number of epochs

Number of epochs

0

100

100

% of pruned kernels

0

100

DarkCovidNet
% of pruned kernels

95

U-Net
% of pruned weights

100

% of pruned weights

% of pruned weights

pruned weights

MLP-300-100

Figure 9: Percentages of pruned weights (left) and pruned kernels/nodes (right) for MLP-300-100 (first row), for
VGG-like (second row) , the U-Net (third row) and DarkCovidNet (forth row) calculated for each epoch. The plots are
presented for different regularization terms: L1 (blue), L2 (red) and elastic-net (yellow) and with no regularization
(purple).

No regularization

MLP-300-100

VGG-Like
0.1

1.2
1

0.15

1

1

0.05
0.5

0.5

10 -3

12

1

1.2
1

10

8

0.1
0.5

0.6
6
0.4

0.6
4

0

50

100

150

0
200

0
0

50

100

150

200

0 0.05
250
0

500

Epoch

Epoch

L2 , non-zeroed nodes

DarkCovidNet

0.8

0.8

0.4

1
0.03

0.15

0.5
0.02

0
1500

0

50

100

150

1

50

100

150

200

0.5

0 0.01
250
0

500

Epoch

0.15
0.1

1000

0
1500

1

1

0.50.01

0.5

50

100

Epoch

150

0
200

0.8

3

0.6

2.5

0.4

2

0.2

1.5
0

20

40

60

80

10

0
0

50

100

150

200

0
250

Epoch

0
100

-3

1.2

2.5

1
0.8
0.6

1
0.4

0.5

0

0

1

3.5

1.5

0.5

0

0
100

1.2

2

0.02

-0.05

80

10 -3

3

0.02

0.05

60

Epoch

0.04

1

40

4

Epoch

0.03

20

0.04

0.03
0.5

0 0.01
0
200

0

4.5

0.02

0.1

0.2

2

Epoch

0.05

1
0.2

1000

Epoch

0.04

0.25

Epoch

L2 , zeroed nodes

U-Net

0.2

0

0

500

1000

Epoch

0
1500

-0.5
0

20

40

60

80

0
100

Epoch

Figure 10: The mean value of non-zero weights per node (blue) and the mean ratio of pruned weights per node
(magenta) for each epoch. The first row displays pruning dynamics w/o regularization, the second and third rows
display pruning dynamics with L2 regularization of the non-zeroed nodes (second) and the zeroed nodes (third).

11

[13] Hanson, S.J., Pratt, L.Y., 1989. Comparing bi- [27] Louizos, C., Welling, M., Kingma, D.P., 2017b.
ases for minimal network construction with backLearning sparse neural networks through l_0 regpropagation, in: Advances in neural information proularization. arXiv preprint arXiv:1712.01312 .
cessing systems, pp. 177–185.
[28] Luo, J.H., Wu, J., Lin, W., 2017. Thinet: A filter
level pruning method for deep neural network com[14] Hassibi, B., Stork, D.G., 1993. Second order derivapression, in: Proceedings of the IEEE international
tives for network pruning: Optimal brain surgeon, in:
conference on computer vision, pp. 5058–5066.
Advances in neural information processing systems,
pp. 164–171.
[29] Molchanov, D., Ashukha, A., Vetrov, D., 2017. Variational dropout sparsifies deep neural networks, in:
[15] He, Y., Liu, P., Wang, Z., Hu, Z., Yang, Y., 2019.
Proceedings of the International Conference on MaFilter pruning via geometric median for deep convochine Learning, pp. 2498–2507.
lutional neural networks acceleration, in: Proceedings of the IEEE Conference on Computer Vision [30] Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz,
and Pattern Recognition, pp. 4340–4349.
J., 2016. Pruning convolutional neural networks for
resource efficient transfer learning. arXiv preprint
[16] He, Y., Zhang, X., Sun, J., 2017. Channel pruning
arXiv:1611.06440 3.
for accelerating very deep neural networks, in: Proceedings of the IEEE International Conference on [31] Neklyudov, K., Molchanov, D., Ashukha, A., Vetrov,
D.P., 2017. Structured bayesian pruning via logComputer Vision, pp. 1389–1397.
normal multiplicative noise, in: Advances in Neural
[17] Hinton, G.E., et al., 1986. Learning distributed repreInformation Processing Systems, pp. 6775–6784.
sentations of concepts, in: Proceedings of the eighth
annual conference of the cognitive science society, [32] Ozturk, T., Talo, M., Yildirim, E.A., Baloglu, U.B.,
Yildirim, O., Acharya, U.R., 2020. Automated
p. 12.
detection of covid-19 cases using deep neural net[18] Huang, Q., Zhou, K., You, S., Neumann, U., 2018.
works with x-ray images. Computers in Biology and
Learning to prune filters in convolutional neural netMedicine , 103792.
works, in: IEEE Winter Conference on Applications
[33]
Pasandi, M.M., Hajabdollahi, M., Karimi, N.,
of Computer Vision (WACV), pp. 709–718.
Samavi, S., 2020. Modeling of pruning techniques
[19] Huang, Z., Wang, N., 2018. Data-driven sparse strucfor deep neural networks simplification. arXiv
ture selection for deep neural networks, in: Proceedpreprint arXiv:2001.04062 .
ings of the European conference on computer vision [34] Ronneberger, O., Fischer, P., Brox, T., 2015. U-net:
(ECCV), pp. 304–320.
Convolutional networks for biomedical image segmentation, in: International Conference on Medical
[20] Kingma, D.P., Ba, J., 2014.
Adam: A
image computing and computer-assisted intervention,
method for stochastic optimization. arXiv preprint
Springer. pp. 234–241.
arXiv:1412.6980 .
[21] Krizhevsky, A., Hinton, G., et al., 2009. Learning [35] Salama, A., Ostapenko, O., Klein, T., Nabi, M., 2019.
Prune your neurons blindly: Neural network commultiple layers of features from tiny images .
pression through structured class-blind pruning, in:
[22] LeCun, Y., Denker, J.S., Solla, S.A., 1990. Optimal
IEEE International Conference on Acoustics, Speech
brain damage, in: Advances in neural information
and Signal Processing (ICASSP), pp. 2802–2806.
processing systems, pp. 598–605.
[36] Simonyan, K., Zisserman, A., 2014. Very deep convo[23] Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf,
lutional networks for large-scale image recognition.
H.P., 2016. Pruning filters for efficient convnets.
arXiv preprint arXiv:1409.1556 .
arXiv preprint arXiv:1608.08710 .
[37] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever,
I., Salakhutdinov, R., 2014. Dropout: a simple way
[24] Liu, C., Wu, H., 2019. Channel pruning based on
to prevent neural networks from overfitting. The
mean gradient for accelerating convolutional neural
journal of machine learning research 15, 1929–1958.
networks. Signal Processing 156, 84–91.
[25] Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, [38] Ullrich, K., Meeds, E., Welling, M., 2017. Soft
weight-sharing for neural network compression.
C., 2017. Learning efficient convolutional networks
arXiv preprint arXiv:1702.04008 .
through network slimming, in: Proceedings of the
IEEE International Conference on Computer Vision, [39] Wang, W., Zhu, L., Guo, B., 2019. Reliable identifipp. 2736–2744.
cation of redundant kernels for convolutional neural
network compression. Journal of Visual Communi[26] Louizos, C., Ullrich, K., Welling, M., 2017a.
cation and Image Representation 63, 102582.
Bayesian compression for deep learning, in: Advances in Neural Information Processing Systems, [40] Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., Sumpp. 3288–3298.
mers, R., 2017. Hospital-scale chest x-ray database
12

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

and benchmarks on weakly-supervised classification and localization of common thorax diseases, in:
IEEE CVPR.
Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H., 2016.
Learning structured sparsity in deep neural networks,
in: Advances in neural information processing systems, pp. 2074–2082.
Xiao, X., Wang, Z., Rajasekaran, S., 2019. Autoprune: Automatic network pruning by regularizing
auxiliary parameters, in: Advances in Neural Information Processing Systems, pp. 13681–13691.
Zhang, C., Bengio, S., Hardt, M., Recht, B.,
Vinyals, O., 2016. Understanding deep learning
requires rethinking generalization. arXiv preprint
arXiv:1611.03530 .
Zhang, T., Ye, S., Zhang, K., Tang, J., Wen, W.,
Fardad, M., Wang, Y., 2018. A systematic dnn
weight pruning framework using alternating direction method of multipliers, in: Proceedings of the
European Conference on Computer Vision (ECCV),
pp. 184–199.
Zhao, C., Ni, B., Zhang, J., Zhao, Q., Zhang, W.,
Tian, Q., 2019. Variational convolutional neural network pruning, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2780–2789.
Zhu, M., Gupta, S., 2017. To prune, or not to prune:
exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878 .
Zhu, X., Zhou, W., Li, H., 2018. Improving deep
neural network sparsity through decorrelation regularization., in: International Joint Conferences on
Artificial Intelligence, pp. 3264–3270.
Zhuang, Z., Tan, M., Zhuang, B., Liu, J., Guo, Y.,
Wu, Q., Huang, J., Zhu, J., 2018. Discriminationaware channel pruning for deep neural networks, in:
Advances in Neural Information Processing Systems,
pp. 875–886.

13

