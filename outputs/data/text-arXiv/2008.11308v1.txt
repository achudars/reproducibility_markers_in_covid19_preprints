Identifying Coordinated Accounts in Disinformation Campaigns
Karishma Sharma

Emilio Ferrara

Yan Liu

University of Southern California
krsharma@usc.edu

University of Southern California
emiliofe@usc.edu

University of Southern California
yanliu.cs@usc.edu

arXiv:2008.11308v1 [cs.SI] 25 Aug 2020

ABSTRACT
Disinformation campaigns on social media, involving coordinated
activities from malicious accounts towards manipulating public
opinion, have become increasingly prevalent. There has been growing evidence of social media abuse towards influencing politics
and social issues in other countries, raising numerous concerns.
The identification and prevention of coordinated campaigns has
become critical to tackling disinformation at its source. Existing
approaches to detect malicious campaigns make strict assumptions
about coordinated behaviours, such as malicious accounts perform
synchronized actions or share features assumed to be indicative of
coordination. Others require part of the malicious accounts in the
campaign to be revealed in order to detect the rest. Such assumptions significantly limit the effectiveness of existing approaches.
In contrast, we propose AMDN (Attentive Mixture Density Network) to automatically uncover coordinated group behaviours from
account activities and interactions between accounts, based on
temporal point processes. Furthermore, we leverage the learned
model to understand and explain the behaviours of coordinated
accounts in disinformation campaigns. We find that the average
influence between coordinated accounts is the highest, whereas
these accounts are not much influenced by regular accounts. We
evaluate the effectiveness of the proposed method on Twitter data
related to Russian interference in US Elections. Additionally, we
identify disinformation campaigns in COVID-19 data collected from
Twitter, and provide the first evidence and analysis of existence of
coordinated disinformation campaigns in the ongoing pandemic.

CCS CONCEPTS

☑

󾓦
☑

👉
😲

☑

☑

Figure 1: Coordinated accounts identified in COVID-19
dataset with the proposed method (AMDN). Example accounts and tweets from the coordinated group, spreading
politically motivated conspiracies, by promoting content of
others in the group and repeated sharing of its own content.

• Computing methodologies → Machine learning.

KEYWORDS
Coordinated Influence Campaigns, Disinformation, Social Media,
Fake News, Temporal Point Process
ACM Reference Format:
Karishma Sharma, Emilio Ferrara, and Yan Liu. 2018. Identifying Coordinated Accounts in Disinformation Campaigns. In Woodstock ’18: ACM
Symposium on Neural Gaze Detection, June 03–05, 2018, Woodstock, NY . ACM,
New York, NY, USA, 9 pages. https://doi.org/10.1145/1122445.1122456

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Woodstock ’18, June 03–05, 2018, Woodstock, NY
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/1122445.1122456

1

INTRODUCTION

In recent times, the persistent abuse of social media for spreading
disinformation and influencing public opinion and social outcomes
has become an increasingly pressing problem [20]. It has largely
been used as a tactic to influence elections [2], public perception on
social issues such as social distancing policies related to COVID-19
[21] and other local and global events. The issue has gained even
more relevance during the ongoing COVID-19 pandemic, where
increased reliance on social media for information related to healthcare and policies, has made it an easy target for large-scale disinformation campaigns [7, 21].
The frequent abuse of social media by malicious individuals and
organizations has greatly reduced trust in social media platforms.
Disinformation or influence campaigns involving coordinated efforts from malicious accounts operating together have become an
increasingly sophisticated form of social media manipulation [29].
The earliest cases of disinformation campaigns surfaced during
the 2016 US Presidential Elections. Investigations by the US Congress revealed 2,752 Twitter accounts that were linked to Russian

Woodstock ’18, June 03–05, 2018, Woodstock, NY

information operation agencies namely the âĂĲInternet Research
AgencyâĂİ (IRA) or Russian “troll farm" in an effort to manipulate
US Elections. The accounts controlled by Russian operatives (human operators tied with such agencies, referred to as “trolls") were
used to influence the 2016 US election by pushing disinformation,
propaganda and politically divisive narratives on social media [3].
More recently, other malicious accounts listed by Twitter originating from Russia, Iran, Venezuela, China and other countries,
suggest persistent efforts to manipulate online discussions [10].
The increasing exploitation of social media to influence politics and
social issues in other countries raises numerous ethical concerns
[14]. Therefore, the identification and prevention of malicious influence campaigns is key to maintaining the integrity of social media
platforms, democracy and elections, and public health.
The detection of coordinated accounts or troll farms is critical
towards tackling disinformation at its source. Earlier approaches
have tried to uncover malicious accounts based on their participation in disinformation cascades, or based on their automated
account activities in bot accounts [20, 23]. Malicious accounts in
troll farms pose more sophisticated challenges for detection. First,
such accounts have been linked to human operators at information
agencies, and have been noted to use tactics such as persuasion,
defamation, and polarization in their narratives besides disinformation to influence opinions [14]. Second, their coordinated efforts
can make the accounts and their contents appear more organic;
Fig. 1 shows an example of coordinated accounts uncovered by
our method on COVID-19. More recent approaches have proposed
techniques specific to detection of troll farms based on individual
or collective group behaviours of the malicious accounts. Individual
features such as deceptive linguistic cues, content and metadata
were found useful [1], however, [30] noted that such features of
trolls were not consistent over time, limiting generalization to unseen troll farms without partially uncovered troll accounts.
In contrast, collective or group anomalous behaviours of malicious accounts involved in coordinated campaigns, can be useful towards detecting such campaigns, without access to partially
uncovered accounts. This approach to detecting coordinated campaigns has been exploited in previous works, primarily based on
assumptions that the account activities are synchronized in time
[4], or from assumed features of coordinated behaviour e.g. similar
sequence of hashtags indicate coordination [17, 25, 28]. Both these
assumptions on synchronization and on hand-crafted coordination
signatures are ineffective at reducing false positives, and make strict
assumptions on coordinated behaviours which need not hold true.
In this work, we address the shortcomings of existing approaches
by instead proposing to automatically discover coordinated behaviours from accounts activities on social media. We propose
AMDN (Attentive Mixture Density Network), to model discrete
event sequences of accounts activities in continuous time, based
on temporal point processes. The model is used to learn the latent
interactions between accounts on the network and estimate the
influence structure between them, directly from the activities. We
leverage the learned model to identify group anomalous patterns to
detect coordinated campaigns. Our contributions are the following,
• We propose AMDN to detect coordinated campaigns from
collective group behaviours inferred from account activities.

Karishma Sharma, Emilio Ferrara, and Yan Liu

• Using AMDN, we uncover characteristic patterns in the influence structure to understand and explain the behaviours
of coordinated accounts in disinformation campaigns.
• The method is evaluated on data related to Russian Inference in US Elections. Furthermore, we conduct analysis on
COVID-19 data collected from Twitter, and are the first work
to provide evidence of existence of coordinated disinformation campaigns related to the ongoing COVID-19 pandemic.

2

RELATED WORK

The problem of disinformation and social media abuse has reduced
trust in online platforms. Efforts to combat false and misleading
information have been widely studied in many different contexts
[20] ranging from detection of false information from content features and the responses to it on social media, to understanding the
diffusion patterns [19] and accounts involved in its spread [8].
The analysis and detection of suspicious accounts and their behaviours plays a pivotal role in tackling the problem at its source.
In line with this approach, several different techniques have been
examined in earlier works. For instance, [19] identify accounts that
are influential in the spread of false information by modeling the
spread of true and false news on the network, and examine its
characteristics. In [18], the authors propose a fake news detection
model that assigns a suspiciousness score to accounts based on
their participation in fake news cascades. The detection of malicious bot accounts that are automated or semi-automated have also
been investigated based on content and social network features of
the accounts [8, 9]. Different from bots and individual malicious
accounts, a growing area of research is the detection of “troll farms"
or coordinated disinformation or influence campaigns, orchestrated
by human operatives at information agencies [13].
Individual behaviours. Several works examine the characteristics of individual accounts (also referenced as trolls) in disinformation campaigns (troll farms) based on the textual and temporal
features of their posts on social media. [1] examined the use of
deceptive linguistic features by trolls. [12] rely on account and
linguistic features such as number of shared links, retweets, mentions to distinguish trolls. [29] study the hashtags, time of day and
week and device of posting, the reported geographical location, and
cross-platform activity of trolls for the Russian inference campaign
linked to the Internet Research Agency. Apart from textual or metadata features, the activity traces of troll accounts have been found
useful for understanding malicious behaviours. In recent work by
[13], the tweet, retweet and reply patterns of Twitter accounts are
utilized to infer the incentives or rewards behind their activities,
formulated as an inverse reinforcement learning problem. Based
on the estimated rewards, the authors found that the behaviours
of trolls was different from regular users when their content was
re-shared or replied to, wherein they appeared to perform their
activity regardless of the responses.
Collective behaviours. The most related to our work are the
approaches that examine the collective or group behaviours as a
whole to detect anomalous malicious accounts. [4, 11] cluster accounts that take similar actions around the same time, based on
the assumption that malicious account activities are synchronized
in time. Other works that cluster or partition a account similarity

Identifying Coordinated Accounts in Disinformation Campaigns

graph defined over hand-crafted features assumed to be indicative of coordinated behaviours, include [17, 25]. For instance, [17]
provides examples of hand-crafted features such as the sequence
of hashtags or articles shared, collectively by a large group of accounts, which can be used as a measure of suspicious coordinated
behaviours. The significant limitation of such approaches is that
the assumption on synchronization or hand-crafted features used
to define coordination might not hold. In contrast, we propose to
automatically uncover signatures of coordination from activities
and interactions between accounts.

3

METHOD

In this section, we introduce the proposed method for detecting
coordinated accounts in disinformation campaigns on social media
from collective group behaviours of the accounts.
Input. The only input we consider are the activity traces of
accounts on the social network available as a sequence of events
ordered in time. The event sequences can be represented as Cs =
[(u 1 , t 1 ), (u 2 , t 2 ), (u 3 , t 3 ), · · · (un , tn )] where the tuple (ui , ti ) corresponds to an activity by account ui at time ti . The activities represent account actions on the network such as posting original
content, re-sharing, or replying, commenting, and reacting to other
posts. In this work, we assume that the type of action or features
such as contents of the post or account metadata are not provided,
although additional available features can be easily incorporated in
the method. This basic information is the most easily available for
any social network. Furthermore, we assume that there is no prior
knowledge about the disinformation campaign and all the accounts
coordinating to orchestrate the campaign are unknown and need to
be detected given the activity traces of all accounts on the network.

3.1

Modeling account activities

In this section, we introduce the method AMDN (Attentive Mixture Density Network) to model account activities and interactions
between accounts towards detecting coordinated campaigns.
In contrast to individual behaviours, collective or group anomalous behaviours of malicious accounts involved in coordinated campaigns, can be useful towards detecting such campaigns, without
access to partially uncovered accounts in the disinformation campaign. Although this approach to detecting coordinated campaigns
has been exploited in previous works discussed earlier [4, 17, 28],
the main challenge lies in identifying what features constitute as
suspicious coordinated behaviours. An example is, determining if a
group of accounts collectively sharing similar hashtags over a period of time suggest malicious coordinated activities? This reliance
on defined features introduces large number of false positives and
prevents a principled solution to address the problem, with multiple
heuristic design choices [17] to reduce false positives, and improve
recall with assumed coordinated behaviours.
Here, we address these challenges by instead proposing to automatically discover coordinated behaviours from the activity traces
of accounts on the network. To that end, we propose to model
account activities or discrete network events in continuous time
based on temporal point processes; and use the model to learn latent interactions between accounts, directly from the activities, to

Woodstock ’18, June 03–05, 2018, Woodstock, NY

discover coordinated group anomalous behaviours. We discuss the
details of the model and architecture in the following subsections.

3.2

Preliminaries on temporal point processes

A temporal point process (TPP) is a stochastic process whose realization is a list of discrete events in continuous time t ∈ R+
[5]. In the case of different event types (here, accounts), a marked
temporal point process assigns a type or mark u to each event [6]
and can be used to capture temporal dynamics of different event
types. Temporal point processes are also equivalently represented
as counting processes N (t) indicating the number of event occurrences up to time t. The history of events in the sequence up to time
t are generally denoted as Ht = {(ui , ti )|ti < t, ui ∈ U} where U
represents the set of event types. The conditional intensity function
λ(t |Ht ) of a point process is defined as the instantaneous rate of
an event in an infinitesimal window at time t, given the history i.e.
λ(t |Ht )dt = E[dN (t)|Ht ]. The conditional density function of the
i th event can be derived from the conditional intensity function [6],
as,
 ∫

p(t |Ht ) = λ(t |Ht ) exp −

t

t i −1

λ(s |Ht )ds

(1)

In social network data, the widely used formulation of the conditional intensity function is the multivariate Hawkes Process (HP)
[32],
Õ
λi (t |Ht ) = µ i +
α i, j κ(t − t j )
(2)
t j <t

where λi (t |Ht ) is the conditional intensity of event type i at time t
with base intensity µ i > 0 and mutually triggering intensity from
past events where α i, j > 0 captures the influence of event type j on
i and κ is a decay kernel to model influence decay over time. µ and
α are learnable parameters which can be inferred from the data by
maximizing the likelihood (MLE) of observed event sequences.
Neural point process models. The Hawkes Process (HP) formulation [32] assumes additive, positive influence from past events
on future ones, generally with an exponential intensity function
based on the exponential temporal decay kernel. It provides limited
flexibility for capturing complex event sequences and triggering
patterns between events. This has led to research efforts in neural
network based point process models [6]. The two forms of representing the point process models are either by modeling the conditional
intensity function [6, 15, 31, 33] or modeling directly the conditional density function [16, 22]. The added flexibility with neural
network point process models comes with the cost of reduced interpretability of the interactions and influence between events and
event types, compared to HP where inferred α i j is an interpretable
form of influence or triggering patterns between event types.

3.3

Model architecture and training

Existing neural network point process models suffer from different
drawbacks (summarized in Table 1) with a trade-off between flexibility in modeling complex event sequences and functional forms
of point process, expressivity in modeling long-range event dependencies, interpretability, and closed-form likelihood for estimating
parameters with maximum likelihood estimation.
Limitations of existing neural methods Methods SAHP and
THP (Table 1 require Monte Carlo sampling to estimate an integral

Karishma Sharma, Emilio Ferrara, and Yan Liu

Event history on the network
h1

h2

h3

h4

MH-Attn

MH-Attn

MH-Attn

MH-Attn

Emb.

Emb.

Emb.

Emb.

Masked
Self-Attention

Position and time
embedding

Context

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Conditional
density
function
p(𝝉|history)

Time

t

Figure 2: Architecture of proposed (AMDN) to model conditional density function of account activities on social media.
Table 1: Summary of neural point process models.
Model

Flexible
intensity
function

HP [32]
RMTPP [6]
FullyNN [16]
LogNormMix [22]
SAHP [31]
THP [33]
AMDN

no
limited
y
y
limited
y
y

Closedform
likelihood
y
y
y
y
no
no
y

Longrange
dependencies
y
limited
limited
limited
y
y
y

Interpretable
Influence
y
no
no
no
y
y
y

in the likelihood function used in MLE (discussed later), as their intensity formulation does not have closed form solutions; which provides noisy gradients in training the model, reducing effectiveness.
On the other hand, HP, RMTPP define specific functional forms, e.g.
exponential for intensity parameterization to have closed form likelihoods but are limited in the flexibility of the selected functional
form of the intensity. LogNormMix and FullyNN alleviate these
issues by modeling the conditional density and cumulative density
getting flexibility and closed-form likelihoods. However, the neural
network parameterizations in RMTPP, LogNormMix, FullyNN are
based on recurrent neural networks and cannot be directly used
to examine the influence between events and event types, which
is possible in the other methods. We address this by modeling the
conditional density function with interpretable neural network parameterizations, so that the influence structure can be learned and
examined to understand coordinated behaviours of malicious accounts. In addition, by choosing more expressive parameterizations,
we improve the ability of the model to capture long range and more
structured dependencies on historical events, improving event time
and type predictions (discussed in the experiment results).
In the following paragraphs, we discuss and provide details of
the model architecture of AMDN and training function for MLE.
Encoding event sequences with masked self-attention. We
model event sequences as temporal point processes with the stochastic random variable τ ∈ R+ representing the inter-event time.
We denote p(τ |Hτ ) as the probability density function of the random variable given the event history. In order to represent the

history Hτ , we can leverage representation learning with neural
networks to automatically extract useful features when learning the
parameters of the model, similar to neural point process approaches.
We encode the input sequence S = {(ui , ti )}1L of L events using a
masked self-attention mechanism. As compared to recurrent neural
networks (RNNs), self-attention based transformer models have
been used extensively in natural language processing [24].
Masked self-attention. The weights of the self-attention mechanism can be computed with matrix operations as follows,
√
A = σ (QK T / d) and Hat t n = AV
(3)
Q = XWq , K = XWk , V = XWv
where σ is the softmax activation to compute attention weights
A of each event pairwise with all other events in the input sequence S. Q, K, V are the query, key, value matrices which are
projections of the input representation X ∈ RL×d of the events
in the sequence. L is the sequence length, d is the input feature
√
dimension. Weights Wq ,Wk ,Wv are learnable parameters. d is
the scaling factor to prevent large attention weights due to dotproducts in high-dimensional inputs. At the end, we apply layer
normalization, dropout and feed-forward layer to Hat t n to get output Hout ∈ RL×d .
The self-attention must capture the dependency of each event
on the sequence of history events prior to it. Therefore, to prevent
the model from looking into future events, we use masked selfattention by masking out the attention weights along the upper
diagonal of A. The output representations of each event i in the
sequence is then a weighted linear combination of projections V of
the historical input events, prior to the event i in the sequence.
Position, event and time encoding. The attention mechanism by
itself does not retain ordering of the events in the input sequence.
Therefore encoding positional information in the input is necessary. We use the sine-cosine functions for each dimension j of
m-dimensional position encoding, proposed in [24].
PEpos,2j = sin(pos/100002j/m ),
PEpos,2j+1 = cos(pos/100002i/m )

(4)

We also need to represent the input event type and time (ui , ti )
information. Since we do not include additional features of accounts,
and rely only on the activity traces, we must represent the event
type with its one-hot encoding, and get its embedding based on
learnable embedding matrix We ∈ R |U |xm for all event types.
In addition, since events sequences consist of irregular events in
time, which is different from sequences used in language models; it
becomes necessary to additionally encode time intervals between
events. We encode the temporal information based on [26] which
allows translation-invariant temporal kernel functions supported
by functional analysis theorems. The kernel function K : T × T →
R can be represented in terms of high-dimensional feature maps
ϕ shown below, and decomposed into multiple periodic kernel
functions of different frequencies ω to embed time information.
K(t 1 , t 2 ) = ψ (t 1 − t 2 ) = ⟨ ϕ(t 1 ), ϕ(t 2 )⟩
√
√
√
ϕ ω (t) = [ c 1 , · · · c 2j cos(jπt/ω), c 2j+1 sin(jπt/ω) · · · ]
ϕ(t) = [ϕ ω1 (t), ϕ ω2 (t) · · · ϕ ωk (t)]T

(5)
(6)
(7)

Identifying Coordinated Accounts in Disinformation Campaigns

Woodstock ’18, June 03–05, 2018, Woodstock, NY

In the translation-invariant kernel function, c i are chosen as free
parameters, ω are set as frequencies at different intervals in a range,
and ϕ ω ∈ Rm , with the time difference between events as input.
Also, ϕ ∈ Rm is obtained by summing along the frequency axis. We
note that recent works SAHP and THP also similarly leveraged selfattention for encoding the event sequences, but proposing heuristic
temporal encoding, as variants of positional encoding but for time.
The embedding X i of event i from input (ui , ti ) with eui as onehot encoding of event type ui ; is a concatenation of event, position
and temporal embedding, as follows. X is for the whole sequence,
and is used as input to the masked self-attention mechanism.
X i = [eui We , PEpos=i , ϕ(ti − ti−1 )]

(8)

Event history context vector. The attention mechanism gives us
representations of each event using attention over events prior to it.
We can use the representation of the last event or a recurrent network layer over the attention outputs Hout ∈ RLxd to summarize
events histories into context vectors C ∈ RLxd where L is event
sequence length. Each c i ∈ C is a context vector encoding history
of events up to ti i.e. history Hti of the temporal point process.
Conditional probability density function. p(τ |Hτ ) mentioned
earlier denotes the conditional probability density function of the
random variable corresponding to the time τ ∈ R+ of the next event
given the history. The conditional density can be represented with
any chosen functional form, and parameterized by the extracted
context vector c i of event history up to event i. Since τ needs to be
positive, [22] proposed to represent the conditional density with
a mixture of log-normal distributions. A mixture distribution can
allow learning of more flexible functional forms of the density function, not restricted to exponential or other monotonic functions.
Therefore the PDF of inter-event time given history, based on [22],
!
K
Õ
(log τi − µ ik )2
1
k
p(τi |w i , µ i , si ) =
wi
(9)
√ exp −
2(sik )2
τsik 2π
k =1
w i = σ (Vw c i + bw ), si = exp(Vs c i + bs ), µ i = Vµ c i + b µ (10)
where the mixture weights w i , means µ i and stddevs si are parameterized by extracted context history c i and learnable V , b. σ is
softmax for mixture weights, and exp is positivity constraint.
The parameters of the model including the parameters used
to encode the history (denoted jointly as θ ) can be learned using
maximum likelihood estimation from observed event sequences, as
the density can be differentiably evaluated at ti for events i in the
sequence and trained with gradient back-propagation.
θ ∗ = argmaxθ

3.4

L
Õ

i=1

log pθ (ti |Hti ) + log pθ (ui |Hti )



(11)

inferred interactions; automatically discovered from the activity
traces without any assumed coordination features. Coordinated
accounts can exhibit different strategies for promoting content
than regular accounts. Therefore, by clustering the learned event
type (account) embeddings we can identify anomalous groups of
accounts that are involved in coordinated campaigns to promote and
amplify the spread of content with focused agenda of manipulating
opinions through social media.
One advantage of encoding event histories based on masked selfattention in this context, is that it can extract structured and longrange dependencies in the event sequence more effectively than
sequential RNNs. In social media activities, structured dependencies
are especially prevalent, since single source posts might be shared
by multiple accounts forming long chains of events that are strongly
dependent on the source and not necessarily intermediate events.
The other advantage is that it provides interpretable influence
between events based on the attention weights across sequences.
This is useful in this context for discovering and explaining behaviours of coordinated accounts in disinformation campaigns. We
take the influence between event i and j to be Ai j from the inferred
attention weights. Then the influence between event types u and
v are the average of the attention weights across all sequences in
which u is in the history of v. We examine these inferred influence
structures in the experiments section.

4

EXPERIMENT RESULTS

In this section, we evaluate effectiveness of AMDN on real datasets
related to Russian Interference in US 2016 Elections and Twitter
data collected on the ongoing COVID-19 pandemic. We provide
evidence of existence of coordinated disinformation campaigns and
provide analysis of narratives and targets of the campaign.

4.1

Datasets

4.1.1 Russian Interference in US 2016 Elections (IRA). This dataset
collected by [1] contains Twitter activities of regular accounts and
trolls accounts connected with the US 2016 elections. The list of
trolls was identified by the US Congress and released by Twitter,
on the Russian Internet Research Agency (IRA) disinformation
campaign. The imbalanced dataset contains 312 trolls and 1713
non-trolls after filtering out accounts that were involved in less
than k=10 active and passive online activities, consistent with [13].
4.1.2 COVID-19 dataset. We collect this dataset of Twitter posts
related to COVID-19 from March to June, 2020. The dataset does not
have any labeled troll accounts, unlike the IRA dataset. However,
we identify misinformation tweets (unreliable, conspiracy, clickbait,
biased) within the dataset from news sources linked to the tweets,
consistent with [21]; used in analysis of discovered campaigns. We
get 995 accounts after filtering sparse activities.

Detecting coordinated accounts

The architecture of the AMDN model to model activities on social
media is shown in Fig 2. AMDN can model and predict the next
event time and type on the network, and therefore learns to capture
the mutual influence and interactions between accounts e.g. which
account triggers or influences activities of another account.
Therefore, the learned model can be used to identify collectively
anomalous behaviours of accounts based on their activity traces and

4.2

Baselines

We compare against existing approaches used to identify coordinated accounts in disinformation campaigns. The baselines are
based on extracting features of coordination from activity patterns
and using them for supervised or unsupervised detection of malicious accounts, based on both individual or collective behaviours.
The following are baselines based on existing approaches.

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Karishma Sharma, Emilio Ferrara, and Yan Liu

Table 2: Results on detection of coordinated disinformation
campaigns of Russian (IRA) interference in US Elections.
Method

AP

AUC

F1

Prec

Rec

MaxF1

Macro

Co-activity
Clickstream
IRL
IRL (S)
HP
HP (S)

16.7
16.9
20.0
67.2
33.7
72.2

53.4
53.5
61.0
89.6
69.4
90.7

24.0
21.5
26.5
55.7
37.6
72.9

18.2
20.5
21.9
78.1
38.7
75.6

35.5
22.8
33.6
43.6
36.5
70.8

27.6
21.5
34.0
63.3
54.5
76.0

50.9
53.2
54.3
74.9
63.3
84.1

AMDN
AMDN (S)

72.2
84.1

91.8
93.1

72.7
75.7

79.5
74.3

67.3
77.8

75.3
78.9

84.1
85.6

Table 3: Results on point process inference in Russian (IRA)
interference in US 2016 Elections dataset.
Method

Neg. Log
Likelihood

Event-Time
NLL

Event-Type
Acc. (%)

HP
RMTPP
FullyNN
LogNormMix
SAHP
THP

17.1894
7.3052
6.3142
6.3345
11.3518
12.7125

9.5798
3.5526
2.7321
2.7224
3.7775
7.8164

12.96
44.57
45.19
45.09
36.42
32.96

AMDN

6.3128

2.7129

45.13

Table 5: Top frequent hashtags in tweets of Non-trolls (NT)
and identified coordinated trolls (T) in COVID-19 data.
Non-Trolls (NT)

Trolls (T)

COVID19
bigdata
Trump
USNews
USA
China
UK

KAG
Trump
QAnon
QAnon2018
QAnon2020
COVID19
China

Method
HP
RMTPP
FullyNN
LogNormMix
SAHP
THP
AMDN

Neg. Log
Likelihood

Event-Time
NLL

Event-Type
Acc. (%)

767.1399
20.8304
12.5810
6.4961
8.1251
7.0429

745.4426
12.9541
4.9130
0.8051
1.2232
1.0384

6.74
3.22
5.91
12.71
2.51
10.47

6.3170

0.7584

13.63

(1) Co-activity clustering [18]. Co-activity features to identify
groups of accounts that repeatedly share the same set of
articles, was proposed in [18] to detect malicious accounts.
Account features are extracted using SVD on a binary eventparticipation matrix representing which articles are shared
by which accounts. We cluster the features for detection.
(2) Clickstream clustering [17, 25]. It is proposed by [25] to analyze user behaviours, and based on hierarchical clustering of
accounts with similar activity patterns can be used to identify coordinated groups [17]. Similarity between activities is
represented based on post, reply and re-share patterns.
(3) IRL (S) [13]. This approach is based on inverse reinforcement
learning to automatically discover motives of trolls from

CoronaVirus
WWG1WGA
DemocratsHateAmerica
TWGRP
Trump2020
MAGA
VoteRedToSaveAmerica

rewards estimated from activity traces. Estimated rewards
are used as features to distinguish malicious accounts. [13]
use the features with AdaBoost classifier to detect remaining
trolls from partially uncovered disinformation campaigns.
(4) IRL [13]. We also compare an unsupervised variant of the
IRL approach based on clustering with Gaussian mixture
models or K-means clustering to detect trolls, without subset
of trolls in the campaign being revealed.
(5) HP and HP (S). In addition, we compare our method to modeling activities using the Hawkes Process (HP) [27] with influence α i j factorized by learnable embeddings of accounts i
and j. We examine both clustering and supervised detection
of trolls from the embedding features.

4.3
Table 4: Results on point process inference in COVID-19 misinformation dataset.

Health
HealthCare
WHO
DonaldTrump
poliics
News
France

Experiment setting

We infer account embeddings and influence structures with AMDN
on the two datasets. We use activity sequences of maximum length
128, splitting longer sequences, batch size of 256 on NVIDIA-1080Ti.
For the embedding sizes we use 32 or 64 dimensions, number of
mixture components for the PDF between {2,4,8,16,32}, single head
and single layer attention module, two layer MLP for event prediction from the encoded history. We use Adam optimizer with 1e-3 lr
and 1e-5 regularization. We train for max 1000 epochs with early
stopping based on validation likelihood of sequences (75/15/10).
The setting is consistent for all point process models training.

4.4

Results

4.4.1 Detection of trolls accounts in disinformation campaigns. The
accuracy of detection of coordinated accounts is evaluated in the
IRA dataset with known disinformation campaigns. Table 2 provides
a comparison of different metrics for detection of troll farms. The
results are averaged over 5-stratified folds of the imbalanced dataset.
AMDN outperforms existing approaches by a large margin by
automatically detecting group anomalous behaviours from network
activities. Hawkes process factorized with account embeddings is
less flexible in modeling activity sequences, and therefore cannot
match the performance of AMDN, but is still a stronger baseline
than other approaches. IRL [13] is the next best, which also automatically infers motives of trolls behaviours from activities. IRL
has higher precision but lower recall, which can be due to considering individual behaviours rather than collective group anomalous
behaviours. Other methods that make assumptions on the type
of coordination have both low precision and low recall. AMDN

Identifying Coordinated Accounts in Disinformation Campaigns

Woodstock ’18, June 03–05, 2018, Woodstock, NY

(b) Account embeddings inferred from the
activities using AMDN for Russian (IRA)
dataset. The top 1713 and botton 312 rows
(a) Influence structure between accounts inferred from attention weights of AMDN for Russian of non-trolls and trolls accounts.
(IRA) dataset. NT: Non-troll accounts and T: Troll accounts. (Left): Avg. influence weights between
trolls and non-trolls accounts. (Right): Influence weights between accounts in log-scale.

Figure 3: AMDN. Analysis of inferred influence weights and interactions between trolls and non-trolls in IRA dataset.

30
20
10
0

Trolls (T)
Non-Trolls (NT)
Misinformation Types Distribution

(a) Distribution over misinformation types
in COVID-19 dataset. Malicious coordinated campaign identified by the proposed
method referred to as Trolls (T) have
higher % of conspiracy posts than for the
Non-trolls (NT) accounts.

Non-trolls (NT)

14
12
10
8
6
4
2
0

Account Creation Date (Year)

(b) Plot of account creation year in COVID19 dataset. Malicious coordinated campaign identified by the proposed method
referred to as Trolls (T) have higher percent of accounts created during 2016-2020
for Trolls than Non-trolls.

20
(%) Trolls in Top-K Influential Accounts

40

Trolls (T)

2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020

Clickbait
Biased

(%) Accounts Created / Year

(%) Misinformation Types in T/NT

Conspiracy
Unreliable

50

100

200

500

50
40
30
20
10
0

IRA

Dataset

COVID-19

(c) Top-K most influential accounts in
datasets IRA and COVID-19 inferred from
influence weights learned by the proposed
method using PageRank for K=20 to 500.
The % of Trolls (T) in each top-K influential accounts is shown.

Figure 4: AMDN. Analysis of uncovered possible disinformation campaigns on COVID-19 dataset.
is also effective in the unsupervised setting, where none of the
trolls accounts from the campaign are revealed. In such cases, the
group behaviours are more important for detection, since there
is no training set of trolls to train supervised classifiers based on
extracted features; and the method must only rely on anomalous
patterns in the data to identify troll behaviours.
4.4.2 Results on point process modeling. We also compare the effectiveness of AMDN at modeling activities on social media with
respect to existing neural point process models (discussed in Table 1). We evaluate the negative log-likelihood (NLL) per event,
next event time and type predictions. Lower NLL indicates better fit

to the observed test sequences (10% split). The results are averaged
over three runs. AMDN achieves better event time modeling due
to the self-attention encoding of history and flexible conditional
density modeling of events, on both datasets (Table 3 and 4). The
performance difference is larger on COVID-19 which has limited
training data with more missing activities than IRA dataset due to
differences in collection from Twitter APIs which provided random
1% sample of tweets in COVID-19 data. SAHP and THP are implemented from the open-source code, however their performance is
lower on this social media data, even with hyperparameter configurations mentioned in their papers. This might be attributed to the

Woodstock ’18, June 03–05, 2018, Woodstock, NY

Karishma Sharma, Emilio Ferrara, and Yan Liu

reliance on Monte Carlo sampling to estimate the likelihood which
can provide noisy gradients in training, and reduce performance.

on non-trolls and the influence they can have on the discourse on
social media.

4.5

5

Analysis

CONCLUSIONS

4.5.1 Uncovering characteristic behaviours from influence structure.
In Figure 3a, we examine the influence structure and account interactions learned by AMDN through the inferred attention weights
between event types. Higher attention paid by an event on a history
event, indicates that the history event has a stronger influence on
the future event in consideration, and might have triggered it.
An interesting insight from the aggregate influence between
trolls and non-troll accounts is that the strongest influence ties
are between trolls (T) accounts, and the least influence is from
non-trolls (NT) to trolls (T). This further indicates that trolls are
not easily triggered by regular accounts, and are primarily focused
on promoting each others contents. These findings also suggest
that trolls behaviours and motives are left behind in their activity
traces and might be hard to erase due to their specific motives to
push particular narratives. Therefore, exploiting these collective
group anomalous patterns might even be effective in detecting
sophisticated coordinated disinformation campaigns such as the
Russian Interference and COVID-19 campaigns examined here.

In this work, we proposed a technique to detect coordinated accounts in disinformation campaigns based on their collective behaviours, inferred directly from their activities on social media. The
proposed method is independent of linguistic, metadata or platform
specific features, and hence can be generalized across platforms and
languages or countries from where the disinformation campaigns
originate. Furthermore, deceptive linguistic cues, content and other
account features can be easily incorporated in the proposed point
process inference model by providing the features as input.
With analysis on Russian Interference and COVID-19 datasets,
we find interesting insights about disinformation campaigns. Moreover, the existence of politically motivated efforts to potentially
influence elections and spread disinformation threatening global
health during the pandemic certainly needs to be prevented. Techniques to quantify the risk and impact of such campaigns, and
strategies to prevent them are important directions of future work.

4.5.2 Uncovering COVID-19 disinformation campaigns. In the data
on COVID-19, we do not have any labeled information about which
accounts might be trolls, or even whether a disinformation campaign exists. We trained AMDN on the account activities and evaluated the performance on point process modeling, discussed in
the previous subsection. We use the learned influence structure to
cluster and identify group anomalous accounts.
Findings. With analysis of the narratives and metadata of the
discovered accounts, we conclude that coordinated disinformation
campaigns likely exist related to the COVID-19 pandemic, and
are primarily politically motivated. Of the 995 accounts, AMDN
finds anomalous group of 208 accounts ("Trolls"). We examine the
misinformation type from the contents shared by this group of
accounts. Fig 4a shows the distribution over types in the likely
trolls, and non-trolls group. There is a distinctly higher percentage
of conspiracy theories spread from trolls (T) accounts, compared to
non-trolls (NT) that have more clickbait.
The account creation date Fig 4b indicates high percentage of
these trolls (T) are accounts created between 2016 to 2019 compared to non-trolls (NT). And from the narratives spread by these
accounts Table 5, a strong political connection of hashtags used by
these accounts can be seen. The account creation period and hashtags such as QAnon1 , provide evidence that this is likely an effort
to influence US Presidential elections in 2020. The timeline of hashtags also shows conspiracies related to Hydroxychloroquine (eg.
Fig 1), FauciFraud, OpenAmericaNow, ChinaMustPay, ChinaLiesPeopleDie,VoterFraud, CriminalPelosi which are strongly politically
motivated and extremely concerning. Lastly, to understand the impact, we also compute the top influential accounts using PageRank
on the inferred influence weights using AMDN. Fig 4c shows that
in both datasets, a fraction of the top influential accounts are trolls,
although in future work we will aim to assess the impact of trolls

[1] Aseel Addawood, Adam Badawy, Kristina Lerman, and Emilio Ferrara. 2019.
Linguistic cues to deception: Identifying political trolls on social media. In Proceedings of the International AAAI Conference on Web and Social Media, Vol. 13.
15–25.
[2] Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake news in the
2016 election. Journal of Economic Perspectives 31, 2 (2017), 211–36.
[3] Adam Badawy, Aseel Addawood, Kristina Lerman, and Emilio Ferrara. 2019.
Characterizing the 2016 Russian IRA influence campaign. Social Network Analysis
and Mining 9, 1 (2019), 31.
[4] Qiang Cao, Xiaowei Yang, Jieqi Yu, and Christopher Palow. 2014. Uncovering
large groups of active malicious accounts in online social networks. In Proceedings
of the 2014 ACM SIGSAC Conference on Computer and Communications Security.
477–488.
[5] Daryl J Daley and David Vere-Jones. 2007. An introduction to the theory of point
processes: volume II: general theory and structure. Springer Science & Business
Media.
[6] Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel GomezRodriguez, and Le Song. 2016. Recurrent marked temporal point processes:
Embedding event history to vector. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. 1555–1564.
[7] Emilio Ferrara. 2020. What types of COVID-19 conspiracies are populated by
Twitter bots? First Monday (2020).
[8] Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, and Alessandro
Flammini. 2016. The rise of social bots. Commun. ACM 59, 7 (2016), 96–104.
[9] Emilio Ferrara, Onur Varol, Filippo Menczer, and Alessandro Flammini. 2016.
Detection of promoted social media campaigns. In tenth international AAAI
conference on web and social media.
[10] Vijaya Gadde and Yoel Roth. 2018. Enabling further research of information
operations on Twitter. Twitter Blog 17 (2018).
[11] Sonu Gupta, Ponnurangam Kumaraguru, and Tanmoy Chakraborty. 2019. Malreg:
Detecting and analyzing malicious retweeter groups. In Proceedings of the ACM
India Joint International Conference on Data Science and Management of Data.
61–69.
[12] Jane Im, Eshwar Chandrasekharan, Jackson Sargent, Paige Lighthammer, Taylor
Denby, Ankit Bhargava, Libby Hemphill, David Jurgens, and Eric Gilbert. 2020.
Still out there: Modeling and identifying russian troll accounts on twitter. In 12th
ACM Conference on Web Science. 1–10.
[13] Luca Luceri, Silvia Giordano, and Emilio Ferrara. 2020. Detecting troll behavior
via inverse reinforcement learning: A case study of Russian trolls in the 2016 US
election. In Proceedings of the International AAAI Conference on Web and Social
Media, Vol. 14. 417–427.
[14] Diego A Martin, Jacob N Shapiro, and Michelle Nedashkovskaya. 2019. Recent
trends in online foreign influence efforts. Journal of Information Warfare 18, 3
(2019), 15–48.
[15] Hongyuan Mei and Jason M Eisner. 2017. The neural hawkes process: A neurally
self-modulating multivariate point process. In Advances in Neural Information
Processing Systems. 6754–6764.

1 https://www.nytimes.com/2018/08/01/us/politics/what-is-qanon.html

REFERENCES

Identifying Coordinated Accounts in Disinformation Campaigns

[16] Takahiro Omi, Kazuyuki Aihara, et al. 2019. Fully neural network based model for
general temporal point processes. In Advances in Neural Information Processing
Systems. 2122–2132.
[17] Diogo Pacheco, Pik-Mai Hui, Christopher Torres-Lugo, Bao Tran Truong, Alessandro Flammini, and Filippo Menczer. 2020. Uncovering Coordinated Networks on
Social Media. arXiv preprint arXiv:2001.05658 (2020).
[18] Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017. CSI: A Hybrid Deep Model for
Fake News Detection. In Proceedings of the 2017 ACM on Conference on Information
and Knowledge Management. ACM, 797–806.
[19] Karishma Sharma, Xinran He, Sungyong Seo, and Yan Liu. 2021. Network Inference from a Mixture of Diffusion Models for Fake News Mitigation. In Fifteenth
international AAAI conference on web and social media.
[20] Karishma Sharma, Feng Qian, He Jiang, Natali Ruchansky, Ming Zhang, and Yan
Liu. 2019. Combating Fake News: A Survey on Identification and Mitigation
Techniques. ACM Transcations on Intelligent Systems and TEchnology (2019).
[21] Karishma Sharma, Sungyong Seo, Chuizheng Meng, Sirisha Rambhatla, Aastha
Dua, and Yan Liu. 2020. Coronavirus on social media: Analyzing misinformation
in Twitter conversations. arXiv preprint arXiv:2003.12309 (2020).
[22] Oleksandr Shchur, Marin Biloš, and Stephan Günnemann. 2020. Intensity-Free
Learning of Temporal Point Processes. ICLR (2020).
[23] Onur Varol, Emilio Ferrara, Clayton A Davis, Filippo Menczer, and Alessandro
Flammini. 2017. Online human-bot interactions: Detection, estimation, and
characterization. arXiv preprint arXiv:1703.03107 (2017).
[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998–6008.

Woodstock ’18, June 03–05, 2018, Woodstock, NY

[25] Gang Wang, Xinyi Zhang, Shiliang Tang, Haitao Zheng, and Ben Y Zhao. 2016.
Unsupervised clickstream clustering for user behavior analysis. In Proceedings of
the 2016 CHI Conference on Human Factors in Computing Systems. 225–236.
[26] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan.
2019. Self-attention with functional time representation learning. In Advances in
Neural Information Processing Systems. 15915–15925.
[27] Hongteng Xu. 2018. PoPPy: A Point Process Toolbox Based on PyTorch. arXiv
preprint arXiv:1810.10122 (2018).
[28] Rose Yu, Xinran He, and Yan Liu. 2015. Glad: group anomaly detection in social
media analysis. ACM Transactions on Knowledge Discovery from Data (TKDD) 10,
2 (2015), 1–22.
[29] Savvas Zannettou, Tristan Caulfield, Emiliano De Cristofaro, Michael Sirivianos,
Gianluca Stringhini, and Jeremy Blackburn. 2019. Disinformation warfare: Understanding state-sponsored trolls on Twitter and their influence on the web. In
Companion proceedings of the 2019 world wide web conference. 218–226.
[30] Savvas Zannettou, Tristan Caulfield, William Setzer, Michael Sirivianos, Gianluca Stringhini, and Jeremy Blackburn. 2019. Who let the trolls out? towards
understanding state-sponsored trolls. In Proceedings of the 10th acm conference
on web science. 353–362.
[31] Qiang Zhang, Aldo Lipani, Omer Kirnap, and Emine Yilmaz. 2020. Self-Attentive
Hawkes Process. ICML (2020).
[32] Ke Zhou, Hongyuan Zha, and Le Song. 2013. Learning social infectivity in
sparse low-rank networks using multi-dimensional hawkes processes. In Artificial
Intelligence and Statistics. 641–649.
[33] Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. 2020.
Transformer Hawkes Process. NeurIPS (2020).

