Large-scale, Language-agnostic Discourse Classification of
Tweets During COVID-19

arXiv:2008.00461v2 [cs.SI] 31 Oct 2020

Oguzhan Gencoglu
Faculty of Medicine and Health Technology, Tampere University
Tampere, Finland
oguzhan.gencoglu@tuni.fi

November 3, 2020

A BSTRACT
Quantifying the characteristics of public attention is an essential prerequisite for appropriate crisis
management during severe events such as pandemics. For this purpose, we propose language-agnostic
tweet representations to perform large-scale Twitter discourse classification with machine learning.
Our analysis on more than 26 million COVID-19 tweets shows that large-scale surveillance of public
discourse is feasible with computationally lightweight classifiers by out-of-the-box utilization of
these representations.
Keywords text classification · sentence embeddings · Twitter · natural language processing · deep learning · health
informatics

1

Introduction

Coronavirus disease 2019 (COVID-19) was declared a pandemic by the World Health Organization on 11 March
2020 [1]. Since first recorded case in Wuhan, China in late December 2019, 45.6 million people have been infected by
COVID-19 and consequently, 1.2 million people have lost their lives globally as of 30 October 2020 [2]. This constitutes
700 times more deaths than SARS and MERS combined [3]. During such large-scale adverse events, monitoring
information seeking behaviour of citizens, understanding general overall concerns, and identifying recurring discussion
themes is crucial for risk communication and public policy making [4, 5]. This need is further amplified in a global
pandemic such as COVID-19 as the primary responsibility of risk management is not centralized to a single institution,
but distributed across society. For instance, a recent study by Zhong et al. shows that people’s adherence to COVID-19
control measures is affected by their knowledge and attitudes towards it [6]. Previous national and global adverse health
events show that social media surveillance can be utilized successfully for systematic monitoring of public discussion
due to its instantaneous global coverage [7, 8, 9, 10, 11, 12].
Twitter, due to its large user-base, has been the primary social media platform for seeking, acquiring, and sharing
information during global adverse events, including the COVID-19 pandemic [13]. Especially during the early stages of
the global spread, millions of posts have been tweeted in a span of couple of weeks [14, 15, 16, 17, 18]. Consequently,
several studies proposed and utilized Twitter as a data source for extracting insights on public health as well as insights
on public attention during the COVID-19 pandemic. Focus of these studies include nowcasting or forecasting of
the disease, sentiment analysis, topic modeling, and quantifying misinformation/disinformation. Due to the novelty
and unknown epidemiological characteristics of COVID-19, accurate quantification of public discussions on social
media becomes especially relevant for disaster management (e.g. devising timely interventions or clarifying common
misconceptions).
So far, manual or automatic topical analyses of discussions on Twitter during COVID-19 pandemic have been performed
in an exploratory or descriptive manner [19, 20, 21]. Characterizing public discourse in these studies rely predominantly
on manual inspection, aggregate statistics of keyword counts, or unsupervised topic modeling by utilizing joint
distributions of word co-occurrences followed by qualitative assessment of discovered topics. Main reasons for previous

N OVEMBER 3, 2020

studies to avoid supervised machine learning approaches can be lack of annotated (labeled) datasets of public discourse
on COVID-19. Furthermore, previous studies either restrict their scopes to a single language (typically English tweets)
or examine tweets from different languages in separate analyses. This is mainly due to limitations of traditional topic
modeling algorithms as they usually do not operate in a multilingual or cross-lingual fashion.
In this study, we propose large-scale characterization of public discourse themes by categorizing more than 26 million
tweets in a supervised manner, i.e., classifying text into semantic categories with machine learning. For this purpose,
we utilize two different annotated datasets of COVID-19 related questions and comments for training our algorithms.
To be able to capture themes from 109 languages in a single model, we employ state-of-the-art multilingual sentence
embeddings for representing the tweets, i.e., Language-agnostic BERT Sentence Embeddings (LaBSE) [22]. Our
results show that large-scale surveillance of COVID-19 related public discourse themes and topics is feasible with
computationally lightweight classifiers by out-of-the-box utilization of these representations. We release the full source
code of our study along with the instructions to access the experiment datasets1 . We believe our work contributes to the
pursuit of expanding social media research for disaster informatics regarding health response activities.

2

Relevant Work

2.1

COVID-19 Twitter

Content analysis of Twitter data has been performed by various studies during the COVID-19 pandemic. Some studies
approach their research problem by manual or descriptive (e.g. n-gram statistics) content analysis of Twitter chatter
for gaining relevant insights [21, 23, 24, 25, 26, 27, 28, 29], while other studies utilize unsupervised computational
approaches such as topic modeling [19, 20, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49].
A high percentage of studies performing topic modeling and topic discovery on Twitter utilize the well-established
Latent Dirichlet Allocation (LDA) algorithm [20, 30, 33, 34, 36, 37, 40, 41, 42, 43, 44, 45, 46, 49]. Similar unsupervised
approaches of word/n-gram clustering [38, 39, 47] or clustering of character/word embeddings [35, 48] have been
proposed as well.
Tweet data utilized for most of these studies are restricted to a single language. Majority of the studies restrict their
analysis only to English tweets [19, 24, 29, 30, 33, 37, 39, 41, 43, 44, 45, 46, 48], possibly exacerbating the already
existing selection bias. Other studies have restricted their datasets to Japanese [47], Korean [21], Persian/Farsi [36], and
Polish [31] tweets. While studies that collect multilingual tweets exist, they have conducted their analyses (e.g. topic
modeling) separately for each language [23, 25, 40].
2.2

Representing Tweets

As effective representation learning of generic textual data has been studied extensively in natural language processing
research, tasks involving social media text benefit from recent advancements as well. While traditional feature
extraction methods relying on word occurrence counts (e.g. bag-of-words or term frequency-inverse document
frequency) have been extensively utilized in previous studies involving Twitter [50, 51, 52], they have been replaced
by distributed representations of words in a vector space (e.g. word2vec [53] or GloVe [54] embeddings). Distributed
word representations are learned from large corpora by a neural network, resulting in words with similar meanings
being mapped to closer vector representations with a feature number that is much smaller than the vocabulary size.
Consequently, sentences, documents, or tweets can be represented, e.g. as an average-pooling of its word embeddings.
Such representations have also been learned specifically from Twitter corpora as tweet2vec [55, 56] or hashtag2vec [57].
While distributed word/sentence embeddings provide effective capturing of semantics, they operate as a static mapping
from the textual space to the latent space. Serving essentially as a dictionary look-up, they often fail to capture
the context of the textual inputs (e.g. polysemy). This drawback has been circumvented by contextual word/token
embeddings such as ELMo [58] or BERT [59]. Contextual word embeddings enable the possibility of same word being
represented as different vectors if it appears in different contexts. Several studies involving tweets utilized these deep
neural network techniques or their variants either as a pre-training for further downstream tasks (e.g. classification,
clustering, entity recognition) or for learning tweet representations from scratch [60, 61, 62, 63, 64, 65, 66, 67]. Even
though BERT word embeddings are powerful as pre-trained language models for task-specific fine-tuning, Reimers
et al. show that out-of-the-box sentence embeddings of BERT and its variants can not capture semantic similarities
between sentences, requiring further training for that purpose [68]. They propose a mechanism for learning contextual
sentence embeddings using BERT neural architecture, i.e. sentence-BERT, enabling large-scale semantic similarity
1

https://github.com/ogencoglu/Language-agnostic_BERT_COVID19_Twitter

2

N OVEMBER 3, 2020

Number of Daily Tweets between 4 January-5 April 2020
900000
800000

WHO declares COVID-19 a pandemic

700000

Count

600000
500000
400000
300000
200000
100000
0

Jan 04

Jan 18

Feb 01

Feb 15

Feb 29

Mar 14

Mar 28

Date
Figure 1: Daily Twitter activity related to COVID-19 during the early stages of the pandemic.

comparison, clustering, and information retrieval with out-of-the-box vector representations [68]. Studies involving
Twitter data have been utilizing these contextual sentence embeddings successfully as well [69, 70, 71, 72].

3

Methods

3.1

Data

For Twitter data, we utilize the publicly available dataset of 152,920,832 tweets (including retweets) related to COVID19 between the dates 4 January 2020 - 5 April 2020 [73]. Tweets have been collected using the Twitter streaming API
with the following keywords: COVID19, CoronavirusPandemic, COVID-19, 2019nCoV, CoronaOutbreak, coronavirus,
WuhanVirus, covid19, coronaviruspandemic, covid-19, 2019ncov, coronaoutbreak, wuhanvirus [74]. As Twitter Terms
of Service does not allow redistribution of tweet contents, only tweet IDs are publicly available. Extraction of textual
content of tweets, timestamps, and other meta-data was performed with the use of open-source software Hydrator2 with
a Twitter developer account. For our study, we discard the retweets and at the time of extraction 26,759,164 unique
tweets were available which is the final number of observations used in this study. Daily distribution of these tweets
(7-day rolling average) can be observed from Figure 1.
For training machine learning classifiers, we utilize the following two recently-curated datasets: COVID-19 Intent [75] and COVID-19 Questions [76]. Intent dataset consists of 4,938 COVID-19 specific utterances (typically a question or a request) categorized into 16 categories to describe the author’s intent [75]. For instance,
the sample “is coughing a sign of the virus” has an intent related to Symptoms. The dataset consists of English, French, and Spanish utterances and has been synthetically created by native-speaker annotators based on
an ontology. We discard the uninformative categories of Hi and Okay/Thanks to end up with 4,325 samples
from this dataset. We combine Can_i_get_from_feces_animal_pets, Can_i_get_from_packages_surfaces,
and How_does_corona_spread categories into a single category of Transmission. Similarly, we merge
What_if_i_visited_high_risk_area category into Travel category to end up with 11 categories (classes).
Questions dataset consists of 1,245 questions categorized into 16 categories collected from 13 sources [76]. 7 of the
sources are frequently asked questions (FAQ) websites of recognized organizations such as the Center for Disease
Control (CDC) and 6 of them are crowd-based sources such as Google Search. We use 594 samples from this dataset
belonging to Prevention, Reporting, Speculation, Symptoms, Transmission, and Treatment categories. In
the end, the dataset for our experiments, i.e., training and validating text classification algorithms, consists of 4,919
textual samples collected from the abovementioned two datasets. 11 category labels of the final dataset are Donate,
News & Press, Prevention, Reporting, Share, Speculation, Symptoms, Transmission, Travel, Treatment,
2

https://github.com/DocNow/hydrator

3

N OVEMBER 3, 2020

Language

Samples

English
French
Spanish

2,119
1,400
1,400

Total
4,919
Table 1: Distribution of languages.

Category

Samples

Donate
News & Press
Prevention
Reporting
Share
Speculation
Symptoms
Transmission
Travel
Treatment
What Is Corona?

310
310
431
389
310
363
348
1,152
615
381
310

Total
4,919
Table 2: Distribution of category labels.

What Is Corona?. Sample distribution of languages and categories among the dataset can be examined from Table 1
and Table 2, respectively.
3.2

Tweet Embeddings

As the daily volume of COVID-19 related discussions on Twitter is enormous, computational public attention surveillance would benefit from lightweight approaches that can still maintain a high predictive power. Preferably, numerical
representations should encode the semantics of tweets in such a way that simple vector operations should suffice
for large-scale retrieval or even classification. Moreover, developed machine learning systems should be able to
accommodate tweets in several languages to be able to capture the public discourse in an unbiased manner. Multilingual
BERT-like contextual word/token embeddings [59] have been shown to be effective as pre-trained models if followed
by a task-specific fine-tuning. However, they do not intrinsically produce effective sentence-level representations [68].
In order to be able to take advantage of multilingual BERT encoders for extracting out-of-the-box sentence embeddings,
we employ Language-agnostic BERT Sentence Embeddings [22].
LaBSE embeddings combine BERT-based dual-encoder framework with masked language modeling (an unsupervised
fill-in-the-blank task where a model tries to predict a masked word) to reach state-of-the-art performance in embedding
sentences across 109 languages [22]. Trained on a corpus of 6 billion translation pairs, LaBSE embeddings provide
out-of-the-box comparison ability of sentences even by a simple dot product (essentially corresponding to cosine
similarity as embeddings are l2 normalized). We encode both the training data and 26.8 million tweets using this
deep learning approach, ending up with vectors of length 768 for each observation. Embeddings are extracted with
TensorFlow (version 2.2) framework in Python 3.7 on a 64 bit Linux machine with an NVIDIA Titan Xp GPU.
3.3

Intent Classification

As our choice of embeddings provide effective, out-of-the-box latent space representations of the textual data, simpler
classifiers can be directly employed for identifying the prevalent topic of a tweet. In fact, LaBSE embeddings provide
representations that are suitable to be compared with simple cosine similarity [22]. We train 3 classifiers (multi-class,
single-label classification), namely k-nearest neighbour (kNN), logistic regression (LR), and support vector machine
(SVM) to classify the observations into 11 categories. We employ a 10-fold cross-validation scheme to evaluate the
performance of the three models. For comparison, we run the same experiments for multilingual BERT embeddings
(base, uncased) as well. Hyperparameters of the classifiers are selected by Bayesian optimization (see Section 3.4).
Once the embedding-classifier pair with its set of hyperparameters giving the highest cross-validation classification
performance is selected, the classifier is trained with full dataset of 4,919 observations. With this model, inference on
26,759,164 samples of Twitter data embeddings is performed.
3.4

Bayesian Hyperparameter Optimization

Typically, machine learning algorithms have several hyperparameters that require tuning for the specific task to avoid
sub-optimal predictive performance. Most influential hyperparameters of k-nearest neighbour classifier are k (number
4

N OVEMBER 3, 2020

UMAP Visualization of Training Data Embeddings on 2D
Donate
News & Press
Prevention
Reporting
Share
Speculation
Symptoms
Transmission
Travel
Treatment
What Is Corona?

Figure 2: UMAP visualization of language-agnostic BERT sentence embeddings belonging to 4,919 observations
among 11 COVID-19 discourse categories.

of neighbours) and distance metric (e.g. cosine3 , euclidean, manhattan, etc.). For support vector machine classifier,
trade-off between training error and margin (essentially regularization), C, and the choice of kernel function (linear,
polynomial, or radial basis function) are the most crucial hyperparameters. l2 regularization coefficient is the main
hyperparameter for logistic regression classifier. We formulate the problem of finding the optimal set of classifier
hyperparameters, θ̂, as a Bayesian optimization problem:
θ̂ = argmax f (θ),
θ

(1)

PN
where f (θ) is the average of cross-validation accuracies for a given set of hyperparameters, i.e., N1 i=1 ACC i .
For our experiments N = 10 as we perform 10-fold cross-validation. We use Gaussian Processes for the surrogate
model [77] of the Bayesian optimization by which we emulate the statistical relationships between the hyperparameters
and model performance, given a dataset. We run the optimization scheme for 30 iterations (each iteration corresponds
to one full cross-validation) for each classifier separately.
Bayesian optimization is especially beneficial in settings where the function to be minimized/maximized, f (θ), is
a black-box function without a known closed-form and expensive to evaluate [78]. As f (θ) corresponds to crossvalidation performance in our case, it indeed is a black-box function that is computationally expensive to evaluate. That
is our motive for employing Bayesian hyperparameter optimization instead of manual tuning or performing grid-search
over a manually selected hyperparameter space.
3

Although, not an official distance metric as it violates triangle inequality.

5

N OVEMBER 3, 2020

BERT
Model
kNN
LR
SVM

3.5

Accuracy (%)

F1 (micro)

LaBSE
F1 (macro)

Accuracy (%)

F1 (micro)

F1 (macro)

72.54
0.725
0.725
82.76
0.828
0.827
76.62
0.766
0.771
86.05
0.844
0.846
81.81
0.818
0.820
86.92
0.876
0.881
Table 3: Cross-validation results of three classifiers for BERT and LaBSE embeddings.

Evaluation

For visual inspection of embeddings, we utilize Uniform Manifold Approximation and Projection (UMAP) to map
the high dimensional embeddings (768 for LaBSE and 1024 for BERT) to a 2-dimensional plane [79]. UMAP is a
frequently used dimensionality reduction and visualization technique that can preserve global structure of the data better
than other similar methods [79]. In their recent study, Ordun et al. employ UMAP visualization of COVID-19 tweets as
well [32].
Evaluation of classifiers and their sets of hyperparameters are performed by 10-fold cross-validation. Randomness (seed)
in cross-validation splits are fixed in order to perform fair comparison. Average accuracy (%) and F1 scores (micro and
macro averages) across 10 folds are reported for all classifiers (for their best performing set of hyperparameters) for
BERT and LaBSE embeddings. Confusion matrix for the best performing representation-classifier pair is reported as
well. After running inference on Twitter data to classify 26.8 million tweets into 11 categories with the best performing
classifier, we aggregate the overall distribution of Twitter chatter into percentages. We also show tweet examples from
each predicted category.

4

Results

UMAP visualization of LaBSE embeddings of the training data is depicted in Figure 2. Most visibly distinctive clusters
belong to categories Donate, Share, and Travel. In this study, a cumulative of 88 hours of GPU computation was
performed for extracting language-agnostic embeddings for the 26.8 million tweets which roughly corresponds to a
carbon footprint of 9.5 kgCO2 eq (estimate by following [80]).
10-fold cross-validation results for the classifiers with the highest scoring set of hyperparameters are shown in Table
3. For all classifiers, LaBSE embeddings outperform multilingual BERT embeddings. Best hyperparameters for
k-nearest neighbour classifier were found to be k = 7 and cosine distance for both embeddings. Optimal regularization
coefficients for logistic regression was found to be 4.94 × 103 for LaBSE and 1.01 × 102 for BERT representations. For
support vector machine, optimal C was found to be 5.07 and 3.51 for LaBSE and BERT representations, respectively.
Best choice of kernel function was found to be radial basis function for both representations. Best performing classifier
was found to be support vector machine classifier applied on LaBSE embeddings with 86.92 % accuracy, 0.876
micro-F1 score, and 0.881 macro-F1 score. Confusion matrix of this classifier out of cross-validation predictions can be
examined from Figure 3. In parallel to visual findings on Figure 2, Donate, Share, and Travel classes reach high
accuracies of 97.1 %, 98.1 %, and 94.0 %, respectively. Classifier has the highest error rate for the Prevention and
Speculation classes, both staying below 80 % accuracy. Our results show that more than 15 % of samples belonging
to Speculation category have been misclassified as Transmission.
Figure 4 depicts the timeline of normalized daily category distributions obtained by running inference on tweets posted
between 26 January and 5 April 2020. Transmission and travel-related chatter as well as speculations (opinions on origin
of COVID-19, myths, and conspiracies) show significance presence throughout the pandemic. What Is Corona?, i.e.
questions and inquiries regarding what exactly COVID-19 is, shows a presence in the early stages of the pandemic but
decreases through time, possibly due to gained scientific knowledge about the nature of the disease. On the contrary,
prevalence of Prevention related tweets increase through time especially after the declaration of pandemic by WHO
on March 11. Similarly, chatter for Donation discussions are observed only starting from March. Timeline curves
become smoother (less spiky) with increasing date as the percentage changes between consecutive days gets smaller.
This is intuitive as the total number of tweets in January is several magnitudes lower than that of April and sudden
percentage jumps in January can be attributed to only a handful of tweets. Finally, random samples of tweets and their
predicted labels can be observed from Table 4.
6

N OVEMBER 3, 2020

Donate

97.1

0.0

0.3

0.0

0.3

0.0

0.0

1.9

0.3

0.0

0.0

News & Press

0.0

91.9

0.0

2.9

0.3

0.3

0.0

0.6

1.3

0.0

2.6

Prevention

0.0

0.0

78.7

0.0

0.0

1.4

0.7

9.7

4.9

4.4

0.2

Reporting

0.0

2.3

1.3

82.3

0.0

4.9

0.5

5.1

1.8

1.3

0.5

Share

0.3

0.3

0.0

0.0

98.1

0.0

0.0

1.0

0.0

0.0

0.3

Speculation

0.0

0.8

2.2

2.2

0.0

75.2

0.3

15.4

0.0

2.2

1.7

Symptoms

0.0

0.6

1.4

0.3

0.0

0.3

85.3

6.9

0.3

0.0

4.9

Transmission

0.3

0.2

4.8

1.0

0.1

3.0

0.8

86.7

1.4

0.9

0.9

Travel

0.0

0.2

3.3

0.5

0.0

0.0

0.2

2.0

94.0

0.0

0.0

Treatment

0.0

0.3

5.2

0.5

0.0

1.3

0.5

3.4

0.8

86.9

1.0

What Is Corona?

0.3

3.5

0.3

1.3

0.3

1.9

3.9

5.8

0.0

2.3

80.3

- 60

%
- 40

- 20

Wha
t Is C
oron
a?

ent
Treat
m

l
Trave

missio
n
Trans

toms
Symp

tion
Specu
la

Share

rting
Repo

Preve
ntion

Press
New
s&

Dona

te

True Label

- 80

Predicted Label
Figure 3: Normalized confusion matrix of SVM classifier predictions across cross-validation folds.

5

Discussion

Adequate risk management in crisis situations has to take into account not only the threat itself but also the perception of
the threat by the public [81]. In digital era, public heavily relies on social media to inform their level of risk perception,
often in a rapid manner. In fact, social media enhances collaborative problem-solving and citizens’ ability to make
sense of the situation during disasters [4]. With this paradigm in mind, we attempt to perform large-scale classification
of 26.8 million COVID-19 tweets using natural language processing and machine learning. We utilize state-of-the-art
language-agnostic tweet representations coupled with simple, lightweight classifiers to be able to capture COVID-19
related discourse during a span of 13 weeks.
Our first observation of “increasing Twitter activity with increased COVID-19 spread throughout the globe” (Figure
1) is in parallel with other studies. For instance, Bento et al. show that Internet searches for “coronavirus” increase
on the day immediately after the first case announcement for a location [82]. Wong et al. correlates announcement of
new infections and Twitter activity [83]. Similar associations have been discovered between official cases and Twitter
activity by causal modeling as well [69]. Secondly, we show that language-agnostic embeddings can be utilized in an
out-of-the-box fashion, i.e., without requiring task-specific fine-tuning of BERT models. A SVM classifier reaches
86.92 % accuracy and 0.881 macro-F1 score for classification into 11 topic categories. Finally, we show that overall
public discourse shifts through the pandemic. Questions of “what coronavirus is” leave their place to donation and
prevention related discussions as the disease spreads into more and more countries especially during March 2020.
Tweets related to donation increase especially around 13 March 2020 when WHO and the United Nations Foundation
start a global COVID-19 donation fund [84].
When compared to existing studies that often employ unsupervised topic modeling, our approach tries to perform public
attention surveillance with a more automated perspective as we formulate the problem as a supervised learning one.
Topic modeling with LDA, which has been employed by majority of previous studies, relies on manual and qualitative
inspection of discovered topics. Furthermore, plain LDA fails to accommodate contextual representations and does not
7

N OVEMBER 3, 2020

Trends of classification of 26.8 million tweets between 26 January-5 April 2020
100
80

%

60
40
20
0
26
Jan

Feb

09

Feb

23

8

r0

Ma

2

r2

Ma

5
r0

Ap

Date
Donate
News & Press
Prevention
Reporting

Share
Speculation
Symptoms
Transmission

Travel
Treatment
What Is Corona?

Figure 4: Distribution of semantic discussion categories in Twitter predicted by the classifier during COVID-19.

assume a distance metric between discovered topics as it is based on the notion that words belonging to a topic are more
likely to appear in the same document. With language-agnostic embeddings, we also include tweets from languages
other than English to our analysis, hence decrease the selection bias.
Utilization of large-scale social media data for extracting health insights is even more pertinent during a global pandemic
such as COVID-19, as running randomized control trials becomes less practical. Moreover, traditional surveys for
public attention surveillance may further stress the participants whose mental health and overall well-being might have
been affected by lockdowns, associated financial issues, and changes in social dynamics [85, 86, 87]. Once accurate
estimation of global or national discourse is possible, social media can also be used to direct people to trusted resources,
counteract misinformation, disseminate reliable information, and enable a culture of preparedness [88]. Assessment of
effectiveness of public risk communication and interventions is also feasible with properly designed computational
systems. Guided by machine learning insights, some of these interventions can be made on social media itself.
Our study has several limitations. First, the training data consists of single label annotations while in reality a tweet can
have several topics simultaneously, e.g. Prevention and Travel. Secondly, we do not employ a confidence threshold
for categorizing tweets which forces our model to classify every observation into one of the 11 categories. Considering
some Twitter discourses related to COVID-19 may not be properly represented by our existing categories, a probability
threshold can be introduced for the final classification decision. Finally, we discard retweets in our analysis, which in
fact contributes to public attention on Twitter.
Future research includes running similar analysis for a more granular category set or sub-categories. For instance,
Speculation category can be divided into conspiracies related to origin of the disease, transmission characteristics,
and treatment options. Including up-to-date Twitter data (after April 2020) as well as extracting location-specific
insights will be performed in future analyses as well.

6

Conclusions

Transforming social media data into actionable knowledge for public health systems face several challenges such as
advancing methodologies to extract relevant information for health services, creating dynamic knowledge bases that
8

N OVEMBER 3, 2020

Tweet

Predicted Class

China Providing Assistance To Pakistani Students Trapped in
Wuhan: Ambassador - #Pakistan

Donate

Results are in. State health officials say three suspected cases
of Coronavirus have tested NEGATIVE. There is a forth possible case from Washtenaw County being sent to the CDC.

News & Press

what are good steps to protect ourselves from the Coronavirus?

Prevention

The first coronavirus case has been confirmed in the U.S.
#virus

Reporting

Share this and save lives #coronavirus #SSOT

Share

#coronavirus Don’t let these ignorant people make you believe
that this corona virus is any different than SARS IN 2003
which was contained after a few months. They want you to
panic as they have ulterior motives such as shorting the stock
market etc.

Speculation

I have a rushing sound in my ears. It doesn’t seem to match
the symptoms for the #coronavirus so perhaps it is the sound
of the #EU leaving my body...

Symptoms

what animals can carry Wuhan coronavirus?

Transmission

can we ban flights from wuhan pls?!?

Travel

¿Qué medicamento nos colará en está ocasión la industria
farmacéutica para combatir al coronavirus?

Treatment

Oque é coronavirus?
What Is Corona?
Table 4: Example tweets and predicted classification categories.

address disaster contexts, and expanding social media research to focus on health response activities [89]. We hope
our study serves this purpose by proving methodologies for large-scale, language-agnostic discourse classification on
Twitter.

References
[1] D Cucinotta and M Vanelli. Who declares covid-19 a pandemic. Acta Bio-medica: Atenei Parmensis, 91(1):157–
160, 2020.
[2] Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard to track covid-19 in real
time. The Lancet Infectious Diseases, 20(5):533–534, 2020.
[3] Elisabeth Mahase. Coronavirus: Covid-19 has killed more people than sars and mers combined, despite lower
case fatality rate, 2020.
[4] Manon Jurgens and Ira Helsloot. The effect of social media on the dynamics of (self) resilience during disasters:
A literature review. Journal of Contingencies and Crisis Management, 26(1):79–88, 2018.
[5] Jay J Van Bavel, Katherine Baicker, Paulo S Boggio, Valerio Capraro, Aleksandra Cichocka, Mina Cikara, Molly J
Crockett, Alia J Crum, Karen M Douglas, James N Druckman, et al. Using social and behavioural science to
support covid-19 pandemic response. Nature Human Behaviour, 4:460–471, 2020.
[6] Bao-Liang Zhong, Wei Luo, Hai-Mei Li, Qian-Qian Zhang, Xiao-Ge Liu, Wen-Tian Li, and Yi Li. Knowledge,
attitudes, and practices towards covid-19 among chinese residents during the rapid rise period of the covid-19
outbreak: a quick online cross-sectional survey. International journal of biological sciences, 16(10):1745, 2020.
[7] Alessio Signorini, Alberto Maria Segre, and Philip M Polgreen. The use of twitter to track levels of disease
activity and public concern in the us during the influenza a h1n1 pandemic. PloS One, 6(5), 2011.
9

N OVEMBER 3, 2020

[8] Xiang Ji, Soon Ae Chun, and James Geller. Monitoring public health concerns using twitter sentiment classifications. In IEEE International Conference on Healthcare Informatics, pages 335–344. IEEE, 2013.
[9] Xiang Ji, Soon Ae Chun, Zhi Wei, and James Geller. Twitter sentiment classification for measuring public health
concerns. Social Network Analysis and Mining, 5(1):13, 2015.
[10] Christopher Weeg, H Andrew Schwartz, Shawndra Hill, Raina M Merchant, Catalina Arango, and Lyle Ungar.
Using twitter to measure public discussion of diseases: a case study. JMIR Public Health and Surveillance,
1(1):e6, 2015.
[11] Liesbeth Mollema, Irene Anhai Harmsen, Emma Broekhuizen, Rutger Clijnk, Hester De Melker, Theo Paulussen,
Gerjo Kok, Robert Ruiter, and Enny Das. Disease detection or public opinion reflection? content analysis of
tweets, other social media, and online newspapers during the measles outbreak in the netherlands in 2013. Journal
of Medical Internet Research (JMIR), 17(5):e128, 2015.
[12] Sophie E Jordan, Sierra E Hovet, Isaac Chun-Hai Fung, Hai Liang, King-Wa Fu, and Zion Tsz Ho Tse. Using
twitter for public health surveillance from monitoring and prediction to public response. Data, 4(1):6, 2019.
[13] Hans Rosenberg, Shahbaz Syed, and Salim Rezaie. The twitter pandemic: the critical role of twitter in the
dissemination of medical information and misinformation during the covid-19 pandemic. Canadian Journal of
Emergency Medicine, 22(4):418–421, 2020.
[14] Emily Chen, Kristina Lerman, and Emilio Ferrara. Covid-19: The first public coronavirus twitter dataset. arXiv
preprint arXiv:2003.07372, 2020.
[15] Zhiwei Gao, Shuntaro Yada, Shoko Wakamiya, and Eiji Aramaki. Naist covid: Multilingual covid-19 twitter and
weibo dataset. arXiv preprint arXiv:2004.08145, 2020.
[16] Rabindra Lamsal. Corona virus (covid-19) tweets dataset, 2020.
[17] Norman Aguilar-Gallegos, Leticia Elizabeth Romero-García, Enrique Genaro Martínez-González, Edgar Iván
García-Sánchez, and Jorge Aguilar-Ávila. Dataset on dynamics of coronavirus on twitter. Data in Brief, 30:105684,
2020.
[18] Emily Chen, Kristina Lerman, and Emilio Ferrara. Tracking social media discourse about the covid-19 pandemic:
Development of a public coronavirus twitter data set. JMIR Public Health and Surveillance, 6(2):e19273, 2020.
[19] Alaa Abd-Alrazaq, Dari Alhuwail, Mowafa Househ, Mounir Hamdi, and Zubair Shah. Top concerns of tweeters
during the covid-19 pandemic: infoveillance study. Journal of Medical Internet Research, 22(4), 2020.
[20] H Raghav Rao, Naga Vemprala, Patricia Akello, and Rohit Valecha. Retweets of officials’ alarming vs reassuring
messages during the covid-19 pandemic: Implications for crisis management. International Journal of Information
Management, 55:102187, 2020.
[21] Han Woo Park, Sejung Park, and Miyoung Chong. Conversations and medical news frames on twitter: Infodemiological study on covid-19 in south korea. Journal of Medical Internet Research, 22(5):e18897, 2020.
[22] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-agnostic bert sentence
embedding. arXiv preprint arXiv:2007.01852, 2020.
[23] David Rushing Dewhurst, Thayer Alshaabi, Michael V Arnold, Joshua R Minot, Christopher M Danforth, and
Peter Sheridan Dodds. Divergent modes of online collective attention to the covid-19 pandemic are associated
with future caseload variance. arXiv preprint arXiv:2004.03516, 2020.
[24] Mike Thelwall and Saheeda Thelwall. Retweeting for covid-19: Consensus building, information sharing, dissent,
and lockdown life. arXiv preprint arXiv:2004.02793, 2020.
[25] Thayer Alshaabi, Joshua R Minot, Michael Vincent Arnold, Jane Lydia Adams, David Rushing Dewhurst,
Andrew J Reagan, Roby Muhamad, Christopher M Danforth, and Peter Sheridan Dodds. How the world’s
collective attention is being paid to a pandemic: Covid-19 related 1-gram time series for 24 languages on twitter.
arXiv preprint arXiv:2003.12614, 2020.
[26] Tymor Carpenter Hamamsy and Richard Bonneau. Twitter activity about treatments during the covid-19 pandemic:
case studies of remdesivir, hydroxychloroquine, and convalescent plasma. medRxiv, 2020.
10

N OVEMBER 3, 2020

[27] Lisa Singh, Shweta Bansal, Leticia Bode, Ceren Budak, Guangqing Chi, Kornraphop Kawintiranon, Colton Padden,
Rebecca Vanarsdall, Emily Vraga, and Yanchen Wang. A first look at covid-19 information and misinformation
sharing on twitter. arXiv preprint arXiv:2003.13907, 2020.
[28] Christian E Lopez, Malolan Vasu, and Caleb Gallemore. Understanding the perception of covid-19 policies by
mining a multilanguage twitter dataset. arXiv preprint arXiv:2003.10359, 2020.
[29] Ramez Kouzy, Joseph Abi Jaoude, Afif Kraitem, Molly B El Alam, Basil Karam, Elio Adib, Jabra Zarka, Cindy
Traboulsi, Elie W Akl, and Khalil Baddour. Coronavirus goes viral: Quantifying the covid-19 misinformation
epidemic on twitter. Cureus, 12(3), 2020.
[30] Philipp Wicke and Marianna M Bolognesi. Framing covid-19: How we conceptualize and discuss the pandemic
on twitter. PLoS ONE, 15(9):e0240010, 2020.
[31] Andrzej Jarynowski, Monika Wójta-Kempa, and Vitaly Belik. Trends in perception of covid-19 in polish internet.
medRxiv, 2020.
[32] Catherine Ordun, Sanjay Purushotham, and Edward Raff. Exploratory analysis of covid-19 tweets using topic
modeling, umap, and digraphs. arXiv preprint arXiv:2005.03082, 2020.
[33] Richard J Medford, Sameh N Saleh, Andrew Sumarsono, Trish M Perl, and Christoph U Lehmann. An "Infodemic":
Leveraging High-Volume Twitter Data to Understand Early Public Sentiment for the Coronavirus Disease 2019
Outbreak. Open Forum Infectious Diseases, 7(7), 6 2020.
[34] Long Chen, Hanjia Lyu, Tongyu Yang, Yu Wang, and Jiebo Luo. In the eyes of the beholder: Sentiment and topic
analyses on social media use of neutral and controversial terms for covid-19. arXiv preprint arXiv:2004.10225,
2020.
[35] Matteo Cinelli, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valensise, Emanuele Brugnoli,
Ana Lucia Schmidt, Paola Zola, Fabiana Zollo, and Antonio Scala. The covid-19 social media infodemic.
Scientific Reports, 10:16598, 2020.
[36] Pedram Hosseini, Poorya Hosseini, and David A Broniatowski. Content analysis of persian/farsi tweets during
covid-19 pandemic in iran using nlp. arXiv preprint arXiv:2005.08400, 2020.
[37] Hyeju Jang, Emily Rempel, Giuseppe Carenini, and Naveed Janjua. Exploratory analysis of covid-19 related
tweets in north america to inform public health institutes. arXiv preprint arXiv:2007.02452, 2020.
[38] Muhammad Saad, Muhammad Hassan, and Fareed Zaffar. Towards characterizing the covid-19 awareness on
twitter. arXiv preprint arXiv:2005.08379, 2020.
[39] Michelle Odlum, Hwayoung Cho, Peter Broadwell, Nicole Davis, Maria Patrao, Deborah Schauer, Michael E
Bales, Carmela Alcantara, and Sunmoo Yoon. Application of topic modeling to tweets as the foundation for health
disparity research for covid-19. Studies in health technology and informatics, 272:24–27, 2020.
[40] Sungkyu Park, Sungwon Han, Jeongwook Kim, Mir Majid Molaie, Hoang Dieu Vu, Karandeep Singh, Jiyoung
Han, Wonjae Lee, and Meeyoung Cha. Risk communication in asian countries: Covid-19 discourse on twitter.
JMIR Preprints, 2020.
[41] Jia Xue, Junxiang Chen, Ran Hu, Chen Chen, ChengDa Zheng, and Tingshao Zhu. Twitter discussions and
concerns about covid-19 pandemic: Twitter data analysis using a machine learning approach. JMIR Preprints,
2020.
[42] Raj Kumar Gupta, Ajay Vishwanath, and Yinping Yang. Covid-19 twitter dataset with latent topics, sentiments
and emotions attributes. arXiv preprint arXiv:2007.06954, 2020.
[43] Xueting Wang, Canruo Zou, Zidian Xie, and Dongmei Li. Public opinions towards covid-19 in california and new
york on twitter. medRxiv, 2020.
[44] Yunhe Feng and Wenjun Zhou. Is working from home the new norm? an observational study based on a large
geo-tagged covid-19 twitter dataset. arXiv preprint arXiv:2006.08581, 2020.
[45] Hui Yin, Shuiqiao Yang, and Jianxin Li. Detecting topic and sentiment dynamics due to covid-19 pandemic using
social media. arXiv preprint arXiv:2007.02304, 2020.
11

N OVEMBER 3, 2020

[46] Liz McQuillan, Erin McAweeney, Alicia Bargar, and Alex Ruch. Cultural convergence: Insights into the behavior
of misinformation networks on twitter. arXiv preprint arXiv:2007.03443, 2020.
[47] Yuka Omoya and Muneo Kaigo. Suspicion begets idle fears - an analysis of covid-19 related topics in japanese
media and twitter. Available at SSRN 3585532, 2020.
[48] Karishma Sharma, Sungyong Seo, Chuizheng Meng, Sirisha Rambhatla, Aastha Dua, and Yan Liu. Coronavirus
on social media: Analyzing misinformation in twitter conversations. arXiv preprint arXiv:2003.12309, 2020.
[49] Md Kabir, Sanjay Madria, et al. Coronavis: A real-time covid-19 tweets analyzer. arXiv preprint arXiv:2004.13932,
2020.
[50] Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Gershman, and Robert Frederking. Topical clustering of tweets.
Proceedings of the ACM SIGIR: SWSM, 63, 2011.
[51] Shakira Banu Kaleel and Abdolreza Abhari. Cluster-discovery of twitter messages for event detection and trending.
Journal of Computational Science, 6:47–57, 2015.
[52] Siaw Ling Lo, Raymond Chiong, and David Cornforth. An unsupervised multilingual approach for online social
media topic identification. Expert Systems with Applications, 81:282–298, 2017.
[53] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In International conference
on machine learning, pages 1188–1196, 2014.
[54] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation.
In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages
1532–1543, 2014.
[55] Soroush Vosoughi, Prashanth Vijayaraghavan, and Deb Roy. Tweet2vec: Learning tweet embeddings using
character-level cnn-lstm encoder-decoder. In Proceedings of the 39th International ACM SIGIR conference on
Research and Development in Information Retrieval, pages 1041–1044, 2016.
[56] Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, and William Cohen. Tweet2vec: Characterbased distributed representations for social media. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), pages 269–274, 2016.
[57] Jie Liu, Zhicheng He, and Yalou Huang. Hashtag2vec: learning hashtag representation with relational hierarchical
embedding model. In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages
3456–3462, 2018.
[58] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227–2237, 2018.
[59] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pages 4171–4186, 2019.
[60] Oguzhan Gencoglu. Deep representation learning for clustering of health tweets. arXiv preprint arXiv:1901.00439,
2018.
[61] Jian Zhu, Zuoyu Tian, and Sandra Kübler. UM-IU@LING at SemEval-2019 task 6: Identifying offensive tweets
using BERT and SVMs. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages
788–795, jun 2019.
[62] Jishnu Ray Chowdhury, Cornelia Caragea, and Doina Caragea. Keyphrase extraction from disaster-related tweets.
In The world wide web conference, pages 1555–1566, 2019.
[63] Jishnu Ray Chowdhury, Cornelia Caragea, and Doina Caragea. On identifying hashtags in disaster twitter data. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 498–506, 2020.
[64] Kevin Roitero, Bozzato Cristian, Vincenzo Della Mea, Stefano Mizzaro, and Giuseppe Serra. Twitter goes to
the doctor: Detecting medical tweets using machine learning and BERT. In Proceedings of the International
Workshop on Semantic Indexing and Information Retrieval for Health from Heterogeneous Content Types and
Languages, volume 2619, 2020.
12

N OVEMBER 3, 2020

[65] Béatrice Mazoyer, Julia Cagé, Nicolas Hervé, and Céline Hudelot. A french corpus for event detection on twitter.
In Proceedings of The 12th Language Resources and Evaluation Conference, pages 6220–6227, 2020.
[66] Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. Bertweet: A pre-trained language model for english tweets.
arXiv preprint arXiv:2005.10200, 2020.
[67] Martin Müller, Marcel Salathé, and Per E Kummervold. Covid-twitter-bert: A natural language processing model
to analyse covid-19 content on twitter. arXiv preprint arXiv:2005.07503, 2020.
[68] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, 2019.
[69] Oguzhan Gencoglu and Mathias Gruber. Causal modeling of twitter activity during covid-19. Computation,
8(4):85, 2020.
[70] Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov, Ahmed Ali, James Glass, and Preslav
Nakov. What was written vs. who read it: News media profiling using text analysis and social media context. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.
[71] Hyunuk Kim and Dylan Walker. Leveraging volunteer fact checking to identify misinformation about covid-19 in
social media. Harvard Kennedy School Misinformation Review, 1(3), 2020.
[72] Oguzhan Gencoglu. Cyberbullying detection with fairness constraints. IEEE Internet Computing, 2020.
[73] Juan M. Banda, Ramya Tekumalla, Guanyu Wang, Jingyuan Yu, Tuo Liu, Yuning Ding, and Gerardo Chowell. A
twitter dataset of 150+ million tweets related to covid-19 for open research, 2020.
[74] Covid-19 twitter chatter dataset for scientific use. http://www.panacealab.org/covid19/. Accessed: 202007-30.
[75] Abhinav Arora, Akshat Shrivastava, Mrinal Mohit, Lorena Sainz-Maza Lecanda, and Ahmed Aly. Cross-lingual
transfer learning for intent detection of covid-19 utterances. OpenReview preprint, 2020.
[76] Jerry Wei, Chengyu Huang, Soroush Vosoughi, and Jason Wei. What are people asking about covid-19? a question
classification dataset. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020. Association for
Computational Linguistics, jul 2020.
[77] Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine Learning,
pages 63–71. Springer, 2003.
[78] Jonas Močkus. On bayesian methods for seeking the extremum. In Optimization Techniques IFIP Technical
Conference, pages 400–404. Springer, 1975.
[79] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger. Umap: Uniform manifold approximation
and projection. Journal of Open Source Software, 3(29):861, 2018.
[80] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions
of machine learning. arXiv preprint arXiv:1910.09700, 2019.
[81] Peter M Sandman. Responding to community outrage: Strategies for effective risk communication. AIHA, 1993.
[82] Ana I Bento, Thuy Nguyen, Coady Wing, Felipe Lozano-Rojas, Yong-Yeol Ahn, and Kosali Simon. Evidence
from internet search data shows information-seeking responses to news of local covid-19 cases. Proceedings of
the National Academy of Sciences, 117(21):11220–11222, 2020.
[83] Catherine Mei Ling Wong and Olivia Jensen. The paradox of trust: perceived risk and public compliance during
the covid-19 pandemic in singapore. Journal of Risk Research, pages 1–10, 2020.
[84] Covid-19
solidarity
response
fund.
https://www.who.int/emergencies/diseases/
novel-coronavirus-2019/donate. Accessed: 2020-07-30.
[85] Cuiyan Wang, Riyu Pan, Xiaoyang Wan, Yilin Tan, Linkang Xu, Cyrus S Ho, and Roger C Ho. Immediate
psychological responses and associated factors during the initial stage of the 2019 coronavirus disease (covid-19)
epidemic among the general population in china. International Journal of Environmental Research and Public
Health, 17(5):1729, 2020.
13

N OVEMBER 3, 2020

[86] W Cullen, G Gulati, and BD Kelly. Mental health in the covid-19 pandemic. QJM: An International Journal of
Medicine, 113(5):311–312, 2020.
[87] Samantha K Brooks, Rebecca K Webster, Louise E Smith, Lisa Woodland, Simon Wessely, Neil Greenberg, and
Gideon James Rubin. The psychological impact of quarantine and how to reduce it: rapid review of the evidence.
The Lancet, 395:912–920, 2020.
[88] Raina M Merchant and Nicole Lurie. Social media and emergency preparedness in response to novel coronavirus.
Journal of the American Medical Association (JAMA), 323(20), 2020.
[89] Jennifer L Chan and Hemant Purohit. Challenges to transforming unconventional social media data into actionable
knowledge for public health systems during disasters. Disaster medicine and public health preparedness, pages
1–8, 2019.

14

