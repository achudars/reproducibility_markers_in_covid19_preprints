Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

Uncertainty quantification for epidemiological forecasts of
COVID-19 through combinations of model predictions

arXiv:2006.10714v2 [stat.AP] 29 Jun 2020

V. E. Bowman† , D. S. Silk† , U. Dalrymple† and D. C. Woods‡
† Defence Science and Technology Laboratory, Porton Down
‡ Statistical Sciences Research Institute, University of Southampton
June 30, 2020
The CrystalCast project is developing and implementing methodology to enhance, exploit and visualise scientific models in decision-making processes. One emphasis of the project is comprehensive uncertainty quantification
in epidemiological modelling, where a common statistical problem is prediction, or forecasting, in the presence of an
ensemble of multiple candidate models. For example, multiple candidate models may be available to predict case numbers in a disease epidemic, resulting from different modelling approaches (e.g. mechanistic or empirical) or differing
assumptions about spatial or age mixing. Alternative models capture genuine uncertainty in scientific understanding of
disease dynamics, and/or different simplifying assumptions underpinning each model derivation. While the analysis of
multi-model ensembles can be computationally challenging, accounting for this ‘structural uncertainty’ can improve
forecast accuracy and reduce the risk of over-estimated confidence [1,2] . A common approach is model averaging,
which tries to find an optimal combination of models in the space spanned by all individual models [3,4] . However, in
many settings this approach can fail dramatically: (i) the required marginal likelihoods (or equivalently Bayes factors)
can depend on arbitrary specifications for non-informative prior distributions of model-specific parameters; and (ii)
asymptotically the posterior model probabilities, used to weight predictions from different models, converge to unity
on the model closest to the “truth”. While this second feature may be desirable when the set of models under consideration contains the true model (the M-closed setting), it is less desirable in more realistic cases when the model set
does not contain the true data generator (M-complete and M-open). Here, this property of Bayesian model averaging
asymptotically choosing a single model can be thought of as a form of overfitting. For these latter settings, alternative methods of combining predictions from model ensembles may be preferred, for example, combining models via
combinations of individual predictive densities [5] . Combination weights can be chosen via application of predictive
scoring, as commonly applied in meteorological and economic forecasting [6,7] . In this paper we look at combining
epidemiological forecasts for COVID-19 daily deaths, hospital admissions, and hospital and ICU occupancy, in order
to improve the predictive accuracy of the short term forecasts.

1

Scoring forecasts

Forecast quality is often assessed via loss functions known as scoring rules [8,9] , that take a predictive density p and
outcome w as arguments. The scores are typically calculated using out-of-sample observations, e.g. future data. A
proper scoring rule ensures that minimum loss is obtained by choosing the data generating process as the predictive
density. Common rules include:
1. log score: Sl (p, w) = −logp(w);
2. continuous ranked probability score (CRPS): Sc (p, w) = E p (Y − w) − 12 E p (Y − Y 0 ), with Y,Y 0 ∼ p and having
finite first moment.
For deterministic predictions, i.e. p being a point mass density with support on ỹ, the CRPS reduces to the mean
absolute error S(ỹ, w) = |ỹ−w|, and hence this score can be used to compare probabilistic and deterministic predictions.
1

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

If only quantiles from p are available, alternative scoring rules include
3. quantile score: Sq,α (p, w) = 2(1{w < q} − α)(q − w) for quantile forecast q at level α ∈ (0, 1);
4. interval score: SI,α (p, w) = (u − l) + α2 (l − w)1{w < l} + α2 (w − u)1{w > u} for(l, u) being a central (1 − α) ×
100% prediction interval from p.
Quantile and interval scores can be averaged across available quantiles/intervals to provide a score for the predictive
density, while CRPS is the integral of the quantile score with respect to α.

2

Combinations of predictive distributions

Consider a model list M = (M1 , . . . , MK ), with the kth model having posterior predictive density pk (ỹ | y) for data
y = (y1 , . . . , yn )T . The ensemble methods considered here mostly have posterior predictive densities of the form
K

p(ỹ | y) =

∑ wk pk (ỹ | y) ,

(1)

k=1

where wk ≥ 0 weights the contribution of the kth model to the overall prediction, with ∑k wk = 1.
Giving equal weighting to each model has proved surprisingly effective in economic forecasting [7] . Alternatively,
given a score function S, weights can be chosen as
max
w

1 n
∑ Ŝ(p(ỹi | y), ỹi ) .
n i=1

(2)

Such approaches have been applied in economics [10] and are the essence of model stacking as described by Yao
et al. [11] . To avoid overfitting, the ỹi would typically not include the data used to train the individual models in the
ensemble, e.g. estimate Ŝ would be constructed using future data or cross-validation. Alternatively, scoring functions
can be used to construct normalised weights

f Ŝk
,
wk =
(3)
∑k f Ŝk
with Ŝk = Ŝ(pk (ỹi | y) being the estimated score for the kth model, and f being a monotonic function; with the logscore and f (·) = exp(·), Akaike Information Criterion (AIC) style weights are obtained [12] .
A number of scenarios can be envisaged for the application of such stacking methods to currently employed
epidemiological models:
1. the only model output available is a pre-determined set of predictive quantiles: (i) quantile or interval scoring
can be used to construct weights, with either individual quantiles directly included in a weighted sum (suitably
adjusted to achieve nominal coverage of each interval) or a (non)-parametric distribution approximated from
each set of quantiles and then included in Eq.1; or (ii) an upper-bound on CRPS scoring can be obtained using
the quantiles.
2. predictive distributions, or samples thereof, are available and hence direct mixing or stacking of predictive
distributions is possible. Ideally, leave-one-out predictions or sequential predictions of future data are available
to mitigate over-fitting. Alternatively, the rather confusingly named Bayesian model combination [13,14] could be
employed, where the ensemble is expanded to include linear combinations of models, e.g. mean predictions.
3. if the models themselves are available, stacking can be applied via estimation of predictive distributions (and
alternative methods of selecting the weights are possible [11] ). If the models are computationally expensive,
preventing the straightforward generation of predictive densities, then a hierarchical Bayesian model can be
applied with Gaussian process priors [15] assumed for each expensive model. Stacking can then be applied to
the resulting posterior predictive distributions, conditioning on model runs and data. This approach would be
preferred, as it provides a holistic treatment of the predictive uncertainty.
2

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

3

Regression-based forecast combination

The stacking methods described above attempt to calibrate the forecast ensemble by fitting a mixture distribution,
with components corresponding to the individual model forecast probability density functions (pdfs). Alternatively,
the calibrated forecasts can be generated from a regression model, with covariates corresponding to forecasts from
the model ensemble. Two such regression-based methods are currently under evaluation; Ensemble Model Output
Statistics [16] (EMOS) and Quantile Regression Averaging [17] (QRA).
EMOS defines a predictive model in the form of a Gaussian distribution:
y ∼ N(a + b1 ỹ1 + ... + bK ỹK + ε, c + dS2 ),

(4)

where ỹ1 , ..., ỹK are the forecasts from the individual models, c and d are non-negative coefficients, S is the ensemble
variance, and a and b1 , ..., bK are regression coefficients. Tuning of the coefficients is achieved by minimizing the
CRPS against training data.
QRA defines a predictive model for a given quantile, q, for the combined forecast as:
yq = b1 ỹ1q + ... + bK ỹKq ,

(5)

where yiq is the q-th reported quantile for model i, and fitting of the coefficients b1 , ...bK is achieved by minimizing
the quantile score of the predictions against training data.
Further details of the implementation of EMOS, QRA and stacking for short term forecasts are provided in Appendix A.

4

Comparing the performance of combined and individual forecasts

The performance of the different combined and individual forecasts has been monitored using the interval score (S̄)
averaged over the forecast window and the 0% (i.e. the quantile score for the median), 50% and 90% intervals, in
addition to the well-established assessment metrics; sharpness, bias and calibration [18] . Sharpness (s) is a measure of
prediction uncertainty, and is defined here as the average width of the 75% central predictive interval over the forecast
window. Bias (b) measures how likely a predictive model is to over or under predict, as the proportion of predictions for
which the reported median is greater than the data value. Calibration (c) quantifies the statistical consistency between
the predictions and data, via the proportion of predictions for which the data lies inside the 75% central predictive
interval.The bias and calibration scores were linearly transformed as b̂ = (0.5 − b)/0.5 and ĉ = (0.5 − c)/0.5, such
that a well-calibrated prediction with no bias or uncertainty corresponds to (b̂, ĉ, s) = c(0, 0, 0).

4.1

Results
Value types
death inc line
hospital inc
hospital prev
icu prev

Description
New daily deaths by date of death
New and newly confirmed patients in hospital
Hospital bed occupancy
ICU occupancy

Table 1: Value types (model outputs of interest) for which forecasts were scored.
For multiple sets of past forecasts, the three metrics were visualized on plots with axes corresponding to b̂ against
ĉ, and with plot point sizes defined by s. Calibration and bias were considered the most important criterion, while
sharpness was used to break ties when the calibration and bias scores were equal. Where both data and model predictions were available, the metrics were evaluated for the four value types (model outputs of interest) and eleven
regions/nations shown in Tables 1 and 2. Two scenarios were considered; firstly where it was required that a full
window of training predictions was available for each model to be included in the combination, and secondly, where

3

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

Nations
England
Scotland
Wales
Northern Ireland

Regions
London
East of England
Midlands
North East and Yorkshire
North West
South East
South West

Table 2: Nations/regions for which forecasts were scored.
models with incomplete training prediction windows were also included. The second scenario has been encountered
multiple times in practice.
Average score
(b) hospital inc

(a) death inc line

Model
QRA
Stacked: time invariant weights
EMOS
Stacked: time varying weights
Stacked: equal weights

p

ĉ2 + b̂2
0.80
0.81
0.9
0.93
0.91

Model
EMOS
QRA
Stacked: time invariant weights
Stacked: time varying weights
Stacked: equal weights

S̄
49
50
54
54
58

p

ĉ2 + b̂2
0.67
0.71
0.84
0.87
0.94

S̄
74
85
109
115
119

(d) icu prev

(c) hospital prev

p
p
Model
ĉ2 + b̂2 S̄
Model
ĉ2 + b̂2 S̄
Stacked: time invariant weights 1.02
488
QRA
0.98
106
Stacked: time varying weights
1.06
514
EMOS
1.08
126
Stacked: equal weights
1.04
541
Stacked: time invariant weights 1.29
190
QRA
1.09
630
Stacked: time varying weights
1.34
190
EMOS
1.17
1183
Stacked: equal weights
1.36
192
Table 3: Average distances from origin for the calibration-bias plots, and average interval scores for each value type,
averaged over regions/nations for each combination algorithm. The rows are ordered by increasing interval score, so
that the best performing models are at the top of the tables.

Both the sharpness, bias and calibration metrics and interval scores suggest that on average over nations/region,
the best performing forecasts (for the first scenario) originated from the QRA method for death inc line and icu prev,
the time-invariant weights stacking method for hospital prev, and EMOS for hospital inc (see Table 3).

4

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

Figure 1: Sharpness, bias and calibration scores for the (left) individual and (right) combined forecasts, for all regions
and value types delivered on (top) 11th May and (bottom) 4th May, 2020. Note that multiple points are hidden when
they coincide. The shading of the quadrants implies a preference for over-prediction to under-prediction, and for
prediction intervals that contain too many data points, rather than too few.

5

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

At finer resolution, the performance of both the combination and individual models was found to vary considerably
between different regions and value types (illustrated in Fig. 1, which overlays all the metric values for the (left panels)
individual and (right panels) combined forecasts for each forecasts delivered on (top) 11th and (bottom) 4th of May
2020 respectively). The best forecasting model was also highly variable across nations/regions and value types (as
shown for calibration and bias in Fig. 2), and was often an individual model.

Figure 2: The best performing model or combination method for each region/nation and value type (for forecasts
delivered on the 11th, 7th, 4th and 1st of May 2020), evaluated using the absolute distance from the origin on the
calibration-bias plots. Ties were broken using the sharpness score.
The variability in performance was particularly stark for QRA and EMOS, which were found in some cases to
vastly outperform the individual and stacked forecasts, and in others to substantially under-perform in comparison
to both individual models and other combinations algorithms (as shown in Fig. 3 below). In the former case, this
occurred when (e.g for occupied ICU beds in Scotland) all the individual model training predictions were highly
biased. In these cases, QRA and EMOS were able to correct the bias using non-convex combinations of regression
6

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

coefficients. Whether this behaviour is desirable depends upon whether the data is believed to be accurate, or itself
subject to large systematic biases, such as under-reporting, that is not accounted for within the model predictions
of the data. For the latter case of under-performance, this occurred in the presence of large discontinuities between
the individual model training predictions and forecasts, corresponding to changes in model structure or parameters.
This disruption to the learnt relationships between individual model predictions, and to the data (as captured by the
regression models), led to increased forecast bias (an example of this is shown in Fig. 4). Appendix A.4 introduces a
simple modification to the QRA algorithm that corrects for this bias. Promising initial results suggest that the modified
QRA algorithm substantially outperforms other combination methods for most value types.

Figure 3: Bar plots showing performance of model and combination method for each region/nation and valueptype (for
√
forecasts delivered on the 11th, 7th, 4th and 1st of May 2020). The height of each bar is calculated as 2 − ĉ2 + b̂2 ,
so that higher bars correspond to better performance.

7

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

Figure 4: QRA forecast for hospital prev in the North West for a forecast window beginning on the 14th May. A Large
discontinuity between the current and past forecasts (black line) of the individual model corresponding to the covariate
with the largest regression coefficient can lead to increased bias for the QRA algorithm. The median, 50% and 90%
QRA prediction intervals are shown in blue, while the data is shown in red.
In comparison, the relative performance of the stacking methods was more stable across different regions/nations
and value types (see Fig. 3), which likely reflects the conservative nature of the normalized weights in comparison to
the optimized regression coefficients. In terms of sharpness, the stacked forecasts were always outperformed by the
tight predictions of some of the individual models, and by the EMOS method.
It is clear from Figure 3 that while some models provided consistently high scoring forecasts for specific regions
and value types, the performance of the individual and combined models varied considerably, even over the limited
number of delivery dates considered. Further analysis of additional short term forecast data is required to establish the
extent to which the combination method or individual model identified as best performing is stable over time.
It is worth noting the delay in the response of the combination methods to changes in individual model performance. For example, the predictions from model two for death inc line in the South East of England improved
considerably to become the top performing model on the 1st of May. However, the model combination methods only
began to weight this model highly within the mixture/regression model on the 11th May. This behaviour arises from
the requirement for sufficient data to become available in order to detect the change in performance.
The results from the second scenario showed that under specific conditions, the naive equal weights mixture
combination can outperform the data driven methods. This occurs when a new model with well calibrated predictions
is introduced, or when the performance of an existing model changes substantially. For example, when initially
delivered, a model will be penalized by the data driven methods for lacking predictions for the full training window. In
comparison, the naive approach assigned the model equal weight, will result in improved performance if a new model
with very low bias is introduced.

8

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

5

Conclusions

Scoring rules are a natural way of constructing weights for the combination of predictions from an ensemble of
multiple models. Weighting via scores has the advantages over Bayesian model averaging of being directly tailored
to approximate a predictive distribution and less sensitive to choice of prior distributions, and overcomes the problem
of model averaging converging, as sample size increases, to a predictive density from a single model, even when the
ensemble of models does not include the true data generator. Some guidance on which situations each method is
appropriate for is given by Höge et al. [19] .
Prediction from multi-model ensembles has a long history of being addressed in climate modelling [6,20,21] . Here,
it is typical to make assumptions about the underlying distribution to which the models belong, e.g. that they can be
approximated by a common regression form or are (co-) exchangeable.
Several methods (stacking, EMOS and QRA) to combine epidemiological forecasts have been investigated, and
their performance evaluated using the well-established sharpness, bias and calibration metrics as well as the interval
score. When averaged over nations/regions, the best performing forecasts according to both the metrics and interval
score, originated from the QRA method for death inc line and icu prev, the time-invariant weights stacking method
for hospital prev, and EMOS for hospital inc. However, the performance metrics for each model and combination
method were found to vary considerably over the different regions and value type combinations. This suggests that
the optimal choice of method for combining short term forecasts will be different for each region and value type.
Whilst some models were observed to perform consistently well for particular region and value type combinations, the
extent to which the best performing models remain stable over time will require further investigation using additional
forecasting data.
The rapid evolution of the models (through changes in both parameterization and structure) during the COVID19 outbreak have led to substantial changes in each models’ predictive performance over time. This represents a
significant challenge for combination methods that essentially use a model’s past performance to predict its future
performance, and has resulted in cases where the combined forecasts do not represent an improvement to the individual
model forecasts. For the stacking approaches, this challenge could be overcome by either (a) additionally providing
quantile predictions from the latest version of the models for (but not fit to) data points within a training window, or
(b) sampled trajectories from the posterior predictive distribution for the latest model at data points that have been
used for parameter estimation. Option (a) would allow direct application of the current algorithms to the latest models
but may be complicated by addition of model structure due to e.g. changes in control measures, whilst (b) would
enable application of full Bayesian stacking approaches using, for example, leave-one-out cross validation [11] . For
the regression approaches, a simple modification to the QRA algorithm (detailed in Appendix A.4) appears to be able
to correct the prediction bias associated with changes to the model structure or parameters. Promising initial results
suggest that the modified QRA algorithm can substantially outperform other combination methods for most value
types.

6

Recommendations
• Forecasts for each region/nation and value type should be selected independently from the available combination
algorithms, as sufficient evidence from their most recent calibration, sharpness and bias scores accumulates.
• The performance of the various combination algorithms should continue to be monitored. The stability of the
performance of the different individual models and algorithms should be assessed using additional forecasts as
they become available.
• The current choice of algorithm for each region/nation and value type should be assessed on a regular basis and
updated as appropriate.
• When forecasts from a new model (or from an existing model for a new region/nation or value type), with no
corresponding training predictions, the equal-weights mixture algorithm should be included for consideration.
• The performance of mixture-based combination methods with optimized weights should be investigated.

9

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

• Further research should be conducted into modified regression based combination methods that are able to adjust
to changes in the underlying models. The performance of the modified QRA algorithm should be monitored
alongside the existing suite of combination algorithms.
• The EMOS method should be further developed to incorporate information from the full set of reported quantiles.
• The viability of stacking approaches whose training predictions originate from up-to-date versions of the models
should be investigated and implemented when available.
• When the performance of the forecast combination methods for each region and value type has stabilized, Dstl
will make a recommendation on a preferred method for each region and value type, for agreement by SPI-M.

Acknowledgments
This work has greatly benefited from parallel research led by Sebastian Funk (LSHTM), and was conducted within
a wider effort to improve the policy response to COVID-19. The authors would like to thank the SPI-M modelling
groups for the data used in this paper. We would also like to thank Tom Finnie at PHE and the SPI-M secretariat for
their helpful discussions.

(c) Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government License
except where otherwise stated. To view this license, visit http://www.nationalarchives.gov.uk/doc/open-governmentlicence/version/3 or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email:
psi@nationalarchives.gsi.gov.uk

10

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

References
[1] M. A. Semenov and P. Stratonovitch. Use of multi-model ensembles from global climate models for assessment
of climate change impacts. Climate research, 41:1–14, 2010.
[2] A. E. Raftery, D. Madigan, and J. A. Hoeting. Bayesian model averaging for linear regression models. Journal
of the American Statistical Association, 92:179–191, 1997.
[3] D. Madigan, A. E. Raftery, C. Volinsky, and J. Hoeting. Bayesian model averaging. In Proceedings of the AAAI
Workshop on Integrating Multiple Learned Models, Portland, OR, pages 77–83, 1996.
[4] J. A. Hoeting, D. Madigan, A. E. Raftery, and C. T. Volinsky. Bayesian model averaging: a tutorial. Statistical
Science, pages 382–401, 1999.
[5] K. F. Wallis. Combining density and interval forecasts: a modest proposal. Oxford Bulletin of Economics and
Statistics, 67:983–994, 2005.
[6] T. Gneiting, A. E. Raftery, A. H. Westveld III, and T. Goldman. Calibrated probabilistic forecasting using
ensemble model output statistics and minimum CRPS estimation. Monthly Weather Review, 133:1098–1118,
2005.
[7] C. McDonald and L. A. Thorsrud. Evaluating density forecasts: model combination strategies versus the RBNZ.
Technical Report DP2011/03, Reserve Bank of New Zealand, 2011.
[8] T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction and estimation. Journal of the American
Statistical Association, 102:359–378, 2007.
[9] T. Gneiting and R. Ranjan. Comparing density forecasts using threshold- and quantile-weighted scoring rules.
Journal of Business and Economic Statistics, 29:411–422, 2011.
[10] A. Opschoor, D. van Dijk, and M. van der Wel. Combining density forecasts using focused scoring rules. Applied
Econometrics, 32:1298–1313, 2017.
[11] Y. Yao, A. Vehtari, D. Simpson, and A. Gelman. Using stacking to average Bayesian predictive distributions
(with discussion). Bayesian Analysis, 13:917–1007, 2018.
[12] K. P. Burnham and D. R. Anderson. Model Selection and Multimodel Inference. Springer, New York, 2nd edition,
2002.
[13] T. P. Minka. Bayesian model averaging is not model combination. Available electronically at http://www. stat.
cmu. edu/minka/papers/bma. html, pages 1–2, 2000.
[14] K. Monteith, J. L. Carroll, K. Seppi, and T. Martinez. Turning Bayesian model averaging into Bayesian model
combination. In The 2011 International Joint Conference on Neural Networks, pages 2657–2663. IEEE, 2011.
[15] M. C. Kennedy and A. O’Hagan. Bayesian calibration of computer models (with discussion). Journal of the
Royal Statistical Society B, 63:425–464, 2001.
[16] Tilmann Gneiting, Adrian E Raftery, Anton H Westveld III, and Tom Goldman. Calibrated probabilistic forecasting using ensemble model output statistics and minimum crps estimation. Monthly Weather Review, 133(5):
1098–1118, 2005.
[17] Jakub Nowotarski and Rafał Weron. Computing electricity spot price prediction intervals using quantile regression and forecast averaging. Computational Statistics, 30(3):791–803, 2015.
[18] Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration and sharpness.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):243–268, 2007.

11

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

[19] M. Höge, A. Guthke, and W. Nowak. The hydrologist’s guide to Bayesian model selection, averaging and
combination. Journal of Hydrology, 572:96–107, 2019.
[20] J. Rougier, M. Goldstein, and L. House. Second-order exchangeability analysis for multimodel ensembles.
Journal of the American Statistical Association, 108:852–863, 2013.
[21] R. E. Chandler. Exploiting strength, discounting weakness: combining information from multiple climate simulators. Philosophical Transactions of the Royal Society A, 371:20120388, 2013.
[22] James Kennedy and Russell Eberhart. Particle swarm optimization. In Proceedings of ICNN’95-International
Conference on Neural Networks, volume 4, pages 1942–1948. IEEE, 1995.

A
A.1

Implementation details
Stacking

The time invariant stacking weights (wk ) for model k was calculated as the normalized sum of the reciprocal quantile
scores (normalized for each data point) for a Tp = 20 day window of past quantile predictions and data,
Tp

wk =

∑i=1 λ Tp −i S̄1

ik

1

,

(6)

∑i,k S̄ik

where S̄ik = ∑SikS , Sik is taken as the sum of quantile scores for model k’s quantile predictions for data point i. Use
k ik
of the exponential decay term (with λ = 0.9), means that more recent observations have more impact on the stacking
weights.
Given the negatively oriented quantile scoring function, constructing the weights as a normalized sum of reciprocals allowed models with a shorter window of predictions to be included with a naturally penalized weight. Where a
subset of prediction quantiles were missing for given time point in the training window, they were estimated by fitting
skewed-normal distributions to the available quantiles.
A simple implementation of time-varying weights was also implemented by exponential interpolation between the
time-invariant stacking weights (one-day ahead) and equal weights (at the end of the forecast window, 14 days ahead).

A.2

EMOS

The covariates, ỹ1 , ...ỹK in Eq. 4, were taken as the median predictions from the individual models. The ensemble
standard deviation is estimated as the standard deviation of the median predictions. Dstl is investigating how to
utilize more of the information provided by the reported quantiles within the EMOS method. Fitting of the regression
coefficients was achieved using a Particle Swarm optimization routine [22] , with the coefficients constrained to nonnegative values (this version of the algorithm is known as EMOS+ [16] ), and with the intercept set to zero in order to
force the combination to use the model predictions.

A.3

QRA

The covariates for quantile q in Eq. 5, were taken as the individual model predictions for quantile q, with coefficient
values assumed to be fixed across the different quantiles. Fitting of the regression coefficients was achieved using a
Particle Swarm optimization routine, with coefficients constrained to be non-negative.

12

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

A.4

SQRA

In Section 4.1 it was observed that large discrepancies between the past and present forecasts for an individual model
could lead to increased bias in the QRA and EMOS combined forecasts. Overlapping of past and present forecasts
allows this discrepancy to be characterized. Dstl is investigating how this information might used to improve combined
forecast performance. Initially, a simple modification to the QRA algorithm has been implemented, that translates the
covariates at time t (ỹkq (t)) in the regression model, such the values of ỹk0.5 (t0 ) match the training predictions for the
start of the forecast window, t0 . For cases where the discrepancy is large (e.g. Fig. 4), the reduction in bias of shifted
QRA (SQRA) over QRA is striking (see Fig. 5).

Figure 5: QRA (blue) and SQRA (green) forecasts for hospital prev in the North West for a forecast window beginning
on the 14th May. SQRA corrects for the discontinuity between past and current forecasts for the individual model
(black line) that corresponds to the covariate with the largest coefficient. Data is shown in red.
Table 4 shows the average interval scores for each combination algorithm for forecasts delivered on the 18th May
2020. For hospital inc, hospital prev and icu prev, the SQRA algorithm has considerably lower average interval scores
than any other algorithm. The cases where QRA slightly outperforms SQRA on average, appear to correspond to
training predictions that are drawn from multiple deliveries over which the evolving models’ forecasts are particularly
volatile. Dstl are investigating how to choose the magnitude of the shift sensibly in these situations.

13

Content includes material subject to c Crown copyright (2020), Dstl. This material is licensed under the terms of the Open Government Licence
except where otherwise stated. To view this licence, visit http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to
the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: psi@nationalarchives.gov.uk.

Average score
(b) hospital inc

(a) death inc line

Model
QRA
Stacked:time varying
SQRA
Stacked:equal weights
Stacked:time invariant
EMOS

Model
SQRA
Stacked:time invariant
Stacked:time varying
Stacked:equal weights
QRA
EMOS

S̄
49.4
57.7
58.4
59.4
60.6
79.0

(c) hospital prev

S̄
66.5
103.0
107.0
110.0
116.3
118.5

(d) icu prev

Model
S̄
Model
S̄
SQRA
422.8
SQRA
53.9
EMOS
443.6
QRA
103.4
Stacked:time invariant 467.6
Stacked:time invariant 159.7
QRA
504.2
Stacked:time varying
177.4
Stacked:time varying
508.2
Stacked:equal weights 177.4
Stacked:equal weights 528.6
EMOS
274.2
Table 4: Interval scores for each value type, averaged over regions/nations for each model and combination algorithm
for forecasts from the 18th May 2020.

14

