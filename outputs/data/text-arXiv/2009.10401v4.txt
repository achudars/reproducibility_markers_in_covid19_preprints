JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Dynamic Fusion-based Federated Learning for
COVID-19 Detection

arXiv:2009.10401v4 [cs.DC] 26 Oct 2020

Weishan Zhang∗ , Tao Zhou∗ , Qinghua Lu† , Xiao Wang‡ , Chunsheng Zhu§ ,
Haoyun Sun∗ , Zhipeng Wang∗ , Sin Kit Lo† , Fei-Yue Wang‡
∗ College of Computer Science and Technology, China University of Petroleum (East China), China
† Data61, CSIRO, Australia
‡ State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese
Academy of Sciences, Beijing, China
§ SUSTech Institute of Future Networks, Southern University of Science and Technology, China

Abstract—Medical diagnostic image analysis (e.g., CT scan
or X-Ray) using machine learning is an efficient and accurate
way to detect COVID-19 infections. However, sharing diagnostic
images across medical institutions is usually not allowed due
to the concern of patients’ privacy. This causes the issue of
insufficient datasets for training the image classification model.
Federated learning is an emerging privacy-preserving machine
learning paradigm that produces an unbiased global model
based on the received updates of local models trained by clients
without exchanging clients’ local data. Nevertheless, the default
setting of federated learning introduces huge communication cost
of transferring model updates and can hardly ensure model
performance when data heterogeneity of clients heavily exists.
To improve communication efficiency and model performance, in
this paper, we propose a novel dynamic fusion-based federated
learning approach for medical diagnostic image analysis to
detect COVID-19 infections. First, we design an architecture
for dynamic fusion-based federated learning systems to analyse
medical diagnostic images. Further, we present a dynamic fusion
method to dynamically decide the participating clients according
to their local model performance and schedule the model fusionbased on participating clients’ training time. In addition, we
summarise a category of medical diagnostic image datasets for
COVID-19 detection, which can be used by the machine learning
community for image analysis. The evaluation results show that
the proposed approach is feasible and performs better than
the default setting of federated learning in terms of model
performance, communication efficiency and fault tolerance.
Index Terms—Federated learning, machine learning, image
processing, classification, COVID-19, architecture, AI, CT, XRay.

I. I NTRODUCTION

T

HE COVID-19 pandemic has introduced an unprecedented global crisis. The rapidly increasing number of
COVID-19 cases leads to a severe shortage of test kits and
calls for a more efficient and accurate way to diagnose
COVID-19 infections. To address the issue of the shortage
of test kits for COVID-19 diagnosis, researchers have been
working on machine learning technologies, especially deep
learning, using medical diagnostic images (e.g., CT scan or
X-Ray). The model performance is heavily dependent on the
training dataset size and diversity. However, data hungriness
Weishan Zhang, Qinghua Lu, and Chunsheng Zhu are the corresponding authors. Email: zhangws@upc.edu.cn, qinghua.lu@data61.csiro.au, chunsheng.tom.zhu@gmail.com

is a critical challenge due to the concern for data privacy. To
protect patients’ privacy, sharing medical data across medical
institutions is not allowed, which causes the issue of insufficient datasets for model training.
The concept of federated learning was introduced by Google
in 2016 as a new machine learning paradigm that produces an
unbiased model while preserving data privacy [1], [2]. In each
round of training, clients (e.g., organisations, data centers, or
mobile/IoT devices) are selected to train a model using local
data and send the updates of local models to a central server
for aggregation without transferring any local raw data.
Federated learning has the potential to connect isolated
medical institutions and train a model for COVID-19 positive
case detection while preserving data privacy. Some recent
works leverage federated learning to diagnose COVID-19
infection through CT or X-Ray images [3], [4]. However,
the above studies adopted the default setting of federated
learning which might introduce huge communication cost of
transferring model updates (e.g. massive matrices of weights)
and under-performs when data heterogeneity of clients heavily
exists.
To improve communication efficiency and model performance, we propose a novel dynamic fusion-based federated
learning approach for COVID-19 positive case detection. First,
we design a dynamic fusion-based federated learning system
architecture for medical diagnosis image analysis to detect
COVID-19 positive cases. The proposed architecture provides
a systematic view of the components’ interactions and serves
as a guide for the design of federated learning systems.
Second, we present a dynamic fusion method to decide the
participating clients according to their local model performance and schedule the model fusion dynamically, based on
the participating clients’ training time. Each client assesses
the local model trained and only uploads the model updates
when it performs better than the previous version while the
central server configures the waiting time for each client to
send model updates based on the average training time for
the last round. Additionally, we summarise a category of
medical diagnostic image datasets for COVID-19 detection,
which can be used by the machine learning community for
image analysis. The evaluation results show that the proposed
approach achieves better detection accuracy, fault tolerance,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Global model

Database of
local models

Predictor
Global
model

Local model

Database of
local data

2

Local model
assessor

Data

Data

Local data
pre-processor

Data

Local
model

Global model

Global model

Model
aggregator

Local model

Aggregation
request

Data

Global model
assessor

Aggregation
scheduler

Database of
global models

Training time

Local data
collector

Model trainer

Job

Job creator

Initial model

Data

Equipment

Client

Central server

Fig. 1: Architecture of federated learning systems for medical diagnostic image analysis
and communication efficiency compared to the default setting
of federated learning.
The remainder of this paper is organized as follow. Section
II presents the approach. Section III evaluates the approach.
Section IV discusses the related work. Section V concludes
the paper.
II. DYNAMIC F USION - BASED F EDERATED L EARNING FOR
COVID-19 D ETECTION
In this section, we present a dynamic fusion-based federated
learning approach for CT scan image analysis to diagnose
COVID-19 infections. Section II-A provides an overview of
the architecture and discusses how the components and their
interactions. Section II-B discusses a dynamic model fusion
method to dynamically decide the participating clients and
schedule the aggregation based on each participating client’s
training time.
A. Architecture
Fig. 1 illustrates the architecture, which consists of two
types of components: central server and clients. The central
server initialises a machine learning job and coordinates the
federated learning process, while clients train local models
specified in the learning job using local data and computation
resources.
Each client gathers images scanned by the diagnostic imaging equipment through the client data collector and cleans the
data (e.g., noise reduction) via the client data pre-processor
and store locally. The job creator initialises a model training
job (including initial model code and the number of aggregation) and configures the initial waiting time for clients to
return the model updates. Each participating client downloads
the job and trains the model via the model trainer. After a set
number of epochs, the model trainer completes this round of
training and uploads the training time to the central server.
The aggregation scheduler updates the waiting time based on
the training time received from participating clients.

The local model assessor on each client compares the performance of the current local model with the previous version.
If the current local model performs better, the client sends a
request for model upload to the central server. Otherwise, the
client will request to not upload the model update for this
round. All the clients that do not complete the set number
of epochs within the current waiting time are not allowed
to participate in this aggregation round. After the set waiting
time, the aggregation scheduler on the central server notifies
the clients that have sent the model upload request. After the
aggregation, the global model assessor measures the accuracy
of the aggregated global model and sends the global model
back to each client for a new round of training.

Algorithm 1 Dynamic fusion algorithm.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:

/*Client*/
Job ← download(ServerU RL)
F usionT imes, M odel ← decode(Job)
F edStep ← 0
while F edStep< F usionT imes do
Acc, T rainingT ime ← train(M odel)
send(T rainingT ime)
M axAcc ← request(ServerU RL)
if Acc ≥ M axAcc then
upload(M odel)
M odel ← receiveModel()
end if
F edStep + +
end while
/*Server*/
W aitingT ime, M axAcc, F usionT imes, M odel ← initialize()
Job ← encode(F usionT imes, M odel)
while true do
T rainingT ime ← receive()
W aitingT ime ← update(T rainingT ime)
ClientM odel ← receiveModel()
if expired(W aitingT ime) == true then
M odel ← aggregate(ClientM odel)
M axAcc ← evalate(M odel)
dispatch(M odel)
end if
end while

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Server

Client

Create a job

Download job from
server

Train the model

Receive training time

Assess model
accuracy

Send training time

Receive all client’s
training time?

No
Yes

Update waiting time

Perform better than
previous version?

Yes

No

Receive request

No

Receive all client’s
requests ?
Yes

Notify clients to send
model updates

Send request for
skipping this round
Send request for
uploading the model
updates

Upload local
model updates

Produce
global model

Assess global model

Distribute
global model

Download
global model
Reach convergence?

No

Yes

Fig. 2: Dynamic fusion process
B. Dynamic Fusion
To improve communication efficiency in federated learning,
the proposed dynamic fusion method consists of two decisionmaking points: client participation and client selection. On the
client side, each client decides whether to join this round of
aggregation based on the performance of the newly trained
model. On the central server side, the model aggregator selects
the participating clients based on the waiting time If a client
does not upload the model update within the waiting time, it
is excluded by the central server for this round of aggregation.
The waiting time of current the round is calculated by averaging the previous round’s training time of each client. The
initial waiting time is configured by the platform owner.
Fig. 2 illustrates the process of the proposed dynamic fusion
method, and Algorithm 1 describes the detailed process. The

process starts with creating a learning job by the central server.
All the clients download the job from the central server and
set up the local training environment. From the second round,
a timer is set for each client based on the average training
time of all the participating clients for the previous round. If
a client does not complete the training within the configured
time, the central server proceeds the aggregation without any
input from this client for this round. On the other hand, if
the model trained by the client for this round performs worse
than last round, the client sends a request to the central server
for skipping this round’s aggregation. Otherwise, the client
notifies the central server to update the model.
III. E VALUATION
Table I summarises a category of medical diagnostic image datasets for COVID-19 detection which include 746 CT

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

TABLE I: A Category of Medical Diagnostic Image Datasets for COVID-19 Detection.
Type

Amount

Size

COVID-19

Negative

VP

CT

746

92.6M

349

397

0

https://github.com/UCSD-AI4H/COVID-CT

X-ray

2905

1168M

219

1341

1345

https://www.kaggle.com/tawsifurrahman/covid19-radiography-database

X-ray

55

14.2M

55

0

0

https://github.com/agchung/Figure1-COVID-chestxray-dataset

TABLE II: Experiment environment.
Node

GPU

RAM

Python

CUDA

Server

RTX 2080Ti

11G

3.6

10.0

Client1

GTX 1070

8G

3.6

10.1

Client2

GTX 1080

8G

3.6

10.1

Client3

TITAN X(Pascal)

12G

3.7

10.1

images and 2960 X-ray images. The proposed approach is
evaluated via quantitative experiments using the datasets as
shown in Table I. The 746 CT dataset includes 349 images of
COVID-19 positive cases and 397 images of negative cases.
The chest X-ray images are from two datasets. The first X-ray
dataset has 2905 images which contain 219 images of COVID19 positive cases, 1341 images of negative cases, and 1345
images of viral pneumonia (VP). The second X-Ray dataset
consists of 55 images of positive cases.
As shown in Table II, the experiments involve one central
server and three clients with different configurations. We
selected 3326 images from the collected datasets and divided
them into 2800 images for the training set and 526 images for
the test set. We set different dataset sizes for each client: 600
images, 900 images, and 1300 images respectively. Considering the difference between CT and X-ray images, we adjusted
the ratio of these two types of images while keeping the same
total amount for each client, which is shown in Table III. In the
test set, there are 71 CT images (31 COVID-19 positive cases,
40 negative cases), and 455 X-ray images (55 COVID-19
positive cases, 200 negative cases, and 200 virus pneumonia).
Please note that the CT images are taken from the top, while
the X-Ray images are taken from the front.
A. Accuracy
To evaluate the accuracy of dynamic fusion-based federated
learning (DF FL), we conducted experiments using three
different models, GhostNet, ResNet50, and ResNet101. The
models were trained with the six groups of datasets listed
in Table III. There are 18 groups of experiments in total.
We compared the results with the default setting of federated
learning (D FL). GFL federated learning framework1 was
used in our experiments.
The results are presented in Fig. 3, Fig. 4, Fig. 5 respectively
for each type of model. The results show that in the 18 groups
of experiments, there are only 4 groups in which the dynamic
fusion-based federated learning (DF FL) achieves lower accuracy than the default setting of federated learning (D FL)
1 https://github.com/GalaxyLearning/GFL

Github Address

(lower than D FL by 1.711%, 0.57%, 0.57%, and 1.141%
respectively). 14 groups in which the dynamic fusion-based
federated learning (DF FL) achieves higher accuracy than
the default setting of federated learning (D FL). Overall, the
proposed dynamic fusion-based federated learning approach
achieved higher accuracy compared to the default setting of
federated learning. Also, an interference is introduced in the
4th group of the dataset for each model, where images of
negative cases are marked as positive COVID-19. The model
trained by fusion-based federated learning can still achieve
relatively steady results and higher accuracy compared to the
default setting, which shows that the proposed fusion-based
federated learning can ensure fault tolerance and robustness.
In addition, we measured the accuracy of each type of
model using the test set which was processed by random
cropping. The results are also shown in Fig. 3, Fig. 4, and
Fig. 5. Similarly, the results demonstrate that the proposed
dynamic fusion-based federated learning (DF FL) achieves
higher accuracy than the default setting of federated learning
(D FL) in 14 groups of experiments. For the rest, DF FL is
lower than D FL for 0.57%, 1.331%, 0.951%, and 1.141%
respectively The results show that the proposed fusion-based
federated learning perform better in real-world datasets than
the default setting of federated learning.
B. Training Time
To evaluate the training efficiency of the proposed dynamic
fusion-based federated learning, we recorded the training time
during the above experiments. The training epochs of the
clients are set to 90 and the maximum network speed is
configured as 10 MB/s for model upload/download (10MB/s).
The recorded training time is illustrated in Fig. 6. The results
show that in GhostNet, the proposed dynamic fusion-based
federated learning does not lower the training time, while
there is an apparent effect on ResNet50 and ResNet101. The
training time of ResNet50 is reduced by 8-10 minutes, while
the training time of ResNet101 is decreased by 25-30 minutes.
Since we found that the proposed dynamic fusion-based
federated learning cannot reduce the training time of GhostNet network in the above experiments, we further study the
influence factor. After measuring the single model transmission time, we observe that the GhostNet has less parameters
compared to the other two networks. Thus, GhostNet costs
less time for model transmission (which is 2.2s on average),
which results in no change in GhostNet training time. In
contrast, ResNet50 and ResNet101 have more parameters that
take more time to transmit the model updates. Thus, there
is an apparent improvement in these two networks in terms
of communication efficiency. We can conclude that applying

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

TABLE III: Dataset Configuration for Each Client.
Client 1

Data Size (MB)

Client 2

Data Size (MB)

Client 3

Data Size

Ratio

Total Data Size (MB)

600/0

76.8

0/900

391.3

0/1300

545.7

600/2200

1013.8

300/300

168.5

0/900

392.5

0/1300

546.6

300/2500

1107.6

200/400

196.8

0/900

389.1

0/1300

534.5

200/2600

1120.4

150/450

209.9

0/900

381.6

0/1300

544

150/2650

1135.5

200/400

197.4

200/700

318.9

0/1300

557.5

400/2400

1073.8

200/400

198.6

200/700

317.2

200/1100

497

600/2200

1012.8

Fig. 3: Accuracy of GhostNet.

Fig. 4: Accuracy of ResNet50.

Amount

2800

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

Fig. 5: Accuracy of ResNet101.
the proposed dynamic fusion-based approach can significantly
reduce the training time when the network is poor and the
model has large amounts of parameters.
C. Communication Efficiency
To evaluate the effect of dynamic fusion on communication,
we measure the upload number and upload time, which are
shown in Fig. 7 and 8 respectively. Here the collected upload
number and time are the total number of three clients, which
in our case is 30 for each client and 90 in total. In comparison
with the default setting of federated learning (D FL) for
GhostNet, the upload number of dynamic fusion is decreased
by an average of 61, matching to a reduction of 110-160s
of upload time (1/3 of the D FL time). For ResNet50, the
upload number of dynamic fusion decreased by an average
of 80, matching to a reduction of 900-1200s of upload time
(1/10 of the D FL time). For ResNet101, the upload number
of dynamic fusion decreased by an average of 78, matching to
a reduction of 3200-4200s on upload time (1/16 of the D FL
time).
Based on the results, we can conclude that dynamic fusion
is capable to reduce the communication overhead through less
model uploading. For models that have a simple structure and
few parameters as GhostNet, the reduction is not significant
(to only 1/3 of D FL). Nevertheless, dynamic fusion has more
obvious effects in treating complicated models with more
parameters (ResNet50 and ResNet101), which scale down to
1/10 and 1/16 of the D FL time.
IV. R ELATED W ORK
The concept of federated learning is first proposed by
Google in 2016 [1], which initially focuses on cross-device
learning. Google initially adopted federated learning to predict

search suggestions, next words and emojis, and the learning of
out-of-vocabulary words [5]–[7]. The scope of federated learning is then extended to cross-silo learning, e.g., for different
organisations or data centers [2], [8], [9]. For example, Sheller
et al. [10] build a segmentation model using brain tumor data
from different medical institutions.
Although communication efficiency can be improved by
only sending model updates instead of raw data, federated
learning systems requires multiple rounds of communications during training to achieve model convergence. Many
researchers work on the methods to reduce communication rounds [11], [12]. One way is through aggregation,
e.g., selective aggregation [13], aggregation scheduling [14],
asynchronous aggregation [15], temporally weighted aggregation [16], controlled averaging algorithms [17], iterative
round reduction [11], and shuffled model aggregation [18].
Furthermore, model compression methods are utilised to reduce the communication cost that occurs during the model
parameters and gradients exchange between clients and the
central server [19]. Additionally, communication techniques
are introduced to improve communication efficiency, e.g.,
over-the-air computation technique [20], multi-channel random access communication mechanism [21].
Federated learning can address statistical and system heterogeneity issues since models are trained locally [22]. However,
challenges still exist in dealing with non-IID data Many
researchers have worked on training data clustering [23],
multi-stage local training [24], and multi-task learning [22].
Also, some works [25], [26] focus on incentive mechanism
design to motivate clients to participate in the machine learning
jobs.
Federated learning has been recently adopted in CT or
X-Ray image processing for COVID-19 positive case detection [3], [4]. However, the above studies do not consider

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

R EFERENCES

(a) GhostNet

(b) ResNet50

(c) ResNet101

Fig. 6: Training time.

the communication efficiency and model accuracy issues of
federated learning. Our research work proposed a dynamic
fusion-based approach to improve communication efficiency
and model performance.
V. C ONCLUSION
This paper proposes a novel dynamic fusion-based federated
learning approach to improve accuracy and communication
efficiency while preserving data privacy for COVID-19 detection. The evaluation results show that the proposed approach
is feasible and performs better than the default setting of
federated learning in terms of model accuracy, fault tolerance,
robustness, and communication efficiency.

[1] H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas,
“Federated learning of deep networks using model averaging,” CoRR,
vol. abs/1602.05629, 2016. [Online]. Available: http://arxiv.org/abs/
1602.05629
[2] S. K. Lo, Q. Lu, C. Wang, H. Paik, and L. Zhu, “A systematic literature
review on federated machine learning: From a software engineering
perspective,” ArXiv, vol. abs/2007.11354, 2020.
[3] B. Liu, B. Yan, Y. Zhou, Y. Yang, and Y. Zhang, “Experiments of
federated learning for covid-19 chest x-ray images,” 2020.
[4] R. Kumar, A. A. Khan, S. Zhang, W. Wang, Y. Abuidris, W. Amin, and
J. Kumar, “Blockchain-federated-learning and deep learning models for
covid-19 detection using ct imaging,” 2020.
[5] T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong,
D. Ramage, and F. Beaufays, “Applied federated learning: Improving
google keyboard query suggestions,” CoRR, vol. abs/1812.02903, 2018.
[Online]. Available: http://arxiv.org/abs/1812.02903
[6] M. Chen, R. Mathews, T. Ouyang, and F. Beaufays, “Federated learning
of out-of-vocabulary words,” 2019.
[7] S. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays, “Federated
learning for emoji prediction in a mobile keyboard,” 2019.
[8] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N.
Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings et al.,
“Advances and open problems in federated learning,” arXiv preprint
arXiv:1912.04977, 2019.
[9] W. Zhang, Q. Lu, Q. Yu, Z. Li, Y. Liu, S. K. Lo, S. Chen, X. Xu, and
L. Zhu, “Blockchain-based federated learning for device failure detection
in industrial iot,” IEEE Internet of Things Journal, pp. 1–1, 2020.
[10] M. J. Sheller, G. A. Reina, B. Edwards, J. Martin, and S. Bakas, “Multiinstitutional deep learning modeling without sharing patient data: A
feasibility study on brain tumor segmentation,” in Brainlesion: Glioma,
Multiple Sclerosis, Stroke and Traumatic Brain Injuries, A. Crimi,
S. Bakas, H. Kuijf, F. Keyvan, M. Reyes, and T. van Walsum, Eds.
Cham: Springer International Publishing, 2019, pp. 92–104.
[11] J. Mills, J. Hu, and G. Min, “Communication-efficient federated learning
for wireless edge intelligence in iot,” IEEE Internet of Things Journal,
pp. 1–1, 2019.
[12] S. Silva, B. A. Gutman, E. Romero, P. M. Thompson, A. Altmann,
and M. Lorenzi, “Federated learning in distributed medical databases:
Meta-analysis of large-scale subcortical brain data,” in 2019 IEEE 16th
International Symposium on Biomedical Imaging (ISBI 2019), April
2019, pp. 270–274.
[13] J. Kang, Z. Xiong, D. Niyato, Y. Zou, Y. Zhang, and M. Guizani,
“Reliable federated learning for mobile networks,” IEEE Wireless Communications, vol. 27, no. 2, pp. 72–80, 2020.
[14] H. H. Yang, Z. Liu, T. Q. S. Quek, and H. V. Poor, “Scheduling
policies for federated learning in wireless networks,” IEEE Transactions
on Communications, vol. 68, no. 1, pp. 317–333, Jan 2020.
[15] C. Xie, S. Koyejo, and I. Gupta, “Asynchronous federated optimization,”
2019.
[16] T. Nishio and R. Yonetani, “Client selection for federated learning with
heterogeneous resources in mobile edge,” in ICC 2019 - 2019 IEEE
International Conference on Communications (ICC), May 2019, pp. 1–
7.
[17] S. P. Karimireddy, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and
A. T. Suresh, “Scaffold: Stochastic controlled averaging for on-device
federated learning,” arXiv preprint arXiv:1910.06378, 2019.
[18] B. Ghazi, R. Pagh, and A. Velingker, “Scalable and differentially private
distributed aggregation in the shuffled model,” 2019.
[19] L. WANG, W. WANG, and B. LI, “Cmfl: Mitigating communication
overhead for federated learning,” in 2019 IEEE 39th International
Conference on Distributed Computing Systems (ICDCS), July 2019, pp.
954–964.
[20] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via overthe-air computation,” IEEE Transactions on Wireless Communications,
pp. 1–1, 2020.
[21] J. Choi and S. R. Pokhrel, “Federated learning with multichannel aloha,”
IEEE Wireless Communications Letters, pp. 1–1, 2019.
[22] L. Corinzia and J. M. Buhmann, “Variational federated multi-task
learning,” arXiv preprint arXiv:1906.06268, 2019.
[23] F. Sattler, K.-R. Müller, and W. Samek, “Clustered federated learning:
Model-agnostic distributed multi-task optimization under privacy constraints,” arXiv preprint arXiv:1910.01991, 2019.
[24] Y. Jiang, J. Konečnỳ, K. Rush, and S. Kannan, “Improving federated learning personalization via model agnostic meta learning,” arXiv
preprint arXiv:1909.12488, 2019.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

[25] Y. J. Kim and C. S. Hong, “Blockchain-based node-aware dynamic
weighting methods for improving federated learning performance,” in
2019 20th Asia-Pacific Network Operations and Management Symposium (APNOMS), Sep. 2019, pp. 1–4.
[26] H. Kim, J. Park, M. Bennis, and S. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, pp. 1–1, 2019.

(a) GhostNet

(b) ResNet50

(c) ResNet101

Fig. 7: Upload number.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

(a) GhostNet

(b) ResNet50

(c) ResNet101

Fig. 8: Upload time.

9

