Dynamic Graph Convolutional Networks Using the Tensor M-Product
Osman Asif Malik∗

Shashanka Ubaru†

Lior Horesh‡

Misha E. Kilmer§

Haim Avron¶

arXiv:1910.07643v3 [cs.LG] 23 Jan 2021

Abstract
Many irregular domains such as social networks, financial
transactions, neuron connections, and natural language
constructs are represented using graph structures. In recent
years, a variety of graph neural networks (GNNs) have been
successfully applied for representation learning and prediction
on such graphs. In many of the real-world applications,
the underlying graph changes over time, however, most
of the existing GNNs are inadequate for handling such
dynamic graphs. In this paper we propose a novel technique
for learning embeddings of dynamic graphs using a tensor
algebra framework. Our method extends the popular graph
convolutional network (GCN) for learning representations
of dynamic graphs using the recently proposed tensor Mproduct technique. Theoretical results presented establish
a connection between the proposed tensor approach and
spectral convolution of tensors. The proposed method
TM-GCN is consistent with the Message Passing Neural
Network (MPNN) framework, accounting for both spatial and
temporal message passing. Numerical experiments on realworld datasets demonstrate the performance of the proposed
method for edge classification and link prediction tasks on
dynamic graphs. We also consider an application related to
the COVID-19 pandemic, and show how our method can be
used for early detection of infected individuals from contact
tracing data.

1

Introduction

Graphs are popular data structures used to effectively
represent interactions and structural relationships between entities in structured data domains. Inspired by
the success of deep neural networks for learning representations in the image and language domains, recently,
application of neural networks for graph representation
learning has attracted much interest. A number of graph
neural network (GNN) architectures have been explored
in the contemporary literature for a variety of graph
related tasks and applications [33, 30]. Methods based
∗ University

of Colorado Boulder, osman.malik@colorado.edu
Research, shashanka.ubaru@ibm.com
‡ IBM Research, lhoresh@us.ibm.com
§ Tufts University, misha.kilmer@tufts.edu
¶ Tel Aviv University, haimav@tauex.tau.ac.il
† IBM

Figure 1: Our proposed TM-GCN approach.
on graph convolution filters which extend convolutional
neural networks (CNNs) to irregular graph domains are
popular [4, 7, 12]. Most of these GNN models operate
on a given, static graph.
In many real-world applications, the underlying
graph changes over time, and learning representations
of such dynamic graphs is essential. Examples include
analyzing social networks [2], detecting fraud and crime
in financial networks [23], traffic control [32], understanding neuronal activities in the brain [6], and analyzing
contact tracing data [28]. In such dynamic settings, the
temporal interdependence in the graph connections and
features also play a substantial role. However, efficient
GNN methods that handle time varying graphs and that
capture the temporal correlations are lacking.
By dynamic graph, we refer to a sequence of graphs
G (t) = (V, A(t) , X(t) ), t ∈ {1, 2, . . . , T }, with a fixed set
V of N nodes, adjacency matrices A(t) ∈ RN ×N , and
(t)
graph feature matrices X(t) ∈ RN ×F where Xn: ∈ RF
is the feature vector consisting of F features associated
with node n at time t. The graphs can be weighted, and
directed or undirected. They can also have additional
properties like (time varying) node and edge classes,
which would be stored in a separate structure. Suppose
we only observe the first T 0 < T graphs in the sequence.
The goal of our method is to use these observations to
predict some property of the remaining T −T 0 graphs. In
this paper, we consider edge classification, link prediction
and node property prediction tasks.
In recent years, tensor constructs have been explored
to effectively process high-dimensional data, in order
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

to better leverage the multidimensional structure of
such data [13]. Tensor based approaches have been
shown to perform well in many applications. Recently,
a new tensor framework called the tensor M-product
framework [3, 10] was proposed that extends matrix
based theory to high-dimensional architectures.
In this paper, we propose a novel tensor variant
of the popular graph convolutional network (GCN)
architecture [12], which we call TM-GCN. It captures
correlation over time by leveraging the tensor M-product
framework. The flexibility and matrix mimeticability
of the framework, help us adapt the GCN architecture
to tensor space. Figure 1 illustrates our method at a
high level: First, the time varying adjacency matrices
A(t) and feature matrices X(t) of the dynamic graph
are aggregated into an adjacency tensor and a feature
tensor, respectively. These tensors are then fed into
our TM-GCN, which computes an embedding that can
be used for a variety of tasks, such as link prediction,
and edge and node classification. GCN architectures are
motivated by graph convolution filtering, i.e., applying
filters/functions to the graph Laplacian [4], and we
establish a similar connection between TM-GCN and
spectral filtering of tensors. Such results suggest possible
extensions of other convolution based GNNs such as [4, 7]
for dynamic graphs using the tensor framework. The
Message Passing Neural Network (MPNN) framework
has been used to describe spatial convolution GNNs [8].
We show that TM-GCN is consistent with the MPNN
framework, and accounts for spatial and temporal
message passing. Experimental results on real datasets
illustrate the performance of our method for the edge
classification and link prediction tasks on dynamic
graphs. We also demonstrate how TM-GCN can be
used in an important application related to the COVID19 pandemic. We show how GNNs can be used for early
identification of individuals who are infected (potentially
before they display symptoms) from contact tracing data
and a dynamic graph based SEIR model [28].
2

Related Work

Unsupervised Embedding: Unsupervised graph
embedding techniques have been popular for link prediction on static graphs [5]. A number of dynamic graph
embedding methods have been proposed recently, which
extend the static ones. DANE [16] adapted the popular
dimensionality reduction approaches such as Eigenmaps
to time varying graphs by efficiently updating the eigenvectors from the prior ones. The popular random walk
based methods have also been extended to obey the
temporal order in recent works [22].
Numerous deep neural network based unsupervised
learning methods have been developed for dynamic

graph embedding. Examples include DynGEM [9],
Know-Evolve [26], DyRep [27], Dynamic-Triad [34], and
others. In most of these methods, a temporal smoothness
regularization is used to obtain stable embedding across
consecutive time-steps.
Supervised Learning: The idea of using graph
convolution based on the spectral graph theory for
GNNs was first introduced by [4]. [7] then proposed
Chebnet, where the spectral filter was approximated
by Chebyshev polynomials in order to make it faster
and localized. [12] presented the simplified GCN, a
degree-one polynomial approximation of Chebnet, in
order to speed up computation further and improve the
performance. There are many other works that deal with
GNNs when the graph and features are fixed/static; see
the review papers [33] and [30] and references therein.
Recently, Li et. al [17] develop a diffusion convolutional RNN for traffic forecasting, where road networks
are modeled assuming both the nodes and edges remain
fixed over time, unlike in our setting. Seo et. al [25]
devise the Graph Convolutional Recurrent Network for
graphs with time varying features, while the edges are
fixed over time. EdgeConv was proposed in [29], which is
a neural network (NN) approach that applies convolution
operations on static graphs in a dynamic fashion. [32]
develop a temporal GCN method called T-GCN, which
they apply for traffic prediction. Here too, the graph
remains fixed over time, and only the features vary. [31]
propose a method which they refer to as a tensor graph
CNN. Here, the standard GCN [12] based on matrix
algebra is considered, and a “cross graph convolution”
layer is introduced to handle the time varying aspect
of the dynamic graph. In particular, the cross graph
convolution layer involves computing a parameterized
Kronecker sum of the current adjacency matrix with
the previously processed adjacency matrix, followed by
a GCN layer. Recently, [18] described a tensor version
of GCN for text classification, where the text semantics are represented as a three-dimensional graph tensor.
This work neither considers time varying graphs, nor the
tensor M-product framework.
The set of methods most relevant to our setting of
learning embeddings of dynamic graphs use combinations of GNNs and recurrent architectures (RNN), to
capture the graph structure and handle time dynamics,
respectively. The approach in [19] uses Long Short-Term
Memory (LSTM), a recurrent network, in order to handle
time variations along with GNNs. They design architectures for semi-supervised node classification and for
supervised graph classification. [23] presented a variant of GCN called EvolveGCN, where Gated Recurrent
Units (GRUs) and LSTMs are coupled with a GCN to
handle dynamic graphs. This paper is currently the

Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

state-of-the-art. [24] proposed the use of a temporal
self-attention layer for dynamic graph representation
learning. However, all these approaches are based on a
heuristic RNN/GRU mechanism to evolve weights, and
the models are not time aware (time is not an explicit
entity). [21] present a tensor NN which utilizes the tensor M-product framework. Their approach is applicable
to image and other high-dimensional data that lie on
regular grids.

Definition 3.2. (Facewise product) Let
X ∈ RI×J×T and Y ∈ RJ×K×T be two tensors.
The facewise product, denoted by X 4 Y ∈ RI×K×T , is
def
defined facewise as (X 4 Y)::t = X::t Y::t .
Definition 3.3. (M-product) Let X ∈ RI×J×T and
Y ∈ RJ×K×T be two tensors, and let M ∈ RT ×T
be an invertible matrix. The M-product, denoted by
X ? Y ∈ RI×K×T , is defined as
X ? Y = ((X ×3 M) 4(Y ×3 M)) ×3 M−1 .
def

3

Tensor M-Product Framework

Here, we cover the necessary preliminaries on tensors
and the M-product framework. For a more general
introduction to tensors, we refer the reader to the review
paper [13]. In the present paper, a tensor is a threedimensional array of real numbers denoted by boldface
Euler script letters, e.g. X ∈ RI×J×T . Matrices are
denoted by bold uppercase letters, e.g. X; vectors are
denoted by bold lowercase letter, e.g. x; and scalars
are denoted by lowercase letters, e.g. x. An element
at position (i, j, t) in a tensor is denoted by subscripts,
e.g. Xijt , with similar notation for elements of matrices
and vectors. A colon will denote all elements along that
dimension; Xi: denotes the ith row of the matrix X, and
X::k denotes the kth frontal slice of X. The vectors Xij:
are called the tubes of X.
The framework we consider relies on a new definition
of the product of two tensors, called the M-product
[3, 11, 10]. A distinguishing feature of this framework is
that the M-product of two three-dimensional tensors is
also three-dimensional, which is not the case for e.g.
tensor contractions [13]. It allows one to elegantly
generalize many classical numerical methods from linear
algebra. The framework, originally developed for threedimensional tensors, has been extended to handle tensors
of dimension greater than three [11]. The following
definitions 3.1–3.3 describe the M-product.

In the original formulation of the M-product, M was
chosen to be the Discrete Fourier Transform (DFT)
matrix, which allows efficient computation using the Fast
Fourier Transform (FFT) [3, 11]. The framework was
later extended for arbitrary invertible M (e.g. discrete
cosine and wavelet transforms) [10]. Additional details
are in the supplement.
4

Tensor Dynamic Graph Embedding

Our approach is inspired by the first order GCN by [12]
for static graphs, owed to its simplicity and effectiveness.
For a graph with adjacency matrix A and feature matrix
X, a GCN layer takes the form Y = σ(ÃXW), where
Ã = D̃−1/2 (A + I)D̃−1/2 ,
P
D̃ is diagonal with D̃ii = 1 + j Aij , I is the matrix
identity, W is a matrix to be learned when training the
NN, and σ is an activation function, e.g., ReLU. Our
approach translates this to a tensor model by utilizing
the M-product framework. We first introduce a tensor
activation function σ̂ which operates in the transformed
space.
def

Definition 4.1. Let A ∈ RI×J×T be a tensor and σ an
elementwise activation function. We define the activation
def
function σ̂ as σ̂(A) = σ(A ×3 M) ×3 M−1 .

Definition 3.1. (M-transform) Let M ∈ RT ×T be
a mixing matrix. The M-transform of a tensor X ∈ We can now define our proposed dynamic graph emN ×N ×T
be a tensor with frontal
RI×J×T is denoted by X ×3 M ∈ RI×J×T and defined bedding. Let A ∈ R
(t)
slices A::t = Ã , where Ã(t) is the normalization of
elementwise as
A(t) . Moreover, let X ∈ RN ×F ×T be a tensor with
0
T
X
frontal slices X::t = X(t) . Finally, let W ∈ RF ×F ×T be
def
(3.1)
(X ×3 M)ijt =
Mtk Xijk .
a weight tensor. We define our dynamic graph embed0
k=1
ding as Y = A ? X ? W ∈ RN ×F ×T . This computation
can also be repeated in multiple layers. For example, a
We say that X ×3 M is in the transformed space. Note
2-layer
formulation would be of the form
that if M is invertible, then (X ×3 M) ×3 M−1 = X.
Consequently, X ×3 M−1 is the inverse M-transform of
Y = A ? σ̂(A ? X ? W(0) ) ? W(1) .
X. The definition in (3.1) may also be written in matrix
def
form as X ×3 M = fold(M unfold(X)), where the unfold
One important consideration is how to choose the
operation takes the tubes of X and stack them as columns matrix M which defines the M-product. For timeinto a T × IJ matrix, and fold(unfold(X)) = X.
varying graphs, we choose M to be lower triangular
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

and banded so that each frontal slice (A ×3 M)::t
is a linear combination of the adjacency matrices
A::max(1,t−b+1) , . . . , A::t , where we refer to b as the
“bandwidth” of M. This choice ensures that each frontal
slice (A ×3 M)::t only contains information from current
and past graphs that are close temporally. We consider
two variants of the lower banded triangular M matrix in
the experiments; see the supplement for details. Another
possibility is to treat M as a parameter matrix to be
learned from the data.
In order to avoid over-parameterization and improve
the performance, we choose the weight tensor W (at
each layer), such that each of the frontal slices of
W in the transformed domain remains the same, i.e.,
(W ×3 M)::t = (W ×3 M)::t0 ∀t, t0 . In other words, the
parameters in each layer are shared and learned over
all the training instances. This reduces the number of
parameters to be learned significantly.
0
An embedding Y ∈ RN ×F ×T can now be used for
various prediction tasks, like link prediction, and edge
and node classification. In Section 5, we apply our
method for edge classification and link prediction by
using a model similar to that used by [23]: Given an
edge between nodes m and n at time t, the predictive
model is
(4.2)
def
p(m, n, t) = softmax(U[(Y ×3 M)m:t , (Y ×3 M)n:t ]> ),
0

0

where (Y ×3 M)m:t ∈ RF and (Y ×3 M)n:t ∈ RF are
0
row vectors, U ∈ RC×2F is a weight matrix, and C
the number of classes. Note that the embedding Y is
first M-transformed before the matrix U is applied to
the appropriate feature vectors. This, combined with
the fact that the tensor activation functions are applied
elementwise in the transformed domain, allow us to avoid
ever needing to apply the inverse M-transform. This
approach reduces the computational cost, and has been
found to improve performance in the edge classification
task.

sition L = Q ? D ? Q> .
Definition 4.2. (Filtering) Given a signal X ∈
RN ×1×T and a function g : R1×1×T → R1×1×T , we define the tensor spectral graph filtering of X with respect
to g as
(4.3)

Xfilt = Q ? g(D) ? Q> ? X,
def

where
(
g(D)mn: =

def

g(Dmn: )
0

if m = n,
if m =
6 n.

In order to avoid the computation of an eigendecomposition, [7] uses a polynomial to approximate the filter
function. We take a similar approach, and approximate
g(D) with an M-product polynomial. For this approximation, we impose additional structure on g.
Assumption 1. Assume that g : R1×1×T → R1×1×T is
defined as
g(V) = f (V ×3 M) ×3 M−1 ,
def

where f is defined elementwise as f (V ×3 M)11t =
f (t) ((V ×3 M)11t ) with each f (t) : R → R continuous.

def

Proposition 4.2. Suppose g satisfies Assumption 1.
For any ε > 0, there exists an integer K and a set
1×1×T
{θ (k) }K
such that
k=1 ⊂ R
g(D) −

K
X

D?k ? θ (k) < ε,

k=0

where k · k is the tensor Frobenius norm, and where
def
D?k = D ? · · · ? D is the M-product of k instances of
D, with the convention that D?0 = I.

As in the work of [7], a tensor polynomial approximation
allows us to approximate Xfilt in (4.3) without computing
4.1 Theoretical Motivation for TM-GCN Here, the eigendecomposition of L:
we present the results that establish the connection beXfilt = Q ? g(D) ? Q> ? X
tween the proposed TM-GCN and spectral convolution
of tensors, in particular spectral filtering and approximaK
X

tion on dynamic graphs. This is analogous to the graph
≈Q?
D?k ? θ (k) ? Q> ? X
convolution based on spectral graph theory in the GNNs (4.4)
k=0
by [4], [7], and [12]. All proofs and additional details are
K
X

provided in Section D of the supplement.
=
L?k ? θ (k) ? X.
Let L ∈ RN ×N ×T be a form of tensor Laplacian
k=0
def
defined as L = I − A. Throughout the remainder
All that is necessary is to compute tensor powers of
of this subsection, we will assume that the adjacency
L. We can also define tensor polynomial analogs of the
matrices A(t) are symmetric.
Chebyshev polynomials and do the approximation in
Proposition 4.1. The tensor L has an eigendecompo- (4.4) in terms of those instead of the tensor monomials
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

D?k . We note that if a degree-one approximation is
used, the computation in (4.4) becomes

Table 1: Dataset statistics.
Window

Dataset

Xfilt ≈ (I ? θ (0) + L ? θ (1) ) ? X

SBM
BitcoinOTC
BitcoinAlpha
Reddit
Chess

= (I ? θ (0) + (I − A) ? θ (1) ) ? X.
Setting θ = θ (0) = −θ (1) , which is analogous to the
parameter choice made in the degree-one approximation
in [12], we get
def

Partitioning

Nodes

Edges

T

(days)

C

Strain

Sval

Stest

1,000
6,005
7,604
3,818
7,301

1,601,999
35,569
24,173
163,008
64,958

50
135
135
86
100

–
14
14
14
31

–
2
2
2
3

35
95
95
66
80

5
20
20
10
10

10
20
20
10
10

However, their framework does not account for time
direction, and the graph is not considered to be evolving.
We define the message passing framework for a dynamic
If we let X contain F signals, i.e., X ∈ RN ×F ×T , and graph as follows: The message passing phase will consti(t)
apply F 0 filters, (4.5) becomes
tute updating the hidden state hv,` at node v of graph
(4.5)

Xfilt ≈ A ? X ? θ.

Xfilt ≈ A ? X ? Θ ∈ RN ×F

0

×T

,

(t)

G (t) in the `th layer with message mv,`+1 as

0

where Θ ∈ RF ×F ×T . This is precisely our embedding
model, with Θ replaced by a learnable parameter tensor
W. These results show: (a) the connection between TMGCN and spectral convolution of tensors, analogous
to the GCN, and (b) that we can indeed develop higher
order convolutional GNNs like [4, 7] for dynamic graphs
using our framework.
4.2 Message Passing Framework The Message
Passing Neural Network (MPNN) framework is popularly
used to describe spatial convolution GNNs [8]. The graph
convolution operation is considered to be a message
passing process, with information being passed from one
node to another along the edges. The message passing
phase of MPNN constitutes updating the hidden state
hv,` at node v in the `th layer with message mv,`+1 as
X
mv,`+1 =
Φ` (hv,` , hw,` , evw ),
w∈N (v)

hv,`+1 = Ψ` (hv,` , mv,`+1 ),
where N (v) is the neighbors of v in the graph, evw is the
edge between nodes v and w, Φ` is a message function,
and Ψ` is an update function. A number of GNN models
can be defined using this standard MPNN framework for
static graphs. For the standard GCN model [12], we have
Φ` (hv,` , hw,` ) = Av,w hw,` , where Av,w is the entry of
adjacency matrix A, and Ψ` (hv,` , mv,`+1 ) = σ(mv,`+1 )
where σ is a pointwise non-linear function, e.g., ReLU.
In this paper, we consider dynamic graphs and the
designed GNN has to do spatial and temporal message
passing. That is, for a graph G (t) at time t, the MPNN
should be modeled such that the information/message is
passed between neighboring nodes, as well as the corresponding nodes in the graphs {G (t−1) , G (t−2) , . . . , G (1) }.
Recently, a spatio-temporal message passing framework
was defined for video processing in computer vision [20].

(t)
mv,`+1

=

t
X X

(τ )
Φ`

w∈N (v) τ =1
(t)

(t)

t
X

(τ )
(τ )
(τ )
Γ` (hv,` ), hw,` , eτvw

!
,

τ =1
(t)

hv,i+1 = Ψ` (hv,` , mv,`+1 ).
(τ )

Here, the function Γ` accounts for the message passing
between hidden states over different time τ ≤ t, and
(τ )
function φ` accounts for message passing between
neighbors N (v) over time τ ≤ t. The model accounts
for extensive spatio-temporal message passing.
For the proposed TM-GCN model, we have a
(τ )
(τ )
(τ )
function Γ` (hv,` ) = Mt,τ hv,` , where Mt,τ is the
(t, τ ) entry of the mixing matrix M. The message
(τ ) (t)
(τ )
(t)
function is Φ` (zv,` , hw,` ) = Mt,τ Av,w,τ , where zv,` =
Pt
(τ )
(τ )
(hv,` ), and Av,w,τ is the entry of the adjacency
τ =1 Γ
(t)

(t)

tensor A. The update function is Ψ` (hv,` , mv,`+1 ) =
(t)

σ(mv,`+1 ) with an elementwise non-linear function σ.
Note that the above message passing model does not
include the inverse transform M−1 as in the definition
of the M-products. This is because, the M-transform
is responsible for the temporal message passing and
undoing it is not necessary. In our experiments too,
we found that transforming back (applying the inverse
transform M−1 ) did not yield improved results as
suggested by the above MPNN model. This does
not affect any of the theory presented in the previous
section since the spectral filtering is performed in the
transformed domain (see the supplement for details).
5

Numerical Experiments

We first present results for edge classification and link
prediction. We then show how we can use GNNs
for predicting the state of individuals from COVID-19
contact tracing graphs.
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

Table 2: Results for edge classification. Performance measures are F1 score† or accuracy∗ . A higher value is better.
Dataset
Method
WD-GCN
EvolveGCN
GCN
TM-GCN - M1
TM-GCN - M2

Bitcoin OTC†

Bitcoin Alpha†

Reddit†

Chess∗

0.3562
0.3483
0.3402
0.3660
0.4361

0.2533
0.2273
0.2381
0.3243
0.2466

0.2337
0.2012
0.1968
0.2057
0.1833

0.4311
0.4351
0.4342
0.4708
0.4513

Table 3: Results for link prediction. Performance measure is MAP. A higher value is better.
Dataset
Method
WD-GCN
EvolveGCN
GCN
TM-GCN - M1
TM-GCN - M2

N

N

Bitcoin OTC

Bitcoin Alpha

Reddit

Chess

0.9436
0.7620
0.9201
0.9684
0.9799

0.8071
0.6985
0.6847
0.8026
0.8458

0.8795
0.7722
0.7655
0.9318
0.9631

0.3896
0.2866
0.3099
0.2270
0.1405

0.1279
0.0915
0.0899
0.1882
0.1514

T

T

N

SBM

Te
Vali sting
dati
on
Train
ing

T

N

N

Te
Vali sting
dati
on
Train
ing

N

Te
Vali sting
dati
on
g

Train
in

Figure 2: Partitioning of A into training, validation and
testing data.
5.1 Datasets and Preprocessing We consider five
datasets (links to the datasets are in the supplement):
The Bitcoin Alpha and OTC transaction datasets [23],
the Reddit body hyperlink dataset [14], a chess results
dataset [15], and SBM is the structure block matrix by [9].
The bitcoin datasets consist of transaction histories
for users on two different platforms. Each node is a
user, and each directed edge indicates a transaction
and is labeled with an integer between −10 and 10
which indicates the senders trust for the receiver. We
convert these labels to two classes: positive (trustworthy)
and negative (untrustworthy). The Reddit dataset is
built from hyperlinks from one subreddit to another.
Each node represents a subreddit, and each directed
edge is an interaction which is labeled with −1 for a
hostile interaction or +1 for a friendly interaction. We
only consider those subreddits which have a total of 20
interactions or more. In the chess dataset, each node
is a player, and each directed edge represents a match
with the source node being the white player and the
target node being the black player. Each edge is labeled
−1 for a black victory, 0 for a draw, and +1 for a

white victory. Table 1 summarizes the statistics for the
different datasets, where T is total # graphs and C is
the # classes. The SBM dataset has no labels and hence
we use it only for link prediction.
The data is temporally partitioned into T graphs,
with each graph containing data from a particular time
window. Both T and the time window length can vary
between datasets. For each node-time pair (n, t) in
these graphs, we compute the number of outgoing and
incoming edges and use these two numbers as features.
The adjacency tensor A is then constructed as described
in Section 4. The T frontal slices of A are divided
into Strain training slices, Sval validation slices, and Stest
testing slices, which come sequentially after each other;
see Figure 2 and Table 1.
Since the adjacency matrices corresponding to
graphs are very sparse for these datasets, we apply
the same technique as [23] and add the entries of each
frontal slice A::t to the following l − 1 frontal slices
A::t , . . . , A::(t+l−1) , where we refer to l as the “edge life.”
Note that this only affects A, and that the added edges
are not treated as real edges in the classification and
prediction problems.
The bitcoin and Reddit datasets are heavily skewed,
with about 90% of edges labeled positively, and the remaining labeled negatively. Since the negative instances
are more interesting to identify (e.g. to prevent financial fraud or online hostility), we use the F1 score to
evaluate the edge classification experiments on these
datasets, treating the negative edges as the ones we want
to identify. The classes are more well-balanced in the

Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

chess dataset, so we use accuracy to evaluate those edge consider two variants of the M matrix (M1 and M2); see
classification experiments.
the supplement for details.
We compare our method with three other methods.
5.2 Graph Tasks For the link prediction experi- The first one is a variant of the WD-GCN by [19], which
ments, we follow [23] and use negative sampling to con- they specify in Equation (8a) of their paper. For the
struct non-existing edges, and use mean average preci- LSTM layer in their description, we use 6 output features
sion (MAP) as a performance measure. The negative instead of N . This is to avoid overfitting and make the
sampling is done so that 5% of edges are existing edges method more comparable to ours which uses 6 output
for each time slice, and all other edges are non-existing. features. The second method is a 1-layer variant of
Precise definitions of the different performance measures EvolveGCN-H by [23]. The third method is a simple
we use are given in Section B of the supplement.
baseline which uses a 1-layer version of the GCN by
For edge classification, we use an embedding Ytrain = [12]. It uses the same weight matrix W for all temporal
A::(1:Strain ) ? X::(1:Strain ) ? W for training. When comput- graphs. Both EvolveGCN-H and the baseline GCN use
ing the embeddings for the validation and testing data, 6 output features as well. We use the prediction model
we still need Strain frontal slices of A, which we get by (4.2) as the final layer in all models we compare.
using a sliding window of slices. This is illustrated in
Tables 2 and 3 show the results for edge classification
Figure 2, where the green, blue and red blocks show the and link prediction, respectively. For edge classification,
frontal slices used when computing the embeddings for our method outperforms the other methods on the
the training, validation and testing data, respectively. two bitcoin datasets and the chess dataset, with WDThe embeddings for the validation and testing data are GCN performing best on the Reddit dataset. For link
Yval = A::(Sval +1:Strain +Sval ) ? X::(Sval +1:Strain +Sval ) ? W prediction, our method outperforms the other methods
and Ytest = A::(Sval +Stest +1:T ) ? X::(Sval +Stest +1:T ) ? W, on the SBM, bitcoin and chess datasets. For Reddit,
respectively. For link prediction, we use the same em- our method performs worse than the other methods.
beddings, with the only difference that the embedding Results from some additional experiments are provided
blocks contain Strain − 1 slices. This is necessary since in Section C of the supplement.
we want to use information up to time t to predict edge
existence at time t + 1. Preliminary experiments with 5.3 COVID-19 Application One of the primary
2-layer architectures did not show convincing improve- challenges related to the COVID-19 pandemic has been
ments in performance. We believe this is due to the the issue of identifying early the individuals who are
fact that the datasets only have two features, and that a infected (ideally before they display symptoms) and
1-layer architecture therefore is sufficient for extracting prescribe testing. Here, we demonstrate how we can
relevant information in the data.
potentially use GNNs on contact tracing data to achieve
For training, we use the cross entropy loss function: this.
(5.6)
Contact tracing, a process where interactions beC
tween individuals (infected and others) are carefully
X X X
loss = −
αc f (m, n, t)c log(p(m, n, t)c ), tracked, has been shown the be an effective method for
t (m,n)∈Et c=1
managing the spread of COVID-19. A variety of contact
tracing methodologies have been used today around the
C
where α ∈ R is a vector summing to 1 which
world, see [1, 28] for lists. Recently, Ubaru et. al [28]
contains the weight of each class. For edge classification,
presented a probabilistic graphical SEIR epidemiologif (m, n, t) ∈ RC is a one-hot vector encoding the true
cal model (Susceptible, Exposed, Infected, Recovered)
class of the edge (m, n) at time t. For link prediction,
to describe the dynamics of the disease transmission.
2
f (m, n, t) ∈ R is also a one-hot vector, but now
Their model considers a dynamic graph (with individuencoding if the edge is an existing or non-existing edge.
als as nodes) that accounts for the interactions between
As appropriate, we weigh the minority class more heavily
individuals obtained from contact tracing, and uses a
in the loss function for skewed datasets, and treat α as
stochastic diffusion-reaction model to describe the disa hyperparameter. See Section B of the supplement for
ease transmission over the graph.
further details on the experiment setup, including the
The novel SEIR model in [28] considers the graph
training setup and how hyperparameter tuning is done.
Laplacian Lt (from contact tracing data) at each time
The experiments are implemented in PyTorch with
t and describes the evolution of the state {S, E, I, R}
some preprocessing done in Matlab. Our code is
for each node/individual. Figure 3 illustrates the state
available at https://github.com/IBM/TM-GCN. In the
{S, E, I, R} evolution as defined by the model on a
experiments, we use an edge life of l = 10, a bandwidth
sample dynamic graph with 10 individuals (for easy
0
b = 20, and F = 6 output features. For TM-GCN, we
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

Figure 3: Graphical SEIR model disease transmission visualization.

Table 4: COVID-19 Data: Mean absolute error and error
ratio for infection state I prediction.
Methods
COVID-19 Dataset
Error
Ratio
WD-GCN
1.667
0.337
EvolveGCN 4.969
0.912
TM-GCN
1.466
0.278
visualization). We see how the infection (one individual
as red node in first graph) transmits, we have magenta
nodes with I > 0.04, and the yellow nodes with I >
0.002, and we note the interactions and the state change
over time. Here, we show how we can use dynamic GNNs
to predict the infection state I at time T + 1, using only
the dynamic graphs up to time T , when the true SEIR
model is unknown.
We consider a simulation with N = 1000 individuals
and total time T = 100. We simulate the contact tracing
dynamic graph as in [28], and assume at each time t
a small number of individuals are tested at random
for both IgM (if positive state I is set to 1) and IgG
(state R is set to 1) antigen tests. The state of the
remaining individuals are determined by the SEIR model.
We train the dynamic GNNs on the first T = 80 time
instances and test the GNNs on the remaining 20 time
instances. Our goal is to train a GNN that learns the
relation between the contact tracing graphs and the
infection state (some exact values for those who were
tested and others from the SEIR model), in order to
better predict the individuals’ state I at time t + 1 than
just using the SEIR model. Table 4 gives the mean
absolute error and error ratio obtained by the three
dynamic GNNs for infection state I prediction on the
test time instances. We note that, the proposed TMGCN yields best results among the three methods,
since it has a better time awareness (explicitly considers
b previous time instances via the M-product) than others.
Using such predictions, we can issue early warnings to
individuals who are infected and prescribe testing.

6

Conclusion

We have presented a novel approach for dynamic
graph embedding which leverages the tensor M-product
framework. We used it for edge classification and
link prediction in experiments on five datasets, where
it performed competitively compared to state-of-theart methods. We also demonstrated the method’s
effectiveness in an important application related to the
COVID-19 pandemic. Future research directions include
further developing the theoretical guarantees for the
method, investigating optimal structure and learning of
the transform matrix M, using the method for other
prediction tasks, and investigating how to utilize deeper
architectures for dynamic graph learning.
Acknowledgments
We thank Stephen Becker and Lingfei Wu for helpful
discussions and feedback. We also thank the reviewers for
their insightful comments and suggestions. Kilmer was
partially supported by a grant from IBM T.J. Watson
and by Tufts T-Tripods Institute under NSF HDR grant
CCF-1934553. Avron was supported by Israel Science
Foundation grant 1272/17 and by an IBM Faculty
Award.
References
[1] H. Alsdurf, Y. Bengio, T. Deleu, P. Gupta, D. Ippolito,
R. Janda, M. Jarvie, T. Kolody, et al. Covi white paper.
arXiv preprint arXiv:2005.08502, 2020.
[2] T. Y. Berger-Wolf and J. Saia. A framework for analysis
of dynamic social networks. In Proceedings of the 12th
ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 523–528. ACM, 2006.
[3] K. Braman. Third-order tensors as linear operators on
a space of matrices. Linear Algebra and its Applications,
433(7):1241–1253, 2010.
[4] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun.
Spectral networks and locally connected networks on
graphs. arXiv preprint arXiv:1312.6203, 2013.

Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

[5] H. Cai, V. W. Zheng, and K. C.-C. Chang. A
comprehensive survey of graph embedding: Problems,
techniques, and applications. IEEE Transactions on
Knowledge and Data Engineering, 30(9):1616–1637,
2018.
[6] F. De Vico Fallani, J. Richiardi, M. Chavez, and
S. Achard. Graph analysis of functional brain networks:
Practical issues in translational neuroscience. Philosophical Transactions of the Royal Society B: Biological
Sciences, 369(1653):20130521, 2014.
[7] M. Defferrard, X. Bresson, and P. Vandergheynst.
Convolutional neural networks on graphs with fast
localized spectral filtering. In Advances in Neural
Information Processing Systems, pages 3844–3852, 2016.
[8] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals,
and G. E. Dahl. Neural message passing for quantum
chemistry. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pages
1263–1272. JMLR. org, 2017.
[9] P. Goyal, N. Kamra, X. He, and Y. Liu. Dyngem: Deep
embedding method for dynamic graphs. arXiv preprint
arXiv:1805.11273, 2018.
[10] E. Kernfeld, M. Kilmer, and S. Aeron. Tensor–tensor
products with invertible linear transforms. Linear
Algebra and its Applications, 485:545–570, 2015.
[11] M. E. Kilmer, K. Braman, N. Hao, and R. C. Hoover.
Third-order tensors as operators on matrices: A theoretical and computational framework with applications
in imaging. SIAM Journal on Matrix Analysis and
Applications, 34(1):148–172, 2013.
[12] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint
arXiv:1609.02907, 2016.
[13] T. G. Kolda and B. W. Bader. Tensor Decompositions
and Applications. SIAM Review, 51(3):455–500, 2009.
[14] S. Kumar, W. L. Hamilton, J. Leskovec, and D. Jurafsky. Community interaction and conflict on the web.
In Proceedings of the 2018 World Wide Web Conference, pages 933–943. International World Wide Web
Conferences Steering Committee, 2018.
[15] J. Kunegis. Konect: The koblenz network collection.
In Proceedings of the 22nd International Conference on
World Wide Web, pages 1343–1350. ACM, 2013.
[16] J. Li, H. Dani, X. Hu, J. Tang, Y. Chang, and H. Liu.
Attributed network embedding for learning in a dynamic
environment. In Proceedings of the 2017 ACM on
Conference on Information and Knowledge Management,
pages 387–396. ACM, 2017.
[17] Y. Li, R. Yu, C. Shahabi, and Y. Liu. Diffusion
convolutional recurrent neural network: Data-driven
traffic forecasting. arXiv:1707.01926, 2017.
[18] X. Liu, X. You, X. Zhang, J. Wu, and P. Lv. Tensor
graph convolutional networks for text classification.
arXiv preprint arXiv:2001.05313, 2020.
[19] F. Manessi, A. Rozza, and M. Manzo. Dynamic graph
convolutional networks. Pattern Recognition, 97, 2020.
[20] E. Mavroudi, B. B. Haro, and R. Vidal. Neural
message passing on hybrid spatio-temporal visual

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

and symbolic graphs for video understanding. arXiv
preprint arXiv:1905.07385, 2019.
E. Newman, L. Horesh, H. Avron, and M. Kilmer.
Stable Tensor Neural Networks for Rapid Deep Learning.
arXiv preprint arXiv:1811.06569, 2018.
G. H. Nguyen, J. B. Lee, R. A. Rossi, N. K. Ahmed,
E. Koh, and S. Kim. Continuous-time dynamic network
embeddings. In Companion Proceedings of the The Web
Conference 2018, pages 969–976, 2018.
A. Pareja, G. Domeniconi, J. Chen, T. Ma, T. Suzumura, H. Kanezashi, T. Kaler, T. B. Schardl, and
C. E. Leisersen. EvolveGCN: Evolving graph convolutional networks for dynamic graphs. arXiv preprint
arXiv:1902.10191, 2019.
A. Sankar, Y. Wu, L. Gou, W. Zhang, and H. Yang. Dynamic graph representation learning via self-attention
networks. arXiv preprint arXiv:1812.09430, 2018.
Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson. Structured sequence modeling with graph convolutional recurrent networks. In Conference on Neural
Information Processing, pages 362–373. Springer, 2018.
R. Trivedi, H. Dai, Y. Wang, and L. Song. Know-evolve:
Deep temporal reasoning for dynamic knowledge graphs.
In International Conference on Machine LearningVolume 70, pages 3462–3471. JMLR. org, 2017.
R. Trivedi, M. Farajtabar, P. Biswal, and H. Zha.
DyRep: Learning representations over dynamic graphs.
In International Conference on Learning Representations, 2019.
S. Ubaru, L. Horesh, and G. Cohen. Dynamic graph
based epidemiological model for COVID-19 contact
tracing data analysis and optimal testing prescription.
arXiv preprint arXiv:2009.04971, 2020.
Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein,
and J. M. Solomon. Dynamic graph CNN for learning
on point clouds. arXiv preprint arXiv:1801.07829, 2018.
Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S.
Yu. A comprehensive survey on graph neural networks.
arXiv preprint arXiv:1901.00596, 2019.
T. Zhang, W. Zheng, Z. Cui, and Y. Li. Tensor
graph convolutional neural network. arXiv preprint
arXiv:1803.10071, 2018.
L. Zhao, Y. Song, C. Zhang, Y. Liu, P. Wang, T. Lin,
M. Deng, and H. Li. T-GCN: A Temporal Graph
Convolutional Network for Traffic Prediction. IEEE
Transactions on Intelligent Transportation Systems,
2019.
J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, and M. Sun.
Graph neural networks: A review of methods and
applications. arXiv preprint arXiv:1812.08434, 2018.
L. Zhou, Y. Yang, X. Ren, F. Wu, and Y. Zhuang.
Dynamic network embedding by modeling triadic closure process. In Thirty-Second AAAI Conference on
Artificial Intelligence, 2018.

Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

Supplementary Material:
Dynamic Graph Convolutional Networks Using the Tensor M-Product

A

Links to Datasets

• The Bitcoin Alpha dataset is available at https://snap.stanford.edu/data/soc-sign-bitcoin-alpha.
html.
• The Bitcoin OTC dataset is available at https://snap.stanford.edu/data/soc-sign-bitcoin-otc.html.
• The Reddit dataset is available at https://snap.stanford.edu/data/soc-RedditHyperlinks.html. Note
that we use the dataset with hyperlinks in the body of the posts.
• The chess dataset was originally downloaded from http://konect.uni-koblenz.de/networks/chess. This
link no longer works, and the Koblenz Network Collection appears to no longer be available online. We
therefore added the chess dataset to the TM-GCN GitHub repository at https://github.com/IBM/TM-GCN.
• SBM (structured block matrix) was generated using the code https://github.com/palash1992/
DynamicGEM/. We used 1000 nodes, 8 communities (blocks) and 50 time instances.
B

Further Details on the Experiment Setup

When partitioning the data into T graphs, as described in Section 5, if there are multiple data points corresponding
to an edge (m, n) for a given time step t, we only add that edge once to the corresponding graph and set the label
equal to the sum of the labels of the different data points. E.g., if bitcoin user m makes three transactions to n
during time step t with ratings 10, 2, −1, then we add a single edge (m, n) to graph t with label 10 + 2 − 1 = 11.
B.1 Edge Classification For training, we run gradient descent with a learning rate of 0.01 and momentum
of 0.9 for 10,000 iterations. For each 100 iterations, we compute and store the performance of the model on the
validation data. The weight vector α in the loss function (5.6) is treated as a hyperparameter in the bitcoin and
Reddit experiments. Since these datasets all have two edge classes, let α0 and α1 be the weights of the minority
(negative) and majority (positive) classes, respectively. Since these parameters add to 1, we have α1 = 1 − α0 . For
all methods, we repeat the bitcoin and Reddit experiments once for each α0 ∈ {0.75, 0.76, . . . , 0.95}. For each
model and dataset, we then find the best stored performance of the model on the validation data across all α0
values. We then treat the corresponding model as the trained model, and report its performance on the test data.
The results for the chess experiment are computed in the same way, but only for a single vector α = [1/3, 1/3, 1/3].
B.2 Link Prediction For training, we run gradient descent with a learning rate of 0.01 and momentum of 0.9
for 1,000 iterations. For each 100 iterations, we compute and store the performance of the model on the validation
data. We used α0 = 0.90 in our experiments, where α0 is the weight of the class corresponding to existing edges.
B.3 Definition of Performance Measures Suppose we are classifying N objects into C classes. Let
u ∈ {1, 2, . . . , C}N be a vector containing our computed classification, and let v ∈ {1, 2, . . . , C}N be a vector
which contains the true classes. Furthermore, let k denote the class we are interested in identifying (i.e., negative
edges in bitcoin and Reddit edge classification problems). Then, the F1 score is defined as follows:
F1 score = 2 ·

precision · recall
,
precision + recall
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

where
true positive
,
true positive + false positive
true positive
def
recall =
,
true positive + false negative

precision =

def

true positive = |{n ∈ {1, 2, . . . , N } : un = vn = k}|,
def

false positive = |{n ∈ {1, 2, . . . , N } : un = k, vn 6= k}|,
def

false negative = |{n ∈ {1, 2, . . . , N } : un 6= k, vn = k}|.
def

For the accuracy in the edge classification experiment on the chess dataset, we simply compute it as the
proportion of correctly labeled edges.
For computing mean average precision, we use the average_precision_score function in the Scikit-learn
library. As input, we use a vector containing the probabilities for the class of interest (i.e., the class corresponding
to existing edges in link prediction) generated by the output model (4.2).
B.4 Choice of M Matrix We consider two variants of the lower banded triangular M matrices in our
experiments. Specifically, in the first matrix M 1, the entries of M are set to
(
1
if max(1, t − b + 1) ≤ k ≤ t,
def
(B.1)
Mtk = min(b,t)
0
otherwise,
P
which implies that k Mtk = 1 for each t. However, this transform gives equal weights to all the previous b time
instances. In the second matrix (M2), we have the entries as:
(
1
if max(1, t − b + 1) ≤ k ≤ t,
def
(B.2)
Mtk = k
0 otherwise.
This transform gives exponential weights to the b previous time instances, weighing recent ones more, and the
weights decay exponentially for older time instances.
C

Additional Experimental Results

Since the graphs in the considered datasets are directed, we also investigate the impact of symmetrizing the
def
adjacency matrices, where the symmetrized version of an adjacency matrix A is defined as Asym = 1/2(A + A> ).
Table 5 shows the results. Our method outperforms the other methods on the Bitcoin OTC dataset and the chess
dataset, and performs similarly but slightly worse than the best performing methods on the Bitcoin Alpha and
Reddit datasets. Overall, it seems like symmetrizing the adjacency matrices leads to lower performance.
Table 5: Results for edge classification when adjacency matrices have been symmetrized. Performance measures
are F1 score† or accuracy∗ . A higher value is better.
Dataset
Method
WD-GCN
EvolveGCN
GCN
TM-GCN - M1

D

Bitcoin OTC†

Bitcoin Alpha†

Reddit†

Chess∗

0.1009
0.0913
0.0769
0.3103

0.1319
0.2273
0.1538
0.2207

0.2173
0.1942
0.1966
0.2071

0.4321
0.4091
0.4369
0.4713

Additional Details and Proofs

Here, we give additional details regarding the tensor M-product framework. We also present a few additional
theoretical results and proofs.
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

D.1 Additional Details A benefit of the tensor M-product framework is that many standard matrix concepts
can be generalized in a straightforward manner. Definitions D.1–D.4 extend the matrix concepts of diagonality,
identity, transpose and orthogonality to tensors [3, 11].
Definition D.1. (f-diagonal) A tensor X ∈ RN ×N ×T is said to be f-diagonal if each frontal slice X::t is
diagonal.
Definition D.2. (Identity tensor) Let Î ∈ RN ×N ×T be defined facewise as Î::t = I, where I is the matrix
def
identity. The M-product identity tensor I ∈ RN ×N ×T is then defined as I = Î ×3 M−1 .
Definition D.3. (Tensor transpose) The transpose of a tensor X is defined as X> = Y ×3 M−1 , where
Y::t = (X ×3 M)>
::t for each t ∈ {1, . . . , T }.
def

Definition D.4. (Orthogonal tensor) A tensor X ∈ RN ×N ×T is said to be orthogonal if X?X> = X> ?X =
I.
Leveraging these concepts, a tensor eigendecomposition can now be defined [3, 11]:
Definition D.5. (Tensor eigendecomposition) Let X ∈ RN ×N ×T be a tensor and assume that each frontal
>

slice (X×3 M)::t is symmetric. We can then eigendecompose these as (X×3 M)::t = Q̂::t D̂::t Q̂::t , where Q̂::t ∈ RN ×N
is orthogonal and D̂::t ∈ RN ×N is diagonal. We assume that the eigenvalues along the diagonal of each D̂::t are
ordered in descending order, i.e., D̂nnt ≥ D̂mmt whenever n < m. The tensor eigendecomposition of X is defined
def
def
def
as X = Q ? D ? Q> , where Q = Q̂ ×3 M−1 is orthogonal, and D = D̂ ×3 M−1 if f-diagonal.
Illustration: Figure 4 (left) illustrates the unfolding operation. Figure 4 (right) shows how, once unfolded,
the matrix unfold(X) is multiplied from the left by M, which has a lower triangular banded structure. The output
is then folded back up into a tensor by doing the inverse operation of that illustrated in Figure 4.

Figure 4: Illustration of (left) unfold operation applied to 4 × 4 × 5 tensor, and (right) matrix product between M
and the unfolded tensor.
D.2 Additional Results Here, we present a few more results related to our analysis in Section 4.1. Much like
the spectrum of a normalized graph Laplacian is contained in [0, 2], the tensor spectrum of L satisfies a similar
property when M is chosen appropriately.
Proposition D.1. (Spectral bound) The entries of D̂ = D ×3 M lie in [0, 2] for the first M matrix M 1.
Proof. Each A::t has a spectrum contained in [−1, 1]. Since A::t is symmetric, it follows that kA::t k2 ≤ 1.
Consequently,
k(A ×3 M)::t k2 =

T
X
j=1

Mtj A::j

2

≤

T
X

|Mtj |kA::j k2 ≤ 1,

j=1

P
where we used the fact that j |Mtj | = 1. So since the frontal slices (A ×3 M)::t are symmetric, they each have a
spectrum in [−1, 1]. It follows that each frontal slice
(L ×3 M)::t = I − (A ×3 M)::t
has a spectrum contained in [0, 2], which means that the entries of D̂ all lie in [0, 2].
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

Following the work by [11], three-dimensional tensors in RM ×N ×T can be viewed as operators on N × T
matrices, with those matrices “twisted” into tensors in RN ×1×T . With this in mind, we define a tensor variant of
the graph Fourier transform.
Definition D.6. (Tensor-tube M-product) Let X ∈ RI×J×T and θ ∈ R1×1×T . Analogously to the definition
def
of the matrix-scalar product, we define X ? θ via (X ? θ)ij: = Xij: ? θ.
Definition D.7. (Tensor graph Fourier transform) Let X ∈ RN ×F ×T be a tensor, and let Q be defined
def
as in Definition 4.1. We define a tensor graph Fourier transform F via F (X) = Q> ? X ∈ RN ×F ×T .
This is analogous to the definition of the matrix graph Fourier transform. This defines a convolution like operation
for tensors similar to spectral graph convolution [4]. Each lateral slice X:j: is expressible in terms of the set
{Q:n: }N
n=1 as follows:
X:j: = Q ? Q> ? X:j: =

N
X

Q:n: ? (Q> ? X:j: )n1: ,

n=1

where each (Q> ? X:j: )n1: ∈ R1×1×T can be considered a tubal scalar. In fact, the lateral slices Q:n: form a basis
for the set RN ×1×T with product ?.
In the following, k · k will denote the Frobenius norm (i.e., the square root of the sum of the elements squared)
of a matrix or tensor, and k · k2 will denote the matrix spectral norm. We first provide a few further results that
clarify the algebraic properties of the M-product. Let R1×1×T denote the set of 1 × 1 × T tensors. Similarly, let
RN ×1×T denote the set of N × 1 × T tensors. Under the M-product framework, the set R1×1×T plays a role similar
to that played by scalars in matrix algebra. With this in mind, the set RN ×1×T can be seen as the length-N
vectors consisting of tubal elements of length T . Propositions D.2 and D.3 make this more precise.
Proposition D.2. (Proposition 4.2 in [10]) The set R1×1×T with product ?, which is denoted by (?, R1×1×T ),
is a commutative ring with identity.
Proposition D.3. (Theorem 4.1 in [10]) The set RN ×1×T with product ?, which is denoted by (?, RN ×1×T ),
is a free module over the ring (?, R1×1×T ).
A free module is similar to a vector space. Like a vector space, it has a basis. Proposition D.4 shows that the
lateral slices of Q in the tensor eigendecomposition form a basis for (?, RN ×1×T ), similarly to how the eigenvectors
in a matrix eigendecomposition form a basis.
Proposition D.4. The lateral slices Q:n: ∈ RN ×1×T of Q in Definition D.5 form a basis for (?, RN ×1×T ).
Proof. Let X ∈ RN ×1×T . Note that
X = I ? X = Q ? Q> ? X =

N
X

Q:n: ? Vn1: ,

n=1

where V = Q> ? X ∈ RN ×1×T . So the lateral slices of Q are a generating set for (?, RN ×1×T ). Now suppose
def

N
X

Q:n: ? Sn1: = 0,

n=1

for some S ∈ RN ×1×T . Then 0 = Q ? S, and consequently
0 = (Q ×3 M) 4(S ×3 M).
Since each frontal face of Q ×3 M is an invertible matrix, this implies that each frontal face of S ×3 M is zero, and
hence S = 0. So the lateral slices of Q are also linearly independent in (?, RN ×1×T ).
Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

D.3

Proofs of Propositions in the Main Text

Proof. (Proposition 4.1) Since each adjacency matrix A(t) and each I::t is symmetric, each frontal slice L::t is also
symmetric. Consequently,
(L ×3 M)ij: = Lij: ×3 M = Lji: ×3 M = (L ×3 M)ji: ,
so each frontal slice of L ×3 M is symmetric, and therefore L has an eigendecomposition.
Lemma D.1. Let X ∈ RM ×N ×T and let M ∈ RT ×T be invertible. Then
kXk ≤ kM−1 k2 kX ×3 Mk.
Proof. We have
kXk = k(X ×3 M) ×3 M−1 k = kM−1 unfold(X ×3 M)k
≤ kM−1 k2 k unfold(X ×3 M)k = kM−1 k2 kX ×3 Mk,
where the inequality is a well-known relation that holds for all matrices.
Proof. (Proposition 4.2) We show the proof for the case when M is defined as in (B.1). However, this proof can
easily be adapted to when M is defined as in (B.2) by adapting the interval [0, 2] in (D.3) appropriately.
1×1×T
By Weierstrass approximation theorem, there exists an integer K and a set {θ̂ (k) }K
such that for
k=1 ⊂ R
all t ∈ {1, 2, . . . , T },
sup f (t) (x) −

(D.3)

x∈[0,2]

K
X

ε

(k)

xk θ̂11t <

k=0

kM−1 k2

√

NT

.

Let θ (k) = θ̂ (k) ×3 M−1 . Note that if m 6= n, then
def

K
X

D?k ? θ (k)

k=0


mn:

=

K
X

((D̂

4k

)mn: ×3 M−1 ) ? θ (k) = 0 = g(D)mn: ,

k=0

since D̂ = D ×3 M is f-diagonal. So
g(D) −

K
X

D?k ? θ (k)

≤ kM−1 k22

N
X

g(D)nn: ×3 M −

n=1

= kM−1 k22

=

g(D)nn: −

n=1

k=0
N
X

2

N X
T
X
n=1 t=1

K
X

2

(D?k )nn: ? θ (k)

k=0
K
X

((D ×3 M)4 k )nn: 4 θ̂ (k)

2

k=0

f (t) ((D ×3 M)nnt ) −

K
X

(k)

(D ×3 M)knnt θ̂11t

2

k=0

< ε2 ,
where the first inequality follows from Lemma D.1, and the last inequality follows since (D ×3 M)nnt ∈ [0, 2] due
to Proposition D.1 (that proposition can easily be adapted to the case when M is defined as in (B.2)). Taking
square roots completes the proof.
Note that, the above proof holds even when we do not apply the inverse transform M−1 , since M−1 only shows
up as the norm kM−1 k2 at the end and the whole proof is still consistent without it.

Copyright © 2021 by SIAM
Unauthorized reproduction of this article is prohibited

