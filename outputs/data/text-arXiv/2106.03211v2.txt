arXiv:2106.03211v2 [cs.LG] 10 Jun 2021

Distributed Learning And Its Application For
Time-Series Prediction
Nhuong V. Nguyen

Sybille Legitime

Dept. Computer Science and Engineering
University of Connecticut
Connecticut, USA
nhuong.nguyen@uconn.edu

Dept. Computer Science and Engineering
University of Connecticut
Connecticut, USA
sybille.legitime@uconn.edu

Abstract—Extreme events are occurrences whose magnitude
and potential cause extensive damage on people, infrastructure,
and the environment. Motivated by the extreme nature of
the current global health landscape, which is plagued by the
coronavirus pandemic, we seek to better understand and model
extreme events. Modeling extreme events is common in practice
and plays an important role in time-series prediction applications.
Our goal is to (i) compare and investigate the effect of some
common extreme events modeling methods to explore which
method can be practical in reality and (ii) accelerate the deep
learning training process, which commonly uses deep recurrent
neural network (RNN), by implementing the asynchronous local
Stochastic Gradient Descent (SGD) framework among multiple
compute nodes. In order to verify our distributed extreme events
modeling, we evaluate our proposed framework on a stock data
set S&P500, with a standard recurrent neural network. Our
intuition is to explore the (best) extreme events modeling method
which could work well under the distributed deep learning
setting. Moreover, by using asynchronous distributed learning,
we aim to significantly reduce the communication cost among the
compute nodes and central server, which is the main bottleneck
of almost all distributed learning frameworks.
We implement our proposed work and evaluate its performance on representative data sets, such as S&P500 stock in 5year period. The experimental results validate the correctness
of the design principle and show a significant training duration
reduction upto 8x, compared to the baseline single compute node.
Our results also show that our proposed work can achieve the
same level of test accuracy, compared to the baseline setting.
Index Terms—extreme event, time-series, deep learning, distributed learning, local SGD, LSTM

I. I NTRODUCTION AND BACKGROUND
Events are considered extreme if they have the potential
for tremendous impacts on social, ecological, and technical
systems [1]. Such events can vary from a coronal mass ejection
during solar storms and volcano eruptions, to extreme rainfall,
stock market crashes, and pandemics. As we are currently
experiencing an extreme event, namely the pandemic caused
by the COVID-19 virus outbreak, our interest grew in understanding the current systems used for time-series and extreme
event prediction. Furthermore, we also seek to investigate
possible areas of improvement in the system training process.
In this project, we mainly focus on time-series prediction
using deep recurrent neural network. Analyzing time-series
data has yielded some important applications, such as weather

forecasting and stock prediction, which often need to obtain
high accuracy under specific safety-critical constraints.
In the scope of this research, we mainly focus on extreme
events, which have an interesting property in time-series
prediction. In practice, extreme events are usually hard to investigate. This comes from the fact that the number of extreme
events is negligible in size, compared to other normal events.
Hence, this leads to the problem of imbalanced data sets,
which still remains an open problem to solve [2]. Specifically,
if we use the standard sampling methods, such as window
sliding time, randomized events can lead to underfitting issues
because an extreme event has a small frequency in appearing
as a training sample. However, we can attempt to integrate the
events to every single sample, i.e, we duplicate the extreme
events to break the imbalanced barrier. By using this trick, our
deep learning model can learn the pattern of extreme events
occurrences; however, in practice, this trick is shown to suffer
from the overfitting issue.
Hence, we will do a sensitivity study on how to tackle
the problem of imbalanced data, especially for time-series
data sets. Due to the time constraint, we will conduct our
research on some common imbalanced data handling tricks,
including on the extreme value theory, which provides better
predictions on future occurrences of extreme events. Note that
there is always a trade-off between the efficiency and the
effectiveness of imbalanced data set handling methods. For
instance, if we choose a randomized time-series sample, we
can significantly accelerate the training process, including by
using a distributed learning framework while extreme value
theory is shown to achieve a decent performance, but suffers
from the computational cost. Therefore, our questions are:
how can we effectively and efficiently parallelize the extreme
event modeling method with deep recurrent neural network,
and which modeling method we can use?
The above analysis raises the question of the distributed
deep learning framework. In practice, there is a wide variety
of distributed learning frameworks, ranging from distributed
multiple cores (process) to distributed multiple compute nodes.
Specifically, in distributed multiple cores setting, we use one
computer equipped with multiple CPUs or GPUs joining the
training process. The information (gradient or the local model)
is exchanged through high-speed computer buses while in

distributed multiple compute nodes, the training process is
run in local machines and the information is exchanged via
the Internet or the local LAN network. In this proposal,
we mainly use the second architecture, which uses multiple
compute nodes. Moreover, from a synchronous standpoint,
we can further classify the distributed learning framework
into a synchronous and an asynchronous setting, which might
contain synchronous points or not. To fully utilize the computation power of the compute nodes, we prefer to use the
asynchronous distributed learning framework in this research
proposal.
So, we can summarise our contributions in this research
proposal as follows:
1) We perform a sensitivity study on the imbalanced data
set handling methods, especially extreme events to investigate which methods we can use in practice.
2) We propose an asynchronous distributed learning for
time-series prediction application. Our goal is to show
that we can significantly reduce the communication
cost while keeping a decent convergence performance,
compared to the baseline serial time-series prediction.
II. R ELATED WORK

tail distribution, to avoid losing valuable extreme event information.
Extreme Value Theory (EVT). Extreme Value Theory
takes a step further on studying these heavy-tailed data,
making it an important tool for many machine learning and
deep learning systems performing extreme event forecasting.
Extreme Value Theory focuses on the stochastic behavior
of extreme events found in the tails of probability distributions [3]; more specifically, it studies the distribution of
maximum in observed samples. A formal definition of the
distribution of the maximum is as follows: suppose T random
variables y1 , ..., yT are i.i.d sampled from distribution FY . The
distribution of the maximum is,
lim P {max(y1 , ..., yT ) ≤ y} = lim F T (y) = 0

T →∞

with the large constants 1 , 2 > 0 as thresholds. For time
step t, it vt = 0, the output yt is defined as a normal event.
If vt > 0 the output yt is defined as a right extreme event. In
the case of vt < 0 the output yt is defined as a left extreme
event.
There is a remarkable correlation between the Extreme
Event Problem and the shape of the data distribution. Previous
work has established this correlation by noticing that realworld data presents a distribution that always appears to be
heavy-tailed. If a random variable X is said to respect a
heavy-tailed distribution, then it usually has a non-negligible
probability of taking extreme values (larger 1 and smaller
than −2 ).
In fact, modeling with light-tailed parametric distributions
including Gaussian and Poisson, as opposed to heavy-tailed
distributions such as Pareto and log-Cauchy distribution, will
bring inevitable losses in the tail of the data. therefore,
addressing the Extreme Event Problem involves adequately
modeling the distribution of real-world data, especially the

(2)

To have P {max(y1 , ..., yT ) ≤ y} non-vanishing, a linear
transformation is applied on the maximum. This insight derives a fundamental result in EVT, as a theorem that states
that the distribution of Y after being linearly transformed is
always limited to a few cases [2]. This theorem represents the
Generalized Extreme Value Distribution G(y)

A. Extreme Event Modeling
As mentioned in the introduction, many deep neural networks suffer from overfitting or underfitting in their predictions
when trained with imbalanced data. D. Ding, M. Zhang et
al. coined the term Extreme Event Problem [2] to refer to
this phenomenon. To establish a formal understanding of the
Extreme Event Problem, they have introduced an auxiliary
indicator sequence [2] V1:T = [v1 , ..., vt ] :


yt > 1
1
(1)
vt = 0
yt ∈ [−2 , 1 ]


−1 yt < −2

T →∞

G(y) =



(
exp −(1 − γ1 y)γ
exp (−e−y )

, γ 6= 0, 1 − γ1 y > 0

(3)

,γ = 0

Modeling the Tail in the Distribution. Work extending
the theorem above models the tail distribution of real-world
time-series data with the following equation



y−ξ
1 − F (y) ≈ (1 − F (ξ)) 1 − log G
,y > ξ
f (ξ)

(4)

where y represents a random variable sampled from distribution F , and ξ > 0 is a sufficiently large threshold. Previous
researches have shown that the approximation in equation 4
can fit the tail distribution well [4]. Leveraging the EVT to
effectively model tailed distributions will alleviate the Extreme
Event Problem that deep neural networks often encounter.
Extreme Value Loss. Work to use Extreme Value Theory
as inspiration to impose approximated tailed distribution on
observations provided a classification called Extreme Value
Loss (EVL) [2]. To define the Extreme Value Loss function,
rather than directly modeling the output, we refer to the
extreme event indication defined in equation 1. Using the
tail modeling approximation in equation 4, we can write the
approximation for right extreme event yt as

1 − F (y) ≈ (1 − P (vt = 1)) log G

yt − 1
f (1 )


(5)

, where positive function f is the scale function. If we consider
a binary classification task for determining right extreme
events, we can set our predicted indicator as ut which can
be seen as a hard approximation for (yt − 1 )/f (1 ). The

approximation represents weights that we add on each term in
binary cross entropy,

γ
ut
vt log(ut )
EV L(ut ) = − β0 1 −
γ

γ
(6)
1 − ut
−β1 1 −
(1 − vt ) log(1 − ut )
γ
, where β0 = P (vt = 0) which is the proportion of normal
events in the dataset and P (vt = 1) is the proportion of right
extreme events in the dataset.γ represents the extreme value
index in the approximation and is a hyper-parameter. This
classification loss function is the Extreme Value Loss.The term
β0 will increase the penalty when the model recognizes the
event yt as a normal event, and the term [1 − ut /γ]γ will also
increase the penalty when the model recognizes the extreme
event with little confidence, effectively finding the proper
weights by adding approximation on the tail distribution of
observations through Extreme Value Theory.
Previous work has introduced a deep neural network framework which combines a memory network module which uses
GRU for memorizing the characteristics of extreme events,
and the Extreme Value Loss, for modeling tail distribution.
While we are not considering integrating the memory network module in this study, We find there to be promise in
incorporating the Extreme Value Loss to our distributed deep
learning framework to achieve better prediction performance
for extreme events in time-series data.
Another Paradigm for Extreme Event Prediction. Genetic Programming, a type of evolutionary algorithm, is used
to discover solutions to problems which are hard to solve by
the best human efforts. Inspired by biological evolution and
its fundamental mechanisms, GP software systems implement
an algorithm that uses random mutation, crossover, a fitness
function, and multiple generations of evolution to resolve a
user-defined task. In the context of time-series, genetic algorithms have been used for stock price prediction [5], and for
extreme event predictions in particular, genetic programming
models have been implemented to downscale extreme rainfall
events, showing reasonable accuracy [6]. Although we will
not, in the context of this project, delve into investigating in
detail the fitness of genetic algorithms, it is noteworthy to
mention other types of algorithms are designed to handle the
extreme event problem.
B. Deep Learning time-series Prediction
Traditional time-series method. Traditional methods such
as autoregressive moving average (ARMA) [7] and nonlinear
autoregressive exogenous (NARX) [8] use statistical models
with few parameters to exploit patterns in time-series data.
Deep recurrent neural network. Recently, with the celebrated success of Deep Neural Networks (DNN) in many
fields such as image classification [9], a number of DNNbased techniques have been subsequently developed for timeseries prediction tasks, achieving noticeable improvements
over traditional methods [10]. As a basic component of these
models, the Recurrent Neural Network (RNN) module serves

as an indispensable factor for these note-worthy improvements [11]. Compared with traditional methods, one of the
major advantages of RNN structure is that it enables deep
non-linear modeling of temporal patterns. In recent literature,
some of its variants show even better empirical performance,
such as the well-known Long-Short Term Memory (LSTM)
and Gated Recurrent Unit (GRU) [12, 13], while the latter
appears to be more efficient on smaller and simpler dataset.
Imbalanced data sets. Most previously studied DNN are
observed to have trouble in dealing with data imbalance [2,
14]. Illustratively, let us consider a binary classification task
with its training set including 99% positive samples and only
1% negative samples, which is said to contain data imbalance.
Moreover, imbalance in data will potentially bring any classifier into either of two unexpected situations: a. the model
hardly learns any pattern and simply chooses to recognize all
samples as positive. b. the model memorizes the training set
perfectly while it generalizes poorly to test set.
C. Distributed Deep Learning
Synchronous SGD. Local SGD [15] or Federated Learning
(FL) [16, 17] is a distributed machine learning approach which
enables training on a large corpus of decentralized data located
on devices like mobile phones or IoT devices. While local
SGD does not care about the data privacy, federated learning
assumes that there is no data shared between any client to
prevent data leakage. Original FL requires synchrony between
the server and clients (compute nodes). It requires that each
client send a full model back to the server in each round
and that each client wait for the next computation round. For
large and complicated models, this becomes a main bottleneck
due to the asymmetric property of internet connection and the
different computation power of devices [18, 19].
Asynchronous SGD. Asynchronous training [20, 21] is
widely used in traditional distributed SGD. Hogwild!, one of
the most famous asynchronous SGD algorithms, was introduced in [22] and various variants with a fixed and diminishing step size sequences were introduced in [23, 24, 25]. Typically, asynchronous training converges faster than synchronous
training in real time due to parallelism. This is because in
a synchronized solution compute nodes have to wait for the
slower ones to communicate their updates after which a new
global model can be downloaded by everyone. This causes
high idle waiting times at compute nodes. Asynchronous
training allows compute nodes to continue executing SGD
recursions based on stale global models. For non-convex problems, synchronous training [26] and asynchronous training
with bounded staleness [20] - or in our terminology, bounded
√
delay - achieves the same convergence rate of O(1/ nK),
where n is the number of compute nodes and K is the total
number of gradient computations. Another work from [27]
introduces the asynchronous SGD with increasing sample
sizes to significantly reduce the communication cost. This
work analyses and demonstrates the promise of this new
technique with the convergence rate guarantee for both i.i.d
and heterogeneous data sets.

Event-triggered SGD. Event-triggered SGD is a special
case of asynchronous SGD, where each client independently
computes the SGD recursions and broadcast their SGD computation only to their neighbors. The event-triggered threshold
plays an important role in this algorithm since it decides
the number of communication cost among clients and the
convergence performance. While the work from [28] measures
the difference between the current local model and the new
one as a threshold to broadcast their new models, other
works [29, 30] use the relevant gradient metric from the clients
to decide the period when this SGD computation can be sent
to other neighbors.
We summarise the setting and information exchanged of
some related distributed learning as in Figure 1 and Figure 2.
Note that the information exchanged here can be gradients
or models. In our work, we also verify which information
exchanged can be used with time-series data and deep learning
models.

Fig. 1. Summary information of SGD-based methods for distributed learning
setting.

Fig. 2. Related work and the relationship with our work.

III. P ROPOSED DIRECTIONS OF TECHNICAL COMPONENTS
A. System design
Our approach is based on the Hogwild!
recursion
wt+1 = wt − ηt ∇f (ŵt ; ξt ),

[22, 27, 31]
(7)

where ŵt represents the vector used in computing the gradient
∇f (ŵt ; ξt ) and whose vector entries have been read (one by
one) from an aggregate of a mix of previous updates that led
to wj , j ≤ t. In a single-thread setting where updates are
done in a fully consistent way, i.e. ŵt = wt , yields SGD
with diminishing step sizes {ηt }. The detailed illustration of
Hogwild! algorithm can be found at Figure 3.
Recursion (7) models asynchronous SGD. We define the
amount of asynchronous behavior by function τ (t):

Fig. 3. Hogwild! algorithm in timeline with delay τ

Definition 1. We say that the sequence {wt } is consistent with
a delay function τ if, for all t, vector ŵt includes the aggregate
of the updates up to and including those
P made during the
(t − τ (t))-th iteration* , i.e., ŵt = w0 − j∈U ηj ∇f (ŵj ; ξj )
for some U with {0, 1, . . . , t − τ (t) − 1} ⊆ U.
Our main insight is that the asynchronous SGD framework
based on Hogwild! can resist much larger delays than the
natural delays caused by the network communication infrastructure,
p in fact, it turns out that τ (t) can scale as much as
t/ ln t for strongly convex problems [25, 32]. Here, t
≈
is the current iteration and our clients use the diminishing
step size scheme. This means that recurrence (7) can be
used/exploited in an asynchronous SGD implementation over
distributed local data sets where much more asynchronous behavior is introduced by design. Moreover, the work from [27]
guarantees that rather than having each round execute the
same/constant number of SGD recursions, we can increase
the number of SGD iterations performed locally at each
compute node from round to round. This reduces the amount
of network communication compared to the straightforward
usage of recurrence (7) where each compute node performs a
fixed number of SGD iterations within each round.
Remark 1. Linearly increasing sample sequences: From theory and simulation [27], in order to bootstrap convergence,
it is best to start the training process with larger step sizes
and start with rounds of small size (measured in the number
of local SGD recursions). After that, these rounds should
start increasing in size and should start using smaller and
smaller step sizes for best performance (in terms of minimizing
communication cost). In practice, for non-convex problems, we
can choose the sample size sequence si = O(i), where i is the
current index of the communication round and the correspond√
ing diminishing step size sequence follows η¯i ∼ O(1/ t),
where t is the number of iterations until communication round
i-th. We can observe that for a fixed number of gradient
computations K,
√
PT the number T of communication rounds
satisfies K = j=0 sj . This makes T proportional to K,
rather than proportional to K for a constant sample size
sequence.
* (7) defines the (t + 1)-th iteration, where ηt ∇f (ŵt ; ξt ) represents the
(t + 1)-th update.

B. Overall system design

B. Data set

Our overall system design is mainly based on the distributed
learning framework, shown as in Figure 4 and the linear
increasing sample SGD recursions from the work [27] and
the delay τ from the Hogwild! algorithm [22].

The data this example will be using is the S&P500 file in
the data folder‡ . This file contains the Open, High, Low, Close
prices as well as the daily Volume of the S&P500 Equity Index
from January 2012 to September 2017. We split the subset of
data set from 2012 to 2014 as training data sets and the subset
of data set from 2015 to 2016 as testing data sets. The chosen
stocks are GOOGL, FB, AAPL, AMZN, IBM, NFLX, EBAY.
In this report, we only show the experimental results for AAPL
and AMZN stock.
C. Experimental tasks

Fig. 4. Asynchronous distributed learning framework

As can be seen from Figure 4† , we will use multiple compute nodes joining the training process. Here, each compute
node can have its own local data set (stock data set in our case)
or can share the same data sets. Moreover, each node will run
their local SGD independently with other compute nodes. In
other words, each node will run one RNN model and exchange
the information (gradient or local model) with the central
server independent with other compute nodes. Moreover, the
compute nodes need to follow the delay τ constraints in
Definition 1.
Other settings, such as the RNN model, the extreme event
threshold, the attention layer for RNN model are mainly
referenced from the work [2]. We will specify any modification
or changes, compared to the baseline method [2] in our report
if possible.
Also, we can see two main parts from Figure 4: the compute
node (client side) and the central server. Our asynchronous
setting is mainly conducted on the compute node’s side. In
order to avoid repeating, the pseudo code for the central
server and the compute node can be found in Algorithm 4
and Algorithm 5 from the work [27], respectively.
IV. E XPERIMENTS
A. Environmental setting
For simulating the asynchronous distributed learning with
extreme event modeling, we use multiple threads where each
thread represents one clients joining the training process. The
experiments are mainly conducted on Linux-64bit OS, with
16 cpu processor, 32Gb RAM and 2 NVIDIA GPUs.
† The server and n clients can work in asynchronous fashion with i.i.d or
heterogeneous datasets. At the client side, after running each si /n-iteration
SGD with stepsize ηi . The audience is referred to the paper [27] for a precise
description of the algorithm.

From the above analysis, our goal is try to explore which
is the best way to handle imbalanced data sets, and extreme
events is our user case. And then we focus on distributing the
training process among multiple compute nodes, in order to
reduce not only the training duration but also the communication cost. Hence, our experiments can be as follows:
1) Sensitivity study on extreme events modeling.
2) Implementing and adapting the standard RNN model
with extreme events modeling.
3) Comparing the effective of the extreme events modeling
methods, in terms of the prediction accuracy.
4) Distributing the RNN model with multiple compute
nodes. Our goal is to show the significantly commnucation cost reduction, compared to the baseline local SGD
method [15].
D. Experimental results
Simulation environment. For simulating our proposed distributed learning framework, we mainly run our experiments
on virtual Linux-64bit machine, with 2 cpu processors, 2Gb
RAM and 1 NVIDIA GPU. We use Pytorch library to implement our proposed framework§ . The baseline method is
traditional LSTM on single machine. We will compare our
work with the baseline in terms of prediction accuracy and
training duration.
Objective function. Our experiments mainly focus on nonconvex problems (deep neural networks). For simplicity, we
choose a simple LSTM network for stock prediction application¶ . The basic parameter setting is summarized in Table I.
The prediction performance of our work are shown from
Figure 5 to Figure 10. As can be seen here, we can obtain the
same level of prediction accuracy, compared to the baseline
with single compute nodes. These figures again confirm the
effectiveness of our proposed work|| .
Turing to the speedup ratio, from Table II, we can see that
the speedup ratio increases when we increase the number of
‡ https://github.com/jaungiers/LSTM-Neural-Network-for-Time-SeriesPrediction/tree/master/data
§ The simulation source code can be found at https://drive.google.com/
file/d/1eaWL8rMP5Un3d8J88JWBuQG7qmvZAvvX/view?usp=sharing
¶ The deep network model we used here is Input Layer – 2 LSTM Layers
– 3 Fully Connected Layers for prediction.
|| The experimental results for other stocks can be found at
https://drive.google.com/drive/folders/1j7kssKkaDjt66fHugOEGu
ZmdxjnSz1a?usp=sharing

TABLE I
S UMMARY OF THE PARAMETER SETTING

Parameter
Initial stepsize η̄0
Stepsize scheme
# of data points Nc
# of iterations K
Linear sample size si
Data set
# of nodes n
Sliding window
Regularization λ

Value
0.01
η̄0 √
, β = 0.01
η̄i =
1+β· t
≈ 4000
{288375}
si = a · ip + b (a = 10, p = 1, b = 0)
S&P500 stock
{1, 2, 5, 10}
20
1/Nc

Fig. 7. Stock prediction of our work for 5 compute nodes (Apple stock).

Fig. 5. Stock prediction of our work for 2 compute nodes (Apple stock).

Fig. 8. Stock prediction of our work for 5 compute nodes (Amazon stock).

Fig. 6. Stock prediction of our work for 2 compute nodes (Amazon stock).

compute nodes. However, the speedup ratio does not exactly
scale up to the number of compute nodes. This can explained
by the reason that the central server needs to execute the
aggregation operation (of local models) to issue new global

Fig. 9. Stock prediction of our work for 10 compute nodes (Apple stock).

2) Our experiments are the simulation artifact. Moreover,
the perspective of asynchronous setting is not considered
yet. We will extend this kind of experiment when
possible.
3) The data set S&P500 is quite simple, we also need to
run the experiments for other time-series data sets to
verify the performance of our work.
VI. C ONCLUSION AND F UTURE WORK

Fig. 10. Stock prediction of our work for 10 compute nodes (Amazon stock).
TABLE II
S UMMARY OF SPEEDUP RATIO FOR DIFFERENT NUMBER OF COMPUTE
NODES , COMPARED TO THE BASELINE – SINGLE COMPUTE NODE .

No. of compute node
2
5
10

Speedup
∼1.5
∼4.2
∼8.3

Notes

model for the next computation round. The results confirm
that there is a speedup saturation in the distributed learning
frameworks. In practice, we need to combine with other
methods to increase the bound of speedup saturation.
V. D ISCUSSION
A. Advantages
From this work, we gain some achievements as follows:
1) We successfully implement the distributed learning for
time-series prediction.
2) Our proposed work can significantly reduce the communication cost by using linearly increasing sample sequences and verify its effectiveness by the experimental
results.
3) We also do a sensitivity study on the extreme events.
This work can be a reference for anyone who start
to work on the time-series prediction with attention of
extreme event.
B. Disadvantages
However, due to time limit and experimental environment,
we have some limitation as below:
1) We are trying to integrate the extreme event loss to the
deep learning model. Because this integration is in its
earliest stages, we do not have a chance to measure
and report the effectiveness of extreme event loss to the
prediction accuracy.

In conclusion, we can (i) speedup the training process by
using multiple compute nodes. Moreover, the communication
cost can be significantly reduced by using linearly increasing
sample sequences. We also (ii) do a sensitivity study on
extreme events and its application for time-series prediction.
We also note that (iii) the information exchange in our work
is the model, not the gradient information. It seems that the
distributed learning framework with gradient exchange does
not work well for time-series prediction** . In nearest future,
we plan to extend the experiments for asynchronous settings
as well as integrate our proposed framework with the extreme
event loss function.
R EFERENCES
[1] D. Qi and A. J. Majda, “Using machine learning to
predict extreme events in complex systems,” Proceedings of the
National Academy of Sciences, vol. 117, no. 1, pp. 52–59, 2020.
[Online]. Available: https://www.pnas.org/content/117/1/52
[2] D. Ding, M. Zhang, X. Pan, M. Yang, and X. He, “Modeling
extreme events in time series prediction,” in Proceedings of the
25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, 2019, pp. 1114–1122.
[3] S. Glen, “Extreme value distribution and the extreme value
theory,” 2016, last accessed 05/07/2021. [Online]. Available:
https://www.statisticshowto.com/extreme-value-distribution/
[4] L. de Haan and A. Ferreira, Extreme Value Theory: An Introduction. Springer-Verlag New York, 2006, vol. 1, no. 1.
[5] A. Sachdev and V. Sharma, “Stock forecasting model based on
combined fuzzy time series and genetic algorithm,” in 2015
International Conference on Computational Intelligence and
Communication Networks (CICN), 2015, pp. 1303–1307.
[6] S. Hadipour, S. Shahid, S. B. Harun, and X. Wang, “Genetic
programming for downscaling extreme rainfall events,” in 2013
1st International Conference on Artificial Intelligence, Modelling and Simulation, 2013, pp. 331–334.
[7] P. Moran, “Hypothesis testing in time series analysis,” 1951.
[8] T. Lin, B. G. Horne, P. Tino, and C. L. Giles, “Learning longterm dependencies in narx recurrent neural networks,” IEEE
Transactions on Neural Networks, vol. 7, no. 6, pp. 1329–1338,
1996.
[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” Advances in
neural information processing systems, vol. 25, pp. 1097–1105,
2012.
[10] S. Dasgupta and T. Osogami, “Nonlinear dynamic boltzmann
machines for time-series prediction,” in Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 31, no. 1, 2017.
** This is the observation from our experiments. It seems that model
averaging is better than gradient averaging for time-series prediction deep
learning models. This observation can be explained by the gradient vanishing
or gradient explosion for RNN models, which makes the gradient averaging
less practical in this case. In practice, we can use gradient clipping or better
activation functions to alleviate this issue.

[11] L. Yan, A. Elgamal, and G. W. Cottrell, “Substructure vibration
narx neural network approach for statistical damage inference,”
Journal of Engineering Mechanics, vol. 139, no. 6, pp. 737–747,
2013.
[12] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[13] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical
evaluation of gated recurrent neural networks on sequence
modeling,” arXiv preprint arXiv:1412.3555, 2014.
[14] X. Wang, X. He, M. Wang, F. Feng, and T.-S. Chua, “Neural
graph collaborative filtering,” in Proceedings of the 42nd international ACM SIGIR conference on Research and development
in Information Retrieval, 2019, pp. 165–174.
[15] S. U. Stich, “Local sgd converges fast and communicates little,”
arXiv preprint arXiv:1805.09767, 2018.
[16] H. B. McMahan, E. Moore, D. Ramage, and B. A. y Arcas,
“Federated learning of deep networks using model averaging,”
ICLR Workshop Track, 2016.
[17] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B.
McMahan et al., “Towards federated learning at scale: System
design,” arXiv preprint arXiv:1902.01046, 2019.
[18] Y. Chen, X. Sun, and Y. Jin, “Communication-efficient
federated deep learning with asynchronous model update
and temporally weighted aggregation,” arXiv preprint, 2019.
[Online]. Available: https://arxiv.org/pdf/1903.07424.pdf
[19] J. Konečnỳ, H. B. McMahan, D. Ramage, and P. Richtárik,
“Federated optimization: Distributed machine learning for ondevice intelligence,” arXiv preprint arXiv:1610.02527, 2016.
[20] X. Lian, Y. Huang, Y. Li, and J. Liu, “Asynchronous parallel
stochastic gradient for nonconvex optimization,” in Advances in
Neural Information Processing Systems, 2015, pp. 2737–2745.
[21] S. Zheng, Q. Meng, T. Wang, W. Chen, N. Yu, Z.-M. Ma,
and T.-Y. Liu, “Asynchronous stochastic gradient descent with
delay compensation,” in Proceedings of the 34th International
Conference on Machine Learning-Volume 70.
JMLR. org,
2017, pp. 4120–4129.
[22] B. Recht, C. Re, S. Wright, and F. Niu, “Hogwild: A lockfree approach to parallelizing stochastic gradient descent,” in
Advances in neural information processing systems, 2011, pp.
693–701.
[23] C. M. De Sa, C. Zhang, K. Olukotun, and C. Ré, “Taming the
wild: A unified analysis of hogwild-style algorithms,” in NIPS,
2015, pp. 2674–2682.
[24] R. Leblond, F. Pedregosa, and S. Lacoste-Julien, “Improved
asynchronous parallel optimization analysis for stochastic incremental methods,” JMLR, vol. 19, no. 1, pp. 3140–3207, 2018.
[25] L. M. Nguyen, P. H. Nguyen, M. van Dijk, P. Richtárik,
K. Scheinberg, and M. Takác, “SGD and hogwild! convergence
without the bounded gradients assumption,” in Proceedings of
the 35th International Conference on Machine Learning, ICML
2018, 2018, pp. 3747–3755.
[26] H. Z. Saeed Ghadimi, Guanghui Lan, “Mini-batch stochastic
approximation methods for nonconvex stochastic composite
optimization,” arXiv preprint arxiv:1308.6594, 2013.
[27] M. van Dijk, N. V. Nguyen, T. N. Nguyen, L. M. Nguyen,
Q. Tran-Dinh, and P. H. Nguyen, “Hogwild! over distributed
local data sets with linearly increasing mini-batch sizes,” arXiv
preprint arXiv:2010.14763, 2020.
[28] J. George and P. Gurram, “Distributed deep learning with eventtriggered communication,” arXiv preprint arXiv:1909.05020,
2019.
[29] K. Hsieh, A. Harlap, N. Vijaykumar, D. Konomis, G. R. Ganger,
P. B. Gibbons, and O. Mutlu, “Gaia: Geo-distributed machine learning approaching {LAN} speeds,” in 14th {USENIX}
Symposium on Networked Systems Design and Implementation
({NSDI} 17), 2017, pp. 629–647.

[30] L. Wang, W. Wang, and B. Li, “Cmfl: Mitigating communication overhead for federated learning.” IEEE International
Conference on Distributed Computing Systems., 2019.
[31] H. Robbins and S. Monro, “A stochastic approximation
method,” The annals of mathematical statistics, pp. 400–407,
1951.
[32] L. M. Nguyen, P. H. Nguyen, P. Richtárik, K. Scheinberg,
M. Takáč, and M. van Dijk, “New convergence aspects of
stochastic gradient algorithms,” Journal of Machine Learning
Research, vol. 20, no. 176, pp. 1–49, 2019.

