arXiv:2009.09899v1 [cs.CV] 5 Sep 2020

Clustering COVID-19 Lung Scans
Jacob Householder, Andrew Householder, and John Paul Gomez-Reed
Mentors: Fredrick Park and Shuai Zhang*
Mathematics Department, Whittier College
Whittier, CA 90601
*Qualcomm AI Research

Friday 4th September, 2020
Abstract
With the recent outbreak of COVID-19, creating a means to stop it’s
spread and eventually develop a vaccine are the most important and challenging tasks that the scientific community is facing right now. The first
step towards these goals is to correctly identify a patient that is infected
with the virus. Our group applied an unsupervised machine learning technique to identify COVID-19 cases. This is an important topic as COVID19 is a novel disease currently being studied in detail and our methodology
has the potential to reveal important differences between it and other viral
pneumonia. This could then, in turn, enable doctors to more confidently
help each patient. Our experiments utilize Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), and
the recently developed Robust Continuous Clustering algorithm (RCC).
We display the performance of RCC in identifying COVID-19 patients
and its ability to compete with other unsupervised algorithms, namely KMeans++ (KM++). Using a COVID-19 Radiography dataset, we found
that RCC outperformed KM++; we used the Adjusted Mutual Information Score (AMI) in order to measure the effectiveness of both algorithms.
The AMI for the two and three class cases of KM++ were 0.0250 and
0.054, respectively. In comparison, RCC scored 0.5044 in the two class
case and 0.267 in the three class case, clearly showing RCC as the superior algorithm. This not only opens new possible applications of RCC,
but it could potentially aid in the creation of a new tool for COVID-19
identification.

1

Introduction

The COVID-19 outbreak has redirected the efforts of the scientific community
as a whole to help relieve the pressures on medical staff, create new effective
treatments, limit the spread of the disease, and, most importantly, create a vaccine to end the pandemic. Our group has focused on the first issue, in trying to
1

aide in the identification of COVID-19 afflicted individual from non-COVID-19
viral pneumonia or healthy individuals. Using unsupervised machine learning,
or clustering techniques, we aim to create a method that can diagnose patients
given a CT lung scan. This is an important, novel, and difficult issue because of
the nature of clustering algorithms; these algorithms are extremely sensitive and
require a great deal of fine-tuning in order to work properly. Clustering is an
essential data analysis task where objects are placed into groups based on similarity of features. Popular supervised classification models would likely yield
more accurate results, but implementing supervised learning requires labeled
data in every application of a given method–a potential limiting factor for novel
data [12, 20]. Creating an unsupervised model with high accuracy would be a
significant accomplishment because of the greater range of real-world applicability the model would have. This is namely due to the fact that it can process
unlabeled data and because of the objective nature of its solution, which simply
deciphers data for what it is and not by reference to any labeling [4]. The work
we have done focuses on using clustering as a means to identify patterns of pulmonary tissue sequelae in X-ray image visualizations via a 2-D representation
of the data. This is an important stipulation as it provides the potential for
direct application of our experiment to real world scenarios. Lastly, the proposed method can be used as an important first step in creating labeled data
for supervised classification.
In this work, we utilize the new COVID-19 Radiography Database by Chowdhury et al. [11, 3]. The dataset is composed of X-ray and CT scan images separated into three classes: COVID-19 cases, Viral Pneumonia cases, and Normal
lungs. We applied the K-Means++ (KM++) [1] and Robust Continuous Clustering (RCC) [13] algorithms to this database of X-ray images of patients labeled
by cause of illness with the goal of exploring the clustering properties latent in
the images. We conducted two tests using this data, the first was processing
the dataset as it stands; while the second was creating a binary class dataset
by combining both the Viral Pneumonia and Healthy lung classes into one nonCOVID-19 class. The images were then isolated from their class labelling in
processing and the classes were only used as a means to label the visualization/graphical representation. As KM++ was found to be one of the top 10
algorithms in unsupervised machine learning [8], we wanted to test whether the
claims of A. Shah and V. Koltun were true [13] and seen if RCC could compete
with KM++ in terms of performance and clustering capabilities. Through our
work, we have found that RCC has dramatically increased clustering performance in the COVID-19 database. We attribute this performance gain to the
connectivity structure that the algorithm relies on [13]. It is able to identify
the group of COVID-19 pneumonia cases with high accuracy, suggesting that
there is information encoded in the image data set that differentiates cases of
COVID-19 from other types of viral pneumonia. Figure 1 showcases lung scans
of patients where the top row contains scans of those with COVID-19. The
bottom row consists of non-COVID-19 cases, meaning they are patients with
either normal lungs or viral pneumonia.
The database used is composed of many different image databases of recently
2

Figure 1:
developed COVID-19 and lung/CT scan databases of varying quality, resolution,
and focuses [11]. The authors of the database complied the images to test supervised learning techniques and published their database on Kaggle to aid other
researchers. Despite the varied origins and quality of the images, our findings are
nonetheless interesting from an exploratory perspective as they illuminate that
there are inherent differences between COVID-19 and non-COVID-19 lung/CT
scans.
This has no impact on the optimization of the RCC algorithm as it is not
parameterized by the number of expected clusters. It does have an impact on
our performance metrics.
Visualizing high dimensional data is a difficult challenge in many data science
problems. Techniques like Principal Component Analysis (PCA) [10, 6] have
been used for dimension reduction for quite some time. Recently, the method
of mapping higher dimensional data to a lower dimensional representation that
preserves neighbor structure in t-Distributed Stochastic Neighbor Embedding
(t-SNE) was introduced by van der Maaten and Hinton [16]. The t-SNE method
has the ability to exhibit global arrangement of data in the sense of clusters at a
varying range of scales. This is extremely useful since high dimensional data, like
image data, can be viewed in as few as 2 or 3 dimensions with similar images
clustering in this lower dimensional space. More details and related work to
t-SNE can be found in [5, 14, 17, 15].
To evaluate clustering performance of our experiment, we utilize the Adjusted Mutual Information (AMI) [9, 18, 19] criterion. AMI is useful in this
setting as we have access to the ground truth categorization of the data set.
This criterion gives us an insight into the efficacy of our clustering routines
while accounting for random chance. We will go into more details later.
The images were PCA reduced to 80 components and then normalized before
being fed into the RCC routine. The RCC routine has a connectivity structure
built with the Mutual k-Nearest Neighbors algorithm [2], which we set to k=30.

3

We noticed that image normalization is very important to obtain convergence of
the RCC algorithm and is done so in the empirical results of the original paper,
although not explicitly mentioned.

2

Methods

We make use of four main data processing techniques in our experiment. These
are Principal Component Analysis (PCA) for dimensionality reduction, t-Distributed
Stochastic Neighbor Embedding (t-SNE) [16] for two-dimensional visualization,
and K-Means++ (KM++) along with Robust Continuous Clustering (RCC) as
clustering methods. Using these techniques, we are able to process the vector
representations of our images in a meaningful manner.
2.0.1

t-SNE

t-SNE is a dimension reduction technique that creates a low dimension representation of high dimensional data points. This technique is primarily used
for visualization in two or three dimensions as it preserves pairwise distances
well while reducing dimension [16]. This is accomplished by computing the Euclidean distances between data points and constructing probability distributions
in both high and low dimensions such that if two high dimensional data points
are close together it is likely that their low dimensional representations are as
well. We start by defining the probabilities as follows:

exp −||xi − xj ||2 /2σi2
(1)
pi|j = P
2
2
k6=i exp (−||xi − xk || /2σi )

exp −||yi − yj ||2
qi|j = P
2
k6=i exp (−||yi − yk || )
where X = {x1 , x2 , ..., xN } are the initial data points and Y = {y1 , y2 , ..., yN }
are their low dimensional counterparts, paired by index, σi which is computed
using bisection search to best represent the distances of the data points. These
conditional probabilities are then symmetrized for each pair of data points in
order to obtain the pairwise similarity:
pij =

pi|j + pj|i
2N

(2)

in the high dimension, and the same for the low dimensional conditional probability q. The algorithm then calls for minimization of the Kullback-Leibler
divergence between the two pairwise probabilities:
 
X
pij
C = KL(P ||Q) =
pij log
(3)
qij
i6=j

4

using gradient descent. The gradient is as follows:
X
δC
yi − yj
.
=4
(pij − qij )qij P
(1
+ ||yk − yl ||2 )
δyi
k6=l
j6=i

This technique allows for visualization of high dimensional data that enables a
visual and spacial intuition into the relationship of the data points and performance of clustering and classification algorithms.

2.1

RCC

RCC is a recently developed clustering technique that evolves a continuous
representation of the input data set such that similar data points form tight
clusters [13]. The objective function follows as:
C(U ) =

n
λ X
1X
||xi − uj ||22 +
wp,q ρ(||up − uq ||2 )
2 i=1
2

(4)

p,q∈ξ

µy 2
µ + y2
D
where X = [x1 , x2 , ...], x ∈ R , and U is the set of representative points, which
are initially set as X. Most importantly, ξ is the set of edges connecting data
points to one another–this is constructed by mutual-K Nearest Neighbors (mKNN). By m-KNN, in order for two or more points to be grouped together,
both points must consider each other as the nearest point, thus the points
have a “mutual neighbor.” This allows RCC to have a parameter that defines
the maximum number of neighbors to look for, highlighting another feature of
RCC: the number of clusters does not need to be known ahead of time, setting it
apart from KM++ greatly. The ρ term is used to represent estimator functions,
in our case the Geman-McClure estimator is used with scale parameter µ. The
term λ balances the contributions of terms to the objective. Below is another
form of the RCC objective function, which takes into account the connections
formed by m-KNN:
ρ(y) =

C(U, L) =

n
1X
λ X
||xi − uj ||22 +
wp,q (lp,q ||up − uq ||2 + Ψ(lp,q ))
2 i=1
2

(5)

p,q∈ξ

p
Ψ(lp,q ) = µ( lp,q − 1)2 .
RCC can be optimized via alternating minimization, which sees the formula
become:
λ X
1
wp,q lp,q (ep − eq )(ep − eq )>
argminU ||X − U ||2F +
2
2

(6)

(p,q)∈ξ

while the optimal value of lp,q becomes:

2
µ
lp,q =
.
µ||up − uq ||22
5

(7)

3

Metric - Adjusted Mutual Information

We calculated the Adjusted Mutual Information Score (AMI) between the dataset’s
ground truth labeling and the labeling produced by our algorithm to evaluate
clustering performance. Mutual Information (MI) is an entropy based measure
which quantifies the amount of information given by a random variable in a
particular clustering based on the probability of a particular point lying in any
given cluster. The formulation of MI is given by:
M I(U, V ) =

R X
C
X

PU V (i, j)log

i=1 i=1

PU V (i, j)
PU (i)PV (j)

(8)

where Ui and Vj are clusters in separate partitions of the same set of data ranging
from {U1 , ..., UR } and {V1 , ..., VC } respectively. The probability of a data point
lying in a given cluster is denoted by PU and PV , and the joint probability
between partitions is labeled PU V . To obtain the AMI, the adjustment of an
index for chance proposed by Hubert and Arabie [7] is then applied to the
formulation of the MI:
Adjusted index =

index − expected index
,
max index − expected index

Using this formula, a number between 0 and 1 is obtained, which determines
the algorithm’s effectiveness is displayed. The closer the number is to 1, the
more effective that algorithm is at clustering the given dataset.

4

Main results

It is clear that the COVID-19 data points naturally cluster with our experiment.
This was initially observed in the t-SNE reduction of the true labeling of the
dataset in Figure 2 (left) where COVID-19 points are colored red. The set of
COVID-19 data points are similar enough to each other to have been grouped
together, and different enough from the other cases to have been remapped to
the exterior of the set of all considered data points. This suggests that there is
enough easily accessible information to cluster our data without complex feature
engineering. So the first cluster analysis technique to try is KM++, a robust
technique that affords an established baseline. The results from KM++, which
are discussed in detail below, produced a rather unsuccessful clustering. Next
up is RCC, which produced very intriguing results. It was able to successfully
identify the COVID-19 cases as separate from the other cases, yet was unable
to separate the viral pneumonia from the healthy cases in any significant way.
We suspect that this behavior is due to the similarity of COVID-19 images and
the connectivity structure that RCC uses. Nonetheless, the result is notable
and quite useful in its ability to discern between COVID-19 and non-COVID19. This is significant as KM++ was unable to discriminate between any of the
cases.
6

Figure 2: 3 class labeling

Figure 3: 2 class labeling
Now the AMI score for each of the clustering algorithms shows that RCC
produces a significantly better clustering in this case.
# Classes
2
2
3
3

Method
KM++
RCC
KM++
RCC

AMI
0.0250
0.5044
0.054
0.267

In order to interpret the data, we define a mapping from cluster labels to
true labels. This map is defined in a manner similar to cluster purity, where
labels are assigned to clusters by majority rule. Using such a mapping enables
us to see if like elements cluster; while also not penalizing for increased cluster
count. This mapping allows us to consider the confusion matrix of the clustering
7

results.

Table 1: RCC Confusion Matrix 2-Classes
True COVID-19
True No COVID-19

pred. COVID-19
169
13

pred No COVID-19
50
2673

Table 2: RCC Confusion Matrix 3-classes
True COVID-19
True Viral Pneum.
True Healthy

pred. COVID-19
169
4
9

pred Viral Pneum.
10
33
5

pred Healthy
40
1304
1331

In the tables above, the confusion matrices for both Figure 2 and 3 are shown.
For Table 1, you can see that the sensitivity is 92.8%, while the specificity is
98.1%; as for T able 2, here are the sensitivity and specificity for each category
(COVID-19, Viral Pneumonia, Healthy) are 92.8% and 51%, 68.75% and 96.8%,
and 49.75% and 93.5%, respectively. This displays that RCC is more successful
in two class situations, rather than three class situation. This fact can be seen
in the three class case’s sensitivity and specificity being 70.43% and 80.43%, a
bit less favorable performance than the two class case.
Figures 2 and 3 show the clustering of the COVID-19 dataset by KM++
(center) and RCC (right). Figure 2 shows the dataset and KM++ prediction in
3 clusters (Viral Pneumonia, Normal, or COVID-19), while Figure 3 shows the
true dataset and KM++ prediction using 2 clusters (non-COVID19 or COVID19); both figures show the same clustering prediction by RCC. In both cases, the
performance of KM++ is left wanting in this dataset as the AMI for each figure
falls below 0.1. On the other hand, RCC performs remarkably well in both
cases, where it scores 0.5044 and 0.267 respectively. It can be seen that RCC
does not separate data into three or even two distinct classes, but it is able to
distinguish COVID-19 cases from other cases quite well. As seen in both figures,
it is apparent that the “red” cluster captures most of the true COVID-19, while
the “purple” cluster captures much of the non-COVID-19 cases, whether or not
they were grouped as such initially. Another thing of note is that, other clusters
predicted by RCC are able to pick out some COVID-19 cases within the larger
purple cluster as well, making note of these outliers in a way KM++ does not.

8

5

Conclusions and Future Work

The results from our experiment showed that the COVID-19 data points in
the data set are clearly different from the non-COVID-19 cases. Even though
RCC does not separate the COVID-19 cases and non-COVID-19 cases into two
distinct classes, it is still able to create clusters that distinguish both cases.
Thus, revealing that there is latent information within COVID-19 images and
there is an underlying similarity between many COVID-19 cases.
RCC likely performs well because it uses a more refined measure of pairwise
similarity than simply `2 distance. The connectivity structure constructed by
m-KNN places a stricter requirement on determining similarities between data
points, enabling the algorithm to perform more informed data clustering.
Potential limitations of our work come from the relative ambiguity as to the
reasoning for classification and the, overall, small sample of COVID-19 cases in
the data which may not be representative of all real COVID-19 cases. It can
be inferred that the COVID-19 cases present in the dataset are likely severe
as they warranted a professional lung scan, meaning that our findings may be
limited to only the more severe COVID-19 cases. Considering this possibility,
our findings are nonetheless relevant as they still display an inherent similarity
between the COVID-19 cases in the data and distinguishes them from other viral
pneumonia. In order to combat this potential limitation, future implementations
of these techniques can be performed on cleaner, more rigorously constructed
datasets.
One direction for future work is to explore the effectiveness of different feature engineering techniques on this data set. Consideration of other dimensionality reduction techniques, other than PCA, that may be more finely tuned to
the COVID-19 dataset is a worthwhile future route. Moreover, as new data
emerges, we can further refine our experimental methodology.

Acknowledgments
We would foremost like to thank the PIC Math Program for this unique and
rewarding opportunity. PIC Math is a program of the Mathematical Association of America (MAA). Support for this MAA program is provided by the
National Science Foundation (NSF grant DMS-1722275) and the National Security Agency (NSA). We thank Dr. Shuai Zhang for proposing this interesting
and important problem and for contributing useful suggestions throughout the
project. We also thank Dr. Fred Park for his dedication, guidance, and support
throughout this work. Lastly, we would also like to thank Qualcomm, Whittier
College, NSF, MAA, and NSA, for their assistance in our work.

References
[1] D. Arthur and S. Vassilvitskii, k-means++: the advantages of careful
seeding, p 1027–1035, in SODA’07: proceedings of the eighteenth annual
9

ACM-SIAM symposium on discrete algorithms. Society for Industrial and
Applied Mathematics, Philadelphia, PA, 2007.
[2] M. Brito, E. Chavez, A. Quiroz, and J. Yukich, Connectivity of the
mutual k-nearest-neighbor graph in clustering and outlier detection, Statistics & Probability Letters, 35 (1997), pp. 33–42.
[3] M. E. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M. A.
Kadir, Z. B. Mahbub, K. R. Islam, M. S. Khan, A. Iqbal, N. AlEmadi, et al., Can ai help in screening viral and covid-19 pneumonia?,
arXiv preprint arXiv:2003.13145, (2020).
[4] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent,
and S. Bengio, Why does unsupervised pre-training help deep learning?,
Journal of Machine Learning Research, 11 (2010), pp. 625–660, http://
jmlr.org/papers/v11/erhan10a.html.
[5] G. E. Hinton and S. T. Roweis, Stochastic neighbor embedding, in
Advances in neural information processing systems, 2003, pp. 857–864.
[6] H. Hotelling, Analysis of a complex of statistical variables into principal
components., Journal of educational psychology, 24 (1933), p. 417.
[7] L. Hubert and P. Arabie, Comparing partitions, Journal of Classification, 2 (1985), pp. 193–218, https://doi.org/10.1007/BF01908075.
[8] A. K. Jain, Data clustering: 50 years beyond k-means, Pattern recognition
letters, 31 (2010), pp. 651–666.
[9] M. Meilă, Comparing clusteringsan information based distance, Journal
of multivariate analysis, 98 (2007), pp. 873–895.
[10] K. Pearson, On lines of closes fit to system of points in space, london, e
dinb, Dublin Philos. Mag. J. Sci, 2 (1901), pp. 559–572.
[11] T. Rahman, M. Chowdhury, and A. Khandakar, Covid-19 radiography database, 2020.
[12] L. Schmarje, M. Santarossa, S.-M. Schrder, and R. Koch, A
survey on semi-, self- and unsupervised techniques in image classification,
(2020).
[13] S. A. Shah and V. Koltun, Robust continuous clustering,
Proceedings of the National Academy of Sciences, 114 (2017),
pp. 9814–9819, https://doi.org/10.1073/pnas.1700770114, https://
www.pnas.org/content/114/37/9814, https://arxiv.org/abs/https:
//www.pnas.org/content/114/37/9814.full.pdf.
[14] L. van der Maaten, Learning a parametric embedding by preserving local
structure, in Artificial Intelligence and Statistics, 2009, pp. 384–391.

10

[15] L. van der Maaten, Accelerating t-sne using tree-based algorithms, Journal of machine learning research, 15 (2014), pp. 3221–3245.
[16] L. van der Maaten and G. Hinton, Viualizing data using t-sne, Journal
of Machine Learning Research, 9 (2008), pp. 2579–2605.
[17] L. van der Maaten and G. Hinton, Visualizing non-metric similarities
in multiple maps, Machine learning, 87 (2012), pp. 33–55.
[18] N. X. Vinh, J. Epps, and J. Bailey, Information theoretic measures for
clusterings comparison: is a correction for chance necessary?, in Proceedings of the 26th annual international conference on machine learning, 2009,
pp. 1073–1080.
[19] N. X. Vinh, J. Epps, and J. Bailey, Information theoretic measures for
clusterings comparison: Variants, properties, normalization and correction
for chance, The Journal of Machine Learning Research, 11 (2010), pp. 2837–
2854.
[20] R. Xu and D. Wunsch, Survey of clustering algorithms, IEEE Transactions on neural networks, 16 (2005), pp. 645–678.

11

