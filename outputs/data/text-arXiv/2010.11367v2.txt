TeX-Graph: Coupled tensor-matrix knowledge-graph embedding for COVID-19
drug repurposing
Charilaos I. Kanatsoulis

∗

arXiv:2010.11367v2 [cs.SI] 25 Oct 2020

Abstract
Knowledge graphs (KGs) are powerful tools that codify relational behaviour between entities in knowledge
bases. KGs can simultaneously model many different
types of subject-predicate-object and higher-order relations. As such, they offer a flexible modeling framework that has been applied to many areas, including
biology and pharmacology – most recently, in the fight
against COVID-19. The flexibility of KG modeling is
both a blessing and a challenge from the learning point
of view. In this paper we propose a novel coupled tensormatrix framework for KG embedding. We leverage tensor factorization tools to learn concise representations
of entities and relations in knowledge bases and employ
these representations to perform drug repurposing for
COVID-19. Our proposed framework is principled, elegant, and achieves 100% improvement over the best
baseline in the COVID-19 drug repurposing task using
a recently developed biological KG.
Keywords— knowledge graphs, tensor, drug repurposing, COVID-19, embedding, network

1

Introduction

How does COVID-19 relate to better-studied viral infections
and biological mechanisms? Can we use existing drugs to
effectively treat COVID-19 symptoms? Since the COVID-19
pandemic has disrupted our lives, there is a pressing need to
answer such questions, and COVID-19 research has swiftly
risen to the top of the scientific agenda, worldwide. While
these questions will ultimately be answered by medical
experts, data-driven methods can help to cut-down the
immense search space, thus helping to accelerate progress
and optimize the allocation of precious research resources.
In this paper, our goal is to derive such a method by using
network science and multi-view analysis tools.
Networks are powerful abstractions that model interactions between the entities of a system [3]. Networks and network science offer concise and informative modeling, analysis
and processing for various biological, engineering and social
systems, to name a few [11, 22]. Networks are usually rep∗ Electrical and Computer Engineering Department, University
of Minnesota. Email: kanat003@umn.edu, † Electrical and Computer Engineering Department, University of Virginia. Email:
nikos@virginia.edu

Nicholas D. Sidiropoulos

†

resented by graphs, that are defined by a set of nodes and
a set of edges connecting pairs of nodes. The entities of a
system are usually represented by the nodes of the graph,
and the interactions by the edges.
A knowledge graph (KG) models the relational behavior of various entities in knowledge bases. A KG is heterogeneous in the sense that it models interactions between
entities of different type, e.g., drugs and diseases, and is also
a multidimensional network (edge-labeled multi-graph) [4],
since the edges (interactions) that connect the nodes (entities) can be multiple and also of different type. Knowledge graphs (KGs) have recently attracted significant attention due to their applicability to various science and engineering tasks. For instance, popular knowledge graphs are
YAGO [32], DBpedia [1], NELL [8], Freebase [5], and the
Google KG [29]. A recent trend codifies knowledge bases
of biomedical components and processes, such as genes, diseases and drugs into KGs e.g., [14, 15, 17]. KGs can model
any relations of the form subject-predicate-object, as well
as higher-order generalizations. However, this broad modeling freedom can sometimes be a challenge, as the entities
can be very diverse and the dimensions of the KG can turn
prohibitively large.
A common way to exploit KGs for data mining and machine learning applications is via knowledge graph embedding.
KG embedding aims to extract meaningful low dimensional
representations of the entities and the relations present in
a KG. A plethora of methods have been proposed to perform KG embedding [2, 6, 12, 18, 20, 21, 23, 25, 26, 30, 33, 34].
The most popular among them adopt a single-layer perceptron or neural network approach e.g., [6, 21, 30, 33, 34]. Various tensor factorization models have also been proposed,
e.g., [2, 12, 20, 23, 25]. Matrix factorization is also a tool that
has been utilized for KG embedding, e.g., [18, 26].
In this paper we propose TeX-Graph, a novel coupled tensor-matrix framework to perform KG embedding.
The proposed KG coupled tensor-matrix modeling extracts
meaningful information from a set of diverse entities with
multi-modal interactions in a principled and concise manner. TeX-Graph avoids modeling inefficiencies in previously
proposed tensor models, and relative to neural network approaches it offers a principled and effective way to produce
unique KG representations. The proposed framework is used
for drug repurposing, a pivotal tool in the fight against
COVID-19 and other diseases. Learning concise representations for drug compounds, diseases, and the relations between them, our approach allows for link prediction between

drug compounds and COVID-19 or other diseases. The impact is critical. First, compound repurposing enables drug
design that drastically reduces the design exploration cycle and the failure rate. Second, it markedly reduces drug
development cost, as developing new therapeutic drugs is
tremendously expensive.
The contributions of our work can be summarized as
follows:

Figure 1: The columns, rows and fibers of a third-order
tensor.

• Novel KG modeling: We propose a principled coupled tensor-matrix model tailored to KG needs for efficient and parsimonious representations.
• Analysis: The TeX-Graph embeddings are unique and
permutation invariant, a property which is important
for consistency and necessary for interpretability.

Figure 2: The horizontal, vertical, and frontal slabs of
a third-order tensor.

• Algorithm: We design a scalable algorithmic framework with lightweight updates, that can effectively handle very large KGs.

2

• Application: The proposed framework is developed
to perform drug repurposing, a pivotal task in the fight
against COVID-19.
• Performance: TeX-Graph achieves 100% performance
improvement compared to the best available baseline
for COVID-19 drug repurposing using a recently developed COVID-19 KG.
Reproducibility: The DRKG dataset used in the experiments is publicly available; we will also release our code
upon publication of the paper.
Notation: Our notation is summarized in Table 1.

Table 1: Overview of notation.
x, y, z
(m, n), (h, r, t)
x, y, z
A, B, C
X, Y , Z
S, Sn+ , Sn−
A(:, f )
A(i, :)
Xk
AT
kAkF

,
,
,
,
,
,
,
,
,
,
,
,

◦
∗
diag(x)
bxc

,
,
,
,

nnz

,

scalars
ordered tuple
vectors
matrices
tensors
sets
f -th column of matrix A
i-th row of matrix A
k-th frontal slab of tensor X
transpose of matrix A
Frobenius norm of matrix A
Khatri-Rao
(columnwise
Kronecker) product
outer product
Hadamard product
diagonal matrix of vector x
largest integer that is less
than or equal to x
number of non-zeros

Preliminaries

In order to facilitate the upcoming discussion we now discuss
some tensor algebra preliminaries. For more background on
tensors the reader is referred to [19, 28].
A third-order tensor X ∈ RI×J×K is a three-way array
indexed by i, j, k with elements X(i, j, k). It has three mode
types – columns X(i, :, k) (: is used to denote all relevant
index values for that mode, i.e., from beginning to end),
rows X(:, j, k), and fibers X(i, j, :) – see Fig. 1. A third
order tensor can also be described by a set of matrices
(slabs), i.e., horizontal X(i, :, :), vertical X(:, j, :) and frontal
slabs X(:, :, k) – see Fig. 2. A rank-one third order tensor
Z ∈ RI×J×K is defined as the outer product of three vectors.
Recall that a rank one matrix is the outer product of two
vectors. Any third order tensor can be decomposed into a
sum of three-way outer products (rank one tensors) as:
(2.1)

X=

F
X

A(:, f ) ◦ B(:, f ) ◦ C(:, f ),

f =1

where A ∈ RI×F , B ∈ RJ×F , C ∈ RK×F are matrices
collecting the respective mode factors, i.e., hold in their
columns the vectors involved in the F three-way outer
products. The above expression is known as the polyadic
decomposition (PD) of a third-order tensor. If F is the
minimum number of outer products required to synthesize
X, then F is the rank of tensor X and the decomposition
is known as the canonical polyadic decomposition (CPD) or
parallel factor analysis (PARAFAC) [13]. For the rest of the
paper we use the notation X = JA, B, CK to denote the
CPD of the tensor.
A striking property of the CPD is that it is essentially
unique under mild conditions, even if the rank is higher than
the dimensions– see [9] for a generic result.
A tensor can be represented in a matrix form by
employing the matricization operation. There are three
common ways to matricize (or unfold) a third-order tensor,
by stacking columns, rows, or fibers of the tensor to form a
matrix. To be more precise let:
(2.2)

X(:, :, k) = X k ∈ RI×J ,

where X k are the frontal slabs of tensor X and in the
context of this paper they model adjacency matrices. Then
the mode-1, mode-2 and mode-3 unfoldings of X are:
h
iT
(2.3) X (1) = X 1 , . . . , X K
= (C B)AT ∈ RJK×I ,

(2.4)

X

(2)

h
i
T
T
= X 1 , . . . , X K = (C

A)B T ∈ RIK×J ,

(2.5)


X

(3)



X 1 (:, 1), · · · , X K (:, 1)


..
 = (B
=
.


1
K
X (:, J), · · · , X (:, J)

Figure 3: Schematic representation of biological KG.
A)C

T

IJ×K

∈R

,

Another important tensor model is the coupled CPD.
In coupled CPD we are interested in decomposing an array
of tensors that share at least one common latent factor.
In particular, consider a collection of N tensors X n ∈
RI×Jn ×Kn , n ∈ {1, . . . N }. The rank-F coupled CPD of
{X n } can be expressed as:
(2.6)

X n = JA, Bn , Cn K , n ∈ {1, . . . N },

where A ∈ RI×F is the common factor and Bn ∈
RJn ×F , Cn ∈ RKn ×F are unshared factors. The coupled
CPD is also unique under certain conditions, even if individual CPDs of Xn are not unique. In this work we will use
the following uniqueness theorem for coupled CPD:
Theorem 1. [31, p. 510] Consider the coupled CPD as
expressed in (2.6). The decomposition is essentially unique
if:
1. A has full column rank,
2. G has full column rank,
where G is defined as:


C2 (C1 ) C2 (B1 )


..

(2.7)
G=
.


C2 (CN ) C2 (BN )
1

∈ R4

PN

n=1

1 F (F −1)
Jn (Jn −1)Kn (Kn −1)× 2

3.1

,

and C2 (Cn ) is the second compound matrix of Cn – see
[10, p. 861-862]. In the context of coupled CPD, essential
uniqueness corresponds to A being unique and {Bn , Cn }
being identifiable up to column scaling and counter-scaling.

3

A schematic representation of a KG, which models relations
between genes, compounds and diseases is presented in Fig.
3.
In this paper, we focus our attention on a biological KG
that models relational triplets between biological entities.
For example, (compound 1, interacts with, compound 2),
(compound 1, activates, gene 1), (gene 1, regulates, gene
2), (compound 1, prevents, disease 1), (gene 2, is linked
with, disease 2) are common triplets in numerous recently
developed knowledge bases [14,15,17]. Modeling these types
of relations as a KG enables embedding entities and relations
in a Euclidean space which can further facilitate any type
of processing and analysis. For instance, obtaining a low
dimensional representation of compounds, diseases and the
‘prevents’ relation allows measuring similarity, and thus
predicting and testing hypotheses regarding (compound,
prevents, disease) interactions. Drug repurposing can be
performed by predicting candidate compounds for new and
existing target diseases. Note that the proposed framework
to be introduced shortly is not limited to biological KGs –
it can be applied to a wide variety of interesting KGs.

Problem Statement

As mentioned in the introduction knowledge graphs (KGs)
have attracted significant attention over the past decade due
to their tremendous modeling capabilities. In particular,
KGs model triplets of subject-predicate-object, denoted as
(head, relation, tail) or (h, r, t). Subjects (heads) and
objects (tails) are entities that are represented as graph
nodes and predicates (relations) define the type of edge
according to which the subject is connected to the object.

Prior Art Several methods have been proposed to
learn low dimensional representations of KGs. To properly
describe them we need to define the score function f (·) and
the loss function L(·).
Let (hn , rn , tn ) be an available triplet and hn ∈
RF , tn ∈ RF and rn ∈ Rd be the low dimensional embeddings we aim to learn. Note that entity and relation
embeddings need not be of the same dimension. The score
function determines the relation model between the head
(subject) and the tail (object). In simple words, high values
of the score function f (hn , rn , tn ) are desirable for existing
triplets (hn , rn , tn ) and low values of f (hn , rn , tn ) for nonexisting ones.
In order to produce the entity and relational embeddings
we define the following forward model for each triplet
(hn , rn , tn ):
(3.8a)



hn = γ WeT ohn ∈ RF ,

(3.8b)



tn = γ WeT otn ∈ RF ,

Table 2: Knowledge Graph models
Model
TransE [6]
TransR [21]
DistMult [34]
RESCAL [23]
RotatE [33]

score function f (h, r, t)
1 − kh + r − tk2 or 1 − kh + r − tk1
1 − kMr h + r − Mr tk2
hT diag(r)t
hT Rt
1 − kh ∗ r − tk


rn = δ WrT orn ∈ Rd ,

(3.8c)

frontal slab Z k is not necessarily symmetric. The reason
is that subject-predicate-object does not necessarily imply
object-predicate-subject. The works in [12, 20] compute
the CPD of Z (or scaled versions of Z) and produce two
embeddings for each entity, one as a subject and another
as an object. Although this is not always a drawback, it
can result in an overparametrized model because in many
applications entities usually act either as a subject or as
an object, but not both. Furthermore, a single unified
representation is usually preferable. In order to overcome
this issue, RESCAL [23] proposed the following model for
each frontal slab:
(3.10)

ohn

Le

otn

Le

orn

min

We ,Wr

k = 1, . . . , Kr ,

Kr

where
∈ {0, 1} ,
∈ {0, 1} ,
∈ {0, 1}
are
one-hot input vectors corresponding to the head, tail and
relation index of the triplet (hn , rn , tn ) respectively, with
Le , Kr being the total number of entities (nodes) and types
of relations; γ(·) and δ(·) are element-wise functions and
We ∈ RLe ×F , Wr ∈ RKr ×d are matrices that contain the
model parameters to be learned.
Popular choices for γ(·) and δ(·) are the identity function
and hyperbolic tangent. If γ(·) or δ(·) are identity functions
then the rows of We or We are the learned embeddings for
entities and relations respectively. For TransE, DistMult and
RotatE F = d, whereas for TransR and RESCAL d 6= F .
In the TransR model Mr ∈ Rd×F is a projection matrix
associated with relation r and in RESCAL R ∈ RF ×F .
In order to learn the embeddings, state-of-the-art methods attempt to minimize the following risk:
(3.9)

Z k = ARk AT ,

N
1 X
L (yn − f (hn , rn , tn ))
N n=1

where N is the number of data points (triplets or nontriplets), yn = 1 if the triplet (hn , rn , tn ) exists, else yn =
0. Typical loss functions include the logistic loss, square
loss, pairwise ranking loss, margin-based ranking loss and
variants of them. In order to tackle the problem in (3.9) the
most popular approach is stochastic gradient descent (SGD)
or batch SGD [7].

3.2 The 3-way model Modeling a KG using a third
order tensor has been considered in [2, 12, 20, 23, 25]. In
these works, the first and second mode of the tensor is the
concatenation of all the available entities, regardless of their
type, whereas the third mode represents the different type
of relations – i.e., each frontal slab of the third order tensor
represents a certain interaction type between the entities of
the KG. The methods in [2,25] work with incomplete tensors,
whereas [12, 20, 23] model each frontal slab as an adjacency
matrix. To be more precise, let Z ∈ {0, 1}Le ×Le ×Kr be the
third order tensor in [12,20,23]. Then Z(i, j, k) = 1 if entity
i interacts with entity j through relation k and Z(i, j, k) = 0
if there is no interaction between entities i and j via the k
relation.
An important observation is that although the first and
second mode of tensor Z represent the same entities, each

where Rk ∈ RF ×F is square matrix holding the relation embeddings associated with relation k. Note that the RESCAL
model is different than the traditional CPD (symmetric in
mode 1 and 2) in the sense that Rk is not constrained to
be diagonal. Relaxing the diagonal constraints allows matrix Rk to absorb in the relation embedding the direction in
which different entities interact. On the downside, this type
of relaxation forfeits the parsimony and uniqueness properties of the CPD. This is an important point, since uniqueness
is a prerequisite for model interpretability when we are interested in exploratory / explanatory analysis (and not simply
in making ‘black box’ predictions).
Another important drawback of the tree-way model is
that it models unnecessary interactions. To see this, consider
a KG that describes interactions between genes and diseases.
Suppose that the observed interactions are of gene-gene and
gene-disease type but there are no available data for diseasedisease interactions. The tree-way model involves diseasedisease interactions in the learning process (as non-edges),
even though there are no data to justify it. As we will see
in the upcoming section our proposed coupled tensor-matrix
modeling addresses all the aforementioned challenges.

4

The TeX-Graph model

In this paper we leverage coupled tensor-matrix factorization to extract low dimensional representations of entities
(head, tail) as well as representations for the interactions
(relation). KGs can be naturally represented by a collection
of tensors and matrices, as shown in Fig. 4. To see this, consider the previous example of gene, compound and disease
entities. Gene-compound interactions, of a certain type, can
be represented by an adjacency matrix. Since there are multiple types of interactions, multiple adjacency matrices are
necessary to model every interaction, resulting in a tensor
X g,c ∈ {0, 1}Lg ×Lc ×Kg,c , where Lg , Lc are the number of
genes and compounds respectively, and Kg,c is the number
of different interactions between genes and compounds. The
same idea can be applied to any (entity,interaction,entity)
triplet.
∈
To facilitate the discussion let X m,n
Lm ×Ln ×Km,n
{0, 1}
be the tensor of interactions between entity of type-m and type-n, e.g., m codifies genes
and n codifies compounds. Also let LT be the total

The proposed TeX-Graph exhibits several favorable
properties. First, the produced embeddings are unique, provided that they appear in more than one adjacency matrices.
Proposition 1. (Uniqueness of the embeddings) If the coupled tensor model in (4.12) is indeed low-rank, F , there exist entity and relation embedding vectors in F dimensional
space that generate the given knowledge base. Then the
F −dimensional TeX-Graph embeddings for type-n entities
and type-(m, n) relations
and permutation inP are unique P
variant provided that m∈Sn+ Km,n + p∈Sn− Kn,p > 1 and
Km,n > 1 respectively, where Sn+ , Sn− are defined in (4.16).

Figure 4:
model.

Schematic representation of TeX-Graph

number of different entity types, then m, n ∈ {1, . . . , LT }.
X m,n (i, j, k) = 1 if the i-th entity of type-m interacts with
the j-th entity of type-n via relation k and X m,n (i, j, k) = 0
if there is no type-k interaction between the i-th entity of
type-m and the j-th entity of type-n. The KG is represented
by a collection of tensors as:
(4.11)

X m,n ∈ {0, 1}Lm ×Ln ×Km,n , (m, n) ∈ S
S = {(m, n) : m ≤ n,

∃ (h, r, t) with (h, t) ∈ type (m, n) or (n, m)},
PLT
P
where n=1 Ln = Le and (m,n)∈S Km,n = Kr . Note that
tensors X m,n and X n,m contain the same information since
T

k
k
Xm,n
= Xn,m
. Therefore we only consider (m, n) tuples
where m ≤ n.
Each of the tensors in the array {X m,n , (m, n) ∈ S}
admits a CPD and the overall model is cast as:

(4.12)

X m,n = JAm , An , Cm,n K , (m, n) ∈ S,

where An ∈ RLn ×F , Cm,n ∈ RKm,n ×F . The i-th row of
An represents the F -dimensional embedding of the i-th
type-n entity and the k-th row of Cm,n represents the F dimensional embedding of the k-th type relation between
type-m and type-n entities. Note that in the case where
entities of type-m interact with entities of type-n via only
one type of relation, Xm,n ∈ {0, 1}Lm ×Ln is a matrix and
can be factored as:
(4.13)

Xm,n = Am diag(cm,n )An T

The model in (4.12) is a coupled CPD as the factors An appear in multiple tensors. For instance, type1-type-1 interactions (gene-gene), type-1-type-2 interactions (gene-compound), type-1-type-3 interactions (genedisease), result in the factor A1 appearing in tensors
X 1,1 = JA1 , A1 , C1,1 K , X 1,2 = JA1 , A2 , C1,1 K and X 1,3 =
JA1 , A3 , C1,3 K.

The proof of Proposition 1 utilizes the uniqueness
results of Theorem 1 and is relegated to the journal version
due to space limitations. In the case where Km,n = 1
and type-m entities appear in multiple tensors but type-n
entities only in one, the TeX-Graph model identifies Am and
An diag(cm,n ), since there is rotational freedom between An
and cm,n .
Another important property of the proposed TeX-Graph
is that it avoids modeling of spurious ‘cross-product’ relations that can never be observed. The coupled tensormatrix model allows for a concise KG representation that
eliminates such spurious relations from the start, contrary
to the three-way model. To see this, consider the previous example of gene-disease KG that observes relational
triplets between gene-gene and gene-disease type but not
for disease-disease type. The proposed TeX-Graph does not
model disease-disease interactions, whereas the three-way
model treats them as non-edges.
It is worth noticing that TeX-Graph makes the implicit
assumption that X n,n are symmetric in the first and second
mode. This is not always the case, since interactions between
some entity types might be directed. To overcome this issue
we assume that (h,r,t) implies (t,r,h) for (h,t) of the same
type. Although this assumption ignores the direction in this
type of interactions, it results in a more parsimonious model
for the entity embeddings.

4.1

Algorithmic framework In order to learn the
F -dimensional embeddings of all entities and relations we
formulate the KG embedding problem as:
X
2
min
X m,n − JAm , An , Cm,n K F ,
(4.14)
{Am },{Cm,n }

(m,n)∈S

The problem in (4.14) is non-convex and NP-hard in general.
In order to tackle it we propose to fix all variables but
one and update the remaining variable. This procedure is
repeated in an alternating fashion. The update for An is a
system of linear equations and takes the form:
(4.15)
X
+
m∈Sn

(Cm,n

(2)

Am )T Xm,n +

X

(Cn,p

(1)

Ap )T Xn,p =

−
p∈Sn





X  T
 X
 T
T
T
T
Cm,n Cm,n ∗ Am Am +
Cn,p Cn,p ∗ Ap Ap  An ,



+
m∈Sn

−
p∈Sn

where
(4.16)

Sn+ = {m : (m, n) ∈ S}, Sn− = {p : (n, p) ∈ S}

Algorithm 1 TeX-Graph
Input: {(hn , rn , tn )}N
n=1 , {Am }, {Cm,n }.
Le
Output: {An }n=1 , {Cm,n }(m,n)∈S .
Create {X m,n }(m,n)∈S from {(hn , rn , tn )}N
n=1
repeat
for n ∈ {1, . . . , LE } do
An ← solve (4.15)
end for
for (m, n) ∈ S do
C(m,n) ← solve (4.17)
end for
until criterion is met.

F which is usually the case in practice the complexity is
linear in the number of triplets participating in each update.
Furthermore the Khatri-Rao products in the (LHS) of (4.15)
and (4.17) are not being instantiated as shown in Appendix
A.

5

Drug Repurposing for COVID-19

In this section we apply TeKGraph to a recently developed
KG [17] in order to perform drug repurposing for COVID19 disease. All algorithms were implemented in Matlab or
Python, and executed on a Linux server comprising 32 cores
at 2GHz and 128GB RAM.

5.1
Algorithm 2 TeX-Graph-initialization
Input: {(hn , rn , tn )}N
n=1 .
e
Output: {An }L
,
{C
m,n }(m,n)∈S .
n=1
Create tensor Z from {(hn , rn , tn )}N
n=1 as explained
in section 3.2;
Form Y as: Y (i, j, k) = min{1, Z(i, j, k) +Z(j, i, k)};
Solve Y = JA, A, CK via sparse EVD;
e
Form {An }L
n=1 , {Cm,n }(m,n)∈S from A, C.

The update for Cm,n is the solution to the following system
of linear equations:


(3)
T
(4.17)
(An Am )T Xm,n
= ATn An ∗ ATm Am Cm,n
The derivations for these updates as well as implementation
details are presented in Appendix A.
The proposed TeX-Graph is presented in Algorithm 1.
TeX-Graph is an iterative algorithm that tackles a nonconvex problem and NP-hard in general. As a result different
initial points might produce different results. Although we
have observed that random initialization is sufficient most of
the times we propose an alternative initialization procedure
that yields consistent and reproducible results. To be more
specific we form a symmetric version of tensor Z as:
(4.18)

Y (i, j, k) = min{1, Z(i, j, k) + Z(j, i, k)}

Then we compute the semi-symmetric CPD of Y =
JA, A, CK using sparse eigenvalue decomposition (EVD)
[27]. The proposed initialization procedure is presented in
Algorithm 2.

4.2

Computational complexity analysis In terms

of memory requirements and computational complexity, the
main bottleneck of TeX-Graph lies in instantiating and
computing the matricized tensor times Khatri-Rao product
(MTTKRP) in the left hand side (LHS) of (4.15) and (4.17).
The number of flops
 needed
Pto compute thePLHS of (4.15)

and (4.17) is O F · nnz
+ X m,n +
− X n,p
m∈Sn
p∈Sn

and O F · nnz X m,n
respectively. For small values of

Data The dataset used in the experiments is the
Drug Repurposing Knowledge Graph (DRKG)1 [17]. It codifies triplets of biological interactions between 97,238 different entities of 13 types, namely, genes, compounds, diseases, anatomy, tax, biological process, cellular component,
pathway, molecular function, anatomical therapeutic chemical (Atc), side effect, pharmacological class, and symptom.
The total number of triplets is 5,874,258 and there are 107
different types of interactions. The KG is organised in 6
adjacency tensors and 11 adjacency matrices. Detailed description of the dataset and the modeling can be found in
Table 3. Each row denotes a different adjacency tensor or
matrix and # type-m entities, # type-m entities, # relation
types correspond to the dimension of mode 1, mode 2, and
mode 3 respectively. The last column (sparsity) denotes the
nnz(X m,n )
sparsity of each tensor, i.e., Lm Ln K
m,n
5.2

Procedure Drug repurposing refers to the task of
discovering existing drugs that can effectively manage certain diseases– COVID-19 in our study. DRKG codifies
relational triplets of (compound,treats,disease) and (compound,inhibits,disease). Therefore drug repurposing in the
context of DRKG boils down to predicting new ‘treats’ and
‘inhibits’ edges (links) between compounds and diseases of
interest.
We follow the evaluation procedure proposed in [17].
In the training phase we learn low dimensional representations for the entities and relations, using all the edges
in DRKG. In the testing phase, we assign a score to
(compound,treats,disease) and (compound,inhibits,disease)
triplets according to the scoring function used for training. For the proposed TeX-Graph, the scores assigned to
the triplet (hyper-edge) (compound i,treats,disease j) and
(compound i,inhibits,disease j) are:
scorei,j,2 = A2 (i, :)diag (C2,3 (2, :)) A2 (j, :)T ,
scorei,j,9 = A2 (i, :)diag (C2,3 (9, :)) A2 (j, :)T ,
since ‘treats’ and ‘inhibits’ relations correspond to the
second and ninth frontal slab of X 2,3 , respectively. The
testing set consists of 34 corona-virus related diseases,
1 github.com/gnn4dr/DRKG

Table 3: Coupled tensor-matrix DRKG modeling.
entity type-m

Gene

Compound

Disease

entity type-n

# type-m entities 1

# type-n entities 2

# relation types

tensor

sparsity

Gene
Compound
Disease
Anatomy
Tax
Biological Process
Cellular Component
Pathway
Molecular Function
Compound
Disease
Atc
Side Effect
Pharmacological Class
Disease
Anatomy
Symptom

39,220
39,220
39,220
39,220
39,220
39,220
39,220
39,220
39,220
24,313
24,313
24,313
24,313
24,313
5,103
5,103
5,103

39,220
24,313
5,103
400
215
11,381
1,391
1,822
2,884
24,313
5,103
4,048
5,701
345
5,103
400
415

32
34
15
3
1
1
1
1
1
2
10
1
1
1
1
1
1

X 1,1 = JA1 , A1 , C1,1 K
X 1,2 = JA1 , A2 , C1,2 K
X 1,3 = JA1 , A3 , C1,3 K
X 1,4 = JA1 , A4 , C1,4 K
X1,5 = A1 diag(c1,5 )AT5
X1,6 = A1 diag(c1,6 )AT6
X1,7 = A1 diag(c1,7 )AT7
X1,8 = A1 diag(c1,8 )AT8
X1,9 = A1 diag(c1,9 )AT9
X 2,2 = JA2 , A2 , C2,2 K
X 2,3 = JA2 , A3 , C2,3 K
X2,10 = A2 diag(c2,10 )AT10
X2,11 = A2 diag(c2,11 )AT11
X2,12 = A2 diag(c2,12 )AT12
X3,3 = A3 diag(c3,3 )AT3
X3,4 = A3 diag(c3,4 )AT4
X3,13 = A3 diag(c3,13 )AT13

6.12 10−5
6.50 10−6
4.13 10−5
0.0154
0.0017
0.0013
0.0013
0.0012
8.610−4
0.0023
6.76 10−5
1.6 10−4
0.0010
1.22 10−4
4.17 10−5
0.0018
0.0016

including SARS, MERS and SARS-COV2 and 8, 103 FDAapproved drugs in Drugbank. Drugs with molecule weight
less than 250 daltons are excluded from testing. Ribavirin
was also excluded from the testing set, since there exist
a ‘treat’ edge in the training set between Ribavirin and a
target disease. In order to evaluate the performance of the
proposed TeX-Graph and the alternatives we retrieve the top100 ranked drugs that appear in the highest testing scoring
(hyper-)edges. These are the proposed candidate drugs for
COVID-19. Then we assess how many of the 32 clinical
trial drugs 2 (Ribavirin is excluded) appear in the proposed
candidate top-100 drugs.

5.3

Methods The methods used in the experiments are:

• TeX-Graph. The proposed TeKGraph algorithm initialized with Algorithm 2. The embedding dimension
is set to F = 50 and the algorithm runs for 10 iterations.
• TransE-DRKG [6,17]. TransE learns low dimensional
KG embeddings using the score function shown in
Table 2. For the the task of drug repurposing we use
the specifications proposed in [17]. The l2 norm is
chosen in the score function and training is performed
using the deep graph library for knowledge graphs [35].
To evaluate the performance of TransE-DRKG on the
drug repurposing task we used the 400−dimensional
pretrained embeddings in [17], with which the drug
repurposing results were better than the stand-alone
code without pretraining.
• 3-way KG embeddings (3-way KGE). We add as
a baseline the embeddings produced by computing the
CPD of tensor Y in (4.18). Recall that we use an
2 www.covid19-trials.com

algebraic CPD of Y to initialize TeX-Graph. In 3-way
KGE we initialize using the same procedure and also run
10 alternating least-squares iterations to compute the
CPD of Y . 3-way KGE is tested with F = 50.

5.4

Results Table 4 shows the clinical trial drugs that
appear in the top-100 recommendations along with their
[rank-order]. The proposed approach retrieves 10 clinical trial drugs in the top-100 positions, and 7 in the top50. Compared to TransE-DRKG that was the first proposed algorithm to perform drug-repurposing for COVID-19,
TeX-Graph achieves 75% and 100% improvement in precision
in the top-50 and top-100 respectively.
It is worth emphasizing that the proposed Tex-Graph
retrieves approximately 1/3 of the COVID-19 clinical trial
drugs, in the top-100, among a testing set of 8, 103 drugs.
This result is pretty remarkable and can essentially help cutting down the immense search space of medical research. For
instance, consider the case of Dexamethasone, which is retrieved by Tex-Graph in the top ranked position (it admitted
the highest score among all 8, 103 drugs). At the onset of
the pandemic, the initial guidance for Dexamethasone and
other corticosteroids was indecisive. Guidelines from different sources issued either a weak recommendation to use
Dexamethasone (with an asterisk that further evidence was
required) or a weak recommendation against corticosteroids
and Dexamethasone [24]. However, recent results indicate
that treatment with Dexamethasone reduces mortality in patients with COVID-19 [16]. The results of Tex-Graph coalign
with the latest evidence and rank Dexamethasone as the top
recommended drug. This suggests that our proposed datadriven approach could have essentially contributed in overturning the initial hesitancy to administrate Dexamethasone
as a first line treatment.

(1)

Table 4: Proposed candidate drugs for COVID-19
TeX-Graph

TransE-DRKG

3-way KGE

F=50
Dexamethasone [1]
Methylprednisolone [6]
Azithromycin [13]
Thalidomide [18]
Losartan [41]
Hydroxychloroquine [47]
Colchine [48]
Oseltamivir[60]
Chloroquine[68]
Deferoxamine [88]

F=400
Dexamethasone [4]
Colchine [8]
Methylprednisolone [16]
Oseltamivir [49]
Deferoxamine [87]

F=50
Oseltamivir [89]

(2)

the MTTKRP (Cn,p Ap )T Xn,p , (Cm,n
We focus on the computation of:
(A.4)

(Cn,p

Am )T Xm,n .

(1)
Ap )T Xn,p
.

Equation (A.4) can be equivalently written as:

 T  1T 
Xn,p
Ap diag (Cn,p (1, :))

 
.. 

..

 
=

.

  . 

T
Kn,p
Ap diag (Cn,p (Kn,p , :))
Xn,p
Kn,p

X

(A.5)

T

k
diag (Cn,p (k, :)) Ap Xn,p
.

k=1

6

Conclusion

In this paper we proposed a novel coupled tensor-matrix
framework for knowledge graph embedding. The proposed
model is principled and enjoys several favorable properties,
including parsimony and uniqueness. The developed algorithmic framework admits lightweight updates and can
handle very large graphs. Finally the proposed TeX-Graph
showed very promising results in a timely application to drug
repurposing, a task of paramount importance in the fight
against COVID-19.

7

Appendix: TeX-Graph updates

(A.2)

2
F

.

−
p∈Sn

2
F

+

X n,p − JAn , Ap , Cn,p K

2
F

,

where Sn+ , Sn− are defined in (4.16). Problem (A.2) can be
written as:
X
2
min
X (1)
Am ) An T
+
m,n − (Cm,n
An

(A.3)

F

+
m∈Sn

X
−
p∈Sn

X m,n − JAm , An , Cm,n K

2
F

,

or equivalently:
min

(3)
Xm,n
− (Am

An ) Cm,n T

2

.
F

Taking the gradient of (A.7) with respect to Cm,n and
setting it to zero yields the equation in (4.17). The main
memory and computation bottleneck of equation (4.17) is
computing the MTTKRP. The formula in (A.5) can be
utilized if Cn,p is replaced by An , Ap is replaced by Am and
kT
the transposed frontal slabs Xm,n
are replaced by vertical
slabs.

References

+
m∈Sn

X

min
C m,n

(m,n)∈S

Then the update for An is the solution of:
X
min
X m,n − JAm , An , Cm,n K
An

(A.6)

C m,n

TeX-Graph solves the following problem
X
(A.1)
min
X m,n − JAm , An , Cm,n K
{Am },{Cm,n }

(2)

(Cm,n Am )T Xm,n is only different in the fact that the
frontal slabs are not transposed, and is thus omitted.
The update for Cm,n is the solution of:

(A.7)

Acknowledgements

The authors would like to acknowledge Ioanna Papadatou,
M.D., Ph.D, for contributing in the medical assessment of
the produced results.

A

It is clear from equation (A.5) that (Cn,p Ap ) need not
be instantiated. Furthermore, the number of flops to
compute (A.5) is O(F · nnz(X n,p )). Note that computing

X (2)
n,p − (Cn,p

Ap ) An T

2

.
F

Taking the gradient of (A.3) with respect to An and
setting it to zero yields the equation in (4.15). The
main bottleneck of (4.15) in terms of memory requirements
and computational complexity is instantiating the KhatriRao products (Cn,p Ap ) , (Cm,n Am ) and computing

[1] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia: A nucleus for a web of open data. In The semantic
web, pages 722–735. Springer, 2007.
[2] Ivana Balazevic, Carl Allen, and Timothy Hospedales.
Tucker: Tensor factorization for knowledge graph completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5188–5197,
2019.
[3] Albert-László Barabási et al. Network science. Cambridge university press, 2016.
[4] Michele Berlingerio, Michele Coscia, Fosca Giannotti,
Anna Monreale, and Dino Pedreschi. Foundations of
multidimensional network analysis. In 2011 international conference on advances in social networks analysis and mining, pages 485–489. IEEE, 2011.

[5] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. Freebase: a collaboratively
created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages
1247–1250, 2008.
[6] Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data.
In Advances in neural information processing systems,
pages 2787–2795, 2013.
[7] Léon Bottou.
Large-scale machine learning with
stochastic gradient descent. In Proceedings of COMPSTAT’2010, pages 177–186. Springer, 2010.
[8] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R Hruschka, and Tom M Mitchell. Toward an architecture for never-ending language learning. In Twenty-Fourth AAAI Conference on Artificial
Intelligence, 2010.
[9] Luca Chiantini and Giorgio Ottaviani. On generic
identifiability of 3-tensors of small rank. SIAM Journal
on Matrix Analysis and Applications, 33(3):1018–1037,
2012.
[10] Ignat Domanov and Lieven De Lathauwer. On the
uniqueness of the canonical polyadic decomposition of
third-order tensors — part ii: Uniqueness of the overall
decomposition. SIAM Journal on Matrix Analysis and
Applications (SIMAX), 34(3):876–903, 2013.
[11] David Easley, Jon Kleinberg, et al. Networks, crowds,
and markets, volume 8. Cambridge university press
Cambridge, 2010.
[12] Thomas Franz, Antje Schultz, Sergej Sizov, and Steffen
Staab. Triplerank: Ranking semantic web data by
tensor decomposition. In International semantic web
conference, pages 213–228. Springer, 2009.
[13] Richard A Harshman, Margaret E Lundy, et al.
Parafac: Parallel factor analysis. Computational Statistics and Data Analysis, 18(1):39–72, 1994.
[14] Daniel S Himmelstein and Sergio E Baranzini. Heterogeneous network edge prediction: a data integration
approach to prioritize disease-associated genes. PLoS
computational biology, 11(7), 2015.
[15] Daniel Scott Himmelstein, Antoine Lizee, Christine
Hessler, Leo Brueggeman, Sabrina L Chen, Dexter
Hadley, Ari Green, Pouya Khankhanian, and Sergio E
Baranzini. Systematic integration of biomedical knowledge prioritizes drugs for repurposing. Elife, 6:e26726,
2017.
[16] Peter Horby, Wei Shen Lim, Jonathan R Emberson,
Marion Mafham, Jennifer L Bell, Louise Linsell, Natalie
Staplin, Christopher Brightling, Andrew Ustianowski,
Einas Elmahi, et al. Dexamethasone in hospitalized
patients with covid-19-preliminary report. The New
England journal of medicine, 2020.
[17] Vassilis N. Ioannidis, Xiang Song, Saurav Manchanda,
Mufei Li, Xiaoqin Pan, Da Zheng, Xia Ning, Xiangxiang Zeng, and George Karypis. Drkg - drug repurposing knowledge graph for covid-19. https://github.

com/gnn4dr/DRKG/, 2020.
[18] Xueyan Jiang, Volker Tresp, Yi Huang, and Maximilian
Nickel. Link prediction in multi-relational graphs using
additive models. SeRSy, 919:1–12, 2012.
[19] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):455–500,
2009.
[20] Tamara G Kolda, Brett W Bader, and Joseph P Kenny.
Higher-order web link analysis using multilinear algebra. In Proceedings of Fifth IEEE International Conference on Data Mining, pages 8–pp. IEEE, 2005.
[21] Hailun Lin, Yong Liu, Weiping Wang, Yinliang Yue,
and Zheng Lin. Learning entity and relation embeddings for knowledge resolution. Procedia Computer Science, 108:345–354, 2017.
[22] Mark Newman. Networks. Oxford university press,
2018.
[23] Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. A three-way model for collective learning on
multi-relational data. In Icml, volume 11, pages 809–
816, 2011.
[24] Hallie C Prescott and Todd W Rice. Corticosteroids in
covid-19 ards: evidence and hope during the pandemic.
Jama, 324(13):1292–1295, 2020.
[25] Steffen Rendle and Lars Schmidt-Thieme. Pairwise interaction tensor factorization for personalized tag recommendation. In Proceedings of the third ACM international conference on Web search and data mining,
pages 81–90, 2010.
[26] Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. Relation extraction with matrix
factorization and universal schemas. In Proceedings of
the 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pages 74–84, 2013.
[27] Eugenio Sanchez and Bruce R Kowalski. Tensorial
resolution: a direct trilinear decomposition. Journal
of Chemometrics, 4(1):29–45, 1990.
[28] Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao
Fu, Kejun Huang, Evangelos E Papalexakis, and Christos Faloutsos. Tensor decomposition for signal processing and machine learning. IEEE Transactions on Signal Processing, 65(13):3551–3582, 2017.
[29] Amit Singhal.
Introducing the knowledge graph:
things, not strings. Official google blog, 16, 2012.
[30] Richard Socher, Danqi Chen, Christopher D Manning,
and Andrew Ng. Reasoning with neural tensor networks for knowledge base completion. In Advances in
neural information processing systems, pages 926–934,
2013.
[31] MIKAEL Sørensen, Ignat Domanov, and L De Lathauwer. Coupled canonical polyadic decompositions and
(coupled) decompositions in multilinear rank-(lr, n, lr,
n, 1) terms—part ii: Algorithms. SIAM Journal on
Matrix Analysis and Applications, 36:1015–1045, 2015.
[32] Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. Yago: a core of semantic knowledge. In
Proceedings of the 16th international conference on

World Wide Web, pages 697–706, 2007.
[33] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian
Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. In International Conference on Learning Representations, 2018.
[34] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng
Gao, and Li Deng. Embedding entities and relations
for learning and inference in knowledge bases. arXiv
preprint arXiv:1412.6575, 2014.
[35] Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao
Ye, Jin Dong, Hao Xiong, Zheng Zhang, and George
Karypis. Dgl-ke: Training knowledge graph embeddings at scale. arXiv preprint arXiv:2004.08532, 2020.

