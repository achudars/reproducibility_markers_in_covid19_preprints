arXiv:2011.05136v1 [q-bio.QM] 9 Nov 2020

Application and Comparison of Deep Learning
Methods in the Prediction of RNA Sequence
Degradation and Stability
Ankit Singhal
Abstract
mRNA vaccines are receiving increased interest as potential alternatives to conventional methods for the prevention of several diseases,
including Covid-19. This paper proposes and evaluates three deep
learning models (Long Short Term Memory networks, Gated Recurrent Unit networks, and Graph Convolutional Networks) as a method
to predict the stability/reactivity and risk of degradation of sequences
of RNA. Reasonably accurate results were able to be generated with
the Graph Convolutional Network being the best predictor of reactivity (RMSE = 0.249) while the Gated Recurrent Unit Network was
the best at predicting risks of degradation under various circumstances
(RMSE = 0.266). Results suggest feasibility of applying such methods
in mRNA vaccine research in the near future.

1

Introduction

Over the last two decades, there has been increasing interest in the field
of RNA-based technologies in the creation of prophylactic vaccines. They
are widely regarded to be plausible alternatives to conventional approaches
due to their high potency, capacity for quick and low-cost manufacturing,
and relatively safe administration. They are currently being researched for
many diseases - included SARS-CoV-2 - although none have approved yet
for human use [6, 8]. One of the primary barriers in the creation of such
therapeutics is the fragility of the RNA molecule; they are susceptible to rapid
degradation within minutes to hours [2] and as such need to be freeze dried
1

or incubated at low temperatures in order to be kept stable. In the current
pandemic, vaccines are seen as the most promising means to control the
novel coronavirus, but with current limitations on mRNA vaccines, efficient
in vivo delivery of such molecules seems improbable; it would likely only
reach a fraction of the population.
Therefore, research into the the stability and degradation of RNA molecules
has received continued interest, to date largely consisting of traditional statistical approaches and biophysical models. However, it still remains unclear
exactly which parts of RNA molecules are more prone to spontaneous degradation and thus difficult to accurately predict the reactivity and degradation
of mRNA[7]. Therefore, experimentation, an incredibly time consuming process, is the default method in determining these values.
The aim of this manuscript is to present three possible Deep Learning
approaches to this problem through the usage of the Stanford OpenVaccine
dataset. Two variants of Recurrent Neural Networks (RNNs) are employed,
Long Short Term Memory Networks (LSTMs) and Gated Recurrent Unit
Networks (GRUs), along with a variant of a Graph Neural Network (GNN),
the Graph Convolutional Network (GCN). These models are applied and
compared to assess whether Machine Learning methods can provide helpful
results in predicting the reactivity and degradation of mRNA molecules.

2
2.1
2.1.1

Materials and Methods
Materials
Software/Packages Used

The majority of this work was done in the Python programming language
using a TensorFlow back end with a surface level Keras API. Refer to Table
1 for all the software/packages used throughout the creation and testing of
the models along with related information.

2

Software/Package
Python 3.7
TensorFlow
Keras
SKLearn
Pandas
Numpy
Matplotlib
ARNiE

Use
Used to write the code for the models
discussed.
Used as a backend for the majority of
the models presented.
Used as a high level API for some
parts of the model.
Used for k-Fold Cross Validation.
Used for data handling.
Used for data handling.
Used to create figures in manuscript.
Used to generate augmentation data.

Developer
Python Software Foundation
Google
Google
Cornapeau and Matthieu
McKinney
Oliphant
Droettboom and Caswell
DAS Labs

Table 1: All the packages/software used in the creation of the models presented.

2.1.2

Dataset Description

The dataset used in this manuscript to evaluate the models is called the
”Stanford OpenVaccine” dataset [3]. It consists of 6034 RNA sequences. The
training/validation set consisted of 3029 of these sequences with a length of
107 nucleotide bases. The testing dataset consists of 3005 sequences with a
length of 130 nucleotide bases. Due to experimental limitations, measurements on the final bases in the sequences cannot be carried out, therefore,
only 68 (for the sequences with length 107) and 91 (for the sequences with
length 130) of the first bases in the respective sequences have experimental
data associated with them.
Three predictors were associated with each sequence: the sequence itself
(described in A,G,C, and U bases), the expected structure of the molecule,
and the Predicted Loop Type (derived from a simulation of the secondary
structure of the RNA molecule). A Base Pair Probability Matrix was also
provided for each individual sequence indicating the probability of certain
base-pair interactions. Five experimentally-determined sets of values (henceforth referred to as ’target values’) were also given for the first 68 or 91 base
pairs in the sequence: Reactivity values, degradation values at pH 10, degradation values at pH 10 with added Magnesium, degradation values at 50◦ ,
and degradation values at 50◦ with added Magnesium. Refer to Table 2 for
more information.

3

Feature

Classification

Sequence

Input

Structure

Input

Predicted
loop type

Input

Reactivity

Target

Deg pH 10

Target

Deg pH 10 Mg

Target

Deg 50◦ C

Target

Deg 50◦ C Mg

Target

Description
A sequence of 107 letters corresponding
to the four bases in the sequence.
Expected structure of the molecule
(length = 107). ’(’ and ’)’ refer to a base
pair interaction. All ’.’ in the middle are
associated with no BP interactions.
Predicted secondary structure of the
RNA molecule at different points. ’S’
refers to a stem structure, ’M’ multiloop,
’I’ internal loop, ’B’ bulge, ’H’ hairpin
loop, ’E’ dangling end, ’X’ external loop.
Reactivity values at each individual
point in thesequence.
Degradation values at pH 10.
Degradation values at pH 10 with
added Mg.
Degradation values at 50◦ C.
Degradation values at 50◦ C
with added Mg.

Sample
A, G, U, U, C, ...

(..()...)()(... ...

S, S, M, S, H, ...

1.23, 3.46, ...
0.89, 2.44, ...
1.28, 0.88, ...
2.02, 1.87, ...
1.11, 2.44, ...

Table 2: Inputs and target features of the Stanford OpenVaccine dataset.

The task of the algorithms that are presented in this paper is to take the
sequence and other structural features of an RNA molecule (features marked
as ’inputs’) and predict its stability (through the five target values).
2.1.3

Data Representation

As mentioned earlier, each base of the sequence has five target and two further
structural features associated with it. This will be represented as a feature
matrix for all three ML models. What is of particular interest however, is
the Base Pair Probability Matrix associated with each sequence which takes
the form N × N , where N is the number of bases in the sequence. It can be
represented as a standard matrix (refer to Figure 1 for visualization) or as
a graph (refer to Figure 2) with nodes and edges. This distinction is import
for the former form is used for the two RNN architectures whereas the latter
is used for the GCN.

4

Figure 1: Visualization of a sample BPPM Matrix. Purple indicates no base pair interaction, the more
green a color, the higher the interaction between those two bases.

Figure 2: Visualization of a an excerpt from sample graph structure of a BPPM. Information in the nodes
correspond to the type of base and its place on the sequence, the width and number next to the edges
refer to the interaction between the two bases (0-1).

2.2
2.2.1

Methods and Models
Data Processing and Overall Model Architecture

Data from the dataset was first mapped to vector matrices and hot encoded.
After splitting the training and testing data according to the specifications
lined out in Section 2.1.2, the training data was augmented using the ARNiE
package, the same package used to create the data in the first place. Finally,
the resulting data was used to create the model using the architectures explained below and predictions were generated on the test data before being
evaluated. Refer to Figure 3.

5

Figure 3: High level diagram of the overall model architecture. ’Model’ in the diagram refers to one of
the three ML approaches discussed in the next section.

2.2.2

Long Short Term Memory Networks

A Long Short Term Memory Network (LSTM) is a type of RNN introduced
by Hochreiter & Schmidhuber in 1997 [4] and further popularized by many
other pieces of work following it. Like other RNNs, it essentially consists of
copies of the same network in which the output of the previous network (or
cell) informs the next one, allowing information to persist. A single cell in
an LSTM, consists of four distinct neural network layers, three of which are
ones that make ’decisions’ about the persistence of information as shown in
equations 1 - 3:
ft = σ(Wf [ht−1 , xt ] + bf )
(1)
it = σ(Wi [ht−1 , xt ] + bi )

(2)

ot = σ(Wo [ht−1 , xt ] + bo )

(3)

Here the output of the previous cell along with the new input is passed
through a basic neural layer with a non-linear activation function (traditionally sigmoid functions were used but in this paper, all activation functions
were ReLu). In parallel, candidate values for the cell state are also produced
as shown by equation 4:
C̃t = φh (WC [ht−1 , xt ] + bC )

(4)

Where the activation function in this case is a hyperbolic tangent function
(compressing the values between -1 and 1). Finally using these output matrices, one can then update the cell state as given by equation 5 and produce
new outputs as shown in equation 6:
Ct = ft · Ct−1 + C̃t · it

(5)

ht = ot · φh (Ct )

(6)

This output is then fed into the next cell. This model was used with the
normal vector matrix BPPM as represented in Figure 1.
6

2.2.3

Gated Recurrent Unit Networks

Introduce by Cho et al. in 2014 [1], a Gated Recurrent Unit Network is a
variation of traditional LSTM networks that have the primary advantage of
faster computation (due to less neural nets in each cell). Like in the LSTM,
decision matrices are produced through two non-linear activation function
based neural networks as shown in equations 7 and 8:
rt = σ(Wr [ht−1 , xt ] + br )

(7)

zt = σ(Wz [ht−1 , xt ] + bz )

(8)

Candidate values are also generated through a hyperbolic-tangent-based network, however, in this case it is done directly for the output as there is no cell
state, unlike in an LSTM. The candidates are then chosen by the ’decision’
matrices to produce the output as shown in equations 9 and 10:
h̃t = φh (Wh [ht−1 , xt ] + bh )

(9)

ht = (1 − zt ) · ht−1 + h̃t · zt

(10)

Like any other form of RNN, this output is then passed to the next hidden
layer.
2.2.4

Graph Convolutional Networks

Graph Convolutional Networks were introduced by Kipf & Welling in 2017 [5]
and provide a novel way to analyze arbitrarily structured data in the form of a
graph. A GCN is not a form of an RNN although they are both connectionist
models. A GCN operates on a graph defined by G = (V, E) where V is the
set of nodes and E the set of edges. Nodes in the graph aggregate the features
of the surrounding nodes and itself and use the following neural net (refer to
11 to generate an output which is then assigned to the node:
ht = σ(Wh · D−1 [ht−1 , Â] + bh )

(11)

Where D is the diagonal node degree matrix of the graph and Â is equal to
A+I, A being the adjacency matrix (taking the form N ×N ) representing the
graph (in this case, the BPPM - see Figure 2). h0 = N · F0 i.e. it is a feature
matrix. After an output is generated, this process can be repeated each time
the output of the nodes propagating outwards. Due to the localization of
such a problem, I only had two repetitions, any more resulted in negligible
influences.
7

2.2.5

Performance Assessment

The models will be evaluated based on the error produced in the prediction
of the target values. The two loss measures employed in this paper are Mean
Absolute Error (MAE) and Root Mean Square Error (RMSE) described in
equations 12 and 13.
r
Σni=1 (ŷi − yi )2
(12)
RM SE =
n
1
(13)
M AE = Σni=1 |ŷi − yi |
n
The difference between the two metrics is that in MAE, all the errors are
averaged by weighting them equally, however, since RMSE has a quadratic
term, larger individual errors will be punished more than smaller ones.

3
3.1

Results
Initial Training and Validation

After being built using the specifications discussed earlier in this manuscript,
the models were trained on the ’training’ dataset discussed in section 2.1.2.
For k-fold cross validation, in the models presented, k = 4 and this was initially repeated three times to improve accuracy. The model was split into
four groups for cross validation as larger values of k were found to have negligible influence on the model results. Training and validation loss (RMSE)
for the three models are show in Figure 4 and Table 3.
Training
Validation
RMSE MAE RMSE MAE
LSTM 0.1089 0.0663 0.1758 0.1052
GRU
0.1143 0.0693 0.1787 0.1072
GCN
0.1752 0.1055 0.2232 0.1394
Table 3: Mean loss values of models after the training and validation processes.

As can be seen after the initial training and validation phase, the LSTM
maintains the best performance. The GRU scores were similar to that of
the LSTM, however, the GCN was in a distant third place averaging about
0.05-0.06 higher RMSE values in comparison to the other two.
8

(a) LSTM Model

(b) GRU Model

(c) GCN Model

Figure 4: Training and validation loss for the twelve histories of the ML Models.

3.2

Model Comparison

The performance for the three machine learning models on the training
dataset are presented in Figures 5 - 9. The GRU maintained the lowest
loss values for all target values, with the only exception being the reactivity
(see Figure 5) and the degradation at pH 10 (only the RMSE - see Figure 6)
in which the GCN had the best predictions. Notably, in Figures 7, 8, and
9, the GCN had lower RMSE values than the LSTM, however the opposite
was true for the MAE values suggesting that the GCN is not as prone to
large, individual errors.

Figure 5: Results (loss) for over 20 trials of the models predicting the reactivity values of the training
set sequences (note change in scale between plots). The GCN performed the best, both in terms of RMSE
and MAE, followed by the GRU and LSTM respectively. Boxplots show the minimum, maximum, and
median values along with the 25th and 75th quartiles.

9

Figure 6: Results (loss) for over 20 trials of the models predicting the degradation at pH 10 of the
training set sequences (note change in scale between plots). The GCN performed the best in terms of
RMSE, however, the GRU performed better in terms of MAE, suggesting the GRU is more prone to
larger errors. Boxplots show the minimum, maximum, and median values along with the 25th and 75th
quartiles.

Figure 7: Results (loss) for over 20 trials of the models predicting the degradation at pH 10 with added
Magnesium of the training set sequences (note change in scale between plots). The GRU performed
the best, both in terms of RMSE and MAE, followed by the GCN in RMSE and LSTM in MAE, further
reaffirming the notion that the GCN is less prone to larger individual errors. Boxplots show the minimum,
maximum, and median values along with the 25th and 75th quartiles.

10

Figure 8: Results (loss) for over 20 trials of the models predicting the degradation at 50◦ C of the
training set sequences (note change in scale between plots). The GRU performed the best, both in terms
of RMSE and MAE, the GCN in RMSE and LSTM in MAE, similar to Figure 7. Boxplots show the
minimum, maximum, and median values along with the 25th and 75th quartiles.

Figure 9: Results (loss) for over 20 trials of the models predicting the degradation at 50◦ C with added
Magnesium of the training set sequences (note change in scale between plots). The GRU performed the
best, both in terms of RMSE and MAE, the GCN in RMSE and LSTM in MAE, similar to Figures 7 and
8. Boxplots show the minimum, maximum, and median values along with the 25th and 75th quartiles.

Mean loss across the 20 trials for each individual target were calculated
along with that of all targets combined for each model. Results are shown
in Figure 10.

11

Figure 10: Mean loss results (RMSE and MAE - note change in scale between plots) for the three machine
learning methods. The GRU performed the best across both metrics.

4

Discussion and Conclusion

Although the creation of stable mRNA molecules remains difficult, datasets
of sequences and corresponding information are becoming more popular and
widely available. Through the use of deep learning architectures, reasonable
predictions of structural features can be obtained, as demonstrated in this
manuscript, with mean RMSE values ranging from 0.24 to 0.31. The usage
of such techniques has the capacity to increase the speed and efficiency of
mRNA vaccine discovery and has further implications in other related fields
of research.
However, such methods, as presented in this paper, are not without their
limitations. It is important to note that while reasonably accurate results
are produced, the error, when taking into account the scale of the values
being predicted (0.5 - 4) is not insignificant; this could have the potential of
incorrectly predicting the stability of an important molecule which may be
overlooked during the discovery phase. Therefore, in its current state, as an
extension, I would suggest a simple binary classification system to predict
whether the molecule is stable, at the end of each of the models that would
aim to minimize the False Negative rate - effectively resulting in the model
being used as a screening test to remove highly unstable sequences, rather
than a full-fledged research tool.
Another limitation of this method is the length of the sequences of the
mRNA molecules used. The length used in this paper ranged from 107-130
bases, while an actual Covid-19 vaccine would likely range from 3000-4000
[2] bases long. As an improvement, this model could be trained on and
12

applied to longer RNA sequences to see how this impacts the accuracy of
such methods.
Despite these limitations, the results of this work show that such prediction algorithms are feasible and have the potential to save time during research processes, an especially valuable commodity during disease outbreaks.
In the long term, such techniques may also better help researches in understanding the reasoning behind the stability of certain RNA molecules and aid
in the development of related technologies. It is hoped that this work will
be of some use to other data scientists as well in creating better prediction
models for this field.

13

5

References
1. Cho, K., Bahdanau, D, et al. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. arXiv
(2014). arXiv:1406.1078v3.
2. Conger, K. Stanford biochemist works with gamers to develop COVID19 vaccine. Scope (2020). https://scopeblog.stanford.edu/2020/05/20/
stanford-biochemist-works-with-gamers-to-develop-covid-19-vaccine.
3. Eterna. OpenVaccine: mRNA Vaccine Degradation Prediction. Kaggle
(2020). https://www.kaggle.com/c/stanford-covid-vaccine/overview.
4. Hochreiter S., Schmidhuber J. Long Short Term Memory. Neural Computation 9(8):1735-1780 (1997).
5. Kipf, T., Welling, M. Semi-Supervised Classification with Graph Convolutional Networks. arXiv (2017). https://arxiv.org/abs/1609.02907v4.
6. Pardi, N., Hogan, M. mRNA vaccines — a new era in vaccinology. Nat
Rev Drug Discov 17, 261–279 (2018). https://doi.org/10.1038/nrd.2017.243.
7. Wayment Steele, H., Soon Kim, D., et al. Theoretical basis for stabilizing messenger RNA through secondary structure design. bioRxiv
(2020). https://doi.org/10.1101/2020.08.22.262931.
8. Zhang, C., Maruggi, G., et al. Advances in mRNA Vaccines for Infectious Diseases. Fronteirs in Immunology (2019). https://doi.org/10.3389
/fimmu.2019.00594.

14

