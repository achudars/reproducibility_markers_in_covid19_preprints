The effects of regularisation on RNN models for time series
forecasting: Covid-19 as an example
Marcus Carpentera,∗, Chunbo Luoa , Xiao-Si Wangb

arXiv:2105.05932v1 [cs.LG] 9 May 2021

a

Department of Computer Science, University of Exeter, North Park Road, Exeter EX4 4QF, UK
b
Applied Research, British Telecommunications plc., Adastral Park, Ipswich, UK

Abstract
Many research papers that propose models to predict the course of the COVID-19
pandemic either use handcrafted statistical models or large neural networks. Even
though large neural networks are more powerful than simpler statistical models, they
are especially hard to train on small datasets. This paper not only presents a model
with grater flexibility than the other proposed neural networks, but also presents a
model that is effective on smaller datasets. To improve performance on small data,
six regularisation methods were tested. The results show that the GRU combined
with 20% Dropout achieved the lowest RMSE scores. The main finding was that
models with less access to data relied more on the regulariser. Applying Dropout to
a GRU model trained on only 28 days of data reduced the RMSE by 23%.
Keywords: COVID-19, time series forecasting, recurrent neural networks, LASSO,
Ridge regression, ElasticNet, Dropout
1. Introduction
Naive time series models tend to perform better than complex time series models.
The main problem with simpler statistical models is that, even though their performance may be better, they cannot be generalised for multiple locations. To model
the COVID-19 pandemic in a new country a new model must be created. Neural
networks, on the other hand, are good at generalising a dataset. This means that
neural networks can learn from data of one country and interpolate this when making
predictions for another location. The problem is that most neural networks models
that have been proposed are vary large, deep learning requires larger datasets, and
no model seems to take advantage of its generalisation capabilities. Essentially all
∗

mc844@exeter.ac.uk

Preprint submitted to Elsevier

14th May 2021

neural network models proposed are as flexible as the statistical models, but lack
their simplicity.
The data provided by the Johns Hopkins University contains multiple multivariate time series data for a total of 187 different locations. These locations are mostly
countries; however, some cruise ships have been included. To make predictions on
all available locations using a single model, a multi-output RNN model is proposed.
Moreover, regularisation methods were applied and tested on the RNN in an attempt
to improve performance on small datasets.
Regularisers add extra penalties during the training process that shrink the
weights of the model and force it to learn more abstract functions. A regularised
model is more robust to noisy data and less prone to overfitting the training samples.
The results show that applying regularisers on the models reduced the RMSE of the
predictions, and the models relied more on regularisers when trained on less data.
In other words, RNN models trained on larger datasets do not require regularisers
and applying them can even harm performance.
The code for the experiments can be found at:
https://github.com/marcusCarpenter97/COVID-19-forecasting
2. Related work
Statistical models were surveyed based on their type or complexity. As for the
neural network models, there have been many proposed predicative models. The
two main traits evaluated in this survey are the size and complexity of the neural
networks, and the model’s capability to generalise the data well enough.
Traditional models include mathematical, statistical and simple machine learning
models. One major limitation of these, simpler, models is that they must be fitted on
individual time series. Therefore, simple models do not have the flexibility of producing forecasts for multiple locations at once. Examples of handcrafted mathematical
models include Polynomials [10] and Logistic growth [6]. Compartmental models
from epidemiology were also used including SEIR [1] and SIR [3]. More complex statistical models, such as the Gaussian Process Regression [7] and ARMA [8], have also
been successfully applied to forecasting COVID-19. Rustam et al. [11] showed that
Exponential Smoothing outperformed the Support Vector Machine further proving
that simpler models can outperform more complex ones.
The Multilayer Perceptron (MLP) was used for forecasting the COVID-19 infection trends. Wieczorek et al. [13] trained an eight layer MLP over 3300 epochs to
benchmark multiple optimisation algorithms. This is a fairly large model considering
it was only capable of producing forecasts for 30 location at a time. Hazarika et al. [4]
2

created a wavelet coupled random vector functional link network (WCRVFL). This
model is a type of MLP that integrates the wavelet transform for signal processing,
in this case a time series. Their experiments focused only on the top five worst hit
countries and their predictions spanned a period of 60 days. However, these 60 days
were in the future at the time the article was published, therefore it is impossible
to verify the quality of forecasts because the data did no yet exist at the time of
writing.
The RNN, and mainly its variant the Long Short Term Memory (LSTM), was
used in the literature. Shastri et al. [12] focused on the LSTM and its variants.
Their experiments showed that the ConvLSTM achieved the lowest Mean Absolute
Percentage Error (MAPE) among all other models. The MAPE is not a suitable
error metric for a dataset that contain zeros which the Johns Hopkins University data
does. Predictions were only made for India and the USA. Zeroual et al. [14] not only
experimented on the LSTM but also included the RNN, GRU and the Variational
AutoEncoder (VAE). The models were trained for 1000 epochs and made predictions
for six countries using a forecast horizon of 17 days. The RMSE was used to measure
forecast performance, and the VAE was determined to be the best model.
3. Methods
It is helpful to first mathematically define the model M as
l
ŷt+1:t+n
= M (xl1:t , IDl , θ)

(1)

where, for a location l, xl1:t represents the time series data from day 1 to t, IDl is
a location’s unique identifier and θ stands for the model’s parameters. The model
outputs the predictions for each day between days t + 1 and t + n for location l. The
values of t and n represent the last day in the training data and the forecast horizon,
respectively.
The data used in the experiments was collected from the Johns Hopkins University GitHub repository [2]. This dataset contains 187 main locations. These locations
tend to be countries, although some cruise ships where included in the data. Because the data contains large values, training the neural networks on the original
data would produce undesirable results. To address this, the data was standardised.
3.1. Model architecture
The model uses a multi-output encoder-decoder architecture. Figure 1 illustrates
the overall architecture of the model.

3

Output
Model

Input

Time Series
Location ID

Confirmed layer

Linear
Context

Time Series

Deceased layer

Time Series

Recovered layer

RNN encoder

Time Series

Figure 1: The model takes as input a location ID and a multivariate time series for each country.
The ID is processed by a single linear node for its affine transformation, and the time series is
processed by the RNN layer. The outputs of the two are concatenated into a context which is then
processed by three separate branches of fully connected nodes. Each branch specialises on one of
the features in the data.

The encoder takes two inputs: the time series data for a location and its unique
identifier. The time series is processed by the RNN layer. The first six characters
were extracted from the SHA-256 hash of the location’s name and used as the unique
identifier for that specific location. This identifier allows the model to better differentiate between different time series; however, Wieczorek et al. stated that this was
not necessary for their experiments to succeed [13]. The identifier was processed by
a linear node for its affine transformation. The context is created by concatenating
the outputs of the RNN and the linear node.
The decoder is composed of three branches. Each branch is made of a single layer
of linear nodes, and the number of nodes is the same as the number of days in the
forecast horizon, in this case 28. This branching of the output allows the model to
be optimised not only for each of the features but also for each of the days in the
forecast horizon.
3.2. Regularisers
Regularisation methods can be applied to neural networks with the objective of
preventing it from overfitting the training data. This allows the model to become
more robust to noisy data because of its new generalisation capabilities. Table 1 lists
the six regularisers explored in the experiments.
The LASSO (L1) and Ridge regression (L2) impose an extra penalty on the
model’s loss function. This extra penalty to the loss function forces the weights of
the neural network to shrink during training, effectively serving as a feature selection
4

Label
L1
L2
No reg
0
0
L1
0.01
0
L2
0
0.01
Dropout
0
0
L1L2
0.01 0.01
All reg 0.01 0.01

Dropout
0
0
0
0.2
0
0.2

Table 1: The different combination of regularisers used in the experiments. Note that the ElasticNet
was labelled L1L2 and No reg represents a standard model without any regularisers applied to it.

[9]. The two regularisers can be combined into what is called the ElasticNet by adding
the two penalty factors together [15]. The main improvement ElasticNet brings is
that the L2 method prevents the L1 form shrinking the weights down to zero. A
sparse weight matrix in a neural network would lead to many connections becoming
ineffective. Equations 2 and 3 show the L1 and L2, where λ is the hyperparameter
for the penalty factor.
L1 = loss(M (xl1:t , IDl , θ)) + λ

n
X

|wi |

(2)

i=1

L2 =

loss(M (xl1:t , IDl , θ))

+λ

n
X

wi2

(3)

i=1

Dropout, on the other hand, is a regularisation method that purposefully makes
the weight matrix sparse by deactivating randomly selected nodes. This, however,
only occurs during the training process, and during the testing all nodes are fully
functional. The amount of deactivated nodes is selected as a percentage. When
Dropout is used, the model is forced to learn a more general function to compensate
for the lack of extra nodes [5].
4. Experiments
All models were trained for 300 epochs using the Adam optimizer with learning
rate of 0.001 and the Mean Squared Error (MSE) as the loss function. The output
layer used a linear activation function mathematically defined as f (x) = x. The
L1 and L2 regularisers were applied on the weights of the RNN encoder which used
one, 20 node, layer. Both, L1 and L2, regularisers used λ = 0.01 while the dropout

5

percentage was 0.2 or 20%. Table 1 outlines how the regularisers where arranged
into six combinations.
The models were cross-validated. Cross-validation provides a robust statistical
evaluation of the model and allows the output for each time period to be visualised.
When cross-validating a time series special precautions have to be made due to the
temporal nature of the data. To address this, the forward chaining method was used
with a forecast horizon of 28 days.
In the experiments the data for each location contained 448 days (04/02/2020 to
27/04/2021). Both the validation and test sets contained 28 days allowing the data
to be split into 14 validation folds. The size of the training data grew from 28 to 392
days and not 448 as the last section of the data must be kept for the validation and
test sets.
5. Discussion
After running the experiments, the models were evaluated for model selection.
There were fourteen cross-validation folds, six regularisers, two model architectures,
and, for each of these, an ensemble of ten models. The best model was selected
based on the smallest average RMSE of the model’s ensemble. Each model produced
a very large output of 187 multivariate time series, and therefore the mean of the
RMSE was taken for each of these outputs. It is important to note that, because
the RMSE is not scale invariant, this average simply becomes a number to minimise
rather than an estimation of people the model missed in its predictions.
The model with the lowest mean and the lowest variance on its RMSE ensemble
was selected as the best. Out of all candidates, the best performing model was the
GRU combined with 20% Dropout rate. This model was compared with the standard
GRU (no regularisers) to analyse the affects of applying Dropout to a GRU. Table
2 reveals that Dropout had the greatest impact on the first validation fold where it
reduced the RMSE by 23%; Dropout negatively impacted the model on the last two
validation folds by up to a 4% increase in RMSE. In other words, models trained on
smaller time series rely more heavily on the regulariser, models trained on medium
sized time series do not rely so much on regularisation methods, and models trained
on larger time series can experience a degradation in performance if combined with
Dropout.
The data can be categorised into four distinct types, these are: smooth, outlier,
step, and flat. These categories affect how the model handles its predictions. Smooth
data is the most predominant type in the dataset. This is a time series with no
anomalies which allows the model to predict it with accuracy. Outlier data occurs
6

Fold
0
1
2
3
4
5
6
7
8
9
10
11
12
13

31/03/2020
28/04/2020
26/05/2020
23/06/2020
21/07/2020
18/08/2020
15/09/2020
13/10/2020
10/11/2020
08/12/2020
05/01/2021
02/02/2021
02/03/2021
30/03/2021

to
to
to
to
to
to
to
to
to
to
to
to
to
to

Dates
No reg
Dropout
28/04/2020
493.796
379.001
26/05/2020
341.281
341.321
23/06/2020
410.034
399.417
21/07/2020
614.241
590.560
18/08/2020
711.503
706.253
15/09/2020
644.467
646.871
13/10/2020
723.875
697.384
10/11/2020 1355.365 1306.728
08/12/2020 1623.798 1515.455
05/01/2021 10206.108 10182.144
02/02/2021 1361.537 1353.741
02/03/2021
930.507
906.355
30/03/2021 1187.182 1188.860
27/04/2021 2101.707 2178.459

Change
-23%
0%
-3%
-4%
-1%
0%
-4%
-4%
-7%
0%
-1%
-3%
0%
4%

Table 2: The RMSE values for the GRU with no regularisers and the GRU with dropout are
presented for all validation folds along with the change in RMSE. Smaller RMSE is better and so
is a greater negative change. Positive change indicates that the baseline model performed better.

when there is a sudden spike on some of the days. This occurs rarely, and the reason
for this is likely to be a mistake in the reporting of the data. The models handle
outliers by ignoring them and simply following the trend of the data. Step data is
defined as having sudden jumps in the recorded cases. Even though this can be a
genuine spike in cases, it is likely that the data collection was intermittent. The
models tend to produce forecasts that resemble smooth data. Consequently, this
can be interpreted as the model having learnt how the number of cases grows and
being able to make predictions for days with missing data. Flat data is defined by an
unchanging number of reported cases either because of a stagnation of the disease or
a lack of data. The model produces forecasts as real numbers; therefore, it cannot
output a precise constant value. To overcome this problem, the model produced
predictions that fluctuate around the constant.
6. Conclusion
Many neural network models have been applied to forecasting the trends of
COVID-19 infections. However, the fact that deep learning requires large amounts
of data and that the time series available of the pandemic is relatively small is often
overlooked. A regularised GRU was proposed that is capable of producing accurate
7

forecasts even when trained on a 28 day time series. From the results, it is possible
to conclude that when making time series forecasts using an RNN, regularisers can
greatly improve performance. However, this is only the case on small datasets and
prediction accuracy was found to decrease when regularisers were applied on larger
time series.
Appendix A. Plots

(a) Australia

(b) Fiji

(c) New Zealand

Figure A.2: Oceania form 30/03/2021 to 27/04/2021

8

9

(a) Algeria

(b) Angola

(c) Cabo Verde

(d) Congo

(e) Ethiopia

(f) Kenya

(g) Madagascar

(h) Nigeria

(i) Niger

(j) Tanzania

Figure A.3: Africa form 30/03/2021 to 27/04/2021

10

(a) Argentina

(b) Bahamas

(c) Bolivia

(d) Brazil

(e) Canada

(f) Colombia

(g) Dominica

(h) Guatemala

(i) Mexico

(j) US

Figure A.4: America form 30/03/2021 to 27/04/2021

11

(a) Belgium

(b) Bulgaria

(c) Croatia

(d) Cyprus

(e) Finland

(f) France

(g) Germany

(h) Holy See

(i) Italy

(j) United Kingdom

Figure A.5: Europe form 30/03/2021 to 27/04/2021

12

(a) Afghanistan

(b) Bangladesh

(c) Bhutan

(d) Burma

(e) India

(f) Iran

(g) Japan

(h) Jordan

(i) South Korea

(j) Syria

Figure A.6: Asia form 30/03/2021 to 27/04/2021

13

Figure A.7: Box plots for all validation folds.

References
[1] Amit Bhati and Anurag Jagetiya. Prediction of COVID-19 outbreak in india
adopting bhilwara model of containment. In 2020 5th International Conference
on Communication and Electronics Systems (ICCES). IEEE, jun 2020. doi:
10.1109/icces48766.2020.9138060.
[2] Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based
dashboard to track COVID-19 in real time. The Lancet Infectious Diseases, 20
(5):533–534, may 2020. doi: 10.1016/s1473-3099(20)30120-1.
[3] Behzad Ghanbari. On forecasting the spread of the COVID-19 in iran: The
second wave. Chaos, Solitons & Fractals, 140:110176, nov 2020. doi: 10.1016/
j.chaos.2020.110176.
[4] Barenya Bikash Hazarika and Deepak Gupta. Modelling and forecasting of
COVID-19 spread using wavelet-coupled random vector functional link networks. Applied Soft Computing, 96:106626, nov 2020. doi: 10.1016/j.asoc.2020.
106626.
[5] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. Improving neural networks by preventing co-adaptation
of feature detectors.
[6] Minni Jain, Prajwal Kumar Bhati, Pranav Kataria, and Rohit Kumar. Modelling logistic growth model for COVID-19 pandemic in india. In 2020 5th International Conference on Communication and Electronics Systems (ICCES).
IEEE, jun 2020. doi: 10.1109/icces48766.2020.9138049.
[7] Y.A. Khan, S.Z. Abbas, and Buu-Chau Truong. Machine learning-based mortality rate prediction using optimized hyper-parameter. Computer Methods and
Programs in Biomedicine, 197:105704, dec 2020. doi: 10.1016/j.cmpb.2020.
105704.
[8] Mohsen Maleki, Mohammad Reza Mahmoudi, Mohammad Hossein Heydari,
and Kim-Hung Pho. Modeling and forecasting the spread and death rate of
coronavirus (COVID-19) in the world using time series models. Chaos, Solitons
& Fractals, 140:110151, nov 2020. doi: 10.1016/j.chaos.2020.110151.

14

[9] R Muthukrishnan and R Rohini. LASSO: A feature selection technique in predictive modeling for machine learning. In 2016 IEEE International Conference on Advances in Computer Applications (ICACA). IEEE, oct 2016. doi:
10.1109/icaca.2016.7887916.
[10] Asoke K. Nandi. Data modeling with polynomial representations and autoregressive time-series representations, and their connections. IEEE Access, 8:
110412–110424, 2020. doi: 10.1109/access.2020.3000860.
[11] Furqan Rustam, Aijaz Ahmad Reshi, Arif Mehmood, Saleem Ullah, Byung-Won
On, Waqar Aslam, and Gyu Sang Choi. COVID-19 future forecasting using
supervised machine learning models. IEEE Access, 8:101489–101499, 2020. doi:
10.1109/access.2020.2997311.
[12] Sourabh Shastri, Kuljeet Singh, Sachin Kumar, Paramjit Kour, and Vibhakar
Mansotra. Time series forecasting of covid-19 using deep learning models: IndiaUSA comparative case study. Chaos, Solitons & Fractals, 140:110227, nov 2020.
doi: 10.1016/j.chaos.2020.110227.
[13] Michal Wieczorek, Jakub Silka, and Marcin Woźniak. Neural network powered
COVID-19 spread forecasting model. Chaos, Solitons & Fractals, 140:110203,
nov 2020. doi: 10.1016/j.chaos.2020.110203.
[14] Abdelhafid Zeroual, Fouzi Harrou, Abdelkader Dairi, and Ying Sun. Deep learning methods for forecasting COVID-19 time-series data: A comparative study.
Chaos, Solitons & Fractals, 140:110121, nov 2020. doi: 10.1016/j.chaos.2020.
110121.
[15] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic
net. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
67(2):301–320, apr 2005. doi: 10.1111/j.1467-9868.2005.00503.x.

15

