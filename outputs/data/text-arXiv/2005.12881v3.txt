Evaluating epidemic forecasts in an interval format
Johannes Bracher1,2 , Evan L. Ray3 , Tilmann Gneiting2,4 and Nicholas G. Reich3
1

Karlsruhe Institute of Technology (KIT), Chair of Econometrics and Statistics
2
Heidelberg Institute for Theoretical Studies
3
University of Massachusetts, School of Public Health and Health Sciences, Department of Biostatistics
and Epidemiology
4
Karlsruhe Institute of Technology (KIT), Institute for Stochastics

arXiv:2005.12881v3 [stat.AP] 8 Jan 2021

January 12, 2021

Abstract
For practical reasons, many forecasts of case, hospitalization and death counts in the context
of the current COVID-19 pandemic are issued in the form of central predictive intervals at
various levels. This is also the case for the forecasts collected in the COVID-19 Forecast Hub
(https://covid19forecasthub.org/). Forecast evaluation metrics like the logarithmic score,
which has been applied in several infectious disease forecasting challenges, are then not available
as they require full predictive distributions. This article provides an overview of how established
methods for the evaluation of quantile and interval forecasts can be applied to epidemic forecasts
in this format. Specifically, we discuss the computation and interpretation of the weighted
interval score, which is a proper score that approximates the continuous ranked probability score.
It can be interpreted as a generalization of the absolute error to probabilistic forecasts and allows
for a decomposition into a measure of sharpness and penalties for over- and underprediction.

Author summary
During the COVID-19 pandemic, model-based probabilistic forecasts of case, hospitalization and
death numbers can help to improve situational awareness and guide public health interventions.
The COVID-19 Forecast Hub (https://covid19forecasthub.org/) collects such forecasts from
numerous national and international groups. Systematic and statistically sound evaluation of
forecasts is an important prerequisite to revise and improve models and to combine different
forecasts into ensemble predictions. We provide an intuitive introduction to scoring methods,
which are suitable for the interval/quantile-based format used in the Forecast Hub, and compare
them to other commonly used performance measures.

1

Introduction

There is a growing consensus in infectious disease epidemiology that epidemic forecasts should be
probabilistic in nature, i.e. should not only state one predicted outcome, but also quantify their
own uncertainty. This is reflected in recent forecasting challenges like the US CDC FluSight
Challenge [1] and the Dengue Forecasting Project [2], which required participants to submit
forecast distributions for binned disease incidence measures. Storing forecasts in this way enables
the evaluation of standard scoring rules like the logarithmic score [3], which has been used in
both of the aforementioned challenges. This approach, however, requires that a simple yet
1

meaningful binning system can be defined and is followed by all forecasters. In acute outbreak
situations like the current COVID-19 outbreak, where the range of observed outcomes varies
considerably across space and time and forecasts are generated under time pressure, it may not
be practically feasible to define a reasonable binning scheme.
An alternative is to store forecasts in the form of predictive quantiles or intervals. This is
the approach used in the COVID-19 Forecast Hub [4, 5]. The Forecast Hub serves to aggregate
COVID-19 death and hospitalization forecasts in the United States (both national and state
levels) and is the data source for the CDC COVID-19 forecasting web page [6]. Contributing
teams are asked to report the predictive median and central prediction intervals with nominal
levels 10%, 20%, . . . , 90%, 95%, 98%, meaning that the (0.01, 0.025, 0.05, 0.10, . . . , 0.95, 0.975,
0.99) quantiles of predictive distributions have to be made available. Using such a format,
predictive distributions can be stored in reasonable detail independently of the expected range
of outcomes. However, suitably adapted scoring methods are required, as e.g. the logarithmic
score cannot be evaluated based on quantiles alone. This note provides an introduction to
established quantile and interval-based scoring methods [3] with a focus on their application to
epidemiological forecasts.

2
2.1

Forecast evaluation using proper scoring rules
Common scores to evaluate full predictive distributions

Proper scoring rules [3] are today the standard tools to evaluate probabilistic forecasts. Propriety
is a desirable property of a score as it encourages honest forecasting, meaning that forecasters
have no incentive to report forecasts differing from their true belief about the future. We start
by providing a brief overview of scores which can be applied when the full predictive distribution
is available.
A widely used proper score is the logarithmic score. In the case of a discrete set of possible
outcomes {1, . . . , M } (as is the case for counts or binned measures of disease activity), it is
defined as [3]
logS(F, y) = log(py ).
Here py is the probability assigned to the observed outcome y by the forecast F . The log score
is positively oriented, meaning that larger values are better. A potential disadvantage of this
score is that it degenerates to −∞ if py = 0. In the FluSight Challenge the score is therefore
truncated at a value of −10 [7]; we note that when this truncation is performed, the score is no
longer proper.
Until the 2018/2019 edition, a variation of the logarithmic score called the multibin logarithmic score was used in the FluSight Challenge. For discrete and ordered outcomes it is defined
as [8]
!
d
X
MBlogS(F, y) = log
py+i ,
i=−d

i.e. also counts probability mass within a certain tolerance range of ±d ordered categories. The
goal of this score is to measure “accuracy of practical significance” [9]. It thus offers a more
accessible interpretation to practitioners, but has the disadvantage of being improper [10, 11].
An alternative score which is considered more robust than the logarithmic score [12] is the
continuous ranked probability score
Z ∞
CRPS(F, y) =
{F (x) − 1(x ≥ y)}2 dx,
−∞

2

0

50

100

150

200

y

600
100

150

upper end u

300
200

2
α

100

ISα
50

lower end l

400

500

observed y

0.8
0.6
0.4
0

slope =

u−l
0

−6
−7

0.000

1 − (1 − F(y))2
F(y)
F(y)2
CRPS

0.2

−5
0.005

cumulative distr. function F(y)

0.010

logS = log(py)

0.015

−4

0.0

observed y

pred. probability py

0.020

1.0

where F is interpreted as a cumulative distribution function (CDF). Note that in the case
of integer-valued outcomes the CRPS simplifies to the ranked probability score, compare [13]
and [14]. The CRPS represents a generalization of the absolute error to probabilistic forecasts
(implying that it is negatively oriented) and has been commonly used to evaluate epidemic
forecasts [15, 16]. The CRPS does not diverge to ∞ even if a forecast assigns zero probability to
the eventually observed outcome, making it less sensitive to occasional misguided forecasts. It
depends on the application setting whether an extreme penalization of such “missed” forecasts
is desirable or not, and in certain contexts the CRPS may seem lenient. A practical advantage,
however, is that there is no need for thresholding it at an arbitrary value.
To facilitate an intuitive understanding of the different scores, Fig 1 graphically illustrates
the definitions of the logarithmic score (left) and CRPS (middle). For the CRPS note that if an
observation falls far into the tails of the predictive distribution, one of the two blue areas representing the CRPS will essentially disappear, while the size of the other depends approximately
linearly on the observed value y (with a slope of 1). This illustrates the close link between the
CRPS and the absolute error.

200

y

0

50

100

150

200

y

Figure 1: Visualization of the logS, CRPS and IS. Left: The logarithmic score only
depends on the predictive probability assigned to the observed event y (of which one takes the
logarithm). Middle: The CRPS can be interpreted as a measure of the distance between the
predictive cumulative distribution function and a vertical line at the observed value. Right: The
interval score ISα is a piecewise linear function which is constant inside the respective prediction
interval and has slope ±2/α outside of it.

2.2

Scores for forecasts provided in an interval format

Both the logS and the CRPS cannot be evaluated directly if forecasts are provided in an interval format. If many intervals are provided, approximations may be feasible to some degree,
but problems arise if observations fall in the tails of predictive distributions (see Discussion
section). It is therefore advisable to apply scoring rules designed specifically for forecasts in a
quantile/interval format. A simple proper score which requires only a central (1 − α) × 100%
prediction interval (in the following: PI) is the interval score [3]
ISα (F, y) = (u − l) +

2
2
× (l − y) × 1(y < l) +
× (y − u) × 1(y > u).
α
α

Here, 1 is the indicator function, meaning that 1(y < l) = 1 if y < l and 0 otherwise. The
terms l and u denote the α/2 and 1 − α/2 quantiles of F . The interval score consists of three
intuitively meaningful quantities:
• The width u − l of the central (1 − α) PI, which describes the sharpness of F .
3

• A penalty term α2 × (l − y) × 1(y < l) for observations falling below the lower endpoint l
of the (1 − α) × 100% PI. The penalty is proportional to the distance between y and the
lower end l of the interval, with the strength of the penalty depending on the level α (the
higher the nominal level (1 − α) × 100% of the PI the more severe the penalty).
• An analoguous penalty term
upper end u of the PI.

2
α

× (y − u) × 1(y > u) for observations falling above the

A graphical illustration of this definition can be found in the right panel of Fig 1. Note that
the interval score has recently been used to evaluate forecasts of SARS-CoV-1 and Ebola [17] as
well as SARS-CoV-2 [18].
To provide more detailed information on the predictive distribution it is common to report
not just one, but several central PIs at different levels (1 − α1 ) < (1 − α2 ) < · · · < (1 − αK ),
along with the predictive median m. The latter can informally be seen as a central prediction
interval at level (1 − α0 ) → 0. To take all of these into account, a weighted interval score can
be evaluated:1
!
K
X
1
WISα0:K (F, y) =
× w0 × |y − m| +
(1)
{wk × ISαk (F, y)} .
K + 1/2
k=1

This score is a special case of the more general quantile score [3] and it is proper for any set of
non-negative (un-normalized) weights w0 , w1 , . . . , wK . A natural choice is to set
wk =

αk
2

(2)

with w0 = 1/2, as for large K and equally spaced values of α1 , . . . , αK (stretching over the unit
interval) it can be shown that under this choice of weights
WISα0:K (F, y) ≈ CRPS(F, y).

(3)

This follows directly from known properties of the quantile score and CRPS [19, 20], see Appendix A. Consequently the score can be interpreted heuristically as a measure of distance
between the predictive distribution and the true observation, where the units are those of the
absolute error, on the natural scale of the data. Indeed, in the case K = 0 where only the predictive median is used, WISα0 (F, y) is equal to the absolute error. Furthermore, the WIS and
CRPS reduce to the absolute error when F is a point forecast [3]. We will use the specification
(2) of the weights in the remainder of the article, but remark that different weighting schemes
may be reasonable depending on the application context.
In practice, evaluation of forecasts submitted to the COVID-19 Forecast Hub will be done
based on the predictive median and K = 11 prediction intervals with α1 = 0.02, α2 = 0.05, α3 =
0.1, . . . , α11 = 0.9 (implying nominal coverages of 98%, 95%, 90%, . . . , 10%). This corresponds
to the quantiles teams are required to report in their submissions and implies that relative to
the CRPS, slightly more emphasis is given to intervals with high nominal coverage.
Similarly to the interval score, the weighted interval score can be decomposed into weighted
sums of the widths of PIs and penalty terms, including the absolute error. These two components
1

Note that this definition has been modified slightly relative to previous versions of this preprint. For the
previous definition
!
K
X
1
WISα0:K (F, y) =
× w0 × 2 × |y − m| +
{wk × ISαk (F, y)}
K +1
k=1

exact equivalence to the expression in equation (4) did not hold. It was therefore decided to revise the definition.

4

represent the sharpness and calibration of the forecasts, respectively, and can be used in graphical
representations of obtained scores (see Section 4.1).
Note that a score corresponding to one half of what we refer to as the WIS was used in
the 2014 Global Energy Forecasting Competition [21]. The score was framed as an average of
pinball losses for the predictive 1st through 99th percentiles. We note in this context that with
the weights wk from equation (2) the WIS can also be expressed as
2K+1
X
1
WISα0:K (F, y) =
×
2 × {1(y ≤ qτk ) − τk } × (qτk − y),
2K + 1

(4)

k=1

see Appendix A. Here, the levels 0 < τ1 < · · · < τK+1 = 1/2 < · · · < τ2K+1 < 1 and the
associated quantiles qτ1 , . . . , qτ2K+1 correspond to the median and the 2K quantiles defining
the central PIs at levels 1 − α1 , . . . , 1 − αK . In this paper, we preferred to motivate the score
through central predictive intervals at different levels, which are a commonly used concept in
epidemiology. However, when applying e.g. quantile regression methods for ensemble building,
formulation (4) may seem more natural.

2.3

Aggregation of scores

To compare different prediction methods systematically it is necessary to aggregate the scores
they achieved over time and for various forecast targets. The natural way of aggregating proper
scores is via their sum or average [3], as this ensures that propriety is maintained. If forecasts are
made at several different horizons it is often helpful to also inspect average scores separately by
horizon and assess how forecast quality deteriorates over time. Scores for longer time horizons
often tend to show larger variability and can dominate average scores. This is especially true
when forecasting cumulative case or death numbers, where forecast errors build up over time,
and a stratified analysis can be more informative in this case.
It may be of interest to formally assess the strength of evidence that there is a difference
in mean forecast skill between methods. Various tests exist to this end, the most commonly
used being the Diebold-Mariano test [22]. However, this test does not have a widely accepted
extension to account for dependencies between multiple forecasts made for different time-series
or locations. Similar challenges arise when predictions at multiple horizons are issued at the
same time. An interesting strategy in this context is to treat these predictions as a path forecast
and assess them jointly [23, 24]. Generally, theoretically principled methods for multivariate
forecast evaluation exist [25] and have found applications in disease forecasting [15].
A topic closely related to path forecasting is forecasting of more qualitative or longer-term
characteristics of an epidemic curve. For instance, in the FluSight challenges [1] forecasts for
the timing and strength of seasonal peaks have been assessed. While such targets can be of
great interest from a public health perspective, it is not always obvious how to define them for
an emerging rather than seasonal disease. A possibility would be to consider maximum weekly
incidences over a gliding time window. This could provide additional information on the peak
healthcare demand expected over a given time period, an aspect which is often not reflected well
in independent week-wise forecasts [26].

3

Qualitative comparison for different scores

We now compare various scores using simple examples, covering scores for point predictions,
prediction intervals and full predictive distributions.

5

3.1

Illustration for an integer-valued outcome

Fig 2 illustrates the behaviour of five different scores for a negative binomial predictive distribution F with expectation µF = 60 and size parameter ψF = 4 (standard deviation ≈ 31.0).
We consider the logarithmic score, absolute error, interval score with α = 0.2 (IS0.2 ), CRPS,
and two versions of the weighted interval score. Firstly, we consider a score with K = 3 and
α1 = 0.1, α2 = 0.4, α3 = 0.7, which we denote by WIS∗ . Secondly, we consider a more detailed
score with K = 11 and α1 = 0.02, α2 = 0.05, α3 = 0.1, . . . , α11 = 0.9, denoted by WIS (as this
is the version used in the COVID-19 Forecast Hub we will focus on it in the remainder of the
article). The resulting scores are shown as a function of the observed value y. Qualitatively
all curves look similar. However, some differences can be observed. The best (lowest) negative
logS is achieved if the observation y coincides with the predictive mode. For the interval-based
scores, AE and CRPS, the best value results if y equals the median (for the IS0.2 in the middle
right panel there is a plateau as it does not distinguish between values falling into the 80% PI).
The negative logS curve is more smooth and increases the more steeply the further away the
observed y is from the predictive mode. The curve shows some asymmetry, which is absent
or less pronounced in the other plots. The IS and WIS curves are piecewise linear. The WIS
has a more modest slope closer to the median and a more pronounced one towards the tails
(approaching −1 and 1 in the left and right tail, respectively). Both versions of the WIS represent a good approximation to the CRPS. For the more detailed version with 11 intervals plus
the absolute error, slight differences to the CRPS can only be seen in the extreme upper tail.
When comparing the CRPS and WIS∗ /WIS scores to the absolute error, it can be seen that the
latter are larger in the immediate surroundings of the median (and always greater than zero),
but lower towards the tails. This is because they also take into account the uncertainty in the
forecast distribution.

3.2

Differing behaviour if agreement between predictions and observations
is poor

Qualitative differences between the logarithmic and interval-based scores occur predominantly
if observations fall into the tails of predictive distributions. We illustrate this with a second
example. Consider two negative binomial forecasts: F with expectation 60 and size 4 (standard
deviation ≈ 31) as before, and G with expectation 80, and size 10 (standard deviation ≈ 26.8).
G thus has higher expectation than F and is sharper. If we now observe y = 190, i.e. a count
considerably higher than suggested by either F or G, the two scores yield different results, as
illustrated in Fig 3.
• The logS favours F over G, as the former is more dispersed and has slightly heavier
tails. Therefore y = 190 is considered somewhat more “plausible” under F than under G
(logS(F, 190) = −9.37, logS(G, 190) = −9.69).
• The WIS (with K = 11 as in the previous section), on the other hand, favours G as its
quantiles are generally closer to the observed value y (WIS(F, 190) = 103.9, WIS(G, 190) =
87.8).
This behaviour of the WIS is referred to as sensitivity to distance [3]. In contrast, the
logS is a local score which ignores distance. Winkler [27] argues that local scoring rules can
be more suitable for inferential problems, while sensitivity to distance is reasonable in many
decision making settings. In the public health context, say a prediction of hospital bed need
on a certain day in the future, it could be argued that for y = 190 the forecast G was indeed
more useful than F . While a pessimistic scenario under G (defined as the 95% quantile of the
predictive distribution) implies 128 beds needed and thus fell considerably short of y = 190, it
still suggested more adequate need for preparation than F , which has a 95% quantile of 118.
6

0.01

7
6

0.005

5
4

CRPS

100

150

0

200

0
0

50

100

y
0.015

100
0.005

50
0

150

150

20% 50% 80%

100
0.005

50

0
100

0

200

0
0

50

y

400
0.005

200
0

150
0.01

150

100
0.005

50

0
100

200

0.015

WIS

IS0.2

0.01

50

150

0.015

90%

600

0

100
y

predictive probability

10%

95%

0.01
WIS*

0.01

0.015
5% 35% 65%

predictive probability

absolute error

150

50

200

y

50%

0

150

predictive probability

50

0.005

50

0
0

0.01
100

0

200

0
0

y

predictive probability

− logS

8

0.015
150

predictive probability

0.015

9

predictive probability

10

50

100

150

200

y

Figure 2: Illustration of different scoring rules. Logarithmic score, absolute error, interval
score (with α = 0.2), CRPS and two versions of the weighted interval score. These are denoted
by WIS∗ (with K = 3, α1 = 0.1, α2 = 0.4, α3 = 0.7) and WIS (K = 11, α1 = 0.02, α2 =
0.05, α3 = 0.1, . . . , α11 = 0.9). Scores are shown as a function of the observed value y. The
predictive distribution F is negative binomial with expectation 60 and size 4. Note that the top
left panel shows the negative logS, i.e. −logS, which like the other scores is negatively oriented
(smaller values are better).
We argue that poor agreement between forecasts and observations is more likely to occur for
COVID-19 deaths than e.g. for seasonal ILI (influenza-like illness) intensity, which due to larger
amounts of historical data is more predictable. Sensitivity to distance then leads to more robust
scoring with respect to decision making, without the need to truncate at an arbitrary value (as
required for the log score). While these are pragmatic statistical considerations, it could be
argued that the choice of scoring rule should depend on the cost of different types of errors. If
the cost of single misguided forecasts is very high, a conservative evaluation approach using the
logarithmic score may be more appropriate. This question is obviously linked to the preferences
and priorities of forecasts recipients, in our case public health officials and the general public.
Studying these preferences in more detail is an interesting avenue for further research.

7

− logS

8

0.01

7
6

0.005

5
4

150
0.01
WIS

9

0.015

50

100

150

0.005

50

0
0

100

0

200

0
0

y

predictive probability

0.015
predictive probability

10

50

100

150

200

y

Figure 3: Disagreement between logarithmic score and WIS. Negative logarithmic score
and weighted interval score (with α1 = 0.02, α2 = 0.05, α3 = 0.1, . . . , α11 = 0.9) as a function of
the observed value y. The predictive distributions F (green) and G (red) are negative binomials
with expectations µF = 60, µG = 80 and sizes ψF = 4, ψG = 10. The black dashed line shows
y = 190 as discussed in the text.

4

Application to FluSight forecasts

In this section some additional practical aspects are discussed using historical forecasts from
the 2016/2017 edition of the FluSight Challenge. Note that these were originally reported in
a binned format, but for illustration we translated them to a quantile format for some of the
below examples.

4.1

An easily interpretable graphic display of the WIS

The decomposition of the WIS into the average width of PIs and average penalty for observations outside the various PIs (see Section 2.1) enables an intuitive graphical display to compare
different forecasts and understand why one forecast outperforms another. Distinguishing also
between penalties for over- and underprediction can be informative about systematic biases or
asymmetries. Note that decompositions of quantile or interval scores for visualization purposes
have been suggested before, see e.g. [28].
Fig 4 shows a comparison of the IS0.2 and WIS (with K = 11 as before) obtained for
one-week-ahead forecasts by the KCDE and SARIMA models during the 2016/2017 FluSight
Challenge, [9, 29], using data obtained from [30]. It can be seen that, while KCDE and SARIMA
issued forecasts of similar sharpness (average widths of PIs, blue bars), SARIMA is more strongly
penalized for PIs not covering the observations (orange and red bars). Broken down to a single number, the bottom right panel shows that predictions from KCDE and SARIMA were on
average off by 0.25 and 0.35 percentage points, respectively (after taking into account the uncertainty implied by their predictions). Both methods are somewhat conservative, with 80% PIs
covering 88% (SARIMA) and 100% of the observations (KCDE). When comparing the plots for
IS0.2 and WIS, it can be seen that the former strongly punishes larger discrepancies between
forecasts and observations while ignoring smaller differences. The latter translates discrepancies
to penalties in a smoother fashion, as could already be seen in Fig 2.

4.2

Visually assessing calibration

While the middle row of Fig 4 provides a good intuition of the sharpness of different forecasts,
different visual tools exist to assess their calibration. A commonly used approach is via probability
integral transform (PIT) histograms [31, 13], see also [15] for adaptations to count data. These
8

KCDE
5
3

1.5

5
4
IS0.2

4
IS0.2

SARIMA
6

penalty for exceeding upper limit of 80% PI
width of 80% PI
penalty for falling below lower limit of 80% PI

3

2

2

1

1

0

0
0

5

10

15

20

25

30

0

5

10

week of season
8

%wILI

%wILI

2
0

0.0

15

20

25

30

0

5

10

15

20

25

30

KCDE

SARIMA

0.4

1.0
0.5

average WIS

2.0
1.5
WIS

WIS

SARIMA

week of season

weighted penalty for exceedance of upper limit
weighted width of PIs
weighted penalty for falling below lower limit

1.5

KCDE

0.5

week of season
2.0

0.5

4

0
10

30

6

2

5

25

8

4

0

20

1.0

week of season

median
80% PI
observed

6

15

average IS0.2

6

0.3

0.2

1.0
0.1
0.5

0.0

0.0

0.0
0

5

10

15

20

25

30

0

5

10

week of season

15

20

25

30

week of season

Figure 4: Interval and weighted interval score applied to FluSight forecasts. Comparison of one-week-ahead forecasts by KCDE and SARIMA over the course of the 2016/2017
FluSight season. The top row shows the interval score with α = 0.2, the bottom row the weighted
interval score with α1 = 0.02, α2 = 0.05, α3 = 0.1, . . . , α11 = 0.9. The panels at the right show
mean scores over the course of the season. All bars are decomposed into the contribution of
interval widths (i.e. a measure of sharpness; blue) and penalties for over- and underprediction
(orange and red, respectively). Note that the absolute values of the two scores are not directly
comparable as the WIS involves re-scaling of the included interval scores.
show the empirical distribution of
PIT(F, yobs ) = F (yobs )
across different forecasts from the same model. Here F represents the predictive cumulative
distribution function. For a calibrated forecast the PIT histogram should be approximately uniform, and deviations from uniformity indicate bias or problems with the dispersion of forecasts.
E.g., L-shaped PIT histograms indicate a downward bias of forecasts, while a J shape indicates
an upward bias. Under- and overdispersed forecasts lead to U and inverse-U-shaped PIT histograms, respectively. Fig 5 shows PIT histograms for one-week-ahead forecasts from KCDE
and SARIMA. While no apparent biases can be seen, both models seem to produce forecasts
with too high dispersion. This is especially visible for KCDE, for which hardly any realizations
fell into the two extreme deciles of the respective forecast distributions.
The exact PIT values cannot be computed if forecasts are reported in a quantile format, but
if sufficiently many quantiles are available one can evaluate in which decile or ventile they fall.
This is sufficient to represent them graphically in a histogram with the respective number of bins.
A technical problem arises if an observation is exactly equal to one or several of the reported
quantiles (which can happen especially in low count settings). The corrections for discreteness
suggested by [13] cannot be applied in this case and there does not seem to be a standard
9

1.5
0.0

0.5

1.0

Density

1.0
0.0

0.5

Density

1.5

2.0

SARIMA

2.0

KCDE

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

PIT

0.4

0.6

0.8

1.0

PIT

Figure 5: PIT histograms. PIT histograms for one-week-ahead forecasts from the KCDE and
SARIMA models, 2016–2017 FluSight season. Note that to account for the discreteness of the
binned distribution we employed the non-randomized correction suggested in [13].
approach for this. A practical strategy is to split up such a count between the neighbouring
bins of the histogram (assigning 1/2 to each bin if the realized value coincides with one reported
quantile, 1/4, 1/2, 1/4 if it coincides with two, 1/6, 1/3, 1/3, 1/6 if it coincides with three, and
so on).

4.3

Empirical agreement between different scores

To explore the agreement between different scores, we applied several of them to one- through
four-week-ahead forecasts from the 2016/2017 edition of the FluSight Challenge. We compare
the negative logarithmic score, the negative multibin logarithmic score with a tolerance of 0.5
percentage points (both with truncation at −10), the CRPS, the absolute error of the median,
the interval score with α = 0.2 and the weighted interval score (with K = 11 and α1 = 0.02, α2 =
0.05, α3 = 0.1, . . . , α11 = 0.9 as in the previous sections). To evaluate the CRPS and interval
scores we simply identified each bin with its central value to which a point mass was assigned.
Fig 6 shows scatterplots of mean scores achieved by 26 models (averaged over weeks, forecast
horizons and geographical levels; the naı̈ve uniform model was removed as it performs clearly
worst under almost all metrics).
As expected, the three interval-based scores correlate more strongly with the CRPS and
the absolute error than with the logarithmic score. Agreement between the WIS and CRPS is
almost perfect, meaning that in this example the approximation (3) works quite well based on
the 23 available quantiles. Agreement between the interval-based score and the logS is mediocre,
in part because the models FluOutlook Mech and FluOutlook MechAug receive comparatively
good interval-based scores (as well as CRPS, absolute errors and even MBlogS), but exceptionally
poor logS. The reason is that while having a rather accurate central tendency, they are too sharp
with tails that are too light. This is sanctioned severely by the logarithmic score, but much less
so by the other scores (this is related to the discussion in Section 3.2). The WIS score (and thus
also the CRPS) shows remarkably good agreement with the MBlogS, indicating that distancesensitive scores may be able to formalize the idea of a score which is slightly more “generous”
than the logS while maintaining propriety. Interestingly, all scores agree that the three best
models are LANL DBMplus, Protea Cheetah and Protea Springbok.

10

8

1.0
0.8
WIS

IS0.2

6
4
2

0.6
0.4
0.2

corr: 0.33

0
3.5

4.0

4.5

corr: 0.34

0.0

5.0

3.5

4.0

−logS
1.0
0.8
WIS

IS0.2

6
4
2

0.6
0.4
0.2

corr: 0.77

0
0.8

0.9

1.0

1.1

1.2

1.3

corr: 0.88

0.0

1.4

0.8

0.9

1.0

−MBlogS

1.1

1.2

1.3

1.4

−MBlogS

8

1.0
0.8
WIS

6
IS0.2

5.0

−logS

8

4
2

0.6
0.4
0.2

corr: 0.8

0
0.0

0.2

0.4

0.6

0.8

corr: 1

0.0

1.0

0.0

0.2

0.4

CRPS

0.6

0.8

1.0

CRPS

8

1.0
0.8
WIS

6
IS0.2

4.5

4
2

0.6
0.4
0.2

corr: 0.6

0
0.0

0.2

0.4

0.6

0.8

corr: 0.94

0.0

1.0

0.0

0.2

0.4

AE

0.6

0.8

1.0

AE
1.0

WIS

0.8
0.6
0.4
0.2
corr: 0.82

0.0
0

2

4

6

8

IS0.2

Figure 6: Comparison of 26 models participating in the 2016/2017 FluSight Challenge
under different scoring rules. Shown are mean scores averaged over one through four week
ahead forecasts, different geographical levels, weeks and forecast horizons. Compared scores:
negative logarithmic score and multibin logarithmic score, continuous ranked probability score,
interval score (α = 0.2), weighted interval score with K = 11. Plots comparing the WIS to
CRPS and AE, respectively, also show the diagonal in grey as these three scores operate on
the same scale. All shown scores are negatively oriented. The models FluOutlook Mech and
FluOutlook MechAug are highlighted in orange as they rank very differently under different
scores.

11

5

A brief remark on evaluating point forecasts

While the main focus of this note is on the evaluation of forecast intervals, we also briefly address
how point forecasts submitted to the COVID-19 Forecast Hub will be evaluated. As in the
FluSight Challenge [7], the absolute error (AE) will be applied. This implies that teams should
report the predictive median as a point forecast [32]. Using the absolute error in combination
with WIS is appealing as both can be reported on the same scale (that of the observations).
Indeed, as mentioned before, the absolute error is the same as the WIS (and CPRS) of a
distribution putting all probability mass on the point forecast.
The absolute error, when averaged across time and space, is dominated by forecasts from
larger states and weeks with high activity (this also holds true for the CRPS and WIS). One
may thus be tempted to use a relative measure of error instead, such as the mean absolute
percentage error (MAPE). We argue, however, that emphasizing forecasts of targets with higher
expected values is meaningful. For instance, there should be a larger penalty for forecasting 200
deaths if 400 are eventually observed than for forecasting 2 deaths if 4 are observed. Relative
measures like the MAPE would treat both the same. Moreover, the MAPE does not encourage
reporting predictive medians nor means, but rather obscure and difficult to interpret types of
point forecasts [32, 14]. It should therefore be used with caution.

6

Discussion

In this paper we have provided a practical and hopefully intuitive introduction on the evaluation
of epidemic forecasts provided in an interval or quantile format. It is worth emphasizing that
the concepts underlying the suggested procedure are by no means new or experimental. Indeed,
they can be traced back to [33] and [34]. As mentioned before, a special case of the WIS was
used in the 2014 Global Energy Forecasting Competition [21]. A scaled version of the interval
score was used in the 2018 M4 forecasting competition [35]. The ongoing M5 competition uses
the so-called weighted scaled pinball loss (WSPL), which can be seen as a scaled version of the
WIS based on the predictive median and 50%, 67%, 95% and 99% PIs [36].
Note that we restrict attention to the case of central prediction intervals, so that each
prediction interval is clearly associated with two quantiles. The evaluation of prediction intervals
which are not restricted to be central is conceptually challenging [37, 38], and we refrain from
adding this complexity.
The method advocated in this note corresponds to an approximate CRPS computed from
prediction intervals at various levels. A natural question is whether such an approximation would
also be feasible for the logarithmic score, leading to an evaluation metric closer to that from
the FluSight Challenge. We see two principal difficulties with such an approach. Firstly, some
sort of interpolation method would be needed to obtain an approximate density or probability
mass function within the provided intervals. While the best way to do this is not obvious,
a pragmatic solution could likely be found. A second problem, however, would remain: For
observations outside of the prediction interval with the highest nominal coverage (98% for the
COVID-19 Forecast Hub) there is no easily justifiable way of approximating the logarithmic
score, as the analyst necessarily has to make strong assumptions on the tail behaviour of the
forecast. As such poor forecasts typically have a strong impact on the average log score, they
cannot be neglected. And given that forecasts are often evaluated for many locations (e.g., over
50 US states and territories), even for a perfectly calibrated model there will on average be
one such observation falling in the far tail of a predictive distribution every week. One could
think about including even more extreme quantiles to remedy this, but forecasters may not
be comfortable issuing these and the conceptual problem would remain. This is linked to the
general problem of low robustness of the logarithmic score. We therefore argue that especially
12

in contexts with low predictability such as the current COVID-19 pandemic, distance-sensitive
scores like the CRPS or WIS are an attractive option.

A

Relationship between quantile score, interval score and CRPS

The standard piecewise linear quantile score [3, 32] for the level τ is defined as
QSτ (F, y) = 2 × {1(y ≤ qτ ) − τ } × (qτ − y),
where qτ is the τ quantile of the forecast F and y is the observed outcome. It can be shown by
some re-ordering of terms that the interval score of a central (1 − α) PI can be computed from
the quantile scores at levels α/2 and 1 − α/2 as
QSα/2 (F, y) + QS1−α/2 (F, y)
.
(5)
α
Interestingly, this is the only available proper interval score that is invariant under translation
[38, Theorem 4], so that for a prediction horizon of one time unit, evaluations in terms of incident
counts yield the same results as evaluations in terms of cumulative counts.
Moreover it is known [19, 20] that
ISα (F, Y ) =

Z

1

QSτ (F, y) dτ,

CRPS(F, y) =
0

≈

=

2K+1
X
1
×
QSτk (F, y),
2K + 1

1
×
2K + 1

k=1
2K+1
X

2 × {1(y ≤ qτk ) − τk } × (qτk − y),

(6)

k=1

with a large number of (approximately) equally spaced levels τk stretching the unit interval
such that τ1 < · · · < τK+1 = 1/2 < · · · < τ2K+1 . Note that expression (6) is the same as the
alternative expression (4) for the WIS from the main text, where τk = αk /2 and τ2K+2−k =
1 − αk /2 for k = 1, . . . , K.
Indeed, starting from the original definition of the WIS in equation (1) with weights w0 = 1/2
and wk = αk /2 for k = 1, . . . , K as in equation (2) from the main text, using equation (5), and
noting that τK+1 = 1/2 and qτK+1 = m is the median, we see that
1
WISα0:K (F, y) =
×
2K + 1
1
=
2K + 1
=

|y − m| +

K
X

αk ×
k=1
K n
X

ISαk (F, y)

!
o
QSτk (F, y) + QSτ2K+2−k (F, y)

QSτK+1 (F, y) +

1
×
2K + 1

!

k=1
2K+1
X

2 × {1(y ≤ qτk ) − τk } × (qτk − y).

k=1

As noted in Section 2.2, the τk we use in practice are not equally spaced in the tails (due
to the addition of the quantiles at levels 0.01, 0.025, 0.975, and 0.99 forming the 95% and 98%
prediction intervals). Relative to the CRPS, we thus put slightly more weight on the tails.

13

Reproducibility
Code to reproduce Fig 1–6 has been made available at
https://github.com/reichlab/proper-scores-comparison. All data used in this paper have
been taken from the public cdc-flusight-ensemble repository [30].

Acknowledgements
We thank Ryan Tibshirani and Sebastian Funk for their insightful comments. The work of
Johannes Bracher was supported by the Helmholtz Foundation via the SIMCARD Information
& Data Science Pilot Project. Tilmann Gneiting is grateful for support by the Klaus Tschira
Foundation. Evan L. Ray and Nicholas G. Reich were supported by the US Centers for Disease
Control and Prevention (1U01IP001122). The content is solely the responsibility of the authors
and does not necessarily represent the official views of the CDC.

14

References
[1] McGowan CJ, Biggerstaff M, Johansson M, Apfeldorf KM, Ben-Nun M, Brooks L, et al.
Collaborative efforts to forecast seasonal influenza in the United States, 2015–2016. Scientific Reports. 2019; paper no. 683. doi:10.1038/s41598-018-36361-9.
[2] Johansson MA, Apfeldorf KM, Dobson S, Devita J, Buczak AL, Baugher B, et al. An open
challenge to advance probabilistic forecasting for dengue epidemics. Proceedings of the
National Academy of Sciences. 2019;116(48):24268–24274. doi:10.1073/pnas.1909865116.
[3] Gneiting T, Raftery AE.
Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association. 2007;102(477):359–378.
doi:10.1198/016214506000001437.
[4] UMass-Amherst Influenza Forecasting Center of Excellence. COVID-19 Forecast Hub;
2020. Accessible online at https://github.com/reichlab/covid19-forecast-hub. Last
accessed 9 October 2020.
[5] Ray EL, Wattanachit N, Niemi J, Kanji AH, House K, Cramer EY, Bracher J, Zheng A,
Yamana TK, Xiong X, Woody S, Wang Y, Wang L, Walraven RL, Tomar V, Sherratt K,
Sheldon D, Reiner RC, Prakash BA, Osthus D, Li ML, Lee EC, Koyluoglu U, Keskinocak P,
Gu Y, Gu Q, George GE, Espana G, Corsetti S, Chhatwal J, Cavany S, Biegel H, Ben-Nun
M, Walker J, Slayton R, Lopez V, Biggerstaff M, Johansson MA, Reich NG, COVID-19
Forecast Hub Consortium. Ensemble forecasts of coronavirus disease 2019 (COVID-19) in
the U.S. Preprint, medRxiv. 2020. doi:10.1101/2020.08.19.20177493
[6] Centers for Disease Control and Prevention. COVID-19 Forecasting: Background
Information.
Accessible online at https://www.cdc.gov/coronavirus/2019-ncov/
cases-updates/forecasting.html. Last accessed 9 October 2020.
[7] Centers for Disease Control and Prevention. Forecast the 2019–2020 Influenza Season
Collaborative Challenge; 2019. Accessible online at https://predict.cdc.gov/api/
v1/attachments/flusight_2019-2020/2019-2020_flusight_national_regional_
guidance_final.docx.
[8] Centers for Disease Control and Prevention. Forecast the 2018–2019 Influenza Season Collaborative Challenge; 2018. Accessible online at https://predict.cdc.gov/
api/v1/attachments/flusight%202018%E2%80%932019/flu_challenge_2018-19_
tentativefinal_9.18.18.docx.
[9] Reich NG, Brooks LC, Fox SJ, Kandula S, McGowan CJ, Moore E, et al. A collaborative multiyear, multimodel assessment of seasonal influenza forecasting in the
United States. Proceedings of the National Academy of Sciences. 2019;116(8):3146–3154.
doi:10.1073/pnas.1812594116.
[10] Bracher J. On the multibin logarithmic score used in the FluSight competitions. Proceedings
of the National Academy of Sciences. 2019;116(42):20809–20810. doi:pnas.1912147116.
[11] Reich NG, Osthus D, Ray EL, Yamana TK, Biggerstaff M, Johansson MA, et al. Reply to
Bracher: Scoring probabilistic forecasts to maximize public health interpretability. Proceedings of the National Academy of Sciences. 2019;116(42):20811–20812. doi:pnas.1912694116
[12] Gneiting T, Balabdaoui F, Raftery AE. Probabilistic forecasts, calibration and sharpness.
Journal of the Royal Statistical Society: Series B (Statistical Methodology). 2007;69(2):243–
268. doi:10.1111/j.1467-9868.2007.00587.x.
15

[13] Czado C, Gneiting T, Held L. Predictive model assessment for count data. Biometrics.
2009;65(4):1254–1261. doi:10.1111/j.1541-0420.2009.01191.x
[14] Kolassa S. Evaluating predictive count data distributions in retail sales forecasting. International Journal of Forecasting. 2016;32(3):788–803. doi:10.1016/j.ijforecast.2019.02.017
[15] Held L, Meyer S, Bracher J. Probabilistic forecasting in infectious disease epidemiology: the
13th Armitage lecture. Statistics in Medicine. 2017;36(22):3443–3460. doi:10.1002/sim.7363.
[16] Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ. Assessing the
performance of real-time epidemic forecasts: A case study of Ebola in the Western
Area region of Sierra Leone, 2014-15. PLOS Computational Biology. 2019;15(2):1–17.
doi:10.1371/journal.pcbi.1006785.
[17] Chowell G, Tariq A, Hyman JM. A novel sub-epidemic modeling framework for short-term
forecasting epidemic waves. BMC Medicine. 2019;17(164). doi:10.1186/s12916-019-1406-6.
[18] Chowell G, Rothenberg R, Roosa K, Tariq A, Hyman JM, Luo R. Sub-epidemic model
forecasts for COVID-19 pandemic spread in the USA and European hotspots, FebruaryMay 2020. Preprint, medRxiv. 2020. doi:10.1101/2020.07.03.20146159
[19] Laio F, Tamea S. Verification tools for probabilistic forecasts of continuous hydrological variables. Hydrology and Earth System Sciences Discussions. 2007;11(4):1267–1277.
doi:10.5194/hess-11-1267-2007
[20] Gneiting T, Ranjan R. Comparing density forecasts using threshold- and quantileweighted scoring rules. Journal of Business and Economic Statistics. 2011;29(3):411–422.
doi:10.1198/jbes.2010.08110
[21] Hong T, Pinson P, Fan S, Zareipour H, Troccoli A, Hyndman RJ. Probabilistic energy forecasting: Global Energy Forecasting Competition 2014 and beyond. International Journal
of Forecasting. 2016;32(3):896–913. doi:10.1016/j.ijforecast.2016.02.001.
[22] Diebold FX, Mariano RS. Comparing predictive accuracy. Journal of Business & Economic
Statistics. 1995;13(3):253–263. doi:10.1080/07350015.1995.10524599.
[23] Pinson P, Girard R. Evaluating the quality of scenarios of short-term wind power generation.
Applied Energy. 2012;96:12–20. doi:10.1016/j.apenergy.2011.11.004.
[24] Golestaneh F, Gooi HB, Pinson P. Generation and evaluation of space-time trajectories of
photovoltaic power. Applied Energy. 2016;176:80–91. doi:10.1016/j.apenergy.2016.05.025.
[25] Gneiting T, Stanberry LI, Grimit EP, Held L, Johnson NA. Assessing probabilistic forecasts
of multivariate quantities, with an application to ensemble predictions of surface winds.
With discussion. Test. 2008;17:211–264. doi:10.1007/s11749-008-0114-x
[26] Juul JL, Græsbøll K, Christiansen LE, Lehmann S. Fixed-time descriptive statistics underestimate extremes of epidemic curve ensembles. Nature Physics. 2021; forthcoming.
https://doi.org/10.1038/s41567-020-01121-y
[27] Winkler RL. Scoring rules and the evaluation of probabilities (with discussion). Test.
1996;4:1–60. doi:10.1007/BF02562681
[28] Bentzien S, Friederichs P. Decomposition and graphical portrayal of the quantile
score. Quarterly Journal of the Royal Meteorological Society. 2014;140(683):1924–1934.
doi:10.1002/qj.2284.
16

[29] Ray EL, Sakrejda K, Lauer SA, Johansson MA, Reich NG. Infectious disease prediction
with kernel conditional density estimation. Statistics in Medicine. 2017;36(30):4908–4929.
doi:10.1002/sim.7488.
[30]

FluSight Network. Repository cdc-flusight-ensemble. 2020. Accessible online at
https://github.com/FluSightNetwork/cdc-flusight-ensemble. Last accessed 13 October 2020.

[31] Dawid AP. Statistical theory: The prequential approach. With discussion. Journal of the
Royal Statistical Society: Series A. 1984;147:278–292. doi:10.2307/2981683
[32] Gneiting T. Making and evaluating point forecasts. Journal of the American Statistical
Association. 2011;106(494):746–762. doi:10.1198/jasa.2011.r10138.
[33] Dunsmore IR. A Bayesian approach to calibration. Journal of the Royal Statistical Society:
Series B (Methodological). 1968;30(2):396–405. doi:10.1111/j.2517-6161.1968.tb00740.x.
[34] Winkler RL. A decision-theoretic approach to interval estimation. Journal of the American
Statistical Association. 1972;67(337):187–191. doi:10.1080/01621459.1972.10481224.
[35] Makridakis S, Spiliotis E, Assimakopoulos V. The M4 Competition: 100,000 time series and 61 forecasting methods. International Journal of Forecasting. 2020;36(1):54–74.
doi:10.1016/j.ijforecast.2019.04.014.
[36] M Open Forecasting Center. The M5 Competition:
Competitors’ Guide.
2020.
Available at https://mofc.unic.ac.cy/wp-content/uploads/2020/03/
M5-Competitors-Guide-Final-10-March-2020.docx (downloaded 12 June 2020).
[37] Askanazi R, Diebold FX, Schorfheide F, Shin M. On the comparison of interval forecasts.
Journal of Time Series Analysis. 2018;39(6):953–965. doi:10.1111/jtsa.12426
[38] Brehmer JR, Gneiting T. Scoring interval forecasts: Equal-tailed, shortest, and modal
interval. Bernoulli. 2021; forthcoming. Preprint available at https://arxiv.org/abs/
2007.05709.

17

