Multi-Modal End-User Programming of
Web-Based Virtual Assistant Skills
Giovanni Campagna Euirim Choi Monica S. Lam
Computer Science Department
Stanford University
{mfischer, gcampagna, euirim, lam}@cs.stanford.edu

arXiv:2008.13510v1 [cs.HC] 24 Aug 2020

Michael H. Fischer

Grandma’s Chocolate Cookies

REC

REC

Ingredients:
½ cup
1 cup

...

BUY

sugar
ﬂour

(a)

Giovanni’s Kitchen Table

(b)

Giovanni’s Kitchen Table

Program Executed
Successfully.

Spaghetti Carbonara in 30 minutes

Spaghetti Carbonara in 30 minutes

Ingredients:

Ingredients:
$ 9.30 Pancetta

4
¼ cup
1 lb
½ cup
a
1 tbsp
2 tbsp

4
¼ cup
1 lb
½ cup
a
1 tbsp
2 tbsp

Eggs
Pancetta
Fettuccine
Parmigiano
Reggiano
Black Pepper
Salt

(c)

(d)

$ 3.99

Eggs

$
2.99 Fettuccine
Eggs
Pancetta
N/A
Parmigiano
Fettuccine
a
Reggiano
Parmigiano
$
1.48 Black Pepper
Reggiano
$
1.09 Salt
Black Pepper
Salt

(e)

Figure 1. A scenario describing the process to create a program by demonstration using VASH. (a) A user sees a cookie recipe on a popular food blog
and wants to see how much the ingredients are. (b) He then enters VASH’s recording mode using his voice and searches for one of the ingredients on
Walmart’s website. (c) He clicks on the first search result and highlights the price, telling VASH via voice that it should be returned. (d) A few weeks
later, he is interested in the “Spaghetti Carbonara" recipe on another food blog. He highlights the ingredients and asks VASH to run the previously
defined program with them. (e) VASH returns to the prices of the items, but also knows to notify him that one of the ingredients is not available on
Walmart.com.

ABSTRACT

While Alexa can perform over 100,000 skills on paper, its
capability covers only a fraction of what is possible on the
web. To reach the full potential of an assistant, it is desirable
that individuals can create skills to automate their personal
web browsing routines. Many seemingly simple routines, however, such as monitoring COVID-19 stats for their hometown,
detecting changes in their child’s grades online, or sending
personally-addressed messages to a group, cannot be automated without conventional programming concepts such as
conditional and iterative evaluation. This paper presents VASH
(Voice Assistant Scripting Helper), a new system that empowers users to create useful web-based virtual assistant skills
without learning a formal programming language.
With VASH, the user demonstrates their task of interest in the
browser and issues a few voice commands, such as naming the
skills and adding conditions on the action. VASH turns these
multi-modal specifications into skills that can be invoked in
voice on a virtual assistant. These skills are represented in a
formal programming language we designed called WebTalk,
which supports parameterization, function invocation, conditionals, and iterative execution.
VASH is a fully working prototype that works on the Chrome
browser on real-world websites. Our user study shows that
users have many web routines they wish to automate, 81% of
which can be expressed using VASH. We found that VASH

is easy to learn, and that a majority of the users in our study
want to use our system.
INTRODUCTION

Virtual assistants provide consumers with a new interaction
paradigm: instead of interacting graphically with a web
browser, users can now issue a wide variety of voice commands, from turning on their lights to controlling their music.
Today, Alexa has an open skill platform, with over 100,000
service providers having implemented skills by defining how
voice commands map to invocations of their APIs [13]. While
having 100,000 skills is impressive, its capability pales in
comparison to what can be done on the web across over 1.8B
websites. Furthermore, skills require the existence of APIs that
smaller websites often don’t have. Could we make a virtual
assistant that can work beyond APIs and operate across the
web? This paper explores how end users, even those without
programming experience, can leverage the web to construct
new virtual assistant skills for personal use.
Figure 1 shows an example of a personalized skill that a typical
user may want to build. The skill takes a recipe, finds the prices
of all the ingredients, and shows their sum to user. It combines
two different websites, making it unlikely a dedicated API
combining both exists. It involves performing operations such
as iteration and aggregation, which current virtual assistants
are unable to do. Yet, it is something that the user is already
capable of performing by hand on the web. Our need-finding

survey shows that there are many such tasks that users are
performing repeatedly on the web that they would like to
automate.
Our system, VASH (Virtual Assistant Scripting Helper), allows
the user to automate web-based tasks by converting them into
personalized virtual assistant skills. VASH features a novel
multi-modal interface combining Programming By Demonstration (PBD) [8] and voice commands. In PBD, the interaction of the user with the web page is recorded and converted
into executable operations. The PBD operations and the voice
commands are converted to a formal WebTalk language, which
defines the capabilities of the system, and provides compositionality to arbitrary programs. WebTalk includes high-level
programming concepts, such as function definitions, parameter
passing, iteration, and filtering.

web pages. A purely graphical approach using programming
by demonstration (PBD) [15] is also inadequate because it is
hard to express the concepts of control constructs. Recently,
PixelTone [14] proposed adding voice to PBD, but their work
cannot handle programming constructs.
Would end-users be able to program these web-based skills,
which may require sophisticated control constructs? We propose VASH, a PBD system with a multi-modal interface that
encompasses a browser extension and a conversation voice
interface that supports control structures.

There are a number of challenges in building a system that
supports end-user programming on the web in a manner that
is both useful and usable.

Our system simplifies programming with control constructs,
using input and output modalities that are well suited for each
construct. For example, voice and natural language are well
suited for specifying control flow. Keyboards are well suited
for quickly allowing the user to specify text. The mouse is
easy to use for specifying clicks and actions. Finally, audio
from the computer is well suited for providing feedback from
the system.

Expressiveness: control constructs

Contributions

Many of the tasks that users want require performing actions
across domains, such as finding the best rated restaurant on
Yelp, looking up reservations on Opentable, or checking their
bank balance prior to making a payment. There are many
possible combinations of functions, and every user has their
own routine they wish to automate.

Our paper makes the following contributions:

Almond is a virtual assistant that lets users perform compound
commands that involve multiple skills. It proposes using a formal virtual assistant programming language, called ThingTalk,
as the target language for natural language translation via a
neural semantic parser. ThingTalk is designed to be translatable from natural language; as such, it has a very simple
control construct. A ThingTalk program can connect together
at most an event, data retrieval operations and one action, along
with filters on the parameter and result values. Each of the
event, data retrieval, and action are pre-defined primitive functions, which involves calling remote APIs using JavaScript.
This language is not expressive enough for the kind of tasks
we wish to automate.
Given that we need to introduce concepts of control constructs
in user-defined tasks, it is even more important that we use a
formal programming language with well-defined semantics as
a target language. Thus we ask the question: How do we improve the expressiveness of the virtual assistant programming
language to cover the tasks we wish to automate?
We propose WebTalk, as an extension to ThingTalk to (1)
include user-defined function definition and invocation, iterations, and conditional execution, and (2) defining functions
as web page operations. Even though such control flow is
common in all programming languages, the unique consideration here is whether it is a good target for translation from an
end-user’s specification.
End-user programming: Multimodal programming

To let individuals automate their personal tasks, they must be
able to specify and execute them. A purely voice-based approach is infeasible because it is hard to describe operations on

1. We conduct a need-finding survey and show that there is
an interest and need for end-users to automate their digital
tasks on the web. Many such tasks require specification
of control structures iterations, conditionals, triggers, and
function composition, many of which cannot be achieved
using today’s virtual assistants or PBD systems.
2. We have developed the first multi-modal interface that allows end-users to specify web-based virtual assistant skills—
which support the use of control structures—by demonstration and voice. This interface supports 81% of the tasks we
collected in our need-finding user survey.
3. To formalize the capability of the PBD system, we have
developed WebTalk, the first virtual assistant programming
language that is sufficiently expressive to support operations
with control structures on web pages. The language is highlevel and can be used as a translation target from the multimodal end-user specification.
4. We show our design is easy to learn and use through a
user study involving five tasks in a controlled environment,
where users are exposed to increasingly more complex
tasks.
5. We developed a full end-to-end prototype of the VASH
design, and in a user-test with four real-world scenarios we
find 80% of the users would find our prototype useful, and
53% agree they would use it.
RELATED WORK
In-Browser Virtual Assistants

Several parties have explored the possibility of integrating
virtual assistants, often with voice interfaces, into the browser.
Hey Scout, a browser-based voice assistant [22] that enabled
users to do simple web browsing tasks via a natural language
interface later paved the way for Mozilla’s public release for
Firefox of a similar system [18]. Unlike our system, however,
neither of these assistants interact with web page content.

Web Automation

Other works have focused on the popular problem of web
automation via programming by demonstration, enabling nonprogrammers to automate complex online tasks. Rousillon [7],
focused on the narrow task of web scraping automation via
PBD, assessing system usability in a user study with computer scientists. Ringer [2] allowed users to automate a more
general set of online tasks including filling forms, but did
not enable program parametrization without delving back into
programming.
Multi-modal Interfaces

With early PBD systems often being difficult to use, some systems have focused on improving usability using multi-modal
interfaces. Using computer vision and language, VASTA [21]
enables program automation across operating systems or platforms by intelligently recognizing interactive elements on
screens. SUGILITE [15] and APPINITE [16] provide users
multi-modal interfaces in the form of primitive dialogue systems and visual interfaces to facilitate smartphone automation.
Brassau [9] automatically generates graphical user interfaces
based on a natural language command. These systems, however, did not provide general support for the program control
structure supported by our system.
Dialogue Systems For Automation

Other systems have attempted to end-user web automation
primarily using natural language. Almond [5], one of the most
prominent of such systems, is an open-source virtual assistant
that lets users issue compound natural language commands
that compose of two of the three clauses: "when" a condition
happens, "get" some data, and "do" some action. Natural
language sentences are translated into ThingTalk programs,
which consist of a single statement that connect together up
to two skills. Each skill maps to some API in the Thingpedia
repository, and has to be created by programmers, limiting
the scope considerably. ThingTalk also does not support highlevel programming concepts such as function definitions.

However, when we show a computation by demonstration,
we only work with concrete values, or actual values, which
result in concrete results for those inputs. On the other hand, a
function needs to be parameterized, with the results dependent
on the input values. For example, suppose we wish to find
the best Italian restaurant on a web page. As users, we would
find the first Italian restaurant on a list sorted by the ratings.
Just recording the click on a particular Italian restaurant fails
to capture the reason behind the action. It is impossible to
intuit how to generalize skills to new inputs without users’
help. VASH lets the users use voice to describe all additional
information to turn the demonstration into a program that
works across different inputs.
Unbeknownst to the users, underlying VASH is a formal programming language, called WebTalk, that we created for this
purpose. WebTalk is designed to be translated from multimodal inputs, with voice, the mouse, and the keyboard. The
language supports variables, function calls (without recursion), iterations, and conditional executions. Grounding the
PBD system with a formal language provides consistency and
compositionality. Our goal is to give users a clean, implicit
conceptual model so they can come to know what to expect,
instead of confronting them with a set of ad hoc features. This
allows them to become more proficient over time. While we
do not expect users to create long function bodies, they can
easily define functions that call other functions, allowing their
capability to compound.
VASH is a programming system that accepts a multi-modal
specification via PBD, translates it into WebTalk program, and
runs it. The specification of a skill has two modalities:
1. The web browsing actions using the mouse and keyboard;
these are translated into web primitives in WebTalk.
2. Voice commands to provide meta-information that translates
the demonstration into parameterized functions; these are
translated into constructs in WebTalk.
VASH SPECIFICATION AND WEBTALK

Introductory Programming Systems

It can be difficult for non-programmers to understand programming concepts, as the way in which non-programmers
solve challenges can often be different from their programmer
counterparts [11, 20]. In this work we build off previous work
in understanding the mental models of introductory programming education in information visualization for novices [4],
practical trigger-action programming [23], and end user uses
of spreadsheets [4].
OVERVIEW

VASH is a system that lets users create skills consisting of
operations involving web page(s) from one or more websites.
These skills are entered into the user’s personal skill repository,
which can be later invoked by voice.
VASH lets users program by demonstration (PBD). The user
can behave as they normally do without learning how to articulate what they wish to automate. This is particularly important
for GUI-based computations.

VASH and WebTalk are designed hand-in-hand to be easy
to use for consumers to automate their routines on the web,
trading away functionality where necessary.
The correspondence between the VASH specification and the
WebTalk language is shown in Tables 1 and 2. Note that
users do not have to use exactly the phrases listed in the VASH
specification, as VASH uses natural language understanding
to interpret the statements. Due to space, we will not detail
the WebTalk grammar, except to note a few less common
conventions, which are borrowed from ThingTalk, a virtual
assistant programming language [5]. Variables are tables of
values; a scalar is a degenerate table with one row. A statement
of the form:
<var-name>, <cond> ⇒ op
says “perform op for every value in the table <var-name> satisfying the condition <cond>”. The condition will name a
field in the table. Tables in WebTalk correspond to HTML
elements, and have fields text (the element’s textual content)

VASH Web Primitives
Open page (url)
Click (element)
Type (element, value)
Select (element)
Paste (element)

WebTalk Web Primitives
@load(url)
@click(selector)
@set_input(selector, value)
let this := @select(selector)
@set_input(selector, value = this)

Description
Navigate the browser to the given url.
Click on the elements matching the CSS selector.
Set the input elements matching the CSS selector to the given value.
Read the text in each element matching the CSS selector.
Set the input elements matching the CSS selector to the content of the current selection,
or the implicit input parameter of the current function.

Table 1. Web primitives that the user can perform in VASH, and the corresponding WebTalk statements. “CSS selector” refers to the selector derived
from the element used during demonstration.
VASH Constructs
“Start recording <func-name>”
“Stop recording”

WebTalk Constructs
function <func-name>(){
}

“Run <func-name>”
“Run <func-name> with this”

<func-name>()
this ⇒ <func-name>(this.text)

“Return this value”

this ⇒ notify

“Run <func-name> if <cond>”

this, <cond> ⇒ <func-name>()

“Run <func-name> with this
if <cond>”
“Run <func-name> at <time>”
“Run <func-name> with this at
<time>”
“Return this value if <cond>”

this, <cond> ⇒ <func-name>(this.text)

“Calculate the <agg-op> of this”

let <agg-op> := aggregate <agg-op> number of this

“This is a <var-name>”
(when an input box is focused)
“This is a <var-name>”
(when some element is selected)
“Return the <var-name>”

@set_input(selector, value = <var-name>)

timer(<time>) ⇒ <func-name>()
timer(<time>) ⇒ this ⇒ <func-name>(this.text)
this, <cond> ⇒ notify

let <var-name> := @select(selector)
<var-name> ⇒ notify

“Run <func-name> with
<var-name>”
“Start selection”

<var-name> ⇒ <func-name>(this.text)

“Stop selection”

n/a

n/a

Description
Begin recording a new function, with the given name.
Complete the current function definition and save it for later
invocation.
Execute a previously-defined function.
Iterate over all elements of the current selection, and execute
the function with each element as the input parameter.
Use the current selection as the return value of the current
function.
Execute a previously-defined function if the condition on the
current selection is satisfied.
Iterate over all elements of the current selection, and execute
the function with each element that satisfies the condition.
Execute the function every day at the given time.
At the given time, iterate over all elements of the current selection, and execute the function with each element.
Use the current selection as the return value of the current
function, if the condition is satisfied.
Compute the given aggregation operator based on the numeric
values in the currently selected elements, and save it as a variable.
Create a new function parameter with the given name, and use
it to set the input elements matching the selector.
Create a new named variable and set the elements currently
selected to it.
Use the given named variable as the return value of the current
function.
Iterate over all elements of the given named variable, and execute the function with each element as parameter.
Enter selection mode, allowing the user to select multiple noncontiguous elements.
Exit selection mode.

Table 2. Constructs that VASH understands, and corresponding WebTalk statements. The user issues each construct as a voice utterance. The table
includes only the canonical form of each utterance.

and number (the element’s numeric value, if the element contains a number). The keyword notify returns the result; if the
command is invoked by the user directly, the result is shown
to the user in a popup.
Basic Function Definitions

We first describe a use case as a running example to illustrate
the basic features of the VASH system. Ann is reading a story
about Apple, when she realizes she keeps looking up stock
quotes for companies in stories she reads. Using VASH, she
creates a skill for herself to return the quote of a stock, as
shown in Table 3. The first column details what Ann does,
classified into GUI actions which generate web primitives
in WebTalk, and voice commands which generate WebTalk
constructs. As shown by the sequence of web primitives in this
example, she looks up the stock quote on finance.yahoo.com as
she normally would. She adds some voice commands to turn
the actions into a function, which she invokes with possibly a
different stock symbol afterwards.

Web primitives

Let us first discuss the GUI actions. The function we define
needs to take the user’s demonstration with a concrete input
value and produce a parameterized version that works with
new inputs. First, we must record all keyboard input in forms,
mouse clicks on buttons and links, as well as select, copy and
paste. We do not need to record operations such as scrolling
or moving the mouse, as those operations only affect the view
of the users. Drawing with the mouse, which involves a click
and a drag, is not currently supported.
Second, we need to make the web primitives relative to the
new input parameters. For example, when Ann selects the
stock price, we need to capture the HTML element selected,
and not the current value of AAPL, because we wish to run
this function on a different stock symbol and at a different
time. We represent the HTML element of interest with a CSS
(Cascading Style Sheets) selector [25]. CSS selectors are
a language for describing a subset of HTML elements in a
page, originally designed for styling. CSS selectors use se-

VASH Specification

WebTalk Code

Web primitive:
Construct:
Web primitive:
Web primitive:
Web primitive:
Web primitive:
Construct:
Construct:

Select and copy “AAPL”
“Start recording stocks”
Open finance.yahoo.com
Paste in the search box
Click Search button
Select current price
“Return this value”
“Stop recording”

let this := @select(selector = “a.company:nth-child(3)”);
function stocks(param : String) {
@load(url = “https://finance.yahoo.com”);
@set_input(selector = “input#search”, value = param);
@click(selector = “button[type=submit]”);
let this := @select(selector = “span#today-quote”);
this ⇒ notify;
}

Web primitive:
Construct:

Select a stock symbol
“Run stock with this”

let this := @select(selector = “span.symbol:nth-child(1)”);
this ⇒ stock(this.text);

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)

Table 3. A Basic Example: Get stock quote. The user performs the actions in the left column, and the corresponding WebTalk program in the right
column is generated. The value of the selector parameter is a CSS selector that refers to a specific HTML element, and is generated from the element
selected, clicked or written into. CSS selectors are simplified in the example for illustration purposes.

mantic information to identify the elements (HTML tag name,
author-specified ID and class on each element), positional and
structural information (ordinal position in document order,
parent-child relationship), and content information.
When recording the action, VASH records which element the
user is interacting with, and generates a CSS selector that
identifies that element uniquely. When available, VASH uses
ID and class information to construct the selector, falling back
to positional selectors when those identifiers are insufficient
to uniquely identify the element. As such, the CSS selectors
VASH generate are robust to changes in the content of the
page and small changes in layout. VASH selectors work best
with static HTML pages where IDs and classes are assigned
according to the semantic meaning of each element.
VASH records all selection operations, such as when Ann
selects and then copies the stock name at the beginning of
the recording (Table 3 line 1). The selection is converted to a
“select” web primitive (Table 1), which refers to the element
that was selected, rather than recording the specific values.
The result of the “select” web primitive is bound to the special
“this” variable, which then can be used in the subsequent web
primitives (Table 3 line 1, lines 6 and 7, and lines 9 and
10). “this” is a special variable that is implicitly defined and
accessed while recording. During execution, it contains the
value of the elements from the page at execution time. The
“this” variable is a list of HTML elements, and it records the
text content and the number value.
Paste operations are mapped to a “set_input” web primitive
(Table 1). If the paste refers to something that was copied in
the same function, the value is set to the text of the currently
selected element (the current “this” variable). If instead, the
paste operation in the input box is paired to a copy operation
outside the function, the value is set to the implicit function
parameter “param” (Table 3 line 4). Finally, if the copied
content comes from outside the browser, the paste operation
generates a “set_input” web primitive having value set to the
constant value being pasted, as if the user typed it explicitly
(Table 1). This design avoids additional copy-paste web primitives while maintaining both the natural flow of copy-pasting
during recording, and the ability of the make the execution
depend on the actual page it is executed on.

Function Definitions

With PBD, functions are defined as they are executed on a
specific information. In this example, Ann demonstrates the
function with the input parameter by copying “AAPL”. The
function is defined by wrapping the actions with a “Start
recording <func_name>” and a “Stop recording”. The parameter “param” is automatically defined if a copied value is
used in the function. The user can refer to the “this” variable
in their voice command, as shown in the “Return this value”
or “Run stock with this” commands.
The user can also say “Run stock with this at 10am every day”.
The assistant runs the skill in the background automatically,
then notifies the users of the results. This functionality is
identical to that of the existing Almond assistant [5].
At most one return statement can appear in the function, but
the return statement need not be the last. It can be followed
by additional web primitives, which do not affect the return
value. This allows the function to perform “clean up” actions,
such as logging out, before returning the result.
Advanced Constructs

To support what users want to automate on the web, VASH supports function composition, iteration, conditional execution,
and aggregation. Due to limited space, we will just overview
these features by way of a more interesting use case. Suppose
Bob is taking up baking, as many are doing these days. He
finds a new recipe website with many interesting recipes, but
he wishes to keep the cost of each experiment below a budget.
Table 4 shows how he can create a recipe skill that looks up
a recipe, looks up each ingredient in that recipe in Walmart,
then sums up the price.
Function Definitions in Functions

As in the previous example, Bob starts recording the “recipe”
function, pastes the recipe name in the search box, and clicks
Search (lines 2 to 4). Now Bob sees the full list of ingredients.
He needs to compute the price of each ingredient, checking
from his favorite grocery chain.
Bob can check the price with a “price” function, but he has
not defined one yet. Fortunately, VASH lets users define another function inside a function, because the user would not
know what helper functions are needed ahead of time, and
the user would not have the context to use PBD to define

VASH Specification

WebTalk Code

Construct:
Web primitive:
Web primitive:
Web primitive:
Web primitive:
Web primitive:
Construct:
Web primitive:
Web primitive:
Web primitive:
Web primitive:
Construct:
Construct:
Web primitive:
Construct:
Construct:
Construct:
Construct:

function recipe(param : String) {
@load(url = “https://allrecipes.com”);
@set_input(selector = “input#search”, value = param);
@click(selector = “button[type=submit]”);
@click(selector = “.recipe:nth-child(1)”);
let this := @select(selector = “.ingredient:nth-child(1)”);
function price(param : String) {
@load(url = “https://walmart.com”);
@set_input(selector = “input#search”, value = param);
@click(selector = “button[type=submit]”);
let this := @select(selector = “.result:nth-child(1) .price”);
this ⇒ notify;
}
let this := @select(selector = “.ingredient”);
let this := this ⇒ price(this.text);
let sum := aggregate sum number of this;
sum ⇒ notify;
}

“Start recording recipe”
Open allrecipes.com
Paste in search box
Click Search button
Click the first result
Copy the first ingredient
“Start recording price”
Open walmart.com
Paste in search box
Click Search button
Select price of top result
“Return this value”
“Stop recording”
Select all ingredients
“Run price with this”
“Calculate the sum of this”
“Return the sum”
“Stop recording”

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
(16)
(17)
(18)

Table 4. Sum The Price of Recipe Ingredients. The user performs the actions in left column, and the corresponding WebTalk program in the right
column is generated. CSS selectors are simplified in the example for illustration purposes.

the function. For simplicity, all functions have global scope,
meaning that they can be invoked anywhere, and they cannot
use any variables from the lexical scope of the function definition. To define the “price” function, Bob copies the first
ingredient, then starts recording the new function. In the new
function, Bob pastes the ingredient name (copied from the
recipe), searches the product, selects the price, returns it, and
completes the “price” function (lines 7 to 13).

Function Call, Iteration and Conditional

With the “price” function in hand, Bob can now apply it to
all ingredients. He goes back to the recipe page, and selects
all the ingredients in the list, then says “run price with this”
(line 15). Because he has selected multiple elements, the
selection is iterated and the “price” function is called on each
element, returning a list of prices. Bob is shown the list of
prices computed immediately.
This example illustrates an important complication in PBD
systems. We need to execute the functions called in a function
definition because the user is always operating with concrete
values. The results need to be returned to allow the user
continue with the demonstration. If the called function returns
a result, the result is displayed as a popup on top of the current
page.
Iteration can also be made conditional, by adding a predicate
(Table 2, line 3). In this case, the function is only applied to
the list elements that satisfy the predicate. For example, “call
alert with this if this is greater than 98.6” can be expressed as:
this, number > 98.6 ⇒ alert(param = this.text);
number is a field of the currently selected HTML elements
(in the “this” variable) and it is computed by extracting any
numeric value in the elements. Our current system only supports a single predicate, which can use equality, inequality, or
comparison between the current selection and a constant. As
the natural language technology improves, we expect in the
future to support arbitrary logical operators (and, or, not).

Aggregation

Having obtained the list of prices, Bob now needs to compute
the sum (line 16). WebTalk supports all the aggregation operations supported by ThingTalk with database extensions [24].
Once a set of numbers is selected in either a table or list,
the sum, count, average, max, and min can be computed and
returned, by issuing the voice command “calculate the <operation> on this”.
The result of the aggregation is immediately displayed to Bob,
so he can decide what to do with it. He can now say “return
the sum”, to use the aggregation result as the result of the
current function (line 17). He could also have used the “sum”
in a subsequent conditional command.
Named Variables

In addition to the “this” variable that is implicitly defined and
used with selection and copy-paste operations, users can also
create named variables. This allows the user to manipulate
multiple selections at once. This is necessary when, for example, the defined function needs more than one parameter.
Named variables can be used in place of “this” in conditional,
function invocation and return statements.
Design Rationale Discussion

VASH is unconventional when compared to typical programming languages because it is intended for consumers, and it
is used in a PBD setting. Most functions can be defined without users naming variables; a single “this” variable is used in
the WebTalk program, which is derived from the GUI actions
recorded while the user demonstrates the program. We allow
the specifications of functions to be nested, even though all
functions have the same global scope. This way, the user can
define helper functions as they discover the need for them
during demonstration, and use them immediately in a function
definition. These two features reduce the mental load on the
user, as confirmed by our user study described later.
Nested function specification is especially important because
WebTalk does not have scoping like most other languages,
which are usually specified with “begin” and “end” or “{” and

“}”. Scoping is hard when the programming is not visual.
Iterators and conditional statements can only be applied to
functions. Iterators are expressed as applying functions to
elements of a collection, rather than “do” or “for” statements,
which are more powerful but harder for non-programmers in a
PBD setting. VASH conditionals do not have an “else” clause.
In PBD where users are operating with concrete values, the
users can only perform actions that follow from conditions
the concrete values satisfy. In the future, we can add “else”
clauses by letting sophisticated users refine a defined function
with additional demonstrations using alternate concrete values.
THE VASH SYSTEM
“Call stocks on
this.”

AAPL
GUI Abstractor

Speech to Text

Event Type:
@select
CSS Selector:
a.company:nth-child(3)

Semantic Parser
Meaning: Run stocks with this.

Code Generator
let this := @select(selector=“a.company:nth-child(3)”);
this ⇒ stock(this.text);

WEBTALK RUNTIME

WebTalk JIT Compiler
Browser Automation API

$297.56

Automated Browser
Figure 2. A high-level overview of the algorithm that VASH uses to convert a multi-modal specification to WebTalk.

The VASH system accepts a user’s demonstration on the
browser annotated with a small number of voice commands
and translates the specification into WebTalk. As discussed
above, to support function calls in a function definition, the
translator also includes a run-time system to execute WebTalk
programs. The high-level system architecture of VASH is
shown in Fig 2.
GUI Abstraction

The first step in converting the specification is to abstract all
GUI operations that the user performs into WebTalk web primitives. To record all actions that the user performs on a page,
VASH uses a browser extension, which injects JavaScript on
each page where VASH is enabled. VASH clearly displays to
the user a prominent indicator when it is recording the user’s
actions.
When the user starts recording a function, VASH must first
record the current context in which the function is recorded.
VASH records the current URL, and generates the corresponding @load web primitive.
VASH’s injected JavaScript code listens to all interaction
events (keyboard, mouse, clipboard) from the browser on all
parts of the page. When an event is intercepted, the injected
JavaScript code considers the HTML element that is the target

of the event, and constructs a new CSS selector that identifies
that element uniquely. The selector is constructed based on
the ID, tag name and class name of the element. The event is
then converted to the corresponding web primitive.
To generate @set_input, VASH records keyboard events in
input elements. It uses the value of the input element as the
value of the web primitive. Multiple consecutive keyboard
events are collapsed into a single web primitive: this allows
the system to replace the web primitive that contains a constant
value with one that refers to a parameter when the user names
that parameter.
Natural Language Commands

VASH continuously listens for the user’s voice, and reacts to
the commands that map to WebTalk control constructs (Table 2). Each utterance is passed to a speech-to-text converter,
followed by a semantic parser that translates it into a canonical
VASH command. VASH responds to the user’s prompt by
executing the command, and then replies to the user in voice.
Commands that use “this” are interpreted according to the
command issued immediately before, and the current selection
on the page. If the user just issued a “run” command that
returns a result, “this” refers to the result of the called function.
Else, if the currently focused element is an input element, “this”
refers to the content of the input element, which is either a
constant (what the user just typed) or a function parameter (that
the user pasted or named explicitly). In that case, VASH uses
the recorded value of the input. Otherwise, VASH generates a
@select web primitive, using a CSS selector that maps to the
currently selected elements. In the last case, “this” in natural
language refers to the “this” variable in WebTalk.
To allow the user to select complex lists of elements, and elements in tables or in pages with complex layouts, in addition
to the plain browser selection, VASH also supports an explicit
selection mode. The user enters selection mode with the voice
command “start selection”. While in selection mode, the page
is not interactive: instead, clicks add or remove the clicked
elements to the current selection. Selection mode is exited
with “stop selection”. Selection mode is treated equivalently to
a native browser selection operation, and generates a WebTalk
@select primitive.
The Code Generator

Given the stream of web primitives produced by the GUI abstractor, and the natural language commands, there is a straightforward translation to the corresponding WebTalk code.
One exception is the “run” command: to allow the user to
demonstrate a function call, when the user issues a “run” command while recording a function, VASH must execute it as
in the context of the page being recorded. To do so, VASH
constructs a WebTalk program containing all the statements
recorded so far, and the function call statement. In the generated program, the parameters of the current function are
replaced with the concrete values they had when the corresponding web primitives were recorded. When executed, the
statements in the current functions will reestablish the context
of the page so the “run” command can be executed faithfully.

The result is then shown to the user in a popup. If the user issues multiple “run” commands in the same function, each time
the whole current function will be executed from the beginning. This allows a clean separation of code generation from
the WebTalk runtime, at the expense of marginal inefficiency.
To support nested function definitions, VASH maintains the
full state of the code generator in a stack. When the user starts
recording a new function, a new state is pushed on the stack,
and it is popped when the recording is complete.

Figure 3. Programming experience of survey participants that proposed
skills for VASH.

WebTalk Run-Time

The WebTalk code is first compiled to native JavaScript code,
using a WebTalk compiler. The compiler is based on the
ThingTalk compiler [5], extended to support nested functions.
The JavaScript code is run on an automated browser: a form
of browser that is driven with an automated API rather than
interactively by the user. Each WebTalk web primitive is
converted to a call to the automation API. Each WebTalk
function call is mapped to a new automation session (roughly
corresponding to a new browser tab); this ensures that function
calls do not interfere with the state of their parent. Other
WebTalk constructs (iteration, conditionals, aggregation) are
implemented in the generated JavaScript code.
IMPLEMENTATION

We implemented a fully functional end-to-end prototype for
VASH, written in JavaScript. The implementation is split in
a Google Chrome browser extension that injects the VASH
recording code in every page the user visits, and a standalone
Nodejs application containing the WebTalk execution code.
The standalone application is based on Almond [5], and is able
to spawn the automated browser and communicate with it.
To handle the user’s speech, we use the Web Speech API, a native speech-to-text and text-to-speech API available in Google
Chrome. We use the annyang library [1] to perform natural
language understanding of the user’s commands. This library
uses a template-based NLU algorithm, requiring the user to
speak exactly the supported words. At the same time, it supports open-domain understanding of arbitrary words, which is
necessary to allow the user to define their own function names.
Open-domain understanding is not reliable in currently existing neural NLU solutions, but we expect that future work will
integrate a more flexible NLU model.
CSS Selectors are generated using the finder library [17].
Event recording code is based off the Puppeteer Recorder
Chrome Extension [19], and event replaying uses the Puppeteer API [10] to automatically control Google Chrome.

Figure 4. Occupation of survey participants that proposed skills.

evaluate specific design choices in VASH. (4) We collect user
feedback on VASH in user-suggested scenarios on real-world
websites.
What Do Users Need To Automate?

Our first study is a need-finding on-line survey to find out what
users are interested in automating and whether the primitives
in VASH are adequate. We recruit 37 participants on Amazon
Mechanical Turk (25 men and 12 women, average age = 34),
each of whom was paid $12 for approximately 60 minutes
of their time. Survey participants had a mix of programming
experience (Fig. 3) and were from a variety of backgrounds
(Fig. 4).
In the survey, respondents were first shown the functionality
of the system, then asked to describe 3 skills each that they
would like to be automated. We collected 71 valid skills.
The proposed skills span 30 different domains, with the most
popular being food, stocks, local utilities, and bills (Figure 5).
Representative tasks are shown in Table 5. Of these 71 skills,
we found 24% do not require any programming construct, 28%
need iteration, 24% need conditional statements, 24% need
a trigger (a timer plus a condition). In summary, 76% of the
skills people want to automate require the control constructs
we introduce to PBD.
99% of the skills are intended for the web and 1% are to be
run on the local computer. 34% of skills are on websites that
need authentication, showing that users are interested in skills
that operate on their personal data. We found 81% of the web
skills can be expressed using VASH. For the remaining 19%,

It is not possible using browser APIs to distinguish between
navigation explicitly caused by the user, and navigation triggered by clicking a link or submitting a form. VASH only
allows one explicit navigation event per function currently.
EXPERIMENTATION

To evaluate our system, we performed four experiments: (1)
We conduct a need-finding survey to learn what types of web
flows users would like to automated. (2) We evaluate whether
users can learn the VASH specification constructs. (3) We

Figure 5. Domains of skills users were interested in creating using VASH.

Strongly disagree

Disagree

Neutral

Agree

Strongly Agree

60%
40%
20%
0%

Exp. A

Exp. B

Easy to learn

Exp. A

Exp. B

Easy to use

Exp. A

Exp. B

Exp. A

PBD useful

Exp. B

Exp. A

MMI useful

Exp. B

VASH useful

Exp. A

Exp. B

Satisfied

Exp. A

Exp. B

Would use

Figure 6. Results of our user studies. “Exp. A” refers to the construct learning study, while “Exp. B” refers to the real world evaluation study.

Example Skill

Domain

Construct

Task

“Make a reservation for the highest rated restaurants in my area.”
“Order a ticket online if it goes under a certain
price.”
“Check my investment accounts every morning
and get a condensed report of which stocks went
up and which went down.”
“Order ingredients online for a recipe I want to
make, but only the ingredients I need.”
“Automate queries I do by hand every day for work
for inventory levels and delivery times.”
“Send birthday text message to people automatically.”
“Alert me when someone moves on the camera of
my home security system.”

Communication

Basic
Iteration
Conditional
Timer
Filter

Automated the clicking of a button.
Send an email to a list of email addresses.
Reserve a restaurant conditioned on rating.
Buy a stock at a certain time.
Show restaurants above a certain rating.

Communication
Stocks

Food
Database
Communication
Unsupported

Table 5. Representative tasks that users wanted to automate.

11% require producing charts, and 8% require understanding
videos and images. These functionalities are orthogonal and
can be added to the system as pre-defined skills. This shows
that despite the simplicity of VASH, it covers what people
want to automate.
Can Users Learn To Program In VASH?

Our next study addresses the question of whether users can
effectively learn the programming constructs employed by
VASH. We conducted a remote user study, using the same
participants as the need-finding survey. In the study, each
participant was asked to perform a set of five tasks, with each
task designed to be a realistic test of an element of the system’s
control structure. The tasks were performed in the order of
increasing complexity, to emulate the learning experience
on the system. The tasks were unsupervised and on demo
websites that we had created for the purpose of testing the
design of the system. For each task, users were asked to watch
a video that demonstrated how the control structure construct
worked so that they could understand the functionality of the
system. After watching the video, they repeated the task.
Lastly, they were asked to do a different task that requires the
same construct. The five tasks they performed on their own
are shown in Table 6. Note that because the “Iteration” task
requires two parameters, the recipient name and their email
address, the users have to name the parameters explicitly,
instead of relying on the copied data as the implicit parameter.
Quantitative Results

Participants were able to successfully complete the tasks in
automating routines using VASH 94% of the time. After the
tasks, users were asked to complete a survey, rating a number

Table 6. Tasks performed by the participants in the programming construct study.

of questions on a 5-point Likert scale from “strong disagre” to
“strongly agree”. The results are shown in Figure 6 as “Exp.
A”. We notice that users consistently found the system easy to
learn (72%), and easy to use (75%). Both the PBD interface,
and the multi-modal interface (“MMI” in the plot) are rated
helpful by 81% of survey participants. Overall, 66% of the
users agree that VASH is useful, and are 91% are satisfied with
the experience of testing it. Furthermore, 55% of the users
say they would use VASH to automate their personal skills.
These results confirm the need and usefulness of VASH, and
suggest that the programming constructs employed by VASH
can be learned. Note that in these experiments, users are only
exposed to simple tasks on a custom website, rather than real
world scenarios, which explains the low propensity to find
VASH useful.
Privacy

When automating a task that involves personal identifiable
information, 83% of user wanted a privacy preserving system
that ran locally and 33% did not. When automating any task,
even ones that did not involve personal identifiable information, 66% of users still wanted a privacy preserving system. As
our system is able to run on the client as a Chrome extension
and native application, we are able to offer users privacy.
Evaluating Design Decisions

Here we evaluate some of the decisions we made in the design
of the VASH specification, with a user study with 14 users
(7 men and 7 women, average age 25) conducted over video
conferencing.
Implicit Variables

Instead of requiring users to define all their variables, VASH
introduces the implicit “this” variable that the users can define
and use with select and paste actions. We ask the users to build
an example skill using both the explicit and implicit naming
methods. Overall, 88% preferred the implicit version, because
it had fewer steps and was faster. We found users didn’t like
talking to their computer as much.

an ingredient on a website to their clipboard. They then
go to walmart.com, paste the ingredient in the search bar,
click on the first item, then add it to their cart. They go back
to the cookie recipe and add the rest of the items to their
cart iteratively using the function created. This tests users’
understanding of creating skills using different websites and
defining a function within a function.

Nested Vs. Unnested
5.0
4.5
4.0

category
unnested
nested

score

3.5
3.0
2.5
2.0
1.5
1.0
mental

temporal

performance

effort

frustration

metric

Figure 7. NASA TLX results from questionnaire on having users specify
functions by nesting them vs. not.

Nesting Function Specification

VASH allows a function’s specification to be nested in another.
We created a test where users created a skill to buy ingredients
for a recipe. They need to create a “buy” function which is
then used on a “recipe” function which applies “buy” on a
list of ingredients. We asked them to do it by (1) using the
nested specification method, and (2) first defining the “buy”
function then the “recipe” function. Figure 7 shows the NASATLX [12] survey results from this study. Across all metrics,
the results point to users preferring the nested specification
method. (Note that a low number is better for all but the
performance question).
Real Scenarios Evaluation

In our final experiment, we want to get users’ feedback of
using the system in real life tasks drawn from our first user
study, using websites they are familiar with: Walmart, a recipe
website, a stock website, and a weather website. This endto-end test also demonstrates that VASH is a fully functional
system.
Each task involves the user defining a VASH skill, and then
invoking it to see the result. We evaluated the following realworld scenarios, which we chose based on the need-finding
experiment:
1. Calculate the average of temperatures. The user creates
a program that goes to weather.gov, enters their zip code,
calculates the average high temperature for the week, and returns that value. This example exercises the multi-selection
and aggregation function.
2. Add items to an online shopping cart. Here, the user has
a shopping list of items that they entered, and they need
to write a program that add them all to a shopping cart on
everlane.com. This scenario requires user input, copy and
paste, and iteration.
3. Notify when stock prices dip. The user creates a skill on
zacks.com to receive a notification when a stock quote goes
under a fixed price. The skill is then triggered each day at a
certain time. This tests the conditional and timer functions
of the system.
4. Add ingredients from a website to a shopping cart. This
task is similar to the task in Fig. 1. The user visits a cooking
website, acouplecooks.com, and uses their keyboard to copy

We conducted this study as an interactive user test (live over
video-conferencing) using the same participants as the design decision study described above. Users first complete a
warm-up task of recording a simple function to familiarize
with VASH. Then they are asked to complete each task manually and on VASH following a predefined script. Whether
each user completes the task manually or using VASH first is
randomized. After each task, we ask the user to complete the
NASA-TLX survey, comparing the system to performing the
task by hand.
All users were able to install VASH on their Chrome browser
and complete the tasks successfully. Fig. 8 shows the result
of the NASA-TLX survey, aggregated across all tasks. We
first observe that there is no statistically significant difference
across all five metrics between completing the tasks by hand
and completing it using VASH. This suggests that recording a
skill with VASH is no harder than performing the same task
by hand, while providing the immediate value of being able
to invoke the skill repeatedly. This is a promising result that
shows automation can be practical for end-users.
Users also complete a Likert-scale evaluation on the whole
system. This survey shows how users would perceive its value
with real-world scenarios. The results are shown in Fig. 6 as
“Exp. B”. 73% of the users find the system easy to learn, but
only 46% find it easy to use, probably due to the complexity of
the tested tasks. 47% of the users find PBD is useful, and 73%
find the multimodal interface useful. Overall, a large majority
of the users (80%) find VASH useful, and 67% are satisfied
with it after testing. Once again, a majority of users (53%)
agree they would personally use VASH.

Qualitative Feedback

During the user test, we also collected qualitative feedback
from the participants. A user that was not able to program
before said, “when you’re raised on sci-fi movies, the thought
of a system that can learn what you need by voice is incredibly
appealing.” A user saw the system as being very helpful in
repeating common tasks to accomplish her job, “for me as
a data person especially, during the COVID-19 crisis when
local governments are behind on data standards, I’ve found
the lack of such tool exhausting. The level of manual data
entry required to achieve my basic analysis goals is often more
than I can make time for, and one day that I fail to check is
data that may be permanently lost. I love the idea of being
able to program that cleanly, with my voice. I love that it
can intelligently extract numbers from characters and perform
basic operations, and run just by speaking.”

LIMITATIONS AND FUTURE WORK

Task 1
5.0

category

4.5

hand
tool

4.0

score

3.5
3.0
2.5
2.0
1.5
1.0
mental

temporal

performance

effort

frustration

Brittle Selectors

Our system records objects a user is referring to when performing an action using their CSS selectors. Hence, our system can
fail whenever the CSS selectors on the web page change. This
can be due to a change in a website’s layout, recompiling dynamic CSS modules, or automatically generated CSS classes.
Additionally, our system cannot select a portion of an element,
and must always select the full element. Future work should
investigate the use of alternative selection strategies, such as
combining clicks with natural language specifications [16], or
selection based on computer vision [21].

metric
Task 2
5.0

category

4.5

hand
tool

Robustness of Execution with Puppeteer

In addition to the limitations of CSS selectors, another source
of potential failures while executing a WebTalk is due to timing, and the amount of dynamic content. For example, inserting variables using Puppeteer in Google’s search bar was very
inconsistent due to the input’s extensive use of JavaScript for
auto-complete. To allow pages to react to the user input, we
rate-limited the calls to the Puppeteer API, but that made the
system quite slow (about as slow as executing by hand). In
the future, we should introduce a mechanism to detect dynamically when the page is changing and when it is stable and
ready for the next action.

4.0

score

3.5
3.0
2.5
2.0
1.5
1.0
mental

temporal

performance

effort

frustration

metric

Natural Language and Voice Recognition

Task 3
5.0

category

4.5

hand
tool

4.0

score

3.5
3.0
2.5
2.0
1.5
1.0
mental

temporal

performance

effort

frustration

metric
Task 4
5.0

category

4.5

hand
tool

4.0

score

3.5
3.0
2.5
2.0

Our current prototype uses a template-based algorithm to understand the user’s commands in natural language. As a result,
users have less freedom to alter the wording of their utterances slightly, requiring them to memorize commands. Users
have reported in the user study that remembering commands
was non-trivial. Future work should investigate the use of
neural semantic parsing to translate from natural language
to WebTalk statements directly [6]. Using a neural semantic
parser would also allow users to compound multiple statements or clauses in a single sentence, for example combine a
timer and a conditional.
Error Correction and Editability

Once a user defines a function by demonstration, the program
is not editable either via demonstration or through direct modification of the WebTalk code. This is particularly problematic
because for conditionals, we can only demonstrate one side of
the conditional branch. In the future users will be able to refine
their created functions, either by recording additional traces
that the system would merge, or by interactively editing the
generated code (perhaps in natural language). Interactive refinement is also a prerequisite for building larger skills, which
need to be debugged.

1.5
1.0

Authentication & Privacy
mental

temporal

performance

effort

frustration

metric

Figure 8. For each of the tasks in the real world scenarios we use the
NASA-TLX score to evaluate the perceived workload of complete the
task using VASH compared with doing it by hand. Lower scores are
better in all categories, except for performance, where higher values are
better.

As the browser profiles used for recording and execution are
entirely separate, our design requires the execution browser
to replicate the entirety of the profile (cookies, local storage,
certificates, saved passwords, etc.). In VASH, this is acceptable because both browsers are run on the user’s machine. At
the same time, giving the execution browser access to all the

user’s cookies can be a significant privacy loss, if the execution browser runs on a separate server, which is desirable for
availability and for performance purposes.
Authentication, Fraud, and Spam

VASH can fail to work well with websites such as GMail
and Facebook that use systems to detect fraudulent logins
and bots. Such anti-fraud systems can detect the use of automated browsing APIs, and can detect input that is driven by a
program as opposed to a user with keyboard and mouse. Various techniques have been proposed to subvert these detection
mechanisms [3] but as the subversion improves, so does the
anti-fraud detection.
CONCLUSION

Virtual assistants are changing the way we interact with computers. Along with this, we need to empower individuals to
build programs for virtual assistants, instead of having to reply
on only applications built by large companies.
This paper proposes VASH, a multimodal system that lets
users create their own web skills using program by demonstration. VASH translates voice, keyboard, and mouse inputs
into a program in WebTalk, a language we created for this
purpose. VASH supports control constructs: function calls,
conditionals, and iterations, which are all found to be essential
for automating tasks our users in our survey want. We find
VASH to be expressive enough to implement 81% of userproposed skills. We also evaluate several main design choices,
showing them to be effective. We find that VASH is easy to
learn, and that a majority of the users in our study want to use
our system.
In summary, VASH is an easy-to-learn system that lets consumers create useful virtual assistant web-based skills that
require sophisticated control constructs.
REFERENCES

[1] Tal Ater. 2019. annyang! Speech recognition for your
site. https://github.com/TalAter/annyang. (2019).
[2] Shaon Barman, Sarah Chasins, Rastislav Bodik, and
Sumit Gulwani. 2016. Ringer: Web Automation by
Demonstration. SIGPLAN Not. 51, 10 (Oct. 2016),
748–764. DOI:
http://dx.doi.org/10.1145/3022671.2984020

[3] berstend. 2020. puppeteer-extra-plugin-stealth.
https://github.com/berstend/puppeteer-extra/tree/
master/packages/puppeteer-extra-plugin-stealth.

(2020).
[4] Margaret Burnett, Curtis Cook, Omkar Pendse, Gregg
Rothermel, Jay Summet, and Chris Wallace. 2003.
End-user software engineering with assertions in the
spreadsheet paradigm. In 25th International Conference
on Software Engineering, 2003. Proceedings. IEEE,
93–103.
[5] Giovanni Campagna, Rakesh Ramesh, Silei Xu, Michael
Fischer, and Monica S. Lam. 2017. Almond: The
Architecture of an Open, Crowdsourced,
Privacy-Preserving, Programmable Virtual Assistant. In
Proceedings of the 26th International Conference on

World Wide Web - WWW ’17. ACM Press, New York,
New York, USA, 341–350. DOI:
http://dx.doi.org/10.1145/3038912.3052562

[6] Giovanni Campagna, Silei Xu, Mehrad Moradshahi,
Richard Socher, and Monica S. Lam. 2019. Genie: A
Generator of Natural Language Semantic Parsers for
Virtual Assistant Commands. In Proceedings of the 40th
ACM SIGPLAN Conference on Programming Language
Design and Implementation (PLDI 2019). ACM, New
York, NY, USA, 394–410. DOI:
http://dx.doi.org/10.1145/3314221.3314594

[7] Sarah E. Chasins, Maria Mueller, and Rastislav Bodik.
2018. Rousillon: Scraping Distributed Hierarchical Web
Data. In Proceedings of the 31st Annual ACM
Symposium on User Interface Software and Technology
(UIST ’18). Association for Computing Machinery, New
York, NY, USA, 963–975. DOI:
http://dx.doi.org/10.1145/3242587.3242661

[8] Allen Cypher, Daniel C. Halbert, David Kurlander,
Henry Lieberman, David Maulsby, Brad A. Myers, and
Alan Turransky. 1993. Watch What I Do: Programming
by Demonstration. MIT Press, Cambridge, MA, USA.
[9] Michael Fischer, Giovanni Campagna, Silei Xu, and
Monica S Lam. 2018. Brassau: automatic generation of
graphical user interfaces for virtual assistants. In
Proceedings of the 20th International Conference on
Human-Computer Interaction with Mobile Devices and
Services. 1–12.
[10] Jack Franklin and others. 2020. Puppeteer Headless
Chrome Node.js API.
https://github.com/puppeteer/puppeteer. (2020).
[11] Shuchi Grover and Roy Pea. 2013. Computational
thinking in K–12: A review of the state of the field.
Educational researcher 42, 1 (2013), 38–43.
[12] Sandra G Hart. 2006. NASA-task load index
(NASA-TLX); 20 years later. In Proceedings of the
human factors and ergonomics society annual meeting,
Vol. 50. Sage Publications Sage CA: Los Angeles, CA,
904–908.
[13] Bret Kinsella. 2019. Amazon Alexa Has 100k Skills But
Momentum Slows Globally. Here is the Breakdown by
Country. https://voicebot.ai/2019/10/01/

amazon-alexa-has-100k-skills-but-momentum-slows-globally-here-is

Voicebot.ai (October 2019).
[14] Gierad P Laput, Mira Dontcheva, Gregg Wilensky,
Walter Chang, Aseem Agarwala, Jason Linder, and
Eytan Adar. 2013. Pixeltone: A multimodal interface for
image editing. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems.
2185–2194.
[15] Toby Jia-Jun Li, Amos Azaria, and Brad A Myers. 2017.
SUGILITE: creating multimodal smartphone
automation by demonstration. In Proceedings of the
2017 CHI conference on human factors in computing
systems. 6038–6049.

[16] Toby Jia-Jun Li, Igor Labutov, Xiaohan Nancy Li,
Xiaoyi Zhang, Wenze Shi, Wanling Ding, Tom M
Mitchell, and Brad A Myers. 2018. APPINITE: A
Multi-Modal Interface for Specifying Data Descriptions
in Programming by Demonstration Using Natural
Language Instructions. In 2018 IEEE Symposium on
Visual Languages and Human-Centric Computing
(VL/HCC). IEEE, 105–114.
[17] Anton Medvedev. 2020. finder: CSS Selector Generator.
https://github.com/antonmedv/finder. (2020).
[18] Mozilla. 2020. mozilla/firefox-voice. (May 2020).
https://github.com/mozilla/firefox-voice

[19] Tim Nolet. 2020. Puppeteer Recorder.
https://github.com/checkly/puppeteer-recorder.

(2020).

[20] John F Pane, Brad A Myers, and others. 2001. Studying
the language and structure in non-programmers’
solutions to programming problems. International
Journal of Human-Computer Studies 54, 2 (2001),
237–264.
[21] Alborz Rezazadeh Sereshkeh, Gary Leung, Krish
Perumal, Caleb Phillips, Minfan Zhang, Afsaneh Fazly,

and Iqbal Mohomed. 2020. VASTA: a vision and
language-assisted smartphone task automation system.
Proceedings of the 25th International Conference on
Intelligent User Interfaces (Mar 2020). DOI:
http://dx.doi.org/10.1145/3377325.3377515

[22] Janice Tsai and Jofish Kaye. 2018. Hey Scout:
Designing a Browser-Based Voice Assistant. (2018).
https:
//aaai.org/ocs/index.php/SSS/SSS18/paper/view/17543

[23] Blase Ur, Elyse McManus, Melwyn Pak Yong Ho, and
Michael L Littman. 2014. Practical trigger-action
programming in the smart home. In Proceedings of the
SIGCHI Conference on Human Factors in Computing
Systems. ACM, 803–812.
[24] Silei Xu, Giovanni Campagna, Jian Li, and Monica S.
Lam. 2020. Schema2QA: Answering Complex Queries
on the Structured Web with a Neural Model. (2020).
[25] Tantek Çelik, Elika J. Etemad, Daniel Glazman, Ian
Hickson, Peter Linss, and John Williams. 2018.
Selectors Level 3 (W3C Recommendation).
https://www.w3.org/TR/selectors-3/. (2018).

