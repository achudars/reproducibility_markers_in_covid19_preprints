To appear at 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM).

Unsupervised and Interpretable Domain Adaptation
to Rapidly Filter Tweets for Emergency Services
Jitin Krishnan

arXiv:2003.04991v2 [cs.CL] 20 Oct 2020

Department of Computer Science
George Mason University
Fairfax, VA, USA
jkrishn2@gmu.edu

Hemant Purohit
Department of Information
Sciences & Technology
George Mason University
Fairfax, VA, USA
hpurohit@gmu.edu

Abstract—During the onset of a natural or man-made crisis
event, public often share relevant information for emergency
services on social web platforms such as Twitter. However,
filtering such relevant data in real-time at scale using social
media mining is challenging due to the short noisy text, sparse
availability of relevant data, and also, practical limitations in collecting large labeled data during an ongoing event. In this paper,
we hypothesize that unsupervised domain adaptation through
multi-task learning can be a useful framework to leverage data
from past crisis events for training efficient information filtering
models during the sudden onset of a new crisis. We present a
novel method to classify relevant social posts during an ongoing
crisis without seeing any new data from this event (fully unsupervised domain adaptation). Specifically, we construct a customized
multi-task architecture with a multi-domain discriminator for
crisis analytics: multi-task domain adversarial attention network
(MT-DAAN). This model consists of dedicated attention layers for
each task to provide model interpretability; critical for real-word
applications. As deep networks struggle with sparse datasets, we
show that this can be improved by sharing a base layer for multitask learning and domain adversarial training. The framework
is validated with the public datasets of TREC incident streams
that provide labeled Twitter posts (tweets) with relevant classes
(Priority, Factoid, Sentiment) across 10 different crisis events such
as floods and earthquakes. Evaluation of domain adaptation
for crisis events is performed by choosing one target event as
the test set and training on the rest. Our results show that
the multi-task model outperformed its single-task counterpart.
For the qualitative evaluation of interpretability, we show that
the attention layer can be used as a guide to explain the
model predictions and empower emergency services for exploring
accountability of the model, by showcasing the words in a tweet
that are deemed important in the classification process. Finally,
we show a practical implication of our work by providing a
use-case for the COVID-19 pandemic.
Index Terms—Social Media, Crisis Analytics, Text Classification,
Unsupervised Domain Adaptation, Interpretability

I. I NTRODUCTION
During the sudden onset of a crisis situation, social media
platforms such as Twitter provide valuable information to aid
crisis response organizations in gaining real-time situational
awareness [1]. Effective analysis of important information
such as affected individuals, infrastructure damage, medical
emergencies, or food and shelter needs can help emergency
responders make time-critical decisions and allocate resources
in the most effective manner [2]–[4].

Huzefa Rangwala
Department of Computer Science
George Mason University
Fairfax, VA, USA
rangwala@gmu.edu

Fig. 1. Problem Statement: Interpretably predict labels for tweets collected
during an ongoing crisis using only the past crisis data, given a) unavailability
of labeled data in the ongoing event, and b) need for interpretability of
machine reasoning behind data filtering for emergency managers.

Several machine learning systems have been deployed to help
towards this humanitarian goal of converting real-time social
media streams into actionable knowledge. Classification being
the most common task, researchers have designed models
[3], [5]–[7] that classify tweets into various crisis-dependent
categories such as priority, affected individuals, type of damage, type of assistance needed, usefulness of the tweet, etc.
Social media streams contain short, informal, and abbreviated
content; with potential linguistic errors and sometimes contextually ambiguous. These inherently challenging properties
of tweets make their classification task and formulation less
trivial when compared to traditional text classification tasks.
In this paper, we address two practically important and underdeveloped aspects of current research in social media mining
for crisis analytics to classify relevant social web posts: a) a
fully unsupervised domain adaptation, and b) interpretability
of predictions. A fully unsupervised domain adaptation uses
no data from the ongoing crisis to train the model. Nguyen et
al., 2016 [5] showed that their convolutional neural network
(CNN) model does not require feature engineering and performed better than the state-of-the-art methods; one of their
models being completely unsupervised [5]. Similarly, Alam et
al., 2018 [6] designed a CNN architecture with adversarial

training on graph embeddings, but utilizing unlabeled target
data. Our goal is to construct an unsupervised model that
does not require any unlabeled target data with the capability
of being interpretable. We specifically address the problem
of data sparsity and limited labels by designing a multitask classification model with domain adversarial training;
which, to the best of our knowledge, is not explored in social
media mining for crisis analytics. Another crucial component
of our model is interpretability. In prior works, when a top
performing model produces an accuracy of 78%, for instance,
it is unclear how trustworthy it is and what features are
deemed important in the model’s decision-making process. An
interpretable model like ours can present with a convincing
evidence of which words the classifier deems important when
making a certain prediction, and helps ensure reliability for
domain users, e.g., emergency managers.
Contributions: a) To address the problems of data sparsity
and limited labels, we construct a customized multi-task learning architecture (MT-DAAN) to filter tweets for crisis analytics
by training four different classification tasks (c.f. examples
in Fig. 3) across ten different crisis events under domain
shift. This multi-task domain adversarial model consists of
dedicated attention layers for each task for interpretability
and a domain classifier branch to promote the model to
be domain-agnostic. b) We demonstrate that the attention
layers provide interpretability for the predictions made by the
classifiers; with the goal to aid emergency services in a more
meaningful way. c) We empirically validate the performance
of the underlying single-task attention-based neural network
architecture by comparing it to the state-of-the-art methods,
for improving generalizability and interpretability for domain
adaptation in unsupervised tweet classification tasks in general.
d) Additionally, through experiments, we show that deep
networks struggle with small datasets, and that this can be
improved by sharing the base layer for multi-task learning
and domain adversarial training.
II. R ELATED W ORK AND BACKGROUND
A. Domain Adaptation
Domain Adaptation in text classification tasks has a long line
of fruitful research [8]–[10] that try to minimize the difference
between the domains so that a model trained solely on one
domain is generalizable to unseen test data from a completely
different domain. With the introduction of Domain-Adversarial
training of Neural Networks (DANN) [11], many state-of-theart models now utilize unlabeled target data to train classifiers
that are indiscriminate toward different domains. The speciality of this architecture is that it consists of an extra branch,
which performs domain classification using unlabeled data
from different domains. Thus, both task and domain classifiers
share some bottom layers but have separate layers towards the
top. A negative gradient (gradient reversal) from the domain
classifier branch is back-propagated to promote the features at
the lower layers of the network incapable of discriminating
the domains. Recent works such as Adversarial Memory
Network (AMN) [12] utilizes attention, in addition to DANN,

to bring interpretability to capture the pivots for sentiment
classification. Hierarchical Attention Network (HATN) [13]
expands upon AMN by first extracting pivots and then jointly
training networks for both pivots and non-pivots.
For filtering social web data for crisis analytics, these models
do not suffice and need customized expansions due to the
following reasons: a) Collecting and using large unlabeled
target data from the new/ongoing crisis event may not be
practically viable, thus, we aim for a fully unsupervised
modeling. b) Having access to unlabeled data from multiple
crisis events can alleviate the above problem to an extent by
using it to train the domain classifier branch to push the model
to be domain independent. c) Due to the low-resource nature
of the dataset, binary classifiers may miss important lower
level features that can be potentially improved by a multi-task
model that shares the lower layers of the network for all the
tasks. This is also evident from our results in Table III and
IV, which show that deep models that perform much better
than simple models on Amazon reviews do not significantly
outperform them on TREC tweet dataset for crises.
B. Multi-Task Learning
Multi-Task Learning (MTL) solves multiple tasks at the same
time with a goal to improve the overall generalization capability of the model [14]. Within the context of Deep Learning,
MTL is performed by sharing (or constraining) lower level
layers and using dedicated upper level layers for various
tasks. A rich overview of MTL in Deep Neural Networks is
presented by Ruder (2017) [15]. MTL has been a successful
strategy over the past few years for many research explorations
such as relationship networks [16] in computer vision and
Sluice networks [17] in natural language processing. Similar
problems in domain adaptation of semantic classification and
information retrieval were addressed by jointly learning to
leverage large amounts of cross-task data [18]. In low resource
datasets such as for crises, the chance of overfitting is very
high. Thus, it seems intuitively better for the model to find a
shared representation capturing different tasks and not just one,
such that feature commonalities across tasks can be exploited.
C. Attention Mechanism
Attention mechanism [19], originally designed for machine
translation problems, has become one of the most successful
and widely used methods in deep learning that can look at a
part of a sentence at a time like humans. This is particularly
useful because of its ability to construct a context vector
by weighing on the entire input sequence unlike previous
sequence-to-sequence models [20] that used only the last
hidden state of the encoder network (typically BiLSTM [21],
LSTM [22], or GRU [23]). For example, in a sentence,
the context vector is a dot product of the word activations
and weights associated with each word; thus leading to an
improved contextual memorization, especially for long sentences. Our method incorporates such attention mechanisms
to enhance interpretability of the classifier.

III. M ETHODOLOGY
A. Problem Statement: Unsupervised Domain Adaptation for
Crisis Tweet Classification
Using notations in Table I, consider a set C of all crisis
events such as Guatemala Earthquake or Typhoon Yolanda.
The task of unsupervised domain adaptation for crisis
analytics is to train a classifier for a specific target crisis
(ct ) using labeled (LC−ct ) and unlabeled (UC−ct ) data from
all other crises; where C − ct denotes the set of all crisis
events minus the target crisis. We assume that no data record
from the target crisis is available for training. Following
the traditional domain adaptation terminology, Xs = LC−ct
represents the labeled data from the source domain S and
Ys = yC−ct represents the ground truth labels on which the
classifier is trained. And, Xt = Lct represents the labeled
data from the target domain T and Yt = yct represents the
ground truth labels; both of which are only used for testing
the classifier. Xd = UC−ct represents the unlabeled data from
different domains minus the target. To summarize:
Input: Xs , Ys , Xd
Output: Ytpred ← predict(Xt )
Notation

Definition

C
Lck
yck
m
Uck

Set of all crisis events {c1 , c2 , ...}
Set of labeled data from the event ck
Set of ground truth labels for Lck .
Number of tasks (Number of bits in each label)
Set of unlabeled data from the event ck

Tx
x<k>
α<k>
a<k>

Number of words in a sentence
k-th word of a sentence
attention from k-th word
BiLSTM activation from k-th word

TABLE I
N OTATIONS

B. Overview
In the following sections, we describe three models: SingleTask Attention Network (ST), Single-Task Domain Adversarial Attention Network (ST-DAAN), and Multi-Task Domain
Adversarial Attention Network (MT-DAAN). ST is the model
we adopt from [24] to build the single-task attention based
baseline. ST-DAAN is constructed on top of ST to make
the model domain agnostic by performing adversarial training
using gradient reversal. Finally, MT-DAAN is constructed on
top of ST-DAAN with dedicated attention layers for each task
on a shared BiLSTM layer. This is shown in Figure 2.

Fig. 2. Fully Unsupervised Domain Adaptation Set-up for Multi-Task Crisis
Tweet Classification.

component via gradient reversal, this method is a fully
unsupervised baseline which also can be customized for
multi-task learning.
2) The method uses attention mechanism which in turn
weighs each word in a sentence based on its importance.
This can be directly utilized for interpretability.
3) The method also runs much faster (only a few minutes),
i.e. highly useful in crisis times, as compared to the top
performing semi-supervised models such as HATN [13]
(hours).
This model [24] consists of a BiLSTM layer which produces
Tx activations, each corresponding to a word in the sentence.
These activations are passed through dense and softmax layers
and
by dot product to produce the context vector
PTxare combined
<k> <k>
α
a
,
where a<k> is the BiLSTM activation
k=1
from k-th word and α<k> is the attention weight of k-th word.
Sentences with words greater than Tx are stripped and those
with words lower than Tx are padded. This single-task (m = 1)
attention network is the building block with which rest of the
following models are constructed. The single-task binary cross
entropy loss function is shown below.
LT = −

N
1 X
[yi log yˆi + (1 − yi ) log(1 − yˆi )]
N i=1

(1)

where T represents the task, y is the true label, and ŷ is the
predicted label.

C. Single-Task Attention Network (ST)

D. Single-Task Domain Adversarial Attention Network
(ST-DAAN)

We first describe the single-task attention network [24] on
top of which we build our models. This model aligns with our
goals of interpretability and unsupervised domain adaptation.
This BiLSTM based model with Attention gives us three main
advantages:
1) Unlike several existing domain adaptation methods that
use unlabeled target data to train the domain adversarial

To study the specific contribution of domain adversarial training, we construct a secondary baseline over the ST architecture
by constructing an additional branch with gradient reversal
layer which is represented by the green blocks in Figure 2.
This is a single-task binary classifier with m = 1. Domain
Adversarial Training of Neural Networks (DANN) [11] was
introduced with a goal to confuse the classifier by back-

propagating a negative gradient from a separate domain classifier branch (right-most branch, as shown in Figure 2). This
makes the classifier agnostic to difference in domains. This
back-propagation is implemented using a gradient reversal
layer [11] which does nothing during the forward pass but
d
pushes a negative gradient (−λ ∂L
∂θf ) during the backward
(gradient update) pass. Ld is the domain classification loss,
λ is the strength of the reversal, and f represents the lower
level layers or features over which the negative gradient update
is performed. In our architecture, the goal is to make the
BiLSTM layer indiscriminate towards various crisis domains
such that the multi-task classification does not depend on the
domain from which the tweet/sentence is coming from. The
ST-DAAN loss function is shown below.
L0T = LT + wd Ld

(2)

CRISIS EVENTS
2012 Guatemala Earthquake
2013 Typhoon Yolanda
2013 Australia Bushfire
2013 Boston Bombings
2013 Queensland Floods
2014 Chile Earthquake
2014 Typhoon Hagupit
2015 Nepal Earthquake
2015 Paris Attacks
2018 Florida School Shooting

Total
Tweets
154
564
677
535
713
311
1470
2048
2066
1118

Vocab Avg
#words
422
18.74
1746 19.47
2102 20.21
1755 19.30
2301 19.08
919
16.54
2893 15.36
4026 13.77
4152 18.62
2940 21.40

P

F

S

I

104
249
152
147
293
48
469
1067
306
329

108
46
213
28
54
26
375
377
183
64

12
119
167
234
173
50
276
741
782
206

15
51
36
198
215
10
101
133
429
70

TABLE II
TREC DATASET S TATISTICS ; S HOWING THE NUMBER OF POSITIVE
SAMPLES FOR EACH OF THE 4 CLASSES . P =P RIORITY, F =FACTOID ,
S=S ENTIMENT, AND I =I RRELEVANT.

where m is the number of tasks, wk is the loss weight and
LTk is the loss term for each task, wd is the domain adversarial
loss weight, and Ld is the domain adversarial loss term.
F. Model Interpretability

where wd is the domain adversarial loss weight. Ld represents
the categorical cross entropy loss for multi-domain discriminator shown below.
Ld = −

N |C−ct |
1 X X
[yij log yˆij ]
N i=1 j=1

(3)

where C − ct is the set of all crisis events without the target
event.

Building on top of ST-DAAN, we construct MT-DAAN,
which is intended to classify problems with multiple tasks or
labels. For each task, a dedicated attention layer is allocated
from which it predicts binary labels. The BiLSTM layer
remains exactly the same as in the single-task model but
multiple attention blocks are added for each task along with a
domain classifier. In the architecture decision process, we first
investigated a multi-label classifier where all layers are shared
with the final softmax layer making multi-label predictions.
In low resource settings, constructing a multi-label classifier
using a shared architecture is challenging for two reasons:
a) jointly balancing positive and negative samples across all
classes is not trivial and potentially challenging to make it
extensible when new classes need to be added, and b) attention
layer may not always produce class-specific insights as the
weights are assigned to train for the combination of labels.
On the other hand, in the multi-task architecture with separate
attention layers, it is easy to add more classes. If some classes
require more training, it is trivial to further tune a model
specific to that class. More importantly, context<tj > vector
for j-th task identifies the influential words from each sentence
for that specific task. The complete architecture is shown in
Figure 2. MT-DAAN loss function is shown below:
m
X

(wk LTk ) + wd Ld

k=1

IV. DATASETS
A. TREC Dataset

E. Multi-Task Domain Adversarial Attention Network
(MT-DAAN)

LM T −DAAN =

The output (α) of the attention layer (AT T ) of each task, is
a Tx -dimensional vector; Tx beingP
the number of words in
Tx
the sentence. The context vector ( k=1
α<k> a<k> ) is the
product of these attention weights and the Tx -dimensional
activation (a) from the BiLST M layer. α essentially weighs
how much each word in the sentence contributes to the
classification result. Thus, α is the component that is evaluated
for model interpretability.

(4)

TREC-IS1 (Text Retrieval Conference - Incident Streams) is
a program that encourages research in information retrieval
from social media posts with the goal to improve the state-ofthe-art social media based crisis analytics solutions. We use
the dataset from 2018 track proposal. Statistics of this curated
dataset of Twitter downloaded from TREC is shown in Table
II. The original dataset consisted of 15 crisis events. However,
due to very low data, we trimmed the events and tasks such
that there are at least 10 positive samples for each task.
The four tasks used in our experiments are shown below:
1) Priority: Different priority levels are assigned for each
tweet: low, medium, high, critical. We convert this into
a binary classification problem where low = 0 and
{medium, high, critical} = 1.
2) Factoid: ‘Factoid’ is a categorical label that represents
if a tweet is stating a fact. Eg: ‘death toll rises ...’
3) Sentiment: ‘Sentiment’ is a categorical label that represents if a tweet represents a sentiment. Eg: ’Worried..
Thoughts and prayers.’
4) Irrelevant: ‘Irrelevant’ is a categorical label for tweets
that do not provide any relevant information.
B. Amazon Reviews Dataset
The standard benchmark dataset2 of Amazon reviews [25] is
widely used for cross-domain sentiment analysis. We chose
1 http://dcs.gla.ac.uk/∼richardm/TREC

IS/

2 http://www.cs.jhu.edu/∼mdredze/datasets/sentiment/

S→T
B→K
B→E
B→D
K→B
K→E
K→D
E→B
E→K
E→D
D→B
D→K
D→E
AVG

LR
76.40
75.53
81.08
76.12
80.37
73.32
74.85
81.85
75.82
81.17
76.42
72.47
77.12

SVM
75.95
74.05
81.43
75.78
81.20
74.98
74.18
81.85
75.83
82.20
77.58
73.68
77.39

CNN
81.20
80.44
82.94
78.78
85.17
76.41
78.08
86.59
78.35
82.26
81.09
79.56
80.91

BiLSTM
84.45
84.61
83.52
80.67
87.37
78.49
81.18
89.00
78.46
84.83
85.21
83.66
83.45

AMN

HATN

81.88
80.55
85.62
79.05
86.68
79.50
77.52
87.83
85.03
84.53
81.67
80.42
82.52

87.03
85.75
87.07
84.88
89.00
84.72
84.03
90.08
84.32
87.78
87.47
86.32
86.54

ST
87.22
85.51
86.32
81.85
87.09
81.13
81.50
89.21
81.37
87.02
86.37
85.63
85.02

TABLE III
P ERFORMANCE COMPARISON ( ACCURACY ) OF VARIOUS MODELS ON THE
STANDARD BENCHMARK DATASET OF AMAZON REVIEWS . M ETHODS IN
BLUE DO NOT USE ANY UNLABELED TARGET DATA ; HENCE RELEVANT IN
OUR CONTEXT. E ACH REPORTED SCORE IS AN AVERAGE OF 10
INDEPENDENT RUNS OF EACH EXPERIMENT.

four domains: Books (B), Kitchen (K), DVD (D), and Electronics (E). The raw data3 , a part of Blitzer’s original raw
dataset, used in this work is from HATN [13]. This dataset
consists of 3000 positive and 3000 negative samples for each
of the 4 domains. This dataset is used for two purposes: 1)
to validate the performance of the state-of-the-art methods
including the single-task baseline and 2) to compare and
contrast the performance of deep models when trained with
rich versus sparse datasets.
C. COVID-19 Tweet Dataset
For the COVID-19 use-case, we use Twitter posts collected
using CitizenHelper [26] system in March 2020, for the geobounding box of the Washington D.C. Metro region. These
tweets were annotated by volunteers of regional Community
Emergency Response Teams (CERTs), with ‘Relevant’ label
denoting how relevant a tweet is for crisis response operations.
The label values range on a scale of 1-4. We convert them into
binary classes by considering values 1 and 2 as −ve (0) class
and values 3 and 4 as +ve (1) class. This dataset consists
of 4911 tweets with −ve (Relevant=0) and 637 tweets with
+ve (Relevant=1) classes. Following unsupervised domain
adaptation criteria, the filtering models are trained using only
the TREC dataset and evaluated on the COVID-19 tweets. For
each independent run of the experiment, a balanced subset of
size 637 for both classes is selected for testing.
V. R ESULTS & D ISCUSSION
We first validate the performance of the adopted unsupervised
ST model [24] by comparing it with the following standard
neural network architectures and state-of-the-art models used
for domain adaption in text. We use the standard benchmark
dataset of Amazon reviews. Following the traditional domain
adaptation experimental setup, each experiment represented as
S → T consists of a source domain (S) on which the model is
trained and a target domain (T) on which the model is tested.
We use Keras deep learning library for our implementations;
with Tx =200 for Amazon reviews and 30 for Tweets. We use
Adam optimizer with a dropout of 0.4, maximum epoch of 50,
3 https://github.com/hsqmlzno1/HATN/tree/master/raw

data

Target
Guatemala Earthquake
Typhoon Yolanda
Australia Bushfire
Boston Bombings
Queensland Floods
Chile Earthquake
Typhoon Hagupit
Nepal Earthquake
Paris Attacks
Florida School Shooting
AVG

LR
60.14
65.39
65.61
71.47
65.56
43.09
49.86
57.11
71.43
58.79
60.85

SVM
56.76
65.97
63.23
75.45
64.81
37.94
46.22
55.39
71.72
63.02
60.05

CNN
60.47
63.05
62.10
69.72
64.13
43.37
49.21
58.61
72.50
58.82
60.20

BiLSTM
65.54
65.49
60.10
71.43
66.01
35.45
54.13
60.49
72.14
59.71
61.05

ST
59.97
65.53
62.44
72.08
66.21
39.23
52.61
61.35
71.31
60.55
61.13

TABLE IV
P ERFORMANCE COMPARISON ( ACCURACY ) OF UNSUPERVISED MODELS
ON TREC-P RIORITY ( TWEET ) DATASET SHOWING THAT DEEP MODELS
ARE NOT STRICTLY SUPERIOR THAN SIMPLER MODELS DUE TO DATA
SPARSITY. E ACH REPORTED SCORE IS AN AVERAGE OF 10 INDEPENDENT
RUNS OF EACH EXPERIMENT. Source = Everything - T arget.

early stopping patience of 3, batch size of 32, and validation
split of 0.15.
1) Simple Baselines: We construct simple baseline classifiers [27]: Logistic Regression (LR) and Support Vector Machines (SVM). The input to these models are
constructed by aggregating the 300-dimensional word
embeddings of words in each review.
2) CNN: A standard Convolutional Neural Network inspired by Kim, 2014 [28] is constructed with the following architecture:
W ord Embeddings(Tx , 300) → Conv1D(128, 5)
→ M axP ooling1D(5) → Conv1D(128, 5)
→ M axP ooling1D(5) → Conv1D(128, 5)
→ GlobalM axP ooling1D() → Dense(128)
→ Dense(2) → y.
This is combined with dropouts, relu activations, and
ending with softmax activation producing labels for binary classification. State-of-the-art deep learning methods for existing social media mining approaches of crisis
analytics [5], [6] use a similar architecture.
3) BiLSTM: This is the bottom-most layer in Figure 2
with the activation a<Tx > passed through the following:
Dense(10) → Dense(2) → y also including dropouts,
relu activation, and ending with softmax.
4) AMN and HATN: AMN [12] and HATN [13] are
attention-based methods which use gradient reversal to
perform domain adversarial training on the unlabeled
data from source and target domains. HATN is an
extension to AMN by adding the hierarchical component
and jointly training pivot and non-pivot networks.
Input to all the models are word vectors4 [29]. The evaluation on amazon reviews shows how well the single-task
(ST) model perform when compared to the existing topperforming domain adaptation models on benchmark dataset.
Table III shows accuracy scores on the Amazon cross-domain
sentiment analysis dataset. HATN uses unlabeled target data,
gradient reversal, explicit pivot extraction, and joint training
making it a computationally expensive method. As shown
in the experimental evaluation, we use the same Amazon
dataset and GoogleNews word vectors for our experiments.
4 https://code.google.com/archive/p/word2vec/

TARGET
ST
Guatemala Earthquake
Typhoon Yolanda
Australia Bushfire
Boston Bombings
Queensland Floods
Chile Earthquake
Typhoon Hagupit
Nepal Earthquake
Paris Attacks
Florida School Shooting
AVG
TARGET

Acc
59.97
65.53
62.44
72.08
66.21
39.23
52.61
61.35
71.31
60.55
61.13

F1
62.39
65.47
66.69
74.29
65.94
40.92
50.59
59.44
76.26
61.75
62.37
ST

Guatemala Earthquake
Typhoon Yolanda
Australia Bushfire
Boston Bombings
Queensland Floods
Chile Earthquake
Typhoon Hagupit
Nepal Earthquake
Paris Attacks
Florida School Shooting
AVG

Acc
96.96
75.81
75.95
81.39
81.69
92.69
84.98
67.75
76.01
68.77
80.20

F1
97.03
77.62
77.58
81.11
80.39
92.91
85.86
68.42
76.63
71.77
80.93

Priority
ST-DAAN
Acc
F1
69.07
69.66
66.07
63.73
61.07
63.42
72.34
73.37
67.19
66.97
38.91
42.37
58.97
58.94
60.18
57.80
70.42
74.08
65.47
64.07
62.97
63.44
Sentiment
ST-DAAN
Acc
F1
96.45
96.68
77.54
79.01
78.80
79.12
80.73
80.70
81.05
81.39
93.10
93.21
85.15
86.14
70.20
70.51
73.65
73.98
67.06
70.03
80.37
81.08

MT-DAAN
Acc
F1
69.05
69.34
67.42
67.30
61.93
64.28
73.80
74.74
66.74
66.46
41.80
46.33
57.50
57.52
61.65
59.49
74.44
77.21
62.51
63.24
63.68
64.59

ST
Acc
68.92
80.50
64.58
83.10
37.56
30.38
68.98
74.04
75.78
76.73
66.06

MT-DAAN
Acc
F1
96.76
92.73
76.82
78.35
78.54
78.92
82.13
82.10
81.53
81.32
93.62
93.68
85.43
86.38
69.96
70.31
74.47
74.60
68.14
71.05
80.74
80.94

Acc
89.36
76.05
35.42
58.15
65.68
75.16
63.21
31.79
33.91
32.66
56.14

F1
68.47
84.42
60.69
88.51
48.90
33.97
70.79
76.08
80.35
82.67
69.49
ST
F1
89.03
79.77
47.164
55.73
65.36
84.98
75.04
42.10
35.25
40.90
61.53

Factoid
ST-DAAN
Acc
F1
79.90
80.76
82.71
85.61
65.64
60.53
81.42
85.90
50.46
59.82
39.87
48.68
71.42
72.44
80.72
81.00
82.35
84.89
84.55
87.51
71.90
74.71
Irrelevant
ST-DAAN
Acc
F1
91.22
91.06
78.49
80.59
53.78
65.11
58.15
57.43
67.26
65.72
80.46
86.38
71.50
78.25
36.97
47.41
44.52
48.32
44.22
55.27
62.66
67.55

MT-DAAN
Acc
F1
84.05
97.01
84.36
86.93
65.04
60.13
85.82
88.82
49.52
59.21
45.28
54.58
69.49
70.08
81.04
81.02
82.52
85.63
85.80
88.15
73.29
77.16
MT-DAAN
Acc
F1
93.11
92.73
80.46
82.31
51.76
63.36
61.49
61.45
67.88
67.27
80.64
86.56
70.22
77.27
41.49
52.87
47.17
51.32
47.64
58.65
64.19
69.38

TABLE V
U NSUPERVISED DOMAIN ADAPTATION RESULTS ON TREC DATASET SHOWING PERFORMANCE BOOST FOR Priority, Factoid, AND Irrelevant TASKS .
H OWEVER , Sentiment TASK DID NOT SHOW A SIGNIFICANT IMPROVEMENT. S EE PERFORMANCE EVALUATION SECTION FOR DETAILS . E ACH REPORTED
SCORE IS AN AVERAGE OF 10 INDEPENDENT RUNS OF EACH EXPERIMENT.

TARGET
ST
COVID-19

Acc
73.25

F1
77.36

Relevant
ST-DAAN
Acc
F1
74.55
77.51

MT-DAAN
Acc
F1
77.00
78.09

TABLE VI
U NSUPERVISED DOMAIN ADAPTATION RESULTS FOR COVID-19 TWEETS
USING ONLY THE TREC DATASET FOR TRAINING . E ACH REPORTED SCORE
IS AN AVERAGE OF 10 INDEPENDENT RUNS OF EACH EXPERIMENT.

ST, being unsupervised with no need of unlabeled target data,
performed competitively with an overall accuracy of 85.02%;
thus establishing a strong fully unsupervised building block
for us to build upon.
A. Crisis Tweets vs Amazon Reviews
Table III and IV show that deep models struggle with
small datasets such as TREC-IS tweets. When ST model
outperformed Logistic Regression by ∼ 8% on the Amazon
reviews dataset, the difference was only less than 1% with
no statistical significance on the TREC-Priority dataset. Note
that we conduct experiments with various parameter combinations on the deep models when using tweets. For example,
Tx = 200 for amazon reviews and Tx = 30 for tweets due
to the difference in their average word-length. Books domain
of Amazon reviews has 182 average number of tokens per
review with a vocab size of 105920. On the other hand, the
event with highest number of tweets in the TREC dataset
(Paris Attacks) has only 18.62 average number of tokens
per tweet with a vocab size of 4152. This difference makes
it intuitively challenging to train deep models with several
parameters that may lead the model to memorize the entire
dataset resulting in poor generalization. Multi-task learning
and domain adversarial training try to alleviate this problem

by training the shared BiLSTM layer with much more data
from different tasks and unlabeled data.
B. MT-DAAN Performance Evaluation
The primary purpose of the MT-DAAN model is to show that
sharing the bottom layer of the model (i.e., shared representation) for different tasks along with domain adversarial training
can help improve the generalizability of some of the tasks
that are otherwise trained alone in the single-task model. The
experiments for MT-DAAN are setup in the same unsupervised
way as for single-task. No data from the test crisis is used for
training. For example, if we are testing our model for the event
‘Typhoon Yolanda’, no data from this crisis is used for training.
Note that the domain classifier component uses unlabeled data
only from rest of the crisis; making it a fully unsupervised
domain adaptation approach. Performance scores of the four
tasks (Priority, Factoid, Sentiment, and Irrelevant) are shown
in Table V. The results show clear performance improvement
for Priority, Factoid, and Irrelevant tasks. However, Sentiment
task did not show significant improvement. We speculate
that this is because other tasks do not generalize the bottom
layer enough to boost the sentiment classification performance.
These results show the usefulness of multi-task learning as
well as domain adversarial training where different tasks in
multiple domains help each other when the data is sparse and
labels are limited.
C. Word Vectors
We use fastText [30] as our word embeddings for tweets
because of its sub-word usage and the ability to create vectors
for arbitrary and out-of-vocabulary words. Although there

Fig. 4. Examples of interpretable results using attention for relevancy
prediction of COVID-19 tweets. With 77% accuracy, although the highly
attended words in the ‘Relevant’ tweets provide some intuitive sense of
interpretability, the highlighted words in the ‘Irrelevant’ tweets are somewhat
ambiguous because it is unclear if those words are chosen due to their specific
or generic nature. This shows both the benefits and challenges of unsupervised
and interpretable domain adaptation.

Fig. 3. Examples of interpretable results using attention; darker the shade,
higher the attention. Recall that no data from the crisis-event for testing is
used for training the model. Even then, relevant keywords such as ‘police
urging’, ‘death toll rises’, ‘worried’, and ‘thoughts with people’ are correctly
picked up by the attention layers of their respective tasks.

exists many alternatives, picking the one that works well
for a specific dataset is not trivial. We conducted experiments using four choices of word embeddings: fastText [30],
GoogleNews [29], Glove [31], and CrisisNLP [32]. Averaging
over 10 crises, we received the following accuracy scores
(in %) respectively for the above word embeddings: {80.20,
81.82, 81.88, 80.73}. Unlike fastText, we fine-tune these
pre-trained vectors to create vectors for out-of-vocabulary
words. Vectors for words that are already in the vocabulary
are locked while tuning for consistency in evaluation. The
tweet-based embeddings such as Glove or CrisisNLP did not
significantly outperform other models. Glove vectors are 200dimensional while the rest are 300-dimensional which makes
the experiment favoring Glove word vectors. This experiment
shows that the problem of finding a strictly superior word
vector model for tweets still remains a challenging task.
D. Interpretability: Attention Visualization
The attention weights used to create the context vector by
the dot product operation with word activations represent the
interpretable layer in our architecture. These weights represent
the importance of each word in the classification process.
Some examples are shown in Figures 3 and 4. Stronger the
color intensity stronger the word attention. In the first example,
‘boston police urging’ is the reason why the tweet is classified
as +ve priority. Similarly, ‘death toll rises’ in the Factoid
example, ‘worried, prayers’ in the Sentiment example, and
‘thoughts with people’ in the Irrelevant example are clear
intuitive indicators of +ve predictions. These examples show

the importance of having interpretability as a key criterion in
crisis domain adaptation tasks for social media.
To the best of our knowledge, in social media mining for
crisis analytics, there does not exist a ground truth dataset
that highlights the words that explain the labels for tweets.
Using our model as a guide, we hope to build a robust
evaluation dataset as our immediate next step so that the
models can be quantitatively evaluated using robust trustevaluation methods such as LIME [33]. It is also crucial to
note that binary classification tasks such as sentiment analysis
of Amazon reviews has a clear class divide that produces
intuitive keywords such as ‘good’, ‘excellent’, or ‘great’ for
+ve reviews and ‘bad’, ‘poor’, or ‘horrible’ for −ve reviews.
However, for short texts such as tweets shown in Figure 4,
‘relevancy’ can depend on the context and it is unclear which
keywords truly represent the examples in the ‘irrelevant’ class.
VI. COVID-19 U SE -C ASE
We show a practical implication of our work by applying
it to COVID-19 tweets described in Section 4.3. Our goal
is to interpretably predict if a COVID-19 tweet is relevant
or not; a binary classification task. The models are trained
using only the TREC dataset and evaluated on the COVID-19
tweets (a balanced subset of size 637 for +ve and −ve labels).
We found that a combination of ‘Priority’ and ‘Irrelevant’
labels from TREC performs better to predict COVID-19’s
‘Relevant’ label (this can be trivially verified by constructing
two binary classifiers). We augment all three methods (ST, STDAAN, and MT-DAAN) with an additional condition before
label prediction: Rc = Pt ∩ It , which means that a COVID-19
tweet is ‘Relevant’ only if it is predicted both ‘Priority’ = 1
and ‘Irrelevant’ = 0. The scores are reported in Table VI and
the attention results are shown in Figure 4, demonstrating the
effectiveness of our proposed method.

VII. C ONCLUSION
We presented a novel approach of unsupervised domain adaptation with multi-task learning to classify relevant information
from Twitter streams for crisis management, while addressing
the problems of data sparsity and limited labels. We showed
that a multi-task learning model that shares the lower layers
of the neural network with dedicated attention layers for
each task along with a domain classifier branch can help
improve generalizability and performance of deep models in
the settings of limited data. Furthermore, we showed that
using an attention-based architecture can help in interpreting the classifier’s predictions by highlighting the important
words that justify the predictions. We also presented an indepth empirical analysis of the state-of-the-art models on both
benchmark dataset of Amazon reviews and TREC dataset of
crisis events. The application of our generic approach for
interpretable and unsupervised domain adaptation within a
multi-task learning framework can benefit social media mining
systems in diverse domains beyond crisis management.
Reproducibility: Source code and instructions for deployment are available at - https://github.com/jitinkrishnan/
Crisis-Tweet-Multi-Task-DA.
R EFERENCES
[1] C. Castillo, Big crisis data: social media in disasters and time-critical
situations. Cambridge University Press, 2016.
[2] M. Imran, P. Mitra, and C. Castillo, “Twitter as a lifeline: Humanannotated twitter corpora for nlp of crisis-related messages,” arXiv
preprint arXiv:1605.05894, 2016.
[3] H. Li, D. Caragea, C. Caragea, and N. Herndon, “Disaster response aided
by tweet classification with a domain adaptation approach,” Journal of
Contingencies and Crisis Management, vol. 26, no. 1, pp. 16–27, 2018.
[4] S. Vieweg, C. Castillo, and M. Imran, “Integrating social media communications into the rapid assessment of sudden onset disasters,” in
International Conference on Social Informatics. Springer, 2014, pp.
444–461.
[5] D. T. Nguyen, K. A. A. Mannai, S. Joty, H. Sajjad, M. Imran, and
P. Mitra, “Rapid classification of crisis-related data on social networks
using convolutional neural networks,” arXiv preprint arXiv:1608.03902,
2016.
[6] F. Alam, S. Joty, and M. Imran, “Domain adaptation with adversarial
training and graph embeddings,” arXiv preprint arXiv:1805.05151, 2018.
[7] R. Mazloom, H. Li, D. Caragea, C. Caragea, and M. Imran, “A hybrid domain adaptation approach for identifying crisis-relevant tweets,”
International Journal of Information Systems for Crisis Response and
Management (IJISCRAM), vol. 11, no. 2, pp. 1–19, 2019.
[8] J. Blitzer, R. McDonald, and F. Pereira, “Domain adaptation with structural correspondence learning,” in Proceedings of the 2006 conference
on empirical methods in natural language processing, 2006, pp. 120–
128.
[9] S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen, “Cross-domain
sentiment classification via spectral feature alignment,” in Proceedings
of the 19th international conference on World wide web. ACM, 2010,
pp. 751–760.
[10] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
“Stacked denoising autoencoders: Learning useful representations in a
deep network with a local denoising criterion,” Journal of machine
learning research, vol. 11, no. Dec, pp. 3371–3408, 2010.
[11] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky, “Domain-adversarial training of
neural networks,” The Journal of Machine Learning Research, vol. 17,
no. 1, pp. 2096–2030, 2016.
[12] Z. Li, Y. Zhang, Y. Wei, Y. Wu, and Q. Yang, “End-to-end adversarial
memory network for cross-domain sentiment classification.” in IJCAI,
2017, pp. 2237–2243.

[13] Z. Li, Y. Wei, Y. Zhang, and Q. Yang, “Hierarchical attention transfer
network for cross-domain sentiment classification,” in Thirty-Second
AAAI Conference on Artificial Intelligence, 2018.
[14] R. Caruana, “Multitask learning,” Machine learning, vol. 28, no. 1, pp.
41–75, 1997.
[15] S. Ruder, “An overview of multi-task learning in deep neural networks,”
arXiv preprint arXiv:1706.05098, 2017.
[16] M. Long and J. Wang, “Learning multiple tasks with deep relationship
networks,” arXiv preprint arXiv:1506.02117, vol. 2, p. 1, 2015.
[17] S. Ruder12, J. Bingel, I. Augenstein, and A. Søgaard, “Sluice networks:
Learning what to share between loosely related tasks,” stat, vol. 1050,
p. 23, 2017.
[18] X. Liu, J. Gao, X. He, L. Deng, K. Duh, and Y.-Y. Wang, “Representation learning using multi-task deep neural networks for semantic
classification and information retrieval,” 2015.
[19] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” arXiv preprint arXiv:1409.0473,
2014.
[20] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
with neural networks,” in Advances in neural information processing
systems, 2014, pp. 3104–3112.
[21] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45, no. 11, pp.
2673–2681, 1997.
[22] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[23] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Gated feedback
recurrent neural networks,” in International conference on machine
learning, 2015, pp. 2067–2075.
[24] J. Krishnan, H. Purohit, and H. Rangwala, “Diversity-based generalization for neural unsupervised text classification under domain shift,”
ECML-PKDD, 2020.
[25] J. Blitzer, M. Dredze, and F. Pereira, “Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification,” in
Proceedings of the 45th annual meeting of the association of computational linguistics, 2007, pp. 440–447.
[26] R. Pandey and H. Purohit, “Citizenhelper-adaptive: expert-augmented
streaming analytics system for emergency services and humanitarian organizations,” in 2018 IEEE/ACM International Conference on Advances
in Social Networks Analysis and Mining (ASONAM). IEEE, 2018, pp.
630–633.
[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine Learning in Python ,” Journal of Machine
Learning Research, vol. 12, pp. 2825–2830, 2011.
[28] Y. Kim, “Convolutional neural networks for sentence classification,”
arXiv preprint arXiv:1408.5882, 2014.
[29] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their compositionality,” in Advances in neural information processing systems, 2013,
pp. 3111–3119.
[30] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, “Advances in pre-training distributed word representations,” in Proceedings
of the International Conference on Language Resources and Evaluation
(LREC 2018), 2018.
[31] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors
for word representation,” in Empirical Methods in Natural Language
Processing (EMNLP), 2014, pp. 1532–1543. [Online]. Available:
http://www.aclweb.org/anthology/D14-1162
[32] M. Imran, P. Mitra, and C. Castillo, “Twitter as a lifeline: Humanannotated twitter corpora for nlp of crisis-related messages,” in Proceedings of the Tenth International Conference on Language Resources and
Evaluation (LREC 2016). Paris, France: European Language Resources
Association (ELRA), may 2016.
[33] M. T. Ribeiro, S. Singh, and C. Guestrin, “” why should i trust you?”
explaining the predictions of any classifier,” in Proceedings of the 22nd
ACM SIGKDD international conference on knowledge discovery and
data mining, 2016, pp. 1135–1144.

