G OOD PROCTOR OR “B IG B ROTHER ”? AI E THICS AND O NLINE
E XAM S UPERVISION T ECHNOLOGIES

arXiv:2011.07647v1 [cs.CY] 15 Nov 2020

A P REPRINT
Simon Coghlan1,3 , Tim Miller1,3 , and Jeannie Paterson2,3
1
School of Computing and Information Systems
2
Melbourne Law School
3
The Centre for AI and Digital Ethics
The University of Melbourne, Australia
{simon.coghlan,tmiller,jeanniep}@unimelb.edu.au

November 17, 2020

A BSTRACT
This article philosophically analyzes online exam supervision technologies, which have been thrust
into the public spotlight due to campus lockdowns during the COVID-19 pandemic and the growing demand for online courses. Online exam proctoring technologies purport to provide effective
oversight of students sitting online exams, using artificial intelligence (AI) systems and human invigilators to supplement and review those systems. Such technologies have alarmed some students
who see them as ‘Big Brother-like’, yet some universities defend their judicious use. Critical ethical
appraisal of online proctoring technologies is overdue. This article philosophically analyzes these
technologies, focusing on the ethical concepts of academic integrity, fairness, non-maleficence, transparency, privacy, respect for autonomy, liberty, and trust. Most of these concepts are prominent in
the new field of AI ethics and all are relevant to the education context. The essay provides ethical
considerations that educational institutions will need to carefully review before electing to deploy
and govern specific online proctoring technologies.
Keywords Online assessment · Exam proctoring · Ethics · Artificial intelligence

1 Introduction
Recently, online exam supervision technologies have been thrust into the public spotlight due to the growing demand
for online courses [Ginder et al., 2019] and lockdowns during the COVID-19 pandemic [Flaherty, 2020]. While
educational institutions can supervise remote exam-takers simply by watching live online video (e.g. via Zoom),
an evolving range of online proctoring (OP) software programs offer more sophisticated, scalable, and extensive
monitoring functions, including both human-led and automated remote exam supervision. Such technologies have
generated confusion and controversy, including vigorous student protests [White, 2020]. Some universities have dug
in against criticism, while others have outright rejected the technologies or have retreated from their initial intentions
to use them [White, 2020]. At the root of disagreement and debate between concerned students and universities are
questions about the ethics of OP technologies. This essay explores these ethical questions. By doing so, it should assist
students and educators in making informed judgements about the appropriateness of OP systems, as well as shining a
light on an increasingly popular digital technology application.
OP software platforms, which first emerged in 2008 [ProctorU, 2020b], are now booming. A 2020 poll found that 54%
of educational institutions now use them [Grajek, 2020]. Increasingly, OP software contains artificial intelligence (AI)
and machine learning (ML) components that analyse exam recordings to identify suspicious examinee behaviours or
suspicious items in their immediate environment. OP companies, which can make good profits from their products
[Chin, 2020], claim that automating proctoring increases the scalability, efficiency, and accuracy of exam supervision
and the detection of cheating. These features have an obvious attraction for universities, some of which believe the

A

PREPRINT

- N OVEMBER 17, 2020

benefits of OP technologies outweigh any drawbacks. However, the complexity and opacity of OP technologies,
especially their automated AI functions [Hagendorff, 2020], can be confusing. Furthermore, some (though not all)
students complain of a “creepy” Big Brother sense of being invaded and surveiled Hubler [2020]. Predictably, some
bloggers are instructing students how to bluff proctoring platforms [Binstein, 2015].
Scholars have just begun exploring remote and automated proctoring from a range of perspectives, including
pedagogical, behavioral, psychological, and technical perspectives [Asep and Bandung, 2019, Cramp et al., 2019,
González-González et al., 2020]. Nonetheless, and despite vigorous ethical discussion in regular media [Zhou, 2020],
blog posts [Torino, 2020], and on social media, the ethics of emerging OP technologies has received limited scholarly analysis (cf. Swauger [2020]). Although moral assessments can be informed by empirical data about online
and in-person proctoring — such as data about test-taker behavior [Rios and Liu, 2017] and grade comparisons
[Goedl and Malla, 2020] — moral assessments depend crucially on philosophical analysis. In the following ethical analysis, we identify and critically explore the key moral values of academic integrity, fairness, non-maleficence,
transparency, privacy, autonomy, liberty, and trust as they apply to OP technologies.
Some of these concepts are prominent in the new field of AI ethics [Jobin et al., 2019], which is burgeoning as AI
moves increasingly into many facets of our lives, including in education. In this paper, we suggest that OP platforms are
neither a silver bullet for remote invigilation nor, as some would have it, a completely “evil” technology [Grajek, 2020].
This ethical analysis will help to inform concerned individuals while setting out important ethical considerations for
educational institutions who are considering OP platforms, including how they might devise appropriate governance
frameworks for their use and remain accountable for their decisions. It will also provide a context for various future
empirical investigations of OP technologies.
The essay is structured as follows. The Philosophical Approach section briefly explains the relevance of the central
moral values to the OP debate. The Background section provides relevant context concerning exam invigilation and
outlines central technological capabilities of popular OP programs. The Discussion section examines important ethical
issues raised by the emergence of OP software. Finally, the Conclusion summarizes the ethical lessons for educational
institutions and others.

2 Philosophical Approach
This essay employs an analytical philosophical approach which includes the application of a range of moral values
and principles. Broadly speaking, the moral values and principles we discuss have a place in the philosophy of
education [Curren et al., 2003], in the burgeoning field of AI ethics [Lin et al., 2017], and at the intersection of these
two fields. An example of this intersection is the ethics of data analytics in student performance [Kitto and Knight,
2019]. Another example of software that raises ethical issues in education is algorithms that make predictions about
student performance Sweeney et al. [2015] that are used to inform grades Hern [2020]. Compared to the field of
philosophy of education, which can be traced back to Plato’s The Republic, the field of AI Ethics (and, more broadly,
digital ethics) is young and still under development. Nonetheless, academics and various organizations that have
weighed in on AI ethics tend to agree on the importance of a number of key moral ideas or principles [Jobin et al.,
2019].
AI ethics principles have occasionally been criticized for their lack of practical specificity and theorical philosophical
rigor and for missing wider issues of social, racial, and economic injustice and power imbalance [Kind, 2020]. Additionally, principles such as fairness may be used in confusingly different ways Mulligan et al. [2019]. However, these
principles or values provide a starting point for scrutinizing AI as a sociotechnical system even if they require further
support and contextual specification. Furthermore, our use of such principles in this paper goes some way toward fleshing them out and specifying their application to a novel, concrete socio-technological case, as well as subsequently
linking them (briefly) to wider social issues and trajectories.
In a comprehensive global survey of AI ethics guidelines, Jobin et al. identify the ethical principles of transparency,
justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability,
dignity, and solidarity [Jobin et al., 2019]. In another recent study, Floridi et al. highlight the ideas of beneficence, nonmaleficence, autonomy, justice, explicability, and accountability [Floridi et al., 2018]. Values such as these feature in
many other discussions in the AI Ethics literature, and several are especially relevant to online proctoring.
Specifically, the values and principles we shall explore in this paper are fairness, non-maleficence, transparency, privacy, respect for autonomy, and liberty and trust. In addition, we explore the value of academic integrity, which is
more specific to education. We shall briefly introduce these values here, and then return to them in more depth in
subsequent sections.
2

A

PREPRINT

- N OVEMBER 17, 2020

Fairness is commonly referred to in discussions of AI ethics, but is also commonly used in a number of different
ways in both this field and in moral philosophy and legal analysis. Thus, concerns about fairness may encompass
an absence of illegitimate bias, equity considerations in terms of accessibility and opportunity, treating people as
ends in themselves not merely as means, and procedural justice. The concept of fairness, which is a species of justice
[Rawls, 2009], is sometimes connected in AI Ethics to the values of transparency and accountability Jobin et al. [2019].
Transparency can refer to the degree to which the determinations or predictions of AI systems are revealed to relevant
parties in ways that those parties prefer and can understand, while accountability refers to the degree to which those
owning or deploying AI systems have or assume responsibility and/or liability for their outputs. Although transparency
is not necessarily or always an ethical value, it is associated with more basic ethical values such as justice and respect
for autonomy sufficiently frequently that it is often treated as a key ethical principle in AI Ethics.
Respect for autonomy, a widely prized modern value the prominence of which goes back to the philosopher Immanuel
Kant, unsurprisingly features both in AI ethics and in the philosophy of education Siegel et al. [2018]. Also holding
a prominent place in education is the value of academic integrity, which requires the preservation and nourishment
of conditions in which honest and genuine teaching and learning can take place. Non-maleficence is a principle
that cautions against doing harm to others. Privacy is relevant to AI ethics because new technologies often collect,
process, retain, and interpret vast amounts of often personal and sensitive data. Finally, the values of liberty and trust
are increasingly important in ethical discussions of and rising public concerns about the intersection of technology
and data gathering and surveillance [Zuboff, 2015]. Values or principles like privacy, fairness, respect for autonomy,
transparency, accountability, and trust are also clearly relevant to the treatment of students by universities. Furthermore,
some of these values and principles are also implicated in the civic responsibilities and cultural roles of universities.
As we shall see, these moral concepts help to illuminate the ethics of OP technologies.

3 Background
Digital technologies are used in education in a number of ways. Plagiarism detection tools like Turnitin are widely
available and have significantly increased the ability of instructors to uncover academic dishonesty and to teach good
academic practice to students. AI teaching systems are emerging that can adapt to the needs and learning styles
of individual pupils Bartneck et al. [2021]. More controversially, AI-based predictions of student performance have
been used as inputs into summative grades for students following exam cancellations during COVID-19 lockdowns
[Simonite, 2020]. Similarly, current circumstances have increased the attraction of using digital technologies for
remote proctoring of exams.
Examinations have a long history in both the West and the East. Written public examinations first took place in Imperial China. Centuries later, exams in academia became established in British universities [Kellaghan and Greaney,
2019]; their advent in the 1800s gave rise to the first institutional invigilators. Today’s proctors, who possess varying standards of professionalism and expertise [Rios and Liu, 2017], may also be employed by specialist agencies.
Proctors also support stressed students [Sloboda, 1990] and provide equitable exam environments. They are thus
required to meet some of the ethical obligations of educational institutions regarding fair and equitable academic
assessment. Although not all instructors use exams as an assessment technique, exams still enjoy wide support in
teaching [Butler and Roediger III, 2007] and are likely to persist in the foreseeable future. One reason for the ongoing
reliance on exams is that unlike coursework they can be readily invigilated. Instructors can therefore have greater
confidence that the work is the student’s own.
Recent technological advances have ushered in software that can be easily integrated into existing university learning
management systems and that can arguably assist or even replace live proctors. Reports of the exam session generated
by the technology can then be uploaded to a dashboard for convenient review. Although different OP platforms perform
broadly similar functions, they sometimes differ, e.g. in the extent of their functionality or in the manner of their use
of human invigilators from universities or, alternatively, OP companies. Given this variety and flexibility between and
within various OP platforms, we will content ourselves with describing below the more important and/or ubiquitous
features of OP technologies. Obviously, their capabilities may increase in time, potentially raising new ethical issues.
3.1 Monitoring and Control of Devices
Typically, students must download OP programs or install a web browser extension (which may be deleted postexam) and permit the commandeering of their computer’s microphone and camera. Different programs allow different
degrees of monitoring. They can variously capture screen images, access web page content, block browser tabs,
analyze keyboard strokes, and change privacy settings Norman [2020]..
3

A

PREPRINT

- N OVEMBER 17, 2020

3.2 Candidate Authentication
OP software can record IP addresses, names, and email addresses, and can request a password or ask other questions
to verify candidates’ identity. Programs typically require candidates to display an officially recognized ID card and
photo to be matched against their faces by a live proctor (or, conceivably, an AI algorithm). Some programs can
analyze the keystroke cadence of typed names to yield biometric substitutes for a handwritten signatures; one program
can even request biometrics like fingerprints [Examity, 2020]. Programs offering more ID data point checks may
improve reliability of authentication, while those offering fewer checks may be championed by purveyors as less
privacy intrusive.
3.3 AI-based and Human Online Proctoring
Online exam invigilation by algorithms and/or a person raise some of the strongest concerns. Examinees may be
prompted to activate their webcam and turn their device around 360° to “scan” the room for unauthorized materials
and family, friends, or housemates Examity [2020], Proctorio [2020]. Some programs can detect other devices like
mobile phones. The face and body of the candidate can also be monitored, either by means of automated or live human
proctoring.
Some AI algorithms can conduct voice and facial recognition but more commonly perform facial detection and analysis. Machine learning algorithms can be trained on thousands of video examples to recognize movements of eyes and
head that appear to correlate with suspicious behavior, like repeatedly glancing away from the screen. The OP system
then raises “red flags” that an authorized person can review to determine misconduct — either during the exam (allowing the human invigilator to immediately intervene) or afterwards. Some OP companies claim that the combination of
AI and trained human proctors provides greatest accuracy and reliability:
The exciting thing about innovating with machine learning technology is that our system is continuously learning, adapting and getting smarter with every exam. ProctorU’s goal in introducing AI into
proctoring is not to replace humans but, rather, to strengthen the accuracy of proctoring by assisting
humans in identifying details such as shadows, whispers or low sound levels, reflections, etc., that
may otherwise go unnoticed ProctorU [2020a].
Purveyors claim that well-designed AI can also mitigate human bias and error Proctorio [2020] and surpass the human
ability to accurately detect cheating. Video and audio recordings and analyses are typically stored for a period of
weeks or months on company-owned or other servers before being deleted.

4 Discussion
As can be seen from this overview, OP technologies have many automated capabilities and, in addition, can readily
facilitate remote human invigilation. We should stress that the ethical issues discussed in the present section may
pertain to some OP functions but not to others. Furthermore, the technology may give institutions discretion over
which capabilities are used. After discussing academic integrity, we examine fairness, non-maleficence, transparency,
privacy, autonomy, liberty, and trust as they apply to OP technology. We touch on accountability in the closing section.
Violations of one principle and its consequences can sometimes overlap with violations of other principles and their
consequences. For example, violations of privacy may cause certain harms and so also be instances of violations of
non-maleficence. Table 1 summarizes these values and principles and their possible implications for our case.
4.1 Academic Integrity
Academic integrity, a vital quality and value in academia, can be threatened by student ignorance, dishonesty, and
misconduct. Forms of academic dishonesty and misconduct include impersonation, unauthorized use of cheat notes,
and the copying of exam answers from fellow students or online sites. OP programs target all these illicit activities.
There are several ethical reasons why it is vital to prevent academic dishonesty [Kaufman, 2008], which we can
briefly enumerate. First, the value and viability of courses and universities depend on their academic integrity and
educational rigor. Second, permitting cheating is unfair on students who are academically honest. Third, knowledge
that others are cheating can create for honest students an invidious moral choice between self-interest (e.g. where class
rankings matter) and personal integrity, and a hurtful sense of both being taken advantage of by fellow students and
let down by the university. Fourth, universities arguably bind themselves to providing students with (in some sense) a
moral education alongside an intellectual education, minimally by nourishing a favorable academic culture in which
academic integrity is salient [Dyer et al., 2020].
4

A

PREPRINT

- N OVEMBER 17, 2020

Value/Principle

Implications for OP exam technology

Academic integrity

Ensuring academic honesty, rigor, excellence, and institutional reputation

Fairness

Equitable access to technology and remote exam settings
Equal, not biased or discriminatory, determination of cheating

Transparency

Transparent use and explanation of the nature of the technology and its selected functions
Transparent use of AI-based “red flags”

Non-maleficence

Effective and safe application of the technology which does not cause harm to the
subject

Privacy

Privacy in collection and security of personal data and exposure of body, behavior, and
home spaces

Respect for autonomy

Examinee autonomy regarding personal data use, use of AI, video recordings,
strangers as proctors

Liberty and trust

Potential wider effects on freedoms, use of digital technologies, and society’s trust in
AI, universities, etc.

Accountability

Accountability by the entity using the technology for misuse and processes for individuals to contest wrongful outcomes
Table 1: Ethical values/principles and their implications for OP exam technology

Although universities rightly encourage in students an autonomous and sincere commitment to honest behavior, failure
to invigilate where necessary to prevent cheating above a certain level can, in addition to other consequences, convey
the impression that academic honesty is unimportant. What that level is in any particular case requires a difficult
judgement. Yet although its effects may be hard to judge, it would be too quick to simply dismiss the idea that student
cheating can sometimes have corrosive effects on academic integrity and all that entails. Some studies suggest that
students more often cheat in online testing environments than traditional exam rooms [Srikanth and Asmatulu, 2014],
although there are conflicting views Stuber-McEwen et al. [2009]. Cheating may help students to achieve higher
grades in assessments, but it may also degrade their learning and longer-term interests. For all these reasons, both
universities and students have significant interests in the maintenance of academic integrity. Consequently, the need to
preserve academic integrity constitutes a non-trivial reason for considering the use of OP technologies.
4.2 Fairness — Equity and Accessibility in AI use
Remote invigilation via OP technologies promises accessibility benefits for students who normally study on campuses.
Institutions may save on costs of hiring exam centers and professional invigilators, which is important in the face
of severe budgetary constraints exacerbated by COVID-19 lockdowns1. OP can facilitate a greater range of online
course offerings and benefit students from distant places or with limited mobility. The technology allows exams to
be scheduled day or night, which could particularly benefit parents. For such reasons, OP may help promote more
equitable outcomes, including for traditionally excluded social groups.
However, some students may be disadvantaged by OP in certain ways. This includes students who lack reliable
internet connections — an online exam may be voided if the internet even momentarily disconnects. Some students
lack appropriate devices, such as web cameras, and home or other environments in which to sit exams. To be fair, OP
providers largely appear eager to ensure that their programs are executable on numerous devices and do not require
super-fast internet connections. Also, universities may loan devices and arrange for select students to sit exams oncampus or at other locations. A potential drawback in some such cases is that doing an exam later than the rest of the
cohort may delay course progression or graduation. Hence, there are logistical issues, with fairness implications, for
institutions to consider.
4.3 Fairness — Bias, Discrimination, and AI-facilitated Determination of Cheating
OP platforms using AI and/or live proctors may increase fairness for honest students by identifying relatively more
cases of cheating. Some proponents claim that digital proctoring does better on this score than traditional invigilation
1

Note however that OP technology can cost many thousands of dollars for institutions [Grajek, 2020].

5

A

PREPRINT

- N OVEMBER 17, 2020

where the ratio of proctors to test-takers is very low Dimeo [2017]. This claim, of course, would need to be backed by
empirical studies. In addition, the potential for OP’s to create unfairness also needs to be considered. Unfairness can
relate to inequitable outcomes and unjust processes. This may play out in different ways. For example, false negative
identification of cheating may constitute unfairness for non-cheating students, while false positives may result in
unfairness for those examinees. Unfairness may flow from the use of AI, from remote human invigilators, or from
both together. This needs to be spelt out.
The general fairness problems created by the use of AI and ML are the subject of vigorous contemporary public and
academic discussion in AI ethics. Deployment of ML has starkly exposed its potential for inaccuracy. For example,
facial recognition technology has been criticized as inaccurate, and has even resulted in legal action, despite the fact
that the ML algorithms may have been trained on thousands or millions of images Peters [2020]. Energetic debate has
similarly centered on the so-called biases that can afflict ML. Again, facial recognition software has been associated
with bias in the (mis)recognition of certain racial groups and gender Buolamwini and Gebru [2018]. Notorious cases
of inaccuracy and bias in ML include automated reviews of curriculum vitae for job applications which favor male
candidates, determinations of parole conditions for offenders which apparently discriminate against people of color,
and the disproportionate allocation of policing to disadvantaged communities O’Neil [2016]. Thus, machine bias can
create unfairness Jobin et al. [2019]. Another form of bias or discrimination may arise through the model used by
the OP for “normal” or “acceptable” exam behavior. OP providers refer to flagging suspicious gestures and even
tracking eye movements. Yet people with disabilities or neuro-atypical features may not always behave in a way that
is recognized by these processes [Swauger, 2020], and this may lead to false positives as such people are red flagged
for cheating through the manifestation of their normal behaviors.
Bias can creep into ML through input of skewed and poorly representative training data or through the mechanisms
of pattern-searching Mehrabi et al. [2019]. Presumably, this could occur in the training and operation of ML in online
cheating analysis. As OP platforms accumulate increasingly larger data sets on which to train, their reliability should
increase. But bias and inaccuracy may never be fully eliminated, and some forms of unfairness may not be solvable
by purely technical means [Selbst et al., 2019], leaving the potential for students to be unfairly charged with cheating.
Nevertheless, unfairness in socio-technical systems need not always be the outcome of ML bias. OP companies
stress that it is, after all, not the AI algorithm that ultimately makes a judgement about academic dishonesty, but a
knowledgeable human being, such as the course instructor. Furthermore, instructors may choose which settings they
will and won’t use — for instance, they might choose to disable or ignore AI algorithms that track eye movements.
While true, this flexibility does not totally eliminate ethical concerns. For example, instructors may have unwarranted
faith in the red flags, such as the automated flagging of “suspicious” head movements. The problem is magnified when
we consider the conscious and unconscious inclination for some people to over-trust AI [Dreyfus et al., 2000]. Even
where psychological bias is absent, instructors may be unsure how to interpret some red flags and may draw incorrect
inferences from them. Certain flagged events, such as when the test-taker is replaced with another person, are relatively
easy to assess. But more subtle flags may be much harder to appraise, such as flags for “low audible voices, slight
lighting variations, and other behavioral cues” [ProctorU, 2020b]. Further, if the ML element is intended to enhance
detection of cheating over and above a human observer carefully attending to the same images (etc.), then it follows
that an independent level of trust is intended to be invested in the AI assessment. As ML technology advances, greater
epistemic weight will likely be placed on its judgements.
4.4 Non-maleficence
Reliance on OP technology raises risks of harm for both students and universities. As mentioned, there is a risk to
students of false claims of cheating that did not occur. Wrongful allegations of academic misconduct, especially where
there is no process for contestability, may affect job prospects, self-confidence, and personal trust in the university. For
universities, false negatives and positives could more broadly undermine social trust in the integrity of the institution2 .
Therefore, it is important that such systems are effective, that they work with a sufficient degree of accuracy, and
that there is clarity about their reliability. But, as we have noted, the operation of such systems is often opaque, and
although claims are made about accuracy, the OP websites rarely or never cite rigorous studies to justify their claims
and to eliminate concerns about false positives; e.g. Examity [2020], Proctorio [2020].
One could imagine hypothetical situations that clearly involve unfairness and harm related to assumed belief, group
membership, or behaviour. Suppose, for example, that some future AI proctoring system red flags the presence in
the examinee’s room of white supremacist propaganda or pornographic material; or suppose the AI system is biased
towards red-flagging suspicious eye movements in people with disabilities, or assumes that black students need closer
2

We discuss trust further below.

6

A

PREPRINT

- N OVEMBER 17, 2020

monitoring than white students. Again, imagine the program casts doubt on students’ honesty purely from a brief
unintelligible exchange of words with someone who happens to enter the room.
Whether AI-led or human-led, post-exam determinations of cheating differ from in-person or live remote invigilation,
where the primary anti-cheating mechanism is typically to warn students at the precise time of the potential infraction
(e.g. when students are seen conversing). Unlike subsequent review of captured OP data, that mechanism does not
depend on an official charge of academic dishonesty, but on its immediate prevention. Some test-takers may simply
have idiosyncratic exam-taking styles, or disabilities and impairments, that trigger specious AI red flags. Even falsely
suggesting that these individuals are academically dishonest, let alone accusing and penalizing them, would potentially
be unfair and harmful. Even though such a false suggestion or imputation is less morally serious than a false official
condemnation, they are still morally serious. Further, recipients of spurious insinuations are likely to receive them
as an injustice and to feel corresponding hurt. In addition, such individuals, in an effort to avoid this potentially
wrongful treatment, may be forced to disclose personal idiosyncrasies or impairments, compromising their privacy
and potentially doing them harm in the process.
4.5 Transparency
Uncertainty may persist about how precisely the AI identifies “cheating behavior.” Some OP websites are more
transparent than others about how their AI systems work. But even with some explanation it can be confusing and
difficult to gain an adequate understanding of how they compute red flags and how reliable those determinations are3 .
One representative explains that their company uses an:
incredibly futuristic AI Algorithm that auto-flags a variety of suspicious cases with 95%+ accuracy
. . . With AI-driven proctoring, the algorithms will soon become trained enough to prevent cheating
100%, a guarantee that a physical invigilator cannot always promise Kanchan [2019].
Compared to, perhaps, the plagiarism detection tool Turnitin, proctoring AI may strike users as highly opaque
[Castelvecchi, 2016]. The problem of AI “black boxes” is one reason why ethicists stress the moral need for transparency in AI [Reddy et al., 2020]. Transparency in this context may work at different levels and different times. At
the outset, students need to understand enough about the OP process to know what is expected of them in exams so as
not to trigger a red flag. Students will also need information on how to contest any adverse finding and their rights of
appeal, and those who are accused of cheating will need to know the basis on which that allegation is made.
Admittedly, not all of this information need be presented to students upfront, especially given concerns about information overload and about students gaming the system. Nonetheless, academic fairness requires that the evidence and
procedures on which accusations of cheating are made are defensible and transparent. To reduce the risks of unfairness
and emotional harm, OP companies and universities should be transparent about how the technology works, how it
will be used in particular circumstances, and how it will impact on students, including those with idiosyncrasies and
disabilities.
4.6 Privacy
OP technologies raise moral concerns about privacy which privacy laws, and university policies and governance, may
not adequately address [Pardo and Siemens, 2014] — especially given that many jurisdictions have privacy laws that
have not been amended to adjust to the data collecting capacities of new digital technologies [Government, 2020]. OP
technologies collect a number of kinds of data, including the capturing and storage of information from devices, the
gathering of biometric and other ID details, and the audio and video recording of students and their environment. It
should not be surprising that some students have a sense of “Big Brother invading their computers” [Dimeo, 2017].
Privacy is a large philosophical topic. A rough distinction can be made between private and public domains
[Warren and Brandeis, 1890]. Privacy can relate to the (non)exposure to other individuals of one’s personal information and one’s body, activities, belongings, and conversation [Gavison, 1980, Moore, 2003]. However, what is
private for one person may, in a recognizable sense, not be private for another [Moore, 2015]. For example, I may
strongly prefer that no strangers gaze inside my bedroom and watch me studying; whereas you, who do not draw your
curtains, may not care. Exposure of my bedroom and activities to passers-by represents in my case a loss of privacy;
in your case it does not. So, there is an intelligible sense in which the determination of privacy and its breaching can
turn partly on individual perspectives about the personal. At the very least, we can say that the moral seriousness of
exposure is plausibly related, to some extent, to these individual preferences.
3
For one example of an attempt to explain how AI-based judgements are made using a “credibility index”; see Mettl [2018].
This program allows users to select which indicators or patterns (e.g. eye movements) to incorporate and which to exclude from the
AI analysis, as well as the weight they carry.

7

A

PREPRINT

- N OVEMBER 17, 2020

While the moral “right” to privacy may sometimes justifiably be infringed (e.g. in law enforcement) it is still a vital
right. For some philosophers, privacy’s value essentially reduces to the value of liberty and autonomy [Thompson,
1979], i.e. to a person’s ability to act and make choices about their own lives. For others, its importance relates to
possible harms resulting from public exposure of the personal [Rachels, 1975], such as social embarrassment and
deleterious financial or employment repercussions. We might regard privacy’s importance not as confined to a single
philosophical conception, but to a range of conceptions that cover respect for autonomy, the causation of various kinds
of harm, and so on.
OP technologies may threaten personal privacy in several ways. Reports exist of inadvertent capture of personal or
sensitive information, such as in one case a student’s credit card details that were accidentally displayed on their
computer screen [Chin, 2020]. While technology designers might address some such risks, there are additional risks
concerning data security. Captured information can be stored in encrypted form on host servers such as Microsoft and
Amazon servers. For their part, many OP companies claim that they have no access to this encrypted information and
therefore cannot view video recordings or obtain sensitive personal data. Furthermore, purveyors claim to be compliant
with legal protections like the EU’s General Data Protection Regulation (GDPR), which carries heavy penalties for
breach.
Companies may also have internal rules against sharing data with third parties and for commercial gain [Dennien,
2020], and universities too are required to have stringent cybersecurity and privacy policies. However, there can be
no absolute guarantee against leakage of data or successful cyberattacks on servers used by companies or universities.
The maintenance of such privacy is never completely certain: these kinds of cyber risks are always present with any
data collected by any institution [ANU, 2019]. It is nonetheless possible that students may feel particularly anxious
about the possible loss of the kinds of sensitive personal information (e.g. video recordings, certain data from personal
computers) collected by OP technologies.
As we saw, some people worry that OP platforms are especially intrusive because they readily facilitate video (and
audio) capture of examinees and its live or subsequent review by a person. This concern may be countered by proponents of OP technologies as follows. Students necessarily relinquish aspects of their privacy in education. In-person
invigilation, which is morally uncontroversial, is already privacy-invasive: strangers or instructors watch students like
hawks, scrutinizing their activities and personal belongings. On this moral view, online proctoring is essentially the
same in an ethical sense as in-person invigilation. We may start by noting that students who have used Examity say
that
“it feels much weirder than proctoring with a professor . . . They’re being watched closer up, by a
stranger, and in a place more private than a classroom. . . students described their experiences as
everything from ‘uncomfortable’ to ‘intrusive’ to ‘sketchy”’ [Chin, 2020].
One element of the privacy intrusion relates to human invigilators seeing into the home environment of the student,
such as bedrooms or lounge rooms. Another element is that of other human beings watching video of the faces and
upper bodies of students themselves. To make the analogy with traditional invigilation more truly comparable, then,
we must imagine an in-person supervisor sitting near the examinee and staring at them throughout the exam. Such
observation would include scrutinizing the student’s expressions or micro-expressions, perhaps with the help of an AI
device.
Furthermore, OP may allow the human invigilator, who may reside locally or on the other side of the world, to re-watch
the video and use its pause function, potentially in private. In traditional exam rooms, the presence of other students
and instructors provides a degree of security and protection. In contrast, the student who is invigilated online cannot
know, even when they are given assurances by universities and OP companies, how the online human proctor uses the
video. For example, students cannot be sure that they are not being leered at that online proctors have not shared their
images shared with third parties. The online scenario should strike us as potentially more invasive of privacy than
in-person invigilation, irrespective of whether students — including students whose histories and psychologies render
them particularly averse to being closely watched by strangers — have the additional concern that viewers may take
a prurient interest in them. Besides, some students evidently have that view. Further, because (as we suggested) what
constitutes a loss of privacy turns in one sense partly on the individual’s own perspective, OP in those cases is more
invasive of privacy. It follows that the hurdle for justifying its use is that much higher.
4.7 Respect for Autonomy
Autonomy might be restricted by online proctoring in a number of ways. For example, it may require students to avoid
doing things they can often do in traditional exams, such as muttering to themselves, looking to the side, going to the
bathroom, etc. — lest they raise automated red flags about suspicious behavior. Some students may simply prefer not
8

A

PREPRINT

- N OVEMBER 17, 2020

to be invigilated by AI or by online human proctors, or to have their images and personal data collected and viewed.
Philosophers often regard respect for autonomy as a fundamental ethical value [Christman, 2018]. Autonomy in this
sense implies self-governance, or the ability of a rational and mature agent to form and act on decisions and personal
values free of compulsion and manipulation. As we saw, some philosophers ground the value of privacy in the value
of autonomy.
We should be clear, however, that respect for autonomy is not reducible to respecting privacy: respect for autonomy can
apply to the use of personal information even where loss of privacy is not at stake (e.g. because data is anonymized).
Further, respecting autonomy may require providing agents with opportunities for informed consent. For informed
consent to apply, the choice must be made voluntarily and exercised with liberty and without coercion, and the chooser
must have adequate knowledge of the nature, risks, and benefits of committing to or refraining from the relevant action
(cf. Clifford and Paterson [2020]). This requires transparency about the nature and potential effects of OP programs.
A genuinely robust standard of consent would also allow students to be able not to consent to OP and to choose
instead a human invigilator. From the perspective of a university, this kind of discretion granted to students may be
unmanageable; but the point emphasizes the need for other processes to protect students’ interests.
It might be observed that autonomy, and the prima facie requirement for informed consent, are already justifiably
restricted in education. Educational limitations on liberty extend, quite obviously, to the prevention of cheating and
much more (as when personal student information is collected for enrolment). One early student criticism of the
plagiarism tool Turnitin likened its use to the coercive drug testing of students [Glod, 2006], but such moral objections
are now often (though not universally) regarded as exaggerated. Indeed, our attitudes towards novel technologies can
change with increased familiarity and understanding. However, deciding when it is justified to limit autonomy for the
sake of academic integrity requires moral (and not just legal) judgement.
Most ethicists, including that classic defender of liberty John Stuart Mill, acknowledge that coercion and compulsion
are sometimes justified, most obviously when the exercise of a freedom is likely to result in significant harm to others
[Gaus et al., 2020, Mill, 1966]. But even though and when that is so, respect for autonomy may imply that limitations
upon autonomy be minimized wherever possible, and that relevant information be provided transparently to students
who are being compelled by their universities. This would include information related to the above concerns, along
with other concerns. For example, purveyors of OP technologies may use data derived from student exams to train
ML algorithms [ProctorU, 2020b] without the students being (adequately) informed that their data will be so used.
Such use may arguably produce good outcomes (e.g. improving accuracy and reducing bias in AI proctoring), but
institutions which fail to investigate data use arrangements and/or inform students accordingly could be said to thereby
disrespect the autonomy of those students.
4.8 Liberty and Trust
To conclude our investigation of the ethical issues raised by OP technologies, we shall briefly discuss some possible
wider ethical risks. Although these risks are admittedly much less certain than those explored above, they are real
enough to warrant taking into account when forming a comprehensive ethical judgement about this emerging sociotechnical example. The above concerns about the intrusive and invasive nature of OP technologies have a possible
connection to broader technological and social changes and trajectories. These trajectories may include increased
surveillance and the step-by-step evolution of a security state [Reiman, 1995], new risks of personal data being publicly
“reidentified” despite claims it is anonymised [Culnane et al., 2017], a constriction of the private domain [Nissenbaum,
2009], and the spread of AI-based decision making in ways that some consider problematic. Consider, for example,
contemporary public perceptions arising from the use of AI facial recognition, the scraping and dubious employment
of personal data from social media, tracking and tracing during the COVID-19 pandemic, AI decision-making in
jurisprudence, and so on [Feldstein, 2019]4 .
It is at least arguable that OP technology could (even if modestly) contribute to these worrying social trajectories or
possibilities. Such risks are, as we stress, very difficult to assess, but that does not mean they may be ignored. Of course,
some universities have chosen not to use OP technology. Furthermore, we can perhaps expect that many universities
will make diligent efforts to protect and foster respect for privacy, liberty, autonomy, and fairness [Kristjánsson, 2017]
when they use these technologies. Such efforts would, of course, be entirely proper. Indeed, it may be suggested
(though we cannot argue it here) that universities should recognize and reaffirm their standing as bulwarks against the
natural proclivity of governments and powerful corporations to intrude into people’s private lives and to chip away,
deliberately or unthinkingly, at their freedoms. Some students and university staff evidently feel that OP platforms
4

For example, a website for “Proctortrack” says that its “RemoteDesk” solution goes beyond exam proctoring to provide automated monitoring of people who work from home” [Kanchan, 2019].

9

A

PREPRINT

- N OVEMBER 17, 2020

could damage a university’s “culture of trust.” Such an effect could have wider reverberations. That is a reason for
taking the ethical aspects of OP technologies seriously.
The weight of the above concerns will depend not only on cultural factors and differences but also partly on factors such
as the extent of opposition to OP technology amongst students and staff, and the relative intrusiveness of the various
proctoring functions which are developed by companies and chosen by institutions. We have already discussed a range
of more or less “invasive” and “intrusive” capabilities that, for some people, have vaguely Big Brother overtones.
Disquiet would mount still further if OP platforms allowed, say, facial recognition, the undisclosed on-selling of testtaker data, and use of AI to generate Uber-style ratings to indicate an examinee’s honesty while closing off avenues
for appeal and rebuttal.
In today’s digital and cultural climate, none of these further possibilities may be dismissed out of hand. At some point
along this line, universities may, in light of their social responsibilities, want to take a stand against not only the ethical
risks to students and to their own reputations, but against the risks created for society more generally of endorsing
particularly invasive or intrusive technologies. In any case, the point we are underlining here is that OP technologies
need to be considered not just from the perspective of their potential immediate and local effects, but also from the
perspective of their more distant, wider, and longer-term potential effects, even if those effects are much harder to
measure and predict with any certainty.

5 Conclusion: Justification and Accountability
Debate and disagreement about the appropriateness of remote OP technologies in distance education, and in circumstances like the ongoing COVID-19 pandemic, is bound to continue. As we saw, there are considerations that speak in
favor of OP technologies despite their drawbacks. Indeed, it is fair to acknowledge here that in-person proctoring is
not ethically perfect either: it can both miss cheating and similarly result in unfair accusations of academic dishonesty.
Furthermore, we have accepted that it is vital to maintain academic integrity to protect both students and institutions.
It is true that the pedagogical value of high-stake examinations is sometimes questioned5; and faced with the ethical
problems of OP technology some academics will adopt alternative assessments. But, on the assumption that highstake exams have value and will persist in education, there are reasons for regarding at least some OP technologies
and capabilities as representing acceptable “proctors” or proctors’ assistants.
Nonetheless, the above analysis revealed that OP platforms raise ethical concerns over-and-above those affecting
live and in-person exam invigilation. These concerns include an uncertain risk of academic unfairness associated
with AI-informed judgement, further diminution of student privacy and autonomy, and (perhaps) increased distrust
towards institutions that are bastions of social values. Another fear, partially dependent on these former fears, is that
OP platforms could contribute to the social trajectories of growing surveillance, liberty and privacy loss, mining of
massed personal data, and dubious instances of AI decision-making.
It is difficult to know whether the benefits of OP technologies outweigh their risks. The most reasonable conclusion we
can reach at the present time is that the ethical justification of OP technologies and their various capabilities requires
balancing as best we can the concerns with the possible benefits. In any case, it behooves educational institutions to
consider the ethical considerations we have explored in this paper, and, should they choose to adopt OP technologies,
to accommodate those considerations in their policies and governance plans. Those institutions need to have the right
systems in place to remain accountable for such choices.
Accountability as a value and principle would require that students are adequately and transparently informed about the
impacts of the capabilities of any particular OP technology that is selected on the ways in which cheating is determined
and privacy potentially affected. Accountability would also require systems for addressing potential injustices. Future
empirical investigations may further illuminate the effects of OP technologies and their widespread uptake on issues
like fairness, privacy, harm, autonomy, trust, liberty, and accountability.
Funding

The authors received no funding for this paper.

Conflicts of interest/Competing interests Nil
Availability of data and material N/A
5
Grajeck reports: “Three in ten institutions are considering broad changes to assessment. Exams are common, but they are only
one way to assess learning. The [COVID-19] pandemic is providing 31% of institutions the opportunity to consider more authentic
demonstrations of knowledge and skills” [Grajek, 2020].

10

A

PREPRINT

- N OVEMBER 17, 2020

Code availability N/A
Declaration of interest No conflict of interest exists in the submission of the manuscript. The work described is
original research that has not been published previously and not under consideration for publication elsewhere, in
whole or in part.

References
ANU.
ANU releases detailed account of data breach,
October 2019.
URL
https://www.anu.edu.au/news/all-news/anu-releases-detailed-account-of-data-breach.
Hadian SG Asep and Yoanes Bandung. A Design of Continuous User Verification for Online Exam Proctoring on
M-Learning. In 2019 International Conference on Electrical Engineering and Informatics (ICEEI), pages 284–289.
IEEE, 2019. ISBN 1-72812-418-2.
Christoph Bartneck, Christoph Lütge, Alan Wagner, and Sean Welsh.
An Introduction
to Ethics in Robotics and AI.
Springer Nature, 2021.
ISBN 3-030-51110-3.
URL
https://library.oapen.org/handle/20.500.12657/41303.
Jake Binstein.
How to Cheat with Proctortrack, Examity, and the Rest, 2015.
URL
https://jakebinstein.com/blog/on-knuckle-scanners-and-cheating-how-to-bypass-proctortrack/.
Joy Buolamwini and Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of Machine Learning Research, volume 81, page 15, 2018.
Andrew C. Butler and Henry L. Roediger III. Testing improves long-term retention in a simulated classroom
setting. European Journal of Cognitive Psychology, 19(4-5):514–527, July 2007. ISSN 0954-1446. doi:
10.1080/09541440701326097. URL https://doi.org/10.1080/09541440701326097.
Davide Castelvecchi. Can we open the black box of AI? Nature News, 538(7623):20, 2016.
Monica Chin.
Exam anxiety: how remote test-proctoring is creeping students out, April 2020.
URL
https://www.theverge.com/2020/4/29/21232777/examity-remote-test-proctoring-online-class-education.
John Christman. Autonomy in Moral and Political Philosophy. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, spring 2018 edition, 2018. URL
https://plato.stanford.edu/archives/spr2018/entries/autonomy-moral/.

Damian Clifford and Jeannie Paterson.
Consumer Privacy and Consent:
Reform in the Light
of Contract and Consumer Protection Law.
Australian Law Journal, 94(10), 2020.
URL
https://www-westlaw-com-au.eu1.proxy.openathens.net/maf/wlau/app/document?&src=search&docguid=I5872e2
Joshua Cramp, John F. Medlin, Phoebe Lake, and Colin Sharp. Lessons learned from implementing remotely invigilated online exams. Journal of University Teaching & Learning Practice, 16(1):10, 2019.
Chris Culnane, Benjamin IP Rubinstein, and Vanessa Teague. Health data in an open world.
arXiv:1712.05627, 2017.

arXiv preprint

Randall Curren, Peter Markie, and Gareth Mathews.
A Companion to the Philosophy of Education.
John Wiley & Sons, Incorporated, Hoboken, 2003.
ISBN 978-0-470-99723-9.
URL
http://ebookcentral.proquest.com/lib/unimelb/detail.action?docID=214150.

Matt Dennien. UQ students raise privacy concerns over third-party exam platform. Brisbane Times, 2020. URL
https://www.brisbanetimes.com.au/national/queensland/uq-students-raise-privacy-concerns-over-third-pa

Jean Dimeo.
Online exam proctoring catches cheaters, raises concerns, 2017.
URL
https://www.insidehighered.com/digital-learning/article/2017/05/10/online-exam-proctoring-catches-che
Hubert Dreyfus, Stuart E. Dreyfus, and Tom Athanasiou. Mind Over Machine. Simon and Schuster, New York, March
2000. ISBN 978-0-7432-0551-1. Google-Books-ID: e9W9m_4q4pYC.
Jarret M Dyer, Heidi Pettyjohn, and Steve Saladin. Academic Dishonesty and Testing: How Student Beliefs and Test
Settings Impact Decisions to Cheat. Journal of the National College Testing Association, 4(1):30, 2020.
Examity. Auto Proctoring, 2020. URL https://examity.com/auto-proctoring/.
Steven Feldstein. The global expansion of AI surveillance, volume 17. Carnegie Endowment for International Peace,
Washington DC, 2019.
Colleen Flaherty.
Online proctoring is surging during COVID-19,
2020.
URL
https://www.insidehighered.com/news/2020/05/11/online-proctoring-surging-during-covid-19.
11

A

PREPRINT

- N OVEMBER 17, 2020

Luciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph
Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy Vayena.
AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations. Minds and Machines, 28(4):689–707, December 2018. ISSN 1572-8641. doi: 10.1007/s11023-018-9482-5.
URL https://doi.org/10.1007/s11023-018-9482-5.
Gerald Gaus, Shane D. Courtland, and David Schmidtz. Liberalism. In Edward N. Zalta, editor, The Stanford
Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, fall 2020 edition, 2020. URL
https://plato.stanford.edu/archives/fall2020/entries/liberalism/.
Ruth Gavison. Privacy and the Limits of Law. The Yale Law Journal, 89(3):421–471, 1980.
Scott A. Ginder, Janice E. Kelly-Reid, and Farrah B. Mann. Enrollment and Employees in Postsecondary Institutions,
Fall 2017; and Financial Statistics and Academic Libraries, Fiscal Year 2017: First Look (Provisional Data). NCES
2019-021Rev. National Center for Education Statistics, 2019.
Maria
Glod.
Students
Rebel
Against
Database
Designed
to
Thwart
Plagiarists.
Washington
Post,
September
2006.
ISSN
0190-8286.
URL
http://www.washingtonpost.com/wp-dyn/content/article/2006/09/21/AR2006092101800.html.
Patricia A. Goedl and Ganesh B. Malla. A Study of Grade Equivalency between Proctored and Unproctored Exams in Distance Education. American Journal of Distance Education, 0(0):1–10, July 2020. ISSN 0892-3647.
doi: 10.1080/08923647.2020.1796376. URL https://doi.org/10.1080/08923647.2020.1796376. Publisher: Routledge _eprint: https://doi.org/10.1080/08923647.2020.1796376.
Carina S. González-González, Alfonso Infante-Moro, and Juan C. Infante-Moro. Implementation of E-Proctoring in
Online Teaching: A Study about Motivational Factors. Sustainability, 12(8):3488, January 2020. doi: 10.3390/
su12083488. URL https://www.mdpi.com/2071-1050/12/8/3488.
Australian
Government.
OAIC
welcomes
Privacy
Act
review,
2020.
URL
https://www.oaic.gov.au/updates/news-and-media/oaic-welcomes-privacy-act-review/.
Susan Grajek.
EDUCAUSE COVID-19 QuickPoll Results: Grading and Proctoring, 2020.
URL
https://er.educause.edu/blogs/2020/4/educause-covid-19-quickpoll-results-grading-and-proctoring.
Thilo Hagendorff.
The Ethics of AI Ethics: An Evaluation of Guidelines.
Minds and Machines, 30
(1):99–120, March 2020.
ISSN 0924-6495, 1572-8641.
doi: 10.1007/s11023-020-09517-8.
URL
http://link.springer.com/10.1007/s11023-020-09517-8.
Alex
Hern.
Ofqual’s
A-level
algorithm:
why
did
it
fail
to
make
the
grade?
The
Guardian,
August
2020.
ISSN
0261-3077.
URL
https://www.theguardian.com/education/2020/aug/21/ofqual-exams-algorithm-why-did-it-fail-make-grade-a
Shawn
Hubler.
Keeping
Online
Testing
Honest?
Or
an
Orwellian
Overreach?
The New York Times,
May 2020.
ISSN 0362-4331.
URL
https://www.nytimes.com/2020/05/10/us/online-testing-cheating-universities-coronavirus.html.
Anna Jobin, Marcello Ienca, and Effy Vayena. The global landscape of AI ethics guidelines. Nature Machine Intelligence, 1(9):389–399, September 2019. ISSN 2522-5839. doi: 10.1038/s42256-019-0088-2. URL
https://www.nature.com/articles/s42256-019-0088-2. Number: 9 Publisher: Nature Publishing Group.
Romila Kanchan.
Top 5 Proctoring Solution and Service Providers, February 2019.
URL
https://blog.mettl.com/top-5-proctoring-solution-providers/.
Heather E. Kaufman. Moral and ethical issues related to academic dishonesty on college campuses. Journal of College
and Character, 9(5), 2008.
Thomas Kellaghan and Vincent Greaney. A Brief History of Written Examinations. In Public Examinations Examined,
pages 43–74. The World Bank, November 2019. ISBN 978-1-4648-1418-1. doi: 10.1596/978-1-4648-1418-1_ch3.
URL https://elibrary.worldbank.org/doi/full/10.1596/978-1-4648-1418-1_ch3.
Carly Kind.
The term ‘ethical AI’ is finally starting to mean something, August 2020.
URL
https://venturebeat.com/2020/08/23/the-term-ethical-ai-is-finally-starting-to-mean-something/.
Kirsty Kitto and Simon Knight. Practical ethics for building learning analytics. British Journal of Educational
Technology, 50(6):2855–2870, 2019.
Kristján Kristjánsson. Emotions targeting moral exemplarity: Making sense of the logical geography of admiration,
emulation and elevation. Theory and Research in Education, 15(1):20–37, 2017.
Patrick Lin, Keith Abney, and Ryan Jenkins. Robot ethics 2.0: From autonomous cars to artificial intelligence. Oxford
University Press, New York, 2017. ISBN 0-19-065295-0.
12

A

PREPRINT

- N OVEMBER 17, 2020

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A Survey on Bias and Fairness in Machine Learning. arXiv:1908.09635 [cs], September 2019. URL http://arxiv.org/abs/1908.09635.
arXiv: 1908.09635.
Mettl.
Mettl ProctorPlus - Experience the Real Power of AI, 2018.
URL
https://www.youtube.com/watch?v=_YPu9X3TCXY&feature=youtu.be.
John Stuart Mill. On Liberty. In John Stuart Mill and John M. Robson, editors, A Selection of his Works, pages 1–147.
Macmillan Education UK, London, 1966. ISBN 978-1-349-81780-1. doi: 10.1007/978-1-349-81780-1_1. URL
https://doi.org/10.1007/978-1-349-81780-1_1.
Adam D. Moore. Privacy: its meaning and value. American Philosophical Quarterly, 40(3):215–227, 2003.
Adam D. Moore. Privacy, Security and accountability: ethics, law and policy. Rowman & Littlefield, London, 2015.
ISBN 1-78348-477-2.
Deirdre K. Mulligan, Joshua A. Kroll, Nitin Kohli, and Richmond Y. Wong. This Thing Called Fairness: Disciplinary Confusion Realizing a Value in Technology. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):1–36, November 2019. ISSN 2573-0142, 2573-0142. doi: 10.1145/3359221. URL
http://arxiv.org/abs/1909.11869. arXiv: 1909.11869.
Helen Nissenbaum. Privacy in context: Technology, policy, and the integrity of social life. Stanford University Press,
Stanford, 2009. ISBN 0-8047-7289-4.
D’Arcy Norman. Online Exam Proctoring, 2020. URL https://darcynorman.net/2020/03/31/online-exam-proctoring/.
Cathy O’Neil. Weapons of math destruction: How big data increases inequality and threatens democracy. Broadway
Books, London, 2016. ISBN 0-553-41883-1.
Abelardo Pardo and George Siemens. Ethical and privacy principles for learning analytics. British Journal of Educational Technology, 45(3):438–450, 2014.
Jay Peters. IBM will no longer offer, develop, or research facial recognition technology, June 2020. URL
https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysisProctorio. A Comprehensive Learning Integrity Platform, 2020. URL https://proctorio.com/.
ProctorU.
Harnessing the Power of Artificial Intelligence to Improve Online Proctoring, 2020a.
URL
https://www.proctoru.com/harnessing-the-power-of-artificial-intelligence.
ProctorU.
ProctorU - The Leading Proctoring Solution for Online Exams, 2020b.
URL
https://www.proctoru.com/.
James Rachels. Why privacy is important. Philosophy & Public Affairs, 4(4):323–333, 1975.
John Rawls. A theory of justice. Harvard university press, Cambridge, MA, 2009. ISBN 0-674-04258-1.
Sandeep Reddy, Sonia Allan, Simon Coghlan, and Paul Cooper. A governance model for the application of AI in
health care. Journal of the American Medical Informatics Association, 27(3):491–497, 2020.
Jeffrey H. Reiman. Driving to the panopticon: A philosophical exploration of the risks to privacy posed by the highway
technology of the future. Santa Clara Computer & High Tech. LJ, 11:27, 1995.
Joseph A. Rios and Ou Lydia Liu. Online proctored versus unproctored low-stakes internet test administration: Is
there differential test-taking behavior and performance? American Journal of Distance Education, 31(4):226–241,
2017.
Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. Fairness and
Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, pages 59–68, New York, NY, USA, January 2019. Association for Computing Machinery. ISBN
978-1-4503-6125-5. doi: 10.1145/3287560.3287598. URL https://doi.org/10.1145/3287560.3287598.
Harvey Siegel, D.C. Phillips, and Eamonn Callan. Philosophy of Education. In Edward N. Zalta, editor, The Stanford
Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, winter 2018 edition, 2018. URL
https://plato.stanford.edu/archives/win2018/entries/education-philosophy/.
Tom Simonite.
Meet the Secret Algorithm That’s Keeping Students Out of College, 2020.
URL
https://www.wired.com/story/algorithm-set-students-grades-altered-futures/.
John A. Sloboda. Combating examination stress among university students: Action research in an institutional context.
British Journal of Guidance & Counselling, 18(2):124–136, 1990.
M. Srikanth and R. Asmatulu. Modern Cheating Techniques, Their Adverse Effects on Engineering Education and
Preventions. International Journal of Mechanical Engineering Education, 42(2):129–140, April 2014. ISSN 03064190. doi: 10.7227/IJMEE.0005. URL https://doi.org/10.7227/IJMEE.0005.
13

A

PREPRINT

- N OVEMBER 17, 2020

Donna Stuber-McEwen, Phillip Wiseley, and Susan Hoggatt. Point, Click, and Cheat: Frequency and Type of Academic Dishonesty in the Virtual Classroom. Online Journal of Distance Learning Administration, 12(3), September
2009. URL https://www.westga.edu/~distance/ojdla/fall123/stuber123.html.
Shea Swauger. Our Bodies Encoded: Algorithmic Test Proctoring in Higher Education, April 2020. URL
https://hybridpedagogy.org/our-bodies-encoded-algorithmic-test-proctoring-in-higher-education/.
Mack Sweeney, Jaime Lester, and Huzefa Rangwala. Next-term student grade prediction. In 2015 IEEE International
Conference on Big Data (Big Data), pages 970–975, October 2015. doi: 10.1109/BigData.2015.7363847.
Ian E. Thompson. The nature of confidentiality. Journal of medical ethics, 5(2):57–64, 1979.
Brunna Torino.
Data Privacy vs. European Universities: Online Proctoring, June 2020.
URL
https://medium.com/@brunnavillar/data-privacy-vs-european-universities-online-proctoring-be38e64fe080
Samuel D. Warren and Louis D. Brandeis. The right to privacy. Harvard law review, 4(5):193–220, 1890.
Nic White. ’Creepy’ software to stop university students cheating in online exams. Daily Mail Australia, April 2020.
URL https://www.dailymail.co.uk/news/article-8243637/Creepy-software-used-stop-university-students-ch
Naaman Zhou. Students alarmed at Australian universities’ plan to use exam-monitoring software. The Guardian,
2020. URL https://www.theguardian.com/australia-news/2020/apr/20/concerns-raised-australian-universiti
Shoshana Zuboff. Big other: surveillance capitalism and the prospects of an information civilization. Journal of
Information Technology, 30(1):75–89, 2015.

14

