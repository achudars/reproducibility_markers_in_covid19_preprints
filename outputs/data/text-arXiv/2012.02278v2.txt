arXiv:2012.02278v2 [eess.IV] 8 Jan 2021

M ULTISCALE ATTENTION G UIDED N ETWORK FOR COVID-19
D IAGNOSIS U SING C HEST X- RAY I MAGES

Jingxiong Li
Key Lab of RF Circuits and Systems of Ministry of Education
Microelectronics CAD Center
Hangzhou Dianzi University
Hangzhou 310018, China
jingxiong.li2019@outlook.com
Shuai Wang
Department of Radiology and BRIC
University of North Carolina at Chapel Hill
Chapel Hill NC 27599, USA
shuaiwang.tai@gmail.com
Jun Liu
Key Lab of RF Circuits and Systems of Ministry of Education
Microelectronics CAD Center
Hangzhou Dianzi University
Hangzhou 310018, China
ljun77@hdu.edu.cn

Yaqi WangB
Communication University of Zhejiang
Hangzhou, China
wangyaqi@hdu.edu.cn

Jun Wang
School of Biomedical Engineering
Shanghai Jiao Tong University
Shanghai, 200240, China

Qun Jin
School of Biomedical Engineering
Shanghai Jiao Tong University
Shanghai, 200240, China
jin@waseda.jp

Lingling Sun
Key Lab of RF Circuits and Systems of Ministry of Education
Microelectronics CAD Center
Hangzhou Dianzi University
Hangzhou 310018, China
sunll@hdu.edu.cn

January 11, 2021

A BSTRACT
Coronavirus disease 2019 (COVID-19) is one of the most destructive pandemic after millennium,
forcing the world to tackle a health crisis. Automated lung infections classification using chest X-ray
(CXR) images could strengthen diagnostic capability when handling COVID-19. However, classifying COVID-19 from pneumonia cases using CXR image is a difficult task because of shared spatial
characteristics, high feature variation and contrast diversity between cases. Moreover, massive data
collection is impractical for a newly emerged disease, which limited the performance of data thirsty
deep learning models. To address these challenges, Multiscale Attention Guided deep network with
Soft Distance regularization (MAG-SD) is proposed to automatically classify COVID-19 from pneumonia CXR images. In MAG-SD, MA-Net is used to produce prediction vector and attention from multiscale feature maps. To improve the robustness of trained model and relieve the shortage of training
data, attention guided augmentations along with a soft distance regularization are posed, which aims
at generating meaningful augmentations and reduce noise. Our multiscale attention model achieves
better classification performance on our pneumonia CXR image dataset. Plentiful experiments are

A PREPRINT - JANUARY 11, 2021

proposed for MAG-SD which demonstrates its unique advantage in pneumonia classification over
cutting-edge models. The code is available at https://github.com/JasonLeeGHub/MAG-SD.
Keywords COVID-19 · X-ray Radiology · Multiscale Attention · Convolutional Neural Network

1

Introduction

The coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)
is one of the most devastating infectious diseases after millennium [1]. This new type of coronavirus is announced in
late December, 2019, then spread globally in 2020. It has been declared as a pandemic by World Health Organization
(WHO) according to its high contagiosity and unprecedented pressure bought to public healthcare system [2]. The
current gold-standard for screening COVID-19 is polymerase chain reaction (PCR) laboratory test, however, the test
capacity is extremely limited and requires professional equipment [3]. [4] also reports that PCR tests suffers from high
false negative rate.
Radiological images collected by X-ray and computed tomography (CT) are important complements to PCR tests.
The virus leads to pneumonia, which is an inflammatory condition of the lung’s air sacs [5]. Radiological signs show
ground-glass opacity, airspace opacities and later consolidation with bilateral, peripheral, and lower zone predominant
distributions [6]. Comparing with CT imaging, CXR diagnosis provides a low-cost and time-saving diagnosis method [7].
Besides, underdeveloped regions can hardly have sufficient CT scanners, making CT based COVID-19 screening
impossible. X-rays are the most common diagnostic imaging equipment available even in rural regions, which means
X-ray diagnosis can cover larger susceptible population [8].
Diagnosis accuracy of COVID-19 and radiography based infection localization are critical for treatment planning
and follow-up evaluations [9]. However, pressure of pandemic forces physicians to evaluate in limited time, which
raises misdiagnosis rate implicitly [10]. As a result, accurate and robust classification methods are required. This is
challenging as COVID-19 is a new type of disease which has low amount of data comparing with available datasets,
such as image data published by [11] or [12]. In addition, the COVID-19 shares characteristic with other types of
pneumonia, which requires the method focus on both global and local features [13]. Moreover, varied parameter settings
causes imparities when collecting X-ray image from different devices.
Massive radiological data and rapid developing computational power give artificial intelligence (AI) a chance to assist
clinical diagnosis. Recently, classification of COVID-19 from radiological images have been explored. Wang and
Wong [14] present a COVID-Net operated on CXR images to classify COVID-19 from pneumonia and normal cases.
COVID-19 cases are extracted from online COVID-19 datasets published by [15] and [16]. Non-COVID-19 image
includes 1591 pneumonia images and 1203 normal images released by National Institutes of Health Clinical Center [17].
The experimental results showed that classification method with residual projection-expansion-projection-extension
(PEPX) design pattern achieves 93.3% accuracy, which is better than general deep models such as VGG-19 (83.0%
Accuracy) and ResNet-50 (90.6% Accuracy). The authors illustrate the locations focused by their model to visualize its
decision making process.
Ghoshal and Tucker [18] present a Bayesian CNN to make diagnosis through model uncertainty. It is trained on 68
COVID-19 cases from [15] and Non-COVID-19 cases from Kaggle’s Chest X-ray Images (Pneumonia) [19], which
improve the classification accuracy of a standard ResNet50V2 model from 86.0% to 89.8%. The authors further discuss
the effectiveness of uncertainty-aware classification by decision visualization.
Zhang et al. [20] design a screening method based on ResNet to detect COVID-19 and find abnormalities from CXR
images. Images are evaluated by an abnormity detecting module producing reference score to optimize classification
loss. The model is trained on 70 COVID-19 images and 1008 non-COVID-19 images, which reaches 96.0%, 70.7%,
95.2% in Sensitivity, Specificity and AUC respectively.
Generally, current studies operated on CXR images mostly depends on online datasets with limited COVID-19 cases.
Insufficient data can hardly evaluate the robustness of the models and restricted their generalizability. Models trained
on extremely imbalanced dataset also lead to long-tail distribution problems. Although plenty of works have discussed
diagnosing COVID-19 by AI, few works address the problem of imbalanced data and limited size of dataset because of
several issues: 1) Models trained by imbalanced data tend to classify all the targets to the dominant class which has
overwhelmingly more labels than other classes. 2) Unique labels on X-ray image, such as L/R position labels, may
easily attract model’s attention then mislead the predictions. 3) COVID-19 cases share features with non-COVID cases,
which requires a sensitive and robust model to do classification.
These challenges inspired us to treat pneumonia classification as a Fine-Grained Visual Classification (FGVC) problem,
which aims at classifying sub-level categories under a basic-level category. FGVC cases are similar apart from some
2

A PREPRINT - JANUARY 11, 2021

minor differences and also has the problem of lacking training data. Classic Convolutional Neural Networks (CNNs),
including VGG [21], ResNet [22] and Inception [23], has difficulties handling this problem. We propose a novel
Multiscale Attention Guided deep network with Soft Distance regularization (MAG-SD) for COVID-19 CXR image
classification. To balance the quantity of different data, a weakly-supervised method is presented, which requires a few
labeled data to do effective augmentations. Multiscale strategy is applied to attention generator, producing detailed
scalar matrix for prediction. Our classification model is motivated from the fact that clinical diagnosis of COVID-19
follows a procedure which firstly evaluates the regional appearance, then makes diagnosis exclusively. Thus, we propose
a multiscale attention module which estimates both shallow and deep layers. Comparing with using feature maps from
only highest level features, the utilization of lower features could increase its ability of finding fine-grained features.
Moreover, a soft distance regularization method is integrated to refine classification result by adaptively adjusting
classification loss. In a nutshell, contribution of this paper is threefold:
1) We design a novel deep network, MA-Net, to treat COVID-19 diagnosis as a FGVC problem. Multiscale attention is
introduced to assess attention maps on multi level features. Composed attention maps are used as guidance for training
steps. Attention pooling is proposed to utilize attention maps for classification.
2) We address data shortage by proposing attention guided data augmentation and multi-shot training phase. It includes
attention mix-up, attention patching and attention dimming that could enhance and search local feature then generating
data. Models are trained on imbalanced COVID-19 datasets and achieve the state-of-the-art.
3) Without introducing other modules or parameters, we formulate a new regularization term utilizing soft distance
between predictions, which works as a constraint to limit classifiers from producing contradicted output for one target.
This paper is organized as follows. In Section. 2, we introduce insightful works which have high relevance with our
contribution. Section. 3 presents the proposed method. In section. 4, database and experimental setup are reported in
detail, then results are presented and discussed individually. The last section concludes this study and highlights the
future work.

2

Related Works

Related works are introduced in this section, including X-ray appearance for typical pneumonia, fine-grained visual
classification, attention mechanism for CNNs and multiscale feature fusion utilized in computer vision.
2.1

Pneumonia X-ray Imagery

Chest X-ray is a widely used imaging modality providing high-resolution pictures to visualize the pathological changes
of thoracic diseases. Diagnosis could be made according to the visual patterns demonstrated on CXR images [19].
Clinical research from Katz and Leung [24] demonstrated that typical image pattern for bacterial pneumonia includes
opacity of single lobe and pleural effusion. Viral pneumonia also has radiological appearance such as pulmonary edema,
small area of effusions, consolidation or lobe mass. Reports from [25], [26], demonstrated that the most common
pattern on CXR in COVID-19 was consolidation or ground-glass opacity. It is notable that COVID-19 shares some
visual feature with viral pneumonia while viral and bacterial pneumonia can hardly be differentiated because of similar
spatial appearance.
2.2

Fine-Grained Visual Classification

Mass application of CNNs revealed its advantage in solving large scale image classification problem [27] and illuminated
a promising way to settle FGVC tasks by using CNN models to explore inconspicuous local features. Some models
relied on local annotations to train part-based detectors, localizing certain parts before prediction [28] [29]. However,
local feature annotation requires expensive human labor, limiting its reproducibility in reality. Recently, approaches
only require global labels also emerged whose motivation was to first localize the corresponding parts and then compare
their local features [30]. Fu et al. [31] introduced WS-DAN, which was a weakly supervised deep network handling
FGVC by posing attention to enhance local feature and guide augmentation. FGVC was also a common problem in
medical image because of spatial similarity between infections. Qin et al. [32] proposed a fine-grained classification
CNN for different types of lung cancer in PET and CT Images.
2.3

Attention for CNNs

For visual task, attention usually indicates a scalar matrix representing the relative importance and inner relevance of
local feature [33]. This nonuniform representation was produced by special designed modules [34]. Works reported that
3

A PREPRINT - JANUARY 11, 2021

applying attention on classification oriented CNN could provide an intuitional way to localize target object, helping to
identify visual properties through local representation. An attention guided method demonstrated by Gondal et al. [35]
reported that attention mechanism is helpful in Diabetic Retinopathy (DR) localization and recognition. Zhang et
al. [36] regulated the attention of deep model by training self-attention blocks for skin lesion classification and surpassed
the baselines. Generally, attention mechanism guide the models to analyze global and local features simultaneously
then generate believable classification results.
2.4

Multiscale Feature Fusion

Extracting hybrid feature maps from multi-resolution input image is a common strategy in computer vision since
the the era of hand-engineered features. CNNs have an inherent multiscale feature in pyramidal shape, which is
advantageous in producing semantically strong representations if effective feature fusion is operated. Models such as
U-Net [37] and V-Net [38] exploited skip connections to associate feature maps across resolutions. FPN [39] leveraged
the prediction of multiscale hierarchy by generating multiple prediction. For CXR image, Huang et al. [40] presented
weight concatenation method to cooperate global and local feature. Thriving of spatial attention gave inspiration to
extract attention from multi-resolution feature map. Sedai et al. [41] proposed A-CNN for chest pathologies localization,
which utilized multiscale attention by calculating convex combination from weighted average of the feature maps.

3

Method

In this episode, we propose our approach that explore multiscale fine-grain feature adaptively. We first produce an
overview for our MAG-SD. Then MA-Net is presented in terms of network architecture with attention modules. A
weakly supervised data augmentation module, Attention Guided Augmentation, is introduced to address the shortage of
COVID-19 cases. At last, Soft Distance Regularization is proposed to erase noise imported by augmentations.
3.1

Overview

COVID-19 CXR images are less distinctive comparing with other pneumonia cases, which requires a model to extract
features for fine-grained feature of input image. WS-DAN [31], which is competitive in fine-grained image classification
has been adopted for this topic. The architecture includes a feature extractor (i.e. ResNet50), an attention generator
operated on feature map and an augmentation generator producing local-enhanced and noise-blended image. An
overview of our MAG-SD is shown in Fig. 1. In primary training route, preprocessed CXR image I00 is fed into
MA-Net for prediction vector P and attention map A. Attention Guided Augmentation is operated on I00 , using A to
produce augmented data I1 , I2 , I3 . In Auxiliary training routes, I1 , I2 , I3 are pushed into MA-Net for prediction vectors
p1 , p2 , p3 . All the vectors (i.e. P, p1 , p2 , p3 ) are utilized by Soft Distance Regularization for a proper loss.
3.2
3.2.1

Multiscale Attention Guided Network (MA-Net)
Network Architecture

Fig. 2 presents a demonstration of our proposed MA-Net. As observed, a CNN based encoder is operated on augmented
images. Encoder utilizes ResNet50 as backbone, extracting size-different feature maps f1 , f2 , f3 from image I.
Multiscale attention generator is used to extract attention map a1, a2, a3 and estimate scale-wised interests. Attention
maps are resized for a single output A from features. Then, the output of encoder f3 and attention map A are assessed
by Attention Pooling to generate prediction vector P .
3.2.2

Multiscale Attention Generator

Attention mechanism has been used in natural image topics to guide feedforward process [42], [43]. Recently, tentative
efforts have been made on deep models such as image classification [44], person perception [45] and sequential decision
tasks [46]. Most of the attention models aim at gathering top level information to decide where to attend for the next
learning steps. The proposed attention generating model is operated on multiscale feature maps, aiming at extracting
attention from different scale. Layers before downsampling are selected as feature map in order to squeeze information
out of single resolution feature. For ResNet50 we used, feature maps with 512 ∗ 28 ∗ 28, 1024 ∗ 14 ∗ 14, 2048 ∗ 7 ∗ 7
sizes are chosen. The number of attention map is 32.
The architecture of multiscale attention generator is shown in Fig. 3. f1 , f2 and f3 are feature maps selected from
feature extractor. Each of them are processed by 1 ∗ 1 convolution to generate corresponding attention. All the attention
maps are downsampled to 7 ∗ 7 and connected residually. The effect of using different number of feature maps is
discussed in experiments.
4

A PREPRINT - JANUARY 11, 2021

Figure 1: The architecture of MAG-SD. The key components are illustrated in colour-wised blocks. (a): MA-Net, which
is a CNN model (e.g. ResNet50) extracting prediction vectors P, p1 , p2 , p3 and attention map A; (b): Soft Distance
Regularization using P, p1 , p2 , p3 to calculate overall loss; (c): Attention Guided Augmentation, which augments
preprocessed data I00 according to A.

Figure 2: MA-Net illustrated in colour-wised blocks. (a): Convolutional Feature Extractor, which is a pretrained CNN
model (e.g. ResNet50) extracting features f1 , f2 , f3 ; (b): Attention Pooling (demonstrated in Fig. 4) takes f3 and
attention map A for prediction vector P ); (c): Multiscale Attention Generator (demonstrated in Fig. 3) uses f1 , f2 , f3
to produce A as output.

3.2.3

Attention Pooling

Attention pooling module mimics the structure proposed by [31], which associates attention output and feature map.
Fig. 4 shows the pipeline of the pooling method. Feature map f3 (2048 ∗ 7 ∗ 7) is extracted from the output of CNN
encoder. Multiscale attention map A presented by attention generator is 32 ∗ 7 ∗ 7. Each attention map focuses on
diverse location that may contain valuable fine-grained feature. Attention biased features (i.e. part feature map (P F ))
are presented by multiplying all the attention maps A, each by each, with feature map. There are 32 P F s which size
equals 2048 ∗ 7 ∗ 7. Global average pooling (GAP) is operated to shrink each P F to 2048 ∗ 1 ∗ 1 in order to describe the
activation intensity of attention on feature map. Feature matrix M is produced by concatenating GAP results, producing
a vector of 65536 ∗ 1 ∗ 1. Eq. (1) describes the calculation of P F .
P Fj = Aj

f3 (j = 1, 2, ..., N )

(1)

where stands for multiplication of elements between two tensors. f3 is feature map extracted by CNN. N represents
the number of attention maps, which is 32 in our work.
5

A PREPRINT - JANUARY 11, 2021

Figure 3: Demonstration of multiscale attention generator. f1 , f2 , f3 are three scales of feature maps. The model choose
1, 2 or 3 feature maps for attention. Attention map is generated by operating 1 ∗ 1 convolutional layer on each feature
map then downsample it to 7 ∗ 7. Global attention map A is produced by operating residual connection between resized
feature maps. ⊕ represents residual connection.

Figure 4: Attention pooling architecture proposed for feature selecting. Feature map f3 is extracted from input image
and A is generated by the module displayed in Fig. 3. Each individual attention map selected from A multiplies ( )
with f3 to produce the features with attention bias, known as part feature Maps (P F ). After global average pooling
(GAP) process, feature matrix M is produced.
P Fj has to go through a downsampling method such as GAP to get description with compressed size, which is
2048 ∗ 1 ∗ 1. Feature matrix M is represented by concatenating all condensed P Fj presented in Fig. 4.
3.3

Attention Guided Augmentation

As mentioned above, attention mechanism emphasizes local feature which affects the classification result. Following
the idea, the performance of classification network could be enhanced if attention guided training cases are considered.
Weakly supervised methods demonstrated in Fig. 5 present effective augmentations for original image. One normalized
6

A PREPRINT - JANUARY 11, 2021

Figure 5: Demonstration of Attention Guided Augmentation. Multiple attention maps are generated by attention
generator, which concentrate on different part of original image. One attention map is chosen randomly for each
augmentation method, including: (a)Attention mix-up, (b)Attention Patching and (c)Attention Dimming.
attention map (A∗ ) is randomly chosen for each instance to do individual augmentation. 1) Attention Mixup: Mixup is
an augmentation strategy which generates data by mixing overall image and regional feature together. As we have a
attention map A∗j , a detailed region Dj could be extracted by doing threshold.

1, if A∗ (l, m) > θm
D(l, m) =
(2)
0, otherwise
For elements in A∗j (l, m), Eq. (2) sets Dj (l, m) to 1 if it is greater than threshold θm ∈ [0, 1]. If not, it will be set to 0.
A bounding box surrounding the extracted region is proposed from the raw region. Region coved by the box is enlarged
to the same size as input image then merged together with original input I0 to get augmented input I1 , which is defined
in Eq. (3).
I1 (p, q) = γI0 (p, q) + (1 − γ)B(p, q)
(3)
where γ is a parameter range in [0, 1] and B stands for the enlarged bounding box. Model could see target precisely by
learning local and global feature together.
2) Attention Patching: Encoder could be sensitive to limited part of reception field as valuable spatial features usually
distribute in similar position. To encourage the encoder to explore feature from varied part, attention patching is
proposed. D mentioned in 1) is patched onto the original image I0 to propose patched data I2 , which is demonstrated
in Fig. 5. Attention patching enlarges the model’s interest region by duplicating the interested area, promoting model to
global evaluate its input.
3) Attention Dimming: When training attention generating module for feature map, multiple attention maps may be
sensitive to similar region. A responsible fine-grain classification model have to focus on different local features of one
target. Attention dimming is proposed to stimulate the attention model searching the whole reception field for valuable
information. We obtain a Dimming Mask (DM ) from A∗ , applying threshold θd ∈ [0, 1], as represented in Eq. (4).

00.1, if A∗ (l, m) > θd
DM (l, m) =
(4)
1,
otherwise
Augmented image I3 is generated by applying the mask onto the input, which is illustrated in Fig. 5(c)
3.4

Soft Distance Regularization

Disturbances are introduced into the original image by using augmentation. (e.g. infection area reduced by attention
dimming). To address this problem, we formulate the uncertainty of predictions via the distance between prediction
vectors. Intuitively, the distance d could be modeled as Eq. (5).
d(x) = |P (I) − p(x)|

(5)

where x denotes the augmented image, P (I), p(x) represent primary and auxiliary prediction vector respectively.
However, the distance between P (I) and p(x) is unstable before the the model well-fitted. Ground truth labels are
7

A PREPRINT - JANUARY 11, 2021

referenced to stabilize gradients. In Algorithm 1, P (I) is replaced by soft label P 0 (I), filtering out low confidence
inferences. Soft distance d0 (x) can be represented in Eq. (6). The value of θ in Algorithm. 1 is 0.7.
d0 (x) = |P 0 (I) − p(x)|
(6)

1
2
3
4
5
6
7
8

Algorithm 1: Soft Distance Regularization
Input: P (I): Primary prediction vector
p(x): Auxiliary prediction vector
Glbl (I): Ground truth labels
θ: Confidence threshold
Output: Lreg : soft distance regularization term
Cross entropy loss Lprim
is calculated between P (I) and Glbl (I);
ce
P (I), p(x) are fed into softmax to extract confidence score over all classes, which are P c (I), pc (x);
if P c (I) > θ then
Let P 0 (I) = P (I)
else
Let P 0 (I) = Glbl (I)
end
Predict variance is represented by soft distance between P 0 (I) and p(x):
d0 (x) = |P 0 (I) − p(x)|

9

Overall loss is combined by Lprim
and mean predicting variance:
ce
Lreg = Lprim
+ d̄0
ce

10

return Lreg
At last, overall loss is modeled by a combination of cross entropy loss and average soft distance, which is demonstrated
in Eq. (7).
Lreg = Lprim
+ d̄0
(7)
ce
where Lprim
operates between labels and primary prediction. If two vectors have different prediction for one target,
ce
Lreg will generate a large value, which reflects the uncertainty of the model on one target. It is also notable that Lreg
punishes soft distance d̄0 , leading the model to generate similar predictions.

4

Experiments

In this section, extensive experiments were conducted to comprehensively assess MAG-SD. Models were trained on
datasets with different types of pneumonia and the performance of each proposed method was evaluated. Then the
models were compared between other baseline methods using several metrics.
4.1

Dataset and Experimental Settings

The proposed model was trained and tested on several datasets to evaluate its classification performance and ability of
fine-grained pneumonia localization. Details of each dataset was shown in Tab. 1. Dataset A was a mutated dataset
with 90 COVID-19 from [15] and 168 other pneumonia cases from [17], which directly assessed model’s fine-grained
classification ability. Dataset B was selected from [47] and [17], aiming at evaluating the model’s performance on
larger scale. Dataset C was the largest dataset we operated on, which included COVID-19 detection and fine-grained
pneumonia classification. Quality of pneumonia localization was evaluated by Localization dataset, which had 13
COVID-19 cases with pixel-wise masks from [48] and 118 non-COVID pneumonia cases with bounding boxes
annotations from [17]. In experiments, classic ResNet50 has been adopted as feature extractor. Its layer4 output was
chosen as feature map. Attention was extracted from the output of layer2, layer3 and layer4 to ensure multiscale
attention. Size of the attention maps were 28 ∗ 28, 14 ∗ 14 and 7 ∗ 7. Both training and testing sets were divided roughly
in the same class proportions. 5-fold cross validation was applied to get reliable results.
Models were implemented using Pytorch and trained on two NVIDIA RTX 2080TI GPUs. The optimizer was Stochastic
Gradient Descent (SGD) with the momentum of 0.9. For each training, 100 training epochs were deployed, with 10−6
weight decay, 32 cases per minibatch and 10−3 learning rate at beginning. Images were resized to 224 ∗ 224 when
training and testing.
8

A PREPRINT - JANUARY 11, 2021

Table 1: Datasets details
Dataset
Dataset A
Dataset B

Dataset C

Localization

Class
COVID-19
Non-COVID-19 pneumonia
COVID-19
Non-COVID-19 pneumonia
Healthy
COVID-19
Viral pneumonia
Bacterial pneumonia
Healthy
COVID-19
Non-COVID-19 pneumonia

Value
90
168
462
1567
1602
462
1449
2816
1602
13
118

Total
258
3631

6329

131

Table 2: Augmentations used and factor setting
Augmentations
Brightness adjustment
Contrast adjustment
Resized cropping
Rotation
Horizontal flipping
Vertical flipping
4.2

Abstract
Random chosen brightness
factor from [0.5, 1.0]
Random chosen contrast
factor from [0.7, 1.0]
Random cropping then
resize to 224 ∗ 224
Random rotation from [0, 120]
-

Pre-Processing and Data Augmentation

X-ray images have different appearance according to varied imaging equipment configurations, resolving that the same
tissue can be radiologically different. To ensure the intensity distribution of one tissue is similar over the dataset,
Z-score normalization was employed when training and testing. Large contrast distribution also introduced extra noise
to the dataset, impacting the performance of trained model. Contrast limited adaptive histogram equalization (CLAHE)
was proposed to enhance contrast between tissues and restrain noise signal [49].
In image classification, data augmentation has been proved as an effective method to improve robustness and evaluate
performance [50]. Augmented data provides more varieties for classification target and remitting the impact of
overfitting. Random number of transformations were chosen from a sequence of linear transformation for each training
sequence. The list is shown in Tab. 2.
Table 3: Evaluation of Models
Model
VGG16 [21]
ResNet18 [22]
ResNet50 [22]
InceptionV3 [23]
[51](ResNet)
[51](InceptionV3)
COVID-Net [14]
BCNN [30]
BCNN(Attention(Ours))
FPN [39]
U-Net [37]
MAG-SD(0AUG)
MAG-SD(Proposed)

4.3

ACC(%)
92.88±1.35
90.94±1.39
92.94±1.19
94.13±1.13
93.88±1.18
94.56±1.75
93.25±1.70
96.00±1.52
96.43±1.45
94.88±1.61
95.76±1.07
94.31±1.28
96.94±1.10

Dataset A
SEN(%)
SPC(%)
91.51±1.17 92.77±1.89
91.87±1.55 88.56±1.79
91.16±1.14 94.25±1.53
92.94±1.02 94.49±1.11
93.01±1.32 93.63±1.36
92.19±1.62 94.91±1.47
91.62±1.88 93.70±1.80
96.43±1.78 94.52±1.60
96.31±1.55 96.16±1.74
95.11±1.75 94.30±1.22
94.92±1.10 95.91±1.62
91.70±1.33 95.89±1.94
97.83±1.53 94.93±1.26

F1(%)
92.08±1.44
89.83±1.46
92.31±1.39
93.62±1.12
93.30±1.55
93.40±1.47
92.51±1.89
95.39±1.42
96.23±1.30
94.65±1.72
95.38±1.13
93.37±1.38
96.23±1.02

ACC(%)
90.68±1.64
92.06±1.14
92.56±0.76
93.06±1.19
93.41±1.14
93.45±1.21
88.94±1.28
94.41±1.37
95.11±1.55
93.27±1.20
93.00±1.55
93.25±0.90
95.85±1.27

Dataset B
SEN(%)
SPC(%)
91.05±2.12 94.90±0.82
92.03±1.13 95.62±1.03
92.07±1.68 95.85±0.47
91.73±1.13 96.23±0.77
93.71±1.72 96.26±0.62
93.00±1.24 96.40±0.50
89.95±2.41 93.75±0.58
95.26±1.23 96.71±0.81
96.61±2.00 97.26±0.72
94.05±0.82 96.20±0.78
92.59±1.39 96.08±0.82
92.01±0.93 96.29±0.45
95.74±1.20 97.73±0.45

F1(%)
89.44±1.98
91.12±1.34
91.74±0.93
92.42±1.51
93.12±1.49
93.04±1.02
87.98±1.56
96.71±1.60
94.12±1.92
92.86±1.11
92.33±1.84
92.06±1.17
95.54±1.59

ACC(%)
80.23±1.44
82.33±1.35
82.94±1.02
84.20±1.19
83.93±1.11
84.93±1.67
78.71±1.76
84.36±1.84
85.04±2.36
82.17±1.89
84.01±1.36
84.13±0.99
87.12±1.55

Dataset C
SEN(%)
SPC(%)
77.97±2.02 92.98±0.43
82.61±1.20 93.57±1.22
84.01±1.16 93.61±0.37
85.19±1.35 94.03±0.99
86.40±0.99 93.85±0.42
85.90±1.01 94.36±0.71
79.26±0.94 92.38±0.47
84.47±0.75 94.15±0.49
86.32±1.55 94.33±0.86
83.58±1.42 93.32±0.50
84.68±1.42 94.14±0.56
85.08±1.27 94.11±0.43
87.20±1.64 95.20±0.64

F1(%)
78.82±1.28
81.30±1.23
82.64±1.39
84.69±1.27
84.39±1.01
84.92±1.70
78.34±1.40
84.47±0.85
84.41±1.52
81.85±1.16
83.81±1.24
84.30±1.43
86.98±1.27

Evaluation Metrics

Experiments were evaluated by several metrics. For Classification, Accuracy (ACC), Sensitivity (SEN), Specificity
(SPC) and F1 score were employed. For multi-class datasets, mean value between classes were calculated to represent
9

A PREPRINT - JANUARY 11, 2021

Table 4: Evaluation of CLAHE
Preprocessing
w/o CLAHE
CLAHE

ACC(%)
95.56±1.14
96.94±1.10

Dataset A
SEN(%)
SPC(%)
93.12±1.59 96.23±1.03
97.83±1.53 94.93±1.26

F1(%)
94.50±0.90
96.23±1.02

ACC(%)
93.45±1.58
95.85±1.27

Dataset B
SEN(%)
SPC(%)
92.75±2.17 96.37±1.40
95.74±1.20 97.73±0.45

F1(%)
92.64±1.23
95.54±1.59

ACC(%)
85.47±1.20
87.12±1.55

Dataset C
SEN(%)
SPC(%)
86.64±1.96 94.59±0.85
87.20±1.64 95.20±0.64

F1(%)
85.96±1.64
86.98±1.27

Dataset C
SEN(%)
SPC(%)
86.56±0.96 94.56±0.36
87.20±1.64 95.20±0.64
86.65±2.29 94.29±0.97

F1(%)
86.36±0.92
86.98±1.27
85.73±2.25

Table 5: Evaluation of Multisize Attention Maps
Attention Maps
7*7
7*7 + 14*14
7*7 + 14*14 + 28*28

ACC(%)
95.31±1.10
96.94±1.10
95.88±1.84

Dataset A
SEN(%)
SPC(%)
96.70±1.49 91.09±1.04
97.83±1.53 94.93±1.26
94.76±2.11 96.63±2.23

F1(%)
93.48±1.07
96.23±1.02
95.54±1.91

ACC(%)
94.75±1.54
95.85±1.27
94.74±1.79

Dataset B
SEN(%)
SPC(%)
94.93±1.44 96.98±1.06
95.74±1.20 97.73±0.45
95.80±2.49 96.92±1.17

F1(%)
94.11±1.57
95.54±1.59
94.57±2.16

ACC(%)
85.44±1.47
87.12±1.55
85.01±1.89

the final performance score of each model. Plots of receiver operating characteristic (ROC) curve and area under the
curve (AUC) value were used to compare model functionality. Localization quality was quantified by intersection over
union (IOU) which has been widely used in target detect and semantic segmentation task [41]. Accuracy describes the
proportion of correctly classified targets, which is expressed in Eq. (8).
Accuracy =

TP + TN
TP + TN + FP + FN

(8)

where TP, TN, FP and FN stand for the number of true positive, true negative, false positive and false negative
predictions. Sensitivity, also known as true positive rate (TPR), is useful to measure the proportion of true positive
predictions over all positive targets, which is defined in Eq. (9).
Sensitivity =

TP
TP + FN

(9)

Specificity, or true negative rate (TNR), is a ratio between the amount of true negative (TN) and false positive (FP)
predictions, defined in Eq. (10).
TN
Specif icity =
(10)
TN + FP
F1 Score considers the performance from both precision and recall, defined in Eq. (11).
F1 =

2T P
2T P + F P + F N

(11)

IoU represents a value calculated by dividing the overlap of prediction and ground truth by their union. It could be
defined straightforward in Eq. (12), where Ao and Au denote area of overlap and area of union respectively.
IoU =
4.4

Ao
Au

(12)

Components Validation and Discussion

The methods composed could be concluded into attention modules, attention guided data augmentation and soft distance
regularization. Each component was studied by evaluating its improvement in classification performance, which has
been quantified by metrics mentioned above. Performance gain was obtained by the following method: the proposed
model was first trained on specific dataset with metrics, then, single component was changed or removed and reevaluate
on the same dataset. For all the tested models, Mean value and standard deviation of ACC, SEN, SPC, F1 were recorded.
Components validations were reported in Tab. 4, 5, 6, 7 and 8. Inter-model comparisons could be found in Tab. 3
and Fig. 6. Regions interested the attention module were presented in Fig. 7. Parameters in all the experiments were
maintained unchanged as possible for condition control. The model were trained on the same size of training set then
evaluated on the same size of testing set.

Table 6: Comparison of Pooling Methods
Pooling
GMP
GAP
Attention Pooling

ACC(%)
94.31±2.21
95.06±1.14
96.94±1.10

Dataset A
SEN(%)
SPC(%)
90.98±2.15 95.85±1.84
95.59±1.11 93.37±1.61
97.83±1.53 94.93±1.26

F1(%)
92.97±2.02
94.35±1.24
96.23±1.02

ACC(%)
94.47±1.44
94.91±1.64
95.85±1.27

Dataset B
SEN(%)
SPC(%)
93.38±1.47 96.94±0.81
95.01±1.76 97.16±0.91
95.74±1.20 97.73±0.45

10

F1(%)
93.80±1.60
94.39±1.68
95.54±1.59

ACC(%)
84.79±1.53
85.09±1.64
87.12±1.55

Dataset C
SEN(%)
SPC(%)
86.14±2.13 94.36±0.48
87.25±2.02 94.36±0.83
87.20±1.64 95.20±0.64

F1(%)
85.15±1.55
85.60±1.81
86.98±1.27

A PREPRINT - JANUARY 11, 2021

Table 7: Contribution of Attention Guided Augmentation
Dataset A
ACC(%)
SEN(%)
SPC(%)
F1(%)
ACC(%)
M∗
A
94.69±1.38 96.06±1.68 92.82±1.50 94.10±1.08 93.56±1.51
M
D∗∗
A +A
95.81±1.23 96.70±1.52 94.86±2.23 95.59±1.34 94.97±1.57
AM + AD + AP ∗∗∗ 96.94±1.10 97.83±1.53 94.93±1.26 96.23±1.02 95.85±1.27
*AM : Attention Mix-up; **AD : Attention Dimming; **AP : Attention Patching.
Augmentation

Dataset B
SEN(%)
SPC(%)
92.79±1.60 96.57±0.68
95.10±1.34 97.25±0.73
95.74±1.20 97.73±0.45

F1(%)
92.65±1.43
94.84±1.38
95.54±1.59

ACC(%)
85.47±1.40
86.31±1.78
87.12±1.55

Dataset C
SEN(%)
SPC(%)
85.48±1.44 94.64±0.54
87.12±1.82 94.83±0.85
87.20±1.64 95.20±0.64

F1(%)
85.10±1.13
86.60±1.64
86.98±1.27

Dataset C
SEN(%)
SPC(%)
85.33±1.34 94.11±0.69
87.20±1.64 95.20±0.64

F1(%)
84.80±0.83
86.98±1.27

Table 8: Comparison of L2 and Soft Distance Regularization
Loss
L2
Soft Distance

ACC(%)
95.93±0.61
96.94±1.10

Dataset A
SEN(%)
SPC(%)
96.36±0.85 95.48±0.71
97.83±1.53 94.93±1.26

F1(%)
95.83±0.79
96.23±1.02

ACC(%)
94.62±0.86
95.85±1.27

Dataset B
SEN(%)
SPC(%)
93.75±0.91 97.05±0.68
95.74±1.20 97.73±0.45

F1(%)
93.75±0.93
95.54±1.59

ACC(%)
83.99±0.94
87.12±1.55

Figure 6: Demonstration of ROC curves and AUC values. Three charts, from left to right, show the performance of all
the trained models operating using datasets A,B and C respectively. Top-3 highest AUC values and their ROC curves
are emphasized. Results demonstrate that comparing with baselines, results generated by our model has advantage in
AUC value, which is over 0.5% in dataset A, B and C. Architecture differences of our proposed method also influence
the performance over datasets. Generally, MAG-SD(proposed) is the most stable model which stays in top-3 in all the
datasets, which is a method given consideration to both generalizability and robustness.

11

A PREPRINT - JANUARY 11, 2021

Figure 7: Demonstration of pneumonia localization. Images were selected from Localization dataset. COVID-19
cases has pixel-wise mask while bounding boxes were provided for other pneumonia. IoU was calculated for each
prediction. Localization result was provided by apply threshold onto the attention map of each case. Results illustrated
that attention focus on different area when detecting various classes.
4.4.1

Architecture Comparing

Advantages of architecture design has been deeply explored. It has been performed by evaluating classic coarsegrained deep neural networks (i.e. VGG16, ResNet18, ResNet50 and InceptionV3), COVID-19 oriented architectures
(i.e. [51](ResNet), [51](InceptionV3), COVID-Net-Large), high performance fine-grained image classification structure
(i.e. BCNN, BCNN(Attention)) and multiscale feature fusion models (i.e. FPN, U-Net). Statistics analysis between
these deep structures helped to explain our advantages in fine-grained feature extraction. It can be observed in Tab. 3
and Fig. 6 that proposed model had noticeably better performance over others. For our model, accuracy on dataset A, B
and C reached 96.94% ± 1.10%, 95.85% ± 1.27%, 87.12% ± 1.55% respectively and performance assessed by AUC
are 99.94%, 98.72%, and 95.11%.
Comparing with classic models, our model was specialized for COVID-19 image classification and attention guided
training phase had advantage in fine-grained visual classification task. Most of the other COVID-19 oriented models
presented better performance than classic models, however, none of them applied attention mechanism or considered
fine-grained features, which impacted their accuracy on large scale, multi-class dataset such as Dataset B and C.
Comparison between FPN, U-Net and classic models demonstrated that FPN presented results over InceptionV3 in
Dataset A and B. In Dataset C, U-Net had higher accuracy than FPN, which exceeded ResNet50. Results indicated that
multiscale feature fusion models reached competitive results using relatively simple structures comparing with classic
deep models, which left us a hint that multiscale attention might be a possible route to improve.
BCNN was a FGVC model with stable performance on multiple datasets. In order to evaluate the generalization ability
of our attention module, multiscale attention and attention pooling were transported to BCNN to train BCNN(Attention).
Statistically, BCNN reached 96.00% ± 1.52%, 94.41% ± 1.37%, 84.36% ± 1.84% in Accuracy, which was competitive
in all the evaluated models. Attention modules remarkably boosted the performance of BCNN, exceeded our proposed
method in Dataset A (SPC) and Dataset B (SEN), which were 96.16% ± 1.74%, 96.61% ± 2.00% respectively.
4.4.2

CLAHE Preprocessing

Images collected by different devices were probably distinct in contrast due to configuration variety. CLAHE was
employed to relieve the noise brought by contrast distribution. Tab. 4 showed the result that CLAHE obviously
12

A PREPRINT - JANUARY 11, 2021

Figure 8: Three charts of confusion matrices generated by proposed MAG-SD, demonstrating the distribution of
predictions. The color of confusion matrices depend on the normalized values of predictions for a better visualization,
which are placed at the top of each grid. The number of predictions are placed below the normalized values. Symbols
used in figure are denoted as: P: Non-COVID19 Pneumonia, VP: Viral Pneumonia, BP: Bacterial Pneumonia, C-19:
COVID-19, and H: Healthy.

improved the performance of proposed model and raised over 1.5% Accuracy on average. Model trained without
CLAHE had notable higher standard deviation value. Larger datasets such as Dataset B and C were reported to have
more performance gain.
4.4.3

Multiscale Attention Generator and Attention Pooling

Normally, state-of-the-art coarse-grained CNN models suffer from similar global features when dealing FGVC. Under
this circumstance, models have to depend on local features, which could be effectively localized by our multiscale
attention module. Models trained with attention module (i.e. MAG-SD(0AUG)) and baseline model (i.e. ResNet50) were
compared in Tab.3, and Fig. 6. Results revealed that proposed model surpasses the baseline on dataset A, B and C using
most of the benchmarks. In dataset B (SPC), ResNet50 has slight advantage. Comparing with AUC, MAG-SD(0AUG)
was 2% over ResNet50. Furthermore, two parts of attention module, attention generating and attention pooling has been
investigated separately. Firstly, models were compiled to assess multiscale attention, with 1, 2 or 3 size of attention
maps considered. Results presented in Tab. 5 and Fig. 6 showed the model considering two feature maps achieved the
best performance in all three datasets. Possible explanation was that the proposed attention module was too simple to
locate valuable fine grained feature on low-level feature maps. Instead of importing meaningful location information,
noise was brought into the proposed model. Secondly, we evaluated attention pooling module with models trained with
other commonly used pooling methods such as global average pooling (GAP) or global max pooling (GMP). Results on
pooling methods were presented in Tab. 6 and Fig. 6, demonstrating that attention pooling surpassed GAP and GMP in
all three datasets.
4.4.4

Attention Guided Augmentation

The generated attention maps emphasized local feature that interested the model, which could be used to effectively
guide data augmentation in Fig. 5. Models trained with 0, 1, 2 or 3 augmentation were discussed in experiment. In the
case of 1 augmentation, attention mixup was selected. 2 augmentations model included attention mixup and attention
patching. The results were presented in Tab. 7 and Fig. 6. The table reflects that model with all three augmentations had
better performance, however, AUC value showed that in dataset A and B, two augmentations was advantageous. The
proposed augmentations emphasized data according to attention map, minimizing negative effect caused by random
augmentations.
13

A PREPRINT - JANUARY 11, 2021

4.4.5

Soft Distance Regularization

Soft distance regularization was presented to relieve augmentation variance. Experiments have been composed to
compare it with L2 distance regularization. Tab. 8 and Fig. 6 illustrated that it surpassed L2 in mean value, but, inferior
in standard deviation. Constraint between auxiliary vector and primary vector screen the false prediction introduced by
attention guided augmentations. Regularization was calculated between ground truth and auxiliary vector when primary
vector cannot provide reliable prediction, keeping the final result away from local minima. L2 compared all predictions
directly with ground truth, which has higher stability and sidestepped the disturbances introduced by primary prediction.

4.4.6

Attention Based Infection Localization

Technically, attention improved the models by roughly localize the part with high activation intensity. This characteristic
of attention inspired us to try MAG-SD on localization topics. The models were trained on the Dataset B we proposed,
then test on Localization dataset demonstrated in Fig. 7. It included COVID-19 cases with pixel-wise segmentation
and non-COVID-19 cases with bounding box for pneumonia infection. Attention maps A were upsampled from 7 ∗ 7
to 224 ∗ 224. Localization masks for COVID-19 cases were extracted by applying threshold to the attention maps.
Bounding boxes for other pneumonia were produced by simply enclosing the localization masks with rectangles. IoU
was calculated to evaluate the quality of localization. Image showed that the attention module we proposed could
roughly indicate the position of different type of pneumonia with over 0.25 IoU score. Attention map emphasized the
influential part from the input image effectively.

4.5

Distribution Analysis

As we imported multiple fine-grained classes into this topic, it was necessary to report the distribution of our prediction
result, which has been shown using confusion matrix in Fig. 8. MAG-SD has been selected to generate the charts
to represent the classification result of deep learning models. It could be inferred that the model was suitable for
searching definitive features from cases showed in dataset A and B as most of the cases were located on the diagonal
line of matrices. In dataset C, classification result between viral pneumonia and bacterial pneumonia was significantly
inferior than others, which impacted the global performance of the classification model. These results proved the
arguments reviewed in Section. 2.1, indicated that the CXR visual appearance between viral and bacterial pneumonia
was insufficient to make accurate diagnosis.

5

Conclusions

We have presented MAG-SD for automatic COVID-19 CXR image classification that reached the state-of-the-art on
our dataset. The proposed novel method treated this topic as a fine-grained image classification task, utilizing local
features efficiently under the guidance of attention mechanism. Attention maps were generated using multiscale features
then used as a reference to data augmentation, helping the model to overcome the lack of COVID-19 cases. The
proposed network learned to weight the predictions from both primary and auxiliary training pathways by calculating
soft distances between vectors, gaining improvements by screening noise generated by augmentations.
Findings of our exploration were demonstrated and discussed in Section. 4. The results indicated the great potential
of applying advanced pattern recognition model to clinical diagnosis and epidemic screening. Trained on the clinical
knowledge acquired by physicians, our model was capable to extract fine-grained spatial features for COVID-19.
Attention was applied in both feature extraction and augmentation stage, which helped to localize pneumonia infection
and accrete the data effectively as part of weakly supervised method. Attention module also shows its capability in
different models. It could be interesting to design more auxiliary training strategies to guide the model to an optimal
solution. Positive feedback on soft distance regularization proved that our method considered auxiliary predictions and
eliminated label noise simultaneously, however, hard threshold may limit its adaptability in complicated data.
Although deep learning methods seem promising in clinical diagnosis and pandemic screening, lacking of prior
knowledge is always the Achilles’ Heel. Supervised learning method, such as MAG-SD we proposed, have to be trained
on expensive labeled data. Newly occurred or rare diseases without available data may not be classified properly.
Abnormal detecting and clustering model could be proposed as a guidance for supervised models to alleviate the
limitations, which is part of our future work.
14

A PREPRINT - JANUARY 11, 2021

Acknowledgments
This work was supported by the National Natural Science Foundation of China (Grant No.KYZ043718114) and
Biomedical Engineering Interdisciplinary Research Fund of Shanghai Jiao Tong University (Grant No.YG2020YQ17).
The authors would like to thank the institutes who generously open-sourced image database.

References
[1] Chen Wang, Peter W Horby, Frederick G Hayden, and George F Gao. A novel coronavirus outbreak of global
health concern. The Lancet, 395(10223):470–473, 2020.
[2] World Health Organization et al. Coronavirus disease 2019 (covid-19): situation report, 72. 2020.
[3] Ioannis Apostolopoulos, Sokratis Aznaouridis, and Mpesiana Tzani. Extracting possibly representative covid-19
biomarkers from x-ray images with deep learning approach and image data related to pulmonary diseases. arXiv
preprint arXiv:2004.00338, 2020.
[4] Tao Ai, Zhenlu Yang, Hongyan Hou, Chenao Zhan, Chong Chen, Wenzhi Lv, Qian Tao, Ziyong Sun, and Liming
Xia. Correlation of chest ct and rt-pcr testing in coronavirus disease 2019 (covid-19) in china: a report of 1014
cases. Radiology, page 200642, 2020.
[5] Chaolin Huang, Yeming Wang, Xingwang Li, Lili Ren, Jianping Zhao, Yi Hu, Li Zhang, Guohui Fan, Jiuyang Xu,
Xiaoying Gu, et al. Clinical features of patients infected with 2019 novel coronavirus in wuhan, china. The lancet,
395(10223):497–506, 2020.
[6] Feng Shi, Jun Wang, Jun Shi, Ziyan Wu, Qian Wang, Zhenyu Tang, Kelei He, Yinghuan Shi, and Dinggang Shen.
Review of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for covid-19.
IEEE Reviews in Biomedical Engineering, 2020.
[7] Yaqi Wang, Ling Ling Sun, and Qun Jin. Enhanced diagnosis of pneumothorax with an improved real-time
augmentation for imbalanced chest x-rays data based on dcnn. IEEE/ACM Transactions on Computational Biology
and Bioinformatics, pages 1–1, 2019.
[8] T Franquet. Imaging of pneumonia: trends and algorithms. European Respiratory Journal, 18(1):196–208, 2001.
[9] Antoni Torres and Catia Cillóniz. Clinical Management of Bacterial Pneumonia. Springer, 2015.
[10] Thirumalaisamy P Velavan and Christian G Meyer. The covid-19 epidemic. Tropical medicine & international
health, 25(3):278, 2020.
[11] Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vayá. Padchest: A large chest x-ray
image dataset with multi-label annotated reports. arXiv preprint arXiv:1901.07441, 2019.
[12] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,
Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty
labels and expert comparison. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages
590–597, 2019.
[13] Nanshan Chen, Min Zhou, Xuan Dong, Jieming Qu, Fengyun Gong, Yang Han, Yang Qiu, Jingli Wang, Ying Liu,
Yuan Wei, et al. Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in
wuhan, china: a descriptive study. The Lancet, 395(10223):507–513, 2020.
[14] Linda Wang and Alexander Wong. Covid-net: A tailored deep convolutional neural network design for detection
of covid-19 cases from chest radiography images. arXiv preprint arXiv:2003.09871, 2020.
[15] Joseph Paul Cohen, Paul Morrison, and Lan Dao.
arXiv:2003.11597, 2020.

Covid-19 image data collection.

[16] Linda Wang.
Figure 1 covid-19 chest x-ray data initiative.
Figure1-COVID-chestxray-dataset. Accessed May 29, 2020.

arXiv preprint

https://github.com/agchung/

[17] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8:
Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of
common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 2097–2106, 2017.
[18] Biraja Ghoshal and Allan Tucker. Estimating uncertainty and interpretability in deep learning for coronavirus
(covid-19) detection. arXiv preprint arXiv:2003.10769, 2020.
15

A PREPRINT - JANUARY 11, 2021

[19] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex
McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al. Identifying medical diagnoses and treatable diseases by
image-based deep learning. Cell, 172(5):1122–1131, 2018.
[20] Jianpeng Zhang, Yutong Xie, Yi Li, Chunhua Shen, and Yong Xia. Covid-19 screening on chest x-ray images
using deep learning based anomaly detection. arXiv preprint arXiv:2003.12338, 2020.
[21] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[23] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception
architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 2818–2826, 2016.
[24] Douglas S Katz and Ann N Leung. Radiology of pneumonia. Clinics in chest medicine, 20(3):549–562, 1999.
[25] Alfonso J Rodriguez-Morales, Jaime A Cardona-Ospina, Estefanía Gutiérrez-Ocampo, Rhuvi Villamizar-Peña,
Yeimer Holguin-Rivera, Juan Pablo Escalera-Antezana, Lucia Elena Alvarado-Arnez, D Katterine Bonilla-Aldana,
Carlos Franco-Paredes, Andrés F Henao-Martinez, et al. Clinical, laboratory and imaging features of covid-19: A
systematic review and meta-analysis. Travel medicine and infectious disease, page 101623, 2020.
[26] Sivaramakrishnan Rajaraman and Sameer Antani. Weakly labeled data augmentation for deep learning: A study
on covid-19 detection in chest x-rays. Diagnostics (Basel, Switzerland), 10(6):358, 2020.
[27] Xiu-Shen Wei, Jianxin Wu, and Quan Cui. Deep learning for fine-grained image analysis: A survey. arXiv preprint
arXiv:1907.03069, 2019.
[28] Ning Zhang, Jeff Donahue, Ross B Girshick, and Trevor Darrell. Part-Based R-CNNs for Fine-Grained Category
Detection. In European Conference on Computer Vision, pages 834–849, 2014.
[29] Xiu-Shen Wei, Chen-Wei Xie, Jianxin Wu, and Chunhua Shen. Mask-CNN: Localizing parts and selecting
descriptors for fine-grained bird species categorization. Pattern Recognition, 76:704–714, 2018.
[30] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear CNN Models for Fine-Grained Visual
Recognition. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 1449–1457, 2015.
[31] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent attention convolutional neural
network for fine-grained image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 4438–4446, 2017.
[32] RuoXi Qin, Zhenzhen Wang, LingYun Jiang, Kai Qiao, Jinjin Hai, Jian Chen, Junling Xu, Dapeng Shi, and Bin
Yan. Fine-Grained Lung Cancer Classification from PET and CT Images Based on Multidimensional Attention
Mechanism. Complexity, 2020:1–12, 2020.
[33] Saumya Jetley, Nicholas A Lord, Namhoon Lee, and Philip Torr. Learn to Pay Attention. In ICLR 2018 :
International Conference on Learning Representations 2018, 2018.
[34] Jun Wang, Yiming Bao, Yaofeng Wen, Hongbing Lu, Hu Luo, Yunfei Xiang, Xiaoming Li, Chen Liu, and Dahong
Qian. Prior-attention residual learning for more discriminative covid-19 screening in ct images. IEEE Transactions
on Medical Imaging, 39(8):2572–2583, 2020.
[35] Waleed M Gondal, Jan M Kohler, Rene Grzeszick, Gernot A Fink, and Michael Hirsch. Weakly-supervised
localization of diabetic retinopathy lesions in retinal fundus images. In 2017 IEEE International Conference on
Image Processing (ICIP), pages 2069–2073, 2017.
[36] Jianpeng Zhang, Yutong Xie, Yong Xia, and Chunhua Shen. Attention residual learning for skin lesion classification. IEEE transactions on medical imaging, 38(9):2092–2103, 2019.
[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medical image computing and computer-assisted intervention,
pages 234–241. Springer, 2015.
[38] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for
volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages
565–571. IEEE, 2016.
[39] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 2117–2125, 2017.
16

A PREPRINT - JANUARY 11, 2021

[40] Zhiwei Huang, Jinzhao Lin, Liming Xu, Huiqian Wang, Tong Bai, Yu Pang, and Teen-Hang Meen. Fusion
high-resolution network for diagnosing chestx-ray images. Electronics, 9(1):190, 2020.
[41] Suman Sedai, Dwarikanath Mahapatra, Zongyuan Ge, Rajib Chakravorty, and Rahil Garnavi. Deep multiscale
convolutional feature learning for weakly supervised localization of chest pathologies in x-ray images. In
International Workshop on Machine Learning in Medical Imaging, pages 267–275. Springer, 2018.
[42] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural
information processing systems, pages 2017–2025, 2015.
[43] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L Yuille. Attention to scale: Scale-aware semantic
image segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
3640–3649, 2016.
[44] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou
Tang. Residual attention network for image classification. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3156–3164, 2017.
[45] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Mask-guided contrastive attention model for person
re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
1179–1188, 2018.
[46] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation.
In Proceedings of the IEEE international conference on computer vision, pages 1520–1528, 2015.
[47] Simon Edwardsson. Covid-19 xray dataset. https://github.com/v7labs/covid-19-xray-dataset. Accessed September 23, 2020.
[48] Maria de la Iglesia Vayá, Jose Manuel Saborit, Joaquim Angel Montell, Antonio Pertusa, Aurelia Bustos, Miguel
Cazorla, Joaquin Galant, Xavier Barber, Domingo Orozco-Beltrán, Francisco Garcia, et al. Bimcv covid-19+: a
large annotated dataset of rx and ct images from covid-19 patients. arXiv preprint arXiv:2006.01174, 2020.
[49] Etta D Pisano, Shuquan Zong, Bradley M Hemminger, Marla DeLuca, R Eugene Johnston, Keith Muller, M Patricia
Braeuning, and Stephen M Pizer. Contrast limited adaptive histogram equalization image processing to improve
the detection of simulated spiculations in dense mammograms. Journal of Digital imaging, 11(4):193, 1998.
[50] Luis Perez and Jason Wang. The effectiveness of data augmentation in image classification using deep learning.
arXiv preprint arXiv:1712.04621, 2017.
[51] Ali Narin, Ceren Kaya, and Ziynet Pamuk. Automatic detection of coronavirus disease (covid-19) using x-ray
images and deep convolutional neural networks. arXiv preprint arXiv:2003.10849, 2020.

17

