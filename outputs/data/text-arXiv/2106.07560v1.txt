arXiv:2106.07560v1 [cs.SI] 14 Jun 2021

Allocating Stimulus Checks in Times of Crisis
Marios Papachristou
Cornell University

Jon Kleinberg
Cornell University

papachristoumarios@cs.cornell.edu

kleinberg@cornell.edu

Abstract
We study the problem of allocating bailouts (stimulus, subsidy allocations) to people participating in a financial network subject to income shocks. We build on the financial clearing
framework of Eisenberg and Noe [EN01] that allows the incorporation of a bailout policy that
is based on discrete bailouts motivated by the types of stimulus checks people receive around
the world as part of COVID-19 economical relief plans. We show that optimally allocating such
bailouts on a financial network in order to maximize a variety of social welfare objectives of
this form is a computationally intractable problem. We develop approximation algorithms to
optimize these objectives and establish guarantees for their approximation ratios. Then, we
incorporate multiple fairness constraints in the optimization problems and establish relative
bounds on the solutions with versus without these constraints. Finally, we apply our methodology to a variety of data, both in the context of a system of large financial institutions with
real-world data, as well as in a realistic societal context with financial interactions between
people and businesses for which we use semi-artificial data derived from mobility patterns. Our
results suggest that the algorithms we develop and study have reasonable results in practice and
outperform other network-based heuristics. We argue that the presented problem through the
societal-level lens could assist policymakers in making informed decisions on issuing subsidies.
Keywords. financial networks, bailouts, optimal bailouts, stimulus allocation, financial clearing, algorithmic fairness, access to opportunity

1

Introduction

The recent coronavirus pandemic has spread uncertainty among households and individuals. Governments around the world are faced with the problem of saving people from economic ruin, so
that people can address their needs, when their income is subject to shocks. One well-discussed
policy framework, applied in many places throughout the world, is the allocation of stimulus checks.
Governments seek to inject cash payments to households in order to stimulate consumption and
avert recession effects [BFM+ 20, Bak18, CCSW20, BP14, JPS06, PSJM13], focusing in particular
on those whose needs are the most severe.
Multiple policies have been devised by governments throughout the world in order to determine
who is qualified for a stimulus check. For instance, in the US the CARES act [act20] gives a stimulus
check within a certain range of income1 and increases the payments proportionally to the number
of dependent members of a household2 . Other countries such as, for example, New Zealand3 and
Between 0 and $75K for individuals and between 0 and $150K for couples filing jointly with a phase-out to $100K
and $200K respectively.
2
Each additional child yields an extra $500 subject to phasing out at higher incomes.
3
https://www.theguardian.com/world/2020/mar/17/new-zealand-launches-massive-spending-package-to
-combat-covid-19
1

1

Greece4 offer stimulus checks of fixed value to employees who are unable to perform their duties
remotely due to the pandemic, with similar criteria followed all over the world. A common pattern
in these cases is that when somebody’s income is below a certain threshold or e.g. the individual
satisfies some criterion (e.g. disruption of employment) then the individual has a right to request
a discrete stimulus check of fixed value5 .
An important limitation is that most of these rules are built on simple criteria which usually do
not take into account contagion effects through the financial network. For example, if a business
defaults that may translate to job loss for the employees who in their turn may not be able to pay
their rent causing economic distress to the landlords and so on. The subject of who to bailout in
a financial crisis is a subject of controversy [Jih09, Con09, CP15, Bri09, Ros06, BBT13, BHM17],
namely the policymakers are faced with the following quandary: do we need to bailout large
businesses whose saving from default would help maintain job positions but can also make them
richer and stronger with respect to the rest of the population, or do we need to bailout individuals
and small businesses so as they are able to afford their rent and groceries with the fear of letting
bigger companies collapse?
In this paper, we address this category of questions for bailouts in a formal model of economic
interaction and contagion. Our study provides insights on who to bailout in a financial crisis based
on the assumption that the financial interactions of people are based on the well-studied EisenbergNoe (EN) model [EN01] with income (asset) shocks [GY15] incorporated into it. We study the
problem of (approximately) maximizing welfare objectives with discrete bailouts subject to the EN
model with the presence of income shocks. Initially, we show that finding the optimal policy under
the discrete bailouts scheme, i.e. “Who to bailout in a crisis?”, under this model corresponds to
solving an NP-Hard problem. This stands in contrast to a version of the same model under fractional
bailouts [AK19, Dem18]. Moreover, although thresholding individuals according to their equity is a
policy that is simple to state and understand, we show that, in general, such thresholding policies
can behave arbitrarily badly where networks effects are taken into account. To remedy this, we
propose approximation algorithms in order to maximize a range of objectives based on both greedy
algorithms and Linear Programming (LP) and provide appropriate approximation-ratio guarantees
with respect to the optimal solutions.
We then turn to a set of questions within the model that relate to fairness and equity considerations in the provision of bailouts. In particular, we optimize the same objectives as above
subject to an algorithmic fairness constraint that regards stimulus distribution via the Gini Coefficient [Gin21]. We study three notions of fairness: (i) the classical Gini Coefficient that measures
all-pairs inequality, (ii) a variant of the GC introduced in [MMD+ 21] adjusted to our model, (iii)
a novel Gini Coefficient index based on the attributes of each node in the financial network (e.g.
minority status). For these constraints we introduce a Price of Fairness (PoF) for bailouts and establish bounds on its behaviour. We show that the PoF can be unbounded in the discrete case and
bounded in the fractional case, and devise bounds on the PoF for the fractional case. Our bounds
relate to quantities from spectral graph theory, namely the conductance of the underlying nominal
liability network. We apply our algorithms on real-world banking datasets from [GY15, CWY16]
from bank stress-testing, and semi-artificial data such as social payments data from the Venmo6
platform, and network topologies inferred by anonymized mobility data from the SafeGraph plat4

https://www.ekathimerini.com/economy/258852/measures-announced-for-relieving-lockdown-pressure
-on-workers-households
5
In the more general case the stimulus value depends on the individual’s features (e.g. number of dependents)
and for two individuals with the same feature set the amount of stimulus is the same.
6
https://venmo.com

2

p̄1 ≈ 1.14

k
pkj

1

cj −Xj

3

j
bj

1

1

pji

3

3

0

p̄3 ≈ 0.28
1

1

p̄2 ≈ 0.57

i

(a) Model Setup for node j.

2

3
0

(b) Toy Instance for X = 0.

Figure 1: Eisenberg-Noe Model.
form7 , the US Census, and the US Economic Census. We compare the results of the proposed
approximation algorithms with other network-based heuristics on these datasets. Moreover, we
also empirically study the incorporation of the fairness constraint both between all pairs of nodes
as well as in a minority/non-minority context on the semi-artificial data. An analytical overview
of our contributions (after setting up the model) can be found in Section 1.2.
Paper Organization. The paper is organized as follows: Section 1.1 gives a brief description
of the Eisenberg-Noe framework for computing clearing payments and introduces the objectives
that will be studied throughout the paper. Section 1.3 describes the Generalized Discrete Bailouts
Problem which is the general formulation of stochastic optimization problem we solve in the paper. Section 2.1 establishes the hardness results of the discrete bailouts problem. The remainder of
Section 2 studies approximation algorithms with provable guarantees, and establishes inapproximability results for certain objectives. Section 3 relates metrics of algorithmic fairness to the problem
and proposes ways to incorporate fairness when designing a stimulus allocation problem. Section 4
provides data-driven experiments with real-world, and semi-artificial datasets. Section 5 discusses
the experiments as well as generalizations of the models studied in this paper, the societal impact
of the work, as well as further related work. Section 6 provides concluding remarks and future
directions. Proofs of theoretical nature have been deferred to Appendix A and are presented in
serial order. An extensive data analysis can be found in Appendix C and data ethics statement in
Appendix D.
Basic Notation. [n] denotes the set {1, . . . , n}. kxkp denotes the p-norm of the vector x. When
p = 2 the subscript may be omitted. 0 (resp. 1) denote the all zeros (resp. all ones) column
vector. 1S represents the indicator column vector of the set S. For a matrix A, kAkp denotes the
induced p-norm of A. x ∧ y (resp. x ∨ y) denotes the coordinate-wise minimum (resp. maximum)
of x, y. (x)+ = 0 ∨ x denotes the non-negative part of x. A t B denotes the disjoint union of A
e
and B (A ∩ B = ∅). ≥, ≤, >, < denote coordinate-wise ordering. O(·)
ignores poly-logarithmic
(k)
factors. A B is the Hadamard (element-wise) product of A, B. A is defined as A(0) = I and
A(k+1) = A(k) A for k ≥ 1. For a vector v > 0 we define ζ(v) = vvmax
≥ 1 to be a measure of
min
conditioning of the vector v in the sense that ζ(v) is the condition number of the n × n matrix
diag(v1 , . . . , vn ). When clear from the context, we use the shorthand notation ζ = ζ(v).

1.1

The Eisenberg-Noe Model

Setup. The EN model considers a directed network of payments G([n], E) with the following
structure. The network has n nodes and internal liabilities which are represented on the directed
edges E. Each node j ∈ [n] of the payment network has an external influx of assets cj ≥ 0 and
external liabilities bj ≥ 0 which correspond to the node’s external exchanges. A direct way to think
7

https://www.safegraph.com/

3

about external liabilities are taxes that natural and legal people owe to their governments. Between
two nodes (j, i) ∈ E there is a liability pji ≥ 0 from j to i, which denotes how much j owes to i (in
terms of monetary units, e.g. USD). The initial wealth, or equity, of node j is given by
wj = cj +

X

pij − pj

i:i∼j

P
where pj = bj + i:j∼i pji are the cumulative liabilities of node j. The network contains no
isolated nodes, i.e. every node j has pj > 0 (either external liabilities, or internal liabilities or both).
With βj = (pj − bj )/pj we refer to the financial connectivity of j, i.e. the fraction of total liability
that j owes within the network (see also [GY15]). Subsequently, 1 − βj = bj /pj denotes the fraction
of total liabilities j owes outside the network. If wj ≥ 0 then j is said to be solvent, i.e. it can pay all
of its obligations, and has a profit of wj − pj . Otherwise the node is default (or insolvent) in which
case the node cannot satisfy all of its payments. In this case, the node defaults to its creditors and,
according to the EN model, rescales its liabilities in order to pay its creditors to a reduced rate,
according to the amount of money it owes to them. The relative liabilities matrix A is defined to
have entries aji = pji /pj whenever pj > 0 and aji = 0 otherwise. We define βmin = minj∈[n] βj and
βmax = maxj∈[n] βj . Later in the paper, the quantity βmax will play an important role regarding
the algorithms we develop. Moreover, βmax determines the contraction properties of the clearing
mapping. The clearing payment vector p̄ is given by the EN equilibrium equations

p̄ = p ∧ AT p̄ + c

(1)

The set of default nodes is defined to be the set D = {j : p̄j < pj } whereas the set of solvent
nodes is the set R = {j : p̄j = pj }, such that V = D t R8 . When βmax = 1 it can be shown that

Φ
the fixed point operator p̄ 7→ p ∧ AT p̄ + c is a non-expansion and thus the EN model has multiple
equilibria which are bounded by the best-case equilibrium p̄+ and the worst-case equilibrium p̄−
where p̄+ ≥ p̄− due to the Knaster-Tarski Theorem [Hay85]. For asserting the uniqueness of the
equilibrium vector, namely imposing that p̄+ = p̄− , [GY15] make the following assumption and
prove uniqueness under it:
Assumption 1 (Uniqueness of Clearing Vector [GY15].). If every node i can access a node j such
that j has bj > 0, then the EN model has a unique equilibrium p̄, i.e. p̄− = p̄+ = p̄.

Under this assumption one can apply the mapping Φ(p̄) = p ∧ AT p̄ + c to iteratively compute
the equilibrium vector. Under Assumption 1, Φ is a contraction since the spectral radius of A is less
than 1, and the uniqueness of the equilibrium is asserted from to Banach’s Fixed Point Theorem
Moreover, Φ is increasing, positive, and concave [EN01]. In our paper, we rely on a weaker, although
very realistic assumption, that every node is directly obliged to the external sector, i.e.
Assumption 2 (Weaker version of A1).). kAT k1 = βmax < 1. The following are equivalent: (i) A
has strictly substochastic rows, and (ii) for all j ∈ [n] there are obligations to the external sector.
Subsequently the spectral radius of A satisfies ρ(A) ≤ βmax < 1.
In the context of studying topologies involving people, Assumption 2 is a reasonable assumption
as we are able to model the external liabilities with a tax (e.g. income tax) or other fixed costs
(e.g. rent/utilities payment) that is owed to an external entity (e.g. government, physical or legal
person).
8
Throughout the rest of the paper we will use superscripts to denote the context under which we refer to the sets
of default and solvent nodes and the clearing payment vector.

4

At equilibrium, the set of solvent nodes is defined to be R = {j : p̄j = pj } and the set of default
nodes is defined to be D = {j : p̄j < pj } = [n] \ R. The vector p̄ can also be found via solving an
optimization problem of the following form [EN01, p. 10]:
max
s.t.

f (p̄)
p̄ ≤ AT p̄ + c

(2)

0 ≤ p̄ ≤ p,
where f is a strictly increasing function of p̄. We will also refer to the convex polytope
P(P, b, c) = { p̄ : 0 ≤ p̄ ≤ p, (I − AT )p̄ ≤ c } as the domain of the network parametrized by
P, b, c with A being the relative liability matrix computed as a function of P and b.
Objectives. The objectives that we study in this paper (in terms of maximization) are the
following:
1. A linear objective parametrized by a vector v > 0, i.e.
f (p̄) = v T p̄.

(L-OBJ)

This form of objective will be largely used throughout the paper since most welfare measures have a natural formulations in this context. Examples of such measures constitute the
following:
(a) Sum of Payments (SoP). The SoP objective is just the sum of the clearing payments,
namely
fSoP (p̄) = f (p̄; 1) = 1T p̄.

(SoP)

(b) Sum of Internal Payments (SoIP). The SoIP objective equals the sum of clearing
payments with respect to within-network liabilities, that is
fSoIP (p̄) = f (p̄; β) = β T p̄.

(SoIP)

(c) Sum of Taxes (SoT). The SoT objective equals the sum of clearing payments with
respect to outside liabilities (i.e. taxes), that is
fSoT (p̄) = f (p̄; 1 − β) = (1 − β)T p̄.

(SoT)

Note that fSoP = fSoIP + fSoT .
(d) Fractional Solvency (FS). The objective is defined as
fFS (p̄) =

X p̄j
.
pj

(FS)

j∈[n]

2. Absolute Solvency (AS). The objective is defined as
fAS (p̄) =

X

1{p̄j = pj } = |R|.

(AS)

j∈[n]

In the above definitions there are some slight technical details that we now clarify. First,
the (AS) objective is not strictly increasing. To obtain an -approximation we augment the objective
to include a small amount of the total payments to be added to the objective controlled by an
approximation parameter  > 0. We defer the statement to Appendix B. The (SoIP) objective
5

3/2

1

1/2

p̄1 =

3
2
1

p̄2 = 1

1/2

1
2

1/3

2

1

1

1/6

(a) Before shock

p̄1 =

p̄2 =

1
3

3/2

2

1

1/3

1/2

(b) After shock

p̄1 =

3
2
1

p̄2 = 1
2

1

(c) After Bailouts

Figure 2: Example 1 instance.
requires β > 0 to be strictly monotone. Secondly, in the (SoIP) objective there may exist βj such
that βj = 0. In this case, we follow the same procedure to calculate the clearing vector if we choose
to do so via optimization. Lastly, note that since the equilibrium is unique, the optimal solution to
all the equations is the same clearing vector, since it can be computed independently by iteratively
applying Φ (see Lemma 4 of [EN01]) and then evaluating f on the fixed point of Φ.
Shock Model. In the presence of shocks, Glasserman and Young [GY15] proposed the following
alteration to the EN model: Suppose that a shock x = (x1 , . . . , xn )T hits the network where each
node j receives a shock xj ∈ [0, cj ]. Then, the adjusted EN equilibrium equations become

p̄ = p ∧ AT p̄ + c − x

(3)

where the notation holds as in the case of (1). The set of default nodes is again defined to be
D = {j : p̄j < pj } and the set of solvent nodes is defined to be R = {j : p̄j = pj }. Following the
paradigm of [GY15], we impose randomness on the shocks. More concretely, the shocks are drawn
from a distribution D with support the set [0, c]. In the context of randomization, we are interested
in the influence of the shocks on the objectives we study on expectation under the distribution D.
In the majority of our proofs, we will first fix a shock, i.e. condition on the event {X = x}, and
then extend the result by computing EX∼D [·] from the law of total expectation.
Finally, we will use the following Lemma as a tool in many of our proofs. Briefly, the Lemma
states that if the external assets vector is increased (point-wise) then the corresponding clearing
vector is (point-wise) greater than the previous one. Formally, we prove the following.
Lemma 1 (Comparison Lemma). Let N1 = (P, c1 , b) and N2 = (P, c2 , b) be two EN instances with
c1 ≥ c2 (w.l.o.g. with X = 0). Then for the clearing vectors p̄1 and p̄2 it holds that p̄1 ≥ p̄2 and
v T p̄1 ≥ v T p̄2 for every vector v > 0. Moreover, if D1 is the set of the default nodes of N1 , then the
vector δ̃ = 1D1 (c1 − c2 ) ≥ 0 satisfies p̄1 ≥ p̄2 + δ̃, and subsequently v T (p̄1 − p̄2 ) ≥ v T δ̃.

1.2

Analytical Overview of Contributions

Having set up the model we are ready to state our contributions in detail:
1. We set up the optimization problem of discrete bailouts regarding various objectives (Section 1.3), and prove that the corresponding optimization problems are NP-Hard (Section 2.1
and Theorem 1). The problems correspond to general Mixed-Integer-Linear-Stochastic-Programming Problems (MILSP).
2. We propose two families of approximation algorithms:
(a) Based on the continuous relaxations of the aforementioned problems we design algorithms with the following approximation guarantees based on simple randomized round6

ing schemes (Section 2.3): More specifically, for every linear objective we introduce a
family of algorithms with approximation ratios equal to 1−βζmax − o(1).
(b) For a linear objective v T p̄ with v > 0, a greedy hill-climbing algorithm that bails
out the node with the largest marginal gain achieves an approximation ratio of 1 −
e−(1−βmax )/ζ − o(1) under the reasonable condition that a Small-Bailout Regime holds.
This subsequently provides approximation guarantees for all the linear objective functions presented in Section 1.1 (Section 2.4, Corollary 3 and Theorem 6).
(c) For the (AS) objective we prove that it is inapproximable within a factor k+n·a(|I|)
=
k+n
Ω (a(|I|)), where a(|I|) ∈ N∗ is a poly-time computable function of the input instance
size |I|, unless P = NP. (Section 2.5 and Theorem 7).
3. We impose fairness constraints to our model in terms of the Gini Coefficient. We study three
alternative indices on bailouts related to the overall inequality of the network, inequality over
groups with a certain property (e.g. minority status), as well spatial inequality (i.e. each
node with respect to its neighbors). We define a Price of Fairness with respect tho these
measures, and study the behaviour of the PoF in these cases, connecting it with ideas from
spectral graph theory. We show that in the discrete case there exist instances where the PoF
is unbounded. On the contrary, in the fractional case, the PoF is always finite for increasing
objectives f (p̄) with f (0) = 0 (such as in the case of linear objectives with v > 0).
4. We run our algorithms on real-world and semi-artificial data and get insights about the
corresponding networks under the presence of random shocks and quantify their behaviour.
Moreover, we track the evolution of the variables of the continuous relaxations with respect
to the changes in number of to-be-bailed-out nodes and devise fairness measures regarding
the fair allocation of stimulus checks. (Section 4)

1.3

The Generalized Discrete Bailouts Problem

We define Generalized Discrete Bailouts Problem (GDBP) as the solution to the following MixedInteger-Linear-Stochastic-Programming Problem, parametrized by the strictly increasing function
f (see Section 1.1), the budget Λ, the relative liabilities A, the total liabilities p, a stimulus vector
L > 0, and the shock distribution D, with decision variables the continuous clearing payments p̄i ,
and the bailout indicator variables z̄i ∈ {0, 1}.
max EX∼D [f (p̄)]
s.t. p̄ ≤ AT p̄ + c − X + LT z̄
0 ≤ p̄ ≤ p

(GDBP)

T

L z̄ ≤ Λ
z̄ ∈ {0, 1}n .
We denote the overall vector as ξ¯ = (p̄, z̄)T ∈ [0, p] × {0, 1}n . Note that bailing out a solvent
node does not increase the objective, so the support of z̄ is a subset of the default nodes. The
continuous relaxation (RGDBP) of (GDBP) allows z̄i to range in the continuous [0, 1] interval,

7

where the variables that refer to the relaxation are denoted by ξe = (e
p, ze);
max EX∼D [f (e
p)]
s.t. pe ≤ AT pe + c − X + LT ze
0 ≤ pe ≤ p

(RGDBP)

LT ze ≤ Λ
0 ≤ ze ≤ 1.
Given an optimal solution OPTf to (GDBP) and an optimal solution OPTRelf to (RGDBP)
the optimality conditions impose that OPTRelf ≥ OPTf since the relaxed problem has a larger
feasible region. A feasible solution of (GDBP) is denoted by SOLf and satisfies SOLf ≤ OPTf .
e an optimal solution by ξ¯∗ (resp. ξe∗ ). For
Finally, we denote a feasible solution by ξ¯ (resp. ξ),
warm-up we give the following example (see Figure 2). Note that if L = ` · 1 then the problem
reduces to finding at most k = Λ/` nodes such that the corresponding objective is maximized.
Example 1 (see Figure 2). Let Λ = 1, L = 1 and the network that has nodes {1, 2}, external assets
c = (1.5, 0)T , external liabilities b = (0.5, 1)T and internal liability matrix P with pij = 1{i = j =
1}. A point mass shock x = (1, 0)T hits the network and causes node 1 to default, which in turn
causes node 2 to default. Thus node 1 can only pay 1/3 to node 2 and 1/6 to the external creditors
and, in turn, node 2 can only pay 1/3 to its external creditors. The optimal solution is to bailout
node 1 with 1 unit of cash at which case everyone is solvent, since the shock is fully averted. We
have that OPTSoP = 5/2, OPTAS = 2, OPTFS = 2, OPTSoT = 3/2, and OPTSoIP = 1.
Decision version of (GDBP). The decision version of the problem takes as input a network with
liability matrix P , external assets c, internal assets b, a stimulus vector L > 0, a stimulus value a
budget Λ ≥ 0 to be used for the bailouts, a shock distribution D, and a lower bound f ∗ and answers
YES if and only if there exists a set of nodes which are bailed out with L such that in equilibrium
we have that EX∼D [f (p̄)] ≥ f ∗ .
Construction of L. The construction of the bailout vector L differs according to cases. For
instance, the stimulus Lj of an node j ∈ [n] under the CARES Act was determined as follows:
First a $1,200 check was allocated to individuals without children, with income at most $75K
whose amount faded out until the value of $100K where for incomes greater than $100K the value
of the stimulus was zero. Married couples filing tax returns jointly received $2.4K if their income
was less than $150K. For each dependent (child) an extra of $500 was allocated. In other cases,
such as in the case of Greece, an employee of a business that shut down due to COVID-19 received
a bailout of a fixed amount. The above policies suggest computing the stimuli as a product of a
non-negative value that depends on the node’s features and a binary decision variable that captures
some other property. In a system with “societal granularity” the bailouts tend not to be fractionally
dependent on the structure of the underlying financial network between people, mainly for equity
issues, since, for instance, two households with identical features (e.g. number of dependents) shall
get equal bailouts. In large-scale financial systems (e.g. banks, economy sectors) fractional bailouts
are considered and fractional allocation of bailouts subject to the EN model has been attempted
in the past (see Section 5.3). This choice of bailouts creates a very interesting computational
dichotomy, namely the fractional bailout allocation problem is efficiently solvable, whereas the
discrete bailout allocation problem is not, as we prove in Section 2.1.
Remark. In contrast to earlier works, we do a slight change in notation. Earlier works, denote
the clearing vector with p instead of p̄ and the total liabilities with p̄ instead of p. We make this
change in notation to improve readability, and ease the reader in distinguishing the input variables,
from the clearing vectors in the discrete bailouts case, and the fractional case.
8

2

Approximation Algorithms

We prove that the problem of determining the optimal bailouts when these are dictated by discrete
decisions is NP-Hard. To mitigate this problem, we develop approximation algorithms. Our first
algorithm is based on randomized rounding. More specifically, we solve the relaxation problem given
in (RGDBP) and then we apply a randomized rounding scheme by rounding the decision variables
randomly with coin flips of bias equal to their optimal values. While the rounding scheme itself
is straightforward, the analysis is more subtle, and shows that this algorithm achieves an approximation guarantee of 1−βζmax − o(1) for every linear objective given by a vector v > 0 of coefficients
with ζ = ζ(v). Our second result is a greedy algorithm that always chooses the node with the
maximum gain in welfare subject to respecting the budget constraints. This type of algorithm has
been extensively used for maximizing monotone submodular functions subject to cardinality constraints [NW78], such as influence maximization problems [KKT03], outbreak detection [LKG+ 07],
facility location problems [CNW83], and many more, and has an approximation ratio equal to
1 − 1/e. Although the family of linear objectives we analyze in this paper is not submodular in
general, we are able to adapt the style of analysis using the properties of Φ and Lemma 1, as we
− 1−βmax

ζ
describe in Section 2.4. The algorithm achieves an approximation guarantee of 1 − e
− o(1)
under a reasonable condition. In addition, the hill-climbing algorithm is efficient and performs
very well, outperforming all the network-based heuristics. The rounding algorithm has a better
worst-case approximation ratio in our theoretical bounds than the greedy algorithm, however it
performs slightly worse than it empirically. Finally, we conclude our results by an inapproximability result for the (non-linear) (AS) objective which resembles inapproximability results appearing
in generalized influence maximization problems where the objective in question is not submodular.

2.1

NP-Hardness Results

In this Section, we prove that maximizing the objectives defined in Section 1.2 is computationally intractable. To give intuition on the reduction and its construction, we proceed by giving a
polynomial-time reduction from a variant of the Set-Cover problem to the decision version for the
maximization of the (FS) objective. Our reduction resembles the reduction presented in [KKT03]
for the Influence Maximization problem, and can be generalized to the other objectives described
in Section 1.2, whose proof sketches we defer to Appendix A. The reduction is outlined below:
The 3-Set-Cover Problem. The (NP-Hard) 3-Set-Cover problem is a variation of the classical
Set-Cover problem [Kar75], and is described as follows:
Given a collection of n items U = {u1 , . . . , un } a collection of m ≤ n sets S1 , . . . , Sm ⊆ U
such S
that |Si | = 3 for every i ∈ [m], does there exist a collection I of |I| = k sets such
that i∈I Si = U?
We prove that:
Theorem 1 (see Figure 3). Under A2, maximizing the (L-OBJ) and (AS) objectives are NP-Hard.

2.2

Threshold Rules Can Have Bad Performance

A natural policy to allocate subsidies is through a threshold policy, regarding one’s existing equity,
that is, if the initial equity wj of a node is below a threshold θ then this node is bailed out with a
stimulus check of value Lj . Thus, the designer’s choice should be to order nodes by their equities
in ascending (or descending) order and bailout as many nodes as possible, in this order, subject to
9

u1
c 1 = x1 = 3
b1 = α
b2 = α
c 2 = x2 = 3

S1

u2

S2

u3

u4

Figure 3: Reduction Construction of Theorem 1 for two sets S1 = {u1 , u2 , u3 }, S2 = {u2 , u3 , u4 },
and four items {u1 , u2 , u3 , u4 }. Here α ∈ (0, 3). Red edges represent a liability of 1 − α/3. White
nodes represent the external sector (either as assets or liabilities). The financial connectivities are
βSj = 1 − α/3 ∈ (0, 1) for j ∈ [2], and βuj = 0, for j ∈ [4], and βmax = 1 − α/3 < 1.
the budget, breaking ties consistently. Such a policy can perform arbitrarily bad. To observe this,
consider a network G on n nodes which is built as follows: It consists of an isolated node v1 with
c1 = b1 = 1 and a directed path v2 → vn where bj = ε/2 for 2 ≤ j ≤ n − 1, bn = 1 − (n − 1)ε + ε/2
and c2 = 1. We let X = c, L = 1, v = 1 and Λ = 1. The optimal policy bails out v2 and achieves a
value of O(n) whereas the naive policy bails out node v1 and achieves an objective of 1. The gap
grows unbounded as n → ∞. The same (bad) example can be altered to fool the policy which bails
out nodes in descending order of their equity.

2.3

Approximation Algorithms Based on Randomized Rounding

In this Section, we prove approximation guarantees based on randomized rounding. Briefly, we
solve the relaxation problem where the indicator variables z̄i are allowed to range in [0, 1], as
presented in (RGDBP). If ξe∗ = (e
p∗ , ze∗ )T is the optimal solution to the relaxation problem, we
construct the corresponding rounded solution based on setting each rounded variable Zi equal to 1
∗9
with probability
zei∗ and
hP
i 0 with probability 1 − zei . To make our algorithm efficient we will allow
EZ∼Be(ez ∗ )
i∈[n] Zi to deviate from Λ by some factor. For the approximation ratio analysis of
the most algorithms, the following technical Lemmas are useful; their proofs, along with the proofs
of the other results in this section, are deferred to Appendix A.
Lemma 2 (Upper Bound Lemma). Let ξe = (e
p, ze)T be any feasible solution to the (RGDBP)
problem,
P and let X = x be
P a point-mass shock. Then, for every S ⊆ [n] the following inequality
holds:
pj ≤ j∈S (cj − xj + Lj zej ).
j∈S (1 − βj )e
Lemma 3 (Lower Bound Lemma). Let ξe∗ = (e
p∗ , ze∗ )T be an optimal solution to the (RGDBP)
problem under a point-mass shock X = x and let j be a default node. Then pe∗j ≥ cj − xj + Lj zej∗ .
Moreover if Zj ∼ Be(e
zj∗ ) and p̄j is the corresponding (feasible) clearing solution of (GDBP) after
rounding, then, conditioned on the fact that j is default we have that p̄j ≥ cj − xj + Lj Zj , and
subsequently EZ∼Be(ez ∗ ) [p̄j ] ≥ cj − xj + Lj zej∗ .
We start by giving an approximation guarantee for the expected costs of the rounded solutions.
When we condition upon a shock X = x we overload the notation SOL, OPT, OPTRel with
SOL(x), OPT(x), OPTRel (x), so that SOL = EX∼D [SOL(X)] (resp. OPT = EX∼D [OPT(X)], and
OPTRel = EX∼D [OPTRel (X)]). Again, we refer to Appendix A for the proofs of the results in this
section.
In fact, any rounding scheme such that E [Zi ] ≥ zei∗ would yield the same approximation ratio guarantee on
expectation.
9

10

Theorem 2. The following results hold for the expected costs under the rounded variables Z given
a fixed shock X = x, for a linear objective f (p̄) = v T p̄ with v > 0:


1 − βmax
· OPTf (x).
EZ∼Be(ez ∗ ) [SOLf (x)] ≥
ζ
We remind that ζ = ζ(v) = vvmax
≥ 1 is related to how “well-conditioned” the linear objective
min
is. Taking expectations over X ∼ D, we get the approximation guarantees for the objectives
Corollary 1 (Approximation Guarantee for Rounding Algorithm). The randomized rounding algorithm achieves the following approximation ratios
1. SOLSoP ≥ ρSoP · OPTSoP where ρSoP = 1 − βmax ;
)βmin
2. SOLSoIP ≥ ρSoIP · OPTSoIP where ρSoIP = (1−ββmax
;
max
(1−βmax )2
1−βmin ;
(1−βmax )pmax
.
pmin

3. SOLSoT ≥ ρSoT · OPTSoT where ρSoT =
4. SOLFS ≥ ρFS · OPTFS where ρFS =

To prove this corollary it suffices to calculate ζ(1) = 1 for (SoP), ζ(β) =

βmax
βmin

for (SoIP),

pmin
pmax

1−βmin
1−βmax

for (SoT), ζ(1 p) =
for (FS).
ζ(1 − β) =
Runtime Analysis. To simulate the algorithms based on randomized rounding with the approximation guarantees devised in Theorem 2, we draw m samples from D and with realizing each
sample we do T runs, such that at the end of the T runs the computed solution is correct with high
probability, i.e. probability that goes to 0 as n → ∞. Given an oracle that solves the corresponding
relaxed problems in time T (n) is O(m(T + kT )) where k is the maximum number of items that
the algorithm selects and the O(k) cost is paid to check the feasibility of the budget constraint. If
the bailouts are equal, i.e. L = ` · 1 for some ` > 0 we know that k = Λ/`.
Theorem 3 analyzes the runtime of the randomized rounding algorithm. We break the runtime
analysis into two pieces to improve readability: First, we analyze the rounding algorithm for equal
bailouts using independent rounding in which case we can directly make use of Chernoff bounds
for the concentration of LT Z. Second, we analyze the same algorithm for different values of the
bailouts. The difference with the case of equal bailouts is that if Z has independent components,
then LT Z may not be concentrated and the runtime of the independent rounding scheme may
explode. To remedy this problem, we make use of the oracle presented in [Sri01] to do dependent
rounding on the variables. We adjust the oracle in a way that (i) the result of Theorem 2 continues
to hold after the dependent rounding procedure, (ii) LT Z obeys Chernoff-like concentration.
In
√
e
both of our rounding schemes, the planner is allowed to use an extra budget of O( Λ) added to the
available budget of Λ. Moreover, due to the simulating the expected value of the rounded and the
optimal solution, the approximation factor drops by an additive factor of O(ε) for some sufficiently
small ε. We present the Theorem below:
Theorem 3. Let x(1) , . . . , x(m) be m samples drawn from D, let A be a randomized rounding
algo-

rithm as in Theorem 2 to maximize an objective f with kf k∞ < ∞, for which EZ∼Be(ez ∗ ) SOLf (x(i) ) ≥
(1 − γf ) · OPTf (x(i) ) for some γf ∈ (0, 1), and for all i ∈ [m]. Then the estimator SOLf =
log n
1 P
(i)
i∈[m] SOLf (x ) satisfies SOLf ≥ (1 − γf − 2ε) · OPTf for m = ε2 with probability 1 −
m
√

e T2 + k4 time, and uses at most Λ + O(
e Λ) budget.
O(log n/(ε2 n)), runs in O
ε

ε

Integrality Gap. The integrality gap of the problem can be used to quantify the worst case ratio
between a fractional optimal solution and an integral optimal solution. The integrality gap is given
by
11

σf =

max

instances I

OPTRelf (I)
OPTf (I)

and is well studied in the theory of approximation algorithms (see [WS11] and the references
therein). From a first glance, we expect that fractionally allocating stimulus is in general much
more efficient than giving the stimulus to certain individuals; even in the optimal case. Indeed,
this intuition is true theoretically, as we show that for every linear objective the integrality gap can
become unbounded:
Theorem 4 (Integrality Gap). For every ε ∈ (0, 1), n ∈ N∗ and k = o(n) there exists an instance
for which the integrality gap for the linear objective f (p̄) = v T p̄, with v > 0, is unbounded as n → ∞
and ε → 1. Subsequently σf → ∞.

2.4

Greedy Approximation Algorithms

We consider a family of greedy hill-climbing algorithms in order to find the optimal bailout set for
a linear objective f (p̄) = v T p̄, where v > 0. These algorithms run in k ≤ Λ/Lmin steps, and at
each step they pick the (feasible) element with the largest marginal gain until the budget constraint
is violated, namely given a fixed shock X
 = x where the current set of bailouts is St , with S0 =
∅, we have ut+1 ∈ argmaxu∈[n]\St feasible v T p̄St ∪{u} − v T p̄St . This algorithm resembles the hillclimbing nature of [NW78] for constrained monotone submodular maximization which guarantees
an (1−1/e)-approximation ratio. Below we prove that under a Small-Bailout Regime our algorithms
achieve an approximation ratio of 1 − e−(1−βmax )/ζ − o(1). We first state the Small-Bailout Regime
condition, that suggests that whenever a node is bailed out, it remains default after the bailout
(however with a greater value). It is important to note here that the hill climbing family of
algorithms we consider would still work if run on an instance that violates the Small-Bailout
Regime, and, in practice, the greedy hill climbing algorithm is the best-performing one, however
the theoretical guarantee would hold only when this condition holds. For the rest of the analysis
we fix a point-mass shock X = x. Given an approximation guarantee for the fixed shock x we can
later argue that this guarantee is achieved in expectation. We state our assumption
Condition 1 (Small-Bailout Regime). Under the randomness of D with probability 1, for every
step t the node ut selected by the algorithm yields a clearing vector p̄St such that p̄St ,ut < put .
This condition says that every node ut we include at iteration t does not get “saturated” in the
context of the constraint p̄St ,ut = put holding and the default constraint being inactive. This fact
would help us establish a lower bound on the marginal gain. We state the following Corollary which
is obtained by applying Lemma 1 with asset vectors c1 = c−x+LT 1St ∪{ut } and c2 = c−x+LT 1St .
Corollary 2. Under C1 and a fixed shock X = x, the marginal gain at each iteration t ≤ k where
node ut is chosen is at least vmin Lut .
Next, we prove the following upper bound regarding bailing out two sets S, T with S ⊆ T .
Lemma 4 (Marginal Gain Upper Bound Lemma). Let f (p̄) = v T p̄ for some v > 0 and let ∅ ⊆
S ⊆ T ⊆ [n] be two sets, and fix a shock X = x. Then if p̄T is the clearing vector
after bailing out
P
T and p̄S is the clearing vector after bailing out S, then v T p̄T − v T p̄S ≤

vmax j∈T \S Lj
.
1−βmax

We use the above lemma to state the following Corollary which results as a combination of
Corollary 2 and Lemma 4.
12

Lemma 5. Let S be a set and S ∗ be the optimal bailout set setunder a fixed shock X = x. Then,
T
T
under C1 we have that v T p̄S ∗ − v T p̄S ≤ 1−βζkmax maxu∈S,u
/
feasible v p̄S∪{u} − v p̄S .
We use Lemma 4 to conclude the approximation ratios, and prove
Theorem 5. Under C1, for every linear objective the greedy hill-climbing algorithm achieves an
approximation guarantee (under the randomness of D, i.e. X ∼ D) of

1−βmax 
−
ζ
SOLf ≥ 1 − e
· OPTf .
This yields the following Corollary:
Corollary 3 (Approximation Guarantee for Greedy Algorithm). Under C1, the greedy algorithm
achieves the following approximation ratios
1. SOLSoP ≥ (1 − e−ρSoP ) OPTSoP where ρSoP = 1 − βmax ;
)βmin
;
2. SOLSoIP ≥ (1 − e−ρSoIP ) OPTSoIP where ρSoIP = (1−ββmax
max
(1−βmax )2
1−βmin ;
(1−βmax )pmax
.
pmin

3. SOLSoT ≥ (1 − e−ρSoT ) OPTSoT where ρSoT =
4. SOLFS ≥ (1 − e−ρFS ) OPTFS where ρFS =

Runtime Analysis. To run the algorithm in a simulation environment we calculate the expectation at each round using the sample average over m i.i.d. samples from D. Since by the problem
constraints imply that 0 ≤ p̄ ≤ p then approximating the expectation within an accuracy ε > 0
with probability of failure at most δ > 0 requires O(kf k2∞ log(1/δ)/ε2 ) samples to be averaged,
where kf k∞ = f (p). The approximation ratio is off by an additive factor of o(1) due to the approximation of the expectation via samples. We can apply a slight variation of Lemma 3.6 of [BBCL14]
to get a similar guarantee.
i.i.d.

Theorem 6. Let x(1) , . . . , x(m) ∼ D, let A be a greedy algorithm of the form of Theorem 5 to
maximize a linear objective f in p̄ with kf k∞ < ∞ and for which SOLf x(i) ≥ (1−γf )·OPTf x(i)
1 P
(i)
for some γf ∈ (0, 1), and for all i ∈ [m]. Then the estimator SOLf = m
i∈[m] SOLf (x ) is an
e
(1 − γf − 2ε)-approximation

 for m = O(Λ/Lmin ) samples, succeeds with probability 1 − O(1/n), and
2
(n+|E|)Λ
e
runs in time O
via invoking the fixed point operator Φ.
Lmin ε2
Comparison with Randomized Rounding. We have proved that for a linear objective v T p̄
the greedy algorithm yields an approximation ratio (on expectation) of 1 − e−(1−βmax )/ζ (under C1)
whereas the randomized rounding algorithm achieves an approximation ratio (on expectation) of
(1 − βmax )/ζ. Using the fact that e−x ≥ 1 − x for all x ∈ R we have that always (1 − βmax )/ζ ≥ 1 −
e−(1−βmax )/ζ , so the approximation ratio of the randomized rounding algorithm is always better than
the approximation ratio of the greedy algorithm through the present analysis. On the contrary, on
the experimental instances of Section 4, the greedy algorithm nearly always outperforms randomized
rounding.

2.5

An Inapproximability Result for (AS)



In the previous Section we devised an 1−βζmax − o(1) -approximation rounding algorithm for the
(L-OBJ) objective. However, if we ask a similar question for the (AS) objective — namely, whether
there is an approximation algorithm for optimizing the (AS) objective — we obtain a negative
answer. We prove that it is NP-hard to approximate a solution to the (AS) objective within any
poly-time computable function of the input.
13

c 1 = x1 = 3
b1 = α

..
.
bm = α
c m = xm = 3

u1,1

u2,1

ua,1

u1,2

u2,2

ua,2

..
.

..
.

u1,n−1

u2,n−1

ua,n−1

u1,n

u2,n

ua,n

v1

...

..
.

vm

a(|I|) Fully-connected Layers

Figure 4: Proof of Theorem 7 (see Appendix A for the full proof). Here α ∈ (0, 3) and a(|I|) ∈ N∗ is
a poly-time computable function of the input instance size |I|. The red edges represent liabilities of
value 1 − α/3. Gray edges represent liabilities of value 1−α/3
n . The maximum financial connectivity
is βmax = 1 − α/3 < 1. A YES answer to the 3-Set-Cover problem implies at least k + a(|I|) · n
solvent nodes, whereas a NO answer implies at most k + n solvent nodes.
Theorem 7 ((AS) Inapproximability (Figure 4)). The (AS) problem cannot be approximated within
a factor of k+a(|I|)n
= Ω(a(|I|)) for every poly-time computable function a(|I|) ∈ N∗ of the input
k+n
instance size |I|, unless P = NP.

3
3.1

Fairness
Fairness Metrics

In this paper, we say that an allocation is fair across the nodes if the allocation (discrete or
fractional) obeys the following property: the amount of bailouts that a certain node gets “does
not differ a lot from its neighbors” where the notion of neighborhood here is general and does
not necessarily refers to the network “neighborhood“ as devised by the EN model. For instance,
the “neighborhood” may refer to measuring inequality among all pairs of nodes, nodes who have
a specific property, and nodes in the actual network. All metrics have to be homogeneous, i.e.
multiplying all the bailouts by a positive number should not affect their value. We consider the
following constraints to incorporate fairness in our model:
Gini Coefficient. The Gini Coefficient [Gin21] measures the fairness of the stimulus allocations
between all pairs of nodes and is defined to be
P
i,j∈[n] |Li z̄i − Lj z̄j |
P
GC(z̄) =
.
(GC)
2n j∈[n] (Lj z̄j )
It evaluates to 0 when the stimuli are equally distributed and is 1 − 1/n when one node gets
all the bailout amount. APuseful optimization constraint
P is making the Gini coefficient at most
g ∈ [0, 1], or equivalently i,j∈[n] |Li z̄i − Lj z̄j | ≤ 2ng j∈[n] Lj z̄j . We say that an allocation for
which the Gini Coefficient is at most g is a g-unfair allocation. We note that a possible disadvantage
of this metric is that it does not take into account each individual node’s debts, i.e. it treats all
nodes on an equalized basis. This issue is mitigated by the (SGC) metric which is presented below.
Property Gini Coefficient. The collection of real-world data from SafeGraph and the US Census
we present in Section 4 consists of attributes characterize nodes. One of the key attributes in
these datasets is the minority status of the owner of a business, if such business participates at

14

the network as a node, or the demographic characteristics of a group of people, for instance the
fraction of people belonging to a minority group within a Census Block Group under which we
want to impose fairness constraints. (That is, to measure the relative assistance between different
groups, in an approximate way).
This type of data motivates the following metric: We introduce the Property Gini Coefficient
(PGC) in which nodes may have a property of interest (such as the demographic group in the
SafeGraph or Census data) along which we want to apply an equity analysis. We model this by a
property vector q ∈ [0, 1]n , where each element
P qj corresponds to the probability that node j ∈ [n]
has this property, as follows: We let nq = j∈[n] qj and n¬q = n − nq be the total weights of the
(soft) bipartition.
P For every node j ∈ [n] is inequality subject to being in the minority group is
given as 2n1¬q i∈[n] (1 − qi )|Li z̄i − Lj z̄j |. The sum over all j with weight qj give the numerator of
P
the (PGC). The denominator of the (PGC) is j∈[n] qj Lj z̄j :
P
j,i∈[n] qj (1 − qi )|Li z̄i − Lj z̄j |
P
PGC(z̄; q) =
.
(PGC)
2 (n − nq ) · j∈[n] qj Lj z̄j
Note that taking q = 12 · 1 reduces (PGC) to the conventional GC. Moreover, for L = ` · 1
and q T z̄ = 0 we observe that (PGC) becomes unbounded since the denominator goes to zero. One
case where this happens, and further justifies the correctness of the criterion is when q and z̄ are
a 0/1 vector and where the entries of q are 1 the entries of z̄ are 0, and vice-versa, where the
entries of q are 0 the entries of z̄ are 1, which corresponds to giving all the bailouts to the majority
group. We say that an allocation which achieves a (PGC) at most g subject to a property q is
(g, q)-unfair. Note that the (PGC) constraint can be combined with the (GC) constraint as follows:
given gbetween , gwithin ≥ 0 we seek to find an allocation that respects both between-fairness, i.e.
PGC(z̄; q) ≤ gbetween , and within-fairness, i.e. GC(z̄ q) ≤ gwithin and GC(z̄ (1 − q)) ≤ gwithin .
Spatial Gini Coefficient. To make the GC take into account network effects, we define its spatial
analogue, the (SGC), to be
P
(j,i)∈E aji |Lj z̄j − Li z̄i |
P
SGC(z̄; A) =
(SGC)
2 j∈[n] βj Lj z̄j
The aforementioned definition also appears in [MMD+ 21] where the graph is assumed to have
unit weights. In our case, the role of the unweighted graph plays the relative liability matrix A.
Since A is substochastic the total
P weight of each row is βi < 1 and the contribution of edge (i, j) is
aij . Normalizing by the sum j∈[n] βj Lj z̄j allows for comparing different population groups and
different bailout magnitudes. When the bailouts are distributed equally (SGC) is 0. If A = AT and
one node gets all the bailouts, then the (SGC) is bounded by 1. We say that an allocation which
achieves an (SGC) of at most g is (g, A)-unfair. We note here that unlike (GC), the (SGC) metric
takes into account each node’s debt, that is a node j with a significant (compared to its neighbors)
liability to node i, i.e. it has aji ≈ βj , then this deviation will get a higher weight in the calculation
of the coefficient compared to j’s deviation from the rest of its neighbors.

3.2

Optimization Formulation

We formulate the following relaxations to the optimization problems involving the aforementioned
fairness metrics for a target fairness metric upper bound g ≥ 0. First, we consider the following optimization problem that extends (GDBP) by adding the GC-dependent constraints (resp. (RGDBP)

15

on the pe, ze variables):
max EX∼D [f (p̄)]
s.t.

(p̄, z̄) ∈ (GDBP) constraints
X
|Li z̄i − Lj z̄j | ≤ 2ngLT z̄,

(GC-Problem)

i,j∈[n]×[n]

max EX∼D [f (e
p)]
s.t.

(e
p, ze) ∈ (RGDBP) constraints
X
$
e ij ≤ 2ngLT ze

(GC-Relaxation)

i,j∈[n]×[n]

$
e ij ≥ 0, −$
e ij ≤ Li zei − Lj zej ≤ $
e ij

(i, j) ∈ [n] × [n].

The optimization problem relaxations can be formulated mutandis mutandis for Equations (PGC)
and (SGC) for a fairness bound g ≥ 0. The optimal fractional solutions to these problems consider
fractional allocations whereas their discrete counterparts consider discrete allocations. Using the
same rounding scheme as presented in Section 2.3, we can deduce approximation guarantees for
the (GC-Problem). We present such a result for (GC). As in the previous section, we refer to
Appendix A for all proofs in this section.
Theorem 8. Let ze∗ (g) be the optimal solutions to (GC-Problem), for f being the (L-OBJ) objective,
Λ
and g > 0 being a fairness constraint. Then,
 if all bailouts are equal,
 i.e. L = ` · 1 with k = ` , and
for g > 1 − nk , the GC problem admits an

(1−βmax )(1−k/n)
gζ

− o(1) -approximation algorithm.

One can extend the runtime analysis of this algorithm (Theorem 3) with the incorporation of the
finite differences inequality [Doo40] to devise high probability bounds for the deviation constraint.
We believe that further runtime analysis lies beyond the main points of this paper.

3.3

Price of Fairness

A natural question we might ask is, “What is the maximum effect of these fairness constraints on
the welfare objective function?”. We define the Price of Fairness (PoF) [BFT11] to be

PoF =

Optimal Value when sans Fairness Constraint
.
Optimum value when the solutinion is g-unfair (resp. (g, q)-unfair / (g, A)-unfair)

It is evident that the since the value in the numerator refers to an optimization problem with a
larger feasible region than the one in the denominator that always PoF ≥ 1. At glance, a natural
question arises: Do there exist instances for which PoF is unbounded in the discrete case, i.e.
PoF = ∞? The answer to this question is affirmative for the discrete case and can be illustrated
through the following Theorem.
Theorem 9 (Unbounded Discrete PoF Instances). There exist finite instances where the discrete
PoF is unbounded, for all the constraints defined by (GC), (PGC), and (SGC), and any linear
objective given by a vector of coefficients v > 0.

16

A natural follow-up question that arises is the following: Does there exist a bound on the PoF
regarding fractional allocations? An answer to this question provide the following Theorems
Theorem 10 (Fractional PoF Boundedness). For every increasing objective f with f (0) = 0,
kf k∞ < ∞, and for every g ≥ 0 the PoF for fractional bailouts is bounded (for all fairness
metrics).
In the sequel, the next question is the derivation of a PoF bound for the fractional case. The
following result gives such a bound for the (GC) case and the (SGC) case when the relative liability
matrix is symmetric and the network G is connected and A = AT . The proof of this Theorem
depends on two steps. Firstly, given that all the optimal fractional allocations consume a certain
budget µ ∈ (0, Λ] there could be two cases: either the budget is distributed equally or at least one
µ
node gets a budget different than nL
. In the former case, to get the desired PoF bound we create
j
a feasible solution by perturbing the values of the optimal solution allocations so they fall into the
second case (i.e. create a suboptimal solution) establishing, this way, an upper bound to the PoF.
We state the Theorem below:
Theorem 11 (Fractional PoF Bounds). Given a finite instance I of the EN model with g > 0 the
following upper bounds are true for any linear objective given by a coefficient vector v > 0: There
exists a finite C(I) > 0 such that:
• For the (GC) constraint
√
2ζg(kck1 + Λ) n
PoFg ≤ C(I) ·
.
(1 − βmax )
• For the (SGC) constraint, if A = AT and G is connected, we have that
√
2ζg(kck1 + Λ)βmax
PoF(g,A) ≤ C(I) ·
.
(1 − βmax )φ(A(2) )
Where φ(A(2) ) is the conductance of the graph with adjacency matrix A(2) .
The above Theorem relates the PoF for a (g, A)-unfair allocation when A is symmetric with two
quantities of a topological nature: the former one is the conductance (of A(2) ), a quantity widely
used in spectral graph theory in order to justify the mixing time of random walks, and the latter
one is βmax , i.e. the maximum row sum of A. Note that the conductance of A(2) and βmax are not
independent of one another. Another useful bound that relates the conductance of A with (SGC)
is given by the following Theorem, which stems from an alternative expression of conductance.
Appendix B.2 contains an algebraic proof of the argument, which is of independent interest. We
state the Theorem:
Theorem 12. Let A be a symmetric and connected network and let ze be any allocation for which
SGC(e
z ; A) 6= 0. Then
φ(A)
≤ SGC(e
z ; A).
2
Theorem 12 states that for A = AT (connected) (SGC) is at least an 1/2 factor away from
φ(A). Moreover, note that in general for Theorems 11 and 12 we have that φ(A) 6= φ(A(2) ).
In addition to these general bounds, it is also interesting to ask about the behaviour of the PoF
for simple topologies, the conductance of which we can control in a standardized manner. With
this in mind, we examine following example created from two equal-sized cliques with randomly
sampled edges between:
17

Figure 5: Randomized Construction of Example 2 with log-log fits.
Example 2. A graph on n nodes is built from 2 cliques of size n/2 between which edges are generated
randomly and independently with probability r ∈ (0, 1]. Each edge (j, i) is endowed with a liability
pji = 1, and each node j has cj = n, bj = 1, Lj = n/2, and Xj = 1{j ≤ n/2}. A log-log plot
relating the log-PoF with log r is presented in Figure 5. We observe that as the nodes become more
densely connected the PoF decreases. Moreover, note that for two target values g1 ≤ g2 for the
(SGC) constraint, the PoF curve relating to g1 is always above the PoF curve relating to g2 . This
is expected, as the latter problem has a larger feasible region and thus solutions of higher welfrare
can be achieved.

4
4.1

Experiments
Data

In this section, we evaluate our methods on public datasets from two kinds of sources: highlevel granularity data, among nodes corresponding to financial institutions in a country, financial
institutions between countries, or financial interactions between different financial sectors of the
same country; and lower granularity data, among nodes corresponding to anonymized groups of
people defined by the US Census. In the latter case, the relevant public datasets are constrained
through anonymization or aggregation due to the privacy considerations of the individuals.
We now present the datasets we analyze in this study. A more extensive data analysis of the
datasets can be found in Appendix C.
• German Banks. Datasets from 22 German banks from the work of [CWY16] where the
internal and external assets and liabilities are reported.
• EBA. Data from 76 banks that participated in the European Banking Authority (EBA) 2011
stress test, provided in [GY15]. The paper reports only the column sums a (aj are the total
internal assets of j) and the row sums l (lj are the total internal liabilities of j), assuming
that a = l. The network structure is unknown and are inferred using the systemicrisk
package, introduced in [GV17] which incorporates a Gibbs sampler to fit data where the row
and column sums of the liabilities are provided. Prior to fitting and sampling, we pertub the
liabilities vector l to ˆl as follows: We draw n i.i.d. Gaussians 1 , . . . , n with mean 0 and std 100

18

m
l
P
Tl
for j ∈ [n − 1] and we finally set ˆln = 1T l − j∈[n−1] ˆlj .
and we set each ˆlj = (lj + j ) 1T1(l+)
To fit the model with systemicrisk we used the complete network on n = 76 nodes.
• Venmo. Data from the Venmo social payments application10 . The data consists of 7M
nodes and 7M transactions between users of Venmo. Each transaction is encoded by its
sender u and its receiver v, represented by a directed edge (u, v) and a timestamp. The
payment amounts are not available in the original data, for data privacy reasons. Finally,
the data consists of hundred of thousands weakly-connected components, many of them with
very small size (less than 10 nodes). We keep the Venmo network of n ≈ 7M nodes and
m ≈ 7M edges by dropping the timestamp column (i.e. assuming a static setting). The
network has a power-law degree distribution (see Appendix C for more). We calculate all the
weakly-connected-components (WCCs) of the network. We observed that most of the WCCs
are very sparsely connected and small for which reason we decided to keep the component
Gi that has ni ≥ 50 nodes and has the maximum density mi /n2i that is equal to 4.44%.
Since the values of the entities were missing we generated values based on models of the
existing literature and the individual analysis that we had performed on the data sources
where data was available (see the distribution fits and regression plots in Appendix C). Using
this knowledge,
i.i.d. internal liabilities11 from Exp(1), (independent) external
 we generated

out · Exp(1), and external liabilities given by b = 0.9 · c 12 .
assets cj = din
j
j
j + dj
• SafeGraph. Data generated based on mobility data from the SafeGraph platform during
April 2020. The nodes in the financial network represent: (i) Points of Interest nodes (POI
nodes) that represent various businesses categorized by their NAICS codes 13 to categories
(i.e. grocery stores, banks, gas stations etc.) and the Census Block Group14 (CBG) they
are located at; (ii) CBG nodes that represent a set of households in a certain location. The
dataset is constructed by access to an initial pair of geographical coordinates (i.e. latitude
and longitude) and a number kkNN of neighboring CBGs. The POI nodes are determined
to be the businesses that are located in the kkNN -nearest neighboring CBGs based on the
Haversine distance metric. Each POI provides data about the CBGs of its unique visitors15
and the dwell times. For the source of its visitors we estimate the number of people that
come from each CBG. From the dwell times of devices that are available we determine the
percentage of people that work on the POIs and the ones who visit the POIs.
In this way, we estimate the number of people that work at a certain business and come from
a certain CBG and the number of people that visit the corresponding POI. For the former
category we create a financial liability edge from the POI to the CBG node to indicate the
payment of a liability (i.e. a wage) and for the latter category we create a liability edge
from the CBG node to the POI representing some form of expense (e.g. groceries). The
weights are multiplied accordingly to represent the set of people that interact with each POI.
The aforementioned process creates a bipartite network. Each CBG node is associated with
multiple data from the US Census, as well as every POI node is associated with data from
the US Economic Census. For each CBG node, we estimate the average size of households
10

The data has been public prior to this study, and available at: https://github.com/sa7mon/venmo-data. As of
2019, the API could be accessed with a GET request at https://venmo.com/api/v5/public.
11
The exponential internal liability model is also followed in the work of [GV17].
12
A linear relation between cj and bj is reasonable as evidence presented in Appendix C underline.
13
https://www.naics.com/search
14
A CBG is a unit used by the US Census. It is the smallest geographical unit for which the bureau publishes
sample data, i.e. data which is only collected from a fraction of all households and contains 600-3K people.
15
A unique visitor is a unique mobile device, i.e. each device is only counted once. We assume that each device
represents a distinct person.

19

per CBG, the average income level and the percentage of people that belong to a minority
group.
We use the above data to estimate the external assets and liabilities of the CBG nodes. For
the bailouts of CBG nodes we calculate a bailout devised by the CARES act that considers as
income the average income of the CBG and as size of household the average size of household
multiplied by an estimate for the number of people in that CBG who interact with the
POI nodes. Similarly, for the POI nodes we use data from the US Economic Census and
NAICS to determine average wages, income and expenses. For the bailouts of the POI
nodes we use loan data regarding loans that were given during April 2020 as part of the
SBA Paycheck Protection Program (PPP) provided by SafeGraph, adjusted to the number
of workers being present in the network and the span of one month. Moreover, the loan data
included demographic characteristics about the bussinesses in question so we were able to
determine (or estimate in the case of missing data) the minority status of a business, i.e.
the probability of a certain business being a business with a minority owner. A complete
description of the data generation process is presented in Appendix C.
Shocks. For all of our experiments we assume that the shocks Xj for each j ∈ [n] are independent
and each shock Xi is sampled from the uniform distribution with support [0, ci ]. We ran experiments
with the shocks being scaled Beta(1/2, 1/2) distributions and the results were similar, and thus were
omitted.
Heuristic Methods. We use the following heuristic methods to allocate stimulus, where for each
step we augment each set (based on the criteria below) maximally with respect to the budget
constraint.
• Wealth Policy. We sort the nodes in increasing order of initial wealth wi , nodes with the
lowest wealths each time. Note that we performed the same experiments with decreasing
order of wealths and the results were similar and thus omitted.
• Out-degree Policy. We take the nodes with the highest outdegrees.
• PageRank [PBMW99]. We take the k-top nodes in decreasing value of their PageRank values.
The calculation of PageRank takes into account the directionality of the graph.
• Eigenvector Centrality [Fre77]. We take the nodes in decreasing value of their eigenvector
centrality (i.e. values of the principal eigenvector of the corresponding random walk transition
matrix). The computation of the eigenvector centrality measure ignores directionality.
• Random Permutation. Baseline criterion that considers a random permutation of the nodes.
All the above policies are well known benchmarks and have been used on similar tasks such as
Influence Maximization [KKT03]. For all experiments we also report the upper bound of OPTf
which is the corresponding relaxation optimum OPTRelf that is an upper bound to the true optimum, which in our case is computationally intractable, that is if we used brute-force in the case
where the stimuli were equal, we would need to search over O(nk ) sets to find the optimal solution
for each k.

4.2

Experiments

Discrete Allocations. Firstly, for various values of the stimuli vector L, either fixed or varying,
we report the corresponding objective values averaged over multiple draws of shocks from the
corresponding shock distribution, where we report both the empirical mean and standard deviation
(std). We parametrize the available budget with a pair of parameters. The former parameter `
parametrizes the budget increase rate, and the latter parameter k parametrizes the multiplicity of
resources. We make assumptions about the bailouts that fall into two main categories: (i) fixed
bailouts: where L = ` · 1 and Λ(k) = ` · k and the number of bailouts k varies along the x-axis of
20

the plots. This is equivalent with bailing out at most k nodes on the network where every node
gets a stimulus value of `. (ii) variable bailouts: in the SafeGraph experiments we determine the
bailouts as discussed in the start of Section 4 and Appendix C. We assume that for each step k the
budget increases by some amount ` so, again, the available budget is Λ(k) = ` · k.
The results of these experiments for the various datasets are reported in Figures 6 to 9. In brief,
the greedy algorithm outperforms all other algorithms in all settings. Then, the rounding algorithm
comes second, outperforming the other heuristics. Finally, note that the Wealth Policy performs
very poorly due the fact that it is independent of the contagion process. This last observation is
consistent with the theoretical observations that we made in Section 2.2).
Fairness. Secondly, we perform experiments where we constrain the fairness of the model as
follows:
• For the German Banks dataset we run the following problems constraining the (GC). First,
we run the unconstrained optimization problem (i.e. allowing Gini to be at most 1), and,
second, we restrict the (GC) to be at most 0.1. We report the relaxation optimum, the
rounded solutions to the problem and the realized (GC) after the optimization on Figure 10.
• For the Venmo dara we run the following problems constraining the (SGC). First, we run
the unconstrained optimization problem (we allow the (SGC) to be at most 1 which in our
experiments suffices to produce the optimal solution in the unconstrained version), and, second, we restrict the (GC) to be at most 0.1. We report the relaxation optimum, the rounded
solutions to the problem and the realized (GC) after the optimization on Figure 10.
• For the SafeGraph and the German Banks data we use the fuzzy version of the (GC) where
the weights represent the probability that a node is a minority node. In other words, we want
to impose constraints between minority and non-minority groups. For the former dataset,
the weights q are the minority scores for each CBG and business. For the latter dataset,
the values of q are sampled i.i.d. from Beta(2, 5). We report the relaxation optimum and
the rounded solutions as well as the total income allocated to the minority group. We use
a budget increase rate of 104 for SafeGraph, similar to the one reported in Figure 9, and a
bailout L = 106 · 1 for German Banks. The results are reported on Figure 12.
• In Figure 13 we plot the relation between the upper bound on the (SGC) coefficient and the
PoF for the Venmo and German Banks Data.
Implementation and Environment. The source code is developed in the Python language
using NumPy for numerics, NetworkX for network analysis, and Google’s or-tools for optimization.
For solving linear programs we use the GLOP Linear Solver, and support multi-threading. The
experiments were run on a server with 144 cores and 1.5TB of RAM.
Source Code. [Ano21].

5

Discusion

5.1
5.1.1

Experimental Results
Discrete Allocations

Figure 6 shows the performance of the various algorithms on the German Banks dataset. In
both (SoP) and (SoIP) the greedy algorithm outperforms all the other algorithms algorithms,
and then come the centrality and PageRank-based heuristics, as well as the LP with randomized
rounding. Note that this network is strongly connected and all edges have both directions so the
eigenvector centrality and the PageRank rankings coincide. In the worst case, the greedy algorithm
outperforms the LP-based one by approximately 15%, whereas the PageRank and centrality-based

21

(a) SoP Objective for L = 106 · 1

(b) SoIP Objective for L = 106 · 1

Figure 6: German Banks Dataset results with uniform shocks. The simulations have been run for
1000 iterations. The error areas represent 1 std.
heuristics are outperformed by greedy by approximately 58% in the worst case. Finally, we note
that in both objectives the random permutation, wealths and outdegree heuristic perform badly.
The significantly decreased performance of the wealth heuristic is justified by the fact that nodes
which are important for the bailout process and have priority (i.e. lower wealth) are not wellconnected which is in accord with theory, where an instance can be constructed such that the
approximation ratio of the wealth heuristic goes to zero. Similarly, one can argue about the other
heuristics, i.e. in the case of the out-degree heuristic nodes with low equity may be well connected
to the other nodes and thus giving them priority does not contribute substantially to the overall
objective. That suggest that the uneven form of such curves represent the fact that these simple
heuristics are not good candidates for this optimization problem. The results regarding (SoT) are
similar, so we omit them.
Figure 7 shows the results on the simulations run on the Venmo network. We observe that both
the greedy and the randomized rounding algorithm outperform the other algorithms by a margin
of approximately 13%. The difference between the greedy and randomized rounding algorithms
is approximately 3.2%. Again we note that the other heuristics do not perform as well as the
greedy or randomized rounding algorithms for the reasons outlined in the previous paragraph.
More precisely, the PageRank and centrality-based heuristics perform substantially bad perhaps
due to the connectivity characteristics of the network (i.e. the network being too sparse).
Figure 8 displays results about the EBA dataset with inferred internal liabilities based on a
complete network backbone. In this type of network, the outdegree, the PageRank, and centrality
heuristics are equivalent to a random permutation, since any tie-breaking mechanism would yield a
different permutation. Again, the greedy algorithm outperforms the wealth heuristic by about 30%
in the worst case whereas the randomized rounding algorithm outperforms the wealth heuristic by
about 23% in the worst case.
Figure 9 displays the results on the SafeGraph data for both the (SoP) and (SoT) objectives
where the results are qualitatively similar in both plots, although in slightly different scales. In
22

(a) SoP Objective for L = 10 · 1

(b) SoIP Objective for L = 10 · 1

Figure 7: Venmo Dataset results with uniform shocks. The Venmo network is taken to be the
densest weakly connected component with at least 50 nodes from the Venmo network. The internal
liabilities have been generated from Exp(1) for every (i, j) ∈ E. The external assets ci are given as
out
ci = (din
i + di ) · Exp(1), and the external liabilities are given as bi = 0.9 · ci . The simulations have
been run for 2000 iterations. The error areas represent 1 std.
this simulation, the randomized rounding algorithm outperforms all benchmarks and is also very
close to the greedy algorithm with a worst case diffeerence of about 7.1% and being as far as
approximately 40% from the other heuristics in the worst case in the (SoT) plot. In the (SoP) plot
the differences are about 18% in the worst case, however the greedy algorithm quickly approaches
the randomized rounding algorithm.
5.1.2

Fairness

Regarding the Gini-constrained problems, Figure 10 has a predictable behaviour. To be more
specific, as the number of bailouts k is small, the instance for which Target Gini ≤ 0.1 has a
lower relaxation optimum as well as rounded value for k ≤ 6 and later approaches the unconstrained
optimum (i.e. where Target Gini ≤ 1.0). Moreover the realized Gini Coefficient calculated by
averaging the relaxation solutions is dropping in the unconstrained problem and meets with the
curve of the constrained problem at k = 20. In the constrained case, the Gini coefficient rises to its
upper bound 0.1 until k = 6 and then starts to drop, that coincides with the objective value plots.
The explaination for this phenomenon is rather simple: at first, when resources are scarce, selecting
certain nodes on the network subject to these resources creates inequality which is mitigated by
the constraint at the expense of the quality of the solution creating a wors PoF of about 1.2 ≥ 1.
When k is large enough, i.e. k ≥ 6, the available resources allow the constrained version to create
a solution close to the unconstrained version (by “rebalancing” some z̄i∗ values) which is reflected
on both objective values, and eventuall the PoF reaches 1 when the two solutions eventually meet.
Also in the unconstrained version, again due to the fact that the number of resources increases the
Gini coefficient drops at almost a linear rate. In Figure 11, where we constrain the (SGC) with the

23

(a) SoP Objective for L = 106 · 1

(b) SoT Objective for L = 106 · 1

Figure 8: EBA Dataset results. The simulations have been run for 1000 iterations. The error areas
represent 1 std.
same values as Figure 10 we observe a worst-case PoF of about 1.09 at k = 13. Indeed, if we plot
the spatial inequality for k = 13 we observe that the mitigation of disparity through the constraint
is considerable in all of the nodes which yields an about 8% lower (SoP) value. In Figure 12 the
PoF for the SafeGraph data is approximately 1, meaning that all the resources have been equitably
allocated between minority/non-minority groups subject to the respective constraint(s). For the
German Banks data, the PoF is approximately 1.16 in the worst case (k = 2) and approaches 1 at
k = 6. Lastly, in Figure 13 we observe that the PoF drops quickly to 1 in most cases and a decent
trade-off between fairness and optimality can be achieved when g = 0.4.

5.2

Societal Implications

Our work suggests a set of societal implications. The first consideration is how the features of
individuals related to (i) the economic shocks and (ii) the financial interactions, are gathered, a
problem which is also highlighted in [AKW20]. While many attributes may be accessible (or at least
can be accurately inferred) by public datasets (such as the US Census, the US Economic Census,
the Poverty Tracker Dataset [pov21] and so on), many features correlated to income shocks and
financial interactions may touch on sensitive and private attributes of the individuals, which suggest
that work needs to be done in the lens of these models where there are missing or noisy data.
Moreover, carefully choosing the objective and the fairness constraint (if any) suggests a separate
issue informing our work. While we mostly study linear objectives of the clearing variable p̄, as well
as maximizing the number of solvent nodes, there are more objectives that one can study in this
context. One of them is a maximin (egalitarian) objective similar to the one of [AKW20] which,
in our case, corresponds to maximizing the worst-possible ratio of the clearing vector ratio over
total liabilities which can be thought of “fractional default”, namely maxp̄∈(GDBP) minj∈[n] p̄j /pj .
While we do not study this objective in the present paper, this objective can be thought of having
built-in fairness and possibly methods like the ones presented in [TWR+ 19] can be deployed.

24

(a) SoP Objective for L estimated from real-world (b) SoT Objective for L estimated from real-world
data with a budget given by Λ(k) = 104 · k
data with a budget given by Λ(k) = 104 · k

Figure 9: Network generated from Safegraph data whose generation is described in Appendix C
for businesses spanning the 3 nearest neighbor CBGs around the geographical location given by
(42.43969750363193, -76.49506530228598) in a monthly basis. The shocks are uniform. The
simulations have been run 50 times and the error areas represent 1 std.
From the perspective of policymakers, there are interesting phenomena that emerge in our work
and can be subject to more extensive discussion. Perhaps the most ubiquitous one is that wealththresholding policies, i.e. allocating stimulus to people below (or, theoretically, even above) a certain
income, does not seem to perform well when contagion happens on a network. This result juxtaposes
conventional rules and underlines that in order for the cumulative welfare to be maximized bailouts
may not be redirected towards both the poorest or, vice-versa, the wealthiest nodes. An intuitive
explanation for the bad performance of this criterion in presence of an underlying network that
propagates contagion, is that such nodes may not have many connections to the network, so the
effect of bailing them out is less than bailing out more central nodes. Ending, we observe that
the PoF showcases a quick phase transition to being 1 (Figure 13) suggesting that a decent tradeoff between being fair (within our framework) and optimal can be achieved, even with moderate
resources.

5.3

Further Related Work

Financial Networks and the Eisenberg-Noe Model. The EN model introcuded in [EN01]
models a financial network where each node has assets and liabilities both with respect to the
internal network, namely the other nodes of the network, and the external sector, namely the
node may, for instance, owe taxes to the government, or has loans to external creditors, as well
as may get social security benefits from the government. According to the EN model, when a
node defaults, namely is not able to pay out its creditors (internal and external), it rescales its

25

(a) Relaxation Optima and rounded values.

(b) Gini Coefficient calculated on the average values
of the fractional solutions.

Figure 10: Optimization subject to Gini Coefficient constraints on the German Banks dataset
with L = 106 · 1. Comparison of the unconstrainted problem (Target Gini ≤ 1) and a fairness
constrainted problem (Target Gini ≤ 0.1). The left plot represents the values of the relaxation
optimum for both values of the constraint, and the corresponding rounded solutions based on the
uniform rounding scheme of Section 2.3, and the right plot represents the actual value of the Gini
P
∗(t)
/m for m = 1000
coefficient calculated upon the optimization with values equal to z̄avg,i = m
t=1 z̄i
simulations. Error areas represent 1 std.
obligations proportionally16 and pays the rescaled responsibilities in full17 . The EN model is used
to calculate these payments, which are usually called clearing payments 18 , by computing a solution
to a fixed-point problem, or, equivalently, an optimal solution to a mathematical program with a
strictly increasing objective.
In the presence of shocks [GY15], the nodes similarly become default due to external shocks
that disrupt their assets. The works of [BBF18, FS19] investigate dynamic variants of the EN
model, where the liabilities and assets evolve as functions of time. Our work investigates the problem from a static viewpoint, and the hardness results obviously hold for the dynamical version.
Moreover, [JP20, AK19], investigate optimal bailouts in the cases of defaulting. More specifically, [JP20], investigates scenaria for the existence of multiple equilibria and provides necessary
and sufficient conditions for solvency under any equilibrium. Besides this, their paper investigates
optimal bailouts that brings the system into solvency and provides computational intractability
results for the minimum bailout problem by a reduction from the partition problem.
An extension of the model includes credit default swaps, that is triads of entities enter contracts
with one another whereas the default of a third entity in the network forces a bilateral transaction
between the other two. Computing a clearing vector in such models was shown to be PPAD16

This property is also known as absolute priority.
This property is also known as “liability over equity”.
18
The general problem has multiple equilibria, however, our paper, for simplicity, examines the cases where the
equilibrium payments are unique.
17

26

(a) Relaxation Optima and rounded values

(b) Spatial Disparity for k = 13

Figure 11: Optimization subject to the (SGC) constraints on the Venmo data with L = 10 · 1.
Comparison of the unconstrainted problem (Target Gini ≤ 1) and a fairness constrainted problem
(Target Gini ≤ 0.1). The left plot represents the values of the relaxation optimum for both
values of the constraint, and the corresponding rounded solutions based on the uniform rounding
scheme of Section 2.3, and the right plot represents the spatial inequality per node for k = 13.
Error areas represent 1 std after 2000 simulations.
Complete by [SSB17]. [PW20] investigates the problem of sequential defaulting in networks with
debt contracts and credit default swaps and shows hardness results regarding identifying the number
of default banks on the best-possible and worst-possible orders at which banks announce their
defaults.
The work of [AK19] considers optimal intervention methods under budget constraints under
the extended model of [GY15] and formulate optimization problems that minimize the systemic
losses as well as minimizing the number of defaulting institutions and apply their methods on
publicly available data on the Korean financial system. Their work considers fractional intervention
policies which differentiates it from ours. More specifically, we consider discrete interventions (i.e.
a node gets the full bailout or does not) in which case the model of [AK19] can be seen as the
fractional relaxation of the optimization objectives we study. Secondly, we experiment both with
high-granularity financial institutions (banks) as well as view the problem from a societal lens,
namely modeling the entities of the system as “societal nodes” (businesses, households, individuals).
Finally, the work of [Fei19] generalizes the classical EN model to a multi-layered one, where
the interconnected entitities of the financial network have trades in multiple assets, develops a
financial contagion model with fire sales that allows institutions to both buy and sell assets subject
to some utility, and studies its equilibria. While the exact contributions of [Fei19] are tangential to
ours generalizing our model in their framework it provides an excellent pathway for extending the
current work.
Income Shocks and Systemic Risk. The theory of income shocks has a long history financial
mathematics. One of the papers that is most closely connected to our work is the work of [GY15]
where financial contagion in the presence of shocks is investigated, and results are provided regard27

(a) Relaxation Optima and rounded values for Safe- (b) Relaxation Optima and rounded values for GerGraph data
man Banks data

Figure 12: (i) Optimization subject to the Property Gini Coefficient constraints between
minorities/non-minorities data (with property data from the US Census and SafeGraph) with
custom bailouts and a budget increase rate of 104 . Error areas represent 1 std after 50 simulations
with constraint target values 0.3 and 1.0. (ii) Optimization subject to the Property Gini Coeffifient on the German Banks data with property data drawn independently from Beta(2, 5) with
constraint target values 0.1 and 1.0. Error areas represent 1 std after 1000 simulations.
ing the probability of contagion of a subset of nodes due to a shock on a specific node. The setting
of this paper is extensively discussed at Section 1.1.
The work of [AKW20] studies subsidy allocations at the presence of income shocks where there
is only temporal but no spatial information about the nodes. The analysis in their paper is based on
ruin processes and considers two main objectives, one is minimizing the expected number of nodes
that are economically ruined (min-sum objective), and the other objective considers minimizing
the worst ruin probability of an node in the network (min-max objective). As they mention in
their paper, a crucial point that highlights income shocks in a societal level is the households have
different abilities to withstand income shocks. The phenomenon disproportionately affects lowincome families and can bring them into long-lasting poverty [Des16, Ata18, S+ 04]. Our paper
can be seen as a part and extension of this work direction which uses optimization methods for
decision-making, and a possible combination of the model of both [AKW20] and ours presents an
interesting research pathway.
The contagion effect in a financial network can trigger a financial crisis through contagion
[AG00, FPR00], with a vivid example being the 2007-2009 financial crisis which followed after the
collapse of Lehman Brothers because of the decline of the housing market [AR+ 09], that initiated
a cascade of failures of financial institutions around the world. This domino of effects prompted
government bailouts around the world, with the lending interests booming, which turned many
nations, such as Greece, unable to service their mounting debts.
Optimal Stimulus Allocation During COVID-19. There are multiple recent works regarding
stimulus allocation and the recent COVID-19 economic crisis response. Most notable plans for

28

(b) German Banks Data, L = 105 · 1

(a) Venmo Data, L = 10 · 1

Figure 13: Relation between fractional PoF and the upper bound g on the (SGC) for varying
resources.
economic relief are the ones of the United States of America [act20], and the European Union
[Uni20] which contain stimulus packages to be distributed to individuals by the governments.
The work of [CCSW20] models the response to the US CARES act on consumption by extending
existing models of consumption to incorporate features of the COVID-19 crisis, where spending has
to be limited, and predicts, that under a short lockdown (as of April 2020) the stimulus payments
will be sufficient to make a recovery. Their work does not incorporate devising a policy to allocate
stimulus checks and investigating issues of fairness, which is the main part of our paper.
Data-driven approaches have been followed by the works of [BFM+ 20]19 and [CFH+ 20] where
the authors used large datasets in order to study how the stimulus payments were allocated and
found out that poorer consumers and consumers who had lower liquidity spent a significantly higher
amount of their stimulus checks quickly. In contrast, our work studies the assignment problem and
its fairness implication, and not the consumption response to the pandemic.
Finally, a work tangential, but fundamentally different from ours is the work of [NSW20] where
the authors use a life-cycle model and build an efficient algorithm to find the optimal policy that
maximizes aggregate consumption. The main difference with [NSW20] is that their work provides
a policy based on a completely different model and does not account for the network effects of the
economy and shocks, and consequently do not know how they affect the bailouts.
Influence Maximization. While not directly related to financial networks, the allocation problem
we study in this paper has very close ties to Influence Maximization (IM). The work of [KKT03]
introduced the influence maximization problem as follows: given a network in which each edge can
transmit information (e.g. disease, marketing information etc.) with probability p independently of
the other edges, the IM problem asks whether there exists a set S with |S| = k such as the number
of influenced nodes is maximized. Based on the submodularity properties of the influence function,
the authors devise an (1 − 1/e − o(1))-approximation algorithm for approximating the optimal
19

https://github.com/econ-ark/Pandemic

29

influence set. This work has been greatly extended by a series of works that optimize its algorithm
(e.g. see [BBCL14, GLL11] and the references therein), and adapt it to different contexts (see e.g.
[LKG+ 07, LAH07, CLCL20]). Another recent interesting work related to ours is the concept of
Fair Influence Maxmimization [TWR+ 19, RJL+ 20] which discusses the maximization of influence
subject of fairness constraints.

6

Conclusions and Future Work

In this work we propose a model of discrete subsidy allocation based on the Eisenberg-Noe model
which assumes a collective of interconnected individuals which posses features such as external
assets and liabilities, internal assets and liabilities, demographic features, and experience shocks
on their assets leading a part of them to default. Our work addresses the problem of averting
as much damage as possible from the shocks subject to a total budget constraint by relying in
optimizing a welfare objective. The subsidy allocations are discrete and can be determined from
each node’s features (such as e.g. the number of children, the income level, the poverty status etc.).
Policymakers are faced with similar planning problems when attempting to optimize some welfare
measure on a population. Although exact network interactions are difficult to be inferred on a
realistic world, our model and the consequent data-driven experiments based on individual level
and business and households-level granularity offer a simple proxy of studying such problems in realworld scenarios, and provide useful insights with respect to the effectiveness of certain strategies.
For instance, thresholding by an ex-ante wealth criterion (i.e. income level), which is a de facto
pathway in policymaking, yields worse results in our model than the greedy or the LP-based policies
for the linear objectives in question. Furthermore, we show that maximizing the (absolute) number
of solvent nodes cannot be approximated within a poly-time computable factor. Closing, we show
how fairness can be incorporated on the model and study the price of fairness both theoretically
and empirically.
There are numerous ways to extend the horizons of the present work. First of all, there are two
very interesting objectives that can be studied: the former is a maximin objective similar to the
one presented in [AKW20] subject to the bailout scheme we study in this paper; the latter one is
the systemic loss objective as presented in [GY15]. For such objectives, it could be an important
contribution to develop approximation algorithms, or study the approximation algorithms presented
in this work on the corresponding objectives. Besides this, it is interesting to examine whether there
exist algorithms with better approximation ratios, or prove the absence thereof, than the ones
developed in this paper, and examine whether more efficient variants of the current algorithms can
be implemented (for instance, using the ideas of [LKG+ 07] and the subsequent works could improve
the runtime of the greedy algorithm). Finally, there is a need to analyze additional real-world
datasets at the societal level. The study, modelling, and inference of income shocks and financial
interactions from real-world datasets suggest a cardinal direction for follow-up work. Note, however,
that obtaining such data may be a difficult task due to data protection legislation, pointing toward
possible future synergy between the academic community, policymakers, and companies.

References
[act20]

CARES act. https://home.treasury.gov/policy-issues/cares, 2020.

[AG00]

Franklin Allen and Douglas Gale. Financial contagion. Journal of political economy,
108(1):1–33, 2000.

30

[AK19]

Dohyun Ahn and Kyoung-Kuk Kim. Optimal intervention under stress scenarios: A
case of the korean financial system. Operations Research Letters, 47(4):257–263, 2019.

[AKW20]

Rediet Abebe, Jon M Kleinberg, and S Matthew Weinberg. Subsidy allocations in the
presence of income shocks. In AAAI, pages 7032–7039, 2020.

[Ano21]

Anonymous. Allocating stimulus checks in times of crisis - supplementary code. Jun
2021.

[AR+ 09]

Viral V Acharya, Matthew P Richardson, et al. Restoring financial stability: how to
repair a failed system, volume 542. John Wiley & Sons, 2009.

[Ata18]

Esso-Hanam Atake. Health shocks in sub-saharan africa: are the poor and uninsured
households more vulnerable? Health economics review, 8(1):1–13, 2018.

[Bak18]

Scott R Baker. Debt and the response to household income shocks: Validation and
application of linked financial account data. Journal of Political Economy, 126(4):1504–
1557, 2018.

[BBCL14] Christian Borgs, Michael Brautbar, Jennifer Chayes, and Brendan Lucier. Maximizing
social influence in nearly optimal time. In Proceedings of the twenty-fifth annual ACMSIAM symposium on Discrete algorithms, pages 946–957. SIAM, 2014.
[BBF18]

Tathagata Banerjee, Alex Bernstein, and Zachary Feinstein. Dynamic clearing and
contagion in financial networks. arXiv preprint arXiv:1801.02091, 2018.

[BBT13]

Benjamin M Blau, Tyler J Brough, and Diana W Thomas. Corporate lobbying, political
connections, and the bailout of banks. Journal of Banking & Finance, 37(8):3007–3017,
2013.

[BFM+ 20] Scott R Baker, Robert A Farrokhnia, Steffen Meyer, Michaela Pagel, and Constantine Yannelis. Income, liquidity, and the consumption response to the 2020 economic
stimulus payments. Technical report, National Bureau of Economic Research, 2020.
[BFT11]

Dimitris Bertsimas, Vivek F Farias, and Nikolaos Trichakis. The price of fairness.
Operations research, 59(1):17–31, 2011.

[BHM17]

Michael M Bechtel, Jens Hainmueller, and Yotam Margalit. Policy design and domestic
support for international bailouts. European Journal of Political Research, 56(4):864–
886, 2017.

[BP14]

Christian Broda and Jonathan A Parker. The economic stimulus payments of 2008 and
the aggregate demand for consumption. Journal of Monetary Economics, 68:S20–S36,
2014.

[Bri09]

Steven Brill. What’sa bailed-out banker really worth? New York Times Magazine,
December, 2009.

[CCSW20] Christopher D Carroll, Edmund Crawley, Jiri Slacalek, and Matthew N White. Modeling the consumption response to the cares act. Technical report, National Bureau of
Economic Research, 2020.

31

[CFH+ 20] Raj Chetty, John Friedman, Nathaniel Hendren, Michael Stepner, et al. How did
covid-19 and stabilization policies affect spending and employment? a new real-time
economic tracker based on private sector data. NBER working paper, (w27431), 2020.
[CLCL20]

Yi-Cheng Chen, Ping-En Lu, Cheng-Shang Chang, and Tzu-Hsuan Liu. A timedependent sir model for covid-19 with undetectable infected persons. IEEE Transactions on Network Science and Engineering, 7(4):3279–3294, 2020.

[CNW83]

Gérard Cornuéjols, George Nemhauser, and Laurence Wolsey. The uncapicitated facility location problem. Technical report, Cornell University Operations Research and
Industrial Engineering, 1983.

[Con09]

Roger D Congleton. On the political economy of the financial crisis and bailout of
2008–2009. Public Choice, 140(3-4):287–317, 2009.

[CP15]

Anthony J Casey and Eric A Posner. A framework for bailout regulation. Notre Dame
L. Rev., 91:479, 2015.

[CWY16]

Hong Chen, Tan Wang, and David D Yao. Financial network and systemic risk a
dynamic model. Technical report, working paper, 2016.

[Dem18]

Gabrielle Demange. Contagion in financial networks: a threat index. Management
Science, 64(2):955–970, 2018.

[Des16]

Matthew Desmond. Evicted: Poverty and profit in the American city. Crown, 2016.

[Doo40]

Joseph L Doob. Regularity properties of certain families of chance variables. Transactions of the American Mathematical Society, 47(3):455–486, 1940.

[EN01]

Larry Eisenberg and Thomas H Noe. Systemic risk in financial systems. Management
Science, 47(2):236–249, 2001.

[Fei19]

Zachary Feinstein. Obligations with physical delivery in a multilayered financial network. SIAM Journal on Financial Mathematics, 10(4):877–906, 2019.

[FPR00]

Xavier Freixas, Bruno M Parigi, and Jean-Charles Rochet. Systemic risk, interbank
relations, and liquidity provision by the central bank. Journal of money, credit and
banking, pages 611–638, 2000.

[Fre77]

Linton C Freeman. A set of measures of centrality based on betweenness. Sociometry,
pages 35–41, 1977.

[FS19]

Zachary Feinstein and Andreas Sojmark. A dynamic default contagion model: From
eisenberg-noe to the mean field. arXiv preprint arXiv:1912.08695, 2019.

[Gin21]

Corrado Gini. Measurement of inequality of incomes.
31(121):124–126, 1921.

[GLL11]

Amit Goyal, Wei Lu, and Laks VS Lakshmanan. Celf++ optimizing the greedy algorithm for influence maximization in social networks. In Proceedings of the 20th
international conference companion on World wide web, pages 47–48, 2011.

[GV17]

Axel Gandy and Luitgard AM Veraart. A bayesian methodology for systemic risk
assessment in financial networks. Management Science, 63(12):4428–4446, 2017.
32

The economic journal,

[GY15]

Paul Glasserman and H Peyton Young. How likely is contagion in financial networks?
Journal of Banking & Finance, 50:383–399, 2015.

[Hay85]

Susumu Hayashi. Self-similar sets as tarski’s fixed points. Publications of the Research
Institute for Mathematical Sciences, 21(5):1059–1066, 1985.

[Jih09]

Zhang Jihong. To save or not to save——analysis on public fund bailout in financial
crisis bailout legal system [j]. Shanghai Finance, 1, 2009.

[JP20]

Matthew O Jackson and Agathe Pernoud. Credit freezes, equilibrium multiplicity, and
optimal bailouts in financial networks. SSRN, 2020.

[JPS06]

David S Johnson, Jonathan A Parker, and Nicholas S Souleles. Household expenditure
and the income tax rebates of 2001. American Economic Review, 96(5):1589–1610,
2006.

[Kar75]

Richard M Karp. On the computational complexity of combinatorial problems. Networks, 5(1):45–68, 1975.

[KKT03]

David Kempe, Jon Kleinberg, and Éva Tardos. Maximizing the spread of influence
through a social network. In Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 137–146, 2003.

[LAH07]

Jure Leskovec, Lada A Adamic, and Bernardo A Huberman. The dynamics of viral
marketing. ACM Transactions on the Web (TWEB), 1(1):5–es, 2007.

[LKG+ 07] Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne VanBriesen, and Natalie Glance. Cost-effective outbreak detection in networks. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 420–429, 2007.
[MMD+ 21] Nuno Mota, Negar Mohammadi, Palash Dey, Krishna P Gummadi, and Abhijnan
Chakraborty. Fair partitioning of public resources: Redrawing district boundary to
minimize spatial inequality in school funding. 2021.
[NSW20]

Vegard M Nygaard, Bent E Sørensen, and Fan Wang. Optimal allocation of the covid19 stimulus checks. Available at SSRN 3691091, 2020.

[NW78]

George L Nemhauser and Laurence A Wolsey. Best algorithms for approximating the
maximum of a submodular set function. Mathematics of operations research, 3(3):177–
188, 1978.

[PBMW99] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank
citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.
[pov21]

Poverty tracker dataset. https://www.povertycenter.columbia.edu/poverty-tra
cker-data, 2021.

[PSJM13]

Jonathan A Parker, Nicholas S Souleles, David S Johnson, and Robert McClelland.
Consumer spending and the economic stimulus payments of 2008. American Economic
Review, 103(6):2530–53, 2013.

[PW20]

Pál András Papp and Roger Wattenhofer. Sequential defaulting in financial networks.
arXiv preprint arXiv:2011.10485, 2020.
33

[RJL+ 20]

Aida Rahmattalabi, Shahin Jabbari, Himabindu Lakkaraju, Phebe Vayanos, Max Izenberg, Ryan Brown, Eric Rice, and Milind Tambe. Fair influence maximization: A
welfare optimization approach. arXiv preprint arXiv:2006.07906, 2020.

[Ros06]

Guillermo Rosas. Bagehot or bailout? an analysis of government responses to banking
crises. American Journal of Political Science, 50(1):175–191, 2006.

[S+ 04]

Thomas M Shapiro et al. The hidden cost of being African American: How wealth
perpetuates inequality. Oxford University Press, USA, 2004.

[Spi12]

Daniel Spielman. Spectral graph theory. Combinatorial scientific computing, (18),
2012.

[Sri01]

Aravind Srinivasan. Distributions on level-sets with applications to approximation algorithms. In Proceedings 42nd IEEE Symposium on Foundations of Computer Science,
pages 588–597. IEEE, 2001.

[SSB17]

Steffen Schuldenzucker, Sven Seuken, and Stefano Battiston. Finding clearing payments
in financial networks with credit default swaps is ppad-complete. LIPIcs: Leibniz
International Proceedings in Informatics, (67), 2017.

[TWR+ 19] Alan Tsang, Bryan Wilder, Eric Rice, Milind Tambe, and Yair Zick. Group-fairness in
influence maximization. arXiv preprint arXiv:1903.00967, 2019.
[Uni20]

European Union. European union coronavirus revovery plan. https://ec.europa.eu
/info/strategy/recovery-plan-europe en, 2020.

[WS11]

David P Williamson and David B Shmoys. The design of approximation algorithms.
Cambridge university press, 2011.

34

A

Omitted Proofs

Proof of Lemma 1. The polytopes P1 (A, c1 , b) and P2 (A, c2 , b) satisfy the relation P2 ⊆ P1 .
Using the fact that the fixed point operator is increasing [EN01] we get that p̄1 ≥ p ∧ AT p̄2 + c1 ≥
p̄2 . Thus p̄1 ≥ p̄2 . Moreover for every vector v ≥ 0, trivially v T p̄1 ≥ v T p̄2 . To find a direction δ ≥ 0
such that p̄1 ≥ p̄2 + δ, let R1 , D1 be the sets of solvent and default nodes for assets c1 and R2 , D2
be the sets of solvent and default nodes for assets c2 . Then we know from above that D1 ⊆ D2
and consequently R2 ⊆ R1 . So R1 ∩ R2 = R2 , and D1 ∩ D2 = D1 . Thus we calculate δ to have
components


0
j ∈ R2

P
δj =  i∼j aij (p̄1i − p̄2i ) + c1j− c2j j ∈ D1
.

+

 p1j − P aij p̄2i − c2j
j ∈ R1 ∩ D2
i∼j

We also get that a feasible solution is given by δ̃ = 1D1 (c1 − c2 ) ≥ 0 such that δ̃ ≤ δ.
Proof of Theorem 1. We first start by designing a polynomial-time reduction from a 3-Set-Cover
instance to an instance of the decision version of the (AS) objective.
Reduction. We fix a constant α ∈ (0, 3). We create a bipartite network H with a node set
partitioned into V and U where V = {v1 , . . . , vm } is the node set that represents all the “set nodes”
and U = {u1 , . . . , un } is the node set that represents all the “item nodes” of the corresponding
3-Set-Cover instance. For each set node vj and each item ui we add an edge (vj , ui ) if and only if
ui ∈ Sj . We set the parameters of the EN model as follows. First, for each “set node”, there is an
external influx of cj = 3, and an external outflux of bj = α. For each edge (vj , ui ) created there is
an obligation of pji = 1 − α/3 from the set to the corresponding item. Thus βmax = 1 − α/3 < 1.
Each item node has no external influx, i.e. ci = 0, and has external liabilities of bi = 1 − α/3. The
shock distribution is taken to be the point-mass distribution that has Xj = 3 if Xj is a “set node”
and is zero otherwise. We also take Lj = 3 for all j ∈ [n], Λ = 3k and r = k + n. After the shock,
the influx of cash is disrupted and all nodes default to having p̄i = 0, namely no-one can meet
his/her obligations. The reduction creates a network with n + m nodes and n + 4m edges and runs
in polynomial-time.
Forward Direction. Suppose that C is a set cover with |C| = k. Giving aid L to set nodes vi such
that i ∈ C we have that every “set node” in C becomes solvent, since it can pay the obligations of
pj = 3 to the “item nodes”, and the corresponding items that it covers become solvent themselves.
Since C is a set cover, every item becomes solvent, and all the set nodes vi for i ∈
/ C remain default,
hence we have a total of k + n solvent nodes.
Reverse Direction. Suppose that there are at least r = k + n solvent nodes, therefore there are
at most d = m − k default nodes. Let J ⊆ U ∪ V be the set with |J| = k elements such that every
element j ∈ J gets an aid of L. We will show that J ⊆ V . Suppose that J has ` elements on U and
k − ` elements on V . Then, the number of solvent nodes is at most ` + k − ` + k − ` = 2k − `, and
the number of default nodes is therefore at least (n − k + `) + (m − k). In order for this to hold we
must have that n − k + ` ≤ 0, which implies that ` = 0. Therefore J ⊆ V . Therefore, every item
node is solvent and thus J must be a set cover for the original problem. The budget constraint is
also satisfied.
(L-OBJ) objective. The reduction construction for the (L-OBJ) problem is very similar: instead
of letting r = k + n we let r = 3k + n. Moreover, we let v = 1.
Proof of Lemma 4.

35

1T p̄T − 1T p̄S = kp̄T − p̄S k1

(Lemma 1)

T

T

T



T



= kp ∧ A p̄T + c − x + L 1T − p ∧ A p̄S + c − x + L 1S k1
(Fixed Point Op.)
≤ kAT p̄T + c − x + LT 1T − AT p̄S − c + x − LT 1S k1
≤ kAT k1 kp̄T − p̄S k1 + LT 1T \S

(Norm Consistency, 1T − 1S = 1T \S for S ⊆ T )

T

(kAT k1 = βmax ∈ (0, 1))

≤ βmax kp̄T − p̄S k1 + L 1T \S .
P

(Non-expansion of p ∧ (·))

Lj

j∈T \S
Rearranging we get 1T p̄T − 1T p̄S ≤ 1−β
. Therefore for a linear objective v T p̄ with v > 0
max
since the EN model would yield the same clearing vector p̄ [EN01] we have
P
v
max
j∈T \S Lj
v T (p̄T − p̄S ) ≤ vmax 1T (p̄T − p̄S ) ≤
.
1 − βmax

Proof of Lemma 5. Let S be a set and let S ∗ be the optimal bailout set. Let S ∗ \S = {j1 , . . . , jq }
for q ≤ k. We have
v T p̄S ∗ ≤ v T p̄S∪S ∗
T

= v p̄S +

(Monotonicity, S ∗ ⊆ S ∪ S ∗ )

q
X

v T p̄S∪{j1 ,...,jl } − p̄S∪{j1 ,...,jl−1 }



(Telescoping Sum)

l=1
q
vmax X
Ljl
≤ v p̄S +
1 − βmax
T

(Lemma 4)

l=1

vmax k
max Lj
(Bound by Maximum)
1 − βmax l∈[q] l
vmin ζk
max Lj
(ζ = vmax /vmin )
≤ v T p̄S +
1 − βmax l∈[q] l

ζk
≤ v T p̄S +
max v T p̄S∪{e} − v T p̄S .
(Condition 1)
1 − βmax e∈S
/
 T

1 − βmax T
v p̄S ∗ − v T p̄S .
⇐⇒
max
v p̄S∪{e} − v T p̄S ≥
(Rearrangement)
kζ
e∈S,e
/
feasible
≤ v T p̄S +

Proof of Theorem 5. Let St be the solution at set t and let k be the total number of iterations
of the algorithm. Let ρ = (1 − βmax )/(kζ) Then, for a fixed shock X = x
v T p̄Sk ≥ ρv T p̄S ∗ + (1 − ρ)v T p̄Sk−1
X
≥ v T p̄∅ + ρv T p̄S ∗
(1 − ρ)l

(Lemma 5)
(Induction)

0≤l≤k

ρ(1 − ρ)k T
· v p̄S ∗
ρ


≥ 1 − e−kρ · v T p̄S ∗


= 1 − e−(1−βmax )/ζ · v T p̄S ∗ .
≥

36

(v T p̄∅ ≥ 0 and Geometric Sum)
(1 + x ≤ ex )
(Definition of ρ)

Taking expectations over X ∼ D in the above expression we get the desired result.
Proof of Theorem 6. Different Bailouts. The proof follows similar philosophy as Lemma 3.6
of [BBCL14]. We construct the set of feasible solutions S = {S ∈ 2[n] : S is a feasible allocation
set}.

log n·Λ/Lmin
A set S ∈ S can have at most Λ/Lmin elements. Applying the Chernoff bound for m = O
ε2
each S ∈ S deviates at most an additive factor of εOPTf from its expectation with probability at
least 1 − 2−Λ/Lmin n−1 . The probability that there exists S ∈ S that violates this event is given by
the union bound on S, with |S| ≤ 2Λ/Lmin , and is at most 1/n. Conditioned on this event which
happens with probability 1 − O(1/n) and letting OPTf denote the sample average of the optimal
solutions we have
SOLf ≥ SOLf −εOPTf ≥ (1−γf )OPTf −εOPTf ≥ [(1 − γf )(1 − ε) − ε] OPTf ≥ (1−γf −2ε)OPTf
Therefore, the average
an (1
 estimator
 achieves

 − γf − 2ε)-approximation. The total number of
2
Λ
Λ
e 2
iterations is therefore O m Lmin = O
2 . Assuming that each iteration is solved by the
 ε (Lmin )

log(1/η)
fixed point iteration in time Top = O (n + |E|) log(1/β
within relative accuracy 0 < η < 1 via
max )
using a sparse-matrix multiplication (or adjacency lists) the total runtime becomes




2
(n + |E|) log nΛ2 log(1/η)
e (n + |E|)Λ ,
O
=
O
Lmin ε2
L2min ε2 log(1/βmax )
where the final solution is within an (1−γf −2ε) approximation factor from the optimal solution
and modulo a (very small) relative approximation error of η.
Proof of Lemma 2. The proof of the Lemma is quite of technical nature, and involves summing
the relative liabilities constraint p̄ ≤ AT p̄ + c − x + LT z̄ over the elements of the set S. More
formally we consider nodes of the set S and sum over S, i.e.
[GY15, (13)]

(1 − β)TS p̄
≤
1TS (I − AT )p̄ ≤ 1TS (c − x) + (L
P
P
Thus j∈S (1 − βj )p̄j ≤ j∈S (cj − xj + Lj z̄j ).

z̄)T 1S .

Proof of Lemma 3. Let ξe∗ = (e
p∗ , ze∗ ) be an optimal solution to the relaxation problem, and let j
be a default node under this solution. Then j satisfies the following equation (due to absolute priority),
X
pe∗j =
aij pe∗i + cj − xj + Lj zej∗ ≥ cj − xj + Lj zej∗ ,
i∼j

where we have used the fact that the optimal solution is feasible and that pe∗ ≥ 0, and aij ≥ 0.
The result for the rounded solution is inferred exactly the same way.
Proof of Theorem 2. Let f (p̄) = v T p̄ for some v > 0 (i.e. a strictly increasing linear function of
p̄). The rounded variables are sampled from Be(e
z ∗ ). So, for a fixed shock X = x
EZ∼Be(ez ∗ ) [SOLf (x)] =

X
z̄∈{0,1}n

Pr

Z∼Be(e
z∗ )

[Z = z̄] · EZ∼Be(ez ∗ ) [SOLf (x)|Z = z̄] .

Conditioned on the event {Z = z̄}, we use the sets DSOL(z̄) and RSOL(z̄) to denote the sets of
default and solvent nodes under the assignment z̄ ∈ {0, 1}n . We therefore break the above sum as
37




X

EZ∼Be(ez ∗ ) [SOLf (x)|Z = z̄] = EZ∼Be(ez ∗ ) 

X

vj p̄j +

j∈DSOL(z̄)

We treat the two components of the partition of [n] as follows:
1. Solvent Nodes. Every solvent node j satisfies p̄j = pj ≥ pe∗j ≥
set of solvent nodes we get

EZ∼Be(ez ∗ ) 


X



vj p̄j Z = z̄  ≥

j∈RSOL(z̄)

1 − βmax
ζ

vj p̄j Z = z̄ 

j∈RSOL(z̄)

1−βmax ∗
pej .
ζ

Summing over the





· EZ∼Be(ez ∗ ) 


X

vj pe∗j Z = z̄  .

j∈RSOL(z̄)

2. Default Nodes. We have



X

EZ∼Be(ez ∗ ) 




X

vj p̄j Z = z̄  ≥ EZ∼Be(ez ∗ ) 

j∈DSOL(z̄)

vj (cj − xj + Lj Zj ) Z = z̄ 

j∈DSOL(z̄)

(Lemma 3)



X

≥ EZ∼Be(ez ∗ ) 

vmin (1 − βj )e
p∗j Z = z̄  (Lemma 2)

j∈DSOL(z̄)




X

≥ EZ∼Be(ez ∗ ) 

j∈DSOL(z̄)


≥

1 − βmax
ζ



1 − βmax
vmax pe∗j Z = z̄ 
ζ
(ζ = vmax /vmin )



· EZ∼Be(ez ∗ ) 

X

vj pe∗j Z = z̄  .

j∈DSOL(z̄)

(vmax ≥ vj )
Thus, since the LP bound is an upper bound to the optimal we get EZ∼Be(ez ∗ ) [SOLf (x)] ≥

1−βmax
· EZ∼Be(ez ∗ ) [OPTf (x)] .
ζ
Proof of Theorem 3. The proof of the argument is straightforward in general, and follows the
classical analysis pathway for this kind of algorithms [WS11]. We start by bounding the probability
(i)
of failure of this step. Let us fix a sample X = x(i) where i ∈ [m] and let Ff be the event that
the i-th random assignment fails. Then we know that


Pr

Z∼Be(e
z∗ )

h

(i)

Ff

i

o n
oi
(i)
(i)
SOLf ≤ (1 − γf − ε2 )OPTf ∪ LT Z (i) ≥ (1 + δΛ )Λ
Z∼Be(e
z∗ )
h
i
h
i
(i)
(i)
≤ Pr
SOLf ≤ (1 − γf − ε2 )OPTf + Pr
LT Z (i) ≥ (1 + δΛ )Λ ,

=

Pr

hn

Z∼Be(e
z∗ )

Z∼Be(e
z∗ )

by virtue of the union bound. We bound the first probability by (reversed) Markov’s Inequality
Pr

Z∼Be(e
z∗ )

h
i
(i)
(i)
SOLf ≤ (1 − γf − ε2 )OPTf ≤
38

ε2
ε2
1
≤
1
−
≤
1
−
.
1 + ε2 /γf
2γf
2

We distinguish the following two cases for our analysis: the former case considers equal bailouts
and the latter one considers unequal bailouts:
1. Equal bailouts. For the second inequality, we know that all the bailouts have
 the same value
 `.
Let k = Λ/`. Then we know from the Chernoff bound that PrZ∼Be(ez ∗ ) 1T Z ≥ (1 + δk )k ≤
p
2
e−δk k/3 and thus letting δk = 3 log(4/ε2 )/k we know that the above probability is at most
√
√
e Λ) for the total budget) is allowed with
e k) (resp. Λ + O(
ε2 /4. Thus a deviation of k + O(
high probability for the case of equal bailouts.
h
i
(i)
Combining the two events above we end up with PrZ∼Be(ez ∗ ) Ff ≤ 1 − ε2 /4. Repeating the
2

procedure T times boosts the probability of failure to at most (1 − ε2 /4)T ≤ e−ε T /4 . Failing
2
in any of the m shocks happens with probability at most me−ε T /4 . Finally, by the one-sided
Chernoff bound we have that for the sample average OPTf of the corresponding optima
Pr

X∼D




2
2
2
OPTf ≥ (1 − ε)OPTf ≤ e−mε OPTf /kf k∞ .
T OPT2

2

We want to make the probability at most e−ε T /4 so choosing m = 4kf k2 f yields the desired
∞
result. Note that m ≤ T /4 since OPTf ≤ kf k∞ , so repeating the process for T /4 steps
2
yields the desired guarantee. The final probability of failure is O(T e−ε T /4 ). Setting T =
4(log n)/ε2 we get that the failure probability is O(log n/(ε2 n)). So the algorithm succeeds
with probability 1 − O(log n/(ε2 n)). The total runtime is





log n
k log n
T
k
e
O
T +
=O
+
.
ε2
ε2
ε2 ε 4
(i)
1 P
Finally, with probability 1 − O(log n/(ε2 n)) we have that for SOLf = m
i∈[m] SOLf
SOLf ≥ (1 − γf − ε2 )OPTf ≥ (1 − γf − ε2 )(1 − ε)OPTf ≥ (1 − γf − 2ε)OPTf .
Remark on the value of ε. A suitable value for ε can be ε = Θ(n−1/4 ) for which we get a
e √n + kn), a success probability of 1 − O(log n/√n) and an approximation
runtime of O(T
guarantee of 1 − γf − Θ(n−1/4 ).
2. Adaptation of the Rounding Scheme of [Sri01] for general bailouts. We can adapt the dependent rounding scheme of [Sri01] to perform randomized rounding such that (i) we obtain the same approximations as in the case of independent rounding; (ii) the constraints of
the problem are satisfied with high probability. The problem that the independent rounding scheme has is that there exist choices of the entries of L (e.g. Lj = 2j−1 ) such that
LT Z is anti-concentrated. In fact, when Lj = 2j−1 and Z ∼ Be(1/2 · 1) then LT Z ∼
Uniform({0, . . . , 2n − 1}). To mitigate this problem we will use the main result of [Sri01] as
follows: We first solve the LP relaxation and get the optimal fractional bailouts ze∗ as in the
independent rounding case. Then we define a vector U as a sample from the oracle of [Sri01]
(described in Section 2, and Section 3.2 that modifies the original algorithm for a non-integral
sum)







L
kLk1 − Λ
kLk1 − Λ
 L
U ∼ Ddep  1 (1 − ze1∗ ), . . . , n (1 − zen∗ ),
−
;
kLk∞
kLk∞
kLk∞
 kLk∞
|

{z

probabilities

39




kLk1 − Λ 
,
kLk∞

} # variables |
{z
}
n+1 ;
| {z }

total weight

L

and let Z = 1 − U . For (i) we have that E [Zj ] = Pr[Zj = 1] = 1 − Pr[Uj = 1] = 1 − kLkj∞ (1 −
zej∗ ) ≥ zej∗ and thus we
P can achieve the desired approximation guarantees of Theorem
P2. For
(ii) we know that
L
U
are
concentrated
and
thus
we
can
easily
show
that
j Lj Z j
j j j
are concentrated as well, and thus with high probability the rounded
bailouts

 are at most
√
T
Λ
e
e
Λ + O( Λ). Finally, the runtime can be similarly deduced to be O ε2 + Lmin ε4 for a success


n
. Thus, again, for ε = Θ(n−1/4 ) we get an approximation ratio of
probability of 1 − O log
2
nε


√n .
1 − γf − Θ(n−1/4 ) with probabiltiy 1 − O log
n
Proof of Theorem 4. We start with the complete network on n nodes Kn , with ci = bi = 1
and pij = 1, and a point-mass shock of Xi = ε for some ε ∈ (0, 1). We also let L = nε/k · 1
and v > 0. In the fractional optimum we have that zei∗ = k/n and the optimum value of kvk1 · n
is achieved. When we choose to round to k nodes, we do that uniformly (since zei∗ = k/n) over
all k-sets, that is the probability of each k-set being selected is n1 . Thus the value optimum
(k )
is just the value of the optimum given that the values of any k-set are set to 1 (w.l.o.g we can
choose the k-set {1, . . . , k}). The rounded nodes’ bailouts would suffice to avert the shock, so they
will be truncated to have a value of ε, from the problem constraints (debts must only be paid
in at most their full value). Since all nodes are marginally default (that holds for the bailed-out
nodes with the truncated values as well), we have fromPLemma 2 (with equality since every node
satisfies the constraint with tight equality) that (1/n) j∈[n] p̄j = n − nε + kε which implies that
OPTf = v T p̄ ≤ kvk∞ [n(n − ε(n − k))]. We thus have σf ≥

ζ −1
1−ε(1−k/n) .

So for k = o(n) (small)

ζ −1
1−ε

which can be arbitrarily bad for ε → 1.
and n large the gap tends to
Proof of Theorem 7 (see Figure 4). Let a(|I|) be a poly-time computable function on the
input size |I|, and α ∈ (0, 3). We construct the same proof as the hardness reduction of Theorem 1
with the only change that instead of adding one copy of “element nodes” we add a(|I|) copies of
“element nodes” resulting in a network with a polynomial number, i.e. m + a(|I|) · n, of nodes.
We connect, with direction from left to right, the element nodes with liabilities 1−α/3
(i.e. a fullyn
connected network) and we set the external liabilities of the a(|I|)-th level to be 1 − α/3. We
distinguish the following cases: If the answer to the 3-Set-Cover problem is YES then there are
at least k + a(|I|) · n solvent nodes. Else, if the answer to the 3-Set-Cover problem is NO then
there are at most k + n solvent nodes. That is, if we were able to distinguish between the (AS)
being between k + n and k + a(|I|) · n in polynomial time we will be able to solve the 3-Set-Cover
in polynomial time, which is a contradiction assuming that P 6= NP. The approximation gap is
k+n·a(|I|)
≥ a(|I|)·n
= a(|I|)
= Ω (a(|I|)).
k+n
2n
2
Proof of Theorem 8.
• GC Problem. We omit the dependence on g to lighten notation. We have that

40


EZ∼Be(ez ∗ ) 



X

j∈[n]



X
1
∗ 
Zj  ≥
E
|Zi − Zj |
2ng Z∼Be(ez )

(Gini Constraint)

i,j∈[n]


1 X
zei∗ (1 − zej∗ ) + zej∗ (1 − zei∗ ) (Definition of Expected Value)
2ng
i,j∈[n]


X
X
1 
≥
n
zej∗ −
zei∗ zej∗ 
(Reorder Summation)
ng
j∈[n]
i,j∈[n]


X
X
1 
zej∗ 
zej∗ − k
(At most k nodes are bailed out)
n
≥
ng
j∈[n]
j∈[n]

X
k
1
1−
zej∗
=
g
n
=

j∈[n]

If

1
g

1−


k
n

< 1, or g > 1 −


k
n

then we follow the same analysis as in Theorem 2 and obtain

an (1−βmaxg)(1−k/n) approximation guarantee. We can use concentration inequalities to devise
bounds about the runtime of the algorithm in a similar manner to Theorem 3.
Proof of Theorem 9. General Instance. Suppose that we have the star network Sn with node 1
being in the center and nodes 2 through n − 1 being the peripheral nodes. Let b = 1, c = n · 1{1} ,
and p1i = 1 for i 6= 1. The bailouts are L = n · 1 and the total budget is Λ = n. The network
is hit with a point-mass shock X = c. We will (w.l.o.g.) focus on the (SoP) objective [It can be
shown for any linear objective given by a vector of coefficients v > 0 the same result holds]. The
unbounded objective has value 2n − 1 since bailing out the central node restores the network to its
initial state. Let g = 0. We have the following results:
• (GC) Solution. Subject to P
absolute equality (i.e. g = 0), it should hold that for the bailout
indicator variables z̄j that j6=1 |z̄1 (g) − z̄j (g)| = 0. Thus z̄j (g) = 0, therefore the objective
value is 0. That yields a PoF of 2n−1
= ∞.
0
• (PGC) Solution. Let q = 1{1} . Again, similarly to the previous case, we get z̄j (g, q) = 0 for
all j. Thus the PoF is ∞.
P
• (SGC) Solution. We again get that (i,j)∈E aij |z̄i (g, A)− z̄j (g, A)| = 0 which yields z̄j (g, A) =
0 for all j ∈ [n]. Again, the PoF is ∞.
Proof of Theorem 10. We will prove the correctness of the Theorem for the (GC) metric since
the other metrics have exactly the same proof. Fix some g ≥ 0. The fractional PoF is equal
to f (e
p∗ (1))/f (e
p∗ (g)). Assume, for contradiction, that the PoF is unbounded. The following can
happen:
1. f (e
p∗ (g)) 6= 0 and f (e
p∗ (1)) is such that the limit of their ratio goes to infinity. This is a
contradiction since f can get values up to kf k∞ . Thus, we arrive at a contradiction.
2. f (e
p∗ (g)) = 0. Since f is strictly increasing with f (0) = 0 then pe∗ (g) = 0. We will show
that pe∗ (g) = 0 if and only if X = c w.p. 1 and ze∗ (g) = 0. We remind here that there
are no isolated nodes (both to the internal and the external sector) and hence p > 0. The
reverse direction is trivial since the fairness constraint is always satisfied (hence it does not
affect the feasible region) and the fixed point operator is simply Φ(e
p∗ (g)) = p ∧ (AT pe(g)), so
Φ(0) = p ∧ 0 = 0, i.e. pe∗ (g) = 0. For the reverse direction, since p > 0 the only way for pe∗ (g)
41

to be 0 is (i) g = Λ = 0 which is impossible since by the definition of the problem Λ > 0,
and (ii) the relative liability non-homogeneous part is zero, i.e. (c − X) + LT ze∗ (g) = 0 (the
fairness constraint is trivially satisfied). Since c − X ≥ 0, L > 0 and ze∗ ≥ 0, the only way
for the equation to hold is X = c with probability 1 and ze∗ (g) = 0, yielding a contradiction.
Therefore, f (e
p∗ (g)) > 0.
Proof of Theorem 11.
• (SGC) Solution. Since g > 0 and A is a connected network (i.e. no isolated nodes) a positive
budget µ ∈ (0, Λ] is spent in total on bailouts. We let
B(µ) = {(e
p0 (g, A), ze0 (g, A))T feasible (g, A)-unfair allocation : LT ze0 = µ}.
Subsequently, we define the following partition of B(µ): Let B1 (µ) = B(µ) ∩ {∃j ∈ [n] :
zej (g, A) 6= µ/(nLj )}, and B2 (µ) = B(µ) ∩ {∀j ∈ [n] : zej (g, A) = µ/(nLj )}. We show that if
B2 (µ) is non-empty then B1 (µ) must be non-empty. Let ze(g, A) ∈ B2 (µ). Then since g > 0
µ
min µ
for 0 < δij ≤ min{ nL
, µ , gβ
βi +βj } we choose two arbitrary nodes i and j such that the
i nLj
former node’s bailout is increased by δij and the latter node’s bailout is decreased by δij ,
that is absolute equality is achieved via the optimal allocation and we are allowed to increase
up to g. The total budget constraint is not violated and the new bailout scheme belongs to
B1 (µ). If the optimal bailout scheme belongs to B2 (µ) perturb the bailouts of two nodes as
we described above to get a sub-optimal assignment (e
p0 (g, A), ze0 (g, A))T ∈ B1 (µ) since the
optimal assignment is also (0, A)-unfair. The altered assignment is a lower bound on the
original assignment per the objective. Below we bound the PoF for the (L-OBJ) objective.
We have:

42

P
e∗j (g, A)
1
j∈[n] vj p
=P
PoF(g,A)
e∗j (1, A)
j∈[n] vj p
P
e0j (g, A)
1
j∈[n] p
(Constrain to B1 (µ))
≥ ·P
ζ
e∗j (1, A)
j∈[n] p
P
P
e0j (g, A) + j∈R(g,A) pe0j (g, A)
1
j∈D(g,A) p
P
= ·P
(Partition of [n])
ζ
e∗j (1, A) + j∈R(1,A) pe∗j (1, A)
j∈D(1,A) p
P
e0j (g, A)
1
j∈D(g,A) p
P
≥ ·P
(Drop Summands)
ζ
e∗j (1, A) + j∈R(1,A) pe∗j (1, A)
j∈D(1,A) p
P
ej0 (g, A))
1 − βmax
j∈D(g,A) (cj − xj + Lj z
P
(Lemmas 2 and 3)
·
≥
ej∗ (1, A))
ζ
j∈[n] (cj − xj + Lj z
P
ej0 (g, A))
1 − βmax
j∈D(g,A) (cj − xj + βj Lj z
P
·
(Budget Constraint, βj < 1)
≥
ζ
j∈[n] (cj − xj ) + Λ
P
1 P
ei0 (g, A) − Lj zej0 (g, A)|
1 − βmax
j∈D(g,A) (cj − xj ) + 2g
(i,j)∈E aij |Li z
P
≥
·
ζ
j∈[n] (cj − xj ) + Λ
(Gini Constraint)
P
0
0
ei (g, A) − Lj zej (g, A)|
1 − βmax
(i,j)∈E aij |Li z
≥
·
(0 ≤ c − x ≤ c)
2gζ
kck1 + Λ
s X
1 − βmax
≥
a2ij (Li zei0 (g, A) − Lj zej0 (g, A))2
(kxk1 ≥ kxk2 )
2gζ(kck1 + Λ)
(i,j)∈E
q
1 − βmax
≥
θe0 (g, A)T L(A A)θe0 (g, A).
(θe0 (g, A) = L ze0 (g, A))
2gζ(kck1 + Λ)
We do a change of variables ye0 (g, A) = θe0 (g, A) − nµ 1 6= 0 since θ0 (g, A) ∈ B1 (µ). Since 1 is
the eigenvector of the 0 eigenvalue we can center the aforementioned quadratic form to be
equal to
ye0 (g, A)T L(A(2) )e
y 0 (g, A) =

ye0 (g, A)T L(A(2) )e
y 0 (g, A)
· ke
y 0 (g, A)k22 .
ke
y 0 (g, A)k22

Moreover,
ye0 (g, A)T L(A(2) )e
y 0 (g, A)
· ke
y 0 (g, A)k22 ≥
ky 0 (g, A)k22

y(g, A)T L(A(2) )y(g, A)
min
ky(g, A)k22
1T y(g,A)=0

!
· ke
y 0 (g, A)k22 .

Using the Courant-Fischer Theorem [Spi12] the above expression evaluates to λ2 (L(A(2) )) ·
ke
y 0 (g, A)k22 , where
ke
y 0 (g, A)k22 =

X
j∈[n]

Lj zej0

2

−

µ2
> 0.
n

For λ2 (L(A(2) )) we use the lower bound provided by Cheeger’s inequality, i.e.
43

λ2 (L(A(2) )) ≥

φ2 (A(2) )
P

2 maxj∈[n]

where φ(A(2) ) = min∅⊂S⊂[n]

2
i∈[n] aji

φ2 (A(2) )
P

≥

2 maxj∈[n]

2
j∈S,i∈S
/ a
P ji
2
i:i∼j aji , j∈[n]\S

i∈[n] aji

P

min{

P

j∈S

P

P

i:i∼j

a2ji }

2 ≥

φ2 (A(2) )
,
2
2βmax

is the conductance of A(2) .

Thus we can bound the PoF by
√

PoF(g,A) ≤

2gζ(kck1 + Λ)βmax
qP
.
(1 − βmax )φ(A(2) ) ·
ej0 )2 − µ2 /n
j∈[n] (Lj z

P
Note that the denominator is always non-zero since βmax < 1, φ(A(2) ) > 0 and j∈[n] (Lj zej0 )2 −
µ2 /n > 0 due to ze0 (g, A) ∈ B1 (µ).
• (GC) Solution. The construction of the sets B(µ), B1 (µ), B2 (µ) is exactly the same as in the
previous case, as well as the derivation of the PoF bound. The only change that in place of
A(2) we put the complete graph Kn on [n]. Since λ2 (L(Kn )) = n we do not need to employ
Cheeger’s inequality, and thus arrive at a bound of the form
√
2gζ(kck1 + Λ) n
qP
PoFg ≤
.
(1 − βmax )
ej0 )2 − µ2 /n
j∈[n] (Lj z
√
Note that there was a factor of 2n on the Gini Coefficient definition so the bound gets a 2 n
factor on the numerator. Again for a finite instance we have that PoFg < ∞.
Proof of Theorem 12. We have that
P

i,j

SGC(e
z ; A) =

where ψ(x; A) =

P
i,j aij |xi −xj |
P
.
j∈[n] βj |xj |

aij |Li zei (g, A) − Lj zej (g, A)|
1
P
= ψ(L
2 j∈[n] βj Lj zej (g, A)
2

z; A).

In Appendix B.2, we prove that

φ(A) =

min

ψ(x; A).

x6=0, median(x)=0

Thus

B
B.1

ψ(L z;A)
2

≥

φ(A)
2 .

Therefore SGC(e
z ; A) ≥

φ(A)
2 .

Transformations
Transforming Weakly-Increasing Objectives to allow clearing vectors

In the case that the (GDBP) problem consists of a function f (p̄) which is increasing (i.e. not
necessarily strictly increasing), there is no guarantee that the solution that will be produced by the
optimization procedure will correspond to a clearing vector (still if the spectral radius of A is less
than 1 the fixed point operator yields a unique solution). For that reason we consider, similarly
to [AK19] the transformed objective, parametrized by  > 0,
(1 − βmax ) T
fˆ(p̄) = f (p̄) +
1 p̄.
2Λ
44

It is obvious that for any p̄ > 0 we have that fˆ(p̄) > f (p̄) since the added term contributes
positively. Moreover fˆ is strictly increasing and therefore an optimal solution p̂ to fˆ subject to
the (GDBP) constraints is a clearing payment vector. Below we will show that if p̄∗ is an optimal
solution to f , and p̂∗ is an optimal solution to fˆ then f (p̂∗ ) − f (p̄∗ ) ≤ . First, from optimality we
have that
fˆ(p̂∗ ) − fˆ(p̄∗ ) ≥ 0.
Equivalently,
f (p̂∗ ) +

(1 − βmax ) T ∗
(1 − βmax ) T ∗
1 p̂ − f (p̄∗ ) −
1 p̄ ≥ 0.
2Λ
2Λ

Equivalently,
f (p̂∗ ) − f (p̄∗ ) ≤

(1 − βmax ) ∗
(1 − βmax )
kp̂ − p̄∗ k1 ≤
kLT ẑ ∗ − LT z̄ ∗ k1 ≤ .
2Λ
2Λ(1 − βmax )

Where the inequalities follow from rearrangements, the triangle inequality, Lemma 5, and that
the maximum distance between any two assignments can be at most 2Λ. Thus the clearing payment
vector from solving the transformed objective yields an objective value which is -close to the desired
optimal value.

B.2

A Property of Conductance

To conclude the proof of Theorem 12 we give a relation between the conductance and Gini-related
Measures. For this subsection only, and for simplicity, we will assume a new notation from the rest
of the paper referring to an unweighted and undirected topology G(V = [n], E) with |V | = n. The
result we prove below is also of independent interest from the rest of the paper. The conductance
of G, φ(G), is given as
φ(G) = min

∅⊂S⊂V

e(S, S̄)
e(S, S̄)
=
min
,
min{|S|, |S̄|} ∅⊂S⊂V,|S|≤n/2 |S|

where e(S, S̄) is the number of edges going from S to its complement S̄ = [n] \ S. More
specifically, we will show that
P

(i,j)∈E

φ(G) = min

|xi − xj |

P

i∈[n] |xi |

x1 ,...,xn

subject to

(COND)

median(x1 , . . . , xn ) = 0
not all xi = 0.

For that reason, we define the function
P
ψ(x) =

(i,j)∈E

P

|xi − xj |

i∈[n] |xi |

.

Firstly, we will show that minx∈(COND) ψ(x) ≤ φ(G). Let S ∗ be the set that achieves φ(G). We
let

45

(
x∗i

=

1
φ(G)

i ∈ S∗

0

i∈
/ S∗

.

Note that since |S ∗ | ≤ n/2 the median of x∗i is 0. Moreover, not all x∗i are 0. We have that
P
P
1
∗
∗
e(S ∗ , S̄ ∗ )
(i,j)∈E,i∈S ∗ ,j∈S̄ ∗ φ(G)
(i,j)∈E |xi − xj |
∗
P
=
=
= φ(G).
ψ(x ) =
∗
1
|S ∗ |
|S ∗ | φ(G)
i∈[n] |xi |
Thus we showed that minx∈(COND) ψ(x) ≤ φ(G). We will now show that minx∈(COND) ψ(x) ≥
φ(G). In this case, we seek to find x1 , . . . xn such that ψ(x) is minimized and the xi ’s have median
0. We define yi = max{xi , 0} and zi = min{xi , 0}. It is elementary to show that |xi | = |yi | + |zi |
and |xi − xj | = |yi − yj | + |zi − zj |. So
P
(i,j)∈E [|yi − yj | + |zi − zj |]
P
ψ(x) =
.
i∈[n] [|yi | + |zi |]
To show our claim, it suffices to show that ψ(y) ≥ φ(G) and ψ(z) ≥ φ(G). This is because
for every a, b, c, d, α ≥ 0 with ac ≥ α and db ≥ α then a+b
c+d ≥ α. By symmetry, we will constrain
ourselves to proving ψ(y) ≥ φ(G), whereas for z the proof is identical. We let y1 = · · · = yn/2 = 0
and yi ≥ 0 for all i ≥ n/2. This guarantees that the median of yi is 0 and that yi ≥ 0 for all i ∈ [n].
For the numerator we have

...

yi

yi+1

yi+2

...

yj

y1 = · · · = yn/2 = 0
C` , ` − 1 ≥ n/2

X

j
X

X

(yj − yi ) =

(y` − y`−1 )

(i,j)∈E,i<j `=i+1

(i,j)∈E,i<j

=

X

(# of copies of the (` − 1, `) segment) · (y` − y`−1 )

`

=

X

=

X

(# of edges that cross over (` − 1, `)) · (y` − y`−1 )

`

`

(# edges in the cut C` partitioning [` − 1] from [n] \ [` − 1] ) ·(y` − y`−1 ).
{z
}
|

A key observation is that
X
`

|C` | · (y` − y`−1 ) ≥

|C` |

|C` |
n−`+1

X

≥ φ(G). Therefore the numerator satisfies

φ(G)(n − ` + 1)(y` − y`−1 ) ≥ φ(G) ·

`

X
`

46

y` = φ(G) ·

X
`

|y` |.

Thus ψ(y) ≥ φ(G). We conclude that φ(G) = minx∈(COND) ψ(x). Theorem 12 is the weighted
version (with weights wijP≥ 0) of this identity where the proof is exactly the same, where we use
P
wij |xi −xj |
P
the function ψ(x; W ) = (i,j)∈E
and di = j wij . Finally, note that if we constrain ψ in
di |xi |
i∈[n]

the domain
σ 0 , for some σ 0 > 0 then we modify an allocation
P in which the denominator is at most
0
x with i∈[n] |xi | = σ > 0 to an allocation x0 = σσ x and the proof will remain valid since ψ is
 0 
scale-invariant, that is ψ(x0 ) = ψ σσ x = ψ(x).

C

Datasets Addendum

German Banks Dataset [CWY16]. The dataset contains n = 22 German Banks, connected
with a liability matrix with m = 435 edges, with a mean outdegree d¯out = 19.8. The network
structure can be described by the ER network G(n = 22, p = 0.94). Below we provide Pareto fits
for the internal liabilities distributions, a regression plot for external assets and external liabilities,
a plot of the financial network with equities (wealths) displayed with colors on a log-scale, and a
distribution plot between external assets and external liabilities with Pareto fits. Observe that the
relation between external assets and external liabilities is a linear function with R2 = 0.999. The
visualization of the dataset appears in Figure 14.
EBA Dataset [GY15]. The EBA dataset contains information about the external assets and
liabilities of each bank as well as the total internal assets of each bank, which by [GY15] are
assumed to be equal to the total internal liabilities of each bank. In Section 4, we have perturbed
the total internal assets by adding some noise, and rebalancing them so that their total sum is
invariant. The assets and liabilities can be approximated by exponential distributions. We report
the distributions and the corresponding fits in Figure 15. The external assets and liabilities are
linearly correlated with R = 0.997.
Venmo Dataset. The venmo dataset consists of n = 7, 178, 381 nodes and m = 7, 024, 837 edges
(transactions) with an average (total) degree per user of 1.9572. The dataset consists of directed
transactions from a sender node to a receiver node. There are no payment amounts data (for privacy
reasons). Figure 16 displays the degree distributions of the dataset and the corresponding powerlaw fits. The indegree distribution law is y ∝ x−2.3 and the outdegree distribution law is y ∝ x−3.7
with corresponding Pearson coefficients −0.86 and −0.89 respectively. We have computed all the
weakly-connected components of the Venmo transaction network and kept 19 weakly-connected
components which contained more than 100 nodes. We generated fake transactions using Pareto
laws.
SafeGraph Data. We give the detailed steps we used and assumptions we relied on to build the
SafeGraph data. We first describe the two different kinds of nodes, that form a bipartite graph
where nodes of each kind lie in their corresponding set of the bipartition:
• POI nodes. We construct the POI nodes by setting a location on Earth and looking at the
kkNN -nearest CBGs that contain POIs according to the Haversine distance. We choose the
location (42.43969750363193, -76.49506530228598) and use kkNN = 3.
• CBG nodes. We use the Monthly Patterns data for the period between April and May 2020
to determine the CBGs that interact with the POIs in question. For each POI, we use the
column visitor home cbg to list the CBGs and the total number of visitors from each CBG
to each POI. These visitors correspond to the number of unique devices that interact with the
POI. Moreover, we assume that each distinct device represents a different person. For each
CBG node we use data from the US Census we also log demographic characteristics such as:
(i) race, (ii) average income, (iii) size of households, (iv) unemployment rates. Using these

47

data for each CBG we calculate: (i) the probability of belonging to a racial minority group
(i.e. non-White), (ii) the average income, (iii) the average size of a household on this CBG,
(iv) the probability of someone in the CBG being unemployed.
The above network consists of 152 nodes (for our choice of parameters), but can be different if
e.g. the starting coordinates or kkNN are changed. We create the edges and the internal liabilities
pij as follows:
• We use the bucketed dwell times to determine the percentage of people that are working in
a business. More specifically, we consider a worker a device that has spent more than 240
min in the POI and we classify the person as a non-worker otherwise. Workers come from
CBGs and are paid wages whereas non-workers come from CBGs and have expenses on the
corresponding POI. For every CBG j let pworker
be this percentage.
j
• For every CBG i that interacts with a POI j with nij people we add an edge (j, i) referring
to nworkers
= bnij × pworker
c workers, and an edge (i, j) referring to nnon−workers
= nij − bnij ×
ij
j
ij
workers
pj
c non-worker nodes. We determine the weights of these edges (per unit) as follows:
for the (j, i) edge we take wage data by the US Economic Census and NAICS and create
a liability by the business equal to the monthly wage of an employee which we multiply by
nworkers
. For the (i, j) edge we use data from the Consumer Expenditure Survey conducted
ij
non−workers
by the US Economic Census and add a liability regarding nij
equal to the average
monthly expenditures due to the specific NAICS code
of
the
business.
P
(and the number of nonThe number of workers for each POI j is nworkers
= i is CBG nworkers
ij
j
workers is defined respectively). Similarly we count the number of workers and number of non
workers for each CBG i. We estimate the total number of households in the CBG that are related
to interaction with the corresponding POIs20 . We estimate the number of households as follows


nworkers
non−workers
i
employed + ni
pi


nhouseholds
=
.
i
 average size of household at CBG i 


The bailouts are determined as follows
• For each CBG i we use the average income of the CBG and the average size of the household
to calculate the value of a bailout as it would be imposed by the CARES act (see [act20]).
The bailout is multiplied by the estimated number of households calculated previously.
• For each POI j we have data from loans that were given on multiple businesses as parts of the
COVID-19 relief plan in the US and are available via SafeGraph. The loan value is normalized
to span a month and is also normalized by the true number of jobs reported in each business
and then is multiplied by the number of workers at POI j in the bipartite network. The
loan data also report the race of its owner (owners can also choose not to answer) so we can
determine if the business is a minority or a non-minority-owned business. To fill the missing
data we use the percentage of minority businesses as a real-valued score for the corresponding
business.
Finally, we determine the external assets and liabilities
• For each POI j we use the total assets earned annually from all establishments, normalized
them by month,divide by the total number of workers for the specific NAICS code that the
POI belongs, and multiply by the number of workers at the POI. Finally, from this amount
we subtract the revenue due to nodes within the network (i.e. inbound edges) and take the
positive part in case the result is negative. The external liabilities of the POI are determined
20

Of course the total number of people and households for each CBG is reported by the US Census the dataset
would not be calibrated if we did not limit the transactions of each CBG with the corresponding POIs.

48

in a similar way with the only change that the total weight of the outbound edges is subtracted
in the end. To ensure that A2 holds throughout the experiments (and, thus, we have a unique
equilibrium) we assert a minimum of $100 for the external liabilities. Such a number is smaller
than the order of most quantities and thus does not significantly affect the results.
• For each CBG i the process is similar where for external assets we use data regarding the
average income as reported by the US Census, following a similar procedure as in the POI
case, but now normalized with respect to the estimated number of households and the inbound
connections from the POIs. Regarding the average expenses of a household, we the national
US average of ∼ $63, 000.
This completes the construction of the SafeGraph semi-artificial data experiment. It is important to note that all values are “normalized” with respect to entities participating in the network.
This normalization process avoids scenaria like the following: there is a CBG where a small percentage of household heads are employed in the corresponding businesses so the weights of the network
(internal liabilities) would have different scale than the external assets and liabilities resulting in
diminishing network effects, namely a lot of liabilities are owned outside the network and hence the
values of βi are very small and thus the effect of the relative internal liability matrix A vanishes (i.e.
βmax = kAT k1 ≈ 0) and thus the clearing payments become approximately p̄ ≈ b ∧ (c − x + LT z̄);
essentially eliminating network effects. In fact, without network effects, the problem is solvable
in O(n log n) time: we calculate p̄0j = bj ∧ (cj − xj + Lj ) in O(n) time, we sort the values vj p0j in
decreasing order in O(n log n) time and then pick the first entries such that the budget constraint is
respected. The correctness of this argument relies on the use of an elementary algebraic argument.

D

Data Ethics Statement

All data in this paper comes from publicly available datasets derived and used in prior research.
The financial networks from banks can be found in the public domain and their corresponding
publications. The Venmo network data are publicly available on GitHub, and do not contain
transaction amounts. The mobility data were obtained from a public SafeGraph dataset in which
privacy techniques have been applied by the data provider, and, thus, no human subjects are identifiable. Because all data consists of public, pre-existing datasets without identifiable individuals,
the current work is exempt from IRB review.

49

(a) Internal liabilities distribution and Pareto fit

(b) External assets and external liabilities regression

(c) Financial network plot with wealths on log-scale (d) External assets and liabilities distributions and
Pareto Fits

Figure 14: German Banks dataset statistics

50

(a) Total Internal Assets distribution with Exponen- (b) External assets and external liabilities regression
tial fit

(c) External assets and liabilities distributions with
Exponential fit

(d) Equity distribution and Exponential fit

Figure 15: EBA dataset statistics

51

(a) Indegree distribution

(b) Outdegree distribution

Figure 16: Venmo dataset degree distributions with power-law fits.

52

