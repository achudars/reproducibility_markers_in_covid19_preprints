arXiv:1902.08123v4 [cs.CV] 28 Oct 2020

Cross-Sensor Periocular Biometrics for Partial Face
Recognition in a Global Pandemic: Comparative
Benchmark and Novel Multialgorithmic Approach
Fernando Alonso-Fernandeza,∗, Kiran B. Rajab , R. Raghavendrab , Christoph Buschb ,
Josef Biguna , Ruben Vera-Rodriguezc , Julian Fierrezc
a School

of Information Technology, Halmstad University, Sweden
University of Science and Technology, Gjøvik, Norway
c School of Engineering, Universidad Autonoma de Madrid, Spain

b Norwegian

Abstract
The massive availability of cameras and personal devices results in a wide variability
between imaging conditions, producing large intra-class variations and a significant
performance drop if images from heterogeneous environments are compared for person recognition purposes. However, as biometric solutions are extensively deployed,
it will be common to replace acquisition hardware as it is damaged or newer designs
appear, or to exchange information between agencies or applications operating in different environments. Furthermore, variations in imaging spectral bands can also occur.
For example, face images are typically acquired in the visible (VIS) spectrum, while
iris images are usually captured in the near-infrared (NIR) spectrum. However, crossspectrum comparison may be needed if, for example, a face image obtained from a
surveillance camera needs to be compared against a legacy database of iris imagery.
Here, we propose a multialgorithmic approach to cope with periocular images captured with different sensors. With face masks in the front line to fight against the
COVID-19 pandemic, periocular recognition is regaining popularity since it is the only
region of the face that remains visible. In this paper, we integrate different biometric
comparators using a fusion scheme based on linear logistic regression, in which fused
∗ Corresponding

author
Email addresses: feralo@hh.se (Fernando Alonso-Fernandez), kiran.raja@ntnu.no (Kiran
B. Raja), raghavendra.ramachandra@ntnu.no (R. Raghavendra),
christoph.busch@ntnu.no (Christoph Busch), josef.bigun@hh.se (Josef Bigun),
ruben.vera@uam.es (Ruben Vera-Rodriguez), julian.fierrez@uam.es (Julian Fierrez)

Preprint submitted to Journal of LATEX Templates

October 29, 2020

scores are represented by log-likelihood ratios. This allows easy interpretation of output scores and the use of Bayes thresholds for optimal decision-making since scores
from different comparators are in the same probabilistic range. We evaluate our approach in the context of the 1st Cross-Spectral Iris/Periocular Competition, whose aim
was to compare person recognition approaches when periocular data from visible and
near-infrared images is matched. The proposed fusion approach achieves reductions
in the error rates of up to 30-40% in cross-spectral NIR-VIS comparisons with respect
to the best individual system, leading to an EER of 0.2% and a FRR of just 0.47% at
FAR=0.01%. It also represents the best overall approach of the mentioned competition. Experiments are also reported with a database of VIS images from two different
smartphones as well, achieving even bigger relative improvements and similar performance numbers. We also discuss the proposed approach from the point of view of
template size and computation times, with the most computationally heavy comparator playing an important role in the results. Lastly, the proposed method is shown to
outperform other popular fusion approaches in multibiometrics, such as the average of
scores, Support Vector Machines, or Random Forest.
Keywords: Periocular recognition, sensor interoperability, cross-spectral,
cross-sensor, ocular biometrics, multibiometrics fusion, linear logistic regression.

1. Introduction
Periocular biometrics has gained attention during the last years as an independent
modality for person recognition [1, 2] after concerns of the performance of face or iris
modality under non-ideal or uncooperative conditions [3, 4]. The mandatory use of
face masks due to the COVID-19 pandemic has produced that, even in cooperative settings, face recognition systems are presented with occluded faces where the periocular
region is often the only visible area. In parallel, hygiene concerns are triggering fears
against the use of contact-based biometric solutions such as fingerprints [5]. Studies
have shown that commercial face recognition engines, even in cooperative settings,
struggle with persons wearing face masks [6], driving vendors to include capabilities
for recognition of masked faces in their products [7].

2

According to the Merriam-Webster dictionary, the medical definition of “periocular” is “surrounding the eyeball but within the orbit”. From a forensic/biometric application perspective, our goal is to improve the recognition performance by using
information extracted from the face region in the immediate vicinity of the eye, including the sclera, eyelids, eyelashes, eyebrows and the surrounding skin (Figure 1).
This information may include textural descriptors, but also the shape of the eyebrows
or eyelids, or color information [1]. With a surprising high discrimination ability, the
resulting modality is the ocular one requiring the least constrained acquisition. It is sufficiently visible over a wide range of distances, even under partial face occlusion (close
distance) or low-resolution iris (long distance), facilitating increased performance in
unconstrained or uncooperative scenarios. It also avoids the need for iris segmentation,
an issue in difficult images [8]. The COVID-19 outbreak has imposed the necessity
of dealing with partially occluded faces even in cooperative applications in security,
healthcare, border control or education. Another advantage in the context of the current global pandemic is that the periocular region appears in iris and face images, so it
can be easily obtained with existing setups for face and iris.

Figure 1: Eye image labeled with some parts of the ocular region.

The ocular region consists of several organs such as the cornea, pupil, iris, sclera,
lens, retina, optical nerve, and the periocular region. Some of them are shown in Figure 1. Among these, iris, sclera, retina and periocular have been studied as biometric
modalities [2]. The significant progress of ocular biometrics in the last decade has been
3

primarily due to efforts in iris recognition since the late 80s, resulting in large-scale deployments [9]. Iris provides very high accuracy with near-infrared (NIR) illumination
and controlled, close-up acquisition. However, deployment to non-controlled environments is not yet mature due to the impact of low resolution, variable illumination, or
off-angle views, which makes very difficult to locate and segment the iris [8]. Even if
the latter can be achieved, the quality of the resulting iris image might not be sufficient
for accurate recognition either [10]. The feasibility of vasculature of the sclera as a
biometric modality (sometimes simply referred to as sclera) has also been established
by several studies [11], although its acquisition in non-controlled environments poses
the same problems than the iris modality. The vasculature of the retina is also very discriminative, and retina is regarded as the most secure biometric modality due to being
extremely difficult to spoof. However, its acquisition is very invasive, requiring high
user cooperation and specialized optical devices.
In this context, periocular has rapidly evolved as a very popular modality for unconstrained biometrics [2, 1, 11], and recently due to the use of face masks even in
constrained settings [6]. The term periocular is used loosely in the literature to refer to
the externally visible region of the face that surrounds the eye socket. Therefore, images of the whole eye, such as the one in Figure 1, are employed as input [11]. While
the iris, sclera and other elements are present in such images, they are not explicitly
used in isolation. It may be that the iris texture or the vasculature of the sclera cannot
be reliably obtained either to be used as stand-alone modalities [10]. Some works even
suggest that with visible light data, recognition performance is improved if components
inside the ocular globe (iris and sclera) are discarded [12]. The fast-growing uptake of
face technologies in social networks and smartphones, as well as the widespread use
of surveillance cameras or face masks, has arguably increased the interest of periocular
biometrics, specially in the visible (VIS) range. In such scenarios, samples captured
with different sensors are to be compared if, for example, users are allowed to use their
own acquisition devices, leading to a cross-sensor comparison in the same spectrum
(VIS-VIS in this case). Unfortunately, this massive availability of cameras results in
heterogeneous quality between images [13], which is known to decrease recognition
performance significantly [9]. These sensor interoperability issues also arise when a
4

biometric sensor is replaced with a newer one without reacquiring the corresponding
template, thus forcing biometric samples from different sensors to co-exist. Sensors
may also operate in a range other than VIS, such as NIR, leading to cross-sensor NIRNIR comparisons, e.g. [14]. In addition, iris images are largely acquired beyond the
visible spectrum [15], mainly using NIR illumination, but there are several scenarios in
which it may be necessary to compare them with periocular images in the VIS range,
leading in this case to a cross-sensor comparison in different spectra (NIR-VIS in this
case), also known as cross-spectral comparison. This happens for example in law enforcement scenarios where the only available image of a suspect is obtained with a
surveillance camera in the VIS range, but the reference database contains images in
the NIR range [16, 17]. These interoperability problems, if not properly addressed,
can affect the recognition performance dramatically. Unfortunately, widespread deployment of biometric technologies will inevitably cause the replacement of hardware
parts as they are damaged or newer designs appear. Another application case is the
exchange of information among agencies or applications which employ different technological solutions, or whose data is captured in heterogeneous environments. The
different types of image comparisons mentioned, based on the spectrum in which they
have been acquired, are summarized in Figure 2.

Figure 2: Sensor interoperability in periocular biometrics.

In this paper, we combine the output of different periocular comparators at the score
level, referred to as multialgorithm fusion (in contrast to multimodal fusion, which
combines information from different modalities) [18, 19]. The consolidation of identity evidence from heterogeneous comparators (also called experts, feature extraction
techniques, or systems in the present paper) is known to increase recognition perfor5

mance, because the different sources can compensate for the limitations of the others
[18, 20]. Integration at the score level is the most common approach, because it only
needs the output scores of the different comparators, greatly facilitating the integration.
With this motivation, we employ a multialgorithm fusion approach to cope with periocular images from different sensors which integrates scores from different comparators.
It follows a probabilistic fusion approach based on linear logistic regression [21], in
which the output scores of multiple systems are combined to produce a log-likelihood
ratio according to a probabilistic Bayesian framework. This allows easy interpretation
of output scores, and the use of Bayes thresholds for optimal decision-making. This
fusion scheme is compared with a set of simple and trained fusion rules widely employed in multibiometrics based on the arithmetic average of normalized scores [22],
Support Vector Machines [23], and Random Forest [24].
The fusion approach based on linear logistic regression served as an inspiration
to our submission to the 1st Cross-Spectral Iris/Periocular Competition (Cross-Eyed
2016) [25], with an outstanding recognition accuracy: Equal Error Rate (EER) of
0.29%, and False Rejection Rate (FRR) of 0% at a False Acceptance Rate (FAR) of
0.01%, resulting in the best overall competing submission. This competition was aimed
at evaluating the capability of periocular recognition algorithms to compare visible and
near-infrared images (NIR-VIS). In the present paper, we also carry out cross-sensor
experiments with periocular images in the visible range only (VIS-VIS), but with two
different sensors. For this purpose, we employ a database captured with two smartphones [26], demonstrating the benefits of the proposed approach to smartphone-based
biometrics as well.
The rest of the paper is organized as follows. This introduction is completed with
the description of the paper contributions. A summary of related works in periocular
biometrics is given in Section 2. Section 3 then describes the periocular comparators
employed. The score fusion methods evaluated are described in Section 4. Recognition experiments using images in different spectra (cross-spectral NIR-VIS) and in
the visible spectrum (cross-sensor VIS-VIS) are described in Sections 5 and 6, respectively, including the databases, protocol used, results of the individual comparators,
and fusion experiments. Finally, conclusions are given in Section 7.
6

Figure 3: Example images from Cross-Eyed (top row) and VSSIRIS (bottom row) databases. First column:
input image. Second: after applying CLAHE (see Sect. 5.1). Third and fourth: ROI of the different biometric
comparators (see Sect. 3).

1.1. Contributions
The contribution of this paper to the state of the art is thus as follows. First, we
summarize related works in periocular biometrics using images from different sensors.
Second, we evaluate nine periocular recognition comparators under the frameworks
of different spectra (NIR-VIS) and same spectrum (VIS-VIS) recognition. The Reading Cross-Spectral Iris/Periocular Dataset (Cross-Eyed) [25] and the Visible Spectrum
Smartphone Iris (VSSIRIS) [26] databases are respectively used for this purpose. We
employ the three most widely used comparators in periocular research, which are used
as a baseline in many studies [1]: Histogram of Oriented Gradients (HOG) [27], Local
Binary Patterns (LBP) [28], and Scale-Invariant Feature Transform (SIFT) key-points
[29]. Three another periocular comparators, proposed and published previously by the
authors, are based on Symmetry Descriptors [30], Gabor features [31], and Steerable
Pyramidal Phase Features [32]. The last three comparators use feature vectors extracted
by three Convolutional Neural Networks: VGG-Face [33], which has been trained for
classifying faces, so the periocular region appears in the training data, and the verydeep Resnet101 [34] and Densenet201 [35] networks. Two example images from the
two databases employed are shown in Figure 3 (first column). The second column
7

shows the two images after applying Contrast Limited Adaptive Histogram Equalization (CLAHE) [36], whereas the last two columns show the regions of interest (ROI)
used by the different comparators. The comparators are evaluated both in terms of performance, template size and computation times. In a previous study [37], we presented
preliminary results with the VSSIRIS database using a subset of the mentioned comparators [10, 30, 31], which are extended in the present paper with additional experiments using new comparators [32, 33, 34, 35] and the mentioned Cross-Eyed database.
Third, we describe our multialgorithm fusion architecture for periocular recognition
using images from different sensors (Figure 4). The input to a biometric comparator is usually a pair of biometric samples, and the output is in general a similarity
score s. A larger score favours the hypothesis that the two samples come from the
same subject (target or client hypothesis), whereas a smaller score supports the opposite (non-target or impostor hypothesis). However, if we consider a single isolated
score from a biometric comparator (say a similarity score of s=1), it is in general not
possible to determine which is the hypothesis the score supports the most, unless we
know the distributions of target or non-target scores. Moreover, since the scores output by the various comparators are heterogeneous, score normalization is needed to
transform these scores into a common domain prior to the fusion process [18]. We
solve these problems by linear logistic regression fusion [38, 39], a trained classification approach in which scores of the individual comparators are combined to obtain a
log-likelihood ratio. This is the logarithm of the ratio between the likelihood that input
signals were originated by the same subject, and the likelihood that input signals were
not originated by the same subject. This form of output is comparator-independent, in
the sense that this log-likelihood-ratio output can theoretically be used to make optimal
(Bayes) decisions. To convert scores from different comparators into a log-likelihood
ratio, we evaluate two possibilities (Figure 5). In the first one (top part), the mapping
function uses as input the scores of all comparators, producing a single log-likelihood
ratio as output. In the second one (bottom), several mapping functions are trained
(one per comparator), so one log-likelihood ratio per comparator is obtained. Under
independence assumptions (as in the case of comparators based on different feature
extraction methods), the sum of log-likelihood ratios results in another log-likelihood
8

Figure 4: Architecture of the proposed fusion strategy.

ratio [40]. Therefore, in the second case, the outputs of the different mapping functions
are just summed. The latter provides a simple fusion framework that allows obtaining
a single log-likelihood ratio by simply summing the (mapped) score given by each
available comparator. This would allow coping with missing modalities [41], since the
output still would be a log-likelihood ratio regardless of the number of systems combined. This fusion approach has been previously applied successfully to cross-sensor
comparison in face and fingerprint modalities [21], achieving excellent results in other
competition benchmarks as well [41]. Fourth, we compare this fusion approach with
a set of simple and trained score fusion rules based on the arithmetic average of normalized scores [22], Support Vector Machines [23], and Random Forest [24]. These
fusion approaches are very popular in the literature, having demonstrated to give good
results in biometric authentication [18]. Fifth, in our experiments, conducted according
to the 1st Cross-Spectral Iris/Periocular Competition (Cross-Eyed 2016) protocol [25],
reductions of up to 29/47% in EER/FRR error rates (with respect to the best individual
system) are obtained by fusion under NIR-VIS comparison, resulting in a cross-spectral
EER of 0.2%, and a FRR @ FAR=0.01% of just 0.47%. Regarding cross-sensor VISVIS smartphone recognition, the reductions in error rates achieved are of 85/93% in
EER/FRR respectively, with corresponding cross-sensor error values of 0.3% (EER)
and 0.3% (FRR).

9

Figure 5: Strategies to convert scores from multiple subsystems to a log-likelihood ratio (LLR). Top: one
single mapping function is trained to convert multiple scores into a single LLR. Bottom: several mapping
functions are trained to convert the score of each subsystem into a LLR. The sum of LLRs from different
subsystems also results in a LLR. See the text for details.

2. Related Works in Periocular Biometrics Using Images from Different Sensors
Interoperability between different sensors is an area of high research interest due to
new scenarios arising from the widespread use of biometric technologies, coupled with
the availability of multiple sensors and vendor solutions. A summary of existing works
in the literature is given in Table 1. Most of them employ the Genuine Acceptance
Rate (GAR) as metric, which is computed as 100-FRR(%). For this reason, in this
subsection, we report GAR values. However, in the rest of the paper, we will follow
the Cross-Eyed protocol, and will report FRR values.
Cross-sensor comparison of images in the visible range (VIS-VIS) from smartphone sensors is carried out for example in [42, 43, 44], while the challenge of comparing images from different sensors in the near-infrared spectrum (NIR-NIR) has been
addressed in [14]. In the work [43], the authors apply Laplacian decomposition (LD) of
the image coupled with dynamic scale selection, followed by frequency decomposition
via Short-Term Fourier Transform (STFT). In the experiments, they employ a subset of
10

Best accuracy
People/
Ref.

Features

GAR @
# Eyes

Comparison

EER

1%FAR

GAR @

GAR @

Database

Images

0.1%FAR 0.01%FAR

Rank-1

CSIP

50/2004

VIS-VIS

single

15.5%

-

-

-

MICHE I

50/n-a

VIS-VIS

single

6.38-8.33%

-

-

-

-

[44] GMM-UBM, SV-SDA, CNN

CSIP

50/2004

VIS-VIS

single

-

-

-

-

83.6-93.3%

this work: 9 comparators

VSSIRIS

56/560

VIS-VIS

single

0.3%

-

-

99.7%

-

-

-

-

Cross-sensor comparisons in the visible range (VIS-VIS)
[42] LBP, HOG, SIFT, ULBP, GIST
[43]

LD+STFT

-

Cross-sensor comparisons in the near-infrared range (NIR-NIR)
[14]

OM

own

300/9000

NIR-NIR

single

20-28%

-

[16]

LBP, NGC, JDSR

own

704/1358

VIS-NIR

single

23%

-

-

-

-

[45]

PHOG

IIITD-IMP 62/1240

VIS-NIR

single/both

-

38.36/47.08%

-

-

-

VIS-night

single/both

-

63.81/71.93%

-

-

-

NIR-night

single/both

-

40.36/48.21%

-

-

-

VIS-SWIR 1.5/50/106m

single

7.32/24.87/31.18%

-

-

-

68.75/33.33/31.94%

Cross-sensor comparisons across different spectra (cross-spectrum)

[46]

Gabor+
WLD/LBP/HOG

[47]

MRF+
TPLBP/FPLB

[48]

DOG+LBP/HOG

Pre-Tinders 48/576
Tinders

48/1255

VIS-NIR 1.5/50/106m

single

4.42/25.71/39.01%

-

-

-

70.31/38.54/10.76%

PCSO

1000/3000

VIS-MWIR 1.5m

single

30.46%

-

-

-

5.58%

Q-FIRE

82/431

VIS-LWIR 2m

single

39.06%

-

-

-

8.09%

VIS-NIR

single

-

-

15.93-18.35%

-

-

VIS-NIR

single

19.8-32.5%

-

45.4-73.2%

-

-

VIS-NIR

single/both

43.85/45.29%

-

24.97/25.03%

-

-

209/12540

VIS-NIR

single/both

18.79/13.87%

-

73.12/83.12%

-

-

Cross-Eyed 120/3840

VIS-NIR

single/both

15.11/10.36%

-

80.03/89.27%

-

-

8 bands

both

-

-

-

-

8.46-91.92%

IIITD IMP 62/1240
PolyU

IIITD-IMP 62/1240
PolyU

[49]

HOG, GIST, LG, BSIF

[50]

CNN

this work: 9 comparators

209/12540

own

52/4160

IIITD-IMP 62/1240

Cross-Eyed 120/3840

VIS-NIR

single

5.19%

88.13%

-

-

-

VIS-night

single

5.13%

88.19%

-

-

-

NIR-night

single

10.19%

81.55%

-

-

-

VIS-NIR

single

0.2%

-

-

99.53%

-

Table 1: Overview of existing works in periocular biometrics using images from different sensors. The works
of each sub-section are in cronological order. The acronyms of this table are fully defined in the text.

50 periocular instances from the MICHE I dataset (Mobile Iris Challenge Evaluation
I dataset) [51], captured with the front and rear cameras of two smartphones in indoor
and outdoor illuminations. The cross-sensor EER obtained ranges from 6.38 to 8.33%
for the different combinations of reference and probe cameras. The authors in [42] use
a sensor-specific color correction technique, which is estimated by using a color chart
in a dark acquisition scene that is further illuminated by a standard illuminant. The
authors also carry out score-level fusion of six iris and five periocular comparators,
which is done by Neural Networks. The five periocular features include Local Bi-

11

nary Patterns (LBP), Histogram of Oriented Gradients (HOG), Scale-Invariant Feature
Transform (SIFT) key-points, Uniform Local Binary Patterns (ULBP) [52], and the
perceptual GIST descriptors [53]. They also presented a new database (CSIP: CrossSensor Iris and Periocular), with 2004 periocular images from 50 subjects captured
with four different smartphones in ten different setups (based on several combinations
involving the use of frontal/rear cameras and flash/no flash). The best reported periocular performance by fusion of the five available comparators is EER=15.5%. The same
database is also employed in [44], where the authors apply three different methods to
solve the cross-sensor task: Gaussian Mixture Models coupled with Universal Background Models (GMM-UBM), GMM Supervectors coupled with Stacked Denoising
Autoencoders (SV-SDA), and deep transfer learning with Convolutional Neural Networks (CNN). They achieve a rank-1 recognition rate of 93.3% in the best possible
case. The work [14], on the other hand, addresses the issue of cross-sensor recognition
in the NIR spectrum. The authors employ a self-captured database with 9000 iris images from 600 eyes (300 people) using three different high-resolution sensors. Sensor
interoperability is dealt with by weighted fusion of information from multiple directions of Ordinal Measures (OM), with a reported cross-sensor periocular EER between
20 and 28%.
Regarding recognition across different spectra (cross-spectral), the work [16] proposes to compare images of the periocular region cropped from VIS face images against
NIR iris images. This is because face images are usually captured in the visible range,
while iris images in commercial systems are usually acquired using near-infrared illumination. They employ three different comparators based on Local Binary Patterns
(LBP), Normalized Gradient Correlation (NGC), and Joint Database Sparse Representation (JDSR). Using a self-captured database with 1358 images of the left eye from
704 subjects, they report a cross-spectral EER of 23% by score-level fusion of the three
comparators.
In another line of work, surveillance at night or in harsh environments has prompted
interest in new imaging modalities. For example, the authors in [45] presented the IIITD Multispectral Periocular database (IIITD-IMP), with a total of 1240 VIS, NIR
and Night Vision images from 62 subjects (the latter captured with a video camera in
12

Night Vision mode). To cope with cross-spectral periocular comparisons, they employ
Neural Networks to learn the variabilities caused by each pair of spectra. The employed comparator is based on Pyramid of Histogram of Oriented Gradients (PHOG)
[54]. They report results for each eye separately, and for the combination of both eyes,
obtaining a cross-spectral GAR of 38-64% at FAR=1% (best of the two eyes), and a
GAR of 47-72% combining the two eyes. The use of pre-trained Convolutional Neural
Networks (CNN) as feature extraction method for NIR-VIS comparison was recently
proposed in [50]. Here, the authors identify the layer of the ResNet101 network that
provides the best performance on each spectrum. Then, they train a Neural Network
that uses as input the feature vector of the best respective layers. Using the IIITD-IMP
database, they report results considering the left and right eyes of a person as different
users (effectively duplicating the number of classes). The obtained cross-spectral accuracy is EER=5-10% and GAR=81-88% at FAR=1%, which outperforms any previous
study with this database. The authors in [47] employ the IIITD-IMP database, and a
newly presented database, the Hong Kong Polytechnic University Cross-Spectral Iris
Images Database (PolyU), with 12540 images from 209 subjects. To carry out NIRVIS comparison, they use Markov Random Fields (MRF) combined with two different
feature extraction methods, variants of Local Binary Patterns (LBP), namely FPLBP
(Four-Patch LBP) and TPLBP (Three-Patch LBP). They report a cross-spectral periocular GAR at FAR=0.1% of 16-18% (IIITD-IMP) and 45-73% (PolyU). These two
databases, together with the Cross-Eyed database (with 3840 images in NIR and VIS
spectra from 120 subjects) [25] are used in the work [48]. To normalize the differences
in illumination between NIR and VIS images, they apply Difference of Gaussian (DoG)
filtering. The comparators employed were based on Local Binary Patterns (LBP) and
Histogram of Oriented Gradients (HOG) features. They report results for each eye
separately, and for the combination of both eyes. The IIITD-IMP database gives the
worst results, with a cross-spectral EER of 45% and a GAR at FAR=0.1% of only 25%
(two eyes combined). The reported accuracy with the other databases is better, ranging
between 10-14% (EER) and 83-89% (GAR).
Latest advancements have resulted in devices with the ability to see through fog,
rain, at night, and to operate at long ranges. In the work [46], the authors carry out ex13

periments with several databases containing images with different wavelengths, namely
VIS, NIR, SWIR (ShortWave Infrared), MWIR (MiddleWave Infrared), and LWIR
(LongWave Infrared). The images are captured at several stand-off distances of 1.5 m,
2 m, 50 m, and 105 m. Feature extraction is done with a bank of Gabor filters, with
the magnitude and phase responses further encoded with three descriptors: Weber Local Descriptor (WLD) [55], Local Binary Patterns (LBP), and Histogram of Oriented
Gradients (HOG). Extensive experiments are done in this work comparing SWIR, NIR,
MWIR and LWIR periocular probes to a gallery of VIS images. As expected, accuracy
decreased as the standoff distance increases. Also, the comparison of MWIR or LWIR
images to VIS images shows poor performance, attributable to the fact that MWIR and
LWIR imagery measures the heat of a body, while visible imagery measures reflected
light. Recently, the work [49] presented a new multispectral database captured in eight
bands across the VIS and NIR spectra (530 to 1000 nm). A total of 4160 images
from 52 subjects were acquired using a custom-built sensor which captures periocular
images simultaneously in the eight bands. The comparators evaluated are based on Histogram of Oriented Gradients (HOG), perceptual descriptors (GIST), Log-Gabor filters
(LG), and Binarized Statistical Image Features (BSIF). The cross-band accuracy varies
greatly depending on the reference and probe bands, ranging from 8.46% to 91.92%
rank-1 identification rate.

3. Periocular Comparators
This section describes the biometric comparators used for periocular recognition.
We employ nine different comparators, whose choice is motivated as follows. Three
comparators are based on the most widely used features in periocular research, which
are employed as a baseline in many studies [1]: Histogram of Oriented Gradients
(HOG) [27], Local Binary Patterns (LBP) [28], and Scale-Invariant Feature Transform (SIFT) key-points [29]. Other three comparators, available in-house, have been
self-developed by the authors and published previously with competitive results. These
are based on Symmetry Descriptors (SAFE) [30], Gabor features (GABOR) [31], and
Steerable Pyramidal Phase Features (NTNU) [32]. We also employ three comparators

14

Figure 6: Example of some feature extraction methods employed. SAFE comparator. Example of symmetric curve families and complex filters used to detect the patterns. Hue in color images encode the direction,
and saturation represents the complex magnitude. GABOR comparator. Gabor filters with vertical orientation (top: real part, bottom: imaginary part). Depicted filters are of size 88×88, with wavelengths spanning
logarithmically the range from 44 (first column) to 6 pixels (last column). LBP and HOG comparators.
Example of LBP and HOG features of the input image shown in Figure 3 (top row).

based on deep Convolutional Neural Networks: the VGG-Face (VGG) network [33],
which has been trained for classifying faces (so the periocular region appears in the
training data), and the two very-deep Resnet101 [34] and Densenet201 [35] architectures.
3.1. Based on Symmetry Patterns (SAFE)
This comparator employs the Symmetry Assessment by Feature Expansion (SAFE)
descriptor [30], which encodes the presence of various symmetric curve families around
image key-points (Figure 6, top). We use the eye center as anchor point for feature extraction. The algorithm starts by extracting the complex orientation map of the image,

15

via symmetry derivatives of Gaussians [56]. We employ S=6 different scales in computing the orientation map, therefore capturing features at different scales, with standard deviation of each scale given by σs = K s−1 σ0 (with s = 1, 2..., S; K = 21/3 ;
σ0 = 1.6). These parameters have been chosen according to [29]. For each scale, we
then project Nf = 3 ring-shaped areas of different radii around the eye center onto
an space of Nh = 9 harmonic functions. We use the result of scalar products of complex harmonic filters (shown in Figure 6) with the orientation image to quantify the
amount of presence of different symmetric pattern families within each annular band.
The resulting complex feature vector is given by an array of S × Nh × Nf elements.
The comparison score M ∈ C between a query q and a test SAFE array t is computed
using the triangle inequality as M =

hq,ti
h|q|,|t|i .

The argument ∠M represents the angle

between the two arrays (expected to be zero when the symmetry patterns detected coincide for reference and test feature vectors, and 180◦ when they are orthogonal), and the
confidence is given by |M | ∈ [0, 1]. To include confidence into the angle difference,
we use M S = |M | cos ∠M , with the resulting score M S ∈ [−1, 1].
The annular band of the first ring is set in proportion to the distance between eye
corners (Cross-Eyed database) or to the radius of the sclera circle (VSSIRIS database),
while the band of the last ring ends at the boundary of the image. This difference
in setting the smallest ring is due to the ground-truth information available for each
database, as explained later. However, in setting the origin of the smallest band, we
have tried to ensure that the different annular rings capture approximately the same
relative spatial region in both databases. The ROI of the SAFE comparator for each
database is shown in Figure 3 (third column). Using the eye corners or the sclera
boundary as reference for the first annular band alleviates the effect of dilation that
affects the pupil, which is more pronounced with visible illumination. Since the eye
corners or the sclera are not affected by such dilation, or by partial occlusion due to
eyelids, they provide a more stable reference [57].
3.2. Based on Gabor Features (GABOR)
This comparator is described in [31], which is based on the face recognition comparator presented in [58]. The periocular image is decomposed into non-overlapped
16

square regions (Figure 3, fourth column), and the local power spectrum is then sampled at the center of each block by a set of Gabor filters organized in 5 frequency and
6 orientation channels. An example of Gabor filters is shown in Figure 6. This sparseness of the sampling grid allows direct Gabor filtering in the image domain without
needing the Fourier transform, with significant computational savings and feasibility
in real-time. Gabor responses from all grid points are grouped into a single complex
vector, and the comparison between two images is done using the magnitude of complex values via the χ2 distance. Prior to the comparison with magnitude vectors, they
are normalized to a probability distribution (PDF). The χ2 distance between a query
N
P
(pq [n]−pt [n])2
q and a test vector t is computed as χ2qt =
pq [n]+pt [n] , where p are entries in
n=1

the PDF, n is the bin index, and N is the number of bins in the PDF (dimensionality).
The χ2 distance, due to the denominator, gives more weight to low probability regions
of the PDF. For this reason, it has been observed to produce better results than other
distances when using normalized histograms [59].
3.3. Based on Steerable Pyramidal Phase Features (NTNU)
Image features from multi-scale pyramids have proven to extract discriminative
features in many earlier works concerned with texture synthesis, texture retrieval, image fusion, and texture classification among others [60, 61, 62, 63, 64, 65, 66, 67].
Inspired by this applicability, we employ steerable pyramidal features for periocular
image classification using images from different sensors. Further, observing the nature
of textures that are different across spectrum (NIR versus VIS), we propose to employ the quantized phase information from the multi-scale pyramid of the image, as
explained next.
A steerable pyramid is a translation and rotation invariant transform in a multiscale, multi-orientation and self-inverting image decomposition into a number of subbands [68, 69, 70]. The pyramidal decomposition is performed using directional derivative operators of a specific order. The key motivation in using steerable pyramids is to
obtain both linear and shift-invariant features in a single operation. Further, they not
only provide multi-scale decomposition but also provide the advantages of orthonormal
wavelet transforms that are both localized in space and spatial-frequency with aliasing
17

effects [68]. The basis functions of a steerable pyramid are K-order directional derivative operators. The steerable pyramids come in different scales and K + 1 orientations.
For a given input image, the features of steerable pyramid coefficients can be represented using S(m,θ) , where m represents the scale and θ represents the orientation.
In this work, we generate a steerable pyramid with 3 scales (m ∈ {1, 2, 3}) and angular coefficients in the range θ1 = 0 to θK+1 = 360, resulting in a pyramid which
covers all directions. The set of sub-band images corresponding to one scale can be
therefore represented as Sm = {S(m,θ1 ) , S(m,θ2 ) , . . . S(m,θK+1 ) }. We further note that
the textural information represented is different in the NIR and VIS domains. In order to obtain domain invariant features, we propose to extract the local phase features
[71] from each sub-band image S(m,θ) in a local region ω in the neighbourhood of n
pixels given by F(m,θ) (u, x) = S(m,θ) (x, y)ωR (y − x) exp{−j2πU T y}, where x, y
represent the pixel location. The local phase response obtained through Fourier coefficients are computed for the frequency points u1 , u2 , u3 and u4 , which relate to four
points [a, 0]T , [0, a]T , [a, a]T , [a, −a]T such that the phase response H(ui ) > 0 [71].
The phase information presented in the form of Fourier coefficients is then separated
into real and imaginary parts of each component, as given by [Re{F }, Im{F }], to
form a vector R with eight elements. Next, the elements Ri of R are binarized to Qi
by assigning a value of 1 to components with a response greater than 1, and 0 otherwise. The phase information is finally encoded to a compact pixel representation P
in the 0 − 255 range by using simple binary to decimal conversion strategy given by
P8
P(m,θ) = j=1 Qj × (2(j−1) ).
This procedure is followed with the different scales and orientations of the selected
space. All the phase responses P(m,θ) of the input image are concatenated into a single
vector. Comparison between feature representations of two images is done using the
χ2 distance.
3.4. Based on SIFT Key-points (SIFT)
This comparator is based on the SIFT operator [29]. SIFT key-points (with dimension 128 per key-point) are extracted in the annular ROI shown in Figure 3, third
column. The recognition metric between two images is the number of paired key18

points, normalized by the minimum number of detected key-points in the two images
being compared. We use a free C++ implementation of the SIFT algorithm1 , with the
adaptations described in [72]. Particularly, it includes a post-processing step to remove
spurious pairings using geometric constraints, so pairs whose orientation and length
differ substantially from the predominant orientation and length are removed.
3.5. Based on Local Binary Patterns (LBP) and Histogram of Oriented Gradients
(HOG)
Together with SIFT key-points, LBP [28] and HOG [27] have been the most widely
used descriptors in periocular research [1]. An example of LBP and HOG features is
shown in Figure 6, bottom. The periocular image is decomposed into non-overlapped
regions, as with the Gabor comparator (Figure 3, fourth column). Then, HOG and
LBP features are extracted from each block. Both HOG and LBP are quantized into
8 different values to construct an 8 bins histogram per block. Histograms from each
block are then normalized to account for local illumination and contrast variations and
finally concatenated to build a single descriptor of the whole periocular region. Image
comparison with HOG and LBP can be made by simple distance measures. Euclidean
distance is usually used for this purpose [10], but here we employ the χ2 distance for
the same reasons than with the Gabor comparator.
3.6. Based on Deep Convolutional Neural Networks (VGG, Resnet101, Densenet201)
Inspired by the works [73, 74, 50] in iris and periocular biometrics, we leverage the
power of existing architectures pre-trained with millions of images to classify hundreds
of thousands of object categories2 . They have proven to be successful in very large
recognition tasks apart from the detection and classification tasks for which they were
designed [75].
Here, we employ the VGG-Face (VGG) [33] and the very deep Resnet101 [34]
and Densenet201 [35] architectures. VGG-Face is based on the VGG-Very-Deep-16
CNN sequential architecture, implemented using ∼1 million images from the Labeled
1 http://vision.ucla.edu/∼vedaldi/code/sift/assets/sift/index.html
2 ImageNet.

http://www.image-net.org

19

Faces in the Wild [76] and YouTube Faces [77] datasets. Since VGG-Face is trained
for classifying faces, we believe that it can provide effective recognition with the periocular region as well, given that this region appears in the training images. Introduced
later, the ResNet networks [34] presented the concept of residual connections to ease
the training of CNNs. By reducing the number of training parameters, they can be
substantially deeper. The key idea of residual connections is to make available the input of a lower layer to a higher layer, bypassing intermediate ones. In this work, we
employ ResNet101, having a depth of 347 layers (including 101 convolutional layers).
In DenseNet networks [35], this concept is taken even further, since the feature-maps
of all preceding layers of a Dense block are used as inputs of a given layer, and its own
feature-maps are used as inputs into all subsequent layers. This encourages feature
reuse throughout the network. In this paper, we employ Densenet201, having a depth
of 709 layers (including 201 convolutional layers).
In using these networks, periocular images are fed into the feature extraction pipeline
of each pre-trained CNN [73, 74]. However, instead of using the vector from the last
layer, we employ as feature descriptor the vector from the intermediate layer identified
as the one providing the best performance. These will be found in the respective experimental sections. This approach allows the use of powerful architectures pre-trained
with a large number of images in a related domain, eliminating the need of designing or
re-training a new network for a specific task, which may be infeasible in case of lack of
large-scale databases in the target domain (as in the case of periocular recognition with
images from different sensors). The extracted CNN vectors can be simply compared
with distance measures. In our case, we employ the χ2 distance, which has proven to
provide better results than other measures such as the cosine or Euclidean distances
[74].

4. Score Fusion Methods
A biometric verification comparator can be defined as a pattern recognition machine that, by comparing two (or more) samples of input signals, is designed to recognize two different classes. The two hypotheses or classes defined for each comparison

20

are: target hypothesis (θt : the compared biometric data comes from the same individual) and non-target hypothesis (θnt : the compared data comes from different individuals). As a result of the comparison, the biometric system outputs a real number
s known as score. The higher the score, the more it supports the target hypothesis,
and vice-versa. The acceptance or rejection of an individual is based on a decision
threshold τ , and this threshold depends on the priors and decision costs involved in the
decision taking process. However, if we do not know the distributions of target or nontarget scores from such comparator or any threshold, we will not be able to classify the
associated biometric samples in general.
Integration at the score level is the most common approach used in multibiometric
systems due to the ease in accessing and combining the scores s = (s1 , . . . , si , . . . , sN )
generated by N different comparators [18]. Unfortunately, each biometric comparator
outputs scores which are in a range that is specific of the comparator, so score normalization is needed to transform these scores into a common domain prior to the fusion
[22], e.g. si ∈ [0, 1] or si ∈ [−1, 1], ∀i ∈ {1, . . . , N }. But even if two comparators
output scores in the same range, the same output value (say si = sj = 0.5 for i 6= j)
might not favor the target or non-target hypotheses with the same strength. The same
can be said about the fusion of such scores. From this viewpoint, outputs are dependent on the comparator, and thus, the acceptance/rejection decision also depends on the
comparator.
These problems can be addressed with the concept of calibrated scores. During
calibration, the scores s = (s1 , . . . , si , . . . , sN ) are mapped to a log-likelihood-ratio


p(s|θt )
(LLR) as scal ≈ log p(s|θ
, where scal represents the calibrated score. Then, a
nt )
decision can be taken using the Bayes decision rule [40]:

For a given s


 decide θt : (p ( s| θt ) /p ( s| θnt )) > τB


(1)

decide θnt : (p ( s| θt ) /p ( s| θnt )) < τB

The parameter τB is known as the Bayes threshold, and its value depends on the
prior probabilities of the hypotheses p (θt ) and p (θnt ) and on the decision costs. This
form of output is comparator-independent, since this log-likelihood-ratio output can
theoretically be used to make optimal (Bayes) decisions for any given target prior and
21

any costs associated with making erroneous decisions [40]. Therefore, the calibration
process gives meaning to scal . In a Bayesian context, a calibrated score scal can be
interpreted as a degree of support to any of the hypotheses. If scal > 0, then the
support to θt is also higher, and vice-versa. Also, the meaning of a log-likelihood
ratio is the same across different biometric comparators, allowing to compare them
in the same probabilistic range. This calibration transformation then solves the two
previously commented problems. First, it maps scores from biometric comparators to
a common domain. Second, it allows the interpretation of biometric scores as a degree
of support.
A number of strategies can be used to train a calibration transformation [78]. Among
them, logistic regression has been successfully used for biometric applications [38, 39,
79, 80, 21]. With this method, the scores of multiple comparators are fused together,
primarily to improve the discriminating ability, in such a way as to encourage good calibration of the output scores. Given N biometric comparators which output the scores
sj = (s1j , s2j , ...sN j ) for an input trial j, a linear fusion of these scores is:

fj = a0 + a1 · s1j + a2 · s2j + ... + aN · sN j

(2)

When the weights {a0 , ..., aN } are trained via logistic regression, the fused score
fj is a well-calibrated log-likelihood-ratio [78, 39]. Let [sij ] be an N × NT matrix of
training scores built from N biometric comparators and NT target trials, and let [rij ] be
an N × NN T matrix of training scores built from the same N biometric comparators
with NN T non-target trials. We use a logistic regression objective [38, 39] that is
normalized with respect to the proportion of target and non-target trials (NT and NN T ,
respectively), and weighted with respect to a given prior probability P = P (target).
The objective is stated in terms of a cost C, which must be minimized:

C=

NT
NT
 1 − P NX

P X
log 1 + e−fj −logitP +
log 1 + e−gj −logitP
NT j=1
NN T j=1

22

(3)

where the fused target and non-target scores are respectively
N
P

fj = a0 +

ai sij

i=1

(4)
N
P

gj = a0 +

ai rij

i=1

and where logitP = log



P
1−P



.

It can be demonstrated that minimizing the objective C with respect to {a0 , ..., aN }
results in a good calibration of the fused scores [78, 39]. In practice, changing the value
of P has a small effect. The default of 0.5 is a good choice for a general application
and it will be used in this work. The optimization objective C is convex and therefore
has a unique global minimum.
Another advantage of this method is that when fusing scores from different comparators, the most reliable comparator will implicitly be given a dominant role in the
fusion (via the trained weights {a0 , ..., aN }). In other standard fusion methods, such
as the average of scores [22], all comparators are given the same weight in the fusion, regardless of its individual accuracy. It is also straightforward to show that if
cal
cal
M calibrated scores {scal
1 , s2 , . . . , sM } come from statistically independent sources
cal
cal
(such as multiple biometric comparators), its sum scal
1 + s2 + . . . + sM also yields

a log-likelihood ratio [40]. The latter allows to calibrate the scores si of each available
biometric comparator separately (by using N =1 in Equation 2), and simply sum the
calibrated scores scal
of each comparator in order to obtain a new calibrated score, as
i
shown in Figure 5. In this paper, the two possibilities are evaluated, i.e. calibrating the
scores of all comparators together vs. calibrating them separately and then summing
them up. In order to perform logistic regression calibration, the freely available Bosaris
toolkit for Matlab has been used3 . For further details of this fusion method, the reader
is referred to [21] and the references therein.
The probabilistic fusion method described above is compared in the present work
with three strategies. Since each biometric comparator usually outputs scores which are
3 https://sites.google.com/site/bosaristoolkit/

23

in a range that is specific of the system, the scores of each comparator are normalized
prior to the fusion using z-score normalization [22]. The three strategies are:
• Average. With this simple rule, the scores of the different comparators are simply averaged. Motivated by their simplicity, simple fusion rules have been used
in biometric authentication with very good results [81, 82]. They have the advantage of not needing training, sometimes surpassing other complex fusion approaches [83].
• SVM. Here, a Support Vector Machine (SVM) is trained to provide a binary
classification given a set of scores from different biometric comparators [84].
The SVM algorithm searches for an optimal hyperplane that separates the data
into two classes. SVM is a popular approach employed in multibiometrics [23],
which has shown to outperform other trained approaches [18]. In this work, we
evaluate Linear, RBF, and Polynomial (order 3) kernels. Instead of using the binary predicted class label, we use the signed distance to the decision boundary as
the output score of the fusion. This allows to present DET curves and associated
EER and FRR measures.
• Random Forest. Another method employed for the fusion of scores from multiple biometric comparators is the Random Forest (RF) algorithm [24]. An extension of the standard classification tree algorithm, the RF algorithm is an ensemble
method where the results of many decision trees are combined [85]. This helps
to reduce overfitting and to improve generalization capabilities. The trees in the
ensemble are grown by using bootstrap samples of the data. In this work, we
evaluate ensembles with 25, 150, and 600 decision trees. Instead of using the
binary predicted class label, we use the weighted average of the class posterior
probabilities over the trees that support the predicted class, so we can present
DET curves and associated measures.

24

(a) Cross-Eyed database (top row: visible images, bottom: near-infrared)

(b) VSSIRIS database (top row: Apple iPhone 5S, bottom: Nokia Lumia 1020; images taken from [26])

Figure 7: Sample periocular images.

5. Cross-Spectral (NIR-VIS) Periocular Recognition
5.1. Database and Protocol
In the cross-spectral recognition experiments of this section, we employ the Reading Cross-Spectral Iris/Periocular Dataset used as the benchmark dataset for the 1st
Cross-Spectral Iris/Periocular Competition (Cross-Eyed 2016) [25]. The dataset contains both visible (VIS) and near-infrared (NIR) images captured with a custom dual
spectrum imaging sensor which acquires images in both spectra synchronously. Periocular images are of size 800 × 900 (height×width) from 120 subjects, with 8 images
of both eyes captured in both spectra, totalling 3840 images. Images are captured at a
distance of 1.5 m, in an uncontrolled indoor environment, containing large variations in
25

ethnicity, eye color, and illumination reflections. Some examples are shown in Figure 7
(top). To avoid usage of iris information by periocular methods during the Cross-Eyed
competition, periocular images were distributed with a mask on the eye region, as discussed in [10]. A new edition of the competition was held in 2017. The 120 subjects
of the Cross-Eyed 2016 database were provided as training set, and an additional set
of 55 subjects were sequestered as test set in the 2017 edition, but the latter was never
released [86].

Cross-Eyed database
Comparison type

Training

Test

(30 subjects)

(90 subjects)

Same-

Genuine

30 × 2E × (7+6+...+1) = 1,680

90 × 2E × (7+6+...+1) = 5,040

Sensor

Impostor

30 × 29 × (4L + 4R) = 6,960

90 × 89 × (2L + 2R) = 32,040

Cross-

Genuine

30 × 2E × 8L × 8R = 3,840

90 × 2E × 8L × 8R =11,520

Spectral

Impostor

30 × 29 × (4L+4R) × 2S = 13,920

90 × 89 × (2L+2R) × 2S = 64,080

Table 2: Cross-Eyed database: Experimental protocol. E=Eyes, L=Left eye, R=Right eye, S=Sensors.

comparator

Cross-Eyed

VSSIRIS

data

SAFE

6×3×9=162

6×3×9=162

complex

GABOR

48×30=1440

56×30=1680

real

SIFT

circa 243200

circa 384000

real

LBP, HOG

48×8=384

56×8=448

real
integer

NTNU

9472

9472

VGG

100352

100352

real

Resnet101

50176

100352

real

Densenet201

6272

43904

real

Table 3: Size of the feature vector per comparator and per database.

Prior to the competition, a training set of images from 30 subjects was distributed.
The test set consisted of images from 80 subjects, sequestered by the organizers, and
distributed after the competition. Images from 10 additional subjects were also released
after the competition that were not present in the test set. Here, we will employ the

26

Cross-Eyed database

VSSIRIS database

Extraction

Comparison

Extraction

Time

Time

Time

Comparison
Time

SAFE

2.98 sec

0.2 ms

11.86 sec

<0.1 ms

GABOR

0.49 sec

0.3 ms

0.53 sec

0.3 ms

SIFT

0.94 sec

0.58 s

1.5 sec

1.1 s

LBP

0.16 sec

<0.1 ms

0.17 sec

<0.1 ms
<0.1 ms

HOG

0.01 sec

<0.1 ms

0.13 sec

NTNU

0.6 sec

0.7 ms

0.56 sec

0.7 ms

VGG

0.51 sec

1.65 ms

0.52 sec

1.43 ms

Resnet101

0.27 sec

0.35 ms

0.48 sec

0.65 ms

Densenet201

0.25 sec

<0.1 ms

0.39 sec

0.42 ms

Table 4: Feature computation times for each database.

same 30 subjects of the training set to tune our algorithms and the remaining 90 subjects
for testing purposes. All images have an annotation mask of the eye region. The
mass center of the mask is set as the reference point (center) of the eye. Images are
then rotated w.r.t. the axis that crosses the two mask corners, and resized via bicubic
interpolation to have the same corner-to-corner distance (set to 318 pixels, average
value of the training set). Then, images are aligned by extracting a region of 613 × 701
around the eye. This size is set empirically to ensure that all available images have
sufficient margin to the four sides of the eye center. Eyes in the Cross-Eyed database
are slightly displaced in the vertical direction, so the eye is not centered in the aligned
images but with a vertical offset of 56 pixels (see Figure 3, top). Images are further
processed by Contrast Limited Adaptive Histogram Equalization (CLAHE) [36], which
is the preprocessing choice with ocular images [87], and then sent to feature extraction.
We carry out verification experiments, with each eye considered a different user.
We compare images both from the same device (same-sensor) and from different devices (cross-spectral). Genuine trials are obtained by comparing each image of an
eye to the remaining images of the same eye. In same-sensor comparisons, to avoid
symmetric comparisons, the first image of an eye is compared to the second to eight
images; the second image is compared to the third to eight images, and so on, leading

27

to (7+6+...+1) genuine scores per eye. This procedure is repeated for the two eyes of all
subjects. This results in 30 subjects × 2 eyes × (7+6+...+1) and 90 × 2 × (7+6+...+1)
genuine scores with the training and test set, respectively. In cross-spectral comparisons, the eight images of an eye in one spectrum are compared against the eight images
in the other spectrum, leading to 8 × 8 genuine scores per eye. This results in 30 subjects × 2 eyes × 8 × 8 and 90 × 2 × 8 × 8 genuine scores with the training and test set,
respectively. Impostor trials are done by comparing the 1st image of an eye to the 2nd
image of the remaining eyes. In same-sensor comparisons, given a subject of the test
set, his/her 1st image of the both eyes is compared against the 2nd image of both eyes
from the remaining 89 subjects. This results in 89 × 4 test impostor scores per subject
and 90 × 89 × 4 impostor scores in total. In cross-spectral comparisons, the number of
impostor scores is doubled by comparing the 1st image in VIS against the 2nd image in
NIR, and the 1st image in NIR against the 2nd image in VIS. This results in 90 × 89 ×
4 × 2 test impostor scores in total. To increase the number of available training scores,
we carry out an additional comparison to the 3rd image of the remaining eyes only
with the training set, effectively duplicating the number of impostor scores per subject.
Since the training set contains 30 subjects, this results in 29 × 4 × 2 (same-sensor) and
29 × 4 × 2 × 2 (cross-spectral) training impostor scores per subject. By multiplying
these amounts by 30, we obtain the total amount of impostor scores with the training
set. The experimental protocol is summarized in Table 2.
The periocular comparators employed have some parameters which are set as follows. It should be highlighted that these parameters are computed in proportion to the
size of the image, without any other training. If the image size changed, they would
adapt dynamically, so that the comparators would always be capturing their features
in the same relative areas of the image. The only input needed is the position of the
eye corners, which were also used to normalize and crop the image to a constant size,
as described above. Regarding the SAFE comparator, the annular band of the first circular ring starts at a radius of R=79 pixels (determined empirically as 1/4 of the eye
corner-to-corner distance), and the band of the last ring ends at the bottom boundary of
the image. This results in a ROI of 501 × 501 pixels around the eye center (as shown in
Figure 3, third column). The grid employed with GABOR, LBP and HOG comparators
28

has 7×8=56 non-overlapping blocks. Based on the size of the input image, each block
is of 88×88 pixels. The 8 central blocks are not considered since they are equal for all
users due to the eye region mask, so features are extracted only from 48 blocks. The
GABOR comparator employs filter wavelengths spanning from 44 to 6 pixels, which
are set proportional to the block size as 88/2=44 to 88/16≈6. The VGG, Resnet101
and Densenet201 comparators employ an input image size of 224×224, so images are
resized to match these dimensions. Table 3 (second column) indicates the size of the
feature vector for a given periocular image with the different comparators employed.
Experiments have been done in a Dell Latitude E7240 laptop with an i7-4600 (2.1 GHz)
processor, 16 Gb DDR3 RAM, and a built-in Intel HD Graphics 4400 card. The OS is
Microsoft Windows 8.1 Professional, and the comparators are implemented in Matlab
x64, with the exception of SIFT that is implemented in C++ and invoked from Matlab via MEX files. The VGG-Face model is from Caffee, which has been imported to
Matlab with the importCaffeNetwork function. The Resnet101 and Densenet201
models are from the pre-trained models available in Matlab r2019a. In line with the
Cross-Eyed competition, we also provide the extraction and comparison time of each
method (Table 4, second and third columns).

Figure 8: Cross-Eyed database: Cross-spectral accuracy (VIS-NIR) of different CNN layers.

5.2. Results: Finding the Optimum Layer of the Convolutional Neural Networks
Normalized periocular images are fed into the feature extraction of each pre-trained
CNN. We investigate the representation capability of each layer by reporting the corresponding cross-spectral accuracy using features from each layer. The recognition
29

Figure 9: Cross-Eyed database, test set: Verification results of the individual comparators. Best seen in color.

accuracy of each network (EER and FRR @ FAR=0.01%) is given in Figure 8. It is
worth noting that the best performance is obtained in some intermediate layer for all
CNNs, in line with previous studies using ocular modalities [73, 74]. In selecting the
best layer, we prioritize the FRR @ FAR=0.01%, since this was the metric employed to
rank submissions to the Cross-Eyed competition, although we seek a balance with the
EER as well. We have also searched for layers which give optimum performance both
with the Cross-Eyed and the VSSIRIS databases simultaneously if possible (results
with the latter are given in Figure 11).
A good performance with VGG is obtained with layer 25, which is a max pooling
layer with 14 × 14 × 512 = 100352 elements. The layer 27 also provides good performance. This is a ReLu layer of the same size than layer 25, but since it has many
elements set to 0 due to the ReLu operation, we prefer to choose layer 25. VGG is a
serial network, with layers arranged one after the other. On the other hand, ResNet101
and Densenet201 are acyclic networks, in which layers have inputs from multiple lay30

ers and outputs to multiple layers. This more intricate architecture may thus explain the
oscillations observed between layers. With ResNet101, a good performance is obtained
with layer 165. This is a convolutional layer with 14 × 14 × 256 = 50176 elements,
and it will be the layer employed with Cross-Eyed. With VSSIRIS, better performance
is obtained with layer 323, which is not the case with Cross-Eyed. This is a ReLu layer
with 7 × 7 × 2018 = 100352 elements. We choose this layer with VSSIRIS instead,
since it provides better EER than other layers as well. Regarding DenseNet201, good
performance with Cross-Eyed (which minimizes both the EER and FRR) is obtained
with layer 223. This is a convolutional layer with only 14 × 14 × 32 = 6272 elements.
Other layers (e.g. 142 or 177) also give a good FRR, but the EER is not as good as with
layer 223. With VSSIRIS, better performance is given by layer 480 instead, which is
an average pooling layer with 7 × 7 × 896 = 43904 elements.

Equal Error Rate (EER)

FRR @ FAR=0.01%

Same sensor

Same sensor

comparator

NIR

VIS

cross-spectral

NIR

VIS

cross-spectral

SAFE

5.85%

5.67%

9.47% (+67%)

22.4%

24.23%

50.38% (+124.9%)

GABOR

5.48%

5.34%

7.94% (+48.7%)

26.25%

23.68%

43.3% (+82.9%)

SIFT

0.02%

0%

0.28% (-)

0.02%

0%

0.88% (-)

LBP

3.03%

3.27%

5.84% (+92.7%)

10.97%

12.86%

63.79% (+481.5%)

HOG

3.84%

4.19%

5.06% (+31.8%)

11.76%

14.93%

34.36% (+192.2%)

NTNU

2.83%

2.45%

4.22% (+72.2%)

3.93%

3.57%

13.8% (+286.6%)

VGG

2.36%

2.53%

3.42% (+44.9%)

8.48%

8.68%

13.59% (+60.3%)

Resnet101

1.52%

1.6%

2.61% (+71.7%)

5.51%

5.01%

12.51% (+149.7%)

Densenet201

1.37%

1.54%

2.09% (+52.6%)

5.69%

5.18%

10.09% (+94.8%)

Table 5: Cross-Eyed database, test set: Verification results of the individual comparators. The relative
variation of cross-spectral performance with respect to the best same-sensor performance is given in brackets.

5.3. Results: Individual Comparators
We now report the performance of all periocular comparators in Table 5. Besides
the EER, we also report the FRR at FAR=0.01%. The latter was the metric used to rank
submissions to the Cross-Eyed competition. We report two types of results: i) samesensor comparisons; and ii) cross-spectral comparisons. In Figure 9 we also give the
DET curves of the cross-spectral experiments.
31

From Table 5, it can be seen that all nine periocular comparators have an equivalent
performance with NIR and VIS data, even if they are based on different image features.
In previous studies, the periocular modality usually performed better with VIS data
[88, 89, 90], so it is generally accepted this modality is most suited to VIS imagery
[1]. On the contrary, some other works show opposite results [45]. However, in the
mentioned studies, the images employed are of smaller size, ranging from 100×160 to
640×480, while the images employed in this paper are of 613×701 pixels. Also, they
evaluate three different periocular comparators at most. In the present paper, the use of
bigger images may be the reason for a comparable performance between NIR and VIS
images.
Regarding cross-spectral experiments, we observe a significant worsening in performance w.r.t. same-sensor comparisons, although not all comparators are affected in
the same way. HOG, NTNU and specially LBP, are the most affected in high security
mode (i.e. low FAR). as can be appreciated in the right part of Table 5. The relative
FRR increase @ FAR=0.01% for these comparators is in the range of 200% to nearly
500%. This effect is not so prominent with the other comparators. It is also worth
noting the increase in error rates suffered by the SIFT comparator under cross-spectral
experiments. Even if its cross-spectral performance is the best among all comparators,
it is about one or two orders of magnitude worse than its same-sensor performance.
This is despite the use of a descriptor of bigger size (see Table 3). This indicates that
the image properties measured by this comparator do not remain consistent across NIR
and VIS spectra to the same extent than the other comparators. The global nature of
all the other comparators (which extract features from the whole ROI instead from a
discrete set of key-points) may help to alleviate the differences produced by different
imaging spectra.
Concerning the individual performance of each comparator, SIFT exhibits very low
error rates, but this comparator is computationally heavy both in processing times and
template size. In this paper, we use the SIFT detector with the same parametrization
employed in [72] for iris images of size 640×480. In the work [72], the iris region
represented ∼1/8 of the image only, leading to some hundreds of key-points per image.
However, images of the Cross-Eyed database are of 613×701 pixels, and the periocular
32

ROI occupies a considerably bigger area than the iris region, leading to an average of
∼1900 key-points per image (circa 243200 real numbers). To compare two images, it
is needed to compare each key-point of one image against all key-points of the other
image to find a pair match. This increases the computation time exponentially when the
number of key-points per image increases, which is one of the drawbacks of key-point
based comparators [1]. The other comparators employed have templates of fixed size,
thus comparison is made very efficiently using distance measures involving a number
of fixed calculations.
In general, there is an inverse proportion between the error rates and the template
size. The comparators with the best performance (SIFT, NTNU and the three CNNs)
are also the ones with the biggest feature vector (see Table 3). It is remarkable the performance of NTNU as well, surpassing the CNNs in some cases, but with a smaller feature vector. When it comes to cross-spectral comparisons, however, the CNNs provide
better performance. This is observed specially with the deeper networks (ResNet and
DenseNet), highlighting the capability of these powerful descriptors pretrained with
millions of images. In the DET curves of Figure 9, it can be better appreciated the superiority of the three CNNs for cross-spectral comparisons w.r.t. the other comparators
(apart from SIFT). It is also remarkable the behaviour of DenseNet, which provides the
second-best result of all comparators, but with a feature vector much smaller than the
other CNNs. Among the three CNNs used in this paper, DenseNet is the one providing
the best performance on the original task for which they were trained (ImageNet), so
it could be expected that such superiority is transferred to other tasks as well. It is also
worth noting the relatively good cross-spectral EER values of some light comparators
such as LBP or HOG. With a feature vector of only 384 real numbers and an EER of
5-6%, they would enable low-security applications where computational resources are
limited.
5.4. Results: Fusion of Periocular Comparators
We then carry out fusion experiments using all the available comparators, according to the fusion schemes presented in Section 4. We have tested all the possible fusion
combinations. Whenever training is needed (i.e. to compute calibration weights, z33

Figure 10: Cross-Eyed database, test set: Verification results for an increasing number of fused comparators.
Best seen in color.

normalization, SVM, or Random Forest models), the training set of the Cross-Eyed
database is used. In Figure 10, we show the best results obtained for an increasing
number M of combined comparators. Following the protocol of Cross-Eyed 2016, the
best combinations are chosen based on the lowest cross-spectral FRR @ FAR=0.01%.
Then, the corresponding EER of the chosen combinations is reported as well in Figure 10. We use the two mentioned calibration possibilities of the fusion method (Figure 5): i) the scores from all comparators are calibrated together (N = M in Equation 2); or ii) the score of each comparator is calibrated separately (N = 1) and the
resulting calibrated scores are summed. These cases are shown in Figure 10 as ‘LLR’
and ‘LLR (sum)’ respectively.
As it can be observed, a substantial performance improvement can be obtained
when combining several comparators. The best cross-spectral performance is obtained
with a combination of 2 to 3 comparators. The FRR remains approximately constant
until 5 comparators are combined, and then it deteriorates when including more. The
EER, nevertheless, deteriorates earlier. We also observe that the probabilistic fusion
method based on calibration (LLR) outperform all the others, demonstrating its superiority. This is more evident at low FAR, with a relative FRR reduction of ∼47% in
comparison to using one comparator only. It is also better if all scores are calibrated
together, rather than calibrating them individually and then summing then up (‘LLR’
vs. ‘LLR (sum)’). Regarding the other fusion methods, the SVM with a linear or polynomial kernel stands out in comparison to the others. The polynomial kernel shows

34

CROSS-EYED DATABASE: cross-spectral performance (VIS-NIR)

1

x
x
x

2

x

x

x
x
3

x
4

5

x

x

2.09

10.09

2.62

12.51

0.23

0.47

x

0.21

0.48

x

0.26

0.52

x

x

x x

0.21

0.49

0.29

0.5

x

x

0.21

0.47

x

x x

x

x
x x

x

10.09

x

2.62

12.51

x

0.25

0.6

x

0.25

0.62

x

0.33

0.66

x

0.25

0.57

x

x

0.28

0.67

x

x

0.28

0.68

0.34

x
x

x

x x

x x

0.2

0.5

x

x

x

x

x

x x

0.31

0.51

x

x

x

x

x x

x x

0.25

0.48

x

x x

x

x

x x

x

x
x

0.88

2.09

10.09

2.62

12.51

0.27

0.57

0.24

0.59

x

0.23

0.61

x x

0.24

0.56

x x

0.27

0.58

x x

x

0.27

0.6

0.66

x

x x x

0.27

0.58

x x

0.27

0.59

x x

0.72

x x

x x

0.25

0.6

0.36

0.79

x x

x x x

0.26

0.59

0.34

0.88

0.34

0.88

x x x

x x

0.22

0.63

x x

x x

0.22

0.64

x x

x x x x

0.28

0.65

x

x x x x

0.28

0.65

x x x x x x

0.27

0.66

x

x x

x x x x

0.28

0.65

1.26

x

x x x

x x x

0.24

0.67

0.68

1.32

x

x x x x x x

0.27

0.67

x x

0.74

1.49

x

x x x x x x x

0.26

0.68

x x x x x x x

0.64

1.51

x x x x x

0.26

0.81

x x x x x x x x

0.81

1.62

x x x x x x x x

0.31

0.84

x x x x x x x x x

0.84

1.96

x x x x x x x x x

0.31

0.85

0.32

0.59

x

x

0.25

0.64

x

x x

x

0.26

0.68

x

x x x x

x

0.43

1.02

x x

0.27

0.69

x

x x

x

x x

0.43

1.04

x x

0.25

0.7

x

x x x x

x

0.41

1.07

x x x x

0.26

0.8

x

x x x x

x x

0.51

1.11

x x

0.29

0.81

x

x x x x x

x

0.55

x x x x x x x

0.22

0.83

x x x x x x

x

x x x x x x x

0.27

0.92

x x x x x x

x x

0.31

0.93

x

x

0.29

0.94

x x x x x x x x x

0.3

0.94

x x x x x x
x x x x x x x

x

0.69

x x

x

x
x

0.3

x x

x x

x

0.28

x x

x

x

EER (%) FRR (%)
0.28

x x x
x x x x x

safe
gabor
sift
lbp
hog
ntnu
vgg
resnet
densenet

2.09

x

x

EER (%) FRR (%)
0.88

x

x

SVM LINEAR FUSION

0.28
x

0.49

x x x x x

9

0.88

0.2

x x x x

8

0.28

x

x x x x

7

EER (%) FRR (%)

x x

x x

x

6

x

AVERAGE FUSION

safe
gabor
sift
lbp
hog
ntnu
vgg
resnet
densenet

safe
gabor
sift
lbp
hog
ntnu
vgg
resnet
densenet

# comparators

LLR FUSION

x

x

x

x

x x x

Table 6: Cross-Eyed database, test set: Verification results for an increasing number of fused comparators.
The best combinations are chosen based on the lowest FRR @ FAR=0.01% of cross-spectral experiments.
The best result of each column is marked in bold.

equal or better performance in some cases, but such kernel is much slower to train. It
is also worth noting than the simple average rule (AVG) provides similar performance
than trained approaches like the SVM, although it deteriorates quickly with the combination of more than 3 comparators. On the other hand, the Random Forest approach
performs among the worst, regardless of the number of decision trees employed.
In Table 6, we show the comparators involved in the best fusion cases. For the sake
of space, we only provide results with a selection of fusion approaches, according to
the observations made above when discussing Figure 10: the LLR method (best case),
SVM linear (a good runner-up which is also faster to train than its polynomial coun-

35

terpart), and AVG or AVERAGE (a simple approach that does not need training). To
allow a more comprehensive analysis, we also provide not only the best cases but also
the second and third best combinations for a given number of comparators. It can be
seen that the best combinations for any given number of comparators always involve
the SIFT method. The excellent accuracy of the SIFT comparator is not jeopardized by
the fusion with other comparators that have a performance one or two orders of magnitude worse, but it is complemented to obtain even better cross-spectral error rates,
specially with trained approaches. A careful look at the combinations of Table 6 shows
that the CNN comparators are also chosen first for the fusion. Together with SIFT,
they are the comparators with the best individual performance, and they appear to be
very complementary too. However, it should not be taken as a general statement that
the best fusion combination always involves the best individual comparators. Different
fusion algorithms may lead to different results [91, 83]. For example, the best FRR
with the simple average rule involves the SAFE comparator. It is also worth noting that
other comparators with worse individual performance and not based on deep networks
(such as SAFE, LBP, or NTNU) are also selected in combinations that have a performance nearly as good as the best cases. At the same time, this shows the power of the
fusion approaches employed, specially of the calibration method, which are capable
of reducing error rates substantially by fusion of comparators with very heterogeneous
performance and different feature representations.
5.5. Results: Comparison with the Cross-Eyed 2016 competition
Table 7 shows the results of the submission of Halmstad University to the CrossEyed 2016 competition. We provide both the results reported by the organizers [25],
and our own computations on the training and test sets of the database using the executables submitted and the protocol described in Section 5.1. For the evaluation, only
the SAFE, GABOR, SIFT, LBP, and HOG comparators were available. We contributed
with three different fusion combinations, named HH1, HH2, and HH3, with the HH1
combination obtaining the first position in the competition. Two key differences in the
results reported in Table 7 in comparison with the present paper are that in our executables: i) the score of each comparator was calibrated separately, and the resulting
36

CROSS-EYED DATABASE: Cross-spectral performance (VIS-NIR)
Competition [25]

EER

HH3

x

x

x

4.5

16.77

4.86

24.59

6.02

11.42

3rd

HH2 x

x

x

x

3.02

12.63

4.51

19.75

5.24

9.14

2nd

HH1 x

x

x

x

0

0

0.28

0.83

0.29

0

1st

sift

hog

Test set

lbp

gabor

safe

approach

Training set

x

FRR

EER

FRR

EER

GF2

Rank

Table 7: Comparison with results of the Cross-Eyed 2016 Competition [25]. GF2 is the Generalized FRR
(GFRR) at a Generalized FAR (GFAR) of 0.01%. The GFRR and GFAR are generalizations of the FRR and
FAR to include Failure to Acquire (FTA) and Failure to Enroll (FTE) rates, according to ISO/IEC standards
[92]. The ranking in the evaluation of the submitted approaches is also given. For more information, refer to
[25].

calibrated scores were summed up; and ii) the LBP and HOG comparators employed
the Euclidean distance (which is the popular choice in the literature with these methods, instead of χ2 ). At the time of submission, the test set had not been released, so our
decisions could only be based on the results on the training set. We observed that the
SIFT comparator already provided cross-spectral error rates of nearly 0% on the training set (not shown in Table 7). However, it was reasonable to expect a higher error with
a bigger dataset, as demonstrated later when the test set was released. Therefore, we
contributed to the competition with a fusion of the five comparators available (called
HH1) to be able to better cope with the generalization issue that is expected when
performance is measured in a bigger set of images. Indeed, in Table 7 it can be seen
that performance on the test set is systematically worse than on the training set. Since
the combination of the five available comparators is computationally heavy in template
size (due to the SIFT comparator), we also contributed by removing SIFT (combination
HH2), and by further removing SAFE (combination HH3), which has a feature extraction time considerably higher than the rest of the comparators in our implementation
(see Table 4). Thus, our motivation behind HH2 and HH3 was to reduce template size
and feature extraction time. Some differences are observable between our results with
the test set and the results reported by the competition [25]. We attribute this to two

37

factors: i) the additional 10 subjects included in the test set released, which were not
used during the competition, and ii) the employment of a different test protocol, since
it is not specified by the organizers the exact images used for impostor trials during the
competition. Therefore, the experimental framework used in this paper is not exactly
the same employed in the Cross-Eyed competition.

6. Cross-Sensor (VIS-VIS) Smartphone Periocular Recognition
6.1. Database and Protocol
In the cross-sensor experiments of this section, we use the Visible Spectrum Smartphone Iris (VSSIRIS) database [26], which has images from 28 subjects (56 eyes)
captured using the rear camera of two smartphones (Apple iPhone 5S, of 3264×2448
pixels, and Nokia Lumia 1020, of 3072×1728 pixels). They have been obtained in unconstrained conditions under mixed illumination (natural sunlight and artificial room
light). Each eye has 5 samples per smartphone, thus 5×56=280 images per device
(560 in total). The acquisition is made without flash, in a single session and with semicooperative subjects. Figure 7 (bottom) shows some examples.
All images of VSSIRIS are annotated manually, so radius and center of the pupil
and sclera circles are available. Images are resized via bicubic interpolation to have
the same sclera radius (set to Rs =145, the average radius of the whole database). We
use the sclera for normalization since it is not affected by dilation. Then, images are
aligned by extracting a square region of 6Rs ×6Rs (871×871) around the sclera center.
This size is set empirically to ensure that all available images have sufficient margin to
the four sides of the sclera center. Here, there is sufficient availability to the four sides
of the eye, so the normalized images have the eye centered in the image, as can be
seen in Figure 3 (bottom). Images are further processed by Contrast-Limited Adaptive
Histogram Equalization (CLAHE) [36] to compensate variability in local illumination.
We carry out verification experiments, with each eye considered a different user.
We compare images both from the same device (same-sensor) and from different devices (cross-sensor). Genuine trials are obtained by comparing each image of an eye
to the remaining images of the same eye, avoiding symmetric comparisons. Impostor

38

VSSIRIS database
Protocol
(28 subjects)

Same-Sensor

Cross-Sensor

Genuine

56 × (4+3+2+1) = 560

56 × 5 × 5 = 1,400

Impostor

56 × 55 = 3,080

56× 55 = 3,080

Table 8: VSSIRIS database: Experimental protocol.

trials are done by comparing the 1st image of an eye to the 2nd image of the remaining eyes. The experimental protocol is summarized in Table 8. The smaller size of
VSSIRIS in comparison with the Cross-Eyed database results in the availability of less
scores. Therefore, whenever a parameter needs training, 2-fold cross-validation [93]
is used, dividing the available number of users in two partitions. Otherwise, we report
results employing the entire VSSIRIS database.
Parameters of the periocular comparators are as follows. As with the Cross-Eyed
database, they are designed to adapt dynamically to the size of the image, being the
sclera boundary the only necessary input. Regarding the SAFE comparator, the annular band of the first circular ring starts at the sclera circle (R=145 pixels), and the band
of the last ring ends at the boundary of the image, resulting in a ROI of 871×871 pixels
around the eye center. The availability of sufficient margin around the four sides of the
eye makes possible to have a bigger ROI with VSSIRIS, as can be shown in Figure 3,
third column. This availability also allows one extra row in the grid employed with
GABOR, LBP and HOG comparators, having 8×8=64 non-overlapping blocks. Given
the size of the input image, each block is of 109×109 pixels. For consistency with
Cross-Eyed, the eight blocks of the image center are not considered, effectively resulting in 56 blocks (some more than Cross-Eyed, which has 48 blocks of size 88×88
each). The GABOR comparator employs filter wavelengths spanning from 55 to 7
pixels, which are set proportional to the block size as 109/2≈55 to 109/16≈7. Regarding VGG, Resnet101 and Densenet201, images are resized to 224×224, which are
the input dimensions of these CNNs. Table 3 (third column) indicates the size of the
39

feature vector for a given periocular image with the different comparators employed.
Experiments have been done in the same machine and with the same algorithm implementations than Cross-Eyed (Section 5.1). The feature extraction and comparison
times are given in Table 4 (right).

Figure 11: VSSIRIS database: Cross-sensor accuracy (VIS-VIS) of different CNN layers.

6.2. Results: Finding the Optimum Layer of the Convolutional Neural Networks
We first identify the optimum layer of each CNN. The cross-sensor accuracy of
each network is given in Figure 11 for each cross-validation fold. When selecting
the best layer, we have tried to find the one that gives optimum performance both with
the Cross-Eyed and the VSSIRIS databases simultaneously. However, it has not always
been possible. According to the discussion in Section 5.2, the best layers with VSSIRIS
are layer 25 (VGG), layer 323 (ResNet101), and layer 480 (DenseNet201). It can be
seen as well that the optimum layers of VSSIRIS are the same for the two folds. With
Cross-Eyed, on the other hand, the best layers were not so deep: 165 (ResNet101), and
layer 223 (DenseNet201).

40

Figure 12: VSSIRIS database: Verification results of the individual comparators. Best seen in color.

6.3. Results: Individual Comparators
The performance of individual comparators is then reported in Table 9. Similarly
as Section 5, we adopt as measures of accuracy the EER and the FRR at FAR=0.01%.
In Figure 12, we give the DET curves of the cross-sensor experiments.
By comparing Table 5 and Table 9, it can be observed that same-sensor experiments
with the VSSIRIS database usually exhibit lower error rates for any given comparator.
Possible explanations might be that the ROI of VSSIRIS images is bigger (871×871 vs.
613×701), or that the VSSIRIS database has fewer users (28 vs. 90 subjects). On the
opposite side, cross-sensor error rates with VSSIRIS are significantly worse for some

41

Equal Error Rate (EER)
Same sensor

FRR @ FAR=0.01%
Same sensor

comparator

iPhone

Nokia

cross-sensor

iPhone

Nokia

cross-sensor

SAFE

1.6%

2.6%

10.2% (+537.5%)

4.6%

11.1%

50.9% (+1006.5%)

GABOR

2.1%

1.5%

7.3% (+386.7%)

4.3%

8.9%

39.1% (+809.3%)

SIFT

0%

0.1%

1.6% (-)

0%

0.7%

12.7% (-)

LBP

4.8%

4.9%

14.1% (+193.8%)

6.8%

16.8%

71.2% (+947.1%)

HOG

3.9%

4.5%

11% (+182.1%)

5.2%

17.3%

70.7% (+1259.6%)

NTNU

0.7%

0.7%

4.1% (+480%)

0.9%

1.8%

23.1% (+2500%)

VGG

0.9%

0.7%

4.4% (+528.6%)

1.6%

1.3%

20.8% (+1500%)

Resnet101

0.5%

0%

2.3% (-)

0.7%

0.4%

10.3% (+2475%)

Densenet201

0.5%

0%

2.4% (-)

0.7%

0.2%

6.2% (+3000%)

Table 9: VSSIRIS database: Verification results of the individual comparators. The relative variation of
cross-sensor performance with respect to the best same-sensor performance is given in brackets.

comparators (e.g. SIFT, HOG, NTNU, or VGG). Lighter comparators such as LBP
or HOG are not capable of providing good cross-sensor performance in low-security
applications either (EER of 11% or higher). The difference is specially relevant with
the SIFT comparator, where cross-sensor error rates on Cross-Eyed (Table 5) were
0.28% (EER) and 0.88% (FRR), but here they increase one order of magnitude, up to
1.6% (EER) and 12.7% (FRR). This is despite the higher number of SIFT key-points
per image with VSSIRIS due to higher image size (∼3000 vs. ∼1900 on average).
It is thus interesting that the comparators employed in this paper are more robust to
the variability between images in different spectra (NIR and VIS) than the variability
between images in the same (VIS) spectrum captured with two different smartphones.
It should be noted though that images in Cross-Eyed are obtained with a dual spectrum
sensor, which captures NIR and VIS images synchronously. Thus, in practice, there
is no scale, 3D rotation or time-lapse difference between corresponding NIR and VIS
samples. Only a spatial offset between the two exist in the plane perpendicular to the
optical axes of the cameras due to the sensors not being perfectly calibrated (which can
be noticed in Figure 7), so images are expected to be very well aligned after cropping.
This synchronicity and absence of time span could be one of the reasons of the better
cross-spectral performance obtained with the Cross-Eyed database.

42

Another observation is that same-sensor performance with VSSIRIS is sometimes
very different depending on the smartphone employed, even if they involve the same
subjects and images are resized to the same size. Contrarily, same-sensor performance
with Cross-Eyed tends to be similar regardless of the spectrum employed (Table 5),
which might be explained as well by the synchronicity in the acquisition mentioned
above. Previous works have suggested that discrepancy in colors between VIS sensors
can lead to variability in performance, which is further amplified when images from
such sensors are compared among them. Although we apply local adaptive contrast
equalization, our results suggest that other device-dependent color correction might be
of help [42]. Another difference observed here is that the best individual comparator
(in terms of FRR) is not SIFT, but DenseNet201, followed by ResNet101. This is
despite the higher number of SIFT key-points per image with VSSIRIS mentioned
above. Nevertheless, the correlation between bigger template size and smaller error
rates remains, since the comparators with the best performance (SIFT, NTNU and the
three CNNs) are also the ones with the biggest feature vector. The superiority of these
comparators can also be observed in the DET curves of Figure 12.

Figure 13: VSSIRIS database, test set: Verification results for an increasing number of fused comparators.
Best seen in color.

6.4. Results: Fusion of Periocular Comparators
We now carry out fusion experiments using all the available comparators. Whenever a fusion method needs training, 2-fold cross-validation [93] was used, dividing the
available number of users in two partitions. We have also tested here all the possible

43

fusion combinations, with the best combinations chosen based on the lowest crosssensor FRR @ FAR=0.01%. The best results obtained for an increasing number M
of combined comparators is given in Figure 13 (average values of the two folds). The
comparators involved in the best fusion cases are also given in Table 10 (as in Section 5.4, the table only shows the results of a selection of fusion approaches).
VSSIRIS DATABASE: cross-sensor performance (VIS-VIS)

x

2.4

6.2

2.3

10.3

1.6

12.7

x

0.8

2.4

x

0.9

2.4

x

1

2.8

x

x

0.9

0.6

x

x

1.4

0.6

x x

x

0.3

0.7

x

x x

x

0.3

0.3

x

x

x

x

x

x

x
x
2

x

x

x

x

x
3

x

x

x
x

4

x

x x

6

7

2.3

10.3

1.6

12.7

x

0.5

0.9

x

0.6

1.6

x

0.9

2.8

x

x

0.6

0.8

x

x

0.5

0.9

x

x x

0.5

1.1

x

0.5

1

x x x

0.6

0.5

x

x

0.5

0.5

x

x

x x

0.3

0.4

x

x

x x

x

x

x x x

0.5

0.4

x

x

x

x x x x

1.4

0.6

x

x x

x

x

x x x x

1

0.5

x

x

x

x

x x x

0.3

0.5

x

x

x

x x

x x x

0.3

0.5

x

x x

x

0.3

0.6

x

1

x x x
x x x x x

x

x
x
x
x

x
x

x

x
x
x

EER (%) FRR (%)
2.4

6.2

2.3

10.3

1.6

12.7

0.8

1.9

0.7

1.9

0.9

2.7

x

x

0.6

0.9

x x

x

0.6

1

x x

x

1

x

0.4

0.7

x

0.6

0.8

x

0.2

0.8

x

0.4

0.6

x x

0.3

0.7

x

0.7

1

x

x

x x

0.5

1.1

x

x x

x x

0.5

1.2

x x x

x x x

0.9

2.3

x

x x

x x

1.1

2.4

x

x

x x

x

0.6

0.8

x x x x

0.9

2.6

x

x

x x

x x

0.6

0.7

x x

0.5

0.8

x

0.6

0.8

x x x x

0.5

0.9

x x x

0.6

0.9

x

x

0.5

x

x x

x x

1

2.7

x x x

x

x

x x

1

3

x x x

x

x x x

x x

x x

1.4

3.5

x x x

0.6

x

x

x x x x x

1.4

3.6

x x x

x

0.3

0.7

x

x x x x

x x

1.2

3.8

x x x

x x

x x

0.6

0.9

0.6

0.7

x x x

x x x x x

1.7

4.1

x x x

x x x x x

0.5

0.9

x x x x x x x x

1.1

0.8

x x x x x x

x x

1.8

4.2

x x x x x

0.2

1.8

x x x x x x

x x

0.9

0.9

x

x x x x x x x

1.6

4.5

x x x x x x x

x

0.3

2.4

x x x x x x x x x

1.2

1.2

x x x x x x x x x

1.9

4.9

x x x x x x x x x

0.3

3.6

x x x x x

x x x x

9

6.2

x

x

EER (%) FRR (%)
2.4

x

x x x x x x

8

x

x

x x x
5

EER (%) FRR (%)

SVM LINEAR FUSION
safe
gabor
sift
lbp
hog
ntnu
vgg
resnet
densenet

safe
gabor
sift
lbp
hog
ntnu
vgg
resnet
densenet

1

AVERAGE FUSION
safe
gabor
sift
lbp
hog
ntnu
vgg
resnet
densenet

# comparator

LLR FUSION

x x x

x

x

x x x

Table 10: VSSIRIS database: Verification results for an increasing number of fused comparators. The best
combinations are chosen based on the lowest FRR @ FAR=0.01% of cross-sensor experiments. The best
result of each column is marked in bold.

Similarly as Cross-Eyed, cross-sensor performance is also improved significantly
here by fusion. The relative EER and FRR improvement of the best fusion case is even
bigger, being of 87.5% and of 95.2% respectively. This is high in comparison with the
reductions observed with Cross-Eyed, which were in the order of 30-40%. It is also

44

remarkable that similar or even better absolute performance values are obtained with
VSSIRIS. This is despite the worse performance observed in the individual comparators, as discussed in the previous section. However, it comes at the price of needing
more comparators to achieve maximum performance. Even if the biggest performance
improvement also occurs after the fusion of two or three comparators, the smallest error
is obtained with the fusion of four comparators. In contraposition, Cross-Eyed needed
only two or three (see Figure 10).
The fusion methods evaluated also rank in the same order here (see Figure 13).
The probabilistic fusion method based on calibration (LLR) outperforms all the others,
followed by SVM linear and polynomial. The simple average rule also matches the
performance of other trained approaches in some points, but it deteriorates quickly
as more comparators are combined. Lastly, the Random Forest approach performs
the worst in general. In addition, the SIFT comparator is also decisive to achieve
lower error rates, as it is always selected in any combination (Table 10). The CNN
comparators are also selected first, but to achieve the best performance, the role of
other comparators are decisive with this database. The best FRR, for example, is given
by the combination of SAFE, SIFT, LBP and DenseNet201. The same can be said with
other fusion methods. The best FRR with the average fusion involves SIFT, NTNU,
and DenseNet201, while the best FRR with the linear SVM engages SAFE, GABOR,
SIFT, HOG and DenseNet201.

7. Conclusion
Periocular biometrics has rapidly evolved to competing with face or iris recognition
[1, 2]. The periocular region has shown to be as discriminative as the full face, with the
advantage that it is more tolerant to variability in expression, blur, downsampling [94],
or occlusions [95, 10]. Under difficult conditions, such as people walking by acquisition portals, [96, 97, 98], distant acquisition, [99, 100], smartphones, [42], webcams,
or digital cameras, [31, 88], the periocular modality is also shown to be clearly superior to the iris modality, mostly due to the small size of the iris or the use of visible
illumination. The COVID-19 pandemic has also imposed the necessity of developing

45

technologies capable of dealing with faces occluded by protective face masks, often
with just the periocular area visible [5, 6, 7].
As biometric technologies are extensively deployed, it will be common to compare
data captured with different sensors, or from uncontrolled non-homogeneous environments. Unfortunately, the comparison of heterogeneous biometric data for recognition
purposes is known to decrease performance significantly [9]. Hence, as new practical applications evolve, new challenges arise, as well as the need for developing new
algorithms to address them. In this context, we address in this paper the problem of
biometric sensor interoperability, with recognition by periocular images as test-bed.
Inspired by our submission to the 1st Cross-Spectral Iris/Periocular Competition
(Cross-Eyed) [25], we propose a multialgorithm fusion strategy at the score level which
combines up to nine different periocular comparators. The aim of this competition
was to evaluate periocular recognition algorithms when images from visible and nearinfrared spectra are compared. We follow a probabilistic score fusion approach based
on linear logistic regression [78, 39]. With this method, scores from multiple comparators are fused together not only to improve the discriminating ability but also to produce
log-likelihood ratios as output scores. This way, output scores are always in a comparable probabilistic domain, since log-likelihood ratios can be interpreted as a degree of
support to the target or non-target hypotheses. This allows the use of Bayes thresholds for optimal decision-making, avoiding the need to compute comparator-specific
thresholds. This is essential in operational conditions since the threshold is critical to
determine the accuracy of the authentication process in many applications. In the experiments of this paper, this method is shown to surpass other fusion approaches such
as the simple arithmetic average of normalized scores [22], or trained algorithms such
as Support Vector Machines [23], or Random Forest [24]. This employed fusion approach has been applied previously to cross-sensor comparison of face or fingerprint
modalities [21] as well, also providing excellent results in other competition benchmarks involving these modalities [41]. We employ in this paper three different comparators based on the most widely used features in periocular research [10], as well as
three in-house comparators that we proposed recently [30, 31, 32], and three comparators based on deep Convolutional Neural Networks [33, 34, 35]. The proposed fusion
46

method, with a subset of the periocular comparators employed here, was used in our
submission to the mentioned Cross-Eyed evaluation, obtained the first position in the
ranking of participants. This paper is complemented with cross-sensor periocular experiments using images from the same spectrum as well. For this purpose, we use the
Visible Spectrum Smartphone Iris database (VSSIRIS) [26], which contains images in
the visible range from two different smartphones.
We first analyze the individual comparators employed not only from the point of
view of its cross-sensor performance (Figures 9 and 12), but also taking into account
its template size and computation times (Tables 3 and 4). We observe that the comparator having the biggest template size and computation time is usually the most accurate
in terms of individual performance, also contributing decisively to the fusion. In the
experiments reported in this paper, significant improvements in performance are obtained with the proposed fusion approach, leading to an EER of 0.2% in visible-to-nearinfrared comparisons (Figure 10), and 0.3% in visible-to-visible comparison of smartphone images (Figure 13). The FRR in high-security environments (at FAR=0.01%) is
also very good, being of 0.47% and 0.3% respectively.
Interestingly, the best performance is not obtained necessarily by the combination
of all available comparators. Instead, the best results are obtained by fusion of just
two to four comparators. A fundamental problem in classifier combination is to determine which systems to retain in order to attain the best results [101]. The systems
retained are not necessarily the best individual ones, specially if they are not sufficiently complementary (for example, if they employ similar features) [83]. When the
comparators are properly chosen (in our case, found by exhaustive search), the performance increases quickly with the addition of a small number of them. Then, it
tends to stabilize, until the addition of new ones actually decreases the performance.
The need to retain the best features only, and the mentioned performance ‘peaking’
effect, is well documented [101], and it can be attributed to the correlation between
classifiers or to the effect of a limited sample size. Such phenomenon have been also
observed in other related studies in biometrics [102, 83, 103, 104, 88]. It is also worth
noting that the comparators producing the best fusion performance (Tables 6 and 10)
have an individual performance that differs in one or two orders of magnitude in some
47

cases. In the probabilistic approach employed, each comparator is implicitly weighted
by its individual accuracy, so the most reliable ones will have a dominant role [105]. It
is, therefore, a very efficient method to cope with comparators having heterogeneous
performance. On the contrary, in conventional score-level fusion approaches (like the
average of scores), each comparator is given the same weight regardless of its accuracy,
a common drawback that makes the worst comparators to produce misleading results
more frequently [22]. Another relevant observation is that cross-sensor error rates of
the individual comparators are higher with the database captured in the same spectrum
(VSSIRIS) than the database which contains images in different spectra (Cross-Eyed).
As a result, there is a need to fuse more comparators with VSSIRIS to achieve maximum performance. This is an interesting phenomenon since one would expect that
the comparison of images captured with visible cameras would produce better results
than the comparison of near-infrared and visible images. Some authors point out that
the discrepancy in colors between sensors in the visible range can be very important,
leading to a significant decrease in performance when images from these sensors are
compared without applying appropriate device-dependent color corrections [42]. Since
NIR images do not contain color information, this effect may not appear in NIR-VIS
comparisons.
In the present work, we use the eye corners or the sclera boundary as references to
extract the periocular region of interest (ROI). While we have employed ground-truth
information, an operational system would demand to locate these parts, so inaccuracies in their location would affect subsequent processing steps. In order to mitigate the
effects of incorrect detection on the periocular matching performance of the different
comparators and obtain a measure of their capabilities in ideal conditions [10], we have
not implemented any detector of the necessary references. Even if errors in the detection will influence the overall performance of the recognition chain, feature extraction
methods are not necessarily affected in the same way. This is seen for example in [106]
with the iris modality, which will serve as inspiration for a similar systematic study with
periocular images. The amount of periocular area around the eye necessary to provide
good accuracy is another subject of study, with studies showing differences depending
on the spectrum [107]. In VSSIRIS, the available images (captured with smartphones)
48

contain a bigger periocular portion than images from the Cross-Eyed database (Figure 7). However, it is not sufficient to provide better cross-sensor accuracy. Therefore,
an interesting source of future research work will be to test the resilience against a
variable amount of periocular area, including occlusions [10].
Another observation is that the proposed fusion method needs to be trained separately for each domain (NIR-VIS or VIS-VIS). This is not exclusive of this method,
but an issue that is common to score-level fusion methods in general. Since the scores
given by different systems do not necessarily lie in the same range, they are made comparable by mapping them to a common interval using score normalization techniques
[18]. Even the score distributions of a given algorithm do not necessarily lie in the same
range if the operational conditions are different, such as operating in NIR-VIS or VISVIS domains. Just changing a sensor by more recent one from the same manufacturer
may have the same effect [37], and the shape of the distributions are not necessarily
equal either. One obvious effect of the difference between score distributions in different domains is that the accuracy of the comparators is different, not only in absolute
numbers but also in the relative differences among them (Table 5 vs. Table 9). For
example, the best comparator in Table 5 is SIFT, and it is one order of magnitude better
than the others. On the other hand, in Table 9, the EER of SIFT is only a little ahead
of Resnet101 or Densenet201, and the FRR is even worse. Another observable effect
of this phenomenon is that the slope of the DET curves is not the same either (Figure 9 vs. Figure 12). For these reasons, the normalization and the fusion algorithms
will usually need different training for each context. The calibration method employed
implicitly finds the weight to be given to each system, so if their absolute or relative
performance changes, the weights need to change accordingly. The same can be said
about the other fusion algorithms evaluated. The number of systems that are needed to
achieve maximum performance will not necessarily be the same either (Figure 10 vs.
Figure 13), nor the individual systems involved in the fusion (Table 6 vs. Table 10).
These observations are also backed-up by a number of previous studies with different
biometrics modalities [88, 108, 109, 83]. As a future work in this direction, we are
looking at the robustness of the different comparators to cross-domain training, i.e.
training the calibration in one domain and testing in the other. We speculate that some
49

comparators may be more robust than others, so using only those for calibration would
allow transferring the training for one domain to the other, without needing to re-train
in the target domain. The use of several databases in one domain is also another way
to test the generalization of the suggested approach by cross-database training [110].
As future work, we are also exploring to exploit deep learning frameworks to learn the
variability between images in different spectra, or captured with different sensors. One
plausible approach is the use of Generative Adversarial Networks [111] to map images
as if they were captured by the same sensor. This has the advantage that images can be
compared using standard feature extraction methods such as the ones employed in this
paper, which have been shown to work better if images are captured using the same
sensor.
In the context of smartphone recognition, where high-resolution images may be
available, fusion with the iris modality is another possibility to increase recognition accuracy [88]. However, it demands segmentation, which might be an issue if the image
quality is not sufficiently high [13]. This motivates pursuing the periocular modality,
as in the current study. We will also validate our methodology using databases not
only limited to two devices or spectra, e.g. [42, 49], and also including more extreme
variations in camera specifications and imaging conditions, such as low resolution, illumination or pose variability. For such low-quality imaging conditions, super-resolution
techniques may also be helpful [112] and will be investigated as well.

Acknowledgment
Part of this work was done while F. A.-F. was a visiting researcher at the Norwegian University of Science and Technology in Gjøvik (Norway), funded by EU COST
Action IC1106. Authors from HH thank the Swedish Research Council, the Swedish
Knowledge Foundation, and the Swedish Innovation Agency VINNOVA for funding
his research. Authors from UAM are funded by BIBECA project (MINECO/FEDER
RTI2018-101248-B-I00).

50

References
[1] F. Alonso-Fernandez, J. Bigun, A survey on periocular biometrics research, Pattern Recognition Letters 82 (2016) 92–105.
[2] I. Nigam, M. Vatsa, R. Singh, Ocular biometrics: A survey of modalities and
fusion approaches, Information Fusion 26 (2015) 1 – 35.
[3] H. Proenca, M. Nixon, M. Nappi, E. Ghaleb, G. Ozbulak, H. Gao, H. K. Ekenel,
K. Grm, V. Struc, H. Shi, X. Zhu, S. Liao, Z. Lei, S. Z. Li, W. Gutfeter, A. Pacut, J. Brogan, W. J. Scheirer, E. Gonzalez-Sosa, R. Vera-Rodriguez, J. Fierrez,
J. Ortega-Garcia, D. Riccio, L. D. Maio, Trends and controversies, IEEE Intelligent Systems 33 (3) (2018) 41–67.
[4] E. Gonzalez-Sosa, J. Fierrez, R. Vera-Rodriguez, F. Alonso-Fernandez, Facial
soft biometrics for recognition in the wild: Recent works, annotation and cots
evaluation, IEEE Trans. on Information Forensics and Security 13 (8) (2018)
2001–2014.
[5] Face id firms battle covid-19 as users shun fingerprinting, Biometric Technology
Today 2020 (4) (2020) 1 – 2.
[6] M. Ngan, P. Grother, K. Hanaoka, Ongoing frvt part 6a: Face recognition accuracy with face masks using pre-covid-19 algorithms, NISTIR 8311 http://www.nist.gov/itl/iad/ig/pft.cfm.
[7] B. Klare, Rank one’s next-generation periocular recognition algorithm (May
2020).
URL

https://blog.rankone.io/2020/05/06/

rank-ones-next-generation-periocular-recognition-algorithm/
[8] R. R. Jillela, A. Ross, V. N. Boddeti, B. V. K. V. Kumar, X. Hu, R. Plemmons,
P. Pauca, Handbook of Iris Recognition, Springer, 2013, Ch. Iris segmentation
for challenging periocular images, pp. 281–308.

51

[9] A. Jain, K. Nandakumar, A. Ross, 50 years of biometric research: Accomplishments, challenges, and opportunities, Pattern Recognition Letters 79 (2016) 80–
105.
[10] U. Park, R. R. Jillela, A. Ross, A. K. Jain, Periocular biometrics in the visible
spectrum, IEEE Trans Information Forensics and Security 6 (1) (2011) 96–106.
[11] A. Rattani, R. Derakhshani, Ocular biometrics in the visible spectrum: A survey,
Image and Vision Computing 59 (2017) 1 – 16.
[12] H. Proença, J. C. Neves, Deep-prwis: Periocular recognition without the iris
and sclera using deep learning frameworks, IEEE Transactions on Information
Forensics and Security 13 (4) (2018) 888–896.
[13] F. Alonso-Fernandez, J. Fierrez, J. Ortega-Garcia, Quality measures in biometric
systems, IEEE Security and Privacy 10 (6) (2012) 52–62.
[14] L. Xiao, Z. Sun, T. Tan, Fusion of iris and periocular biometrics for cross-sensor
identification, Proc. 7th Chinese Conference on Biometric Recognition, CCBR
(2012) 202–209.
[15] M. Moreno-Moreno, J. Fierrez, J. Ortega-Garcia, Biometrics beyond the visible spectrum: Imaging technologies and applications, in: J. Fierrez, J. OrtegaGarcia, A. Esposito, A. Drygajlo, M. Faundez-Zanuy (Eds.), Proceedings of
BioID-Multicomm, Vol. 5707 of LNCS, Springer, 2009, pp. 154–161.
[16] R. R. Jillela, A. Ross, Matching face against iris images using periocular information, Proc Intl Conf Image Processing, ICIP (2014) 4997–5001.
[17] P. Tome, R. Vera-Rodriguez, J. Fierrez, J. Ortega-Garcia, Facial soft biometric
features for forensic face recognition, Forensic Science International 257 (2015)
171–284.
[18] J. Fierrez, A. Morales, R. Vera-Rodriguez, D. Camacho, Multiple classifiers in
biometrics. part 1: Fundamentals and review, Information Fusion 44 (2018) 57–
64.
52

[19] A. Lumini, L. Nanni, Overview of the combination of biometric matchers, Information Fusion 33 (2017) 71 – 85.
[20] M. Singh, R. Singh, A. Ross, A comprehensive overview of biometric fusion,
Information Fusion 52 (2019) 187 – 205.
[21] F. Alonso-Fernandez, J. Fierrez, D. Ramos, J. Gonzalez-Rodriguez, Qualitybased conditional processing in multi-biometrics: Application to sensor interoperability, IEEE Trans. on Systems, Man and Cybernetics-Part A: Systems and
Humans 40 (6) (2010) 1168–1179.
[22] A. Jain, K. Nandakumar, A. Ross, Score normalization in multimodal biometric
systems, Pattern Recognition 38 (12) (2005) 2270–2285.
[23] B. Gutschoven, P. Verlinde, Multi-modal identity verification using support vector machines (svm), in: Proceedings of the Third International Conference on
Information Fusion, Vol. 2, 2000, pp. THB3/3–THB3/8 vol.2.
[24] Y. Ma, B. Cukic, H. Singh, A classification approach to multi-biometric score
fusion, in: T. Kanade, A. Jain, N. K. Ratha (Eds.), Audio- and Video-Based Biometric Person Authentication, Springer Berlin Heidelberg, Berlin, Heidelberg,
2005, pp. 484–493.
[25] A. F. Sequeira, L. Chen, J. Ferryman, F. Alonso-Fernandez, J. Bigun, K. B. Raja,
R. Raghavendra, C. Busch, P. Wild, Cross-eyed - cross-spectral iris/periocular
recognition database and competition, in: Proc Intl Conf of the Biometrics Special Interest Group, BIOSIG, 2016, pp. 1–5.
[26] K. B. Raja, R. Raghavendra, V. K. Vemuri, C. Busch, Smartphone based visible iris recognition using deep sparse filtering, Pattern Recognition Letters 57
(2015) 33–42.
[27] N. Dalal, B. Triggs, Histograms of oriented gradients for human detection,
in: Proc IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (CVPR), Vol. 1, 2005, pp. 886–893 vol. 1.

53

[28] T. Ojala, M. Pietikainen, T. Maenpaa, Multiresolution gray-scale and rotation
invariant texture classification with local binary patterns, IEEE Trans. on Pattern
Analysis and Machine Intelligence 24 (7) (2002) 971–987.
[29] D. Lowe, Distinctive image features from scale-invariant key points, Intl Journal
of Computer Vision 60 (2) (2004) 91–110.
[30] F. Alonso-Fernandez, A. Mikaelyan, J. Bigun, Compact multi-scale periocular
recognition using SAFE features, in: 2016 23rd International Conference on
Pattern Recognition (ICPR), 2016, pp. 1455–1460.
[31] F. Alonso-Fernandez, J. Bigun, Near-infrared and visible-light periocular recognition with gabor features using frequency-adaptive automatic eye detection,
IET Biometrics 4 (2) (2015) 74–89.
[32] K. B. Raja, R. Raghavendra, C. Busch, Scale-level score fusion of steered pyramid features for cross-spectral periocular verification, in: 2017 20th International Conference on Information Fusion (Fusion), 2017, pp. 1–7.
[33] O. M. Parkhi, A. Vedaldi, A. Zisserman, Deep face recognition, in: M. W. J.
Xianghua Xie, G. K. L. Tam (Eds.), Proceedings of the British Machine Vision
Conference (BMVC), BMVA Press, 2015, pp. 41.1–41.12.
[34] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
in: Proc IEEE Conference on Computer Vision and Pattern Recognition, CVPR,
2016, pp. 770–778.
[35] G. Huang, Z. Liu, L. v. d. Maaten, K. Q. Weinberger, Densely connected convolutional networks, in: 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017, pp. 2261–2269.
[36] K. Zuiderveld, Graphics Gems IV (1994) 474–485.
[37] F. Alonso-Fernandez, K. B. Raja, C. Busch, J. Bigun, Log-likelihood score level
fusion for improved cross-sensor smartphone periocular recognition, in: 2017
25th European Signal Processing Conference (EUSIPCO), 2017, pp. 271–275.
54

[38] S. Pigeon, P. Druyts, P. Verlinde, Applying logistic regression to the fusion of the
NIST’99 1-speaker submissions, Digital Signal Processing 10 (2000) 237–248.
[39] N. Brummer, L. Burget, J. Cernocky, O. Glembek, F. Grezl, M. Karafiat,
D. van Leeuwen, P. Matejka, P. Scwartz, A. Strasheim, Fusion of heterogeneous
speaker recognition systems in the STBU submission for the NIST Speaker
Recognition Evaluation 2006, IEEE Trans. on Audio, Speech and Signal Processing 15 (7) (2007) 2072–2084.
[40] R. Duda, P. Hart, D. Stork, Pattern Classification - 2nd Edition, 2004.
[41] N. Poh, T. Bourlai, J. Kittler, L. Allano, F. Alonso-Fernandez, O. Ambekar, J. Baker, B. Dorizzi, O. Fatukasi, J. Fierrez, H. Ganster, J. OrtegaGarcia, D. Maurer, A. Salah, T. Scheidat, C. Vielhauer, Benchmarking qualitydependent and cost-sensitive score-level multimodal biometric fusion algorithms, IEEE Trans. on Information Forensics and Security 4 (4) (2009) 849
– 866.
[42] G. Santos, E. Grancho, M. V. Bernardo, P. T. Fiadeiro, Fusing iris and periocular
information for cross-sensor recognition, Pattern Recognition Letters 57 (2015)
52–59.
[43] K. B. Raja, R. Raghavendra, C. Busch, Dynamic scale selected laplacian decomposed frequency response for cross-smartphone periocular verification in
visible spectrum, in: Proc 19th International Conference on Information Fusion
(FUSION), 2016, pp. 2206–2212.
[44] C. Kandaswamy, J. C. Monteiro, L. M. Silva, J. S. Cardoso, Multi-source deep
transfer learning for cross-sensor biometrics, Neural Computing and Applications 28 (9) (2017) 2461–2475.
[45] A. Sharma, S. Verma, M. Vatsa, R. Singh, On cross spectral periocular recognition, in: Proc IEEE International Conference on Image Processing (ICIP), 2014,
pp. 5007–5011.

55

[46] Z. Cao, N. A. Schmid, Fusion of operators for heterogeneous periocular recognition at varying ranges, Pattern Recognition Letters 82, Part 2 (2016) 170 –
180.
[47] N. P. Ramaiah, A. Kumar, On matching cross-spectral periocular images for
accurate biometrics identification, in: Proc IEEE 8th International Conference
on Biometrics Theory, Applications and Systems (BTAS), 2016, pp. 1–6.
[48] S. S. Behera, M. Gour, V. Kanhangad, N. Puhan, Periocular recognition in crossspectral scenario, in: Proc IEEE International Joint Conference on Biometrics,
IJCB, 2017, pp. 681–687.
[49] N. Vetrekar, K. B. Raja, R. Ramachandra, R. Gad, C. Busch, Multi-spectral
imaging for robust ocular biometrics, in: 2018 International Conference on Biometrics (ICB), 2018, pp. 195–201.
[50] K. Hernandez-Diaz, F. Alonso-Fernandez, J. Bigun, Cross spectral periocular
matching using resnet features, in: Proc International Conference on Biometrics
(ICB), 2019.
[51] M. D. Marsico, C. Galdi, M. Nappi, D. Riccio, FIRME: Face and iris recognition
for mobile engagement, Image and Vision Computing 32 (12) (2014) 1161 –
1172.
[52] Z. Guo, L. Zhang, D. Zhang, A completed modeling of local binary pattern operator for texture classification, IEEE Transactions on Image Processing 19 (6)
(2010) 1657–1663.
[53] A. Oliva, A. Torralba, Modeling the shape of the scene: A holistic representation
of the spatial envelope, Intl Journal of Computer Vision 42 (3) (2001) 145–175.
[54] A. Bosch, A. Zisserman, X. Munoz, Representing shape with a spatial pyramid
kernel, Proc. of the 6th ACM International Conference on Image and Video
Retrieval, CIVR (2007) 401–408.

56

[55] J. Chen, S. Shan, C. He, G. Zhao, M. Pietikainen, X. Chen, W. Gao, Wld: A robust local image descriptor, IEEE Transactions on Pattern Analysis and Machine
Intelligence 32 (9) (2010) 1705–1720.
[56] J. Bigun, Vision with Direction, Springer, 2006.
[57] C. Padole, H. Proenca, Periocular recognition: Analysis of performance degradation factors, Proc Intl Conf Biometrics, ICB (2012) 439–445.
[58] F. Smeraldi, J. Bigün, Retinal vision applied to facial features detection and face
authentication, Pattern Recognition Letters 23 (4) (2002) 463–475.
[59] M. Bulacu, L. Schomaker, Text-independent writer identification and verification using textural and allographic features, IEEE Trans. on Pattern Analysis
and Machine Inteligence 29 (4) (2007) 701–717.
[60] M. Unser, N. Chenouard, D. V. D. Ville, Steerable pyramids and tight wavelet
frames in l2(bbrd), IEEE Trans. Image Processing 20 (2011) 2705–2721.
[61] M. N. Do, M. Vetterli, Rotation invariant texture characterization and retrieval
using steerable wavelet-domain hidden markov models, IEEE Transactions on
Multimedia 4 (4) (2002) 517–527.
[62] G. Tzagkarakis, B. Beferull-Lozano, P. Tsakalides, Rotation-invariant texture retrieval with gaussianized steerable pyramids, IEEE Transactions on Image Processing 15 (9) (2006) 2702–2718.
[63] J. Portilla, E. P. Simoncelli, A parametric texture model based on joint statistics
of complex wavelet coefficients, International journal of computer vision 40 (1)
(2000) 49–70.
[64] S. Lyu, E. P. Simoncelli, Modeling multiscale subbands of photographic images
with fields of gaussian scale mixtures, IEEE Transactions on pattern analysis
and machine intelligence 31 (4) (2009) 693–706.

57

[65] S.-T. Li, Y. Li, Y.-N. Wang, Comparison and fusion of multiresolution features
for texture classification, in: Proc Intl Conf on Machine Learning and Cybernetics, Vol. 6, IEEE, 2004, pp. 3684–3688.
[66] M. El Aroussi, M. El Hassouni, S. Ghouzali, M. Rziza, D. Aboutajdine, Novel
face recognition approach based on steerable pyramid feature extraction, in:
Proc IEEE Intl Conf on Image Processing (ICIP), IEEE, 2009, pp. 4165–4168.
[67] C. Su, Y. Zhuang, L. Huang, F. Wu, Steerable pyramid-based face hallucination,
Pattern Recognition 38 (6) (2005) 813–824.
[68] E. P. Simoncelli, W. T. Freeman, The steerable pyramid: A flexible architecture
for multi-scale derivative computation, in: Proc Intl Conf on Image Processing,
Vol. 3, IEEE, 1995, pp. 444–447.
[69] W. T. Freeman, E. H. Adelson, et al., The design and use of steerable filters,
IEEE Transactions on Pattern analysis and machine intelligence 13 (9) (1991)
891–906.
[70] H. Greenspan, S. Belongie, R. Goodman, P. Perona, Rotation invariant texture recognition using a steerable pyramid, in: Proc IAPR Intl Conf on Pattern
Recognition, Vol. 2, IEEE, 1994, pp. 162–167.
[71] V. Ojansivu, J. Heikkilä, Blur insensitive texture classification using local phase
quantization, in: Image and signal processing, Springer, 2008, pp. 236–243.
[72] F. Alonso-Fernandez, P. Tome-Gonzalez, V. Ruiz-Albacete, J. Ortega-Garcia,
Iris recognition based on sift features, in: Proc First IEEE International Conference on Biometrics, Identity and Security (BIdS), 2009, pp. 1–8.
[73] K. Nguyen, C. Fookes, A. Ross, S. Sridharan, Iris recognition with off-the-shelf
cnn features: A deep learning perspective, IEEE Access 6 (2018) 18848–18855.
[74] K. Hernandez-Diaz, F. Alonso-Fernandez, J. Bigun, Periocular recognition using CNN features off-the-shelf, in: Proc International Conference of the Biometrics Special Interest Group (BIOSIG), 2018, pp. 1–5.
58

[75] A. S. Razavian, H. Azizpour, J. Sullivan, S. Carlsson, Cnn features off-the-shelf:
An astounding baseline for recognition, in: Proc IEEE Conference on Computer
Vision and Pattern Recognition Workshops, CVPRW, 2014, pp. 512–519.
[76] G. B. Huang, M. Ramesh, T. Berg, E. Learned-Miller, Labeled faces in the wild:
A database for studying face recognition in unconstrained environments, Tech.
Rep. 07-49, University of Massachusetts, Amherst (October 2007).
[77] L. Wolf, T. Hassner, I. Maoz, Face recognition in unconstrained videos with
matched background similarity, in: Proc. Conf on Computer Vision and Pattern
Recognition, CVPR, 2011, pp. 529–534.
[78] N. Brummer, J. du Preez, Application Independent Evaluation of Speaker Detection, Computer Speech and Language 20 (2006) 230–275.
[79] J. Gonzalez-Rodriguez, P. Rose, D. Ramos, D. Toledano, J. Ortega-Garcia, Emulating DNA: Rigorous quantification of evidential weight in transparent and
testable forensic speaker recognition, IEEE Trans. on Audio, Speech and Language Processing 15 (7) (2007) 2104–2115.
[80] L. Ferrer, M. Graciarena, A. Zymnis, E. Shriberg, System combination using
auxiliary information for speaker verification, in: Proc IEEE Intl Conf on Acoustics, Speech and Signal Processing, 2008, pp. 4853–4856.
[81] E. Bigun, J. Bigun, B. Duc, S. Fischer, Expert Conciliation for Multi Modal
Person Authentication Systems by Bayesian Statistics, Proc Intl Conf Audioand Video-Based Biometric Person Authentication, AVBPA Springer LNCS1206 (1997) 291–300.
[82] J. Kittler, M. Hatef, R. Duin, J. Matas, On Combining Classifiers, IEEE Trans.
on Pattern Analysis and Machine Intelligence 20 (3) (1998) 226–239.
[83] J. Fierrez-Aguilar, L. Nanni, J. Ortega-Garcia, R. Capelli, D. Maltoni, Combining Multiple Matchers for Fingerprint Verification: A Case Study in FVC2004,
Proc Int Conf Image Analysis and Processing, ICIAP Springer LNCS-3617
(2005) 1035–1042.
59

[84] V. N. Vapnik, The Nature of Statistical Learning Theory, Springer-Verlag New
York, Inc., New York, NY, USA, 1995.
[85] L. Breiman, Random forests, Machine Learning 45 (1) (2001) 5–32.
[86] A. F. Sequeira, L. Chen, J. Ferryman, P. Wild, F. Alonso-Fernandez, J. Bigun,
K. B. Raja, R. Raghavendra, C. Busch, T. de Freitas Pereira, S. Marcel, S. S. Behera, M. Gour, V. Kanhangad, Cross-eyed 2017: Cross-spectral iris/periocular
recognition competition, in: IEEE International Joint Conference on Biometrics
(IJCB), 2017, pp. 725–732.
[87] C. Rathgeb, A. Uhl, Secure iris recognition based on local intensity variations,
Proc ICIAR 6112 (2010) 266–275.
[88] F. Alonso-Fernandez, A. Mikaelyan, J. Bigun, Comparison and fusion of multiple iris and periocular matchers using near-infrared and visible images, in: Proc
3rd Intl Workshop on Biometrics and Forensics (IWBF), 2015, pp. 1–6.
[89] K. Hollingsworth, S. S. Darnell, P. E. Miller, D. L. Woodard, K. W. Bowyer, P. J.
Flynn, Human and machine performance on periocular biometrics under nearinfrared light and visible light, IEEE Trans Information Forensics and Security
7 (2) (2012) 588–601.
[90] D. L. Woodard, S. J. Pundlik, J. R. Lyle, P. E. Miller, Periocular region appearance cues for biometric identification, in: Proc IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops, 2010, pp.
162–169.
[91] J. Fierrez-Aguilar, Y. Chen, J. Ortega-Garcia, A. Jain, Incorporating image quality in multi-algorithm fingerprint verification, Proc Intl Conf Biometrics, ICB
Springer LNCS-3832 (2006) 213–220.
[92] International Organization for Standardization, ISO/IEC 19795-1:2006 Biometric Performance Testing and Reporting – Part 1: Principles and Framework,
JTC1/SC37/Biometrics - https://www.iso.org/standard/41447.html (2006) 0.

60

[93] A. Jain, R. Duin, J. Mao, Statistical pattern recognition: A review, IEEE Trans.
on Pattern Analysis and Machine Intelligence 22 (1) (2000) 4–37.
[94] P. E. Miller, J. R. Lyle, S. J. Pundlik, D. L. Woodard, Performance evaluation of
local appearance based periocular recognition, in: Proc Fourth IEEE Intl Conf
on Biometrics: Theory, Applications and Systems (BTAS), 2010, pp. 1–6.
[95] F. Juefei-Xu, M. Savvides, Subspace-based discrete transform encoded local binary patterns representations for robust periocular matching on NIST face recognition grand challenge, IEEE Trans Image Processing 23 (8) (2014) 3490–3505.
[96] D. L. Woodard, S. Pundlik, P. Miller, R. Jillela, A. Ross, On the fusion of periocular and iris biometrics in non-ideal imagery, in: Proc IAPR Intl Conf on
Pattern Recognition, 2010, pp. 201–204.
[97] V. Boddeti, J. Smereka, B. Kumar, A comparative evaluation of iris and ocular
recognition methods on challenging ocular images, Proc Intl Joint Conf Biometrics, IJCB (2011) 1–8.
[98] A. Ross, R. Jillela, J. Smereka, V. Boddeti, B. Kumar, R. Barnard, X. Hu,
P. Pauca, R. Plemmons, Matching highly non-ideal ocular images: An information fusion approach, Proc Intl Conf Biometrics, ICB (2012) 446–453.
[99] C.-W. Tan, A. Kumar, Human identification from at-a-distance images by simultaneously exploiting iris and periocular features, Proc Intl Conf Pattern Recognition, ICPR (2012) 553–556.
[100] P. Tome, J. Fierrez, R. Vera-Rodriguez, M. Nixon, Soft biometrics and their application in person recognition at a distance, IEEE Transactions on Information
Forensics and Security 9 (3) (2014) 464–475.
[101] S. J. Raudys, A. K. Jain, Small sample size effects in statistical pattern recognition: recommendations for practitioners, IEEE Transactions on Pattern Analysis
and Machine Intelligence 13 (3) (1991) 252–264.

61

[102] F. Roli, J. Kittler, G. Fumera, D. Muntoni, An experimental comparison of
classifier fusion rules for multimodal personal identity verification systems, in:
F. Roli, J. Kittler (Eds.), Multiple Classifier Systems, Springer Berlin Heidelberg, Berlin, Heidelberg, 2002, pp. 325–335.
[103] F. Alonso-Fernandez, J. Fierrez-Aguilar, H. Fronthaler, K. Kollreider, J. OrtegaGarcia, J. Gonzalez-Rodriguez, J. Bigun, Combining multiple matchers for fingerprint verification: A case study in biosecure network of excellence, Annals
of Telecommunications 62 (1-2) (2007) 62–82.
[104] S. Garcia-Salicetti, J. Fierrez-Aguilar, F. Alonso-Fernandez, C. Vielhauer,
R. Guest, L. Allano, T. Doan Trung, T. Scheidat, B. Ly Van, J. Dittmann,
B. Dorizzi, J. Ortega-Garcia, J. Gonzalez-Rodriguez, M. B. di Castiglione,
M. Fairhurst, Biosecure reference systems for on-line signature verification: A
study of complementarity, Annals of Telecommunications 62 (1-2) (2007) 36–
61.
[105] J. Fierrez-Aguilar, J. Ortega-Garcia, J. Gonzalez-Rodriguez, J. Bigun, Discriminative Multimodal Biometric Authentication Based on Quality Measures, Pattern Recognition 38 (5) (2005) 777–779.
[106] H. Hofbauer, F. Alonso-Fernandez, J. Bigun, A. Uhl, Experimental analysis regarding the influence of iris segmentation on the recognition rate, IET Biometrics 5 (3) (2016) 200–211.
[107] F. Alonso-Fernandez, J. Bigun, Best regions for periocular recognition with NIR
and visible images, Proc Intl Conf Image Processing, ICIP.
[108] P. Tome, J. Fierrez, F. Alonso-Fernandez, J. Ortega-Garcia, Scenario-based
score fusion for face recognition at a distance, Proc IEEE Workshop on Biometrics, in association with CVPR (2010) 67 –73.
[109] F. Alonso-Fernandez, J. Fierrez-Aguilar, A. Gilperez, J. Galbally, J. OrtegaGarcia, Robustness of signature verification systems to imitators with increasing
skills, Proc IAPR Intl Conf Document Analysis and Recognition, ICDAR.
62

[110] E. López-López, X. M. Pardo, C. V. Regueiro, R. Iglesias, F. E. Casado, Dataset
bias exposed in face verification, IET Biometrics 8 (4) (2019) 249–258.
[111] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, Y. Bengio, Generative adversarial nets, in: Z. Ghahramani,
M. Welling, C. Cortes, N. D. Lawrence, K. Q. Weinberger (Eds.), Advances
in Neural Information Processing Systems 27, Curran Associates, Inc., 2014,
pp. 2672–2680.
[112] F. Alonso-Fernandez, R. A. Farrugia, J. Bigun, J. Fierrez, E. Gonzalez-Sosa,
A survey of super-resolution in iris biometrics with evaluation of dictionarylearning, IEEE Access 7 (2019) 6519–6544.

63

