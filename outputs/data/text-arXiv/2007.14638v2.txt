JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

1

COVID-19 CT Image Synthesis with a Conditional
Generative Adversarial Network

arXiv:2007.14638v2 [eess.IV] 3 Dec 2020

Yifan Jiang, Member, IEEE, Han Chen, Murray Loew, Fellow, IEEE and Hanseok Ko, Senior Member, IEEE

Abstract—Coronavirus disease 2019 (COVID-19) is an ongoing
global pandemic that has spread rapidly since December 2019.
Real-time reverse transcription polymerase chain reaction (rRTPCR) and chest computed tomography (CT) imaging both play
an important role in COVID-19 diagnosis. Chest CT imaging
offers the benefits of quick reporting, a low cost, and high
sensitivity for the detection of pulmonary infection. Recently,
deep-learning-based computer vision methods have demonstrated
great promise for use in medical imaging applications, including
X-rays, magnetic resonance imaging, and CT imaging. However,
training a deep-learning model requires large volumes of data,
and medical staff faces a high risk when collecting COVID-19
CT data due to the high infectivity of the disease. Another issue
is the lack of experts available for data labeling. In order to meet
the data requirements for COVID-19 CT imaging, we propose a
CT image synthesis approach based on a conditional generative
adversarial network that can effectively generate high-quality
and realistic COVID-19 CT images for use in deep-learningbased medical imaging tasks. Experimental results show that
the proposed method outperforms other state-of-the-art image
synthesis methods with the generated COVID-19 CT images and
indicates promising for various machine learning applications
including semantic segmentation and classification.
Index Terms—COVID-19, computed topography, image synthesis, conditional generative adversarial network

I. I NTRODUCTION
ORONAVIRUS disease 2019 (COVID-19) [1], which
was first identified in Wuhan, China, in December 2019,
was declared a pandemic in March 2020 by the World Health
Organization (WHO). As of 21 July, there had been more than
14 million confirmed cases and 609,198 deaths across 188
countries and territories [2]. COVID-19 is the result of severe
acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and
its most common symptoms include fever, dry cough, a loss
of appetite, and fatigue, with common complications including
pneumonia, liver injury, and septic shock [3], [4].
There are two main diagnostic approaches for COVID-19:
rRT-PCR and chest computed tomography (CT) imaging [4].
In rRT-PCR, an RNA template is first converted by reverse
transcriptase into complementary DNA (cDNA), which is
then used as a template for exponential amplification using
polymerase chain reaction (PCR). However, the sensitivity of
rRT-PCR is relative low for COVID-19 testing [5], [6]. As an

C

This research work is supported by a National Research Foundation
(NRF) grant funded by the MSIP of Korea (number 2019R1A2C2009480).
(Corresponding author: Hanseok Ko.) Y. Jiang, H. Chen, and H. Ko are
with the School of Electrical Engineering, Korea University, Seoul 02841,
South Korea (e-mail: yfjiang@ispl.korea.ac.kr;hanchen@ispl.korea.ac.kr;
hsko@korea.ac.kr). M. Loew is with School of Engineering & Applied
Science, George Washington University, Washington, DC 20052, United States
(e-mail: loew@gwu.edu).

alternative, chest CT scans can be used to take tomographic
images from the chest area at different angles with postcomputed X-ray measurements. This approach has a higher
sensitivity to COVID-19 and is less resource-intensive than
traditional rRT-PCR [5], [6].
Over time, artificial intelligence (AI) has come to play
an important role in medical imaging tasks, including CT
imaging [7], [8], magnetic resonance imaging (MRI) [9] and
X-ray imaging [10]. Deep learning is a particularly powerful
AI approach that has been successfully employed in a wide
range of medical imaging tasks due to the massive volumes
of data that are now available. These large datasets allow
deep-learning networks to be well-trained, extending their
generalizability for use in various applications. However, the
collection of COVID-19 data for use in deep-learning models
is far more difficult than normal data collection. Because
COVID-19 is highly contagious [4], medical staff require fulllength protection for CT scans, and the CT scanner and other
equipment need to be carefully disinfected after an operation.
In addition, certain tasks, such as CT image segmentation,
require well-labeled data, which is labor-intensive. These
problems mean that the COVID-19 CT data collection process
can be difficult and time-consuming.
In order to speed up the COVID-19 CT data collection
process for deep-learning-based CT imaging and to protect
medical personnel from possible infection when coming into
contact with COVID-19 patients, we propose a novel cGAN
structure which contains a global-local generator and a multiresolution discriminator. Both the above generator and discriminator are dual network design so that they can learn
global and local information of CT images individually. Also,
the dual structure has a communication mechanism for information exchange so that it helps to generate a realistic
CT image with both stable global structure and diverse local
details. The main contributions of the proposed method are as
follows:
(1) We presented a dual generator structure (global-local
generator). This dual global-local generator contains two
individual generators that address and reflect differentlevel of information from CT data.
(2) We proposed a dual discriminator (multi-resolution discriminator) that contains two sub-discriminators. These
two discriminators learn to distinguish input from real or
fake. They are specially designed for learning from fullresolution CT data and half-resolution ones, respectively.
(3) A dynamic communication mechanism is proposed for
both generator and discriminator. In the case of the generator, a dynamic element-wise sum process (DESUM)

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

2

II. R ELATED WORKS

Chest
CT image

Lung
CT image

Lung
segmentation map

Fig. 1. Example CT images from three COVID-19 patients. The first column
shows CT images of the entire chest, the second column contains CT images
of the lungs only, and the third column shows the corresponding segmentation
map, with the lung region colored red, ground-glass opacity colored blue, and
areas of consolidation colored green.

helps generators balance the information of the lung area
and small lesion area by dynamically weighting two terms
during the element-wise sum process. It also prevents the
generator from overweighting details like a traditional
cGAN model does for wild scene dataset. For the discriminator, a dynamic feature matching process (DFM)
is proposed for dynamically weighting the loss terms
from two inputs with different resolutions. In particular,
it allows the half-resolution discriminator to receive more
information with lung structure or large lesion area. It also
offers more features of small lesion area to full-resolution
discriminator. This dual multi-resolution discriminator
helps to stabilize the training process and improves the
image quality of the synthetic data.
(4) The proposed method outperforms other state-of-the-art
image synthesizers in several image-quality metrics and
demonstrates its potential for use in image synthesis for
computer vision tasks such as semantic segmentation for
COVID-19 chest CT imaging.
(5) A safe COVID-19 chest CT data collection method based
on image synthesis is presented. The potential applications of proposed method are summarized as follows:
(a) COVID-19 CT synthesis method can be applied
to the data augmentation task for the deep learning
based COVID-19 diagnosis approaches; (b) COVID-19
CT synthesis method can also be utilized to train the
intern radiologists who may need abundant snapshots
of COVID-19 CT scans for training purposes; (c) The
proposed COVID-19 CT synthesis method can be easily
transferred from CT imaging domain to another medical
imaging area (e.g. X-ray, MRI).

Generative adversarial networks. Generative adversarial networks (GANs) were first reported in 2014 [11], and they
have since been widely applied to many practical applications, including image synthesis [12], [13], [14], [15], image
enhancement [16], [17], human pose estimation [18], [19],
and video generation [20], [21]. A GAN structure generally
consists of a generator and a discriminator, where the goal
of the generator is to fool the discriminator by generating a
synthetic sample that cannot be distinguished from real samples. A common GAN extension is the conditional generative
adversarial network (cGAN) [22], which generates images
that are conditional on class labels. cGAN always produces
more realistic results than traditional GANs due to the extra
information from these conditional labels.
Conditional image-to-image translation. Conditional imageto-image translation methods can be divided into three categories based on the input conditions. Class-conditional methods take class-wise labels as input to synthesize images [22],
[23], [24], [25] while, more recently, text-conditional methods
have been introduced [26], [27]. cGAN-based methods [12],
[13], [14], [15], [28], [29], [30], [31], [27], [26], [32] have been
widely used for various image-to-image translation methods,
including unsupervised [30], high-quality [13], multi-modal
[28], [14], [15], and semantic layout conditional image-toimage translation [12], [13], [14], [15]. In semantic layout
conditional methods, realistic images are synthesized under
the navigation of the semantic layout, meaning that it is easier
to control a particular region of the image.
AI-based diagnosis using COVID-19 CT imaging. Since
the outbreak of COVID-19, many researchers have turned to
CT imaging technology in order to diagnose and investigate
this disease. COVID-19 diagnosis methods based on chest
CT imaging have been introduced in order to improve test
efficiency [33], [34], [35], [36]. Rather than using CT imaging for rapid COVID-19 diagnosis, semantic segmentation
approaches have been employed to clearly label the focus
position in order to make it easier for medical personnel to
identify infected regions in a CT image [37], [38], [39], [40],
[41]. As an alternative to working at the pixel-level, highlevel classification or detection approaches have been proposed
[42], [43], [44], which can allow medical imaging experts
to rapidly locate areas of infection, thus speeding up the
diagnosis process. Though two CT image synthesis methods
have been previously reported [45], [46], they did not focus
on COVID-19 or lung CT imaging. cGAN was introduced
to COVID-19 CT image synthesis task firstly by [47], which
transforms a normal 3D CT slice to an abnormally synthetic
slice under the condition of 3D noise.
III. COVID-19 CT I MAGE S YNTHESIS WITH A
C ONDITIONAL G ENERATIVE A DVERSARIAL N ETWORK
In this paper, we propose a cGAN-based COVID-19 CT
image synthesis method. Here, COVID-19 CT image synthesis is formulated as a semantic-layout-conditional image-toimage translation task. The structure consisting of two main
components: a global-local generator and a multi-resolution

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

...

DESUM

Fglobal

To
discriminator

Half Resolution
segmentation
map

3

⊗ ⊕

1-α

Half resolution
synthesized
image

Flocal

DESUM

α

...

To
discriminator
Full resolution
synthesized

Segmentation
map

image

⊗

Training process of
global-local
generator

Full-resolution
discriminator

Data augmented
segmentation
map

Or

Concatenated seg and fake image

Global-local
generator

DFM
Concatenated seg and real image

Patch-wise
decisions
Synthesized
image

Concatenated seg and fake image

Or

Concatenated seg and real image

D i1

DFM
β

Half-resolution
discriminator

Complemented
synthesized
image

Training process of
multi-resolution
discriminator

Testing
process

Legend
Global information
generator
Multi-resolution
Discriminator

Local details
generator
CT image
completion process

Individual training for the
global information generator
Individual training for the
local details generator

+
+

Joint training for
the global-local
generator

Fig. 2. Overview of the proposed method. The upper section containing the training process of the global-local generator and multi-resolution discriminator,
while the lower right section shows the testing process. Within the global-local generator blocks, two types of generator are present: a global information
generator and local detail generator. Two individual training processes and single-joint training process are depicted in three different colorized arrows. DESUM
block represents the dynamic element-wise sum process which is shown in purple. The multi-resolution discriminator is depicted in blue. And the dynamic
feature matching process (DFM) is also shown as a blue block. The synthesized images are transferred from the generator to the discriminator, and this
process is shown as the dashed arrow. The yellow arrow shows the completion step for the process in which the non-lung region for the synthesized lung
image is added.

discriminator. During the training stage, the semantic segmentation map of a corresponding CT image is passed to
the global-local generator, where the label information from
the segmentation map is extracted via down-sampling and rerendered to generate a synthesized image via up-sampling. The
segmentation map is then concatenated with the corresponding
CT image or synthesized CT image to form the input for the

multi-resolution discriminator, which is used to distinguish the
input as either real or synthesized. The decisions from the
discriminator are used to calculate the loss and update the
parameters for both the generator and discriminator. During the
testing stage, only the generator is involved. A data augmented
segmentation map is used as input for the generator, from
which a realistic synthesized image can be obtained after

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

4

extraction and re-rendering. This synthesized lung CT image
is then combined with the non-lung area to form a completely
synthesized CT image as the final result. Figure 2 presents an
overview of the proposed method.

7×7-512×512-Conv-64, Relu

A. Global-local generator
The global-local generator G is a dual network which has
two sub-components: global-information generator G1 and
local-detail generator G2 . These generators work together
by moving in a coarse-to-fine direction. G1 takes charge of
learning and re-rendering global information, which always
contains high-level knowledge (e.g., semantic segmentation
labels and image structure information). G2 is then used for
detail enhancement (e.g., image texture and fine structures).
We train the global-local generator using a three-step process:

Downsampling

3×3-512×512-Conv-128, Relu
512×512-ResBlk-128, Relu

...

512×512-ResBlk-128, Relu

9 Residual
blocks

3×3-512×512-Conv-128, Relu
Up-sampling
7×7-512×512-Conv-64, Relu
Fig. 4. Network structure of local details generator G2 .

Fig. 3. The network structure of the global information generator G1 . The
parameters of each layer are separated by the notation ’-’, e.g. for the first
layer, 7 × 7 means the kernel size is 7, 256 × 256 is the size of the feature,
Conv denotes the category of the layer, 64 is the channel number, and Relu
is the activation function.

2) Individual training for the local details generator: The
structure of the local detail generator G2 , which is similar to
the structure of G1 , is shown in Figure 4. Rather than taking
a low-resolution segmentation map as input, the local detail
generator begins the synthesis process with a full-resolution
segmentation map (512×512) and maintains this size throughout. That allows the local detail generator to fully learn the
fine texture and structure and focus on low-level information
within the input image. G2 has a similar encoding-decoding
training procedure as G1, though the output synthesized image
is 512 × 512.
3) Joint training for the global-local generator: After
training G1 and G2 separately, a joint training process is
conducted. This is shown in the global-local generator region
of Figure 2. In the joint training stage, both G1 and G2
take the same input but with different resolutions (half- and
full-resolution, respectively). The two networks run a forward
process that differs from the individual training stage in which
the residual blocks in G2 takes the dynamic element-wise sum
from the output feature maps from the up-sampling process in
G1 and the output feature maps from down-sampling in G2 ,
meaning that G2 receives both global and local information to
reconstruct the output.
This training strategy enables the global-local generator G
to effectively learn both global information and local details
while also stabilizing the training process by simplifying it
into three relatively simple procedures.

1) Individual training for the global information generator:
The training process for G starts with the training of the
global information generator G1 . As shown in Figure 3, G1
takes a half-resolution (256 × 256) segmentation map as input,
which is then sent for down-sampling to reduce the feature
dimensions to 32 × 32. Nine residual blocks that maintain the
dimensions at 32 × 32 are used to reduce the computational
complexity and generate a large reception field. Finally, the
features are up-sampled and reconstructed back into a halfresolution (256 × 256) synthesized image.

B. Multi-resolution discriminator
A multi-resolution discriminator D is proposed in this paper.
This dual network structure consists of two sub-components:
the full-resolution discriminator D1 and the half-resolution
discriminator D2 . We design above two discriminators by
following the PatchGAN discriminator [12], therefore, proposed discriminator contains two PatchGAN discriminators.
Two discriminators make patch-wise decisions rather than
making a decision for the whole image. D1 takes the fullresolution input and learns the local information from CT

7×7-256×256-Conv-64, Relu

3×3-128×128-Conv-128, Relu
3×3-64×64-Conv-256, Relu

Downsampling

3×3-32×32-Conv-512, Relu
32×32-ResBlk-512, Relu

...

32×32-ResBlk-512, Relu

9 Residual
blocks

3×3-32×32-Conv-512, Relu
3×3-64×64-Conv-256, Relu
Up-sampling

3×3-128×128-Conv-128, Relu
7×7-256×256-Conv-64, Relu

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

Concat

4×4-2x↓ -Conv-64, LReLu
4×4-2x↓-Conv-128, LReLu
4×4-2x↓-Conv-256, LReLu
4×4-Conv-512, LReLu
4×4-Conv-1
Fig. 5. Network structure of the multi-resolution discriminator D. 2× ↓
denotes down-sampling with a factor of 2.

image, on the other hand, D2 takes half-resolution image as
input and focuses on the global information from CT image.
In addition, we proposed a dynamic feature matching process
(DFM) for improving the communication quality between D1
and D2 during the training process. As shown in Figure 5, we
first down-sample the segmentation map and real image into
half-resolution form, then the synthesized and real images are
randomly chosen to be concatenated with the segmentation
map to form two inputs (full- and half-resolution) for D.
The target discriminator takes corresponding input and makes
decision through a 70 × 70 receptive field, and the decision
is represented as a decision matrix, which represents the
patch-wise decisions for corresponding inputs. Then, patchwise decisions are used to update D with the DFM in order
to adaptively share multiple resolution intermediate features
between two discriminators.
The dual-discriminator design and dynamic feature matching process enable the multi-resolution discriminator D to
effectively learn local details, which can significantly improve
the quality of the synthesized image. By assigning local and
global discrimination to individual discriminators D1 and D2 ,
and dynamically weighting the feature matching losses from
D1 and D2 , the global structure can be maintained while also
enhancing the details of the synthesized images.
C. Dynamic communication mechanism
1) Dynamic element-wise sum process: The dynamic
element-wise sum (DESUM) process is utilized in the joint
training step of global-local generator. As shown in Figure
2, DESUM process takes two feature maps (Fglobal , Flocal )
from G1 and G2 , respectively. We trained a weighting network
that contains three convolutional layers and 2 fully-connected
layers to dynamically compute the weight of two input terms
from G2 . The DESUM process can be formulated as follow:
Fout = αFlocal ⊕ (1 − α)Fglobal

(1)

where α is learned by the weighting network. This weighting
network is updated during the joint training step. The DESUM

5

process effectively helps to dynamically adjust the generator
to specific input and balance the attention from global information to local details. To be specific, the DESUM process
can dynamically weight more to Flocal when receiving an
input that contains complex lesion area. On the other hand,
the DESUM process is able to avoid the generator from
overweight on tiny lesions but ignore global lung structure.
2) Dynamic feature matching process: As shown in Figure
2, the dynamic feature matching process (DFM) computes
weight parameter β for the dynamic feature matching loss
(LDF M ) which will be discussed in sub-section D. Similar
to DESUM, DFM uses a CNN structure to calculate the
weight parameter by observing an intermediate feature from
D1 . However, DFM works on loss level rather than the feature
level. By applying the DFM process to DFM loss, multiresolution discriminator D is able to balance between two
resolution inputs and communicate with each other. Since
the weight parameter β is decided by intermediate feature
D1i , which is the intermediate feature from ith layer of fullresolution discriminator D1 , the DFM network can obtain
enough information of full-resolution input and weight LDF M
correctly.
D. Learning objective
The overall learning objective of proposed approach can be
represented by equation (2):
"
min max
G

2
X

D1 ,D2

LcGAN {G(m), Di (m, x), Di (m, G(m))}+

i=1
2
X

λ

#
LDF M {G(m), Di (m, x), Di (m, G(m))}

i=1

(2)
There are two main loss terms in the overall learning objective
function (2): the loss for the cGAN LcGAN and the loss for
dynamic feature matching LDF M . The variable x is the real
input image and m is the corresponding segmentation map.
G represents global-local generator while Di represents the
full-resolution discriminator D1 or half-resolution discriminator D2 . G(m) denotes the synthesized image produced by
generator G with input segmentation map s, Di (m, x) and
Di (m, G(m)) are the patch-wise decisions made by multiresolution discriminator D with the real image or synthesized
image as input, respectively. λ is the weight factor of feature
matching loss term.
We designed the cGAN loss function based on pix2pix [12],
as shown in (3)
LcGAN = Em,x [logD(m, x)] + Em,x [log(1 − D(m, G(m)))]
(3)
This loss term allows cGAN to generate a realistic synthesized
image that can fool discriminator under the condition of the
input segmentation map.
In order to help to improve the communication efficiency
between multi-resolution discriminator D1 and D2 , we pro-

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

posed a dynamic feature matching loss (Eq. (4)) which is
inspired by the feature matching loss from ref [48]:
LDF M = Em,x

3
X
β
{ [||D2i (m, x) − D2i (m, G(m))||1 +
N
i
i=1

1−β
[||D1i (m, x) − D1i (m, G(m))||1 ]}
Ni
(4)
where i represents the ith layer of D and Ni is the total
number of elements in the ith layer. β is a weight parameter
which is computed by dynamic feature matching process
(described in sub-section C). Original feature matching loss
only considers to manage the feature map difference between
different layers within a single discriminator. In order to overcome the communication problem between two discriminators,
the dynamic feature matching loss (DFM loss) dynamically
weights the feature matching losses from the full- and halfresolution discriminators through observing an intermediate
feature D1i . By applying DFM loss, it allows us to train D1
and D2 synchronously, and to learn the details from the inputs
with different resolutions effectively.
E. Testing process
Rather than using both global-local generator G and multiresolution discriminator D as in the training stage, we only
utilize the pre-trained G in the testing process. The input for G
in this stage is a data augmented segmentation map from the
real data. During the practical deployment, the segmentation
maps can be obtained by augmenting the segmentation maps
which are made by experienced radiologists using standard
image editing software. After passing it through G, a synthesized CT image of the lung area is generated. The final step
in the process combines the synthesized lung image with the
corresponding non-lung area from the real image to produce
a complete synthesized image.
IV. E XPERIMENTS
A. Experimental settings
Dataset. In order to evaluate the proposed method and compare its performance to other state-of-the-art methods, we
use 829 lung CT slices from nine COVID-19 patients, which
were made public on 13 April 2020, by Radiopaedia [49].
This dataset includes the original CT images, lung masks,
and COVID-19 infection masks. The infection masks contain ground-glass opacity and consolidation labels, which are
the two most common characteristics used for COVID-19
diagnosis in lung CT imaging [50]. In this experiment, we
select 446 slices that contained the areas of infection. We
divide the selected dataset into three parts: a training set for
image synthesis task (300), a test set for image synthesis
task (73), a test set for semantic segmentation task (73). To
fully train the deep-learning-based model, data augmentation
pre-processing is applied (Table I). The training set for the
semantic segmentation tasks consists of real data and synthetic
data: the real data comes from the test set of image synthesis
task and the synthetic data is generated from the segmentation
maps from the test set of image synthesis task.

6

TABLE I
ORGANIZATION OF THE COVID-19 CT IMAGE DATASET
Dataset
Training set
(image synthesis)
Test set
(image synthesis)
Training set
(semantic segmentation)
Test set
(semantic segmentation)

Original count

After data augmentation

300

12,000

73

10,220

-

-

73

10,220

The data augmentation methods include random resizing
and cropping, random rotation, Gaussian noise, and elastic
transform.
Evaluation metrics. To accurately assess model performance,
we utilize both image quality metrics and medical imaging
semantic segmentation metrics:
Four image quality metrics are considered in this study:
Fréchet inception distance (FID) [51], peak-signal-to-noise
ratio (PSNR) [52], structural similarity index measure (SSIM)
[52], and root mean square error (RMSE) [15]. FID measures
the similarity of the distributions of real and synthesized
images using a deep-learning model. PSNR and SSIM are the
most widely used metrics when evaluating the performance
of image restoration and reconstruction methods. The former
represents the ratio between the maximum possible intensity
of a signal and the intensity of corrupting noise, while the
latter reflects the structural similarity between two images.
Three semantic segmentation metrics for medical imaging
are used in this experiment: the dice score (Dice), sensitivity
(Sen), and specificity (Spec) [53], [54]. The dice score evaluates the area of overlap between a prediction and the ground
truth, while sensitivity and specificity are two statistical metrics for the performance of binary medical image segmentation
tasks. The former measures the percentage of actual positive
pixels that are correctly predicted to be positive, while the
latter measures the proportion of actual negative pixels that
are correctly predicted to be negative. These three metrics are
employed for semantic segmentation based on the assumption
that, if the quality of the synthesized images is high enough,
excellent segmentation performance can be achieved when
using the synthesized images as input.
Implementation details. We transform all of the CT slices
into gray-scale images on a Hounsfield unit (HU) scale [600,1500]. The sizes of the images and segmentation maps
are then rescaled from 630 × 630 to 512 × 512. All of the
image synthesis methods are trained with 20 epochs, with a
learning rate that is maintained at 0.0002 for the first 10 epochs
before linearly decaying to zero over the following ten epochs.
Global-local generator G and multi-resolution discriminator D
are trained using an Adam optimizer with parameters β1 = 0.5
and β2 = 0.999. The feature matching loss weight λ is set at
10. The batch size used to train the proposed method is 16. All
of the experiments are run in an Ubuntu 18.04 environment
using an Intel i7 9700k CPU and two GeForce RTX Titan
graphics cards (48 GB VRAM).

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

7

TABLE II
IMAGE QUALITY EVALUATION RESULTS OF SYNTHETIC CT IMAGES
(T HE BEST EVALUATION SCORE IS MARKED IN BOLD . ↑ MEANS HIGHER NUMBER IS BETTER , AND ↓ INDICATES LOWER NUMBER IS BETTER .)
Categories
Metrics
OURS
SEAN [15]
SPADE [14]
Pix2pixHD [13]
Pix2pix [12]

FID (↓)
0.0327
0.0341
0.0389
0.0430
0.0611

Complemented images
PSNR (↑)
SSIM (↑)
26.89
0.8936
26.69
0.8922
26.50
0.8903
26.63
0.8893
26.56
0.8870

RMSE (↓)
0.0813
0.0837
0.0854
0.0840
0.0913

FID (↓)
0.3641
0.3575
0.4812
0.4283
8.4077

Lung only images
PSNR (↑) SSIM (↑)
28.17
0.8959
28.02
0.8952
27.79
0.8928
27.82
0.8910
26.56
0.8855

RMSE (↓)
0.2747
0.2795
0.2864
0.2856
0.3301

TABLE III
EXPERIMENTAL RESULTS FOR CT IMAGES USING SEMANTIC SEGMENTATION METHODS
(REPLACING REAL DATA WITH DIFFERENT PROPORTIONS OF SYNTHETIC DATA)
(T HE BEST EVALUATION SCORE IS MARKED IN BOLD . ↑ MEANS HIGHER NUMBER IS BETTER , AND ↓ INDICATES LOWER NUMBER IS BETTER . R ATIO
MEANS REPLACING CERTAIN PROPORTION OF SYNTHETIC DATA . ε REPRESENTS A SMALL POSITIVE QUANTITY WHICH IS SMALLER THAN 1e−5 . 50% (1) ,
50%(2) , 50%(3) , 50%(4) REPRESENT THE SYNTHETIC DATA ARE FROM SEAN [15], SPADE [14], P IX 2 PIX HD [13] AND P IX 2 PIX [12], RESPECTIVELY.
Focus
Ratio
0%
10%
20%
30%
40%
50%
50%(1)
50%(2)
50%(3)
50%(4)

Ground-glass opacity
Dice (%, ↑)
Sen (%, ↑)
Spec (%, ↑)
87.55±0.20
86.84±0.31
99.82±0.01
87.34±0.27
85.08±0.45
99.85±0.01
84.22±0.36
83.38±0.41
99.77±0.01
87.43±0.35
85.73±0.41
99.84±0.01
87.07±0.27
86.70±0.29
99.80±0.01
86.98±0.35
86.46±0.40
99.80±0.01
85.23±0.40
84.99±0.31
99.80±0.01
83.04±0.28
81.56±0.24
99.80±0.01
81.24±0.37
79.53±0.29
99.79±0.01
75.33±0.24
71.02±0.38
99.75±0.01

Consolidation
Sen (%, ↑)
82.80±0.51
84.23±0.55
83.60±0.33
87.14±0.21
81.94±0.36
82.18±0.36
83.66±0.19
81.50±0.30
78.14±0.46
70.55±0.21

Dice (%, ↑)
84.88±0.33
85.91±0.44
84.30±0.19
86.01±0.21
85.81±0.24
85.58±0.23
83.54±0.22
81.89±0.20
80.20±0.44
72.01±0.25

Spec (%, ↑)
99.96±ε
99.96±ε
99.95±ε
99.95±ε
99.97±ε
99.97±ε
99.97±ε
99.96±ε
99.96±ε
99.95±ε

Dice (%, ↑)
89.57±0.18
89.35±0.32
86.67±0.36
89.32±0.22
88.92±0.20
89.19±0.24
86.04±0.29
85.99±0.21
83.22±0.45
79.10±0.25

Infection
Sen (%, ↑)
88.58±0.23
87.14±0.39
85.83±0.27
88.11±0.30
87.90±0.30
88.12±0.37
86.00±0.39
84.05±0.15
83.01±0.48
78.89±0.39

Spec (%, ↑)
99.82±0.01
99.85±0.01
99.76±0.01
99.82±0.01
99.81±0.01
99.82±0.01
99.82±0.01
99.81±0.01
99.80±0.01
99.77±0.01

TABLE IV
EXPERIMENTAL RESULTS FOR CT IMAGES USING SEMANTIC SEGMENTATION METHODS
(ADDING SYNTHETIC DATA WITH DIFFERENT PROPORTIONS)
(T HE BEST EVALUATION SCORE IS MARKED IN BOLD . ↑ MEANS HIGHER NUMBER IS BETTER , AND ↓ INDICATES LOWER NUMBER IS BETTER . R ATIO
MEANS ADDING CERTAIN PROPORTION OF SYNTHETIC DATA . ε REPRESENTS A SMALL POSITIVE QUANTITY WHICH IS SMALLER THAN 1e−5 .)
Focus
Ratio
0%
10%
20%
30%
40%
50%

Ground-glass opacity
Dice (%, ↑)
Sen (%, ↑)
Spec (%, ↑)
87.55±0.20
86.84±0.31
99.82±0.01
87.65±0.40
85.82±0.32
99.84±0.01
87.87±0.34
87.67±0.28
99.81±0.01
87.99±0.36
87.25±0.36
99.82±0.01
88.33±0.22
88.71±0.32
99.81±0.01
88.16±0.30
86.88±0.01
99.84±0.01

Dice (%, ↑)
84.88±0.33
86.12±0.33
85.52±0.34
86.33±0.30
87.25±0.28
87.09±0.34

Consolidation
Sen (%, ↑)
82.80±0.51
86.02±0.63
84.16±0.53
86.38±0.51
86.30±0.38
86.54±0.47

B. Quantitative results
The performance of the proposed method is assessed according to both image quality and medical imaging semantic
segmentation.
1) Image quality evaluation: In this study, common image
quality metrics are employed to assess the synthesis performance of the proposed method and four other state-ofthe-art image synthesis methods: SEAN [15], SPADE [14],
Pix2pixHD [13], and Pix2pix [12]. We evaluate image quality
for two synthetic image categories: complete and lung-only
images. The complete images are those CT images generated
by merging a synthesized lung CT image with its corresponding non-lung CT image. The evaluation results are presented
in Table II.
The proposed method outperforms other state-of-the-art
methods based on the four image quality metrics for both
the complete and lung-only images. Due to the design of the
global-local generator and multi-resolution discriminator, the

Spec (%, ↑)
99.96±ε
99.95±ε
99.96±ε
99.95±ε
99.96±ε
99.96±ε

Dice (%, ↑)
89.57±0.18
89.67±0.31
89.87±0.12
89.78±0.22
90.19±0.17
90.06±0.30

Infection
Sen (%, ↑)
88.58±0.23
88.11±0.32
89.44±0.20
89.17±0.27
90.34±0.31
88.88±0.18

Spec (%, ↑)
99.82±0.01
99.84±0.01
99.81±0.01
99.82±0.01
99.81±ε
99.83±0.01

proposed model can generate realistic lung CT images for
COVID-19 with a complete global structure and fine local
details and maintain a relatively high signal-to-noise ratio.
Thus, the proposed method can achieve state-of-the-art image
synthesis results based on image quality.
2) Medical imaging semantic segmentation evaluation:
To evaluate the reconstruction capability of the proposed
method, we utilize Unet, a common medical imaging semantic
segmentation approach [55]. We first train the Unet model on
a mix of synthetic and real CT images. The training set of
this task consists of real and synthetic data from the test set
of the image synthesis tasks, and the test set here we use the
training set of the image synthesis task.
This evaluation consists of two independent experiments: (1)
keeping the total number of images the same while replacing
the real data with synthesized data from a proportion of 0%
to 50% in steps of 10% and (2) keeping the number of real
images the same and adding a certain proportion of synthetic
images from 0% to 50% in steps of 10%. The first experiment

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

evaluates how similar the synthetic and real data are and the
second evaluates the image synthesis potential of the synthetic
data. We consider three categories in the assessment: groundglass opacity, consolidation, and infection (which considers
both ground-glass opacity and consolidation). The evaluation
results for the two experiments are presented in Table III and
Table IV, respectively. The pre-trained Unet model is then
tested with a fixed real CT image dataset. 10,220 images from
the test set are divide equally into 10 folds, the evaluation
results are reported with the format as MEAN ± 95% CONFIDENCE INTERVAL among above folds.
In Table III, we describe the experimental results of different
replacing ratios of synthetic data. We can obtain the best
performance when using pure real data as a training set. By
replacing the real data with a ratio of synthetic data, the
semantic segmentation performance of Unet does not decrease
and stay at a stable level. By replacing real data with 30%
synthetic data, the Unet obtains the best performance on the
Spec metric for ground-glass opacity focus, also it gets the best
performance on Dice and Sen metrics for consolidation focus.
The experimental results from Table III show that synthetic CT
images are similar to real CT images. They are realistic enough
even replacing the real data with a large ratio of synthetic data,
the semantic segmentation performance of Unet still seems
promising. Besides, we also demonstrate the performance
comparison with other state-of-the-art image synthesizers in
Table III. Under the condition of replacing real data with 50%
synthetic data which is generated by four different competitors,
the proposed method shows the competitive performance on
the semantic segmentation tasks.
Table III presents the experimental results for different
replacement ratios for the synthetic data. We obtain the best
performance when using pure real data as the training set.
By replacing the real data with a proportion of synthetic
data, the semantic segmentation performance of Unet does
not decrease, but rather remains stable. By replacing real data
with 30% synthetic data, Unet obtains the best performance
for the Spec metric for ground-glass opacity and for the Dice
and Sen metrics for consolidation. The experimental results
thus indicate that the synthetic CT images are similar to
real CT images. They are sufficiently realistic for semantic
segmentation with Unet to be successful even when real data
is replaced with a large proportion of synthetic data.
Table IV presents the semantic segmentation results when
a certain proportion of extra synthetic data is added to the
real data. The best performance is obtained when adding 40%
synthetic data. Overall, the results indicate that the synthetic
CT images are sufficiently diverse and realistic, meaning that
they have the potential to be utilized in image synthesis to
improve the dataset quality for deep-learning-based COVID19 diagnosis.
C. Qualitative results
To intuitively demonstrate synthetic results and easily compare them with the results from other state-of-the-art image
synthesis methods, we show the synthetic examples in both
Figure 6 and Figure 7 in this subsection.

8

The synthetic images from three individual cases are compared in Figure 6. The first case shows that a consolidation
infection area locates on the lower left of CT image. By
comparing the synthetic results from the proposed method,
SEAN [15] and SPADE [14], the infection area remains the
original structure and texture in the result which is generated
by the proposed method, however, we found that in the results
of SEAN and SPADE, there some unnatural artifacts (holes)
are generated in the position that yellow arrow points out.
For the second case, a large area of ground-glass infection is
detected, the results of SPADE and SEAN ignore some small
lung area in the middle of the infection area, but the proposed
method can still reflect above small lung area correctly. In the
final case, it contains both two categories of infection area:
consolidation and ground-glass opacity, and the ground-glass
opacity is surrounded by the consolidation area. If we focus
on the surrounded area, we can found out that the boundary
of two infection area is not clear in the synthetic image of
SEAN, and the ground-glass area are mistakenly generated as
lung area in the synthesized image of SPADE. The result of
the proposed method in case 3 shows that it has the ability to
handle this complex situation and produce realistic synthetic
CT images with high image quality.
We present some synthetic examples that are generated by
the proposed method in Figure 7. We select one example for
each patient (8 samples from 9 patients; patient #3 is skipped
because the segmentation maps were miss-labeled). For Patient
#0, the consolidation area is located at the bottom of the lung
area; the synthetic image shows a sharp and high-contrast
consolidation area that can be easily distinguished from the
surrounding non-lung region. The slides for Patients #1 and
#4 have a similarity in that the lung area contains widespread
ground-glass opacity. Consolidation is sporadically located
within this ground-glass opacity. The small consolidation area
can be easily identified due to the clear boundary between the
two infection areas. Patient #6 shows ground-glass opacity and
consolidation that are distant from each other. The results thus
illustrate that the proposed method can handle the two types
of infection areas together in a single lung CT image. The CT
slides of Patients #5, #7, and #8 show the simplest cases, with
only a single category of infection (ground- glass opacity).
The experimental results thus indicate that realistic groundglass opacity can be obtained using the proposed method.
D. Discussion
In order to further discuss the efficiency of DESUM process
and DFM process, and justify an optimal structure of cGAN,
we follow the experimental settings of the image quality evaluation and medical imaging semantic segmentation evaluation
in sub-section B. Experimental results are shown in Table V.
1) Dynamic element-wise sum process (DESUM): In the
second part of Table V, we evaluate the performance of
three variations that are related to DESUM. Without DESUM,
using Fglobal , one fixed α number. The performance shows
that DESUM can effectively improve the image quality and
segmentation performance. The intermediate feature Fglobal
offers less information than Fglobal does, which influences the

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

Segmentation
map

Original CT
image

OURS

SEAN [15]

9

SPADE [14]

Case 1
no zoom in

Case 1
zoom in

Case 2
no zoom in

Case 2
zoom in

Case 3
no zoom in

Case 3
zoom in

Fig. 6. Synthetic lung CT images generated by the proposed method and the other two competitive state-of-the-art image synthesis approaches. The first
column shows the segmentation map including the lung (red), ground-glass opacity (blue), and consolidation (green) areas. The second column shows the
original CT image. The third, fourth, fifth columns show the synthetic samples which are generated by the proposed method, SEAN [15] and SPADE [14] in
order. Each case is presented with zoom in order to show more details, and the yellow arrows point out the special area which is described in the main text.

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

Segmentation
map

Original CT
image

Synthesized CT
image

Original lung
CT image

10

Synthesized lung
CT image

Patient #0

Patient #1

Patient #2

Patient #4

Patient #5

Patient #6

Patient #7

Patient #8

Fig. 7. Synthetic lung CT images generated by the proposed method. Eight samples are selected, each from an individual patient. The first column shows the
segmentation map including the lung (red), ground-glass opacity (blue), and consolidation (green) areas. The second and third columns show the original and
synthetic CT images, respectively. The synthetic CT images here merge the synthetic lung CT image and the corresponding real non-lung area. The fourth
and fifth columns depict CT images for the original lung and synthesized CT images, respectively.

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

11

Ours

0.0327

26.89

89.19±0.24

w/o DESUM
using Fglobal
Fixed α = 0.5

0.0395
0.0355
0.0380

26.77
26.70
26.51

84.60±0.39
87.88±0.29
85.87±0.31

w/o DFM
using D2i
Fixed β = 0.5

0.0381
0.0340
0.0404

26.62
26.75
26.71

86.10±0.33
88.84±0.21
83.50±0.40

pared with common image synthesis approaches. In addition,
the evaluation results for semantic segmentation performance
demonstrated that the high image quality and fidelity of the
synthetic CT images enable their use in image synthesis for
COVID-19 diagnosis using AI models. For future research, the
authors plan to fully utilize high-quality synthetic COVID-19
CT images to improve specific computer vision approaches
that can help in the fight against COVID-19, such as lung CT
image semantic segmentation and rapid lung CT image-based
COVID-19 diagnosis.

D=1
D=3

0.0579
0.0330

26.55
26.80

79.44±0.80
88.93±0.26

R EFERENCES

G=1
G=3

0.0604
0.0325

26.51
26.85

76.03±0.52
89.01±0.20

TABLE V
A BLATION STUDY OF VARIOUS PROPOSED MODEL STRUCTURE
Method

FID (↓)

PSNR (↑)

Dice (%, ↑)

efficiency of DESUM. Using a fixed number of α can not
help to boost the performance, sometimes it may reduce the
performance since a fixed weight is not suitable for diverse
COVID-19 CT data.
2) Dynamic feature matching process (DFM): In this subsection, we discuss the evaluation of DFM. As shown in the
third part of Table V, we compared the performance of the
various (a) without DFM, (b) using the intermediate feature
from D1 , (c) using a fixed β number. The evaluation results
show that DFM can help to train discriminator stably and
improve the performance with multiple metrics. Moreover, the
intermediate feature from D1 contains many more details that
can help the DFM process to weight correctly. The results
also tell that a dynamic weight of β is critical for training our
multi-resolution discriminator.
3) Fine-tuning level optimization: We also investigated
some potential fine-tuning optimizations which are the number
of generators and the number of discriminators. In the last two
parts of Table V, we found that the number of generators and
discriminators is the important hyper-parameters of COVID-19
CT image synthesis task. A proper number of generator and
discriminator can not only avoid the model from overfitting
with details from multiple resolution sources but also improve
the training efficiency and stability. Experimental results show
that the performance can benefit from the dual structure
of both generator and discriminator the most, because this
dual structure can trade-off well between performance and
efficiency.
V. C ONCLUSION AND FUTURE STUDY
In this paper, we proposed a cGAN-based COVID-19 CT
image synthesis method that can generate realistic CT images
that included two main infection types; ground-glass opacity
and consolidation. The proposed method takes the semantic
segmentation map of a corresponding lung CT image, and the
cGAN structure learns the characteristics and information of
the CT image. A global-local generator and a multi-resolution
discriminator are employed to effectively balance global information with local details in the CT image. The experimental
results have shown that the proposed method was able to
generate realistic synthetic CT images and achieve state-ofthe-art performance in terms of image quality when com-

[1] “Coronavirus disease (covid-19) pandemic,” https://www.who.int/
emergencies/diseases/novel-coronavirus-2019.
[2] “Johns hopkins coronavirus resource center,” https://coronavirus.jhu.edu/
map.html.
[3] W.-j. Guan, Z.-y. Ni, Y. Hu, W.-h. Liang, C.-q. Ou, J.-x. He, L. Liu,
H. Shan, C.-l. Lei, D. S. Hui et al., “Clinical characteristics of coronavirus disease 2019 in china,” New England journal of medicine, vol.
382, no. 18, pp. 1708–1720, 2020.
[4] “Interim clinical guidance for management of patients with confirmed
coronavirus disease (covid-19),” https://www.cdc.gov/coronavirus/
2019-ncov/hcp/clinical-guidance-management-patients.html.
[5] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, Q. Tao, Z. Sun, and
L. Xia, “Correlation of chest ct and rt-pcr testing in coronavirus disease
2019 (covid-19) in china: a report of 1014 cases,” Radiology, p. 200642,
2020.
[6] Y. Fang, H. Zhang, J. Xie, M. Lin, L. Ying, P. Pang, and W. Ji,
“Sensitivity of chest ct for covid-19: comparison to rt-pcr,” Radiology,
p. 200432, 2020.
[7] K. Yan, Y. Peng, V. Sandfort, M. Bagheri, Z. Lu, and R. M. Summers,
“Holistic and comprehensive annotation of clinically significant findings
on diverse ct images: learning from radiology reports and label ontology,” in Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 2019, pp. 8523–8532.
[8] Z. Cui, C. Li, and W. Wang, “Toothnet: automatic tooth instance segmentation and identification from cone beam ct images,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2019, pp. 6368–6377.
[9] Z. Zhang, A. Romero, M. J. Muckley, P. Vincent, L. Yang, and
M. Drozdzal, “Reducing uncertainty in undersampled mri reconstruction
with active acquisition,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2019, pp. 2049–2058.
[10] X. Ying, H. Guo, K. Ma, J. Wu, Z. Weng, and Y. Zheng, “X2ctgan: reconstructing ct from biplanar x-rays with generative adversarial
networks,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2019, pp. 10 619–10 628.
[11] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014, pp. 2672–
2680.
[12] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017, pp. 1125–
1134.
[13] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro,
“High-resolution image synthesis and semantic manipulation with conditional gans,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2018, pp. 8798–8807.
[14] T. Park, M.-Y. Liu, T.-C. Wang, and J.-Y. Zhu, “Semantic image
synthesis with spatially-adaptive normalization,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2019,
pp. 2337–2346.
[15] P. Zhu, R. Abdal, Y. Qin, and P. Wonka, “Sean: Image synthesis with
semantic region-adaptive normalization,” 2019.
[16] Y.-S. Chen, Y.-C. Wang, M.-H. Kao, and Y.-Y. Chuang, “Deep photo
enhancer: Unpaired learning for image enhancement from photographs
with gans,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2018, pp. 6306–6314.
[17] Z. Wan, B. Zhang, D. Chen, P. Zhang, D. Chen, J. Liao, and F. Wen,
“Bringing old photos back to life,” arXiv preprint arXiv:2004.09484,
2020.

JIANG et al.: COVID-19 CT IMAGE SYNTHESIS WITH A CONDITIONAL GENERATIVE ADVERSARIAL NETWORK

[18] W. Yang, W. Ouyang, X. Wang, J. Ren, H. Li, and X. Wang, “3d human
pose estimation in the wild by adversarial learning,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2018, pp. 5255–5264.
[19] L. Ma, X. Jia, Q. Sun, B. Schiele, T. Tuytelaars, and L. Van Gool, “Pose
guided person image generation,” in Advances in Neural Information
Processing Systems, 2017, pp. 406–416.
[20] T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, G. Liu, A. Tao, J. Kautz, and B. Catanzaro, “Video-to-video synthesis,” arXiv preprint arXiv:1808.06601,
2018.
[21] T.-C. Wang, M.-Y. Liu, A. Tao, G. Liu, J. Kautz, and B. Catanzaro,
“Few-shot video-to-video synthesis,” arXiv preprint arXiv:1910.12713,
2019.
[22] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”
arXiv preprint arXiv:1411.1784, 2014.
[23] A. Odena, C. Olah, and J. Shlens, “Conditional image synthesis with
auxiliary classifier gans,” in Proceedings of the 34th International
Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp.
2642–2651.
[24] H. Caesar, J. Uijlings, and V. Ferrari, “Coco-stuff: Thing and stuff classes
in context,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2018, pp. 1209–1218.
[25] L. Mescheder, A. Geiger, and S. Nowozin, “Which training methods for
gans do actually converge?” arXiv preprint arXiv:1801.04406, 2018.
[26] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and
X. He, “Attngan: Fine-grained text to image generation with attentional
generative adversarial networks,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2018, pp. 1316–1324.
[27] S. Hong, D. Yang, J. Choi, and H. Lee, “Inferring semantic layout
for hierarchical text-to-image synthesis,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
7986–7994.
[28] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in Proceedings
of the IEEE international conference on computer vision, 2017, pp.
2223–2232.
[29] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang,
and E. Shechtman, “Toward multimodal image-to-image translation,” in
Advances in neural information processing systems, 2017, pp. 465–476.
[30] M.-Y. Liu, T. Breuel, and J. Kautz, “Unsupervised image-to-image
translation networks,” in Advances in neural information processing
systems, 2017, pp. 700–708.
[31] X. Huang, M.-Y. Liu, S. Belongie, and J. Kautz, “Multimodal unsupervised image-to-image translation,” in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 172–189.
[32] B. Zhao, L. Meng, W. Yin, and L. Sigal, “Image generation from layout,”
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2019, pp. 8584–8593.
[33] H. Kang, L. Xia, F. Yan, Z. Wan, F. Shi, H. Yuan, H. Jiang, D. Wu,
H. Sui, C. Zhang et al., “Diagnosis of coronavirus disease 2019 (covid19) with structured latent multi-view representation learning,” IEEE
transactions on medical imaging, 2020.
[34] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha, W. Liang,
C. Wang, K. Wang et al., “Clinically applicable ai system for accurate
diagnosis, quantitative measurements, and prognosis of covid-19 pneumonia using computed tomography,” Cell, 2020.
[35] A. A. Ardakani, A. R. Kanafi, U. R. Acharya, N. Khadem, and
A. Mohammadi, “Application of deep learning technique to manage
covid-19 in routine clinical practice using ct images: Results of 10
convolutional neural networks,” Computers in Biology and Medicine,
p. 103795, 2020.
[36] L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, J. Bai, Y. Lu,
Z. Fang, Q. Song et al., “Artificial intelligence distinguishes covid-19
from community acquired pneumonia on chest ct,” Radiology, p. 200905,
2020.
[37] T. Zhou, S. Canu, and S. Ruan, “An automatic covid-19 ct segmentation based on u-net with attention mechanism,” arXiv preprint
arXiv:2004.06673, 2020.
[38] W. Xie, C. Jacobs, J.-P. Charbonnier, and B. van Ginneken, “Relational
modeling for robust and efficient pulmonary lobe segmentation in ct
scans,” IEEE Transactions on Medical Imaging, 2020.
[39] A. Voulodimos, E. Protopapadakis, I. Katsamenis, A. Doulamis, and
N. Doulamis, “Deep learning models for covid-19 infected area segmentation in ct images,” medRxiv, 2020.
[40] X. Chen, L. Yao, and Y. Zhang, “Residual attention u-net for automated
multi-class segmentation of covid-19 chest ct images,” arXiv preprint
arXiv:2004.05645, 2020.

12

[41] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, and
L. Shao, “Inf-net: Automatic covid-19 lung infection segmentation from
ct images,” IEEE Transactions on Medical Imaging, 2020.
[42] C. Zheng, X. Deng, Q. Fu, Q. Zhou, J. Feng, H. Ma, W. Liu, and
X. Wang, “Deep learning-based detection for covid-19 from chest ct
using weak label,” medRxiv, 2020.
[43] O. Gozes, M. Frid-Adar, N. Sagie, H. Zhang, W. Ji, and H. Greenspan,
“Coronavirus detection and analysis on chest ct with deep learning,”
arXiv preprint arXiv:2004.02640, 2020.
[44] S. Hu, Y. Gao, Z. Niu, Y. Jiang, L. Li, X. Xiao, M. Wang, E. F. Fang,
W. Menpes-Smith, J. Xia et al., “Weakly supervised deep learning for
covid-19 infection detection and classification from ct images,” arXiv
preprint arXiv:2004.06689, 2020.
[45] A. D. Lauritzen, X. Papademetris, S. Turovets, and J. A. Onofrey,
“Evaluation of ct image synthesis methods: From atlas-based registration
to deep learning,” arXiv preprint arXiv:1906.04467, 2019.
[46] Y.-W. Chen, H.-Y. Fang, Y.-C. Wang, S.-L. Peng, and C.-T. Shih, “A
novel computed tomography image synthesis method for correcting the
spectrum dependence of ct numbers,” Physics in Medicine & Biology,
vol. 65, no. 2, p. 025013, 2020.
[47] S. Liu, B. Georgescu, Z. Xu, Y. Yoo, G. Chabin, S. Chaganti, S. Grbic,
S. Piat, B. Teixeira, A. Balachandran et al., “3d tomographic pattern
synthesis for enhancing the quantification of covid-19,” arXiv preprint
arXiv:2005.01903, 2020.
[48] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
style transfer and super-resolution,” in European conference on computer
vision. Springer, 2016, pp. 694–711.
[49] “Covid-19 ct segmentation dataset,” https://medicalsegmentation.com/
covid19/.
[50] M. Chung, A. Bernheim, X. Mei, N. Zhang, M. Huang, X. Zeng, J. Cui,
W. Xu, Y. Yang, Z. A. Fayad et al., “Ct imaging features of 2019 novel
coronavirus (2019-ncov),” Radiology, vol. 295, no. 1, pp. 202–207, 2020.
[51] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
“Gans trained by a two time-scale update rule converge to a local nash
equilibrium,” in Advances in Neural Information Processing Systems,
2017, pp. 6626–6637.
[52] A. Hore and D. Ziou, “Image quality metrics: Psnr vs. ssim,” in 2010
20th International Conference on Pattern Recognition. IEEE, 2010,
pp. 2366–2369.
[53] A. Fenster and B. Chiu, “Evaluation of segmentation algorithms for
medical imaging,” in 2005 IEEE Engineering in Medicine and Biology
27th Annual Conference. IEEE, 2006, pp. 7186–7189.
[54] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
neural networks for volumetric medical image segmentation,” in 2016
Fourth International Conference on 3D Vision (3DV). IEEE, 2016, pp.
565–571.
[55] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference on
Medical image computing and computer-assisted intervention. Springer,
2015, pp. 234–241.

