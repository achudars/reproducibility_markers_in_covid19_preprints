Multi-stage transfer learning for lung segmentation
using portable X-ray devices for patients with
COVID-19

arXiv:2011.00133v1 [eess.IV] 30 Oct 2020

Plácido L. Vidala,b,∗, Joaquim de Mouraa,b , Jorge Novoa,b , Marcos Ortegaa,b
a Centro

de investigación CITIC, Universidade da Coruña, Campus de Elviña, s/n, 15071 A
Coruña, Spain
b Grupo VARPA, Instituto de Investigación Biomédica de A Coruña (INIBIC), Universidade
da Coruña, Xubias de Arriba, 84, 15006 A Coruña, Spain

Abstract
In 2020, the SARS-CoV-2 virus causes a global pandemic of the new human
coronavirus disease COVID-19. This pathogen primarily infects the respiratory
system of the afflicted, usually resulting in pneumonia and in a severe case
of acute respiratory distress syndrome. These disease developments result in
the formation of different pathological structures in the lungs, similar to those
observed in other viral pneumonias that can be detected by the use of chest
X-rays. For this reason, the detection and analysis of the pulmonary regions,
the main focus of affection of COVID-19, becomes a crucial part of both clinical
and automatic diagnosis processes.
Due to the overload of the health services, portable X-ray devices are widely
used, representing an alternative to fixed devices to reduce the risk of crosscontamination. However, these devices entail different complications as the
image quality that, together with the subjectivity of the clinician, make the
diagnostic process more difficult.
In this work, we developed a novel fully automatic methodology specially
designed for the identification of these lung regions in X-ray images of low
∗ Corresponding

author
Email addresses: placido.francisco.lizancos.vidal@udc.es (Plácido L. Vidal),
joaquim.demoura@udc.es (Joaquim de Moura), jnovo@udc.es (Jorge Novo), mortega@udc.es
(Marcos Ortega)

Preprint submitted to arXiv

November 3, 2020

quality as those from portable devices. To do so, we took advantage of a large
dataset from magnetic resonance imaging of a similar pathology and performed
two stages of transfer learning to obtain a robust methodology with a low
number of images from portable X-ray devices. This way, our methodology
obtained a satisfactory accuracy of 0.9761 ± 0.0100 for patients with COVID19, 0.9801 ± 0.0104 for normal patients and 0.9769 ± 0.0111 for patients with
pulmonary diseases with similar characteristics as COVID-19 (such as pneumonia)
but not genuine COVID-19.
Keywords: CAD system, radiography, X-ray, lung segmentation, COVID-19,
transfer learning

1. Introduction
The World Health Organization (WHO) declared a global health emergency
on January 30th, 2020, due to the spread of SARS-CoV-2 and its disease COVID19 beyond the People’s Republic of China. Thus, the pandemic surpassed the
million of deaths as well as tens of millions of people infected worldwide [1].
One of the first and most prominent symptoms is the development of viral
pneumonia, highlighting fever, cough, nasal congestion, fatigue, and other respiratory tract related affections [2]. In more serious cases, the patients may present
acute respiratory distress syndrome or even systemic symptomatic manifestations
[3, 4, 5].
Because of this characteristics of primarily affecting the respiratory tract,
real-time polymerase chain reaction (RT-PCR) detection in mucosal and sputum
samples of known virus DNA strands became the standard method of diagnosis
at the beginning of the pandemic [6, 7]. However, this test has demonstrated
improvable sensitivity and specificity. Furthermore, these tests do not account
for the emergence of new variations in the DNA of the virus or are not updated
quickly enough to successfully control the pandemic [7].
Due to the saturation of the health services worldwide and the necessity
of a quick diagnosis and analysis of severe cases, among others, the use of

2

computerized tomography (CT) scans and chest X-ray image analysis has been
prominent, even complementary to the PCR tests. These devices are especially
useful after initial triage and referral to emergency services to further assess
known lung regions of virus infection (despite the expertise needed to correctly
study them [8]).
For these reasons, at the dawn of the pandemic, proven computational
methodologies of medical image analysis have been tested, as well as developing
new ones with the aim of facilitating, accelerating and reducing the subjectivity
factor of diagnostics at a critical moment for humanity [9, 10]. Most of these
methodologies are based on deep learning strategies, except for some particular
proposals that use classic machine learning approaches [11] or others that actually
use these techniques as support for deep learning methods [12, 13].
Regarding methodologies that aimed to help with the diagnostic of COVID19 based on deep learning, one of the first trends is to use these strategies to
perform a medical screening. These methodologies return a label or severity of a
COVID-19 candidate patient [14, 15, 16, 17].
Other trend with deep learning automatic approaches is to aid in the segmentation of the pulmonary region of interest. This region, as mentioned, is
hard to correctly assess due to the difficulties of analyzing a radiography [8] but
critical, as the COVID-19 clinical picture mainly manifests its effects in the lung
parenchyma (even after the patient has been discharged [18]). These works are
usually integrated as input of other methodologies to improve their results by
reducing the search space to only the region of interest or as a mean to greatly
improve a posterior visualization of these results [19].
The third trend consists in, instead of trying to segment these lung regions,
as they tend to be obfuscated by other tissues in the chest region, try to directly
obtain the pathological structures of COVID-19 [20, 21].
And, finally, works that try to palliate or complement their approaches
by merging some (or all) of the mentioned trends into a single methodology
[22, 23, 24].
Our work aims at following the second paradigm, extracting the lung regions,
3

but specifically for images that are captured by portable X-ray devices, with
lower details and, therefore, higher complexity. This is specially difficult and,
to the best of our knowledge, there are no other systems specially designed to
work with chest radiographs obtained from these portable machines. This is
specially relevant as these devices are recommended by the American College of
Radiology (ACR), as in cases as the COVID-19, they help to minimize the risk
of cross-infection and allow for a comfortable and flexible imaging of the patients
[25]. These systems are ideal for emergency and saturation of the healthcare
services situations, as they do not require strict structuring of the established
circuit and protocol [26].
However, performing a diagnostic with these portable devices is particularly
challenging, as the generated images are of lesser quality due to the capture
conditions, more difficult to diagnose (as they usually only allow for a supine
image instead of the usual multiple perspectives) and, due to the fact that they
are obtained in emergencies, less available to researchers. Nonetheless, the use of
these radiological studies is critical, since they not only serve for the monitoring
and assessment of patients already referred by triage, but they also permit to
compensate and mitigate the false negative rates of PCR studies in that same
previous triage step. Additionally, their use facilitates the work in crowded wards
[27]. As an example, Figure 1 shows three random images from clinical practice
with these portable devices for three different cases: patients with diagnosed
COVID-19, patients with pathologies unrelated to COVID-19 but with similar
impact in the lungs, and normal lungs. These images show how the images that
are taken with these portable devices tend to blur the tissues of the lung region,
as well as the pathological artifacts (specially in the images from afflicted lungs).
For this reason, in this work we designed a segmentation methodology especially for images of low quality from portable devices. To the best of our
knowledge, there is no other methodology specifically designed to analyze a set
of images including COVID-19, also being taken in these particular challenging
capture conditions. To solve this issue, we developed a training methodology
based on two stages of transfer learning between designed subsequent domains.
4

COVID-19

Pathological

Normal

Figure 1: Examples of images from portable devices for patients diagnosed with COVID-19,
non-COVID-19 pathological lungs with similar characteristics and with normal lungs.

Firstly, we took advantage of the knowledge learnt by a segmentation network
from another medical imaging domain trained with a larger number of images
and adapted it to be able to segment general lung chest images of high quality,
including COVID-19 patients. Then, using a limited dataset composed by images
from portable devices, we adapted the trained model from general lung chest
X-ray segmentations to work specifically with images from these portable devices.
The proposal would allow to delimit the pulmonary region of interest, critical
for the location of the pathological structures caused by COVID-19, independently
from the subjectivity of the clinician (a subject particularly sensitive in situations
of high stress and psychological wear) and under adverse capture conditions.
Moreover, this system can be used as input to another methodology to reduce
the search space to the lung region of interest or facilitate the subsequent
visualization of the results.
In summary, the main contributions of this article are:
• Fully automatic methodology to segment the pulmonary region in X-ray
images.
• Tested with images from COVID-19, pulmonary pathologies with similar
characteristics to COVID-19 and normal lungs.
5

• Datasets obtained from real clinical practice with portable devices (recommended when risk o cross-contamination and crowded hospital wings).
• Multiple stages of transfer learning between designed subsequent image
domains to work with a limited number of portable X-ray samples.
• To the best of our knowledge, our proposal represents the only fully
automatic study specifically designed to work with portable capture devices.
• Robust and accurate even with poor quality images from these portable
devices.
This document is divided into three parts. Section 2 Materials and Methods presents the resources that are used in this work, as well as the followed
methodology in detail. Section 3 Results presents the realization of all the steps
presented in the previous section. Finally, in Section 4 Discussion, we analyze
the relevance of each obtained result as well as the final conclusions (Subsection
4.1) drawn from the research.

2. Materials and Methods
2.1. Datasets
2.1.1. General COVID lung dataset
This first dataset was formed from public available datasets [28, 29]. The
dataset contains images with varying resolutions, ranging from 5600 × 4700
pixels to 156 × 156 pixels including chest, lateral X-rays and CT images. For
our purpose we discarded the latter two types. This was done because the
portable devices of the consulted healthcare services were used only for chest
X-rays. The dataset was labeled online in collaboration with different experts
through the Darwin platform [30] and is composed of 6,302 chest radiographs,
from which 438 correspond to patients diagnosed with COVID-19, 4,262 with
lung pathologies similar to COVID-19 and 1,602 belonging to patients who (in
principle) do not suffer from any of the previously mentioned conditions (albeit
they can be affected by other pathologies).
6

2.1.2. CHUAC dataset (portable devices)
The second dataset was provided by the Radiology Service of the Complexo
Hospitalario Universitario de A Coruña (CHUAC) obtained from three portable
X-ray devices in a real diagnostic scenario: an Agfa dr100E GE, an Optima
Rx200 and a Siemens Ysio Max. All the images were obtained after triage in
medical wings specially dedicated for the treatment and monitoring of patients
suspected of being afflicted by COVID-19. These images were captured during
clinical practice and emergency healthcare services in the height of the pandemic.
This dataset contains 200 images of patients diagnosed with COVID-19, 200
images of patients with lung affections similar to (but not caused by) COVID-19
and 200 patients with, in principle, no pulmonary afflictions but that may be
affected by other diseases, for a total of 600 images. We want to remark the
complexity of the presented pathological scenario (in addition to the complexity
of the portable X-ray images).
2.2. Software resources
Regarding the software resources, we have used Python 3.7.9 with Pytorch
1.6.0 [31] and Scikit-Learn 0.23.2 [32]. Additionally, we used a pre-trained model
from the work of Buda et al. [33, 34], trained with images from 110 patients
for a total of 7858 images [35]. This network is an implementation of an U-Net
[36] dedicated to the identification of brain structures in magnetic resonance
imaging or MRI [33]. Specifically, the original network has been trained to
detect gliomas, a type of brain tumor diagnosed mainly by this imaging modality
[37, 33], problematic that share similar characteristics to our case, which is
herein exploited. Thanks to the use of this model trained with a large number of
images from a similar domain, we are able to compensate for the limited number
of chest radiographs obtained from portable devices available to the method.
2.3. Methodology
To successfully develop a system able to work with radiographs from portable
devices with a limited amount available from the saturated health services, we

7

General COVID lung dataset

Transfer learning

Portable X-ray dataset

Lung Segmentation model

MRI segmentation model
Transfer learning

Portable X-ray model

Figure 2: Diagram of the fully automatic methodology to obtain a model able to segment lung
regions in radiographs from portable devices.

followed a workflow that allowed us to progressively adapt information from a
different medical imaging domain and pathology to ours. The main workflow
followed in this work can be seen in the Figure 2.
As can be seen in this figure, the proposed fully automatic methodology
was divided into two main stages of transfer learning. A first transfer learning
stage to adapt the filters developed in the network for the MRI domain to chest
radiography and a second one to further refine these weights specifically into
the sensibly harder radiographs from portable devices. In the following sections
each stage will be explained in more detail.
2.4. Inter domain knowledge transfer stage: MRI to common chest X-ray
For this first step, we started from a model previously trained in a medical
imaging domain with a large and heterogeneous set of images that presents
similar characteristics to those we would find in the target domain (from which
we have available a limited number of samples). In our case, we used the
U-Net trained with MRI images for glioma segmentation. As can be seen in
Figure 3, these tumors look very similar to normal brain tissues. The same
way, the pulmonary regions of interest can be found superimposed with the

8

Figure 3: Example MRI images used to train the original model from which we will apply
transfer learning to our domain. First row shows the original MRI images, and the second row
the output of the pretrained model (glioma regions)

rest of the tissues that the X-rays have had to pass through (due to the way
the capture method itself works) or even mixed with pathological structures
resulting from different pulmonary lesions caused by the considered pathologies.
For this reason, the knowledge transfer was direct. This was not only because of
the similarity of characteristics of both image domains, but also because of the
similar complications present in both image domains and pathologies. Also, it
has successfully proven its worth and robustness on its medical imaging domain.
These factors made it an ideal candidate network to be the “knowledge donor”
for our purpose.
This way, initially, we carried out a knowledge transfer stage by continuing
the training of the network trained with a complete dataset of MRI images with
general images of the domain to which we want to direct our work. Specifically,
in this case, we have opted for the aforementioned public dataset. This dataset
contains numerous radiographs obtained from different hospital and medical
centers around the world (and, therefore, from a wide range of X-ray devices).

9

2.5. Inter device type knowledge transfer stage: common to X-ray images from
portable devices
Once we had successfully trained a model to identify and segment lung regions
in general chest X-ray images from patients with COVID-19, lung afflictions
with similar characteristics or normal patients; we carried out the second stage
of transfer learning. That is, we took advantage of the general patterns that the
system has learned when looking for the pulmonary regions and we challenged
them with images taken in adverse conditions to further refine the segmentations
generated by the network. Consequently, when this second transfer learning
stage was completed, we obtained a model specifically trained to search for
pulmonary regions in the adverse conditions defined by the general dataset and
our unfavorable dataset composed by radiographs taken with portable devices
in adverse conditions
In this stage, we also divided the dataset of 600 chest radiographs from
portable X-ray devices obtained during clinical practice in the CHUAC into two
datasets of 300 samples. This was done to use only one of the dataset halves to
perform the knowledge transfer, and the other to evaluate the performance and
improvement of the system before and after this stage.
2.6. Training details
In order to maintain consistency and allow for proper transfer learning, we
have employed the same loss function used in the model trained with the brain
MRI images for the subsequent transfer learning stages. Therefore, both models
have been trained using the Smooth Dice Loss (Equation 1).

SmoothDiceLoss = 1 − 2

|Op ∩ Ot| + λ
|Op| + |Ot| + λ

(1)

Where Op represents the predicted system output and Ot the expected output
(target). λ is the smoothing factor, which has been defined as 1 in this work.
As optimizer, we have used adaptive moment estimation (ADAM) [38], with a
learning rate of 0.001 that is adjusted dynamically according to the necessities

10

and training progression of the model. Finally, for the calculation of the number
of training epochs we have used an early stopping strategy. That is, the algorithm
will automatically stop when the model is not able to improve its performance.
Specifically, the system evaluated the validation loss and had a patience of
20 epochs without obtaining any improvement. As for the distribution of the
dataset, 60% of the samples have been used for the training of the model, 20%
for the validation, and the remaining 20% for the unbiased testing of the model.
Finally, as result of the training, the weights of the model of the network that
obtained the best result in validation were recovered. This training process was
repeated 25 times for a better evaluation of the training stages. Additionally, to
increase the effective amount of available images in the dataset and to further
improve the training, data augmentation techniques have been implemented.
Specifically, the images were randomly rotated random degrees between -10º and
+10º with a probability of 75% to simulate feasible postural variations of the
patient.
2.7. Evaluation
To evaluate the performance of our proposal in each stage, we analyzed the
results in a wide range of metrics that allowed us to study the performance of
each of the trained models from different points of view. To do so, we evaluated
its accuracy (ACC), area under the ROC curve (AUC), Dice coefficient (DICE),
Jaccard index (JACC), precision (PREC)1 , recall (RECA)1 , F1-Score (F1-SC)1 ,
sensitivity (SENS) and specificity (SPEC). In our case, and using as reference the
True Positives (TP), True Negatives (TN), False Positives (FP). False Negatives
(FN), Ot as the target pixel values and Op as the values predicted by the system
for a given image, these metrics are defined as follows:
TP + TN
TP + TN + FP + FN
P
(Ot × Op)
P
DICE = 2 × P
Ot + Op
P
(Ot × Op)
P
P
P
JACC =
( Ot + Op) − (Ot × Op)
ACC =

11

(2)
(3)
(4)

TP
TP + FP
TP
RECA1 =
TP + FN
P REC 1 =

F 1 − SC 1 = 2 ∗

(P REC × RECA)
(P REC + RECA)

TP
TP + FN
TN
SP EC =
TN + FP

SEN S =

(5)
(6)
(7)
(8)
(9)

Finally, AUC returns the probability that the analyzed model has of assigning
a higher value to a positive sample over a negative sample [39].

3. Results
In this section we will proceed to present the results that were obtained during
the development of this work, product of the previously presented methodology.
3.1. Evaluation of the inter domain knowledge transfer stage: MRI to common
chest X-ray
Now, we will proceed to present the results for the inter domain learning
stage, where we took advantage of a model that was trained with a large number
of images from a similar image domain (that allowed us to generate a robust
methodology despite the scarcity of images available from portable devices). On
this first stage, we adapted from this domain to common lung radiographs. The
average of all the repetitions for this training process can be seen in Figure 4,
and the mean test results of each of the chosen models in Table 1. In this Figure
4, we see that (on average) it does not need too many cycles to learn the patterns
of the new domain, thanks to the already mentioned transfer of knowledge from
similar modalities instead of starting the training from scratch.
1 PREC,

RECA and F1-SC are a macro average: they are calculated for both the positive

and negative classes and then averaged (thus, RECA and SENS display different values).

12

Validation
Training

Loss

0.4

0.2

0

10

20

30

40

50

60

70

Epoch
Figure 4: Training and validation loss for the 25 repetitions for the inter domain knowledge
transfer stage.

Table 1: Mean and standard deviation of test results for the 25 repetitions of the inter domain
knowledge transfer stage.
ACC

AUC

DICE

JACC

PREC

RECA

F1-SC

SENS

SPEC

Mean

0.8813

0.9702

0.9554

0.9156

0.9064

0.8267

0.8366

0.6703

0.9832

St. dev.

0.0855

0.0244

0.0250

0.0432

0.0573

0.1247

0.1315

0.2528

0.0132

As can be seen, thanks to the knowledge transfer stage we obtain a system
capable of successfully segmenting the pulmonary region of interest. The only
weak measurement is the one referring to the sensitivity of the model, with a
considerably high standard deviation as well. However, the specificity obtained
is considerably high, and with a very low deviation (which indicates consistency
throughout the repetitions). These two factors indicate that the model is overadjusting some of the detections. This is possibly due to the images showing
foreign bodies such as pacemakers or other such objects, as the dataset masks
(targets) have been corrected to try to estimate the complete lung surface even
if it is obscured by these objects.

13

3.2. Evaluation of the inter device type knowledge transfer stage: common to
portable devices
After this first inter domain transfer learning, we now present the results
of the inter device type transfer learning step. In this step, we used the model
adapted for a general chest X-ray and continued the training to adapt this model
to the final objective of this work: obtaining a robust system able to successfully
segment lung regions in images taken in adverse conditions with portable devices.
In Figure 5 we can see that, as in the inter domain learning stage, thanks to
the use of image domains with similar properties, in just a few cycles we obtained
the desired result. The graph can give the appearance of an slight over-training
tendency, but we have to take into account two things: the first, that what is
shown is the average of each epoch for 25 trainings, so the result shown is not
really the training of a single model that shows a given behavior but multiple
different behaviors averaged; the other is that we are dealing with a training that
employs early stopping with 20 epochs of patience. The latter indicates that
every model, in the same moment that they began to overtrain, automatically
stopped the training and we were left with the best previous model. Despite
the graph reaching more than 50 epochs, not all the models reached that many
steps (and the further we go, the less models are affecting said mean, reflected in
the standard deviation). Although the training decreases significantly compared
to the validation error, the chosen model will not really present this pattern of
training. Rather, what it indicates is that in early stages all models converge
because they are based on an already-adapted model to the domain.
Table 2: Mean and standard deviation of test results for the 25 repetitions of the of the inter
device type knowledge transfer stage.
ACC

AUC

DICE

JACC

PREC

RECA

F1-SC

SENS

SPEC

Mean

0.9773

0.9695

0.9436

0.8936

0.9656

0.9644

0.9641

0.9423

0.9864

St. dev.

0.0097

0.0243

0.0243

0.0419

0.0175

0.0228

0.0152

0.0500

0.0107

Finally, as can be seen in the test results of the chosen model in Table 2, the
system appears to return more balanced results across all the metrics. We can

14

Validation
Training

Loss

0.4

0.2

0

5

10

15

20

25

30

35

40

45

50

Epoch
Figure 5: Training and validation loss for the 25 repetitions for the inter device type knowledge
transfer stage.

see how the sensitivity of the system has sensibly improved and the system is
now more balanced. Now, we will proceed to evaluate both systems under an
unbiased dataset to better assess their differences and improvements.
3.2.1. Evaluation of improvement between both knowledge transfer stages
For this final test we used the 300 independent images from the portable
CHUAC dataset that we separated for further analysis. The results of these
tests can be seen detailed in Tables 3 and 4; where we present the results for the
test of the model before and after the second stage of transfer learning (where
the model is adapted to portable X-ray devices), respectively.
Complementarily, this improvement is better observed in the comparison
plots of Figures 6, 7 and 8. These graphs show that where the most noticeable
change has been in images that have some kind of pathology with effects similar
to COVID-19, improving by almost 0.02 points in Jaccard and DICE coefficients.
On the other hand, we also noticed a remarkable increase in the sensitivity of
the models, being this measurement critical in systems oriented to the medical
sciences and clinical practice and also highly increased after the inter device type
transfer learning stage into the portable X-ray image domain.

15

Table 3: Breakdown of the results from the model from the inter domain knowledge transfer
stage tested with the CHUAC dataset and by pathology.
COVID-19
ACC

AUC

DICE

JACC

PREC

RECA

F1-SC

SENS

SPEC

Mean

0.9570

0.9377

0.8936

0.8142

0.9481

0.9286

0.9348

0.8729

0.9844

St. dev.

0.0293

0.0318

0.0698

0.1046

0.0471

0.0372

0.0418

0.0745

0.0230

ACC

AUC

DICE

JACC

PREC

RECA

F1-SC

SENS

SPEC

Mean

0.9555

0.9326

0.8854

0.8014

0.9484

0.9220

0.9305

0.8576

0.9864

St. Dev.

0.0439

0.0288

0.0731

0.1073

0.0473

0.0493

0.0588

0.0973

0.0197

ACC

AUC

DICE

JACC

PREC

RECA

F1-SC

SENS

SPEC

Mean

0.9476

0.9228

0.8536

0.7551

0.9268

0.9145

0.9173

0.8536

0.9754

St. dev.

0.0294

0.0323

0.0928

0.1293

0.0584

0.0346

0.0468

0.0608

0.0286

Normal

Pathological

Table 4: Breakdown of the results from the model from the inter device type transfer learning
stage tested with the CHUAC dataset and by pathology.
COVID-19
ACC

AUC

DICE

JACC

PREC

RECA

F1-SC

SENS

SPEC

Mean

0.9761

0.9705

0.9447

0.8961

0.9653

0.9655

0.9644

0.9444

0.9867

St. dev.

0.0100

0.0204

0.0241

0.0411

0.0205

0.0193

0.0145

0.0443

0.0108

Mean

0.9801

0.9752

0.9528

0.9103

0.9724

0.9688

0.9701

0.9470

0.9906

St. dev.

0.0104

0.0158

0.0161

0.0288

0.0122

0.0171

0.0115

0.0373

0.0059

Mean

0.9769

0.9649

0.9414

0.8910

0.9674

0.9616

0.9637

0.9340

0.9891

St. dev.

0.0111

0.0334

0.0322

0.0532

0.0184

0.0256

0.0189

0.0525

0.0077

Normal

Pathological

4. Discussion
In Figure 9 we can see more closely examples of outputs generated by the
proposed method. There is a clear distinction between the behavior of the two
models. As we saw in the statistics, the model that was trained with a large
number of MRI images and then its knowledge exploited to improve the training
with the common lung radiographs tends to generate more adjusted and limited
segmentations. This is particularly noticeable in those images that present less

16

1

0.98 0.99
0.98

0.97

0.97

0.97

0.96

0.96

0.95

0.95

0.94

0.94

0.94
0.93

0.93

0.9

0.90

0.89

0.87

0.85
0.81

0.8
ACC AUC DICE JACC PREC RECA F1-SC SENS SPEC
Inter domain transfer learning

Inter device transfer learning

Figure 6: Comparison between the model trained with common chest radiographs and the
model adapted to portable devices for images from patients diagnosed with COVID-19.

1

0.99 0.99

0.98

0.98

0.97

0.96

0.95

0.97

0.97

0.95

0.95

0.93

0.93
0.92
0.91

0.9

0.89
0.86

0.80

0.8

ACC AUC DICE JACC PREC RECA F1-SC SENS SPEC
Inter domain transfer learning

Inter device transfer learning

Figure 7: Comparison between the model trained with common chest radiographs and the
model adapted to portable devices for images from patients without lung afflictions.

definition of the lung region, where the model would have to try to reconstruct
the shape of the lung region on its own based on its inherited knowledge of the
domain. On the other hand, the network that has been trained with the second
stage of transfer learning presents more defined detections and better coverage
of the lung regions.
17

1

0.99
0.98

0.98

0.97

0.96
0.95

0.96

0.96

0.94

0.93

0.93

0.92

0.91

0.9

0.92

0.89
0.85

0.85

0.8
0.76

ACC AUC DICE JACC PREC RECA F1-SC SENS SPEC
Inter domain transfer learning

Inter device transfer learning

Figure 8: Comparison between the model trained with common chest X-ray radiographs and
the model adapted to portable devices for images from patients diagnosed with lung afflictions
similar to COVID-19 (such as pneumonia) but not COVID-19 related.

However, the model resulting from the inter domain transfer learning stage
also presents some explicit unwanted artifacts: it creates bridges between the lung
regions and connected components isolated from the two lungs as can be seen in
Figure 10. In the same way, we see that in the final model from the inter device
type transfer learning step all these artifacts have completely disappeared. Thus,
we can see the reason behind the three phenomena observed in the comparison
of the experiments: the overall improvement of the segmentations, the increase
of the sensitivity and at the same time the reduction of the standard deviation of
the metrics (as their stability is significantly improved with the disappearance of
these unwanted artifacts from the inter domain transfer learning stage model).
4.1. Conclusions
In this work, we have proposed a system with the purpose of segmenting lung
regions in thoracic radiographs, especially aimed at those obtained by portable
X-ray devices in adverse conditions and with a limited supply of images. These
devices, which represent an alternative to fixed devices to reduce the risk of crosscontamination in the diagnosis of diseases such as COVID-19, are critical for
18

(a)

(b)

(c)

(d)

Figure 9: Sample images of the network output from the inter domain transfer learning stage
(red) and inter device learning stage (blue). (a): Normal, (b): COVID-19, (c) & (d): Non-covid
lung pathologies. First row: original image, Second row: comparison between models.

medical emergency situations, so methodologies aimed to help in the diagnostic
process that are functional with these images are critical. To solve the problem
of poor image quality due to the capture conditions and devices themselves, we
propose a fully automatic methodology based on two stages of transfer learning.
A first stage based on knowledge transfer from a domain similar to radiographs
and trained with a large number of images (ensuring its robustness) to common
chest radiographs obtained from different public sources and a second stage,
in which knowledge is refined to adapt it to specific radiographs of a dataset
obtained in adverse conditions in the clinical practice during the pandemic.
As we have shown in the metrics of the results and in the discussion, while
the first stage of transfer learning allows the system to acquire the knowledge
bases of the domain to generate an initial segmentation, the second stage of
knowledge transfer to the particular domain manages to refine satisfactorily the
obtained segmentations even with a limited set of samples. This second stage
19

(a)

(b)

(c)

Figure 10: Sample images with unwanted artifacts and formations from the inter domain
transfer learning stage (red) and that same output from the inter device type learning stage
(blue). (a): Normal, (b): COVID-19, (c): Non-covid lung pathologies. First row: original
image, Second row: comparison between models.

of transfer learning allows not only to better estimate the pulmonary region,
but also to eliminate various artifacts resulting from the lower sample quality
present in the images from portable devices.
Thus, as a final result of this work, we have successfully obtained a fully
automatic methodology based on deep methodologies, using a limited number
of images from portable devices and capable of working with these images in a
robust and consistent way, regardless of the image quality and capture conditions.
In this way, as future work, we are considering its application as an intermediate
stage for computer aided diagnostic systems that can help with the detection
and monitoring of those affected by COVID-19 during the pandemic.

20

Funding
This research was funded by Instituto de Salud Carlos III, Government of
Spain, DTS18/00136 research project; Ministerio de Ciencia e Innovación y Universidades, Government of Spain, RTI2018-095894-B-I00 research project, Ayudas para la formación de profesorado universitario (FPU), grant ref. FPU18/02271;
Ministerio de Ciencia e Innovación, Government of Spain through the research
project with reference PID2019-108435RB-I00; Consellerı́a de Cultura, Educación e Universidade, Xunta de Galicia, Grupos de Referencia Competitiva,
grant ref. ED431C 2020/24; CITIC, Centro de Investigación de Galicia ref.
ED431G 2019/01, receives financial support from Consellerı́a de Educación,
Universidade e Formación Profesional, Xunta de Galicia, through the ERDF
(80%) and Secretarı́a Xeral de Universidades (20%).

References
[1] J. H. Coronavirus Resource Center, Coronavirus 2019-ncov global cases by
johns hopkins csse, available from: https://gisanddata.maps.arcgis.c
om/apps/opsdashboard/index.html (9 2020).
[2] T. P. Velavan, C. G. Meyer, The COVID-19 epidemic, Tropical Medicine &
International Health 25 (3) (2020) 278–280. doi:10.1111/tmi.13383.
URL https://doi.org/10.1111/tmi.13383
[3] M. Gavriatopoulou, E. Korompoki, D. Fotiou, I. Ntanasis-Stathopoulos,
T. Psaltopoulou, E. Kastritis, E. Terpos, M. A. Dimopoulos, Organ-specific
manifestations of COVID-19 infection, Clinical and Experimental Medicine
(Jul. 2020). doi:10.1007/s10238-020-00648-x.
URL https://doi.org/10.1007/s10238-020-00648-x
[4] C. Lodigiani, G. Iapichino, L. Carenzo, M. Cecconi, P. Ferrazzi, T. Sebastian,
N. Kucher, J.-D. Studt, C. Sacco, B. Alexia, M. T. Sandri, S. Barco, Venous
and arterial thromboembolic complications in covid-19 patients admitted

21

to an academic hospital in milan, italy, Thrombosis Research 191 (2020) 9 –
14. doi:https://doi.org/10.1016/j.thromres.2020.04.024.
URL http://www.sciencedirect.com/science/article/pii/S0049384
820301407
[5] S. Zaim, J. H. Chong, V. Sankaranarayanan, A. Harky, COVID-19 and
multiorgan response, Current Problems in Cardiology 45 (8) (2020) 100618.
doi:10.1016/j.cpcardiol.2020.100618.
URL https://doi.org/10.1016/j.cpcardiol.2020.100618
[6] V. M. Corman, O. Landt, M. Kaiser, R. Molenkamp, A. Meijer, D. K. Chu,
T. Bleicker, S. Brünink, J. Schneider, M. L. Schmidt, D. G. Mulders, B. L.
Haagmans, B. van der Veer, S. van den Brink, L. Wijsman, G. Goderski,
J.-L. Romette, J. Ellis, M. Zambon, M. Peiris, H. Goossens, C. Reusken,
M. P. Koopmans, C. Drosten, Detection of 2019 novel coronavirus (2019nCoV) by real-time RT-PCR, Eurosurveillance 25 (3) (Jan. 2020). doi:
10.2807/1560-7917.es.2020.25.3.2000045.
URL https://doi.org/10.2807/1560-7917.es.2020.25.3.2000045
[7] L. Lan, D. Xu, G. Ye, C. Xia, S. Wang, Y. Li, H. Xu, Positive RT-PCR
test results in patients recovered from COVID-19, JAMA 323 (15) (2020)
1502. doi:10.1001/jama.2020.2783.
URL https://doi.org/10.1001/jama.2020.2783
[8] R. Joarder, N. Crundwell, Chest X-ray in clinical practice, Springer Science
& Business Media, 2009.
[9] F. Shi, J. Wang, J. Shi, Z. Wu, Q. Wang, Z. Tang, K. He, Y. Shi, D. Shen,
Review of artificial intelligence techniques in imaging data acquisition,
segmentation and diagnosis for COVID-19, IEEE Reviews in Biomedical
Engineering (2020) 1–1doi:10.1109/rbme.2020.2987975.
URL https://doi.org/10.1109/rbme.2020.2987975
[10] A. Shoeibi, M. Khodatars, R. Alizadehsani, N. Ghassemi, M. Jafari,
P. Moridian, A. Khadem, D. Sadeghi, S. Hussain, A. Zare, et al., Automated
22

detection and forecasting of covid-19 using deep learning techniques: A
review, arXiv preprint arXiv:2007.10785 (2020).
[11] A. E. Hassanien, L. N. Mahdy, K. A. Ezzat, H. H. Elmousalami, H. A.
Ella, Automatic x-ray COVID-19 lung image classification system based
on multi-level thresholding and support vector machine (Apr. 2020). doi:
10.1101/2020.03.30.20047787.
URL https://doi.org/10.1101/2020.03.30.20047787
[12] X. Mei, H.-C. Lee, K. yue Diao, M. Huang, B. Lin, C. Liu, Z. Xie, Y. Ma,
P. M. Robson, M. Chung, A. Bernheim, V. Mani, C. Calcagno, K. Li,
S. Li, H. Shan, J. Lv, T. Zhao, J. Xia, Q. Long, S. Steinberger, A. Jacobi,
T. Deyer, M. Luksza, F. Liu, B. P. Little, Z. A. Fayad, Y. Yang, Artificial
intelligence–enabled rapid diagnosis of patients with COVID-19, Nature
Medicine 26 (8) (2020) 1224–1228. doi:10.1038/s41591-020-0931-3.
URL https://doi.org/10.1038/s41591-020-0931-3
[13] P. K. Sethy, S. K. Behera, Detection of coronavirus disease (COVID-19)
based on deep features (Mar. 2020). doi:10.20944/preprints202003.0
300.v1.
URL https://doi.org/10.20944/preprints202003.0300.v1
[14] M. Z. Islam, M. M. Islam, A. Asraf, A combined deep cnn-lstm network for
the detection of novel coronavirus (covid-19) using x-ray images, Informatics
in Medicine Unlocked 20 (2020) 100412. doi:https://doi.org/10.1016/
j.imu.2020.100412.
URL http://www.sciencedirect.com/science/article/pii/S2352914
820305621
[15] J. de Moura, J. Novo, M. Ortega, Fully automatic deep convolutional
approaches for the analysis of covid-19 using chest x-ray images (May 2020).
doi:10.1101/2020.05.01.20087254.
URL https://doi.org/10.1101/2020.05.01.20087254

23

[16] T. Ozturk, M. Talo, E. A. Yildirim, U. B. Baloglu, O. Yildirim, U. R.
Acharya, Automated detection of COVID-19 cases using deep neural networks with x-ray images, Computers in Biology and Medicine 121 (2020)
103792. doi:10.1016/j.compbiomed.2020.103792.
URL https://doi.org/10.1016/j.compbiomed.2020.103792
[17] J. de Moura, L. Ramos, P. L. Vidal, M. Cruz, L. Abelairas, E. Castro,
J. Novo, M. Ortega, Deep convolutional approaches for the analysis of
covid-19 using chest x-ray images from portable devices (Jun. 2020). doi:
10.1101/2020.06.18.20134593.
URL https://doi.org/10.1101/2020.06.18.20134593
[18] X. Mo, W. Jian, Z. Su, M. Chen, H. Peng, P. Peng, C. Lei, R. Chen,
N. Zhong, S. Li, Abnormal pulmonary function in covid-19 patients at time
of hospital discharge, European Respiratory Journal 55 (6) (2020). arXiv:
https://erj.ersjournals.com/content/55/6/2001217.full.pdf,
doi:10.1183/13993003.01217-2020.
URL https://erj.ersjournals.com/content/55/6/2001217
[19] Q. Yan, B. Wang, D. Gong, C. Luo, W. Zhao, J. Shen, Q. Shi, S. Jin,
L. Zhang, Z. You, Covid-19 chest ct image segmentation–a deep convolutional neural network solution, arXiv preprint arXiv:2004.10987 (2020).
[20] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, L. Shao,
Inf-net: Automatic COVID-19 lung infection segmentation from CT images,
IEEE Transactions on Medical Imaging 39 (8) (2020) 2626–2637. doi:
10.1109/tmi.2020.2996645.
URL https://doi.org/10.1109/tmi.2020.2996645
[21] X. Chen, L. Yao, Y. Zhang, Residual attention u-net for automated multiclass segmentation of covid-19 chest ct images (2020). arXiv:2004.05645.
[22] M. Z. Alom, M. M. S. Rahman, M. S. Nasrin, T. M. Taha, V. K. Asari,
Covid mtnet: Covid-19 detection with multi-task deep learning approaches
(2020). arXiv:2004.03747.
24

[23] J. Zhang, Y. Xie, Z. Liao, G. Pang, J. Verjans, W. Li, Z. Sun, J. He, Y. Li,
C. Shen, Y. Xia, Viral pneumonia screening on chest x-ray images using
confidence-aware anomaly detection (2020). arXiv:2003.12338.
[24] F. Shan, Y. Gao, J. Wang, W. Shi, N. Shi, M. Han, Z. Xue, D. Shen, Y. Shi,
Lung infection quantification of covid-19 in ct images with deep learning
(2020). arXiv:2003.04655.
[25] American college of radiology, acr recommendations for the use of chest
radiography and computed tomography (ct) for suspected covid-19 infection,
accessed: Oct, 20, 2020.
URL https://www.acr.org/Advocacy-and-Economics/ACR-PositionStatements/Recommendations-for-Chest-Radiography-and-CT-forSuspected-COVID19-Infection
[26] A. Jacobi, M. Chung, A. Bernheim, C. Eber, Portable chest x-ray in
coronavirus disease-19 (COVID-19): A pictorial review, Clinical Imaging 64
(2020) 35–42. doi:10.1016/j.clinimag.2020.04.001.
URL https://doi.org/10.1016/j.clinimag.2020.04.001
[27] H. Y. F. Wong, H. Y. S. Lam, A. H.-T. Fong, S. T. Leung, T. W.-Y.
Chin, C. S. Y. Lo, M. M.-S. Lui, J. C. Y. Lee, K. W.-H. Chiu, T. W.-H.
Chung, E. Y. P. Lee, E. Y. F. Wan, I. F. N. Hung, T. P. W. Lam, M. D.
Kuo, M.-Y. Ng, Frequency and distribution of chest radiographic findings
in patients positive for COVID-19, Radiology 296 (2) (2020) E72–E78.
doi:10.1148/radiol.2020201160.
URL https://doi.org/10.1148/radiol.2020201160
[28] J. P. Cohen, P. Morrison, L. Dao, K. Roth, T. Q. Duong, M. Ghassemi,
Covid-19 image data collection: Prospective predictions are the future,
arXiv 2006.11988 (2020).
URL https://github.com/ieee8023/covid-chestxray-dataset
[29] D. Kermany, Labeled optical coherence tomography (oct) and chest x-ray

25

images for classification (2018). doi:10.17632/RSCBJBR9SJ.2.
URL https://data.mendeley.com/datasets/rscbjbr9sj/2
[30] Labeled COVID-19 Chest X-Ray Dataset, Darwin, V7Labs, available at: ht
tps://darwin.v7labs.com/v7-labs/covid-19-chest-x-ray-dataset.
[31] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai,
S. Chintala, Pytorch: An imperative style, high-performance deep learning
library, in: H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,
E. Fox, R. Garnett (Eds.), Advances in Neural Information Processing
Systems 32, Curran Associates, Inc., 2019, pp. 8024–8035.
URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative
-style-high-performance-deep-learning-library.pdf
[32] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine
learning in Python, Journal of Machine Learning Research 12 (2011) 2825–
2830.
[33] M. Buda, A. Saha, M. A. Mazurowski, Association of genomic subtypes of
lower-grade gliomas with shape features automatically extracted by a deep
learning algorithm, Computers in Biology and Medicine 109 (2019) 218–225.
doi:10.1016/j.compbiomed.2019.05.002.
URL https://doi.org/10.1016/j.compbiomed.2019.05.002
[34] U-Net for brain MRI, Pytorch Hub, available at: https://pytorch.org/
hub/mateuszbuda brain-segmentation-pytorch unet/.
[35] Brain MRI images together with manual FLAIR abnormality segmentation
masks obtained from The Cancer Imaging Archive (TCIA), available at:
https://www.kaggle.com/mateuszbuda/lgg-mri-segmentation.

26

[36] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for
biomedical image segmentation, in: Lecture Notes in Computer Science,
Springer International Publishing, 2015, pp. 234–241. doi:10.1007/9783-319-24574-4 28.
URL https://doi.org/10.1007/978-3-319-24574-4 28
[37] D. A. Forst, B. V. Nahed, J. S. Loeffler, T. T. Batchelor, Low-grade gliomas,
The Oncologist 19 (4) (2014) 403–413. doi:10.1634/theoncologist.20
13-0345.
URL https://doi.org/10.1634/theoncologist.2013-0345
[38] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, arXiv
preprint arXiv:1412.6980 (2014).
[39] A. P. Bradley, The use of the area under the ROC curve in the evaluation of
machine learning algorithms, Pattern Recognition 30 (7) (1997) 1145–1159.
doi:10.1016/s0031-3203(96)00142-2.
URL https://doi.org/10.1016/s0031-3203(96)00142-2

27

