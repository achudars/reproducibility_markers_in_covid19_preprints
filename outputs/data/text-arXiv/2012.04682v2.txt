Transformer Query-Target Knowledge Discovery (TEND): Drug
Discovery from CORD-19
Leo K. Tam
NVIDIA
2788 San Tomas Expy
Santa Clara, CA. 95051

arXiv:2012.04682v2 [cs.CL] 11 Dec 2020

Abstract

Xiaosong Wang
NVIDIA
2788 San Tomas Expy
Santa Clara, CA. 95051

Daguang Xu
NVIDIA
2788 San Tomas Expy
Santa Clara, CA. 95051

through interrogation with metrics such as cosine similarity. Advances in transformer attentionPrevious work established skip-gram
based architectures treat one possible weakness in
word2vec models could be used to mine
word
embedding learning approaches by conditionknowledge in the materials science literature
ing
on
contextual information for a downstream
for the discovery of thermoelectrics. Recent
task (Vaswani et al., 2017). The pretrain-finetune
transformer architectures have shown great
progress in language modeling and associated
paradigm, whereby a language model is trained
fine-tuned tasks, but they have yet to be
on a large corpus through a task such as masked
adapted for drug discovery. We present a
cloze or autoregressive variants and then trained
RoBERTa transformer-based method that
again (fine-tuned) for a specific application encounextends the masked language token prediction
tered success (Dai and Le, 2015; Collobert and
using query-target conditioning to treat the
Weston,
2008). While word2vec implementations
specificity challenge. The transformer discovfocused on analogies evaluation as a downstream
ery method entails several benefits over the
word2vec method including domain-specific
task indicative of semantic learning, transformer
(antiviral) analogy performance, negation
architectures surveyed a collection of downstream
handling, and flexible query analysis (specific)
tasks such as sentiment, sentence similarity, natural
and is demonstrated on influenza drug dislanguage (NL) inference, question and answering,
covery. To stimulate COVID-19 research, we
and reading comprehension, etc. present in the
release an influenza clinical trials and antiviral
GLUE and super GLUE benchmark (Wang et al.,
analogies dataset used in conjunction with the
2019b,a). Word2vec discovery methods (Tshitoyan
COVID-19 Open Research Dataset Challenge
et al., 2019) constitute a return to analogy evalua(CORD-19) literature dataset in the study. We
examine k-shot fine-tuning to improve the
tion, though eschewing generic semantic analogies
downstream analogies performance as well
to highlight analogies in materials science. It is
as to mine analogies for model explainability.
hypothesized that resolving analogies may form
Further, the query-target analysis is verified
the basis for higher reasoning such as advancing
in a forward chaining analysis against the
the limits of domain-specific research (Kuniyoshi
influenza drug clinical trials dataset, before
et al., 2020; Hansson et al., 2020) and measuring
adapted for COVID-19 drugs (combinations
scholastic achievement (Khot et al., 2017). For the
and side-effects) and on-going clinical trials.
In consideration of the present topic, we
case of applications in the medical domain (Lee
release the model, dataset, and code.1
et al., 2020), transformer pretrain-finetune performance was dependent on the quality and in-domain
1 Introduction
nature of source and target datasets.
The COVID-19 literature has experienced exponenSimilar to novel materials discovery, drug discovtial growth and analysis tools have arisen to digest
ery is an intensive and arduous process requiring
the literature (Brainard, 2020). To mine the litera- trial and error. Drug discovery is the problem of
ture, word embedding methods (Huang et al., 2012; allocating resources to numerous promising candiMikolov et al., 2013) operate in a high-dimensional
dates, and thus ranking promising candidates asspace where semantic relationships are exposed
sists the discovery process in a top-down fashion.
1
To date globally, only nine influenza drugs have
https://www.kaggle.com/leotam/
novel-drug-discovery-from-clinical-trials-on-dgx-2
received full approval for use (De Clercq and Li,

2016). Where the Tshitoyan et al. (2019) method
uses a rigid prediction method to mine associative
analogies for undiscovered materials, the advances
in tokenization (Sennrich et al., 2016) have relaxed
exact vocabulary registration though complicating
the method. The present work examines a querytarget (QT) token conditioning method, which extends the masked language modeling (MLM) inference. The QT method is used to rank prediction association with clinical trials efficacy treating a specificity problem when moving from a
fixed vocabulary to whole language tokenization.
Moreover, the RoBERTa-large model trained on
the CORD-19 literature dataset (29500 provided at
the time of work and now updated to over 200000
articles) can be enhanced for drug analogies evaluation via a k-shot method (Raffel et al., 2019; Brown
et al., 2020).

2

Related work

Previous work has examined drug and side-effect
relationships in a bipartite graph focusing on literature in a four year range (Jeong et al., 2020). While
Jeong et al. (2020) considered 169766 PubMed
abstracts across drugs,Hansson et al. (2020) considered solely type II diabetes related drugs, clustering them around heuristic expert topics. Scoring was conducted via co-mention weighting and
a five year look-back literature mention weighting. Both previous methods incorporate expert
information in a semi-supervised method, the first
through database registration for drug - side effect
pairs and the second through semantic concept selection. The present method is an unsupervised
method excepting for physician review for the auxiliary analogies task. Further, the method uses a
majority of full texts in a narrow focus (Kohlmeier
et al., 2020). Results from Alsentzer et al. (2019);
Lee et al. (2020) revealed how the detail and relevance of the dataset influences the final result. We
corroborate that domain-specific datasets improve
domain-specific analogy performance.

3

Methods

The overview of our method is presented in Fig. 1,
which depicts both training and inference modes.
During training, the MLM task is held without
modification from Liu et al. (2019), namely 13.5%
of tokens are targeted for replacement with 90%
replaced with <mask>, 10% corrupted with a
random token. The MLM task is chosen as it

Table 1: CORD-19 Dataset

Sources
CZI
PMC
bioRxiv
medRxiv
Total

Records
1236
27337
566
361
29500

License
Custom
Noncomm.
Commercial
Arxiv

Count
17102
2353
9118
927
29500

closely replaces the function in Tshitoyan et al.
(2019) for predicting a target word given the context around the word. The RoBERTa transformer
method is a pure application of MLM, removing
the next sentence prediction task while scaling to
longer sequences with dynamic masking. A crossentropy loss is used for prediction, the RoBERTa
50K byte-pair encoding tokenization is used, and
hyper-parameters are left at default settings from
Wolf et al. (2019). During training, the inputs are
the CORD-19 dataset described in sec. 3.1 dynamically masked ten times. The MLM training from
scratch runs across 16 NVIDIA V100 GPUs in
a single-node (DGX-2) configuration for 100000
steps over approximately 36 hours. The MLM prediction is used for analogy evaluation via the structure “A is to B as C is to <mask>” as suggested
by Raffel et al. (2019). The complete set of analogies is provided with the code. For the word2vec
implementation, the procedure followed Tshitoyan
et al. (2019), including vocabulary generation and
evaluation. The QT inference mode is discussed in
Sec. 3.2.
3.1

Datasets

The CORD-19 dataset is the largest machine readable full text literature COVID-19 dataset curated
by Kohlmeier et al. (2020). Some statistics such
as records sourcing and license split on CORD-19
are presented in Tab. 1. The quality of the dataset
may be attributed to the full text access on a narrow focus that was not available in previous medical fine-tuning studies (Alsentzer et al., 2019; Lee
et al., 2020; Tshitoyan et al., 2019). On disk as raw
text, the dataset is 875 MB with 20% of the dataset
reserved for testing during language modeling.
The United States Food and Drug Administration (FDA) approved drugs and global clinical trials data are drawn from De Clercq and Li (2016)
and USGov (2020) respectively. In Tab. 2, counts
such as the number of trials and drugs trialed per
year’s end is collected. Namely, we use influenza

Figure 1: A RoBERTa-large transformer query-target method for drug discovery reveals positive and negative
associations

Figure 2: The black line represents influenza drugs receiving FDA approval. To date, there are only eight antiviral drugs approved for influenza strains globally with a ninth drug, remdesivir receiving emergency approval
subsequent to the analyses performed here.

Table 2: TEND Clinical Trials Dataset

Years End
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019

Records
17
41
74
112
157
199
244
275
313
348
382
411
435
463
659

Drugs
16
39
69
107
152
194
237
268
306
341
375
371
394
419
621

Addl. Diseases
4
8
18
30
45
66
85
102
114
128
142
157
170
190
328

as the condition, check the search term to filter
by drug treatments, and focus on years prior to
2016, when the last antiviral drug was approved.
De-duplication is performed on trade and scientific names using the chart from De Clercq and Li
(2016). The number of candidate and approved
drugs specifically for influenza per year is plotted
on Fig. 2.
For analogy evaluation, the set of language
(grammar) analogies and drug analogies are drawn
from relationships in Tshitoyan et al. (2019) and
De Clercq and Li (2016) respectively. A list of
the analogy categories is presented in Tab. 3. For
k-shot training, a random set of k=5 analogies from
each category is used as additional pretraining for
MLM (Brown et al., 2020).
3.2

Query-target inference

During inference, a query and target phrase are selected based on the relationships of interest. We
examine the relationship with the RoBERTa training objective, adopting the formulation from Yang
et al. (2019) for the objective as:

Hθ is the RoBERTa-large architecture with parameters θ that maps a S length text sequence into a
sequence of hidden vectors. Optimizing the training objective results in accurate MLM inference.
For MLM inference (Devlin et al., 2019), the K
masked tokens in the query q = [q1 , · · · , qK ] are
targeted for token prediction, i.e.

exp Hθk (qk )

.
(2)
Pk = P
exp Hθj (qk )
j

For QT prediction, we condition the masked token
targets on the query targets y = [y1 , · · · , yL ]:

exp Hθl (qk )


R := Pk (qk |qk ∈ y) = Pl
exp Hθj (qk )
P

(3)

j

follows by independence assumption in Eqn. 1 and
therefore is contained in the training objective (i.e.
accurate QT prediction is implied). When q = y,
we observe the QT conditioning decomposes to
the MLM task prediction. The QT conditioning
is more focused than reformulating a span prediction method such as in Devlin et al. (2019) due
to rejection of extraneous tokens that would be
admitted in a dot product formulation. Once QT
prediction has been formed, the analogy MLM task
may be permuted with the QT terms using “Q is to
T as Q is to <mask>” to analyze the top-k related
terms without conditioning. For rank prediction,
tokens with positive and negative associations are
not intentionally mixed as they are for visualization
purposes.
3.3

Attention visualization

Typically transformer attention visualization examines the sequence to sequence attention (Vaswani
et al., 2017; Huang et al., 2019), namely plotting
the per-token attention:

exp Hθt (xt )| e(xt )

.
At = P
(4)
j
|
j exp Hθ (xt ) e(xt )


exp Hθt (x̂)| e(xt )
 For QT visualization, the token attention per l-th
max log pθ (x̄|x̂) ≈
δt log P
θ
exp Hθt (x̂)| e(x0 )
t
query targets, namely:
x0
(1)

exp Hθl (qk )e(qk )
where δt is 1 if t indicates a masked token and 0

.
Rl = P
(5)
otherwise, x = [x1 , · · · , xS ] is a text sequence, x̂
exp Hθj (qk )e(qk )
j
represents a corrupted token, x̄ represents a masked
token, e(x) is the embedding of the sequence, and
X

Table 3: Analogy Semantic Learning Evaluation

Category
drug – inhibition
drug – group
drug – abbreviation
drug – approved target
opposites
comparatives
superlatives
present participles
past tense
plural
plural verbs

Number
211
57
57
73
703
651
651
4031
4031
4169
993

Subcategory
antiviral
antiviral
antiviral
antiviral
grammar
grammar
grammar
grammar
grammar
grammar
grammar

Figure 3: Self attention visualization (top) and query-target function (bottom left) show associated (light) and
unassociated (dark) values. A passage from De Clercq and Li (2016) is highlighted on a per-sentence basis using
the target term “efficacy”.

3.4

Forward chaining (FC) analysis

To preserve the casual nature of time series data, the
rank calculation from Eqn. 3 is performed on the
year-limited data in Tab. 3. The target query is set
as “clinical trials efficacy” and the candidate drugs
are drawn from the number specified in column 2
of Tab. 2. The candidate drugs are a subset of the
total drugs tested as trials cover additional diseases
(column 4, Tab. 2).

4

Results

After training for 100000 steps, the MLM task
reached a perplexity of 2.4696 on the held-out test
data. The attention relationship for self-sequence
to sequence and QT is visualized in Fig. 3 as
per eqns. 3 and 4. While the QT scoring may
be adapted to sentence highlighting (Fig. 3), a
comparison with the span extraction or abstractive summarization method in Devlin et al. (2019)
is beyond the scope of the current work. While
negation handling (Fig. 3) is an expected result
Devlin et al. (2019); Wang et al. (2019a), it represents an advancement over the word2vec scoring method. Analogies evaluation is collected in
Tab. 4. Although a comparison on simple grammar
analogies can be conducted, a simple extension
cannot be performed as the 600000+ word2vec vocabulary built from standard procedures does not
adequately capture the phrases in the drug analogies. In the categories where RoBERTa-large can
be compared to word2vec (opposites, comparatives,
superlatives) significant improvement is observed
(83.0% accuracy vs 50.4% accuracy). The few-shot
and semi-supervised learning approaches are critical to performance, generating 23.8% and 18.8%
improvement in top-1 accuracy for grammar and
antiviral analogies respectively.
While synthetic analogies can be captured to
some degree by the CORD-19 RoBERTa-large
model, is the model relevant for forward predictions as in Tshitoyan et al. (2019)? Fig. 4 shows the
FC analysis for the period where clinical trials data
is reliably available. Below the FC figure, a ranking
of drugs under current clinical trials is presented.
Shortly after the analysis was issued ([anon URL]),
the antiviral remdesivir entered emergency FDA
approval, reflecting Fig. 4. As a possible failure
mode, hydroxychloroquine was ranked as a distant
third and was later shown to have no correlation
with positive or negative outcomes (Geleris et al.,
2020). In Fig. 4 (bottom), the permuted MLM task

mines relationships that mirror the relationship of
remdesivir with clinical trials efficacy. Inverting
the analogy mining operation (not pictured) does
not recover the QT function as predicted terms are
too generic to focus on candidate drugs. While
further experiments are collected on negative terms
(side effects) and drug combinations in Fig. 5, a
reliable method to test and verify these results has
not been collected.

5

Conclusion and perspectives

A transformer QT conditioning specifies the discovery method on a narrow literature dataset to predict
clinical trials approval as verified by FC, real-time
prediction, and relationship mining. The conditioning operation is a straight-forward calculation at
inference time for the transformer language model
permissible by the independence assumption during pretraining. For language models where independence is not assumed, such as the permutation
language objective (Yang et al., 2019), conditioning
would be performed via estimation of the posterior
distribution, i.e. via a Metropolis-Hastings algorithm. The ranking task can be used to determine a
per-sentence passage highlighting (Fig. 3) with a
specific query. The scope of the QT method can be
given since q, y ∈ X is in the set of all statements
in the corpus, and only finite sets could be generated (though by motivation this is unwieldy). For
more validation, the field of online learning may
offer independent verification through the marginal
contribution to accuracy of each datum (Jia et al.,
2019).
Besides the accessible resource of clinical drug
trials, other quantitative methods of determining
drug function are feasible given detailed dataset
formulation. Such methods could focus on canonical measures such as the inhibitory constant (Ki ),
effective dose at 95% (ED95), or number needed
to treat (NNT). Still further are works examining
protein receptor binding, but the connection to literature machine learning methods is unclear as well
requiring specialized dataset expertise. Due to the
relatively limited number of successes for antiviral
drugs, analysis suffers from sample bias. Further
comparison on the materials dataset was not possible due to unavailability of the dataset after request.
Despite limitations, the study suggests transformer language models are a flexible tool in mining literature.

Table 4: Analogies accuracy using language models trained on various corpora. *The word2vec cannot adequately
adapt to the phrases used in drug analogies at 600000+ vocabulary with standard procedures.

Model Top-5 Accuracy
RoBERTa-large
k-shot (k=5) RoBERTa-large
CORD-19 RoBERTa-large
k-shot (k=5) CORD-19 RoBERTa-large

Grammar
0.241
0.925
0.727
0.705

Antivirals
0.365
0.538
0.525
0.579

Model Top-1 Accuracy
RoBERTa-large
k-shot (k=5) RoBERTa-large
CORD-19 RoBERTa-large
k-shot (k=5) CORD-19 RoBERTa-large
word2vec skip-gram
COVID word2vec skip-gram

Grammar
0.082
0.830
0.428
0.572
0.504
0.592

Antivirals
0.124
0.312
0.190
0.396
*
*

Figure 4: (Left) A year-limited FC ranking analysis of influenza drugs under clinical trials for FDA approval. Only
two drugs received approval in the period between 2005 and 2016. (Right) Ranking of current clinical trials for
COVID-19 drugs. (Bottom) The permuted MLM task probes the relationship of remdesivir with clinical trials
efficacy.

Figure 5: (Top) Examination of side effects using the transformer query-target method. (Bottom) Drug combinations are evaluated via concatenation.

References
Emily Alsentzer, John R. Murphy, Willie Boag, WeiHung Weng, Di Jin, Tristan Naumann, and Matthew
B. A. McDermott. 2019. Publicly available clinical
BERT embeddings. CoRR, abs/1904.03323.
Jeffrey Brainard. 2020. New tools aim to tame pandemic paper tsunami. Science, 368(6494):924–925.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, volume 307 of ACM International Conference Proceeding Series, pages 160–
167. ACM.
Andrew M. Dai and Quoc V. Le. 2015.
Semisupervised sequence learning. In Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec,
Canada, pages 3079–3087.
Erik De Clercq and Guangdi Li. 2016. Approved antiviral drugs over the past 50 years. Clin Microbiol
Rev, 29(3):695–747.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2019, Minneapolis, MN,
USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186. Association for Computational Linguistics.
Joshua Geleris, Yifei Sun, Jonathan Platt, Jason Zucker,
Matthew Baldwin, George Hripcsak, Angelena Labella, Daniel K Manson, Christine Kubin, R Graham Barr, Magdalena E Sobieszczyk, and Neil W
Schluger. 2020. Observational study of hydroxychloroquine in hospitalized patients with covid-19.
N Engl J Med, 382(25):2411–2418.
Lena K Hansson, Rasmus Borup Hansen, Sune
Pletscher-Frankild, Rudolfs Berzins, Daniel Hvidberg Hansen, Dennis Madsen, Sten B Christensen,

Malene Revsbech Christiansen, Ulrika Boulund, Xenia Asbæk Wolf, Sonny Kim Kjærulff, Martijn
van de Bunt, Søren Tulin, Thomas Skøt Jensen, Rasmus Wernersson, and Jan Nygaard Jensen. 2020. Semantic text mining in early drug discovery for type
2 diabetes. PLoS One, 15(6):e0233956.
Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings
of the Conference, July 8-14, 2012, Jeju Island, Korea - Volume 1: Long Papers, pages 873–882. The
Association for Computer Linguistics.
Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019.
Clinicalbert: Modeling clinical
notes and predicting hospital readmission. CoRR,
abs/1904.05342.
Yoo Kyung Jeong, Qing Xie, Erjia Yan, and Min Song.
2020. Examining drug and side effect relation using author–entity pair bipartite networks. Journal of
Informetrics, 14(1):100999.
Ruoxi Jia, David Dao, Boxin Wang, Frances Ann
Hubis, Nick Hynes, Nezihe Merve Gürel, Bo Li,
Ce Zhang, Dawn Song, and Costas J. Spanos. 2019.
Towards efficient data valuation based on the shapley
value. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 1618 April 2019, Naha, Okinawa, Japan, volume 89 of
Proceedings of Machine Learning Research, pages
1167–1176. PMLR.
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2017.
Answering complex questions using open information extraction. In Proceedings of the 55th Annual
Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 August 4, Volume 2: Short Papers, pages 311–316.
Association for Computational Linguistics.
Sebastian Kohlmeier, Kyle Lo, Lucy Lu Wang, and
JJ Yang. 2020. Covid-19 open research dataset
(cord-19).
Fusataka Kuniyoshi, Kohei Makino, Jun Ozawa, and
Makoto Miwa. 2020. Annotating and extracting synthesis process of all-solid-state batteries from scientific literature. In Proceedings of The 12th Language
Resources and Evaluation Conference, LREC 2020,
Marseille, France, May 11-16, 2020, pages 1941–
1950. European Language Resources Association.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2020. Biobert: a pre-trained
biomedical language representation model for
biomedical text mining. Bioinform., 36(4):1234–
1240.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,

Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word representations in vector space. In 1st International Conference on Learning Representations, ICLR 2013,
Scottsdale, Arizona, USA, May 2-4, 2013, Workshop
Track Proceedings.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for
Computer Linguistics.
Vahe Tshitoyan, John Dagdelen, Leigh Weston,
Alexander Dunn, Ziqin Rong, Olga Kononova,
Kristin A Persson, Gerbrand Ceder, and Anubhav
Jain. 2019. Unsupervised word embeddings capture
latent knowledge from materials science literature.
Nature, 571(7763):95–98.
USGov. 2020. Clinicaltrials.gov.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5998–6008.
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R. Bowman. 2019a. Superglue:
A stickier benchmark for general-purpose language
understanding systems. In Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019,
NeurIPS 2019, 8-14 December 2019, Vancouver, BC,
Canada, pages 3261–3275.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019b.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
2019. OpenReview.net.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019.
Xlnet: Generalized autoregressive pretraining for
language understanding. In Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information Processing Systems
2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 5754–5764.

