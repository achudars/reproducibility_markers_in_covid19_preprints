arXiv:2009.01657v1 [eess.IV] 27 Aug 2020

A FREE WEB SERVICE FOR FAST COVID-19 CLASSIFICATION OF
CHEST X-R AY IMAGES

Jose David Bermudez Castro
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
bermudez@ele.puc-rio.br

Ricardo Rei
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
ricardo.rei@puc-rio.br

Jose Ruiz
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
joseruiz@puc-rio.br

Pedro Achanccaray Diaz
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
pachanccarayd@uni.pe

Smith Arauco Canchumuni
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
saraucoc@uni.pe

Cristian Muñoz Villalobos
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
crismunoz@puc-rio.br

Felipe Borges Coelho
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
borges@puc-rio.br

Leonardo Forero Mendoza
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
mendonza@ele.puc-rio.br

Marco Aurelio C. Pacheco
Department of Electrical Engineering (DEE)
Pontifical Catholic University of Rio de Janeiro
Rio de Janeiro, Brasil
marco@ele.puc-rio.br

September 4, 2020

A BSTRACT
The coronavirus outbreak became a major concern for society worldwide. Technological innovation
and ingenuity are essential to fight COVID-19 pandemic and bring us one step closer to overcome it.
Researchers over the world are working actively to find available alternatives in different fields,
such as the Healthcare System, pharmaceutic, health prevention, among others.With the rise of
artificial intelligence (AI) in the last 10 years, IA-based applications have become the prevalent
solution in different areas because of its higher capability, being now adopted to help combat against
COVID-19.This work provides a fast detection system for COVID-19 using X-Ray images based
on deep learning (DL) techniques. This system is available as a free web deployed service for fast
patient classification, alleviating the high demand for standards method for COVID-19 diagnosis. It is
constituted of two deep learning models, one to differentiate between X-Ray and non-X-Ray images
based on Mobile-Net architecture, and another one to identify chest X-Ray images with characteristics

A PREPRINT - S EPTEMBER 4, 2020

of COVID-19 based on the DenseNet architecture.For real-time inference, it is provided a pair of
dedicated GPUs, which reduce the computational time. The whole system can filter out non-chest
X-Ray images, and detect whether the X-Ray presents characteristics of COVID-19, highlighting the
most sensitive regions.
Keywords COVID-19 · Deployed System · Free Web Service · Deep Learning

1

Introduction

The coronavirus disease 2019 (COVID-19) began as an outbreak in Wuhan, China and was declared a public health
emergency of international concern by the World Health Organization (WHO) on January 30 [1]. COVID-19 has
become a pandemic with more than 8 million confirmed cases and almost 500K deaths around 216 countries [2]. There
have been many efforts to fight COVID-19 around the world such as research a vaccine against COVID-19, massive
creation of ventilators as well as personal protective equipment (PPE), development of fast diagnosis systems from
chest X-Ray and/or thoracic computer tomography (CT), among others in many different fields.
Artificial Intelligence (AI) can be applied in many different ways to fight COVID-19. As stated in [3], between the
most important applications of AI in COVID-19 pandemic are: early detection of the infection from medical imaging,
monitoring and prediction of the spread of the virus and mortality rates, tracing the virus by identifying hot spots, and
development of drugs and vaccines by helping in clinical trials. In São Paulo, Brazil, regional authorities in cooperation
with telephone companies are employing the Social Isolation Index (SII) [4], which is computed based on Geo-location
data, to quantify the percentage of population that stays at home. Landing AI [5] developed a monitoring tool that
issues an alert when anyone is less than the desired distance from another person, promoting social distancing in the
streets due to its effectiveness to slow down the COVID-19 spread. On the other hand, machine learning methods have
been applied to predict the number of active cases around the world [6], for fast COVID-19 detection from CT [7] and
X-Ray [8], and to develop systems for detection of those who are not wearing facial masks [9].
Currently, one of the most employed methods for COVID-19 diagnosis is the viral nucleic acid detection using real-time
polymerase chain reaction (RT-PCR). However, testing this method in thousands of suspenseful patients it is very
delayed. To address this drawback, efforts have been made using AI to detect patients with COVID-19 in a very fast
time through medical imaging technologies like CT and X-Ray images. It is because, COVID-19 causes acute, highly
lethal pneumonia with clinical symptoms similar to those reported for SARS-CoV and MERS-CoV [10, 11, 12]. The
most common and widely available method using this type of technologies is by chest X-Ray images because it is
available in ambulatory care facilities, handy with portable X-Ray systems, and enabling rapid classification (speeding
up the disease screening). In contrast, CT scanners may not be available in many underdeveloped regions, disabling its
use for quick diagnoses. Despite the advantages to detect COVID-19 by X-Ray, one of the main bottlenecks is the need
of specialists (radiologists) to interpret the images. Taking this “specialists” expertise as a prior knowledge to build an
AI system can help them for quick triage, especially as a tool to triage patients for radiologists with case overloads.
Related Works. Several AI-based approaches have been proposed to detect patients with COVID-19 using X-Ray
images. Most of them use pre-trained models and data augmentation, mainly due to the lack of data. [8] create a free
repository 1 of X-Ray image data collections to train machine learning approaches for COVID-19 classification. In
addition, [13] published a survey on public medical imaging data resources (CT, X-Ray, MRT and others), which can
be used to increase the number of X-Ray samples. Reviews of known deep learning models used to create an automatic
detection of COVID-19 from X-Ray images are presented by [14], [15] and [16], which used VGG-19, MobileNetV2,
Inception, Xception and ResNetV2 as pre-trained models. They used a small number of X-Ray samples with confirmed
COVID-19 (224, 50 and 25 respectively). [17] collected more X-Ray samples to increase the training dataset and
use pre-trained models followed by highlighting class-discriminating regions using gradient-guided class activation
maps(Grad-CAM) [18] for better visualization.
In [19] was introduced a model called Decompose, Transfer and Compose (DeTracC) to classify between healthy,
severe acute respiratory syndrome (SARS) and COVID-19 in X-Ray images. This network was built in tree stages:
(i) features extraction, (ii) class decomposition, and (iii) classification. The authors used AlexNet and ResNet18, as
pre-trained models, and Principal Component Analysis (PCA), as a decomposition class. The training was performed
with 80 samples of normal X-Ray, 105 samples of COVID-19, and 11 samples of SARS, and data augmentation to
increase the dataset to 1764 was employed. In [20] was presented a new model based on capsule networks called
COVIDS-CAPS to classify between non-COVID-19 (normal, Bacterial and Viral) and COVID-19 using less number of
trainable parameters compared with pre-trained models. Their results show that COVIDS-CAPS is able to predict X-Ray
1

COVID-19 repository: https://github.com/ieee8023/

2

A PREPRINT - S EPTEMBER 4, 2020

with COVID-19. Despite the fact that the network was trained with few X-Ray samples, which poses uncertainties
regarding the network suitability for new samples.
Not only AI-based research was developed (several papers using deep learning to predict COVID-19 in X-Ray images),
also software and services have been created. The startup “DarwinIA” developed the COVID-Net [21], which can
help to identify COVID-19 patients using chest X-Ray radiography. Also, they made a free Kubernetes Bundle for
inferencing COVID-19 X-Rays images 2 . In addition, Seoul-based medical AI software company Lunit 3 released its
AI-powered software for chest X-Ray analysis with a limitation of 20 uses per day per user.
Contributions. In this context, this work presents a free and open source system available as a web service 4 for fast
detection of COVID-19 in chest X-Ray images using deep learning. This system employs deep learning models to detect
COVID-19 effects on lungs from chest X-Ray, which can be used for classification, triage, and monitor COVID-19
cases.
The main contributions of this work can be summarized as follows: First, two deep learning models are presented, one
to differentiate between X-Ray and non-X-Ray images based on Mobile-Net architecture, and another one to detect
chest X-Ray images with characteristics of COVID-19 using a network bases on dense blocks and initialized from a
pre-trained ImageNet. Second, we provide a free and open-source service with dedicated GPUs to make inferences in
real-time. The source code is available at https://github.com/ICA-PUC/ServiceIA-COVID-19.
The remainder of the paper divides into four sections. Section 2 describes the methodology adopted for each part of
the system. Section 3 presents the datasets employed to train the models for X-Ray images filter and the COVID-19
classifier. Section 4 shows the results obtained in our experiments for each model, and the web interface of the whole
system. Finally, Section 5 summarizes the conclusions and next steps to be followed in the development of our system.

2

Methodology

This section presents the methodologies adopted for each of three parts of the project, the X-Ray image filter, the
COVID-19 classifier and the Web page description. Figure 1 summarizes the workflow of the system.

Figure 1: Inference workflow for X-ray images. Each step refers to a single abstraction layer.
The first box in Figure 1, from left to right, illustrates the beginning of the detection process. It starts with a simple
extension verification to check whether the uploaded file is an image (web interface). Next, the Backend (second box)
sends and stores the input image to the GPU cluster (Server), which performs the network’s inferences after the Backend
verifies the correct uploading of the images to it. Finally, the last box shows how the web interface stores and displays
the result of the inference.
In the following, it is described in detail the methodology employed for each constituted part of the system.
2

https://www.weave.works/blog/firekube-covid-ml
Lunit : https://www.lunit.io/en/covid19/
4
http://www.iacontracovid.com.br/analise-de-imagens/

3

3

A PREPRINT - S EPTEMBER 4, 2020

CAM
Softmax

GA Pooling

Dense Block

Convolution

Avg Pooling

Convolution

Convolution

Dense Block

No Finding
Lung Opacity
COVID-19

x2
Figure 2: Description of the method followed in this work to classify X-Ray images for detection COVID-19. It is
made of three Densenet Blocks, followed by a linea classifier.
2.1

X-Ray Images Filter

Due to the input of our system to predict COVID-19 are X-Ray images from frontal AP and PA views, we first guarantee
that the inputs present these characteristics to have a correct assessment. Therefore, we implement a neural network
based on Mobile-Net architecture [22] to identify whether it is a valid image (pulmonary frontal X-Ray image) or
a non-valid image (rotated pulmonary X-Ray images, colored or natural images, for instance). In other words, this
network operates as a filter that passes to the next process, just frontal pulmonary X-ray images from valid views.
2.2

COVID-19 Classifier

Figure 2 illustrates the processing scheme followed in this work to classify COVID-19 X-Ray images. It receives as
input an X-Ray image and delivers as outputs a vector of scores indicating the probability of the image is not presenting
any issues (No Finding), with opacity in Lungs (Lung Opacity), or with characteristics of COVID-19. The core of this
system is a classifier built on the DenseNet [23] network with weights initialized from a pre-trained ImageNet [24]
model. We selected a DenseNet-based architecture due to it has demonstrated high capability for classifying X-Ray
images [25, 26, 27, 28, 29]. It is because the utilization of Dense blocks helps to improve the gradient flow information
through the network, allowing the optimization of deeper architectures [25].
As described in Figure 2, we use a DenseNet architecture made of three densely blocks connected in cascade, which are
separated by downsampling blocks constituted of one convolution, and an average pooling layer. Finally, to the output
of the last densely block, it is stacked a convolution followed by a global average pooling, and a softmax classification
layer, which performs the final decision of the network.
For optimizing the network parameters, we split this process into two stages due to the scarcity of COVID-19 samples.
At the first stage, we train the DenseNet to discern between No Finding and Lung Opacity images, and at the second
one, the model was finetuned incorporating the COVID-19 class information. During the first stage, COVID-19 samples
were considered as Lung Opacity because of existing correlation between COVID-19 and Pneumonia, and processed
independently in the second stage. By adopting this stratified training, we take advantage of the number of samples per
class in the first stage to guarantee the learning of representative features for identifying opacities in the Lungs. Then,
in the second stage, these features are finetuned to classify whether the opacity was originated by COVID-19 or not.
2.3

Web Page Description

The web page was created to display all the content of this research. Besides, health specialists may use the platform to
support patients’ classification. The web page is accessible worldwide, being available in two languages: Portuguese
(original) and English. It also describes in detail information about the datasets, structure, technical details, and
performance of classifiers. Also, there is an area on the web page where public users can experience the predicted
classification using personal X-Ray images. At the end of the page, it is possible to view a 3D projection based on PCA
analyses, which from a set of images samples, forms groups of images that most resemble each other.
Front-end and back-end interface - service architecture. Due to the web page was developed as a free service for
providing a simple solution to perform image analysis, and that can be utilized by any user, it was necessary to combine
different environments programming languages to make this possible. At follow, it is detailed the web site structure, as
well as the environments employed for building this service.
The web page interface (called front-end) was developed in HyperText Markup Language (HTML), Cascading Style
Sheets (CSS) and JavaScript (JS), which enabled its rapid implementation by using some libraries and frameworks
like: jQuery, that facilitated the implementation of JS code on the site; Dropzone5 , an open source library that provides
5

Dropzone: https://www.dropzonejs.com/

4

A PREPRINT - S EPTEMBER 4, 2020

drag-and-drop file uploads with image previews; and Bootstrap 4.06 , one of the world’s most popular HTML, CSS and
JS libraries to quickly customize responsive websites.
The implemented service that maintains the operation of the website, known as backend, was developed in Hypertext
Preprocessor (PHP), Python, and Shell Command Language (Sh). The Deep Learning neural network models were
implemented in Keras engine, using Tensorflow as backend. The server run over a Unix operating system, which
controls the running process via Bourne-again shell (Bash).

3

X-Ray Dataset

Public data, collected from different places, was used for our implementations.
3.1

X-Ray Images Filter

For filtering frontal X-Ray images, we selected three public groups of images:
• Frontal pulmonary X-Ray [30].
• Frontal non-pulmonary X-Ray 7 .
• Other kind of images that are not X-Ray images: PASCAL VOC 8 and Computational Vision at CALTECH
dataset 9 .
Taking into account that the performance of the classification system would be reduced if it process images inverted or
rotated (90◦ clockwise and counterclockwise), we created a group of images with these characteristics to be identified
as non-valid by the X-Ray filter. Particularly, we randomly selected a set of images from the X-Ray dataset and applied
rotations of 90◦ , 180◦ , and 270◦ to each image. Table 1 summarizes the distribution of the dataset used for the filter
network.
Class
Valid
Non-valid

# of samples
Train Validation Test
Frontal pulmonary X-Ray
4,400
300 300
Rotated frontal pulmonary X-Ray 4,800
100 100
Frontal non-pulmonary X-Ray
530
100 100
Not X-Ray
9,800
100 100
Table 1: Dataset distribution for the X-Ray Images Filter.
Data Type

Total
5,000
5,000
730
10,000

Next, we split the dataset randomly into three sets; for training, validating, and testing the model (see Table 1). Each set
contains the same among of samples for each class: valid and non-valid image.
3.2

COVID-19 Classifier

For detecting COVID-19 using X-Ray imagery, we selected four publicly available datasets, two associated with COVID19 and the others with opacities in Lungs caused by different pathologies, like pneumonia, infiltration, consolidation,
among others. Both COVID-19 datasets, Cohen [8] and Figure1-COVID 10 datasets, are community repositories
updated regularly with X-Ray imagery from patients from different countries, already diagnosed or suspected of having
COVID-19, and others pneumonia infections. At the date this paper was written, the Cohen dataset contained 438
X-Ray images from AP and PA views, 359 associated with COVID-19, and 79 with the others pathologies. The
Figure1-COVID dataset is a smaller one comprising 40 images, where 35 are from patients confirmed with COVID-19,
three with pneumonia caused by others infections, and two with no finding pathology. There is also a set of images
from patients with symptoms not confirmed as positive of COVID-19 that were filtered out. We merged the Cohen and
the Figure1-COVID dataset forming the COVID-19 dataset. In Table 2 is summarized the distribution of samples, and
in Figure 3 is shown some examples of images from these datasets from patients diagnosed with COVID-19.
6

Bootstrap: https://getbootstrap.com/
Images were collected using Web Scraping
8
http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html
9
http://www.vision.caltech.edu/archive.html
10
https://github.com/agchung/Figure1-COVID-chestxray-dataset
7

5

A PREPRINT - S EPTEMBER 4, 2020

Datasets
Chest X-Ray RSNA COVID-19
No Finding
1,150
8,851
4
Lung Opacity
3,100
6,012
82
COVID-19
–
–
394
Table 2: Distrubtion of samples of Chest X-Ray, RSNA, and COVID-19 datasets.
Pathology

Figure 3: Examples of X-Ray images from patients diagnosed with COVID-19.
The non-COVID-19 datasets are the Chest X-Ray Pneumonia [31] and the RSNA Pneumonia detection challenge [32].
The Chest X-Ray Pneumonia dataset contains a total of 5, 863 PA view X-Ray images, collected from pediatric patients
from one to five years old. The dataset is distributed into two categories: Normal or Pneumonia, and it is already
organized into training, validation, and testing sets. Additionally, it also reported whether the pneumonia was caused by
a virus or bacterial. Figure 4 illustrates some examples of images registered as “Normal”, “Virus Pneumonia”, and
“bacterial Pneumonia”.

(a) No Finding

(b) Virus Pneumonia

(c) Bacterial Pneumonia

Figure 4: X-Ray images from patients diagnosed as No Finding (a), bacterial pneumonia (b), and virus pneumonia (c).
From the RSNA dataset, we selected the images corresponding to the “No Finding” and “Lung Opacity” classes,
filtering out the images rotated at (90◦ -270◦ ) approx using the X-Ray filter. Table 2 summarizes the final distribution of
samples after performed this process. Examples of the “No Finding” and “Lung Opacity” classes are shown in Figure 5.
In this work, we rename the images labeled as Normal and Pneumonia to the non-Finding and Lung opacity class to
have the same nomenclature of RSNA dataset, which contains images of more general cases.

6

A PREPRINT - S EPTEMBER 4, 2020

(a) No Finding

(b) No Finding

(c) No Finding

(d) Lung Opacity

(e) Lung Opacity

(f) Lung Opacity

Figure 5: Examples of images from patients labeled as No Finding (a, b, c) and Lung Opacity (d, e, f)

4

Results

This section presents the experimental protocol used for training and evaluating the neural networks. Afterward, it is
shown and analyzed the results obtained for each of them.
4.1

X-Ray Images Filter

Experimental Protocol. The images were resized to 224 × 224 × 1 and normalized between zero to one. Besides,
the training samples were augmented by performing random rotations of 5◦ and zooming up to 10%. The experiments
were carried out in an NVIDIA GPU Tesla P100 of 16 GB of RAM and 3584 Cuda cores. Table 3 summarizes the
metaparameters setup used for training.
Configurations
Value
Batch
128
Steps by epoch
5
Number of epochs
100 (maximum)
Optimizer
Adam
Learning Rate
0.001 (decay a 0.5 factor each 5 epochs)
Stop Criteria
Follow up the loss function (15).
Metrics
Accuracy
Table 3: Training configuration used in the filter network.

Result. The training time was 30 minutes approximately, running a total of 69 epochs, when the early-stopping was
activated (see Table 3). Figures 6 and 7 shows the evolution of the loss and accuracy function over training. Notice
that the network achieved the best performance at epoch 54, where the loss function was 0.0075, and the accuracy was
99.83% in the validation set. Similar classification rates were obtained by evaluating the trained network in the testing
set, reaching an accuracy of 99.3%.
4.2

COVID-19 Classifier

Experimental Protocol. We first split each dataset into three groups: training, validation, and testing sets, adopting
different criteria for each dataset. In the COVID-19 datasets, we selected the samples according to the patient to avoid
having samples from the same patient in training and validation/testing sets, respectively. Specifically, we selected a
proportion of 80%, 10%, and 10% of patients for training, validation, and testing, respectively. In the RSNA dataset,
7

A PREPRINT - S EPTEMBER 4, 2020

8

101

Validation
Training

7

Validation
Training

100

6
Loss

Loss

5
4

10−1

3
10−2

2
1

10−3

0
0

10

20

30
40
Epochs

50

60

70

0

10

20

(a)

30
40
Epochs

50

60

70

(b)

Figure 6: Loss Function in (a) linear, and (b) logarithm scales

1.0

Accuracy

0.9
0.8
0.7
0.6
Validation
Training

0.5
0

10

20

30
40
Epochs

50

60

70

Figure 7: Training and Validation Accuracy learning curves per epoch.
the samples were split randomly following the same proportion of the COVID-19 datasets because there is no more
than one image per patient. In the Chest X-Ray Pneumonia dataset was not necessary to perform this procedure as this
distribution was already provided. Finally, the corresponding groups were merged to constitute the final sets.
All images were resized to 224 × 224 and normalized according to the mean and standard deviation of the ImageNet
dataset. Due to the dataset imbalance, we both oversampled the COVID-19 images and weighted the cross-entropy loss
to enhance the classification of COVID-19 imagery. We assigned the weights for each class using equation 1,
Nci
,0 < i < C
Ncmax
where Nci is the number of samples for class i, Ncmax the majority class and C the number of classes.
ωci =

(1)

Considering the presence of images potentially labeled incorrectly, it was also applied the label smoothing as a
regularization technique, setting alpha to 0.1. It prevents the model from making predictions too confidently during
training that can be reflected in poor generalization. Then, cross-entropy loss takes the form described in equation 2
L = ysof t ∗ log(ŷ) ∗ ωci + (1 − ysof t ) ∗ log(1 − ŷ)
where ysof t = (1 − α) ∗ y + α/C, and y and ŷ the true and predicted labels, respectively.

(2)

Besides, we also augmented the training and validation samples by performing random horizontal flipping, rotation,
brightness, scaling, occlusions at top of the images, cropping, and resize transformations. This procedure was carried
out to avoid the network learns to maximize the inter-class differences based on the distribution of the noise that
characterizes each dataset. For instance, it can be observed among the datasets, annotations like letters, arrows, captions,
etc., associated with the software used for capturing the images, or/and by analysis achieved by the specialist.
8

A PREPRINT - S EPTEMBER 4, 2020

During training, we monitored the validation loss function applying early stopping when no improvements were
observed throughout five consecutive epochs. Also, the learning rate was reduced by a factor of 0.5 after a plateau on
the validation loss. We initiated the learning rate of Adam optimizer to 1e − 5, fixing the other parameters to default
values.
Finally, we assessed the performance of the model quantitatively and qualitatively. The foremost was performed in
terms of sensitivity and specificity metrics, which quantifies the classifier’s ability to avoid false negatives and false
positives, respectively. The sensitivity is defined as the ratio between the number of true positives and the sum of
true positive and false negatives, and the specificity as the ratio between the true negatives and the sum of the true
negatives and false positives. The qualitative assessment was performed by analyzing the generated Class Activation
Maps (CAM), which indicates the regions that were more relevant in the classification of the evaluated image.
Results Tables 4 and 5 summarize the performance of the model in terms of the confusion matrix, and sensitivity and
specificity metrics, respectively after executing the experimetens 50 times. Specifically, Table 4 reports the sum of all
classification matrix, whereas Table 5 presents the mean and standard deviation statistics of corresponding specificity
and sensitivity metrics. Results demonstrate the high capability of the model to discern between the assessed classes. In
particular, it is observed that the classifier exhibits a lower rate of missclassification regarding the COVID-19 class,
although the lower amount of samples. Notice the major confusion occurs between Lung Opacity and COVID-19
samples. These results are expected due to the high correlation between Lung Opacity and COVID-19 samples, where
almost all patients with COVID-19 symptoms present manifestations of pneumonia.
These results are reflected in the sensitivity and specificity metrics, where the classification rates are above 84.4% in
mean for all classes. However, it is necessary the collection of more COVID-19 images to improve the capability of the
models for this task. For a real scenario, it is important to have a classifier with a lower rate of false positives to reduce
the propagation of the virus, as well as with lower false positives to avoid classifying healthy persons.
Predicted
No Finding
Lung Opacity
No Finding
53821/92.5%
3552/6.1%
Actual Lung Opacity
3116/6.1%
46620/91.2%
COVID-19
0
356/11.6%
Table 4: Sum of all Confusion Matrix/Normalized confusion matrix obtained
corresponding testing set after running the experiments 50 times.

COVID-19
763/1.3%
1380/2.7%
2712/88.4%
by evaluating the classifier in the

Metrics
No Finding Lung Opacity
Covid-19
Sensitivity 92.6 ± 1.2% 91.2 ± 1.0% 84.4 ± 3.2%%
Specificity 94.3 ± 0.5% 93.6 ± 1.0% 98.0 ± 0.7%%
Table 5: Results obtained by the classifier in terms of Sensitivity and Specificity metrics. It is reported the mean and
standard desviations statistics.

Figure 8 shows three examples per class of classification activation maps (CAMs) corresponding to images from the
testing set. Specifically, it is shown a composition of the images and the associated CAM as heatmaps. By performing
this analysis, it can be determined the most relevant regions considered by the network to make the final classification.
In fact, it can be noticed the higher intensities on heatmaps are localized around the lungs as expected, and artifacts as
rows or letters present on the images are ignored by the network to make the final prediction. These results indicate that
the network is not making the final decision based on the noise characteristics of each dataset. As it is observed in the
images of Figure 3, there are artifacts that can easily be identified by the network to discern between images from the
assessed classes. Regarding images belonging to Lung Opacity, it is remarked that the network was able to identify the
regions where these opacities occur. Similar behavior can be observed in the COVID-19 images where the network also
highlights the most whited regions. These results are expected considering the high correlation between COVID-19
image and Pneumonia, as COVID-19 causes Pneumonia. For the Non Finding class, the heatmaps are concentrated in
the central part of the chest, between the Lungs.
Finally, Figure 9 shows examples of images missclassified. It can be observed that images classified as COVID-19
being of n Lung Opacit, presents similar characteristics and vice-versa. This situation is expected to be common in
patients with advanced degrees of pneumonia. This situation is expected to be common for patients with advanced
stages of pneumonia independently the cause of it.
9

A PREPRINT - S EPTEMBER 4, 2020

(a) No Finding

(b) No Finding

(c) No Finding

(d) Lung Opacity

(e) Lung Opacity

(f) Lung Opacity

(g) COVID-19

(h) COVID-19

(i) COVID-19

Figure 8: Examples of images from patients labeled as Non-Finding (a, b, c), Lung Opacity (d, e, f), and COVID-19 (g,
h, i, respectively)

Actual

COVID-19

Lung Opacity

No Finding

Lung Opacity

COVID-19

Lung Opacity

CAM

Predicted

Figure 9: Examples of images missclassified

10

A PREPRINT - S EPTEMBER 4, 2020

Figure 10: Initial state of the site, where the user can send an X-Ray image to obtain a prediction of COVID-19.

Figure 11: Section of results. History and classification of X-Ray images tested.

4.3

Web Page Interface

The site is structured in three sections, being the first one, where the user can interact with the website to test owned
X-Ray images or examples available on the website. The second one describes the motivation and objectives of the
project, and the third one presents the proposed solution as well as the technical explanation of its implementation.
Figure 10 displays a snip of the first section of the web site at its initial state, while Figure 11 shows examples of testing
performed on it. In particular, it is illustrated in diagrams of bars the result obtained for the actual test, and a history of
the last experiment carried out. Besides, for each test, it is shown a brief description of the results, indicating if the
image evaluated is from a healthy person, with opacity in the Lungs, or with characteristics of COVID-19.
11

A PREPRINT - S EPTEMBER 4, 2020

Figure 12: A PCA projection of a dataset to visualized COVID-19 and non-COVID-19 cases.
Finally, the web page also shows an Embedding projector [33]11 to visualize and interpret the features extracted from
294 X-Ray images from the dataset. For instance, Figure 12 presents this iterative tool visualizing a basic clustering
using the PCA algorithm, where red and blue samples correspond to No Finding and COVID-19 images, respectively

5

Conclusion

In this work, a free web service for fast corona virus 2019 (COVID-19) classification using chest X-ray images is
presented. The web service is handy and accessible, being possible to use it from desktop computer or mobile devices.
It has dedicated GPUs for real-time inference and relies on two deep learning models: a model to differentiate between
X-Ray and non-X-Ray images, and a model to detect COVID-19 characteristics in chest X-ray images.
The web service is composed of three main parts: a friendly web interface, an X-Ray image filter, and a COVID-19
classifier. The web interface allows the user to upload a chest X-Ray image for consultation and receive a classification,
which can be Healthy, Characteristics of Pneumonia, or COVID-19. Moreover, a heatmap with the Class Activation
Maps (CAM) is generated, which indicates the regions that were more relevant in the classification of the evaluated
image.
An X-Ray image filter have been implemented to differentiate between valid images (pulmonary frontal X-Ray images),
and non-valid images (rotated pulmonary X-Ray images, natural images, other kind of images). A pre-trained MobileNet
architecture have been employed for this purpose, achieving an accuracy of 99.3%.
The COVID-19 Classifier was implemented by optimizing a pre-trained ImageNet DenseNet. Results demonstrated the
capability of the model to discern between the assessed classes, recording values above 89% in terms of sensitivity and
specificity metrics. Additionally, a qualitative evaluation, based on the analysis of the generated CAM, indicates that
the trained network makes its decisions focuses on the regions of the image where the Lungs are located. This analysis
shows that the strategy of augmenting the data and replicating the COVID-19 samples helped to mitigate the problem of
overfitting considering the scarcity of COVID-19 samples.
Future works consider the assessment of the system in hospitals to have feedback from specialists. We will provide a
platform to annotate samples to increase the dataset, and retrain the network to have a more accurate model.
11

Embedding projector https://projector.tensorflow.org/

12

A PREPRINT - S EPTEMBER 4, 2020

Besides, we will extend our methodology to make classification in Computerized Tomography images where more
COVID-19 images are available.

Acknowledgements
This work could not have been done without the collaboration of the entire team of the Applied Computational
Intelligence Laboratory (ICA) and Cenpes / Petrobras, partners for 20 years in the research and development of artificial
intelligence projects for oil and gas sector.

Disclaimer of liability
It is noteworthy that there is no way to guarantee 100 % effectiveness in any predictive process. For this reason, it is
extremely important that any medical diagnosis is made by specialized healthcare professionals. The objective of this
project is only to assist decision-making by specialists.

References
[1] World Health Organization - WHO. WHO Timeline - COVID-19, 2020 (accessed on: Jun 5th, 2020). https:
//www.who.int/news-room/detail/27-04-2020-who-timeline---covid-19.
[2] World Health Organization - WHO.
Coronavirus disease (COVID-19) pandemic, 2020 (accessed
on:
Jun 22nd, 2020).
https://www.who.int/emergencies/diseases/novel-coronavirus2019?gclid=CjwKCAjw_LL2BRAkEiwAv2Y3Sbs8yMUHDwSc0ph0Cc0xbvuYd1DAXohRS4GBXjGaCrHbfUF2VVb2BoCRokQAvD_BwE.
[3] Raju Vaishya, Mohd Javaid, Ibrahim Haleem Khan, and Abid Haleem. Artificial Intelligence (AI) applications for
COVID-19 pandemic. Diabetes & Metabolic Syndrome: Clinical Research & Reviews, 14(4):337 – 339, 2020.
[4] inloco.
Mapa brasileiro da COVID-19, 2020 (accessed on:
May 26th, 2020).
https:
//mapabrasileirodacovid.inloco.com.br/pt/?hsCtaTracking=68943485-8e65-4d6f-8ac0af7c3ce710a2%7C45448575-c1a6-42c8-86d9-c68a42fa3fcc.
[5] Landing AI. Landing AI creates an AI tool to help customers monitor social distancing in the workplace,
2020 (accessed on: May 26th, 2020). https://landing.ai/landing-ai-creates-an-ai-tool-to-helpcustomers-monitor-social-distancing-in-the-workplace/.
[6] Institute for New Economic Thinking (INET) - University of Cambridge Faculty of Economics. When will the
Covid-19 Pandemic Peak?, 2020 (accessed on: May 27th, 2020). http://covid.econ.cam.ac.uk/linton-ukcovid-cases-predicted-peak.
[7] Ophir Gozes, Maayan Frid-Adar, Hayit Greenspan, Patrick D. Browning, Huangqi Zhang, Wenbin Ji, Adam
Bernheim, and Eliot Siegel. Rapid ai development cycle for the coronavirus (covid-19) pandemic: Initial results
for automated detection & patient monitoring using deep learning ct image analysis, 2020.
[8] Joseph Paul Cohen, Paul Morrison, and Lan Dao. COVID-19 Image Data Collection, 2020.
[9] LeewayHertz.
Face mask detection system, 2020 (accessed on:
www.leewayhertz.com/face-mask-detection-system/.

May 26th, 2020).

https://

[10] Chaolin Huang, Yeming Wang, Xingwang Li, Lili Ren, Jianping Zhao, Yi Hu, Li Zhang, Guohui Fan, Jiuyang
Xu, Xiaoying Gu, Zhenshun Cheng, Ting Yu, Jiaan Xia, Yuan Wei, Wenjuan Wu, Xuelei Xie, Wen Yin, Hui Li,
Min Liu, Yan Xiao, Hong Gao, Li Guo, Jungang Xie, Guangfa Wang, Rongmeng Jiang, Zhancheng Gao, Qi Jin,
Jianwei Wang, and Bin Cao. Clinical features of patients infected with 2019 novel coronavirus in wuhan, china.
The Lancet, 395(10223):497 – 506, 2020.
[11] S. Khan, A. Ali, R. Siddique, and G. Nabi. Novel coronavirus is putting the whole world on alert. Journal of
Hospital Infection, 104(3):252 – 253, 2020.
[12] Yan-Chao Li, Wan-Zhu Bai, and Tsutomu Hashikawa. The neuroinvasive potential of sars-cov2 may play a role in
the respiratory failure of covid-19 patients. Journal of Medical Virology, 92(6):552–555, 2020.
[13] Roman Kalkreuth and Paul Kaufmann. COVID-19: A Survey on Public Medical Imaging Data Resources, 2020.
[14] Ioannis D. Apostolopoulos and Tzani A. Mpesiana. Covid-19: automatic detection from X-ray images utilizing
transfer learning with convolutional neural networks. Physical and Engineering Sciences in Medicine, 2020.
13

A PREPRINT - S EPTEMBER 4, 2020

[15] Muhammad Ilyas, Hina Rehman, and Amine Nait-ali. Detection of Covid-19 From Chest X-ray Images Using
Artificial Intelligence: An E Review, 2020.
[16] Ezz El-Din Hemdan, Marwa A. Shouman, and Mohamed Esmail Karar. COVIDX-Net: A Framework of Deep
Learning Classifiers to Diagnose COVID-19 in X-ray Images, 2020.
[17] Md. Rezaul Karim, Till Döhmen, Dietrich Rebholz-Schuhmann, Stefan Decker, Michael Cochez, and Oya Beyan.
DeepCOVIDExplainer: Explainable COVID-19 Predictions Based on Chest X-ray Images, 2020.
[18] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations
from deep networks via gradient-based localization. In 2017 IEEE International Conference on Computer Vision
(ICCV), pages 618–626, 2017.
[19] Asmaa Abbas, Mohammed Abdelsamea, and Mohamed Gaber. Classification of COVID-19 in chest X-ray images
using DeTraC deep convolutional neural network. medRxiv, 2020.
[20] Parnian Afshar, Shahin Heidarian, Farnoosh Naderkhani, Anastasia Oikonomou, Konstantinos N. Plataniotis, and
Arash Mohammadi. COVID-CAPS: A Capsule Network-based Framework for Identification of COVID-19 cases
from X-ray images, 2020.
[21] Linda Wang and Alexander Wong. COVID-Net: A Tailored Deep Convolutional Neural Network Design for
Detection of COVID-19 Cases from Chest X-Ray images, 2020.
[22] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
Applications, 2017.
[23] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely Connected Convolutional Networks. In
2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2261–2269, 2017.
[24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.
[25] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul,
Curtis Langlotz, Katie Shpanskaya, et al. Chexnet: Radiologist-level pneumonia detection on chest x-rays with
deep learning. arXiv preprint arXiv:1711.05225, 2017.
[26] Sebastian Guendel, Sasa Grbic, Bogdan Georgescu, Siqi Liu, Andreas Maier, and Dorin Comaniciu. Learning
to recognize abnormalities in chest x-rays with location-aware dense networks. In Iberoamerican Congress on
Pattern Recognition, pages 757–765. Springer, 2018.
[27] Hongyu Wang and Yong Xia. Chestnet: A deep neural network for classification of thoracic diseases on chest
radiography. arXiv preprint arXiv:1807.03058, 2018.
[28] Ivo M Baltruschat, Hannes Nickisch, Michael Grass, Tobias Knopp, and Axel Saalbach. Comparison of deep
learning approaches for multi-label chest x-ray classification. Scientific reports, 9(1):1–10, 2019.
[29] Fengqi Yan, Xin Huang, Yao Yao, Mingming Lu, and Maozhen Li. Combining lstm and densenet for automatic
annotation and classification of chest x-ray images. IEEE Access, 7:74181–74189, 2019.
[30] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex
McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al. Identifying medical diagnoses and treatable diseases by
image-based deep learning. Cell, 172(5):1122–1131, 2018.
[31] Daniel Kermany, Kang Zhang, and Michael Goldbaum. Labeled optical coherence tomography (oct) and chest
x-ray images for classification. Mendeley data, 2, 2018.
[32] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. ChestX-ray8:
Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of
common thorax diseases. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
pages 2097–2106, 2017.
[33] Daniel Smilkov, Nikhil Thorat, Charles Nicholson, Emily Reif, Fernanda B Viégas, and Martin Wattenberg.
Embedding projector: Interactive visualization and interpretation of embeddings. arXiv preprint arXiv:1611.05469,
2016.

14

