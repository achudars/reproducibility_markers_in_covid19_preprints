Checking individuals and sampling populations
with imperfect tests
arXiv:2009.04843v1 [q-bio.PE] 1 Sep 2020

Giulio D’Agostini1 and Alfredo Esposito2
Abstract
In the last months, due to the emergency of Covid-19, questions related to
the fact of belonging or not to a particular class of individuals (‘infected or not
infected’), after being tagged as ‘positive’ or ‘negative’ by a test, have never
been so popular. Similarly, there has been strong interest in estimating the
proportion of a population expected to hold a given characteristics (‘having
or having had the virus’). Taking the cue from the many related discussions
on the media, in addition to those to which we took part, we analyze these
questions from a probabilistic perspective (‘Bayesian’), considering several effects that play a role in evaluating the probabilities of interest. The resulting
paper, written with didactic intent, is rather general and not strictly related
to pandemics: the basic ideas of Bayesian inference are introduced and the
uncertainties on the performances of the tests are treated using the metrological concepts of ‘systematics’, and are propagated into the quantities of
interest following the rules of probability theory; the separation of ‘statistical’
and ‘systematic’ contributions to the uncertainty on the inferred proportion
of infectees allows to optimize the sample size; the role of ‘priors’, often overlooked, is stressed, however recommending the use of ‘flat priors’, since the
resulting posterior distribution can be ‘reshaped’ by an ‘informative prior’ in
a later step; details on the calculations are given, also deriving useful approximated formulae, the tough work being however done with the help of direct
Monte Carlo simulations and Markov Chain Monte Carlo, implemented in R
and JAGS (relevant code provided in appendix).
“Grown-ups like numbers”
(The Little Prince)
“The theory of probabilities is basically
just common sense reduced to calculus”
(Laplace)
“All models are wrong, but some are useful”
(G. Box)
1
2

Università “La Sapienza” and INFN, Roma, Italia, giulio.dagostini@roma1.infn.it
Retired, alfespo@yahoo.it

Contents
1 Introduction

3

2 Rough reasoning based on expectations
2.1 Setting up the problem . . . . . . . . . . . . . . . . . . .
2.2 Fraction of sampled positives being really infected or not
2.3 Fraction of infectees in the positive sub-sample . . . . . .
2.4 Estimating the proportion of infectees in the population .
2.5 Moving to probabilistic considerations . . . . . . . . . .
2.6 Summing up . . . . . . . . . . . . . . . . . . . . . . . . .
3 Probability of infected in the light of the available
3.1 Bayes’ rule at work . . . . . . . . . . . . . . . . . .
3.2 Initial odds, final odds and Bayes’ factor . . . . . .
3.3 What do we learn by a second test? . . . . . . . . .

.
.
.
.
.
.

7
7
8
9
11
12
13

information
. . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .

13
14
17
19

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

4 Uncertainty about π1 and π2
20
4.1 From P (nPI | nI , π1 ) to f (π1 | nPI , nI ): Bayes’ rule applied to ‘numbers’ 20
4.2 Conjugate priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.3 Expected value or most probable value of π1 and π2 ? . . . . . . . . . 25
4.4 Effect of the uncertainties on π1 and π2 on the probabilities of interest 26
4.5 Adding also the uncertainty about p . . . . . . . . . . . . . . . . . . 29
4.6 Uncertainty about P (Inf | Pos) and P (NoInf | Neg)? . . . . . . . . . . 30
5 Predicting the number of positives resulting from testing a sample
5.1 Expected number of positives and its standard uncertainty . . . . . .
5.2 Taking into account the uncertainty on π1 and π2 . . . . . . . . . . .
5.2.1 Approximated formulae . . . . . . . . . . . . . . . . . . . . .
5.3 General considerations on the approximated evaluation of σ(nP ) . . .
5.3.1 Contribution of the uncertainty on ps due to sampling . . . . .

31
31
34
36
39
40

6 Sampling a population
6.1 Proportion of infected individuals in the random sample
6.2 Expected number of positives assuming exact values of π1
6.2.1 Approximated results . . . . . . . . . . . . . . . .
6.3 Detailed study of the four contributions to σ(fP ) . . . .
6.4 Statistical and systematic contributions to σ(fP ) . . . . .

41
42
44
44
46
50

2

. . . . .
and π2
. . . . .
. . . . .
. . . . .

.
.
.
.
.

.
.
.
.
.

7 Measurability of p
7.1 Probabilistic model . . . . . . . . . . . . . . . . . . . . . .
7.2 Monte Carlo estimates of f (nP ) and f (fP ) . . . . . . . . .
7.2.1 Using the R random number generators . . . . . . .
7.2.2 Using JAGS . . . . . . . . . . . . . . . . . . . . . .
7.2.3 Further check of the approximated formulae . . . .
7.3 Resolution power . . . . . . . . . . . . . . . . . . . . . . .
7.4 Predicting fractions of positives sampling two populations

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

56
56
57
58
60
64
64
66

8 Inferring p from the observed number of positives in the sample
8.1 From the general problem to its implementation in JAGS . . . . . . .
8.2 Inferring p and nI with our ‘standard parameters’ . . . . . . . . . . .
8.3 Dependence on our knowledge concerning π1 and π2 . . . . . . . . . .
8.4 Quality of the inference as a function of ns and fP . . . . . . . . . . .
8.5 Updated f (π1 ) and f (π2 ) in the case of ‘anomalous’ number of positives
8.6 Inferring the proportions of infectees in two different populations . . .
8.7 Which priors? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.7.1 Symmetric role of prior and ‘integrated likelihood’ . . . . . . .
8.7.2 Some examples . . . . . . . . . . . . . . . . . . . . . . . . . .
8.7.3 Some approximated rules . . . . . . . . . . . . . . . . . . . . .

68
68
70
72
74
76
78
79
80
81
83

9 Exact evaluation of f (p)
9.1 Setting up the problem . . . . . . . . . . .
9.2 Normalization factor and other moments of
9.3 Result and comparison with JAGS . . . .
9.4 More remarks on the role of priors . . . . .

85
85
87
88
91

10 Conclusions

. . . . .
interest
. . . . .
. . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

93

References . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
Appendix A – Some remarks on ‘Bayes’ formulae’ . . . . . . . . . 99
Appendix B – R and JAGS code . . . . . . . . . . . . . . . . . 103

1

Introduction

The Covid-19 outbreak of these months raised a new interest in data analysis, especially among lay people, for long locked down and really flooded by a tidal wave of
numbers, whose meaning has often been pretty unclear, including that of the body
counting, which should be in principle the easiest to assess. As practically anyone
3

who has some experience in data analysis, we were also tempted – we have to confess
– to build up some models in order to understand what was going on, and especially
to forecast future numbers. But we immediately gave up, and not only because faced
with numbers that were not really meaningful, without clear conditions, within reasonable uncertainty, about how they were obtained. The basic question is that, we
realized soon, we cannot treat a virus spreading in a human population like a bacterial
colony in a homogeneous medium, or a continuous (or discretized) thermodynamic
system. People live – fortunately! – in far more complex communities (‘clusters’),
starting from the families, villages and suburbs; then cities, regions, countries and
continents of different characteristics, population densities and social behaviors. Then
we would have to take into account ‘osmosis’ of different kinds among the clusters,
due to local, intermediate and long distance movements of individuals. Not to speak
of the diffusion properties of viruses in general and of this one in particular.
A related problem, which would complicate further the model, was the fact that
tests were applied, at least at the beginning of pandemic, mainly to people showing
evident symptoms or at risk for several reasons, like personnel of the health system.
We were then asking ourselves rather soon, why tests were not also made on a possibly representative sample of the entire population, independently of the presence of
symptoms or not.1 This would be, in our opinion, the best way to get an idea of the
proportion of the population affected at a given ‘instant’ (to be understood as one
or a few days) and to take decisions accordingly. It is quite obvious that surveys of
this kind would require rather fast and inexpensive tests, to the detriment of their
quality, thus unavoidably yielding a not negligible fraction of so called false positives
and false negatives.
When we read in a newspaper [2] about a rather cheap antibody blood test able to
tag the individuals being or having been infected 2 we decided to make some exercises
in order to understand whether such a ‘low quality’ test would be adequate for the
purpose and what sample size would be required in order to get ‘snapshots‘ of a population at regular times. In fact Ref. [2] not only reported the relevant ‘probabilities’,
1
For example we would have started choosing, in Italy, the families involved in the Auditel
system [1], created with the purpose to infer the share of television programs, on the basis of which
advertisers pay the TV channels. In general, in order to make sampling meaningful, the selection of
individuals cannot be left to a voluntary choice that would inevitably bias the outcomes of the test
campaign.
2
In fact, the test reported in Ref. [2] was claimed to be sensitive both to Immunoglobulin M
(IgM), the antibody related to a current infection, and Immunoglobulin G (IgG) related to a past
infection [3, 4]. Obviously, the effectiveness of these kind of ‘serological tests’ is not questioned here.
In particular, two kinds of immunoglobulins will take some time to develop and they are most likely
characterized by decay times. Therefore, the generic expression infected individuals (or in short
infectees) has to be meant as the members of the population which hold some ‘property’ to which the
test is sensitive at the time in which it is performed.

4

namely 98% to tag an Infected (presently or previously) as Positive (‘sensitivity’)
and 88% to tag a not-Infected as Negative (‘specificity’), but also the numbers of
tests from which these two numbers resulted. This extra information is important
to understand how believable these two numbers are and how to propagate their
uncertainty into the other numbers of interest, together with other sources of uncertainty. This convinced us to go through the exercise of understanding how the main
uncertainties of the problem would affect the conclusions:
• uncertainty due to sampling;
• uncertainty due to the fact that the above probabilities differ from 1;
• uncertainty about the exact values of these ‘probabilities’.3
Experts might argue that other sources of uncertainty should be considered, but our
point was that already clarifying some issues related to the above contributions would
have been of some interest. From the probabilistic point of view, there is another
source of uncertainty to be taken into account, which is the prior distribution of the
proportion of infectees in the population, however not as important as when we have
to judge from a single test if an individual is infected or not.
The paper, written with didactic intent4 (and we have to admit that it was useful
to clarify some issues even to us), is organized in the following way.
• Section 2 shows some simple evaluations based on the nominal capabilities of
the test, without entering in the probabilistic treatment of the problem. The
limitations of such ‘rough reasoning’ become immediately clear.
• Then we move in Sec. 3 to probabilistic reasoning, applied to the probability
that a person tagged as positive/negative ‘is’ (or ‘has been’) really infected or
not infected. The probabilistic tool needed to make this so called ‘probabilistic
inversion’ (Bayes’ theorem) is then reminded and applied, showing the relevance of the probability that the individual is infected or not, based on other
pieces of information/knowledge (‘prior probability’), a fundamental ingredient
of inference often overlooked.5
3

If you are not used to attach a probability to numbers that might have by themselves the
meaning of probability, Ref. [5] is recommended.
4
The educational writing is an old idea that both the authors pursued in the past (see e.g.
Refs. [6, 7, 8]), strongly believing in the necessity of making the management of uncertainty a basic
tenet of scholastic (and not only) curricula.
5
This problem has been recently addressed by an article on Scientific American [9], with arguments similar to the simplistic one we are going to show in Sec. 2, although complemented by a
rather popular visualization of the question. But we have been surprised by the lack of any reference
to probability theory and to the Bayes’ rule in the paper.

5

• The effect of the uncertainty on sensititivity, specificity and proportion of infectees in the population is discussed in Sec. 4. But, before doing that, we have
to model the probability density function for these uncertain quantities. Hence
an introduction to the application of Bayes’ theorem to continuous quantities
is required, including some notes on the use of conjugate priors.
• From Sec. 5 we switch our focus from single individuals to populations. Our
aim, that is inferring the proportion of ‘infectees’ (meaning, let us repeat it once
more, ‘individuals being or having being infected’) will be reached in Secs. 8 and
9. But, for didactic purposes, we proceed by step, starting from the expected
number of positives, examining in depth the various sources of uncertainty. In
particular, in Sec. 7 we study the measurability of p and the dependence of
its ‘resolution power’ on the test performances and the sample size. Most of
the work is done using Monte Carlo methods, but some useful approximated
formulae for the evaluation of uncertainty on the result are given as well.
• The probabilistic inference of p, that is evaluating its probability density function f (p), conditioned by data and well stated hypotheses, is finally done in
Sec. 8. Having to solve a multidimensional problem, in which f (p) is finally
obtained by marginalization, Markov Chain Monte Carlo (MCMC) methods become a must. In particular, we use JAGS [10], interfaced with R [11] through
the package rjags [12]. We also evaluate, by the help of JAGS, some joint
probability distributions and the correlation coefficients among the variables of
interest, thus showing the great power of MCMC methods, that have given a
decisive boost to Bayesian inference in the past decades.
• However, we show in Sec. 9 how to solve the problem exactly, although not in
closed form, and limiting ourselves to the pdf of p. A simple extension of the
expression of the normalization constant allows to evaluate the first moments
of the distribution, from which expected value, variance, skewness and kurtosis
can be computed (and then an approximation of f (p) can be ‘reconstructed’).
• An important issue, also of practical relevance, is the inference of the proportions of infectees in different populations, analyzed in Sec. 8.6, after having
been anticipated in Sec. 7.4. In fact, since the uncertainties about sensitivity
and specificity act as systematic errors (hereafter ‘systematics’), the differences
between these proportions can be determined better than each of them.
• The role of the prior in the inference of p, already analyzed in detail in Sec. 8.7,
is discussed again in Sec. 9.4, with particular emphasis to the case in which
priors are at odds ‘with the data’ (in the sense specified there). The take away
6

message will be to be very careful in taking literally ‘comfortable’ mathematical
models, never forgetting the quotes by Laplace and Box reminded in the front
page.
Two appendixes complete the paper. Appendix A is a kind of summary of ‘Bayesian
formulae’, with emphasis on the practical importance of unnormalized posteriors
obtained by a suitable choice of the so called chain rule of probability theory and
on which most Monte Carlo methods to perform Bayesian inference are based. In
Appendix B several R scripts are provided in order to allow the reader to reproduce
most of the results presented in the paper.

2

Rough reasoning based on expectations

2.1

Setting up the problem

Let us imagine we have a population of N elements, a proportion p of which shares
a given character. The simplest example is that of a box containing N balls, n1
white and n2 black. Let p be the proportion of white balls, i.e. p = n1 /N. If we
extract at random m balls, then we roughly expect m1 ≈ n1 × m/N = p · m white
and m2 ≈ n2 × m/N = (N − n1 ) × m/N = (1 − p) · m black. A classical problem
in probability theory is to infer the proportion p from the observed (‘measured’)
proportion pm = m1 /m.
Obviously, if m is equal to N, i.e. if we completely empty the box, then we acquire
full knowledge of the box content and the solution is trivial. However, in most cases
we are unable to analyze the entire population and we have to infer p from a sample.
Therefore, although pm can be a reasonable rough estimate of p, we can never be sure
about the true proportion. At most, there are numerical values we shall believe more
(those around pm ) and others we shall believe less. This problem was first tackled
analytically by Laplace in 1774 [13].
Let us now complicate the problem, taking into account the fact that we are
not even sure about the characteristics of each sampled individual, as, instead, it
happens with black and white balls. This is exactly what happens with infections of
different kinds, unless the symptoms are so evident and unique to rule out any other
explanation. We have then to rely on tests that are typically not perfect, especially if
we have neither time nor money to inspect in detail each individual in order to really
see the active agent. Sticking to tests providing only a binary response,6 as we hear
6

But we hardly believe that they only provide binary information, of the kind Yes/No, and we
wonder why a (although slightly) more refined scale is not reported, even discretized in a few steps,
like when we rank goods and services with stars. Anyway, we shall not touch this question in the
present paper, but only wanted to express here our perplexity.

7

and read in the media, and assuming that such testing devices and procedures are
planned to detect the infected individuals, we expect that if the answer is positive
then there should be a quite high chance that the individual is really infected, and a
small chance that she is not. Similarly, if the answer is negative, there should be a
high chance that the individual is not infected. (The conditionals are due to the fact
that there are other pieces of information to take into account, as we shall see.)
We can characterize therefore the test by two virtually continuous numbers π1
and π2 in the range between 0 an 1 such that, depending on whether the individual
is infected or not, the test procedure provides positive and negative answers with
probabilities
P (Pos | Inf) = π1
P (Neg | Inf) = 1 − π1 ;
P (Pos | NoInf) = π2
P (Neg | NoInf) = 1 − π2 ,
with self-evident meaning of the symbols (we just remind that the ‘|’ indicates that
what follows it plays the role of conditions and therefore ‘|’ should be read as “under
the condition”, or “conditioned by”). More technically, π1 is defined as test sensitivity,
while (1 − π2 ) is the test specificity (see e.g. Ref. [14]). Therefore, in order to fix the
ideas, the test to which we are referring [2] has 98% sensitivity and 88% specificity.
As it is easy to understand, the numerical quantities of π1 and π2 do not come
from first principles, but result from previous measurements. They are therefore
affected by uncertainty as all results in measurements typically are [15]. Therefore,
probability distributions have to be associated also to the possible numerical values
of these two test parameters. Anyway, within this section we take the freedom to use
their nominal values of 0.98 and 0.12 for our first rough considerations.

2.2

Fraction of sampled positives being really infected or not

Putting all together, our rough expectation is that our sample of m individuals will
contain m1 ≈ p · m infected, although we shall write it within this section as an
equality (‘m1 = p·m’), and ditto for other related numbers. Out of these m1 infected,
π1 · m1 will be tagged as positive and (1 − π1 ) · m1 as negative. Of the remaining
m2 = (1 − p) · m, not infected, π2 · m2 will be tagged as positive and (1 − π2 ) · m2 as
negative. In sum, the expected numbers of positive and negative will be
nP = π1 · m1 + π2 · m2
nN = (1 − π1 ) · m1 + (1 − π2 ) · m2 ,
8

(1)
(2)

which we can rewrite as
nP = π1 · p · m + π2 · (1 − p) · m
nN = (1 − π1 ) · p · m + (1 − π2 ) · (1 − p) · m ,

(3)
(4)

So, just to fix the ideas with a numerical example and sticking to π1 = 0.98 and
π2 = 0.12 of Ref. [2], in the case we sample 10000 individuals we get, assuming 10%
infected (p = 0.10),
• number of infected in the sample: 1000 (and hence 9000 not infected);
• infected tagged as positive: 980;
• infected tagged as negative: 20;
• not infected tagged as positive: 1080;
• not infected tagged as negative: 7920;
• total number of positive: 2060;
• total number of negative: 7940;
• fraction of the positives really infected: 980/2060 = 47.6 %.

2.3

Fraction of infectees in the positive sub-sample

We see therefore that, contrary to naive intuition, in spite of the apparent rather
good quality of the test (π1 = 0.98), the result is quite unreliable on the individual
base: a positive person has roughly 50 % chance of being really infected.7 But this
does not mean that the test was really useless. It has indeed increased the probability
of a randomly chosen individual to be infected from 10% to 48%. On the contrary,
the fraction of negatives really not infected is 7920/7940 = 99.75%. This result is
also surprising on a first sight, being the specificity (1 − π2 ) only 88%, i.e. not ‘as
good’ as the sensitivity (π1 ), as high as 98%. We shall see the reason in a while. For
the moment we just remark that in this second case the probability of a randomly
chosen individual to be not infected has increased from 90 % to 99.75 %.
7

This point is quite relevant when the so called false positive regards some disease with a strong
social stigma (e.g. AIDS). Bad practices and negligence in dealing with test results and ignoring the
population background caused genuine emotional suffering, heavy distress, up to suicide attempts
[16]. The same applies in forensics, where individual freedom and justice can be badly influenced
by evidence mismanagement (See Ref. [17, 18] and the references there). In a less tragic context,
ignoring the role of the priors can cause bad decisions to be made (see e.g. Ref. [19] for an application
concerning Information Security).

9

1.0
0.8
0.6
0.4
0.2
0.0

Fraction of True Positive/Negative

0.0

0.2

0.4

0.6

0.8

1.0

Assumed proportion of infectees (p)

Figure 1: Fraction of ‘true positives’ (red line, starting at 0 for p = 0) and ‘true negatives’
(green line, starting at 1 for p = 0) in the sample, as a function of the assumed proportion
p of infected individuals in the population, assuming π1 = P (Pos | Inf) = 0.98 and π2 =
P (Pos | NoInf) = 0.12 . The results in correspondence of p = 0.1, arbitrarily used as reference
value in the numerical example of this section, are marked by the vertical dashed line.

The reason of these counter-intuitive results is due to the role of the prior probability of being infected or not, based on the best knowledge of the proportion of
infected individuals in the entire population.8 The easy explanation is that, given
the numbers we are playing with, the number of positives is strongly ‘polluted’ by
the large background of not infected individuals.
In order to see how the outcomes depend on p, let us lower its value from 10%
to 1%. In this case our expectation will be of 1286 positives, out of which only
98 infected and 1188 not infected (the details are left as exercise). The fraction of
positives really infected becomes now only 7.6 %. On the other hand the fraction of
negatives really not infected is as high as 99.98 %. Figure 1 shows how these numbers
depend on the assumed proportion of infectees in the population (and then in the
sample, because of the rough reasoning we are following in this section).
8

We remind that we are not taking into account symptoms or other reasons that would increase
or decrease the probability of a particular individual to be infected. For example, the journalist of
Ref. [2] tells that he had ‘some suspicions’ he could have been infected on a plane.

10

This should make definitively clear that the probabilities of interest not only
depend, as trivially expected, on the performances of the test, summarized here by
π1 and π2 , but also – and quite strongly! – on the assumed proportion of infectees
in the population. More precisely, they depend on whether the individual shows
symptoms possibly related to the searched for infection and on the probability that
the same symptoms could arise from other diseases. However we are not in the
condition to enter into such ‘details’ in this paper and shall focus on random samples
of the population. Therefore, up to Sec 4.5, in which we deal with the probability
that a tested individual is infected or not on the basis of the test result, we shall refer
to p as ‘proportion of infectees’ in the population. But everything we are going to say
is valid as well if p is our ‘prior’ probability that a particular individual is infected,
based on our best knowledge of the case.

2.4

Estimating the proportion of infectees in the population

Now, after having seen what we can tell about a single individual chosen at random
and of which we have no information about possible symptoms, contacts or behavior,
let us see what we can tell about the proportion p of infected in the population, based
on the tests performed on the sampled individuals. The first idea is to solve Eqs. (3)
and (4) with respect to p, from which it follows
p =

nP − π2 · m
.
(π1 − π2 ) · m

(5)

Applying this formula to the 2060 positives got in our numerical example we re-obtain
the input proportion of 10%, somehow getting reassured about the correctness of the
reasoning. If, instead, we get more positives, for example 2500, 3000 or 3500, then the
proportion would rise to 15.1%, 20.1% and 26.7%, respectively, which goes somehow
in the ‘right direction’. If, instead, we get less, for example 2000 or 1500, then the
proportion lowers to 9.3% and 3.5%, respectively, which also seems to go into the
right direction.
However, keeping lowering the number of positives something strange happens.
For nP = 1200 Eq. (5) vanishes and it becomes even negative for smaller numbers
of positives, which is something concerning, indicating that the above formula is not
valid in general. But why did it nicely give the exact result in the case of 2060
positives? And what is the reason why it yields negative proportions below 1200
positives? Moreover, Eq. (5) has a worrying behavior of diverging for π1 = π2 , even
though irrelevant in practice, because such a test would be ridiculous – the same as
tossing a coin to tag a person Positive or Negative (but in such a case we would expect
to learn nothing from the ‘test’, certainly not that the real proposition of infectees
diverges!).
11

Let us see the limits of validity of the equation.
• The lower limit p ≥ 0 implies, as we have already seen in the numerical example,
nP ≥ π2 · m and π1 > π2 .9
• The upper limit p ≤ 1 is reflected in the condition nP ≤ π1 · m (and π1 > π2 ).
In our numeric example this would mean to have less than 9800 positives in
our sample of 10000. But this ignores the fact that the proportion of infectees
in the sample could be higher than that in the population.
Anyway, it is clear that when the model contemplates probabilistic effects we have
to use sound methods based on probability theory.

2.5

Moving to probabilistic considerations

Let us start seeing what is going on when there are no infected individuals in the
population, i.e. when p = 0. In our rough reasoning none of the 10000 sampled
individual will be infected. But 12% of them will be tagged as positive, exactly
the critical value of 1200 we have seen above. In reality we have neglected the fact
that 1200 is an expectation, in the probabilistic meaning of expected value, but that
other values are also possible. In fact, given the assumed properties of the test, the
number of individuals which shall result positive to the test is uncertain, and precisely
described by the well known binomial distribution with ‘probability parameter’ (see
Ref. [5] for clarifications) π2 . The expectation has therefore an uncertainty, that
we quantify with the standard uncertainty [15], i.e. the standard deviation of the
related probability distribution. Using the well known formula
resulting from the
p
binomial distribution, which in our case reads as σ = π2 · (1 − π2 ) · m, we get,
using our numbers, σ = 32.5. Since we are dealing with reasonably large numbers,
the Gaussian approximation holds and we can easily calculate that there is about
16% probability to get a number of positives equal or below 1167, and so on. In
particular we get 0.1% probability to observe a number equal or below 1100, which
we could consider a safe limit for practical purposes.
But, unfortunately, the story is a bit longer. In fact we don’t have to forget that
π2 comes itself from measurements and is therefore uncertain. Therefore, although
0.12 is its ‘nominal value’, also values below 0.10 are easily possible, yielding e.g. an
9

Mathematically, also negative numerator and denominator would yield a positive value of p,
although this case makes no sense in practice, requiring π1 smaller than π2 . Moreover, the mathematical divergence of Eq. (5) – of no practical relevance, as we have already commented – for π1 = π2
is indeed due to the fact Eq. (3) and (4) become then nP = π1 · m and nN = (1 − π1 ) · m, not depending any longer on p. [In more detail, taking π2 = π1 − ǫ, we get p = (nP − π1 · m + ǫ · m)/(ǫ · m),
diverging for ǫ → 0.]

12

expected number of positives, among the not infected individuals, of 1000 ± 30 for
π2 = 0.10 and 800 ± 27 for π2 = 0.08 (hereafter, unless indicated otherwise, we quote
standard uncertainties).
Then there is the question that we apply the tests on the sample, and not on the
entire population. Therefore, unless the proportion of infectees in the population is
exactly 0 or 1, the proportion of infectees in the sample (‘ps ’), will differ from p. For
example, sticking to a reference p = 0.1, in the 10000 individuals sampled from a
population ten times larger we do not expect exactly 1000 infected, but 1000 ± 28
as we shall see in detail in Sec. 6.1 (we only anticipate, in answer to somebody who
might have quickly checked the numbers, that the standard uncertainty differs from
30, calculated from a binomial distribution, because this kind of sampling belongs,
contrary to the binomial, to the model ‘extraction without reintroduction’).

2.6

Summing up

The simple reasoning based on mean expectations leads to correct results only when
all probabilistic effects are negligible, an approximation which holds, generally speaking, only for ‘large numbers’. Under this approximation the numbers of individuals
tagged as Positive or Negative can be considered to follow in a deterministic way from
the assumptions, one of which is the proportion of infectees. This number can then
be obtained inverting the deterministic relation, thus yielding Eq. (5). But when fluctuations around the mean expectations become important we need to use probability
theory in order to infer the parameter of interest.
As far as telling from a single test if a person tagged as Positive is really infected,
we have seen that the prior ‘assumed proportion’ of infected individuals in the entire
populations plays a major role. We have seen how to get the probability of interest
reasoning on the fraction of positives really infected in the sample of positives. In
more general terms this probability has to be calculated using Bayes’ theorem, that
will be shortly reminded in the next section.

3

Probability of infected, in the light of the test
result and of other relevant information

Having seen the limitations of rough reasoning in evaluating the probabilities of interest, let us now start using consistently the rules of probability theory. We begin
focusing on the probability of infected or not infected, given the test results and the
performances of the test. We shall move to predict the number of positives in a
sample of tested individuals starting from Sec. 5.

13

3.1

Bayes’ rule at work

The probability of Infected or Not Infected, given the result of the test, is easily
calculated using a simple rule of probability theory known as Bayes’ theorem (or
Bayes’ rule),10 thus obtaining, for the two probabilities to which we are interested
(the other two are obtained by complement),
P (Inf | Pos) =
P (NoInf | Neg) =

P (Pos | Inf) · P0 (Inf)
P (Pos)

(6)

P (Neg | NoInf) · P0 (NoInf)
,
P (Neg)

(7)

where P0 () stands for the initial, or prior probability, i.e. ‘before’11 the information
of the test result is acquired, i.e. the degree of belief we attach to the hypothesis
that a person could be e.g. infected, based on our best knowledge of the person
(including symptoms and habits) and of the infection. As we have already said, if
the person is chosen absolutely at random, or we are unable to form our mind even
having the person in front of us, we can only use for P0 (Inf) the proportion p of
infected individuals in the population, or assume a value and provide probabilities
conditioned by that value, as we shall do in a while. Therefore, hereafter the two
‘priors’ will just be P0 (Inf) = p and P0 (NoInf) = 1 − p.
Applying another well known theorem, since the hypotheses Inf and NoInf are
exhaustive and mutually exclusive, we can rewrite the above equations as
P (Inf | Pos) =
P (NoInf | Neg) =

P (Pos | Inf) · P0 (Inf)
P (Pos | Inf) · P0 (Inf) + P (Pos | NoInf) · P0 (NoInf)

(8)

P (Neg | NoInf) · P0 (NoInf)
.
P (Neg | Inf) · P0 (Inf) + P (Neg | NoInf) · P0 (NoInf)

(9)

In our model P (Pos | Inf) and P (Neg | NoInf) depend on our assumptions on the
10

See Appendix A for details.
This usual expression, regularly used in the literature together with the term prior, could
transmit the wrong idea of time order strictly needed, leading to the absurdity that the Bayes’
theorem could not be applied if one did not ‘declare’ (to a notary?) in advance her priors. What
really matters, e.g. in this specific example, is the probability that the tested person could be
infected or not, taking into account all other information but the test result. (We shall comment
further on the meaning and the role of the priors, in particular in Sec. 8.7.)
11

14

parameters π1 and π2 , that is, including the other two probabilities of interest,
P (Pos | Inf, π1 )
P (Pos | NoInf, π2 )
P (Neg | Inf, π1 )
P (Neg | NoInf, π2 )

=
=
=
=

π1
π2 ,
1 − π1
1 − π2 ,

(10)
(11)
(12)
(13)

In the same way we can rewrite Eqs. (8) and (9), adding, for completeness, also the
other two probabilities of interest, as
π1 · p
π1 · p + π2 · (1 − p)
(1 − π2 ) · (1 − p)
.
P (NoInf |Neg, π1 , π2 , p) =
(1 − π1 ) · p + (1 − π2 ) · (1 − p)
π2 · (1 − p)
P (NoInf | Pos, π1 , π2 , p) =
π1 · p + π2 · (1 − p)
(1 − π1 ) · p
.
P (Inf | Neg, π1 , π2 , p) =
(1 − π1 ) · p + (1 − π2 ) · (1 − p)
P (Inf | Pos, π1 , π2 , p) =

(14)
(15)
(16)
(17)

We also remind that the denominators have the meaning of ‘a priori probabilities of
the test results’, being
P (Pos | π1 , π2 , p) = π1 · p + π2 · (1 − p)
P (Neg | π1 , π2 , p) = (1 − π1 ) · p + (1 − π2 ) · (1 − p) .
For example, taking the parameters of our numerical example (p = 0.1, π1 = 0.98
and π2 = 0.12), an individual chosen at random is expected to be tagged as positive
or negative with probabilities 20.6% and 79.4%, respectively. Figure 2 shows these
two probabilities as a function of p for some values of π1 and π2 .
Figure 3 shows, by solid lines, P (Inf | Pos, π1 , π2 , p) and P (NoInf |Neg, , π1 , π2 , p)
as a function of p, having fixed π1 and π2 at our nominal values 0.98 and 0.12. They
are identical to those of Fig. 1, the only difference being the label of the y axis, now
expressed in terms of conditional probabilities. In the same figure we have also added
the results obtained with other sets of parameters π1 and π2 , as indicated directly in
the figure caption.12
12

The reader might be surprised to see plots in which p goes up to 1, but the reason is twofold:
first, p can be also interpreted in these plots as the purely subjective degree of belief of the expert
that the tested individual is infected, independently of the test result; second, the aim of this paper is
rather general and, from a physicist’s perspective, p could have the meaning of a detector efficiency,
a branching ratio in particle decays, and whatever can be modeled by a binomial distribution.

15

1.0
0.8
0.6
0.4
0.2
0.0

P(Pos|p,π1,π2), P(Neg|p,π1,π2)

0.0

0.2

0.4

0.6

0.8

1.0

Assumed proportion of infectees (p)

Figure 2: Probability that an individual chosen at random will result Positive (red lines with
positive slope) or Negative (green lines, negative slope) as a function of the assumed proportion
of infectees in the population. Solid lines for π1 = 0.98 and π2 = 0.12; dashed for π1 = 0.98 and
π2 = 0.02; dotted for π1 = 0.99 and π2 = 0.01; dashed-dotted for π1 = 0.999 and π2 = 0.001.

Analyzing the above four formulae, besides the trivial ideal condition obtained by
π1 = 1 and π2 = 0, one can make a risk analysis in order to optimize the parameters,
depending on the purpose of the test. For example, we can rewrite Eq. (14) as
P (Inf | Pos, π1 , π2 , p) =

1
1+

π2
π1

·

(1−p)
p

:

(18)

if we want to be rather sure that a Positive is really infected, then we need π2 /π1 ≪ 1,
unless p ≈ 1. Similarly, we can rewrite Eq. (15) as
P (NoInf |Neg, , π1 , π2 , p) =

1
1+

1−π1
1−π2

·

p
1−p

:

in this case, as we have learned, in order to be quite confident that the negative test
implies no infection, we need (1 − π1 ) ≪ 1, that is, for realistic values of π2 , a value
of π1 practically equal to 1, unless p is rather small, as we can see from Fig. 3. (In
16

1.0
0.8
0.6
0.4
0.2
0.0

P(Inf | Pos), P(NoInf | Neg)

0.0

0.2

0.4

0.6

0.8

1.0

Assumed proportion of infectees (p)

Figure 3: Probability of ‘Infected if tagged as Positive’ [ P (Inf | Pos), red line, null at p = 0 ] and
probability of ‘Not Infected if tagged as Negative’ [ P (NoInf | Neg), green line, null at p = 1 ] as
a function of p, calculated from Eqs. (14) and (15) for π1 = 0.98 and π2 = 0.12 (solid lines). For
comparison, we have also included (dashed lines) the case of π2 reduced to 0.02, thus increasing
the ‘specificity’ to 0.98. Then there are the cases of a higher quality test [π1 = (1− π2 ) = 0.99],
shown by dotted lines and of an extremely good test [π1 = (1 − π2 ) = 0.999)] shown by dotteddashed lines. (The probabilities to tag an individual, chosen at random, as positive or negative,
for the same sets of parameters, were shown in Fig. 2.)
order to show the importance to reduce π2 , rather than to increase π1 , in the case of
low proportion of infectees in the population, we show in Fig. 4 the results based on
some other sets of parameters.)

3.2

Initial odds, final odds and Bayes’ factor

Let us go again to the above formulae, which we rewrite in different ways in order
to get some insights on what is going on. Before the test, if no other information is
available, the initial odds Infected vs Not Infected are given by
p
P0 (Inf)
=
,
P0 (NoInf)
1−p
17

1.0
0.8
0.6
0.4
0.2
0.0

P(Inf | Pos), P(NoInf | Neg)

0.0

0.2

0.4

0.6

0.8

1.0

Assumed proportion of infectees (p)

Figure 4: Same as Fig. 3, but with different parameters. Solid lines: π1 = 0.99 and π2 = 0.10.
Dashed lines (the red one, describing P (Inf | Pos) overlaps perfectly with the continuous one):
π1 = 0.999 and π2 = 0.10. Dotted lines (the green one, describing P (NoInf | Neg), almost
overlaps the solid one): π1 = 0.99 and π2 = 0.01.

equal to 1/9 for our reference value of p = 0.1. After the test has resulted in Positive
the new probability of Infected is given by Eq. (8). The corresponding probability of
Not Infected is given by a fraction that has the same denominator but P (Pos | NoInf)·
P0 (NoInf) as numerator. The final odds are then given by
P (Inf | Pos)
P (Pos | Inf)
P0 (Inf)
=
×
.
P (NoInf | Pos)
P (Pos | NoInf) P0 (NoInf)
Using our numerical values, we get
P (Inf | Pos)
p
π1
×
=
P (NoInf | Pos)
π2 1 − p
1
≈ 8.2 × .
9

18

(19)

The effect of the test resulting in Positive has been to modify the initial odds by the
factor
P (Pos | Inf)
BFInf vs NoInf (Pos) =
,
P (Pos | NoInf)

known as Bayes’ Factor.13 In our case this factor is equal to π1 /π2 ≈ 8.2. This means
that after a person has been tagged as Positive, the odds Infected vs Not Infected
have increased by this factor. But since the initial odds were 1/9, the final odds are
just below 1, that is about 1-to-1, or 50-50.
In the same way we can define the Bayes Factor Not Infected vs Infected in the
case of a negative result:
BFNoInf vs Inf (Neg) =

P (Neg | NoInf)
1 − π2
= 44
=
P (Neg | Inf)
1 − π1

This is the reason why, for a hypothetical proportion of infectees in the population of
10%, a negative result makes one practically sure to be not infected. The initial odds
of 9-to-1 are multiplied by a factor 44, thus reaching 396, about 400-to-1, resulting
into a probability of not being infected of 396/397, or 99.75%.

3.3

What do we learn by a second test?

Let us imagine that the same individual undergoes a second test and that the result
is again Positive. How should we update our believes that this individual is infected,
in the light of the second observation? The first idea would be to apply Bayes’ rule
in sequence, thus getting an overall Bayes’ Factor of (π1 /π2 )2 ≈ 67 that, multiplied
by the initial odds of 1/9, would give posterior odds of 7.4, or a probability of being
infected of 88%, still far from a practical certainty. But the real question is if we can
apply twice the same kind of test to the same person. It is easy to understand that the
multiplication of the Bayes’ factors assumes (stochastic) independence among them.
In fact, according to probability theory we have to replace now Eq. (19) by
P (Inf | Pos(1) , Pos(2) )
P (Pos(1) , Pos(2) | Inf)
P0 (Inf)
,
=
×
(1)
(2)
(1)
(2)
P (NoInf | Pos , Pos )
P (Pos , Pos | NoInf) P0 (NoInf)

(20)

having indicated by Pos(1) and Pos(2) the two outcomes. Numerator and denominator
of the Bayes’ Factor are then
P (Pos(1) , Pos(2) | Inf) = P (Pos(2) | Pos(1) , Inf) · P (Pos(1) | Inf)
P (Pos(1) , Pos(2) | NoInf) = P (Pos(2) | Pos(1) , NoInf) · P (Pos(1) | NoInf) ,
13

A more proper name could be Bayes-Turing factor, or perhaps even better Gauss-Turing factor [20], but we stick here to the conventional name.

19

which can be rewritten as
P (Pos(1) , Pos(2) | Inf) = P (Pos(2) | Inf) · P (Pos(1) | Inf)
P (Pos(1) , Pos(2) | NoInf) = P (Pos(2) | NoInf) · P (Pos(1) | NoInf) ,
and therefore we can factorize the two Bayes’ factors, only if the two test results are
independent. But this is far from being obvious. If the test response depends on
something one has in the blood, different from the virus one is searching for, a second
test of the same kind will most likely give the same result.

4

Uncertainty about π1 and π2

Until now we have used the nominal values of π1 and π2 of Ref. [2], and have already
seen how our probabilistic conclusions change if other sets of values are employed. But
these two model parameters come from tests performed on selected people, known
with certainty14 to be infected or not. More precisely π1 = 0.98 results from 400
surely infected, 392 of which resulted positive; π2 = 0.12 from 200 surely not infected,
176 of which resulted negative [2].

4.1

From P (nPI | nI , π1 ) to f (π1 | nPI , nI ): Bayes’ rule applied to
‘numbers’

It is rather obvious to think that, repeating the same test with samples of exactly
the same size, but involving different individuals, no one would be surprised to count
different numbers of positives and negatives in the two samples. In fact, sticking for a
while only to infectees and assuming an exact value of π1 , the number nPI of positives
is given by the binomial distribution,
f (nPI | nI , π1 ) ≡ P (nPI | nI , π1 ) =

nI !
nP
· π1 I · (1 − π1 )nI −nPI , (21)
nPI ! · (nI − nPI )!

that is, in short (with ‘∼’ to be read as ‘follows. . . ’),
nPI ∼ Binom(nI , π1 ).
The probability distribution (21) describes how much we have to rationally believe to
observe the possible values of nPI (integers between 0 and nI ), given nI and π1 .
An inverse problem is to infer π1 , given nI and the observed number nPI (indeed,
there is also a second inverse problem, that is inferring nI from nPI and π1 – the three
problems are represented graphically by the networks of Fig. 5). This is the kind of
14

This is what we assume, although we are not in the position to enter into the details.

20

nI

π1
√

√

nI

π1

nI

π1
√

√

nPI

nPI

√

nPI

√

Figure 5:
Graphical models of the binomial distribution (left) and its ‘inverse problems’. The
√

symbol ‘ ’ indicates the ‘observed’ nodes of the network, that is the value of the quantity
associated to it is (assumed to be) certain. The other node (only one in this simple case) is
‘unobserved’ and it is associated to a quantity whose value is uncertain.

Problem in the Doctrine of Chances first solved by Bayes [21], and, independently
and in a more refined way, by Laplace [13] about 250 years ago. Applying the result
of probability theory that nowadays goes under the name of Bayes’ theorem (or
Bayes’ rule) that we have introduced in the previous section, we get, apart from the
normalization factor [hereafter the same generic symbol is used for both probability
functions and probability density functions (pdf), being the meaning clear from the
context]:15
f (π1 | nPI , nI ) ∝ f (nPI | π1 , nI ) · f0 (π1 )
nPI

∝ π1

· (1 − π1 )nI −nPI · f0 (π1 ) ,

(22)
(23)

where f0 (π1 ) is the prior pdf, that describes how we believe in the possible values
of π1 ‘before’ (see footnote 11 and Sec. 8.7) we get the knowledge of the experiment
resulting into nPI successes in nI trials. Naively one could say that all possible
15

Some clarifications are provided in Appendix A. With reference to Eq. (A.8) there, Eq. (22)
derives from
f (π1 | nPI , nI )

∝ f (π1 , nPI , nI )
∝ f (nPI | π1 , nI ) · f (π1 | nI ) · f (nI )

∝ f (nPI | π1 , nI ) · f (π1 ) ,

in which we have used a pedantic chain rule derived from a bottom-up analysis of the second
graphical model of Fig. 5 (the one in which π1 is unknown) and taking into account, in the final
step, that π1 does not depend on nI , which has a precise, well known value in this problem. We can
note also that f (π1 , nPI , nI ) involves the continuous variable π1 and the discrete values nPI and nI ,
being then strictly speaking neither a probability function nor a probability density function, while
the meaning of each term of the chain rule is clear from the nature (continuous or discrete) of each
variable (see Appendix A for details).

21

values of π1 are equally possible, thus resulting in f0 (π1 ) = 1. But this is absolutely
unreasonable,16 in the case of instrumentation and procedures devised by experts in
order to hopefully tag infected people as positive. Therefore the value of π1 should be
most likely in the region above ≈ 90%, though without sharp cut below it. Similarly,
reasonable values of π2 are expected to be in the region below ≈ 10%.

4.2

Conjugate priors

At this point, remembering Laplace’s dictum that “probability is good sense reduced to
a calculus”, we need to model the prior in a reasonable but mathematically convenient
way.17 A good compromise for this kind of problem is the Beta probability function,
which we remind here, written for the generic variable x and neglecting multiplicative
factors in order to focus, at this point, on its structure:18

r, s > 0
r−1
s−1
f (x | r, s) ∝ x
· (1 − x)
(24)
0 ≤ x ≤ 1.
We see that for r = s = 1 a uniform distribution is recovered. An important remark
is that for r > 1 the pdf vanishes at x = 0; for s > 1 it vanishes at x = 1. It follows
that, if r and s are both above 1, we can see at a glance that the function has a single
maximum. It is easy to calculate that it occurs at (‘modal value’)
xm =

r−1
.
r+s−2

(25)

r
r+s

(26)

Expected value and variance (σ 2 ) are
µ = E(X) =
σ 2 = Var(X) =

r·s
.
(r + s + 1) · (r + s)2

(27)

In the case of uniform distribution,
recovered by r = s = 1, we obtain the well known
√
E(X) = 1/2 and σ(X) = 1/ √
12 (and, obviously, there is no single modal value). For
large r = s, we get σ(X) ≈ 1/ 8 r: as the values of r and s increases, the distribution
becomes very narrow around 1/2. Examples, with values of r and s to possibly model
16

Nevertheless, we shall comment in Sec. 8.7 about the practical importance of using a flat prior,
because it is possible to modify the result in a second step, ‘reshaping’ the posterior by personal,
informative priors based on the best knowledge of the problem, which might be different for different
experts (remember that the ‘prior’ does not imply time order, as remarked in footnote 11).
17
See Sec. 9.4 for advice about the usage of mathematically convenient models.
18
Our preferred vademecum of Probability Distributions is the homonymous app [22]. More details
are given in Sec. 9.

22

10
8
6

f(x)
4
2
0
0.0

0.2

0.4

0.6

0.8

1.0

x

Figure 6: Examples of Beta distributions. The curves preferring small values of the generic
variable x, all having E(X) = 0.2 are obtained with (widest to narrowest) r = 1.1, 2, 5, 10
and s = 4 r (σ: 0.16, 0.12, 0.078, 0.056). Those preferring larger values of x, all having
E(X) = 0.9 are obtained with (again widest to narrowest) s = 1.1, 2, 5 and r = 9 s (σ: 0.087,
0.065, 0.042).
the priors we are interested in, are shown in Fig. 6.
Using the Beta distribution for f0 (π1 ), our inferential problem is promptly solved,
since Eq. (23) becomes, besides a normalization factor and with parameters indicated
as r0 and s0 in order to remind their role of prior parameters,
nPI

f (π1 | nI , nPI , r0 , s0 ) ∝ π1
∝

· (1 − π1 )nI −nPI · π1r0 −1 (1 − π1 )s0 −1

nP +r0 −1
π1 I

· (1 − π1 )(nI −nPI )+s0 −1

(28)
(29)

So, the posterior is still a Beta distribution, with parameters updated according to
the simple rules
rf = r0 + nPI
sf = s0 + (nI − nPI ) .

(30)
(31)

For this reason the Beta is known to be the prior conjugate of the binomial distribu23

tion. In terms of our variables,
nPI ∼ Binom(nI , π1 ) =⇒ π1 ∼ Beta(r0 + nPI , s0 + nI − nPI ) .

(32)

The advantage of using the Beta prior conjugate is self-evident, if we can choose
values of r0 and s0 that reasonably model our prior belief about π1 . For this reason
it might be useful to invert Eq. (26) and (27), thus getting
r0
s0

(1 − µ0 ) · µ20
− µ0
=
σ02
1 − µ0
=
· r0 .
µ0

(33)
(34)

So, for example, if we think that π1 should be around 0.95 with a standard uncertainty
of about 0.05, we get then r0 = 17.1 and s0 = 0.9, the latter slightly increased ‘by
hand’ to s0 = 1.1 because our rational prior has to assign zero probability to π1 = 1,
that would imply the possibility of a perfect test.19 The experimental data update
then r and s to r = 409.1 and s = 9.1. For π2 we model a symmetric prior, with
expected value 0.05 and σ = 0.05. We just need to swap r and s, thus getting r0 = 1.1
and s0 = 17.1, updated by the data to r = 25.1 and s = 193.1. The results are
shown in Fig. 7. Expressed in terms of expected value ± standard deviation they are
π1 = 0.978 ± 0.007
π2 = 0.115 ± 0.022 .

(35)
(36)

As we can easily guess, using simply 0.98 and 0.12, as we have done in the previous
sections, will give essentially the same results, in terms of expectations. Anyway, in
order to be internally consistent hereafter our reference values will be π1 = 0.978
and π2 = 0.115.20
19

To be fastidious, s0 < 1 is not acceptable, because we do not believe a priori that a test could
be perfect, and therefore f0 (π1 ) has to vanish at π1 = 1. This implies that s0 must be slightly above
1, for example 1.1. But in our case the observation of at least one Negative would automatically
rule out π1 = 1. Anyway, although this little numerical difference is irrelevant in our case, we use
s0 = 1.1 only because, since we plot priors and posteriors in Fig. 7 we do not like to show a prior not
vanishing at 1. [We are admittedly a bit pedantic here for didactic purposes, but we shall be more
pragmatic later (see Sec. 8.7) and even critical about the literal use of mathematical expressions
that should instead only be employed for convenience and cum grano salis (see Sec. 9.4).]
20
If, instead, we had used flat prior over the two parameters, we would get, by the Laplace’ rule
of succession that we shall see in a while, 0.978 and 0.124. The result is identical (within rounding)
for π1 and practically the same for π2 , because with hundreds of trials the inference is dominated
by the data. (We insist in being fastidiously pedantic because of the didactic aim of this paper. For
more on priors, and for the practical importance of routinely using a flat one, see Sec. 8.7.)

24

60
50
40
30
20

f(π)

π1

0

10

π2

0.0

0.2

0.4

π

0.6

0.8

1.0

Figure 7: Priors (dashed) and posterior (solid) probability density functions of π1 and π2 .

4.3

Expected value or most probable value of π1 and π2 ?

At this point someone would object that one should use the most probable values
of π1 and π2 , rather than their expected values. The answer is rather simple. Let
us consider again Eq. (10). Assuming a well precise value of π1 , the probability of
Positive if Infected is exactly equal to π1 . However, if we want to evaluate P (Pos | Inf),
taking into account all possible values of π1 and how much we believe each of them,
that is f (π1 ), we just to need to use a well known result of probability theory:
P (Pos | Inf) =

Z

0

1

P (Pos | Inf, π1 ) · f (π1 ) dπ1 .

(37)

But, being P (Pos | Inf, π1 ) = π1 , we get
P (Pos | Inf) =

Z

0

25

1

π1 · f (π1 ) dπ1 ,

(38)

in which we recognize the expected value of π1 .21

4.4

Effect of the uncertainties on π1 and π2 on the probabilities of interest

The immediate question that follows is how the uncertainties concerning these two
parameters change the probabilities of interest. We start reporting in Tab. 1 the
dependence of P (Inf | Pos, π1 , π2 , p) and P (NoInf | Neg, π1 π2 , p), on which we particularly focused in the previous sections, on the three parameters. The dependence on
p is shown in the different columns, while the sets of π1 and π2 are written explicitly
in the conditionands of the different probabilities. We start from the nominal values
of 0.98 and 0.12 taken from Ref. [2] (first two rows of the table). Then we use the
expected values calculated in the previous section (third and fourth rows, in boldface), followed by variations of π1 and π2 based on ± one standard deviation from
their expected values.
We see that the probabilities of interest do not change significantly, the main
effect being due to the assumed proportion of infectees in the population. One could
argue that the dependence on π1 and π2 could be larger, if larger deviations of the
parameters were considered. Obviously this is true, but one has to take also into
account the (small) probabilities of large deviations from the mean values, especially
if we allow simultaneous deviations of both parameters.
A more relevant question is, instead, how do P (Inf | Pos) and P (NoInf | Neg)
change, if we take into account all possible variations of the two parameters (weighed
by their probabilities!). This is easily done, applying the result of probability theory that we have already used above. We get, for the probabilities we are mostly
interested in,
Z 1Z 1
P (Inf | Pos, p) =
P (Inf | Pos, π1 , π2 , p) · f (π1 , π2 ) dπ1 dπ2 .
(39)
0

P (NoInf | Neg, p) =
21

0

Z 1Z
0

0

1

P (NoInf | Neg, π1 , π2 , p) · f (π1 , π2 ) dπ1 dπ2 ,

(40)

In the case of a uniform prior, i.e. r0 = s0 = 1, we get
P (Pos | Inf) =

nP + 1
rf
= I
,
rf + sf
nI + 2

known as Laplace’s rule of succession. In particular, for large values of nPI and nI , P (Pos | Inf) ≈
nPI /nI : more frequently past tests applied to surely infected individuals resulted in Positive, more
probably we have to expect a positive outcome of a new test of the same kind applied to an infected
individual.

26

p
Probabilities
0.01
0.0762
0.9998

0.05
0.301
0.999

0.10
0.476
0.997

0.15
0.590
0.996

0.20
0.671
0.994

0.50
0.891
0.978

P (Inf | Pos, π1 = 0.978, π2 = 0.115, p)
P (NoInf | Neg, π1 = 0.978, π2 = 0.115, p)

0.0791
0.9997

0.309
0.999

0.486
0.997

0.600
0.996

0.680
0.994

0.895
0.976

P (Inf | Pos, π1 = 0.985, π2 = 0.115, p)
P (NoInf | Neg, π1 = 0.985, π2 = 0.115, p)

0.0796
0.9998

0.311
0.999

0.488
0.998

0.602
0.997

0.682
0.996

0.895
0.983

P (Inf | Pos, π1 = 0.971, π2 = 0.115, p)
P (NoInf | Neg, π1 = 0.971, π2 = 0.115, p)

0.0786
0.9998

0.308
0.998

0.484
0.996

0.598
0.994

0.679
0.992

0.894
0.968

P (Inf | Pos, π1 = 0.978, π2 = 0.137, p)
P (NoInf | Neg, π1 = 0.978, π2 = 0.137, p)

0.0673
0.9997

0.273
0.999

0.442
0.997

0.557
0.996

0.641
0.994

0.877
0.975

P (Inf | Pos, π1 = 0.978, π2 = 0.093, p)
P (NoInf | Neg, π1 = 0.978, π2 = 0.093, p)

0.0960
0.9998

0.356
0.999

0.539
0.997

0.650
0.996

0.724
0.994

0.913
0.976

P (Inf | Pos, π1 = 0.985, π2 = 0.093, p)
P (NoInf | Neg, π1 = 0.985, π2 = 0.093, p)

0.0966
0.9998

0.358
0.999

0.541
0.998

0.651
0.997

0.726
0.996

0.914
0.984

P (Inf | Pos, π1 = 0.971, π2 = 0.137, p)
P (NoInf | Neg, π1 = 0.971, π2 = 0.137, p)

0.0668
0.9997

0.272
0.998

0.441
0.996

0.556
0.994

0.639
0.992

0.876
0.967

P (Inf | Pos, π1 = 0.985, π2 = 0.137, p)
P (NoInf | Neg, π1 = 0.985, π2 = 0.137, p)

0.0677
0.9998

0.275
0.999

0.444
0.998

0.559
0.997

0.643
0.996

0.878
0.983

P (Inf | Pos, π1 = 0.971, π2 = 0.093, p)
P (NoInf | Neg, π1 = 0.971, π2 = 0.093, p)

0.0954
0.9997

0.355
0.998

0.537
0.996

0.648
0.994

0.723
0.992

0.913
0.969

0.0815
0.9998

0.314
0.999

0.490
0.997

0.603
0.996

0.682
0.994

0.895
0.976

P (Inf | Pos, π1 = 0.98, π2 = 0.12, p)
P (NoInf | Neg, π1 = 0.98, π2 = 0.12, p)

P (Inf | Pos, p)
P (NoInf | Neg, p)

Table 1: Probability of Infected and Not Infected, given the test result, as a function of
the model parameters. The third and fourth rows, in boldface, are for our reference values
of π1 and π2 . The last two rows are the results ‘integrating over’ all the possibilities of π1
and π2 , according to Eq. (39) and (40), with the integrals done in practice by Monte Carlo
sampling.

27

where f (π1 , π2 ) can be factorized into f (π1 ) · f (π2 ).22 The integral can be easily done
by Monte Carlo,23 whose implementation in the R language [11], both for P (Inf | Pos)
and P (NoInf | Neg), is given in Appendix B.1.
We get, for our arbitrary reference value of p = 0.1, P (Inf | Pos, p = 0.1) =
0.49038 and P (NoInf | Neg, p = 0.1) = 0.99727, to be compared to 0.4858 and 0.9973,
respectively, if the expected values were used. The results, shown with an exaggerated
number of digits just to appreciate tiny differences, are practically the same. This
result could sound counter-intuitive, especially if one thinks that π2 has an almost
20% intrinsic standard uncertainty. The reason is due to the fact that the dependence
of the probabilities of interest on π1 and π2 is rather linear in the region where their
probability mass is concentrated, as shown in Fig. 8. This rather good linearity
causes a high degree of cancellations in the integral.24 This explains why the only
perceptible effect appears in P (Inf | Pos, p = 0.1), slightly larger than the number
calculated at the expected values (49.04% vs 48.58%), caused by the small nonlinearity of that probability as a function of π2 , as shown in the upper, right hand
plot of Fig. 8: symmetric variations of π2 cause slightly asymmetric variations of
22

In principle π1 and π2 are not really independent, because they might depend on how the test
‘technology’ has been optimized, and it could be easily that aiming to reach high ‘sensitivity’ affects
‘specificity’. But with the information available to us we can only take them independent, each one
obtained by the number of positives and negatives observed in, hopefully, well controlled samples of
infected and not infected individuals.
23
The rational is quite easy to understand, starting e.g. from Eq. (39) and remembering that
f (π1 , π2 ) dπ1 dπ2 represents the infinitesimal probability dP that π1 and π2 occur in the infinitesimal
cell dπ1 dπ2 . We can discretize the plane (π1 , π2 ) in N cells and indicate by Pi the probability that
a point of π1 and π2 falls inside it. Equation (39) can be approximated as
P (Inf | Pos, p) ≈
≈

N
X
i=1

N
X
i=1

P (Inf | Pos, π1i , π2i , p) · Pi
P (Inf | Pos, π1i , π2i , p) · fi =

N
X
i=1

P (Inf | Pos, π1i , π2i , p) ·

ni
,
ntot

in which we have approximated each Pi by its expected relative frequency of occurrence fi = ni /ntot
(Bernoulli’s theorem). As one can see, we have approximated the integral by a weighted average,
in which the cells in the plane that are expected to be more probable count more. In reality we
do not even need to subdivide the plane into cells. We just extract at random π1 and π2 in the
plane, according to their probability distributions, calculate P (Inf | Pos, π1i , π2i , p) at each point and
calculate the average. When we consider a very large ntot , then we expect that the average will not
differ much from the integral.
24
A similar effect happens in evaluating the contribution of systematics on measured physical
quantity. If the dependence of the ‘influence factor’ [15] is almost linear, then the ‘central value’ is
practically not affected, and only its ‘standard uncertainty’ increases. [But in our case we are only
interested on its ‘central value’, that is e.g. the result of the integrals of Eqs. (39)-(40).]

28

0.65
0.35

0.45

0.55

0.65
0.55
0.45

P(Inf|Pos,p=0.1)

0.35

0.98

0.99

1.00

0.96

0.97

0.98

0.99

1.00

0.08

0.10

0.12

0.14

0.16

0.18

0.06

0.08

0.10

0.12

0.14

0.16

0.18

0.998
0.994

0.996

0.998
0.996
0.994

P(NoInf|Neg,p=0.1)

0.06
1.000

0.97

1.000

0.96

π1

π2

Figure 8: Dependence of P (Inf | Pos) (upper plots) and P (NoInf | Neg) (lower plots) on π1
(left hand plots, for π2 = 0.115 and p = 0.1) and on π2 (right hand plots, for π1 = 0.978 and
p = 0.1). The parameters π1 and π2 are allowed to change withing a range of ±3 σ’s around
their expected values.
P (Inf | Pos, p = 0.1), thus slightly favoring higher values of that probability.

4.5

Adding also the uncertainty about p

Now that we have learned the game, we can use it to include also the uncertainty
concerning p. At a given stage of the pandemic we could have good reasons to guess
a proportion of infected around 10%, as we have been done till now, with a sizable
uncertainty, for example 5% (i.e. p = 0.10 ± 0.05). We model, also in this case, f (p)

29

with a Beta distribution, getting r = 3.5 and s = 31.5. Equation (39) becomes then
Z 1Z 1Z 1
P (Inf | Pos) =
P (Inf | Pos, π1 , π2 , p) · f (π1 , π2 , p) dπ1 dπ2 dp
(41)
0

=

0

0

Z 1Z 1Z
0

0

0

1

P (Inf | Pos, π1 , π2 , p) · f (π1 ) · f (π2 ) · f (p) dπ1 dπ2 dp , (42)

in which we have made explicit that the joint pdf factorizes, considering π1 , π2 and p
independent.25 With a minor modification to the script provided in Appendix B.126
we get P (Inf | Pos) = 0.4626 and P (NoInf | Neg) = 0.9972, reported again with an
exaggerated number of digits. We only note a small effect in P (Inf | Pos). As a
further exercise, let also take into account p = 0.20 ± 0.10, modeled by a Beta(r =
3, s = 12). In this case the Monte Carlo integration yields P (Inf | Pos) = 0.641 and
P (NoInf | Neg) = 0.993, to be compared with 0.682 and 0.994 of Tab. 1.27

4.6

Uncertainty about P (Inf | Pos) and P (NoInf | Neg)?

As we have seen, the probabilities of interest, taking into account all the possibilities of
π1 , π2 and p are obtained as weighted averages, with weights equal to f (π1 , π2 , p). One
could then be tempted to evaluate the standard deviation too, attributing to it the
meaning of ‘standard uncertainty’ about P (Inf | Pos) and P (NoInf | Neg). But some
care is needed. In fact, although is quite obvious that, sticking again to P (Inf | Pos),
we can form an idea about the variability of P (Inf | Pos, π1 , π2 , p) varying π1 , π2 and p
according to f (π1 , π2 , p) (something like we have done in Tab. 1, although we have not
associated probabilities to the different entries of the table), one has to be careful in
making a further step. The fact that the weighted average is P (Inf | Pos) comes from
the rules of probability theory, namely from Eq. (41), but there is not an equivalent
rule to evaluate the uncertainty of P (Inf | Pos).
25

The question could be a bit more sophisticated, and we have already commented in footnote 22
on the possible dependency of π1 and π2 . But, given the information at hand and the purpose of
this paper, this is a more than reasonable assumption.
26
One just needs to replace ‘p = 0.1’ by ‘p = rbeta(n, 3.5, 31.5)’, to be placed after n has
been defined.
27
The reason why the integral over all possible values of p gives P (Inf | Pos) smaller than that
obtained at a fixed value of p can be understood looking at the solid red curve of Fig. 3 showing
P (Inf | Pos) as a function of p around p = 0.1, indicated by the vertical dashed line. If p has a
symmetric variation around 0.1 of ±0.1 (just to make things more evident), than P (Inf | Pos) has
an asymmetric variation of +0.20
−0.47 around 0.476 and therefore the Monte Carlo average will be quite
below 0.476 (but the Beta distribution used for p is skewed on the right side and therefore there is
a little compensation). For the same reason P (NoInf | Neg), practically flat in that region of p, is
instead rather insensitive on the exact value of p (unless we take unrealistic values around 0.9).

30

In order to simplify the notation, let us indicate in the following lines P (Inf | Pos)
by P. In order to speak about standard uncertainty of P, we first need to define the
pdf f (P), and then evaluate average and standard deviation. But Eq. (41) does not
provide that, but only a single number, that is P itself.
Let us reword what we just stated using a simple example. Given the ‘random
variable’ X and the pdf associated to it f (x), mean and standard deviation of f (x)
provide expected value (‘µX ’) and standard deviation of X, and not of µ.

5

Predicting the number of positives resulting from
testing a sample

The previous sections have been dedicated to the evaluation of the probability that a
particular individual, tagged as positive in a test, is really infected. In those sections
we have understood how, in absence of any other hints, it is important to know the
percentage p of infectees in the population. Knowing this parameter is paramount
also for better designing a containing strategy in addressing the pandemic. Therefore
we move now to the related, but quite different problem: ‘counting’, although not in
an exact way, the number of infected individuals in a population. Given the didactic
spirit of this paper, we keep proceeding step-by-step. First we focus on the number
of positives that we expect to observe if we check a sample using the quite imperfect
test we are considering. Then we also take into account the effect of sampling a
population, since, as it is rather obvious, the proportion of infected in a sample of
size ns will not be exactly equal to that in the whole population of N individuals.
For this reason we distinguish, hereafter, ps of the sample from p of the population.

5.1

Expected number of positives and its standard uncertainty

In Sec. 2.2 we have considered the numbers of positives and negatives that we expect
to observe, analyzing 10000 individuals, using our initial parameters (ps = 0.10,
π1 = 0.98, π2 = 0.12) but without taking into account the unavoidable ‘statistical
fluctuation’. We do it now, using the probabilistic graphical model shown in Fig. 9,
obtained by doubling the basic one of Fig. 5, one branch for the infectees and a second
for the others. Then the numbers of positives resulting from the two contributions
are added up. Note in Fig. 9 the dashed arrows from the nodes nPI and nPNI to the
node nP : they indicate a deterministic link,28 being nP = nPI + nPNI .
28

This convention is standard in the literature, although one might object – and we agree – that
the opposite one would have been a better choice, a solid line better representing a deterministic

31

nI

nN I

π1

π2

nPI

nPN I

nP

Figure 9: Graphical model in which the number of positives could come from infected or not
infected individuals. Arrows with dashed lines stand for a deterministic link, being nP simply
equal to the sum of nPI and nPNI .

The probability distribution of nP is with good approximation Gaussian, due to
the well known large numbers behavior of the binomial distribution (and, moreover,
to the properties of the sum of ‘random variables’). On the other hand, the expected
value and the standard deviation of nP can been calculated exactly, using the properties of expected values and variances, thus getting (summarizing for sake of space
with the symbol I, staying for all available information, the conditions on which the
various quantities depend):
E(nP | I) =
=
=
σ 2 (nP | I) =
=

E(nPI | I) + E(nPNI | I)
π1 · nI + π2 · nN I
π1 · ps · ns + π2 · (1 − ps ) · ns
σ 2 (nPI ) + σ 2 (nPI )
π1 · (1 − π1 ) · ps · ns + π2 · (1 − π2 ) · (1 − ps ) · ns
p
π1 · (1 − π1 ) · ps · ns + π2 · (1 − π2 ) · (1 − ps ) · ns ,
σ(nP | I) =

(43)
(44)
(45)

with ns the sample size. Expected value and standard deviation of the fraction of
the number of individuals tagged as positive (fP = nP /ns ) are then
E(nP | I)
= π1 · ps + π2 · (1 − ps )
ns
s
π1 · (1 − π1 ) · ps + π2 · (1 − π2 ) · (1 − ps )
σ(nP | I)
=
.
σ(fP | I) =
ns
ns

E(fP | I) =

link than a dashed one, but we stick to the convention.

32

(46)
(47)

For example, making use of our reference numbers (ns = 10000, π1 = 0.978 and
π2 = 0.115) we get for some values of ps (expected value ± standard uncertainty):
nP |(ns =10000, π1 =0.978, π2 =0.115, ps = 0.0)
nP |(ns =10000, π1 =0.978, π2 =0.115, ps = 0.1)
nP |(ns =10000, π1 =0.978, π2 =0.115, ps = 0.2)
nP |(ns =10000, π1 =0.978, π2 =0.115, ps = 0.5)

=
=
=
=

1150 ± 32
2013 ± 31
2876 ± 29
5465 ± 25

−→
−→
−→
−→

fP
fP
fP
fP

= 0.1150 ± 0.0032
= 0.2013 ± 0.0031
= 0.2876 ± 0.0029
= 0.5465 ± 0.0025 .

From this numbers we can get an idea about the precision we could get on ps , if
π1 and π2 were perfectly known, although their values are rather far from what one
would ideally desire. For example, since under the hypotheses ps = 0.1 and ps = 0
(and similar numbers are obtained varying ps from 0.1 to 0.2) the expected difference
of positives is ∆nP = 863 ± 45, it follows that, varying ps by ±0.01 the expected
number of positives would vary by ≈ ± (86 ± 4.5). This means that, roughly speaking,
it could be possible to estimate ps with an uncertainty of ±0.01 or better.
Before taking into account the effects due to the uncertainties of π1 and π2 , let us
also see how the quality of the measurement depends on the sample size. In order to
do this, we fix this time ps to our arbitrary value of 0.1 and vary the sample size by
about half order of magnitude (that is ≈ 10k/2 , with k = 6, 7, . . . , 10), reporting in
this case directly the expected fraction of positives:
fP |(ns = 1000, π1 =0.978, π2 =0.115, ps =0.1)
fP |(ns = 3000, π1 =0.978, π2 =0.115, ps =0.1)
fP |(ns = 10000, π1 =0.978, π2 =0.115, ps =0.1)
fP |(ns = 30000, π1 =0.978, π2 =0.115, ps =0.1)
fP |(ns = 100000, π1 =0.978, π2 =0.115, ps =0.1)

=
=
=
=
=

0.2013 ± 0.0097
0.2013 ± 0.0056
0.2013 ± 0.0031
0.2013 ± 0.0018
0.2013 ± 0.0010 .

As we can see, if we knew perfectly π1 and π2 , already a sample of a few thousands
individuals would allow us to predict the fraction of tagged positives with a relative
uncertainty of a few percent. However there are other effects to be taken into account:
• there is uncertainty about π1 and π2 ;
• the proportion of infectees in the sample is different from that in the population
(that is, in general ps differs from p);
• the inference from the observed numbers of positives to ps , and then to p, has
to be done using sound probabilistic inferential methods.

33

5.2

Taking into account the uncertainty on π1 and π2

As we have seen in Sec. 4.4, the way to take into account all possible values of π1 and
π2 , using the rules of probability theory, consists in evaluating the following integral
Z 1Z 1
f (nP | ns , ps ) =
f (nP | ns , ps , π1 , π2 ) · f (π1 ) · f (π2 ) dπ1 dπ2 .
(48)
0

0

Before tacking the problem of how to evaluate this integral, a very important remark
on how we are going to model the uncertainty about π1 and π2 is in order.
• When we write f (nP | ns , ps , π1 , π2 ), we are assuming, trivially, the same exact
values of π1 and π2 for all the tests performed on the ns individuals of the
sample.
• If, instead, their value is uncertain, and we describe their uncertainty by f (π1 )
and f (π2 ), again it means that the same two numerical values influence the
results of the ns tests. But we just do not know with certainty which are these
values.
• In particular, associating to these two parameters the pdf’s f (π1 ) and f (π2 )
does not mean that π1 and π2 fluctuate from one test to one other. The two
pdf’s only describe the uncertainty on their numerical values.
• It is however reasonable to think that, from how the ‘test devices’ are built up,
each item could perform slightly differently than the other, but we shall ignore
these possible test-to-test fluctuations, although they could be taken into account just extending the model.
Going back to the practical issue of evaluating the integral, we use again Monte
Carlo methods, employing e.g. the R script provided in Appendix B.2, for the case
of ns = 10000 and ps = 0.1. The result, shown in the bottom plot of Fig. 10, is quite
impressive, compared to the top one, in which the precise values π1 = 0.978 and
π2 = 0.115 were used. The mean of the distribution is unchanged, as more or less
expected (see Sec. 4.4), but its standard deviation, which quantifies the uncertainty
of the prediction, increases by more than a factor six. We have then good reasons
to expect a similar effect when we will be interested in the ‘reverse’ problem, that is
inferring the number of infectees in the sample from the resulting number of positives.
Going into details, we see that the expected number of positives is essentially the same
of Sec. 2.2 (the reduction from 2060 to 2013 is simply due to the new reference values
for π1 and π2 we are using starting from Sec. 4). But this number is now accounted
by an uncertainty, which rises to about 10% of its value, when the uncertainties about
π1 and π2 are also taken into account.
34

0.012
0.010
0.006
0.000

0.002

0.004

f(nP)

0.008

mean = 2013
sigma = 31

1500

2000

2500

0.0010

mean = 2013
sigma = 196

0.0000

0.0005

f(nP)

0.0015

0.0020

nP

1500

2000

2500

nP

Figure 10: Probabilistic prediction of the numbers of positives, based on a hypothetical test
on 10000 individuals, exactly 1000 of them being infected. In the upper plot we use π1 = 0.978
and π2 = 0.115. In the lower plot we take into account their possible variability (see text).
The over-imposed curve shows a Gaussian with average 2013 and standard deviation 200, values
obtained by the approximated Eqs. (49) and (50). (The top histogram is repeated, with enlarged
horizontal scale, in Fig. 12.)

35

5.2.1

Approximated formulae

Although Monte Carlo integration is a powerful tool to solve at best non trivial
problems of this kind, it is very useful to get, whenever it is possible, approximate
solutions in order to have an idea, analyzing the resulting formulae, of how the result
depends on the assumptions. First at all, in analogy to what we have seen in Sec. 4.4,
we can be rather confident that the expected value of nP is not significantly affected,
as also confirmed by the Monte Carlo results shown in Fig. 10. The variance, given by
Eq. (44) is, instead, increased by terms whose approximated values can be obtained
by linearization.29 These are the resulting approximated expressions:30
E(nP ) ≈ E(π1 ) · ps · ns + E(π2 ) · (1 − ps ) · ns
(49)
2
σ (nP ) ≈ E(π1 ) · (1 − E(π1 )) · ps · ns + E(π2 ) · (1 − E(π2 )) · (1 − ps ) · ns
+ σ 2 (π1 ) · p2s · n2s + σ 2 (π2 ) · (1 − ps )2 · n2s .
(50)
Applying them to the case shown in Fig. 10 we obtain an expected value of 2013 and
a standard deviation of 200, in excellent agreement with the Monte Carlo result. In
order to have an idea of the deviation from ‘normality’ we also over-impose, to the
bottom histogram of the figure, the Gaussian having average and standard deviation
calculated by Eq. (49) and (50) – we remind that the top histogram has instead
strong theoretical reasons to be, with very good approximation, normally distributed
(a zoomed version of the same histogram is reported in Fig. 12).
As a further check, let us see what happens in the case of no infected individuals in the sample, that is ps = 0. The Monte Carlo results are shown in Fig. 11,
We see that, as already stated qualitatively in Sec. 2, number of positives can occur
well below the value one would compute only reasoning on rough estimates (1150
in this case). Therefore, since the formulae derived in that way were unreliable, a
probabilistic treatment of the problem is needed in order to take into account the
fact that fluctuations around expected values do usually occur. Also in this case the
approximated results obtained by Eqs. (49) and (50) are in excellent agreement with
the Monte Carlo estimates, yielding 1150 ± 222 (and, again, the Gaussian approximation is not too bad, at least within a couple of standard deviations from the mean
value). The approximation remains good also for high values of ps . For example, for
29

See Sec. 6.4 of Ref. [23] and Sec. 8.6 of Ref. [24].
The first two terms of the r.h.s. of Eq. (50) come from Eq. (44), in which the precise values π1 and
π2 have been replaced by their expected value. The other two terms are obtained by linearization,
yielding e.g. for the contribution due to π1 (remember that ps is, so far, a precise parameter)
30




 2
∂
π1 · ps · ns + π2 · (1 − ps ) · ns
∂π1

E(π1 ,π2 )

36

· σ 2 (π1 ) =

ps · ns

2

· σ 2 (π1 ) .

0.012
0.010
0.008
0.006

sigma = 32

0.000

0.002

0.004

f(nP)

mean = 1150

500

1000

1500

2000

0.0015

nP

0.0010

sigma = 218

0.0000

0.0005

f(nP)

mean = 1150

500

1000

1500

2000

nP

Figure 11: Same as Fig. 10, but in the case of no infected individual in the sample (ps = 0).
The over-imposed Gaussian has average 1150 and standard deviation 222.

37

ps →
E(fP ) →
ns :
100
300
1000
3000
10000
30000
100000

0.01
0.124
(0.032)
[0.038]
(0.018)
[0.028]
(0.010)
[0.024]
(0.006)
[0.022]
(0.003)
[0.022]
(0.002)
[0.021]
(0.001)
[0.021]

0.05
0.158

0.10
0.15
0.20
0.201
0.244
0.287
standard uncertainties
(0.031) (0.031) (0.030) (0.029)
[0.037] [0.036] [0.035] [0.034]
(0.018) (0.018) (0.017) (0.017)
[0.027] [0.026] [0.025] [0.024]
(0.010) (0.010) (0.009) (0.009)
[0.023] [0.022] [0.021] [0.020]
(0.006) (0.006) (0.005) (0.005)
[0.021] [0.020] [0.019] [0.018]
(0.003) (0.003) (0.003) (0.003)
[0.021] [0.020] [0.019] [0.018]
(0.002) (0.002) (0.002) (0.002)
[0.021] [0.020] [0.018] [0.017]
(0.001) (0.001) (0.001) (0.001)
[0.021] [0.019] [0.018] [0.017]

0.50
0.546
(0.025)
[0.027]
(0.014)
[0.018]
(0.008)
[0.014]
(0.005)
[0.012]
(0.002)
[0.012]
(0.001)
[0.011]
(0.001)
[0.011]

Table 2: Predicted fraction of tagged positives in a sample (fP ) as a function of the
assumed proportion of infected individuals in the sample (ps ), also taking into account the
uncertainty on the test parameters π1 and π2 (numbers in squared brackets – those in round
brackets are evaluated at the expected values of π1 and π2 ).
the quite high value of ps = 0.5, the Monte Carlo integration gives 5465 ± 116 versus
an approximated result of 5465 ± 118.
A natural question is how the results change not only with the proportion of
infectees in the sample, but also with the size of the sample. The answer is given
in Tab. 2, with ns varying, in steps of roughly half order of magnitude, from the
ridiculous value of 100 up to 100000 (that is ≈ 10k/2 , with k = 4, 5, . . . , 10). The
chosen values of ps are the same of p of Tab. 1. For an easier comparison, the fraction
fp of positively tagged individual is provided. The expected value of fP , depending
essentially only on ps , is reported in the second row of the table. Two standard
uncertainties are reported for each combination of ps and ns : the first, in round
brackets, only takes into account the two binomial distributions (‘statistical errors’,
in old style31 physicist’s jargon); the second, in square brackets takes into account also
the possible variability of π1 and π2 (‘systematic error’, in the same jargon). They
have all been evaluated by Monte Carlo, but the agreement with the approximated
formula (50) has been checked.
31

For this question see the ISO’s GUM [15].

38

5.3

General considerations on the approximated evaluation
of σ(nP ) by Eq. (50)

At this point some further remarks on the utility of Eq. (50) is in order. Its advantage,
within its limits of validity (checked in our case), is that it allows to disentangle the
contributions to the overall uncertainty. In particular we can rewrite it as
σ(nP ) ≈ σR (nP ) ⊕ σπ1 (nP ) ⊕ σπ2 (nP ) ,

(51)

that is a ‘quadratic sum’ (or ‘quadratic combination’, indicated by the symbol ‘⊕’)
of three contributions,
p
σR (nP ) =
E(π1 ) · (1 − E(π1 )) · ps · ns + E(π2 ) · (1 − E(π2 )) · (1 − ps ) · ns
σπ1 (nP ) = σ(π1 ) · ps · ns
σπ2 (nP ) = σ(π2 ) · (1 − ps ) · ns ,
due, as indicated by the suffixes, to the binomials (‘R’ standing for ‘random’), to the
uncertainty on π1 and to that on π2 .
This quadratic combination of the contributions can be easily extended, just dividing by ns , to the uncertainty on the fraction of positives, thus getting
σ(fP ) ≈ σR (fP ) ⊕ σπ1 (fP ) ⊕ σπ2 (fP ) ,

(52)

quadratic sum of
p
√
σR (fP ) =
E(π1 ) · (1 − E(π1 )) · ps + E(π2 ) · (1 − E(π2 )) · (1 − ps )/ ns (53)
σπ1 (fP ) = σ(π1 ) · ps
(54)
σπ2 (fP ) = σ(π2 ) · (1 − ps ) .
(55)
We see immediately, for example, that for ps around 0.1 the contribution due to π2
dominates over that due to π1 by a factor 0.022/0.007×0.9/0.1 ≈ 30. This allows us to
evaluate, on the basis of the Monte Carlo results shown in Tab. 2, the contribution due
the systematic effects alone. For example we get, for our customary values of ps = 0.1
and ns = 10000, σ(fP ) equal to 0.003 and 0.020, respectively.
√ Assuming a quadratic
combination, the contribution due to systematics is then 0.0202 − 0.0032 = 0.0198.
Besides questions of rounding,32 it is clear that the uncertainty is largely dominated
by the uncertainty on π1 and π2 . We can check this result by a direct, although
approximated, calculation using Eq. (54) and (55):
σπ1 (fP ) = 0.007 × 0.1 = 0.0007
σπ2 (fP ) = 0.022 × 0.9 = 0.0198
σπ1 (fP ) ⊕ σπ2 (fP ) ≈ σπ2 (fP ) = 0.0198 ,
32

Using the values 0.0196 and 0.0031 of Fig.10 we would get 0.194.

39

getting the same result.
Looking at the numbers of Tab. 2, we see √
that this effect starts already at
ns = 1000. For example, for ps = 0.1 we get 0.0222 − 0.0102 = 0.0196, twice
the standard uncertainty of 0.010 due to the binomials alone. The sample size at
which the two contributions have the same
√ weight in the global uncertainty is around
300 (for example, for ps = 0.1 we get 0.0262 − 0.0182 = 0.019). The take-home
message is, at this point, rather clear (and well known to physicists and other scientists): unless we are able to make our knowledge about π1 and π2 more accurate,
using sample sizes much larger than 1000 is only a waste of time.
However, there is still another important effect we need to consider, due to the
fact that we are indeed sampling a population. This effect leads unavoidably to extra
variability and therefore to a new contribution to the uncertainty in prediction (which
will be somehow reflected into uncertainty in the inferential process).
Before moving to this other important effect, let us exploit a bit more the approximated evaluation of σ(fP ). For example, solving with respect to ns the condition
σR (fP ) = σπ1 (fP ) ⊕ σπ2 (fP )
we get from Eqs. (53)-(55)
n∗s ≈

E(π1 ) · (1 − E(π1 )) · ps + E(π2 ) · (1 − E(π2 )) · (1 − ps )
,
σ 2 (π1 ) · p2s + σ 2 (π2 ) · (1 − ps )2

(56)

which gives a rough idea of the sample size above which the uncertainty due to
systematics starts to dominate. For example, for ps = 0.1 we get ns = 240 of the
order of magnitude (≈ 300) got from the Monte Carlo study. If we require, to be
safe, σπ1 (fP ) ⊕ σπ2 (fP ) = (2-3) × σR (fP ) we get ns ≈ 1000 and ns ≈ 2200, again
in reasonable agreement with the results of Tab. 2. We shall go through a more
complete analysis of n∗s in Sec. 6.4, in which a further contribution to the uncertainty
will be also taken into account.
5.3.1

Including in the approximated formulae the contribution of the
uncertainty on ps due to sampling

Next section will be dedicated to the effect of sampling ns individuals from a population. However, having taken some confidence with the approximated formulae, we
can already extend them in order to see how the uncertain ps , characterized by its
expected value E(ps ) and standard uncertainty σ(ps ), whose evaluation will be the
subject of Sec. 6, affects our prediction about the number of individuals resulting
positive in the test. In the approximated expression for the expected value of nP

40

(Eq. 49) we have to replace ps by its expected value E(ps ), while in the variance we
have to add a term again obtained by linearization,33 thus getting
E(nP ) ≈ E(π1 ) · E(ps ) · ns + E(π2 ) · (1 − E(ps )) · ns
(57)
2
σ (nP ) ≈ E(π1 ) · (1 − E(π1 )) · E(ps ) · ns + E(π2 ) · (1 − E(π2 )) · (1 − E(ps )) · ns
+ σ 2 (π1 ) · E2 (ps ) · n2s + σ 2 (π2 ) · (1 − E(ps ))2 · n2s
(58)
+ σ 2 (ps ) · (E(π1 ) − E(π2 ))2 · n2s .
As far as the fraction of positives is concerned, we have the following four contributions to the global uncertainty,
σ(fP ) ≈ σR (fP ) ⊕ σπ1 (fP ) ⊕ σπ2 (fP ) ⊕ σps (fP ) ,

(59)

the first three given by Eqs. (53-55), in which ps has to be replaced by its expected
value E(ps ), and the fourth term being
σps (fP ) = σ(ps ) · |E(π1 ) − E(π2 )| .

(60)

(Note that also the fourth term is of ‘random nature’, although, from the ‘perspective’
we are now seeing the problem it could be considered as a third contribution to
systematics.34 )

6

Sampling a population

In Sec. 5 we went through the question of predicting the number of positives when we
plan to test an entire sample of ns individuals, a fraction ps of which is assumed to
be infected. At this point we have to take into account the last source of uncertainty
we have to deal with. If we sample at random ns individuals out of the N of the
entire population, the sample will contain a fraction of infected ps usually different
from the (‘true’) fraction p of the population and described by f (ps | ns , N, p). Once
the pdf of ps has been somehow evaluated, we can get the pdf of interest, that is
f (nP | ns , N, p), extending Eq. (48) to
Z 1Z Z1 1
f (nP | ns , N, p) =
f (nP | ns , ps , π1 , π2 )·f (ps | p, ns, N)·f (π1 )·f (π2) dps dπ1 dπ2 .
0

0 0

(61)

33



2

The contribution to σ (nP ) due to σ(ps ), evaluated by linearization, is given by

2
 2
∂
· σ 2 (ps ) = E(π1 ) · ns − E(π2 ) · ns · σ 2 (ps ) .
π1 · ps · ns + π2 · (1 − ps ) · ns
∂ps
E(π1 ,π2 ,ps )

34

Note that this terminology is a matter of convention and habits. From a probabilistic point of
view we just apply probability theory to all quantities with respect to which we are in condition of
uncertainty, considering the ‘fixed ones’ as conditionands.

41

6.1

Proportion of infected individuals in the random sample
– Binomial and hypergeometric distributions

We have already reminded and made use of the binomial distribution, assumed well
known to the reader. A related problem in probability theory is that of extraction
without replacement, which we introduce here for two reasons. The first is that it
is little known even by many practitioners (we think e.g. to ourselves and to our
colleagues physicists). The second is that some care is needed with the parameters
used in literature and in scientific/statistical libraries of computer languages.
Let us imagine an urn containing m white and n black balls. Let us imagine then
that we are going to take out of it, at random, k balls and that we are interested
in the number X of white balls that we shall get (for convenience of the reader,
and also for us who never worked before with such a distribution, we use the same
idealized objects and symbols of the R help page – obtained e.g. by ‘?dhyper’). The
probability distribution of X is known as hypergeometric.35 In short, referring to the
parameters of the probability functions of the R language (see footnote 35),
X ∼ HG(m, n, k)
35

Some care is needed with this distribution because, as it is easy to understand, different sets of
parameters can be used. For example, the app already suggested [22] uses
X

∼

HG(n, N, M ) ,

with n the sample size, N the population size and M the number of white balls, thus leading to
the following correspondence with respect to the parameters of the probability functions of the R
language, to which we are going to adhere in the text
app ←→ R
n ←→ k
N
M

←→
←→

m+n
m.

Expected value and variance are, using the app convention,
E(X) =
σ 2 (X) =

M
N 
 

M
N −n
M
n
·
.
· 1−
N
N
N −1

n

(In Wikipedia [25] there is a similar convention, apart from the names, being the ‘random variable’
indicated by k and the number of ‘white balls in the urn’ by K.)

42

with expected value and variance
m
m+n 
 

m+n−k
n
m
2
·
.
·
σ (X) = k ·
m+n
m+n
m+n−1
E(X) = k ·

In terms of the proportion of ‘objects’ having the characteristic of interest (‘white’),
their fraction in the urn is then assumed to be p = m/(m + n), corresponding, in our
problem, to the proportion of infectees. Using the symbol ns for the sample size k,
as we have done so far, and N for the total number of individuals in the population,
the above equations can be conveniently rewritten as
E(X) = p · ns

(62)

σ 2 (X) = ns · p · (1 − p) ·



N − ns
N −1



.

(63)

The expression of the expected value is identical to that of a binomial distribution,
while that of the variance differs from it by a factor depending on the difference
between the population size and the sample size, vanishing when ns is equal to N.
That is simply because in that case we are going to empty the ‘urn’ and therefore we
shall count exactly the number of ‘white balls’. When, instead, ns is much smaller
than N (and then N ≫ 1), we recover the variance of the binomial. In practice it
means that the effect of replacement, related to the chance to extract more than once
the same object, becomes negligible.
Moving to our problem, the role of the generic variable X is played by the number
of infectees in the sample, indicated by nI in the previous sections. In terms of their
proportion, being ps = X/k = nI /ns , we get
 
m
nI
E(ps ) = E
=
= p,
(64)
ns
m+n
as intuitively expected. As far as the variance is concerned, being simply σ(ps ) =
σ(nI )/ns , we get


σ 2 (nI )
1
N − ns
2
σ (ps ) =
=
(65)
· p · (1 − p) ·
n2s
ns
N −1

1
ns 
≈
· p · (1 − p) · 1 −
(66)
ns
N
being N ≫ 1 in all practical cases of (our) interest.
43

Finally, if the sample size is much smaller than the population size, then the last
factor can be neglected and the variance can be approximated by p · (1 − p)/ns , thus
yielding
s
p · (1 − p)
,
(67)
σ(ps )|ns ≪N ≈
ns
the well known standard deviation of the fraction of successes in a binomial distribution with ns trials, each with probability p. The reason is that – it is worth
repeating it – when the sample size is much smaller than the population size, then
we can neglect the effects of no-replacement and consider the trials as (conditionally)
independent Bernoulli processes, each with probability of success p.

6.2

Expected number of positives sampling of a population
(assuming exact values of π1 and π2 )

At this point we can convolute the uncertainty on the number of positives in a sample,
analyzed in Sec. 5, with the uncertain value of ps due to sampling:
Z 1
f (nP | ns , N, p, π1 , π2 ) =
f (nP | ns , ps , π1 , π2 )·f (ps | p, ns , N) dps .
(68)
0

We start, as usual, with our exact reference values of test sensitivity and specificity of
97.8% and 88.5% (π1 = 0.978 and π2 = 0.115), respectively, and perform the integration by Monte Carlo.36 Some results are shown in Fig. 12, where, for comparison with
what we have seen in the previous sections, a sample size of 10000 individuals is used,
taken from a population of 10000 (top histogram), 100000 (middle) and 1000000 (bottom), and assuming p = 0.1. [Note that first case corresponds exactly to the assumed
value of ps = 0.1 shown in the top plot of Fig. 10, since, being ns = N, the standard
uncertainty on ps vanishes.] Increasing the population size the standard deviation increases, as an effect of σ(ps ), although this growth saturates for N a bit higher than
≈ 10 × ns , above which the size dependent factor of Eq. (66) becomes negligible. In
fact, the asymptotic value, given by Eq. (67) is in this case σ(ps )|N →∞ = 0.0030. For
N/ns = 10 the standard uncertainty on ps becomes 0.00285, vanishing for ns = N
(the value of 0.0015, half of the asymptotic one, is reached for N = 4/3 × ns ).
6.2.1

Approximated results

It is interesting to compare the Monte Carlo results of Fig. 12 to those obtained by
the approximated values of expected value and standard deviation given by Eqs. (57)(58) just putting σ(π1 ) = σ(π2 ) = 0. The contribution to the uncertainty due to the
36

The R code for N = 105 , ns = 104 and p = 0.1 is provided in Appendix B.3.

44

0.000 0.002 0.004 0.006 0.008 0.010 0.012

f(nP)

mean = 2013
sigma = 31

1900

1950

2000

2050

2100

2150

0.010

nP

0.004

0.006

sigma = 39

0.000

0.002

f(nP)

0.008

mean = 2013

1900

1950

2000

2050

2100

2150

0.010

nP

0.004

0.006

sigma = 40

0.000

0.002

f(nP)

0.008

mean = 2013

1900

1950

2000

2050

2100

2150

nP

Figure 12: Probabilistic prediction of the numbers of positives in a sample of 10000 individuals
taken from a population of 10000, 100000 and 1000000 individuals (in order, from top to
bottom), 10% of which are infected (p = 0.1), assuming π1 = 0.978 and π2 = 0.115.

45

two binomials of Fig. 9 is σR (nP ) = 30.6 (rounded to 31 in Fig. 12), while those due
to σ(ps ) are equal to 0, 24.6 and 25.9, for the three population sizes. The combined
standard uncertainties are then 30.6, 39.3 and 40.1, in perfect agreement with the
results shown in Fig. 12.

6.3

Detailed study of the four contributions to σ(fP )

At this point it is time to release the limiting assumption of exact values of sensitivity
and specificity, i.e. σ(π1 ) = σ(π2 ) = 0. Moreover, having checked that the approximated formulae can take into account with great accuracy also the contribution due
to the uncertain value of ps , we find it interesting and useful to study the individual
contributions to the uncertainty with which we can forecast the fraction fP of tested
individuals resulting positive. For the reader’s convenience, we summarize here the
relevant, approximated expressions, making also use, in order to simplify them, of
the equality E(ps ) = p :
E(fP ) ≈ E(π1 ) · p + E(π2 ) · (1 − p)
σ(fP ) ≈ σR (fP ) ⊕ σps (fP ) ⊕ σπ1 (fP ) ⊕ σπ2 (fP )
p
√
E(π1 ) · (1 − E(π1 )) · p + E(π2 ) · (1 − E(π2 )) · (1 − p)/ ns
σR (fP ) =
σπ1 (fP ) = σ(π1 ) · p
σπ2 (fP ) = σ(π2 ) · (1 − p) .
σps (fP ) = σ(ps ) · |E(π1 ) − E(π2 )|
p
√
≈ |E(π1 ) − E(π2 )| · p · (1 − p) · (1 − ns /N)/ ns

(69)
(70)
(71)
(72)
(73)
(74)

We can note that σπ2 (fP ) and σπ1 (fP ) are independent of the sample size ns , while
√
σR (fP ) and σps (fP ) exhibit the typical ‘statistical dependence’ ∝ 1/ ns . Therefore
we shall refer hereafter to σR (fP ) and σps (fP ) as random (or statistical) contributions;
to the others as contributions due to systematics, which cannot be improved increasing
the sample size.
The upper plot of Fig. 13 shows, for our reference value of p = 0.1 and for
uncertain π1 and π2 (summarized as π1 = 0.978 ± 0.007 and π2 = 0.115 ± 0.022), the
relative uncertainty on fP , that is σ(fP )/E(fP ), as a function of ns , highlighting the
different contributions to the total uncertainty. The horizontal lines represent the
two systematic contributions, independent from ns , while their quadratic sum does
not appears in the plot, because it overlaps practically exactly with the dominant
systematic contribution, due to the uncertain π2 . The ‘straight lines with negative
slopes’ (in log-log plot, which notoriously linearizes power laws) are the individual
statistical contributions (solid and dashed, respectively – see the figure caption for
details) and their quadratic sum (dotted). The uppest (dotted brown) curve is the
46

0.100
0.020
0.005
0.001

σ(fP) E(fP)

1e+02

1e+03

1e+04

1e+05

1e+06

1e+05

1e+06

0.020
0.005
0.001

σ(fP) E(fP)

0.100

ns

1e+02

1e+03

1e+04

ns
Figure 13: Contributions to the relative uncertainty on the fraction of positives as a function
of the sample size ns , assuming it much smaller than the population size N , for a proportion of
infected individuals p = 0.1. The solid blue line with negative slope is the contribution from
σR (fP ), the dashed blue one is the contribution from σps (fP ), the dotted line is the ‘quadratic
sum’ of the two; the lower horizontal red one is the contribution from σπ1 (fP ) and the upper
horizontal one is the contribution from σπ2 (fP ) (a dotted red line, showing their ‘quadratic
sum’ is indeed overlapping the π2 contribution). The overall uncertainty is shown by the uppest
curve (dotted brown). The upper plot is for a standard uncertainty on π2 σ(π2 ) = 0.022. The
lower plot is for the case of uncertainty reduced to σ(π2 ) = σ(π1 ) = 0.007.

47

overall uncertainty, dominated at small ns by the statistical contributions and at high
ns by the systematic ones, namely by σπ2 (fP ). (We shall come in a while into the
meaning and the importance of the vertical line.)
Since the dominant contribution due to σ(π2 ) limits the relative uncertainty on
fP to about 10%, reached for ns above a few thousands, it is interesting to see what
we would gain reducing σ(π2 ) to the value of σ(π1 ). This is done in the bottom
plot of Fig. 13, which shows a clear improvement, although the contribution due to
σ(π2 ) still dominates with respect to that due to σ(π1 ), because the former enters, for
p = 0.1, with a weight 9 times higher than the latter, as it results from Eqs. (72) and
(73). Moreover, since all contributions to the uncertainty on fP depend also on p,
we report in Fig. 14 the case of a supposed proportion of infectees37 as high as 50%
(i.e. p = 0.5). One of the remarkable difference with respect to Fig. 13 is that the
contribution from σps (fP ) becomes larger than that from σR (fP ) (remaining always
‘parallel’ as a function of ns in ‘log-log’ plots, since they depend on the same power
of the sample size). Indeed, σps (fP ) starts dominating from p ≈ 0.15 up to p ≈ 0.95,
as shown in Fig. 15, in which the ratio σps (fP )/σR (fP ) as a function of p, is reported,
exhibiting a whale-like shape.
As a further example we show in Fig. 16 the contributions to the relative uncertainty of fP for the case of improved specificity of the test, i.e. reducing the expected
value of π2 from 0.115 to 0.022, keeping its uncertainty equal to that of π1 , that is
0.007. This means that we consider specificity equal to sensitivity, both in expected
value and in uncertainty. In practice this is done swapping the parameters of the
related Beta distributions, that is r2 = s1 and s2 = r1 (see Sec. 4.2).
In order to make evident the differences with what has been shown in the previous
cases, we plot σps (fP )/E(fP ) for both p = 0.1 (upper plot) and p = 0.5 (lower plot).
In particular, in order to see the effect of this last improvement of the specificity
(i.e. increasing its expected value from 0.885 to 0.978, keeping the same standard
uncertainty) we need to compare the upper plot of Fig. 16 with the lower plot of
Fig. 13; the lower plot of Fig. 16 with the lower plot of Fig. 14. The result is, at least
at a first sight, quite counter-intuitive, since to a sizable improvement in specificity
there is a reduction in the relative accuracy with which the fraction of positives is
expected (effect particularly important for p = 0.1). We shall comment about it in
the next sub-section, in which we start describing the vertical lines in the plots of
Figs. 13, 14 and 16, commenting on their importance.
37

We remind once more that this paper is rather general, although motivated by Covid-19 related
issues, and therefore we also analyze the possibility of very large p.

48

0.100
0.020
0.005
0.001

σ(fP) E(fP)

1e+02

1e+03

1e+04

1e+05

1e+06

1e+05

1e+06

0.020
0.005
0.001

σ(fP) E(fP)

0.100

ns

1e+02

1e+03

1e+04

ns
Figure 14: Same as Fig. 13 for a proportion of infected individuals of 50% (p = 0.5). In this
case the contribution from sampling the population σps (fP ) is larger than that from σR (fP ).
Note that in the lower plot the two solid horizontal lines collapse into a single one, being
the contribution from σπ1 (fP ) and σπ2 (fP ) equal. It is, instead, visible, with respect to the
plots of Fig. 13 the horizontal dotted line showing the quadratic combination of the systematic
contributions, reached asymptotically by the top dotted curve representing the global relative
uncertainty on fP .

49

0.0

0.5

1.0

1.5

Ratio of ’statistical’ contributions

0.0

0.2

0.4

0.6

0.8

1.0

p

Figure 15: Ratio of σps (fP ) to σR (fP ) as a function of the population fraction of infected p.

6.4

Balance between statistical and systematic contributions
to the uncertainty on fP

The vertical dashed line in the plots of Figs. 13, 14 and 16 indicates the critical value
n∗s at which the contribution to total uncertainty due to σπ2 (fP ) and σπ1 (fP ) is equal
to that due to σR (fP ) and σps (fP ), that is for ns = n∗s statistical and systematic
contributions are equal. It follows that, due to the quadratic combination rule, the
global uncertainty√at that critical value of the sample size will be larger than each of
them by a factor 2.
Being n∗s an important parameter in order to plan a test campaign, it is worth getting its closed, although approximated expression, obtained extending the condition
(56) to
σR2 (fP ) + σp2s (fP ) = σπ21 (fP ) + σπ22 (fP ) ,

(75)

The result, under the minimal assumption N ≫ 1, is
n∗s

=



2
 

E(π1 ) − E(π2 ) ·p·(1 − p) + E(π1 )·(1 − E(π1 ))·p + E(π2 )·(1 − E(π2 ))·(1 − p)

 

.
σ 2 (π1 ) · p2 + σ 2 (π2 ) · (1 − p)2 + (E(π1 ) − E(π2 ))2 · p · (1 − p) /N
(76)

(Note how in the limit N ≫ ns , i.e. N → ∞, the second term at the denominator
50

0.100
0.020
0.005
0.001

σ(fP) E(fP)

1e+02

1e+03

1e+04

1e+05

1e+06

1e+05

1e+06

0.020
0.005
0.001

σ(fP) E(fP)

0.100

ns

1e+02

1e+03

1e+04

ns
Figure 16: Same quantities of Figs. 13 and 14, but in the symmetric case of specificity
equal to sensitivity, i.e. E(π2 ) = 1 − E(π1 ) = 0.022, again with equal uncertainties, i.e.
σ(π2 ) = σ(π1 ) = 0.007. The upper plot, for p = 0.1, has to be compared to the lower plot of
Fig. 13; the lower plot, for p = 0.5, has to be compared to the lower plot of Fig. 14.

51

of Eq. (76) can be neglected.38 ) The top plot of Fig 17 shows the dependence of
n∗s on p, for: our reference values of σ(π1 ) and σ(π2 ) (solid line – see also top plots
of Figs. 13 and 14); the improved case of σ(π2 ) = σ(π1 ) = 0.007 (dashed line – see
also bottom plots of Figs. 13 and 14); the mirror-symmetric case in which E(π2 ) =
1 − E(π1 ) = 0.022 and σ(π2 ) = σ(π1 ) = 0.007 (dotted line – see also Fig. 16). Once
we know the dependence of n∗s on p, since the uncertainty on fP depends on ns and
p, we can evaluate the relative uncertainty on the predicted fraction of positives that
will result from the test campaign, as a function of p under the condition ns = n∗s ,
that is σ(fP )/E(fP )|ns =n∗s . The result is shown in the bottom plot of Fig 17 for the
three cases of the upper plot of the same figure.
When we reduce the uncertainty about σ(π2 ), keeping constant its expected value,
the systematic contribution to the uncertainty is reduced and then, as we have already
learned from Figs. 13, 14 and 16, it becomes meaningful to analyze larger samples.
We can then predict the fraction of individuals tagged as positive with improved
accuracy, i.e. σ(fP )/E(fP ) decreases. This intuitive reasoning is confirmed by the
plots of Fig 17, moving from the solid curves to the dashed ones. Instead, improving
the specificity to 0.885 to 0.978, i.e. reducing E(π2 ) from 0.115 to 0.022, keeping
the same uncertainty of 0.007, leads to surprising results at low values of p, at least
at a first sight (dashed curves → dotted curves). In fact, one would expect that
from this further improvement in the quality of the test (which definitively makes
a difference when testing a single individual, as discussed in Sec. 4) should follow a
general improvement in the prediction of the fraction of positives.
The reason of this counter-intuitive outcome is due to the combination of two
effects. The first is the dependence on E(π1 ) and E(π2 ) of the statistical contributions
to the uncertainty, as we can see from Eqs. (71) and (74). The second is that,
decreasing E(π2 ), the expected value of fP decreases too (less ‘false positives’) and
therefore the relative uncertainty on fP , i.e. σ(fP )/E(fP ), increases. While the
second effect is rather obvious and there is little to comment, we show the first
one graphically, for p = 0.1 at which the effects becomes sizable, in the three plots
of Fig. 18: the upper plot for our reference values of π1 and π2 , the middle one
improving σ(π2 ) to 0.007, and the bottom one also reducing the expected value of
π2 to 0.022. But differently from Figs. 13, 14 and 16, these plots show σ(fP ) instead
of σ(fP )/E(fP ), so that we can focus only on the contributions to the uncertainty,
38

Indeed, in such a limit the condition (75) becomes

 

2
E(π1 ) · (1 − E(π1 )) · p + E(π2 ) · (1 − E(π2 )) · (1 − p) + (E(π1 ) − E(π2 )) · p · (1 − p) ≈


ns · σ 2 (π1 ) · p2 + σ 2 (π2 ) · (1 − p)2 ,

whose solution is trivial, differing from Eq. (76) just for the term at the denominator containing the
factor N −1 .

52

5000
1000
100

200

500

n s*

0.0

0.2

0.4

0.6

0.8

1.0

0.8

1.0

0.20

p

0.05
0.01

0.02

σ(fP) E(fP)

0.10

ns = ns*

0.0

0.2

0.4

p

0.6

Figure 17: Top plot: dependence of n∗s on p for the standard values of σπ1 and σπ2 (solid
line), for σπ1 = σπ2 = 0.007 (dashed line) and for specificity equal to sensitivity, i.e. E(π2 ) =
1 − E(π1 ) = 0.022 (dotted line). Bottom plot: relative uncertainty on fP at ns = n∗s for the
same cases.

53

2e−02
5e−03
1e−03

σ(fP)

2e−04

1e+03

1e+04

1e+05

1e+06

1e+02

1e+03

1e+04

1e+05

1e+06

1e+02

1e+03

1e+04

1e+05

1e+06

5e−03
5e−03
2e−04

1e−03

σ(fP)

2e−02

2e−04

1e−03

σ(fP)

2e−02

1e+02

ns

Figure 18: Contributions to σ(fP ) varying σ(π2 ) and E(π2 ) for p = 0.1 (see text).
54

NI

NN I

p

ns

nI

nI

nN I

π1

nN I

π1

π2

nPI

ns

π2

nPI

nPN I

nPN I

nP

nP

Figure 19: Graphical network of Fig. 9, augmented by the sampling process, modeled by a
hypergeometric distribution (left) or by a binomial distribution (right) with p = NI /(NI +NN I ).

not ‘distracted’ by the variation of the expected value of fP . Moving from the top
plot to the middle one, only the contribution due to π2 is reduced, all the others
remaining exactly the same. Then, when we increase the specificity, i.e. we reduce
E(π2 ) from 0.115 to 0.022, keeping unaltered its uncertainty, its contribution to σ(fP )
is unaffected, while the statistical contributions do change. In particular σR (fP ) is
strongly reduced, while σps (fP ) increases a little bit. The combined effect is a decrease
of the overall statistical contribution, thus lowering n∗s .
Summing up, the combination of the two plots of Fig. 17 gives at a glance, for
an assumed proportion of infectees p, an idea of the ‘optimal’ relative uncertainty we
can get on fP (bottom plot) and the sample size needed to√achieve it (upper plot).
We remind that the lowest relative uncertainty, equal to 1/ 2 of the value shown in
the plot, is reached when the sample size ns is about one order of magnitude larger
than n∗s , i.e. when the random contribution to the uncertainty is absolutely negligible
and
√ any further increase of ns not justifiable. But, anyway – think about it – being
1/ 2 ≈ 0.7, is it worth increasing so much (≈ 10 times) the sample size in order to
reduce σ(fP ) by only 30%?

55

7

Measurability of p

It is now time to put together all the items discussed so far and to move to the
question of how well we can measure the proportion p of infectees in a population of
N individuals, out of which ns have been sampled at random and tested, resulting
in a total of nP positives. Again we proceed by steps. In this section we tackle the
question of how the expected number of positives and its uncertainty depend on the
proportion p of infectees in the population. The real inferential problem, consisting
in stating which values of p are more or less believable, will be analyzed in Secs. 8
and 9.

7.1

Probabilistic model

The graphical model describing the quantities of interest is shown in the left hand
network of Fig. 19, based on that of Fig. 9, to which we have added parents to the
nodes nI and nN I , the number of Infected and Not Infected in the sample, respectively. More precisely, the number of infectees nI in the sample is described by a
hypergeometric distribution, that is
nI ∼ HG(NI , NN I , ns ) ,
with NI and NN I the numbers of infected and not infected individuals in the population. Then, the number nN I of not infected people in the sample is deterministically
related to nI , being nN I = ns − nI .
However, since in this paper we are interested in sample sizes much smaller than
those of the populations, we can remodel the problem according to the right hand
network of Fig. 19, in which nI is described by a binomial distribution, that is
nI ∼ Binom(ns , p) ,
with p = NI /(NI + NN I ). This simplified model has been re-drawn √
in the network
shown in the left hand side of Fig. 20, indicating by the symbol ‘ ’ the certain
variables in the game (indeed those which are for some reason assumed), in contrast
to the others, which are uncertain and whose values will be ranked in degree of belief
following the rules of probability theory. Note that in this diagram π1 and π2 are
assumed to be exactly known. Instead, as we have already seen in Sec. 4, their values
are uncertain and their probability distribution can be conveniently modeled by Beta
probability functions characterized by parameters r’s and s’s. The graphical model
which takes into account also the uncertainty about π1 and π2 is drawn in the same
Fig. 20 (right side).
56

p

√

√

ns √
r1

nI

ns

s1

nN I

nI

π2

√
√ r2

√

√

π1

p

s2
√

nN I

π1

π2

√

√

nPI

nPI

nPN I

nPN I

nP

nP

Figure 20:

Simplified graphical model of Fig. 19 rewritten in order to make explicit
√
‘known’/‘assumed’ quantities, tagged by the symbol ’ ’, and the uncertain ones. In particular, in the left hand diagram precise values of π1 and π2 are assumed, while in the the right
hand one the uncertainty on their values is modeled with Beta pdf’s with parameters (r1 , s1 )
and (r2 , s2 ).

We have already discussed extensively, in Sec. 6, how the expectation of nP ,
and therefore of the fraction on positives in the sample, fP , depends on the model
parameters. Now we go a bit deeper into the question of the dependence of fP on
the fraction of infectees in the population and, more precisely, which are the ‘closest’
(to be defined somehow) two values of p, such that the resulting fP ’s are ‘reasonably
separated’ (again to be defined somehow) from each other. Moreover, instead of
simply relying on the approximated formulae developed in Sec. 6, we are going to
use Monte Carlo methods in different ways: initially just based on R random number
generators; then using (well below its potentials!) the program JAGS, which will
then be used in Sec. 8 for inferences. However we shall keep using the approximated
formulae for cross check and to derive some useful, although approximated, results
in closed form.

7.2

Monte Carlo estimates of f (nP ) and f (fP )

Analyzing the graphical model in the right hand side of Fig. 20, the steps we have to
go through become self-evident:
57

1. generate a value of nI according to a binomial distribution, then calculate nN I ;
2. generate a value of π1 and of π2 according to Beta distributions;
3. generate a value of nPI and a value of nPNI according to binomial distributions;
4. sum up nPI and nPNI in order to get nP .
7.2.1

Using the R random number generators

The implementation in R is rather simple, thanks also to the capability of the language to handle ‘vectors’, meant as one dimensional arrays, in a compact way. The
steps described above can then be implemented into the following lines of code:
n.I
n.NI
pi1
pi2
nP.I
nP.NI
nP

<<<<<<<-

rbinom(nr, ns, p)
ns - n.I
rbeta(nr, r1, s1)
rbeta(nr, r2, s2)
rbinom(nr, n.I, pi1)
rbinom(nr, n.NI, pi2)
nP.I + nP.NI

# 1.
# 2.
# 3.
# 4.

We just need to define the parameters of interest, including nr, number of extractions, run the code and add other instructions in order to print and plot the results
(a minimalist script performing all tasks is provided in Appendix B.5). The only
instructions worth some further comment are the two related to the step nr. 3. A
curious feature of the ‘statistical functions’ of R is that they can accept vectors for
the number of trials and for probability at each trial, as it is done here. It means
that internally the random generator is called nr times, the first time e.g. with n.I[1]
and pi1[1], the second time with n.I[2] and pi1[2], and so on, thus avoiding us to
use explicit loops. Note that, if precise values of π1 and π2 were assumed, then we
just need to replace the two lines of step nr. 2 with the assignment of their numeric
values.
Figure 21 shows the results obtained for some values of p and ns , and modeling
the uncertainty of π1 and π2 in our default way, summarized by π1 = 0.978 ± 0.007
and π2 = 0.115 ± 0.022. The values of ns have been chosen in steps of roughly half
order of magnitude in the region of n∗s of interest, as we have learned in Sec. 6. We see
that for the smallest ns shown in the figure, equal to 300, varying p by 0.1 produces
distributions of fP with quite some overlap. Therefore with samples of such a small
size we can say, very qualitatively that we can resolve different values of p if they
do not differ less than ≈ O(0.1). The situations improves (the separation roughly
doubles) when we increase ns to 1000 or even to 3000, while there is no further gain
58

p = 0.20
m = 0.288
sd = 0.031

p = 0.30
m = 0.374
sd = 0.032

p = 0.40
m = 0.460
sd = 0.032

20

p = 0.10
m = 0.201
sd = 0.030

0

10

f (f P )

30

40

ns = 300
p = 0.00
m = 0.115
sd = 0.028

0.1

0.2

0.3

0.4

0.5

p = 0.10
m = 0.201
sd = 0.023

p = 0.20
m = 0.287
sd = 0.023

p = 0.30
m = 0.374
sd = 0.022

p = 0.40
m = 0.460
sd = 0.021

20

p = 0.00
m = 0.115
sd = 0.024

0

10

f (f P )

30

40

ns = 1000

0.1

0.2

0.3

0.4

0.5

p = 0.10
m = 0.201
sd = 0.021

p = 0.20
m = 0.287
sd = 0.019

p = 0.30
m = 0.374
sd = 0.018

p = 0.40
m = 0.460
sd = 0.016

20

p = 0.00
m = 0.115
sd = 0.022

0

10

f (f P )

30

40

ns = 3000

p = 0.10
m = 0.201
sd = 0.020

0.3

ns = 10000
p = 0.20
m = 0.288
sd = 0.018

0.4

p = 0.30
m = 0.374
sd = 0.016

0.5

p = 0.40
m = 0.460
sd = 0.014

20

p = 0.00
m = 0.115
sd = 0.022

0.2

0

10

f (f P )

30

40

0.1

0.1

0.2

fP

0.3

0.4

0.5

Figure 21: Predictive distributions of fP as a function of p and ns for our default uncertainty
on π2 , summarized as π2 = 0.115 ± 0.022.

59

reaching ns = 10000. This is in agreement with what we have learned in the previous
section.
Since, as we have already seen, the limiting effect is due to systematics, and in
particular, in our case, to the uncertainty about π2 , we show in Fig. 22 how the result
changes if we reduce σ(π2 ) to the level of σ(π1 ).39 As we can see (a result largely
expected), there is quite a sizable improvement in separability of values of p for large
values of ns . Again qualitatively, we can see that values of p which differ by ≈ O(0.01)
can be resolved.
Finally we show in Fig. 23 the case in which sensitivity and specificity are equal,
both as expected value and standard uncertainty. The first thing we note in these
new histograms is that for p = 0 they are no longer symmetric and Gaussian-like.
This is due to the fact that no negative values of nP are possible, and then there is a
kind of ‘accumulation’ for low values of nP , and therefore of fP (this kind of skewness
is typical of all probability distributions of variables defined to be positive and whose
standard deviation is not much smaller than the expected value – think e.g. at a
Poissonian with λ = 2).
7.2.2

Using JAGS

Let us repeat the Monte Carlo simulation improperly using the program JAGS [10],
interfaced to R via the package rjags [12]. JAGS is a powerful tool developed, as
open source, multi-platform clone of BUGS,40 to perform Bayesian inference by Markov
Chain Monte Carlo (MCMC) using the Gibbs Sampler algorithm, as its name reminds,
acronym of Just Another Gibbs Sampler. For the moment we just get familiar with
JAGS using it as kind of ‘curious’ random generator.
The first thing to do is to write down the probabilistic model that relates the
different variables that enter the game. For example, the left hand graphical model
of Fig. 20 is implemented in JAGS by the following self-explaining piece of code
model {
n.I ~ dbin(p, ns)
n.NI <- ns - n.I
nP.I ~ dbin(pi1, n.I)
39

This can be done evaluating r2 and s2 from Eqs. (33) and (34) with µ = 0.978 and σ = 0.007.
Introducing MCMC and related algorithms goes well beyond the purpose of this paper and we
recommend Ref. [26]. Moreover, mentioning the Gibbs Sampler algorithm applied to probabilistic
inference (and forecasting) it is impossible not to refer to the BUGS project [27], whose acronym
stands for Bayesian inference using Gibbs Sampler, that has been a kind of revolution in Bayesian
analysis, decades ago limited to simple cases because of computational problems (see also Sec. 1 of
Ref.[10]). In the project web site [28] it is possible to find packages with excellent Graphical User
Interface, tutorials and many examples [29].
40

60

60

ns = 300
p = 0.20
m = 0.288
sd = 0.027

p = 0.30
m = 0.374
sd = 0.028

p = 0.40
m = 0.460
sd = 0.029

40

p = 0.10
m = 0.201
sd = 0.024

0

20

f (f P )

p = 0.00
m = 0.115
sd = 0.020

p = 0.10
m = 0.201
sd = 0.014

0.3

ns = 1000
p = 0.20
m = 0.288
sd = 0.015

0.4

p = 0.30
m = 0.374
sd = 0.016

0.5

p = 0.40
m = 0.460
sd = 0.017

40

p = 0.00
m = 0.115
sd = 0.012

0.2

0

20

f (f P )

60

0.1

p = 0.10
m = 0.201
sd = 0.010

0.3

ns = 3000
p = 0.20
m = 0.288
sd = 0.010

0.4

p = 0.30
m = 0.374
sd = 0.010

0.5

p = 0.40
m = 0.460
sd = 0.010

40

p = 0.00
m = 0.115
sd = 0.009

0.2

0

20

f (f P )

60

0.1

p = 0.10
m = 0.201
sd = 0.008

0.3

ns = 10000
p = 0.20
m = 0.288
sd = 0.007

0.4

p = 0.30
m = 0.374
sd = 0.007

0.5

p = 0.40
m = 0.460
sd = 0.007

40

p = 0.00
m = 0.115
sd = 0.008

0.2

0

20

f (f P )

60

0.1

0.1

0.2

fP

0.3

0.4

0.5

Figure 22: Same as Fig. 21, but for an improved knowledge of π2 , summarized as π2 =
0.115 ± 0.007.

61

60

ns = 300
p = 0.20
m = 0.213
sd = 0.024

p = 0.30
m = 0.309
sd = 0.027

p = 0.40
m = 0.404
sd = 0.029

40

p = 0.10
m = 0.117
sd = 0.020

0

20

f (f P )

p = 0.00
m = 0.022
sd = 0.011

p = 0.10
m = 0.117
sd = 0.012

0.2

0.3

ns = 1000
p = 0.20
m = 0.213
sd = 0.014

p = 0.30
m = 0.309
sd = 0.016

0.4

p = 0.40
m = 0.404
sd = 0.016

40

p = 0.00
m = 0.022
sd = 0.008

0.1

0

20

f (f P )

60

0.0

p = 0.10
m = 0.117
sd = 0.009

0.2

0.3

ns = 3000
p = 0.20
m = 0.213
sd = 0.010

p = 0.30
m = 0.309
sd = 0.010

0.4

p = 0.40
m = 0.404
sd = 0.010

40

p = 0.00
m = 0.022
sd = 0.008

0.1

0

20

f (f P )

60

0.0

p = 0.10
m = 0.117
sd = 0.007

0.2

0.3

ns = 10000
p = 0.20
m = 0.213
sd = 0.007

p = 0.30
m = 0.309
sd = 0.007

0.4

p = 0.40
m = 0.404
sd = 0.007

40

p = 0.00
m = 0.022
sd = 0.007

0.1

0

20

f (f P )

60

0.0

0.0

0.1

0.2

fP

0.3

0.4

Figure 23: Same as Fig. 22, but for an improved specificity, summarized as π2 = 0.022±0.007.
62

nP.NI ~ dbin(pi2, n.NI)
nP ~ sum(nP.I, nP.NI)
fP <- nP / ns
}
in which we have added the last instruction to model the trivial node (not shown in
Fig. 20) relating in a deterministic way fP to nP and ns. In the model the symbol ‘∼’
indicates that, e.g. n.I is described by a binomial distribution defined by p and ns
(be aware of the different order of the parameters with respect to the R function!),
while ‘<−’ stands for a deterministic relation (indeed the symbols of assignment in
R). A nice thing of such a model is that the order of the instructions is not relevant.
In fact it is only needed – let us put it so – to describe the related graphical model.
All the rest will be done internally by JAGS at the compilation step.
Then, obviously, we have to
• pass to the program the model parameters (observed nodes), which are p, ns,
pi1 and pi2;
• instruct it on how many ‘iterations’ to do;
• analyze, among all unobserved nodes (n.I, n.NI, nP.I, nP.NI, nP and fP), the
ones of interest, the most important one being, for us, fP.
Moving to the second model of Fig. 20, in which we also take into account the
uncertainty about π1 and π2 , is straightforward: we just need to add two instructions
to tell JAGS that pi1 and pi2 are indeed unobserved and that they depend on r1,
s1, r2 and s2:
model {
n.I ~ dbin(p, ns)
n.NI <- ns - n.I
nP.I ~ dbin(pi1, n.I)
nP.NI ~ dbin(pi2, n.NI)
pi1 ~ dbeta(r1, s1)
pi2 ~ dbeta(r2, s2)
nP ~ sum(nP.I, nP.NI)
fP <- nP / ns
}
Once the model is defined, it has to be saved into a file, whose location is then passed
to JAGS (we shall regularly use the temporary file tmp model.bug, whose extension
‘.bug’ is the BUGS/JAGS default).
At this point, moving to the R code to interact with JAGS, we need to
63

• load the interfacing package rjags;
• prepare a R ‘list’ containing the data (in particular, the values of the ‘observed’
nodes);
• call jags.model() to ‘setup’ the model;
• call coda.samples() to ask JAGS to perform the sampling, also specifying
the variables we want to monitor, whose ‘histories’ will be returned in a single
‘object’.41
Finally we have to show the result. All this is done, for example, in the R script
provided in Appendix B.6 (note that the temporary model file is written directly
from R, a convenient solution for small models). Needless to say, we get, apart from
statistical fluctuations inherent to Monte Carlo methods, ‘exactly’ the same results
obtained with the script of Appendix B.5, which only uses R statistical functions.
7.2.3

Further check of the approximated formulae

Finally, we have checked the validity of the approximated formulae (69)-(74) to evaluate the expected value and the standard uncertainty of fP in all cases considered in
Figs. 21-23. The agreement is indeed excellent, even in the cases of p = 0 of Fig. 23,
characterized by skewed distributions. The R script to reproduce all numbers of all
three figures is provided in Appendix B.7.

7.3

Resolution power

Having to turn the qualitative judgment regarding the ‘separation’ of the distributions
of fP for different p, as it results from Figs. 21-23, into a resolution power, one
needs some convention. First, we remind that, unless p is very small, we have good
theoretical reasons, confirmed by Monte Carlo simulations, that fp is about Gaussian,
at least in the range of a few standard deviations around its mean value. But Gaussian
curves are, strictly speaking, never separated from each other, because they have as
domain the entire real axis for all µ’s and σ’s. In fact this is a “defect” of such
distribution, as Gauss himself called it [30] and some grain of salt is required using
it. In order to form an idea of how one could define conventionally ‘resolution’, the
upper plot of Fig. 24 shows some Gaussians having unitary σ, with µ’s differing by
one 1σ.
41

To the returned object is assigned the name ‘chain’ in the script of Appendix B.6. In order to
get information about the kind of object, just issue the command ‘str(chain)’.

64

0.4
0.3
0.2
0.0

0.1

f(x)

−2

0

2

4

6

0.08
0.06
0.04
0.00

0.02

Resolution power in p

0.10

x

500

1000

2000

5000

10000

ns

Figure 24: Upper plot: Examples of Gaussians whose µ parameters are separated by 1 σ.
Bottom plot: resolution power in p, defined by Eq. (78), for κ = 3 (lines between points just to
guide the eye). Filled (blue) circles for p = 0.1 and open (cyan) circles for p = 0.5. Solid lines for
π2 = 0.115 ± 0.022, dashed lines for π2 = 0.115 ± 0.007 and dotted lines for π2 = 0.022 ± 0.007
(π1 = 0.978 ± 0.007 in all cases).

65

We see that a ‘reasonable separation’ is achieved when they differ by a few σ’s –
let us say, generally speaking, κ σ, although absolute separation can never occur, for
the already quoted intrinsic “defect” of the distribution. Having to choose a value,
we just opt arbitrarily for κ = 3, corresponding to the two solid lines of the figure,
although the conclusions that follow from this choice can be easily rescaled at wish.
Moreover, as we can see from Figs. 21-23 (and as it results from the approximated
formulae)
• the standard deviation of the distributions varies smoothly with p;
• the mean value depends linearly on p for obvious reasons.42
Therefore the resolution power in the interval [p, p + ∆p] can be evaluated by a simple
proportion
R(p, p + ∆p) ≈

∆p
· κ · σ(fP )|p+∆p/2 .
E(fP )|p+∆p − E(fP )|p

(77)

For example, using the numbers of the Monte Carlo evaluations shown in Fig. 21, for
p = 0.1 and ns = 300 we get 0.1/(0.288 − 0.201) × 3 × 0.0305 = 0.105, reaching at
best ≈ 0.022 in the case of ns = 10000 shown in Fig. 23. The resolution power at a
given value of p, is obtained in the limit ‘∆p → 0’:
R(p) ≈

∆p
· κ · σ(fP )|p
E(fP )|p+∆p − E(fP )|p

(∆p → 0).

(78)

The bottom plot of Fig. 24 shows the variation of the resolution power in p for the
same values of ns of Figs. 21-23 and for the usual cases of π1 and π2 of those figures
(in the order: solid, dashed and dotted line – the lines are drawn just to guide the
eye and to easily identify the conditions). The resolution power has been evaluated
using the approximated formulae, for κ = 3, around p = 0.1 (blue filled circles) and
around p = 0.5 (cyan open circles), using for the gradient ∆p = 0.01 (the exact value
is irrelevant for the numerical evaluation, provided it is small enough). [Obviously, if
one prefers a different value of κ (in particular one might like κ = 1), then one just
needs to rescale the results.]

7.4

Predicting the fractions of positives obtained sampling
two different populations

An interesting question then arises: what happens if we measure, using tests having
the same uncertainties on sensitivity and specificity, two different populations, having
42

Deviations from linearity are expected for p ≈ 0 and rather small ns , but, as we have checked
with approximated formulae, the effect is negligible for the values of interest.

66

proportions of infectees p(1) and p(2) , respectively? For example, in order to make use
of results we have got above, let us take the results shown in Fig. 21 for ns = 10000,
p(1) = 0.1 and p(2) = 0.2. For this value of the sample size and for our standard
hypotheses for sensitivity and specificity, summarized as π1 = 0.978 ± 0.007 and
π2 = 0.115 ± 0.022, the uncertainties are dominated by the systematic contributions.
(1)
(2)
Our expectations are then fP = 0.201 ± 0.020 and fP = 0.288 ± 0.018. The
(2)
(1)
difference of expectations is therefore ∆fP = fP − fP = 0.087.
Now it is interesting to know how much uncertain this number is. One could
improperly use a quadratic combination of the two standard uncertainties, thus getting ∆fP = 0.087 ± 0.027. But this evaluation of the uncertainty on the difference
(1)
(2)
is incorrect because fP and fP are obtained from the same knowledge of π1 and
π2 , and are therefore correlated. Indeed, in the limit of negligible uncertainties on
these two parameters, the expectations would be much more precise, as we can see
from the upper plot of Fig. 13, with a consequent reduction of σ(∆fP ). These are
the results, obtained by Monte Carlo evaluation using only R commands (see script
in Appendix B.8),43 with one extra digit with respect to Fig. 21 and adding also the
correlation coefficient:
(1)

fP

(2)
fP

= 0.2013 ± 0.0199

= 0.2876 ± 0.0179
= 0.0863 ± 0.0064

∆fP


(1)
(2)
= 0.9470
ρ fP , fP

The uncertainty on ∆fP is about one fourth of what naively evaluated above and
about one third of the individual predictions, due to the well known effect of (at
least partial) cancellations of uncertainties in differences, due to common systematic
contributions. In this case, in fact, the standard deviation of ∆fP , calculated from
standard deviations and correlation coefficient, is given by44
r


σ(∆fP ) =

(1)

(2)

(1)

(2)

σ 2 (fP ) + σ 2 (fP ) − 2 ρ fP , fP

(1)

(2)

· σ(fP ) · σ(fP ) = 0.0064 ,

43

As alternative, one could use JAGS, of which we provide the model in Appendix B.9, leaving
the R steering commands as exercise. JAGS will be instead used in Sec. 8.6 to infer p(1) , p(2) and
∆p = p(2) − p(1) .
44
It might be useful to remind that, given a linear combination Y = c1 · X1 + c2 · X2 , the variance
of Y is given by
σ 2 (Y ) =
=

c21 · σ 2 (X1 ) + c22 · σ 2 (X2 ) + 2 c1 · c2 · Cov(X1 , X2 )

c21 · σ 2 (X1 ) + c22 · σ 2 (X2 ) + 2 c1 · c2 · ρ(X1 , X2 ) · σ(X1 ) · σ(X2 ) .

67

in perfect agreement with what we get from Monte Carlo sampling.
An important consequence of the correlation among the predictions of the numbers
of positives in different populations is that we have to expect a similar correlation
in the inference of the proportion of infectees in different populations. This implies
that we can measure their difference much better than how we can measure a single
proportion. And, if one of the two proportions is precisely known using a different
kind of test, we can take its value as kind of calibration point, which will allow a
better determination also of the other proportion. We shall return to this interesting
point in Sec. 8.6.

8

Inferring p from the observed number of positives in the sample

Let us finally move to the probabilistic inference of the proportion of infected individuals, p, based on the number of positives nP in a sample of size ns and given our
best knowledge of the performance of the test, all summarized in the graphical model
√
of Fig. 25, which differs from that of Fig. 20 only for the symbol ‘ ’ moved from
node p (now ‘unobserved’) to node nP (now ‘observed’). The diagram contains also
the probabilistic and deterministic relations among the nodes, written directly using
the JAGS language.45

8.1

From the general problem to its implementation in JAGS

The most general problem would be to evaluate the joint conditional probability
of the uncertain (‘unobserved’) quantities, conditioned by the ‘observed’ (‘known’/
‘assumed’/‘postulated’) ones, that is, in this case46 (see Appendix A),
f (p, nI , nN I , nPI , nPNI , π1 , π2 | nP , ns , r1 , s1 , r2 , s2 ) ,

(79)

although in practice we are indeed interested in f (p | nP , ns , r1 , s1 , r2 , s2 ), and perhaps
in f (nI | nP , ns , r1 , s1 , r2 , s2 ) and f (nPI | nP , ns , r1 , s1 , r2 , s2 ). This is done marginal45

The relation, ‘nP ∼ sum(nPI , nPN I )’ is logically equivalent to ‘nP < - nPI + nPN I ’, but the latter
instruction would not work because JAGS prohibits ‘observed nodes’ to be defined by a deterministic
assignment, as, instead, it has been done in the case of nN I , defined as ‘nN I <– ns − nI ’.
46
Note that we can in principle learn something also about π1 and π2 , because we can properly
marginalize Eq. (79) in order to get f (π1 , π2 | nP , ns , r1 , s1 , r2 , s2 ). In the limit that they are very
well known (condition reflected into very large ri and si ) we expect that their joint probability
distribution is not updated much by the new pieces of information. But, if instead they are poorly
known, we get some information on them, at the expense of the quality of information we can get
on the main quantity of interest, that is p (although we are not going into the details, see Sec. 8.5
for a case in which π2 is updated by the data).

68

‘f0(p)’
p
√

r1

ns

√

s1 √

√

nI

nI ∼ dbin(p, ns)

r2

nN I

π1

s2 √
nN I <- ns − nI

π2
π2 ∼ dbeta(r2, s2)

π1 ∼ dbeta(r1, s1)
nPI

nPN I
nPN I ∼ dbin(π2, nN I )

nPI ∼ dbin(π1, nI )
nP √
nP ∼ sum(nPI , nPN I )

Figure 25: Graphical model of Fig. 20, re-drawn in order to emphasize its inferential use and
including the commands to build up the JAGS model. ‘f0 (p)’, left open in this diagram, stands
for the prior distribution of p.

izing Eq. (79) i.e. summing (or integrating, depending on their nature) over the
variables on which we are not interested (see Appendix A). As commented in the
same appendix, Eq. (79) is obtained, apart from a normalization factor, from
f (p, nI , nN I , nPI , nPNI , π1 , π2 , nP , ns , r1 , s1 , r2 , s2 ),

(80)

and the latter from a properly chosen chain rule. The steps used to build up Eq. (80)
by the proper chain rule are exactly the instructions given to JAGS to set up the
model, if we start from the bottom of the diagram of Fig. 25 and ascend through the
parents (see Sec. 7.2.2):
model {
nP ~ sum(nP.I, nP.NI)
nP.I ~ dbin(pi1, n.I)
nP.NI ~ dbin(pi2, n.NI)
pi1 ~ dbeta(r1, s1)
pi2 ~ dbeta(r2, s2)
n.I ~ dbin(p, ns)
n.NI <- ns - n.I
69

p ~ dbeta(r0,s0)
}
The differences with respect to the JAGS model of Sec. 7.2.2 are
• the last instruction there, ‘fP <- nP/ns’, is here irrelevant;
• we have to add a prior to p, because all unobserved nodes having no parents
need a prior (for practical convenience, as we have seen in Sec. 4.2, we shall use
a Beta distribution, as indicated in the code);
• the sequence of the statements has been changed, but this has been done only
in order to stress the analogy with the chain rule constructed ascending the
graphical model of Fig. 25 (let us remind that the order is irrelevant for JAGS,
which organizes all statements at the stage of compilation).
Hereafter we proceed using, very conveniently, JAGS, showing in Sec. 9 the steps
needed from writing down the chain rule till the exact evaluation of f (p) after
marginalization.

8.2

Inferring p and nI with our ‘standard parameters’

Let us start using as nP the expected value of positives of ≈ 2010, obtained from
what has been our starting set of parameters through the paper, that is p = 0.1 with
ns = 10000, with the uncertain parameters π1 and π2 modeled by Beta distributions
with (r1 = 409.1, s1 = 9.1) and (r2 = 25.2, s2 = 193.1), respectively. Also for the
prior of p we use a Beta, starting with r0 = s0 = 1, that models a flat prior, although
we obviously do not believe that p = 0 or p = 1 are possible. We shall discuss in
Sec. 8.7 the role of such at a first glance an insane prior (see also Sec. 9).
These are the R command to set the parameters of the game, call JAGS and show
some results (for the complete script see Appendix B.10).
#---- data and parameters
nr = 1000000
ns = 10000
nP = 2010
r0 = s0 = 1
r1 = 409.1; s1 = 9.1
r2 = 25.2; s2 = 193.1
# define the model and load rjags (omitted)
# .........................................
70

#---- call JAGS --------data <- list(ns=ns, nP=nP, r0=s0, s0=s0, r1=r1, s1=s1, r2=r2, s2=s2)
jm <- jags.model(model, data)
update(jm, 10000)
to.monitor <- c(’p’, ’n.I’)
chain <- coda.samples(jm, to.monitor, n.iter=nr)
#---- show results
print(summary(chain))
plot(chain, col=’blue’)
Here are the results shown by ‘summary(chain)’
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean
SD Naive SE Time-series SE
n.I 991.12477 225.85901 2.259e-01
16.079460
p
0.09919
0.02278 2.278e-05
0.001601
2. Quantiles for each variable:
2.5%
25%
50%
75%
97.5%
n.I 506.00000 838.00000 1012.000 1153.0000 1389.0000
p
0.05046
0.08372
0.101
0.1155
0.1396
So, for this run we get p = 0.0992 ± 0.023 and a number of infectees in the sample
equal to 991 ± 226, in agreement with our expectations. The results of the Monte
Carlo sampling are shown in the ‘densities’ of Fig. 26, together with the ‘traces’,
i.e. the values of the sampled variables during the 106 iterations.47 As it is easy to
guess and as it appears from the two traces of the figure, there is some degree of
correlation between the two variables, because they are obtained in a joint inference.
The correlation is made evident in the scatter plot of Fig. 27 and quantified by
ρ(p, nI ) = 0.9914. 48
47

Indeed the traces show that the sampling is, so to say, not optimal, and more iterations would
be needed. But for our needs here and for reminding the care needed in applying this powerful tool,
we prefer to show this not ideal case of sampling with a quite larger but not large enough number
of iterations. (Later on, when critical, we shall increase nr up to 107 .)
48
Plot and correlation coefficient are obtained by the following R commands

71

Figure 26: Plots showing some JAGS results (see text).

8.3

Dependence on our knowledge concerning π1 and π2

As we have already well understood, the uncertainty on the result is highly dependent
of the uncertainty concerning π1 and π2 . Therefore, as we have done in the previous sections, let us also change here our assumptions and see how the main result
changes accordingly (nI is of little interest, at this point, also because of its very high
correlation with p, and hence we shall not monitor it any longer in further examples).
chain.df <- as.data.frame( as.mcmc(chain) )
plot(chain.df, col=’blue’)
cor(chain.df)

72

Figure 27: Scatter plot of p vs nI , showing the very high correlation between the two variables.
1. First we start assuming negligible uncertainty on sensitivity and specificity.49
As a result, the standard uncertainty σ(p) becomes 0.0046, that is about a
factor ≈ 5 smaller.
2. Then, as we have done in the above sections, we keep π1 to its default value
(r1 = 409.1, s1 = 9.1), only reducing the uncertainty of π2 to 0.007.50 Also in
this case the uncertainty decreases, getting σ(p) = 0.0085.
3. Finally, we make the pdf of π2 mirror symmetric with respect to that of π1 , that
is r2 = 9.1, s2 = 409.1. But, obviously we need to change the number of the
observed positives, choosing this time 1170, as suggested by our expectations
(see Fig. 23). As a result we get p = 0.0995 ± 0.0076, with an uncertainty not
differing so much with respect to the previous case. Indeed, as we have have
49

In order to avoid to modify the JAGS model, we simply multiply all relevant Beta parameters
by the large factor a = 106 , thus reducing all uncertainties by a factor thousand (see Eq. (27)). This
is done by adding the following command
a=1e6; r1=r1*a; s1=s1*a; r2=r2*a; s2=s2*a
50

We exploit the same trick of the previous item redefining the Beta parameters as follows
a=(22/7)^2; r2=r2*a; s2=s2*a

73

already noted in the previous sections, improving the specificity (π2 reduced by
a factor five) has only a little effect on the quality of the measurement, being
more important the uncertainty with which that test parameter is known. (And
we expect that something like that is also true for the sensitivity.)

8.4

Quality of the inference as a function of the sample size
and of the fraction of positives in sample

A more systematic study of the quality of the inference is shown in Tab. 3, which
reports the inferred value of p, summarized by the expected value and its standard
deviation evaluated by sampling, as a function of the sample size and the number
of positives in the sample. The three blocks of the table correspond to our typical
hypotheses on the knowledge of sensitivity and specificity, and summarized, from top
to bottom, by (π1 = 0.978 ± 0.007, π2 = 0.115 ± 0.022), (π1 = 0.978 ± 0.007, π2 =
0.115 ± 0.007) and (π1 = 0.978 ± 0.007, π2 = 0.022 ± 0.007), corresponding then to
the cases shown, in the same order, in Figs. 21-23 (we have added an extra column
with the numbers of positives yielding p ≈ 0.5). We see that, from columns 2 to 6, we
get p ranging from 0.1 to 0.5 at steps of 0.1, with standard uncertainty varying with
ns and nP (and therefore with the fraction of positives fP ) in agreement with what
we have learned in Sec. 7, studying the predictive distributions (note the difference
between resolution power, used there, and standard uncertainty, used here).
We note that, instead, the results of the first column is “not around zero, as
expected” (naively). The reason is very simple and it is illustrated in Fig. 28 for the
case of ns = 10000. It is true that, if there were no infected in the population, then we
would expect nP ≈ 1150 (with a standard uncertainty of 220), but the distribution of
p provided by the inference cannot have a mean value zero, simply because negative
values of p are impossible.51 Obviously the smaller is the number of positives in the
sample and more peaked is the distribution of p close to 0. But what happens if, for
ns = 10000, nP is much smaller of 1150? This interesting case will be the subject of
the next subsection.
51

In particular, we would like to point out that this question has nothing to do with the story
of the ‘biased estimators’ of frequentists. In probabilistic inference the result is not just a single
number (the famous ‘estimator’), but rather the distribution of the quantity of interest, of which
mean and standard deviation are only some of the possible summaries, certainly the most convenient
for several purposes.

74

ns

[nP ]
E(p) ± σ(p)

300

[34]
0.026 ± 0.019

[60]
0.100 ± 0.034

[86]
0.200 ± 0.036

[112]
0.299 ± 0.037

[138]
0.399 ± 0.037

[164]
0.495 ± 0.036

1000

[115]
0.021 ± 0.015

[201]
0.099 ± 0.028

[288]
0.198 ± 0.026

[374]
0.298 ± 0.025

[460]
0.399 ± 0.024

[546]
0.498 ± 0.023

3000

[345]
0.018 ± 0.014

[604]
0.099 ± 0.024

[863]
0.198 ± 0.023

[1122]
0.299 ± 0.020

[1381]
0.399 ± 0.019

[1640]
0.499 ± 0.017

10000

[1150]
0.018 ± 0.013

[2013]
0.099 ± 0.022

[2876]
0.198 ± 0.020

[3739]
0.299 ± 0.019

[4602]
0.399 ± 0.016

[5465]
0.499 ± 0.015

300

[34]
0.019 ± 0.015

[60]
0.101 ± 0.028

[86]
0.201 ± 0.031

[112]
0.300 ± 0.033

[138]
0.400 ± 0.034

[164]
0.496 ± 0.034

1000

[115]
0.011 ± 0.009

[201]
0.100 ± 0.016

[288]
0.200 ± 0.018

[374]
0.299 ± 0.019

[460]
0.400 ± 0.019

[546]
0.499 ± 0.019

3000

[345]
0.009 ± 0.006

[604]
0.100 ± 0.011

[863]
0.200 ± 0.012

[1122]
0.300 ± 0.012

[1381]
0.400 ± 0.012

[1640]
0.500 ± 0.012

10000

[1150]
0.007 ± 0.005

[2013]
0.100 ± 0.009

[2876]
0.200 ± 0.008

[3739]
0.300 ± 0.008

[4602]
0.400 ± 0.008

[5465]
0.500 ± 0.008

300

[7]
0.010 ± 0.008

[35]
0.102 ± 0.021

[64]
0.199 ± 0.025

[93]
0.299 ± 0.028

[121]
0.400 ± 0.030

[150]
0.500 ± 0.031

1000

[22]
0.007 ± 0.005

[118]
0.100 ± 0.013

[213]
0.200 ± 0.015

[309]
0.300 ± 0.016

[404]
0.400 ± 0.017

[500]
0.500 ± 0.017

3000

[66]
0.006 ± 0.004

[353]
0.100 ± 0.009

[640]
0.200 ± 0.010

[926]
0.300 ± 0.011

[1213]
0.400 ± 0.011

[1500]
0.500 ± 0.011

10000

[220]
0.006 ± 0.004

[1176]
0.100 ± 0.008

[2132]
0.200 ± 0.008

[3088]
0.300 ± 0.007

[4044]
0.400 ± 0.007

[5000]
0.500 ± 0.007

Table 3: Proportion p of infected in a population, inferred from the number nP of positives
in a sample of nS individuals. The three blocks of the table corresponds to the assumptions
summarized by π1 = 0.978 ± 0.007 and π2 = (0.115 ± 0.022, 0.115 ± 0.007, 0.022 ± 0.007).
75

Figure 28: Inference of p from ns = 10000 and nP = 1150.

8.5

Updated knowledge of π1 and π2 in the case of ‘anomalous’ number of positives

Let us imagine that, instead of 1150 positives, we ‘had observed’ a much smaller
number (in terms of standard deviation of prediction, that, we remind, is about
220). For example, an under-fluctuation of 3 σ’s would yield 490 positives. But let
us exaggerate and take as few as 50 positives, corresponding to −5 σ’s. The JAGS
result (this time monitoring also π1 and π2 ), obtained using our usual uncertainties
concerning π1 and π2 (0.978 ± 0.007 and 0.115 ± 0.022, respectively), is showed in
Fig. 29 and summarized as
1. Empirical mean and standard deviation for each variable,
plus standard error of the mean:
Mean
SD Naive SE Time-series SE
p
0.0001022 0.0001023 1.023e-07
1.998e-07
pi1 0.9781819 0.0071445 7.145e-06
7.161e-06
pi2 0.0073581 0.0008463 8.463e-07
8.463e-07
2. Quantiles for each variable:
2.5%
25%
50%
75%
97.5%
p
2.586e-06 2.949e-05 7.091e-05 0.0001415 0.0003771
pi1 9.622e-01 9.738e-01 9.789e-01 0.9833334 0.9899013
76

Figure 29: JAGS inference of p, π1 and π2 from ns = 10000 and nP = 50 (see text).
77

pi2 5.790e-03 6.771e-03 7.327e-03 0.0079113 0.0091035
As we see, the distribution of p looks exponential, with mean and standard deviation
practically identical and equal to 1.0 × 10−4 (we remind that it is a property of
the exponential distribution to have expected value and standard deviation equal).
In this case the quantiles produced by R are particularly interesting, providing e.g.
P (p ≤ 3.77 × 10−4 ) = 97.5%.
The fact that a small number of infectees squeezes the distribution of p towards
zero follows the expectations. More surprising, at first sight, is the fact that also the
value of π2 does change:
π2 : 0.115 ± 0.022

−→ 0.0074 ± 0.0008 .

The reason why π2 can change (also π1 could, although JAGS ‘thinks’ this is not the
case) is due to the fact that it is now a unobserved node, and the Beta(r2 , s2 ) with
which we model it is just the prior distribution we assign to it. In other words, the
very small number of positives could be not only due to a very small value of p, but
also to the possibility that π2 is indeed substantially smaller than what we initially
thought. This sounds absolutely reasonable, but telling exactly what the result will
be can only be done using strictly the rules of probability theory, although with the
help of MCMC, because in multivariate problems of this kind intuition can easily
fail.52

8.6

Inferring the proportions of infectees in two different
populations

Let us now go through what has been anticipated in Sec. 7.4, talking about predictions. We have seen that, since (at least in our model) an important contribution to
the uncertainty is due to systematics, related to the uncertain knowledge of π1 and
π2 , we cannot increase at will the sample size with the hope to reduce the uncertainty
on p. Nevertheless, as a consequence of what we have seen in Sec. 7.4, we expect to
be able to measure the difference of proportions of infectees in two populations much
better than how we can measure a single proportion.
Let us use again sample sizes of 10000 (they could be different for the different
populations) and imagine that we get numbers of positives rather ‘close’, as we know
(1)
(2)
from the predictive distribution: nP = 2000 and nP = 2200. As far as sensitivity
and specificity are concerned, since we have learned their effect, let us stick, for this
52

Although we cannot go through the details in this paper, it would be interesting to use ‘wider
priors’ about π1 and π2 in order to see how they get updated by JAGS, and then try to understand
what is going on making pairwise scatter plots of the resulting p, π1 and π2 .

78

exercise, to our default case, summarized by π1 = 0.978±0.007 and π2 = 0.115±0.022.
The R script is given in Appendix B.11. Here is the result of the joint inference and
of the difference of the proportions:
p(1)
p(2)
∆p = p(2) − p(1)
ρ(p(1) , p(2) )

=
=
=
=

0.097 ± 0.023
0.120 ± 0.022
0.023 ± 0.007
0.955 .

As we see, p(1) and p(2) are, as we use to say, ‘equal within the uncertainties’, but
nevertheless their difference is rather ‘significative’. This is due to the fact that the
common systematics induce a quite strong positive correlation among the determination of the two proportions, quantified by the correlation coefficient. The relevance
of measuring differences has been already commented in Sec. 7.4, in which we also
provided some details on how to evaluate the uncertainty of the difference from the
other pieces of information. We would just like to stress its practical/economical
importance. For example, dozens of regions of a state could be sampled and tested
with ‘rather cheap’ kits, with performances of the kind we have seen here (but it is
important that they are the same!), and only one region (or a couple of them, just
for cross-checks) also with a more expensive (and hopefully more accurate) one. The
region(s) tested with the high quality kit could then be used as calibration point(s)
for the others and the practical impact in planning a test campaign is rather evident.

8.7

Which priors?

After having read in the first part of the paper the dramatic role of the prior, when
we had to evaluate the probability of individual being infected, given the test result,
one might be surprised by the regular use of a flat prior of p throughout the present
section. First at all, we would like to point out that we are doing so, in this case,
not “in order to leave the data to ‘speak’ by themselves”, as someone says. It is,
instead, the other way around: the values of p preferred by the data, starting from a
uniform prior, are characterized by a distribution much narrower than what we could
reasonably judge, based on previous rational knowledge. In other words, they are
not at odds with what we could believe independently of the data. But this is not
always the case, and experts could have more precise expectation, grounded on their
knowledge.
Anyway, a prior distribution is something that we have to plug in the model, if we
want to perform a probabilistic inference. In practice – and let us remind again that
“probability is good sense reduced to a calculus” – we model the prior in a reasonable
and mathematically convenient way, and the Beta distribution is well suited for this
79

case, also due to the flexibility of the shapes that it can assume, as seen in Sec. 4.2.
Once we have opted for a Beta, a uniform prior is recovered for r = 1 and s = 1,
although we are far from thinking that p = 0 or p = 1 are possible, as well as that p
could be above 0.9 with 10% chance, and so on.
8.7.1

Symmetric role of prior and ‘integrated likelihood’

Since we cannot go into indefinite and sterile discussions on all the possible priors
that we might use (remember that if we collect and analyze data is to improve our
knowledge, often used to make practical decision in a finite time scale!) it is important
to understand a bit deeper their role in the inference. This can be done factorizing
Eq. (80), written here in compact notation as
f (. . .) = f (p, nI , nN I , nPI , nPNI , π1 , π2 , nP , ns , r1 , s1 , r2 , s2 ) ,

(81)

into two parts: one that only contains f0 (p) and the other containing the remaining
factors of the ‘chain’, indicated here as f∅ (. . .):
f (. . .) = f∅ (. . .) · f0 (p) .

(82)

The unnormalized pdf of p, conditioned by data and parameters, can be then rewritten (see Appendix A) as


XXX X Z Z
f (p | nP , ns , r1 , s1 , r2 , s2 ) ∝ 
f∅ (. . .) dπ1 dπ2  · f0 (p) (83)
nI nNI nPI nPNI

∝ L(p ; nP , ns , r1 , s1 , r2 , s2 ) · f0 (p) ,

(84)

in which we have indicated with the usual symbol used for the (‘integrated’) likelihood
(in which constant factors are irrelevant) the part which multiplies f0 (p). It is then
rather evident the role of L in ‘reshaping’ f0 (p).53 In the particular case in which
f0 (p) = 1 the inference is simply given by
f (p | nP , ns , r1 , s1 , r2 , s2 , f0 (p) = 1) ∝ L(p ; nP , ns , r1 , s1 , r2 , s2 )
53

(85)

It is worth pointing out the cases, occurring especially in frontier science, in which the likelihood
is constant in some regions, and therefore it does not update/reshape f0 (v), where ‘v’ stands for
the generic variable of interest (see chapter 13 of Ref. [24]). An interesting instance, in which v
has the role of rate of gravitational waves r, is discussed in Ref. [31], where the concept of relative
belief updating ratio was first introduced. Another frontier physics case, applied to the Higgs boson
mass mH , on the basis of the experimental and theoretical information available before year 1999,
is reported in Ref. [32]. The two cases are complementary because in the first one sensitivity is lost
for r → 0 (‘likelihood open on the left side’), while in the second for mH → ∞ (‘likelihood open on
the right side’). (For recent developments and applications, see Ref. [33].)

80

(“the inference is determined by the likelihood”).
If, instead, the prior is not flat, then it does reshape the posterior obtained by
L alone. Therefore there are two alternative ways to see the contributions of L and
f0 (p): each one reshapes the other. In particular
• in the regions of p set to zero by either function the posterior vanishes;
• the function which is more narrow around its maximum ‘wins’ against the
smoother one.
Therefore, for the case shown in Fig. 26, obtained by a flat prior, the ‘density of p’
is nothing but the shape of L(p; nP , ns , r1 , s1 , r2 , s2 ). If f0 (p) is constant, or varies
slowly, in the range [0.02, 0.17] it provides null or little effect. If, instead, it is very
peaked around 0.15 (e.g. with a standard deviation of ≈ 0.01) it dominates the
inference.
But what is more interesting is that the reshape by f0 (p) can be done in a second
step.54 This is the importance of choosing a flat prior (and not just a question of
laziness): the data analysis expert could then present a result of the kind of Fig. 26
to an epidemiologist who could then reshape her priors (or, equivalently, reshape the
curve provided by the data analyst with her priors). But she could also have such a
strong prior on the variable under study, that she could reject tout court the result,
blaming the data analysis expert that there must be something wrong in the analysis
or in the data – see Sec. 9.4.
8.7.2

Some examples

Let us illustrate these ideas with a simple case on which exact calculations can be
also done: the inference of p of a binomial distribution, based on n successes got in
N trials. We went through it in Sec. 4, but we do it solve it now with JAGS in order
to provide some details on ‘reshaping’. The model is really trivial
model {
n ~ dbin(p, N)
p ~ dbeta(r0,s0)
}
and the full script is provided in Appendix B.12. For N = 10 and n = 3 and a flat
prior the JAGS result is shown by the histogram of Fig. 30. The blue line along
the profile of the histogram is the analytic result obtained starting from of a Beta
54

As already remarked in footnote 11, ‘prior’ does not mean that you have to declare ‘before’
you sit down to make the inference! It just means that it is based on other pieces of information
(‘knowledge’) on the quantity under study.

81

3.5
3.0
2.5
2.0

f(p)
1.5
1.0
0.5
0.0
0.0

0.2

0.4

p

0.6

0.8

1.0

Figure 30: Inference of p with binomial distributions obtained with different priors and different
ways to make use of an ‘informative prior’ (see text).

prior with r0 = s0 = 1, that is Beta(1+n, 1+N −n). Then the ‘informative prior’
(rather vague indeed), modeled by a Beta(4, 2) and therefore having a mean value
of 4/(4 + 2) = 2/3, is shown by the magenta curve having the maximum value at
3/4 = [(4 − 1)/(4 + 2 − 2)]. The distribution obtained reweighing the posterior got
from a flat prior (histogram) by this new prior is shown by the blue broken curve,
while the red broken curve shows the JAGS result obtained using the new prior
(the latter curves overlap so much that they can only be identified by color code).
Finally, the green continuous curve is the analytic posterior obtained updating the
Beta parameters, that is Beta(4 + 3, 2 + 7). The agreement of the three results is
‘perfect’ (taking into account that two of them are got by sampling).
The second example is our familiar case of 2010 positives in a sample of 10000
individuals shown in detail in Sec. 8.2 and of which a different Monte Carlo run,
with nr = 4 × 106 in order to get a smoother histogram, is shown in Fig. 31. The
new prior is indicated by the magenta curve, modeled by a Beta(6, 14), having its
mode at 5/18 ≈ 0.28. The reshaped posterior is indicated by the blue curve, having
mean 0.1134 and standard deviation 0.0182. The result of JAGS using as prior
the Beta(6, 14) is shown by the red curve, characterized by a mean of 0.1145 and
82

25
20
15

f(p)
10
5
0
0.00

0.05

0.10

0.15

p

0.20

0.25

0.30

Figure 31: Inference of the proportion p of infected in a population, having measured 2010
positives in a sample of 10000 individuals: JAGS result based on a flat prior (histograms) and
effect of ‘reshaping’ based on an informative prior. (see text).
a standard deviation 0.0184 (we are using an exaggerated number of digits just for
checking – using one digit for the uncertainty both results become ‘0.11 ± 0.02’). The
degree of agreement is excellent, also taking into account that they have intrinsic
Monte Carlo fluctuations. It is interesting to note that, besides increasing slightly
the mean values (but one could object that “they are equal within the uncertainties”),
the main effect of the new prior is to practically rule out values of p below 0.05.
8.7.3

Some approximated rules

Having seen the utility of reshaping the posterior got from a flat prior, once a different
prior is assumed, we also try to find some practical rules based on the mean and the
standard deviations of the distributions involved.
1. The first is based on Gaussian approximation, and it holds if both the prior
and the posterior got by JAGS assuming a uniform prior appear somehow ‘bellshaped’, although we cannot expect that they are perfectly symmetric, especially if small or large values of p are preferred. In this case the following (very
83

rough) approximation is obtained for the mean and the standard deviation55
µL /σL2 + µ0 /σ02
1/σL2 + 1/σ02
1
1
=
+ 2,
2
σL σ0

µp =

(86)

1
σp2

(87)

where µL and σL are the mean and the standard deviation got from JAGS with
a flat prior; µ0 and σ0 are those summarizing the priors; µp and σp should be
(approximately) equal to the JAGS results we had got using the prior summarized by µ0 and σ0 . Applying this rule to the case in Fig. 31, for which
µL = 0.0987, σL = 0.0229, µ0 = 0.30 and σ0 = 0.10, we get p = 0.1087 ± 0.022,
that, rounding the uncertainty to one digit becomes ‘0.11 ± 0.02’, equal to the
one obtained above by reshaping or re-running JAGS with the new prior.
2. The second rule makes use of the Beta and its usage as prior conjugate when
inferring p of a binomial, as we have seen in Sec. 4.2. The idea is to see the
pdf estimated by JAGS with flat prior as a ‘rough Beta’ whose parameters
can be estimated from the mean and the standard deviation using Eqs. (33)(34). We can then imagine that the pdf of p could have been estimated by a
‘virtual’ Poisson processes whose outcomes update the parameters of the Beta
according to Eqs. (30)-(31). The trick consists then in modifying the Beta
parameters according to the simple rules:
rp = rL + r0 − 1
sp = sL + s0 − 1 ,
where rL and sL are evaluated from µL and σL making use of Eqs. (33) and
(34). Then the new mean and standard deviation are evaluated from rp and sp
(see Sec. 4.2).
For example, in the case of Fig. 30 we have (with an exaggerated number of
digits) p = 0.0987±0.0229, which could derive from a Beta having rL = 16.7 and
sL = 152.4. If we have a prior somehow peaked around 0.3, e.g. p0 = 0.3 ± 0.1,
it can be parameterized by a Beta with r0 = 6 and s0 = 14. Applying the above
rule we get
rp = 16.7 + 6 − 1 = 21.7
sp = 152.4 + 14 − 1 = 165.5 ,
which yield then p = 0.1159 ± 0.0223, very similar to what was obtained by
reshaping or re-running JAGS (0.12 ± 0.02 at two decimal digits).
55

See e.g. Sec. 2 of Ref. [34].

84

As we see, these approximated rules are rather rough, but they have the advantage
of being fast to apply, if one wants to arrive quickly to some reasonable conclusions,
based on her personal priors.56

9

Exact evaluation of f (p)

After having solved the inferential task by MCMC making use of JAGS, let us now
attempt to solve our problem exactly, although limiting ourselves to the inference of
p.

9.1

Setting up the problem

As we have seen in Sec. 8, the inference of the ‘unobserved’ variables, based on the
‘observed’ one, for the problem represented graphically in the ‘Bayesian’ network of
Fig. 25, consists in evaluating ‘somehow’
f (p, nI , nN I , nPI , nPNI , π1 , π2 | nP , ns , r1 , s1 , r2 , s2 ) ,

(88)

from which the most interesting probability distribution, at least for the purpose of
this paper,
f (p | nP , ns , r1 , s1 , r2 , s2 )
can be obtained by marginalization (see also Appendix A). Besides a normalization
factor, Eq. (88) is proportional to Eq. (80), hereafter indicated by ‘f (. . .)’ for compactness, which can be written making use of the chain rule obtained following the
bottom-up analysis of the graphical model of Fig. 25:
f (. . .) = f (nP | nPI , nPNI ) · f (nPI | π1 , nI ) · f (nPNI | π2 , nN I ) · f (π1 | r1, s1 ) ·
f (π2 | r2 , s2 ) · f (nN I | ns , nI ) · f (nI | p, ns ) · f0 (p)
in which
f (nP | nPI , nPNI ) = δnp , nPI +nPNI
 
nI
· π1nI · (1 − π1 )nI −nPI
f (nPI | π1 , nI ) =
nPI


nN I
nP
· π2 NI · (1 − π2 )nNI −nPNI ‘
f (nPNI | π2 , nN I ) =
nPNI
56

(89)
(90)
(91)

The main reason of the not excellent level of agreement is due to the quite pronounced tail on
the left side of the distribution. The rule could work better for other values of nP , given ns , but we
have no interest in showing the best case and try to sell it as ‘typical’. We just stuck to the numeric
case we have used mostly throughout the paper.

85

π1r1 −1 · (1 − π1 )s1 −1
β(r1 , s1 )
r2 −1
π
· (1 − π2 )s2 −1
f (π2 | r2, s2 ) = 2
β(r2 , s2 )
f (nN I | ns , nI ) = δnNI , ns −nI
 
ns
· pnI · (1 − p)ns −nI ,
f (nI | p, ns ) =
nI
f (π1 | r1, s1 ) =

(92)
(93)
(94)
(95)

where δm,k is the Kroneker delta (all other symbols belong to the definitions of the
binomial and the Beta distributions) and we have left to define the prior distribution
f0 (p). The distribution of interest is then obtained by summing up/integrating
XXX X Z Z
f (p | nP , ns , r1 , s1 , r2 , s2 ) ∝
f (. . .) dπ1 dπ2 ,
nI nNI nPI nPNI

where the limits of sums and integration will be written in detail in the sequel.
As a first step we simplify the equation by summing over nPNI and nN I and
exploiting the Kroneker delta terms (89) and (94). We can then replace nPNI with
nP − nPI and nN I with ns − nI


ns −nI
· π2 (nP −nPI ) · (1 − π2 )(ns −nI )−(nP −nPI ) (96)
f (nP −nPI |ns −nI , π2 ) =
nP −nPI
with the obvious constraints ns − nI > nP − nPI (i.e. nN I > nPNI ) and nPI < nI .
The inferential distribution of interest f (p | nP , ns , r1 , s1 , r2 , s2 ), becomes then,
besides constant factors and indicating all the status of information on which the
inference is based as ‘I’, that is I ≡ {nP , ns , r1 , s1 , r2 , s2 },
f (p | I) ∝ f0 (p) ·

nP
ns Z
X
X

nPI =0 nI =0

1
0

Z

0

1

f (nPI | nI , π1 ) · f (nP − nPI |ns − nI , π2 ) ·

f (π1 | r1 , s1 ) · f (π2 | r2 , s2 ) · f (nI | p, ns ) dπ1 dπ2

nP
ns Z 1 Z 1 
X
X
nI
∝ f0 (p) ·
· π1 nPI · (1 − π1 )nI −nPI ·
n
PI
0
nPI =0 nI =0 0


ns − nI
· π2 (nP −nPI ) · (1 − π2 )(ns −nI )−(nP −nPI ) ·
nP − nPI
 
ns
r1 −1
r2 −1
s1 −1
s2 −1
· pnI · (1 − p)ns −nI dπ1 dπ2
π1
· (1 − π1 )
· π2
· (1 − π2 )
·
nI
86

  
 
nP X
ns 
X
ns
ns − nI
nI
·pnI ·(1 − p)ns −nI ·
·
·
∝ f0 (p) ·
nI
nP − nPI
nPI
nPI =0 nI =0
Z 1
π1 nPI +r1 −1 · (1 − π1 )nI −nPI +s1 −1 dπ1
0
Z 1
π2 (nP −nPI +r2 −1) · (1 − π2 )(ns −nI )−(nP −nPI )+s2 −1 dπ2

(97)

0

where we have dropped all the terms not depending on the variables summed up/integrated.
The two
appearing in Eq. (97) are, in terms of the generic variable x, of
R 1 integrals
α−1
the form 0 x
· (1 − x)β−1dx, which defines the special function beta B(α, β), whose
value can be expressed in terms of Gamma function as B(α, β) = Γ(α)·Γ(β)/Γ(α+β).
We get then
  
 
nP
ns 
X
X
ns
ns − nI
nI
· pnI · (1 − p)ns −nI ·
f (p | I) ∝ f0 (p) ·
·
·
n
n
−
n
n
I
P
PI
PI
n =0 n =0
PI

I

Γ(nPI + r1 ) · Γ(nI − nPI + s1 )
·
Γ(r1 + nI + s1 )

Γ(nP − nPI + r2 ) · Γ(ns − nI − nP + nPI + s2 )
.
Γ(ns − nI + s2 + r2 )

9.2

(98)

Normalization factor and other moments of interest

The normalization factor Nf is given by the integral in dp of this expression, once f0 (p)
has been chosen. As we have done in the previous section, we opt for Beta(r0 , s0 ),
taking the advantage not only of the flexibility of the probability distribution to
model our ‘prior judgment’ on p, but also of its mathematical convenience. In fact,
with this choice, the resulting term in Eq. (98) depending on p is given by pr0 −1+nI ·
(1 − p)s0 −1+(ns −nI ) . The integral over p from 0 to 1 yields again a Beta function, that
is B(r0 + nI , s0 + ns − nI ), thus getting
 
  
nP X
ns 
X
Γ(nPI + r1 ) · Γ(nI − nPI + s1 )
nI
ns − nI
ns
·
Nf =
·
·
·
Γ(r
+
n
+
s
)
n
n
−
n
n
1
I
1
P
P
P
I
I
I
n =0 n =0
pI

I

Γ(nP − nPI + r2 ) · Γ(ns − nI − nP + nPI + s2 )
·
Γ(ns − nI + s2 + r2 )

Γ(r0 + nI ) · Γ(s0 + ns − nI )
Γ(r0 + s0 + ns )

(99)

Similarly, we can evaluate the expression of the expected values of p and of p2 , from
which the variance follows, being σ 2 (p) = E(p2 ) − E2 (p). For example, being E(p)
87

given by
E(p) =

Z

0

1

p · f (p | I)dp ,

in the integral the term depending on p becomes p · pr0 −1+nI · (1 − p)s0 −1+(ns −nI ) ,
increasing the power of p by 1 and thus yielding
 

nP X
ns 
1 X
Γ(nPI + r1 ) · Γ(nI − nPI + s1 )
ns
ns − nI
nI
E(p) =
·
·
·
·
·
Nf n =0 n =0 nPI
Γ(r1 + nI + s1 )
nI
nP − nPI
pI

I

Γ(nP − nPI + r2 ) · Γ(ns − nI − nP + nPI + s2 )
·
Γ(ns − nI + s2 + r2 )

Γ(r0 + nI +1) · Γ(s0 + ns − nI )
,
Γ(r0 + s0 + ns +1)

(100)

while E(p2 ) is obtained replacing ‘+1’ by ‘+2’. A script to evaluate expected value
and standard deviation of p is provided in Appendix B.13.
The expression can be extended to ‘+3’ by ‘+4’, thus getting E(p3 ) and E(p4 ),
from which skewness and kurtosis can be evaluated. Finally, making use of the so
called Pearson Distribution System implemented in R [35], f (p) can be obtained with
a quite high degree of accuracy, unless the distribution is squeezed towards 0 o 1, as
e.g. in Fig. 29.57 A script to evaluate mean, variance, skewness and kurtosis, and
from them f (p) by the Pearson Distribution System is shown in Appendix B.14.

9.3

Result and comparison with JAGS

The pdf of p, given the set of conditions I, to which we have added r0 and s0 in order
to remind that it also depends on the chosen family for the prior, is finally
  
 
nP
ns 
X
X
1
ns
ns − nI
nI
r0 −1
s0 −1
f (p | I, r0, s0 ) =
·
·
·
· [p
· (1 − p)
]·
nI
nP − nPI
Nf
nPI
n =0 n =0
PI

Γ(nPI + r1 ) · Γ(nI − nPI + s1 )
·
Γ(r1 + nI + s1 )

+ r2 ) · Γ(ns − nI − nP + nPI + s2 )
.
Γ(ns − nI + s2 + r2 )

pnI · (1 − p)ns −nI ·
Γ(nP − nPI

I

(101)

So, although we have not been able to get an analytic solution, which for problems
of this kind is out of hope, we have got an expression for f (p | I, r0, s0 ), that we
57

The R package PearsonDS [35] also contains a random number generator, used in the script,
very convenient if further Monte Carlo integrations/simulations starting from f (p) are needed.

88

can compute numerically and check against the JAGS results seen in Sec. 8. For
the purpose of this work, we did not put particular effort in trying to speed up the
calculation of Eqs. (98)-(99) and therefore the comparison concerns only the result,
and not the computer time or other technical issues. The agreement is excellent, even
when we are dealing with numbers as large as 10000 for ns (and a few thousands for
nP ). For example, the comparison using the same values of nP = 2010 and ns = 10000
of Sec. 8 is shown in the upper plot of Fig. 32:
• The histogram peaked around p = 0.1 is the JAGS result obtained by 107
iterations, with over-imposed the pdf evaluated making use of Eq. (101), starting from a uniform prior (magenta dashed line). In terms of expected value
± standard uncertainty the direct calculation (exact – see Eq. (100) and Appendix B.13) gives p = 0.0981 ± 0.0233 versus p = 0.0984 ± 0.0224 of JAGS
(with exaggerated number of decimal digits just for detailed comparison).
• Then we have changed the prior, choosing one strongly preferring high values of
p (dotted magenta curve) with rather small uncertainty: a Beta(57, 38), yielding
an expected value of 0.60 with standard deviation of 0.05. This new prior has
the effect of ‘pulling’ the distribution of p on the right side. The agreement
of the results obtained by the two methods is again excellent, resulting in p =
0.1669 ± 0.0093 and p = 0.01658 ± 0.0093 (direct and JAGS, respectively).
Then we repeat the game with a sample ten times smaller, that is ns = 1000, and
assuming nP = 201 (lower plot in the same figure). Again the agreement between
direct calculation and MCMC sampling is excellent.
It is worth noting that the possibility to write down an expression for the pdf
of interest for an inferential problem with several nodes, after marginalization over
six variables, has to be considered a lucky case, thanks also to the approximation of
modeling the sampling by a binomial rather than a hypergeometric and to the use of
conjugate priors. The purpose of this section is then mainly didactic, being the valuation of the pdf’s of other variables (and of several variables all together) and of their
moments prohibitive. It is then clear the superiority of estimates based on MCMC
methods, whose advent several decades ago has been a kind of revolution, which have
given a boost to Bayesian methods for ‘serious’ multidimensional applications, tasks
before not even imaginable.58
58

It seems (the episode has be referred to one of us by a statistician present at the lectures) that
in the 80’s Dennis Lindley ended a lecture series telling something like “You see, I have shown you
a wonderful, logically consistent theory. There is only a ‘little’ problem. We are unable to do the
calculations for the high dimensional problems that occur in real applications.”

89

40
30

f(p)

20
10
0

0.1

0.2

0.3

0.0

0.1

0.2

0.3

p

0.4

0.5

0.6

0.4

0.5

0.6

15
0

5

10

f(p)

20

25

0.0

p

Figure 32: Direct computation of Eq. (101) (solid lines) vs JAGS results (histograms) for the
flat prior (magenta dashed line) and for a Beta(57, 38) (magenta dotted line). Upper plot:
nP = 2010 and ns = 10000. Lower plot: nP = 201 and ns = 1000.

90

9.4

More remarks on the role of priors

Having checked the agreement between the two methods, let us now focus the attention on the results themselves. Looking at the results from the smaller sample we
note:
• The width of the distribution using a flat prior is wider for the small sample
than that obtained with larger ‘statistics’, as expected, with a tiny variation in
the mean value: p = 0.099 ± 0.027.
• The prior Beta(57, 38) causes a larger shift of the distribution towards higher
values of p, thus yielding p = 0.204 ± 0.015.
It is interesting to compare these results with what we have seen in Sec. 8.7 (see
Fig. 31). In that case the non-flat, ‘informative’ prior had the role of ‘reshaping’ the
posterior derived by a flat prior, making thus the result acceptable by the ‘expert’,
because the outcome was not in contrast with her prior belief. Here, instead, the
result provided by a flat prior is so far from the rational belief (most likely shared
by the relevant scientific community) of the expert, that the result would not be
accepted acritically. Most likely the expert would mistrust the data analysis, or the
data themselves. But she would perhaps also analyze critically her prior beliefs in
order to understand on what they were really grounded and how solid they were. As
a matter of fact, scientists are ready to modify their opinion, but with some care, and,
as the famous motto says, “extraordinary claims require extraordinary evidence”.
Since scientific priors are usually strongly based on previous experimental information, the problem of ‘logically merging’ a prior preference summarized by ≈ 0.60±0.05
and a new experimental results preferring ‘by itself’ (that is when the result is dominated by the ‘likelihood’ – see Sec. 8.7), summarized as 0.098 ± 0.023 (or ±0.027,
depending on ns ) is similar to that of ‘combining apparently incompatible results.’
Also in that case, nobody would acritically accept the ‘weighted average’ of the two
results which appear to be in mutual disagreement. A so called ‘skeptical combination’ should be preferred, which would even yield a multi-modal distribution [34].
This means that in a case like those of Fig. 32 the expert could think that either
• she is right, with probability P, and she would just stick to her prior f0 (p);
• she is wrong, with probability 1 − P, and she would switch to the posterior
provided by the likelihood alone, let us indicate it with fL (p).
Therefore the degrees of belief of p will be described by f (p) = P ·f0 (p)+(1−P)·fL (p).
As far as we understand from our experience she would hardly believe the result
obtained, ‘technically’, plugging her prior in the formulae – and we keep repeating
once more Laplace’s dictum that “probability is good sense reduced to a calculus”.
91

1e−02
1e−09
1e−16
1e−30

1e−23

f(p)

0.00

0.05

0.10

0.15

0.20

0.25

0.30

p

Figure 33: Closer look at the effect of the prior Beta(57, 38) shown in Fig. 32.
In order to make our point more clear, let us look into the details of the situation
depicted in Fig. 32 with the help of Fig. 33, in which f (p) is reported in log scale, and
the abscissa limited to the region of interest. The blue curves, which are dominant
below p ≈ 0.10, represent the posteriors obtained by a flat prior (solid for ns = 10000
and nP = 2010; dashed for ns = 1000 and nP = 201). Then, the dotted magenta
curve is the tail at small p of the prior Beta(57, 58), which prefers values of p around
≈ 0.60±0.05. Then the red curves (solid and dashed as previously) show the posterior
distributions obtained by this new prior.
The shift of both distributions towards the right side is caused by the dramatic reshaping due to prior in the region between p ≈ 0.1 and p ≈ 0.3 in which
f0 (p | Beta(57, 38)) varies by about 25 orders of magnitudes (!). The question is then
that no expert, who believes a priori that p should be most likely in the region be92

tween 0.5 and 0.7 (and almost certainly not below 0.40-0.45), can have a defensible,
rational belief that values of p around 0.3 are 10≈25 times more probable than values
around 0.1. More likely, once she has to give up her prior, she would consider small
values of p equally likely. For this reason – let us put in this way what we have
said just above – she will be in the situation either to completely mistrust the new
outcome, thus keeping her prior, or the other way around. The take-away message is
therefore just the (trivial) reminder that mathematical models are in most practical
cases just dictated by practical convenience and should not been taken literally in
their extreme consequences, as Gauss promptly commented on the “defect” of his
error function immediately after he had derived it [30]. Therefore our addendum to
Laplace’s dictum reminded above is don’t get fooled by math.

10

Conclusions

In this paper we went through the issues of ‘stating’ if an individual belong to a particular class and in ‘counting’ the number of individuals in a population belonging to
that class. Since the casus belli was the Covid-19 pandemic, we have been constantly
speaking of (currently and past) ‘infectees’, although our work is rather general. A
well understood complication related to the above tasks is due to the fact that the
assignment of an individual to the class of interest is performed by ’proxies’ provided
by the test result, in this case ‘positive’ or ‘negative’. Having defined π1 the probability that the test result gives positive if the individual is infected (‘sensitivity’) and
π2 the probability of positive if not infected (1 − π2 being the ‘specificity’), we have
analyzed the impact on the results of the fact that not only these ‘test parameters’ are
far from being ideal (π1 6= 1 and π2 6= 0), but that their values are indeed uncertain.
We have started our work using parameters that can be summarized as π1 =
0.978 ± 0.007 and π2 = 0.115 ± 0.022, based on the nominal data provided by Ref. [2]
(π1 = 0.98 and π2 = 0.12), and used probability theory, and in particular the so
called Bayes’ rule, in order to
• evaluate the probability that an individual declared positive is infected (and so
on for the other possibilities);
• evaluate the proportion of infectees in a population, based on the number of
positive in a tested sample.
In both problems the role of ‘priors’ is logically crucial, although in practice it has a
different impact on the numerical result:
• the probability that an individual tagged as positive is infected depends strongly
on the probability of being infected based on other pieces of information and
93

knowledge (in the idealistic case of ‘zero knowledge’ this prior probability is
just the assumed proportion of infectees in the population);
• the probability density function of the proportion of infectees in the population
has, instead, usually a weak dependence on the prior beliefs about the same
proportion.
The dependence on the fact that the tests are ‘imperfect’ has a different impact on
the result:
• the probability of infected if positive depends strongly, as expected, on the
values (‘expected values’, in probabilistic terms) of π1 and π2 , while, rather
surprisingly, it depends very little on their uncertainty;
• the inference of the proportion of infectees, instead, depends strongly on their
uncertainty, but very little on their expected values.
The latter outcome is important for planning test campaigns to count and regularly
monitor the number of infectees in a population, for which tests with relatively low
sensitivity and specificity can be employed. This second task has been analyzed in
detail by exact evaluations, Monte Carlo methods and approximated formulae, first
to understand the accuracy of the predictions of the number of positives that would
result in a sample of the population, assuming a given proportion of infectees in
the population; then to infer the proportion of infectees in the population from the
observed number of positives.
The preliminary work of predicting the number of positives has been particularly
important because it has allowed us to produce approximated formulae with which
we can disentangle the contributions to the overall uncertainty of prediction, which
has a somehow specular relation with the uncertainty in inference. This allows to
classify then the contributions into ’statistics’ (those depending on the sample size,
due to the probabilistic effects of sampling) and ‘systematics’ (those not depending
on the sample size, due then to the uncertainties on π1 and π2 ). As a consequence
it is possible to evaluate the critical sample size, above which uncertainties due to
systematics are dominant, and therefore it is not worth increasing the sample size.
Moreover, the fact that the uncertainties about π1 and π2 act as systematics
(within the limitation of our model, clearly stated in Sec. 5.2) suggests that we can
evaluate differences of proportions of infectees in different populations much better
than how we can measure a single proportion. This observation has an important
practical consequence, because one could measure the proportion of infectees in a
subpopulation (think e.g. to a Region of a Country) both with a test of higher quality
(and presumably more expensive) and with a cheaper, rapid and less accurate one
and therefore use the result as calibration point for the other subpopulations.
94

References
[1] https://it.wikipedia.org/wiki/Auditel .
[2] Marco Lillo, Covid: sono stato infettato, ma l’ho scoperto da solo, Il Fatto
Quotidiano, 8 April 2020,
https://www.ilfattoquotidiano.it/in-edicola/articoli/2020/04/08/
covid-sono-stato-infettato-ma-lho-scoperto-da-solo/5763424/.
[3] https://en.wikipedia.org/wiki/Immunoglobulin_M .
[4] https://en.wikipedia.org/wiki/Immunoglobulin_G .
[5] G. D’Agostini, Probability, Propensity and Probability of Propensities (and of Probabilities), AIP Conference Proceedings 1853, 030001 (2017),
https://arxiv.org/abs/1612.05292.
[6] G. D’Agostini, A defense of Columbo (and of the use of Bayesian inference in forensics): A multilevel introduction to probabilistic reasoning,
https://arxiv.org/abs/1003.2086 .
[7] G. D’Agostini and A. Esposito, Cosı̀ è. . . probabilmente – Il saggio, l’ingenuo e la
signorina Bayes, Ilmiolibro, 2016, (Italian only), https://ilmiolibro.kataweb.
it/libro/storia-e-filosofia/102643/cos-probabilmente/.
[8] G. D’Agostini, N. Cifani and A. Gilardi, Talking about Probability, Inference and
Decisions. Part 1: The Witches of Bayes (Italian only), Progetto Alice, Vol. XIX,
nr. 55 pp. 73-134, 2018, https://arxiv.org/abs/1802.10432.
[9] S. L. Frasier, Coronavirus Antibody Tests Have a Mathematical Pitfall, Scientific American, July 1, 2020, https://www.scientificamerican.com/article/
coronavirus-antibody-tests-have-a-mathematical-pitfall/ .
[10] M. Plummer, JAGS: A Program for Analysis of Bayesian Graphical Models Using
Gibbs Sampling, Proceedings of the 3rd International Workshop on Distributed Statistical Computing (DSC 2003), March 20–22, Vienna, Austria. ISSN 1609-395X,
http://mcmc-jags.sourceforge.net/ .
[11] R Core Team (2018), R: A language and environment for statistical computing. R
Foundation for Statistical Computing, Vienna, Austria.
https://www.R-project.org/ .
[12] M. Plummer, rjags: Bayesian Graphical Models using MCMC.
R package version 4-10, https://CRAN.R-project.org/package=rjags .

95

[13] P.S. Laplace, Mémoire sur la probabilité des causes par les événements”, Mémoire
de l’Académie royale des Sciences de Paris (Savants étrangers), Tome VI, p. 621,
1774, https://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32 .
[14] https://en.wikipedia.org/wiki/Sensitivity_and_specificity .
[15] International Organization for Standardization (ISO), Guide to the expression of
uncertainty in measurement, Geneva, Switzerland, 1993.
[16] Nicholas M. Coquillard, Note, Negligent HIV Testing and False-Positive Plaintiffs:
Pardoning the Traditional Prerequisites for Emotional Distress Recovery, 43 Clev.
St. L. Rev. 655 (1995).
[17] N. Fenton et al., Bayes and the Law, Ann. Rev. Stat. Appl., Vol. 3, pp. 51-77 (2016),
doi:10.1146/annurev-statistics-041715-033428,
https://www.researchgate.net/publication/298425265_Bayes_and_the_Law.
[18] J. J. Koehler, Forensic Fallacies and a famous Judge, Jurimetrics, vol. 54,
no. 3, 2014, pp. 211-219. JSTOR, https://www.jstor.org/stable/24395599.
Accessed26Aug.2020.
[19] A. Esposito, Debunking some myths about biometric authentication,
https://arxiv.org/abs/1203.0333 .
[20] G. D’Agostini, The Gauss’ Bayes Factor,
https://arxiv.org/abs/2003.10878 .
[21] Bayes, Thomas and Price, Richard An Essay towards solving a Problem in the
Doctrine of Chance. By the late Rev. Mr. Bayes, communicated by Mr. Price, in a
letter to John Canton, A.M.F.R.S., Philosophical Transactions of the Royal Society
of London. 53: 370–418, (1763), https://doi.org/10.1098%2Frstl.1763.0053 .
[22] M. Bognar, Probability distributions,
https://play.google.com/store/apps/details?id=com.mbognar.probdist,
https://apps.apple.com/us/app/probability-distributions/id889106396 .
[23] G. D’Agostini, Bayesian Inference in Processing Experimental Data: Principles and
Basic Applications, Rept.Prog.Phys. 66 (2003) 1383-1420,
https://arxiv.org/abs/physics/0304102 .
[24] G. D’Agostini, Bayesian Reasoning in Data Analysis. A critical Introduction, World
Scientific, 2003.
[25] https://en.wikipedia.org/wiki/Hypergeometric_distribution .
[26] C. Andrieu et al., An introduction to MCMC for Machine Learning, Machine Learning 50 5-43 (2003), https://doi.org/10.1023/A:1020281327116 .

96

[27] D. Lunn et al., The BUGS project: Evolution, critique and future directions, Statistics in Medicine 28 3049-3067 (2008), https://doi.org/10.1002/sim.3680 .
[28] The BUGS Project, http://www.mrc-bsu.cam.ac.uk/software/bugs/ .
[29] http://www.openbugs.net/w/Examples .
[30] C.F. Gauss, Theoria motus corporum coelestium in sectionibus conicis solem ambientum, Hamburg 1809, https://archive.org/details/bub_gb_ORUOAAAAQAAJ .
[31] P. Astone and G. D’Agostini, Inferring the intensity of Poisson processes at the limit
of the detector sensitivity (with a case study on gravitational wave burst search),
CERN-EP/99-126, https://arxiv.org/abs/hep-ex/9909047 .
[32] G. D’Agostini and G. Degrassi, Constraints on the Higgs boson mass from direct
searches and precision measurements, Eur. Phys. J. C10 (1999) 633, https://
arxiv.org/abs/hep-ph/9902226 .
[33] S. Gariazzo, Constraining power of open likelihoods, made prior-independent,
https://arxiv.org/abs/1910.06646.
[34] G. D’Agostini, Sceptical combination of experimental results using JAGS/rjags with
application to the K± mass determination,
https://arxiv.org/abs/2001.03466v1 .
[35] M. Becker and S. Klössner, PearsonDS: Pearson Distribution System,
https://cran.r-project.org/web/packages/PearsonDS/index.html .

97

Appendix A – Some remarks on ‘Bayes’ formulae’
Equation (6) is a straight consequence of the probability rule relating joint probability
to conditional probability, that is, for the generic ‘events’ A and B,
P (A ∩ B) = P (B | A) · P (A) = P (A | B) · P (B) ,

(A.1)

having added to P (Inf) of Eq. (6) the suffix ‘0’ in order to emphasize its role of ‘prior’
probability. Equation (A.1) yields trivially
P (A | B) =

P (B | A) · P (A)
P (B)

−→

P (B | A) · P0 (A)
,
P (B)

(A.2)

having also emphasized that P (A) in r.h.s. is the probability of A before it is updated
by the new condition B.59 But, indeed, the essence of the Bayes’ rule is given by
P (A | B) =

P (A ∩ B)
P (A, B)
=
,
P (B)
P (B)

(A.3)

in which we have rewritten the ‘A ∩ B’ in the way it is custom for uncertain numbers (‘random variables’), as we shall see in while. Moreover, as we can ‘expand’
the numerator (using the so called chain rule) to go from Eq. (A.3) to Eq. (A.2),
and then Eq. (6), similarly we can expand the denominator in two steps. We start
‘decomposing’ B into B ∩ A and B ∩ A, from which it follows
B = (B ∩ A) ∪ (B ∩ A)
P (B) = P (B ∩ A) + P (B ∩ A)
= P (B | A) · P (A) + P (B | A) · P (A)
After the various ‘expansions’ we can rewrite Eq. (A.3) as
P (A | B) =

P (B | A) · P (A)
.
P (B | A) · P (A) + P (B | A) · P (A)

(A.4)

59
Remember that all elicitations of probabilities always depend on some
tions/hypotheses/assumptions. Therefore Eq. (A.2) should be written, more properly, as

P (A | B, I)

=

condi-

P (B | A, I) · P0 (A | I)
P (B | I)

with I the (common!) background status of information under which all probabilities appearing
in the equation are evaluated, although it is usually implicit in the equations to make them more
compact, as we have done in this paper.

98

Finally, if instead of only two
P possibilities A and A, we have a complete class of
hypotheses Hi , i.e. such that i P (Hi ) = 1 and P (Hi ∩ Hj ) = 0 for i 6= j, we get the
famous
P (E | Hi ) · P (Hi )
P (Hi ∩ E)
P (Hi | E) = P
,
(A.5)
←−
P (E)
i P (E | Hi ) · P (Hi )

having also replaced the symbol B by E, given its meaning of effect, upon which the
probabilities of the different hypotheses Hi are updated. Moreover, the sum in the
denominator of the first r.h.s. of Eq. (A.5) makes it explicit that the denominator
is just a normalization factor, and therefore the essence of the reasoning can be
expressed as
P (Hi | E) ∝ P (E | Hi ) · P (Hi ) = P (Hi ∩ E)

(A.6)

The extension to discrete ‘random variables’ is straightforward, since the probability
distribution f (x) has the meaning of P (X = x), with X the name of the variable
and x one of the possible values that it can assume. Similarly, f (x, y) stands for
P (X = x, Y = y) ≡ P ((X = x) ∩ (Y = y)), f (x | y) for P (X = x | Y = y), and so
on. Moreover all possible values of X, as well as all possible values of Y , form a
complete class of hypotheses (the distributions are normalized). Equation (A.3) and
its variations and ‘expansions’ becomes then, for X and Y ,
f (x, y)
f (y | x) · f (x)
f (y | x) · f (x)
f (y | x) · f (x)
f (x | y) =
=
= P
=P
f (y)
f (y)
x f (y, x)
x f (y | x) · f (x)
∝ f (y | x) · f (x) = f (x, y) ,
(A.7)

which can be further extended to several other variables. For example, adding Z, V
and W and being interested to the joint probability that X and Z assume the values
x and z, conditioned by Y = y, V = v and W = w, we get
f (x, y, v, w, z)
f (x, z | y, v, w) =
.
(A.8)
f (y, v, w)
To conclude, some remarks are important, especially for the applications:
1. Equations (A.7) and (A.8) are valid also for continuous variables, in which case
the various ‘f ()’ have the meaning of probability density function, and the sums
needed to get the (possibly joint) marginal in the denominator are replaced by
integration.

2. The numerator of Eq. (8) is ‘expanded’ using a chain rule, choosing, among
the several possibilities, that which makes explicit the (assumed) causal connections60 of the different variables in the game, as stressed in the proper places
through the paper (see e.g. footnote 15, Sec. 7 and Sec. 9).
60

Causality is notoriously something tricky, and conditioning does not necessarily imply causation!

99

3. A related remark is that, among the variables entering the game, as those
of Eq. (A.8), some may be continuous and other discrete and the probabilistic
meaning of ‘f (. . .)’, taking the example of a bivariate case f (x, y) with x discrete
and y continuous, is given by P (XP= x,
R y ≤ Y ≤ y + dy) = f (x, y) dy, with the
normalization condition given by x f (x, y) dy = 1.

4. Finally, a crucial observation is that, given the model which connects the variables (the graphical representations of the kinds shown in the paper are very
useful to understand it) and its parameters, the denominator of Eq. (A.8) is
just a number (although often very difficult to evaluate!), and therefore, as we
have seen in Eq. (A.7), the last equation can be rewritten as(∗)
f (x, z | y, v, w) ∝ f (x, y, v, w, z) ,

(A.9)

or, denoting by f˜() the un-normalized posterior distribution,
f˜(x, z | y, v, w) = f (x, y, v, w, z) .

(A.10)

The importance of this remark is that, although a closed form of posterior is
often prohibitive in practical cases, an approximation of it can be obtained by
Monte Carlo techniques, which allow us to evaluate the quantities of interest,
like averages, probability intervals, and so on (see references in footnote 40).
——————————————————————————————————
(∗)

Perhaps a better way to rewrite (A.9) and (A.10), in order to avoid confusion, could be
f (x, z | y = y0 , v = v0 , w = w0 ) ∝
f˜(x, z | y = y0 , v = v0 , w = w0 ) =

f (x, y0 , v0 , w0 , z)
f (x, y0 , v0 , w0 , z) ,

in order to emphasize the fact that y, v and w assume precise values, under which the possible
values of x and z are conditioned. Anyway, it is just a question of getting used with that notation.
For example, sticking to a textbook two dimensional case, the bivariate normal distribution is given
by



1
1
(x − µx )2
(x − µx )(y − µy ) (y − µy )2
p
f (x, y) =
exp −
.
−2ρ
+
2 (1 − ρ2 )
σx2
σx σy
σy2
2 π σx σy 1 − ρ2

100

The distribution of x, conditioned by y = y0 is then



(x − µx )2
(x − µx )(y0 − µy ) (y0 − µy )2
1
1
p
−
2
ρ
+
f (x | y0 ) ∝
exp −
2 (1 − ρ2 )
σx2
σx σy
σy2
2 π σx σy 1 − ρ2



1
(x − µx )2
x (y0 − µy )
∝ exp −
−2ρ
2
2
2 (1 − ρ )
σ
σx σy
 x



σx
1
2
x − 2 x µx + ρ (y0 − µy )
∝ exp −
2 (1 − ρ2 ) σx2
σy
h

i 

σ
2
x
 x − 2 x µx + ρ σ (y0 − µy ) 
y
∝ exp −
2


2 (1 − ρ ) σx2
 h

i2 


 x − µx + ρ σσx (y0 − µy )

y
∝ exp −
,


2 (1 − ρ2 ) σx2



p
in which we recognize a Gaussian distribution with µx|y0 = x+ρ σσyx (y0 −µy ) and σx|y0 = 1 − ρ2 σx .
[In the various steps all factors (and hence all addends at the exponent) not depending on x have
been ignored. Finally, in the last step the ‘trick’ of complementing the exponential has been used,

2
because adding µx + ρ σσxy (y0 − µy ) / (2 (1 − ρ2 ) σx2 ) at the exponent is the same as multiplying
by a constant factor.]

101

Appendix B – R and JAGS code
B.1 – Monte Carlo evaluation of Eqs. (39) and (40)
r.pi1 = 409.1; s.pi1 = 9.1
r.pi2 = 25.1; s.pi2 = 193.1
p = 0.1
n = 100000
pi1 <- rbeta(n, r.pi1, s.pi1)
pi2 <- rbeta(n, r.pi2, s.pi2)
P.Inf.Pos.i
<- pi1*p/(pi1*p + pi2*(1-p))
P.NoInf.Neg.i <- (1-pi2)*(1-p) / ((1-pi1)*p + (1-pi2)*(1-p))
P.Inf.Pos
<- mean(P.Inf.Pos.i)
P.NoInf.Neg <- mean(P.NoInf.Neg.i)
cat(sprintf("Integral (by MC): P(Inf|Pos) = %.4f; P(NoInf|Neg) = %.4f \n",
P.Inf.Pos, P.NoInf.Neg))
E.pi1 = r.pi1 / (r.pi1 + s.pi1)
E.pi2 = r.pi2 / (r.pi2 + s.pi2)
cat(sprintf("Using E(..): P(Inf|Pos) = %.4f; P(NoInf|Neg) = %.4f \n",
E.pi1*p/(E.pi1*p + E.pi2*(1-p)),
(1-E.pi2)*(1-p) / ((1-E.pi1)*p + (1-E.pi2)*(1-p))))

B.2 – Monte Carlo evaluation of Eq. (48)
ns = 10000
ps = 0.1
n.I = ps * ns
n.NI = (1 - ps) * ns
r1=409.1; s1=9.1
r2=25.1; s2=193.2
nr =100000
pi1 <- rbeta(nr, r1, s1)
pi2 <- rbeta(nr, r2, s2)
nP.I <- rbinom(nr, n.I, pi1)
nP.NI <- rbinom(nr, n.NI, pi2)
nP <- nP.I + nP.NI
hist(nP, nc=100, col=’cyan’, freq=FALSE)
cat(sprintf("nP: mean = %.1f, sd = %.1f\n",mean(nP),sd(nP)))

102

B.3 – Monte Carlo estimate of f (nP | N = 105, ns = 104, p = 0.1)
reported in Fig. 12
N = 100000; ns = 10000; p = 0.1
N.I <- as.integer(p*N)
N.NI <- N - N.I
E.pi1 = 0.978; E.pi2 = 0.115
nr =100000
n.I <- rhyper(nr, m=N.I, n=N.NI, k=ns)
n.NI <- ns - n.I
nP.I <- rbinom(nr, n.I, E.pi1)
nP.NI <- rbinom(nr, n.NI, E.pi2)
nP <- nP.I + nP.NI
cat(sprintf("mean = %.0f, sigma = %.0f\n", mean(nP),
hist(nP, nc=90, col=’cyan’, freq=FALSE)

sd(nP) ))

B.4 – σ(fP ) and σ(fP )/E(fP ), with detailed contributions to
them, using the approximated Eqs. (69)-(74)
p = 0.1
N = 10^7
# N >> ns --> binomial
ns = 1000
E.pi1 = 0.978; sigma.pi1 = 0.007
E.pi2 = 0.115; sigma.pi2 = 0.022
# E.pi2 = 0.115; sigma.pi2 = sigma.pi1
# E.pi2 = 1 - E.pi1; sigma.pi2 = sigma.pi1

# reduced sigma.pi2
# improved specificity

E.ps <- p
E.fP <- E.pi1*E.ps + E.pi2*(1-E.ps)
s.fP.R
s.fP.pi1
s.fP.pi2
s.ps
s.fP.ps

<<<<<-

sqrt( E.pi1*(1-E.pi1)*E.ps + E.pi2*(1-E.pi2)*(1-E.ps) )/sqrt(ns)
sigma.pi1*E.ps
sigma.pi2*(1-E.ps)
sqrt(p*(1-p)*(1-ns/N))/sqrt(ns)
s.ps*abs(E.pi1-E.pi2)

s.fP.stat <- sqrt(s.fP.R^2+s.fP.ps^2)
s.fP.syst <- sqrt(s.fP.pi1^2+s.fP.pi2^2)
s.fP = sqrt(s.fP.stat^2 + s.fP.syst^2)

103

cat(sprintf("p = %.2f; ns = %d\n", p, ns))
cat(sprintf("E(fN) = %.3f; sigma(fN) = %.3f; sigma(fN)/E(fN) = %.2f\n",
E.fP, s.fP, s.fP/E.fP))
cat("Contributions : \n")
cat(sprintf("
s.fP.R
= %.3e; s.fP.R/E.fP
= %.2e\n",
s.fP.R, s.fP.R/E.fP))
cat(sprintf("
s.fP.p1
= %.3e; s.fP.p1/E.fP
= %.2e\n",
s.fP.pi1, s.fP.pi1/E.fP))
cat(sprintf("
s.fP.p2
= %.3e; s.fP.p2/E.fP
= %.2e\n",
s.fP.pi2, s.fP.pi2/E.fP))
cat(sprintf("
s.fP.ps
= %.3e; s.fP.ps/E.fP
= %.2e\n",
s.fP.ps, s.fP.ps/E.fP))
cat(sprintf("
s.fP.stat = %.3e; s.fP.stat/E.fP = %.2e\n",
s.fP.stat, s.fP.stat/E.fP))
cat(sprintf("
s.fP.syst = %.3e; s.fP.syst/E.fP = %.2e\n",
s.fP.syst, s.fP.syst/E.fP))

B.5 – Monte Carlo estimate of fP using R functions, as described in Sec. 7.2.1 (see Fig.21)
p
ns
r1
r2

= 0.1
= 1000
= 409.1;
= 25.1;

s1 = 9.1
s2 =193.2

nr = 10000
n.I
<- rbinom(nr, ns, p)
n.NI <- ns - n.I
pi1
<- rbeta(nr, r1, s1)
pi2
<- rbeta(nr, r2, s2)
nP.I <- rbinom(nr, n.I, pi1)
nP.NI <- rbinom(nr, n.NI, pi2)
nP
<- nP.I + nP.NI

# 1.
# 2.
# 3.
# 4.

fP <- nP/ns
cat(sprintf("fP: %.3f +- %.3f\n", mean(fP), sd(fP)))
hist(fP, col=’cyan’)
# barplot(table(fP), col=’cyan’) # alternative, for small ns and p

104

B.6 – Monte Carlo estimate of fP JAGS from R via rjags
(Sec. 7.2.2)
#-------------------------- JAGS model -----------------------------model.name = "tmp_model.bug"
write("
model {
n.I ~ dbin(p, ns)
n.NI <- ns - n.I
nP.I ~ dbin(pi1, n.I)
nP.NI ~ dbin(pi2, n.NI)
pi1 ~ dbeta(r1, s1)
pi2 ~ dbeta(r2, s2)
nP ~ sum(nP.I, nP.NI)
fP <- nP / ns
}
", model)
#--------------------------- call JAGS --------------------------------library(rjags)
data <- list(ns=1000, p=0.1, r1=409.1, s1=9.1, r2=25.2, s2=193.1)
jm <- jags.model(model, data)
chain <- coda.samples(jm, c(’n.I’, ’fP’), n.iter=10000)
#--------------------------- Results -----------------------------------print(summary(chain))
plot(chain, col=’blue’)

B.7 – Check of approximated formulae
get.prediction <- function(ns, N, p, E.pi1, sigma.pi1, E.pi2, sigma.pi2) {
E.ps <- p
E.fP <- E.pi1*E.ps + E.pi2*(1-E.ps)
s.fP.R
s.fP.pi1
s.fP.pi2
s.ps
s.fP.ps

<<<<<-

sqrt( E.pi1*(1-E.pi1)*E.ps + E.pi2*(1-E.pi2)*(1-E.ps) )/sqrt(ns)
sigma.pi1*E.ps
sigma.pi2*(1-E.ps)
sqrt(p*(1-p)*(1-ns/N))/sqrt(ns)
s.ps*abs(E.pi1-E.pi2)

s.fP.stat <- sqrt(s.fP.R^2+s.fP.ps^2)
s.fP.syst <- sqrt(s.fP.pi1^2+s.fP.pi2^2)

105

s.fP = sqrt(s.fP.stat^2 + s.fP.syst^2)
return(list(E.fP=E.fP, s.fP=s.fP))
}
N = 10^7
p.v <- 0:4 / 10
ns.v <- c(300, 1000, 3000, 10000)
E.pi1 = 0.978; sigma.pi1 = 0.007
E.pi2 = 0.115; sigma.pi2 = 0.022
for(case in 1:3) {
if(case == 2) sigma.pi2 = sigma.pi1
if(case == 3) E.pi2 = 1 - E.pi1
cat(sprintf("\n [pi1 = %.3f+-%.3f;", E.pi1, sigma.pi1))
cat(sprintf(" pi2 = %.3f+-%.3f]\n", E.pi2, sigma.pi2))
for(i in 1:length(ns.v)) {
ns <- ns.v[i]
cat(sprintf(" ns = %d\n p: ", ns))
for(j in 1:length(p.v)) cat(sprintf("
%.2f
", p.v[j]))
cat("\nfP: ")
for(j in 1:length(p.v)) {
p <- p.v[j]
pred <- get.prediction(ns, N, p, E.pi1, sigma.pi1, E.pi2, sigma.pi2)
cat(sprintf("%.3f+-%.3f ", pred$E.fP, pred$s.fP))
}
cat("\n")
}
}

B.8 – Predicting the fractions of positives obtained sampling
two different populations (Sec. 7.4)
get.fP <- function(nr, p, ns, pi1, pi2) {
n.I
<- rbinom(nr, ns, p)
n.NI <- ns - n.I
nP.I <- rbinom(nr, n.I, pi1)
nP.NI <- rbinom(nr, n.NI, pi2)
nP
<- nP.I + nP.NI
fP
<- nP/ns
return(fP)
}

106

p1
p2
ns
r1
r2

=
=
=
=
=

0.1
0.2
10000
409.1;
25.1;

s1 = 9.1
s2 =193.2

nr = 100000
pi1 <- rbeta(nr, r1, s1)
pi2 <- rbeta(nr, r2, s2)
fP1 <- get.fP(nr, p1, ns, pi1, pi2)
fP2 <- get.fP(nr, p2, ns, pi1, pi2)
cat(sprintf("fP1: %.4f +- %.4f\n", mean(fP1), sd(fP1)))
cat(sprintf("fP2: %.4f +- %.4f\n", mean(fP2), sd(fP2)))
cat(sprintf("fP2-fP1: %.4f +- %.4f\n", mean(fP2-fP1), sd(fP2-fP1)))
cat(sprintf("rho(fP1,fP2): %.4f\n", cor(fP1,fP2)))
cat(sprintf("Check of sigma(fP2-fP1) from correlation: %.4f\n",
sqrt(var(fP1)+var(fP2)-2*cov(fP1,fP2))))
(Note how the ‘random’ sequences of values of pi1 and pi2 are generated before the calls
to get.fP(). This is crucial in order to get the correlation among fP1 and fP2 discussed
in Sec. 7.4. If these two sequences are defined, each time, inside the function, or they
a generated before each call to the function, the correlation will disappear. This way of
generating the events is consequence of our model assumptions, as stressed in Sec. 5.2.)

B.9 – JAGS model to perform the same Monte Carlo evaluation done in Appendix B.8
(Only the model is provided – steering R commands are left as exercise.)
model {
n.I.1 ~ dbin(p1, ns1)
n.NI.1 <- ns1 - n.I.1
nP.I.1 ~ dbin(pi1, n.I.1)
nP.NI.1 ~ dbin(pi2, n.NI.1)
nP.1 ~ sum(nP.I.1, nP.NI.1)
fP.1 <- nP.1 / ns1
n.I.2 ~ dbin(p2, ns2)
n.NI.2 <- ns2 - n.I.2
nP.I.2 ~ dbin(pi1, n.I.2)
nP.NI.2 ~ dbin(pi2, n.NI.2)
nP.2 ~ sum(nP.I.2, nP.NI.2)

107

fP.2 <- nP.2 / ns2
D.fP <- fP.2 - fP.1
pi1 ~ dbeta(r1, s1)
pi2 ~ dbeta(r2, s2)
}

B.10 – JAGS model to infer p (see Sec. 8.2)
#---- data and parameters
nr = 1000000
ns = 10000
nP = 2010
r0 = s0 = 1
r1 = 409.1; s1 = 9.1
r2 = 25.2; s2 = 193.1
#---- JAGS model -----------------------------model = "tmp_model.bug"
# name of the model file (’temporary’)
write("
model {
nP ~ sum(nP.I, nP.NI)
nP.I ~ dbin(pi1, n.I)
nP.NI ~ dbin(pi2, n.NI)
pi1 ~ dbeta(r1, s1)
pi2 ~ dbeta(r2, s2)
n.I ~ dbin(p, ns)
n.NI <- ns - n.I
p ~ dbeta(r0,s0)
}
", model)
#---- call JAGS --------------------------------------------library(rjags)
data <- list(ns=ns, nP=nP, r0=r0, s0=s0, r1=r1, s1=s1, r2=r2, s2=s2)
jm <- jags.model(model, data)
update(jm, 10000)
to.monitor <- c(’p’, ’n.I’)
chain <- coda.samples(jm, to.monitor, n.iter=nr)
#---- show results

108

print(summary(chain))
plot(chain, col=’blue’)

B.11 – Inferring the proportions of infected in two different
populations (see Sec. 8.6)
model = "tmp_model.bug"
write("
model {
n.I.1 ~ dbin(p1, ns1)
p1 ~ dbeta(r0, s0)
n.NI.1 <- ns1 - n.I.1
nP.I.1 ~ dbin(pi1, n.I.1)
nP.NI.1 ~ dbin(pi2, n.NI.1)
nP.1 ~ sum(nP.I.1, nP.NI.1)
n.I.2 ~ dbin(p2, ns2)
p2 ~ dbeta(r0, r0)
n.NI.2 <- ns2 - n.I.2
nP.I.2 ~ dbin(pi1, n.I.2)
nP.NI.2 ~ dbin(pi2, n.NI.2)
nP.2 ~ sum(nP.I.2, nP.NI.2)
Dp <- p2 - p1
pi1 ~ dbeta(r1, s1)
pi2 ~ dbeta(r2, s2)
}
", model)
library(rjags)
set.seed(193)
nr = 1000000
ns1 = 10000
ns2 = 10000
# they could be different
nP.1 = 2000
nP.2 = 2200
r0 = s0 = 1
# flat prior
r1 = 409.1; s1 = 9.1
r2 = 25.2; s2 = 193.1
data <- list(ns1=ns1, ns2=ns2, nP.1=nP.1, nP.2=nP.2,

109

r0=r0, s0=s0, r1=r1, s1=s1, r2=r2, s2=s2)
jm <- jags.model(model, data)
update(jm, 10000)
to.monitor <- c(’p1’, ’p2’, ’Dp’)
chain <- coda.samples(jm, to.monitor, n.iter=nr)
print(summary(chain))

B.12 – Reshaping by an informative prior the posterior distribuition obtained starting from a flat prior (see Sec. 8.7)
pause <- function() { cat ("\n >> press Enter to continue\n"); scan() }
call.jags <- function(model, data, nr) {
jm <- jags.model(model, data)
update(jm, 100)
chain <- coda.samples(jm, ’p’, n.iter=nr)
print(summary(chain))
chain.df <- as.data.frame( as.mcmc(chain) )
return(chain.df$p)
}
library(rjags)
model = "tmp_model.bug"
write("
model {
n ~ dbin(p, N)
p ~ dbeta(r0,s0)
}
", model)
nr = 100000
N = 10; n = 3
r0 = s0 = 1

# name of the model file (’temporary’)

# flat prior

# First call to JAGS
data <- list(N=N, n=n, r0=r0, s0=s0)
p <- call.jags(model, data, nr)
pause()
h <- hist(p, freq=FALSE, nc=100, col=’cyan’, xlim=c(0,1), ylim=c(0,4))
p.m <- sum(h$mids*h$counts)/sum(h$counts)
p.s <- sqrt(sum(h$mids^2*h$counts)/sum(h$counts) - p.m^2)

110

cat(sprintf(">>> JAGS:
pause()

%.4f +- %.4f\n", p.m, p.s))

# overimpose the posterion got from a Beta conjugate
curve(dbeta(x,r0+n,s0+(N-n)), col=’blue’, add=TRUE)
pause()
# Not-flat prior used to reshape the JAGS result
curve(dbeta(x,r,s), col=’magenta’, add=TRUE)
pause()
# reweighing
w <- dbeta(h$mids, r, s)
f.p.w <- h$density*w
f.p.w <- f.p.w/sum(f.p.w)/(h$mids[2]-h$mids[1])
points(h$mids, f.p.w, col=’blue’, ty=’l’, lwd=2)
p.w.m <- sum(h$mids*f.p.w)/sum(f.p.w)
p.w.s <- sqrt(sum(h$mids^2*f.p.w)/sum(f.p.w) - p.w.m^2)
cat(sprintf(">>> Reweighed: %.4f +- %.4f\n\n", p.w.m, p.w.s))
pause()
# second call to JAGS, with the new prior
data <- list(N=N, n=n, r0=r, s0=s)
p1 <- call.jags(model, data, nr)
h1 <- hist(p1, nc=100, plot=FALSE)
points(h1$mids, h1$density, col=’red’, ty=’l’, lwd=1.5)
p1.m <- sum(h1$mids*h1$counts)/sum(h1$counts)
p1.s <- sqrt(sum(h1$mids^2*h1$counts)/sum(h1$counts) - p1.m^2)
cat(sprintf(">>> JAGS(%d,%d): %.4f +- %.4f\n", r,s, p1.m, p1.s))
pause()
# New Beta obtained by the well known updating rule
curve(dbeta(x,r+n,s+(N-n)), col=’green’, add=TRUE)

B.13 – Exact calculation of E(p) and σ(p) using a Beta prior
(see Sec. 9, footnote 57)
nP
r0
r1
r2

= 201; ns = 1000
= 1; s0 = 1
= 409.1; s1 = 9.1
= 25.1; s2 = 193.1

111

Nf
<- 0
sum.p <- 0
sum.p2 <- 0
for (nPI in 0:nP) {
for (nI in 0:ns) {
l0 <- ( lchoose(nI, nPI) + lchoose(ns-nI,nP-nPI) + lchoose(ns,nI)
+ lgamma(nPI+r1) + lgamma(nI-nPI+s1) - lgamma(r1+nI+s1)
+ lgamma (nP-nPI+r2) + lgamma(ns-nI-nP+nPI+s2) - lgamma(ns-nI+s2+r2)
+ lgamma(s0+ns-nI) )
Nf
<- Nf
+ exp( l0 + lgamma(r0+nI)
- lgamma(r0+s0+ns) )
sum.p <- sum.p + exp( l0 + lgamma(r0+nI+1) - lgamma(r0+s0+ns+1) )
sum.p2 <- sum.p2 + exp( l0 + lgamma(r0+nI+2) - lgamma(r0+s0+ns+2) )
}
}
E.p <- sum.p/Nf
E.p2 <- sum.p2/Nf
cat(sprintf("nP=%d, ns=%d\n", nP, ns))
cat(sprintf("E(p)
: %.4f\n",E.p))
cat(sprintf("sigma(p): %.4f\n",sqrt(E.p2-E.p^2)))

B.14 – Approximated f (p) from the first four moments of the
distribution (see Sec. 9, Eq. (100))
library("PearsonDS") # (package needs to be installed)
pause <- function() { cat ("\n >> press Enter to continue\n"); scan() }
nP = 201; ns = 1000
r0 = 1; s0 = 1
r1 = 409.1; s1 = 9.1
r2 = 25.1; s2 = 193.1
Nf = sum.p = sum.p2 = sum.p3 = sum.p4 = 0
for (nPI in 0:nP) {
for (nI in 0:ns) {
l0 <- ( lchoose(nI, nPI) + lchoose(ns-nI,nP-nPI) + lchoose(ns,nI)
+ lgamma(nPI+r1) + lgamma(nI-nPI+s1) - lgamma(r1+nI+s1)
+ lgamma (nP-nPI+r2) + lgamma(ns-nI-nP+nPI+s2) - lgamma(ns-nI+s2+r2)
+ lgamma(s0+ns-nI) )
Nf
<- Nf
+ exp( l0 + lgamma(r0+nI)
- lgamma(r0+s0+ns) )
sum.p <- sum.p + exp( l0 + lgamma(r0+nI+1) - lgamma(r0+s0+ns+1) )
sum.p2 <- sum.p2 + exp( l0 + lgamma(r0+nI+2) - lgamma(r0+s0+ns+2) )
sum.p3 <- sum.p3 + exp( l0 + lgamma(r0+nI+3) - lgamma(r0+s0+ns+3) )
sum.p4 <- sum.p4 + exp( l0 + lgamma(r0+nI+4) - lgamma(r0+s0+ns+4) )
}

112

}
E.p <- sum.p/Nf
E.p2 <- sum.p2/Nf
s2.p <- E.p2-E.p^2
s.p <- sqrt(s2.p)
cat(sprintf("nP=%d, ns=%d\n", nP, ns))
cat(sprintf("E(p)
: %.4f\n",E.p))
cat(sprintf("sigma(p): %.4f\n",sqrt(E.p2-E.p^2)))
E.p3 <- sum.p3/Nf
Skew <- ( E.p3 - 3*E.p2*E.p + 2*E.p^3 ) / s.p^3
cat(sprintf("E(p^3) = %.3e; Skewness = %.3f \n",E.p3, Skew))
E.p4 <- sum.p4/Nf
Kurt <- ( E.p4 - 4*E.p3*E.p + 6*E.p2*E.p^2 - 3*E.p^4 )
/ s.p^4
cat(sprintf("E(p^4) = %.3e; Kurtosis = %.3f\n", E.p4, Kurt))

0

5

f(p)

10

15

moments <- c(mean = E.p,variance = s2.p,skewness = Skew, kurtosis = Kurt)
curve(dpearson(x, moments=moments), max(0.001,E.p-4*s.p), min(0.999,E.p+4*s.p),
lwd=2, col=’blue’, xlab=’p’, ylab=’f(p)’)
pr <- rpearson(100000, moments = moments)
hist(pr , nc=100, freq=FALSE, col=’cyan’, add=TRUE)
cat(sprintf("\n\n mean
: %.4f\n", mean(pr)))
cat(sprintf(" sigma: %.4f\n",sd(pr)))

0.00

0.05

0.10

p

113

0.15

0.20

