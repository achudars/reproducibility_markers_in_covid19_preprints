1

COVID-19 Infection Map Generation and Detection
from Chest X-Ray Images

arXiv:2009.12698v2 [eess.IV] 6 Jan 2021

Aysen Degerli, Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Muhammad E. H. Chowdhury, Khalid Hameed,
Tahir Hamid, Rashid Mazhar, and Moncef Gabbouj

Abstract—Computer-aided diagnosis has become a necessity
for accurate and immediate coronavirus disease 2019 (COVID19) detection to aid treatment and prevent the spread of the virus.
Numerous studies have proposed to use Deep Learning techniques
for COVID-19 diagnosis. However, they have used very limited
chest X-ray (CXR) image repositories for evaluation with a small
number, a few hundreds, of COVID-19 samples. Moreover, these
methods can neither localize nor grade the severity of COVID19 infection. For this purpose, recent studies proposed to explore
the activation maps of deep networks. However, they remain
inaccurate for localizing the actual infestation making them
unreliable for clinical use. This study proposes a novel method
for the joint localization, severity grading, and detection of
COVID-19 from CXR images by generating the so-called infection
maps. To accomplish this, we have compiled the largest dataset
with 119,316 CXR images including 2951 COVID-19 samples,
where the annotation of the ground-truth segmentation masks
is performed on CXRs by a novel collaborative human-machine
approach. Furthermore, we publicly release the first CXR dataset
with the ground-truth segmentation masks of the COVID-19
infected regions. A detailed set of experiments show that stateof-the-art segmentation networks can learn to localize COVID19 infection with an F1-score of 83.20%, which is significantly
superior to the activation maps created by the previous methods.
Finally, the proposed approach achieved a COVID-19 detection
performance with 94.96% sensitivity and 99.88% specificity.
Index Terms—SARS-CoV-2, COVID-19 Detection, COVID-19
Infection Segmentation, Deep Learning

I. I NTRODUCTION
ORONAVIRUS disease 2019 (COVID-19) caused by
severe acute respiratory syndrome Coronavirus-2 (SARsCoV-2) was first reported in December 2019 in Wuhan, China.
The highly infectious disease rapidly spread around the World
with millions of positive cases. As a result, COVID-19 was
declared as a pandemic by the World Health Organization in
March 2020. The disease may lead to hospitalization, intubation, intensive care, and even death, especially for the elderly
[1], [2]. Naturally, reliable detection of the disease has the
utmost importance. However, the diagnosis of COVID-19 is
not straight-forward since its symptoms, such as cough, fever,

C

Aysen Degerli, Mete Ahishali, Mehmet Yamac, and Moncef Gabbouj are
with the Faculty of Information Technology and Communication Sciences,
Tampere University, Tampere, Finland (e-mail: name.surname@tuni.fi).
Serkan Kiranyaz and Muhammad E. H. Chowdhury are with the Department of Electrical Engineering, Qatar University, Doha, Qatar (e-mail:
mkiranyaz@qu.edu.qa and mchowdhury@qu.edu.qa).
Khalid Hameed is an MD in Reem Medical Center, Doha, Qatar (e-mail:
dr.khalid@reemmedicalcenter.com).
Tahir Hamid is a consultant cardiologist in Hamad Medical Corporation
Hospital and with Weill Cornell Medicine - Qatar, Doha, Qatar. Rashid Mazhar
is an MD in Hamad Medical Corporation Hospital, Doha, Qatar.

Fig. 1: The COVID-19 sample CXR images, their corresponding ground-truth segmentation masks which are annotated by
the collaborative human-machine approach, and the generated
infection maps from the state-of-the-art segmentation models.

breathlessness, and diarrhea are generally indistinguishable
from other viral infections [3], [4].
The diagnostic tools to detect COVID-19 are currently
reverse transcription of polymerase chain reaction (RT-PCR)
assays and chest imaging techniques, such as Computed Tomography (CT) and X-ray imaging. Primarily, RT-PCR has
become the gold standard in the diagnosis of COVID-19
[5], [6]. However, RT-PCR arrays have a high false alarm
rate which may be caused by the virus mutations in the
SARS-CoV-2 genome, sample contamination, or damage to
the sample acquired from the patient [7], [8]. In fact, it
is shown in hospitalized patients that RT-PCR sensitivity is
low and the test results are highly unstable [6], [9]–[11].
Therefore, it is recommended to perform chest CT imaging
initially on the suspected COVID-19 cases [12], since it is
a more reliable clinical tool in the diagnosis with higher

2

sensitivity compared to RT-PCR. Hence, several studies [12]–
[14] suggest performing CT on the negative RT-PCR findings
of the suspected cases. However, there are several limitations
of CT scans. Their sensitivity is limited in the early COVID19 phase groups [15], and they are limited to recognize only
specific viruses [16], slow in image acquisition, and costly.
On the other hand, X-ray imaging is faster, cheaper, and less
harmful to the body in terms of radiation exposure compared
to CT [17], [18]. Moreover, unlike CT devices, X-ray devices
are easily accessible; hence, reducing the risk of COVID19 contamination during the imaging process [19]. Currently,
chest X-ray (CXR) imaging is widely used as an assistive tool
in COVID-19 prognosis, and it is reported to have a potential
diagnosis capability in recent studies [20].
In order to automate COVID-19 detection/recognition from
CXR images, many studies [17], [21]–[29] have proposed to
use deep Convolutional Neural Networks (CNNs). However,
the main limitation of these studies is that the data is scarce
for the target COVID-19 class. Such a limited amount of data
degrades the learning performance of the deep networks. Two
recent studies [30] and [31] have addressed this drawback
with a compact network structure and achieved the stateof-the-art detection performance over the benchmark QaTaCOV19 (initial version) and Early-QaTa-COV19 datasets that
consist of 462 and 175 COVID-19 CXR images, respectively.
Although these datasets were the largest available at that time,
such a limited number of COVID-19 samples raises robustness
and reliability issues for the proposed methods in general.
Moreover, all these previous machine learning solutions
with X-ray imaging remain limited to only COVID-19 detection. However, as stated by Shi [32], COVID-19 pneumonia
screening is important for evaluating the status of the patient
and treatment. Therefore, along with the detection, COVID19 related infection localization is another crucial problem.
Hence, several studies [33]–[35] produced activation maps
that are generated from different Deep Learning (DL) models
trained for COVID-19 detection (classification) task to localize
COVID-19 infection in the lungs. Infection localization has
two vital objectives: an accurate assessment of the infection
location and the severity of the disease. However, the results
of previous studies show that the activation maps generated
inherently from the underlying DL network may fail to accomplish both objectives, that is, irrelevant locations with biased
severity grading appeared in many cases. To overcome these
problems, two studies [36], [37] proposed to perform lung
segmentation as the first step in their approaches. This way,
they have narrowed the region of interest down to the regions
of lungs to increase the reliability of their methods. Overall,
until this study, screening COVID-19 infection from such
activation maps produced by classification networks was the
only option for the localization due to the absence of groundtruth of the datasets available in the literature. Many studies
[32], [36], [38]–[40] have COVID-19 infection ground-truths
for CT images; however, ground-truth segmentation masks for
CXR images are non-existent.
In this study, in order to overcome the aforementioned
limitations and drawbacks, first, the benchmark dataset QaTaCOV19 proposed by the researchers of Qatar University and

Tampere University in [30] and [31] is extended to include
2951 COVID-19 samples. This new dataset is 3-20 times larger
than those used in earlier studies. The extended benchmark
dataset, QaTa-COV19 with around 120K CXR images, is not
only the largest ever composed dataset, but it is the first dataset
that has the ground-truth segmentation masks for COVID-19
infection regions, as some samples are shown in Fig. 1. A
crucial property of QaTa-COV19 dataset is that it contains
CXRs with other (non-COVID-19) infections and anomalies
such as pneumonia and pulmonary edema, both of which
exhibit high visual similarity to COVID-19 infection in the
lungs. Therefore, this is significantly more challenging task
than distinguishing COVID-19 from the normal (healthy) cases
as almost all studies in the literature did.
To obtain the ground-truth segmentation masks for the
COVID-19 infected regions, a human-machine collaborative
approach is introduced. The objective is to significantly reduce
the human labor and thus to speed up and also to improve the
segmentation masks because when they are drawn solely by
medical doctors (MDs), human error due to limited perception,
hand-crafting, and subjectivity will deteriorate the overall
quality. This is an iterative process, where MDs initiate the
segmentation by ”manually-drawn” segmentation masks for
a subset of CXR images. Then, the trained segmentation
networks over this subset generate their own ”competing”
masks and the MDs are asked to compare them pair-wise
(initial manual segmentation vs. machine-segmented masks)
for each patient. Such a verification improves the quality of
the generated masks as well as the (following) training runs.
Over the best masks selected by experts, the networks are
trained again this time over a larger set (or even perhaps
over the entire dataset), and among the masks generated by
the networks, the best masks are selected by the MDs. This
human-machine collaboration process continues until the MDs
are fully satisfied, i.e., a satisfactory mask can be found among
the masks generated by the networks for all CXR images in
the dataset. In this study, we show that even with two stages
(iterations), highly superior infection maps can be obtained
using which an elegant COVID-19 detection performance can
be achieved.
The rest of the paper is organized as follows. In Section
II-A, we introduce the benchmark QaTa-COV19 dataset. Our
novel human-machine collaborative approach for the groundtruth annotation is explained in Section II-B. Next, the details
of COVID-19 infected region segmentation, and the infection
map generation and COVID-19 detection are presented in
Sections II-C and II-D, respectively1 . The experimental setup
and results with the benchmark dataset are reported in Section
III-A and III-B, respectively. Finally, we conclude the paper
in Section IV.
II. M ATERIALS AND M ETHODOLOGY
The proposed approach in this study is composed of three
main phases: 1) training the state-of-the-art deep models for
1 The live demo of the proposed approach is implemented on
http://qatacov.live/

3

Fig. 2: The pipeline of the proposed approach has three stages: COVID-19 infected region segmentation, infection map
generation, and COVID-19 detection. The CXR image is the input to the trained E-D CNN and the network’s probabilistic
prediction is used to generate infection maps. The generated infection maps are used for COVID-19 detection.

COVID-19 infected region segmentation using the groundtruth segmentation masks, 2) infection map generation from
the trained segmentation networks, and 3) COVID-19 detection as it can be depicted in Fig. 2. In this section, we first
detail the creation of the benchmark QaTa-COV19 dataset.
Then, the proposed approach for collaborative human-machine
ground-truth generation is introduced.
A. The Benchmark QaTa-COV19 Dataset
The researchers of Qatar University and Tampere University have compiled the largest COVID-19 dataset up to
date with nearly 120K CXR images: QaTa-COV19 including
2951 COVID-19 CXRs. To create QaTa-COV19, we have
utilized several publicly available, scattered, and different format datasets and repositories. Therefore, the collected images
from the datasets had some duplicate, over-exposed and lowquality images that were identified and removed in the preprocessing stage. Consequently, the COVID-19 CXRs are from

Fig. 3: The COVID-19 CXR samples from the benchmark
QaTa-COV19 dataset.

different publicly available sources resulting in high intraclass dissimilarity as depicted in Fig. 3. The image sources of
COVID-19 and control group CXRs are detailed as follows:
COVID-19 CXRs: BIMCV-COVID19+ [41] is the largest
publicly available dataset with 2473 COVID-19 positive CXR
images. The CXR images of BIMCV-COVID19+ dataset were
recorded with computed radiography (CR) and digital X-ray
(DX) machines. Hannover Medical School and Institute for
Diagnostic and Interventional Radiology [42] released 183
CXR images of COVID-19 patients. A total of 959 CXR
images are from public repositories: Italian Society of Medical
and Interventional Radiology (SIRM), GitHub, and Kaggle
[37], [43]–[46]. As mentioned earlier, any duplication and lowquality images are removed since COVID-19 CXR images are
collected from different public datasets and repositories. In this
study, a total of 2951 COVID-19 CXRs are gathered from the
aforementioned datasets. Therefore, COVID-19 CXRs are of
different age, group, gender, and ethnicity.
Control Group CXRs: In this study, we have considered
two control groups in the experimental evaluation. GroupI consists of only normal (healthy) CXRs with a smaller
number of images compared to the second group. RSNA
pneumonia detection challenge dataset [47] is comprised of
about 29.7K CXR images, where 8851 images are normal. All
CXRs in the dataset are in DICOM format, a popularly used
format for medical imaging. Padchest dataset [48] consists of
160, 868 CXR images from 67, 625 patients, where 37, 871
images are from normal class. The images are evaluated
and reported by radiologists at Hospital Sun Juan in Spain
during 2009−2017. The dataset includes six different position
views of CXR and additional information regarding image
acquisition and patient demography. Paul Mooney [49] has
released an X-ray dataset of 5863 CXR images from a total
of 5856 patients, where 1583 images are from normal class.
The data is collected from pediatric patients aging one to
five years old at Guangzhou Women and Children’s Medical
Center, Guangzhou. The dataset in [50] consists of 7470 CXR
images and the corresponding radiologist reports from the
Indiana Network for Patient Care, where a total of 1343 frontal

4

Fig. 4: The two stages of the human-machine collaborative approach. Stage I: A subset of CXR images with manually drawn
segmentation masks are used to train three different deep networks in a 5-fold cross-validation scheme. The manually drawn
ground-truth (a), and the three predictions (b, c, d) are blindly shown to MDs, and they select the best ground-truth mask. Stage
II: Five deep networks are trained over the best segmentation masks selected. Then, they are used to produce the segmentation
masks for the rest of the CXR dataset (a, b, c, d, e), which are shown to MDs.

CXR samples are labeled as normal. In [51], there are 80
normal CXRs from the tuberculosis control program of the
Department of Health and Human Services of Montgomery
County and 326 normal CXRs from Shenzhen Hospital. In
this study, a total of 12, 544 normal CXRs are included in control Group-I from the aforementioned datasets. On the other
hand, Group-II consists of 116, 365 CXRs from 15 different
classes. ChestX-ray14 [52] consists of 112, 120 CXRs with
normal and 14 different thoracic disease images, which are
atelectasis, cardiomegaly, effusion, infiltration, mass, nodule,
pneumonia, pneumothorax, consolidation, edema, emphysema,
fibrosis, PT, hernia, and normal (no findings). Additionally,
from the pediatric patients [49], 2760 bacterial and 1485 viral
pneumonia CXRs are included in Group-II.
B. Collaborative Human-Machine Ground-Truth Annotation
Recent developments in the machine and deep learning
techniques led to state-of-the-art performance in many computer vision (CV) tasks, such as image classification, object
detection, and image segmentation. However, supervised DL
methods require a huge amount of annotated data. Otherwise,
the limited amount of data degrades the performance of the
deep network structures since their generalization capability
depends on the availability of large datasets. Nevertheless,
to produce ground-truth segmentation masks, pixel-accurate
image segmentation by human experts can be a cumbersome
and highly subjective task even for moderate size datasets.
In order to overcome this challenge, in this study, we
propose a novel collaborative human-machine approach to

accurately produce the ground-truth segmentation masks for
infected regions directly from the CXR images. The proposed
approach is performed in two main stages. First, a group of
expert MDs manually segment the infected regions of a subset
of (500 in our case) CXR images. Then, several segmentation
networks that are inspired by the U-Net [53] structure with
a 5-fold cross-validation scheme, are trained over the initial
ground-truth masks. For each fold, the segmentation masks of
the test samples are predicted by the networks. The network
predicted masks along with the initial (MD drawn) groundtruth masks, and original CXR image are assessed by the MDs,
and the best segmentation mask among them is selected. Steps
of Stage-I are illustrated in Fig. 4 (top). At the end of the
first stage, collaboratively annotated ground-truth masks for
the subset of CXR images are formed, and they are obviously
superior to the initial manually drawn masks since they are
selected by the MDs. An interesting observation in this stage
was that MDs preferred the machine-generated masks over the
manually drawn masks in the first stage in three out of five
cases.
In the second stage five deep networks, inspired by U-Net
[53], UNet++ [54], and DLA [55] architectures are trained
over the collaborative masks, which were formed in Stage-I.
The trained segmentation networks are used to predict the segmentation masks of the rest of the data, which is around 2400
unannotated COVID-19 images. Among the five predictions,
the expert MDs select the best one as the ground-truth or deny
all if none was found successful. For the latter case, MDs were
asked to draw the ground-truth masks manually. However,

5

we notice that this was indeed a minority case that included
less than 5% of unannotated data. The steps of Stage-II are
shown in Fig. 4 (bottom). As a result, the ground-truth masks
for 2951 COVID-19 CXR images are gathered to construct
the benchmark QaTa-COV19 dataset. The proposed approach
does not only save valuable human labor time, but it also
improves the quality and reliability of the masks by reducing
the subjectivity with Stage-II verification step.
C. COVID-19 Infected Region Segmentation
Segmentation of COVID-19 infection is the first step of our
proposed approach as depicted in Fig. 2. Once the groundtruth annotation for QaTa-COV19 benchmark dataset is formed
as explained in the previous section, we perform infected
region segmentation extensively with 24 different network
configurations. We have used three different segmentation
models: U-Net, UNet++ and DLA, with four different encoder
structures: CheXNet, DenseNet-121, Inception-v3 and ResNet50, and frozen & not frozen encoder weight configurations.
1) Segmentation Models: We have tried distinct segmentation model structures starting from shallow to deep structures
with varied configurations as follows:
• U-Net [53] is an outperforming network for medical
image segmentation applications with a u-shaped architecture as the encoder part is symmetric with respect to
its decoder part. Therefore, this unique decoder structure
with many feature channels allows the network to carry
the information through its latest layers.
• UNet++ [54] has further developed the decoder structure
of U-Net by connecting the encoder to the decoder with
the nested dense convolutional blocks. This way, the
bridge between the encoder and decoder parts are more
firmly knit; thus, the information can be transferred to
its final layers more intensively compared to the classic
U-Net.
• DLA [55] investigates the connecting bridges between
the encoder and decoder, and proposes a way to fuse
the semantic and spatial information with dense layers,
which are progressively aggregated by iterative merging
to deeper and larger scales.
2) Encoder Selections for Segmentation Models: In this
study, we use several deep CNNs to form the encoder part
of the above-mentioned segmentation models as follows:
• DenseNet-121 [56] is a deep network with 121 layers,
each with additional input nodes connecting all the layers
directly with each other. Therefore, the maximum information flow through the network is satisfied.
• CheXNet [57] is based on the architecture of DenseNet121, which is trained over the ChestX-ray14 dataset [52]
to detect pneumonia cases from CXR images. In [57],
DenseNet-121 is initialized with the ImageNet weights
and fine-tuned over 100K CXR images resulting from
the state-of-the-art results on the ChestX-ray14 dataset
with a better performance compared to the conclusions
of radiologists.
• Inception-v3 [58] achieves state-of-the-art results with
much less computational complexity compared to its

deep competitors by factorizing the convolutions and
pruning the dimensions inside the network. Despite the
less complexity, it preserves a higher performance.
• ResNet-50 [59] introduces a deep residual learning
framework that forces the desired mapping of the input
to a residual mapping. It is possible to achieve this goal
by the shortcut connections on the stacked layers. These
connections enable to merge the input and output of
the stacked layers by addition operations; therefore, the
problem of gradient vanishing is prevented.
We perform transfer learning on the encoder side of the segmentation models by initializing the layers with the ImageNet
weights, except for CheXNet which is pre-trained on the
ChestX-ray14 dataset. We tried two configurations, in the first
we freeze the encoder layers while in the second, they are
allowed to vary.
3) Hybrid Loss Function: In this study, we have performed
training the segmentation networks with a hybrid loss function
by combining focal loss [60] with dice loss [61] to achieve
a better segmentation performance. We use focal loss since
COVID-19 infected region segmentation is an imbalanced
problem: the number of background pixels is superior to the
foreground’s. Let the ground-truth segmentation mask be Y,
where each pixel class label is defined as y, and the network
prediction as ŷ. We define the pixel class probabilities as for
the positive class P (y = 1) = p, and for the negative class
P (y = 0) = 1 − p. On the other hand, the network prediction
probabilities are modeled by the logistic function using the
sigmoid curve as,
P (ŷ = 1) =
P (ŷ = 0) = 1 −

1
=q
1 + e−z

(1)

1
=1−q
1 + e−z

(2)

where z is some function of the input CXR image X. Then,
we define the cross-entropy (CE) loss as follows:
CE(p, q) = −p log q − (1 − p) log(1 − q).

(3)

A common solution to address the class imbalance problem
is to add a weighting factor α ∈ [0, 1] for the positive class,
and 1 − α for the negative class, which defines the balanced
cross-entropy (BCE) loss as,
BCE(p, q) = −αp log q − (1 − α)(1 − p) log(1 − q). (4)
In this way, the importance of positive and negative samples
are balanced. However, adding the α factor does not solve the
issue for the large class imbalance scenario. This is because the
network cannot distinguish outliers (hard samples) and inliers
(easy samples) with the BCE loss. To overcome this drawback,
focal loss [60] proposes to set focusing parameter γ ≥ 0 in
order to down-weight the loss of easy samples that occur with
small errors; so that the model can be forced to learn hard
negative samples. The focal (F) loss is defined as,
F (p, q) = −α(1−q)γ p log q−(1−α)q γ (1−p) log(1−q). (5)
where F loss is equivalent to BCE loss when γ = 0. In our
experimental setup, we use the default setting as α = 0.25, and

6

γ = 2 for all the networks. To achieve a good segmentation
performance, we combined focal loss with dice loss, which is
based on the dice coefficient (DC) defined as follows:
DC =

2|Y ∩ Ŷ|
|Y| ∪ |Ŷ|

(6)

where Ŷ is the predicted segmentation mask of the network.
Hence, the DC can be interpreted as a dice (D) loss as follows:
P
2 ph,w qh,w
P
D(p, q) = 1 − P
(7)
ph,w + qh,w
where h and w are the height and width of the ground-truth and
prediction masks Y and Ŷ, respectively. Finally, we combined
D and F losses by summation to achieve the so-called hybrid
loss function for the segmentation networks.

D. Infection Map Generation and COVID-19 Detection
Having the training set of COVID-19 CXR images via the
collaborative human-machine approach explained in Section
II-A, we train the aforementioned segmentation networks
to produce infection maps. After training the segmentation
networks, we feed each test CXR sample X into the trained
network. Then, we obtain the network prediction mask Ŷ,
which is used to generate an infection map that is a measure
of infected region probabilities on the input X. Each pixel
in Ŷ is defined as Ŷh,w ∈ [0, 1], where h and w represent
the size of the image. We then apply an RGB-based color
transform, i.e., the jet color scale to obtain the RGB version
of the prediction mask, ŶR,G,B as shown in Fig. 5 for a pseudocolored probability measure visualization. The infection map
is generated as a reflection of the network prediction ŶR,G,B
onto the CXR image X. Hence, for visualization, we form
the imposed image by concatenating the hue and saturation
components of ŶH,S,V , and value component of XH,S,V . Finally, the imposed image is converted back to RGB domain.
In the infection map, we do not show the pixels/regions with
zero probabilities for a better visualization effect. This way,
the infected regions, where Ŷ > 0 are shown translucent as
in Fig 5.
Along with the infection map generation, which already
provides localization and segmentation of COVID-19 infection, COVID-19 detection can easily be performed using the
proposed approach. The detection of COVID-19 is performed
based on the predictions of the trained segmentation networks.
Accordingly, a test sample is classified as COVID-19 class if
Ŷ ≥ 0.5 at any pixel location.

Fig. 5: The three COVID-19 CXR test samples, X with
the corresponding ground-truth masks, Y. The color-coded
network predictions, ŶR,G,B are reflected translucent onto the
X to generate an infection map on the lungs, where Ŷ > 0.

A. Experimental Setup
Quantitative evaluations for the proposed approach are performed for both COVID-19 infected region segmentation and
COVID-19 detection. COVID-19 infected region segmentation
is evaluated on a pixel-level, where we consider the foreground
(infected region) as the positive class, and background as
the negative class. For COVID-19 detection, the performance
is computed per CXR sample, and we consider COVID-19
as the positive class and the control group as the negative
class. Overall, elements of the confusion matrix are formed as
follows: true positive (TP): the number of correctly detected
positive class members, true negative (TN): the number of
correctly detected negative class samples, false positive (FP):
the number of misclassified negative class members, and false
negative (FN): the number of misclassified positive class
samples. The standard performance evaluation metrics are
defined as follows:
Sensitivity =

(8)

where sensitivity (or Recall) is the rate of correctly detected
positive samples in the positive class samples,
Specif icity =

TN
TN + FP

(9)

where specificity is the ratio of accurately detected negative
class samples to all negative class samples,

III. E XPERIMENTAL R ESULTS
In this section, first, the experimental setup is presented.
Then, both numerical and visual results are reported with an
extensive set of comparative evaluations over the benchmark
QaTa-COV19 dataset. Finally, visual comparative evaluations
are presented between the infection maps and the activation
maps extracted from state-of-the-art deep models.

TP
TP + FN

P recision =

TP
TP + FP

(10)

where precision is the rate of correctly classified positive class
samples among all the members classified as positive samples,
Accuracy =

TP + TN
TP + TN + FP + FN

(11)

7

TABLE I: Average performance metrics (%) for COVID-19 infected region segmentation computed on the Group-I test (unseen)
set from 5-folds with three state-of-the-art segmentation models, four encoder architectures, and weight initializations. The
initialized encoder layers are set to frozen (3) and not frozen (7) states during the investigation.
Model

Encoder

Encoder
Layers

Sensitivity

Specificity

Precision

F1-Score

F2-Score

Accuracy

AUC

U-Net

CheXNet
CheXNet
DenseNet-121
DenseNet-121
Inception-v3
Inception-v3
ResNet-50
ResNet-50

3
7
3
7
3
7
3
7

81.20 ± 1.6 × 10−4
82.23 ± 1.6 × 10−4
82.29 ± 1.6 × 10−4
84.00 ± 1.5 × 10−4
80.42 ± 1.7 × 10−4
82.34 ± 1.6 × 10−4
81.43 ± 1.6 × 10−4
79.90 ± 1.7 × 10−4

99.55 ± 5 × 10−6
99.56 ± 5 × 10−6
99.61 ± 4 × 10−6
99.66 ± 4 × 10−6
99.59 ± 5 × 10−6
99.70 ± 4 × 10−6
99.62 ± 4 × 10−6
99.70 ± 4 × 10−6

83.78 ± 2.6 × 10−5
84.54 ± 2.5 × 10−5
86.02 ± 2.4 × 10−5
87.77 ± 2.3 × 10−5
84.94 ± 2.5 × 10−5
88.87 ± 2.2 × 10−5
86.07 ± 2.4 × 10−5
88.64 ± 2.2 × 10−5

82.47 ± 2.7 × 10−5
83.34 ± 2.6 × 10−5
84.11 ± 2.6 × 10−5
85.81 ± 2.5 × 10−5
82.62 ± 2.7 × 10−5
85.43 ± 2.5 × 10−5
83.67 ± 2.6 × 10−5
83.89 ± 2.6 × 10−5

81.70 ± 2.7 × 10−5
82.66 ± 2.7 × 10−5
83.01 ± 2.6 × 10−5
84.71 ± 2.5 × 10−5
81.28 ± 2.7 × 10−5
83.54 ± 2.6 × 10−5
82.31 ± 2.6 × 10−5
81.43 ± 2.7 × 10−5

99.03 ± 6.9 × 10−6
99.08 ± 6.7 × 10−6
99.13 ± 6.5 × 10−6
99.22 ± 6.2 × 10−6
99.05 ± 6.8 × 10−6
99.21 ± 6.2 × 10−6
99.11 ± 6.6 × 10−6
99.15 ± 6.5 × 10−6

99.19 ± 6.3 × 10−6
99.18 ± 6.3 × 10−6
99.35 ± 5.6 × 10−6
99.19 ± 6.3 × 10−6
99.20 ± 6.3 × 10−6
98.82 ± 7.6 × 10−6
99.30 ± 5.9 × 10−6
98.98 ± 7.1 × 10−6

UNet++

CheXNet
CheXNet
DenseNet-121
DenseNet-121
Inception-v3
Inception-v3
ResNet-50
ResNet-50

3
7
3
7
3
7
3
7

80.29 ± 1.7 × 10−4
81.45 ± 1.6 × 10−4
82.38 ± 1.6 × 10−4
82.36 ± 1.6 × 10−4
82.87 ± 1.6 × 10−4
83.49 ± 1.6 × 10−4
82.07 ± 1.6 × 10−4
82.64 ± 1.6 × 10−4

99.59 ± 5 × 10−6
99.60 ± 5 × 10−6
99.61 ± 4 × 10−6
99.68 ± 4 × 10−6
99.57 ± 5 × 10−6
99.66 ± 4 × 10−6
99.59 ± 5 × 10−6
99.62 ± 4 × 10−6

85.19 ± 2.5 × 10−5
85.60 ± 2.5 × 10−5
85.99 ± 2.4 × 10−5
88.07 ± 2.3 × 10−5
84.83 ± 2.5 × 10−5
87.60 ± 2.3 × 10−5
85.41 ± 2.5 × 10−5
86.52 ± 2.4 × 10−5

82.64 ± 2.7 × 10−5
83.47 ± 2.6 × 10−5
84.14 ± 2.6 × 10−5
85.08 ± 2.5 × 10−5
83.81 ± 2.6 × 10−5
85.45 ± 2.5 × 10−5
83.71 ± 2.6 × 10−5
84.45 ± 2.5 × 10−5

81.21 ± 2.7 × 10−5
82.24 ± 2.7 × 10−5
83.08 ± 2.6 × 10−5
83.42 ± 2.6 × 10−5
83.24 ± 2.6 × 10−5
84.22 ± 2.6 × 10−5
82.72 ± 2.7 × 10−5
83.33 ± 2.6 × 10−5

99.05 ± 6.8 × 10−6
99.09 ± 6.7 × 10−6
99.13 ± 6.5 × 10−6
99.19 ± 6.3 × 10−6
99.10 ± 6.6 × 10−6
99.20 ± 6.3 × 10−6
99.10 ± 6.6 × 10−6
99.14 ± 6.5 × 10−6

99.01 ± 7 × 10−6
99.01 ± 7 × 10−6
99.19 ± 6.3 × 10−6
99.30 ± 5.9 × 10−6
99.21 ± 6.2 × 10−6
99.18 ± 6.3 × 10−6
99.15 ± 6.5 × 10−6
99.27 ± 6 × 10−6

DLA

CheXNet
CheXNet
DenseNet-121
DenseNet-121
Inception-v3
Inception-v3
ResNet-50
ResNet-50

3
7
3
7
3
7
3
7

79.99 ± 1.7 × 10−4
82.84 ± 1.6 × 10−4
82.48 ± 1.6 × 10−4
82.84 ± 1.6 × 10−4
80.28 ± 1.7 × 10−4
83.44 ± 1.6 × 10−4
81.26 ± 1.6 × 10−4
82.07 ± 1.6 × 10−4

99.61 ± 4 × 10−6
99.56 ± 5 × 10−6
99.62 ± 4 × 10−6
99.56 ± 5 × 10−6
99.63 ± 4 × 10−6
99.68 ± 4 × 10−6
99.63 ± 4 × 10−6
99.65 ± 4 × 10−6

85.57 ± 2.5 × 10−5
84.63 ± 2.5 × 10−5
86.40 ± 2.4 × 10−5
84.63 ± 2.5 × 10−5
86.43 ± 2.4 × 10−5
88.18 ± 2.3 × 10−5
86.48 ± 2.4 × 10−5
86.99 ± 2.4 × 10−5

82.66 ± 2.7 × 10−5
83.71 ± 2.6 × 10−5
84.36 ± 2.6 × 10−5
83.71 ± 2.6 × 10−5
83.19 ± 2.6 × 10−5
85.73 ± 2.5 × 10−5
83.78 ± 2.6 × 10−5
84.45 ± 2.5 × 10−5

81.04 ± 2.8 × 10−5
83.19 ± 2.6 × 10−5
83.21 ± 2.6 × 10−5
83.19 ± 2.6 × 10−5
81.41 ± 2.7 × 10−5
84.34 ± 2.6 × 10−5
82.25 ± 2.7 × 10−5
83.00 ± 2.6 × 10−5

99.06 ± 6.8 × 10−6
99.09 ± 6.7 × 10−6
99.14 ± 6.5 × 10−6
99.09 ± 6.7 × 10−6
99.09 ± 6.7 × 10−6
99.22 ± 6.2 × 10−6
99.12 ± 6.6 × 10−6
99.15 ± 6.5 × 10−6

99.12 ± 6.6 × 10−6
99.17 ± 6.4 × 10−6
99.16 ± 6.4 × 10−6
99.17 ± 6.4 × 10−6
99.02 ± 6.9 × 10−6
99.29 ± 5.9 × 10−6
99.08 ± 6.7 × 10−6
99.31 ± 5.8 × 10−6

where accuracy is the ratio of correctly classified elements
among all the data,
F (β) = (1 + β 2 )

(P recision × Sensitivity)
β 2 × P recision + Sensitivity

(12)

where F -score is defined by the weighting parameter β. The
F1-Score is calculated with β = 1, which is the harmonic
average of precision and sensitivity. The F2-score is calculated
with β = 2, which emphasizes FN minimization over FPs. The
main objective of both COVID-19 segmentation and detection
is to maximize sensitivity with a reasonable specificity in
order to minimize FP COVID-19 cases or pixels. Equivalently,
maximized F2-score is targeted with an acceptable F1-Score
value. The performances with their 95% confidence interval
(CI) for both COVID-19 infected region segmentation and
detection are given in Tables I and III, respectively. The range
of values can be calculated for each performance as follows:
p
r = ±z metric(1 − metric)/N ,
(13)
where z is the level of significance, metric is any performance evaluation metric, and N is the number of samples.
Accordingly, z is set to 1.96 for 95% CI.
We have implemented the deep networks with Tensorflow
library [62] using Python on NVidia ® GeForce RTX 2080
Ti GPU card. For training, Adam optimizer [63] is used with
the default momentum parameters, β1 = 0.9 and β2 = 0.999
using the aforementioned hybrid loss function. The segmentation networks are trained with 50-epochs with a learning rate
of α = 10−4 and a batch size of 32.
For comparing the computed infection maps, the activation
maps are computed as follows: the encoder structures of the
segmentation networks are trained for the classification task
with a modification at the output layer by adding 2-neurons for
the number of total classes. The activation maps extracted from
the classification models are then compared with the infection
maps of the segmentation models. The classification networks,
CheXNet, DenseNet-121, Inception-v3 and ResNet-50 are
fine-tuned using categorical cross-entropy as loss function with

10 epochs and a learning rate of α = 10−5 , which is a
sufficient setting to prevent over-fitting, based on our previous
study [31]. Other settings of the classifiers are kept the same
with the segmentation models.
B. Experimental Results
The experiments are carried out for both COVID-19 infected
region segmentation and COVID-19 detection. We extensively
tested the benchmark QaTa-COV19 dataset using three different state-of-the-art segmentation networks with four different
encoder options for the initial dataset consisting of control
Group-I. We also investigated the effect of frozen encoder
weights on the performance. On the other hand, the leading
model is selected and evaluated on the extended dataset, which
includes more negative samples with the control Group-II.
1) Group-I Experiments: We have evaluated the networks
in a stratified 5-fold cross-validation scheme with a ratio of
80% training to 20% test (unseen folds) over the benchmark
QaTa-COV19 dataset. The input CXR images are resized to
224 × 224 pixels. Table II shows the number of CXRs per
fold in the dataset. Since the two classes are imbalanced, we
have applied data augmentation in order to balance the classes.
Therefore, COVID-19 samples are augmented up to the same
number of samples as the control Group-I in the training set for
each fold. The data augmentation is performed using Image
Data Generator in Keras: the CXR samples are augmented
by randomly shifting them both vertically and horizontally by
10% and randomly rotating them in a range of 10 degrees.
TABLE II: Number of CXR samples in control Group-I per
fold before and after data augmentation.
Data
COVID-19
Group-I
Total

Number of
Samples
2951
12544
15495

Training
Samples
2361
10035
12396

Augmented
Training Samples
10035
10035
20070

Test
Samples
590
2509
3099

8

TABLE III: Average COVID-19 detection performance results (%) computed from 5-folds over the Group-I test (unseen) set
with three network models, four encoder architectures, and weight initializations. The initialized encoder layers are set to frozen
(3) and not frozen (7) states during the investigation.

DLA

UNet++

U-Net

Encoder
CheXNet
CheXNet
DenseNet-121
DenseNet-121
Inception-v3
Inception-v3
ResNet-50
ResNet-50
CheXNet
CheXNet
DenseNet-121
DenseNet-121
Inception-v3
Inception-v3
ResNet-50
ResNet-50
CheXNet
CheXNet
DenseNet-121
DenseNet-121
Inception-v3
Inception-v3
ResNet-50
ResNet-50

Encoder
Layers
3
7
3
7
3
7
3
7
3
7
3
7
3
7
3
7
3
7
3
7
3
7
3
7

Sensitivity

Specificity

Precision

F1-Score

F2-Score

Accuracy

97.56 ± 0.0056
97.97 ± 0.0051
98.07 ± 0.0050
98.37 ± 0.0046
97.93 ± 0.0051
97.22 ± 0.0059
98.24 ± 0.0047
96.37 ± 0.0067
97.80 ± 0.0053
97.49 ± 0.0056
97.70 ± 0.0054
96.51 ± 0.0066
98.31 ± 0.0047
96.92 ± 0.0061
97.80 ± 0.0053
96.78 ± 0.0064
97.46 ± 0.0057
97.32 ± 0.0058
97.36 ± 0.0058
97.09 ± 0.0061
96.92 ± 0.0062
96.71 ± 0.0064
97.49 ± 0.0056
96.17 ± 0.0069

91.10 ± 0.0050
92.74 ± 0.0045
94.66 ± 0.0039
98.05 ± 0.0024
90.00 ± 0.0052
98.37 ± 0.0022
93.88 ± 0.0042
97.82 ± 0.0026
91.70 ± 0.0048
93.65 ± 0.0043
94.81 ± 0.0039
99.16 ± 0.0016
90.54 ± 0.0051
98.37 ± 0.0022
93.39 ± 0.0043
97.43 ± 0.0028
92.47 ± 0.0046
94.93 ± 0.0038
95.66 ± 0.0036
99.07 ± 0.0017
93.24 ± 0.0044
99.13 ± 0.0016
95.30 ± 0.0037
98.15 ± 0.0024

72.07 ± 0.0071
76.04 ± 0.0067
81.20 ± 0.0062
92.25 ± 0.0042
69.74 ± 0.0072
93.33 ± 0.0039
79.06 ± 0.0064
91.21 ± 0.0045
73.49 ± 0.0069
78.33 ± 0.0065
81.58 ± 0.0061
96.44 ± 0.0029
70.96 ± 0.0071
93.34 ± 0.0039
77.69 ± 0.0066
89.87 ± 0.0048
75.27 ± 0.0068
81.87 ± 0.0061
84.08 ± 0.0058
96.08 ± 0.0031
77.13 ± 0.0066
96.32 ± 0.0030
82.98 ± 0.0059
92.44 ± 0.0042

82.90 ± 0.0059
85.62 ± 0.0055
88.84 ± 0.0050
95.21 ± 0.0034
81.47 ± 0.0061
95.24 ± 0.0034
87.61 ± 0.0052
93.72 ± 0.0038
83.92 ± 0.0058
86.87 ± 0.0053
88.91 ± 0.0049
96.48 ± 0.0029
82.43 ± 0.0060
95.10 ± 0.0034
86.59 ± 0.0054
93.20 ± 0.0040
84.94 ± 0.0056
88.93 ± 0.0049
90.23 ± 0.0047
96.58 ± 0.0029
85.90 ± 0.0055
96.52 ± 0.0029
89.65 ± 0.0048
94.27 ± 0.0037

91.11 ± 0.0045
92.62 ± 0.0041
94.16 ± 0.0037
97.08 ± 0.0027
90.61 ± 0.0046
96.42 ± 0.0029
93.69 ± 0.0038
95.30 ± 0.0033
91.73 ± 0.0043
92.94 ± 0.0040
93.98 ± 0.0037
96.50 ± 0.0029
91.27 ± 0.0044
96.18 ± 0.0030
92.98 ± 0.0040
95.31 ± 0.0033
92.03 ± 0.0043
93.78 ± 0.0038
94.38 ± 0.0036
96.88 ± 0.0027
92.19 ± 0.0042
96.63 ± 0.0028
94.20 ± 0.0037
95.40 ± 0.0033

92.33 ± 0.0042
93.73 ± 0.0038
95.31 ± 0.0033
98.12 ± 0.0021
91.51 ± 0.0044
98.15 ± 0.0021
94.71 ± 0.0035
97.54 ± 0.0024
92.86 ± 0.0041
94.39 ± 0.0036
95.36 ± 0.0033
98.66 ± 0.0018
92.02 ± 0.0043
98.10 ± 0.0021
94.23 ± 0.0037
97.31 ± 0.0025
93.42 ± 0.0039
95.39 ± 0.0033
95.99 ± 0.0031
98.69 ± 0.0018
93.94 ± 0.0040
98.67 ± 0.0018
95.71 ± 0.0032
97.77 ± 0.0023

After shifting and rotating the images, blank sections are filled
using the nearest mode.
The performance of the segmentation models for COVID19 infected region segmentation are presented in Table I. Each
model structure is evaluated with two configurations: frozen
and not frozen encoder layers. We have used transfer learning
on the encoder layers with ImageNet weights, except for the
CheXNet model, which is pre-trained on the ChestX-ray14
dataset. The evaluation of the models with frozen encoder
layers is also important since this process can lead to a better
convergence and improved performance. However, as the

Fig. 6: The Precision-Recall curves of the three leading models
all with the not frozen encoder layers setting.

results show, better performance is obtained when the network
continues to learn on the encoder layers as well. For each
model, we have observed that two encoders: DenseNet-121
and Inception-v3 are the top-performing ones for the infected
region segmentation task. The U-Net model with DenseNet121 encoder holds the leading performance by 84% sensitivity, 85.81% F1-Score, and 84.71% F2-Score. DenseNet121 produces better results compared to other encoder types
since it can preserve the information coming from earlier
layers through the output by concatenating the feature maps
from each dense layer. However, in the other segmentation
models, Inception-v3 outperforms the other encoder types. The
presented segmentation performances are obtained by setting
the threshold value to 0.5 to compute the segmentation mask
from the network probabilities. The Precision-Recall curves
are plotted in Fig. 6 by varying this threshold value.
The performances of the segmentation models for COVID19 detection are presented in Table III. All the models are
evaluated by stratified a 5-fold cross-validation scheme, and
the table shows the averaged results of these folds. The
most crucial metric here is the sensitivity since missing any
patient with COVID-19 is critical. In fact, the results indicate
the robustness of the model as the proposed approach can
achieve high sensitivity levels of 98.37% with a 97.08% F2Score. Additionally, the proposed approach achieves an elegant
specificity of 99.16%, indicating a significantly low false
alarm rate. It can be observed from Table III that DenseNet121 encoder with the not frozen encoder layer setting gives
the most promising results among the others. The confusion
matrices, accumulated on each fold’s test set, are presented
in Table IV. The highest sensitivity in COVID-19 detection
is achieved by the U-Net DenseNet-121 model (Table IVa).
Accordingly, the U-Net DenseNet-121 model only misses 48

9

TABLE IV: Cumulative confusioFn matrices of COVID-19
detection by the best performing U-Net and UNet++ models
with DenseNet-121 encoder.

TABLE VI: COVID-19 infected region segmentation and detection results (%) computed on the Group-II test set from the
U-Net model with DenseNet-121 encoder.

U-Net
Ground
Truth

Group-I
COVID-19

U-Net

Predicted
Group-I COVID-19
12300
244
48
2903

(b) UNet++ DenseNet-121
Predicted
Group-I COVID-19
12439
105
103
2848

UNet++
Ground
Truth

Group-I
COVID-19

TABLE V: Number of CXR samples in control Group-II
before and after data augmentation.

COVID-19
Bacterial
Pneumonia
ChestX-ray14
Viral
Pneumonia
Total

Training
Samples
2078

3

Augmented
Training Samples
10, 000

Test
Samples
873

3

5000

630

86, 524

7

86, 524

25, 596

1146

3

5000

339

106,524

27,438

2130

91,878

Augmented

Performance
Metrics
Sensitivity
Specificity
Precision
F1-Score
F2-Score
Accuracy

Infected Region
Segmentation
81.72
99.93
84.74
83.20
82.31
99.85

Detection
94.96
99.88
96.40
95.67
95.24
99.73

TABLE VII: Cumulative confusion matrices of COVID-19
detection by the best performing U-Net model with DenseNet121 encoder.

COVID-19 patients out of 2951. On the other hand, the highest
specificity is achieved by UNet++ DenseNet-121 model (Table
IVb). The UNet++ model only misses a minor part of the
control class with 105 samples out of 12544.
2) Group-II Experiments: We have selected the leading
model from the Group-I experiments as U-Net with not frozen
DenseNet-121 encoder setting. In Group-II experiments, we
have gathered around 120K CXRs. The CXRs from the
ChestX-ray14 dataset [52] are already divided into train and
test sets. Accordingly, we have randomly separated the train
and test sets of COVID-19, viral pneumonia, and bacterial
pneumonia CXRs by keeping the same train/test ratio as in
ChestX-ray14 [52]. Table V shows the number of training
and test samples of the Group-II experiments. Additonally, we
have applied augmentation to data except for ChestX-ray14
samples with the same set-up as in the Group-I experiments.
In these experiments, we do not perform any cross-validation
since ChestX-ray14 has predefined training and test sets.

Data

DenseNet-121

(a) U-Net DenseNet-121

The performance of the U-Net model for COVID-19 infected region segmentation and detection is presented in Table VI. The model achieved a segmentation performance by
81.72% sensitivity and 83.20% F1-Score. In comparison to
initial experiments with the control Group-I data, the model
can still achieve an elegant segmentation performance even
with numerous samples in the test set. On the other hand, the
COVID-19 detection performance with 27, 438 CXR images
is very successful by 94.96% sensitivity, 99.88% specificity,
and 96.40% precision. This indicates a very low false alarm
rate of only 0.12%. Table VII shows the confusion matrix on
the test set. Accordingly, the model only misses 44 COVID-19
samples. In the control Group-II, only 31 CXR samples are

U-Net
Ground
Truth

Group-II
COVID-19

Predicted
Group-II COVID-19
26, 534
31
44
829

missed, which is a minor section in 26, 565 negative samples.
The results show that the leading model is still robust on the
extended data, where it consists of 15 different classes with
14 thoracic diseases and normal samples.

C. Infection vs Activation Maps
Several studies [33]–[35] propose to localize COVID-19
from CXRs by extracting activation maps from the deep
classification models trained for COVID-19 detection. Despite
the simplicity of the idea, there are many limitations of this approach. First of all, without any infected region segmentation
ground-truth masks, the network can only produce a rough
localization, and the extracted activation maps may entirely
fail to localize COVID-19 infection.
In this study, we check the reliability of our proposed
COVID-19 detection approach by comparing it with DL models trained for the classification task. In order to achieve this
objective, we compare the infection map and activation map
of CXR images, which are generated from the segmentation
and classification networks, respectively. Therefore, we have
trained the encoder structures of the segmentation networks,
which are CheXNet, DenseNet-121, Inception-v3, and ResNet50 to perform COVID-19 classification task. We have extracted
activation maps from these trained models by the Gradientweighted Class Activation Mapping (Grad-CAM) approach
proposed in [64]. The localization Grad-CAM LcGrad-CAM ∈
Rh×w of height h and width w for class c is calculated by
the gradient of mc before the softmax with respect to the
c
convolutional layer’s feature maps Ak as ∂m
. The gradients
∂Ak
are passed through from the global average pooling during
back-propagation;
αkc =

1 X X ∂mc
,
Z i j ∂Ak

(14)

where α is the weight that shows the important feature map
k from A for a target class c. Then, the linear combination is

10

Fig. 7: Several CXR images with their corresponding ground-truth masks. The activation maps extracted from the classification
models are presented in the middle block. The last block is the generated infection maps from the segmentation models. It is
evident that the infection maps yield a superior localization of COVID-19 infection compared to activation maps.

Despite their elegant performance, activation maps extracted
from deep classification networks are not suitable for localizing COVID-19 infection as depicted in Fig 7. In fact, infections
found by the activation maps are highly irrelevant indicating
false locations outside of the lung areas. On the other hand,
infection maps can generate a highly accurate location with
an elegant severity grading of COVID-19 infection. The proposed infection maps can conveniently be used by medical
experts for an enhanced assessment of the disease. Real-time
implementation of the infection maps will obviously speed
up the detection process, can also monitor the progression of
COVID-19 infection in the lungs.

Encoder

U-Net

k

TABLE VIII: The number of trainable and non-trainable
parameters of the models with their inference time (ms) per
sample. The initialized encoder layers are set to frozen (3) or
not frozen (7).

UNet++

performed following by ReLU to obtain the Grad-CAM;
X
LcGrad-CAM = ReLU (
αkc Ak ).
(15)

In this section, we present the computational times of
the networks and their number of trainable & non-trainable
parameters. Table VIII shows the elapsed time in milliseconds
(ms) during the inference step for each network used in the
experiments. The results in the table represent the running
time per sample. It can be observed from the table that the UNet model is the fastest among the others due to its shallow
structure. The fastest network is U-Net Inception-v3 with

DLA

D. Computational Complexity Analysis

CheXNet
DenseNet-121
Inception-v3
ResNet-50
CheXNet
DenseNet-121
Inception-v3
ResNet-50
CheXNet
DenseNet-121
Inception-v3
ResNet-50
CheXNet
DenseNet-121
Inception-v3
ResNet-50
CheXNet
DenseNet-121
Inception-v3
ResNet-50
CheXNet
DenseNet-121
Inception-v3
ResNet-50

Encoder
Layers
3
3
3
3
3
7
7
7
3
3
3
3
7
7
7
7
3
3
3
3
7
7
7
7

Trainable

Non-Trainable

5.19M
5.19M
8.15M
9.06M
12.06M
12.06M
29.9M
32.51M
7.53M
7.53M
8.68M
10.88M
14.40M
14.40M
30.43M
34.34M
6.27M
6.27M
7.20M
8.74M
13.15M
13.15M
28.96M
32.2M

6.96M
6.96M
21.79M
23.50M
85.63K
85.63K
36.42K
47.56K
6.96M
6.96M
21.79M
23.51M
88.45K
88.45K
39.23K
50.37K
6.96M
6.96M
21.79M
23.51M
88.45K
88.45K
39.23K
50.37K

Time
(ms)
2.56
2.58
2.53
2.54
2.62
2.58
2.61
2.64
5.17
5.10
5.32
5.58
5.24
5.25
5.32
5.46
4.65
4.63
4.70
4.90
4.63
4.65
4.72
4.90

11

frozen encoder layers taking up 2.53 ms. On the other hand,
the slowest model is UNet++ structure since it has the largest
number of trainable parameters. The most computationally
demanding model is UNet++ ResNet-50 with frozen encoder
layers, which takes 5.58 ms. We, therefore, conclude that all
models can be used as real-time clinical applications.

IV. C ONCLUSIONS
The immediate and accurate detection of highly infectious
COVID-19 plays a vital role in preventing the spread of the
virus. In this study, we used CXR images since X-ray imaging
is cheaper, easily accessible, and faster than the conventional
methods commonly used such as RT-PCR and CT. As a
major contribution, the largest CXR dataset, QaTa-COV19,
which consists of 2951 COVID-19, and 116, 365 control group
images, has been compiled and shared publicly as a benchmark
dataset. Moreover, for the first time in the literature, we
release the ground-truth segmentation masks of the infected
regions along with the introduced benchmark QaTa-COV19.
Furthermore, we proposed a human-machine collaborative
approach, which can be used when a fast and accurate groundtruth annotation is desired but manual segmentation is slow,
costly, and subjective. Finally, this study reveals the first
approach ever proposed for infection map generation in CXR
images. Our extensive experiments on QaTa-COV19 show that
a reliable COVID-19 diagnosis can be achieved by generating
infection maps, which can locate the infection on the lungs
by 81.72% sensitivity, and 83.20% F1-Score. Moreover, the
proposed joint approach can achieve an elegant COVID-19
detection performance with 94.96% sensitivity and 99.88%
specificity.
Many COVID-19 detectors proposed in the literature reported similar or even better detection performances. However,
not only they are evaluated over small-size datasets, but also
they can only discriminate between COVID-19 and normal
(healthy) data, which is a straightforward task. The proposed
joint approach is the only COVID-19 detector that can distinguish it from other thoracic diseases as being evaluated over
the largest CXR dataset ever composed. Accordingly, the most
important aspect of this study is that the generated infection
maps can assist MDs for a better and objective COVID-19
assessment. For instance, it can show the time progress of
the disease if the time series CXR data are generated by
the proposed infection maps. It is clear that when compared
with the activation maps extracted from deep models, the
proposed infection maps are highly superior and reliable cues
for COVID-19 infection.

V. DATA AVAILABILITY
This study introduces the QaTa-COV19 dataset, a publicly shared benchmark dataset, which is available at
https://www.kaggle.com/aysendegerli/qatacov19-dataset. The
live demo of the proposed approach is implemented on
http://qatacov.live/.

R EFERENCES
[1] “Severe Outcomes Among Patients with Coronavirus Disease 2019
(COVID-19)-United States, February 12-March 16, 2020. MMWR Morb
Mortal Wkly Rep 2020;69:343-346.” DOI:http://dx.doi.org/10.15585/
mmwr.mm6912e2.
[2] World Health Organization, “Coronavirus disease 2019 (covid-19): situation report, 88,” 2020.
[3] C. Sohrabi, Z. Alsafi, N. O’Neill, M. Khan, A. Kerwan, A. AlJabir, C. Iosifidis, and R. Agha, “World health organization declares
global emergency: A review of the 2019 novel coronavirus (covid-19),”
International Journal of Surgery, 2020.
[4] T. Singhal, “A review of coronavirus disease-2019 (covid-19),” The
Indian Journal of Pediatrics, pp. 1–6, 2020.
[5] P. Kakodkar, N. Kaka, and M. Baig, “A comprehensive literature
review on the clinical presentation, and management of the pandemic
coronavirus disease 2019 (covid-19),” Cureus, vol. 12, no. 4, 2020.
[6] Y. Li, L. Yao, J. Li, L. Chen, Y. Song, Z. Cai, and C. Yang, “Stability
issues of rt-pcr testing of sars-cov-2 for hospitalized patients clinically
diagnosed with covid-19,” Journal of medical virology, 2020.
[7] A. Tahamtan and A. Ardebili, “Real-time rt-pcr in covid-19 detection:
issues affecting the results,” Expert Review of Molecular Diagnostics,
vol. 20, no. 5, pp. 453–454, 2020.
[8] J. Xia, J. Tong, M. Liu, Y. Shen, and D. Guo, “Evaluation of coronavirus
in tears and conjunctival secretions of patients with sars-cov-2 infection,”
Journal of medical virology, vol. 92, no. 6, pp. 589–594, 2020.
[9] A. T. Xiao, Y. X. Tong, and S. Zhang, “False-negative of rt-pcr and
prolonged nucleic acid conversion in covid-19: rather than recurrence,”
Journal of medical virology, 2020.
[10] Y. Yang, M. Yang, C. Shen, F. Wang, J. Yuan, J. Li, M. Zhang, Z. Wang,
L. Xing, J. Wei et al., “Laboratory diagnosis and monitoring the viral
shedding of 2019-ncov infections,” MedRxiv, 2020.
[11] World Health Organization, “Laboratory testing for coronavirus disease
2019 (covid-19) in suspected human cases: interim guidance, 2 march
2020,” World Health Organization, Tech. Rep., 2020.
[12] S. Salehi, A. Abedi, S. Balakrishnan, and A. Gholamrezanezhad, “Coronavirus disease 2019 (covid-19): a systematic review of imaging findings
in 919 patients,” American Journal of Roentgenology, pp. 1–7, 2020.
[13] Y. Fang, H. Zhang, J. Xie, M. Lin, L. Ying, P. Pang, and W. Ji,
“Sensitivity of chest ct for covid-19: comparison to rt-pcr,” Radiology,
p. 200432, 2020.
[14] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, Q. Tao, Z. Sun, and
L. Xia, “Correlation of chest ct and rt-pcr testing in coronavirus disease
2019 (covid-19) in china: a report of 1014 cases,” Radiology, p. 200642,
2020.
[15] A. Bernheim, X. Mei, M. Huang, Y. Yang, Z. A. Fayad, N. Zhang,
K. Diao, B. Lin, X. Zhu, K. Li et al., “Chest ct findings in coronavirus
disease-19 (covid-19): relationship to duration of infection,” Radiology,
p. 200463, 2020.
[16] Y. Li and L. Xia, “Coronavirus disease 2019 (covid-19): role of chest
ct in diagnosis and management,” American Journal of Roentgenology,
vol. 214, no. 6, pp. 1280–1286, 2020.
[17] A. Narin, C. Kaya, and Z. Pamuk, “Automatic detection of coronavirus
disease (covid-19) using x-ray images and deep convolutional neural
networks,” arXiv preprint arXiv:2003.10849, 2020.
[18] D. J. Brenner and E. J. Hall, “Computed tomography—an increasing
source of radiation exposure,” The New England Journal of Medicine,
vol. 357, no. 22, pp. 2277–2284, 2007.
[19] G. D. Rubin, C. J. Ryerson, L. B. Haramati, N. Sverzellati, J. P. Kanne,
S. Raoof, N. W. Schluger, A. Volpi, J.-J. Yim, I. B. Martin et al.,
“The role of chest imaging in patient management during the covid19 pandemic: a multinational consensus statement from the fleischner
society,” Chest, vol. 158, no. 1, pp. 106–116, 2020.
[20] F. Shi, J. Wang, J. Shi, Z. Wu, Q. Wang, Z. Tang, K. He, Y. Shi, and
D. Shen, “Review of artificial intelligence techniques in imaging data
acquisition, segmentation and diagnosis for covid-19,” IEEE Reviews in
Biomedical Engineering, 2020.
[21] N. K. Chowdhury, M. M. Rahman, and M. A. Kabir, “Pdcovidnet: a
parallel-dilated convolutional neural network architecture for detecting
covid-19 from chest x-ray images,” Health information science and
systems, vol. 8, no. 1, pp. 1–14, 2020.
[22] T. D. Pham, “Classification of covid-19 chest x-rays with deep learning:
new models or fine tuning?” Health Information Science and Systems,
vol. 9, no. 1, pp. 1–11, 2020.
[23] M. E. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M. A. Kadir,
Z. B. Mahbub, K. R. Islam, M. S. Khan, A. Iqbal, N. Al-Emadi et al.,

12

[24]

[25]
[26]
[27]
[28]
[29]

[30]

[31]

[32]

[33]

[34]
[35]

[36]
[37]
[38]
[39]

[40]
[41]

[42]
[43]
[44]
[45]
[46]

“Can ai help in screening viral and covid-19 pneumonia?” arXiv preprint
arXiv:2003.13145, 2020.
I. D. Apostolopoulos and T. A. Mpesiana, “Covid-19: automatic detection from x-ray images utilizing transfer learning with convolutional
neural networks,” Physical and Engineering Sciences in Medicine, p. 1,
2020.
L. O. Hall, R. Paul, D. B. Goldgof, and G. M. Goldgof, “Finding covid19 from chest x-rays using deep learning on a small dataset,” arXiv
preprint arXiv:2004.02060, 2020.
L. Wang and A. Wong, “Covid-net: A tailored deep convolutional neural
network design for detection of covid-19 cases from chest x-ray images,”
arXiv preprint arXiv:2003.09871, 2020.
P. K. Sethy and S. K. Behera, “Detection of coronavirus disease (covid19) based on deep features,” Preprints, vol. 2020030300, p. 2020, 2020.
J. Zhang, Y. Xie, Y. Li, C. Shen, and Y. Xia, “Covid-19 screening on
chest x-ray images using deep learning based anomaly detection,” arXiv
preprint arXiv:2003.12338, 2020.
P. Afshar, S. Heidarian, F. Naderkhani, A. Oikonomou, K. N. Plataniotis,
and A. Mohammadi, “Covid-caps: A capsule network-based framework
for identification of covid-19 cases from x-ray images,” arXiv preprint
arXiv:2004.02696, 2020.
M. Yamac, M. Ahishali, A. Degerli, S. Kiranyaz, M. E. Chowdhury,
and M. Gabbouj, “Convolutional sparse support estimator based covid19 recognition from x-ray images,” arXiv preprint arXiv:2005.04014,
2020.
M. Ahishali, A. Degerli, M. Yamac, S. Kiranyaz, M. E. Chowdhury,
K. Hameed, T. Hamid, R. Mazhar, and M. Gabbouj, “Advance warning
methodologies for covid-19 using chest x-ray images,” arXiv preprint
arXiv:2006.05332, 2020.
F. Shi, L. Xia, F. Shan, D. Wu, Y. Wei, H. Yuan, H. Jiang, Y. Gao, H. Sui,
and D. Shen, “Large-scale screening of covid-19 from community
acquired pneumonia using infection size-aware classification,” arXiv
preprint arXiv:2003.09860, 2020.
C.-F. Yeh, H.-T. Cheng, A. Wei, K.-C. Liu, M.-C. Ko, P.-C. Kuo, R.-J.
Chen, P.-C. Lee, J.-H. Chuang, C.-M. Chen et al., “A cascaded learning
strategy for robust covid-19 pneumonia chest x-ray screening,” arXiv
preprint arXiv:2004.12786, 2020.
Y. Oh, S. Park, and J. C. Ye, “Deep learning covid-19 features on cxr
using limited training data sets,” IEEE Transactions on Medical Imaging,
2020.
T. Ozturk, M. Talo, E. A. Yildirim, U. B. Baloglu, O. Yildirim, and U. R.
Acharya, “Automated detection of covid-19 cases using deep neural
networks with x-ray images,” Computers in Biology and Medicine, p.
103792, 2020.
M. Z. Alom, M. Rahman, M. S. Nasrin, T. M. Taha, and V. K.
Asari, “Covid mtnet: Covid-19 detection with multi-task deep learning
approaches,” arXiv preprint arXiv:2004.03747, 2020.
A. Haghanifar, M. M. Majdabadi, and S. Ko, “Covid-cxnet: Detecting
covid-19 in frontal chest x-ray images using deep learning,” arXiv
preprint arXiv:2006.13807, 2020.
F. Shan, Y. Gao, J. Wang, W. Shi, N. Shi, M. Han, Z. Xue, and
Y. Shi, “Lung infection quantification of covid-19 in ct images with
deep learning,” arXiv preprint arXiv:2003.04655, 2020.
K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha, W. Liang,
C. Wang, K. Wang et al., “Clinically applicable ai system for accurate
diagnosis, quantitative measurements, and prognosis of covid-19 pneumonia using computed tomography,” Cell, 2020.
Y. Qiu, Y. Liu, and J. Xu, “Miniseg: An extremely minimum network
for efficient covid-19 segmentation,” arXiv preprint arXiv:2004.09750,
2020.
M. d. l. I. Vayá, J. M. Saborit, J. A. Montell, A. Pertusa, A. Bustos,
M. Cazorla, J. Galant, X. Barber, D. Orozco-Beltrán, F. Garcia et al.,
“Bimcv covid-19+: a large annotated dataset of rx and ct images from
covid-19 patients,” arXiv preprint arXiv:2006.01174, 2020.
“COVID-19
Image
Repository,”
2020.
[Online].
Available:
https://github.com/ml-workgroup/covid-19-image-repository.
[Accessedon16-September-2020]
“COVID-19
DATABASE,”
2020.
[Online].
Available:
https://www.sirm.org/category/senza-categoria/covid-19/
.[Accessedon16-September-2020]
J. P. Cohen, P. Morrison, and L. Dao, “Covid-19 image data collection,”
arXiv preprint arXiv:2003.11597, 2020.
“COVID-19 Radiography Database,” 2020. [Online]. Available:
https://www.kaggle.com/tawsifurrahman/covid19-radiography-database.
[Accessedon16-September-2020]
“Chest Imaging,” 2020. [Online]. Available: https://www.eurorad.org/
.[Accessedon16-September-2020]

[47] “RSNA Pneumonia Detection Challenge,” 2018. [Online]. Available: https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/
overview.[Accessedon22-September-2020]
[48] A. Bustos, A. Pertusa, J.-M. Salinas, and M. de la Iglesia-Vayá,
“Padchest: A large chest x-ray image dataset with multi-label annotated
reports,” Medical Image Analysis, p. 101797, 2020.
[49] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim, H. Liang, S. L.
Baxter, A. McKeown, G. Yang, X. Wu, F. Yan et al., “Identifying
medical diagnoses and treatable diseases by image-based deep learning,”
Cell, vol. 172, no. 5, pp. 1122–1131, 2018.
[50] D. Demner-Fushman, M. D. Kohli, M. B. Rosenman, S. E. Shooshan,
L. Rodriguez, S. Antani, G. R. Thoma, and C. J. McDonald, “Preparing
a collection of radiology examinations for distribution and retrieval,”
Journal of the American Medical Informatics Association, vol. 23, no. 2,
pp. 304–310, 2016.
[51] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. Wáng, P.-X. Lu, and
G. Thoma, “Two public chest x-ray datasets for computer-aided screening of pulmonary diseases,” Quantitative imaging in medicine and
surgery, vol. 4, no. 6, p. 475, 2014.
[52] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers,
“Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common thorax
diseases,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 2097–2106.
[53] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference on
Medical image computing and computer-assisted intervention. Springer,
2015, pp. 234–241.
[54] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:
A nested u-net architecture for medical image segmentation,” in Deep
Learning in Medical Image Analysis and Multimodal Learning for
Clinical Decision Support. Springer, 2018, pp. 3–11.
[55] F. Yu, D. Wang, E. Shelhamer, and T. Darrell, “Deep layer aggregation,”
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 2403–2412.
[56] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4700–4708.
[57] P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Ding,
A. Bagul, C. Langlotz, K. Shpanskaya et al., “Chexnet: Radiologistlevel pneumonia detection on chest x-rays with deep learning,” arXiv
preprint arXiv:1711.05225, 2017.
[58] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2016, pp.
2818–2826.
[59] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[60] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss
for dense object detection,” in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 2980–2988.
[61] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
neural networks for volumetric medical image segmentation,” in 2016
fourth international conference on 3D vision (3DV). IEEE, 2016, pp.
565–571.
[62] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
Corrado, A. Davis, J. Dean, M. Devin et al., “Tensorflow: Large-scale
machine learning on heterogeneous distributed systems,” arXiv preprint
arXiv:1603.04467, 2016.
[63] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
arXiv preprint arXiv:1412.6980, 2014.
[64] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via
gradient-based localization,” in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 618–626.

