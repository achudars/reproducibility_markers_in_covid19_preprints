R EAL - TIME M ASK D ETECTION ON G OOGLE E DGE TPU
Keondo Park, Wonyoung Jang, Woochul Lee, Kisung Nam, Kihong Seong, Kyuwook Chai, Wen-Syan Li
Department of Data Science
Seoul National University, Korea

arXiv:2010.04427v1 [cs.CV] 9 Oct 2020

{gundo0102, jwy4888, woochulee, kisung.nam, kev94yo, manapool, wensyanli}@snu.ac.kr

A BSTRACT
After the COVID-19 outbreak, it has become important to automatically detect whether people are
wearing masks in order to reduce risk of front-line workers. In addition, processing user data locally
is a great way to address both privacy and network bandwidth issues. In this paper, we present
a light-weighted model for detecting whether people in a particular area wear masks, which can
also be deployed on Coral Dev Board, a commercially available development board containing
Google Edge TPU. Our approach combines the object detecting network based on MobileNetV2
plus SSD and the quantization scheme for integer-only hardware. As a result, the lighter model
in the Edge TPU has a significantly lower latency which is more appropriate for real-time execution while maintaining accuracy comparable to a floating point device. Code is available at:
https://github.com/KeondoPark/coral_mask
Keywords: Real-time Mask Detection; Edge Computing; Convolutional Neural Network; Transfer
Learning

1

Introduction

After the COVID-19 pandemic, the necessity of wearing masks in public places cannot be overemphasized. To prevent
the spread of the virus, monitoring whether people are wearing masks has become an important task. However, it is
quite inefficient to manually check who is wearing a mask or not. Especially, front-line workers face a great risk because
they stay in places with a large floating population and checking whether all people wear masks are burdensome. In
addition, there are many public places that are susceptible to virus infection because many people can be crowded
in a particular area. Therefore, installing cameras for monitoring in all of those places could entail a large cost. To
mention another problem, if the camera simply records a video and transmits it to the cloud for the inference, privacy
and network bandwidth issues can arise. In order to address these problems, processing data locally, especially on
low-cost devices, can be a great solution.
In this paper, we present a light-weighted model for detecting whether people are wearing masks that could fit in
Coral Dev Board [1], a commercially available development board containing Google Edge TPU coprocessor—a 8-bit,
fixed point hardware. Our approach combines the object detection network utilizing MobileNetV2 [2] plus SSD [3]
and the quantization scheme [4, 5] for integer-only devices. MobileNetV2 is a model for performing computer vision
applications in a mobile devices. It features reduced computation using few parameters while retaining similar accuracy
to complex CNN models. Single Shot Detector (SSD) model detects objects from images with a single neural network.
SSD predicts bounding boxes from feature maps from different resolutions, thereby handles different objects well. Our
model combined MobileNetV2 and SSD, a single neural network. We employed transfer learning from existing face
detection neural network as well as integer-only post-training quantization.
As a result, we demonstrate that the lighter model in the Edge TPU has a significantly lower latency while
maintaining accuracy in comparison to a floating point hardware. This makes it plausible to detect whether people are
wearing masks in real-time without privacy and network issues, consequentially widely applicable in the real world.

2

Related Work

Convolutional Neural Networks (CNN) has been widely used for image classification since AlexNet [6], and it has
developed through VGG [7], GoogLeNet [8], and ResNet [9]. However, these studies were focused on improving

R EAL - TIME M ASK D ETECTION ON G OOGLE E DGE TPU

the accuracy of the model which increased the number of layers and parameters gradually without consideration of
computational efficiency. As artificial intelligence expands its application, deep neural network models are implemented
on smartphones or edge computational devices. This encouraged many approaches to reduce latency and model size,
while maintaining its accuracy. In order to reduce computational complexity, SqueezeNet [10] applied 1×1 convolution
and reduced filter size and MobileNet [11] employed depthwise-separable convolution. MobileNetV2 [2] achieved
state-of-the-art results using inverted residuals and linear bottlenecks. Another approach to improve computational
efficiency of CNN is quantization [4, 5] that reduces the precision of the weight parameter rather than the number of
parameters. Using quantization, the size of the model can be significantly reduced while maintaining accuracy.
Object detection methods can be categorized in two frameworks: region proposal networks such as R-CNN [12],
Fast R-CNN [13], and Faster R-CNN [14] and regression/classification based networks such as MultiBox [15], YOLO
[16], and SSD [3]. Region proposal networks make proposals for the potential region where the object lies, and then
classify the object. Regression/classification based networks consider object detection and classification tasks as a single
regression task. Since regression/classification based networks process the input image directly to bounding boxes and
probabilities, it takes less time compared to region proposal networks. In this paper, we utilized the architecture of SSD
[3] to ensure the accuracy and performance in edge computational devices.

3

Model

In this section, we describe our model.
3.1

Architecture

We started our journey with two neural network structure, namely 2NN model. This model uses two different neural
networks: one for detection and the next one for classification. We used MobileNetV2 plus SSD model to detect faces
and another MobileNetV2 model to classify the detected face into mask or no mask. We did not train the detection
model, but used the pre-trained model from tensorflow model zoo[17]. We trained the classification model with our
data by transfer learning from MobileNetV2.
In order to improve the model efficiency we incorporated the classification model into detection model, namely
1NN model. 1NN model uses MobileNetV2 plus SSD architecture[2] [3]. As will be described in the following section,
our dataset has relatively small number of images. Therefore we took advantage of transfer learning from pre-trained
face detection model. We took the face detection model from Tensorflow 1 Object Detection Model Zoo[17] and used
this model as our base model. Before we start training, we made some change on base model, for better transfer learning:
Since the face detection model has only one class(face) but our model needed two classes(Mask, NoMask), we extended
the class prediction layer to have 2 classes. We copied the weights corresponding to face detection from the base model
and applied same weights for both mask and nomask class in our model. For mask class, however, we slightly changed
the weights from the base model: we added 1e-7 to each weight corresponding to mask class. By doing so, the initial
model would not give the same score for both mask and nomask class but give priority to the nomask class. We did
nit change weights for image feature maps and box prediction. After this setup is completed, we started our transfer
learning - this is explained in the following section.
The high level comparison of 1NN and 2NN model architecture is described in Figure 1
The second neural network in 2NN model is comprised of MobileNet v2 for classification + Average Pooling,
Flatten and Dense Layers. The second neural network has additional layer on top of MibileNetV2. All the parameters in
the base MobileNetV@ layer is set ’non-trainable’. Next, two-dimensional (2D) average pooling layer is added for
down-sampling of tensor. And flatten layer is applied followed by dense layer. The output of dense layer is (None, 128)
with activation function of ’RELU’. And then, drop out layer is applied for regularization fraction of the input units
with dropout ratio of 0.5. The training scheme is described in
3.2

Data

We collected 2,127 images of people’s faces wearing masks and not wearing masks. We split the dataset into training
set of 1,270 images, validation set of 425 images and test set of 425 images. When splitting the dataset, we took special
care so that images with people wearing masks or not should be evenly distributed across training, validation and test
set. Since the face images with masks on were fewer than raw face images, we added mask on top of faces, as in Figure
3(a). This helps to balance the number of data between mask and nomask class.
2

R EAL - TIME M ASK D ETECTION ON G OOGLE E DGE TPU

(a) 2NN model architecture

(b) 1NN model architecture

Figure 1: Comparison of 1NN and 2NN model architecture

3.2.1

1NN Model

For 1NN model, we marked the groundtruth boxes and labels in all faces in each image The number of images for
training, validation and test set is same as described above.
3.2.2

2NN Model

For 2NN model, we did not train the first network for face detection as mentioned in the previous section. The second
network is trained to distinguish whether the detected face is with a mask or without a mask. Therefore we cropped
faces from training and validation set, and used these faces for training and validation set of 2NN model, respectively.
Because some images we collected contain more than one face, there are greater number of data for 2NN model training
and evaluation. The resulting number of images are 1,893 for training/validation. We used the same evaluation set as
the 1NN model in order to consistently compare the performance of 1NN and 2NN models.
3.3
3.3.1

Training
1NN Model

In transfer learning process, we did not freeze any layers in the base MobilenetV2 plus SSD model. We used batch size
of 32. We used RMS Prop optimizer with initial learning rate of 0.04, momentum optimizer value of 0.9 and decaying
at 0.9. The learning rate is also subject to exponential decaying schedule with decaying factor of 0.95 and decaying step
of 800,720. Images are pre-processed to have 320 X 320 resolution, before used for training. We did quantization-aware
3

R EAL - TIME M ASK D ETECTION ON G OOGLE E DGE TPU

Figure 2: Architecture of the second network in 2NN

(a) Artificial image from non-mask image

(b) Real mask image

Figure 3: Example of mask images used for training

training for quantized model and same hyperparameters were used for both models. Both models were trained for
50,000 epochs. The training and validation losses are described in 4. We used Intel Xeon Silver 4216 CPU and Nvidia
Titan RTX GPU 1EA for training.
3.3.2

2NN Model

Training Setup We train our models using TensorFlow[18]. We used GPU server. GPU server is built by Intel Xeon
Silver 4216 CPU(32 cores, 2.1GHz), 512GB memory and Nvidia Titan RTX GPU 1EA. Initial learning rate is 1e-2 and
learning rate decay rate is initial learning rate divided by the number of each epoch. Image size is 224 X 224. Input
images are pre-processed to 224 X 224, whatever magnitudes they have. Binary cross entropy was used as loss function.
The number of epochs is 32 and batch size is 32.

4

Experimental Evaluation

In this section, we describe our evaluation methodology and results.
4

R EAL - TIME M ASK D ETECTION ON G OOGLE E DGE TPU

(a) Base model

(b) Quantized model

Figure 4: Train and validation loss of the model
4.1

Model Size

1NN model has 5.5 million parameters and its size is 22.9MB before quantization. The model size decreased to 5.4MB
after 8-bit full integer quantization. The number of parameters increased slightly because of the additional parameters
required for quantization. Special type of compiling is necessary to run the model on EdgeTPU. This so called EdgeTPU
compiling increases the size of model to 6.4MB.
The first detection model in 2NN model has 3.4 million parameters and its size is 22.2MB before quantization
and 5.2MB after quantization. The second classification model has 2.9 million parameters and its size is 17.2MB before
quantization and 3.5MB after quantization. After EdgeTPU compiling, the model size increases to 6.4MB and 3.55MB,
respectively.

Model

Params

1NN
2NN Detection
2NN Classification

5.5M
3.4M
2.9M

Size
Baseline

Quantization

EdgeTPU compiling

22.9MB
22.2MB
17.2MB

5.4MB
5.2MB
3.5MB

6.4MB
6.4MB
3.55MB

Table 1: Summary of model sizes

4.2

Experiment Environment

For comparison, experiments of measuring latency and accuracy of 1NN and 2NN models are held in a CPU and an
Edge TPU environment. Experiments for the CPU environment are conducted on Intel® Core™ i7-6700(3.40 GHz).
Experiments for the Edge TPU environment are measured on Google Edge TPU coprocessor, included in the Coral Dev
Board.
The Coral Dev Board is a single-board computer released by Google in 2019. The Coral Dev Board comprises
an on-board System-on-Module(SoM). The SoM provides a fully-integrated system, especially including the Google
Edge TPU coprocessor. The Google Edge TPU coprocessor is an ASIC designed by Google to perform machine
learning inference on edge devices. The Google Edge TPU coprocessor is capable of performing 4 trillion operations
(tera-operations) per second (TOPS), using 0.5 watts for each TOPS (2 TOPS per watt)[1].
For both environments, the 2NN Model is experimented only with the version after quantization. The 1NN model
is experimented with both before and after quantization in both environments.
4.3

Latency

The amount of time to run a single inference is referred as latency, and is measured in this part. We measured the time
spent for the inference of 425 test images and averaged the time spent for 1 image. Also we ran each model 3 times and
averaged them. For the 2NN model, we need to go through two networks as mentioned before. Therefore, its latency is
calculated as a sum of average inference time of face detection, and average inference time of mask classification. The
5

R EAL - TIME M ASK D ETECTION ON G OOGLE E DGE TPU

1NN model only utilizes one network, and the average inference time of that network is calculated as its latency. Three
different versions are tested for each model; the baseline model, model after quantization, and model after Edge TPU
compiling. The experiment is done in both CPU and Edge TPU.
Overall, the 1NN model showed faster results, with the quantized version achieving 455ms of average inference
time in the CPU environment, while the 2NN model took 1267ms for inference. In the Edge TPU environment, the
1NN model was still superior and showed 6.4ms of average inference time, while it took 26ms for the 2NN model. The
baseline version of the 1NN model took 507ms for inference, which is faster than the quantized version of the 2NN
model in a CPU environment. Latency testing results are summarized in Table 2.
Environment

Data Type

1NN

2NN

CPU

32-bit
8-bit

507ms
455ms

1267ms

Edge TPU

8-bit

6.4ms

26ms

Table 2: Latency of the model according to different environments and architectures

4.4

Accuracy

The test accuracy is measured based on COCO mAP, which is widely used for evaluating image detection and
classification results. Specifically, we measured AP at IoU=.50:.05:.95, which is used as the primary challenge metric.
In the case of the 2NN model, mAP is not directly applicable, our evaluation needs few more steps. First, we get the
bounding boxes and corresponding scores from the first model. Then we passed each bounding box through the second
model and multiplied the score from the first model by that from the second model. This produces the same result set as
if the test image passed through a single detection model, so we could apply the same evaluation metric. A total of 425
images are used as the test set, including images with more than 2 people. The test set also includes augmented images
that combined masks on the bare face, and images of people wearing masks of various colors such as white, blue, black,
and gray.
In conclusion, the difference in test accuracy according to the device was not large. First, for the 2NN model in
8-bit floating devices, the test accuracy of the CPU model was 55.0%, and that of the Edge TPU model was 54.8%.
That is, there is little sacrifice in accuracy for Edge TPU compared to CPU. The same was true for the 1NN model. The
test accuracy in 8-bit floating devices was 58.8% for CPU model and 58.4% for Edge TPU model, which was not a big
difference. Compared to the 32-bit floating CPU, there was a difference of about 6% points. Overall, the 1NN model
showed better test accuracy than the 2NN model. Table 3 summarizes the test accuracy.
Environment

Data Type

1NN

2NN

CPU

32-bit
8-bit

64.7%
58.8%

55.0%

Edge TPU

8-bit

58.4%

54.8%

Table 3: Accuracy of the model according to different environments and architectures

5

Conclusion

We presented a light-weighted convolutional neural network model for mask detection which is applicable in the real
world in terms of latency and model size, by using MobileNetV2 plus SSD and quantization scheme. Our model, even
in a much smaller size compared to a model for 32-bit floating point devices, shows extremely low latency without
sacrifice in accuracy. This model can effectively avoid cost and network issues, accordingly can be widely used for
public purposes in real life.

References
[1] Google Coral Dev Board. https://coral.ai/docs/dev-board/datasheet/, 2020.
6

R EAL - TIME M ASK D ETECTION ON G OOGLE E DGE TPU

[2] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Inverted residuals
and linear bottlenecks: Mobile networks for classification, detection and segmentation. CoRR, abs/1801.04381,
2018.
[3] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C.
Berg. SSD: Single shot multibox detector. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
Computer Vision – ECCV 2016, pages 21–37, Cham, 2016. Springer International Publishing.
[4] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural networks:
Training neural networks with low precision weights and activations. J. Mach. Learn. Res., 18(1):6869–6898,
January 2017.
[5] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and
Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
[6] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information
Processing Systems 25, pages 1097–1105. Curran Associates, Inc., 2012.
[7] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition,
2014.
[8] C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 1–9, 2015.
[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2016.
[10] Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer.
SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size, 2016.
[11] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for mobile vision applications,
2017.
[12] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and
semantic segmentation. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 580–587,
2014.
[13] Ross Girshick. Fast R-CNN, 2015.
[14] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with
region proposal networks. In Proceedings of the 28th International Conference on Neural Information Processing
Systems - Volume 1, NIPS’15, page 91–99, Cambridge, MA, USA, 2015. MIT Press.
[15] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In
2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 2155–2162, 2014.
[16] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You Only Look Once: Unified, real-time object detection. In
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 779–788, 2016.
[17] TensorFlow 1 Detection Model Zoo. https://github.com/tensorflow/models/blob/master/research/object_detection/
g3doc/tf1_detection_zoo.md. 2020.
[18] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael
Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat
Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever,
Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden,
Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015.

7

