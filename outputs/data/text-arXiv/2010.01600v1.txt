ON NONNEGATIVE MATRIX AND TENSOR DECOMPOSITIONS FOR COVID-19 TWITTER DYNAMICS

arXiv:2010.01600v1 [cs.IR] 4 Oct 2020

LARA KASSAB1 , ALONA KRYSHCHENKO2 , HANBAEK LYU3 , DENALI MOLITOR 3 , DEANNA NEEDELL3 , ELIZAVETA REBROVA3

A BSTRACT. We analyze Twitter data relating to the COVID-19 pandemic using dynamic topic modeling techniques to
learn topics and their prevalence over time. Topics are learned using four methods: nonnegative matrix factorization
(NMF), nonnegative CP tensor decomposition (NCPD), online NMF, and online NCPD. All of the methods considered
discover major topics that persist for multiple weeks relating to China, social distancing, and U.S. President Trump. The
topics about China dominate in early February before giving way to more diverse topics. We observe that NCPD and
online NCPD can detect topics that are prevalent over a few days, such as the outbreak in South Korea. The topics detected
by NMF and online NMF, however, are prevalent over longer periods of time. Our results are validated against external
news sources.

1. I NTRODUCTION
Analyzing social media using topic models has become popular for studying and tracking various public health
events around the world [35, 11, 34]. Topic modeling is an unsupervised machine learning technique used to reveal
latent themes from large text datasets. Dynamic topic modeling investigates how topics evolve in a sequentially organized corpus of documents, where the data is typically divided by time slices e.g. by day [38, 5, 19, 18]. Here, we
perform dynamic topic modeling in two ways. The first investigates the change in prevalence of topics over time
while the second considers the evolution of the topics themselves over time. A popular topic modeling method is
nonnegative matrix factorization (NMF) [50, 26]. Natural extensions of NMF that capture topic evolution are its
online versions, when the data can be updated with time, or tensor analogues [1, 43, 10, 2, 47]. Here, we analyze the
use of NMF and tensor nonnegative CP decomposition (NCPD) for dynamic topic modeling. Other popular dynamic topic modeling techniques include latent Dirichlet allocation [20, 54], a probabilistic approach as opposed
to the dimension reduction approach of NMF and NCPD.
Here, we consider data from the the social media platform, Twitter. During the COVID-19 pandemic, Twitter has experienced increased usage including discussion and dissemination of information relating to the pandemic [44, 9]. A number of related works consider Twitter and other social-media data related to the COVID-19
pandemic via various statistical and learning approaches, with specific aims to understand the effects and prevalence of bots and misinformation [14, 51], polarization [15, 6], sentiment and emotional state [52, 22], gender
differences [46], racism and xenophobia [55], and other aspects [33]. In this work, we illustrate the abilities of
four different unsupervised nonnegative matrix and tensor factorization approaches for dynamic topic modeling
to study the topics learned from social media and how they coincide with events portrayed in news outlets. We
compare the various advantages of the methods and how they incorporate temporal information, and showcase
that the topics identified may be used to gauge how discussions among the public change over time, even in tumultuous circumstances.
1.1. Data. We consider Twitter text data related to the COVID-19 pandemic from Feb. 1 to May 1 of 2020 [9]. Twitter
is a social media website where users post and interact with short texts known as tweets. The number of COVID-19
related tweets is large, typically containing hundreds of thousands of tweets each hour. While the online methods
considered are well-suited to handling large streaming data, we subsample this dataset for ease of computation
and to compare results on a fixed dataset with more standard methods that require loading the data into memory.
Specifically, we consider the top 1000 retweeted English tweets from each day. Using the most retweeted tweets,
limits the data to information that was shared and spread widely.
1 Department of Mathematics, Colorado State University, Fort Collins (kassab@math.colostate.edu).
2 Department of Mathematics, California State University Channel Islands, Camarillo (alona.kryshchenko@csuci.edu).
3 Department of Mathematics, University of California, Los Angeles ({hlyu, dmolitor, deanna, rebrova}@math.ucla.edu).

1

1.2. Preliminaries and Notation. Tensors are common algebraic representations for multidimensional arrays. The
order of a tensor is the number of dimensions, which is also referred to as ways or modes [23]. We denote vectors
with lowercase letters x with x(k) denoting its k th entry, matrices with uppercase boldface letters, X, and thirdorder tensors with uppercase calligraphic letters X . For a matrix X, the vector x k denotes its k th column. We let
k·kF and k·k1 denote the entrywise Frobenius norm, and the entrywise L 1 norm respectively. The set of nonnegative
real numbers [0, ∞) is denoted R+ . We let ⊗ denote the outer product of two vectors. For tensors A and B of the
same size, denote by A ¯ B the Hadamard (pointwise) product. When B is a matrix, for each 1 ≤ j ≤ n, we denote
their j -mode product by A × j B. See [23] for an excellent survey of related definitions and tensor algorithms.
1.3. Contributions. We analyze Twitter text data related to the COVID-19 pandemic [9] using four unsupervised
topic modeling techniques, specifically NMF and NCPD as well as online variants that are well-suited for large
data. We illustrate each of these methods as a dynamic topic modeling technique, revealing latent themes in the
data with emphasis on temporal evolution.
From the text data, the methods discover topics trending in news sources, including political events, personal
beliefs about COVID-19 and calls to action, and successfully attribute them to the days they were trending (see
Table 1 for a rough timeline and sources used1). We find that tensor methods capture more topics that are present
primarily over short time periods compared with NMF and online NMF. We discover many such fleeting topics in
the early days of COVID-19, while larger trends dominate as time goes on. We demonstrate that online counterparts to NMF and NCPD produce similar results and are viable alternatives that allow for processing much larger
amounts of data without subsampling. To our knowledge, this is the first application of online NCPD to text data.
We compare and contrast the various approaches, showing there are advantages to using each method, and that
significant pandemic related information can be obtained through monitoring social media data.
2. M ATRIX M ETHODS
We describe how the matrix decompositions NMF and online NMF can be used to analyze the dynamics of
topics discussed on Twitter during the first several months of the COVID-19 pandemic.
2.1. Nonnegative Matrix Factorization. NMF is a popular tool for extracting hidden topics from text data [7, 25].
For a data matrix X ∈ Rm×n , one learns a low-rank dictionary W ∈ Rm×r and code matrix H ∈ Rr ×n that minimize
kX − WHk2F , where r > 0 is typically chosen such that r < min{m, n}. The dictionary matrix W represents topics in
terms of the original features (in our case unigrams and bigrams). Each column of the code matrix H represents a
data point as a linear combination of the dictionary elements with nonnegative coefficients. We use NMF to learn
a dictionary W from all data and analyze topic dynamics through changes in topic prevalence over time in the code
matrices Hi from each day’s data for a total of T = 90 days.
2.2. Online NMF. Similar to NMF, we use online NMF (Algorithm 1) to learn global topics and then determine the
distribution over topics for each time interval. In online NMF, however, the topics are learned sequentially from
the data, allowing past learned topics to influence those learned for current data [28, 30, 16, 53].
Online NMF has some advantages over standard NMF when applied to large temporal data. First, online NMF
can be used with streaming data as opposed to processing all of the data at once. This ability is especially useful
if data is continually added or if the data is too large to load into memory at once. Second, the dictionary W can
be updated to reflect historic topics, but also incorporate changes in the data. The balance between preserving
historic topics and updating the topics based on new data is controlled by the learning rate. The learning rate, t −β ,
decreases as the number of updates t increases (see Algorithm 1). Smaller values of β ≥ 0 lead to a larger learning
rate, so that changes in the most recent data have a stronger impact on the learned topics.
When learning topics, online NMF only requires storing the current dictionary matrix W, data from a single
time slice, Xt , and the current code matrix H. If loading a single time slice, Xt , requires too much memory, a
e t can be loaded into memory at each inner iteration. On the other hand, NMF requires access to the
subsampled X
full data matrix at each iteration [30]. See Table 2 for a comparison of the memory requirements of the methods
considered.
Under certain conditions, the expected loss for the decomposition learned by online NMF converges almost
surely to that of NMF [28, 30, 16, 53]. Despite the fact that the streamed COVID-19 related tweets do not exhibit the
1The topics learned from data and news references may contain factually inaccurate information. While we include some news references

to validate results, many additional sources exist. We make no claims that these sources are factually accurate.
2

Algorithm 1 Online NMF
Require: data slices Xt , initial matrix W, learning rate parameter β
1: Set fˆ0 ≡ 0
2: for t = 1, . . . , T do
3:
for i = 0, . . . , iterations do
e t ← subsample minibatch from Xt
4:
X
°
°
e t − WH°2
5:
H ← argminH∈Rr ×n °X
F
+
°
°
e t − W0 H°2
6:
fˆt (W0 ) := (1 − t −β ) fˆt −1 (W0 ) + t −β °X
F
7:
W ← argmin 0 m×r fˆt (W0 )

. Calculate dictionary W

. See [28] for implementation details

W ∈R+

8:
9:
10:

. Calculate topic representations

for t = 1, . . . , T do
Ht ← argminH∈Rr ×n kXt − WHk2F
+

return W and Ht for 1 ≤ t ≤ T

structure necessary for theoretical convergence guarantees, we see in Subsection 4.4 that online NMF is a viable
proxy for NMF in learning topics over time.
3. T ENSOR M ETHODS
We describe and compare the use of NCPD and online NCPD for analyzing the dynamics of topics of the COVID19 Twitter dataset organized into a 3-order tensor, where the modes correspond to time (in days), vocabulary
(alphabetical), and tweets (descending in frequency).
3.1. Nonnegative CP Tensor Decomposition. A main goal of dynamic topic modeling is to extract latent themes in
the data along with their temporal evolution. One strategy for achieving this is to decompose higher-dimensional
data tensors into interpretable lower-dimensional representations. Similar to NMF, one might adapt tensor decompositions with some additional structure, such as nonnegativity, which allows for interpretability of topics.
NCPD is considered as a dynamic topic modeling technique for tensor data that successfully showcases topic variation across all modes of the tensor (including the temporal mode), thus preserving the temporal information in
the data [1, 10, 2].
NCPD factorizes a tensor into a sum of nonnegative component rank-one tensors, defined as outer products of
n ×n ×n
nonnegative vectors [8, 17]. In particular, given a third-order tensor X ∈ R+1 2 3 and a fixed integer r > 0, the
P
n 3 ×r
n 1 ×r
n 2 ×r
approximate NCPD of X seeks matrices A ∈ R+ , B ∈ R+ , C ∈ R+ , such that X ≈ rk=1 a k ⊗ b k ⊗ c k , where
the nonnegative vectors a k , b k , and c k are the columns of A, B, and C, respectively. The matrices A, B, and C are
referred to as the NCPD factor matrices.
To obtain a nonnegative
approximation with fixed rank r we consider
°
°
P
minimizing the reconstruction error, °X − rk=1 a k ⊗ b k ⊗ c k °F among all the nonnegative vectors a k , b k , and c k .
Several approaches have been proposed for solving this problem including, alternating least squares method, proposed in the original papers [8, 17], and the multiplicative update algorithm [39].
Applying NCPD to the third-order tensor data X , we obtain three factor matrices capturing topic variation
across each mode: time representation of topics (changing topic prevalence through time: emerging, trending,
fading etc), term representation (the unigrams/bigrams that characterize each topic) and tweet representation
(the tweets associated with each topic).
3.2. Online NCPD. We consider a special case of the recently introduced online CP-dictionary learning [41] as an
online counterpart of NCPD. We specifically assume that the factor matrices are constrained to be nonnegative
and call the method online NCPD (Algorithm 2).
n ×n ×n
Given a probability distribution π on the set of data tensors R+1 2 3 , online NCPD seeks nonnegative factor
n 3 ×r
n 1 ×r
n 2 ×r
matrices A ∈ R+ , B ∈ R+ , C ∈ R+
by minimizing the expected reconstruction error, f (A, B, C), defined as
°
°2
"
#
°
°
r
X
°
°
(1)
EX ∼π inf °X −
h(k) a k ⊗ b k ⊗ c k ° + λkhk1
°
h∈Rr+×1 °
k=1
F

where the random data tensor X is sampled from the distribution π and λ ≥ 0 is a sparsity regularizer. For each
realization of X , the optimal choice of h ∈ Rr+×1 gives the nonnegative coefficients to combine the rank-1 tensors
3

a k ⊗ b k ⊗ c k . One can think of such h’s as the columns of the code matrix H in NMF. Minimizing Equation (1) is
equivalent to minimizing the NCPD reconstruction error when the distribution π is supported on a single data
tensor and λ = 0. In this case, one can absorb h(k) into a k as there is a single data tensor to factorize.
The expected reconstruction error f (Equation (1)) is approximated by the empirical reconstruction error
f t (A, B, C) :=

(2)

t
1X
`(Xs , A, B, C),
t s=1

where `(X , A, B, C) denotes the infimum in Equation (1). The theory of empirical processes [45] guarantees k f −
f t k∞ → 0 almost surely as t → ∞ when the factor matrices A, B, C are restricted in a compact set and the data
tensors Xs are asymptotically distributed as π with some mild condition (see also [28, Lem. 7.10]). Hence the
problem of minimizing the expected reconstruction error f in Equation (1) can be reformulated as follows: Data
tensors Xt arrive online (sequentially), and for each t ≥ 1, one finds factor matrices At , Bt , Ct that minimize the
empirical reconstruction error f t in Equation (2). However, this is a computationally difficult problem since f t is
nonconvex.
Algorithm 2 gives a high-level description of online NCPD. Given the new observed tensor Xt , we first find
its best linear rank-1 approximation using the current best factor matrices A, B, C, then update a surrogate loss
function fˆt that majorizes the empirical loss function f t in Equation (2), and then update the factor matrices A, B, C
sequentially with the updated surrogate loss function. This follows the general scheme of Stochastic MajorizationMinimization [29], which also generalizes the online NMF algorithm in Algorithm 1. Defining the full surrogate loss
function fˆt as in Algorithm 2 (and hence storing all previous data tensors X1 , · · · , XN in memory) can be avoided
n ×n ×n ×r
by using aggregate tensors of size Rr+×r and R+1 2 3 . See [41] for implementation details.
Algorithm 2 Online NCPD
n ×n 2 ×n 3

Require: data tensors Xt ∈ R+1
eters r ≥ 1, λ ≥ 0, β ≥ 0.
2: for t = 1, · · · , N ·do

1:

n ×r

, 1 ≤ t ≤ N , initial factor matrices [A, B, C] ∈ R+1

¸
°
°2
° °
P
°
°
h ← argmin °Xt − rk=1 h 0 (k)a k ⊗ b k ⊗ c k ° + λ °h 0 °1

4:

´
³
fˆt (A , B0 , C0 ) := 1 − t1β fˆt −1 (A0 , B0 , C0 )

h 0 ∈Rr+

6:
7:
8:

n ×r

× R+3

, param-

. Coding

3:

5:

n ×r

× R+2

0

A ← argminA0 ∈Rn1 ×r fˆt (A0 , B, C)
+
B ← argmin 0 n2 ×r fˆt (A, B0 , C)

. Update surrogate function
F
h°
i
°2
P
+ t1β °Xt − rk=1 h(k)a k0 ⊗ b k0 ⊗ c k0 °F + λ khk1
. Update factor matrices sequentially

B ∈R+

C ← argminC0 ∈Rn3 ×r fˆt (A, B, C0 )
+

n ×r

return [A, B, C] ∈ R+1

n ×r

× R+2

n ×r

× R+3

The coding step (Line 3) of Algorithm 2 is a convex problem and can be easily solved by a number of known
algorithms (e.g., LARS [13], LASSO [42], and feature-sign search [27]). The surrogate loss function fˆt is quadratic in
each matrix coordinate, so each of the three subproblems in the factor matrix update step in Algorithm 2 is a constrained quadratic problem and can be solved by projected gradient descent (see [30, 28, 41]). The more general
version of online NCPD was shown to converge almost surely to the set of stationary points of the expected reconstruction error in Equation (1) under mild regularity assumptions of the sub-problems and Markovian dependence
in data tensors X1 , · · · , XN [41].
4. T WITTER T OPIC DYNAMICS
We apply NMF, online NMF, NCPD, and online NCPD to recover topics related to the COVID-19 pandemic discussed on Twitter and their prevalence over time. The dataset is detailed in Subsection 1.1 [9]. Note that the
objective functions for NMF and NCPD are nonconvex and, thus, the resulting low-dimensional representations
depend on the optimization algorithm used and its initialization. Twenty topics are used in order to recover both
general topics that occur over longer time spans (for example, new cases, social distancing) and topics that are
discussed over shorter time periods (such as specific outbreaks, cruise ships). Table 1 gives a timeline with notable
events discovered in the data.
4

Date

News

2 Feb

‘No Meat, No Coronavirus’ [49]

3 Feb

COVID-19 cruise ship outbreak [21]

7 Feb

Death of Dr. Li Wenliang [3]

8 Feb

COVID-19 death toll overtakes SARS [12]

18 Feb Spike of cases in South Korea [40]
26 Feb Mike Pence appointed to lead Coronavirus task force [37]
28 Feb ‘Trump calls Coronavirus Democrats’ ‘new hoax” [31]
29 Feb First COVID-19 death in the U.S. [32]
11 Mar WHO declares a pandemic [48]

TABLE 1. Event dates, headline summaries and references for news events relevant to identified
topics.
4.1. Experimental Setup. Tweets are converted to term frequency–inverse document frequency (TFIDF) vector
representations using the sklearn TFIDFVectorizer [36]. Unigrams and bigrams are considered and referred to as
terms. We use the default tokenizer and parameters, and limit the vocabulary size to 5000. The NLTK English stopword list [4] is removed as well as non-word sequences such as “https". We additionally remove words that are
essentially synonymous with COVID-19 as all the tweets in the dataset are related to this common topic. Specifically, we remove all words containing “coron" or “cov" that are not followed by the letter “e". is represented by a
positive linear combination of terms. Terms with larger values being more significant to the topic.
We present results for each of the methods in the form of a heatmap that summarizes both the term representation and temporal prevalence of topics (see Figures 1 to 4). For NMF and online NMF, the mean topic representation for each day is given in the columns of the heatmaps, while for NCPD and online NCPD, the factor matrix
showcasing the temporal representation of the topics is shown. Each row of the heatmaps corresponds to a learned
topic and a three-term summary of each topic is included for convenience 2. Further, each column of the heatmap
indicates the distribution over the extracted topics for each day. For online NMF and online NCPD, we also present
three-month evolution of r = 5 topics and their prevalence (see Subsection 4.7).
4.2. Common Topics and Patterns. Each of the methods separately recover several common trends in the data.
Generally, China-related topics are most prominent in early and mid Feb. The prevalence of these topics then
decreases in mid Feb. A topic relating to new cases spikes in prevalence in mid Feb. as outbreaks begin to occur
around the world. In late Feb. to mid Mar., a topic relating to U.S. President Trump and his administration’s response spikes in prevalence. Separate “social distancing", “stay home," and “lockdown" topics begin in early to
mid Mar. These topics typically persist throughout Apr.
4.3. Nonnegative Matrix Factorization. We first consider standard NMF and plot the average distribution over
learned topics for each day in Feb.-Apr. of 2020. Results are shown in Figure 1 (see Subsection 4.1 for plot description). The learned topic prevalence is largely as expected. China related topics 5 and 12 are most prominent in early
and mid Feb. The prevalence of these topics then decreases in mid Feb. Topic 7, relating to new cases and deaths,
peaks in mid Feb. as the virus spread around the world, particularly in Italy and Iran. In late Feb., the topic relating
to U.S. President Trump and his administration’s response spikes in prevalence as the topics relating to China decrease in prevalence. An “outbreak/spread" topic is gradually replaced by a “pandemic/global pandemic" topic in
early Mar., which coincides with the World Health Organization’s declaration of a pandemic on Mar. 11 [48]. One
of the most prevalent topics from late Feb. on is a general urgency topic (topic 4: “get", “need", “time").
2The process for generating term summaries is detailed in the appendix. Keywords and their associated weights for the learned topics are

also summarized in tables in the appendix.
5

0.10

0.15

0.20

0.25

people, many people, people died: 1
like, looks like, would like: 2
health, public health, world health: 3
get, need, time: 4
wuhan, chinese, hospital: 5
lockdown, day, police: 6
new cases, deaths, total: 7
social distancing, practice social, social media: 8
us, help us, let us: 9
trump, trump administration, response: 10
stay home, stay safe, home stay: 11
china, world, communist: 12
cruise ship, japan, hong kong: 13
president, president trump, realdonaldtrump: 14
first, case, first case: 15
death toll, deaths, breaking: 16
tested positive, tests positive, test positive: 17
united states, country, cdc: 18
pandemic, global pandemic, world: 19
outbreak, spread, due: 20

02
02-01
02-07
02-13
02-19
03-25
03-02
03-08
03-14
03-20
04-26
04-01
04-07
04-13
04-19
-2
5

Topic

0.05

Date
F IGURE 1. The normalized mean topic representation of tweets per day learned via NMF with
rank 20.
4.4. Online NMF. The final learned topics for online NMF with 20 topics and the distribution over the topics for
each day is shown in Figure 2, with learning parameter β = 0.7, minibatch size of 50 tweets, and 100 inner iterations.
The progression of the learned topics with the same parameters but five topics is shown in Figure 5. Comparing

0.2

help, fight, spread: 1
us, let us, tell us: 2
stay home, stay safe, home stay: 3
like, would, looks like: 4
president, president trump, states: 5
social distancing, practicing, guidelines: 6
time, first, day: 7
cases, deaths, total: 8
tested positive, tests positive, test positive: 9
trump, donald trump, americans: 10
please, need, let: 11
government, news, good: 12
lockdown, end, day: 13
china, world, wuhan: 14
pandemic, global pandemic, middle pandemic: 15
health, workers, care: 16
state, april, india: 17
testing, get, patients: 18
new, new york, new cases: 19
people, many people, people died: 20

02
02-01
02-07
02-13
02-19
03-25
03-02
03-08
03-14
03-20
04-26
04-01
04-07
04-13
04-19
-2
5

Topic

0.1

Date
F IGURE 2. The normalized mean topic representation of tweets per day learned via online NMF
with rank 20.
Figures 1 and 2, the term representations of the final topics learned via online NMF with β = 0.7 are similar to those
learned via NMF.
6

The similarity of the results for NMF and online NMF suggests that despite no guarantee of convergence for this
temporal data online NMF is a reasonable alternative to NMF for streaming data and for data that is prohibitively
large for NMF. The topic prevalences are, however, more evenly spread among the topics, especially from mid Mar.
through Apr. Some differences between NMF and online NMF are the inclusion of topics relating to New York
(topic 5), India (topic 19), the UK (topic 11), and a “health workers" topic (topic 18) in online NMF. Topics relating
to Japan, Hong Kong, cruise ships, and the CDC appear with NMF, but do not occur in the learned online NMF
topics.
90×5000×1000
4.5. Nonnegative CP Tensor Decomposition. We compute the rank-20 NCPD of the tensor data X ∈ R+
with multiplicative updates [39] using TensorLy [24]. Figure 3 shows the normalized factor matrix representing the
distribution over the topics for each day. We omit the factor matrices for the term representation, and tweet representation of topics due to their high dimensionality. We instead showcase topic summaries (see the appendix for
details). In Figure 3, we primarily observe two different types of temporal behavior among the topics: (i) topics

0.2

0.4

0.6

people, china, health: 1
eating meat, stop eating, god: 2
hoax, trump, new hoax: 3
social distancing, today, believes: 4
china, wuhan, chinese: 5
lockdown, social distancing, people: 6
lockdown, easter, social distancing: 7
trump, cdc, president: 8
mike pence, charge, indiana: 9
li wenliang, chinese doctor, died: 10
china, wuhan, cases: 11
death toll, sars, breaking: 12
pandemic, trump, stay home: 13
pandemic, cases, new: 14
cases, south korea, new cases: 15
washington state, first death, breaking: 16
stay home, please, stay safe: 17
hong kong, strike, border: 18
cruise ship, passengers, japan: 19
south korea, cases, total: 20

02
02-01
02-07
02-13
02-19
03-25
03-02
03-08
03-14
03-20
04-26
04-01
04-07
04-13
04-19
-2
5

Topic

0.0

Date
F IGURE 3. The normalized factor matrix of NCPD with rank 20. Each column of the heatmap
indicates the distribution over the extracted topics for each day.
that persist through a period of time, and (ii) topics that are short-lived (i.e. prominent for 1-3 days). The topics
that persist through a period of time, admit various temporal behaviors. Topics 5 and 11 are persistent, and primarily capture China and Wuhan related COVID-19 events. The topic summaries indicate that topic 11 captures
“new cases" in Wuhan and more generally China, whereas topic 5 captures the events of Pakistan standing with
China that peaks on Feb. 8 and then decreases in prevalence. We observe that topics 6 and 7 primarily capture
lockdown and social distancing events, where in addition topic 7 incorporates the Easter holiday, and topic 6 “government/trump" events. Both topics primarily trend in Apr., with topic 7 peaking on Sunday, Apr. 12 (Easter), and
topic 6 increasing in prevalence in the end of Mar. In addition, we observe that topics 13 and 14 primarily capture
a “pandemic" topic, with topic 13 relating to deaths, whereas topic 14 relates to new cases. Indeed, we observe
in Figure 3 that topic 13 (deaths) gradually replaces topic 14 (cases). Furthermore, a general stay home, urgency
topic (“please", “stay safe", “help"), evolves and experiences a significant peak in mid Mar.
On the other hand, certain topics admit a sparse temporal representation, which indicates that they were
present for only a short period of time. The timing of these learned topics aligns with related events as can be
seen in Table 1. For example, we observe in Feb. that topic 2 is followed by topics 18, 19, 10, and 12 in Figure 3.
7

For instance, topic 2 relating to the beliefs surrounding eating meat and COVID-19 peaks on Feb. 2, and topic 19
related to the passengers of the cruise ship, Diamond Princess, peaks on Feb. 5. Further, political topics such as
topic 3 relating to President Trump’s claims of COVID-19 being the Democrats’ new hoax peaks on Feb. 28, and
topic 9 relating to Vice President Mike Pence’s appointment as chair of the White House Coronavirus Task Force,
peaks on Feb. 26. Other topics related to deaths include topic 10 on the death of the Chinese doctor, Dr. Li Wenliang, which peaks on Feb. 6, and topic 16 related to the first death in Washington State due to COVID-19, which
peaks on Feb. 28. Lastly, topics related to South Korea include topic 15 and 20, which are most prominent in Feb.
20-22. Topic 20 captures “cases/new cases" events related to the outbreak in South Korea more generally, whereas
topic 15 captures the outbreak event in Iran.
90×5000×100
4.6. Online NCPD. For online NCPD (Algorithm 2), the input is a sequence of tensors (Xi )1≤i ≤50 ∈ R+
generated by choosing 100 slices independently and uniformly from the tweets mode of the data tensor X . Rank
r = 20, sparsity regularizer λ = 1, and learning rate β = 0.7 are used. The resulting factor matrices for topics and
their temporal evolution are shown in Figure 4 (see also the timeline in Table 1).

0.1

0.2

0.3

0.4

0.5

china, wuhan, chinese 1
new cases, south korea, hoax 2
china, people, trump 3
cruise ship, people, japan 4
italy, iran, cases 5
meat, eating meat, wuhan 6
hospital, wuhan, china 7
china, chinese, wuhan 8
pakstandswithchina, pompeo, china 9
china, iran, south korea 10
stay home, please, social distancing 11
trump, people, pandemic 12
social distancing, pandemic, please 13
trump, hoax, president 14
people, home, new 15
lockdown, pandemic, stay home 16
trump, people, positive 17
china, wuhan, people 18
china, new, people 19
trump, people, lockdown 20

02
02 01
02 07
02 13
02 19
03 25
03 02
03 08
03 14
03 20
04 26
04 01
04 07
04 13
04 19
-2
5

Topic

0.0

Date
F IGURE 4. The normalized factor matrix for topic prevalence learned via online NCPD with rank
20, learning rate parameter β = 0.7 and regularization λ = 1.
Topics that are present in early-mid Feb. that decrease in prevalence toward the end of Feb. include topics 1,
7, 8 for the outbreak of the virus in Wuhan, topic 4 for the cruise ship passengers in Japan, and topic 10 for the
outbreak in Iran, South Korea, and Japan. Other topics emerge in late Feb. to early Mar. and decrease in prevalence
toward Apr., such as topics 11 and 13 for staying home and social distancing, topic 14 on political terms such as U.S.
President Trump, hoax, and Mike Pence. Lastly, topics such as topics 12, 16, and 20 on lockdown and pandemic
become more prevalent towards the end of Apr. Online NCDP also detects topics that are sharply concentrated in
a short period of time (1-3 days) such as beliefs surrounding eating meat and COVID-19 (topic 6) Pakistan standing
with China (topic 9), topic 5 on the outbreak in Italy which peaked around Feb. 22. Some short-lived topics detected
by NCPD in Figure 3 are not captured by online NCPD, such as Vice President Mike Pence’s appointment as chair
of the White House Coronavirus Task Force, the first COVID-19 death in Washington, and the death of Dr. Li.
4.7. Sequential Learning with Online Algorithms. Online NMF and online NCPD can be used to detect how topics themselves, not only their prevalence, evolve over time. To do this, we partition the data into a sequence of
inputs. For online NMF, the topic matrices are continuously adapted to data from each day by Algorithm 1 (with
8

April

March

February

β = 0.7, minibatches of 50 tweets, and 100 iterations). Topic prevalence within each month is computed with the
30×5000×1000
topic matrix learned at the end of the month. For online NCPD, each tensor Xs ∈ R+
, s = 1, 2, 3 contains
a month’s data. For online NCPD, topics and their prevalences are learned simultaneously from each monthly slice
Xs using Algorithm 2 (subsampling (30, 5000, 100)-tensors from Xs for N = 100 iterations with β = 0.7 and λ = 0.1),
where the prior factor matrices are taken to be the initial factor matrices for the algorithm freshly applied to the
next slice. In Figures 5 and 6, topics are represented as wordclouds, and colorbars show the prevalence of the topics
for each day of the month chronologically.

Topic 1

Topic 2

Topic 3

Topic 4

Topic 5

F IGURE 5. Learned topics of W after each month of online NMF. The heatmap bars indicate topic
prevalence throughout the month (black=low, white=high).
The topics learned in Feb. primarily focus on China, Wuhan, South Korea, Hong Kong, and Trump, while by Apr.,
the topics move away from discussing specific places and focus on general topics such as staying home, new cases
and deaths, testing, lockdowns and social distancing. Further, some short-lived topics that occur in a particular
month are detected in Figure 6, but not elsewhere. For example, topic 2 of Figure 6 in Mar. captures the historic
plunge in oil price and topic 1 in Apr. relates to the the World Health Organization.
Lastly, we compare topic prevalences in each month in Figures 5 and 6. When using online NCPD, the topics
and their prevalences are learned at the same time, and the following month’s topic prevalence is updated from
the previous month’s. Hence is not likely to have both the topic content and prevalence change drastically at the
same time. For example, topic 2 in Figure 6 sharply peaks at around day 5-7 in each month, where topics adapted
significantly from Dr. Li to oil to staying home. On the other hand, when using online NMF, one first learns the
topic representations from each month’s data and then prevalences of those topics throughout the month are
computed. Consequently, while the topics typically change gradually each month due to the continuous learning,
this is not necessarily true for the the post-processed prevalences, as we observe in topic 3 of Figure 5.
4.8. Method Comparison. We find that NCPD and online NCPD are better at learning and identifying topics that
occur over short time periods. On the other hand, NMF and online NMF primarily detect topics that occur over
longer periods of time (at least a week). Thus, for a given day, the focus is typically spread over a larger number of
topics instead of having topics that occur as short intense pulses.
A major advantage of the online methods is reduced memory requirements as the data is processed in slices.
On the other hand NMF and NCPD hold the entire dataset in memory during the learning process. See Table 2 for
a summary of memory requirements.
9

February
March
April
Topic 1

Topic 2

Topic 3

Topic 4

Topic 5

F IGURE 6. Three-month evolution of topics and associated temporal modes sequentially learned
by online NCPD with the history refreshed after each month (with rank 5 and β = 0.7). The
heatmap bars indicate topic prevalence throughout the month (black=low, white=high).
Method
NMF Online NMF NCPD Online NCPD
Memory O(n`T )
O(n`r )
O(n`T )
O(n`0 r T )
TABLE 2. Memory complexity comparison of methods for learning rank r factor matrices, for `
(or `0 subsampled) tweets per T days, and a vocabulary with n terms.
5. C ONCLUSION
We demonstrate NMF, online NMF, NCPD, and online NCPD as dynamic topic modeling techniques that are
able to identify significant topics discussed on Twitter relating to the COVID-19 pandemic and detect changes in
the prevalence of these topics over time. We validate our results using outside news sources, and show that the
methods discover topics trending in news sources, and successfully attribute them to the days they were trending
(see Table 1). We discuss and compare the temporal topic patterns learned through each of these methods. For
large data, online variants, online NMF and online NCPD serve as viable alternatives for learning topics and their
temporal patterns. NCPD and online NCPD appear to better identify topics that experience a lot of interest over
only a few days compared to NMF and online NMF. All methods considered detect major topics that are present
over multiple weeks. Future work includes studying how extracted topics vary when considering various sampling
schemes, and comparing the extracted topics from Twitter text data with other social media platforms.
A CKNOWLEDGMENTS
We thank Jacob Moorman for his contributions to the code used. Molitor, Needell and Rebrova were partially
supported by NSF CAREER DMS #1348721 and NSF BIGDATA #1740325. Elizaveta Rebrova also acknowledges
sponsorship by Capital Fund Management.
A PPENDIX A. T OPIC K EYWORDS
We report summaries of each topic based on the top keywords associated with each topic for NMF, online NMF,
NCPD, and online NCPD in Tables 3 to 6. Each learned topic is represented by a positive linear combination
of terms. Terms with larger values in a particular topic are more significant for that topic and, thus, the terms
10

Topic 1:

people (0.78)

many people (0.06)

people died (0.06)

infected (0.06)

people die (0.04)

Topic 2:

like (0.76)

looks like (0.09)

would like (0.05)

flu (0.05)

look like (0.04)

Topic 3:

health (0.49)

public health (0.23)

world health (0.12)

health care (0.09)

workers (0.07)

Topic 4:

get (0.26)

need (0.21)

time (0.20)

know (0.19)

going (0.14)

Topic 5:

wuhan (0.43)

chinese (0.25)

hospital (0.17)

patients (0.10)

wuhan china (0.06)

Topic 6:

lockdown (0.85)

day (0.06)

police (0.04)

india (0.03)

italy (0.02)

Topic 7:

new cases (0.50)

deaths (0.15)

total (0.14)

confirmed cases (0.13)

reports (0.09)

Topic 8:

social distancing (0.85)

practice social (0.06)

social media (0.04)

practicing (0.03)

distancing measures (0.02)

Topic 9:

us (0.88)

help us (0.04)

let us (0.03)

cdc (0.03)

tell us (0.03)

Topic 10:

trump (0.70)

trump administration (0.09)

response (0.08)

donald trump (0.07)

cdc (0.05)

Topic 11:

stay home (0.66)

stay safe (0.14)

home stay (0.09)

please stay (0.06)

home order (0.05)

Topic 12:

china (0.84)

world (0.07)

communist (0.03)

countries (0.03)

india (0.03)

Topic 13:

cruise ship (0.38)

japan (0.20)

hong kong (0.14)

diamond princess (0.14)

passengers (0.14)

Topic 14:

president (0.52)

president trump (0.20)

realdonaldtrump (0.13)

news (0.10)

president realdonaldtrump (0.05)

Topic 15:

first (0.34)

case (0.27)

first case (0.13)

breaking (0.13)

confirmed (0.12)

Topic 16:

death toll (0.59)

deaths (0.15)

breaking (0.10)

toll rises (0.09)

iran (0.08)

Topic 17:

tested positive (0.62)

tests positive (0.13)

test positive (0.12)

tested negative (0.07)

breaking (0.06)

Topic 18:

united states (0.71)

country (0.10)

cdc (0.08)

president united (0.06)

state (0.05)

Topic 19:

pandemic (0.75)

global pandemic (0.12)

world (0.05)

pandemic response (0.04)

amid pandemic (0.03)

Topic 20:

outbreak (0.50)

spread (0.16)

due (0.15)

help (0.11)

amid outbreak (0.07)

TABLE 3. Topic keywords for each NMF topic.

Topic 1:

help (0.54)

fight (0.18)

spread (0.12)

stop (0.10)

Topic 2:

us (0.81)

let us (0.08)

tell us (0.05)

help us (0.04)

could help (0.06)
join us (0.03)

Topic 3:

stay home (0.61)

stay safe (0.11)

home stay (0.11)

home orders (0.10)

home order (0.08)

Topic 4:

like (0.72)

would (0.12)

looks like (0.06)

get (0.06)

going (0.04)

Topic 5:

president (0.54)

president trump (0.21)

states (0.11)

realdonaldtrump (0.08)

briefing (0.06)

Topic 6:

social distancing (0.74)

practicing (0.08)

guidelines (0.07)

distancing measures (0.07)

years old (0.05)

Topic 7:

time (0.37)

first (0.27)

day (0.16)

death (0.12)

every (0.08)

Topic 8:

cases (0.36)

deaths (0.33)

total (0.13)

new cases (0.10)

confirmed cases (0.08)

Topic 9:

tested positive (0.67)

tests positive (0.13)

test positive (0.07)

died (0.07)

hospital (0.06)

Topic 10:

trump (0.73)

donald trump (0.08)

americans (0.07)

disinfectant (0.06)

response (0.06)

Topic 11:

please (0.41)

need (0.23)

let (0.14)

stayhome (0.12)

know (0.10)

Topic 12:

government (0.43)

news (0.23)

good (0.23)

fake news (0.06)

good news (0.05)

Topic 13:

lockdown (0.88)

end (0.04)

day (0.03)

plan (0.02)

lifted (0.02)

Topic 14:

china (0.52)

world (0.27)

wuhan (0.09)

chinese (0.08)

lab (0.05)

Topic 15:

pandemic (0.84)

everyone (0.03)

amid pandemic (0.03)

Topic 16:

health (0.32)

workers (0.29)

care (0.21)

public health (0.10)

health care (0.09)

Topic 17:

state (0.28)

april (0.27)

india (0.16)

may (0.15)

pm (0.14)

Topic 18:

testing (0.42)

get (0.19)

patients (0.17)

test (0.11)

tests (0.11)

global pandemic (0.07) middle pandemic (0.03)

Topic 19:

new (0.58)

new york (0.22)

new cases (0.09)

york city (0.06)

reports (0.05)

Topic 20:

people (0.74)

many people (0.08)

people died (0.07)

people die (0.06)

people dying (0.05)

TABLE 4. Topic keywords for each online NMF topic with learning rate parameter β = 0.7, minibatches of 50 tweets and 100 inner iterations.

with the largest values provide interpretable descriptions of the topics. Frequently, a bigram and one or both its
components occurs as the top weighted terms for a topic (for example, (“stay", “safe", “stay safe")). To derive more
concise topic summaries, we use the terms with the largest values for each topic, but exclude unigrams that are
included in a bigram whose weight is at least half that of the unigram. We additionally exclude terms that do not
contain any alphabet characters.
11

Topic 1:

people (0.29)

china (0.22)

health (0.17)

outbreak (0.17)

Topic 2:

eating meat (0.55)

stop eating (0.13)

god (0.12)

ji maharaj (0.10)

like (0.15)
sin (0.10)

Topic 3:

hoax (0.55)

trump (0.18)

new hoax (0.10)

called hoax (0.10)

democrats (0.08)

Topic 4:

social distancing (0.83)

today (0.04)

believes (0.04)

practice social (0.04)

currently (0.04)

Topic 5:

china (0.51)

wuhan (0.24)

chinese (0.14)

pakistan (0.06)

pakstandswithchina (0.05)

Topic 6:

lockdown (0.70)

social distancing (0.12)

people (0.07)

government (0.06)

trump (0.06)

Topic 7:

lockdown (0.75)

easter (0.08)

social distancing (0.07)

day (0.06)

stayhome (0.05)

Topic 8:

trump (0.43)

cdc (0.18)

president (0.18)

realdonaldtrump (0.11)

administration (0.10)

Topic 9:

mike pence (0.46)

charge (0.20)

indiana (0.13)

hiv (0.13)

response (0.08)

Topic 10:

li wenliang (0.34)

chinese doctor (0.23)

died (0.17)

dr li (0.16)

warn (0.10)

Topic 11:

china (0.42)

wuhan (0.18)

cases (0.14)

chinese (0.13)

new (0.13)

Topic 12:

death toll (0.37)

sars (0.26)

breaking (0.13)

cases (0.13)

new (0.11)

Topic 13:

pandemic (0.34)

trump (0.23)

stay home (0.16)

deaths (0.14)

new (0.14)

Topic 14:

pandemic (0.30)

cases (0.22)

new (0.18)

us (0.16)

positive (0.15)

Topic 15:

cases (0.32)

south korea (0.29)

new cases (0.15)

reports (0.12)

iran (0.12)

first death (0.23)

breaking (0.13)

health officials (0.12)

died (0.11)

Topic 16: washington state (0.41)
Topic 17:

stay home (0.56)

please (0.14)

stay safe (0.12)

people (0.10)

help (0.08)

Topic 18:

hong kong (0.33)

strike (0.25)

border (0.22)

mainland (0.11)

closure (0.11)

Topic 19:

cruise ship (0.31)

passengers (0.21)

japan (0.18)

quarantined (0.17)

board (0.14)

Topic 20:

south korea (0.35)

cases (0.29)

total (0.12)

new cases (0.12)

korea reports (0.11)

TABLE 5. Topic keywords for NCPD with 20 topics.
Topic 1:

china (0.37)

wuhan (0.25)

chinese (0.17)

outbreak (0.13)

doctor (0.08)

Topic 2:

new cases (0.31)

confirmed cases (0.31)

south korea (0.17)

hoax (0.11)

breaking (0.09)

Topic 3:

china (0.29)

people (0.22)

trump (0.17)

new (0.16)

cases (0.16)

Topic 4:

cruise ship (0.31)

people (0.19)

japan (0.18)

wuhan (0.16)

passengers (0.16)

Topic 5:

italy (0.24)

iran (0.22)

new cases (0.20)

south korea (0.20)

outbreak (0.14)

Topic 6:

eating meat (0.41)

wuhan (0.16)

outside china (0.15)

philippines (0.14)

god (0.13)

Topic 7:

hospital (0.26)

wuhan (0.25)

china (0.19)

strike (0.17)

hong kong (0.13)

Topic 8:

china (0.51)

chinese (0.17)

wuhan (0.15)

tom cotton (0.09)

lab (0.09)

Topic 9:

pakstandswithchina (0.36)

pompeo (0.18)

china (0.18)

pakistan (0.14)

wuhan (0.14)

Topic 10:

china (0.40)

iran (0.17)

south korea (0.15)

cdc (0.14)

japan (0.13)

Topic 11:

stay home (0.39)

please (0.16)

social distancing (0.16)

help (0.15)

people (0.14)

Topic 12:

trump (0.26)

people (0.23)

pandemic (0.19)

us (0.16)

cases (0.15)

Topic 13:

social distancing (0.34)

pandemic (0.22)

please (0.17)

need (0.14)

people (0.13)

Topic 14:

president trump (0.39)

hoax (0.20)

pence (0.14)

realdonaldtrump (0.13)

people (0.13)

Topic 15:

people (0.25)

home (0.19)

new (0.19)

hospital (0.19)

hands (0.17)

Topic 16:

lockdown (0.36)

pandemic (0.26)

stay home (0.14)

people (0.13)

trump (0.11)

Topic 17:

trump (0.25)

people (0.24)

positive (0.17)

tested (0.17)

like (0.16)

Topic 18:

china (0.38)

wuhan (0.18)

people (0.16)

cases (0.15)

chinese (0.14)

Topic 19:

china (0.43)

new (0.15)

people (0.14)

cases (0.14)

wuhan (0.13)

Topic 20:

trump (0.22)

people (0.21)

lockdown (0.21)

pandemic (0.21)

us (0.15)

TABLE 6. Topic keywords for online NCPD with 20 topics learning rate parameter β = 0.7 and
regularization λ = 1.
R EFERENCES
[1] Miju Ahn, Nicole Eikmeier, Jamie Haddock, Lara Kassab, Alona Kryshchenko, Kathryn Leonard, Deanna Needell, RWMA Madushani,
Elena Sizikova, and Chuntian Wang. On large-scale dynamic topic modeling with nonnegative cp tensor decomposition. arXiv preprint
arXiv:2001.00631, 2020.
12

[2] Sanaz Bahargam and Evangelos Papalexakis. A constrained coupled matrix-tensor factorization for learning time-evolving and emerging
topics. arXiv preprint arXiv:1807.00122, 2018.
[3] BBC. Li wenliang: Coronavirus kills chinese whistleblower doctor, 2020.
[4] Steven Bird, Ewan Klein, and Edward Loper. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit.
" O’Reilly Media, Inc.", 2009.
[5] David M Blei and John D Lafferty. Dynamic topic models. In Proceedings of the 23rd International Conference on Machine Learning, pages
113–120, 2006.
[6] Svenja Boberg, Thorsten Quandt, Tim Schatto-Eckrodt, and Lena Frischlich. Pandemic populism: Facebook pages of alternative news
media and the corona crisis–a computational content analysis. arXiv preprint arXiv:2004.02566, 2020.
[7] Ioan Buciu. Non-negative matrix factorization, a new tool for feature extraction: Theory and applications. International Journal of Computers, Communications and Control, 3(3):67–74, 2008.
[8] J Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling via an n-way generalization of “eckartyoung” decomposition. Psychometrika, 35(3):283–319, 1970.
[9] Emily Chen, Kristina Lerman, and Emilio Ferrara. Tracking social media discourse about the covid-19 pandemic: Development of a public
coronavirus twitter data set. JMIR Public Health and Surveillance, 6(2):e19273, 2020.
[10] Huiyuan Chen and Jing Li. Modeling relational drug-target-disease interactions via tensor factorization with multiple web sources. In The
World Wide Web Conference, pages 218–227, 2019.
[11] Liangzhe Chen, KSM Tozammel Hossain, Patrick Butler, Naren Ramakrishnan, and B Aditya Prakash. Flu gone viral: Syndromic surveillance of flu on twitter using temporal topic models. In 2014 IEEE International Conference on Data Mining, pages 755–760. IEEE, 2014.
[12] CNBC. Global death toll for new Coronavirus soars past 800 to overtake sars, 2020.
[13] Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. Least angle regression. The Annals of Statistics, 32(2):407–499, 2004.
[14] Emilio Ferrara. What types of covid-19 conspiracies are populated by twitter bots? First Monday, 25(6), May 2020.
[15] Jon Green, Jared Edgerton, Daniel Naftel, Kelsey Shoub, and Skyler J Cranmer. Elusive consensus: Polarization in elite communication on
the covid-19 pandemic. Science Advances, 6(28):eabc2717, 2020.
[16] Naiyang Guan, Dacheng Tao, Zhigang Luo, and Bo Yuan. Online nonnegative matrix factorization with robust stochastic approximation.
IEEE Transactions on Neural Networks and Learning Systems, 23(7):1087–1099, 2012.
[17] Richard A Harshman et al. Foundations of the parafac procedure: Models and conditions for an "explanatory" multimodal factor analysis.
1970.
[18] Jiajun Hu, Xiaobing Sun, David Lo, and Bin Li. Modeling the evolution of development topics using dynamic topic models. In 2015 IEEE
22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER), pages 3–12. IEEE, 2015.
[19] Tomoharu Iwata, Takeshi Yamada, Yasushi Sakurai, and Naonori Ueda. Online multiscale dynamic topic models. In Proceedings of the 16th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 663–672, 2010.
[20] Shatha Jaradat and Mihhail Matskin. On dynamic topic models for mining social media. In Emerging Research Challenges and Opportunities in Computational Social Network Analysis and Mining, pages 209–230. Springer, 2019.
[21] K Kakimoto, H Kamiya, T Yamagishi, T Matsui, M Suzuki, and T Wakita. Initial investigation of transmission of covid-19 among crew
members during quarantine of a cruise ship — yokohama, japan, february 2020. MMWR Morb Mortal Wkly Rep, 69:312–313, 2020.
[22] Bennett Kleinberg, Isabelle van der Vegt, and Maximilian Mozes. Measuring emotions in the covid-19 real world worry dataset. arXiv
preprint arXiv:2004.04225, 2020.
[23] Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM review, 51(3):455–500, 2009.
[24] Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. Tensorly: Tensor learning in python. Journal of Machine Learning
Research, 20(26):1–6, 2019.
[25] Da Kuang, Jaegul Choo, and Haesun Park. Nonnegative matrix factorization for interactive topic modeling and document clustering. In
Partitional Clustering Algorithms, pages 215–243. Springer, 2015.
[26] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):788, 1999.
[27] Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Y Ng. Efficient sparse coding algorithms. In Advances in neural information processing
systems, pages 801–808, 2007.
[28] Hanbaek Lyu, Deanna Needell, and Laura Balzano. Online matrix factorization for markovian data and applications to network dictionary
learning, 2019.
[29] Julien Mairal. Stochastic majorization-minimization algorithms for large-scale optimization. In Advances in Neural Information Processing
Systems, pages 2283–2291, 2013.
[30] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and sparse coding. Journal of
Machine Learning Research, 11(Jan):19–60, 2010.
[31] NBCNews. Trump calls Coronavirus democrats’ ’new hoax’, 2020.
[32] NBCNews. Washington state man becomes first U.S. death from Coronavirus, 2020.
[33] Catherine Ordun, Sanjay Purushotham, and Edward Raff. Exploratory analysis of covid-19 tweets using topic modeling, umap, and digraphs. arXiv preprint arXiv:2005.03082, 2020.
[34] Michael J Paul and Mark Dredze. You are what you tweet: Analyzing twitter for public health. In Fifth international AAAI conference on
weblogs and social media. Citeseer, 2011.
[35] Michael J Paul and Mark Dredze. Discovering health topics in social media using topic models. PloS one, 9(8):e103408, 2014.
[36] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas,
A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning
Research, 12:2825–2830, 2011.
[37] Politico. Trump puts Pence in charge of Coronavirus response, 2020.
13

[38] Ankan Saha and Vikas Sindhwani. Learning evolving and emerging topics in social media: A dynamic nmf approach with temporal regularization. In Proceedings of the fifth ACM International Conference on Web Search and Data Mining, pages 693–702, 2012.
[39] Amnon Shashua and Tamir Hazan. Non-negative tensor factorization with applications to statistics and computer vision. In Proceedings
of the 22nd international conference on Machine learning, pages 792–799, 2005.
[40] Statista. Number of new coronavirus (covid-19) cases in south korea from january 20 to august 21, 2020, 2020.
[41] Christopher Strohmeier, Hanbaek Lyu, and Deanna Needell. Online nonnegative tensor factorization and cp-dictionary learning for markovian data. arXiv preprint arXiv:2009.07612, 2020.
[42] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological),
58(1):267–288, 1996.
[43] Abraham Traoré, Maxime Berar, and Alain Rakotomamonjy. Non-negative tensor dictionary learning. 2018.
[44] TwitterIR.
Q2’
2020
shareholder
letter.
https://s22.q4cdn.com/826641620/files/doc_financials/2020/q2/
Q2-2020-Shareholder-Letter.pdf, July 2020.
[45] Aad W Van der Vaart. Asymptotic Statistics, volume 3. Cambridge University Press, 2000.
[46] Isabelle Van der Vegt and Bennett Kleinberg. Women worry about family, men about the economy: Gender differences in emotional
responses to covid-19. arXiv preprint arXiv:2004.08202, 2020.
[47] Yining Wang, Hsiao-Yu Tung, Alexander J Smola, and Anima Anandkumar. Fast and guaranteed tensor decomposition via sketching. In
Advances in Neural Information Processing Systems, pages 991–999, 2015.
[48] WHO. Timeline of who’s response to covid-19, 2020.
[49] Wire. ’no meat, no coronavirus’ makes no sense, 2020.
[50] Wei Xu, Xin Liu, and Yihong Gong. Document clustering based on non-negative matrix factorization. In Proceedings of the 26th annual
international ACM SIGIR conference on Research and development in informaion retrieval, pages 267–273, 2003.
[51] Kai-Cheng Yang, Christopher Torres-Lugo, and Filippo Menczer. Prevalence of low-credibility information on twitter during the covid-19
outbreak. arXiv preprint arXiv:2004.14484, 2020.
[52] Hui Yin, Shuiqiao Yang, and Jianxin Li. Detecting topic and sentiment dynamics due to covid-19 pandemic using social media. arXiv
preprint arXiv:2007.02304, 2020.
[53] Renbo Zhao, Vincent Tan, and Huan Xu. Online nonnegative matrix factorization with general divergences. In Artificial Intelligence and
Statistics, pages 37–45, 2017.
[54] Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. Comparing twitter and traditional media
using topic models. In European conference on information retrieval, pages 338–349. Springer, 2011.
[55] Caleb Ziems, Bing He, Sandeep Soni, and Srijan Kumar. Racism is a virus: Anti-asian hate and counterhate in social media during the
covid-19 crisis, 2020.

14

