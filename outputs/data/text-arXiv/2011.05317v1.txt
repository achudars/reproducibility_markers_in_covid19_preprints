Noname manuscript No.
(will be inserted by the editor)

Explainable COVID-19 Detection Using Chest CT Scans
and Deep Learning

arXiv:2011.05317v1 [eess.IV] 9 Nov 2020

Hammam Alshazly1,2 · Christoph Linse1 · Erhardt Barth1 · Thomas
Martinetz1

Received: date / Accepted: date

Abstract This paper explores how well deep learning models trained on chest CT images can diagnose
COVID-19 infected people in a fast and automated process. To this end, we adopt advanced deep network architectures and propose a transfer learning strategy using custom-sized input tailored for each deep architecture to achieve the best performance. We conduct extensive sets of experiments on two CT image datasets,
namely the SARS-CoV-2 CT-scan and the COVID19CT. The obtained results show superior performances
for our models compared with previous studies, where
our best models achieve average accuracy, precision,
sensitivity, specificity and F1 score of 99.4%, 99.6%,
99.8%, 99.6% and 99.4% on the SARS-CoV-2 dataset;
and 92.9%, 91.3%, 93.7%, 92.2% and 92.5% on the
COVID19-CT dataset, respectively. Furthermore, we
apply two visualization techniques to provide visual
explanations for the models’ predictions. The visualizations show well-separated clusters for CT images of
COVID-19 from other lung diseases, and accurate localizations of the COVID-19 associated regions.
Keywords Coronavirus · COVID-19 detection ·
SARS-CoV-2 · Chest CT images · visual explanations
1 Introduction
Coronavirus disease 2019 (COVID-19) is an infectious
disease caused by the new coronavirus named severe
acute respiratory syndrome coronavirus-2 (SARS-CoV2). The virus is highly contagious and can be trans1

Institute for Neuro- and Bioinformatics, University of
Lübeck, 23562 Lübeck, Germany
2
Department of Mathematics, Faculty of Science, South Valley University, Qena 83523, Egypt

mitted by direct and/or indirect contact with infected
people through respiratory droplets when they sneeze,
cough or even talk [1–3]. The real-time polymerase
chain reaction (RT-PCR) test is the standard reference
for confirming COVID-19, and with the rapid increment
in the number of infected people, most of the countries
are encountering shortage in testing kits. Moreover, RTPCR testing has high turnaround times and a high false
negative rate [4]. Thus, it is highly desirable to consider
other testing tools for identifying COVID-19 contaminated patients to isolate them and mitigate the pandemic impact on the life of many people.
Chest computed tomography (CT) is an applicable
supplement to RT-PCR testing and has been playing a
role in screening and diagnosing COVID-19 infections.
In recent studies [5, 6], the authors manually examined
chest CT scans for more than a thousand patients and
confirmed the usefulness of chest CT scans for diagnosing COVID-19 with high sensitivity rates. In some
cases, the patients initially had a negative PCR test,
however, confirmation was based on their positive CT
findings. Moreover, chest CT screening has been recommended when patients show symptoms compatible
with viral infections, but the result of their PCR test
is negative [5, 7]. Nevertheless, diagnosing COVID-19
from chest CT images by radiologists takes time, and
manually checking every CT image might not be feasible in emergency cases. Therefore, there is a need for
automated detection tools that exploit the recent deep
learning techniques and CT images to expedite the process and provide consistent performance.
This paper adopts the most advanced deep Convolutional Neural Network (CNN) architectures, which
are top performers in the ImageNet recognition challenge [8], and presents a comprehensive study for detecting COVID-19 based on CT images. We explore

2

CNN models that have different architectural designs
and varying depths to obtain the best detection performance. Even though we conduct our experiments on
two of the largest CT scan datasets available for research, their size is still insufficient to train deep networks from scratch. An effective strategy to overcome
this limitation is to use transfer learning [9], where deep
networks trained on visual tasks are utilized to initialize networks for different but related target tasks. Most
of the published works that applied transfer learning
strategies using the ImageNet [10] pretrained networks
followed the strict fixed-sized input for each deep network and resized their target images accordingly. We
argue that resizing images with different aspect ratios
to match a specific resolution can distort the image
severely. We address the problem by placing the images into a fixed-sized canvas determined specifically
for each CNN architecture, where the aspect ration of
the original image is preserved. This has proven to be a
less violating procedure and more effective to achieve
better results as reported in [11]. Moreover, we utilize the layer-wise adaptive large batch optimization
technique called LAMB [12], which has demonstrated
better performance and convergence speed for training
deep networks. The performance of the models is measured quantitatively using accuracy, precision, sensitivity, specificity, F1-score and the confusion matrix for
each model. Our obtained results indicate the effectiveness of our strategy to achieve state-of-the-art results
on the considered datasets.
In order to provide better explainability of the deep
models and making them more transparent we apply two visualization techniques. The first approach
is the t-distributed Stochastic Neighboring Embedding
(t-SNE) [13], which is a dimensionality reduction and
visualization technique for visualizing clusters of instances in a high-dimensional space. The obtained visualizations of the t-SNE embeddings show well-separated
clusters representing CT images for COVID-19 and
Non-COVID-19 cases. The second approach is the
Gradient-weighted Class Activation Mapping (GradCAM) [14], which is a visualization technique for CNNbased models. It provides high-resolution and classdiscriminative visualizations that localize the important image regions considered for the model prediction.
The Grad-CAM visualizations show how accurately our
models localize the COVID-19 associated regions. Overall, this paper exhibits the following contributions:
– A comparative experimental study is conducted on
how well advanced deep CNNs trained on chest CT
images can identify COVID-19 cases. To this end,
we experiment with 12 deep networks that have
different architectural designs and varying depths,

Alshazly et al.

and provide quantitative and qualitative analyses.
– We propose a domain adaptation strategy to
fine-tune deep networks using custom-sized inputs determined specifically for each architecture,
and utilize the LAMB optimizer for training the
networks. Our experimental results prove the
effectiveness of our optimization configurations to
obtain state-of-the-art performance on the considered CT image datasets. Our best models achieve
an average accuracy of 99.4% and 92.9%, and
average sensitivity rates of 99.8% and 93.7% on the
largest datasets of CT images available for research.
– We provide visualizations of the extracted features
from different models to understand how deep
networks represent CT images in the feature space.
The visualizations show well-separated clusters
representing the CT images of the different classes,
which indicates that our models have learned
discriminative features to distinguish CT images of
different cases.
– We show discriminative localizations and visual explanations obtained by our models for detecting
COVID-19 associated regions in CT images as annotated by expert radiologists.
The rest of the paper is structured as follows. We review
the related work in the next section. The deep CNN architectures are described in Section 3 and the methodology to learn discriminative features in Section 4. The
experimental settings and the obtained results are reported in Section 5. Finally, we draw the main conclusion in Section 6.
2 Related Work
This section highlights some relevant work that adopted
deep CNNs for building computer-aided diagnosis
(CADs) systems based on medical images. The authors in [15] employed different deep CNN architectures, which were pretrained on the ImageNet dataset
[10], and fine-tuned them on specific CT scans for
thoraco-abdominal lymph node detection and interstitial lung disease classification. Their study indicated
the effectiveness of deep CNNs for CADs problems even
when training data is limited. In [16], the authors proposed the CheXNet model to detect different types of
pneumonia from chest X-ray images. The model consisted of 121-layers and was trained on a large dataset
that contained over 100,000 X-ray images for 14 different thoracic diseases. The model showed outstanding

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

detection performance at the level of practicing radiologists.
In the context of the COVID-19 pandemic, extensive research has been conducted to develop automated
image-based COVID-19 detection and diagnosis systems [17–21]. We hereafter review the proposed approaches for reliable detection systems based on chest
X-ray and CT-scan imaging modalities. These techniques follow either one of two main paradigms.
On one hand, new deep network architectures have
been developed and tailored specifically for detecting
and recognizing COVID-19. COVID-Net [22] represents
one of the earliest convolutional networks designed for
detecting COVID-19 cases automatically from X-ray
images. The performance of the network showed an acceptable accuracy of 83.5% and a high sensitivity of
100% for COVID-19 cases. Hasan et al. [23] proposed
a CNN-based network named Coronavirus Recognition
Network (CVR-Net) to automatically detect COVID19 cases from radiography images. The network was
trained and evaluated on datasets with X-ray and CT
images. The obtained results showed varying accuracy
scores based on the number of classes in the underlying
X-ray image dataset and an average accuracy of 78%
for the CT image dataset. Further modifications were
applied to COVID-Net to improve its representational
ability for one specific image modality and to make the
network computationally more efficient as in [24].
On the other hand, some deep networks have been
proposed for similar tasks of automated detection and
recognition of COVID-19 cases, however, these networks are based on well-designed and existing CNN
architectures, such as ReseNet [25], Xception [26] and
Capsule Networks [27, 28]. The authors in [29] adopted
transfer learning from deep networks for automatic
COVID-19 detection based on X-ray images from patients with bacterial and COVID-19 pneumonia and
normal cases. They reported the best results for the
two- and three-class classification tasks with an accuracy of 98.75% and 93.48, respectively. Minaee et al. [30]
applied transfer learning by fine-tuning four popular
pretrained CNNs to identify COVID-19 infection. They
experimented on a prepared X-ray image dataset with
5,000 chest X-rays. Their best approaches obtained an
average sensitivity and specificity of 98% and 90%, respectively. Brunese et al. [31] utilized transfer learning
with a pretrained VGG-16 network [32] to automatically detect COVID-19 from chest X-rays. On a combined dataset from different sources with X-rays for
healthy and pulmonary disease they reported an average accuracy of 97%.
Zhou et al. [33] highlighted the importance of deep
learning techniques and chest CT images for differenti-

3

ating COVID-19 pneumonia and influenza pneumonia.
The study was conducted on CT images for confirmed
COVID-19 patients from different hospitals in china.
Their study proved the potential of accurate COVID-19
diagnosis from CT images and the effectiveness of their
proposed classification scheme to differentiate between
the two types of pneumonia. DeepPneumonia [34] was
developed to identify COVID-19 cases (88 patients),
bacterial pneumonia (100 patients) and healthy cases
(86 subjects) based on CT images. The model achieved
an accuracy of 86.5% for differentiating bacterial and
viral (COVID-19) pneumonia and an accuracy of 94%
for distinguishing COVID-19 and healthy cases. The
authors in [35] used CT images to classify COVID-19
infected patients from Non-COVID-19 people utilizing
a pretrained DenseNet201 network. The model achieved
an accuracy of 96.25%.
Very few studies employed handcrafted feature extraction methods and conventional classifiers. In [36],
texture features were extracted from X-ray images using popular texture descriptors. The features were combined with those extracted from a pretrained Inception
V3 [37] using different fusion strategies. Then, various
classifiers were used to differentiate between normal Xrays and different types of pneumonia. The best classification scheme achieved an F1-score of 83%. In [38], the
authors proposed an approach to differentiate between
positive and negative COVID-19 cases based on CT
scans. Different texture features were extracted from
CT images with Gabor filters, and then support vector
machines were trained for classification. Their proposed
scheme achieved an average accuracy of 95.37% and a
sensitivity of 95.99%.
The discussion about related works indicates the
prominence of deep learning methods to address the
task of automated detection of COVID-19. We build
on the existing body of published work and adopt advanced deep networks for detecting COVID-19 using
CT images. We conduct experiments on two of the
largest CT image datasets and compare the performance of 12 deep networks using standard evaluation
metrics. We also provide visualizations for better explainability of the resulting models.
3 Deep Network Architectures
This section describes the deep CNN architectures employed to identify COVID-19 using chest CT scans.
These networks are state-of-the-art deep models for
image recognition. They differ in their architectural
design and were proposed in order to achieve better
representational power or to reduce their computational complexity. In this work we consider the most

4

Alshazly et al.

Input

Input
1x1

Squeeze

1x1
1x1

pool

3x3

1x1

3x3

1x1

3x3

1x1

3x3

Concat

Expand
Fig. 2: A variant of the Inception module used in InceptionV3 architecture.
Fig. 1: The fire module used in SqueezeNet.

advanced networks such as SqueezeNet [39], Inception [37], ResNet [40], ResNeXt [41], Xception [42],
ShuffleNet [43] and DenseNet [44].

3.1 SqueezeNet
The SqueezeNet architecture is a deep CNN proposed
for computer vision tasks with the main concerns on
efficiency (having fewer parameters and smaller model
size) [39]. The basic building block for the SqueezeNet
architecture is the fire module depicted in Figure 1.
The module incorporates the squeeze phase and the expand phase. The squeeze phase applies a set of 1 × 1
filters followed by a ReLU activation. The number of
learned squeeze filters is always smaller than the size
of the input volume. Consequently, the squeeze phase
can be considered as a dimensionality reduction process, and at the same time it captures the pixel correlations across the input channels. The output of the
squeeze phase is fed into the expand phase, in which a
combination of 1 × 1 and 3 × 3 convolutions are learned.
The larger 3 × 3 filters are used to capture the spatial
correlation amongst pixels. The outputs of the expand
phase are concatenated across the channel dimension
and then evaluated by a ReLU activation.
The original paper proposed using n, 1 × 1; and n,
3 × 3 filters in the expand phase, where n is 4× larger
than number of filters used in the squeeze phase. The
entire SqueezeNet architecture is constructed by stacking conventional convolution layers, max-pooling, fire
modules, and ends with an average pooling layer. The
model has no fully connected layers. For more details
about the number of fire modules for each stage, their
order, and number of squeeze and expand filters for the
different stages, see [39].

3.2 Inception
The Inception network is a deep convolutional architecture introduced as GoogLeNet (Inception V1) in 2014
by Szegedy et al. [45]. The architecture has been refined
in various ways such as adding batch normalization layers to accelerate training (Inception V2 [46]), and factorizing convolutions with larger spatial filters for computational efficiency (Inception V3 [37]). We adopt the
Inception V3 model due to its outstanding performance
in image recognition and object localization.
The fundamental building block for all Inceptionstyle networks is the Inception module of which several
forms exist. Figure 2 shows one variant of the Inception
module that is used in the Inception V3 model. The
module accepts an input and then branches into four
different paths each performing a specific set of operations. The input passes through convolutional layers
with different kernel sizes (1 × 1 and 3 × 3) as well as
a pooling operation. Applying different kernel sizes allows the module to capture complex patterns at different scales. The outputs of all branches are concatenated
channel-wise.
The overall architecture of the Inception V3 network
is composed of conventional 3 × 3 convolutional layers
at the early stages of the network, where some of these
layers are followed by max-pooling operations. Subsequently a stack of various Inception modules is applied.
These modules have different designs with respect to
the number of applied filters, filter sizes, depth of the
module after symmetric or asymmetric factorization of
larger convolutions, and when to expand the filter bank
outputs. The last Inception module is followed by an
average-pooling operation and a fully connected layer.

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

3.3 ResNet
Deep Residual Networks (ResNet) proposed by He et al.
in [40], represent a family of extremely deep CNN architectures that won the 2015 Large Scale Visual Recognition Challenge (ILSVRC-2015) for image recognition,
object detection and localization [8]. The winning network is composed of 152 layers, which confirms the beneficial impact of network depth on visual representations. However, two major problems are encountered
when training networks of increasing depth; vanishing
gradients and performance degradation. The authors
addressed the problems by adding skip connections to
prevent information loss as the network gets deeper.
The cornerstone for constructing deep residual networks is the residual module of which two variants are
depicted in Figure 3. The left path of the residual module in Figure 3 (a) is composed of two convolutional layers, which apply 3 × 3 kernels and preserve the spatial
dimensions. Batch normalization and ReLU activation
are also applied. The right path is the skip connection
where the input is added to the output of the left path.
This variant is used in the ResNet18 model. Another
variant of the residual module named the bottleneck
residual module is depicted in Figure 3 (b), in which the
input signal also passes through two branches. However,
the left path performs a series of convolutions using 1×1
and 3 × 3 kernel sizes, along with batch normalization
and ReLU activation. The right path is the skip connection, which connects the module’s input to an addition
operation with the output of the left path. This variant
is utilized in ResNet50 and ResNet101 models.
A deep residual network is constructed by stacking
multiple residual modules on top of each other along
with other conventional convolution and pooling layers. For our experiments we adopt three variants of
ResNet, the ResNet18, ResNet50 and ResNet101 models. The full configurations and overall structure about
each model are given in [40].
3.4 ResNeXt
The ResNeXt architecture proposed in [41] is a deep
CNN model constructed by stacking residual building
blocks of identical topology in a highly modularized
fashion. Its simple design shares similarities with the
ResNet architecture. ResNeXt also exploits the splittransform-merge strategy of the Inception module in
an easy and extendable manner. The ResNeXt building block uses an identical set of transformations in
every branch and hence allows the number of branches
to be investigated as an independent hyperparameter.
ResNeXt refers to the size of the set of transformations

5
Input

Input

1x1

3x3

BN, ReLU

BN, ReLU

3x3

3x3

BN, ReLU

1x1

BN

+

BN

+

ReLU

(a)

ReLU

(b)

Fig. 3: The basic building block residual module employed in ResNet18 (a), and the bottleneck residual
module used in ResNet50 and ResNet101 (b), both as
introduced in [40].

as the cardinality, which represents an important dimension for improving the network’s representational
power. Figure 4 depicts a ResNeXt building block with
a cardinality of 32. Each branch applies the same set
of transformations and their outputs are aggregated by
summation.

Input
256

256, 1x1, 4

256, 1x1, 4

4, 3x3, 4

4, 3x3, 4

4, 1x1, 256

4, 1x1, 256

...
32 paths
in total

...

256, 1x1, 4

4, 3x3, 4

4, 1x1, 256

+
+
256

Fig. 4: A ResNeXt building block with cardinality of
32 [41].

The entire network is constructed by stacking
ResNeXt blocks along with other conventional convolution and pooling layers. For our experiments we implement two ResNeXt models, the 50-layer and the 101layer networks. In a similar manner as their ResNet
counterparts, ResNeXt models use RGB-inputs of size
224×224. However, we found an input size of 349×253,
similar to the ResNet models, achieves the best performance on the considered datasets.

6

Alshazly et al.

3.5 Xception
Xception is a deep CNN architecture proposed in [42]. It
is inspired by the Inception architecture and utilizes the
residual connections proposed in ResNet models [40].
However, it replaces the Inception modules with depthwise separable convolution layers. A depthwise separable convolution consists of a depthwise convolution
(spatial convolution of 3 × 3, 5 × 5, etc.) performed over
each channel of an input to map the spatial correlations,
followed by a pointwise convolution (1 × 1) to map the
cross-channel correlations.
The Xception architecture depends entirely on
depthwise separable convolution layers with a strong
assumption that spatial correlations and cross-channel
correlations can be mapped separately. The network
consists of 36 convolutional layers structured into 14
modules. All modules have residual connections except
for the first and last modules. The reader is referred to
[42] for a complete description of the model specification.
Due to its superior performance in vision tasks, we
adopt the Xception model in our experiments. Even
though the original model uses an RGB-input of size
299 × 299, we found that an input size of 327 × 231
obtains the best results.
3.6 ShuffleNet
ShuffleNet is a very computationally-efficient CNN architecture that is mainly designed for mobile devices
with constrained computational power [43, 47]. The architecture introduces two important operations to significantly reduce the computational cost while maintaining accuracy. The first operation is pointwise group
convolutions, which can reduce the computational complexity of the 1 × 1 convolutions. The second operation
consists of shuffling the channels, which assists the information flow across feature channels.
The cornerstone of the ShuffleNet model is the ShuffleNet unit depicted in Figure 5. It is a bottleneck residual module in which the 3 × 3 convolutional layer is replaced by a 3 × 3 depthwise separable convolution as in
[42]. Also, the first 1 × 1 convolutional layer is replaced
by a pointwise group convolution followed by a channel
shuffle operation. The second pointwise group convolutional layer is used to retrieve the channel dimension to
match the left path of the unit. The overall ShuffleNet
network is composed of a stack of these units grouped
into three different stages along with other conventional
convolution and pooling layers.
In this study we adopt the recent variant of the ShuffleNet architecture. The original model uses an RGB-

Input

1x1 GConv
BN, ReLU
Channel Shuffle

3x3 DWConv
BN
1x1 GConv
BN

+

ReLU

Fig. 5: The building unit of the ShuffleNet architecture.

input of 224 × 224, however, we found that an input
resolution of 321 × 225 works better for the considered
datasets.

3.7 DenseNet
Densely
Connected
Convolutional
Networks
(DenseNets) are a class of CNN architectures introduced in [44] with several compelling characteristics.
They alleviate the vanishing gradients problem, foster
feature reuse, achieve high performance, consolidate
feature propagation, and are computationally efficient.
DenseNets modify the shortcut connections from
ResNet by concatenating the output of the convolutions instead of summing them up. So, the input to the
next layer will be the feature maps of all the preceding
layers.
Figure 6 shows a 3-layer Dense block where each
layer performs a set of batch normalization (BN), ReLU
activation and 3 × 3 Convolution operations. Previous
feature maps are concatenated and presented as the
input to a layer, which then generates k feature maps.
k is a newly introduced hyper-parameter, denoted as
the growth rate. Thus, if the input to layer x0 is k0 ,
then the number of feature maps at the end of a 3layer Dense block is 3 × k + k0 . To prevent the number
of feature maps from increasing too rapidly, DenseNet
introduces a bottleneck layer with 1×1 convolution and
4 × k filters. To tackle the difference in the feature map
sizes when transitioning from a large feature map to a
smaller one, DenseNet applies a transition layer made
of 1 × 1 convolution and average pooling.

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

Input

x0
BN, ReLU, Conv2D

H1

x1
Concat

BN, ReLU, Conv2D

H2

x2
Concat

BN, ReLU, Conv2D

H3

x3

7

egy only the weights of some newly added layers are
optimized during training, while in the second strategy all the weights are optimized for the new task.
Here, we consider fine-tuning as a more effective strategy that outperforms feature extraction and achieves
better performance. As our pretrained networks explicitly require an RGB-input, we assign identical values
to the R, G and B channels. Since the CT images in
the two datasets have varying spatial sizes, the images
need to be scaled to match the target input size. One
strategy to unify images with different aspect ratios involves stretching or excessive cropping. We opted for a
different, less violating procedure and embed the image into a fixed-sized canvas. The aspect ratio of the
original image is not altered and padding is applied to
match the target shape.

Concat

y0

Fig. 6: A 3-layer Dense block in DenseNet. The input
to each layer is all the previous feature maps.

A deep DenseNet is constructed by stacking multiple Dense blocks with transition layers. Conventional
convolution and pooling layers are used at the beginning of the network. Eventually the output is pooled by
Global average pooling, flattened and passed to a softmax classifier. For our study we experiment with three
variants of DenseNet, the 121-layer, 169-layer and 201layer architectures. The original models use an RGBinput of 224×224, however, we found that an input size
of 349 × 253 achieves better results for images from the
used datasets.
Table 1 summarizes the important characteristics
of the adopted deep CNN models. This includes the
square-sized input for each network, our proposed
custom-sized input, trainable parameters in millions,
number of layers and the model size in megabytes.

4 Transfer Learning
Transfer learning is an effective representation learning approach in which the networks trained on abundant amount of images (millions) are used to initialize
the networks for tasks for which data is scarce (a few
hundreds or thousands of images). In the context of
deep learning there are two common strategies to apply transfer learning from pretrained networks: feature
extraction and fine-tuning [48, 49]. In the first strat-

5 Experiments and Results
This section presents our experimental setup and extensive experiments to show the efficacy of our fine-tuned
networks. First, we describe the CT image datasets.
Second, we state the experimental settings and performance evaluation metrics. Third, we discuss the obtained results of different models on each dataset. Finally, we apply two visualization methods to facilitate
interpretation of the results and to localize the COVID19 associated regions.

5.1 Datasets
SARS-CoV-2 CT Scan dataset [50]: The dataset
was collected from hospitals of Sao Paulo, Brazil, with
a total of 2482 CT scans acquired from 120 patients of
both genders. It is composed of 1252 scans for patients
infected with SARS-CoV-2 and 1230 scans for patients
infected with other lung diseases. The CT scans have
varying spatial sizes between 119 × 104 and 416 × 512,
and are available in PNG format. CT scans from this
dataset are shown in Figure 7.
COVID19-CT dataset [51]: The dataset consists
of a total of 746 CT images. There are 349 CT images of patients with COVID-19 and 397 CT images
showing Non-COVID-19, but other pulmonary diseases.
The positive CT images were collected from preprints
about COVID-19 on medRxiv and bioRxiv, and they
feature various manifestations of COVID-19. Since the
CT images were taken from different sources, they have
varying sizes between 124 × 153 and 1485 × 1853. Figure 8 shows example CT images from the COVID19-CT
dataset.

8

Alshazly et al.

Table 1: Characteristics of the deep CNN architectures considered for this work.
Model
SqueezeNet
ShuffleNet
ResNet18
ResNet50
ResNet101
ResNeXt50
ResNeXt101
InceptionV3
Xception
DenseNet121
DenseNet169
DenseNet201

Default input size
227 × 227
224 × 224
224 × 224
224 × 224
224 × 224
224 × 224
224 × 224
299 × 299
299 × 299
224 × 224
224 × 224
224 × 224

Model characteristics
Custom input size Layers Parameters (M)
335 × 255
18
0.73
321 × 225
51
0.34
349 × 253
18
11.17
349 × 253
50
23.51
349 × 253
101
42.50
349 × 253
50
22.98
349 × 253
101
86.74
331 × 267
48
21.79
327 × 231
37
20.81
349 × 253
121
6.95
349 × 253
169
12.48
349 × 253
201
18.09

Model size (MB)
3.0
1.5
44.8
94.3
170.6
92.3
347.9
87.4
83.5
28.3
50.8
73.6

Fig. 7: Examples of chest CT scans from the SARS-CoV-2 dataset. The first row represents scans diagnosed with
COVID-19, whereas the second row represents Non-COVID-19 but other lung diseases.

Fig. 8: Examples of chest CT images from the COVID19-CT dataset. The first row represents images diagnosed
with COVID-19, whereas the second row represents Non-COVID-19 cases.
5.2 Experimental Settings
To assess the performance of our models we perform
five-fold cross-validation. The final performance of the
models is computed by averaging the obtained values
from the five networks on their test fold respectively.
Data augmentation methods are implemented to effectively increase the amount of training samples for
improved generalization. Affine transformations like ro-

tation and shearing turned out to have a worsening effect on performance, so we excluded this type of augmentations. More augmentation steps include cropping,
adding blur with a probability of 25%, adding a random amount of Gaussian noise, changes in brightness
and contrast and random horizontal flipping. Finally,
the images are normalized according to the ImageNet
dataset.

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

We follow a set of optimization configurations for all
deep networks. The networks are optimized by applying
the LAMB optimizer [12] on a binary cross-entropy loss.
The initial learning rate is set to 0.0003 and is scheduled
to decrease according to the following steps: epoch 50:
0.0001, epoch 70: 0.00003, epoch 80: 0.00001, epoch 90:
0.000003. We use a batch size of 32 and we apply a
high weight decay of 1 for regularization. The networks
are implemented using the PyTorch framework and are
trained for 100 epochs on a PC with Intel(R) Core(TM)
i7-3770 CPU, 8 MB RAM and Nvidia GTX 1080 GPU.

5.3 Evaluation Metrics
We consider different performance evaluation metrics
for evaluating our models. For each model we count the
number of predicted cases as True Positives (TP), True
Negatives (TN), False Positives (FP) and False Negatives (FN). Then, we compute the following metrics.

Accuracy =

TP + TN
TP + TN + FP + FN

(1)

P recision =

TP
TP + FP

(2)

Recall (sensitivity) =

Specif icity =

TP
TP + FN

TN
TN + FP

F 1−score = 2 ×

Precision × Recall
Precision + Recall

(3)

(4)

(5)

5.4 Results and Discussion
Here, we present and discuss the obtained results for detecting COVID-19 on the considered CT image datasets
with different deep networks. We report the quantitative results along with the confusion matrices for every
single architecture of the adopted networks.
Table 2 summarizes the average values of the evaluation metrics achieved by different deep networks on
the two CT image datasets. All values are given in percentages and the best results are written in bold. We
also compare with the previously obtained results from
the literature when applicable. Generally, we observe
some performance differences between the obtained results on the SARS-CoV-2 CT and the COVID19-CT

9

datasets. Also, we observe the superiority of our model
compared with similar models from recently published
works, which indicates the effectiveness of our optimization and learning strategy.
On the SARS-CoV-2 CT dataset, ResNet101
achieves the best overall performance with respect to
almost all evaluation metrics, with an average accuracy and F1-score of 99.4% and 99.4%, respectively.
The model also achieves an average sensitivity rate of
99.1% indicating that, on average, only two COVID-19
images are falsely predicted as negatives. It is also powerful enough to correctly identify all Non-COVID-19
cases with only one false positive resulting a specificity
rate of 99.6%. The highest sensitivity score of 99.8%
is achieved by the InceptionV3 model, where only one
COVID-19 image is falsely predicted as negative on average. The SqueezeNet model obtains the lowest performance with respect to all evaluation metrics with a
fairly acceptable average accuracy and sensitivity scores
of 95.1% and 96.2%, respectively. Also the ShuffleNet
architecture obtains satisfactory performance with approximately 2% improvements on average for all metrics compared with SqueezeNet. Although the results
obtained by these models are inferior compared with
the rest of models, but they are more efficient. This
matches their main objective of reducing the computational costs rather than improving their visual recognition abilities. The rest of models achieve competitive
performance and very promising results with slight performance differences. Comparing the different variants
of ResNet and DenseNet, we can see that the deeper
variants from each architecture yield a slightly better
performance. The deeper ResNet101 and ResNeXt101
show a marginal gain in performance compared with
their shallower counterparts. The details about classwise results for each model are summarized in the confusion matrices in Figure 9.
It is worthy mentioning that on the SARS-CoV-2
CT dataset the inter-fold variations are minimal and
usually below one percent, showing the robustness of
our fine-tuning strategy. For some of the architectures
like the DenseNet variants we observe a larger confidence interval than their actual differences in recognition performance. This means that the DenseNets and
the deeper ResNet variants share a very similar performance and are almost indistinguishable from each
other. Overall, the obtained results by our models are
better than the recently published ones even when using
the same network architectures. We attribute this to the
better optimization and transferability of the learned
features when applying our fine-tuning strategy.
On the COVID19-CT dataset, the overall performance with respect to all evaluation metrics is inferior

10

Alshazly et al.

Table 2: Performance comparison of different deep models for detecting COVID-19 using various evaluation metrics.
The results are given in the form of mean and standard deviation scores. For a direct comparison, the results from
recently published works are included when applicable.
Dataset

SARS-CoV-2 CT

COVID19-CT

Model
SqueezeNet
ShuffleNet
ResNet18
ResNet50
ResNet101
ResNeXt50
ResNeXt101
InceptionV3
Xception
DenseNet121
DenseNet169
DenseNet201
xDNN [50]
DenseNet201 [35]
Modified VGG19 [52]
COVID CT-Net [53]
Contrastive Learning [24]
SqueezeNet
ShuffleNet
ResNet18
ResNet50
ResNet101
ResNeXt50
ResNeXt101
InceptionV3
Xception
DenseNet121
DenseNet169
DenseNet201
DenseNet169 [51]
Decision function [54]
ResNet101 [57]
DenseNet121+SVM [55]
DenseNet169 [56]
Contrastive Learning [24]

Accuracy
95.1 ± 1.3
97.5 ± 0.8
98.3 ± 0.8
99.2 ± 0.3
99.4 ± 0.4
99.1 ± 0.5
99.2 ± 0.3
99.1 ± 0.5
98.8 ± 0.6
99.3 ± 0.3
99.3 ± 0.5
99.2 ± 0.2
97.3
96.2
95.0
90.8 ± 0.9
87.3 ± 3.2
87.9 ± 2.6
90.3 ± 2.5
90.8 ± 1.9
89.8 ± 2.5
90.6 ± 2.2
90.9 ± 1.8
89.4 ± 2.0
88.5 ± 2.6
88.9 ± 1.2
91.2 ± 1.4
92.9 ± 2.2
83.0
88.3
80.3
85.9 ± 5.9
87.7 ± 4.7
78.6 ± 1.5

to that on the SARS-CoV-2 dataset. This can be attributed to the cross-source heterogeneity of the CT
images in the dataset. The Non-COVID-19 CT images were taken from different sources and show diverse
findings which pose difficulty to distinguish between
COVID-19 and other findings associated with lung diseases due to the potential overlap of visual manifestations (see Figure 8). Another reason is that, the CT
images in the COVID19-CT dataset show strong variations in contrast, variable spatial resolution and other
visual characteristics, which could affect the model’s
ability to extract more discriminative and generalizable
features.
It is also worthy mentioning that for the COVID19CT dataset the inter-fold variations grow substantially
due to the small size of the dataset. During 5-fold crossvalidation the training set consists of about 600 images

Evaluation Metrics
Precision
Recall
Specificity
94.2 ± 2, 0
96.2 ± 1.4
94.0 ± 2.2
96.1 ± 1.4
99.0 ± 0.2
95.9 ± 1.5
97.2 ± 1.2
99.6 ± 0.3
97.1 ± 1.4
99.1 ± 0.5
99.4 ± 0.5
99.1 ± 0.5
99.6 ± 0.3
99.1 ± 0.6
99.6 ± 0.3
99.0 ± 0.5
99.3 ± 0.5
98.9 ± 0.6
99.2 ± 0.4
99.3 ± 0.5
99.2 ± 0.4
98.5 ± 0.8
99.8 ± 0.3
98.5 ± 0.8
99.0 ± 1.0
98.6 ± 1.1
98.9 ± 1.1
99.4 ± 0.2
99.2 ± 0.5
99.4 ± 0.2
99.4 ± 0.6
99.3 ± 0.5
99.3 ± 0.7
99.0 ± 0.4
99.4 ± 0.2
98.9 ± 0.4
99.1
95.5
96.2
96.2
96.2
95.3
94.0
94.7
85.0 ± 0.2
96.2 ± 0.1
95.7 ± 0.4
85.8 ± 1.1
86.3 ± 6.1
86.5 ± 2.3
87.9 ± 6.3
84.5 ± 2.5
90.8 ± 3.9
85.4 ± 2.7
87.1 ± 4.1
93.1 ± 2.5
87.9 ± 4.9
90.2 ± 5.0
90.0 ± 3.6
91.4 ± 5.0
88.0 ± 3.7
90.5 ± 1.9
89.2 ± 3.8
87.4 ± 3.6
93.4 ± 3.4
88.2 ± 4.4
88.1 ± 3.5
93.1 ± 2.9
88.9 ± 4.0
87.7 ± 2.5
90.0 ± 2.4
88.9 ± 2.4
87.3 ± 2.7
88.3 ± 4.7
88.7 ± 2.9
87.6 ± 2.6
88.8 ± 1.4
88.9 ± 2.9
88.1 ± 2.5
93.7 ± 1.2
88.9 ± 2.7
91.3 ± 2.2 93.7 ± 3.4
92.2 ± 2.2
78.2
85.7
84.9 ± 8.4
86.8 ± 6.3
90.2 ± 6.0
85.6 ± 6.7
78.0 ± 1.3
79.7 ± 1.4
-

F1-score
95.2 ± 1.2
97.5 ± 0.8
98.4 ± 0.7
99.2 ± 0.3
99.4 ± 0.4
99.1 ± 0.5
99.2 ± 0.3
99.1 ± 0.5
98.8 ± 0.6
99.3 ± 0.3
99.3 ± 0.4
99.2 ± 0.2
97.3
96.2
94.3
90.0 ± 0.1
90.8 ± 1.3
86.5 ± 3.0
87.6 ± 2.8
90.1 ± 2.3
90.1 ± 1.9
89.3 ± 2.4
90.3 ± 2.2
90.6 ± 1.8
88.8 ± 2.2
87.7 ± 2.9
88.2 ± 1.0
90.8 ± 1.4
92.5 ± 2.4
81.0
86.7
81.8
87.8 ± 5.0
78.8 ± 1.4

only and the test fold has less than 200 images, which
has to produce statistical fluctuations. Metrics considering the overall performance like the accuracy have less
inter-fold variation. However, we observe stronger variations in metrics, that test the bias towards one of the
classes like the specificity. The standard deviation of the
specificity indicates that the different folds tend to encourage the model to focus more on COVID or more on
Non-COVID cases. This phenomenon occurs even for
stratified 5-fold cross-validation, where the distribution
of classes in each fold represents the class distribution
of the entire dataset, and it seems to originate from the
small number of images only.
Our models achieve fairly good performance compared with the recently published work using the exact network architectures. This can bet attributed to a
better optimization of our models and the effectiveness

non-Covid

241

10

15

231

Covid

non-Covid

248

3

10

236

predicted

249

1

7

239

non-Covid

249

2

2

244

predicted
(c) ResNet18

Covid

non-Covid

Covid

non-Covid

248

2

249

2

1

245

3

243

actual
non-Covid
Covid

Covid

actual
non-Covid
Covid

predicted

predicted
(f) ResNeXt50

Covid

non-Covid

Covid

non-Covid

Covid

non-Covid

249

2

250

1

247

4

2

244

4

242

3

243

predicted

actual
non-Covid
Covid

(e) ResNet101

actual
non-Covid
Covid

(d) ResNet50

predicted

Covid

non-Covid

248

2

1

244
predicted

(j) DenseNet121

(h) InceptionV3

actual
non-Covid
Covid

(g) ResNeXt101

predicted

Covid

non-Covid

249

2

2

244
predicted

(k) DenseNet169

(i) Xception

actual
non-Covid
Covid

actual
non-Covid
Covid

non-Covid

(b) ShuffleNet

predicted

actual
non-Covid
Covid

Covid

predicted

(a) SqueezeNet

actual
non-Covid
Covid

11

actual
non-Covid
Covid

Covid

actual
non-Covid
Covid

actual
non-Covid
Covid

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

Covid

non-Covid

249

2

3

243
predicted

(l) DenseNet201

Fig. 9: Confusion matrices for the different deep CNN models. These results are the average counts of the five
models obtained by 5-fold cross-validation on the SARS-CoV-2 CT dataset.
of our fine-tuning strategy using custom-sized inputs
determined specifically for each architecture. Here, we
see that DenseNet201 outperforms all other architectures. The model achieves average accuracy and sensitivity scores of 92.9% and 93.7%, respectively. It also

identifies all COVID-19 images with only four images,
on the average, are falsely predicted as Non-COVID19. DenseNet169 achieves the second best average accuracy of 91.6% and a very high sensitivity identical to
the best model. The DenseNet121 and Xception models

non-Covid

60

9

10

70

Covid

non-Covid

63

6

12

68

predicted

65

5

10

70

non-Covid

63

7

7

73

predicted
(c) ResNet18

Covid

non-Covid

Covid

non-Covid

63

7

65

5

9

71

9

70

actual
non-Covid
Covid

Covid

actual
non-Covid
Covid

predicted

predicted
(f) ResNeXt50

Covid

non-Covid

Covid

non-Covid

Covid

non-Covid

65

5

63

7

62

8

9

71

9

71

9

70

predicted

actual
non-Covid
Covid

(e) ResNet101

actual
non-Covid
Covid

(d) ResNet50

predicted

Covid

non-Covid

62

8

9

71
predicted

(j) DenseNet121

(h) InceptionV3

actual
non-Covid
Covid

(g) ResNeXt101

predicted

Covid

non-Covid

65

4

9

71
predicted

(k) DenseNet169

(i) Xception

actual
non-Covid
Covid

actual
non-Covid
Covid

non-Covid

(b) ShuffleNet

predicted

actual
non-Covid
Covid

Covid

predicted

(a) SqueezeNet

actual
non-Covid
Covid

actual
non-Covid
Covid

Covid

actual
non-Covid
Covid

Alshazly et al.

actual
non-Covid
Covid

12

Covid

non-Covid

65

4

6

73
predicted

(l) DenseNet201

Fig. 10: Confusion matrices for the different deep CNN models. These results are the average counts of the five
models obtained by 5-fold cross-validation on the COVID19-CT dataset.
have nearly identical results for all evaluation metrics.
We observe that small-sized networks such as ResNet18
achieves comparable results with other deeper models.
The SqueezeNet and ShuffleNet models perform at a
similar level of accuracy. The variants of the ResNeXt

models have comparable results and perform as good
as the different ResNet variants. A detailed analysis on
the class-wise results for individual models is presented
in the confusion matrices in Figure 10.

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

5.5 Visual Explanations
This subsection provides visual explanations to make
our models more transparent. We start with a 2D projection of the learned features using t-SNE [13], and
then present the localization maps for highlighting the
COVID-19 associated regions using Grad-CAM [14].
5.5.1 The t-SNE visualization
To understand how the deep neural networks represent the CT images in the high-dimensional feature
space we apply the t-SNE algorithm to visualize these
features. For each image in the SARS-CoV-2 dataset
we first extract the 2048-dimensional feature vector
from the penultimate layer of the Inception V3 model.
Next, we apply t-SNE to map the features on to 2D
space and then visualize the embeddings of training
and test representations. Figure 11 clearly shows two
well-separated clusters of the CT images of COVID-19
and Non-COVID-19. This indicates that the distribution of training and test features are quite similar to
each other, which indicates good generalization capabilities of our model. The clear and wide margin between the two classes shows how nicely the CT images
are separated in feature space.
We also repeat the same procedure for the
COVID19-CT dataset. The feature vectors are extracted from the penultimate layer of the DenseNet169
model. The length of the feature vectors is 1664 dimensions. We again apply t-SNE to map the features on to
2D space to explore and visualize them. Figure 12 shows
two clusters representing CT images for the COVID-19
and Non-COVID-19 classes. Even though the classes
are fairly distinguishable with a clear decision boundary, however, we can see that some CT images are misclassified, and more specifically the Non-COVID-19 CT
images from the test set.
5.5.2 The Grad-CAM visualization
In order to make our models more transparent and
provide detailed visual analysis, we present the GradCAM localization maps obtained by different models.
We consider CT images with COVID-19 abnormalities
from the test set of each dataset and highlight the important regions considered for the prediction. For the
SARS-CoV-2 dataset we use the Inception V3 model.
Figure 13 shows the original CT images and their localization maps. Our model is capable to detect regions
that show abnormalities in the CT scans.
In a similar way, we consider classifying the test CT
scans from the COVID-19 dataset by the DenseNet169

13

model and highlight the important regions considered
for predictions. We present the original CT images and
their localization maps in Figure 13. We can also see
that our model is capable to detect the COVID-19 related regions as marked (small square in some images)
by expert radiologists.
A wide variety of typical and atypical CT abnormalities have been reported for COVID-19 patients in
various studies [58, 59]. So, we tested our models on external CT images extracted from these two publications
as they feature typical findings of COVID-19 pneumonia marked by specialists. In order to make sure that
not any of the extracted images are unintentionally included in our datasets, specifically the COVID19-CT
dataset, we use the model trained on the SARS-CoV-2
dataset. First, the InceptionV3 model is employed to
classify the extracted CT images. The model is able to
correctly classify the given CT images as COVID-19.
Second, in order to interpret the model’s generalization
capabilities, we apply the Grad-CAM technique to visualize the regions of abnormalities that are considered.
By assessing the different CT images in Figure 15, we
can see that the model accurately localizes the diseaserelated regions. Even more interesting is the fact that
the model ignores any specific marks in the images like
letters and only localizes the COVID-19 related regions.
These visual explanations show the success of our models to learn relevant, generic visual features related to
COVID-19 and are capable to correctly classify CT images outside the datasets on which they are trained.
Figure 16 shows various CT scans where only one
lung is visible. The CT scans are also extracted from
the paper [58] and show different CT manifestations
of COVID-19 pneumonia marked by red squares. The
InceptionV3 model is capable to classify them correctly
as COVID-19, although it is trained on CT scans where
the entire lung is visible. Intriguingly, when applying
Grad-CAM we can see that all regions of abnormalities
are accurately localized. This also proves the potential
of our model to detect COVID-19 abnormalities in CT
images outside the dataset used for training.
6 Conclusion
We proposed different deep learning based approaches
for accurate COVID-19 detection using chest CT images. The most advanced deep network architectures
and their variants were considered and extensive experiments were conducted on the two datasets with the
largest amount of CT images available so far. Moreover, we investigated different configurations and determined custom-sized input for each network to achieve
the best detection performance. The resulting networks

14

Alshazly et al.
80

60

40

20

0

20

40

60
40

20

0

20

40

Fig. 11: Visualization of the t-SNE embeddings for the entire SARS-CoV-2 CT dataset. We clearly see two different
clusters representing COVID-19 (red for train and blue for test samples) and Non-COVID-19 (yellow for train and
green for test samples) classes.

30

20

10

0

10

20

7.5

5.0

2.5

0.0

2.5

5.0

7.5

Fig. 12: Visualization of the t-SNE embeddings for the entire COVID-19 CT dataset. As in Figure 11 we can see
two different clusters representing COVID-19 and Non-COVID-19 classes.

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

15

Fig. 13: Grad-CAM visualizations for samples CT images from the SARS-CoV-2 dataset. The InceptionV3 model
correctly classified them as COVID-19 and localized the most relevant regions used for its decision. The first and
third columns show CT images with COVID-19 findings, whereas the second and fourth columns represent their
corresponding localization maps generated by Grad-CAM.

Fig. 14: Grad-CAM visualizations for samples CT images from the COVID19-CT dataset. The DenseNet169 model
correctly classified them as COVID-19 and localized the most relevant regions as shown in the localization maps.
showed a significantly improved performance for detecting COVID-19. Our models achieved state-of-the-art

performance with an average accuracy of 99.4% and
92.9%, and a sensitivity score of 99.8% and 93.7% on

16

Alshazly et al.

Fig. 15: Grad-CAM visualizations for CT images taken from two publications [58, 59]. The CT images were
correctly classified as COVID-19 and the disease-related regions are accurately localized as marked by specialists.

Fig. 16: Grad-CAM visualizations for CT images taken from [58]. The CT scans show different manifestations
of COVID-19 marked by red frames or white arrows. Our model was able to identify them as COVID-19 and
accurately localize the COVID-19 associated abnormalities.
the SARS-CoV-2 CT and COVID19-CT datasets, respectively. This indicates the effectiveness of our proposed approaches and the potential of using deep learning for fully automated and fast diagnosis of COVID19. In order to explain the obtained results we em-

ployed two visualization methods. First, we explored
the learned features using the t-SNE algorithm and the
resulting visualizations showed well-separated clusters
for COVID-19 and Non-COVID-19 cases. We also assessed the obtained networks using the Grad-CAM al-

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

gorithm to obtain high-resolution visualizations showing the discriminative regions of abnormalities in the
CT images. Moreover, we tested our models on external
CT images from different publications. Our models were
capable to detect all COVID-19 cases and accurately localize the COVID-19 associated regions as marked by
expert radiologists.
Acknowledgements The authors gratefully acknowledge
the constructive feedback from Prof. Dr. Jörg Barkhausen
from the Clinic for Radiology and Nuclear Medicine at the
Universitätsklinikum Schleswig-Holstein (UKSH), Lübeck.
The work of Hammam Alshazly was supported by the
Bundesministerium für Bildung und Forschung (BMBF)
through the KI-Lab Project. The work of Christoph
Linse was supported by the Bundesministeriums für
Wirtschaft und Energie (BMWi) through the Mittelstand 4.0Kompetenzzentrum Kiel Project.

References
1. J. Liu, X. Liao, S. Qian, J. Yuan, F. Wang, Y. Liu,
Z. Wang, F.-S. Wang, L. Liu, and Z. Zhang, “Community transmission of severe acute respiratory
syndrome coronavirus 2, shenzhen, china, 2020,”
Emerging Infectious Diseases, vol. 26, no. 6, pp.
1320–1323, 2020.
2. I. Ghinai, T. D. McPherson, J. C. Hunter, H. L.
Kirking, D. Christiansen, K. Joshi, R. Rubin,
S. Morales-Estrada, S. R. Black, M. Pacilli et al.,
“First known person-to-person transmission of severe acute respiratory syndrome coronavirus 2
(SARS-CoV-2) in the USA,” The Lancet, vol. 395,
no. 10230, pp. 1137–1144, 2020.
3. N. Chen, M. Zhou, X. Dong, J. Qu, F. Gong,
Y. Han, Y. Qiu, J. Wang, Y. Liu, Y. Wei et al.,
“Epidemiological and clinical characteristics of 99
cases of 2019 novel coronavirus pneumonia in
wuhan, china: A descriptive study,” The Lancet,
vol. 395, no. 10223, pp. 507–513, 2020.
4. C. Long, H. Xu, Q. Shen, X. Zhang, B. Fan,
C. Wang, B. Zeng, Z. Li, X. Li, and H. Li, “Diagnosis of the Coronavirus disease (COVID-19): rRTPCR or CT?” European journal of radiology, vol.
126, p. 108961, 2020.
5. Y. Fang, H. Zhang, J. Xie, M. Lin, L. Ying, P. Pang,
and W. Ji, “Sensitivity of chest CT for COVID19: Comparison to RT-PCR,” Radiology, vol. 296,
no. 2, p. 200432, 2020.
6. T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv,
Q. Tao, Z. Sun, and L. Xia, “Correlation of chest
CT and RT-PCR testing in coronavirus disease
2019 (COVID-19) in China: A report of 1014

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

17

cases,” Radiology, vol. 296, no. 2, pp. E32–E40,
2020.
J. Kanne, “Chest ct findings in 2019 novel coronavirus (2019-ncov) infections from wuhan, china:
Key points for the radiologist.” Radiology, vol. 295,
no. 1, pp. 16–17, 2020.
O. Russakovsky, J. Deng, H. Su, J. Krause,
S. Satheesh, S. Ma, Z. Huang, A. Karpathy,
A. Khosla, M. Bernstein et al., “Imagenet large
scale visual recognition challenge,” International
Journal of Computer Vision, vol. 115, no. 3, pp.
211–252, 2015.
K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A
survey of transfer learning,” Journal of Big Data,
vol. 3, no. 1, p. 9, 2016.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and
L. Fei-Fei, “ImageNet: A Large-scale Hierarchical
Image Database,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009, pp. 248–255.
H. Alshazly, C. Linse, E. Barth, and T. Martinetz,
“Deep convolutional neural networks for unconstrained ear recognition,” IEEE Access, vol. 8, pp.
170 295–170 310, 2020.
Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, and C.-J. Hsieh,
“Large batch optimization for deep learning: Training bert in 76 minutes,” in International Conference on Learning Representations (ICLR), 2020.
L. v. d. Maaten and G. Hinton, “Visualizing data
using t-SNE,” Journal of Machine Learning Research, vol. 9, pp. 2579–2605, 2008.
R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam,
D. Parikh, and D. Batra, “Grad-CAM: visual explanations from deep networks via gradient-based
localization,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2017, pp. 618–626.
H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu,
I. Nogues, J. Yao, D. Mollura, and R. M. Summers,
“Deep convolutional neural networks for computeraided detection: CNN architectures, dataset characteristics and transfer learning,” IEEE Transactions on Medical Imaging, vol. 35, no. 5, pp. 1285–
1298, 2016.
P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta,
T. Duan, D. Ding, A. Bagul, C. Langlotz, K. Shpanskaya et al., “Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning,” arXiv preprint arXiv:1711.05225, 2017.
L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong,
J. Bai, Y. Lu, Z. Fang, Q. Song et al., “Using Artificial Intelligence to Detect COVID-19 and

18

18.

19.

20.

21.

22.

23.

24.

25.

26.

27.

28.

Alshazly et al.

Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy,” Radiology, vol. 296, no. 2, pp. E65–E71, 2020.
X. Xu, X. Jiang, C. Ma, P. Du, X. Li, S. Lv, L. Yu,
Q. Ni, Y. Chen, J. Su et al., “A deep learning system to screen novel coronavirus disease 2019 pneumonia,” Engineering, 2020.
C. Zheng, X. Deng, Q. Fu, Q. Zhou, J. Feng, H. Ma,
W. Liu, and X. Wang, “Deep learning-based detection for COVID-19 from chest CT using weak
label,” medRxiv, 2020.
S. Wang, B. Kang, J. Ma, X. Zeng, M. Xiao,
J. Guo, M. Cai, J. Yang, Y. Li, X. Meng et al., “A
deep learning algorithm using CT images to screen
for Corona Virus Disease (COVID-19),” medRxiv,
2020.
F. Shan, Y. Gao, J. Wang, W. Shi, N. Shi, M. Han,
Z. Xue, and Y. Shi, “Lung infection quantification
of covid-19 in ct images with deep learning,” arXiv
preprint arXiv:2003.04655, 2020.
L. Wang and A. Wong, “COVID-Net: A Tailored
Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-Ray Images,” arXiv preprint arXiv:2003.09871, 2020.
M. Hasan, M. Alam, M. Elahi, E. Toufick, S. Roy,
S. R. Wahid et al., “CVR-Net: A deep convolutional neural network for coronavirus recognition
from chest radiography images,” arXiv preprint
arXiv:2007.11993, 2020.
Z. Wang, Q. Liu, and Q. Dou, “Contrastive Crosssite Learning with Redesigned Net for COVID-19
CT Classification,” IEEE Journal of Biomedical
and Health Informatics, vol. 24, no. 10, pp. 2806–
2813, 2020.
M. Farooq and A. Hafeez, “COVID-ResNet: A deep
learning framework for screening of covid19 from
radiographs,” arXiv preprint arXiv:2003.14395,
2020.
A. I. Khan, J. L. Shah, and M. M. Bhat, “CoroNet:
A deep neural network for detection and diagnosis
of COVID-19 from chest x-ray images,” Computer
Methods and Programs in Biomedicine, vol. 196, p.
105581, 2020.
P. Afshar, S. Heidarian, F. Naderkhani,
A. Oikonomou, K. N. Plataniotis, and A. Mohammadi, “COVID-CAPS: A capsule network-based
framework for identification of covid-19 cases from
x-ray images,” arXiv preprint arXiv:2004.02696,
2020.
S. Toraman, T. B. Alakus, and I. Turkoglu, “Convolutional capsnet: A novel artificial neural network
approach to detect covid-19 disease from x-ray images using capsule networks,” Chaos, Solitons &

Fractals, vol. 140, p. 110122, 2020.
29. I. D. Apostolopoulos and T. A. Mpesiana, “Covid19: automatic detection from x-ray images utilizing transfer learning with convolutional neural
networks,” Physical and Engineering Sciences in
Medicine, vol. 43, pp. 635–640, 2020.
30. S. Minaee, R. Kafieh, M. Sonka, S. Yazdani, and
G. J. Soufi, “Deep-COVID: Predicting COVID19 From Chest X-Ray Images Using Deep Transfer Learning,” Medical Image Analysis, vol. 65, p.
101794, 2020.
31. L. Brunese, F. Mercaldo, A. Reginelli, and A. Santone, “Explainable deep learning for pulmonary
disease and coronavirus COVID-19 detection from
X-rays,” Computer Methods and Programs in
Biomedicine, vol. 196, p. 105608, 2020.
32. K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in Proceedings of the International Conference on Learning Representations (ICLR), 2015,
pp. 1–14.
33. M. Zhou, Y. Chen, D. Wang, Y. Xu, W. Yao,
J. Huang, X. Jin, Z. Pan, J. Tan, L. Wang et al.,
“Improved deep learning model for differentiating
novel coronavirus pneumonia and influenza pneumonia,” medRxiv, 2020.
34. Y. Song, S. Zheng, L. Li, X. Zhang, X. Zhang,
Z. Huang, J. Chen, H. Zhao, Y. Jie, R. Wang
et al., “Deep learning enables accurate diagnosis of
novel coronavirus (COVID-19) with CT images,”
medRxiv, 2020.
35. A. Jaiswal, N. Gianchandani, D. Singh, V. Kumar, and M. Kaur, “Classification of the COVID19 infected patients using DenseNet201 based deep
transfer learning,” Journal of Biomolecular Structure and Dynamics, pp. 1–8, 2020.
36. R. M. Pereira, D. Bertolini, L. O. Teixeira, C. N.
Silla Jr, and Y. M. Costa, “COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios,” Computer Methods and
Programs in Biomedicine, vol. 194, p. 105532, 2020.
37. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens,
and Z. Wojna, “Rethinking the inception architecture for computer vision,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, pp. 2818–2826.
38. D. Al-Karawi, S. Al-Zaidi, N. Polus, and S. Jassim,
“Machine Learning Analysis of Chest CT Scan Images as a Complementary Digital Test of Coronavirus (COVID-19) Patients,” medRxiv, 2020.
39. F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf,
W. J. Dally, and K. Keutzer, “Squeezenet: Alexnetlevel accuracy with 50x fewer parameters and¡ 0.5

Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning

40.

41.

42.

43.

44.

45.

46.

47.

48.

49.

50.

mb model size,” in Proceedings of the International
Conference on Learning Representations (ICLR),
2017.
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings
of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2016, pp. 770–778.
S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He,
“Aggregated residual transformations for deep neural networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2017, pp. 1492–1500.
F. Chollet, “Xception: Deep learning with depthwise separable convolutions,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017, pp. 1251–1258.
N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “ShuffleNet V2: Practical guidelines for efficient CNN architecture design,” in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp.
116–131.
G. Huang, Z. Liu, L. Van Der Maaten, and
K. Q. Weinberger, “Densely connected convolutional networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 4700–4708.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2015, pp. 1–9.
S. Ioffe and C. Szegedy, “Batch normalization:
Accelerating deep network training by reducing
internal covariate shift,” in Proceedings of the
32nd International Conference on Machine Learning (ICML), 2015, pp. 448–456.
X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient convolutional neural
network for mobile devices,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018, pp. 6848–6856.
M. Huh, P. Agrawal, and A. A. Efros, “What
makes ImageNet good for transfer learning?” arXiv
preprint arXiv:1608.08614, 2016.
H. Alshazly, C. Linse, E. Barth, and T. Martinetz,
“Ensembles of deep learning models and transfer learning for ear recognition,” Sensors, vol. 19,
no. 19, p. 4139, 2019.
E. Soares, P. Angelov, S. Biaso, M. H. Froes, and
D. K. Abe, “SARS-CoV-2 CT-scan dataset: A large
dataset of real patients CT scans for SARS-CoV-2
identification,” medRxiv, 2020.

19

51. X. He, X. Yang, S. Zhang, J. Zhao, Y. Zhang,
E. Xing, and P. Xie, “Sample-Efficient Deep Learning for COVID-19 Diagnosis Based on CT Scans,”
medrxiv, 2020.
52. H. Panwar, P. Gupta, M. K. Siddiqui, R. MoralesMenendez, P. Bhardwaj, and V. Singh, “A Deep
Learning and Grad-CAM based Color Visualization
Approach for Fast Detection of COVID-19 Cases
using Chest X-ray and CT-Scan Images,” Chaos,
Solitons & Fractals, vol. 140, p. 110190, 2020.
53. S. Yazdani, S. Minaee, R. Kafieh, N. Saeedizadeh, and M. Sonka, “COVID CT-Net: Predicting Covid-19 From Chest CT Images Using Attentional Convolutional Network,” arXiv preprint
arXiv:2009.05096, 2020.
54. A. K. Mishra, S. K. Das, P. Roy, and S. Bandyopadhyay, “Identifying COVID19 from Chest CT Images: A Deep Convolutional Neural Networks Based
Approach,” Journal of Healthcare Engineering, vol.
2020, 2020.
55. A. S. Jokandan, H. Asgharnezhad, S. S. Jokandan,
A. Khosravi, P. M. Kebria, D. Nahavandi, S. Nahavandi, and D. Srinivasan, “An Uncertainty-aware
Transfer Learning-based Framework for Covid-19
Diagnosis,” arXiv preprint arXiv:2007.14846, 2020.
56. A. R. Martinez, “Classification of covid-19 in ct
scans using multi-source transfer learning,” arXiv
preprint arXiv:2009.10474, 2020.
57. M. Saqib, S. Anwar, A. Anwar, M. Blumenstein
et al., “COVID19 detection from Radiographs: Is
Deep Learning able to handle the crisis?” TechRxiv,
2020.
58. Z. Ye, Y. Zhang, Y. Wang, Z. Huang, and B. Song,
“Chest CT manifestations of new coronavirus disease 2019 (COVID-19): a pictorial review,” European radiology, vol. 30, pp. 4381–4389, 2020.
59. C. Hani, N. H. Trieu, I. Saab, S. Dangeard, S. Bennani, G. Chassagnon, and M.-P. Revel, “COVID-19
pneumonia: A review of typical CT findings and differential diagnosis,” Diagnostic and Interventional
Imaging, vol. 101, no. 5, pp. 263–268, 2020.

