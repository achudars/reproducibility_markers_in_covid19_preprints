Pre-print

Constructing and Evaluating an Explainable Model for
COVID-19 Diagnosis from Chest X-rays
Rishab Khincha

f20160517@goa.bits-pilani.ac.in

BITS Pilani, Goa, India

Soundarya Krishnan

f20160472@goa.bits-pilani.ac.in

arXiv:2012.10787v2 [eess.IV] 12 Feb 2021

BITS Pilani, Goa, India

Tirtharaj Dash

tirtharaj@goa.bits-pilani.ac.in

BITS Pilani, Goa, India

Lovekesh Vig

lovekesh.vig@tcs.com

TCS Research, New Delhi, India

Ashwin Srinivasan

ashwin@goa.bits-pilani.ac.in

BITS Pilani, Goa, India

Abstract
In this paper, our focus is on constructing models to assist a clinician in the
diagnosis of COVID-19 patients in situations where it is easier and cheaper
to obtain X-ray data than to obtain
high-quality images like those from CT
scans. Deep neural networks have repeatedly been shown to be capable of
constructing highly predictive models
for disease detection directly from image data. However, their use in assisting
clinicians has repeatedly hit a stumbling
block, because of their black-box nature. Some of this difficulty can be alleviated if predictions were accompanied
by explanations expressed in clinicallyrelevant terms. In this paper, deep
neural networks are used to extract
domain-specific features(morphological
features like ground glass opacity and
disease indications like pneumonia) directly from the image data. Predictions about these features are then used
to construct a symbolic model (a decision tree) for the diagnosis of COVID19 from chest X-rays. We accompany
these predictions with two kinds of explanations: visual (saliency maps, derived from the neural stage), and tex© R. Khincha, S. Krishnan, T. Dash, L. Vig & A. Srinivasan.

tual (logical descriptions, derived from
the symbolic stage). A radiologist rates
the usefulness of the visual and textual
explanations. Our results suggest that:
(a) There is no significant loss in predictive accuracies when using the neuralsymbolic model over using a single endto-end model; and (b) The radiologist
usually finds at least one of visual or
textual explanations relevant; In addition, the textual explanations are better than a “textual baseline” more often than the corresponding visual case.
This suggests either: the textual representation is is more relevant to the diagnosistic task, or that the visual representation is not very relevant for the
diagnostic task, or both. Taken together, these results demonstrate that
neural models can be employed usefully
in identifying domain-specific features
from low-level image data; that textual explanations in terms of clinicallyrelevant features may be useful; and
that visual explanations will need to be
clinically meaningful to be useful.
Keywords: Hybrid models, NeuralSymbolic models, Human-in-the-loop,
Explainable ML, COVID-19

Explainable Models for COVID-19

(b) Hierarchical Model

(a) Single End-to-End Model

Figure 1: Two approaches to building explainable models

1. Introduction

for acceptance by physicians, as well as for
sanity checks during model development.

The use of explainable symbolic models for
diagnosis of disease is not new (MYCIN
and Internist-I, for example, were developed
from the early 1970s, with just this purpose
Shortliffe and Buchanan (1975); Miller et al.
(1982)). Neither is the use of hierarchical
approaches to Computer Vision, in which
high-level symbolic models use the results of
low-level feature-extraction from raw images
(see for example, Ballard and Brown (1982)).
For most of these, the symbolic component
has fulfilled purpose of generating limited answers to “Why” type questions. Recent developments in hardware, storage, and algorithmic optimisation has however made possible the construction of very large end-toend neural network models that are capable
of predicting the probability of disease, directly from raw image data (see Fig. 1)

Saliency-like visual explanations (GSInquire in Wang and Wong (2020) and GradCAMs in Mangal et al. (2020); Karim et al.
(2020) have been used to highlight the relevant regions for the diagnosis made by the
model. However, these maps may have less
utility in a medical setting, consistent to
previous findings like in Arun et al. (2020).
None of these works explore any other means
of explanations. Most crucially, most works
do not consult clinicians to find out which
types of explanations are actually meaningful to them and assist them in diagnosis.
In this paper, we investigate the use of
a hierarchical neural-symbolic approach for
COVID-19 diagnosis, accompanied by a description of why the diagnosis was reached.
We consider two kinds of explanations: (a)
Visual, in the form of saliency maps from
the neural-stage that construct the domainspecific features used for diagnosis; and (b)
Textual, in the form of a description obtained
from the symbolic model when making a diagnosis. We compare these against the following baselines that are agnostic to any decision being made: (c) A simple segmentation map of the lungs; and (d) A simple tabulation of the input to the symbolic model
for the data instance being diagnosed. The
thinking here is that for (a) to be a relevant
visual explanation, it should be judged (by
a radiologist) to be at least better than (c);

Although end-to-end models can produce
accurate diagnosis of COVID-19 (Asnaoui
and Chawki, 2020; Toğaçar et al., 2020; Mangal et al., 2020; Wang and Wong, 2020; Gozes
et al., 2020), they are often considered as
black-box models, with little or no explanatory power (Lipton, 2016). This has proved
to be a stumbling block in their clinical acceptability.
However, interpretability and explainability of these models has not been a focus for
these works. Work in these areas is crucial
2

Explainable Models for COVID-19

(a) Morphological annotations for COVID19 X-rays from the COVIDx dataset - we
call this the COVIDr dataset; (b) Images
for pneumonia patients taken from the NIH
dataset; (c) Images for healthy and tuberculosis patients taken from the Pulmonary
Chest X-ray dataset. The COVIDr dataset
contains annotatations made by a practising
radiologist from the NHS, UK. We describe
the annotation process first.

and for (b) to be relevant, it should be judged
(again, by a radiologist) to be at least better
than (d).
Recently, a categorisation of model explainability has been proposed by Arrieta
et al. (2019). In this categorisation, explanations in (a) constitute local, visual explanations; and (b) constitute global, textual
explanations. To promote reproducibility of
our findings, and for any future extensions,
we provide the datasets collected from an experienced radiologist on request.

Chest X-rays from COVID-19 positive patients are annotated with one of 7 morphological labels. Here, we are interested in
simply two broad categories of these labels,
namely the presence (or absence) of Ground
Glass Opacity (GGO) and Air Space Opacification (ASO). In COVID-19 images, GGO
is present if the Ground Glass Opacity annotation is marked as present; and ASO is
present if any of: Bilateral or Unilateral Air
Space Opacification annotations are marked
as present (See Appendix A for more details
on the COVIDr dataset). For healthy samples, we assume both GGO and ASO are
absent. For tuberculosis and pneumonia patients, both GGO and ASO are not known
(these images were not seen during annotation).

2. Neural-Symbolic Modelling for
Diagnosis and Explanation
In this section, we describe a hierarchical approach for obtaining diagnoses and explanations. The first stage of the hierarchical approach performs a form of domain-specific
representation-learning. Two deep neural
networks are used to obtain clinically relevant features directly from images. The Smodel is a deep network that predicts symptomatic features like consolidations, pleural
effusions and so on. The R-model is a deep
network that predicts morphological features
like ground-glass opacity and air-space opacifications (see Fig. 2). The principal datasets
used in each of the models are shown in Table 1.

The R-model is constructed to predict a
1-hot encoding of of one of 5 values: only
GGO is present; only ASO is present; both
GGO and ASO are present; both GGO and
ASO are absent; and both GGO and ASO
are unknown. The model is constructed using a training, validation, and the independent held-out test data as described Section
3.3. Input data for constructing the model
consist of chest X-ray images, and the predictions of the S-model. The final model is
obtained by minimising the categorical crossentropy loss using an Adam optimizer.

2.1. Stage 1: Neural Models for
Clinical Features
The S-model is a CheXNet (Rajpurkar et al.,
2017) model, obtained by minimising the binary crossentropy loss using an Adam optimizer.
The R-model is a deep model that identifies radiologically-relevant morphological
symptoms. The model in this paper focuses
on predicting the occurrence of various forms
of opacification in chest X-rays. To train the
R-model, we use a dataset assembled from:
3

Explainable Models for COVID-19

Figure 2: Neural-Symbolic Pipeline for COVID-19 diagnosis. The S-model is a deep network
for predicting symptoms; the R-model is a deep network for predicting radiological features;
and the T -model is a decision-tree. E refers to a set of procedures for extracting visual and
symbolic explanations.

Figure 3: Explanations for COVID-19 diagnosis
2.2. Stage 2: Symbolic Model for
Diagnosis

COV+ or COV–, where the latter refers to
images of healthy, pneumonia and tuberculosis patients).1 Details of tree-construction
are in Section 3.3.

In this stage, outputs of the S and R-models
along with labels for images are used to construct a decision-tree for COVID-19 diagnosis (the class-labels for the decision-tree are

1. The input to for tree-construction are probability vectors for symptoms and boolean-values for
the morphological features. Recall the R-models

4

Explainable Models for COVID-19

Table 1: Datasets Table
Dataset

NIH Chest X-ray
Wang et al. (2017)

Model

Details

S-model

112,120 X-ray images with 14 disease
labels from 30,805 unique patients
Default train-val-test split used.

R-model

Pneumonia images used.
Train(234) and test(88).

Seg-model

Chest X-rays and their masks used.
Train (566) and Test (134)

R-model

Tuberculosis and healthy Chest X-rays used.
Tuberculosis - Train (295) and Test (88);
Healthy - Train(294) and Test(110)

Seg-model

Chest X-rays and their manually
annotated masks used.

R-model

COVID-19 positive Chest X-rays used.
Train (196) and Test (30)

R-model

Relevant radiological annotations from a
practicing radiologist for COVID-19 positive
Chest X-rays from COVIDx.
See Appendix A for details.

Pulmonary Chest X-ray
Jaeger et al. (2014)

COVIDx
Cohen et al. (2020)

COVIDr

Visual, Descriptive. This acts of a baseline for the corresponding inductive exWe examine two kinds of model-based explanation. It consists of masks showing
planations visual and textual, each with the
a segmentation of the lungs obtained uscorresponding baselines. We use the term
ing a deep network.
“inductive” to qualify the explanation based
on a machine-constructed model. The base- Symbolic, Inductive. This is a translation
lines are simple summarisations of the releof the sequence of decisions made by the
vant features of the data, and are qualified
symbolic model in reaching a diagnosis.2
by the term “descriptive”. We require the
inductive explanations to be at least as use- Symbolic, Descriptive. This acts as a
baseline for assessing the utility of the
ful to the clinicians as the descriptive sumcorresponding inductive explanation. It
maries. Details of these categories are:
is a tabulation of discretised values of
probabilities of the outputs of the S and
Visual, Inductive. This consists of class
activation maps (GradCAMs, proposed 2. For simplicity, we leave out statements about
by Selvaraju et al. (2016)), showing gramissing values when generating explanations. It
is unclear whether statements of the form “X
dations of saliency in the R-model.
2.3. Explanations

is missing” are of value to the radiologist, but
such an expression can nevertheless have predictive value.

essentially predict whether GGO and ASO are
present, absent, or missing.

5

Explainable Models for COVID-19

1. Split the COVIDx dataset
COVIDxTrain and COVIDxTest

R-models. The discretisation is into 3
simple categories (high, medium, and
low).

into

2. Construct an end-to-end deep model on
the COVIDxTrain data and use it to
obtain predictions on the COVIDxTest
data.

Examples of each of these kinds of these categories are shown in Fig. 3.

3. Empirical Evaluation

3. Construct an S-model using the N IH
dataset and use it to obtain predictions
for the COVIDxTrain data.

3.1. Aims
Our aim is to test the neural-symbolic system
proposed in Section 2 along the following dimensions:

4. Construct an R-model using pre-trained
weights (see details below), and the
COVIDxTrain data and use it to construct predictions for morphological features for the COVIDxTrain data.

• Predictive We investigate if there is
any significant loss in predictive accuracy over using a single end-to-end deep
network model with access to the same
image data.

5. Construct a T -model (decision tree) using the outputs of the S-model (Step 3)
and the R-model (Step 4) and use it to
obtain predictions on the COVIDxTest
data

• Explanatory We investigate the clinical usefulness of the visual and symbolic
explanations described earlier.

6. Compare the estimated accuracies on
COVIDxTest of the T -model (Step 5)
and the end-to-end deep model (Step 2)

3.2. Materials
3.2.1. Data

The following details are relevant:

The datasets used in this paper are shown
in Table 1. The COVIDr dataset has been
annotated a practising radiologist. It captures important ground glass opacities and
air space opacifications from the Chest Xrays relevant to the diagnosis of COVID-19
(See Appendix A for more details).

• We use the default split provided by
Cohen et al. (2020) for splitting the
COVIDx data in COVIDxTrain and the
completely held out test-set, COVIDxTest
• Pre-trained weights for the end-to-end
model are from ImageNet. The learning
rate used was 10−4 and the model was
trained for 500 epochs.

3.2.2. Algorithms and Machines
All the experiments are conducted in Python
environment (Tensorflow/Keras) in a machine with 64GB main memory, 16-core Intel
processor and 8GB NVIDIA P4000 graphics
processor.

• The train, validation and test set for
the S-model uses the default split from
Wang et al. (2017) with 62428, 6336,
and 1518 images respectively.

3.3. Methods

• There are two choices for pre-trained
weights for the R-model. We have
a choice of either using a generic

We adopt the following method for evaluation of predictive accuracy:
6

Explainable Models for COVID-19

obtained using the lung segmentation
model trained in Section 2.3.

pre-training (using ImageNet) or the
domain-specific pre-training (using the
end-to-end model in Step 2). We use a
subset of COVIDxTrain as a validation
set to decide between these alternatives,
which resulted in the use of domainspecific pre-trained weights.

• Textual descriptive baselines are obtained from the predictions of the R and
the S-models by binning the probabilities into Low (≤ 0.33), Medium (≤ 0.67)
or High.

• We split the COVIDxTrain set into a
85-15 train-val set to construct the Rmodel. We perform a sweep across 3
learning rate settings (10−3 , 10−4 and
10−5 ) and epochs (100, 250, 500) and
the train split to build 9 R-models. The
one performing best on the val split is
selected as our final R-model.

• The radiological assessments of the utility for each of the individual explanations as well as the comparative utilities are obtained by asking the radiologist a series of questions described in
Appendix C.
3.4. Results

• Assessments of comparative difference
in accuracies are made using a Gaussian approximation to the Binomial distribution. Given an accuracy of p on
COVIDxTest, the
p standard deviation
(s.d) is given by p(1 − p)/n, where n
is the number of instances in COVIDxTest. Then a difference in accuracy is
taken to be significant if it is 2 or more
s.d’s away.

The main observations to note are these:3
(1) The estimated predictive accuracy of the
end-to-end approach is 0.985±0.007 and that
of the Neural-Symbolic approach is 0.973 ±
0.009 (Table 2); (2) Visual explanations are
marked as relevant on 21/30 instances. Of
these, in 5/21 instances the inductive explanation is marked as being more useful
than the corresponding descriptive baseline
(see Table 4a); (3) Textual explanations are
marked as relevant on 18/30 instances. Of
these, in 13/18 instances the inductive explanation is marked as being more useful than
that the corresponding descriptive baseline
(see Table 4b); (4) Either one of visual or
textual explanation is marked as relevant in
29/30 instances. (See Table 3 for details
about the utility of each of the representations we provide.
[Explanations are Useful.]
Either
visual or textual explanations were considered radiologically relevant in 29/30
instances. This suggests that it could be
useful to augment clinical diagnoses with
such additional information.

We adopt the following method for evaluation of explanations. For a subset of images
in COVIDxTest:
1. Obtain inductive visual and textual explanations along with their corresponding baselines. explanations (inductive
and descriptive)
2. Obtain a radiological assessment of (a)
Utility of visual and textual explanations; (b) Comparative utility of visual
and textual explanations with the baselines.
The following details are relevant:
• Visual inductive explanations are class
activation maps obtained using GradCAM. Visual descriptive baselines are

3. Here, we term an explanation as “relevant” if it
is useful or somewhat useful.

7

Explainable Models for COVID-19

Table 2: Confusion Matrices for COVID-19 Prediction. (COV+ is COVID-19 positive;
COV− is not COVID-19 positive)
(a) End to End model

Pred
Act

(b) Neural-Symbolic Model

COV+

COV−

26
1

4
297

COV+
COV−

Pred
Act

COV+

COV−

23
2

7
296

COV+
COV−

Table 3: Usefulness of explanations and baselines as rated by the radiologist (Vis-Visual;
Text-Textual; Ind-Inductive; Des-Descriptive)
Vis-Ind

Vis-Des

Text-Ind

Text-Des

14
7
9

21
5
1

17
1
12

6
4
20

Useful
Somewhat Useful
Not Useful

Table 4: Conditional comparison of inductive explanations (I) and descriptive baseline (D).
(X > Y means that X is more useful than Y )
(b) Comparison given Textual Explanations (T)
are relevant.

(a) Comparison given Visual Explanations (V)
are relevant.

Count
I>D
I<D
I=D

Count

5
8
8

I>D
I<D
I=D

13
0
5

Table 5: Agreements between the model Table 6: Correct and Incorrect predictions
(M) and the radiologist (R) when the lat- of the model (M) when the radiologist (R)
ter is sure
is unsure
M
R
Sure

Agree

Disagree

19

6

M
R
Unsure

[Domain-Specific
Representations
are Important.] There seems to be a
reversal in preference from the descriptive
baseline to the inductive explanation when
going from visual to textual explanations.
This is against a backdrop of a high
frequency of preference for visual repre-

Correct

Incorrect

4

1

sentations (21/30 instances). We believe
several factors to be involved here. First,
radiological diagnosis is largely image-based.
This results in a natural preference for visual
representations. However, the fact that the
baseline of a simple lung segmentation is
preferred over the saliency maps suggests
8

Explainable Models for COVID-19

4. Concluding Remarks

that the latter do not correspond in any
meaningful way. This is not the case with
textual explanations that use radiologically
meaningful features: hence the greater
preference for the inductive explanation
over the tabulation of descriptive statistics.
Taken together, the results highlight the
need to construct explanations in clinically
meaningful terms. This supports a finding
by Arun et al. (2020) that direct presentation of GradCAM visualisation may not
be ‘trustworthy’, or relevant in a medical
context.

In this paper, we have examined the use of
a hierarchical neural-symbolic approach for
diagnosis of COVID-19 from chest X-rays.
Our findings suggest that it is possible to
construct hierarchical models with accuracies comparable to end-to-end models, with
additional benefits arising the ability to construct visual and textual explanations using
the neural and symbolic stages of the approach. We have also presented some evidence of an assessment of these explanations.
Specifically, we find that while explanations
are relevant, they are better than a simple
“baseline explanation” only for the symbolic
explanation that is a simple logical description using domain-specific concepts. This
does not mean visual explanations are not
meaningful: rather that the specific use of
saliency maps is not useful.

[“Simpler” Explanations are Preferable.] We think the apparent preference for
visual baseline and textual inductive explanations over the visual inductive explanation
also indicate an underlying preference for
simpler explanations. The visual baseline
is simply a delineation of the lungs; and
the preferred inductive textual explanation
is a small set of conditions leading to the
diagnosis. In contrast, the saliency maps
and tabulations of probabilities contains
significantly more detail, most of which
is probably not thought to be clinically
relevant.
While the lung-segmentation
image does not constitute any kind of
explanation for the diagnosis of the model,
it is not entirely irrelevant, since radiologists
do look at lung-appearance during diagnosis.

A focus on explanations and keeping the
clinician-in-the-loop is crucial for deployment of models as clinical assistants, so that
clinicians gain trust in the decision of these
machines, and possibly make better judgements. The deployment of quick and reliable
model-based tools is becoming increasingly
important in the context of COVID-19, as it
threatens to overwhelm medical practitioners
in countries with limited medical expertise.
It has been repeatedly shown in machine
learning, that accuracy of models depends
crucially on the representation used, and
that the acceptance of machine-learnt models depends crucially on whether the models
are able to capture meaningful mechanisms.
Future work in this area needs to focus therefore on the domain-specific features to identify automatically from image-data, and how
to develop convey the concepts identified to
the radiologists in a manner in which they
can assess clinical significance. We think this
can only be achieved by a continuous dia-

[Models Can Assist.] Our results in
Tables 5 and 6 suggest that the model is not
simply a clone of the radiologist, and could
be helpful in cases where the diagnosis is
not clear-cut. This opens up the possibility
of using such systems as a second opinion
when making diagnoses.

9

Explainable Models for COVID-19

logue between clinicians and developers of
machine-learning models.
Acknowledgements
The authors would like to thank the radiologist from NHS, UK for helping us carry out
this project.

10

Explainable Models for COVID-19

References

patient monitoring using deep learning ct
image analysis, 2020.

Alejandro Barredo Arrieta, Natalia Dı́azRodrı́guez, Javier Del Ser, Adrien Ben- Stefan Jaeger, Sema Candemir, Sameer Annetot, Siham Tabik, Alberto Barbado, Saltani, Yı̀-Xiáng Wáng, Pu-Xuan Lu, and
vador Garcı́a, Sergio Gil-López, Daniel
George Thoma. Two public chest x-ray
Molina, Richard Benjamins, Raja Chatila,
datasets for computer-aided screening of
and Francisco Herrera. Explainable arpulmonary diseases. Quantitative imaging
tificial intelligence (xai): Concepts, taxin medicine and surgery, 4:475–7, 12 2014.
onomies, opportunities and challenges todoi: 10.3978/j.issn.2223-4292.2014.11.20.
ward responsible ai, 2019.
Md. Rezaul Karim, Till Döhmen, Dietrich Rebholz-Schuhmann, Stefan Decker,
Nishanth Arun, Nathan Gaw, Praveer
Michael Cochez, and Oya Beyan. DeepSingh, Ken Chang, Mehak Aggarwal,
covidexplainer: Explainable covid-19 diagBryan Chen, Katharina Hoebel, Sharut
nosis based on chest x-ray images, 2020.
Gupta, Jay Patel, Mishka Gidwani,
Julius Adebayo, Matthew D. Li, and
Zachary Chase Lipton. The mythos of model
Jayashree Kalpathy-Cramer. Assessing
interpretability. CoRR, abs/1606.03490,
the (un)trustworthiness of saliency maps
2016.
URL http://arxiv.org/abs/
for localizing abnormalities in medical
1606.03490.
imaging, 2020.
Arpan Mangal, Surya Kalia, Harish RajKhalid El Asnaoui and Youness Chawki.
gopal, Krithika Rangarajan, Vinay NamUsing x-ray images and deep learnboodiri, Subhashis Banerjee, and Chetan
ing for automated detection of coronArora. Covidaid: Covid-19 detection usavirus disease. Journal of Biomolecuing chest x-ray, 2020.
lar Structure and Dynamics, 0(0):1–12,
2020. URL https://doi.org/10.1080/ Randolph A. Miller, Harry E. Pople,
07391102.2020.1767212.
and Jack D. Myers.
Internist-i, an
experimental
computer-based
diagDana Harry Ballard and Christopher M.
nostic consultant for general interBrown. Computer Vision. Prentice Hall
nal medicine.
New England Journal
Professional Technical Reference, 1st ediof Medicine,
307(8):468–476,
1982.
tion, 1982. ISBN 0131653164.
doi:
10.1056/NEJM198208193070803.
URL
https://doi.org/10.1056/
Joseph Paul Cohen, Paul Morrison, Lan
NEJM198208193070803. PMID: 7048091.
Dao, Karsten Roth, Tim Q Duong, and
Marzyeh Ghassemi. Covid-19 image data Pranav Rajpurkar, Jeremy Irvin, Kaylie
collection: Prospective predictions are the
Zhu, Brandon Yang, Hershel Mehta, Tony
future, 2020.
Duan, Daisy Ding, Aarti Bagul, Curtis
Langlotz, Katie Shpanskaya, Matthew P.
Ophir Gozes, Maayan Frid-Adar, Hayit
Lungren, and Andrew Y. Ng. Chexnet:
Greenspan, Patrick D. Browning, Huangqi
Radiologist-level pneumonia detection on
Zhang, Wenbin Ji, Adam Bernheim, and
chest x-rays with deep learning, 2017.
Eliot Siegel. Rapid ai development cycle
Abhishek
for the coronavirus (covid-19) pandemic: Ramprasaath R. Selvaraju,
Das, Ramakrishna Vedantam, Michael
Initial results for automated detection &
11

Explainable Models for COVID-19

Cogswell, Devi Parikh, and Dhruv
Batra. Grad-cam: Why did you say
that?
visual explanations from deep
networks via gradient-based localization.
CoRR, abs/1610.02391, 2016.
URL
http://arxiv.org/abs/1610.02391.
Edward H. Shortliffe and Bruce G.
Buchanan.
A model of inexact reasoning in medicine.
Mathematical
Biosciences, 23(3):351 – 379, 1975. ISSN
0025-5564.
doi:
https://doi.org/10.
1016/0025-5564(75)90047-4. URL http:
//www.sciencedirect.com/science/
article/pii/0025556475900474.
Mesut Toğaçar, Burhan Ergen, and Zafer
Cömert.
Covid-19 detection using
deep learning models to exploit social
mimic optimization and structured
chest x-ray images using fuzzy color
and stacking approaches.
Computers
in Biology and Medicine, 121:103805,
2020.
ISSN 0010-4825.
URL http:
//www.sciencedirect.com/science/
article/pii/S0010482520301736.
Linda Wang and Alexander Wong. Covidnet: A tailored deep convolutional neural
network design for detection of covid-19
cases from chest x-ray images, 2020.
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and
Ronald Summers. Chestx-ray8: Hospitalscale chest x-ray database and benchmarks on weakly-supervised classification
and localization of common thorax diseases. arXiv:1705.02315, 05 2017.

12

Explainable Models for COVID-19

Appendix A. COVIDr Dataset

3. P(ASO) <= 0.5
&& P(Missing GGO/ASO) <= 0.5
&& P(Infiltration) <= 0.406
&& P(Emphysema)>0.127
&& P(Edema) <= 0.085

The details of the COVIDr dataset are seen
in Table 7. An example of the way we parse
the dataset is shown in Fig. 4.

4. P(ASO) <= 0.5
&& P(Missing GGO/ASO) <= 0.5
&& P(Infiltration) <= 0.406
&& P(Emphysema) <= 0.127

Appendix B. Model Details
The R- and S-Model architectures are shown
in Fig. 5. Details of conversion of the Neural
Radiology Model (R-model) output to Symbolic Tree (T -model) input are shown in Table 8.
To check for the optimal maximum leaf
nodes, we plot (Fig. 6a) the test accuracy
against the number of leaf nodes. We also
try to find the optimal maximum depth of
the tree, as shown in Fig. 6b.
The tree obtained from the Symbolic
model that is described in Section 1 is shown
in Fig. 7. The extracted rules are shown
below.
Rules for COVID-Positive
1. P(ASO) > 0.5

Appendix C. Web-Based interface
for collecting
feedback
A practising radiologist from the NHS, UK
provides utility assessments for 30 Chest Xray images sampled from the held-out test
set (COVID-19 positive - 5, healthy - 5, tuberculosis - 5 and pneumonia - 5) , along
with the COVID-19 diagnosis made by our
model and all the explanations as stated in
Section 2.3. We provide a web-based interface and ask the following questions in the
given order :
1. Prior to revealing the diagnosis of a
given Chest X-ray image made by the
model (true label is never revealed) or
the corresponding explanations, we ask
for the radiologist’s diagnosis. We then
reveal the diagnosis made by the model.

2. P(ASO) <= 0.5
&& P(Missing GGO/ASO) <= 0.5
&& P(Infiltration) > 0.406
&& P(Emphysema) <= 0.122
3. P(ASO) <= 0.5
&& P(Missing GGO/ASO) <= 0.5
&& P(Infiltration) <= 0.406
&& P(Emphysema) > 0.127
&& P(Edema) > 0.085

2. We ask the radiologist to rate the visual
quality of the image as Low, Medium or
High.
3. The visual explanations(VisInd, VisDed ) provided by our model are then
displayed side by side along with the
Chest X-ray. We then ask if each of
these explanations individually are Useful, Somewhat Useful or Not Useful. We
also ask whether the VisInd explanations was more useful, VisDed explanations was more useful or if both are the
same.

Rules for COVID-Negative
1. P(ASO) <= 0.5
&& P(Missing GGO/ASO) <= 0.5
2. P(ASO) <= 0.5
&& P(Missing GGO/ASO) <= 0.5
&& P(Infiltration) > 0.406
&& P(Emphysema)>0.122
&& P(Emphysema) <= 0.122
13

Explainable Models for COVID-19

Figure 4: Parsing the COVIDr dataset
Table 7: The COVIDr dataset consists of 7 CXR finding labels for COVID-19 positive
images. The number of positive instances of these labels are shown in the accompanying
table.
CXR-Finding

Positive Instances

None
Ground glass opacity
Bilateral patchy air-space opacification
Bilateral symmetrical air-space opacification
Bilateral peripheral air-space opacification
Predominantly unilateral air-space opacification Rt
Predominantly unilateral air-space opacification Lt

44/245 (17.95%)
145/245 (59.18%)
30/245 (12.24%)
18/245 (7.34%)
54/245 (22.04%)
36/245 (14.69%)
28/245 (11.4%)

Table 8: Conversion of Neural Radiology
Model output to Symbolic Tree input
Max Prob.
ASO
GGO
ASO GGO
No ASO GGO
Missing ASO GGO

ASO

GGO

1
0
1
0
?

0
1
1
0
?

5. Finally, we ask which explanations were
more useful - Visual, Symbolic or both
were the same.
Snippets of the web interface are shown in
Fig. 8

Appendix D. Additional Results
D.1. COVIDr Baselines

We benchmark the COVIDr dataset by training different variation to predict the morphological symptoms. The results are shown in
4. The symbolic explanations(SymInd, Table 9.
SymDed ) provided by our model are
displayed side by side along with the
D.2. Symbolic Model Baselines
Chest X-ray. We then ask if each of
these explanations individually are The results of our symbolic model are shown
Useful, Somewhat Useful or Not Use- in Table 10. We compare these results with
ful. We also ask whether the SymInd a single end-to-end, as well as other treeexplanations was more useful, SymDed based models. We find that the loss in acexplanations was more useful or if both curacy in our symbolic model is not significant in comparison with the black box model,
are the same.
14

Explainable Models for COVID-19

(b) S-Model
(a) R-Model

Figure 5: Neural model architectures

(a) Accuracy vs. Max. Leaf Nodes

(b) Accuracy vs. Max. Depth

Figure 6: Optimal Leaf Nodes and Depth
and the tree model additionally provides in- power as with other tree ensembling methods
terpretable explanations. We also see that like Random Forest and XGBOOST.
our decision tree has very similar predictive

15

Explainable Models for COVID-19

Figure 7: Tree used to generate textual inductive explanations

Table 9: Results on the Radiology Model.
(Sym– Model using embeddings from the Smodel. PT– VGG16 uses pre-trained weights
from the end-to-end model)
Model
VGG-16
VGG-16 + Sym
VGG-16 + Sym (PT)

Accuracy

AUC

0.83
0.86
0.88

0.96
0.97
0.99

Table 10: Results on the COVID-19 prediction
Model
End-to-End
NS-RF
NS-XGB
NS-DTree

Accuracy

F1

0.985 ± 0.007
0.973 ± 0.009
0.973 ± 0.009
0.973 ± 0.009

0.95
0.915
0.930
0.915

16

Explainable Models for COVID-19

Figure 8: Snippet of Interface that the Radiologist uses to evaluate explanations and baselines

17

