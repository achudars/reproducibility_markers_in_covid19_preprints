arXiv:2012.02164v2 [cs.SI] 23 Dec 2020

Predicting Misinformation and Engagement in COVID-19
Twitter Discourse in the First Months of the Outbreak
MIRELA SILVA, University of Florida, USA
FABRÍCIO CESCHIN, Federal University of Paraná, Brazil
PRAKASH SHRESTHA, University of Florida, USA
CHRISTOPHER BRANT, University of Florida, USA
JULIANA FERNANDES, University of Florida, USA
CATIA S. SILVA, University of Florida, USA
ANDRÉ GRÉGIO, Federal University of Paraná, Brazil
DANIELA OLIVEIRA, University of Florida, USA
LUIZ GIOVANINI, University of Florida, USA
Disinformation entails the purposeful dissemination of falsehoods towards a greater dubious agenda and the
chaotic fracturing of a society. The general public has grown aware of the misuse of social media towards these
nefarious ends, where even global public health crises have not been immune to misinformation (deceptive
content spread without intended malice). In this paper, we examine nearly 505K COVID-19-related tweets
from the initial months of the pandemic to understand misinformation as a function of bot-behavior and
engagement. Using a correlation-based feature selection method, we selected the 11 most relevant feature
subsets among over 170 features to distinguish misinformation from facts, and to predict highly engaging
misinformation tweets about COVID-19. We achieved an average F-score of at least 72% with ten popular
multi-class classifiers, reinforcing the relevance of the selected features. We found that (i) real users tweet
both facts and misinformation, while bots tweet proportionally more misinformation; (ii) misinformation
tweets were less engaging than facts; (iii) the textual content of a tweet was the most important to distinguish
fact from misinformation while (iv) user account metadata and human-like activity were most important to
predict high engagement in factual and misinformation tweets; and (v) sentiment features were not relevant.
CCS Concepts: • Security and privacy → Human and societal aspects of security and privacy; Social
network security and privacy.
Additional Key Words and Phrases: Engagement, misinformation, social media, machine learning
This material is based upon work supported by the National Science Foundation under grants 2028734 and 1662976, Google
Security and Privacy Research Award and University of Florida SeedFund OR-DRPD-ROF2020.
Authors’ addresses: Mirela Silva, msilva1@ufl.edu, Electrical & Computer Engineering, University of Florida, P.O. Box 116200,
Gainesville, FL, USA, 32611; Fabrício Ceschin, fjoceschin@inf.ufpr.br, Federal University of Paraná, Cel. Francisco Heráclito
dos Santos, 100, Curitiba, Paraná, Brazil, 81530-000; Prakash Shrestha, prakash.shrestha@ufl.edu, Electrical & Computer
Engineering, University of Florida, P.O. Box 116200, Gainesville, FL, USA, 32611; Christopher Brant, g8rboy15@ufl.edu,
Electrical & Computer Engineering, University of Florida, P.O. Box 116200, Gainesville, FL, USA, 32611; Juliana Fernandes,
juliana@jou.ufl.edu, College of Journalism and Communications, University of Florida, P.O. Box 118400, Gainesville, FL, USA,
32611; Catia S. Silva, catiaspsilva@ece.ufl.edu, Electrical & Computer Engineering, University of Florida, P.O. Box 116200,
Gainesville, FL, USA, 32611; André Grégio, gregio@inf.ufpr.br, Federal University of Paraná, Cel. Francisco Heráclito dos
Santos, 100, Curitiba, Paraná, Brazil, 81530-000; Daniela Oliveira, daniela@ece.ufl.edu, Electrical & Computer Engineering,
University of Florida, P.O. Box 116200, Gainesville, FL, USA, 32611; Luiz Giovanini, lfrancogiovanini@ufl.edu, Electrical &
Computer Engineering, University of Florida, P.O. Box 116200, Gainesville, FL, USA, 32611.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2020 Association for Computing Machinery.
XXXX-XXXX/2020/11-ART111 $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111

111:2

Silva et al.

ACM Reference Format:
Mirela Silva, Fabrício Ceschin, Prakash Shrestha, Christopher Brant, Juliana Fernandes, Catia S. Silva, André
Grégio, Daniela Oliveira, and Luiz Giovanini. 2020. Predicting Misinformation and Engagement in COVID19 Twitter Discourse in the First Months of the Outbreak. 37, 4, Article 111 (November 2020), 26 pages.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

INTRODUCTION

Disinformation refers to false or deceptive content distributed via any communication medium
(e.g., word-of-month, print, Internet, radio, broadcast) by an adversary who aims to hurt a target
(usually a country, political party, or a community) via the spread of propaganda and promotion
of societal division, thus casting doubt in democratic processes, government institutions, and on
science as a whole. Over the past few years, our society has grown wearily aware of the highly
polarized schism that has developed beyond the context of mere political discourse. The perceived
extremities of our thoughts and opinions are now intimately meshing with falsehoods and outright
lies, calling into question the integrity of our government agencies’, political representatives’, and
our own individual handling of public health crises, such as the COVID-19 pandemic [51].
The pervasive spread of disinformation we have been witnessing today is not a new phenomenon.
The Active Measures disinformation campaigns employed during the Cold War [7, 41] bear a
disturbing resemblance to what we are witnessing today. We are immersed in an environment of
highly polarized, scoop-hungry media, with some outlets spreading demonstrably false information.
Twitter and Facebook have become the 21st century version of Cold War balloons spreading
disinformation beyond the Iron Curtain. The Active Measures disinformation campaigns also
leveraged science as an indirect target to harm the reputation of the US and its Western allies. One
of the first measures, leveraging forged documents and false testimonies, alleged that the US army
in Korea was using bacteriological weapons against the enemy front [7]. Nearly a decade later,
another disinformation campaign was launched when the US began researching the neutron bomb,
capable of highly localized impact—a would-be game-changer for a potential conflict between
the US and the Soviet Union; the goal, according to Rid [41] was to prevent the development of
the technology, pitch the European allies against the US, and buy time for the Soviet Union to
catch up on their own research. Within a period of weeks in 1977, the CIA reported ∼3K negative
stories about the neutron bomb in the press [41], and the research initiative was successfully
paralyzed as a result of decreased domestic and NATO support. The 1980s then brought a series of
near-comical science-related disinformation campaigns, such as the CIA’s factory for weaponized
mosquitoes [41], or the “AIDS made in the USA” narrative, spread by the KGB [7] and which
appeared more than forty times in the international press [54].
The 21st century disinformation campaigns have largely leveraged the affordances of social media
platforms [5, 36], which allow narratives to spread quickly, complicating attribution and using bot
accounts to automate delivery and amplify the reach of disinformation narratives. Following in the
steps of history, during the markedly turbulent times innate to a pandemic, it is sadly unsurprising
that, in June of 2020, the Associated Press reported that Russian intelligence services are behind the
spread of disinformation about the coronavirus [53]. However, COVID-19-related misinformation
is also coming from domestic sources, as we have seen politicians, pundits, and personalities
pushing disinformation narratives [6] that may prevent society from controlling the spread of
the coronavirus, potentially increasing the number of unnecessary deaths. With an upcoming
vaccine [37], disinformation will unequivocally be used to discredit its effectiveness, preventing
efficient immunization and fueling further hyperpartisanship.
In this paper, we aim to investigate the relationship between misinformation and user engagement
in COVID-19-related tweets. We opt to use the term misinformation instead of disinformation
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak

111:3

because the latter implies purposeful malice. When false information is spread unintentionally—for
example, when a user is successfully deceived by a piece of disinformation, and naively retweets
the falsehood—this spread of deceptive content is instead referred to as misinformation. For the
remainder of this paper, we will use the term misinformation to refer to tweets spreading deceptive
content, even though some misinformation tweets might have been created with malice.
Using a dataset [17] originally containing over 3M tweets (later decreased to nearly ∼504K tweets
after pre-processing) with the labels of fact and misinformation, collected during February and
March 2020, just prior to the World Health Organization’s (WHO) recognition of COVID-19 as a
pandemic, we sought to answer the following research questions:
• RQ1: Do Twitter bot accounts spread more misinformation than real accounts?
• RQ2: Are misinformation tweets more engaging than factual tweets?
• RQ3: Which features are most relevant to distinguish between factual and misinformation
tweets?
• RQ4: Which features are most relevant to predict high engagement in misinformation tweets?
• RQ5: Which features are most relevant to predict high engagement in factual tweets?
To do so, we first investigated bot-like behavior in the dataset using the Tweetbotornot21 a
machine learning-based tool that estimates the likelihood of a given Twitter account being a bot.
Then, we measured engagement as the summation of # likes and # of retweets and proceeded to label
tweets as high or low engagement based on the median value of this combined engagement metric.
After completing textual preprocessing on the tweets themselves, we submitted our dataset to
several multi-class classifiers to make predictions about tweet engagement. Our Gradient Boosting,
Multinomial Naive Bayes, and Random Forest classifiers achieved the highest performance for
RQs 2, 4, and 5, respectively (average F-scores of 76.59%, 71.99%, and 73.39%), whereas the baseline
approach (a rule-based model generating random predictions) for all three RQs hovered close to an
F-score of 50%. We found that (i) real user accounts tweet both facts and misinformation, while bot
accounts were more likely to tweet misinformation; (ii) misinformation tweets were less engaging
than factual tweets; (iii) features relating to the textual content of a tweet (e.g., authenticity score
indicating honesty vs. deceptiveness of the text, percentage of cognitive process- and death-related
words) were the most important to help distinguish between facts and misinformation while (iv)
user account metadata (e.g., whether the user’s account is verified, presence of both banner image
and URL in the user profile) and the presence of bot-like activity were most relevant to predict high
engagement in both factual and misinformation tweets; (v) sentiment features were not relevant to
detecting misinformation nor predicting high engagement; and (vi) presence of images in the tweet
was relevant both for distinguishing misinformation and predicting engagement.
Research on COVID-19 disinformation is in its infancy as most relevant research papers on
COVID-related misinformation are pre-prints focused on the detection, prevalence and sentiments
of COVID-19 misinformation in social media [10, 23, 34, 43, 46, 49, 57]). Most works [2, 10, 23, 34, 46,
49, 57], especially on Twitter users’ perception of the pandemic [42, 50]. To the best of our knowledge,
none of these preprint studies focused on measuring users’ engagement and discriminating features
for high/low engagement in COVID-19 related misinformation and information tweets, as proposed
in this paper. This paper makes the following contributions:
(1) Investigation of factual/misinformation tweets for engagement: we analyze twitter discourse on
COVID-19 to discover whether misinformation tweets are more engaging than factual tweets.
(2) Identification of discriminating features for factual/misinformation: we identify discriminating
characteristics of a tweet and its author that can distinguish factual and misinformation tweets
on COVID-19.
1 www.tweetbotornot2.mikewk.com

, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:4

Silva et al.

(3) Identification of discriminating features for high/low engaging tweets: we identify discriminating
features of a tweet and its author that can predict high/low engaging tweets (both factual and
misinformation) on COVID-19.
(4) Release of dataset: we will release our dataset2 containing a total of 179 features and labels obtained from several analyses (factual/misinformation-, bot-hunter-, sociolinguistic-, sentiment-,
and engagement-focused) for the ∼505k COVID-19 related tweets originally made available by
Elhadad et al. [17].
Though technological advancements enable better disinformation campaigns that can reach
more people faster and complicate attribution, better technology also gives society better tools for
defense against this threat: faster exposure, detection of disinformation seeds, and better reach for
awareness campaigns. Understanding how automated solutions can distinguish dis/misinformation
and what makes for engaging dis/misinformation can aid future defense approaches. Furthermore,
insights about features predicting engagement in health-related topics in short social media texts
can be leveraged by public health organizations and officials to convey important public health
information to society.
Our paper is organized as follows. Section 2 reviews prior works on misinformation and public
health, and considers the added value of our work. Section 3 discusses our dataset, its curation
process, and the preprocessing and feature extraction steps taken. Section 4 then analyses our
cleaned dataset’s results via statistical tests and machine learning algorithms. Section 5 discusses
the take-aways and limitations of our analyses, as well as the future directions for this line of work.
Section 6 concludes the paper.
2

RELATED WORKS

Since the pandemic outbreak in late 2019, researchers have started conducting COVID-19-related
studies, with the vast majority of them made available as pre-prints awaiting peer-review. In this
section, we provide a description of literature (mostly pre-prints) relevant to our work, including
Twitter datasets on COVID-19 released in 2020.
2.1

Misinformation Analysis

Various recent researchers have explored the presence, prevalence, and sentiment of misinformation
on social media of COVID-19 discourse [2, 10, 23, 34, 46, 49, 57], user’s susceptibility and psychological perceptions on this public health crisis [42, 50], the predictors of fake news [4, 24] and the role
of bots [34, 57] on spreading COVID-19 misinformation. For instance, Sharma et al. [46] examined
Twitter data to identify misinformation tweets leveraging state-of-the-art fact-checking tools (e.g.,
Media Bias/Fact Check, NewsGuard, and Zimdars) along with topics, sentiments, and emerging
trends in the COVID-19 Twitter discourse. Al-Rakhami and Al-Amri [2] proposed an ensemblestacking framework to identify misinformation tweets. In their COVID-19 tweet dataset, 7̃0% of
the tweets were identified as uncredible by human annotators. Singh et al. [49] found that misinformation and myths on COVID-19 are discussed at a lower volume than other pandemic-specific
themes on Twitter. They also concluded that information flow on Twitter shows a spatio-temporal
relationship with the infection rates. Jiang et al. [25] examined the usage of hashtags in 2.3M
tweets in the United States, and observed that the American public frames the pandemic as a core
political issue. Cinelli et al. [10] went beyond Twitter and analyzed data from four other social
media platforms: Instagram, YouTube, Reddit, and Gab, finding different volumes of misinformation
on each platform. Although several works have been conducted on COVID-19 misinformation,
2 In

order to comply with Twitter’s Terms of Service (https://developer.twitter.com/en/developer-terms/agreement-andpolicy), we omitted the tweet’s raw text, as well as any features that could potentially reveal the users’ identity.
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak

111:5

to the best of our knowledge, none of these studies focused on measuring users’ engagement
and discriminating features for high/low engagement in COVID-19 related misinformation and
information tweets, as proposed in this paper.
Several works [18, 34, 57] have studied the prevalence of COVID-19 misinformation on Twitter
and characterized the role of bots in spreading misinformation, similar to our study’s RQ1. The
authors found that the fraction of bots sharing misinformation was much higher than those sharing
factual information, which is inline with our findings (see Sec. 4). Memon et al. [34] classified
the users into informed (sharing factual information), misinformed (sharing misinformation), and
irrelevant groups; they also conducted sociolinguistic analysis leveraging the LIWC program.
Although both informed and misinformed users’ tweets had a highly negative tone, the authors
did not find any significant differences in the emotional tone among these communities. In this
work, we go beyond exploring the role of bots in spreading misinformation and sociolinguistic
analysis within informed/misinformed communities, and seek the characteristics and features of
a tweet and its author that can distinguish both factual/misinformation and high/low engaging
tweets about COVID-19.
Apuke and Omar [4] and Islam et al. [24] used online surveys to model predictors of fake
news dissemination on social media. While Apuke and Omar found that altruism, information
sharing and seeking, socialization, and pass time predict fake news sharing (altruism being the
strongest individual predictor), Islam et al. found that self-promotion, entertainment, and lack of
self-regulation predict misinformation sharing (deficient of self-regulation being the strongest
predictor). Contrary to these works that investigated the users’ personal characteristics that drive
the users sharing misinformation on social media, we focus on determining what characteristics of
misinformation itself make it most influential and engaging.
Huang et al. [23] analyzed 67.4M tweets and observed that news media and government officials
tweets are highly engaging, and that most discussion on misinformation originates from the United
States. Unlike this work that explored the kind of users involved in and the location of dissemination
of highly engaging tweets, we aim to identify the set of tweet and user characteristics that can
predict factual/misinformation tweets and engagement with factual/misinformation tweets.
Roozenbeek et al. [42] conducted a large international survey in the UK, USA, Ireland, Spain, and
Mexico to investigate susceptibility to misinformation on COVID-19 and its influence on key healthrelated behaviors. Their survey showed that the majority of people self-report low susceptibility
to misinformation; however, a substantial segment of the public were consistently susceptible to
certain misinformation claims. They also found a clear association of belief in misinformation
with vaccine hesitancy and reduced self-protective measures. Similar association was found in the
survey of Swami and Barron [50], where they conducted a survey with a nationally representative
sample of 530 adult users in the United Kingdom.
Few studies [9, 44] have conducted exploratory research on visual misinformation videos. Brennen
et al. [9] conducted a small preliminary study on 96 visual misinformation (marked by independent
professional fact-checkers) related to COVID-19 and found that authoritative agency, virulence,
medical efficacy, intolerance, prophecy, and satire are the most common framings of expressing
misinformation amongst visual media. Serrano et al. [44] introduced an NLP methodology for
detecting COVID-19 related misinformation videos on YouTube by processing through the user
comments on videos. Unlike these works, our work focuses on tweet texts rather than on video;
however, we did find that the presence of an image in the tweet was a top feature in distinguishing
misinformation from factual information, and in predicting engagement for both factual and
misinformation tweets.

, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:6

2.2

Silva et al.

COVID-19 Twitter Datasets

Several datasets pertaining to COVID-19 discourse on Twitter have been released in 2020. Examples
include the TweetsCOV19 [15] and Coronavirus Tweets [28] datasets, which contain metadata
and sentiment scores for 8M and 600M English-language tweets between the periods of October
2019–April 2020 and March 2020–present (updated daily), respectively. In addition, [29] contains
the IDs of English-language tweets originated from India during the first week of each of the
four phases of nationwide lockdowns initiated by its government. Some other similar datasets
can be found on Kaggle, a popular online data science community. For example, a dataset3 with
metadata and text for nearly 14.5M multilingual tweets from April 2020, and another dataset4 with
text, metadata, and sentiment labels for 45k tweets between March 2020 to April 2020. Highly
specific datasets can also be found, e.g., metadata and text for ∼22k tweets with the #CovidVaccine
hashtag5 tweeted in the period of August 2020 until present (updated on a daily basis). Some of
these datasets publicly release only the IDs of the collected tweets in order to comply with Twitter’s
Terms of Service. (For more information on different COVID-19 datasets, please refer to surveys
from Shuja et al. [48] and Latif et al. [30].) In line with this trend and with the goal to foster future
research on COVID-19-related misinformation, we will also make available our dataset of nearly
505k tweets from February 2020 to March 2020, along with all 179 features.
3

COVID MISINFORMATION DATASET: PREPROCESSING AND FEATURE
ENGINEERING

This section details our process for choosing the COVID-19-FAKES dataset [17] used in our analyses,
along with the data preprocessing steps completed, and the steps taken to extract metadata, bot-like
behavior, sociolinguistic, and sentiment features from the dataset.
3.1

Dataset Selection

Several Twitter datasets pertaining to COVID-19 discourse have been released in the last few
months and some have been specifically designed for the analysis of misinformation involving
the pandemic. Most of the misinformation datasets include ground truth labels of true/factual
information and fake/misinformation for tweets, replies, and news articles (often included in
tweets via URLs) through automatic annotation via machine learning. Examples of such a dataset
are presented in Table 1 along with their size.
Table 1. Twitter datasets for COVID-19 misinformation analysis available in related works.
Dataset

Misinformation tweets

Factual tweets

Total

CoAID [12]
ReCOVery [60]
HealthStory [13]
HealthRelease [13]
COVID-19-FAKES [16, 17]

10,900 (6.8%)
26,418 (18.8%)
94,342 (24.6%)
22,247 (47.0%)
61,711 (2.0%)

149,343 (93.2%)
114,402 (81.2%)
289,731 (75.4%)
25,091 (53.0%)
2,985,399 (98.0%)

160,243
140,820
384,073
47,338
3,047,110

We decided to use the COVID-19-FAKES dataset introduced by Elhadad et al. [17] in our study
because (i) it is, to the best of our knowledge, the largest Twitter dataset currently available for
COVID-19 misinformation analysis in English-language tweets, and (ii) the automatic generation of
ground truth labels was extensively explored in the original study through several machine learning
(ML) models, feature extraction techniques, and performance metrics, with state-of-the-art results
reported by the authors. More specifically, the study by Elhadad et al. [17] was originally divided
3 https://www.kaggle.com/smid80/coronavirus-covid19-tweets-early-april
4 https://www.kaggle.com/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv
5 https://www.kaggle.com/kaushiksuresh147/covidvaccine-tweets

, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak

111:7

into two phases: the model-building phase, aimed to train and evaluate a binary classification model
for 13 different ML algorithms6 combined with seven different feature extraction techniques7 to
predict whether a given piece of information is fact or misleading, and the annotation phase,
where the trained model was used to assign ground-truth labels (factual information and
misinformation) to a set of 3,047,110 tweets related to COVID-19. In both phases, the data was
subject to the same preprocessing and feature engineering steps (see Sec. 3.4 for more details).
For phase one (training and evaluation), the authors collected a set of ground-truth information
related to COVID-19 by scraping the official websites of the WHO, UNICEF, and UN as sources of reliable information. Next, they enriched the collected ground truth with COVID-19 pre-checked facts
from various fact-checking websites, including snopes.com, washingtonpost.com/news/factchecker, and politifact.com. In total, there were 7,486 ground truth samples—1,602 (21.4%)
misleading information and 5,884 (78.6%) factual information—with about 75% of the samples
having up to 200 characters and up to 30 words. The sample was then split into 80% training and
20% test datasets, and the ML models were trained and evaluated using 5-fold cross validation with
12 performance metrics8 . The reported results are highly promising and suggest the validity of all
tested models (F-scores greater than 95%), with the Neural Networks, Decision Tree, and Logistic
Regression classifiers reaching superior overall performance. The authors also deployed a voting
ensemble method to generate ground truth labels based on the output of all tested classifiers.
For phase two (annotation), the trained models were then used to assign ground-truth labels to a
set of ∼3M English-language tweets containing at least one of 12 trending keywords9 about the
coronavirus pandemic chosen in the original study [17]. These tweets were continuously collected10
from February 04, 2020 (four days after the outbreak was declared a Public Health Emergency of
International Concern by the World Health Organization) to March 10, 2020 (one day before the
outbreak was declared a Pandemic).
The dataset released11 by the authors includes 3,047,110 tweet IDs along with the respective
ground truth labels (real/misleading) generated by each of the 13 classifiers tested via the seven
feature extraction techniques, as well as the voting ensemble method. We decided not to use the
labels assigned through the voting method because it may sometimes fail due to the contribution of
classifiers with poorer performance. Instead, we opted to use a voting scheme involving only Neural
Networks, Decision Tree, and Logistic Regression—the three classifiers with best performance
(F-scores greater than 99%)—with features extracted via TF, character level, and TF, respectively,
which outperformed the other techniques in each case.
3.2

Data Collection

After discarding repeated tweet IDs from the dataset, we found 3,046,993 unique tweet IDs. We then
collected these tweets by using the Twitter API, which also allowed us to obtain some metadata
6 Decision

Tree, k-Nearest Neighbor, Logistic Regression, Linear Support Vector Machines, Multinomial Naive Bayes,
Bernoulli Naive Bayes, Perceptron, Neural Network, Ensemble Random Forest, Extreme Gradient Boosting, Bagging
Meta-Estimator, AdaBoost, and Gradient Boosting.
7 Term Frequency (TF), Term Frequency Inverse Document Frequency (TF-IDF, at the unigram, bigram, trigram, N-gram,
character levels), and Word Embedding.
8 Accuracy, error rate, precision, recall, F1-score, area under the curve, specificity, geometric-mean, miss-rate, fall-out rate,
false discovery rate, false omissions rate.
9 “‘Coronavirus,” “Corona_virus,” “Corona-virus,” “Novel_Coronavirus,” “2019-nCoV,” “Novel-Coronavirus,” “NovelCoronavirus,” “2019_nCoV,” “nCoV,” “COVID-19,” “SARS-CoV-2,” “covid19.”’
10 The authors reported they missed collecting some tweets for some days due to technical issues (e.g., connection errors,
Internet problems, power issues) and limitations of the Twitter API (3,200 tweets per query) that was used to collect the
data in a real-time fashion.
11 https://github.com/mohaddad/COVID-FAKES/tree/COVID-FAKES-E
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:8

Silva et al.

related to the tweets themselves (e.g., language, lists of hashtags, symbols, user mentions, and URLs
included), the information about the users tweeting those tweets (e.g., name, profile description,
account date of creation, # of followers, # of friends, and the users’ engagement to those tweets
(e.g., # of retweets and # of likes). However, we were only able to retrieve data for 1,315,509
tweets (out of the 3,046,993 unique IDs). The remaining tweets were no longer available/accessible
by the time of the data collection, most likely because they had been deleted by either the user or
Twitter.
3.3

Data Filtering

In this paper, we focused on analyzing COVID-19 related misinformation in English-language
tweets, so we filtered out tweets written in any other languages. Although the dataset was alleged
to contain only English-language tweets, we also found tweets written in Portuguese, Spanish,
Japanese, and Thai, among other languages; this decreased the dataset to 1,201,481 tweets. Next, we
discarded retweets to eliminate duplicate data, decreasing the dataset to 587,153 tweets. Lastly, we
decided to discard the tweets for which we could not obtain information using the Tweetbotornot2
tool that checks for potential bot-like accounts; some of the accounts no longer existed at the time
of our analysis, while others were not accessible by the Tweetbotornot2 tool. This decreased and
finalized our sample dataset to 504,965 tweets, where 497,681 (98.6%) were factual tweets and 7,284
(1.4%) were misleading tweets according to our voting scheme.
3.4

Data Preprocessing

Before feature extraction, the collected tweets were subject to the following preprocessing steps:
• Slang lookup to convert slang words into proper English words (e.g., “b4” becomes “before,” “u”
becomes “you”).
• Numeric value processing to convert words (e.g., “one million”) or mix of words and numbers
(e.g., “1 million”) into numbers (e.g., 1,000,000).
• Emoji extraction to remove all emojis from the tweets.
Other typical NLP preprocessing steps, such as tokenization (to break down sentences into a list of
words), text normalization (removal of URLs, hashtags, mentions, and punctuations), removal of
stop words, and lemmatization (to reduce words to their root forms) were not performed, as both
LIWC and sentiment analysis packages can work with raw text.
3.5

Feature Extraction

From the cleaned dataset, we extracted a total of 178 features per tweet, including features derived
from the metadata (i.e., tweet- and user-related descriptors), containing information about the
behavior of the Twitter accounts in terms of bot-like activity (e.g., user tweet rate), and addressing
sociolinguistic (e.g., cognitive and structural components, such as formal and logical language) as
well as sentiment characteristics of the tweet texts.
3.5.1 Tweet Metadata, User Metadata, and Engagement. We extracted the following features from
the collected Twitter metadata:
• Nine tweet-related features, including # of likes, # of retweets, # of hashtags, # of symbols, #
of user mentions (e.g., @WHO), presence of links/URLs (yes/no), presence of images (yes/no),
source (e.g., iPhone, Android, Web app), and quote of another tweet (yes/no).
• Eleven user-related features, including # of followers, # of friends, # of lists, account age, # of
favorited tweets, account type (verified or not), # of tweets, presence of profile image (yes/no),
presence of background image (yes/no), presence of banner image (yes/no), presence of URL in
profile (yes/no).
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak

111:9

• Four engagement-related features, including three normalized engagement metrics plus a
binary label of engagement level (high/low).
As mentioned, we included three engagement metrics in our analysis: (1) # of likes, (2) # of
retweets, and (3) “combined engagement” (# of likes plus # of retweets, i.e., the summation of the
two first metrics). Note that all engagement metrics were log-normalized following [3] because
they were highly skewed, with more weight in the left tail of the distribution. According to Aldous
and Jansen [3], engagement can be captured in a 4-level scale where lower levels indicate less public
expressions of engagement. Likes and retweets encompass different levels of user engagement,
with the former being classified as level-2 and the latter as level-4 (the highest level). The reasoning
behind this is intuitive: while liking a tweet does involve some degree of engagement because the
user publicly exposes their preference about a certain topic, retweeting a given content displays a
higher level of engagement because the user deliberately seeks to amplify the reach of that content
by disseminating it through different networks using their own feed—in other words, a retweet
is the most public level of engagement that can be observed for Twitter. The most private level
of engagement (level-1) includes viewing content posted by other users, while commenting on
content is considered level-3 as it involves more public exposure than simply liking it but less
exposure than sharing (retweeting) it. As the collected dataset does not have level-1 and level-3
engagement metrics, we decided to analyze likes (level-2) and retweets (level-4) separately and
then combined. Then, following a study by Aldous and Jansen [3], we divided the misinformation
and factual tweets, respectively, into the top and bottom 50% quantiles based on our combined
engagement metric (log-normalized # of likes plus # of retweets). The tweets belonging to the
bottom quantile were assigned a low engagement label, while the tweets located at the top quantile
received a high engagement label.
3.5.2 Bot-hunter Analysis. To check for potential bot-like accounts, we queried the usernames in
the dataset through the Tweetbotornot2 API, which estimates the likelihood of a given Twitter
account being a bot using a supervised classifier based on features of network, user, content,
and time intervals between tweets. Tweetbotornot2 has a competitive performance compared to
BotoMeter [14], which is a state-of-the-art algorithm for bot detection. Following the authors
in [34], we adopted a confidence threshold of 0.75 in bot-likelihood to distinguish between real and
bot accounts. We then created a binary feature to flag bot-like behavior for every tweet from an
account with a probability ≥ 0.75. We also stored the original bot probability for all tweets using
a numerical feature. In addition, we collected 55 other features used by Tweetbotornot2 in the
bot-hunter analysis (e.g., user tweet rate, whether user has profile and background images, tweet
word count variation).
3.5.3 Sociolinguistic Analysis. We performed a sociolinguist analysis on the collected tweets using
the Linguistic Inquiry and Word Count (LIWC) software (version 2015) [39]. This tool estimates
the rate at which certain emotions, moods, and cognition (e.g., analytical thinking) are present in a
piece of text based on word counts (e.g., the words “nervous,” “afraid,” and “tense” are counted as
expressing anxiety). More specifically, we extracted 93 features related to emotional, cognitive, and
structural components from the collected tweets, including:
• Four language metrics: total number of words, average number of words per sentence, number
of words containing more than six letters, and number of words found in the LIWC dictionary.
• Eighty-five dimensions, including function words (e.g., pronouns, articles, prepositions), grammar characteristics (e.g., adjectives, comparatives, numbers), affect words (e.g., positive and
negative emotions), social words (e.g., family, friends, male/female referents), cognitive process
(e.g., insight, certainty), core needs (e.g., power, risk/prevention focus), time orientation (e.g.,
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:10

Silva et al.

past/present/future focus), personal concerns (e.g., home, money, death), informal speech (e.g.,
swear words, netspeak), and punctuation (e.g., periods, commas, question marks). These features
reflect the percentage of total words per dimension (e.g., “positive emotions” equal to 7.5 means
that 7.5% of all words in the tweet were positive emotion words).
• Four summary variables expressed in a scale ranging from 0 (very low) to 100 (very high): (i)
analytical thinking, which is related to formal, logical, and hierarchical thinking patterns based
on eight different word dimensions; (ii) clout, which refers to relative social status, confidence, or
leadership expressed through writing; (iii) authenticity, which refers to writing that is personal
and honest, and; (iv) emotional tone, where the higher the number, the more positive and upbeat
the tone (scores lower than 50 usually suggest a more negative tone).
3.5.4 Sentiment analysis. We considered three out-of-the-box packages for the sentiment analysis
of our dataset:
(1) VADER [20] is a rule-based NLP library built specifically for sentiment analysis and is available
with NLTK, a popular NLP suite of libraries. Among the outputs provided by VADER is the
compound score, a uni-dimensional normalized, weighted composite score. A compound score
≥ 0.05 denotes a positive sentiment, between −0.05 and 0.05 denotes a neutral sentiment, and
≤ −0.05 denotes a negative sentiment.
(2) TextBlob [31] is another rule-based NLP library and offers the polarity score, which ranges
from −1.0 (most negative sentiment) to 1.0 (most positive sentiment). Inspired by VADER’s
compound score system, we opted to label scores between −0.05 and 0.05 as neutral sentiment.
(3) Flair [1] is an embedding-based framework built on PyTorch. Its pre-trained sentiment models
output value as positive or negative along with the confidence score in the range of 0.5 (low
confidence) to 1.0 (high confidence). To add a new sentiment label neutral, we normalized
the confidence scores in the range of 0.0 to 1.0, and negated the confidence score for negative
sentiment. Following VADER’s compound score system, we then labeled scores between −0.05
and 0.05 as neutral sentiment.
We tested the above three sentiment analysis packages with a dataset of 44,957 COVID-relevant
tweets, manually labeled for sentiment (19, 592 positive, 17, 031 negative, and 8, 334 neutral) [35]
using the F-score as the performance metric. We chose VADER for our sentiment analysis because
it outperformed not only the two other sentiment analysis packages, but also all three packages
combined in a voting scheme (average F-score of 82.0% with VADER versus 48.0% with Flair, 50.0%
with TextBlob, and 77.0% with the voting scheme). Then, for each collected tweet, we extracted
three binary sentiment features: positive, negative, and neutral.
Additionally, because emojis (pictures) and emoticons (icons created with marks, letters, and
numbers, e.g., “:-)”) are commonly used in tweets to display sentiment or emotions, we opted to
implement an emoji and emoticon counter. However, we later decided to disregard emoticons after
observing a high occurrence of false positives; many of the combinations of regular punctuations
were incorrectly identified as emoticons (e.g., in tweets containing the text “d:” and “%)” were
misclassified as emoticons). In other words, we only counted for emojis.
4

DATA RESULTS & ANALYSIS

This section details the statistical and machine learning analyses performed on the dataset to answer
each of our five research questions, as well as their results. All statistical tests were performed with
the help of the SciPy Library [55] for Python based on a 1% significance level (𝛼 = .01).
4.1

Statistical Tests

RQ1: Do Twitter bot accounts spread more misinformation than real accounts?
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111:11

Factual Tweets
(N = 497,702 Tweets)

5.2%

Real Accounts

94.8%

Missinformation Tweets
(N = 7,285 Tweets)

Bot Accounts

8.5%

Real Accounts

Bot Accounts

91.5%

Fig. 1. Comparison of the ratio between bot and real Twitter accounts, for the tweets assigned the factual
and misinformation labels.

We compared the number of bot-like accounts in the groups of labeled factual and misinformation
tweets, seeking to understand if there was a higher prevalence of bot accounts in the latter. For this
purpose, we used the binary feature obtained during the bot-hunter analysis that distinguishes
real accounts from bot-like accounts based on a confidence threshold in the bot-likelihood (see
Sec. 3.5.2 for more details).
Figure 1 shows that the vast majority of both factual and misinformation tweets were generated
by real accounts, though bot accounts tweeted more misinformation (8.5%) relative to factual
information (5.2%). This was supported by the Chi-Squared test with Yates’ correction for continuity
(𝜒 2 (1, 𝑁 = 504, 965) = 155.53, 𝑝 < .001); therefore, the data suggests a statistically significant results,
wherein the variables of user account type (real vs. bot) and tweet type (factual vs. misinformation)
are dependent. With so few misinformation labels relative to factual labels in the dataset (1.4%
vs. 98.6%, respectively), the Fisher’s Exact test (𝑝 < .001) implies that our data’s imbalance is
statistically significant such that both misinformation and factual tweets are more likely to be
tweeted by user account but bots are more likely to tweet misinformation.
The output of the Tweetbotornot2 tool is a bot likelihood between 0 and 1 where the higher
values indicate increased likelihood of a bot account. Based on [34], we used a threshold 𝜏 ≥ 0.75
to assign bot-like behavior to accounts tweeting about COVID-19. Though we could have used
a less conservative threshold, we chose to follow Memon et al. [34] to reduce the risk of falsely
labeling real accounts as bots. As such, we are aware that our results for RQ1 are exploratory in
nature. It is, however, possible that this reflects a natural phenomenon of Twitter in that there were
fewer bot accounts than real accounts during the period that the tweets were originally collected
(prior to the COVID-19 outbreak being considered a pandemic).
Contrary to our results, a recent report, upon analyzing over 200M tweets discussing COVID-19,
found up to two times as much bot activity based on previous crises and elections, and nearly half
of tweets discussing “reopening America” were from bot accounts [59]. Similarly, when analyzing
two similar datasets of COVID-19 disinformation, Dai et al. [13] found that users who propagate
misinformation are slightly more likely to be bots. However, we found a low prevalence of bot-like
accounts compared to real accounts in our analyzed dataset. This might have been because the
dataset was collected at the beginning of the public’s awareness of the coronavirus pandemic, just
prior to its pandemic declaration by the WHO; therefore, a well-organized infrastructure targeting
COVID-19 may not have yet been developed during this time. Additionally, it is possible that our
removal of retweets may have skewed our dataset away from larger and more significant findings
relating to bots—for example, Young et al. [59] found that, of the top 50 most influential retweeters
in their COVID dataset, 82% were found to be bots.
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:12

Silva et al.

Real Twitter accounts tweet both factual and misinformation more than bot accounts, but
bots tweeted relatively more misinformation than factual information.
RQ2: Are misinformation tweets more engaging than factual tweets?
To address this research question, we compared the three log-normalized engagement metrics
(# of likes, # of retweets, and combined engagement—see Sec. 3.5.1) between the factual and
misinformation tweets. More specifically, we first confirmed that the three engagement metrics
do not follow a normal distribution for the factual and misinformation groups of tweets using the
Shapiro-Wilk (𝑝 < .001) and D’Agostino’s K-squared (𝑝 < .001) tests (see Tables 2 for summary
of test statistics). Note that the 𝐾 2 statistic for each metric was extremely large, even after lognormalization, likely reflecting the naturally occurring phenomenon of tweets wherein the majority
of tweets are heavily skewed at near-zero (see Fig. 2). We also confirmed that the distribution of
the data was not homogeneous by comparing the log-normalized combined engagement metric for
the factual tweets group against the misinformation tweets group (𝑊 = 37.8, 𝑝 < .001). Therefore,
we opted to analyze the log-normalized combined engagement metric for the two groups of tweets
using non-parametric tests.
Table 2. Summary results for statistical tests conducted on engagement metrics and bot/user account labels.
Data

Measure

1: # likes,
2: # retweets,
3: combined engagement

Shapiro-Wilk

D’Agostino’s K-squared
Log-norm of combined engagement for factual
tweets vs. misinformation tweets

Levene
Two-Sample Kolmogorov-Smirnov
Mann-Whitney U

Low/high engagement vs. misinformation/factual tweet†
Bot/user account vs. misinformation/factual tweet

Chi-Squared
Chi-Squared
Fisher’s Exact

Measurement Statistics
𝑊1 = 0.0073***,
𝑊2 = 0.0082***,
𝑊3 = 0.0077***
𝐾12 = 3077164***,
𝐾22 = 2954320***,
𝐾32 = 3031005***
𝑊 = 37.81***
𝐾𝑆 = 0.04438***
𝑈 = 2, 381, 270, 024.5***,
𝑟 = 0.66
2
𝜒 (1, 𝑁 = 504, 965) = 12.84**
𝜒 2 (1, 𝑁 = 504, 965) = 155.53***
***

* Significant at 𝑝 < .05
** Significant at 𝑝 < .01
*** Significant at 𝑝 < .001
†See Sec. 4.2 for details on how the high and low engagement labels were determined.

From the Mann-Whitney U-test, we conclude that the combined engagement of the factual
tweets was statistically and significantly higher compared to the misinformation tweets (𝑈 =
2, 381, 270, 024.5, 𝑝 < .001). Given that 𝑈𝑚𝑎𝑥 = 𝑁 1 × 𝑁 2 = 3, 625, 759, 070, we can convert the
U-statistic to an effect size, 𝑟 = 𝑈 /𝑈𝑚𝑎𝑥 = 0.66. Put into words, there is a large and significant
probability that a combined engagement value from the factual tweets will be greater than misinformation tweets. Similarly, from the Two-Sample Kolmogorov Smirnov test, we determined
that the distribution of combined engagement for factual tweets is significantly different from that
of the misinformation tweets (𝐾𝑆 = 0.04438, 𝑝 < .001). This can also be seen in Fig. 2—though
the log-normalization of the combined engagement metric for factual and misinformation tweets
appears to be visually similar for approximately the bottom 50% quantile (with different frequency
magnitudes due to different sample sizes), we can see that factual tweets experienced higher
combined engagement relative to misinformation tweets.
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111:13

Factual tweets
Misinformation tweets

105

Frequency

104
103
102
101
100
0

2
4
6
8
10
Combined engagement (log-normalized)

12

Fig. 2. Histogram of the log-normalized combined engagement metric, split into real (i.e., factual) and
misinformation tweets.

Misinformation tweets about COVID-19 were statistically significantly less engaging than
factual tweets.
4.2

Machine Learning Analysis

To address our remaining research questions, we conducted machine learning experiments to
identify which features are relevant to distinguish between factual and misinformation tweets
(RQ3), as well as to predict high engagement in misinformation (RQ4) and factual tweets (RQ5)
related to COVID-19. We therefore designed three experimental scenarios to address each RQ
respectively, as follows:
• Scenario A, relevant features to distinguish factual vs. misinformation tweets: considering
the entire dataset (𝑁 = 504, 965) and splitting it into factual tweets (𝑁 = 497, 681, 98.6%) and
misinformation tweets (𝑁 = 7, 284, 1.4%).
• Scenario B, relevant features for predicting engagement in misinformation tweets: discarding
the factual tweets and splitting the misinformation tweets (𝑁 = 7, 284) into highly engaging
(𝑁 = 4, 104, 56.3%) and lowly engaging (𝑁 = 3, 180, 43.7%).
• Scenario C relevant features for predicting engagement in factual tweets: discarding the misinformation tweets and splitting the factual tweets (𝑁 = 497, 681) into highly engaging (𝑁 = 291, 200,
58.5%) and lowly engaging (𝑁 = 206, 481, 41.5%).
For Scenario A, we split the collected tweets into factual and misinformation based on a voting
scheme that considered the labels assigned by top three classifiers with best performance (see Sec.
3 for details). For Scenarios B and C, we used the binary label of engagement level (low/high (see
Sec. 3.5.1) to split the misinformation and factual tweets, respectively, into highly engaging and
lowly engaging. In both scenarios, we confirmed significant differences between the low and high
engagement groups of tweets by using the Chi-Squared test (𝜒 2 (1, 𝑁 = 504, 965) = 12.84, 𝑝 = 0.003),
similarly done in [3]. We then discarded all engagement-related features (e.g., engagement metrics,
# of retweets, # of likes) from the dataset for Scenarios B and C.
We aimed to distinguish the two groups of tweets in each scenario via machine learning models
and identify the most relevant features in each case. Feature selection methods not only allow
one to know the most relevant features in predicting the class variable, but also help improve
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:14

Silva et al.

Table 3. Summary of results from the machine learning experiments for Scenarios A (entire dataset, split
into factual and misinformation tweets), B (misinformation tweets, split into high and low engagement), and
C (factual tweets, split into high and low engagement) considering only the top 10% quantile of features
selected during our feature selection process.
Scenario A
Scenario B
Scenario C
p-value
p-value
p-value
Average F-score
Average F-score
(model vs. baseline)
(model vs. baseline)
(model vs. baseline)
Baseline approach
50.05%
52.80%
53.94%
Random Forest
76.34%
< .001†
69.35%
< .001
73.39%
< .001
Decision Tree
69.86%
< .001†
65.40%
< .001
68.89%
< .001
SVM
72.90%
< .001†
68.95%
< .001
72.47%
< .001
MLP
73.50%
< .001
69.30%
< .001
73.17%
< .001
Logistic Regression
72.75%
< .001†
69.40%
< .001
72.30%
< .001
Multinomial NB
70.19%
< .001
71.99%
< .001
73.30%
< .001
Bernoulli NB
68.45%
< .001†
67.94%
< .001
69.92%
< .001
Adaboost
75.52%
< .001†
69.71%
< .001
73.10%
< .001
Gradient Boosting
76.59%
< .001†
69.53%
< .001
73.10%
< .001
Bagging classifier
73.39%
< .001†
70.23%
< .001
70.69%
< .001
†P-value is the result of the nonparametric Wilcoxon test, chosen because the results for the Shapiro Wilk test indicated that at least one sample did not
follow a normal distribution (𝑝 < .01). Conversely, p-values without the †are the result of the parametric t-test, wherein the Shapiro Wilk test indicated
that all samples follow a normal distribution (𝑝 > .01).
Average F-score

the results by eliminating redundant information from the dataset [22]. For this purpose, before
training the models, we performed feature selection in each scenario using a filter method based
on the Pearson correlation coefficient [22]. This coefficient ranges from −1 to 1, where results
closer to 0 indicate weaker correlation while results closer to −1 and 1 suggest stronger negative
and positive correlations, respectively. A negative correlation between two given variables means
that, as the value of one variable increases, the value of the other variable decreases. Conversely,
a positive correlation means that as the value of one variable increases, so does the value of the
other variable.
For each scenario, we first computed the correlation of each feature with the class variable
(factual/misinformation for Scenario A, high/low engagement for Scenarios B and C). We
then computed the correlation of the features with each other and then performed pairwise
comparisons of the results. If the correlation between two given features was greater than |0.5|—our
threshold to identify high correlation—we kept the feature with higher correlation with the class
variable and discarded the other. Lastly, from the resulting subset of features, we selected the top
10% quantile of features based on the correlation coefficient (i.e., the ones most correlated with the
class variable) to be used in the machine learning experiments. Preliminary tests showed that the
inclusion of additional features did not increase the classification results. The features selected for
each scenario can be seen in Figs. 3 and 4 and Table 4, and are discussed in detail in Sec. 5.
Inspired by Elhadad et al. [17], we chose 10 popular multi-class classifiers to perform our
machine learning experiments: Random Forest, Decision Tree, Linear Support Vector Machine
(SVM), Multilayer Perceptron (MLP), Logistic Regression, Multinomial Naive Bayes (NB), Bernoulli
NB, Adaboost, Gradient Boosting, and Bagging classifier. We used their implementations available
in the scikit-learn [38]—a popular machine learning library for Python—adopting the default
parameters of scikit-learn in all cases. To establish a baseline for comparing our results, we opted
to include a rule-based model called DummyClassifier [38], which we configured to generate
predictions uniformly at random.
Multi-class classifiers are likely to reach better results than one-class models because they have access to all classes when building the predictive model, thus creating better decision boundaries [52];
however, they may not be suitable when the dataset is highly unbalanced because they can overfit
the predominant class. Indeed, we performed a preliminary test using the aforementioned classifiers
in Scenario A (features distinguishing factual from misinformation tweets), which is our most
imbalanced dataset, and observed that, after training the models, nearly every tested tweet was
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111:15

classified as factual (the predominant class), thus indicating overfitting. Therefore, instead of
using all the factual tweets at once to train and test the models, we decided to downsample them
by removing instances from the majority class [26], thus creating 68 subsets of 7, 300 factual tweets
each and then combining each subset with the fixed set of misinformation tweets (𝑁 = 7, 284) in
batches.
To evaluate these classifiers in each scenario, following the standard practice in the machine
learning field and using only the top 10% quantile of features selected, we randomly split the dataset
composed of low and high engagement tweets into training and test sets containing, respectively,
2/3 and 1/3 of the data [22]. We first normalized the training set using the min-max scaler [38] and
then the test set using the coefficients obtained from the training set. As per the evaluation metric,
we opted to use the F-score, which is the harmonic mean of precision and recall [22]. According to
Han et al. [22], precision is a measure of exactness (i.e., the percentage of tweets predicted by the
classifier as factual that are actually factual), while recall is a measure of completeness (i.e., the
percentage of factual tweets that are predicted by the classifier as factual). The F-score is a more
accurate evaluation metric for unbalanced datasets (our case) than other popular metrics such as
accuracy [22]. In Scenario A (features distinguishing factual from misinformation tweets), for all
classifiers, we computed the average F-score from the 68 batches. In Scenarios B and C (features
predicting engagement in misinformation and factual tweets, respectively), for all models, the
F-scores were obtained from the average of ten runs, with a different random state per run [8].
In all three scenarios, we compared the F-score of each classifier with the baseline approach using
either the t-test, if both samples displayed a normal distribution according to the Shapiro-Wilk test
(𝑝 < .01), or the Wilcoxon test, if at least one sample did not exhibit a normal distribution. From the
results, we verified that all chosen classifiers yielded statistically (𝑝 < .001) and significantly greater
F-scores than the rule-based model in distinguishing factual from misinformation tweets (Scenario
A), as well as engagement in misinformation (Scenario B) and factual (Scenario C) COVID-19 tweets
(see Table 3 for summary of test statistics). The most relevant features in each case are summarized
in Table 4. We can see from Fig. 3 that the Pearson correlation coefficients obtained were low for all
three scenarios, wherein the highest 𝑟 achieved for Scenario A was only 0.08 (taking the absolute
value), indicating a weak correlation with the class variable (factual/misinformation).
All classifiers outperformed the random prediction baseline model (with statistical significance at 𝑝 < 0.001), showing the relevance of the selected features in distinguishing
between factual and misinformation COVID-19 related tweets, as well as between high
and low engagement for misinformation and factual tweets. The highest F-score observed
in each case was, respectively, 76.59% with the Gradient Boosting (versus 50.05% with
the baseline approach), 71.99% with the Multinomial NB (versus 52.80% with the baseline
approach), and 73.39% with the Random Forest (versus 53.94% with the baseline approach).

RQ3: Which features are most relevant to distinguish between factual and misinformation tweets?
For Scenario A (dataset split into factual and misinformation tweets), our feature selection analysis
revealed that none of the 178 extracted features were strongly correlated with the class variable
(factual/misinformation), given that the highest correlation coefficient was 0.08. Nonetheless,
the analysis also revealed that the features from the tweet metadata, bot-hunter, and LIWC types
were more correlated with the class variable than those from the user metadata, engagement, and
sentiment types (Fig. 5a). After filtering out features highly correlated with each other, we obtained
a total of 11 features with the highest correlations with the class variable, including seven from the
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:16

Silva et al.

Fig. 3. Pearson’s correlation coefficient (absolute value) for all features, split by experimental scenario.
Scenario A

Scenario C

Scenario B

Not selected
Selected

Not selected
Selected

0.175

0.04
0.02
0.00

0.150
0.125
0.100
0.075
0.050
0.025
0.000

Tweet
User EngagementBot-hunter
metadata metadata
Type of feature

LIWC

Sentiment

(a) Scenario A (entire dataset, split
into factual and misinformation
tweets).

Tweet
metadata

Not selected
Selected

0.20
Pearson correlation coefficient

0.06

Pearson correlation coefficient

Pearson correlation coefficient

0.08

0.15
0.10
0.05
0.00

User
metadata

Bot-hunter

LIWC

Sentiment

Type of feature

(b) Scenario B (misinformation
tweets, split into high and low engagement).

Tweet
metadata

User
metadata

Bot-hunter

LIWC

Sentiment

Type of feature

(c) Scenario C (factual tweets, split
into high and low engagement).

LIWC group, four from the tweet metadata group, and one from the bot-hunter group, as displayed
in Fig. 4a).
The most relevant features for distinguishing misinformation vs. factual COVID-19 tweets
were related with the tweets’ sociolinguist structures (e.g., percentage of conjunctions and
death-related words) and metadata (e.g., # of hashtags and URLs). No sentiment, user related
(e.g., # of friends), nor engagement features were found to be amongst the most relevant.
RQ4: Which features are most relevant to predict high engagement in misinformation tweets?
Figure 5b shows the most relevant subset of features for Scenario B (misinformation tweets, split
into high and low engagement), which is composed of 11 features with the highest correlations with
the output variable (high/low engagement for misinformation tweets). We see that the extracted
features were slightly more strongly correlated with the class variable than the features in Scenario
A (entire dataset, split into factual and misinformation tweets), though the highest correlation
was 0.175, indicating a low correlation. As one can see in Fig. 4b, this subset—composed by 11
features—contains features from user metadata, tweet metadata, and the bot-hunter analysis.
The most relevant features for predicting high engagement in misinformation COVID-19
tweets were associated with human-like behavior (e.g., a user’s follower-friend ratio, the
presence of banner image or URL in the user profile), and a tweets’ entities (e.g., presence
of image). Sentiment and sociolinguistic features (e.g., a tweet’s authenticity score) were
not found relevant to predict high engagement in misinformation tweets..
RQ5: Which features are most relevant to predict high engagement in factual tweets?
Following our feature selection procedure, we obtained 11 features (Fig. 5c) based on the highest
correlations with the Scenario C (factual tweets, split into high and low engagement) output
variable (high/low engagement for factual tweets, including features from the bot-hunter analysis,
user metadata, tweet metadata, LIWC analysis (Fig. 4c). The extracted features were slightly more
strongly correlated with the class variable than the features in Scenarios A and B, with the highest
correlation at 0.22, i.e., low to moderate correlation.
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111:17

Fig. 4. Pearson’s correlation coefficient (absolute value) for the most relevant features based on top 10%
quantile, split by experimental scenario.
Scenario A
Selected features

authenticity score
# of URLs
user word count variation
% of male words
% of adverbs
presence of an image
% of cognitive words
% of death-related words
% of conjunction words
# of hashtags
# of LIWC words
0.00

0.01

0.02

0.03

0.04
0.05
Pearson correlation coefficient

0.06

0.07

0.08

(a) Scenario A (entire dataset, split into factual and misinformation tweets).
Scenario B
Selected features

user has URL
# of URLs
presence of an image
tweet count in 30-min
user # of friends
user word count variation
tweet extended media
user follower-friend ratio
user has banner image
user # of lists
is account verified
0.000

0.025

0.050

0.075
0.100
Pearson correlation coefficient

0.125

0.150

0.175

(b) Scenario B (misinformation tweets, split into high and low engagement).
Scenario C
Selected features

user # of followers
account is bot or not
user follower-friend ratio
tweet sources
presence of an image
# of words
user has url
tweet media variation
user word count variation
user has banner image
is account verified
0.00

0.05

0.10
Pearson correlation coefficient

0.15

0.20

(c) Scenario C (factual tweets, split into high and low engagement).

Similarly to Scenario B, the most relevant features for predicting high engagement in factual
COVID-19 tweets were those associated with human-like behavior (e.g., user follower-friend
ratio), followed by user/tweet metadata features (e.g., presence of banner image and URL in
the user profile), and word counts (a sociolinguistic features). Sentiment features were not
found to be relevant to predict high engagement.

, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:18

Silva et al.

In sum, to distinguish misinformation from factual COVID-19 tweets (Scenario A) the sociolinguistic and tweet metadata features were the most relevant. To predict engagement in both
misinformation (Scenario B) and factual COVID-19 tweets (Scenario C), user metadata features
(e.g., whether the user account is verified) and human-like behavior (e.g., word count variation)
were the most important. Moreover, the highest levels of correlation between the extracted features
and the class variable was observed in Scenario C, followed by Scenarios B and A. However, the
results did not suggest strong correlations in any of these scenarios, as the coefficients (taking the
absolute value) were all closer to 0 than 1.
Table 4. Summary of top 10% most important features, based on Pearson’s correlation coefficient for Scenarios
A (entire dataset, split into factual and misinformation tweets), B (misinformation tweets, split into high and
low engagement), and C (factual tweets, split into high and low engagement).
Feature Type

Twitter metadata

User metadata

Bot-hunter analysis

LIWC

Feature
Presence of image(s) in the tweet
# of URLs in the tweet
# of hashtags in the tweet

Scenarios
A B C
x
x
x

Whether the user’s account is verified
Whether the user profile has banner image
Whether the user profile has URL
# of lists
# of friends
# of followers

B&C

x
x

x

x

x
x
x
x
x

x
x
x

x
x
x

Overlaps
A&B A&C A&B&C
x
x

x

x

x

x

x

x

User’s word count variation among last 200 tweets*
User follower-friend ratio
Tweet extended media (e.g., GIF, video) variation
Tweet count maximum in 30-min intervals
Tweet media type variation
Tweet sources (e.g., Twitter for iPhone)
Whether the user account has a bot-like behavior (i.e., bot label)

x

# of words contained in the LIWC dictionary
Authenticity score
Percentage of male references (e.g., “boy,” “his,” “dad”)
Percentage of common adverbs (e.g., “very,” “really”)
Percentage of conjunctions (e.g., “and,” “but,” “whereas”)
Percentage of death-related words (e.g., “bury,” “coffin,” “kill”)
Percentage of cognitive process-related words (e.g., “cause,” “know,” “ought”)
Word count (i.e., # of words)

x
x
x
x
x
x
x

x
x
x
x

x
x

x
x

x
x
x

x

Exploratory analysis. In addition to the top 10% quantile of features chosen for Scenarios A–C,
we also explored these features alongside those extracted from the tweets’ text, with the hypothesis
that they could help improve our classification results. To do so, we extracted the textual features
of these tweets using TF-IDF (Term Frequency Inverse Document Frequency), a statistical measure
that evaluates how important a word is to a text in relation to a collection of texts [33]. TF-IDF was
also one of the better achieving feature extraction methods used to create the COVID-19-FAKES
dataset [17]. We leveraged the TF-IDF implementation available in scikit-learn [38] with the default
parameters and 𝑚𝑎𝑥_𝑓 𝑒𝑎𝑡𝑢𝑟𝑒𝑠 = 10000, considering only the training set to build its vocabulary
in order to extract features from both training and test sets. The top features considered in for
Scenarios A–C were concatenated with the ones extracted by TF-IDF and were then normalized
using max-abs scaler, given that min-max scaler does not support sparse inputs [38].
Similar to the prior experiments, we compared the F-scores of each classifier with that of
the baseline approach using the t-test (for parametric distributions) or the Wilcoxon test (for
nonparametric distributions). As shown in Fig. 5, all classifiers significantly outperformed the
baseline for all three scenarios (𝑝 < .001). We then compared the F-scores of each classifier before
and after including the TF-IDF features for all three scenarios using the independent t-test (for
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111:19

parametric distributions) or the Mann-Whitney test (for nonparametric distributions). There was a
notable increase in the F-scores for Scenario A (distinguishing misinformation from factual tweets)
when including the TF-IDF features (Fig. 6a). This increased performance further supports our
conclusions that textual information is pivotal in distinguishing misinformation from factual tweets.
5

DISCUSSION

In this paper, we set out to answer five research questions relating COVID-19 tweets as a function
of being misinformation or fact, exhibiting human or bot-like behavior, and attaining low or
high engagement. Towards this end, we investigated features related to bot-like behavior and
engagement in a dataset of COVID-19 tweets previously classified as misinformation or factual
information. We investigated these features in the context of three experimental scenarios. In
Scenario A, we investigated the most relevant features distinguishing factual vs. misinformation
tweets using a downsampling strategy to mitigate the class imbalance of the dataset. Next, we
investigated features predicting low and high engagement in misinformation and factual tweets. In
Scenario B, we considered only misinformation tweets and partitioned this data into low and high
engagement tweets. In Scenario C, we partitioned the subset of factual tweets into low and high
engagement tweets. In all three scenarios, we leveraged multi-class machine learning classifiers
to investigate the relevance of the selected features. This section summarizes the take-aways and
limitations of our work, and suggests possible future directions of research.
5.1

Take-Aways

Bot accounts. Our results show that human Twitter accounts tweet both factual information
and misinformation related to COVID-19, while bot accounts tweeted more misinformation than
factual information, corroborating prior works [18, 34, 57]. We further add support towards DiResta
et al.’s [40] hypothesis that, for the 2020 US election cycle, disinformation campaigns in social
media platforms such as Twitter would exhibit a decreased reliance on bot-like activity, and instead
relying more heavily on real humans to curate and spread misinformation. Because the COVID-19
pandemic occurred during the 2020 US presidential election, it is plausible that changes in bot
activity in the election environment are also reflected in the Twitter discourse for COVID-19.
Distinguishing Factual from Misinformation Tweets. First, it is important to note that distinguishing between factual and misinformation COVID-related tweets was relatively challenging
for the tested machine learning models with the types of features extracted in this work, as the
highest F-score results achieved was 76.59%. This is unsurprising, as research has shown that
automatic detection of disinformation is a nuanced and open research problem in the machine
learning field [61]. Not only is disinformation intentionally written to deceive and pass as factual,
but social media platforms are inherently rooted in big data that is unstructured and noisy [47].
Both of these problems exacerbate the difficulty of detecting misinformation.
Importantly, all tested classifiers significantly outperformed the rule-based model used as baseline
approach, thus suggesting that our choice of features were impactful in distinguishing misinformation from factual COVID tweets. These features were mostly related to the tweets’ sociolinguist
structures extracted through the LIWC analysis (e.g., LIWC’s authenticity score indicating honesty
vs. deceptiveness of the text) and metadata (e.g., # of hashtags and URLs). Therefore, Scenario
A’s most important features were related to the content of the tweet itself; this finding is further
supported by the results of our exploratory analysis, wherein F-score performance of all classifiers
increased significantly after the inclusion of TF-IDF textual features. URLs are included in a tweet
to refer the viewers to external sources, such as news and videos, which are usually used by the
author to reinforce factual or misleading claims. Conversely, our analysis indicated that the tweet’s
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:20

Silva et al.

Fig. 5. Comparison of F-scores for all classifiers with (blue bars) and without (gray bars) the TF-IDF textual
features. Asterisk denotes statistically superior performance at 𝑝 < .001.

Scenario A
*

Bagging classifier
Gradient Boosting
Adaboost
Bernoulli NB
Multinomial NB
Logistic Regression
MLP
SVM
Decision Tree
Random Forest
Baseline Approach

*
*
*
*
*
*
*
*
*
0

5

10

15

20

25

30

35

40

45 50 55
F-Score (%)

Top 10% Features

60

65

70

75

80

85

90

95

100

Top 10% Features + TF-IDF

(a) Scenario A (entire dataset, split into factual and misinformation tweets).

Scenario B
Bagging classifier
Gradient Boosting
Adaboost
Bernoulli NB
Multinomial NB
Logistic Regression
MLP
SVM
Decision Tree
Random Forest
Baseline Approach

*
*
*
*
*
*

0

10

20

30

40

50
F-Score (%)

Top 10% Features

60

70

80

90

100

Top 10% Features + TF-IDF

(b) Scenario B (misinformation tweets, split into high and low engagement).

Scenario A
*

Bagging classifier
Gradient Boosting
Adaboost
Bernoulli NB
Multinomial NB
Logistic Regression
MLP
SVM
Decision Tree
Random Forest
Baseline Approach

*
*
*
*
*
*
*
*
*
0

5

10

15

20

25

30

35

40

45 50 55
F-Score (%)

Top 10% Features

60

65

70

75

80

Top 10% Features + TF-IDF

(c) Scenario C (factual tweets, split into high and low engagement).
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

85

90

95

100

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111:21

sentiment, level of engagement, and user account’s features were not relevant features to distinguish misinformation vs factual tweets. When contrasting sentiment’s lack of importance with the
importance of LIWC’s authenticity score and the percentage of cognitive processing-related words,
it may suggest that COVID-related tweets were appealing to logic over emotion.
We also found that factual tweets were statistically more engaging than misinformation
tweets. To the best of our knowledge, our study is the first to analyze engagement in COVID
tweets. Our feature selection analysis indicated four features that predict high engagement for
both factual and misinformation tweets: (i) whether the user’s account is verified, (ii) whether the
user’s profile has a banner image, (iii) whether the user profile contains a URL, and (iv) the user’s
follower-friend ratio. Operating under the assumption that a real Twitter account is more likely to
be verified and have a banner/URL than a bot account, then these features also intrinsically help
distinguish real users from bots. Based on this assumption, we can infer that, irrespective to the
nature of the content of the tweet (factual information or misinformation), COVID-related tweets
by real users offer credibility and thus are likely more engaging than tweets by bot accounts. This
corroborates prior research providing evidence that the credibility of the source of the information
greatly helps in the detection of fake news [61].
Predicting Engagement in Factual and Misinformation Tweets. The user account metadata
(e.g., whether the user is verified) and thus human-like features derived from our bot analysis
(e.g., follower-friend ratio) played the most important roles in predicting engagement for both
factual information and misinformation. Engagement for misinformation tweets relied more on
user metadata, whereas engagement for factual tweets relied more on the human-like features.
Conversely, the textual content of the tweet itself (based on LIWC analysis) appears not to be
relevant for engagement. Interestingly, we found that tweet sentiment was not relevant to
predict engagement. The relevance of such features was successfully validated in our machine
learning experiments, with ten classifiers statistically outperforming the baseline approach based
on random predictions.
These findings suggest that users may be more willing to engage with a Twitter account that
purports “human-like” behavior and ethos. In other words, human-like behavior appears to garner
more attention from users. As such, the textual content of the tweet itself appears to not be a major
factor in deciding and predicting engagement. We also found that the length of the tweet is more
relevant to predict engagement for factual information than for misinformation. Also, bot-like
behavior was only relevant to predict engagement for factual tweets, but not for misinformation
tweets. This lack of relevance in the bot-behavior feature for disinformation tweets might be due
to the fact that bot accounts are already more likely to spread disinformation than real accounts,
thus diluting the distinguishing factor of such feature for classification [13, 59].
Overlaps in Predicting Facts/Misinformation and Engagement. There were two features
deemed important in predicting boh misformation and high engagement in all three scenarios: the
presence of an image in the tweet and the word count variation across a user’s tweets. Historically,
we have seen the use of short texts, lots of images, a touch of sex, and a tendency towards sensationalism used as a recipe for propaganda success, leveraged by the KGB, Stasi, and CIA [41]. The
presence of an image and the amount of text (and therefore information that a user must process)
in a tweet might be leveraged by both disinformation campaigns and reputable sources alike to
help users quickly digest information. Additionally, this could denote that users are more likely
to engage with an image over words, especially considering that sociolinguistic and sentiment
features were not of utmost importance in predicting engagement.
We also found that the # of ULRs in the tweet is an important feature both for distinguishing misinformation and predicting high engaging misinformation. We suspect that URLs could potentially
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:22

Silva et al.

increase the veracity of the information presented in the tweet, thus helping distinguish factual
information from misinformation (Scenario A) and reinforce false claims in misinformation tweets,
therefore increasing their engagement (Scenario B).
Interestingly, Scenarios A (features distinguishing misinformation) and C (predicting highly
engaging factual tweets) exhibited no overlap in their top 10% most important features. However,
we investigated only linear relationships between the features, so an overlap between scenarios A
& C might occur for features whose relationship is non-linear.
5.2

Limitations & Future Works

Here we discuss study’s limitations and potential ideas for future works.
Sample size and representativeness. Our dataset was highly imbalanced, wherein 98.6% of the
tweets were factual information and only 1.4% were misinformation. However, when analyzing
misinformation, we see this limitation as part of the phenomenon. Indeed, most Twitter datasets
for COVID-19 misinformation analysis exhibit a significantly larger amount of samples related to
facts than misleading information (see Table 1). This lack of variety of misinformation samples
limits the generalizability of our findings. For example, different from what we observed in this
study, it is possible that misinformation tweets actually display similar or even higher engagement
than factual tweets. In addition, this can potentially affect the performance of machine learning
models attempting to distinguish factual from misinformation tweets, likely causing the models to
overfit the factual tweets (the group with most samples) and misdetect misinformation tweets, as
we observed in a preliminary test (which led us to adopt a downsampling strategy). A possible way
to overcome this limitation of imbalanced data—potential advice for future works—would be to
increase the number of misinformation tweets artificially, by generating larger synthetic datasets
applying GAN [45].
The ratio of bot-like accounts to real accounts (5.3% and 94.7%, respectively) was skewed, which
could have affected our findings about bot-like and human-like accounts. However, works such
as DiResta et al. [40] posit that, for the 2020 US general election cycle—which greatly overlapped
and intertwined with the spread of the coronavirus—there would be a decrease in reliance on
bot accounts for the spreading of misinformation, as automation techniques have become better
policed [11]. In fact, from our set of ∼580k tweets (disregarding retweets) originally collected from
early February to early May 2020, we were unable (as of November 2020) to retrieve data for ∼82k
with the bot-hunter tool because the original accounts were no longer accessible, possibly because
they had indeed been blocked by Twitter for exhibiting nefarious activity.
Bot-hunter analysis. We identified some limitations related to our bot-hunter analysis, which
might have influenced the imbalance observed between bot-like accounts (5.3%) and real accounts
(94.7%). First, in order to remove repeated samples, we discarded nearly 80k retweets from the
dataset, which might have caused the removal of several bot-like accounts given that such accounts
are often used to spread content via retweets [57, 59]. However, considering these retweets in
our experiments would have added substantial redundancy to the dataset and likely increased the
imbalance between factual and misinformation tweets, therefore worsening the machine learning
results to distinguish misinformation vs. factual tweets (Scenario A). Second, following the study
by Memon and Carley [34], we used a threshold of 0.75 in the bot-likelihood to distinguish bot-like
accounts (≥ 0.75) from real accounts tweeting about COVID-19. However, this threshold might be
above the optimal value, thus causing bot-accounts to be misclassified as real accounts. Future works
are advised to test bot-hunter tools using COVID-related tweets from known real (e.g., WHO) and
known bot (from bot repositories) Twitter accounts to find the optimal threshold for distinguishing
these two types of accounts. Lastly, the detection of bot-like accounts may be improved in future
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111:23

studies through the combination of different bot-hunter tools (e.g., Botometer [58]) using a voting
scheme.
Feature engineering, feature selection, and classification models. We leveraged almost 200
features for our data analysis, but there is still a plethora of additional features that future works
can investigate (see [61]). For example, we considered only the count of emojis in a tweet, but we
did not analyze emojis as a proxy of sentiment, which was ultimately found to not be a decisive
feature to distinguish misinformation nor to predict engagement. Works such as Novak et al. [27]
and Fersini et al. [19] have found that emojis provide useful sentiment information, which may
be of significant importance in the informal communication environment of Twitter. Automatic
feature extractors, such as Word Embedding, Word2Vec, BERT, and GloVe, may also provide relevant
features to predict misinformation and engagement in COVID-related tweets. In our preliminary
and exploratory analysis of TF-IDF textual features, we saw a sharp increase in the F-score of all
classifiers when distinguishing factual from misinformation tweets in Scenario A (for example,
our Logistic Regression model achieved an F-score of 72.75% using only the top 10% quantile
of features, but increased to 88.98% after the addition of TF-IDF features). By combining some
of the aforementioned feature extractors with a set of popular multi-class classifiers, Elhadad et
al. [17] obtained F-scores greater than 99% when distinguishing factual from misleading pieces
of information (e.g., news) related to COVID-19. It should be noted, however, that textual feature
extractors are usually sensitive to the text’s size, which may be a limitation when handling tweets
given their limit of characters.
Another limitation of this study is that we tested a single feature selection method (among many
others available in the literature) to identify the most relevant features to predict misinformation
and engagement in COVID tweets. In addition, this method has limitations since it is based on
Pearson’s correlation coefficient, which measures only linear relationships between the features
with each other and with the class variable. This might have affected the results of our machine
learning experiments in all three scenarios. For example, principal component analysis (PCA) could
be used to decorrelate the features before training the learning models, as well as to present how
many features are needed to explain the dataset, instead of looking at pairwise features (i.e., looking
at predictive power that all features in combination are able to provide instead of focusing on
single feature predictive value). Future work might also seek to understand the feature selection
process by checking whether there is a positive or negative correlation between the groups of
features (e.g., tweet metadata features and sociolinguistic features). Future studies testing different
feature selection methods from both filter (selecting relevant features regardless of a particular
classifier) and wrapper (searching for optimal features tailored to a particular model) approaches
are warranted. Lastly, given the size and imbalance of our dataset, we did not test state-of-the-art
deep learning models for natural language processing (e.g., Recurrent Neural Networks, LSTM), so
future works are also advised to do so.
The digital revolution and the integration of social media into our daily lives have been leveraged
as tools for the faster propagation of disinformation campaigns. Research has shown that humans
are poor at detecting deception [21] and our ability to detect digital fake news is “bleak” [56].
Understanding how machines can detect highly engaging dis/misinformation will provide a first
line of defense against deception in the online sphere. This knowledge can be used by government
agencies and organizations to convey critical public health information to the general populace.
For example, the Italian Ministry of Health, Lovari [32] found that keeping the public constantly
informed via dissemination of information in understandable forms (e.g., data and visuals), helps
reduce the spread of misinformation.
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:24

6

Silva et al.

CONCLUSION

This paper analyzed a dataset of nearly 505k COVID-19-related misinformation and factual tweets
to investigate misinformation as a function of bot-like behavior and to discover which features
in COVID-19-related tweets are the most relevant in distinguishing between misinformation and
facts, and predicting high user engagement. Via the use of statistical and machine learning analyses,
we offer the following conclusions: (i) bot-like accounts tweeted more misinformation than facts,
while real user accounts tweet both facts and misinformation; (ii) misinformation tweets were less
engaging than factual tweets; (iii) a tweet’s metadata (e.g., # of hashtags and URLs) along with its
sociolinguistic features (e.g., authenticity score) were the most important features for distinguishing
fact from misinformation; (iv) user account information features (e.g., presence of banner image and
URL in the user profile) were the most important features for predicting high engagement in both
misinformation and factual tweets; (v) sentiment features were not found relevant for predicting
engagement; and (vi) the presence of images in the tweet was relevant both for distinguishing
misinformation and predicting high engaging misinformation and factual information. Cognizant
of the limitations of our dataset, we propose several directions and suggestions for future works
on misinformation in the online sphere. In particular, our insights on what features predict high
engagement can be leveraged for defense approaches against disinformation, such as increasing
the engagement of factual tweets, especially those coming from verified government accounts and
reputable organizations (e.g., WHO, NIH), thus contributing to factual public health information
reaching the masses.
REFERENCES
[1] Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual String Embeddings for Sequence Labeling. In
Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics,
Santa Fe, New Mexico, USA, 1638–1649. https://www.aclweb.org/anthology/C18-1139
[2] Mabrook S Al-Rakhami and Atif M Al-Amri. 2020. Lies Kill, Facts Save: Detecting COVID-19 Misinformation in Twitter.
IEEE Access 8 (2020), 155961–155970.
[3] Kholoud Khalil Aldous, Jisun An, and Bernard J. Jansen. 2019. View, Like, Comment, Post: Analyzing User Engagement
by Topic at 4 Levels across 5 Social Media Platforms for 53 News Organizations. Proceedings of the International AAAI
Conference on Web and Social Media 13, 01 (Jul. 2019), 47–57. https://ojs.aaai.org/index.php/ICWSM/article/view/3208
[4] Oberiri Destiny Apuke and Bahiyah Omar. 2020. Fake news and COVID-19: modelling the predictors of fake news
sharing among social media users. Telematics and Informatics (2020), 101475.
[5] Ahmer Arif, Leo Graiden Stewart, and Kate Starbird. 2018. Acting the Part: Examining Information Operations Within
#BlackLivesMatter Discourse. Proc. ACM Human-Computing. Interaction 2, CSCW, Article 20 (Nov. 2018), 27 pages.
https://doi.org/10.1145/3274289
[6] Benjamin Bell and Fergal Gallagher. 2020. Who is spreading COVID-19 misinformation and why. https://abcnews.go.
com/US/spreading-covid-19-misinformation/story?id=70615995. Accessed: 2020-11-21.
[7] Ladislav Bittman. 1972. The Deception Game: Czechoslovak Intelligence in Soviet Political Warfare. Syracuse University
Press.
[8] Leo Breiman. 2001. Random Forests. Machine Learning 45, 1 (01 Oct 2001), 5–32. https://doi.org/10.1023/A:
1010933404324
[9] J Scott Brennen, Felix M Simon, and Rasmus Kleis Nielsen. 2020. Beyond (Mis) Representation: Visuals in COVID-19
Misinformation. The International Journal of Press/Politics (2020).
[10] Matteo Cinelli, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valensise, Emanuele Brugnoli, Ana Lucia
Schmidt, Paola Zola, Fabiana Zollo, and Antonio Scala. 2020. The covid-19 social media infodemic. arXiv preprint
arXiv:2003.05004 (2020).
[11] Elizabeth Dwoskin Craig Timberg. 2018. Twitter is sweeping out fake accounts like never before, putting user growth
at risk.
[12] Limeng Cui and Dongwon Lee. 2020. CoAID: COVID-19 Healthcare Misinformation Dataset. arXiv:2006.00885 [cs.SI]
[13] Enyan Dai, Yiwei Sun, and Suhang Wang. 2020. Ginger Cannot Cure Cancer: Battling Fake Health News with a
Comprehensive Data Repository. Proceedings of the Fourteenth International AAAI Conference on Web and Social Media
(ICWSM 2020) 14 (Jan. 2020), 853–862.
, Vol. 37, No. 4, Article 111. Publication date: November 2020.

Predicting Misinformation and Engagement in COVID-19 Twitter Discourse in the First Months of the Outbreak 111:25
[14] Clayton Allen Davis, Onur Varol, Emilio Ferrara, Alessandro Flammini, and Filippo Menczer. 2016. Botornot: A system
to evaluate social bots. In Proceedings of the 25th international conference companion on world wide web. 273–274.
[15] Dimitar Dimitrov, Erdal Baran, Pavlos Fafalios, Ran Yu, Xiaofei Zhu, Matthäus Zloch, and Stefan Dietze. 2020.
TweetsCOV19–A Knowledge Base of Semantically Annotated Tweets about the COVID-19 Pandemic. arXiv preprint
arXiv:2006.14492 (2020).
[16] Mohamed K Elhadad, Kin Fun Li, and Fayez Gebali. 2020. COVID-19-FAKES: A twitter (Arabic/English) dataset for
detecting misleading information on COVID-19. In International Conference on Intelligent Networking and Collaborative
Systems. Springer, 256–268.
[17] Mohamed K Elhadad, Kin Fun Li, and Fayez Gebali. 2020. Detecting Misleading Information on COVID-19. IEEE Access
8 (2020), 165201–165215.
[18] Emilio Ferrara. 2020. What types of COVID-19 conspiracies are populated by Twitter bots? First Monday (2020).
[19] E Fersini, E Messina, and F A Pozzi. 2016. Expressive signals in social media languages to improve polarity detection.
Inf. Process. Manag. 52, 1 (Jan. 2016), 20–35.
[20] Che Gilbert and Erric Hutto. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media text.
In Eighth International Conference on Weblogs and Social Media (ICWSM-14), Vol. 81. 82. https://www.aaai.org/ocs/
index.php/ICWSM/ICWSM14/paper/view/8109
[21] Pär Anders Granhag, Lars O Andersson, Leif A Strömwall, and Maria Hartwig. 2004. Imprisoned knowledge: Criminals’
beliefs about deception. Legal and Criminological Psychology 9, 1 (Feb. 2004), 103–119. https://doi.org/10.1348/
135532504322776889
[22] Jiawei Han, Jian Pei, and Micheline Kamber. 2011. Data mining: concepts and techniques. Elsevier.
[23] Binxuan Huang and Kathleen M Carley. 2020. Disinformation and Misinformation on Twitter during the Novel
Coronavirus Outbreak. arXiv preprint arXiv:2006.04278 (2020).
[24] AKM Najmul Islam, Samuli Laato, Shamim Talukder, and Erkki Sutinen. 2020. Misinformation sharing and social
media fatigue during COVID-19: An affordance and cognitive load perspective. Technological Forecasting and Social
Change 159 (2020), 120201.
[25] Julie Jiang, Emily Chen, Shen Yan, Kristina Lerman, and Emilio Ferrara. 2020. Political polarization drives online
conversations about COVID-19 in the United States. Human Behavior and Emerging Technologies 2, 3 (2020), 200–211.
[26] Harsurinder Kaur, Husanbir Singh Pannu, and Avleen Kaur Malhi. 2019. A systematic review on imbalanced data
challenges in machine learning: Applications and solutions. ACM Computing Surveys (CSUR) 52, 4 (2019), 1–36.
[27] Petra Kralj Novak, Jasmina Smailović, Borut Sluban, and Igor Mozetič. 2015. Sentiment of Emojis. PLoS One 10, 12
(Dec. 2015), e0144296.
[28] Rabindra Lamsal. 2020. Coronavirus (COVID-19) Tweets Dataset. https://doi.org/10.21227/781w-ef42
[29] Rabindra Lamsal. 2020. Tweets Originating from India During COVID-19 Lockdowns 1, 2, 3, 4. https://doi.org/10.
21227/k8gw-xz18
[30] S. Latif, M. Usman, S. Manzoor, W. Iqbal, J. Qadir, G. Tyson, I. Castro, A. Razi, M. N. Kamel Boulos, A. Weller, and J.
Crowcroft. 2020. Leveraging Data Science To Combat COVID-19: A Comprehensive Review. IEEE Transactions on
Artificial Intelligence (2020), 1–1. https://doi.org/10.1109/TAI.2020.3020521
[31] Steven Loria, P Keen, M Honnibal, R Yankovsky, D Karesh, E Dempsey, et al. 2014. Textblob: simplified text processing.
Secondary TextBlob: simplified text processing 3 (2014).
[32] Alessandro Lovari. 2020. Spreading (dis) trust: Covid-19 misinformation and government intervention in Italy. Media
and Communication 8, 2 (2020), 458–461.
[33] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA.
[34] Shahan Ali Memon and Kathleen M Carley. 2020. Characterizing covid-19 misinformation communities using a novel
twitter dataset. arXiv preprint arXiv:2008.00791 (2020).
[35] Aman Miglani. 2020. Coronavirus tweets NLP - Text Classification. https://www.kaggle.com/datatattle/covid-19-nlptext-classification
[36] Robert S Mueller. 2019. The Mueller Report: Report on the Investigation into Russian Interference in the 2016 Presidential
Election. Technical Report. U.S. Department of Justice.
[37] Office of the Commissioner. 2020. Coronavirus (COVID-19) Update: FDA Announces Advisory Committee Meeting to
Discuss COVID-19 Vaccine Candidate. https://www.fda.gov/news-events/press-announcements/coronavirus-covid19-update-fda-announces-advisory-committee-meeting-discuss-covid-19-vaccine. Accessed: 2020-11-22.
[38] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. the
Journal of machine Learning research 12 (2011), 2825–2830.
[39] James W Pennebaker, Ryan L Boyd, Kayla Jordan, and Kate Blackburn. 2015. The development and psychometric
properties of LIWC2015. Technical Report.

, Vol. 37, No. 4, Article 111. Publication date: November 2020.

111:26

Silva et al.

[40] Renee DiResta, Dr. Kris Shaffer, Becky Ruppel, David Sullivan, Robert Matney, Ryan Fox, Dr. Jonathan Albright, Ben
Johnson. 2019. The Tactics & Tropes of the Internet Research Agency. Technical Report. New Knowledge.
[41] Thomas Rid. 2020. Active Measures: The Secret History of Disinformation and Political Warfare. Farrar, Straus and
Giroux.
[42] Jon Roozenbeek, Claudia R Schneider, Sarah Dryhurst, John Kerr, Alexandra LJ Freeman, Gabriel Recchia, Anne Marthe
van der Bles, and Sander van der Linden. 2020. Susceptibility to misinformation about COVID-19 around the world.
Royal Society Open Science 7, 10 (2020), 201199.
[43] Leonard Schild, Chen Ling, Jeremy Blackburn, Gianluca Stringhini, Yang Zhang, and Savvas Zannettou. 2020. " go eat
a bat, chang!": An early look on the emergence of sinophobic behavior on web communities in the face of covid-19.
arXiv preprint arXiv:2004.04046 (2020).
[44] Juan Carlos Medina Serrano, Orestis Papakyriakopoulos, and Simon Hegelich. 2020. NLP-based Feature Extraction
for the Detection of COVID-19 Misinformation Videos on YouTube. In Proceedings of the 1st Workshop on NLP for
COVID-19 at ACL 2020.
[45] Pourya Shamsolmoali, Masoumeh Zareapoor, Linlin Shen, Abdul Hamid Sadka, and Jie Yang. 2020. Imbalanced Data
Learning by Minority Class Augmentation using Capsule Adversarial Networks. arXiv:2004.02182 [cs.LG]
[46] Karishma Sharma, Sungyong Seo, Chuizheng Meng, Sirisha Rambhatla, and Yan Liu. 2020. Covid-19 on social media:
Analyzing misinformation in twitter conversations. arXiv preprint arXiv:2003.12309 (2020).
[47] Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and Huan Liu. 2017. Fake News Detection on Social Media: A Data
Mining Perspective. SIGKDD Explor. Newsl. 19, 1 (Sept. 2017), 22–36.
[48] Junaid Shuja, Eisa Alanazi, Waleed Alasmary, and Abdulaziz Alashaikh. 2020. Covid-19 open source data sets: A
comprehensive survey. Applied Intelligence (2020), 1–30.
[49] Lisa Singh, Shweta Bansal, Leticia Bode, Ceren Budak, Guangqing Chi, Kornraphop Kawintiranon, Colton Padden,
Rebecca Vanarsdall, Emily Vraga, and Yanchen Wang. 2020. A first look at COVID-19 information and misinformation
sharing on Twitter. arXiv preprint arXiv:2003.13907 (2020).
[50] Viren Swami and David Barron. 2020. Analytic thinking, rejection of coronavirus (COVID-19) conspiracy theories, and
compliance with mandated social-distancing: Direct and indirect relationships in a nationally representative sample of
adults in the United Kingdom. (2020).
[51] Fabio Tagliabue, Luca Galassi, and Pierpaolo Mariani. 2020. The “Pandemic” of Disinformation in COVID-19. SN
Compr Clin Med 2 (Aug. 2020), 1287–1289. https://doi.org/10.1007/s42399-020-00439-1
[52] David Martinus Johannes Tax. 2002. One-class classification: Concept learning in the absence of counter-examples.
(2002).
[53] Eric Tucker. 2020. US officials: Russia behind spread of virus disinformation. https://apnews.com/article/ap-top-newshealth-moscow-ap-fact-check-elections-3acb089e6a333e051dbc4a465cb68ee1. Accessed: 2020-11-21.
[54] United States Department of State. 1987. Soviet influence activities: a report on active measures and propaganda, 1986-87.
Technical Report. U.S. Department of State. 22, 54 pages.
[55] Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski,
Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. 2020. SciPy 1.0: fundamental algorithms for scientific
computing in Python. Nature methods 17, 3 (2020), 261–272.
[56] Sam Wineburg, Sarah McGrew, Joel Breakstone, and Teresa Ortega. 2016. Evaluating information: The cornerstone of
civic online reasoning. Stanford Digital Repository. Retrieved January 8 (2016), 2018.
[57] Kai-Cheng Yang, Christopher Torres-Lugo, and Filippo Menczer. 2020. Prevalence of Low-Credibility Information on
Twitter During the COVID-19 Outbreak. (2020). https://doi.org/10.36190/2020.16
[58] Kai-Cheng Yang, Onur Varol, Pik-Mai Hui, and Filippo Menczer. 2020. Scalable and Generalizable Social Bot Detection
through Data Selection. AAAI 34, 01 (April 2020), 1096–1103.
[59] Virginia Alvino Young. 2020. Nearly Half of the Twitter Accounts Discussing ’Reopening America’ May Be Bots.
https://www.scs.cmu.edu/news/nearly-half-twitter-accounts-discussing-reopening-america-may-be-bots. Accessed:
2020-11-19.
[60] Xinyi Zhou, Apurva Mulay, Emilio Ferrara, and Reza Zafarani. 2020. ReCOVery: A Multimodal Repository for
COVID-19 News Credibility Research. arXiv preprint arXiv:2006.05557 (2020).
[61] Xinyi Zhou and Reza Zafarani. 2020. A Survey of Fake News: Fundamental Theories, Detection Methods, and
Opportunities. ACM Comput. Surv. 53, 5 (Sept. 2020), 1–40.

, Vol. 37, No. 4, Article 111. Publication date: November 2020.

