1st Bingjie Yan

co-1st Boyi Liu

School of Compute Science and Cyberspace Security
Hainan University
Haikou, China
bj.yan@ieee.org

State Key Laboratory of Internet of Things for Smart City
University of Macao
Macau SAR, China
by.liu@ieee.org

2nd Yize Zhou

3rd Zhixuan Liang

4th Cheng-Zhong Xu

School of Science
Hainan University
Haikou, China
Yz.zhou@ieee.org

Department of Computing
The Hong Kong Polytechnic University
HongKong, China
zhixuan.liang@connect.polyu.hk

Faculty of Science and Technology
University of Macao
Macau SAR, China
czxu@um.edu.mo

Abstract—Federated Learning (FL) creates an ecosystem for
multiple agents to collaborate on building models with data
privacy consideration. The method for contribution measurement
of each agent in the FL system is critical for fair credits
allocation but few are proposed. In this paper, we develop
a real-time contribution measurement method FedCM that is
simple but powerful. The method defines the impact of each
agent, comprehensively considers the current round and the
previous round to obtain the contribution rate of each agent with
attention aggregation. Moreover, FedCM updates contribution
every round, which enable it to perform in real-time. Real-time is
not considered by the existing approaches, but it is critical for FL
systems to allocate computing power, communication resources,
etc. Compared to the state-of-the-art method, the experimental
results show that FedCM is more sensitive to data quantity
and data quality under the premise of real-time. Furthermore,
we developed federated learning open-source software based on
FedCM. The software has been applied to identify COVID-19
based on medical images.

This paper is supported in part by National Key Research and Development Program of China (No.2019YFB2102100), the Science and Technology Development Fund of Macau SAR (No. 0015/2019/AKP), the national
Natural Science Foundation of China (Grant No. 61762033) and Hainan
Provincial Natural Science Foundation of China (Grant No. 2019RC041 and
2019RC098).
Corresponding author: Boyi Liu, Email: by.liu@ieee.org
Boyi Liu and Bingjie Yan contribute equally to this work.

...

Increasing developments of artificial intelligence make machine learning services become parts of our daily lives. Numerous Data are generated, collected, and accessed every day by
smart terminals. Nevertheless, it is usually challenging to share
them considering the privacy. For instance, it is impractical to
use the language model trained by centralizing private data
to predict the next word or even the entire reply because of
data privacy [1]. The emergence of federated learning (FL)
breaks barriers among agents. FL enables terminal devices
to use their local computing capabilities to coordinate model
training while keeping data locally.

Contribution
Measurements
of Agents
Agent 1
Agent 2

Returned
Model

Agent1

Agent2

Upload local
parameters

Upload local
parameters

FedCM

Contribution
distribution

I. I NTRODUCTION

Federated Learning
With Contribution Measurement

Contribution
distribution

arXiv:2009.03510v2 [cs.LG] 11 Feb 2021

FedCM: A Real-time Contribution Measurement
Method for Participants in Federated Learning

Agent3

Agentn

Fig. 1. Federated learning framework with FedCM.

FL system needs to encourage different agents to contribute
their data and participate in the collaboration federation.
Rewarding mechanism is crucial for the incentives current
and potential participants of FL. Before that, FL needs a fair
evaluation mechanism to give agents reasonable rewards. Each
agent in FL should get corresponding rewards based on its
contribution to the shared model, not the same reward. There
are several proposed methods for contribution measurement.
Such as [2] measured the contribution of each group features
in vertical federated learning, and [3] proposed an incentive
mechanism to stimulate each agent to contribute better data
in both quality and quantity. However, most of them consume
considerable computing resources and calculate offline. Therefore, the existing methods are not capable of performing in
real-time in FL systems.

The performance of real-time is critical to FL systems.
Federated learning is a type of loosely distributed machine
learning, which requires timely measurements of the contributions of the agents to allocate rewards such as computing resources, communication resources, etc. The existing
computing-intensive and offline methods are difficult to deploy
on FL systems.
To address the issue, we develop a real-time contribution
measurement method (FedCM) for participants in federated
learning. The proposed method is simple but powerful. In the
method, the attention mechanism of model aggregation is used
to calculate the attention weight of all agents in FL. Each agent
trains locally and uploads the parameters to the server. The
server then adjusts to the centralized model and distributes it
to the agents. “Attention” is assigned to each agent. The server
then updates the parameters based on the “attention” of each
agent. In FedCM, the method calculates the attention of each
agent on the server, and get the attention of each agent on
each layer of the centralized model. Then, the initial and final
values of the server parameters are used to obtain the changed
value of each parameter of the server. Finally, a mathematical
formula is established to solve using the attention-based model
[4]. The evaluator can compare the ideal results in the data
transfer update to determine the contributions of all agents,
and measure the contribution of each agent.
FedCM is convenient to deploy in the FL system, and we
have actually developed a Federated Learning software based
on this. More meaningfully, we open source the software1 and
it has been applied to medical image recognition of COVID19.
Our contributions in the work are summarized as follows:
• We propose a method to measure contributions of agents
in the FL system, which consumes less computing resources and performs in real time. And we open sourced
the method2 .
• The work conducts training for natural language processing with federated learning. The method performs more
sensitive to the quantity and quality of data of agents in
FL system than the state-of-the-art method.
• An open sourced federated learning software is developed. The software has been applied to medical images
recognition of COVID-19.
In the next chapters of this paper, we first briefly introduce Federated Learning. We then introduce the proposed
method. The forth chapter presents experiments. The software
”FedMedical” presents in chapter five. We conclude the paper
with future work in the last chapter.
II. R ELATED W ORK
In this section, we will introduce federated learning and its
applications briefly. In addition, it also introduce the latest
contribution measurement algorithm for participants in the
federated learning system.
1 FedMedical
2 Code

is available at https://github.com/beiyuouo/paddle-fl-gui
is available at https://github.com/beiyuouo/FedCM

A. Federated Learning
Distribute the training data on each mobile device to maintain the localization of the data, instead of transmitting the
data to the central server, updating the model locally, and
uploading the update results to the server. While maintaining
data localization and privacy, it can aggregate the data of each
agent. The central server collects agent data and uses FedSGD,
FedAVG [5] and other algorithms to maintain the centralized
model in combination with the different optimizer [6], [7], and
sends the updated model to each agent. During the transmission process, methods such as homomorphic encryption are
used to protect the security of data transmission and maintain
the continuous iterative update of the model. This method is
federated learning.
At present, for different datasets, federated learning framework can be classified into horizontal federated learning,
vertical federated learning, and federated transfer learning
[8]. Horizontal federated learning is suitable for situations
where the data provided by the agents has more of the same
characteristics. In [9], Google proposed a solution to update
the horizontal federated learning model in Android phones.
vertical federated learning is suitable for situations where
there is less feature overlap but more user id overlap. [10]
proposed a vertical federated learning method for training a
logistic model. [11] proposed the FedHealth method, which
uses federated learning to aggregate data and uses transfer
learning to obtain a personalized model.
Under the framework of federated learning, there are many
different directions of research. [12] use the data transmitted
by the homomorphic encryption agents and the central server
for model training, which further strengthens the privacy of the
agents’ data. In [13], [14], data verification is carried out in
conjunction with the blockchain to prevent the gradient information from being maliciously tampered with. [15] research
on reducing the consumption of communication resources in
federated learning.
B. Application and Commercialization of Federated Learning
Since federated learning was proposed, federated learning
has been successfully applied to more and more scenarios.
When considering data privacy issues, many companies will
choose to use federated learning to protect data privacy to
achieve cooperation. Such WeBank has successfully used
federated learning in bank federations for credit evaluation
and other financial aspect. In [16], federated learning is
applied to dynamic IoT system. [13], [14] use blockchain
for data verification as an alternative to the commercialization
of federated learning. [17] achieved model commercialization
by providing a healthy marketplace for collaborative-training
models.
In the commercialization process of federated learning, the
method of using this decentralized training model provides
privacy, security, supervision and economic benefits, but also
brings fairness challenges. Therefore, a fair contribution measurement method is essential for FL system.

C. Contribution Measurement and Incentive Mechanism in
Federated Learning
[2] proposed grouped instance influence to measure contribution of agents in horizontal federated learning and grouped
feature importance in vertical federated learning. This approach equitably and accurately realized the measurement of
contributions of all agents but it cannot take into account the
quantity of data.
[18] proposed a federated learning optimization algorithm
based on differential privacy protection of the agents that can
hide the contribution of the agents. In order to balance the
contribution of the soft training model and ensure the collaborative convergence, [19] proposed the corresponding parameter
aggregation scheme. [20] used the weighted subjective logic
model to design a reputation-based worker selection plan to
achieve reliable federated learning. In [3], the authors designed
an incentive mechanism based on deep reinforcement learning
to determine the optimal pricing strategy for parameter servers
and the optimal training strategy for agents.
Our work focus on real-time contribution measurement
mechanism for each agent in vertical federated learning.
Current researches have problems such as poor real-time
performance and high resource consumption by contribution
measurement methods. The method proposed in this paper can
obtain real-time contribution measurement to each agent in the
process of federated learning. At the same time, it verifies that
the method proposed in this paper has high sensitivity on data
quantity and data quality.
III. M ETHODOLOGY
In this section, we will introduce the basic framework of
federated learning and how parameters are updated. Based on
this, we then introduce the proposed method in detail.

Algorithm 1 Federated Averaging Algorithm
1: Server model update
2: K is the total number of agents, wt is the parameters
of current central server model, wtk is the parameters of
current agents’ model with index k.
k
3: Input: server parameters wt at t, agent parameters wt+1
at t + 1
4: Output: server parameter wt+1 at t + 1
5: Initialize w0
6: for each round t = 1, 2, · · · do
7:
m ← max{C · K, 1}
8:
st ← random set of m agents from all K agents
9:
for each k ∈ St do
k
10:
wt+1
← ClientUpdate(k, wtk ) //agents update model
on local device
11:
end for
k
12:
wt+1 ← ServerOptimization(wt ,wt+1
)
13: end for
1: Local model update
2: K is the total number of agents, B is the local minibatch size, E is the number of local epochs, S is a set of
all agents, wt is the parameters of current central server
model, wtk is the parameters of current agents’ model with
index k, and η is the learning rate.
3: Input: index of client k, user private data xk
k
4: Output: client parameters wt+1
5: B ← Split user data into local mini-batch size B
6: for each local epoch e from 1 to E do
7:
for batch b ∈ B do
8:
w ← w − η 5 L(w; b)
9:
end for
10: end for
11: return w to server

A. The General Federated Learning Approach
Federated learning is a distributed learning method in which
the cloud(server) maintains a centralized model and distributes
it to individual agents. Rather than uploading agents’ data and
training models in the cloud, FL lets agents train a separate
model with their own and upload those black-box model
parameters to the server and fuse the models. The server
sets up a fraction C to select the agents proportionally for
the server’s centralized model update. The server updates the
centralized model, and distributes the model to the agents. This
not only avoids the transmission and storage of user’s sensitive
personal data, but also utilizes the computational power on
the agents and reduce the computation pressure from their
central server [2]. Federated averaging [9] algorithm is the
basic method of federated learning, which can be expressed
as Algorithm 1.
B. Attention Aggregation in FedCM
FedCM runs on the central server of federated learning
system, and performs after receiving the parameters of each
agent every round or every several rounds. It utilizes attention
mechanism. The server of Federated Learning with FedCM

uses equation 1 to calculate the value of attention α of each
layer parameter that should be allocated to the agent, and
multiplies each layer parameter by the corresponding attention
to update the centralized model. The server aggregates the
update parameters uploaded by the agents, and finally completes the update of the local model through the downlink
communication between the server and the agent. The process
of attention aggregation in FedCM can be summarized as the
following formulas and the attention aggregation function in
Algorithm 2.
l

esk
αkl = softmax(slk ) = P sl
i
ie

(1)

The slk is the norm difference from the central model. We
use wl to represent layer l parameters of the server and wkl for
lth layer parameters of the agent model k. So the definition
of slk is as follows
slk = ||wl − wkl ||p

(2)

To protect the agents’ data privacy, you can add the randomized mechanism before the agent passes parameters to
the server. Randomly generate a random vector that obeys
the standard distribution N (0, σ 2 ), multiply the corresponding
weight β, the results of the final update parameters as shown
in equation 3.
wt+1 ← wt − 

m
X

αk (wt − wtk + βN (0, σ 2 ))

(3)

k=1

Attention calculation is the first step of FedCM. The method
performs contributions quantification based on the parameters
obtained from attention aggregation.

Algorithm 2 FedCM-drived fedearted learning
1: Attention Calculation
2: l is the ordinal of neural layers;  is the stepsize of server
optimization
3: Input: server parameters wt at t, agents parameters
1
2
m
wt+1
, wt+1
, · · · , wt+1
at t + 1
4: Output: server parameters wt+1 at t + 1
5: Initialize attention α = {α1 , α2 , · · · , αm }
6: for each layer l in model do do
7:
for each agents k ∈ St from 1 to m do do
8:
Skl = ||wl − wkl ||p
9:
end for
sl
10:
αkl = sof tmax(slk ) = Pe ksl
i

C. Contributions Quantification in FedCM
This section introduces the procedure of contributions quantification in FedCM. Before that, we make the following
assumptions: Each agent does not tamper with the updated
gradient itself during the previous process of transmitting
between the server and the agents.
FedCM calculate each agent contribution to the system with
each layer of agents’ parameter attention. When the server has
obtained the attention of agents K, it can calculate the impact
of the agents K on the parameters of the centralized model in
the T -th step of update. We define impkt as follows:
log (wt − wtk ) + 1
+ βN (0, σ 2 )] + γ · impkt−1
log (wt+1 − wt ) + 1
(4)
where γ ∈ (0, 1) is forgetting coefficient, because large
variations in the early stage of the centralized model, the model
is not stable, and the impact of the previous endpoint should
be reduced after multiple iterations. If the agent k is not in the
selected M agents this time, we think that the impact of the
agent t round is impkt = impkt−1 , that is, only the number of
rounds that the agent participates in the update is calculated.
Note that the server’s attention to the agents are the attention
of each layer, so when calculating the impact it also calculates
the impact of each layer by reweighting and averaging.
FedCM normalizes agents’ attention in a limited range with
the MinMaxScaler and Softmax, to obtain the contribution of
each agent. We use conkt to represent the contribution of agent
k at the t step.
impkt = αk [

IV. EXPERIMENT
In this section, we will introduce the verification experiments performed for our proposed agents’ contribution measurement method.

11:
12:
1:
2:

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

e

i

end for
return αk = {αk1 , αk2 , · · · , αkl }
Contributions Quantification
Input: the ordinal of neural network layers l; the stepsize
of server optimization , server parameters wt at time t+1,
1
2
m
, · · · , wt+1
at time t + 1,
agents parameters wt+1
, wt+1
forgetting coefficient γ, selected agents set St .
Output: Agents contributions cont at t
k
Attention Calculation(wt , wt+1
)
for each agents k from 1 to K do
if k ∈ St then
log (wt −wtk )+1
2
impkt = αk [ log (wt+1
−wt )+1 + βN (0, σ )] + γ ·
k
impt−1
else
impkt = impkt−1
end if
end for
return cont = Softmax(MinMaxScaler(impt ))

images classification with datasets of MNIST3 and FashionMNIST4 to verify the effectiveness of FedCM and its sensitivity
to special variables. At the same time, we believe that the data
of each agent is independent and identically distributed. The
parameters are listed in Tab. I.
TABLE I
PARAMETERS
Name

Represent letter

Value

Number of agents
Server training rounds
The number of local epoch
The fraction of agents
Batch size
Step size
Differential privacy
Forgetting coefficient

K
round
epoch
C
B

β
γ

10
10
1
1
128
1.2
0.001
0.7

A. Experiment for Image Classification
1) Experimental Environment and Models: The system
used in experiments is Ubuntu 18.04 LTS, the backend used
is the GPU version of Pytorch, with the NVIDIA GTX1660Ti
GPU acceleration. We use ResNet to perform experiments for

3 MNIST

is available at http://yann.lecun.com/exdb/mnist
is available at https://github.com/zalandoresearch/
fashion-mnist
4 FashionMNIST

TABLE II
E XPERIMENT R ESULT OF I MAGE C LASSIFICATION

Model

Option

Dataset
MNIST

Normal
FashionMNIST

MNIST
Random

ResNet

FashionMNIST

MNIST
Mislabeled
FashionMNIST

 2 X U V
 6 K D S O H \  9 D O X H

    

Method

Contributions

FedCM

0.059;0.161;0.087;0.070;0.074;0.077;0.121;0.144;0.134;0.073

Shapley

0.100;0.100;0.104;0.099;0.099;0.103;0.099;0.098;0.102;0.096

FedCM

0.091;0.097;0.126;0.082;0.054;0.143;0.074;0.107;0.146;0.081

Shapley

0.099;0.101;0.099;0.099;0.099;0.100;0.099;0.101;0.100;0.102

FedCM

0.112;0.111;0.112;0.112;0.112;0.111;0.111;0.111;0.067;0.041

Shapley

0.110;0.110;0.110;0.110;0.109;0.109;0.109;0.109;0.062;0.062

FedCM

0.113;0.113;0.113;0.113;0.113;0.113;0.113;0.113;0.042;0.052

Shapley

0.106;0.105;0.106;0.106;0.105;0.105;0.106;0.106;0.078;0.077

FedCM

0.114;0.114;0.114;0.114;0.114;0.114;0.114;0.114;0.046;0.042

Shapley

0.110;0.109;0.110;0.109;0.109;0.110;0.109;0.109;0.063;0.062

FedCM

0.110;0.109;0.110;0.109;0.109;0.110;0.109;0.109;0.063;0.062

Shapley

0.106;0.106;0.105;0.105;0.106;0.105;0.105;0.106;0.077;0.078

 2 X U V
 6 K D S O H \  9 D O X H

    

 2 X U V
 6 K D S O H \  9 D O X H

 2 X U V
 6 K D S O H \  9 D O X H

    

    

    

    
    
    

    
    
    
    

    

    

    

    

    

 

 

 

 

 
 
 $ J H Q W V  , Q G H [

 

 

 

 

 

 

 

 

(a)

 
 
 $ J H Q W V  , Q G H [

 

 

 

 

    

    

    

 2 X U V
 6 K D S O H \  9 D O X H

    

    

 $ J H Q W V  & R Q W U L E X W L R Q  5 D W H

    

    

    

    

 

 

 
 
 $ J H Q W V  , Q G H [

(a)

 

 

 

 

 

 

 

 

 
 
 $ J H Q W V  , Q G H [

 

 

 

 

    

 

 

 

 

 
 
 $ J H Q W V  , Q G H [

 

 

 

 

(b)

Fig. 4. Experimental results of mislabeled case on FashionMNIST(a) and
MNIST(b).

proposed a FL measurement method based on Shaply Value.
It provides a way to measure the impact and contribution of
various agents. The definition of Shapley Value is:

    

    

ϕi (x) =

    

 

    

(a)

 2 X U V
 6 K D S O H \  9 D O X H
    

 

    

(b)

Fig. 2. Experimental results of normal measurement on FashionMNIST(a)
and MNIST(b).

 $ J H Q W V  & R Q W U L E X W L R Q  5 D W H

    

    

    

    

    

 $ J H Q W V  & R Q W U L E X W L R Q  5 D W H

    

 $ J H Q W V  & R Q W U L E X W L R Q  5 D W H

 $ J H Q W V  & R Q W U L E X W L R Q  5 D W H

 $ J H Q W V  & R Q W U L E X W L R Q  5 D W H

    

    

X
Q⊆S\{i}

 

 

 

 

 
 
 $ J H Q W V  , Q G H [

 

 

 

 

(b)

Fig. 3. Experimental results of random noise case on FashionMNIST(a) and
MNIST(b).

2) Experimental Result: We assumed three scenarios in
image classification federation learning to verify the effectiveness of our method and sensitivity to data. And we compared
it with the results of the Shapley Value evaluation. Shapley
Value is originated from coalitional game theory and has
proven theoretical properties. And the state-of-the-art work [2]

|Q|! − (|S| − |Q| − 1)!
(∆Q∪{i} (x)−∆Q (x))
|S|!

(5)
S is the set of all agents, Q ⊂ S = 1, 2, · · · , n is a subset
of the agent set S, i is the index of the agents, | · | represents
the size of the set, ∆Q (x) = impQ denotes the impact of
agent set Q. We use the following estimation method to get
the ϕi (x) of each agent because the complexity of directly
calculating Shapley value is too high.
ϕi (x) =

M
1 X
(∆Qm ∪{i} (x) − ∆Qm (x))
M m=1

(6)

where M is the number of iterations. ∆Qm (x) denotes
the impact of random set Qm . Finally, all the data obtained

converted word vector is then used as an input to the RNN
model, and the prediction result is finally output.
We use testing perplexity as an indicator of the evaluation
model. In information theory, perplexity is a measurement
of how well a probability distribution or probability model
predicts a sample. The perplexity of a discrete probability
distribution is defined as:
ppl(x) = 2H(p) = 2−

P

x

p(x)log2 p(x)

(7)

In the above equation, H(p) is the expected probability
distribution. If the prediction result of our model is m(x),
then the perplexity of the language model is defined as:
ppl(x) = 2H(p,m) = 2−

P

x

p(x)log2 m(x)

(8)

Obviously, H(p) 6 H(p, m). Therefore, the smaller the
perplexity, the more representative the probability distribution
can be to better predict the sample distribution.
s h a p le y v a lu e
fe d -a tt

0 .0 8

A g e n ts c o n trib u tio n ra te s

by Shapley Value is subjected to the same regularization
processing as ours.
Note that even if use estimation method, the contribution
of Shapley Value obtained must be recalculated after removing some agents, and FedCM does not require recalculation
and can get real-time contributions after each round of
centralized model aggregation. This mechanism enables
FedCM perform in real-time during the federated learning
process.
– Experimental results of the normal measurement
We did not do any special treatment to any agents and
randomly divided the dataset into each agent. The result is
shown in Fig. 2. It can be seen that the deviation of each
agent is not large, only individual agents are too high or too
low. And the result of Shapley Value is more even.
– Experimental results of the random noise case
In this experiment, we modified the datasets of the last two
agents’ data to random noise, such as Fig. 5. These should be
regarded as dirty data by the model, so as to get a smaller
return. The results are shown in Fig. 3. Both the method
proposed in this paper and the Shapley Value method can
identify these bad agents and give them a small contribution.
The gap between normal agents and special agents is more
obvious in our result.
– Experimental results of the mislabeled case
In image classification scenarios, wrong labels may seriously affect the quality of the data. In this experiment, we
randomly change the label in the special agents’ data and the
results are shown in Fig. 4. The results are similar to the results
of the previous experiment, and a better result than Shapley
Value can be obtained, indicating that our method is more
sensitive to such poor quality data.

S p e c ia l a g e n ts

0 .0 6

0 .0 4

0 .0 2

0 .0 0
0

1

2

3

4

5

6

7

8

9

1 0

1 1

1 2

1 3

1 4

1 5

1 6

1 7

1 8

1 9

A g e n ts in d e x

Fig. 6. Experimental results of normal measurement.
0 .0 7

s h a p le y v a lu e
fe d -a tt

A g e n ts c o n trib u tio n ra te s

0 .0 6

Fig. 5. Random noise data belonging to special agents.

0 .0 5
0 .0 4
0 .0 3

S p e c ia l a g e n ts

0 .0 2
0 .0 1
0 .0 0
0

B. Experiment for Language Processing
1) Experimental Environment and Models: The system
environment is the same as above. But we add agents number
to 20 and perform verification experiments for the language
processing model on public language dataset of Penn Treebank5
In natural language processing, the RNN model is often
used for processing. We used a smaller GRU-based agent
language model. First, take texts as input, and convert words
into word vectors according to a pre-built dictionary. The
5 Penn Treebank is available at https://github.com/wojzaremba/lstm/tree/
master/data

1

2

3

4

5

6

7

8

9

1 0

1 1

1 2

1 3

1 4

1 5

1 6

1 7

1 8

1 9

A g e n ts in d e x

Fig. 7. Experimental results of randomize word sequence case.

2) Experiment Result: We randomly divided the data in
the dataset and distributed them evenly to all agents. In the
language model, the amount of data will be more important, so
that the model can be expressed to a meaningful degree. In this
experiment we imagined three kinds of different scenarios: reduce the amount of data by 30% and 70%, randomly generate
the word sequence. Comparing the measurement results after
special processing with the results when unprocessed data, it is
concluded that the sensitivity of variables such as data quantity

TABLE III
E XPERIMENT R ESULT OF I MAGE C LASSIFICATION
Model

Option

Dataset

normal

ptb

less

ptb

Method

Contributions

Ours

0.060;0.028;0.032;0.046;0.032;0.043;0.068;0.071;0.075;0.051;0.035;0.060;0.044;0.036;0.071;0.039;0.039;0.071;0.046;0.053

Shapley

0.041;0.059;0.052;0.044;0.038;0.052;0.066;0.062;0.052;0.047;0.030;0.051;0.041;0.050;0.045;0.042;0.083;0.040;0.054;0.052

Ours

0.057;0.050;0.048;0.054;0.051;0.053;0.059;0.063;0.068;0.061;0.054;0.054;0.059;0.052;0.062;0.055;0.025;0.025;0.025;0.025

Shapley

0.044;0.057;0.049;0.059;0.086;0.045;0.057;0.042;0.047;0.041;0.058;0.047;0.046;0.064;0.059;0.044;0.049;0.041;0.033;0.032

Ours

0.056;0.044;0.048;0.059;0.050;0.054;0.061;0.064;0.064;0.068;0.052;0.051;0.057;0.049;0.059;0.055;0.031;0.031;0.025;0.025

RNN
lessrank

ptb

random

ptb

0 .1 0

Shapley

0.040;0.034;0.073;0.062;0.042;0.053;0.068;0.047;0.068;0.042;0.064;0.045;0.062;0.054;0.051;0.066;0.027;0.045;0.027;0.030

Ours

0.059;0.050;0.050;0.056;0.050;0.054;0.061;0.060;0.063;0.061;0.060;0.057;0.058;0.049;0.061;0.056;0.023;0.024;0.024;0.023

Shapley

0.056;0.058;0.057;0.056;0.054;0.058;0.060;0.060;0.057;0.056;0.052;0.057;0.055;0.058;0.055;0.055;0.025;0.022;0.024;0.023

s h a p le y v a lu e
fe d -a tt

A g e n ts c o n trib u tio n ra te s

0 .0 8

0 .0 6
S p e c ia l a g e n ts
0 .0 4

0 .0 2

0 .0 0
0

1

2

3

4

5

6

7

8

9

1 0

1 1

1 2

1 3

1 4

1 5

1 6

1 7

1 8

1 9

A g e n ts in d e x

Fig. 8. Experimental results of reduce data case.
s h a p le y v a lu e
fe d -a tt

A g e n ts c o n trib u tio n ra te s

0 .0 8

0 .0 6

identify these bad agents and give them a small contribution.
– Experimental results of the reducing data case
To demonstrate the sensitivity of our measurement method
to the amount of data, we performed the experiment. In
this experiment, we reduced the data amount of the last 4
agents by 70%, and the data amount of other agents remained
unchanged. As seen in Fig. 8, the contributions of the specially
treated agents are significantly reduced. But it is relatively not
obvious in the measurement results of Shapley Value.
In order to reflect the relative relationship between the
amount of data, we processed the data of the last 4 agents:
agents with index 16 and 17 reduced the amount of data by
30%, agents with index 18 and 19 reduced the amount of data
by 70%. It can be seen in Fig. 9 that the method proposed
in this paper can reflect the reduced amount of data, while
Shapley Value cannot show it well. Note that the method
proposed in this paper can better reflect the difference of
smaller data volume.

S p e c ia l a g e n ts
0 .0 4

0 .0 2

0 .0 0
0

1

2

3

4

5

6

7

8

9

1 0

1 1

1 2

1 3

1 4

1 5

1 6

1 7

1 8

1 9

A g e n ts in d e x

Fig. 9. Experimental results of reduce data different levels.

and data quality is evaluated.
– Experimental results of normal measurement
We did not do any special treatment to any agents and
randomly divided the dataset into each agent. The result is
shown in Fig. 6. It can be seen that the deviation of each
agent is not large, only individual agents are too high or too
low. And the measurement result is similar to Shapley Value.
– Experimental results of random word sequence case
In this experiment, we modified the datasets of the last
four agents into randomize word sequences. These should be
regarded as dirty data by the model, so as to get a smaller
return. The results are shown in Fig. 7. Both the method
proposed in this paper and the Shapley Value method can

V. F EDERATED LEARNING SOFTWARE WITH F ED CM
In addition to proposing an effective federated learning approach, we also developed a software “FedMedical” based on
FedCM. FedMedical is written in Python and can be deployed
on Linux, using the PaddleFL framework as the back-end of
the neural network to perform parameter updates and aggregation. FedMedical also reserves the contribution measurement
interface for implementation based on the FedCM method
and the deployment of some incentive mechanisms. We will
continue to improve these in the future.

Fig. 10. Interfaces of FedMedical.

At present, we have applied it in the medical field, such
as COVID-19 detection based on medical images. As shown

[2]
[3]
[4]
[5]
Fig. 11. Detection results of FedMedical.

[6]
[7]

in the figures above, they are interfaces of the software
FedMedical. FedMedical can realize COVID-19 detection
based on medical images under the premise of privacy protection based on the federated learning framework. This is of
great significance for shared medicine, because FedMedical
can use medical data from different places to train powerful AI
models for disease detection. And in this process, FedMedical
completely saves the patient’s data locally. If hospitals around
the world can apply FedMedical, the accuracy of our disease
detection model will be greatly improved.
VI. CONCLUSION
Federated learning is a machine learning framework for
protecting distributed data privacy and has participated in
commercial activities. However, there is a lack of a sufficiently
fair contribution measurement mechanism to distribute each
agent’s reward. To address this issue, we propose a real-time
contribution measurement method FedCM. FedCM defines
the impact of each agent, comprehensively considers the
current round and the previous round to obtain each agent’s
contribution rate based on attention aggregation. Moreover,
it is capable of performing in real-time which is important
in the real federated learning system. We conduct sufficient
experiments to verify the effects of FedCM. Moreover, the
federated learning software Fedmedical is developed and open
sourced for disease detection based on medical images.
For future work, we expect some more advanced algorithms
can be found that can more accurately measure the contribution to malicious agents, and at the same time prevent the
contribution confusion caused by more than half of malicious
agents. It also should consider that the negative contribution of agents measurement and the comprehensiveness of
the measurement. Preliminary classification (i.e., positive and
negative) is carried out at the end to avoid attacks under
the federated learning framework mechanism [21]; at the
same time, the concept of game theory is introduced, such
as the establishment of a PVCG mechanism on the supply
side [22]. Improvements in these directions will promote
the implementation and application of the federated learning
incentive mechanism.
R EFERENCES
[1] A. Ion, J. Frohnhofen, L. Wall, R. Kovacs, M. Alistar, J. Lindsay,
P. Lopes, H.-T. Chen, and P. Baudisch, “Metamaterial mechanisms,” in

[8]
[9]

[10]

[11]
[12]
[13]

[14]
[15]
[16]
[17]

[18]
[19]
[20]

[21]
[22]

Proceedings of the 29th annual symposium on user interface software
and technology, 2016, pp. 529–539.
G. Wang, C. X. Dang, and Z. Zhou, “Measure contribution of participants in federated learning,” in 2019 IEEE International Conference on
Big Data (Big Data). IEEE, 2019, pp. 2597–2604.
Y. Zhan, P. Li, Z. Qu, D. Zeng, and S. Guo, “A learning-based incentive
mechanism for federated learning,” IEEE Internet of Things Journal,
2020.
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
computation, vol. 9, no. 8, pp. 1735–1780, 1997.
H. Brendan McMahan, E. Moore, D. Ramage, S. Hampson, and
B. Agüera y Arcas, “Communication-efficient learning of deep networks
from decentralized data,” arXiv preprint arXiv:1602.05629, 2016.
V. Felbab, P. Kiss, and T. Horváth, “Optimization in federated learning.”
in ITAT, 2019, pp. 58–65.
S. Ji, S. Pan, G. Long, X. Li, J. Jiang, and Z. Huang, “Learning
private neural language modeling with attentive aggregation,” in 2019
International Joint Conference on Neural Networks (IJCNN). IEEE,
2019, pp. 1–8.
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning:
Concept and applications,” ACM Trans. Intell. Syst. Technol., vol. 10,
no. 2, Jan. 2019. [Online]. Available: https://doi.org/10.1145/3298981
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in Artificial Intelligence and Statistics. PMLR, 2017, pp. 1273–
1282.
S. Hardy, W. Henecka, H. Ivey-Law, R. Nock, G. Patrini, G. Smith,
and B. Thorne, “Private federated learning on vertically partitioned data
via entity resolution and additively homomorphic encryption,” arXiv
preprint arXiv:1711.10677, 2017.
Y. Chen, X. Qin, J. Wang, C. Yu, and W. Gao, “Fedhealth: A federated
transfer learning framework for wearable healthcare,” IEEE Intelligent
Systems, 2020.
Y. Aono, T. Hayashi, L. Wang, S. Moriai et al., “Privacy-preserving deep
learning via additively homomorphic encryption,” IEEE Transactions on
Information Forensics and Security, vol. 13, no. 5, pp. 1333–1345, 2017.
Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain and
federated learning for privacy-preserved data sharing in industrial iot,”
IEEE Transactions on Industrial Informatics, vol. 16, no. 6, pp. 4177–
4186, 2019.
H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, no. 6, pp.
1279–1283, 2019.
J. Konečnỳ, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and
D. Bacon, “Federated learning: Strategies for improving communication
efficiency,” arXiv preprint arXiv:1610.05492, 2016.
J. Ren, H. Wang, T. Hou, S. Zheng, and C. Tang, “Federated learningbased computation offloading optimization in edge computing-supported
internet of things,” IEEE Access, vol. 7, pp. 69 194–69 201, 2019.
X. Bao, C. Su, Y. Xiong, W. Huang, and Y. Hu, “Flchain: A blockchain
for auditable federated learning with trust and incentive,” in 2019 5th
International Conference on Big Data Computing and Communications
(BIGCOM). IEEE, 2019, pp. 151–159.
R. C. Geyer, T. Klein, and M. Nabi, “Differentially private federated
learning: A client level perspective,” arXiv preprint arXiv:1712.07557,
2017.
Z. Xu, Z. Yang, J. Xiong, J. Yang, and X. Chen, “Elfish: Resourceaware federated learning on heterogeneous edge devices,” arXiv preprint
arXiv:1912.01684, 2019.
J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive mechanism for reliable federated learning: A joint optimization approach
to combining reputation and contract theory,” IEEE Internet of Things
Journal, vol. 6, no. 6, pp. 10 700–10 714, 2019.
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How
to backdoor federated learning. corr,” arXiv preprint arXiv:1807.00459,
2018.
M. Cong, X. Weng, H. Yu, J. Qu, and S. M. Yiu, “Optimal procurement
auction for cooperative production of virtual products: Vickrey-clarkegroves meet cremer-mclean,” 2020.

