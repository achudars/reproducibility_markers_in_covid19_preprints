Highlights
Audio, Speech, Language, & Signal Processing for COVID-19: A

arXiv:2011.14445v1 [cs.SD] 29 Nov 2020

Comprehensive Overview
Gauri Deshpande, Björn W. Schuller
• Past research on audio, speech, language, and signal processing for
COVID-19 related health problems.
• COVID-19 screening, diagnosing, monitoring, and post-care using speech
processing.

Audio, Speech, Language, & Signal Processing for
COVID-19: A Comprehensive Overview
Gauri Deshpandea,b , Björn W. Schullera,c
a

Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg,
Germany
b
TCS Research Pune, India
c
GLAM – Group on Language, Audio, & Music, Imperial College London, UK

Abstract
The Coronavirus (COVID-19) pandemic has been the research focus worldwide in the year 2020. Several efforts, from collection of COVID-19 patients’
data to screening them for the virus’s detection are taken with rigour. A
major portion of COVID-19 symptoms are related to the functioning of the
respiratory system, which in-turn critically influences the human speech production system. This drives the research focus towards identifying the markers of COVID-19 in speech and other human generated audio signals. In this
paper, we give an overview of the speech and other audio signal, language
and general signal processing-based work done using ’Artificial Intelligence’
techniques to screen, diagnose, monitor, and spread the awareness about
COVID-19. We also briefly describe the research related to detect according COVID-19 symptoms carried out so far. We aspire that this collective
information will be useful in developing automated systems, which can help
in the context of COVID-19 using non-obtrusive and easy to use modalities
such as audio, speech, and language.
Keywords: COVID-19, digital health, audio processing, computational

Preprint submitted to Pattern Recognition

December 1, 2020

para-linguistics, affective computing
1. Introduction
More than 38 million confirmed cases of coronavirus-induced COVID-19
infected individuals are detected in more than 200 countries across the world
at the time of writing of this overview. The COVID-19 pandemic had a
wide spectrum of effects on the population, ranging from no symptoms to
life-threatening medical conditions. As per the world health organisation
(WHO)1 , the most common symptoms of COVID-19 are fever, dry cough,
and fatigue, and the symptoms of a severe COVID-19 condition are mainly
shortness of breath, loss of appetite, confusion, persistent pain or pressure
in the chest, and temperature above 38 degrees Celsius. The heavy droplets
generated when the infectious individual sneezes or coughs transmits the
virus causing COVID-19. Even breathing and talking to someone in the close
proximity of a COVID-19 infected individual can transmit the disease. With
the cognizance of these transmitting factors and symptoms, every individual
along with the health care professionals is required to take steps to cease the
spread.
It is vital to have an easy to use tool for screening, diagnosing, and monitoring the virus and its proliferation. An automated approach to detect
and monitor the presence of COVID-19 or its symptoms could be developed
using Artificial Intelligence (AI) based techniques. Although, AI techniques
are still in the process of reaching a matured stage, they can be still be used
for early detection of the symptoms, especially in the form of a self-care tool
1

www.who.int

2

Screening & Diagnosis
Of
COVID-19 Symptoms

Monitoring
Mask Wearing,
Cough &
Emotional Wellness

Cough Signal

Telephonic
Conversations

Awareness
About
COVID-19 Symptoms,
Care & Precautions

Speech Signal

Breathing Signal

Uploads on
Web Interface

Recording
Face-to-Face
Conversations

Figure 1: Capturing and processing speech and audio signals for COVID-19 applications

in reducing the spread, taking early care, and hence avoiding the severe conditions to propagate. As reviewed by the authors of [1], AI based approaches
using speech and other audio modalities have several opportunities in this
space. Also, the authors of [2] have been among the very first to identify and
collate the useful AI based techniques and the efforts taken for COVID-19,
right when the pandemic spread was at its peak.
As per the WHO department of mental health and substance abuse, the
current scenario of COVID-19 is susceptible to elevate the rates of stress
or anxiety among individuals. Especially, lock-down and social distancing,
quarantine and its after effects in the society might have adverse effects on
the levels of loneliness, depression, self-harm, or suicidal behaviour. A special
attention is needed for the health care providers, having to face the trauma
directly and spending long working hours in such scenarios. Both physical
and mental health needs have increased and require AI to provide faster and

3

easy to access solutions. Not only does identification and monitoring need
digital assistance, but also the post trauma phase would need it.
As depicted in Figure 1, we are discussing about capturing, pre-processing
and applying speech and other human audio data for screening, diagnosing,
monitoring and spreading the awareness of COVID-19 in this paper. Tables
1, 2, 3, 4, and 5 explain past speech and audio related work done to provide solutions for COVID-19 related health problems such as cough sound,
asthmatic sound, obstructive sleep apnea, breathing rate, and stress detection from speech signals. The references given in these tables can be used
readily by those who wants to develop and provide immediate solutions in
this space. These tables also mention the machine learning and deep learning
technologies used on the mentioned data-sets to provide mentioned accuracy.
For each speech or audio application, there is a vast space in the literature.
We have selected only a handful of them, considering their relevance in the
COVID-19 situation. Not everything that can be developed, can be used
in this scenario considering other factors such as social distancing and personal and environmental hygiene. Hence, the developments are required to
be driven by guidance from the clinicians and health care providers.
The rest of the paper is organised as follows. We start with Section 2
to explain the recent advances towards providing COVID-19 screening and
diagnosing tools using speech and audio signals. Similarly, in Section 3 we
talk about the recent development towards the applications of speech and
audio processing for monitoring the spread of the COVID-19 virus. In Section
4, we describe the use of speech based technologies in the growing awareness
about the pandemic so that the accurate knowledge about the disease, factors

4

that control its expansion and preliminary care that one has to take is known
to most of the individuals. We talk about the gaps where further research
and development is required in Section 5, and we conclude our discussion in
Section 6.
2. Screening and Diagnosing for COVID-19
In the clinical test for diagnosing COVID-19 infection, the anterior nasal
swabs sample is collected as suggested in [9]. These tests are performed by
the healthcare providers belonging to local or state healthcare departments.
As the medicinal drug or vaccine for the treatment of COVID-19 is still not
available, it is imperative to detect the infected individuals and physically
separate them from the healthy community to stop its wide spread. Hence,
additionally, as a part of any personal informatics system, the availability
of self-screening or self-diagnosing methodologies can aid in detecting and
self-isolating at an early stage itself.
There exists a fine line between screening and diagnosing, where screening
gives an early indication of the presence of a disease and diagnosing confirms
the presence/absence of disease. Screening is probabilistic, whereas diagnosis
is binary in nature. To scale-up the detection of COVID-19 virus, we discuss
different algorithms/applications using audio processing for screening and
diagnosis of COVID-19 in this section.
2.1. Cough Detection
Cough detection is about identifying the cough sound and differentiate
it from other similar sounds such as speech and laughter and also, to identify COVID-19 specific cough. As a first step, it requires cough and speech
5

Table 1: Past speech and audio analysis related to COVID-19 health problems: Cough
sound detection

Ref.

Features

[3]

12 Mel Frequency Cepstral
Coefficients (MFCCs)

Result
[4]

Result

[5]

Result
[7]

Result
[8]

Result

Dataset

Audio data of 7 Chronic Obstructive Pulmonary Disease (COPD)
patients for 90 days
Area under the curve (AUC) of around 0.916 using XGBoost.
Non-negative Matrix Fac- Cough samples from 9 patients
torisation (NMF) with Parameterisation using Gaussian distribution
1 % increment in accuracy, recall and f1 score with NMF than
traditional feature sets such as MFCC, and Gammatone Frequency Cepstral Coefficients (GFCC).
Short Time Fourier Trans- Google audio set extracts from
form
(STFT),
MFCC, 1.8 million Youtube videos and
Mel-filter Bank (MFB) the Freesound audio database [6]
using Deep Neural Network containing cough signals along
(DNN), convolutional Neu- with other sounds such as speech,
ral Network (CNN), and sneeze, throat clearing, and home
Long-Short Term Memory sounds
(LSTM)
MFB followed by STFT gives highest performance across
DNN, CNN, and LSTM.
Hu moments and k-Nearest Used open source audio signals
Neighbour (k-NN) classifier & sounds datasets and created
two using data collection experiments: a) Cough sounds in noisy
environments and b) 13 patients’
audio recordings including cough,
speech, and forced respiration.
88.51 % sensitivity and 99.7 % specificity in a variety of noise
conditions.
Spectrograms and CNN ar- 1 500 cough samples of 20 difchitecture
ferent subjects from 41 YouTube
videos and 5 cough samples from
the SoundSnap website.
92 % sensitivity at 99 % specificity.
6

9000
8000

2660

235

7000
6000

2001

5000
1001

4000
3000

632
30

2000
104

1000

165

70

299

0

Total Subjects

Publicly Available

COVID-19 Subjects

Sample Count

Figure 2: The Groups (mentioned on the x-axis) collected data from (the numbers mentioned on the y-axis) healthy and COVID-19 subjects. Only Coughvid and VoiceMed have
reported number of samples; all others have reported number of subjects. The datasets
from: Cambridge, Coughvid, and Coswara are publicly available.

samples of the same subject followed by collecting the COVID-19 and nonCOVID-19 cough sound samples so as to develop an AI model that can differentiate between them on its own. Figure 2 shows the number of healthy and
COVID-19 positive subjects or samples data collected by different groups.
Cough audio samples can be collected using a simple smartphone microphone. The Cambridge University2 provided a web based platform and an
android application for the general population to upload their cough sounds
along with some additional information such as their age, gender, brief medical history, location, symptoms, and if the participating subjects had been
tested positive. This data collection platform ask the participants to also
2

https://www.covid-19-sounds.org/en

7

read a sentence so that their speech can also be recorded. The samples comprise of 3 coughs, 5 breaths, and voice while reading via a periodic survey
which captured this data after every 2 days. However, in the work presented
in [10], only cough and breathing sounds are considered. As explained in [10],
the crowdsourced data collected comes from more than 10 different countries
and comprises of samples from more than 7 000 subjects with more than
200 COVID-19 positive subjects. With manual examination of each sample, 141 cough and breathing samples of COVID-19 positive tested users and
298 samples from non-COVID-19 users (those having no medical history, no
symptoms, and belonging to non-prevalent countries at that time) are used
for building a binary classification model to distinguish between COVID-19
and non-COVID-19 users. Similary, 54 ”COVID-19 with cough” samples are
distinguished from 32 ”non-COVID-19 cough” samples, and from 20 nonCOVID-19 asthmatic cough samples. Further, while analysing this data, the
hand crafted features such as acoustic tempo, period, Root Mean Square Energy (RMSE), spectral centroid, roll-off energy, Zero Crossing Rate (ZCR),
MFCC, and its derivatives are extracted. Along with the handcrafted features, the authors of [10] used another approach as well, in which transfer
learning using the VGGish model is developed using videos from YouTube.
The authors achieved an AUC of 0.8 for distinguishing COVID-19 subjects
from non-COVIDs using logistic regression and again a AUC of 0.8 for distinguishing COVID-19 cough from non-COVID-19 and asthmatic cough using
a support vector machine. The authors found handcrafted features along
with VGGish based features to give the best performance. Together, cough
and breathing signals perform the best in classifying COVID-19 users from

8

non-COVID-19 users. However, breathing signals alone are better suited for
classifying COVID-19 positive users from non-COVID-19 users having asthmatic cough. With data augmentation, the authors could improve the results
by 7-8 %.
Coughvid3 is another such app from EPFL (Ecole Polytechnique Fédérale
de Lausanne) to detect COVID-19 cough from other cough categories such as
normal cold and seasonal allergies. Till date, this dataset [18] has more than
20 000 cough samples collected, where all the samples are passed through an
open-source cough detection machine learning model to identify the cough
segments. More than 2 000 samples are labelled by 3 expert pulmonologists
for the respiratory conditions along with the COVID-19 status. The dataset
has 632 COVID-19 labels given by expert 1 alone, however, there exists
no agreement on the COVID-19 diagnosis (Fleiss’ Kappa score 0.00). This
dataset is made publicly available along with a machine learning model for
identifying cough from other sounds.
Another such effort called, ’Breath for Science’

4

– a team of scientists

from NYU –, have developed a web based portal to register the participants
where they can enter similar details along with a phone number. On pressing
a ’call me’ button, the participants receive a callback where they have to
cough three times after the beep. They have mentioned that this is available
for only US citizens as of now.
As explained in [19], another corpus called ”Coswara” of 941 subjects
and 9 different sounds is formed using a web interface developed by IISC
3
4

https://coughvid.epfl.ch
https://www.breatheforscience.com

9

Table 2: Past speech and audio analysis related to COVID-19 health problems: Cough
sound detection (continued)

Ref.

Features

[11]

MFCC with RandomForest
Classifier (RFC).

Result

[12]

Result
[13]

Result
[14]

Result
[15]

Result
[17]

Result

Dataset

Forced cough and snore sounds
from 26 healthy subjects superimposed with the AC noise.
An average accuracy of 0.96 ± 0.08, F1 score of 0.96 ± 0.08,
and AUC-Receiver Operating characteristics (ROC) of
0.98 ± 0.04 in classifying snore and cough sounds
Visual analysis using CNN Audio signals captured using a
and sequential analysis us- piezo sensor of 14 healthy subing Recurrent Neural Net- jects comprising of cough, speech,
work (RNN)
and other sounds
CNN Specificity: 92.2 % & RNN sensitivity: 87.7 %.
CNN with device-agnostic Cough, speech, laughter, throat
bagging ensemble method
clearing, and forced expiration
sounds of 43 healthy participants.
Mean value range 85.9 % to 90.9 % over 5 different recording
devices.
Principal
Component Acted cough from 17 patients
Analysis (PCA) on audio having a cough due to common
spectrograms, Fast Fourier cold (n=8), asthma (n=3), alTransfor (FFT) coefficients, lergies (n=1), and chronic cough
and RFC
(n=5)
True positive rate: 92 %, false positive rate: 0.5 %.
Three spectral band fea- 43 real-world environment recordtures along with a logistic ings from the video sources of
regression classifier
[16], with sounds of COPD, pertussis, croup, common cold, bronchitis, bronchiolitis & asthma.
Sensitivity: 90.31 %, specificity: 98.14 %, F1-score: 88.70 %.
Local image (Hu) moments 13 subjects samples with 3 respiover audio spectrograms
ratory conditions for 1 day having low, medium, and high noisy
background.
Smartphone based cough detector having 88.94 % sensitivity
and 98.64 % specificity in noisy environments with optimal
battery consumption.
10

Bangalore India. The nine sounds include, shallow and deep cough, shallow
and deep breathing, the vowels /ey/, /i/, and /u/, and one to twenty digit
counting in normal and fast speaking rate. The metadata collected from the
participants includes age, gender, location, current health status (healthy /
exposed / cured / infected) and the presence of co-morbidity. Here, along
with the subjects labelling their own data, one more annotation is provided
by the listener to assign it to one of the 9 categories. The dataset comprises
of audio samples from 104 unhealthy users. After curating, the dataset is
publicly available at Github5 . The authors achived an accuracy of 66.74 % in
classifying the samples into 9 categories using acoustic features such as spectral contrast, 13 MFCCs, spectral roll-off, spectral centroid, mean square
energy, polynomial fit to the spectrum, ZCR, spectral bandwidth, and spectral flatness. A subset of Coswara along with data gathered using the Stanford University led Virufy mobile app6 and Google’s AudioSet [20] is used
to train a MFCC-CNN model for identifying COVID-19 cough [21]. The
authors have used a total of 16 COVID-19 cough samples and achieved an
accuracy of 100 % in the classification task, however, the dataset considered
too small to draw conclusions for generalisation ability in real-life settings.
VoiceMed7 is another android and web application which captures crowdsourced speech and cough sounds and returns the COVID-19 infection status
on the fly. The different stages in this cloud-based pre-trained CNN based
system comprises of pre-processing the collected signal, using a cough detec5

https://github.com/iiscleap/Coswara-Data
Virufy, http://archive.is/hbrfE
7
https://voicemed-791a3.firebaseapp.com
6

11

tor to identify if it is a cough signal and then a COVID-19 cough detector to
further detect if the audio signal is a COVID-19 cough. As explained in 8 ,
the authors used 900 cough and 2 000 non-cough audio samples for building
the cough detector. Similarly, the authors have used 165 COVID-19 and
613 non-COVID-19 samples for building the COVID-19 cough detector. The
accuracy of the cough classifier is reported to be 83.7 % and the accuracy of
COVID-19 classifier is reported as 89.69 % using spectrograms. The authors
mention that identifying older COVID-19 patients and COVID-19 patients
with respiratory disorders is a further complex problem.
However, as opposed to the self annotated data, the authors of [22] collected a dataset of 3 087 clinically validated samples, in which, 1 001 positive
and 2 086 negative COVID-19 samples are present. The feature set used for
cough classification are, MFCC, ZCR, roll-off frequency, and spectral centroid. The authors have described a deep convolutional architecture termed
as ’DeepCough’, which is further deployed as a web based pre-screening tool
called ’CoughDetect’ 9 .
Another web interface ’CoughAgainstCovid’ 10 for COVID-19 cough sample collection is an initiative by the Wadhwani AI group in collaboration with
the Stanford University11 . As explained in [23], the authors have collected
the cough sounds forcefully produced by 3 621 individuals using a smartphone microphone in the setups established at testing facilities and isolation wards across India. This dataset contains data from 2 001 COVID-19
8

https://health-sounds.cl.cam.ac.uk/workshop20/Thayabaran Kathiresan.mp4
https://coughdetect.com
10
https://www.coughagainstcovid.org
11
https://www.stanford.edu
9

12

positive subjects, where these subjects’ Reverse Transcription–Polymerase
Chain Reaction (RT-PCR) tests are also conducted for confirmation. The
authors have also developed a CNN architecture based model for classification of COVID-19 cough from Non-COVID-19 cough sound. The feature set
of RMSE, tempo, and MFCCs is giving a specificity of 31 % for a sensitivity
of 90 %.
Similarly, cellular call recordings of 88 subjects, with 29 positive and 59
negative COVID-19 clinically tested individuals is collected in [24]. The subjects were asked to provide the speech of /ah/ and /z/, counting from 50
to 80 and coughs for 14 days. The authors have compared the performance
using three deep learning components, attention based transformer, a GRUbased expert classifier with aggressive regularisation and ensemble stacking.
The authors report that the performance with /z/ phoneme is better when
compared to /ah/ and counting. Among the deep learning techniques, transformer based experiments gave better F1 scores.
A cloud based smartphone app is described in [31], for detecting COVID19 cough. As a first step, the authors have used a CNN based cough detector,
which identifies cough sounds from over 50 environmental sounds. The authors built this detector using the ESC-50 dataset [32]. In the next stage,
to diagnose a COVID-19 cough, they collected 96 bronchitis, 130 pertussis, 70 COVID-19, and 247 normal cough samples to train their COVID19 cough detector model.

Using MFCCs for feature representation and

t-distributed stochastic neighbour embedding for dimensionality reduction,
they have trained three models: a deep transfer learning-based multi-class
classifier (using a CNN), a classical machine learning-based multi-class clas-

13

Table 3: Past speech and audio analysis related to COVID-19 health problems: Asthmatic
sound detection

Ref.

Features

[25]

INTERSPEECH
2013 Speech from 47 asthmatic and 48
Computational
Paralin- healthy controls
guistics Challenge baseline
acoustic features [26]
78 % Accuracy.
MFCC, pitch, intensity, Vowel pronunciation samples
jitter, shimmer, formants, of 21 asthmatic and 21 nonharmonicity, fundamental asthmatic individuals
frequency features with
Dynamic Time Warping
(DTW)
Markers of asthmatic individuals: Lower pitch, higher standard deviation of pitch, higher degree of voice breaks, lower
intensity, shimmer value greater than 3.8, higher jitter, average Harmonics to Noise Ratio (HNR) of 14.4, higher F1, and
lower F2.
MFCCs with Gaussian Mix- Respiratory sound of 9 asthmatic
ture Model (GMM)
and 9 non-asthmatic adults
Sensitivity of 0.881 and a specificity of 0.995.
Correlation between HNR Spirometry data, Vowel (/a:/,
of speech signal and Forced /e:/, /i:/, /o:/, /u:/, /epExpiratory
Volume
to silon:/, /open-o:/) pronunciation
Forced
Vital
Capacity and phrase “She sells” of 150
(FEV1/FVC) ratio ob- asthmatic subjects, analysis done
tained from spirometry
on 33 samples.
Highest correlation coefficient of 42.08 found between the
(FEV1/FVC) ratio and HNR of vowel /epsilon:/ sound.
Speech features: pause time Samples from 91 with asthma
and frequency, Prosodic fea- and/or COPD, and 40 healthy
tures: absolute and relative controls.
jitter and shimmer
68 % accuracy in differentiating patients from healthy individuals and 89 % accuracy in differentiating the subset of patients
with the highest disease severity from healthy ones.

Result
[27]

Result

[28]
Result
[29]

Result
[30]

Result

Dataset

14

sifier (using a Support Vector Machine (SVM)), and deep transfer learning
based binary classifier (again using a CNN). These three models reside in
the AI4COVID engine, where a decision is made as COVID-19 positive or
negative if the output of all the three models’ outputs are the same; else, it
says that the test is inconclusive. With this, the authors report an accuracy
of more than 95 % in identifying cough sounds from non-cough sounds. The
three engine-based models yield an accuracy of 92.64 %, 88 % and 92.85 %
respectively for detecting a COVID-19 cough sound. The overall performance indicates that the app can detect COVID-19 infected individual with
a probability of 77.3 %.
The COVID-19 cough data collection at Massachusetts Institute of Technology (MIT) is done using the web app

12

, in which each subject gave 3

forced cough recordings, diagnosis details, and other demographic metadata.
The authors have used MFCCs with a CNN architecture, and ResNets to
build a baseline model using the collected dataset. To understand the impact on COVID-19 diagnosis by the four bio-markers: muscle degradation,
vocal cords, sentiments, and lung & respiratory tract, the authors of [33]
have used these bio-markers as a pre-training step for the baseline model.
This baseline model’s performance is then compared with the four variants,
in which it is found that the lung & respiratory tract bio-markers have the
most, and the sentiment bio-marker has the least effect on improving the
baseline performance of detecting COVID-19 cough samples. The authors
have reported an accuracy of 98.5 % in detecting COVID-19 cough, however,
the data used for building the models is not clinically validated, hence they
12

opensigma.mit.edu

15

intend to work with clinically validated data next.
The winners of the 72h online hackathon ”#CodeVsCOVID19” which was
organised in March-2020, have developed a COVID-19 cough detector using
400 cough samples

13

. They used a re-trainable RandomForest classifier for

the COVID-19 cough detector. Their plans are to move from crowd-sourced
data to clinically validated data.
Although the above mentioned algorithms are attaining good results on
their respective test datasets, it is essential to validate these systems by using them in real time. From the hygiene perspective, it is not advised to
cough on open surface. As explained in [34], the infection is primarily transmitted through large droplets generated during coughing and sneezing by
symptomatic and also by asymptomatic individuals before an onset of symptoms. These infected droplets can spread 1-2 m, deposit on surfaces, and
can remain viable for days in favourable atmospheric conditions, but can be
destroyed in a minute by common disinfectants. Hence, for the collection
procedures, it is important to remind the participants to cover the mouth
and then only provide the cough sound samples. Otherwise, this may result in further spreading of the disease. Also, after giving the samples, the
smartphone surface should be applied with a disinfectant.
Some initiatives have focused on identifying the symptoms instead of
working on the COVID-19 positive users data. A web based application
(available only in the US)14 , trained a running nose cough detector, which is
one of the COVID-19 symptoms, using 2 352 cough samples from 193 sub13
14

https://detectnow.org
https://www.coughmode.com

16

jects. They found that the data was easily separable, however, the data collected for other symptoms such as difficulty in breathing and sore throat was
not forming distinguishable clusters. A system proposed in [35], comprises
of a ’client’ mobile and a ’supervisor’ desktop app. A FFT-based analysis is
used to classify the cough carrying COVID-19 symptoms such as dry and wet
cough with an accuracy of around 90 %. A headset based system is proposed
in [36], to capture noise robust audio signals for detecting cough sounds and
tracking other COVID-19 symptoms such as the respiration rate. The data
collected from Google’s audioset and ESC-50 is used by the authors of [37],
in which they manually labelled the samples under dry-cough category as
COVID-19 cough and attempted to classify these samples from others. With
MFCCs as features and CNN architecture, the authors report an accuracy
of 70.58 % in classifying dry cough from others.
2.2. Speech Analysis
Considering the magnifying effects of coughing sound on spreading the
infection in the absence of any preventive measures, capturing and analysing
speech signals is a dependable alternative. A web interface to capture speech
along with cough of COVID-19 patients is developed by Voca15 . The data collected has been analysed by the author of [38], where the data comprises of 30
positively diagnosed and 1 811 healthy participants’ speech and cough samples. While speaking, the candidates are asked to speak specifically ’Ahhh’,
’Ehhh’, ’Ohhh’, 1-20 counting, a–z alphabet, and reading a segment from
a story. In classifying sick individuals from the healthy ones, the author
15

https://voca.ai/corona-virus

17

Figure 3: Acoustic features’ usage for detecting COVID-19

reached a maximum of 70 % accuracy using MFCC features and a CNN architecture.
As seen in Figure 3, MFCCs are used in almost 50 % of the total efforts.
However, a study done in [39] with MFCCs extracted from the cough, deep
breath and speech signals from 7 COVID-19 patients and 7 healthy individuals shows that MFCCs from speech are not dependable features for classifying
COVID-19 and healthy individuals. Hence, it is required to understand the
exact speech-based features to differentiate COVID-19 patients from healthy
individuals. In the work driven by Carnegie Mellon University (CMU), the
features from models of voice production are explored to understand the
presence of COVID-19 symptoms from the speech signals. These features,
18

that the authors used for this analysis, are described in [40], which is based
on the ADLES (Adjoint Least-Squares) method algorithm, which extracts
the features representing the oscillatory nature of the vocal fold while pronouncing phonemes. The voice production model that is referred to here is
called the “asymmetric body-cover” model that estimates parameters such
as the glottal pressure, mass, spring, and damping from the left and right
vocal folds motion speed and acceleration. CMU’s data collection happened
through the web interface along with certain collaborations with a clinical
setting. Through these collaborations, they could collect 530 subjects’ data,
all of which went through a COVID-19 test to validate; also, their recordings
were manually vetted. The data comprises of signals from 299 positive and
231 negative tested subjects. Each subject provided 6 recordings, which includes alphabets, counting 1-20, extended vowel sounds, and coughs. From
their observations, they have concluded that the results with extended vowels
are better than that with cough signals. Authors of [41] have explored the
significant features for the detection of COVID-19 with the focus on voice
production process. They have analysed the differential dynamics of the glottal flow waveform (GFW) during voice production with the recorded speech,
as it is not possible to analyse the GFW of COVID-19 patients. They hypothesize that a greater similarity between the two indicates normal voice
and the larger difference would mean the presence of anomalies. A CNN
based 2-step attention model is used to detect these anomalies from the 19
subjects’ extended vowels /a/, /i/, /u/ recordings, in which 9 recordings belong to COVID-19 positive subjects. The authors report the residual and
the phase difference between the two GFWs as the most promising features

19

yielding the best classification AUC of 0.9 on the extended vowels /i/+/u/.
Another web based interface for detecting COVID-19 symptoms from the
voice is the ”Spira Project” 16 . This interface asks the participants to record
three phrases. In describing their initial results using MFCCs and a CNN
architecture, the authors reported an accuracy of 91 % in detecting COVID19 symptoms related to respiratory disorders.
The Afeka college of engineering has developed a mobile application for
remote pre-diagnostic assesment of COVID-19 symptoms from the voice and
speech signals captured from infected and healthy individuals. Their training
dataset comprises of 70 speakers and 235 events in the training, and 18
speakers and 57 events in the test set

17

. They achieved precision of 0.79 on

the test set.
Speech recordings of TV interviews of COVID-19 positive speakers available on YouTube are collected and analysed in [42] for classifying COVID-19
patients from healthy individuals. The data-set is publicly available 18 which
comprises of 19 speakers with 10 of them tested COVID-19 positive. The data
collected is manually segmented, after which MFB features are calculated for
the speech segments. Using the ASpIRE chain model, which is a time delayed
neural network trained on the Fisher English dataset described in [43], the
authors of [42] have extracted the posterior probability of phonemes for each
frame. When concatenated, this gives a feature vector for each speech utterance. Using a SVM classifier, the authors report an accuracy of 88.6 % and
16

https://spira.ime.usp.br/coleta
https://health-sounds.cl.cam.ac.uk/workshop20/Alon Barnea Vered Aharonson.mp4
18
https://github.com/shareefbabu/covid data telephone band
17

20

F1-score of 92.7 % in classifying COVID-19 patients from healthy speakers.
The authors of [44] discuss the interpretability of their framework of
COVID-19 diagnosis using embeddings for the cough features and symptoms’
metadata. In this study, cough, breathing, and speech of counting from 1-10
is collected from 150 subjects, in which 100 subjects were COVID-19 positive,
and 50 were tested negative during their RT-PCR test. Apart from this, the
authors also collected data for bronchitis and asthma cough from online and
offline sources. The authors report an improvement of 5-6 % in the accuracy,
F1-score, sensitivity, specificity, and precision when using both the symptoms’ metadata and cough features for the classification tasks. Similarly,
the authors of [45] have used additional information such as: airflow from
spirometer, body temperature from thermal camera, heart rate from ECG,
electrical activity that causes muscle contraction around the heart and chest
region, along with cough detection from a microphone to build a sensor-based
system for identifying COVID-19 subjects.
2.3. Breathing Analysis
As discussed before, shortness of breath is also one of the symptoms of the
virus for which smartphone apps are designed to capture breathing patterns
by recording the speech signal. As described in Section 2.1 and 2.2, multiple
attempts are made to analyse respiration along with cough and speech signals. Especially, the authors of [10] have reported that the breathing signals
are better suited for classifying COVID-19 positive users from healthy users
having asthma and a cough. In an another study of [51] with a small dataset
of 60 healthy and 20 COVID-19 positive subjects, the authors report better
accuracy with RNN based analysis using both breathing and cough data than
21

Table 4: Past speech and audio analysis related to COVID-19 health problems: Speech
breathing analysis and stress detection

Ref.

Features

[46]

Cepstrogram and SVM with
radial basis function
89 % F1 score and RMSE of
breathing rate.
Spectrogram with a CNN
and a RNN

Result
[47]

Result

[48]

Result

[49]

Result

Dataset
Speech recordings of 16 participants of age group 21 years mean
4.5 breaths/min for the speech-

20 healthy subjects’ speech
recorded
using
microphone
and breathing signal using two
respiratory transducer belts
91.2 % sensitivity for breath event detection and mean absolute error of 1.01 breaths per minute for breathing rate estimation (Breathing signal detection for conversational speech)
Wavelet de-noising and Em- 255 breath cycles captured using
pirical Mode Decomposition smartphone microphone
for data pre-processing. Instantaneous frequency and
envelop features, and SVM
classifier.
Accuracy of (75.21 % ±2 %) for asthmatic inspiratory cycles
and (75.5 % ±3 % for complete respiratory sounds cycle with
a diagnostic odds ratio of 20.61 % and 13.87 % respectively.
Low Level Descriptors and 31 emergency call recordings of
functional features ex- the Integrated Rescue System of
tracted using openSMILE the 112 emergency line from the
[50] with k-NN, SVM, and Czech Republic
CNN classifiers
Accuracy of 87.9 % with SVM and 87.5 % with CNN in classifying stress from neutral speech

that of speech. The feature set used by the authors include spectral centroid,
spectral roll-off, zero crossing rate, MFCCs and their derivatives. However,
as compared to cough classification, analysing breathing signals are less pop-

22

ular owing to the challenges in capturing noiseless breathing signals. There
have also been efforts taken in correlating speech signals with the breathing
signals. In the Breathing Sub-challenge of Interspeech 2020 ComParE [52],
the authors have achieved a baseline pearson’s correlation of 0.507 on a development, and 0.731 on the test dataset, respectively. They have used two
piezoelectric respiratory belts for capturing breathing patterns.
In an another effort of correlating speech signals with breathing signals,
an ensemble system with fusion at both feature and decision level of two
approaches is presented in [53]. One of the two approaches is a 1D-CNN
based end-to-end model having two LSTM layers stacked above it. The other
approach uses a pre-trained 2D-CNN ResNet18 with two Gated Recurrent
Unit (GRU) layers stacked above it. The fusion that happens at feature level
combines the embeddings at the last layers of the 1D-CNN and the ResNet18
to train a two-layer LSTM network. The decision level fusion combines the
predictions from the 1D-CNN, ResNet18, and the feature level fusion-based
approaches. The authors have reported a Pearson’s correlation coefficient (rvalue) of 0.763 between the speech signal and corresponding breathing values
of the test set.
Further, the authors of [54], modified the end-to-end baseline architecture, by replacing the LSTMs by Bi-LSTM. The authors also augmented
the challenge dataset, with the same dataset being modified to emulate the
Voice over Internet (VoIP) conditions. With the above modifications, they
achieved an r-value of 0.728 on the test dataset.
To explore attention mechanisms, the authors of [55] have used an end-toend approach along with a Convolutional RNN (CRNN) for two prediction

23

tasks: the breathing signals captured using a respiratory belt, and the inhalation events. The authors report a maximum of 0.731 r-value in predicting the
breathing pattern from the speech signal and the macro averaged F1 value
of 75.47 % in predicting the inhalation events. The attention step is found to
improve the metrics by 0.003 r-value, and .726 % F1 value for the two tasks,
respectively.
Outside of the challenge, the authors of [56] have attempted to correlate
high quality speech signals captured using an Earthworks microphone M23
at 48 kHz with the breathing signal captured using two NeXus respiratory
inductance plethysmography belts over the ribcage and abdomen to measure
the changes in the cross-sectional area of the ribcage and abdomen at a
sample rate of 2 kHz. The authors have achieved a correlation of 0.42 to the
actual breathing signal and a breathing error rate of 5.6 % and sensitivity of
0.88 for breath event detection.
The feasibility of capturing and analysing breathing sounds using smartphone microphones to even understand COVID-19 symptoms as a part of a
personal analytics is discussed in [63]. A preliminary analysis of the sound
signals of respiration from 9 COVID-19 patients and 4 healthy volunteers is
executed in [64] using FFT harmonics. The respiration sounds are recorded
using a smartphone microphone. The rule based analysis shows distinction
in the spectrum of healthy and infected individuals, however, more data
from more subjects are required to strengthen the analysis. The Cambridge
University app that captures breathing sounds along with cough and speech
features a COVID-19 breath analyser as well. Another such app detecting
anomalies from the breathing sound has been developed by the TCS Re-

24

Table 5: Past speech and audio analysis related to COVID-19 health problems: Obstructive Sleep Apnea Detection (OSA)

Ref.

Features

[57]

Breathing detection: MFCCs with a 90 Male subjects’ speech
single layer NN, OSA classification: and sleep quality meaMFCCs, energy, pitch, kurtosis, and sures using WatchPAT
ZCR with SVM.
[58]
Cohen’s kappa coefficient of 0.5 for breathing detection and
0.54 for OSA detection.
Formant frequencies and bandwidth Speech of 25 OSA sub(F1, F2, and F3), HNR, jitter, spec- jects and 20 controls
tral flux, F0, MFCCs, and Linear
prediction cepstral coefficients with
an ensemble of SVM, Linear Discriminant Analysis, and k-NN
True-Positive-Rate of 88 %, and a True-Negative-Rate of 80 %
verified on an in-the-wild data-set acquired from YouTube.
i-vector techniques with Support OSA suspected 426 male
Vector Regression
Spanish speakers speech
Poor results due to limiting factors such as high correlation of
the speech signal with parameters such as age, sex, and others
and over-fitting of models with lengthy feature-sets.
Review paper
84 papers reviewed on
OSA from 2003 to 2017
ECG-based analysis supersedes sound-based analysis for OSA
detection, both for data availability and accuracy perspective.
This can be attributed to the availability of the cleaner ECG
signal as compared to noisy respiratory sounds and speech
signals. Most prominently used ML techniques include SVM,
k-NN, and Neural Network.
Authors have reviewed the Machine 23 papers from 2015 to
Learning (ML) techniques in locat- 2019
ing the excitation of snore sound
Audio features along with traditional ML techniques and state
of the art deep learning techniques are reviewed. Due to absence of fundamental knowledge regarding the acoustic properties of snoring sounds and publicly available datasets, the
research in this direction has not progressed much.
25

Result
[59]

Result
[60]
Result

[61]
Result

[62]

Result

Dataset

search team [65]. A MFCC based breath detector detects the breath sound.
Further, MFCC along with Power Spectral Density (PSD) and spectrograms
are used in an anomaly detection engine which finds an anomaly present in
the breath sound. The anomaly detection engine is built using SVM. The
dataset used in this project comprises of data donated by TCS internals along
with that from the RALE repository [66] of lung sounds. The authors have
demonstrated that their app can detect COVID-19 anomalies as well.
2.4. Chat-Bots
As the count of positive COVID-19 cases are increasing every day, it
brings up the need of automating the conversation a physiologist would have
to understand the presence of symptoms in an individual. Microsoft and
CDC have come up with a chatbot named “Clara” for initial screening of
COVID-19 patients by asking them questions and capturing the responses.
This uses speech recognition and speech synthesis technologies. The riskassessment test is designed based on advice from the WHO and the Union
Ministry of Health and Family Welfare India19 . “Siri” from Apple is also
updated to answer the variations of the general question, “Siri, do I have the
coronavirus?” based on WHO guidelines. If a person shows severe symptoms, then it is advised by Siri to call 911. Similarly, Amazon’s Alexa is
also updated with answering COVID-19 screening questions based on CDC
guidelines. Considering the trauma that the health care providers are going
through, a web-based chat-bot named Symptoma developed in [67], is a significant step. It can differentiate 20 000 diseases with an accuracy of more
19

https://www.mohfw.gov.in

26

Figure 4: Chat-bots in COVID-19 context

than 90 % and can identify COVID-19 from other diseases having similar
symptoms with an accuracy of 96.32 %. It uses semantic analysis on the free
text data entered by the user.
The robot based system for COVID-19 risk analysis mentioned in [68]
uses speech and audio analysis techniques such as speech recognition, keyword detection, and cough detection and classification in real-time. The
authors have used a CNN architecture for detection of cough events from
robot-human conversations and a classifier based on attentional similarity
for detecting a COVID-19 infection. They have 1 283 speech recording segments of the interaction of 184 respondents aged 6 to 80 years, which includes 392 segments from 64 lab confirmed COVID-19 patients. The rest
of the data includes segments from respondents with long smoking history,
acute bronchitis, chronic pharyngitis, and children with pertussis along with
healthy candidates. 21 COVID-19 candidates are having other chronic diseases such as hypertension, diabetes, and heart related diseases. Before the
conversation starts between the robot and the candidate, other measures
such as temperature and demographic information are collected, stored and

27

analysed. During the conversation, the robot monitors the data for cough
signals, if not received, it asks the candidate to cough forcefully towards the
end of the conversation. At the end, a report is generated and shared with
the candidate and authorised doctors.
2.5. Tele-Health
The increasing count of COVID-19 infectants among healthcare providers
suggest the need of telehealth systems where the required care, guidance, and
consultation can be provided remotely. The authors of [69] have enlisted the
telemedicine providers, regulations, and have discussed the telehealth approach, its benefits, challenges, and the parameters that the clinicians need
to look for to confirm the COVID-19 indicators. The major barriers is being
the ignorance towards this mode and its usage, trustworthiness of traditional
face-to-face communication and preference of interacting with known healthcare providers. The audio-visual assessment that the healthcare providers
need to perform includes checking the temperature, ill appearance, calculation of the respiratory rate, presence or absence of cough, and other clinical
symptoms. These enlisted parameters can be the focus of the Artificial Intelligence community to bring automation into this space. A bot named
“Aapka Chikitsak” [70] was designed using Natural Language Processing
and voice-over techniques to cater the needs of the rural Indian population
in this time of COVID-19 crisis. It is designed and developed with an intent
to provide generic healthcare information, preventive measures, home remedies, and consultation for India-specific context with multi-lingual support
to provide healthcare and wellbeing at no extra cost. In addition to the
physical diagnostic benefits, [71] talks about the mitigation of psychological
28

problems where psychotherapy sessions can be conducted using video calls.
The need for such a telehealth system is highlighted in [72], as well as stating the primary advantages such as care for healthcare providers, avoiding
overcrowding, and elderly care. However, this also raises the need for proper
infrastructure and sufficient bandwidth to enable data transmission (audio,
video, and images).
While the readiness of available voice assistants such as Alexa and Siri is
analysed in [73], it is found that they need to be context aware and should be
updated with the latest, reliable, and relevant content to be used as a part
of a telehealth system for COVID-19. Also, the use of voice as a digital bio
marker is missing.
2.6. Chest X-Ray
Outside of audio, speech, and language processing, imaging is a dominant
signal processing and AI-based diagnosis method. The study performed in
[74] on 1 014 cases found that the diagnosis using chest CT has a sensitivity
of 97 % in the diagnosis of COVID-19. Multiple efforts by several groups are
put in the direction of developing a classifier to detect COVID-19 symptoms
using chest X-Ray, such as those in [75], [76], [77], [78], [79], and many
more. However, as stated in [80], a multinational consensus is that imaging
is indicated only for patients with worsening respiratory status. Hence, it
is advised for only those patients who have moderate-severe clinical features
and a high pre-test probability of disease. Hence, unlike in the Sections 2.1,
2.3, and 2.4, such measures are not suggested for early identification, and are
preferred for diagnosis in a clinical setup.
As concluded by [81], the chest X-ray findings in COVID-19 patients were
29

found to be peaked at 10-12 days from symptom onset. Also, it is still required to visit a well-equipped clinical facility for such approaches. On a
positive note, in the presence of mobile X-Ray machines, this approach can
help in speeding up the diagnosis. The authors of [82] have found from an experimental outcome that the chest X-Ray may be useful as a standard method
for the rapid diagnosis of COVID-19 to optimise the management of patients.
However, CT is still limited for identifying specific viruses and distinguishing
between viruses. The smartphone application framework mentioned in [83]
uses multiple sensor data such as that from a camera, microphone, fingerprint
sensor, and inertial sensor for feeding CT scan images, human video tracking,
as well as capturing cough sounds, temperature, and 30 seconds sit-to-stand
movements to the lower layers for processing. With this, the authors plan to
detect COVID-19 symptoms such as abnormalities in the CT-Scan images,
nausea, fatigue, cough, and fever levels. They further use machine learning
techniques such as a CNN and a RNN to derive the COVID-19 result based
on the symptoms identified.
3. Monitoring
In an endeavour to detect the symptoms in each individual before a case
gets serious, the government authorities in multiple countries had started examining and asking every individual if they have any COVID-19 symptoms.
Such initiatives need a lot of human efforts to be invested. An alternative
automation to accomplish such surveys can be using a system as described
in [84]. Using speech recognition, synthesis, and natural language understanding techniques, the CareCall system monitors individuals of Korea and
30

Japan who had a contact with COVID-19 patients. This monitoring is done
over a phone call using with and without human-in-the-loop process for three
months. The system is used with over 13 904 calls in which the authors reported 0 % false negative (self-reported COVID-19 subjects are not identified
by the CareCall system) and 0.92 % false positive rate.
On the other hand, at an individual level, the precautions for stopping
the virus spread include social distancing, wearing a mask, and maintaining
respiratory hygiene. Especially at public places, the concerned authorities
can monitor the population to confirm that the precautionary measures are
adopted by them. This section describes such monitoring tools developed for
the COVID-19 scenario.
3.1. Face Mask Detection from Voice
As described in Section 2.1, collecting cough samples without covering
mouth can lead to further spreading of the disease; mask wearing has to be
mandated for donating a cough sample which requires an app to detect if the
donor is wearing a mask or not. This year’s Interspeech 2020 Computational
Paralinguistics challengE (ComParE) [52] featured a mask detection subchallenge, where the task is to recognise whether the speaker was recorded
while wearing a facial mask or not. The results from this challenge and the
participants will be useful to develop a voice monitoring tool which detects
the mask or no-mask of a speaker. Also, these algorithms serve as a pre-step
for other speech processing algorithms such as breathing recognition, cough
detection, or speech-based COVID-19 screening. One associated work in
this direction is discussed in [85], in which the authors have used techniques
such as SpecAugment and mixup to generalise the deep models and have
31

crossed the baseline by more than 4 % Unweighted Average Recall (UAR).
The winners of this sub-challenge [86] have used a deep convolutional neural
network-based image classifier on the linear-scale 3-channel spectrograms of
the speech segments to identify the mask wearing speakers. The authors
have achieved a UAR of 80.1 % which is 8.3 % higher than the baseline using
an ensemble of VGGNet, ResNet, and DenseNet architectures.
3.2. Face Mask Detection from Image
One of the precaution measures while stepping outside home suggested
by the WHO to reduce the chance of getting infected or spreading COVID-19
is to wear a facial mask covering nose and mouth. Also, the mathematical
analysis presented in [87] suggests that wearing a mask can reduce community
transmission and can help reduction in the peak death rate. Hence, similar to
what has been outlined above, it is important that the concerned authorities
keep a check on individuals wearing masks or not at public places, especially
where the population is quite dense such as at airports, railway stations, and
hospitals.
The authors of [88] have provided the Masked Face Detection Dataset
(MFDD), Real-world Masked Face Recognition Dataset (RMFRD), and the
Simulated Masked Face Recognition Dataset (SMFRD) for the detection of
masked and unmasked faces using image processing techniques. They have
achieved 95 % accuracy in recognising masked faces. An Apple store app
developed by LeewayHertz can be integrated with the existing setup of an
Internet Protocol (IP) camera for getting alerts on detecting no-mask on a
face. The system provides a facility to add phone numbers for receiving the
alerts and also mechanism to see the face not wearing a mask for the admin.
32

3.3. Cough and Breathing Analysis
While at quarantine, the doctors need to monitor the cough history of
patients, which can be done with a continuous cough monitoring device. After we cross the crisis and organisations think of resuming the operations,
continuous monitoring of common spaces such as canteens and lobbies can
be realised to record COVID-19 specific coughs. One such monitoring application is developed by the FluSense platform in [89]. It is a surveillance
tool to detect influenza like illness from hospital waiting areas using cough
sounds. Continuous monitoring and identification of abnormalities from the
breathing rate has been done by [90] using image processing. A real-time
application of cough detection is also applied using camera devices, which
track and record the person who coughed, their location, and the number of
coughs using a deep learning model for cough sound classification

20

. The

authors achieved a test accuracy of 87.4 % in identifying cough sounds in
an office environment. However, they have not tried identifying COVID-19
cough sounds.
3.4. Mental Health – Emotion Detection
The disease spread has equally affected the physical and mental health of
the individuals, which is primarily due to plenty of mandatory precautionary measures such as social-distancing, work from home and the quarantine
procedures which usually takes around 15 days for the patients to be alone.
Also, the health care providers owing to their hectic routines followed by
20

https://healthcare-in-europe.com/en/news/covid-19-cough-camera-device-detectslocation-of-coughing-sounds-in-real-time.html

33

quarantine days are subject to undergo mental health issues. To cater for
the growing need of addressing this issue, not only is there a high demand,
but the physical presence of the available psychologists is also missing. As
found in [91], the COVID-19 pandemic has generated unprecedented demand
for tele-health based clinical services.
Among several initiatives taken against mental health issues such as
stress, anxiety, and depression, we are yet to see these emotions being analysed from the speech signal during the COVID-19 period. This demands
for the relevant data. Very recently, a study is conducted by the authors of
[92] on the speech signal of COVID-19 diagnosed patients. The behavioural
parameters detected from speech includes, sleep quality, fatigue, and anxiety
considering corresponding self-reported measures as ground truth and have
achieved an average accuracy of 0.69 in estimating the severity of COVID-19.
The authors have collected 5 speech recordings from each of the 52 patients
while speaking neutral statements along with 3 responses to the questions
regarding their sleep quality, fatigue and anxiety. The metadata of these patients consist of age, gender, height, and weight. The authors report UAR of
0.61 %, 0.46 % and 0.56 % for the three tasks sleep quality, fatigue and anxiety
respectively. They have extracted eGeMAPS feature set using OpenSMILE
toolkit [50] and used SVM classifier .
This year’s ComParE challenge at Interspeech 2020 [52] had an Elderly
Emotion Detection sub-challenge, where the speech captured from elderly
subjects had to be classified into low, medium, and high valence and arousal.
It was found that specific age groups such as that of infants [93] and elderly
above 60 years are more prone to the infection, due to which this age group

34

presumably largely needs to follow the restrictions posed by the pandemic
for a larger period in the future as well. This shows that it will be crucial to
understand the effects of this phase on their psychological parameters such
as emotions. The winner of this challenge, [94] used acoustic features for
arousal classification and linguistic features for valence classification. The
authors used fisher vector encoding of the 76 dimensional acoustic features
comprising of 24 MFCCs and the rest of the Perceptual Linear Prediction
cepstrum (RASTA-PLP) for 12th order linear prediction, together with their
temporal delta coefficients. For arousal detection, the ensemble of these
features combined with the baseline features gave UAR of 57.5 %, which is
7.1 % higher than the baseline. For valence detection, the authors used an
ensemble of TF-IDF features, FastText+Polarity features and Dictionarybased features to achieve the UAR of 62.3 %.
3.5. Text Analysis
Text data from several individuals on social media, hospitality feedback
from the patients on the treatment given to them, and forums such as blogs
provide useful information about different COVID-19 related aspects. The
mental health support communities on Reddit are analysed in [95], to understand the effects of COVID-19 on the mental health of society. It is observed
that the posts on this community forum per day have increased specifically in
the subreddits of Anxiety and SuicideWatch, however, decrease in the subreddit of depression during the pandemic period. Similarly, in [96], the twitter
data with 5GCoronavirus hashtag is analysed to understand the drivers of
the 5G COVID-19 conspiracy theory and strategies to help in reducing the
spread of mis-information circulating in society.
35

The authors of [97] have used machine learning techniques to categorise
sentiments from the responses to Press Ganey patient experience surveys.
From this analysis, it is found that the patients have expressed negative comments about the cleanliness and logistics and have given positive comments
about their interactions with clinicians.
A text-mining based study on the impact of COVID-19 on the individuals
of Paris and France from April 23 to June 18, 2020 is studied by the authors
of [98]. The authors used twitter data, having 1,496,375 tweets from 285,114
users, specific to the given dates, location, French language and COVID-19.
They found a decreasing pattern of publications/interest, and an increased
impact on the health crisis and economy generated by coronavirus. Another
study done by the authors of [99], on twitter data to understand the impact
of COVID-19 on loneliness among individuals. The authors found that the
highly influential users were talking more about the mental health effects of
loneliness during COVID-19. To accelerate the text-mining based research on
the COVID-19 related publications, [100] CORD-19 is the dataset formed of
research paper from PubMed Central, bioRxiv and medRxiv preprint servers,
WHO Covid-19 Database, Elsevier and Springer Nature under special Covid19 open access licenses. 92 % of the papers are from Biology, Medicine, and
Chemistry domains.
4. Awareness
Speech recognition and synthesis algorithms have been widely used in the
development of chat-bots to provide human like interactions. In this time of
crisis, chat-bots are helping in spreading valuable information about COVID36

19 to end users. Once an infected gets cured, they can help researchers with
not only their experience but also with donating components such as plasma.
CDC has been encouraging recovered individuals to donate their plasma for
development of blood related therapies. For collection of plasma, Microsoft
has developed the chat-bot

21

which interacts with individuals to gather the

required information such as, the duration for which they are tested negative,
their age, weight, and also takes their pin-code to help them know their
nearest donation center.
5. Next Steps and Challenges
With the smartphone likely being the most convenient and available asset
that every individual carries all the time, more of smartphone-based applications for detecting COVID-19 symptoms will help in controlling the virus
spread. The authors of [101] have addressed the memory and power consumption issues for importing a deep learning model for detection of cold from the
speech signal. They propose network pruning and quantisation techniques
to reduce the model size, with which they achieved a size reduction of 95 %
in MBytes without affecting the recognition performance.
Along with the physical COVID-19 symptoms, the behavioural aspects
also need greater attention. Since the work culture is moving more towards
working-from-home, it will be required to detect some behavioural parameters such as fatigue and sleepiness for the self monitoring of the working
professionals. The authors of [102] had organised a challenge at Interspeech
2019, for detecting the sleepiness among individuals where a labelled dataset
21

https://covig-19plasmaalliance.org

37

of 950 subjects with Karolinska Sleepiness Scale score is provided. The winners of this challenge [103] had reported the spearman’s correlation coefficient
of 0.383 on the test dataset using COMPARE features, Bag of Audio Words
(BoAW), and Fisher vectors. The same dataset is used in [104], in which
the feature representation done by attention and autoencoder techniques are
fused to train a support vector regressor. The authors report Spearman’s
correlation coefficients of 0.367 on the test dataset.
The behavioural parameters such as stress, anxiety and depression needs
greater attention to be paid in these days. An audio-visual emotion detection challenge organised by [105] provides a speech dataset named, ”Distress
Analysis Interview Corpus – Wizard of Oz (DAICWOZ)” which is a part
of a larger corpus, the ”Distress Analysis Interview Corpus” (DAIC) [106],
that contains clinical interviews designed to support the diagnosis of psychological distress conditions such as anxiety, depression, and post-traumatic
stress disorder. In DAICWOZ, the participants rate themselves on the eightitem Patient Health Questionnaire depression scale (PHQ-8). The authors
of [107], have worked with the same dataset using Hierarchical Attention
Transfer Network and report an RMSE of 5.66 and an Mean Absolute Error
of 4.28 on the test dataset.
A major challenge given the social distancing norms is getting the relevant
and accurate speech data for developing machine learning models. Speech
enabled chat-bots can play a significant role in this. There are other challenges as well from the design perspective of chat-bots. The authors of [108]
have expressed both positive effects and drawbacks associated with using
chatbots for sharing the latest information, encouraging desired health im-

38

pacting behaviours, and reducing the psychological damage caused by fear
and isolation. This shows that the design of chat-bots should be well thought
of for using them, otherwise, they might have negative impact as well. An
optimistic approach in these difficult times has been to work towards safe
and secure environment for the post pandemic situation so that the society
gains the trust and confidence back. This shows the need of accurate and
reliable screening and monitoring measures at public places.
6. Conclusion
Speech and human audio analysis is found to be predominantly useful for
COVID-19 analysis. Several initiatives towards identifying cough sound and
distinguishing COVID-19 cough from other illnesses are currently taken. It
looks promising that soon a cough sound-based sufficiently accurate COVID19 detector for several real-world use-cases will be available. Such detectors,
when integrated with chat-bots can enhance the screening, diagnosing, and
monitoring efforts with reduction in human interventions. Further research is
required for breathing and speech signal-based COVID-19 analysis, where it
is more important to identify the exact bio-markers. With increasing correlation established between speech and breathing signals, detecting breathing
disorders from the speech signals will be useful. In many countries, a second, and even a third wave of COVID-19 infection has been found to occur
infecting many more individuals. This suggests for the urgent need of robust
monitoring mechanisms. Many elderly individuals have been inside home
for almost the entire year. The past research on the detection of OSA and
stress needs to be taken forward in the COVID-19 context for the elderly
39

population. Besides, promising applications for the usage of language processing and other signal analyses have been shown. In sum, we are positive
that the combination of intelligent audio, speech, language, and other signal
analysis can help make an important contribution in the fight against the
COVID-19 and oncoming similar pandemics – alone, or in combination with
other methods.
7. Acknowledgements
We would like to thank all researchers, health supporters, and others
helping in this crisis. Our hearts are with those affected and their families
and friends. We acknowledge funding from the German BMWi by ZIM grant
No. 16KN069402 (KIrun).
References
[1] B. W. Schuller, D. M. Schuller, K. Qian, J. Liu, H. Zheng, X. Li,
Covid-19 and computer audition: An overview on what speech & sound
analysis could contribute in the sars-cov-2 corona crisis, arXiv preprint
arXiv:2003.11117 (2020).
[2] G. Deshpande, B. Schuller, An overview on audio, signal, speech, & language processing for covid-19, arXiv preprint arXiv:2005.08579 (2020).
[3] L. Di Perna, G. Spina, S. Thackray-Nocera, M. G. Crooks, A. H.
Morice, P. Soda, A. C. den Brinker, An automated and unobtrusive
system for cough detection, in: 2017 IEEE Life Sciences Conference
(LSC), IEEE, 2017, pp. 190–193.
40

[4] M. You, H. Wang, Z. Liu, C. Chen, J. Liu, X.-H. Xu, Z.-M. Qiu, Novel
feature extraction method for cough detection using nmf, IET Signal
Processing 11 (5) (2017) 515–520.
[5] I. D. Miranda, A. H. Diacon, T. R. Niesler, A comparative study
of features for acoustic cough detection using deep architectures, in:
2019 41st Annual International Conference of the IEEE Engineering in
Medicine and Biology Society (EMBC), IEEE, 2019, pp. 2601–2605.
[6] E. van Miltenburg, B. Timmermans, L. Aroyo, The vu sound corpus:
Adding more fine-grained annotations to the freesound database, in:
Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), 2016, pp. 2124–2130.
[7] J. Monge-Álvarez, C. Hoyos-Barceló, P. Lesso, P. Casaseca-de-la
Higuera, Robust detection of audio-cough events using local hu moments, IEEE journal of biomedical and health informatics 23 (1) (2018)
184–196.
[8] L. Kvapilova, V. Boza, P. Dubec, M. Majernik, J. Bogar, J. Jamison, J. C. Goldsack, D. J. Kimmel, D. R. Karlin, Continuous sound
collection using smartphones and machine learning to measure cough,
Digital biomarkers 3 (3) (2019) 166–175.
[9] K. E. Hanson, A. M. Caliendo, C. A. Arias, J. A. Englund, M. J.
Lee, M. Loeb, R. Patel, A. El Alayli, M. A. Kalot, Y. Falck-Ytter,
V. Lavergne, R. L. Morgan, M. H. Murad, S. Sultan, A. Bhimraj,

41

R. A. Mustafaand, Infectious diseases society of america guidelines on
the diagnosis of covid-19, Clinical Infectious Diseases (2020).
[10] C. Brown, J. Chauhan, A. Grammenos, J. Han, A. Hasthanasombat,
D. Spathis, T. Xia, P. Cicuta, C. Mascolo, Exploring automatic diagnosis of covid-19 from crowdsourced respiratory sound data, arXiv
preprint arXiv:2006.05919 (2020).
[11] S. Vhaduri, Nocturnal cough and snore detection using smartphones
in presence of multiple background-noises, in: Proceedings of the 3rd
ACM SIGCAS Conference on Computing and Sustainable Societies,
2020, pp. 174–186.
[12] J. Amoh, K. Odame, Deep neural networks for identifying cough
sounds, IEEE transactions on biomedical circuits and systems 10 (5)
(2016) 1003–1011.
[13] F. Barata, K. Kipfer, M. Weber, P. Tinschert, E. Fleisch, T. Kowatsch,
Towards device-agnostic mobile cough detection with convolutional
neural networks, in: 2019 IEEE International Conference on Healthcare
Informatics (ICHI), IEEE, 2019, pp. 1–11.
[14] E. C. Larson, T. Lee, S. Liu, M. Rosenfeld, S. N. Patel, Accurate and
privacy preserving cough sensing using a low-cost microphone, in: Proceedings of the 13th international conference on Ubiquitous computing,
2011, pp. 375–384.
[15] R. X. A. Pramono, S. A. Imtiaz, E. Rodriguez-Villegas, Automatic
cough detection in acoustic signal using spectral features, in: 2019 41st
42

Annual International Conference of the IEEE Engineering in Medicine
and Biology Society (EMBC), IEEE, 2019, pp. 7153–7156.
[16] R. X. A. Pramono, S. A. Imtiaz, E. Rodriguez-Villegas, A cough-based
algorithm for automatic diagnosis of pertussis, PloS one 11 (9) (2016)
e0162128.
[17] C. Hoyos-Barceló, J. Monge-Álvarez, Z. Pervez, L. M. San-JoséRevuelta, P. Casaseca-de-la Higuera, Efficient computation of image
moments for robust cough detection using smartphones, Computers in
biology and medicine 100 (2018) 176–185.
[18] L. Orlandic, T. Teijeiro, D. Atienza, The coughvid crowdsourcing
dataset: A corpus for the study of large-scale cough analysis algorithms, arXiv preprint arXiv:2009.11644 (2020).
[19] N. Sharma, P. Krishnan, R. Kumar, S. Ramoji, S. R. Chetupalli, N. R.,
P. K. Ghosh, S. Ganapathy, Coswara - a database of breathing, cough,
and voice sounds for covid-19 diagnosis, Proceedings INTERSPEECH.
Shanghai, China: ISCA (2020).
[20] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence,
R. C. Moore, M. Plakal, M. Ritter, Audio set: An ontology and humanlabeled dataset for audio events, in: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE,
2017, pp. 776–780.
[21] R. Dunne, T. Morris, S. Harper, High accuracy classification of covid19 coughs using mel-frequency cepstral coefficients and a convolutional
43

neural network with a use case for smart home devices, ResearchSquare
(2020).
[22] J. Andreu-Perez, H. Pérez-Espinos, E. Timone, M. I. Girón-Pérez,
M. Kiani, A. B. Benitez-Trinidad, D. Jarchi, A. Rosales-Pérez, Z. Ali,
N. Gatzoulis, O. F. Reyes-Galaviz, A. A. Torres-Garcı́a, C. A. ReyesGarcı́a, F. Rivas, A novel deep learning based recognition method and
web-app for covid-19 infection test from cough sounds with a clinically
validated dataset, University of Essex, School of Computer Science and
Electronic Engineering (2020).
[23] P. Bagad, A. Dalmia, J. Doshi, A. Nagrani, P. Bhamare, A. Mahale,
S. Rane, N. Agarwal, R. Panicker, Cough against covid: Evidence of
covid-19 signature in cough sounds, arXiv preprint arXiv:2009.08790
(2020).
[24] G. Pinkas, Y. Karny, A. Malachi, G. Barkai, G. Bachar, V. Aharonson,
Sars-cov-2 detection from voice, IEEE Open Journal of Engineering in
Medicine and Biology 1 (2020) 268–274.
[25] S. Yadav, M. Keerthana, D. Gope, U. K. Maheswari, P. K. Ghosh,
Analysis of acoustic features for speech sound based classification of
asthmatic and healthy subjects, in: Proceedings IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP),
IEEE, 2020, pp. 6789–6793.
[26] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer,
F. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi, M. Mor44

tillaro, H. Salamin, A. Polychroniou, F. Valente, S. Kim, The interspeech 2013 computational paralinguistics challenge: Social signals,
conflict, emotion, autism, in: Proceedings INTERSPEECH 2013, 14th
Annual Conference of the International Speech Communication Association, Lyon, France, 2013, pp. 148–152.
[27] Sonu, R. K. Sharma, Disease detection using analysis of voice parameters, National Institute of Technology, Kurukshetra (2018).
[28] B.-S. Lin, B.-S. Lin, Automatic wheezing detection using speech recognition technique, Journal of Medical and Biological Engineering 36 (4)
(2016) 545–554.
[29] J. Kutor, S. Balapangu, J. K. Adofo, A. A. Dellor, C. Nyakpo, G. A.
Brown, Speech signal analysis as an alternative to spirometry in asthma
diagnosis: investigating the linear and polynomial correlation coefficient, International Journal of Speech Technology 22 (3) (2019) 611–
620.
[30] V. Nathan, K. Vatanparvar, M. M. Rahman, E. Nemati, J. Kuang,
Assessment of chronic pulmonary disease patients using biomarkers
from natural speech recorded by mobile devices, in: 2019 IEEE 16th
International Conference on Wearable and Implantable Body Sensor
Networks (BSN), IEEE, 2019, pp. 1–4.
[31] A. Imran, I. Posokhova, H. N. Qureshi, U. Masood, S. Riaz, K. Ali,
C. N. John, M. Nabeel, Ai4covid-19: Ai enabled preliminary diag-

45

nosis for covid-19 from cough samples via an app, arXiv preprint
arXiv:2004.01275 (2020).
[32] K. J. Piczak, Esc: Dataset for environmental sound classification, in:
Proceedings of the 23rd ACM international conference on Multimedia,
2015, pp. 1015–1018.
[33] J. Laguarta, F. Hueto, B. Subirana, Covid-19 artificial intelligence diagnosis using only cough recordings, IEEE Open Journal of Engineering
in Medicine and Biology (2020).
[34] T. Singhal, A review of coronavirus disease-2019 (covid-19), The Indian
Journal of Pediatrics (2020) 1–6.
[35] N. Petrellis, A covid-19 multipurpose platform, Digital Biomarkers
4 (3) (2020) 89–98.
[36] R. Stojanović, A. Škraba, B. Lutovac, A headset like wearable device
to track covid-19 symptoms, in: 2020 9th Mediterranean Conference
on Embedded Computing (MECO), IEEE, 2020, pp. 1–4.
[37] V. Bansal, G. Pahwa, N. Kannan, Cough classification for covid-19
based on audio mfcc features using convolutional neural networks, in:
2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON), IEEE, 2020, pp. 604–608.
[38] T. Dubnov, Signal analysis and classification of audio samples from individuals diagnosed with covid-19, Ph.D. thesis, UC San Diego (2020).

46

[39] M. B. Alsabek, I. Shahin, A. Hassan, Studying the similarity of covid19 sounds based on correlation analysis of mfcc, in: 2020 International
Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI), IEEE, 2020, pp. 1–5.
[40] W. Zhao, R. Singh, Speech-based parameter estimation of an asymmetric vocal fold oscillation model and its application in discriminating
vocal fold pathologies, in: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE,
2020, pp. 7344–7348.
[41] S. Deshmukh, M. A. Ismail, R. Singh, Interpreting glottal flow dynamics for detecting covid-19 from voice, arXiv preprint arXiv:2010.16318
(2020).
[42] K. V. S. Ritwik, S. B. Kalluri, D. Vijayasenan, Covid-19 patient detection from telephone quality speech data, arXiv preprint
arXiv:2011.04299 (2020).
[43] T. Ko, V. Peddinti, D. Povey, S. Khudanpur, Audio augmentation for
speech recognition, in: Sixteenth Annual Conference of the International Speech Communication Association, 2015.
[44] A. Pal, M. Sankarasubbu, Pay attention to the cough: Early diagnosis of covid-19 using interpretable symptoms embeddings with cough
sound signal processing, arXiv preprint arXiv:2010.02417 (2020).
[45] A. N. Belkacem, S. Ouhbi, A. Lakas, E. Benkhelifa, C. Chen, Endto-end ai-based point-of-care diagnosis system for classifying res47

piratory illnesses and early detection of covid-19, arXiv preprint
arXiv:2006.15469 (2020).
[46] A. Routray, M. I. Y. Arafath K., Automatic measurement of speech
breathing rate, in: 2019 27th European Signal Processing Conference
(EUSIPCO), IEEE, 2019, pp. 1–5.
[47] V. S. Nallanthighal, H. Strik, Deep sensing of breathing signal during
conversational speech, Radboud Repository of the Radboud University
Nijmegen (2019).
[48] M. A. Azam, A. Shahzadi, A. Khalid, S. M. Anwar, U. Naeem, Smartphone based human breath analysis from respiratory sounds, in: 2018
40th Annual International Conference of the IEEE Engineering in
Medicine and Biology Society (EMBC), IEEE, 2018, pp. 445–448.
[49] P. Partila, J. Tovarek, J. Rozhon, J. Jalowiczor, Human stress detection
from the speech in danger situation, in: Mobile Multimedia/Image
Processing, Security, and Applications 2019, Vol. 10993, International
Society for Optics and Photonics, 2019, p. 109930U.
[50] F. Eyben, M. Wöllmer, B. Schuller, Opensmile: the munich versatile
and fast open-source audio feature extractor, in: Proceedings of the
18th ACM international conference on Multimedia, 2010, pp. 1459–
1462.
[51] A. Hassan, I. Shahin, M. B. Alsabek, Covid-19 detection system using recurrent neural networks, in: 2020 International Conference on

48

Communications, Computing, Cybersecurity, and Informatics (CCCI),
IEEE, 2020, pp. 1–5.
[52] B. W. Schuller, A. Batliner, C. Bergler, E.-M. Messner, A. Hamilton, S. Amiriparian, A. Baird, G. Rizos, M. Schmitt, L. Stappen,
H. Baumeister, A. D. MacIntyre, S. Hantke, The interspeech 2020
computational paralinguistics challenge: Elderly emotion, breathing &
masks, Proceedings INTERSPEECH. Shanghai, China: ISCA (2020).
[53] M. Markitantov, D. Dresvyanskiy, D. Mamontov, H. Kaya, W. Minker,
A. Karpov, Ensembling end-to-end deep models for computational paralinguistics tasks: Compare 2020 mask and breathing sub-challenges,
Proceedings INTERSPEECH. Shanghai, China: ISCA (2020).
[54] J. Mendonça, F. Teixeira, I. Trancoso, A. Abad, Analyzing breath signals for the interspeech 2020 compare challenge, Proceedings INTERSPEECH. Shanghai, China: ISCA (2020) 2077–2081.
[55] A. D. MacIntyre, G. Rizos, A. Batliner, A. Baird, S. Amiriparian,
A. Hamilton, B. W. Schuller, Deep attentive end-to-end continuous
breath sensing from speech, Proceedings INTERSPEECH. Shanghai,
China: ISCA (2020) 2082–2086.
[56] V. S. Nallanthighal, A. Härmä, H. Strik, Speech breathing estimation using deep learning methods, in: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE, 2020, pp. 1140–1144.

49

[57] R. M. Simply, E. Dafna, Y. Zigel, Obstructive sleep apnea (osa) classification using analysis of breathing sounds during speech, in: 2018 26th
European Signal Processing Conference (EUSIPCO), IEEE, 2018, pp.
1132–1136.
[58] Y. J. Gan, L. Lim, Y. K. Chong, Validation study of watchpat 200 for
diagnosis of osa in an asian cohort, European Archives of Oto-RhinoLaryngology 274 (3) (2017) 1741–1745.
[59] M. C. Botelho, I. Trancoso, A. Abad, T. Paiva, Speech as a biomarker
for obstructive sleep apnea detection, in: ICASSP 2019-2019 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE, 2019, pp. 5851–5855.
[60] F. Espinoza-Cuadros, R. Fernández-Pozo, D. T. Toledano, J. D.
Alcázar-Ramı́rez, E. Lopez-Gonzalo, L. A. Hernandez-Gomez, Reviewing the connection between speech and obstructive sleep apnea,
Biomedical engineering online 15 (1) (2016) 20.
[61] F. Mendonca, S. S. Mostafa, A. G. Ravelo-Garcı́a, F. Morgado-Dias,
T. Penzel, A review of obstructive sleep apnea detection approaches,
IEEE journal of biomedical and health informatics 23 (2) (2018) 825–
837.
[62] K. Qian, C. Janott, M. Schmitt, Z. Zhang, C. Heiser, W. Hemmert,
Y. Yamamoto, B. Schuller, Can machine learning assist locating the
excitation of snore sound? a review, IEEE Journal of Biomedical and
Health Informatics (2020).
50

[63] M. Faezipour, A. Abuzneid, Smartphone-based self-testing of covid-19
using breathing sounds, Telemedicine and e-Health (2020).
[64] E. G. Furman, A. Charushin, E. Eirikh, S. Malinin, V. Sheludko,
V. Sokolovsky, G. Furman, The remote analysis of breath sound in
covid-19 patients: A series of clinical cases, medRxiv (2020).
[65] S. Harini, P. Deshpande, B. Rai, Breath sounds as a biomarker for
screening infectious lung diseases, Sciforum, MDPI (2020).
[66] R. Beck, N. Elias, S. Shoval, N. Tov, G. Talmon, S. Godfrey, L. Bentur,
Rale repository of respiratory sounds, BMC Pediatr (2008).
[67] A. Martin, J. Nateqi, S. Gruarin, N. Munsch, I. Abdarahmane,
B. Knapp, An artificial intelligence-based first-line defence against
covid-19: digitally screening citizens for risks via a chatbot, bioRxiv
(2020).
[68] W. Wei, J. Wang, J. Ma, N. Cheng, J. Xiao, A real-time robot-based
auxiliary system for risk evaluation of covid-19 infection, Proceedings
INTERSPEECH. Shanghai, China: ISCA (2020) 701–705.
[69] J. Portnoy, M. Waller, T. Elliott, Telemedicine in the era of covid-19,
The Journal of Allergy and Clinical Immunology: In Practice 8 (5)
(2020) 1489–1491.
[70] U. Bharti, D. Bajaj, H. Batra, S. Lalit, S. Lalit, A. Gangwani, Medbot: Conversational artificial intelligence powered chatbot for delivering tele-health after covid-19, in: 2020 5th International Conference
51

on Communication and Electronics Systems (ICCES), IEEE, 2020, pp.
870–875.
[71] H. Leite, I. R. Hodgkinson, T. Gruber, New development:‘healing at a
distance’ — telemedicine and covid-19, Public Money & Management
(2020) 1–3.
[72] A. Khaleghi, M. R. Mohammadi, G. P. Jahromi, H. Zarafshan, New
ways to manage pandemics: Using technologies in the era of covid-19, a
narrative review, Iranian Journal of Psychiatry 15 (3) (2020) 236–242.
[73] E. Sezgin, Y. Huang, U. Ramtekkar, S. Lin, Readiness for voice assistants to support healthcare delivery during a health crisis and pandemic, npj Digital Medicine 3 (1) (2020) 1–4.
[74] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, Q. Tao, Z. Sun,
L. Xia, Correlation of chest ct and rt-pcr testing in coronavirus disease
2019 (covid-19) in china: a report of 1014 cases, Radiology (2020)
200642.
[75] S. Toraman, T. B. Alakus, I. Turkoglu, Convolutional capsnet: A novel
artificial neural network approach to detect covid-19 disease from x-ray
images using capsule networks, Chaos, Solitons & Fractals 140 (2020)
110122.
[76] P. Afshar, S. Heidarian, F. Naderkhani, A. Oikonomou, K. N. Plataniotis, A. Mohammadi, Covid-caps: A capsule network-based framework
for identification of covid-19 cases from x-ray images, arXiv preprint
arXiv:2004.02696 (2020).
52

[77] I. D. Apostolopoulos, T. A. Mpesiana, Covid-19: automatic detection
from x-ray images utilizing transfer learning with convolutional neural
networks, Physical and Engineering Sciences in Medicine (2020) 1.
[78] M. Barstugan, U. Ozkaya, S. Ozturk, Coronavirus (covid-19) classification using ct images by machine learning methods, arXiv preprint
arXiv:2003.09424 (2020).
[79] M. A. Elaziz, K. M. Hosny, A. Salah, M. M. Darwish, S. Lu, A. T.
Sahlol, New machine learning method for image-based diagnosis of
covid-19, Plos one 15 (6) (2020).
[80] G. D. Rubin, C. J. Ryerson, L. B. Haramati, N. Sverzellati, J. P.
Kanne, S. Raoof, N. W. Schluger, A. Volpi, J.-J. Yim, I. B. K. Martin,
D. J. Anderson, C. Kong, T. Altes, A. Bush, S. R. Desai, O. Goldin,
J. M. Goo, M. Humbert, Y. Inoue, H.-U. Kauczor, F. Luo, P. J. Mazzone, M. Prokop, M. Remy-Jardin, L. Richeldi, C. M. Schaefer-Prokop,
N. Tomiyama, A. U. Wells, A. N. Leung, The role of chest imaging in
patient management during the covid-19 pandemic: a multinational
consensus statement from the fleischner society, Chest (2020).
[81] H. Y. F. Wong, H. Y. S. Lam, A. H.-T. Fong, S. T. Leung, T. W.-Y.
Chin, C. S. Y. Lo, M. M.-S. Lui, J. C. Y. Lee, K. W.-H. Chiu, T. W.H. Chung, E. Y. P. Lee, E. Y. F. Wan, I. F. N. Hung, T. P. W. Lam,
M. D. Kuo, M.-Y. Ng, Frequency and distribution of chest radiographic
findings in covid-19 positive patients, Radiology (2020) 201160.
[82] Y. Li, L. Xia, Coronavirus disease 2019 (covid-19): role of chest ct in di53

agnosis and management, American Journal of Roentgenology 214 (6)
(2020) 1280–1286.
[83] H. S. Maghdid, K. Z. Ghafoor, A. S. Sadiq, K. Curran, K. Rabie, A novel ai-enabled framework to diagnose coronavirus covid 19
using smartphone embedded sensors: Design study, arXiv preprint
arXiv:2003.07434 (2020).
[84] S.-W. Lee, H. Jung, S. Ko, S. Kim, H. Kim, K. Doh, H. Park, J. Yeo,
S.-H. Ok, J. Lee, S. Lim, M. Jeong, S. Choi, S. Hwang, E.-Y. Park, G.J. Ma, S.-J. Han, K.-S. Cha, N. Sung, J.-W. Ha, Carecall: a call-based
active monitoring dialog agent for managing covid-19 pandemic, arXiv
preprint arXiv:2007.02642 (2020).
[85] T. Koike, K. Qian, B. W. Schuller, Y. Yamamoto, Learning higher representations from pre-trained deep models with data augmentation for
the compare 2020 challenge mask task, Proceedings INTERSPEECH.
Shanghai, China: ISCA (2020) 2047–2051.
[86] J. Szep, S. Hariri, Paralinguistic classification of mask wearing by image
classifiers and fusion, Proceedings INTERSPEECH. Shanghai, China:
ISCA (2020) 2087–2091.
[87] S. E. Eikenberry, M. Mancuso, E. Iboi, T. Phan, K. Eikenberry,
Y. Kuang, E. Kostelich, A. B. Gumel, To mask or not to mask: Modeling the potential for face mask use by the general public to curtail
the covid-19 pandemic, Infectious Disease Modelling (2020).

54

[88] Z. Wang, G. Wang, B. Huang, Z. Xiong, Q. Hong, H. Wu, P. Yi,
K. Jiang, N. Wang, Y. Pei, H. Chen, Y. Miao, Z. Huang, J. Liang,
Masked face recognition dataset and application, arXiv preprint
arXiv:2003.09093 (2020).
[89] F. Al Hossain, A. A. Lover, G. A. Corey, N. G. Reich, T. Rahman,
Flusense: a contactless syndromic surveillance platform for influenzalike illness in hospital waiting areas, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4 (1) (2020)
1–28.
[90] Y. Wang, M. Hu, Q. Li, X.-P. Zhang, G. Zhai, N. Yao, Abnormal
respiratory patterns classifier may contribute to large-scale screening of
people infected with covid-19 in an accurate and unobtrusive manner,
arXiv preprint arXiv:2002.05534 (2020).
[91] P. D. Patel, J. Cobb, D. Wright, R. Turer, T. Jordan, A. Humphrey,
A. L. Kepner, G. Smith, S. T. Rosenbloom, Rapid development of
telehealth capabilities within pediatric patient portal infrastructure for
covid-19 care: Barriers, solutions, results, Journal of the American
Medical Informatics Association (2020).
[92] J. Han, K. Qian, M. Song, Z. Yang, Z. Ren, S. Liu, J. Liu, H. Zheng,
W. Ji, T. Koike, X. Li, Z. Zhang, Y. Yamamoto, B. W. Schuller, An
early study on intelligent analysis of speech under covid-19: Severity, sleep quality, fatigue, and anxiety, Proceedings INTERSPEECH.
Shanghai, China: ISCA (2020).

55

[93] X. Li, K. Qian, L.-l. Xie, X.-j. Li, M. Cheng, L. Jiang, B. W. Schuller,
A mini review on current clinical and research findings for children
suffering from covid-19, medRxiv (2020).
[94] G. Sogancıoglu, O. Verkholyak, H. Kaya, D. Fedotov, T. Cadée, A. A.
Salah, A. Karpov, Is everything fine, grandma? acoustic and linguistic
modeling for robust elderly speech emotion recognition, Proceedings
INTERSPEECH. Shanghai, China: ISCA (2020).
[95] L. Biester, K. Matton, J. Rajendran, E. M. Provost, R. Mihalcea,
Quantifying the effects of covid-19 on mental health support forums,
arXiv preprint arXiv:2009.04008 (2020).
[96] W. Ahmed, J. Vidal-Alaball, J. Downing, F. L. Seguı́, Dangerous messages or satire? analysing the conspiracy theory linking 5g to covid-19
through social network analysis, J. Med Internet Res (2020).
[97] S. Guney, C. Daniels, Z. Childers, Using ai to understand the patient
voice during the covid-19 pandemic, NEJM Catalyst Innovations in
Care Delivery 1 (2) (2020).
[98] J. E. C. Saire, J. F. O. Cruz, Study of coronavirus impact on parisian
population from april to june using twitter and text mining approach,
medRxiv (2020).
[99] J. X. Koh, T. M. Liew, How loneliness is talked about in social media
during covid-19 pandemic: text mining of 4,492 twitter feeds, Journal
of Psychiatric Research (2020).

56

[100] L. L. Wang, K. Lo, Y. Chandrasekhar, R. Reas, J. Yang, D. Eide,
K. Funk, R. Kinney, Z. Liu, W. Merrill, P. Mooney, D. Murdick,
D. Rishi, J. Sheehan, Z. Shen, B. Stilson, A. D. Wade, K. Wang,
C. Wilhelm, B. Xie, D. Raymond, D. S. Weld, O. Etzioni, S. Kohlmeier,
Cord-19: The covid-19 open research dataset, ArXiv (2020).
[101] M. Albes, Z. Ren, B. Schuller, N. Cummins, Squeeze for sneeze: Compact neural networks for cold and flu recognition, Proceedings INTERSPEECH. Shanghai, China: ISCA (2020) 4546–4550.
[102] B. W. Schuller, A. Batliner, C. Bergler, F. B. Pokorny, J. Krajewski, M. Cychosz, R. Vollmann, S.-D. Roelen, S. Schnieder, E. Bergelson, A. Cristia, A. Seidl, A. S. Warlaumont, L. Yankowitz, E. Nöth,
S. Amiriparian, S. Hantke, M. Schmitt, The interspeech 2019 computational paralinguistics challenge: Styrian dialects, continuous sleepiness,
baby sounds & orca activity., in: Interspeech, 2019, pp. 2378–2382.
[103] G. Gosztolya, Using fisher vector and bag-of-audio-words representations to identify styrian dialects, sleepiness, baby & orca sounds, Proceedings of the INTERSPEECH 2019, Graz, Austria (2019) 2413–2417.
[104] S. Amiriparian, P. Winokurov, V. Karas, S. Ottl, M. Gerczuk,
B. W. Schuller, A novel fusion of attention and sequence to sequence autoencoders to predict sleepiness from speech, arXiv preprint
arXiv:2005.08722 (2020).
[105] F. Ringeval, B. Schuller, M. Valstar, J. Gratch, R. Cowie, S. Scherer,
S. Mozgai, N. Cummins, M. Schmitt, M. Pantic, Avec 2017: Real-life
57

depression, and affect recognition workshop and challenge, in: Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge,
2017, pp. 3–9.
[106] J. Gratch, R. Artstein, G. Lucas, G. Stratou, S. Scherer, A. Nazarian,
R. Wood, J. Boberg, D. DeVault, S. Marsella, D. Traum, S. Rizzo,
L.-P. Morency, The distress analysis interview corpus of human and
computer interviews., in: LREC, 2014, pp. 3123–3128.
[107] Z. Zhao, Z. Bao, Z. Zhang, N. Cummins, H. Wang, B. Schuller, Hierarchical attention transfer networks for depression assessment from
speech, in: ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp.
7159–7163.
[108] A. S. Miner, L. Laranjo, A. B. Kocaballi, Chatbots in the fight against
the covid-19 pandemic, npj Digital Medicine 3 (1) (2020) 1–4.

58

