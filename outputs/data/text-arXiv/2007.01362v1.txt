Thermodynamics 2.0 Meeting, June 22-24, 2020

Thermodynamics and the Evolution of Stochastic Populations
Themis Matsoukas∗

arXiv:2007.01362v1 [cond-mat.stat-mech] 2 Jul 2020

Department of Chemical Engineering, The Pennsylvania State University, University Park, PA 16802.
(Dated: June 20, 2020)
The appeal of thermodynamics to problems outside physics is undeniable, as is the growing
recognition of its apparent universality, yet in the absence of a rigorous formalism divorced from
the peculiarities of molecular systems all attempts to generalize thermodynamics remain qualitative
and heuristic at best. In this paper we formulate a probabilistic theory of thermodynamics and and
set the basis for its application to generic stochastic populations.

I.

The solution to this variational problem is the canonical
probability of microstate,

WHAT IS THERMODYNAMICS?

There is growing recognition that thermodynamics is
a universal science but what is thermodynamics? The
typical answer is some variation of “thermodynamics is
the study of the equilibrium properties of matter.” Undoubtedly the study of the equilibrium state of matter
falls within the scope of thermodynamics, clearly though,
such statement is far too narrow to serve as a definition
of thermodynamics, far too narrow to justify its appeal
in areas outside physics. The answer describes one particular problem (albeit a very important one) that thermodynamics can tackle, but tells little about what thermodynamics is capable of. What is thermodynamics?
Until Boltzmann and Gibbs thermodynamics was
about heat and work and its central aim was to determine the maximum amount of work that can be extracted
from heat in a cycle. This is a variational problem and its
solution was found via a variational principle, the maximization of entropy. With Boltzmann and Gibbs [1],
thermodynamics entered the realm of the probabilistic
world –while still footed in the physical world of mechanical particles, atoms and molecules. Entropy now
has a probabilistic interpretation, it is a functional of the
probability distribution pi of microstates,
X
pi log pi .
(1)
S[{p}] = −
i

The equilibrium distribution p∗i is identified among all
feasible distributions as the one that maximizes this functional:
S[{p∗ }] ≥ S[p].

(2)

A feasible distribution is one that satisfies the macroscopic constraints that define the system. For a system
with fixed mean energy Ē, volume V and number of particles N this means any distribution with mean energy
Ē, i.e., any distribution that satisfies
X
Ei pi = Ē.
(3)

p∗i =

e−βEi
,
Q(β, V, N )

(4)

Q is the canonical partition function and β is the inverse
temperature. With minimal additional calculus β and
Q are expressed in terms of the microcanonical partition
function Ω(E, V, N ),
β=

∂ log Ω
;
∂E

log Q = log Ω − Ē

∂ log Ω
;
∂E

(5)

and similarly Ē is expressed in terms of the canonical
partition function
Ē = −

∂ log Q
.
∂β

(6)

Equations (1) through (6) cast the variational problem
in terms of a probability space, the space of all probability distributions that are feasible under the macroscopic
specification of state. These equations bear only indirect
relationship to the physical laws that govern that particles that produce the microstates in question via Ei . It
should not be a surprise that thermodynamics survived
the quantum revolution. Quantum mechanics changes
the calculation of the energy Ei of microstate but leaves
the entire network of Eqs. (2)–(6) intact. The hint is
clear: thermodynamics is not a physical theory.
The door that allowed thermodynamics to escape the
confines of physics was unlocked by Shannon, who arrived
at the entropy functional, the same functional as that of
Boltzmann and Gibbs, by methods that have nothing to
do with physical particles or laws of motion [2]. Even
so the door remained closed until Jaynes pushed it open.
Jaynes conjectured that thermodynamics is about inference, not physics:
[...] we may have now reached a state where
statistical mechanics is no longer dependent on
physical hypotheses, but may become simply an
example of statistical inference [3].

i

∗

txm11@psu.edu

With this bold statement the toolbox of thermodynamics
was made available to anyone who can use it. During the
more than 60 years since Jaynes’s pronouncement thermodynamics has been steadily moving closer to achieving

2
the status of universal language with applications in ecology [4], epidemics [5], neuroscience [6], financial markets
[7], and in the study of complexity in general. But what
are the rules of this universal science? We have no comprehensive theory of generalized thermodynamics and no
guidance on how to connect a particular system to thermodynamics. One is largely left to construct analogies
to molecular systems and hope they hold.
Here we attempt to decipher the universal grammar
of thermodynamics. First, we construct a mathematical formalism based on variational calculus that generates the entire network of mathematical thermodynamics, Eqs. (1) through (6), via a variational construction
that assigns probabilities to a set of probability distributions and seeks the most probable distribution in the set.
The most probable distribution is identified by maximizing its probability and this leads to the maximization of
a functional analogous to but different from entropy, in
fact, a generalized (negative) free energy. We show that
any probability distribution can be expressed in the formalism of thermodynamics and illustrate how these ideas
can be applied to a generic population that evolves under
a stochastic process. The paper builds on work recently
published in Refs. [8] and [9].

II.

where rN is a normalization constant. We require the
bias to be of the form[9]
log W ({n}) =

i

(7)

and its log is
log P ({h}) = −N

X
i

pi log

pi
.
hi

∂ log W ({n}) X
ni log wi (10)
=
∂ni
i

X
log P ({p})
pi
pi log
− log r.
=−
N
h
i wi
i

p∗i =

We start with a discrete random variable X = i,
i = 1, 2 · · · with probability distribution P (X = i) = hi .
We collect a random sample of N points and count
the number ni of samples with X = i. The (intensive) empirical distribution of the sample is pi = ni /N
and represents a guess of the true distribution hi . The
probability to obtain a particular (extensive) distribution
{n} = N {h} is
N
Y
hni i
,
n!
i=1 i

ni

with log wi = ∂ log W ({n})/∂ni . This makes log W homogeneous functional of {n} with degree 1 and ensures
that sampling converges to a limiting distribution when
N → ∞ [9]. The log of probability to obtain the empirical distribution pi = ni /N now is

Sampling Statistics

P ({n}) = P ({h}) = N !

X

(11)

when N → ∞, the set of feasible distributions contains
all
P distributions that satisfy the normalization condition
i hi = 1. The most probable distribution is obtained
by maximizing Eq. (11) with respect to pi :

THERMODYNAMICS BEYOND
MOLECULES
A.

We now bias the sampling process so that we may obtain a different distribution as the most probable distribution from the one that is being sampled. We bias the
process by a functional ({n}) such that the probability
to obtain a sample with extensive distribution {n} is
!
N
Y
hni i
W ({n})
P ({n}) =
,
(9)
N!
rN
n!
i=1 i

(8)

We recognize the result as the Kullback-Leibler divergence [10] (or relative entropy). This functional, as is
well known, is maximized by p∗i = hi , a result that makes
intuitive sense: in a random sample taken from some
distribution the most likely distribution to materialize
in the sample is the distribution that is being sampled,
even though other distributions are possible. When N is
large (thermodynamic limit), pi is overwhelmingly more
probable than any other distribution.

wi
hi .
r

(12)

As we intended, the most probable distribution is different from the sampled distribution by a factor wi /r. We
can choose the partial bias wi so as to obtain any distribution as the MPD of the biased sampling process: if
we pick wi = rfi /hi , where fi is any distribution in the
domain of hi , then fi will materialize as the most probable distribution by this biased sampling. In the limit
N → ∞ the MPD is overwhelmingly more probable than
any other distribution.
The conclusion from this analysis is this: Sampling
from a distribution establishes an entire space of distributions that contains every distributions that is defined
on the domain of the sampled distribution. The bias
functional establishes a probability measure on the distributions of this space. This bias can be constructed
so as to select any of these distributions as the most
probable distribution. For this reason we refer to W as
selection functional.
B.

Microcanonical Sampling

We now sample from an exponential distribution with
mean x̄,
hi =

e−i/x̄
,
x̄

(13)

3
and consider the subspace of empirical distributions with
the same mean. The probability of empirical distribution
{p} is given by Eq. (11) with a new normalization constant r′ . We write the result in the form
X
pi
log P ({p})
− log ω.
=−
pi log
N
w
i
−

(14)

with log ω = 1 + log x̄ + log r′ . The MPD is obtained
by maximizing this functional with respect to all hi that
satisfy the constraint and whose mean is x̄. The result is
p∗i =

wi e−βi
,
q

(15)

with β and q parameters associated with the Lagrange
multipliers for the two constraints in this maximization.
We can show [9]
x̄ = −

d log q
,
dβ

(16)

which establishes a relationship between the mean of the
MPD and its parameters β and q.
C.

pi to refer to the probability of microstate. The selection
functional in this case is uniform, W = wi = 1, and expresses the postulate of equal a priori probabilities. With
these substitutions (15) reverts to the familiar canonical
probability distribution and Eq. (20) reads

Universal Thermodynamics

Define the microcanonical functional ρ[{p}] =
log P ({p})/N use Eq. (14) to write it as
X
pi
ρ[{p}] = −
pi log
− log ω.
(17)
wi
−
The condition that defines the most probable distribution
now is expressed in the form of the inequality ρ[{p∗ }] ≥
ρ[{p}]. For large N the MPD is overwhelmingly more
probable than any other distribution. Then P ({p∗ }) → 1
and ρ[{p∗ }] → 0. The condition that defines the MPD
then becomes

log ω ≥ S[{p}].

(22)

This is the inequality of the second law: The MPD has
the maximum entropy among all possible probability distributions that can be assigned to the set of microstates.
With Eqs. (15), (16), (20) and (21) we have obtained
the generalized forms of the thermodynamic set in Eqs.
(1)–(6) via an argument that contains no physics. In
this formulation the inequality of the second law has a
simple non controversial and in fact trivial meaning: it
states that the most probable distribution is more probable than any other in the set of feasible distributions.
III.

EVOLUTION OF EXTENSIVE
POPULATIONS

The implications for the universality of thermodynamics are profound. Any probability distribution in the
domain of the exponential distribution (x ≥ 0) can be
obtained as the most probable distribution in this domain and therefore can be described in the formalism
of thermodynamics. The key to this formulation is the
selection bias that selects the MPD among all distributions, but what is W ? In the case of molecular systems
it is fixed by the postulate of equal a priori probabilities, a model assumptions that assigns a probability to
the elements of the feasible space. In the general case it
is set by the rules of the stochastic process that governs
the evolution of the random variable in question. Let us
sketch how this works in the case of an extensive population n = (n1 , n2 · · · ). We suppose the elements of the
population undergo transitions of the form
pi→j

ρ[{p}] ≤ 0,

with the equal sign only for {p} = {p∗ }. This inequality
is the central variational condition of the theory. Apply
this to Eq. (17)
X
log ω ≥ −
hi log pi + log W [{p}],
(19)
i

or more compactly,
log ω ≥ S[{p}] + log W [{p}].

(20)

With {p} = {p∗ } from Eq. (15) the above becomes an
exact equality. The result is
log ω = β x̄ + log q.

i −−−→ j

(18)

(21)

The simplest way to make contact with thermodynamics
is to take i to refer to the energy Ei of microstate and

(23)

with transition probability pi→j . Accordingly, the distribution of walkers is transformed by these transitions and
produces a Markov chain of the form
· · · n′

G−1

→n

G

···

(24)

where G = 0, 1 · · · is the generation of n and counts the
number of transitions it has undergone from fixed initial
state. We then have
X
X
ni ,
(25)
g i ni , N =
G=
i

i

Here gi is the number of transitions that produce element
i and N is the number of elements in n and is constant
in all generations.
The ensemble EG,N is the set of all distributions that
are formed by G sequential transitions starting from the

4
with n! = N !/n1 !n2 ! · · · and Ω = ω N . The distributions
of the ensemble are sampled via the trajectories that lead
to them and the selection functional W is determined by
the transition probabilities along these trajectories. The
most probable distribution, obtained by maximizing Eq.
(26) under the two constraints in (21), is given by Eq.
(15) and obeys all thermodynamic relationships in Eqs.
(16), (20) and (21). The entire formalism of thermodynamics is transferred to generic stochastic processes.
IV.

FIG. 1. Trajectories of N = 4 walkers undergoing one transition at a time. Filled circles represent transitions accumulated
by the walker. In this example ni is he number of walkers with
gi = i transitions. Distributions are represented pictorially
and in vector form as (n0 , n1 , n2 , n3 ).

same initial state (Fig. 1). The probability of distribution
n in EG,N is given by Eq. (9), which we now write as

P (n) =

n!
W (n),
ΩG,N

(26)

CONCLUSIONS

Stripped to its core statistical thermodynamics is the
calculus of the most probable distribution. The theory
outlined here establishes a space of distributions (the microcanonical ensemble) and a probability measure on it
via a sampling process. This mathematical construction
gives rise to the relationships we recognize as thermodynamics. A stochastic process may be viewed as a network of transitions that emanate from a known initial
state and branch into the future. The ensemble is the set
of distributions that can be reached in a given number
of steps, and sampling refers to arriving to a distribution via a trajectory of transitions. The specifics of the
process (transition probabilities) enter via the selection
functional –this is the contact point between generalized
thermodynamics and the particulars (“physics”) of system that is being studied. We suggest that this formalism
can be applied to a variety of stochastic populations and
plan to offer examples in the near future.

[7] J. Voit, The Statistical Mechanics of Financial
[1] J. W. Gibbs, Elementary Principles in Statistical MeMarkets (Springer Berlin Heidelberg, Berlin, Heichanics (Ox Bow Press, Woodbridge, CT, 1981), (reprint
delberg,
2005),
ISBN 978-3-540-26289-3,
URL
of the 1902 edition).
https://doi.org/10.1007/3-540-26289-X.
[2] C. E. Shannon, Bell System Technical Journal 27, 379
[8] T. Matsoukas, Generalized Statistical Thermodynam(1948).
ics:
Thermodynamics of Probability Distributions
[3] E. T. Jaynes, Phys. Rev. 106, 620 (1957).
and Stochastic Processes (Springer International
[4] J. Harte, T. Zillio, E. Conlisk, and A. B. Smith,
Publishing, 2019), ISBN 978-3-030-04149-6, URL
Ecology 89, 2700 (2008), ISSN 1939-9170, URL
https://doi.org/10.1007/978-3-030-04149-6.
http://dx.doi.org/10.1890/07-1369.1.
[9] T. Matsoukas, Entropy 21 (2019), ISSN 1099-4300, URL
[5] R. Durrett,
SIAM Review 41,
677 (1999),
https://www.mdpi.com/1099-4300/21/9/890.
https://doi.org/10.1137/S0036144599354707,
URL
[10] S.
Kullback
and
R.
A.
Leibler,
Ann.
https://doi.org/10.1137/S0036144599354707.
Math.
Statist.
22,
79
(1951),
URL
[6] N. M. Timme and C. Lapish, eNeuro (2018),
http://dx.doi.org/10.1214/aoms/1177729694.
http://www.eneuro.org/content/early/2018/06/29/ENEURO.005218.2018.full.pdf, URL http://www.eneuro.org/content/early/2018/06/29/ENEURO.0052-18.2018.

