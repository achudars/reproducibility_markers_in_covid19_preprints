Two-Stage Adaptive Pooling with RT-qPCR for
COVID-19 Screening
Anoosheh Heidarzadeh and Krishna Narayanan
Department of Electrical and Computer Engineering
Texas A&M University

arXiv:2007.02695v1 [cs.IT] 6 Jul 2020

College Station, TX 77843

Abstract
We propose two-stage adaptive pooling schemes, 2-STAP and 2-STAMP, for detecting COVID-19
using real-time reverse transcription quantitative polymerase chain reaction (RT-qPCR) test kits. Similar to
the Tapestry scheme of Ghosh et al., the proposed schemes leverage soft information from the RT-qPCR
process about the total viral load in the pool. This is in contrast to conventional group testing schemes
where the measurements are Boolean. The proposed schemes provide higher testing throughput than the
popularly used Dorfman’s scheme. They also provide higher testing throughput, sensitivity and specificity
than the state-of-the-art non-adaptive Tapestry scheme. The number of pipetting operations is lower than
state-of-the-art non-adaptive pooling schemes, and is higher than that for the Dorfman’s scheme. The
proposed schemes can work with substantially smaller group sizes than non-adaptive schemes and are
simple to describe. Monte-Carlo simulations using the statistical model in the work of Ghosh et al.
(Tapestry) show that 10 infected people in a population of size 961 can be identified with 70.86 tests on
the average with a sensitivity of 99.50% and specificity of 99.62%. This is 13.5x, 4.24x, and 1.3x the
testing throughput of individual testing, Dorfman’s testing, and the Tapestry scheme, respectively.

I. I NTRODUCTION
There is broad consensus among epidemiologists, economists and policy makers that wide-scale testing
of asymptomatic patients is the key for reopening the economy. While the benefits of testing are obvious,
shortage of testing kits, reagents and the ensuing low-throughput of individual testing protocols has
prevented deployment of wide-scale testing. Group testing or, pooling is an alternative way to substantially
increase the testing throughput.
This material is based upon work supported by the National Science Foundation (NSF) under Grant CCF-2027997.

1

The idea of group testing was introduced by Dorfman [1] during World War II for testing soldiers for
syphilis without having to test each soldier individually. Dorfman’s scheme consists of two stages (or
rounds). In the first stage, the set of people to be tested is split into disjoint pools and a test is performed
on each pool. If a pool tested negative, everyone in that pool will be identified as non-infected. Otherwise,
if a pool tested positive, we proceed to the second stage where all people in a positive pool will be tested
individually, and then identified as infected or non-infected accordingly. When the prevalence is small,
Dorfman’s scheme requires substantially fewer tests than individual testing.
Dorfman-style testing has been implemented in the past in screening for many diseases including
HIV [2], Chlamydia and Gonorrhea [3]. It has also been considered for screening for influenza [4]. For
COVID-19, several experimental results have confirmed the feasibility of using Dorfman-style pooling
and it has been implemented in Nebraska, Germany, India and China [5], [6], [7], [8].
While Dorfman-style pooling is easy to implement, it is not optimal. Over the past 75 years, more
sophisticated group testing schemes that provide higher testing throughput have been designed. The
literature on group testing is too vast to review in detail and an overview of the techniques can be
found in [9] and [10]. Group testing is also related to compressed sensing and insights from compressed
sensing have been used to design group testing schemes. An important difference between group testing
and compressed sensing is that in group testing, the measurements are Boolean (test result is either
positive or negative) and they naturally correspond to non-linear functions of the unknown vector.
The vast majority of the work using group testing with real-time reverse transcription quantitative
polymerase chain reaction (RT-qPCR) has only considered Boolean measurements even though the RTqPCR process can produce more fine-grained information (soft information) about the total viral load in
the pool. It is well-known in information theory that such soft information can potentially be used to
increase testing throughput substantially. However, group testing schemes that leverage soft information
from the RT-qPCR process remain largely unexplored.
Very recently, Ghosh et al. in [11] developed a statistical model relating the soft information from
the RT-qPCR to the total viral load in the pool. They designed a scheme called Tapestry, which uses
non-adaptive group testing using Kirkman triples and they considered several decoding algorithms that
use the soft information. They showed substantial gains in testing throughput over Dorfman’s scheme and
to the best of our knowledge, this scheme is the state of the art non-adaptive group testing scheme that
works with RT-qPCR, especially since it is the only work we are aware of that uses the soft information
from the RT-qPCR measurement process.
Here, we propose two simple and effective two-stage adaptive pooling schemes that use the soft
information from the RT-qPCR process and provide several advantages over Dorfman’s scheme and

2

the Tapestry scheme. We refer to these algorithms as the Two-stage Adaptive Pooling (2-STAP) and
the Two-stage Adaptive Mixed Pooling (2-STAMP) schemes/algorithms. The proposed schemes provide
substantially higher throughput than Dorfman-style testing. Compared to the Tapestry scheme in [11],
2-STAP and 2-STAMP have higher testing throughput and under the statistical model developed in [11],
for all tested cases, our algorithms have higher sensitivity and higher specificity. The proposed algorithms
require fewer pipetting operations than Tapestry, but require more pipetting operations than Dorfman’s
scheme. Finally, 2-STAP and 2-STAMP work with much smaller pool sizes and population sizes than
the Tapestry algorithm and hence, is easy to describe and implement in the lab. Monte-Carlo simulations
using the statistical model in the work of Ghosh et al. (Tapestry), show that 10 infected people in a
population of size 961 can be identified with 70.86 tests on the average with a sensitivity of 99.50% and
specificity of 99.62% with a pool size of 31. This is 13.5x, 4.24x, and 1.3x the testing throughput of
individual testing, Dorfman’s testing, and the Tapestry scheme, respectively.
Unlike Tapestry, which is a non-adaptive scheme, 2-STAP and 2-STAMP require storage of the swab
samples and their accessibility for the second round of testing—similar to that of Dorfman’s scheme.
II. P ROBLEM S ETUP
In this section, we explain the problem setup. Throughout, we will consider the example of poolingbased testing for COVID-19 using the real-time reverse transcription quantitative polymerase chain
reaction (RT-qPCR) technique—considered also in [11]—as an application of sensing with binary matrices
for support recovery of sparse signals.
Let R≥0 = { x ∈ R : x ≥ 0} and R>0 = R≥0 \ {0}. For any integer i > 0, we denote {1, . . . , i } by

[i], and define [0] = ∅.
Consider a population of n people, labeled 1, . . . , n, that are to be tested for COVID-19. The vector
of viral loads of these people can be modeled by a signal x = [ x1 , . . . , xn ]T , x j ∈ R≥0 , where the jth
coordinate of x represents the viral load of the jth person. If the jth person is infected (i.e., COVID19 positive), then x j is a nonzero value; otherwise, if the jth person is not infected (i.e., COVID-19
negative), then x j is zero. We assume that every coordinate in x is nonzero with probability p (or zero
with probability 1 − p), independently from other coordinates, and every nonzero coordinate takes a value
from R>0 according to a fixed and known probability distribution p x . Note that the sparsity parameter
p may or may not be known. In this context, the sparsity parameter p is known as prevalence.
We denote the number of nonzero coordinates in x (e.g., the number of infected people) by n+ , and
denote the number of zero coordinates in x (e.g., the number of non-infected people) by n− . Note that

3

n+ + n− = n. We denote by S(x) the support set of x, i.e., the index set of all nonzero coordinates in
x. Note that | S(x)|= n+ .
The ith binary linear measurement y of x is defined as a linear combination of coordinates x j ’s
according to the coefficients ai j ’s that are elements from {0, 1}. That is, yi = ai · x = ∑nj=1 ai, j x j ,
where ai = [ ai,1 , . . . , ai,n ], ai, j ∈ {0, 1}. (Note that a binary linear measurement is different from a
Boolean measurement. In the former, the coefficients are binary but the measurement value can be a real
number, whereas in the latter both the coefficients and the measurement value are binary.) For example,
a measurement yi represents the sum of viral loads of a subset of people to be tested for COVID-19.
Given a measurement yi , any coordinate xi j such that ai, j = 1 (or ai, j = 0) is referred to as an active
(or inactive) coordinate in the measurement y.
Suppose we sense the signal x by making the measurements y1 , y2 , . . . , and observe noisy versions of
y1 , y2 , . . . , denoted by z1 , z2 , . . . . The ith measurement yi and the noisy measurement zi are given by
n

yi =

∑ ai, j x j

j=1

zi = yiεi ,

(1)

where εi ’s are independent realizations of a random variable ε – taking values from R>0 according to
a fixed and known probability distribution pε . (The reason we consider a multiplicative noise model,
instead of the commonly-used additive noise model, will be discussed shortly.) Note that zi = 0 if and
only if yi = 0 (i.e., all active coordinates in the ith measurement are zero coordinates), and zi 6= 0 if
and only if yi 6= 0 (i.e., there exists at least one nonzero coordinate among the active coordinates in the
ith measurement). A detailed explanation about the multiplicative noise model in (1) can be found in
Appendix I.
Our goal is to collect as few noisy measurements z1 , z2 , . . . as possible for any signal x such that the
support set S(x) can be recovered from z1 , z2 , . . . , with a target level of accuracy as defined shortly.
We refer to the process of generating the measurements as sensing, and refer to the process of estimating
the support set from the noisy measurements as (signal-support) recovery. Given a sensing algorithm and
a recovery algorithm, we denote the estimate of S(x) by Sb(x), which depends on the noisy measurements
z1 , z2 , . . . and the sensing and recovery algorithms. Any coordinate x j such that j ∈ S \ Sb(x) is referred
to as a false negative, and any coordinate x j such that j ∈ Sb(x) \ S(x) is referred to as a false positive.
Similarly, any coordinate x j such that j ∈
/ S(x) ∪ Sb(x) is referred to as a true negative, and any coordinate
x j such that j ∈ S(x) ∩ Sb(x) is referred to as a true positive. We denote by f − (x) the number of false
negatives, i.e., f − (x) = | S(x) \ Sb(x)|. Similarly, we denote by f + (x) the number of false positives, i.e.,

4

f + (x) = | Sb(x) \ S(x)|. Note that f − (x) and f + (x) depend on the noisy measurements z1 , z2 , . . . and
the sensing and recovery algorithms.
Given a sensing algorithm and a recovery algorithm, the false negative rate r− is defined as the expected
value of the ratio of the number of false negatives to the number of nonzero coordinates, i.e., r− =

E[ f − (x)/n+ (x)], where the expectation is taken over all signals x. Similarly, the false positive rate r+ =
E[ f + (x)/n− (x)]. It should be noted that the ratios f − (x)/n+ (x) and f + (x)/n− (x) are random variables,
because they depend on x, which is itself random in both the deterministic and probabilistic models defined
earlier. Also, conditioned on x having k nonzero coordinates and n − k zero coordinates, we denote the
[k]

[k]

conditional false negative rate by r− and the conditional false positive rate by r+ . The quantities 1 − r−
and 1 − r+ are known as (unconditional) sensitivity and specificity, respectively. Analogously, we refer
[k]

[k]

to 1 − r− and 1 − r+ as conditional sensitivity and conditional specificity, respectively.
For given thresholds 0 ≤ δ− , δ+ < 1, our goal is to design a sensing algorithm and a recovery algorithm
[k]

such that with minimum number of measurements the constraints r− ≤ δ− and r+ ≤ δ+ (or r− ≤ δ−
[k]

and r+ ≤ δ+ ) are satisfied. The thresholds δ− and δ+ specify the target level of accuracy for support
recovery.
III. S INGLE -S TAGE S CHEMES VERSUS M ULTI -S TAGE S CHEMES
In a single-stage sensing scheme, also known as non-adaptive sensing, m measurements y1 , . . . , ym
are made in parallel, and m noisy measurements z1 , . . . , zm are observed. The coefficient vectors of the
measurements y1 , . . . , ym can be represented by an m × n sensing matrix A with entries from {0, 1}.
T T
That is, A = [aT
1 , . . . , am ] where ai = [ ai,1 , . . . , ai,n ] represents the ith row of the sensing matrix A, i.e.,

the coefficients of x j ’s in the ith measurement yi . It should be noted that in a single-stage sensing scheme,
the sensing matrix A is designed in advance, prior to any initial sensing of the signal, and hence the
name “non-adaptive sensing”. We denote the vector of measurements by y = [ y1 , . . . , ym ]T , the vector
of noisy measurements by z = [ z1 , . . . , zm ]T , and the vector of noise values by ε = [ε1 , . . . , εm ]T . Then,
y = Ax, and z = y ◦ ε, where the symbol “◦” denotes the element-wise (Hadamard) product. Given a
sensing matrix A, a recovery algorithm seeks to compute an estimate Sb(x) of the support set S(x) of
the signal x from the noisy measurement vector z.
The idea of single-stage sensing can be extended to T-stage sensing (for any natural number T) as
follows. A T-stage sensing scheme consists of T sensing matrices A(1) , . . . , A(T ) , where A(t) is an
m(t) × n matrix with entries from {0, 1}. Similarly as in the case of single-stage sensing scheme, for
(t)

(t) T
] ,
m(t)
by ε(t)

each stage t we denote the vector of measurements by y(t) = [ y1 , . . . , y
measurements by z(t) =

(t)
(t)
[ z 1 , . . . , z (t ) ]T ,
m

and the vector of noise values

5

the vector of noisy
(t)

= [ε1 , . . . , ε

(t) T
] .
m(t)

Then, y(t) = A(t) x and z(t) = y(t) ◦ ε(t) . For each t > 1, the measurements in the tth stage are all made
in parallel, similar to the single-stage case, but the design of sensing matrix A(t) depends on the design
of all sensing matrices A(1) , . . . , A(t−1) and all noisy measurement vectors z(1) , . . . , z(t−1) . Given the
sensing matrices A(1) , . . . , A(T ) , a recovery algorithm seeks to compute an estimate Sb(x) of S(x) from
z(1) , . . . , z( T ) .
IV. R ELATED P ROBLEMS
The similarities and differences between this work and those in the literature of Compressed Sensing
(CS) and Quantitative Group Testing (QGT) are summarized below:
•

Binary versus non-binary signals: In QGT, the nonzero coordinates of the signal have all the same
value; whereas in CS – similar to the present work, the nonzero coordinates can take different values.

•

Real versus binary sensing matrices: Many of the existing CS algorithms rely on sensing matrices
with real entries; whereas the sensing matrices in the QGT algorithms – similar to this work, are
binary.

•

Unconstrained versus constrained sensing matrices: In CS and QGT, there is often no explicit
constraint on the number of nonzero entries per row (referred to as the row weight) and the number
of nonzero entries per column (referred to as the column weight) of the sensing matrices. However,
in this work, the focus is on sensing matrices in which the row weights and the column weights are
constrained.

•

Additive versus multiplicative noise: Almost all existing work on CS and QGT consider either the
noiseless setting, or the settings in which the noise is additive. The focus of this work is, however,
on the multiplicative noise.

•

Single-stage versus multi-stage sensing: Most of the existing work on CS focus on single-stage
(non-adaptive) sensing algorithms. However, in QGT – similar to this work, both single-stage and
multi-stage sensing algorithms have been studied.

•

Signal recovery versus signal-support recovery: In QGT, the signal recovery and the support recovery
problems are equivalent, since the signal is binary-valued. In CS, however, the goal is often to
estimate the signal itself, rather than estimating the support of the signal only. In this work, the
goal is to recover the support of the signal (similar to QGT), even though the signal is non-binary
(similar to CS).

6

V. R ELATED W ORK : TAPESTRY
For the same setting as the one in this work, a scheme called Tapestry was recently proposed in [11].
In what follows, we briefly explain the sensing and recovery algorithms in this scheme.

A. Sensing Algorithms
The Tapestry scheme employs a single-stage sensing algorithm. The sensing matrices used in this
scheme are based on the Kirkman triples, a special class of the Steiner triples, borrowed from the
combinatorial design theory.
Let m be a multiple of 3, and let c be an integer such that c ≤

m−1
2 .

A (balanced) Kirkman triple

system (with parameters m and c) can be represented by a binary matrix with m rows and

m
3

× c columns

such that:
•

The support set of each column has size exactly 3;

•

The intersection of support sets of any two columns has size at most 1;

•

For any 0 ≤ i ≤

m
3 ( c − 1 ) such that i is a multiple
i + 1, i + 2, . . . , i + m3 is equal to an all-one vector.

of

m
3,

the sum of the columns indexed by

B. Recovery Algorithm
In [11], several different recovery algorithms from the CS literature were considered. In the following
we explain one of these recovery algorithms which will also be used as part of the proposed schemes in
this paper, and refer the reader to Appendix II for detailed description of the other recovery algorithms
considered in [11].
Combinatorial Orthogonal Matching Pursuit (COMP): The COMP algorithm – which will also be
used as part of the proposed schemes in this paper, works based on a simple observation: if zi = 0
(and hence, yi = 0 since the noise εi takes only nonzero values), then all active coordinates in the ith
measurement are zero coordinates. The algorithm initially marks all coordinates x j ’s as “unknown”. Then,
the algorithm finds all i such that zi = 0. Let I = {i ∈ [m] : zi = 0}. For every i ∈ I, the algorithm
marks all active coordinates in the ith measurement as “known”. Upon termination of the algorithm,
the index set of all coordinates that are marked “unknown” yields an estimate Sb(x) of the support set
S(x). (Any coordinate marked “known” is surely a zero coordinate in the signal, and its index will not
be included in the estimated support set.) Note that the support set estimated by the COMP algorithm
contains no false negatives, but it may contain some false positives. That is, S(x) ⊆ Sb(x), or in other
words, Sb(x) is a superset estimate of S(x).

7

VI. 2-STAP: A T WO -S TAGE A DAPTIVE P OOLING
In this section, we propose a two-stage sensing algorithm and an associated recovery algorithm which
we collectively refer to as the 2-STAP scheme. The proposed scheme is inspired by the well-known
two-stage Dorfman’s scheme which was originally proposed in the context of group testing but requires
substantially fewer measurements. Translating the Dorfman’s scheme into the language of our work, in
the first stage, the signal coordinates are pooled into a number of disjoint groups of equal size, and one
measurement is made for each pool where all coordinates in the pool are active in the measurement. In
the second stage of Dorfman’s scheme, one measurement is made for every coordinate in a positive pool
(i.e., a pool whose measurement in the first stage is nonzero), and no additional measurements are made
for any negative pool (i.e., a pool whose measurement in the first stage is zero).
The first stage of the 2-STAP scheme is the same as that of Dorfman’s scheme. The second stage,
however, differs from that of Dorfman’s scheme. In particular, unlike the second stage of Dorfamn’s
scheme, in the second stage of the 2-STAP scheme, a number of measurements are made for different
(not necessarily singleton) subsets of coordinates, in each positive pool. Similar to the Dorfman’s scheme,
the second stage of the 2-STAP scheme makes no additional measurements for any negative pools.
In the following, we denote by 1t or 0t an all-one or an all-zero row vector of length t, respectively.
Given a signal x, we partition the n signal coordinates x1 , . . . , xn into q pools of size s = n/q. We
denote by xl the lth pool of coordinates, i.e., xl = [ x(l −1)s+1 , . . . , xls ]T .
(1)

We denote by ml

(2)

and ml

the number of measurements for the lth pool in the first and second
(1)

stage, respectively. Also, we denote by Al

(2)

and Al

the sensing matrix corresponding to the lth pool
(1)

(2)

in the first and second stage, respectively, and denote by Al = [(Al )T , (Al )T ]T the overall sensing
(1)

matrix corresponding to the lth pool. Note that Al
and Al is an ml × s matrix, where ml =

(1)
ml

(1)

is an ml

(2)
+ ml .

(2)

× s matrix, Al

Let m(1) =

(1)
∑l ∈[q] ml

(2)

is an ml

× s matrix,
(2)

and m(2) = ∑l ∈[q] ml .

Note that m(1) and m(2) are the total number of measurements in the first and second stage, respectively.
A. Sensing Algorithm for First Stage
(1)

In the first stage, for each pool l ∈ [q], we make one measurement yl
(1)

and Al

(1)

= 1s · xl . That is, ml

=1

= 1s . Thus, the total number of measurements in the first stage is m(1) = q, and the sensing
(1)

matrix in the first stage, A(1) , is a q × n matrix whose lth row is al

= [ 0 ( l − 1 ) s , 1 s , 0 ( q − l ) s ].

B. Recovery Algorithm for First Stage
(1)

Suppose we observe the noisy measurements zl

(1) (1)

= yl εl

(1)

for l ∈ [q]. Let L = {l ∈ [q] : zl

= 0 }.

Assume, without loss of generality (w.l.o.g.), that L = [q] \ [t] for some 0 ≤ t ≤ q. That is, the first t

8

pools are all positive, and the last q − t pools are all negative.
C. Sensing Algorithm for Second Stage
For any l ∈ [t], we need to make additional measurements for the lth pool in the second stage, i.e.,
(2)

ml

> 0 for all l ∈ [t], because such a pool contains at least one nonzero coordinate. For any l ∈ [q] \ [t],

the lth pool contains only zero coordinates, and we do not need to make any additional measurements
(2)

for any such pool in the second stage, i.e., ml

= 0 for all l ∈ [q] \ [t]. Below, we focus on the positive

pools, i.e., the pools 1, . . . , t.
(2)

Intuitively, the larger is ml , the smaller will be the (conditional) false negative/positive rate, but the
larger will be the average number of measurements. Since it is not known how to theoretically optimize
(2)

(2)

ml , we resort to a heuristic approach to choose ml . In particular, we present two variants of the 2STAP scheme: 2-STAP-I and 2-STAP-II. In 2-STAP-I, for all positive pools, the number of measurements
and the pooling scheme in the second stage will be the same, regardless of the observed measurements
(2)

for these pools in the first stage. That is, ml

is fixed for all l ∈ [t]. On the other hand, in 2-STAP-II,

for each positive pool, the number of measurements and the pooling scheme in the second stage will be
chosen based on the number of nonzero coordinates in xl , denoted by kl . Note that kl may not be known
a priori, but an estimate b
kl of kl can be computed using the observed measurement for the lth pools in
the first stage, as follows.
Let p(k) be the probability that xl has k nonzero coordinates and s − k zero coordinates, and let
(1)

(1)

p( zl |k) be the probability density of zl

(1) (1)

= yl εl

(1)

where yl

= 1s · xl given that xl has k nonzero
(1)

coordinates and s − k zero coordinates. If the sparsity parameter p is known, for any k, p(k) and p( zl |k)
can be computed exactly or approximately (depending on the distribution of values of the nonzero
(1)

coordinates in the signal and the distribution of the noise). In this case, given a noisy measurement zl ,
(1)
a maximum-a-posteriori (MAP) estimate of kl is given by b
kl = argmaxk p(k) p( zl |k). If p is not
1

known, we first compute a maximum-likelihood (ML) estimate pb = 1 − (1 − qt ) s of p, and then use pb,
(1)

instead of p, to first compute p(k) and p( zl |k) for any k, and then compute a MAP estimate b
kl of kl .
(Note that the average number of positive pools is q(1 − (1 − p)s ), and t is a realization of the number
of positive pools. Setting these quantities equal to each other and solving for p, we get the estimate pb
of p, as defined above.)
(2)

(2)

Given ml , the optimal design of Al
(2)

Al

(2)

from the ensemble of all ml

is not known. In this work, for each l ∈ [t], we randomly choose

× s binary matrices (with distinct rows and distinct columns) with a

pre-specified row/column weight profile. The weight profile should be chosen to obtain a good trade-off

9

between the computational complexity (of the sensing and recovery algorithms) and false negative/positive
rates. The weight profile of matrices used in our simulations can be found in Appendix II.

D. Recovery Algorithm for Second Stage
The recovery algorithm in the second stage consists of three steps: COMP algorithm, MAP decoding,
and list generation.
(2)

For each l ∈ [t], suppose the noisy measurement vector zl
(1)

(2)

(2) (2)

= yl εl

(2)

is observed, where yl

=

(2)

Al xl . Let zl = [ zl , (zl )T ]T be the overall noisy measurement vector corresponding to the lth pool.
(Note that zl is a vector of length ml and xl is a vector of length s.) We will estimate the support set Sl
of xl as follows.
1) COMP Algorithm: First, we use the COMP algorithm to find a (superset) estimate Sbl of Sl . In
particular, the COMP algorithm recovers Sbl from zl given Al . Let Il = {i ∈ [ml ] : (zl )i = 0}, where

(zl )i denotes the ith coordinate in zl . We denote by x∗l the sub-vector of xl restricted to the coordinates
indexed by Sbl ; denote by A∗l the sub-matrix of Al restricted to the rows indexed by [ml ] \ Il and the
columns indexed by Sbl ; and denote by z∗ the sub-vector of zl restricted to the coordinates indexed by
l

[ml ] \ Il . Let

m∗l

= | Il | and

s∗l

= | Sbl |. In the next step, we will estimate the support set S∗l of x∗l from

z∗l , given A∗l .
2) MAP Decoding: Given the estimate b
kl of kl (the number of nonzero coordinates in x∗l ), let kmin =
max{b
kl − 1, 1} and kmax = min{b
kl + 1, s∗l }. For any kmin ≤ k ≤ kmax , and for any k-subset T of Sbl ,
we compute
f (T ) =

max

b
x∗l is T
x∗l : support set of b

p(b
x∗l |z∗l )

x∗l given z∗l is maximum.
by finding b
x∗l with support set T such that the conditional probability density of b
Maximizing p(b
x∗l |z∗l ) is equivalent to maximizing p(b
x∗l ) p(z∗l |b
x∗l ) = ∏ j∈[s∗ ] p((b
x∗l ) j ) ∏i∈[m∗ ] p((z∗l )i |b
x∗l ),
l
l
where (b
x∗l ) j denotes the jth coordinate in b
x∗l . For any b
x∗l , p((b
x∗l ) j ) = (1 − p)δ ((b
x∗l ) j ) + p × p x ((b
x∗l ) j ),
where δ ( x) is the Dirac delta function, and p((z∗l )i |b
x∗l ) = pε ((z∗l )i /((A∗l )i b
x∗l )), where (A∗l )i denotes
the ith row of A∗l . Thus, if the sparsity parameter p is known, f ( T ) (for any T) can be approximated by
solving a (potentially non-linear and/or non-convex) optimization problem (depending on the distributions
p x and pε ) numerically. (In our simulations, the “fmincon” function in MATLAB was used to compute
an approximation of f ( T ).) If p is not known, f ( T ) can be approximated similarly as above, except
using the ML estimate pb everywhere, instead of p.
3) List Generation: Let f ∗ = max T f ( T ) where the maximization is over all T defined as above. We
find all T, say T1 , T2 , . . . , T` , such that f ( T ) ≥ α · f ∗ for a given 0 < α ≤ 1, and use T1 ∪ T2 ∪ . . . ∪ T`

10

as the estimate of the support set S∗l of x∗l . Note that the larger is the threshold α, the smaller will be
the (conditional) average false negative rate and the larger will be the (conditional) average false positive
rate.
VII. 2-STAMP: A T WO -S TAGE A DAPTIVE M IXED P OOLING
The proposed scheme in this section, which we refer to as the 2-STAMP scheme, is a generalization of
the 2-STAP scheme. The first stage of the 2-STAMP scheme is the same as that in the 2-STAP scheme,
but in the second stage we make measurements on mixtures of positive pools together, instead of making
measurements on separate pools only. For the ease of exposition, we explain a special case of the 2STAMP scheme when up to two pools can be mixed together. The same idea can be easily extended for
mixing larger number of pools.
The main idea behind mixing pools in the second stage is as follows. Consider two positive pools that
we expect to contain a relatively small number of nonzero coordinates. By mixing these pools together
and sensing the mixed pool altogether (instead of sensing the pools individually), we can save a few
measurements while maintaining the implementation/computational complexity of both the sensing and
recovery algorithms affordable. However, the rest of the pools that are expected to contain a relatively
large number of nonzero coordinates will be sensed individually, so as to avoid the sensing and/or recovery
algorithms to become too complex implementation-wise or computationally.

A. Sensing Algorithm for Second Stage
(1)

At the end of the first stage, suppose the noisy measurements zl
Let L = {l ∈ [q] :

(1)
zl

(1) (1)

= yl εl

for l ∈ [q] are observed.

= 0}. Similarly as before, w.l.o.g., assume that L = [q] \ [t] for some

0 ≤ t ≤ q. Let b
k1 , . . . , b
kt be our estimates of k1 , . . . , kt , where kl is the number of nonzero coordinates
k1 ≥ b
k2 ≥ . . . ≥ b
kt . For a given integer κ ≥ 1, we partition
in the lth pool. Assume, w.l.o.g., that b
the t positive pools as follows: (i) find τ = max{l ∈ [t] : b
kl > κ }, and (ii) construct the partition

{1}, . . . , {τ }, {τ + 1, τ + 2}, {τ + 3, τ + 4}, . . . . The last part is either {t − 1, t} or {t} depending
on whether t − τ is even or odd. Let r = d t+2τ e be the number of parts in the partition. For the ease
of exposition, we assume that the last part is {t − 1, t}. We define r mixed pools x̃1 , . . . , x̃r , where
T
T
x̃l = xl for all l ∈ [τ ] and x̃l = [xT
2l −τ −1 , x2l −τ ] for all l ∈ [r ] \ [τ ]. (Note that the lth pool has

size s or 2s for l ∈ [τ ] or l ∈ [r] \ [τ ], respectively.) Also, we define the r noisy measurement vectors
(1)

(1)

(1)

(corresponding to the first stage) for the mixed pools by z̃1 , . . . , z̃r , where z̃l
and

(1)
z̃l

=

(1)
(1)
[ z2l −τ −1 , z2l −τ ]T

for all l ∈ [r] \ [τ ].

11

(1)

= zl

for all l ∈ [τ ]

(2)

For each l ∈ [τ ], we choose ml

(the number of measurements on the lth mixed pool in the second

(2)
stage) based on b
kl and we choose Al (the sensing matrix for the lth mixed pool in the second stage)
(2)

(2)

based on ml , the same as that in the 2-STAP-II scheme. For each l ∈ [r] \ [τ ], ml

is chosen based on

(2)
(2)
both b
k2l −τ −1 and b
k2l −τ , and Al is chosen based on ml , the same way as in the 2-STAP-II scheme,
(2)

expect from an ensemble of binary matrices of size ml

(2)

× 2s, instead of ml

× s.

B. Recovery Algorithm for Second Stage
The recovery algorithm in the second stage follows the same procedure as the one in the 2-STAP
scheme, except that in this scheme we estimate the support set of each mixed pool (rather than estimating
the support set of each pool individually). In particular, for each mixed pool l ∈ [τ ] (which is essentially
an individual pool), the recovery algorithm remains the same as that in the 2-STAP scheme. For each
mixed pool l ∈ [r] \ [τ ], we compute f ( T ) for all subsets T = T1 ∪ T2 of Sbl (the support set estimated
by the COMP algorithm for the lth mixed pool) such that T1 is a subset of Sbl ∩ {1, . . . , s} of size
between max{b
k2l −τ −1 − 1, 1} and min{b
k2l −τ −1 + 1, | Sbl ∩ {1, . . . , s}|}, and T2 is a subset of Sbl ∩

{s + 1, . . . , 2s} of size between max{bk2l −τ − 1, 1} and min{bk2l −τ + 1, | Sbl ∩ {s + 1, . . . , 2s}|}.
VIII. C OMPARISONS B ETWEEN THE P ROPOSED S CHEMES AND TAPESTRY
The key differences between the proposed schemes and the Tapestry scheme are listed below:
•

Tapestry is a single-stage scheme, and hence all measurements can be made in parallel. The proposed
schemes are, however, two-stage schemes; and notwithstanding that all measurements in each stage
can be made in parallel, the measurements in the second stage can only be made after those in the
first stage.

•

When compared to Tapestry, in the tested cases, the proposed schemes achieve a better tradeoff
between the average number of measurements and the (conditional) average false negative/positive
rate. This comes from two facts: (i) the sensing algorithm of the proposed schemes is more flexible
than that of the Tapestry scheme. This is because the total number of measurements in the latter can
vary for different signal realizations, whereas the former is oblivious to different signal realizations
and uses the same number of measurements always; and (ii) the measurements in the proposed
schemes are localized to small pools. This makes it possible to implement recovery algorithms that
are carefully designed for the multiplicative noise model with reasonable computational complexity.
In contrast, the recovery algorithms discussed in the Tapestry scheme were borrowed from the CS
literature where the noise model is assumed to be additive. Unlike the recovery algorithms used for

12

Tapestry, the recovery algorithm proposed in this work also takes into account the signal and noise
distributions and the sparsity parameter or its estimate.
•

The sensing algorithm of the proposed schemes can potentially have a substantially lower computational and implementation complexity than that of the Tapestry scheme. This follows from two
facts: (i) the total number of nonzero entries in the overall sensing matrix of the proposed schemes
can be much smaller than that in the Tapestry scheme; and (ii) the nonzero entries in each row of
the sensing matrix in the Tapestry scheme are spread out everywhere, whereas the nonzero entries
in the sensing matrix of the proposed schemes are localized in each row. In particular, in the first
stage each measurement is localized to a pool of consecutive coordinates, and the measurements in
the second stage for each pool are over the coordinates in that pool only.
IX. S IMULATION R ESULTS

In this section, we present our simulation results. As a case study, in these simulations, we have
considered a population of n = 961 people to be tested for COVID-19 and assumed that the prevalence
is p = 0.01. (In the simulations, we considered both cases where p is known or unknown a priori,
and we did not observe any significant difference in the performance of either of the proposed schemes,
2-STAP and 2-STAMP.) For both the proposed schemes, we have considered pooling the population into
q = 31 pools, each of size s =

n
q

= 31, in the first stage. Three different values of number of infected

people in the population (k), namely k ∈ {5, 10, 15}, have been considered. For every k, we performed
100 Monte-Carlo simulations, where the statistical models used for viral load and measurement noise
were obtained from [11]. In particular, in each simulation trial, a signal of length 961 was generated
(independently from other trials) as follows: k coordinates were randomly chosen to take a nonzero value
from the interval [1, 1000] according to a (continuous) uniform distribution, and the remaining n − k
coordinates were all set to value zero. In the simulations, the measurement noise ε was also assumed to
have a log-normal distribution with parameters µε = 0 and σε = 0.1 ln(1.95).
Table I summarizes our results for the proposed 2-STAP (both variants) and 2-STAMP schemes and
the results for the Tapestry scheme for the same problem model (i.e., the same population size and
the same viral load and noise distributions) where each measurement is made on a pool of 31 people
(see [11]). In this table, mmin , mmax , mstd , and mave represent the minimum, maximum, standard
deviation, and the average of the number of measurements used in 100 simulations, respectively. The
sensitivity and specificity results are rounded to two decimal places, for fair comparison with the results
reported in [11]. More detailed results for the 2-STAP-I, 2-STAP-II, and 2-STAMP schemes are presented
in Tables II, III, and IV, respectively. In these tables, (i) α is the threshold used in the list generation

13

TABLE I
P ERFORMANCE RESULTS FOR THE TAPESTRY SCHEME [11] AND THE PROPOSED 2-STAP AND 2-STAMP

SCHEMES

( SENSITIVITY AND SPECIFICITY RESULTS ARE ROUNDED TO TWO DECIMAL PLACES )

k

mmin

mmax

mstd

mave

93

93

0

93

5

Conditional

Conditional

Sensitivity

Specificity

Tapestry + COMP + NN-LASSO [11]

1.00

1.00

Tapestry + COMP + NN-OMP [11]

1.00

1.00

Tapestry + COMP + SBL [11]

1.00

1.00

Pooling Scheme

49

61

2.94

59.08

2-STAP-I

1.00

1.00

47

57

2.26

54.55

2-STAP-II

1.00

1.00

46

55

2.25

52.56

2-STAMP

1.00

1.00

Tapestry + COMP + NN-LASSO [11]

0.98

0.99

Tapestry + COMP + NN-OMP [11]

0.96

1.00

Tapestry + COMP + SBL [11]

0.99

0.99

93

93

0

93

10
73

91

5.54

82.96

2-STAP-I

1.00

1.00

66

81

4.18

74.98

2-STAP-II

0.99

1.00

63

76

3.74

70.86

2-STAMP

1.00

1.00

Tapestry + COMP + NN-LASSO [11]

0.94

0.97

Tapestry + COMP + NN-OMP [11]

0.86

0.99

Tapestry + COMP + SBL [11]

0.98

0.97

93

93

0

93

15
85

121

7.31

103.30

2-STAP-I

0.98

0.99

78

106

5.65

92.66

2-STAP-II

0.98

0.99

74

99

5.02

86.85

2-STAMP

0.99

0.99

step of the recovery algorithm (see Section VI-D3), which controls the trade-off between sensitivity and
specificity; and (ii) the conditional negative (or positive) predictive value is the average of the ratio of the
number of true negatives (or positives) to the total number of true and false negatives (or positives). Note
that the negative predictive value represents the probability that a person is truly non-infected given that
the recovery algorithm identifies them as non-infected, and the positive predictive value represents the
probability that a person is truly infected given that the recovery algorithm identifies them as infected.
Comparing the results of Tapestry and the two variants of 2-STAP in Table I, it can be seen that
for k ∈ {5, 10}, 2-STAP-I requires smaller number of measurements on the average for the same (or
even higher) sensitivity and specificity (see also Table II). For k = 15, 2-STAP-I uses about 10 more
measurements than Tapestry on the average, but it achieves a substantially higher specificity by about
2% for almost the same sensitivity (see also Table II). It can also be seen that for all k ∈ {5, 10, 15},

14

TABLE II
D ETAILED PERFORMANCE RESULTS FOR THE 2-STAP-I SCHEME

k

5

10

15

mmin

49

73

85

mmax

61

91

121

mstd

2.94

5.54

7.30

mave

59.08

82.96

103.30

α

Conditional
Sensitivity

Conditional
Specificity

Conditional Negative
Predictive Value

Conditional Positive
Predictive Value

0.943

0.9920

0.9995

1.0000

0.9495

0.926

0.9960

0.9994

1.0000

0.9492

0.893

0.9980

0.9993

1.0000

0.9374

0.943

0.9810

0.9988

0.9998

0.9225

0.909

0.9930

0.9982

0.9999

0.8980

0.885

0.9950

0.9978

0.9999

0.8853

0.847

0.9820

0.9939

0.9997

0.7709

0.758

0.9893

0.9885

0.9998

0.6475

0.649

0.9940

0.9817

0.9999

0.5290

TABLE III
D ETAILED PERFORMANCE RESULTS FOR THE 2-STAP-II SCHEME

k

mmin

mmax

mstd

mave

5

47

57

2.26

54.55

10

15

66

78

81

106

4.18

5.65

74.98

92.66

α

Conditional
Sensitivity

Conditional
Specificity

Conditional Negative
Predictive Value

Conditional Positive
Predictive Value

0.813

0.9920

0.9986

0.9999

0.8259

0.794

0.9940

0.9983

1.0000

0.8026

0.787

0.9980

0.9982

1.0000

0.7953

0.943

0.9810

0.9987

0.9998

0.9074

0.794

0.9930

0.9954

0.9999

0.7471

0.730

0.9950

0.9927

0.9999

0.6559

0.787

0.9820

0.9903

0.9997

0.6699

0.671

0.9893

0.9812

0.9998

0.5052

0.515

0.9940

0.9724

0.9999

0.4109

2-STAP-II can provide higher sensitivity and higher specificity than Tapestry with even smaller (average)
number of measurements. For instance, for the case of k = 10 infected people, with an average number
of measurements about 75, 2-STAP-II can achieve a sensitivity of 99.30% and a specificity of 99.54%,
see Table III. However, using Tapestry, for k = 10, one can achieve a sensitivity and a specificity between
98.50% and 99.49% with 93 measurements (about 20% more measurements than that in 2-STAP-II).
These improvements in the performance are mainly due to the fact that 2-STAP is an adaptive scheme
(although with a very small degree of adaptivity, i.e., using only one round of feedback), whereas Tapestry

15

TABLE IV
D ETAILED PERFORMANCE RESULTS FOR THE 2-STAMP

k

5

10

15

mmin

46

63

74

mmax

55

76

99

mstd

2.25

3.74

5.02

mave

52.56

70.86

86.85

SCHEME

α

Conditional
Sensitivity

Conditional
Specificity

Conditional Negative
Predictive Value

Conditional Positive
Predictive Value

0.893

0.9920

0.9989

1.0000

0.8698

0.877

0.9940

0.9986

1.0000

0.8473

0.847

0.9980

0.9978

1.0000

0.7832

0.980

0.9800

0.9990

0.9998

0.9324

0.935

0.9930

0.9981

0.9999

0.8830

0.885

0.9950

0.9962

0.9999

0.7955

0.909

0.9813

0.9945

1.0000

0.9010

0.877

0.9900

0.9916

1.0000

0.8474

0.840

0.9947

0.9847

1.0000

0.7736

is a non-adaptive scheme. In particular, identifying all negative pools (which contain a relatively large
fraction of population for sufficiently low prevalence) at the end of the first stage and using a relatively
small number of additional measurements only for each positive pool in the second stage enable us to
achieve a better trade-off between average number of measurements, sensitivity, and specificity.
As can be seen in Table I, 2-STAMP can achieve a sensitivity and a specificity higher than those
attainable with 2-STAP-I and 2-STAP-II, with even smaller average number of measurements. For
instance, for k = 10, with only about 71 (< 75 in 2-STAP-II) measurements on average, 2-STAMP can
achieve a sensitivity of 99.30% (the same as that in 2-STAP-II) and a specificity of 99.81% (> 99.54%
in 2-STAP-II), see Table III. The advantage of 2-STAMP over both variants of 2-STAP comes from the
saving in the number of measurements in the second stage. In particular, in 2-STAMP, mixing small
groups (namely, groups of size two) of pools with small number of infected people gives rise to an
opportunity for making a smaller number of measurements on the mixed super-pool (as compared to the
total number of measurements used in 2-STAP for all pools in the mix) without compensating the overall
accuracy.
R EFERENCES
[1] R. Dorfman, “The detection of defective members of large populations,” The Annals of Mathematical Statistics, vol. 14,
no. 4, pp. 436–440, 1943.
[2] J. Emmanuel, M. Bassett, H. Smith, and J. Jacobs, “Pooling of sera for human immunodeficiency virus (HIV) testing: an
economical method for use in developing countries.” Journal of clinical pathology, vol. 41, no. 5, pp. 582–585, 1988.

16

[3] J. L. Lewis, V. M. Lockary, and S. Kobic, “Cost savings and increased efficiency using a stratified specimen pooling
strategy for chlamydia trachomatis and neisseria gonorrhoeae,” Sexually Transmitted Diseases, vol. 39, no. 1, pp. 46–48,
2012.
[4] T. T. Van, J. Miller, D. M. Warshauer, E. Reisdorf, D. Jernigan, R. Humes, and P. A. Shult, “Pooling nasopharyngeal/throat
swab specimens to increase testing capacity for influenza viruses by PCR,” Journal of clinical microbiology, vol. 50, no. 3,
pp. 891–896, 2012.
[5] C. R. Bilder, P. C. Iwen, B. Abdalhamid, J. M. Tebbs, and C. S. McMahan, “Tests in short supply? try group testing,”
Significance (Oxford, England), vol. 17, no. 3, p. 15, 2020.
[6] N. Shental, S. Levy, S. Skorniakov, V. Wuvshet, Y. Shemer-Avni, A. Porgador, and T. Hertz, “Efficient high throughput
SARS-CoV-2 testing to detect asymptomatic carriers,” medRxiv, 2020.
[7] I. Yelin, N. Aharony, E. Shaer-Tamar, A. Argoetti, E. Messer, D. Berenbaum, E. Shafran, A. Kuzli,
N. Gandali, T. Hashimshony, Y. Mandel-Gutfreund, M. Halberthal, Y. Geffen, M. Szwarcwort-Cohen, and
R. Kishony, “Evaluation of covid-19 RT-qPCR test in multi-sample pools,” medRxiv, 2020. [Online]. Available:
https://www.medrxiv.org/content/early/2020/03/27/2020.03.26.20039438
[8] C. A. Hogan, M. K. Sahoo, and B. A. Pinsky, “Sample pooling as a strategy to detect community transmission of SARSCoV-2,” Jama, vol. 323, no. 19, pp. 1967–1969, 2020.
[9] D. Du and F. K. Hwang, Combinatorial group testing and its applications.

World Scientific, 2000, vol. 12.

[10] M. Aldridge, O. Johnson, and J. Scarlett, Group Testing: An Information Theory Perspective.

now, 2019.

[11] S. Ghosh, A. Rajwade, S. Krishna, N. Gopalkrishnan, T. E. Schaus, A. Chakravarthy, S. Varahan, V. Appu, R. Ramakrishnan,
S. Ch et al., “Tapestry: A single-round smart pooling technique for COVID-19 testing,” medRxiv, 2020.
[12] A. Nalci, I. Fedorov, M. Al-Shoukairi, T. T. Liu, and B. D. Rao, “Rectified gaussian scale mixtures and the sparse
non-negative least squares problem,” IEEE Transactions on Signal Processing, vol. 66, no. 12, pp. 3124–3139, 2018.

A PPENDIX I
D ETAILED E XPLANATION ABOUT M ULTIPLICATIVE N OISE M ODEL
The multiplicative noise model is inspired by the current RT-qPCR technology for COVID-19 testing.
To keep the discussion simple, suppose that an individual person is to be tested for COVID-19 by using
this technology. The sample collected from the person to be tested is dispersed into a liquid medium, and
the reverse transcription (RT) process is applied to convert the RNA molecules of the SARS-CoV-2 virus
(the coronavirus that causes COVID-19) in the liquid (if the person is infected) into cDNA. Followed by
adding primers that are complementary to the cDNA of the viral genome, these primers attach themselves
to the cDNA of the viral genome, and together they undergo an exponential amplification process by
the RT-qPCR machine [11]. This process consists of a maximum of Cmax cycles. The output of the
RT-qPCR process is the cycle count C after which the concentration of DNA exceeds a pre-specified
threshold Dmin or Cmax . The thresholds Dmin and Cmax are often chosen so that: (i) if a person is not
infected, the DNA concentration does not exceed Dmin over the course of Cmax cycles, and (ii) if a
person is infected, the DNA concentration exceeds Dmin at some point, say cycle C, over the course of

17

Cmax cycles. Note that for a fixed Dmin , the larger is the viral load of an infected person, the smaller
C will be. Ideally, the concentration of DNA molecules is doubled in every cycle, i.e., after C cycles
the concentration of DNA molecules is x2C , where x is the viral load of the person to be tested. In
reality, however, the amplification process may not be ideal. To reflect the randomness in the process, we
use the same model as the one suggested in [11] and assume that the concentration of DNA molecules
after C cycles is given by xbC+∆ for some positive constant b (close to 2), where x is the viral load
of the person to be tested, and ∆ is a Gaussian random variable with mean zero and variance σ∆2 . The
multiplicative term ε = b∆ can be viewed as the noise in the amplification process. The random variable
ε has a log-normal distribution with parameters µε = 0 and σε = σ∆ ln b. Note that the closer are b to 2
and σ∆ to 0, the weaker will be the noise and the closer will be the process to ideal. For any x > 0, let
Cx be the number of cycles it takes for the concentration of DNA molecules to be approximately equal
to Dmin . That is, xbCx +∆ ≈ Dmin . The cycle count measurement Cx can be converted to an equivalent
measurement z given by z = Dmin b−Cx . It can then be seen that z = xε gives us the measurement
model with multiplicative noise in (1).
A PPENDIX II
OTHER R ECOVERY A LGORITHMS IN [11]
Recall the index set I = {i ∈ [m] : zi = 0} of zero measurements and the superset estimate Sb(x)
b
of S(x) from the COMP algorithm. For simplifying the notation, denote S(x) and Sb(x) by S and S,
b denote
respectively. Also, denote by x∗ the sub-vector of x restricted to the coordinates indexed by S;
b
by A∗ the sub-matrix of A restricted to the rows indexed by [m] \ I and the columns indexed by S;
and denote by z∗ the sub-vector of z restricted to the coordinates indexed by [m] \ I. Let m∗ = | I | and
n∗ = | Sb|. Note that x∗ is a vector of length n∗ , A∗ is an m∗ × n∗ matrix, and z∗ is a vector of length
m∗ .
The rest of the recovery algorithms discussed in [11] consist of two phases. In the first phase, all of
these algorithms use COMP for an initial signal-support recovery and reduce the instance (x, A, z) to the
instance (x∗ , A∗ , z∗ ), and in the second phase, each of these algorithms employs a different technique
for signal recovery. The second phase of these algorithms are briefly described as follows.
For simplifying the notation, in the following we omit the superscript “∗”, and denote x∗ , A∗ , z∗ , m∗ , n∗
by x, A, z, m, n, respectively.
Non-negative LASSO (NN-LASSO): NN-LASSO is a sparse signal recovery technique to compute an
estimate b
x of x by trying to minimize the L2 -norm of the residual (additive) error vector z − Ab
x, i.e.,

kz − Ab
xk2 , subject to (i) a sparsity constraint on b
x – imposed by an upper bound on the L1 -norm of b
x,
18

i.e., kb
xk1 ≤ λ, and (ii) a non-negativity constraint on all coordinates in b
x. The choice of the threshold
λ depends on the sensing matrix A, the noisy measurement vector z, and the distribution of the noise ε
(for details, see [11]).
Non-negative Orthogonal Matching Pursuit (NN-OMP): NN-OMP is an iterative greedy technique for
non-negative sparse signal recovery that finds an estimate b
x of x by seeking to minimize the L0 -norm
of b
x, i.e., kb
xk0 , subject to (i) a constraint on the L2 -norm of the residual (additive) error vector z − Ab
x,
i.e., kz − Ab
xk2 ≤ , and (ii) a non-negativity constraint on all coordinates in b
x. Followed by initializing
r(0) by z and Sb(0) by the empty set, in each iteration l, the algorithm first updates the support set
Sb(l ) = Sb(l −1) ∪ { j(l ) }, where the j(l ) th column of A has the maximum correlation with the residual
error vector r(l −1) . That is, j(l ) = argmax j∈[n] ATj r(l −1) , where A j denotes the jth column of A. Next,
the algorithm updates the residual error vector r(l ) = z − Ab
x, where b
x minimizes kz − Ab
xk2 subject to
xbj ≥ 0 for all j ∈ Sb(l ) , and xbj = 0 for all j ∈ [n] \ Sb(l ) . Once the stopping criterion kz − Ab
xk2 ≤ 
is satisfied, the algorithm terminates and returns b
x as an estimate of x. The choice of the threshold 
depends on A, z, and the distribution of ε (see [11]).
Sparse Bayesian Learning (SBL): In SBL, for an estimate b
x of x, the jth coordinate in b
x is assumed
to be an independent Gaussian random variable with mean zero and variance σ 2j , and the coordinates of the residual (additive) error vector z − Ab
x are assumed to be independent Gaussian random
variables with mean zero and variance σ 2 . Under these assumptions, the likelihood of b
x is given by
1

2 b2
b
p(b
x; {σ j }) = ∏ j∈[n] (2πσ 2j )− 2 exp(− 21 σ −
j x j ), and the conditional likelihood of z given x is given
m

by p(z|b
x; σ ) = (2πσ 2 )− 2 exp(− 12 σ −2 kz − Ab
xk22 ). Let I be an m × m identity matrix, and let Σ z =
σ 2 I + A diag({σ 2j })AT , where diag({v j }) is a square diagonal matrix such that the ( j, j)th entry is v j .
Marginalizing the joint distribution p(z, b
x; {σ j }, σ ) = p(b
x; {σ j }) p(z|b
x; σ ), the likelihood of z is given
m

1

1
by p(z; {σ j }, σ ) = (2π )− 2 |Σ z |− 2 exp(− 12 zT Σ−
z z ). SBL uses the Expectation-Maximization (EM)

algorithm, which is an iterative technique, to find {σ j } and σ that maximize p(z; {σ j }, σ ). Followed by
(0)

(l )

initializing {σ j } by {σ j } and σ by σ (0) , in each iteration l, the EM algorithm updates σ j

and σ (l )

as follows:
(l )

σj


1
2
= (Σ x ) j, j + xb2j
1


σ

(l )

=m

− 12

kz −

2

Ab
xk22

+ (σ

(l −1) 2

)

∑

j∈[n]



1−

(l −1) −2
(σ j
) (Σ x ) j, j



(l −1) −2
) }))−1

where (Σ x ) j, j is the ( j, j)th entry in the matrix Σ x = ((σ (l −1) )−2 AT A + diag({(σ j

and

b
x = (σ (l −1) )−2 Σ x AT z. The iterations continue until convergence of b
x. (The EM algorithm is guaranteed
to converge to a fixed point, which may or may not be a local optimum.) Upon convergence, the algorithm

19

terminates and returns b
x. Since b
x may have some negative coordinates, as suggested in [11] one can use

[max{ xb1 , 0}, . . . , max{ xbn , 0}]T as an estimate of x = [ x1 , . . . , xn ]T . A smarter yet more elaborate
approach to impose the non-negativity constraint as part of the optimization problem is to use algorithms
such as Rectified SBL [12] that assume the coordinates in b
x follow a different distribution than the
Gaussian distribution used in SBL.
A PPENDIX III
N UMBER OF M EASUREMENTS AND S ENSING M ATRICES U SED IN S IMULATIONS
(2)

For the 2-STAP-I scheme, for each positive pool l, ml

= 6 measurements were used in the second

stage, and the pooling matrix used in the simulations can be found in Fig. 1.

16
17
17
17
18
19

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

1
0
0
1
1
0

0
1
0
1
0
1

0
1
1
0
0
1

0
0
1
1
0
1

1
1
1
0
0
0

1
1
0
0
1
0

0
1
1
0
1
0

1
0
1
0
0
1

1
1
0
0
0
1

1
0
1
1
0
0

1
0
0
0
1
1

0
1
0
0
1
1

0
1
0
1
1
0

0
0
1
0
1
1

1
0
0
1
0
1

0
0
1
1
1
0

0
1
1
1
0
0

0
0
0
1
1
1

1
0
1
0
1
0

1
1
0
1
0
0

1
0
1
1
0
1

1
0
1
0
1
1

1
1
1
0
0
1

1
0
0
1
1
1

1
1
0
1
1
0

0
1
0
1
1
1

0
0
1
1
1
1

0
1
1
1
0
1

1
1
0
0
1
1

0
1
1
0
1
1

0
1
1
1
1
0

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

4

Fig. 1. The sensing matrix with 6 rows and 31 columns used in the simulations for the 2-STAP-I scheme. (The number of 1’s
in each row is shown in front of the row, and the number of 1’s in each column is shown below the column.)
(2)

For the 2-STAP-II scheme, the number of measurements ml

used in the second stage for each positive

pool l was chosen as follows (depending on the estimate b
kl of the number of infected people in pool l):



5, b
kl = 1,






6, b
kl = 2,
(2)
ml =


7, b
kl = 3,





8, b
kl ≥ 4.
(2)

(2)

For each ml , the column and row weight distributions of the ml
the simulations are described below (see Fig. 2):
•

(2)

ml

= 5:

– 1, 5, 10, 10, 5 columns of weights 0, 1, 2, 3, 4, respectively;
– 5 rows of weight 15.
•

(2)

ml

= 6:

– 16, 15 columns of weights 3, 4, respectively;

20

(2)

× 31 sensing matrix Al

used in

– 6 rows of weight 18.
•

(2)

ml

= 7:

– 16, 15 columns of weights 3, 4, respectively;
– 4, 3 rows of weights 15, 16, respectively.
•

(2)

ml

= 8:

– 16, 15 columns of weights 3, 4, respectively;
– 4, 4 rows of weights 13, 14, respectively.
For the 2-STAMP scheme, we used the mixing sparsity threshold κ = 2. The number of measurements
(2)
ml

used in the second stage for each positive pool l such that b
kl ≥ 3 was chosen the same as that in
(2)

Scheme I, i.e., ml

(2)
= 7 for bkl = 3, and ml = 8 for bkl ≥ 4. For each (mixed) super-pool l – formed
(2)

by combining two positive pools l1 and l2 – the number of measurements ml

used in the second stage

was chosen as follows (depending on both b
kl1 and b
kl2 ):



9,
(bkl1 , bkl2 ) = (1, 1),



(2)
ml = 10, (b
kl1 , b
kl2 ) = (2, 1),





11, (b
kl1 , b
kl2 ) = (2, 2).
(2)

(2)

For each ml , the column and row weight distributions of the ml
the simulations are described below (see Fig. 3):
•

(2)

ml

= 9:

– 31, 31 columns of weights 3, 4, respectively;
– 1, 6, 2 rows of weights 23, 24, 25, respectively.
•

(2)

ml

= 10:

– 31, 31 columns of weights 3, 4, respectively;
– 4, 5, 1 rows of weights 21, 22, 23, respectively.
•

(2)

ml

= 11:

– 31, 31 columns of weights 3, 4, respectively;
– 5, 4, 2 rows of weights 19, 20, 21, respectively.

21

(2)

× 62 sensing matrix Al

used in

15
15
15
15
15
15
15
15
15
15
15
15
15
15
15

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

0
1
0
00
010
000
000
00
00
01

1
2
0
10
020
010
001
00
10
02

0
3
1
00
130
000
011
00
10
03

1
4
1
10
140
010
012
00
20
04

0
5
0
01
050
100
001
01
10
05

1
6
0
11
060
110
002
01
20
06

0
7
1
01
170
100
012
01
20
07

1
8
1
11
180
110
013
01
30
08

0
9
0
00
091
000
101
00
11
09

1
10
121
010
110
003
01
30
20

1
10
130
011
010
103
00
31
30

0
11
040
110
001
013
10
30
41

0
01
051
101
100
113
01
31
50

0
01
060
101
001
113
10
31
61

1
01
170
100
011
013
10
30
71

0
10
081
010
101
003
11
30
81

0 1 1 1 0 0 0 1 1
11 00 11 00 10 00 01 10 00
090 10
11 11
10 12
10 13
01 14
01 15
01 16
10 17
11
1
0
1
0
1
0
0
11 01 10 01 01 01 10 010 001
000 110 010 011 100 101 101 011 111
113 103 013 103 103 103 013 003 104
00 01 00 10 01 11 11 10 11
1 0
31 31 30 31 31 3
3 0
3 1
4
0
0
0
1
0
1
1
1
9 10 11 12 13 14 15 16 171

1
01
18
11
101
110
114
01
14
180

0
10
021
010
100
001
013
10
30
1

0
01
031
100
101
010
113
00
31
0

0
11
041
110
100
010
013
00
30
0

1
11
150
110
010
010
003
00
30
0

0
10
060
011
000
101
003
11
30
1

1
00
170
000
011
001
103
10
31
1

1
00
180
001
010
101
003
11
30
1

1
00
191
001
110
100
013
01
30
0

0
01
10
01
101
100
110
013
01
30
0

0
00
11
01
001
101
100
113
01
31
0

0
11
12
00
110
000
011
003
10
30
1

1
10
13
10
011
010
100
003
01
30
0

0
00
14
00
001
001
101
103
11
31
1

1
01
15
10
100
011
010
103
00
1
3
0

0
10
16
01
010
101
000
113
00
1
3
0

0
00
17
01
001
101
101
114
11
14
1

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

4

18 00
18 11
18 0
18 1
18 11
18
18 01
18
18 1
18 10
18
18 10
18
18
013
18
18 01
18 0
3
18 10
15 13
15 10
15
15 10
1
15
15 0
15 10
15
16 0
15 00
15
16 0
15 01
16
16 0
15 1
16 10
16 3
16 10
16 1
3
16 1
3

1
0
10
10
01
010
102
00
21
010

1 0 1 0 1 0 1 0
1 0 0 1 1 0 0 1
10 01 11 01 11 00 10 00
12
11 13
01 14
01 15
11 16
11 17
00 18
00 19
10
1
0
1
0
1
0
00 10 10 10 10 01 011 001
113 102 103 113 114 001 002 012
00 01 01 01 01 10 10 10
1 311
1
02 02
31 (a)21 5 ×
3
3 sensing
4 0
1 matrix
012 013 014 015 016 117 118 119

1
1
10
20
10
011
013
10
03
120

0
0
01
21
00
101
002
11
02
121

1
0
11
22
00
1
1
01
0
3
1
1
03
122

0
1
01
23
10
0
11
1
03
1
1
03
123

1
1
11
24
10
1
11
1
04
1
1
04
124

0
0
00
25
0
001
011
2
01
12
125

1
0
10
26
0
101
011
3
01
13
126

0
1
00
27
1
001
111
3
01
13
127

1
1
10
28
1
101
111
4
01
14
128

0
0
01
29
0
011
011
3
11
13
129

1
0
11
30
0
111
011
4
11
14
130

0
1
01
31
1
011
111
4
11
14
131

1
01
19
10
101
011
114
10
14
1
19

1
01
20
11
100
111
014
11
04
1
20

1
10
21
11
011
110
104
01
14
0
21

1
11
22
11
1
1
10
1
10
0
14
0
04
0
22

0
11
23
01
1
11
0
10
1
14
1
0
14
0
23

1
10
24
11
1
00
1
11
0
04
1
1
04
1
24

0
01
25
0
011
011
111
114
14
1
25

0
11
26
0
111
010
101
114
04
1
26

1
10
27
1
100
101
011
014
14
1
27

0
11
28
0
110
001
111
014
14
1
28

1
11
29
1
110
101
110
004
14
0
29

0
10
30
0
101
011
011
114
14
1
30

1
11
31
1
110
100
101
014
04
1
31

0
01
18
01
101
101
110
114
01
14
0

1
10
19
11
010
110
001
014
10
04
1

1
00
20
10
001
011
101
104
11
14
1

0
11
21
01
110
100
011
014
10
04
1

1
00
22
11
0
0
10
1
01
0
11
1
04
1
14
1

0
11
23
00
1
1
01
0
11
1
00
1
14
0
14
0

1
01
24
10
0
11
1
00
1
11
0
04
1
1
04
1

1
10
25
1
101
111
010
100
104
04
0

1
01
26
1
011
111
110
100
104
04
0

0
01
27
0
010
001
111
011
114
14
1

0
11
28
0
110
000
101
011
014
14
1

1
11
29
1
110
101
110
000
104
04
0

1
01
30
1
011
110
101
110
004
14
0

1
10
31
1
100
100
001
011
014
14
1

4

4

4

4

4

4

4

4

4

4

4

4

4

4

10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

1
0
11
11
00
112
00
21
011

11 11 21 10 20 20 31 11 20 21 30 21 30 30 41 10 20 20 30 21 31 31 41 20 31 31 41 31 41 41
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

(b) 6 × 31 sensing matrix
13 03 13 13 13 03 03 0 3 0 3 0 3 13 13 03 03 13 04 04 14 04 14 04 14 04 14 04 04 41 41 40 41
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

(c) 7 × 31 sensing matrix

13
13
13
13
14
14
14
14

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

0
1
0
1
0
0
1
0

0
0
0
1
0
1
1
0

1
0
0
1
0
1
0
0

0
1
1
0
1
0
0
0

1
1
0
0
0
1
0
0

0
0
0
1
1
0
1
0

0
1
0
0
1
0
0
1

0
0
1
0
0
1
1
0

0
0
0
0
0
1
1
1

0
1
0
0
0
1
1
0

0
0
1
1
0
0
0
1

1
0
0
0
0
1
0
1

0
0
1
1
1
0
0
0

0
0
1
0
0
1
0
1

0
1
0
0
1
0
1
0

0
0
0
0
1
1
0
1

1
1
1
0
0
0
1
0

1
0
1
0
1
0
1
0

1
0
0
1
1
0
0
1

1
1
0
0
0
1
0
1

0
0
1
0
1
1
0
1

0
0
1
1
0
1
1
0

1
1
1
0
1
0
0
0

1
0
1
0
0
1
0
1

1
0
0
1
0
0
1
1

0
1
0
0
1
0
1
1

1
0
1
0
0
0
1
1

1
1
0
1
0
0
0
1

0
1
0
1
1
1
0
0

0
1
0
1
1
0
1
0

1
0
1
1
1
0
0
0

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

4

4

4

4

4

(d) 8 × 31 sensing matrix
1

2

3

23 0

1

0

4

5

6

7

8

0

0

0

0

0

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45
0

0

1

0

0

1

1

0

1

1

0

0

0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0
0 of1 each
0 row
0 1is 1shown
0 0in front
0 0 of1the0row,
1 and
0 the
1 1weight
0 1of 0each0 column
1 0 is
0 shown
1 0 below
1 0 the
1 column.)
0 0 0

1

24 0

1

0

0

0

1

0

0

1

1

0

1

0

24 1

0

0

0

0

1

0

1

0

1

0

0

1

1

0

0

0

0

0

1

1

0

1

0

0

0

0

1

0

1

1

0

0

0

0

1

1

0

1

1

1

0

0

0

0

24 0

0

1

0

1

1

1

1

0

0

1

0

1

0

1

0

1

0

0

0

0

1

0

1

1

1

1

0

0

0

0

1

0

0

0

1

1

1

0

1

0

1

0

1

1

24 0

1

1

0

0

1

0

0

1

0

0

0

0

0

1

1

0

1

1

1

1

1

0

1

0

0

1

0

1

0

1

0

0

1

0

0

0

0

1

0

0

0

1

0

0

24 1

1

1

0

0

0

0

0

0

1

0

1

0

0

0

0

1

0

1

0

0221

1

1

1

1

0

0

1

0

0

1

1

0

0

0

0

0

1

0

0

1

0

0

1

24 0

1

1

1

1

0

1

0

1

0

1

0

0

1

0

0

0

0

1

0

1

0

0

0

1

1

1

0

0

1

0

1

0

1

1

1

0

0

0

0

0

0

0

0

0

25 1

0

0

0

1

1

0

0

1

0

0

1

0

1

0

1

0

1

0

0

1

0

0

0

0

0

0

0

0

0

0

0

1

0

1

0

0

0

0

1

1

1

1

0

0

Fig. 2. The sensing matrices with 5, 6, 7, 8 rows and 31 columns used in the simulations for the 2-STAP-II scheme. (The weight

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31

0
1
0
1
0
14 0
14 1
14 0

0
0
0
1
0
1
1
0

1
0
0
1
0
1
0
0

0
1
1
0
1
0
0
0

1
1
0
0
0
1
0
0

0
0
0
1
1
0
1
0

0
1
0
0
1
0
0
1

0
0
1
0
0
1
1
0

0
0
0
0
0
1
1
1

0
1
0
0
0
1
1
0

0
0
1
1
0
0
0
1

1
0
0
0
0
1
0
1

0
0
1
1
1
0
0
0

0
0
1
0
0
1
0
1

0
1
0
0
1
0
1
0

0
0
0
0
1
1
0
1

1
1
1
0
0
0
1
0

1
0
1
0
1
0
1
0

1
0
0
1
1
0
0
1

1
1
0
0
0
1
0
1

0
0
1
0
1
1
0
1

0
0
1
1
0
1
1
0

1
1
1
0
1
0
0
0

1
0
1
0
0
1
0
1

1
0
0
1
0
0
1
1

0
1
0
0
1
0
1
1

1
0
1
0
0
0
1
1

1
1
0
1
0
0
0
1

0
1
0
1
1
1
0
0

0
1
0
1
1
0
1
0

1
0
1
1
1
0
0
0

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

3

4

4

4

4

4

4

4

4

4

4

4

4

4

4

4

13
13
13
13
14

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62

23 0

1

0

0

0

0

0

0

0

1

1

1

1

0

0

0

0

0

0

0

0

0

1

0

1

0

0

1

1

0

1

0

0

1

0

0

1

0

0

1

1

0

1

1

0

1

1

0

0

0

0

0

1

1

0

0

1

1

0

0

1

0

24 0

0

0

1

0

0

1

1

0

0

0

0

1

0

1

0

1

1

0

1

0

0

1

0

0

1

0

1

0

1

0

0

0

1

0

0

0

1

0

0

1

1

0

1

0

0

0

1

1

0

0

1

0

0

1

0

0

0

1

1

0

1

24 1

0

0

0

0

1

0

1

0

1

0

0

1

1

0

0

0

0

0

1

1

0

1

0

0

0

0

1

0

1

1

0

0

0

0

1

1

0

1

1

1

0

0

0

0

0

0

1

1

0

1

1

1

0

0

0

0

0

1

0

1

0

24 0

0

1

0

1

1

1

1

0

0

1

0

1

0

1

0

1

0

0

0

0

1

0

1

1

1

1

0

0

0

0

1

0

0

0

1

1

1

0

1

0

1

0

1

1

0

0

0

0

0

0

0

0

0

1

1

0

0

0

0

0

0

24 0

1

1

0

0

1

0

0

1

0

0

0

0

0

1

1

0

1

1

1

1

1

0

1

0

0

1

0

1

0

1

0

0

1

0

0

0

0

1

0

0

0

1

0

0

0

0

0

0

1

0

0

0

1

1

1

0

1

1

0

0

0

24 1

1

1

0

0

0

0

0

0

1

0

1

0

0

0

0

1

0

1

0

0

1

1

1

1

1

0

0

1

0

0

1

1

0

0

0

0

0

1

0

0

1

0

0

1

0

0

1

1

0

1

0

1

0

1

0

0

0

0

0

0

1

24 0

1

1

1

1

0

1

0

1

0

1

0

0

1

0

0

0

0

1

0

1

0

0

0

1

1

1

0

0

1

0

1

0

1

1

1

0

0

0

0

0

0

0

0

0

1

0

0

0

1

0

0

0

0

0

1

1

1

0

1

0

0

25 1

0

0

0

1

1

0

0

1

0

0

1

0

1

0

1

0

1

0

0

1

0

0

0

0

0

0

0

0

0

0

0

1

0

1

0

0

0

0

1

1

1

1

0

0

1

1

1

0

0

1

1

1

1

0

1

0

0

0

1

0

1

25 0

0

0

1

1

0

0

0

0

0

0

1

0

0

0

1

1

0

0

1

0

0

0

1

0

0

0

0

0

0

1

0

1

0

1

1

0

1

0

0

0

0

1

1

1

1

1

0

1

1

1

1

0

0

0

0

1

0

0

1

1

1

3

4

4

3

4

4

3

3

3

3

3

4

4

3

3

3

4

3

3

4

4

3

4

4

4

4

3

3

3

3

4

3

3

4

3

4

3

3

3

4

4

4

4

4

3

4

3

4

4

3

4

4

4

3

4

4

3

3

3

4

3

4

(a) 9 × 62 sensing matrix

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62

21 0

0

0

0

0

0

1

0

0

1

0

0

1

0

1

0

1

0

0

0

0

1

1

0

0

1

1

1

0

0

0

0

0

1

1

0

0

0

0

0

1

0

1

1

1

1

1

0

1

0

1

0

1

0

0

0

0

0

0

0

0

0

21 1 21 30 41 50 60 71 80 90 10
0 11
1 12
0 13
0 14
0 15
0 16
1 17
0 18
1 19
0 20
0 21
1 22
1 23
0 24
0 25
1 26
0 27
0 28
0 29
0 30
1 31
0 32
1 33
0 34
0 35
0 36
1 37
0 38
0 39
0 40
1 41
0 42
0 43
1 44
1 45
1 46
0 47
0 48
0 49
1 50
1 51
0 52
1 53
0 54
0 55
0 56
0 57
0 58
0 59
1 60
0 61
0 62
0

21 01 0 0 0 0 0 10 0 0 10 0 01 10 0 10 01

10 0 0 0 01

10 10 0 0 10

10 10 0 0 01

01 0 10 10 01

0 01 01 01

10 0 10 10 1

10 10 0

1

0

1

0

10

01

01

0

01

01

01

01

01

01

21 1 10 01 10 0 01 10 0 01 01 10 0 0 01 0 10

0 10 0 01 10

10 0 01 1 01

01 0 0 10 01

10 01 01 0 10

01 0 0 10

01 01 10 10 10

0 01 01

10

10

0

10

0

0

0

0

01

0

10

01

0

0

21
22 10 01 0 0 01 0 0 01 0 01 0 10 0 0 01 1

01 0 0 01 10

01 0 0 0 0

0 01 01 01 10

1 0 0 0 10

0 1 10 10

01 0 0 01 10

0 0 0

10

01

10

0

0

1

10

0

10

1

10

1

1

1

21
22 10 0 10 0 0 10 0 01 10 1 01 0 01 1 01 01

0 0 01 10 0

0 01 10 10 1

10 0 0 01 10

0 10 10 01 01

10 0 01 01

10 1 0 0 0

0 1 1

01

0

01

0

0

0

01

0

10

0

0

10

0

01

22 0 10 0 01 1 01 01 10 0 10 0 0 0 01 10 10

10 01 01 10 01

10 0 01 0 0

0 10 1 10 0

10 01 0 0 0

01 1 01 0

10 0 01 10 0

01 01 0

0

10

0

01

0

10

0

01

01

10

01

10

1

10

22 0 01 0 01 0 0 01 1 01 10 1 01 10 1 10 10

01 0 10 0 0

0 10 0 0 10

01 0 0 1 0

0 01 01 10 10

01 0 10 1

0 10 0 0 0

01 10 1

10

01

10

0

0

01

10

0

0

01

01

0

0

1

22 0 0 01 10 1 10 10 0 0 0 0 01 0 10 01 0

0 1 10 0 10

0 01 1 0 0

0 01 1 0 01

01 1 01 01 01

10 10 10 0

0 01 10 0 0

10 10 0

0

01

0

1

01

0

0

1

10

0

10

01

1

0

22
23 0 10 01 1 0 01 10 1 1 0 10 1 01 10 0 0

10 0 01 01 0

0 01 0 01 0

1 0 01 10 01

0 10 10 0 0

1 0 0 10

0 0 01 01 0

10 0 10

0

10

01

01

01

1

01

01

0

10

10

0

0

10

22 03 03 13 04 13 03 04 04 03 04 03 14 03 04 14 04

03 13 03 03 03

03 14 13 03 03

04 13 14 04 14

14 14 14 13 14

04 03 03 04

03 13 04 04 03

03 04 03

04

14

04

14

13

04

03

13

03

03

04

14

14

04

23 0

0

1

1

0

1

0

1

1

0

0

1

1

0

0

0

0

0

1

1

0

0

1

0

1

0

1

3

3

3

4

3

3

4

4

3

4

3

4

3

4

4

4

3

3

3

3

3

3

4

3

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62

19 1

0

0

1

0

0

1

0

1

19 10 21 30
19 10 01 0
19 0 1 0
19 0 10 01
20 01 10
19
20 0 0
19
20 10 0
20 0 0

40 51 60
10 0 01
0 10 0
0 0 10

0 01 0
10 01 0
0 10 01
01 10 0

71 80 90
10 01 10
10 0 0
01 1 0

0 0 01
01 10 10
0 0 10
1 01 01

1

0

0

0

0

0 11
0 12
0 13
0 14
0
10
10 01 0 0 01
01 01 0 01 01
0 1 0 01 10

0 10 10
01 0 10
0 0 01
10 0 0

0 1 10
01 10 01
01 10 0
1 0 1

0

0

1

0

1

0

0

1

0

1

0

0

0

0

0

1

1

0

0

0

0

0

0

1

1

1

1

1

1

0

0

0

0

0

0

3 3 4 3 4 4 4 4 4 4 3 4 4 3
(b)
10 × 62 sensing matrix

3

4

3

3

4

4

3

3

4

3

4

4

4

4

3

4

3

3

3

3

4

4

4

4

0

0

0

0

0

0

0

1

0

0

0

1

0

1

0

0

0

0

0

1

0

1

0

0

1

0

0

0

0

0

1 16
0 17
1 18
0 19
0
15
0 01 10 01 10
1 0 10 01 0
0 10 0 10 0

0 21
0 22
0 23
0 24
1
20
01 0 1 01 0
0 0 0 0 1
1 01 10 10 0

1 26
0 27
0 28
1 29
1
25
0 0 0 0 0
10 0 01 10 10
0 0 0 01 0

0 31
0 32
0 33
0 34
1
30
01 01 10 0 0
01 0 0 0 1
10 1 01 01 0

0 36
0 37
1 38
0 39
0
35
10 10 01 0 10
0 0 10 01 01
0 0 10 0 0

0 41
0 42
0 43
0 44
0
40
01 0 01 01 0
0 01 01 0 0
10 0 10 10 0

10 01
0 0
0 10

0 0
1 10
0 01

0 01
0 01
0 10

10 0
0 10
0 01

01 0
0 0
1 01

0
0

0
21 0 0 01 0 1 0 0 0 01 01 10 10 01 0 0
20
21 01 0 1 0 0 1 10 10 0 0 0 10 0 10 01
20
21 03 03 14 03 13 04 04 04 13 13 04 03 14 04 03

01
0

21 1

0

1

0

0

1

0

0

0

0

0

0

0

0

1

3

3

4

3

3

4

4

4

3

3

4

3

4

4

3

0 10 01
0 0 01
0 01 10
01 0 1

0
1

10
03

10
0 1 0 0
1 0 10 01
04 14 04 04

0

1

0

0

1

3

4

4

4

4

0 0 10
0 01 0
0 0 0
0 10 01

10
01

0
13

01
0 01 0 01
01 0 1 10
03 13 04 13

0

1

0

1

0

3

3

3

4

3

10 0 0
01 10 01
01 01 01
10 0 10

0
1

0
13

01
10 10 10 0
0 0 01 1
03 03 04 04

0

0

0

1

1

3

3

3

4

4

01 01 10
1 10 01
10 1 0
10 0 10

01
10

0
14

0
0 1 01 10
0 0 0 0
04 14 14 03

0

0

0

0

0

4

4

4

4

3

01 1 10
0 0 0
1 10 01
0 01 0

01
0

10 1 01
0 0 0
0 10 1
0 01 0

0

1

0

0

0

0 46
1 47
1 48
1 49
0
45
01 10 0 0 01
0 10 10 10 0
10 01 0 0 1

01 01
0 0
10 10

1

1

0

0

0

1

0

1

0

1

0 51
1 52
0 53
1 54
1
50
10 10 0 0 0
0 10 0 10 1
01 01 0 01 0

0 56
1 57
0 58
0 59
1
55
10 0 10 0 1
0 10 0 01 10
0 0 0 01 10

0 01 01
10 0 01
0 10 10
0 01 10

0
10

0
1

0
10

01
01

0
10

1
0

0
0

0
01

0
0

10
1

0
0

0
0

0
10

0
0

10
10

01
0

10
0

01
0

01
10

0
01

0
1

0
0

01
0

0
0

0
01

0
01

10
01

0
0

10
01

10
0

01
10

0
10

01
0

1

0

0

0 61
0 62
0
60
10 0 0
0 01 01

01
0

01
10

0
10

1
0

10
01

01
0

10
01

0
10

1
0

01
01 0 10 0
01 10 0 1
13 04 03 03

0
01

1
04

10 0 1 10
0 1 0 01
04 04 14 03

0
13

01 0 0
0 1 01
14 04 04

13

03

04

03

04

03

03

03

13

04

14

0
14

0
03

10
03

1

0

1

0

1

1

0

0

1

0

0

1

1

0

0

1

1

1

0

1

0

0

0

0

0

0

0

4

4

4

4

3

3

4

3

3

3

4

4

4

3

3

4

3

4

3

3

3

3

4

4

4

3

3

(c) 11 × 62 sensing matrix

Fig. 3. The sensing matrices with 9, 10, 11 rows and 62 columns used in the simulations for the 2-STAMP scheme. (The weight
of each row is shown in front of the row, and the weight of each column is shown below the column.)

23

