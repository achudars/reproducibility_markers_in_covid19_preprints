iGOS++: Integrated Gradient Optimized Saliency by Bilateral Perturbations
Saeed Khorram* , Tyler Lawson* , Li Fuxin

arXiv:2012.15783v1 [cs.CV] 31 Dec 2020

Collaborative Robotics and Intelligent Systems (CoRIS) Institute, Oregon State University
{khorrams, lawsonty, lif}@oregonstate.edu

Abstract
The black-box nature of the deep networks makes the explanation for ”why” they make certain predictions extremely
challenging. Saliency maps are one of the most widely-used
local explanation tools to alleviate this problem. One of the
primary approaches for generating saliency maps is by optimizing a mask over the input dimensions so that the output of
the network is influenced the most by the masking. However,
prior work only studies such influence by removing evidence
from the input. In this paper, we present iGOS++, a framework
to generate saliency maps that are optimized for altering the
output of the black-box system by either removing or preserving only a small fraction of the input. Additionally, we propose
to add a bilateral total variation term to the optimization that
improves the continuity of the saliency map especially under
high resolution and with thin object parts. The evaluation results from comparing iGOS++ against state-of-the-art saliency
map methods show significant improvement in locating salient
regions that are directly interpretable by humans. We utilized
iGOS++ in the task of classifying COVID-19 cases from x-ray
images and discovered that sometimes the CNN network is
overfitted to the characters printed on the x-ray images when
performing classification. Fixing this issue by data cleansing
significantly improved the precision and recall of the classifier.

Introduction
As deep networks achieve excellent performance in many
tasks, more and more people want to open these black boxes
to understand how they make their decisions under the hood.
Especially, explaining deep classifiers can help to potentially
“debug” them to understand how they make mistakes, and fix
those mistakes e.g. by additional data preprocessing. This is
increasingly important as deep learning is starting to be used
in critical decision-making scenarios such as autonomous
driving and medical diagnosis.
Saliency map or heatmap visualization is a fundamental tool for explaining convolutional networks (CNNs). It
is mostly used to directly explain classification decisions,
but approaches that explain intermediate network nodes or
forming new concepts would depend on them as well. In
earlier days, heatmap visualizations were mostly based on
computing gradient variants of the network with respect to
the input (Selvaraju et al. 2017; Zhang et al. 2016; Simonyan,
* Equal

contribution.

Vedaldi, and Zisserman 2014). However due to the highly
nonlinear nature of the CNNs, those one-step gradients only
account for infinitesimal changes in the function values and
do not necessarily correlate to the features that CNNs actually use for decision making (Adebayo et al. 2018). This
has caused some of the earlier works to be disillusioned
on heatmap research. The most successful of the gradientbased approaches, Grad-CAM (Selvaraju et al. 2017), partially avoids this issue by not backpropagating into the convolutional layers, hence correlating reasonably well with CNN
classification. However, its heatmaps are quite low-resolution
since it only works at the final layers where the images are
already low-resolution.
Another group of approaches optimize for a small mask
so that CNNs can no longer classify masked images (Fong
and Vedaldi 2017; Qi, Khorram, and Fuxin 2020). These
approaches provably correlate with the CNN classification,
since it can usually be shown that the CNN would no longer
predict the original category once the image is masked. However, as we experiment with those approaches, we find that
sometimes it is much simpler to “break” the important features CNNs are using to classify, without necessarily capturing all the important features. An intuitive example of this
is an object with its important parts being long and thin, as
shown in Fig. 1. The mask only needs to cover a small amount
of points to “break” the legs of the tiger beetle, making it
disconnected and making the CNN no longer capable of classification. However, those areas, when revealed to the CNN,
do not necessarily contain enough information for CNNs to
classify them correctly. As a result, those methods do not
perform well especially at higher resolutions, as their masks
become disconnected and “adversarial”, focusing only on
reducing the CNN prediction rather than locating all the informative areas, the capability of CNN classifying the image
to its original category based on the masked parts drops noticeably when moving from 14 × 14 heatmap resolution to a
224 × 224 resolution (Qi, Khorram, and Fuxin 2020).
In this paper, we propose novel improvements to address
this gap. The first novelty is to optimize for an additional
insertion loss which aims for the CNN to correctly classify
the object even if only a small informative part of the image
is revealed. Instead of just adding a simple loss term, we
found that separating the deletion mask and insertion mask
during the optimization process helps the visualization perfor-

Figure 1: Comparison of heatmaps generated from (Qi, Khorram, and Fuxin 2020) (I-GOS) and our proposed approach (iGOS++) for a tiger
beetle image. I-GOS focuses only on breaking existing evidence (e.g. legs of the beetle), hence generated a mask that is highly scattered under
high resolution (2nd image), and prediction confidence with the masked area (middle image) on the tiger beetle category is very low. However,
with the same amount of pixels (6%) from the proposed iGOS++ attention map, the network has 99.2% confidence (5th image)

mance. Our second novelty is to propose a new bilateral total
variation term that makes the mask smooth on image areas
with similar color, alleviating heatmap diffusion at higher
resolutions. A combination of these techniques enables us to
obtain heatmap visualizations with significantly better performance, especially when the heatmaps are generated at a
high resolution – which are more prone to adversarial solutions if the masks are derived solely on deletion loss. In the
tiger-beetle example (Fig.1), it can be seen that iGOS++ is
capable of capturing entire legs of the beetle hence capable
of generating a high-confidence prediction even with only
6% of the pixels in the original image.
To showcase the capabilities of our method in detecting
bugs in the classifier, we utilized iGOS++ in the task of detecting COVID-19 patients based on x-ray imaging. Interestingly,
we found that in some cases, the classifier is overfitted to characters printed on the x-ray images which clearly should not
be the indicator. Once we pre-processed all images to remove
written characters, meaningful performance improvements
on the COVID detection task was observed. This shows the
utility of the heatmap visualization algorithms in realistic
tasks.

Related Work
We only review related work in heatmap visualizations rather
than the broader problem of explaining deep networks. Most
visualization approaches can be categorized into gradientbased approaches and perturbation-based approaches.
Gradient-based approaches for generating saliency
maps commonly use different backpropagation heuristics
to derive the sensitivity of the output score with respect to
the input. Deconv and Saliency Maps (Zeiler and Fergus
2014; Simonyan, Vedaldi, and Zisserman 2014) attach special
deconvolutional layers to the convolutional layers. Guidedbackprop (Springenberg et al. 2015) works in a similar, yet
different, method to (Zeiler and Fergus 2014), masking values based on negative gradients. (Shrikumar et al. 2016)
multiplies the gradient with the image RGB values. (Sundararajan, Taly, and Yan 2017) proposes integrated gradients,
which compute multiple gradients along a straight line in
the image space and average them. (Bach et al. 2015) computes the relevance across different layers and assigns an

importance to each pixel that is used to create a saliency map.
(Zhang et al. 2016) uses a winner take all probabilistic model
to send top-down signals through the network and generate
probabilities based on the weights that are used to create the
saliencies. Grad-CAM (Selvaraju et al. 2017) is the most popular visualization method in this category, it generalizes the
existing class activation method (Zhou et al. 2016) to work
on any CNN-based neural network and maps class scores
back to the previous convolution layer.
However, gradients reflect one-step infinitesimal changes
in the input, which do not necessarily correspond to a direction in which the output score from a deep network would
drop significantly — particularly for deep networks that
are highly non-linear functions. In addition, some of these
saliency maps were shown to be completely or somewhat
independent of the category, only showing strong image
edges (Adebayo et al. 2018).
Perturbation-based approaches work by modifying the
input in some way, e.g. masking, and testing how the output
of the network changes. (Zhou et al. 2014) is a method that
iteratively removes sections of the image using the gradient
until the image contains only the information needed for classification of the target class. In (Dabkowski and Gal 2017),
a new network is trained to generate saliency maps. RISE
(Petsiuk, Das, and Saenko 2018) and LIME (Ribeiro, Singh,
and Guestrin 2016) are similar perturbation based methods
that treat the model as a black-box, and thus do not use
gradients at all. They both involve randomly perturbing the
image. RISE weighs all of the random masks by the model’s
confidence and combines them, taking into account each pixels distribution in the random masks. In LIME, the random
masks are used to fit a linear model to the black-box model in
the local space around the image. The distance from the original image is used in the loss function and the final weights
of the linear model are used to generate an explanation for
the image. LIME has a reliance on super-pixels to avoid
adversarial masks. Some methods utilize optimization with
multiple iterations to generate a heatmap visualization (Fong
and Vedaldi 2017; Qi, Khorram, and Fuxin 2020). Here the
main challenge is the highly non-convex nature of the optimization problem. (Fong and Vedaldi 2017) optimizes a mask
to reduce the prediction confidence of a target class. Follow-

ing (Fong and Vedaldi 2017), (Fong, Patrick, and Vedaldi
2019) uses a fixed-area binary mask that maximally effects
the output. This is advantageous as it mitigates the balancing
issue in the original mask optimization. (Qi, Khorram, and
Fuxin 2020) is the most related to our work. It combines the
algorithms in (Fong and Vedaldi 2017) and (Sundararajan,
Taly, and Yan 2017), by utilizing integrated gradient to optimize the mask. This is shown to significantly improve the
performance of the optimization.

Bilaterally-Optimized Perturbations

of fc (M ) with respect to M can be formulated as follows,


S ∂fc Φ I0 , I˜0 , s M
X
S
1
,
∇IG
I0 fc (M ) =
S s=1
∂M

where it accumulates the gradients along the straight-line
path from the perturbed image Φ I0 , I˜0 , M towards the
baseline I˜0 , which approximately solves the global optimum
to the unconstrained problem eq. (1). Equivalently, IG can be
thought of as performing gradient descent to simultaneously
optimize the performance of multiple masks:

Background
Heatmap Visualizations by Optimization We consider
the well-known image classification task, where a black-box
network f predicts a score fc (I0 ) on class c for input image
I0 . Let denote the Hadamard product.
The idea of optimization-based heatmap visualization is
to locate the regions in the input image I0 that are most
important for the network f in outputting fc (I0 ). These local
perturbations can be formulated by the inner-product of a realvalued mask M to the image f (I0 M ) (Fong and Vedaldi
2017). Prior work optimizes for the deletion task, namely
that masking the image so that it has e.g. low predictive
confidence for class c. Afterwards, one can visualize the mask
to find the salient regions that caused the output confidence
to decrease. Mathematically:

min Fc (I0 , M ) = fc Φ(I0 , I˜0 , M ) + g(M ),
M

s.t.

g(M ) = λ1 ||1 − M ||1 + λ2 TV(M ),
Φ(I0 , I˜0 , M ) = I0 M + I˜0 (1 − M ),

(1)
0 ≤ M ≤ 1,

where I˜0 is a baseline image with near zero evidence about
the target class c, fc (I˜0 ) ≈ minI fc (I). It is often chosen
to be a constant, noise, or a blurred version of the image I0
(Fong and Vedaldi 2017). The masking operator Φ(I0 , I˜0 , M )
uses a weighted version between I0 and I˜0 to block the
influence of certain pixels, decided by the mask M values. The aim of optimization problem (1) is to locate a
small and smooth mask M which identifies the regions that
are most informative to the black-box f by maximally reducing the output
 score (predictive confidence on class c)
fc Φ(I0 , I˜0 , M )  fc (I0 ). The regularization term g(M )
encourages the mask to be small, by penalizing the magnitude of M with coefficient λ1 , and smooth, by penalizing
the total-variation (TV) in M (Fong and Vedaldi 2017) with
coefficient λ2 .
Integrated Gradient Eqn. (1) is a complicated non-convex
optimization problem. (Fong and Vedaldi 2017) optimizes
the mask by gradient descent. However, this is slow and can
take hundreds of iterations to converge. In addition, gradient
descent can converge to a local optimum and is not able to
jump out of it. (Qi, Khorram, and Fuxin 2020) has alleviated
this issue by using Integrated-gradient (IG) (Sundararajan,
Taly, and Yan 2017) rather than conventional gradient as the
descent direction for solving the mask optimization. The IG

(2)

min
M

S
1 X   ˜ s 
fc Φ I0 , I0 , M
+ g(M )
S s=1
S

(3)

which means it is a proper optimization algorithm. Practically,
(Qi, Khorram, and Fuxin 2020) has shown that it improves
the optimization performance of eq. (1) as well.

Bilateral Minimal Evidence
As shown in Fig. 1 and as discussed in the introduction: the
capability of destroying a CNN feature does not by itself fully
explain the features a CNN used. In this section we propose
I-GOS++, which improves upon (Qi, Khorram, and Fuxin
2020) by also optimizing for the insertion task. This would
make the CNN predict the original class given information
from only a small and smooth area. We believe that the deletion task and the insertion task are complementary and need
to be considered simultaneously, since both of them contain
important information that introduces a novel ”look” into the
network behavior when perturbing the input. Besides, considering one task alone is prone to reach adversarial solutions
(Szegedy et al. 2014), particularly when removing evidence.
However, it would be unlikely to find adversarial solutions
that can satisfy both aforementioned criteria.
A naı̈ve approach to implement this would be to add an
insertion loss −fc Φ(I0 , I˜0 , 1−M ) directly to eq. (1). However, empirically we found that optimizing separate masks
performed better than the direct approach. Our method aims
to optimize separate deletion and insertion masks over the
input with the constraint that the product of the two also satisfies both the deletion and insertion criteria for a target class
c, i.e. deletion of the evidence from the input I0 drastically
reduces the output score while retaining the same evidence
preserves the initial output score fc (I0 ). Formally, we solve
the optimization problem with 3 masks:
min
M =(Mx ,My )


Fc (I0 , M ) = fc Φ(I0 , I˜0 , Mx )


− fc Φ(I0 , I˜0 , 1 − My ) + fc Φ(I0 , I˜0 , Mxy )

− fc Φ(I0 , I˜0 , 1 − Mxy ) + g(Mxy )
(4)

subject to g(Mxy ) = λ1 ||1 − Mxy ||1 + λ2 BTV(Mxy );
Mxy = Mx My ; 0 ≤ Mx , My ≤ 1

where Mxy is taken as the final solution to the above optimization problem.

In the above formulation eq. (4), the resolution of the
masks is flexible. If the mask has lower resolutions than the
original input I0 , it is first up-sampled to the input size using
bi-linear interpolation, and then is applied over the image.
The choice of the mask resolution depends on the application
and the amount of detail desired in the output mask. Commonly, lower resolution masks e.g. 14 × 14 tend to generate
more coarse and smooth saliency maps, while higher resolution masks, e.g. 224 × 224, generate more detailed and
scattered ones. Lower resolution masks also have the advantage of being more robust against adversarial solutions (Qi,
Khorram, and Fuxin 2020; Szegedy et al. 2014). The BTV
term stands for Bilateral Total Variation which is explained
later in section . In addition, one can add regularization terms
on the individual masks Mx and My , but we have found
regularization only on the final mask Mxy to be sufficient.
Note that the integrated
gradient for the insertion mask

−fc Φ(I0 , I˜0 , 1 − M ) is slightly different from Eq. (2) as
it calculates the negative IG along the straight-line path from
the image perturbed
using the inverse mask 1 − M , i.e.,

Φ I0 , I˜0 , 1 − M , toward the original image I0 ,


S ∂fc Φ I˜0 , I0 , s M
X
S
1
IG
.
∇IG
I0 hc (M ) = −∇I0 fc (1 − M ) = −
S s=1
∂M
(5)

We use IG to substitute the conventional gradient for the
partial objective fc (.) in eq. (4) and the simple gradient for
the convex regularization terms g(·). The following is the
total-gradient (TG) for each iteration of the mask update:
IG
IG
T G(M ) = ∇IG
I0 fc (Mx ) + ∇I0 hc (My ) + ∇I0 fc (Mxy ) (6)

+ ∇IG
I0 hc (Mxy ) + ∇g(Mxy ).

TG contains separate integrated gradients w.r.t the deletion and insertion masks (Mx and My ). ∇IG
I0 is indicative of the direction to the unconstrained problem eq. (4)
while ∇g(Mxy ) regularizes the gradients toward a local
and smooth mask and discards unrelated information. Moreover, to make the masks to generalize better and be less
dependent over the individual dimensions, we add noise to
the perturbed image (Fong and Vedaldi 2017; Qi, Khorram,
and Fuxin 2020) during each step of the IG calculation in
IG
∇IG
I0 fc (Mx ) + ∇I0 hc (My ).
Bilateral Total-Variance To further alleviate the problem
of scattered heatmaps shown in Fig. 1, we introduce a new
variation of the TV loss (Fong and Vedaldi 2017), called
Bilateral Total-Variance (BTV),
BT V =

X

2

e−∇I(u)

/σ 2

k∇M (u)kββ

(7)

u∈Λ

where M is the mask, I is the input image, and β and σ
are hyperparameters. This enforces the mask to not only be
smooth in its own space but also to consider the pixel value
differences in the image space. This is intuitive since BTV

would discourage mask value changes when the input image
pixels have similar color. In other words, BTV penalizes the
variation in the mask when it is over a single part of an object.
This helps particularly in high-resolution mask optimizations
and prevents having scattered and adversarial masks.
Backtracking Line Search for Computing the Step Size
Similar to (Qi, Khorram, and Fuxin 2020), we use backtracking line search at each iteration of mask update. Appropriate
step size plays a significant role in avoiding local optimum
and accelerates convergence. To that end, we revised the
Armijo-Goldstein condition as follows,
S
X
s=1


Fc

 X
S
s
s
(M k − αk · T G(M k )) −
Fc ( M k )
S
S
s=1

≤ − αk · β · T G(M k )T T G(M k ),

(8)

where αk is the step size at time step k and β ∈ (0, 1) is
the control parameter. This tries to determine the maximum
movement in the search direction that corresponds to an
adequate decrease in the objective function Fc (M k ). Note,
this is slightly different from the revised condition in (Qi,
Khorram, and Fuxin 2020) in that the objective function
decrease is calculated over the IG intervals rather than at the
mask M k . This is due to the fact that IG actually solves an
optimization problem similar to eq. (3).

Experiments
Metrics and Evaluation Setup
Although visual assessment of the saliency maps might seem
straight forward, quantitative comparisons still pose a challenge. For example, one of the widely-used metrics is the
pointing game (Zhang et al. 2016) which assesses how accurate the saliency maps can locate the objects using the
bounding-boxes annotated by humans. However, this is not
necessarily correlated with the underlying decision making
process of the deep networks, and localization of the objects
is only intelligible for humans. Also, pointing game requires
correct localization even for misclassified examples, and its
results correlate poorly with other metrics (e.g. Excitation-BP,
which has excellent pointing game scores, suffers from the
fallacies pointed out in (Adebayo et al. 2018)). This casts
doubt on the validity of this metric. Due to these flaws, we
do not utilize this metric in our evaluations.
We opted to follow the causal metrics introduced in (Petsiuk, Das, and Saenko 2018), which is the evolved version of
the ”deletion game” introduced in (Fong and Vedaldi 2017).
Although not perfect, we find this to be a better evaluation
metric, as it performs interventions on the input image, a
necessary approach to understand causality. These metrics
are: the deletion metric that evaluates how sharply the confidence of the network drops as regions are removed the
input. Starting from the original image, relevant pixels as indicated by the heatmap are gradually deleted from the image
and are replaced by pixels from the highly-blurred baseline.
This goes on until all pixels from the original image are removed and network has near-zero confidence. The deletion

Figure 2: Heatmap visualization from different methods. It can
be seen that iGOS++ has better insertion and deletion curves than
baselines. Note the difference in detail between the 28 × 28 and
224 × 224 masks (Best viewed in Color)

10% − 25% improvement over prior work. This is mainly
due to the novel incorporation of the insertion objective in
our eq. (4). Note that, for all the methods, the insertion score
tends to be lower on higher resolution heatmaps, since it is
easier for CNNs to extract features if a significant contiguous
part is inserted, rather than isolated pixels in high-resolution
masks. On the other hand, on higher resolutions, it is easier
to break an image feature by destroying a small amount of
pixels, hence the deletion metric is usually better. We note
that the drop of our insertion performance from 7 × 7 (or
14 × 14) to 224 × 224 is significantly smaller than that of
I-GOS, hence it is safer to choose a higher-resolution with
iGOS++.
In Fig.2, we show visualizations from different saliency
map methods. It can be seen that iGOS++ performs well even
if the object has long and thin parts, whereas I-GOS generates
much more scattered heatmaps at high resolutions. GradCAM
works well on insertion, but included too many irrelevant
regions due to its low resolution, a similar issue shared with
RISE. More visualizations are shown in the supplementary
materials.

Results and Analysis
Insertion and Deletion Scores. Table 1 and Table 2 show
our results on the Insertion and Deletion metrics, averaged
over 5, 000 random ImageNet images, for ResNet50 and
VGG19 architectures respectively. We experimented with
masks of the size 224×224, 28×28, 7×7 (only on Resnet50),
and 14×14 (only on VGG19) in our experiments. The choice
of the masks is to provide fair comparison with other baselines, e.g GradCam (Selvaraju et al. 2017) that for ResNet50
is best on its“layer4” (7×7 resolution). In addition, Integrated
Gradients visualization (Sundararajan, Taly, and Yan 2017)
has heatmaps directly on the input (224 × 224). Also note
that binary masks for RISE (Petsiuk, Das, and Saenko 2018)
are generated at 7 × 7 and then up-sampled to 224 × 224.
As shown in Tables 1 and 2, iGOS++ has superior performance at all resolutions compared to the other approaches.
Particularly, for insertion score, our approach is showing

True
Random

score is the area under the curve (AUC) of the classification
confidences. The lower the deletion score is the better. The
Insertion metric, which is complementary to the deletion one,
shows how quickly the original confidence of the network
can be reached if relevant evidence are presented. Starting
from a baseline image with near-zero confidence, relevant
pixels from the image, based on the ranking provided by the
heatmap, are gradually inserted into the baseline image. This
goes on until all pixels in the baseline are replaced by the
original image. The insertion score is also the AUC of the
classification scores, with higher insertion score indicating
better performance. While adversarial examples can break
the deletion metric with ease and achieve near perfect score,
they usually do poorly on the insertion score. Yet only relying on the insertion score will sometimes invite irrelevant
background regions to be ranked higher than relevant areas.
That is why one needs to jointly look at both scores.

Image

Fashion-MNIST

Figure 3: Sanity check using the Fashion MNIST dataset. A
simple CNN model was trained first using ground truth labels
and then with random labels. The masks on the second row
were generated with the ground truth trained model, while the
third row are masks generated for the random label trained
model
Sanity Check. Following the work of (Adebayo et al.
2018), we perform sanity checks on our method to justify
its validity, as only relying on visual assessment can be fallacious. First is the model randomization test which checks
whether the generated heatmaps are indeed independent of
the model parameters or not. Since by randomizing the model
(even over one layer), the output score of the network will go
near-zero, our method simply returns the initialization M ≈ 1
which is different from the final mask we generate. Second,
for the label randomization test, we trained a simple Convolutional Neural Network (CNN) over the Fashion-MNIST
dataset. The CNN was trained with two sets of data. First it
was trained using the ground truth labels of the dataset until
it reached 100% accuracy. Heatmaps were generated for this
model and then compared to maps generated for a model
trained with random labels. The idea is if the heatmaps are
the same, then the method might be interpreting the image
rather than explaining the models decision. Figure 3 shows
that iGOS++ generates meaningful heatmaps on real classes
that are significantly different from random classes.
Adversarial Examples In addition to the sanity check presented in (Adebayo et al. 2018), the visualization from our

ResNet50
GradCam (Selvaraju et al. 2017)
Integrated Gradients (Sundararajan, Taly, and Yan 2017)
RISE (Petsiuk, Das, and Saenko 2018)
Mask (Fong and Vedaldi 2017)
IGOS (Qi, Khorram, and Fuxin 2020)
iGOS++ (ours)

224×224
Deletion Insertion
––
0.0907
0.1196
0.0468
0.0420
0.0328

––
0.2921
0.5637
0.4962
0.5846
0.7261

28×28
Deletion Insertion
––
––
––
0.1151
0.1059
0.0929

––
––
––
0.5559
0.5986
0.7284

7×7
Deletion Insertion
0.1675
––
––
0.2259
0.1607
0.1810

0.6521
––
––
0.6003
0.6632
0.7332

Table 1: Quantitative comparison in terms of deletion (lower is better) and insertion (higher is better) metrics using ResNet50.
The top row shows the different resolutions
VGG19
GradCam (Selvaraju et al. 2017)
Integrated Gradients (Sundararajan, Taly, and Yan 2017)
RISE (Petsiuk, Das, and Saenko 2018)
Mask (Fong and Vedaldi 2017)
I-GOS (Qi, Khorram, and Fuxin 2020)
iGOS++ (ours)

224×224
Deletion Insertion
––
0.0663
0.1082
0.0482
0.0336
0.0344

––
0.2551
0.5139
0.4158
0.5246
0.6537

28×28
Deletion Insertion
––
––
––
0.1056
0.0899
0.0796

––
––
––
0.5335
0.5701
0.7066

14×14
Deletion Insertion
0.1527
––
––
0.1753
0.1213
0.1123

0.5938
––
––
0.5647
0.6387
0.7061

Table 2: Quantitative comparison in terms of deletion/insertion metrics using VGG19. The top row shows the different resolutions.
method on adversarial examples have been analyzed. Figure
4 shows four image samples from ImageNet Validation set
where the heatmaps generated on the adversarial images have
been visualized along with the original natural images. In
this experiment, VGG19 architecture and two different mask
resolutions: a low-resolution (28 × 28) and a high-resolution
(224×224) are used. To generate adversarial examples (target
class: persian cat), we used the MI-FGSM (Dong et al. 2018)
on the VGG19 model. There are two main takeaways from
this analysis: first, for adversarial examples, the heatmaps
generated by iGOS++ does not provide meaningful explanations. Second, it can be observed that for adversarial examples, the insertion score is significantly lower and it only
reaches the original score after almost all the pixels have
been inserted. This shows that the insertion score is a good
indication of whether the generated mask is adversarial or
not.
Ablation Study. The results from the ablation study are
presented in the Table 3. The experiments are run at 224×224
and 28 × 28 resolutions using the ResNet50 model on ImageNet. We observe that optimizing a mask by replacing
the deletion loss in (Qi, Khorram, and Fuxin 2020) with an
insertion one obtains a good insertion score on both resolutions. However, it significantly hurts the deletion score. On
the other hand, naı̈vely incorporating the insertion loss to
eq. (1) by just adding an insertion loss term clearly does not
work as well. In fact, it is performing worst than either of the
(Qi, Khorram, and Fuxin 2020) and insertion optimization
alone. Further, we find that adding noise makes a clear difference on high resolution (224 × 224). Moreover, having
a fixed step size is worse than using adaptive step size with
line search. Finally, by removing the bilateral TV term we
observed that the deletion score decreases, particularly in
high resolution while the insertion score also was impacted

negatively. This shows the benefit of the bilateral TV term in
avoiding adversarial solutions.
An Application on COVID-19 Detection from X-ray
Imaging. COVID-19 has been devastating to human lives
throughout the world in 2020. Currently, diagnostic tools
such as RT-PCR has non-negligible false negative rates hence
it is desirable to be able to diagnose COVID-19 cases directly
from chest imaging. X-ray imaging is significantly cheaper
than CT or other more high-resolution imaging tools, hence
there would be significant socio-economical benefits if one
can diagnose COVID-19 reliably from X-ray imaging data.
To that end, we used the COVIDx dataset (Linda Wang and
Wong 2020) which is one of the largest publicly-available
COVID-19 dataset with 13,786 training samples and 1,572
validation samples comprised of X-ray images from Normal,
Pneumonia, and COVID-19 patients. Following the setting
from (Linda Wang and Wong 2020), we trained a classifier
over these images and reproduced the reported performance.
When we applied iGOS++ on the trained classifier, we noticed that in occasional cases the classifier seems to have
overfitted to singleton characters printed on the x-ray image
(Fig. 5), such that even when only the character region is
available, there is a non-negligible chance of classifying for
COVID-19. Note that the higher resolution explanation from
iGOS++ is important in pinpointing the heatmap to the character whereas low-resolution alternatives such as GradCAM
are not informative. Noticing this “bug” of the classifier, we
utilized the state-of-the-art character detector, CRAFT (Baek
et al. 2019), and removed the spurious characters from all the
x-ray images in both the training and the testing sets by cleaning and in-painting the detected regions. Examples from the
original and cleaned dataset, referred to as COVIDx++, can
be found in the supplementary materials. The results shown
in Table 4 show that the recall of COVID-19 detection on

Ablation
I-GOS
Insertion
I-GOS + Insertion (naı̈ve)
iGOS++ (no noise)
iGOS++ (fix step size)
iGOS++ (no BTV)
iGOS++

224×224
Deletion Insertion
0.0420
0.5846
0.0760
0.6192
0.0322
0.6175
0.0490
0.5943
0.0332
0.5695
0.0245
0.6742
0.0328
0.7261

28×28
Deletion Insertion
0.1059
0.5986
0.1321
0.7231
0.2037
0.5103
0.0904
0.7108
0.1052
0.7060
0.0813
0.6825
0.0929
0.7284

Table 3: Results from ablation study on ResNet50. The top row shows the different mask resolutions.
this experiment in the future and obtain more meaningful
knowledge from this data.

Figure 5: Showcase for capability of the iGOS++ in detecting bugs

(a)

in a COVID-19 classification pipeline. It can be noted that unlike
Grad-Cam which provides coarse (8×8 resolution) and ineffective
explanation, iGOS++ can generate a more-detailed explanation
(32×32 resolution) to discover the most salient regions. In the rightmost image, only by inserting the top 6% of the pixels from iGOS++
heatmap (highlighting character “R”), the classifier predicts it as
COVID-19 (confidence 43 %).

Dataset
COVIDx
COVIDx++

Accuracy
95.19
95.93

F1-Score
93.81
95.08

Precision
95.75
95.70

Recall
91.85
94.49

Table 4: Classification performance on the validation set of the
COVIDx and COVIDx++ datasets. COVID-NET (Linda Wang and
Wong 2020) is used as the classifier.

Conclusion and Discussion
(b)

Figure 4: Comparison of heatmap visualizations from
iGOS++ for natural and adversarial examples on two different resolutions: a) 28×28 b) 224×224. Adversarial examples severely suffer in increasing the prediction score while
for natural images, the insertion curve goes up quickly. The
heatmaps are also different, and for adversarial examples,
they do not represent meaningful explanations.

the validation dataset improved by 2.5% and the F1 score improved by more than 1%. This small exercise showcases that
“bugs” do exist in deep network classifiers as they do not have
common sense on what part of the data is definitely noise,
and that heatmap visualizations can help humans locate these
bugs as a useful debugging tool. We hope to dig more into

We propose a new approach, iGOS++, for creating a heatmap
visualization to explain decisions made by the CNNs.
iGOS++ differs from other approaches by optimizing for
separate deletion and insertion masks which are tied together
to output a single explanation. We show empirically that
with this approach, significantly better insertion performance
at all resolutions can be achieved. Besides, we introduce a
new term for regularization using the bilateral total variance
of the image. This is shown to improve the smoothness of
the generated heatmaps as well. As a real-life example, we
showed that in a task of classifying COVID-19 patients from
x-ray images, sometimes the classifier would overfit to the
characters printed on the image. Removing this bug improved
the classifier performance meaningfully. We hope the highfidelity heatmaps generated with iGOS++ will be helpful for
downstream tasks in explainable deep learning as well as
other tasks such as weakly-supervised segmentation.

References
Adebayo, J.; Gilmer, J.; Muelly, M.; Goodfellow, I.; Hardt,
M.; and Kim, B. 2018. Sanity checks for saliency maps. In
Advances in Neural Information Processing Systems, 9505–
9515.
Bach, S.; Binder, A.; Montavon, G.; Klauschen, F.; Müller,
K.; and Samek, W. 2015. On pixel-wise explanations for
non-linear classifier decisions by layer-wise relevance propagation. PLoS One 10. ISSN 1932-6203. doi:10.1371/journal.
pone.0130140.
Baek, Y.; Lee, B.; Han, D.; Yun, S.; and Lee, H. 2019. Character Region Awareness for Text Detection. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 9365–9374.

Visual Recognition Challenge. International Journal of Computer Vision .
Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;
Parikh, D.; and Batra, D. 2017. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.
In 2017 IEEE International Conference on Computer Vision
(ICCV), 618–626.
Shrikumar, A.; Greenside, P.; Shcherbina, A.; and Kundaje,
A. 2016. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences. CoRR
abs/1605.01713.
Simonyan, K.; Vedaldi, A.; and Zisserman, A. 2014. Deep
Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR Workshop .

Dabkowski, P.; and Gal, Y. 2017. Real time image saliency
for black box classifiers. In Advances in Neural Information
Processing Systems, 6967–6976.

Simonyan, K.; and Zisserman, A. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. International Conference on Learning Representations .

Dong, Y.; Liao, F.; Pang, T.; Su, H.; Zhu, J.; Hu, X.; and
Li, J. 2018. Boosting Adversarial Attacks With Momentum.
In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).

Springenberg, J.; Dosovitskiy, A.; Brox, T.; and Riedmiller,
M. 2015. Striving for Simplicity: The All Convolutional
Net. In ICLR Workshop. URL http://lmb.informatik.unifreiburg.de/Publications/2015/DB15a.

Fong, R.; Patrick, M.; and Vedaldi, A. 2019. Understanding
deep networks via extremal perturbations and smooth masks.
In Proceedings of the IEEE International Conference on
Computer Vision, 2950–2958.

Sundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic
Attribution for Deep Networks. In Precup, D.; and Teh, Y. W.,
eds., Proceedings of the 34th International Conference on
Machine Learning, 3319–3328. PMLR.

Fong, R. C.; and Vedaldi, A. 2017. Interpretable Explanations
of Black Boxes by Meaningful Perturbation. In 2017 IEEE
International Conference on Computer Vision (ICCV), 3449–
3457. ISSN 2380-7504. doi:10.1109/ICCV.2017.371.

Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan, D.;
Goodfellow, I.; and Fergus, R. 2014. Intriguing properties of
neural networks. In International Conference on Learning
Representations. URL http://arxiv.org/abs/1312.6199.

He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual
Learning for Image Recognition. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR).

Telea, A. 2004. An image inpainting technique based on the
fast marching method. Journal of graphics tools 9(1): 23–34.

Linda Wang, Z. Q. L.; and Wong, A. 2020. COVID-Net: A
Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest Radiography Images.

Zeiler, M. D.; and Fergus, R. 2014. Visualizing and Understanding Convolutional Networks. In Fleet, D.; Pajdla, T.;
Schiele, B.; and Tuytelaars, T., eds., Computer Vision – ECCV
2014, 818–833. Cham: Springer International Publishing.

Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;
DeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A.
2017. Automatic differentiation in PyTorch. In NIPS-W.

Zhang, J.; Lin, Z.; Brandt, J.; Shen, X.; and Sclaroff, S. 2016.
Top-down neural attention by excitation backprop. In European Conference on Computer Vision, 543–559. Springer.

Petsiuk, V.; Das, A.; and Saenko, K. 2018. RISE: Randomized Input Sampling for Explanation of Black-box Models.
In Proceedings of the British Machine Vision Conference
(BMVC).

Zhou, B.; Khosla, A.; Lapedriza, À.; Oliva, A.; and Torralba,
A. 2014. Object Detectors Emerge in Deep Scene CNNs.
CoRR abs/1412.6856. URL http://arxiv.org/abs/1412.6856.

Qi, Z.; Khorram, S.; and Fuxin, L. 2020. Visualizing deep
networks by optimizing with integrated gradients. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34.
Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. Why Should
I Trust You?: Explaining the Predictions of Any Classifier.
In Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 1135–
1144. ACM.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;
Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;
Berg, A. C.; and Fei-Fei, L. 2015. ImageNet Large Scale

Zhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; and Torralba,
A. 2016. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer
vision and pattern recognition, 2921–2929.

Supplementary Materials
Deletion (Mx ) v.s. Insertion (My ) Mask. Mx and My
masks are optimized with different objectives and exhibit
differences. To make the behavior of each of the Mx and My
masks more clear, the insertion and deletion scores for each
of them are reported in Table 5. As backed by the results, My
(insertion mask) tends to be more local and smooth while
Mx is often more scattered. Multiplying them together, as in
iGOS++ methodology, reduces adversariality and directs the
optimization out of saddle points that come with the combination of the dueling loss functions as mentioned previously. In
addition, it can be observed that in lower resolution (28×28),
Mx is less adversarial than in higher resolution (224×224).
Mx & My
Mx
My
Mxy (iGOS++)

224×224
Deletion Insertion
0.0268
0.5008
0.0594
0.7184
0.0328
0.7261

28×28
Deletion Insertion
0.1011
0.5536
0.1788
0.6912
0.0929
0.7332

Table 5: Comparison of the Insertion/Deletion scores from
iGOS++ with Mx and My masks at two different resolutions
using ResNet50.
Failure Case In Fig.1, we show an example of where our
method does not work properly. This image is incorrectly predicted by the network and has low prediction confidence. As
it can be observed, an adversarial mask has been generated
from our method as the insertion curve requires almost all
pixels to be inserted to get to the original confidence. Generally, when the initial prediction confidence is low, or the
network is predicted incorrectly, our method does not work
very well.

Figure 6: Failure case for our method by finding and adversarial
mask on 28 × 28. When the network initially has low prediction
confidence, our method does not work well. It can be observed that
the insertion score does not go up until near the end.

Set of Hyperparameters (Insertion/Deletion): To quantitatively evaluate our method (iGOS++) against baselines
in terms of the casual metrics, insertion and deletion scores,
we use ImageNet (Russakovsky et al. 2015) benchmark. For
the reported deletion/insertion scores, we use ResNet50
(He et al. 2016) and VGG19 (Simonyan and Zisserman
2015) pre-trained on the ImageNet (from the PyTorch model
zoo (Paszke et al. 2017)) and generate heatmaps for 5,000
randomly selected images from the ImageNet validation set.
All the baseline results presented in this paper are either
obtained from the published paper when applicable or by run-

ning the publicly available implementations. The choice of
hyperparameters is also chosen to have the best performance
— from the paper/code repositories. During all experiments,
for high-resolution mask (224×224), we set λ1 = 10, and
set it to 1 for all other resolutions. This is to avoid having
a diffuse mask as the penalty over the mask size can easily
be larger at high resolutions. In addition, λ2 is set to 20 for
all resolutions. The line search parameters are similar to
(Qi, Khorram, and Fuxin 2020). Also, We set β = 2 and
σ = 0.01 for the BTV term. BTV and TV term (with β = 2)
also can be averaged and added to the main objective. Note
that throughout all the experiments presented in the paper,
the same set of hyperparameters is used for all network
architectures (ResNet50, VGG19, COVID-NET, etc.) as well
as all datasets (ImageNet, Fashion-MNIST, COVIDx, etc.),
showcasing the robustness of the algorithm. We used the
publicly-available implementation for COVID-NET 1 and
the best F-1 score from the validation set is reported in the
paper.
Data Cleaning for the Generation of The COVIDx++
dataset: The first stage in cleansing the COVIDx dataset is
to detect the text in the X-ray images. For this purpose, the
CRAFT text detection method (Baek et al. 2019) is used. We
used the general pre-trained model available at the official
code repository 2 . The text confidence threshold of 0.7 is
used for the experiments. To inpaint inside the bounding
boxes detected from the CRAFT, we used the built-in
OpenCV inpainting function (cv2.inpaint(...)) with
the algorithm by (Telea 2004) and inpaint radius of 10 pixels.
Figure 7 shows some samples from the COVID-19 patients
in the COVIDx dataset (Linda Wang and Wong 2020), the
corresponding detected bounding boxes detected by the
CRAFT text detector, and the final inpainted images.
Running Time. The results from running time comparison
of our proposed method against the other perturbation-based
methods, namely, I-GOS (Qi, Khorram, and Fuxin 2020),
Mask (Fong and Vedaldi 2017), and RISE (Petsiuk, Das,
and Saenko 2018), are presented in Table 6 at two different
resolutions. We followed the setting in (Qi, Khorram, and
Fuxin 2020) and an NVIDIA GeForce GTX 1080 Ti GPU
along with an 8-core CPU is used for this experiment. The
reported numbers are averaged over 5,000 images from the
validation set of ImageNet. The gradient-based methods
are faster to compute as they only need one forward and
backward pass though the network — generally speaking,
this would take under one second per image to generate
a heatmap. However, the resolution of their generated
heatmaps is more restricted to the CNN architecture and not
as flexible as in our method. They also are shown to perform
poorer than our proposed method in both quantitative and
qualitative evaluations. We improved the implementation
code for (Qi, Khorram, and Fuxin 2020)3 and that is the
reason the running time reported is faster than what the
1

https://github.com/velebit-ai/COVID-Next-Pytorch
https://github.com/clovaai/CRAFT-pytorch
3
https://github.com/zhongangqi/IGOS
2

Figure 7: Example of X-ray images from COVID-19 patients in the COVIDx dataset (left column), the detected bounding boxed
using the CRAFT text detector pipeline (middle column), and the final inpainted images (COVIDx++ dataset).
original paper reported. The main reason our method is
slower than (Qi, Khorram, and Fuxin 2020) is due to
difference in calculating the step size using backtacking line
search as explained in the methodology section.
Convergence Behavior. Throughout our experiments, we
found the convergence behavior of our method robust against
the choice of hyperparameters. The choice of hyperparameters were also transferable between all the datasets and models as stated in the Set of Hyperparameters section. Table.
7, shows the final value of the combined deletion and inser-



tion loss fc Φ(I0 , I˜0 , M ) − fc Φ(I0 , I˜0 , 1 − M ) (eq. (4)
in the main paper) after optimizing with iGOS++ and the
naı̈ve extension of I-GOS (Qi, Khorram, and Fuxin 2020) —
when the insertion loss is directly added to it. The reported
numbers are averaged over 500 images for different choices
of hyperparameters λ1 and λ2 . Although, the naı̈ve extension is directly optimized over the combined deletion and
insertion loss, iGOS++ shows superior performance in all settings. Note the loss value can be negative since the insertion
loss is maximized during the optimization. This validates the
capability of iGOS++ in achieving a lower objective com-

Running
Time (s)
28×28
224×224

iGOS++

I-GOS

Mask

RISE

16.67
12.25

1.50
1.48

14.66
17.03

61.77

GradientBased
<1

Table 6: Running time comparison of iGOS++ against other
visualization methods on ResNet50 at two different resolutions. The shown numbers are averaged over 5,000 images
from the ImageNet validation set.
pared to using the naive extension of I-GOS for the same
objective, showing that it found better optima in our difficult
non-convex optimization problem.
Loss Value
I-GOS + Ins (naı̈ve)
iGOS++

λ1 =1
λ2 =2
-0.869
-0.971

λ1 =1
λ2 =20
-0.374
-0.649

λ1 =10
λ2 =20
0.346
0.158

λ1 =10
λ2 =200
0.693
0.654

Table 7: Optimization objective comparison with naive addition of the insertion loss at 28×28 resolution, averaged
over 500 images. Our algorithm resulted a lower objective
than the naive version for the same objective, showing that it
found better optima in this difficult non-convex optimization
problem. Note that λ2 = 200 was never used in the actual
experiments and only included here for completeness. Also
note the loss could be negative because there is a negative
sign on the insertion loss in eq. (4) in the main paper
iGOS++: Visual Comparisons. Figure. 8 compares the
visual explanation from iGOS++ with other gradient-based
(Grad-Cam (Selvaraju et al. 2017), Integrated-Gradient (Sundararajan, Taly, and Yan 2017), and Gradient (Simonyan,
Vedaldi, and Zisserman 2014).) and perturbation-based methods (I-GOS (Qi, Khorram, and Fuxin 2020), RISE (Petsiuk,
Das, and Saenko 2018)). The images are selected from the
ImageNet validation set, and the ResNet50 (He et al. 2016)
is used as the classifier. The AUC for the deletion and insertion metrics are indicated under each visualization (for the
deletion AUC, lower is better. For the insertion AUC, higher
is better). For RISE visualizations, 8000 7×7 random masks
with p = 0.5 are generated. The official implementations
have been used for all the visualizations.
In Fig. 8, it can be noted that Grad-Cam and RISE, due to
the nature of their low-resolution masks (7×7), highlight irrelevant regions in the image. In addition, Gradient method
is calculated by the infinitesimal changes at the input image
that change the prediction. To that end, as it can be seen
in the figure, its visualizations are diffuse and not intuitive
to human understanding. The same issue can be noted for
Integrated-Gradient, however, it shows a good deletion score.
Yet, Integrated-Gradient suffers in the insertion score. Similar to iGOS++, I-GOS has a flexibility in generating various
resolution masks that can be chosen depending on the task at
hand. Nevertheless, the visualizations from I-GOS are more
scattered compared to the iGOS++ visualization (which are
local on the object) and it suffers more on the insertion score.
As an example, the visualization for the ”BulBul” image
(third row from the top) clearly underlines this issue.

Figure 9 compares the visualizations from iGOS++ at different resolutions. Our method has flexibility in the resolution
of the generated mask and it can go from low-resolution and
coarse visualizations (e.g. at 7×7) to high-resolution and
detailed explanations (e.g. 224×224). It can be noted, when
multiple objects (e.g. bottom four rows) are present in the
image, low resolution masks perform poorer in locating them.
This improves as the resolution of the explanations increases.
For example, the ”Granny Smith” image in the Fig. 9 illustrates this. In addition, the visualization for the ”Bullmastiff”
image shows that iGOS++, unlike some visualization methods that perform (partial) image recovery (Adebayo et al.
2018), generates faithfull explanations when objects from
different classes exist in the image.
To visually compare the smoothness from our proposed
BTV term with the TV term introduced in (Fong and Vedaldi
2017), refer to Figure 10 where the iGOS++ visualizations
are shown for different smoothness losses as well as different values for their penalty constant λ2 . For this analysis,
ResNet50 architecture is used. The generated mask for the
”Tiger Beetle” are at 224×224 resolution. It can be seen
that when the BTV term is used (as in the original iGOS++
method), the visualizations are local to the object while for
the TV term the visualizations are still scattered. For further
details on the BTV term refer to the methodology section in
the main paper. λ2 = 20 is used in all of our experiments and
different values in the Fig. 10 are for highlighting the effect
of the smoothness loss.
COVID-19. Figure. 11 showcases further examples that
using our visualization method can give insights to the underlying decision making process of the classifier and potentially
debuging and improving it, as stated in the main paper. It
can be seen that the iGOS++ visualizations (second column)
are highly weighting the text region in the images, i.e. the
letter ”R”. The third column shows images when only a small
fraction of the pixels (e.g. 6%), referred to as Pixel Ratio
(PR), are inserted back into the baseline image (a Gaussianblurred of the original image). However, these small amount
of pixels, which mostly include the text region, are predicted
as COVID-19 classes with their corresponding confidence
(shown in green color). On the other hand, using the same
iGOS++ visualizations, when only a small fraction of the pixels (e.g. PR: 12%) are removed from the original image, the
classifier shows different predictions (than COVID-19). The
corresponding confidence (shown in red color) is depicted on
the top of the perturbed images. It should be noted that this
confidence is not high enough so that the images get classify
as COVID-19.
Figure 12 shows examples of the case when the sole insertion of the text region into a baseline is enough for the
classifier to change its prediction, falsely to COVID-19. For
the purpose of this analysis, we used a few examples of Xray images from COVID-19 (Fig. 12a) and Pneumonia (Fig.
12b) patients, depicted in the left columns. When feeding the
highly-blurred version of these images to the classifier, they
were all predicted as Normal (the second columns). However,
when only revealing the text regions back into the baselines
(using the bounding boxed from the CRAFT text detector
(Baek et al. 2019)), the classifier falsely predicts all those

images as COVID-19, shown by red color (third columns).
This suggests that the text regions are directly influencing the
decision making of the classifier, a bug that we hypothesized
using our visualizing method. Nevertheless, in reality, one
assumes these should be regarded as noise in the data and
should be discarded by the classifier. This is an example of
the fragility of trusting the decision making of the black-box
classifiers, particularly in tasks where the human life is at
stake.

Figure 8: iGOS++ visual explanations for different classes (written on the left) compared with other gradient and perturbation based
visualization methods: I-GOS (Qi, Khorram, and Fuxin 2020), RISE (Petsiuk, Das, and Saenko 2018), Grad-Cam (Selvaraju et al. 2017),
Integrated-Gradient (Sundararajan, Taly, and Yan 2017), and Gradient (Simonyan, Vedaldi, and Zisserman 2014). For the deletion AUC, lower
is better. For the insertion AUC, higher is better. (Best viewed in color.)

res

Figure 9: iGOS++ visual explanations for different classes (written on the left) at different mask resolutions (written on the top). This figure
shows the flexibility of our method in finding the most salient regions in the image from coarse and low resolution explanations (e.g. 7×7) to
refined and high resolution explanation (e.g. 224×224). The last four rows also demonstrate examples where multiple objects are present in the
image — both from the same class, as in the ”Ostrich”, ”Kite”, and ”Granny Smith” images, and from different classes, as in the ”Bullmastiff”
image. iGOS++ shows reliable explanation in both cases.

Figure 10: Smoothness comparison of our proposed BTV term with the TV term (Fong and Vedaldi 2017) for the iGOS++ visualizations.
Top row represents iGOS++ (with our proposed BTV term) while the bottom row shows when the TV term (Fong and Vedaldi 2017) is used
instead. Each column shows different value for λ2 hyperparameter (smoothness penalty). As it can be observed, when the λ2 value increases,
the generated masks become more smooth. In our experiments, we used λ2 = 20 for all resolutions, models, and datasets. In addtion, it can be
noted that BTV outperforms the TV as it generates a less scattered and local visualization which is more intuitive for human interpretation.

Figure 11: Showcase for capability of the iGOS++ in detecting bugs in a COVID-19 classification pipeline. The X-ray images in
the first column belong to COVID-19 patients. The second column shows iGOS++ visualizations at 32×32 resolution where our
method highlights the text region, the letter ”R”. The third column shows images when only a small fraction of the pixels (e.g.
6%), referred to as Pixel Ratio (PR), are inserted back into the baseline. Interestingly, the insertion of these small amount of
pixels, which mostly include the text region, is enough for the perturbed images to be classified as COVID-19 (The corresponding
confidence scores are shown in color green). The fourth column shows the opposite game where only a small fraction of the pixels
(e.g. PR: 12%) are removed (blurred) from the original image and that is enough for the perturbed images to be mis-classified, i.e.
not COVID-19. Obviously, the confidence score for these images (shown in color red) are lower than of Normal or Pneumonia
classes.

(a)

(b)

Figure 12: The prediction of the classifier for each image is shown on the top. X-ray images of a) COIVD-19 b) Pneumonia patients from the
COVIDx dataset (Linda Wang and Wong 2020) are depicted in the left columns. When the highly-blurred version of these images are fed to the
classifier, they are all predicted as Normal (middle columns). However, when only the text regions are inserted back into the baselines, the
classifier falsely predicts all those images as COVID-19, shown by red color (third columns).This suggests that the text regions are directly
influencing the decision making of the classifier.

