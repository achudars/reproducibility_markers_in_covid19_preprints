The COVID-19 Infodemic: Twitter versus Facebook

arXiv:2012.09353v2 [cs.SI] 3 Apr 2021

Kai-Cheng Yang,1 Francesco Pierri,1, 2 Pik-Mai Hui,1 David Axelrod,1
Christopher Torres-Lugo,1 John Bryden,1 and Filippo Menczer1
1

Observatory on Social Media, Indiana University, Bloomington, USA
2
Dipartimento di Elettronica, Informatica e Bioingegneria,
Politecnico di Milano, Italy
Abstract
The global spread of the novel coronavirus is affected by the spread of related misinformation — the so-called COVID-19 Infodemic — that makes populations more
vulnerable to the disease through resistance to mitigation efforts. Here we analyze
the prevalence and diffusion of links to low-credibility content about the pandemic
across two major social media platforms, Twitter and Facebook. We characterize crossplatform similarities and differences in popular sources, diffusion patterns, influencers,
coordination, and automation. Comparing the two platforms, we find divergence among
the prevalence of popular low-credibility sources and suspicious videos. A minority of
accounts and pages exert a strong influence on each platform. These misinformation
“superspreaders” are often associated with the low-credibility sources and tend to be
verified by the platforms. On both platforms, there is evidence of coordinated sharing
of Infodemic content. The overt nature of this manipulation points to the need for
societal-level solutions in addition to mitigation strategies within the platforms. However, we highlight limits imposed by inconsistent data-access policies on our capability
to study harmful manipulations of information ecosystems.

1

Introduction

The impact of the COVID-19 pandemic has been felt globally, with almost 70 million detected cases and 1.5 million deaths as of December 2020 (coronavirus.jhu.edu/map.html).
Epidemiological strategies to combat the virus require collective behavioral changes. To
this end, it is important that people receive coherent and accurate information from media
sources that they trust. Within this context, the spread of false narratives in our information environment can have acutely negative repercussions on public health and safety.
For example, misinformation about masks greatly contributed to low adoption rates and
increased disease transmission [1]. The problem is not going away any time soon: false
vaccine narratives [2] will drive hesitancy, making it difficult to reach herd immunity and
prevent future outbreaks.
It is concerning that many people believe, and many more have been exposed to, misinformation about the pandemic [3, 4, 5, 6]. The spread of this misinformation has been
termed the Infodemic [7]. Social media play a strong role in propagating misinformation
because of peer-to-peer transmission [8]. There is also evidence that social media are manipulated [9, 10] and used to spread COVID-19 misinformation [11]. It is therefore important
1

to better understand how users disseminate misinformation across social media networks.
As described in the literature reviewed in the next section, a limitation of most existing
studies regarding misinformation spreading on social media is that they focus on a single
platform. However, the modern information ecosystem consists of different platforms over
which information propagates concurrently and in diverse ways. Each platform can have
different vulnerabilities [12]. A key goal of the present work is to compare and contrast the
extent to which the Infodemic has spread on Twitter and Facebook.
A second gap in our understanding of COVID-19 misinformation is in the patterns of
diffusion within social media. It is important to understand how certain user accounts, or
groups of accounts, can play a disproportionate role in amplifying the spread of misinformation. Inauthentic social media accounts, known as social bots and trolls, can play an
important role in amplifying the spread of COVID-19 misinformation on Twitter [13]. The
picture on Facebook is less clear, as there is little access to data that would enable a determination of social bot activity. It is, however, possible to look for evidence of manipulation
in how multiple accounts can be coordinated with one another, potentially controlled by a
single entity. For example, accounts may exhibit suspiciously similar sharing behaviors [14].
We extract website links from social media posts that include COVID-19 related keywords. We identify a link with low-credibility content in one of two ways. First, we follow
the convention of classifying misinformation at the source rather than the article level [15].
We do this by matching links to an independently-generated corpus of low-credibility website
domains (or sources). Second, in the case of links to YouTube, we label videos as suspicious if they have been banned by the site or are otherwise unavailable to the public. This
enables us to quantify the prevalence of individual uploads likely to propagate COVID-19
misinformation and the different ways in which they are shared on Twitter and Facebook.
The main contributions of this study stem from exploring three sets of research questions:
1. What is the prevalence of low-credibility content on Twitter and Facebook? Are there
similarities in how sources are shared over time? How does this activity compare to
that of popular high-credibility sources? Are the same suspicious sources and YouTube
videos shared in similar volumes across the two platforms?
2. Is the sharing of misinformation concentrated around a few active accounts? Do a few
influential accounts dominate the resharing of popular misinformation? What is the
role of verified accounts and those associated with the low-credibility sources on the
two platforms?
3. Is there evidence of inauthentic coordinated behavior in sharing low-credibility content? Can we identify clusters of users, pages, or groups with suspiciously similar
sharing patterns? Is low-credibility content amplified by Twitter bots more prevalent
on Twitter as compared to Facebook?
After systematically reviewing the literature regarding health misinformation on social
media in the next section, we describe the methodology and data employed in our analyses.
The following three sections present results to answer the above research questions. Finally,
we discuss the limitations of our analyses and implications of our findings for mitigation
strategies.

2

2

Literature review

Concerns regarding online health-related misinformation existed before the advent of online social media. Studies mostly focused on evaluating the quality of information on the
web [16], and a new research field emerged, namely “infodemiology,” to assess health-related
information on the Internet and address the gap between expert knowledge and public perception [17].
With the wide adoption of online social media, the information ecosystem has seen
large changes. Peer-to-peer communication can greatly amplify fake or misleading messages
by any individual [8]. Many studies reported on the presence of misinformation on social
media during the time of epidemics such as Ebola [18, 19, 20, 21] and Zika [22, 23, 24, 25].
Misinformation surrounding vaccines has been particularly persistent and is likely to reoccur
whenever the topic comes into public focus [26, 27, 28, 29, 30, 31].
These studies focused on specific social media platforms including Twitter [26, 25], Facebook [23, 30], Instagram [22], and YouTube [28, 24]. The most common approach was
content-based analysis of sampled social media posts, images, and videos to gauge the topics of online discussions and estimate the prevalence of misinformation. Unfortunately, the
datasets analysed in these studies were usually small (at a scale of hundreds or thousands
of items) due to difficulties in accessing and manually annotating large scale collections.
Unsurprisingly, the COVID-19 pandemic has inspired a new wave of health misinformation studies. In addition to traditional approaches like qualitative analyses of social media
content [32, 33, 34, 35, 36, 37] and survey studies [3, 5, 6], quantitative studies on the prevalence of links to low-credibility websites at scale have gained popularity in light of the recent
development of computational methods [38, 39, 40, 41, 42, 43, 44].
Many of these studies aimed to assess the prevalence of, and exposure to, COVID-19
misinformation on online social media [45]. However, different approaches yielded disparate
estimates of misinformation prevalence levels ranging from as little as 1% to as much as
70%. These widely varying statistics indicate that different approaches to experimental
design, including uneven access to data on different platforms and inconsistent definitions
of misinformation, can generate inconclusive or misleading results. In this study, we follow
the reasoning from Gallotti et al. that it is better to clearly define a misinformation metric,
and then use it in a comparative way to look at how misinformation varies over time or is
influenced by other factors [40].
We identify two gaps in the literature reviewed above. First, it is still unclear how the
spreading patterns can differ on different social networks since studies comparing multiple
platforms are rare. This might be due to the obstacles in accessing data from different
sources simultaneously and the lack of a unified framework to compare very different services. Second, our understanding of the role that different account groups play during the
misinformation dissemination is very limited. We hope to address these gaps in the present
study.

3

Methods

In this section we describe in detail the methodology employed in our analyses, allowing
other researchers to replicate our approach. The outline is as follows: we collect social
media data from Twitter and Facebook using the same keywords list. We then identify lowand high-credibility content from the tweets and posts automatically by tracking the URLs

3

Table 1: List of high-credibility sources.
huffpost.com
newyorker.com
msnbc.com
newsweek.com
cnn.com
nytimes.com
economist.com
time.com
washingtonpost.com reuters.com
apnews.com
npr.org
usatoday.com
wsj.com
foxnews.com
marketwatch.com
nypost.com
dailycaller.com
theblaze.com
dailywire.com
cdc.gov
who.int

linking to the domains in a pre-defined list. Finally, we identify suspicious YouTube videos
by their availability status.

3.1

Identification of low-credibility information

We focus on news articles linked in social media posts and identify those pertaining to lowcredibility domains by matching the URLs to sources, following a corpus of literature [15, 9,
46, 47, 48]. We define our list of low-credibility domains based on information provided by
the Media Bias/Fact Check website (MBFC, mediabiasfactcheck.com), an independent
organization that reviews and rates the reliability of news sources. We gather the sources
labeled by MBFC as having a “Very Low” or “Low” factual-reporting level. We then add
“Questionable” or “Conspiracy-Pseudoscience” sources and we leave out those with factualreporting levels of “Mostly-Factual,” “High,” or “Very High.” We remark that although
many websites exhibit specific political leanings, these do not affect inclusion in the list.
The list has 674 low-credibility domains [49].

3.2

High-credibility sources

As a benchmark for interpreting the prevalence of low-credibility content, we also curate
a list of 20 more credible information sources. We start from the list provided in a recent
Pew Research Center report [50] and used in a few studies on online disinformation [51,
52], and we select popular news outlets that cover the full U.S. political spectrum. These
sources have a MBFC factual-reporting level of “Mixed” or higher. In addition, we include
the websites of two organizations that acted as authoritative sources of COVID-19 related
information, namely the Centers for Disease Control and Prevention (CDC) and World
Health Organization (WHO). For simplicity we refer to the full list in Table 1 as highcredibility sources.

3.3

Data collection

We collect data related to COVID-19 from both Twitter and Facebook. To provide a
general and unbiased view of the discussion, we chose the following generic query terms:
coronavirus, covid (to capture keywords like covid19 and covid-19), and sars (to capture sars-cov-2 and related variations).
4

Original tweets

Tweet

Tweet
Retweet

Root users

Leaf users

Original posts

Post

Reshare
Comment
React

Post

Root groups/pages

Leaf users

Figure 1: Structure of the data collected from Twitter and Facebook. On Twitter, we
have the information about original tweets, retweets, and all the accounts involved. On
Facebook, we have information about original posts and public groups/pages that posted
them. For each post, we also have aggregate numbers of reshares, comments, and reactions,
with no information about the users responsible for those interactions.
3.3.1

Twitter data.

Our Twitter data was collected using an API from the Observatory on Social Media [53],
which allows to search tweets from the Decahose, a 10% random sample of public tweets.
We searched for English tweets containing the keywords between Jan. 1 and Oct. 31, 2020,
resulting in over 53M tweets posted by about 12M users. Note that since the Decahose
samples tweets and not users, the sample of users in our Twitter dataset is biased toward
more active users.
Our collection contains two types of tweets, namely original tweets and retweets. The
content of original tweets is published by users directly, while retweets are generally used
to endorse/amplify original tweets by others (no quoted tweets are included). We refer to
authors of original tweets as “root” users, and to authors of retweets as “leaf” users (see
Fig. 1).
3.3.2

Facebook data.

We used the posts/search endpoint of the CrowdTangle API [54] to collect data from Facebook. We filtered the entire set of English posts published by public pages and groups in
the period from Jan. 1 to Oct. 31, 2020 using the above list of keywords, resulting in over
37M posts by over 140k public pages/groups.
5

Figure 2: Pearson correlation coefficients between Facebook metrics aggregated at the
domain level for low-credibility domains. A reaction can be a “like,” “love,” “wow,” “haha,”
“sad,” “angry,” or “care.” All correlations are significant (p < 0.01).
Our Facebook data collection is limited by the coverage of pages and groups in CrowdTangle, a public tool owned and operated by Facebook. CrowdTangle includes over 6M
Facebook pages and groups: all those with at least 100k followers/members, U.S. based
public groups with at least 2k members, and a very small subset of verified profiles that can
be followed like public pages. We include these public accounts among pages and groups.
In addition, some pages and groups with fewer followers and members are also included by
CrowdTangle upon request from users. This might bias the dataset in ways that are hard
to gauge. For example, requests from researchers interested in monitoring low-credibility
pages and groups might lead to over-representation of such content.
As shown in Fig. 1, the collected data contains information about original Facebook
posts and the pages/groups that published these posts. For each post, we also have access to
aggregate statistics such as the number of reshares, comments, and reactions (e.g., “likes”)
by Facebook users. The numbers of comments and reactions are highly correlated with
reshares (Fig. 2), so we focus on reshares in this study.
Similarly to Twitter, Facebook pages and groups that publish posts are referred to as
“roots” and users who reshare them are “leaves.” However, in contrast to Twitter, we don’t
have access to any information about leaf users on Facebook. We refer generically to Twitter
users and Facebook pages and groups as “accounts.”
To compare Facebook and Twitter in a meaningful way, we compare root users with root
pages/groups, original tweets with original posts, and retweet counts with reshare counts.
We define prevalence as the sum of original tweets and retweets on Twitter, and as the sum
of original posts and reshares on Facebook.
3.3.3

YouTube data.

We observed a high prevalence of links pointing to youtube.com on both platforms —
over 64k videos on Twitter and 204k on Facebook. Therefore, we also provide an analysis

6

Table 2:
bit.ly
goo.gl
buff.ly
nyp.st
reut.rs
rebrand.ly
t.co
bitly.com
wp.me
mol.im
usat.ly
crwd.fr
owl.li

List of URL
dlvr.it
ift.tt
back.ly
dailysign.al
drudge.tw
covfefe.bz
shr.lc
crfrm.us
voat.co
read.bi
aje.io
zpr.io

shortening
liicr.nl
ow.ly
amzn.to
j.mp
shar.es
trib.al
po.st
flip.it
zurl.co
disq.us
sc.mp
scq.io

services.
tinyurl.com
fxn.ws
nyti.ms
wapo.st
sumo.ly
yhoo.it
dld.bz
mf.tt
fw.to
tmsnrt.rs
gop.cm
trib.in

of popular videos published on Facebook and Twitter. Specifically, we focus on popular
YouTube videos that are likely to contain low-credibility content. An approach analogous
to the way we label links to websites would be to identify sources that upload low-credibility
videos and then label every video from those sources as misinformation. However, this
approach is infeasible because the list of YouTube channels would be huge and fluid. To
circumvent this difficulty, we use removal of videos by YouTube as a proxy to label lowcredibility content. We additionally consider private videos to be suspicious, since this can
be used as a tactic to evade the platform’s sanctions when violating terms of service.
To identify the most popular and suspicious YouTube content, we first select the 16,669
videos shared at least once on both platforms. We then query the YouTube API Videos:list
endpoint to collect their metadata and focus on the 1,828 (11%) videos that had been
removed or made private. To validate this approach for identifying low credibility YouTube
content, we follow a two-step manual inspection process for a sample of about 3% of the
unavailable videos, comprising a mix of randomly selected and popular ones. We first search
for the deleted video IDs in other YouTube videos and web pages. When these references
contain the deleted videos’ titles, we search for these titles on bitchute.com to find copies
of the original videos. This process allows us to identify the narratives of 40 deleted videos,
90% of which contain misinformation. A similar approach was also adopted by [55] in their
recent study of COVID-19 misinformation on YouTube.
3.3.4

Ethical considerations.

Our studies of public Twitter and Facebook data have been granted exemption from IRB
review (Indiana University protocols 1102004860 and 10702, respectively). The data collection and analysis are also in compliance with the terms of service of the corresponding
social media platforms.

3.4

Link extraction

Estimating the prevalence of low-credibility information requires matching URLs, extracted
from tweets and Facebook metadata, against our lists of low- and high-credibility websites.
As shortened links are very common, we also identified 49 link shortening services that

7

Table 3: Breakdown of Facebook and Twitter posts/tweets matched to low- and highcredibility domains.
Low-credibility High-credibility
Facebook
Original posts
303,119
1,194,634
Reshares
20,462,035
98,415,973
Twitter
Original tweets
245,620
734,409
Retweets
653,415
2,184,050

appear at least 50 times in our datasets (Table 2) and expanded shortened URLs referring
to these services through HTTP requests to obtain the actual domains. We finally match
the extracted and expanded links against the lists of low- and high-credibility domains. A
breakdown of matched posts/tweets is shown in Table 3. For low-credibility content, the
ratio of retweets to tweets is 2.7:1, while the ratio of reshares to posts is 68:1. This large
discrepancy is due to various factors: the difference in traffic on the two platforms, the fact
that we only have a 10% sample of tweets, and the bias toward popular pages and groups
on Facebook.

4

Infodemic prevalence

In this section, we provide results about the prevalence of links to low-credibility domains
on the two platforms. As described in the Methods section, we sum tweets and retweets
for Twitter, and original posts and reshares for Facebook. Note that deleted content is not
included in our data. Therefore, our estimations should be considered as lower bounds for
the prevalence of low-credibility information on both platforms.

4.1

Prevalence trends

We plot the daily prevalence of links to low-credibility sources on Twitter and Facebook
in Fig. 3(a). The two time series are strongly correlated (Pearson r = 0.87, p < 0.01).
They both experience a drastic growth during March, when the number of COVID-19 cases
was growing worldwide. Towards summer, the prevalence of low-credibility information
decreases to a relatively low level and then becomes more stable.
To analyze the Infodemic surge with respect to the pandemic’s development and public
awareness, Fig. 3(b) shows the worldwide hospitalization rate and the overall volume of
tweets in our collection. The Infodemic surge roughly coincides with the general attention
given to the pandemic, captured by the overall Twitter volume. The peak in hospitalizations
trails by a few weeks. A similar delay was recently reported between peaks of exposure to
Infodemic tweets and of COVID-19 cases in different countries [40]. This plot suggests that
the delay is related to general attention toward the pandemic rather than specifically toward
misinformation.
To further explore whether the decrease in low-credibility information is organic or due
to platform interventions, we also compare the prevalence of low-credibility content to that
of links to credible sources. As shown in Fig. 3(c), the ratios are relatively stable across
the observation period. These results suggest that the prevalence of low-credibility content

8

1e4
(a)

1e5
Twitter
Facebook

0.50

Hospitalizations / million Facebook low-cred volume

0.75

3
2
1

0.25
0.00

Low-/high-cred volume ratio Total number of tweets

Twitter low-cred volume

1.00

1e6
1.0 (b)

1e3
Tweets
Hosp. rate

0
4

0.5

2

0.0

0

0.6
0.4
0.2
0.0

(c)
. 1 . 1 . 1 . 1 1 . 1 . 1 . 1 . 1 . 1 31
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Oct.

Figure 3: Infodemic content surge on both platforms around the COVID-19 pandemic waves,
from Jan. 1 to Oct. 31, 2020. All curves are smoothed via 7-day moving averages. (a) Daily
volume of posts/tweets linking to low-credibility domains on Twitter and Facebook. Left
and right axes have different scales and correspond to Twitter and Facebook, respectively.
(b) Overall daily volume of pandemic-related tweets and worldwide COVID-19 hospitalization rates (data source: Johns Hopkins University). (c) Daily ratio of volume of lowcredibility links to volume of high-credibility links on Twitter and Facebook. The noise
fluctuations in early January are due to low volume. The horizontal lines indicate averages
across the period starting Feb. 1.

9

(a)

low cred combined
cnn.com
foxnews.com
nytimes.com
washingtonpost.com
npr.org
dailywire.com
nypost.com
msnbc.com
huffpost.com
cdc.gov
newsweek.com
breitbart.com
washingtontimes.com
usatoday.com
reuters.com
theblaze.com
wsj.com
dailycaller.com
cnsnews.com
time.com
who.int
cbn.com
pjmedia.com
apnews.com
rt.com
wnd.com
newyorker.com
dailystar.co.uk
iflscience.com
analyzingamerica.org
trendingpolitics.com
thepoliticalinsider.com
hannity.com
economist.com
politicalflare.com
marketwatch.com
bongino.com
judicialwatch.org
thefederalistpapers.org
100

Facebook

Twitter
(b)
low cred combined
nytimes.com
cnn.com
washingtonpost.com
thegatewaypundit.com
reuters.com
foxnews.com
breitbart.com
nypost.com
npr.org
wsj.com
usatoday.com
newsweek.com
msnbc.com
huffpost.com
dailycaller.com
truepundit.com
zerohedge.com
cdc.gov
apnews.com
theblaze.com
time.com
washingtontimes.com
dailywire.com
economist.com
rt.com
swarajyamag.com
newyorker.com
marketwatch.com
summit.news
pjmedia.com
waynedupree.com
bongino.com
who.int
trendingpolitics.com
davidicke.com
oann.com
bluntforcetruth.com
Low-cred
Low-cred
infowars.com
High-cred
High-cred
bigleaguepolitics.com
4
4
2
6
8
0
2
10
10
10
10
10
10
10
106
Number of posts + reshares
Number of tweets + retweets

Figure 4:
Total prevalence of links to low- and high-credibility domains on both (a)
Facebook and (b) Twitter. Due to space limitation, we only show the 40 most frequent
domains on the two platforms. The high-credibility domains are all within the top 40. We
also show low-credibility information as a whole (cf. “low cred combined”).
is mostly driven by the public attention to the pandemic in general, which progressively
decreases after the initial outbreak. We finally observe that Twitter exhibits a higher ratio
of low-credibility information than Facebook (32% vs. 21% on average).

4.2

Prevalence of specific domains

We use the high-credibility domains as a benchmark to assess the prevalence of low-credibility
domains on each platform. As shown in Fig. 4, we notice that the low-credibility sources
exhibit disparate levels of prevalence. Low-credibility content as a whole reaches considerable volume on both platforms, with prevalence surpassing every single high-credibility
domain considered in this study. On the other hand, low-credibility domains generally
exhibit much lower prevalence compared to high-credibility ones (with a few exceptions,
notably thegatewaypundit.com and breitbart.com).

10

breitbart.com
washingtontimes.com
pjmedia.com
trendingpolitics.com
wnd.com
dailystar.co.uk
cbn.com

rt.com
bongino.com
oann.com

thegatewaypundit.com
swarajyamag.com
waynedupree.com

gellerreport.com
100percentfedup.com
thepoliticalinsider.com
hannity.com

nk
ra

nk
ra
er
itt
Tw

nk

nk

ra

ra

More popular on Twitter

k

k

oo

More popular on Facebook

oo

(b)

sputniknews.com
thenationalpulse.com

eb

b
ce

Fa

(a)

newspunch.com

c
Fa

newswars.com
conservativefighters.com
pantsonfirenews.com

er

awm.com
leftaction.com
awarenessact.com

zerohedge.com
americanthinker.com

itt

truepundit.com
summit.news
infowars.com
bluntforcetruth.com
gnews.org
zenith.news

Tw

iflscience.com
focusonthefamily.com
bipartisanreport.com
analyzingamerica.org
iheartintelligence.com
thefederalistpapers.org

Figure 5: (a) Rank comparison of low-credibility sources on Facebook and Twitter. Each
dot in the figure represents a low-credibility domain. The most popular domain ranks first.
Domains close to the vertical line have similar ranks on the two platforms. Domains close to
the edges are much more popular on one platform or the other. We annotate a few selected
domains that exhibit high rank discrepancy. (b) A zoom-in on the sources ranked among
the top 50 on both platforms (highlighted square in (a)).

4.3

Source popularity comparison

As shown in Fig. 4, we observe that low-credibility websites may have different prevalence
on the two platforms. To further contrast their prevalence levels on Twitter and Facebook,
we measure the popularity of websites on each platform by ranking them by prevalence, and
then compare the resulting ranks in Fig. 5. The ranks on the two platforms are not strongly
correlated (Spearman r = 0.57, p < 0.01). A few domains are much more popular or only
appear on one of the platforms (see annotations in Fig. 5(a)). We also show the domains
that are very popular on both platforms in Fig. 5(b). They are dominated by right-wing
and state sources, such as breitbart.com, washingtontimes.com, thegatewaypundit.com,
oann.com, and rt.com.

4.4

YouTube Infodemic content

Thus far, we examined the prevalence of links to low-credibility web page sources. However, a significant portion of the links shared on Twitter and Facebook point to YouTube
videos, which can also carry COVID-19 misinformation. Previous work has shown that bad
actors utilize YouTube in this manner for their campaigns [56]. Specifically, anti-scientific
narratives on YouTube about vaccines, Idiopathic Pulmonary Fibrosis, and the COVID-19
pandemic have been documented [28, 57, 37, 55].
To measure the prevalence of Infodemic content introduced from YouTube, we consider
the unavailability (deletion or private status) of videos as an indicator of suspicious content,
as explained in the Methods section. Fig. 6 compares the prevalence rankings on Twitter
and Facebook for unavailable videos ranked within the top 500 on both platforms. These

11

David Icke talks about a whole set of conspiracy
theories about COVID-19

Dr. Richard Bartlett claims that Budesonide is
effective for treating COVID-19

Dr. Shiva Ayyadurai talks about deep state
conspiracy theory in an interview

Carrie Madej talks about COVID-19 vaccine
misinformation and conspiracy theory
Vladimir Zelenko talks about curing Coronavirus
with Zinc, Hydroxychloroquine, and Azythromycin
in an interview

Dr. Rashid Buttar reveals that coronavirus was
patented by Bill Gates

Tw

ok

itt

bo

er

ra

ce
Fa
ra

Dr. Anthony Fauci's ex-employee attacks Fauci
The Last American Vagabond conspiracy theory
video

nk

Clip of Plandemic featuring Judy Mikovits

OAN video with multiple doctors talking about
inaccurate information like hydroxychloroquine
being safe and effective for COVID-19

nk

The Charlie Kirk Show with multiple doctors talking
about inaccurate information like hydroxychloroquine being safe and effective for COVID-19

Hydroxychloroquin being the cure for COVID-19
More popular on Twitter

More popular on Facebook

Dr. Shiva Ayyadurai talks about COVID-19
conspiracy theories in an interview

Figure 6: Rank comparison of suspicious YouTube videos within the top 500 on both
Facebook and Twitter. The most popular video ranks first. Each dot in the figure represents
a suspicious video. Videos close to the vertical line have similar ranks on both platforms.
Videos close to the edges are more popular on one platform or the other. We annotated a
few selected videos with their narratives extracted from their copies on bitchute.com or
other web pages.
videos are linked between 6 and 980 times on Twitter and between 39 and 64,257 times on
Facebook. While we cannot apply standard rank correlation measures due to the exclusion
of low-prevalence videos, we do not observe a correlation in the cross-platform popularity
of suspicious content from a qualitative inspection of the figure. A caveat to this analysis
is that the same video content (sometimes re-edited) can be recycled within many video
uploads, each having a unique video ID. Some of these videos are promptly removed while
others are not. Therefore, the lack of correlation could partly be driven by YouTube’s efforts
to remove Infodemic content in conjunction with attempts by uploaders to counter those
efforts [55].
Having looked at the prevalence of suspicious content from YouTube, we wish to explore
the question from another angle: are videos that are popular on Facebook or Twitter more
likely to be flagged as suspicious? Fig. 7 shows this to be the case on both platforms: a
larger portion of videos with higher prevalence are unavailable, but the trend is stronger
on Twitter than on Facebook. The overall trend suggests that YouTube may have a bias
toward moderating videos that attract more attention. This may be a function of the fact
that an Infodemic video that is spreading virally on Twitter/Facebook may receive more
abuse reports through YouTube’s reporting mechanism. The fact that this trend is greater
on Twitter may be explained by the differences between each platform’s demographics.
Survey data cited in the Discussion section shows that Twitter users are younger and more
educated; it is therefore plausible that the average Twitter user is more likely to report
unreliable content.

5

Infodemic spreaders

Links to low-credibility sources are published on social media through original tweets and
posts first, then retweeted and reshared by leaf users. In this section, we study this dis-

12

Percentage of unavailable videos

20%

Twitter
Facebook

15%
10%
5%
0%

0-20%
20-40%
40-60%
YouTube video rank by prevalence

Figure 7: Percentages of suspicious YouTube videos against their percent rank among all
videos linked from pandemic-related tweets/posts on both Twitter and Facebook.
semination process on Twitter and Facebook with a focus on the top spreaders, or “superspreaders”: those disproportionately responsible for the distribution of Infodemic content.

5.1

Concentration of influence

We wish to measure whether low-credibility content originates from a wide range of users,
or can be attributed to a few influential actors. For example, a source published 100 times
could owe its popularity to 100 distinct users, or to a single root whose post is republished
by 99 leaf users. To quantify the concentration of original posts/tweets or reshares/retweets
for a source s, we use the inverse normalized entropy [58], defined as:
Cs = 1 +

Ns
X
Pr (s) log Pr (s)

log Ns

r=1

,

where r represents a root user/group/page linking to source s, Pr (s) stands for the fraction
of posts/tweets or reshares/retweets linking to s and associated with r, and Ns is the
total number of roots linking to s. Entropy measures how evenly quantities of content are
distributed across roots; it is normalized to account for the varying numbers of roots in
different cases. Its inverse captures concentration, and is defined in the unit interval. It
is maximized at 1 when the content originates from a single root user/group/page, and
minimized at 0 when each root makes an equal contribution. We set Cs = 1 when Ns = 1.
Let us gauge the concentration of activity around root accounts through their numbers of
original tweets/posts for each source. Similarly, we calculate the concentration of popularity
around the root accounts using their numbers of retweets/reshares for each source. We show
the distributions of these concentration variables in Fig. 8(a). On both platforms, we find
that popularity is significantly more concentrated around root accounts compared to their
activity (p < 0.001 for paired sample t-tests). This suggests the existence of superspreaders:
despite the diversity of root accounts publishing links to low-credibility content on both
platforms, only messages from a small group of influential accounts are shared extensively.

13

(a)

Twitter

(b)

Facebook

***

***

From verified roots

Twitter

***

al

O

ts

ee

tw

ts
ee

tw
Re

O

s
ts
are
os
sh
lp
na
Re
ir gi

Facebook
Percentage from verified roots

Total number

Concentration around roots

***

in
rig

(c)

Facebook

Retweets

Reshares

Twitter

na

igi
Or

ts

ee

l tw

ee

tw

Re

ts

na

igi
Or

lp

ts

os

Re

a
sh

res

Figure 8: Evidence of Infodemic superspreaders. Boxplots show the median (white line),
25th–75th percentiles (boxes), 5th–95th percentiles (whiskers), and outliers (dots). Significance of statistical tests is indicated by *** (p < 0.001). (a) Distributions of the concentration of original tweets, retweets, original posts, and reshares linking to low-credibility
domains around root accounts. Each domain corresponds to one observation. (b) Distributions of the total number of retweets and reshares of low-credibility content posted by
verified and unverified accounts. Each account corresponds to one observation. (c) Fractions
of original tweets, retweets, original posts, and reshares by verified accounts.

5.2

Who are the Infodemic superspreaders?

Both Twitter and Facebook provide verification of accounts and embed such information in
the metadata. Although the verification processes differ, we wish to explore the hypothesis
that verified accounts on either platform play an important role as top spreaders of lowcredibility content. Fig. 8(b) compares the popularity of verified accounts to unverified ones
on a per-account basis. We find that verified accounts tend to receive a significantly higher
number of retweets/reshares on both platforms (p < 0.001 for Mann-Whitney U-tests).
We further compute the proportion of original tweets/posts and retweets/reshares that
correspond to verified accounts on both platforms. Verified accounts are a small minority
compared to unverified ones, i.e., 1.9% on Twitter and 4.5% on Facebook, among root
accounts involved in publishing low-credibility content. Despite this, Fig. 8(c) shows that
verified accounts yield almost 40% of low-credibility retweets on Twitter and almost 70% of
reshares on Facebook.
These results suggest that verified accounts play an outsize role in the spread of Infodemic content. Are superspreaders all verified? To answer this question, let us analyse superspreader accounts separately for each low-credibility source. We extract the top
user/page/group (i.e., the account with most retweets/reshares) for each source, and find
that 19% and 21% of them are verified on Twitter and Facebook, respectively. While these
values are much higher than the percentages of verified accounts among all roots, they show
that not all superspreaders are verified.
Who are the top spreaders of Infodemic content? Table 4 answers this question for
the 23 top low-credibility sources in Fig. 5(b). We find that the top spreader for each
source tends to be the corresponding official account. For instance, about 20% of the
retweets containing links to thegatewaypundit.com pertain to @gatewaypundit, the official
handle associated with The Gateway Pundit website, on Twitter. (The @gatewaypundit
14

Table 4: Official social media handles for the 23 top low-credibility sources from Fig. 5(b).
Accounts with a checkmark (X) are verified. Accounts with an asterisk (*) are the top
spreaders for the corresponding domains. Accounts with a dagger (†) were suspended as of
February 2021.
Domain Twitter handle
Facebook page/group
thegatewaypundit.com @gatewaypundit X* †
@gatewaypundit X*
breitbart.com @BreitbartNews X*
@Breitbart X*
zerohedge.com @zerohedge *
@ZeroHedge *
washingtontimes.com @WashTimes X*
@TheWashingtonTimes X
rt.com @RT com X*
@RTnews X*
swarajyamag.com @SwarajyaMag X*
@swarajyamag X*
pjmedia.com @PJMedia com *
@PJMedia X*
waynedupree.com @WayneDupreeShow X* @WayneDupreeShow X*
bongino.com @dbongino X*
@dan.bongino X*
trendingpolitics.com –
@trendingpoliticsdotcom
oann.com @OANN X*
@OneAmericaNewsNetwork X*
wnd.com @worldnetdaily X*
@WNDNews *
sputniknews.com @SputnikInt X*
@SputnikNews X*
dailystar.co.uk @dailystar X*
@thedailystar X*
politicalflare.com @nicolejames
@nicolejameswriter
thenationalpulse.com @RaheemKassam X*
@thenationalpulse *
americanthinker.com @AmericanThinker
@AmericanThinker *
gellerreport.com @PamelaGeller X†
@pamelageller X
cbn.com @CBNOnline X
@cbnonline X
100percentfedup.com @100PercFEDUP *
@100PercentFEDUp *
newspunch.com –
@thepeoplesvoicetv *
thepoliticalinsider.com @TPInsidr
@ThePoliticalInsider X*
hannity.com @seanhannity X*
@SeanHannity X*

15

account was suspended by Twitter in February 2021.) The remaining retweets have 10,410
different root users. Similarly, on Facebook, among all 2,821 pages/groups that post links to
thegatewaypundit.com, the official page @gatewaypundit accounts for 68% of the reshares.
We observe in Table 4 that most of the top low-credibility sources have official accounts on
both Twitter and Facebook, which tend to be verified (71.4% on Twitter and 65.2% on
Facebook). They are also the top spreaders of those domains in 16 out of 21 cases (76.2%)
on Twitter and 18 out of 23 (78.3%) on Facebook.

6

Infodemic manipulation

Here we consider two types of inauthentic behaviors that can be used to spread and amplify
COVID-19 misinformation: coordinated networks and automated accounts.

6.1

Coordinated amplification of low-credibility content

Social media accounts can act in a coordinated fashion (possibly controlled by a single entity)
to increase influence and evade detection [59, 60, 14]. We apply the framework proposed by
[14] to identify coordinated efforts in promoting low-credibility information, both on Twitter
and Facebook.
The idea is to build a network of accounts where the weights of edges represent how
often two accounts link to the same domains. A high weight on an edge means that there
is an unusually high number of domains shared by the two accounts. We first construct a
bipartite graph between accounts and low-credibility domains linked in tweets/posts. The
edges of the bipartite graph are weighted using TF-IDF [61] to discount the contributions
of popular sources. Each account is therefore represented as a TF-IDF vector of domains.
A projected co-domain network is finally constructed, with edges weighted by the cosine
similarity between the account vectors.
We apply two filters to focus on active accounts and highly similar pairs. On Twitter, the
users must have at least 10 tweets containing low-credibility links, and we retain edges with
similarity above 0.99. On Facebook, the pages/groups must have at least 5 posts containing
links, and we retain edges with similarity above 0.95. These thresholds are selected by
manually inspecting the outputs.
Fig. 9 shows densely connected components in the co-domain networks for Twitter and
Facebook. These clusters of accounts share suspiciously similar sets of sources. They likely
act in a coordinated fashion to amplify Infodemic messages, and are possibly controlled by
the same entity or organization. We highlight the fact that using a more stringent threshold
on the Twitter dataset yields a higher number of clusters than a more lax threshold on
the Facebook dataset. However, this does not necessarily imply a higher level of abuse on
Twitter; it could be due to the difference in the units of analysis. On Facebook, we only
have access to public groups and pages with a bias toward high popularity, and not to all
accounts as on Twitter.
An examination of the sources shared by the suspicious clusters on both platforms shows
that they are predominantly right-leaning and mostly U.S.-centric. The list of domains
shared by likely coordinated accounts on Twitter is mostly concentrated on the leading lowcredibility sources, such as breitbart.com and thegatewaypundit.com, while likely coordinated groups and pages on Facebook link to a more varied list of sources. Some of the amplified websites feature polarized rhetoric, such as defending against “attack by enemies” (see

16

thegatewaypundit.com
breitbart.com
truepundit.com

thegatewaypundit.com
breitbart.com
trendingpolitics.com
truepundit.com

thegatewaypundit.com
breitbart.com
zerohedge.com

thegatewaypundit.com
breitbart.com
pjmedia.com

thegatewaypundit.com
breitbart.com
bongino.com

thegatewaypundit.com
breitbart.com
truepundit.com
aapsonline.org

swarajyamag.com
rt.com
thegatewaypundit.com
breitbart.com
waynedupree.com

rt.com
sputniknews.com

thewashingtonsentinel.com
godfatherpolitics.com

explainlife.com
washingtontimes.com
wnd.com

100percentfedup.com
thegatewaypundit.com

breitbart.com
washingtontimes.com

breitbart.com
thegatewaypundit.com

pjmedia.com
washingtontimes.com
breitbart.com

swarajyamag.com
fairus.org
breitbart.com
dcclothesline.com
canadafreepress.com

zerohedge.com
nationalfile.com
childrenshealthdefense.org

charismanews.com
cbn.com
breitbart.com
washingtontimes.com

secondamendmentdaily.com
breitbart.com
wnd.com
frontpagemag.com

Figure 9: Networks showing clusters that share suspiciously similar sets of sources on (top)
Twitter and (bottom) Facebook. Nodes represent Twitter users or Facebook pages/groups.
The size of the each node is proportional to its degree. Edges are drawn between pairs of
nodes that share an unlikely high number of the same low-credibility domains. The edge
weight represents the number of co-shared domains. The most shared sources are annotated
for some of the clusters. Facebook pages associated with Salem Media Group radio stations
are highlighted by a dashed box.

17

200

103

0

200

101
100

Twitter rank Facebook rank

Number of tweets by likely humans

400

105

104
102
106
Number of tweets by likely bots

400

Figure 10: Total number of tweets with links posted by likely humans vs. likely bots for
each low-credibility source. The slope of the fitted line is 1.04. The color of each source
represents the difference between its popularity rank on the two platforms. Red means more
popular on Facebook, blue more popular on Twitter.
www.frontpagemag.com/about) or accusations of “liberal bias” (cnsnews.com/about-us),
among others. Additionally, there are clusters on both platforms that share Russian stateaffiliated media such as rt.com and an Indian right-wing magazine (swarajyamag.com).
In terms of the composition of the clusters, they mostly consist of users, pages, and
groups that mention President Trump or his campaign slogans. Some of the Facebook
clusters are notable because they consist of groups or pages that are owned by organizations
with a wider reach beyond the platform, or that are given an appearance of credibility by
being verified. Examples of the former are the pages associated with The Answer radio
stations (highlighted in Fig. 9). These are among 100 stations owned by the publicly traded
Salem Media Group, which also airs content on 3,100 affiliate stations. Examples of verified
pages engaged in likely coordinated behavior are those affiliated with the non-profit Media
Research Center, some of which have millions of followers. On Twitter, some of the clusters
include accounts with tens of thousands of followers. Many of the suspicious accounts in
Fig. 9 no longer exist.

6.2

Role of social bots

We are interested in revealing the role of inauthentic actors in spreading low-credibility information on social media. One type of inauthentic behavior stems from accounts controlled
in part by algorithms, known as social bots [62]. Malicious bots are known to spread lowcredibility information [9] and in particular create confusion in the online debate around
health-related topics like vaccination [63].
We adopt BotometerLite (rapidapi.com/OSoMe/api/botometer-pro), a publicly-available
tool that allows efficient bot detection on Twitter [64]. BotometerLite generates a bot score
between 0 and 1 for each Twitter account; higher scores indicate bot-like profiles. To the best
of our knowledge, there are no similar techniques designed for Facebook because insufficient
training data is available. Therefore we limit this analysis to Twitter.
When applying BotometerLite to our Twitter dataset, we use 0.5 as the threshold to

18

categorize accounts as likely humans or likely bots. For each domain, we calculate the total
number of original tweets plus retweets authored by likely humans (nh ) and bots (nb ). We
plot the relationship between the two in Fig. 10. The linear trend on the log-log plot signifies
a power law nh ∼ nγb with exponent γ ≈ 1.04, suggesting a weak level of bot amplification
(4%) [9].
While we are unable to perform automation detection on Facebook groups and pages,
the ranks of the low-credibility sources on both platforms allow us to investigate whether
sources with more Twitter bot activity are more prevalent on Twitter or Facebook. For
each domain, we calculate the difference of its ranks on Twitter and Facebook and use the
value of the difference to color the dots in Fig. 10. The results show that sources with more
bot activity on Twitter are equally shared on both platforms.

7

Discussion

In this paper, we provide the first comparison between the prevalence of low-credibility
content related to the COVID-19 pandemic on two major social media platforms, namely
Twitter and Facebook. Our results indicate that the primary drivers of low-credibility
information tend to be high-profile, official, and verified accounts. We also find evidence
of coordination among accounts spreading Infodemic content on both platforms, including
many controlled by influential organizations. Since automated accounts do not appear to
play a strong role in amplifying content, these results indicate that the COVID-19 Infodemic
is an overt, rather than a covert, phenomenon.
We find that low-credibility content, as a whole, has higher prevalence than content
from any single high-credibility source. However, there is evidence of differences in the
misinformation ecosystems of the two platforms, with many low-credibility websites and
suspicious YouTube videos at higher prevalence on one platform when compared to the
other. Such a discrepancy might be due to a combination of the supply and demand factors.
On the supply side, the official accounts associated with specific low-credibility websites are
not symmetrically present on both platforms. On the demand side, the two platforms have
very different user demographics. According to recent surveys, 69% of adults in the U.S. say
they use Facebook, but only 22% of adults are on Twitter. Further, while Facebook usage is
relatively common across a range of demographic groups, Twitter users tend to be younger,
more educated, and have higher than average income. Finally, Facebook is a pathway to
consuming online news for around 43% of U.S. adults, while the same number for Twitter
is 12% [65, 66].
During the first months of the pandemic, we observe similar surges of low-credibility
content on both platforms. The strong correlation between the timelines of low- and highcredibility content volume reveals that these peaks were likely driven by public attention to
the crisis rather than by bursts of malicious content.
Our results provide us with a way to assess how effective the two platforms have been
at combating the Infodemic. The ratio of low- to high-credibility information on Facebook
is lower than on Twitter, suggesting that Facebook may be more effective. On the other
hand, we also find that verified accounts played a stronger role on Facebook than Twitter in
spreading low-credibility content. However, the accuracy of these comparisons is subject to
the different data collection biases. Suspicious YouTube uploads also exhibit an asymmetric
prevalence between Facebook and Twitter. As stated previously, this may be partly a
result of uploaders recycling sections of videos and uploading the content with a new video

19

ID. Having such duplicates can mean that one version becomes popular on Facebook and
another on Twitter, each potentially shared by a different demographic. This asymmetry
might also be driven by Twitter users being more likely to flag videos. YouTube may then
quickly remove reported videos before Facebook users have a chance to share them.
There are a number of limitations to our work. As we have remarked throughout the
paper, differences between platform data availability and biases in sampling and selection
make direct and fair comparisons impossible in many cases. The content collected from
the Twitter Decahose is biased toward active users due to being sampled on a per-tweet
basis. The Facebook accounts provided by CrowdTangle are biased toward popular pages
and public groups, and data availability is also based upon requests made by other researchers. The small set of keywords driving our data collection pipeline may have introduced additional biases in the analyses. This is an inevitable limitation of any collection
system, including the Twitter COVID-19 stream (developer.twitter.com/en/docs/labs/
covid19-stream/filtering-rules). The use of source-level rather than article-level labels
for selecting low-credibility content is necessary [15], but not ideal; some links from lowcredibility sources may point to credible information. In addition, the list of low-credibility
sources was not specifically tailored to our subject of inquiry. Finally, we do not have access
to many deleted Twitter and Facebook posts, which may lead to an underestimation of
the Infodemic’s prevalence. All of these limitations highlight the need for cross-platform,
privacy-sensitive protocols for sharing data with researchers [67].
Low-credibility information on the pandemic is an ongoing concern for society. Our
study raises a number of questions. For example, user demographics might strongly affect
the consumption of low-credibility information on social media: how do users in distinct
demographic groups interact with different information sources? The answer to this question
can lead to a better understanding of the Infodemic and more effective moderation strategies,
but will require methods that scale with the nature of big data from social media. Another
critical question is how social media platforms are handling the flow of information and
allowing dangerous content to spread. Regrettably, since we find that high-status accounts
play an important role, addressing this problem will prove difficult. As Twitter and Facebook
have increased their moderation of COVID-19 misinformation, they have been accused of
political bias. While there are many legal and ethical considerations around free speech and
censorship, our work suggests that these questions cannot be avoided and are an important
part of the debate around how we can improve our information ecosystem.

References
[1] Wei Lyu and George L. Wehby. Community use of face masks and COVID-19: Evidence
from a natural experiment of state mandates in the US. Health Affairs, 39(8):1419–
1425, June 2020.
[2] Sahil Loomba, Alexandre de Figueiredo, Simon J Piatek, Kristen de Graaf, and Heidi J
Larson. Measuring the impact of COVID-19 vaccine misinformation on vaccination
intent in the UK and USA. Nature Human Behaviour, 2021.
[3] Amy Mitchell and J. Baxter Oliphant. Americans immersed in coronavirus news; most
think media are doing fairly well covering it. Pew Research Center, March 2020. https:
//pewrsr.ch/3dbTpxs (Accessed November 2020).

20

[4] Katherine Schaeffer. Nearly three-in-ten Americans believe COVID-19 was made in
a lab. Pew Research Center, April 2020. https://pewrsr.ch/2XlJqAa (Accessed
November 2020).
[5] Sophie Nightingale, Marc Faddoul, and Hany Farid. Quantifying the reach and belief
in COVID-19 misinformation. arXiv:2006.08830, June 2020.
[6] Jon Roozenbeek, Claudia R. Schneider, Sarah Dryhurst, John Kerr, Alexandra L. J.
Freeman, Gabriel Recchia, Anne Marthe van der Bles, and Sander van der Linden.
Susceptibility to misinformation about COVID-19 around the world. Royal Society
Open Science, 7(10):201199, 2020. Publisher: Royal Society.
[7] John Zarocostas. How to fight an infodemic. The Lancet, 395(10225):676, 2020.
[8] Soroush Vosoughi, Deb Roy, and Sinan Aral. The spread of true and false news online.
Science, 359(6380):1146–1151, 2018.
[9] Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng Yang, Alessandro Flammini, and Filippo Menczer. The spread of low-credibility content by social
bots. Nature Communications, 9:4787, 2018.
[10] Massimo Stella, Emilio Ferrara, and Manlio De Domenico. Bots increase exposure to
negative and inflammatory content in online social systems. Proceedings of the National
Academy of Sciences, 115(49):12435–12440, 2018.
[11] Emilio Ferrara, Stefano Cresci, and Luca Luceri. Misinformation, manipulation, and
abuse on social media in the era of COVID-19. Journal of Computational Social Science,
3(2):271–277, November 2020.
[12] Hunt Allcott, Matthew Gentzkow, and Chuan Yu. Trends in the diffusion of misinformation on social media. Research & Politics, 6(2):2053168019848554, April 2019.
[13] Emilio Ferrara. What types of COVID-19 conspiracies are populated by Twitter bots?
First Monday, 25(6), 2020.
[14] Diogo Pacheco, Pik-Mai Hui, Christopher Torres-Lugo, Bao Tran Truong, Alessandro
Flammini, and Filippo Menczer. Uncovering coordinated networks on social media:
Methods and case studies. In Proceedings of the AAAI International Conference on
Web and Social Media (ICWSM), 2021. Forthcoming.
[15] David Lazer, Matthew Baum, Yochai Benkler, Adam Berinsky, Kelly Greenhill, et al.
The science of fake news. Science, 359(6380):1094–1096, 2018.
[16] Gunther Eysenbach, John Powell, Oliver Kuss, and Eun-Ryoung Sa. Empirical studies
assessing the quality of health information for consumers on the world wide web: a
systematic review. JAMA, 287(20):2691–2700, 2002.
[17] Gunther Eysenbach. Infodemiology: The epidemiology of (mis) information. The
American Journal of Medicine, 113(9):763–765, 2002.
[18] Fang Jin, Wei Wang, Liang Zhao, Edward Dougherty, Yang Cao, C Lu, and Naren
Ramakrishnan. Misinformation propagation in the age of Twitter. IEEE Annals of the
History of Computing, 47(12):90–94, 2014.
21

[19] Ranjan Pathak, Dilli Ram Poudel, Paras Karmacharya, Amrit Pathak, Madan Raj
Aryal, Maryam Mahmood, and Anthony A Donato. YouTube as a source of information
on Ebola virus disease. North American Journal of Medical Sciences, 7(7):306, 2015.
[20] Isaac Chun-Hai Fung, King-Wa Fu, Chung-Hong Chan, Benedict Shing Bun Chan, ChiNgai Cheung, Thomas Abraham, and Zion Tsz Ho Tse. Social media’s initial reaction
to information and misinformation on Ebola, August 2014: facts and rumors. Public
Health Reports, 131(3):461–473, 2016.
[21] Tara Kirk Sell, Divya Hosangadi, and Marc Trotochaud. Misinformation and the us
ebola communication crisis: analyzing the veracity and content of social media messages
related to a fear-inducing infectious disease outbreak. BMC Public Health, 20:1–10,
2020.
[22] EK Seltzer, E Horst-Martz, M Lu, and RM Merchant. Public sentiment and discourse
about Zika virus on Instagram. Public Health, 150:170–175, 2017.
[23] Megha Sharma, Kapil Yadav, Nitika Yadav, and Keith C Ferdinand. Zika virus pandemic—analysis of Facebook as a social media health information platform. American
Journal of Infection Control, 45(3):301–302, 2017.
[24] Kaustubh Bora, Dulmoni Das, Bhupen Barman, and Probodh Borah. Are internet
videos useful sources of information during global public health emergencies? A case
study of YouTube videos during the 2015–16 Zika virus pandemic. Pathogens and Global
Health, 112(6):320–328, 2018.
[25] Michael J Wood. Propagating and debunking conspiracy theories on Twitter during
the 2015–2016 Zika virus outbreak. Cyberpsychology, Behavior, and Social Networking,
21(8):485–490, 2018.
[26] L Meghan Mahoney, Tang Tang, Kai Ji, and Jessica Ulrich-Schad. The digital distribution of public health news surrounding the human papillomavirus vaccination: a
longitudinal infodemiology study. JMIR Public Health and Surveillance, 1(1):e2, 2015.
[27] Chi Y Bahk, Melissa Cumming, Louisa Paushter, Lawrence C Madoff, Angus Thomson,
and John S Brownstein. Publicly available online tool facilitates real-time monitoring
of vaccine conversations and sentiments. Health affairs, 35(2):341–347, 2016.
[28] Gabriele Donzelli, Giacomo Palomba, Ileana Federigi, Francesco Aquino, Lorenzo Cioni,
Marco Verani, Annalaura Carducci, and Pierluigi Lopalco. Misinformation on vaccination: A quantitative analysis of youtube videos. Human Vaccines & Immunotherapeutics, 14(7):1654–1659, 2018.
[29] Donatella Panatto, Daniela Amicizia, Lucia Arata, Piero Luigi Lai, and Roberto Gasparini. A comprehensive analysis of Italian web pages mentioning squalene-based influenza vaccine adjuvants reveals a high prevalence of misinformation. Human Vaccines
& Immunotherapeutics, 14(4):969–977, 2018.
[30] Ana Lucı́a Schmidt, Fabiana Zollo, Antonio Scala, Cornelia Betsch, and Walter Quattrociocchi. Polarization of the vaccination debate on Facebook. Vaccine, 36(25):3606–
3612, 2018.

22

[31] Matthew R. DeVerna, Francesco Pierri, Bao Tran Truong, John Bollenbacher, David
Axelrod, Niklas Loynes, Christopher Torres-Lugo, Kai-Cheng Yang, Filippo Menczer,
and John Bryden. CoVaxxy: A global collection of English Twitter posts about COVID19 vaccines. arXiv preprint arXiv:2101.07694, 2021.
[32] Cristina M Pulido, Beatriz Villarejo-Carballido, Gisela Redondo-Sama, and Aitor
Gómez. COVID-19 infodemic: More retweets for science-based information on coronavirus than for false information. International Sociology, 35(4):377–392, July 2020.
[33] M. S. Al-Rakhami and A. M. Al-Amri. Lies Kill, Facts Save: Detecting COVID-19
Misinformation in Twitter. IEEE Access, 8:155961–155970, 2020.
[34] Shahan Ali Memon and Kathleen M. Carley. Characterizing COVID-19 misinformation
communities using a novel Twitter dataset. arXiv:2008.00791, September 2020.
[35] Ramez Kouzy, Joseph Abi Jaoude, Afif Kraitem, Molly B El Alam, Basil Karam, Elio
Adib, Jabra Zarka, Cindy Traboulsi, Elie W Akl, and Khalil Baddour. Coronavirus
goes viral: quantifying the COVID-19 misinformation epidemic on Twitter. Cureus,
12(3), 2020.
[36] Heidi Oi-Yee Li, Adrian Bailey, David Huynh, and James Chan. YouTube as a source
of information on COVID-19: a pandemic of misinformation? BMJ Global Health,
5(5):e002604, 2020.
[37] A. Dutta, N. Beriwal, L. M. Van Breugel, et al. YouTube as a source of medical and
epidemiological information during COVID-19 pandemic: A cross-sectional study of
content across six languages around the globe. Cureus, 12(6):e8622, 2020.
[38] David A. Broniatowski, Daniel Kerchner, Fouzia Farooq, Xiaolei Huang, Amelia M.
Jamison, Mark Dredze, and Sandra Crouse Quinn. The COVID-19 social media Infodemic reflects uncertainty and state-sponsored propaganda. arXiv:2007.09682, July
2020.
[39] Matteo Cinelli, Walter Quattrociocchi, Alessandro Galeazzi, Carlo Michele Valensise,
Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo, and Antonio Scala.
The COVID-19 social media Infodemic. Scientific Reports, 10(1):16598, 2020.
[40] Riccardo Gallotti, Francesco Valle, Nicola Castaldo, Pierluigi Sacco, and Manlio
De Domenico. Assessing the risks of ‘infodemics’ in response to COVID-19 epidemics.
Nature Human Behaviour, 4:1285–1293, 2020.
[41] Lisa Singh, Shweta Bansal, Leticia Bode, Ceren Budak, Guangqing Chi, Kornraphop Kawintiranon, Colton Padden, Rebecca Vanarsdall, Emily Vraga, and Yanchen
Wang. A first look at COVID-19 information and misinformation sharing on Twitter.
arXiv:2003.13907, March 2020.
[42] Kai-Cheng Yang, Christopher Torres-Lugo, and Filippo Menczer. Prevalence of lowcredibility information on twitter during the COVID-19 outbreak. In Proceedings of
the ICWSM International Workshop on Cyber Social Threats, 2020.
[43] Lisa Singh, Leticia Bode, Ceren Budak, Kornraphop Kawintiranon, Colton Padden,
and Emily Vraga. Understanding high-and low-quality URL Sharing on COVID-19
Twitter streams. Journal of Computational Social Science, 3:343–366, 2020.
23

[44] Stefano Guarino, Francesco Pierri, Marco Di Giovanni, and Alessandro Celestini. Information disorders during the COVID-19 infodemic: The case of Italian Facebook.
Online Social Networks and Media, 22:100124, 2021.
[45] Wen-Ying Sylvia Chou, April Oh, and William MP Klein. Addressing health-related
misinformation on social media. JAMA, 320(23):2417–2418, 2018.
[46] Nir Grinberg, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and David
Lazer. Fake news on Twitter during the 2016 US presidential election. Science,
363(6425):374–378, 2019.
[47] Gordon Pennycook and David G Rand. Fighting misinformation on social media using
crowdsourced judgments of news source quality. Proceedings of the National Academy
of Sciences, 116(7):2521–2526, 2019.
[48] Alexandre Bovet and Hernán A Makse. Influence of fake news in Twitter during the
2016 US presidential election. Nature Communications, 10(1):1–14, 2019.
[49] Kai-Cheng Yang, Francesco Pierri, Pik-Mai Hui, David Axelrod, Christopher TorresLugo, John Bryden, and Filippo Menczer. Dataset for paper: The COVID-19 Infodemic: Twitter versus Facebook. Zenodo, 2020. https://doi.org/10.5281/zenodo.
4313903.
[50] Amy Mitchell, Jeffrey Gottfried, Jocelyn Kiley, and Katerina Eva Matsa. Political
polarization & media habits. Pew Research Center, 2014. http://pewrsr.ch/1vZ9MnM
(Accessed November 2020).
[51] Francesco Pierri, Carlo Piccardi, and Stefano Ceri. A multi-layer approach to disinformation detection in US and Italian news spreading on twitter. EPJ Data Science,
9(35), 2020.
[52] Francesco Pierri, Carlo Piccardi, and Stefano Ceri. Topology comparison of Twitter
diffusion networks effectively reveals misleading news. Scientific Reports, 10:1372, 2020.
[53] Clayton A Davis, Giovanni Luca Ciampaglia, Luca Maria Aiello, Keychul Chung,
Michael D Conover, Emilio Ferrara, Alessandro Flammini, et al. OSoMe: the IUNI
observatory on social media. PeerJ Computer Science, 2:e87, 2016.
[54] CrowdTangle Team. CrowdTangle. Menlo Park, CA: Facebook., 2020. Accessed November 2020.
[55] Aleksi Knuutila, Aliaksandr Herasimenka, Hubert Au, Jonathan Bright, Rasmus
Nielsen, and Philip N Howard. COVID-related misinformation on YouTube: The spread
of misinformation videos on social media and the effectiveness of platform policies. Oxford, UK: Project on Computational Propaganda, 2020. https://comprop.oii.ox.
ac.uk/research/posts/youtube-platform-policies/.
[56] Tom Wilson and Kate Starbird. Cross-platform disinformation campaigns: Lessons
learned and next steps. Harvard Kennedy School Misinformation Review, 1(1), 2020.
[57] Gillian C. Goobie, Sabina A. Gulera, Kerri A. Johannson, Jolene H. Fisher, and Christopher J. Ryerson. YouTube videos as a source of misinformation on idiopathic pulmonary
fibrosis. Annals of the American Thoracic Society, 16(5):572—-579, 2019.
24

[58] Dimitar Nikolov, Mounia Lalmas, Alessandro Flammini, and Filippo Menczer. Quantifying biases in online information exposure. Journal of the Association for Information
Science and Technology, 70(3):218–229, 2019.
[59] Leonardo Nizzoli, Serena Tardelli, Marco Avvenuti, Stefano Cresci, and Maurizio Tesconi. Coordinated behavior on social media in 2019 UK general election.
arXiv:2008.08370, 2020.
[60] Karishma Sharma, Emilio Ferrara, and Yan Liu. Identifying coordinated accounts in
disinformation campaigns. arXiv:2008.11308, 2020.
[61] Karen Spärck Jones. A statistical interpretation of term specificity and its application
in retrieval. Journal of Documentation, 28(1):11–21, 1972.
[62] Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, and Alessandro Flammini.
The rise of social bots. Communications of the ACM, 59(7):96–104, 2016.
[63] David A Broniatowski, Amelia M Jamison, SiHua Qi, Lulwah AlKulaib, Tao Chen,
Adrian Benton, Sandra C Quinn, and Mark Dredze. Weaponized health communication:
Twitter bots and Russian trolls amplify the vaccine debate. American Journal of Public
Health, 108(10):1378–1384, 2018.
[64] Kai-Cheng Yang, Onur Varol, Pik-Mai Hui, and Filippo Menczer. Scalable and generalizable social bot detection through data selection. Proceedings of the AAAI Conference
on Artificial Intelligence, 34(1):1096–1103, 2020.
[65] Andrew Perrin and Monica Anderson. Share of US adults using social media, including
Facebook, is mostly unchanged since 2018. Pew Research Center, 2019. https://
pewrsr.ch/2VxJuJ3 (Accessed February 2021).
[66] Stephan Wojcik and Hughes Adam. Sizing up Twitter users. Pew Research Center,
2019. https://pewrsr.ch/2VUkzj4 (Accessed February 2021).
[67] Irene V. Pasquetto, Briony Swire-Thompson, et al. Tackling misinformation: What
researchers could do with social media data. Harvard Kennedy School Misinformation
Review, 1(8), 2020.

25

