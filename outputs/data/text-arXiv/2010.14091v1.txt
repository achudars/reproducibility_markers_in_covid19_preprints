1

Triple-view Convolutional Neural Networks for
COVID-19 Diagnosis with Chest X-ray

arXiv:2010.14091v1 [eess.IV] 27 Oct 2020

Jianjia Zhang, Luping Zhou, Lei Wang,

Abstract—The Coronavirus Disease 2019 (COVID-19) is affecting increasingly large number of people worldwide, posing
significant stress to the health care systems. Early and accurate
diagnosis of COVID-19 is critical in screening of infected patients
and breaking the person-to-person transmission. Chest X-ray
(CXR) based computer-aided diagnosis of COVID-19 using deep
learning becomes a promising solution to this end. However,
the diverse and various radiographic features of COVID-19
make it challenging, especially when considering each CXR scan
typically only generates one single image. Data scarcity is another
issue since collecting large-scale medical CXR data set could
be difficult at present. Therefore, how to extract more informative and relevant features from the limited samples available
becomes essential. To address these issues, unlike traditional
methods processing each CXR image from a single view, this
paper proposes triple-view convolutional neural networks for
COVID-19 diagnosis with CXR images. Specifically, the proposed
networks extract individual features from three views of each
CXR image, i.e., the left lung view, the right lung view and
the overall view, in three streams and then integrate them for
joint diagnosis. The proposed network structure respects the
anatomical structure of human lungs and is well aligned with
clinical diagnosis of COVID-19 in practice. In addition, the
labeling of the views does not require experts’ domain knowledge,
which is needed by many existing methods. The experimental
results show that the proposed method achieves state-of-theart performance, especially in the more challenging three class
classification task, and admits wide generality and high flexibility.
Index Terms—COVID-19, Computer-aided Diagnosis, Chest Xray, Deep Learning, Convolutional Neural Networks

I. I NTRODUCTION

T

HE Coronavirus Disease 2019 (COVID-19), caused
by severe acute respiratory syndrome coronavirus 2
(SARSCoV-2), is quickly spreading over the world and a
huge number of people have been affected. Nearly 27 million
COVID-19 cases and 0.9 million deaths have been confirmed
globally as of 07 Sep. 2020 [1]. And the numbers still keep
rising dramatically due to high rate of community transmission and lack of appropriate treatment and vaccines [2]. A
critical measure to stop the virus transmission and restrain
the outbreak is early diagnosis [3], [4]. Early diagnosis not
only enables timely treatment for the patients affected, but
Jianjia Zhang is with the School of Computer Science, University
of Technology Sydney, Sydney, NSW 2007, Australia, e-mail: Jianjia.zhang@uts.edu.au.
Luping Zhou is with School of Electrical and Information Engineering,
The University of Sydney, Sydney, NSW 2006, Australia, e-mail: luping.zhou@sydney.edu.au
Lei Wang is with School of Computing and Information Technology,
University of Wollongong, Wollongong, NSW 2522, Australia, e-mail:
leiw@uow.edu.au

also allows quick isolation of their close contacts for disease
containment [4].
Although
real-time
reverse-transcriptionpolymerasechainreaction (RT-PCR) is considered as the golden standard
to make a definitive diagnosis of COVID-19 infection, it
encounters several issues in such a global pandemic. Firstly,
its false negative rate is high. A patient initially tested
negatively could be later tested positive [5], [6]. Therefore, a
series of RT-PCR tests, which can take up to two days [4],
may be required to confirm a case. Secondly, RT-PCR test kits
may not be sufficiently available in all areas across the world,
especially when considering that the global supply chains are
at risk due to major disruptions. In addition, the laboratory
equipment required by RT-PCR could be also a bottleneck to
conduct large scale tests. These issues may result in delayed
or even missed diagnosis and a computer-aided diagnosis
system could, at least partially, automate the diagnosis and
facilitate large scale screening of COVID-19 patients.
SARSCoV-2 infection can attack various types of lung
cells [7] and trigger an inflammatory response in the air sacs
of lungs, leading to a typical symptom of COVID-19 patients:
pneumonia. Making matters even worse, both the left and
right lungs are often involved for early, intermediate and late
stage patients [8], causing breathlessness and even death. The
inflammatory response can be detected by radiology examinations, especially chest computed tomography (CT) or chest
X-ray (CXR). Typical radiographic features of COVID-19
patients in these scans include ground-glass opacities (GGO),
multifocal patchy consolidation and/or interstitial changes with
a peripheral distribution [3], [9]. These visual features specific
to COVID-19 patients are used by clinicians for COVID19 diagnosis. At the same time, they admit the possibility
of computer-aided diagnosis. There have been considerable
research interests devoted to it and the existing works can
be summarized as follows from four different perspectives:
1) Radiology exam used: CXR [2], [10], [11], [12], [13],
[14] and CT [8], [3], [15] scans are the most commonly used
radiology exams by recent works for computer-aided diagnosis
of COVID-19. Ultrasound is also explored in a few recent
studies, such as [16], [17]. At the same time, some other
studies explore a combination of multiple source data for
joint learning. For example, the works in [4], [18] integrate
CT scans with non-imaging clinical metadata, e.g., clinical
symptoms, exposure history, and/or laboratory testing, for
COVID-19 prediction. 2) Diagnostic objectives: The existing
works can be categorized into three groups according to their
aims. The first group, e.g., in [11], [2], [12], [13], [14], [18], is
to differentiate normal controls, COVID-19 patients, and other

2

non-COVID-19 pneumonia, e.g., Severe Acute Respiratory
Syndrome (SARS) and Middle East Respiratory Syndrome
(MERS), which forms a three class classification problem. The
second group [8], [3], [15] aims to solve a binary classification
problem of distinguishing COVID-19 from other non-COVID19 pneumonia. The remaining group [4] also works on a binary
classification problem, but it focuses on recognizing COVID19 from normal. 3) Labeling information: Some existing
works, e.g., [11], [8], [12], [13] only use the ground truth
diagnosis results as labels in the training stage. Meanwhile,
many other works [2], [3], [4], [14], [18] also utilize the
lung mask or lesion segmentation results to train a more
focused classifier. For example, the work in [3] proposes
an online attention module in Convolutional neural networks
(CNN) to focus on the infection regions in lungs when making
diagnosis decisions. 4) Shallow or deep models: Most of
existing works [16], [2], [3] use CNN in developing COVID19 diagnosis model. Shallow models are also explored in a
few works [11], [8] with hand-crafted features or combined
with deep models as in [4], [18].
There are several challenging issue faced by the earlier
studies. One critical issue is that the radiographic features of
COVID-19 are diverse and can vary [19]. Another challenge
is the scarcity of training data since collecting large scale
training data is difficult, if not impossible, in the current
emergent situation of COVID-19 pandemic. Moreover, manual
segmentation of lung or lesion masks for each scan required
by the works in [2], [3], [4], [14], [18] not only is costly and
time-consuming, but also requires experts’ domain knowledge.
These issues can affect the scalability and generality of the
models. In this case, it is desirable to design an effective model
using limited training data without requirement of domain
knowledge. This is attempted this paper.
Inspired by the encouraging success achieved by the earlier
studies and motivated by the issues we are facing, this work
proposes a novel triple-view CNN for COVID-19 diagnosis
with CXR images and we denote it as TV-CovNet in the
remaining sections. Specifically, the proposed TV-CovNet
consists of three streams with three views of the lungs in each
CXR image, i.e., the left lung view, the right lung view and the
overall view, as inputs and conducts diagnosis by integrating
the information from them. This idea respects the anatomical
lung structure and pathology of COVID-19. The bronchi of
the left and right lungs are internally connected by the trachea,
therefore the SARSCoV-2 could easily transit form one lung
to the other, especially when considering the high transmission
rate of COVID-19. That’s why bilateral lung involvement can
often be observed for COVID-19 patients. The proposed TVCovNet is also inspired by clinical practice in diagnosing
COVID-19 with CXR images, i.e., a clinician usually checks
the left and right lungs individually and then jointly before
making a decision. In comparison with traditional methods
which consider each CXR image as an single-view image,
the triple-view structure and joint decision making enable TVCovNet to extract more representative and relevant features
from each CXR image. This is especially important when the
training data are limited. Moreover, for each CXR image at
either the training or the test stage, we only need to provide

three lung bounding boxes and no expert knowledge from clinicians is required. This improves the generality of the proposed
method in comparison with the methods in [2], [3], [4], [14],
[18], which highly rely on accurate contour segmentation of
lung or lesion areas. As far as we are aware, we are among
the first ones to investigate the individual left and right lung
views for CXR based COVID-19 diagnosis. CXR scan is used
in the proposed TV-CovNet since it is cheaper, faster and more
accessible across the world in comparison with CT.
This paper’s main contributions are summarized as follows:
• We propose a novel triple-view CNN to conduct COVID19 diagnosis using CXR images. The proposed networks
can extract and integrate the diagnostic clues from the left
lung, right lung and overall lung views for a joint decision, which respects the anatomical structure of lungs and
is well aligned with the practical diagnosis by clinicians.
• Various methods are explored to integrate the information
from the three views in a fusion layer. As will be
demonstrated in the experimental evaluation, the average
pooling at score layer attains the best performance.
• The superior performance of the proposed method over
the competing methods and its wide generality are consistently verified in two tasks with three backbone network architectures. Specifically, one task is differentiating
normal from COVID-19, and the other is distinguishing
normal, COVID-19, and other non-COVID-19 pneumonia. Most recent works only study one of these two
tasks, but both of them are covered in this paper since
we believe that either one may be important in certain
applications. The three backbone network architectures
refer to ResNet-50, ResNet-101 and ResNet-152.
• We also demonstrate the high flexibility of the proposed
network, i.e., it could be easily integrated with other stateof-the-art deep learning methods to further improve the
diagnosis performance.
II. R ELATED W ORKS
Motivated by their success in computer vision tasks, e.g.,
object detection and image classification, deep learning has
been intensively studied for diagnosis of various conditions
ranging from breast lesions [20], cardiac disease [21] to, the
focus of this paper, pneumonia [22] in recent years. Due
to the emergent COVID-19 pandemic at present, there is
intensive research interest devoting to computer-aided COVID19 diagnosis using deep learning with radiology imaging.
As one of the main complications caused by COVID19, pneumonia is an infection of the lung tissues, including
the bronchi, bronchioles and alveoli, resulting in breathing
difficulties or even respiratory failure. CXR [2], [10], [11],
[12], [13], [14] and CT [8], [3], [15], [4], [18] scans are
the most commonly used radiologic examinations by clinicians in identification of pneumonia inflammation. COVID19 positive cases present radiographic abnormalities such as
ground-glass opacity and bilateral patchy shadowing in CXR
and CT images [23], [13]. Although CT scan could provide
more detailed diagnostic clues in identification of COVID19 positive cases, CXR is probably a more practical option,

3

especially in resource-constrained or heavily-affected areas for
its various advantages over CT [13]. Firstly, X-ray machines
are more widely and readily accessible across the world. It
is one of the most equipped device in all levels of medical
institutions due to its wide application and significantly lower
price in comparison with CT. Secondly, CXR has higher
scanning efficiency, admitting rapid screening. Thirdly, there
are various kinds of portable CXR machines, which can better
adapt to various application contexts. Last but not least, a
CXR scan delivers much less dose of radiation in comparison
with a CT scan, typically less than 4% of the later [24].
These advantages motivate this paper to develop a CXR based
COVID-19 diagnosis method.
While diagnosis via CXR scan enjoys various advantages as
explained above, it also brings challenges to develop a robust
and effective diagnosis method. On the one hand, each CXR
scan typically only generates one single image for diagnosis.
In comparison, a CT scan generates 3D volumetric data and
enable detailed visualization from multiple perspectives. On
the other hand, the CXR scans from COVID-19 patients that
can be used as training data are limited since it is difficult to
collect large scale of samples considering the current emergent situation. In this case, how to better extract informative
features from each sample becomes essential. Many existing
works [2], [14] resort to manual labeling of lesion or lung
masks, however, this is time-consuming and can only be done
by experts. This paper attempts to address this issue without
requirement of experts in labeling from another perspective.
That is, unlike the existing works which take a CXR scan as
a single view image, to explore multi-view feature extraction
from each CXR scan by building multi-stream networks.
In fact, there has been a number of attempts to develop
multi-stream networks in computer vision tasks, e.g., action
recognition [25] and 3D reconstruction [26]. In these tasks, the
input data can be naturally decomposed into multiple components. For example, a video clip of action sequence is split into
spatial and temporal components in [25]. In the area of medical
data analysis, a similar methodology has been explored to fuse
multi-stream information within shallow models [27], [28], [8]
or deep learning models [29], [4]. These works can be roughly
categorized into two groups: i) the first group aims to fuse
different modalities, e.g., MRI and PET data in [28], PET
and CT in [29] and CT and clinical meta data in [4]; ii) the
second group works on combining multiple features extracted
from the same modality, e.g., features with multiple templates
of brain regions of interest in [27] and multiple hand-crafted
features from CT images in [8]. These works consistently
demonstrate that a fusion of multiple modalities or features is
able to improve the performance over any single modality or
feature. Unfortunately, these multi-stream networks can hardly
be readily applied to sole CXR scan, upon which we are trying
to develop computer-aided diagnosis method in this paper. One
obvious reason is that we presume no other modality scan or
clinical information is available except a single CXR image
per subject. In addition, although a combination of multiple
hand-crafted features in shallow models is always doable,
CNN based end-to-end methods are more preferable due to
their capability to learn more specific and adaptive features,

especially for challenging CXR images.
After a careful review of the existing works and the challenges as explained above in this line, we propose TV-CovNet
for COVID-19 diagnosis with CXR images. As aforementioned, the proposed TV-CovNet is in alignment with both
of the anatomical lung structures and clinicians’ practical
diagnosis of COVID-19.
III. M ETHOD
The overall framework of the proposed TV-CovNet is illustrated in Fig 1. As seen, each input CXR image, which is
assumed in posteroanterior (PA) view in this paper, is firstly
cropped into the left lung view (the top blue stream), the
overall view (the middle orange stream) and the right lung
view (the bottom green stream). In comparison with lesion
or lung mask labeling in the literature, such as in [2], [3],
[4], [14], [18], this cropping step is efficient and no domain
expert knowledge is required. Therefore, this cropping step is
not restricted to clinicians and this could significantly improve
the generality of the proposed method. Also, it will still be
applicable when large scale of training data is available in
future. Once cropped, the three views are fed into the three
streams of TV-CovNet respectively. As shown in Fig 1, three
colors, i.e., blue, orange and green, are used to indicates the
corresponding streams. The backbone network architecture of
each stream is flexible, i.e., be it a specifically-designed or offthe-shelf network architecture. And the network architectures
in the three streams can be identical or different. The features
extracted from the three streams will be combined in a fusion
layer, which will be explained in detail in the following
section. The fusion layer will be followed by a final fully
connected classification layer. The advantages of the proposed
TV-CovNet are recapped as follows:
• In comparison with the existing works taking a single
overall view of CXR scans as input, TV-CovNet could
extract more detailed and complementary information
from the left lung, right lung and the overall views in
three different streams, providing more clues for accurate
diagnosis. This is motivated by the clinical practice in
diagnosing COVID-19. Each CXR scan of human lungs
in PA view composes of the left and right lungs, and
these two lungs may present different visual characteristics although they are internally connected and, in most
COVID-19 cases, bilaterally affected. Therefore, the left
and right lungs are usually reviewed individually and
jointly by a clinician to identify more diagnostic clues;
• TV-CovNet does not require experts in manual labeling of
data, admitting high generality to different sized data sets.
More detailed labeling information, e.g., lesion or lung
mask labeling in [2], [3], [4], [14], [18], could certainly
help to train a more focused network and may improve
the diagnosis accuracy. At the same time, however, it
may limit the generality of the networks due to extensive
requirement on experts’ domain knowledge. In contrast,
cropping three views in TV-CovNet is much simpler and
more efficient without requirement of domain knowledge;
In addition, as a byproduct, TV-CovNet could possibly
alleviate the effects of dataset bias. In order to construct a

4

CXR training dataset for CNN based COVID-19 diagnosis,
most works in the literature combine multiple datasets from
various institutes as one, as in [2], [3], [11], [12]. However,
as pointed out in [30], [11], joining different databases may
add bias from the non-lung areas, which can be learned by the
networks to recognize which origin database the test sample is
from rather than the lung injuries, especially when the training
data is scarce. To a certain extent, the proposed method could
minimize the negative effects of this issue by only feeding the
cropped lung areas in the left and right lung views. On the one
hand, the left and right lung views are non-overlapping, so no
specific non-lung area will appear in both of these two views.
On the other hand, the strategy of diagnosing from triple views
of the lungs will help to alleviate the effects of the non-lung
areas in the overall view.

be combined. Let sL , sO and sR denote the corresponding
class score vectors from the left, overall and right lung streams.
At this level, a fusion function F: sL , sO , sR → s
combines the class score vectors sL , sO , sR into the final class
score s. We investigate the following two fusion functions, as
illustrated in sub-figures (e), (f) in Fig. 2:
• Max score pooling. Similar to the Max feature pooling
above, s = Fmax (sL , sO , sR ) takes the element-wise
maximum of the three input score vectors:
si = max{sL,i , sO,i , sR,i },
where i indicates the i-th class.
• Mean score pooling. Similarly, s = Fmean (sL , sO , sR )
takes the mean of the three input score vectors:
si = mean{sL,i , sO,i , sR,i }.
IV. E VALUATION

A. Fusion methods

A. Dataset

The above section explains how TV-CovNet extracts features from three streams. With these output features, a following issue is how to effectively integrate them in a fusion
layer, as illustrated in Fig. 1. To address this issue, five fusion
methods will be studied. Specifically, the five fusion methods
performs at two levels, i.e., feature level (sub-figure (b), (c),
(d) in Fig. 2) and score level ( sub-figure (e), (f) in Fig. 2).
Feature level fusion. Let us denote the output feature
vectors from the left lung, overall and right lung streams
as fL , fO and fR , respectively, and they are assumed to
have the same dimensionality D, i.e., fL , fO , fR ∈ RD ,
as demonstrated in Fig. 2 (a). This can be easily met by using
the same backbone network architecture in the three streams or
appending a fully connected layer with D-dimensional output
if different network architectures are used in the three streams.
At this level, a fusion function F: fL , fO , fR → f combines the three features fL , fO , fR into a combined feature f .
The combined feature f will be fed into the classification layer
to calculate the class score s. We investigate the following
three fusion functions illustrated in Fig. 2. (b), (c), (d):

The dataset used in our evaluation is constructed by CXR
images from the following two publicly accessible sources:
• COVID-19 image data collection [31]. This dataset
contains CXR images of patients which are confirmed as
COVID-19 or other non-COVID-19 pneumonia. All the
CXR images in PA view are collected into the combined
dataset. The COVID-19 cases form the “COVID-19”
class while the other non-COVID-19 pneumonia cases
are assigned to the “Other” class.
• Chest X-ray Database [32], [33]. This CXR dataset is
composed of normal cases and patients with tuberculosis.
The normal cases form the “Normal” class and tuberculosis cases are assigned to the “Other” class.
The numbers of CXR samples in each class of the combined
dataset are shown in Table I. The left lung, right lung and
overall views of each CXR scan, as shown in Fig. 1, in the
combined dataset will be cropped.

•

•

•

Max feature pooling. f = Fmax (fL , fO , fR ) takes the
element-wise maximum of the three input feature vectors:
fd = max{fL,d , fO,d , fR,d },
where d ∈ [1, D] indicates the d-th feature dimension.
The resulting feature f after max pooling operation is
still a vector with the same dimension D as input vectors.
Mean feature pooling. f = Fmean (fL , fO , fR ) takes
the mean of the three input feature vectors:
fd = mean{fL,d , fO,d , fR,d }, with d ∈ [1, D]
Similarly, the resulting feature f after mean pooling is
also a D-dimensional vector.
Feature concatenation. f = Fcat (fL , fO , fR ) stacks
the three input feature vectors:
fd = fL,d when d ∈ [1, D]
fd = fO,d−D when d ∈ [D + 1, 2D]
fd = fR,d−2D when d ∈ [2D + 1, 3D]
The resulting concatenated feature f is 3D-dimensional.

Score level fusion. If fusion is applied at the score level, the
scores of the last classification layer in the three streams will

TABLE I
N UMBER OF CXR SCANS IN EACH CLASS OF THE COMBINED DATASET.
class
number

Normal
326

COVID-19
217

Other
389

Total
932

B. Implementation Details
We use ResNet [34] architecture pretrained on ImageNet
[35] as the backbone network due to its promising performance
in recent works on COVID-19 diagnosis [12], [13], [14]. In
order to verify the generality of the proposed TV-CovNet,
three backbone networks are adopted in two classification
tasks. Specifically, ResNet-50, ResNet-101 and ResNet-152
will be applied respectively within TV-CovNet to evaluate
their performance. And the two classification tasks refer to
a binary classification task between Normal and COVID-19
and a three class classification task among Normal, COVID19 and Other. Cross entropy is used as the objective function
with learning rate of 0.01 and momentum of 0.9. The training
process is iterated for 100 epochs with a batch size of 10
samples and the learning rate is decreased by a factor of 0.1

5

Fig. 1. Overview of the proposed triple-view Convolutional Neural Networks for COVID-19 Diagnosis with Chest-X-ray.

Fig. 2. Illustration of the fusion methods in the proposed triple-view CNN structure.

every 20 epochs. Among all the samples in each class, we
randomly select 60% of them as training set and the rest as
test set. This random training/test partition is repeated 15 times
for each classification task to obtain stable statistics.

their network by following the paper. As in the paper, 100
random patches are cropped from the overall view of each
CXR scan for majority voting during the inference stage.
D. Preliminary Study

C. Competing Methods
The following two recent works achieving state-of-the-art
performance on COVID-19 diagnosis with CXR images will
be involved in our experimental evaluation:
• COVID-Net [13]. COVID-Net is one of the earliest
work on designing CNN for the detection of COVID-19
cases with CXR images. It works on classifying normal,
COVID-19 and non-COVID-19 pneumonia, which shares
the same setting as that of the three class classification
task in this paper. Their publicly accessible implementation on https://github.com/lindawangg/COVID-Net is
used in our evaluation and the overall view of CXR
images is used as input to train the networks.
• LocalPatch [2]. LocalPatch is another recent work
closely related to ours. LocalPatch tries to address the
issue of data scarcity by developing a patch-based CNN
with many random patches cropped from each CXR
image. The final classification result for a test sample is
obtained by majority voting from inference results of the
patchy crops from the test CXR image. We implemented

The essential objective of this paper is to extract informative
visual features from the left and right lung views in addition
to the overall view to boost the diagnosis accuracy. From this
sense, an implicit assumption behind is that the left or right
lung view should contain certain information for diagnosis.
In order to verify the validity of this assumption, this section
explores COVID-19 diagnosis using a single stream networks
with a single view of CXR images as input. Specifically,
a pretrained ResNet, i.e., one of ResNet-50, ResNet-101 or
ResNet-152, is used as backbone networks and the final classification layer is reset to predict three classes of “Normal”,
“COVID-19” and “Other”. Note that each classifier has only
one stream as in the commonly used setting from the literature,
and it is fine-tuned with samples from one of the overall, left
lung or right lung views. The performance of the fine-tuned
networks is reported in Table II. As seen, with the overall view,
ResNet-50 achieves an accuracy of 79.4%, which is reasonably
good considering the challenges of CXR based COVID-19
diagnosis. If left or right lung view is used, ResNet-50 obtains
80.2% and 79.6%, respectively, which are comparable or even

6

slightly higher than that of the overall view. This demonstrates
that a single lung view indeed contains substantial valuable
information for diagnosis. Moreover, it is interesting to see that
the left or right lung view could even outperform the overall
view considering that the left or right lung view is essentially
an interior part of the overall view. This is probably because although the overall view contains all the visual contents in both
of the left and right lung views, it also contains considerable
non-lung areas, whose visual characteristics could introduce
bias or disturbance to the classifier. In contrast, these non-lung
areas are partially excluded from the left or right lung views,
so the networks could focus more on the lung area and extract
COVID-19 related features. Similar results can be observed
when ResNet-101 or ResNet-152 is used. These results lay
a solid foundation for integration of multiple views to extract
complementary diagnostic information and alleviate the effects
of non-related features.
TABLE II
C LASSIFICATION ACCURACY (%) ON THREE CLASS COVID-19
DIAGNOSIS WITH UNIQUE VIEW.
Network
ResNet-50
ResNet-101
ResNet-152

Overall view
79.4
78.8
78.9

Left lung view
80.2
79.1
79.1

Right lung view
79.6
80.5
78.9

E. Diagnostic Results
The above section verifies the assumption of the proposed
TV-CovNet, and this section will evaluate its effectiveness to
integrate the left lung, right lung and overall views for joint
diagnosis. For simplicity and clear comparison, an identical
backbone network architecture is applied to the three steams
of TV-CovNet. The network architecture is set as ResNet50, ResNet-101 and ResNet-152, respectively, to verify its
generality with respect to different network depths. Both
binary classification, i.e., between Normal and COVID-19, and
three class classification among Normal, COVID-19 and Other
will be performed. The results on binary case will be evaluated
by five metrics with true positive (TP), true negative (TN),
false positive (FP), and false negative (FN) values, including
• Accuracy = (TN + TP)/(TN + TP + FN + FP)
• Precision = TP/(TP + FP)
• Recall = TP/(TP + FN)
• Specificity = TN/(TN + FP)
• F1 score = 2(Precision × Recall)/(Precision + Recall)
In the three class case, accuracy is used as the evaluation metric. As aforementioned, the random partition of training/test
sets are repeated 15 times for each task to obtain stable
statistics. Table III reports the results in four portions, with
the first portion containing the competing methods, including
COVID-Net [13] and ResNet-18 based LocalPatch [2] method.
The rest three portions report the performance of methods with
ResNet-50, ResNet-101 and ResNet-152 [34], respectively. In
each of these three portions, we firstly show the performance
of the baseline method, i.e., a single stream Resnet with
the overall view as input. It is followed by the results of
LocalPatch [2] method with ResNet. Then we report the

performance of the proposed TV-CovNet with five fusion
methods, where “fea” denotes feature level fusion and “sc”
refers to score level fusion introduced in Section III-A, while
“cat”, “max” and “mean” indicate feature concatenation, max
pooling and mean pooling, respectively.
As reported in the first portion, the competing method
COVID-Net [13] obtains an accuracy of 78.0% in the three
class classification case. Its is not applied to the two class
case since the CNN in that method is intentionally designed
for the three class classification task. The ResNet-18 based
LocalPatch [2] method improves the performance to 80.5% in
the three class classification case, achieving an improvement
of 2.5 percentage points.
In the second portion, as seen in the binary classification
case, ResNet-50 [34] already achieves very promising accuracy of 98.7%. LocalPatch [2] with ResNet-50 improves the
performance of ResNet-50 in all the five metrics, verifying
the effectiveness of generating patches for major voting in
[2]. When the proposed TV-CovNet is applied with ResNet-50
architecture as backbone networks, among the feature fusion
methods, TV-CovNet fea cat and TV-CovNet fea max obtain
comparable accuracy to the baseline ResNet-50 [34], but no
improvement is observed. In contrast, TV-CovNet fea mean
improves ResNet-50 in all the five metrics. This demonstrates
the effectiveness of integrating triple-view information for
joint diagnosis. When score fusion methods are applied, TVCovNet sc max is comparable to ResNet-50 [34] while TVCovNet 50 sc mean outperforms ResNet-50 [34], becoming
comparable to ResNet-50 + LocalPatch [2].
In the more challenging three class classification task, a similar trend can be observed. Specifically, TV-CovNet fea cat,
TV-CovNet fea max and TV-CovNet sc max are comparable to ResNet-50 while TV-CovNet fea mean and TVCovNet 50 sc mean achieve considerable improvement over
ResNet-50. Especially, TV-CovNet 50 sc mean outperforms
all the competing methods, achieving an improvement of 4.3
percentage points over ResNet-50 and 2.1 percentage points
over the state-of-the-art method LocalPatch [2].
Similar results can be observed with ResNet-101 and
ResNet-152 architectures as backbone networks, as shown in
the bottom two portions of Table III. Especially, the proposed
mean score pooling (denoted as sc mean) method consistently
obtains the best performance among the competing methods.
In order to provide more details on the results above, the
accuracies of the baseline ResNet and TV-CovNet with mean
score pooling in 15 splits are shown in Table IV, including
both of the binary classification case and the three class
classification case. As seen, regardless of ResNet-50, ResNet101 or ResNet-152 is used, the proposed TV-CovNet sc mean
outperforms ResNet in most splits and the improvement is
statistically evident, as verified by the small p-value (< 0.05)
of student’s t-test. The corresponding confusion matrices averaged over 15 splits are shown in Table V, including both of the
binary classification case and the three class classification case.
As can be seen that all the diagonal entries of the confusion
matrices in the right column obtained by the proposed method
are enlarged while all the off-diagonal entries are reduced in
comparison with the results of baseline ResNet in the left

7

TABLE III
C OMPARISON ON COVID-19 D IAGNOSIS W ITH C HEST X- RAY I MAGING (T WO EXPERIMENTS ).

Methods in comparison
COVID-Net [13]
ResNet-18 + LocalPatch [2]
ResNet-50 [34]
ResNet-50 + LocalPatch [2]
TV-CovNet 50 fea cat
TV-CovNet 50 fea max
TV-CovNet 50 fea mean
TV-CovNet 50 sc max
TV-CovNet 50 sc mean
ResNet-101 [34]
ResNet-101 + LocalPatch [2]
TV-CovNet 101 fea cat
TV-CovNet 101 fea max
TV-CovNet 101 fea mean
TV-CovNet 101 sc max
TV-CovNet 101 sc mean
ResNet-152 [34]
ResNet-152 + LocalPatch [2]
TV-CovNet 152 fea cat
TV-CovNet 152 fea max
TV-CovNet 152 fea mean
TV-CovNet 152 sc max
TV-CovNet 152 sc mean

Accuracy
99.4 ±0.5
98.7 ±0.8
99.7 ±0.4
97.9 ±1.2
97.8 ±1.2
99.1 ±0.6
98.3 ±1.2
99.8 ±0.3
98.7 ±1.1
99.7 ±0.3
97.8 ±1.1
97.8 ±1.3
99.0 ±0.6
98.1 ±1.3
99.6 ±0.4
98.4 ±1.2
99.8 ±0.3
98.4 ±0.9
97.8 ±1.7
98.9 ±1.2
98.3 ±1.0
99.8 ±0.3

Precision
99.2 ±1.0
98.3 ±1.8
99.7 ±0.5
97.1 ±1.9
98.0 ±2.0
99.0 ±1.1
98.4 ±1.3
99.5 ±0.6
98.2 ±2.2
99.6 ±0.6
97.3 ±1.9
97.0 ±1.9
98.6 ±1.5
98.2 ±1.2
99.7 ±0.5
98.3 ±1.4
99.6 ±0.5
97.9 ±1.8
97.6 ±2.2
98.8 ±1.6
97.8 ±2.1
99.6 ±0.5

2 classes
Recall
99.4 ±0.8
98.5 ±1.5
99.6 ±0.7
97.7 ±1.8
96.6 ±2.1
98.7 ±1.0
97.5 ±2.4
99.9 ±0.3
98.6 ±1.2
99.8 ±0.5
97.1 ±2.3
97.6 ±2.4
98.9 ±1.1
96.9 ±2.6
99.4 ±0.9
97.8 ±2.3
99.9 ±0.3
98.2 ±1.5
97.0 ±3.3
98.5 ±2.6
98.1 ±1.7
99.9 ±0.4

Specificity
99.4 ±0.7
98.9 ±1.2
99.8 ±0.4
98.1 ±1.3
98.7 ±1.2
99.3 ±0.8
98.9 ±0.8
99.7 ±0.4
98.7 ±1.7
99.7 ±0.4
98.2 ±1.3
97.9 ±1.3
99.1 ±1.0
98.8 ±0.8
99.8 ±0.4
98.8 ±1.0
99.7 ±0.4
98.6 ±1.2
98.4 ±1.5
99.2 ±1.1
98.5 ±1.4
99.7 ±0.4

3 classes
Accuracy
78.0 ±2.1
80.5 ±1.9
79.4 ±2.2
81.6 ±1.8
79.3 ±2.0
80.4 ±2.2
82.1 ±2.4
79.0 ±2.6
83.7 ±2.0
78.8 ±2.5
81.1 ±1.7
78.8 ±2.8
80.5 ±2.2
81.2 ±1.9
79.4 ±2.2
83.0 ±2.0
78.8 ±1.9
81.4 ±2.1
79.1 ±2.1
80.4 ±3.3
81.3 ±2.0
79.8 ±2.3
84.4 ±1.6

F1 score
99.3 ±0.7
98.4 ±1.0
99.6 ±0.5
97.4 ±1.4
97.2 ±1.6
98.9 ±0.8
97.9 ±1.6
99.7 ±0.4
98.4 ±1.2
99.7 ±0.4
97.2 ±1.4
97.3 ±1.7
98.7 ±0.7
97.5 ±1.8
99.6 ±0.5
98.0 ±1.5
99.8 ±0.4
98.0 ±1.0
97.3 ±2.2
98.6 ±1.6
97.9 ±1.3
99.7 ±0.3

TABLE IV
P ERFORMANCE ON COVID-19 D IAGNOSIS W ITH C HEST X- RAY I MAGING ( TWO CLASSES ).
Split

1

2

3

4

5

ResNet-50
TV-CovNet 50 sc mean
ResNet-101
TV-CovNet 101 sc mean
ResNet-152
TV-CovNet 152 sc mean

98.6
100
99.1
100
97.7
100

97.7
99.5
99.1
100
100
100

99.6
100
95.7
99.1
96.1
99.6

98.1
99.5
97.6
100
98.6
100

99.5
99.1
98.6
100
97.2
99.5

ResNet-50
TV-CovNet 50 sc mean
ResNet-101
TV-CovNet 101 sc mean
ResNet-152
TV-CovNet 152 sc mean

80.0
84.9
80.8
81.4
78.9
83.2

78.1
84.1
77.3
82.4
78.9
84.1

81.4
84.8
81.9
85.9
79.1
84.3

77.7
84.7
75.7
82.3
77.4
85.6

80.0
86.8
83.0
81.1
82.5
85.8

6

7
8
two classes
97.8 97.2 98.6
100 99.5 100
99.6 97.7 98.2
98.7 99.1 100
97.4 96.8 99.1
99.1 99.5 100
three classes
78.2 75.5 81.1
84.4 80.9 82.4
78.0 74.7 76.5
82.0 83.2 82.7
80.1 76.3 76.5
85.1 81.6 84.6

column. This observation is consistent in both binary and three
class cases with any of ResNet-50, -101 or -152.
In summary, as verified in our experiments above, fusion at
the score layer performs better than fusion at the feature level
in the proposed TV-CovNet. Regarding the fusion methods,
mean pooling performs better than feature concatenation or
max pooling methods. TV-CovNet with mean pooling at the
score level consistently outperforms the competing methods
and demonstrates the state-of-the-art performance.
V. D ISCUSSION
A. Comparison with Ensemble Methods
The above section has verified the effectiveness of the
proposed TV-CovNet, which integrates the visual features
from the left lung, right lung and overall views. A natural
question arises that, if the key objective is to integrate the
complementary information from the three views for joint

9

10

11

12

13

14

15

Average p-value

98.6
100
99.5
100
100
100

98.6
100
99.0
99.5
98.1
100

99.1
99.5
100
99.5
99.1
99.5

99.5
100
99.1
99.5
99.5
100

98.2
100
98.6
100
98.6
100

99.5
99.5
99.0
99.5
99.5
100

100
100
99.5
99.5
98.1
99.5

98.7
99.8
98.7
99.6
98.4
99.8

5.3x10−5
3.5x10−3
1.3x10−4

84.2
82.1
81.5
83.2
81.0
87.5

80.1
86.2
79.3
87.6
77.3
83.1

80.2
81.8
78.1
83.6
79.9
83.6

79.9
83.6
77.5
81.2
80.7
83.9

77.1
85.8
78.9
85.0
75.8
83.7

80.1
83.7
81.2
83.4
79.5
87.3

76.7
79.6
76.9
79.6
77.5
82.3

79.4
83.7
78.8
83.0
78.8
84.4

4.4x10−6
1.9x10−5
2.4x10−9

diagnosis, would ensemble methods also serve this purpose?
To answer this question, we train three classifiers separately
with the the cropped left lung, right lung and overall views,
respectively, and ensemble the output scores of these three
classifiers during test phase by applying max or mean functions. The results are reported in Table VI. As seen, the
ensemble method with max, denoted by ensemble max, or
mean, denoted by ensemble mean, consistently improves the
performance of baseline ResNet in both of the two and three
class classification cases no matter ResNet-50, ResNet-101 or
ResNet-152 is used. However, the ensemble methods do not
perform as well as the proposed TV-CovNet sc mean method
since the later could better integrate the complementary information from the three views in an end-to-end learning
manner. In contrast, the ensemble methods treat the three
views independently and do not allow interaction between
them during the training stage.

8

TABLE V
C ONFUSION M ATRICES OF D IFFERENT METHODS FOR COVID-19
D IAGNOSIS W ITH C HEST X- RAY I MAGING (N - N ORMAL , C - COVID-19,
O - OTHER ).
Two class case
Predicted

Actual

N
C

N

TV-CovNet 50 sc mean

128.3 1.7
1.2
85.8

N
C

Actual

129.7 0.3
0.5
86.5

N

TV-CovNet 101 sc mean
C

N

ResNet-101

N
C

129.6 0.4
0.07 86.93

C

N

ResNet-50

C

N

C

N
C

128.5 1.5
2
85

C

N
C

128.5 1.5
2
85

N
C

ResNet-152

C

Actual

N

Predicted

129.7 0.3
0.1
86.9

TABLE VII
C OMPARISON BETWEEN DOUBLE - VIEW AND THE PROPOSED METHODS .

TV-CovNet 152 sc mean
Three class case

Methods in comparison

25.7
8.3
113.9

24.2
7.1
110.3

Actual

105.6 1.8
1.1
80.0
31.2 15.5

O

C

110.1 1
0.7
79.5
23.0 13.6

O

C

N
N
C
O

22.6
6.7
107.1

ResNet-152

N
C
O

113.9 0.5
0.4
80.1
22.0 13.0

15.6
6.5
119.5

TV-CovNet 152 sc mean

TABLE VI
C OMPARISON BETWEEN ENSEMBLE AND THE PROPOSED METHODS .
Methods in comparison
ResNet-50
ResNet-50 ensemble max
ResNet-50 ensemble mean
TV-CovNet 50 sc mean (proposed)
ResNet-101
ResNet-101 ensemble max
ResNet-101 ensemble mean
TV-CovNet 101 sc mean (proposed)
ResNet-152
ResNet-152 ensemble max
ResNet-152 ensemble mean
TV-CovNet 152 sc mean (proposed)

ResNet-50
DV-CovNet
TV-CovNet
ResNet-101
DV-CovNet
TV-CovNet
ResNet-152
DV-CovNet
TV-CovNet

50 sc mean
50 sc mean (proposed)
101 sc mean
101 sc mean (proposed)
152 sc mean
152 sc mean (proposed)

2 classes
Accuracy

3 classes
Accuracy

98.7 ±0.8
99.2 ±0.8
99.8 ±0.3
98.7 ±1.1
99.4 ±0.6
99.6 ±0.4
98.4 ±1.2
99.1 ±0.6
99.8 ±0.3

79.4 ±2.2
82.2 ±2.8
83.7 ±2.0
78.8 ±2.5
81.8 ±2.4
83.0 ±2.0
78.8 ±1.9
82.0 ±1.6
84.4 ±1.6

18.9
6.7
118.5

TV-CovNet 101 sc mean
O

C

N

ResNet-101

N
C
O

15.9
7.3
118.1

TV-CovNet 50 sc mean
O

C

102.9 2.9
0.7
78.5
28.3 15.5

113.5 0.6
0.4
79.4
23.3 13.1

O

N
C
O

N
C
O

C

Actual

N

ResNet-50

N

O

C

103.0 1.3
0.9
77.9
27.0 13.5

Predicted

N

Actual

N

Predicted
N
C
O

views already contain all the visual information in the two
lungs? To this end, we remove the overall view stream from the
proposed TV-CovNet and train double-view networks, denoted
as DV-CovNet, with the left lung and right lung only. Its
performance is compared with TV-CovNet. Since the mean
score pooling has consistently obtained the best performance
among the five fusion methods in all the experiments above,
only it is applied in this section. As reported in Table VII,
DV-CovNet indeed improves the baseline, ResNet, but it is
outperformed by the proposed TV-CovNet counterpart. This
demonstrates that the overall view is indeed contributive in
TV-CovNet. This is probably because, although the left and
right lung views contain all the visual contents of the two
lungs, the overall view enables learning certain co-occurrence
features and forming a hierarchical representation of an CXR
image, which could benefit final diagnosis.

2 classes
Accuracy

3 classes
Accuracy

98.7 ±0.8
99.0 ±0.6
99.0 ±0.8
99.8 ±0.3
98.7 ±1.1
99.1 ±0.8
99.2 ±0.5
99.6 ±0.4
98.4 ±1.2
98.8 ±0.7
98.9 ±0.9
99.8 ±0.3

79.4 ±2.2
81.2 ±1.9
82.8 ±1.6
83.7 ±2.0
78.8 ±2.5
81.1 ±2.2
82.4 ±2.0
83.0 ±2.0
78.8 ±1.9
81.3 ±1.8
82.9 ±1.9
84.4 ±1.6

B. Comparison with Double-view Networks
This section will investigate is the overall view still required
in TV-CovNet considering that the left lung and right lung

C. If pretrained networks help?
In the experiments above, the ResNet parameters are initialized with the pretrained networks on ImageNet [35], however,
the images from ImageNet are natural images rather than
medical images. This section will verify does initialization
with pretrained parameters benefit the diagnosis accuracy
given the huge domain gap between the images from ImageNet
and those from the CXR dataset constructed in this paper.
Table VIII reports the performance of ResNet and TV-CovNet
with or without pretrained parameter initialization. As seen,
initialization with pretrained parameters always outperforms
random initialization in both cases of the ResNet and the
proposed TV-CovNet with any of ResNet-50, -101 or -152.
D. Weighted fusion via adaptive learning
It has been verified above that, among the five fusion
methods in III-A, the mean score pooling method leads to
the best performance in TV-CovNet. In that method, the three
views are treated equally important. This section aims to
study will weighted fusion be able to further improve the
performance. To this end, three positive scalars wL , wO , wR
are assigned to the corresponding scores of three streams
sL , sO , sR , respectively, as weight of the steam. These
weights are adaptively optimized during the training stage.
The weighted score mean, i.e., wL +w1O +wR Σi∈{L,O,R} wi si ,
is used as the final prediction score. As reported in Table IX,

9

TABLE VIII
C OMPARISON BETWEEN NON - PRETRAINED AND PRETRAINED
INITIALIZATION .
3 classes
Accuracy

98.3 ±0.8
98.7 ±0.8
98.5 ±0.8
99.8 ±0.3
98.5 ±0.8
98.7 ±1.1
98.7 ±0.8
99.6 ±0.4
98.0 ±0.9
98.4 ±1.2
98.6 ±0.6
99.8 ±0.3

77.9 ±2.3
79.4 ±2.2
79.9 ±1.6
83.7 ±2.0
78.4 ±1.7
78.8 ±2.5
79.8 ±1.7
83.0 ±2.0
78.3 ±1.7
78.8 ±1.9
80.1 ±1.9
84.4 ±1.6

the weighted mean performs worse than TV-CovNet sc mean.
This is probably due to the limited training data scale, and the
adaptively learned weights may lead to over-fitting in this case.
TABLE IX
C OMPARISON BETWEEN DYNAMICALLY- LEARNED WEIGHTED
COMBINATION METHODS AND THE PROPOSED METHODS .
Methods in comparison
ResNet-50
TV-CovNet
TV-CovNet
ResNet-101
TV-CovNet
TV-CovNet
ResNet-152
TV-CovNet
TV-CovNet

50 sc mean (weighted)
50 sc mean (proposed)
101 sc mean (weighted)
101 sc mean (proposed)
152 sc mean (weighted)
152 sc mean (proposed)

2 classes
Accuracy
98.7 ±0.8
99.2 ±0.7
99.8 ±0.3
98.7 ±1.1
99.5 ±0.6
99.6 ±0.4
98.4 ±1.2
99.3 ±0.5
99.8 ±0.3

Methods in comparison
ResNet-50 [34]
ResNet-50 + LocalPatch [2]
TV-CovNet 50 sc mean
TV-CovNet 50 sc mean + LocalPatch
ResNet-101 [34]
ResNet-101 + LocalPatch [2]
TV-CovNet 101 sc mean
TV-CovNet 101 sc mean + LocalPatch
ResNet-152 [34]
ResNet-152 + LocalPatch [2]
TV-CovNet 152 sc mean
TV-CovNet 152 sc mean + LocalPatch

3 classes
Accuracy

0.925

79.4 ±2.2
82.6 ±1.9
83.7 ±2.0
79.4 ±2.2
82.7 ±2.2
83.0 ±2.0
79.4 ±2.2
82.1 ±2.1
84.4 ±1.6

0.900

F. Study on Training Data Scale
This section studies the performance trend of the baseline ResNet and proposed TV-CovNet with respect to different scales of training data in the three class classification case. As shown in Fig 3, the x-axis indicates the ratios of samples assigned to training set and y-axis shows

79.4 ±2.2
81.6 ±1.8
83.7 ±2.0
86.4 ±1.6
78.8 ±2.5
81.1 ±1.7
83.0 ±2.0
85.4 ±2.1
78.8 ±1.9
81.4 ±2.1
84.4 ±1.6
85.5 ±1.7

TV-CovNet_50_sc_mean
ResNet-50

0.875
0.850
0.825
0.800
0.775
0.750
0.1

E. Integration with other methods
The proposed TV-CovNet is an flexible framework, which
can be easily integrated with various network architectures
or training strategies. This section shows an integration of
the proposed method with LocalPatch [2] as an example.
Specifically, we generate random region crops from each of the
three views and use these crops as input to train TV-CovNet
by following LocalPatch [2]. During the test stage, we also
generate 100 crops for each view and apply major voting
to obtain the final label as in [2]. As seen in Table X, the
integrated method, TV-CovNet 101 sc mean + LocalPatch,
further improves the diagnosis accuracy in the three class
classification task with any of ResNet-50, -101 or -152. The
two class case is not reported since the existing methods
already achieved very high accuracy and the challenging three
class case could better demonstrate the comparison.

3 classes
Accuracy

the corresponding performance of ResNet-50 and the proposed TV-CovNet 50 sc mean. As seen, with increasing ratios, although both methods achieve higher accuracy, TVCovNet 50 sc mean admits faster accuracy increase and its
improvement over ResNet-50 becomes larger. This study indicates the promising potential learning capacity of TV-CovNet.

Accuracy

Methods in comparison
ResNet-50 (non-pretrained)
ResNet-50 (pretrained)
TV-CovNet 50 sc mean (non-pretrained)
TV-CovNet 50 sc mean (pretrained)
ResNet-101 (non-pretrained)
ResNet-101 (pretrained)
TV-CovNet 101 sc mean (non-pretrained)
TV-CovNet 101 sc mean (pretrained)
ResNet-152 (non-pretrained)
ResNet-152 (pretrained)
TV-CovNet 152 sc mean (non-pretrained)
TV-CovNet 152 sc mean (pretrained)

2 classes
Accuracy

TABLE X
C OMBINATION OF PATCH BASED AND THE PROPOSED METHODS .

0.2

0.3

0.4 0.5 0.6 0.7
Ratio of training samples

0.8

0.9

Fig. 3. The comparison between the proposed TV-CovNet 50 sc mean and
ResNet-50 with different ratios of training samples.

VI. C ONCLUSION AND FUTURE WORK
In order to better extract informative visual features from
the two lungs in CXR images for COVID-19 diagnosis,
we proposed a triple-view network structure. The proposed
structure respects the anatomical structure of human lungs and
is well aligned with clinician’s diagnosis practice with CXR
images. The advantages and effectiveness of the proposed
structure are experimentally verified in both binary classification task between normal and COVID-19 cases and three
class classification task among normal, COVID-19 and nonCOVID-19 pneumonia. All the results consistently show that
the proposed structure obtains the state-of-the-art performance.
Various properties of the proposed method are discussed,
including its comparison with ensemble methods, its flexibility
to be extended and promising modeling capacity with increasing training data scale etc. These discussions present more
insights on why the proposed method performs well and may
inspire future explorations in this line.

10

Several open issues are worth exploring along this line of
research. Firstly, the proposed networks can be extended to
other scan modalities, e.g., CT, or a combination of multiple
scans. Secondly, the backbone networks in different streams
are not necessarily the same. Specially designed networks for
each stream may admit more adaptive feature extraction and
achieve better diagnosis accuracy. Last but not least, collecting
larger scale of training data for the proposed method is also
critical for further evaluation and performance improvement.
R EFERENCES
[1] W. H. Organization, “Weekly epidemiological update, coronavirus disease 2019 (covid-19), 07 september 2020,” 2020.
[2] Y. Oh, S. Park, and J. C. Ye, “Deep learning covid-19 features on CXR
using limited training data sets,” IEEE Transactions on Medical Imaging,
2020.
[3] X. Ouyang, J. Huo, L. Xia, F. Shan, J. Liu, Z. Mo, F. Yan, Z. Ding,
Q. Yang, B. Song et al., “Dual-sampling attention network for diagnosis
of covid-19 from community acquired pneumonia,” IEEE Transactions
on Medical Imaging, 2020.
[4] X. Mei, H.-C. Lee, K. Diao, M. Huang, B. Lin, C. Liu, Z. Xie,
Y. Ma, P. M. Robson, M. Chung et al., “Artificial intelligence for rapid
identification of the coronavirus disease 2019 (covid-19),” medRxiv,
2020.
[5] Z. Y. Zu, M. D. Jiang, P. P. Xu, W. Chen, Q. Q. Ni, G. M. Lu, and
L. J. Zhang, “Coronavirus disease 2019 (covid-19): a perspective from
china,” Radiology, p. 200490, 2020.
[6] J. F.-W. Chan, S. Yuan, K.-H. Kok, K. K.-W. To, H. Chu, J. Yang,
F. Xing, J. Liu, C. C.-Y. Yip, R. W.-S. Poon et al., “A familial cluster
of pneumonia associated with the 2019 novel coronavirus indicating
person-to-person transmission: a study of a family cluster,” The Lancet,
vol. 395, no. 10223, pp. 514–523, 2020.
[7] H. Li, L. Liu, D. Zhang, J. Xu, H. Dai, N. Tang, X. Su, and B. Cao,
“Sars-cov-2 and viral sepsis: observations and hypotheses,” The Lancet,
2020.
[8] H. Kang, L. Xia, F. Yan, Z. Wan, F. Shi, H. Yuan, H. Jiang, D. Wu,
H. Sui, C. Zhang, and D. Shen, “Diagnosis of coronavirus disease 2019
(COVID-19) with structured latent multi-view representation learning,”
IEEE Transactions on Medical Imaging, pp. 1–1, 2020.
[9] M. Chung, A. Bernheim, X. Mei, N. Zhang, M. Huang, X. Zeng, J. Cui,
W. Xu, Y. Yang, Z. A. Fayad et al., “CT imaging features of 2019 novel
coronavirus (2019-ncov),” Radiology, vol. 295, no. 1, pp. 202–207, 2020.
[10] S. Basu and S. Mitra, “Deep learning for screening covid-19 using chest
x-ray images,” arXiv preprint arXiv:2004.10507, 2020.
[11] R. M. Pereira, D. Bertolini, L. O. Teixeira, C. N. Silla Jr, and Y. M.
Costa, “Covid-19 identification in chest x-ray images on flat and
hierarchical classification scenarios,” Computer Methods and Programs
in Biomedicine, p. 105532, 2020.
[12] E. Luz, P. L. Silva, R. Silva, and G. Moreira, “Towards an efficient deep
learning model for covid-19 patterns detection in x-ray images,” arXiv
preprint arXiv:2004.05717, 2020.
[13] L. Wang and A. Wong, “Covid-net: A tailored deep convolutional neural
network design for detection of covid-19 cases from chest x-ray images,”
arXiv preprint arXiv:2003.09871, 2020.
[14] M. J. Horry, M. Paul, A. Ulhaq, B. Pradhan, M. Saha, N. Shukla et al.,
“X-ray image based covid-19 detection using pre-trained deep learning
models,” 2020.
[15] H. X. Bai, R. Wang, Z. Xiong, B. Hsieh, K. Chang, K. Halsey, T. M. L.
Tran, J. W. Choi, D.-C. Wang, L.-B. Shi et al., “Ai augmentation of
radiologist performance in distinguishing covid-19 from pneumonia of
other etiology on chest ct,” Radiology, p. 201491, 2020.
[16] S. Roy, W. Menapace, S. Oei, B. Luijten, E. Fini, C. Saltori, I. Huijben,
N. Chennakeshava, F. Mento, A. Sentelli et al., “Deep learning for
classification and localization of covid-19 markers in point-of-care lung
ultrasound,” IEEE Transactions on Medical Imaging, 2020.
[17] G. Soldati, A. Smargiassi, R. Inchingolo, D. Buonsenso, T. Perrone,
D. F. Briganti, S. Perlini, E. Torri, A. Mariani, E. E. Mossolani et al.,
“Is there a role for lung ultrasound during the covid-19 pandemic?”
Journal of Ultrasound in Medicine, 2020.
[18] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha, W. Liang,
C. Wang, K. Wang et al., “Clinically applicable ai system for accurate
diagnosis, quantitative measurements, and prognosis of covid-19 pneumonia using computed tomography,” Cell, 2020.

[19] H. Shi, X. Han, N. Jiang, Y. Cao, O. Alwalid, J. Gu, Y. Fan, and
C. Zheng, “Radiological findings from 81 patients with covid-19 pneumonia in wuhan, china: a descriptive study,” The Lancet Infectious
Diseases, 2020.
[20] J.-Z. Cheng, D. Ni, Y.-H. Chou, J. Qin, C.-M. Tiu, Y.-C. Chang, C.S. Huang, D. Shen, and C.-M. Chen, “Computer-aided diagnosis with
deep learning architecture: applications to breast lesions in us images
and pulmonary nodules in ct scans,” Scientific reports, vol. 6, no. 1, pp.
1–13, 2016.
[21] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A.
Heng, I. Cetin, K. Lekadir, O. Camara, M. A. G. Ballester et al.,
“Deep learning techniques for automatic mri cardiac multi-structures
segmentation and diagnosis: is the problem solved?” IEEE transactions
on medical imaging, vol. 37, no. 11, pp. 2514–2525, 2018.
[22] P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Ding,
A. Bagul, C. Langlotz, K. Shpanskaya et al., “Chexnet: Radiologistlevel pneumonia detection on chest x-rays with deep learning,” arXiv
preprint arXiv:1711.05225, 2017.
[23] W.-j. Guan, Z.-y. Ni, Y. Hu, W.-h. Liang, C.-q. Ou, J.-x. He, L. Liu,
H. Shan, C.-l. Lei, D. S. Hui et al., “Clinical characteristics of coronavirus disease 2019 in china,” New England journal of medicine, vol.
382, no. 18, pp. 1708–1720, 2020.
[24] Y. Y. Kim, H. J. Shin, M.-J. Kim, and M.-J. Lee, “Comparison of
effective radiation doses from x-ray, ct, and pet/ct in pediatric patients
with neuroblastoma using a dose monitoring program,” Diagnostic and
Interventional Radiology, vol. 22, no. 4, p. 390, 2016.
[25] K. Simonyan and A. Zisserman, “Two-stream convolutional networks
for action recognition in videos,” in Advances in neural information
processing systems, 2014, pp. 568–576.
[26] E. Gundogdu, V. Constantin, A. Seifoddini, M. Dang, M. Salzmann,
and P. Fua, “Garnet: A two-stream network for fast and accurate 3d
cloth draping,” in Proceedings of the IEEE International Conference on
Computer Vision, 2019, pp. 8739–8748.
[27] M. Liu, D. Zhang, E. Adeli, and D. Shen, “Inherent structure-based multiview learning with multitemplate feature representation for alzheimer’s
disease diagnosis,” IEEE Transactions on Biomedical Engineering,
vol. 63, no. 7, pp. 1473–1482, 2015.
[28] M. Zhang, Y. Yang, F. Shen, H. Zhang, and Y. Wang, “Multi-view
feature selection and classification for alzheimer’s disease diagnosis,”
Multimedia Tools and Applications, vol. 76, no. 8, pp. 10 761–10 775,
2017.
[29] D. Jin, D. Guo, T.-Y. Ho, A. P. Harrison, J. Xiao, C.-k. Tseng, and L. Lu,
“Accurate esophageal gross tumor volume segmentation in pet/ct using
two-stream chained 3d deep network fusion,” in International Conference on Medical Image Computing and Computer-Assisted Intervention.
Springer, 2019, pp. 182–191.
[30] G. Maguolo and L. Nanni, “A critic evaluation of methods for
covid-19 automatic detection from x-ray images,” arXiv preprint
arXiv:2004.12823, 2020.
[31] J. P. Cohen, P. Morrison, and L. Dao, “Covid-19 image data
collection,” arXiv 2003.11597, 2020. [Online]. Available: https:
//github.com/ieee8023/covid-chestxray-dataset
[32] S. Candemir, S. Jaeger, K. Palaniappan, J. P. Musco, R. K. Singh,
Z. Xue, A. Karargyris, S. Antani, G. Thoma, and C. J. McDonald, “Lung
segmentation in chest radiographs using anatomical atlases with nonrigid
registration,” IEEE transactions on medical imaging, vol. 33, no. 2, pp.
577–590, 2013.
[33] S. Jaeger, A. Karargyris, S. Candemir, L. Folio, J. Siegelman,
F. Callaghan, Z. Xue, K. Palaniappan, R. K. Singh, S. Antani et al.,
“Automatic tuberculosis screening using chest radiographs,” IEEE transactions on medical imaging, vol. 33, no. 2, pp. 233–245, 2013.
[34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[35] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
A Large-Scale Hierarchical Image Database,” in CVPR09, 2009.

