SRIB Submission to Interspeech 2021 DiCOVA Challenge
Vishwanath Pratap Singh∗, Shashi Kumar∗, Ravi Shekhar Jha∗, and Abhishek Pandey
Samsung Research and Development Institute Bangalore, India
vp.singh@samsung.com, sk.kumar@samsung.com, rs.jha@samsung.com, abhi3.pandey@samsung.com
characterize the cough sounds of unhealthy speaker should
Abstract
therefore be helpful in diagnosis of the COVID-19.
The COVID-19 pandemic has resulted in more than 125 million
Recently, several cough and breath data sets have been
infections and more than 2.7 million casualties. In this paper,
published. Examples include the ‘Coughvid’ [7], ‘Breath for
we attempt to classify covid vs non-covid cough sounds using
Science’ [8], ‘Coswara’ [9], and ‘CoughAgainstCovid’[10].
signal processing and deep learning methods. Air turbulence,
Multiple deep learning methods are proposed on these data
vibration of tissues, movement of fluid through airways,
sets to detect COVID-19 from the cough and breath sounds [11,
opening and closure of glottis are some of the causes for
12, 13, 14, 15]. However, availability of limited training data is
production of acoustic sound signal during cough. Does the
still a challenge in developing such systems.
COVID-19 alter the acoustic characteristics of breath, cough,
In this paper, we experiment with different data
and speech sounds produced through the respiratory system?
augmentation methods, handcrafted features, small-footprint
This is an open question waiting for answers.
novel deep neural network architectures, and efficient model
In this paper we incorporated novel data augmentation
selection methods for evaluation. The system is developed as
methods for cough sound augmentation, and multiple deep
part of Interspeech 2021 special session and challenge on
neural network architectures and methods along with
diagnosing COVID-19 using acoustics (DiCOVA) challenge. Our
handcrafted features. Our proposed system gives 14%
proposed method secured the 5th position on leaderboard
absolute improvement in area under the curve (AUC). The
among 29 participants.
proposed system is developed as part of Interspeech 2021
This paper is organised as follows. In Section 2, we explain
special sessions and challenges viz. diagnosing of COVID-19
the shared data and the overview of the challenge. In Section
th
using acoustics (DiCOVA). Our proposed method secured 5
3 and Section 4, we present the data augmentation methods
position on leaderboard among 29 participants.
and handcrafted features, respectively. In section 5, we discuss
Index Terms: COVID-19, acoustics, machine learning,
the model training methods, and deep neural network
1
augmentation, respiratory diagnosis, healthcare
architectures. Results and analysis are presented in section 6.
Conclusion with future works are provided in Section 7.

1. Introduction
The COVID-19, novel coronavirus or SARS-Cov-2, has claimed
thousands of lives and affected millions of people all around
the world with the number of deaths and infections growing
day-by-day. According to [1], the cough has highest prevalence
among the symptoms of COVID-19 with 60.4% of all
occurrence. For diagnosing COVID-19, the real-time
polymerase chain reaction (RT-PCR) is a standard diagnostic
test, but, it can be considered as a time-consuming test. Thus
using the deep learning methods to classify non-covid and
covid cough sounds can play a critical role in COVID-19
diagnosis.
Cough is a powerful natural respiratory defense
mechanism that clears the central airways in human breathing
system. The characteristics of the cough sound is dependent
on the flow and configuration of the tissue elements involved,
and as such is likely to change during cough as the physiological
configuration evolves [2, 3]. The temporal pattern of the cough
sound can be analyzed in three phases the first starts with an
explosive burst, opening of glottis, followed by the second
phase which is a period of noisy sound and slow decay of the
noise as flow reduces due to glottal closure, transient forms
the third phase [4]. These sounds can provide us enough
information to distinguish between wet and dry cough, and
ailment cough vs. voluntary cough [5]. Thus the ability to

1

∗ Represents the equal contribution

2. Data and Challenge Overview
The Shared data set contains 1040 audio files, stored in .FLAC
format and a list containing train and validation lists for 5 folds.
In each fold, the train set contains 822 audio files and
remaining 218 audio files are part of validation set. Models
need to be trained on all 5 folds without mixing the audio
across the folds and need to be evaluated on respective
validation set. Validation folds are used to select the best
model across the folds. A separate blind test set containing 233
audio files is also shared for final evaluation on leaderboard.
Shared data is part of Coswara data set [8]. It is to be noted
that only 75 shared audio files are COVID-19 positive of which
50 audio files are part of train set and 25 are part of validation
set. We also experimented ‘Coughvid’ [7] data set and
obtained the results.

3. Feature Extraction Methods
First the raw cough speech signal is passed to energy threshold
based speech activity detector and then normalized between 1 to 1. The spectrograms of cough sounds of speakers suffering
from COVID-19 have been observed. There are marked
differences observed in the spectrograms. It is noted that in
non-covid cough sound higher proportion of signal energy lies

in higher frequency range compare COVID-19 cough sound
signal as can be seen in Fig. 3 and Fig. 4.
We have experimented with 39 dimensional mel
frequency cepstral coefficient (MFCC) and 24 dimensional
handcrafted features to capture the time domain and
frequency domain variability between COVID-19 negative and
positive cough sounds. Handcrafted features include per frame
energy of the signal(1D), fundamental frequency(1D), first four
formants(4D), alpha ratio(1D) with cut-off frequency at 1400
Hz, relative average perturbation(1D), spectral flatness( D),
kurtosis(1D), spectral contrast(7D), second order spectral
polynomial(2D), spectral centroid(1D), spectral roll off(1D),
and spectral bandwidth(1D), root mean square value of the
signal(1D), and zero crossing rate of the signal(1D). These
features are designed to capture the source and vocal tract
variabilities. Then delta and double delta of the feature vector
is computed to form 189 (3∗63) dimensional feature.

augment the cough speech data set. The basic process of noisy
training for deep neural network is as follows: first of all,
sample some noise signals from some real-world recordings
and then mix these noise signals with the original training data.
Our noise set contains 1130 noise samples from kitchen, digital
appliances, babble, music, traffic and other backgrounds. The
signal to noise ratio (SNR) was varying between 5-20 dB and
was uniformly distributed across the range. Noise based
augmentation was performed for the entire data.

4. Data Augmentation Methods
4.1. Spectrum Interpolation
It is noted that only 75 cough audio files are COVID-19 positive
and 50 out of them are part of train set in each fold. This makes
the COVID negative vs COVID positive data ratio to 16:1 during
training due to which the performance of classifier is
suboptimal. We incorporate a novel spectrum interpolation
method to increase the proportion COVID-19 cough sound
sample in train set. First, we obtain the 1024 point discrete
fourier transform (DFT) for each COVID-19 positive audio.
Then, for each audio we obtain corresponding 5 nearest
neighbour based on Euclidean distance of their DFT. Then, the
linear combination of DFT of each COVID-19 positive audio and
corresponding 5 nearest neighbour’s is used to augment the
COVID-19 cough sound. The linear combination coefficients
are obtained from a uniform distribution between 0 and 1. In
this way we obtain 5 augmented spectrum for each COVID-19
positive audio. We also experiment with the only nearest
neighbour spectrum interpolation method to get 1 augmented
spectrum for each COVID19 positive audio. Results for both of
these methods are presented.

Figure 2: Val ROC plot with data augmentation

4.3. Vocal Tract Length Perturbation
Vocal Tract Length Perturbation (VTLP) is used in speech
recognition to add the speaker to speaker variability that result
primarily from differences in vocal tract length [17]. In this
paper, we experiment with VTLP for cough sound
augmentation. The warp factor was randomly selected
between 0.85 to 1.15.

Figure 3: COVID-19 positive cough sound signal and
spectrogram.

Figure 1: Val ROC plot before data augmentation

4.2. Noise Based Augmentation
Additive noise based augmentation has been proven to be
beneficial for automatic speech recognition task [16]. Noise
based augmentation improves the performance when the
acoustic environment(background noise) are different during
train and test time, which will be true in case system is to be
used for diagnosis of COVID-19. We explore this method to

Figure 4: Non-covid cough sound signal and spectrogram. It is
noted that higher proportion of signal energy lies in higher
frequency range compare COVID-19 cough sound signal.

Table 1: results of different models on combined validation and augmented validation set
model

fold

#layers

hidden dim

type

wtPos

norm

seq length

weight decay

AUC

specificity

LSTM

1

1

48

uni

1

utt-wise

30

0

75.33

67.5

LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
LSTM
ResNet-18
ResNet-34
CNN
CNN
CNN
JVAE
JVAE

1
2
2
1
2
1
1
5
1
2
3
1
3
2
4
2
3
5
4
1
1
4
1
1
3
1
3
1
2
2
3
1
1
3
1
1

1
1
1
1
1
1
1
1
1
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
NA
NA
4
4
2
3
3

48
64
64
64
48
48
48
48
128
48
64
64
48
48
48
48
48
48
48
48
48
48
48
48
48
48
128
32
32
32
32
64
64
128
32
48

uni
uni
uni
uni
uni
uni
uni
uni
uni
uni
uni
uni
uni+5-fold
seq-to-concat
seq-to-concat
seq-to-concat
seq-to-last1
seq-to-last1
uni
uni
uni
uni
uni
bidir
bidir
uni+coughvid
uni+coughvid
uni+AUROC loss
uni+AUROC loss
filter=3
filter=3
filter=5
filter=5
filter=5
uni
uni

1
1
3
1
2
2
2
1
1
1
1
2
2
1
1
1
7
2
3
1
2
2
2
2
2
2
2
1
1
1
1
2
1
1
2
2

utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
global
utt-wise
utt-wise
utt-wise
utt-wise
global
utt-wise
global
global
global
global
global
global
utt-wise
utt-wise
global
global
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise
utt-wise

30
30
30
40
30
30
40
30
30
30
30
40
40
30
30
30
30
30
20
30
40
40
50
40
30
30
40
30
30
inCh=3
inCh=3
inCh=3
inCh=3
inCh=3
30
30

0.001
0
0
0
0
0
0
0
0
0
0
0.001
0
0
0
0.001
0.001
0
0
0
0
0
0
0
0
0
0.001
0
0
0
0
0
0
0
0
0

76.7
76.48
75.78
76.84
74.97
77.08
76.52
73.64
79
75.03
77.17
75.27
75.47
74.58
74.89
72.02
74.98
74.36
76.63
75.03
76.58
76.99
76.01
76.07
76.11
76.72
77.38
77.54
76.13
75.96
76.13
75.96
75.76
75.46
74.29
74.11

62.11
59.5
61
55
52.17
59
55.9
50.31
56.52
55.28
60.87
57.14
54.04
55.28
48.45
55.9
52.17
60.24
50.31
46.58
63.35
57.8
51.55
63.35
62.11
67.08
49.69
69.57
61.49
53.42
54.66
59.63
49.69
56.28
60.25
50.31

ensemble results on our set

85.89

70.19

ensemble results on blind test set (leaderboard)

83.93

70.83

5. Classifier Description
We have experimented with standard machine learning
algorithms such as, support vector machine (SVM) and random
forest classifiers, and deep neural network architectures such
as, LSTM, CNN and ResNet. We experiment with number of
layers, sequence length and class weights in cross entropy (CE)
loss.
We train LSTM based models in four different
configuration. First type is regular uni-directional LSTM,

termed as “uni”. Second type is bi-directional LSTM, termed as
“bidir”. In model type termed as “seq-to-concat”, we
concatenate output vector of all elements in a sequence to
form a super-vector. For example, if output dimension of LSTM
layer is n with a sequence length of seqL then dimension of
super-vector would be n∗seqL. This super-vector is then
passed through the final fully connected layer to predict
probabilities. The “seq-to-concat” type model effectively
processes seqL number of frames before making predictions.

In “seq-to-last1” type model, output corresponding to the last
timestamp of a sequence is passed further. This type of model
makes prediction after processing all frames in a sequence.
Here, the final fully connected layer takes n dimensional vector
as input. It may be noted that both these model types, namely
“seq-to-concat” and “seq-to-last1”, make segment level
predictions. All LSTM models use 20 frames as initial context
before processing seqL number of frames for every sequence.
The initial context frames set the hidden state of LSTM layer
before processing actual inputs for which loss will be back
propagated.
In CNN based models, we use 1-D convolution layers with filter
length 5. Feature vector for each frame is reshaped into 63x3
before passing to CNN models implying that the input is now a
63 dimensional vector with 3 channels. After convolution
layer, batch normalization is applied and then relu activation is
applied. We also use Residual Networks [18], namely ResNet18
and ResNet-34, in our experimentation. To reduce learnable
weights, we decrease filter size to 3 with 32 output channels
for every convolution layer in default residual networks
configuration.

sound sequentially and one covid cough sound randomly. We
pass these two cough sounds through model and average the
output of final fully connected layer for individual cough
sounds. Thus, we have 1-D output value for each covid and
non-covid cough. We then calculate the loss described in Eq 1
and back propagate the gradients to train the model.
5.2. JVAE
Following [22], the variational lower bound (ELBO) of joint VAE
(JVAE) can be written as

To train JVAE model, we minimize negative of the ELBO
described in Eq 2. In original JVAE formulation, all the above
conditional distributions are modelled by diagonal Gaussian
distribution. We now propose to model pθ(y|x,z) by a binomial
distribution.
Now, minimizing negative of
Eqφ(z|x) log pθ(y|x,z) in ELBO (Eq 2) reduces it to binary cross
entropy (BCE) loss. In practice, the actual loss used to train
JVAE network is given by
Loss = λ1 MSEx + λ2 BCEy + λ3 KLD,

(3)

We choose value of hyper parameters in Eq 3 following [22].
Thus, λ1 is taken as 1, λ2 is taken as 10 and λ3 is taken as 0.1.

6. Results

Figure 5: Spectrum interpolation based augmentation of
COVID-19 positive cough sound signal and spectrogram shown
in Figure 1.

5.1. AUROC Loss
In this challenge, our main task is to maximize area under curve
(AUC). It is known that CE loss based classifier training does not
have proportional relationship with AUC. We now formulate
the AUROC loss function. We pick one non-covid cough sound
at random and let Vn be its predicted value. Similarly, we pick
one covid cough sound at random and Vp be its predicted value
[19, 20, 21]. Then AUC can be seen proportional to the
probability that the predicted values are in right order, that is
Vn < Vp. It can be seen that this score is not differentiable as it
does not make smooth transitions. So to force differentiability,
we apply continuous approximation to this score using sigmoid
function. Specifically, we calculate binary cross entropy (BCE)
loss with assumpion that Sigmoid (Vp − Vn) belongs to class
1. For stable model training, the final loss we use is given by
Loss = BCE(Sigmoid(Vp − Vn),1)
+ BCE(Sigmoid(Vn − Vp),0)

(1)

The model architecture we use with AUROC loss has only one
output node. From dataset, we pick one non-covid cough

Patient level receiver operating characteristic (ROC) on
validation set is presented in Fig. 1 and Fig. 2. It can be
observed that the proposed data augmentation methods and
architectures give significant improvement in the area under
the curve (AUC). Detailed results are shown in Table 1. The first
column depicts the type of model used. The “fold” column
mentions the dataset fold on which the model is trained and
validated. For all the experiments, we train our classifier model
on each of the 5 folds of dataset and then choose the best
model. Hidden dimension of layer before the final fully
connected layer in our model architecture is presented in
“hidden dim” column. Variation in model architecture type and
its specialities is disclosed in “type” column. Details of these
variations are discussed in Section 5.
In CE loss, values in “wtPos” column is multiplied with loss
corresponding to covid positive cough data. This is done to
counter the imbalance of covid vs non-covid cough sounds in
dataset. We normalize the input feature vector with mean
before passing through classifier model during both training
and testing. The “utt-wise” token in “norm” column means
mean is calculated per-utterance. We also calculate mean of
all dataset beforehand, termed as global-mean. The “global”
token in “norm” column specifies that the global-mean vector
is used for feature normalization. This is done to ensure that
handcrafted features do not vanish for any input cough sound.
It also ensures that inter-frame variations in handcrafted
features and their absolute values remain expressive.
The sequence length parameter of LSTM models is
reported in “seq length” column. We experiment with

sequence lengths 20, 30, 40 and 50. For CNN and residual
networks, batches are not prepared sequentially. So, we
mention number of channels (“inCh”) of input features. The
Adam optimizer in pytorch toolkit offers weight regularization
in form of “weight decay”. We use weight decay with a factor
of 10−3 for training of some models. Rest of the models are
trained without weight decay – mentioned as 0 in “weight
decay” column. We report AUC yielded by classifier models in
“AUC” column. We also specificity at > 80% sensitivity in
“specificity” column of Table 1.

7. Conclusion
In this paper we explore various deep learning architectures,
data augmentation techniques, and feature extraction method
to improve state of the art cough sound based COVID-19
detection system. Our proposed system gives 14% absolute
improvement in area under the curve (AUC) compare to
baseline system released as part of the challenge.
Future work includes the experimentations with attention
based architectures on raw audio. It will also be interesting to
explore the developed method for other cough symptom
based ailments.

8. References
[1]

Jouhyun Jeon, Gaurav Baruah, Sarah Sarabadani, and Adam
Palanica, “Identification of Risk Factors and Symptoms of COVID19: Analysis of Biomedical Literature and Social Media Data,”
Journal of Medical Internet Research, vol. 22, no. 10, 2020.

[2]

V. P. Singh, J. M. S. Rohith and V. K. Mittal, “Preliminary analysis
of cough sounds,” Annual IEEE India Conference (INDICON),
2015, pp. 1-6, doi: 10.1109/INDICON.2015.7443512

[3]

Singh, Vishwanath and Rohith, J. and Mittal, Yash and Mittal,
Vinay, “IIIT-S CSSD: A Cough Speech Sounds Database,” Twenty
Second National Conference on Communication (NCC) 2016, pp 16. 10.1109/NCC.2016.7561190.

[4]

William Thorpe, Miranda Kurver, Greg King and Cheryl Salome,
“Acoustic analysis of cough,” Seventh Australian and New
Zealand Intelligent Information Systems Conference, pp. 391–
394, November 2001.

[5]

R. S. Jha, V. P. Singh and V. K. Mittal, “Discriminant feature
vectors for characterizing ailment cough vs. simulated cough,”
IEEE Region 10 Conference (TENCON), 2016, pp. 2461-2465, doi:
10.1109/TENCON.2016.7848475.

[6]

Thomas Quatieri, Tanya Talkar, and Jeffrey Palmer, “ A
Framework for Biomarkers of COVID-19 Based on Coordination
of Speech Production Subsystems,” IEEE Open Journal of
Engineering in Medicine and Biology, (2009.11644), 1:203–206,
2020.

[7]

Lara Orlandic, Tomas Teijeiro,´ and David Atienza, “ The
COUGHVID crowdsourcing dataset: A corpus for the study of
large-scale cough analysis algorithms,” arXiv, (2009.11644),
2020. 11 pages.

[8]

Neeraj Sharma, Prashant Krishnan, Rohit Kumar, Shreyas Ramoji,
Srikanth Raj Chetupalli, Nirmala R., Prasanta Kumar Ghosh, and
Sriram Ganapathy, “Coswara – A Database of Breathing, Cough,
and Voice Sounds for COVID-19 Diagnosis,” In Proc. Interspeech,
pages 4811–4815, Shanghai, China, 2020.

[9]

Piyush Bagad, Aman Dalmia, Jigar Doshi, Arsha Nagrani, Parag
Bhamare, Amrita Mahale, Saurabh Rane, Neeraj Agarwal, and
Rahul Panicker, “Cough Against COVID: Evidence of COVID-19

Signature in Cough Sounds,” arXiv, (2009.08790), 2020. 12
pages.
[10] Chloe Brown, Jagmohan Chauhan, Andreas Grammenos, Jing¨
Han, Apinan Hasthanasombat, Dimitris Spathis, Tong Xia, Pietro
Cicuta, and Cecilia Mascolo, “ Exploring Automatic Diagnosis of
COVID-19 from Crowdsourced Respiratory Sound Data,” In Proc.
Knowledge Discovery and Data Mining, pages 3474–3484, 2020.
[11] Katrin D. Bartl-Pokorny, Florian B. Pokorny, Anton Batliner,
Shahin Amiriparian, Anastasia Semertzidou, Florian Eyben, Elena
Kramer, Florian Schmidt, Rainer Schonweiler, Markus Wehler,
and¨ Bjorn W. Schuller, “The voice of COVID-19: Acoustic
correlates¨ of infection,” arXiv, 2020, 8 pages.
[12] Kotra Venkata Sai Ritwik, Shareef Babu Kalluri, and Deepu
Vijayasenan, “COVID-19 Patient Detection from Telephone
Quality Speech Data,” arXiv, (2011.04299), 2020. 6 pages.
[13] Jordi Laguarta, Ferran Hueto, and Brian Subirana, “COVID-19
Artificial Intelligence Diagnosis using only Cough Recordings,”
IEEE Open Journal of Engineering in Medicine and Biology, pages
1–1, 2020.
[14] Gadi Pinkas, Yarden Karny, Aviad Malachi, Galia Barkai, Gideon
Bachar, and Vered Aharonson, “SARSCoV-2 Detection From
Voice,” IEEE Open Journal of Engineering in Medicine and
Biology, 1:268–274, 2020.
[15] Ali Imran, Iryna Posokhova, Haneya Naeem Qureshi, Usama
Masood, Sajid Riaz, Kamran Ali, Charles N. John, and Muhammad
Nabeel, “AI4COVID-19: AI Enabled Preliminary Diagnosis for
COVID-19 from Cough Samples via an App,” arXiv, (2004.01275),
2020, 27 pages.
[16] Shi Yin, Chao Liu, Zhiyong Zhang, Yiye Lin, Dong Wang, Javier
Tejedor, Thomas Fang Zheng, and Yinguo Li , “Noisy training for
deep neural networks in speech recognition,” EURASIP Journal
on Audio, Speech, and Music Processing, 2015.
[17] Navdeep Jaitly, and Geoffrey E. Hinton, “Vocal Tract Length
Perturbation (VTLP) improves speech recognition,” Proceedings
of the 30th International Conference on Machine Learning,
Atlanta, Georgia, USA, 2013. JMLR: W and CP volume 28.
[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 770–778.
[19] [Online]. Available: https://github.com/iridiumblue/roc-star/
[20] [Online]. Available: https://www.erikdrysdale.com/auc max/
[21] L. Yan, R. H. Dodier, M. Mozer, and R. H. Wolniewicz, “Optimizing
classifier performance via an approximation to the
wilcoxonmann-whitney statistic,” in Proceedings of the 20th
international conference on machine learning (icml-03), 2003,
pp. 848–855.
[22] M. K. Chelimilla, S. Kumar, and S. P. Rath, “Joint distribution
learning in the framework of variational autoencoders for farfield speech enhancement,” in Accepted in ASRU. IEEE, 2019.

