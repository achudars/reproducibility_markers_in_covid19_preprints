1

COVID-19 Cough Classification using Machine
Learning and Global Smartphone Recordings

arXiv:2012.01926v2 [cs.SD] 14 Jun 2021

Madhurananda Pahar, Marisa Klopper, Robin Warren, and Thomas Niesler
Abstract—We present a machine learning based COVID-19 cough classifier which can discriminate COVID-19 positive coughs from
both COVID-19 negative and healthy coughs recorded on a smartphone. This type of screening is non-contact, easy to apply, and can
reduce the workload in testing centres as well as limit transmission by recommending early self-isolation to those who have a cough
suggestive of COVID-19. The datasets used in this study include subjects from all six continents and contain both forced and natural
coughs, indicating that the approach is widely applicable. The publicly available Coswara dataset contains 92 COVID-19 positive and
1079 healthy subjects, while the second smaller dataset was collected mostly in South Africa and contains 18 COVID-19 positive and
26 COVID-19 negative subjects who have undergone a SARS-CoV laboratory test. Both datasets indicate that COVID-19 positive
coughs are 15%-20% shorter than non-COVID coughs. Dataset skew was addressed by applying the synthetic minority oversampling
technique (SMOTE). A leave-p-out cross-validation scheme was used to train and evaluate seven machine learning classifiers: logistic
regression (LR), k-nearest neighbour (KNN), support vector machine (SVM), multilayer perceptron (MLP), convolutional neural network
(CNN), long short-term memory (LSTM) and a residual-based neural network architecture (Resnet50). Our results show that although
all classifiers were able to identify COVID-19 coughs, the best performance was exhibited by the Resnet50 classifier, which was best
able to discriminate between the COVID-19 positive and the healthy coughs with an area under the ROC curve (AUC) of 0.98. An
LSTM classifier was best able to discriminate between the COVID-19 positive and COVID-19 negative coughs, with an AUC of 0.94
after selecting the best 13 features from a sequential forward selection (SFS). Since this type of cough audio classification is
cost-effective and easy to deploy, it is potentially a useful and viable means of non-contact COVID-19 screening.
Index Terms—COVID-19, cough classification, machine learning, logistic regression (LR), k-nearest neighbour (KNN), support vector
machine (SVM), multilayer perceptron (MLP), convolutional neural network (CNN), long short-term memory (LSTM), Resnet50

F

1

I NTRODUCTION

C

OVID19 (COrona VIrus Disease of 2019), caused by
the Severe Acute Respiratory Syndrome Coronavirus
2 (SARS-CoV2) virus, was declared a global pandemic on
February 11, 2020 by the World Health Organisation (WHO).
It is a new coronavirus but similar to other coronaviruses,
including SARS-CoV (severe acute respiratory syndrome
coronavirus) and MERS-CoV (Middle East respiratory syndrome coronavirus) which caused disease outbreaks in 2002
and 2012, respectively [1, 2].
The most common symptoms of COVID-19 are fever, fatigue and dry coughs [3]. Other symptoms include shortness
of breath, joint pain, muscle pain, gastrointestinal symptoms
and loss of smell or taste [4]. At the time of writing, there
were 142.1 million active cases of COVID-19 globally, and
there had been 3 million deaths, with the USA reporting the highest number of cases (31.7 million) and deaths
(567,729) [5]. The scale of the pandemic has caused some
health systems to be overrun by the need for testing and the
management of cases.
Several attempts have been made to identify early symptoms of COVID-19 through the use of artificial intelligence
•

•

Madhurananda Pahar and Thomas Niesler works at Department of Electrical and Electronics Engineering, University of Stellenbosch, Stellenbosch, South Africa - 7600.
E-mail: mpahar@sun.ac.za, trn@sun.ac.za
Marisa Klopper and Robin Warren works at SAMRC Centre for Tuberculosis Research, University of Stellenbosch, Cape Town, South Africa 7505.
E-mail: marisat@sun.ac.za, rw1@sun.ac.za

applied to images. The 50-layer residual neural network
(Resnet50) architecture has been shown to perform better
than other pre-trained models such as AlexNet, GoogLeNet
and VGG16 in these tasks. For example, it has been demonstrated that COVID-19 can be detected from computed
tomography (CT) images with an accuracy of 96.23% by
using a Resnet50 architecture [6]. The same architecture
was shown to detect pneumonia due to COVID-19 with an
accuracy of 96.7% [7] and to detect COVID-19 from x-ray
images with an accuracy of 96.30% [8].
Coughing is one of the predominant symptoms of
COVID-19 [9] and also a symptom of more than 100 other
diseases, and its effect on the respiratory system is known to
vary [10]. For example, lung diseases can cause the airway
to be either restricted or obstructed and this can influence
the acoustics of the cough [11]. It has also been postulated
that the glottis behaves differently under different pathological conditions [12, 13] and that this makes it possible to
distinguish between coughs due to TB [14, 15], asthma [16],
bronchitis and pertussis (whooping cough) [17, 18, 19, 20].
Respiratory data such as breathing, sneezing, speech,
eating behaviour and coughing can be processed by machine learning algorithms to diagnose respiratory illness
[21, 22, 23]. Simple machine learning tools, like binary
classifiers, are able to distinguish COVID-19 respiratory
sounds from healthy counterparts with an area under the
ROC curve (AUC) exceeding 0.80 [24]. Detecting COVID19 by analysing only the cough sounds is also possible.
AI4COVID-19 is a mobile app that records 3 seconds of
cough audio which is analysed automatically to provide

2

COSWARA DATASET

SARCOS DATASET

PREPROCESSING
& FEATURE
EXTRACTION

COVID-19
COUGH
CLASSIFIER

TRAINED AND
EVALUATED ON
COSWARA DATASET
TRAINED ON COSWARA
DATASET AND
EVALUATED ON SARCOS
DATASET

BEST CLASSIFIER
(RESNET 50) ACHIEVES
AUC = 0.98
BEST CLASSIFIER
(LSTM) ACHIEVES AUC
= 0.94 WHEN USING
THE BEST 13 FEATURES

Fig. 1. Location of participants in the Coswara and the Sarcos datasets: Participants in the Coswara dataset were located on five different
continents, excluding Africa. The majority (91%) of participants in the Coswara dataset are from Asia, as indicated in Figure 2. Sarcos participants
who supplied geographical information are mostly (75%) from South Africa, as shown in Figure 3.

an indication of COVID-19 status within 2 minutes [25].
A deep neural network (DNN) was shown to distinguish
between COVID-19 and other coughs with an accuracy of
96.83% on a dataset containing 328 coughs from 150 patients
of four different classes: COVID-19, asthma, bronchitis and
healthy [26]. There appear to be unique patterns in COVID19 coughs that allow a pre-trained Resnet18 classifier to
identify COVID-19 coughs with an AUC of 0.72. In this
case, cough samples were collected over the phone from
3621 individuals with confirmed COVID-19 [27]. COVID-19
coughs were classified with a higher AUC of 0.97 (sensitivity
= 98.5% and specificity = 94.2%) by a Resnet50 architecture,
trained on coughs from 4256 subjects and evaluated on 1064
subjects that included both COVID-19 positive and COVID19 negative subjects by implementing four biomarkers [28].
A high AUC exceeding 0.98 was also achieved in [29] when
discriminating COVID-19 positive coughs from COVID-19
negative coughs on a clinically validated dataset consisting
of 2339 COVID-19 positive and 6041 COVID-19 negative
subjects using DNN based classifiers.
Data collection from COVID-19 patients is challenging
and the datasets are often not publicly available. Nevertheless, efforts have been made to compile such datasets. For
example, a dataset consisting of coughing sounds recorded
during or after the acute phase of COVID-19 from patients
via public media interviews has been developed in [30].
The Coswara dataset is publicly available and collected in
a more controlled and targeted manner [31]. At the time of
writing, this dataset included usable ‘deep cough’ i.e. loud
coughs recordings from 92 COVID-19 positive and 1079
healthy subjects. We have also begun to compile our own
dataset by collecting recordings from subjects who have
undergone a SARS-CoV laboratory test in South Africa. This
Sarcos (SARS COVID-19 South Africa) dataset is currently
still small and includes only 44 subjects (18 COVID-19
positive and 26 COVID-19 negative).
Both the Coswara and Sarcos datasets are imbalanced
since COVID-19 positive subjects are outnumbered by nonCOVID-19 subjects. Nevertheless, collectively these two
datasets contain recordings from all six continents, as
shown in Figure 1. To improve machine learning classification performance, we have applied the synthetic minority over-sampling technique (SMOTE) to balance our
datasets. Furthermore, we have found that the COVID19 positive coughs are 15%-20% shorter than non-COVID
coughs. Hence, feature extraction is designed to preserve
the time-domain patterns over an entire cough. Classifier
hyperparameters were optimised by using the leave-p-out

cross-validation, followed by training and evaluation of machine learning approaches, namely logistic regression (LR),
k-nearest neighbour (KNN), support vector machine (SVM),
multilayer perceptron (MLP) and deep neural networks
(DNN) such as a convolutional neural network (CNN), long
short-term memory (LSTM) and Resnet50. The Resnet50
produced the highest AUC of 0.976 ≈ 0.98 when trained
and evaluated on the Coswara dataset, outperforming the
baseline results presented in [32]. No classifier has been
trained on the Sarcos dataset due to its small size. It can
also not be combined with Coswara as it contains slightly
different classes. Instead, this dataset has been used for an
independent validation of the best-performing DNN classifiers developed on the Coswara dataset. In these validation
experiments, it was found that the highest AUC of 0.938 ≈
0.94 is achieved when using the best 13 features identified
using the greedy sequential forward selection (SFS) algorithm and an LSTM classifier. We conclude that it is possible
to identify COVID-19 on the basis of cough audio recorded
using a smartphone. Furthermore, this discrimination between COVID-19 positive and both COVID-19 negative and
healthy coughs is possible for audio samples collected from
subjects located all over the world. Additional validation is
however still required to obtain approval from regulatory
bodies for use as a diagnostic tool.

2

DATA

We have used two datasets in our experimental evaluation:
the Coswara dataset and the Sarcos dataset.
2.1

The Coswara Dataset

The Coswara project is aimed at developing a diagnostic
tool for COVID-19 based on respiratory, cough and speech
sounds [31]. Public participants were asked to contribute
cough recordings via a web-based data collection platform
using their smartphones (https://coswara.iisc.ac.in). The
collected audio data includes fast and slow breathing, deep
and shallow coughing, phonation of sustained vowels and
spoken digits. Age, gender, geographical location, current
health status and pre-existing medical conditions are also
recorded. Health status includes ‘healthy’, ‘exposed’, ‘cured’
or ‘infected’. Audio recordings were sampled at 44.1 KHz
and subjects were from all continents except Africa, as
shown in Figure 2. In this study, we have made use of
the raw audio recordings and applied pre-processing as
described in Section 2.3.

3
COVID Positive and
Healthy Subjects

Age Distribution

COVID Positive and
Negative Subjects

Male and Female
Subjects

100

Healthy

COVID
Positive

18
COVID
Negative

282

0
0

20

40

60

80

FEMALE

MALE

COVID
Positive

Subjects with
COVID-19 contacts

9

17

40
20

13

27

26

889

60
92

Days since the Lab Test
14

80
1079

Male and Female
Subjects

4
Female

Male

1 to 3

4 to 6

3

1

7 to 9 10 to 12 13 to 15

>15

Days since Coughing started

Do they have a
Normal Cough?
25

26

Asia (91%)

25
19

18

5

Australia (0.14%)
0

Europe (2.75%)

No

North America (5.5%)

Yes

No

Yes

5

3

2

1 to 3 4 to 6 7 to 9 10 to
12

1

3

13 to
15

>15

Country of Origin

South America (0.14%)

2% 2%
Brazil (1)

Fig. 2. Coswara dataset at the time of experimentation: There are
1079 healthy and 92 COVID-19 positive subjects in the pre-processed
dataset, used for feature extraction and classifier training. Most of the
subjects are aged between 20 and 50. There are 282 female and 889
male subjects and most of them are from Asia. Subjects are from five
continents: Asia (Bahrain, Bangladesh, China, India, Indonesia, Iran,
Japan, Malaysia, Oman, Philippines, Qatar, Saudi Arabia, Singapore,
Sri Lanka, United Arab Emirates), Australia, Europe (Belgium, Finland,
France, Germany, Ireland, Netherlands, Norway, Romania, Spain, Sweden, Switzerland, Ukraine, United Kingdom), North America (Canada,
United States), and South America (Argentina, Mexico).

2.2

The Sarcos Dataset

A similar initiative in South Africa encouraged participants
to allow the voluntarily recording their coughs using an
online platform (https://coughtest.online) under the research project name: ‘COVID-19 screening by cough sound
analysis’. This dataset will be referred to as ‘Sarcos’ (SAR
COVID-19 South Africa). Only coughs were collected as
audio samples, and only subjects who had recently undergone a SARS-CoV laboratory test were asked to participate.
The sampling rate for the audio recordings was 44.1 KHz.
In addition to the cough audio recordings, subjects were
presented with a voluntary and anonymous questionnaire,
providing informed consent. The questionnaire prompted
for the following information.
•
•
•
•
•
•
•
•
•
•

Age and gender.
Whether tested by an authorised COVID-19 testing
centre.
Days since the test was performed.
Lab result (COVID-19 positive or negative).
Country of residence.
Known contact with COVID-19 positive patient.
Known lung disease.
Symptoms and temperature.
Whether they are a regular smoker.
Whether they have a current cough and for how
many days.

Among the 44 participants, 33 (75%) subjects asserted
that they are South African residents and therefore represent
the African continent, as shown in Figure 3. As there were
no subjects from Africa in the Coswara dataset, together the
Coswara and Sarcos dataset include subjects from all six
continents.

21%

India (1)
Prefer not to say (9)

75%
South Africa (33)

Fig. 3. Sarcos dataset at the time of experimentation: There are 26
COVID-19 negative and 18 COVID-19 positive subjects in the processed
dataset. Unlike the Coswara dataset, there are more female than male
subjects. Most of the subjects had their lab test performed within two
weeks of participation. Only 19 of the subjects reported coughing as a
symptom, and for these the reported duration of coughing symptoms
was variable. There were 33 subjects from Africa (South Africa), 1 from
South America (Brazil), 1 from Asia (India) and the rest declined to
specify their geographic location.

2.3

Data Pre-processing

The raw cough audio recordings from both datasets have
the sampling rate (µ) of 44.1 KHz and is subjected to some
simple pre-processing steps, described below. We note, timewindow length (λ) as 0.05 seconds and amplitude threshold
value (Φ) as 0.005, where both of these values were determined manually and interactively, as the silence removal
was validated by visual inspection in all cases.
The original cough audio ci (t) is normalised by following Equation 1.

ci (t) = 0.9 ×

ci (t)
|max(ci (t))|

(1)

The processed final cough audio is shown in Figure 4
and noted as: C(t). Here, I denotes the time-window and
we define:

CI (t) = Cjµλ (t) · · · C(j+1)µλ (t)

(2)

For example, when j = 0; CI will be the portion of signal
where C0 · · · C2205 , as µ = 44100 Hz and λ = 0.05 seconds.
Ξ
0 ≤ j ≤ b µλ
c, where Ξ is the length of signal ci (t). C(t) is
calculated by following Equation 3.
n
C(t) = C(t) ⊕ CI if CI (t) ≥ Φ
(3)
where, ⊕ means concatenation and, CI (t) ≥ Φ, if CIi (t) ≥
Φ, where ∀i ∈ I .
Thus, the amplitudes of the raw audio data in the
Coswara and the Sarcos dataset were normalised, after
which periods of silence were removed from the signal

4

to within a 50 ms margin using a simple energy detector.
Figure 4 shows an example of the original raw audio, as
well as the pre-processed audio.

FEATURE
EXTRACTION

MFCC
MFCC ∆
MFCC ∆∆

LOG ENERGIES
PRE-PROCESSED COUGH
AUDIO

SPLIT THE PRE-PROCESSED COUGH
INTO MULTIPLE SEGMENTS

ZCR
KURTOSIS
FEATURES FOR TRAINING
AND TESTING

Fig. 5. Feature Extraction: Pre-processed cough audio recordings,
shown in Figure 4, are split into individual segments after which features
such as MFCCs, MFCCs velocity (∆), MFCCs acceleration (∆∆), log
frame energies, ZCR and kurtosis are extracted. So, for M number
of MFCCs and S number of segments, the final feature matrix has
(3M + 3, S ) dimensions.

Fig. 4. A processed COVID-19 cough audio which is shorter than the
original cough audio but keeps all spectrum resolution. Amplitudes are
normalised and extended silences are removed in the pre-processing.

After pre-processing, the Coswara dataset contains 92
COVID-19 positive and 1079 healthy subjects and the Sarcos
dataset contains 18 COVID-19 positive and 26 COVID19 negative subjects, as summarised in Table 1. In both
datasets, COVID-19 positive coughs are 15%-20% shorter
than non-COVID coughs.
2.4

Dataset Balancing

Table 1 shows that COVID-19 positive subjects are underrepresented in both datasets. To compensate for this imbalance, which can detrimentally affect machine learning [33, 34], we have applied SMOTE data balancing to
create equal number of COVID-19 positive coughs during
training [35, 36]. This technique has previously been successfully applied to cough detection and classification based
on audio recordings [15, 18, 37].
SMOTE oversamples the minor class by generating synthetic examples, instead of for example random oversampling. In our dataset, for each COVID-19 positive cough, 5
other COVID-19 positive coughs were randomly chosen and
the closest in terms of the Euclidean distance is identified
as xN N . Then the synthetic COVID-19 positive samples are
created using Equation 4.

xSM OT E = x + u · (xN N − x)

(4)

The multiplicative factor u is uniformly distributed between 0 and 1 [38].
We have also implemented other extensions of SMOTE
such as borderline-SMOTE [39, 40] and adaptive synthetic
sampling [41]. However, the best results were obtained by
using SMOTE without any modification.

3

F EATURE E XTRACTION

The feature extraction process is illustrated in Figure 5. Features such as mel-frequency cepstral coefficients (MFCCs),
log frame energies, zero crossing rate (ZCR) and kurtosis
are extracted. MFCCs have been used very successfully
as features in audio analysis and especially in automatic
speech recognition [42, 43]. They have also been found to

be useful in differentiating dry coughs from wet coughs
[44] and classifying tuberculosis coughs [45]. We have used
the traditional MFCC extraction method considering higher
resolution MFCCs along with the velocity (first-order difference, ∆) and acceleration (second-order difference, ∆∆) as
adding these has shown classifier improvement in the past
[46]. Log frame energies can improve the performance in
audio classification tasks [47]. The ZCR [48] is the number
of times a signal changes sign within a frame, indicating the
variability present in the signal. The kurtosis [49] indicates
the tailedness of a probability density. For the samples
of an audio signal, it indicates the prevalence of higher
amplitudes. These features have been extracted by using
the hyperparameters described in Table 2 for all cough
recordings.
We have extracted features in a way that preserves the
information regarding the beginning and the end of a cough
event to allow time-domain patterns in the recordings to be
discovered while maintaining the fixed input dimensionality expected by, for example, a CNN. From every recording,
we extract a fixed number of features S by distributing
the fixed-length analysis frames uniformly over the timeinterval of the cough. The input feature matrix for the
classifiers then always has the dimension of (3M + 3, S ) for
M number of MFCCs along with M number of velocity (∆)
and M number of acceleration (∆∆), as illustrated in Figure
5. If Λ is the number of samples in the cough audio, we
can calculate the number of samples between consecutive
frames δ using Equation 5.

δ=

 
Λ
S

(5)

So, for example a 2.2 second long cough audio event
contains 97020 samples, as the sampling rate is 44.1 KHz. If
the frame length is 1024 samples and number
of segments

are 100, then the frame skip (δ ) is

97020
100

= 971 samples.

In contrast with the more conventionally applied fixed
frame rates, this way of extracting features ensures that
the entire recording is captured within a fixed number
of frames, allowing especially the CNN classifiers to discover more useful temporal patterns and provide better
classification performance. This particular method of feature
extraction has also shown promising result in classifying
COVID-19 breath and speech [37].

5

TABLE 1
Summary of the Coswara and Sarcos Datasets: In the COSWARA dataset, there were 1171 subjects with usable ‘deep cough’ recordings, 92
of whom were COVID-19 positive while 1079 were healthy. This amounts to a total of 1.05 hours of cough audio recordings (after pre-processing)
that will be used for experimentation. The Sarcos dataset contains data from a total of 44 subjects, 18 of whom are COVID-19 positive and 26 who
are not. This amounts to a total of 2.45 minutes of cough audio recordings (after pre-processing) that has been used for experimentation.
COVID-19 positive coughs are 15%-20% shorter than non-COVID coughs.
Dataset
Coswara
Coswara
Coswara
Sarcos
Sarcos
Sarcos

4

Label
COVID-19 Positive
Healthy
Total
COVID-19 Positive
COVID-19 Negative
Total

Subjects
92
1079
1171
18
26
44

Total audio
4.24 mins
0.98 hours
1.05 hours
0.87 mins
1.57 mins
2.45 mins

Average per subject
2.77 sec
3.26 sec
3.22 sec
2.91 sec
3.63 sec
3.34 sec

Standard deviation
1.62 sec
1.66 sec
1.67 sec
2.23 sec
2.75 sec
2.53 sec

C LASSIFIER ARCHITECTURES
1

We have trained and evaluated seven machine learning
classifiers in total. LR models have been found to outperform other more complex classifiers such as classification trees, random forests, SVM in some clinical prediction
tasks [14, 50, 51]. We have used gradient descent weight
regularisation as well as lasso (l1 penalty) and ridge (l2
penalty) estimators during training [52, 53]. This LR classifier has been intended primarily as a baseline against
which any improvements offered by the more complex
architectures can be measured. A KNN classifier bases its
decision on the class labels of the k nearest neighbours
in the training set and in the past has been able to both
detect [54, 55, 56] and classify [17, 45, 57] sounds such as
coughs and snores successfully. SVM classifiers have also
performed well in both detecting [58, 59] and classifying
[60] cough events. The independent term in kernel functions
is chosen as a hyperparameter while optimising the SVM
classifier. An MLP, a neural network with multiple layers
of neurons separating the input and output [61], is capable
of learning non-linear relationships and have for example
been shown to be effective when discriminating influenza
coughs from other coughs [62]. MLP have also been applied
to classify tuberculosis coughs [45, 59] and detect coughs in
general [63, 64]. The penalty ratios, along with the number
of neurons are used as the hyperparameters which were
optimised using the leave-p-out cross-validation process
(Figure 8 and Section 5.2).
A CNN is a popular deep neural network architecture,
primarily used in image classification [65]. For example,
in the past two decades CNNs were applied successfully
to complex tasks such as face recognition [66]. It has also
performed well in classifying COVID-19 breath and speech
[37]. A CNN architecture [67, 68] along with the optimised hyperparameters (Table 3) is shown in Figure 6. An
LSTM model is a type of recurrent neural network whose
architecture allows it to remember previously-seen inputs
when making its classification decision [69]. It has been
successfully used in automatic cough detection [15, 70], and
also in other types of acoustic event detection [71, 72]. The
hyperparameters optimised for the LSTM classifier [73] are
mentioned in Table 3 and visually explained in Figure 7. The
50-layer deep residual learning (Resnet50) neural network
[74] is a very deep architecture that contains skip layers, and
has been found to outperform other very deep architectures

α2

α1

α4

8

0

α2
INPUT FEATURE
MATRIX

CONVOLUTIONAL
2D LAYERS

MAX-POOLING
WITH
DROPOUT

REDUCE DENSE
LAYER UNITS TO 8,
THEN 2-UNIT
SOFTMAX

FLATTENING
WITH
DROPOUT

RATE = α3

RATE = α3

Fig. 6. CNN Classifier: Our CNN classifier uses α1 two-dimansional
convolutional layers with kernel size α2 , rectified linear units as activation functions and a dropout rate of α3 . After max-pooling, two
dense layers with α4 and 8 units respectively and rectified linear activation functions follow. The network is terminated by a two-dimensional
softmax where one output (1) represents the COVID-19 positive class
and the other (0) healthy or COVID-19 negative class. During training,
features are presented to the neural network in batches of size β3 for β4
epochs.

1

β1
INPUT FEATURE
MATRIX

β1 LSTM UNITS

α4
FLATTENING
WITH DROPOUT
RATE = α3

8

0
REDUCE DENSE
LAYER UNITS TO
8, THEN 2-UNIT
SOFTMAX

Fig. 7. LSTM classifier: Our LSTM classifier has β1 LSTM units, each
with rectified linear activation functions and a dropout rate of α3 . This
is followed by two dense layers with α4 and 8 units respectively and
rectified linear activation functions. The network is terminated by a twodimensional softmax where one output (1) represents the COVID-19
positive class and the other (0) healthy or COVID-19 negative class.
During training, features are presented to the neural network in batches
of size β3 for β4 epochs.

such as VGGNet. It performs particularly well on image
classification tasks on the dataset such as ILSVRC, the CIFAR10 dataset and the COCO object detection dataset [75].
Resnet50 has already been used in successfully detecting
COVID-19 from CT images [6], coughs [28], breath, speech
[37] and Alzheimer’s [76]. Due to extreme computation load,
we have used the default Resnet50 structure mentioned in
Table 1 of [74].

6

5

C LASSIFICATION P ROCESS

FULL DATASET OF N SUBJECTS

5.1 Hyperparameter Optimisation
Both feature extraction and classifier architectures have a
number of hyperparameters. They are listed in Table 2 and 3
and were optimised by using a leave-p-out cross-validation
scheme.

NEXT TEST SET

OUTER LOOP
N - J SUBJECTS (classifier performance)

J SUBJECTS

NEXT DEV SET

TABLE 2
Feature extraction hyperparameters optimised using the leave-p-out
cross-validation as described in Section 5.2

Hyperparameter
MFCC (M)
Frame (F )
Seg (S )

Description
Number of lower-order
MFCCs to keep
Frame-size in which
audio is segmented
Number of frames
extracted from the audio

K SUBJECTS

Range
13 × k1 , where
k1 = 1, 2, 3, 4, 5
2k2 where
k2 = 8, · · · , 12
10 × k3 , where
k3 = 5, 7, 10, 12, 15

As the sampling rate is 44.1 KHz in both the Coswara
and Sarcos dataset, by varying the frame lengths from 28
to 212 i.e. 256 to 4096 samples, features are extracted from
frames whose duration varies between approximately 5 and
100 ms. Different phases in a cough carry important features
[44] and thus each cough has been divided between 50 and
150 segments with steps of 20 to 30, as shown in Figure 5. By
varying the number of lower-order MFCCs to keep (from 13
to 65, with steps of 13), the spectral resolution of the features
was varied.
5.2 Cross-validation
All our classifiers have been trained and evaluated by using
a nested leave-p-out cross-validation scheme, as shown in
Figure 8 [77]. Since only the Coswara dataset was used for
training and parameter optimisation, N = 1171 in Figure 8.
We have set the train and test split as 4 : 1; as this ratio
has been used effectively in medical classification tasks [78].
Thus, J = 234 and K = 187 in our experiments.
The figure shows that, in an outer loop, J subjects are
removed from the complete set of N subjects to be used
for later independent testing. Then, a further K subjects are
removed from the remaining N − J subjects to serve as a
development set to optimise the hyperparameters listed in
Table 3. The inner loop considers all such sets of K subjects,
and the optimal hyperparameters are chosen on the basis of
all these partitions. The resulting optimal hyperparameters
are used to train a final system on all N − J subjects which
is evaluated on the test set consisting of J subjects. If the
N − J subjects in the training portion contain C1 COVID-19
positive and C2 COVID-19 negative coughs, then (C2 − C1 )
synthetic COVID-19 positive coughs are created by using
SMOTE. AUC has always been the optimisation criterion in
this cross-validation. This entire procedure is repeated for
all possible non-overlapping test sets in the outer loop. The
final performance is evaluated by calculating and averaging
AUC over these outer loops.
This cross-validation procedure makes the best use of
our small dataset by allowing all subjects to be used for
both training and testing purposes while ensuring unbiased
hyperparameter optimisation and a strict per-subject separation between cross-validation folds.

INNER LOOP
(hyperparameters)

TEST

N – J – K SUBJECTS

TRAIN

DEV
EVALUATE

CHOOSE OPTIMUM
HYPERPARAMETERS
EVALUATE

Fig. 8. Leave p-out cross-validation, used to train and evaluate the
classifiers. The development set (DEV) consisting K subjects has been
used to optimise the hyperparameters while training on the TRAIN set,
consisted of N − J − K subjects. The final evaluation of the classifiers
in terms of the AUC occurs on the TEST set, consisting J subjects.

5.3

Classifier Evaluation

Receiver operating characteristic (ROC) curves were calculated within the inner and outer loops shown in Figure 8.
The area under the ROC curve (AUC) indicates how well the
classifier has performed over a range of decision thresholds
[79]. From these ROC curves, the decision that achieves an
equal error rate (γEE ) was computed. This is the threshold
for which the difference between the classifier’s true positive rate (TPR) and false positive rate (FPR) is minimised.
We note the mean per-frame probability that a cough is
from a COVID-19 positive subject by P̂ :
K
P

P̂ =

P (Y = 1|Xi , θ)

i=1

(6)

K

where K indicates the number of frames in the cough and
P (Y = 1|Xi , θ) is the output of the classifier for feature
vector Xi and parameters θ for the ith frame. Now we define
the indicator variable C as:
(
1 if P̂ ≥ γEE
C=
(7)
0 otherwise
We then define two COVID-19 index scores (COV ID I1
and COV ID I2 ) in Equations 8 and 9 respectively.
N
P1

COV ID I1 =

C

i=1

N1

(8)

7

TABLE 3
Classifier hyperparameters, optimised using the leave-p-out cross-validation as described in Section 5.2
Hyperparameter

Description

Classifier

ν1
ν2
ν3
ξ1
ξ2
ζ1
ζ2
η1
η2
η3
α1
α2
α3
α4
β1
β2
β3
β4

Regularisation strength
l1 penalty
l2 penalty
Number of neighbours
Leaf size
Regularisation strength
Kernel Coefficient
No. of neurons
l2 penalty
Stochastic gradient descent
No. of Conv filters
Kernel size
Dropout rate
Dense layer size
LSTM units
Learning rate
Batch Size
No. of epochs

LR
LR
LR
KNN
KNN
SVM
SVM
MLP
MLP
MLP
CNN
CNN
CNN, LSTM
CNN, LSTM
LSTM
LSTM
CNN, LSTM
CNN, LSTM

N
P2

COV ID I2 =

6

to 107 )

to 107 )
to 107 )
to 107 )

P (Y = 1|Xi )

i=1

(9)
N2
In Equation 8, N1 is the number of coughs from the
subject in the recording while in Equation 9, N2 indicates
the total number of frames of cough audio gathered from
the subject. Hence Equation 6 computes a per-cough average
probability while Equation 9 computes a per-frame average
probability. For the Coswara dataset, N1 = 1.
The COVID-19 index scores, given by Equations 8 and 9,
can both be used to make classification decisions. We have
found that for some classifier architectures one will lead to
better performance than the other. Therefore, we have made
the choice of the scoring function an additional hyperparameter to be optimised during cross-validation.
We have calculated the specificity and sensitivity from
these predicted values and then compared them with the
actual values and finally calculated the AUC and used it
as a method of evaluation. The mean specificity, sensitivity,
accuracy and AUC along with the optimal hyperparameters
for each classifier are shown in Table 4 and 5.

6.1

Range
10i1 where i1 = −7, −6, . . . , 6, 7 (10−7
0 to 1 in steps of 0.05
0 to 1 in steps of 0.05
10 to 100 in steps of 10
5 to 30 in steps of 5
10i3 where i3 = −7, −6, . . . , 6, 7 (10−7
10i4 where i4 = −7, −6, . . . , 6, 7 (10−7
10 to 100 in steps of 10
10i2 where i2 = −7, −6, . . . , 6, 7 (10−7
0 to 1 in steps of 0.05
3 × 2k4 where k4 = 3, 4, 5
2 and 3
0.1 to 0.5 in steps of 0.2
2k5 where k5 = 4, 5
2k6 where k6 = 6, 7, 8
10k7 where k7 = −2, −3, −4
2k8 where k8 = 6, 7, 8
10 to 250 in steps of 20

R ESULTS
Coswara dataset

Classification performance for the Coswara dataset is shown
in Table 4. The Coswara results are the average specificity, sensitivity, accuracy and AUC along with its standard
deviation calculated over the outer loop test-sets during
cross-validation. These tables also show the values of the
hyperparameters which produce the highest AUC during
cross-validation.
Table 4 shows that all seven classifiers can classify
COVID-19 coughs and the Resnet50 classifier exhibits the
best performance, with an AUC of 0.976 when using a 120dimensional feature matrix consisting of 39 MFCCs with
appended velocity and acceleration extracted from frames
that are 1024 samples long and when grouping the coughs

Fig. 9. Mean ROC curves for the classifiers trained and evaluated
on the Coswara dataset: The highest AUC of 0.98 was achieved by
the Resnet50, while the LR classifier has the lowest AUC of 0.74.

into 50 segments. The corresponding accuracy is 95.3%
with sensitivity 93% and specificity 98%. The CNN and
LSTM classifiers also exhibited good performance, with
AUCs of 0.953 and 0.942 respectively, thus comfortably
outperforming the MLP, which achieved an AUC of 0.897.
The optimised LR and SVM classifiers showed substantially
weaker performance, with AUCs of 0.736 and 0.815 respectively. Table 4 also shows that DNN classifiers exhibit lower
standard deviation across the folds than other classifiers.
This suggests that DNN classifiers are also prone to perform
better on new datasets without further hyperparameter
optimisation.
The mean ROC curves for the optimised classifier of each
architecture are shown in Figure 9. We see that LSTM, CNN
and Resnet50 classifiers achieve better performance than
the remaining architectures at most operating points. Furthermore, the figure confirms that the Resnet50 architecture
also in most cases achieved better classification performance
than the CNN and LSTM. There appears to be a small region
of the curve where the CNN outperforms the Resnet50
classifier, but this will need to be verified by future further
experimentation with a larger dataset.

8

TABLE 4
Classifier performance when training and evaluating on the Coswara dataset. The best two classifiers along with their feature extraction and
optimal classifier hyperparameters are mentioned. The area under the ROC curve (AUC) has been the optimisation criterion during
cross-validation. The mean specificity (spec), sensitivity (sens), accuracy (ACC) and standard deviation of AUC (σAU C ) are also shown. The best
performance is achieved by the Resnet50.

Classifier

Best Feature
Hyperparameters

Optimal Classifier Hyperparameters
(Optimised inside nested cross-validation)

Spec

Performance
Sens
ACC

AUC

σAU C

LR
LR
KNN
KNN
SVM
SVM
MLP
MLP
CNN
CNN
LSTM
LSTM
Resnet50
Resnet50

M=13, F =1024, S =120
M=26, F =1024, S =70
M=26, F =2048, S =100
M=26, F =1024, S =70
M=39, F =2048, S =100
M=26, F =1024, S =50
M=26, F =2048, S =100
M=13, F =1024, S =100
M=26, F =1024, S =70
M=39, F =1024, S =50
M=13, F =2048, S =70
M=26, F =2048, S =100
M=39, F =1024, S =50
M=26, F =1024, S =70

ν1 = 10−4 , ν2 = 0.25, ν3 = 0.75
ν1 = 10−2 , ν2 = 0.45, ν3 = 0.55
ξ1 = 70, ξ2 = 20
ξ1 = 60, ξ2 = 25
ζ1 = 10−2 , ζ2 = 10−3
ζ1 = 10−4 , ζ2 = 102
η1 = 40, η2 = 10−3 , η3 = 0.4
η1 = 60, η2 = 10−1 , η3 = 0.55
α1 =48, α2 =2, α3 =0.3, α4 =16, β3 =128, β4 =130
α1 =96, α2 =2, α3 =0.1, α4 =16, β3 =256, β4 =170
β1 =128, β2 =10−3 , α3 =0.3, α4 =32, β3 =256, β4 =150
β1 =256, β2 =10−2 , α3 =0.3, α4 =16, β3 =256, β4 =110
Default Resnet50 (Table 1 in [74])
Default Resnet50 (Table 1 in [74])

57%
59%
65%
64%
74%
74%
87%
84%
99%
98%
97%
97%
98%
98%

94%
74%
83%
81%
71%
74%
88%
68%
90%
90%
91%
90%
93%
93%

0.736
0.729
0.781
0.776
0.815
0.804
0.897
0.833
0.953
0.950
0.942
0.931
0.976
0.963

0.057
0.049
0.041
0.039
0.046
0.051
0.033
0.041
0.039
0.039
0.043
0.041
0.018
0.011

75.70%
66.32%
74.70%
73.80%
72.28%
73.91%
87.51%
76.02%
94.57%
94.35%
94.02%
93.65%
95.33%
95.01%

TABLE 5
Classifier performance when training on the Coswara dataset and evaluating on the Sarcos dataset. The best performance was achieved
by the LSTM classifier, and further improvements were achieved by applying SFS.

Classifier
CNN
LSTM
Resnet50
LSTM + SFS

Best Feature
Hyperparameters
M=26, F =1024, S =70
M=13, F =2048, S =70
M=39, F =1024, S =50
M=13, F =2048, S =70

Optimal Classifier Hyperparameters
(trained on Coswara dataset in Table 4)
α1 =48, α2 =2, α3 =0.3, α4 =16, β3 =128, β4 =130
β1 =128, β2 =10−3 , α3 =0.3, α4 =32, β3 =256, β4 =150
Default Resnet50 (Table 1 in [74])
β1 =128, β2 =10−3 , α3 =0.3, α4 =32, β3 =256, β4 =150

Spec
61%
73%
57%
96%

Performance
Sens
ACC
85% 73.02%
75% 73.78%
93% 74.58%
91% 92.91%

AUC
0.755
0.779
0.742
0.938

We also see from Table 4 that using a larger number of
MFCCs consistently leads to improved performance. Since
the spectral resolution used to compute the 39-dimensional
MFCCs surpasses that of the human auditory system, we
conclude that the classifiers are using information not generally perceivable to the human listeners. We have come
to similar conclusions in previous work considering the
classification of coughing sounds due to tuberculosis [14].
6.2

Sarcos dataset

Classification performance for the Sarcos dataset is shown
in Table 5. Here the CNN, LSTM and Resnet50 classifiers
trained on the Coswara dataset (as shown in Table 4) were
tested on the 44 subjects in Sarcos dataset. No further hyperparameter optimisation was performed and hence Table 5
simply notes the same hyperparameters presented in Table
4. We see that performance has in all cases deteriorated
relative to the better-matched Coswara dataset. The best
performance was achieved by the LSTM classifier, which
achieved an AUC of 0.779. In the next section, we improve
this classifier by applying feature selection.
6.2.1

Feature Selection

As an additional experiment, SFS has been applied to the
best-performing system in Table 5, the LSTM. SFS is a
greedy selection method for the individual feature dimensions that contribute the most towards the classifier performance [80].

Fig. 10. Sequential Forward Selection, when applied to a feature
matrix composed of 13 MFCCs with appended velocity (∆) and acceleration (∆∆), log frame energies, ZCR and kurtosis (Equation 5). Peak
performance is observed after selecting the best 13 features.

The feature selection hyperparameters in these experiments were 13 MFCCs extracted from 2048 samples (i.e.
0.46 sec) long frames while coughs were grouped into 70
segments. Thus, SFS could select from a total of 42 features:
MFCCs along with their velocity (∆) and accelerations
(∆∆), log frame energy, ZCR and Kurtosis (Equation 5).
After performing SFS to the LSTM classifier, a peak AUC
of 0.938 was observed on the Sarcos dataset when using the
best 13 features among those 42, as shown in Figure 10 and
Table 5. These 13 selected features led to an improvement
of AUC from 0.779 to 0.938 (Figure 11) and they include

9

sounds is viable. Since the data has been captured on
smartphones, and since the classifier can in principle also
be implemented on such device, such cough classification
is cost-efficient, easy to apply and deploy. Furthermore,
it could be applied remotely, thus avoiding contact with
medical personnel.
In ongoing work, we are continuing to enlarge our
dataset and to apply transfer learning in order take advantage of the other larger datasets. We are also beginning to
consider the best means of implementing the classifier on a
readily-available consumer smartphone.

Fig. 11. Mean ROC curve for the best performed LSTM classifier
trained on Coswara dataset and evaluated on Sarcos dataset: AUC
of 0.78 has been achieved while using all 42 features. After applying
SFS and selecting the best 13 features, the AUC has been improved to
0.94.

MFCCs ranging from 3 to 12 along with their velocity (∆)
and acceleration (∆∆), suggesting all dimensions of feature
matrix carry equally-important COVID-19 signatures.

7

C ONCLUSION AND F UTURE W ORK

We have developed COVID-19 cough classifiers using
smartphone audio recordings and seven machine learning
architectures. To train and evaluate these classifiers, we
have used two datasets. The first, larger, dataset is publicly
available and contains data from 1171 subjects (92 COVID19 positive and 1079 healthy) residing on all five continents except Africa. The smaller second dataset contains
recordings from 18 COVID-19 positive and 26 COVID-19
negative subjects, 75% of whom reside in South Africa.
Thus, together the two datasets include data from subjects
residing on all six continents. After pre-processing the cough
audio recordings, we have found that the COVID-19 positive coughs are 15%-20% shorter than non-COVID coughs.
Then we have extracted MFCCs, log frame energy, ZCR
and kurtosis features from the cough audio using a special feature extraction technique which preserves the timedomain patterns and then trained and evaluated those seven
classifiers using the nested leave-p-out cross-validation. Our
best-performing classifier is the Resnet50 architecture and is
able to discriminate between COVID-19 coughs and healthy
coughs with an AUC of 0.98 on the Coswara dataset.
These results outperform the baseline result of the AUC
of 0.7 in [32]. When testing on the Sarcos dataset, the
LSTM model trained on the Coswara dataset exhibit the
best performance, discriminating COVID-19 positive coughs
from COVID-19 negative coughs with an AUC of 0.94 while
using the best 13 features determined by sequential forward
selection (SFS). Furthermore, since better performance is
achieved using a larger number of MFCCs than is required
to mimic the human auditory system, we also conclude
that at least some of the information used by the classifiers
to discriminate the COVID-19 coughs and the non-COVID
coughs may not be perceivable to the human ear.
Although the systems we describe require more stringent validation on a larger dataset, the results we have
presented are very promising and indicate that COVID19 screening based on automatic classification of coughing

ACKNOWLEDGEMENTS
This project was funded by the South African Medical
Research Council (SAMRC) through its Division of Research
Capacity Development under the SAMRC Intramural Postdoctoral programme, the South African National Treasury,
as well as an EDCTP2 programme supported by the European Union (TMA2017CDF-1885). We would like to thank
the South African Centre for High Performance Computing
(CHPC) for providing computational resources on their
Lengau cluster for this research, and gratefully acknowledge
the support of Telcom South Africa. We would also like
to thank Jordan Govendar and Rafeeq du Toit for their
invaluable assistance in the Sarcos data collection.
The content and findings reported are the sole deduction,
view and responsibility of the researchers and do not reflect
the official position and sentiments of the SAMRC, EDCTP2,
European Union or the funders.

R EFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

WHO et al., “Summary of probable SARS cases with
onset of illness from 1 November 2002 to 31 July 2003,”
http://www.who.int/csr/sars/country/table2004 04 21/en/index.
html, 2003.
R. Miyata, N. Tanuma, M. Hayashi, T. Imamura,
J.-i. Takanashi, R. Nagata, A. Okumura, H. Kashii,
S. Tomita, S. Kumada et al., “Oxidative stress in patients
with clinically mild encephalitis/encephalopathy with
a reversible splenial lesion (MERS),” Brain and Development, vol. 34, no. 2, pp. 124–127, 2012.
D. Wang, B. Hu, C. Hu, F. Zhu, X. Liu, J. Zhang,
B. Wang, H. Xiang, Z. Cheng, Y. Xiong et al., “Clinical characteristics of 138 hospitalized patients with
2019 novel coronavirus–infected pneumonia in Wuhan,
China,” JAMA, vol. 323, no. 11, pp. 1061–1069, 2020.
A. Carfı̀, R. Bernabei, F. Landi et al., “Persistent symptoms in patients after acute COVID-19,” JAMA, vol.
324, no. 6, pp. 603–605, 2020.
John Hopkins University. (2020, Nov.) COVID-19
Dashboard by the Center for Systems Science
and Engineering (CSSE). John Hopkins University.
[Online]. Available: https://coronavirus.jhu.edu
S. Walvekar, D. Shinde et al., “Detection of COVID19 from CT images using Resnet50,” in 2nd International Conference on Communication & Information Processing (ICCIP) 2020, May 2020, available at SSRN: https://ssrn.com/abstract=3648863 or
http://dx.doi.org/10.2139/ssrn.3648863.

10

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

H. Sotoudeh, M. Tabatabaei, B. Tasorian, K. Tavakol,
E. Sotoudeh, and A. L. Moini, “Artificial Intelligence
Empowers Radiologists to Differentiate Pneumonia Induced by COVID-19 versus Influenza Viruses,” Acta
Informatica Medica, vol. 28, no. 3, p. 190, 2020.
M. Yildirim and A. Cinar, “A Deep Learning Based
Hybrid Approach for COVID-19 Disease Detections,”
Traitement du Signal, vol. 37, no. 3, pp. 461–468, 2020.
A. Chang, G. Redding, and M. Everard, “Chronic
wet cough: protracted bronchitis, chronic suppurative
lung disease and bronchiectasis,” Pediatric Pulmonology,
vol. 43, no. 6, pp. 519–531, 2008.
T. Higenbottam, “Chronic cough and the cough reflex
in common lung diseases,” Pulmonary Pharmacology &
Therapeutics, vol. 15, no. 3, pp. 241–247, 2002.
K. F. Chung and I. D. Pavord, “Prevalence, pathogenesis, and causes of chronic cough,” The Lancet, vol. 371,
no. 9621, pp. 1364–1374, 2008.
J. Korpáš, J. Sadloňová, and M. Vrabec, “Analysis of the
cough sound: an overview,” Pulmonary Pharmacology,
vol. 9, no. 5-6, pp. 261–268, 1996.
J. Knocikova, J. Korpas, M. Vrabec, and M. Javorka,
“Wavelet analysis of voluntary cough sound in patients
with respiratory diseases,” Journal of Physiology and
Pharmacology, vol. 59, no. Suppl 6, pp. 331–40, 2008.
G. Botha, G. Theron, R. Warren, M. Klopper, K. Dheda,
P. Van Helden, and T. Niesler, “Detection of tuberculosis by automatic cough sound analysis,” Physiological
Measurement, vol. 39, no. 4, p. 045005, 2018.
M. Pahar, I. Miranda, A. Diacon, and T. Niesler, “Deep
Neural Network based Cough Detection using Bedmounted Accelerometer Measurements,” in ICASSP
2021 - 2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2021, pp. 8002–
8006.
M. Al-khassaweneh and R. Bani Abdelrahman, “A
signal processing approach for the diagnosis of asthma
from cough sounds,” Journal of Medical Engineering &
Technology, vol. 37, no. 3, pp. 165–171, 2013.
R. X. A. Pramono, S. A. Imtiaz, and E. RodriguezVillegas, “A cough-based algorithm for automatic diagnosis of pertussis,” PloS one, vol. 11, no. 9, p. e0162128,
2016.
A. Windmon, M. Minakshi, P. Bharti, S. Chellappan,
M. Johansson, B. A. Jenkins, and P. R. Athilingam,
“Tussiswatch: A smart-phone system to identify cough
episodes as early symptoms of chronic obstructive
pulmonary disease and congestive heart failure,” IEEE
Journal of Biomedical and Health Informatics, vol. 23, no. 4,
pp. 1566–1573, 2018.
R. V. Sharan, U. R. Abeyratne, V. R. Swarnkar, and
P. Porter, “Automatic croup diagnosis using cough
sound recognition,” IEEE Transactions on Biomedical Engineering, vol. 66, no. 2, pp. 485–495, 2018.
G. Rudraraju, S. Palreddy, B. Mamidgi, N. R. Sripada,
Y. P. Sai, N. K. Vodnala, and S. P. Haranath, “Cough
sound analysis and objective correlation with spirometry and clinical diagnosis,” Informatics in Medicine
Unlocked, p. 100319, 2020.
G. Deshpande and B. Schuller, “An Overview on Audio, Signal, Speech, & Language Processing for COVID-

19,” arXiv preprint arXiv:2005.08579, 2020.
[22] A. N. Belkacem, S. Ouhbi, A. Lakas, E. Benkhelifa, and
C. Chen, “End-to-End AI-based Point-of-Care Diagnosis System for Classifying Respiratory Illnesses and
Early Detection of COVID-19: A Theoretical Framework,” Frontiers in Medicine, vol. 8, p. 372, 2021.
[23] B. W. Schuller, D. M. Schuller, K. Qian, J. Liu, H. Zheng,
and X. Li, “COVID-19 and computer audition: An
overview on what speech & sound analysis could contribute in the SARS-CoV-2 Corona crisis,” arXiv preprint
arXiv:2003.11117, 2020.
[24] C. Brown, J. Chauhan, A. Grammenos, J. Han,
A. Hasthanasombat, D. Spathis, T. Xia, P. Cicuta,
and C. Mascolo, “Exploring Automatic Diagnosis of
COVID-19 from Crowdsourced Respiratory Sound
Data,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,
2020, pp. 3474–3484.
[25] A. Imran, I. Posokhova, H. N. Qureshi, U. Masood,
S. Riaz, K. Ali, C. N. John, and M. Nabeel, “AI4COVID19: AI enabled preliminary diagnosis for COVID-19
from cough samples via an app,” Informatics in Medicine
Unlocked, vol. 20, p. 100378, 2020.
[26] A. Pal and M. Sankarasubbu, “Pay attention to the
cough: Early diagnosis of COVID-19 using interpretable symptoms embeddings with cough sound signal processing,” in Proceedings of the 36th Annual ACM
Symposium on Applied Computing, 2021, pp. 620–628.
[27] P. Bagad, A. Dalmia, J. Doshi, A. Nagrani, P. Bhamare,
A. Mahale, S. Rane, N. Agarwal, and R. Panicker,
“Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds,” arXiv preprint arXiv:2009.08790,
2020.
[28] J. Laguarta, F. Hueto, and B. Subirana, “COVID-19 Artificial Intelligence Diagnosis using only Cough Recordings,” IEEE Open Journal of Engineering in Medicine and
Biology, 2020.
[29] J. Andreu-Perez, H. Pérez-Espinosa, E. Timonet,
M. Kiani, M. I. Giron-Perez, A. B. Benitez-Trinidad,
D. Jarchi, A. Rosales, N. Gkatzoulis, O. F. Reyes-Galaviz
et al., “A Generic Deep Learning Based Cough Analysis
System from Clinically Validated Samples for Point-ofNeed Covid-19 Test and Severity Levels,” IEEE Transactions on Services Computing, no. 01, pp. 1–1, 2021.
[30] M. Cohen-McFarlane, R. Goubran, and F. Knoefel,
“Novel Coronavirus Cough Database: NoCoCoDa,”
IEEE Access, vol. 8, pp. 154 087–154 094, 2020.
[31] N. Sharma, P. Krishnan, R. Kumar, S. Ramoji, S. R.
Chetupalli, P. K. Ghosh, S. Ganapathy et al., “Coswara–
A Database of Breathing, Cough, and Voice Sounds for
COVID-19 Diagnosis,” arXiv preprint arXiv:2005.10548,
2020.
[32] A. Muguli, L. Pinto, N. Sharma, P. Krishnan, P. K.
Ghosh, R. Kumar, S. Ramoji, S. Bhat, S. R. Chetupalli,
S. Ganapathy et al., “Dicova Challenge: Dataset, task,
and baseline system for COVID-19 diagnosis using
acoustics,” arXiv preprint arXiv:2103.09148, 2021.
[33] J. Van Hulse, T. M. Khoshgoftaar, and A. Napolitano,
“Experimental perspectives on learning from imbalanced data,” in Proceedings of the 24th International
Conference on Machine Learning, 2007, pp. 935–942.

11

[34] B. Krawczyk, “Learning from imbalanced data: open
challenges and future directions,” Progress in Artificial
Intelligence, vol. 5, no. 4, pp. 221–232, 2016.
[35] N. V. Chawla, K. W. Bowyer, L. O. Hall, and
W. P. Kegelmeyer, “SMOTE: synthetic minority oversampling technique,” Journal of Artificial Intelligence Research, vol. 16, pp. 321–357, 2002.
[36] G. Lemaı̂tre, F. Nogueira, and C. K. Aridas,
“Imbalanced-learn: A Python toolbox to tackle the
curse of imbalanced datasets in machine learning,” The
Journal of Machine Learning Research, vol. 18, no. 1, pp.
559–563, 2017.
[37] M. Pahar and T. Niesler, “Machine Learning
based COVID-19 Detection from Smartphone Recordings: Cough, Breath and Speech,” arXiv preprint
arXiv:2104.02477, 2021.
[38] L. L. Blagus, R., “SMOTE for high-dimensional classimbalanced data,” BMC Bioinformatics, vol. 14, p. 106,
2013.
[39] H. Han, W.-Y. Wang, and B.-H. Mao, “BorderlineSMOTE: a new over-sampling method in imbalanced
data sets learning,” in International Conference on Intelligent Computing. Springer, 2005, pp. 878–887.
[40] H. M. Nguyen, E. W. Cooper, and K. Kamei, “Borderline over-sampling for imbalanced data classification,”
International Journal of Knowledge Engineering and Soft
Data Paradigms, vol. 3, no. 1, pp. 4–21, 2011.
[41] H. He, Y. Bai, E. A. Garcia, and S. Li, “ADASYN:
Adaptive synthetic sampling approach for imbalanced
learning,” in 2008 IEEE International Joint Conference on
Neural Networks (IEEE World Congress on Computational
Intelligence). IEEE, 2008, pp. 1322–1328.
[42] Wei Han, Cheong-Fat Chan, Chiu-Sing Choy, and
Kong-Pang Pun, “An efficient MFCC extraction
method in speech recognition,” in IEEE International
Symposium on Circuits and Systems, 2006.
[43] M. Pahar and L. S. Smith, “Coding and Decoding
Speech using a Biologically Inspired Coding System,”
in 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE, 2020, pp. 3025–3032.
[44] H. Chatrzarrin, A. Arcelus, R. Goubran, and F. Knoefel,
“Feature extraction for the differentiation of dry and
wet cough sounds,” in IEEE International Symposium on
Medical Measurements and Applications. IEEE, 2011.
[45] M. Pahar, M. Klopper, B. Reeve, G. Theron, R. Warren,
and T. Niesler, “Automatic Cough Classification for
Tuberculosis Screening in a Real-World Environment,”
arXiv preprint arXiv:2103.13300, 2021.
[46] M. M. Azmy, “Feature extraction of heart sounds using
velocity and acceleration of MFCCs based on support
vector machines,” in 2017 IEEE Jordan Conference on
Applied Electrical Engineering and Computing Technologies
(AEECT), 2017, pp. 1–4.
[47] S. Aydın, H. M. Saraoğlu, and S. Kara, “Log energy
entropy-based EEG classification with multilayer neural networks in seizure,” Annals of Biomedical Engineering, vol. 37, no. 12, p. 2626, 2009.
[48] R. Bachu, S. Kopparthi, B. Adapa, and B. D. Barkana,
“Voiced/unvoiced decision for speech signals based
on zero-crossing rate and energy,” in Advanced Techniques in Computing Sciences and Software Engineering.

Springer, 2010, pp. 279–282.
[49] L. T. DeCarlo, “On the meaning and use of kurtosis.”
Psychological Methods, vol. 2, no. 3, p. 292, 1997.
[50] E. Christodoulou, J. Ma, G. S. Collins, E. W. Steyerberg,
J. Y. Verbakel, and B. Van Calster, “A systematic review
shows no performance benefit of machine learning
over logistic regression for clinical prediction models,”
Journal of Clinical Epidemiology, vol. 110, pp. 12–22, 2019.
[51] S. Le Cessie and J. C. Van Houwelingen, “Ridge estimators in logistic regression,” Journal of the Royal Statistical
Society: Series C (Applied Statistics), vol. 41, no. 1, pp.
191–201, 1992.
[52] Y. Tsuruoka, J. Tsujii, and S. Ananiadou, “Stochastic
gradient descent training for l1-regularized log-linear
models with cumulative penalty,” in Proceedings of the
Joint Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Language
Processing of the AFNLP, 2009, pp. 477–485.
[53] H. Yamashita and H. Yabe, “An interior point method
with a primal-dual quadratic barrier penalty function
for nonlinear optimization,” SIAM Journal on Optimization, vol. 14, no. 2, pp. 479–499, 2003.
[54] J. Monge-Álvarez, C. Hoyos-Barceló, P. Lesso, and
P. Casaseca-de-la Higuera, “Robust Detection of AudioCough Events Using Local Hu Moments,” IEEE Journal
of Biomedical and Health Informatics, vol. 23, no. 1, pp.
184–196, 2018.
[55] R. X. A. Pramono, S. A. Imtiaz, and E. RodriguezVillegas, “Automatic cough detection in acoustic signal
using spectral features,” in 2019 41st Annual International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC). IEEE, 2019, pp. 7153–7156.
[56] S. Vhaduri, T. Van Kessel, B. Ko, D. Wood, S. Wang,
and T. Brunschwiler, “Nocturnal cough and snore
detection in noisy environments using smartphonemicrophones,” in 2019 IEEE International Conference on
Healthcare Informatics (ICHI). IEEE, 2019, pp. 1–7.
[57] J.-C. Wang, J.-F. Wang, K. W. He, and C.-S. Hsu,
“Environmental sound classification using hybrid
SVM/KNN classifier and MPEG-7 audio low-level descriptor,” in The 2006 IEEE International Joint Conference
on Neural Network Proceedings. IEEE, 2006, pp. 1731–
1735.
[58] V. Bhateja, A. Taquee, and D. K. Sharma, “PreProcessing and Classification of Cough Sounds in
Noisy Environment using SVM,” in 2019 4th International Conference on Information Systems and Computer
Networks (ISCON). IEEE, 2019, pp. 822–826.
[59] B. H. Tracey, G. Comina, S. Larson, M. Bravard, J. W.
López, and R. H. Gilman, “Cough detection algorithm
for monitoring patient recovery from pulmonary tuberculosis,” in 2011 Annual International Conference of the
IEEE Engineering in Medicine and Biology Society. IEEE,
2011, pp. 6017–6020.
[60] R. V. Sharan, U. R. Abeyratne, V. R. Swarnkar, and
P. Porter, “Cough sound analysis for diagnosing croup
in pediatric patients using biologically inspired features,” in 2017 39th Annual International Conference of
the IEEE Engineering in Medicine and Biology Society
(EMBC). IEEE, 2017, pp. 4578–4581.
[61] H. Taud and J. Mas, “Multilayer perceptron (MLP),” in

12

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

[70]

[71]

[72]

[73]

[74]

[75]

[76]

Geomatic Approaches for Modeling Land Change Scenarios.
Springer, 2018, pp. 451–455.
L. Sarangi, M. N. Mohanty, and S. Pattanayak, “Design
of MLP Based Model for Analysis of Patient Suffering
from Influenza,” Procedia Computer Science, vol. 92, pp.
396–403, 2016.
J.-M. Liu, M. You, Z. Wang, G.-Z. Li, X. Xu, and Z. Qiu,
“Cough detection using deep neural networks,” in
2014 IEEE International Conference on Bioinformatics and
Biomedicine (BIBM). IEEE, 2014, pp. 560–563.
J. Amoh and K. Odame, “DeepCough: A deep convolutional neural network in a wearable cough detection
system,” in 2015 IEEE Biomedical Circuits and Systems
Conference (BioCAS). IEEE, 2015, pp. 1–4.
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural
networks,” Communications of the ACM, vol. 60, no. 6,
pp. 84–90, 2017.
S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back,
“Face recognition: A convolutional neural-network approach,” IEEE Transactions on Neural Networks, vol. 8,
no. 1, pp. 98–113, 1997.
S. Albawi, T. A. Mohammed, and S. Al-Zawi, “Understanding of a convolutional neural network,” in 2017
International Conference on Engineering and Technology
(ICET). IEEE, 2017, pp. 1–6.
X. Qi, T. Wang, and J. Liu, “Comparison of support
vector machine and softmax classifiers in computer
vision,” in 2017 Second International Conference on Mechanical, Control and Computer Engineering (ICMCCE).
IEEE, 2017, pp. 151–155.
S. Hochreiter and J. Schmidhuber, “Long short-term
memory,” Neural Computation, vol. 9, no. 8, pp. 1735–
1780, 1997.
I. D. Miranda, A. H. Diacon, and T. R. Niesler, “A comparative study of features for acoustic cough detection
using deep architectures,” in 2019 41st Annual International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC). IEEE, 2019, pp. 2601–2605.
E. Marchi, F. Vesperini, F. Weninger, F. Eyben, S. Squartini, and B. Schuller, “Non-linear prediction with LSTM
recurrent neural networks for acoustic novelty detection,” in 2015 International Joint Conference on Neural
Networks (IJCNN). IEEE, 2015, pp. 1–7.
J. Amoh and K. Odame, “Deep neural networks for
identifying cough sounds,” IEEE transactions on Biomedical Circuits and Systems, vol. 10, no. 5, pp. 1003–1011,
2016.
A. Sherstinsky, “Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM)
network,” Physica D: Nonlinear Phenomena, vol. 404, p.
132306, 2020.
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual
learning for image recognition,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770–778.
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,
D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft
coco: Common objects in context,” in European Conference on Computer Vision. Springer, 2014, pp. 740–755.
J. Laguarta, F. Hueto, P. Rajasekaran, S. Sarma, and

[77]

[78]

[79]
[80]

B. Subirana, “Longitudinal Speech Biomarkers for Automated Alzheimer’s Detection,” 2020.
S. Liu, “Leave-p-out Cross-Validation Test for Uncertain Verhulst-Pearl Model With Imprecise Observations,” IEEE Access, vol. 7, pp. 131 705–131 709, 2019.
A. Rácz, D. Bajusz, and K. Héberger, “Effect of Dataset
Size and Train/Test Split Ratios in QSAR/QSPR Multiclass Classification,” Molecules, vol. 26, no. 4, p. 1111,
2021.
T. Fawcett, “An introduction to ROC analysis,” Pattern
Recognition Letters, vol. 27, no. 8, pp. 861–874, 2006.
P. A. Devijver and J. Kittler, Pattern recognition: A statistical approach. Prentice Hall, 1982.

Madhurananda Pahar received his BSc in
Mathematics from the University of Calcutta, India, and his MSc in Computing for Financial Markets followed by his PhD in Computational Neuroscience from the University of Stirling, Scotland. Currently, he is working as a post-doctoral
fellow at the University of Stellenbosch, South
Africa. His research interests are in machine
learning and signal processing for audio signals
and smart sensors in bio-medicine such as the
detection and classification of TB and COVID-19
coughs in a real-world environment.

Marisa Klopper is a researcher at the Division
of Molecular Biology and Human Genetics of
Stellenbosch University, South Africa. She holds
a PhD in Molecular Biology from Stellenbosch
University and her research interest is in TB
and drug-resistant TB diagnosis, epidemiology
and physiology. She has been involved in cough
classification for the last 6 years, with application
to TB and more recently COVID-19.

Robin Warren is the Unit Director of the South
African Medical Research Council’s Centre for
Tuberculosis Research and Distinguished Professor at Stellenbosch University. He has a B2
rating by the National Research Council (NRF)
and is a core member of the DSI-NRF Centre
of Excellence for Biomedical Tuberculosis Research and head the TB Genomics research
thrust. He has published over 320 papers in the
field of TB and have an average H-index (Scopus, Web of Science and Google Scholar) of 65.

13

Thomas Niesler obtained the B.Eng (1991) and
M.Eng (1993) degrees in Electronic Engineering
from the University of Stellenbosch, South Africa
and a Ph.D. from the University of Cambridge,
England, in 1998. He joined the Department
of Engineering, University of Cambridge, as a
lecturer in 1998 and subsequently the Department of Electrical and Electronic Engineering,
University of Stellenbosch, in 2000, where he
has been Professor since 2012. His research
interests lie in the areas of signal processing,
pattern recognition and machine learning.

