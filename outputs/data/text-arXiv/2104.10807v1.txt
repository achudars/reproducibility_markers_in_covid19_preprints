arXiv:2104.10807v1 [cs.SI] 22 Apr 2021

COVID-19 and Big Data: Multi-faceted Analysis
for Spatio-temporal Understanding of the Pandemic
with Social Media Conversations
Shayan Fazeli

Davina Zamanzadeh

Anaelia Ovalle

UCLA
Los Angeles, CA
shayan@cs.ucla.edu

UCLA
Los Angeles, CA
davina@cs.ucla.edu

UCLA
Los Angeles, CA
anaeliaovalle@g.ucla.edu

Thu Nguyen

Gilbert Gee

Majid Sarrafzadeh

UCSF
San Francisco, CA
thu.nguyen@ucsf.edu

UCLA
Los Angeles, CA
gilgee@ucla.edu

UCLA
Los Angeles, CA
majid@cs.ucla.edu

Abstract—COVID-19 has been devastating the world since the
end of 2019 and has continued to play a significant role in major
national and worldwide events, and consequently, the news. In
its wake, it has left no life unaffected. Having earned the world’s
attention, social media platforms have served as a vehicle for
the global conversation about COVID-19. In particular, many
people have used these sites in order to express their feelings,
experiences, and observations about the pandemic. We provide
a multi-faceted analysis of critical properties exhibited by these
conversations on social media regarding the novel coronavirus
pandemic. We present a framework for analysis, mining, and
tracking the critical content and characteristics of social media
conversations around the pandemic. Focusing on Twitter and
Reddit, we have gathered a large-scale dataset on COVID19 social media conversations. Our analyses cover tracking
potential reports on virus acquisition, symptoms, conversation
topics, and language complexity measures through time and by
region across the United States. We also present a BERT-based
model for recognizing instances of hateful tweets in COVID-19
conversations, which achieves a lower error-rate than the stateof-the-art performance. Our results provide empirical validation
for the effectiveness of our proposed framework and further
demonstrate that social media data can be efficiently leveraged
to provide public health experts with inexpensive but thorough
insight over the course of an outbreak.
Index Terms—social media, COVID-19, computational linguistic, hate speech, symptoms, machine learning, data science

I. I NTRODUCTION
In the wake of the rapidly spreading pandemic, COVID19 has stormed to the center stage of the world. ICUs and
hospitals have been bombarded with COVID-19 cases in a
short amount of time as healthcare providers and researchers
seek to further understand and mitigate the effects of COVID19. This endeavor requires collecting as much data as possible
surrounding the disease, including significant public health
outcomes. The works on tasks such as predicting or detecting
COVID-19 and prognosis for mortality risk using lab-work,
or electronic health records, have contributed to an arsenal

of tools to help mitigate the effects of the pandemic [1], [2].
Though electronic health records provide a valuable resource
for making inferences regarding the physical effects of the
disease, gaining an in-depth insight into how a pandemic at
this scale impacts lives is not limited solely to health data from
hospitalized patients.
With social media platforms now becoming an inseparable
part of daily life for many, a majority of internet users spend
considerable time sharing content and their points of view
with other members. Given the plethora of chatter occurring,
our goal is to dig into COVID-19 related conversations taking
place across the US and unearth and monitor the key patterns
that can shed light on the focus and orientation of public
opinion regarding the matters related to the pandemic. These
components include the major topics of discussion in the
COVID-19 conversations, self-reports of virus acquisition,
mentions of explicit and implicit symptoms, as well as generation and spread of hate speech content related to the pandemic and conspiracy theories. We concentrated on sampling
and monitoring data from specific geographical regions (US
states), which enables the formation of spatio-temporal signals
corresponding to each region. As the pandemic impacts are not
uniform across all regions (e.g., New York was affected earlier
and with a higher severity at the onset of the pandemic), such
analyses are consequential for effective tracking of pandemic
impacts on each community and ultimately on the broader
society.
In this work, we present a comprehensive tracking and
analytics framework for monitoring pandemic-related conversations across regions, which addresses all of the key
components required for such platforms, as mentioned above.
First, we explored conversation topics that co-occur with
COVID-19 as they provide qualitative insight into subjects
and/or themes that people are concerned about as they relate
to COVID-19. To this end, we perform topic discovery via

Latent Dirichlet Allocation (LDA).
Second, We monitored self-reported symptoms in order to
capture symptoms of those affected by COVID-19 that might
be able to or do not choose to get medical care, as well as to
capture those that may not directly have contracted COVID19 but may know someone who did. In order to capture
symptoms, we are tasked with entity discovery, which we
effectively tackle by using our designed Recursive Transition
Networks (RTNs), such as the one shown in Figure 2. In
addition to monitoring and recognition of such instances
within Tweets through time and across different regions, we
performed a targeted acquisition of posts in Reddit as another
instance of a major social media platform. These data have
helped us present a comprehensive statistical summary on the
mentioned symptoms within online COVID-19 conversations.
Additionally, high-level attributes of language complexity
are essential indicators for a variety of objectives, such as
mental fatigue or long-term health outcomes [3], [4]. Therefore, we attempted to monitor these features across the entire
data on the pandemic-related conversations.
Lastly, we prepared a state of the art machine learning
model as another main contribution which is trained on the
task of recognizing hate speech instances in the pandemic
tweets. COVID-19 does not only appear to have different
health outcomes for different racial/ethnic groups [5] but
also social outcomes. Furthermore, wider impacts such as the
emergence of COVID-19 stigma have been causes for concern.
Derogatory terms such as ”kung flu” only started to appear
in tweets after the coronavirus outbreak. Given the surge
in hateful comments and evolution of terminology, further
monitoring capability of discriminatory language is needed.
It is particularly critical to monitor the spread of hate speech
related to the pandemic in social media across regions and be
able to cross-reference the fluctuations and patterns in it with
various news articles and events. Leveraging geo-located social
media data can facilitate the cross-referencing of COVID-19
conversations with events that are potentially influential. This
objective has numerous critical public health applications as
the spread of stigma and conspiracy theory can impact the
tendency of minority communities to seek medical help when
needed.
Considering that social media platforms are a widely used
outlet where users express their views on different matters
and events, event-centered analysis on the use of language is
incredibly valuable in assessing health and broader impacts of
the pandemic.
A. Related Work
Provided its ubiquity, a multitude of research has focused on
extracting information from social media data to understand
the links between online behavior and health outcomes. For
instance, work has been done regarding the manifestation and
impacts of ”social media addiction” on people’s lives and their
mental health [6]–[10]. Previous literature has also pointed to
how social media can be an invaluable resource for health

and education while also noting that care must be taken in its
respective analyses. [11], [12].
Social media data has allowed for numerous effective data
mining and knowledge extraction methodologies, such as sentiment analysis and recognition of hate speech, cyber-bullying,
and misinformation [13]–[21]. Now, with COVID-19, several
works have incorporated social media data to better understand
the disease. For example, previous work tackles tracking topics
of conversation that coincide with COVID-19 chatter, both on
social media [22] and in academics [23].
Additionally, detecting and tracking symptom reports in
social media is an ongoing research area [24]–[27]. While
focusing on more traditional approaches such as analyzing
word frequencies or binary classification, the aforementioned
works illustrate the effectiveness of using social media data
for symptom discovery. Mental health and social media consumption has also been explored during COVID-19 [28]. The
CORD-19 challenge has also focused on the automatic extraction of entities and information from articles on the COVID19 pandemic and other pandemics with similar characteristics
[29]. Still, further studies are needed with data modalities that
are qualitatively diverse and real-time in order to facilitate
action by public health experts [30]. Moreover, analyses on
symptoms and greater public health impacts through time
and place are a necessary component of monitoring platforms
as experts observe drastic differences in infection rate and
government intervention across various regions [31].
Other datasets surrounding the pandemic on Twitter exist.
Some datasets collect data about COVID-19 in general [32],
while others have more specific focuses. For example, one
dataset is a collection of Tweets containing the words “China”
and “coronavirus” [33]. While there exists other Twitter hate
speech datasets that revolve around COVID-19, they might
focus on other languages such as Arabic [34]. By contrast,
our dataset focuses on geo-tagged English tweets from the
United States that relate to COVID-19. This implies that our
dataset is a subset of pandemic-related tweets that originate
from authors who have enabled geo-tagging.
To the best of our knowledge, no other work in this area
has focused on a cohesive integration of the spatio-temporal
analysis of social media conversations, extraction and tracking
of important entities, and the assessment of sentiment and
various linguistic capacities. Our paper expands on existing
literature and addresses the aforementioned research gaps to
further understand the COVID-19 pandemic and its impacts
on society. To do this, we gathered and analyzed largescale COVID-19 data from popular social media platforms.
We extracted likely mentions of COVID-19 infections and
symptom entities from the raw and unstructured text and shed
light on fluctuations of symptoms in conversation trajectories
through time, data domains, and events.
In particular, our contributions are as follows:
1
• We created COVISION , a large-scale dataset containing
over 3.5 million geo-tagged tweets related to COVID1 Our

code is available at https://github.com/shayanfazeli/covid and bigdata

TABLE I
COVISION S TATISTICS
Number of Tweets
Number of Authors
Earliest Observed Date
Latest Observed Date

3, 506, 405
750, 315
2020-01-01
2020-10-01

Number of Tweets

1M

0.8M

0.6M

data from social media platforms. While there are several
social media datasets presented in the literature, the main focus
so far has been on including a large number of tweets. In this
work, our data acquisition criteria were focused on geo-tagged
tweets to enable the formation and analysis of spatio-temporal
signals in addition to in-depth probing of individual posts.
Given that geo-tagged tweets comprise a considerably small
portion of all the posted tweets, we gathered our data in a longterm span of 9 months, covering the onset of the pandemic
and the first waves. These choices resulted in a corpus of 3.5
million tweets captured from the publicly available tweets in
the date range of January 1st, 2020, until October 1st, 2020.
In summary, each tweet in our corpus:
•

0.4M

0.2M

•
0
Jan 2020

Mar 2020

May 2020

Jul 2020

Sep 2020

Month

Fig. 1. Distribution of COVID-19 tweets per month across the US in the
COVISION dataset

•

•

•

•

19 across nine months. We additionally collected over
20, 000 Reddit posts on COVID-19 experiences and
symptoms to create a diverse source of information on
self-reports of symptoms and physical and mental health.
We utilized our framework for an in-depth analysis of
individual posts from the four key aspects of:
1) Self-reported symptoms and virus acquisition
2) Topic of Conversation related to the COVID-19
(e.g., social distancing)
3) Language characteristics
4) Hate and Stigma-related content
We present a machine learning model based on neural
networks achieving the state of the art results on the
task of recognizing hate speech in COVID-19 data with
a significant increase over the baseline performance.
We created a platform able to form and track aggregate spatio-temporal signals relevant to the corresponding
social media activity and demonstrated the presence of
critical information in the signals by visualizing the
change points cross-referenced with major pandemicrelated events in the news.
We used a wide set of quantitative and qualitative methods and empirically validated the efficacy of our proposed
platform for performing the multi-faceted analysis of
pandemic conversation in social media.
II. M ETHODS

A. Data Collection
The first step towards building an effective monitoring platform is gathering a large-scale corpus on COVID-19 related

•

Contains a reference to the COVID-19 pandemic (e.g.
”covid19”, ”quarantine”, ”chinesevirus”, Please refer to
the appendix for the full list of our search terms.)
Is posted in the United States.
Contains location information (coordinates or other place
attributes) which permits the identification of the US state
in which it was posted.

Figure 1 shows the distribution of tweets per month, and as
expected, the peak appears in March and April of 2020 as the
pandemic problem was starting to worsen across the US. Many
states in the US had not issued stay-at-home order until early
to mid-March, which is when COVID-19 became a jarring
centerpiece of conversation for many people. As everyone
adjusted to life with COVID-19, we expect, and ultimately
observe, that conversation surrounding the pandemic observed
a reduction in the number of geo-tagged tweets in the coming
months.
The number of tweets is not evenly distributed across the
months, which is a reasonable pattern. The number of tweets
is an informative signal given that limiting our data to geotagged corpus leads us to gather a representative subsample of
the overall distribution of conversations. Therefore, we do not
observe the saturation in tweet counts due to API rate-limits.
As suggested in the literature, there are significant differences in people’s approach towards social media based on
the type of platform [35]. To add to the diversity of our
data reflecting on the self-reports, we additionally focused on
the Reddit platform to obtain more data regarding people’s
experiences with COVID-19. Given the level of anonymity
that Reddit provides, one could argue that users find it easier
to share more detailed descriptions of their experiences on
this platform. In order to limit our search scope to COVID19 experiences, we focused on the ”AskReddit” subreddit.
”AskReddit” is a popular sub-community on Reddit dedicated
to threads where a poster poses a question, often personal,
and users participate and answer the question by sharing
their experiences. We gathered 23, 770 responses to questions
about the COVID-19 pandemic experience and symptoms
experienced by users or people close to them. It is worth mentioning that we chose these threads based on the attention they
received from the broader user-base. Also, Reddit primarily
functions as a forum, therefore, posts are not geo-tagged and

conversations are focused on a singular subject. While this
format does not allow us to utilize this data in improving the
spatio-temporal conversation trajectories, it provides a great
source of information for refining our findings on symptoms
and self-reports of experience with COVID-19.
B. Post Probing
1) Self-reports: One of our primary tasks was extracting
mentions of COVID-19 acquisition and symptom entities from
the raw and unstructured text. We designed topic-specific Recursive Transition Networks (RTN) to look for corresponding
manifestations in the text. RTNs are graph structures describing different rules of a context-free grammar. Our grammarbased designs help in a targeted search for instances of terms
that are likely associated with reports of virus acquisition
or experienced symptoms. Figure 2 demonstrates our RTN
instance used for extracting symptoms.
As mentioned before, the social media data in COVISION
are acquired focusing on the conversation around the subject
of pandemic. This relationship means the presence of COVID19 as the main underlying context in interpreting these data
is already established. Therefore, a grammar-based search
is a viable and efficient approach to exploring tweets and
discovering patterns that indicate a high likelihood of an
important entity being mentioned.

approach helps better understand the symptoms and characteristics exhibited by those who contracted COVID-19 and
sheds light on the less noticeable aftermath of the disease and
lengthy quarantines and shutdowns, such as the increase in the
manifestation of depression and quarantine fatigue.
2) Discussed Topics: Given that all of the tweets in the
COVISION dataset are related to the COVID-19 pandemic
as the primary data acquisition criteria, we focused our efforts on discovering the topics of discussion in these tweets.
We explored a range of numbers for the topics present in
this dataset, and used Latent Dirichlet Allocation (LDA) to
find, associate, and interpret these topics. LDA is a popular
technique for topic modeling and models documents as a
combination of k latent topics and topics as a combination of
words (following a unigram language model) [36]. LDA treats
text as an unordered ”bag of words” and assumes that the
document-to-topic vector of probabilities and topic-to-word
vector of probabilities are both sampled from the Dirichlet
distribution.
One of the most common metrics used to evaluate the
performance of LDA is perplexity. Perplexity, defined in
Equation 1, is inversely related to the likelihood of observing
the held-out data.
PT
perplexity = exp(−

<E>
<WORD>

START

<SYMPTOM>

<E>
<WORD>

<E>
<WORD>

<ISSUE>

<E>
<WORD>

<ACTIVITY>

<E>
<WORD>

<ISSUE>

<E>
<WORD>

<BODY>

STOP

Fig. 2. Our Recursive Transition Network for Symptom Search Grammar

TABLE II
T HE GRAMMAR - BASED TAGS AND EXAMPLES .
Tag
h symptom i
h issue i
h activity i
h body i

Description
Medical Symptom
Issues in Activity or Body
Physical Activity
Body Parts

Example
I have a headache
I have pain in my neck
For me it is hard to breathe
I have tremor in my hand

We designed RTN instances for four main categories of
COVID-related entity discovery. The main RTNs shown in
Figure 2 were used to tag tweets for potential mentions
of physical and mental symptoms. Table II contains further
descriptions of each tag we defined in the RTNs and what they
might look like in free text. A separate RTN was designed in
the same fashion to look for possible reports of COVID-19
infection.
The outputs of this methodology provide information on
the nature and quantity of entities reported in tweets that
are potentially associated with COVID-19. This data-driven

ln Pr(w(i) |Θ)
)
PT
i=1 Ni

i=1

(1)

In the above formulation, the held-out data is composed
of T documents. w(i) is the sequence of words for the ith
document, and Θ is the set of model parameters. Also, Ni is
the number of keywords in the ith document. In this case, the
goal of the model is to maximize the probability of observing
held-out data, for which a common practice is to minimize
perplexity. If the learned topics capture the true latent topics
well, the model will generalize and assign a high likelihood
of observing the test data documents, resulting in a lower
perplexity score.
To find the best number of topics with LDA, we monitored the perplexity trend across a varying number of topics,
performed additional qualitative assessments, and chose the
number of topics to consider and interpret accordingly.
3) Readability: Measures related to the complexity of post
language have previously been shown in the literature to be
correlated with a person’s physical and mental well-being
in the long run. We employ several well-known metrics in
assessing every tweet in terms of complexity and readability,
and augment the tweet datasets using these metrics [37]–
[44]. These scores are not sensitive to text order and they
use a count-based approach to determining the score, which
often requires long text data. Therefore we first aggregated
by time and region to reflect on the overall readability of the
conversation, which can be thought of as a large text to which
many authors have contributed. Do note that the language
complexity measures are agnostic to the number of authors
for a piece of text. Please refer to section C of the appendix
for more details on these algorithms.

4) Sentiment: The analysis of the sentiment in public posts
associated with COVID-19-related hate and stigmatization is
another important component of our framework. Given the rise
in the number of hate-related posts across social media platforms since the onset of the pandemic, an effective monitoring
mechanism is required to be able to detect which posts are hate
speech-related, including both hateful and counter-hate tweets.
A tweet that is labeled as counter-hate denounces a hateful
opinion. This helps to shed light on the patterns of hate speech
generation and how it is spread, which is especially helpful in
a pandemic as conspiracy theories and stigma associated with
pandemic-related hate speech can lead to disparities against
minority communities [45]. Using our framework of analysis
allows the experts to optimize the required resource allocation
to alleviate such negative impacts of a pandemic.
Recently, CLAWS, a large dataset on COVID-19 hate
speech on Twitter, was released [46]. This dataset includes
more than 3 million tweet ids along with the corresponding
hard labels of hate, neutral, or counterhate. We focused on
this dataset to train our hate speech modules and evaluate
the performance of our platform in recognizing hate speech
content on Twitter in the midst of conversation about COVID19. This is due to the fact that there is a domain shift in
the hateful content related to the COVID-19 and what was
published in the hate speech datasets before the pandemic.
After crawling the tweet ids, we obtained 2.5 million tweets
with labels, a number that is still considerably larger than the
cardinality of many of the older hate speech datasets (which
contain tens of thousands of tweets).
We propose a deep inference pipeline to process the tweets
and associate a label with them according to the task defined
in [46], marking them as hateful, counter-hate, or neutral.
Our neural network architecture is composed of the standard
BERT transformer architecture initialized with the pre-trained
weights [47], along with a shallow classification layer based
on fully connected neural networks on top of the average
pooled BERT representations. Given that the CLAWS dataset
is relatively limited in size, this allows us to utilize inductive
transfer learning and obtain more efficient semantic representations in terms of generalization and performance. We then
employ gradients to train and fine-tune the network, including
the core language model, on the CLAWS task.
After training and evaluation of our hate speech recognition
module, we followed the protocol in [46] and defined the
CLAWS task as labeling a tweet with one of the three labels:
{neutral, hate, counter-hate}. This was then used as part of our
platform to employ on our gathered data, to provide another
critical signal from the information each post contains, and
to also help with forming the corresponding aggregate spatiotemporal trajectories.
Due to the fact that we trained our model using the CLAWS
dataset, we were careful regarding the change in domain
distribution while applying the model to our gathered data.
We qualitatively assessed the results on our data and did
not observed an explicit domain shift. This is an expected
observation as most of the new instances of hate speech (e.g.,

TABLE III
M OST FREQUENT POTENTIAL SYMPTOMS MENTIONED IN POSTS IN OUR
DATASET

Term (Twitter)
tired
cough
depression
fever
coughing
weak
fatigue
coughed
aches
shortness of breath

Freq.
24.29%
15.24%
14.59%
13.27%
7.47%
7.34%
2.16%
1.76%
1.43%
1.42%

Term (Reddit)
fever
cough
coughing
fatigue
aches
shortness of breath
tired
chills
weak
diarrhea

Freq.
27.67%
23.05%
8.51%
6.60%
6.33%
4.38%
4.30%
2.83%
2.69%
2.63%

derogatory terms such as kung flu) were introduced in the onset
of the pandemic and remained relatively the same throughout
the coming months. In addition, the CLAWS dataset includes a
large number of tweets from the United States and is therefore
expected to cover enough material for our model to effectively
learn the task of hate speech labeling2 .
C. Spatio-temporal Aggregation
Similar to the case of monitoring pandemic outcomes within
subregions (e.g., time-series of the number of cases per
state), aggregating information and forming temporal signals
associated with each region helps better illustrate the regional
variations in the characteristics of the conversation surrounding
the topics related to the pandemic. In our platform, aggregated information on the number of self-reported entities,
hate/stigma-related sentiment, and language complexity were
used to form these temporal signals. We also considered the
major events related to the pandemic from news outlets and
observed that cross-referencing such events can illustrate potential relationships and help experts form and test hypotheses
regarding the pandemic conversations.
III. R ESULTS AND D ISCUSSION
A. Post Probing
1) Self-reports: We designed our RTNs for a grammarbased searching scheme in UniTex GramLab version 3.2 [48].
We then tagged the tweets by employing the designed RTNs
so as to extract the reported symptoms and infections from
the Twitter and Reddit data. Thorough examples are shown in
Figure 5 and Figure 6, where it is shown that even an extensive
list of potential symptom entities within a single post can be
extracted with ease.
Table III shows the top 10 most frequently mentioned
symptoms in the COVID-19 conversation across Twitter and
Reddit posts in our dataset, respectively.
2 It is noteworthy that our data and CLAWS dataset are almost entirely
independent, as less than 0.2% of our tweets can be found in the subset of
the CLAWS dataset used in our work.

New Jersey Nursing Homes Incident

State
NJ
NY

Linsear Write Score

15.5

NJ (smooth)
NY (smooth)

15

14.5

14

13.5
Apr 10
2020

Apr 12

Apr 14

Apr 16

Apr 18

Apr 20

Apr 22

Date
Fig. 3. Daily Average of Linsear Write Score for New York and New Jersey showing a decrease in overall readability

>> tweet (raw): I cant sleep early again, this covid
,→ 19 has got me all messed up! Fever, chills,
,→ headache, tired, cough, shortness of breath,
,→ no sense of smell, nausea, diarrhea, my eye
,→ socket hurts. No wonder some people just give
,→ up or the body just gives up.
>> tweet (tagged): cant sleep early again this covid
,→ has got me all messed up <symptom>fever<
,→ symptom> <symptom>chills<symptom> headache <
,→ symptom>tired<symptom> <symptom>cough<symptom
,→ > <symptom>shortness of breath<symptom> no
,→ sense of smell <symptom>nausea<symptom> <
,→ symptom>diarrhea<symptom> my eye socket hurts
,→ no wonder some people just give up or the
,→ body just gives up.
<symptom>: fever::chills::tired::cough::shortness of
,→ breath::nausea::diarrhea
<covid_report>: none
<impact_body>: none
<impact_activity>: none

Fig. 5. Example tagged tweet using RTN

>> reddit comment (tagged): My fiancee is an ICU
,→ nurse, so of course we both <covid_report>got
,→ it<covid_report> (and our toddler <
,→ covid_report>got it<covid_report>).It sucked.
,→ Mostly for me; toddler got through it with
,→ barely any effects, my fiancee got through it
,→ with being mildly ill and <symptom>tired<
,→ symptom> for a week or so...I thought I had a
,→ pretty mild case until suddenly I got winded
,→ by walking ˜10 meters. I was just barely not
,→ sick enough to get admitted to the hospital
,→ for it. Apart <covid_report>from that<
,→ covid_report>, the worst thing for me was the
,→ <symptom>fatigue<symptom>.I had symptoms for
,→ around 2 weeks, and the middle-to-last part
,→ I was just so damn <symptom>tired<symptom>
,→ all the time. A few days, I slept around
,→ 15-16 hours out of the day. We lost our sense
,→ of taste and smell, though they are *mostly*
,→ back; taste is completely back, but
,→ sometimes it is <impact_activity>hard to
,→ smell<impact_activity> the scent of things.
<symptom>: tired::fatigue::tired
<covid_report>: got it::got it::from that
<impact_body>: none
<impact_activity>: hard to smell

Fig. 6. Example tagged Reddit post using RTN

2) Discussed Topics: We trained our model for assigning
topics to each post by employing a full-batch LDA algorithm,
considering unigrams, bigrams, trigrams and omitting words
that appear fewer than 50 times in the dataset. We also
lemmatize the words as an essential preprocessing step to
help improve topic discovery. The results of the quantitative
measurements of performance as perplexity on the hold-out
set, in addition to our qualitative assessments, indicate that the

CLAWS Task: Counter-hate Probability

BLM Protests Begin

0.02

state
CA
NY
CA_smooth
NY_smooth

0.015

0.01

0.005

0
Apr 26
2020

May 3

May 10

May 17

May 24

May 31

Jun 7

Jun 14

Jun 21

Jun 28

Date
Fig. 4. Daily Average of CLAWS Task: Counter-hate Probability around event: BLM Protests Begin - Half Window: 30 days

TABLE IV
T OPICS FROM LDA T OPIC M ODELING USING R EDDIT DATA FROM
COVISION

0.05

Daily Average

Number of Symptoms in Tweet

Daily Average (smooth)

Topic Name
Acquisition
Support
Cost and Response

Terms
symptom, cough, fever, smell, taste
family, home, friend
healthcare, hospital, insurance, cost, bill

0.04

TABLE V
P ERPLEXITIES ON A HOLD - OUT VALIDATION SET FOR OUR R EDDIT

0.03

CORPUS

Number of Topics
2
3
4
5

0.02

0.01

Perplexity
1883.65
646.680
2021.64
2231.07

0
Ja

Ma
Ma
Ju
Se
n2
l2
p2
r2
y2
02
02
02
02
02
0
0
0
0
0

Date

Fig. 7. Average number of symptom entities in tweets through time

model with around 20 topics is best to fit our Twitter corpus.
The term probabilities per topic are displayed in Figure 10
for our 22 topic instance. The qualitative evaluation of the
topics and posts led us to consider the topic labels mentioned
in Figure 10 as the prominent conversation centroids for the
tweets in our dataset.
We performed the topic modeling analysis with LDA on our
Reddit corpus as well. The perplexity values computed on a
hold-out set are shown in Table V. We qualitatively assessed

the discovered topics, and the version with 3 topics appeared
to be the best fit for this data. Table IV outlines some topics
discovered and their respective terms.
It is worth noting that the hold-out set for both Twitter
and Reddit corpora in COVISION was done by shuffling and
splitting author ids, ensuring that each set is composed of
different users creating the posts.
3) Readability: Following the literature regarding monitoring language complexity as an indicator for the long-term
health-related issues, we assessed the COVISION data on
several language complexity measures and a text readability
measure to understand temporal patterns that crop up.
As visible in Table VI, there is a general increasing pattern
in the language complexity indicative of a slight decrease
in overall tweet readability at the onset of the COVID-19

Number of Potential Coronavirus Reports in Tweet

200

Announcing First COVID-19 Case

First American Death due to COVID-19

state
CA
NY
WA

150

CA_smooth
NY_smooth
WA_smooth

100

50

0

Jan 12
2020

Jan 19

Jan 26

Feb 2

Feb 9

Feb 16

Feb 23

Mar 1

Mar 8

Date

Fig. 8. Potential COVID-19 acquisition reports in major states at the onset of the pandemic

hold-out set
Minimum

7200

Perplexity

7000
6800
6600
6400
6200
6000
10

20

30

40

Number of Topics

our results on a single large hold-out set to measure the
performance.
Utilizing this trained network, we created per-tweet assessments and aggregated them to create spatio-temporal trajectories marked with events. Figure 4 shows the pattern of the
daily average value of counter-hate probability in the two states
of California and New York. The counter-hate probability is
the probability of a tweet denouncing a hateful opinion. An
example of a tweet classified as counter-hate is shown in
Figure 11. The inference on this dataset is made by employing
the described hate assessment model which we have trained
on CLAWS task. The plots in Figure 4 indicate an increase
in the average daily counter-hate probability score per tweet
during the span of 30 days before to 30 days after the start of
Black Lives Matter protests.

Fig. 9. Perplexity vs Number of Topics in our Twitter corpus - Full Batch
LDA Topic Modeling

pandemic in the US. While not a significant overall increase,
this difference can be associated with concepts such as public
anxiety or quarantine fatigue. The upward trajectories of
Linsear Write in Figure 3 is another example showing a
noticeable increasing pattern in the language complexity in
New York. Such fluctuations are important to keep track of as
they could be potentially correlated with health outcomes.

tweet (raw): We need to put negative comments about
,→ coronavirus aside you must realize it is not
,→ about who faults. We really don’t know who
,→ faults it is. Stop attacked Asians because
,→ you can not tell difference between one over
,→ another. We need to find cure and help each
,→ other. Stop racist
hate distribution: (neutral: 0.0002, hate: 0.0001,
,→ counter-hate: 0.9997, other: 0.0001)
Fig. 11. Example tweet classified as counter-hate

B. Hate and Stigma
We trained the proposed model for hate recognition on the
CLAWS dataset. The implementation was done in PyTorch
1.7, and the optimization was performed with the Adam
algorithm for 50 epochs. The results are shown in Table VII
and indicate the effectiveness of this network in monitoring
hatefulness in tweets, improving upon the classifiers proposed
for this task. Given the large size of the dataset, which included
over 2.5 million tweets, and the complexity of the proposed
model, instead of following a K-fold validation protocol, as
done in [46] for much simpler architectures, we evaluated

C. Spatio-temporal Trajectories
To better understand the dynamics of the conversations surrounding COVID-19, we aggregate our computed inferences
on tweets per state and per day. This led to a series of
spatio-temporal trajectories with a daily resolution. Our results
indicate that for various groups of regions, these data can help
further illustrate the patterns and directions of conversations
on the COVID-19 during the course of the outbreak.
We focused on geo-tagged tweets, meaning that the metadata information regarding the US state from which every
tweet was authored is present in the dataset. We normalized

Pandemic and Flu

0.04

term

Work from Home

0.06

0.04

term

term

Family and Friends

covid
family
day
life
know
just
bad
friend
feel
today
0.00

0.02
prob

0.04

pandemic
global
global pandemic
time
middle
just
know
world
covid pandemic
need
0.00

term

National Affairs

0.02 0.04
prob

term
0.10

test
covid
positive
test positive
covid test
positive covid
month
test positive covid
player
challenge
0.000 0.025 0.050 0.075
prob

0.02

Virus Origin

0.05
prob

0.10

Symptoms

0.02
0.04
prob

trump
covid
american
president
coronavirus
pandemic
response
lie
vote
country
0.00

0.05 0.10
prob

Donald Trump

0.02
prob

0.04

Waves and Vaccine

quarantine
year
self
cancel
game
play
come
check
season
self quarantine
0.00 0.01 0.02 0.03
prob

Testing and Positivity Rates

term

News

0.05
prob

0.10

Games

school
covid
kid
foot
student
spread
right
close
child
parent
0.00 0.01 0.02 0.03
prob

covid
thing
good
look
update
covid coronavirus
coronavirus
coronavirus covid
news
free
0.00

term

term

term

Schools

new
case
coronavirus
covid
state
county
death
york
new york
report
0.00

0.05
prob

0.01
prob

Quarantine and Time

term

0.02
prob

covid
hear
coronavirus
risk
say
catch
symptom
spread
cure
disease
0.00

quarantine
day
time
make
just
day quarantine
watch
good
start
week
0.00

term

term

term

Social Distancing

social
distancing
social distancing
distance
social distance
practice
practice social
park
practice social distancing
walk
0.00

covid
think
coronavirus
video
vaccine
fight
force
stop
write
wave
0.00

0.02 0.04
prob

0.06

Plans

term

0.02 0.04
prob

work
make
pay
home
job
sure
money
need
covid
help
0.000 0.025 0.050 0.075
prob

quarantine
old
use
hit
eat
drink
restaurant
open
dog
beach
0.00

virus
corona
corona virus
china
chinese
wuhan
chinese virus
world
say
news
0.00

term

Mask

Pandemic Fatigue

really
rona
fuck
shit
want
just
tell
miss
know
covid
0.00

0.10

stayhome
covid
thank
safe
stay
time
home
great
love
happy
0.00 0.01 0.02 0.03
prob

term

mask
wear
covid
wear mask
face
support
pandemic
relief
business
pm
0.00

0.05
prob

Stay at Home and Safety

stay
home
stay home
hand
order
wash
wash hand
home order
stay home order
store
0.00 0.02 0.04 0.06
prob

term

term

Stay at Home Orders

Quarantine and Activities

term

0.02
prob

people
covid
die
death
flu
kill
protest
sick
black
die covid
0.00

term

Healthcare
term

term

covid
health
coronavirus
patient
care
public
hospital
number
state
medical
0.00

coronavirus
just
say
time
week
plan
news
outbreak
coronavirus outbreak
long
0.00 0.05 0.10 0.15
prob

Global Affairs

0.05 0.10
prob

Fig. 10. An overview of the main topics of the COVID-19 conversation around the US during the first 9 months of the year 2020.

TABLE VI
M EASURES OF R EADABILITY - M ONTHLY AVERAGE S CORES ACROSS THE U NITED S TATES
Month
January
February
March
April
May
June
July
August
September

Coleman-Liau Index
10.95
11.60
11.52
11.74
11.58
10.97
10.80
10.79
10.67

Flesch-Kincaid
10.83
11.99
11.21
11.49
11.91
12.01
11.95
11.97
11.86

Dale-Chall
9.11
9.33
9.10
9.16
9.16
9.05
9.00
9.01
9.02

Gunning-Fog
13.74
15.02
14.32
14.70
15.14
15.14
15.07
15.13
14.99

TABLE VII
M ACRO - AVERAGED P ERFORMANCE M ETRICS FOR OUR MODEL ON THE
THREE CLAWS TASK AND THE AUROC SCORE REPORTED IN [46] FOR
BERT- BASED CLASSIFICATION .

•

Our Model
Precision
91.92
Recall
94.83
F1-Score
93.32
AUROC
99.82
CLAWS baseline (AUROC)
Hate
86.4
Counter-hate
83.3
Neutral
82.3

•

our metrics by the overall output for each region so that we
could reliably compare regions since larger or more populated
states such as California have a larger volume of tweets than
a smaller or less populated state such as Maine. Our results
indicate that following the trends around major news events
can be very informative. Figures similar to Figure 4 and
3 can be constructed using our platform, providing public
health experts with an overview of critical pandemic-related
temporal patterns. More results on spatio-temporal trajectories
are available in the appendix.3
In addition, we can extract informative auxiliary information
by computing descriptive statistics such as count on our
aggregate measurements across time and space. For example,
Figure 7 shows the fluctuations in the average number of
phrases likely to be describing symptoms present in the tweets
across the US.
The fact that the curve flattens and plateaus after the first
few months is consistent with the intuitive idea that the
speculations about the specific characteristics of COVID-19
were more intensive when it was initially discovered. As
expected, the peaks fall in the first few months as people focus
on the common symptoms expected to be experienced due to
COVID-19 during the early stages of the pandemic.
D. Limitations
Given the sensitive nature of research in this area, it is
critical to have an in-depth discussion of each approach’s
limitations:
• There are potential gaps in the data collected from
the Twitter API due to privacy measures from Twitter,
3 The

full table of spatio-temporal features will be available in our codebase.

•

•

SMOG
13.18
11.79
12.40
13.24
12.34
13.19
13.13
13.71
12.40

SPACHE
6.76
7.14
6.82
6.98
7.17
7.30
7.30
7.32
7.32

Linsear-Write
12.67
14.31
13.49
14.09
15.00
15.56
15.57
15.65
15.53

ARI
10.99
12.48
11.96
12.55
13.11
13.27
13.19
13.25
13.11

network errors, and/or API errors. The same goes for the
Reddit data.
It is possible for the aforementioned RTN-based methodology of searching for COVID-19 entities to incorrectly
tag phrases due to structural ambiguity, for example:
”covid-19 is really damaging. it is genuinely hard to see”
leads to the substring ”hard to see” being returned as
an instance of impact on activity. While our approach
tremendously reduces the need for human supervision, a
minimal qualitative assessment is still required.
Designing LDA-based topic modeling frameworks, as
well-recognized as they are, has known limitations. It
should be noted that the quantitative measures of the topic
fitness, such as perplexity, are not always in accordance
with human interpretation [49].
We utilized different measures of language complexity as
a relative assessment of readability. The absolute values
for these methodologies and interpretations rely heavily
on the nature of the text they process. For example, it is
likely for a tweet not to have proper punctuation. While
these limitations may lead to translations in the baseline,
the results of relative assessments are consistent.
In analyzing and comparing the spatio-temporal curves,
one should be mindful of the difference in the number
of samples used in computing each aggregate datapoint,
even when normalizing the data. Though we normalize
the aggregates by the volume of tweets per region and
time-point, a state with a much higher volume of tweets
is more likely to capture a representative set of tweets
than a state with a much lower volume of tweets. For
instance, the number of tweets made in California in the
COVISION dataset is 15 times more than the number of
tweets made in Kentucky. In comparing different regions,
therefore, it is better to choose places for which the
aggregate values are computed with a similar number of
samples. This is why, in presenting some of our critical
results, we focused on states with higher populations,
such as New York and California, from which a larger
number of tweets exist in our corpus.
IV. C ONCLUSION

This study methodically gathered a comprehensive dataset
on social media conversations around the COVID-19 pandemic. We focused on Reddit for acquiring comments that

share experiences with COVID-19. Additionally, we collected
geo-tagged tweets from across the United States that cover
a broad time period starting at the beginning of the pandemic. We proposed efficient search and analysis methodologies based on text structure and machine learning to better
understand and assess the spatio-temporal trajectories of social media conversations on the novel coronavirus pandemic
and empirically validated the performance of the proposed
approach. The framework of analysis we propose offers an
automated pipeline that produces informative insights that
requires minimal manual input. This work demonstrates the
effectiveness of analyzing social media data in order to extract
critical information and knowledge with respect to public
health interests during the COVID-19 pandemic.
R EFERENCES
[1] A. Vaid, S. K. Jaladanki, J. Xu, S. Teng, A. Kumar, S. Lee, S. Somani,
I. Paranjpe, J. K. De Freitas, T. Wanyan, K. W. Johnson, M. Bicak,
E. Klang, Y. J. Kwon, A. Costa, S. Zhao, R. Miotto, A. W. Charney,
E. Böttinger, Z. A. Fayad, G. N. Nadkarni, F. Wang, and B. S.
Glicksberg, “Federated Learning of Electronic Health Records Improves
Mortality Prediction in Patients Hospitalized with COVID-19.” [Online].
Available: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7430624/
[2] L. Wynants, B. Van Calster, G. S. Collins, R. D. Riley, G. Heinze,
E. Schuit, M. M. J. Bonten, D. L. Dahly, J. A. A. Damen, T. P. A.
Debray, V. M. T. de Jong, M. De Vos, P. Dhiman, M. C. Haller,
M. O. Harhay, L. Henckaerts, P. Heus, N. Kreuzberger, A. Lohmann,
K. Luijken, J. Ma, G. P. Martin, C. L. Andaur Navarro, J. B. Reitsma,
J. C. Sergeant, C. Shi, N. Skoetz, L. J. M. Smits, K. I. E. Snell,
M. Sperrin, R. Spijker, E. W. Steyerberg, T. Takada, I. Tzoulaki,
S. M. J. van Kuijk, F. S. van Royen, J. Y. Verbakel, C. Wallisch,
J. Wilkinson, R. Wolff, L. Hooft, K. G. M. Moons, and M. van
Smeden, “Prediction models for diagnosis and prognosis of covid-19:
Systematic review and critical appraisal,” vol. 369. [Online]. Available:
https://www.bmj.com/content/369/bmj.m1328
[3] D. A. Snowdon, S. J. Kemper, J. A. Mortimer, L. H. Greiner, D. R.
Wekstein, and W. R. Markesbery, “Linguistic ability in early life and
cognitive function and alzheimer’s disease in late life: Findings from
the nun study,” Jama, vol. 275, no. 7, pp. 528–532, 1996.
[4] D. D. Danner, D. A. Snowdon, and W. V. Friesen, “Positive emotions
in early life and longevity: findings from the nun study.” Journal of
personality and social psychology, vol. 80, no. 5, p. 804, 2001.
[5] S. Sze, D. Pan, C. R. Nevill, L. J. Gray, C. A. Martin,
J. Nazareth, J. S. Minhas, P. Divall, K. Khunti, K. R. Abrams,
L. B. Nellums, and M. Pareek, “Ethnicity and clinical outcomes
in COVID-19: A systematic review and meta-analysis,” vol. 29.
[Online]. Available: https://www.thelancet.com/journals/eclinm/article/
PIIS2589-5370(20)30374-6/abstract
[6] A. C. Nakaya, “Internet and social media addiction,” Webology, vol. 12,
no. 2, pp. 1–3, 2015.
[7] N. S. Hawi and M. Samaha, “The relations among social media
addiction, self-esteem, and life satisfaction in university students,” Social
Science Computer Review, vol. 35, no. 5, pp. 576–586, 2017.
[8] C. Liu and J. Ma, “Social media addiction and burnout: The mediating
roles of envy and social media use anxiety,” Current Psychology, pp.
1–9, 2018.
[9] L.-Y. Leong, T.-S. Hew, K.-B. Ooi, V.-H. Lee, and J.-J. Hew, “A hybrid
sem-neural network analysis of social media addiction,” Expert Systems
with Applications, vol. 133, pp. 296–316, 2019.
[10] N. Esgi, “Development of social media addiction test (smat17),” Journal
of education and training studies, vol. 4, no. 10, pp. 174–181, 2016.
[11] W. Whyte and C. Hennessy, “Social media use within medical education:
A systematic review to develop a pilot questionnaire on how social media
can be best used at bsms,” MedEdPublish, vol. 6, 2017.
[12] T. M. Duymus, H. Karadeniz, M. A. Çaçan, B. Kömür, A. Demirtaş,
S. Zehir, and İ. Azboy, “Internet and social media usage of orthopaedic
patients: A questionnaire-based survey,” World journal of orthopedics,
vol. 8, no. 2, p. 178, 2017.

[13] K. Florio, V. Basile, M. Polignano, P. Basile, and V. Patti, “Time of your
hate: The challenge of time in hate speech detection on social media,”
Applied Sciences, vol. 10, no. 12, p. 4180, 2020.
[14] M. Mondal, L. A. Silva, and F. Benevenuto, “A measurement study of
hate speech in social media,” in Proceedings of the 28th acm conference
on hypertext and social media, 2017, pp. 85–94.
[15] S. Malmasi and M. Zampieri, “Detecting hate speech in social media,”
arXiv preprint arXiv:1712.06427, 2017.
[16] N. Makrynioti, A. Grivas, C. Sardianos, N. Tsirakis, I. Varlamis,
V. Vassalos, V. Poulopoulos, and P. Tsantilas, “Palopro: a platform for
knowledge extraction from big social data and the news,” International
Journal of Big Data Intelligence, vol. 4, no. 1, pp. 3–22, 2017.
[17] Y. Ma, L. Liao, and T.-S. Chua, “Automatic fashion knowledge extraction from social media,” in Proceedings of the 27th ACM International
Conference on Multimedia, 2019, pp. 2223–2224.
[18] L. Bode and E. K. Vraga, “In related news, that was wrong: The
correction of misinformation through related stories functionality in
social media,” Journal of Communication, vol. 65, no. 4, pp. 619–638,
2015.
[19] L. Wu, F. Morstatter, X. Hu, and H. Liu, “Mining misinformation in
social media,” Big Data in Complex and Social Networks, pp. 123–152,
2016.
[20] H. Allcott, M. Gentzkow, and C. Yu, “Trends in the diffusion of
misinformation on social media,” Research & Politics, vol. 6, no. 2,
p. 2053168019848554, 2019.
[21] D. Trottier and D. Lyon, “Key features of social media surveillance,”
Internet and Surveillance: the challenges of Web 2.0 and social media,
vol. 16, pp. 89–105, 2012.
[22] C. Ordun, S. Purushotham, and E. Raff, “Exploratory analysis of covid19 tweets using topic modeling, umap, and digraphs,” 2020.
[23] A. Älgå, O. Eriksson, and M. Nordberg, “Analysis of Scientific
Publications During the Early Phase of the COVID-19 Pandemic:
Topic Modeling Study,” vol. 22, no. 11. [Online]. Available:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7674137/
[24] S. C. Guntuku, G. Sherman, D. C. Stokes, A. K. Agarwal, E. Seltzer,
R. M. Merchant, and L. H. Ungar, “Tracking mental health and symptom
mentions on twitter during covid-19,” Journal of general internal
medicine, vol. 35, no. 9, pp. 2798–2800, 2020.
[25] M. A. Al-Garadi, Y.-C. Yang, S. Lakamana, and A. Sarker, “A text
classification approach for the automatic detection of twitter posts
containing self-reported covid-19 symptoms,” 2020.
[26] A. Sarker, S. Lakamana, W. Hogg-Bremer, A. Xie, M. A. Al-Garadi, and
Y.-C. Yang, “Self-reported covid-19 symptoms on twitter: An analysis
and a research resource,” medRxiv, 2020.
[27] S. A. Marshall, C. C. Yang, Q. Ping, M. Zhao, N. E. Avis, and E. H.
Ip, “Symptom clusters in women with breast cancer: an analysis of
data from social media and a research study,” Quality of Life Research,
vol. 25, no. 3, pp. 547–557, 2016.
[28] J. Gao, P. Zheng, Y. Jia, H. Chen, Y. Mao, S. Chen, Y. Wang, H. Fu,
and J. Dai, “Mental health problems and social media exposure during
covid-19 outbreak,” Plos one, vol. 15, no. 4, p. e0231924, 2020.
[29] L. L. Wang, K. Lo, Y. Chandrasekhar, R. Reas, J. Yang, D. Eide,
K. Funk, R. Kinney, Z. Liu, W. Merrill et al., “Cord-19: The covid19 open research dataset,” ArXiv, 2020.
[30] R. R. German, J. M. Horan, L. M. Lee, B. Milstein, and C. A. Pertowski,
“Updated guidelines for evaluating public health surveillance systems;
recommendations from the guidelines working group,” 2001.
[31] D. Dave, A. I. Friedson, K. Matsuzawa, and J. J. Sabia, “When do
shelter-in-place orders fight covid-19 best? policy heterogeneity across
states and adoption time,” Economic Inquiry, vol. 59, no. 1, pp. 29–52,
2021.
[32] E. Chen, K. Lerman, and E. Ferrara, “Tracking social media discourse
about the covid-19 pandemic: Development of a public coronavirus
twitter data set,” JMIR Public Health Surveill, vol. 6, no. 2, p. e19273,
May 2020. [Online]. Available: http://www.ncbi.nlm.nih.gov/pubmed/
32427106
[33] L. Fan, H. Yu, and Z. Yin, “Stigmatization in social media: Documenting
and analyzing hate speech for covid-19 on twitter,” Proceedings of the
Association for Information Science and Technology, vol. 57, no. 1, p.
e313, 2020. [Online]. Available: https://asistdl.onlinelibrary.wiley.com/
doi/abs/10.1002/pra2.313
[34] R. Alshalan, H. Al-Khalifa, D. Alsaeed, H. Al-Baity, and S. Alshalan,
“Detection of hate speech in covid-19–related tweets in the arab
region: Deep learning and topic modeling approach,” J Med Internet

[35]

[36]
[37]
[38]
[39]
[40]

[41]
[42]
[43]
[44]
[45]

[46]
[47]
[48]
[49]
[50]
[51]

Res, vol. 22, no. 12, p. e22609, Dec 2020. [Online]. Available:
http://www.ncbi.nlm.nih.gov/pubmed/33207310
C. J. Kennedy, G. Bacon, A. Sahn, and C. von Vacano, “Constructing
interval variables via faceted rasch measurement and multitask deep
learning: a hate speech application,” arXiv preprint arXiv:2009.10277,
2020.
D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,”
vol. 3, pp. 993–1022.
G. H. Mc Laughlin, “Smog grading-a new readability formula,” Journal
of reading, vol. 12, no. 8, pp. 639–646, 1969.
E. A. Smith and R. Senter, “Automated readability index.” AMRL-TR.
Aerospace Medical Research Laboratories (US), p. 1, 1967.
E. Dale and J. S. Chall, “A formula for predicting readability: Instructions,” Educational research bulletin, pp. 37–54, 1948.
J. P. Kincaid, R. P. Fishburne Jr, R. L. Rogers, and B. S. Chissom,
“Derivation of new readability formulas (automated readability index,
fog count and flesch reading ease formula) for navy enlisted personnel,”
Naval Technical Training Command Millington TN Research Branch,
Tech. Rep., 1975.
M. Coleman and T. L. Liau, “A computer readability formula designed
for machine scoring.” Journal of Applied Psychology, vol. 60, no. 2, p.
283, 1975.
“Arkansas bar association — what’s new,” https://web.archive.org/web/
20080111064508/http://www.arkbar.com/Ark Lawyer Mag/Articles/
JURYFall02.html, accessed: 2020-12-26.
J. O’hayre, Gobbledygook has gotta go. US Department of the Interior,
Bureau of Land Management, 1966.
G. Spache, “A new readability formula for primary-grade reading
materials,” The Elementary School Journal, vol. 53, no. 7, pp. 410–413,
1953. [Online]. Available: http://www.jstor.org/stable/998915
H. Budhwani and R. Sun, “Creating covid-19 stigma by referencing the
novel coronavirus as the “chinese virus” on twitter: quantitative analysis
of social media data,” Journal of Medical Internet Research, vol. 22,
no. 5, p. e19301, 2020.
C. Ziems, B. He, S. Soni, and S. Kumar, “Racism is a virus: Anti-asian
hate and counterhatein social media during the covid-19 crisis,” arXiv
preprint arXiv:2005.12423, 2020.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.
“Unitex/gramlab,” https://unitexgramlab.org/, accessed: 2020-12-26.
J. Chang, S. Gerrish, C. Wang, J. Boyd-Graber, and D. Blei, “Reading
tea leaves: How humans interpret topic models,” Advances in neural
information processing systems, vol. 22, pp. 288–296, 2009.
P. Ley and T. Florio, “The use of readability formulas in health care,”
Psychology, Health & Medicine, vol. 1, no. 1, pp. 7–28, 1996.
A. S. Hedman, “Using the smog formula to revise a health-related
document,” American Journal of Health Education, vol. 39, no. 1, pp.
61–64, 2008.

A PPENDIX
A. Data Acquisition
The search terms used in our data acquisition scheme are
shown in Table VIII.
TABLE VIII
Q UERY TERMS USED IN OUR DATA ACQUISITION SCHEME

corona.*virus
ncov
covid†
sars.*cov
coronaalert
corona.*outbreak
kung.*flu
wuhan
coronavirusapocalypse
pandemic
epidemic
quarantine
rona
commie cough
wu-hanic plague
mad-cau disease
chinese virus
chinesevirus
chingchongvirus
kungflufighting

Query Terms
kungfuflu
chingchongprague
commiecough
wuflu
miss rona
ms. rona
coronaviruschallenge
coro
coro coro
cororo
miss coco v
la rona
miss corona v
corov
corov19
corov-19
corovid19
corovid-19
cocov
coronuh

coronatoiletpaper
socialdistancing
social distancing
herdimmunity
quarentine
quarantine
quarantinelife
quarentinelife
coronacurfew
chinadisease
wuhanpneumonia
pandumbic
chinacorona
washhands
wash your hands
stayhome
stay at home
6 ft
6 feet
#rona

B. Spatio-temporal Trajectories around Events
In this section of the appendix, several other potential
indicators of significant relationships are shown. Figure 12
indicates an increase in the average probability of a tweet in
California and New York to exhibit COVID-related hateful
sentiment. The increase could be associated with the considerable events in that time-span, such as the worsening of
COVID-19 across US. A similar increase is shown in the onset
of the pandemic as well, as visible in Figure 13.

Algorithm 1 Linsear Write Readability Metric
1: procedure L INSEAR –W RITE (Text)
2:
output ← 0
3:
sentCount ← 0
4:
for each sentence s ∈ Text do
5:
sentCount ← sentCount + 1
6:
for each word ω ∈ s do
7:
if getSyllablesCount(ω) <= 2 then
8:
output ← output +1
9:
else
10:
output ← output +3
11:
end if
12:
end for
13:
end for
14:
if output > 20 then
15:
output ← output /2
16:
else
17:
output ← output /2 − 1
18:
end if
19:
return output
20: end procedure

Number of Letters
Number of Words
Number of Sentences
− 15.8 (3)
− 29.6 ×
Number of Words
4) SMOG Grade: The Simple Measure of Gobbledygook
(SMOG) Grade was developed as a readability test that approximates the years of education necessary to comprehend
an input text. This readability test is widely used in the health
domain as well [50], [51]. The formula for this test is as
follows:
CLI = 5.88 ×

C. Language Complexity Metrics
1) Linsear Write Metric: Developed for the United States
Air Force, the linsear write readability metric was used to
provide statistics on the readability of technical manuals. This
test depends mainly on sentence length, and word syllables
[42]. This algorithm’s main intuition is that shorter sentences
and shorter words are believed to be easier to read [43].
The algorithm for a 100 word sample is as follows:
2) Dale-Chall Readability Test: This numeric readability
metric provides a measure on the difficulty and comprehensibility of the given text [39]. Given a list of difficult words,
the Dale-Chall formula is as follows:

5) Gunning Fog Index: The Gunning Fog Index is a readbility test to measure the number of years of formal education
needed to understand the input text on the first read. This
formula defines complex words as words with more than three
syllables, and the output score is computed as follows:

Number of Words
Number of Sentences
Number of Difficult Words
+ 0.1579 ×
(2)
Number of Words
3) Coleman–Liau Readability Test: The Coleman-Liau test
is a language complexity metric that approximates the US
grade level necessary to comprehend the text. The formula
for this test is as follows:

Number of Words
Number of Sentences
Number of Complex Words
+ 100 ×
) (5)
Number of Words
6) Flesch-Kincaid Readability Tests: Several instances of
the aforementioned readability tests relied on the ratio of
syllables to describe the text complexity. The Flesch-Kincaid

DC = 0.0496 ×

SMOG = 3.1291
+ 1.0430×
s
Number of Polysyllables ×

GFog = 0.4 × (

30
(4)
Number of Sentences

CLAWS Task: Hate Probability

Coronavirus Worsens across US

0.04

state
CA
NY
CA_smooth
NY_smooth

0.03
0.02
0.01
0
Jun 14
2020

Jun 21

Jun 28

Jul 5

Jul 12

Date

Fig. 12. Daily averages of CLAWS Task: Hate probability around event: COVID-19 pandemic worsens across the US - Half Window: 15 days

CLAWS Task: Hate Probability

0.07

US State of Emergency

Stimulus Package #1 Approved by Senate

state
CA
NJ
NY
WA
CA_smooth
NJ_smooth
NY_smooth
WA_smooth

0.06
0.05
0.04
0.03
0.02
0.01
0
Mar 8
2020

Mar 15

Mar 22

Mar 29

Apr 5

Date

Fig. 13. Daily average of CLAWS Task: Hate probability in the onset of the pandemic

readability tests also focus on these [40], with the reading ease
test computed as follows:
Number of Words
Number of Sentences
Number of Syllables
− 84.6 ×
(6)
Number of Words
The higher the score, the easier the input text is expected
to be in terms of readability. The modified version of the
Equation 6 is used to estimate the US grade level needed to
understand this text, and it is shown in Equation 7.
FRES = 206.835 − 1.015 ×

required to understand a text [38]. Similar to the aforementioned metrics, it is computed as follows:

ARI = 4.71 ×

Number of Characters
Number of Words
Number of Words
+ 0.5 ×
− 21.43
Number of Sentences

(8)

After rounding up the computed index, the value can be used to
estimate the grade level. Higher values indicate a more difficult
input text.
8) SPACHE Readability Formula: This measure of language complexity, which in nature is very similar to DaleChall algorithm, works by checking the words against a list
Number of Syllables
Number of Words
0.39×
+11.8×
−15.59of well-known words, and any word not appearing on that list
Number of Sentences
Number of Words
(7) is counted once and will be employed in computing the output.
7) Automated Readability Index: The Automated Readabil- This measure is basically prepared for assessing primary texts
ity Index (ARI) is another test to approximate the grade level [44].

State

16

Coleman-Liau Score
Flesch-Kincaid Score

Value

14

Dale-Chall Score
Gunning-Fog Score

12

SMOG Score
SPACHE Score

10

Linsear Write Score
ARI-based score

8
6
Jan 2020

Feb 2020

Mar 2020

Apr 2020

May 2020

Jun 2020

Jul 2020

Aug 2020

Sep 2020

Date

Fig. 14. Daily Average of Measures of Readability Through Time

D. Topics
In addition to the per-topic distributions shown in Figure
10, the word-clouds for each topic is also shown in Figure 15.

Oct 2020

Fig. 15. Wordclouds of the main topics of conversation in the first 9 months of 2020 across the US

