1

Potential Features of ICU Admission in X-ray
Images of COVID-19 Patients

arXiv:2009.12597v2 [eess.IV] 21 Jan 2021

Douglas P. S. Gomes, Anwaar Ulhaq, Manoranjan Paul, Michael J. Horry, Subrata Chakraborty, Manash Saha,
Tanmoy Debnath, D.M. Motiur Rahaman

Abstract—X-ray images may present non-trivial features with
predictive information of patients that develop severe symptoms
of COVID-19. If true, this hypothesis may have practical value in
allocating resources to particular patients while using a relatively
inexpensive imaging technique. The difficulty of testing such a
hypothesis comes from the need for large sets of labelled data,
which need to be well-annotated and should contemplate the
post-imaging severity outcome. This paper presents an original
methodology for extracting semantic features that correlate to
severity from a data set with patient ICU admission labels
through interpretable models. The methodology employs a neural
network trained to recognise lung pathologies to extract the
semantic features, which are then analysed with low-complexity
models to limit overfitting while increasing interpretability. This
analysis points out that only a few features explain most of the
variance between patients that developed severe symptoms. When
applied to an unrelated larger data set with pathology-related
clinical notes, the method has shown to be capable of selecting
images for the learned features, which could translate some
information about their common locations in the lung. Besides
attesting separability on patients that eventually develop severe
symptoms, the proposed methods represent a statistical approach
highlighting the importance of features related to ICU admission
that may have been only qualitatively reported. While handling
limited data sets, notable methodological aspects are adopted,
such as presenting a state-of-the-art lung segmentation network
and the use of low-complexity models to avoid overfitting. The
code for methodology and experiments is also avaliable 1 .
Index Terms—Covid-19, deep learning, ICU, severity, X-ray.

I. I NTRODUCTION
OVID-19 remains a serious worldwide health pandemic
with infections numbering 21,294,845 as of 16 August
2020 with 761,779 deaths [1]. One emerging characteristic
of COVID-19 disease is the wide range of symptoms experienced by infected persons ranging from entirely asymptomatic
through admission to the general ward for a range of symptoms

C

D.P.S. Gomes, Anwaar Ulhaq, Manoranjan Paul, and Tanmoy Debnath were with the Machine Vision and Digital Health (MAVIDH) Research group, School of Computing and Mathematics, Charles Sturt University, NSW, Australia. E-mails: douglas.uf@gmail.com, aulhaq@csu.edu.au,
mpaul@csu.edu.au, tdebnath@csu.edu.au.
Michael J. Horry and Subrata Chakraborty were with the Centre for
Advanced Modelling and Geospatial Information Systems (CAMGIS),
School of Information, Systems, and Modeling, Faculty of Engineering and IT, University of Technology Sydney, NSW, Australia. E-mails:
michael.j.horry@student.uts.edu.au, subrata.chakraborty@uts.edu.au
Manash Saha was with the Wollongong Hospital, NSW, Australia. Email:
manash.saha@health.nsw.gov.au
DM Motiur Rahaman was with the National Wine and Grape Industry
Centre, Charles Sturt University, Australia. Email: drahaman@csu.edu.au.
Michael J. Horry was also with the IBM Australia Limited, Sydney, NSW,
Australia.
1 https://github.com/dougpsg/covid mavidh icufeatures scoring

including fever, cough, fatigue, headache and diarrhoea [2] to
severe pneumonia requiring admission to ICU with mechanical
ventilation [3]. Reported case-fatality rates vary from 1% to
greater than 7%, usually due to respiratory failure [4].
Whilst care in modern ICUs will result in death rates
towards the lower end of the range, life-sustaining therapies
will, in practice, be limited by lack of personnel, infrastructure
and materials and equipment. This limitation of resources leads
clinicians to make prognostic decisions based on criterion such
as old age, fragility and comorbidity that can lead to the death
of patients with poor prognosis in favour of patients with better
prognosis [4].
The most widely used testing methodology for COVID-19 is
the real-time reverse-transcriptase-polymerase-chain-reaction
(RT-PCR) test [2], [5]. This test provides a binary indication of
whether a person is infected with COVID-19 or not. Similarly,
serology point of care tests that detect COVID-19 antibodies
provide a retrospective measure of infection [5] but do not
provide information relating to the severity or progression of
the disease and are not useful in helping clinicians to devise
an individual plan of treatment for a patient.
Chest medical imaging has proven to be useful in managing
more serious COVID-19 infections since respiratory dysfunction iso ne of the primary sources of COVID-19 morbidity and
mortality. Several researchers have shown Deep Learning to
be useful in classifying COVID-19 cases from medical images
including CXR [6]–[9], CT [10]–[12] with some research
also achieving promising results with the Ultrasound imaging
mode [?], [13], [14]. However, using Deep Learning as a
diagnostic tool can be problematic as it is hard to assess biases,
risk, potential overfitting, and ability to generalize in clinical
settings [15], [16]. Its value resides more on the prognosis
and treatment side than in actual diagnostic use [17]. Chest
X-rays (CXR) and Computed Tomography (CT) imaging are
useful tools in the management of COVID-19 cases since
these methods help clinicians to establish a baseline pulmonary
status and identify underlying pulmonary conditions that may
contribute to the patients’ risk. Chest X-rays, in particular,
are also non-invasive and can be potentially used as bed-side
patient monitoring tool [18]. Compared to CT, CXR is less
expensive, more available, and require less technical expertise
to perform and interpret than Ultrasound [?]. Over the course
of the COVID-19, subsequent imaging can be useful for
assessing COVID-19 progression and secondary complications
[15].
This paper therefore focuses on the potential disease progression and provides a machine learning-based method for

2

assessing features with predictive potential to classify patients
who are more likely to develop severe COVID-19 symptoms
and be eventually admitted in ICU. The method proposed here
leverages the existence of a data set with patient outcome
labels (only publicly accessible for CXR images) and the fact
that such images were taken before the patients were admitted
to ICU. As such a data set has a small sample size, significant
measures were taken to aggressively limit potential overfitting;
from image pre-processing techniques and lung segmentation
to model and feature selection. Measures like lung segmentation and cropping are not only needed given the data set
size but also to prevent the method to rely on confounding
image features, as already pointed as a known problem in
a recent work [20]. For these tasks, lung segmentation and
cropping, a model achieving state-of-the-art results is also
proposed and part of the contribution herein. The validation is
not only attested from separability metrics using the selected
semantic features in the ICU data set but also by correlating the
results to a more extensive external data set. Despite not having
ICU-related labels, such a data set contains pathologies and
localizations labels in its metadata that allows for assessing
the significance of the selected semantic features. The authors
present such a machine learning system in the hopes that it
will inform practitioners of the weight of particular features
in disease progression, thereby helping early intervention to
improve patient outcomes and plan better to allocate critical
resources. A second significant benefit of this novel framework
is that it can be used as a method to assist the medical
community in quickly identifying features in medical images
that are associated with severe progression of COVID-19,
similar diseases, or other potential future infections.
II. M ETHODS
Given the limited amount of data to learn the severitycorrelating features, a focus on limiting potential overfitting
was central to most methodological decisions. Instead of
disregarding such investigation simply because the data is
limited, the idea proposed here is to use low-complexity, interpretable methods to test features for potential predictive value.
Therefore, in an attempt to increase the features’ generalization
potential, a conservative bias for complexity was adopted
when choosing models and hyper-parameters. Some of this
concern for overfitting manifested as procedures adopted in the
pre-processing of images: histogram equalizations (adaptive
and standard) and lung segmentation and cropping. In the
learning stage, more importantly, it inspired the choice to
use a low complexity (low Vapnik–Chervonenkis dimension),
shallow decision tree, as to limit the ability of the method to
simply shatter the data set. Such an approach also presents the
considerable advantage of resulting in a highly interpretable
model, which is much desired in systems designed to support
human decision-making. To improve understandability, Fig.
1 is set to depict a high-level graphical summary of the
methodology.
The authors nevertheless understand that, despite the care
taken for overfitting and model selection, it is highly desirable
to have additional validation methods that can attest for

Training

Correlate
Cohen
data set

BIMCV
data set

Filter ICUrelated images

Histogram
equalization

Create masks

Pre-processing
and feature
extraction

Metadata
Label extraction

Segment

Strech / resize

Dictionary
Histogram eq.
+
CLAHE

Filter images

Feature
extraction
torchxrayvision

Gradient
features

Fit tree

Predict

Visualize

Results

Frequency
analysis

Fig. 1: Graphical summary of the methodology. ICU-related
images are filtered from a limited data set, processed, and
features are extracted via a specialized neural network. A few
of the extracted features are used to fit a shallow decision tree,
which is further applied to an external data set. Results from
training and external validation are compared, and visualizations are presented.

the model’s generalization ability. To this end, this study
also contemplates the application of such a model to a
larger, different dataset (1000+ images). Although lacking the
metadata regarding ICU admission, such X-ray images have
clinical annotations that can be tokenized and correlated to
the semantic features learned by the low-complexity model.
Given the features’ semantic nature, one can also correlate
them to other works in the related literature that report on
the lung pathologies present in patients that developed severe
symptoms.
A. Setting up the ICU data set
The data set used to learn the features present in patients
with higher chances of being admitted to ICU was a subset of
the data presented by Cohen et al. [21]. It is one of the most
popular data sets on the literature, favourited more than 2000
times on Github. One of its most positive characteristics is its
rich metadata containing categories such as sex, age, location,
patient condition, and outcome (ICU admission), allowing for

3

the investigation presented here. However, it should be noted
that many of such interesting labels are sparsely distributed in
the data set and not present for every every image. For the
images that had rich descriptors, two prominent labels were
of interest when setting up this ICU data set: ‘went-icu’ and
‘in-icu’. An image taken from a patient marked with ‘Y’ on
the former label and ‘N’ on the latter is a sample from a
patient that eventually developed severe symptoms before they
were admitted in ICU. Therefore, one can reasonably form the
hypothesis that there might be features in these images that
are associated with patients that were eventually admitted in
ICU. In total, 100 images containing these two labels were
used in the analysis, which were further multiplied by a factor
of approximately 10 (1040 images) by gentle affine random
data augmentations: rotation, piecewise affine transformation,
translation, and shear.
B. Pre-processing pipeline
The selected images are assigned to a class (ICU or not) and
fed through a pre-processing pipeline established to normalize
and remove potential bias-inducing artifacts. The sequential
blocks, illustrated in Fig. 1, are responsible for histogram
equalizations, lung segmentation and lung-area cropping. Histogram equalization is set to impose a normalizing effect
on the contrast of images by equalizing the distribution of
pixel intensities that might be concentrated in a narrow range.
Such a procedure usually improves the visual contrast in the
X-ray images, which can come from particular distribution
depending on the equipment manufacturer.
Subsequent to the normalization step, a lung-segmentation
model is applied to each X-ray image to improve the signalto-noise ratio for machine learning classification. Such a step
is potentially the most important regarding the mitigation of
potential bias as all areas that are not lung-related are excluded, allowing the model the focus on lung-specific features.
Regarding COVID-19 as a pulmonary disease, the objective
region of interest considered when segmenting the lung was
the region within the thorax, primarily the lung lobes. The
utility of this approach in medical image classification is
supported in past studies [22], and recent works have pointed
the fact that that lack of such practice can make the model
focus on confounding, not disease-related features [20]. A
comprehensive review of lung area segmentation techniques
may be found in [23], which notes that Deep Learning methods
provide similar accuracy as in inter-observer performance at
the expense of a long training process.
With the assumption that one could improve the U-Net
architecture to yield better dice similarity co-efficient results, as well as using training data from two additional
data sources, a lung-segmentation model was proposed and
trained. The additional images are from Montgomery and
Shenzen datasets [24] and resulted in a combination corpus
of 1185 CXR image/mask pairs. As typical U-net designs
are based on simple convolution stacks similar to the VGG
network architecture [25], a U-Net based on a skip-connection
architecture following [26], [27] is also proposed. Such an
architecture allows the network to re-use features learned in

(a)

(b)

(c)

Fig. 2: Chest X-Ray images in different stages of the preprocessing pipeline. a) Original images with different contrast
and histogram. b) Segmented lungs without the normalization
techniques. c) Images with lungs segmented and normalization
applied.
earlier layers, thereby improving its ability to learn the identity
function and increase generalization. As an additional benefit,
this architecture also allows the U-Net to train on a smaller
number of epochs, thereby reducing the training time. Given
the state-of-the-art dice similarity coefficient achieved by the
lung-segmentation model proposed here, validated in three
independent data corpora, it is expected that the model will
be useful in unseen images. Artefacts in the generated lung
field masks were removed by a combination of morphological
closing, contour filling and flood-filling, resulting in a set of
automatically segmented lung-field images. As a final step,
the images were also automatically cropped to fully contain
the segmented lung field, resulting in a uniform image set to
the downstream classifier and normalizing for different lung
sizes. In examples of the result of applying the pre-processing
pipeline, Fig. 2 illustrates three images in the input and output
stage, with and without normalization.
C. Feature extraction
1) TorchXRayVision: The feature extraction model is a
DenseNet [28] pre-trained on 80,000+ lung X-ray images.
The model, part of the TorchXRayVision library, was firstly
presented in [29] and also later used in a work proposing a
COVID-19 pneumonia severity score [18]. A particular aspect
of this model is that the author trained it to detect specific
lung pathologies from large data sets such as the CheXpert
(Stanford) [30], ChestX-ray8 (NIH) [31] and MIMIC-CXR
(MIT) [32]. The semantic labelling was performed by adding
an 18-node layer at the end of the network and training

4

Intermediate layer
(1024)
Segmented,
streched and
normalized images

Last layer (18)
Atelectasis

TorchXrayVision
DenseNet

Cardiomegaly
Consolidation

(a)

Pleural Thick.
Pneumonia
Pneumothorax

Gradient features

Mid-layer features

Entropy calc.

Last-layer
features

AutoGrad
(Gradient maps)

(b)

Sectioning

Fig. 3: Feature extraction pipeline. The segmented and normalized images are processed by a specialized pre-trained
network. Features are extracted from parts of its architecture
and hand-engineered from gradient maps.

it to classify different pathology labels through a sigmoid
activation layer. The labels that each of the nodes was trained
to classify were the ones found in the large data sets used:
Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion,
Emphysema, Enlarged Cardiomediastinum, Fibrosis, Fracture,
Hernia, Infiltration, Lung Lesion, Lung Opacity, Mass, Nodule,
Pleural Thickening, Pneumonia, Pneumothorax.
Feature extraction from such a model is feasible from many
parts of its architecture. In a way, any activation value from
the forward pass through the network can be used as features.
In this work, specifically, three modes were useful to attest
separability between classes, assess the semantic meaning, and
aid the pathology location. These methods were labelled ‘midlayer’, ‘last-layer’, and ‘class-activation map’ features. The
first method, extracting features from an intermediate layer,
extracts the activation values from a layer previous to last,
which contains 1024 nodes. These features were helpful to
attest the separability between classes but, as they are not
set to translate any semantic meaning, they were not used to
infer any correlation with particular pathologies. The last-layer
features were extracted from the activations of the 18-node
layer, pre-sigmoid, and were useful at correlating some of the
mentioned pathologies to the defined classes (ICU admission
or not). The class-activation map features, or gradient features
for short, were hand-engineered features soon to be described,
set with the intention of aid the localization of the pathologies
in the lung. Fig. 3 is set to illustrate potential feature-extraction
branches in the network architecture. It is worth mentioning
that none of these features was combined or even used in their
totality; they are presented as different classification scenarios
where just a few are used, complying with the discussed initial
intention to limit potential overfitting.
2) Gradient features: The mid-layer and last-layer features,
despite sufficient when attesting separability, do not translate

Fig. 4: Gradient features illustrations. a) Example of how
the entropy metric changes; from left to right, entropy values: 2.92e-5, 6.77e-5, 14.19e-5 . b) Sectioning of gradient
maps where the features are calculated: longitudinal (left) and
transversal (right) cuts.

any information regarding the location of the pathologies in
the lung. Therefore, gradient features were developed as a way
to aid the correlation of the location of the lung pathologies
with the patients with higher chances of developing severe
symptoms. The calculation of the gradients in the network
graph leaves in respect to an input image was performed by
the autograd torch class, which is often used to create saliency
maps and associate explainability to models [18], [33], [34].
In this study, however, a procedure to create features from the
gradient maps is proposed. First, the maps are construed with
the energy of the accumulated gradients and then sectioned
in two cuts: longitudinal and transversal. For each of these
cuts, an entropy measure is calculated. The resulting feature
is proposed here as a measure of the spread of the activations
in their respective cuts. Such a single value measure is inspired
by the Shannon entropy [35] can be trivially calculated with
Eq. 1. Examples of such features and how the images were
sectioned for spatial correlation are illustrated in Fig. 4.
E=−

XX
j

p log p

(1)

i

D. Classification and validation
Given the advantages of interpretability and the commitment
to limit the model complexity as primary factors in model selection, it is hard to argue for the application of another method
than decision trees [36]. Besides being human interpretable,
one can easily limit the effects of overfitting by mechanisms
of pruning, such as setting the minimum amount of samples
in the tree leaves. For the experiments performed here, for
example, trees were pruned to contain at least 20 samples in
each leaf, so nodes are not branched in just a few samples. This
constraint resulted in very shallow trees that only relied on a

5

few features to make the classification (usually three to five).
Such a pruning mechanism was the only hyper-parameter set
when fitting the trees; the rest were all default values from
the scikit learn package [37]. It is worth noting here that
the goal, despite trying to be as accurate as possible, is not
claiming the ability to precisely predict the patients that will
be admitted to ICU but discover potential semantic predictors
that are correlated with such patients.
The results are presented in three distinct scenarios with
respective feature sets that the tree-fitting algorithms could
greedily choose from mid-layer, last-layer, and gradient features. In each of these scenarios, the results are presented
by fitting the tree in the whole data set and by performing
cross-validation. To the latter, given the size of the data set,
a simple 5-fold would probably not be representative of the
predictive potential (especially when limited to leaf size of 10
samples). The number of folds was given by a leave-two-out
approach where all the data set is used for training, except for
2 samples (one of each class), which are then used for testing.
This procedure is repeated until all the samples in the dataset
are used for testing. The shallow trees resulting from fitting
the whole data set are also illustrated to depict their simple
structure and interpretability.

TABLE I: Previously proposed lung segmentation methods and
respective dice similarity coefficient
Authors, citation

Dice similarity coef.

Li et al., [43]

0.964

Candemir et al., [44]

0.967

Shao et al., [45]

0.972

Novikov et al., [42]

0.974

Yang et al., [46]

0.975

Hwang et al., [39]

0.980

Ours

0.988

their respective frequency for each group. Such a frequency
dictionary is then normalized by the set size and compared
between classes. The correlation is a measure of how the
features relevant to the classification in the first data set are
represented (frequency) in the different classes assigned in the
second data set. The null hypotheses, in this case, is that if the
images were sampled at random, the frequency of labels would
be equal in both sets. Such a procedure will become clearer
in the presentation of the results in the following section.

E. Correlating results with an external data set
In an attempt to add to the validation and address potential
generalization concerns, a larger, external data set was also
used in testing the fitted decision trees. Such a data set could
only be used to attest correlations since its metadata does
not contain labels regarding patient outcome (went to ICU
or not). The absence of these labels means that it could not be
used for validation (in the strict sense), but the information
it contains could be correlated to results from the learned
semantic features. This data set of X-ray images from COVID19 patients, named BIMCV [38] and publicly available, contains doctor’s annotations for each subject that were used here
as discrete labels and descriptive terms. Besides numerous,
such labels refer not only to the pathologies observed by the
doctors in the patient’s lungs but also points to their location.
Therefore, because they have a semantic meaning, they can
then be correlated to the features from the last layer, which
are also descriptive.
The method in which the features are correlated to the
external data set is also presented as a contribution and, to the
best of the authors’ knowledge, not previously presented. As
the quality of images in the data set varies a lot, the procedure
starts by running the images through the same pre-processing
pipeline described before with histogram equalization, lung
segmentation and cropping, and feature extraction. Some images that did not have the lungs successfully segmented or
were very low-quality are filtered out, but a total of 1312
images were labelled as valid. Subsequent to the processing
pipeline, the decision tree performs the classification of features in the binary classes 0 and 1, and the samples pertaining
to each class are assigned to two different sets. At this
point, the correlation method enters the picture by creating
a dictionary with the pathologies and locations labels and

III. R ESULTS
Prior to discussing the classification metrics and feature
work, it is worth noting the outcomes of training the lung
segmentation network, also part of the contribution herein.
Trained on large chest X-rays image data sets, the designed
architecture achieved a maximum validation dice similarity coefficient of 0.988 at epoch 93, which is comparable to the best
result encountered thus far. The best performance observed
in the literature was achieved using a complex CNN-based
Deep Learning system, achieving a dice similarity coefficient
of 0.980 [39]. A much simpler approach leveraging a U-Net
architecture [40], similar to the the one here, was trained on
the JSRT dataset [41] consisting of 385 CXR images with
gold-standard masks to achieve a dice similarity co-efficient
of 0.974 [42]. Table I is set to contrast the performance of
previously methods and the one proposed here.
A. Separability
The separability metrics were calculated for the final semantic features and other two different scenarios. The first refers
to the non-task-specific features in the intermediate layer,
which although contemplating 1024 nodes, had only 4 features
used to attest invariance between classes. The second scenario
regards the separability of the actual semantic (pathology
labels) features output by the 18-node last layer of the network.
The third, gradient features, will have its discussion delayed
to a further sub-section regarding the pathology localizations,
which is where its value really lies. The metrics presented as
‘whole set’ refers to using all samples to fit the tree while
‘cross-val’ indicates the ones resulting from the leave-two-out
cross-validation approach. Such an approach means fitting the
same number of trees as in the number of samples in the

6

smallest class. For example, if one class has n samples and
another has m, n decision trees will be fitted if n < m.
The resulting metrics showed that the mid-layer features are
better at separating the classes but not for a large margin when
compared to the semantic features from the last layer. The midlayer whole set features resulted in 0.89 accuracy and 0.88 F1score, while the last-layer features’ accuracy was 0.80 with a
0.74 F1-score. Such metrics were obtained when limiting the
minimum leaf size to 20 samples with a maximum tree depth
of 4; both values were arbitrarily set by trial and error attempts
guided by cross-validation. The whole-set tree is the result
to be highlighted here as it has allegedly semantic features
relating to the lung pathologies. If the features are ranked by
samples under their respective node, it can result in interesting
insights since they are trained to have semantic meaning. The
three features with most samples, in this case, were ‘Effusion’,
‘Consolidation’, and ‘Cardiomegaly’. Fig. 5 depicts a 3-level
version of the tree for better illustration purposes, which has
performance closely equivalent to the 4-level version. Such
labels will be important in the following subsection when correlating with the external dataset and in the discussion section
where they will be qualitative compared to the pathologies
referred to in the literature regarding patients that develop
severe symptoms. As for the cross-val scenario, mid-layers
and last-layer features resulted in 0.86 and 0.76 accuracies,
respectively. The distribution of predicted versus real labels
can be inspected by the decision matrices presented in Fig.
5. As an additional worth-mentioning note from preliminary
experiments, it was observed that such accuracies would drop
to values close to 0.5 if the normalization techniques were not
applied, which was significant evidence of their importance.

Effusion
<=0.89
Yes

No

Consolidation
<=-0.67
No
Yes
Fracture
<=-1.36
Yes

Lung Opacity
<=-0.82

Cardiomegaly

<=-155

Yes

No

Not ICU
0.86

Pneumothorax
<=1.25
No

Yes

ICU
0.72

No

Not ICU
0.79

Yes
ICU
0.86

Not ICU
0.64

No

Not ICU
0.59

ICU
0.83

(a)
Feat. 870
<=0.15
Yes
Feat. 566
<=-0.41
Yes

No

Yes

Feat. 39
<=0.95
Yes
Not ICU
0.76

No

Feat. 6
<=1.64
No
ICU
0.82

Yes
ICU
0.91

Feat. 920
<=0.95

Feat. 421
<=0.80

No
Not ICU
0.71

Yes
Not ICU
0.96

No
ICU
0.76

No
Feat. 735
<=-0.27
Yes
No
ICU
0.96

Not ICU
0.75

B. Correlating with an external data set
The many labels contained in the BIMCV data set allowed
for an interesting comparison between the classes after classification was applied to the images. From the 1312 images
selected after filtering, 73 did not have text labels assigned, but
most had at least a few pathology and location labels. After
the images went through the processing and feature extraction
pipeline, the fitted decision tree (whole set, last layer) classified the features and their respective text labels were assigned
to sets given by their predicted class. In this manner, two
dictionaries could be construed with the keys being the unique
words in the set, and the values representing the frequency
of such labels. The five most frequent pathological labels in
the data set can be listed as examples of the types of terms
describes: ‘pattern’ (467), ‘COVID’ (443), ‘pneumonia’ (301),
‘increased density’ (368) and ‘unchanged’ (272).
The comparison presented here was set to be the normalized
ratio between the frequency of words in the potentially severe
class (1) and not (0). By ‘normalized’ ratio, it is meant
that the frequency of words was multiplied by the inverse
of the number of samples in the respective set before the
ratio calculation. The number of samples in each set after
classification was 806 images in class 0 and 506 in class 1.
The ratio was calculated only for words that had the minimal
arbitrary number of appearances of 20 in each class. It may

(b)

Fig. 5: Decision trees and confusion matrices from the three
scenarios presented: a) last-layer and b) mid-layer features.

be worth noting that, if the samples were picked at random,
despite the selection of images before classification or the
number of images in a set, such a ratio would be equal to
1, given the normalization. Table II presents the pathological
and localization labels with normalized ratios higher than 1.2
and lower than 0.8, in descending order. The most interesting
aspect from such comparison is certainly the fact that two
of the most over-represented features in the images of the
external data set were also the ones chosen as most important
when fitting the decision tree in the data set used for training.
This shows that the features used to separate the data set used
for training are also consistently detected on the external data
set. Moreover, two over-represented features in the class of
higher chances to go severe — ‘Consolidation’ and ‘Bilateral’
— are often cited as pathologies relating to the severity in
the literature [48], [49]. It is also pointed out that a systematic
review on the subject showed that the prevalent locations were
bilateral and peripheral (another over-represented term from

7

Fibrosis
Inferior < -0.5
Yes

No

Effusion
Superior < -0.3
Yes
No
ICU
0.96

Edema
Superior < 0.09
No
Yes
ICU
0.63

Not ICU
0.79

ICU
0.63

Fig. 6: Decision tree and confusion matrix from the entropy
gradient features.

results presented here).

(a)

C. Spatial analysis and visualization
While the pathological features presented in the previous
subsections are expressive and correlate with the frequency
of labels in the external data set, they cannot be correlated to
any information regarding the locations of the pathologies. The
features are scalars given by their respective layer’s activation
function, which does not translate information about their
spatial distribution. The simple method proposed here uses
hand-engineered features on gradient maps to check for areas
where the predictive features are prevalent. The decision trees
were trained in the same manner as other features but, in
this case, only a two-level tree was used to aggressively limit
overfitting. Every feature relates to an entropy metric in one
of the segments extracted from the gradient maps: left and
right longitudinal, and superior and inferior transversal cuts
(see Fig. 4).
The entropy features extracted from the gradient maps
segments and selected as splits in the decision tree presented
some correlation with the external data set and other reported findings. Regarding the separability metrics, fitting a
shallow decision tree with only three features in the whole
data set resulted in 74% accuracy (separability), while with
72% accuracy score in cross-validation. Shown in Fig. 6, the
three features were ‘Fibrosis’ on the Inferior, and ‘Effusion’
and ‘Edema’ on the Superior cut. Although these features
don’t translate any definitive information, such an interpretable
method shows to be a promising way to analyse the spatial
relationships between features and potential severity.
Due to the fact that the methodology includes the segmentation and cropping of the lung area as an important
normalization measure, an interesting way to visualize the
spatial distribution of some features becomes possible. Such a
way proposed here differs from the usual by not just plotting
the energy of the gradient maps of a particular feature but
averaging all the maps of a certain class and then platting
that as a surface. An example is presented in Fig. 7 where
the chosen feature is the one in the top of the tree of

(b)

Fig. 7: Surface plots of the average of gradient maps in each
class: a) Non-ICU and b) ICU.
semantic features: ‘Effusion’. The two images are the average
of activations of the two classes, normalized by class size.
The plotted surfaces show that such a feature not only has
gradients with higher energy in the images but also are more
spatially distributed throughout the lung (smaller entropy).
IV. D ISCUSSIONS
It is worth iterating that the contribution here is not the
development of a deployment-ready classifier but the presentation of pathological features with similar characteristics
as ones observed in patients with higher chances of developing severe symptoms. As works in the literature have
been discussing [15]–[17], using Deep Learning methods in
clinical settings is highly inadvisable, even when interpretable.

8

TABLE II: Over- and under-represented pathology and localization labels and their frequency ratio
Pathology
Feature
‘Consolidation’ (127)
‘Alveolar’ (151)
‘Effusion’ (60)
‘COVID’ (443)
‘Pneumonia’ (301)
‘Pleural’ (67)
‘Infiltrates’ (184)
‘Interstitial’ (223)
...
‘Normal’ (168)

Ratio (C1/C0)
1.67
1.34
1.30
1.24
1.24
1.22
1.23
1.2

Localization
Feature
Ratio (C1/C0)
‘Bilateral’ (449)
1.58
‘Middle’ (326)
1.39
‘Lower’ (452)
1.31
‘Peripheral’ (489)
1.28
‘Upper’ (305)
1.25
‘Left’ (549)
1.23
...
‘Hilar’ (99)
0.66
‘Mediastinum’ (84)
0.56

0.48

Practical prediction claims would already be restrained by
the classification metrics presented here, but they still can’t
take away the fact that some features showed interesting
correlations and significance to patient severity. Moreover, the
contribution here is not constrained to the predictive potential
of features; it also includes the methodology developed to
evaluate and select them. More than informing practitioners
of evidence for setting heavier weights to the importance
of some pathologies in disease development, one can hope
that the method presented could be useful in for problems in
similar domains. Given that computer vision-based COVID-19
diagnostic tools are not recommended for practical use due to
their potential bias and risk [15]–[17], developing systems that
can support human decisions in critical resource management
becomes one of the most promising uses of Machine Learning
[17].
Regarding significance, it is arguable that the most significant aspect of the results is how the decision tree, fitted
by a limited dataset, selected images on an external data set
that had an over-representation of labels relating to disease
severity. The two most over-represented features in the class of
higher chances to go severe — ‘Consolidation’ and ‘Bilateral’
— are often cited as pathologies relating to severity in the
literature [47]–[49]. For example, the authors in [50] reported
that the evolution from ground-glass opacities to consolidation
was present in some severe patients. They also pointed out
that a systematic review on the subject showed that the
prevalent locations were bilateral and peripheral (another overrepresented term from results presented here). Another highly
cited work [2] reported that most of the patients had bilateral
involvements and that ICU-admitted patients showed bilateral
multiple lobular and subsegmental areas of consolidation.
Nevertheless, one should also note that these pathologies
were not present in all severe patients, neither in the analysis
presented here and in the literature. This observation should
be evident since lung pathologies are not the only factor in
determining severity of COVID-19 and ICU admission. Comorbidities and other systemic involvement in COVID-19 also
play an important role in this disease, thus representing the
primary limitation of the present work. Fig. 8 summarizes and
illustrates some of the worth-noting correlations found here are
for improved clarity and comprehension.

External
Dataset

Internal
Dataset
MIddle

Effusion
Pleural

Lung Lesions

Alveolar

Consolidation

Pneumothorax

Lower

Bilateral
Lung
opacities

Peripheral
Pneumonia

Literature

Fig. 8: Venn diagram illustrating some of the correlated
findings from the internal, external, and literature reported
pathologies and locations.

V. C ONCLUSIONS
The results presented in this paper point to the fact that
some semantic features (presented here as pathology scores)
have reasonable predicting potential to patients that eventually
develop severe symptoms and are admitted to ICU. Extracted
from a limited data set of chest x-ray images, the features
only presented any predictive potential when enough care was
taken to prevent overfitting. Such measures included training a
state-of-the-art lung segmentation network, using a pre-trained,
specialized network to extract the features, image normalization techniques, and strictly selecting an interpretable classifier
of low complexity. Results from running the fitted classifier
on an external data set showed that it indeed selects for the
semantic features chosen when learning. These features not
only appeared more frequently in the set of patients classified
as with higher chances of going severe but a frequency
analysis on all labels showed that descriptors often used to
describe COVID-19 pathologies were over-represented in such
patients. This observation was also attested in localizations

9

labels, which showed over-representation of important terms
like ‘bilateral’, ‘peripheral’, and ‘middle’. Such a methodology
for creating correlations between interpretable features and
patient outcome may aid human decision-making in resource
management and be a template method for similar problems
from other domains. More results, analyses, and source code
of experiments can be found in the in work repository 2 .
R EFERENCES
[1] World Health Organization, “Coronavirus disease 2019 (covid-19):
situation report,” 2020. [Online]. Available: https://www.who.int/
emergencies/diseases/novel-coronavirus-2019/situation-reports
[2] C. Huang, Y. Wang, X. Li, L. Ren, J. Zhao, Y. Hu, L. Zhang, G. Fan,
J. Xu, X. Gu et al., “Clinical features of patients infected with 2019
novel coronavirus in wuhan, china,” The lancet, vol. 395, no. 10223,
pp. 497–506, 2020.
[3] W.-j. Guan, Z.-y. Ni, Y. Hu, W.-h. Liang, C.-q. Ou, J.-x. He, L. Liu,
H. Shan, C.-l. Lei, D. S. Hui et al., “Clinical characteristics of coronavirus disease 2019 in china,” New England journal of medicine, vol.
382, no. 18, pp. 1708–1720, 2020.
[4] J.-L. Vincent and F. S. Taccone, “Understanding pathways to death in
patients with covid-19,” The Lancet Respiratory Medicine, vol. 8, no. 5,
pp. 430–432, 2020.
[5] Y.-W. Tang, J. E. Schmitz, D. H. Persing, and C. W. Stratton, “Laboratory diagnosis of covid-19: current issues and challenges,” Journal of
clinical microbiology, vol. 58, no. 6, 2020.
[6] J. Civit-Masot, F. Luna-Perejón, M. Domı́nguez Morales, and A. Civit,
“Deep learning system for covid-19 diagnosis aid using x-ray pulmonary
images,” Applied Sciences, vol. 10, no. 13, p. 4640, 2020.
[7] Y. Oh, S. Park, and J. C. Ye, “Deep learning covid-19 features on cxr
using limited training data sets,” IEEE Transactions on Medical Imaging,
2020.
[8] F. Ucar and D. Korkmaz, “Covidiagnosis-net: Deep bayes-squeezenet
based diagnostic of the coronavirus disease 2019 (covid-19) from x-ray
images,” Medical Hypotheses, p. 109761, 2020.
[9] S. H. Yoo, H. Geng, T. L. Chiu, S. K. Yu, D. C. Cho, J. Heo, M. S.
Choi, I. H. Choi, C. Cung Van, N. V. Nhung et al., “Deep learningbased decision-tree classifier for covid-19 diagnosis from chest x-ray
imaging,” Frontiers in medicine, vol. 7, p. 427, 2020.
[10] Z. Li, Z. Zhong, Y. Li, T. Zhang, L. Gao, D. Jin, Y. Sun, X. Ye, L. Yu,
Z. Hu et al., “From community acquired pneumonia to covid-19: A
deep learning based method for quantitative analysis of covid-19 on
thick-section ct scans,” medRxiv, 2020.
[11] Q. Ni, Z. Y. Sun, L. Qi, W. Chen, Y. Yang, L. Wang, X. Zhang, L. Yang,
Y. Fang, Z. Xing et al., “A deep learning approach to characterize 2019
coronavirus disease (covid-19) pneumonia in chest ct images,” European
radiology, pp. 1–11, 2020.
[12] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha, W. Liang,
C. Wang, K. Wang et al., “Clinically applicable ai system for accurate
diagnosis, quantitative measurements, and prognosis of covid-19 pneumonia using computed tomography,” Cell, 2020.
[13] J. Born, G. Brändle, M. Cossio, M. Disdier, J. Goulet, J. Roulin,
and N. Wiedemann, “Pocovid-net: automatic detection of covid-19
from a new lung ultrasound imaging dataset (pocus),” arXiv preprint
arXiv:2004.12084, 2020.
[14] S. Roy, W. Menapace, S. Oei, B. Luijten, E. Fini, C. Saltori, I. Huijben,
N. Chennakeshava, F. Mento, A. Sentelli et al., “Deep learning for
classification and localization of covid-19 markers in point-of-care lung
ultrasound,” IEEE Transactions on Medical Imaging, 2020.
[15] G. D. Rubin, C. J. Ryerson, L. B. Haramati, N. Sverzellati, J. P. Kanne,
S. Raoof, N. W. Schluger, A. Volpi, J.-J. Yim, I. B. Martin et al.,
“The role of chest imaging in patient management during the covid19 pandemic: a multinational consensus statement from the fleischner
society,” Chest, 2020.
[16] L. Wynants, B. Van Calster, M. M. Bonten, G. S. Collins, T. P. Debray,
M. De Vos, M. C. Haller, G. Heinze, K. G. Moons, R. D. Riley et al.,
“Prediction models for diagnosis and prognosis of covid-19 infection:
systematic review and critical appraisal,” bmj, vol. 369, 2020.
[17] J. P. Cohen, P. Morrison, L. Dao, K. Roth, T. Q. Duong, and
M. Ghassemi, “Covid-19 image data collection: Prospective predictions
are the future,” arXiv 2006.11988, 2020. [Online]. Available:
https://github.com/ieee8023/covid-chestxray-dataset
2 https://github.com/dougpsg/covid

mavidh icufeatures scoring

[18] J. P. Cohen, L. Dao, P. Morrison, K. Roth, Y. Bengio, B. Shen,
A. Abbasi, M. Hoshmand-Kochi, M. Ghassemi, H. Li et al., “Predicting
covid-19 pneumonia severity on chest x-ray with deep learning,” arXiv
preprint arXiv:2005.11856, 2020.
[19] G. D. Rubin, C. J. Ryerson, L. B. Haramati, N. Sverzellati, J. P. Kanne,
S. Raoof, N. W. Schluger, A. Volpi, J.-J. Yim, I. B. K. Martin, D. J.
Anderson, C. Kong, T. Altes, A. Bush, S. R. Desai, O. Goldin, J. M. Goo,
M. Humbert, Y. Inoue, H.-U. Kauczor, F. Luo, P. J. Mazzone, M. Prokop,
M. Remy-Jardin, L. Richeldi, C. M. Schaefer-Prokop, N. Tomiyama,
A. U. Wells, and A. N. Leung, “The role of chest imaging in patient
management during the covid-19 pandemic: A multinational consensus
statement from the fleischner society,” Radiology, vol. 296, no. 1, p.
172—180, July 2020.
[20] A. J. DeGrave, J. D. Janizek, and S.-I. Lee, “Ai for radiographic covid19 detection selects shortcuts over signal,” medRxiv, 2020.
[21] J. P. Cohen, P. Morrison, and L. Dao, “Covid-19 image data
collection,” arXiv 2003.11597, 2020. [Online]. Available: https:
//github.com/ieee8023/covid-chestxray-dataset
[22] A. Rabinovich, A. Vedaldi, and S. J. Belongie, Does image segmentation
improve object categorization? Citeseer, 2007.
[23] S. Candemir and S. Antani, “A review on lung boundary detection in
chest x-rays,” International journal of computer assisted radiology and
surgery, vol. 14, no. 4, pp. 563–576, 2019.
[24] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. Wáng, P.-X. Lu, and
G. Thoma, “Two public chest x-ray datasets for computer-aided screening of pulmonary diseases,” Quantitative imaging in medicine and
surgery, vol. 4, no. 6, p. 475, 2014.
[25] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[26] E. Mique and A. Malicdem, “Deep residual u-net based lung image segmentation for lung disease detection,” IOP Conference Series: Materials
Science and Engineering, vol. 803, p. 012004, may 2020.
[27] Z. Zhang, Q. Liu, and Y. Wang, “Road extraction by deep residual unet,” IEEE Geoscience and Remote Sensing Letters, vol. 15, no. 5, pp.
749–753, 2018.
[28] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4700–4708.
[29] J. P. Cohen, M. Hashir, R. Brooks, and H. Bertrand, “On the
limits of cross-domain generalization in automated x-ray prediction,”
in Medical Imaging with Deep Learning, 2020. [Online]. Available:
https://arxiv.org/abs/2002.02497
[30] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo, R. Ball, K. Shpanskaya et al., “Chexpert: A large
chest radiograph dataset with uncertainty labels and expert comparison,”
in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33,
2019, pp. 590–597.
[31] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers,
“Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common thorax
diseases,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 2097–2106.
[32] A. E. Johnson, T. J. Pollard, N. R. Greenbaum, M. P. Lungren, C.-y.
Deng, Y. Peng, Z. Lu, R. G. Mark, S. J. Berkowitz, and S. Horng,
“Mimic-cxr-jpg, a large publicly available database of labeled chest
radiographs,” arXiv preprint arXiv:1901.07042, 2019.
[33] O. Gozes, M. Frid-Adar, H. Greenspan, P. D. Browning, H. Zhang,
W. Ji, A. Bernheim, and E. Siegel, “Rapid ai development cycle for the
coronavirus (covid-19) pandemic: Initial results for automated detection
& patient monitoring using deep learning ct image analysis,” arXiv
preprint arXiv:2003.05037, 2020.
[34] S. Hu, Y. Gao, Z. Niu, Y. Jiang, L. Li, X. Xiao, M. Wang, E. F. Fang,
W. Menpes-Smith, J. Xia et al., “Weakly supervised deep learning for
covid-19 infection detection and classification from ct images,” IEEE
Access, vol. 8, pp. 118 869–118 883, 2020.
[35] C. E. Shannon, “A mathematical theory of communication,” ACM
SIGMOBILE mobile computing and communications review, vol. 5,
no. 1, pp. 3–55, 2001.
[36] T. Hastie, R. Tibshirani, and J. Friedman, The elements of statistical
learning: data mining, inference, and prediction. Springer Science &
Business Media, 2009.
[37] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine
Learning Research, vol. 12, pp. 2825–2830, 2011.

10

[38] M. de la Iglesia Vayá, J. M. Saborit, J. A. Montell, A. Pertusa, A. Bustos,
M. Cazorla, J. Galant, X. Barber, D. Orozco-Beltrán, F. Garcı́a-Garcı́a
et al., “Bimcv covid-19+: a large annotated dataset of rx and ct images
from covid-19 patients.” arXiv preprint arXiv:2006.01174, 2020.
[39] S. Hwang and S. Park, “Accurate lung segmentation via networkwise training of convolutional networks,” in Deep learning in medical
image analysis and multimodal learning for clinical decision support.
Springer, 2017, pp. 92–99.
[40] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference on
Medical image computing and computer-assisted intervention. Springer,
2015, pp. 234–241.
[41] J. Shiraishi, S. Katsuragawa, J. Ikezoe, T. Matsumoto, T. Kobayashi, K.i. Komatsu, M. Matsui, H. Fujita, Y. Kodera, and K. Doi, “Development
of a digital image database for chest radiographs with and without a
lung nodule: receiver operating characteristic analysis of radiologists’
detection of pulmonary nodules,” American Journal of Roentgenology,
vol. 174, no. 1, pp. 71–74, 2000.
[42] A. A. Novikov, D. Lenis, D. Major, J. Hladůvka, M. Wimmer, and
K. Bühler, “Fully convolutional architectures for multiclass segmentation
in chest radiographs,” IEEE Transactions on Medical Imaging, vol. 37,
no. 8, pp. 1865–1876, 2018.
[43] X. Li, S. Luo, Q. Hu, J. Li, D. Wang, and F. Chiong, “Automatic lung
field segmentation in x-ray radiographs using statistical shape and appearance models,” Journal of Medical Imaging and Health Informatics,
vol. 6, no. 2, pp. 338–348, 2016.
[44] S. Candemir, S. Jaeger, K. Palaniappan, J. P. Musco, R. K. Singh,
Z. Xue, A. Karargyris, S. Antani, G. Thoma, and C. J. McDonald, “Lung
segmentation in chest radiographs using anatomical atlases with nonrigid
registration,” IEEE transactions on medical imaging, vol. 33, no. 2, pp.
577–590, 2013.
[45] Y. Shao, Y. Gao, Y. Guo, Y. Shi, X. Yang, and D. Shen, “Hierarchical
lung field segmentation with joint shape and appearance sparse learning,”
IEEE transactions on medical imaging, vol. 33, no. 9, pp. 1761–1780,
2014.
[46] W. Yang, Y. Liu, L. Lin, Z. Yun, Z. Lu, Q. Feng, and W. Chen,
“Lung field segmentation in chest radiographs from boundary maps
by a structured edge detector,” IEEE journal of biomedical and health
informatics, vol. 22, no. 3, pp. 842–851, 2017.
[47] S. H. Yoon, K. H. Lee, J. Y. Kim, Y. K. Lee, H. Ko, K. H. Kim, C. M.
Park, and Y.-H. Kim, “Chest radiographic and ct findings of the 2019
novel coronavirus disease (covid-19): analysis of nine patients treated in
korea,” Korean journal of radiology, vol. 21, no. 4, pp. 494–500, 2020.
[48] A. Bernheim, X. Mei, M. Huang, Y. Yang, Z. A. Fayad, N. Zhang,
K. Diao, B. Lin, X. Zhu, K. Li et al., “Chest ct findings in coronavirus
disease-19 (covid-19): relationship to duration of infection,” Radiology,
p. 200463, 2020.
[49] W. Zhao, Z. Zhong, X. Xie, Q. Yu, and J. Liu, “Relation between chest
ct findings and clinical conditions of coronavirus disease (covid-19)
pneumonia: a multicenter study,” American Journal of Roentgenology,
vol. 214, no. 5, pp. 1072–1077, 2020.
[50] M.-Y. Ng, E. Y. Lee, J. Yang, F. Yang, X. Li, H. Wang, M. M.-s.
Lui, C. S.-Y. Lo, B. Leung, P.-L. Khong et al., “Imaging profile of the
covid-19 infection: radiologic findings and literature review,” Radiology:
Cardiothoracic Imaging, vol. 2, no. 1, p. e200034, 2020.

