C AN QUESTIONS SUMMARIZE A CORPUS ?
U SING QUESTION GENERATION FOR CHARACTERIZING COVID-19 RESEARCH

Gabriela Surita1 , Rodrigo Nogueira1,2,3 , and Roberto Lotufo1,2

arXiv:2009.09290v1 [cs.IR] 19 Sep 2020

1

School of Electrical and Computing Engineering, UNICAMP
2
NeuralMind Inteligência Artificial
3
David R. Cheriton School of Computer Science, University of Waterloo

September 22, 2020

A BSTRACT
What are the latent questions on some textual data? In this work, we investigate using question
generation models for exploring a collection of documents. Our method, dubbed corpus2question,
consists of applying a pre-trained question generation model over a corpus and aggregating the
resulting questions by frequency and time. This technique is an alternative to methods such as topic
modelling and word cloud for summarizing large amounts of textual data.
Results show that applying corpus2question on a corpus of scientific articles related to COVID-19
yields relevant questions about the topic. The most frequent questions are “what is covid 19” and
“what is the treatment for covid”. Among the 1000 most frequent questions are “what is the threshold
for herd immunity” and “what is the role of ace2 in viral entry”. We show that the proposed method
generated similar questions for 13 of the 27 expert-made questions from the CovidQA question
answering dataset.
The code to reproduce our experiments and the generated questions are available at: https://
github.com/unicamp-dl/corpus2question

1

Introduction

Methods for exploring, summarizing, and automatically organizing large volumes of textual data are commonly used
by information retrieval and natural language processing systems. They allow researchers to quickly grasp what are
widely discussed topics on specific literature as well as capture directions for further exploration. Examples of such
methods are word cloud [1], topic modelling [2] and document clustering [3, 4]. Still, these techniques are known
to have limitations. Word cloud and topic modelling rely purely on n-gram statistics of documents in the corpus, and
thus their outputs are keywords isolated from their original contexts. Clustering based on document embeddings try
to capture the whole document context into a condensed vector, but sometimes fail to differentiate between parts of a
document. Also, clusters are represented by document-long texts, whose assimilation can be cognitively demanding for
users that are not familiar with the topic. Hence, clustering methods are often used in conjunction with visualization or
summarization techniques to make their outputs more concise to humans.
We propose a different method for exploring and compressing information of a corpus based on the questions its
documents can answer. We expect questions to capture deeper context knowledge than n-grams, but yet be a diverse
and natural description of a corpus. This paper tries to answer two core questions: can deep learning models generate
relevant research questions on scholarly articles on a zero-shot setting (i.e., without any training data on that particular
domain)? If so, are these questions suitable for a compact description of a collection of documents?
We use existing question generation models, that, to our knowledge, have not been applied to out-domain data. To our
knowledge there are no metrics to assess the quality of zero-shot question generation methods over large volumes of
textual data, so we propose two quantitative methods to evaluate the model output in comparison to human-generated
questions. We apply our method, called corpus2question, on CORD-19 [5], a collection of research papers related

corpus2question (ours)
#

question

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

what is the treatment for covid
what is covid 19
what is covid-19
what is sars cov
what is the risk of covid
what is the impact of covid
what is covid
what is the prevalence of covid
what is sars-cov
what is the mortality rate of covid
what is the incubation period for covid
what is the sir model
what is sars-cov-2
what is the incidence of covid
what is social distancing

Topic Modelling - LDA
count

#

topics

3185
2604
1775
1694
1479
1449
1316
1084
991
942
924
863
823
805
683

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

data model used using based
research social students new work
cov sars sars_cov cells protein
patients treatment 19 covid covid_19
air 2020 temperature study data
cases number model 19 covid
study health participants 19 covid
social distancing public contact social_distancing
covid 19 covid_19 risk age
patients 19 covid covid_19 disease
covid patients 19 covid_19 patient
19 covid covid_19 sars cov
data model time results using
patients 19 covid covid_19 study
economic countries food global crisis

Word Cloud

Figure 1: Results for corpus2question, topic modelling using Latent Dirichlet Allocation (LDA) and word cloud
over the CORD-19 dataset. The results for corpus2question show the 15 most frequent questions. The counts
column represents the number of text spans that originated the question. Results for LDA uses 2-grams and 20 topics.
We present 15 out of 20 topics with 5 most relevant n-grams for each topic.
to COVID-19, and compare its questions against expert-generated questions from the CovidQA dataset [6]. Finally, we
include a qualitative discussion about the generations and some examples of how the data can be further aggregated.
To demonstrate the relevance of the proposed method, we present its most frequent generated questions in Figure 1,
alongside with results from alternative methods.
The rest of the paper is organized as follows: Section 2 presents related work on methods for exploring datasets and
question generation; Section 3 describes the models applied for question generation, the dataset preprocessing methods
and two experiments proposed for evaluating the task; Section 4 presents the results for the proposed experiments, as
well as qualitative discussions about the generations; Section 5 presents limitations of the current method and directions
for future research. Finally, Section 6 concludes the work by reiterating key results.

2

Related Work

In this section, we present related work on dataset exploration techniques, more specifically, topic modelling and
document clustering. We also describe the question generation techniques related to the proposed method.
2

A virus is a
submicroscopic infectious
agent that replicates only
inside the living cells of
an organism.

What is a virus?
What is a virus?

Where viruses
replicate?
A pandemic is an
epidemic of an infectious
disease that has spread
across a large region.

Molecular virology is the
study of viruses on a
molecular level. Viruses
are submicroscopic
parasites that replicate
inside host cells.

Question
Generator

What is a pandemic

Aggregations
and Sorting
What is a pandemic

What is molecular
virology?
What is a virus?

Figure 2: Illustration of the question generation pipeline followed by basic aggregations.

2.1

Topic Modelling

Topic modelling methods represent a corpus by underlying themes (described by n-grams) and map documents as
combinations of these themes. One of the most relevant algorithms in this field is Latent Dirichlet Allocation (LDA) [2].
LDA is a statistical generative method that given a number of topics and a tokenized text dataset produces a term
distribution per topic and a topic distribution per document.
LDA has been an inspiration for several variants that followed. Further work includes techniques for finding an optimal
number of topics [7] or improving the term definition [8]. Other works try to combine LDA with word embeddings [9]
or document embedding techniques [10].
Still, some publications point that LDA alone is sensitive to noise and can generate irrelevant topics on specific
domains [11]. Recent work has tried to address these issues with human-in-the-loop strategies [12]. Others combine
topic models with visualization tools like sankey charts to make generations more friendly to users [13].
2.2

Document Clustering

Document clustering is another widely used technique to represent large corpora. It consists of defining a vector
representation for a document, a similarity metric between two vectors and then applying an unsupervised clustering
algorithm [3]. Known document representations are information-theoretical methods such as tf–idf [14] followed by
a dimensionality reduction technique (such as PCA) and neural network methods such as doc2vec [4] and SentenceBERT [15].
These techniques are used in information retrieval systems and usually are suitable for search applications [16]. Still,
clusters are represented by document-long texts, which require additional inspection. Other work suggests that these
techniques sometimes fail to replicate similarity as humans understand it [17]. Some document representations also
have low explainability, making it hard to know why a pair of documents is considered similar.
2.3

Question Generation

Extractive question answering is a widely studied task in NLP, with the SQuAD dataset [18] being one of its most
known examples. Given an input text and a question, the objective of this task is to answer the question using passages
of the input text. The inverse task of extractive question answering is known as oriented question generation (QG) [19].
Given an answer and a text passage, the objective is to generate a meaningful question. Question generation methods
have been used in teaching assistant softwares [20], to improve academic writing [21] and to create synthetic datasets
for improving question answering systems [22, 23].
As most text generation tasks, the state-of-the-art for this task is based on the transformer architecture [24, 25]. Still,
requiring an answer to generate a question limits the range of applications of such methods. Recent work suggests that
transformers can generate meaningful questions given a text without any answer metadata [26]. A very similar task to
question generation is known as query prediction [27, 28]. In this task, given an input document, the model should
generate a search engine query in which the document is relevant. In this setup, there is also no short answer span, or
3

the whole document is considered as an answer. Search performance can then be improved by expanding the document
with its predicted queries before indexing.

3

Methods

In this section, we present the question generation method, as well as preprocessing and post-processing techniques and
the proposed experiments to evaluate the model.
3.1

Preprocessing and Post-processing

The first step of the preprocessing method consists of removing non-natural language passages using regex (e.g. email
addresses, URLs, DOIs, citation tags and section numbers). We then apply sentence tokenization on every input
document. We remove passages with only one sentence as they usually represent section names (i.e. Introduction).
Before generation, we break documents into sentences and then group sentences into spans of 10 sentences with a stride
of 5. For every span, we generate a single question using doc2query, which is described in Section 3.2.
After the generation, we apply a simple post-processing step that removes publisher names and questions containing the
words “preprint” and “copyright” as these were frequent on the top questions.
3.2

Question generation - doc2query

The generation model consists of a pre-trained language model based on T5-base [29] that has been fine-tuned on the
MS MARCO passage ranking dataset [30]. The dataset contains over 500k queries sampled from the Bing search
engine. Queries are paired with at least one manually annotated relevant passage. The model has originally been used
for document expansion through query generation [28] and is available on Github. 1 We do not apply any gradient
updates to the publicly available model, i.e., we use it as an off-the-shelf model. For each input passage, we generate a
single query using beam search decoding with 4 beams.
3.3

Per-document question accuracy

Here we present a method for capturing the quality and relevance of the generated question for a single document
regardless of other document generations. The data pipeline is presented in Figure 3(a). We use a dataset that has
questions created by humans and their respective documents that contain an answer. For each document, we generate
questions using the model described in Section 3.2. Our goal is to validate for every document if at least one of the
questions generated by the model is the same as the human-created question. We apply a ranking method based on
BERTScore [31] to extract the most similar questions to the reference question. It uses contextual embeddings from
BERT [32] to compute cosine similarity between candidate and reference questions. BERTScore has been shown to
have closer correlation to human evaluations than other metrics such as BLEU and ROUGE. We present the reference
and the top 3 ranked generated questions to a non-expert human evaluator that annotates the extracted questions as
strong, weak, or no match with respect to the reference question.
BERTScore is used as a ranker so the evaluator does not have to search through all questions generated from the
document, which makes the annotation process faster. Two levels of semantic matches are considered to capture the
quality of the generation. A strong match implies that the question generated by the model is semantically the same
question as the reference. A weak match is when the model generates a semantically broader question than the reference.
To be considered a weak match, the response of the reference question should be contained on the generated question.
3.4

Frequent question relevance

This experiment tries to capture if important questions are frequently generated by the model. More specifically, if
among the most frequent questions we can find questions from a human-made question dataset. The data pipeline is
presented in Figure 3(b). We first generate questions for all documents of the dataset. Then we use BERTScore [31] to
measure how similar each question generated by the model is to a reference question. Finally, we present the reference
and top 3 scored questions to a non-expert evaluator that annotates the generated question as strong, weak, or no match.
Pairing is done again with a ranking function based on BERTScore to simplify the evaluation step, reducing human
annotation workload to only 3 generated questions per reference question. The annotation labels are the same as the one
described on the per-document experiment.
1

https://github.com/castorini/docTTTTTquery

4

COVID
QA
What cells does a virus
replicate?
Strong
A virus is a
submicroscopic infectious
agent that replicates only
inside the living cells of
an organism.

Weak

Question
Generator

What is a virus?

BERTScore

Where viruses
replicate?

Ranking

Where viruses
replicate?

Evaluator

No Match

(a)
27 questions

COVID
QA

CORD19

Question
Generator

Top 28k
most frequent
Questions
Datset

BERTScore
Pairing

Strong
Top-3
Matches

Weak
Evaluator

No Match

(b)
Figure 3: Illustration of the per-document question accuracy (a) and the dataset question relevance (b) experiments. The
first experiment captures the relevance of the question generated with respect to an expert reference for every document
individually. The second experiment verifies the relevance of the most frequent generated questions with respect to a
list of questions in a reference dataset.
Reference: How does temperature affect the transmission of COVID-19?
Strong Match: What is the influence of temperature on the transmission and spread of COVID-19?
Weak Match: What is the influence of meteorological factors on the transmission and spread of COVID-19?
No Match: What is the influence of humidity on the transmission and spread of COVID-19?
Figure 4: Examples of weak, strong, and no match evaluations. A strong match represents that the two sentences are
semantically the same. A weak match represents that the generated question is broader than the reference, but within
the same topic.

4

Experiments and Discussion

Here we present the experiments to verify corpus2question as a viable technique for text exploration. In Sections
4.2 and 4.3 we compare the questions generated by the model with questions produced by experts. In section 4.4 we
showcase some examples of frequency and time aggregations over our generated questions, and finally in Section 4.5
we compare the results produced by our model with LDA and word cloud results.
4.1

Datasets

We apply corpus2question on the CORD-19 dataset [5], which is a collection of research papers on COVID-19 and
related diseases. We used the release of August 26th, 2020, which contains 101,688 full documents. These papers are
extracted from several sources and made available in text format. Documents are presented in JSON format, preserving
structures such as paragraph breaks and section headers as separate passages.
We preprocess the dataset to remove articles about similar respiratory diseases that are not COVID-19. The motivation
is to reduce the time spent on query generation, as well as to reduce noise on the dataset. For that we keep only articles
with publication date after October 2019 and that include several covid-related terms such as COVID, SARS-CoV,
SARS-2, Wuhan, and China. The resulting dataset contains 41,526 full papers.
For the quantitative experiments, we used CovidQA [6] as our reference question dataset. CovidQA is bootstrapped
from literature reviews made available on Kaggle over the CORD-19 dataset. Its version 0.2 contains 27 unique
questions whose answers were found in 156 documents.
5

Table 1: Samples of strong and no matches extracted from the frequent question relevance experiment.

4.2

Reference
Generation

Tag

What is the incubation period of the virus?
what is the incubation period of the virus

Strong

What is the proportion of patients who were asymptomatic?
what is the proportion of asymptomatic cases

Strong

Effects of temperature on the transmission of COVID-19
what is the effect of temperature on the spread of covid

Strong

What is the RR for severe infection in COVID-19 patients with hypertension?
what is the incidence of thrombotic complications in patients with covid 19 infection?

None

What is the mortality rate for COVID-19 patients with hypertension?
what is the mortality rate for covid patients

None

What is the HR for death in COVID-19 patients with diabetes?
what is the leading cause of mortality in patients with covid 19?

None

Per-document question accuracy

On the per-document question accuracy experiment described in Section 3.3, we evaluated the 136 documents from the
CovidQA that contains answers to 27 different questions. We found that 67 (47%) of the best-ranked questions matched
the reference questions, with 45 (33%) strong and 22 (16%) weak matches.
We also show that the accuracy drastically varies according to the question. Among the questions with the largest
number of documents, reference question "Incubation period of the virus" found 22 strong matches out of 23. In
contrast "Sample size used in COVID-19 studies" found only 1 strong match among 26 documents. This result suggests
that the model is more inclined to generate some annotated questions more than others.
The lack of a baseline makes it hard to compare the relevance of this result, but the absolute number indicates that the
model does capture at least half of the human-made generations.
4.3

Frequent question relevance

For the question relevance experiment, we first generated questions for all the selected examples of the CORD-19
dataset, with a total of 736,219 generations and 470,749 unique questions. Among those, we filtered those that showed
up on at least 3 different documents, summing a total of 28,362 questions (3,8%). We compare these to the 27 questions
on the CovidQA dataset.
We found 13 (48 %) question matches out of 27 after pairing. With 8 strong and 5 weak matches. We show that the
model was again able to generate almost half of the annotated questions among its most frequent generations.
We also present in Table 1 some strong and no match examples. We see that while strong matches usually occur for
short and simple questions, there are generally no matches for specific and targeted questions.
4.4

Question aggregation methods

In this section, we present further explorations we made over the generated questions that have not been validated
through quantitative experiments. We use this space to showcase some of the exploration possibilities enabled by the
proposed technique with qualitative discussions about the quality of the generations.
4.4.1

Most frequent questions

In Figure 1, we show the top 15 most frequent questions generated from the corpus. We note that frequently generated
questions are concise descriptions of the main theme of the corpus. Some examples are related to the disease name
(COVID-19), the virus (SARS-CoV-2), treatments, the resulting pandemic, and also other related topics such as
symptoms, the outbreak, and social distancing. Among the top 1000 most frequent questions, we show some interesting
6

Table 2: Samples from the 1000 most frequent questions from the CORD-19 dataset.
#

question

counts

177
246
265
322
338
365
413
431
524
537
559
567
593
623
671
692
734
736
748
757
807
898
907
972

what is the long term effect of covid
what is the cytokine storm in covid
what is the most common complication of covid
what organs are affected by sars
what is the threshold for herd immunity
what is the cause of death for covid
what is the most common comorbidity of covid
what is the incidence of covid in children
how long does it take to recover from covid
what is the basic reproduction number of covid
what is the treatment protocol for covid
what is the clinical diagnosis of covid
what is the cutaneous manifestation of covid
what is the epitope of sars-cov-2
what is the role of t cells in sars
what is the role of ace2 in viral entry
What is the effect of temperature of the spread of covid
what is the incidence of aki in covid
how long does it take for sars to show up in blood
what is the sensitivity of a chest ct scan
what is the vertical transmission of sars
what is the innate immune response to sars
what is the mechanism of action of ace2
what is the role of ace2 in lung injury

105
84
79
70
67
63
58
57
49
47
46
45
44
42
39
39
37
37
36
36
34
31
31
29

ones in Table 2. We argue that those questions cannot be immediately perceived by someone less familiar with the
corpus. Hence, corpus2question might help in generating hypothesis or developing underexplored areas of research.
4.4.2

Questions over time

Another interesting property is that we can plot the question frequency over time using the article publication date.
Figure 5 shows the number of questions related to incubation periods, treatments and vaccines over time. To select
those questions we check if the question contain the words “incubation period”, “treatments” or “vaccine”. Since the
beginning of the pandemic, the search for a treatment has been steadily increasing. The interest for incubation periods
has appeared early on 2020, but has not grown substantially since then, suggesting that this topic reached consensus.
We see that the interest for a vaccine is delayed when compared to the interest in incubation periods, but the former has
surpassed the latter in the last couple of months.
4.5

Comparison with LDA and Word Clouds

Here we compare our method with more traditional techniques such as LDA and word cloud. We present results for
topic modelling over CORD-19 in Figure 1 obtained with LDA using tri-grams and 20 topics, with the same pre and
post-processing techniques described on Section 3.1. We manually remove 5 non-sensical topics (e.g “et al et_al”, “la
en el”, and “doi author funder”), which results in a total of 15 topics being presented. We also present a word cloud
using tri-grams.2 LDA and word cloud techniques were able to identify a few buzzwords from the subject, such as
“covid”, “sars”, “health”, and “pandemic”, but they fail to provide more context. For example, someone not familiar
with the theme will probably find difficult to learn anything from the keywords “data model time results using” (topic
13 from Figure 1). However, if presented with the question “what is the sir model”, one could infer that the data refers
to the SIR model for infectious diseases. We argue that this suggests that methods such as corpus2question that
capture local context, can be useful text exploration tools.
2

https://github.com/amueller/word_cloud

7

term
incubation period
treatment
vaccine

80

counts

60

40

20

-09
20
20

-08
20
20

-07
20
20

-06
20
20

-05
20
20

-04
20
20

20
20

-03

0

date

Figure 5: Number of questions related to incubation periods, treatments and vaccines for COVID-19. The horizontal
axis denotes the publication date of the paper that the question was generated from.
4.5.1

Questions associated with LDA

We can use questions as an alternative representation of documents for methods like topic modelling and clustering.
In Table 3 we present the topics obtained by applying LDA to questions generated by corpus2question. In this
experiment, every question is used as a document. We also present the most relevant question for every topic, selected
by calculating the probability of the question belonging to the topic multiplied by the frequency of the question.
While we do not have a metric to compare if the topics generated by this method explain the dataset better than the
ones computed over the original corpus, we can highlight some interesting properties of these results. First, we obtain
a concise representation of a topic in the format of a question (e.g “rate covid ct mechanism rate_covid” can be
represented by “what is the mortality rate of covid”). Second, we substantially increase the variability of the questions
in comparison to using only frequency aggregation presented in Figure 1, while still maintaining relevance within the
topic.

5

Limitations

In this section, we highlight important limitations of the proposed method.
5.1

No definition of irrelevant or redundant questions

In our experiments, we do not evaluate irrelevant, redundant or unrelated questions. One could argue that the probability
of generating matching questions for a document increases with the number of generated questions just by chance. Still,
the most frequent questions presented in our experiments suggest that meaningful questions are more frequent than
noisy ones.
To prevent matching from improving just by having more passages, we chose the largest span window that fits in our
GPU and a single generation per span. The decision for not optimizing hyperparameters was to prevent improving over
the proposed metrics while generating too many irrelevant or redundant questions. A metric that accounts for "bad"
questions would allow a better selection of hyperparameters such as window size, stride, and the number of generations
per window.
8

Table 3: Topic modelling (LDA) applied to questions generated by corpus2question. We present 20 topics and their
respective 5 most relevant n-grams. For every topic, we also present one relevant question selected using the frequency
of the question among all generated questions multiplied by the probability of the question belonging to the topic.

5.2

#

topics

question

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

function pcr virus mers sars
purpose covid prevalence cause sensitivity
treatment vaccine sars cov2 sars_cov2
role health hcq care study
important role viral replication il
normal range normal_range sars wuhan
risk factor risk_factor remdesivir icu
covid age average use ppe
pandemic period impact sars incubation
rate covid ct mechanism rate_covid
does long long_does coronavirus mortality
covid 19 covid_19 did pandemic
scale temperature rna relationship social
sars does long test long_does
used protein spike number coronavirus
covid clinical epidemic pandemic important
difference network vitamin target rbd
model purpose covid crisis disease
definition effect necessary air pollution
sars cov sars_cov role ace2

what is rt pcr
what is the prevalence of covid
what is the treatment for covid
what is the clinical manifestation of covid
what is il-6
what is the most common symptom of covid
what is the risk factor for covid
why do people wear masks
what is the incubation period for covid
what is the mortality rate of covid
what is social distancing
what is covid 19
what is the genotype of covid
how long does it take for sars to spread
what is the basic reproduction number
what is the clinical course of covid
what is vitamin d deficiency
what is the sir model
what is the definition of covid
what is sars cov

Long tail of infrequent questions

From the 720,545 generated questions, 391,910 (54%) have a frequency of 1, that is, they are generated from only
one passage. This statistic shows that while frequent questions are suitable for direct aggregation there is still a large
number of infrequent questions. We do not have a clear explanation of how this relates to the question generation model
or the source documents. Using better aggregation techniques or highlighting representative questions of a group of
infrequent questions may be an important future work direction.
5.3

Influence of the fine-tuning dataset

We do not know to what extend the dataset used to fine-tune the question generation model might have influenced the
results as we use on all experiments an off-the-shelf model fine-tuned on the MS MARCO dataset. We considered using
the same generation model but fine-tuned on the SQuAD 2.0 dataset [33]. Preliminary experiments showed that this
model generated long and factual questions, not suitable for aggregation. To illustrate the effect, in Table 4, we present
some sample questions extracted from each dataset.
It seems that the quality of the fine-tuning dataset affects the results, but we do not investigate this further. We suspect
that a task-specific dataset based on literature reviews of scholarly papers might be highly beneficial for this task.
Table 4: Questions from the SQuAD 2.0 and the MS MARCO datasets. Questions from MS MARCO are shorter and
broader than the ones from SQuAD.
Dataset
SQuAD 2.0
MS MARCO

Examples
How did the black death make it to the Mediterranean and Europe?
What word is the word pharmacy taken from?
How would the word apothecary be viewed by contemporary English speakers?
what makes your hands burn
what is medicare summary notice
best anti inflamatory after surgery

9

5.4

Influence of the model size

All experiments presented are done with a small model compared to state-of-the-art models for other tasks. Recent
work has demonstrated that size matters on text generation tasks [29, 34]. Further investigating the influence of models
sizes and architectures may also be an important future step to improve the quality of generations.

6

Conclusions

We introduced corpus2question, a method that uses question generation models followed by frequency aggregations
to summarize corpora. We demonstrate that this method applied to the CORD-19 dataset yields several relevant questions
about COVID-19. We show that corpus2question is able to capture contextual information about documents when
compared to other exploration methods such as word cloud and topic modelling. We also propose two quantitative
experiments to compare the questions to human-generated questions that may be used to evaluate future research on the
subject. Finally, we conclude our work by presenting its limitations and opportunities for future work in this field. Due
to the technical nature of the CORD-19 dataset, we note that further evaluating the relevance and accuracy of frequent
questions generated by the model may require assistance from experts with medical and biomedical backgrounds.

References
[1] Weiwei Cui, Yingcai Wu, Shixia Liu, Furu Wei, Michelle X Zhou, and Huamin Qu. Context preserving dynamic
word cloud visualization. In 2010 IEEE Pacific Visualization Symposium (PacificVis), pages 121–128. IEEE, 2010.
[2] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning
research, 3(Jan):993–1022, 2003.
[3] Jiawei Han, Jian Pei, and Micheline Kamber. Data mining: concepts and techniques. Elsevier, 2011.
[4] Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In International conference
on machine learning, pages 1188–1196, 2014.
[5] Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn Funk,
Rodney Kinney, Ziyang Liu, William Merrill, et al. Cord-19: The covid-19 open research dataset. ArXiv, 2020.
[6] Raphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil Gupta, Phuong Cam, Kyunghyun Cho, and Jimmy Lin.
Rapidly bootstrapping a question answering dataset for covid-19, 2020.
[7] Rajkumar Arun, Venkatasubramaniyan Suresh, CE Veni Madhavan, and MN Narasimha Murthy. On finding
the natural number of topics with latent dirichlet allocation: Some observations. In Pacific-Asia conference on
knowledge discovery and data mining, pages 391–402. Springer, 2010.
[8] X. Wang, A. McCallum, and X. Wei. Topical n-grams: Phrase and topic discovery, with an application to
information retrieval. In Seventh IEEE International Conference on Data Mining (ICDM 2007), pages 697–702,
2007.
[9] Christopher E Moody. Mixing dirichlet topic models and word embeddings to make lda2vec, 2016.
[10] Donghwa Kim, Deokseong Seo, Suhyoun Cho, and Pilsung Kang. Multi-co-training for document classification
using various document representations: Tf–idf, lda, and doc2vec. Information Sciences, 477:15–29, 2019.
[11] Jordan Boyd-Graber, David Mimno, and David Newman. Care and Feeding of Topic Models: Problems,
Diagnostics, and Improvements. CRC Handbooks of Modern Statistical Methods. CRC Press, Boca Raton, Florida,
2014.
[12] Philip Resnik, Katherine E Goodman, and Mike Moran. Developing a curated topic model for covid-19 medical
research literature. 2020.
[13] Karin Verspoor, Simon Šuster, Yulia Otmakhova, Shevon Mendis, Zenan Zhai, Biaoyan Fang, Jey Han Lau,
Timothy Baldwin, Antonio Jimeno Yepes, and David Martinez. Covid-see: Scientific evidence explorer for
covid-19 related research. arXiv preprint arXiv:2008.07880, 2020.
[14] Akiko Aizawa. An information-theoretic perspective of tf–idf measures. Information Processing & Management,
39(1):45–65, 2003.
[15] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv
preprint arXiv:1908.10084, 2019.
[16] Prafulla Bafna, Dhanya Pramod, and Anagha Vaidya. Document clustering: Tf-idf approach. In 2016 International
Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT), pages 61–66. IEEE, 2016.
10

[17] Jey Han Lau and Timothy Baldwin. An empirical evaluation of doc2vec with practical insights into document
embedding generation. arXiv preprint arXiv:1607.05368, 2016.
[18] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine
comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
[19] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading comprehension.
arXiv preprint arXiv:1705.00106, 2017.
[20] Ghader Kurdi, Jared Leo, Bijan Parsia, and Salam Al-Emari. A systematic review of automatic question generation
for educational purposes. International Journal of Artificial Intelligence in Education, 11 2019.
[21] Ming Liu, Rafael A Calvo, and Vasile Rus. G-asks: An intelligent automatic question generation system for
academic writing support. Dialogue & Discourse, 3(2):101–124, 2012.
[22] Tassilo Klein and Moin Nabi. Learning to answer by learning to ask: Getting the best of gpt-2 and bert worlds,
2019.
[23] Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. Synthetic qa corpora generation
with roundtrip consistency. In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics, pages 6168–6173, 2019.
[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages
5998–6008, 2017.
[25] Kettip Kriangchaivech and Artit Wangperawong. Question generation by transformers, 2019.
[26] Luis Enrico Lopez, Diane Kathryn Cruz, Jan Christian Blaise Cruz, and Charibeth Cheng. Transformer-based
end-to-end question generation, 2020.
[27] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction. arXiv
preprint arXiv:1904.08375, 2019.
[28] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. From doc2query to doctttttquery. Online preprint, 2019.
[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint
arXiv:1910.10683, 2019.
[30] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew
McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang.
MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268v3, 2018.
[31] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text
generation with bert, 2019.
[32] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[33] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for squad.
arXiv preprint arXiv:1806.03822, 2018.
[34] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam
McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.

11

