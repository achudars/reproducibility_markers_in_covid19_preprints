N EURAL VOLUME R ENDERING : N E RF AND B EYOND

arXiv:2101.05204v2 [cs.CV] 14 Jan 2021

Frank Dellaert
Georgia Institute of Technology
dellaert@cc.gatech.edu

1

Lin Yen-Chen
Massachusetts Institute of Technology
yenchenl@mit.edu

I NTRODUCTION

Besides the COVID-19 pandemic and political upheaval in the US, 2020 was also the year in
which neural volume rendering exploded onto the scene, triggered by the impressive NeRF paper by Mildenhall et al. (2020). Both of us have tried to capture this excitement, Frank on a
blog post (Dellaert, 2020) and Yen-Chen in a Github collection (Yen-Chen, 2020). This note
is an annotated bibliography of the relevant papers, and we posted the associated bibtex file
on the repository.
To start with some definitions, the larger field of Neural Rendering is defined by the excellent
review paper by Tewari et al. (2020) as
“deep image or video generation approaches that enable explicit or implicit control of scene properties such as illumination, camera parameters, pose, geometry,
appearance, and semantic structure.”
It is a novel, data-driven solution to the long-standing problem in computer graphics of the realistic
rendering of virtual worlds.
Neural volume rendering refers to methods that generate images or video by tracing a ray into the
scene and taking an integral of some sort over the length of the ray. Typically a neural network like
a multi-layer perceptron encodes a function from the 3D coordinates on the ray to quantities like
density and color, which are integrated to yield an image.
Outline Below we first discuss some very relevant related works that lead up to the “NeRF explosion”, then discuss the two papers that we think started it all, followed by an annotated bibliography
on follow-up work. We are going wide rather than deep, and provide links to all project sites or
Arxiv entries.

2

T HE P RELUDE : N EURAL I MPLICIT S URFACES

The immediate precursors to neural volume rendering are the approaches that use a neural network
to define an implicit surface representation. Many 3D-aware image generation approaches used voxels, meshes, point clouds, or other representations, typically based on convolutional architectures.
But at CVPR 2019, no less than three papers introduced the use of neural nets as scalar function
approximators to define occupancy and/or signed distance functions.
2.1

O CCUPANCY

AND

S IGNED D ISTANCE F UNCTIONS

Below are the three papers from CVPR 2019, and one (PIFu) from ICCV 2019:
• Occupancy networks (Mescheder et al., 2019) introduce implicit, coordinate-based learning of occupancy. A network consisting of 5 ResNet blocks take a feature vector and a 3D
point and predict binary occupancy.
• IM-NET (Chen and Zhang, 2019) uses a 6-layer MLP decoder that predicts binary occupancy given a feature vector and a 3D coordinate. Can be used for auto-encoding, shape
generation (GAN-style), and single-view reconstruction.
1

• DeepSDF (Park et al., 2019) directly regresses a signed distance function from a 3D coordinate and optionally a latent code. It uses an 8-layer MPL with skip-connections to layer
4, setting a trend!
• PIFu (Saito et al., 2019) shows that it is possible to learn highly detailed implicit models
by re-projecting 3D points into a pixel-aligned feature representation. This idea will later
be reprised, with great effect, in PixelNeRF.
2.2

B UILDING

ON I MPLICIT

F UNCTIONS

Several other approaches build on top of the implicit function idea.
• Structured Implicit Functions (Genova et al., 2019) show that you can combine these implicit representations, e.g., simply by summing them.
• CvxNet (Deng et al., 2020b) combines signed distance functions by taking a pointwise max
(in 3D). The paper also has several other elegant techniques to reconstruct an object from
depth or RGB images.
• BSP-Net (Chen et al., 2020) is in many ways similar to CvxNet, but uses binary space
partitioning at its core, yielding a method that outputs polygonal meshes natively, rather
than via an expensive meshing method.
• Deep Local Shapes (Chabra et al., 2020) store a DeepSDF latent code in a voxel grid to
represent larger, extended scenes.
• Scene Representation Networks (Sitzmann et al., 2019) or SRN are quite similar to
DeepSDF in terms of architecture but adds a differentiable ray marching algorithm to find
the closest point of intersection of a learned implicit surface, and add an MLP to regress
color, enabling it to be learned from multiple posed images.
• Differentiable Volumetric Rendering (Niemeyer et al., 2019) shows that an implicit scene
representation can be coupled with a differentiable renderer, making it trainable from images, similar to SRN. They use the term volumetric renderer, but really the main contribution is a clever trick to make the computation of depth to the implicit surface differentiable:
no integration over a volume is used.
• Implicit Differentiable Renderer (Yariv et al., 2020) presents a similar technique, but has
a more sophisticated surface light field representation, and also shows that it can refine
camera pose during training.
• Neural Articulated Shape Approximation (Deng et al., 2020c) or NASA composes implicit
functions to represent articulated objects such as human bodies.

3

N EURAL VOLUME R ENDERING

As far as we know, two papers introduced volume rendering into the field, with NeRF being the
simplest and ultimately the most influential.
A word about naming: the two papers below and all Nerf-style papers since build upon the work
above that encode implicit surfaces, and so the term implicit neural methods is used quite a bit.
However, especially in graphics that term is more associated with level-set representations for curves
and surfaces. What they do have in common with occupancy/SDF-style networks is that MLP’s are
used as functions from coordinates in 3D to a scalar or multi-variate fields, and hence these methods
are also sometimes called coordinate-based scene representation networks. Of that larger set, we’re
concerned with volume rendering versions of those below.
3.1

N EURAL VOLUMES

While not entirely in a vacuum, we believe volume rendering for view synthesis was introduced in
the Neural Volumes paper by Lombardi et al. (2019), regressing a 3D volume of density and color,
albeit still in a (warped) voxel-based representation. A latent code is decoded into a 3D volume, and
a new image is then obtained by volume rendering.
2

One of the most interesting quotes from this paper hypothesizes about the success of neural volume
rendering approaches (emphasis is ours):
[We] propose using a volumetric representation consisting of opacity and color at
each position in 3D space, where rendering is realized through integral projection.
During optimization, this semi-transparent representation of geometry disperses
gradient information along the ray of integration, effectively widening the basin
of convergence, enabling the discovery of good solutions.
We think that resonates with many people, and partially explains the success of neural volume
rendering. We won’t go into any detail about the method itself, but the paper is a great read. Instead,
let’s dive right into NeRF itself below. . .
3.2

N E RF

The paper that got everyone talking was the Neural Radiance Fields or NeRF paper by
Mildenhall et al. (2020). In essence, they take the DeepSDF architecture but regress not a signed distance function, but density and color. They then use an (easily differentiable) numerical integration
method to approximate a true volumetric rendering step.
A NeRF model stores a volumetric scene representation as the weights of an MLP, trained on many
images with known pose. New views are rendered by integrating the density and color at regular
intervals along each viewing ray.
One of the reasons NeRF is able to render with great detail is because it encodes a 3D point and
associated view direction on a ray using periodic activation functions, i.e., Fourier Features. This
innovation was later generalized to multi-layer networks with periodic activations, aka SIREN (SInusoidal REpresentation Networks). Both were published later at NeurIPS 2020.
While the NeRF paper was ostensibly published at ECCV 2020, at the end of August, it first appeared
on Arxiv in the middle of March, sparking an explosion of interest, not only because of the quality
of the synthesized views, but perhaps even more so at the incredible detail in the visualized depth
maps.
Arguably, the impact of the NeRF paper lies in its brutal simplicity: just an MLP taking in a 5D
coordinate and outputting density and color. There are some bells and whistles, notably positional
encoding and a stratified sampling scheme, but many researchers were taken aback (we think) that
such a simple architecture could yield such impressive results. That being said, vanilla NeRF left
many opportunities to improve upon:
• It is slow, both for training and rendering.
• It can only represent static scenes.
• It “bakes in” lighting.
• A trained NeRF representation does not generalize to other scenes/objects.
In this Arxiv-fueled computer vision world, these opportunities were almost immediately capitalized
on, with almost 25 papers appearing on Arxiv in the span of six months. Below we list all of them
we could find.

4

P ERFORMANCE

Several projects/papers aim at improving the rather slow training and rendering time of the original
NeRF paper.
• JaxNeRF (Deng et al., 2020a) uses JAX (https://github.com/google/jax) to dramatically
speed up training using multiple devices, from days to hours.
• AutoInt (Lindell et al., 2020) greatly speeds up rendering by learning the volume integral
directly.
3

• Learned Initializations (Tancik et al., 2020) uses meta-learning to find a good weight initialization for faster training.
• DeRF (Rebain et al., 2020) decomposes the scene into ”soft Voronoi diagrams” to take
advantage of accelerator memory architectures.
• NERF++ (Zhang et al., 2020) proposes to model the background with a separate NeRF to
handle unbounded scenes.
• Neural Sparse Voxel Fields (Liu et al., 2020) organize the scene into a sparse voxel octree
to speed up rendering by a factor of 10.

5

DYNAMIC

At least four efforts focus on dynamic scenes, using a variety of schemes.
• Nerfies (Park et al., 2020) and its underlying D-NeRF model deformable videos using a
second MLP applying a deformation for each frame of the video.
• D-NeRF (Pumarola et al., 2020) is quite similar to the Nerfies paper and even uses the same
acronym, but seems to limit deformations to translations.
• Neural Scene Flow Fields (Li et al., 2020) take a monocular video with known camera
poses as input but use depth predictions as a prior, and regularize by also outputting scene
flow, used in the loss.
• Space-Time Neural Irradiance Fields (Xian et al., 2020) simply use time as an additional
input. Carefully selected losses are needed to successfully train this method to render freeviewpoint videos (from RGBD data!).
• NeRFlow (Du et al., 2020) uses a deformation MLP to model scene flow and integrates it
across time to obtain the final deformation.
• NR-NeRF (Tretschk et al., 2020) also uses a deformation MLP to model non-rigid scenes.
It has no reliance on pre-computed scene information apart from camera parameters but
generates slightly less sharp outputs compared to Nerfies.
• STaR (Yuan et al., 2021) takes multi-view RGB videos as input and decomposes the scene
into a static and a dynamic volume. However, currently it only supports one object in
motion.
Besides Nerfies, two other papers focus on avatars/portraits of people.
• Portrait NeRF (Gao et al., 2020) creates static NeRF-style avatars but does so from a single
RGB headshot. To make this work, light-stage training data is required.
• DNRF (Gafni et al., 2020) focuses on 4D avatars and hence impose a strong inductive bias
by including a deformable face model into the pipeline.

6

R ELIGHTING

Another dimension in which NeRF-style methods have been augmented is in how to deal with
lighting, typically through latent codes that can be used to re-light a scene.
• NeRV (Srinivasan et al., 2020) uses a second ”visibility” MLP to support arbitrary environment lighting and ”one-bounce” indirect illumination.
• NeRD (Boss et al., 2020) or “Neural Reflectance Decomposition” is another effort in which
a local reflectance model is used, and additionally, a low-res spherical harmonics illumination is removed for a given scene.
• Neural Reflectance Fields (Bi et al., 2020) improve on NeRF by adding a local reflection
model in addition to density. It yields impressive relighting results, albeit from single point
light sources.
• NeRF-W (Martin-Brualla et al., 2020) is one of the first follow-up works on NeRF, and
optimizes a latent appearance code to enable learning a neural scene representation from
less controlled multi-view collections.
4

7

S HAPE

Latent codes can also be used to encode shape priors.
• pixelNeRF (Yu et al., 2020) is closer to image-based rendering, where N images are used
at test time. It is based on PIFu, creating pixel-aligned features that are then interpolated
when evaluating a NeRF-style renderer.
• GRF Trevithick and Yang (2020) is pretty close to pixelNeRF in setup but operates in a
canonical space rather than in view space.
• GRAF (Schwarz et al., 2020) i.e., a “Generative model for Radiance Fields” is a conditional variant of NeRF, adding both appearance and shape latent codes, while viewpoint
invariance is obtained through GAN-style training.
• pi-GAN (Chan et al., 2020) is similar to GRAF but uses a SIREN-style implementation of
NeRF, where each layer is modulated by the output of a different MLP that takes in a latent
code.

8

C OMPOSITION

It could be argued that none of this will scale to large scenes composed of many objects, so an
exciting new area of interest is how to compose objects into volume-rendered scenes.
• Object-Centric Neural Scene Rendering (Guo et al., 2020) learns ”Object Scattering Functions” in object-centric coordinate frames, allowing for composing scenes and realistically
lighting them, using Monte Carlo rendering.
• GIRAFFE (Niemeyer and Geiger, 2020) support composition by having object-centric
NeRF models output feature vectors rather than color, then compose via averaging, and
render at low resolution to 2D feature maps that are then upsampled in 2D.
• Neural Scene Graphs (Ost et al., 2020) supports several object-centric NeRF models in a
scene graph.

9

P OSE E STIMATION

Finally, at least one paper has used NeRF rendering in the context of (known) object pose estimation.
• iNeRF (Yen-Chen et al., 2020) uses a NeRF MLP in a pose estimation framework and is
even able to improve view synthesis on standard datasets by fine-tuning the poses. However, it does not yet handle illumination.

10

C ONCLUDING T HOUGHTS

Neural Volume Rendering and NeRF-style papers have exploded on the scene in 2020, and the last
word has not been said. This note definitely does not rise to the level of a thorough review, but we
hope that an annotated bibliography is useful for people working in this area or thinking of joining
the fray.
However, it is far from clear -even in the face of all this excitement- that neural volume rendering
is going to carry the day in the end. While the real world does have haze, smoke, transparencies,
etc., in the end, most of the light is scattered into our eyes from surfaces. NeRF-style networks
might be easily trainable because of their volume-based approach, but we already see a trend where
authors are trying to discover or guess the surfaces after convergence. In fact, the stratified sampling
scheme in the original NeRF paper is exactly that. Hence, as we learn from the NeRF explosion we
can easily see the field moving back to SDF-style implicit representations or even voxels, at least at
inference time.
5

R EFERENCES
Bi, S., Xu, Z., Srinivasan, P., Mildenhall, B., Sulkavalli, K., Hašan, M., Hold-Geoffroy, Y., Kriegman, D., and Ramamoorthi, R. (2020). Neural reflectance fields for appearance acquisition.
https://arxiv.org/abs/2008.03824.
Boss, M., Braun, R., Jampani, V., Barron, J. T., Liu, C., and Lensch, H. (2020). NeRD: Neural
reflectance decomposition from image collections. https://arxiv.org/abs/2012.03918.
Chabra, R., Lenssen, J., Ilg, E., Schmidt, T., Straub, J., Lovegrove, S., and Newcombe, R. (2020).
Deep local shapes: Learning local SDF priors for detailed 3D reconstruction. In The European
Conference on Computer Vision (ECCV).
Chan, E., Monteiro, M., Kellnhofer, P., Wu, J., and Wetzstein, G. (2020). pi-GAN: Periodic implicit
generative adversarial networks for 3D-aware image synthesis. https://arxiv.org/abs/2012.00926.
Chen, Z., Tagliasacchi, A., and Zhang, H. (2020). BSP-Net: Generating compact meshes via binary
space partitioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 45–54.
Chen, Z. and Zhang, H. (2019). Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
5939–5948.
Dellaert, F. (2020). NeRF Explosion 2020. https://dellaert.github.io/NeRF/.
Deng, B., Barron, J. T., and Srinivasan, P. (2020a). JaxNeRF: an efficient JAX implementation of
NeRF. https://github.com/google-research/google-research/tree/master/jaxnerf.
Deng, B., Genova, K., Yazdani, S., Bouaziz, S., Hinton, G., and Tagliasacchi, A. (2020b). CvxNet:
Learnable convex decomposition. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 31–44.
Deng, B., Lewis, J., Jeruzalski, T., Pons-Moll, G., Hinton, G., Norouzi, M., and Tagliasacchi, A.
(2020c). Neural articulated shape approximation. In The European Conference on Computer
Vision (ECCV). Springer.
Du, Y., Zhang, Y., Yu, H.-X., Tenenbaum, J. B., and Wu, J. (2020). Neural radiance flow for 4D
view synthesis and video processing. arXiv preprint arXiv:2012.09790.
Gafni, G., Thies, J., Zollhöfer, M., and Nießner, M. (2020). Dynamic neural radiance fields for
monocular 4D facial avatar reconstruction. https://arxiv.org/abs/2012.03065.
Gao, C., Shih, Y., Lai, W.-S., Liang, C.-K., and Huang, J.-B. (2020). Portrait neural radiance fields
from a single image. https://arxiv.org/abs/2012.05903.
Genova, K., Cole, F., Vlasic, D., Sarna, A., Freeman, W., and Funkhouser, T. (2019). Learning shape
templates with structured implicit functions. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pages 7154–7164.
Guo, M., Fathi, A., Wu, J., and Funkhouser, T. (2020). Object-centric neural scene rendering.
https://arxiv.org/abs/2012.08503.
Li, Z., Niklaus, S., Snavely, N., and Wang, O. (2020). Neural scene flow fields for space-time view
synthesis of dynamic scenes. https://arxiv.org/abs/2011.13084.
Lindell, D., Martel, J., and Wetzstein, G. (2020). AutoInt: Automatic integration for fast neural
volume rendering. https://arxiv.org/abs/2012.01714.
Liu, L., Gu, J., Lin, K. Z., Chua, T.-S., and Theobalt, C. (2020). Neural sparse voxel fields. In
Advances in Neural Information Processing Systems (NeurIPS), volume 33.
Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., and Sheikh, Y. (2019). Neural
volumes: Learning dynamic renderable volumes from images. ACM Trans. Graph.
6

Martin-Brualla, R., Radwan, N., Sajjadi, M., Barron, J. T., Dosovitskiy, A., and Duckworth,
D. (2020). NeRF in the wild: Neural radiance fields for unconstrained photo collections.
https://arxiv.org/abs/2008.02268.
Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., and Geiger, A. (2019). Occupancy Networks: Learning 3D reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. (2020).
NeRF: Representing scenes as neural radiance fields for view synthesis. In The European Conference on Computer Vision (ECCV).
Niemeyer, M. and Geiger, A. (2020). GIRAFFE: Representing scenes as compositional generative
neural feature fields. https://arxiv.org/abs/2011.12100.
Niemeyer, M., Mescheder, L., Oechsle, M., and Geiger, A. (2019). Differentiable volumetric rendering: Learning implicit 3D representations without 3D supervision. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
Ost, J., Mannan, F., Thürey, N., Knodt, J., and Heide, F. (2020). Neural scene graphs for dynamic
scenes. https://arxiv.org/abs/2011.10379.
Park, J. J., Florence, P., Straub, J., Newcombe, R., and Lovegrove, S. (2019). DeepSDF: Learning
continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pages 165–174.
Park, K., Sinha, U., Barron, J. T., Bouaziz, S., Goldman, D., Seitz, S., and Martin-Brualla, R. (2020).
Deformable neural radiance fields. https://arxiv.org/abs/2011.12948.
Pumarola, A., Corona, E., Pons-Moll, G., and Moreno-Noguer, F. (2020). D-NeRF: Neural radiance
fields for dynamic scenes. https://arxiv.org/abs/2011.13961.
Rebain, D., Jiang, W., Yazdani, S., Li, K., Yi, K. M., and Tagliasacchi, A. (2020). DeRF: Decomposed radiance fields. https://arxiv.org/abs/2011.12490.
Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., and Li, H. (2019). PIFu: Pixelaligned implicit function for high-resolution clothed human digitization. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV).
Schwarz, K., Liao, Y., Niemeyer, M., and Geiger, A. (2020). Graf: Generative radiance fields for
3D-aware image synthesis. In Advances in Neural Information Processing Systems (NeurIPS),
volume 33.
Sitzmann, V., Zollhöfer, M., and Wetzstein, G. (2019). Scene representation networks: Continuous
3D-structure-aware neural scene representations. In Advances in Neural Information Processing
Systems (NeurIPS), pages 1121–1132.
Srinivasan, P., Deng, B., Zhang, X., Tancik, M., Mildenhall, B., and Barron, J. T.
(2020). NeRV: Neural reflectance and visibility fields for relighting and view synthesis.
https://arxiv.org/abs/2012.03927.
Tancik, M., Mildenhall, B., Wang, T., Schmidt, D., Srinivasan, P., Barron, J. T., and Ng,
R. (2020). Learned initializations for optimizing coordinate-based neural representations.
https://arxiv.org/abs/2012.02189.
Tewari, A., Fried, O., Thies, J., Sitzmann, V., Lombardi, S., Sunkavalli, K., Martin-Brualla, R.,
Simon, T., Saragih, J., Nießner, M., Pandey, R., Fanello, S., Wetzstein, G., Zhu, J.-Y., Theobalt,
C., Agrawala, M., Shechtman, E., Goldman, D. B., and Zollhöfer, M. (2020). State of the Art on
Neural Rendering. Computer Graphics Forum (EG STAR 2020).
Tretschk, E., Tewari, A., Golyanik, V., Zollhöfer, M., Lassner, C., and Theobalt, C. (2020). Nonrigid neural radiance fields: Reconstruction and novel view synthesis of a deforming scene from
monocular video. https://arxiv.org/abs/2012.12247.
7

Trevithick, A. and Yang, B. (2020). GRF: Learning a general radiance field for 3D scene representation and rendering. https://arxiv.org/abs/2010.04595.
Xian, W., Huang, J.-B., Kopf, J., and Kim, C. (2020). Space-time neural irradiance fields for freeviewpoint video. https://arxiv.org/abs/2011.12950.
Yariv, L., Kasten, Y., Moran, D., Galun, M., Atzmon, M., Basri, R., and Lipman, Y. (2020). Multiview neural surface reconstruction by disentangling geometry and appearance. In Advances in
Neural Information Processing Systems (NeurIPS).
Yen-Chen, L. (2020). Awesome neural radiance fields. https://github.com/yenchenlin/awesomeNeRF.
Yen-Chen, L., Florence, P., Barron, J. T., Rodriguez, A., Isola, P., and Lin, T.-Y. (2020). iNeRF:
Inverting neural radiance fields for pose estimation. https://arxiv.org/abs/2012.05877.
Yu, A., Ye, V., Tancik, M., and Kanazawa, A. (2020). pixelNeRF: Neural radiance fields from one
or few images. https://arxiv.org/abs/2012.02190.
Yuan, W., Lv, Z., Schmidt, T., and Lovegrove, S. (2021). STaR: Self-supervised tracking and reconstruction of rigid objects in motion with neural rendering. arXiv preprint arXiv:2101.01602.
Zhang, K., Riegler, G., Snavely, N., and Koltun, V. (2020). NERF++: Analyzing and improving
neural radiance fields. https://arxiv.org/abs/2010.07492.

8

