arXiv:2005.06709v1 [stat.ME] 14 May 2020

Inference for a test-negative case-control study with
added controls
Bikram Karmakar

Dylan S Small

University of Florida

University of Pennsylvania

May 15, 2020

Running Head: Test-negative case-control study with added controls.
Keywords: Case-control studies; closed testing; multi-step procedure; test-negative designs.
Conflict of interest statement: There is no conflict of interest.
Source of funding: None.
Data and code availability: We provide computing code in the supplementary materials.

Address for Correspondence: Bikram Karmakar, Department of Statistics, University of Florida, 226 Griffin
Floyd Hall, Gainesville, FL 32611 (E-mail: bkarmakar@ufl.edu).
∗

Abstract
Test-negative designs with added controls have recently been proposed to study COVID19. An individual is test-positive or test-negative accordingly if they took a test for a disease but tested positive or tested negative. Adding a control group to a comparison of testpositives vs test-negatives is useful since additional comparison of test-positives vs controls
can have potential biases different from the first comparison. Bonferroni correction ensures
necessary type-I error control for these two comparisons done simultaneously. We propose two new methods for inference which have better interpretability and higher statistical
power for these designs. These methods add a third comparison that is essentially independent of the first comparison, but our proposed second method often pays much less for these
three comparisons than what a Bonferroni correction would pay for the two comparisons.

keywords: Case-control studies; Closed testing; Confidence intervals; Potential biases; Second
control group.
Test-negative studies compare exposures in cases who take a test for a particular disease and
test positive vs. controls who also take the test but test negative. 5;7 A test-negative study with
added controls (TNSWAC) supplements with controls who did not take the test. TSNWACs
have been used to study antibiotic resistance 4 and proposed to study COVID-19. 8 The standard
inference approach has been to present two exposure rate comparisons,
(i) test-positives to test-negatives
(ii) test-positives to controls
To control the familywise Type I error rate for multiple comparisons at level α (e.g., α = 0.05),
the Bonferroni inequality can be used and each comparison done at level α/2. Here we propose
different inference strategies that can provide greater interpretability and power.
A valuable feature of TSNWACs is that comparisons (i) and (ii) may have different potential
biases. 8 Evidence is strengthened when diverse approaches with diverse potential biases produce similar results. 6;3 However, comparisons (i) and (ii) are dependent – see Figure 1A – and
1

might tend to agree just because of this dependence. It is important to distinguish new evidence
from the same evidence repeated twice. 3 To this end, it is useful to supplement comparisons
(i)-(ii) with comparison (iii) test-positives pooled with test-negatives to controls, which is essentially independent of (i) – see Figure 1B and supplement – and may suffer from different
potential biases than (i). 1 For example, it has been hypothesized that smoking protects against
Covid-19. 2 Comparison (i) may be biased because test-negatives may have some other infection (e.g., the flu) for which smoking increases risk and comparisons (ii) and (iii) may be biased
because test-takers tend to be “health seeking.” 7 Finding evidence of smoking being protective
in all comparisons (i)-(iii) would strengthen evidence compared to just comparisons (i)-(ii) in
part because the latter comparisons are dependent.
The following are two procedures that consider comparisons (i)-(iii) and control the familywise error rate for multiple comparisons at α (proof/code in supplement). The first procedure is
(1) test the null hypothesis of no exposure effect in comparison (i), H0(i) , at level α/2 (i.e., reject
if p-value ≤ α2 ) and test the null of no exposure effect in comparison (ii), H0(ii) , at level α/2
and (2) if and only if both nulls are rejected, test the null of no exposure effect in comparison
(iii), H0(iii) , at level α. The second procedure is
(1) Test H0(ii) at level α/2. If H0(ii) is rejected, set λ = α; otherwise, λ = α/2.
(2) Test the null of no exposure effect in either comparison (i) and/or comparison (iii), H0(i) ∪
H0(iii) , at level λ. This could be done by Fisher’s combination method since comparisons
(i) and (iii) are essentially independent under the null. 1 If the null H0(i) ∪ H0(iii) is not
rejected, stop testing.
(3) If H0(i) ∪ H0(iii) was rejected in (2), then test H0(i) and H0(iii) each at level λ.
(4) If λ = α/2 and both H0(i) and H0(iii) were rejected in (3), then test H0(ii) at level α and
reject if p-value ≤ α.
For example, suppose the p-values for H0(i) , H0(ii) and H0(iii) were .04, .03 and .04 respec2

tively, then the standard procedure would not reject any null hypotheses whereas the second
procedure would reject all nulls (note: p-value for H0(i) ∪ H0(iii) using Fisher’s combination
test is .012). Confidence intervals for magnitudes of effect can be formed using both procedures, see supplement. Figure 1C compares the power of the two proposed procedures and the
standard procedure in a simulation. Both proposed procedures increase power over the standard
procedure in the simulated setting with the second procedure providing more power.

References
[1] Bikram Karmakar, Chyke A Doubeni, and Dylan S Small. Evidence factors in a casecontrol study with application to the effect of flexible sigmoidoscopy screening on colorectal cancer. Annals of Applied Statistics, forthcoming, 2020.
[2] Makoto Miyara, Florence Tubach, and Zahir Amoura. Low incidence of daily active tobacco smoking in patients with symptomatic covid-19 infection. Preprint, 04 2020. doi:
10.32388/WPP19W.
[3] Paul R Rosenbaum. Evidence factors in observational studies. Biometrika, 97(2):333–345,
2010.
[4] Mette Søgaard, Uffe Heide-Jørgensen, Jan P Vandenbroucke, Henrik C Schønheyder, and
CMJE Vandenbroucke-Grauls. Risk factors for extended-spectrum β-lactamase-producing
escherichia coli urinary tract infection in the community in denmark: a case–control study.
Clinical Microbiology and Infection, 23(12):952–960, 2017.
[5] Sheena G Sullivan, Eric J Tchetgen Tchetgen, and Benjamin J Cowling. Theoretical basis
of the test-negative study design for assessment of influenza vaccine effectiveness, 2016.
[6] Mervyn Susser. Causal thinking in the health sciences: concepts and strategies of epidemi-

3

ology. In Causal thinking in the health sciences: concepts and strategies of epidemiology.
1973.
[7] Jan P Vandenbroucke and Neil Pearce. Test-negative designs: Differences and commonalities with other case–control studies with other patient controls. Epidemiology, 30(6):
838–844, 2019.
[8] Jan P Vandenbroucke, Elizabeth B Brickley, Christina MJE Vandenbroucke-Grauls, and
Neil Pearce. Analysis proposals for test-negative design and matched case-control studies during widespread testing of symptomatic persons for sars-cov-2.
arXiv:2004.06033, 2020.

4

arXiv preprint

B
1.8

1.8

1.6

1.6
Odds ratio for (iii)

Odds ratio for (ii)

A

1.4
1.2
1.0
0.8

1.4
1.2
1.0
0.8

0.6

0.6

0.4

0.4
0.5

1.0

1.5

2.0

0.5

1.0

Odds ratio for (i)
1.0

1.0

0.8

0.8

0.6

0.6

0.4
0.2
0.0

2.0

Odds ratio for (i)

Power

Power

C

1.5

0.4
0.2

Standard

New
method (1)

0.0

New
method (2)

Reject (i) and (ii)

Standard

New
method (1)

New
method (2)

Reject (i), (ii) and (iii)

Figure 1: Simulated results on inference for TNSWAC. In panels A and B the null hypothesis
is true and there is no difference in the exposure between the test-positives, test-negatives and
controls. In panel A analyses (i) and (ii) show dependence and in panel B analyses (i) and (iii)
show approximate uncorrelatedness. Two plots in Panel C show the simulated powers of three
different methods of analyses of TNSWAC, calculated from 10,000 simulated instances. One
simulated study consists of 1,250 individuals of, on average, 40% controls, 30% test-negatives
and 30% test-positives. Under the null, in Panel A and B, the frequency of an exposure is
constant 20% in each of these three groups. In panel C, the odds ratio of an exposure is 1.75 for
test-positives versus controls and the odds ratio of an exposure is 1.75 for test-positives versus
test-negatives. R code for the simulation is provided in the supplement.

5

Supplement to “Inference for a test-negative case-control
study with added controls”
Bikram Karmakar and Dylan Small
University of Florida and University of Pennsylvania

1 Familywise error rate control
Setup: Consider the following three null hypotheses: H0(i) , no difference in exposure between
test-positives and test-negatives; H0(ii) , no difference in exposure between test-positives and
controls; and H0(iii) , no difference in the test-positives or test-negatives and controls. In the
following P(i) , P(ii) and P(iii) correspond to the three p-values calculated for these hypotheses
from the corresponding comparisons.
In this setup a method provides a level α familywise error rate control if the probability of
rejecting any true null hypothesis among the three null hypotheses is at most α. In the following
we let RS denote the event that at least one of the nulls are rejected among {H0s : s ∈ S} where
S ⊆ {(i), (ii), (iii)}. We show here that familywise error rate is controlled for both Method 1
and Method 2.

Method 1. Note first that H0(iii) is false when and only when one of H0(i) or H0(ii) were false.
Since Method 1 can reject H0(iii) in step (2) only when both H0(i) and H0(ii) are rejected at
step (1), we have R(iii) ⊆ R(i) ∩ R(ii) , hence R(i),(ii),(iii) ⊆ R(i),(ii) .
To show familywise error rate control, consider now the different possibilities of the three
hypotheses being true or false separately.
Address for Correspondence: Bikram Karmakar, Department of Statistics, University of Florida, 226 Griffin
Floyd Hall, Gainesville, FL 32611 (E-mail: bkarmakar@ufl.edu).

6

(a) When all three hypotheses are true, the familywise error rate is

pr(R(i),(ii),(iii) ) ≤ pr(R(i),(ii) )
≤ pr(R(i) ) + pr(R(ii) )
= pr(P(i) ≤ α/2) + pr(P(ii) ≤ α/2)
≤ α/2 + α/2 = α.

(b) When H0(i) is true but H0(ii) is false, hence H0(iii) is false, the familywise error rate is

pr(R(i) ) = pr(P(i) ≤ α/2) ≤ α/2 ≤ α.

(c) Finally, when H0(ii) is true but H0(i) is false, hence H0(iii) is false, the familywise error rate
is
pr(R(ii) ) = pr(P(ii) ≤ α/2) ≤ α/2 ≤ α.
Hence, the familywise error rate is always controlled.
Method 2. First we expand the notation RS to denote the event that at least one of the nulls
are rejected among {H0s : s ∈ S} where S ⊆ {(i), (ii), (iii), (i) ∧ (ii)}, where H0(i)∧(iii) =
H0(i) ∩ H0(iii) . Thus, H0(i)∧(iii) is false is the same as at least one H0(i) H0(iii) is false, and only
when both H0(i) and H0(iii) are true we will have H0(i)∧(iii) true.
Now we use the result that P(i) and P(iii) are essentially independent and P(i)∧(iii) , Fisher’s
combination of these two p-values, is a valid p-value under H0(i)∧(iii) . 1 (see footnote)
Consider again the different combinations of the three hypotheses being true or false. We
can reduce some effort in this enumeration by noting that H0(iii) is false when and only when
one of H0(i) or H0(ii) were false.
Two analyses are essentially independent if the joint distribution of the p-values from these analyses is stochastically larger than the uniform distribution on unit square. Here, (i) and (iii) are nearly independent since we can
show pr(P(i) ≤ p, P(iii) ≤ q) ≤ pq for all 0 ≤ p, q ≤ 1. With larger sample size this inequality becomes sharper,
and asymptotically they are independent.

7

(a) When all three of H0(i) , H0(ii) and H0(iii) are true, the familywise error rate is

pr(R(i),(ii),(iii) ) ≤ pr(R(ii) at level α/2 in step (1) or R(i)∧(iii) at level α/2 in step (2))
≤ pr(R(ii) at level α/2) + pr(R(i)∧(iii) at level α/2)
= pr(P(ii) ≤ α/2) + pr(P(i)∧(iii) ≤ α/2)
≤ α/2 + α/2 = α.

(b) When H0(i) is true but H0(ii) is false, hence H0(iii) is false, the familywise error rate is

pr(R(i) ) = pr(R(i) at level α/2 or at level α in step (3), by whether λ = α/2 or = α)
≤ pr(R(i) at level α)
= pr(P(i) ≤ α) ≤ α.

(c) Finally, when H0(ii) is true but H0(i) is false, hence H0(iii) is false, the familywise error rate
is
pr(R(ii) ) = pr(R(ii) at level α/2 or at level α in step (3), by whether λ = α/2 or = α)
≤ pr(R(ii) at level α)
= pr(P(ii) ≤ α) ≤ α.

Hence, the familywise error rate is always controlled.

2 Confidence sets for the magnitude of effects
Notation: We can create confidence sets for the effects of the exposure using the methods
discussed in the letter. Some new notation are needed. In the following a subscript P is for testpositives, N for test-negatives, and C for the added controls. Also, n with appropriate subscript

8

denotes the counts of a particular group of individuals. For example, nP 1 denotes the number of
exposed test-positives and nC0 the number of unexposed test-negatives, and nP N 1 is the number
of exposed test-positives or test-negatives.
Data tables: The collected data can be tabulated in three tables corresponding to the three
comparisons (i), (ii) and (iii).
Comparison (i)

Comparison (ii)

Exposed Unexposed

Exposed Unexposed

Test-positive

nP 1

nP 0

Test-positive

nP 1

nP 0

Test-negative

nN 1

nN 0

Control

nC1

nC0

Total

nP N 1

nP N 0

Total

nP C1

nP C0

Comparison (iii)
Exposed Unexposed
Test-positive or negative
Control
Total

nP N 1

nP N 0

nC1

nC0

nP N C1

nP N C0

A p-value for a given one of the three comparisons can be calculated from the corresponding
table, e.g., using Fisher’s exact test. For example, P(ii) is the p-value calculated from the 2-by-2
table above with the numbers nP 1 , nC1 , nP 0 and nC0 .
Effects of interest: We have three effects of interest for the exposure, between test-positives
and test-negatives, between test-positives and controls, and one between test-negatives and controls. We denote these effects as θP,N , θP,C and θN,C , which are defined below. These are called
attributable effects.
The effect θP,N is the ratio of the number of individuals who became test-positive because
of the exposure, but in the absence of it would have been test-negative minus the number of individuals who became test-negative because of the exposure but in the absence of it would have

9

been test-positive, divided by the number of exposed test-positives or test-negatives. Notice
that θP,N is a number between -1 and 1; θP,N = 0 if exposure did not move anyone from being
test-positive compared to test-negative without exposure or the reverse. If θP,N is positive, there
individuals for whom the exposure caused them to become test-positive. Similarly, if θP,N is
negative, there are individuals for whom the exposure caused them to become test-negative. In
summary, θP,N is the net effect of the exposure on becoming test-positive over test-negative for
exposed tested individuals.
The second effect θP,C is defined similarly. By our definition, θP,C is the net effect of the
exposure for test-positives versus controls relative to all exposed individuals either test-positive
or control. We have θP,C = 0 if the exposure did not make any change in who became testpositive over control or the reverse.
Finally, we define a third attributable effect θN,C in the same way to denote the net effect of
the exposure on becoming test-negative over control for all exposed non test-positive individuals.
A method that calculates p-values using the three tables above is testing the hypothesis of
no effect of the exposure that θP,N = 0, θP,C = 0 and θN,C = 0.
Confidence sets: We construct confidence sets for the effects θP,N , θP,C and θN,C . To do this
⋆

⋆

⋆

⋆

⋆

we have to explain how to test that θP,N = θP,N , θP,C = θP,C and θN,C = θN,C where θP,N , θP,C
⋆

and θN,C could be different from 0, not no effect of the exposure. When they are different from
0, we adjust the observed tables based on these effects to create tables of the potential outcomes
under no exposure.
Adjusted comparison (i)
Exposed
Test-positive

Unexposed

⋆

⋆

nP 0

⋆

⋆

nN 0

nP 1 − θP,N nP N 1 − θP,C nP C1

Test-negative nN 1 + θP,N nP N 1 − θN,C nN C1

10

Adjusted comparison (ii)
Exposed
⋆

Unexposed
⋆

Test-positive nP 1 − θP,N nP N 1 − θP,C nP C1
⋆

⋆

nC1 + θP,C nP C1 + θN,C nN C1

Control

nP 0
nC0

Adjusted comparison (iii)
Exposed

Unexposed

⋆

⋆

Test-positive or negative nP N 1 − θP,C nP C1 − θN,C nN C1
⋆

⋆

nC1 + θP,C nP C1 + θN,C nN C1

Control

nP N 0
nC0

Using either Method 1 or Method 2 we could test these three tables at level α. Either method
⋆

⋆

⋆

will make decisions to reject or not reject these adjusted tables. Then we write R(i) (θP,N , θP,C , θN,C ),
⋆

⋆

⋆

⋆

⋆

⋆

R(ii) (θP,N , θP,C , θN,C ) and R(iii) (θP,N , θP,C , θN,C ) as binary variables which are 1 or 0 according
to whether comparison (i), (ii) or (iii) is rejected, respectively, based on these adjusted tables.
Our confidence interval is
n

⋆

⋆

⋆

(θP,N , θP,C , θN,C ) :

o
⋆
⋆
⋆
Rs (θP,N , θP,C , θN,C ) = 0 .

Y

s∈{(i),(ii),(iii)}

Since either method performed at level α provides familywise error rate control at α, this
confidence interval will have a minimal coverage of 1 − α for both Method 1 and Method 2.

3 R code to implement new method (2)
Let p_i, p_ii and p_iii be variables in R that record the p-values from the three comparisons. They can be calculated using the syntax p_i = fisher.test(e_i, g_i)$p
where e_i is a variable recording of exposure status, and g_i is a variable recording the case
status only for the test-positives and test-negatives. e_ii, g_ii and e_iii, g_iii have the
same role in the following code corresponding to the comparisons (i) and (iii) respectively.
11

## Significance level for familywise error rate control
alpha <- 0.05
alpha.2 <- alpha/2

## p-values computed from the three comparisons
p_i = fisher.test(e_i, g_i)$p
p_ii = fisher.test(e_ii, g_ii)$p
p_iii = fisher.test(e_iii, g_iii)$p

### Start of Method 2 ###
r_i = r_ii = r_iii = 0

# an inference for reject, value 1, or 0.

## Step (1)
r_ii = 1*(p_ii < alpha.2)
lambda = ifelse(r_ii, alpha, alpha-alpha.2)
## Step (2)
# Fisher’s combination
p_i_or_iii = pchisq(-2*log(p_i*p_iii), 4, lower.tail=FALSE)
r_i_or_iii = 1*(p_i_or_iii < lambda)
## Step (3)
if(r_i_or_iii)
r_i = 1*(r_i < lambda);

r_iii = 1*(r_iii < lambda)

## Step (4)
if(r_i & r_iii)

r_ii = 1*(p_ii < alpha)

### Final inference
c(r_i, r_ii, r_iii)
#### END OF CODE ####

12

