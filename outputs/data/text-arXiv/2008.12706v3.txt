Nowcasting in a Pandemic using Non-Parametric
Mixed Frequency VARs
Florian HUBERa , Gary KOOPb , Luca ONORANTEc,d ,
Michael PFARRHOFER∗a , and Josef SCHREINERe
a

University of Salzburg
University of Strathclyde
c
European Commission
d
European Central Bank
e
Oesterreichische Nationalbank

arXiv:2008.12706v3 [econ.EM] 1 Dec 2020

b

This paper develops Bayesian econometric methods for posterior inference in
non-parametric mixed frequency VARs using additive regression trees. We argue
that regression tree models are ideally suited for macroeconomic nowcasting in
the face of extreme observations, for instance those produced by the COVID-19
pandemic of 2020. This is due to their flexibility and ability to model outliers.
In an application involving four major euro area countries, we find substantial
improvements in nowcasting performance relative to a linear mixed frequency VAR.

JEL: C11, C32, C53, E37
KEYWORDS : Regression tree models, Bayesian, macroeconomic forecasting, vector
autoregressions

∗

Corresponding author : Michael Pfarrhofer. Department of Economics and Salzburg Centre of European
Union Studies (SCEUS), University of Salzburg. Address: Mönchsberg 2a, 5020 Salzburg, Austria. Email :
michael.pfarrhofer@sbg.ac.at. Florian Huber and Michael Pfarrhofer gratefully acknowledge financial support
from the Austrian Science Fund (FWF, grant no. ZK 35). We would like to thank Serena Ng, Niko Hauzenberger,
Paul Hofmarcher, William McCausland, James MacKinnon, Simon van Norden and participants of the Montreal
Econometrics Seminar for valuable comments and suggestions. Opinions expressed in this paper do not necessarily
reflect the official viewpoint of the European Commission, the European Central Bank, the Oesterreichische
Nationalbank or the Eurosystem.

1

Introduction

Mixed frequency vector autoregressions (MF-VARs) have enjoyed great popularity in recent
years as a tool for producing timely high frequency nowcasts of low frequency variables. A
common practice (see, e.g., Schorfheide and Song, 2015)1 is to choose a quarterly macroeconomic
variable such as gross domestic product (GDP) and a set of monthly variables and model them
together in a VAR so as to produce monthly nowcasts of GDP. The fact that statistical agencies
release data such as GDP with a delay, whereas appropriately chosen monthly variables are
released with less of a delay further enhances the benefits of the MF-VAR. Nowcasts can be
updated in a timely fashion.
The pandemic lockdown of 2020 has further increased the need for timely, high frequency
nowcasts of economic activity. And the increasing availability of a variety of high frequency (i.e.,
monthly, weekly or daily) and quickly released data (i.e., some variables are released almost
instantaneously) presents rich opportunities for the mixed frequency modeler. However, the
pandemic also poses challenges to the conventional, linear, MF-VAR. During the pandemic, we
have seen values of variables that are far from the range of past values. Linear time series
econometric methods seek to find average patterns in past data. If current data is very different,
using such patterns and linearly extrapolating them may be highly questionable.
This has led researchers to try to develop new VAR frameworks for nowcasting during
the pandemic. For instance, Schorfheide and Song (2020) find that the model developed in
Schorfheide and Song (2015) nowcasts poorly, but that if they estimate their MF-VAR using
data through 2019 and then produce conditional forecasts for the first half of 2020, improvements
were obtained. In essence, the extreme data in the first half of 2020 caused estimates of the
full sample MF-VAR coefficients to change in a manner which led to poor forecasts. Lenza and
Primiceri (2020) propose an alternative VAR-based approach which allows the error covariance
matrix to have a mixture distribution. In essence, the pandemic is treated as a large variance
shock and pandemic observations are, thus, drastically downweighted in the model estimation.
They conclude: ”Our results show that the ad-hoc strategy of dropping these observations may
be acceptable for the purpose of parameter estimation. However, disregarding these recent data
is inappropriate for forecasting the future evolution of the economy, because it vastly underestimates uncertainty.” Thus, although Schorfheide and Song (2020) and Lenza and Primiceri
(2020) adopt very different approaches, they end up with similar advice: discard the pandemic
observations when estimating the model.
1

A few other recent MF-VAR references adopting similar strategies include Eraker et al. (2015), Ghysels (2016),
Brave et al. (2019) and Koop et al. (2020).

2

It is possible to envisage other approaches to modifying the MF-VAR for pandemic times.
These would involve parameter change of some form (e.g., structural break or time-varying
parameter, TVP, models). But structural break models would be plagued by the fact that there
are too few observations post-break to permit reliable estimation. This problem would not occur
with TVP models which assume smoothly adjusting coefficents. But such TVP models are not
capable of adjusting for sudden and strong jumps in the endogenous variables within a few
months such as have been occurring in the pandemic. In light of these considerations, we adopt
a different, non-parametric, approach. We argue that such an approach should automatically
decide how to treat the pandemic observations in a sensible fashion. In an empirical exercise
involving four European countries, we demonstrate the superior nowcasting performance of our
approach.
The non-parametric model we adopt involves Bayesian additive regression trees (BART,
see Chipman et al., 2010). BART is a flexible and popular approach in many fields of statistics.2
But BART has rarely been used in time series econometrics. Huber and Rossini (2020) develop
Bayesian methods which build BART into a VAR leading to the Bayesian additive vector autoregressive tree (BAVART) model and demonstrate that it forecasts well. In this paper, we
develop Bayesian methods for the mixed frequency version of this model (MF-BAVART). This
development is non-trivial and, thus, represents an econometric contribution to the literature
even apart from the pandemic context. The MF-VAR is a Gaussian linear state space model
and well-established Bayesian methods exist for estimation and predictive inference. However,
the MF-BAVART is not linear and, thus, these methods are not directly available. MF-VARs
treat the unobserved high frequency values of the low frequency variables as latent states. Conditional on these latent states, we obtain the BAVART and methods similar to those of Huber
and Rossini (2020) can be used. It is drawing the latent states (conditional on the BAVART
parameters) which is more challenging. We deal with this challenge by rendering the model
conditionally Gaussian using recently developed methods for estimating effect sizes in so-called
black-box models such as BART, see Crawford et al. (2018, 2019).3 In simulations, we show that
this approximation is accurate for data generating processes (DGPs) that resemble the behavior
of GDP during the pandemic.
We apply the resulting model to nowcast GDP growth in selected euro area economies
(Germany, Spain, France and Italy) and show that our approach outperforms the linear MFVAR model. With some exceptions, it produces slightly better nowcasts through 2019. But
2

Tan and Roy (2019) is an excellent introduction to BART and includes a long list of papers using BART in
a variety of scientific disciplines.
3
The methods derived in this literature and used in the present paper can also be used with other black-box
models such as neural networks or Gaussian process regressions.

3

when the first two quarters of 2020 are included, the improvements offered by MF-BAVART rise
substantially. We investigate where these gains come from in a detailed study of the predictive
densities for the first six months of 2020. Our results suggest that they stem from the superior
ability of our model to adjust the predictive variance in a timely manner and thus increase the
probability of observing the extreme observations during the pandemic. These increases in the
predictive variance are mostly driven by a flexible Bayesian prior and using a large number of
regression trees. Moreover, we also assess whether data on COVID-19 cases and Google mobility
trends can improve nowcasts despite the fact that the time series data on these variables is very
short. Our model is equipped to extract information from such series and predictive accuracy
is increased in some cases.
The remainder of this paper is organized as follows. The next section discusses our econometric methods. We define the MF-BAVART model, illustrate how it can handle extreme
observations such as those that have occurred during the 2020 pandemic and discuss prior elicitation. It also contains the relevant full conditional posterior distributions and sketches the
Markov Chain Monte Carlo (MCMC) algorithm for posterior inference. Section 3 carries out an
artificial data exercise where we investigate the properties of the MF-BAVART model. Section 4
of the paper contains our empirical work. Section 5 offers a summary and conclusions. Appendix
A provides some technical details. There is also an Online Appendix that includes additional
empirical results and MCMC convergence diagnostics.4

2

Econometric Methods

2.1

The MF-BAVART

0 , y 0 )0
Suppose we are interested in modeling an M -dimensional vector of time series yt = (ym,t
q,t

where ym,t is an Mm vector and yq,t is an Mq vector and t = 1, . . . , T indicates time at the
monthly frequency. The variables in ym,t are observed, but we do not observe yq,t at any point
in time. Instead the statistical agency produces a quarterly figure, yQ,t . Assuming that yq,t
are monthly growth rates (log difference relative to the previous month) and yQ,t are quarterly
growth rates (log difference relative to the previous quarter), the relationship between them is
(see Mariano and Murasawa, 2003):5
1
2
1
2
1
yQ,t = yq,t + yq,t−1 + yq,t−2 + yq,t−3 + yq,t−4 .
9
9
3
9
9
4

(1)

All codes and data are available from the corresponding author upon request. Replication files are also
available for download at github.com/mpfarrho/mf-bavart.
5
We divide our latent monthly growth rates by three to make their scales comparable. Thus, the right hand
side of this equation divides that of Mariano and Murasawa (2003) by three.

4

We refer to this as the intertemporal restriction and note that it applies every third month (e.g.,
the statistical agency produces quarterly data for the quarter covering January, February and
March, but not the quarter covering February, March, April).
We assume that yt evolves according to a general multivariate model of the form:
yt = F (Xt ) + εt ,

εt ∼ N (0, Σ),

(2)

0 , . . . , y 0 )0 denoting a K(= M p)-dimensional vector of covariates, F (X ) =
with Xt = (yt−1
t
t−p

(f1 (Xt ), . . . , fM (Xt ))0 being an M -dimensional vector of potentially non-linear functions fj :
RK → R and Σ denotes an M × M -dimensional variance-covariance matrix.
This is a state space model where unobserved monthly growth rates, yq,t , are treated
as states. The state equations are given by Eq. (2). The measurement equations are the
intertemporal restriction in Eq. (1) (applicable every third month) and those which simply
state that ym,t are observed every month.
If F (Xt ) is a vector of linear functions, then we obtain the linear MF-VAR of, e.g.,
Schorfheide and Song (2015). Assuming a conditionally Gaussian prior for the VAR coefficients
(e.g., the Minnesota prior or a conditionally Gaussian global-local shrinkage prior), posterior
and predictive inference is straightforward. That is, standard Bayesian MCMC methods such as
Forward-Filtering Backward-Sampling (FFBS, see, e.g., Frühwirth-Schnatter, 1994) for Gaussian
linear state space models can be used.
In this paper, we wish to treat F (Xt ) non-parametrically. In principle, any model can be
used for F (e.g., kernel regression, deep neural networks, tree-based models, Gaussian process
regression) and the methods derived below could be used with minor modifications. In this paper,
we approximate F using BART as, for reasons discussed below, it should be well-designed to
capture large shocks and outliers such as those produced by the pandemic.
BART approximates each fj (Xt ) as follows:

fj (Xt ) ≈

S
X
s=1

gjs (Xt |Tjs , µjs ),

(3)

where gjs is a single regression tree function and Tjs are the corresponding so-called tree structures related to the j th element in yt . Moreover, µjs are tree-specific terminal nodes and S
denotes the total number of trees. The dimension of µjs is denoted by bjs and depends on the
complexity of the tree (i.e., this dimension is the number of leaves on the tree).
The literature on BART models typically sets S between 200 and 250. Chipman et al.
(2010) show that setting this number too low hurts predictive performance. If S is increased,
5

predictive performance increases up to a certain level of S. Increasing the number beyond this
level (which is often between 100 to 150) hardly impacts predictive accuracy. In our empirical
work, including the artificial data exercise, we set S = 250. This choice provides sufficient
flexibility to capture sharp shifts in the conditional mean. Section 4.2.2 contains results showing
that this is a reasonable choice.
To understand how BART works, we begin with a single tree and single equation (and,
for simplicity, suppress the j, s subscripts which distinguish the various trees and equations in
the VAR). In the language of regression, a tree takes as an input the value for the explanatory
variables for an observation and produces as an output a fitted value for the dependent variable
for that observation. These fitted values are the parameters related to the terminal nodes. It
does this by dividing the space of explanatory variables into various disjoint regions using a
sequence of binary rules. These so-called splitting rules take the form {X ∈ Ar } or {X 6∈ Ar }

with Ar being a partition set for r = 1, . . . , b and X = (X1 , . . . , XT )0 a full-data matrix of
dimension T × K. The partition rules involve an explanatory variable and depend on whether
they are above or below a threshold, c. If we let X•i denote the ith column of X, then the
partition set takes the form {X•i ≤ c} or {X•i > c}.

For Y (which is a T -dimensional vector in our simple example) we obtain the fitted values
as follows:
g(X|T , µ) =

b
X
r=1

µr I(X ∈ Ar ),

(4)

which is a step function where I denotes an indicator function that equals one if its argument
is true. As we will show in our simple example in the next sub-section, this is an analysis of
variance (ANOVA) model that can, conditional on knowing the indicators, be cast in a linear
regression form.
A key point to emphasize is that everything defining the tree is treated as an unknown
parameter and estimated. This includes the terminal node parameters (µ which is the vector
of fitted values the algorithm can choose between), their number (b) as well as all the elements
of the tree structure (i.e. the explanatory variable, X•i , and threshold, c chosen to define each
splitting rule). To illustrate our model and show what an estimated tree looks like in our dataset,
the following sub-section provides an empirical illustration.

2.2

Empirical Illustration of How BART Works and Handles the Pandemic

To provide some additional intuition of what BART is doing and why it might be a good
approach to handle the extreme observations associated with the pandemic, we preview our
6

Figure 1: Estimated tree structures for Germany using a single tree.
(a) Without pandemic observation
GDPt−1 < −1.392
PMIt−5 ≥ 0.013

IPt−1 < 1.774

IPt−2 < 3.424

ESIt−1 < 0.899

GDPt−1 ≥ −4.687
−9.588 (2)

−5.114 (3)

−1.825 (5)

IPt−1 < −1.610

2.447 (1)

−1.018 (9)

0.246 (99)

0.833 (39)

1.594 (19)

(b) Including pandemic observation
IPt−1 < −16.960
IPt−2 < −9.492

GDPt−1 < −1.392
PMIt−5 ≥ 0.013

IPt−1 < 1.774

IPt−2 < 3.424

ESIt−1 < 0.899

GDPt−1 ≥ −4.687
−19.960 (2)

−12.549 (4)

−9.588 (2)

−5.114 (3)

−1.825 (5)

2.447 (1)

IPt−1 < −1.610
−1.018 (9)

0.246 (99)

0.833 (39)

1.594 (19)

Notes: The variables in the trees are GDP, IP (industrial production), ESI (economic sentiment indicator), and
PMI (purchasing manager’s index). Complete definitions are given in the empirical section of this paper. The
number of observations choosing each terminal node is in parentheses. The splitting rules are defined such that,
if the condition holds you move down the left branch of the tree, else you move down the right branch.

empirical application in a simple way. Full details of our data and application are provided
below, suffice it to note here that results in this sub-section are for GDP growth in Germany
and the full sample of data runs through 2020Q2. We use a single tree with a relatively noninformative prior so as to allow for more complex tree structures. This is just for illustration.
In our main empirical work, we use many trees and a regularization prior.
Figure 1 shows two estimated regression trees for Germany. The top panel uses data
through 2019Q4 and the bottom panel uses the full sample. The general structure of a regression
tree can be illustrated from panel (a). The tree is organized with a condition (e.g., GDPt−1 <
−1.392) at the top of every binary split. If this condition holds, you move down the left branch,
else you move down the right branch. So, for example, the rightmost terminal node (1.594) is
chosen by observations with GDPt−1 greater than or equal to −1.392 (go right at the first split)
and have the first lag of industrial production (IPt−1 ) greater than or equal to 1.774 (go right
at the second split). Hence, the fitted value for GDP growth for observations with last month’s
industrial production growth greater than 1.774 and last month’s estimated GDP growth above
−1.392 is 1.594. There are 19 observations which fall in this category.
Recall that everything in the tree is estimated by the algorithm. This includes all the
numbers (i.e., the values of the terminal nodes and the thresholds in the splitting conditions),
7

the choice of variables in the splitting conditions (e.g., some of the conditions depend on GDP
growth, others depend on the growth in industrial production and both appear at various lags)
and the number of splits that occur. For instance, to get to the leftmost terminal node in panel
(b) involves checking two conditions (two splits), to get to the rightmost terminal node involves
checking three conditions (three splits). To get to some terminal nodes there are multiple splits
involving different variables, which is particularly useful for correlated explanatory variables.
All in all, BART has great flexibility in capturing any sort of behavior, including characteristics common with macroeconomic data. Using Eq. (4), we can illustrate this by casting our
model into ANOVA form based on panel (a) of Figure 1. The corresponding model is given by:
Y•j = − 9.588 {I(GDPt−1 < −1.392)I(PMIt−5 ≥ 0.013)}
− 5.114 {I(GDPt−1 < −1.392)I(PMIt−5 < 0.013)I(IPt−2 < 3.424)}
− 1.825 {I(GDPt−1 < −1.392)I(PMIt−5 < 0.013)I(IPt−2 ≥ 3.424)I(GDPt−1 < −4.687)}
+ 2.447 {I(GDPt−1 < −1.392)I(PMIt−5 < 0.013)I(IPt−2 ≥ 3.424)I(GDPt−1 < −4.687)}
− 1.018 {I(GDPt−1 ≥ −1.392)I(IPt−1 < 1.774)I(ESIt−1 < 0.899)I(IPt−1 < −1.610)}
+ 0.246 {I(GDPt−1 ≥ −1.392)I(IPt−1 < 1.774)I(ESIt−1 < 0.899)I(IPt−1 ≥ −1.610)}
+ 0.833 {I(GDPt−1 ≥ −1.392)I(IPt−1 < 1.774)I(ESIt−1 ≥ 0.899)}
+ 1.594 {I(GDPt−1 ≥ −1.392)I(IPt−1 ≥ 1.774)} + ε•j .
Here we let Y•j and ε•j the j th column of Y and ε, respectively. Since GDP is ordered first in
Y , j = 1 denotes its equation. Each row in the equation above corresponds to an assignment
depending on whether the indicators in the parentheses are equal to one. The equation illustrates
that BART is capable of handling multi-way interaction effects. For instance, the first row
suggests a two-way interaction effect between the first lag of GDP and the fifth lag of the
purchasing managers’ index (PMI) while the second row corresponds to a three-way interaction
between GDP, PMI and IP.
Another point to note is that our MF-BAVARTs involve six variables, not just the four
variables which appear in the estimated trees. The BART algorithm has decided that the other
two variables should not be involved in the splitting conditions and have no useful explanatory
power for GDP growth (loosely analogous to these other variables being insignificant).
With regards to modeling during the pandemic, we now turn to a comparison of panels
(a) and (b) in Figure 1. Note that panel (b) has a splitting condition IPt−1 < −16.960 at the
top, an extremely low value for the growth in industrial production. The branch of the tree
which satisfies this condition contains six observations. These are the six monthly pandemic
8

observations. The branch of the tree which does not satisfy this condition contains all the nonpandemic observations and is the same as the tree in panel (a). What our tree-based model
is doing is creating nodes for capturing outliers. Whereas the parameter estimates in a linear
model can be substantially affected by an outlier, BART can simply add a new branch to control
for it without affecting the main body of the tree. We will explore this issue in more detail in
our empirical work, but this is one of the reasons why our MF-BAVART ends up nowcasting
better than the linear MF-VAR, particularly around the time of the pandemic.
Since this example is a special case with a single tree and thus S = 1, it ignores another
dimension of flexibility in terms of handling outliers: the option to add additional trees. The
”A” in BART stands for additive and the BART algorithm allows for new trees to be introduced
to deal with the pandemic observations. Through 2019, we could see a tree (or trees) which had
terminal nodes covering the range of pre-2020 GDP growth values. For the pandemic period, a
new tree could yield an estimate of the average GDP growth rate within the pandemic (which
would probably be close to −10 percent). Adding such a tree would imply that the remaining
tree(s) lack the pandemic-related branches. Thus this new tree could serve to account for
deviations from the pre-pandemic GDP growth rate.

2.3

The Priors

In this section we discuss our prior setup. For computational reasons discussed below, all priors
are specified in an equation-specific manner and any fixed hyperparameters are the same across
equations. In the following discussion, the index j = 1, . . . , M refers to the different equations
of the model.
BART can be interpreted as a non-parametric approach capable of approximating any
non-linear function. But, as with any non-parametric approach, BART risks over-fitting. This
is why Bayesian methods have been commonly used since prior information can mitigate this
problem. We use regularization priors to reduce the complexity of the tree structures and to
shrink the terminal nodes. In the jargon of this literature, we force each tree to be small and,
thus, act as a weak learner. This essentially implies that for a large S, each tree explains only a
limited fraction of the variation in yt . For each equation we closely follow Chipman et al. (2010)
and use a regularization prior that can be factorized as follows:
p ((Tj1 , µj1 ), . . . , (TjS , µjS )) =

Y
s

with p(µjs |Tjs ) =

Q

i p(µi,js |Tjs )

p(µjs |Tjs )p(Tjs ).

and µi,js being the ith element of µjs . Within trees, we assume

that the terminal leaf parameters are independent of each other but depend on the specific tree
9

structure Tjs .
We specify a tree generating stochastic process (see Chipman et al., 1998) on Tjs that
consists of three parts. The first part relates to the probability that a given node at stage
n = 0, 1, 2, . . . is not a terminal node. This probability is specified such that
α(1 + n)−β .
α ∈ (0, 1) and β > 0 denote scalar hyperparameters. Smaller (larger) values of α (β) introduce a
larger penalty on more complex tree structures. This prior thus controls for overparameterization
by keeping trees rather small and simple (and they thus act as weak learners). In our empirical
application we set α = 0.95 and β = 2. This is the standard choice proposed by Chipman
et al. (2010) that works well for a wide range of different datasets and in simulations. The
second part concerns the possible values the thresholds c can take. Here we assume a discrete
uniform distribution over all possible values of the ith covariate X•i . Finally, the last part deals
with the specific variables used in the splitting rules. Again, in the absence of substantial prior
information about which variables should define the splitting rules we use a uniform distribution
over the K columns of X.
It is worth noting that our choice for the tree generating prior flexibly adjusts to the data
since the implied prior on the decision rules depends on the range of the columns in X. Hence,
if X contains extreme observations (such as the ones observed during the pandemic), our prior
places equal weights on these extremes without artificially bounding away prior mass from the
boundary of the parameter space of the thresholds used to define splitting rules.
On the terminal node parameters we use a Gaussian prior that places substantial prior
mass on the range of the M columns in Y = (y1 , . . . , yT )0 . The prior on µi,js is given by:
2
µi,js |Tjs ∼ N (0, σµj
).

The prior standard deviation σµj is set as follows:
σµj =

max(Y•j ) − min(Y•j )
√
,
2γ S

(5)

with γ denoting a suitable positive constant. Notice that if the number of trees S or γ are
increased, the prior is pushed towards zero and the effect of a single tree becomes smaller. This
specification also implies that the prior variance widens with the range of Y•j . Hence, if we
observe extreme observations, the range of Y•j sharply increases and the prior on µi,js becomes
looser (for fixed S and γ). This feature helps us in a pandemic since we introduce little shrinkage
10

if outliers arise and consequently allow for a large range of possible realizations of Y•j . And this
increases the likelihood of capturing outlying observations when interest centers on predictive
inference.
Consistent with Chipman et al. (2010), we construct the prior on µi,js by transforming
the endogenous variable such that the transformed values range from −0.5 to 0.5, which implies
that the numerator in Eq. (5) is one. Following much of the recent literature, we set γ = 2. This
value implies an approximate 95 percent probability that the conditional mean of the model is
between the minimum and maximum of each column of Y .
We specify a prior on the covariance parameters and the error variances σj2 separately. Let
2 ) denotes
Q be an M × M lower triangular matrix with unit diagonal and H = diag(σ12 , . . . , σM

a diagonal matrix with elements σj2 , such that Σ = QHQ0 . The free elements associated with
the j th row of Q are stored in a (j − 1)-dimensional vector qj . On each element of qj we use a
Horseshoe (HS) prior:
2 2
qji |τji , λ ∼ N (0, τji
λ ),

τji ∼ C + (0, 1),

λ ∼ C + (0, 1).

(6)

Here we let C + denote the half Cauchy distribution and τji and λ scaling parameters. Note
that λ does not feature any indices and thus serves as a common shrinkage factor across the free
2 , . . . , τ2
elements of Q. For later convenience we let V j = λ2 ×diag(τj1
jj−1 ) denote a (j−1)×(j−1)

dimensional prior scaling matrix.
For σj2 we use the conjugate inverse Chi-square distribution:
σj2 ∼ νj ξj /χ2νj ,
where νj and ξj denote hyperparameters that are calibrated using a data-based estimate of
σj , σˆj . This data-based estimate is taken to be the OLS standard deviation from a univariate
AR(5) model.6 The values of νj and ξj are then chosen such that the v th quantile of the prior is
centered on σˆj with P (σj < σˆj ) = v. In our application we use v = 0.75 and set the degrees of
freedom νj = T /2. We found that this choice avoids too large values of σj2 during the pandemic
and thus forces the tree-based model to fit more aggressively. Smaller values of νj yield similar
results if the sample is expanded to include the first two quarters of 2020, but at the cost of
potential numerical stability issues of the algorithm.
This completes our prior setup. For reference, Table 1 summarizes all our prior choices.
6
In our real-time nowcasting exercise we compute this quantity on a rolling window basis, always taking into
account the available data up to the time that the nowcast is produced.

11

Table 1: Summary of prior hyperparameters.
Description

Parameters

Hyperparameters

Trees (probability of non-terminal node)

α(1 + n)−β

Terminal nodes

2
µi,js |Tjs ∼ N (0, σµj
)

Error variances

σj2 ∼ νj ξj /χ2νj
ξj s.t. P (σj < σ̂j ) = v
2 2
qji |τji , λ ∼ N (0, τji
λ )

α = 0.95
β=2
√
σµj = (max(Y•j ) − min(Y•j )) /(2γ S)
γ=2
S = 250 (number of trees)
νj = T /2
v = 0.75
τji ∼ C + (0, 1)
λ ∼ C + (0, 1)

Covariances

Notes: “Description” provides information about the respective model parameter. “Parameters” shows the respective distribution or probability. “Hyperparameters” indicates our choice for the hyperparameters. Y•j refers to the j th column of
Y , σ̂j is the OLS standard deviation from a univariate AR(5) model and “s.t.” is an abbreviation for such that. We use
this prior setup for the simulation study and our empirical application.

2.4

Posterior Simulation

In terms of posterior and predictive computation, the point to note is that efficient MCMC
algorithms have been derived for estimating BART models. In our MF-BAVART model, we use
these conditional on yq,t . That is, one block of the MCMC algorithm (to be discussed below)
provides draws of yq,t and, conditional on these draws, we use standard algorithms for drawing
the BART parameters. In principle, we could draw the parameters of the trees and Σ as an
entire M dimensional system. However, we follow Carriero et al. (2019) and estimate the model
on an equation-by-equation basis by conditioning on the lower Cholesky factor of Σ. This speeds
up computation time enormously.7
2.4.1

Drawing the Trees

In this sub-section, we discuss the sampling step involved in estimating the trees. Each tree
structure Tjs is obtained using the Bayesian backfitting strategy discussed in Chipman et al.
(2010). Under our prior setup, Chipman et al. (2010) show that the trees can be sampled
marginally of µjs :
p(Tjs |Rjs , qj , Zj , σj ) ∝ p(Tjs )

Z
|

p(Rjs |µjs , Tjs , qj , Zj , σj )p(µjs |Tjs , qj , σj )dµjs .
{z
}

(7)

p(Rjs |Tjs ,qj ,Zj ,σj )

We let Rjs denote a partial residual vector that depends on the trees s 6= s as follows:
Rjs = Y•j −
7

X
s6=s

gjs (X|Tjs , µjs ) − Zj qj ,

Appendix A shows how to write the model as a system of unrelated regressions (conditional on Σ).

12

with Zj = (Zj1 , . . . , ZjT )0 and Zjt being composed of the structural shocks of the preceding
j − 1 equations (see Appendix A). Hence, p(Rjs |Tjs , qj , Zj , σj ) can be viewed as a conditional
likelihood that takes a relatively simple form and, more importantly, does not depend on µjs .
A draw from Eq. (7) can be obtained by using the Metropolis-Hastings (MH) algorithm
proposed in Chipman et al. (1998). This algorithm starts by generating a candidate value Tjs∗

from a probability distribution q(Tjsi , Tjs∗ ), with the superscript i denoting the accepted draw at
the ith iteration of the algorithm. We set Tjsi+1 = Tjs∗ with probability:
a(Tjsi , Tjs∗ )

=

q(Tjs∗ , Tjsi ) p(Rjs |Tjs∗ , qj , Zj , σj ) p(Tjs∗ )

q(Tjsi , Tjs∗ ) p(Rjs |Tjsi , qj , Zj , σj ) p(Tjsi )

.

(8)

The transition kernel q(Tjsi , Tjs∗ ) is constructed by switching randomly between four moves.
The first move grows a terminal node with a probability of 0.25. The second move randomly
selects a parent of two terminal nodes and transforms it into a terminal node with a probability
of 0.25. The third step randomly picks some interior node and changes its splitting rule with
probability 0.4. The final move swaps a decision rule between a parent (i.e., the node above)
and child (i.e., the node below) with probability 0.1.
The key feature of this algorithm which leads to convenient properties is that µjs is
integrated out and thus the dimension of the estimation problem is kept fixed.
2.4.2

Drawing the Latent States and Predictive Inference in the MF-BAVART

The previous sub-sections defined the MF-BAVART and discussed how well-established MCMC
methods can be used to draw the BART parameters conditional on the states (i.e., the unobserved high frequency values of the low frequency variables). To complete the MCMC algorithm
we need a method for drawing the states, conditional on the BART parameters. In a linear
MF-VAR this is done using standard Bayesian state space algorithms such as FFBS. But with
the non-parametric MF-BAVART this is more complicated since the model is highly non-linear
and FFBS is not directly applicable. Accordingly, we borrow from the literature that deals with
estimating effect sizes in black-box models (see Crawford et al., 2018, 2019) to produce a linear
approximation to F (Xt ).
This linear approximation has been originally proposed in the context of Gaussian kernel
(GK) and Gaussian process (GP) regressions. But the method is much more general and applies
to several popular techniques in machine learning and econometrics (Ish-Horowicz et al., 2020).
The only requirement is that we have learned a non-linear function F = (F (X1 ), . . . , F (XT ))0
evaluated at the T observations and we can sample from the relevant posterior distribution.

13

To explain the linear approximation, note that in linear regression models, the effect size
(or regression coefficient) is interpreted as the magnitude of the projection of X onto Y which
takes the form:
Â = Proj(X, Y ) = X † Y ,
with X † (which is K × T ) being the Moore-Penrose inverse. In the case where X is a full rank

matrix this projection is simply (X 0 X)−1 X 0 Y and the effect size is the least squares estimate

(i.e., it is an estimate of the magnitude of the marginal effect of the explanatory variables on
the dependent variables).
We follow Crawford et al. (2018, 2019) and project X onto the matrix F .8 This produces
the following estimate which can be interpreted as an effect size:
Ã = Proj(X, F ) = X † F .
The main intuition behind this approximation is that the regression of F on X provides information on how much variance is explained through X. This is a simple way of understanding
the relationship between X•i (i = 1, . . . , K) on F . In the absence of additional regularization,
the projection essentially implies that X Ã ≈ F .
Given this linear approximation, FFBS can be used to draw yq,t . Thus, this step in
the MCMC algorithm is an approximate one, but our simulation study and empirical results
indicate the approximation is a good one. We use Ã to produce a linear approximation to the
non-parametric multivariate model:
yt = Ã0 Xt + εt .

(9)

Since we now have an approximated linear model with Gaussian shocks, standard techniques
such as FFBS can be used to draw yq,t based on the Gaussian linear state space model defined
by Eq. (9) and the intertemporal restriction in Eq. (1).
FFBS provides draws from the nowcast distribution based on a linearized version of the
non-parametric and non-linear state space model. In our empirical work (and if interest centers on out-of-sample forecasting) we refrain from using these linearized estimates but use the
posterior distribution of F to produce now/forecasts.
In the case of the one-step-ahead forecast, the corresponding predictive distribution is
8
An alternative that might be able to better pick up non-linearities would be to consider non-linear transformations of X such as X 2 as well.

14

then simply given by:
p(yT +1 |XT +1 ) =

Z

p(yT +1 |Ξ, Y )p(Ξ|Y )dΞ,

(10)

with Ξ being a generic notation that refers to all parameters and latent states in the model.
The conditional distribution p(yT +1 |Ξ, Y ) is:
yT +1 |Ξ, Y ∼ N (F (XT +1 ), Σ).

(11)

Within our algorithm, draws from Eq. (11) can be mapped back to the quarterly values using
the intertemporal restriction in Eq. (1).
Iteratively using this equation allows us to compute higher order forecasts or other functions of the parameters. Equation (10) implies that we can use the BART model to produce
the one-step-ahead predictive distribution. After integrating out Ξ, this distribution will take
a highly non-standard form that allows for multi-modality, fat tails and asymmetries in the
forecast distributions. At this point, we would like to stress that for nowcasts, the differences
between using the smoothed estimates of yT or using Eq. (11) are negligible. Both approaches,
after integrating out Ξ, yield very similar estimates of the predictive distribution with the same
interesting features that make this approach suitable for handling the pandemic.
2.4.3

Drawing the Remaining Parameters

The steps involved in simulating the remaining parameters are standard with the conditional
posteriors taking a well-known form. The terminal node parameters µjs can be obtained by
simulating µi,js from independent Gaussian distributions which take a textbook conjugate form.
The same can be said about the error variances. These can be simulated from a conditional
posterior which follows an inverse Gamma distribution.
We sample qj from a multivariate Gaussian posterior. This posterior is given by:
qj |? ∼ N (mj , Ωj ) ,

−1
Ωj = (Zj0 Zj + V −1
j ) ,

mj = Ωj Zj0 Ỹj

(12)

with Ỹj = Y•j − fj (X) and the ? notation indicating that we condition on the remaining model
parameters and the latent states.
The scaling parameters of the HS prior are obtained using methods outlined in Makalic
and Schmidt (2015). Introducing additional auxiliary parameters allows us to simulate τji and λ
from inverse Gamma distributions. More precisely, the corresponding full conditional posteriors

15

are:
2
τji
|? ∼ G −1

2
qji
1
+ 2
1,
wji 2λ

!
,

for i = 1, . . . , j − 1,



M (M − 1) + 2 1 1 X X
λ2 |? ∼ G −1 
, +
4
ζ
2
i

j

2
qji

!

2
τji

.

The auxiliary parameters wji and ζ are simulated from:
−2
wji |? ∼ G −1 (1, 1 + τji
),

2.4.4

ζ|? ∼ G −1 (1, 1 + λ−2 ).

Full Conditional Posterior Sampling

Our MCMC algorithm iteratively samples from the full conditional posterior distributions outlined in the previous sub-sections. Conditional on a suitable set of starting values, the algorithm
cycles through the following steps:
1. Simulate the S trees Tjsi for each equation using the Metropolis Hastings algorithm with
acceptance probability given by Eq. (8) and proposal distribution q(Tjsi , Tjs∗ ).

2. Simulate the terminal node parameters associated with each tree and for each equation
j from a Gaussian distribution. This Gaussian distribution takes a standard form and
depends only on the elements in Y•j − Zj qj that are allocated to the respective terminal
node and the prior variance.
3. For each equation, simulate the error variance σj2 from an inverted Gamma distribution.
4. The covariance parameters associated with the j th row of Q are simulated from a multivariate Gaussian posterior described in Eq. (12).
5. The parameters related to the HS prior are obtained by sampling from the relevant conditional distributions described above.
6. We use a conventional FFBS algorithm to simulate the latent states based on the approximate model derived in Sub-section 2.4.2.
7. In case we are interested in producing now/forecasts based on the estimated BART model,
use Eq. (11) to draw from the predictive distribution.
This algorithm is used to produce 30, 000 draws. We then discard the first 15, 000 draws
as burn-in. Standard convergence diagnostics indicate rapid convergence towards the joint posterior distribution and thus closely mirror the excellent performance of the original algorithm
16

of Chipman et al. (2010). Some of these diagnostics are shown in the supplementary Online
Appendix.

3

Simulation Study

In this section we investigate the properties of our model and algorithm using various DGPs.
Since our empirical work deals with nowcasting in the pandemic, we construct DGPs that
mirror the dynamic properties observed for actual macroeconomic aggregates such as output
and unemployment. We assume that the data is generated by a persistent VAR(1) process, but
during the last part of the sample, we have a substantial deviation from linearity by assuming
a rapid decline followed by a sharp and abrupt recovery during the last few observations. To
investigate how the quality of our approximation varies with the size of the model, we consider
different values for M ∈ {3, 5, 7} as well as different numbers of observations T ∈ {150, 250, 350}.
Assuming that the first element of yt , y1t , is the latent process that determines the low
frequency variable, our DGP takes the following form:
yt = A1 yt−1 + Qut ,

εt ∼ N (0M , IM ), for t = 1, . . . , T − 25,

where A1 is an M ×M matrix with off-diagonal elements sampled from a N (0, 0.12 ) distribution
and its diagonal elements equal to 0.9. Q is a lower triangular matrix with the diagonal elements
equal to 0.2 and off-diagonal elements sampled from a N (0, 0.12 ) distribution.9 The DGP is
initialized by simulating y0 from a standard Gaussian distribution.
Non-linearities in the DGP are introduced by assuming that y1t follows a VAR for the first
T − 25 observations. For the remaining 25 observations we assume that:

y1t =



−ϑ0t for t = T − 24, . . . , T − 5,

+ϑ for t = T − 4, . . . , T.
1t

ϑjt (j = 0, 1) denotes an element of an evenly spaced grid with the length given by the number
of observations within a regime (i.e., 20 for the downturn and 5 for the quick recovery). This
grid is scaled with the standard deviation of y1t , σy1 , over the first T − 25 observations. In the
case of the downturn, we let the grid range from 0.1σy1 to 6σy1 . In the recovery, the grid runs
from 2σy1 to 5σy1 . This introduces substantial non-linearities and yields realizations of y1t that
behave similar to output during the first half of 2020. It is worth noting that we introduce a
9

Since adding random noise to A1 can lead to an unstable DGP with eigenvalues exceeding unity, we introduce
the restriction that the maximum absolute eigenvalue of A1 must not exceed one.

17

2
0
−2
−4

0

50

100

150

200

250

300

350

Figure 2: Single realization from the DGP for M = 7 and T = 150 and posterior estimates
Notes: The gray shaded area and the outer solid black lines refer to the 5th and 95th th credible intervals, the
middle solid black line denotes the posterior median and the red line is the true outcome. The dashed black line
marks the beginning of the “crisis” episode of our DGP.

larger number of pandemic observations in our simulations than the model gets to see in our
empirical work (especially during the downturn). Nevertheless, the rapid increase in y1t takes
place over five observations and thus serves to illustrate how our model deals with detecting
rapid shifts using few observations.
Notice that the share of these non-linear regimes depends on the length of the time series
T and goes from close to seven percent (for T = 350) to almost 17 percent (for T = 150).
These latent indicators are then mapped back to the observed quantities using the intertemporal
aggregation scheme outlined in Eq. (1).
This DGP allows us to assess how well our approach recovers the latent high frequency
series y1t . Estimation accuracy is measured by computing root mean squared errors (RMSEs)
between the posterior median of y1t and the true outcome. We compare results from our MFBAVART specification to a standard linear MF-VAR which is identical in all respects except
that it is linear. This implies that we set F (Xt ) = AXt with A being an M × K coefficient
matrix. On a = vec(A) we use the HS prior defined in Eq. (6). The prior on Σ is the same in
the two models. All experiments are repeated 150 times.
It is worth noting that the chosen DGP is a tough case for our model. This is because
the DGP assumes linearity for the majority of periods and then includes a rather small number
of extreme observations. Hence, our model needs to learn the behavior of y1t using relatively
few data points. And this is the challenge we face when our aim is to model macroeconomic
18

Table 2: Relative performance for differently sized models and sample size.
Sample size
Model size
M =3
M =5
M =7

T = 150

T = 250

T = 350

0.87
0.89
0.89

0.89
0.89
0.91

0.90
0.89
0.93

Notes: The table shows relative root mean squared errors (RMSEs) with respect to the “true” latent monthly process between the MF-VAR and MF-BAVART for the estimated monthly processes over 150 simulated data generating processes
(DGPs). Values below unity indicate that our model yields more precise estimates of y1t than the competing approach,
and refer to the mean of relative RMSEs over all simulation repetitions.

quantities in a pandemic.10
Before discussing RMSEs comparing our model and the MF-VAR, we present Figure 2
which illustrates the properties of the MF-BAVART model for a single realization from the DGP
for M = 5 and T = 350. Other realizations typically look very similar and are thus omitted for
brevity. From this figure, we observe that during the first part of the sample (which we label the
non-crisis part), our model tracks the actual evolution of y1t remarkably well. Once we hit the
crisis regime, it adjusts and the corresponding credible sets include the actual outcome in the
vast majority of periods. Only during the beginning of the rapid expansion do the credible sets
of MF-BAVART not cover the true series. The final few observations are also well captured with
only a single observation not included in the credible intervals. This shows that MF-BAVART
works well when the DGP is characterized by substantial non-linearities towards the end of the
sample.
To investigate whether this strong performance is consistent across replications of the
DGP and how our approach performs relative to the standard MF-VAR, Table 2 shows average
RMSE ratios between the MF-BAVART and the MF-VAR across different configurations of the
DGPs. Values below unity indicate that our model yields more precise estimates of y1t than the
competing approach. The figure suggests that for small-scale models (M = 3), our approach
substantially outperforms the linear model for different sample sizes. When the model size is
increased, the MF-BAVART still improves upon the MF-VAR but to a slightly lesser extent.
This experiment based on synthetic data shows that our approach yields reasonable estimates
of the latent states, even in the presence of substantial deviations from normality and linearity.
10

In principle, we could also showcase our model using DGPs that feature substantial non-linearities over the
full sample. However, such DGPs imply an unfair advantage of our model relative to the standard MF-VAR. We
have carried out some simulation experiments using such DGPs and find that our model performs remarkably
well. The results are available from the corresponding author upon request.

19

4

Empirical Results

In this section, we investigate the performance of our MF-BAVART model for nowcasting GDP
growth using four data sets with relatively short samples. The short sample arises since some of
the variables have only been collected for a short time period. This is an issue with many of the
new data sets that are becoming popular (e.g., internet search data) and, accordingly, we felt it
useful to test our methodology in the type of context where it might be used in the future. All
models use a lag length of five since this is the number of lags in the intertemporal restriction
in Eq. (1).

4.1

Data and Design of the Real Time Nowcasting Exercise

We use monthly and quarterly data on Germany (DE), France (FR), Italy (IT) and Spain (ES)
from 2005M03/2005Q1 to 2020M06/2020Q2 on the following M = 6 variables:
1.

GDP growth: quarterly GDP growth (abbreviated GDP), released six weeks after the end
of the respective quarter.

2.

Industrial production: monthly growth rate of industrial production (abbreviated IP),
released with a lag of approximately six weeks.

3.

Economic sentiment indicator: monthly growth rate of the economic sentiment indicator
(abbreviated ESI), released on the next-to-last working day of the respective month.

4.

New car registrations: monthly growth rate of new car registrations (abbreviated CAR),
released with a delay of two and a half weeks.

5.

Purchasing managers’ index: monthly growth rate of the purchasing managers’ index
(abbreviated PMI), released on the first working day of the next month.

6.

One-year-ahead interest rates (abbreviated EUR), monthly average of the level of interest
rates, available immediately after the end of the respective month.

Growth rates are exclusively computed by taking log-differences. Data on GDP and industrial
production is obtained from the OECD real time database, the Economic Sentiment Indicator
is provided by the European Commission, figures on new car registrations are released by the
European Automobile Manufacturers Association (ACEA), PMI readings come from Markit and
the interest rate data is obtained from Macrobond. In this case the number of columns in X is
K = 30.

20

Our main results use the data just described. However, the paper ends with a detailed
examination of nowcasting during the COVID-19 pandemic of 2020. One of the models considered there includes two additional variables which we refer to as pandemic variables. The first of
these is the number of new COVID-19 infections which is obtained from the Centers for Disease
Control. We include this variable as end of period values using the transformation log(1 + x)
where x indicates the number of cases. First cases in the respective countries were reported in
January 2020. The second variable reflects information associated with social distancing measures and relevant lock-downs by using Google mobility trends data. More precisely, we take
the average of the series on Retail & Recreation, Workplaces and Transit Stations to obtain an
aggregate measure of social distancing. Both of these variables are only available for the first
six months of 2020. In order to include the pandemic variables in the MF-BAVART we fill in
pre-2020 values with zero. For this model, K increases to K = 40.
Recall that following standard BART practice, our variables are transformed to lie in
the interval [−0.5, 0.5]. This transformation is used for estimating the tree-based components.
After obtaining the tree structures and terminal node parameters, the remaining quantities of
the model are estimated by transforming the variables back to their original scale.
Given the relatively short sample size we begin evaluating nowcasts in 2011Q1. Within
each quarter, we produce three nowcasts, one for each month in the quarter (subsequently
abbreviated Mth./Qt.). Our model nowcasts monthly growth rates, yq,t , which are turned into
quarterly growth rates for comparison with the actual realization of quarterly GDP growth. All
of our nowcasts respect the release calendar and data revisions (e.g., a nowcast produced for
January will be made at the beginning of February using the data that has been released by
then).

4.2
4.2.1

Results of the Real Time Nowcasting Exercise
Comparing the MF-BAVART to the MF-VAR

Table 3 summarises our findings for the nowcasting exercise. The table offers a comparison of
MF-BAVART to the conventional MF-VAR in terms of RMSEs and log predictive scores (LPSs,
which are log predictive likelihoods summed over the nowcast evaluation period). To investigate
the pandemic period, we produce two sets of results: one ending in 2019Q4 (the left panel of
Table 3), and one for the full sample (including the pandemic period, the right panel of Table
3).
Note first that, as we move from month to month within a quarter, our nowcasts almost
always improve. This statement holds true for both nowcast evaluation metrics and countries.

21

Table 3: Results of the nowcasting exercise.
Through 2019Q4
MF-BAVART
RMSE

LPS

Through 2020Q2

MF-VAR
RMSE

MF-BAVART

LPS

RMSE

LPS

MF-VAR
RMSE

LPS

DE
Mth./Qt. 1
Mth./Qt. 2
Mth./Qt. 3

0.586∗∗
0.587
0.537

−54.735∗∗∗
−54.251∗∗∗
−45.714∗∗∗

0.646
0.654
0.536

−112.792
−114.451
−66.582

1.891∗∗∗
1.699
1.610

−411.289∗
−87.527∗∗∗
−85.816∗∗

1.971
1.452
1.001

−628.246
−182.194
−144.233

ES
Mth./Qt. 1
Mth./Qt. 2
Mth./Qt. 3

0.490∗
0.428∗∗
0.354∗∗∗

−31.535∗∗∗
−30.103∗∗∗
−27.170∗∗∗

0.556
0.484
0.402

−151.984
−121.163
−89.600

3.496
2.828∗∗
2.814∗∗∗

−590.363
−378.179∗∗
−367.837∗

3.421
2.865
2.964

−2108.736
−815.515
−682.207

FR
Mth./Qt. 1
Mth./Qt. 2
Mth./Qt. 3

0.320
0.312
0.313

−32.317∗∗∗
−27.547∗∗
−33.201∗

0.330
0.333
0.296

−65.015
−61.902
−49.014

2.669
1.991
2.057

2.656
1.900
1.498

−2893.765
−1122.662
−1141.538

IT
Mth./Qt. 1
Mth./Qt. 2
Mth./Qt. 3

0.391∗
0.366
0.297∗

−15.288∗∗
−12.075∗∗
−6.824∗∗

0.473
0.401
0.333

−70.330
−49.272
−26.648

2.323
1.808
1.758

2.133
2.153
1.558

−872.414
−548.016
−592.571

−2800.367
−716.782
−753.489
−360.691∗
−319.175∗
−314.247

Notes: The table shows root mean squared errors (RMSE) and cumulative log predictive scores (LPS). Mth./Qt. denotes
which month within the quarter the nowcast was made. Asterisks indicate p-values of the Diebold and Mariano (1995) test
with MF-VAR as the benchmark model, with levels of significance ∗ (10%), ∗∗ (5%), ∗∗∗ (1%).

This provides evidence that mixed frequency methods are useful for nowcasting in these data
sets. As new information is released each month, our nowcasts of GDP growth improve.
In terms of the comparison of linear versus non-parametric mixed frequency methods,
prior to the pandemic, we find that MF-BAVART nowcasts better than the linear MF-VAR.
If we consider the properties of our density nowcasts using LPS as the evaluation metric, the
superior performance of MF-BAVART holds for each month within the quarter and for every
country. The nowcast improvements provided by MF-BAVART are statistically significant using
the Diebold and Mariano (DM, 1995) test.11 For point nowcast performance, RMSEs indicate
that MF-BAVART is outperforming the MF-VAR, but the nowcast improvements tend not to
be statistically significant.
When we turn to the right panel of Table 3, which includes the pandemic period, we tend
to see even larger improvements in the nowcast performance of the MF-BAVART relative to the
MF-VAR. Note in particular the huge improvements found using LPS (a measure based on the
entire predictive density) which occur for every country and for every month within the quarter.
In the right panel of the table, differences in LPS between the MF-BAVART and the MF-VAR
are typically measured in the hundreds whereas in the left panel they are typically measured
in the tens. Given the relationship between log predictive likelihoods and marginal likelihoods,
these findings constitute extremely strong evidence in favor of the MF-BAVART. Thus, standard
11

We would like to stress that the DM test provides only a rough measure of statistical accuracy. This is
because we use an expanding estimation sample and the number of observations in the hold-out period is quite
low. Moreover, the DM test is conservative when applied to short-term forecasting problems.

22

Bayesian model comparison methods are showing strong evidence that MF-BAVART is handling
the pandemic better than the MF-VAR. Assessing statistical difference by means of DM tests
points towards superior predictive capabilities in terms of LPS at the 10 percent significance
level for the majority of countries (except France) and months within the quarter.
4.2.2

A Deeper Examination of why MF-BAVART is Forecasting so Well

RMSEs are not as consistently favorable to the MF-BAVART as LPSs are when the pandemic
observations are included. This indicates that the benefits of using the non-parametric approach
lie largely in its ability to better model second and higher predictive moments for the extreme
observations for the first half of 2020. More precisely, once we move into the pandemic period
our model yields wider predictive intervals. And it produces these wider predictive distributions
in a timely manner.
To explain why BART is producing these features, note that BART handles outliers either
through creating a new terminal node or by adding trees. The corresponding nodes (either in
an existing complicated tree or a newly grown tree) will feature very few observations (often
only one or two). This implies little information in the likelihood which is coupled with a large
prior variance (mainly driven by our data-based prior discussed in Section 2) thus leading to a
large posterior variance.
We empirically demonstrate this effect in Figure 3. This plots the posterior interval width
(defined as the difference between 5th and 95th percentiles of the posterior distribution) of latent
GDP against the leverage for all T values of each. The leverage values lt (t = 1, . . . , T ) are the
diagonal elements of the projection matrix X(X 0 X)−1 X 0 . Since parts of X are unobserved in
our MF model, we compute the leverage scores for each draw of the latent quantities and then
compute the posterior mean of the scores.12
The figure clearly shows that there exists a positive relationship between leverage and
posterior uncertainty. This implies that if we observe outlying observations (such as during
the pandemic), our model yields wider credible sets. This feature is especially pronounced
during the pandemic (the red dots in the figure) but we also find substantial evidence of a
similar relationship during the global financial crisis (the blue dots). We conjecture that this is
the main mechanism at work that yields improved predictive distributions through sharp and
timely increases in the predictive variance.
Adding more trees (i.e., increasing S) to capture the pandemic-related outliers will further
increase the predictive variance relative to a model which sets S small. To support this statement
12

Plugging in the posterior mean of the latent states to compute the leverage values leads to results which are
qualitatively very similar.

23

Figure 3: Posterior interval width of latent GDP versus leverage.
(b) ES

2

1.5

4

Posterior interval width
6
8
10
12

Posterior interval width
2.0
2.5
3.0
3.5

14

4.0

(a) DE

0.0

0.2

0.4
0.6
Leverage

0.8

0.0

0.2

0.4
0.6
Leverage

0.8

0.4
0.6
Leverage

0.8

(d) IT

2

2

Posterior interval width
3
4
5
6
7

Posterior interval width
4
6
8
10

8

12

(c) FR

0.0

0.2

0.4
0.6
Leverage

0.8

0.0

0.2

1.0

Notes: The scatterplot shows the posterior interval width (defined as the difference between the 5th and 95th
percentiles of the posterior distribution) against leverage. The red dots refer to the pandemic observations and
the blue dots refer to the global financial crisis (i.e., 2008M06 to 2009M03) while the solid black line is the line of
best fit.

consider Figure 4 which shows log predictive likelihoods (LPLs) by period for the first two
quarters of 2020, with nowcasts updated each month for all four countries. The lines in this
figure vary in the number of trees used, ranging from a single tree to 250 trees. While we
observe several differences, these tend to become smaller the more trees we include, with 250
trees indicating superior performance in almost all cases. However, once we get to 150 trees,
further forecast improvements tend to be small.
Another reason for its good performance is that BART is able to detect these tail events
not only through “remembering” partitions of the covariate space during certain periods such as

24

Figure 4: Log predictive likelihoods during the first two quarters of the pandemic for different
numbers of trees.

S=1
S=25

−200
S=200
S=250

2020Q2−M3

2020Q2−M2

2020Q2−M1

2020Q1−M3

2020Q1−M2

2020Q1−M1

2020Q2−M3

2020Q2−M2

2020Q2−M1

2020Q1−M3

2020Q1−M2

2020Q1−M1

2020Q2−M2

2020Q2−M1

2020Q1−M3

2020Q2−M3
S=75
S=150

−600

−400

−1000

Number of trees

2020Q1−M2

2020Q1−M1

−2000

−2000
2020Q2−M3

2020Q2−M2

2020Q2−M1

2020Q1−M3

2020Q1−M2

2020Q1−M1

IT

0

0
−1000

−400 −300 −200 −100

LPL by period

FR
0

ES

0

DE

S=300

Notes: Log predictive likelihood (LPL) by period for 2020Q1 and 2020Q2, with M1, M2 and M3 indicating the
month during the quarter when the nowcast is produced. S refers to the number of trees used for BART, with
S = 250 being the benchmark specification used for our empirical work.

the global financial crisis but also through recognizing that specific combinations of the covariates
have not been observed in the past and deserve creating a new tree (or terminal node within a
single tree). Again, such a new tree (or terminal node) will likely be equipped with relatively
few observations and little information on the conditional mean function. This, in turn, will
again (correctly) lead to inflated posterior intervals.
4.2.3

Assessing Model Calibration using Probability Integral Transforms

The preceding sub-section compared the relative performance of the MF-BAVART to the MFVAR, but did not present any evidence on the nowcast performance of either in an absolute
sense. In the Online Appendix, we provide graphs of the nowcasts of both approaches plotted against realized GDP growth for the four countries and three monthly nowcasts within
each quarter. An examination of them indicates that the MF-BAVART’s nowcasts are better
calibrated, particularly for Spain.
In this sub-section, we investigate this issue more formally using Probability Integral
Transforms (PITs). In particular, we follow a common practice (e.g., Clark, 2011) and produce
PITs for our nowcasts and transform them using the inverse of the c.d.f. of a standard Gaussian
distribution. We denote these transformed PITs as rt for the time of our nowcast evaluation
period. Perfectly calibrated nowcasts should lead to rt having mean zero, variance one and

25

Table 4: Summary statistics of transformed PITs.
Through 2019Q4
MF-BAVART
Mth./Qt.

Through 2020Q2

MF-VAR

MF-BAVART

MF-VAR

1

2

3

1

2

3

1

2

3

1

2

3

DE
µ
σ2
AR(1)

0.44
3.59
−0.33

0.37
3.52
−0.32

0.45
3.12
−0.37

0.83
6.97
−0.29

0.91
6.85
−0.15

0.87
4.14
−0.18

−0.48
22.97
0.83

0.06
5.20
−0.07

0.12
5.23
−0.12

−0.27
35.58
0.60

0.43
10.87
0.08

0.39
8.79
0.04

ES
µ
σ2
AR(1)

−0.25
2.33
0.71

−0.66
2.52
0.51

−0.56
2.77
0.56

−0.29
10.56
0.72

−0.73
8.60
0.65

−0.58
7.06
0.58

−1.45
30.64
1.24

−1.45
19.79
0.29

−1.31
19.77
0.24

−1.55
41.35
0.69

−1.97
42.65
0.52

−1.72
36.42
0.45

FR
µ
σ2
AR(1)

0.34
3.34
0.17

0.35
3.02
0.15

0.33
3.50
−0.06

0.49
5.53
0.19

0.69
5.07
0.12

0.55
4.54
0.09

−0.70
24.37
0.93

−0.76
40.29
0.22

−0.3
14.73
0.28

−0.57
26.77
0.83

−0.10
18.47
0.46

−0.16
16.35
0.37

IT
µ
σ2
AR(1)

−0.10
1.54
0.60

−0.05
1.38
0.46

0.01
1.17
0.32

−0.05
5.67
0.58

0.06
4.45
0.40

0.18
3.09
0.30

−0.96
19.58
0.39

−0.79
17.51
0.23

−0.74
17.38
0.21

−1.45
47.28
0.51

−0.99
30.66
0.35

−0.84
33.35
0.20

Notes: Mth./Qt. denotes which month within the quarter the nowcast was made. µ, σ 2 and AR(1) denote the sample
mean, variance and AR(1) coefficient of the transformed probability intergral transforms (PITs).

being uncorrelated over time. We calculate the sample mean (labeled µ in the tables), variance
(labeled σ 2 ) and estimated autoregressive coefficient (labeled AR(1)). Table 4 displays these
summary statistics for the sample through 2019Q4 and the full sample, respectively.
Beginning with the linear MF-VAR, note that even in the pre-pandemic sample, there is
some evidence of poor calibration. For the sample mean, the point estimates are consistently
well away from zero. The sample variances are often substantially higher than one indicating
the predictive variance of the linear model is too small. There is sometimes evidence of autocorrelation in rt . When we move to the full sample, these problems get much worse, particularly
for the sample variance of rt which now becomes very large.
If we turn to the MF-BAVART for the sample ranging through 2020Q2, it can be seen
that the nowcasts are better calibrated. Even for the full sample, the sample mean of rt is
often close to zero. The estimated AR(1) coefficient often indicates autocorrelation in the first
month per quarter, while second and third month nowcasts indicate more favorable calibration.
It is the case that the sample variance of rt is still too high, but to a much lesser extent than
for the MF-VAR. Thus, use of the MF-BAVART has gone a large way towards improving the
calibration problems of the MF-VAR, even if it has not completely fixed them.
4.2.4

A Closer Look at the Pandemic Period

In this sub-section, we provide additional insight as to how MF-BAVART is nowcasting in the
two pandemic quarters. In addition to taking a closer look at the densities produced by the

26

Table 5: Log predictive likelihoods for MF-BAVART for each month within the first two
quarters of 2020.
Benchmark

Pandemic

2020Q1
Mth./Qt.
DE
ES
FR
IT

1
−29.6
−190.4
−925.4
−319.4

2
−22.6
−337.3
−721.2
−291.8

2020Q2
3
−32.2
−321.6
−696.6
−307.5

1
−349.8
−538.3
−1918.6
−6.3

2020Q1

2

3

−6.3
−6.7
−4.1
−3.8

−4.9
−5.4
−4.1
−3.6

1
−10.9
−77.8
−346.4
−92.5

2
−2.3
−94.3
−15.3
−5.7

2020Q2
3
−12.4
−3.3
−107.4
−29.6

1

2

3

−4.2
−6.7
−9.1
−4.6

−3.8
−4.8
−4.6
−3.7

−3.5
−4.2
−4.0
−4.0

Notes: “Benchmark” refers to the information set described above in the context of our main nowcast evaluation. “Pandemic” includes further variables measuring infection rates and Google mobility trends. Mth./Qt. denotes which month
within the quarter the nowcast was made.

MF-BAVART and MF-VAR models (labeled “Benchmark” models in the following figures) of
the preceding sub-sections, we also produce results for an eight dimensional MF-BAVART which
adds the two pandemic variables (see Sub-section 4.1, this is labeled the “Pandemic” model in
the figures).13
In Sub-section 4.2.1, we showed that (with one exception) the benchmark MF-BAVART
produces greatly superior predictive likelihoods relative to the MF-VAR during the six pandemic months. Table 5 offers additional insight by comparing log predictive likelihoods for the
benchmark and pandemic versions of the MF-BAVART model. It can be seen that adding the
pandemic variables often improves the nowcasting performance of the MF-BAVART. There are
several exceptions to this, but in the crucial pandemic months of March, April and May the
inclusion of pandemic variables is beneficial in all four countries. Thus, despite the very short
time series and the very simple way these variables are included in the model, BART finds a
way to usefully incorporate the information they contain.
One interesting result is that both time series even improve nowcasts produced in January
and February 2020. Inspection of the pandemic time series reveals that all countries have
reported the first infections already at the end of January. Since we assume that the nowcasts
are always produced towards the end of each month, BART is essentially using this increase from
zero to very few infections in combination with deteriorating values in the PMI, car registrations
and the ESI. As we will show below, this does not signal a downturn in GDP but merely
reflects the fact that uncertainty surrounding the predictive distribution is increasing (which
also happens technically because the size of the model is increased as well). And this is beneficial
for nowcasting.
Figure 5 plots the predictive densities themselves for the three models for the first six
13
Since the pandemic variables are non-zero for an extremely short time and feature rapid shifts, the MF-VAR
was not capable of handling them without substantial prior tuning and hence we do not include results from the
MF-VAR using the pandemic variables.

27

months of 2020. The key general finding is that, as expected, MF-BAVART is much more
flexible than the MF-VAR. Particularly in 2020Q2, the predictive densities it produces tend to
be much more dispersed, feature fatter tails, are often asymmetric and even sometimes multimodal. This contrasts with the MF-VAR where the predictive densities tend to be closer to
Gaussian densities.
In light of the recent interest of macroeconomics in models involving asymmetries and
multi-modalities (see, e.g., Adrian et al., 2019a,b) this feature of the MF-BAVART is particularly
attractive and is the source of the improvements in nowcast performance during the pandemic.
If we compare the benchmark and pandemic versions of the non-parametric model, a clear
pattern emerges. As the pandemic hits, the predictive densities for the pandemic model quickly
become much more dispersed than those of the benchmark model. The latter, in turn, are more
dispersed than those of the linear model. BART is deciding to incorporate information in these
short time series in such a way that the predictive densities reflect the increased uncertainty.
This is due to the fact that inclusion of these two additional very short time series is akin to
adding a time-varying intercept to the model with a rather large state innovation variance from
January 2020 onward. And this random intercept term scales up the variance of the predictive
distribution and thus improves density nowcasts (especially in the beginning of the pandemic
period).

5

Summary and Conclusions

MF-VARs have been a standard tool for producing timely, high frequency nowcasts of low
frequency variables for several years. With the arrival of the COVID-19 pandemic of 2020 the
need for such nowcasts has become even more acute. However, conventional linear MF-VARs
nowcast poorly during the pandemic due to their inability to effectively deal with the extreme
observations that have occurred. In this paper, we have developed the MF-BAVART which is a
non-parametric model using additive regression trees. MF-BAVART can be cast as a non-linear
state space model. We develop an approximate MCMC algorithm where the parameters defining
the conditional mean of the VAR are drawn using a standard BART algorithm and, conditional
on these, the states are drawn using a linear approximation. This linear approximation is taken
from the machine learning literature on black-box models and we use simulations to show that
it also works well for a DGP that closely matches the evolution of GDP during the pandemic.
Our nowcasting exercise, involving four major euro area countries, shows that MF-BAVART,
with few exceptions, performs better than the linear MF-VAR at all times in our sample, with
major nowcasting benefits during the pandemic. We show how and why this occurs by providing
28

Figure 5: Predictive densities for the first two quarters of 2020 across countries.
(a) DE
2020Q1

2020Q2
M1

M2

M3

M1

1.00

1.25

M2

M3

0.3
1.00

0.75

1.0

Density

Density

1.0

0.75
0.50

0.5

0.5

0.2

0.25
0.0
−2

0

2

4

0.1

0.00
−0.5 0.0

0.5

1.0

0.2

0.50
0.25

0.0
0.00
−1.0 −0.5 0.0 0.5 1.0 1.5

0.3

1.5

0.1

0.0
−10

0

10

0.0
−20

−10

GDP
MF−BAVART, benchmark
MF−BAVART, pandemic
MF−VAR, benchmark

Model

0

10

20

−20

−10

0

10

20

GDP
Model

MF−BAVART, benchmark
MF−BAVART, pandemic
MF−VAR, benchmark

(b) ES
2020Q1

2020Q2
M1

M2

M1

M2

M3

0.8

1.0

1.5

1.5

1.0

1.0

0.5

0.5

0.5

0.0

0.0
−0.5

0.0

−0.5 0.0

0.5

1.0

1.5

0.0

0.5

1.0

1.5

0.20

Density

1.5

Density

M3

0.6

0.2

0.15

0.4

0.10

0.1
0.2

0.05

0.0
−6

−3

0

3

6

0.0
−10

0

10

0.00
−20

−10

GDP
MF−BAVART, benchmark
MF−BAVART, pandemic
MF−VAR, benchmark

Model

0

10

20

−30 −20 −10 0

10 20

GDP
Model

MF−BAVART, benchmark
MF−BAVART, pandemic
MF−VAR, benchmark

(c) FR
2020Q1

2020Q2
M1

M2

M3

M1

M2

2

1

2.0

2.0

1.5

1.5

1.0

1.0

0.5

0.5

0
0.0
−0.25 0.00 0.25 0.50 0.75 1.00

Density

Density

2.5
1.5

0.3

1.0

0.2

0.5

0.1

0.0

0.0
−20

0.15
0.10
0.05

0.0
−1.0 −0.5 0.0 0.5 1.0 1.5
−0.50−0.25 0.00 0.25 0.50

−5

0

5

0.00
−10

GDP

0

10

−20

−10

0

10

GDP

MF−BAVART, benchmark
MF−BAVART, pandemic
MF−VAR, benchmark

Model

M3
0.20

Model

MF−BAVART, benchmark
MF−BAVART, pandemic
MF−VAR, benchmark

(d) IT
2020Q1

2020Q2
M1

M2

M3

M1

M2

M3

1.5

1.5

1.0

1.0

1.0

0.5

0.5

0.5

0.0

0.0

0.0

Density

1.5

−0.5

0.0

0.5

1.0

−2

−1

0

1

2

Density

2.0

−0.5

0.0

0.5

0.4

0.20

0.3

0.15

0.2

0.10

0.1

0.05

0.0
−15 −10 −5

0.00

0.15
0.10

0

5

10

0.05
0.00
−20

GDP
Model

−10

0

10

−20

−10

0

10

GDP

MF−BAVART, benchmark
MF−BAVART, pandemic
MF−VAR, benchmark

Model

MF−BAVART, benchmark
MF−BAVART, pandemic
MF−VAR, benchmark

Notes: “Benchmark” refers to the information set described above in the context of our main nowcast evaluation.
“Pandemic” includes further variables measuring infection rates and Google mobility trends. The black vertical
line marks zero.

a detailed comparison of nowcast densities in the first six months of 2020.
The techniques outlined in this paper have a wide range of potential applications, as they
can be applied to any non-linear and non-parametric learner commonly used in the literature.
Our focus on using BART is motivated by its strong performance in various applications (see,

29

e.g., Bleich et al., 2014; Linero, 2018; Kapelner and Bleich, 2015) as well as its flexibility in
handling outliers. As a fruitful avenue for further research one could assess how different learners
perform and then combine them using Bayesian model averaging techniques.

References
Adrian T, Boyarchenko N, and Giannone D (2019a), “Multi-modality in macro-financial dynamics,” Federal Reserve Bank of New York Staff Reports 903.
——— (2019b), “Vulnerable growth,” American Economic Review 109, 1263–1289.
Bleich J, Kapelner A, George EI, and Jensen ST (2014), “Variable selection for BART: An
application to gene regulation,” Annals of Applied Statistics 1750–1781.
Brave S, Butters R, and Justiano A (2019), “Forecasting economic activity with mixed frequency
BVARs,” International Journal of Forecasting 35, 1692–1707.
Carriero A, Clark TE, and Marcellino M (2019), “Large Bayesian vector autoregressions with
stochastic volatility and non-conjugate priors,” Journal of Econometrics 212(1), 137–154.
Chipman HA, George EI, and McCulloch RE (1998), “Bayesian CART Model Search,” Journal
of the American Statistical Association 93(443), 935–948.
——— (2010), “BART: Bayesian additive regression trees,” Annals of Applied Statistics 4(1), 266–298.
Clark T (2011), “Real-time density forecasts from Bayesian vector autoregressions with stochastic
volatility,” Journal of Business & Economic Statistics 29, 327–341.
Crawford L, Flaxman S, Runcie D, and West M (2019), “Variable prioritization in nonlinear
black box methods: A genetic association case study,” Annals of Applied Statistics 13, 958–989.
Crawford L, Wood K, Zhou X, and Mukherjee S (2018), “Bayesian Approximate Kernel Regression With Variable Selection,” Journal of the American Statistical Association 113, 1710–1721.
Diebold FX, and Mariano RS (1995), “Comparing predictive accuracy,” Journal of Business &
Economic Statistics 13, 253–263.
Eraker B, Chiu C, Foerster A, Kim T, and Seoane H (2015), “Bayesian mixed frequency VAR’s,”
Journal of Financial Econometrics 13, 698–721.
Frühwirth-Schnatter S (1994), “Data augmentation and dynamic linear models,” Journal of Time
Series Analysis 3, 183–202.
Ghysels E (2016), “Macroeconomics and the reality of mixed frequency data,” Journal of Econometrics
193, 294–314.
Huber F, and Rossini L (2020), “Inference in Bayesian additive vector autoregressive tree models,”
https://arxiv.org/abs/2006.16333 .
Ish-Horowicz J, Udwin D, Scharfstein K, Flaxman S, Crawford L, and Filippi S (2020), “Interpreting deep neural networks through variable importance,” Journal of Machine Learning Research
21, 1–30.
Kapelner A, and Bleich J (2015), “Prediction with missing data via Bayesian additive regression
trees,” Canadian Journal of Statistics 43(2), 224–239.
Koop G, McIntyre S, Mitchell J, and Poon A (2020), “Regional output growth in the United
Kingdom: More timely and higher frequency estimates from 1970,” Journal of Applied Econometrics
35, 176–197.
Lenza M, and Primiceri G (2020), “How to estimate a VAR after March 2020,” manuscript .
Linero AR (2018), “Bayesian regression trees for high-dimensional prediction and variable selection,”
Journal of the American Statistical Association 113(522), 626–636.
Makalic E, and Schmidt DF (2015), “A simple sampler for the horseshoe estimator,” IEEE Signal
Processing Letters 23(1), 179–182.
Mariano R, and Murasawa Y (2003), “A new coincident index of business cycles based on monthly
and quarterly series,” Journal of Applied Econometrics 18, 427–443.
Raftery AE, and Lewis S (1992), “How many iterations in the Gibbs sampler?” in JM Bernardo,
JO Berger, AP Dawid, and AFM Smith (eds.) “Bayesian Statistics 4,” 763–773, Oxford: Oxford
University Press.
Schorfheide F, and Song D (2015), “Real-time forecasting with a mixed-frequency VAR,” Journal
of Business & Economic Statistics 33(3), 366–380.
——— (2020), “Real-time forecasting with a (standard) mixed-frequency VAR during a pandemic,”
manuscript .
Tan Y, and Roy J (2019), “Bayesian additive regression trees and the general BART model,”
https://arxiv.org/abs/1901.07504 .

30

Appendix
A

Equation-by-Equation Estimation of the VAR

In this Appendix we show how to rewrite the VAR as a system of unrelated regression models.
This approach has the advantage that the computational burden is drastically reduced since we
can perform equation-by-equation estimation. Notice that the first equation of Eq. (2) can be
written as:
y1t = f1 (Xt ) + η1t ,

η1t ∼ N (0, σ12 ).

The second equation is given by:
y2t = f2 (Xt ) + q21 η1t + η2t ,

η2t ∼ N (0, σ22 ).

In general, the j th > 1 equation can be written as:
yjt = fj (Xt ) + qj0 Zjt + ηjt ,

ηjt ∼ N (0, σj2 ).

(A.1)

This implies that, conditional on the shocks to the previous j − 1 equations, the j th equation is a
standard regression model that features a non-parametric part given by fj (Xt ) and a regression
part qj0 Zjt with qj = (qj1 , . . . , qjj−1 )0 and Zjt = (η1t , . . . , ηj−1t )0 . The (j − 1)-dimensional
vector qj stores the first j − 1 elements of the j th row of Q. These equations are conditionally
independent and standard MCMC techniques can be readily applied.
Alternative algorithms replace the shocks with the contemporaneous values of yt . This
introduces order dependence which we avoid by conditioning on the shocks. Thus, we are using
a standard sampling algorithm that is commonly used to sample from the multivariate Gaussian
(Carriero et al., 2019).

31

Online Appendix
A
A.1

Additional Empirical Results
Log Predictive Scores Over Time

The LPS is the preferred comparison metric for many Bayesians. To investigate patterns in it
more deeply, consider Figures A.1a, A.1b, A.1c and A.1d which plot cumulative sums of log
predictive likelihoods over time. There tends to be a pattern where the advantage in nowcast
performance of the MF-BAVART relative to the MF-VAR steadily and gradually increases over
time through 2019. Then in 2020, there tends to be a large jump in the LPS’s of the MFBAVART relative to the MF-VAR. There is only one exception to this: French nowcasts made
by the MF-BAVART in the first month of each quarter show a large jump in 2020Q1, but then
display a fall in 2020Q2. Thus, we are doing a poor nowcasting job in April 2020 for France
relative to the linear model. But with this one exception, the MF-BAVART model is doing an
excellent job of handling the pandemic.

A.2

Predictive Densities and Actual Realizations

Here, we plot the nowcasts against the realizations. Our model produces monthly nowcasts of
GDP growth which are converted into quarterly nowcasts to be comparable to the realization.
To improve readability, we present results through 2019Q4 and through 2020Q2 as separate
graphs.

A.3

Estimates of the Latent States using FFBS compared to using BART

In this sub-section, we briefly compare the posterior density obtained from using FFBS to the
ones we obtain by using the estimated BART model. This serves to illustrate that both approaches (i.e. the one based on linearizing the non-linear model and the actual non-linear model)
yield predictive distributions which are very similar. Notice that the main difference is that the
FFBS-based posterior distribution features slightly tighter credible sets. These differences, however, only have a minor impact on the quality of the predictive density.

32

Figure A.1: Log predictive scores for MF-BAVART (solid line) relative to MF-VAR (dashed).

Until 2019

(a) DE
M1

60

M2

40
20
0
2012

2014

2016

2018

2020

20
15
10
5
0
2012

2014

M1

Incl. 2020

M3

60
40
20
0
2016

2018

2020

2012

M2

200
150
100
50
0
2014

2016

2018

2020

2012

2014

2016

2018

2020

2016

2018

2020

M3

60
40
20
0

75
50
25
0
2012

2014

2012

2014

2016

2018

2020

Until 2019

(b) ES
M1

125
100
75
50
25
0

M2

2012

2014

2016

2018

2020

2012

2014

M1

Incl. 2020

M3
60
40
20
0

75
50
25
0
2016

2018

2020

2012

2014

M2

1500
1000
500
0

2018

2020

M3
300
200
100
0

400
300
200
100
0
2012 2014 2016 2018 2020

2016

2012 2014 2016 2018 2020

2012 2014 2016 2018 2020

(c) FR
Until 2019

M1

M3
15
10
5
0
−5

30
20
10
0
2012

Incl. 2020

M2

30
20
10
0
2014

2016

2018

2020

2012

2014

M1

600

2016

2018

2020

M2

200
0
2012

2014

2016

2018

2020

2012

2014

2014

2016

2018

2020

2016

2018

2020

M3

400
300
200
100
0

400
300
200
100
0

400

2012

2012

2014

2016

2018

2020

(d) IT
Until 2019

M1

M2

20
0
2012

2014

2016

2018

2020

2012

2014

M1

Incl. 2020

M3
20
15
10
5
0

30
20
10
0

40

2016

2018

2020

2016

2018

2020

2016

2018

2020

M3

200
150
100
50
0
2014

2014

M2

500
400
300
200
100
0
2012

2012

200
100
0
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Notes: Samples ending in 2019Q4 are marked “Until 2019,” those ending in 2020Q2 are indicated as “Incl. 2020.”
M1, M2 and M3 refer to the month during the quarter when the nowcast was produced.

33

Figure A.2: Predictive densities for Germany.
(a) Until 2019
MF−BAVART

MF−BAVART

MF−BAVART

M1

M2

M3

MF−VAR

MF−VAR

MF−VAR

M1

M2

M3

2
1

GDP growth

0
−1

2
1
0
−1
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Date

(b) Including the pandemic
MF−BAVART

MF−BAVART

MF−BAVART

M1

M2

M3

MF−VAR

MF−VAR

MF−VAR

M1

M2

M3

0

GDP growth

−4
−8

0
−4
−8
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Date

Notes: Columns are months per quarter in which the nowcast was produced. Realizations are marked as X’s, and
shown alongside the estimate for the posterior median and the 68 percent credible set.

34

Figure A.3: Predictive densities for Spain.
(a) Until 2019
MF−BAVART

MF−BAVART

MF−BAVART

M1

M2

M3

MF−VAR

MF−VAR

MF−VAR

M1

M2

M3

1
0

GDP growth

−1
−2

1
0
−1
−2
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Date

GDP growth

(b) Including the pandemic
MF−BAVART

MF−BAVART

MF−BAVART

M1

M2

M3

MF−VAR

MF−VAR

MF−VAR

M1

M2

M3

0
−5
−10
−15
−20

0
−5
−10
−15
−20
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Date

Notes: Columns are months per quarter in which the nowcast was produced. Realizations are marked as X’s, and
shown alongside the estimate for the posterior median and the 68 percent credible set.

35

Figure A.4: Predictive densities for France.
(a) Until 2019
MF−BAVART

MF−BAVART

MF−BAVART

M1

M2

M3

MF−VAR

MF−VAR

MF−VAR

M1

M2

M3

1.0
0.5

GDP growth

0.0
−0.5

1.0
0.5
0.0
−0.5
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Date

(b) Including the pandemic
MF−BAVART

MF−BAVART

MF−BAVART

M1

M2

M3

MF−VAR

MF−VAR

MF−VAR

M1

M2

M3

0
−5

GDP growth

−10
−15

0
−5
−10
−15
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Date

Notes: Columns are months per quarter in which the nowcast was produced. Realizations are marked as X’s, and
shown alongside the estimate for the posterior median and the 68 percent credible set.

36

Figure A.5: Predictive densities for Italy.

GDP growth

(a) Until 2019
MF−BAVART

MF−BAVART

MF−BAVART

M1

M2

M3

MF−VAR

MF−VAR

MF−VAR

M1

M2

M3

1.0
0.5
0.0
−0.5
−1.0

1.0
0.5
0.0
−0.5
−1.0
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Date

(b) Including the pandemic
MF−BAVART

MF−BAVART

MF−BAVART

M1

M2

M3

MF−VAR

MF−VAR

MF−VAR

M1

M2

M3

0

GDP growth

−5
−10

0
−5
−10
2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

2012

2014

2016

2018

2020

Date

Notes: Columns are months per quarter in which the nowcast was produced. Realizations are marked as X’s, and
shown alongside the estimate for the posterior median and the 68 percent credible set.

37

Figure A.6: Comparison of FFBS-based and BART-based estimates of the monthly GDP
indicator.
(b) ES

−25

−40

−20

−30

−15

−20

−10

−5

−10

0

0

5

(a) DE

2010

2015

2020

2015

2020

(d) IT

−25

−20

−20

−15

−15

−10

−10

−5

−5

0

0

(c) FR

2010

2010

2015

2020

2010

2015

2020

Notes: The figure shows the 16th , 50th and 84th precentiles of the posterior distribution of the latent monthly
GDP indicators using Forward-Filtering Backward-Sampling (FFBS, in orange) and based on the estimated BART
specification (in black). The dashed red line marks the beginning of the pandemic (March 2020).

38

B

MCMC Diagnostics

To assess convergence properties of our sampling algorithm, we present inefficiency factors (IF)
and the Raftery and Lewis (1992) diagnostic (RL) of the total number of runs required to achieve
a certain level of precision. The quantiles for the latter are set to 0.025, the degree of accuracy
is 0.025, and the probability of attaining this required accuracy is 0.95.
Table B.1 shows the corresponding measures across all countries for the final data vintage.
For Ã we compute the median across all coefficients, det(Σ) is the determinant of the covariance
matrix. For yq,2005:2019 , we present the median over time from 2005M03 to 2019M12, while for
yq,2020 we show the median over time from 2020M01 to 2020M06.
IFs and RLs signal an overall strong performance of the algorithm. Inefficiency factors are
below five for all parameters and latent states except for yq,2020 in Spain. Considering the RL
diagnostic, we find that the number of iterations required is far below the total number of iterations considered for most parameters. Overall the convergence statistics indicate satisfactory
performance of our algorithm.
Table B.1: Summary statistics of MCMC diagnostics.
DE

Ã
det(Σ)
yq,2005:2019
yq,2020

ES

FR

IT

IF

RL

IF

RL

IF

RL

IF

RL

2.224
4.058
1.538
3.458

3951
4338
3802
3792

2.672
4.870
1.353
5.308

3940
4340
3782
8937

2.693
4.708
1.411
4.247

3994
4314
3761
8258

2.507
4.453
1.554
4.545

3983
4458
3802
11830

Notes: For Ã we compute the median measures across all coefficients, for yq,2005:2019 , we present the median over
time from 2005M03 to 2019M12, while for yq,2020 we show the median over time from 2020M01 to 2020M06. IF
and RL denote inefficiency factors and the Raftery and Lewis (1992) diagnostic, respectively.

39

