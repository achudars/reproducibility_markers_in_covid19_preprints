ESTIMATING AND INCREASING THE STRUCTURAL
ROBUSTNESS OF A NETWORK

arXiv:2103.00247v1 [math.NA] 27 Feb 2021

SILVIA NOSCHESE∗ AND LOTHAR REICHEL†
Abstract. The capability of a network to cope with threats and survive attacks is referred
to as its robustness. This paper discusses one kind of robustness, commonly denoted structural
robustness, which increases when the spectral radius of the adjacency matrix associated with the
network decreases. We discuss computational techniques for identifying edges, whose removal may
significantly reduce the spectral radius. Nonsymmetric adjacency matrices are studied with the aid
of their pseudospectra. In particular, we consider nonsymmetric adjacency matrices that arise when
people seek to avoid being infected by Covid-19 by wearing facial masks of different qualities.
Key words. Network analysis, Perron vector, pseudospectra, structured perturbation

1. Introduction. Networks appear in many areas, including transportation, social science, and chemistry; see, e.g., Estrada [5] and Newman [15] for many examples.
An edge-weighted network is represented by a graph G = {N , E, W}, which consists
of a set of nodes N = {nj }nj=1 , a set of edges E = {ej }m
j=1 that connect the nodes,
and a set of edge weights W = {wj }m
that
indicate
the
importance of the edges.
j=1
The weights are assumed to be positive. For instance, in a road network, the nodes
nj may represent cities, the edges ej may represent roads between the cities, and the
edge weight wj may be proportional to the amount of traffic on the road represented
by edge ej . We refer to a graph G as undirected if for each edge ej , there is an edge
ekj that points in the opposite direction and has the same weight as ej . If this is not
the case, then the graph G is said to be directed.
The adjacency matrix A = [aij ]ni,j=1 ∈ Rn×n associated with the graph G has the
entry aij = wk if there is an edge ek emerging from node ni and ending at node nj ; if
the graph is undirected, then also aji = wk . Other matrix entries vanish. Thus, the
matrix A is symmetric if and only if the graph G is undirected. We will assume that
there are no self-loops and no multiple edges. The former implies that the diagonal
entries of A vanish. Typically, the number of edges, m, satisfies 1 ≤ m ≪ n2 . Then
the matrix A is sparse.
The maximum of the magnitudes of the eigenvalues of A is known as the spectral
radius of A. We will denote the spectral radius of A by ρ(A). It has been shown that
the spectral radius is an important indicator of how flu-type infections spread in the
network that is associated with the adjacency matrix A; the smaller ρ(A), the less
spread; see, e.g., [11, 14] and below. This paper seeks to shed light on how the spectral
radius of an adjacency matrix can be reduced by targeted edge perturbations, i.e.,
by reducing edge-weights or removing edges. It is well known that reducing an edgeweight, or removing an edge, does not increase the spectral radius of a nonnegative
matrix; see, e.g., [9, Corollary 8.1.19]. We are interested in identifying which weights
should be reduced, or which edges should be removed, to achieve a possibly significant
decrease of the spectral radius.
Howard et al. [10] discuss the benefits of wearing facial masks to reduce Covid-19
transmission. Several studies found 70% or higher efficacy of facial masks in protecting
the wearer of Covid-19 infections. They found that wearing a mask protects people
∗ Dipartimento di Matematica “Guido Castelnuovo”, SAPIENZA Università di Roma, P.le A.
Moro, 2, I-00185 Roma, Italy. E-mail: noschese@mat.uniroma1.it
† Department of Mathematical Sciences, Kent State University, Kent, OH 44242, USA. E-mail:
reichel@math.kent.edu

1

around persons wearing masks, as well as the people who wear a mask, but to lesser
degree. Also the type of mask is important; see also Gandhi et al. [6, 7] for related
discussions.
Let the nodes in a graph represent people and the edge weights represent the
possibility of getting a sufficient viral load to become ill with Covid-19. The modeling
of facial masks of different quality results in a nonsymmetric adjacency matrix A associated with the graph; see Section 3. We are interested in investigating which weights
should be reduced or which edges should be removed to reduce ρ(A) significantly.
This paper is organized as follows. Section 2 discusses the structural robustness
of an adjacency matrix A. In particular, the sensitivity of the eigenvalues to perturbations of A is considered. Also adjacency matrices that model the role of face
masks are described. Section 3 is concerned with the calculation of the spectral radius of a large matrix, and of the determination of edges that should be eliminated,
or whose weight should be reduced, to reduce the spectral radius of A. Properties of
the pseudospectrum of a matrix are reviewed and applied. Some large-scale computed
examples are presented in Section 4, and concluding remarks can be found in Section
5.
2. Structural robustness. A formulation of structural robustness comes from
spectral graph theory. Epidemiological theory predicts that if the effective infection
rate of a virus in an epidemic is below the reciprocal of the spectral radius ρ(A) of
the adjacency matrix A associated with the graph that represents the network, then
the virus contamination in the network dies out over time. In more detail, assume
a universal virus birth rate β along each edge that is connected to an infected node,
and a virus death rate δ for each infected node. Then if the effective infection rate,
given by β/δ, is below the epidemic threshold for the network, i.e., if
1
β
<
,
δ
ρ(A)
then the infections tend to zero exponentially over time; see, e.g., [11] and references
therein. The smaller ρ(A), the higher is the structural robustness of the network
against the spread a virus. Hence, in order to enhance the structural robustness of a
network, one may want to reduce the weights of suitable edges in E of the graph G,
or eliminate certain edges; see [14].
Let ej = [0, . . . , 0, 1, 0, . . . , 0]T ∈ Rn denote the jth axis vector and consider the
matrix
Ehk = −ahk eh eTk ,
where the superscript
trix

T

denotes transposition. Regard the perturbed adjacency mae = A + εEhk ,
A

e is nonnegative. Assume
where ε > 0 is chosen small enough so that the matrix A
for the moment that the graph G associated with the adjacency matrix A is strongly
connected, i.e., that starting at any node of the graph, one can reach any other node of
the graph by traversing the edges along their directions. Then the Perron–Frobenius
theorem applies; see, e.g., [9, Chapter 8]. This is equivalent to A being irreducible.
By the Perron–Frobenius theorem, the eigenvalue of A of largest magnitude is unique
2

and equals ρ(A). This eigenvalue is commonly referred to as the Perron root of A.
Moreover, right and left eigenvectors of A associated with the Perron root,
u = [u1 , u2 , . . . , un ]T

and v = [v1 , v2 , . . . , vn ]T ,

respectively, are unique up to scaling. They can be normalized to be of unit Euclidean
norm and only have positive entries. These normalized vectors are known as the
right and left Perron vectors, respectively, of A. We define the spectral impact of the
directed edge ehk ∈ E on the spectral radius of A as the relative change of the spectral
radius ρ(A) induced by the perturbation of the edge, i.e.,
(ρ(A))

shk

=

e
ρ(A) − ρ(A)
.
ρ(A)

(ρ(A))

A first order approximation of shk
(17)] as follows. Observe that
e ≈−
ρ(A) − ρ(A)

, when 0 < ε ≪ 1, is derived in [14, Eq.

vT εEhk u
ε ahk vh uk
=
> 0,
vT u
vT u

The condition number of the largest eigenvalue ρ(A) of A is given by
κ(ρ(A)) =

1
vT u

.

Therefore,
(ρ(A))

shk

≈ αhk

ε κ(ρ(A))
,
ρ(A)

(2.1)

where
αhk = ahk vh uk .

(2.2)

Notice that the first order approximation (2.1) of the spectral impact of the edge
ehk ∈ E depends on the right and left Perron vectors of A, as well as on the weight of
the edge ehk . To make ρ(A) smaller, we may consider reducing weight(s) associated
with the largest coefficients (2.2). To determine these coefficients, one needs the
Perron vectors u and v.
When the matrix A is symmetric, it is meaningful to require the perturbation of
A also be symmetric. We therefore define the symmetric perturbation matrix
(S)

Ehk = −ahk (eh eTk + ek eTh ).
Consider the perturbed matrix
e = A + εE (S)
A
hk

(ρ(A))

for some small ε > 0. Then a first order approximation of the spectral impact shk
of the undirected edges ehk , ekh ∈ E on the spectral radius ρ(A) is given by
(ρ(A))

shk

≈ αhk
3

ε
,
ρ(A)

where we have used the fact that the right and left Perron vectors coincide, and
αhk = 2 ahk uh uk ;
see [14, Eq. (21)].
Remark 1. Let the adjacency matrix A = [ahk ]nh,k=1 ∈ Rn×n be diagonalizable, i.e., A = XΛX −1 , where the columns of X ∈ Rn×n are linearly independent
eigenvectors of A, and Λ = diag[λ1 , λ2 , . . . , λn ] contains the eigenvalues. Then
ρ(A)k ≤ kAk k ≤ κ(X)ρ(A)k ,

(2.3)

where k · k denotes the spectral matrix norm and κ(X) = kXkkX −1k is the spectral
condition number of X. In particular, when A is symmetric, we have ρ(A)k = kAk k
for all k.
A walk of length k starting at node ni and ending at node nj is a sequence of
k + 1 nodes nℓ1 , nℓ2 , . . . , nℓk+1 with nℓ1 = ni and nℓk+1 = nj such that there is an edge
eqp that points from node nℓp to node nℓp+1 for p = 1, 2, . . . , k; see [5, 15]. Edges in
a walk may be repeated. If the graph is unweighted, then the entry (i, j) of Ak equals
the number of walks of length k from node ni to node nj . In view of the bounds (2.3),
it may be a good idea to eliminate, or reduce the weight of, edges in long walks. For
weighted graphs, the entries of Ak are suitably modified.
qP
n
2
Consider the Frobenius matrix norm kAkF =
h,k=1 ahk . The inequalities

kAk ≤ kAkF and (2.3) suggest that in order to reduce ρ(A) the most, it may be a
good idea to remove nodes of G with many edges, or reduce the weights of edges that
emerge from or end at these nodes.
We conclude this section with a few illustrations for some weighted graphs that
are associated with tridiagonal adjacency matrices. First consider the case when each
node represents a person, and all persons wear the same kind of facial mask. Then
the adjacency matrix is


0 σ
O
 σ 0 σ





σ 0 ·



 ∈ Rn×n ,
· · ·
A=
(2.4)



·
·
·



· · σ 

O

σ

0

where the edge weight σ > 0 depends on properties of the mask. A high-quality mask
corresponds to a small value of σ > 0. The graph associated with the matrix (2.4) is
undirected, (strongly) connected and weighted.
Proposition 2.1. The Perron root ρ of the nonnegative symmetric tridiagonal
π
Toeplitz matrix (2.4) is 2σ cos n+1
. The Perron vector u = [u1 , u2 , . . . , un ]T , suitably
kπ
scaled, has the entries uk = sin n+1 , 1 ≤ k ≤ n. In particular, when n is odd, the
largest entry is u(n+1)/2 , and when n is even the two largest entries, un/2 and un/2+1 ,
have the same size.
Proof. Explicit formulas for eigenvalues and eigenvectors of tridiagonal Toeplitz
matrices can be found in, e.g., [19].
Note that the Perron vector in Proposition 2.1 is independent of the numerical
value σ 6= 0 of the entries of (2.4). Moreover, the Perron vector suggests that the
4

node n(n+1)/2 for n odd, and the nodes nn/2 and n(n+2)/2 for n even, are the most
important nodes of the graph; see, e.g., Bonacich [3]. This is in agreement with the
intuition that the nodes “in the middle” of the graph are the best connected nodes
and, therefore, the most important ones. According to the estimate (2.1), edges that
connect these nodes to the graph have the largest spectral impact. Consequently, to
decrease the spectral radius ρ(A) of the matrix (2.4) maximally, we should reduce the
weights of the edges ehk , ekh ∈ E, where
• h = (n + 1)/2 and k = (n + 3)/2, or h = (n + 1)/2 and k = (n − 1)/2, if n is
odd;
• h = n/2 and k = (n + 2)/2 if n is even.

Note that setting the edge-weights to zero results in a disconnected graph. It is often
meaningful to keep a small positive weight. This results in an irreducible adjacency
matrix. Properties of tridiagonal matrices with some “tiny” positive off-diagonal
entries have been studied by Parlett and Vömel [21].
Example 2.1. Let A ∈ R25×25 be the symmetric tridiagonal Toeplitz matrix
(2.4) with σ = 1. Thanks to Proposition 2.1, one easily computes the spectral radius
ρ(A) = 1.985418 and the unit norm Perron vector u. If one chooses to reduce the
weights for the edges e13,14 and e14,13 , as we suggested in the above discussion, then
obtains the perturbed adjacency matrix for a weighted graph,
e = A + εE (S) = A − ε(e13 eT + e14 eT ).
A
14
13
13,14
e = 1.973080. The spectral impact of reducing the weights
Setting ε = 0.1 yields ρ(A)
(ρ(A))
for the edges e13,14 and e14,13 is s13,14 = 0.006241, and its first order approximation
is 2 u13 u14 ε/ρ(A) = 0.007692.
If, instead, one chooses to reduce the weights for the edges e1,2 and e2,1 and
constructs the perturbed adjacency matrix
e = A + εE (S) = A − ε(e1 eT + e2 eT ),
A
2
1
1,2
(ρ(A))

e = 1.985055. Here, s
with ε = 0.1, one gets ρ(A)
1,2
0.000224.

= 0.000182 and 2 u1 u2 ε/ρ(A) =

(ρ(A))

(ρ(A))

Thus, s13,14 can be seen to be significantly larger than s1,2 . This example
shows the reduction of the spectral radius of the adjacency matrix to be much larger
when the weight of an “important” edge is reduced than when the weight of a less
important edge is reduced by the same amount. This illustrates the importance of
well-connected people wearing high-quality face mask, which correspond to a small
edge weight.

We now turn to a more accurate model of the role of facial masks. Let node vi
(o)
represent a person who wears a mask, and assume that the fraction wi of viruses
(i)
penetrates the mask from the outside in unit time, and the fraction wi penetrates
the mask from the inside in unit time. Let again the adjacency matrix A ∈ Rn×n be
(i) (o)
tridiagonal. The edge from vi to vi+1 has the weight wi wi+1 for i = 1, 2, . . . , n − 1,
(i+1)

and the edge from vi+1 to vi has the weight wi
5

(o)

wi

for i = 1, 2, . . . , n − 1. This

yields the adjacency matrix


0
 w(i) w(o)
 2 1



A=





(i)

(o)

w1 w2
0
(i) (o)
w3 w2

(i)

(o)

w2 w3
·
·

·
· ·
· ·
·

O

(i)



O

·
·
(i) (o)
wn wn−1

(i)

(o)

wn−1 wn
0






 ∈ Rn×n (2.5)




(i)

(o)

(o)

with 0 < wj , wj ≤ 1 for all j; if person k does not wear a mask, then wk = wk =
1. This model assumes that all interactions are of the same duration and that the
(o)
(i)
distance between the people is the same; a rescaling of the wj and wj is required
to model interactions of different durations and of people being at different distances
from each other. In any case, the matrix (2.5) typically is nonsymmetric.
We obtain an adjacency matrix that is simpler to analyze by projecting the matrix
(2.5) orthogonally onto the subspace T of tridiagonal Toeplitz matrices of order n.
Let T be the orthogonal projection of the matrix (2.5) onto T. Then


0

 t−1



T =





t1
0
t−1

O
t1
0
·

O

·
· ·
· ·
·

·
·

t−1

t1
0







 ∈ Rn×n ,





(2.6)

where the superdiagonal entry t1 is the average of the superdiagonal entries of the
matrix (2.5), and the subdiagonal entry t−1 is the average of the subdiagonal entries
(o)
(i)
of (2.5); see, e.g., [18]. When all wj and wj are positive, so are t1 and t−1 , and it
follows that the matrix (2.6) is irreducible.
(o)
(i)
Assume for the moment that wj = wj > 0 for all j. Then the matrices (2.5)
and (2.6) are symmetric. It follows from a result due to Bhatia [1] that if the relative
distance between these symmetric matrices (2.5) and (2.6) is small in the Frobenius
norm, then the relative difference in the spectra of (2.5) and (2.6) also is small. In
detail, let the matrices M1 ∈ Rn×n and M2 ∈ Rn×n be symmetric, and consider the
relative distance between these matrices in the Frobenius norm,
dM1 ,M2 =

kM1 − M2 kF
.
kM1 kF

Order the eigenvalues λj (M1 ) of M1 and λj (M2 ) of M2 according to λ1 (M1 ) ≥
λ2 (M1 ) ≥ . . . ≥ λn (M1 ) and λ1 (M2 ) ≥ λ2 (M2 ) ≥ . . . ≥ λn (M2 ). Then
pPn
2
i=1 (λi (M1 ) − λi (M2 ))
pPn
≤ dM1 ,M2 .
2
i=1 λi (M1 )

However, as the following example shows, the spectral radius of M1 may be much
smaller than the spectral radius of M2 , also when dM1 ,M2 is small.
6

1.5
spectrum of A
spectrum of T

1

0.5

0

-0.5

-1

-1.5
0

10

20

30

40

50

60

70

80

90

100

Fig. 2.1. Example 2.2

Example 2.2. Let A ∈ R100×100 be a symmetric tridiagonal irreducible matrix
with uniformly distributed random entries in the interval [0, 1]. These entries were
generated with the random number generator rand in MATLAB. Let T ∈ R100×100
denote the closest symmetric tridiagonal Toeplitz matrix to A. We obtain dA,T = 0.49
and
qP
100
2
i=1 (λi (A) − λi (T ))
qP
= 0.19,
100
2
i=1 λi (A)

where the eigenvalues of A and T are ordered in non-increasing order. Figure 2.1
shows the eigenvalues of A and T as functions of their index. The extreme eigenvalues
of A and T are seen not to be close. In particular, the spectral radius of T is quite a
bit smaller than the spectral radius of A.
The following result is an analogue of Proposition 2.1 for nonsymmetric tridiagonal Toeplitz matrices.
Proposition 2.2. The Perron root ρ of the nonnegative tridiagonal Toeplitz
√
π
matrix (2.6) is 2 t−1 t1 cos n+1
. The right Perron vector u = [u1 , u2 , . . . , un ]T , suitkπ
ably scaled, has the entries uk = (t−1 /t1 )k/2 sin n+1
, 1 ≤ k ≤ n. The left Perron
kπ
T
,
vector v = [v1 , v2 , . . . , vn ] , suitably scaled, has the entries vk = (t1 /t−1 )k/2 sin n+1
1 ≤ k ≤ n.
Proof. Explicit formulas for eigenvalues and eigenvectors of tridiagonal Toeplitz
matrices can be found in, e.g., [19].
Remark 2. Symmetrizing the matrix (2.6), i.e., considering an undirected graph
instead of the directed graph represented by the adjacency matrix (2.6), gives the
symmetric adjacency matrix A with Perron root


π
t−1 + t1
cos
.
ρ(A) = 2
2
n+1
Thus, the Perron root is determined by the arithmetic mean of t−1 and t1 , while the
Perron root of the matrix (2.6) is defined by the geometric mean of these quantities;
cf. Proposition 2.2.
Example 2.3. Let A ∈ R25×25 be the tridiagonal Toeplitz matrix (2.6) with
t−1 = 1.5 and t1 = 0.5. This matrix may model a situation where the probability of
7

inhaling infected droplets is three times larger than the probability of exhaling them;
e.g., people wearing chirurgical masks. Proposition 2.2 yields ρ(A) = 1.719422 and
the unit norm right and left Perron vectors u and v. It is easy to see that edge e12,13
is a maximizer of maxh,k αhk , where αhk = ahk vh uk ; see (2.1)–(2.2). In order to
reduce the weight of e12,13 , one constructs the perturbed matrix
e = A + εE12,13 = A − ε12,13 e12 eT13 .
A

e = 1.713348. Thus, the spectral impact of the perturSetting ε = 0.1, we obtain ρ(A)
(ρ(A))
bation is s12,13 = 0.003532; its first order approximation
α12,13

εκ(ρ(A))
εκ(ρ(A))
= a12,13 v12 u13
= 0.003846
ρ(A)
ρ(A)

is fairly close.
This example shows, if there only is one high-quality facial mask available, which
person should be wearing it to reduce the spectral radius the most. Notice that symmetrizing the matrix A would have given both the matrix and the results of Example
2.1.
3. Estimating and reducing the spectral radius. This section discusses
several ways to estimate the spectral radius, and the right and left Perron vectors, of
a large adjacency matrix A ∈ Rn×n . If A just is required to be nonnegative, then there
is a nonnegative vector x ∈ Rn , such that Ax = ρ(A)x. However, this vector is not
necessarily unique up to scaling; see [9, Theorem 8.3.1 and p. 505]. In this section, we
will assume that A is a nonnegative irreducible adjacency matrix. Then its right and
left Perron vectors are unique up to scaling, and can be scaled to have only positive
entries. These vectors are used to determine which edge-weights to reduce to obtain a
new adjacency matrix with, hopefully, a significantly reduced spectral radius. If our
aim just is to determine the spectral radius of A, then irreducibility is not required.
We first describe a computational method that is well suited for large networks,
whose associated adjacency matrix is nonnegative and irreducible, but does not have
other structure that can be exploited. Subsequently, we will discuss methods that are
able to use certain structural properties.
3.1. Computation of the left and right Perron vectors of a nonnegative
irreducible matrix. Let A ∈ Rn×n be a large nonnegative irreducible adjacency
matrix. The approach of this section does not exploit any additional structure that
A may posses. We determine approximations of the right and left Perron vectors of
A by the two-sided Arnoldi method. This method was first described by Ruhe [22]
and has more recently been studied and improved by Zwaan and Hochstenbach [27].
We carry out the following steps:
• Apply the two-sided Arnoldi method to A to compute the Perron root ρ(A),
and the unit right and left Perron vectors u and v, respectively, with positive
entries.
• Let
E = vuT .
The Perron root ρ(A + εE) of the matrix A + εE satisfies
ρ(A + εE) = ρ(A) + ε
8

vT Eu
+ O(ε2 )
vT u

(3.1)

for |ε| sufficiently small; see Wilkinson [25, Chapter 2]. We refer to the matrix
(3.1) as a Wilkinson perturbation. This is the worst perturbation for ρ(A) in
the following sense. For any nonnegative matrix E with kEk = 1, one has
|vT Eu|
kvkkEkkuk
1
vT Eu
=
≤
= T ,
T
v u
vT u
vT u
v u
with equality for the matrix (3.1). Moreover,
ρ(A + εE) − ρ(A) ≈ ε

vT Eu
= εκ(ρ(A)).
vT u

We will let ε > 0.
Note that the spectrum of A + εE may be considered a very sparse approximation of the ε-pseudospectrum of A. The size of ε used in the computations
may depend on whether the adjacency matrix is contaminated by errors. For
instance, the edge weights may not be known exactly; see Trefethen and
Embree [24] for insightful discussions on pseudospectra.
• Typically, the first order approximation
ρ(A) + ε

vT Eu
vT u

of ρ(A + εE) is sufficiently accurate. In the rare occasions when this is
not the case, we can compute an improved approximation by applying the
(standard) Arnoldi method described, e.g., by Saad [23], or the implicitly
restarted (standard) Arnoldi method described in [13] and implemented by
the MATLAB function eigs.
We note that the perturbed matrix A + εE is nonnegative and irreducible if
this holds for A. Indeed, if all entries of the Perron vectors are positive, then
so are all entries of A + εE for ε > 0.
The Perron root ρ(A + εE) is a rightmost ε-pseudoeigenvalue of A. We note that
ρ(A + εE) may be much larger than ρ(A) when the Perron root is ill-conditioned, i.e.,
when vT u is small.
The analysis in Section 2 suggests that in order to reduce ρ(A) by removing an
edge of G, we should choose an edge ehk with a large weight ahk that corresponds to a
large entry of the matrix vuT in (3.1); see (2.1)–(2.2). Removing an edge corresponds
to setting its edge-weight to zero. We can in the same manner choose which edgeweight to reduce to a a smaller positive value in order to reduce the spectral radius.
Example 3.1. Consider a matrix T ∈ R10×10 of the form (2.6) with t−1 = 0.1
and t1 = 1. The eigenvalues of T are real and appear in ± pairs. Thus, there are two
eigenvalues of largest magnitude. The positive one is about 0.6. Now we add suitable
entries in the (1, 10) and (10, 1) positions to transform T into a circulant matrix Tc .
Then ρ(Tc ) = 1.1 and Tc also has the eigenvalue −1.1. The remaining eigenvalues
are complex-valued.
The large perturbation induced in the spectrum of T by the perturbation can be
explained by analyzing the structure of the matrix E in (3.1), which we construct by
using the left and right Perron vectors given in Proposition 2.2. Figure 3.1 visualizes
the size of the entries in E. Notice that the largest entries are confined to the bottom
left corner. Thus, adding the entry 1 in the sensitive position (10, 1) induces a large
perturbation in the Perron root.
9

1

0.9

2

0.8

3

0.7

4
0.6

5
0.5

6
0.4

7
0.3

8
0.2

9
0.1

10
1

2

3

4

5

6

7

8

9

10

Fig. 3.1. Example 3.1

When the adjacency matrix A is very large, we may consider replacing the vectors
u and v in (3.1) by the vector e = √1n [1, 1, . . . , 1]T and compute ρ(A) and ρ(A+ε e eT )
by the (standard) Arnoldi or restarted Arnoldi methods to determine the structural
robustness of the graph with adjacency matrix A. This approach was applied in [20]
to estimate pseudospectra of large matrices.
The large perturbation in ρ(A) illustrated in Example 3.1 would not have occurred
if the sparsity structure of the matrix T would have been taken into account, i.e., if one
only would allow perturbations of positive edge-weights. We therefore are interested in
determining perturbations εE of A that take the sparsity structure of A into account.
3.2. Approximation of the spectral radius taking the sparsity structure
into account. The method in this subsection is suitable when it is desirable that
the perturbation εE of the adjacency matrix A has the same sparsity structure as
A. Let S denote the cone of all nonnegative matrices in Rn×n with same sparsity
structure as A, and let M |S be the matrix in S that is closest to a given nonnegative
matrix M with respect to the Frobenius norm. It is straightforward to verify that the
matrix M |S is obtained by replacing all the entries of M outside the sparsity structure
by zero. This approach takes possible uncertainty of the available edge-weights into
account. The analysis in [20] lead to the following numerical method:
• Apply the two-sided Arnoldi method to A ∈ S to compute the Perron root
ρ(A), as well as the unit right and left Perron vectors u and v, respectively,
with positive entries.
• Project vuT into S and normalize the projected matrix to have unit Frobenius
norm. Let
E=

vuT |S
.
kvuT |S kF

(3.2)

We refer to the matrix (3.2) as an S-structured analogue of the Wilkinson
perturbation. This is the worst S-structured perturbation for ρ(A). For any
E ∈ S with kEkF = 1, we have that
|vT Eu|
kvkkvuT |S kF kuk
kvuT |S kF
vT Eu
=
≤
=
,
vT u
vT u
vT u
vT u
10

with equality for the matrix (3.2); see [16]. Hence,
ρ(A + εE) − ρ(A) ≈ ε

vT Eu
= εκS (ρ(A)),
vT u

where
κS (ρ(A)) =

kvuT |S kF
vT u

denotes the S-structured condition number of ρ(A); see [16, 12]. We let ε > 0.
Similarly as above, the spectrum of A + εE is a very sparse approximation of
the S-structured ε-pseudospectrum of A; see, e.g., [20].
• If desired, compute ρ(A + εE) by the (standard) Arnoldi or restarted Arnoldi
methods. We note that the perturbed matrix A + εE is nonnegative and
irreducible if this holds for A, and exhibits the same sparsity structure as A.
The Perron root ρ(A + εE) helps us to estimate the structural robustness of the
network. Indeed, it represents an approximate S-structured ε-pseudospectral radius
of the S-structured ε-pseudospectrum of the adjacency matrix A ∈ S.
We note that ρ(A + εE) may be much larger than ρ(A) when the Perron root has
a large S-structured condition number κS (ρ(A)).
As mentioned above, in case the network is very large, we may consider replacing
the vectors u and v in (3.2) by the vector e. An analogous S-structured perturbation
of the adjacency matrix A is given by
eeT |S
.
keeT |S kF
We may apply the (standard) Arnoldi or implicitly restarted Arnoldi methods to
estimate ρ(A) and


eeT |S
.
ρ A+ε
keeT |S kF
This approach has been applied in [20] to estimate structured pseudospectra of large
matrices.
3.3. Approximation of the spectral radius for perturbations of tridiagonal Toeplitz matrices. Structure respecting projections, analogous to the ones
discussed in the above subsection, also can be applied to impose other structures. This
subsection illustrates how they can be used to impose tridiagonal Toeplitz structure.
Let T be a nonnegative tridiagonal Toeplitz matrix (2.6). We denote by T the cone of
all nonnegative tridiagonal Toeplitz matrices with zero diagonal in Rn×n and by M |T
the matrix in T closest to a given nonnegative matrix M ∈ Rn×n with respect to the
Frobenius norm. It is straightforward to verify that M |T is obtained by replacing the
sub- and super- diagonal entries of M by their respective arithmetic mean.
To approximate the spectral radius of T ∈ T , we carry out the following steps:
• Apply the formulas in Proposition 2.2 to T to compute the Perron root ρ(T )
and the unit right and left Perron vectors u and v, respectively, with positive
entries.
• Project vuT into T and normalize the projected matrix to have unit Frobenius
norm. Let
E=
11

vuT |T
.
kvuT |T kF

(3.3)

We refer to the matrix (3.3) as a T -structured analogue of the Wilkinson
perturbation. Similarly as above, we have for any E ∈ T with kEkF = 1, that
|vT Eu|
kvkkvuT |T kF kuk
kvuT |T kF
vT Eu
=
≤
=
,
vT u
vT u
vT u
vT u
with equality for the matrix (3.3); see [17]. It follows that
ρ(T + εE) − ρ(T ) ≈ ε

vT Eu
= εκT (ρ(T )),
vT u

where
κT (ρ(T )) =

kvuT |T kF
,
vT u

denotes the T -structured condition number of ρ(T ); see [17, 12].
We will let ε > 0. The spectrum of T + εE is a sparse approximation of the
T -structured ε-pseudospectrum of T ; see, e.g., [20].
• Determine ρ(T + εE) by applying Proposition 2.2 to T + εE. The latter
matrix is nonnegative and irreducible if this holds for T , and exhibits the
same structure as T .
The Perron root ρ(T + εE) may be regarded as an approximate T -structured
ε-pseudospectral radius and provides an estimate of the structural robustness of the
structured network. It may be much larger than ρ(T ). It is known that when considering the class T of tridiagonal Toeplitz matrices the most ill-conditioned eigenvalues
with regard to T -structured perturbations are the eigenvalues of largest magnitude;
see, e.g., [19]. We remark that an algorithm for computing the T -structured pseudospectrum of a tridiagonal Toeplitz matrix and its rightmost pseudoeigenvalue is
described in [4]. However, the computational cost of this algorithm can be quite large
for the matrices considered in this paper.
Finally, replacing u and v in (3.3) by the vector e as described above is particularly
efficient when the considered subspace is T ; see Section 4.2.2.
4. Numerical tests.
4.1. Complex networks.
4.1.1. Air500. Consider the adjacency matrix A ∈ R500×500 for the network
Air500, which describes 24009 flight connections between the top 500 airports within
the United States based on total passenger volume during one year from July 1, 2007,
to June 30, 2008; see [2]. Thus, the airports are nodes and the flights are edges in
the graph determined by the network. The matrix A has the entry ai,j = 1 if there
is a flight that leaves from airport i to airport j. Generally, but not always, ai,j = 1
implies that aj,i = 1. This makes A close to symmetric.
Apply the computational steps described in Section 3.1. The Perron root ρ(A) is
82.610276 with eigenvalue condition number κ(ρ(A)) = 1.001668. Let ε = 0.5. The
Perron root ρ(A+εE), where E is the matrix in (3.1), is 83.111096. Thus, the spectral
radius increases by 0.500820, as we could have foreseen since εκ(ρ(A)) = 0.500834.
The value ρ(A+εE) is an accurate approximation of the ε-pseudospectral radius. This
is seen by determining the ε-pseudospectral radius by the MATLAB program package
Eigtool [26]. Our approximation of the ε-pseudospectral radius agrees with the value
determined by Eigtool in all decimal digits returned by Eigtool. Pseudospectra of A
are visualized in Figure 4.1.
12

0.5
30
0
20
-0.5
10
-1
0
-1.5
-10
-2
-20
-2.5
-30
-20

0

20

40

60

80

-3

Fig. 4.1. Pseudospectra of the adjacency matrix of the network in Section 4.1.1. The contour
levels are from 10−3 to 100.5 . The figure is produced by EigTool.

Assume we are interested in removing a single route so that the structural robustness of the network is increased the most. Then this route should be an edge that
maximizes ah,k vh uk over h and k; see (2.1)–(2.2). For the present network, we find
b so obtained is
that the edge e224,24 ∈ E should be removed. The adjacency matrix A
b = 82.590199. The edge e224,24 corresponds to flights from the
irreducible with ρ(A)
JFK airport in New York to the Hartsfield–Jackson airport in Atlanta.
Finally, we observe that if one replaces E in (3.1) by the matrix of all ones
normalized to have unit Frobenius norm, the increase of spectral radius ρ(A) results
to be 0.255450. Thus, this perturbation gives a significantly less accurate estimate of
the sensitivity of ρ(A) to worst-case perturbations.
4.1.2. Airlines. Consider the adjacency matrix A ∈ R235×235 determined by
the network Airlines with 235 nodes and 2101 edges. The nodes represent airports
and the directed edges represent flights between them. This network is available at
[8].
Computations described in Section 3.1 yield ρ(A) = 26.545430 and the condition
number κ(ρ(A)) = 1.005219. Let ε = 0.5. The Perron root ρ(A + εE), where E is
the matrix (3.1), is 27.047941. Thus, the spectral radius increases by 0.502511, as
we could have expected since εκ(ρ(A)) = 0.502609. The spectral radius ρ(A + εE)
approximates the ε-pseudospectral radius and coincides with the digits with the value
returned by Eigtool. Pseudospectra of A are shown in Figure 4.2.
The route to remove, in order to increase the structural robustness of the network
b obtained when
most is represented by the edge e51,137 ∈ E. The adjacency matrix, A,
b
setting the entry a51,137 of A to zero is irreducible with ρ(A) = 26.452922.
Finally, we observe that if one replaces E in (3.1) by the matrix of all ones,
normalized to have unit Frobenius norm, ρ(A) increases to 0.223135.
4.2. Synthetic networks. This subsection considers projections of the adjacency matrix for the Air500 network.
4.2.1. The tridiagonal part of Air500. We set all entries of the adjacency
matrix for the Air500 network outside the tridiagonal part of the adjacency matrix
to zero. The number of flight connections is now 144. This yields a nonsymmetric
13

0

-0.5
10

-1

5

0

-1.5

-5
-2

-10

-2.5
-15

-10

-5

0

5

10

15

20

25

-3

Fig. 4.2. Pseudospectra of the adjacency matrix of the network in Section 4.1.2. Contour
curves are for levels from 10−3 to 100 . The graph is produced by EigTool.

tridiagonal matrix A ∈ R500×500 . Carry out the computations described in Section
3.2, with S the subspace of all tridiagonal matrices with zero-diagonal in R500×500 .
This yields the Perron root ρ(A) = 1.801938 and its S-structured condition number
is κS (ρ(A)) = 0.613714.
Let ε = 0.9. The Perron root ρ(A + εE), where E is the matrix in (3.2), is
2.362116. Thus, the spectral radius increases by 0.560178, as we could have foreseen
since εκS (ρ(A)) = 0.552342.
Computations similar to those of Subsection 4.1 suggest that in order to increase
the structural stability the most by removing one edge, we should choose the edge
e494,493 or e493,494 in E. However, removal of one or both of these edges would result in
a graph with a reducible adjacency matrix. To preserve irreducilbility of the adjacency
matrix, one may instead schedule fewer flights on the routes that correspond to the
edges e494,493 and e493,494 . This reduces the weight associated with these edges.
Finally, we observe that, if one replaces the matrix vuT in (3.2) by the matrix of
all ones, normalized to be of unit Frobenius norm, then the spectral radius increases by
0.052606. Clearly, this is not an accurate estimate of the actual worst-case sensitivity
of ρ(A) to perturbations.
4.2.2. Projection of Air500 into a tridiagonal Toeplitz structure. We
construct a tridiagonal Toeplitz matrix with zero-diagonal T ∈ R500×500 by averaging
the sub- and super- diagonals of the matrix in Section 4.2.1. Then we carry out the
computations as described in Section 3.3, and make use of Proposition 2.2. Then
ρ(T ) = 0.288460 and its T -structured condition number is κT (ρ(T )) = 0.063357.
Let ε = 0.9. Then ρ(T + εE) = 0.345466, where E is the matrix in (3.3). Thus,
the spectral radius increases by 0.057006. This is in agreement with εκT (ρ(T )) =
14

0.057021.
Finally, we observe that, if one replaces the matrix vuT in (3.2) by the matrix of
all ones, scaled to be of unit Frobenius norm, then ρ(T ) increases by 0.056995. Thus,
the latter perturbation provides a very accurate estimate of the spectral radius when
the matrix vuT in (3.3) is used.
5. Conclusion. It is important to be able to estimate the structural robustness
of a network, and to determine which nodes to remove or weights to decrease to
increase the structural robustness. This paper describes several iterative methods
that can be applied to fairly large networks to gain insight into these issues. Both the
sensitivity of the structural robustness to worst-case Wilkinson perturbations and to
structured perturbations are discussed and illustrated.
Acknowledgment. The authors would like to thank Ian Zwaan for MATLAB
code for the two-sided Arnoldi method used in the numerical experiments.
REFERENCES
[1] R. Bhatia, The distance between the eigenvalues of Hermitian matrices, Proc. Amer. Math.
Soc., 96 (1986), pp. 41–42.
[2] Biological
Networks
Data
Sets
of
Newcastle
University.
Available
at
http://www.biological-networks.org/
[3] P. Bonacich, Power and centrality: A family of measures, Am. J. Sociol., 92 (1987), pp. 1170–
1182.
[4] P. Buttà, N. Guglielmi, and S. Noschese, Computing the structured pseudospectrum of a
Toeplitz matrix and its extreme points, SIAM J. Matrix Anal. Appl., 33 (2012), pp. 1300–
1319.
[5] E. Estrada, The Structure of Complex Networks: Theory and Applications, Oxford University
Press, Oxford, 2011.
[6] M. Gandhi and L. C. Marr, Uniting infectious disease and physical science principles on
the importance of face masks for COVID-19, Med Commentary, 2 (2021), pp. 21–32.
https://doi.org/10.1016/j.medj.2020.12.008
[7] M. Gandhi and G. W. Rutherford, Facial masking for Covid-19 – potential for “variolation” as we await a vaccine, N. Engl. J. Med., 383; 18 (Oct. 29, 2020).
https://www.nejm.org/doi/pdf/10.1056/NEJMp2026913?articleTools=true
[8] Gephi Sample Data Sets, http://wiki.gephi.org/index.php/Datasets
[9] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University Press, Cambridge, 1985.
[10] J. Howard et al., An evidence review of face masks against COVID-19, PNAS, 118 (2021), Art.
e2014564118.
[11] A. Jamakovic, R. E. Kooij, P. Van Mieghem, E. R. van Dam, Robustness of networks against
viruses: The role of the spectral radius, in Symposium on Communications and Vehicular
Technology, 2006, pp. 35–38.
[12] M. Karow, D. Kressner, and F. Tisseur, Structured eigenvalue condition numbers, SIAM J.
Matrix Anal. Appl. 28 (2006), pp. 1052–1068.
[13] R. B. Lehoucq, D. C. Sorensen, and C. Yang, ARPACK Users’ Guide: Solution of LargeScale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods, SIAM, Philadelphia, 1998.
[14] A. Milanese, J. Sun, and T. Nishikawa, Approximating spectral impact of structural perturbations in large networks, Phys. Rev. E, 81 (2010), Art. 046112.
[15] M. E. J. Newman, Networks: An Introduction, Oxford University Press, Oxford, 2010.
[16] S. Noschese and L. Pasquini, Eigenvalue condition numbers: Zero-structured versus traditional,
J. Comput. Appl. Math., 185 (2006), pp. 174–189.
[17] S. Noschese and L. Pasquini, Eigenvalue patterned condition numbers: Toeplitz and Hankel
cases, J. Comput. Appl. Math., 206 (2007), pp. 615–624.
[18] S. Noschese, L. Pasquini, and L. Reichel, The structured distance to normality of an irreducible
real tridiagonal matrix, Electron. Trans. Numer. Anal., 28 (2007), pp. 65–77.
[19] S. Noschese, L. Pasquini, and L. Reichel, Tridiagonal Toeplitz matrices: Properties and novel
applications, Numer. Linear Algebra Appl., 20 (2013), pp. 302–326.
15

[20] S. Noschese and L. Reichel, Approximated structured pseudospectra, Numer. Linear Algebra
Appl., 24 (2017), Art. e2082.
[21] B. Parlett and C. Vömel, The spectrum of a glued matrix, SIAM J. Matrix Anal. Appl., 31
(2009), pp. 114–132.
[22] A. Ruhe, The two-sided Arnoldi algorithm for nonsymmetric eigenvalue problems, in Matrix
Pencils, eds. B. Kågström and A. Ruhe, Springer, Berlin, 1983, pp. 104–120.
[23] Y. Saad, Numerical Methods for Large Eigenvalue Problems, 2nd ed., SIAM, Philadelphia,
2011.
[24] L. N. Trefethen and M. Embree, Spectra and Pseudospectra, Princeton University Press, Princeton, 2005.
[25] J. H. Wilkinson, The Algebraic Eigenvalue Problem, Clarendon Press, Oxford, 1965.
[26] T. G. Wright, EigTool. http://www.comlab.ox.ac.uk/pseudospectra/eigtool/, 2002.
[27] I. N. Zwaan and M. E. Hochstenbach, Krylov–Schur-type restarts for the two-sided Arnoldi
method, SIAM J. Matrix Anal. Appl., 38 (2017), pp. 297–321.

16

