ORG FAQ:

A New Dataset and Analysis on Organizational FAQs and User
Questions

Guy Lev, Michal Shmueli-Scheuer, Achiya Jerbi∗, David Konopnicki
IBM Research AI - Haifa
{guylev,shmueli,davidko}@il.ibm.com, achiyaj@gmail.com

arXiv:2009.01460v1 [cs.CL] 3 Sep 2020

Abstract
Frequently Asked Questions (FAQ) webpages
are created by organizations for their users.
FAQs are used in several scenarios, e.g., to answer user questions. On the other hand, the
content of FAQs is affected by user questions
by definition. In order to promote research in
this field, several FAQ datasets exist. However,
we claim that being collected from community
websites, they do not correctly represent challenges associated with FAQs in an organizational context. Thus, we release ORG FAQ, a
new dataset composed of 6988 user questions
and 1579 corresponding FAQs that were extracted from organizations’ FAQ webpages in
the Jobs domain. In this paper, we provide
an analysis of the properties of such FAQs,
and demonstrate the usefulness of our new
dataset by utilizing it in a relevant task from
the Jobs domain. We also show the value of
the ORG FAQ dataset in a task of a different
domain - the COVID-19 pandemic.

1 Introduction
FAQs are used by organizations (e.g., enterprises,
governments, educational institutions) in several
scenarios, e.g., to answer user questions in websites or bots interactions. On the other hand, the
content of FAQs is affected by user questions by
definition. In customer support scenarios, FAQs
help reduce support agents workload, improve
user experience and more. In fact, FAQ pages are
used by 73% of users1 .
FAQs are usually formalized by organizations’
experts and reflect the organizational voice. They
are well-written and focused. We refer to those
FAQs as organizational FAQs, i.e. organizational
questions and answers.
Questions written by users w.r.t. services and
products appear on the web in various contexts:
∗
1

This work was done while working at IBM.
https://tinyurl.com/uk9qwv2

User question: Oh no! Ive spilled wine over my favourite
rug and the stain is not coming off! What should I do?
Community question: how do i remove a red wine
spillage on our new carpet?!! thanx?
Organizational question: How to remove a wine stain
from carpet?

Figure 1: Examples of user question, community FAQ,
and an organizational FAQ, on “wine stain removal” issue.

questions are being sent to search-engines and
bots, posted on web forums and sent by email. We
refer to those questions as user questions. Specifically, in web forums, such as Yahoo! Answers and
Reddit, user questions are answered by other users
or, sometimes, by organizations’ representatives.
Selected questions and answers from these forums
constitute community FAQs (community questions
and answers), as opposed to the official organizational FAQs. Note that the community questions
are a subset of user questions.
Several research FAQ datasets exist, based on
data extracted from web forums. Older datasets
like U SENET FAQ2 , and Game FAQ3 only contain
FAQs (questions and answers). Recent datasets
as FAQIR (Karan and Šnajder, 2016) and S TACK FAQ (Karan and Šnajder, 2018) associate FAQ
questions with additional user questions having
similar meaning. Those user questions are either extracted from forums or created using crowdsourcing. Those richer datasets can be used for
different tasks like question answering, text generation and more.
Figure 1 shows a user question and a related
community FAQ4 , both from the FAQIR dataset,
2

https://tinyurl.com/wsywgpg
www.gamefaqs.com/
4
For clarity, in this paper, the term FAQ refers only to the
question, while the answer is explicitly referred to as the FAQ
answer.
3

along with a relevant organizational FAQ from a
cleaning-products company. The community FAQ
contains uncommon terminology (“wine spillage”
vs. “wine stain”), extra information (“new carpet” vs. “carpet”), grammatical errors (“on” vs.
“from”), etc.
We argue that the language used in community FAQs and user questions is different from
the language of organizational FAQs. Hence, existing FAQ datasets, which are based on community FAQs, are unsuitable for some tasks: for example, question answering using organizational
FAQs, or updating organizations’ FAQ pages so
they are in tune with users’ current questions. In
those cases, the language mismatch between user
questions and organizational FAQs is an issue not
addressed by current datasets.
Hence, our main contributions are as follows:
(1) we study the difference between organizational
FAQs, community FAQs and user questions; (2)
we create and publish a new dataset, ORG FAQ
(acronym for organizational FAQ) , containing
6988 user questions mapped to 1579 organizational FAQs from the Jobs domain5 ; (3) we demonstrate the usefulness of this dataset by automatically generating organizational FAQs given user
questions, and evaluating the results for the Jobs
domain as well as for the urging COVID-19 pandemic domain. To our knowledge, this is the first
work that studies these important aspects of organizational FAQs.

2 Related Work
Many works on FAQs focus on the question answering task, usually given user queries.
Early works (Burke et al., 1997; Hammond et al.,
1995) use semantic features to match questions
and answers. Sakata et al. (2019) presents a
method for using question similarity and BERTbased question-answer relevance in FAQ retrieval.
Xie et al. (2019) uses a knowledge graph-based
Q&A framework to improve questions understanding and answers retrieval, and evaluates it on
a new community FAQ dataset6 . Some recent
works explore customer use of FAQs in the context of e-commerce7 : Cui et al. (2017) shows
how to search for customer questions in existing
FAQ pairs. Kulkarni et al. (2019) focuses on deep
5

Data will be available upon paper acceptance.
6
Not available yet.
7
Not publicly available.

learning-based ranking model and ontology-based
matching to answer question on products. All
these previous works use community FAQ datasets
(either public or not), whereas our work focuses
on organizational FAQs, and differences between
them and user questions.
Most relevant to our work is Razzaghi et al.
(2016) which studies community FAQs and user
questions. They collected around 3000 questions
from forums into FAQ and non-FAQ classes. Then,
they analyzed different features, and found that
named entities and personal pronouns are good discriminators between the classes. The main difference with our work is that they focused on community FAQs, while we focus on organizational
FAQs, and we further study the differences between community and organizational FAQs.
The task of FAQs generation was studied in
previous works (Raazaghi, 2015; Bihani, 2018).
Those works aim at generating community FAQs
from online forum whereas we target organizational FAQs.
Evaluation of different aspects of pragmatics are explored in several other works.
Rao and Tetreault (2018) studies differences
between formal and informal questions. Aspects
such as informativeness and implicatures are
presented in Lahiri (2015a); Lahiri et al. (2011).
Recently, it is worth noting some relevant questions datasets related to the COVID-19 pandemic.
CovidQA (Tang et al., 2020) comprises 124 questions associated with answers from scientific articles, and the task is to identify the answer passage within the article. The questions were created
by medical experts (epidemiologists, medical doctors, and medical students), and were curated from
scientific articles. In COVID-Q (Wei et al., 2020)
the authors present a set of 1690 questions about
COVID-19 from different sources and clustered
into various categories. Most of their questions
are extracted from FAQ pages. Some questions
were extracted from Quora, as well as by keyword
search on search engines. Although some of their
question sources are similar to ours, the focus of
their work is different.

3 FAQ Analysis
3.1 User question vs. Organizational FAQ
To study the differences between organizational
FAQs and user questions, and since there is no
publicly available dataset of organizational FAQs,

we collected a large dataset of the two classes.
To do so, we made use of the FAQPage schema8
defined by the Schema.org community, that publishes standard schemas for structured data on the
web. This schema is used by organization experts
to specify FAQs and answers in their websites,
and thus, can be treated as ground-truth organizational FAQs. We collected 19474 FAQs from
2514 organization websites. For the user question class, we used existing datasets that include
user questions from either forums or search logs.
Those questions were all written by users, not
representing any organization, and thus represent
ground-truth for user questions. In order to create
a balanced dataset, we randomly selected around
3200 questions from Quora9 , Yahoo! Answers
L510 , Reddit11 , forums including Ubuntu, Stackoverflow, Stats and Unix (Azzopardi et al., 2019),
and WikiQA (Yang et al., 2015) datasets. The data
collection process ensures that both classes contain questions from various domains.
We train a Q UESTION - TYPE classifier as follows: we split the collected data into training
(60%), validation (20%) and test (20%) sets, and
train an RNN model as in (Howard and Ruder,
2018), using a pre-trained, 300-dimensional
GloVe word embedding (Pennington et al., 2014).
The hyper-parameters of the training are: RNN
hidden layer size is 256, dropout is not applied,
Adam optimizer is used with learning rate of 3e-5
and batch size is 256. This classifier’s accuracy on
the test set is 91.8%, implying that there is indeed
an inherent difference between the two classes: organizational FAQs and user questions.

FAQs. We evaluate these community FAQs using
the following metrics:
Organizational FAQ Percent.
Using our
Q UESTION - TYPE classifier , we predicted how
many of the FAQs in the two datasets were
likely to be organizational FAQs, and report the
percentage of them.
Formal Percent. We trained an RNN classifier
(with same hyper-parameters as described for the
Q UESTION - TYPE classifier) on the S QUINKY
dataset (Lahiri, 2015b), which includes sentences
labeled by their formality level, and used it to
classify the FAQs in each dataset12 . We report the
percentage of FAQs classified as “formal”.
Grammar Errors.
Following Kantor et al.
(2019), we used the Grammarly13 grammatical
error correction system in order to count the
number of errors per FAQ. We report the average
number of grammatical errors.
Readability.
We used textstat14 implementation for Flesch-Kincaid grade level (F-K
grade) (Kincaid, 1975) to assess readability of
the FAQs, and report mean grade. The common
wisdom for web content is average F-K grade of
up to 715 , and on social media it is reported to be
lower (4 on BuzzFeed, and 2.5 on Facebook)16 .

3.2

Table 1 summarizes the evaluation of FAQIR
and S TACK FAQ datasets. In both, a low percentage of community FAQs were classified as organizational FAQs. In FAQIR the majority of questions are not formal, and grammar error rate is
high. Both datasets, especially S TACK FAQ, suffer from poor FAQ readability. This analysis emphasizes the need of a new dataset that better models a professional user-support setting: users pose
questions in their own words, and such questions
should be matched against organizational FAQs in

Community FAQ vs. Organizational FAQ

Given that we are able to detect organizational
FAQs, we next try to evaluate whether community
FAQs indeed differ from organizational FAQs.
Existing FAQ datasets contain FAQs, to which
related user questions are matched. FAQIR is
composed of 4313 FAQs and 1233 additional user
questions in the “maintenance & repair” domain,
taken from Yahoo! Answers. S TACK FAQ is composed of 719 FAQs and 1250 user questions in the
“web app” domain, taken from the StackExchange
website. In both datasets, the sources of all FAQs
are web forums. Hence, these are community
8

https://schema.org/FAQPage
https://tinyurl.com/yx2o2uo3
10
https://tinyurl.com/thtjgpf
11
https://tinyurl.com/sl2uq38
9

Metric
Organizational FAQ Percent
Formal Percent
Grammar Errors (mean)
Readability F-K Grade (mean)

FAQIR
7
44
1.70
6

S TACK FAQ
39
86
0.17
4.86

Table 1: Descriptive metrics for community FAQs.

12

Similarly to (Pavlick and Tetreault, 2016), we consider
sentences with formality level > 3.75 as “formal”, and those
with formality level < 3.25 as “informal”.
13
app.grammarly.com
14
https://pypi.org/project/textstat/
15
https://tinyurl.com/we6om8c
16
https://tinyurl.com/qnl5naa

order to find appropriate answers and, on the other
hand, frequent user-questions can be gathered and
used to generate organizational FAQs.

4 The ORG FAQ Dataset
4.1

Data Collection

ORG FAQ

is composed of organizational FAQs
and user questions. Collecting FAQs from organization websites that have enough corresponding
user questions in community sources is a major
challenge. Since organizational FAQs are usually focused on a specific services or products, it
is very difficult to find corresponding user questions, as communities typically focus only on a
small set of very popular products. Interestingly,
many organizations maintain an FAQ page that
focuses on Jobs. Such FAQ pages include questions which are general and common across organizations, e.g.: “What should I expect in my
interview?”. This makes the Jobs domain an appropriate source from which a sufficiently large
dataset can be drawn. Thus, we collected jobrelated FAQs from 170 organizations’ websites,
yielding a total of 1688 FAQ questions and answers. In parallel, we found two relevant subreddits “/r/jobs/” and “/r/careerguidance/” where
users discuss related recruitment issues. In total,
around 134K user questions were collected. All
organization names appearing in the obtained data
were replaced by a special “ORG NAME” token.
4.2

Dataset Creation

Given the sets of collected organizational FAQs
and user questions, our goal was to create a highquality dataset in which each organizational FAQ
is associated with one or more corresponding user
questions. Note that not each organizational FAQ
in the collected data had necessarily a matching
user question, and vice versa. Thus, we created
the ORG FAQ dataset in two main stages, namely
automatic annotation followed by human annotation, as described in the following sub-sections.
4.2.1

Automatic Annotation

We created automatically-labeled data by associating each of the collected FAQs with three user
questions. This was done as follows:
Step 1. The entire set of 134K user questions
was indexed in a full-text index. For each organizational FAQ, up to 10 best-matching user questions were retrieved from this index. The FAQ

question itself served as the query, and the BM25
(Robertson et al., 2009) metric was used for the retrieval.
Step 2. We wrote a simple software tool for manually labeling the data. The tool presents to the
labeler a series of organizational FAQs, each with
the (up to 10) best-matching candidate user questions, according to the BM25 metric. Now, the
human labeler annotates each candidate user questions as matching or not. This way, the authors
of this paper manually labeled 1521 pairs, out of
which, 268 pairs were labeled as matching.
Step 3. Using the labeled data obtained in the
previous step, we fine-tuned a pre-trained BERT
model (Devlin et al., 2018), for the task of classifying whether a pair of an organizational FAQ and
a user question conveys the same meaning or not.
Step 4. Finally, the fine-tuned BERT model was
used for drawing the top-ranked user questions,
per organizational FAQ. Due to running time considerations, we first used BM25 to retrieve the
top-ranked 1000 user questions, for each organizational FAQ. We then re-ranked the retrieved user
questions using the fine-tuned BERT model, and
selected the top-three questions.
4.2.2 Human Annotation
The automatically-labeled data created at the previous step might be noisy. To ensure the quality
of the dataset, we defined the following crowdsourcing task: each organizational FAQ was presented to a crowd annotator along with the three
retrieved candidate user questions associated with
it at the previous step. For each candidate pair of
FAQ and user question, the annotators had to decide whether they convey the same meaning, and if
not, they had to rewrite the user question with minimal changes, so that it will match the FAQ. Asking the annotators to rephrase the user question,
rather than writing a new one, carries a few benefits: writing requires more creativity than rephrasing, and hence, might result in sentences of low
diversity, or very simple ones (Jiang et al., 2017).
In addition, we wanted to retain the language style
of the original user question (including syntax, typos, grammar, etc.) as much as possible. Each pair
of FAQ and user question was annotated by three
crowd annotators. We used the Appen platform17 ,
and in order to ensure high quality data, we required only English native speakers who belong to
17

https://appen.com/

Organizational FAQs
’how’:20.5
’what’:18.9
’i’:12.0
’can’:9.0
’do’:7.5
’is’:3.9
’will’:3.6
’when’:3.0
’does’:3.0
’where’: 2.7
’if’: 2.5
’why’: 2.1
’are’: 2.1
91.7
1805
11.99

First word distribution
(above 2%, in%)

Total (above 2%, in%)
Vocabulary size (unique words)
Mean question length (words)

User questions
’how’:21.5
’what’:15.6
’should’:8.1
’is’: 6.8
’can’: 5.7
’i’: 5.1
’do’: 3.2
’when’: 2.9
’where’: 2.3

71.2
2538
10.08

User questions
- how to apply for position on your website that is not currently available
- am still really being considered for the
position posted on your website
- still waiting on an interview for a position posted on your website
- how to know if a position is still available?
- what is the minimum age to work in
the company?
- what minimum age should i have to
apply?
- minimum age to work
- what is the minimum age to get the
job?
- should apply again after being rejected
- i can run again, i was previously rejected

Table 2: Statistics of ORG FAQ dataset.
Metric
Organizational FAQ Percent
Formal Percent
Grammar Errors (mean)
Readability F-K Grade (mean)

Organizational FAQs
79
90
0.08
8.2

User questions
60
74
0.30
6.62

Table 3: Descriptive metrics for ORG FAQ dataset.

the Highest Quality group (smallest group of most
experienced, highest accuracy contributors). The
inter-rater reliability, measured by Fleiss’s Kappa,
was 0.76, indicating a high level of agreement. In
addition, we randomly sampled 10% of the pairs
to be evaluated by two of the authors of this paper. The authors have found 88% of the sentences
to be valid, verifying the high quality of the data.
In total, 6988 pairs were created, and the number
of user questions mapped to the same FAQ varies
between 1 and 10, with an average of 4.4.
4.3

Dataset Analysis

Analysis of the organizational FAQs and user questions in the ORG FAQ dataset are summarized in
Table 2 and Table 3.
Table 2 provides statistics about the dataset w.r.t
vocabulary, length, etc. The first row captures
the question type distribution, by showing the first
word distribution for words with frequency above
2% (for example, 20.5% of the FAQs start with
’how’), which sums up to 91.7%, and 71.2% for
the organizational FAQs, and user questions, respectively. This can indicate a more coherent and
focused language style used in the organizational
FAQs. In addition, while the mean question length
is higher for the organizational FAQs, the vocabulary size is smaller, which also supports the indication above.
Table 3 shows evaluation of the dataset using
the metrics described in section 3.2. Collecting

FAQ question
How do I know if
a position posted on
your website is still
available?

FAQ answer
We post all open positions on our website.
We remove
those positions once
they are filled, canceled or put on hold.

Whats the minimum age requirement to work at
ORG NAME?

The minimum age
to be eligible for
employment
at
ORG NAME is 18
years old. However,
many positions require a 21-year-old
minimum.
ORG NAME
recruits on a postby-post basis, so
your
application
will only have been
for the particular
post
advertised.
This means you are
free to apply again.

I have applied before and was rejected. Should I try
again?

Table 4: Examples from the ORG FAQ dataset.

FAQs from organizations’ FAQ pages resulted in
a high percentage (79%) of FAQs classified as organizational FAQs18 . The organizations’ attention
in creating their FAQs is reflected in a high formality percentage (90%) and a very low grammar
error rate (0.08). In addition, their average readability level (8.2) is significantly higher than the
ones of FAQIR and S TACK FAQ. Such readability
level corresponds to experts’ recommendation for
business content to be higher than 819 .
Finally, the user questions in our dataset are
shown to be of lower quality than the FAQs in all
metrics, as expected.
4.4 Dataset Samples
Table 4 shows some representative examples from
our dataset. Note that several user questions could
be mapped into the same FAQ question. For example, in the first row, there are four different user
questions mapped to a single FAQ. We provide
the FAQ answers (although it is out of this papers’
scope, and is left for future work) for the completeness of the dataset.

5 Experiments
In this section, we show how the ORG FAQ dataset
can be used through the task of “Organizational
FAQ Generation from User Questions”: given a
set of user questions with similar meaning, the
goal is to produce a corresponding organizational
FAQ. With such a capability, an organization can
18
To avoid exposure to the Jobs domain, FAQ pages whose
URLs included words such as jobs, career, etc. were excluded
from the training set of the Q UESTION - TYPE classifier.
19
https://tinyurl.com/y9mtysnb

gather user questions on the web, assign them to
clusters (e.g. by semantic similarity), and generate an appropriate FAQ for each cluster. We first
show how ORG FAQ can be used for this task in
the domain of jobs, and then how this can be generalized to another domain.

User questions

Generated FAQ
Target FAQ

User questions

5.1

Organizational FAQ Generation for the
Jobs Domain

Naturally, we can use the ORG FAQ dataset for
training an FAQ generator in the jobs domain.
To do so, we used the abstractor neural network of Chen and Bansal (2018), a sequence-tosequence model with attention and copy mechanism. A training sample consists of a set of up
to 10 user questions (4.4 on average) as the input, and the organizational FAQ as the target. The
user questions were concatenated into a single
sequence of tokens, with a special token separator. We split a total of 1579 samples into training
(85%), validation (5%) and test (10%) sets. Default training hyper-parameters were used, and the
validation set was used for early stopping. Using ROUGE metrics (Lin, 2004), we compare our
model’s performance against a baseline method
which randomly selects one of the input user questions. The results, shown in Table 5, are mean results over ten experimental rounds. On each round,
a generation model was trained and evaluated using a different random training/validation/test split.
Table 6 shows examples of a generated FAQs,
given input user questions. The first example, (A),
shows a high-quality output - it demonstrates the
ability of the model to convert several user questions with different kinds of errors (grammatical
errors, missing question mark, “when” instead of
“where”) into a properly-phrased question which
can form an adequate organizational FAQ. The second example, (B), shows a lower-quality output
FAQ, where the subject of the sentence is wrong.
Method
Baseline
FAQ-Generator

ROUGE-1
0.46
0.53

ROUGE-2
0.25
0.33

ROUGE-L
0.43
0.50

Table 5: Mean ROUGE F1 scores of our FAQGenerator vs. a baseline method which randomly selects one of the input user questions.

Generated FAQ
Target FAQ

when are you located ?
where is the job
how do answer where are you located
where should located
where they are
where are you located ?
where are you located ?
(A) High-Quality Generation
can i ask how the salary is paid ?
is it okay to ask how the salary is paid ?
how do you pay your salaries ?
how do i talk about mode of pyment of salary ?
how do i pay your salaries ?
how is salary paid ?
(B) Low-Quality Generation

Table 6: Examples of FAQs generated by the model,
given input user questions. Also shown are the targets
(ground-truth FAQs).

5.2 Generation for a new Domain: the
COVID-19 Use Case
Here, we present a use-case that utilizes the
ORG FAQ dataset for a new domain, showing its
usefulness.
5.2.1 Setting
The COVID-19 pandemic, also known as the coronavirus pandemic was first identified in December
2019, and quickly affected millions of people all
over the globe: hence, information has become a
critical aspect of the pandemic. People were frantically seeking information, ranging from “what is
the covid-19 virus?” to “can pets get the coronavirus?”, and more. People posted questions in different public social media channels such as Twitter, Quora, Reddit, etc. In addition, new dedicated
channels (using emails, forms, etc.) were created
by different organizations such as countries, municipalities, universities and companies. People
were encouraged to use those channels for asking
questions. For example, the city of Hillsboro, Oregon, USA provided a form to ask questions about
the Covid-1920 . Practically, those organizations
used questions from different channels in order to
create and update their COVID-19 FAQs pages.
An FAQ-Generator, as described in section 5.1,
can help automating this process and reduce human effort. Given the lack of a sufficiently large
training data, we explored the benefit of utilizing
a pre-trained model trained on a different domain,
such as FAQ-Generator trained on the ORG FAQ
dataset, as a starting point for further training on a
20

https://tinyurl.com/y9ftbpbs

small COVID-19 dataset.
5.2.2

Questions Collection

As before, the inputs to the FAQ-Generator should
be user questions clustered by similar topics.
In order to create such clusters, and given the
limited number of publicly available user questions, our approach was to define topics, and to
use COVID-19 dedicated social media sources,
namely Quora “Shared knowledge and experiences” about COVID-1921 and, Kaggle Coronavirus (covid19) Tweets dataset22 , to find user
questions which are relevant for each topic. In
addition, we asked colleagues to write additional
relevant user questions. In total, we created a set
of 57 topics, and an average of 13.7 user questions
per topic. Finally, we created ground-truth labels
for each topic by searching FAQs in official websites like the CDC (Centers for Disease Control
and Prevention)23 , WHO (World Health Organization)24 and more. An average of 4.3 FAQs were
collected per topic. Table 7 shows an example of
collected questions on the topic “Blood donation”.
User questions

FAQ questions

I’m not sick with coronavirus can donate?
is it okay to donate blood
are the any evidences of virus transmission thru blood
can COVID19 be transmitted thru blood
Can covid-19 be transmitted by blood donation?
Can covid-19 be transmitted by blood transfusion?

Table 7: An example of “Blood donation” topic, with
four user questions and two ground-truth FAQs.

5.2.3

COVID-19 FAQ Generation

Out of our collected COVID-19 data, we prepare
data for FAQ-Generator training, similarly as described in section 5.1. Again, each training sample
consists of a set of up to 10 user questions as input, and an organizational FAQ as target. A topic
with n user questions and k FAQ questions contributes k training samples, each with min(n, 10)
user questions (in case n > 10, we randomly select 10 user questions for each training sample).
For ROUGE evaluation, all k FAQ questions of a
topic from the test set, serve as ground-truth references of the corresponding test sample. Thus, a
rather small dataset of 245 samples is obtained out
of the 57 topics. Splitting to training (80%), validation (10%) and test (10%) sets was done at the
21

https://tinyurl.com/ybnm4b5k
22
https://tinyurl.com/yb5znxcr
23
https://tinyurl.com/vwxtxpp
24
https://tinyurl.com/y8674r39

topic level, to avoid same user questions appearing
in both training and test (or validation) sets. Same
neural network architecture and hyper-parameters
were used as described in section 5.1. The validation set was used for early stopping. As before, the
reported ROUGE results are mean results over ten
experimental rounds.
Here we would like to explore the benefit of
leveraging a pre-trained model trained on the Jobs
domain, given the insufficiently large COVID-19
dataset which we were able to collect. Hence,
we compare the performance of two models: one
trained solely on the COVID-19 data, while the
other is initialized with the parameters of a model
pre-trained on the ORG FAQ dataset, and finetuned on the COVID-19 data. We additionally
evaluate a model trained solely on the ORG FAQ
data, without fine-tuning, and a baseline method
which randomly selects one of the input user questions. The results, shown in Table 8, demonstrate
the usefulness of our ORG FAQ dataset. While the
COVID-19 dataset by itself is too small to obtain a
better model than the baseline, pre-training on the
Jobs data significantly improves performance. It
implies that some of the knowledge learnt by the
Jobs pre-trained model is generalizable to other domains. However, pure transfer learning was not
sufficient, and the fine-tuning stage was needed for
obtaining improved results. Table 9 shows examples of generated FAQs by the fine-tuned model.
The first one, (A), exemplifies a concise and accurate output FAQ, in spite of some noise and irrelevant details in some of the input user questions. In
the second example, (B), the generated FAQ is of
lower quality, due to wrong polarity of the verb.
Method
Baseline
FAQ-Generator - ORG FAQ only
FAQ-Generator - COVID-19 only
FAQ-Generator - COVID-19 fine-tuned

ROUGE-1
0.33
0.29
0.27
0.37

ROUGE-2
0.14
0.11
0.08
0.17

ROUGE-L
0.31
0.27
0.26
0.36

Table 8: Mean ROUGE F1 scores of FAQ-Generator
pre-trained on the ORG FAQ data and fine-tuned on the
COVID-19 data, versus alternative models which lack
pre-training or fine-tuninig. The baseline method randomly selects one of the input user questions.

6 Conclusion
We study differences between community FAQs
and organizational FAQs, propose a new dataset
to unleash research in the field, and show its usefulness on a new task for different domains. For

User questions

Generated FAQ
Target FAQ

User questions

Generated FAQ
Target FAQ

where can i get tested in alabama
i live in austin , where can i get tested for coronavirus
where can i get tested in new mexico
i live in ontario , where can i get tested ?
how do i get a coronavirus test ?
where can i go to get tested for coronavirus ?
where i can get tested
how do i get screened ?
i need to be checked for coronavirus .
how can i get a coronavirus test
how can i get tested ?
where can i go to get tested ?
(A) High-Quality Generation
why are there no masks and respirators in pharmacies ?
how is it possible that the state does not have enough masks and respirators ?
why can’t i get or find a mask or a respirator anywhere ?
when will masks be available ?
why aren’t there respirators ?
why are no masks or respirators available anywhere ?
i’d like a respirator
where can i get masks ?
where do people get protective masks or disinfectants ?
when can i buy a mask at a pharmacy ?
why can i buy a mask ?
is there a shortage of masks ?
(B) Low-Quality Generation

Table 9: Examples of FAQs generated by the COVID19 fine-tuned model, given sets of input user questions.
Also shown are the targets (ground-truth FAQs).

future work, we plan to use this dataset on more
tasks, such as question answering, and look into
the properties of FAQ answers.

K. Hammond, R. Burke, C. Martin, and S. Lytinen.
1995. Faq finder: a case-based approach to knowledge navigation. In Proceedings the 11th Conference on Artificial Intelligence for Applications,
pages 80–86.
Jeremy Howard and Sebastian Ruder. 2018.
Universal language model fine-tuning for text classification.
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 328–339, Melbourne, Australia.
Association for Computational Linguistics.
Youxuan Jiang, Jonathan K. Kummerfeld, and Walter S.
Lasecki. 2017. Understanding task design trade-offs
in crowdsourced paraphrase collection. In Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), pages 103–109.
Yoav Kantor, Yoav Katz, Leshem Choshen, Edo CohenKarlik, Naftali Liberman, Assaf Toledo, Amir
Menczel, and Noam Slonim. 2019. Learning to combine grammatical error corrections. arXiv preprint
arXiv:1906.03897.
Mladen Karan and Jan Šnajder. 2016. Faqir – a frequently asked questions retrieval test collection. In
Text, Speech, and Dialogue, pages 74–81.

References
Leif Azzopardi, Benno Stein, Norbert Fuhr, Philipp
Mayr, Claudia Hauff, and Djoerd Hiemstra, editors.
2019. Advances in Information Retrieval - 41st
European Conference on IR Research, ECIR 2019,
Cologne, Germany, April 14-18, 2019, Proceedings,
Part I, volume 11437 of Lecture Notes in Computer
Science. Springer.
Ankita Bihani. 2018. Faqtor : Automatic faq generation using online forums.
Robin Burke, Kristian Hammond, Vladimir Kulyukin,
Steven Lytinen, Noriko Tomuro, and Scott Schoenberg. 1997. Question answering from frequently
asked question files: Experiences with the faq finder
system. AI Magazine, 18:57–66.

Mladen Karan and Jan Šnajder. 2018. Paraphrasefocused learning to rank for domain-specific frequently asked questions retrieval. Expert Systems
with Applications, 91:418–433.

J.P. Kincaid. 1975. Derivation of New Readability Formulas: (automated
Research Branch report. Chief of Naval Technical
Training, Naval Air Station Memphis.
Ashish Kulkarni, Kartik Mehta, Shweta Garg, Vidit
Bansal, Nikhil Rasiwasia, and Srinivasan Sengamedu. 2019. Productqna: Answering user questions on e-commerce product pages. In Companion
Proceedings of The 2019 World Wide Web Conference, WWW ’19, pages 354–360.

S. Lahiri. 2015a. Squinky! a corpus of sentence-level
formality, informativeness, and implicature. arXiv
Yen-Chun Chen and Mohit Bansal. 2018.
preprint
arXiv:11506.02306.
Fast abstractive summarization with reinforce-selected sentence
rewriting.
In Proceedings of the 56th Annual Meeting of the
Shibamouli Lahiri. 2015b. Squinky! a corpus of
Association for Computational Linguistics (Volume
sentence-level formality, informativeness, and impli1: Long Papers), pages 675–686. Association for
cature. arXiv preprint arXiv:1506.02306.
Computational Linguistics.
Lei Cui, Shaohan Huang, Furu Wei, Chuanqi Tan,
Chaoqun Duan, and Ming Zhou. 2017. SuperAgent:
A customer service chatbot for e-commerce websites. In Proceedings of ACL 2017, System Demonstrations, pages 97–102.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Shibamouli Lahiri, Prasenjit Mitra, and Xiaofei Lu.
2011. Informality judgment at sentence level and
experiments with formality score. In Proceedings of
the 12th International Conference on Computational
Linguistics and Intelligent Text Processing - Volume
Part II, CICLing’11, pages 446–457.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text summarization
branches out, pages 74–81.

Ellie Pavlick and Joel Tetreault. 2016. An empirical analysis of formality in online communication.
Transactions of the Association for Computational
Linguistics, 4:61–74.
Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.
Fatemeh Raazaghi. 2015. Auto-faq-gen: Automatic
frequently asked questions generation. In Advances
in Artificial Intelligence, pages 334–337.
Sudha Rao and Joel Tetreault. 2018. Dear sir or
madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style
transfer. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 129–140.
F. Razzaghi, H. Minaee, and A. A. Ghorbani. 2016.
Context free frequently asked questions detection
using machine learning techniques.
In 2016
IEEE/WIC/ACM International Conference on Web
Intelligence (WI), pages 558–561.
Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and beyond. Foundations and Trends R in Information Retrieval, 3(4):333–389.
Wataru Sakata, Tomohide Shibata, Ribeka Tanaka, and
Sadao Kurohashi. 2019. Faq retrieval using queryquestion similarity and bert-based query-answer relevance. In Proceedings of the 42Nd International
ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR’19, pages
1113–1116.
Raphael
Tang,
Rodrigo
Nogueira,
Edwin Zhang, Nikhil Gupta, Phuong Cam,
Kyunghyun Cho, and Jimmy Lin. 2020.
Rapidly bootstrapping a question answering dataset for covid-19.
Jerry
Wei,
Chengyu
Huang,
Soroush
Vosoughi,
and
Jason
Wei.
2020.
What are people asking about covid-19? a question classification dataset.
Ruobing Xie, Yanan Lu, Fen Lin, and Leyu Lin. 2019.
Faq-based question answering via knowledge anchors.
Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
WikiQA: A challenge dataset for open-domain question answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language
Processing, pages 2013–2018.

