Projected Statistical Methods for Distributional Data on the
Real Line with the Wasserstein Metric
Matteo Pegoraro∗, Mario Beraha†‡

arXiv:2101.09039v1 [stat.ME] 22 Jan 2021

January 25, 2021

Abstract
We present a novel class of projected methods, to perform statistical analysis on a data
set of probability distributions on the real line, with the 2-Wasserstein metric. We focus
in particular on Principal Component Analysis (PCA) and regression. To define these
models, we exploit a representation of the Wasserstein space closely related to its weak
Riemannian structure, by mapping the data to a suitable linear space and using a metric projection operator to constrain the results in the Wasserstein space. By carefully
choosing the tangent point, we are able to derive fast empirical methods, exploiting a
constrained B-spline approximation. As a byproduct of our approach, we are also able to
derive faster routines for previous work on PCA for distributions. By means of simulation
studies, we compare our approaches to previously proposed methods, showing that our
projected PCA has similar performance for a fraction of the computational cost and that
the projected regression is extremely flexible even under misspecification. Several theoretical properties of the models are investigated and asymptotic consistency is proven. Two
real world applications to Covid-19 mortality in the US and wind speed forecasting are
discussed.
Keywords: Wasserstein spaces, Wasserstein metric, Principal Component Analysis,
Wasserstein Regression, Metric Projection, Monotonic B-splines

1. Introduction
In many fields of machine learning and statistics, performing inference on a set of distributions is an ubiquitous but arduous task. The Wasserstein distance provides a powerful
tool to compare distributions, as it requires very little assumptions on them and is at the
same time reasonably easy to compute numerically. In fact, many other distances for distributions either require the existence of a probability density function or are impossible
to evaluate, cf. Cuturi (2013), Peyré et al. (2019), Panaretos and Zemel (2020).
The Wasserstein distance recently gained popularity both in the statistics and machine
learning community. See for instance Bassetti et al. (2006), Bernton et al. (2019a), Catalano et al. (2019) for statistical properties of the Wasserstein distance, Cao et al. (2019),
Cuturi et al. (2019) and Cuturi and Doucet (2014) for applications in the field of machine
and deep learning, Bernton et al. (2019b) and Srivastava et al. (2015) for applications in
Bayesian computation.
In this work, we focus on the situation in which the single observation itself can be
seen as a distribution, as in the analysis of images (Cuturi and Doucet, 2014; Banerjee
et al., 2015), census data (Cazelles et al., 2018), econometric surveys Potter et al. (2017)
∗. MOX – Department of Mathematics, Politecnico di Milano
†. Department of Mathematics, Politecnico di Milano
‡. Department of Computer Science, Università degli Studi di Bologna

1

and process monitoring (Hron et al., 2014). In particular, we consider observations to be
distributions on the real line. There exist several possible ways to represent distributions,
such as histograms, probability density functions (pdfs) and cumulative density functions
(cdfs), each characterized by different constraints. For instance, histograms sum to one,
pdfs integrate to one, and the limits for cdfs are 0 and 1, moreover all of these functions
are nonnegative. These constraints translate into complex geometrical structures that
characterize the underlying spaces these objects live in.
1.1 Previous work on distributional data analysis
One of the first works defining PCA for a data set of distributions is Kneip and Utikal
(2001), where the authors apply tools from functional data analysis (FDA) directly to a
collection of probability density functions. This approach, however, completely ignores
the constrained nature of probability density functions, leading to poor interpretability of
the results.
Based on theoretical results in Egozcue et al. (2006), who defines a Hilbert structure on
a space of densities (called a Bayes space), Delicado (2011) and Hron et al. (2014), propose
a more reasonable approach to the problem of PCA for density functions. In particular,
in Hron et al. (2014), the authors use the geometric properties of the Bayes space, coupled
with a suitable transformation from the Bayes space to an L2 space, to perform PCA on
a set of pdfs using FDA tools, and then map back the results to the Bayes space.
Another, perhaps less widely used, approach focuses on borrowing tools from symbolic
data analysis (SDA) in the context of histogram data (Nagabhushan and Pradeep Kumar,
2007; Rodrı́guez et al., 2000; Le-Rademacher and Billard, 2017). Moreover, in Verde et al.
(2015) some of these attempts are extended to generic distributional data using Wasserstein
metrics.
Finally, Bigot et al. (2017) and Cazelles et al. (2018) propose two PCA formulations
based on the geometric structure of the Wasserstein space: a geodesic PCA and a log
PCA. In a similar fashion the recent preprints of Chen et al. (2020) and Zhang et al.
(2020) propose linear regression and autoregressive models, respectively, for distributional
data using the Wasserstein geometry.
We now highlight some key aspects of the aforementioned approaches. Hron et al.
(2014) assumes that all the probability measures have the same support. This is hardly
verified in practice, so that to apply their techniques one needs either to truncate the
support of some of the probability density functions or to extend others (for instance,
by adding a small constant value and renormalizing), leading to numerical instability as
discussed in Section 7.
The SDA-based methods in Nagabhushan and Pradeep Kumar (2007); Rodrı́guez et al.
(2000); Le-Rademacher and Billard (2017) and Verde et al. (2015) share the poor interpretability of SDA.
The methods in Bigot et al. (2017), Cazelles et al. (2018), Chen et al. (2020) and Zhang
et al. (2020) are based on the weak Riemannian structure of the Wasserstein space, cf.
Section 2.2. Such structure enables the authors to borrow ideas and terminologies from
statistical frameworks defined on Riemannian manifolds (see Bhattacharya et al., 2012;
Pennec, 2006, 2008; Huckemann et al., 2010; Patrangenaru and Ellingson, 2015; Fletcher,
2013; Banerjee et al., 2015). We can roughly distinguish those frameworks in two main
approaches: the intrinsic/geodesic one and extrinsic/log one.
Briefly, intrinsic methods are defined using the metric structure of the Wasserstein
space, working with geodesic curves and geodesic subsets, so that they faithfully respect
the metric of the underlying space. However, in general, intrinsic methods present many
2

practical difficulties in that the optimization problems they lead to are usually nontrivial,
as we discuss in Section 5.3. Instances of intrinsic methods for distributional data are
the geodesic PCA in Bigot et al. (2017), the linear models in Chen et al. (2020) and the
autoregressive models in Zhang et al. (2020).
On the other hand, extrinsic methods resort to the linear structure of suitably defined
tangent spaces, by mapping data from the Wasserstein space to the tangent (through the
so-called log map) and then mapping back the results to the Wasserstein space (through
the exp map). Of course, this approach is less respectful of the underlying geometry than
the intrinsic one, but usually presents several numerical advantages. An example of such
extrinsic methods is the log PCA in Cazelles et al. (2018).
The main issue with this log PCA is that the image of the log map inside the tangent of
the Wasserstein space is not a linear space, but rather a convex cone embedded in a linear
space (see Section 2.2). Hence, while exploiting the linear structure of the tangent, it is
possible that the projection of some points onto the principal components end up outside
of the cone. For these points, the exp map from the tangent to the Wasserstein space used
in Cazelles et al. (2018) is not a metric projection, which in general is not available, so
that the results in this setting are hardly interpretable.
1.2 Our contribution and outline
The main contribution of this work is the proposal of alternative PCA and regression
models for distributional data in the Wasserstein space. We term these models projected,
in opposition to the log PCA in Cazelles et al. (2018). Our framework lies in between
the log one and the geodesic one, since we use an analogous to the log map to transform
our data, as for extrinsic methods, but do not resort to the exp map to return to the
Wasserstein space, using instead the metric projection operator. Thanks to this, our
projected methods are more respectful of the underlying geometry than the log ones,
while at the same time retaining the same reduced computational complexity. Thus, the
projected methods expand the range of situations where extrinsic methods are an effective
and efficient alternative to intrinsic tools: in our examples, the performance loss in general
is marginal (see Section 7).
By centering the analysis in appropriate points of the Wasserstein space, one can
identify the space of space of probability measures (with finite second moment) with the
space of square integrable monotonically non-decreasing functions on a compact set. We
use a suitable quadratic B-spline expansion to get a very handy representation of such
functions. Through such B-spline expansion, it is possible to approximate the metric
projection onto the Wasserstein space as a constrained quadratic optimization problem
over a convex polytope, that is a well-established problem, cf. Potra and Wright (2000).
This allows us to exploit the underlying linear structure of an L2 space, so that all the
machinery developed for functional data analysis can be directly applied to this setting. We
address the issue of interpretability of the results, tackling a number of diverse applications
and developing different ways to measure the loss of information caused by the extrinsic
nature of our methods.
We observe that the idea of representing nondecreasing functions through B-splines for
statistical purposes has been proposed also by Das and Ghosal (2017), in the context of
Bayesian quantile regression, where the authors use B-splines with (random) monotonic
coefficients as a generative model for random quantile functions. However, their focus
is on defining a generative model, and not on developing a statistical setting exploiting
the geometry given by the constrained representation. Along this direction, they do not
restrict their attention to quadratic splines and consider cubic ones.
3

The second contribution of this work is the derivation of alternative numerical optimization schemes for the geodesic PCA in Bigot et al. (2017) and Cazelles et al. (2018),
based the proposed quadratic B-spline expansion.
The remaining of the paper is organized as follows. Section 2 covers the basic concepts of Wasserstein distance and the weak Riemannian structure of the Wasserstein space,
along with a brief discussion on a suitable way to exploit such structure for our purposes.
Section 3 defines the projected PCA and projected regression in a general setting. In Section 4 we discuss the choice of the base point in which we center our analysis and how to
efficiently approximate the metric projection through B-splines; in Section 5 we present
the numerical algorithms needed to compute our projected methods and an alternative
optimization routine for the geodesic PCA in Cazelles et al. (2018). Section 6 discusses
the asymptotic properties of the spline approximation and of the projected models, establishing consistency of the estimators under some assumptions. Numerical illustrations
on simulated data sets are shown in Section 7 and in Section 8 our projected methods
are applied to two real world problems: we perform PCA on the US data on Covid-19
mortality by age and sex and perform a distribution regression to forecast the wind speed
near a wind farm. Finally, the article concludes in Section 9. The Appendix collects all the
proofs of the theoretical results, additional details on the simplicial PCA and regression,
and further simulations.

2. Preliminaries
In the following, we will consider probability measures on the real line R endowed with
the usual Borel σ-field, we will skip references to the σ-field whenever it is obvious.
Given a measure µ on R define its cumulative distribution function Fµ (x) = µ((−∞, x])
for x ∈ R and the associated quantile function Fµ− (t) = inf{x ∈ R : t ≤ Fµ (x)}. When
Fµ is continuous and strictly monotonically increasing, Fµ− = (Fµ )−1 .
2.1 Wasserstein metric and Wasserstein spaces
We start by recalling the definition of the 2-Wasserstein distance between two probability
measures µ, ν on R:
Z
2
W2 (µ, ν) = inf
|x − y|2 dγ(x, y),
(1)
γ∈Γ(µ,ν) R×R

where Γ(µ, ν) is the collection of all probability measures on R × R with marginals µ
and ν. Closely related to the definition of Wasserstein distance lies the one of Optimal
Transport (OT). In particular, (1) identifies the Wasserstein distance with the minimal
total transportation cost between µ and ν in the Kantorovich problem with quadratic cost
(Ambrosio et al., 2008).
For our purposes, it is convenient to consider another formulation of the OT problem,
originally introduced in Monge (1781). Given two measures µ, ν as before, the optimal
transport map from µ to ν is the solution of the problem
Z
inf
|x − T (x)|2 dµ(x),
(2)
T :T #µ=ν

Ω

where # denotes the pushfoward operator, that is for any measurable set B and measurable
function
f : R → R,
(f #µ)(B) = µ(f −1 (B)).
(3)
4

Note that any solution of (2) induces one and only one solution of (1); moreover if the OT
problem has a unique solution, then also the Wasserstein distance problem has only one
solution. However not all Wasserstein distance problems can be solved through Monge’s
formulation (Ambrosio et al., 2008).
The unidimensional setting is a remarkable exception in that there exist explicit formulas for both problems. In particular, the Wasserstein distance can be computed as
W22 (µ, ν)

Z
=

1

|Fµ− (s) − Fν− (s)|2 ds,

(4)

0

and, if the measure µ has no atoms, then there exists a unique solution to Monge’s problem
given by Tµν = Fν− ◦ Fµ . For a proof of these results, see Chapter 6 of Ambrosio et al.
(2008).
It is clear that, in general, the Wasserstein distance between two probability measures can be unbounded (for instance when in (4) Fµ− is not square integrable on [0, 1]).
Nonetheless, when restricting the focus on the set of probability measures with finite second moment, then it holds that W2 defines a metric (see, for instance, Chapter 7 of Villani,
2008). Formally, let the Wasserstein space:
Z
W2 (R) = {µ ∈ P(R) :
x2 dµ < +∞}
R

then (W2 (R), W2 ) is a separable complete metric space.
2.2 Weak Riemannian structure of the Wasserstein Space
Thanks to the uniqueness of the transport maps, by fixing an absolutely continuous (a.c.)
probability measure
associate to any ν ∈ W2 (R) the optimal transport
R can
R µν ∈ W2 2 (R), we
2
ν
map Tµ . Since R |Tµ (x)| dµ = R x dν we can define the following map ϕµ : W2 (R) →
Lµ2 (R) with the rule: ϕµ (ν) = Tµν .
We note several immediate but interesting properties of the map ϕµ . First, it is an
isometry (and so a homeomorphism onto its image) since
Z
Z
ν
η
2
|Tµ (x) − Tµ (x)| dµ =
|Fν− − Fη− |2 ds = W22 (ν, η).
R

[0,1]

Second, the image of ϕµ is a closed convex cone in Lµ2 (R): a set closed under addition and
positive scalar multiplication. In fact, for any λ ≥ 0, λTµν is still a transport map from
µ to another measure whose quantile is λFµ− ; and similarly Tµν + Tµη = (Fµ− + Fη− ) ◦ Fµ .
Being W2 (R) complete, ϕµ (W2 (R)) is closed in Lµ2 (R). Third, ϕµ (µ) = idR (where idC
denotes the identity map of the set C). Finally, it is straightforward to show that ϕµ
is not surjective and ϕµ (W2 (R)) is the set of µ-a.e. non decreasing functions in Lµ2 (R)
(Panaretos and Zemel, 2020).
The inverse of the map of ϕµ is the measure pushforward (see Equation 3) and it is
defined on the whole Lµ2 (R): given f ∈ Lµ2 (R), then ν = f #µ is a measure in W2 (R). In
fact:
Z
Z
2
|x| dν = |f (x)|2 dµ = kf k2µ
A natural way to define a tangent structure for W2 (R) is therefore to take advantage of
the cone structure given by ϕµ . In fact for closed convex cones, there are already notions
of tangent cones. Similarly to Rockafellar and Wets (1998), Theorem 6.9, we can define:
Tanµ (W2 (R)) := TidR (Lµ2 (R)) = {f ∈ Lµ2 (R)|∃h > 0 : id + hf ∈ ϕµ (W2 (R))}
5

Lµ
2 (R)

(5)

We remark that Theorem 6.9 in Rockafellar and Wets (1998) is stated in Rn , but it
holds also more generally, for instance in an Hilbert space (see Aubin and Frankowska
(2009), Chapter 4).
A geometric interpretation of (5) is the following. The tangent space consists of all the
vectors f that move the base point inside the cone ϕµ (W2 (R)), when considered up to a
scale factor h. Hence, f plays the role of direction of a tangent vector going out from the
tangent point. Furthermore, since for every f ∈ ϕµ (W2 (R)) then f + id ∈ ϕµ (W2 (R)) we
have that ϕµ (W2 (R) is included in the tangent space. As shown later in this Section, the
inclusion is strict and the tangent space is much larger than ϕµ (W2 (R).
Note that we can recover the definition of tangent space given by Ambrosio et al. (2008)
and Panaretos and Zemel (2020) by a simple “change of variable”: calling g = id + hf
then substituting (g − id)/h in (5) gives the following definition of tangent
Lµ
2 (R)

Tanµ (W2 (R)) = {λ(f − id)|f ∈ ϕµ (W2 (R)); λ > 0}

,

which is the one given in Ambrosio et al. (2008) and Panaretos and Zemel (2020). As
shown in Panaretos and Zemel (2020) the tangent cone Tanµ (W2 (R)) is indeed a linear
space. For this reason we refer to it as tangent space, instead of cone.
In analogy to Riemannian geometry, following Ambrosio et al. (2008) and Panaretos
and Zemel (2020), we define the logµ and expµ maps. Having fixed µ absolutely continuous:
logµ : W2 (R) → Tanµ (W2 (R))
ν 7→

Tµν

expµ : Tanµ (W2 (R)) → W2 (R)

− id

f 7→ (id + f )#µ

(6)

We briefly highlight some properties of these maps; properties which immediately
follows from the discussion above.
Remark 1 The map logµ is defined on the whole space W2 (R). Moreover, it is clearly an
isometry: W2 (η, ν) = k logµ (η) − logµ (ν)kLµ2 (R) (Panaretos and Zemel, 2020). This shows
that there is no local-approximation issue when working in the tangent space, in contrast
with the usual Riemannian manifold setting. There, the tangent space usually provides
good approximation only in a neighborhood of the tangent point.
Remark 2 The map logµ is not surjective on Tanµ , indeed its image Im(logµ ) is a closed
convex subset of Lµ2 (R) given by all the maps f such that f + id ∈ ϕµ (W2 (R), that is,
f + id is µ-a.e. increasing. The restriction of expµ on Im(logµ ), henceforth denoted by
expµ| logµ (W2 (R)) , is an isometric homeomorphism and its inverse is logµ . In particular, we
observe that logµ ◦expµ is not a metric projection in Lµ2 . That is, in general logµ ◦expµ (f ) 6=
arg ming∈Im(logµ ) ||f − g||Lµ2 .
2.3 Intrinsic and extrinsic methods in the Wassestein space
As mentioned in Section 1.1, borrowing ideas from Riemannian geometry leads to discerning statistical methods on the Wasserstein space in the classes of intrinsic and extrinsic
methods.
The Weak Riemannian structure presented in Section 2.2 provides a suitable environment for developing intrinsic methods. In fact, the geodesic structure of W2 (R) can be
recovered through the linear structure of any Lµ2 (R) space through the isometry ϕµ . Pointwise interpolation of the transport maps coincide with the geodesic between measures. In
other words, given µ a.c., the geodesic between ν and η is given by:
γ(t) = ((1 − t) · Tµν + t · Tµη )#µ
6

(7)

Thus, such geodesic structure can be recovered in many different (but equivalent) ways,
depending on µ.
On the other hand, Remark 1 motivates the development of extrinsic tools, since
working in the image of logµ inside the tangent space Tanµ is exactly like working in
W2 (R). This is not common in Riemannian manifold framework, since usually the tangent
space provides a good approximation only near to the tangent point. As a consequence, if
in the general Riemannian manifold framework the choice of the tangent point µ is crucial
(since results for extrinsic methods might be significantly altered for different choices of
µ) when working with W2 (R) this is not the case.
To further motivate this key point, consider µ and ν a.c. measures; the maps
logν ◦(expµ|logµ (W2 (R)) ) and ϕν ◦ ϕ−1
µ are isometric homeomorphisms (as composition of
isometries and homeomorphisms). In other words, they preserve distances and send border elements of logµ (W2 (R)) or ϕµ (W2 (R)) into border elements of logν (W2 (R)) and
ϕν (W2 (R)), respectively, and the same with internal points (and so in particular, they
preserve distances from any point to the border). In Chen et al. (2020), Bigot et al.
(2017) and Zhang et al. (2020) µ is chosen as the barycentric measure x̄ of the observations xi ∈ W2 (R). The discussion above implies that considering the tangent space at the
Wasserstein barycenter x̄ and working on logx̄ (xi ) = logx̄ (xi ) − logx̄ (x̄) is exactly the same
as considering the tangent space at any µ a.c. and working on logµ (xi ) − logµ (x̄) for our
statistical purposes. So the choice of the tangent space from the theoretical point of view
is completely arbitrary.
Moreover, centering the analysis in the barycenter presents a drawback when studying
asymptotic properties of the models under consideration, since x̄ changes as the sample
size grows. In Section 4.1 we propose to fix µ as the uniform measure on [0, 1]. This choice
not only allows us to derive empirical methods that are extremely simple to implement, cf.
Section 5, but also allows us to study asymptotic properties of the models in Section 6.2
without resorting to parallel transport, as done for instance in Chen et al. (2020).
2.4 Tangent vs. Lµ2
Lastly, we briefly discuss the major differences between using a tangent space representation of W2 (R) and using the representation given by some ϕµ .
We recall that, for a fixed µ a.c., the two representations are indeed quite similar
ϕµ (ν) = Tµν , logµ (ν) = Tµν −id; a priori one may prefer the tangent representation, because
it already expresses data as vectors coming out of a point. Therefore, for instance, it
might result practically more convenient to center the analysis in the barycenter and work
on vectors, taking away any “data centering” issues. At the same time, also notational
coherence with already existing methods might benefit from this choice.
However, especially when dealing with extrinsic techniques, we found slightly more
practical to use the ϕµ representation in that it is more straightforward to represent
ϕµ (W2 (R)) compared to logµ (W2 (R)): the first one can in fact be represented directly as
the cone of the µ-a.e non-decreasing functions.

3. Projected Models in the Wasserstein Space
In this section, exploiting the embeddings given by ϕµ , we define a class of projected
statistical methods to perform extrinsic analysis for data in the Wasserstein space.
To give a general framework, we do not restrict our attention to a particular ϕµ yet,
even though in Section 4 we argue that a natural choice which allows an easier implementation of the empirical methods is letting µ be the uniform distribution on [0, 1]. Hence, for
7

the sake of notation, we consider a generic case of data laying in a closed convex cone X inside a separable Hilbert space H. In our setting, H would be Lµ2 (R) and X = ϕµ (W2 (R)),
for some µ ∈ W2 (R) absolutely continuous.
3.1 Principal component analysis
We start by defining one of the main contributions of our work: the projected PCA. We
recall that for an H-valued random variable X , PCA is a well established technique and
amounts to finding the eigenfunctions of the Karhunen-Loéve expansion of the covariance
operator of X , see Ramsay (2004). Observe that any X-valued random variable can be
considered as an H-valued one (by the inclusion map), so that a notion of PCA is already
available.
When defining principal components, a key notion is the one of dimension of the
principal component (PC). In this work, principal components will be closed convex subsets
of H, and we will always define the dimension of a subset of H as the dimension the smallest
affine subset of H containing it. Lastly, for a generic closed convex C ⊂ H, let ΠC denote
the metric projection onto C: ΠC (x) := arg minc∈C ||x − c|| and, for a set of vectors U ,
denote with Sp(U ) its linear span.
Definition 1 (Projected PCA). Given X a random variable with values in X ⊂ H,
let Uk = {w1 , ..., wk } be its first k H-principal components centered in x0 = E[X ]. A
x0 ,k
(k, x0 )−projected principal component of X is the biggest closed convex subset UX
of X
x0 ,k
x0 ,k
x0 ,k
such that: (i) x0 ∈ UX , (ii) dim(UX ) = k, and (iii) UX ⊆ ΠX (Sp(Uk )).
In other words, the projected principal component is obtained by approximating the
span of the principal components found in H, with convex subsets in X. Note that the
principal components in H might “capture” some variability which is not present when
measuring distances inside X. In fact the projection of a point belonging to X onto a
direction wj might end up being outside X, see Section 3.3. However, as we will show in
Section 7, in our examples the projected PCA behaves well and this issue does not seem
to affect significantly the performance.
Remark 3 Convex sets are essential in our analysis since, thanks to (7) convex sets in X
are precisely the subsets of W2 (R) which are geodesically complete: the geodesic connecting
any pair of points in the subset, is contained in the subset. Geodesic subsets are a natural
generalization of linear spaces.
Remark 4 The metric projection of a linear subspace onto a convex subset can end up
being a nonconvex set. In addition to that, while loosing convexity, the dimension of the
metric projection of a convex subset can be bigger of the dimension of the original subset.
A simple example where both cases happen is the projection of y = −x onto x, y > 0 in
R2 .
We observe that inside a projected principal component, we have a preferential orthonormal basis given by the principal components in H; for this reason we call Uk =
{w1 , ..., wk } principal directions.
Although it might seem impractical to find the projected component, the following
Lemma provides a more convenient alternative characterization.
x0 ,k
x0 ,k
Lemma 1 Let UX
be as in Definition 1, then UX
= (x0 + Sp(Uk )) ∩ X.

8

Natural alternatives to Definition 1 would be, for instance, to let the projected principal directions (component) be the metric projection of w1 , . . . , wk (the linear span of
{w1 , . . . , wk }) onto X, respectively. In the former case, the projection would not guarantee the orthogonality of the projected directions, which is instead essential to properly
explore the variability. Moreover, since the “tip” of the projected unit vectors would likely
lie on the border of X, the projection of a new observations on a direction would still lie
outside of X as soon as the score associated to that direction is larger than 1. The latter
case, instead, presents the drawbacks pointed out in Remark 4.
We argue that, despite its simplicity, Definition 1 is indeed very well suited for statistical analysis in the Wasserstein Space. For instance, we are guaranteed that, as the
dimension grows up, the k projected components provide a monotonically better fit to
the data. This is easily verified because ΠX is a strictly non-expansive operator, being X
closed and convex (see Deutsch (2012)), which implies the following Proposition.
Proposition 1 With the same notation as Definition 1, let Πk be the orthogonal projection
on Span(Uk ). Then for any x ∈ X: kΠX (Πk (x)) − xk ≥ kΠX (Πk+1 (x)) − xk → 0 with
k → +∞.
Once a principal component is found, a classical task that one may want to perform
x0 ,k
, for instance for dimensionality
is to project a new “observation” x∗ ∈ X onto UX
reduction purposes. In general, the metric projection on generic convex subsets might be
arduous to find, we will deal with this issue in Section 4. Nevertheless, we can use the
following Proposition to reduce in advance the dimension of the parameters involved in the
problem; turning it into a projection problem inside the principal projected component,
which allows for faster computations (see Equation 13).
x0 ,k
Proposition 2 Let x∗ ∈ X. The projection of x∗ onto UX
is given by

arg min kx∗ − vk = ΠU x0 ,k (Πk (x∗ )).

(8)

X

x ,k

v∈UX0

Lastly, we observe that, since projected principal components are not linear subspaces,
the scores of some points on a principal direction can vary as we increase the dimension
of the principal component.
3.2 Regression
Broadly speaking, a regression model between two variables with values in two different
spaces is given by an operator between such spaces, which for every input value of the independent variable, returns a predicted value for the dependent variable. In the following,
let us denote with Z the independent variable and with Y the dependent one. A regression
model is usually understood as an operator Γ specifying the conditional value of Y given
Z, that is, E[Y|Z] = Γ(Z).
If the spaces where Z and Y take values possess a linear structure, this linearity is
usually exploited by means of a (kernel) linear operator, with possibly an “intercept”
term. To define our projected regression model, we want to exploit the cone structure of
X in a similar fashion. In fact, such linear kernel operators combine good optimization
properties and interpretability since their kernels can provide insights into the analysis,
much like coefficients in multivariate linear regression.
We treat separately the cases where the X-valued variable is the independent or the
dependent one. The case when both variables are X-valued follows naturally. To keep the
9

notation light, in what follows we will not distinguish between “proper” linear operators
and linear operators with an added intercept term, which could as well be employed in all
the incoming definitions to gain flexibility.
Consider the case in which we have an independent X-valued random variable, and
denote with V the space where the dependent variable takes value. Despite the fact that X
is not a linear space, with an abuse of notation, we call “linear” an operator which respect
sum and positive scalar multiplication for elements in X. Such operators are in fact
obtained by restricting on X linear operators defined on H. Following this idea, in order
to define linear regression for an X-valued independent random variable, we consider such
variable as H-valued, obtain the regression operator and then take the restriction of the
operator on X. In this way, when H = Lµ2 (R) and X = ϕµ (W2 (R)), it is possible to exploit
the classical FDA framework to perform all kinds of distribution on scalar/vector/etc...
regression. For brevity, we report only the definition with V = R.
Definition 2 Let Z an X-valued random variable, and Y a real valued one. Let Γβ :
H → R be a functional linear regression model for such variables, with Z considered as
H-valued and Γβ (v) = hβ, vi. A projected linear regression model for (Z, Y) is given by
(Γβ )|X .
Now we turn to the cases which feature an X valued dependent variable and a Z
valued dependent one, for Z a generic Hilbert space. Through the inclusion X ,→ H,
we can consider a regression problem with X-valued dependent variable, as a problem
with H-valued dependent variable. Comparing this situation with the previous one, it
is clear that we now face a “dual” problem. Indeed, while before we needed to restrict
the domain from H to X, we now need to force the codomain of Γ to lie inside X. We
would like to retain the same properties that make linear kernel operators appealing as
regression operators between Hilbert spaces. A possibility could be considering a linear
kernel operator Γ with values in H and restricting it to Γ−1 (X). However, this would
imply that for any z 6∈ Γ−1 (X) no prediction would be available.
We argue that a more reasonable approach consists in finding an operator ΓP : Z → X
as close as possible (in some sense that will be clear later) to the linear kernel operator Γ
aforementioned. Hence, we relax the linearity assumption in favor of Lipschitzianity, and
take as regression operator ΠX ◦ Γ, whose image always lies in X. Note that ΓP inherits
the interpretability of the kernel of Γ.
To motivate such choice, we give the following notion of a projected operator.
Definition 3 Let Z be a normed space and consider Z an Z-valued random variable. Let
Γ : Z → H a generic Lipschitz operator between Z and H. A (Z, X)-projection of Γ is an
operator ΓP : Z → X such that:
ΓP = arg min EZ [kΓ(v) − T (v)k2 ]
T :Z→X

In other words, ΓP provides the best pointwise approximation of the H-valued operator
Γ, averaged w.r.t. the measure induced by Z. Hence, given a Z a Z-valued random variable
and Y an X-valued random variable and a linear regression model Γ : Z → H for (Z, Y),
the projected regression model induced by Γ is ΓP .


Proposition 3 With the same notation as above, if E kZk2 < ∞, then ΓP = ΠX ◦ Γ.
Proof For any T : Z → X, it holds: kΓ(z) − ΠX (Γ(z))k ≤ kΓ(v) − T (v)k. Moreover,
Γ and ΠX ◦ Γ are Lipschitz, and being ΠX non-expansive, they share the same constant
L > 0:
kΓ(v) − ΠX ◦ Γ(v)k2 ≤ 2Lkvk2
10

3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
0.5
1.0

projected
geodesic

0

1

2

3

4

5

Figure 1: Comparison of projected and geodesic PCA when H = R2 and X is the shaded
rectangle. The projected principal direction is rather different from the geodesic one
because most of the observations (blue dots) are concentrated around the borders
and thus EZ [kΓ(z) − ΠX ◦ Γ(z)k2 ] is bounded iff Z has finite second moment.

The only case left out from the treatment above is when both the independent and the
dependent variables are X-valued. This case, however, follows naturally by combining the
two approaches and we report the definition below.
Definition 4 Let Z and Y two X-valued random variables. Let Γ : H → H be a functional linear regression model for the variables considered as H-valued. A projected linear
regression model for (Z, Y) is given by (ΠX ◦ Γ)|X .
Remark 5 When considering a regression with X-valued independent variable, one may
want to relax the restriction on X in Definition 2 for various reasons; for instance one may
have measurement errors, or by design the test set may consider points also outside X. In
such cases it is worth considering the problem of how many continuous linear extensions
of Γ|X are possible on the whole H. A sufficient condition for the uniqueness of such
extension S
is the following: there exist a sequence of linear subspaces of H, say {HJ }J≥1 ,
such that J HJ is dense in H and XJ := HJ ∩ X contains a basis of HJ for every J.
Remark 6 When H = Lµ2 (R) and X = ϕµ (W2 (R)) the condition in Remark 5 is verified,
for instance, by Remark 8 in Section 4.3. Moreover, observe that the uniqueness of the
extension can also be proven thank to Jordan’s representation of functions f : R → R
with bounded variation (BV). In fact any f with BV can be written as the difference of
monotone functions and thus Γ(f ) is fixed. Then by the density of BV functions in H, we
define Γ on the remaining elements of H.
3.3 Comparison with intrinsic methods
We now compare the projected methods defined earlier in this Section and the intrinsic
counterparts. In particular, we focus on the geodesic PCA defined in Bigot et al. (2017)
and Cazelles et al. (2018) and on the distribution on distribution regression model in Chen
et al. (2020).
Bigot et al. (2017) and Cazelles et al. (2018) define two different PCA, namely a global
and a nested one; we report their definitions below.

11

Definition 5 (Global geodesic PCA) Let X a random variable with values in
 X with
∗
2
E[X ] = x0 . A (k, x0 )-global geodesic PC is a set C minimizing E d(X , C) over the
closed convex sets C ⊂ X such that x0 ∈ C and dim(C) ≤ k
Definition 6 (Nested geodesic PCA) Let X a random variable with values in X with
E[X ]= x0 . For k = 1, a (k, x0 )-nested geodesic PC is a set Ck∗ such that Ck∗ is a minimizer
of E d(X , C)2 over the closed convex sets C ⊂ X such that x0 ∈ C and dim(C)
 ≤ k; for
k ≥ 1, a (k, x0 )-nested geodesic PC is a set Ck∗ such that Ck∗ is a minimizer of E d(X , C)2
∗ , where
over the closed convex sets C ⊂ X such that: x0 ∈ C, dim(C) ≤ k, and C ⊃ Ck−1
∗
Ck−1 is a (k − 1, x0 )-nested geodesic PC.
The first key difference between the global and the nested geodesic PCA is that the
latter provides a notion of preferential directions in the principal component, while the
first one does not. In fact, the first nested principal component corresponds to the first
principal direction, and it is possible to find the remaining principal directions by imposing
orthogonality constraints as we obtain nested PCs of higher dimensions. Thus, the nested
geodesic PCA is more suitable to explore and visualize the variability in a data set, see
also Section 7. On the other hand, exactly because of the lack of such constraints, the
global PCA is in general more flexible and provides superior performance in terms of
reconstruction error, cf. Section 7.
Comparing these definitions with the one of our projected PCA, the key difference
is that geodesic PCAs do not exploit the Hilbert structure of H. Thus, as we discuss
in Section 5.3, the numerical routines needed to find such principal components rely on
nonlinear constrained optimization, which can be extremely demanding and nontrivial to
implement. This is in sharp contrast with our projected PCA in Definition 1, that, thanks
to Lemma 1 can be straightforwardly computed. However, as a result, the projected PCA
is in general less respectful of the underlying metric structure. By investigating this issue
in simpler settings, for instance when H = Rd and X is a convex polytope in Rd , we noticed
that the differences between the projected principal directions and the nested geodesic ones
become appreciable only if the random variable X gives significant probability to values
near the borders of X. See for instance Figure 1. While this intuition remains valid also
in the more complex setting that we investigate in this paper, it is harder to imagine
realizations of X near the borders of X.
Note that the interpretability of the projected PCA is determined by the level of discrepancy between the definitions, as in Figure 1, which depends on how much variability
it is correctly captured by the component, that is how much of the variability captured
by the projected component lies in X. This intuition is formalized in Section 7.1.1 where
two measures of “reliability” of the projected PCA are proposed.
Intrinsic methods have been proposed also to tackle regression problems. Chen et al.
(2020) define a distribution on distribution linear regression model in the Wasserstein
space. Their approach consists in considering two different tangent spaces of W2 (R) (the
first one centered in the barycenter of the independent variable and the second one centered
in the barycenter of the dependent variable) and map the observations to the corresponding
b between
tangent spaces. They then use FDA tools to estimate a functional linear model Γ
those two spaces. We point out two main differences, a practical and a theoretical one.
First, the approach in Chen et al. (2020) involves a higher computational cost compared
to our projected regression, mainly because they do not resort to B-splines and need to
approximate transport maps, which in the general setting can be burdensome, cf. Cazelles
et al. (2018). Second, to derive the asymptotic properties of their model, Chen et al. (2020)
assume that the image of the regression operator Γ lies inside the image of the log map
12

Tangent Space

Quantile Functions
1.0

40

0.8

20

0.6
0
0.4
20

0.2

40

0.0
0.0

0.2

0.4

0.6

regression output

0.8

1.0

40

metric projection

20

0

20

40

boundary projection

Figure 2: Comparison between the metric projection in Section 4 and the boundary projection when the base point µ is the uniform measure on [0, 1]. In the left panel are represented
the elements of the tangent space while in the right panel the associated quantile functions.
The blue line represents a possible regression output, lying slightly outside (in terms of
L2 norm) the image of logµ , the orange line is the projected linear regression output while
the green one corresponds to the boundary projection operator in Chen et al. (2020).
centered in the dependent variable’s barycenter. As the authors in Chen et al. (2020)
notice, this is hardly verified in practice, so that whenever the output of the regression
operator is not a distribution, they resort to squeezing such a value with some scalar
multiplication, namely “boundary projection”, which in general is not a metric projection.
The difference between the metric projection we employ and the boundary projection,
for a possible regression output, is depicted in Figure 2. Note that, by construction,
such a procedure shrinks the tails of the output. Even when the regression output is
slightly outside the image of the log map, the boundary projection result can be extremely
far from the regression output and from the metric projection in terms of Wasserstein
distance. For example, in Figure 2, the regression output and the projected method assign
positive probability to values in the range [−45, 45], while the output of the boundary
projection assigns zero probability to values outside [−17, 17]. This underrepresentation
of the variability might be a crucial issue depending on the application considered.
3.4 Comparison with log PCA
Cazelles et al. (2018) propose the definition of a log PCA as an alternative to the geodesic
PCAs in Bigot et al. (2017). Both the log and the projected PCA are extrinsic methods:
they proceed by carrying out the PCA in a linear space H and then map back the results
to the Wasserstein space.
For the log PCA, H is the tangent space at µ, for the projected H is Lµ2 (R). Given Uk =
{w1 , . . . , wk } the first k H-principal components, the log principal component in W2 (R) is
expµ (Sp(Uk )) . Analogously, by considering the convex cone X =: logµ (W2 (R)) ⊆ H, the
principal component in X is logµ expµ (Sp(Uk )) .
We notice two key differences between the log and projected PCA. First, as pointed out
in Remark 2, logµ ◦ expµ is not a metric projection in Lµ2 so that given a point x ∈ H \ X,
logµ (expµ (x)) might end up being extremely different from x. In the context of PCA, this
13

means that as soon as the projection onto Sp(Uk ) of observation lies outside of X, the PCA
is no longer interpretable.
Second, as discussed in Remark 4, there is no guarantee that

logµ expµ (Sp(Uk )) remains a convex set and also that the dimension of the X principal
component agrees with the one of the H principal one. Lastly, log PCA cannot, in general,
define a set of principal directions which span the principal component. Hence, it is not
possible to work directly on the scores of the PCA.
Combined, we believe that the above mentioned issues present a major drawback of
the log PCA when compared to the projected PCA, as they prevent the possibility of doing
proper dimensionality reduction and working on the scores of data points on the principal
components. Finally, we also point out that approximating the expµ map is a nontrivial
task, involving computing numerically the preimages of an arbitrary large number of sets
and numerical differentiation, that can lead to numerical instability of the log PCA.

4. Computing the metric projection through B-spline approximation
The projected methods defined in Section 3 depend heavily on the availability of projection
operators on the closed convex cone X = ϕµ (W2 (R)). Being X a cone inside a linear space,
such operators are always well defined, but their implementation might be nontrivial. In
this Section, we present a possible solution to this problem, based on choosing a particular
µ as base point and constructing a B-spline representation of the cone X.
4.1 Choosing µ as the uniform distribution on [0, 1]
As already mentioned, our projected methods can be carried out by choosing µ arbitrarily and there is no theoretical difference between different choices of µ, cf. Section 2.2.
Nonetheless, in practice, a clever choice of µ can lead to substantially easier and more
numerically stable algorithms. For instance, by choosing a measure µ with compact support C in R, then the ambient space becomes Lµ2 (C) since we work up to zero-measure
sets. This greatly simplifies any numerical procedure since we could work with grids over
bounded sets, and do not need to resort to any truncation procedure, which would be
mandatory in case the support of µ was unbounded. Moreover, note that evaluating the
maps ϕµ in a certain measure ν amounts to computing the transport map Tµν = Fν− ◦ Fµ ,
hence it is clear that the choice of Fµ numerically influences the results.
For the aforementioned reasons, we argue that a reasonable choice is to center our
analysis in µ = U ([0, 1]). In fact, in this case, Lµ2 (R) = L2 ([0, 1]), and Fµ = id[0,1] (the
transport maps are simply given by quantile functions).
4.2 Metric Projection
Having chosen µ as Section 4.1 leads to an explicit characterization of the image of ϕµ as
the set of square integrable a.e. non-decreasing functions on [0, 1]. Hence, the operator
ΠX in Section 3 is the metric projection onto the cone of a.e. non-decreasing functions in
L2 ([0, 1]).
Projection onto monotone functions has been widely studied in the field of order restricted inference, (Anevski et al., 2006; Dykstra et al., 2012). For instance, in Anevski
and Soulier (2011) an explicit characterization of such a projection is given, which however does not lead to a closed form solution, while in Ayer et al. (1955) several numerical
algorithms to approximate the projection operator are proposed. Those algorithms are
based on approximating the function to be projected with a step function defined on n

14

intervals and can be shown to have a computational complexity that is linear in n (Best
and Chakravarti, 1990).
Despite the numerical convenience of the aforementioned approximations, we believe
that they are not suited for distributional data analysis. First and foremost, suppose that
observations are given as probability density functions, so that one may want to interpret
the results of a PCA, for instance, in terms of pdfs and not of quantile functions. If one
were to estimate discontinuous principal directions through any of the algorithms in Ayer
et al. (1955), it would not be possible to do so, as the corresponding cdfs would not be
differentiable. In addition to that, the choice of the number of intervals n is not obvious
when quantile functions are not directly observed but obtained with transformation. If n
needs to be big to faithfully approximate the true quantile functions, this projection can
be quite slow.
For these reasons, we propose to resort to a B-spline expansion, through which we can
derive an alternative approximation of the projection operator ΠX , without incurring in
the issues of the algorithms in Ayer et al. (1955). Moreover, we will also show in Section 5.3
that the proposed B-spline expansion also leads us to a simpler and faster reformulation
of the geodesic PCA in Bigot et al. (2017).
4.3 Monotone B-splines representation
In what follows, let µ = U ([0, 1]). Moreover, denote with x = [x1 , . . . , xk ]0 ∈ Rk a generic
vector.
As already said, through the ϕµ map, we can identify W2 (R) with the space
L2 ([0, 1])↑ := {F − ∈ L2 ([0, 1]) s.t. F − is monothonically nondecreasing}
This leads us to consider a suitable B-spline basis for the space, to efficiently evaluate
all the computations needed in our algorithms and for a convenient way to express the
constraints which define L2 ([0, 1])↑ . In particular, we consider the basis of quadratic splines
with equispaced knots in [0, 1]. The reason for this particular choice is two-folded. First
of all, splines of degree greater than one enjoy the nice property of uniform approximation
of all continuous functions as the maximum distance between knots goes to zero, in turn
this means that the closure of the linear space generated by the spline basis w.r.t the L2
norm coincides with L2 ([0, 1]). Secondly, quadratic splines are particularly well suited to
characterize monotonic functions by looking at the coefficients of the (quadratic) B-spline
expansion, as shown in the next Proposition.
Proposition 4 Let {ψjk }Jj=1 be a basis of B-splines of order k defined over the knots
P
x1 , . . . , xJ+k+2 . Let f (x) = Jj=1 aj ψjk (x), then:
1. If the coefficients {aj } are monotonically increasing (decreasing) f is monotonically
increasing (decreasing)
2. If k = 2, then 1. holds with an “if and only if ”
Before proceeding, let us fix some notation. From now on, we omit the dimension
index “k” for the spline basis, writing ψj for ψj2 , moreover we will let {ψj }Jj=1 with fixed
J > 0 denote a B-spline basis in L2 ([0, 1]).
Remark 7 Let RJ↑ be the set of vectors v ∈ RJ withP
nondecreasing coefficients. That is,
letting G = {gij } be the J ×J binary matrix such that j gij vj = vi −vi−1 , for any element
15

v ∈ RJ it holds that Gv ≥ 0. Using Proposition 4, through the coordinates operator, the
set L2 ([0, 1])↑ ∩ Span{ψj }Jj=1 is fully identifiable with RJ↑ , endowed with the metric given
by the symmetric positive definite matrix E with entries
Eij = hψi , ψj iL2 ([0,1]) .

(9)

The norm induced is therefore kxk2E = xT Ex.
Remark 8 It is possible to find a basis for RJ with vectors lying in RJ↑ (and so in XJ ),
namely the vectors (0, . . . , 0, 1), (0, . . . , 0, 1, 1) etc. In other words, Span(L2 ([0, 1])↑ ∩
Span{ψj }Jj=1 ) = Span{ψj }Jj=1 for every J > 0. This tells us that the convex cone of
monotone splines is indeed quite big inside the spline space, and this a priori is beneficial
for extrinsic methods, especially for PCA.
From now on, to lighten the notation, we deliberately confuse the coefficients of
the splines, living in RJ or RJ↑ (with the metric given by E), with the corresponding
spline functions living in the subsets of L2 ([0, 1]) given by L2 ([0, 1])↑ ∩ Span{ψj }Jj=1 and
Span{ψj }Jj=1 .
Remark 9 Lastly, we point out that RJ↑ has the structure of a convex polytope, since
the constraints given by Gv ≥ 0 (guaranteeing that v ∈ RJ↑ ) are linear. Such geometric
property makes optimization on RJ↑ handy and is key for the empirical methods developed
in the remaining of the paper.
As a consequence of Remark 9, the optimization problem given by the projection of a
vector v ∈ RJ onto RJ↑ can be formulated as follows:
ΠRJ↑ (v) = arg min kv − wkE .

(10)

Gw≥0

The computational complexity required to solve (10) is at most cubic in the number of
basis elements J (Potra and Wright, 2000).
Preliminary analysis showed that solving the optimization problem in (10) compares
favorably with the Pool Adjacent Violators Algorithm (PAVA) in Ayer et al. (1955). In
particular, computing PAVA with n = 100 approximation intervals is roughly eight times
slower than (10) with J = 20 (a reasonable choice, leading to negligible approximation
error, in our examples, with a quadratic spline basis). Increasing n = 1000 for PAVA
makes it 700 times slower than (10).
In addition to that, resorting to a discretized approximation of quantiles would also
increase the cost of the projected PCA, due to the need of using some functional PCA
implementation, as opposed to the low-dimensional multivariate model we are able to
implement with the B-spline basis functions.

5. Empirical Models with B-splines
In this Section, we present the empirical counterparts of the projected PCA defined in
Section 3 and provide an illustrative example of projected linear regression, namely when
both the dependent and independent variables are distributions.
Let {ψj }Jj=1 be a fixed quadratic B-spline basis. Upon approximating the observed
quantile functions with their spline expansion, thanks to Remark 7, we can develop our
methodology in RJ , considering the metric induced by E instead of the usual one. Indeed,
16

given P
a vector w ∈ RJ , we can identify the corresponding function in L2 by the map
w 7→ Jj= wj ψj .
For the projected PCA in Section 5.1 and for the geodesic PCA in Section 5.3 we
consider observations F1− , . . . , Fn− , and let F0− be the centering point of the PCA. In our
examples, F0− will always be the barycenter of the observations. As a preprocessing step,
we approximate each of these quantile functions through a B-spline expansion and denote
by ai = {aij }j and a0 = {a0j }j the coefficients of the spline representation associated
PJ
to Fi− and F0− respectively, that is, Fi− ≈
j= aij ψj . For the projected regression in
−
−
−
n
Section 5.2, let observations {(Fz , Fy )i }i=1 , where the Fzi
’s are realizations of the in−
dependent variable Z and the Fyi ’s are realizations of the dependent variable Y. We
(z)

apply the same preprocessing step and let ai
−
−
approximation of Fzi
and Fyi
respectively.

(y)

and ai

denote the coefficient of the spline

5.1 Empirical PCA
Denote with A the (n × J) matrix with rows a1 , . . . , an . As in standard PCA, the first
principal component centered in a0 is found by solving the optimization problem:
X
w1∗ = arg max
|hai − a0 , wiE |2 = arg max kAEwk2
(11)
w:kwkE =1

w:kwkE =1

i

where A is the matrix whose i–th row is given ai − a0 . The optimization problem (11) can
be solved similarly to a Rayleigh quotient: using Lagrange multipliers, (11) is equivalent
to
L(w) := wT (A E)T A E w − λ(wT E w − 1)
(12)
Deriving (12) w.r.t w and equating the derivative to zero shows that the solutions to
dL(w)/dw = 0 are the eigenvectors of the matrix AT AE. Hence, ordering the eigenvalues
of AT AE in decreasing order, the first principal component w1∗ corresponds to the first
eigenvector. Using similar arguments it can be shown that w2∗ , . . . wJ∗ correspond to the
remaining eigenvectors.
Once the first k principal directions w1∗ , . . . , wk∗ are found, the projection of a new
PJ
k,x0
∗
(see Lemma 1) is found using the projection
observation x∗ =
j=1 aj ψj onto UX
operator in (10). In particular, the following optimization problem is to be solved:
arg mink(ha∗ − a0 , wi∗ iE − λi )ki=1 k
λj ∈R

s.t. G

k
X

λi wi∗



,

(13)

+ a0 ≥ 0

i=1

which is equivalent to the minimization of a norm inside a polytope, that is a well-studied
problem in RJ (see Sekitani and Yamamoto, 1993) and there exist a variety of fast numerical routines to solve it.
5.2 Empirical Regression
In this Section, we provide the details of the estimation procedure for a projected regression
model where both the independent and the dependent variables are distribution-valued.
It is straightforward to extend our methodology to cases when only one of these variables
is distribution-valued and the other one takes values in Rq .
First, we outline how to obtain an estimator for the linear operator Γ in Definition 4.
Following Section 3.2 we first embed both Y and Z in L2 ([0, 1]) through the inclusion
17

operator L2 ([0, 1])↑ ,→ L2 ([0, 1]), and assume the functional linear model presented in
Ramsay (2004) and Prchal and Sarda (2007)
Z 1
β(t, s)Z(s)ds + ε(t),
t ∈ [0, 1]
(14)
Y(t) = α(t) +
0

R1
so that Γ = Γα,β is the operator Γα,β (v)(t) = α(t) + 0 β(t, s)v(s)ds. The goal is then
to estimate α ∈ L2 ([0, 1]) and β ∈ L2 ([0, 1]2 ). Further, we assume that ε and Z are
uncorrelated: E[Z(s)ε(t)] = 0 for every t, s ∈ [0, 1].
Consider now observations {(Fz− , Fy− )i }ni=1 and the corresponding spline coefficients.
P
Further, we project α(t) on the same spline basis, so that α ≈ Jj=1 θαj ψ(j) and β(t, s) on
P
the basis on [0, 1]2 with J ×J elements, so that β(t, s) ≈ Ji,j0=1 Θβij ψi (t)ψj (s). Neglecting
the spline approximation error, model (14) entails
(y)

ai

(z)

(ε)

= θα + Θβ Eai + ai ,

i = 1, . . . , n

(15)

(ε)

where ai denotes the spline expansion coefficients of the unobserved error εi (t).
We propose to estimate (15) using the same approach of Prchal and Sarda (2007), but
extending it to account for spline approximations for both dependent and independent
b β of Θβ since once such estimate is obtained,
variables. We focus only on the estimate Θ
the estimate for aα can be straightforwardly derived, (see Cai and Hall, 2006) as:
b β Ea(z)
θ̂α = a(y) − Θ
where a(y) and a(z) are the means of a(y) and a(z) respectively.
b β is found by penalized least square minimization:
The estimator Θ
n




X  (y)
(z)
b β = arg min 1
Θ
k ai − a(y) − ΘE ai − a(z) k2 + ρPen(1, Θ)
n
Θ

(16)

i=1

where ρ > 0 is a penalization parameter to be fixed (usually through cross-validation) and
Pen(1, Θ) is a penalization term defined in Prchal and Sarda (2007).
Briefly, the term Pen(1, Θ) in (16) penalizes both the norm of β(t, s) and its derivatives,
thus favoring smoother solutions. As shown in Prchal and Sarda (2007), (16) has a closed
form solution. Nonetheless, the form of our solution differs from the one presented in Prchal
and Sarda (2007), since they work directly on discretized functions while we propose to
estimate spline coefficients, and some care must be taken since they can use (up to scaling)
the usual inner product in the Euclidean space of discretized functions, while we must
consider the inner product induced by E. However, the procedure for obtaining our result
is identical to the one in Prchal and Sarda (2007). Hence we only report the expression
for the estimate.
Let Ĉ be the matrix with entries
n

Ĉks = h

1 X (z)
(z)
hai , bk iE ai , bs iE ,
n
i=1

where bk and bs are the k-th and s-th elements of the standard Euclidean basis in RJ .
Further let D̂ the matrix with entries
n

D̂ks

1 X (z)
(y)
=h
hai , bk iE ai , bs iE .
n
i=1

18

0 =< ψ 0 , ψ 0 > (where ψ 0 denotes the first
Finally, let E 0 denote the matrix with entries Eij
i
j
i
derivative of the B-spline basis function ψi ), Cρ = E T ⊗(Ĉ +ρE 0 ), and P = E 0T ⊗E +E T ⊗
E 0 , where ⊗ denotes the Kronecker product. Then the solution of (16) can be expressed
as
b β ) = (Cρ + ρP )−1 vec(D̂)
vec(Θ

where vec(·) denotes the vectorization of the matrix.
Finally, our projected regression model is the composition of the operator induced by
(θ̂α , Θ̂β ) with the projection on R↑J :


(y)
(z)
(z)
b β Ea(z) .
E[ai | ai ] = ΓP (ai ) = ΠRJ↑ θ̂α + Θ
i
5.3 An alternative optimization routine for the geodesic PCA and a
comment on the computational costs
We now show how the framework in Section 4 can be employed also to derive faster
numerical algorithms to find the global and nested geodesic PCA as of Definition 5 and
Definition 6.
Proposition 5 (Global geodesic PCA) A k dimensional global geodesic PC centered in
a0 is the subset of RJ↑ spanned by {w1 , · · · , wk }, linearly independent, which solve:
arg min

n
X

||ai − a0 −

k
{λi }n
1 ,{wj }1 i=1

s.t. G

k
X

λij · wj ||2E

j=1

X

(17)


λij wj + a0 ≥ 0

j

Proposition 6 (Nested geodesic PCA) With the same notation as above, a k dimensional
nested geodesic PC, centered in a0 is the set spanned by {w1 , · · · , wk } in RJ↑ , where the
wi s are found recursively from w1 to wk , such that wh is a solution, for every h, of:
arg min
{λi }n
i=1 ,w

n
X

kai − a0 − λi wk2E

i=1

s.t. hwj , wiE = 0, j = 1, . . . , h − 1


G λi w + a0 ≥ 0, kwkE = 1

(18)

To solve (17) and (18) we employ an interior point method using the solver Ipopt
(Waechter and Biegler, 2006). When comparing our implementation with J = 20 spline
basis and the one in Cazelles et al. (2018), we notice a substantial performance improvement, by a factor of 35 for a data set of n = 100 distributions, due to the fact working
with spline approximations reduces greatly the number of parameters in the optimization
problem.
Further, note that (17) and (8) seem extremely similar. However, in (8) the optimization is carried out having fixed w1∗ , . . . , wk∗ and for a single observation, while in (17)
the optimization is done over a much larger set of parameters. In fact, the number of
parameters in (17) is (n + k)J, hence the computational complexity needed to solve (17)
is cubic in both the number of bases and the number of observations. On the other hand,
the projected PCA requires a linear time in the number of observations (computation of
AT AE) and cubic time in the number of basis J (eigendecomposition and projections of
new observations).
19

6. Asymptotic Properties
In this Section, we study the convergence of the proposed projected empirical methods.
First of all, we show that as the number of spline basis J increases, the error due to the
spline approximation vanishes if the data is sufficiently regular. Further, under a suitable
set of assumptions, we establish consistency results for the projected PCA and for the
projected distribution on distribution regression.
6.1 Convergence of Quadratic B-splines
In the following, denote with Wkr ([0, 1]) the space of functions whose weak derivatives up
to order k belong to Lr ([0, 1]), further denote with D the (weak) derivative operator, so
that Df = f 0 , D2 f = f 00 and so on,
Proposition 7 Let µ a probability measure on R, Fµ− its quantile function such that
Fµ− ∈ W3∞ . For each J let {ψj }Jj=1 denote a quadratic B-spline basis on J equispaced
P
(J)
knots in [0, 1]. Then there exist a sequence of spline functions SJ = Jj=1 λ(J) ψj , with
(J)

λj

monotonically non-decreasing in j for every J, such that:
kSJ − Fµ− k∞ ≤ CkD2 fµ− k∞ J −2

with fµ− = DFµ− and C > 0 constant.
Let us remark two important facts.
Remark 10 Since the inclusion L∞ ([0, 1]) ⊂ L2 ([0, 1]) is continuous, thanks to Hölder
inequality, the convergence rates hold also for the L2 norm. By default we will use the L2
norm if not stated differently.
Remark 11 By Poincaré inequality, if kD3 f k∞ < C then f belongs to a sphere in
W3∞ ([0, 1]) whose radius depends on C and on the Poincaré constant of [0, 1]; viceversa,
all the elements in the sphere of radius C in W3∞ ([0, 1]) clearly have (weak) derivatives
bounded by C.
6.2 Consistency
In this Section we prove the consistency of the projected methods under some assumptions
on the data-generating process. In particular, we show that that there exists a number of
basis functions J > 0 and a sample size n such that the error committed by the empirical
models in Section 5 is smaller than ε > 0, for any fixed ε.
6.2.1 PCA
Consistency of spline-based PCA for functional data has been addressed, among the first,
by Silverman et al. (1996) and Qi and Zhao (2011). As one of the main building blocks
of our projected PCA is the PCA in the ambient space, that is L2 ([0, 1]), it is natural to
follow Qi and Zhao (2011) in making the following assumptions. Consider data µ1 , . . . , µn ,
F1− , . . . , Fn− the corresponding quantile functions, then:
(P1) The data generating process satisfies F1− , . . . , Fn− ∼ F with the Fi− independent and
E[F] = 0.

20

(P2) F1− , . . . , Fn− can be approximated by functions in W3∞ with uniformly bounded third
derivative.
(P3) E[kFi− (t)k4 ] < ∞, i = 1, . . . , n.
(P4) The eigenvalues of the covariance operator of F have multiplicity 1.
(P5) The eigenfunctions of the covariance operator of F belong to some bounded set in
W3∞ ([0, 1]) ⊂ W32 ([0, 1]).
Before stating the main results, let us comment on assumptions (P1)-(P5). First
of all, (P2) is essential in order to apply Proposition 7 and get uniform errors on the
data set. Moreover, (P2) is satisfied, for instance, if the Fi− ’s lie in the L2 -closure of
a ball of radius M > 0 in W3∞ . (P4) is a rather standard condition and is satisfied if
µ1 , . . . , µn ∈ W4 (R). (P4) and (P5) imply the assumptions that in Qi and Zhao (2011)
are used for the consistency results. In particular, (P5) is stronger than the corresponding
assumption in Qi and Zhao (2011), where the eigenfunctions are assumed to belong to
W22 ([a, b]). Similarly, in such work, there is no counterpart of assumption (P2); in fact we
need these stronger regularity conditions to get uniform errors when using B-splines. Still
some of the examples Qi and Zhao (2011) provide of situations satisfying their assumptions,
meet also our requirements. Finally, the zero-mean assumption in (P1) might seem a little
odd, since we know that the quantile functions are monotonically nondecreasing. However,
observe that it is always possible to subtract the empirical mean from the observations to
satisfy (asymptotically) this assumption.
Let J denote the dimension of a quadratic B-spline basis on [0, 1] and let aJi the
coefficients of the B-spline approximation of Fi− . In what follows, to lighten the notation,
we refer to a set of spline coefficients both as elements of RJ with the E-norm, or as
functions in L2 , without making explicit reference to the coordinate operator and its
inverse.
Proposition 8 Under assumptions (P1)-(P5), for any ε > 0 there exists a sample size
n > 0 and a number of basis functions J > 0 such that:
1X −
1X J
hFi , wi2L2 − max
hai , wi2E < Kε
kwkL2 =1 n
kwkE =1 n
max

i

i

for some constant K > 0.
Proposition 8 ensures the consistency of the B-spline approximation of the PCA for monotone functional data in H which is equivalent to the consistent estimation of the projected
principal directions.
Suppose now to have computed UkJ = {whJ∗ }kh=1 , that is the approximations of the
principal directions Uk = {wh∗ }kh=1 found with J basis functions. We observe that Sp(UkJ )∩
↑ = Sp(U J ) ∩ RJ↑ . Since for any set of coefficients λ we have the convergence
L
h
P2 ([0, 1])
P k∗
J∗
λh wh →
λh wh , we obtain that the projection of a point onto Sp(UkJ ) ∩ L2 ([0, 1])↑
converges to the projection onto Sp(Uk ) ∩ L2 ([0, 1])↑ . Thus we also have convergence of
the projection onto the principal components.
6.2.2 Regression
We consider model (14) given samples {(Fz− , Fy− )i }ni=1 . We make the following assumptions:
21

(R1) The data generating process satisfies (14) and E[Z(s)ε(t)] = 0 for every t, s ∈ [0, 1].
(R2) α ∈ L2 ([0, 1]) and β ∈ L2 ([0, 1] × [0, 1]).
(R3) With probability 1, each quantile function in the samples {(Fz− , Fy− )i }ni=1 lies inside
3 ([0, 1]).
a sphere of radius K > 0 in W∞
Without loss of generality, suppose that both the dependent and the independent
variables have been centered by subtracting their mean so that E[Z] = E[Y] = 0 and
α = 0.
The strategy to prove the consistency of the projected linear regression is the following.
b J converges to the estimator Θ
b PS , defined in
First of all, we prove that the estimator Θ
Prchal and Sarda (2007), for large enough n and J. Second, we exploit the consistency of
the estimator in Prchal and Sarda (2007) combined with the approximation results of the
metric projection, to establish consistency in terms of the prediction error of our projected
regression operator.
b PS is obtained by minimizing an objective function similar to the one in (16),
Briefly Θ
−
−
but where the spline approximation is used only for Θ, while the Fzi
’s and the Fyi
’s are
assumed fully observed, and not approximated through splines. Calling B the vector of
b PS is defined as:
functions with entries ψ1 , . . . , ψJ , Θ
X
−
−
b PS = arg min 1
kFyi
− hFzi
, B T ΘBik2 + ρPen(1, Θ).
Θ
n
Θ
i

b J to Θ
b PS is shown in the next proposition
Convergence of Θ
b
Proposition 9 Under assumptions (R1)-(R3), if the number of samples is big enough Θ
b
b
b
and ΘJ exists with probability close to 1, and there is J > 0 such that kΘPS − ΘJ kE⊗E < ε.
b PS B and βbJ = B T Θ
b J B. Since kβbPS (s, t) −
Let βbPS and βbJ be the kernels βbPS = B T Θ
b PS − Θ
b J kE⊗E , we established strong converge of our kernel to the
βbJ (s, t)kL2 ([0,1]2 ) = kΘ
estimator of Prchal and Sarda (2007). This implies that the consistency results for the
b PS holds also for Θ
b J , with respect to the seminorm induced by the covariance
estimator Θ
operator of Z.
Specifically, given Z H-valued random variable and its covariance operator CZ , for any
ϕ ∈ L2 ([0, 1]2 ), we consider the semi-norm on L2 ([0, 1]2 ) given by:
Z
kϕkΓZ =
hCZ ϕ(·, t), ϕ(·, t)idt
[0,1]

Thus, the following result is immediately implied since strong convergence implies
seminorm convergence (see Appendix A).
Corollary 1 For J > 0 big enough E[kβ − βbJ kCZ ] < ε.
Proof We use the seminorm triangle inequality:
b C + kβb − βbJ kC .
kβ − βbJ kCZ ≤ kβ − βk
Z
Z
The first term on the right hand side converges to zero thanks to Theorem 2 in Prchal
and Sarda (2007), while the second term converges to zero thanks to Proposition 9 and the
previous observations.

22

b Γ
Lastly, we need to take into account the projection step. First, we notice that kβ− βk
Z
corresponds to the expected prediction error, in fact, as in Prchal and Sarda (2007):
Z
h
i
b
kβ − βJ kCZ =
E hZ, β(·, t) − βbJ (·, t)i2 | βbJ dt,
[0,1]

h
i
further, by Hölder’s inequality E |hZ, β − βbJ i| βbJ → 0, which straightforwardly yields
h
i
E kΓβ (z) − ΓβbJ (z)k βbJ → 0.
Thus, the following simple lemma ensures the consistency of the spline approximation
of the projection on X and leads to the consistency of the projected regression in terms of
prediction error. Again, following Remark 7, we can identify the space monotone B-splines
with J basis functions with RJ↑ . Hence, to lighten the notation, we denote ΠRJ↑ the metric
projection operator onto the space of monotone B-splines with J basis functions.
Lemma 2 Given βn → β in H, for any ε > 0 there exists n, J > 0 such that kΠRJ↑ (βn ) −
ΠL2 ([0,1])↑ (β)k ≤ ε.

7. Simulation Study
In this Section we test our projected statistical methods on different simulated data sets.
In Section 7.1 we address performance and intepretability issues for the projected PCA. In
Section 7.2 we test the projected linear regression on data generated from a linear structure
different from the natural cone structure on L2 ([0, 1])↑ presented in the previous Sections.
7.1 PCA
We consider three different simulations to compare both the interpretability and the ability
to compress information of different PCAs.
We compare our projected PCA with the nested and global geodesic PCAs (Bigot et al.,
2017; Cazelles et al., 2018) and the simplicial PCA (Hron et al., 2014).
Briefly, the simplicial PCA applies a transformation that maps densities defined on the
same compact interval I into functions in L2 (I), called centered log ratio. Then, a standard
L2 PCA is performed on the transformed pdfs and, by the inverse of the centered log ratio
transform, the results are mapped back to the space of densities, called Bayes space (for
a more accurate definition, see Egozcue et al., 2006). In particular, we remark that, to
be well defined, the simplicial PCA requires that all the pdfs have support equal to I,
which is a strong assumption in practice. Further details about simplicial PCA are given
in Appendix B.
For the projected, nested and geodesic PCAs we need to fix a B-spline basis to express
the quantile functions on [0, 1]. In particular, we fix an equispaced quaratic B-spline basis
with J interior knots on [0, 1]. Similarly, to compute the simplicial PCA, we resort to
another B-spline approximation on the transformed pdfs. Hence, we need to select a Bspline basis on the support of the pdfs I. In this case, we fix a cubic B-spline basis with
J 0 interior knots on I. In all the simulations presented, we fix J = J 0 = 20 B-spline basis
as this choice yielded a negligible approximation error for both quantile functions and
transformed pdfs.

23

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
10.0

7.5

5.0

2.5

0.0

2.5

5.0

7.5

10.0

Figure 3: Data set of pdfs generated from (19)

1st PD

SIMPLICIAL

WASS - GLOBAL

0.30

0.8

0.25

0.4

0.25

0.6

0.20

0.3

0.20

0.15

0.4

0.0

4

2

0

2

4

0.00

4

2

0

2

0.0

4

0.7

0.8

0.0

10

5

0

5

10

0.3

0.3

0.2

0.2

0.1

0.1

0.0

0

5

0.4

0.4
0.2

5

0.5

0.5

0.4

0.05

0.6

0.6

0.6

0.10

0.1

0.05

5

0

0.0

5

WASS - NESTED

0.15

0.2

0.10

0.2

2nd PD

WASS - PROJECTED

0.30

5

0

5

0.00
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

5

0

5

0

5

5

Figure 4: Top row: first principal direction. Bottom row: second principal direction. Each
line represents the pdf associated to λwi where wi is the i–th principal direction (i = 1, 2)
and λ is a score ranging from −2 (darkest blue) to +2 (darkest red).
In the first scenario, we simulate data from


1
exp (x − µi )2 (2σi2 ) I(x ∈ [−10, 10]),
σi
µi ∼ 0.5N (−3, (0.2)2 ) + 0.5N (3, (0.2)2 )

pi (x) ∝

i = 1, . . . 100
(19)

σi ∼ Uniform([0.5, 2.0])
Where “proportional to” stands for the fact that we confine the density to the support
[−10, 10] and renormalize it so that it integrates to 1.
Observe that there are two sources of variability across the pdfs from the data generating process (19). The first one is the location of the peak µi and the second one is the
width of the distribution around the peak, controlled by σi . See Figure 3.
Figure 4 shows the first two principal directions obtained using the different methods.
We can notice several differences between them. Focusing on the first principal direction,
we can see that the simplicial, projected and nested PCAs detect a change in the location
of the peak of the pdf. In particular, the first direction for the Wasserstein PCAs represents
24

a shift from left to right of this peak, while for the simplicial PCA the first direction is
associated to a peak in 3 (blue lines, negative values of the scores) or to a peak in −3 (red
lines, positive value of the scores). This also highlights the difference in the geometries
underlying the Wasserstein and Bayes spaces. Looking at the second principal direction
instead, we can see how in the Wassrestein PCAs it clearly represents a change in the
width of the distribution, while for the simplicial PCA the interpretation is somewhat
obscure.
The global geodesic PCA deserves a separate discussion. Indeed, from Definition 5 it is
clear that a global principal component is a convex set without any notion of preferential
directions, so that it is not possible to interpret separately the variation along the first
and second direction found by the global PCA.
Now we present two additional simulations that quantify the amount of information
that is “lost” by performing the PCA. As a metric, we consider the reconstruction error,
that is, the quantity
n
1X
REk =
W2 (Fi− , Fei− )
(20)
n
i=1

Fi− ’s

where the
are the observed probability measures, Fei− are the reconstructed ones
and k is the dimension of the principal component. More in detail Fei− is found by first
projecting (Fi− −F0− ) into Rk using the PCA and then applying the inverse transformation.
Informally, the reconstruction error is a measure of the quantity of information lost by
applying the PCA as a black-box dimensionality reduction.
As evident in Equation (20), we measure the performance of PCAs just in terms of
Wasserstein metric. This is likely to favor the performance of the Wasserstein PCAs over
the simplicial one. Thus, the interesting performance comparison is the one between the
geodesic PCAs and the projected PCA. Nevertheless, we think that is worth reporting
also the results for the simplicial PCA, which is an intrinsic method in the Bayes space,
to show that the underlying metric structures are extremely different. This also helps
to appreciate the results in Section 7.2. Given the difference in the metric structure
between Wasserstein and Bayes spaces, we believe that the choice between simplicial and
Wasserstein frameworks is not trivial and should be application-driven.
To measure raw performance differences between geodesic and projected PCAs, we
simulate data so that there is little recognizable structure in them, unlike in the previous
example. The data generating process is as follows:
pi (x) ∝

K
X
j=1

wij

 2 
1
) I(x ∈ [−10, 10]) + 10−5 ,
exp (x − µij )2 (2σij
σij

i = 1, . . . 100

wi ∼ DirichletK (1/K)
(µij , σij ) ∼ N (dµij ; 0, 22 )Uniform(dσij , 0.5, 2.0)
(21)
Observe that (21) is a finite dimensional approximation of the Dirichlet Process mixture
model, a popular workhorse in Bayesian nonparametric statistics, that is well known to
be dense in the space of densities on R, see for instance Ferguson (1983). An example of
the kind of pdfs generated from (21) is shown in Figure 5(a).
To separate the effect of the B-spline smoothing procedure, in this scenario we evaluate
the reconstruction error in (20) considering µ
ei to be the reconstructed quantile functions
(for the Wasserstein PCAs) or pdfs (for the simplicial PCA) and µi to be the probability measure represented by the B-spline approximation of the quantile function or the
(centered log ratio of) the pdf respectively.
25

projected
simplicial
nested
global

0.6
0.5
0.4

10

1

10

2

0.3
0.2
0.1
0.0
10.0

7.5

5.0

2.5

0.0

2.5

5.0

7.5

10.0

2

3

4

(a)

5

6

7

8

9

10

(b)

Figure 5: Left panel: example of simulated data set for Scenario 2. Right panel: reconstruction error as a function of the dimension of the principal component employed
for the different methods. The solid lines represent the mean of 10 independent runs on
independent data sets from (21) and the shaded area represent ± one standard deviation.
Figure 5(b) shows the reconstruction error as a function of the dimension of the principal component, that is, REk as a function of k. We can see how the three Wasserstein
PCAs consistently outperform the simplicial one. Moreover, as to be expected, the global
geodesic PCA obtains the lowest reconstruction error for all the choices of dimension k,
with the nested geodesic PCA being a close runner-up. However, the computational cost of
finding the nested or global geodesic PCA can become prohibitive as the sample size or the
number of bases in the B-spline expansion or the dimension k increases. For comparison,
finding the 10-dimensional projected PCA is around 1,000 times quicker than finding the
corresponding global geodesic PCA and 200 times quicker than finding the nested geodesic
one.
As an additional simulation, in Appendix C we investigate the effect of the number of
B-spline basis J. In particular, we conclude that, for a fixed dimension k the reconstruction error (20) increases with the number of basis functions, both for the projected and
the simplicial PCA. Furthermore, we also observe that the reconstruction error for the
simplicial PCA exhibits a larger variance than the reconstruction error for the projected
PCA. Our insight is that this is due to the different degree of smoothness of the pdfs and
the quantile functions. Since the quantile functions are in general smoother than the pdfs,
their B-spline expansion should have lower variance.
7.1.1 Assessing the reliability of the projected PCA
A classical measure of performance of the standard Euclidean PCA, also useful to determine the dimension of the principal component to use, is the proportion of the explained
variance. For a k-dimensional Euclidean
principal
component, this quantity is easily comP
Pk
puted as a ratio of eigenvalues:
λ
λ
j=1 j
j≥1 j . Upon truncating the series at the
denominator, the same quantity can also be computed for PCA in infinite dimensional
Hilbert spaces.
Due to the projection step involved in our definition of PCA, we argue that the proportion of explained variance might not be a reliable indicator of performance, nor should
it be used to guide the choice of the dimension k. Instead, we propose a fast alternative
based on the Wasserstein distance that we believe better represents the properties of the

26

projected PCA, that is, the normalized reconstruction error:
− e−
1 Pn
i=1 W2 (Fi , Fi )
n
N REk = 1 Pn
−
− ,
i=1 W2 (Fi , F0 )
n
where the numerator corresponds to the reconstruction error in (20) and the denominator
is the average distance between the observed measures and their barycenter. Observe that
in Euclidean spaces, this quantity is closely related to the proportion of explained variance,
since in Euclidean spaces maximizing variance in a subspace, amounts to minimizing the
average distance from the subspace to data points.
Given its extrinsic nature, for a fixed dimension, the projected PCA might sometimes
fail to capture the variability of some particular data set and, in those situations, an intrinsic approach should be preferred. However, given the high computational cost associated
to geodesic PCAs, one would carry out such analysis only knowing that the results would
be significantly better than the ones obtained by projected PCA. This calls for discerning
whether the poor performance of projected PCA is due to its extrinsic nature or rather
to the scarceness of structure in the data set under consideration: in the former situation
it is likely that a geodesic approach would yield better results, in the latter instead, it is
likely that results remain the same.
We propose now two empirical indicators of the “reliability” of the empirical projected
PCA. The first one measures how reliable are the projected principal directions and gives
an idea of how different the projected PCA and the L2 PCA are. This is achieved by
computing the scores ηkmin and ηimax for every principal direction such that
ηkmin = min{a0 + ηwk∗ ∈ RJ↑ }
η∈R

where a0 is the spline coefficient vector associated to the barycenter F0− . The scalar ηkmax
is found analogously. Hence (ηkmin wk∗ , ηkmax wk∗ ) is the segment spanned by the principal
direction living inside the convex cone RJ↑ . If the scores of all observations along this
direction lie within the range (ηkmin , ηkmax ), then the (empirical) projected PCA coincides
with the (empirical) geodesic one and they both are equal to the (empirical) L2 PCA.
Contrary, the more the scores lie outside the range, the more the directions found by
the nested PCA and the projected one will be different. Hence, we propose the following
interpretability score
n

1X
ISk = 1 −
d sik , [ηkmin , ηkmax ] sik ,
n
i=1

where sik is the score of observation i along direction k according to the L2 PCA. A value
of ISk equal to one corresponds to perfect interpretability, that is, the projected and nested
directions coincide, while values of ISk closer to zero indicate large differences between
the directions. The ISk score is useful to interpret the directions one at a time. However,
max )
it can be the case that some scores along one direction k 0 lie outside the (ηkmin
0 , ηk 0
0
range but that the L2 projection on the k ≥ k component still lies within the projected
component. Using the terminology of Proposition 2 this corresponds to Πk (F −∗ − F0− ) =
Π F − ,k (Πk (F −∗ − F0− )) for a given observation F −∗ . To quantify the loss of information
UX0

at the level of the component (instead of direction), we propose to measure the “ghost
variance” captured by the L2 PCA:
n

.
1X
GVk =
kΠk (Fi− − F0− ) − Π F − ,k (Πk (Fi− − F0− ))k2 kFi− − F0− k2 ,
n
UX0
i=1

27

that is, the GVk score measures the quantity of information that is lost due to the projection
step or, in other words, the information that we trained our PCA on, but that does not
appear in the Wasserstein Space.
Finally, although this situation never occurred in our experience, it might happen that
GVk is small but some ISk0 (k 0 ≤ k) is large. This means that the subspace identified by
the projected PCA is suitable for representing the data, but the single principal directions
are not interpretable. In this case, we suggest to take a hybrid approach: use the projected
PCA as a fast black-box dimensionality reduction step, thus reducing the dimensionality of
each observation from J to k, and then use the nested PCA, in dimension k, to estimate the
directions, the main advantage being the reduction in the computational cost to estimate
the nested PCA in this lower dimensional space.
7.2 Regression
In this section, we compare the Wasserstein projected and simplicial (see Appendix B)
approaches when the task at hand is distribution on distribution regression. In particular, we consider two data generating processes as follows. In the first setting, data are
generating from the Wasserstein regression: independent variables z1 , . . . , zn are generP30 (z) (3)
−
− such that F =
where
, . . . , Fzn
ated by considering quantile functions Fz1
zi
h=1 aih ψj
(3)

(3)

(z)

(z)

ψ1 , . . . , ψ30 is a cubic spline basis over equispaced knots in [0, 1] and ai1 = 0, ai2 = δi1 ,
(z)
(z)
aij = aij−1 + δij−1 , and (δi2 , . . . , δi30 ) ∼ Dirichlet(1, . . . , 1). This data generating proce−
−
−
dure ensures the Fzi
(0) = 0, Fzi
(1) = 1 and Fzi
is monotonically increasing, cf. Propo−
−
sition 4. The dependent variables Fy1 , . . . , Fyn are generated using the same spline ex(y)

(z)

pansion of the dependent variables and letting ai = Bai . B is a randomly generated
matrix with rows b1 , . . . , b30 , and each bi is generated as follows: bi1 ∼ Uniform(0, 0, 5)
(y)
bij = bij−1 + b̃ij and b̃ij ∼ Uniform(0, 0, 5), so that the coefficients aij are monothonically
−
non decreasing for each i and thus the Fyi
’s can be considered quantile functions.
By numerical inversion and differentiation, we compute the associated pdf to each
quantile function. Since the simplicial regression takes as input (a trasformation of) the
pdfs while the Wasserstein regression works directly on the quantile functions, and also due
to the fact that numerical errors can be introduced in the data set during the inversion and
differentiation, we consider as ground truth the pdfs and, for the Wasserstein approach,
re-compute numerically the quantile functions.
In the second setting instead, we generate data from the simplicial regression model:
independent variables z1 , . . . , zn are generated by applying the inverse of the centered log
P
(z) (3)
ratio to a random spline expansion as follows. For each i = 1, . . . , n let p̃zi = 30
j=1 aij ψj
(3)

(z)

where the ψj ’s are the same B-spline basis as in the previous setting. Here, the aij ’s
are generated iid from a Gaussian distribution with mean 0 and standard deviation 0.2.
P
(y) (3)
(y)
(z)
The dependent variables are generated by letting p̃yi = 30
and ai = Bai ,
j=1 aij ψj
where B is a randomly generated 30 × 30 matrix with entries drawn iid from a standard
normal distribution. Finally the pdfs pzi (pyi ) are recovered by applying the inverse of the
centered log ratio to p̃zi (p̃yi ), see Appendix B for more details.
Note that under the second data generating process, both the dependent and independent distributions have support in [0, 1] by construction, whereas under the first data
generating process the independent variables might have a larger support. Thus, to fit the
simplicial regression in the first scenario, as common practice (cf. Appendix B), we extend
the support of all the distributions (both dependent and independent) to the smallest

28

Wasserstein
Simplicial

First scenario
(4 × 10−7 , 7 × 10−8 )
(0.9, 2.66)

Second scenario
(5 × 10−3 , 6 × 10−3 )
(4 × 10−4 , 5 × 10−4 )

Table 1: Cross validation (leave one out) errors and standard deviations for the Wasserstein
and Simplicial regression under the two simulated examples
interval of the real line containing all the supports. This is done by adding a small term
to the pdfs (in our example, 10−12 ) and then renormalizing them.
For both examples, we simulated 100 observations and compared the projected-Wasserstein
and simplicial regression using leave-one-out cross-validation. In particular, for both approaches we use J = 20 quadratic spline basis and choose the penalty term ρ in (16)
through grid search. Table 1 shows the pairs of mean squared error and standard deviation of the cross validation, the metric to compare the ground truth and the prediction
is the 2-Wasserstein distance. As one might expect, the Wasserstein regression performs
better in the first scenario while the simplicial regression performs better in the second
scenario. However, it is surprising how the Wasserstein geometry can capture (in terms of
Wasserstein metric) dependence generated by a linear structure which we have shown to
be very different from the Wasserstein one, making the projected regression a promising
tool for such inferential problems

8. Empirical Applications
In this Section, we consider two applications to real world data sets. Specifically, in
Section 8.1 we perform principal component analysis on the Covid-19 dataset, available
from the US Centers for Disease Control and Prevention website, and in Section 8.2 we
address the problem of one day ahead wind speed forecasting nearby a wind farm.
8.1 PCA on the Covid-19 mortality data
We perform PCA analysis on the Covid-19 mortality data publicly available at data.
cdc.gov as of the first December 2020. The data set collects the total number of deaths
due to Covid 19 in the US from January 1st 2020 to the current date, data are subdivided by state, sex, and age. In particular, the ages of the deceased are grouped in eleven
bins: [0, 1), [1, 5), [5, 15), [15, 25), [25, 35), [35, 45), [45, 55), [55, 65), [75, 85), [85, +∞) but we
truncate the last bin to 95 years for numerical convenience. Further, we remove Puerto
Rico from the analysis because it presented too many missing values. Our final data set,
shown in Figure 6(a), consists of 106 samples of the distribution of the ages of patients deceased due to Covid-19, divided by sex and pertaining 53 between US states and inhabited
territories.
We apply our usual B-spline approximation with J = 20 basis to the quantile functions
obtained starting from the histograms in Figure 6. This choice of J yields an average
approximation error, in terms of Wasserstein distance, of 0.02. An error this low is to
be expected since the quantile functions are piecewise linear functions defined on eleven
intervals.
Finally, we compute the projected PCA. The reconstruction error and the proportion
of explained variance (computed considering the quantiles as L2 ([0, 1]) valued random
variables), are shown in Figure 6(b). In particular, the two-dimensional principal component explains more than 90% of the L2 variability and N RE2 = 0.05. We thus fix
k = 2 for subsequent analysis. The interpretability scores equal IS1 = 1 and IS2 ≈ 0.89,
29

1.0

0.06
0.05

0.8

0.04

0.6

0.03

NRE

0.4

0.02

0.2

0.01
0.00

0.0
0

20

40

60

80

0

1

2

3

(a)

4

5

6

7

8

9

(b)

Figure 6: Left panel: distributions of age at the time of death for Covid-19 patients divided
by sex: orange corresponds to female and blue to males. Different lines correspond to
different US states / inhabited territories. Right panel: normalized reconstruction error
as a function of the dimension of the principal component. The 0-th principal component
is the emprical mean.

First PD

0.06

0.06

0.05

0.05

0.04

0.04

0.03

0.03

0.02

0.02

0.01

0.01

0.00

40

60

80

0.00

Second PD

Scores
2

0.05

1

0.04

0

0.03

1

0.02

2

0.01

3
40

60

80

0.00
5

0

5

0

50

Figure 7: The first two panels show the variability along the first two principal directions
(first and second panel), using the same visualization technique as in Figure 4. The third
panel reports the scores of the projections on the two dimensional principal component
(orange for women and blue for men) and the fourth panel shows three particular distributions, also highlighted in the third panel. In particular, the red distribution is the one
of women in Vermont, the green one are males in Alaska and the purple one are women
in West Virginia.
while GV2 = 0.05. Given the reconstruction error and the GV2 score, we can conclude
that the two-dimensional principal component provides an almost perfect fit to the data,
and that both selected principal directions are well behaved with respect to their scores,
guaranteeing interpretable results.
Figure 7 reports the analysis having selected k = 2 principal directions. In particular,
the first principal direction shows that the greatest variability is due to the elders: low
negative values along this direction correspond to most of the mortality being concentrated
among in the 80+ range. The red and the green distributions shown in the rightmost panel
show two antithetic behaviors which correspond to scores along the first principal direction
of roughly −8.5 and 7 as shown in the third panel of Figure 7. In fact, the red distribution

30

Average daily wind speed
14
12
10
8
6
4
2

0

200

400

600

800

Figure 8: Daily average wind speed
is concentrated almost exclusively on the last two bins of the histogram, with the 85+ bin
weighting for more of 60% of the deaths. At the opposite, the green distribution gives more
weight to lower age values. The second direction instead shows variability in the 40 − 80
range. The purple distribution, characterized by the highest score along this direction,
shows that a significant percentage of deaths occurred in the age range 60 − 75.
Finally, the third panel of Figure 7 reports the scores along the first two principal
directions for the whole data set, blue dots representing males and orange dots women.
We can appreciate how women tend to have lower scores on both directions. This is in
line with our understanding that Covid-19 is more severe among the male population (see
for instance Mandavilli, 2020), which explains why males are more susceptible to death
even at younger ages, while deaths among women are more concentrated in the 70+ age
range, being the elders in general more fragile.
8.2 Wind speed distribution forecasting from a set of experts
We consider the problem of forecasting the distribution of the wind speed nearby a wind
farm from a set of experts. The data set is publicly available at www.kaggle.com/
theforcecoder/wind-power-forecasting. In particular, data consists of measurements
of the wind speed collected every ten minutes for a period of 821 days starting from the
31st December 2017. The daily average wind speed is shown in Figure 8.
We assume to have access to a set of experts, that is a set of trained models, that
provide a probabilistic one-day-ahead forecast for the average wind speed. Here, our goal
is to combine this set of experts and provide a point estimate of the wind speed distribution
for the whole day, which can be helpful when planning the maintenance of the wind mills
for instance.
−
Formally, let K denote the number of experts considered, Fzij
is the quantile function
associated to the probabilistic forecast of the average wind speed for day i given by expert
−
j = 1, . . . , K; Fyi
is the empirical quantile function of the wind speed for day i. In
particular, we consider K = 4 experts built from the Prophet model by Facebook (Taylor
and Letham, 2018) as follows: model M 1 is the classical Prophet, without additional
covariates or seasonality trends; model M 2 includes the ambient temperature as covariate
but not seasonality; model M 3 includes a yearly seasonality and no covariates and model
M 4 includes both yearly seasonality and ambient temperature as covariate. The models are
estimated using variational inference on rolling samples of 365 days and produce one day
31

MSE

R1
(1.22 ± 1.32)

R2
(1.19 ± 1.26)

R3
(1.15 ± 1.07)

R2
(1.24 ± 1.23)

RF
(0.86 ± 0.82)

Table 2: Mean square prediction error ± one standard deviation on the held-out test set.
ahead probabilistic forecasts for the average wind speed. The final sample size corresponds
to n = 456.
We consider a trivial extension of the distribution on distribution regression model in
Section 5.2 as follows:
−
E[Fyi

|

−
−
Fzi1
, . . . , FziK
]



= ΠL2 ([0,1])↑ α +

K Z
X
j=1

1

0

−
βj (t, s)Fzij
(t) dt



(22)

Having approximated all the functions through a B-spline expansion, the model reads
(y)
E[ai

|

(z)
(z)
ai1 , . . . , aiJ ]



= ΠRJ↑ θα +

K
X

(z)
Θβj Eaij



.

j=1

The procedure for estimating θα and Θβ1 , . . . ΘβK is analogous to the one outlined in
Section 5.2.
We compare the prediction performance of five distribution on distribution regression
models. Models R1 to R4 are obtained by fitting model (22) using only one of the four
experts, M 1 to M 4, while the fifth model (RF ) is the “full” model in (22) considering all
the four experts. For this comparison, we perform a train-test split of the 456 days for
which the experts produced the prediction, considering the last 100 days as test. We select
hyperparameters (namely, the penalty coefficient ρ in (16) and whether to include or not
the intercept term α) by a grid search cross validation on the training set, and compare
the mean square error on the held-out test set. Results of the comparison are reported
in Table 2. As expected, the model with the four predictors (RF ) is the best performer.
Interestingly, all the other models R1-R4 perform similarly and present a much higher
mean square error when compared to RF , thus suggesting that the best performance is
achieved by combining the different experts together and no expert alone can be a good
predictor. This is possibly explained by some experts being able to better forecast one
scenario (for instance, light winds) and other experts being able to better forecast other
scenarios.
We conclude with some descriptive analysis. Figure 9 shows the point estimates for
the coefficients βj . We can interpret as highly influential for the regression the areas of
the βj ’s with high absolute value, and as negligible area with values close to zero.
We can highlight some differences among the coefficients in Figure 9. In particular,
model M 1, seems influent when predicting the tails of the distribution, in particular with
negative weights for the left tail and positive weights for the right tail. Model M 2, seems to
be affecting all the steps of the prediction and in particular to be model affecting the most
the median of the distribution. Model M 3, appears to be, with M 2, the most important
model for the prediction: the absolute value in the corresponding regressor β3 is often
very high and with noticeable peaks corresponding to areas predicting the left tail and
towards the right tail. Finally, the regressor corresponding to M 4 has very low values thus
resulting inof minor importance in terms of regression influence.
Interestingly, the experts providing the most precious inputs to our regression model are
M 2 and M 3, that incorporate only the seasonality effect and the temperature covariate
32

beta 1

0.0

beta 2

0.0

beta 3

0.0

beta 4

0.0

0.10

0.2

0.2

0.2

0.2

0.4

0.4

0.4

0.4

0.6

0.6

0.6

0.6

0.05

0.8

0.8

0.8

0.8

0.10

0.05

1.0
0.0

0.2

0.4

0.6

0.8

1.0

1.0
0.0

0.2

0.4

0.6

0.8

1.0

1.0
0.0

0.2

0.4

0.6

0.8

1.0

1.0
0.0

0.00

0.2

0.4

0.6

0.8

1.0

0.15

Figure 9: Estimates of the βi (t, s)’s evaluated on [0, 1]2 . The variable t runs across columns,
and variable s across rows
10

0.125

8

0.100
0.075

6

0.050

4

0.025

2

0.000

0

0.025
0.0

0.2

0.4

0.6

0.8

2

1.0

0.0

0.2

0.4

observed
predicted

alpha

0.6

M1
M2

0.8

1.0

M3
M4

Figure 10: Estimate of α (left) and prediction of one Fy− of the test set (right). In the
right panel, the blue line corresponds to the empirical quantile function, the orange one
to the prediction from RF and the green ones to the average wind predictions obtained
from the experts M 1-M 4.
respectively, while M 4, which incorporates both, seems to be less important. Hence,
the regression model in (22) finds more effective combining experts trained on different
covariates than correcting an expert already trained on all the covariates. In particular,
our insight is that M 2 is responsible for centering the median of the output distribution.
The tails of the distribution seem to need also the contribution of seasonality data, given
by M 3. Finally, we also observe that the left tail of the wind distribution seems the most
difficult to be predicted, needing very high positive and negative weights across different
models, to be obtained.

9. Discussion
In this paper, we propose a novel class of projected statistical methods for distributional
data on the real line, focusing in particular on the definition of a projected PCA and
a projected linear regression. By investigating the weak Riemannian structure of the
Wasserstein space, and the transport maps between probability measures, we represent
the Wasserstein space as a closed convex cone inside an Hilbert space.
Similarly to log methods, our models exploit the possibility to map data into a linear
space to perform statistics in an extrinsic fashion. However, instead of using operators
like the exp map or a typesome kind of boundary projection to return to the Wasserstein

33

space, we rely on a metric projection operator that is more respectful of the underlying
metric.
By choosing as base point the uniform measure on [0, 1], we are able to efficiently
approximate the metric projection operator so that our models combine the ease of implementation of extrinsic methods while retaining a performance similar to the one of
intrinsic methods. Further, through a quadratic B-spline approximation, we can greatly
reduce the dimensionality of the optimization problems involved, resulting in fast empirical methods. As a byproduct of this approach, we also derive faster numerical routines
for the geodesic PCA in Bigot et al. (2017).
We study asymptotic properties of the proposed methods, concluding that under reasonable regularity assumptions, our projected models provide consistent estimates and that
the B-spline approximation error becomes negligible. We showcase our approach in several
simulation studies and using two real world datasets, comparing our models to intrinsic
ones and to the simplicial approach in Hron et al. (2014), concluding that the projected
PCA and regression constitute a valid candidate for performing inference on a data set of
distributions.
Several extensions and modifications of our approach are possible. One possibility is to
extend our framework to encompass more models, such as generalized linear models and
independent component analysis. Although this should be straightforward in theory, the
numerical computations could become more burdensome. Furthermore, as an alternative
to our approach based on B-splines approximation, one could use such B-spline expansion
only to approximate the metric projection operator. Another interesting line of research
would consist in building hybrid approaches (as anticipated in Section 7.1.1) to analyze
distributions in the Wasserstein space, using both extrinsic and intrinsic methods to exploit
the advantages of both worlds, while mitigating the disadvantages. Lastly, we think that
a deeper comparison between the Wasserstein and the simplicial geometries could help
practitioners in choosing between them.

Acknowledgments
Thanks to Riccardo Scimone for helpful feedbacks and comments on an earlier draft of
this paper and to Federico Bassetti, Alessandra Guglielmi and Piercesare Secchi for helpful
discussions.

34

Appendix A. Proofs
Proof of Proposition 2.
The proofs follows by noticing that being Πk the orthogonal projection onto a subspace,
x − Πk ⊥Span(Uk ) and thus for v ∈ Span(Uk ):
kx∗ − vk2 = kx∗ − Πk (x∗ )k2 + kΠk (x∗ ) − vk2
so that
arg min kx∗ − vk = arg min kΠk (x∗ ) − vk
x ,k

x ,k

v∈UX0

v∈UX0

and the result follows.
Proof of Proposition 4.
1. As shown in the supplementary of Pya
(2015) by standard
PJ and Wood
PJ B-spline fork
0
mulas we obtain that given f (x) = j=1 aj ψj (x), then f (x) = j=1 (aj − aj−1 ) ·
ψjk−1 (x). Being the B-spline basis function nonnegative by definition, we obtain the
result.
2. With k = 2, f 0 (x) on the interval [xj+1 , xj ] has the following expression:
x − xj
xj+1 − x
· (αj − αj−1 ) +
· (αj−1 − αj−2 )
xj+1 − xj
xj+1 − xj
so:
limx→x− f 0 (x) = αj − αj−1
j+1

and the result follows.
Proof of Proposition 5 and 6.
We report here Propositions 3.3 and 3.4 of Bigot et al. (2017), with the notation adapted
to our manuscript. In the following H is a separable Hilbert space, X is a closed convex
subset of H, X is an X-valued square integrable random variable, x0 a point in X and
k ≥ 1 an integer.
Proposition 10 Let U ∗ = {u∗1 , .., u∗k } be a minimizer over orthonormal sets U of H of
x0
x0
cardinality k, of DX
(X , U ) := Ed2 (X , (x0 + Sp(U )) ∩ X), then UX
:= (x0 + Sp(U )) ∩ X
is a (k, x0 )−global principal component of X .
Proposition 11 Let U ∗ = {u∗1 , .., u∗k } be an orthonormal set such that Ui∗ = {u∗1 , .., u∗i }
x0
is a minimimizer of DX
(X , U ) over the orthonormal sets of cardinality “i” such that
∗x
∗
0
U ⊃ Ui−1 ; then UX is a (k, x0 )−nested principal convex component of X .
Applying Propositions 10 and 11 we can obtain equivalent definitions of geodesic and
nested PCA as optimization problems in L2 ([0, 1]). If we fix J ∈ N > 0 and a quadratic
B-spline basis {ψj }Jj=1 , we can use Propositions 10 and 11 with X = L2 ([0, 1])J↑ and
H = L2 ([0, 1])J . Thanks to Remark 7 we obtain the results.
Proof of Proposition 7.
P
P (J)
(J) (J)
(J)
(J)
(J)
Let SJ = Jj=1 λj ψj and its derivative sJ = j (λj − λj−1 )ψej where ψej denotes
the linear spline basis on the same equispaced grid in [0, 1].
35

Let fµ− = (Fµ− )0 , of course it can be seen that fµ− is non-negative. Moreover, it is
obvious that fµ− ∈ W2∞ ([0, 1]). Then, from De Boor and Daniel (1974) we get that there
exist sJ such that ksJ − fµ− k∞ ≤ CkD2 fµ− k∞ J −2 , where C is a constant depending on the
interval [0, 1] but not on n.
(J)
Hence, we can determine the coefficients {λj }, starting from the spline sJ , up to a
translation factor.
(J)
We fix a particular set of coefficients by letting SJ (0) = λ1 = Fµ− (0) for each J. So
that:
Z x
Z x
Z x
−
−
−
sJ (t) − fµ− (t)dt
fµ (t)dt − SJ (0) + Fµ (0) =
sJ (t)dt −
SJ (x) − Fµ (x) =
0

0

0

By using the previous result, the integral we have that SJ (x) − Fµ− (x) ≤ CJ −2 for all x
which proves the proposition.
Proof of Proposition 8.
By the Assumptions in Section 6.2.1 and Remark 10 there exists a ball BK in W3∞ ([0, 1]) of
radius K for some K > 0, such that each Fi− can be ε-approximated by Fei− ∈ W3∞ ([0, 1])
with Fe− ∈ BK . We can suppose that also the eigenvectors of the covariance operator of
the generating process belong to such sphere, otherwise we just increase its radius of some
finite amount.
By Proposition 7 we can choose a spline basis (that is, a number of elements J > 0),
such that we get a ε-uniformly good approximation of BK (and thus we can 2ε-approximate
its L2 closure). To lighten notation, thanks to Remark 7 we deliberately confuse RJ↑ and
the space monotone B-splines with J basis functions, the inner product we are referring
to will always be clear by looking at its entries.
Now consider the following inequalities, with aJi obtained as 2ε approximations of Fi− ,
J
w ∈ RJ , w ∈ L2 ([0, 1]):
1X −
1X J J 2
hFi , wi2 −
hai , w i ≤
n
n
i
i
X
X
X
1 X −
hFi , wi2 −
haJi , wi2 +
haJi , wi2 −
haJi , wi2 ,
n
i

i

i

i

where the inner product haJi , wi is to be intended as the L2 inner product between the
spline function with coefficients aJi and the L2 function w. Consider now:
1X
(hFi− , wi2 − haJi , wi2 ) =
n
i
1X
(hFi− , wi − haJi , wi)(hFi− , wi + haJi , wi) =
n
i
1X −
hFi − aJi , wihFi− + aJi , wi ≤
n
i
X
1
hFi− − aJi , wi · hFi− + aJi , wi ≤
n
i
1X
2εkwk2 2K = 4εKkwk2
n
i

36

Similarly:
1X J
(hai , wi2 − haJi , wJ i2 ) ≤ kaJi k2 · kw − wJ k · (kwk + kwJ k)
n
i

P
We know that a solution to the problem maxkwkL2 =1 n1 i hFi− , wi2 is given by the first
eigenfunction w
b of the covariance operator of the empirical process. Now we are in the
condition to apply results in Dauxois et al. (1982), or in Qi and Zhao (2011) (with α → 0)
to conclude that w
b converges to the first eigenfunction w̄ of the covariance operator of
the process that generates Fi− . By hypotesis, such eigenfunction w̄ lies in BK and thus
can be approximated with our fixed spline basis. Thus for high enough n, also w
b can be
approximated up to 2ε.
Let awb be the coefficients of the spline expasion of w
b spline approximation, that is,
kw − aw k ≤ 2ε. Observe that kwk
b 2 − kawb kE ≤ 2ε, just as kaiJ k ≤ K + 2ε. Thus, up to
adding another ε to the approximation error kw
b − awb k, we can suppose kawb k2 = 1. Hence:
1X J
(hai , wi
b 2 − haJi , awb i2 ) ≤ (K + 2ε) · 3ε · 2
n
i

Which leads to:
max

kwkL2 =1

X
haJi , wi2 −
i

max

kwJ kE =1

X
haJi , wJ i2 ≤ (K + 2ε) · 3ε · 2
i

Finally, combining the above results and the fact that | max f − max g| ≤ max |f − g|
for any pair of real valued functions f and g, we obtain:
1X
1X J J 2
hfi , wi2 − max
hai , w i ≤
kwkL2 =1 n
kwJ kE =1 n
max

i

i

max 4εKkwk + (K + 2ε) · 6ε ≤ 6εK(1 + 2ε)

kwkL2 =1

Thus for instance if we ask that ε < 1, we obtain the desired result with D = 18 · K.
Consistency follows since kawb − w̄k ≤ kawb − wk
b + kw
b − w̄k.
Proof of Lemma 2.
Since for any x ∈ X we have ΠRJ↑ (x) → x, for any v ∈ H:
kv − ΠRJ↑ (v)k ≤ kv − ΠRJ↑ (ΠX (v))k ≤ kv − ΠX (v)k + kΠX (v) − ΠRJ↑ (ΠX (v))k
which implies ΠRJ↑ (v) → ΠX (v). Consider now βn → β in H; we have the inequality:
kΠRJ↑ (βn ) − Π(β)k ≤ kΠRJ↑ (βn ) − ΠX (βn )k + kΠX (βn ) − ΠX (β)k
the first term of the right hand side of the inequality can be sent to 0 by increasing J, the
other by increasing n.
Proof of Proposition 9.
We call ai the spline coefficients associated to xi and bi the ones associated to yi . Again
we deliberately confuse the spaces where the coefficients live to lighten the notation. Since

37

the penalty term does not depend on the data, we have:
X
1 X
kyi − hxi , B T ABik2 −
kbi − hai , B T ABiL2 ([0,1]) k2 | =
n
i
i
1 X
|
(kyi − hxi , B T ABik2 − kbi − hai , B T ABiL2 ([0,1]) k2 )| ≤
n
i
1X
kyi − hxi , B T ABik2 − kbi − hai , B T ABiL2 ([0,1]) k2 |
n
i

Now, since
kyi − hxi , B T ABik2 − kbi − hai , B T ABiL2 ([0,1]) k2 =
(kyi − hxi , B T ABik − kbi − hai , B T ABik)×
(kyi − hxi , B T ABik + kbi − hai , B T ABik)
Then for some constant K depending on the bounds in the Assumptions, we get:
kyi − hxi , B T ABik2 − kbi − hai , B T ABiL2 ([0,1]) k2 ≤
kyi − hxi , B T ABi − bi + hai , B T ABik2K =

kyi − bi k + hai − xi , B T ABi 2K
Thus, if J is such that we have ε-approsimations of the data, by Cauchy-Schwartz we
obtain:
X
1 X
kyi − hxi , B T ABik2 −
kbi − hai , B T ABiL2 ([0,1]) k2 ≤ K 0 · ε
n
i

i

for some K 0 constant.
Thanks to the results in Prchal and Sarda (2007), for any ε > 0, if the number of
b and Θ
b J exist with probability 1 − ε and are unique. Since the value of
samples is big, Θ
the minimization problem the solve are arbitrarily close, then the minimizers converge in
RJ×J with the metric given by the spline basis.
Strong convergence implies semi-norm convergence.
Let Z be an H-valued random variable and CZ the covariance operator associated to Z,
that is:
Z
(CZ f )(s) =
cov(x(s), x(t))f (t)dt.
[0,1]

In the following, we denote with k · kL2 the L2 ([0, 1]2 ) norm. Further, recall that
kcov(Z(s), Z(t))kL2 ([0,1]2 ) = E[kZk2 ]. We want to look at the behavior of kβbPS − βbJ kCZ .
Z
hCZ (βbPS (s, t) − βbJ (s, t)), βbPS (s, t) − βbJ (s, t)idt ≤
[0,1]

kCZ (βbPS (s, t) − βbJ (s, t))kL2 · kβbPS (s, t) − βbJ (s, t)kL2 ≤
E[kxk2 ] · kβbPS (s, t) − βbJ (s, t)kL2 · kβbPS (s, t) − βbJ (s, t)kL2 .
So kβbPS − βbJ kCZ ≤ M · kβbPS − βbJ k2L2 for some constant M . Thus k · kL2 convergence
implies k · kCZ convergence.
38

Appendix B. The simplicial approach
The simplicial approach to distributional data analysis is based on the definition of Bayes
space B 2 (I) (Egozcue et al., 2006). Formally, let I ⊂ R a closed interval, the Bayes spaces
B 2 (I)R is defined the equivalence class of probability densities p(x) on I (that is p(x) ≥ 0
and I p(x)dx = 1) with square integrable logarithm.
The Bayes space is endowed with a linear space starting from the definition of the
perturbation and powering operators, that are analogous to the sum and multiplication
times a scalar, and inner product. Moreover Menafoglio et al. (2014) defines an isometric
isorphism between B 2 (I) and L2 ([0, 1]) through the so-called centered log ratio (clr) map
defined as
Z b
1
pe(x) := clr(p)(x) = log(p(x)) −
log p(t)dt
(23)
b−a a
for every p ∈ B 2 (I). The inverse map is defined as
p(x) = clr−1 (e
p)(x) = R

exp(e
p(x))
p(x))dx
I exp(e

Thus, it is possible to define a simplicial PCA and simplicial regression on the Bayes
space starting from the clr map. In particular, let p1 , . . . , pn be observed densities on the
interval I and let pei = clr(pi ). Denote with w
e1 , . . . , w
ek the first k principal directions
estimated from the pei ’s, then a k dimensional simplicial principal component is the span
of {wi = clr−1 (w
ei )}ki=1 in B 2 (I).
Similarly, for pdfs {(pz , py )i }ni=1 a simplicial regression model is defined starting from
e denote a functional regression model in L2 for variables
the clr transformed variables. Let Γ
n
{(e
pz , pey )i }i=1 , then the simplicial regression states:


e pzi ) .
E[pyi | pzi ] = clr−1 Γ(e
Apart from the different geometries of the Wasserstein and Bayes space, which are discussed in Section 7, we can highlight one particular drawback from the simplicial approach
which we believe poses a significant limit to its usefulness. In fact, the main assumption
is that all the pdfs pi share the same support, which might not be the case (for instance it
is not the case for our example in Section 8.2). In practice, one may circumvent this need
by either “padding” all the pdfs to the same support, i.e considering
pi (x) ∝ pi (x) + εI[x ∈ I],

(24)

where I[·] denotes the indicator function, and the proportionality is due to the need of
re-normalizing the pi ’s so that they integrate to 1. Another approach could consist in
considering I as the intersection of all the supports of the different pi ’s let truncate all the
pdfs to the shared interval I.
Both approaches present undesired side effects that can greatly alter the results. The
second approach might end up with a very small interval I, so that a lot of information is
lost due to this pre-processing step. The drawback of the first approach instead is due to
numerical instability. In fact, one would like ε in (24) to be small in order not to corrupt
the true signal given by pi . However, considering the transformation in (23) having a small
ε would cause the pei to present some extreme values (negative) in correspondence to ε.
Performing PCA on a dataset processed in this way would greatly alter the results, as
most of the variability of the pei ’s would be masked by a difference in their support.
39

30
25
20
15
10
5
0
0.0

0.2

0.4

0.6

0.8

1.0

Figure 11: Example of data set from (25)

Appendix C. Additional Simulations
In this simulation, we show how the number of B-spline basis functions affects the inference
in our projected PCA and in the simplicial one. In this Scenario, the probability measures
are simulated as mixture of beta densities, also known as Bernstein polynomials, as follows:
pi (x) =

K
X

wij β(x; j, K − j)

j=1

(25)

wi ∼ DirichletK (0.01)
Where β(x; a, b) denotes the density of a beta distributed random variable with parameters
(a, b) evaluated in x. By definition, the pi s generated from (25) have a fixed support
I = [0, 1]. See Figure 11.
In this setting instead, we let µi in (20) be the probability measure associated to pi
and not its smoothed version. Hence, in addition to the amount of information lost during
the PCA another factor comes into play: the amount of information that is lost due to
the B-spline representation.
Figure 12 shows the results. We can see that the reconstruction errors decrease when
the dimension of the principal component increases both for the simplicial and projected
PCA. Moreover, as the number of B-spline basis increase, the performance tend to get a
little bit worse for both the approaches. We believe that this is due to an increased variance
in the B-spline estimation of the quantile functions and (clr of) pdfs. In fact, computing the
spline approximation for a single function amounts to solving a linear regression problem
and increasing the dimension of the B-spline basis corresponds to increasing the number
of regressors. Hence, letting B the matrix with columns ψ1 , . . . , ψJ (evaluated on a grid),
the variance of the OLS estimate of the coefficients a is proportional to (B T B)−1 . When
increasing the number of B-splines, the entries in B T B become closer to zero, since the
support of each of the spline basis becomes smaller. This leads to smaller precision (and
higher variance) in the estimator for a.
Another interesting thing to notice is that the simplicial PCA exhibits a much larger
variance in the reconstruction error. This is possibly due to the different degree of smoothness of the quantile functions and of the pdfs. As the quantile functions are smoother than
the pdfs, their B-spline basis expansion should have lower variance and be more similar to
the true quantiles.

40

0.25

0.25

0.25

0.20

0.20

0.20

0.15

0.15

0.15

0.10

0.10

0.10

0.05

0.05

0.05

5 10 15 20 25 30 35 40 45 50

5 10 15 20 25 30 35 40 45 50

projected
simplicial

5 10 15 20 25 30 35 40 45 50

Figure 12: Results for the third scenario. All the panels show the reconstruction error as
a function of the number of the spline basis functions. From left to right the results are
obtained using the 2, 5 and 10 dimensional PCA. The solid lines represent the mean of
10 independent runs on independent data sets from (25) and the shaded area represent ±
one standard deviation.

References
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces and
in the space of probability measures. Springer Science & Business Media, 2008.
Dragi Anevski and Philippe Soulier. Monotone spectral density estimation. The Annals
of Statistics, 39(1):418–438, 2011.
Dragi Anevski, Ola Hössjer, et al. A general asymptotic scheme for inference under order
restrictions. The Annals of Statistics, 34(4):1874–1930, 2006.
Jean-Pierre Aubin and Hélène Frankowska. Set-valued analysis. Springer Science & Business Media, 2009.
Miriam Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and Edward Silverman. An
empirical distribution function for sampling with incomplete information. The Annals
of Mathematical Statistics, 26(4):641–647, 1955. ISSN 00034851. URL http://www.
jstor.org/stable/2236377.
Monami Banerjee, Rudrasis Chakraborty, Edward Ofori, David Vaillancourt, and Baba C
Vemuri. Nonlinear regression on riemannian manifolds and its applications to neuroimage analysis.
In International Conference on Medical Image Computing and
Computer-Assisted Intervention, pages 719–727. Springer, 2015.
Federico Bassetti, Antonella Bodini, and Eugenio Regazzini. On minimum kantorovich
distance estimators. Statistics & probability letters, 76(12):1298–1302, 2006.
Espen Bernton, Pierre E Jacob, Mathieu Gerber, and Christian P Robert. On parameter
estimation with the wasserstein distance. Information and Inference: A Journal of the
IMA, 8(4):657–676, 2019a.
Espen Bernton, Pierre E Jacob, Mathieu Gerber, Christian P Robert, et al. Approximate
bayesian computation with the wasserstein distance. Journal of the Royal Statistical
Society Series B, 81(2):235–269, 2019b.
41

Michael J Best and Nilotpal Chakravarti. Active set algorithms for isotonic regression; a
unifying framework. Mathematical Programming, 47(1-3):425–439, 1990.
Rabindra N Bhattacharya, L Ellingson, X Liu, V Patrangenaru, and M Crane. Extrinsic
analysis on manifolds is computationally faster than intrinsic analysis with applications
to quality control by machine vision. Applied Stochastic Models in Business and Industry, 28(3):222–235, 2012.
Jérémie Bigot, Raúl Gouet, Thierry Klein, Alfredo López, et al. Geodesic PCA in the
Wasserstein space by convex PCA. In Annales de l’Institut Henri Poincaré, Probabilités
et Statistiques, volume 53, pages 1–26. Institut Henri Poincaré, 2017.
T. Tony Cai and Peter Hall. Prediction in functional linear regression. The Annals of
Statistics, 34:2159–2179, 2006.
Jiezhang Cao, Langyuan Mo, Yifan Zhang, Kui Jia, Chunhua Shen, and Mingkui Tan.
Multi-marginal Wasserstein GAN. In Advances in Neural Information Processing Systems, pages 1776–1786, 2019.
Marta Catalano, Antonio Lijoi, and Igor Prünster. Bayesian model comparison based on
Wasserstein distances. In Book of Short Papers SIS 2019. Pearson Italia, Milano, 2019.
Elsa Cazelles, Vivien Seguy, Jérémie Bigot, Marco Cuturi, and Nicolas Papadakis.
Geodesic PCA versus log-PCA of histograms in the Wasserstein space. SIAM Journal on Scientific Computing, 40(2):B429–B456, 2018.
Yaqing Chen, Zhenhua Lin, and Hans-Georg Müller. Wasserstein regression. arXiv preprint
arXiv:2006.09660, 2020.
Marco Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In
Advances in Neural Information Processing Systems, pages 2292–2300, 2013.
Marco Cuturi and Arnaud Doucet. Fast Computation of Wasserstein Barycenters. In
International Conference on Machine Learning, pages 685–693, 2014.
Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable Ranking and Sorting
using Optimal Transport. In Advances in Neural Information Processing Systems, pages
6861–6871, 2019.
Priyam Das and Subhashis Ghosal. Bayesian quantile regression using random B-spline
series prior. Computational Statistics & Data Analysis, 109:121–143, 2017.
J. Dauxois, A. Pousse, and Y. Romain. Asymptotic Theory for the Principal Component
Analysis of a Vector Random Function: Some Applications to Statistical Inference.
Journal of Multivariate Analysis, 12:136–154, 1982.
Carl De Boor and James W Daniel. Splines with Nonnegative B-spline Coefficients. Mathematics of computation, 28(126):565–568, 1974.
Pedro Delicado. Dimensionality reduction when data are density functions. Computational
Statistics & Data Analysis, 55:401–420, 01 2011.
Frank Deutsch. Best Approximation in Inner-Product Spaces. Springer Science & Business
Media, 2012.

42

Richard Dykstra, Tim Robertson, and Farrol T Wright. Advances in Order Restricted
Statistical Inference: Proceedings of the Symposium on Order Restricted Statistical Inference Held in Iowa City, Iowa, September 11–13, 1985, volume 37. Springer Science
& Business Media, 2012.
Juan José Egozcue, José Luis Dı́az-Barrero, and Vera Pawlowsky-Glahn. Hilbert Space of
Probability Density Functions Based on Aitchison Geometry. Acta Mathematica Sinica,
22(4):1175–1182, 2006.
Thomas S Ferguson. Bayesian density estimation by mixtures of normal distributions. In
Recent advances in statistics, pages 287–302. Elsevier, 1983.
P. Fletcher. Geodesic Regression and the Theory of Least Squares on Riemannian Manifolds. International Journal of Computer Vision, 105, 11 2013.
K Hron, Alessandra Menafoglio, Matthias Templ, Klára Hrůzová, and P Filzmoser. Simplicial principal component analysis for density functions in Bayes spaces. Computational
Statistics & Data Analysis, 94:330–350, 07 2014.
Stephan Huckemann, Thomas Hotzand, and Axel Munk. Intrinsic shape analysis: Geodesic
PCA for Riemannian manifolds modulo isometric lie group actions. Statistica Sinica,
20:1–58, 2010.
Alois Kneip and Klaus J. Utikal. Inference for Density Families Using Functional Principal
Component Analysis. Journal of the American Statistical Association, 96(454):519–542,
2001.
J. Le-Rademacher and L. Billard. Principal component analysis for histogram-valued data.
Advances in Data Analysis and Classification, 11(2):327–351, 2017.
Apoorva Mandavilli. Why does the coronavirus hit men harder? a new clue. 08 2020. URL
https://www.nytimes.com/2020/08/26/health/coronavirus-men-immune.html.
Alessandra Menafoglio, Alberto Guadagnini, and Piercesare Secchi. A kriging approach
based on Aitchison geometry for the characterization of particle-size curves in heterogeneous aquifers. Stochastic Environmental Research and Risk Assessment, 28(7):1835–
1851, 2014.
Gaspard Monge. Mémoire sur la théorie des déblais et des remblais. Histoire de l’Académie
Royale des Sciences de Paris, 1781.
P. Nagabhushan and R. Pradeep Kumar. Histogram PCA. In Derong Liu, Shumin Fei,
Zengguang Hou, Huaguang Zhang, and Changyin Sun, editors, Advances in Neural
Networks – ISNN 2007, pages 1012–1021, Berlin, Heidelberg, 2007. Springer Berlin
Heidelberg.
Victor M Panaretos and Yoav Zemel. An Invitation to Statistics in Wasserstein Space.
Springer Nature, 2020.
Vic Patrangenaru and Leif Ellingson. Nonparametric Statistics on Manifolds and Their
Application to Object Data Analysis. CRC Press, 2015.
Xavier Pennec. Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric
Measurements. Journal of Mathematical Imaging and Vision, 25:127–154, 07 2006.
43

Xavier Pennec. Statistical Computing on Manifolds: From Riemannian geometry to Computational Anatomy. In LIX Fall Colloquium on Emerging Trends in Visual Computing,
pages 347–386. Springer, 2008.
Gabriel Peyré, Marco Cuturi, et al. Computational Optimal Transport: With Applications
to Data Science. Foundations and Trends in Machine Learning, 11(5-6):355–607, 2019.
Florian A Potra and Stephen J Wright. Interior-point methods. Journal of Computational
and Applied Mathematics, 124(1-2):281–302, 2000.
Simon Potter, Marco Del Negro, Giorgio Topa, and Wilbert Van der Klaauw. The advantages of probabilistic survey questions. Review of Economic Analysis, 9(1):1–32,
2017.
Luboš Prchal and Pascal Sarda. Spline estimator for functional linear regression with
functional response. Technical Report, 2007.
Natalya Pya and Simon N Wood. Shape constrained additive models. Statistics and
Computing, 25(3):543–559, 2015.
Xin Qi and Hongyu Zhao. Some theoretical properties of Silverman’s method for smoothed
functional principal component analysis. Journal of Multivariate Analysis, 102:741–767,
2011.
James O Ramsay. Functional data analysis. Encyclopedia of Statistical Sciences, 4, 2004.
R. Tyrrell Rockafellar and Roger J.-B. Wets. Variational Analysis. Springer Verlag,
Heidelberg, Berlin, New York, 1998.
Oldemar Rodrı́guez, Edwin Diday, and Suzanne Winsberg. Generalization of the Principal
Components Analysis to Histogram Data. pages 12–16, 2000.
Kazuyuki Sekitani and Yoshitsugu Yamamoto. A recursive algorithm for finding the minimum norm point in a polytope and a pair of closest points in two polytopes. Mathematical Programming, 61:233–249, 1993.
Bernard W Silverman et al. Smoothed functional principal components analysis by choice
of norm. The Annals of Statistics, 24(1):1–24, 1996.
Sanvesh Srivastava, Volkan Cevher, Quoc Dinh, and David Dunson. Wasp: Scalable Bayes
via barycenters of subset posteriors. In Artificial Intelligence and Statistics, pages 912–
920, 2015.
Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72
(1):37–45, 2018.
Rosanna Verde, Antonio Irpino, and Antonio Balzanella. Dimension reduction techniques
for distributional symbolic data. IEEE transactions on cybernetics, 46, 01 2015.
Cédric Villani. Optimal Transport: old and new, volume 338. Springer Science & Business
Media, 2008.
A Waechter and LT Biegler. On the implementation of a primal-dual interior point filter
line search algorithm for large-scale nonlinear programming. Mathematical Programming, 106:25–56, 2006.
44

Chao Zhang, Piotr Kokoszka, and Alexander Petersen. Wasserstein autoregressive models
for density time series. arXiv preprint arXiv:2006.12640, 2020.

45

