arXiv:2011.06333v1 [stat.ME] 12 Nov 2020

Shift identification in time varying regression quantiles
Subhra Sankar Dhar

Weichi Wu

IIT Kanpur

Center for Statistical Science

Department of Mathematics & Statistics

Department of Industrial Engineering

Kanpur 208016, India

Tsinghua University 10084 Beijing, China

email: subhra@iitk.ac.in

email: wuweichi@mail.tsinghua.edu.cn

November 13, 2020

Abstract
This article investigates whether time-varying quantile regression curves are the
same up to the horizontal shift or not. The errors and the covariates involved in the
regression model are allowed to be locally stationary. We formalise this issue in a
corresponding non-parametric hypothesis testing problem, and develop a integratedsquared-norm based test (SIT) as well as a simultaneous confidence band (SCB)
approach. The asymptotic properties of SIT and SCB under null and local alternatives are derived. We then propose valid wild bootstrap algorithms to implement SIT
and SCB. Furthermore, the usefulness of the proposed methodology is illustrated via
analysing simulated and real data related to Covid-19 outbreak and climate science.

Keyword: bootstrap, comparison of curves, confidence band, hypothesis testing, locally
stationary process, nonparametric quauntile regression, Covid-19.

1

1

Introduction

Consider two regression models with common response variable and the same covariates
1
2
for two different groups. Formally speaking, suppose that (yi,1 , xi,1 )ni=1
and (yi,2 , xi,2 )ni=1
are

two sets of data, where the covariates xi,1 = (xi,1,1 , ...xi,p1 ,1 )> and xi,2 = (xi,1,2 , ...xi,p2 ,2 )>
are p1 × 1 and p2 × 1 vectors, respectively. Now, for τ ∈ (0, 1) and s = 1 and 2, we define
the conditional quantiles

Qτ (yi,s |xi,s ) := inf{s : Fyi,s |xi,s (s|xi,s ) > τ } = θ1,τ,s

i
ns




xi,1,s + · · · + θps ,τ,s

i
ns


xi,ps ,s .
(1.1)

Note that model (1.1) can be written as
yi,s =

x>
i,s θ τ,s



i
ns


+ ei,τ,s , i = 1, · · · , ns , s = 1 and 2,

(1.2)

where for s = 1 and 2, θ τ,s (t) = (θ1,τ,s (t), ..., θps ,τ,s (t))> are ps × 1 vectors with each
element being a smooth function on [0, 1], and the errors ei,τ,s satisfy Qτ (ei,τ,s |xi,s ) = 0.
The last condition on the τ -th quantile of the conditional distribution of the errors given
the covariate ensures the model (1.2) is identifiable, and further technical assumptions
on xi,s and ei,τ,s will explicitly be discussed in Section 3. We are now interested in the
following hypothesis problem. For a pre-specified vectors c1 ∈ Rp1 ×1 and c2 ∈ Rp2 ×1 , define
>
ms (t) = c>
s θ τ,s (t) for s = 1 and 2, cs = (c1,s , · · · , cps ,s ) , and we want to test

H0 : m1 (t) = m2 (t + d) for 0 < t < 1 − d and some unknown constant d ∈ [0, 1). (1.3)
Let us now discuss a special case. Note that when d = 0, H0 will be equivalent to test
the equivalence of m1 (t) and m2 (t) for t ∈ (0, 1). In this case, when c1 = (1, 0, . . . , 0)>
and c2 = (1, 0, . . . , 0)> , the problem will coincide with comparing the curves θ1,τ,1 (t) and
θ1,τ,2 (t) for t ∈ (0, 1), and such comparison can be carried out by an appropriate functional
notion of difference between estimated θ1,τ,1 (t) and θ1,τ,2 (t). Such types of problems have
already been explored in the literature. For example, one can see Munk and Dette (1998).
However, our proposed testing of hypothesis problem described in (1.3) is fundamentally
2

different from the aforesaid case. Firstly, we are comparing two sets of certain linear
combinations of the components of the quantile coefficients of (1.1); it is not a direct
comparison between particular quantiles of two different distributions. Secondly, note
that in (1.1) and (1.2), the quantiles are time varying, which is entirely different from
the usual regression quantiles. Finally, in (1.3), we are checking whether there is any
nonnegative shift between two functions m1 and m2 or not. Note that if d = 0, as said
before, testing equality between m1 (t) and m2 (t) can be performed based on the suitable
functional difference between estimated m1 (t) and m2 (t) over (0, 1). However, in our case,
i.e., when d > 0, the same approach will not work since both functions will not coincide
over whole interval (0, 1). Strictly speaking, it will be a comparison between the curves
m1 (t) and m2 (t) up to a certain shift on the horizontal axis. Such relationship among the
quantile curves (at a fixed index value) with respect to the time parameter is often visible
in real life also. Exemplary, the Gross Domestic Product (GDP) curves of two nations over
a fixed period of time may have this feature. For usual mean based time varying models,
such type of problem was studied by Gamboa et al. (2007), Vimond (2010), Collier and
Dalalyan (2015) and a few references therein. However, none of them studied such problems
in the framework of quantile regression (i.e., (1.1) or (1.3)) for time varying models. This
article thoroughly studies this problem, and our major contributions are the following.
The first major contribution is to develop a formal test in checking the hypothesis (1.3)
for dependent and non-stationary data. In Section 2, we will establish that the hypothesis
0

−1
0
0
(1.3) is equivalent to the equality of (m−1
1 (.)) and (m2 (.)) (for any function f , f denotes

the derivatives of f , and f −1 denotes its inverse). We here formulate the test statistic based
−1
0
0
on the L2 -norm of a smooth estimator of the difference between (m−1
1 (.)) and (m2 (.)) ,

and the asymptotic distribution of the test statistic is derived under null hypothesis (i.e.,
the hypothesis described in (1.3)) and local alternatives. The test is denoted as the SIT
test as the test is the squared integral test. In this context, we would like to mention that
there have been a few research articles on conditional quantiles of independent data, and
the readers are referred to Zheng (1998), Horowitz (2002), He and Zhu (2003), Kim (2007)
and a few references therein. However, none of the above research articles considered the
more widely applicable testing of hypothesis problems that we consider here (see (1.3)) for
time varying quantile regression models (see (1.1)).

3

The second major contribution is to develop the simultaneous confidence band (SCB)
−1
0
0
for the difference between (m−1
1 (.)) and (m2 (.)) , and an asymptotic property of the SCB
0
is derived, which asserts the form of the confidence band of the difference between (m−1
1 (.))
0
and (m−1
2 (.)) for a preassigned level of significance. Recently, Wu and Zhou (2017) studied

the limiting properties of simultaneous confidence bands of the corresponding functional
considered in their article, which is different than key term of our work. The advantage of
proposing such graphical device is that one can expect a certain percentage of time, the
−1
0
0
value of the difference between (m−1
1 (.)) and (m2 (.)) will lie inside a band with a specific

probability. Our result will ensure that this coverage probability is asymptotically correct.
The third major contribution is to propose a robust Bootstrap procedure to have a good
finite sample performance of the SIT and the SCB. In principle, one can carry out the test
based on SIT and construct the SCB using the results in Theorems 3.1 and 3.2. However,
for small or moderate sample size, directly implementing those results may not produce
satisfactory performance due to slow convergence rate, and to overcome this problem, the
bootstrap method is proposed, and a better rate of convergence of the Bootstrap method
is established as well. In this context, the readers may also look at Zhou and Wu (2010)
and a few references therein.
The rest of the article is organized as follows. In Section 2, we characterize the null
hypothesis stated in (1.3), which is a key observation in subsequent theoretical studies.
Section 2.1 discusses the local linear quantile estimator for time varying regression coefficients, and in Section 2.2, basic ideas related to the estimation of the regression function
and its derivative are studied. The two-stage estimator of the shift parameter is also developed in this section along with the formualation of the SIT and the SCB tests. Section
3 investigates the SIT and the SCB tests. Section 4 explores issues related to implementation of the tests and provides the estimation of a complicated expression involved in
the implementation of the test described in Section 4.1. The bootstrap version of the
tests with the algorithm of implementation is studied in Section 4.2. A real data related
to COVID-19 outbreak is analysed in Section 5. Supplementary material contains some
bandwidths conditions, the choices of tuning parameters, all technical details and proofs
along with simulation studies and an analysis of real data related to climate science.

4

2

Methodology

0
Observe that inspecting null hypothesis described in (1.3) is equivalent to checking (m−1
1 (u)) −
0
(m−1
2 (u)) = 0 when u belongs to a certain interval. Proposition 2.1 states this result ex-

plicitly.
Proposition 2.1 Let m1 (t) and m2 (t) be strictly increasing functions on [0, 1]. Then
−1
0
0
H0 holds if and only if (m−1
1 (u)) − (m2 (u)) = 0 for all u ∈ [m1 (0), m1 (1 − d)] when

d = m−1
2 (m1 (0)).
In practice, under the null hypothesis (1.3), the estimation of d and deriving its asymptotic properties is a complicated task. Therefore, asymptotic performance of statistic based
ˆ where m̂1 , m̂2 and dˆ are estimators of m1 , m2 and d, respectively,
on m̂1 (t) − m̂2 (t + d),
could be intractable, and moreover, it is likely to have unsatisfactory results as various
issues like different rate of convergences are involved. In contrast, the assertion of Proposi−1
0
0
tion 2.1 suggests that we can test (1.3) based on the estimate of (m−1
1 (u)) − (m2 (u)) . To

use Proposition 2.1 in the theoretical results, one needs to know about the various issues
such as estimation of the time varying quantile regression functions and their derivatives,
formulation of test statistics etc, which are discussed in the following subsections.

2.1

Local linear quantile estimate

We estimate time varying quantile regression coefficients θ τ,s (t) using the concept of local
linear quantile estimators. Specifically, for s = 1 and 2, the local linear quantile estimate
>

of (θ τ,s (t), θ >
τ,s (t)) is denoted by (θ̂ τ,s,bn,s (t), θ̂ τ,s,bn,s (t)), where
0
(θ̂ τ,s,bn,s (t), θ̂ τ,s,bn,s (t))

=

argmin

ns
X


ρτ

yi,s −

β0 ∈Rps ,β1 ∈Rps i=1

x>
i,s β0

−

x>
i,s β1



i
−t
ns




Kbn,s


i
−t ,
ns
(2.1)

where
 ρτ (x) = τ x1[0,∞) (x) − (1 − τ )x1(−∞,0) (x), K(·) is a kernel function with Kbn,s (·) =
·
, and bn,s is the sequence of bandwidth associated with the s-th sample (s = 1
K bn,s
and 2). Note that the local linear (quantile) estimators have been extensively studied
5

in the literature of non-parametric statistics for both independent and dependent data,
see for example, Yu and Jones (1998), Chaudhuri (1991), Dette and Volgushev (2008),
Qu and Yoon (2015), Wu and Zhou (2017), Wu and Zhou (2018b) among many others.
Among them, Wu and Zhou (2017) investigated the estimator (2.1) with locally stationary
covariates and errors, and this locally stationary processes have been developed in the
literature to model the slowly changing stochastic structure, which can be found in many
real world time series data; see for instance, Dahlhaus et al. (1997), Zhou and Wu (2009),
Dette and Wu (2020), Dahlhaus et al. (2019). These articles motivated us to work on the
hypothesis (1.3) assuming local stationarity. The list of detailed assumptions are deferred
to Section 3.
We now estimate m1 (t) and m2 (t) through a biased-corrected estimate of θ̃ τ,s =
(θ̃τ,s,1 , ..., θ̃τ,s,ps )> for s = 1 and 2. That is
>
m̂1 (t) = c>
1 θ̃ τ,1 (t), m̂2 (t) = c2 θ̃ τ,2 (t),

(2.2)

where for s = 1 and 2, and for 1 ≤ j ≤ ps ,
(b

)

(

bn,s
√

)

(b

)

n,s
n,s
(t) = 2θ̂τ,s,j2 (t) − θ̂τ,s,j
θ̃τ,s,j
(t).

(2.3)

The sup-script inside the parentheses denotes the bandwidth used for the corresponding
estimator, and it will be omitted in the rest of the article for the sake of notational sim(b

)

plicity. Notice that (θ̃s,jn,s (t), 1 ≤ j ≤ ps , bn,s ≤ t ≤ 1 − bn,s ) are equivalent to the local
√
√
linear quantile estimators using the second-order kernel K̄(x) = 2 2K( 2x) − K(x). It
(b

)

n,s
(t) has a bias at
can be shown similarly to Section 4.1 of Dette and Wu (2019) that θ̃τ,s,j

(b

)

n,s
the order of b3n,s , while the unadjusted estimator θ̂τ,s,j
(t) has a bias of the order O(b2n,s ),

which is non-negligible and hard to evaluate. Therefore, the de-biased estimator has been
widely applied in non-parametric inference, see for example Schucany and Sommers (1977)
and Wu and Zhao (2007).

6

2.2

Basic ideas and the tests

Suppose that H is a smooth kernel function, hs (s = 1 and 2) is a sufficiently small
−1 0
0
bandwidth, and N is a sufficiently large number. We then estimate (m−1
1 ) (t) and (m2 ) (t),

which are denoted by ĝ1 (t) and ĝ2 (t), respectively :




N
N
m̂1 ( Ni ) − t
m̂2 ( Ni ) − t
1 X
1 X
ĝ1 (t) =
H
, and ĝ2 (t) =
H
.
N h1 i=1
h1
N h2 i=1
h2

(2.4)

Notice that N is not the sample size; it is used for Riemann approximation. Further,
observe that
N

1 X
ĝs (t) ≈
H
N h i=1



ms ( Ni ) − t
hs



1
≈
hs

Z1


H

ms (x) − t
hs


dx

(2.5)

0

(ms (1)−t))/h
s
Z

H(u)((ms )−1 )0 (t + uhs )du ≈ ((ms )−1 )0 (t)1(ms (0) < t < ms (1)),

=
(ms (0)−t))/hs

where 1(A) denotes the indicator function of set A. Therefore, the estimator defined in
(2.4) is a smooth approximation to the step function ((ms )−1 )0 (t)1(ms (0) < t < ms (1))
and is differentiable with respect to t. Such type of estimator was proposed by Dette et al.
(2006) and studied extensively by Dette and Wu (2019) for non-stationary time series
models.
Now, using ĝs (s = 1 and 2), one can estimate m−1
s (t) (s = 1 and 2) by
Zt
Ĝs (t) =

ĝs (u)du.

(2.6)

ms (0)

This fact motivates us to estimate the horizontal shift d under null hypothesis as follows.
Note that for 0 ≤ t ≤ 1 − d, we have m1 (t) = m2 (t + d) = u when m1 (0) ≤ u ≤ m1 (1 − d),
and therefore,
−1
d = m−1
2 (u) − m1 (u), m1 (0) ≤ u ≤ m1 (1 − d).

7

(2.7)

This fact drives us to estimate d by
˜
m̂1 R
(1−d)

[Ĝ2 (u) − Ĝ1 (u)]du

dˆ =

m̂1 (0)

˜ − m̂1 (0)
m̂1 (1 − d)

,

(2.8)

where d˜ = m̂−1
2 (m̂1 (0)) is a preliminary estimator of d by letting u = m1 (0) in (2.7). With
ˆ one can therefore estimate the endpoints of intervals in (2.7), i.e., a := m1 (0) and
this d,
b := m1 (1 − d). Let â and b̂ be the estimators of a and b, respectively, where â := m̂1 (0)
ˆ under null hypothesis, and their properties under null and alternative
and b̂ := m̂1 (1 − d)
are discussed in Proposition E.2 of the supplementary material.
Next, to formulate the test statistic, we use the fact in Proposition 2.1 and propose the
SIT and the SCB tests to check the hypothesis described in (1.3) based on ĝ1 (t) − ĝ2 (t).
For the SIT test, the test statistics is defined as
Z
Tn1 ,n2 =

(ĝ1 (t) − ĝ2 (t))2 ŵ(t)dt, where ŵ(t) = 1(â + η ≤ t ≤ b̂ − η),

(2.9)

and η = ηn1 ,n2 is a positive sequence diminishes sufficiently slow as n1 , n2 → ∞. For
instance, one may consider ηn1 ,n2 vanishes at the rate of

1
.
log(n1 +n2 )

The purpose of intro-

ducing η here is to avoid the issues related to the boundary points; for details, see remark
E.1 of the supplementary material. Observe that Tn1 ,n2 is an estimate of distance between
−1 0
0
(m−1
1 ) (t) and (m2 ) (t) in L2 sense, and we shall reject the null hypothesis when Tn1 ,n2

is a large enough. The second test is the simultaneous confidence band centered around
ĝ1 (t) − ĝ2 (t), whose detailed expression is provided in the statement of Theorem 3.2. Using
the relation between the testing of hypothesis and the confidence band, it is easy to see
that the SCB test is rejected at significance level α if the [â + η, b̂ − η] is not entirely
contained by the 100(1 − α)% SCB.

3

Asymptotic Results

In this section, we investigate the asymptotic properties of Tn1 ,n2 and the asymptotic
form of the SCB at a presumed significance level α. We start from a few concepts and
8

(1)

(2)

(1)

(2)

assumptions for the model described in (1.2). Let (ζi )i∈Z , (ζi )i∈Z , (ηi )i∈Z and (ηi )i∈Z
be i.i.d. random vectors, and the filtrations for s = 1 and 2 are the following: Fi,s =
(s)

(s)

(s)

(s)

(s)

(s)

(ζ−∞ , ..., ζi−1 , ζi ) and Gi,s = (η−∞ , ..., ηi−1 , ηi ). We assume that the covariates and errors
are both locally stationary process in the sense of Zhou and Wu (2009), i.e.,

xi,s = Hs




i
i
, Gi,s , ei,s = Ls
, Fi,s , Gi,s
n
n

for s = 1 and 2, where Hs and Ls are the marginal filters. We list some basic as∗
=
sumptions of processes xi,s and ei,s in conditions (A3) and (A4). Now, write Fi,s
(s),∗

(F−1,s , ζ0

(s)

(s)

(s)

(s),∗

, ζ1 ..., ζi−1 , ζi ), where (ζi

(s)

∗
)i∈Z is an i.i.d. copy of (ζi )i∈Z and define Gi,s

also in a similar way. For a p dimensional (random) vector v := (v1 , . . . , vp )> , let |v| =
√ Pp
( i=1 vi2 ), and for any random vector v, write kvkq = (E(|v|q ))1/q , which is its Lq -norm
q ≥ 1. Let χ ∈ (0, 1) be a fixed constant, and suppose that M and η are sufficiently
large and sufficiently small positive constants, respectively; though it may vary from line
to line. For any positive semi-definite matrix Σ, write λmin (Σ) as its smallest eigenvalue.
We first give out the following set of conditions, which enable us to study the deviation of
the nonparametric quantile estimator, θ̃ τ − θ τ .
00
00
,1 ≤ i ≤
(t), ..., θp00s ,τ,s (t))> for s = 1 and 2. Assume that (θi,τ,s
(A1) Define θ 00τ,s (t) = (θ1,τ,s

ps )s=1,2 is Lipschitz continuous on [0, 1].
(A2) Define Qτ (Ls (t, Fi,s , Gi,s )|Gi,s ) := inf {x : P(Ls (t, Fi,s , Gi,s ) ≥ x|Gi,s ) ≥ τ }. Assume
x

that Qτ (Ls (t, Fi,s , Gi,s )|Gi,s ) = 0.
(A3) For errors process, we assume that for s = 1 and 2,
∗
∗
i) δ(L, i) := sup kLs (t, Fi,s
, Gi,s
) − Ls (t, Fi,s , Gi,s )k1 = O(χi ) for i ≥ 0.

(3.1)

t∈[0,1]

ii)

sup

kLs (t1 , F0,s , G0,s ) − Ls (t2 , F0,s , G0,s ))kv ≤ M |t1 − t2 |

(3.2)

0≤t1 ,t2 ≤1

for a constant v ≥ 1.
(A4) For covariate processes, we assume that for s = 1 and 2, there exists a constant

9

tx > 0 such that,
i) max E(exp(tx |xi,s |)) ≤ M < ∞,

(3.3)

∗
ii) δ(H, i) := sup kHs (t, Gi,s
) − Hs (t, Gi ))k1 = O(χi ) for i ≥ 0,

(3.4)

1≤i≤ns

t∈[0,1]

iii)

kHs (t1 , F0,s , G0,s ) − H(t2 , F0,s , G0,s ))k2 ≤ M |t1 − t2 |.

sup

(3.5)

0≤t1 ,t2 ≤1

(A5) For conditional densities, we define for s = 1 and 2, and for 0 ≤ q ≤ 2ps + 1,
Fs(q) (t, x|Fi−1,s , Gi,s ) =

∂q
P(Ls (t, Fi,s , Gi,s ) ≤ x|Fi−1,s , Gi,s ).
∂xq

(3.6)

(1)

In particular, we write fs (t, x|Fi−1,s , Gi,s ) = Fs (t, x|Fi−1,s , Gi,s ) for brevity. Assume
that

sup

|Fs(q) (t, x|Fi−1 , Gi,s )| ≤ M . Further, define

t∈[0,1],x∈R
(q)

δl,s (i − 1) =

sup

∗
kFs(q) (t, x|Fi−1,s
, Gi,s ) − Fs(q) (t, x|Fi−1,s , Gi,s )kl ,

(3.7)

t∈[0,1],x∈R
(q)

and assume that δ1,s (i − 1) = O(χi ) for i ≥ 0.
(A6) Define for s = 1 and 2, conditional on Gi,s , the conditional quantity and the quantile
design matrix as
fs (t, x|Gi,s ) =

∂
P(ei,s (t) ≤ x|Gi ), Σs (t) = E(fs (t, 0|Gi,s )Hs (t, Gi,s )H>
s (t, Gi,s )).
∂x
(3.8)

Assume that
sup kfs (t, 0|Gi ) − fs (t, 0|Gi∗ )k2 = O(χi ) for i ≥ 0,

(3.9)

t∈[0,1]

inf λmin (Σs (t)) ≥ η > 0 and sup |fs(1) (t, 0|G0 )| ≤ M.

t∈[0,1]

(3.10)

t∈[0,1]

(A7) Let ψτ (x) = τ − 1(x ≤ 0) be the left derivative of ρτ (x). For s = 1 and 2, define the

10

gradient vector process
Us (t, Fi,s , Gi,s ) = ψτ (Ls (t, Fi,s , Gi,s ))Hs (t, Gi,s ).

(3.11)

Notice that by definition, U( nis , Fi,s , Gi,s ) = ψτ (ei,s )xi,s , which is the gradient vector.
Now define the long run covariance matrices for U, which is
Vs2 (t)

=

∞
X

Cov(Us (t, F0,s , G0,s ), Us (t, Fj,s , Gj,s )).

(3.12)

inf λmin (Vs2 (t)) ≥ η̃ > 0.

(3.13)

j=−∞

Assume that for s = 1 and 2,

t∈(0,1]

(A8) The kernel functions K(·) and H(·) are symmetric and twice differentiable functions
R1
R1
with support [−1, 1]. Also, K(x)dx = 1, H(x)dx = 1, and K 00 , H 00 are Lipschitz
−1

−1

continuous on [−1, 1].
Conditions (A1)-(A8) are associated with the smoothness for the quantile regression
coefficients, conditional quantiles, errors and covariates. The quantities δ(L, i) and δ(G, i)
are called ‘physical dependence measure’ in the literature (see Zhou and Wu (2009)), and
ii) of conditions (A3) and (A4) postulate stochastic Lipschitz continuity Ls and Hs , respectively. In fact, conditions (A3) and (A4) ensure that the error and covariates are both
short range locally stationary processes with geometrically decaying dependence measure.
The verification of these conditions is uncomplicated for a general class of locally stationary processes; refer to Zhou and Wu (2009) for more details. Condition (A5) is a standard
assumption on the dependence measures of the derivatives of the errors’ conditional densities for non-stationary time series quantile regression, see Wu and Zhou (2017), Wu and
Zhou (2018a) among many others for details and Zhou and Wu (2009) for the verification
of this condition on representative examples. Assumption (A6) ensures that the process
Pns
>
i=1 fs (i/ns , 0|Gi )xi,s xi,s Kbn,s (i/ns − t)/(ns bn,s ) converges to the non-degenerate quantile
design matrix. Similar conditions are also assumed in Kim (2007), Qu (2008) and a few
references therein. Condition (A7) means that the long-run covariance matrices of the
11

gradient vectors Us (t, Fi,s , Gi,s ) are non-degenerate. Condition (A8) is a mild condition
for kernels, and the well known Epanechnikov and many more kernel functions satisfy the
assumptions stated in (A8). Notice that conditions (A1)-(A7) generalize the conditions
(A1)-(A5) of Wu and Zhou (2017) for multiple curves.
−1
2 > −1
> 1/2
Now, for s = 1 and 2, we define Tn,s = (bn,s , 1−bn,s ) and Mcs (t) = ((c>
.
s Σs (t))Vs (cs Σs (t)) )

We then have the following result related to uniform approximation of m̂s (t) − ms (t)
through a Gaussian process. The proof of this proposition is based on a Bahadur Representation of m̂s (t)−ms (t). We further consider a few more conditions (B) on the bandwidth
and the regression functions and state it in Section A of the supplementary material.
Before stating the main results on Tn1 ,n2 and SCB, we introduce a few more notations.
Define for s = 1 and 2,
ǧs (t) =

−1 0
2
Mcs (m−1
s (t))((ms ) (t))

Z

H 0 (y)ydy,

(3.14)

g̃s2 (ms (u))(K̄ 0 ? K̄ 0 (y))2 dudy.

(3.15)

R

g̃s (ms (u)) = ǧs2 (ms (u))w(ms (u))m0s (u), V̌s =

Z Z
R

R

Write m2,1 (u) = m−1
(m1 (u)), g̃1,2 (m1 (u)) = ǧ1 (m1 (u))ǧ2 (m1 (u))w(m1 (u))m01 (u) and V̌12 (r) =
2
2

 2
R 2
R
R 0
R 0
RR 2
0
0
K̄ (x)dx
H (y)dy
K̄ (x)K̄ (rm21 (u)x + y)dx dudy. For s = 1 and 2, let B̌s =
g̃1,2 (m1 (u))
R R

R

Notice that under the null hypothesis (1.3), m02,1 (u) ≡ 1.

R

R

Theorem 3.1 Assume the conditions stated in (A1)-(A8) and (B1), (B2), (B3) in the
supplementary material. Now, suppose that n1 /n2 → γ0 , bn,1 /bn,2 → γ1 , η −1 = O(log(n1 +
−1 0
0
n2 )), η = o(1). Further, let (m−1
1 ) (t) − (m2 ) (t) = ρn κ(t) for some bounded non-zero
5/2

function κ(t), and ρn := ρn1 ,n2 = (n1 bn1 )−1/2 . We then have
5/2
n1 bn,1 Tn1 ,n2

−

−1/2
bn,1 (B̌1

+

γ0 γ13 B̌2 ))

Z
−

κ2 (t)w(t)dt ⇒ N (0, VT ),

(3.16)

R

where VT = V̌1 + γ02 γ15 V̌2 + 4γ0 γ13 V̌12 (γ1 ).
Under null hypothesis, κ ≡ 0. Therefore, Theorem 3.1 suggests to reject null hypothesis

12

R

of (1.3) whenever
−1/2

Tn1 ,n2 >

bn,1




1/2
ˆ + γ γ 3 B̌
ˆ
B̌
1
0 1 2 + z1−α V̂T
5/2

,

(3.17)

n1 bn,1

where α is the significance level, z1−α is the (1 − α)-th quantile of a standard normal
ˆ , B̌
ˆ and V̂ are appropriate estimates of asymptotic bias parameter B ,
distribution, B̌
1
2
T
1
B2 and the asymptotic variance VT , respectively. Moreover, Theorem 3.1 shows that the
√
5/2
SIT test is able to detect the alternative which converges to null at a rate of (n1 bn,1 ),
with asymptotic power


R

1 − Φ z1−α −

R

κ2 (t)w(t)dt
1/2

VT


,

(3.18)

where Φ(·) denotes the CDF of a standard normal random variable.
Theorem 3.2 Assume the conditions stated in (A1)-(A8) and (B1), (B2), (B3) in the
supplementary material. Further, assume that for s = 1 and 2,
n2
n1

→ cn,2 ∈ (0, 1),

h2
h1

→ ch,2 ∈ (0, 1), η = o(1), η

−1

bn,2
bn,1

→ cb,2 ∈ (0, 1),

= O(log(n1 + n2 )), and let cb,1 = cn,1 =

ch,1 = 1. Define

K1 (t) =

K2 (t) =

2
X
s=1

0

K̄ 02 (y)dy 

(3.19)


2
Z
Z
K̄ 2 (y)dy  H 00 (x)dx .

(3.20)

2
cn,s c3b,s (m0s (m−1
s (t)))
R

Mc2s (m−1
s (t))
4
2
cn,s cb,s ch,s (m0s (m−1
s (t)))

Z

H 0 (x)xdx ,

s

s=1

2



Z
2
−1 02
X
Mc2 (m−1
s (t))(ms ) (t)

R

R

R

0

−1
Then, if (m−1
1 ) (t) − (m2 ) (t) = ρn1 ,n2 κ(t) for some non-zero bounded function κ(t) and

ρn1 ,n2 = o(η), as min(n1 , n2 ) → ∞, we have
√
P

sup
t∈Iâ,b̂

−1 0
−1 0
−1 0
0
(n1 b3n,1 )|(m̂−1
1 ) (t) − (m̂2 ) (t) − ((m1 ) (t) − (m2 ) (t))|
1/2

K1 (t)

!
√
≥ (−2 log(πα/κ0 )) → α,
(3.21)

13

where Iâ,b̂ = (â + η, b̂ − η) and
bn,1
κ0 = 2
h1

−1
m1 (1−m
Z2 (m1 (0)))

K2 (t)
K1 (t)

1/2
dt.

(3.22)

m1 (0)

0

0

−1
Theorem 3.2 gives us the following simultaneous confidence band of (m−1
1 ) (t) − (m2 ) (t):
1/2

0
(m̂−1
1 ) (t)

where κ̂0 =

bn,1
h21

b̂−η
R
â+η

−

0
(m̂−1
2 ) (t)

√ K̂2 (t)
K̂1 (t)

K̂ (t) √
±√ 1 3
(−2 log(πα/κ̂0 )), t ∈ Iâ,b̂ ,
(n1 bn,1 )

(3.23)

dt, and K̂1 (t) and K̂2 (t) are appropriate estimators of K1 (t)

and K2 (t), respectively. Therefore we can reject the null hypothesis (1.3) at significance
level α. Furthermore, it follows from condition (B) and (3.22) that the width of (3.23) is
√

(log n−log α)
√
.
(n1 b3n,1 )

at a rate of

Consequently, the SCB test is able to detect the alternative converging to null

√
log n
√
,
(n1 b3n,1 )

which indicates the SIT test is asymptotically more powerful than the

SCB test when bandwidths are of the same order. However, for moderately large sample
0

0

−1
size, the SCB test performs well when (m−1
1 ) (t) − (m2 ) (t) is ‘bumpy’, or equivalently the

majority part of two curves m1 (t) and m2 (t) are same up to the horizontal shift while minor
parts of m1 (t) and m2 (t) have notably different shapes so that their differences cannot be
eliminated by a horizontal shift.

4
4.1

Implementation of the tests
Estimation of Mcs (t)

The implementation of the SIT test and the SCB test require the estimation of Mcs (t).
For s = 1 and 2, let êi,s (t) = yi,s − x>
i,s θ̂ τ,s (t), and
Σ̂s (t) =

1
ns bn,s wn,s



ns
X
êi,s (t)
xi,s x>
φ
i,s Kbn,s (i/ns − t),
w
n,s
i=1

14

(4.24)

where bandwidth wn,s is such that wn,s = o(1), ns wn,s → ∞, and φ(·) is the probability
density function of the standard normal distribution. It has been shown in Theorem 6 in
Wu and Zhou (2017) that with appropriate choices of wn,s , Σ̂s (t) is a consistent estimator
of Σs (t) uniformly on [bn,s , 1 − bn,s ]. To estimate Vs (t), we define
Ξ̂i,s





Ms 
X
i+j
=
τ − 1 êi+j,s
≤0
xi+j,s ,
n
s
j=−M

(4.25)

s

where Ms → ∞, and Ms = o(ns ) is the window size.
ˆi =
Furthermore, let ∆

>

Ξ̂i,s Ξ̂i,s
,
2Ms +1

and

V̂s (t) =

ns
1 X
ˆ i.
Kb (i/ns − t)∆
ns bn,s i=1 n,s

(4.26)

With appropriate choices of bandwidth,h Theorem 5 in Wu and Zhou
i (2017) shows that
V̂s (t) converges to V(t) uniformly on bn,s + Mns s+1 , 1 − bn,s − Mns s+1 . We then estimate
Mcs (t) by



> 1/2
> −1
2
> −1
cs Σ̂s (t) V̂s cs Σ̂s (t)
M̂cs (t) :=
ms +1
], and
ns
Ms +1
bn,s − ns ) for

for t ∈ [bn,s , 1 − bn,s −

M̂cs (t) = M̂cs (bn,s +

M̂cs (t) = M̂cs (1 −

t ∈ (1 − bn,s −

Ms +1
)
ns

Ms +1
, 1].
ns

for 0 ≤ t < bn,s +

(4.27)
Ms +1
,
ns

Consequently, the estimator

M̂cs (t) is a consistent estimator of Mcs (t) under appropriate choices of Ms and wn,s . We
move the choice of Ms and wn,s to Section B of the supplementary material.

4.2

Boostrap-Based Test

Let {Vj,1 , j ∈ Z} and {Vj,2 , j ∈ Z} be i.i.d. standard normal random variables. Theorems 3.1 and 3.2 are built on the fact that the distribution of ĝ1 (t) − ĝ2 (t) can be well
approximated by
0
−1
0
(m−1
1 (t)) − (m2 (t)) + Z(t, {Vj,s }j∈Z,s=1,2 ),

15

where Z(t, {Vj,s }j∈Z,s=1,2 ) is a Gaussian process defined by Z(t, {Vj,s }j∈Z,s=1,2 ) := Z1 (t, {Vj,1 }j∈Z )−
Z2 (t, {Vj,2 }j∈Z ), and for s = 1 and 2,
Zs (t, {Vj,s }j∈Z ) =

ns
X

Ws (ms , j, t)Vj,s and

j=1

Ws (ms , j, t) =

1

N
X

ns bn,s N h2s

i=1

Mcs (i/N )H

0



ms (i/N ) − t
hs


K̄bn,s (j/ns − i/N ).

The limiting distribution is established by the asymptotic limit of quadratic form of
the Gaussian process Z(t, {Vj,s }j∈Z,s=1,2 ) for Theorem 3.1, and the convergence of extreme
values of Z(t, {Vj,s }j∈Z,s=1,2 ) for Theorem 3.2. However, the direct implementation of
Theorem 3.1 and Theorem 3.2 is difficult. The former involves a complicated bias
of
 term 
−1/2
the order (bn,s ) to be estimated, and the latter has a slow convergence rate O √log1 ns ,
which follows from the proof of Theorem 3.2. To circumvent this difficulty, we propose the
following Bootstrap-assisted algorithm based on Z(t, {Vj,s }j∈Z,s=1,2 ).
Algorithm 4.1 (Bootstrap-SIT)
(a) Estimate m1 and m2 by (2.2) and estimate Mcs (·).
(Q)

(Q)

1
2
(b) Generate Q copies of i.i.d. standard normal random variables {Vj,1 }nj=1
and {Vj,2 }nj=1

to obtain the statistic
MQ =

Z 

2
(Q)
(Q)
Z1 (t, {Vj,1 }j∈Z ) − Z2 (t, {Vj,2 }j∈Z ) ŵ(t)dt.

R

(c) Let M(1) ≤ M(2) ≤ . . . ≤ M(B) be the ordered statistics of {Ms , 1 ≤ s ≤ Q}. We reject
the null hypothesis (1.3) at level α, whenever
Tn1 ,n2 > M(bB(1−α)c) .
The p-value of this test is given by 1 − Q∗ /Q, where Q∗ = max{r : M(r) ≤ Tn1 ,n2 }.
Algorithm 4.2 (Bootstrap-SCB)
16

(4.28)

(a) Estimate m1 and m2 by (2.2) and estimate Mcs (·).
(Q)

(Q)

1
2
(b) Generate Q copies of i.i.d. standard normal random variables {Vj,1 }nj=1
and {Vj,2 }nj=1

to obtain the statistic
(Q)

(Q)

1/2

M̃Q = sup (Z1 (t, {Vj,1 }j∈Z ) − Z2 (t, {Vj,2 }j∈Z ))/K1 (t) .
t∈Iâ,b̂

(c) Let M̃(1) ≤ M̃(2) ≤ . . . ≤ M̃(B) be the ordered statistics of {M̃s , 1 ≤ s ≤ Q}. Then, the
−1 0
0
(1 − α)- SCB of (m−1
1 ) (t) − (m2 ) (t) is
1/2

0
−1 0
(m̂−1
1 ) (t) − (m̂2 ) (t) ± M̃(bB(1−α)c) K̂1 (t).

(4.29)
−1/2

To apply Algorithm 4.1, there is no need to estimate the bias term bn,1 (B̌1 + γ0 γ13 B̌2 ))
as well as the asymptotic variance VT . The validity of these algorithms are based on the
approximation of Z(t, {Vj,s }j∈Z,s=1,2 ) to ĝ1 (t) − ĝ2 (t) (see (2.4) for the expressions of ĝ1 (t)
and ĝ2 (t)), which is discussed in detail in the proof of Theorem 3.1. Notice that the cutoff
values M(bB(1−α)c) and M̃(bB(1−α)c) are obtained for fixed n1 and n2 , while the critical values
in Theorem 3.1 and Theorem 3.2 are based on the limiting distribution. Therefore, similar
to Zhao and Wu (2008), we expect that Algorithms 4.1 and 4.2 will outperform the test
using the critical values of Theorems 3.1 and 3.2. Finally, to implement Algorithm 4.2, we
0
−1 0
need to estimate K1 , which consists of the estimate of m−1
s , (ms ) and ms for s = 1 and
Rt
0
2. We suggest to estimate these quantities by
ĝs (u)du, ĝs (u), and c>
s θ̃ τ,s (t) for s = 1
ms (0)
b

n,s
0( √
0 (b
)
0
0
0
n,s )
and 2, respectively, where θ̃ τ,s (t) = (θ̃ τ,s,1 (t), . . . , θ̃ τ,s,p (t))> with 2θ̂τ,s,j2 (t)− θ̂τ,s,j
(t),
0 (b
0 (b
0
n,s )
(bn,s )
n,s )
θ̂ τ,s (t) = (θ̂τ,s,1
(t), . . . , θ̂τ,s,p
(t))> is defined in (2.1) having bandwidth bn,s .

5

and

Real Data Analysis

Cumulative infected cases and deaths due to COVID-19: This data set consists of
two variables, namely, the cumulative number of infected cases and the cumulative number
of deaths due to Covid-19 outbreak in a particular country for the period from December
17

31, 2019 to October 7, 2020, i.e., n = 282 days. We here consider two countries, namely,
France and Germany as they are from the same continent. Our analysis is based on the
log transformed data since the data is varying from a small values to a quite large values.
The data set is available at https://ourworldindata.org/coronavirus-source-data.
The analysis has three parts, namely, (A) Analysis of cumulative infected cases and deaths
in France due to Covid-19 outbreak, (B) Analysis of cumulative infected cases in France
and Germany due to Covid-19 outbreak and (C) Analysis of Cumulative deaths in France
and Germany due to Covid-19 outbreak. All three analyses are done for τ = 0.5, i.e.,
based on the median curve of the cumulative infected cases and deaths. Besides, in order
to implement our proposed methodology, we consider n = 282 equally spaced points on
[0, 1], and the plots are prepared on the time interval [0, 1].

5.1

Cumulative infected cases and deaths in France : Covid-19
outbreak

Let us first discuss a few observations. The left diagram in Figure 1 indicates that both
cumulative infected cases and deaths are increasing over time in France, which is also
expected as the new cases are added to the data on every day. In fact, it is observed in the
right diagram in Figure 1 that the median curves (i.e., τ = 0.5) of the cumulative infected
cases and the deaths have an increasing trend over time. First, we now implement the
SIT and the SCB tests on the full data, and the tests are carried out using the procedure
explained in Section 4.2. In this context, we would like to mention that there is no covariate here, and hence, the choice of c does not have any role. For B = 1000, the p-values
of the SIT and the SCB tests are computed when τ = 0.5, and the p-values are 0.067 and
0.072 for the SIT and the SCB tests, respectively. These p-values of both tests indicate
the rejection of the null hypothesis at 8% level of significance, i.e., in other words, the
cumulative infected cases and deaths in France due to Covid-19 do not follow the model
described in (1.3).
However, we obtain the large p-values as 0.473 and 0.446 for the SIT and the SCB tests,
respectively when the tests are implements on the data corresponds to time on [0, 0.4], i.e.,
the data for the period from December 31, 2019 to April 13, 2020, i.e., altogether for the

18

Median : Infected cases and deaths in France

0.0

0.2

December 31, 2019

0.4

April 13, 2020

time

0.6

0.8

1.0

October 7, 2020

0

5

10

15

Median : Infected cases and deaths (log scale)

15
10
5
0

Infected cases and Deaths (log scale)

Infected cases and Deaths (France)

0.0
December 31, 2019

0.2

0.4

0.6

April 13, 2020

time

0.8

1.0

October 7, 2020

Figure 1: The left diagram plots the cumulative infected cases (solid line) and deaths
(dashed line) in France. The right diagram plots the median curves of cumulative infected
cases (solid line) and deaths (dashed line) in France.
period of 105 days. These p-values indicate that the cumulative infected cases and deaths in
France have pattern like the model described in (1.3). In this study, we obtain dˆ = 0.0564,
i.e., in other words, till April 13, 2020, in France, the curves of cumulative infected cases
and deaths followed the same pattern but the cumulative infected cases was ahead about
six days (as 0.0564 × 105 = 5.922) compared to the cumulative deaths. Afterwards, as the
death rate went down, the same shift difference was not observed till October 7, 2020.

5.2

Cumulative infected cases in France and Germany : Covid19 outbreak

In this case, we implement the SIT and the SCB tests on the cumulative infected cases
of France and Germany. For B = 1000 and τ = 0.5, we obtain the p-values as 0.523 and
0.458 of the SIT test and the SCB test, respectively, which indicates that the cumulative
infected cases in France and Germany follows the model described in (1.3). Moreover, we
obtain dˆ = 0.003, i.e., in other words, one can conclude that the cumulative infected cases
in France have the same pattern as that of Germany, but they are approximately ahead
of a day (as 0.003 × 282 = 0.846) compared to Germany’s number for the period from
December 31, 2019 to October 7, 2020, i.e., altogether the period of 282 days.
19

Infected cases (France and Germany)

0.0

0.2

December 31, 2019

0.4

0.6

April 13, 2020

time

0.8

0

5

10

15

Median curve : Infected cases (log scale)

15
10
5
0

Infected cases (log scale)

Median : Infected cases (France and Germany)

0.0

1.0

October 7, 2020

0.2

December 31, 2019

0.4

0.6

April 13, 2020

time

0.8

1.0

October 7, 2020

Figure 2: The left diagram plots the cumulative infected cases of France (solid line) and
that of Germany (dashed line). The right diagram plots the median curves of cumulative
infected cases of France (solid line) and that of Germany (dashed line).

5.3

Cumulative deaths in France and Germany : Covid-19 outbreak

First observe that the left diagram in Figure 3 indicates that the cumulative death cases
in both France and Germany are increasing over time, and it is also expected as the new
cases are added to the data on every day. In addition, it is observed in the right diagram
in Figure 3 that the median curves (i.e., τ = 0.5) of the cumulative deaths in France and
Germany have an increasing trend over time. We now implement the SIT and the SCB
tests on the full data, and the tests are carried out as the earlier cases. For B = 1000
and τ = 0.5, the p-values are obtained as 0.084 and 0.081 of the SIT and the SCB tests,
respectively. These p-values of both tests indicate the rejection of the null hypothesis at
9% level of significance, i.e., in other words, the cumulative deaths due to Covid-19 in
France and Germany do not follow the model described in (1.3).
However, we see the opposite scenario when the SIT and the SCB tests are implemented
on the data corresponds to time on [0, 0.4], i.e., the data for the period from December 31,
2019 to April 13, 2020, i.e., altogether the period of 105 days. For this period of time, the
p-values are 0.633 and 0.596 for the SIT test and the SCB test, respectively. These large
p-values indicate that the cumulative deaths in France and Germany have pattern like the
20

Median Curve : Deaths (France and Germany)

2

4

6

8

10

Median Curve : Deaths (log scale)

10
8
6
4
2

0

0

Deaths in France and Germany (log scale)

Deaths in France and Germany

0.0

0.2

December 31, 2019

0.4

0.6

April 13, 2020

time

0.8

1.0

October 7, 2020

0.0
December 31, 2019

0.2

0.4

0.6

April 13, 2020

time

0.8

1.0

October 7, 2020

Figure 3: The left diagram plots the cumulative death of France (solid line) and that of
Germany (dashed line). The right diagram plots the median curves of cumulative death of
France (solid line) and that of Germany (dashed line).
model described in (1.3). In this study, we obtain dˆ = 0.075, i.e., in other words, till April
13, 2020, the cumulative deaths in France have the same pattern as that of Germany,
but they were approximately ahead of eight days (as 0.075 × 105 = 7.875) compared to
Germany’s number. Afterwards, as the death rate went down, the same shift difference
was not observed till October 7, 2020.
Acknowledgment Weichi Wu (corresponding author) is funded by NSFC Young
program (No.11901337). Subhra Sankar Dhar is funded by SERB project MATRICS
(MTR/2019/000039).

6

SUPPLEMENTARY MATERIAL

The supplementary material contains the conditions on bandwidth, the procedure of bandwidth selection, all results of simulation study, an analysis of real data related to climate
science which considers various covariates, and all technical details.

21

Supplementary material to
“Shift identification in time varying regression quantiles”
Abstract
In this supplementary material, we discuss some bandwidth conditions in Section
A and the choice of bandwidths in Section B, presenting simulation studies in Section
C and a real temperature data study in Section D, and display all proofs with some
propositions in Section E.

A

Additional assumptions (B) on bandwidths and regression functions

We consider a few more conditions (B) on the bandwidth and the regression function.
√
4/3
ns
= o(1), hs /bn,s = o(1), πn,s = o( log ns ), √log
= o(1),
(ns bn,s )hs


2
P
4
ns
and assume that
and hs /b2n,s → ∞. Let Ωn =
Θn,s + hs + N1hs + (nblog
3/2 h2
n,s )
s
s=1
 2

P
5/2 −1/2
Ωn = o
(ns bn,s )
.

(B1) For s = 1 and 2,

(log3/2 ns )
√
(ns b2n,s )

s=1

(B2) (m02 )−1 (m1 (0)) ∈ (0, 1).
(B3) bn,s → 0,

nb4n,s
log8 n

→ ∞, nι bn,s = o(1) for some ι > 0.

The condition (B1) implies that in practice we should choose hs small, which was remarked
by Dette et al. (2006) also. Further, (B1) ensures that our proposed estimator b̂ is well
defined under alternative hypothesis, and under null, (B2) clearly means that d ∈ (0, 1).
Condition (B3) guarantees that the nonparameteric estimate m̂s approximate well ms ,
s = 1, 2.

B

Bandwidth Selection

In this section, we first discuss the choices of the smoothing parameters, namely, bn,s
and hs (s = 1 and 2) for calculating Tn1 ,n2 . According to Dette et al. (2006), when hs
22

is sufficiently small, it has a negligible impact to the test, and therefore, by considering
−1/3

bandwidth conditions (B), we recommend choosing hs = ns

as a rule of thumb. For bn,s ,

we propose to choose this tuning parameter by a corrected-Generalized Cross Validation (CGCV) method (see Craven and Wahba (1978)). Notice that for the local linear regression
with bandwidth b, the estimator can be written as Ŷs = Ds (b)Ys for some ns × ns matrix
Ds (b), and then the GCV selects
b̂n,s,mean = arg min GCV (b), where GCV (b) =
b

>
n−1
s (Ŷs (b) − Ys ) (Ŷs (b) − Ys )
.
(1 − trace(Ds (b)/ns ))2

(B.1)

Following the arguments of Yu and Jones (1998), it is appropriate to select b̂n,s by correcting
b̂n,s,mean . First, we define b̂on,s = 2Cs b̂n,s,mean with


1/5

R1

 trace(M1s (t))dt 


Cs =  0 1

R

trace(M̃s (t))dt

,

(B.2)

0
−1
> 1/2
2 > −1
, 1s is a ps -dimensional identity vector,
where M1s (t) = ((1>
s Σs (t))Vs (t) (1s Σs (t)) )
−1

−1

Vs (t) is the same as defined in (3.12) of the main article, M̃s (t) = Σ̃s (t)Λ̃s (t)Σ̃s (t) with
Λ̃s (t) =

∞
X

Cov(Hs (t, G0,s )L̃(t, F0,s , G0,s ), Hs (t, Gj,s )L̃(t, Fj,s , Gj,s )),

(B.3)

j=−∞

(L̃(t, Fj,s , Gj,s ), 1 ≤ j ≤ ns ) is the errors process of local linear regression for s-th sample
(s = 1 and 2), and Σ̃s (t) = E(Hs (t, G0 )Hs (t, G0 )> ). We refer to Zhou and Wu (2010) for
the estimation of M̃s (t). Then as recommended by Zhou (2010), for the SIT test, we use
−1/45

b̂n,s = b̂on,s × ns

while for the SCB test, we use b̂n,s = b̂on,s .

We now discuss the selection of wn,s and Ms for the estimation of the quantity M̂cs (t)
1/3

in Section 4 of the main article. As a rule of thumb, we propose to choose Ms = bns c
and select wn,s by minimum volatility method. Specifically, consider a grid of possibly
wn,s : {ws,1 , ..., ws,k }. Together with Ms and bn,s , one can calculate M̂cs,1 , ..., M̂cs,k using

23

{ws,1 , ..., ws,k }, respectively. Then, for a positive integer u (u = 5 say), define

ise(M̂cs ,u (l)) =

1
u−1

l+u−1
X

M̂cs,v (t) −

v=l

1
u−1

l+u−1
X

!2
M̂cs,v (t)

.

(B.4)

v=l

Now, let l0 be the minimiser of ise(M̂cs ,u (l)), and we select ws,l0 +bu/2c as wn,s . The validity
of these methods for choosing wn,s and Ms are given in Wu and Zhou (2017), which also
proposed methods of tuning parameters for refinement. For simplicity, we omit the detailed
description of the tuning procedure for refinement in our paper. Our empirical study finds
that our choices of tuning parameters bn,s , hs , Ms , wn,s and the estimate of Mcs (t) work
reasonably well.

C

Finite Sample Simulation Studies

This section studies the finite sample performance of the test based on Tn1 ,n2 , i.e., the
SIT test and the SCB test. The performance of the tests is carried out for n1 = n2 =
n = 50, 100, 200 and 500, the number of repetitions = 1000 and the number of Bootstrap
replication (i.e., B) = 500. In this study, we consider the Epanechnikov kernel (e.g., see
Silverman (1998)) unless mentioned otherwise, and the upper limit of the Riemann sum is
−1/3

the same as the sample size, i.e., N = n. Apart from these choices, hs = ns

is chosen,

and we choose bn,1 and bn,2 as described in Section B.
The covariate random variable and the error random variable are generated by the
following way. For s = 1 and 2,

xi,s = Hs




i
i
, Gi,s , ei,s = Ls
, Fi,s , Gi,s ,
n
n

where the notations are defined at the beginning of Section 3 of the main article. Here
ξi ’s and ηi ’s (involved in the expression of Hs and Ls ) are i.i.d. standard normal random
√
variables and t5 / (5/3) random variables, respectively, and t5 denotes the t-distribution
with 5 degrees of freedom.

24

Example 1: Let
y1,i

 
 
i
i
= θ1,τ,1
x1,i,1 + θ2,τ,1
x2,i,1 + (ei,1 − ei,τ,1 )
n
n

y2,i

 
 
i
i
= θ1,τ,2
x1,i,2 + θ2,τ,2
x2,i,2 + (ei,2 − ei,τ,2 ).
n
n

and

Here ei,τ,1 and ei,τ,2 are same as defined in (1.2) in the main manuscript. Suppose that
θ1,τ,1 (t) = t, θ2,τ,1 (t) = log t, and θ1,τ,2 (t) = t − 0.1, θ2,τ,2 (t) = log(t − 0.1). Further, consider
c1,1 = c2,1 = c1,2 = c2,2 = 1. In the numerical studies, we consider τ = 0.5, 0.7 and 0.8.
Example 2: Let
y1,i

 
 
 
i
i
i
= θ1,τ,1
x1,i,1 + θ2,τ,1
x2,i,1 + θ3,τ,1
x3,i,1 + (ei,1 − ei,τ,1 )
n
n
n

y2,i

 
 
 
i
i
i
= θ1,τ,2
x1,i,2 + θ2,τ,2
x2,i,2 + θ3,τ,2
x3,i,2 + (ei,2 − ei,τ,2 ).
n
n
n

and

Here ei,τ,1 and ei,τ,2 are the same as defined in (1.2) in the main manuscript. Suppose
that θ1,τ,1 = t2 , θ2,τ,1 = sin πt
, θ3,τ,1 = et , and θ1,τ,2 = (t − 0.1)2 , θ2,τ,2 = sin π(t−0.1)
and
2
2
θ3,τ,2 = et−0.1 . Further, consider c1,1 = c2,1 = c3,1 = c1,2 = c2,2 = c3,2 = 1, and here also,
τ = 0.5, 0.7 and 0.8 are considered in the numerical study.
Note that for both Examples 1 and 2, the choices of time varying coefficients (i.e,
θ(t)’s) satisfy the null hypothesis described in (1.3). Tables 1 and 2 show the rejection
probabilities of the SIT test and the SCB test for Examples 1 and 2, respectively when
the level of significance is 5% and 10%.
For power study, we consider the the same error and covariate processes and used the
following examples :
Example 3: Let
y1,i

 
 
i
i
= θ1,τ,1
x1,i,1 + θ2,τ,1
x2,i,1 + (ei,1 − ei,τ,1 )
n
n

25

model
n = 50 n = 100 n = 200 n = 500
Example 1 (α = 5% , τ = 0.5) 0.063
0.061
0.058
0.051
Example 1 (α = 10%, τ = 0.5) 0.119
0.116
0.110
0.103
Example 1 (α = 5%, τ = 0.7)
0.062
0.061
0.057
0.052
Example 1 (α = 10%, τ = 0.7) 0.118
0.117
0.113
0.104
Example 1 (α = 5%, τ = 0.8)
0.063
0.060
0.057
0.052
Example 1 (α = 10%, τ = 0.8) 0.115
0.114
0.109
0.106
Example 2 (α = 5%, τ = 0.5)
0.067
0.060
0.059
0.052
Example 2 (α = 10%, τ = 0.5) 0.122
0.120
0.114
0.107
Example 2 (α = 5%, τ = 0.7)
0.065
0.059
0.056
0.051
Example 2 (α = 10%, τ = 0.7) 0.120
0.119
0.111
0.104
Example 2 (α = 5%, τ = 0.8)
0.061
0.059
0.056
0.053
Example 2 (α = 10%, τ = 0.8) 0.117
0.115
0.109
0.105
Table 1: The estimated size of the SIT test for different sample sizes n1 = n2 = n. The
levels of significance (denoted as α) are 5% and 10%.
model
n = 50 n = 100 n = 200 n = 500
Example 1 (α = 5%, τ = 0.5)
0.067
0.063
0.057
0.053
Example 1 (α = 10%, τ = 0.5) 0.122
0.111
0.107
0.102
Example 1 (α = 5%, τ = 0.7)
0.064
0.060
0.056
0.051
Example 1 (α = 10%, τ = 0.7) 0.125
0.119
0.111
0.105
Example 1 (α = 5%, τ = 0.8)
0.062
0.061
0.055
0.050
Example 1 (α = 10%, τ = 0.8) 0.124
0.117
0.106
0.101
Example 2 (α = 5%, τ = 0.5)
0.065
0.062
0.057
0.049
Example 2 (α = 10%, τ = 0.5) 0.129
0.118
0.111
0.104
Example 2 (α = 5%, τ = 0.7)
0.067
0.063
0.054
0.050
Example 2 (α = 10%, τ = 0.7) 0.126
0.118
0.110
0.102
Example 2 (α = 5%, τ = 0.8)
0.065
0.057
0.053
0.050
Example 2 (α = 10%, τ = 0.8) 0.123
0.116
0.107
0.101
Table 2: The estimated size of the SCB test for different sample sizes n1 = n2 = n. The
levels of significance (denoted as α) are 5% and 10%.
and
y2,i

 
 
i
i
= θ1,τ,2
x1,i,2 + θ2,τ,2
x2,i,2 + (ei − ei,τ,2 ).
n
n

Here ei,τ,1 and ei,τ,2 are the same as defined in (1.2) in the main manuscript. Suppose
that θ1,τ,1 (t) = t, θ2,τ,1 (t) = log t, and θ1,τ,2 (t) = t2 , θ2,τ,2 (t) = (log t)2 . Further, consider
26

c1,1 = c2,1 = c1,2 = c2,2 = 1. In the numerical studies, we consider τ = 0.5, 0.7 and 0.8.
Example 4: Let
y1,i

 
 
 
i
i
i
= θ1,τ,1
x1,i,1 + θ2,τ,1
x2,i,1 + θ3,τ,1
x3,i,1 + (ei,1 − ei,τ,1 )
n
n
n

y2,i

 
 
 
i
i
i
= θ1,τ,2
x1,i,2 + θ2,τ,2
x2,i,2 + θ3,τ,2
x3,i,2 + (ei,2 − ei,τ,2 ).
n
n
n

and

Here ei,τ,1 and ei,τ,2 are the same as defined in (1.2) in the main manuscript. Suppose
that θ1,τ,1 = t2 , θ2,τ,1 = sin πt
, θ3,τ,1 = et , and θ1,τ,2 = t3 , θ2,τ,2 = cos πt
and θ3,τ,2 = log t.
2
2
Further, consider c1,1 = c2,1 = c3,1 = c1,2 = c2,2 = c3,2 = 1, and here also, τ = 0.5, 0.7 and
0.8 are considered in the numerical study.
Note that in Examples 3 and 4, the choices of the time varying regression coefficients do
not satisfy the assertion of null hypothesis described in (1.3). Table 4 shows the rejection
probabilities of the test based on Tn1 ,n2 when data follow the models described in Examples
3 and 4.
model
n = 50 n = 100 n = 200 n = 500
Example 3 (α = 5%, τ = 0.5)
0.367
0.401
0.546
0.777
Example 3 (α = 10%, τ = 0.5) 0.445
0.517
0.699
0.901
Example 3 (α = 5%, τ = 0.7)
0.422
0.499
0.627
0.818
Example 3 (α = 10%, τ = 0.7) 0.497
0.563
0.776
0.923
Example 3 (α = 5%, τ = 0.8)
0.378
0.444
0.622
0.816
Example 3 (α = 10%, τ = 0.8) 0.412
0.535
0.701
0.888
Example 4 (α = 5%, τ = 0.5)
0.422
0.477
0.661
0.825
Example 4 (α = 10%, τ = 0.5) 0.447
0.500
0.708
0.917
Example 4 (α = 5%, τ = 0.7)
0.475
0.503
0.688
0.848
Example 4 (α = 10%, τ = 0.7) 0.510
0.582
0.727
0.949
Example 4 (α = 5%, τ = 0.8)
0.398
0.419
0.589
0.801
Example 4 (α = 10%, τ = 0.8) 0.419
0.475
0.623
0.878
Table 3: The estimated power of the test of the test based on Tn1 ,n2 , i.e., the SIT test for
different sample sizes n1 = n2 = n. The levels of significance (denoted as α) are 5% and
10%.
It follows from the results of Examples 1 and 2 that the test based on Tn1 ,n2 , i.e., the
27

model
n = 50 n = 100 n = 200 n = 500
Example 3 (α = 5%, τ = 0.5)
0.343
0.376
0.519
0.743
Example 3 (α = 10%, τ = 0.5) 0.421
0.487
0.665
0.874
Example 3 (α = 5%, τ = 0.7)
0.395
0.470
0.592
0.789
Example 3 (α = 10%, τ = 0.7) 0.466
0.525
0.739
0.888
Example 3 (α = 5%, τ = 0.8)
0.344
0.417
0.589
0.786
Example 3 (α = 10%, τ = 0.8) 0.387
0.509
0.668
0.849
Example 4 (α = 5%, τ = 0.5)
0.434
0.496
0.675
0.842
Example 4 (α = 10%, τ = 0.5) 0.472
0.521
0.735
0.946
Example 4 (α = 5%, τ = 0.7)
0.499
0.530
0.723
0.880
Example 4 (α = 10%, τ = 0.7) 0.538
0.611
0.752
0.981
Example 4 (α = 5%, τ = 0.8)
0.421
0.443
0.614
0.824
Example 4 (α = 10%, τ = 0.8) 0.445
0.498
0.651
0.901
Table 4: The estimated power of the SCB test for different sample sizes n1 = n2 = n. The
levels of significance (denoted as α) are 5% and 10%.
SIT test and the SCB test can achieve the nominal level of significance when τ = 0.5,
0.7 and 0.8. In terms of estimated power, the results of Examples 3 and 4 indicate that
the SIT and the SCB tests can achieve the maximum power as the sample size increases.
Precisely speaking, for Example 3, the SIT test is marginally more powerful than the SCB
test whereas for Example 4, the SCB test is faintly more powerful than the SIT test.
We also observe the same phenomena for unequal n1 and n2 but for the sake of concise
presentation, we have not here reported the values of the estimated size and power.

D

Average Temperature Anomaly

This data set consists of four variables, namely, average temperature anomaly, the carbon
emission in the form of gas, solid and liquid. We here consider two regions, namely, the
northern hemisphere and the southern hemisphere since the feature of average temperature anomaly and the carbon emission in the form of gas, solid and liquid are different in
two hemispheres, and they are monotonically increasing over time which causes interest of
study in climate science (see, e.g., Raupach et al. (2014)). The data set for these two regions of the aforementioned four variables are available in https://ourworldindata.org/
28

co2-and-other-greenhouse-gas-emissions and https://cdiac.ess-dive.lbl.gov/
trends/emis/glo_2014.html. These yearly data sets reported the values of the variables
for the period from 1850 to 2018, i.e., n = 169. In this study, the average temperature
anomaly is considered as the response variable (denoted as y), and the carbon emission in
the form of gas (denoted as x1 ), solid (denoted as x2 ) and liquid (denoted as x3 ) are the
covariates.
Here also, we would like to discuss a few more observations on this data : The diagrams
in Figure 4 indicate that for both northern and southern hemispheres, y, x1 , x2 and x3 increase over time, which is well-known feature in climate science. Moreover, we observe from
Figure 5 that the fitted quantile coefficient curves associated with x1 , x2 and x3 are monotonically increasing over time for a given quantile (in Figure 5, τ = 0.5). We now investigate
the performance of the test based on Tn1 ,n2 (here n1 = n2 = n = 169) to check whether this
data favours H0 (see (1.3)) or not when c = (c1,1 , c2,1 , c3,1 , c1,2 , c2,2 , c3,2 ) = (1, 0, 0, 1, 0, 0)
and (0, 0, 1, 0, 0, 1), and the test is carried out following the procedure described in Section
4.2. For B = 1000, we here also computed the p-values for τ at 5% level of significance.
For τ = 0.5 and c = (1, 0, 0, 1, 0, 0), the p-values of the SIT test and the SCB test are 0.347
and 0.299, respectively. Next, when c = (0, 0, 1, 0, 0, 1) and τ = 0.5, the p-values of the SIT
test and the SCB test are 0.574 and 0.513, respectively. These p-values of both the tests
SIT and SCB indicate that this data set favours the null hypothesis for c = (1, 0, 0, 1, 0, 0)
and = (0, 0, 1, 0, 0, 1) when τ = 0.5, which is consistent with the nature of the curves drawn
on the left and the right diagrams of Figure 5.
However, for τ = 0.5 and c = (c1,1 , c2,1 , c3,1 , c1,2 , c2,2 , c3,2 ) = (0, 1, 0, 0, 1, 0), the p-values
of the SIT test and the SCB tests are 0.087 and 0.081, respectively, which indicate a
rejection of the null hypothesis at 9% level of significance. These small p-values obtained
in the last case is consistent with the feature of the curves illustrated in the middle diagram
in Figure 5. Specifically for τ = 0.5, the case c = (0, 1, 0, 0, 1, 0) corresponds to the quantile
coefficient curve presented in the middle diagram of Figure 5, which clearly indicates that
there is no any constant shift between the quantile coefficient curves for the northern and
the southern hemispheres. This fact leads to relatively small p-values.

29

Temp Anomaly in both hemisphares

500

1000

Metric Tons

0.5
0.0

0

-0.5

Temp. Anomaly

1500

1.0

Carbon (gas) emission in both hemisphares

1850

1900

1950

1850

2000

1900

1950

2000

year

Carbon (liquid) emission in both hemisphares

Carbon (solid) emission in both hemisphares

3000
1000

2000

Metric Tons

1500

0

0

500

Metric Tons

2500

4000

year

1850

1900

1950

2000

1850

year

1900

1950

2000

year

Figure 4: Plots of temperature anomaly, carbon (gas, liquid and solid) emission in northern and southern hemispheres. In each diagram, the line curve represents for the northern
hemisphere, and the dotted curve represents for the southern hemisphere.

30

15

1850

1900

1950
year

2000

10

Fitted quantile

0

0

0

5

5

5

10

Fitted quantile

15

20
15
10

Fitted quantile

Quantile coeifficent curve,τ=0.5

20

Quantile coeifficent curve,τ=0.5

25

Quantile coeifficent curve,τ=0.5

1850

1900

1950
year

2000

1850

1900

1950

2000

year

Figure 5: The plots of fitted quantile coefficients curves associated with x1 (left diagram),
x2 (middle diagram) and x3 (right diagram). Here τ denotes the index of the quantile. In
each diagram, the line curve represents for the northern hemisphere, and the dotted curve
represents for the southern hemisphere.

E

Proofs

Sketch of the proofs. The properties of the estimators â and b̂ follow from the stochastic
expansion of the deviation of the local linear quantile estimator m̂s (t) − ms (t), (cf. Porposition 1 of Wu and Zhou (2017)) as well as the monotonicity of functions µs , s = 1, 2.
The proof of Theorem 3.1 have two steps. The first step is to expand the deviation of
the test statistics under null and local alternatives, and approximate it by some Gaussian
process using the extended argument of proof of Theorem 4.1 of Dette and Wu (2019).
Notice that in our case, we consider two samples as well as the local linear quantile regression. The second step is to using Theorem 2.1 of de Jong (1987) to figure out the
asymptotic behavior of a quadratic Gaussian integrals via tedious calculations. Based on
step 1 of the proof of Theorem 3.1, we further show Theorem 3.2 using the approximation
formula in Proposition 1 of Sun and Loader (1994) to obtain the simultaneous confidence
band. Intensive calculations are provided in our proof to determine the parameters in the
approximation formula of Sun and Loader (1994).

31

E.1

Some propositions

Proposition E.1 Assume conditions (A1)-(A8) and (B3). Then on a possibly richer
probability space, there exists i.i.d. sequence of standard normal random variables (Vi,1 )i∈Z
and (Vi,2 )i∈Z , such that for s = 1 and 2,
sup m̂s (t) − ms (t) −

Mcs (t)

Pns

i=1

t∈Tn,s

where Θn,s =

1/4
ns
+ √(nπsn,s
,
ns bn,s
bn,s )

Vi,s K̄bn,s (i/ns − t)
= Op (Θn,s ),
nbn,s

(E.2)

√
and πn,s = bn,s log6 ns +(ns bn,s )−1/4 log3 ns +b3n,s (ns bn,s ) log3 ns .

Proof. The proposition follows immediately from Proposition 1 of Wu and Zhou (2017).

The following proposition provides the convergence rate of â and b̂ under the null and
the local alternatives.
Proposition E.2 Under the conditions of Proposition E.1, assuming (B1), (B2) and η =
0

0

−1
o(1), η −1 = O(log(n1 + n2 )). Then if (m−1
1 ) (t) − (m2 ) (t) = ρn1 ,n2 κ(t) for some non-zero

bounded function κ(t) and ρn1 ,n2 = o(η), we have that
i) max(|â − m1 (0)|, |b̂ − m1 (1 − m−1
2 (m1 (0)))|) = Op

2
X
s=1

√

(log ns )
√
+ ρn1 ,n2
(ns , bn,s )bn,s

!
.
(E.3)

Notice that under null, the LHS of (E.3) will be reduced to max(|â − a|, |b̂ − b|). Moreover,
ii)

lim

n1 →∞,n2 →∞

P(m−1
s (t) ∈ (bn , 1 − bn ), s = 1, 2, for all t ∈ (â + c1 η, b̂ − c2 η)) = 1. (E.4)

for any given positive constant c1 , c2 > 0.
Remark E.1 Note that i) in Proposition E.2 shows that ŵ(t) in (2.9) is consistent under
null hypothesis and local alternatives. Further, observe that ii) in Proposition E.2 shows
that by introducing η, we avoid bandwidth conditions since ii) excludes regions where ms (t)
(s = 1 and 2) are close to 0 and 1.
32

Proof of Proposition 2.1: We extend the proof of Lemma 2.1 in Dette et al. (2019).
We shall show (1.3) is equivalent to
0
(m2−1 (u) − m−1
1 (u)) = 0

(E.5)

for m1 (0) < u < m1 (1 − d).
If m1 (t) = m2 (t + d) for 0 < t < 1 − d for some unknown d, and m1 (t) and m2 (t) are
monotonically increasing for 0 < t < 1 − d, one then can write u = m1 (t) = m2 (t + d) for
m1 (0) < u < m1 (1 − d), which implies that t = m−1 (u) and t + d = m−1
2 (u). Therefore,
−1
(m−1
2 (u) − m1 (u)) = d

for m1 (0) < u < m1 (1 − d). Since d is a constant, we have proven that (1.3) implies (E.5).
On the other hand, by (E.5), one can see that for any t ∈ (m1 (0), m1 (1 − d)),
Zt
m1 (0)

Zt

0
(m−1
1 (u)) du =

0
(m−1
2 (u)) du.

(E.6)

m1 (0)

−1
−1
−1
As a result, we have m−1
1 (t) = m2 (t) − m2 (m1 (0)), and d = m2 (m1 (0)). Therefore, by

rearranging the equation and taking m02 on both sides of it, one can conclude that
−1
−1
t = m2 (m−1
1 (t) + m2 (m1 (0))) = m1 (m1 (t))
−1
for 0 < m−1
1 (t) < 1 − d, which finishes the proof by setting u = m1 (t).

(E.7)


√
Proposition E.3 Along with the conditions of Proposition E.1, assume that πn,s = o( log ns ),
ns b2n,s log2 ns → ∞ for s = 1 and 2. Then, we have that for s = 1 and 2,

√
log ns
.
sup |m̂s (t) − ms (t)| = Op √
ns bn,s
t∈Tn,s

33

(E.8)

Proof. It follows from Lemma 1 of Zhou and Wu (2010) that
Pns

i=1

sup
t∈Tn,s

Vi,s K̄bn,s (i/ns − t)
= Op
ns bn,s


√
log n
√
.
n s bs

Then the assertion of this proposition follows from Proposition E.1.

(E.9)


Proof of Proposition E.2: Notice that (m2 )−1 (t) − (m01 )−1 (t) = ρn1 ,n2 κ(t) implies that
Zu

−1
−1
m−1
2 (u) − m1 (u) = m2 (m1 (0)) +

ρn1 ,n2 κ(t)dt.

(E.10)

m1 (0)

Proof of assertion i): By proposition E.3 and (E.10), it is sufficient to show that for s = 1
and 2,

sup |ĝs (u) − gs (u)| = Op
u∈[a,b]

where gs (t) =

1
N hs

N
P

H



m̂s ( Ni )−t
hs


√
log ns
√
,
(ns bn,s )bn,s

(E.11)



. After carefully inspecting the proof of Theorem 3.1, one

can find that supt∈[a,b] |ĝs (t) − gs (t)| = Op supt∈[a,b] Zs (t) , where Zs (t) is defined in (E.28).
i=1

Then the proposition follows from (E.48) in the proof
3.1. The assertion in (ii)
 of Theorem

follows from condition (B2), (E.10) (with u = m̂1 1 − dˆ especially), strict monotonicity
of ms s = 1 and 2, the mean value theorem and the fact that bn1 = o(η), bn2 = o(η) and
ρn1 ,n2 = o(η), which can be explained as follows. Note that using (i), with probability
tending to 1, we have
m1 (0) < â1 + c1 η.

(E.12)

Now, since bn1 = o(η), and m−1
1 is differentiable, using mean value theorem with probability
tending to 1, we have
m1 (bn1 ) < â + c1 η,

(E.13)

and hence, lim P(bn1 < m−1
1 (â + c1 η)) = 1. Arguing in a similar way, one can establish
n→∞

34

−1
that lim P(1 − bn1 > m−1
1 (b̂ − c2 η)) = 1. Then (ii) holds since the function m (·) is
n→∞

monotone.



Proof of Theorem 3.1: Write
Z
T̃n1 ,n2 =

(ĝ1 (t) − ĝ2 (t))2 w(t)dt, where w(t) = 1(a + η ≤ t ≤ b − η).

(E.14)

R

In the following, we shall prove that, under conditions of Theorem 3.1,
5/2
n1 bn,1 T̃n1 ,n2

−

−1/2
bn,1 (B̌1

+

γ0 γ13 B̌2 ))

Z
−

κ2 (t)w(t)dt ⇒ N (0, VT ),

(E.15)

R
5/2
n1 bn,1 (T̃n1 ,n2

− Tn1,n2 ) = op (1).

(E.16)

Proof of (E.15):

Define gs (t) =

1
N hs

N
P
i=1

H



ms ( Ni )−t
hs



. Then we have the following decomposition:
Z

(J1 (t) − J2 (t) + J3 (t))2 w(t)dt,

(E.17)

Js (t) = ĝs (t) − gs (t), s = 1 and 2, J3 (t) = g1 (t) − g2 (t).

(E.18)

T̃n1 ,n2 =
R

Using the similar argument of page 471 of Dette et al. (2006), we have
J3 (t) =

0
(m−1
1 ) (t)



1
1
−
+ O h1 + h2 +
+
N h1 N h2


1
1
= ρn κ(t) + O h1 + h2 +
+
.
N h1 N h2
0
(m−1
2 ) (t)

(E.19)

Next, by Taylor series expansion, we have that for s = 1 and 2, the following decomposition
holds:
Js (t) = Js,1 (t) + Js,2 (t),
35

(E.20)

where


N
1 X 0 ms (i/N ) − t
H
(m̂s (i/N ) − ms (i/N )),
Js,1 (t) =
N h2s i=1
hs


N
1 X 00 ms (i/N ) − t + νs∗ (m̂s (i/N ) − ms (i/N ))
Js,2 (t) =
(m̂s (i/N ) − ms (i/N ))2
H
2N h3s i=1
hs
4/3

ns
for some νs∗ ∈ [−1, 1] (s = 1 and 2). Notice that √log
= o(1), and thus the number
n,s )hs
 4/3(ns b
log
n
of non-zero summands in Js,2 (t) is of order O √(ns bn,ss) with probability 1. Using Propo-

sitions E.1 and E.3, with the same arguments in the proof of Theorem 4.1 in the online
supplement of Dette and Wu (2019) for obtaining bound for ∆2,N in their paper, we obtain
that for s = 1 and 2, and uniformly for t ∈ [a + η, b − η],

Js,2 (t) = Op

log4 ns
(ns bn,s )3/2 h2s


.

(E.21)

Therefore, by applying the assertion of Proposition E.1 to Js,1 (t), equations (E.19)–(E.21),
on a possibly richer probability space, there exists a sequence of i.i.d. standard normal
random variables (Vs,i )i∈Z , s = 1 and 2 such that T̃n1 ,n2 can be written as
Z
T̃n1 ,n2 =

0
−1 0
2
[(m−1
1 ) (t) − (m2 ) (t) + Z(t) + R(t)] w(t)dt

R

Z
=

[(ρn κ(t) + Z(t) + R(t)]2 w(t)dt,

(E.22)

R

where


2
N ns
X
(−1)s−1 X X
ms (i/N ) − t
0
Mcs (i/N )H
K̄bn,s (j/ns − i/N )Vj,s , (E.23)
Z(t) =
2
n
b
N
h
h
s
n,s
s
s
s=1
i=1 j=1
and

sup

|R(t)| = Op (Ωn ).

(E.24)

t∈[a+η,b−η]

36

As
5/2
n1 bn,1 ρ2n

Z

Z

2

κ2 (t)w(t)dt,

κ (t)w(t)dt =

(E.25)

R

R

to prove (E.15), it is sufficient to show that
5/2
n1 bn,1

Z

2

Z (t)w(t)dt −

−1/2
bn,1 (B̌1

+

γ0 γ13 B̌2 ))

Z
−

R

κ2 (t)w(t)dt ⇒ N (0, VT ),

R


5/2

(E.26)


Z

n1 bn,1 

Z
R(t)(R(t) + ρn κ(t) + Z(t))w(t)dt +

R

ρn κ(t)Z(t)w(t)dt = op (1).

(E.27)

R

Proof of (E.26):
We decompose Z(t) by Z(t) := Z1 (t) − Z2 (t). Here for s = 1 and 2, we have
Zs (t) =

ns
X

Ws (ms , j, t)Vj,s , , where

(E.28)

j=1

Ws (ms , j, t) =

1

N
X

ns bn,s N h2s

i=1

Mcs (i/N )H

0



ms (i/N ) − t
hs


K̄bn,s (j/ns − i/N ).

(E.29)

As a result, we have
Z

(Z1 (t) − Z2 (t))2 w(t)dt = A1 + A2 − 2A12 , where

(E.30)

R

Z
As =
R

Z
A12 =
R

n1
X

ns
X

!2
Ws (ms , j, t)Vj,s

w(t)dt, s = 1 and 2,

(E.31)

j=1

!
W1 (m1 , j, t)Vj,1

j=1

n1
X

!
W2 (m2 , j, t)Vj,2 w(t)dt.

(E.32)

j=1

We first prove the results for A1 , and the result for A2 can be evaluated in a similar

37

way. Notice that A1 = A1,a + A1,b , where

A1,a



Z
n1
X
2
 W12 (m1 , j, t)w(t)dt Vj,1
,
=
j=1

R




Z

X

A1,b = 2

(E.33)

W1 (m1 , j1 , t)W1 (m1 , j2 , t)w(t)dt Vj,1 Vj,2 .


1≤j1 <j2 ≤n1

(E.34)

R

Now, using the bandwidth condition hs = o(bn,s ), calculating the Riemann sum with
widths of summands 1/N and change of variable y = (ms (u) − t)/hs , a few steps algebraic
calculations show that (note that we use the similar calculation in Proposition E.2)
Ws (ms , j, t) =

1

Z

ns bn,s hs

0
Mcs (m−1
s (t + hs y))H (y)×

R
−1 0
K̄bn,s (j/ns − m−1
s (t + hs y))(ms ) (t + hs y)dy + O(R(j, s, n, t)),

(E.35)

where
R(j, s, n, t) =



1
ns bn,s h2s N

1


j/ns − m−1
s (t)
≤1
bn,s + M hs

for a sufficiently large constant M . Since H is chosen to be symmetric, we have

(E.36)
R

H 0 (x)dx =

R

0. Therefore, by Taylor series expansion, for t with w(t) 6= 0, it follows that for s = 1 and
s
+ N1hs ))),
2, the leading term of Ws (ms , j, t) can be written as W̃s (ms , j, t)(1 + O(bn,s + bhn,s

where


Z
−1
j/ns − m−1
s (t)
−1 0
2
−1
0
W̃s (ms , j, t) =
Mcs (ms (t))K̄
((ms ) (t))
H 0 (y)ydy
2
ns bn,s
bn,s
R


−1
−1
j/ns − ms (t)
=
ǧs (t)K̄ 0
.
(E.37)
2
ns bn,s
bn,s
Next, by using (E.37), we have (note that we use the similar calculation in Proposition

38

E.2.)
E(A1,a ) =

n1 Z Z
X
j=1

W̃12 (m1 , j, t)w(t)dt(1 + o(1))

R

2


Z

Z
Z
1 
0
−1
4
2

H (y)dy
=
K̄ (x)dx Mc21 (m−1
s (t))(ms (t)) w(t)dt(1 + o(1))
n1 b3n,1
R
R
R
2

Z
Z
Z
1 
0
2
H (y)dy 
=
K̄ (x)dx Mc21 (u)u4 m01 (u)w(m1 (u))du(1 + o(1))
n1 b3n,1
R

R

R

1
=
B̌1 (1 + o(1)).
n1 b3n,1

(E.38)

On the other hand, the similar calculations show that
2

 2 


Z
n1
X
n1 bn,1
1
2
 W1 (m1 , j, t)w(t)dt = O
Var(A1,a ) =
=O
.
n41 b8n,1
n31 b6n,1
j=1

(E.39)

R

Then for A1,b , we have that

Var(A1,b ) =

ns
X

ns
X

j1 =1 j2 =1,j2 6=j1

=

ns
X

ns
X

j1 =1 j2 =1,j2 6=j1

=

1
n2s b8n,s


2
Z
 W1 (m1 , j1 , t)W1 (m1 , j2 , t)w(t)dt
R


2
Z
 W̃1 (m1 , j1 , t)W̃1 (m1 , j2 , t)w(t)dt (1 + o(1))
R


2

 

Z Z Z
−1
−1
 ǧ12 (t)K̄ 0 u − m1 (t) K̄ 0 v − m1 (t) w(t)dt dudv
bn,1
bn,1
R

R

R

× (1 + o(1)).
Now, by changing variable, i.e., letting

(E.40)
u−m−1
1 (t)
bn,1

39

= x, and using the fact that g̃1 (m1 (u)) =

ǧ12 (m1 (u))w(m1 (u))m01 (u), we have
Var(A1,b )
2


 ǧ12 (m1 (u))K̄ 0 (x)K̄ 0 x + v − u w(m(u))m0 (u)dx dudv(1 + o(1))
bn,1
R R
R

2


Z Z
Z
v−u
g̃12 (m1 (u))  K̄ 0 (x)K̄ 0 x +
dx dudv(1 + o(1))
bn,1
R R
R
Z
Z
1
g̃12 (m1 (u))(K̄ 0 ? K̄ 0 (y))2 dudy(1 + o(1)) = 2 5 V̌1 (1 + o(1)).
(E.41)
n1 bn,1


=

=
=

Z Z

1
n21 b6n,1
1
n21 b6n,1
1
n21 b5n,1

R

Z

R

Combining (E.38) (E.39) and (E.41), it follows that
1
B̌1 (1 + o(1)),
n1 b3n,1


1
Var(A1,a ) = O
,
n31 b6n,1
1
Var(A1,b ) = 2 5 V̌1 (1 + o(1)).
n1 bn,1
E(A1,a ) =

(E.42)
(E.43)

Similarly,
1
E(A2,a ) =
B̌2 (1 + o(1)), Var(A2,a ) = O
n2 b3n,2



1
n32 b6n,2


, Var(A2,b ) =

1
V̌2 (1 + o(1)).
n22 b5n,2
(E.44)

On the other hand, for A12 , we have

2
Z
n1 X
n2
X
 W1 (m1 , j1 , t)W2 (m2 , j2 , t)w(t)dt
Var(A12 ) =
j1 =1 j2 =1

R

1
=
n1 n2 b4n,1 b4n,2

Z Z
R

R


2

 

Z
−1
−1
u
−
m
(t)
v
−
m
(t)
1
2
 ǧ1 (t)ǧ2 (t)K̄ 0
K̄ 0
w(t)dt dudv(1 + o(1)).
bn,1
bn,2
R

(E.45)

40

Now, by change of variable using x = (u − m−1
1 (t))/bn,1 , we have that

2

 

Z
−1
−1
 ǧ1 (t)ǧ2 (t)K̄ 0 u − m1 (t) K̄ 0 v − m2 (t) w(t)dt dudv
bn,1
bn,2
R
R R

2
 0

Z
Z Z
m21 (u)bn,1 x + v − m21 (u)
2
(m1 (u))  K̄ 0 (x)K̄ 0
dx dudv(1 + o(1))
= b2n,1
g̃1,2
bn,2
R
R R

2
 0

Z Z
Z
m21 (u)bn,1 x
2
g̃1,2
(m1 (u))  K̄ 0 (x)K̄ 0
= b2n,1 bn,2
+ y dx dudy(1 + o(1)). (E.46)
bn,2

Z Z

R

R

R

Therefore, we have that
Var(A12 ) =

V̌12 (γ1 )
(1 + o(1))
n1 n2 b2n,1 b3n,2

(E.47)

Notice that A1,b , A2,b , A12 are mutually uncorrelated. Using this fact, bandwidth conditions
and (E.30), (E.42), (E.44) and (E.47), equation (E.26) follows from Theorem 2.1 of de Jong
(1987) and tedious calculations.
Proof of (E.27): By (E.28), (E.29), (E.35)-(E.37), we obtain for s = 1, 2
√

log ns
sup |Zs (t)| = √
,
(ns bn,s )bn,s
t∈[0,1]

(E.48)

which together with bandwidth conditions yields (E.27). Hence (E.15) holds.
Proof of Theorem 3.1 (conclusion part): By the proof of (E.15) and the bandwidth
conditions, it suffices to show that
5/2
n1 bn,1

Z

(Z(t))2 (ŵ(t) − w(t))dt = op (1).

(E.49)

R

Meanwhile, assertion in (E.49) follows from (E.48), Proposition E.2 and bandwidth conditions. Now, by (E.15) and (E.16), Theorem 3.1 follows.
Proof of Theorem 3.2:

41



We prove the theorem in two steps.
Step 1: Recall Z(t) defined in (E.23). We first evaluate E(Z 2 (t)), E(Z 02 (t)) and
E(Z(t)Z 0 (t)). Since hs = o(bn,s ), uniformly for t ∈ [a + η, b − η], we have
E(Z 2 (t))
!2


ns
N
X
X
1
m
(i/N
)
−
t
s
=
K̄bn,s (j/ns − i/N )
Mcs (i/N )H 0
2 )2
(n
b
N
h
h
s
n,s
s
s
s=1
j=1
i=1



 2


Z
ns
2
X
X
1
 Mcs (u)H 0 ms (u) − t K̄bn,s (j/ns − u)du + O 1(|j/ns − t| ≤ M̄ bn,s ) 
=
2
2
(ns bn,s hs ) j=1
hs
N
s=1
R

2




Z
Z
2
X
1
m
(u)
−
t
1
1
s
0
 Mcs (u)H
K̄bn,s (v − u)du dv + O
+
=
2 )2
2 b2 h2
n
(b
h
h
n
ns N b2n,s h2s
s
n,s
s
s
s
n,s
s
s=1
R
R
2



Z Z
2
−1
2
X
Mcs (ms (t))
1
−1
0


=
H (x)K̄bn,s (v − ms (t + xhs ))dx dv + O
2
n (b h )2 (m0s (m−1
ns bn,s hs
s (t)))
s=1 s n,s s
R
R

2





Z
Z
2
−1 02
−1
2
X
Mcs (ms (t))(ms ) (t)
hs
1
02
0


K̄ (y)dy
H (x)xdx
1+O
.
=
+O
3 (m0 (m−1 (t)))2
n
b
b
n
b
h
s
n,s
s
n,s
s
n,s
s
s
s=1
2
X

R

R

42

Next, we have
E(Z 02 (t))
=

2
X
s=1

=

2
X
s=1

=

2
X

2
X
s=1

ns
X

1
(ns bn,s h3s )2

2
X

Mcs (i/N )H 00


Z


j=1

1

Mcs (u)H 00

Z

ns (bn,s h3s )2
R
 R

1
+O
n2s b2n,s h4s

Mcs (u)H

Mc2s (m−1
s (t))
2
ns (bn,s h2s )2 (m0s (m−1
s (t)))

Z

00

ms (u) − t
hs

K̄bn,s (j/ns − i/N )




K̄bn,s (j/ns − u)du 1 + O



1
N hs



2




ms (u) − t
hs



2




1

K̄bn,s (v − u)du dv 1 + O
N hs


2



Z
1
00
−1
 H (x)K̄bn,s (v − ms (t + xhs ))dx dv 1 + O
N hs

R

ns bn,s h3s



ms (i/N ) − t
hs

!2



R

Z

1



i=1



+ O(

=

j=1



s=1

=

1
(ns bn,s N h3s )2

ns
N
X
X

R

)

Mc2s (m−1
s (t))
4
0
2
n b h (ms (m−1
s (t)))
s=1 s n,s s

2


Z

2

Z

R

00

H (x)dx

K̄ (y)dy 
R

43






1
hs
1
1+O
+
+O
.
N hs bn,s
ns bn,s h3s

Finally, by the symmetry of K and H, we have
E(Z(t)Z 0 (t))
!


ns
N
X
X
1
m
(i/N
)
−
t
s
=
K̄bn,s (j/ns − i/N )
Mcs (i/N )H 0
2 )2 h
(n
b
N
h
h
s
n,s
s
s
s
s=1
j=1
i=1
!


N
X
ms (i/N ) − t
00
K̄bn,s (j/ns − i/N )
×
Mcs (i/N )H
hs
i=1






Z
ns
2
X
X
1
 Mcs (u)H 0 ms (u) − t K̄bn,s (j/ns − u)du + O 1(|j/ns − t| ≤ M̄ bn,s ) 
=
2
2
(ns bn,s hs ) hs j=1
hs
N
s=1
R






Z
m
(u)
−
t
1(|j/n
−
t|
≤
M̄
b
)
s
s
n,s
00
 Mcs (u)H

K̄bn,s (j/ns − u)du + O
hs
N
R




Z Z
2
X
1
 Mcs (u)H 0 ms (u) − t K̄bn,s (v − u)du
=
2
2
n (b h ) hs
hs
s=1 s n,s s
R
R






Z
1
1
 Mcs (u)H 00 ms (u) − t K̄bn,s (v − u)du dv + O
+
hs
n2s b2n,s h3s ns N bn,s h4s
R


Z Z
2
−1
2
X
Mcs (ms (t))
 H 0 (x)K̄bn,s (v − m−1

=
s (t + xhs ))dx
2 h m0 (m−1 (t))
n
(b
h
)
s
n,s
s
s
s
s
s=1
R
R




Z
1
 H 00 (x)K̄bn,s (v − m−1

dv + O
s (t + xhs ))dx
nbn,s h2s
R


hs + (hs /bn,s )2
=O
.
ns bn,s h3s
2
X

Step 2: We use Proposition 1 of Sun and Loader (1994) to evaluate the maximum deviation
of Z(t). For any two p-dimensional vectors u = (u1 , ...up )> and v = (v1 , ...., vp )> , write
p
P
< u, v >=
ui vi , and kukE =< u, u >. Define
i=1

Tj,s (t) = ws (ms , j, t), s = 1 and 2, 1 ≤ j ≤ ns ,

>
T(x) = T1,1 (x), ...., Tn1 ,1 (x), T1,2 (x), ..., Tn2 ,2 (x) ,

44

(E.50)
(E.51)

1 +n2
and V = (V1 , ....Vn1 , Vn1 +1 , ..., Vn1 +n2 )> , where {Vi }ni=1
is a i.i.d. sequence of standard

normal random variables. Then, for any 0 < a < b < 1 and η > 0, sup |Z(t)| has the
t∈[a,b]

same distribution as

| < T(t), V > |. Therefore, by Proposition 1 of Sun and

sup
t∈[a+η,b−η]

Loader (1994), we have that
!
lim P

c→∞

sup | < T(t), V > | ≥ c
t∈[a,b]

 2
c
κ0 (a + η, b − η)
exp −
+ 2(1 − φ(c)) + o(exp(−c2 /2)),
=
π
2
(E.52)

where κ0 (a + η, b − η) =

b−η
R
a+η

T(x) 
∂
∂x kT(x)kE

dx. Notice that
E

∂T(x)
∂
=
Z(x) ,
∂x
∂x
E
∂
< T(x), ∂x
T(x) >
∂
E(Z(t)Z 0 (t))
kT(x)kE =
, and
=
∂x
kT(x)kE
kZ(t)k


T(x)
∂
− ∂x
kT(x)kE T(x)
∂
T(x)
kT(x)kE
=
.
∂x kT(x)kE
kT (x)k2E
E
kT(x)kE = kZ(x)k,

(E.53)
(E.54)
(E.55)

E

By using (E.53)–(E.55) and the results of E(Z 2 (t)), E(Z 02 (t)) and E(Z(t)Z 0 (t)) from Step
1, we have that
∂
∂x



T(x)
kT(x)kE


=
E

bn,1 √ K2 (t)
(1 + o(1)).
h21 K1 (t)

(E.56)

Furthermore, by(E.3), we have


lim P max(|â − m1 (0)|, |b̂ − m1 (1 −

n1 →∞

45

m−1
2 (m1 (0)))|)


≤ η/2 = 1.

(E.57)

Therefore,
!
P

| < T(t), V > | ≥ c

sup
t∈[m̂1 (0)+3η/2,m̂1 (1−m−1
2 (m1 (0)))−3η/2]

!
≤P

| < T(t), V > | ≥ c

sup
t∈[â+η,b̂−η]

!
≤P

| < T(t), V > | ≥ c .

sup

(E.58)

t∈[m̂1 (0)+η/2,m̂1 (1−m−1
2 (m1 (0)))−η/2]

As a consequence, by (E.56), (E.52) and (E.58), and the fact that η = o(1), we have that

lim

n1 →∞


=

bn,1
h21

bn,1
h21

−1

!

−1
lim P

c→∞

sup

| < T(t), V > | ≥ c

t∈[â+η,b̂−η]

 2
c
κ0 (m1 (0), m1 (1 − m−1
2 (m1 (0)))
exp −
,
π
2

which completes the proof by solving

κ0 (m1 (0),m1 (1−m−1
2 (m1 (0)))
π

 2
exp − c2 = α.

(E.59)



References
Chaudhuri, P. (1991). Nonparametric estimates of regression quantiles and their local
bahadur representation. The Annals of Statistics, 19(2):760–777.
Collier, O. and Dalalyan, A. S. (2015). Curve registration by nonparametric goodness-of-fit
testing. Journal of Statistical Planning and Inference, 162:20–42.
Craven, P. and Wahba, G. (1978). Smoothing noisy data with spline functions. Numerische
mathematik, 31(4):377–403.
Dahlhaus, R. et al. (1997). Fitting time series models to nonstationary processes. The
Annals of Statistics, 25(1):1–37.
Dahlhaus, R., Richter, S., Wu, W. B., et al. (2019). Towards a general theory for nonlinear
locally stationary processes. Bernoulli, 25(2):1013–1044.

46

de Jong, P. (1987). A central limit theorem for generalized quadratic forms. Probability
Theory and Related Fields, 75(2):261–277.
Dette, H., Dhar, S. S., and Wu, W. (2019). Identifying shifts between two regression
curves. arXiv preprint arXiv:1908.04328.
Dette, H., Neumeyer, N., Pilz, K. F., et al. (2006). A simple nonparametric estimator of
a strictly monotone regression function. Bernoulli, 12(3):469–490.
Dette, H. and Volgushev, S. (2008). Non-crossing non-parametric estimates of quantile
curves. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
70(3):609–627.
Dette, H. and Wu, W. (2019). Detecting relevant changes in the mean of nonstationary
processes—a mass excess approach. The Annals of Statistics, 47(6):3578–3608.
Dette, H. and Wu, W. (2020). Prediction in locally stationary time series. arXiv preprint
arXiv:2001.00419, to appear Journal of Business & Economic Statistics.
Gamboa, F., Loubes, J., and Maza, E. (2007). Semi-parametric estimation of shifts.
Electronic Journal of Statistics, 1:616–640.
He, X. and Zhu, L. (2003). A lack-of-fit test for quantile regression. Journal of the American
Statistical Association, 98:1013–1022.
Horowitz, J. L, S. V. G. (2002). An adaptive, rate-optimal test of linearity for median
regression models. Journal of the American Statistical Association, 97:822–835.
Kim, M.-O. (2007). Quantile regression with varying coefficients. The Annals of Statistics,
pages 92–108.
Munk, A. and Dette, H. (1998). Nonparametric comparison of several regression functions:
exact and asymptotic theory. The Annals of Statistics, 26(6):2339–2368.
Qu, Z. (2008). Testing for structural change in regression quantiles. Journal of Econometrics, 146(1):170–184.

47

Qu, Z. and Yoon, J. (2015). Nonparametric estimation and inference on conditional quantile processes. Journal of Econometrics, 185(1):1–19.
Raupach, M. R., Davis, S. J., Peters, G. P., Andrew, R. M., Canadell, J. G., Ciais, P.,
Friedlingstein, P., Jotzo, F., van Vuuren, D. P., and Le Quéré, C. (2014). Sharing a
quota on cumulative carbon emissions. Nature Climate Change, 4:873–879.
Schucany, W. and Sommers, J. P. (1977). Improvement of kernel type density estimators.
Journal of the American Statistical Association, 72(358):420–423.
Sun, J. and Loader, C. R. (1994). Simultaneous confidence bands for linear regression and
smoothing. The Annals of Statistics, 22(3):1328–1345.
Vimond, M. (2010). Efficient estimation for a subclass of shape invariant models. The
Annals of Statistics, 38:1885–1912.
Wu, W. and Zhou, Z. (2017). Nonparametric inference for time-varying coefficient quantile
regression. Journal of Business & Economic Statistics, 35(1):98–109.
Wu, W. and Zhou, Z. (2018a). Gradient-based structural change detection for nonstationary time series m-estimation. The Annals of Statistics, 46(3):1197–1224.
Wu, W. and Zhou, Z. (2018b). Simultaneous quantile inference for non-stationary longmemory time series. Bernoulli, 24(4A):2991–3012.
Wu, W. B. and Zhao, Z. (2007). Inference of trends in time series. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 69(3):391–410.
Yu, K. and Jones, M. (1998). Local linear quantile regression. Journal of the American
statistical Association, 93(441):228–237.
Zhao, Z. and Wu, W. B. (2008). Confidence bands in nonparametric time series regression.
The Annals of Statistics, 36(4):1854–1878.
Zheng, J. X. (1998). A consistent nonparametric test of parametric regression models
under conditional quantile restrictions. Econometric Theory, 14:123–138.

48

Zhou, Z. (2010). Nonparametric inference of quantile curves for nonstationary time series.
The Annals of Statistics, 38(4):2187–2217.
Zhou, Z. and Wu, W. B. (2009). Local linear quantile estimation for nonstationary time
series. The Annals of Statistics, 37(5B):2696–2729.
Zhou, Z. and Wu, W. B. (2010). Simultaneous inference of linear models with time varying
coefficients. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
72(4):513–531.

49

