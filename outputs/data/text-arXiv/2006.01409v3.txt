arXiv:2006.01409v3 [eess.IV] 11 Nov 2020

COVIDGR DATASET AND COVID-SDN ET METHODOLOGY FOR
PREDICTING COVID-19 BASED ON C HEST X-R AY IMAGES

S. Tabik∗
Andalusian Research Institute
in Data Science and
Computational Intelligence
University of Granada
18071 Spain
siham@ugr.es
I. Sevillano-Garcı́a
Andalusian Research Institute
in Data Science and
Computational Intelligence
University of Granada
18071 Spain

A. Gómez-Rı́os
Andalusian Research Institute
in Data Science and
Computational Intelligence
University of Granada
18071 Spain

M. Rey-Area
atlanTTic Research Center for
Telecommunication Technologies
University of Vigo
Galicia, Spain

E. Guirado
Multidisciplinary Institute for
Environment Studies “Ramón Margalef”
University of Alicante
03690, Spain

M.A. Valero-González
Hospital Universitario Clı́nico
San Cecilio de Granada, Spain

J.L. Suárez
Andalusian Research Institute
in Data Science and
Computational Intelligence
University of Granada
18071 Spain

P. Garcı́a-Villanova
Hospital Universitario Clı́nico
San Cecilio de Granada, Spain

J.L. Martı́n-Rodrı́guez
Hospital Universitario Clı́nico
San Cecilio de Granada, Spain

D. Charte
Andalusian Research Institute
in Data Science and
Computational Intelligence
University of Granada
18071 Spain
J. Luengo
Andalusian Research Institute
in Data Science and
Computational Intelligence
University of Granada
18071 Spain
E. Olmedo-Sánchez
Hospital Universitario Clı́nico
San Cecilio de Granada, Spain

F. Herrera
Andalusian Research Institute
in Data Science and
Computational Intelligence
University of Granada
18071 Spain

November 12, 2020

A BSTRACT
Currently, Coronavirus disease (COVID-19), one of the most infectious diseases in the 21st century,
is diagnosed using RT-PCR testing, CT scans and/or Chest X-Ray (CXR) images. CT (Computed
Tomography) scanners and RT-PCR testing are not available in most medical centers and hence in
many cases CXR images become the most time/cost effective tool for assisting clinicians in making decisions. Deep learning neural networks have a great potential for building COVID-19 triage
systems and detecting COVID-19 patients, especially patients with low severity. Unfortunately,
∗

Corresponding author

A PREPRINT - N OVEMBER 12, 2020

current databases do not allow building such systems as they are highly heterogeneous and biased
towards severe cases. This paper is three-fold: (i) we demystify the high sensitivities achieved by
most recent COVID-19 classification models, (ii) under a close collaboration with Hospital Universitario Clı́nico San Cecilio, Granada, Spain, we built COVIDGR-1.0, a homogeneous and balanced
database that includes all levels of severity, from normal with Positive RT-PCR, Mild, Moderate to
Severe. COVIDGR-1.0 contains 426 positive and 426 negative PA (PosteroAnterior) CXR views and
(iii) we propose COVID Smart Data based Network (COVID-SDNet) methodology for improving
the generalization capacity of COVID-classification models. Our approach reaches good and stable
results with an accuracy of 97.72% ± 0.95%, 86.90% ± 3.20%, 61.80% ± 5.49% in severe, moderate
and mild COVID-19 severity levels2 . Our approach could help in the early detection of COVID-19.
COVIDGR-1.0 along with the severity level labels are available to the scientific community through
this link https://dasci.es/es/transferencia/open-data/covidgr/.

1

Introduction

In the last months, the world has been witnessing how COVID-19 pandemic is increasingly infecting a large mass of
people very fast everywhere in the world. The trends are not clear yet but some research confirm that this problem
may persist until 2024 ([16]). Besides, prevalence studies conducted in several countries reveal that a tiny proportion
of the population have developed antibodies after exposure to the virus, e.g., 5% in Spain 3 . This means that frequently
a large number of patients will need to be assessed in small time intervals by few number of clinicians and with very
few resources.
In general, COVID-19 diagnosis is carried out using at least one of these three tests.
• Computed Tomography (CT) scans-based assessment: it consists in analyzing 3D radiographic images from
different angles. The needed equipment for this assessment is not available in most hospitals and it takes
more than 15 minutes per patient in addition to the time required for CT decontamination 4 .
• Reverse Transcription Polymerase Chain Reaction (RT-PCR) test: it detects the viral RNA from sputum or
nasopharyngeal swab ([29]). It requires specific material and equipment, which are not easily accessible
and it takes at least 12 hours, which is not desirable as positive COVID-19 patients should be identified and
tracked as soon as possible. Some studies found that RT-PCR results from several tests at different points
from the same patients were variable during the course of the illness producing a high false-negative rate
([18]). The authors suggested that RT-PCR test should be combined with other clinical tests such as CT.
• Chest X-Ray (CXR): The required equipment for this assessment are less cumbersome and can be lightweight
and transportable. In general, this type of resources is more available than the required for RT-PCR and CTscan tests. In addition, CXR test takes about 15 seconds per patient ([29]), which makes CXR one of the most
time/cost effective assessment tools.
Few recent studies provide estimates on expert radiologists sensitivity in the diagnosis of COVID-19 based on CT
scans, RT-PCR and CXR. A study on a set of 51 patients with chest CT and RT-PCR essay performed within 3 days,
reported a sensitivity in CT of 98% compared with RT-PCR sensitivity of 71% ([12]). A different study on 64 patients
(26 men, mean age 56 ± 19 years) reported a sensitivity of 69% for CXR compared with 91% for initial RT-PCR
([29]). According to an analysis of 636 ambulatory patients ([28]), most patients presenting to urgent care centers with
confirmed coronavirus disease 2019 have normal or mildly abnormal findings on CXR. Only 58.3% of these patients
are correctly diagnosed by the expert eye.
In a recent study ([29]), authors proposed simplifying the quantification of the level of severity by adapting a previously
defined Radiographic Assessment of Lung Edema (RALE) score ([27]) to COVID-19. This new score is calculated
by assigning a value between 0-4 to each lung depending on the extent of visual features such as, consolidation and
ground glass opacities, in the four parts of each lung as depicted in Fig. 1. Based on this score, experts can identify
the level of severity of the infection among four severity stages, Normal 0, Mild 1-2, Moderate 3-5 and Severe 6-8.
In practice, a patient classified by expert radiologist as Normal can have positive RT-PCR. We refer to these cases as
Normal-PCR+. Expert annotation adopted in this work is based in this score.
2

Paper accepted for publication in Journal of Biomedical and Health Informatics
https://english.elpais.com/society/2020-05-14/antibody-study-shows-just-5-of-spaniards-have-contracted-thecoronavirus.html
4
//www.acr.org/Advocacy-and-Economics/ACR-Position-Statements/Recommendations-for-Chest-Radiography-and-CT-forSuspected-COVID19-Infection
3

2

A PREPRINT - N OVEMBER 12, 2020

Figure 1: The stratification of radiological severity of COVID-19. Examples of how RALE index is calculated.

Automated image analysis via Deep learning (DL) models have a great potential to optimize the role of CXR images
for a fast diagnosis of COVID-19. A robust and accurate DL model could serve as a triage method and as a support
for medical decision making. An increasing number of recent works claim achieving impressive sensitivities > 95%,
far higher than expert radiologists. These high sensitivities are due to the bias in the most used COVID-19 dataset,
COVID-19 Image Data Collection ([5]). This dataset includes a very small number of COVID-19 positive cases,
coming from highly heterogeneous sources (at least 15 countries) and most cases are severe patients, an issue that
drastically reduces its clinical value. To populate Non-COVID and Healthy classes, AI researchers are using CXR
images from diverse pulmonary disease repositories. The obtained models will have no clinical value as well since
they will be unable to detect patients with low and moderate severity, which are the target of a clinical triage system.
In view of this situation, there is still a huge need for higher quality datasets built under the same clinical protocol and
under a close collaboration with expert radiologists.
Multiple studies have proven that higher quality data ensures higher quality models. The concept of Smart Data refers
to the process of converting raw data into higher quality data with higher concentration of useful information ([19]).
Smart data includes all pre-processing methods that improve value and veracity of data. Examples of these methods
include noise elimination, data-augmentation ([25]) and data transformation ([23]) among other techniques.
In this work, we designed a high clinical quality dataset, named COVIDGR-1.0 that includes four levels of severity, Normal-PCR+, Mild, Moderate and Severe. We identified these four severity levels from a recent COVID-19
radiological study ([29]). We also propose COVID Smart Data based Network (COVID-SDNet) methodology. It combines segmentation, data-augmentation and data transformations together with an appropriate Convolutional Neural
Network (CNN) for inference.
The contributions of this paper can be summarized as follows:
3

A PREPRINT - N OVEMBER 12, 2020

• We analyze reliability, potential and limitations of the most used COVID-19 CXR datasets and models.
• From a data perspective, we provide the first public dataset, called COVIDGR-1.0, that quantifies COVID-19
in terms of severity levels, normal, mild, moderate and severe, with the aim of building triage systems with
high clinical value.
• From a pre-processing perspective, we combined several methods. To eliminate irrelevant information from
the input CXR images, we used a new pre-processing method called segmentation-based cropping. To increase discrimination capacity of the classification model, we used a class-inherent transformation method
inspired by GANs.
• From a post-processing perspective, we proposed a new inference process that fuses the predictions of the four
transformed classes obtained by the class-inherent transformation method to calculate the final prediction.
• From a global perspective, we designed a novel methodology, named COVID-SDNet, with a high generalization capacity for COVID-19 classification based on CXR images. COVID-SDNet combines segmentation,
data-transformation, data-augmentation, and a suitable CNN model together with an inference approach to
get the final prediction.
Experiments demonstrate that our approach reaches good and stable results especially in moderate and severe levels,
with 97.72% ± 0.95% and 86.90% ± 3.20% respectively. Lower accuracies were obtained in mild and normal-PCR+
severity levels with 61.80% ± 5.49% and 28.42% ± 2.58%, respectively.
This paper is organized as follows: A review of the most used datasets and COVID-19 classification approaches is
provided in Section 2. Section 3 describes how COVIDGR-1.0 is built and organized. Our approach is presented in
Section 4. Experiments, comparisons and results are provided in Section 5. The inspection of the model’s decision
using heatmaps is provided in Section 6 and the conclusions are pointed out in Section 7.

2

Related works

The last months have known an increasing number of works exploring the potential of deep learning models for
automating COVID-19 diagnosis based on CXR images. The results are promising but still too much work needs
to be done at the level of data and models design. Given the potential bias in this type of problems, several studies
include explication methods to their models. This section analyzes the advantages and limitations of current datasets
an models for building automatic COVID-19 diagnosis systems with and without decision explication.
2.1

Datasets

There does not exist yet a high quality collection of CXR images for building COVID-19 diagnosis systems of high
clinical value. Currently, the main source for COVID-19 class is COVID-19 Image Data Collection ([5]). It contains
76 positive and 26 negative PA views. These images were obtained from highly heterogeneous equipment from all
around the world. Another example of COVID-19 dataset is Figure-1-COVID-19 Chest X-ray Dataset Initiative ([8]).
To build Non-COVID classes, most studies are using CXR from one or multiple public pulmonary disease data-sets.
Examples of these repositories are:
• RSNA Pneumonia CXR challenge dataset on Kaggle ([6]).
• ChestX-ray8 dataset ([10]).
• MIMIC-CXR dataset ([9]).
• PadChest dataset ([7]).
For instance, COVIDx 1.0 ([26]) was built by combining three public datasets: (i) COVID-19 Image Data Collection
([5]), (ii) Figure-1-COVID- 19 Chest X-ray Dataset Initiative ([8]) and (iii) RSNA Pneumonia Detection Challenge
dataset ([6]). COVIDx 2.0 was built by re-organizing COVIDx 1.0 into three classes, Normal (healthy), Pneumonia
and COVID-19, using 201 CXR images for COVID class, including PA(PosteroAnterior) and AP(AnteroPosterior)
views (see Table 1). Notice that for a correct learning front view (PA) and back view (AP) cannot be mixed in the
same class.
Although the value of these datasets is unquestionable as they are being useful for carrying out first studies and
reformulations, they do not guarantee useful triage systems for the next reasons. It is not clear what annotation
protocol has been followed for constructing the positive class in COVID-19 Image Data Collection. The included data
is highly heterogeneous and hence DL-models can rely on other aspects than COVID visual features to differentiate
4

A PREPRINT - N OVEMBER 12, 2020

Version
1.0
2.0

Normal(healthy) Pneumonia
COVID-19
1,583
4,273 (Bacterial+viral) 76
8,066
8,614
190
Table 1: A brief description of COVIDx dataset ([5]) (only PA views are counted).

between the involved classes. This dataset does not provide a representative spectrum of COVID-19 severity levels,
most positive cases are of severe patients ([17]). In addition, an interesting critical analysis of these datasets has shown
that CNN models obtain similar results with and without eliminating most of the lungs in the input X-Ray images [20],
which confirms again that there is a huge need of COVID-19 datasets with high clinical value.
Our claim is that the design of a high quality dataset must be done under a close collaboration between expert radiologists and AI experts. The annotations must follow the same protocol and representative numbers of all levels of
severity, especially Mild and Moderate levels, must be included.
Ref.
([26])
([1])
([22])
([15])
([13])
([2])

Classes
Normal,
Pneumonia,
COVID
Normal, COVID
No-Findings, COVID
No-Findings, Pneumonia, COVID
Normal,
Pneumonia,
COVID
Normal, Bacterial, Viral,
COVID
Normal,
Pneumonia,
COVID

Datasets
COVIDx 1.0

Model
COVIDNet

Partition
98% - 2%

Sens.
87.1%

Acc.
92.6%

COVIDx 1.0

COVID-CAPS

([5]) + ([10])

DarkCovidNet

98% - 2%
5-FCV
5-FCV

90%
90.65%
97.9%

95.7%
98.08%
87.02%

COVIDx 2.0+([6])

VGG-19
+
DenseNet-161
Bayesian
ResNet50V2
MobileNet

70% - 30%

93%

96.77%

80% - 20%

85.71%

89.82%

10-FCV

98.66%

96.78%

([5])+([6])
[5] + ([6])+ other sources

Table 2: Summary of related works that analyze variations of COVIDx with CNN.

2.2

DL classification models

Existing related works are not directly comparable as they consider different combinations of public data-sets and
different experimental setup. A brief summary of these works is provided in Table 2.
The most related studies to ours as they proposed different models to the typical ones are ([26]) and ([1]). In ([26]),
the authors designed a deep network, called COVIDNet. They affirmed that COVIDNet reaches an overall accuracy
of 92.6%, with 97.0% sensitivity in Normal class, 90.0% in Non-COVID-19 and 87.1% in COVID-19. The authors of
a smaller network, called COVID-CAPS ([1]), also claim that their model achieved an accuracy of 98.7%, sensitivity
of 90%, and specificity of 95.8%. These results look too impressive when compared to expert radiologist sensitivity,
69%. This can be explained by the fact that the used dataset is biased to severe COVID cases ([17]). In addition, the
performed experiments in both cited works are not statistically reliable as they were evaluated on one single partition.
The stability of these models, in terms of standard deviation, has not been reported.

Dataset
COVIDGR-1.0

Class
Negative
COVID-19

#images
426
426

women
239
190

men
187
236

#img. per severity level
Normal-PCR+: 76
Mild: 100
Moderate: 171
Severe: 79

Table 3: A brief summary of COVIDGR-1.0 dataset. All samples in COVIDGR 1.0 are segmented CXR images
considering only PA view.
5

A PREPRINT - N OVEMBER 12, 2020

2.3

DL classification models with explanation approaches

Several interesting explanations were proposed to help inspect the predictions of DL-models ([13, 15]) although all
their classification models were trained and validated on variations of COVIDx. The authors in ([15]) first use an
ensemble of two CNN networks to predict the class of the input image, as Normal, Pneumonia or COVID. Then
highlight class-discriminating regions in the input CXR image using gradient-guided class activation maps (GradCAM++) and layer-wise relevance propagation (LRP). In ([13]), the authors proposed explaining the decision of the
classification model to radiologists using different saliency map types together with uncertainty estimations (i.e., how
certain is the model in the prediction).

3

COVIDGR-1.0: Data acquisition, annotation and organization

Instead of starting with an extremely large and noisy dataset, one can build a small and smart dataset then augment
it in a way it increases the performance of the model. This approach has proven effective in multiple studies. This
is particularly true in the medical field, where access to data is heavily protected due to privacy concerns and costly
expert annotation.
Under a close collaboration with four highly trained radiologists from Hospital Universitario Clı́nico San Cecilio,
Granada, Spain, we first established a protocol on how CXR images are selected and annotated to be included in the
dataset. A CXR image is annotated as COVID-19 positive if both RT-PCR test and expert radiologist confirm that
decision within less than 24 hours. CXR with positive PCR that were annotated by expert radiologists as Normal are
labeled as Normal-PCR+. The involved radiologists annotated the level of severity of positive cases based on RALE
score as: Normal-PCR+, Mild, Moderate and Severe.
COVIDGR-1.0 is organized into two classes, positive and negative. It contains 852 images distributed into 426 positive and 426 negative cases, more details are provided in Table 3. All the images were obtained from the same
equipment and under the same X-ray regime. Only PosteriorAnterior (PA) view is considered. COVIDGR-1.0 along
with the severity level labels are available to the scientific community through this link: https://dasci.es/es/
transferencia/open-data/covidgr/.

4

COVID-SDNet methodology

In this section, we describe COVID-SDNet methodology in detail, covering pre-processing to produce smart data,
including segmentation and data transformation for increasing discrimination between positive and negative classes,
combined with a deep CNN for classification.
One of the pieces of COVID-SDNet is the CNN-based classifier. We have selected Resnet-50 initialized with ImageNet
weights for a transfer learning approach. To adapt this CNN to our problem, we have removed the last layer of the
net and added a 512 neurons layer with ReLU activation and a two or four neurons layer (according to the considered
number of classes) with softmax activation.
Let X be the set of n images and K the total number of classes. Each image xi ∈ X has a true label yi with
i = 1, 2, . . . , n. The softmax function computes the probability that an image belongs to class k with k = 1, . . . , K.
Let w = (w1 , . . . , wK ) be the output of the last fully connected layer before the softmax activation is applied. Then,
this function is defined as: softmax : RK → [0, 1]K ,
exp(wj )
softmax(w)j = PK
.
k=1 exp(wk )
Let ybi be the class prediction of the network for the image xi , then ybi = argmax(softmax(w)), where w is the output
vector of the last layer before softmax is applied for the input xi .
All the layers of the network were fine-tuned. We used a batch size of 16 and SGD as optimizer.
The main stages of COVID-SDNet are three, two associated to pre-processing for producing quality data (smart data
stages) and the learning and inference process. A flowchart of COVID-SDNet is depicted in Fig. 2.
1. Segmentation-based cropping: Unnecessary information elimination
Different CXR equipment brands include different extra information about the patient in the sides and contour
of CXR images. The position and size of the patient may also imply the inclusion of more parts of the
body, e.g., arms, neck, stomach. As this information may alter the learning of the classification model, first,
6

A PREPRINT - N OVEMBER 12, 2020

Figure 2: Flowchart of the proposed COVID-SDNet methodology.

we segment the lungs using the U-Net segmentation model provided in ([21]), pre-trained on Tuberculosis
Chest X-ray Image datasets ([14]) and RSNA Pneumonia CXR challenge dataset ([6]). Then, we calculate
the smallest rectangle that delimits the left and right segmented-lungs. Finally, to avoid eliminating useful
information, we add 2.5% of pixels to the left, right, up and down sides of the rectangle. The resulting
rectangle is cropped. An illustration with example of this pre-processing is shown in Fig. 3.
2. Class-inherent transformations Network
To increase the discrimination capacity of the classification model, we used, FuCiTNet ([23]), a Classinherent transformations (CiT) Network inspired by GANs (Generative Adversarial Networks). This transformation method is actually an array of two generators GP and GN , where P refers to the positive class and
N refers to the negative class. GP learns the inherent-class transformations of the positive class P and GN
learns the inherent-class transformations of the negative class N. In other words, GP learns the transformations that bring an input image from its own k domain, with k ∈ {P, N}, to the P class domain. Similarly,
7

A PREPRINT - N OVEMBER 12, 2020

(a) Input image

(b) The smallest rectangle that de- (c) 2.5% of pixels are added to the
limits the left and right segmented left, right, up and down sides of the
lungs is calculated
rectangle then the final rectangle is
cropped

Figure 3: The segmentation-based cropping pre-processing applied to the input X-ray image

(a) Original Negative

(b) Negative transf.

(c) Positive transf.

Figure 4: Class-inherent transformations applied to a negative sample. a) Original negative sample; b) Negative
transformation; c) Positive transformation
GN learns the transformations that bring the input image from its k space, with k ∈ {P, N}, to the N class
space. The classification loss is introduced in the generators to drive the learning of each specific k-class
transformations. That is, each generator is optimized based on the following loss function:
Lgenk = lM SE + 0.006 · lP erceptual + λ · lCE (y == k)

(1)

Where lM SE is a pixel-wise Mean Square Error, lP erceptual is a perception Mean Square Error and lCE is
the classifier loss. The weighted factor λ indicates how much the generator must change its outcome to suit
the classifier. More details about these transformation networks can be found in ([23]).
The architecture of the generators consists of 5 identical residual blocks. Each block has two convolutional
layers with 3 × 3 kernels and 64 feature maps followed by batch-normalization layers and Parametric ReLU
as activation function. The last residual block is followed by a final convolutional layer which reduces the
output image channels to 3 to match the input’s dimensions. The classifier is a ResNet-18 which consists of
an initial convolutional layer with 7×7 kernels and 64 feature maps followed by a 3×3 max pool layer. Then,
4 blocks of two convolutional layers with 3 × 3 kernels with 64, 128, 256 and 512 feature maps respectively
followed by a 7 × 7 average pooling and one fully connected layer which outputs a vector of N elements.
ReLU is used as activation function.
Once the generators learn the corresponding transformations, the dataset is processed using GP and GN .
−
+
−
Two pair of images (x+
i , xi ) will be obtained from each input image xi , i = 1, . . . , n, where xi and xi
are respectively the positively and negatively transformed images of xi . Note that, once the entire dataset is
processed, we have four classes (P+, P−, N+, N−) instead the original P and N classes. Let yi be the class
+
of xi , yi ∈ {P, N}. If yi = P, GP and GN will produce the positive transformation x+
i with yi = P+
8

A PREPRINT - N OVEMBER 12, 2020

−
and the negative transformation x−
i with yi = P−, respectively. If yi = N, GP and GN will produce the
+
+
−
positive transformation xi with yi = N+ and the negative transformation x−
i with yi = N−, respectively.
Fig. 4 illustrates with example the transformations applied by GN and GP . Notice that these transformations
are not meant to be interpretable by the human eye but rather help the classification model better distinguish
between the different classes.

3. Learning and inference based on the fusion of CNN twins
The CNN classification model described above in this section (Resnet-50) is trained to predict the new four
classes: P+, P−, N+, N−. The output of the network (after softmax is applied) for each transformed image
associated to the original one is a vector θ = (θP+ , θP− , θN+ , θN− ), where θj is the probability of the
transformed image to belong to class j ∈ {P+, P−, N+, N−}. Herein, we propose an inference process
−
to fuse the output of the two transformed images x+
i and xi to predict the label of the original image xi .
+
−
In this way, for each pair (xi , xi ), the prediction of the original image ybi will be either P or N. Let
+
−
yc
= argmax θ = argmax (θ , θ , θ , θ ) and yc
= argmax ψ = argmax (ψ , ψ , ψ , ψ )
P+

i

P−

N+

N−

P+

i

P−

N+

N−

−
be the ResNet-50 predictions for x+
i and xi respectively. Then:

c
+
−
(a) If yc
bi = N.
i = N+ and yi = N−, then y
c
c
+
−
(b) If y = P+ and y = P−, then yb = P.
i

i

i

(c) If none of the above applies, then
(
ybi =

N if max(θNj , ψNj ) > max(θPj , ψPj ),
j ∈ {+, −}
P otherwise .

Experimentally, we used a batch size of 16 and SGD as optimizer.

5

Experiments and Results

In this section we (1) provide all the information about the used experimental setup, (2) evaluate two state-of-the-art
COVID classification models and FuCiTNet alone ([23]) on our dataset then, analyze (3) the impact of data preprocessing and (4) Normal-PCR+ severity level on our approach.
5.1

Experimental setup

Due to the high variations between different executions, we performed 5 different 5 fold cross validations in all the
experiments. Each experiment uses 80% of COVIDGR-1.0 for training and the remaining 20% for testing. To choose
when to stop the training process, we used a random 10% of each training set for validation. In each experiment,
a proper set of data-augmentation techniques is carefully selected. All results, in terms of sensitivity, specificity,
precision, F1 and accuracy, are presented using the average values and the standard deviation of the 25 executions.
The used metrics are calculated as follows:
recall(positive class) = sensitivity =

TP
actual positives

recall(negative class) = specificity =

TN
actual negatives

precision(positive class) =

TP
predicted positives

precision(negative class) =

TN
predicted negatives

accuracy =

TP+TN
total predictions
9

A PREPRINT - N OVEMBER 12, 2020

Class
Metric
COVIDNet-CXR A ([26])
Retrained COVIDNet-CXR A
COVID-CAPS ([1])
Retrained COVID-CAPS

Negative
Specificity
Precision
0.23
16.00
88.82±0.90 3.36±6.15
26.30
45.81
65.74±9.93 65.62±3.98

Positive (COVID-19)
Sensitivity
Precision
99.29
33.54
46.82±17.59 81.65±6.02
69.01
48.36
64.93±9.71 66.07±4.49

Accuracy
49.76
67.82±6.11
47.66
65.34±3.26

Table 4: COVIDNet and COVID-CAPS results on our dataset
Class
Metric
COVIDNet-CXR
COVID-CAPS
Without seg.
With seg.
FuCiTNet
COVID-SDNet

Specificity
88.82±0.90
65.74±9.93
79.87±8.91
78.41±7.09
80.79±6.98
79.76±6.19

N
Precision
3.36±6.15
65.62±3.98
71.91±3.12
73.36±4.66
72.00±4.48
74.74±3.89

F1
73.31±3.79
65.15±5.02
75.40±4.91
75.46±2.97
75.84±3.18
76.94±2.82

Sensitivity
46.82±17.59
64.93±9.71
68.63±6.08
70.80±8.26
67.90±8.58
72.59±6.77

P
Precision
81.65±6.02
66.07±4.49
78.75±6.31
77.17±4.79
78.48±4.99
78.67±4.70

F1
56.94±15.05
64.87±4.92
72.689±3.45
73.40±4.01
72.35±4.76
75.71±3.35

Accuracy
67.82±6.11
65.34±3.26
74.25±3.61
74.60±2.93
74.35±3.34
76.18±2.70

Table 5: Results of COVID-19 prediction using Retrained COVIDNet-CXR A, Retrained COVID-CAPS, ResNet-50
with and without segmentation, FuCiTNet and COVID-SDNet. All four levels of severity in the positive class are
taken into account.

F1 = 2 ·

precision · recall
precision + recall

TP and TN refers respectively to the number of true positives and true negatives.
5.2

Analysis of COVIDNet and COVID-CAPS

We compare our approach with the two most related approaches to ours, COVIDNet ([26]) and COVID-CAPS ([1]).
• COVIDNet: Currently, the authors of this network provide three versions, namely A, B and C, available at
([11]). A has the largest number of trainable parameters, followed by B and C. We performed two evaluations
of each network in such a way that the results will be comparable to ours.
– First, we tested COVIDNet-A, COVIDNet-B and COVIDNet-C, pre-trained on COVIDx, directly on
our dataset by considering only two classes: Normal (negative), and COVID-19 (positive). The whole
dataset (426 positive images and 426 negative images) is evaluated. We report in Table 4 recall and
precision results for Normal and COVID-19 classes.
– Second, we retrained COVIDNet on our dataset. It is important to note that as only a checkpoint of each
model is available, we could not remove the last layer of these networks, which has three neurons. We
used 5 different 5 fold cross validations. In order to be able to retrain COVIDNet models, we had to add
a third Pneumonia class into our dataset. We randomly selected 426 images from the Pneumonia class
in COVIDx dataset. We used the same hyper-parameters as the ones indicated in their training script,
that is, 10 epochs, a batch size of 8 and a learning rate of 0.0002. We changed covid weight to 1 and
covid percent to 0.33 since we had the same number of images in all the classes. Similarly, we report in
Table 4 recall and precision of our two classes, Normal and COVID-19, and omit recall and precision of
Pneumonia class. The accuracy reported in the same table only takes into account the images from our
two classes. As with our models, we report here the mean and standard deviation of all metrics.
Although we analyzed all three A, B and C variations of COVIDNet, for simplicity we only report the results
of the best one.
• COVID-CAPS: This is a capsule network-based model proposed in ([1]). Its architecture is notably smaller
than COVIDNet, which implies a dramatically lower number of trainable parameters. Since the authors also
provide a checkpoint with weights trained in the COVIDx dataset, we were able to follow a similar procedure
than with COVIDNet:
– First, we tested the pretrained weights using COVIDx on COVIDGR-1.0 dataset. COVID-CAPS is
designed to predict two classes, so we reused the same architecture with the new dataset and compute
the evaluation metrics shown in Table 4.
10

A PREPRINT - N OVEMBER 12, 2020

– Second, COVID-CAPS architecture was retrained over the COVIDGR-1.0 dataset. This process finetunes the weights to improve class separation. The retraining process is performed using the same setup
and hyper-parameters reported by the authors. Adam optimizer is used across 100 epochs with a batch
size of 16. Class weights were omitted as with COVIDNet, since this dataset contains balanced classes
in training as well as in test. Evaluation metrics are computed for five sets of 5-fold cross-validation test
subsets and summarized in Table 4.
The results from Table 4 show that COVIDNet and COVID-CAPS trained on COVIDx overestimate COVID-19 class
in our dataset, i.e., most images are classified as positive, resulting in very high sensitivities but at the cost of low
positive predictive value. However, when COVIDNet and COVID-CAPS are re-trained on COVIDGR-1.0 they achieve
slightly better overall accuracy and a higher balance between sensitivity and specificity, although they seem to acquire
a bias favoring the negative class. In general, none of these models perform adequately for the detection of the disease
from CXR images in our dataset.
5.3

Results and Analysis of COVID prediction

The results of the baseline COVID classification model considering all the levels of severity, with and without segmentation, FuCiTNet ([23]), and COVID-SDNet are shown in Table 5.
In general, COVID-SDNet achieves better and more stable results than the rest of approaches. In particular, COVIDSDNet achieved the highest balance between specificity and sensitivity with 76.94 ± 2.82 F1 in the negative class and
75.71 ± 3.35 F1 in the positive class. Most importantly, COVID-SDNet achieved the best sensitivity 72.59 ± 6.77 and
accuracy with 76.18 ± 2.70. FuCiTNet provides in general good but lower and less stable results than COVID-SDNet.
When comparing the results of the baseline classification model with and without segmentation, we can observe that
the use of segmentation improves substantially the sensitivity, which is the most important criteria for a triage system.
This can be explained by the fact that segmentation allows the model to focus on most important parts of the CXR
image.
Analysis per severity level
To determine which levels are the hardest to distinguish by the best approach, we have analyzed the accuracy per
predictions(S)
severity level (S), with accuracy(S) = Correct
, where S ∈ {Normal-PCR+, Mild, Moderate, Severe}.
Total number(S)
The results are shown in Table 6.
S (Severity level) accuracy (S)(%)
Normal-PCR+
28.42 ± 2.58
Mild
61.80 ± 5.49
Moderate
86.90 ± 3.20
Severe
97.72 ± 0.95
Table 6: Results of COVID-SDNet per severity level.

As it can be seen from these results, COVID-SDNet correctly distinguish Moderate and Severe levels with an accuracy
of 86.90% and 97.72%, respectively. This is due to the fact that Moderate and Severe CRX images contain more
important visual features than Mild and Normal-PCR+ which ease the classification task. Normal-PCR+ and Mild
cases are much more difficult to identify as they contain few or none visual features. These results are coherent with
the clinical studies provided in ([28]) and ([29]) which report that expert sensitivity is very low in Normal-PCR+ and
Mild infection levels. Recall that the expert eye does not see any visual signs in Normal-PCR+ although the PCR is
positive. Those cases are actually considered as asymptomatic patients.
5.4

Analysis of the impact of Normal-PCR+

To analyze the impact of Normal-PCR+ class on COVID-19 classification, we trained and evaluated the baseline
model, FuciTNet, COVID-SDNet classification stage, COVIDNet-CXR-A and COVID-CAPS, on COVIDGR-1.0 by
eliminating Normal-PCR+. The results are summarized in Table 7.
Overall, all the approaches systematically provide better results when eliminating Normal-PCR+ from the training and
test processes, including COVIDNet-CXR-A and COVID-CAPS. In particular, COVID-SDNet still represents the best
and most stable approach.
11

A PREPRINT - N OVEMBER 12, 2020

Class
Metric
COVIDNet-CXR
COVID-CAPS
With seg.
FuCiTNet
COVID-SDNet

Specificity
83.42± 15.39
65.09± 10.51
80.57±8.72
82.63±6.61
85.20±5.38

N
Precision
69.73± 10.34
71.72± 5.57
78.68±6.57
79.94±4.28
78.88±3.89

F1
74.45± 8.86
67.52±5.29
78.97±3.20
81.05±3.44
81.75±2.74

Sensitivity
61.82± 22.44
73.31±9.74
76.80±10.15
78.91±5.88
76.80±6.30

P
Precision
79.50± 11.47
68.40±5.13
80.70±5.56
82.43±5.43
84.23±4.59

F1
65.64± 15.90
70.20±4.31
78.01±4.29
80.37±3.16
80.07±0.04

Accuracy
72.62± 7.6
69.20±3.61
78.69±3.00
80.77±3.15
81.00±2.87

Table 7: Results of the baseline classification model with segmentation, COVID-SDNet, retrained COVIDNet-CXR-A
and retrained COVID-CAPS. Only three levels of severity are considered, Mild, Moderate and Severe.

(a) Original Positive (Mild)

(b) why positive

(c) why negative

Figure 5: Heatmap showing the parts of the input image that triggered the positive prediction (b) and counterfactual
explanation (c)

Analysis per severity level
A further analysis of the accuracy at the level of each severity degree (see Table 8) demonstrates that eliminating
Normal-PCR+ decreases the accuracy in Mild and Moderate severity levels by 15.8% and 1.52% respectively.
S (Severity level) accuracy (S)(%)
Mild
46.00 ± 7.10
Moderate
85.38 ± 1.85
Severe
97.22 ± 1.86
Table 8: Results of COVID-SDNet by severity level without considering Normal-PCR+.
These results show that although Normal-PCR+ is the hardest level to predict, its presence improves the accuracy of
lower severity levels, especially Mild level.

6

Inspection of model’s decision

Automatic DL diagnosis systems alone are not mature yet to replace expert radiologists. To help clinician making
decisions, these tools must be interpretable so that clinicians can decide whether to trust the model or not ([3]). We
inspect what led our model make a decision by showing the regions of the input image that triggered that decision
along with its counterfactual explanation by showing the parts that explain the opposite class. We adapted Grad-CAM
method ([24]) to explain the decision of the negative and positive class.
Fig. 5, 6 and 7 show (a) the original CXR image, (b) visual explanation by means of a heat-map that highlights the
regions/pixels which led the model to output the actual prediction and (c) its counterfactual explanation using a heatmap that highlights the regions/pixels which had the highest impact on predicting the opposite class. Higher intensity
in the heat-map indicates higher importance of the corresponding pixel in the decision. The larger higher intensity
areas in the heat-map determine the final class. However, Fig. 8(b) represents first the counterfactual explanation and
Fig. 8(c) represents the explanation of the actual decision.
As expected, negative and positive interpretations are complementary, i.e, areas which triggered the correct decision
are opposite, in most cases, to the areas that triggered the decision towards negative. In CXR images with different
12

A PREPRINT - N OVEMBER 12, 2020

(a) Original Positive (Moderate)

(b) why positive

(c) why negative

Figure 6: Heatmap showing the parts of the input image that triggered the positive prediction (b) and counterfactual
explanation (c)

(a) Original Positive (Severe)

(b) why positive

(c) why negative

Figure 7: Heatmap showing the parts of the input image that triggered the positive prediction (b) and counterfactual
explanation (c)

severity levels, the heat-maps correctly point out opaque regions due to different levels of infiltrates, consolidations
and also to osteoarthritis.
In particular, in Fig. 5(b), the red areas in the right lung points out a region with infiltrates and also osteoarthritis in the
spine region. Fig. 6 (b) correctly shows moderate infiltrates in the right lower and lower-middle lung fields in addition
to a dilation of ascending aorta and aortic arch (red color in the center). Fig. 5(c) shows normal upper-middle fields
of both lungs (less important on the left due to aortic dilation). Fig. 7(b) indicates an important bilateral pulmonary
involvement with consolidations.
As it can be observed in Fig. 8(c), the explanation of the negative class correctly highlights a symmetric bilateral
pattern that occupies a larger lung volume especially in regions with high density. In fact, a very similar pattern is
shown in the counterfactual explanation of the positive class in Fig. 5(c), 6(c) and 7(c).

7

Conclusions

This paper introduced a dataset, named COVIDGR-1.0, with high clinical value. COVIDGR-1.0 includes the four
main COVID severity levels identified by a recent radiological study ([29]). We proposed a methodology, called
COVID-SDNet, that combines segmentation, data-augmentation and data transformation. The obtained results show
the high generalization capacity of COVID-SDNet, specially on severe and moderate levels as they include important
visual features. The existence of few or none visual features in Mild and Normal-PCR+ reduces the opportunities for
improvement.
As main conclusions, we must highlight that COVID-SDNet can be used in a triage system to detect especially moderate and severe patients. Finally, we must also mention that more robust and accurate triage system can be built by
fusing our approach with other approaches such as the one proposed in ([4]).
13

A PREPRINT - N OVEMBER 12, 2020

(a) Original Negative

(b) why positive

(c) why negative

Figure 8: Heatmap that explains the parts of the input image that triggered the counterfactual explanation (b) and the
negative actual prediction (c).

As future work, we are working on enriching COVIDGR-1.0 with more CXR images coming from different hospitals.
We are planning to explore the use of additional clinical information along with CXR images to improve the prediction
performance.

Acknowlegments
This work was supported by the project DeepSCOP-Ayudas Fundación BBVA a Equipos de Investigación Cientı́fica
en Big Data 2018, COVID19 RX-Ayudas Fundación BBVA a Equipos de Investigación Cientı́fica SARS-CoV-2 y
COVID-19 2020, and the Spanish Ministry of Science and Technology under the project TIN2017-89517-P. S. Tabik
was supported by the Ramon y Cajal Programme (RYC-2015-18136). A. Gómez-Rı́os was supported by the FPU
Programme FPU16/04765. D. Charte was supported by the FPU Programme FPU17/04069. J. Suárez was supported
by the FPU Programme FPU18/05989. E.G was supported by the European Research Council (ERC Grant agreement
647038 [BIODESERT])

Ethics
This project is approved by the Provincial Research Ethics Committee of Granada.

References
[1] P. Afshar and et al. COVID-CAPS: A capsule network-based framework for identification of COVID-19 cases
from x-ray images. arXiv preprint arXiv:2004.02696, 2020.
[2] I.D. Apostolopoulos and T.A. Mpesiana. Covid-19: automatic detection from x-ray images utilizing transfer
learning with convolutional neural networks. Physical and Engineering Sciences in Medicine, page 1, 2020.
[3] A. Arrieta and et al. Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges
toward responsible AI. Information Fusion, 58:82–115, 2020.
[4] J. P. Cohen and et al. Predicting COVID-19 pneumonia severity on chest x-ray with deep learning. arXiv preprint
arXiv:2005.11856, 2020.
[5] Joseph Paul Cohen and et al. Covid-19 image data collection. arXiv 2003.11597, 2020.
[6] [dataset]. Radiological society of north america. RSNA pneumonia detection challenge, 2019.
[7] Aurelia [dataset] Bustos and et al. Padchest: A large chest x-ray image dataset with multi-label annotated reports.
arXiv preprint arXiv:1901.07441, 2019.
[8] [dataset] Chung et al. Figure 1 COVID-19 chest X-ray dataset initiative. 2020.
14

A PREPRINT - N OVEMBER 12, 2020

[9] Alistair EW [dataset] Johnson and et al. Mimic-cxr: A large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019.
[10] Xiaosong [dataset] Wang and et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weaklysupervised classification and localization of common thorax diseases. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 2097–2106, 2017.
[11] M. A. [dataset] Warren and et al. COVIDNet. Accesible en: https://github.com/lindawangg/COVID-Net,
2020.
[12] Yicheng Fang and et al. Sensitivity of chest ct for covid-19: comparison to rt-pcr. Radiology, page 200432, 2020.
[13] B. Ghoshal and A. Tucker. Estimating uncertainty and interpretability in deep learning for coronavirus (COVID19) detection. arXiv preprint arXiv:2003.10769, 2020.
[14] Stefan Jaeger, Sema Candemir, Sameer Antani, Yı̀-Xiáng J Wáng, Pu-Xuan Lu, and George Thoma. Two public
chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and
surgery, 4(6):475, 2014.
[15] Md Karim and et al. Deepcovidexplainer: Explainable covid-19 predictions based on chest x-ray images. arXiv
preprint arXiv:2004.04582, 2020.
[16] Stephen M Kissler and et al. Projecting the transmission dynamics of sars-cov-2 through the postpandemic
period. Science, 2020.
[17] S. Kundu and et al. How might ai and chest imaging help unravel covid-19’s mysteries?, 2020.
[18] Yafang Li and et al. Stability issues of rt-pcr testing of sars-cov-2 for hospitalized patients clinically diagnosed
with COVID-19. Journal of Medical Virology, 2020.
[19] J. Luengo and et al. Big Data Preprocessing - Enabling Smart Data. Springer, 2020.
[20] Gianluca Maguolo and Loris Nanni. A critic evaluation of methods for covid-19 automatic detection from x-ray
images. arXiv preprint arXiv:2004.12823, 2020.
[21] E. Mineo.
U-Net lung segmentation.
Accesible en: https://www.kaggle.com/eduardomineo/
u-net-lung-segmentation-montgomery-shenzhen, 2020.
[22] T. Ozturk and et al. Automated detection of COVID-19 cases using deep neural networks with x-ray images.
Computers in Biology and Medicine, page 103792, 2020.
[23] M. Rey-Area and et al. Fucitnet: Improving the generalization of deep learning networks by the fusion of learned
class-inherent transformations. arXiv preprint arXiv:2005.08235, 2020.
[24] R.R. Selvaraju and et al. Grad-cam: Visual explanations from deep networks via gradient-based localization. In
Proceedings of the IEEE international conference on computer vision, pages 618–626, 2017.
[25] S. Tabik and et al. A snapshot of image pre-processing for convolutional neural networks: case study of mnist.
International Journal of Computational Intelligence Systems, 10(1):555–568, 2017.
[26] L. Wang and et al. COVID-Net: A tailored deep convolutional neural network design for detection of covid-19
cases from chest radiography images, 2020.
[27] M. A. Warren and et al. Severity scoring of lung oedema on the chest radiograph is associated with clinical
outcomes in ards. Thorax, 73(9):840–846, 2018.
[28] M.B. Weinstock and et al. Chest x-ray findings in 636 ambulatory patients with COVID-19 presenting to an
urgent care center: A normal chest x-ray is no guarantee. J Urgent Care Med,(14 (7)), pages 13–18, 2020.
[29] H.Y.F. Wong and et al. Frequency and distribution of chest radiographic findings in COVID-19 positive patients.
Radiology, page 201160, 2020.

15

