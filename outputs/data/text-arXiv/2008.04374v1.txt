Can We Spot the “Fake News”
Before It Was Even Written?

arXiv:2008.04374v1 [cs.CL] 10 Aug 2020

Preslav Nakov1[0000−0002−3600−1510]
Qatar Computing Research Institute, HBKU, Qatar
pnakov@hbku.edu.qa

Abstract. Given the recent proliferation of disinformation online, there
has been also growing research interest in automatically debunking rumors, false claims, and “fake news.” A number of fact-checking initiatives
have been launched so far, both manual and automatic, but the whole
enterprise remains in a state of crisis: by the time a claim is finally factchecked, it could have reached millions of users, and the harm caused
could hardly be undone. An arguably more promising direction is to focus on fact-checking entire news outlets, which can be done in advance.
Then, we could fact-check the news before it was even written: by checking how trustworthy the outlets that published it is. We describe how
we do this in the Tanbih news aggregator, which makes readers aware
of what they are reading. In particular, we develop media profiles that
show the general factuality of reporting, the degree of propagandistic
content, hyper-partisanship, leading political ideology, general frame of
reporting, and stance with respect to various claims and topics.
Keywords: Fake News · Disinformation · Media Bias · Propaganda

1

The “Fake News” Phenomenon

Recent years have seen the rise of social media, which have enabled people to
easily share information with a large number of online users, without quality
control. On the bright side, this has given the opportunity to anybody to become a content creator, and it has enabled a much faster information dissemination. On the not-so-bright side, it has also made it easy for malicious actors
to spread disinformation much faster, potentially reaching very large audiences.
In some cases, this included building sophisticated profiles for individual users
based on a combination of psychological characteristics, metadata, demographics, and location, and then micro-targeting them with personalized “fake news”
and propaganda campaigns that have been weaponized with the aim of achieving
political or financial gains.
To be clear, false information in the news has always been around, e.g., think
of tabloids. However, social media have changed everything. They have made it
possible for malicious actors to micro-target specific demographics, and to spread
disinformation much faster and at scale, at the disguise of news. Thanks to social
media, the news could be weaponized at an unprecedented scale.

2

P. Nakov

As social media are optimized for user engagement, “fake news” thrive on
these platforms, as users cannot recognize it and thus they share it, which can
be further amplified by bots. Studies have shown that 70% of the users cannot
distinguish real from “fake news”, and that “fake news” spread in social media
six times faster than real ones [75]. “Fake news” are like spam on steroids: if a
spam message reaches a thousand people, it would die there; in contrast, “fake
news” can be shared again and again, and it could eventually reach millions.
As a result, there have been growing concerns about disinformation in recent
years. The year 2016 gave rise to the term fake news, as the general public got
concerned about the dangers of possible manipulations of major events such
as the 2016 US Presidential elections and Brexit. Then along came year 2020,
when a new term was coined, infodemic, which reflects the general concern about
the spread of disinformation related to the COVID-19 pandemic. Yet, the real
underlying problem all along has been the weaponization of the news.
In the public discourse, fake news is the preferred term when discussing the
issue. Even though declared Word of the Year 2016 by Macquarie Dictionary
and of Year 2017 by Collins dictionary, we find the term unhelpful, as it can
easily mislead people to only focus on the veracity aspect. At the EU level, a
more precise term is preferred: disinformation,1 which refers to information that
is both (i ) false, and (ii ) intents to harm. The often-ignored latter aspect is the
real reason why the society started worrying back in 2016.
Another problem with the term fake news is that it has no generally agreedupon definition. For example, the Meriam-Webster dictionary finds the term
“self-explanatory” and sees no need to include it in their dictionary,2 arguing
that it was not a new term and that it was already in use back in the 1890s.
However, “fake news” is an oxymoron since, as news is supposed to be true.
The Collins dictionary defines the term as “false, often sensational, information
disseminated under the guise of news reporting”,3 and it does not mention the
intention to do harm, and the use of social media and micro-targeting. The
Cambridge dictionary does better and defines it as “false stories that appear to
be news, spread on the internet or using other media, usually created to influence
political views or as a joke”,4 but it does not mention social media and microtargeting, and it covers satire, which is protected free speech.5
Over time, the term fake news started meaning different things to different
people, and for some politicians, even “news that I do not like.” Yet, the fundamental issue with the term is that it misleads people to focus on veracity and
to ignore the intention to do harm. Focusing on the veracity aspect only is dangerous as weaponized news do not need to lie. For instance, it could cherry-pick
the facts, it could appeal to emotions when presenting actual events, and so on.
In essence, it could tell the truth, only the truth, but not the whole truth.
1
2
3
4
5

http://eeas.europa.eu/topics/countering-disinformation_en
http://www.merriam-webster.com/words-at-play/the-real-story-of-fake-news
http://www.collinsdictionary.com/dictionary/english/fake-news
http://dictionary.cambridge.org/dictionary/english/fake-news
http://en.wikipedia.org/wiki/Hustler_Magazine_v._Falwell

Can We Spot the “Fake News” Before It Was Even Written?

3

Overall, thanks to social media, people today are much more likely to believe
in conspiracy theories. For example, according to a 2019 study, 57% of Russians
believed that the USA did not put a man on the Moon. In contrast, when the
event actually occurred, there was absolutely no doubt about it in the USSR,
and Neil Armstrong was even invited to visit Moscow, which he did.
Indeed, disinformation has become a global phenomenon: a number of countries had election-related issues with “fake news”. To get an idea of the scale,
150 million users on Facebook and Instagram saw inflammatory political ads,
and Cambridge Analytica had access to the data of 87 million Facebook users
in the USA, which it used for targeted political advertisement; for comparison,
the 2016 US Presidential elections in the USA were decided by 80,000 voters in
three key states.
While initially the focus has been on influencing the outcome of political
elections, “fake news” has also caused direct life loss. For example, disinformation
on WhatsApp has resulted in people being killed in India, and disinformation on
Facebook was responsible for the Rohingya genocide, according to a UN report.
Disinformation can also put people’s health in danger, e.g., think of the antivaccine websites and the damage they cause to public health worldwide, or of
the ongoing COVID-19 pandemic, which has also given rise to the first global
infodemic.

2

Related Work

Recently, there has been a lot of research interest in studying disinformation and
bias in the news and in social media. This includes challenging the truthiness of
claims [6,52,81], of news [17,33,36,37,42,60,61], of news sources [8], of social media users [3,26,49,50,51,59], and of social media [18,19,64,82], as well as studying
credibility, influence, and bias [7,8,20,45,49,51]. The interested reader can also
check several recent surveys that offer a general overview on “fake news” [46], or
focus on topics such as the process of proliferation of true and false news online
[75], on fact-checking [71], on data mining [67], or on truth discovery in general [47]. For some specific topics, research was facilitated by specialized shared
tasks such as the SemEval-2017 task 8 and the SemEval-2019 task 7 on Determining Rumour Veracity and Support for Rumours (RumourEval) [28,35], the
CLEF 2018–2020 CheckThat! lab on Automatic Identification and Verification
of Claims [4,5,13,14,16,31,32,38,39,57,58,65], the FEVER-2018 and FEVER-2019
tasks on Fact Extraction and VERification [72,73], and the SemEval-2019 Task
8 on Fact Checking in Community Question Answering Forums [53,54], among
others.
Finally, note that the veracity of information is a much bigger problem than
just “fake news”. It has been suggested that “Veracity” should be seen as the
fourth “V” of Big Data, along with Volume, Variety, and Velocity.6
6

http://www.ibmbigdatahub.com/sites/default/files/infographic_file/
4-Vs-of-big-data.jpg

4

3

P. Nakov

Focusing on the Source

In order to fact-check a news article, we can analyze its contents, e.g., the language it uses, and the reliability of its source, which can be represented as a
number between 0 and 1, where 1 indicates a very reliable source, and 0 stands
for a very unreliable one:
f actuality (article) = reliability (language (article))
+ reliability (website (article))

(1)

In order to fact-check a claim (as opposed to an article), we can retrieve
articles discussing the claim, then we can detect the stance of each article with
respect to the claim, and we can take a weighted sum (here, the stance is a
number between -1 and 1, where it is -1 if the article disagrees with the claim,
it is 1 if it agrees, and it is 0 if it just discusses the claim or if it is unrelated):
f actuality(claim) =

X

[reliability (articlei ) ∗ stance (articlei , claim)]

(2)

i

Note that in formula (1), the reliability of the website that hosts an article
serves as a prior to compute the factuality of an article, while in formula (2),
we use the factuality of the retrieved articles to compute a factuality score for a
target claim. The idea is that if a reliable article agrees/disagrees with the claim,
this is a good indicator for it being true/false, and it is the other way around
for unreliable articles.
Of course, the formulas above are oversimplifications, e.g., one can fact-check
a claim based on the reactions of users in social media [41], based on the claim’s
spread over time in social media [48], based on information in a knowledge graph
[70], extracted from the Web [43] or from Wikipedia [72], using similarity to
previously fact-checked claims [64], etc. Yet, the formulas give the general idea
that the reliability of the source should be an important element of fact-checking
articles and claims. Yet, it is an understudied problem.
Characterizing entire news outlets is an important task on its own right. We
argue that it is more useful than fact-checking claims or articles, as it is hardly
feasible to fact-check every single piece of news. Doing so also takes time, both
to human users and to automatic programs, as they need to monitor the way
reliable media report about a given target claim, how users react to it in social
media, etc., and it takes time to get enough such evidence accumulated in order
to be able to make a reliable prediction. It is much more feasible to check entire
news outlets. Note that we can also fact-check a number of sources in advance,
and we can then fact-check the news before it was even written! Once
it is published online, it would be enough to check how trustworthy the outlets
that published it are, in order to get an initial (imperfect) idea about how much
we should trust this news. This would be similar to the movie Minority Report,
where the authorities could detect a crime before it was even committed.

Can We Spot the “Fake News” Before It Was Even Written?

5

In general, fighting disinformation is not easy; as in the case of spam, this is an
adversarial problem, where the malicious actors constantly change and improve
their strategies. Yet, when they share news in social media, they typically post a
link to an article that is hosted on some website. This is what we are exploiting:
we try to characterize the news outlet where the article is hosted. This is also
what journalists typically do: they first check the source.
Finally, even though we focus on the source, our work is also compatible with
fact-checking a claim or a news article, as we can provide an important prior
and thus help both algorithms and human fact-checkers that try to fact-check a
particular news article or a claim.
How can we profile a news source? Note that disinformation typically focuses on emotions, and political propaganda often discusses moral categories
[23]. There are many incentives for news outlets to publish articles that appeal
to emotions: (i ) this has a strong propagandistic effect on the target user, (ii ) it
makes it more likely to be shared further by the users, and (iii ) it will be favored
as a candidate to be shown in other users’ newsfeed as this is what algorithms
on social media optimize for. And news outlets want to get users to share links
to their content in social media as this allows them to reach larger audience.
This kind of language also makes them potentially detectable for Artificial Intelligence (AI) systems; yet, these outlets cannot do much about it as changing
the language would make their message less effective and it would also limit its
spread.
While the analysis of the language used by the target news outlet is the most
important information source, we can also consider information in Wikipedia
and in social media, traffic statistics, and the structure of the target sites URL
as shown in Figure 1:
1. the text of a few hundred articles published by the target news outlet, analyzing the style, subjectivity, sentiment, offensiveness [77,78,79,62], toxicity [30],
morality, vocabulary richness, propagandistic content, etc.;
2. the text of its Wikipedia page (if any), including infobox, summary, content,
categories, e.g., it might say that the website spreads false information and
conspiracy theories;
3. metadata and statistics about its Twitter account (if any): is it an old account, is it verified, is it popular, how is the medium self-describing, is there
a link to its website, etc.;
4. whether people in social media, e.g., on Twitter, post links in articles to the
target sources in the context of a polarizing topic, and which side of the
debate are these users from;
5. whether there is liberal-vs-moderate-vs-conservative bias of the audience of
the target medium in social media, e.g., in Facebook;
6. the language used in videos by the target medium, e.g., in their Youtube
channels (if any), where the focus is on analysis of the speech signal, i.e., not
on what is said but on how it is said, e.g., is it emotional [44];
7. Web traffic information: whether this is a popular website;
8. the structure of site’s URL: is it too long, does it contain a sequence of
meaningful words, does it have a fishy suffix such as “.com.co”, etc.

6

P. Nakov

Fig. 1. Information sources used to predict the factuality of reporting of a news outlet.

4

The Tanbih Mega-Project

Characterizing media in terms of factuality of reporting and bias is part of
a larger effort at the Qatar Computing Research Institute, HBKU: the Tanbih
mega-project7 aims to limit the effect of “fake news”, disinformation, propaganda
and media bias by making users aware what they are reading, thus promoting
media literacy and critical thinking.
The mega-projects flagship initiative is the Tanbih news aggregator [80],8
which shows real-time news from a variety of news sources [68]. It builds a
profile for each news outlet, showing a prediction about the factuality of its
reporting[8,9,10], its leading political ideology [29], degree of propaganda [12,15],
hyper-partisanship [63,66], and general frame of reporting (e.g., political, economic, legal, cultural identity, quality of life, etc.), and stance with respect to
various claims and topics [11,27,55,56,69]. For the individual news, it signals
when an article is likely to be propagandistic. It further mixes Arabic and English news, and allows the user to see them all in English/Arabic thanks to
QCRIs Machine Translation technology. Tanbih also offers analytics capabilities, allowing a user to explore the media coverage, the frame of reporting, and
the propaganda around topics such as Brexit, Sri Lanka bombings, and COVID19.9 Moreover, it performs fine-grained analysis of the propaganda techniques in
the news [21,22,24,25,76].10
7
8
9
10

http://tanbih.qcri.org/
http://www.tanbih.org/
http://www.tanbih.org/subject/4535/CORONAVIRUS%20OUTBREAK%2019-20
http://www.tanbih.org/prta

Can We Spot the “Fake News” Before It Was Even Written?

7

We developed tools such as a Web browser plugin,11 a mechanism to share
media profiles and stories in social media, a tool to detect check-worthiness for
English and Arabic [34,40,74],12 a Twitter fact-checking bot,13 and an API to the
Tanbih functionality.14 The latter is used by Aljazeera and other partners. More
recently, we have been developing tools for fighting the COVID-19 infodemic by
modeling the perspective of journalists, fact-checkers, social media platforms,
policy makers, and the society [1,2].
Tanbih was developed in close collaboration with MIT-CSAIL.15 We were
also partners in a large NSF project on Credible Open Knowledge Networks,16
and we further collaborate with Carnegie Mellon University in Qatar, Qatar
University, Sofia University, the University of Bologna, Aljazeera, Facebook, the
United Nations, Data Science Society, and A Data Pro, among others. As part
of a larger team, including Al Jazeera, Associated Press, RTE Ireland, Tech
Mahindra, and Metaliquid, and V-Nova, we won an award at IBC 2019 by TM
Forum and IBC 2019 for our Media-Telecom Catalyst project on AI Indexing
for Regulatory Practice.17
In its 2.5 years of history, the Tanbih mega-project has produced 30+ toptier publications, a Best Demo Award (honorable mention) at ACL-2020, and
several patent applications. The project was featured in 30+ keynote talks, and
it was highlighted by 100+ media including Forbes, Boston Globe, Aljazeera,
MIT Technology Review, Science Daily, Popular Science, Fast Company, The
Register, WIRED, and Engadget.

5

The Future of “Fake News”

It is widely believed that “fake news” can and has affected major political events.
In reality, the true impact is unknown; however, given the buzz that was created,
we should expect a large number of state and non-state actors to give it a try.
From a technological perspective, we ca expect further advances in “deep
fakes”, such as machine-generated videos, and images. This is a really scary
development, but probably only in the mid-long run; as of present, “deep fakes”
are still easy to detect both using AI and also by experienced users.
We also expect advances in automatic news generation, thanks to recent
developments such as GPT-3. This is already a reality and a sizable part of the
news we are consuming daily are machine generated, e.g., about the weather,
the markets, and sport events. Such software can describe a sport event from
11

12
13
14
15
16
17

http://chrome.google.com/webstore/detail/tanbih/
igcppjdbignhkiikejdjpjemejoognen
http://claimrank.qcri.org/
http://twitter.com/factchecker_bot/
http://app.swaggerhub.com/apis/yifan2019/Tanbih/0.6.0#/
http://qcri.csail.mit.edu/node/25
http://cokn.org/
http://www.tmforum.org/ai-indexing-regulatory-practise/

8

P. Nakov

various perspectives: neutrally or taking the side of the winning or the losing
team. It is easy to see how this can be used for disinformation purposes.
Yet, we hope to see “fake news” gone the way of spam: not entirely eliminated
(as this is impossible), but put under control. AI has already helped a lot in the
fight against spam, and we expect that it would play a key role in putting “fake
news” under control as well.
A key element of the solution would be limiting the spread. Social media
platforms are best positioned to do this on their own platforms. Twitter has
suspended more than 70 million accounts in May and June 2018, and these
efforts continue to date; this can help in the fight against bots and botnets,
which are the new link farms: 20% of the tweets during the 2016 US Presidential
campaign were shared by bots. Facebook, from its part, warns users when they
try to share a news article that has been fact-checked and identified as fake by
at least two trusted fact-checking organizations, and it also downgrades “fake
news” in its news feed. We expect the AI tools used for this to get better, just
like spam filters have improved over time.
Yet, the most important element of the fight against disinformation is raising
user awareness and develop critical thinking. This would help limit the spread as
users would be less likely to share it further. We believe that practical tools such
as the ones we develop in the Tanbih mega-project would help in that respect.

References
1. Alam, F., Dalvi, F., Shaar, S., Durrani, N., Mubarak, H., Nikolov, A., Martino,
G.D.S., Abdelali, A., Sajjad, H., Darwish, K., Nakov, P.: Fighting the COVID-19
infodemic in social media: A holistic perspective and a call to arms. ArXiv preprint
2007.07996 (2020)
2. Alam, F., Shaar, S., Dalvi, F., Sajjad, H., Nikolov, A., Mubarak, H., Martino,
G.D.S., Abdelali, A., Durrani, N., Darwish, K., Nakov, P.: Fighting the COVID19 infodemic: Modeling the perspective of journalists, fact-checkers, social media
platforms, policy makers, and the society. ArXiv preprint 2005.00033 (2020)
3. Atanasov, A., De Francisci Morales, G., Nakov, P.: Predicting the role of political
trolls in social media. In: Proceedings of the Conference on Computational Natural
Language Learning. pp. 1023–1034. CoNLL ’19, Hong Kong, China (2019)
4. Atanasova, P., Màrquez, L., Barrón-Cedeño, A., Elsayed, T., Suwaileh, R., Zaghouani, W., Kyuchukov, S., Da San Martino, G., Nakov, P.: Overview of the
CLEF-2018 CheckThat! Lab on automatic identification and verification of political claims, Task 1: Check-worthiness. In: Cappellato, L., Ferro, N., Nie, J.Y.,
Soulier, L. (eds.) CLEF 2018 Working Notes. Working Notes of CLEF 2018 Conference and Labs of the Evaluation Forum. CEUR Workshop Proceedings,
CEUR-WS.org, Avignon, France (2018)
5. Atanasova, P., Nakov, P., Karadzhov, G., Mohtarami, M., Martino, G.D.S.:
Overview of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification of Claims. Task 1: Check-Worthiness. In: Cappellato, L., Ferro, N., Losada,
D., Müller, H. (eds.) CLEF 2019 Working Notes. Working Notes of CLEF 2019
- Conference and Labs of the Evaluation Forum. CEUR Workshop Proceedings,
CEUR-WS.org, Lugano, Switzerland (2019)

Can We Spot the “Fake News” Before It Was Even Written?

9

6. Atanasova, P., Nakov, P., Màrquez, L., Barrón-Cedeño, A., Karadzhov, G., Mihaylova, T., Mohtarami, M., Glass, J.: Automatic fact-checking using context and
discourse information. J. Data and Information Quality 11(3), 12:1–12:27 (May
2019)
7. Ba, M.L., Berti-Equille, L., Shah, K., Hammady, H.M.: VERA: A platform for veracity estimation over web data. In: Proceedings of the 25th International Conference Companion on World Wide Web. pp. 159–162. WWW ’16, Montréal, Québec,
Canada (2016)
8. Baly, R., Karadzhov, G., Alexandrov, D., Glass, J., Nakov, P.: Predicting factuality
of reporting and bias of news media sources. In: Proceedings of the Conference on
Empirical Methods in Natural Language Processing. pp. 3528–3539. EMNLP ’18
(2018)
9. Baly, R., Karadzhov, G., An, J., Kwak, H., Dinkov, Y., Ali, A., Glass, J., Nakov,
P.: What was written vs. who read it: News media profiling using text analysis and
social media context. In: Proceedings of the Annual Meeting of the Association for
Computational Linguistics. pp. 3364–3374. ACL ’20, Online (2020)
10. Baly, R., Karadzhov, G., Saleh, A., Glass, J., Nakov, P.: Multi-task ordinal regression for jointly predicting the trustworthiness and the leading political ideology of
news media. In: Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies. pp. 2109–2116. NAACL-HLT ’19, Minneapolis, MN, USA (2019)
11. Baly, R., Mohtarami, M., Glass, J., Màrquez, L., Moschitti, A., Nakov, P.: Integrating stance detection and fact checking in a unified corpus. In: Proceedings of
the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 21–27. NAACL-HLT ’18,
New Orleans, LA, USA (2018)
12. Barrón-Cedeño, A., Da San Martino, G., Jaradat, I., Nakov, P.: Proppy: Organizing the news based on their propagandistic content. Information Processing &
Management 56(5), 1849 – 1864 (2019)
13. Barrón-Cedeño, A., Elsayed, T., Nakov, P., Da San Martino, G., Hasanain, M.,
Suwaileh, R., Haouari, F.: CheckThat! at CLEF 2020: Enabling the automatic
identification and verification of claims in social media. In: Proceedings of the 42d
European Conference on Information Retrieval. pp. 499–507. ECIR ’20, Lisbon,
Portugal (2020)
14. Barrón-Cedeño, A., Elsayed, T., Suwaileh, R., Màrquez, L., Atanasova, P., Zaghouani, W., Kyuchukov, S., Da San Martino, G., Nakov, P.: Overview of the
CLEF-2018 CheckThat! Lab on automatic identification and verification of political claims, Task 2: Factuality. In: Cappellato, L., Ferro, N., Nie, J.Y., Soulier,
L. (eds.) CLEF 2018 Working Notes. Working Notes of CLEF 2018 - Conference
and Labs of the Evaluation Forum. CEUR Workshop Proceedings, CEUR-WS.org,
Avignon, France (2018)
15. Barrón-Cedeño, A., Da San Martino, G., Jaradat, I., Nakov, P.: Proppy: A system
to unmask propaganda in online news. In: Proceedings of the AAAI Conference on
Artificial Intelligence. pp. 9847–9848. AAAI’19, Honolulu, HI, USA (2019)
16. Barrón-Cedeño, A., Elsayed, T., Nakov, P., Da San Martino, G., Hasanain, M.,
Suwaileh, R., Haouari, F., Babulkov, N., Hamdan, B., Nikolov, A., Shaar, S., Ali,
Z.: Overview of CheckThat! 2020 — automatic identification and verification of
claims in social media. In: Proceedings of the 11th International Conference of the
CLEF Association: Experimental IR Meets Multilinguality, Multimodality, and
Interaction. CLEF ’2020, Thessaloniki, Greece (2020)

10

P. Nakov

17. Brill, A.M.: Online journalists embrace new marketing function. Newspaper Research Journal 22(2), 28 (2001)
18. Canini, K.R., Suh, B., Pirolli, P.L.: Finding credible information sources in social networks based on content and social structure. In: Proceedings of the IEEE
International Conference on Privacy, Security, Risk, and Trust, and the IEEE International Conference on Social Computing. pp. 1–8. SocialCom/PASSAT ’11,
Boston, MA, USA (2011)
19. Castillo, C., Mendoza, M., Poblete, B.: Information credibility on Twitter. In:
Proceedings of the 20th International Conference on World Wide Web. pp. 675–
684. WWWZ’11, Hyderabad, India (2011)
20. Chen, C., Wu, K., Srinivasan, V., Zhang, X.: Battling the Internet Water Army:
detection of hidden paid posters. In: Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. pp.
116–120. ASONAM ’13, Niagara, Ontario, Canada (2013)
21. Da San Martino, G., Barrón-Cedeño, A., Wachsmuth, H., Petrov, R., Nakov, P.:
SemEval-2020 task 11: Detection of propaganda techniques in news articles. In:
Proceedings of the International Workshop on Semantic Evaluation. SemEval ’20,
Barcelona, Spain (2020)
22. Da San Martino, G., Barron-Cedeno, A., Nakov, P.: Findings of the NLP4IF2019 shared task on fine-grained propaganda detection. In: Proceedings of the 2nd
Workshop on NLP for Internet Freedom (NLP4IF): Censorship, Disinformation,
and Propaganda. pp. 162–170. NLP4IFEMNLP ’19, Hong Kong, China (2019)
23. Da San Martino, G., Cresci, S., Barrón-Cedeño, A., Yu, S., Di Pietro, R., Nakov, P.:
A survey on computational propaganda detection. In: Proceedings of the International Joint Conference on Artificial Intelligence and the Pacific Rim International
Conference on Artificial Intelligence. IJCAI-PRICAI ’20, Yokohama, Japan (2020)
24. Da San Martino, G., Shaar, S., Zhang, Y., Yu, S., Barrón-Cedeño, A., Nakov, P.:
Prta: A system to support the analysis of propaganda techniques in the news. In:
Proceedings of the Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 287–293. ACL ’20 (2020)
25. Da San Martino, G., Yu, S., Barron-Cedeno, A., Petrov, R., Nakov, P.: Fine-grained
analysis of propaganda in news articles. In: Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing. pp. 5636–5646. EMNLP ’19,
Hong Kong, China (2019)
26. Darwish, K., Alexandrov, D., Nakov, P., Mejova, Y.: Seminar users in the Arabic Twitter sphere. In: Proceedings of the 9th International Conference on Social
Informatics. pp. 91–108. SocInfo ’17, Oxford, UK (2017)
27. Darwish, K., Aupetit, M., Stefanov, P., Nakov, P.: Unsupervised user stance detection on twitter. In: Proceedings of the International AAAI Conference on Web
and Social Media. pp. 141–152. ICWSM ’20, Atlanta, GA, USA (2020)
28. Derczynski, L., Bontcheva, K., Liakata, M., Procter, R., Wong Sak Hoi, G., Zubiaga, A.: SemEval-2017 Task 8: RumourEval: Determining rumour veracity and
support for rumours. In: Proceedings of the 11th International Workshop on Semantic Evaluation. pp. 60–67. SemEval ’17, Vancouver, Canada (2017)
29. Dinkov, Y., Ali, A., Koychev, I., Nakov, P.: Predicting the leading political ideology of Youtube channels using acoustic, textual and metadata information. In:
Proceedings of the 20th Annual Conference of the International Speech Communication Association. pp. 501–505. INTERSPEECH ’19, Graz, Austria (2019)
30. Dinkov, Y., Koychev, I., Nakov, P.: Detecting toxicity in news articles: Application
to Bulgarian. In: Proceedings of the International Conference on Recent Advances
in Natural Language Processing. pp. 247–258. RANLP ’19, Varna, Bulgaria (2019)

Can We Spot the “Fake News” Before It Was Even Written?

11

31. Elsayed, T., Nakov, P., Barrón-Cedeño, A., Hasanain, M., Suwaileh, R., Atanasova,
P., Da San Martino, G.: CheckThat! at CLEF 2019: Automatic identification and
verification of claims. In: Proceedings of the 41st European Conference on Information Retrieval (ECIR’19). pp. 309–315. ECIR ’19, Cologne, Germany (2019)
32. Elsayed, T., Nakov, P., Barrón-Cedeño, A., Hasanain, M., Suwaileh, R., Da San
Martino, G., Atanasova, P.: Overview of the CLEF-2019 CheckThat!: Automatic
identification and verification of claims. In: Experimental IR Meets Multilinguality,
Multimodality, and Interaction. LNCS, Springer, Lugano, Switzerland (2019)
33. Finberg, H., Stone, M.L., Lynch, D.: Digital journalism credibility study. Online
News Association. Retrieved November 3, 2003 (2002)
34. Gencheva, P., Nakov, P., Màrquez, L., no, A.B.C., Koychev, I.: A context-aware
approach for detecting worth-checking claims in political debates. In: Proceedings
of the 2017 International Conference on Recent Advances in Natural Language
Processing. RANLP ’17, Varna, Bulgaria (September 2017)
35. Gorrell, G., Kochkina, E., Liakata, M., Aker, A., Zubiaga, A., Bontcheva, K.,
Derczynski, L.: SemEval-2019 task 7: RumourEval, determining rumour veracity
and support for rumours. In: Proceedings of the 13th International Workshop on
Semantic Evaluation. pp. 845–854. SemEval ’19, Minneapolis, Minnesota, USA
(2019)
36. Hardalov, M., Koychev, I., Nakov, P.: In search of credible news. In: Proceedings of
the 17th International Conference on Artificial Intelligence: Methodology, Systems,
and Applications. pp. 172–180. AIMSA ’16, Varna, Bulgaria (2016)
37. Hardalov, M., Koychev, I., Nakov, P.: In search of credible news. In: Proceedings of
the 17th International Conference on Artificial Intelligence: Methodology, Systems,
and Applications. pp. 172–180. AIMSA ’16, Varna, Bulgaria (2016)
38. Hasanain, M., Haouari, F., Suwaileh, R., Ali, Z., Hamdan, B., Elsayed, T., BarrónCedeño, A., Da San Martino, G., Nakov, P.: Overview of the CLEF-2020 CheckThat! lab on automatic identification and verification of claims in social media:
Arabic tasks. In: Working Notes of CLEF 2020—Conference and Labs of the Evaluation Forum. CLEF ’2020, Thessaloniki, Greece (2020)
39. Hasanain, M., Suwaileh, R., Elsayed, T., Barrón-Cedeño, A., Nakov, P.: Overview
of the CLEF-2019 CheckThat! Lab on Automatic Identification and Verification
of Claims. Task 2: Evidence and Factuality. In: Cappellato, L., Ferro, N., Losada,
D., Müller, H. (eds.) CLEF 2019 Working Notes. Working Notes of CLEF 2019
- Conference and Labs of the Evaluation Forum. CEUR Workshop Proceedings,
CEUR-WS.org, Lugano, Switzerland (2019)
40. Jaradat, I., Gencheva, P., Barrón-Cedeño, A., Màrquez, L., Nakov, P.: ClaimRank:
Detecting check-worthy claims in Arabic and English. In: Proceedings of the 16th
Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 26–30. NAACL-HLT ’18,
New Orleans, LA, USA (2018)
41. Jiang, S., Wilson, C.: Linguistic signals under misinformation and fact-checking:
Evidence from user comments on social media. Proc. ACM Hum.-Comput. Interact.
2(CSCW) (Nov 2018)
42. Karadzhov, G., Gencheva, P., Nakov, P., Koychev, I.: We built a fake news &
click-bait filter: What happened next will blow your mind! In: Proceedings of the
International Conference on Recent Advances in Natural Language Processing.
RANLP ’17, Varna, Bulgaria (2017)
43. Karadzhov, G., Nakov, P., Màrquez, L., Barrón-Cedeño, A., Koychev, I.: Fully automated fact checking using external sources. In: Proceedings of the International

12

44.

45.

46.

47.
48.

49.

50.

51.

52.

53.

54.

55.

56.

57.

P. Nakov
Conference on Recent Advances in Natural Language Processing. pp. 344–353.
RANLP ’17, Varna, Bulgaria (2017)
Kopev, D., Ali, A., Koychev, I., Nakov, P.: Detecting deception in political debates using acoustic and textual features. In: Proceedings of the IEEE Automatic
Speech Recognition and Understanding Workshop. pp. 652–659. ASRU ’19, Singapore (2019)
Kulkarni, V., Ye, J., Skiena, S., Wang, W.Y.: Multi-view models for political ideology detection of news articles. In: Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing. EMNLP ’18, Brussels, Belgium (2018)
Lazer, D.M., Baum, M.A., Benkler, Y., Berinsky, A.J., Greenhill, K.M., Menczer,
F., Metzger, M.J., Nyhan, B., Pennycook, G., Rothschild, D., Schudson, M., Sloman, S.A., Sunstein, C.R., Thorson, E.A., Watts, D.J., Zittrain, J.L.: The science
of fake news. Science 359(6380), 1094–1096 (2018)
Li, Y., Gao, J., Meng, C., Li, Q., Su, L., Zhao, B., Fan, W., Han, J.: A survey on
truth discovery. SIGKDD Explor. Newsl. 17(2), 1–16 (Feb 2016)
Ma, J., Gao, W., Mitra, P., Kwon, S., Jansen, B.J., Wong, K.F., Cha, M.: Detecting
rumors from microblogs with recurrent neural networks. In: Proceedings of the 25th
International Joint Conference on Artificial Intelligence. pp. 3818–3824. IJCAI ’16,
New York, NY, USA (2016)
Mihaylov, T., Georgiev, G., Nakov, P.: Finding opinion manipulation trolls in news
community forums. In: Proceedings of the Nineteenth Conference on Computational Natural Language Learning. pp. 310–314. CoNLL ’15, Beijing, China (2015)
Mihaylov, T., Koychev, I., Georgiev, G., Nakov, P.: Exposing paid opinion manipulation trolls. In: Proceedings of the International Conference Recent Advances in
Natural Language Processing. pp. 443–450. RANLP ’15, Hissar, Bulgaria (2015)
Mihaylov, T., Mihaylova, T., Nakov, P., Màrquez, L., Georgiev, G., Koychev, I.:
The dark side of news community forums: Opinion manipulation trolls. Internet
Research 28(5), 1292–1312 (2018)
Mihaylov, T., Nakov, P.: Hunting for troll comments in news community forums.
In: Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics. pp. 399–405. ACL ’16, Berlin, Germany (2016)
Mihaylova, T., Karadjov, G., Pepa, A., Baly, R., Mohtarami, M., Barrón-Cedeño,
A., Da San Martino, G., Nakov, P.: SemEval-2019 task 8: Fact checking in community question answering forums. In: Proceedings of the International Workshop
on Semantic Evaluation. SemEval ’19, Minneapolis, MN, USA (2019)
Mihaylova, T., Nakov, P., Màrquez, L., Barrón-Cedeño, A., Mohtarami, M.,
Karadjov, G., Glass, J.: Fact checking in community forums. In: Proceedings
of the Thirty-Second AAAI Conference on Artificial Intelligence. pp. 5309–5316.
AAAI ’18, New Orleans, Louisiana, USA (2018)
Mohtarami, M., Baly, R., Glass, J., Nakov, P., Màrquez, L., Moschitti, A.: Automatic stance detection using end-to-end memory networks. In: Proceedings of the
16th Annual Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. pp. 767–776. NAACLHLT ’18, New Orleans, LA, USA (2018)
Mohtarami, M., Glass, J., Nakov, P.: Contrastive language adaptation for crosslingual stance detection. In: Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing. pp. 4442–4452. EMNLP ’19, Hong Kong,
China (2019)
Nakov, P., Barrón-Cedeño, A., Elsayed, T., Suwaileh, R., Màrquez, L., Zaghouani,
W., Atanasova, P., Kyuchukov, S., Da San Martino, G.: Overview of the CLEF2018 CheckThat! Lab on automatic identification and verification of political

Can We Spot the “Fake News” Before It Was Even Written?

58.

59.

60.

61.

62.

63.

64.

65.

66.

67.
68.

69.

13

claims. In: Proceedings of the Ninth International Conference of the CLEF Association: Experimental IR Meets Multilinguality, Multimodality, and Interaction.
pp. 372–387. Lecture Notes in Computer Science, Springer, Avignon, France (2018)
Nakov, P., Barrón-Cedeño, A., Elsayed, T., Suwaileh, R., Màrquez, L., Zaghouani,
W., Gencheva, P., Kyuchukov, S., Da San Martino, G.: CLEF-2018 lab on automatic identification and verification of claims in political debates. In: Working
Notes of CLEF 2018 – Conference and Labs of the Evaluation Forum. pp. 372–387.
CLEF ’18, Avignon, France (2018)
Nakov, P., Mihaylova, T., Màrquez, L., Shiroya, Y., Koychev, I.: Do not trust
the trolls: Predicting credibility in community question answering forums. In: Proceedings of the International Conference on Recent Advances in Natural Language
Processing. RANLP ’17, Varna, Bulgaria (2017)
Nguyen, V.H., Sugiyama, K., Nakov, P., Kan, M.Y.: FANG: Leveraging social
context for fake news detection using graph representation. In: Proceedings of
the ACM International Conference on Information and Knowledge Management.
CIKM ’20 (2020)
Potthast, M., Kiesel, J., Reinartz, K., Bevendorff, J., Stein, B.: A stylometric
inquiry into hyperpartisan and fake news. In: Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics. pp. 231–240. NAACLHLT ’18, Melbourne, Australia (2018)
Rosenthal, S., Atanasova, P., Karadzhov, G., Zampieri, M., Nakov, P.: A largescale semi-supervised dataset for offensive language identification. ArXiv preprint
2004.14454 (2020)
Saleh, A., Baly, R., Barrón-Cedeño, A., Da San Martino, G., Mohtarami, M.,
Nakov, P., Glass, J.: Team QCRI-MIT at SemEval-2019 task 4: Propaganda analysis meets hyperpartisan news detection. In: Proceedings of the 13th International
Workshop on Semantic Evaluation. pp. 1041–1046. SemEval ’19, Minneapolis, Minnesota, USA (2019)
Shaar, S., Babulkov, N., Da San Martino, G., Nakov, P.: That is a known lie:
Detecting previously fact-checked claims. In: Proceedings of the Annual Meeting
of the Association for Computational Linguistics. pp. 3607–3618. ACL ’20 (2020)
Shaar, S., Nikolov, A., Babulkov, N., Alam, F., Barrón-Cedeño, A., Elsayed, T.,
Hasanain, M., Suwaileh, R., Haouari, F., Da San Martino, G., Nakov, P.: Overview
of the CLEF-2020 CheckThat! lab on automatic identification and verification of
claims in social media: English tasks. In: Working Notes of CLEF 2020—Conference
and Labs of the Evaluation Forum. CLEF ’2020, Thessaloniki, Greece (2020)
Shaprin, D., Da San Martino, G., Barrón-Cedeño, A., Nakov, P.: Team jack ryder
at SemEval-2019 task 4: Using BERT representations for detecting hyperpartisan
news. In: Proceedings of the 13th International Workshop on Semantic Evaluation.
pp. 1012–1015. SemEval ’19, Minneapolis, Minnesota, USA (2019)
Shu, K., Sliva, A., Wang, S., Tang, J., Liu, H.: Fake news detection on social media:
A data mining perspective. SIGKDD Explor. Newsl. 19(1), 22–36 (Sep 2017)
Staykovski, T., Barrón-Cedeño, A., Da San Martino, G., Nakov, P.: Dense vs.
sparse representations for news stream clustering. In: Proceedings of the Second International Workshop on Narrative Extraction from Texts. pp. 47–52.
Text2Story@ECIR ’19, Cologne, Germany (2019)
Stefanov, P., Darwish, K., Atanasov, A., Nakov, P.: Predicting the topical stance
and political leaning of media using tweets. In: Proceedings of the Annual Meeting
of the Association for Computational Linguistics. pp. 527–537. ACL ’20 (2020)

14

P. Nakov

70. Tchechmedjiev, A., Fafalios, P., Boland, K., Gasquet, M., Zloch, M., Zapilko, B.,
Dietze, S., Todorov, K.: ClaimsKG: A knowledge graph of fact-checked claims. In:
The Semantic Web – ISWC 2019. pp. 309–324. Springer International Publishing,
Cham (2019)
71. Thorne, J., Vlachos, A.: Automated fact checking: Task formulations, methods and
future directions. In: Proceedings of the 27th International Conference on Computational Linguistics. pp. 3346–3359. COLING ’18, Santa Fe, NM, USA (2018)
72. Thorne, J., Vlachos, A., Christodoulopoulos, C., Mittal, A.: FEVER: a large-scale
dataset for fact extraction and VERification. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 809–819. NAACL-HLT ’18, New Orleans,
LA, USA (2018)
73. Thorne, J., Vlachos, A., Cocarascu, O., Christodoulopoulos, C., Mittal, A.: The
second Fact Extraction and VERification (FEVER2.0) shared task. In: Proceedings
of the Second Workshop on Fact Extraction and VERification. pp. 1–6. FEVER ’19,
Hong Kong, China (2019)
74. Vasileva, S., Atanasova, P., Mrquez, L., Barrn-Cedeo, A., Nakov, P.: It takes nine
to smell a rat: Neural multi-task learning for check-worthiness prediction. In: Proceedings of the International Conference on Recent Advances in Natural Language
Processing. pp. 1229–1239. RANLP ’19, Varna, Bulgaria (2019)
75. Vosoughi, S., Roy, D., Aral, S.: The spread of true and false news online. Science
359(6380), 1146–1151 (2018)
76. Yu, S., Da San Martino, G., Nakov, P.: Experiments in detecting persuasion techniques in the news. In: Proceedings of the NeurIPS 2019 Joint Workshop on AI for
Social Good. NeurIPS ’19, Vancouver, Canada (2019)
77. Zampieri, M., Malmasi, S., Nakov, P., Rosenthal, S., Farra, N., Kumar, R.: Predicting the type and target of offensive posts in social media. In: Proceedings
of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1415–1420.
NAACL-HLT ’19, Minneapolis, MN, USA (2019)
78. Zampieri, M., Malmasi, S., Nakov, P., Rosenthal, S., Farra, N., Kumar, R.:
SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval). In: Proceedings of the International Workshop on Semantic Evaluation. pp. 75–86. SemEval ’19, Minneapolis, MN, USA (2019)
79. Zampieri, M., Nakov, P., Rosenthal, S., Atanasova, P., Karadzhov, G., Mubarak,
H., Derczynski, L., Pitenis, Z., Çöltekin, c.: SemEval-2020 task 12: Multilingual offensive language identification in social media. In: Proceedings of the International
Workshop on Semantic Evaluation. SemEval ’20, Barcelona, Spain (2020)
80. Zhang, Y., Martino, G.D.S., Barrn-Cedeo, A., Romeo, S., An, J., Kwak, H.,
Staykovski, T., Jaradat, I., Karadzhov, G., Baly, R., Darwish, K., James Glass,
P.N.: Tanbih: Get to know what you are reading. In: Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing. pp. 223–228.
EMNLP ’19, Hong Kong, China (2019)
81. Zlatkova, D., Nakov, P., Koychev, I.: Fact-checking meets fauxtography: Verifying
claims about images. In: Proceedings of the Conference on Empirical Methods
in Natural Language Processing. pp. 2099–2108. EMNLP ’19, Hong Kong, China
(2019)
82. Zubiaga, A., Liakata, M., Procter, R., Wong Sak Hoi, G., Tolmie, P.: Analysing how
people orient to and spread rumours in social media by looking at conversational
threads. PLoS ONE 11(3), 1–29 (03 2016)

