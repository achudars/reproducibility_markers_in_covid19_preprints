Monitoring Depression Trend on Twitter
during the COVID-19 Pandemic
Yipeng Zhang1 , Hanjia Lyu1∗, Yubao Liu1∗, Xiyang Zhang2 , Yu Wang1 , Jiebo Luo1
1
University of Rochester, 2 University of Akron
{yzh232,hlyu5}@ur.rochester.edu

arXiv:2007.00228v2 [cs.SI] 2 Jul 2020

Abstract
The COVID-19 pandemic has severely affected people’s daily lives and caused tremendous economic loss worldwide. However,
its influence on people’s mental health conditions has not received as much attention. To
study this subject, we choose social media
as our main data resource and create by far
the largest English Twitter depression dataset
containing 2,575 distinct identified depression
users with their past tweets. To examine the
effect of depression on people’s Twitter language, we train three transformer-based depression classification models on the dataset,
evaluate their performance with progressively
increased training sizes, and compare the
model’s “tweet chunk”-level and user-level
performances. Furthermore, inspired by psychological studies, we create a fusion classifier that combines deep learning model scores
with psychological text features and users’ demographic information and investigate these
features’ relations to depression signals. Finally, we demonstrate our model’s capability
of monitoring both group-level and populationlevel depression trends by presenting two of its
applications during the COVID-19 pandemic.
We hope this study can raise awareness among
researchers and the general public of COVID19’s impact on people’s mental health.

1

Introduction

COVID-19, or the coronavirus disease of 2019, is
an infectious disease that reportedly originated in
China and has been spreading rapidly across the
globe in 2020. It was first identified on December
31st, 2019, and was officially declared as a pandemic by the World Health Organization (WHO)
on March 11th, 2020.1 As of May 29th, 2020,
COVID-19 has infected 216 countries, areas or territories with over 5.7 million confirmed cases and
∗

Equal contribution.
1
https://www.who.int/emergencies/diseases/novelcoronavirus-2019

3.5 hundred thousand confirmed deaths1 . In response to the pandemic, over 170 countries have
issued nationwide closure of educational facilities2 ,
and many governments have issued flight restrictions and stay-at-home-orders, affecting everyday
lives of people around the globe.
Multiple studies have investigated the economic
and social impacts of COVID-19 (Fernandes, 2020;
Baker et al., 2020; Nicola et al., 2020), but what
mental impact the drastic life changes bring to people and how to quantify it at the population level
are yet to be studied. Mental disorders are disturbing approximately 380 million people of all
ages worldwide.3 Previous studies have shown that
mental disorders lead to many negative outcomes
including suicide (Inskip et al., 1998; San Too et al.,
2019), but individuals who suffer from mental disorders are sometimes unwilling or ashamed to seek
help (Yoshikawa et al., 2017). Moreover, it is infeasible for psychological studies to obtain and track a
large sample of diagnosed individuals and perform
statistically significant numerical analysis.
In the past decade, people have been increasingly relying on social media platforms such as
Facebook, Twitter, and Instagram to express their
feelings. We believe that social media is a resourceful medium to mine information about general public’s mental health conditions. As shown in Figure 1, we use data from the ForSight by Crimson
Hexagonplot4 to plot the word frequencies of several mental disorders on Twitter, including “depression”, “PTSD”, “bipolar disorder”, “autism”, from
January 1st, 2020 to May 4th, 2020. Note that
we exclude false positive tweets that contain misleading phrases such as “economic depression” or
“great depression”. We notice a rapid growth of the
word frequencies of autism and depression starting
2

https://en.unesco.org/covid19/educationresponse
https://www.who.int/en/news-room/factsheets/detail/mental-disorders
4
https://www.brandwatch.com/
3

from March 17th, when the pandemic spread across
most of the globe1 . Since depression occurs substantially more frequently compared to the other
three mental disorders, we focus on understanding
COVID-19’s impact on depression in this study.

Figure 1: Density of Twitter coverage regarding “depression”, “ptsd”, “bipolar disorder”, and “autism”.

Previous studies have used n-gram language
models (Coppersmith et al., 2014), topic models
(Armstrong, 2018), and deep learning models such
as 1-dimensional convolutional neural networks
(CNN) and bidirectional long short-term memory
(BiLSTM) (Orabi et al., 2018) to classify depression at the user level using Twitter data. All these
works use small samples of fewer than 500 users.
Shen et al. (2017) extend previous studies by expanding the dataset to contain 1,402 depression
users and using a multimodal dictionary learning
approach to learn the latent features of the data.
In this study, we create a dataset of 5,150 Twitter
users, including half identified depression users and
half control users, along with their tweets within
past three months and their Twitter activity data.
We develop a chunking and regrouping method to
construct 32,420 tweet chunks, 250 words each
in the dataset. Recently, transformer-based deep
learning language models have achieved state-ofthe-art performance in multiple language modeling
tasks. We investigate the performance of some
of these models, including BERT (Devlin et al.,
2018), RoBERTa (Liu et al., 2019), and XLNet
(Yang et al., 2019) on our dataset. We progressively add data to our training set and notice a clear
performance growth on all models, which validates
the importance of our dataset. We compare the
models’ performance at the chunk level as well
as the user level, and observe further performance
gain which adds credibility to our chunking method.

Furthermore, we build a more accurate classification model upon the deep learning models along
with linguistic analysis of dimensions including
personality, Linguistic Inquiry and Word Count
(LIWC) - a well-validated psycholinguistic dictionary (Tausczik and Pennebaker, 2010), sentiment
features and demographic information.
De Choudhury et al. (2013a) demonstrate that
depression prediction models can potentially be
used on the population level. However, to the best
of our knowledge, all Twitter user depression identification studies reviewed above focus on either
tweet-level or user-level classification rather than
applying the model to analyzing the mental health
trends of a large population. The word frequency
trend shown in Figure 1 is apparently filled with
noise and lacks plausible explanation, making it
unable to reflect the actual mental health trends of
the population. In this study, we present two applications to monitor the change of depression level in
different groups as the disease propagates. We use
latent Dirichlet allocation (LDA) and psychological theories to analyze the trend we discover. To
the best of our knowledge, we are the first to apply
these transformer-based models to identifying depression users on Twitter using a large-scale dataset
and to monitoring the public depression trend.
In summary, our main contributions are:
• We develop an effective search method to
identify depression users on Twitter. Using this method, we are able to create by
far the largest dataset with 2,575 depression
users along with their past tweets within three
months. Our experiment shows that the performance of deep learning models increase as
the size of the training set grows.
• To the best of our knowledge, we are the
first to investigate the potential of transformerbased deep learning models on Twitter user
depression classification.
• We build a tool marrying the deep learning
models and psychological text analysis to further improve the classification performance.
• We build a pipeline to monitor public depression trend by aggregating individuals’ past
tweets within a time frame using our model,
and analyze the depression level trends during
COVID-19, thus shedding light on the psychological impacts of the pandemic.

2

Related Work

The potential of machine learning models for identifying Twitter users who have been diagnosed with
depression was pioneered by De Choudhury et al.
(2013b), who analyzed how features obtained by
LIWC are related to depression signals on social
media and how that can be used for user-level classification on a dataset containing 171 depression
users. The data was collected by designing surveys
to volunteers through crowdsourcing. Following
the work, Coppersmith et al. (2014) used LIWC, 1gram language model, character 5-gram model, and
user’s engagement on social media (user mention
rate, tweet frequency, etc.) to perform tweet-level
classification on a dataset containing 441 depression users.
The CLPsych 2015 Shared Task dataset containing 447 diagnosed depression users (Coppersmith
et al., 2015) was published in 2015 and was favored by a wide range of studies (Armstrong, 2018;
Nadeem, 2016; Jamil et al., 2017; Orabi et al.,
2018). The data was gathered by regular expression
search in tweets in combination with manual annotation. Among these studies, topic modeling was
shown effective by Armstrong (2018) using LDA;
performance of traditional machine learning classification algorithms (decision trees, SVM, naive
Bayes, logistic regression) on 1-grams and 2-grams
was investigated by Nadeem (2016); Jamil et al.
(2017) used SVM on bag of words (BOW) and
depression word count, along with LIWC features
and NRC sentiment features; Orabi et al. (2018) explored the performance of small deep neural architectures - 1-dimensional CNN and BiLSTM with
context-aware attention on the task.
Tsugawa et al. (2015) performed analysis of
models using BOW, LDA, and social media engagement features on a dataset containing 81 Japanesespeaking depression Twitter users collected by
crowdsourcing. One recent work (Shen et al., 2017)
proposed a multimodal dictionary learning method
that utilizes topic, social media engagement, profile
image, emotional features to learn a latent feature
dictionary that performed well on a dataset of 1402
depression users, the largest dataset used to the best
of our knowledge.

3

Data Collection and Analysis

We use the Tweepy API to obtain 41.3 million
tweets posted from March 23rd to April 18th and
the information of their authors. We look for sig-

nals that can tell whether the author suffers from
depression from both the text and the user profile description. Normally, Twitter users suffering from depression describe themselves as depression fighters in their descriptions. Some of
them may also post relevant tweets to declare that
they have been diagnosed with depression. Inspired by Coppersmith et al. (2014), we use regular expression to find these authors by examining
their tweets and descriptions. Building upon their
method, we further extend our regular expression
search based on some patterns we notice on manually identified depression users, in pursuit of efficacy. In tweets, we search for phrases such as “I
have/developed/got/suffer(ed) from X depression”,
“my X depression”, “I’m healing from X depression”, “I’m diagnosed with X depression”, where
X’s are descriptive words such as “severe” and “major”. In descriptions, we further add phrases such as
“depression fighter/sufferer/survivor” to the regular
expression list; we remove users that have “practitioner” and “counselor” in their descriptions to
exclude mental health practitioners.
Once we have found the targeted Twitter users,
we use the Tweepy API to retrieve the public tweets
posted by these users within last 3 months by the
time of posting the depression-related tweet with a
max cap of 200 tweets per user. If the user is identified from the description, we limit the time scope to
3 months by April 18th. In the end, 2,575 distinct
Twitter users are considered suffering from depression (DP, Depression). We randomly select another
2,575 distinct users such that depression-related
terms do not appear in their past 200 tweets or descriptions as our control group. Users in this group
are not considered to be suffering from depression
(ND, Non-Depression).
3.1

Personality

Previous psychological research shows that the
Big-Five personality traits (openness, conscientiousness, extraversion, agreeableness and neuroticism) are related to depression (Bagby et al., 1996).
We estimate individuals’ personality scores using
IBM’s Personality Insights service.5 For each individual, we aggregate all their tweets into a single
textual input and call the Personality Insights API
to obtain the scores. The minimum number of
words for using the API is 100 and we are able to
5
https://www.ibm.com/cloud/watson-personalityinsights.

retrieve 4,697 (91.2%) of the 5,150 users’ scores.
Summary statistics are shown in Appendix A.
3.2

Sentiments

Besides personality, we hypothesize that individuals’ sentiment and emotions could also reflect
whether they are experiencing depression or not.
We estimate individuals’ sentiments using Valence Aware Dictionary and Entiment Reasoner
(VADER). VADER is a lexicon and rule-based
model developed by researchers from Georgia Institute of Technology (Hutto and Gilbert, 2014). We
aggregate a users tweets into a single chunk, apply
VADER, and retrieve its scores for positive and
negative emotions. In Figure 2, we report the distributions of positive emotions and negative emotions
among DP and ND, respectively.
Positive Emotions

Negative Emotions

Depression
Non-Depression

0.0

0.2

0.4

0.6

0.8

1.0

Depression
Non-Depression

0.0

0.1

0.2

0.3

0.4

0.5

0.6

Figure 2: Compared with individuals with no depression, those with depression tend to exhibit both
stronger positive and negative emotions.

3.3

Demographics

Previous psychological studies show differences
in depression rates among people of different ages
and of different genders (Mirowsky and Ross, 1992;
Wang et al., 2020; Wainwright and Surtees, 2002).
To estimate the age and gender of the user, we adopt
the M3-inference model proposed by Wang et al.
(2019). The M3 model performs multimodal analysis on a user’s profile image, username, and description. Following M3’s structure, we label each
user a binary gender label (as approximation) and
a one-hot age label among four age intervals (≤18,
19-29, 30-39, ≥40), which are then used in our
fusion model. We are able to retrieve 5,059/5,150
(98.2%) users’ demographic information.

and Pennebaker, 2001; Rude et al., 2004), as well
as 7 other features that we find relevant to our
study. Similar to the method conducted in (Chen
et al., 2020), we apply LIWC to the concatenated
tweets of individuals. Figure 3 shows the linguistic profiles for the tweets of DP and ND. Both DP
and ND exhibit slightly positive tone, with negligible differences. The tweets of ND show more
analytical thinking, more clout and less authentic
expression than those of DP. The tweets of DP
score higher in both positive and negative emotion categories than the ones of ND in which suggests a higher degree of immersion (Holmes et al.,
2007). Moreover, the tweets of DP also show more
anxiety, anger emotions and include more “swear”
words, consistent with the findings of Coppersmith
et al. (2014). Death-related words appear more frequently in the tweets of DP, which echoes (Stirman
and Pennebaker, 2001). Similar to Coppersmith
et al. (2014); Stirman and Pennebaker (2001), we
find more first-person singular in the tweets of DP.
We also find that the tweets of DP express more
sadness emotion and use words related to the biological process more frequently. While there is
no clear link between biological-process-related
words and depression, this finding shows that people who suffer from depression may pay more attention to their biological statuses. The “power” score
for the tweets of ND is higher which reflects a
higher need for the power according to the findings
of McClelland (1979). By comparing the “work”
scores of DP and ND, we find that the users of ND
pay more attention to work-related issues as well.

Figure 3: Linguistic profiles for the DP/ND tweets.

3.5
3.4

Linguistic Inquiry Word Count (LIWC)

We use LIWC to capture people’s psychological
states by analyzing the contents of their tweets6 .
We choose 8 features that were analyzed in previous works (Coppersmith et al., 2014; Stirman
6

https://liwc.wpengine.com/how-it-works/

Social Media Engagement

We use the proportion of tweets with mentions,
number of responses, unique users mentions, users
mentions, and tweets to measure the social media engagement of each user, as did Coppersmith
et al. (2014). Our analysis details are reported in
Appendix A.

4
4.1

Model
Tweet Chunking and Preprocessing

We perform stratified random sampling on our
dataset. We first sample 500 users to form our
testing set. On the rest of the users, we progressively add users to the training sets and record the
performance of the models trained on sets of 1,000,
2,000, and 4,650 users, respectively. All the training and testing sets have a 1-1 DP-ND ratio.
Jamil et al. (2017) have shown that one single
tweet does not contain enough signals to determine whether a user has depression. Thus, we
concatenate consecutive tweets of the same user
together to create tweet chunks of 250 words and
label the chunks based on the user’s label. We
preprocess the text using the tweet preprocessing
pipeline proposed by Baziotis et al. (2017). We
adopt this method especially due to its capability of
marking Twitter-specific text habits and converting
them to special tokens such as “hallcapsi” (capitalized words), “helongatedi” (repeated letters),
“hrepeatedi” (repeated words), etc. For example,
YESSSSS, I love it so so much!!!
after preprocessing will be in the form of
Yes hallcapsi helongatedi, I love it so
hrepeatedi much! helongatedi
After chunking and preprocessing, on average, each
user has 6-7 text chunks, making the actual sizes
of the 4,650-user train-val set and the 500-user
testing set to be 29,315 and 3,105, respectively.
The preprocessed tweet chunk datasets are then
passed to deep learning models for training.
4.2

Deep Learning Models

We use deep learning models to perform chunklevel classification. We set up 2 baseline models,
multi-channel CNN and bidirectional LSTM with
context-aware attention (Attention BiLSTM), as
described in Orabi et al. (2018). We use the pretrained GloVe embedding (840B tokens, 300d vectors) (Pennington et al., 2014) augmented with the
special tokens added during preprocessing. The
embedding weights are further trained jointly with
the model. We also train three representative
transformer-based sequence classification models
- BERT, RoBERTa and XLNet - with their own
pretrained tokenizers augmented with the special
tokens for tokenization. We choose to use the base
models for all of them since we find no noticeable
performance gains using their larger counterparts.

4.3

Signal Fusion

We run the models on all the tweet chunks of the
same user and take average of the confidence scores
to get the user-level confidence score. There are
4,163/4,650 (89.5%) users remaining in the training
set and 446/500 (89.2%) in the testing set whose
entire features are retrievable. We then pass different combinations of user-level scores (personality,
VADER, demographics, engagement, LIWC, and
average confidence) to machine learning classification algorithms including random forest, logistic
regression, and support vector machines (SVM)
provided by the scikit-learn library.7 We only use
the explainable LIWC features mentioned in the
data collection section for training the classifiers.
4.4

Training Details

During training, we randomly split the train-val
set to training and validation sets with a ratio of
9:1. We record the models’ performances on the
validation set after each epoch and keep the model
with the highest accuracy and F1 scores while training until convergence. Hyper-parameters and other
experimental settings are described in Appendix B.

5

Experimental Results

5.1

Chunk-Level Classification

In Table 1, we report our classification results at
the chunk level on the testing set. Our evaluation
metrics include accuracy, F1 score, AUC, precision and recall.8 One immediate observation is
that regardless of the model type, the classification
performance improves as we increase the size of
our train-val set. This shows that for building depression classification models it is imperative to
have a large number of training samples. At the
same time, it also confirms that the larger number
of training samples in our experiments is indeed an
advantage.
Another observation is the performance gain of
transformer-based models over BiLSTM and CNN
models. The CNN model slightly outperforms BiLSTM, which replicates the findings of Orabi et al.
(2018). We observe BERT, RoBERTa and XLnet
invariably outperform BiLSTM and CNN regardless of the size of our training set. In particular,
the XLNet model records the best AUC as well as
accuracy of all models when trained with our full
training set.
7
8

https://scikit-learn.org/stable/
We use 0.5 as the threshold when calculating the scores.

Model
Attention BiLSTM
CNN
BERT
RoBERTa
XLNet

Train-Val Set
1k users
2k users
4.65k users
1k users
2k users
4.65k users
1k users
2k users
4.65k users
1k users
2k users
4.65k users
1k users
2k users
4.65k users

Accuracy
70.7
70.3
72.7
71.8
72.8
74.0
72.7
75.7
76.5
74.4
75.9
76.2
73.7
74.6
77.1

F1
69.0
68.3
71.6
72.6
74.5
70.9
74.4
76.3
77.5
75.7
77.9
78.0
75.1
76.8
77.9

AUC
76.5
77.4
79.3
77.4
80.3
81.0
79.8
82.9
83.9
82.0
83.2
84.1
80.7
82.6
84.4

Precision
70.9
70.7
72.1
72.7
72.2
77.4
72.0
76.1
76.3
74.2
73.8
74.4
73.2
72.6
77.5

Recall
67.3
66.1
71.1
72.6
76.9
68.9
76.9
75.7
78.8
77.3
82.5
81.9
77.2
81.5
78.3

Table 1: Chunk-level performance (%) of all 5 different models using training-validation sets of different sizes.

5.2

User-Level Classification
Features
VADER
Demographics
Engagement
Personality
LIWC
V+D+E+P+L
XLNet
All (Rand. Forest)
All (Log. Reg.)
All (SVM)

Accuracy
54.9
58.7
58.7
64.8
70.6
71.5
78.1
78.4
78.3
78.9

F1
61.7
56.0
62.3
67.8
70.8
72.0
77.9
78.1
78.5
79.2

AUC
54.6
61.4
61.7
72.4
76.0
78.3
84.9
84.9
86.4
86.1

Table 2: User-level performance (%) using different
features. We use SVM for classifying individual features.

Next, we report our experiment results at the
user level. Since XLNet trained on the 4,650-user
dataset outperforms other models, we take it for
user-level performance comparison. Our experimental results demonstrate a huge increase on
the user-level scores of XLNet shown in Table 2
compared to the chunk-level score shown in Table 1. This indicates that more textual information
of a user yields more reliable results on determining whether the user has depression. Building on
the user-level XLNet scores, we further include
VADER (V), demographics (D), engagement (E),
personality (P), and LIWC (L) scores as signals.
We first use all features and compare the performance of random forest, logistic regression, and
SVM. We notice that SVM achieves the best scores
on accuracy and F1, slightly surpassing logistic
regression. Thus, we use SVM for testing the performance when using part of the features collected.
The results are shown in Table 2. Results have
shown that using VADER, demographics, and social media engagement features alone does not help

the classification by much. Classifiers using personality features and LIWC features perform relatively
better. We then use all these five feature groups and
obtain a better result (accuracy 71.5%, F1 72.0%).
However, the classifier is still outperformed by XLNet, showing that the transformer-based models
indeed works better on depressive Twitter text modeling compared with other approaches. We further
increase the classifier’s performance by using all
the features, namely, VDEPL features and the averaged XLNet confidence score; the performance
of the three ML algorithms does not vary much
and the SVM classifier achieves the best accuracy
(78.9%) and F1 (79.2%) scores.

0.030

Personality
VADER
Social Engagement
LIWC

0.025
0.020
0.015
0.010
0.005
0.000

ss ss n ss m g u s d ts s s s s tic ut tic i o o x er d ar th io er rk
ne ne sio ne cis ne ne popountween mtnonser mtnr mtnnaly clothen osemegem anang saswe dea bpow wo
en us ver le oti
o p s s a au
p n
opentio xtraeeabneur
com #porti# resue u # u
e
r
q
sci ag
pro # uni
con

Figure 4: Permutation importance of different features.

In an attempt to investigate what specific textual
features besides those extracted by XLNet have the
most impact on depression classification, we calculate the permutation feature importance (Altmann
et al., 2010) on the trained random forest classifier
using the VEPL features with 10 repeats. The importance scores of individual features are shown in
Figure 4. Among the LIWC features, “i”, “bio”,

“power”, “sad”, “authentic”, “clout” and “analytic”
are shown to be important in classification. Among
the five personality features, “conscientiousness”
and “neuroticism” are shown to be closely related
to depression cues. We do not observe a strong
relation between VADER sentiment features or social media engagement features and the depression
signals. As for the LIWC sentiment features, only
“sad” and “anxiety” are shown to be relatively important; we hypothesize that the ideas a person tries
to express on Twitter are more related to depression
cues than his/her sentiments, which are likely to
have only short-term effects.

6

Applications

In this section, we report two COVID-19 related
applications of our XLNet based depression classifier: (a) monitoring the evolution of depression
level among the depression group and the nondepression group, and (b) monitoring the depression level at the U.S. country level and state level
during the pandemic.
6.1

Depression Monitoring on DP/ND Group

We take the 500 users from the testing set along
with their tweets from January 1st, 2020 to May
22nd, 2020. We concatenate a user’s tweets consecutively from January 1st one by one until reaching
250 words and label this chunk’s date as the date
of the author posting the tweet that is in the middle of the chunk. We group three days into a bin
from January 1st and assign the chunks to the bins
according to the labeled date. We run the XLNet
model on the preprocessed tweet chunks and record
the confidence scores. We trim the upper and lower
10% of the data to reduce the skew in the score distribution. We then take the mean of the scores for
each time bin and plot the depression trend shown
in Figure 5a. We further take a moving average of
5 time bins to smooth the curves.
We mark three important time points on the plot
- the first confirmed case of COVID-19 in the U.S.
(January 21st), U.S. National Emergency announcement (March 13th), and the last stay-at-home order issued (South Carolina, April 7th). In January,
both groups experience a drop in depression scores.
This may be caused by the fact that people’s mood
usually hits its lowest in winter (Thompson et al.,
1988). From the day when there was the first confirmed case in the U.S. to the day of the announcement of U.S. National Emergency, the trends of
DP and ND are different. The depression level of

DP goes down slightly while the depression level
of ND goes up. Aided by psychological findings,
we hypothesize that depressive users are less affected by negative events happening in the outside
world because they focus on their own feelings and
life events, since (a) they are mostly affected by
negative events that threaten them directly (Yue
et al., 2016), and (b) more interactions with the
outside world give them more negative feedback
(Winer and Salem, 2016). Moreover, the depression levels of DP and ND both increase after the
announcement of the U.S. National Emergency.
To better understand the trend, we apply the LDA
model to retrieve the topics before and after the announcement of the U.S. National Emergency. Each
chunk of the tweets is assigned 5 weights for the 5
topics respectively. We label the topic of the highest weight as the dominant topic of this chunk of
the tweets, and count the frequency of each topic
shown in Figure 5b. Details about the keywords of
the topics are reported in Appendix C. Before the
announcement, the 2 most frequent topics of DP
and ND are the discussions about U.S. President
Donald Trump and about school and work. The
third most frequent topic of ND is about health,
while that of DP is about entertainment. This supports the difference of the depression level trends
of two groups. After the announcement of the U.S.
National Emergency, the most frequent topic of
DP is depression and anxiety during COVID-19,
while this is the third frequent topic of ND. Further,
all the 5 topics of each group are about COVID19. This shows that when people mostly talk about
COVID-19, depression signals rise for both groups.
6.2

Aggregated Depression in COVID-19

To investigate country- and state-level depression
trend during COVID-19, we randomly sample
users who have U.S. state locations stated in their
profiles and crawl their tweets between March 3rd,
2020 and May 22nd, 2020, the period right before
and after the U.S. announced National Emergency
on March 13th. Using the same logic as in Section
6.1, we plot the change of depression scores of all
geo-located 9,050 users as the country-level trend.
For state-level comparison, we plot the aggregated
scores of three representative states - economical
center New York on the East Coast that is highly
affected by the virus, tech center California on the
West Coast that is also struck hard by the virus,
and the less affected tourism center Florida in the

0.800

0.425

depression (DP)

non-depression (ND)

0.775

0.400

U.S. National
Emergency

0.375

First Confirmed Case
in U.S.

0.725

0.350

0.700

0.325

0.675

0.300

0.650

0.275

Last Stay-At-Home
Order

0.625
0.600

ND Users Depression Level

DP Users Depression Level

0.750

1/1/2020

1/25/2020

2/18/2020

3/13/2020

4/6/2020

4/30/2020

0.250
0.225
5/24/2020

(a) DP-ND trends

(b) Percentage of DP-ND topics

0.42

0.40

US
NY
CA
FL

Depression Level

0.38

U.S. National
Emergency
0.36

0.34

0.32

0.30

0.28

3/3/2020

3/18/2020

4/2/2020

4/17/2020

5/2/2020

5/17/2020

(c) State-level trends

(d) Percentage of State-level topics

Figure 5: (a) Aggregated depression level trends of DP users and ND users from January 1st, 2020 to May 22nd, 2020. We
use different y-axes for the 2 groups in order to compare them side by side. (b) Topics of DP and ND before and after the
announcement of the U.S. National Emergency. (c) Aggregated depression level trends of U.S., NY, CA, and FL from Mar 3rd,
2020 to May 22nd, 2020. (d) Top 5 topics (state-level) after the announcement of the U.S. National Emergency.

southeast. Each selected state has at least 550 users
in the dataset to validate our findings. Their depression levels are shown in Figure 5c.
The first observation of the plot is that depression scores of all three states and the U.S. behave
similarly during the pandemic; they experience a
decrease right before the National Emergency, a
steady increase after that, a slight decrease past
April 23rd, and another sharp increase after May
10th. We also notice that the overall depression
score of Florida is significantly lower than the U.S.
average as well as the other two states. Since
Florida has a lower score both before and after the
virus breakout, we hypothesize that it has a lower
depression level overall compared to the average
U.S. level irrespective of the pandemic.
We calculate the topics at the state level after the
announcement of the U.S. National Emergency. As
shown in the Figure 5d, the most frequent topic is
the government’s policy on COVID-19. California
and Florida are the states that pay relatively more
attention to this topic compared to the U.S. average
and New York. Florida also talks more about the
life change during COVID-19. Another finding
is that people in New York talk more about the

hospital news, likely because the state contains the
majority of cases in the country by May 22nd 9 .

7

Conclusion

COVID-19 has infected over five million people
globally, virtually brought the whole world to a halt.
During this period, social media witnessed a spike
in depression terms. Against this backdrop, we
have developed transformer-based models trained
with by far the largest dataset on depression. We
have analyzed our models’ performance in comparison to existing models and verified that the large
training set we compile is beneficial to improving
the models’ performance. We further show that our
models can be readily applied to the monitoring of
stress and depression trend of targeted groups over
geographical entities such as states. We notice significant increases in depression signals as people
talk more about COVID-19. We hope researchers
and mental health practitioners find our models useful and this study can raise the awareness of the
mental health impacts of the pandemic.
9
https://www.cdc.gov/coronavirus/2019-ncov/casesupdates/cases-in-us.html

References
André Altmann, Laura Toloşi, Oliver Sander, and
Thomas Lengauer. 2010. Permutation importance:
a corrected feature importance measure. Bioinformatics, 26(10):1340–1347.
William Armstrong. 2018. Using topic models to investigate depression on social media. Technical
report, Technical report, University of Maryland,
USA, 2015. Scholarly paper.
R Michael Bagby, Deborah R Schuller, Anthony J
Levitt, Russell T Joffe, and Kate L Harkness. 1996.
Seasonal and non-seasonal depression and the fivefactor model of personality. Journal of Affective Disorders, 38(2-3):89–95.
Scott R Baker, Nicholas Bloom, Steven J Davis, and
Stephen J Terry. 2020. Covid-induced economic uncertainty. Technical report, National Bureau of Economic Research.
Christos Baziotis, Nikos Pelekis, and Christos Doulkeridis. 2017. DataStories at SemEval-2017 task 4:
Deep LSTM with attention for message-level and
topic-based sentiment analysis. In Proceedings of
the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 747–754, Vancouver,
Canada. Association for Computational Linguistics.
Long Chen, Hanjia Lyu, Tongyu Yang, Yu Wang, and
Jiebo Luo. 2020. In the eyes of the beholder: Sentiment and topic analyses on social media use of
neutral and controversial terms for covid-19. arXiv
preprint arXiv:2004.10225.

Nuno Fernandes. 2020. Economic effects of coronavirus outbreak (covid-19) on the world economy.
Available at SSRN 3557504.
Danielle Holmes, Georg W Alpers, Tasneem Ismailji,
Catherine Classen, Talor Wales, Valerie Cheasty, Andrew Miller, and Cheryl Koopman. 2007. Cognitive
and emotional processing in narratives of women
abused by intimate partners.
Violence against
women, 13(11):1192–1205.
Clayton J Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model for sentiment analysis
of social media text. In Eighth international AAAI
conference on weblogs and social media.
Hazel Inskip, Clare Harris, and Brian Barraclough.
1998. Lifetime risk of suicide for affective disorder,
alcoholism and schizophrenia. The British Journal
of Psychiatry, 172(1):35–37.
Zunaira Jamil, Diana Inkpen, Prasadith Buddhitha, and
Kenton White. 2017. Monitoring tweets for depression to detect at-risk users. In Proceedings of the
Fourth Workshop on Computational Linguistics and
Clinical PsychologyFrom Linguistic Signal to Clinical Reality, pages 32–40.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.
David C McClelland. 1979. Inhibited power motivation and high blood pressure in men. Journal of Abnormal Psychology, 88(2):182.

Glen Coppersmith, Mark Dredze, and Craig Harman.
2014. Quantifying mental health signals in twitter.
In Proceedings of the workshop on computational
linguistics and clinical psychology: From linguistic
signal to clinical reality, pages 51–60.

John Mirowsky and Catherine E Ross. 1992. Age and
depression. Journal of health and social behavior,
pages 187–205.

Glen Coppersmith, Mark Dredze, Craig Harman,
Kristy Hollingshead, and Margaret Mitchell. 2015.
Clpsych 2015 shared task: Depression and ptsd on
twitter. In Proceedings of the 2nd Workshop on
Computational Linguistics and Clinical Psychology:
From Linguistic Signal to Clinical Reality, pages 31–
39.

Maria Nicola, Zaid Alsafi, Catrin Sohrabi, Ahmed
Kerwan, Ahmed Al-Jabir, Christos Iosifidis, Maliha
Agha, and Riaz Agha. 2020. The socio-economic
implications of the coronavirus and covid-19 pandemic: A review. International Journal of Surgery.

Moin Nadeem. 2016. Identifying depression on twitter.
arXiv preprint arXiv:1607.07384.

Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013a. Social media as a measurement tool
of depression in populations. In Proceedings of the
5th Annual ACM Web Science Conference, pages 47–
56.

Ahmed Husseini Orabi, Prasadith Buddhitha, Mahmoud Husseini Orabi, and Diana Inkpen. 2018.
Deep learning for depression detection of twitter
users. In Proceedings of the Fifth Workshop on
Computational Linguistics and Clinical Psychology:
From Keyboard to Clinic, pages 88–97.

Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013b. Predicting depression via social media. In Seventh international AAAI
conference on weblogs and social media.

Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Stephanie Rude, Eva-Maria Gortner, and James Pennebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121–1133.

Lay San Too, Matthew J Spittal, Lyndal Bugeja,
Lennart Reifels, Peter Butterworth, and Jane Pirkis.
2019. The association between mental disorders and
suicide: A systematic review and meta-analysis of
record linkage studies. Journal of affective disorders.
Guangyao Shen, Jia Jia, Liqiang Nie, Fuli Feng, Cunjun Zhang, Tianrui Hu, Tat-Seng Chua, and Wenwu
Zhu. 2017. Depression detection via harvesting social media: A multimodal dictionary learning solution. In IJCAI, pages 3838–3844.
Shannon Wiltsey Stirman and James W Pennebaker.
2001. Word use in the poetry of suicidal and nonsuicidal poets. Psychosomatic medicine, 63(4):517–
522.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and computerized text analysis methods. Journal of language
and social psychology, 29(1):24–54.
Chris Thompson, Deborah Stinson, Margaret Fernandez, Jeffrey Fine, and Geoffrey Isaacs. 1988. A
comparison of normal, bipolar and seasonal affective disorder subjects using the seasonal pattern assessment questionnaire. Journal of Affective Disorders, 14(3):257–264.
Sho Tsugawa, Yusuke Kikuchi, Fumio Kishino, Kosuke Nakajima, Yuichi Itoh, and Hiroyuki Ohsaki.
2015. Recognizing depression from twitter activity.
In Proceedings of the 33rd annual ACM conference
on human factors in computing systems, pages 3187–
3196.
NWJ Wainwright and Paul G Surtees. 2002. Childhood
adversity, gender and depression over the life-course.
Journal of affective disorders, 72(1):33–44.
Cuiyan Wang, Riyu Pan, Xiaoyang Wan, Yilin Tan,
Linkang Xu, Cyrus S Ho, and Roger C Ho. 2020.
Immediate psychological responses and associated
factors during the initial stage of the 2019 coronavirus disease (covid-19) epidemic among the general population in china. International journal of environmental research and public health, 17(5):1729.

Eisho Yoshikawa, Toshiatsu Taniguchi, Nanako
Nakamura-Taira, Shin Ishiguro, and Hiromichi Matsumura. 2017. Factors associated with unwillingness to seek professional help for depression: a webbased survey. BMC research notes, 10(1):673.
Li Yue, Zhang Dajun, Liang Yinghao, and Hu Tianqiang. 2016. Meta-analysis of the relationship between life events and depression in adolescents.
Journal of Pediatric Care, 2(1):1–13.

A

Supplementary Data Statistics

In Table 3, we report the summary statistics of the
personality estimates. We observe that all the estimates fall between 0 and 1. The standard deviations
range from 0.24 to 0.28. Openness has the highest
mean value at 0.61 and conscientiousness has the
lowest mean value at 0.28.
Count Min Max Mean SD
Openness
Conscientiousness
Extraversion
Agreeableness
Neuroticism

4697
4697
4697
4697
4697

N

4697

1.00
1.00
1.00
1.00
1.00

0.61
0.28
0.32
0.30
0.56

0.28
0.26
0.24
0.26
0.28

Table 3: Summary statistics of personality scores of
users in the data set. SD stands for standard deviation.

In Table 4, we further report the correlation coefficients between the personality variables. We
observe that extraversion is highly correlated with
conscientiousness and agreeableness (correlation
coefficients > 0.45). Meanwhile, neuroticism is
negatively correlated with openness, conscientiousness, and extraversion.
Openness

Zijian Wang, Scott A. Hale, David Adelani, Przemyslaw A. Grabowicz, Timo Hartmann, Fabian Flö”ck,
and David Jurgens. 2019. Demographic inference
and representative population estimates from multilingual social media data. In Proceedings of the
2019 World Wide Web Conference. ACM.

0.00
0.00
0.00
0.00
0.00

Openness

Conscientious Extraversion Agreeableness Neuroticism

1

Conscientious

0.238∗∗∗

1

Extraversion

0.265∗∗∗

0.475∗∗∗

∗∗∗

Agreeableness -0.0768
Neuroticism

-0.232∗∗∗

1

∗∗∗

0.441

0.469∗∗∗

1

-0.279∗∗∗

-0.285∗∗∗

0.0460∗∗

1

E Samuel Winer and Taban Salem. 2016. Reward
devaluation: Dot-probe meta-analytic evidence of
avoidance of positive information in depressed persons. Psychological bulletin, 142(1):18.

Table 4: Correlation between the personality variables.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for
language understanding. In Advances in neural information processing systems, pages 5754–5764.

To better understand the difference of social media engagement between DP and ND, we add 0.1
to the number of responses, unique users mentions,
users mentions, and tweets and take the logarithm.

∗

p < 0.05, ∗∗ p < 0.01, ∗∗∗ p < 0.001

Figure 6: Density plots of proportion of tweets with mentions, log-scale number of responses, unique users mentions, users mentions, and tweets.

The density comparisons are shown in Figure 6. By
applying the Mann-Whitney rank test, we find that
except for the number of unique users mentions,
other features are statistically different (p < 0.05)
between DP and ND. The users of DP post more
tweets and reply more. They tend to post fewer
tweets with mentions while the number the users
mentioned for DP is larger, which suggests that
when users of DP post tweets to interact with other
users, it involves more users.

B

Hyper-Parameters and Other
Experimental Settings

We manually select the hyper-parameters that give
the best accuracy and F1 scores on the deep learning models. We use Adam optimizer with learning
rate 7e-3, weight decay 1e-4 for training Attention
BiLSTM. We use Adam optimizer with a learning
rate of 5e-4 for training CNN. We use AdamW
optimizer with a learning rate of 2e-5 for training
BERT and RoBERTa, 8e-6 for training XLNet. We
use the cross entropy loss for all our models during training. We use SGD optimizer with adaptive
learning rate with initial learning rate as 0.1 for
training SVM and logistic regression classifier.
The experiments of the deep learning models are
performed on a single NVIDIA Tesla T4 GPU. We
record the models’ number of parameters and their
average training time per epoch on the 4,650-user
training set. The BiLSTM and CNN model has
23 million and 57 million parameters, respectively
(including the embedding weights). We train BiLSTM using batch size 32 and it takes 27 seconds
per epoch. We train CNN using batch size 32 and it
takes 39 minutes per epoch. The transformer-based
models (BERT, RoBERTa, XLNet) have 110 million parameters and their speeds do not vary much.
We train these models using batch size 8 and they
take 1 hour 37 minutes per epoch. We train SVM,
logistic regression classifier, and random forest on
CPU.

We use the ekphrasis Python library10 for majority of the preprocessing and use the transformers
library11 for building our transformer-based models.

C

LDA Topics

We use latent Drichlet allocation (LDA) provided
by the gensim library12 to model the topics of the
tweets. To better understand the topics, we remove
all the adjectives, adverbs and verbs from the text,
and only keep the nouns. The number of the topics
is set to be 5. In total we have trained three LDA
models: one for tweets before the announcement
of U.S. National Emergency, one for tweets after
the announcement of U.S. National Emergency,
and one for national and state level tweets between
March 3rd, 2020 and May 22nd, 2020. Table 5
shows the top 15 keywords for each topic.

10

https://pypi.org/project/ekphrasis/
https://github.com/huggingface/transformers
12
https://pypi.org/project/gensim/
11

Model 1: Before Announcement of U.S. National Emergency
time, day, people, anyone, thing, support, work, man,

School and Work

class, today, school, week, watch, love, dog
day, world, time, week, thank, today, ko,

Health

health, hope, video, news, something, hell, job, people
people, life, time, day, work, love, trump, lot,

Trump Discussions

man, today, person, someone, thing, way, hope
time, year, people, day, work, way, tweet, city,

Cancer

cancer, head, friend, death, today, love, problem
love, people, time, way, song, something, day, man

Entertainment

nothing, everyone, tonight, year, today, game, guy
Model 2: After Announcement of U.S. National Emergency
Hoarding related to COVID-19

time, dog, people, man, everything, call, work, way,
covid, hope, news, food, night, thank, someone
time, work, love, day, people, something, man, thank,

Quarantine and Depression

thing, hope, everyone, life, quarantine, house, home
Chinese News about COVID-19

china, people, time, love, day, street, song, name,
person, trump, news, government, expert, virus, dey

Depression and Anxiety during COVID-19

people, today, time, day, covid, work, home, love,
virus, man, hope, tweet, thing, depression, everyone
day, people, time, love, everyone, job, quarantine,

Work from Home

today, year, thing, life, home, way, week, video
Model 3: State-level Topics
Life Change during COVID-19

day, today, march, morning, week, life, run, friend,
show, death, man, place, mayor, order, change
day, time, love, quarantine, today, ass, night, year,

Quarantine

thank, tomorrow, house, video, game, life, miss
covid, state, home, work, health, news, county, week,

Hospital News

hospital, testing, today, day, world, time, order
Government’s policy on COVID-19

trump, people, president, time, virus, care, vote, country,
china, money, need, medium, nothing, job, everyone

Family and Work

time, people, love, man, way, job, thank,
day, work, lot, someone, something, thing, family, today
Table 5: Top 15 keywords for each topic

