arXiv:2006.10964v2 [cs.IR] 23 Jun 2020

A Qualitative Evaluation of Language Models on Automatic
Question-Answering for COVID-19
David Oniani

Yanshan Wang

Mayo Clinic
Kern Center for the Science of Health Care Delivery
Rochester, MN, USA
oniani.david@mayo.edu

Mayo Clinic
Division of Digital Health Sciences
Rochester, MN, USA
wang.yanshan@mayo.edu

ABSTRACT

1 INTRODUCTION

COVID-19 (2019 Novel Coronavirus) has resulted in an ongoing
pandemic and as of 12 June 2020, has caused more than 7.4 million cases and over 418,000 deaths. The highly dynamic and rapidly
evolving situation with COVID-19 has made it diﬃcult to access accurate, on-demand information regarding the disease. Online communities, forums, and social media provide potential venues to
search for relevant questions and answers, or post questions and
seek answers from other members. However, due to the nature of
such sites, there are always a limited number of relevant questions
and responses to search from, and posted questions are rarely answered immediately. With the advancements in the ﬁeld of natural
language processing, particularly in the domain of language models, it has become possible to design chatbots that can automatically answer consumer questions. However, such models are rarely
applied and evaluated in the healthcare domain, to meet the information needs with accurate and up-to-date healthcare data. In this
paper, we propose to apply a language model for automatically answering questions related to COVID-19 and qualitatively evaluate
the generated responses. We utilized the GPT-2 language model
and applied transfer learning to retrain it on the COVID-19 Open
Research Dataset (CORD-19) corpus. In order to improve the quality of the generated responses, we applied 4 diﬀerent approaches,
namely tf-idf (Term Frequency - Inverse Document Frequency),
Bidirectional Encoder Representations from Transformers (BERT),
Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT), and Universal Sentence Encoder (USE)
to ﬁlter and retain relevant sentences in the responses. In the performance evaluation step, we asked two medical experts to rate
the responses. We found that BERT and BioBERT, on average, outperform both tf-idf and USE in relevance-based sentence ﬁltering
tasks. Additionally, based on the chatbot, we created a user-friendly
interactive web application to be hosted online and made its source
code available free of charge to anyone interested in running it locally, online, or just for experimental purposes. Overall, our work
has yielded signiﬁcant results in both designing a chatbot that produces high-quality responses to COVID-19-related questions and
comparing several embedding generation techniques.

Coronavirus disease 2019 (COVID-19) is an infectious disease caused
by severe acute respiratory syndrome coronavirus 2 (SARS-CoV2) [22]. As of 12 June 2020, more than 7.4 million cases have been
recorded, resulting in over 418,000 deaths [44]. The sudden global
outbreak of COVID-19 made millions of people quarantined, due
to the social distancing measures. Additionally, the COVID-19 pandemic caused a historic rise in mental health problems, such as
depression, post-traumatic stress disorder, and suicide, due to the
state-wise quarantine. People are isolated and stressed, and may
develop long-term psychological consequences, beyond the quarantine period [36] [16] [21]. Therefore, most of the time, people
rely on online and web-based resources for getting news and updates concerning COVID-19. Given that currently many web sources
do not hold the accurate information about the pandemic and the
misinformation campaigns are running rampant [39], it is critically
important that people and patients receive accurate, up-to-date,
and useful information regarding COVID-19. Online communities,
forums, and social media provide potential venues to search for relevant questions and answers, or post questions and seek answers
from other members. However, due to the nature of such sites,
there are always a limited number of relevant questions and responses to search from, and posted questions are rarely answered
immediately. To address these issues, we propose to develop a chatbot enhanced by neural language models that is able to automatically answer questions related to COVID-19 through conversational interactions.
A conversational chatbot is a software which is able to conduct
a conversation via text and/or other means. There are diﬀerent taxonomies for the type of conversational chatbot. Based on how the
natural language conversations are generated, there are two main
categories: script chatbot and intelligent chatbot. The entire interaction in a script chatbot is based on a pre-determined model that
determines what the chatbot can and cannot do. The “script” is
usually a decision tree that is manually crafted by domain experts
to determine which speciﬁc path to take given a response to one
question task. It is usually very labor-expensive and nongeneralizable to develop conversation decision trees. The intelligent chatbot is built using Artiﬁcial Intelligence (AI) and Natural Language
Processing (NLP) techniques that automatically generate natural
language on the back end. With the advancements in AI and NLP,
the functionality and the performance of modern chatbots have
been dramatically improved. However, these techniques are rarely
applied and evaluated in the healthcare domain to meet the information needs with accurate, up-to-date, and interactive healthcare
information.

KEYWORDS
covid-19, cord-19, gpt-2, bert, biobert, use, tf-idf, dataset, nlp, ai,
semantic similarity

arXiv, June 23, 2020

David Oniani and Yanshan Wang

open-domain chatbot trained end-to-end on data mined social media conversations. Yin et al. [47] have developed a deep learning
based chatbot for psychological therapy purposes.
Semantic similarity of texts, on the other hand, has been studied
for a long time and recent breakthroughs allowed for development
of new models such as BERT, BioBERT, and Universal Sentence En• We applied and compared the performance of four emcoder (USE). Today, one of the state-of-the art conversational AI
bedding generation approaches, namely tf-idf (Term Fremodels is GPT-2. GPT-2 is a pretrained model, so we have applied
quency - Inverse Document Frequency) [18], Bidirectional
transfer learning utilizing CORD-19 for retraining purposes. The
Encoder Representations from Transformers (BERT) [42],
resulted chatbot gave irregularly long responses that would not
BioBERT (Bidirectional Encoder Representations from Trans- be typical of a human. We have therefore decided to further ﬁlter
formers for Biomedical Text Mining) [28], and Universal
the responses via applying embedding generation algorithms and
Sentence Encoder (USE) [14] for reﬁning the automatically
models such as tf-idf, BERT, BioBERT, and USE and then using segenerated answers.
mantic similarity approaches such as cosine similarity and inner
• We proposed a qualitative evaluation guideline for autoproduct. In other words, we ﬁrst let a human ask a question and
matic question-answering for COVID-19.
make GPT-2 come up with an answer. We the further processed
• We assessed the performance of the proposed “hybrid” apthe response with additional ﬁlters and ultimately, applied an emproach for automatic question-answering for COVID-19.
bedding generation model for ﬁnding the sentences that are most
• We built a web-based chatbot using the language models
relevant to the question.
that facilitate question-answering for users.
Cosine similarity is one of the most commonly used approaches
in calculating semantic similarity of texts. Therefore, it is naturally
This paper is organized as follows. We will proceed by discussing
employed in NLP tasks. Many NLP applications need to compute
the related work and the eﬀorts in Section 2. Section 3 will be dedithe semantic similarity between two short texts. Its ﬂexibility alcated to materials and Section 4 to the proposed approach. We will
lows one to apply it under virtually any settings, as long as docureport the chatbot evaluation strategy and the experimental results
ments can be represented as vectors. Besides, ﬁnding cosine simiin Sections 7 and 6, respectively. Finally, we will also discuss a weblarity is usually not a time-consuming task and can be done really
based chatbot with the proposed model and future work in Section
quickly. Therefore, it is also commonly used for benchmarking pur7, and conclude the work in Section 8.
poses [49].
Our study has produced a chatbot that is both performant and
extensible. Additional layer of ﬁlters have shown success in classi2 RELATED WORK
fying sentences. The chatbot is also able to be retrained and readRecent neural language models of dialogue generation oﬀer great
justed to the new data, in case there are new discoveries or scipromise for generating responses for intelligent chatbots. The LSTM
entiﬁc achievements related to COVID-19. Furthermore, chatbot
(Long Short-Term Memory) sequence-to-sequence (seq2seq) model
responses have been annotated by medical experts and the results
is one type of neural generation model that maximizes the probabilwere consistent across the annotators.
ity of generating a response given the previous dialogue turn [37] [19] [41].
XLNet uses a context of the word for predicting the next word
where the context word is constrained to two directions (backward
3 MATERIALS
or forward) [45]. SAM is a technique (Self-Attentive Associative
The
White House Oﬃce of Science and Technology Policy alongMemory) where two memories are wired into a single sequential
side with the coalition of leading research groups has released a
model capable of both memorization and relational reasoning [26].
COVID-19 machine readable dataset - COVID-19 Open Research
In the GPT-2 domain, Lee and Hsiang [29] have ﬁne-tuned GPTDataset (CORD-19) [2]. It consisted of over 128,000 scholarly arti2 for generating patent claims. Klein and Nabi [25] have applied
cles regarding COVID-19, SARS-CoV-2, and related coronaviruses,
GPT-2 in conjunction with BERT for automatic question generaincluding over 59,000 with full text, and called researchers globally
tion purposes. Zhang, Sun, et al. developed a large and tunable neuto develop text and data mining tools for ﬁnding answers to the
ral conversational model DialoGPT using GPT-2 [48]. Lee, Shu et al.
questions within this content in support of the ongoing COVIDdeveloped RecipeGPT for automatic generation of cooking recipes
19
response eﬀorts worldwide [30].
by ﬁne-tuning GPT-2 on a large cooking recipe dataset [27]. We
We used CORD-19 to train a language model that would auare unaware of the work which applied GPT-2 model for transfer
tomatically answer questions related to COVID-19. The chatbot
learning purposes on CORD-19.
would
not only help improve information acquisition, but also serve
In regard to the work related to comparing pretrained AI models,
as
a
knowledge
base for COVID-19. We harvested the data from the
Jin et al. made some eﬀorts conducting probing experiments and
initial commercial use subset of CORD-19, containing 9000 scholcomparing BERT, ELMo [31], and BioBERT. Sharma and Daniel [40]
arly articles in the form of JSON ﬁles. We extracted the abstract
compared the performance of BERT networks to that of FLAIR [12].
and the main body of the article from every JSON ﬁle, combined
In the general AI-based chatbot domain, Serbal et al. [38] have
them together, and used as a corpus for retraining the language
applied deep reinforcement learning for building a conversational
model.
AI chatbot. Adiwardana et al. [11] have developed a multi-turn
The outbreak of COVID-19 has motivated us to develop a chatbot with advanced NLP techniques and evaluate the approach in
automatically answering questions related to COVID-19. To the
best of our knowledge, this is the ﬁrst study of such kind. Our contributions are:

A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19

Human
Stage 1 Question
GPT-2 Language Model
Stage 2 Response
Filtering Based on Regex/String Manipulation
Stage 3 Filtered Response
Filtering Based on Semantic Similarity to the Question
Stage 4 Filtered Response
Final Response

Figure 1: Workﬂow for Response Generation.

4

METHODS

We applied a hybrid approach for generating responses: GPT-2 was
used to generate the answer to the question, then an additional ﬁltering step was applied for pruning the irrelevant sentences from
the answer, and subsequently, semantic similarity methods were
employed to retain the sentences that are most semantically similar to the question. Such hybrid approach to the response generation produced high quality answers to COVID-19-related questions. Figure 1 illustrates the pipeline of the proposed approach.

4.1

arXiv, June 23, 2020

Adam is an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates
of lower-order moments [23]. It is highly memory-eﬃcient and has
shown good results in retraining our chatbot. We have also tried
SGD [24], yet Adam has shown the better performance and hence,
we have released the Adam-based retrained model.
The original GPT-2 was written in tensorﬂow [10] and this is
the version we used. That said, for retraining purposes, we applied
the TPU-trainable version of the GPT-2 [32].
As for the hardware, Google Colaboratory provided us with cloud
TPUs and training capabilities. It came 25 GB RAM and since we
connected the Colab to Google Drive [20], we had enough storage
to do transfer learning.
The link for downloading the model is available on our GitHub
page [17].

4.2 Filtering Based on Regex/String
Manipulation
The GPT-2 responses are usually very lengthy and for the most
part, the answer is not relevant to the question. To prune the responses generated from GPT-2, we ﬁrst chunked the answer into
the list of sentences using Python’s built-in module for dealing
with regular expressions (re [6]) and then for each answer in the
list of answers, performed the following regex/string operations:
(1) Eliminated redundant spaces
(2) Eliminated extra punctuation marks (speciﬁcally, “.”, “!”,
and “?”)
(3) Removed redundant parentheses and square brackets
(4) Further split the sentence into separate sentences if it contained a period (“.”)
Steps 2 and 4, once again, employed re module while for steps
1 and 4, just the built-in string operations were suﬃcient (hence,
no built-in or external module was used).
These operations have signiﬁcantly improved the quality of the
answer and allowed us directly passing them to the pretrained
models for generating embeddings.

GPT-2 Language Model

GPT-2 has a Transformer-based [43] architecture which, in many
ways, is similar to Open AI GPT model[34][33].
There are a total of 4 diﬀerent GPT-2 models that were released
by OpenAI: 124 million (124M), 355 million (355M), 774 million
(774M), and 1.5 billion (1.5B) parameters [4] models. While the
model with 1.5 billion parameters showed the best results in the
original paper [34], in our experiments, we found that it was diﬃcult to ﬁne-tune and use for the transfer learning purposes. Besides,
the training was unbearably slow, even if run on TPUs (Tensor Processing Unit) provided by Google Colaboratory [20] which we used
as our training ground.
We therefore utilized 774M model and ran transfer learning for
2500 iterations with the batch size of 8. After 2000 iterations, the
loss was not decreasing so we let the language model train for the
additional 500 iterations and stopped the training. The batch size
of 8 was chosen due to the memory limitations of Google Colaboratory. As for the optimizer, we used Adam [23] and set the learning
rate of 0.0001 (1e-4).

4.3 Filtering Based on Semantic Similarity to
the Question
Semantic similarity is a metric that quantiﬁes the degree to which
two texts or text documents are similar to each other. The two approaches we have used include cosine similarity and inner product.
The diﬀerence between the two is that cosine similarity pays attention to only the angle between the vectors, while the inner product
cares about both the angle and the magnitude. That said, if one has
the normalized data, both approaches are nearly equivalent.
To put each sentence in a vector representation, we tested and
applied four diﬀerent approaches for generating embeddings:
• tf-idf [7]: a simple, tf-idf based embedding-generation method.
• BERT: Large model, uncased (24 layers and 340M parameters) [35].
• BioBERT: Large v1.1 (+PubMed 1M) model based on BERTlarge Cased (custom 30k vocabulary) [3].
• USE: Version 3, large [8].

arXiv, June 23, 2020

David Oniani and Yanshan Wang

In all cases, the similar strategy was applied for ﬁltering sentences. The following equation deﬁnes the embedding generation
process:
E = emb(S)

(1)

where S = (s 1 , s 2 , . . . sn , q) and denotes the list of sentences obtained by performing the split based on regex/string manipulation
plus the question, emb denotes one of the embedding generation
approaches (i.e., tf-idf, BERT, BioBERT, or USE), and q denotes the
question being asked. E denotes the embeddings generated for the
list of sentences. In other words, the list of sentences is passed to
the embedding generation function and the feature matrix is returned.
Once the embeddings were generated, we have applied cosine
similarity and inner product for ranking the sentences. Inner product was used only with USE since its embeddings were approximately normalized. The following equation shows the ranking process:
M = (sim(e 1, eq ), sim(e 2, eq ) . . . sim(en , eq ))

(2)

where e 1 , e 2 , . . . en , eq ∈ E and sim is a function that ﬁnds the cosine similarity or the inner product between two embeddings. For
every sentence embedding e 1 , e 2 , . . . en , the approach calculates
the cosine similarity to the question embedding eq . M is a similarity matrix where every column denotes the similarity vector for
a particular sentence. The similarity vector contains the similarity
values of the particular sentence with respect to all the sentences
(including itself). The last column/similarity vector in the matrix
has the similarity measures with respect to the question embedding.
Finally, we ranked the sentences based on their similarity scores,
selected top ﬁve most similar sentences using Python’s numpy [5]
package, and concatenated them into a single, uniﬁed answer.

4.4

Example Responses

We show an example responses in the form of a Human-Bot conversation generated by the GPT-2 language model, as follows.
human: What do we know about vaccines and therapeutics of COVID-19?
bot:
Because of their critical role in viral pathogenesis
and host defense against infections, several drugs
have been developed against COVID-19 With this
in mind, we present evidence that drugs that inhibit pro-inﬂammatory responses (e.g., IL-6) have
the potential to inhibit COVID-19 infection and
viral replication in nonhuman primates after administration up to 12 days following infection Because of their critical role in viral pathogenesis
and host defense against infections, several drugs
have been developed against COVID-19
human: What do we know about COVID-19 risk factors?
bot:
However, COVID-19 is now a threat to global health
security According to the latest World Health Organization report, a large number of travel is seen
as one of the major causes of COVID-19 spread

However, COVID-19 is now a threat to global health
security
human: What is known about transmission, incubation,
and environmental stability of COVID-19?
bot:
For example, a number of reports have suggested
that animal and human hosts may have provided
the necessary environment for the transmission
of COVID-19 in people and poultry In fact, all
known animal hosts that have been associated with
animal-to-human transmission of COVID-19 are
wild birds, including chickens, geese, mule deer,
herons, and turkey In fact, all known animal hosts
that have been associated with animal-to-human
transmission of COVID-19 are wild birds, including chickens, geese, mule deer, herons, and turkey
The dialogue shows a consistent pattern in generating responses.
Response to the ﬁrst question regarding the vaccines and therapeutics states that “several drugs have been developed against COVID19.” Furthermore, it says that “drugs that inhibit pro-inﬂammatory
responses (e.g., IL-6) have the potential to inhibit COVID-19 infection and viral replication in nonhuman primates.” Regarding the
response to the second question, concerning the risk factors, it addressed the question directly by stating that “a large number of
travel is seen as one of the major causes of COVID-19 spread.” As
for the third question, about transmission, incubation, and environmental stability of COVID-19, it has mentioned that “large number
of travel is seen as one of the major causes of COVID-19 spread”
and additionally, talks about “animal-to-human transmission.” In
all cases, sentences were highly readable and understandable. That
said, in some cases, the same sentences were repeated due to how
the hybrid approach was implemented. This can be avoided, which
we discuss in the section 7.

5 QUESTIONS AND EVALUATION
In order to evaluate the performance of the proposed approaches as
well as the overall performance of the chatbot, it is crucial to have
a question dataset that both are frequently asked and related to
COVID-19. For this purpose, we decided to use 12 questions from
the Kaggle’s COVID-19 Open Research Dataset Challenge (CORD19) [15]. Most of the questions included the term “COVID-19” but
others did not, in which case we appended the term to the end of
the question. Table 1 presents all 12 questions.
For every one of the 12 questions, we generated ﬁve diﬀerent answers by applying four diﬀerent embedding generation techniques,
resulting in a total of 240 answers. Therefore, the response for every question was generated exactly 5 times using the same technique. This ensured a fair and consistent distribution of both the
questions and the approaches across the dataset. We made all of the
answers publicly available on GitHub [1]. We then asked two experienced medical experts to evaluate the quality of these responses
by assigning diﬀerent relevance scores according to the categories
in Table 2. Having 5 categories allowed for a ﬂexibility and diversity of opinions/judgements as well as a broad range of scores that
ultimately gave us a better way to evaluate our approaches. The
evaluation was done primarily by averaging the scores for a particular approach.

A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19

arXiv, June 23, 2020

Table 2: 5 Rating Categories.

Table 1: Testing questions from CORD-19.
Number Question

Category

Description

#1

Are there geographic variations in the mortality rate of COVID-19?

Relevant

#2

What is known about transmission, incubation, and environmental stability of COVID19?

The answer partially or fully an- 5
swers the question and/or makes
clear attempts to do so and is related to the question

Well-formed

the answer makes a logical sense 4
and is somewhat related to both
the question and COVID-19, yet
it does not (partially or fully) answer the question

Informative

The answer is not related to the 3
question, but provides some information about COVID-19 and
makes a logical sense

Acceptable

The answer makes some logical
sense and is weakly related to
the question or COVID-19, but is
mostly diﬃcult to understand

2

Poor

the answer is totally unrelated to
the question or COVID-19 and/or
does not make a logical sense

2

#3

Is there any evidence to suggest geographic
based virus mutations of COVID-19?

#4

Are there geographic variations in the rate of
COVID-19 spread?

#5

What do we know about virus genetics, origin,
and evolution of COVID-19?

#6

What has been published about ethical and social science considerations of COVID-19?

#7

What has been published about medical care
of COVID-19?

#8

What do we know about diagnostics and
surveillance of COVID-19?

#9

What do we know about COVID-19 risk factors?

#10

What has been published about information
sharing and inter-sectoral collaboration of
COVID-19?

#11

What do we know about vaccines and therapeutics of COVID-19?

#12

What do we know about non-pharmaceutical
interventions of COVID-19?

Our annotation process had two phases. In the ﬁrst phase, we
let the annotators evaluate the test subset of the responses generated by the language model. The test subset was comprised of
20 questions. We then computed the IAA (Inner Annotator Agreement) which was approximately equal to 0.389. Due to having 5 categories, we used Pearson correlation coeﬃcient for computing the
IAA (as opposed to Cohen’s Kappa, etc). Low correlation value led
us to having a meeting with both annotators where we discussed
why the they had diﬀerent scores on the particular responses to
questions. Finally, both annotators reached the agreement and gave
same scores for every question in the test subset of 20. Once the
agreement was reached, we then let the annotators evaluate the
remaining 220 questions. Note that we evaluated our model based
on the 240 responses and included initial subset, where both annotators agreed on the judgement. This was done for the sake of
fairness and consistency.

Point(s)

6 EMPIRICAL RESULTS
6.1 Performance by Approach.
Table 3 lists the evaluation results of diﬀerent approaches. It shows
the approach, the average scores based on the approach for each
annotator, and the overall average across the annotators. The ﬁrst
annotator rated BERT as the best approach with the average score
of 4.167. BioBERT shows slightly worse performance with a score
of 4.133 than BERT. The tf-idf approach performs well with a score
of 3.967, yet it could not outperform either BERT or BioBERT. USE
has the worst performance out of all embedding generation techniques with the score of 3.683 out of 5. The second annotator, similarly, gave the highest average score to BERT (4.283). USE was
the second best with the score of 4.083 followed by BioBERT with
approximately the same score of 4.067. The tf-idf approach has
yielded the worst results, rated 3.8.
In general, the results are consistent between two annotators
with an inner annotator agreement score of 0.521, which was calculated using the Pearson correlation. Models from the BERT family
showed the best performance in automatically answering COVID19 questions, with BERT slightly outperforming BioBERT (4.225
vs. 4.100 - average scores) being the best. The tf-idf approach and
USE show roughly similar performance (3.884 vs. 3883)., yet inferior to BERT and BioBERT. All four approaches, on average, can
be considered to be in the “well-formed” category with BERT and
BioBERT being close to the “Relevant” category. The overall average was 4.023 (Well-formed).

arXiv, June 23, 2020

David Oniani and Yanshan Wang

Table 3: Average Scores of Embedding Generation Approaches Across the Annotators.
Approach

A1

A2

Overall

Question

A1

A2

Average

Diﬀerence

tf-idf

3.967

3.8

3.884

#1

4.200

3.450

3.825

0.750 (A1)

BERT

4.167

4.283

4.225

#2

4.350

4.100

4.225

0.250 (A1)

BioBERT

4.133

4.067

4.100

#3

4.550

4.850

4.700

0.300 (A2)

USE

3.683

4.083

3.883

#4

4.100

4.150

4.125

0.050 (A2)

#5

3.600

3.950

3.775

0.350 (A2)

#6

4.100

4.250

4.175

0.150 (A2)

#7

2.650

2.650

2.650

0.000 (NA)

#8

3.850

4.450

4.150

0.000 (A2)

#9

4.650

4.450

4.550

0.200 (A1)

# 10

3.412

3.706

3.559

0.294 (A2)

# 11

4.600

4.800

4.700

0.200 (A2)

# 12

3.700

3.850

3.775

0.150 (A2)

6.2

Performance by Question

Table 4 shows the average scores for annotators A1 and A2, the
overall average, and the diﬀerence based on the question asked to
the language model. From the table, it is clear that the proposed approach had the best results for responses to questions # 11, # 3, and
# 9 with average relevance scores of 4.700, 4.700, and 4.550, respectively After conducting the one-tailed one sample t-test, taking the
diﬀerences as the values, we conclude that the diﬀerences for the
scores (0.200, 0.300, and 0.200 respectively) are not signiﬁcant with
p-values being approximately 0.682, 0.221, and 0.682 respectively
(all being above our threshold value of 0.05). This means that both
annotators made consistent judgement.
It should be noted that all 3 of these questions seem to be rather
short in length. The responses to question # 7, on the other hand,
has the worst average score and interestingly, both annotators have
given the same score of 2.667. That said, the question is also one
of the shortest questions in length. Therefore, the length does not
seem to always have a correlation with the score (hence, the performance).
To further analyze why responses to the question # 7 had the
lowest average score, we determined whether the terms of the
question are present in the dataset. Terms “inter-sectoral collaboration” and “information sharing” were both present in CORD-19.
Therefore, it is likely that the issue stems from the model itself and
not the dataset.
According to the scores, we also ﬁnd that the terms in the question has some correlation with the score. For example, the questions that featured words strongly linked to COVID-19, such as
virus in question # 3, vaccine in question # 11, and risk in question
# 9, had higher average response scores than those that did not
(e.g., question # 10).

7

Table 4: Question-Based Average Scores of Embedding Generation Approaches Across the Annotators.

DISCUSSION

The project had several limitations. First, due to hardware constraints and the diﬃculty of ﬁne-tuining, we have not used the
larger 1.5B GPT-2 model that could potentially yield better results
in generating responses. Second, the question pool was also limited
and comprised of 12 questions. Additionally, we have tried only 4
speciﬁc embedding generation approaches, which might not be a
fair representation of all such techniques in the domains of AI and
NLP.
In order to make the language model more accessible to the general audience for automating the response generation, we built a
web-based chatbot using the trained GPT-2 with options of tf-idf,

BERT, BioBERT, and USE approaches. Please ﬁnd the released code
on our GitHub: . The application is powered by Python’s Flask [9]
package and gives a simple and user-friendly interface for the interactive communication with the chatbot. Please note that the health
information generated by the chatbot is for general research purposes only. It is not a diagnostic tool, nor is it a substitute for medical advice or treatment for speciﬁc conditions.
Although our work has demonstrated the feasiblity of using language models for automatically answering COVID-19 questions,
much can be done in the further research. At ﬁrst, we would like
to explore why certain questions had the higher scores than others. Secondly, other approaches for generating embeddings for sentences, such as BioWordVec [46], could potentially improve the performance of the chatbot and can be another avenue for exploration.
From the dialogue presented in Section 4, it is clear that GPT-2
could generate duplicate sentences that may be irrelevant to the
question. In that case, the same sentence might be repeated in the
ﬁnal answer. One could incorporate an additional simple step of
eliminating duplicate sentences which could potentially improve
the quality of the answers. Adding an additional, third layer of
ﬁltering can also be tested and checked whether it improves the
quality of the responses. Additionally, the GPT-2 model can always
be further retrained on a new corpus, which could potentially improve the result. The 1.5B GPT-2 model could also be applied for retraining purposes. Finally, given that a larger GPT language model
(GPT-3) was recently released [13], we believe that it is feasible
for the chatbot with the capable hardware to explore the realm of
possibilities with this model which could also evolve into an interesting future work.

8 CONCLUSION
In this paper, we applied the GPT-2 language model to automatically answer questions related to COVID-19, and quantitatively

A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19
evaluate the proposed approach. To reﬁne the responses generated by GPT-2, we compared four diﬀerent embedding generation
techniques, namely tf-idf, BERT, BioBERT, and USE. We utilized
the collected corpus from the CORD-19 task to pretrain the GPT2 model, and evaluated the automatically generated answers on
twelve questions from the CORD-19. The results were evaluated by
two medical experts. In general, the results are consistent between
two annotators. The empirical results show that BERT achieved
the best performance in automatically answering COVID-19 questions. We also built a web-based chatbot using the trained GPT-2
model and opensoured the code.

arXiv, June 23, 2020

(2018). arXiv:arXiv:1803.11175
[15] Kaggle (An AI challenge with AI2, CZI, MSR, Georgetown, and
NIH & The White House). 2020.
COVID-19 Open Research
Dataset Challenge (CORD-19).
(2020).
Retrieved 2020-26-05 from
https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks
[16] Xiao Chunfeng. 2020.
A Novel Approach of Consultation on 2019
Novel Coronavirus (COVID-19)-Related Psychological and Mental Problems: Structured Letter Therapy.
(2020).
Retrieved 2020-30-05 from
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7047000/
[17] Oniani David and Wang Yanshan. 2020. GitHub: COVID-19 Chatbot. (2020).
Retrieved 21-06-2020 from https://github.com/oniani/covid-19-chatbot
[18] Salton G., Wong. A, and Yang C. S. 2020. A Vector Space Model for Automatic
Indexing. (2020). https://dl.acm.org/doi/pdf/10.1145/361219.361220
[19] Giovanni Gennaro, Amedeo Buonanno, Antonio Girolamo, Armando Ospedale,
and Francesco Palmieri. 2020. Intent Classiﬁcation in Question-Answering Using LSTM Architectures. (01 2020). https://arxiv.org/pdf/2001.09330.pdf
[20] Google. 2020. Google Colaboratory. (2020). Retrieved 21-06-2020 from
ACKNOWLEDGMENTS
https://colab.research.google.com/
[21] EA Holmes, RC OâĂŹConnor, and VH Perry et al. 2020.
MultidisThis work was supported by NIH grant R01LM11934, the Mayo
ciplinary research priorities for the COVID-19 pandemic: a call for acClinic Center for Health Equity and Community Engagement Retion for mental health science.
(2020).
Retrieved 2020-30-05 from
search Award, and the Mayo Clinic Oﬃce of Patient Education. The
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7159850/
[22] Ghinai Isaac, McPherson Tristan, Hunter Jennifer, and Kirkfunders had no role in the design of the study, or collection, analying Hannah et al. 2020.
(2020).
Retrieved 2020-04-04 from
sis, and interpretation of data and in preparation of the manuscript.
https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)30607-3/fulltext
The views presented in this report are not necessarily representa[23] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. (2014). arXiv:arXiv:1412.6980
tive of the funder’s views and belong solely to the authors.
[24] Kiefer J. Kingma and Wolfowitz J. 1952.
Stochastic Estimation of the Maximum of a Regression Function.
(1952).
https://projecteuclid.org/euclid.aoms/1177729392
REFERENCES
[25] Tassilo Klein and Moin Nabi. 2019. Learning to Answer by Learning to Ask:
[1] 2020.
Annotation
Results
and
Generated
Answers
Getting the Best of GPT-2 and BERT Worlds. (2019). arXiv:arXiv:1911.02365
on
GitHub.
(2020).
Retrieved
2020-15-05
from
[26] Hung Le, Truyen Tran, and Svetha Venkatesh. 2020. Self-Attentive Associative
https://github.com/oniani/covid-19-chatbot/tree/master/annotations
Memory. (02 2020).
[2] 2020. COVID-19 Open Research Dataset (CORD-19). (2020). Retrieved 2020-16[27] Helena Lee, Ke Shu, Palakorn Achananuparp, Philips Kokoh Prasetyo, Yue Liu,
03 from https://pages.semanticscholar.org/coronavirus-research
Ee-Peng Lim, and Lav Varshney. 2020. RecipeGPT: Generative Pre-training
[3] 2020. GitHub: DMIS Laboratory - Korea University. (2020). Retrieved 21-06-2020
Based Cooking Recipe Generation and Evaluation System. (03 2020).
from https://github.com/dmis-lab/biobert
[28] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,
[4] 2020.
GPT-2 Model Card.
(2020).
Retrieved 21-06-2020 from
Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical lanhttps://github.com/oniani/covid-19-chatbot
guage representation model for biomedical text mining. Bioinformatics (09 2019).
[5] 2020. NumPy. (2020). Retrieved 21-06-2020 from https://numpy.org/
DOI:https://doi.org/10.1093/bioinformatics/btz682
[6] 2020. re âĂŤ Regular expression operations. (2020). Retrieved 21-06-2020 from
[29] Jieh-Sheng Lee and Jieh Hsiang. 2019. Patent Claim Generation by Fine-Tuning
https://docs.python.org/3/library/re.html
OpenAI GPT-2. (2019). arXiv:arXiv:1907.02052
[7] 2020. Scikit-learn: Tﬁdf Vectorizer. (2020). Retrieved 21-06-2020 from
[30] The White House Oﬃce of Science and Technology Policy. 2020.
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.Tﬁdf Vectorizer.html
Call to Action to the Tech Community on New Machine Read[8] 2020.
Tensorﬂow Hub: Universal Sentence Encoder (Verable COVID-19 Dataset.
(2020).
Retrieved 2020-16-03 from
sion 3, Large).
(2020).
Retrieved 21-06-2020 from
https://www.whitehouse.gov/brieﬁngs-statements/call-action-tech-community-new-machine-rea
https://tfhub.dev/google/universal-sentence-encoder-large/3
[31] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
[9] 2020. Welcome to Flask – Flask Documentation. (2020). Retrieved 21-06-2020
Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repfrom https://ﬂask.palletsprojects.com
resentations. (2018). arXiv:arXiv:1802.05365
[10] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
[32] Shawn Presser. 2020.
(2020).
Retrieved 21-06-2020 from
Craig Citro, Greg S. Corrado, Andy Davis, Jeﬀrey Dean, Matthieu Devin, Sanhttps://github.com/shawwn/gpt-2/
jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoﬀrey Irving, Michael Isard,
[33] Alec Radford and Ilya Sutskever. 2018. Improving Language Understanding by
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh LevenGenerative Pre-Training. (2018).
berg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike
[34] Alec Radford, Jeﬀ Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
[35] GitHub: Google Research. 2020.
(2020).
Retrieved 21-06-2020 from
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
https://github.com/google-research/bert
2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.
[36] Rajkumar RP. 2020.
COVID-19 and mental health: a review
(2015). http://tensorﬂow.org/ Software available from tensorﬂow.org.
of the existing literature.
(2020).
Retrieved 2020-30-05 from
[11] Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel,
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7151415/
Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
[37] Hochreiter Sepp and Schmidhuber JÃĳrgen. 1997.
LONG
and Quoc V. Le. 2020. Towards a Human-like Open-Domain Chatbot. (2020).
SHORT-TERM MEMORY.
(1997).
Retrieved 2020-30-05 from
arXiv:arXiv:2001.09977
https://www.bioinf.jku.at/publications/older/2604.pdf
[12] Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual String Em[38] Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang,
beddings for Sequence Labeling. In COLING 2018, 27th International Conference
Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chanon Computational Linguistics. 1638–1649.
dar, Nan Rosemary Ke, Sai Rajeswar, Alexandre de Brebisson, Jose M. R. Sotelo,
[13] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared KaDendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau, and
plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Yoshua Bengio. 2018. A Deep Reinforcement Learning Chatbot (Short Version).
Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
(2018). arXiv:arXiv:1801.06700
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeﬀrey Wu, Clemens
[39] Gautam Kishore Shahi, Anne Dirkson, and Tim A. Majchrzak. 2020. An
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Exploratory Study of COVID-19 Misinformation on Twitter.
(2020).
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadarXiv:arXiv:2005.05710
ford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot
[40] Shreyas Sharma and Ron Daniel Jr. 2019. BioFLAIR: Pretrained Pooled ConLearners. (2020). arXiv:cs.CL/2005.14165
textualized Embeddings for Biomedical Sequence Labeling Tasks. (2019).
[14] Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St.
arXiv:arXiv:1908.05760
John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Yun[41]
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to Sequence LearnHsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Universal Sentence Encoder.
ing with Neural Networks. (2014). https://arxiv.org/pdf/1409.3215.pdf

arXiv, June 23, 2020

David Oniani and Yanshan Wang

[42] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well[46] Zhang Yijia, Chen Qingyu, Yang Zhihao, Lin Hongfei, and Lu Zhiyong. 2019.
Read Students Learn Better: On the Importance of Pre-training Compact Models.
BioWordVec, improving biomedical word embeddings with subword informaarXiv preprint arXiv:1908.08962v2 (2019).
tion and MeSH. (05 2019). https://www.nature.com/articles/s41597-019-0055-0
[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor[47] Junjie Yin, Zixun Chen, Kelai Zhou, and Chongyuan Yu. 2019.
A
eit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia PoloDeep Learning Based Chatbot for Campus Psychological Therapy. (2019).
sukhin. 2017.
Attention is All you Need.
(2017), 5998–6008.
arXiv:arXiv:1910.06707
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
[48] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xi[44] World Health Organization (WHO). 2020.
Coronavirus disang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2019. DialoGPT: Largeease 2019 (COVID-19).
(2020).
Retrieved 2020-06-12 from
Scale Generative Pre-training for Conversational Response Generation. (2019).
https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200612-covid-19-sitrep-144.pdf?sfvrsn=66ﬀ9f4f_2
arXiv:cs.CL/1911.00536
[45] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,
[49] Vitalii Zhelezniak, Aleksandar Savkov, April Shen, and Nils Y. Hammerla.
and Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining for Lan2019. Correlation Coeﬃcients and Semantic Textual Similarity. (2019).
guage Understanding. (2019). https://arxiv.org/pdf/1906.08237.pdf
arXiv:arXiv:1905.07790

