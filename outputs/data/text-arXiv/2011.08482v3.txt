The Role of Edge Robotics As-a-Service in
Monitoring COVID-19 Infection
Haimiao Mo, Shuai Ding*, Member, IEEE, Shanlin Yang, Xi Zheng, Member, IEEE,
Athanasios V. Vasilakos
Abstract—Deep learning technology has been widely used in edge computing. However, pandemics like covid-19 require deep learning
capabilities at mobile devices (detect respiratory rate using mobile robotics or conduct CT scan using a mobile scanner), which are severely
constrained by the limited storage and computation resources at the device level. To solve this problem, we propose a three-tier architecture,
including robot layers, edge layers, and cloud layers. We adopt this architecture to design a non-contact respiratory monitoring system to break
down respiratory rate calculation tasks. Experimental results of respiratory rate monitoring show that the proposed approach in this paper
significantly outperforms other approaches. It is supported by computation time costs with 2.26 ms per frame, 27.48 ms per frame, 0.78 seconds
for convolution operation, similarity calculation, processing one-minute length respiratory signals, respectively. And the computation time costs of
our three-tier architecture are less than that of “edge+cloud” architecture and cloud architecture. Moreover, we use our three-tire architecture for
CT image diagnosis task decomposition. The evaluation of a CT image dataset of COVID-19 proves that our three-tire architecture is useful for
resolving tasks on deep learning networks by edge equipment. There are broad application scenarios in smart hospitals in the future.
Index Terms—Deep learning, edge computing, mobile robots, respiratory rate monitoring

1

INTRODUCTION

I

n recent years, physical health monitoring has received
extensive attention [1]. Respiratory rate (RR) monitoring
is one of the crucial indicators for evaluating the
physiological status [2], especially in scenes of doctors
diagnosing respiratory diseases, such as chronic obstructive
pulmonary disease (COPD), asthma, interstitial lung disease,
pulmonary sarcoidosis, pneumoconiosis [3], and sleep apnea
[4]. The recent advances in artificial intelligence and big data
technology led to the development of robot-based health
monitoring systems [5], [6]. Health monitoring robots could
help us understand physical and mental health [7], [8], [9],
[10], which are widely applied to medical human-robot
interaction scenarios, intervention for children with autism
spectrum disorder (ASD) [11], efficient human activity
recognition [12], and other fields of healthcare. At present,
the coronavirus disease 2019 (COVID-19) is spreading
quickly in various countries around the world. Medical
personnel on the frontline of the epidemic face extremely
difficult situations such as the high risk of cross-infection,
shortage of protective clothing, and high work pressure.
Traditional physiological monitoring equipment could not
sufficiently meet the needs of this unusual scenario. There is
an urgent need for a robot system with non-contact

physiological parameter detection and remote interaction
functions to be deployed into the epidemic area to help
epidemic prevention and control. The outbreak of COVID19 has accelerated the development of healthcare-related
robots [13], which have been widely used in non-contact
ultraviolet (UV) surface disinfection, remote interaction [14],
and mental health improvement [15].
Mobile medical robots usually have severely limited
power supply and low computing power. It is challenging to
complete tasks that require intensive computing by
themselves [16][17]. Thus, the low latency and high
computation need of non-contact mobile monitoring
systems pose a great challenge. Furthermore, the increasing
interaction between users and the cloud may cause delays in
data transmission and service requests [18]. Delayed
diagnostic response directly affects medical safety and even
patients’ life or health, especially for patients diagnosed with
acute diseases. Edge computing is applied to the healthcare
field to address these issues [19], which effectively solves
data transmission and computing resource allocation. In
recent years, deep learning-based solutions have been
widely used in many edges [20].
In traditional medical scenarios, mobile robots could

Haimiao Mo, Shuai Ding and Shanlin Yang are with the School of
Management, Hefei University of Technology, Anhui Hefei 23009, China, and
also with the Key Laboratory of Process Optimization and Intelligent DecisionMaking, Ministry of Education, China (email: dingshuai@hfut.edu.cn,
yangsl@hfut.edu.cn).

Xi Zheng is with the Department of Computing, Macquarie University,
Sydney, Australia(james.zheng@mq.edu.au).
A. V. Vasilakos is with the Department of Computer Science, Electrical and
Space Engineering, LuleåUniversity of Technology, 93187 Skellefteå, Sweden,
and also with the Department of Computer Science and Technology, Fuzhou
University, Fuzhou 350108, China (athanasios.vasilakos@ltu.se).

generally support ward inspections and remote interactions,
but they could not monitor physiological indicators in realtime [4]. In the medical scene of non-contact monitoring of
physiological indicators, mobile robots need to using video
frames captured by the camera mounted on the mobile robot
at a near real-time speed in order to track the target patient,
obtain physiological signals, and assess physical health.
These mobile robots have limited resources and could not
meet the needs of unique medical scenarios [21]. In such a
situation, task decomposition of the lightweight target
tracking network with physiological monitoring is an
effective way to handle the issues [5].
To solve real-time diagnosis in actual medical scenarios
and make up for the lack of computing power of mobile
robots, we use a three-tier architecture to decompound
different computing tasks. In this way, not only have we
realized remote diagnosing patient health but also
effectively avoided the cross-infection of frontline medical
staff. The study of edge robotics as-a-service in monitoring
COVID-19 infection has essential research significance. We
design a three-tier architecture based on deep learning task
decomposition. The three-tier architecture consists of robot
layers, edge layers, and cloud layers. They are applied to
data collection tasks, data processing tasks, and support
decision tasks, respectively.
We summarize our main contributions as follows.
i) We achieved a three-tier architecture, including robot
layers, edge layers, and cloud layers. We divide the task into
a three-tier architecture according to the computing
resources of different devices. Heavy tasks are put on edge
layers. Light tasks are placed on robot layers and cloud
layers.
ii) We collected thermal imaging face video data of 15
subjects, such as medical staff, researchers, and patients in
the hospital. We verified that the Deep Learning-based
Respiratory Rate Monitoring System (DLRRMS), based on
our three-tier architecture, still works when the face is
blocked, or the head moves unconsciously.
iii) We adopted a three-tier architecture to extract
respiratory signals and provide sufficient information for
doctors to achieve rapid diagnoses. Moreover, we have
verified the three-tier architecture's usefulness through a
public dataset of CT COVID-19 images.
The paper is organized as follows. Section 2 presents the
related works of respiratory rate monitoring methods
(including contact monitoring methods and non-contact
methods), robot-based health monitoring, and deep learning
on edges. Section 3 describes our three-tier architecture,
including data collection at robot layers, data processing at
edge layers, decision support at cloud layers. Section 4,
Section 5, Section 6 shows our three-tier architecture in the
hospital case study, simulation study, and COVID-19
patients study. Section 7 and section 8 present our discussion
and conclusion, respectively.

2

RELATED WORK

2.1 RESPIRATION RATE MONITORING METHODS
2.1.1 CONTACT MONITORING METHODS
The contact monitoring method collects breathing signals
through contact sensors. Contact monitoring devices, such
as conventional electrocardiograms (ECG) [22], pulse
oximetry [23], and innovative wearable devices [24], are
currently available in the fields of medicine [25]. These
methods require physical contact between skin and
electrodes, infrared sensors, or pressure sensors [26]. As for
RR monitoring, physical appearances in hospitals or special
mobile types of equipment, for example, a thermistor [27], a
spirometer [28], and a breathing belt sensor [29], are required.
These types of equipment usually measure only one of the
following parameters: breathing sound [30], breathing
airflow, breathing-related chest or abdominal movement,
and breathing carbon dioxide emissions. Although
traditional methods could effectively monitor the
physiological parameters of the human body, most methods
are cumbersome.
2.1.2 NON-CONTACT MONITORING METHODS
The non-contact monitoring methods include visible light
technology,
Doppler
effect
technology,
infrared
thermography technology. Visible light technologies mainly
include remote PPG (rPPG). The technology of rPPG mainly
collects face videos remotely through a visible light camera
to obtain PPG signals [31] [32]. However, the contaminated
motion artifact (MA) in remote PPG (rPPG) signals seriously
interferes with physiological indicators' estimation. Changes
in blood flow caused by exercises are the leading cause of
MA. So far, many noise reduction techniques have been
proposed, such as independent component analysis [33],
wavelet denoising, and empirical mode decomposition.
According to the Doppler effect principle, Radar
technology, electromagnetic induction technology, and wifi
technology could monitor RR [34]. Radar technology uses
bio-radar to transmit to the human thoracic cavity at a
wavelength λ, which generates an echo signal due to the
chest cavity's undulating motion. There is a phase difference
between the echo signal and the transmitted signal, and the
phase difference will change with the displacement of the
thoracic cavity. Echo signals could then be used to extract
respiration signals for respiration rate estimations. In normal
physiological activities of the human body, breathing
motion may cause changes in the lungs' conductivity.
Electromagnetic induction technology could be applied to
detect tissue conductivity changes to monitor the RR [35].
With the emergence of a new generation of detectors,
near-infrared and mid-infrared regions have also been used
for medical thermal imaging [36]. Thermal imaging, also
known as infrared thermal imaging (IRT), a remote noncontact monitoring method, has become a promising
monitoring and diagnostic technology in the medical field
[37]. RR monitoring technology based on thermal imaging
mainly collects thermal imaging face in successive frames

and tracks the ROI (such as nose and carotid artery) to obtain
the breath signals. G. Scebba et al. proposed a novel method
based on multispectral data fusion that aims at estimating
RR and addressing apnea detection tasks [38].
Contact monitoring equipment needs to be in direct
contact with the human body. Prolonged contact would
cause much discomfort to the tested body. Non-contact
physiological health monitoring, on the other hand, has the
characteristics of non-interference, non-invasiveness, and
remote control. In non-contact monitoring, methods other
than IRT are susceptible to environmental factors, such as
light, noise, and magnetic fields. IRT has the characteristics
of high blur and unclear contours. It is difficult to track the
ROI to extract physiological signals. In previous research,
infrared face ROI could not be tracked accurately when the
subject spoke, head moved, or face was partially obscured.
Moreover, it is rarely considered to use deep learning
methods to track infrared face ROI to extract physiological
signals.

2.2 ROBOT-BASED HEALTH MONITORING
Due to the rapid development of artificial intelligence
technology and the convenience of remote interaction with
robots, health monitoring robots are widely used in the fields
of medicine [8] [10] [39]. Michaelis et al. designed a learning
companion robot to expand guided reading activities and
studied the impact of robots on family reading experience [8].
K. Lin et al. proposed a multi-sensor fusion method in
medical human-computer interaction scenarios to improve
fusion decision-making performance [40]. Al-Taee et al.
designed a new eHealth platform that incorporates
humanoid robots to support emerging multi-dimensional
care methods to treat diabetes [10]. Social robots are applied
to intervention research on children with ASD in order to
improve their social skills [11]. The children interact with
caregivers and robots in a three-way interaction for 30
minutes per day to complete activities related to emotional
storytelling, opinion acquisition and sorting.
At present, most of the robot research in the field of public
health is aimed at non-contact ultraviolet (UV) surface
disinfection, remote interaction, improvement of mental
health[13] [14] [15]. They are deployed in different wards to
reduce the dare of frontline medical staff. In addition, CT
images are usually used to screen for Coronavirus by
artificial intelligence technology [41][42][43]. Among
patients with COVID-19, the most common predictors of
new coronary pneumonia include age, body temperature,
signs, and symptoms [44]. However, there are few reports of
real-time health monitoring robots for special public health
scenarios of COVID-19.
2.3 DEEP LEARNING ON EDGES
As miniaturization continues and computing power
increases, edge computing becomes more powerful. It paves
ways for autonomous decision-making in the periphery.
Ning et al. [44] proposed a transmission strategy based on
deep learning in edges to reduce waiting time and improve

the throughput of data transmission between vehicles.
Zhang et al. proposed a novel cyber-physical systems (CPS)
edge computing platform based on joint learning [45]. They
trained machine learning models in a trusted joint learning
framework to realize smart services and ensured the
trustworthiness of smart services that used joint learning
frameworks to test and monitor CPS behavior. Yang et al.
designed a finger vein recognition system with template
protection based on deep learning to improve its security
[46]. Due to the limited processing power of existing edge
nodes, Li et al. [20] designed a novel offloading strategy to
optimize the performance of the internet of things (IoT) deep
learning applications through edge computing. Zhang et al.
designed a voting strategy so that fog nodes can be selected
as coordinators based on distance and computing power
indicators to help speed up the training process [47]. Based
on distributed deep learning, Tian et al. [48] designed a
distributed deep learning system for web attack detection on
edges. Chen et al. [49] adopted a distributed intelligent video
surveillance (DIVS) system using deep learning (DL)
algorithms and deployed it in an edge computing
environment to reduce the huge network communication
overhead. Zhang et al. proposed a fog-based democratic,
collaborative learning program [50], which used fog to
obtain a deep learning model with good performance in a
cloud-free IoT environment and reduced the data locality
problem of each fog node.
In medical scenarios, higher real-time performance is
required. A large number of interactions between users and
the cloud may cause communication delays. It would lead to
medical safety accidents and even affect the patient’s health
diagnosis or life safety, especially for patients diagnosed
with acute disease. In this case, we not only need to consider
real-time performance but also the accuracy of physiological
indicators. Only after both of them meet the requirements
simultaneously, it becomes possible to provide doctors with
decision-making assistance in real-time scenarios.

3

THREE-TIER ARCHITECTURE: DEEP LEARNINGBASED TASK DECOMPOSITION

3.1 OVERALL FRAMEWORK
The three-tier architecture for task decomposition is
shown in Fig.1. We employ a lightweight computing
architecture to divide the different tasks into three-tier
architecture: robot layers, edge layers, and cloud layers.
They are used for data collection, data processing, decision
support, respectively.
For respiratory rate monitoring, respiration rate
calculation tasks include facial feature extraction, Spatiotemporal features tracking, respiratory signal extraction, and
preprocessing. We break down respiratory rate calculation
tasks and place them on robot layers, edge layers, and cloud
layers.
For CT image diagnosis, diagnosis tasks include CT image
acquisition, feature extraction, ReLU-Pooling, and fully
connected. They are employed on robot layers, edge layers,
and cloud layers, respectively.

Robot layers

Searched
image Z

Sample
image X

Convolution 1

Convolution 1

Convolution 2

Convolution 2

...

...

Convolution 5

Convolution 5

Robot layers

CT image acquisition

Robot layers

CT image feature
extraction

Edge layers

*

Edge layers

Edge layers

Similarity calculation

Temporal-spatial characteristics

ReLU-Pooling
Extract respiratory signals

Cloud layers
Cloud layers
Fully connected
Respiratory signals preprocessing

(a) DLRRMS

Cloud layers
(b) The three-tier architecture

(c) CT image diagnosis

Fig. 1. The three-tier architecture: deep learning task decomposition

3.2 DATA COLLECTION AT ROBOT LAYERS
We deploy robot layers to mobile robots. Suppose there
are N mobile robots. Each mobile robot is mainly responsible
for collecting data.
In non-contact respiratory rate monitoring scenarios,
robots are used to collect infrared face videos. Each video
lasts for one minute and contains m picture frames. We
regard the feature extraction task of m picture frames as a
task. We stipulate that a robot can only handle one subtask.
One minute picture frames are divided into K subtasks for
feature extraction by CNN. Each subtask is calculated on the
corresponding robot. The process of disintegrating the i-th
task of robot layers into several subtasks is described as:
(1)
TaskRi  [subtaskR1 , subtaskR2 ,..., subtaskRK ]
Where subtaskRK is the k-th subtask of robot layers, and

KN.

N

K =  Ri

(2)

i =1

Where Ri {0,1} , Ri =0 means that the i-th robot is idle, and
vice versa.
We deploy the task of facial feature extraction to robot
layers. Robot layers adopt a siamese network to extract facial
features. The siamese network consists of two lightweight
convolutional neural networks (CNN) with five
convolutional layers, ReLU layers, and Pooling layers. One
CNN is used to extract features of searched image Z, and the
other CNN is used to extract features of sample image X.
They are prepared in the next layers to measure the
similarity and track Spatio-temporal features. In this case, Z
is a picture containing an infrared face. X is the target area of
interest in image Z, such as the nose. One input of the
siamese network is a sample image X pre-selected by the
user, and the other input is a larger search image Z.
In the scene of CT image diagnosis, robot layers are
deployed to the CT scanner of mobile robots, such as CTSOMATOM On.site. We adopt robot layers to break down

tasks of CT image acquisition or original CT image format
conversion.

3.3 DATA PROCESSING AT EDGE LAYERS
We deploy edge layers to edge equipment. Suppose there
are M edges. Edge equipment is used to resolve the task of
data processing.
For the respiratory rate monitoring, we deploy the task of
Spatio-temporal features tracking to edge layers. We use
these idle edges to decompose similarity calculation tasks.
The process of decomposing the similarity calculation task is
described as
taskE j  [taskE j1 , taskE j 2 ,..., taskE jM ]
(3)
Where taskE j is the j-th similarity calculation task of edge
layers.
We adopt edge layers to calculate the similarity between
searched image Z and sample image X to track face ROIs
when robot layers complete a feature extraction of Z and X.
By measuring the similarity between the sample image X
and each part of the search image Z, edge layers could give
a similarity score map. The target is located and tracked
according to the similarity score map. According to the
maximum value of similarity, we could locate and track the
nose ROI. In order to reduce the computational cost, the
process of calculating the similarity score of each sector
could be replaced with only one cross-correlation layer by a
fully convolutional network (FCN) [50].
The fully-convolutional siamese (SiameseFC) network [51]
is used to evaluate the similarity between the current frame
X and the search image Z [52]. Accurately tracking face ROI
is one of the key factors in obtaining respiratory signals. We
use the SiameseFC network to track thermal imaging face
ROI to ensure the accurate extraction of the temporal-spatial
features and get high-quality breathing signals. For the i-th
frame, position information

roii = [ ximin , yimin , wi , hi ] of the

target could be obtained according to the position
corresponding
to
the
maximum
similarity

max(responses[i]) . The main steps for extracting Spatiotemporal

ROI  roi1 , roi2 ,  , roim  could

features

be

described as Algorithm 1 and Fig. 2.
Algorithm 1. Extract Spatio-temporal features of face ROI
Input： coordinate of first frame face ROI and one-minute thermal
infrared face video
Output: ROI  roi1 , roi2 ,  , roim 
Begin
1. Read video;
2. i = 0
3. While(video is not none):
4.
Read pictures
5.
if i == 0:
6.
init the target of the first frame X1;
7.
face
ROI
selection
by

manual

labeling

roi1 = [ x1min , y1min , w1 , h1 ] ;
8.
9.
10.
11.
12.

init SiameseFC network model;
else:
track the target:
a) Search images in image Zi
b) Calculate the responses[i] (similarity) between the
current frame ROI of Xi and search picture of Zi , and similarity
calculation task is decomposed to idle edges;
13.
c) Upsample responses[i] by interpolating and penalize
scale changes by responses[i]=responses[i]* ,  is a penalty
14.

coefficient;
d) find the maximum responses and locate the target
min
min
center roii = [ xi , yi , wi , hi ] . For the i-th frame, wi, hi is the
min
min
width and height of the target box, ( xi , yi ) are the

15.
16.

coordinates of the upper left corner of the target box.
Endif
Add roii to ROI  roi1 , roi2 ,  , roim  ;

17.
i = i +1;
18. End while
End

Simultaneously, we adopt edge equipment at edge layers to
break down the tasks of CT image feature extraction. It is
prepared for the next cloud layers of decision support.

3.4 DECISION SUPPORT AT CLOUD LAYERS
We deploy edge layers to clouds. Suppose there are M’
clouds. We use clouds to break down the tasks of decision
support. Once the subtasks of edge layers are completed,
cloud layers begin to calculate tasks from edge layers.
For respiration signals, we deploy the task of respiration
signal extraction and preprocessing to cloud layers. Once the
face ROI information in a frame is obtained from edge layers,
respiratory signal feature extraction tasks are decomposed at
robot layers. After the respiratory signal feature extraction of
one minute is completed, initial respiration signals are
obtained. We get the respiration rate by preprocessing the
initial respiration signal. Respiratory signal preprocessing
includes detrending, normalization, and Butterworth
filtering.
Respiratory signal extraction is mainly to convert the face
ROI into a gray image according to thermal fluctuations
under the nostrils [27][53], and then calculate the gray
image's average pixel by formula 4. The tasks of calculating
the average pixel are implemented at cloud layers. Suppose
there are L clouds at cloud layers. Tasks of calculating the
pixel average are divided into L subtasks, which are
calculated in different clouds.
1 W H
avg _ pixelt =
(4)
 roit ( xi , y j )
H *W i =1 j =1
Where avg _ pixelt is the average pixel at time t, W and H
are the corresponding width and height of the grayscale
image roit ; roit ( xi , y j ) represents the pixel value of the
coordinate point ( xi , y j ) . Moreover, the respiratory signal
（RS）is described as :

X

RS = [avg _ pixel1 , avg _ pixel2 ,..., avg _ pixelm ]

φ

*

ROI(t+1)

z
Similarity calculation

φ

Face ROI tracking

...

ROI(0)
ROI(1)

ROI(2)
ROI(t)
ROI(m-2)
ROI(m-1)
ROI(m)

Spatiotemporal Features of ROI

Fig. 2. Spatio-temporal features tracking

For CT image diagnosis, we use the densenet169 to realize
classification tasks [42]. The densenet169 is divided into two
parts. One part is feature extraction layers, and the other part
is ReLU-Pooling and fully connected layers. Moreover, the
feature extraction layers of densenet169 are deployed on
edge layers. In addition, each edge processes the CT image
feature extraction that is greater than or equal to one task.

(5)

Where m is the total number of frames of a thermal infrared
video.
The initial respiratory signal of RS has a certain degree of
oscillation, and low-frequency components may also affect
the respiratory signal quality. In addition, due to the
instability of the signal acquisition device and the sensitivity
to interference from the surrounding environment, this often
causes the signal to deviate from the baseline over time. The
entire process of deviating from the baseline over time is
called the “trend-term” of the signal. It would affect the
quality and accuracy. The initial respiratory signals are
detrended using a technique based on a smoothness priors
approach [54].
The respiratory signal needs to be normalized. At present,
there are many methods of data normalization, which could
be divided into linear methods (such as extreme value
method and standard deviation method), polyline methods
(such as trifold method), and curvilinear methods (such as
semi-normal distribution). We adopt the following approach

to normalize the respiration rate signal [54].
SN − 
SN ' =



(6)

Where  is a standard deviation,  is the mean of the
original signal, SN is the signal before normalization, and
SN ' is the signal after normalization.
In the RR monitoring method based on PPG, frequency
domain features are widely used to obtain heart rate
information. This method usually uses a bandpass filter to
obtain signals in the frequency range related to heart rate.
The normal RR is 9-42 bpm, and the corresponding
frequency band is 0.15-0.7 Hz. We use a fast Fourier
transform (FFT) to convert the time-domain signal into the
frequency domain. We retain data from 0.15 to 0.7 Hz
through bandpass filtering and set the data in other
frequency ranges to zero. The noise frequency could be
eliminated in this way.
Moreover, frequency domain information useful for
respiration rate analysis should be extracted. The waveform
of breathing is relatively stable, and it corresponds to a lowfrequency signal. We decompose the original signal to
achieve the purpose of denoising. The low-frequency part is
retained, and the high-frequency part is filtered out.
Butterworth filters have the maximum flat amplitude
characteristic in the passband, and the amplitude in the
positive frequency decreases monotonically with increasing
frequency [55]. It is usually used for low-pass filtering,
which could filter out the noise of respiratory signals.
Butterworth filter is defined as

H ( j ) =

1
1 +  ( /  p ) 2 N
2

(7)

Where  p and  are the upper and lower cut-off
frequencies of the passband, and N is the Butterworth filter's
order. However, we could add algorithms through plugins
at cloud layers to adapt to different scenarios. After
Butterworth filtering, respiration signals need a false peak
estimation algorithm for additional processing to improve
the respiration rate accuracy.
For CT image classification, we adopt cloud layers to
resolve the calculation task of ReLU-Pooling and fully
connected layers of densenet169. ReLU-Pooling layer is used
to downsampling. The fully connected layer of densenet169
is used as a classifier to classify CT images. Similarly, once
the feature extraction subtask from edge layers is completed,
cloud layers begin to break down the calculation task of
ReLU-Pooling and fully connected layers. When the CT
image classification task is completed, the doctor can
provide support for decision-making based on the
classification results.

3.5 THE ALGORITHM OF FALSE PEAKS ESTIMATION FOR
RESPIRATION CASE

After signal preprocessing, initial breathing signals are
obtained. We need to detect peaks in the initial breathing

signal and eliminate false peaks.
The signal of RR peaks always fluctuates within a specific
range. Moreover, the peak point should satisfy two
properties: i) its position is near the maximum amplitude
point in the current cycle; ii) its amplitude is greater than
other points in its neighborhood. We only need to find such
a point.
If the peaks at points A and B are equal, and they both
satisfy the two conditions above, but only one of them could
be used as a peak. We mark the first occurrence as the default
peak. The peak detection algorithm detects the peaks of the
preprocessing signal and marks the peak information
according to the above principle. An example of RR
calculation is shown in Fig.3. Due to various factors, some
erroneously detected peaks may still be retained after
preprocessing respiratory signals. These erroneous peaks
need to be eliminated. We set an amplitude threshold to
eliminate false peaks. The main steps for removing false
peaks are the following:
i)
After
eliminating
the
peaks
in

Peak  peak1 , peak2, , peakN  that are less than zero, the
candidate

peak

position

information

Peak  peak1 , peak 2, , peak M  is obtained. M is the total
'

'

'

'

number of peaks after the filtered signal is less than zero. N
is the total number of peaks of the filtered signal. Due to the
influence of low-frequency signals, error peaks different
from the breathing signal would be generated. The value of
these peaks is less than zero.
ii) Calculate average peak amplitude avg_value_peaks in
the candidate peaks by formula8.
1
(8)
avg _ value _ peaks =
*( peak1' + peak 2' + ... + peak M' )
M
iii) Calculate peak threshold low_value_peak according to
formulas 8 to 9. If the amplitude value of candidate peaks is
greater than or equal to low_value_peak, one breath is
accumulated.

(a) Respiratory signal extraction

(b) Detrend, normalized

(c)Butterworth filtering

(d) Erroneous peaks eliminated

Fig. 3. An example of RR calculation

low _ value _ peak = avg _ value _ peaks − avg _ value _ peaks * 

(9)

ε is an amplitude parameter (adjust the ε through
experimentation) during the process of error peak detection.
Suppose peak_num is the number of peaks obtained after
removing the wrong peaks. Then we mark the index of the
first peak and the last peak as peak_start and peak_end,
respectively. The average distance between two adjacent
peaks is avg _ dis _ peak . Suppose the total number of
frames of thermal infrared video acquired in one minute is
total_frame. The final respiration rate is calculated as follows
[56].

TABLE 1
Total MAE and RMSE of GW with different width
Evaluation

w=50

w=60

w=70

w=80

w=90

w=100

w=110

w=120

w=130

w=140

w=150

Total MAE

8.9772

6.4491

4.4591

2.9535

1.9057

1.2824

1.0015

0.8704

0.8359

0.8791

0.9661

Total RMSE

9.9065

7.4396

5.4025

3.8177

2.5631

1.7995

1.4165

1.2548

1.1734

1.2227

1.4308

RR =

peak _ start + (total _ frame − peak _ end )
+ peak _ num
avg _ dis _ peak

(10)

Since the breathing information [0, peak _ start ] and

[ peak _ end , total _ frame] is not considered, the number of
breaths in this part needs to be considered.

4

4.1 EXPERIMENTAL SETUP
In the Second Affiliated Hospital of Anhui Medical
University, medical staff participated in collecting clinical
data. Fifteen healthy subjects were recruited. They are
between 20 and 31 years old and weigh between 46 and 105
kg. Participation in the study was voluntary, and all
participants provided informed consent. The local ethics
committee approved this study.
We collected data in an environment with a temperature
of 23-26°C. We started collecting data on March 14th, 2020.
The amount of data collected per day was 5 to 30 minutes.
Subjects were allowed to perform unconscious head
movements in the process of data collection. As of April 14th,
we collected a total of 305 minutes of data. Due to equipment
and subject issues, a portion of the data collected was invalid.
After removing invalid data, 274 minutes of useful video
was used for our experiment.
Firstly, the purpose of our comparative experiments is to
evaluate the accuracy among advanced methods and
DLRRMS of ours. Advanced methods include DLRRMS
without eliminating erroneous peaks (DLRRMS-EEP),
Gaussian window (GW) [31], and frequency domain
analysis method (FDAM) [57]. DLRRMS-EEP is a method of
DLRRMS without steps to eliminate error peaks. It is to
evaluate the effectiveness of false peaks elimination. GW is a
scheme with a fixed width of the Gaussian window to
process the respiratory signal after Butterworth filtering.
FDAM is a method of converting time-domain signals into
frequency domains to calculate RR. We use mean absolute
error (MAE), root means square error (RMSE), and mean
square error (MSE) to evaluate the performance of
DLRRMS. MAE represents the average value of the absolute
error between the predicted value and the observed value.
RMSE represents the sample standard deviation of the
difference between the predicted value and the observed
value, which illustrates the degree of dispersion of the
sample. MSE is the square of RMSE.
m

1
 h( x ( i ) ) − y ( i )
m i =1

MSE ( X , h) =

1 m
 ( h( x ( i ) ) − y ( i ) ) 2
m i =1

(12)

1 m
 ( h( x ( i ) ) − y ( i ) ) 2
m i =1

(13)

(i )
(i )
Where h( x ) is the i-th ground truth of respiration rate, y

HOSPITAL CASE STUDY

MAE ( X , h) =

RMSE ( X , h) =

(11)

is the i-th respiratory rate calculated according to the method
proposed in this paper, m is the total number of respiration
data.
Secondly, the purpose of our comparative experiments is
to evaluate the consistency between the respiration rate
monitoring by advanced methods and the real respiration
rate. The consistency evaluation usually uses the Bland–
Altman method. The problem of comparing two detection
methods is often faced in clinical research. For example, A is
a classic method, and B is a new method. It is by the 95%
consistency limit of the measurement results of the two
methods. The average of ground truth and predictive value
is the horizontal axis. We draw a scatterplot and mark the 95%
consistency limit. Finally, combining with the maximum
error allowed by clinical practice, it is concluded whether the
two methods are consistent.

4.2 EXPERIMENTAL RESULTS
4.2.1 ACCURACY EVALUATION
Nose ROI was selected to extract the respiration rate
signal to compare with existing RR monitoring methods,
including DLRRMS without eliminating erroneous peaks
(DLRRMS-EEP), Gaussian window (GW) [31], and
frequency domain analysis method (FDAM) [57].
The width of the Gaussian window was set to be 50, 60,
70, 80, 90, 100, 110, 120, 130, 140, 150 to process the
respiratory signal. The results were shown in TABLE 1. The
overall results were the best when the width of the Gaussian
window was 130. At this time, their MAE and RMSE were
0.8359 and 1.1734, respectively.
TABLE 2
The comparison of Total MAE and RMSE for four methods
Evaluation
Total
MAE
Total
RMSE
MSE

FDAM [57]

DLRRMS -EEP

GW [31]

DLRRMS

6.8131

15.4377

0.8359

0.7952

10.0692

15.8026

1.1734

1.1169

101.3888

249.7222

1.3769

1.2475

TABLE 2 summarizes the comparison results of total MAE
and RMSE for four methods, which were evaluated for RR
estimation accuracy with a total of 274 minutes (13 subjects)
of spontaneous respiration tasks. DLRRMS has the best
performance in comparison methods of this paper.

Compared to the second-ranked GW method, the MAE and
RMSE of DLRRMS are reduced by 5.12%, 5.06%, respectively.
It is supported by the lowest MAE (0.7952 breaths/min) and
RMSE (1.1169 breaths/min). GW ranks second with a total
MAE of 0.8359 bpm and RMSE 1.1734 bpm. FDAM ranks
third with a total MAE of 6.8131 bpm and RMSE of 10.0692
bpm. Furthermore, DLRRMS-EEP has the worst
performance with an MAE of 15.4377 bpm and an RMSE of
15.8026 bpm.
Fig.4 shows the comparison of MAE and RMSE for four
methods. Due to the unbalanced amount of data collected
from each subject, MAE and RMSE are more volatile.
Furthermore, each subject's height, weight, and age are
different. The differences in pixel intensity due to the
temperature of the nostril airflow may cause different
respiratory signals. Due to motion artifacts and the absence
of eliminating erroneous peaks, MAE and RMSE
fluctuations obtained by DLRRMS-EEP have significant

changes. FDAM is greatly affected by factors such as
individual differences in subjects and motion artifacts. In
general, DLRRMS outperforms the other three schemes for
most subjects, except for the 5-th and 11-th subjects. The
lowest MAE and RMSE support it in most instances.
4.2.2 CONSISTENCY EVALUATION
The Bland-Altman method was used to evaluate the
consistency between RR monitoring by DLRRMS and the
real respiration rate. The difference between the
measurement results is the vertical axis. The average of
ground truth and predictive value is the x-axis. The
difference between measurement results is the y-axis. We
draw a scatterplot and mark the 95% consistency limit. Fig.5
shows that DLRRMS has the best consistency. It is supported
by the lowest mean of difference  and the smallest interval

[ − sd ,  + sd ] . Compared with the other three methods, 
of DLRRMS is reduced by 0.2330 breaths/min, 6.6089
breaths/min, and 15.2975 breaths/min, respectively.

(a) MAE

(b) RMSE

Fig. 4. The comparison of MAE and RMSE for four methods.
(a) The plot of MAE comparison for four methods on different subjects. (b) The plot of RMSE for four methods on different subjects.
μ+ 1.96*sd= 22.0552

μ + 1.96*sd= 2.3120

0.1402

15.4377

μ - 1.96*sd= -2.0317
μ- 1.96*sd= 8.8203

(a) DLRRMS

(b) DLRRMS-EEP

μ+ 1.96*sd= 21.3953
μ+ 1.96*sd= 2.5536
0.3732
6.7491

μ- 1.96*sd= -1.8073

(c) GW

μ- 1.96*sd= -7.8971

(d) FDAM

Fig.5. The comparison of Bland-Altman plots for four methods.
(a) DLRRMS: with 95% limits of the agreement being -2.0317 to 2.3120 bpm. (b) DLRRMS-EEP: with 95% limits of the agreement being 8.8203
to 22.0552 bpm. (c) GW: with 95% limits of the agreement being -1.8073 to 2.5536 bpm. (d) FDAM: with 95% limits of the agreement being -7.8971 to
21.3953 bpm. Red highlighted is the mean of difference  , and sd is the standard deviation  . RRgt is the ground truth of RR, and RRpredict is the
predicted value of RR.

Arch.

TABLE 3
The time cost of RR calculation
Convolution
Similarity
Signal
operation
calculation
proces
(ms per frame)
(ms per frame)
s

Comm.
time
cost

Comp. time cost

0.78 s

12.86 s

58.25 s

26.506

0.80s

8.73 s

51.98 s

1309.398

0.70 s

23.03 s

1993.62 s

(NE, NR,
NC)

Total
frames

(1, 1,1)

1500*1

2.255

27.483

(-,1,1)

1500*1

1.792

(-,-,1)

1500*1

3.860

Robot +
EDGE+
Cloud
EDGE+
Cloud
Cloud

NE: number of edge devices (NE). NR: number of robots. NC: number of clouds. A breathing signal is obtained by calculating a minute
video with 1500 frames through our RR monitoring system. We regard it as a task.

5

SIMULATION STUDY

5.1 EXPERIMENTAL SETUP
The robot layers consist of many mobile robots. There are
three cameras and a display on a mobile robot. Two visible
light cameras are on the bottom (Logitech HD Webcam C310)
and top of the robot (c922 Pro Stream Webcam). The visible
light camera on the top enables interaction with subjects,
which is designed to provide a view for robot navigation.
Another thermal infrared camera (Guide Sensmart IPT640)
on the top is used to collect thermal infrared videos. The
wavelength range of this camera is 8~14 μm, the temperature
measurement range is -20 to 150 degrees Celsius, and the
working environment temperature is -10 to 50 degrees
Celsius. The resolution of the three cameras is 640*480 pixels.
The computing tasks of edge layers are undertaken by a
workstation, which simulates the computing power of the
cloud. Its configuration is Intel (R) Core (TM) i7-8565 with
main frequency 1.8 GHz and highest turbo frequency 4.6
GHz, NVIDIA Geforce MX150, and RAM 8GB.
The computing tasks of cloud layers are implemented by
a workstation with Intel (R) Core (TM) i7-10510U, main
frequency 1.8 GHz and highest turbo frequency 4.80 GHz,
and RAM 16.0 GB.
The purpose of our experiments is to analyze the
differences between the "Robot+Edge+Cloud" architecture
of ours, the "Edge+Cloud" architecture, and cloud
architecture. Moreover, we use time costs and system utility
to evaluate the performance of each architecture. Time costs
are composed of two parts: communication time costs and
computation time costs. The former refers to network delay
and time costs of data exchange between subtasks. The latter
refers to the computational cost of performing calculation
tasks related to RR calculation at robot layers, edge layers,
and cloud layers. In computing task decomposition, we use
the amount of decomposed tasks to evaluate system utility.
Our goal is to maximize the tasks decomposed per unit time.
It is described as
T

N

−

SU =  U i (t ) Ei (t )

(15)

t =1 i =1

−

Where U i (t ) is an average utility of t-th edge at time t, Ei (t )
refers to whether the i-th edge is idle at time t.

5.2 EXPERIMENTAL RESULTS
We have done simulation experiments to simulate
different numbers of robots, edges, and clouds to decompose
calculation tasks. It aims to explore the impact of different
architectures on time costs and system utility.
The experimental results of time costs are shown in Table
3. One breathing signal is obtained by calculating 1500
picture frames through our RR monitoring system. We
regard this process as a task. When NR=1, NE=1, and NC=1,
the cost of convolution operation, similarity calculation,
signal process, communication time costs, and computation
time costs are 2.255 ms per frame, 27.483 ms per frame, 0.78
seconds (for processing one-minute length respiratory
signals), 12.86 seconds, 58.25 seconds, respectively. “Robot
+Edge+ Cloud” (REC) architecture could ensure the realtime monitoring of respiratory rate. It is supported by that
its computation time costs are 58.25 seconds.
We have done simulation experiments using different
numbers of robots, edges, and clouds. The effect of different
numbers of devices on the three architectures is shown in
Fig.6 to Fig.8. Fig.6 shows the communication time costs of
the three architectures. Their communication time costs
increase linearly as the number of frames increases. The
communication time costs of “Robot+Edge+Cloud”
architecture are less than that of cloud architecture and
“Edge+Cloud” (EC) architecture.

Fig.6. Communication time costs of three architectures

From Fig.7, computation time costs of three architectures
increase linearly as the number of frames increases. Edges
have the best computing power. The computing power of
clouds is relatively weak, and clouds need more time to

process tasks. Overall, the computation time costs of ERC
architectures and EC architecture are not much different.
The results of Fig.8 show that we could increase the
number of devices to increase computing power and system
utility, which reduces computation time costs of task
decomposition. It is supported by the REC curve (Robot=5,
Edge=20, Cloud=20), and the REC curve (Robot=5, Edge=10,
Cloud=10). Communication time costs increase, and system
utility reduces when subtasks compete for computing
resources in the early stage. It is supported by the curve of
REC (Robot=5, Edge=20, Cloud=20) and the curve of EC
(Edge=20, Cloud=20). When Robot=5, Edge=20, and
Cloud=20, the size of each task queue is 5000 frames, and
each task queue is divided into ten equal subtasks. When
Edge=20 and Cloud=20, the size of each task queue is 7500
frames, and each task queue is divided into ten equal
subtasks.

Fig. 7. The computation time costs of the three architectures

two architectures.

6

COVID-19 PATIENTS STUDY

6.1 EXPERIMENTAL SETUP
We
use
our
three-layer
architecture
of
“Robot+Edge+Cloud” to decompose classification tasks of
CT images for COVID-19 patients. The public COVID-CT
dataset includes 349 CT images positive for COVID-19 from
216 patients and 397 CT images that are negative for COVID19 [42]. CT COVID-19 images are collected from COVID-19
related papers by medRxiv, bioRxiv, NEJM, JAMA, Lancet.
The dataset is accessible on https://github.com/UCSDAI4H/COVID-CT.
We adopt the pre-trained network structure of
densenet169 to do classification tasks. The classification
network is divided into feature extraction layers, ReLUPooling layer, and fully connected layer. We put feature
extraction layers with a heavier computing task on the edges.
ReLU-Pooling and fully connected layers are deployed on
mobile robots and clouds, respectively.
In the “Edge+Cloud” architecture, the feature extraction
layer and ReLU-Pooling layer are deployed on edges while
the fully connected layer is deployed on clouds. In the cloud
architecture, feature extraction layers, the Relu-Pooling layer,
and the fully connected layer of the classification model is
deployed in the cloud.
6.2 EXPERIMENTAL RESULTS
The public dataset of CT images for COVID-19 patients is
used to verify the effectiveness of the proposed
“Robot+Edge+Cloud” architecture. We design comparative
experiments on classification tasks to analyze differences in
communication time costs, computation time costs, and
system utility of “Robot+Edge+Cloud” architecture,
“Edge+Cloud” architecture, and cloud architecture. We
adopt a classification model to diagnose COVID-19 patients.
It is based on our “Robot+Edge+Cloud” architecture to
decompose classification tasks. The experimental results are
shown in Fig.9 to Fig.11.

Fig. 8. System utility and computation time costs for different numbers
of edge devices and mobile robots decomposing computing tasks

When the proportion of edges and mobile robots increases,
our respiratory monitoring system has good scalability,
which is supported by the curve of REC (Robot=10, Edge=20,
Cloud=40; the size of each task queue is 7500 frames, and
each task queue is divided into ten equal subtasks) and the
curve (Robot=5, Edge=10, Cloud=10). As computing
equipment increases, the overall computing power increases.
The performance of REC architecture is better than the other

Fig. 9. The communication time costs of
the three architectures for diagnosing COVID-19

From Fig.9 and Fig.10, the experimental results show that
the communication time costs and computation time costs of
the three architectures increase as the number of diagnosed
pictures increases. Among the three architectures, the
“Robot+Edge+Cloud” architecture has the best performance.
REC architecture uses the least communication time and
computational time costs when the number of diagnosed
pictures is the same. Since the computing power at edges is
the best, the computational time costs of the
“Robot+Edge+Cloud” and “Edge+Cloud” architectures are
not much different.
From the experimental results in Fig.11, it shows that the
“Robot+Edge+Cloud” architecture has the best system
utility among the three architectures. “Robot+Edge+Cloud”
and “Edge+Cloud” architectures decompose the more
computing tasks into edge layers with better computing
power, and their overall system utility is not much different.

noise to the respiratory signal, and it may affect the RR
accuracy of FDAM. DLRRMS-EEP calculates RR without
eliminating falsing peaks, and it counts more peaks that
should not be retained. However, DLRRMS effectively
overcomes the defects of the above existing methods by
setting an amplitude threshold, and it is verified through the
experimental results of TABLE 2 and Fig.5.
Fig.12 shows the plots of GW, DLRRMS, original signal.
Both GW and DLRRMS use the same signal preprocessing
method, such as detrending, standardization, Butterworth
filtering. However, the steps applied to eliminate error peaks
are different. The former uses a polynomial Gaussian fitting
method, and the latter eliminates false peaks by setting an
amplitude peak threshold. Since GW and DLRRMS use the
same signal preprocessing method and only use different
methods to eliminate false peaks, the experimental results
between them are very close. Moreover, it could be seen that
the correlation of peaks retained between DLRRMS and the
original signal is higher than that of GW and the original
signal from Fig.12. This is the reason why DLRRMS
proposed is better than GW.

Fig. 10. The computation time costs of
the three architectures for diagnosing COVID-19

Fig. 12. The plots of GW, DLRRMS, and the original signal.

There are many broad application prospects in the
medical field. However, due to the imbalance of data and
individual differences, we need to collect more data and
explore related future work issues. Moreover, we need to
integrate more non-contact and contact physiological
parameters into our system to provide a scientific basis for
realizing comprehensive health monitoring.

Fig. 11. The system utility and computation time costs of
the three architectures for diagnosing COVID-19

7

DISCUSSION

7.1 HOSPITAL CASE STUDY
In the first comparative experiments, GW with a fixed
width processes the respiratory signal after Butterworth
filtering. It is not suitable for peak filtering of any width
when the instantaneous breathing frequency changes and
may miss some peaks. In addition, the unconscious head
movement would produce motion artifacts and bring the

7.2 SIMULATION STUDY
In the second comparative experiment, as the number of
robots, edges, and clouds increases, the computing resources
of architecture increase. We decompose time-consuming
computing tasks into devices with relatively good
computing capabilities. In “Robot+Edge+Cloud” and
“Edge+Cloud” architectures, the most time-consuming
Spatio-temporal feature tracking calculation tasks are placed
on edges, and respiration signal extraction and
preprocessing tasks are placed on clouds. In the
“Edge+Cloud” architecture, we put facial feature extraction
tasks on edges. Moreover, respiratory signal feature
extraction and preprocessing tasks are placed on clouds. In

cloud architecture, all computing tasks are placed on clouds.
In this case, the overall system effect increases by increasing
the number of edges. However, when the number of devices
increases to a certain extent, communication delays and
communication time costs increase. Communication time
costs are mainly induced by information interaction and
resource competition between robots, edges, and clouds.
When the communication network is congested, the system
utility is reduced.

7.3 COVID-19 PATIENTS STUDY
Assuming that CT images from COVID-19 patients are
collected from the robot, we have adopted our
“Robot+Edge+Cloud” architecture to break down tasks of
diagnosing patients with COVID-19. Additionally, we adopt
“Robot+Edge+Cloud”
architecture,
“Edge+Cloud”
architecture, and cloud architecture to do a comparative
experiment. The time cost and system utility are used to
evaluate the system performance of the three architectures.
Computing performance at edges is better than that of robots
and clouds. In REC architecture and EC architecture,
because the computing tasks of feature extraction are all
placed on the edge, the communication time cost and
computing time cost between them are not much different.
Similarly, the system utility between REC architecture and
EC architecture is not significantly different.
However, there is still much work to be improved before
it could be realistically applied to special medical scenarios.
Firstly, CT images from COVID-19 patients are not collected
by our mobile robots. We have not considered the delay in
the process of CT acquisition. At the same time, since most
of the public CT images are collected from COVID-19 related
papers, they are not the original TC image, which could not
reflect the actual amount of calculation for diagnosing
COVID-19 in the medical scene. In addition, we have not
tested our three-tier architecture under network congestion
or network instability, which is what we need to strengthen
in the future. Furthermore, in our three-tier structure, there
is a lack of privacy protection for confirmed patients with
COVID-19. Finally, in real medical application scenarios, we
still need to consider the dynamic allocation of computing
resources in future work.

8

signals while considering the limitations of mobile robots'
computing resources. Compared with “Edge+Cloud”
architecture and cloud architecture, our three-tier
architecture
of
“Robot+Edge+Cloud”
has
better
performance in task decomposition. It spends fewer time
costs than the other two architectures. In this situation, realtime and effective breathing signals could provide data
support for medical staff to make decisions if more
physiological data is integrated into the system. In terms of
accuracy, compared with current mainstream GW, FDAM,
and DLRRMS-EEP, the MAE and RMSE of DLRRMS
proposed in this paper are reduced by 5.12%, 5.06% at least,
respectively. Furthermore, the consistency evaluation of
DLRRMS is better than other comparison methods. In short,
our DLRRMS could not only calculate the respiratory rate in
real-time but also ensure the accuracy of respiratory
monitoring. Furthermore, we have conducted experiments
on a public dataset of CT COVID-19 images to verify the
three-layer architecture's effectiveness. And our three-tier
architecture will have broad application scenarios in future
unmanned hospitals.

ACKNOWLEDGMENTS
The authors would like to thank Professor Chao Lu from
the Second Affiliated Hospital of Anhui Medical University
for supporting data collection and anonymous reviewers for
their detailed and thoughtful feedback, which improved the
quality of this paper significantly. This work is fully
supported by the National Natural Science Foundation of
China No. 91846107, Astronaut Research and Training
Center Commissioned project of China No. SMFA19K01,
and Basic Research Fund of Hefei University of Technology
No. PA2019GDQT0021.

REFERENCES
[1]

[2]

CONCLUSION

In this paper, we designed a deep learning-based
respiratory rate monitoring system using mobile robots and
edge equipment. Due to the limited computing resources of
mobile robots and the real-time requirements of
physiological parameter monitoring in medical scenarios, a
three-tier architecture with robot layers, edge layers, and
cloud layers is adopted to decompose different computing
tasks. We deployed feature extraction tasks, Spatio-temporal
features tracking tasks, and respiration signal extraction and
preprocessing tasks at robot layers, edge layers, and cloud
layers, respectively. This architecture meets the needs of
real-time tracking of infrared noses and extracting breathing

[3]

[4]
[5]

[6]

L. C. McKay, W. A. Janczewski, and J. L. Feldman, “Sleepdisordered breathing after targeted ablation of preBötzinger
complex neuronsSleep-disordered breathing after targeted
ablation of preBötzinger complex neurons,” Nat. Neurosci., vol.
8, no. 9, pp. 1142–1144, 2005.
C. K. Chang, H. Y. Jiang, H. Ming, and K. Oyama, “Situ: A
situation-theoretic approach to context-aware service
evolution,” in IEEE Transactions on Services Computing, 2009,
vol. 2, no. 3, pp. 261–275.
S. L. James et al., “Global, regional, and national incidence,
prevalence, and years lived with disability for 354 Diseases and
Injuries for 195 countries and territories, 1990-2017: A
systematic analysis for the Global Burden of Disease Study
2017,” Lancet, vol. 392, no. 10159, pp. 1789–1858, 2018.
M. Bsoul, H. Minn, and L. Tamil, “Apnea MedAssist: Real-time
sleep apnea monitor using single-lead ECG,” IEEE Trans. Inf.
Technol. Biomed., vol. 15, no. 3, pp. 416–427, 2011.
Z. Ma et al., “Lightweight Privacy-preserving Medical
Diagnosis in Edge Computing,” IEEE Trans. Serv. Comput., pp.
1–1, 2020.
Q. Zhang, M. Lin, L. T. Yang, Z. Chen, S. U. Khan, and P. Li,
“A Double Deep Q-Learning Model for Energy-Efficient Edge
Scheduling,” IEEE Trans. Serv. Comput., vol. 12, no. 5, pp. 739–
749, 2019.

[7]
[8]
[9]

[10]

[11]

[12]
[13]

[14]
[15]

[16]

[17]

[18]

[19]

[20]

[21]
[22]

[23]
[24]

[25]
[26]

[27]

P. Tschandl et al., “Human–computer collaboration for skin
cancer recognition,” Nat. Med., pp. 1–6, 2020.
J. E. Michaelis and B. Mutlu, “Reading socially: Transforming
the in-home reading experience with a learning-companion
robot,” Sci. Robot., vol. 3, no. 21, p. eaat5999, 2018.
D. Schall, F. Skopik, and S. Dustdar, “Expert discovery and
interactions in mixed service-oriented systems,” IEEE Trans.
Serv. Comput., vol. 5, no. 2, pp. 233–245, 2012.
M. A. Al-Taee, W. Al-Nuaimy, Z. J. Muhsin, and A. Al-Ataby,
“Robot Assistant in Management of Diabetes in Children
Based on the Internet of Things,” IEEE Internet Things J., vol. 4,
no. 2, pp. 437–445, 2017.
B. Scassellati et al., “Improving social skills in children with
ASD using a long-term, in-home social robot,” Sci. Robot., vol.
3, no. 21, p. eaat7544, 2018.
J. Lu, X. Zheng, M. Sheng, J. Jin, and S. Yu, “Efficient human
activity recognition using a single wearable sensor,” IEEE
Internet Things J., p. 1, 2020.
J. Chen et al., “COVID-19 infection: the China and Italy
perspectives,” Cell Death and Disease, vol. 11, no. 6. pp. 1–17,
2020.
G. Z. Yang et al., “Combating COVID-19-The role of robotics in
managing public health and infectious diseases,” Science
Robotics, vol. 5, no. 40. p. eabb5589, 2020.
B. Scassellati and M. Vázquez, “The potential of socially
assistive robots during infectious disease outbreaks,” Sci.
Robot., vol. 5, no. 44, p. eabc9014, 2020.
W. Chen, D. Wang, and K. Li, “Multi-User Multi-Task
Computation Offloading in Green Mobile Edge Cloud
Computing,” IEEE Trans. Serv. Comput., vol. 12, no. 5, pp. 726–
738, 2019.
C. Liu et al., “A New Deep Learning-Based Food Recognition
System for Dietary Assessment on An Edge Computing
Service Infrastructure,” IEEE Trans. Serv. Comput., vol. 11, no.
2, pp. 249–261, 2018.
W. Shi, J. Cao, Q. Zhang, Y. Li, and L. Xu, “Edge Computing:
Vision and Challenges,” IEEE Internet Things J., vol. 3, no. 5, pp.
637–646, 2016.
L. Ma, X. Liu, Q. Pei, and Y. Xiang, “Privacy-Preserving
Reputation Management for Edge Computing Enhanced
Mobile Crowdsensing,” IEEE Trans. Serv. Comput., vol. 12, no.
5, pp. 786–799, 2019.
H. Li, K. Ota, and M. Dong, “Learning IoT in Edge: Deep
Learning for the Internet of Things with Edge Computing,”
IEEE Netw., vol. 32, no. 1, pp. 96–101, 2018.
B. Yang, X. Cao, C. Yuen, and L. Qian, “Offloading
optimization in edge computing for deep learning enabled
target tracking by internet-of-UAVs,” arXiv. 2020.
M. S. Mahmud, H. Wang, A. M. Esfar-E-Alam, and H. Fang, “A
Wireless Health Monitoring System Using Mobile Phone
Accessories,” IEEE Internet Things J., vol. 4, no. 6, pp. 2009–2018,
2017.
C. M. Lochner, Y. Khan, A. Pierre, and A. C. Arias, “All-organic
optoelectronic sensor for pulse oximetry,” Nat. Commun., vol.
5, no. 1, pp. 1–7, 2014.
S. Yun, S. Park, B. Park, S. Ryu, S. M. Jeong, and K. U. Kyung,
“A soft and transparent visuo-haptic interface pursuing
wearable devices,” IEEE Trans. Ind. Electron., vol. 67, no. 1, pp.
717–724, 2020.
I. C. Jeong, D. Bychkov, and P. C. Searson, “Wearable devices
for precision medicine and health state monitoring,” IEEE
Trans. Biomed. Eng., vol. 66, no. 5, pp. 1242–1258, 2019.
P. B. Shull, S. Jiang, Y. Zhu, and X. Zhu, “Hand Gesture
Recognition and Finger Angle Estimation via Wrist-Worn
Modified Barometric Pressure Sensing,” IEEE Trans. Neural
Syst. Rehabil. Eng., vol. 27, no. 4, pp. 724–732, 2019.
J. Fei and I. Pavlidis, “Thermistor at a distance: Unobtrusive
measurement of breathing,” IEEE Trans. Biomed. Eng., vol. 57,
no. 4, pp. 988–998, 2010.

[28]

[29]

[30]

[31]

[32]

[33]
[34]

[35]

[36]

[37]

[38]
[39]

[40]
[41]

[42]
[43]

[44]
[45]

[46]

B. A. Reyes, N. Reljin, Y. Kong, Y. Nam, and K. H. Chon, “Tidal
Volume and Instantaneous Respiration Rate Estimation using
a Volumetric Surrogate Signal Acquired via a Smartphone
Camera,” IEEE J. Biomed. Heal. Informatics, vol. 21, no. 3, pp.
764–777, 2017.
J. Wo, H. Wang, Q. Sun, P. P. Shum, and D. Liu, “Noninvasive
respiration movement sensor based on distributed Bragg
reflector fiber laser with beat frequency interrogation,” J.
Biomed. Opt., vol. 19, no. 1, p. 017003, 2014.
Y. Nam, B. A. Reyes, and K. H. Chon, “Estimation of
Respiratory Rates Using the Built-in Microphone of a
Smartphone or Headset,” IEEE J. Biomed. Heal. Informatics, vol.
20, no. 6, pp. 1493–1501, 2015.
Y. Cho, S. J. Julier, N. Marquardt, and N. Bianchi-Berthouze,
“Robust tracking of respiratory rate in high-dynamic range
scenes using mobile thermal imaging,” Biomed. Opt. Express,
vol. 8, no. 10, p. 4480, Oct. 2017.
M. van Gastel, S. Stuijk, and G. de Haan, “Robust respiration
detection from remote photoplethysmography,” Biomed. Opt.
Express, vol. 7, no. 12, pp. 4941–4957, 2016.
Y. Nam, J. Lee, and K. H. Chon, “Respiratory rate estimation
from the built-in cameras of smartphones and tablets,” Ann.
Biomed. Eng., vol. 42, no. 4, pp. 885–898, 2014.
A. Pastor-Aparicio, J. Segura-Garcia, J. Lopez-Ballester, S.
Felici-Castell, M. Garcia-Pineda, and J. J. Perez-Solano,
“Psychoacoustic Annoyance Implementation with Wireless
Acoustic Sensor Networks for Monitoring in Smart Cities,”
IEEE Internet Things J., vol. 7, no. 1, pp. 128–136, 2020.
N. Mohammed, K. Cluff, J. Griffith, and B. Loflin, “A
Noninvasive, Electromagnetic, Epidermal Sensing Device for
Hemodynamics Monitoring,” IEEE Trans. Biomed. Circuits Syst.,
vol. 13, no. 6, pp. 1393–1404, 2019.
J. R. Mansfield, M. G. Sowa, J. R. Payette, B. Abdulrauf, M. F.
Stranc, and H. H. Manisch, “Tissue viability by multispectral
near infrared imaging: A fuzzy C-means clustering analysis,”
IEEE Trans. Med. Imaging, vol. 17, no. 6, pp. 1011–1018, 1998.
B. F. Jones, “A reappraisal of the use of infrared thermal image
analysis in medicine,” IEEE Trans. Med. Imaging, vol. 17, no. 6,
pp. 1019–1027, 1998.
G. Scebba, G. Da Poian, and W. Karlen, “Multispectral Video
Fusion for Non-contact Monitoring of Respiratory Rate and
Apnea,” IEEE Trans. Biomed. Eng., pp. 1–1, 2020.
S. Jain, B. Thiagarajan, Z. Shi, C. Clabaugh, and M. J. Matarić,
“Modeling engagement in long-term, in-home socially
assistive robot interventions for children with autism spectrum
disorders,” Sci. Robot., vol. 5, no. 39, p. eaaz3791, 2020.
K. Lin, Y. Li, J. Sun, D. Zhou, and Q. Zhang, “Multi-sensor
fusion for body sensor network in medical human–robot
interaction scenario,” Inf. Fusion, vol. 57, pp. 15–26, 2020.
J. G. S Wang, B Kang, J Ma, X Zeng, M Xiao, “A deep learning
algorithm using CT images to screen for Corona Virus Disease
(COVID-19),” MedRxiv, 2020.
P. Zhao, Jinyu and Zhang, Yichen and He, Xuehai and Xie,
“COVID-CT-Dataset: a CT scan dataset about COVID-19,”
arXiv Prepr. arXiv2003.13865, 2020.
T. and G. Bai, Xiang and Fang, Cong and Zhou, Yu and Bai,
Song and Liu, Zaiyi and Xia, Liming and Chen, Qianlan and
Xu, Yongchao and Xia, “Predicting COVID-19 malignant
progression with AI techniques,” 2020.
L. Wynants et al., “Prediction models for diagnosis and
prognosis of covid-19: Systematic review and critical appraisal,”
BMJ, 2020.
Z. Ning et al., “Deep Learning in Edge of Vehicles: Exploring
Trirelationship for Data Transmission,” IEEE Trans. Ind.
Informatics, vol. 15, no. 10, pp. 5737–5746, 2019.
T. Zhang, Z. Shen, J. Jin, and X. Zheng, “A Democratically
Collaborative Learning Scheme for Fog-enabled Pervasive
Environments,” in 2020 IEEE International Conference on

[47]

[48]

[49]
[50]

[51]

[52]

[53]

[54]

[55]
[56]

[57]

Pervasive Computing and Communications Workshops (PerCom
Workshops), 2020, pp. 1–4.
Z. Tian, C. Luo, J. Qiu, X. Du, and M. Guizani, “A Distributed
Deep Learning System for Web Attack Detection on Edge
Devices,” IEEE Trans. Ind. Informatics, vol. 16, no. 3, pp. 1963–
1971, 2020.
J. Chen, K. Li, Q. Deng, K. Li, and P. S. Yu, “Distributed Deep
Learning Model for Intelligent Video Surveillance Systems
with Edge Computing,” IEEE Trans. Ind. Informatics, pp. 1–1,
2019.
T. Zhang, Z. Shen, J. Jin, … X. Z.-I. I. of, and U. 2020,
“Achieving democracy in edge intelligence: a fog-based
collaborative learning scheme,” IEEE Internet Things J., 2020.
E. Shelhamer, J. Long, and T. Darrell, “Fully Convolutional
Networks for Semantic Segmentation,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 39, no. 4, pp. 640–651, 2017.
L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P.
H. S. Torr, “Fully-convolutional siamese networks for object
tracking,” in Lecture Notes in Computer Science (including
subseries Lecture Notes in Artificial Intelligence and Lecture Notes
in Bioinformatics), 2016, pp. 850–865.
D. Li, F. Porikli, G. Wen, and Y. Kuai, “When Correlation
Filters Meet Siamese Networks for Real-Time Complementary
Tracking,” IEEE Trans. Circuits Syst. Video Technol., vol. 30, no.
2, pp. 509–519, 2020.
C. B. Pereira, X. Yu, M. Czaplik, R. Rossaint, V. Blazek, and S.
Leonhardt, “Remote monitoring of breathing dynamics using
infrared thermography,” Biomed. Opt. Express, vol. 6, no. 11, pp.
4378–4394, 2015.
D. McDuff, S. Gontarek, and R. W. Picard, “Remote detection
of photoplethysmographic systolic and diastolic peaks using a
digital camera,” IEEE Trans. Biomed. Eng., vol. 61, no. 12, pp.
2948–2954, 2014.
H. H. Chang et al., “A method for respiration rate detection in
wrist PPG signal using Holo-Hilbert spectrum,” IEEE Sens. J.,
vol. 18, no. 18, pp. 7560–7569, 2018.
W. Karlen, S. Raman, J. M. Ansermino, and G. A. Dumont,
“Multiparameter respiratory rate estimation from the
photoplethysmogram,” IEEE Trans. Biomed. Eng., vol. 60, no. 7,
pp. 1946–1953, 2013.
M. H. Li, A. Yadollahi, and B. Taati, “Noncontact Vision-Based
Cardiopulmonary Monitoring in Different Sleeping Positions,”
IEEE J. Biomed. Heal. Informatics, vol. 21, no. 5, pp. 1367–1375,
2017.

Haimiao Mo is a PhD student at the
School of Management, Hefei
University of Technology, China. His
current research is in the area of noncontact health monitoring, data
mining, and machine learning.

Shuai Ding (Member, IEEE) is a professor of
information systems at the School of
Management, Hefei University of Technology,
China. He received his Ph.D. in MIS from the
Hefei University of Technology in 2011. He has
been a visiting the University of Pittsburgh. His
research interests include social networks,
information artificial intelligence, cloud
computing, and business intelligence. He has
published more than 40 high-quality
publications in international journals (IEEE
Transactions on Knowledge and Data
Engineering, ACM Transactions on Knowledge Discovery from Data, ACM

Transactions on Internet Technology, Decision Support Systems, IEEE Journal
of Biomedical and Health Informatics).
Shanlin Yang is a member of the Chinese
Academy of Engineering and the leading
Professor in management science and
information system at the School of
Management,
Hefei
University
of
Technology. He is the director of academic
board of Hefei University of Technology,
and the director of National-Local Joint
Engineering Research Center of Intelligent
Decision and Information System. He has
won 2 second class prizes for State Scientific
and Technological Progress Award, and 6
first class prizes for provincial and ministerial level science and
technology award. His research interests include information systems,
social networks, cloud computing, and artificial intelligence.
Xi Zheng (Member, IEEE) received the Ph.D.
in Software Engineering from UT Austin,
now Director of Intelligent systems research
center (itseg.org) at Macquarie University.
He specialized in Service Computing, IoT
Security, and Reliability Analysis. Published
more than 80 high-quality publications in
top journals and conferences(PerCOM, ICSE,
IEEE
Communications
Surveys
and
Tutorials, IEEE Transactions on Cybernetics,
IEEE Transactions on Industrial Informatics, IEEE Transactions on
Vehicular Technology, IEEE IoT journal, ACM Transactions on
Embedded Computing Systems) and awarded multiple best papers in
leading peer-reviewed international conferences. Guest Editor and PC
members for top journals and conferences (IEEE Transactions on
Industry Informatics, Future Generation Computer Systems, PerCOM,
TrustCOM). WIP Chair for PerCOM 2020 and Track Chair for
CloudCOM 2019. Publication Chair for ACSW 2019 and reviewers for
many Trans journals and CCF A/CORE A* conferences.
Athanasios V. Vasilakos (SM’07) received
the Ph.D. degree in computer engineering
from the University of Patras, Patras,
Greece, in 1988. He is currently a
Distinguished Professor with the Luleå
University of Technology, Luleå, Sweden,
and also with the Department of Computer
Science
and
Technology,
Fuzhou
University, Fuzhou, China. Dr. Vasilakos
has served or is serving as an Editor for
many technical journals, such as the IEEE
Transactions on network and service management, the IEEE
Transactions on systems, man, and cybernetics—part b: cybernetics, the
IEEE Transactions on information technology in biomedicine, ACM
Transactions on autonomous and adaptive systems, and the IEEE
Journal on selected areas in communications special issues in May 2009,
and January and March 2011. He is a chairman of the council of
computing of the European Alliances for Innovation.

