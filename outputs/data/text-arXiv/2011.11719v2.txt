arXiv:2011.11719v2 [eess.IV] 2 Dec 2020

Explainable-by-design Semi-Supervised Representation Learning for COVID-19
Diagnosis from CT Imaging
Abel Díaz Berenguera,b,p , Hichem Sahlia,b,c , Boris Joukovskya,b,c , Maryna Kvasnytsiaa,b,c , Ine Dirksa,b,c , Mitchel
Alioscha-Pereza,b , Nikos Deligiannisa,b,c , Panagiotis Gonidakisa,b,c , Sebastián Amador Sáncheza,b,c , Redona
Brahimetaja,b,c , Evgenia Papavasileioua,b,c , Jonathan Cheung-Wai Chana,b , Fei Lia,b,1 , Shangzhen Songa,b,1 , Yixin
Yanga,b,1 , Sofie Tilborghsa,d,o , Siri Willemsa,d,o , Tom Eelbodea,d,o , Jeroen Bertelsa,d,o , Dirk Vandermeulena,d,o , Frederik
Maesa,d,o , Paul Suetensa,d,o , Lucas Fidona,e , Tom Vercauterena,e , David Robbena,d,f,o , Arne Brysa,f , Dirk Smeetsa,f ,
Bart Ilsena,g , Nico Bulsa,g , Nina Wattéa,g , Johan de Meya,g , Annemiek Snoeckxa,h,i , Paul M. Parizela,h,j,k , Julien
Guiota,l , Louis Depreza,m , Paul Meuniera,m , Stefaan Gryspeerdta,n , Kristof De Smeta,n , Bart Jansena,b,c , Jef
Vandemeulebrouckea,b,c
a icovid consortium
Universiteit Brussel (VUB), Department of Electronics and Informatics (ETRO), Brussels, Belgium
c Interuniversity Microelectronics Centre (IMEC), Heverlee, Belgium
d KU Leuven, Department of Electrical Engineering, ESAT/PSI, Leuven, Belgium
e School of Biomedical Engineering & Imaging Sciences, King’s College London, London, United Kingdom
f icometrix, Leuven, Belgium
g Vrije Universiteit Brussel (VUB), Universitair Ziekenhuis Brussel (UZ Brussel), Brussels, Belgium
h University of Antwerp (UA), Antwerp, Belgium
i Antwerp University Hospital (UZA), Antwerp, Belgium
j Royal Perth Hospital, Perth, Australia
k University of Western Australia, Perth, Australia
l University Hospital of Liège, Department of Pneumology, Liège, Belgium
m University Hospital of Liège, Department of Radiology, Liège, Belgium
n AZ Delta, Department of Radiology, Roeselare, Belgium
o UZ Leuven, Medical Imaging Research Center, Leuven, Belgium
p Universidad de las Ciencias Informáticas (UCI), La Habana, Cuba
q Northwestern Polytechnical University, Xi’an, China
r XiDian University, Xi’an, China
b Vrije

Abstract
Our motivating application is a real-world problem: COVID-19 classification from CT imaging, for which we present
a explainable Deep Learning approach based on a semi-supervised classification pipeline that employs variational
autoencoders to extract efficient feature embedding. We have optimized the architecture of two different networks
for CT images: (i) a novel conditional variational autoencoder (CVAE) with a specific architecture that integrates the
class labels inside the encoder layers and uses side information with shared attention layers for the encoder, which
make the most of the contextual clues for representation learning, and (ii) a downstream convolutional neural network
for supervised classification using the encoder structure of the CVAE. With the explainable classification results, the
proposed diagnosis system is very effective for COVID-19 classification. Based on the promising results obtained
qualitatively and quantitatively, we envisage a wide deployment of our developed technique in large-scale clinical
studies. Code is available at https://git.etrovub.be/AVSP/ct-based-covid-19-diagnostic-tool.git

Preprint submitted to Elsevier

December 3, 2020

1. Introduction

interobserver variation in terms of the Fleiss’ kappa score
of 0.47 (Prokop et al., 2020).
The aim of the current work is to develop a method
for automated diagnosis of COVID-19 from CT imaging
capable of improving the reproducibility and diagnostic
accuracy obtained through manual reading. We present a
complete semi-supervised classification pipeline, that that
is explainable by design. We have optimized the architecture of two different networks for CT images. A generative model based on conditional variational autoencoders
(CVAE) (Sohn et al., 2015), (Ivanov et al., 2018), relying
on two inputs: the CT images and the class labels along
with a representation learning algorithm which incorporates side information in the form of an additional image modality at training time, representing the segmented
lung lesions, to produce a more informed representation
learning.
Once we learn the CVAE encoder that maps images
into a disentangled latent space, the weights of the encoder are used to initialize a downstream convolutional
neural network (CNN) for supervised classification. Feature aggregation layers are added on top of the CVAE’s
encoder base, and the resulting network is trained, in conjunction with a focal loss function, to classify CT scans.
While our method is generic and hence widely applicable, we apply the pipeline to the problem of classifying
CT volumes into COVID-19 positive versus negative patients. With the explainable classification results, our system largely simplifies and accelerates the diagnosis process for manual reading.
The remainder of this paper is organized as follows. In
Section 2 we give an overview of the related work. In Section3 we present the collected COVID-CT data-set. The
proposed diagnosis system is detailed in Section 4. The
experimental results are addressed in Section 5. Finally,
conclusions are drawn in Section 6.

In December 2019, an outbreak of pneumonia of unknown causes was first reported in the Hubei province,
China. It was soon discovered to be associated with
a novel coronavirus called severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The virus and associated coronavirus disease 2019 (COVID-19) spread
globally and was declared a pandemic, infecting nearly
7 million people on 7 June 2020.
The principal approaches to control the spread of the
virus are early detection and isolation. Currently, realtime reverse transcription polymerase chain reaction (RTPCR) assay from pharyngeal swabs is considered the reference diagnostic test for COVID-19. However, its limited availability and lengthy turnaround time have led
many to explore complementary methods as a frontline
diagnostic tool.
CT imaging has been widely used during the COVID19 pandemic. CT findings associated with COVID-19
include bilateral pulmonary parenchymal ground-glass
and consolidative pulmonary opacities, sometimes with
a rounded morphology and a peripheral lung distribution (Bernheim et al., 2020). Mild or moderate progression of the disease is manifested by increasing extent and
density of lung opacities (Chung et al., 2020).
CT imaging is recommended for rating the disease
severity and triage of patients for referral to RT-PCR testing in case of shortage of testing material. Its use for diagnosis in patients suspected of having COVID-19 and
presenting with mild clinical features is not indicated unless they are at risk for disease progression (Rubin et al.,
2020). A recent meta-analysis (Xu et al., 2020), covering 16 independent studies, reported pooled sensitivity of
92% (range 61% to 99%) with only two studies reporting
specificity, 25% and 33%, respectively.
Several systems standardizing the assessment and reporting of COVID-19 on non-enhanced chest CT have
been proposed (Prokop et al., 2020; Simpson et al., 2020;
Salehi et al., 2020). An example is the COVID-19 Reporting and Data System (CO-RADS) (Prokop et al., 2020),
assessing the suspicion for pulmonary involvement of
COVID-19 on a scale from 1 (very low) to 5 (very high).
In a study investigating the use of CO-RADS, an average
area under the curve (AUC) of 0.95 ( CI 0.91-0.99) for
predicting clinical diagnosis was reported, along with an

2. Related work
Several recent works targeted the development of
deep learning models to classify CT images and predict
whether the CTs are positive for COVID-19. The recent state-of-art models can be categorized into (a) direct CT classification, (b) COVID-19 lesion detection followed by classification, or (c) CT classification followed
by COVID-19 lesion segmentation.
2

For the purpose of CT scan classification into COVID19 positive versus non-infected patients, the authors
of (Singh et al., 2020) proposed a convolutional neural network (CNN) consisting of 3 convolutional layers, each followed by ReLU and max-pooling, and three
fully-connected layers followed by SoftMax for classification. The CNN hyperparameters have been tuned using
a multi-objective differential evolution algorithm (Zhang
and Sanderson, 2009) considering a fitness function combining sensitivity and specificity.
Hasan et al. (Hasan et al., 2020) proposed the combination of CNN features with hand-crafted texture features (Q-deformed entropy), extracted from individual
CT scans, along with a feature selection process and a
long short-term memory (LSTM) neural network classifier for discriminating between COVID-19, pneumonia and healthy CT lung scans. The CNN features have
been obtained with an architecture consisting of 4 convolutional layers followed by max-pooling and a fullyconnected last layer.
The authors of (He et al., 2020) proposed a selfsupervised transfer learning approach where contrastive
self-supervised learning (Chen et al., 2020) is integrated
into a transfer learning process to adjust the network
weights pre-trained on a source data-set. The authors
evaluated the efficacy of different transfer learning strategies with different source data-sets (ImageNet and the
Lung Nodule Malignancy (LNM) data-set (Shan et al.,
2020)) and network architectures such as ResNet (He
et al., 2016), DenseNet (Huang et al., 2017) and proposed
an architecture for binary classification (CRNet) consisting of a combination of a 2D-convolution with a ReLU
activation repeating four times. For the purpose of interpretability of the model, the authors adopted the Gradientweighted Class Activation Mapping (Grad-CAM) (Selvaraju et al., 2017) method to visualize the important regions leading to the decision of the proposed model. They
evaluated their approach on a publicly available dataset (Zhao et al., 2020).
Butt et al. (Butt et al., 2020) considered the classification of CT-scans in three classes: COVID-19, influenza viral pneumonia, or no-infection. They proposed
a ResNet-18 (He et al., 2016) network structure for image feature extraction. The output of the convolution
layer was flattened to a 256-dimensional feature vector
and then converted into a 16-dimensional feature vec-

tor using a fully-connected layer, to which they concatenated a location-attention vector, estimated as the relative distance-from-edge. Next, three fully-connected layers were followed to output the final classification result together with the confidence score. Image patches
from the same cube (center image, together with the two
neighbors) voted for the type and confidence score of
the candidate, and the overall analysis report for one CT
sample was calculated using the Noisy-or Bayesian function (Onisko et al., 2001).
Song et al. (Song et al., 2020) used a pre-trained
ResNet-50 (He et al., 2016) architecture, on which the
Feature Pyramid Network (FPN) (Lin et al., 2017) has
been coupled to the recurrent attention convolutional neural network (Fu et al., 2017) to learn discriminative region
attention. The results were aggregated using mean pooling for final classification.
In (Li et al., 2020), the authors proposed COVNet,
a 3D deep learning framework for the classification of
CT scans as COVID-19, community acquired pneumonia (CAP), and non-pneumonia. COVNet consists of a
RestNet50 (He et al., 2016) as backbone. It takes a series
of CT slices as input and generates features for the corresponding slices. The extracted features from all slices
are then combined by a max-pooling operation. The final
feature map is fed to a fully-connected layer and SoftMax
to generate a probability score for each class. The authors
adopted the Gradient-weighted Class Activation Mapping
(Grad-CAM) (Selvaraju et al., 2017) method to visualize
the important regions leading to the decision of the COVNet model.
The authors of (Hu et al., 2020) proposed a multi-scale
learning scheme to cope with variations of the size and
location of COVID-19 lesions. They used a CNN architecture with 5 convolutional layers, andapplied a spatial
aggregation with a Global Max Pooling (GMP) operation
to obtain categorical scores. To deal with the problem
of imbalanced classification, they added a class-balanced
weighting factor to the cross-entropy loss. They also extracted the position of the lesions or inflammations caused
by the COVID-19 using the integrated gradients feature
attribution method (Sundararajan et al., 2017) that assigns
to the pixels an importance score representing how much
the pixel value adds or subtracts from the network output.
Wang et al. (Wang et al., 2020) proposed DeCoVNet,
a 3D deep CNN for COVID-19-positive and COVID3

19-negative classifications. DeCoVNet consists of three
stages and takes as input the CT volume and its 3D lung
mask. The first stage is a vanilla 3D convolution with a
kernel size of 5 × 7 × 7, a batchnorm layer and a pooling
layer. The second stage is composed of two 3D residual blocks (ResBlocks). The third stage is a progressive classifier composed of three 3D convolution layers
and a fully-connected (FC) layer with the SoftMax activation function. The authors also proposed a weaklysupervised lesion localization architecture, by selecting
overlapping regions inferred from the DecCovNet after
applying the class activation mapping (CAM) (Zhou et al.,
2016) and regions obtained using a 3D connected component method (Liao et al., 2019) to the CT scans.
For the purpose of automated COVID-19 lesions detection and classification, Gozes et al. (Gozes et al., 2020)
proposed a system that analyzes CT scans at two levels:
a first level that analysis the 3D lung volume, using a
commercial off-the-shelf software1 , and detects nodules
and small opacities, and a second level that analyzes each
2D slice separately, using a pre-trained Resnet-50 (He
et al., 2016) for detecting COVID-19 related abnormalities, fine-tuned with annotated per slice normal and abnormal (COVID-19) data. To overcome the limited amount
of cases, the authors employed data augmentation techniques. To mark a case as COVID-19 positive, the authors calculated the ratio of positive detected slices out
of the total slices of the lung (positive ratio). A positive
case-decision is made if the positive ratio exceeds a predefined threshold. In a follow-up abnormality localization
step, given a new slice classified as positive, the authors
extracted network-activation maps using the Grad-CAM
technique (Selvaraju et al., 2017) to produce a visual explanation for the network decision.
Following the same idea of lesion segmentation followed by lesions classification, Shi et al. (Shi et al., 2020)
proposed a system that first preprocesses the CT volume
to obtain the segmentations of both lesions and lung using
the VB-Net network of (Shan et al., 2020). Then an infection Size Aware Random Forest method (iSARF) is applied, in which subjects were first categorized into groups
with different ranges of infected lesion size. In each group
random forests is adopted for final classification using a

1

series of handcrafted features, including infected lesion
number, volume, histogram distribution, and surface area,
estimated from the lesions and lung.
Jin et al. (Jin et al., 2020) also proposed a combined lesion segmentation step, via 3D U-net++ (Zhou
et al., 2018), followed by a lesion classification step using
ResNet-50 (He et al., 2016). The two models have been
trained separately. The classification model takes as input dual-channel information, i.e., the lesion regions and
their corresponding segmentation masks (obtained from
the previous segmentation model) for the final classification results (positive or negative).
Considering a classification followed by a lesion segmentation system, Wu et al. (Wu et al., 2020) proposed
Res2Net (Gao et al., 2019) as classifier trained using the
image mixing approach (Zhang et al., 2018). In a second
step, the authors applied an encoder-decoder based segmentation model for detecting the lesion areas. The encoder is based on the VGG-16 (Simonyan and Zisserman,
2014) backbone, without the last two fully-connected layers, to which the authors added two sequential Grouped
Atrous Modules and a max pooling function between
them. The decoder has 5 side-outputs, with an Attentive Feature Fusion strategy to aggregate the feature maps
from different stages and predict the side-output of each
stage (Wu et al., 2020). For the purpose of interpretability,
the authors adopted the Activation Mapping (Selvaraju
et al., 2017) method to visualize the important regions
leading to the decision of the classification model.
A multi-task deep learning model has been proposed
by Amyar et al. (Amyar et al., 2020). The model allows, (i) a COVID-19 vs non-COVID-19 classification,
(ii) COVID-19 lesion segmentation, and (iii) image reconstruction. The architecture consists in a common encoder for the three tasks which takes a CT scan as input,
and its output is then used to the reconstruction of the image via a first decoder, to the segmentation via a second
decoder, and to the classification via a multi-layer perceptron. The authors made use of a 2D U-net (Ronneberger
et al., 2015) encoder-decoder for both reconstruction and
segmentation. The encoder architecture consists of 10
convolutional layers, and the decoder contains 10 layers
of convolutions with up-sampling and 4 additional convolutional layers. The network has been trained in an endto-end fashion using a combined loss function including a
reconstruction loss, a dice loss, and binary cross entropy

RADLogics Inc. www.radlogics.com/

4

loss.
Different from most of the above literature, based on
pre-trained or existing backbone models, in this work we
have optimized the architecture of two different networks
for CT classification: (i) a generative model based on
Conditional VAE (Sohn et al., 2015), (Ivanov et al., 2018)
used for learning disentangled representation, and (ii) a
downstream CNN network for supervised classification.
Both models take as input a dual-channel information,
i.e., the masked lung images and their corresponding lesion regions segmentation masks, obtained from a multiclass lesion segmentation model (Tilborghs et al., 2020),
as described in Section 4.

involvement is shown in Figure 1. Additionally, the independent test-set was rated according to CO-RADS assessment scheme (Prokop et al., 2020) (Figure 2).

3. COVID-19 CT data-set
We collected 1419 non-contrast enhanced CT scans
covering the entire lungs, comprising 795 COVID-19 positive cases and 624 COVID-19 negative cases. Data was
retrospectively collected from 10 institutions throughout
Europe and Latin-America during the current pandemic.
Positive cases were all confirmed with at least one positive RT-PCR test. Inclusion criteria for negative cases included an initial negative RT-PCR test result, and confirmation through repeated RT-PCR testing over time and/or
radiological confirmation of the absence of lesions or the
presence of an alternative pathology. All scans collected
in this way concerned symptomatic patients.
Data from 9 centers was pooled and complemented
with scans from the open LIDC-IDRI (Armato III et al.,
2011) data and scans of lung pathologies acquired prior
to November 2019, and used for training, validation and
test sets. Data from one center was kept aside and used as
an independent test-set. Table 1 summarizes the collected
data used for this study.
The characteristics of the data was analysed in terms of
percentage of lung involvement. To this end, all pathological tissue was delineated using the binary lesion segmentation method 2DS described in (Tilborghs et al., 2020).
This consists of a 2D U-Net (Ronneberger et al., 2015)
with 5 levels and 16 features at the first level. The model
samples patches of 128 × 128 from the axial plane within
the bounding box around the lungs. The percentage of
lung involvement is then derived as the volume of the affected tissue over the total lung volume. The ratio of lung

Figure 1: Percentage of lung involvement for the training, validation,
test, independent-test sets per COVID-19 status.

Figure 2: Number of patients per CO-RADS classification and COVID19 status.

5

Table 1: Data description.

# Scans

COVID-19 status

Source of data

Pathology

Training

150
143
504

Negative
Negative
Positive

LIDC-IDRI
Pooled Centers
Pooled Centers

NSCLC
Various
COVID-19

Validation

0
25
24

Negative
Negative
Positive

LIDC-IDRI
Pooled Centers
Pooled Centers

NSCLC
Various
COVID-19

Test

0
50
48

Negative
Negative
Positive

LIDC-ICRI
Pooled Centers
Pooled Centers

NSCLC
Various
COVID-19

Independent
test

256
219

Negative
Positive

Independent Center
Independent Center

Various
COVID-19

4. CT-Based COVID-19 Diagnostic System

reproducible separation of the individual patterns was not
feasible during manual annotation. We refer the reader
4.1. System Overview
to (Tilborghs et al., 2020) for a detailed description of the
The pre-processing part of our workflow starts with preprocessing, training, and analysis of comparative perlung segmentation, to outline lungs as region of interest formance with respect to other segmentation approaches
for all following steps, for which the chest CT images in the field.
intensities were clipped at −1100 HU and 100 HU and
Both the masked lung images and their correspondrescaled to the [0, 1] range. Automatic lung segmenta- ing lesion segmentation masks are input to the following
tion is performed using a 3D neural network based on the steps of the diagnostic system, i.e., representation learnDeepMedic architecture (Kamnitsas et al., 2017) available ing module, CT classification module, and explainability
in the consortium (Tilborghs et al., 2020).
module, for which the chest CT images intensities were
The second step of the processing pipeline consists cropped at [−1000, 400] HU in order to preserve brighter
of a multi-class lesion segmentation approach, from structures within the lungs, and re-sampled with variable
which we extract the lesion segmentation masks. We in-plane resolution and 3 mm slice increment.
use a pre-trained multi-class lesion segmentation approach (Tilborghs et al., 2020) to identify the main pat4.2. Classification Model
terns suggestive for COVID-19, namely ground glass
opacities (GGO), consolidation (CO) and crazy paving
Image classification is the task of assigning a class label
pattern (CPP). In brief, the segmentation network is com- from a fixed set of categories to a given input image, for
posed of the following elements: a 3D U-Net (Çiçek which deep learning models have shown great promise.
et al., 2016) with 4 levels, 32 features at the lowest level, Deep learning methods are representation-learning methleaky rectified linear unit (ReLU), instance normaliza- ods with multiple levels of representation layers. In
tion (Ulyanov et al., 2016), Max Pooling, and linear up- the context of CNN, the feature representation learning
sampling. The generalized Wasserstein Dice loss (Fidon task consists of optimizing the parameters of the CNNs,
et al., 2017) is employed to deal with hierarchical ground given the training data, to discover the underlying propertruth labels. In this case, the label combined pattern, ties (Donahue et al., 2014; Sharif Razavian et al., 2014),
was used for any combination of 2 or more of the above- but this representation is heavily dependent on the datamentioned patterns (GGO, CO, CPP) for regions where set used for training.
6

In the case of COVID-19, considering the diversity of
CT findings associated with COVID-19, a positive patient
can show different types of lesions and levels of severity
of infection as the illness evolves (Chung et al., 2020).
As we wish to learn a representation that is faithful to
the underlying data structure, we followed the approach
of disentangled representation, suggested in the seminal
work of Bengio et al. (Bengio et al., 2013), where different high-level generative factors are independently encoded, thus capturing the multiple explanatory factors and
sharing factors across tasks priors. In our case, a highlevel representation of COVID-19 could include bilateral
and peripheral ground-glass and consolidative pulmonary
opacities, sometimes with a rounded morphology and peripheral lung distribution (Bernheim et al., 2020). Learning such disentangled factors of variation should allow the
model to generalize more easily to similar, but different
(from different institutions/cases), data.
In the literature, three main paradigms address disentangled learning (Higgins et al., 2018), (Ruiz et al., 2019):
unsupervised, supervised, and weakly-supervised learning. Unsupervised models (Oveneke et al., 2016) are
trained without specific information about the generative
factors of interest. They can identify simple explanatory components, however, do not allow latent variables
to model specific high-level factors. Supervised models
are learned by using a training set where the factors of
interest are explicitly labeled. Following this paradigm,
we can find different models, such as the semi-supervised
deep generative models (Kingma et al., 2014), and the
conditional variational auto-encoders (Sohn et al., 2015).
Weakly-supervised approaches use implicit information
about factors of variation provided during training, such
as the reference-based variational autoencoders (Ruiz
et al., 2019), able to impose high-level semantics into the
latent variables by exploiting the weak supervision provided by the reference set.
In the past years,
variational autoencoder
(VAE) (Kingma and Welling, 2014), coupling deep
learning with variational inference, have emerged as a
powerful latent variable model able to learn abstract data
representations. VAE includes an encoder that transforms a given input into a typically lower-dimensional
representation, and a decoder that reconstructs the input
based on the latent representation. VAE learns an efficient feature representation that can be used to improve

the performance of any standard supervised learning
algorithm.
VAE learns a distribution over objects p(x) and allows
sampling from this distribution. In our case, we are interested in learning a conditional distribution p(x|y), where
x is a CT chest image, and y could be the characteristics describing the affected COVID-19 lung (lesion’s
shape and appearance, class membership, etc. . . ). Such
CVAE (Sohn et al., 2015), (Ivanov et al., 2018) are popular approaches. In this work we use a generative model
based on CVAE, relying on two inputs: the CT images
and the class labels, and propose a representation learning
algorithm that incorporates side information in the form
of an additional label image (representing the segmented
lung lesions) at training time to produce a more informed
representation learning. Once we learn the CVAE encoder
that maps images into a disentangled latent space, the
weights of the encoder are used to initialize a downstream
CNN network for supervised classification. Feature aggregation layers are added on top of the encoder base, and
the resulting network is trained, in conjunction with a focal loss function, to classify CT chest image. With the
explainable classification results, our system largely simplifies and accelerates the diagnosis process for manual
reading.

4.2.1. Conditional Variational Autoencoder

To outline the generative approach of the CVAE in our
work, let’s define x as a training data instance corresponding to a certain patient and y the binary ground truth labels (COVID-19 positive or COVID-19 negative) for such
a patient. The objective of CVAE is to approximate the
conditional variable distribution p( x̂|x, y), x and y being
the primary input to the CVAE and x̂ the output. Then,
the stochastic latent variable z is drawn from the prior distribution p(z|x, y) and the data instance x̂ is generated from
the generative distribution p( x̂|z). As such, the generative
distribution is conditioned on the data instance and the binary label corresponding to the patient. CVAE models the
distributions using Neural Networks to learn their param7

Figure 3: CVAE model details. µ, σ are the Gaussian parameters and  is auxiliary Gaussian randomness

eters, formally defined as follows:

puts are paired. In this manner, our CVAE learns the generative distribution conditioned also on relevant lesions
related to COVID-19.
The architecture of the global and side information
CNN branches incorporates one convolutional layer of 16
5 × 5 filters with a ReLU layer as activation function followed by a max pooling (MP) layer. Subsequently, a second convolutional layer of 32 3 × 3 filters is employed,
with leaky ReLU layer as activation function followed by
MP. Afterwards, a third convolutional layer of 64 3 × 3
filters, also with leaky ReLU layer as activation function.
The two CNN branches in the encoder operate simultaneously. Consequently, after the third convolutional layer
we obtain two feature maps of the same dimensions, that
is, the feature map from the global CNN branch and a
“relevance” map from the side information CNN branch.
The input to the side information CNN is the (binary)
lesion segmentation mask. We wish to enforce high activation in these regions of interest for diagnosis. We
therefore apply a gating mechanism in which the resulting feature maps from the global and side information
CNN branches are multiplied in an element-wise manner
to enforce the attention to such a regions. Subsequently,
a ReLU is applied to the attained feature maps to enable
information flow while preventing the gradients to vanish.
The gating mechanism is formally defined as follows:

pθ (z|x, y) = ϕ prior (x, y; θ)
qφ (z|x, y) = ϕcond (x, y; φ)

(1)

pψ ( x̂|z) = ϕgen (z; ψ)
where pθ (z|x, y), qφ (z|x, y) and pψ ( x̂|z) are the prior, conditional and generative distributions respectively. The
conditional qφ (z|x, y) is an approximation of the posterior. We use Gaussian distributions, N(µ; diag(σ)2 ), parameterized with means, µ pri , µcond , and µgen , as well as
standard deviations, σ pri , σcond and σgen , which are the
outputs of the neural networks ϕ prior (x, y; θ), ϕcond (x, y; φ)
and ϕgen (z; ψ) with learning parameters θ, φ and ψ respectively.
A general overview of the proposed CVAE-based disentangled representation learning is depicted in Figure 3.
It involves one encoder with two branches, the latent variables and the decoder. The encoder consists of a global
branch along with side information branch, each one consists in a CNN. Furthermore, the CNN in the side information branch is symmetric to the CNN of the global branch,
namely it has an identical set of filters. The difference between them is that each CNN accepts distinct input. The
input to the global branch are the CT images within the
CT chest volume corresponding to a certain patient, and
the input to the side information branch is the lesion segmentation obtained from such CT images, that is, both in-

F = g(Fgcnn
8

F scnn )

(2)

where Fgcnn and F scnn are the feature maps resulting from
the global and side information branches respectively,
is the Hadamard product and g the ReLU. The proposed
gating approach acts as an inhibitor of the feature maps
from the global branch activated on those regions where
the salient information is not related to COVID-19 findings. In other words, the gating mechanism allows selecting relevant visual information for COVID-19 diagnosis.
Hence, it empowers the model to focus attention on the
most meaningful visual cues for such a task, which leads
to purposeful feature representations.
To condition the VAE to the input ground truth labels,
we feed the ground truth labels into an embedding neural
network to attain fixed-length low dimensional representation:
e = ϕembedding (y; θ)

mechanism, we use a neural network followed by a nonlinear activation and the output of the neural network is
multiplied element-wise by the input representation, formally (Kmiec et al., 2018):
fˆ = ϕcg ( f ; θ)

f

(5)

where f is the input vector representation, ϕcg is a neural
network with a non-linear activation function and learning
parameters θ, fˆ is the transformed output vector.
The architecture of the decoder in the CVAE consists
of one transposed convolutional layer of 32 3 × 3 filters
with Leaky ReLU layer as activation function followed
by bilinear interpolation for up-sampling. Subsequently,
a second transposed convolutional layer is employed of 16
3 × 3 filters, with Leaky ReLU layer as activation function
followed by bilinear interpolation for up-sampling. Finally, a third transposed convolutional layer of one 2 × 2
filters, also with Leaky ReLU layer as activation function
and followed by bilinear interpolation for up-sampling.
To learn the parameters of the CVAE we optimize the
variational lower bound, that is an appropriate choice for
the problem addressed (Sohn et al., 2015). Therefore, we
introduce the following objective function:

(3)

where y is the input binary ground truth label and
ϕembedding is a Neural Network with learning parameters
θ.
Once the low-dimensional representation of the binary
ground truth labels is computed, we reshape the feature
maps obtained in Equation 2 and pass the reshaped vector into a neural network. Afterwards, the output of the
neural network is concatenated with the representation of
the binary ground truth to merge thereafter using a neural network. As a result, we expect attaining a representation that synthesizes and carries purposeful visual information from the input data instance, due to the gating
with the corresponding lesion segmentation, whereas enforcing the prior to consider the binary labels:

h
i
LCV AE (x, y, θ, φ, ψ) = Eqφ (z|x,y) log pψ ( x̂|z) − DKL (qφ (z|x, y)||pθ (z|x, y))

(6)

where DKL (qφ (z|x, y)||pθ (z|x, y)) is the Kullback–Leibler
divergence, θ, φ, ψ are the model trainable parameters.
We use Gaussian distributions to model the latent variables and apply the parametrization approach of Kingma
and Welling (Kingma and Welling, 2014). The basic
idea is to approximate the sampling with auxiliary Gausf = ϕmerge (ϕ([F, e]); θ)
(4) sian randomness . Then, for each Gaussian distribution
 is element-wise multiplied by the standard deviation,
where ϕ and ϕmerge are neural networks with non-linear and the resulting value is added to the mean as follows:
activation and learning parameters θ. The operator [·] µ + σ . In such a way, we do not sample directly from
represents a concatenation between the reshaped feature the Gaussian distribution and we can optimize the parammaps and the ground truth label embedding.
eters of the CVAE using Stochastic Gradient Descent.
The components of the representation f contain salient
information from the lungs and lesion input x and the bi- 4.2.2. Pairing the Encoder for Classification
nary label y. Therefore, we propose to model non-linear
inter-dependencies between these feature representations
For the final COVID-19 diagnosis task, we build upon
using a context gating (CG) mechanism. Indeed, the role
of the CG mechanism is to re-calibrate the significance the parameters of the encoder optimized on the CVAE as
of each dimension of the representation f . For the CG illustrated in Figure 4. We therefore transfer the learned
9

Figure 4: Classification framework. It takes as input a series of CT slices and generates a CT volume classification prediction. The SPP features
maps of CT slices are fed to a NN followed by the NetVLAD layer and the resulting feature map is fed to a fully connected layer to generate a
probability score for each class.

parameters to a classification model for the final COVID19 diagnosis with the objective of taking advantage of the
parameters already trained using the same global and side
information branches. Hence, our COVID-19 classification model has exactly the same structure as the encoder
layers used in the CVAE.
As can be observed from Figure 4, both CNN branches
as well as the gating mechanism between them are identical to the CVAE encoder, to explicitly force the attention of the model on the most meaningful visual information for COVID-19 diagnosis. However, in contrast
to the CVAE we do not use labels as input. Furthermore, we incorporate a spatial pyramid pooling (SPP) (He
et al., 2015) layer, replacing the third max pooling (MP)
layer of the encoder of the CVAE. The SPP layer is followed by a neural network. Subsequently, we adopt the
NetVLAD (Arandjelovic et al., 2016) approach to aggregate CT slice-level features over the whole CT volume.
Finally we employ a linear layer with 2 neurons for the
diagnosis score.
The SPP layer is suitable for COVID-19 diagnosis because it can capture features at multi-level spatial bins and

at different scales, while the features are computed over
the whole slice at once. Additionally, the SPP layer increases the capacity of our model to deal with variable
size inputs as it outputs fixed-length representations regardless of input size, enabling further benefits, such as
the possibility of training with variable size images to enhances the scale-invariant properties.
Different from most previous work (see Section 2) using 3D-CNNs or 2D-CNNs in conjunction with pooling
mechanisms to classify a CT volume, in our model we
adopt the NetVLAD layer (Arandjelovic et al., 2016),
with M clusters, to aggregate the slice-level descriptors,
provided by the SPP layer, for classification. NetVLAD
offers a powerful aggregation mechanism with learnable
parameters that can be easily plugged into any CNN architecture.
The focal loss (Lin et al., 2020) (FLCM ) was chosen
over the widely used binary cross entropy loss as the latter has a tendency to be influenced heavily by trivial examples in case of class imbalance. Moreover focal loss
increases the focus on hard misclassified training data in10

stances:

the spatial pyramidal pooling stage, we only propagate
relevance to the maximum activation path. For the imageFLCM = −λt (1 − ŷt )γ log(ŷt )
(7) level layer, we apply the LRPzB rule (Montavon et al.,
2019).
where γ and λt are adjustable parameters to deal with hard
Precautions must be taken for the attention (gating) and
misclassified samples and class imbalance respectively, the aggregation stages (NetVLAD), for which there exists
and
(
no well-defined LRP rules. In these cases, we use the
ŷ
i f y = 1,
Gradient × Input model (Shrikumar et al., 2016) (similar
ŷt =
1 − ŷ
otherwise.
to LRP0 in essence) to back-propagate the incoming relevance scores. Moreover, during the backward pass, we
4.3. Explainable Model
approximate the feature aggregation step of NetVLAD by
We provide visual explanations for the network deci- a linear matrix multiplication by freezing the clustering
sion in the form of heatmaps that highlight the excitatory matrix, so that explanations are limited to features imporfeatures captured by the model during the forward pre- tant to the last classification stage.
diction pass. We apply the layer-wise relevance propagation (LRP) framework proposed by Bach et al. (Bach
5. Experimental results
et al., 2015), where individual pixel contributions or relevance scores are computed by back-propagating the acti- 5.1. Implementation and training details
vation of the class of interest to the input. Our approach
We have implemented the proposed CVAE and classiis based on the composite strategy LRPCMP (Montavon
fication model on top of Pytorch2 . The neural networks
et al., 2019), which consists of applying different LRP
used in the CVAE for embedding the binary ground truth
propagation rules at different stages of the network. We
labels and merging the embedding with the gated feature
employ the LRP0 rule at the classifier stage. For a linear
maps have 64 neurons. The number of neurons of the
layer, LRP0 redistributes relevance scores R(l+1)
at layer
j
neural network used for the context gating mechanism is
l + 1 to the preceding layer l as:
set to 64. Furthermore, the dimension of the latent variX ri j
ables was set to 16. That is to say, the neural networks
R(l)
R(l+1)
,
(8) employed to parameterize the Gaussian distributions have
j
i =
rj
j
16 neurons. We trained the architecture illustrated in FigP
where ri j ≡ xi wi j , r j ≡ i ri j , with xi corresponding to ure 3 using Adam for 200 epochs with an initial learnthe forward pass activations of layer l and wi j being the ing rate of 5e − 3. For the classifier (Figure 4), the SPP
layer weights. This simple rule is similar to the Gradi- layer has three MP levels whose kernels’ sizes are 5 × 5,
ent × Input method (Shrikumar et al., 2016). For the fea- 3 × 3, and 2 × 2 respectively. The neural network folture extraction stage (i.e., convolutional layers), we em- lowing the SPP has 512 neurons. NetVLAD is employed
ploy the LRPαβ that treats positive and negative contribu- with M = 64 clusters. The classification model was also
trained using Adam for 200 epochs and initial learning
tions asymmetrically:
rate of 1e − 5. Additionally, we regularized the model

−
X  ri+j
with a weight decay of 1e − 5 and early stopping. The
r

α + β i j  R(l+1) ,
R(l)
=
(9)
focal loss parameter λ was set to 0.25 and 0.35 for the


i
j
r+j
r−j
j
classes COVID-19 Negative and COVID-19 Positive respectively, and the parameter γ was set to 5. We built upon
with α + β = 1, ri+j and ri−j corresponding to the positive
the validation set to empirically tune all the adjustable paand negative preactivations respectively. This rule leads
rameters and we did not employ any data augmentation
to heatmaps that are less noisy, as it is more robust to gradient shattering. In our case, we use LRPαβ with α = 2
and β = −1. Extending these rules to convolutional layers
https://pytorch.org/
is straightforward. For the max pooling layers, including 2
11

technique. For the lung and lesion segmentation, we used
the pre-trained models (Tilborghs et al., 2020).

Table 3: Ablation study results. The ablation study was performed on
the reduced data set described in Table 2.
Model Sensitivity Specificity F1 -score

5.2. Ablation Study
We progressively designed the proposed model, by
thoughtfully incorporating the different components, following the data provision from our partner hospitals. To
evaluate the benefit of the proposed approach, we performed an ablation study on a small data-set (Table 2),
consisting of 122 COVID-19 negatives and 74 COVID19-positives, provided late March 2020 by two hospitals.
Table 3 summarizes the ablation study results. As can
be seen, the classification model without CVAE and side
information provided a poor classification performance.
This confirmed the poor feature representation obtained
by a simple CNN. Hence, aiming at learning a disentangled representation, we incorporated the CVAE with the
global branch. One can notice that once the CVAE was integrated and paired with the classification model the performance increased significantly; reaching higher scores
over all the reported metrics. This confirms that coupling
the variational inference to learn faithful abstract representations is an appropriate approach to improve the performance of supervised learning algorithms. The performance is increased further when the side branch information is incorporated. We hypothesise that the feature maps
gating contributed to enhance the attention of the model
on patterns suggestive of COVID-19, which improved the
diagnosis.
Table 2: Data employed for the ablation study.
# Scans

COVID-19 status

Source of data

Pathology

Training

75
48

Negative
Positive

Two Centers
Two Centers

Various
COVID-19

Validation

4
3

Negative
Positive

Two Centers
Two Centers

Various
COVID-19

Test

43
23

Negative
Positive

Two Centers
Two Centers

Various
COVID-19

5.3. COVID-19 Diagnosis
We conducted training, validation and testing on the
data set described in Table 1. In Table 4 we report the

Classification without CVAE
CVAE without side information
Proposed model

0.57
0.74
0.83

0.59
0.68
0.72

0.48
0.63
0.70

Table 4: Final classification performance on the test and independent
test set given in Table 1.
Evaluation metrics
Sensitivity Specificity F1 -score
Test-set (98 cases)
Independent test-set (475 cases)

0.94
0.96

0.82
0.59

0.88
0.79

outcomes of the proposed method. As can be seen our
proposed COVID-19 diagnosis system yielded satisfactory results. For the test-set, the proposed method diagnosed accurately 41 COVID-19 Negative and 45 COVID19 Positive patients, respectively, while 12 of the 98 patients were misdiagnosed. The ROC curve is shown in
Figure 5 and was generated by employing the bootstrapping method of Perez et al. (Pérez Fernández et al., 2018)
and the Demidenko confidence (Demidenko, 2012), with
2000 iterations and 95% confidence interval. An AUC
of 0.97 was obtained and the FPR for TPR=95% was
0.28[0.02-0.5].

Finally, aiming at validating the generalization of the
proposed method, we conducted experiments on an independent test set of 475 patients as listed in Table 1. We
report the qualitative results on the independent test in Table 4. On this challenging independent set, a lower specificity was obtained, while sensitivity was maintained. Accuracy, sensitivity and specificity were also computed per
CO-RADS rating (Figure 6). As expected, the achieved
performance varied over the different CO-RADS classes.
Surprisingly, the model was able to achieve high sensitivity for CO-RADS1 and CO-RADS2, accurately classifying 31 COVID-19 positive patients which were rated
as very low and low level of suspicion for COVID-19 by
radiologists, while missing only three.
The ROC curve is shown in Figure 7, with an AUC =
0.93[0.91−0.95] and the FPR for TPR=95% is 0.31[0.18−
12

Figure 5: Receiver operating characteristic curve of the model, with
AUC = 0.97[0.93 − 0.99] for the test set of Table 1.

Figure 7: Receiver operating characteristic curve of the model on the
independent data-set (475 patients), AUC = 0.93[0.91 − 0.95].

0.52].

5.4. Visual explanations

Figure 6: Accuracy, sensitivity and specificity per CO-RADS rating.

In Figure 8, we provide 4 examples of relevance
heatmaps for correctly-classified COVID-19 positive patients. Multi-focal consolidations and peripheral groundglass opacities can be observed in the input images, and
were mostly correctly segmented using the lesion segmentation process. We observe that features from both
inputs are incorporated by the attention mechanism, with
an important emphasis on the pre-identified lesions. The
distribution of the heat on the segmented lungs either correlates with the segmented lesions, or appears to be imbalanced between the two input modalities. For instance,
in the third example of Figure 8 where a ground glass region is left undetected by the lesions segmentation process. This demonstrates that the complementarity of both
input features is correctly exploited. The lung heatmaps
show that pulmonary abnormalities typical to COVID-19
(including peripheral consolidations) are brought out in
combination to other normal structures; although the nonspecific features are generally less highlighted. A careful
inspection is still required to fully disentangle the differ13

Figure 8: Examples of relevance heatmaps for correctly-classified COVID-19 positive patients (positive attributions only). From top to bottom: original CT-scan slice, segmented lungs input with relevance attributions, and lesions binary segmentation map with relevance attributions. Heatmaps
are obtained using the LRPCMP -based method and are post-processed using Gaussian filtering.

ent types of structures revealed by the explanation process.

Some examples of true predictions are shown in Figure 9.(A-F) and of false predictions in Figure 9.(G-I). Lesions are indicated with a red arrow. Though the patient
in Figure 9.E has large lesions, it is correctly classified
as COVID-19 negative. For the case in Figure 9.G, there
is a small lesion in the left lung, the case was wrongly
classified as COVID-19 positive. For the false negative
Figure 9.I, the system missed the very subtle ground glass
opacity.

5.5. Computational complexity
We analyzed the computational burden of our classification model along with the lung and lesions segmentation models. To carry out this analysis, we first computed
the amount of memory required by the models. Second,
we calculated the number of floating point operations per
second (FLOPs) executed in a forward pass of the models.
Third, we leveraged the inference time to diagnose a patient, which includes the time required to obtain the lung
and lesions segmentation in conjunction with the time required to classify and provide the explanations for the
diagnose. In Table 5 an overview is given of the numFigure 9: Examples of predictions. A-C: true positives; D-F: true negatives; G-H: false positives: I: false negative. Lesions are indicated with ber of parameters, FLOPS and inference time required
a red arrow.
by our method to diagnose COVID-19. As a whole, our
14

proposal has 17.34 million parameters, requiring approximately 6.46 GB of memory. The models perform 5407.7
GigaFLOPS. Finally, the average inference time is 78 seconds, which was deemed acceptable, considering a radiologist spends approximately 15 minutes to diagnose a patient. Inference time was benchmarked using the NVIDIA
TITAN Xp GPU, with 12 GB of RAM.

suggestive for COVID-19. An interesting extension of the
method would be the use of the multi-class lesion segmentation and not the binarized form, as well as extending the
CVAE to a hierarchical architecture to utilize the additional COVID-19 lesions category information for more
accurate prediction.
Acknowledgement

Table 5: Computational complexity of the models employed to diagnose
COVID-19 based on CT scans.
Method
Lung segmentation
Lesion segmentation
Classification
Visual explanations
Overall

Parameters (M)
2.12
13.3
1.92
17.34

FLOPs(G)
4262.7
1113.8
3.55
27.65
5407.7

Inferece time (S)
40
32
0.5
4.01
76.51

This research has been partially financed by the
Flemish Government under the "Onderzoeksprogramma
Artificiële Intelligentie (AI) Vlaanderen" programme
(AI Research programme) and by the FWO (project
G0A4720N). We would like to thank MD.AI and Materialise to provide access to their platforms.
References

6. Conclusions

Amyar, A., Modzelewski, R., Ruan, S., 2020. Multi-task
deep learning based ct imaging analysis for covid-19:
Classification and segmentation. doi:10.1101/2020.
04.16.20064709.

In this paper, we explore the CVAE as a generative
model and found that it can achieve competitive results. In
addition, the performance of the classification combined
with the CVAE-based model was found to lead to com- Arandjelovic, R., Gronat, P., Torii, A., Pajdla, T., Sivic,
J., 2016. Netvlad: Cnn architecture for weakly superparatively accurate results in case of a smaller training
vised place recognition, in: The IEEE Conference on
set. We further propose an extension of the basic CVAE
Computer Vision and Pattern Recognition (CVPR), pp.
model to incorporate side-information, in the form of a le5297–5307.
sions segmentation mask, for learning disentangled representations. Combined with the explainable classification
Armato III, S.G., McLennan, G., Bidaut, L., McNittresults, our system could be of interest for medical speGray, M.F., Meyer, C.R., Reeves, A.P., Zhao, B.,
cialists to diagnose and understand better the COVID-19
Aberle, D.R., Henschke, C.I., Hoffman, E.A., Kazeinfection at early stages via computed tomography. The
rooni, E.A., MacMahon, H., van Beek, E.J.R., Yankele3
developed tools will be integrated into the icolung servitz, D., Biancardi, A.M., Bland, P.H., Brown, M.S.,
vices, and can be used free of charge. The code for the
Engelmann, R.M., Laderach, G.E., Max, D., Pais,
developed models in this paper will be publicly available.
R.C., Qing, D.P.Y., Roberts, R.Y., Smith, A.R.,
We did not perform hyper-parameter optimization for
Starkey, A., Batra, P., Caligiuri, P., Farooqi, A., Gladany of the experiments thus we expect the results to get
ish, G.W., Jude, C.M., Munden, R.F., Petkovska, I.,
better if further optimized. More tests on more difficult
Quint, L.E., Schwartz, L.H., Sundaram, B., Dodd,
datasets, e.g. more classes and a higher variance, are deL.E., Fenimore, C., Gur, D., Petrick, N., Freysirable.
mann, J., Kirby, J., Hughes, B., Vande Casteele, A.,
A limitation of the proposed CVAE method is that it
Gupte, S., Sallam, M., Heath, M.D., Kuhn, M.H.,
utilizes binary labels for training and ignores the availDharaiya, E., Burns, R., Fryd, D.S., Salganicoff, M.,
able hierarchical ground truth labels of the main patterns
Anand, V., Shreter, U., Vastagh, S., Croft, B.Y.,
Clarke, L.P., 2011. The lung image database consortium (lidc) and image database resource initia3
https://icometrix.com/services/icolung
tive (idri): A completed reference database of lung
15

nodules on ct scans. Medical Physics 38, 915– Çiçek, Ö., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ron931. URL: https://aapm.onlinelibrary.wiley.
neberger, O., 2016. 3D U-Net: learning dense volucom/doi/abs/10.1118/1.3528204, doi:10.1118/
metric segmentation from sparse annotation, in: Inter1.3528204.
national conference on medical image computing and
computer-assisted intervention, Springer. pp. 424–432.
Bach, S., Binder, A., Montavon, G., Klauschen, F.,
Muller, K.R., Samek, W., 2015. On pixel-wise ex- Demidenko, E., 2012. Confidence intervals and bands for
the binormal roc curve revisited. Journal of applied
planations for non-linear classifier decisions by layerstatistics 39, 67–79.
wise relevance propagation. PLOS ONE 10, 1–46.
URL: https://doi.org/10.1371/journal.pone. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N.,
0130140, doi:10.1371/journal.pone.0130140.
Tzeng, E., Darrell, T., 2014. Decaf: A deep convoluBengio, Y., Courville, A., Vincent, P., 2013. Representation learning: A review and new perspectives. Pattern
Analysis and Machine Intelligence, IEEE Transactions
on 35, 1798–1828.

tional activation feature for generic visual recognition,
in: International conference on machine learning, pp.
647–655.
Fidon, L., Li, W., Garcia-Peraza-Herrera, L.C.,
Ekanayake, J., Kitchen, N., Ourselin, S., Vercauteren, T., 2017. Generalised Wasserstein dice score
for imbalanced multi-class segmentation using holistic
convolutional networks, in: International MICCAI
Brainlesion Workshop, Springer. pp. 64–76.

Bernheim, A., Mei, X., Huang, M., Yang, Y., Fayad, Z.A.,
Zhang, N., Diao, K., Lin, B., Zhu, X., Li, K., Li, S.,
Shan, H., Jacobi, A., Chung, M., 2020. Chest ct findings in coronavirus disease-19 (covid-19): Relationship
to duration of infection. Radiology 295, 200463.
Fu, J., Zheng, H., Mei, T., 2017. Look closer to see betURL:
https://doi.org/10.1148/radiol.
ter: Recurrent attention convolutional neural network
2020200463,
doi:10.1148/radiol.2020200463,
for fine-grained image recognition, in: 2017 IEEE ConarXiv:https://doi.org/10.1148/radiol.2020200463.
ference on Computer Vision and Pattern Recognition
pMID: 32077789.
(CVPR), pp. 4476–4484.
Butt, C., Gill, J., Chun, D., Babu, B.A., 2020. Deep learn- Gao, S., Cheng, M., Zhao, K., Zhang, X., Yang, M., Torr,
ing system to screen coronavirus disease 2019 pneuP.H.S., 2019. Res2net: A new multi-scale backbone
monia. Applied Intelligence URL: https://www.
architecture. IEEE Transactions on Pattern Analysis
ncbi.nlm.nih.gov/pmc/articles/PMC7175452/,
and Machine Intelligence , 1–1.
doi:10.1007/s10489-020-01714-3.
Gozes, O., Frid-Adar, M., Greenspan, H., Browning, P.D.,
Zhang, H., Ji, W., Bernheim, A., Siegel, E., 2020.
Chen, T., Kornblith, S., Norouzi, M., Hinton, G.E., 2020.
Rapid ai development cycle for the coronavirus (covidA simple framework for contrastive learning of visual
19) pandemic: Initial results for automated detection
representations. ArXiv abs/2002.05709.
and patient monitoring using deep learning ct image
analysis. arXiv:2003.05037.
Chung, M., Bernheim, A., Mei, X., Zhang, N.,
Huang, M., Zeng, X., Cui, J., Xu, W., Yang, Y., Hasan, A., AL-Jawad, M., Jalab, H., Shaiba, H., Ibrahim,
Fayad, Z.A., Jacobi, A., Li, K., Li, S., Shan,
R., AL-Shamasneh, A., 2020. Classification of covidH., 2020.
Ct imaging features of 2019 novel
19 coronavirus, pneumonia and healthy lungs in ct
coronavirus (2019-ncov).
Radiology 295, 202–
scans using q-deformed entropy and deep learning fea207. URL: https://doi.org/10.1148/radiol.
tures. Entropy .
2020200230,
doi:10.1148/radiol.2020200230,
arXiv:https://doi.org/10.1148/radiol.2020200230.
He, K., Zhang, X., Ren, S., Sun, J., 2015. Spatial pyrapMID: 32017661.
mid pooling in deep convolutional networks for visual
16

recognition. IEEE transactions on pattern analysis and
machine intelligence 37, 1904–1916.

fully connected crf for accurate brain lesion segmentation. Medical Image Analysis 36, 61 – 78.
URL: http://www.sciencedirect.com/science/
article/pii/S1361841516301839, doi:https://
doi.org/10.1016/j.media.2016.10.004.

He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual
learning for image recognition, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 770–778.

Kingma, D.P., Rezende, D.J., Mohamed, S., Welling, M.,
2014. Semi-supervised learning with deep generative
He, X., Yang, X., Zhang, S., Zhao, J., Zhang, Y., Xing, E.,
models, in: Proceedings of the 27th International ConXie, P., 2020. Sample-efficient deep learning for covidference on Neural Information Processing Systems 19 diagnosis based on ct scans. doi:10.1101/2020.
Volume 2, pp. 3581–3589.
04.13.20063941.
Kingma, D.P., Welling, M., 2014. Auto-encoding variaHiggins, I., Amos, D., Pfau, D., Racanière, S., Matthey,
tional bayes. arXiv:1312.6114.
L., Rezende, D.J., Lerchner, A., 2018. Towards
a definition of disentangled representations. ArXiv Kmiec, S., Bae, J., An, R., 2018. Learnable poolabs/1812.02230.
ing methods for video classification. arXiv preprint
arXiv:1810.00530 .
Hu, S., Gao, Y., Niu, Z., Jiang, Y., Li, L., Xiao, X., Wang,
M., Fang, E.F., Menpes-Smith, W., Xia, J., Ye, H., Li, L., Qin, L., Xu, Z., Yin, Y., Wang, X., Kong,
Yang, G., 2020. Weakly supervised deep learning for
B., Bai, J., Lu, Y., Fang, Z., Song, Q., Cao, K.,
covid-19 infection detection and classification from ct
Liu, D., Wang, G., Xu, Q., Fang, X., Zhang,
images. arXiv:2004.06689.
S., Xia, J., Xia, J., 2020. Artificial intelligence
distinguishes covid-19 from community acquired
Huang, G., Liu, Z., Van Der Maaten, L., Weinberger,
pneumonia on chest ct.
Radiology 0, 200905.
K.Q., 2017. Densely connected convolutional netURL:
https://doi.org/10.1148/radiol.
works, in: 2017 IEEE Conference on Computer Vision
2020200905,
doi:10.1148/radiol.2020200905,
and Pattern Recognition (CVPR), pp. 2261–2269.
arXiv:https://doi.org/10.1148/radiol.2020200905.
pMID: 32191588.
Ivanov, O., Figurnov, M., Vetrov, D., 2018. Variational autoencoder with arbitrary conditioning. Liao, F., Liang, M., Li, Z., Hu, X., Song, S., 2019. EvalarXiv:1806.02382.
uate the malignancy of pulmonary nodules using the
Jin, S., Wang, B., Xu, H., Luo, C., Wei, L., Zhao,
W., Hou, X., Ma, W., Xu, Z., Zheng, Z., Sun, W.,
Lan, L., Zhang, W., Mu, X., Shi, C., Wang, Z.,
Lee, J., Jin, Z., Lin, M., Jin, H., Zhang, L., Guo,
J., Zhao, B., Ren, Z., Wang, S., You, Z., Dong, J.,
Wang, X., Wang, J., Xu, W., 2020. Ai-assisted ct
imaging analysis for covid-19 screening: Building and
deploying a medical ai system in four weeks. medRxiv
URL:
https://www.medrxiv.org/content/
early/2020/03/23/2020.03.19.20039354,
doi:10.1101/2020.03.19.20039354.

3-d deep leaky noisy-or network. IEEE Transactions
on Neural Networks and Learning Systems 30, 3484–
3495.
Lin, T., Dollár, P., Girshick, R., He, K., Hariharan, B.,
Belongie, S., 2017. Feature pyramid networks for object detection, in: 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 936–944.
Lin, T., Goyal, P., Girshick, R., He, K., Dollár, P., 2020.
Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 42,
318–327.

Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon, D.K., Rueckert, D., Montavon, G., Binder, A., Lapuschkin, S., Samek, W.,
Glocker, B., 2017. Efficient multi-scale 3d cnn with
Müller, K.R., 2019. Layer-wise relevance propagation:
17

an overview, in: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer, pp. 193–
209.

Tomiyama, N., Wells, A.U., Leung, A.N., 2020. The
role of chest imaging in patient management during the covid-19 pandemic: A multinational consensus statement from the fleischner society. Chest
URL: http://www.sciencedirect.com/science/
article/pii/S0012369220306735, doi:https://
doi.org/10.1016/j.chest.2020.04.003.

Onisko, A., Druzdzel, M.J., Wasyluk, H., 2001. Learning bayesian network parameters from small data
sets: application of noisy-or gates. International
Journal of Approximate Reasoning 27, 165 – 182.
URL: http://www.sciencedirect.com/science/ Ruiz, A., Martinez, O., Binefa, X., Verbeek, J., 2019.
article/pii/S0888613X01000391, doi:https://
Learning disentangled representations with referencedoi.org/10.1016/S0888-613X(01)00039-1.
based variational autoencoders, in: ICLR workshop on
Learning from Limited Labeled Data, pp. 1–17. URL:
Oveneke, M.C., Aliosha-Perez, M., Zhao, Y., Jiang,
https://hal.inria.fr/hal-01896007.
D., Sahli, H., 2016. Efficient convolutional autoencoding via random convexification and frequency- Salehi, S., Abedi, A., Balakrishnan, S., Ghodomain minimization. arXiv:1611.09232.
lamrezanezhad, A., 2020.
Coronavirus disease 2019 (covid-19) imaging reporting and
Pérez Fernández, S., Martínez Camblor, P., Filzmoser, P.,
data system (covid-rads) and common lexicon:
Corral Blanco, N.O., 2018. nsroc: An r package for
a proposal based on the imaging data of 37
non-standard roc curve analysis. The R Journal, 10 (2)
studies.
European Radiology URL: https:
.
//doi.org/10.1007/s00330-020-06863-0,
doi:10.1007/s00330-020-06863-0.
Prokop, M., van Everdingen, W., van Rees Vellinga, T., Quarles van Ufford, J., Ströger, L.,
Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R.,
Beenen, L., Geurts, B., Gietema, H., Krdzalic,
Parikh, D., Batra, D., 2017. Grad-cam: Visual explanaJ., Schaefer-Prokop, C., van Ginneken, B., Brink,
tions from deep networks via gradient-based localizaM.a., 2020.
Co-rads - a categorical ct assesstion, in: 2017 IEEE International Conference on Comment scheme for patients with suspected covid-19:
puter Vision (ICCV), pp. 618–626.
definition and evaluation.
Radiology 0, 201473.
URL:
https://doi.org/10.1148/radiol. Shan, F., Gao, Y., Wang, J., Shi, W., Shi, N., Han, M.,
2020201473,
doi:10.1148/radiol.2020201473,
Xue, Z., Shen, D., Shi, Y., 2020. Lung infection quanarXiv:https://doi.org/10.1148/radiol.2020201473.
tification of covid-19 in ct images with deep learning.
pMID: 32339082.
arXiv:2003.04655.

Ronneberger, O., Fischer, P., Brox, T., 2015.
U- Sharif Razavian, A., Azizpour, H., Sullivan, J., Carlsson,
Net: Convolutional Networks for Biomedical Image
S., 2014. Cnn features off-the-shelf: an astounding
Segmentation, in: Medical Image Computing and
baseline for recognition, in: Proceedings of the IEEE
Computer-Assisted Intervention - MICCAI 2015, pp.
conference on computer vision and pattern recognition
234–241. doi:10.1007/978-3-319-24574-4_28,
workshops, pp. 806–813.
arXiv:1505.04597.
Shi, F., Xia, L., Shan, F., Wu, D., Wei, Y., Yuan, H.,
Rubin, G.D., Ryerson, C.J., Haramati, L.B., SverzelJiang, H., Gao, Y., Sui, H., Shen, D., 2020. Large-scale
lati, N., Kanne, J.P., Raoof, S., Schluger, N.W.,
screening of covid-19 from community acquired pneuVolpi, A., Yim, J.J., Martin, I.B., Anderson, D.J.,
monia using infection size-aware classification. ArXiv
Kong, C., Altes, T., Bush, A., Desai, S.R., Goldin,
abs/2003.09860.
J., Goo, J.M., Humbert, M., Inoue, Y., Kauczor,
H.U., Luo, F., Mazzone, P.J., Prokop, M., Remy- Shrikumar, A., Greenside, P., Shcherbina, A., Kundaje,
Jardin, M., Richeldi, L., Schaefer-Prokop, C.M.,
A., 2016. Not just a black box: Learning important fea18

tures through propagating activation differences. arXiv
preprint arXiv:1605.01713 .

N., Gonidakis, P., Amador Sánchez, S., Parizel, P.M.,
de Mey, J., Vandermeulen, D., Vercauteren, T., Robben,
D., Smeets, D., Maes, F., Vandemeulebroucke, J.,
Suetens, P., 2020. Comparative study of deep learning
methods for the automatic segmentation of lung, lesion,
and lesion type in ct scans of covid-19 patients. submitted to A Joint Special Issue between Medical Image
Analysis (Elsevier) and Transactions on Medical Imaging (IEEE) .

Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition.
arXiv:1409.1556.

Simpson, S., Kay, F.U., Abbara, S., Bhalla, S., Chung,
J.H., Chung, M., Henry, T.S., Kanne, J.P., Kligerman,
S., Ko, J.P., Litt, H., 2020. Radiological society of north
america expert consensus statement on reporting chest Ulyanov, D., Vedaldi, A., Lempitsky, V., 2016. Instance
ct findings related to covid-19. endorsed by the society
normalization: The missing ingredient for fast stylizaof thoracic radiology, the american college of radioltion. arXiv preprint arXiv:1607.08022 .
ogy, and rsna. Radiology: Cardiothoracic Imaging 2,
e200152. URL: https://doi.org/10.1148/ryct. Wang, X., Deng, X., Fu, Q., Zhou, Q., Feng, J., Ma, H.,
Liu, W., Zheng, C., 2020. A weakly-supervised frame2020200152,
doi:10.1148/ryct.2020200152,
arXiv:https://doi.org/10.1148/ryct.2020200152. work for covid-19 classification and lesion localization
from chest ct. IEEE Transactions on Medical Imaging
Singh, D., Kumar, V., Vaishali, Kaur, M., 2020. Classi, 1–1.
fication of covid-19 patients from chest ct images using multiobjective differential evolution based convo- Wu, Y.H., Gao, S.H., Mei, J., Xu, J., Fan, D.P., Zhao,
C.W., Cheng, M.M., 2020. Jcs: An explainable covidlutional neural networks. European Journal of Clinical
19 diagnosis system by joint classification and segmenMicrobiology and Infsctious Diseases doi:10.1007/
tation. arXiv:2004.07054.
s10096-020-03901-z.
Sohn, K., Yan, X., Lee, H., 2015. Learning structured
output representation using deep conditional generative models, in: Proceedings of the 28th International
Conference on Neural Information Processing Systems
- Volume 2, MIT Press, Cambridge, MA, USA. pp.
3483–3491.
Song, Y., Zheng, S., Li, L., Zhang, X., Zhang, X.,
Huang, Z., Chen, J., Zhao, H., Jie, Y., Wang,
R., Chong, Y., Shen, J., Zha, Y., Yang, Y., 2020.
Deep learning enables accurate diagnosis of novel
coronavirus (covid-19) with ct images. medRxiv
URL:
https://www.medrxiv.org/content/
early/2020/02/25/2020.02.23.20026930,
doi:10.1101/2020.02.23.20026930.
Sundararajan, M., Taly, A., Yan, Q., 2017. Axiomatic
attribution for deep networks, in: Proceedings of the
34th International Conference on Machine Learning,
pp. 3319–3328.
Tilborghs, S., Dirks, I., Fidon, L., Willems, S., Eelbode,
T., Bertels, J., Ilsen, B., Brys, A., Dubbeldam, A., Buls,

Xu, B., Xing, Y., Peng, J., Zheng, Z., Tang, W., Sun, Y.,
Xu, C., Peng, F., 2020. Chest ct for detecting covid19: a systematic review and meta-analysis of diagnostic accuracy. European Radiology URL: https://
doi.org/10.1007/s00330-020-06934-2, doi:10.
1007/s00330-020-06934-2.
Zhang, H., Cissé, M., Dauphin, Y., Lopez-Paz, D., 2018.
mixup: Beyond empirical risk minimization. ArXiv
abs/1710.09412.
Zhang, J., Sanderson, A.C., 2009. Jade: Adaptive differential evolution with optional external archive. Trans.
Evol. Comp 13, 945–958. doi:10.1109/TEVC.2009.
2014613.
Zhao, J., He, X., Yang, X., Zhang, Y., Zhang, S., Xie, P.,
2020. Covid-ct-dataset: A ct scan dataset about covid19. arXiv:2003.13865.
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba,
A., 2016. Learning deep features for discriminative localization, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2921–2929.
19

Zhou, Z., Rahman Siddiquee, M.M., Tajbakhsh, N.,
Liang, J., 2018. Unet++: A nested u-net architecture
for medical image segmentation, in: Deep Learning in
Medical Image Analysis and Multimodal Learning for
Clinical Decision Support, pp. 3–11.

20

