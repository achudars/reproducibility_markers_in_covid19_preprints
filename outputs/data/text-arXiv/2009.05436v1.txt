Semi-Supervised Active Learning for COVID-19
Lung Ultrasound Multi-symptom Classification
1st Lei Liu, 1st Wentao Lei,
3th Xiang Wan, 4th Li Liu‚àó

arXiv:2009.05436v1 [cs.CV] 9 Sep 2020

Shenzhen Research Institute of Big Data
the Chinese University of Hong Kong, Shenzhen
Shenzhen, China
{220019056,220019019}@link.cuhk.edu.cn,
wanxiang@sribd.cn, liuli@cuhk.edu.cn

1st Yongfang Luo, 2nd Cheng Feng
Department of Medical Ultrasonics,
National Clinical Research Center for Infectious Disease
Shenzhen Third People‚Äôs Hospital (Second Hospital Affiliated
to Southern University of Science and Technology)
Shenzhen, China
luoyongfang2005@foxmail.com, chaosheng-01@szsy.sustech.edu.cn

Abstract‚ÄîUltrasound (US) is a non-invasive yet effective medical diagnostic imaging technique for the COVID-19 global pandemic. However, due to complex feature behaviors and expensive
annotations of US images, it is difficult to apply Artificial Intelligence (AI) assisting approaches for lung‚Äôs multi-symptom (multilabel) classification. To overcome these difficulties, we propose
a novel semi-supervised Two-Stream Active Learning (TSAL)
method to model complicated features and reduce labeling costs
in an iterative procedure. The core component of TSAL is
the multi-label learning mechanism, in which label correlations
information is used to design multi-label margin (MLM) strategy
and confidence validation for automatically selecting informative
samples and confident labels. On this basis, a multi-symptom
multi-label (MSML) classification network is proposed to learn
discriminative features of lung symptoms, and a human-machine
interaction is exploited to confirm the final annotations that
are used to fine-tune MSML with progressively labeled data.
Moreover, a novel lung US dataset named COVID19-LUSMS is
built, currently containing 71 clinical patients with 6,836 images
sampled from 678 videos. Experimental evaluations show that
TSAL using only 20% data can achieve superior performance to
the baseline and the state-of-the-art. Qualitatively, visualization
of both attention map and sample distribution confirms the good
consistency with the clinic knowledge.
Index Terms‚ÄîCOVID-19, Ultrasound Imaging, Multi-Label
Classification, Active Learning, Semi-Supervised Learning

I. I NTRODUCTION
The novel coronavirus (COVID-19) has spread worldwide
and is now officially a global pandemic. Typical diagnosing tools mainly include Computed tomography (CT) and
X-ray, which are characterized by their relatively accurate
performances [1]. However, due to the prevalence of COVID19, in practice, deep learning-based CT or X-ray approaches
remain several challenges. Firstly, CT and X-ray tools are
generally time-consuming and inflexible, and involve extra
radiations. Secondly, their images of COVID-19 patients are
not easy to collect because their imaging procedures involve
isolating patients, operating complex clinical equipment, and
many other nontrivial processes.
In contrast, lung ultrasound (US) imaging is preferred as
a mature tool for its fast, flexible, and reliable deployment,
* Corresponding author

A-line

B-line

Pleural lesion

Pleural effusion

Fig. 1. Symptoms of the established COVID19-LUSMS dataset.

especially in emergencies [2]. More importantly, it is noninvasive and can work at the bedside. Recently, some works
[3]‚Äì[5] focused
symptom
detection
B-line based on
B-line
B-line
B-line on COVID-19
lung USPleural
images.
Indeed,
based
on
lung
US
images,
automatic
Pleural lesion
Pleural lesion Pleural lesion
lesion
Pleural effusion
Pleural effusion
Pleural effusion
AI assisting approaches
for COVID-19
symptoms
classification can also make significant sense for medical diagnoses
of doctors. Therefore, we focus on lung ultrasound multisymptom classification in this work.
In practice, the automatic classification of COVID-19 lung
symptoms is difficult for twofold reasons. Firstly, the lung
US images of COVID-19 patients may simultaneously involve multiple symptoms, which may exhibit complicated
features (see in Fig. 1). One possible solution is following
the multi-label learning, which targets to judge whether an
image possesses multiple characteristics denoted by labels
[6], [7]. Besides, it is expensive and tedious to collect and
annotate numerous COVID-19 Lung US images. To address
this difficulty, the feasible solution is active learning algorithm
[8], [9], which aims to achieve effective performance given a
limited labeling cost.
To achieve effective performance with the least labeling
cost, we propose a novel semi-supervised Two-Stream Active
Learning (TSAL) model, which works in an iterative procedure by a sample selection, pseudo-label validation, humanmachine interaction (HMI) and CNN parameters updating
alternately. In TSAL, a multi-symptom multi-label (MSML)
classification network is constructed as the basic model for
feature learning. Based on the label correlation information,
the sample stream works for informative sample selection by
newly designed multi-label margin strategy (MLM), while the
label stream is exploited to assign confident pseudo-label for
selected images. Then the HMI is used for confirming the

Sample Stream
ùëÉ11 ùëÉ12 ùëÉ13 ùëÉ14

Make an action

ùëÉ21 ùëÉ22 ùëÉ23 ùëÉ24

+
224

‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶

64

64

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

3

stage4
stage3

Decision Agent

stage2

Selected samples

stage1
22

4

State

Input

Unlabeled Set

64
Conv
7√ó7

64
bn1
relu

1

4

ùëÉùëõ1 ùëÉùëõ2 ùëÉùëõ3 ùëÉùëõ4

Classifier

Sigmoid

3
maxpooling

Label Stream
1

0

1

0

0

1

1

1

Assign a label

‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶

MSML

0

1

0

1

Pseudo label

Labels
1000
0011
‚Ä¶
0101

Confidence Validation

Fine-tune

Human-machine interaction

Fig. 2. Overview of the proposed framework of MSML-TSAL. Sample stream
is to select informative samples by state and decision agent, which are used to
describe the prediction results of all unlabeled images and determine which
image candidate will be selected. Label stream is to validate and assign pseudo
labels for the selected images. Human-machine interaction is to confirm
the final annotations for fine-tuning MSML model. This iterative process
terminates when it reaches the annotation budget or required performance.

final annotations to fine-tune the MSML. An overview of the
proposed method is shown in Fig. 2.
Besides, a large-scale dataset of lung US images for
COVID-19 is built for this work. Some examples are shown
in Fig. 1. Experiments on this dataset show that our proposed
method achieves superior classification performance, outperforming the baseline models and the state-of-the-art (SOTA)
using less than 20% of the labeled images. To the best of our
knowledge, this is the first work on automatic multi-symptom
multi-label classification for COVID-19 lung US images.
In summary, this work contains the following three contributions:
1) A novel large-scale dataset containing lung US images of
COVID-19 is built specifically for this work. This dataset
is annotated in the multi-label form by medical experts.
2) We propose a novel semi-supervised TSAL method,
which effectively reduces the labeling cost. It exploits label correlations information to select informative samples
and confident pseudo labels.
3) Experimental results show that our method achieves superior performance using less than 20% labeled data,
compared with baseline and SOTA. Explainable analyses
using attention map and sample distribution well confirm
that our results are well consistent with clinic expertise.
II. R ELATED W ORK
A. Lung US techniques for COVID-19
Many studies [2], [10], [11] reported the superiority of US
imaging in diagnosing pneumonia and related lung conditions. Sloun et al. [12] proposed a fully convolutional neural
network to identify and localize the B-lines in clinical lung
ultrasonography. Born et al. [3] trained a POCOVID-Net on
a 3-class dataset and achieved good accuracy in classification.
The work [13] presented a novel network that simultaneously
predicted the disease severity score associated with an input
frame. However, they generally require a large-scale labeled
US-image dataset, where annotations are expensive.

Fig. 3. Overview of the MSML model‚Äôs architecture.

B. Multi-label image classification
As an important branch of classification, multi-label classification [14], [15] has been widely explored in recent years,
supported by CNNs [16], [17]. Multi-label image classification
plays an important part in bridging the gap between lowlevel features and high-level semantic information [6]. Recently, some multi-label classification methods applied CNNs
for competitive performances. For a classical example, [18]
optimizes top-k ranking objectives combined with convolutional architectures to learn a better feature representation.
Hypotheses-CNN-Pooling [19] incorporates object segmentation hypotheses with max pooling to generate multi-label
predictions. For lung US images, a COVID-19 patient may
perform multiple symptoms at the same time. Thus, COVID19 symptoms classification can be formulated as a multisymptom multi-label classification task.
C. Active learning
AL has been successfully deployed into semantic segmentation [20]), image classification [21]), human pose estimation
[8], etc. These applications indicate that AL is a great choice
for labeling efforts reduction. Besides, there have been many
popular selection strategies in the literature, mainly including
query by committee, expected error reduction, expected model
change, and uncertainty sampling. These strategies are usually
exploited for single-label classification tasks, which is not
suitable for COVID-19 lung US images classification with
complex multi-label feature behaviors. In this work, to reduce
the labeled cost for COVID-19 diagnosis, we introduce TSAL
into multi-symptom multi-label classification to actively select
informative images for annotations.
III. M ETHODOLOGY
A. Multi-symptom multi-label model
We propose a MSML network to tackle the COVID-19
lung US classification task, as shown in Fig. 3. Backbone
is ResNet50 [16] with ImageNet pre-train for its competitive
performance. The hidden FC layer is removed, while layers
after the final convolutional layer are also removed to avoid
too deep or too complicated architecture. Then it is followed
by one specifically designed classifier consisting of average
pooling and FC layer. Sigmoid cross-entropy loss is adpoted
to learn the discriminative features, which is defined as:
Lcross = ‚àí

N
1 X
(y √ó (log xÃÇ) + (1 ‚àí y) √ó (log(1 ‚àí xÃÇ))), (1)
N i=1

where x is the output vector before sigmoid activation on the
ground truth class y of the input image, N is the batch size,
and xÃÇ = 1/1 + e‚àíx .

Background logic

B. Two-stream deep active learning framework

Mx = p(l1 | x) ‚àí max p(li | x) ,
2‚â§i‚â§N

(2)

where p(l1 |x) is the probability of input image x for A-line.
Besides, it is noticed that there are few active strategies for
the multi-label multi-class classification task. We redesign the
classical and common strategies of multi-class classification,
including Least Confidence and Entropy [22], [23] for multi-

Probability

MSML

Human-Machine Interaction

The detailed framework of MSML-TSAL is presented in
Fig. 2, which contains a sample stream and a label stream.
The core of sample stream is to select informative samples.
The label stream is to assign pseudo labels by confidence
validation. At first, we perceive all unlabeled images as the
candidate pool. At discrete AL iteration t, for each unlabeled
image, MSML provides a prediction state St . Then decision
agent can make an action for sample selection according to
St and selection strategy. Then pseudo labels are assigned
by confidence validation. The final annotations would be
confirmed by human-machine interaction (HMI) to fine-tune
the MSML model. This iterative operation repeats until the
expected performance of MSML or the empty candidate pool.
1) Sample stream: Action: The action is to select unlabeled
images from the candidate pool. The pre-defined annotation
efforts for each AL iteration are denoted by Kmax , which
restricts the maximal annotation efforts for the entire MSMLTSAL. At each iteration t with the state St , the decision
agent makes an action to select images based on the decision
function f (At |St ). Once At = {i1 , i2 , ..., iKmax } is executed,
the decision agent is able to query them for annotations via
HMI and remove them from the candidate pool.
State: The state is utilized to describe the relationship
between unlabeled images and the prediction capability of
the model. Prediction probability has been widely exploited
to measure the prediction capability of the model in active
learning tasks by using the outputs of the MSML model. In
this work, we construct a prediction probability matrix to take
output probabilities as the state value. Given a candidate pool
C = {c1 , c2 , ..., cs }, where s is the size. The ith prediction
vector can be extracted and written as pi = {pi1 , pi2 , ..., pim },
where m is the number of the labels. The state St at discrete
AL iteration t can be denoted by these as St = {p1 , p2 , ..., ps }.
Decision Agent: The decision agent is used to measure
which image is worth annotating using selection strategies.
To this end, we specially design an active-learning strategy
called multi-label margin (MLM), which aims to evaluate
the informativeness for each unlabeled sample. For lung US
images of COVID-19 patients, A-line denotes the health while
others denote the disease. Besides, an image cannot exhibit
A-line and other symptoms simultaneously, which is also not
reasonable in medical knowledge. Intuitively, if the probability
of A-line has a small margin with other symptoms, it is
difficult for the model to judge whether the image is healthy or
unhealthy, which may be caused by the un-learned information
of this image. The MLM is defined as:

Selected image

Prediction

A-line

0

B-line

0.8

P-lesion

0.4

P-effusion

0.9

Pseudo
label

Confidence
Validation

Symptom Example

A-line

0

B-line

1

P-lesion

0

P-effusion

1

Annotations

A-line
B-line

B-line
Ôºà0.8Ôºâ

A-line
Ôºà0Ôºâ

Active
Annotation

Active
Comparison

P-lesion
P-effusion

Selected image
P-lesion
(0.4)

P-effusion
(0.9)

‚àö

‚àö

CONFIRM

HMI UI

Fig. 4. Overview of the interface of human-machine interaction.

label multi-class classification. The detailed decision functions
are as follows.
Least confidence (LC): lower confidence illustrates that it
is a hard image for the current classifier to make a correct
prediction. The calculation of LC is:
Lx = min

max p(li | x),

1‚â§x‚â§M 1‚â§i‚â§N

(3)

where M is the size of the unlabeled set and N is the number
of labels (i.e., symptoms). p(li |x) denotes the probability of
input image x with symptom li .
Multi-label entropy (MLE): higher entropy indicates that the
image carries rich information. The MLE is denoted as:
Ex =

N
X
i=1

(p(li | x) log p(li | x) + p(li | x) log p(li | x)), (4)

where p(li |x) is the probability of input image x with symptom
li . p(li |x) is the probability of x that not with symptom li .
2) Label stream: Pseudo label: In the label stream, the
pseudo label denotes as the most probable symptom for the
input image. To this end, a fixed threshold is assigned for
a pseudo label (0 or 1). For each symptom, if its prediction
probability is higher than the threshold, its pseudo label is
set as ‚Äú1‚Äù, otherwise it is ‚Äù0‚Äù. The final annotation is the
combination of these pseudo labels for all symptoms, i.e.,
‚Äú0101‚Äù.
Confidence validation: Label correlations information has
been widely employed for multi-label learning [24] by mining
the potential relationships among different labels, which also
can be exploited for confidence validation. In this work,
we construct a label correlation table to store the intracombination distribution information for each label combination. To construct this table, given the selected and labeled
samples in AL iteration t, their prediction probabilities are denoted as a probability matrix P t (m, k), where m is the number
of the labels and k is the number of the labeled samples. P t
can be divided into several submatrixes according to the label
combinations of these samples, i.e., P t ‚Üí {P1t , P2t , ¬∑ ¬∑ ¬∑ , Pnt },

where n is the number of the label combinations. Then we
can obtain average probability vectors of these submatrixs as:
ptavg = average(Pit ), i = {1, 2, ¬∑ ¬∑ ¬∑ , n}.

(5)

piavg
, i = {1, 2, ¬∑ ¬∑ ¬∑ , n},
pri = Pm i
j=1 pavg j

(6)

The relationship vectors RV = {pr1 , pr2 , ¬∑ ¬∑ ¬∑ , prn } can be
calculated via normalization operation:

where piavg j is the jth value of piavg vector. Each RV vector reflects the intra-combination distribution for each label
combination. Thus the label correlation table is to map from
label combination to intra-combination distribution, denoted
as T ab(Com, RV ). The table is initialized after the first AL
iteration and updated in the next iterations.
Given a pseudo-label combination with its prediction probability vector pÃÇ, normalization operation is also executed
to transform pÃÇ into the RSV form pÃÇr . The top nearest
label combination in T ab(Com, RV ) of pÃÇr is defined as
N ear(pÃÇr , T ab(Com, RV )), which is obtained through:
arg min Dist(pÃÇr , T ab(Com, RV )),

(7)

RV

where Dist is the Manhattan distance, and the distance
between pÃÇr with each RV vector in T ab(Com, RV ) denotes
Dist(pÃÇr , T ab(Com, RV )). N ear(pÃÇr , T ab(Com, RV )) is the
most confident label combination, which can be used to refine
the pseudo-label combination.
After refining all pseudo-label combinations for the selected
samples, a new T ab(Com, RV ) can be computed. To better
update the table, we introduce one constraint condition to
restrict the updating of T ab(Com, RV ). For each RV in the
new T ab(Com, RV ), it replaces the corresponding RV in
T ab(Com, RV ) only if it satisfies the following condition:
r
pr‚àó
i √ó li ‚â• pi √ó li , i = {1, 2, . . . , m},

(8)

where li is the ith label in the label combination.
3) Human-machine interaction: In this section, given the
selected samples and pseudo labels, a human-machine interface is designed for the convenience of annotations. To better
understand the HMI, we illustrate the interface in Fig. 4. The
first row is the pipeline of pseudo-label generation, which is
the background logic of the interface. The second line is the
user interface, which would exhibit examples for each label
and the selected image. Besides, the annotations would be
made as defaults according to the pseudo label. The human
annotator only needs to judge if the default annotations are
consistent with the examples.
4) CNN network updating: MSML is updated with a finetuning algorithm. At each AL iteration t, the CNN is finetuned via selected set Sett . During fine-tuning, only weights
of the last three layers in MSML will be updated, while the
remained weights will be frozen to the values from the pretraining. When more images are selected and annotated, the
model becomes more robust. The renewed network is exploited
upgrading the state initialization. This iterative training scheme
stops with a fixed annotation budget or satisfied performance.

IV. E XPERIMENTS
A. Implementation Details
We build the first version of the COVID19-LUSMS dataset,
called COVID19-LUSMS v1. US videos are collected in the
Third Peoples Hospital of Shenzhen, China. A total of 71
COVID-19 patients are inspected, including 678 videos. Random rotation (up to 10 degrees) and horizontal flips are used
as data augmentation transformations. The SGD optimization
is adopted with a learning rate as 2 √ó 10‚àí3 , batch size as 32
and momentum as 0.9. Kmax of TSAL is set as 100. The
active iteration was repeated for 20 iterations at most.
B. Quantitative Analysis
1) MSML: The performance of the proposed MSML on
the multi-symptom classification task is visualized in Fig.6,
depicting the ROC curve for each label. Clearly, the model
learns to classify the images, with the ROC-AUC scores of Aline, B-line and Pleural-effusion all above or equal to 0.985.
While the ROC-AUC scores of Pleural-lesion is lower than
other symptoms, but still more than 0.85.
In Tab. I, we illustrate the performance comparisons about
two SOTA methods and four baselines including POCOVIDNet [3], NNBD [12], VGG16 [17], and ResNet [16], [16]. In
detail, (1) Accuracy: MSML achieves the best performance
with accuracy at 100%, 95.72%, and 80.98% for A-line, Bline and pleural lesion, respectively. MSML model almost
outperforms all baseline models concerning accuracy. (2) Sensitivity: MSML model achieves 100%, 98.78%, 81.38%, and
6.08% sensitivity for A-line, B-line, pleural lesion and pleural
effusion, respectively. Our MSML model performs similar
sensitivity with baseline models, which is mainly because
of the relatively distinct patterns. Besides, these methods
perform poor sensitivity for pleural effusion. Our explanation is that multiple symptoms appearing simultaneously may
cause complicated patterns, especially for pleural effusion. (3)
Specificity: MSML model achieves 100%, 81.81%, 80.67%,
and 100% specificity for A-line, B-line, pleural lesion and
pleural effusion, respectively. MSML model performs best for
all symptoms compared with baseline models, because the
missed diagnosing cases are very few due to distinct patterns.
2) MSML-TSAL: We report in Tab. I the performance of
MSML-TSAL with four selection strategies. (1) Accuracy:
MLE strategy only uses 27.6% data to train an effective
MSML model, whose accuracy can outperform the baseline
model except for pleural effusion. MLE can use 14.7%
annotated data to obtain a similar performance as the full
training data. Only using 16.6% data, LC strategy can obtain
comparable accuracy results as the full training set. (2) Sensitivity: All strategies can achieve similar sensitivity using fewer
images for A-line and B-line, but perform worse for pleural
lesion, because the pleural lesion is generally exhibited with
other symptoms together. For pleural effusion, all strategies
obtain almost 0 sensitivity when using less than 30% data,
because the image number of pleural effusion is far less than
others and the pleural effusion is easily influenced by the

TABLE I
C OMPARISONS WITH THE BASELINES AND SOTA. T HE BOLDS ARE OF OUR METHOD . L ESS DATA IS BETTER AND THE HIGHER IS BETTER FOR OTHERS .
A-line

Method
VGG16
ResNet34
ResNet50
ResNet101
POCOVID-Net
NNBD
MSML+TSAL(Random)
MSML+TSAL(MLE)
MSML+TSAL(LC)
MSML
MSML+TSAL(MLM)

B-line

P-lesion

P-effusion

data

Acc

Sen

Spe

Acc

Sen

Spe

Acc

Sen

Spe

Acc

Sen

Spe

100
100
100
100
100
100
99.85
100
100
100
100

100
100
100
100
100
100
100
100
100
100
92.38

100
100
100
100
100
100
100
100
100
100
100

88.39
90.88
88.60
90.45
84.97
90.31
90.74
89.52
94.30
95.72
98.50

98.34
96.87
98.95
98.17
90.35
99.91
98.52
97.56
95.91
98.78
98.79

43.08
63.63
41.50
55.33
60.47
46.64
65.61
52.96
86.95
81.81
92.49

60.82
73.36
80.34
79.91
80.84
71.86
83.47
80.34
83.19
80.98
83.26

76.77
81.71
82.86
82.37
79.90
68.20
77.92
75.94
78.74
81.38
76.77

48.68
67.00
78.41
78.04
81.55
74.65
98.87
83.68
86.57
80.67
96.36

88.53
89.31
89.45
89.45
91.02
89.45
89.38
89.45
89.45
90.09
89.45

0
0
0
0
14.86
0
0
0
0
6.08
0

98.96
99.84
100
100
100
100
99.92
100
100
100
100

(a) Accuracy for A-line

(b) Accuracy for B-line

(c) Accuracy for P-effusion

100%
100%
100%
100%
100%
100%
16.6%
27.6%
16.6%
100 %
14.7%

(d) Accuracy for P-lesion

Fig. 5. Comparisons of the proposed MSML-TSAL method with different selection strategies on accuracy. Baseline means using all labeled images in the
dataset without using any selection strategy. Abscissa indicates the training iterations.

C. Qualitative Analysis

Pleural
lesion

Pleural
lesion

Pleural
effusion

Pleural
effusion

B-line

B-line

A-line

A-line

Fig. 6. ROC curve for the proposed MSML model.

original

grad CAM grad CAM++ grad CAM* grad CAM++*

(a) Test samples for MSML

doctor‚Äôs operations. (3) Specificity: These strategies merely
exploit 16.6%, 27.6%, 16.6%, and 14.7% data to achieve
similar or better specificity performance, among which the
random strategy performs worst. Surprisingly, MSML-TSAL
can improve specificity for B-line and pleural effusion by a
large margin, which is because that selection strategies can
alleviate the unbalanced problem.
In Fig. 5, we summarize the accuracy improvement curves
of MSML-TSAL combined with four selection strategies that
achieve the best performance on COVID19-LUSMS dataset.
Almost all curves of different strategies can achieve a similar
and better performance compared with the baseline that uses
the full training set (light blue curves). During the whole training process, these curves have several oscillations, indicating
that features in the COVID19-LUSMS dataset are complex.
Besides, we note that MLM (red curves) performs the best
among the 4 strategies, with its highest accuracy at the final
AL iteration. From the view of the smoothness, the MLM
performs the most stable changing tendency.

original

grad CAM grad CAM++ grad CAM* grad CAM++*

(b) Selected samples for MSML-TSAL

Fig. 7. Visualized results. (a) The training weights of MSML is used
for visualization. (b) The training weights of MSML-TSAL is used for
visualization of selected samples. (*) denotes that the attention map is
overlapped on the original image.

1) MSML: As shown in Fig. 7(a), attention regions from
the MSML model are consistent with the regions from the
doctor. Based on clinical medical observation, A-line has an
obvious horizontal-line region in the upper part of the images.
B-line has an obvious vertical-line region in the whole image.
Pleural effusion has many obvious irregular regions in the

TABLE II
A MOUNT OF MANUALLY LABELED DATA .
Iteration
HMI
HMI + pseudo-label
HMI + pseudo-label + CV

1st
100%
100%
100%

2nd
100%
29%
2%

3rd
100%
11%
1%

4th
100%
5%
0%

5th
100%
1%
0%

whole image. Pleural lesion has many obvious irregular lines
around the pleura, which lies in the top position of the lung
US image. Corresponding to the attention heatmaps, all of the
visualized attention regions perform consistent results with the
clinical medical observation.
2) MSML-TSAL: As shown in Fig. 7(b), these images contain large dark regions, which are not pathological changing
regions and may cause complicated characteristics for the
model learning. These characteristics are not well learned via
previous training because the attention regions are likely to
focus on the dark regions. Thus, these images should join in
further training for their complicated information.
D. Component analysis for label stream
To justify that label stream works well for pseudo-label
assignment, we conduct an ablation study for the component analysis. CV is confidence validation. As shown in
Tab. II, human annotators need to manually annotate all
selected images in each iteration, only using HMI without
pseudo-label and confidence validation. By pseudo-label, the
manual annotations gradually decrease with the performance
improvement of MSML. Confidence validation can further
reduce the manual annotations, i.e., nearly zero after the first
iteration. Through label stream, the selected samples can be
automatically annotated, while human annotators only need to
confirm annotations in HMI rather than manual annotations.
V. C ONCLUSION
To achieve accurate classification of COVID-19 multiple
symptoms of the lung US image with less annotated data,
we innovatively propose a TSAL framework to effectively
train the MSML model with less labeling efforts in a semisupervised manner. Specifically, we design the MLM strategy
and confidence validation for TSAL by label correlations
information. Moreover, a novel large-scale lung US image
dataset with multiple COVID-19 symptoms is built in this
work. Quantitative and qualitative experimental results show
that the TSAL model can achieve competitive performance,
and we can train an effective MSML model merely using
less than 20% data of full training set. In future work, it is
worthwhile to explore the reinforcement learning to learn a
powerful and adaptive policy for image selection.
R EFERENCES
[1] D. Wang, B. Hu, C. Hu, F. Zhu, X. Liu, J. Zhang, B. Wang, H. Xiang,
Z. Cheng, Y. Xiong et al., ‚ÄúClinical characteristics of 138 hospitalized
patients with 2019 novel coronavirus-infected pneumonia in wuhan,
china,‚Äù JAMA, vol. 323, no. 11, pp. 1061‚Äì1069, 2020.
[2] J. E. Bourcier, S. Braga, and D. Garnier, ‚ÄúLung ultrasound will soon
replace chest radiography in the diagnosis of acute community-acquired
pneumonia,‚Äù Current Infectious Disease Reports, vol. 18, no. 12, pp.
1534‚Äì3146, 2016.

[3] J. Born, G. Brndle, M. Cossio, M. Disdier, J. Goulet, J. Roulin, and
N. Wiedemann, ‚ÄúPocovid-net: Automatic detection of covid-19 from a
new lung ultrasound imaging dataset (pocus),‚Äù 2020.
[4] Y. Huang, S. Wang, Y. Liu, Y. Zhang, C. Zheng, Y. Zheng, C. Zhang,
W. Min, M. Yu, and M. Hu, ‚ÄúA preliminary study on the ultrasonic manifestations of peripulmonary lesions of non-critical novel coronavirus
pneumonia (covid-19),‚Äù 2020.
[5] Q.-Y. Peng, X.-T. Wang, and L.-N. Zhang, ‚ÄúFindings of lung ultrasonography of novel corona virus pneumonia during the 20192020 epidemic,‚Äù
Intensive Care Medicine, pp. 1‚Äì2, 2020.
[6] L. Li, S. Wang, S. Jiang, and Q. Huang, ‚ÄúAttentive recurrent neural
network for weak-supervised multi-label image classification,‚Äù in Proceedings of the ACM International Conference on Multimedia, 2018.
[7] H. Guo, X. Fan, and S. Wang, ‚ÄúHuman attribute recognition by refining
attention heat map,‚Äù Pattern Recognition Letters, vol. 94, pp. 38‚Äì45,
2017.
[8] B. Liu and V. Ferrari, ‚ÄúActive learning for human pose estimation,‚Äù in
IEEE International Conference on Computer Vision, 2017.
[9] A. Vezhnevets, J. M. Buhmann, and V. Ferrari, ‚ÄúActive learning for
semantic segmentation with expected change,‚Äù in IEEE International
Conference on computer vision and pattern recognition, 2012, pp. 3162‚Äì
3169.
[10] J. E. Bourcier, J. Paquet, M. Seinger, E. Gallard, J. P. Redonnet,
F. Cheddadi, D. Garnier, J. M. Bourgeois, and T. Geeraerts, ‚ÄúPerformance comparison of lung ultrasound and chest x-ray for the diagnosis
of pneumonia in the ed,‚Äù American Journal of Emergency Medicine,
vol. 32, no. 2, pp. 115‚Äì118, 2014.
[11] A. S. Claes, P. Clapuyt, R. Menten, N. Michoux, and D. Dumitriu,
‚ÄúPerformance of chest ultrasound in pediatric pneumonia,‚Äù European
Journal of Radiology, vol. 88, no. Complete, pp. 82‚Äì87, 2017.
[12] R. J. G. Van Sloun and L. Demi, ‚ÄúLocalizing b-lines in lung ultrasonography by weakly supervised deep learning, in-vivo results,‚Äù IEEE
Journal of Biomedical and Health Informatics, vol. 24, no. 4, pp. 957‚Äì
964, 2020.
[13] S. Roy, W. Menapace, S. Oei, B. Luijten, E. Fini, C. Saltori, I. Huijben,
N. Chennakeshava, F. Mento, A. Sentelli, E. Peschiera, R. Trevisan,
G. Maschietto, E. Torri, R. Inchingolo, A. Smargiassi, G. Soldati,
P. Rota, A. Passerini, R. J. G. Van Sloun, E. Ricci, and L. Demi, ‚ÄúDeep
learning for classification and localization of covid-19 markers in pointof-care lung ultrasound,‚Äù IEEE Transactions on Medical Imaging, 2020.
[14] G. Tsoumakas and I. Katakis, ‚ÄúMulti-label classification: An overview,‚Äù
International Journal of Data Warehousing and Mining, vol. 3, no. 3,
pp. 1‚Äì13, 2009.
[15] M.-L. Zhang and Z.-H. Zhou, ‚ÄúA review on multi-label learning algorithms,‚Äù IEEE Transactions on Knowledge and Data Engineering,
vol. 26, pp. 1819‚Äì1837, 08 2014.
[16] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in IEEE International Conference on computer vision and
pattern recognition, 2016, pp. 770‚Äì778.
[17] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for
large-scale image recognition,‚Äù in International Conference on Learning
Representations, 2015.
[18] Y. Gong, Y. Jia, T. Leung, A. Toshev, and S. Ioffe, ‚ÄúDeep convolutional
ranking for multilabel image annotation,‚Äù 2014.
[19] Y. Wei, W. Xia, J. Huang, B. Ni, J. Dong, Y. Zhao, and S. Yan, ‚ÄúHcp:
A flexible cnn framework for multi-label image classification,‚Äù IEEE
Transactions on Pattern Analysis and Machine Intelligence, pp. 1901‚Äì
1907, 2016.
[20] Q. Sun, A. Laddha, and D. Batra, ‚ÄúActive learning for structured probabilistic models with histogram approximation,‚Äù in IEEE International
Conference on computer vision and pattern recognition, 2015.
[21] W. H. Beluch, T. Genewein, A. NuÃàrnberger, and J. M. KoÃàhler, ‚ÄúThe
power of ensembles for active learning in image classification,‚Äù in IEEE
International Conference on computer vision and pattern recognition,
2018.
[22] A. Holub, P. Perona, and M. C. Burl, ‚ÄúEntropy-based active learning
for object recognition,‚Äù in 2008 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition Workshops, 2008, pp. 1‚Äì8.
[23] B. Settles, ‚ÄúActive learning literature survey,‚Äù University of Wisconsin‚Äì
Madison, Computer Sciences Technical Report 1648, 2009.
[24] M. Zhang and Z. Zhou, ‚ÄúA review on multi-label learning algorithms,‚Äù
IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 8,
pp. 1819‚Äì1837, 2014.

