1

Decision-Making Algorithms for Learning and
Adaptation with Application to COVID-19 Data
Stefano Marano, Senior Member, IEEE and Ali H. Sayed, Fellow, IEEE

arXiv:2012.07844v1 [eess.SP] 14 Dec 2020

Abstract
This work focuses on the development of a new family of decision-making algorithms for adaptation and learning, which
are specifically tailored to decision problems and are constructed by building up on first principles from decision theory. A
key observation is that estimation and decision problems are structurally different and, therefore, algorithms that have proven
successful for the former need not perform well when adjusted for decision problems. We propose a new scheme, referred to
as BLLR (barrier log-likelihood ratio algorithm) and demonstrate its applicability to real-data from the COVID-19 pandemic in
Italy. The results illustrate the ability of the design tool to track the different phases of the outbreak.
Index Terms
Learning and adaptation, LMS algorithm, decision systems, COVID-19 pandemic.

I. I NTRODUCTION
Performing inference by a network of interconnected agents is the primary goal in many practical applications, as seen in [1]–
[12]. In these works, the desired inference is typically obtained at a central unit that processes the data received from remote
agents. Substantial improvements in system robustness, reliability and scalability can be obtained with fully-flat architectures
without a central unit at the cost of more capable agents. These agents will now be required to obtain local inference solutions
in a fully distributed manner by exploiting data exchanges among neighboring agents.
In fully-flat architectures the signal-processing mechanism can be based on consensus strategies [13]–[19] or diffusion
strategies [20]–[32]. Among the latter class, the most successful strategy is the ATC (adapt-then-combine) fusion rule, which
consists of two steps. First, in the adaptation step, after collecting a new observation about the phenomenon of interest, the
agent updates its state (inference statistic) by incorporating the information provided by the fresh measurement. Then, the
updated state is combined with those of nearby agents and progressively diffused throughout the network. The adaptation step
is based on the popular LMS (least-mean-square) algorithm, see, e.g., [33], while the second step consists of computing a
convex combination of the states of neighboring agents.
Historically, in learning and adaptation contexts, estimation problems were considered first, which led to the adoption of the
LMS algorithm for the adaptation step, because of its well-known adaptation properties [21], [22], [33]. When dealing with
decision problems, it appears natural to maintain the LMS protocol as the basic engine due to its simplicity in order to track
drifts in the state of nature. For this reason, most works in the literature addressing distributed decision problems exploit the
ATC diffusion strategy, in the form of LMS iterates followed by a convex combination of states, as originally designed for
estimation problems [27]–[31].

A. Contribution and Scope
In this paper we design alternative decision-making algorithms that are specifically tailored to decision problems, by building
up on first principles from decision theory rather than relying directly on the LMS update. We introduce performance indexes
that quantify the tradeoff between learning and adaptation. Using these performance figures, we show that the proposed agents’
updating rule outperforms the one based on the LMS iteration. In this contribution, no network aspects are considered and the
focus is on the operation of a single agent. In addition, we limit our study to the case of two possible states of nature, say H0
and H1 , which are known to the decision maker. This means that at any time epoch, the observations collected by the agent
are independently drawn from one of two distributions, but we do not know which, and this underlying distribution is allowed
to change at any time according to arbitrary patterns of the kind . . . H0 7→ H1 7→ H0 7→ H1 . . . . Extensions to more than two
states of nature are left for future studies.
We illustrate that the learning and adaptation technique developed in this paper is useful in tracking different phases of the
COVID-19 pandemic. As an example, the proposed tool is demonstrated on pandemic data from Italy.
S. Marano is with DIEM, University of Salerno, via Giovanni Paolo II 132, I-84084, Fisciano (SA), Italy (e-mail: marano@unisa.it). A. H. Sayed is with
the Ecole Polytechnique Federale de Lausanne EPFL, School of Engineering, CH-1015 Lausanne, Switzerland (e-mail: ali.sayed@epfl.ch).

2

B. Notation
Boldface symbols denote random variables and normal font their realizations and deterministic quantities. For scalars, the
time index (or algorithm iteration number) is enclosed in parentheses. Thus, for instance, x(n) denotes the random scalar x
at time n. Conversely, in the case of vectors, the time dependence is indicated by a subscript, as, for example, ui denotes a
random vector u evaluated at time i. Superscript T denotes vector transposition. Statistical expectation, variance, and probability
operators are denoted by E, V, and P, respectively. They always are computed under the hypothesis in force Hh , and the
pertinent subscript h = 0, 1 is usually added to the operator symbol.
The remaining part of this article is organized as follows. Section II discusses the genesis of LMS in decision contexts.
The proposed alternative to LMS is presented in Sec. III and its performance is investigated in Sec. V in terms of the criteria
discussed in Sec. IV. Examples using synthetic data are given in Sec. VI while an application to COVID-19 pandemic time-series
is discussed in Sec. VII. Section VIII contains conclusive remarks.
II. G ENESIS

OF THE

LMS A LGORITHM

IN

D ECISION C ONTEXTS

Let us start by considering an estimation problem. Let d ∈ ℜ be a zero-mean scalar random variable with variance Ed2 > 0,
and u ∈ ℜM a zero-mean random vector with positive-definite covariance matrix EuuT > 0. The quantity d is unknown
while u is observed. The goal is to solve the optimization problem minw J(w), where w ∈ ℜM is a weight vector and
J(w) : ℜM 7→ ℜ represents a cost function that quantifies the penalty incurred when the unknown d is replaced by the
linear transformation uT w of the observation. One common choice is the quadratic cost function J(w) = E(d − uT w)2 , in
which case the solution wo is given by wo = (EuuT )−1 Edu, and the linear least-mean-square estimator of d given u is
b = uT wo [33, Th. 8.1, p. 142].
d
A recursive solution to the optimization problem minw J(w) with quadratic cost function is provided by the steepest-descent
algorithm: set w0 equal to some initialization vector, and iterate as follows:


(1)
wi = wi−1 + µ Edu − EuuT wi−1 , i = 1, 2, . . . ,

where the step-size µ > 0 is sufficiently small (less than 2 divided by the largest eigenvalue of matrix EuuT ), see [33, Th. 8.2,
p. 147]. It can be shown that Edu− EuuT wi−1 = −∇J(wi−1 ), which makes it possible to rewrite (1) in terms of the gradient
vector ∇J(wi−1 ). The resulting expression is useful when alternative cost functions are used.
What is especially relevant in the adaptive framework is the consideration that the quantities EuT u and Edu may not be
known beforehand and are expected to vary over time. In these situations, assuming that we have access to streaming data in
the form of a sequence of realizations {d(i), ui }i≥1 of d and u, a viable alternative to (1) is obtained if we drop the expectation
signs and replace the random variables by their current realizations, yielding the following algorithm: set w0 = some initial
guess,


(2)
wi = wi−1 + µui d(i) − uTi wi−1 , i = 1, 2, . . . ,

with a sufficiently small µ. This stochastic gradient approximation (because the true gradient is replaced by a noisy version
thereof) is known as the LMS algorithm, see [33, Th. 10.1, p. 166]. The LMS algorithm learns the data statistics and at the
same time is able to track statistical drifts, which are essential characteristics for the design of cognitive intelligent inference
systems with learning and adaptation properties.
We now move from an estimation to a decision context, paralleling the way in which this happened in the literature. Suppose
M = 1, namely wi = w(i) and ui = u(i) are scalars, and suppose also u(i) = 1 for all i. By assuming independent and
identically distributed (IID) data {d(i)}i≥1 , formal substitution in (2) gives: w(0) = 0,
w(i) = w(i − 1) + µ[d(i) − w(i − 1)],

i ≥ 1,

(3)

Note that the right-hand side of (3) is a convex combination: µd(i) + (1 − µ)w(i − 1). Iterating (3), we get the output of the
LMS algorithm in the form:
w(i) =

i−1
X

µ(1 − µ)k d(i − k),

(4)

k=0

and we have
Ew(i) = [1 − (1 − µ)i ] Ed,
µ
Vd.
Vw(i) = [1 − (1 − µ)2i ]
2−µ

(5a)
(5b)

From (5), we see that the output of the algorithm approximates Ed when the number i of iterations is sufficiently large and
the step-size µ is ≪ 1. This property, along with the inherent adaptation ability, motivated the use of (3) in decision problems.
Indeed, the algorithm formalized in (3) represents the basic building block for the development of adaptation and learning
diffusion algorithms over networks faced with decision problems, which has been addressed in a series of papers [27]–[31].

3

A. Alternative Derivation
Since the main motivation for this paper is to explore alternatives to the LMS block, it is important to consider alternative
contexts in which LMS arises and analyze the related motivation for its usage. Two contexts in which the LMS is used are now
briefly discussed. The first is a minimax formulation of an estimation problem, and the second is linked to decision procedures.
An algorithm similar to that shown in (2) emerges in the following scenario, see [33, p. 731]. Suppose that we observe
{d(i), ui }i≥1 , modeled as
= s(i)+v(i), with s(i) , uTi wo , for some sequence of vectors {ui }i≥1 satisfying a “persistence
Pd(i)
∞
of excitation” condition i=1 ui uTi = ∞, and some unknown wo ∈ ℜM×1 . Here v(i) ∈ ℜ is a “noise” term with finite energy
P
∞
2
b(i|i − 1) denote a strictly causal estimator of s(i) based on the data {d(k)}i−1
i=1 v (i) < ∞. Let s
k=0 . Then, the LMS
algorithm is the optimal solution to the min-max problem [34]:
P∞
s(i|i − 1) − s(i)]2
i=1 [b
P∞ 2 ,
(6)
inf
sup
{b
s(i|i−1)} {v(i)},w o µ−1 kwo k2 +
i=1 v (i)

and, moreover, the fraction in (6) takes value 1 at optimality. In particular, let w0 = 0 and, for 1 ≤ i < n, consider the iteration
sb(i|i − 1) = uTi wi−1 ,

(7a)

wi = wi−1 + µui [d(i) −

uTi wi−1 ].

It can be shown that this version of the LMS algorithm satisfies the following (robustness) condition for every i < n:
Pi
s(k|k − 1) − s(k)]2
k=1 [b
< 1,
P
2
µ−1 kwo k2 + i−1
k=1 v (k)

(7b)

(8)

n−1
if, and only if, all the matrices {µ−1 I − uTi ui }i=0
are positive-definite [33, Alg. 45.4, p. 731].
It is also useful to mention that the LMS has been advocated in decision problems and, specifically, in the context of
continuous inspection schemes and related control charts. As seen in (4), LMS employs exponentially-scaled weights, which
is exactly the idea behind the geometric moving average control charts, see [35, Sec. 2.1.2, p. 28] and [36, Sec. 8.1.2, p. 373].
In these contexts, LMS is known under the name of GMA (Geometric Moving Average) or EWMA (Exponentially Weighted
Moving Average). We refer to [35], [36] for details.

III. P ROPOSED A LGORITHM : BARRIER LLR
As discussed in the previous section, in the literature of adaptation and learning, decision problems have been approached
by exploiting schemes and protocols initially conceived for estimation problems. Since decision and estimation problems are
structurally different in many respects, it makes sense to start anew, with the goal of exploring possible alternatives to the
LMS component with better performance for decision tasks. The idea is to modify a classical decision algorithm in order to
make it more suitable to adaptation contexts. Let us consider a standard binary decision problem in which IID data {x(i)}i≥1
are observed and the following binary hypothesis test must be solved:
H1 : x(i) ∼ f1 (x),
H0 : x(i) ∼ f0 (x),

i = 1, 2, . . .
i = 1, 2, . . .

(9)

where f1,0 (x) are the probability density functions (PDFs) of the data under the two hypotheses H1 and H0 , respectively.
These PDFs are assumed to exist and are known to the decision maker. It is well-known that under the most popular optimality
criteria, the optimal decision maker exploits the log-likelihood of the data [37], which is
d(i) = log

f1 (x(i))
.
f0 (x(i))

(10)

Exploiting the IID property of the observations, the optimal decision based on vector [x(1), . . . , x(n)]T is
z(n) =

n
X

d(i) =

i=1

n
X
i=1

log

H
f1 (x(i)) >1
γ,
f0 (x(i)) <
H0

(11)

where γ is a suitable threshold, chosen according to the desired optimality criterion [37]. Expression (11) can be regarded as
a random walk: z(0) = 0,
z(i) = z(i − 1) + d(i),

i ≥ 1,

(12)

with step d(i). The following relationships are well known [the argument “(i)” is suppressed when non-essential]:
E1 [d] = D10 > 0,

E0 [d] = −D01 < 0

(13)

4

Algorithm 1: BLLR
max
Input: input sequence {d(n)}n
n=1 ; initialization z(0); barriers a, b
max
Output: decision statistic {z(n)}n
n=1

n = 0;
while n < nmax
n=n+1
z(n) = inf{b, sup{−a, z(n − 1) + d(n)}}
end

Algorithm 2: LMS
max
Input: input sequence {d(n)}n
n=1 ; initialization w(0); step-size µ
max
Output: decision statistic {w(n)}n
n=1

n = 0;
while n < nmax
n=n+1
w(n) = µd(n) + (1 − µ)w(n − 1)
end

R
R
where D10 = f1 (x) log[f1 (x)/f0 (x)] dx and D01 = f0 (x) log[f0 (x)/f1 (x)] dx denote the two Kullback-Leibler (KL)
distances (or divergences) between the PDFs f1 (x) and f0 (x) [38]. In (13) we have assumed that that these KL distances exist
and are strictly positive, which implies that f1 (x) and f0 (x) are distinct over a set of nonzero probability.
A SSUMPTIONS . The following assumptions are used throughout the paper. Under Hh , h = 0, 1, the random variables
{d(i)}i≥1 are continuous, with finite first- and second-order moments, and their probability distribution admits a density with
respect to the usual Lebesgue measure. In addition, E1 d > 0, E0 d < 0, Ph (d > 0) > 0 and Ph (d < 0) > 0, h = 0, 1.

In (11), we see that the optimal decision maker compares to a threshold the value of a random walk with positive drift under
H1 and negative drift under H0 . This optimal learning scheme is not adaptive, as is easily revealed by the following informal
arguments. Suppose that the state of nature is H1 for time steps 1 ≤ i ≤ n, and then switches to H0 . Observations are IID
under each hypothesis, but their distribution is different under the two hypotheses. Assuming n ≫ 1, with high probability
the decision statistic z(n) takes on very large values because the random walk is drifting to +∞ for 1 ≤ i ≤ n. For i > n
the drift is negative but the random walk “starts” at z(n), implying that the time required to approach the negative values that
are typical of hypothesis H0 is very large. A straightforward way to prevent {z(i)}i≥0 from reaching extreme values is to
introduce two barriers −a < 0 < b, as follows: z(0) = 0 and for i ≥ 1:


z(i − 1) + d(i) ≤ −a,
−a,
z(i) = z(i − 1) + d(i), −a < z(i − 1) + d(i) < b,
(14)


b,
z(i − 1) + d(i) ≥ b.
A more compact expression for the iteration in (14) is: z(0) = 0 and

z(i) = inf b, sup{−a, z(i − 1) + d(i)} ,

i ≥ 1.

(15)

The lower and upper barriers limit the range of values that |z(i)| takes on, and hence we can tradeoff adaptation and learning
by a careful choice of a and b. Large values favor the learning (decision) ability of the system, while small values favor its
adaptation ability. In the following, the decision procedure based on comparing (14) to a threshold γ ∈ (−a, b) will be referred
to as the barrier log-likelihood ratio (BLLR) test: the BLLR decision at any arbitrary time epoch n is
H1
z(n) >
< γ.
H0

(16)

The BLLR decision procedure (16), with z(n) shown in (14) or (15), represents the proposed alternative. For easy reference,
the BLLR and LMS procedures are summarized in Algorithms 1 and 2.
Typical choices for the threshold appearing in (16) are: γ = 0, which, in the unbounded case of a, b → ∞ corresponds to
the maximum likelihood (ML) decision criterion adopted, among other cases, in the Bayesian framework in which the two
hypotheses are equally likely; the mid-point threshold γ = b−a
2 ; or the value of γ for which the error probability of deciding
H1 under state of nature H0 takes on a prescribed value (false alarm criterion), as in the Neyman-Pearson formulation [37].
Likewise, when considering the LMS iterate (3), a test similar to (16) is used and the threshold is chosen with the same criteria.
For LMS, the mid-point threshold is γ = (D10 − D01 )/2.

5

Suppose we know that H0 is in force for 1 ≤ i ≤ n and H1 is in force for all i > n, with IID data under each hypothesis,
and the change epoch n is unknown. The celebrated Page’s test for quickest detection of a change in the state of nature is
obtained from (14) by setting a = 0 and letting b be the decision threshold: once the decision statistic hits the value b, the
change in the state of nature H0 7→ H1 is declared and the test stops [35]. This reveals that BLLR test (14) is a generalization
of Page’s test. Indeed, the BLLR test in (14) can be seen as an infinite sequence of Page’s tests for successively detecting the
changes H0 7→ H1 7→ H0 7→ H1 . . . . To see this, let us assume that H0 is true and set z(0) = −a. The BLLR test is equivalent
to a Page’s test with threshold a + b for detecting the change H0 7→ H1 , followed by a sign-reversed Page’s test initialized at
b, driven by negative drifts, with threshold a + b, to detect the successive change H1 7→ H0 , and so forth indefinitely.1 In turn,
Page’s test can be regarded as a sequence of Wald’s SPRTs (sequential probability ratio tests) [39], [40], which reveals that the
decision algorithm shown in (14) and (16) is a modified version of a sequence of SPRTs. Not surprisingly, the performance
analysis of BLLR relies on standard results of sequential analysis, some results of which are collected in Appendices A-C, for
self-consistency.
In view of the analogy with sequential analysis, our approach is close in spirit to the SPRT approach pursued in [41] for
cooperative sensing. As done in [41], in Sec. VII we resort to the GLRT (generalized likelihood ratio test) approach to deal with
the presence of unknown parameters. However, the nature of these parameters and the corresponding estimates are structurally
different from those in [41], resulting in substantially different decision procedures.
IV. P ERFORMANCE A SSESSMENT
A. Performance Criteria
1) Performance for BLLR Test: We introduce two performance indexes: the error rate r, related to the learning capability,
and the expected delay ∆ that quantifies the adaptation capability.
Let us consider the learning aspects first. Perhaps, the most natural performance figures would be the probability that
limi→∞ z(i) exceeds γ under H0 , and the probability that limi→∞ z(i) goes below γ under H1 . In general, these steady-state
probabilities are guaranteed to exist [42], [43], however they are not easy to compute and do not lead to simple closed-form
expressions from which insights can be easily gained. We instead introduce performance figures whose computation is tractable.
For h = 0, 1, consider the following quantity, defined with the state of nature Hh held fixed:2
Th (z0 ; z1 ) = Eh inf {i : z(i) R z1 ; with z(0) = z0 },
i≥1

(17)

wherein the sign ≥ applies if z0 < z1 , and ≤ applies if z0 > z1 . The quantity in (17) represents the expected time to reach
the value z1 starting from z0 , under hypothesis Hh .
Using (17), the learning ability of the system is quantified by the two indexes
T0 (−a; γ)

and

T1 (b; γ).

(18)

The interpretation is as follows. The quantity T0 (−a; γ) represents the expected time to cross the threshold γ, yielding a decision
in favor of H1 , in the H0 steady-state situation, when the BLLR iteration is initialized to z(0) = −a, which we call the “typical”
value taken by the statistic under H0 . Likewise, T1 (b; γ) represents the expected time to cross the threshold yielding the H0
decision, in the H1 steady-state situation, when the BLLR iteration is initialized at the “typical” value under H1 , which is
z(0) = b. Note that these quantities are related to — but different from — the expected time between false alarms and miss

detections, respectively. The expected error time is defined in terms of the quantities in (18), by Terr = 12 T0 (−a; γ)+T1(b; γ) ,
and the error index quantifying the learning ability is its inverse, which we call the rate:
r=

2
1
=
.
Terr
T0 (−a; γ) + T1 (b; γ)

(19)

The second performance index ∆ quantifies the adaptation ability and is again defined in terms of Th (z0 ; z1 ) shown in (17).
Specifically, we consider:
T1 (−a; b)

and

T0 (b; −a).

(20)

For the decision statistic z(n) initialized at z(0) = −a, T1 (−a; b) represents the expected time needed to hit for the first time
the barrier b, under a steady-state state of nature H1 . Likewise, T0 (b; −a) is the expected time for the decision statistic z(n),
taking value b at epoch 0, to hit for the first time the barrier −a, with fixed state of nature H0 . The expected delay ∆ is
defined as the arithmetic mean
i
1h
(21)
∆ = T1 (−a; b) + T0 (b; −a) .
2
1 We
2 In

are making the simplifying assumption that the hits at the thresholds are really due to change in the state of nature, not to error events.
the following we also use the qualification “steady-state” state of nature to signify that the state of nature is assumed forever constant.

6

In the previous discussion, the “typical” value of the statistic under H0 is −a, and the “typical” value under H1 is b. These
choices are natural because −a and b are barriers. With these choices, as we shall see soon, we obtain simple closed-form
expressions for the operational characteristic (r, ∆) of the decision-maker.
However, when comparing the performance of BLLR with that of the LMS test, sensible performance indexes for the BLLR
decision-maker are obtained by replacing in (18)-(21) the “typical” values of the decision statistic under the two hypotheses,
by the corresponding expected values:
−a 7→ E0 [z(∞)]

and

b 7→ E1 [z(∞)],

(22)

wherein we define z(∞) = limi→∞ z(i). The distribution of z(∞) is investigated, e.g., in [42].
2) Performance Criteria for LMS Test: The performance indexes of the LMS test are defined in a way similar to that of
BLLR, with the notable difference that, in absence of barriers, one cannot define the typical values of the statistic under the
two hypotheses as done before, and we instead rely upon expected values. To elaborate, assuming a steady-state hypothesis
Hh , let us introduce the quantity:
Th′ (w0 ; w1 ) = Eh inf {i : w(i) R w1 ; with w(0) = w0 },
i≥1

(23)

wherein the sign ≥ applies if w0 < w1 , and ≤ applies if w0 > w1 . Recall that {w(n)}n≥0 is defined in (3) and note the
superscript ′ to distinguish quantities related to the LMS test from the corresponding quantities referring to BLLR.
As error performance indexes for LMS we consider the quantities T0′ (−D01 ; γ) and T1′ (D10 ; γ). The rationale is obvious. For
h = 0, 1, when the state of nature is Hh and assuming that the iteration starts from Eh [w(∞)], we compute the expected time
required to cross the threshold and therefore decide for the opposite hypothesis H1−h . Using T0′ (−D01 ; γ) and T1′ (D10 ; γ),
′
′
we define the expected error time Terr
as the arithmetic mean of these two quantities, and the error rate as the inverse of Terr
:
r′ =

1
2
= ′
.
′
Terr
T0 (−D01 ; γ) + T1′ (D10 ; γ)

Likewise, introducing T1′ (−D01 ; D10 ) and T0′ (D10 ; −D01 ), we define the expected delay as
i
1h
∆′ = T1′ (−D01 ; D10 ) + T0′ (D10 ; −D01 ) .
2

(24)

(25)

B. Average Run Length for Page’s Test
The performance of the BLLR test can be computed by borrowing results from the analysis of Page’s test. To show this, it
is convenient to introduce a version {zP (i)}i≥0 of the iteration (15) with arbitrary starting point c and a single lower barrier
at 0. This is exactly the celebrated Page’s test for quickest detection [44]: zP (0) = c ≥ 0, and
zP (i) = sup{0, zP (i − 1) + d(i)},

i ≥ 1.

(26)

For γP > 0, let us define the average run length (ARL):
Lh (c; γP ) = Eh inf {i : zP (i) ≥ γP ; with zP (0) = c},
i≥1

(27)

computed under steady-state hypothesis Hh , h = 0, 1.
In Appendix A, an exact expression for Lh (c; γP ) is derived, involving integral equations. Since not much physical insight
is gained from these integral representations, we opt for relying on approximate, but simpler and closed-form, performance
formulas for Lh (0; γP ). These formulas are derived in Appendix B, exploiting standard Wald’s approximations [39], [40]. The
final result is [35, Eq. 5.2.44]:
e γ P − γP − 1
,
D01
γP + e−γP − 1
L1 (0; γP ) ≈
.
D10
L0 (0; γP ) ≈

(28a)
(28b)

For large γP , expressions (28) simplify to:
L0 (0; γP ) ≈

e γP
,
D01

L1 (0; γP ) ≈

γP
.
D10

(29)

For Page’s test, L0 (0; γP ) represents the mean time between false alarms and L1 (0; γP ) the worst mean delay for detection [35, Eqs. 5.2.18, 5.2.19]. Note the role played by the KL divergences D10 and D01 . The larger D10 and D01 are, the
smaller L1 (0; γP ) and L0 (0; γP ) become, respectively. The former has a positive impact on the performance, the latter has
a negative impact. Differently from the classical hypothesis testing problem where an increase of either or both D10 and
D01 yields enhanced performance, in quickest detection problem enhanced performance is obtained by increasing D10 and/or
1/D01 , as seen in (28).

7

V. T EST P ERFORMANCE
A. BLLR Test
It is easy to express the performance indexes introduced in Sec. IV-A1 in terms of the ARL Lh (0; γP ) shown in (28).
Consider first the case in which the random walk {z(i)}i≥0 starts at z(0) = −a. For the quantities on the left of (18) and (20)
we have the obvious equalities:
eγ+a − (γ + a) − 1
,
D01
(b + a) + e−(b+a) − 1
.
T1 (−a; b) = L1 (0; b + a) =
D10

T0 (−a; γ) = L0 (0; γ + a) =

(30)
(31)

When the random walk {z(i)}i≥0 starts at z(0) = b, we consider the reversed process {z− (i)}i≥0 obtained by replacing
in (14) the sequence of log-likelihoods {d(i)}i≥1 with its sign-reversed counterpart {−d(i)}i≥1 . Then, for the quantity T1 (b; γ)
appearing on the right of (18), we have
T1 (b; γ) = E1 inf {i : z(i) ≤ γ; with z(0) = b},
i≥1

= E1 inf {i : z− (i) ≥ b − γ; with z− (0) = 0}
=
=

i≥1
−
L1 (0; b
b−γ

e

− γ)

(32a)
(32b)

− (b − γ) − 1
,
D10

(32c)

where (32a) follows by symmetry; the minus sign “− ” appended to the ARL in (32b) refers to a “reversed” Page’s test in which
the sequence {d(i)}i≥1 appearing in (26) is replaced by {−d(i)}i≥1 ; and (32c) follows by noting that the ARL L−
1 (0; γP ) is
the same of the ARL for a standard Page’s test evolving under H0 with steps whose expectation is D10 . Similar arguments
lead to
T0 (b; −a) = L−
0 (0; b + a) =

(b + a) + e−(b+a) − 1
.
D01

(33)

In the case that the threshold is at the midpoint between the barriers, γ = b−a
2 , the performance figures in (30)-(33) can be
expressed in terms of the range R , (b + a) of the detained random walk. Assuming γ = b−a
2 , recalling the definitions of
expected error time and expected delay in (19) and (21), we get
eR/2 − R/2 − 1
,
Deff
R + e−R − 1
,
∆=
Deff

Terr =

(34)
(35)

where we have defined the effective divergence between the hypotheses as
Deff , 2

D01 D10
.
D01 + D10

(36)

r=

Deff
.
− R/2 − 1

(37)

The inverse of Terr in (34) is the error rate:
eR/2

For R ≫ 1, we obtain
r ≈ Deff e−R/2 ,

∆≈

R
,
Deff

(38)

yielding a simple insightful expression for the operational characteristic (r, ∆) of the BLLR test:
r(∆) = Deff e−

Deff ∆
2

.

(39)

The function r(∆) is strictly decreasing and convex and quantifies the fundamental trade-off of the decision procedure. It is
also worth noting that, for a fixed ∆, r grows with Deff as long as Deff < ∆/2, while it is a decreasing function of Deff for
Deff > ∆/2. This behavior is to be interpreted in light of the comments reported at the end of Sec. IV-B.

8

Gaussian example

100
10

-1

10

-2

10

-3

10

-4

10

-5

10

-6

10

-7

0

50

100

150

200

250

300

Fig. 1. Gaussian example of Sec. VI-A. Operational characteristic (error rate r versus expected delay ∆) for the BLLR and LMS decision procedures. “BLLR
simulation” shows the results of computer experiments involving 103 Monte Carlo runs. “BLLR theory” refers to the theoretical formulas (35) and (37),
while “BLLR theory (large R)” shows the large-R approximation (39). The curve in gray labelled as “BLLR theory (correction)” refers to expressions (19)
and (21) wherein a and b are replaced by the expected values as shown in (22), for a fairer comparison with the LMS scheme. “LMS simulation” shows the
results of computer experiments involving 103 Monte Carlo runs, while the curve labelled by “LMS numerical” is obtained by solving numerically (40) as
detailed in Appendix C.

B. LMS Test
Simple closed-form approximations for the test performance, similar to those shown in (30)-(33), are not available in the
case of the LMS iteration (3). The technical difference is that {w(i)}i≥0 is not a random walk and the stopped martingale
approach illustrated in Appendix B does not apply. However, the performance of LMS can be expressed by Fredholm integral
equations similar to those in (A.3).
To show this, we follow the approach of [45] with reference to the error figure T0′ (−D01 ; γ) defined in Sec. IV-A2, see (23).
The hypothesis in force is H0 , the iteration {w(i)}i≥0 is initialized to w(0) = −D01 , and the event of crossing the threshold γ
is considered. At the first step of the iteration, two mutually exclusive events can occur. Either d(1) causes a threshold crossing,
i.e., w(1) = µd(1) + (1 − µ)w(0) > γ, an event whose probability we denote by p, or w(1) = µd(1) + (1 − µ)w(0) ≤ γ.
In the latter case the iteration restarts from w(1) and the additional expected run length is given by ET0′ (w(1); γ), where the
. Note that all distributions are computed under
expectation involves the distribution of d(1) conditioned to d(1) ≤ γ−(1−µ)w(0)
µ
H0 , even if not explicitly indicated. Let fd|cnd(ξ) denote such conditional distribution, which is related to its unconditional
d (ξ)
counterpart fd (ξ) by fd|cnd(ξ) = f1−p
for ξ ≤ γ−(1−µ)w(0)
, and fd|cnd(ξ) = 0 otherwise. We obtain
µ

T0′ (w(0); γ) = p + (1 − p)
Z ∞ h
i
1 + T0′ µξ + (1 − µ)w(0); γ fd|cnd(ξ) dξ
×
=p+

Z

−∞

−∞

=1+

1
µ

h

γ−(1−µ)w(0)
µ

Z

γ

−∞

1 + T0′ µξ + (1 − µ)w(0); γ

T0′ (ξ; γ)fd

 ξ − (1 − µ)w(0) 
µ

dξ.

i

fd (ξ) dξ
(40)

The average run length T0′ (w(0); γ) needed for the iteration {w(i)}i≥0 with initial value w(0) to exceed the threshold γ
can be computed by solving numerically (40). The numerical solution to (40) used in the examples discussed in Sec. VI is
motivated by the arguments provided in Appendix C. The quantity T1′ (−D01 ; D10 ) can be computed similarly to T0′ (−D01 ; γ),
while T1′ (D10 ; γ) and T0′ (D10 ; −D01 ) require to consider the reversed random walk process whose steps are {−d(i)}i≥1 . The
details are omitted.

9

Gamma example

10-1

10

-2

10

-3

10

-4

0

50

100

150

200

250

300

Fig. 2. Example with Gamma distributions, see Sec. VI-A. The operational characteristic r versus ∆ for the BLLR and LMS decision procedures are shown.
See the caption to Fig. 1 for details.

VI. E XAMPLES
A. Gaussian Shift-in-Mean
Consider the following hypotheses with IID observations {x(i)}i≥1 : for i = 1, 2, . . . ,
2

H1 : x(i) ∼ f1 (x) =
H0 : x(i) ∼ f0 (x) =

(x−m)
√1 e− 2σ2
σ 2π
x2
√1 e− 2σ2 .
σ 2π

,

(41)

It is easily seen that the log-likelihood is
d(i) = log

m x(i)
f1 (x(i))
− D,
=
f0 (x(i))
σ2

(42)

2

m
2
where D = D10 = D01 = 2σ
2 . The PDF of d is N (−D, 2D) under H0 and N (D, 2D) under H1 , where N (m, σ ) denotes
2
2
a Gaussian distribution with mean m and variance σ . In the present experiment we assume σ = 1, m = 1/2 and γ = 0,
which is also the midpoint threshold because D10 = D01 . For the LMS test, different values of the step-size in the range from
from 8.5 10−3 to 0.3 are considered. In the case of the BLLR test, with little loss of generality, we set the barriers as follows:

a = b = µ−1 D.

(43)

In this way, we use a single parameter µ for both LMS and BLLR decision algorithms, with the meaning that smaller values
of µ imply slower adaptation. Special attention is devoted to the slow-adaptation regime µ ≪ 1.
Figure 1 shows the results of computer simulations for both decision schemes. For BLLR we also show the theoretical
performance shown in (35) and (37) (theory), as well as the large-R expression given in (39) [theory (large R)]. For the
LMS decision scheme we also show the performance obtained by solving numerically (40) as detailed in Appendix C (LMS
numerical). The figure confirms the accuracy of the theoretical formulas for performance prediction.
The superiority of the BLLR decision algorithm is evident, at least in the regime of small adaptation (large values of ∆).
However, recall from the discussion in Sec. IV-A1 that a fair comparison between the two decision schemes requires to modify
the performance indexes of the BLLR as indicated in (22). The expectations shown in (22) have been computed numerically and
the resulting operational characteristic is shown by the gray curve in Fig. 1, labelled as “theory (correction)”. The substantial
superiority of BLLR in the low-adaptation regime is confirmed: for large ∆, we see that the rate r scales exponentially with
the delay ∆ for both the decision schemes, but the exponent is substantially larger for the BLLR decision algorithm.
B. Example with Gamma Distributions

R∞
Recall the definition of the Gamma function: Γ(α) = 0 ξ α−1 e−ξ dξ, with α > 0. Let κ, θ > 0. With slight abuse of
notation we use the symbol x ∼ Γ(κ, θ) to signify that x is a Gamma-distributed random variable whose PDF is
fΓ (x) =

1
xκ−1 e−x/θ ,
Γ(κ)θκ

x > 0.

(44)

10

Exponential example

100
10-1
10-2
10-3
10-4
10-5
10-6
10-7

0

50

100

150

200

250

300

Fig. 3. Example with exponential distributions, see Sec. VI-C. The operational characteristic r versus ∆ for the BLLR and LMS decision procedures are
shown. See the caption to Fig. 1 for details.

For x ∼ Γ(κ, θ), it follows by straightforward algebra that y = log x ∼ LΓ(κ, θ), which is called log-Gamma distribution,
having the following PDF:
fLΓ (y) =

y
1
κy − eθ
,
e
e
Γ(κ)θκ

y > 0.

(45)

d
log Γ(x) is known as digamma function. We now consider
For y ∼ LΓ(κ, θ), we have Ey = log θ + ψ(κ), where ψ(κ) = dx
the following hypotheses with IID observations {x(i)}i≥1 and ρ > 0:

H1 : x(i) ∼ Γ(κ + ρ, θ),
H0 : x(i) ∼ Γ(κ, θ).

(46)

Γ(κ+ρ)
Simple algebra shows that the corresponding log-likelihood is d(i) = ρ log x(i)
θ − log Γ(κ) , and therefore, with obvious
notation:

H1 : d(i) ∼ ρ LΓ(κ + ρ, 1) − log Γ(κ+ρ)
Γ(κ) ,

(47)

H0 : d(i) ∼ ρ LΓ(κ, 1) − log Γ(κ+ρ)
Γ(κ) ,
yielding

Γ(κ + ρ)
,
Γ(κ)
Γ(κ + ρ)
= −ρ ψ(κ) + log
.
Γ(κ)

D10 = ρ ψ(κ + ρ) − log

(48a)

D01

(48b)

In this experiment the barriers for the BLLR decision scheme are
a = µ−1 D01 ,

b = µ−1 D10 ,

(49)

where µ is the step-size of the LMS algorithm, and the threshold is γ = b−a
2 . We assume θ = 1, κ = 10 and ρ = 1.
The results of computer experiments for the BLLR and the LMS decision algorithms are shown in Fig. 2. The comments
are similar to those of the Gaussian example. In a nutshell: the formulas for performance prediction are very accurate and the
BLLR algorithm outperforms LMS, at least in the small-adaptation regime of large delays.
C. Exponentially Distributed Observations
Our last example involves exponentially distributed observations: x ∼ E(η) with PDF fE (x) =
The two hypotheses are
H1 : x(i) ∼ E(η1 ),
H0 : x(i) ∼ E(η0 ),

e−x/η
η ,

for x > 0 and η > 0.

(50)

11

with η1 > η0 > 0. The corresponding log-likelihood is d(i) = (η0−1 − η1−1 )x(i) − log ηη01 . Defining ηe =
of the likelihood d one gets
H1 : fd (z) =
H0 : fd (z) =

ηe
− z+log
1
ηe −1 ,
ηe −1 e
ηe
ηe
−ηe z+log
ηe −1
ηe −1 e

z > − log ηe ,
, z > − log ηe ,

η1
η0

> 1, for the PDFs

(51)

and D10 = ηe − 1 − log ηe and D01 = ηe−1 − 1 + log ηe . In this experiment we assume η0 = 1 and η1 = 1.5. As for the
Gamma example, the barriers for the BLLR decision scheme are a = µ−1 D01 , and b = µ−1 D10 , where µ is the step-size of
the LMS algorithm, and we use the mid-point threshold γ = b−a
2 .
The results of computer simulations compared to the theoretical formulas are shown in Fig. 3. The comments are similar
to the previous case, but in the exponential case the theoretical formulas are less accurate. The slope of the operational curve
r(∆) seems correctly predicted by the analytical formulas, but a multiplicative correction appears to be necessary. This is a
manifestation of the poor accuracy of Wald’s approximations of neglecting the excess over the boundaries, which have been
exploited to derive the theoretical formulas. Improvements in this regard are possible, e.g., via nonlinear renewal theory [36,
Sec. 2.6], [46], but not pursued here. In addition, in the exponential case, the theoretical operational characteristic of the LMS
decision scheme is not reported because of instability of the numerical procedure detailed in Appendix C to solve (40).
VII. A N A PPLICATION R ELATED TO COVID-19 PANDEMIC
During the course of a pandemic, one of the most challenging tasks for authorities is to decide when to impose or relax
restrictive measures with huge societal and economic costs, such as: closure of schools, universities, shops, factories, limitation
of social activities, strict lockdown. In this respect, learning and adaptation algorithms can be useful to support informed and
rational decision making. In this section, we discuss a variation of the BLLR test, which is particularly relevant in connection
to the analysis of COVID-19 pandemic time-series.
Let us start by considering the classical SIR model of pandemic evolution introduced by Kermack and McKendrick in
1927 [47]. Let3 s(t), i(t) and r(t) denote the fractions of susceptible, infected, and recovered (or dead) individuals, respectively.
Let β be the infection rate (infected individuals per unit time), and γ the recovering rate. The celebrated SIR equations are [48]:
 ds(t)

= −β s(t)i(t),
 dt
di(t)
(52)
= β s(t)i(t) − γ i(t),
dt

 dr(t)
= γ i(t),
dt
with the initial conditions r(0) = 0, s(0) = 1 − i(0). We assume 0 < i(0) ≪ 1, where i(0) represents the small fraction of
the total population from which the infection originates. Let us focus on the situation in which the pandemic is mostly under
control, because of restrictions imposed by the authorities such as social distancing, but at the same time is not eradicated.
Then, it is reasonable to assume that the fraction of susceptible individuals is maintained almost constant s(t) ≈ s∗ , implying
that the second equation in (52) reduces to
di(t)
= (βs∗ − γ) i(t).
(53)
dt
Since data about the infections are typically collected on a daily basis, consider a discrete-time version of (53) with unit-step
discretization (we loosely use the same notation i(·) for the time-discrete version):
∆i(k) , i(k) − i(k − 1) = (βs∗ − γ) i(k − 1)
⇒ i(n) = i(0)(1 + βs∗ − γ)n ,

(54)
(55)

for some i0 > 0. From (54) we see that the ratio i(k)/i(k − 1) is constant. It is evident that real-world data are “noisy”
versions of the previous deterministic equations. Accordingly, we model the ratio i(k)/i(k − 1) , x(k) as a random variable.
Precisely, we assume:
i(n) = i(0)

n
Y

x(k),

n ≥ 1,

(56)

k=1

where {x(k)}k≥1 is a sequence of independent random variables. In light of (56), x(k) is referred to as growth rate.
By exploiting the publicly available data of the COVID-19 illness spread in Italy (freely downloadable at https://github.com/pcmdpc/COVID-19/), we obtain4 the sequence of growth rates {x(k)}k≥1 and verify that the x(k)’s are well-represented by Gaussian
3 We adopt a standard notation for the SIR model. Thus, in this section r(t) denotes the fraction of recovered individuals, not to be confused with the rate r
introduced in Sec. IV-A1. The fraction of infected individuals is denoted by i(t) and should not be confused with the time index i. Finally, the recovering
rate should not be confused with the threshold γ previously introduced. The differences should be clear from the context.
4 Precisely, the procedure is as follows. The sequence of the new positives per day {p(k)}
k≥1 is downloaded and smoothed by a moving mean filter of
length 7 days, to clean gross errors from data. Then, the growth rate is computed as x(k) = p(k + 1)/p(k), which is the same as x(k) = i(k)/i(k − 1),
see [49], [50].

12

1.3
1.2
1.1
1
0.9
0.8
Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov
2020

Mar

Apr

May

Jun

Jul

Aug

Sep

Oct

Nov
2020

6
4
2
0
-2
-4
-6

Fig. 4. Top: The sequence of growth rates {x(n)} of new positive individuals in Italy, during the COVID-19 pandemic. Bottom: Decision statistics obtained by
running the BLLR and LMS procedures on the sequence {x(n)}. Decision is for H1 when the statistic is positive and for H0 otherwise. We use σ = 0.036,
a = b = 5 and µ = 0.05.

random variables; details can be found in [49], [50]. The expected value Ex(k) is time-varying and unknown, and characterizes
the specific phase of the pandemic: when the pandemic is under control — a situation here referred to as hypothesis H0 — we
have m0 (k) , E0 x(k) ≤ 1. Conversely, under the alternative hypothesis H1 , m1 (k) , E1 x(k) > 1 and the contagion grows
exponentially fast.
Lacking knowledge of the sequences of the mean values {m0 (k)}k≥1 and {m1 (k)}k≥1 , one cannot compute the loglikelihood in (10) and the related BLLR statistic in (14). We then resort to a GLRT approach [37], which amounts to replacing
the unknown parameters appearing in the log-likelihood with their ML estimates. The approach is similar to that pursued in [41]
in the context of SPRT problems, but the estimates are structurally different. In our case the number of unknown parameters
grows linearly in time and the estimates are constrained to m
b 0 (k) ≤ 1 and m
b 1 (k) > 1, for all k. It is simple to see that
(x(k)−m)2
1
−
2σ2
= min(x(k), 1),
e
2
2πσ
(x(k)−m)2
1
−
2σ2
e
m
b 1 (k) = arg max √
= max(x(k), 1).
2
m>1
2πσ

m
b 0 (k) = arg max √
m≤1

(57)
(58)

Computing the log-likelihood yields:
log

f1 (x(k))
1
=
[(x(k) − m0 (k))2 − (x(k) − m1 (k))2 ].
f0 (x(k))
2σ 2

(59)

Substituting the ML estimates (57) and (58) in place of m0 (k) and m1 (k) in (59), one obtains
1
(x(k) − 1)2 sign(x(k) − 1).
2σ 2
Using d(k) shown in (60), in place of (10), we get the following “GLRT” version of BLLR: z(0) = 0 and

z(k) = inf b, sup{−a, z(k − 1) + d(k)} , k ≥ 1.
d(k) =

(60)

(61)

Clearly, the definition of d(k) in (60) can also be used in the LMS iteration (3) to obtain a “GLRT” version of LMS. These
“GLRT” versions are exactly as described in Algorithms 1 and 2, but take a different sequence{d(k)} in input.
We now demonstrate the GLRT versions of BLLR and LMS on COVID-19 time-series recorded in Italy. Top panel of
Fig. 4 shows the growth rate sequence {x(n)}n≥1 , where n denotes the index of the day corresponding to the date on the
abscissa, from February 25 to November 15, 2020. Bottom panel shows the BLLR and LMS decision statistics computed from
{x(n)}n≥1 . The standard deviation appearing in (60), estimated from the data, is σ = 0.036. For BLLR, we use a = b = 5;
for LMS, µ = 0.05. Assuming zero threshold for both decision statistics, in the case of BLLR we see that passage H1 7→ H0
is declared on April 15, followed by passage H0 7→ H1 on July 18, while, for LMS, the former is declared on May 4, and
the latter on July 24. We see that BLLR reacts more promptly to the change of state of nature.
To investigate the performance of BLLR and LMS on COVID-19 pandemic data, we compute ∆ and r by computer
experiments, assuming that the mean values of the data are IID random variables uniform in (1 − ǫ, 1) under H0 , and uniform
in (1, 1 + ǫ) under H1 , for some 0 < ǫ < 1. Given the mean value, Gaussian data are generated with standard deviation
σ = 0.036. In the case of BLLR, we explore various values of a = b. In the case of LMS, we compute numerically
E1 d = −E0 d, see (13), and explore various values of µ. In both cases, 103 Monte Carlo runs are used and the mid-point
threshold 0 is selected. The resulting operational characteristics are depicted in Fig. 5. We see that BLLR outperforms LMS,
consistently with the results of Sec. VI.

13

10-1

10-2

10-3

10

-4

10-5

0

20

40

60

80

100

Fig. 5. Operational characteristics of the “GLRT” versions of BLLR and LMS, relevant to COVID-19 pandemic control. The mean values of the growth rate
are drawn uniformly from (1 − ǫ, 1) under H0 and uniformly from (1, 1 + ǫ) under H1 . We use 103 Monte Carlo runs for each value of a = b (BLLR) and
µ (LMS), mid-point (zero) threshold and σ = 0.036.

VIII. C ONCLUSION
Learning and adaptation algorithms have been originally designed for solving estimation tasks. Then, they have been exploited
in decision contexts to compute an efficient online estimation of the optimal decision statistic. Thus, it seemed natural to build
the decision system on the same LMS component used for estimation purposes. In this paper, we focus on learning and
adaptation schemes for solving decision problems. We propose an alternative to LMS, called BLLR, as core element of the
decision system. Performance analysis reveals that BLLR can outperform LMS, in the slow-adaptation regime. Our study is
limited to a single decision maker and paves the way to further investigations aimed at designing the diffusion step for a
network of interconnected decision makers, much in the same way as the ATC diffusion rule has been advocated to be used
in combination with LMS.
An application to COVID-19 pandemic data has been discussed, using a “GLRT” version of BLLR. Elaborating on the
time-series of daily new positive individuals from Italy in the period from February 25 to November 15, 2020, we show that
the proposed approach effectively tracks changes of pandemic phases, providing a rigorous tool to quickly detect the passage
from a controlled regime H0 in which the number of new positives tends to decrease or be stable, to a critical regime H1
of pandemic explosion, and vice versa. In line with the considerations made in [49], [50], aside from the growth rate of
new positives, the BLLR algorithm can be executed on other pandemic time-series (e.g., hospitalizations, ratio positive/tested
individuals, etc.), paving the way to more comprehensive analyses of COVID-19 data.
A PPENDIX A
E XACT S OLUTIONS FOR THE ARL
Page’s test can be regarded as a set of parallel open-ended Wald’s SPRTs (see [39], [40]). The following analysis is based
on this similitude, see [35, Sec. 5.2.2.1]. Setting c = 0 in (27), Lh (0; γP ) can be written as the sum of two contributions. The
first is the average sample number (ASN) of the SPRT for i ≥ 1 given that the SPRT hits back the level 0 before crossing
γP , multiplied the expected number of such “back to 0” cycles (a random quantity which is easily seen to follow a geometric
distribution with range {0, 1, 2 . . . }, because any time the process hits 0 it probabilistically restarts). The second contribution
is the ASN of the SPRT given that it crosses the threshold γP before hitting back the 0 level. Denoting by p0 the probability
of hitting zero before crossing γP (starting from 0), we see that the expected number of back-to-0 cycles is p0 /(1 − p0). Then,
denoting by Eh n0 the ASN of the SPRT we have:
p0
+ Eh [n0 |cross γP ]
1 − p0
Eh [n0 |back to 0]p0 + Eh [n0 |cross γP ](1 − p0 )
=
1 − p0
Eh n0
.
=
1 − p0

Lh (0; γP ) = Eh [n0 |back to 0]

(A.1)

Using Lh (0; γP ), a similar equation for the case of c > 0 can also be derived. For the SPRT with lower threshold at 0, let
pc be the probability of hitting zero before crossing γP , when the random walk starts from zP (0) = c, and let Eh nc be the
corresponding ASN. Starting from c, two mutually exclusive situations may occur: either zP (i) hits zero before crossing γP ,

14

which happens with probability pc , or its complement occurs and zP (i) crosses γP without hitting zero. In the former case
the average run length equals the time needed to hit 0 plus Lh (0; γP ). In formula:
h
i
Lh (c; γP ) = pc Eh [nc |back to 0] + Lh (0; γP )
+ (1 − pc ) Eh [nc |cross γP ]
= pc Lh (0; γP ) + Eh nc .

(A.2)

It can be shown that pc and Eh nc are solutions to the following Fredholm integral equations of the second kind [35, p.
168]: for 0 ≤ c ≤ γP :
Z −c
Z γP
pξ fd (ξ − c) dξ
(A.3a)
pc =
fd (ξ) dξ +
0
−∞
Z γP
Eh nξ fd (ξ − c) dξ,
(A.3b)
Eh nc = 1 +
0

where fd (ξ) is the PDF of the log-likelihood d(i) shown in (10), under hypothesis Hh (dependence not made explicit for
notational simplicity). Among others, iterative methods have been proposed for solving these equations, see e.g., [35, Eq.
5.2.28]. After computing pc and Eh nc , we obtain Lh (c; γP ) from (A.1) and (A.2), and the ARL is found.
A PPENDIX B
C LASSICAL R ESULTS ON SPRT S USING M ARTINGALES
Suppose the state of nature is H0 . Consider the iteration in (12): zS (0) = 0 and
zS (i) = zS (i − 1) + d(i),

i ≥ 1,

(B.1)

where d(i) is the log-likelihood of the i-th observation defined in (10), and we have
Pn appended an “S” to indicate that we
now consider a SPRT. Recall that observations {x(i)}i≥1 are IID. Clearly, zS (n) = i=1 d(i). The random process ezS (n) is
called a martingale with respect to the sequence {d(i)}i≥1 because:
E0 [ezS (n) |d(n − 1), . . . , d(1)] = ezS (n−1) E0 ed(n)
= ezS (n−1) ,

(B.2)

and because the regularity condition E0 |ezS (n) | < ∞, ∀n, is obviously met. For some pair of thresholds −γℓ < 0 < γu , let
n = inf {i : zS (i) ≥ γu or zS (i) ≤ −γℓ }
i≥1

(B.3)

be a random time for the process ezS (n) . It can be shown that P0 (n < ∞) = 1, e.g., by considering that the sequence of
descending ladder heights of ezS (n) is non terminating, see [51]. Because n is finite with probability one, n is a stopping time,
and the expected value of the martingale at the stopping time equals the expected value at the initial time [52, Th. 6.2.2]:


f1 (x)
zS (n)
zS (1)
E0 [e
] = E0 [e
] = E0
= 1.
(B.4)
f0 (x)
Using the so-called Wald’s approximations of neglecting the excesses over the boundaries, namely,
E0 [ezS (n) |zS (n) ≥ γu ] ≈ eγu ,
E0 [e
from (B.4) we arrive at

zS (n)

|zS (n) ≤ −γl ] ≈ e

−γl

(B.5a)
,

(B.5b)



1 = E0 [ezS (n) |zS (n) ≥ γu ] 1 − P0 (zS (n) ≤ −γℓ )

+ E0 [ezS (n) |zS (n) ≤ −γℓ ]P0 (zS (n) ≤ −γℓ )


≈ eγu 1 − P0 (zS (n) ≤ −γℓ ) + e−γℓ P0 (zS (n) ≤ −γℓ ),

(B.6)

or
P0 (zS (n) ≤ −γℓ ) ≈

e γu − 1
.
eγu − e−γℓ

(B.7)

Assuming E0 |d| < ∞ and E0 n < ∞ we have (Wald’s equation) [52, Th. 3.3.2]:
E0 [zS (n)] = E0 d E0 n = −D01 E0 n.

(B.8)

15

Using approximations similar to those in (B.5), this yields


γu 1 − P0 (zS (n) ≤ −γℓ ) − γℓ P0 (zS (n) ≤ −γℓ )
E0 [n] ≈
−D01
P0 (zS (n) ≤ −γℓ )(γu + γℓ ) − γu
.
=
D01

(B.9)

Consider now expression (A.1) under H0 , namely for h = 0:
L0 (0; γP ) =

E0 n0
.
1 − p0

(B.10)

By setting γu = γP and taking the limit γℓ → 0, E0 n becomes E0 n0 , P0 (zS (n) ≤ −γℓ ) reduces to p0 , and the quantity
in (B.10) can be computed by exploiting relationships (B.7) and (B.9):
1
lim
L0 (0; γP ) =
D01 γℓ →0
=

eγP −1
eγP −e−γℓ

e γ P − γP − 1
.
D01

(γP + γℓ ) − γP

1−e−γℓ
eγP −e−γℓ

(B.11)

Similar calculations hold under H1 , considering this time the martingale e−zS (n) , yielding
L1 (0; γP ) =

γP + e−γP − 1
.
D10

A PPENDIX C
N UMERICAL S OLUTIONS

TO

(B.12)

(40)

In many cases of interest, an approximate numerical solution to (40) can be obtained by a simple iterative procedure, based
on heuristic arguments. Let us start by considering a modified version of the problem in which the iteration {w(i)}i≥0 with
initial point w(0) = −D01 evolves up to cross one of the two thresholds γℓ < −D01 and γ > −D01 . The same arguments
used to derive (40) lead to the Fredholm integral equation
Z
 ξ + (1 − µ)D 
1 γ ′
01
′
dξ,
(C.1)
T0 (ξ; γ)fd
T0 (−D01 ; γ) = 1+
µ −γℓ
µ
wherein µ is the step-size and fd (·) is the distribution of d under H0 . Let us consider an operator P : C[−γℓ , γ] 7→ C[−γℓ , γ]
mapping the complete metric (Banach) space of continuous functions C[−γℓ , γ] defined over the closed interval [−γℓ , γ], into
itself. The operator P is defined by:
Z
 ξ − (1 − µ)x 
1 γ
P(g(x)) = 1 +
g(ξ)fd
dξ.
(C.2)
µ −γℓ
µ
As seen in (C.1), T0′ (x; γ), x ∈ [−γℓ , γ] is a fixed point of the operator P, namely P(T0′ (x; γ)) = T0′ (x; γ). Thus, the problem
of solving (C.1) reduces to the problem of finding such fixed point, provided that it exists and is unique.
Picard-Banach fixed point principle states that a mapping P of a complete metric space into itself has a unique fixed point
g ∗ provided that the mapping is a contraction [53, Sec. 9.7]. The operator P is called a contraction if
kP(g1 ) − P(g2 )k ≤ δkg1 − g2 k,

(C.3)

for some 0 < δ < 1. In addition, for a contraction, the iteration
gn = P(gn−1 ),

n ≥ 1,

(C.4)

converges to the fixed point g ∗ with a rate of convergence
kgn − g ∗ k ≤
where g0 ∈ C[−γℓ , γ] is some initial guess [53, Eq. (9.21)].

δn
kg1 − g0 k,
1−δ

(C.5)

16

Thus, (C.4) provides a simple numerical recipe to solve (C.1). To check that P is a contraction, using the supremum norm,
note that
kP(g1 ) − P(g2 )k =
Z

sup

P(g1 (x)) − P(g2 (x))

x∈[−γℓ ,γ]

h
i  ξ − (1 − µ)x 
dξ
g1 (ξ) − g2 (ξ) fd
µ
−γℓ
Z γ
 ξ − (1 − µ)x 
1
≤
sup
dξ
g1 (ξ) − g2 (ξ) fd
µ x∈[−γℓ ,γ] −γℓ
µ
Z γ
1  ξ − (1 − µ)x 
dξ
fd
= g1 (x∗ ) − g2 (x∗ )
sup
µ
x∈[−γℓ ,γ] −γℓ µ

=

1
sup
µ x∈[−γℓ ,γ]

γ

≤ g1 (x∗ ) − g2 (x∗ ) ≤

sup

g1 (x) − g2 (x)

(C.6a)
(C.6b)

x∈[−γℓ ,γ]

= kg1 (x) − g2 (x)k,

(C.6c)

where in (C.6a) x∗ ∈ [−γℓ , γ] and the equality follows by the mean value theorem for integrals [54, Th. 5, p.352], while the
first inequality in (C.6b) follows because the integrand in (C.6a) represents a valid PDF.
The rigorous approach provided by the theory of fixed points does not apply directly to our case because in (40) we have
γℓ → ∞ and the derivation leading to (C.6c) fails for unbounded intervals, in which the mean value theorem for integrals do
not apply. Numerical approaches to the solution of Fredholm integral equations defined over unbounded intervals have been
proposed, see [55] and the references therein. However, for the sake of simplicity, here we limit ourselves to apply (C.4),
checking numerically the convergence. Summarizing, we use the following heuristic approach: start with some initial guess
g0 (x), x ∈ (−∞, γ], and iterate for n ≥ 1:
Z
 ξ − (1 − µ)x 
1 γ
dξ
(C.7)
gn−1 (ξ)fd
gn (x) = 1 +
µ −∞
µ
up to convergence, if any. In the computer experiments presented in this work the initial guess g0 (x) is taken as a constant
function, whose value is equal to the value T0′ (−D01 ; γ) obtained by simulations. The numerical evaluations of the performance
for the LMS decision algorithm presented in this paper are based on (C.7) and similar iterates, using numerical integration for
integrals.
R EFERENCES
[1] J.-F. Chamberland and V. V. Veeravalli, “Decentralized detection in sensor networks,” IEEE Trans. Signal Process., vol. 51, no. 2, pp. 407–416, Feb.
2003.
[2] J. B. Predd, S. R. Kulkarni, and H. V. Poor, “Distributed learning in wireless sensor networks,” IEEE Signal Process. Mag., vol. 23, no. 4, pp. 56–69,
Jul. 2006.
[3] J. Akyildiz, W. Su, Y. Sankarasubramaniam, and E. Cayirci, “A survey on sensor networks,” IEEE Commun. Mag., vol. 40, no. 8, pp. 102–114, Aug.
2002.
[4] P. K. Varshney, Distributed Detection and Data Fusion. New York, NY: Springer, 1997.
[5] R. Viswanathan and P. K. Varshney, “Distributed detection with multiple sensors: Part I – fundamentals,” Proc. IEEE, vol. 85, no. 1, pp. 54–63, Jan.
1997.
[6] R. S. Blum, A. Kassam, and H. V. Poor, “Distributed detection with multiple sensors: Part II – advanced topics,” Proc. IEEE, vol. 85, no. 1, pp. 64–79,
Jan. 1997.
[7] J. N. Tsitsiklis, “Decentralized detection,” in Advances in Signal Processing, H. V. Poor and J. B. Thomas, Eds., JAI Press, 1993, pp. 297–344.
[8] ——, “Decentralized detection by a large number of sensors,” Math. Contr., Signals, Syst., vol. 1, pp. 167–182, 1988.
[9] L. Tong, Q. Zhao, and S. Adireddy, “Sensor networks with mobile agents,” in Proceedings of MILCOM 2003, vol. 1, Boston MA, Oct. 2003, pp.
688–693.
[10] S. Marano, V. Matta, P. Willett, and L. Tong, “DOA estimation via a network of dumb sensors under the SENMA paradigm,” IEEE Signal Process.
Lett., vol. 12, no. 10, pp. 709–712, Oct. 2005.
[11] S. Marano, V. Matta, and L. Tong, “Secrecy in cooperative SENMA with unauthorized intrusions,” in Proceedings of IEEE Workshop on Signal Processing
Advances in Wireless Communications (SPAWC), Cannes, France, July 2-5 2006.
[12] Z. Yang and L. Tong, “On the error exponent and the use of LDPC codes for cooperative sensor networks with misinformed nodes,” IEEE Trans. Inf.
Theory, vol. 53, no. 9, pp. 3265–3274, Sep. 2007.
[13] S. S. Boyd, A. Ghosh, and B. S. D. Prabhakar, “Gossip algorithms: Design, analysis and applications,” in Proc. of INFOCOM, Miami, USA, March,
13-17 2005, pp. 1653–1664.
[14] P. Braca, S. Marano, and V. Matta, “Enforcing consensus while monitoring the environment in wireless sensor networks,” IEEE Trans. Signal Process.,
vol. 56, no. 7, pp. 3375–3380, 2008.
[15] P. Braca, S. Marano, V. Matta, and P. Willett, “Asymptotic optimality of running consensus in testing binary hypotheses,” IEEE Trans. Signal Process.,
vol. 58, no. 2, pp. 814–825, 2010.
[16] S. Kar and J. M. F. Moura, “Convergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental limits and tradeoffs,” IEEE J.
Sel. Topics Signal Process., vol. 5, no. 4, pp. 674–690, Aug. 2011.
[17] D. Bajovic, D. Jakovetić, J. Xavier, B. Sinopoli, and J. M. F. Moura, “Distributed detection via Gaussian running consensus: Large deviations asymptotic
analysis,” IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4381–4396, Sep. 2011.
[18] D. Jakovetić, J. M. F. Moura, and J. Xavier, “Distributed detection over noisy networks: Large deviations analysis,” IEEE Trans. Signal Process., vol. 60,
no. 8, pp. 4306–4320, Aug. 2012.

17

[19] D. Bajovic, D. Jakovetić, J. M. F. Moura, J. Xavier, and B. Sinopoli, “Large deviations performance of consensus+innovations distributed detection with
non-Gaussian observations,” IEEE Trans. Signal Process., vol. 60, no. 11, pp. 5987–6002, Nov. 2012.
[20] ——, “Distributed detection over adaptive networks using diffusion adaptation,” IEEE Trans. Signal Process., vol. 59, no. 5, pp. 1917–1932, 2011.
[21] A. H. Sayed, S.-Y. Tu, J. Chen, X. Zhao, and Z. J. Towfic, “Diffusion strategies for adaptation and learning over networks,” IEEE Signal Process. Mag.,
vol. 30, no. 3, pp. 155–171, 2013.
[22] A. H. Sayed, “Adaptation, learning, and optimization over networks,” in Foundations and Trends in Machine Learning. Boston-Delft: NOW Publishers,
2014, vol. 7, no. 4–5, pp. 311–801.
[23] ——, “Adaptive networks,” Proc. IEEE, vol. 102, no. 4, pp. 460–497, Apr. 2014.
[24] J. Chen and A. H. Sayed, “On the learning behavior of adaptive networks—Part I: Transient analysis,” IEEE Trans. Inf. Theory, vol. 61, no. 6, pp.
3487–3517, Jun. 2015.
[25] ——, “On the learning behavior of adaptive networks—Part II: Performance analysis,” IEEE Trans. Inf. Theory, vol. 61, no. 6, pp. 3518–3548, Jun.
2015.
[26] Z. Towfic, J. Chen, and A. H. Sayed, “Excess-risk of distributed stochastic learners,” IEEE Trans. Inf. Theory, vol. 62, no. 10, pp. 5753–5785, Oct.
2016.
[27] V. Matta, P. Braca, S. Marano, and A. H. Sayed, “Diffusion-based adaptive distributed detection: Steady-state performance in the slow adaptation regime,”
IEEE Trans. Inf. Theory, vol. 62, no. 8, pp. 4710–4732, Aug. 2016.
[28] ——, “Distributed detection over adaptive networks: Refined asymptotics and the role of connectivity,” IEEE Trans. Signal and Inf. Process. over
Networks, vol. 2, no. 4, pp. 442–460, Dec. 2016.
[29] S. Marano and A. H. Sayed, “Detection under one-bit messaging over adaptive networks,” IEEE Trans. Inf. Theory, vol. 65, no. 10, pp. 6519–6538, Oct.
2019.
[30] ——, “Adaptation and learning in multi-task decision systems,” in Proc. of the 45th IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP 2020), Barcelona, May 4-8 2020.
[31] ——, “Decision learning and adaptation over multi-task networks,” June 2020, submitted.
[32] J. Chen, C. Richard, and A. H. Sayed, “Multitask diffusion adaptation over networks with common latent representations,” IEEE J. Sel. Topics Signal
Process., vol. 11, no. 3, pp. 563–579, Apr. 2017.
[33] A. H. Sayed, Adaptive Filters. NY: Wiley, 2008.
[34] B. Hassibi, A. H. Sayed and T. Kailath, “H∞ optimality of the LMS algorithm,” IEEE Trans. Signal Process., vol. 44, no. 2, pp. 267-280, Feb. 1996.
[35] M. Basseville and I. V. Nikiforov, Detection of Abrupt Changes: Theory and Application. Englewood Cliffs, N.J: Prentice-Hall, 1993.
[36] A. Tartakovsky, I. Nikiforov and M. Basseville, Sequential Analysis. Hypothesis Testing and Changepoint Detection. Boca Raton, FL, USA: CRC Press,
Taylor & Francis Group, 2015.
[37] H. V. Poor, An Introduction to Signal Detection and Estimation. New York: Springer-Verlag, 1988.
[38] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed. New Jersey, USA: Wiley-Interscience, 2006.
[39] A. Wald, Sequential Analysis. New York: Dover, 1947.
[40] A. Wald and J. Wolfowitz, “Optimum character of the sequential probability ratio test,” The Annals of Statistics, vol. 19, no. 3, pp. 326–339, Sep. 1948.
[41] Q. Zou, S. Zheng and A. H. Sayed, “Cooperative sensing via sequential detection,” IEEE Trans. Signal Process., vol. 58, no. 12, pp. 6266–6283, Dec.
2010.
[42] L. G. Afanas’eva and E. V. Bulinskaya, “Certain asymptotic results for random walks in a strip,” Theory of Probability and its Applications, vol. XXIX,
no. 4, pp. 677–693, 1985.
[43] A. A. Borovkov “On a walk in a strip with inhibitory boundaries,” Institute of Mathematics, Siberian Branch, Academy of Sciences of the USSR.
Translated from Matematicheskie Zametki, vol. 17, no. 4, pp. 649-657, April 1975.
[44] E. Page, “Continuous inspection schemes,” Biometrika, vol. 41, pp. 100–115, Jan. 1954.
[45] S. V. Crowder, “A simple method for studying run-length distributions of exponentially weighted moving average charts,” Technometrics, vol. 29, no. 4,
pp. 401–407, 1987.
[46] D. Siegmund, Sequential Analysis: Tests and Confidence Intervals. New York: Springer-Verlag, 1985.
[47] W. O. Kermack and A. G. McKendrick, “A contribution to the mathematical theory of epidemics,” Proc. Roy. Soc. Lond. A, vol. 115, pp. 700–721, 1927.
[48] L. J. S. Allen, “A primer on stochastic epidemic models: Formulation, numerical simulation, and analysis,” Infectious Disease Modelling, vol. 2, pp.
128–142, 2017.
[49] P. Braca, D. Gaglione, S. Marano, L. M. Millefiori, P. Willett, and K. Pattipati, Quickest Detection of Critical COVID-19 Phases: When Should Restrictive
Measures Be Taken? Nov. 2020, submitted. Accessible at http://arxiv.org/abs/2011.11540
[50] —— Quickest Detection of COVID-19 Pandemic Onset. Nov. 2020, to be submitted. Accessible at http://arxiv.org/abs/2011.10502
[51] W. Feller, An Introduction to Probability and Its Applications, Volume 2. New York: John Wiley & Sons, 1971.
[52] S. Ross, Stochastic Processes, 2nd ed. New York: John Wiley & Sons, Inc., 1996.
[53] V. A. Zorich, Mathematical Analysis II. Berlin: Springer, 2004.
[54] ——, Mathematical Analysis I. Berlin: Springer, 2004.
[55] Z. Avazzadeh and M. Heydari, “Integral mean value method for solving Fredholm integral equations of the second kind on the half line,” Journal of
Multidisciplinary Engineering Science Studies (JMESS), vol. 2, no. 1, pp. 230–235, Jan. 2016.

