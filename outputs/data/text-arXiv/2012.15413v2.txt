Noname manuscript No.
(will be inserted by the editor)

New Bag of Deep Visual Words based Features to Classify
Chest X-ray Images for COVID-19 diagnosis

arXiv:2012.15413v2 [eess.IV] 28 Jan 2021

Chiranjibi Sitaula

· Sunil Aryal

Received: DD Month YEAR / Accepted: DD Month YEAR

Abstract Purpose: Because the infection by Severe
Acute Respiratory Syndrome Coronavirus 2 (COVID19) causes the pneumonia-like effect in the lung, the
examination of Chest X-Rays (CXR) can help diagnose
the disease. For automatic analysis of images, they are
represented in machines by a set of semantic features.
Deep Learning (DL) models are widely used to extract
features from images. General deep features extracted
from intermediate layers may not be appropriate to represent CXR images as they have a few semantic regions. Though the Bag of Visual Words (BoVW)-based
features are shown to be more appropriate for different types of images, existing BoVW features may not
capture enough information to differentiate COVID19 infection from other pneumonia-related infections.
Methods: In this paper, we propose a new BoVW
method over deep features, called Bag of Deep Visual
Words (BoDVW), by removing the feature map normalization step and adding the deep features normalization step on the raw feature maps. This helps to
preserve the semantics of each feature map that may
have important clues to differentiate COVID-19 from
pneumonia. Results: We evaluate the effectiveness of
our proposed BoDVW features in CXR image classification using Support Vector Machine (SVM) to diagnose COVID-19. Our results on four publicly available
COVID-19 CXR image datasets reveal that our features produce stable and prominent classification accuracy, particularly differentiating COVID-19 infection
from other pneumonia. Conclusion: Our method could

C. Sitaula · S. Aryal
School of Information Technology, Deakin University
75 Pigdons Rd, Waurn Ponds VIC 3216
E-mail: {c.sitaula, sunil.aryal}@deakin.edu.au

be a very useful tool for the quick diagnosis of COVID19 patients on a large scale.
Keywords Bag of Visual Words (BoVW) · Bag
of Deep Visual Words (BoDVW) · Chest X-Ray ·
COVID-19 · Deep Features · SARS-CoV-2

1 Introduction
The disease caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) [24, 29, 44], commonly known as COVID-19, was originated in Wuhan
city of China in late 2019 [46]. It is believed to be originated from bats [25, 35]. The virus has been transmitting from human to human all around the world [16,
11, 3]. It has spread over 200 countries in the world at
present and become a pandemic that has killed 2,184,120
people1 and 909 people in Australia alone2 , so far. While
analyzing the effect of the SARS-CoV-2 virus in the human body, it has been known that it causes the pneumonialike effect in the lungs. Thus, the study of chest xray images could be an alternative to a swab test for
early quick diagnosis of the COVID-19. An automated
chest x-ray (CXR) image analysis tool can be very useful to health practitioners for mass screening of people
quickly.
For automatic analysis of images using algorithms,
they are represented in machines by a set of semantic
features. Large artificial neural networks, also known
1
https://www.worldometers.info/coronavirus/ (accessed
date: 28/01/2021)
2
https://www.health.gov.au/news/health-alerts/novelcoronavirus-2019-ncov-health-alert/coronavirus-covid19-current-situation-and-case-numbers.
(accessed
date:
28/01/2021)

2

as Deep Learning (DL) models, are widely used to extract features from images and shown to work well in
various types of images [51, 47, 49, 50, 13, 34]. A few
research studies have used DL models to analyze CXR
images for coronavirus diagnosis, too. For instance, two
recent works [30, 34] include the fine-tuning approach
of transfer-learning on pre-trained DL models such as
AlexNet [22], ResNet-18 [14], GoogleNet [53], etc. These
methods normally require a massive amount of data to
learn the separable features in addition to extensive
hyper-parameter tuning tasks. However, most of the
biomedical images (e.g., COVID-19 CXR images) are
normally limited because of privacy issues. Thus, working on a limited amount of data is always a challenging
problem in deep learning (DL) models. Similarly, unlike other types of images, existing feature extraction
methods such as GAP (Global Average Pooling) features achieved from pre-trained models may not provide accurate representation for CXR images because
of their sparsity (i.e., having fewer semantic regions in
them). Also, CXR images of lungs infected by COVID19 and other pneumonia look similar (i.e., there is a
high degree of inter-class similarities). There might be
subtle differences at very basic level, which, in our understanding, may be captured using the Bag of Words
approach over deep features.
Bag of Visual Words (BoVW)-based features are
shown to be more appropriate in images with the characteristics discussed above (sparsity and high inter-class
similarity). They consider visual patterns/clues (known
as visual words) in each image in the collection, thereby
capturing sparse interesting regions in the image, which
are useful in dealing with the inter-class similarity problem to some degree. BoVW-based feature extraction
approach is popular not only in traditional computer
vision-based methods such as Scale Invariant Features
Transform (SIFT) [31] but also in DL-based methods
due to its ability to capture semantic information extracted from the feature map of pre-trained DL models.
The Bag of Deep Visual Words (BoDVW) features designed for one domain may not work well for another
domain due to the varying nature of the images. For example, the Bag of Deep Convolutional Features (DCFBoVW) [56] designed for satellite images may not work
exactly for biomedical images such as CXR images.
This is because of the fact that satellite image contains numerous semantic regions scattered in the image
(dense) and thus, DCF-BoVW could capture enough
semantic regions of such images. However, the CXR
images contain fewer semantic regions (sparse), which
may not be captured accurately by DCF-BoVW.
In this paper, we propose a new BoDVW-based feature extraction method to represent CXR images. Our

Chiranjibi Sitaula

, Sunil Aryal

(a) DCF-BoVW

(b) Our method
Fig. 1 Scatter plot of two dimensional projection of features
produced by DCF-BoVW and our proposed method based on
t-SNE visualization on chest x-ray images of Dataset 4 [8, 20].

method eliminates some of the intermediate steps present
in DCF-BoVW [56] and adds new steps because of the
nature of CXR images. For this, we adopt the following steps. First, we extract the raw feature map from
the mid-level (4th pooling layer) of the VGG16 pretrained DL model [45] for each input image. We prefer
the 4th pooling layer in our work, which has been chosen
by empirical study and suggestion from recent work by
Sitaula et al. [48]. Next, we perform L2-normalization
of each deep feature vector over the depth of the feature map. Using the training set, we design a codebook/dictionary over such deep features extracted from
all the training images. Next, based on the codebook,
we achieve our proposed features using a bag of visual words method for each input image. Last, such
features based on the bag of visual words method is
normalized by L2-norm, which acts as the final representation of the input image. Because our final features
are based on patterns extracted from mid-level features
from training images, they capture the more discriminating clues of sparse CXR images. The comparison
of two-dimensional projections of features produced by
DCF-BoVW and our proposed method on the COVID19 image dataset [8] based on the t-SNE visualization

New Bag of Deep Visual Words based Features to Classify Chest X-ray Images for COVID-19 diagnosis

[33] is shown in Fig. 1. It reveals that our features impart the higher separability among different classes.
The main contributions in our work are listed below:
(a) Propose to use the improved version of a bag of
visual words method over deep features to work for
the covid-19 CXR image representation.
(b) Analyze the classification performance of our method
across deep features extracted from five different
pooling layers of the VGG16 model. Due to higher
discriminability of deep features extracted from midlevel VGG16 model (see details in Sec. 4.4 and Sitaula
et al. [48]), we leverage fourth pooling layer (p 4) for
feature extraction in our work. To design a codebook
from deep features in our work, we use unsupervised
clustering with the simple k-means algorithm.
(c) Evaluate our method on four datasets against the
state-of-the-art methods based on pre-trained DL
models in the covid-19 CXR classification task using
the Support Vector Machine (SVM) classifier. The
results show that our method produces stable and
state-of-the-art classification performance.
The remainder of the paper is organized as follows.
In Sec. 2, we review some of the recent related works
on CXR image representation and classification. Similarly, we discuss our proposed method in Sec. 3 in a
step-wise manner. Furthermore, Sec. 4 details the experimental setup, performance comparison, and ablative study associated with it. Finally, Sec. 5 concludes
our paper with potential directions for future research.

2 Related works
Deep Learning (DL) has been a breakthrough in image
processing producing significant performance improvement in tasks such as classification, object detection,
etc. A DL model is a large Artificial Neural Network
(ANN), which has been designed based on the working paradigm of brain. If we design our DL model from
scratch and train it, it is called a user-defined DL model.
Similarly, if we use existing deep learning architectures
pre-trained on large datasets, such as ImageNet [10]
or Places [57], they are called pre-trained DL models.
The features extracted from intermediate layers of DL
models, either user-defined or pre-trained, provide rich
semantic features to represent images that result in significantly better task-specific performance than traditional computer vision methods such as Scale Invariant Feature Transform (SIFT) [31], Generalized Search
Tree (GIST)-color [37], Generalized Search Trees (GIST)
[36], Histogram of Gradient (HOG) [9], Spatial Pyramid
Matching (SPM) [26], etc.

3

Thus, in this section, we review some of the recent works in chest x-ray classification using DL models
[52, 18, 2, 55, 7, 30, 43, 34, 38, 32, 39, 48]. We categorize them into two groups: 2.1 standalone deep learning
algorithms and 2.2 ensemble learning algorithms
2.1 Standalone deep learning algorithms
At first, Stephen et al. [52] presented a new model
for the detection of pneumonia using DL and machine
learning approach. They trained a Convolutional Neural Network (CNN) from scratch using a collection of
CXR images. Islam et al [18] devised a Compressed
Sensing (CS)-based DL model for the automatic classification of CXR images for pneumonia disease. Similarly, Ayan et al. [2] used DL models on CXR images for
early diagnosis of pneumonia. They used Xception [5]
and VGG16 [45] pre-trained models. Their results unveil that the VGG16 model outperforms the Xception
model in terms of classification accuracy. This strengthens the efficacy of VGG16 model for CXR image representation and classification. Thus, the use of a pretrained model became widespread in the representation
and classification CXR images. For example, Varshni
et al. [55] leveraged several pre-trained models such as
VGG16 [45], Xception [5], ResNet50 [14], DenseNet121
[17], and DenseNet169 [17] individually as the features
extractors and trained four classifiers separately using
SVM [15], Random Forest [4], k-nearest neighbors [1],
and Naı̈ve Bayes [27] for the classification purpose. Furthermore, Loey et al. [30] used Generative Adversarial
Networks (GAN) [12] and fine-tuning on AlexNet [22],
ResNet18 [14], and GoogleNet [53] for the classification
of the COVID-19 CXR dataset, where images belong to
4 categories.In their method, GAN was used to augment
the x-ray images to overcome the over-fitting problem
during the training phase. Moreover, Khan et al. [21]
devised a new deep learning model using the Xception
[5] model, where they performed fine-tuning using CXR
images.
Moreover, Ozturk et al. [38] established a new deep
learning model for the categorization of COVID-19 related CXR images that uses DarkNet19 [41]. Furthermore, Luz et al. [32] devised another novel deep learning (DL) model, which uses the EfficientNet [54] model,
which adopts transfer learning over CXR images for the
classification task. Furthermore, Panwar et al. [39] established a new model, which is called nCOVnet, using
the VGG16 model, which imparts a prominent accuracy
for COVID-19 CXR image analysis. This further claims
that the VGG16 model, which was quite popular in the
past, is still popular in CXR image analysis. Recently,
Sitaula et al. [48] established an attention module on

4

Chiranjibi Sitaula

, Sunil Aryal

Fig. 2 The overall pipeline of the proposed method. Based on the codebook/dictionary achieved from training block, the
proposed features vector is extracted for each input image using the bag of visual features approach.

top of the VGG16 model (AVGG) for the CXR images
classification. Their method outperforms several stateof-the-art methods.

2.2 Ensemble learning algorithms
Ensemble learning methods have also been used in CXR
image representation and classification where different
types of features are combined for better discrimination of images. Zhou et al. [58] proposed an ensemble
learning approach of several ANNs for the lung cancer cell identification task. Sasaki et al. [43] established
an ensemble learning approach using DL on CXR images. In their method, they performed several filtering
and pre-processing operations on images and then ensembled them using DL for the detection of abnormality in CXR images. Li et al. [28] also utilized multiple CNNs to reduce the false positive results on lung
nodules of CXR images. Moreover, Islam et al. [18]

designed an ensemble method to aggregate different
pre-trained deep learning models for abnormality detection in lung images. Chouhan et al. [7] introduced a
model, where the outputs of 5 pre-trained deep learning models, namely AlexNet, ResNet18, DenseNet121,
GoogleNet, and Inception-V3, were ensembled for the
detection of pneumonia using transfer learning. This
helps to learn multiple types of information achieved
from various pre-trained DL models to bolster the classification performance. Nevertheless, ensemble learning
algorithms are arduous for which we need to be vigilant in hyper-parameter tuning in addition to the overfitting problem.
Most existing methods in the literature need a huge
amount of data for fine-tuning DL models and most of
them extract high-level features, which may not be sufficient for CXR images. They require mid-level features
that are neither more generic nor more specific. In the
next section, we introduce our proposed approach to
extract such mid-level features.

New Bag of Deep Visual Words based Features to Classify Chest X-ray Images for COVID-19 diagnosis

3 Proposed method
The mid-level features of CXR images can be achieved
from the feature maps extracted from the intermediate layers of pre-trained models using a Bag of Visual
Words (BoVW) method. Since CXR images are sparse
(having few semantic regions), an existing bag of visual words method that has been applied to represent
other images (e.g., satellite images) may not work accurately in this domain. To this end, we propose an improved version of a bag of visual words method on deep
features to represent CXR images more accurately. In
this section, we discuss the steps involved in our proposed feature extraction method. There are three main
steps in our method: deep features extraction (Sec. 3.1),
unsupervised codebook (dictionary) design (Sec. 3.2),
and proposed features extraction (Sec. 3.3). The overall pipeline of the proposed method is shown in Fig.
2.

3.1 Deep features extraction
At first, we extract the deep features from the feature
map of the 4th pooling (p 4) layer from VGG16 [45],
which is a deep learning model pre-trained on ImageNet [10]. We prefer VGG16 in our work because of
three reasons. First, it has a unrivalled performance in
recent biomedical image analysis works such as COVID19 CXR image analysis [48], breast cancer image analysis [47], etc. Second, it is easy to analyze and experiment with its five pooling layers. Third, it uses smallersized kernels, which could learn distinguishing features
of biomedical images at a smaller level.
We believe that 4th layer of such a model has a
higher level of discriminability than other layers as seen
in Fig. 3. The detailed discussion about the efficacy of
the 4th pooling layer is also presented in Sec. 4.4. Furthermore, we use the VGG16 model due to its simple
and prominent features extraction capability in various
types of image representation tasks [51, 23, 13]. Authors
in [48, 47] highlight the importance of 4th pooling layer
compared to other layers in biomedical imaging for separable feature extraction. The size of the features map
from the p 4 layer of the VGG16 model is 3-D shape
having H = 14 (height), W = 14 width, and L = 512
(length). From each feature map, we achieve 14 × 14
number of features, each of size 512. Then, each feature vector is L2-normalized. This normalization helps
to preserve the separability of deep features of images
[13]. Let us say that an input image yields feature map
with 14 × 14 = 196 number of features vectors that are
represented by x0 , x1 , x2 ,· · · ,x196 . Each features vec-

5

tor xi is of 512-D size (i.e., |xi | = 512), which is then
normalized by L2-norm as seen in Eq. (1).

x0i =

xi
||xi ||2 + 

(1)

In Eq. (1), the features vector x0i represents the ith
normalized deep features vector extracted from the corresponding feature map. While achieving such features
vector, we add  = 0.00000008 with denominator to
avoid the divide by zero exception because the feature
map obtained for chest x-ray images is sparse and it is
more likely to encounter the divide by zero exception
in most cases.

3.2 Unsupervised dictionary (codebook) design
We used deep features (extracted from the VGG16 model
as discussed above in Sec. 3.1) of all training images to
design a dictionary or codebook. Each image provides
{x0i }196
i=1 deep features and let’s say there are m training
images. Thus, the total number of deep features to design our codebook is 196 × m. To design the codebook
or dictionary, we utilize a simple, yet popular unsupervised clustering algorithm called k-means [19] that
groups deep features having similar patterns into clusters. Given a parameter k, k-means provide k groups or
clusters ({c1 , c2 , · · · , ck }) of deep features where deep
features in each group are similar (i.e., they capture
similar patterns of images). We use such k cluster centroids as a dictionary or codebook of deep visual words
which is used to extract features for each input image.

3.3 Proposed feature extraction
To extract features of each input image y, we first follow step 3.1 to achieve 196 normalized deep features of
y and then, design a histogram based on the dictionary
defined in step 3.2. The size of histogram is k (the dictionary size) where each code (cluster centroid) in the
dictionary cj has a weight wj . All 196 deep features of
y are assigned to their nearest centroids. The weight wj
is the number of deep features assigned to the cluster
cj . In other words, histogram is a bag of visual words
(centroids) where weights are their frequencies. The resulting features of y is a k-D vector {w1 , w2 , · · · , wk }.
The extracted bag of visual words features vector is,
finally, normalized as in Eq. (1), which acts as our proposed features of the corresponding input image.

6

Chiranjibi Sitaula

, Sunil Aryal

Fig. 3 Feature maps of an input image from each of the four categories in the COVID-19 dataset extracted from the five
pooling layers of VGG16. p i (i = 1, 2, · · · , 5) represents the ith polling layer.

3.4 Difference between our BoVW and DCF-BoVW
features
The main differences between our BoVW and DFCBoVW features are explained in three different aspects.
Firstly, the L1-normalisation used by the DCF-BoVW
method is more suitable for dense images such as satellite images. However, since the chest x-ray images are

(a)

(b)

(c)

sparse in nature, such normalization becomes counterproductive as it masks some discriminating clues. Thus,
we eliminate this normalization in our method due to
the nature of chest x-ray images.
Secondly, we apply L2-normalisation to the deep
features extracted from the unnormalized feature maps
to exploit the property of cosine similarity in the kmeans clustering. Note that Euclidean distance on the
L2-normalised feature is equivalent to using cosine distance. The directions of deep features are more important than their lengths to group vectors with similar
patterns into clusters to define our codebook. This will
help us to detect sparse patterns in images which can be
useful in discriminating abnormalities in x-ray images.
Finally, we replace the L1-normalisation of the final
BoVW features used in the DCF-BoVW method by
L2-normalisation. Again, this allows us to exploit the
property of cosine similarity in the SVM’s RBF kernel. Because BovW features are sparse as many vector
entries are zeros, cosine similarity is more appropriate
than the Euclidean distance.

4 Experimental setup and comparison
(d)
Fig. 4 Example images of chest x-ray images from Dataset
4 [8, 20] for four classes: (a) Covid, (b) Normal, (c) PneumoniaB, and (d) PneumoniaV.

4.1 Dataset
We utilize 4 COVID-19 CXR image datasets that are
publicly available. To evaluate our method on such datasets,

New Bag of Deep Visual Words based Features to Classify Chest X-ray Images for COVID-19 diagnosis
Table 1 Detailed description of datasets used in our work
Dataset

#
of
images

Categories

Ref.

Dataset
1 (D1)
Dataset
2 (D2)

1,125

[38]

Dataset
3 (D3)

2,138

Dataset
4 (D4)

320

Covid-19, Pneumonia, No findings
Covid,
Normal,
PneumoniaB, PneumoniaV
Covid,
Normal,
No findings, PneumoniaB, PneumoniaV
Covid,
Normal,
PneumoniaB, PneumoniaV

1,638

[21]

[21, 38]

[8, 20]

we divide the images of each dataset into a 70:30 ratio
for the train:test set for each category. We utilize the
average accuracy of five different runs to present in the
table for the comparison purpose.
Dataset 1 [38] comprises 3 categories: Covid-19,
Pneumonia, and No findings. Here, each category has
at least 125 images. The No findings category comprises
several ambiguous and challenging CXR images.
Dataset 2 [21] comprises 4 categories: Covid, Normal, Pneumonia Viral (PneumoniaV) and Pneumonia
Bacteria (PneumoniaB)
Dataset 3 [21, 38] includes 5 categories: Covid,
No findings, Normal, Pneumonia Bacteria (PneumoniaB),
and Pneumonia Viral (PneumoniaV). Dataset 3 is the
combination of No finding category from Dataset 1 and
other categories from Dataset 2. Here, each category includes at least 320 CXR images.
Dataset 4[8, 20] has 4 categories: Covid, Normal,
PneumoniaV, and PneumoniaB, where each category
contains at least 69 images. This dataset has been used
by [30], which can be downloaded from the link 3
Example images of covid-19 are shown in Fig. 4.
Also, further detailed information of all datasets are
provided in Table 1.

4.2 Implementation
To implement our work, we use Keras [6] implemented
in Python [42]. Keras is used to implement the pretrained model in our work. We use the number of clusters k = 400 in k-means clustering to define the dictionary to extract proposed features. For the classification purpose, we use a Support Vector Machine (SVM)
classifier implemented using Scikit-learn [40] in Python.
We normalize and standardize our features to feed into
3
COVID-19
Dataset
Available
online:
https://drive.google.com/uc?id=1coM7x3378fOu2l6Pg2wldaOI7Dntu1a (accessed on Apr 17, 2020).

7

Table 2 Comparison with previous methods on four datasets
(D1, D2, D3, and D4) using average classification accuracy
(%) over five runs. Note that - represents the unavailable
accuracy because of the over-fitting problems in existing DLbased methods using transfer learning on D4.
Method

D1 (%)

D2 (%)

D3 (%)

D4 (%)

DCF-BoVW,
2018 [56]
CoroNet, 2020
[21]
Luz et al.,
2020 [32]
nCOVnet,
2020 [39]
AVGG, 2020
[48]

75.31

81.53

83.72

72.46

76.82

80.60

83.41

-

47.51

84.29

79.96

-

62.95

70.62

67.67

-

79.58

85.43

87.49

-

Ours

82.00

87.86

87.92

83.22

the SVM classifier. Moreover, we fix the kernel as radial basis function (RBF ) and γ parameter as 1e − 05
in SVM. We automatically tune the cost parameter C
in the range of {1, 10, 20, · · · , 100} on the training set
using a 5-fold cross-validation method and use the optimal setting to train the model using the entire training
set and test on the test set. We execute all our experiments on a workstation with NVIDIA Geforce GTX
1050 GPU and 4 GB RAM.

4.3 Comparison with state-of-the-art methods
We present the results of the experiments conducted
to compare our method with five recent state-of-theart methods (one method uses the BoW approach over
deep features and four methods adopt transfer-learning
approach) that are based on pre-trained models on four
CXR image datasets (D1, D2, D3, and D4) in Table 2.
In the table, the second, third, fourth, and fifth columns
enlist the accuracies of contending methods in D1, D2,
D3, and D4, respectively. Note that the accuracies reported in the table are averaged accuracy of five runs
for each method.
Results in the second column of Table 2 show that
our method outperforms all five contenders with the
accuracy of 82.00% on D1. This further highlights that
it imparts the performance increment of at least 2.50%
from the second-best method (AVGG [48]) and at least
40% accuracy from the worst method (Luz et al. [32]).
Similarly, on D2 in the third column of Table 2, we notice that our method outperforms all five methods with
an accuracy of 87.86%, which is at least 2.43% higher
than the second-best method (AVGG [48]) and at least
17% higher than the worst-performing method (nCOVnet [39]). In the fourth column of Table 2 on D3, we ob-

8

Chiranjibi Sitaula

, Sunil Aryal

Fig. 5 Average classification accuracy (%) achieved by our
method on D4 using deep features extracted from the five
pooling layers (p 1 to p 5) of the VGG16 model.

Fig. 6 Average classification accuracy (%) with different
cluster number on D4. Note that deep features from the 4th
pooling layer (p4 ) were used.

serve that our method, which yields 87.92% accuracy, is
superior to the second-best method (AVGG [48]) with
a slim margin of 0.43%, whereas it imparts over 20%
accuracy against the worst performing method (nCOVnet [39]). Last but not the least, in the fifth column of
Table 2 on D4, we notice that our method, which produces 83.22%, outperforms the DCF-BoVW [56] with
the margin of over 10% accuracy. Please note that for
D4, we only compare our method with DCF-BoVW
[56], which can work for a limited amount of data, only
and do not compare with other DL-based methods that
uses transfer learning because this dataset has a very
limited number of CXR images.
The comparison of our method against five different
recent DL-based methods on four datasets unveils that
our method provides a stable and prominent performance. This result further underscores that the classification performance of the bag of words approach, which
capture the more detailed spatial information of deteriorated regions more accurately than other methods,
seems more appropriate to CXR image analysis (e.g.,
COVID-19 CXR images) than other DL-based methods
using transfer learning approach.

4.5 Ablative study of cluster numbers
We analyze different number of unsupervised patterns
to be used in our experiments on D4. For this, we vary
the cluster numbers from 100 to 500 using the interval of
50 and present the results in Fig. 6. From the line graph,
we notice that the appropriate number of clusters that
produce the best result is k = 400.

4.6 Ablative study of class-wise performance
We study the average class-wise performance of our
method on D4. The average class-wise performance are
reported using precision, recall, and f1-score, which are
defined in Eqs. (2),(3), and (4), respectively.
Precision =

Recall =

TP
,
TP + FP

TP
,
TP + FN

(2)

(3)

4.4 Ablative study of pooling layers
In this subsection, we present the results of an ablative
study on D4, which is the smallest dataset, to analyze
the effect on the classification accuracy of using deep
features from the five different pooling layers of VGG16
in our method. The detailed results are presented in
Fig. 5. While observing the line graph, we notice that
the 4th pooling layer of the VGG16 model produces
highly separable features than other pooling layers on
the COVID-19 dataset.

F1-score =

2 × (Recall × Precision)
,
(Recall + Precision)

(4)

where T P , F P , and F N represent true positive, false
positive, and false negative results, respectively. We present
the average precision, recall, and f1-score in Table 3.
The results show the discriminability of our proposed
method in all four classes. It shows that our method can
distinguish the Covid and normal class well and there
is some confusion among two pneumonia classes.

New Bag of Deep Visual Words based Features to Classify Chest X-ray Images for COVID-19 diagnosis

9

American Statistician 46(3):175–185
2. Ayan E, Ünver HM (2019) Diagnosis of pneumonia
from chest x-ray images using deep learning. In:
Class
Precision
Recall (%)
F1-score
In Proc. Scientific Meeting on Electrical-Electronics
(%)
(%)
& Biomedical Engineering and Computer Science
Covid
100.00
97.20
98.40
(EBBT), pp 1–5
Normal
94.20
93.60
93.80
PneumoniaB
75.80
67.60
71.00
3. Bastola A, Sah R, Rodriguez-Morales AJ, Lal BK,
PneumoniaV
68.00
76.80
71.80
Jha R, Ojha HC, Shrestha B, Chu DK, Poon LL,
Costello A, et al. (2020) The first 2019 novel coronavirus case in nepal. The Lancet Infectious Diseases
5 Conclusion and future works
20(3):279–280
4. Breiman L (2001) Random forests. Machine learnIn this paper, we propose a new feature extraction method
ing 45(1):5–32
based on Bag of Deep Visual Words (BoDVW) to repre5. Chollet F (2017) Xception: Deep learning with
sent chest x-ray images. Empirical results on the clasdepthwise separable convolutions. In: Proc. IEEE
sification of chest x-ray images using the COVID-19
Conf. Comput. Vis. Pattern Recognit., pp 1251–
dataset show that our method is more appropriate to
1258
represent chest x-ray images. This is mainly because our
6. Chollet F, et al. (2015) Keras. https://github.
features can capture a few interesting regions (sparse
com/fchollet/keras
markers) indicating abnormalities well. Our features are
7. Chouhan V, Singh SK, Khamparia A, Gupta D,
extracted using a visual dictionary defined by the clusTiwari P, Moreira C, Damaševičius R, de Albutering of deep features from all training images. Therequerque VHC (2020) A novel transfer learning
fore, they can capture patterns in each training image
based approach for pneumonia detection in chest
and thus helps to capture potential markers for varx-ray images. Applied Sciences 10(2):559
ious lung infections such as COVID-19 and pneumo8. Cohen JP, Morrison P, Dao L (2020) Covid-19 imnia. Also, the size of our proposed features is relatively
age data collection. arXiv preprint arXiv:200311597
very small compared to other existing methods and our
9. Dalal N, Triggs B (2005) Histograms of oriented
method runs faster than other existing methods.
gradients for human detection. In: Proc. IEEE
Though the evaluation is done on a relatively small
Comput. Soc. Conf. Comput. Vis. Pattern Recogdataset, our method shows promising results to detect
nit. (CVPR), pp 886–893
and distinguish lung infection due to pneumonia and
10. Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei
COVID-19. COVID-19 being a relatively new disease
L (2009) ImageNet: a large-scale hierarchical imand there are not a lot of chest x-ray images available.
age database. In: Proc. IEEE Conf. Comput. Vis.
Nevertheless, given the current crisis with the COVIDPattern Recognit. (CVPR)
19 pandemic, our method which is accurate and fast
11. Giovanetti M, Benvenuto D, Angeletti S, Ciccozzi
can be very useful for health professionals for mass
M (2020) The first two cases of 2019-ncov in italy:
screening of people for COVID-19. Accurate detection
Where they come from? Journal of medical virology
and distinction of lung infections due to COVID-19 and
12. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B,
pneumonia are very important for COVID-19 diagnosis
Warde-Farley D, Ozair S, Courville A, Bengio Y
as people infected by these diseases show similar symp(2014) Generative adversarial nets. In: Proc. Adtoms.
vances in Neural Information Processing Systems,
In the future, it would be interesting to verify our
pp 2672–2680
results in a large study with more sample images includ13. Guo Y, Liu Y, Lao S, Bakker EM, Bai L, Lew MS
ing other types of lung infection such as tuberculosis.
(2018) Bag of surrogate parts feature for visual
Another potential direction is to investigate if a similar
recognition. IEEE Trans Multimedia 20(6):1525–
approach can be used to represent other types of medi1536
cal images such as CT scans, histopathological images,
14. He K, Zhang X, Ren S, Sun J (2016) Deep residcolonoscopy images, etc.
ual learning for image recognition. In: Proc. IEEE
Conf. on Computer Vision and Pattern Recognition
(CVPR), pp 770–778
References
15. Hearst MA (1998) Support vector machines. IEEE
Intelligent Systems 13(4):18–28
1. Altman NS (1992) An introduction to kernel and
nearest-neighbor nonparametric regression. The
Table 3 Average class-wise study (%) over five runs of our
method on D4 using precision, recall, and f1-score.

10

16. Holshue ML, DeBolt C, Lindquist S, Lofy KH,
Wiesman J, Bruce H, Spitters C, Ericson K, Wilkerson S, Tural A, et al. (2020) First case of 2019
novel coronavirus in the united states. New England Journal of Medicine
17. Huang G, Liu Z, Van Der Maaten L, Weinberger
KQ (2017) Densely connected convolutional networks. In: Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., pp 4700–4708
18. Islam SR, Maity SP, Ray AK, Mandal M (2019)
Automatic detection of pneumonia on compressed
sensing images using deep learning. In: In Proc.
Canadian Conference of Electrical and Computer
Engineering (CCECE), pp 1–4
19. Jin X, Han J (2010) K-Means Clustering, Springer
US, Boston, MA, pp 563–564
20. Kermany DS, Goldbaum M, Cai W, Valentim CC,
Liang H, Baxter SL, McKeown A, Yang G, Wu X,
Yan F, et al. (2018) Identifying medical diagnoses
and treatable diseases by image-based deep learning. Cell 172(5):1122–1131
21. Khan A, Shah J, Bhat M (2020) Coronet: A deep
neural network for detection and diagnosis of covid19 from chest x-ray images. Computer Methods and
Programs in Biomedicine 196:105581
22. Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep convolutional neural
networks. In: Proc. Adv. Neural Inf. Process. Syst.
(NIPS), pp 1097–1105
23. Kumar A, Singh SK, Saxena S, Lakshmanan K,
Sangaiah AK, Chauhan H, Shrivastava S, Singh
RK (2020) Deep feature learning for histopathological image classification of canine mammary tumors and human breast cancer. Information Sciences 508:405–421
24. Lai CC, Shih TP, Ko WC, Tang HJ, Hsueh PR
(2020) Severe acute respiratory syndrome coronavirus 2 (sars-cov-2) and corona virus disease-2019
(covid-19): the epidemic and the challenges. International Journal of Antimicrobial Agents p 105924
25. Latinne A, Hu B, Olival KJ, Zhu G, Zhang L, Li H,
Chmura AA, Field HE, Zambrana-Torrelio C, Epstein JH, Li B, Zhang W, Wang LF, Shi ZL, Daszak
P (2020) Origin and cross-species transmission of
bat coronaviruses in china. Nature Communications 11(4235), DOI 10.1038/s41467-020-17687-3
26. Lazebnik S, Schmid C, Ponce J (2006) Beyond bags
of features: Spatial pyramid matching for recognizing natural scene categories. In: Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., pp
2169–2178
27. Lewis DD (1998) Naive (bayes) at forty: The independence assumption in information retrieval. In:

Chiranjibi Sitaula

28.

29.

30.

31.

32.

33.

34.

35.

36.
37.

38.

39.

40.

, Sunil Aryal

Proc. European Conference on Machine Learning,
pp 4–15
Li C, Zhu G, Wu X, Wang Y (2018) False-positive
reduction on lung nodules detection in chest radiographs by ensemble of convolutional neural networks. IEEE Access 6:16060–16067
Li J, Li JJ, Xie X, Cai X, Huang J, Tian X, Zhu H
(2020) Game consumption and the 2019 novel coronavirus. The Lancet Infectious Diseases 20(3):275–
276
Loey M, Smarandache F, M Khalifa NE (2020)
Within the lack of chest covid-19 x-ray dataset: a
novel detection model based on gan and deep transfer learning. Symmetry 12(4):651
Lowe DG (2004) Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision 60(2):91–110
Luz E, Silva PL, Silva R, Moreira G (2020) Towards an efficient deep learning model for covid-19
patterns detection in x-ray images. arXiv preprint
arXiv:200405717
Maaten L, Hinton G (2008) Visualizing data using t-sne. Journal of machine learning research
9(Nov):2579–2605
Narin A, Kaya C, Pamuk Z (2020) Automatic detection of coronavirus disease (covid-19) using xray images and deep convolutional neural networks.
arXiv preprint arXiv:200310849
Nguyen TT, Abdelrazek M, Nguyen DT, Aryal S,
Nguyen DT, Khatami A (2020) Origin of novel
coronavirus (covid-19): A computational biology
study using artificial intelligence. bioRxiv DOI
10.1101/2020.05.12.091397
Oliva A (2005) Gist of the scene. In: Neurobiology
of Attention, pp 251–256
Oliva A, Torralba A (2001) Modeling the shape of
the scene: a holistic representation of the spatial
envelope. Int J Comput Vis 42(3):145–175
Ozturk T, Talo M, Yildirim EA, Baloglu UB,
Yildirim O, Acharya UR (2020) Automated detection of covid-19 cases using deep neural networks with x-ray images. Computers in Biology and
Medicine p 103792
Panwar H, Gupta P, Siddiqui MK, MoralesMenendez R, Singh V (2020) Application of deep
learning for fast detection of covid-19 in x-rays using ncovnet. Chaos, Solitons & Fractals p 109944
Pedregosa F, Varoquaux G, Gramfort A, Michel
V, Thirion B, Grisel O, Blondel M, Prettenhofer P,
Weiss R, Dubourg V, et al. (2011) Scikit-learn: Machine learning in python. Journal of Machine Learning Research 12:2825–2830

New Bag of Deep Visual Words based Features to Classify Chest X-ray Images for COVID-19 diagnosis

41. Redmon J, Farhadi A (2017) Yolo9000: better,
faster, stronger. In: Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. (CVPR), pp 7263–7271
42. Rossum G (1995) Python reference manual. Tech.
rep., Amsterdam, The Netherlands
43. Sasaki T, Kinoshita K, Kishida S, Hirata Y, Yamada S (2012) Ensemble learning in systems of
neural networks for detection of abnormal shadows
from x-ray images of lungs. Journal of Signal Processing 16(4):343–346
44. Sharfstein JM, Becker SJ, Mello MM (2020) Diagnostic testing for the novel coronavirus. Jama
45. Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:14091556
46. Singhal T (2020) A review of coronavirus disease2019 (covid-19). The Indian Journal of Pediatrics
pp 1–6
47. Sitaula C, Aryal S (2020) Fusion of whole and part
features for the classification of histopathological
image of breast tissue. Health Information Science
and Systems 8(1):1–12
48. Sitaula C, Hossain M (2020) Attention-based vgg16 model for covid-19 chest x-ray image classification. Applied Intelligence pp 1–14
49. Sitaula C, Aryal S, Xiang Y, Basnet A, Lu X (2020)
Content and context features for scene image representation. arXiv preprint arXiv:200603217
50. Sitaula C, Xiang Y, Aryal S, Lu X (2020) Scene image representation by foreground, background and
hybrid features. arXiv preprint arXiv:200603199
51. Sitaula C, Xiang Y, Basnet A, Aryal S, Lu X (2020)
Hdf: hybrid deep features for scene image representation. In: Proc. International Joint Conference on
Neural Networks (IJCNN), pp 1–8
52. Stephen O, Sain M, Maduh UJ, Jeong DU (2019)
An efficient deep learning approach to pneumonia
classification in healthcare. Journal of healthcare
engineering 2019
53. Szegedy C, Liu W, Jia Y, Sermanet P, Reed
S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A (2015) Going deeper with convolutions.
In: Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pp 1–9
54. Tan M, Le QV (2019) Efficientnet: Rethinking
model scaling for convolutional neural networks.
arXiv preprint arXiv:190511946
55. Varshni D, Thakral K, Agarwal L, Nijhawan R,
Mittal A (2019) Pneumonia detection using cnn
based feature extraction. In: In Proc. International
Conference on Electrical, Computer and Communication Technologies (ICECCT), pp 1–7

11

56. Wan J, Yilmaz A, Yan L (2018) Dcf-bow: Build
match graph using bag of deep convolutional features for structure from motion. IEEE Geoscience
and Remote Sensing Letters 15(12):1847–1851
57. Zhou B, Khosla A, Lapedriza A, Torralba A, Oliva
A (2016) Places: An image database for deep scene
understanding. arXiv preprint arXiv:161002055
58. Zhou ZH, Jiang Y, Yang YB, Chen SF (2002)
Lung cancer cell identification based on artificial
neural network ensembles. Artificial Intelligence in
Medicine 24(1):25–36

