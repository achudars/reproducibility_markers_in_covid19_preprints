Does transitioning to online classes mid-semester
affect conceptual understanding?

arXiv:2101.09908v1 [physics.ed-ph] 25 Jan 2021

Emanuela Carleschi, Anna Chrysostomou, Alan S. Cornell
Department of Physics, University of Johannesburg, PO Box 524, Auckland Park
2006, South Africa
E-mail: ecarleschi@uj.ac.za, annachrys97@gmail.com, acornell@uj.ac.za

Wade Naylor
Immanuel Lutheran College, PO Box 5025, Maroochydore 4558, Australia
E-mail: naylorw@immanuel.qld.edu.au
Abstract. The Force Concept Inventory (FCI) can be used as an assessment tool
to measure the gains in a cohort of students. In this study it was given to first
year mechanics students (N = 256 students) pre- and post-mechanics lectures, for
students at the University of Johannesburg. From these results we examine the effect
of switching mid-semester from traditional classes to online classes, as imposed by the
COVID-19 lockdown in South Africa. Overall gains and student perspectives indicate
no appreciable difference of gain, when bench-marked against previous studies using
this assessment tool. When compared with 2019 grades, the 2020 semester grades do
not appear to be greatly affected. Furthermore, initial statistical analyses also indicate
a gender difference in mean gains in favour of females at the 95% significance level (for
paired data, N = 48). A survey given to students also appeared to indicate that
most students were aware of their conceptual performance in physics, and the main
constraint to their studies was due to difficulties associated with being online. As such,
the change in pedagogy and the stresses of lockdown were found to not be suggestive
of a depreciation of FCI gains and grades.

Keywords: Physics Education; Large Cohort Courses; Online Teaching; Force Concept
Inventory; FCI.

Does transitioning to online classes mid-semester affect conceptual understanding?

2

1. Introduction
Many studies through the years have advocated for various changes to teaching pedagogy
away from the so-called traditional lecturing pedagogy, such as flipped classrooms, and
peer-assessment [1, 2, 3, 4, 5, 6], to name but a few. Many of these studies have
used assessment tools, such as the Force Concept Inventory (FCI) [7, 8, 9], to assess the
efficacy of these changes, where such drastic changes have been regarded over many years
of studies. As such, with the FCI now considered as a de facto standard for assessing
the conceptual basis of Newtonian mechanics, and a gain when comparing these test
results both before and after an introductory mechanics course of approximately 25%
[10, 11] as a benchmark for standard performance in a course’s approach, we can now
ask the question of what happens when the pedagogical approach is forcibly changed
during the semester, with no forewarning to either the lecturers or the students.
Whilst this study has been conducted for only one year worth of data, where as
mentioned above, usual analyses of such changes in pedagogy are conducted over years,
during the 2020 lockdown experienced in South Africa as a result of the COVID-19
pandemic, we were presented with a unique opportunity to test the resilience of this
assessment tool, as well as the effects such sudden changes could have on the usual gains
in a first year mechanics course. As such, our study was conducted with a very diverse
group of first year students enrolled in the University of Johannesburg (UJ), Faculty of
Engineering and the Built Environment (FEBE), whose demographics (average 2015 2019) range as follows: African 92.8%; White 3.8%; Indian 2.3%; Coloured 1.1% [12].
Yet for such a diverse background, the gains appear to have remained comparable to
the bench-marked studies of the last three decades.
Given the necessary switch to online platforms, such as Blackboard [13], we are
also able to study the engagement of students within this online learning environment
(through their attendance and marks on continuous assessments). This also includes
the time taken to complete various assessment tasks.
It may be considered somewhat surprising that even with the above-mentioned
change in pedagogy, we found no appreciable change in the FCI gain across this course
(see later comments). In seeking to break this down we shall look at a number of factors,
including the previously studied Gender Gap [14, 15, 16, 17, 18, 19, 20], where there
has been a resulting gender difference in favour of males in the student performance
on standardised assessments, such as the FCI in previous studies. However, our results
indicate this does not seem to be the case here. Another aspect at play here could
be the increased peer scaffolding (along the lines of Mazur’s peer evaluation [21])
as there had been an increased reliance on discussion groups with peers, due to the
COVID lockdown. This persisted into the second semester, as was highlighted by the
perspectives of the mechanics lecturers, where a range of discussion boards, interactive
tutorials, and WhatsApp groups were still used. As presented in Table 1, it can be seen
that the scores for the average semester 2 mark for both 2019 and 2020, which includes
a combination of coursework, practicals and exams, were not appreciably different. The

Does transitioning to online classes mid-semester affect conceptual understanding?

3

only noticeable difference was in the number of students who passed after the November
exam, or the course throughput, where explained in the table caption, in 2020 a slightly
lower semester mark was used to gain entrance to the final exam.
Given these motivations our paper shall be presented as follows: In section 2 we
shall detail the methodology of our study, followed by the analysis tools and techniques
used in this study in section 3, along with supporting appendices, and finally we shall
conclude in section 4.

no. of enrolled students (excluding cancellations)
average semester mark (%), all students
average semester mark (%), only students who qualified for exam
% students qualified for written exam
average exam mark (%)
average course mark (%), all students
average course mark (%), only students who qualified for exam
course throughput (%)

2019

2020

349
54
58
75
48
45
53
49

404
63
66
93
50
55
58
67.5

Table 1: Comparison of the 2019 and 2020 marks for engineering physics 1 in the second
semester. Note that: 1) the course throughput is calculated after the main exam only,
excluding the results of the supplementary exams; 2) In 2020 the entrance requirement
for the exam was lowered to 30% for the theory part of the course (instead of the usual
40% in 2019 and previous years) in light of the COVID-19 pandemic.

2. Methodology
The subjects for our testing were the 2020 first year cohort of engineering students at
the University of Johannesburg. The class consisted of approximately 400 students.
The course was initially taught as a traditional lecture based course, with a weekly
online assessment, fortnightly tutorials and fortnightly practicals (these being done in
person in groups of approximately 30 students with graduate students acting as tutors
and demonstrators of the practicals). The academic year had begun in early February
of 2020, where the pre-mechanics course FCI test was conducted in late February on
N = 256 students.†
The end result of the FCI is defined by the normalised gain G [10]:
G=
†

h%Sf i − h%Si i
,
100 − h%Si i

(1)

From a cohort of 400 students 256 students sat the FCI, where for pre- and post-test cases there
were 144 and 166 students, respectively. After removing several redundant attempts, there were
256 data subjects in total. Those who took both tests (paired) were 48 in total.

Does transitioning to online classes mid-semester affect conceptual understanding?

4

where %Sf and %Si are the final and initial scores respectively. We found that for this
cohort (for paired data) N = 48 the average gain was at 24%. We will comment further
on these results in Sec. 3.1.
First we should note that South Africa was placed in a hard lockdown in midMarch 2020, and teaching was switched within the period of a few weeks to a purely
online format. Lectures were replaced with recorded video content, and online platforms
for engagement with students were employed (such as consultations using BlackBoard
Collaborate Ultra, and WhatsApp discussion groups). As the easing of the lockdown
occurred towards the end of the mechanics course, a post-FCI test was possible to
administer to a smaller voluntary group of students, of which N = 48 had done the
pre-test.
Note that online teaching in a similar manner continued into the second semester
(where the physics syllabus covers electromagnetism and optics), where one of the
authors was a lecturer for this course. As such, continued performance could also be
assessed for these students over the entirety of their first year physics studies, as well as
conducting an online survey with these students for their perspectives of the course and
their feelings of success or failure. It should also be noted that the authors of this study
are actively taking part in the running, teaching and tutoring of the students. Hence,
we can also provide the perspectives of the lecturers of this course, and those of the
tutors involved, including the stresses of changing the teaching format mid-semester.
The methodology employed to unpack this collected data relied primarily on
standard statistical parameters, including the mean, standard deviation, percent
differences, p-values for the t-test difference of means, and correlations through R [22, 23]
and a spreadsheet.‡
Using our data from this cohort in Sec. 3 we shall investigate:
(i) student performance from pre-test to post-test, including an analysis of their
performance in the pre- and post-tests via a question breakdown, see Sec. 3.1,
(ii) the existence of a polarisation effect in 6 particular questions [25], see Sec. 3.2,
(iii) a possible gender difference in the FCI for paired data, see Sec. 3.3,
(iv) and a discussion of student and staff surveys, where as is usual we used a Likert
scale [26], see Sec. 3.4.
3. Results & Analyses
3.1. Question Break Down, Means and Gains
In this section we first present some analyses of the distribution of the scores for students
in the pre- and post-tests (N = 256), starting in the left panel of Fig. 1 (where the right
‡

Interested readers who wish to familiarise themselves with the basics of statistical analysis,
including the t-test, correlation, ANOVA, and measures of variation (e.g. standard deviation
and standard error of the mean), among others, may want to consult Refs. [22, 24].

Does transitioning to online classes mid-semester affect conceptual understanding?
0

10

20

30

40

50

60

30

70

80

90

100

pre-test marks (ALL)

20

20

15

15

10

10

5

No of students

25

5

0

0
0

10

20

30

40

50

60

70

80

90

10

20

30

10

20

30

40

50

60

30

70

80

90

7

6

6

5

5

4

4

3

3

2

2

1

1
0
10

20

30

40

50

10

10

5

5

0

60

70

80

90

100

80

90

20

30

40

50

60

70

80

90

100
11

post-test marks (PAIRED)

10

9

9

8

8

7

7

6

6

5

5

4

4

3

3

2

2

1

1

0

0
70

10

10

No of students

No of students

10

8

11

30

15

60

11

7

0

15

50

100

9

100

20

40

90

8

0

20

30

80

0

25

20

70

Percentage (%)

25

10

60

9

100

post-test marks (ALL)

0

50

pre-test marks (PAIRED)

Percentage (%)
0

40

10

25

No of students

0
11

30

5

0
0

100

10

20

30

40

50

60

70

80

90

100

Percentage (%)

Percentage (%)

Figure 1: A frequency histogram for the pre- and post-test data in total (N = 256)
and for the paired data (N = 48).

pre-test ALL
pre-test PAIRED
post-test ALL
post-test PAIRED

Mean

SD

Min.

Max.

34.3
34.7
44.1
50.8

15.2
17.2
22.8
22.4

3
3
0
10

80
80
100
100

Table 2: Mean, standard deviation (SD), minimum and maximum % marks as
displayed in Fig. 1. The paired data consisted of a subset (N = 48) of the N = 256
who sat either the pre- or post test.

panel is for the paired data, N = 48). The difference in distributions (pre- compared to
post-test) have well defined shifts indicating a gain, particularly for the paired data. In
Table 2 the results for the pre- and post-test scores can be seen. The gain of G = 0.24
was calculated from the paired data and is the expected gain for a standard physics
course [10]. The difference in mean scores was checked via the paired samples t-test
(N = 48) and was not due to random fluctuations at the 95% confidence level (p-value
= 0.000002614, two-tailed), implying we found a statistically significant difference in
the means (pre- and post-test).§
§

The means for the pre- or post-tests groups used an independent samples t-test and was found to
be not due to random fluctuations at the 95% confidence level; p-value = 0.00001182, two-tailed.

Does transitioning to online classes mid-semester affect conceptual understanding?

Post-test marks (%)

0

5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

85

100

100

90

90

80

80

70

70

60

60

50

50

40

40

30

30

20

Best linear fit: PostTest = 27.4 + 0.66*PreTest

10

r = 0.504
0

5

10

15

20

25

30

35

40

45

50

55

60

65

70

75

80

6

20
10

85

Pre-test marks (%)

Figure 2: Moderate positive correlation between pre- and post-tests, for paired data
N = 48.

This relationship between pre- and post-test scores can also be seen in Fig. 2,
where the correlation in was found to be moderate and positive. It should be noted
that the gain from the pre-test mean, as compared to the post-test mean, is not used to
determine the gains [10], although we have performed a question by question breakdown.
In terms of numbers, Pearson’s correlation coefficient, in Fig. 2, gives a moderate
positive correlation of r = 0.504 with p-value = 0.000261 < α at the 95% significance
level (α < 0.05).
As student ideas may not be clearly understood, nor how much they have learnt
according to their own perspectives or from interviews, we can identify them through
analyses of question by question responses. Fig. 3 indicates the percentages of students
who correctly answered each question in the pre- and post-tests, with the differences
also shown (diagonal hatching/blue). A similar schematic is shown for the paired data
(see bottom panel Fig. 3) for comparison. We note here the presence of negative
gains for certain questions, which are indicative of poorer performance in the post-test.
This “loss” is especially pronounced in Fig. 3, top panel, for questions 14, 20, 22, and
24; this also arises in the paired data of Fig. 3 for question 12, bottom panel; the
negligible gains in question 21 for the former and questions 14 and 24 for the latter are
also worth mentioning. The concepts assessed by these questions are standard topics
such as projectile motion (questions 12 and 14), as well as kinematics and Newton’s
second law (questions 20-24). However, these questions possibly exploit scenarios with
which students are unlikely to have had personal experience with (i.e. objects fired from
cannons and motion in deep space) and visual tools like ticker-tapes and displacementtime graphs. In doing so, they ensure that students respond based on intuition gained

Does transitioning to online classes mid-semester affect conceptual understanding?
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

80

18

19

20

21

22

23

24

25

26

27

28

29

30
80

% correct answer, post-test (all students)
% correct answer, pre-test (all students)
difference
questions with majority right answer, post-test (all students)
questions with majority right answer, pre-test (all students)

70

Percentage (%)

17

7

70

60

60

50

50

40

40

30

30

20

20

10

10

0

0

-10

-10

-20

-20
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

18

19

20

21

22

23

24

25

26

27

28

29

30

Question number
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

80
70
60

Percentage (%)

80

% correct answer, post-test (paired students)
% correct answer, pre-test (paired students)
difference
questions with majority right answer, post-test
(paired students)
questions with majority right answer, pre-test
(paired students)

70
60

50

50

40

40

30

30

20

20

10

10

0

0

-10

-10

-20

-20
1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

Question number

Figure 3: Top: Correct answer analysis for all the students (N = 256) who wrote preand post-tests. Bottom: Correct answer analysis for all the paired students (N = 48)
who wrote pre- and post-tests.

from their mechanics course rather than empirical evidence gathered from daily life.
Finally, poorer post-test performance in these more conceptual questions may
demonstrate that students are not confident in their ability to apply their knowledge to
unfamiliar scenarios. This may be a consequence of superficial learning or dependence on
preconceived ideas rather than physics. The presence or development of misconceptions
may also have come into play. Additionally, we note that these questions might indicate
an issue with language ability, a concern shared by other studies conducted in regions
where English is not the first language of most students [25, 27].

Does transitioning to online classes mid-semester affect conceptual understanding?

8

3.2. Polarising Questions
Another way to investigate individual questions has recently been performed in Refs.
[20, 25, 28], where a breakdown of the type of response to individual questions can lead
to a polarisation of a correct answer and one predominantly incorrect answer.¶ In Figs.
4 and 5 we can see the effect of the polarising questions: 5, 11, 13, 18, 29 and 30 in the
FCI, e.g., see Ref. [25]. A similar pattern emerges for the cohort at UJ, where there
clearly appears to be a subset where asides from the correct answer there is another
polarising choice, and apart from Q18 in the “paired” data, Fig. 5, we find the same
dominant incorrect (polarised) response [25].
It may be that certain misconceptions drive this polarisation. For example, consider
question 5, where the dominant answer of C can be read from the pre-test data of Figs.
4 and 5. This answer claims that the motion of a ball is driven by gravity as well as
“a force in the direction of motion”, which indicates the common misconception that
motion requires an active force. The presence of a force in the direction of motion is
favoured also in answers 11C, 13C, and 18D, as well as implied in 30E. Though there
is a general decrease from pre- to post-test in the selection of these erroneous answers,
the post-test data of Figs. 4 and 5 suggest that these misconceptions can be difficult to
alleviate, as we have found at UJ.
Such observations connect well to the work of Bani-Salameh [30] and others; we
shall interrogate these ideas in greater depth in a future work, see Sec. 4. It can certainly
be inferred that there are subsets of incorrect answers where misconceptions in students’
understanding consistently leads to the same kind of wrong answer [25].
Group

Pre-test(%)
Post-test(%)
Gain(%)

Female
n = 16

Male
n = 32

Difference

Mean (SD)
31.0 (12.3)
56.9 (25.2)
38.0

Mean (SD)
37.0 (19.4)
47.6 (20.5)
17.3

(Male-Female)
6.0
-9.3
-20.7

Table 3: Paired FCI results (in percent) for female and male participants (N = 48).

3.3. Possible Gender Gains
In this section we now analyse the differences between the male and female participants
who sat for both pre- and post-tests (paired). In Table 3 the means for female and
male participants are presented and it clearly appears that female participants have
performed better in the FCI as compared to male participants. Interestingly, and it has
¶

See Refs. [29, 27, 30, 31] for a discussion of other ways to analyse and interpret individual question
responses in the FCI.

Does transitioning to online classes mid-semester affect conceptual understanding?

9

Figure 4: Distribution of student answers for Questions 5, 11, 13, 18, 29 and 30, pre(red) and post-(blue), for all N = 256 students. NA stands for “no answer”, while the
five options are labelled from A to E.

been found by Alinea & Naylor [20], the table shows that although male participants
did better in the pre-test, female participants had a higher average in the post-test.
To further try and verify these results, where due to the average number of
participants in each group being 24, we have in Appendix A performed multiple
statistical tests to confirm if this difference in means is statistically significant. From this
we have found that at the 95% significance level we can neglect the null hypothesis. We
emphasize that besides parametric tests for normal distributions we have also performed
a non-parametric Wilcoxon test that led to a statistical difference in the medians, see
App. A.
In comparison to Fig. 2, in Fig. 6 the correlation for female and male groups was
found to be mild and positive with: rF = 0.417; p-value = 0.05418 and rM = 0.662; pvalue= 0.00001816, respectively. Clearly, there is a more reliable correlation for the male
cohort with p-value < 0.05. Finally, this can be compared to the combined correlation
(independent of gender, N = 48) where r = 0.504; p-value= 0.02757 in Fig. 2.
The reason for the apparently higher gain for the female part of the cohort might be
due to, as found by Sadler and Tai [32] (also see Adams et al [33]), professor-to-gender
matching to student gender was second only to the quality of high-school physics course
in predicting students’ performance in college physics. It may be worth mentioning that
at UJ, during the 2020 academic year, a female instructor was the senior academic for
the mechanics course. As we discuss in Sec. 4, we will leave these preliminary results

Does transitioning to online classes mid-semester affect conceptual understanding? 10

Figure 5: Paired distribution of student answers for questions 5, 11, 13, 18, 29 and 30,
pre-(red) and post-(blue), tests for the paired group (N = 48). The same conventions
are used as in Fig. 4.
100

Gender
Female
Male

Post-test (%)

80

60

40

20

20

40

60

Pre-test (%)

Figure 6: Correlations for combined scores in terms of gender.

for a follow-up work.

80

Does transitioning to online classes mid-semester affect conceptual understanding? 11
3.4. Staff & Student Surveys
In this section we briefly remark on student and staff surveys conducted at UJ in 2020
after the sitting of the FCI post-test. In this survey we used a Likert scale [26] for
most of the data collection. From Adams et al. [33] - interviews revealed that the use
of a five-point scale in the survey - as opposed to a three-point scale - was important.
Students expressed that agree vs strongly agree (and disagree vs strongly disagree) was
an important distinction and that without the two levels of agree and disagree they would
have chosen neutral more often.
One month after the FCI post-test was administered, students were asked to
complete a feedback form detailing their experience with the FCI assessment tool. This
survey was primarily designed to gauge student perspectives compared with the FCI test
mark obtained (student numbers served as the only identifier to preserve anonymity).
As the survey was not compulsory (and this was during a hard lockdown) a rather small
sample (22) of the 256 students opted to take the survey. Of these student respondents,
only 8 had taken either the pre- or post-test, while the other 14 had written both.
Already a pattern could be seen in terms of students that sat both tests, “paired”,
compared with those who sat only the pre- or post-test.
If we look at the average mark obtained by the students who sat both tests, which
was 10.80 for the pre-test and 12.57 for the post-test (out of 30), then when asked to
rate their performance, most chose 3 (x̄ = 2.67) and 2 (x̄ = 2.14) on the Likert scale
for the pre- and post-tests, respectively. Only one response exceeded 3: a student who
had received 40% for the pre-test ranked their performance as 4. These results suggest
that, for the most part, students are aware of their shortfalls and do not overestimate
their ability.
Three survey questions directly inquired about the students’ experience regarding
the shift to online learning. The first, asking if students felt they engaged with
lecturers/tutors/classmates more through digital platforms than through standard
classes, was met with a mixed response: most selected 3 (x̄ = 2.91), but of these,
∼ 18% chose “strongly agree” while ∼ 23% chose “strongly disagree”. Similarly, ∼ 18%
claimed they “strongly agreed” that their performance was better thanks to online
learning, although the average response was “disagree”. However, the final such question
offered the most clarity: the average response to whether working from home challenged
academic performance was 3.59; most answered “agree”, while ∼ 18% selected “strongly
agree”. Given the South African context in general, and the UJ context specifically
(where typically one third of the enrolled students are from Quintiles 1 and 2 schoolsk,
see page 76 in Ref. [35], indicating a diffuse level of poverty), and that data is among
the most expensive in the world [36], the difficulties of online learning can be especially
pronounced.
k

Public schools in South Africa are classified in so-called Quintiles, from 1 to 5. Quintile 1 schools
include the poorest 20% schools, while Quintile 2 schools are the next poorest 20% of schools, and
so on. Government funding is dispensed to public schools according to their Quintile classification,
with the aim of redressing poverty and inequality in education, see Ref. [34].

Does transitioning to online classes mid-semester affect conceptual understanding? 12
4. Concluding remarks
In this article we have used the Force Concept Inventory (FCI) to look at the conceptual
understanding of a large cohort of physics/engineering students at the University of
Johannesburg (UJ) during the 2020 academic year. Mid-semester UJ went into lockdown and students then switched from a traditional lecture format to online platforms,
yet this led to the very informative scenario where we have found no overall drop in
gains (G = 0.24). This is reminiscent of what happened with regards to the Christchurch
Earthquakes (2010-2011) which led to the closure of various high schools, where although
a minority had negatives impacts there were were many positives [37].∗∗ This was
further established through the comparison of 2019 and 2020 semester marks at UJ (see
Table 1) where we found no appreciable drop in marks.
In Sec. 3.2 we looked at a subset of questions where a polarisation of choices
occurred, in that either the correct answer or one main incorrect answer dominated
the post-test responses [25]. We found similar patterns for the students at UJ, which
followed a very similar pattern to the data found in Refs. [25, 20, 28]. The importance
of these questions relates to the fact that they ask the student to be able to understand
certain particular concepts in physics, such as circular motion and motion requiring a
force.
In Sec. 3.3 we looked at a possible out-performance of female students on the overall
gain in the FCI. As was also found by Alinea & Naylor [20], although the male group
started with a higher average pre-test score, their gain was less. As mentioned earlier,
the main course lecturer was female, which may lead to professor gender matching
in this cohort [32]. Although we rigorously checked that the difference in means was
statistically significant (at the 95% confidence level, see App. A) we will report on a
larger cohort inclusive of 2021 in a forthcoming work.
The article has raised some questions, such as why the general performance for the
group of paired students was higher than those who took either of the pre- or post-tests.
This is often due to the fact that students who are diligent are more likely to take both
pre- and post-tests, and this can be seen from the fact that this group of students was
also more likely to take the survey as well. Usually, overall gains are taken only from
paired data, which is then used to compare to other cohorts and institutions. However,
the question of using “unpaired” pre- and/or post-test data sets in some form has not
really been investigated in the literature (see however, Ref. [27]) and we intend to
comment more on this issue in future work.
As for other possible directions to investigate, besides extending the FCI to further
years, which also appears to be disrupted by more COVID lock-downs, we intend to
look at matriculation results in order to establish correlations between the FCI and
high school exit grades in physics, maths and English scores. In the case of UJ, the
first language of the students enrolled in the UJ FEBE (average 2015-2019): English
14.3%; Isixhosa 5.9%; African 1.5%; Other 78.3% [12]. This relates to comments by
∗∗

In terms of physics performance during COVID see Refs. [38, 39, 40].

Does transitioning to online classes mid-semester affect conceptual understanding? 13
Bani-Salameh [27], and also work performed by Alinea & Naylor [25], in relation to
performance on the English version of the FCI, where English is not the student’s first
language necessarily. It might also be interesting to look at correlations between the
FCI and cognitive reflective tests [28].
Acknowledgements
EC and ASC are supported in part by the National Research Foundation of South
Africa (NRF). AC is grateful for the support of the National Institute for Theoretical
Physics (NITheP), South Africa. We would like to thank all staff and students who took
part in this study. WN would like to thank useful discussions with Margaret Marshman,
University of the Sunshine Coast (USC). The authors are also grateful to Allan L. Alinea
(University of the Philippines) for his useful comments.
A. Statistical Analyses of Differences in Gender Means
In this appendix we look at the differences in gender means for paired data (N = 48,
comprising of 16 female participants and 32 male participants). We found that the
mean of the gains for female participants (µFG = 0.38) was greater than the mean for
male participants (µM
G = 0.17). However, to clarify if the difference is purely a random
fluctuation, we performed the following tests, using R [23], at the 95% significance level
(α < 0.05):
(a) The t-test for independent samples (and unequal variances) had a p-value:
0.02757 < α for a single tail (directional difference µFG − µM
G > 0).
(b) A two-way ANOVA with replication led to an F -statistic: 4.5107 and p-value:
0.03909 < α.
(c) A linear regression analysis also led to an F -statistic: 4.511 and p-value: 0.03909 <
α.
(d) A non-parametric two-sample Wilcoxon test led to medians of 0.2496296 and
0.1602564 for female participants and male participants, respectively, with a W statistic of W = 341 and p-value = 0.03223 < α.
It may be worth mentioning that two-way ANOVA with replication was unbalanced,
as the two group sizes were different (16 and 32, respectively). However, we were able
to double-check the results obtained by converting gender to a dichotomous variable
(Female= 1, Male= 0) and used a linear regression. At the 95% significance level (or
α < 0.05) we can reject the null-hypothesis whenever the p-value < 0.05.
In conjunction with the t-test, a two-way ANOVA and a linear regression analysis
both agree at the 95% significance level, and suggest that there is a statistically
significant difference between the means of female participants and male participants.
This was further confirmed in item (d), where we performed a non-parametric test (using
medians) and found the critical value to be p = 0.03223 < α. These findings indicate

Does transitioning to online classes mid-semester affect conceptual understanding? 14
a real difference in gender (for this group) with female participants having better gains
than male participants, even though male participants started with a higher average
pre-test score.
References
[1] P. Heller, R. Keith, and S. Anderson, “Teaching problem solving through cooperative grouping.
part 1: Group versus individual problem solving,” American Journal of Physics, vol. 60, no. 7,
pp. 627–636, 1992.
[2] A. P. Fagen, C. H. Crouch, and E. Mazur, “Peer instruction: Results from a range of classrooms,”
The Physics Teacher, vol. 40, no. 4, pp. 206–209, 2002.
[3] V. Cahyadi, “The effect of interactive engagement teaching on student understanding of
introductory physics at the faculty of engineering, university of surabaya, indonesia,” Higher
Education Research & Development, vol. 23, no. 4, pp. 455–464, 2004.
[4] M. K. Smith, W. B. Wood, W. K. Adams, C. Wieman, J. K. Knight, N. Guild, and T. T. Su,
“Why peer discussion improves student performance on in-class concept questions,” Science,
vol. 323, no. 5910, pp. 122–124, 2009.
[5] J. Watkins and E. Mazur, “Retaining students in science, technology, engineering, and mathematics
(stem) majors,” J. Coll. Sci. Teach., vol. 42, p. 36–41, 2013.
[6] S. Freeman, S. L. Eddy, M. McDonough, M. K. Smith, N. Okoroafor, H. Jordt, and M. P.
Wenderoth, “Active learning increases student performance in science, engineering, and
mathematics,” Proceedings of the National Academy of Sciences, vol. 111, no. 23, pp. 8410–
8415, 2014.
[7] H. I. A. and H. D., “The initial knowledge state of college physics students,” American Journal of
Physics, vol. 53, p. 1043, 1985.
[8] D. Hestenes, M. Wells, and G. Swackhamer, “Force concept inventory,” The Physics Teacher,
vol. 30, no. 3, pp. 141–158, 1992.
[9] H. D, “Who needs physics education research!?,” American Journal of Physics, vol. 66, p. 465,
1998.
[10] H. RR, “Interactive-engagement versus traditional methods: A six-thousand-student survey of
mechanics test data for introductory physics courses,” American Journal of Physics, vol. 66,
p. 64, 1998.
[11] V. P. Coletta and J. A. Phillips, “Interpreting fci scores: Normalized gain, preinstruction scores,
and scientific reasoning ability,” American Journal of Physics, vol. 73, no. 12, pp. 1172–1182,
2005.
[12] U. FEBE, “University of johannesburg faculty of engineering and the built environment 2019
annual report,” 2020. https://www.uj.ac.za/faculties/febe/Pages/annual-report.aspx.
[13] Blackboard, “Blackboard collaborate,” 2016. http://www.blackboard.com/online-collaborativelearning/blackboard-collaborate.aspx.
[14] J. Docktor and K. Heller, “Gender differences in both force concept inventory and introductory
physics performance,” AIP Conference Proceedings, vol. 1064, no. 1, pp. 15–18, 2008.
[15] H. M. Glasser and I. John P. Smith, “On the vague meaning of “gender” in education research:
The problem, its sources, and recommendations for practice,” Educational Researcher, vol. 37,
no. 6, pp. 343–350, 2008.
[16] A. Madsen, S. B. McKagan, and E. C. Sayre, “Gender gap on concept inventories in physics: What
is consistent, what is inconsistent, and what factors influence the gap?,” Phys. Rev. ST Phys.
Educ. Res., vol. 9, p. 020121, Nov 2013.
[17] S. Bates, R. Donnelly, C. Macphee, D. Sands, M. Birch, and N. Walet, “Gender differences
in conceptual understanding of newtonian mechanics: A uk cross-institution comparison,”
European Journal of Physics, vol. 34, no. 2, pp. 421–434, 2013.

Does transitioning to online classes mid-semester affect conceptual understanding? 15
[18] V. Coletta, “Reducing the fci gender gap,” in Physics Education Research Conference 2013, PER
Conference, (Portland, OR), pp. 101–104, July 17-18 2013.
[19] J. R. Shapiro and A. Williams, “The role of stereotype threats in undermining girls’ and women’s
performance and interest in stem fields,” Sex Roles, vol. 66, pp. 175–183, 2012.
[20] A. L. Alinea and W. Naylor, “Gender gap and polarisation of physics on global courses,” Physics
Education, IAPT, vol. 33, 2017.
[21] E. Mazur, Peer Instruction: A User’s Manual. Series in Educational Innovation, Prentice Hall,
1997.
[22] M. J. Crawley, Statistics: An Introduction Using R (2nd ed.). Wiley, West Sussex UK, 2014.
[23] R Core Team, R: A Language and Environment for Statistical Computing. R Foundation for
Statistical Computing, Vienna, Austria, 2013.
[24] T. Urdan, Statistics in Plain English (3rd ed.). Routledge, 2010.
[25] A. L. Alinea and W. Naylor, “Polarization of physics on global courses,” Physics Education, vol. 50,
pp. 210–217, 2015.
[26] R. Likert, “A method for measuring the sales influence of a radio program,” Journal of Applied
Psychology, vol. 20, pp. 175–182, 1936.
[27] H. N. Bani-Salameh, “How persistent are the misconceptions about force and motion held by
college students?,” Physics Education, vol. 52, p. 014003, dec 2016.
[28] A. L. Alinea, “Cognitive reflection test and the polarizing force-identification questions in the
FCI,” European Journal of Physics, vol. 41, p. 065707, oct 2020.
[29] T. Martı́n-Blas, L. Seidel, and A. Serrano-Fernández, “Enhancing force concept inventory
diagnostics to identify dominant misconceptions in first-year engineering physics,” European
Journal of Engineering Education, vol. 35, no. 6, pp. 597–606, 2010.
[30] H. N. Bani-Salameh, “Using the method of dominant incorrect answers with the FCI test
to diagnose misconceptions held by first year college students,” Physics Education, vol. 52,
p. 015006, nov 2016.
[31] J.-i. Yasuda, N. Mae, M. M. Hull, and M.-a. Taniguchi, “Analyzing false positives of four questions
in the force concept inventory,” Phys. Rev. Phys. Educ. Res., vol. 14, p. 010112, Mar 2018.
[32] P. M. Sadler and R. H. Tai, “Success in introductory college physics: The role of high school
preparation,” Science Education, vol. 85, no. 2, pp. 111–136, 2001.
[33] K. Adams, S. Hean, P. Sturgis, and J. M. Clark, “Investigating the factors influencing professional
identity of first-year health and social care students,” Learning in Health and Social Care, vol. 5,
no. 2, pp. 55–68, 2006.
[34] H. van Dyk and C. White, “Theory and practice of the quintile ranking of schools in south africa:
A financial management perspective,” South African Journal of Education, vol. 39, p. 1820,
2019.
[35] UJ,
“University
of
johannesburg
2018
stakeholder
report,”
2018.
https://www.uj.ac.za/about/Documents/UJ%20Stakeholder%20Report%202018%20FINAL%20(4).pdf.
[36] B. I. S. A. Edward-John Bottomley, “Sa has some of africa’s most expensive data, a new report
says – but it is better for the richer,” May 05, 2020. https://www.businessinsider.co.za/howsas-data-prices-compare-with-the-rest-of-the-world-2020-5.
[37] B. Beaglehole, C. Bell, C. Frampton, and S. Moor, “The impact of the canterbury earthquakes on
successful school leaving for adolescents,” Australian and New Zealand Journal of Public Health,
vol. 41, no. 1, pp. 70–73, 2017.
[38] J. D. White, “Teaching general physics in a COVID-19 environment: insights from taiwan,” Physics
Education, vol. 55, p. 065027, oct 2020.
[39] P. Klein, I. Lana, M. N. Dahlkemper, K. Jeličić, M.-A. Geyer, S. Küchemann, and A. Susac,
“Studying physics during the covid-19 pandemic: Student assessments of learning achievement,
perceived effectiveness of online recitations, and online laboratories,” arXiv, 2020.
[40] M. F. J. Fox, A. Werth, J. R. Hoehn, and H. J. Lewandowski, “Teaching labs during a pandemic:
Lessons from spring 2020 and an outlook for the future,” arXiv, 2020.

