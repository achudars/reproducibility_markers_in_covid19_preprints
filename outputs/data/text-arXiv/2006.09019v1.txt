Lio - A Personal Robot Assistant for Human-Robot Interaction and
Care Applications

arXiv:2006.09019v1 [cs.RO] 16 Jun 2020

Justinas Mišeikis, Pietro Caroni, Patricia Duchamp, Alina Gasser, Rastislav Marko, Nelija Mišeikienė,
Frederik Zwilling, Charles de Castelbajac, Lucas Eicher, Michael Früh, Hansruedi Früh
Abstract— Lio is a mobile robot platform with a multifunctional arm explicitly designed for human-robot interaction
and personal care assistant tasks. The robot has already
been deployed in several health care facilities, where it is
functioning autonomously, assisting staff and patients on an
everyday basis. Lio is intrinsically safe by having full coverage
in soft artificial-leather material as well as having collision
detection, limited speed and forces. Furthermore, the robot has
a compliant motion controller. A combination of visual, audio,
laser, ultrasound and mechanical sensors are used for safe
navigation and environment understanding. The ROS-enabled
setup allows researchers to access raw sensor data as well as
have direct control of the robot. The friendly appearance of
Lio has resulted in the robot being well accepted by health
care staff and patients. Fully autonomous operation is made
possible by a flexible decision engine, autonomous navigation
and automatic recharging. Combined with time-scheduled task
triggers, this allows Lio to operate throughout the day, with a
battery life of up to 8 hours and recharging during idle times.
A combination of powerful on-board computing units provides
enough processing power to deploy artificial intelligence and
deep learning-based solutions on-board the robot without the
need to send any sensitive data to cloud services, guaranteeing
compliance with privacy requirements. During the COVID-19
pandemic, Lio was rapidly adjusted to perform additional functionality like disinfection and remote elevated body temperature
detection. It complies with ISO13482 - Safety requirements for
personal care robots, meaning it can be directly tested and
deployed in care facilities.

I. INTRODUCTION
Recently robots have been gaining popularity outside the
factory floors and entering unstructured environments such
as homes, shops and hospitals. They range from small
devices designed for internet-of-things (IoT) applications to
larger physical robots which are capable of autonomously
navigating in indoor and outdoor environments, sharing the
workspace with people and even interacting with them.
Given the issue of ageing population and shortage of
medical and nursing staff in many countries, this naturally
leads to attempts to use robotics and automation addressing
this problem [1] [2]. For example, in Switzerland, the number
of people over 80 years of age is expected to double from
2015 to 2040 [3]. It will result in nearly triple nursing costs
for the Swiss healthcare sector.
Furthermore, healthcare employees are experiencing severe working conditions due to stress, underpayment and
overtime. For example, Between 8% and 38% of health
workers suffer physical violence at some point in their
Authors

are

with

F&P

Robotics

AG,

Zürich,

Switzerland,

info@fp-robotics.com, www.fp-robotics.com

careers [4]. A possible staff shortage of 500’000 healthcare
employees is estimated in Europe by the year of 2030 [5].
Care robotics is not an entirely new field. There has
been significant development in this direction. One of the
most known robots is Pepper by SoftBank Robotics, which
was created for interaction and entertainment tasks. It is
capable of voice interactions with humans, face and mood
recognition. In the healthcare sector Pepper is used for
interaction with dementia patients [6].
Another example is the robot RIBA by RIKEN. It is
designed to carry around patients. The robot is capable of
localising a voice source and lifting patients weighing up to
80 kg using remote control [7].
The Sanbot Elf robot by QIHAN has numerous sensors
allowing it to monitor the health condition of patients and
residents. A recent study with Sanbot Elf even tested its
capability to detect fallen elderly residents [8]. However, the
robot is not capable of handling and manipulating objects.
Another robot is WALKER by UBTECH. It can manipulate
objects, climb stairs and do yoga [9].
The COVID-19 crisis has given an increase in robots
applicable to the health care sector. Most of them are not able
to manipulate objects, they are optimised for autonomous
driving and delivery of objects. Robots like Peanut from
Keenon Robotics Co., the SAITE Robot or the NOAH Robot
have been deployed to assist with it.
The Care-o-bot 4 by Unity Robotics and Fraunhofer IPA is
used in four shopping centres. It can recognise faces, provide
daily news and handle objects [10]. Despite the initial focus
on the healthcare market, the existing applications are limited
to pilot projects only.
One more healthcare oriented robot is Moxi by Diligent
Robotics. Compared to previously described robots, the focus
of Moxi is not interacting with people, but rather lies in the
logistic of hospitals. The robot can deliver medical samples,
carry laundry, bring supplies and greet patients [11]. It uses a
mobile platform, arm, gripper and an integrated lift to adjust
its height. Moxi is mostly capable of completing the tasks in
an end-to-end manner. Door opening and closing procedures
are not in the range of its capabilities, it relies on automatic
door systems or places the packages outside the room.
Furthermore, the robots of PAL Robotics, primarily the
REEM-C and TIAGo robots, could be used in the healthcare
sector. They are developed with research and industry in
mind but no specific application. Open platforms allow
users to freely explore possibilities, such as gesture recognition [12].

6 DoF Robotic Arm with Soft
Skin out of artiﬁcial leather

Microphone
Holders for placing items
and custom attachments

LED Strip for direction indication
Ultrasonic sensors
LiDAR scanners
Floor sensors

Soft Fingers with sensors
to grasp objects

Camera for person- and
object recognition
Embedded
Computing Units

1630

Display to show the
state and information

Fisheye camera
Speakers

740

Intel RealSense Cameras

790
Fig. 1.

Overview of Lio: sensor and hardware setup.

In the market of social robots, there are more platforms
focused on interaction without manipulation capabilities such
as PARO, KIKI, AIBO, Buddy, Kuri, Mykie and Cozmo. While
such robots often achieve their goal of creating positive
emotions, or even therapeutic progress, they are incapable
of assisting or solving tasks.
In this paper, a personal robot platform - Lio, shown
in Fig. 1, is presented. The robot is specifically designed
for autonomous operation in health care facilities and home
care by taking into account the limitations of other robot
platforms and improving upon it. Lio is intrinsically safe
for human-robot interaction. It is the next iteration robot
platform built upon the experience of creating a P-Care
robot, which was a well-accepted personal robot developed
for Chinese markets [13].
First of all, the system is described from the hardware
point of view, explaining the functionality and capabilities of
the robot. Then software, interfaces and existing algorithms
are presented followed by the challenges of operating a robot
in hospital environments. Eventually, existing use cases and
deployments of Lio are presented incorporating evaluation
and current limitations of the system. The paper is concluded
with an overview of Lio in the context of personal care and
research applications followed by current developments and
future work.
II. SYSTEM DESCRIPTION
Lio, a personal assistant robot for care applications can
complete a wide variety of complex tasks. It can handle
and manipulate objects for applications in health care institutions and home environments. Furthermore, it operates
autonomously in an existing environment without requiring
significant adaptations for its deployment. The design of Lio
evolved as an iterative process. During the deployments,
observations and interactions with staff and patients were
taken into account for further development.
A set of heterogeneous sensors are embedded in the robot.
It provides Lio with the capabilities of navigating, understanding and interacting with the environment and people.
A combination of in-house developed robot control and
programming software myP together with algorithms based
on the Robot Operating System (ROS) introduces a nodebased system functionality. It enables the communication

Fig. 2.

580

Technical drawing of Lio.

between all the modules and an easy overview, control and
interchangeability of all the software modules. For custom
development, full access to the sensor data and control of
the robot is provided.
A. Robot Hardware Overview
Lio consists of a robot arm placed on top of a mobile
platform. The robot was designed by keeping in mind the
workspace of the robot arm. It enables Lio to combine arm
and mobile platform movements to grasp objects from the
ground, reach table-tops and be at a comfortable height for
interaction with people in wheelchairs. It was a necessary
measure to ensure that the robot is not too tall in order not
to be intimidating to people who are sitting.
Padded artificial-leather covers ensure that Lio will not
cause injuries in the event of a collision but also provide
significantly more cosy and friendly appearance compared
to hard-shell robots. Majority of the body has a soft-feel to
it, which helps with the acceptance of the robot.
In any health care environment, data privacy is at the
utmost importance. Lio was specifically designed to have
enough processing power to be able to run complex algorithms, including deep learning-based ones, entirely locally
without the need for cloud computing or transferring data
outside of the robot. Lio has four embedded computing units:
Intel NUC, Nvidia Jetson AGX Xavier, Raspberry Pi and an
embedded PC with Atom processor.
Each of the computing units is responsible for running
separate modules with constant communication using an
internal secure Ethernet network. Communication is based
on ROS topics, meaning that additional modules could be
integrated into the system by using standard ROS communication protocols and make use of sensor data.
B. Mobile Platform and Navigation
The mobile platform allows Lio to safely navigate in the
environment and plan movement trajectories even in cluttered
places. The mobile platform is 790x580 mm in size, as
shown in Fig. 2. Therefore, it is suitable to operate in any
environment prepared for wheelchairs. The mobile platform
uses a differential drive system with two caster wheels,
allowing it to turn around in small spaces.

For safe navigation, different sensors are embedded in
the mobile platform. Two LiDARs are placed behind the
bumpers in the front and the back of the mobile platform
for coverage around the robot. Their measurements are
merged into a 360◦ scan. Additionally, distance sensors are
distributed around the circumference of the mobile platform
for redundant proximity detection of the obstacles. In the
scenario of Lio bumping into an obstacle, four mechanical
bumpers ensure that the collision is detected and the robot
will stop before any damage is caused. Four downwardfacing infrared floor sensors were added to detect stairs or
any edges. The robot moves at speeds up to 1 m/s, ensuring
short stopping distance when needed. To provide visual
information, two Intel RealSense D435 Depth Cameras are
embedded in the front. One camera is facing downwards to
detect the floor plane, while the other one faces up for higher
obstacle detection in front of the robot. Fields of view (FoV)
of both cameras overlap fully and cover the frontal view. In
addition to that, a wide-angle fisheye camera overviews the
full frontal view with over 170◦ FoV.
The map of the environment is created from the data of
the two LiDARs. Mapping uses gmapping from OpenSLAM
package and a probabilistic localisation system is based on
the adaptive Monte Carlo method [14] [15]. Both of these
methods were specifically adapted for Lio. On the map of the
facility, there is a possibility to add a layer of virtual objects
to prevent the robot from going to these areas. Lio can locate
the charging station and navigate to it autonomously when
running low on battery.
Custom positions can be saved on the map such as specific
waypoints or general areas like kitchen, patient rooms and
nursing stations can be added. All the sensor information
is used to ensure safe navigation through the facilities for
Lio. Furthermore, the LED strip placed around the mobile
platform indicates the current driving direction and state of
the robot allowing people to understand the intentions of Lio.
For interaction purposes, a non-touch display is embedded
in the front of the mobile platform. Use case studies have
shown that voice and touch interactions with the robot-arm
are preferred over the touch screen interface [16]. Mounting
touch-screen devices low on the platform could potentially
be a hazard due to bending down and risk of falling.
The display shows the status of the robot, displays text during the voice interaction and custom-designed visualisations
that might be beneficial during the operation of the robot.
Visualisation on the screen is fully customisable through
the scripts. Additionally, robot camera feeds can also be
visualised on the screen. Embedded loudspeakers, together
with a multi-directional microphone, enable Lio to interact
using voice and sound. Lio can understand voice commands
as well as generate speech from text or play music.
For safety purposes, an emergency button is located on
the back of the robot platform and is easily reachable in
case the robot has to be suddenly stopped. When pressed,
the robot and the mobile platform are released, making it
easy to move by hand. Upon the release of the emergency
button, Lio returns to the normal operation mode.

C. Robot Arm
The robot arm placed on top of the mobile platform is a
P-Rob 3. It is a six-degree-of-freedom (DoF) collaborative
robot arm designed for human-robot interaction tasks with
the maximum payload of 3-5 kg. Optical pseudo-absolute
encoders simplify the calibration task upon startup. The
calibration can be executed from any position, takes under 3
seconds, and each joint is moved by a maximum of 5◦ .
A trajectory planner is offered by myP, which differentiates between joint space and tool space paths. An extensive
kinematics module handles singularities and all possible
robot configurations. Both analytical and numerical calculations with various positional and directional constraints can
be executed. High-frequency calculations are made possible
by having the kinematics module as a standalone C++
library, which can also be imported into external projects.
For increased safety, the Motion Control Module (MCM)
includes a compliant position control mode. A feed-forward
proportional-derivative (PD) controller with adjustable gain
and stiffness creates a soft motion behaviour. The mode is
particularly useful when interacting with people as well as
upon contact with rigid real-world objects such as doors.
Compliant control is used for human-robot interaction tasks
as an input option. For example, users can push the head
of the robot to initiate actions or provide a positive answer,
pushing the head side-to-side indicates a negative answer.
Safety is ensured by limited joint accelerations and velocities,
as well as limited forces and collision detection.
P-Rob 3 is specifically designed to have easily interchangeable end-effectors to allow task-specific manipulation.
Custom made mechanical interface requires only one screw
to lock or release the connection. Integrated electrical pins
pass through electrical signals as well as 24V power, which
can then be adjusted using converters if needed. Additionally,
a gigabit Ethernet cable is embedded in the robot arm. It
allows establishing a connection between the devices in the
gripper and the internal computers of Lio.
D. Gripper
Lio comes with a customised P-Grip gripper, which has
a friendly appearance and detachable magnetic eyes. The
gripper has two custom-designed fingers with soft covers for
grasping and lifting and manipulating objects up to 130 mm.
One proximity sensor is embedded at the base of the
gripper and each gripper finger has four infrared-based
proximity sensors. They are used to detect whether an object
is inside the gripper and measure the distance to the objects
over and under the gripper fingers. It can be used for
collision avoidance, surface and object detection as well
as for interactive tasks like handshakes and entertainment.
Covering the finger sensors by hand can also be used to
confirm specific actions. Gripper fingers can be controlled
using position and force controllers. Lio is also capable of
learning objects using the sensor readings combined with
finger positions to classify them later on during the grasping
motion.

Voice
Commands

Nursing Interface

Node Monitor

Home Interface

myP

Water

Elderly Care

Functions

Give Ball

Exercise

Fig. 3. Available software interfaces for Lio: myP, Nursing Interface,
Home Interface and Node Monitor. All of the interfaces are web-based
and can be accessed from any browser-enabled device.

Gripper fingers are easily interchangeable. Four digital
inputs and four digital outputs together with two 24 VDC
power outputs are available for the sensors.
Furthermore, there is an interchangeable camera module
attached below the gripper fingers. It provides a video feed
of 30 frames per second (FPS).
For additional input options, an interactive ring can be
mounted at the gripper, which has several programmable
buttons. Also, Force Sensitive Resistor (FSR) sensors are
placed below the artificial leather of the gripper to detect
physical presses on the head.
E. Software and Interfaces
P-Rob 3 is controlled by a C++ based low-level architecture called Motion Control Module (MCM) and an optional
Python-based high level architecture myP. The MCM allows
the immediate position and current control with a 100 Hz
update cycle via TCP socket and includes all important
safety features. There is also Python and C++ API for the
MCM. myP provides database, path planning and machine
learning features which can be accessed through the robots
user-friendly browser interface. The browser interface also
includes a Python IDE for applications development. Control
of myP is possible from any device using multiple interfaces
such as TCP, ROS and Modbus (PLC). Vice versa, the robot
can also control other machines via the same interfaces.
Different access levels can be set up on myP. If a multiaccess configuration is needed, each user can work according
to the permissions granted. Any scripts developed in Python
IDE can be copied between the robots and maintained using
version control. The robot can be updated remotely for fixing
bugs, updating functionality and adding new features.
The status of the robot can be monitored in real-time on
multiple levels. It allows easy reconfiguration of Lio not
only before the operation but also during runtime. For nontechnical users, there are multiple simple interfaces with
clearly visualised information. The Home Interface provides
high-level functions like launching specific scripts, switching
autonomy mode on and off and remote control. For more
detailed control, Nursing Interface is used to schedule tasks
and monitor system status in an easy to understand manner.
It consists of the following information:

Location of the robot
Battery level
• Current action and calendar
• History of actions and logs
• Real-time feeds of the cameras
• Patient data and teaching-in face recognition
• Remote control and manual steering of the robot
Additional sensor status information together with the
script development and testing environment is available on
myP, specifically designed for advanced, technical users and
developers. The connection between ROS and myP is established using messages and services providing full control of
the robot from both interfaces. The Node Monitor provides
a summary of all the ROS nodes that are running on the
robot together with the ability to start, stop and restart each
node individually and dynamically adjust parameters of the
running processes. All of the interfaces can be accessed from
any browser-enabled device. Additionally, all the standard
ROS tools and interfaces fully function on Lio for more
in-depth debugging and development. Examples of myP,
Nursing Interface, Home Interface and Node Monitor are
shown in Fig. 3.
•
•

F. Perception and Behaviour Algorithms
Lio utilises sensors to enable behaviours used for assistance in everyday operations in health care institutions
as well as for research purposes. Each of the algorithms
can be enabled or disabled depending on the use case and
overridden if needed. From the development perspective,
it allows projects to reuse what is needed for the main
operation of the robot while focusing on the development
or improvement in specific areas.
Additionally, there are several AI algorithms deployed and
fully integrated on Lio. These include:
• Human Pose Estimation
• Object Detection and Recognition
• Object Grasping
• Face Detection and Recognition
• Door Opening and Closing
• Voice Recognition and Synthesis
Lio is equipped with a variety of sensors to get information
about the environment. Developed algorithms make use of
them by fusing necessary information to improve certain
tasks such as grasping. To assist people grasping an object
from the floor, the following information is used:
• The object detection for classification of the object and
its location in the camera image
• The 3D pointcloud of the RealSense camera for object
localisation in the space
• The mobile platform’s position for knowledge about the
position of the platform in the environment
By fusing all this information, Lio is able to accurately drive
to the target and move the robotic arm to grasp it. To improve
the success rate of grasping, information of proximity sensors
in the gripper fingers are used to fine-tune the gasping point
and confirm successful grasp.

Currently, special markers need to be placed next to
special objects for localisation in unstructured environments,
for example, door handles, during the door opening task.
However, the rest of the task, of observing if the door is
open, trajectory calculation and opening action success is
observed and confirmed by fusing visual and LiDAR data.
G. Adaptations for COVID-19
In the face of the COVID-19 pandemic, additional functionality was introduced to assist health care professionals in
these extenuating circumstances. In contrast to other COVID19 specific robots, Lio carries out this functionality next to
its regular routines in health care institutions. Autonomous
item delivery to staff and patients has already proven to be
beneficial as it can be carried out in a contact-less manner.
Room disinfection is important measure to prevent the
spread of the virus, especially in hospitals. Lio was adapted
to carry out disinfection using an approved UV-C light capable of effectively killing exposed germs, bacteria and viruses
on the surfaces. The development focus was targeted at
disinfection of frequently touched surfaces like door handles,
light switches, elevator buttons and handrails. A custom
holder for an UV-C light was designed for Lio that can be
carried on the back of the robot. At the current stage, ArUco
markers are placed next to the items to be disinfected. During
the disinfection routine, Lio drives to dedicated locations
on the map, locates the marker, grasps the UV-C light and
the places it over the object to be disinfected. During the
operation, Lio indicates warning signs on the screen, LED
light strip and gives a visual and verbal warning if a person
is detected close to the robot. Exposure is limited only to
the object of interest by the covered design of the holder.
One indicator of possibly infected people is Elevated
Body Temperature (EBT). In the institutions where visitors
are allowed, a common practice during the pandemic is
to take body temperature measurements of people entering
the building. To assist with that, remote measurement of
passers-by was developed. The system is based on a thermal
camera placed on the gripper, which is coupled with the
colour gripper camera. Faces detected in the colour image
are mapped to the thermal camera image. The point at
the tear glands on the inside of the eye is located, as it
has been proven to be a stable point with close to actual
body temperature [17]. For persons wearing non-transparent
glasses, a second stable point, the maximum temperature in
the area of the forehead is detected. EBT detection is done by
comparing relative temperature differences between healthy
people, who are recorded during the calibration process to
the temperature detected of the passer-by. If EBT is detected
with high confidence, a member of staff is notified to take
a manual control measurement of the particular person with
an approved medical thermometer.
H. Error Handling
To achieve a reliable performance of a complex system
like Lio in an unpredictable environment, such as elderly
care home requires a sophisticated error handling.

Error handling on Lio is divided into three main areas:
Low-level and hardware related error handling is provided
by myP for the cases like arm collisions, activation of an
emergency stop and sensor failure. For the arm collisions,
multiple resolution options are available: pausing and resuming the movement or stopping the movement so that the
skill fails followed by the next high-level skill resolving the
issue. Software components for sensing and perception tasks
are monitored and eventually restarted by operating system
services and in the case of ROS nodes by the Node Monitor.
Higher-level skill-specific error is handled in behavioural
scripts. To explain the concept, an example use case of
bringing a bottle of water to the person is taken. If a path
is blocked while navigating to the person, the robot will
actively ask a person for help. After the bottle is grasped
from the inventory, a sanity check is performed by verifying
the angle of gripper fingers in order to confirm the object
was grasped. Upon failure, either the same motion is tried,
or an alternative inventory slot is used to grasp the object
again. If a person fails to take an object from Lio during
the object handover, the robot would detect it and place the
object back into the inventory.
III. ROBOT OPERATION
Lio is an autonomous robot proactively performing tasks.
On a high-level, this proactive autonomy is controlled by the
decision engine system, which gathers information about the
robot status and environment to choose the best-suited action
to perform at a given moment [18].
Operation of Lio is ensured by a combination of perception, memory and planning components. Perception of
the environment ranges from low-level sensory inputs up
to high-level information delivered by AI algorithms. This
real-time feed, together with introspective information and
robot memory, serves as the information pool for the decision
engine - the core component ensuring reactive autonomous
behaviour of the robot. It is a high-level component deciding
which behaviour should be executed in a particular situation.
The decision engine uses the SWI Prolog logic programming language to model relations in the environment of
Lio [19]. In addition to pre-defined common-sense rules,
the users can define their reasoning mechanisms tailored to
their application. All information available to the robot is
evaluated by the set of decision rules to produce the most
suitable action proposal at the given moment. This proposal
is compared with inputs from manual user commands and
scheduled actions from the calendar. Based on a priority
system the most important action is selected for execution.
This priority selection is also able to interrupt running skills
to start more important ones.
Commonly the deployment of Lio in a facility is connected
with a set of project-specific routines. For example, in some
institutions, Lio is distributing mail around the clinic and
collecting blood samples to be delivered for analysis. A
routine may require special equipment, material or tools. Lio
can have custom holders for the tasks. The robot requests the
staff to place a specific holder before starting the routine.

A. Health Care Institution Requirements
Given the target robot deployment area being health care
institutions, followed by home environments in the future,
it is critical to comply with their requirements. The main
concern when discussing robot deployment in health care
institutions is data protection. Hospitals, rehabilitation clinics
or elderly homes do have concerns about having sensitive
information in the institution. With several cameras on the
robot, it has to be ensured that any unwanted observations do
not take place. It includes remote monitoring of the sensor
data, storing or sending out any visual information.
To comply with the privacy requirements, Lio was designed to have all the visual and navigation data processed
by on-board computers. If any information needs to be
stored, for example, learning face recognition data, it is
anonymised and encoded in a way that original images
cannot be recovered. At the moment, only voice data is
processed off-board, but a data protection contract is signed
with companies providing these services. Furthermore, it is
possible to set up a safe Virtual Private Network (VPN)
connection to access and support Lio remotely.
For full-feature operation, Lio needs a secure WiFi connection, which typically is present in many health care
institutions. Lio can connect to existing networks, so the
IT department of the institution sets security measures. The
robot can operate without a wireless connection, but access
to the interfaces will not be possible.
Another common request is to prevent the robot from
driving to restricted areas. Allowed and no-go areas can
be defined after creating a floor map of the institution and
adjusted as needed later on. Also, it is possible to define
these areas for different times of the day depending on the
task Lio is executing.
Integration to existing hospital systems, like phones or
nursing call system is also requested. However, the existing
standards depend on the systems used, and it can be challenging to have a universal solution. Given that the system
used has an existing API, integration over TCP or scripting
language, it can be done individually for each institution.

find a designated charging spot for the robot. The teaching
of the staff is done in two steps. One is to introduce staff
members of the facility to the robot, explain the capabilities
and limitations of the robot. Then, a selected number of employees are given more extensive training on how to operate
the robot, restart, manually charge it if needed and a full
introduction to Nursing and Home Interfaces for controlling
the robot. It is common to have a small Welcome Lio event
involving residents and patients in the facility. From then on,
Lio starts working autonomously in the facility.
Lio deployment in research institutions follows similar
steps. However, depending on the project goals of the lab
acquiring the robot, the robot is adapted to the needs and
training plan is tailored according to the development and
use plans. For the labs focusing on in-depth algorithm
development, a thorough introduction to myP, scripting and
ROS integration is provided.
IV. EXPERIMENTS AND USE CASES
Lio robots have been deployed in seven different health
care institutions in Germany and Switzerland. Some robots
already operate for over a year. Robots have daily routines
such as collecting lab samples from different wards and
bringing them to the pick-up point at the reception and then
delivering the mail to those same wards in a rehabilitation
clinic. In the remaining time, Lios entertain patients and
visitors with stories of former patients or encourages to join
in on a few simple exercises. Robots also remind residents
about their scheduled activities by knocking on their door,
opening it, saying out loud information about upcoming
events and closing the door when leaving. Staff can control
Lio using a tablet to schedule reminders and start high-level
commands like playing the music and offer snacks. Robots
are also capable of driving to the rooms of the residents
and taking the menu orders. This information is sent to the
catering service.

B. Lio Deployment
Deployment of Lio in health care institutions consists of
several steps, including preparation. First of all, the facility
is analysed by indicating the areas of operation for the robot,
evaluating the navigation and traversability capabilities and
recording the areas, usually by taking photos of the floor,
hallways, doors, rooms and handles that the robot will need
to operate. Then, the specific tasks of the robot are discussed
and decided upon with the client. The developer will adapt
the existing functionalities and develop new functions. The
client prepares the WiFi network for Lio according to their
privacy requirements and making sure the robot can connect
and has good coverage along the operational routes. Furthermore, a responsible person in the facility for operating Lio
is assigned.
Once Lio is delivered, the developer will install the robot.
One of the first steps is to create a map of the facility and to

Fig. 4. Patients and staff are interacting with Lio in a rehabilitation clinic
in Switzerland.

In one of the projects, Lio was deployed in the home of a
person with paraplegia for several weeks. The robot allowed
the woman to have more independence by assisting with her

daily tasks like taking and opening a bottle using a builtin automatic bottle opener and handing it over. She could
manually control Lio over her smartphone to assist with more
complex tasks such as taking off the jacket.
A. Usability Evaluation
According to the guidance document for medical devices by the FDA, development of the behaviours of
Lio was guided by human factors and usability engineering [20]. Contextual inquiries, interviews and formative
evaluations were conducted to evaluate all the existing functions [21] [13] [22]. Following the simulated testing at the
lab, usability tests were conducted with the actual end-users
in their natural environment.
In a user study on co-robots, it was determined that the
introduction of an innovative device such as a robot in health
care leads to unexpected challenges [23]. One finding was
that the head which is sometimes used to interact with Lio
was not always reachable for a person in a wheelchair due
to the distance. For safety reasons, the head of Lio was
programmed not to extend beyond the base of the mobile
platform which impeded interaction with the robot [21].
Operational data was collected in one of the rehabilitation
clinics where Lio was deployed. On a monthly basis, for
delivery tasks, Lio drove on average 16.8 km for delivery
tasks and additionally 25.2 km during entertainment tasks.
Entertainment functions were triggered 96 times per month
on average. The success rate of the autonomous daily delivery function was 85.5% in the period from February 1 to May
8, 2020. In total there were 186 planned deliveries of transporting mail and blood-samples around the clinic. The most
common reasons for failure were device failure of low-level
sensors, charging issues, and network connection problems.
There were also errors such as the pressed emergency stop
not being detected.
Generally, elderly people and health care staff are very
curious and open towards the robot. People are polite to
Lio even though they are aware that it is a machine [22].
Also, multiple usability tests and qualitative interview studies
have been conducted to improve the interaction patterns.
Supporting findings from other home trials [24] [25] [26],
patients expected the robot to have a certain personality.
During an ethical evaluation of the robot’s deployment,
an elderly person described Lio as a play companion. He
consciously engaged in a game with Lio multiple times,
giving an impression that it is a sentient being. The robot
gets often patted on the head by passersby.
On the interaction side, additional studies were done on the
topic of robot speech. Despite female voices being perceived
more friendly according to the studies, Lio is programmed
to speak using a male voice [27]. A deeper male voice was
easier to understand for elderly people. If the robot speaks
in a realistic human voice, users are more likely to reply
to the robot using voice over other types of inputs [22].
Typically the pronunciation and conversations used by Lio
are simplified, which in turn resulted in a higher response
rate.

B. Current Limitations
New challenges are constantly observed during the deployments of Lio in real-life situations. Even though combined
voice and tactile inputs have proven to be a suitable option, in certain situations, it is difficult for Lio to reliably
understand some people with disabilities as well as some
elderly people. Also, navigation in cluttered environments
and some narrower places not adapted for wheelchairs can
be challenging.
Furthermore, depth camera is not currently present in the
gripper, making precise picking of the objects placed higher
up, like a tabletop more complicated compared to the picking
from the floor. For some specific manipulation actions,
like a door opening, markers are still needed. Language
understanding is currently limited to keyword matching.
Language understanding service still relies on online service,
thus constant listening is not used due to privacy concern.
Currently, Lio does not proactively approach people for
interaction. Because of this, a reduction of interest in the
robot was observed among some people as time passes. At
the moment, additional functionality is gradually introduced
as it is developed during the lifetime of the robot to keep
people interacting with Lio and discovering new behaviours.
V. CONCLUSIONS AND FUTURE WORK
Lio is considered an all-in-one platform suitable for
human-robot interaction and personal care tasks. Its design
has evolved, both in hardware and software, to address the
limitation of similar platforms and ensure the requirements
of health care institutions, as well as home care, are met.
Lio is intrinsically safe due to the padded artificial-leather
covers covering the majority of the robot, limited forces
and speed, soft mode, advanced navigation and behaviour
algorithms. The robot complies with ISO13482 standard
- safety requirements for personal care robots. Lio has a
robotic arm placed on a moving platform; it is capable of
both, assisting and interacting with people.
Lio has been proven during multiple deployments in
health care institutions and had positive feedback regarding
functionality as well as acceptance by patients, residents and
medical staff. Currently, the majority of the robots were sold
as products, while the remaining deployments are part of the
on-going projects. Autonomous operation capability ensures
that Lio can be easily deployed with just a brief training
time for staff on how the robot operates and how it should
be used.
What differentiates Lio from other service robots are the
manipulation capabilities of the large arm and gripper which
allows opening doors, grasping, bringing, and handing over
water bottles, and handling tools for UV-C disinfection.
Combined with large and customisable inventory space, it
allows Lio to carry and operate a large variety of tools.
All the basic functionalities, as well as some advanced
navigation, perception and AI algorithms, are deployed on
the robot. Algorithms can be easily enabled and disabled
depending on the project needs making it easy for researchers

and developers to use existing algorithms and focus on their
field of expertise for improvements.
Having multiple user interfaces makes Lio usable by both,
tech-savvy and inexperienced users and allows them to easily
observe the status of the robot, schedule and adjust the behaviour according to the needs. With a possibility of having
a TCP communication, integration can be implemented with
external systems.
According to customer feedback, Lio is constantly being
updated both, in terms of hardware and software. The development is focused to improve upon identified issues and
to increase the level of autonomy. Multi-floor mapping and
elevator use is planned, as well as marker-less identification
and localisation of objects like door handles, elevator buttons
and light switches to allow the robot operation in larger areas
of the facilities. Lio can manipulate a variety of objects and
have them in inventory. Motion planner will be adapted to
take items held on the gripper or on the back of the robot
into consideration when calculating the collision-free path.
Another goal is to make Lio more proactive in terms of
interaction. It means advanced behaviour models to allow
the robot to find specific people around the facility, actively
approaching them and enabling scene understanding to determine if certain expected actions occurred. For example, if
a glass of water was handed to a person, to ensure that he
or she drank the water to stay hydrated.
Additionally, hardware adjustments are planned to include
advanced 3D sensor in the gripper, as well as to update the
appearance to convey emotional responses in user communication. Improvements on natural language processing are
planned as well to enhance the communication with wakeword recognition and chatbot capabilities.
To enhance the robot behaviour development, improve
testing and evaluate capabilities of Lio before acquiring the
robot, a full-feature realistic simulation is being developed
with full support of ROS and ROS2, including all the interfaces described in this paper.
ACKNOWLEDGMENT
Authors would like to acknowledge the hard work and
dedication of all F&P Robotics AG employees and partners
in developing and testing a personal assistant robot Lio.
R EFERENCES
[1] P. Flandorfer, “Population ageing and socially assistive robots for
elderly persons: the importance of sociodemographic factors for user
acceptance,” International Journal of Population Research, vol. 2012,
2012.
[2] D. Oliver, C. Foot, and R. Humphries, Making our health and care
systems fit for an ageing population. King’s Fund London: UK, 2014.
[3] Credit Suisse, “Die Zukunft des Pflegeheimmarkts,” Pfäffikon: Schellenberg Druck AG, 2015.
[4] World Health Organisation, “WHO — Violence against health
workers,” https://www.who.int/violence injury prevention/violence/
workplace/en/, (Accessed on 05/13/2020).
[5] R. Heinz, M. Rolf, and U. Rainer, Themenreport “Pflege 2030” Was
ist zu erwarten was ist zu tun? Bertelsmann Stiftung, 2014.
[6] T. Ikeuchi, R. Sakurai, K. Furuta, Y. Kasahara, Y. Imamura, and
S. Shinkai, “Utilizing social robot to reduce workload of healthcare
professionals in psychiatric hospital: A preliminary study,” Innovation
in Aging, vol. 2, no. suppl 1, pp. 695–696, 2018.

[7] M. Goeldner, C. Herstatt, and F. Tietze, “The emergence of care
roboticsA patent and publication analysis,” Technological Forecasting
and Social Change, vol. 92, pp. 115–131, 2015.
[8] J. Bauer, L. Gründel, J. Seßner, M. Meiners, M. Lieret, T. Lechler,
C. Konrad, and J. Franke, “Camera-based fall detection system with
the service robot sanbot ELF,” in Smart Public Building 2018 Conference Proceedings, Stuttgart, DE, 2018, pp. 15–28.
[9] “UBTECH Shows Off Massive Upgrades to Walker Humanoid Robot
- IEEE Spectrum,” https://spectrum.ieee.org/automaton/robotics/
humanoids/ubtech-upgrades-walker-humanoid-robot, (Accessed on
05/19/2020).
[10] R. Kittmann, T. Fröhlich, J. Schäfer, U. Reiser, F. Weißhardt, and
A. Haug, “Let me introduce myself: I am Care-O-bot 4, a gentleman
robot,” Mensch und computer 2015–proceedings, 2015.
[11] E. Ackerman, “Moxi Prototype from Diligent Robotics Starts
Helping Out in Hospitals,” IEEE Spectrum. https://spectrum.
ieee. org/automaton/robotics/industrial-robots/moxi-prototype-fro mdiligent-robotics-starts-helping-out-in-hospitals, 2018.
[12] S. Pfeiffer and C. Angulo, “Gesture learning and execution in a humanoid robot via dynamic movement primitives,” Pattern Recognition
Letters, vol. 67, pp. 100–107, 2015.
[13] M. Früh and A. Gasser, “Erfahrungen aus dem Einsatz von Pflegerobotern für Menschen im Alter,” in Pflegeroboter. Springer, 2018,
pp. 37–62.
[14] C. Stachniss, U. Frese, and G. Grisetti, “OpenSLAM,” URL:
http://www. openslam. org, vol. 2, 2007.
[15] D. Fox, W. Burgard, F. Dellaert, and S. Thrun, “Monte carlo localization: Efficient position estimation for mobile robots,” AAAI/IAAI, vol.
1999, no. 343-349, pp. 2–2, 1999.
[16] D. Fischinger, P. Einramhof, K. Papoutsakis, W. Wohlkinger, P. Mayer,
P. Panek, S. Hofmann, T. Koertner, A. Weiss, A. Argyros, et al.,
“Hobbit, a care robot supporting independent living at home: First
prototype and lessons learned,” Robotics and Autonomous Systems,
vol. 75, pp. 60–78, 2016.
[17] “Thermal Imaging for Detecting Elevated Body Temperature —
FLIR Systems,” https://www.flir.com/discover/public-safety/thermalimaging-for-detecting-elevated-body-temperature/, (Accessed on
05/05/2020).
[18] T. Niemueller, G. Lakemeyer, and A. Ferrein, “Incremental task-level
reasoning in a competitive factory automation scenario,” in 2013 AAAI
Spring Symposium Series, 2013.
[19] J. Wielemaker, T. Schrijvers, M. Triska, and T. Lager, “SWI-Prolog,”
Theory and Practice of Logic Programming, vol. 12, no. 1-2, pp. 67–
96, 2012.
[20] Food, D. Administration, et al., “Applying human factors and usability
engineering to medical devices: Guidance for industry and Food and
Drug Administration staff,” Washington, DC: FDA, 2016.
[21] L. Wirth, J. Siebenmann, and A. Gasser, “Erfahrungen aus dem Einsatz
von Assistenzrobotern für Menschen im Alter,” in Mensch-RoboterKollaboration. Springer, 2020, pp. 257–279.
[22] A. Gasser, S. Steinemann, and K. Opwis, “A Qualitative View on
Elders Interacting with a Health Care Robot with Bodily Movements,”
Masters thesis presented to the Department of Psychology of the
University of Basel for the degree of Master of Science in Psychology,
2017.
[23] O. Bendel, “Co-Robots as Care Robots,” arXiv preprint
arXiv:2004.04374, 2020.
[24] D. Fischinger, P. Einramhof, W. Wohlkinger, K. Papoutsakis, P. Mayer,
P. Panek, T. Koertner, S. Hofmann, A. Argyros, M. Vincze, et al.,
“HOBBIT-The Mutual Care Robot,” in RSJ International Conference
on Intelligent Robots and Systems, Tokyo (November 2013), 2013.
[25] E. Broadbent, R. Stafford, and B. MacDonald, “Acceptance of healthcare robots for the older population: Review and future directions,”
International journal of social robotics, vol. 1, no. 4, p. 319, 2009.
[26] D. S. Syrdal, K. Dautenhahn, M. L. Walters, and K. L. Koay, “Sharing
Spaces with Robots in a Home Scenario-Anthropomorphic Attributions and their Effect on Proxemic Expectations and Evaluations in
a Live HRI Trial.” in AAAI fall symposium: AI in Eldercare: new
solutions to old problems, 2008, pp. 116–123.
[27] E. Broadbent, R. Tamagawa, A. Patience, B. Knock, N. Kerse, K. Day,
and B. A. MacDonald, “Attitudes towards health-care robots in a
retirement village,” Australasian journal on ageing, vol. 31, no. 2,
pp. 115–120, 2012.

