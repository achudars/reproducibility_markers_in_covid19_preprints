JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Domain Adaptation based COVID-19 CT Lung
Infections Segmentation Network

arXiv:2011.11242v1 [eess.IV] 23 Nov 2020

Han Chen, Member, IEEE, Yifan Jiang and Hanseok Ko, Senior Member, IEEE

Abstract—Coronavirus disease (COVID-19 pneumonia) has
spread rapidly and become a global epidemic, which has had
a great impact on public health and the economy. The automatic
segmentation of lung infections from computed tomography (CT)
has become an effective method for diagnosis. In order to
realize the full potential of deep learning models in COVID19 pneumonia infections segmentation, a great deal of annotated
CT data is needed for training. The data is difficult to collect
due to the high risk of infection, and it is laborious to annotate.
Recent advances in image synthesis make it possible to train
deep learning models on realistic synthetic data with computergenerated annotations. However, the domain shift between real
data and synthetic data significantly reduces segmentation performance. In order to solve this issue, we propose a novel domain
adaptation based COVID-19 CT lung infections segmentation
network. In this work, we use limited real data without annotations and a large amount of annotated synthetic data to
train the U-Net segmentation network jointly. To overcome the
domain mismatch, we introduce conditional GAN for adversarial
training. We update the segmentation network with the crossdomain adversarial loss. This makes the embedding distribution
learned by segmentation network from real data and synthetic
data closer, thus greatly improving the representation ability of
the segmentation network. The experiment results demonstrate
that our proposed network significantly outperforms the baseline
and state-of-the-art methods.
Index Terms—COVID-19, Automatic Segmentation, Computed
Tomography, Domain Adaptation, Adversarial Training

I. I NTRODUCTION

T

HE novel coronavirus disease (COVID-19 pneumonia)
outbreak has become one of the most serious global pandemics. COVID-19 is caused by the infection of severe acute
respiratory syndrome coronavirus 2 (SARS-CoV-2) which can
be spread by breathing, coughing, sneezing, or other means.
According to the WHO [1], as of June 2020, over 10 million
people around the world had been infected with COVID-19,
with a fatality rate of over 4%.
Due to the fast progression and high infectivity of COVID19, it is urgent to develop tools for the accurate diagnosis
and evaluation of the disease. Although real-time polymerase
chain reaction (RT-PCR) assays of the sputum is considered
the gold standard for diagnosis, it is time-consuming and
has been reported to suffer from high low sensitivity, while
repeated testing is typically required for accurate confirmation.
Chest computed tomography (CT), an effective diagnostic
This research work is supported by a National Research Foundation
(NRF) grant funded by the MSIP of Korea (number 2019R1A2C2009480).
(Corresponding author: Hanseok Ko.) H. Chen, Y. Jiang and H. Ko are
with the School of Electrical Engineering, Korea University, Seoul 02841,
South Korea (e-mail: hanchen@ispl.korea.ac.kr; yfjiang@ispl.korea.ac.kr;
hsko@korea.ac.kr).

tool for pneumonia, has been strongly recommended for use
in suspected COVID-19 cases for both initial evaluations
and follow-ups. A COVID-19 CT imaging feature report [2]
shows that chest CT scans are very useful in detecting the
typical radiographic features of COVID-19, with CT features
including ground-glass opacity and consolidation. Therefore,
the qualitative evaluation of infections using CT scans could
provide important information in the fight against COVID-19.
The segmentation technique has proven effective in COVID-19
image analysis [3], [4], [5], but it remains challenging because
(1) the high variation in the distribution, texture, and size
of the infections leads to many false-negative segmentation
results from CT images and (2) the ground-class opacity
and consolidation have a similar appearance in CT scans of
confirmed cases. When the inter-class difference is very small,
diagnosis is more difficult.
Deep learning for automatic medical image segmentation is
a powerful technique for data-driven medical imaging analysis
[6]. The strong performance of deep convolutional neural
networks can be attributed to the availability of large volumes
of labeled training data. However, acquiring pixel-wise labeled
data for segmentation task is laborious and time-consuming.
In particular, it is more difficult to collect sufficient medical
images with annotations for the training of deep networks due
to privacy issues, and the lack of experts available for annotation. To overcome this problem, traditional parameterized
transformation has been widely used for data augmentation.
To achieve higher generalizability and data diversity, efforts
have been devoted to generating synthetic data that closely
resembles real data. In medical imaging, there has been recent
success in generating realistic synthetic abnormal brain MRI
images with tumors using standard generative adversarial
networks (GAN) [7], which demonstrates the effectiveness
of improving tumor segmentation. In our previous work [8],
we proposed a GAN-based CT image synthesis approach that
effectively generated high-quality and realistic COVID-19 CT
images with corresponding pixel-wise annotations.
However, models trained on synthetic data fail to perform
well with real data due to the gap between the source (synthetic) domain and the target (real) domain, because there are
differences between synthetic and real data due to the complex
and diverse features of real human tissue. Domain adaptation
techniques may be used by machine learning methods to
bridge this gap [9], [10]. The most widely used domain
adaptation methods are mapping-based methods and networkbased methods. Mapping-based methods map instances from
the source domain and target domain into a new data space, in
which the instances are similar and suitable for a union deep

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

neural network. Network-based methods involve the reuse of
a partial network that is pre-trained in the source domain and
then transferred it to be a part of the deep neural network used
in the target domain [11]. Recently, benefiting from the GAN
[12] technique, adversarial training has also been introduced
to domain adaptation. It attempts to find transferable representations that is applicable to both the source and the target
domain.
We address the challenge of accurately segmenting COVID19 infections without pixel-level annotations of real CT scan
images through adversarial training based domain adaptation
technique. Our novel method leverages a large number of
annotated synthetic COVID-19 CT images. We utilize the
conditional GAN to map the feature embeddings of source
(synthetic) domain and the target (real) domain to image
space. Then conduct cross-domain adversarial training on
the generated images, and use this gradient to update the
segmentation block, so that the segmentation block can learn
transferable feature representations which can be used in both
domains. The contributions of this work can be summarized
as follows:
(1) A novel domain adaptation based COVID-19 CT lung
infections segmentation network is proposed. The whole network is composed of conditional GAN block and U-Net segmentation block. The main segmentation task is accomplished
by the U-Net block, and the conditional GAN is introduced to
overcome the problem of domain shift through its adversarial
training. Specifically, the conditional GAN block takes the
feature embeddings from segmentation block as input, then
provides adversarial clues to guide the training of segmentation
block.
(2) We propose to utilize conditional GAN to align the
distribution of source data and target data in feature space.
First, we project the intermediate feature embeddings obtained
from the segmentation block into the image space by training
a conditional generator using reconstruction loss and withindomain adversarial loss. Then, the constraint of domain alignment is realized by imposing the cross-domain adversarial loss
to the segmentation block, so that the segmentation block has
the following feature extraction abilities. (a) When extracted
target domain features are passed to the generator, the sourcelike images can be reconstructed; (b) When extracted source
domain features are passed to the generator, the target-like
images can be reconstructed. Finally, through training each
block in turn, the domain adaptation is achieved.
(3) We apply our proposed network to the unsupervised
segmentation task of COVID-19 CT lung infections. The competitive results of our proposal in quantitative and qualitative
prove that the proposed network can achieve the accurate
segmentation of infected areas even without the annotations
for real COVID-19 CT images. This demonstrates the great
potential of our proposed network for COVID-19 diagnosis
and assessment.
II. R ELATED WORKS
Domain adaptation. Domain adaptation has been widely
applied when using synthetic data for real-world tasks. When

2

transferring knowledge from synthetic to real data, there often
exists some discrepancy between the training and test phases.
The aim of domain adaptation is to minimize the difference
and tune the models to improve generalization when testing
[13]. Existing work on domain adaptation has mostly focuses
on image classification and detection problems. Some studies
have utilized maximum mean discrepancy (MMD) [14] to
minimize feature distribution differences [15], [16], [17], but
the effect is limited by whether the distribution satisfies the
Gaussian distribution. Another strategy is self-training, which
utilizes the predictions from an ensembled model as pseudolabels for unlabeled data to train the current model [18], [19],
[20]. There is increasing interest in leveraging adversarial
learning to achieve domain adaptation, it reduces the domain
shift by forcing the features from different domains to fool the
discriminator, thus leading the features from different domains
to have a similar distribution [21], [22].
Domain adaptation for medical segmentation. For medical
image segmentation task, domain adaptation techniques have
had a positive effect in a wide variety of studies [23], [24],
[25], [26], [27]. Based on the idea of aligning latent feature
spaces, Degel et al. [23] minimized the segmentation loss
with a domain discriminator to encourage feature domaininvariance across ultrasound datasets for left atrium segmentation. Christian et al. [25] addressed the domain shift by
extending the unsupervised domain adaptation self-ensembling
method to MRI image segmentation. The most relevant work
to ours is [27], which conducted domain adaptation via adversarial learning and utilized synthetic data with sufficient
labeled data to help brain lesion segmentation. Nevertheless,
to the best of our knowledge, this present work is the first to
address the problem of domain shift in COVID-19 infections
segmentation by unsupervised adversarial training.
COVID-19 lung infections segmentation. The COVID-19
pandemic has spread globally, and medical imaging such as
CT plays an important role in the global fight against it.
Segmentation is an essential step for CT image processing and
the assessment of COVID-19 [6]. By this technique, we can
locate the regions of interest, e.g., ground-glass opacity, consolidation, and the lung. For COVID-19 image segmentation,
data with annotations is limited because manual annotation is
laborious and time-consuming. Most existing works employ
conventional transformation methods to augment data and
conduct supervised training on this augmented data [28], [29],
[30]. Some works solve this problem by constructing new
networks suitable for small scale data. Qiu et al. [31] proposed
a lightweight network to solve the over fitting problem caused
by limited training data for COVID-19 segmentation. Laradji
et al. [32] proposed a new COVID-19 segmentation model
using point-level rather than full image-level annotations,
which addressed the labelling challenge to some extent.
We instead explore the use of annotated synthetic data and
limited real COVID-19 CT images to train high-quality lung
infections segmentation networks without gaining access to
the annotations of real images. We obtain synthetic data from
[8], which is designed to generate high-quality and realistic
COVID-19 CT images. Although a large synthetic dataset
is sufficient to train deep networks efficiently, it cannot be

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Reconstruction Loss

Discriminator

Generator

Real Source

Fake Source

Embeddings

source-real
source-fake
target-real
target-fake

Adversarial loss

Real Target

Fake Target
Conditional GAN block

...
Real Source
Segmentation
Result

Segmentation block
Encoder path

Ground Truth

Pixel-wise classification Loss

Decoder path

...

Test
procedure
CT image

Segmentation map
Segmentation block

Fig. 1. The overall architecture of our domain adaptation based COVID-19 CT lung infections segmentation network. In the training process shown in the
top section, the segmentation block (encoder and decoder) is trained jointly with the conditional GAN block (generator and discriminator) and updated using
the combination of pixel-wise classification loss from the source domain and adversarial loss. We use adversarial training to enhance the segmentation block’s
feature representation ability for the target data. When testing, only the segmentation block is used.

effectively generalized to real-world images. Therefore, we
utilize adversarial training to eliminate the domain mismatch
between synthetic and real CT data. Through cross-domain
learning, the network can learn the common representations of
the two domains to maintain the features of COVID-19 lung
infections in real CT images. The learned representations are
used to build a robust infections segmentation network.
III. D OMAIN ADAPTATION BASED COVID-19 CT LUNG
INFECTIONS SEGMENTATION NETWORK

In this section, we provide details of the proposed domain
adaptation based COVID-19 lung infections segmentation network. We start by describing the overall network architecture
followed by explaining the two core components, the conditional GAN block and the segmentation block. We also present
the adversarial training strategy and loss function. The overall
architecture of the proposed network is illustrated in Figure 1.

A. Overview
Given source image xs with pixel-wise annotation y s (i.e,
synthetic CT image and corresponding annotation map generated using image synthesis technique) and target image xt
(i.e, real CT image), we want to train a network for lung
infections segmentation and test it on the target image. The
aim is to make the test performance as close as possible to
the performance of the network trained with the target data
in a supervised way. The whole network can be divided into
two parts: the segmentation block and the conditional GAN
block. The segmentation block follows U-Net style with a
symmetric encoder-decoder structure and skip connections.
Given an image from the source domain or the target domain,
the encoder is used to extract high-resolution, semantically
weak features, the decoder is treated as a pixel-wise classifier,
of which the output is a label map up-sampled to be the same
size as input. The conditional GAN network is employed to

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

overcome the domain shift problem. The generator takes the
feature embeddings from the U-Net encoder to reconstruct the
image, then the discriminator classifies the input as real or fake
in within-domain/cross-domain. The cross-domain adversarial
losses are used for updating the segmentation block, so that it
can learn transferable representations which are applicable to
both the source and the target domain.
For source image and label pair {xs , y s }, we utilize the
encoder of U-Net to extract the feature representation. The
decoder of U-Net takes the embeddings as input and produces
image-sized segmentation map ŷ s . Generator reconstructs the
source image conditioned on the source embeddings from
the encoder. The discriminator distinguishes between the
real source image (source-real) and generated source image
(source-fake), it also performs auxiliary classification task.
For target image {xt }, generator takes the target embeddings
from the encoder of U-Net as input and reconstructs the target
image. The discriminator is utilized to distinguish between the
real target image (target-real) and the generated target image
(target-fake). Note that, because there is no pixel-wise ground
truth of the target images when training, this is an unsupervised
lung infections segmentation network for target domain.

4

embedding 3
embedding 2
embedding 1

3×3-32×32-ConvTrans-256, Relu

34×34-ResnetBlock-256
2×2-34×34-ConvTrans-512, Relu

64×64-ResnetBlock-512

The conditional GAN [33] is an extension of the traditional
GAN [12], in which extra prior information is included to constrain the generator and discriminator. This prior information
can be images [34], [35], text [36], [37], etc. In this work, a
novel GAN conditioned on the embeddings from the encoder
of the segmentation network was used. This conditional GAN
block is the combination of generator and discriminator. The
generator takes the learned embeddings from the segmentation
network as input then utilizes the embeddings to reconstruct
the image for source and target domain. The discriminator
is used to distinguish the input as real or fake. The training
process of this conditional GAN block can be formulated as
a min-max game between the generator and the discriminator.
The structure of generator is shown in Figure 2. It takes
embedding #1 (the 32 × 32 feature map) from the encoder
as input, then the input is sent to a series of up-sampling and
residual layers to increase the resolution to 64 × 64. Then
the embedding #2 (the 64 × 64 feature map) is concatenated
with the up-sampled feature map. A similar operation is
employed on embedding #3 (the 128 × 128 feature map).
The residual layers are used to help reduce the computational
complexity. The final output is the reconstructions of the
source or the target image. The generator is updated by the
combination of the reconstruction loss Lsrec , Ltrec and the
adversarial loss Lsadv , Ltadv . The reconstruction loss is the
pixel-wise L1 loss between the input real source (target) and
the generated fake source (target), which can ensure the fidelity
of the reconstruction. The adversarial loss leads discriminator
to classify the fake image generated by generator as real, and
is thus used to make the output of generator resemble real
data so that to fool the discriminator. The loss function for

×2

concatenate

4×4-64×64-ConvTrans-256, Relu
128×128-ResnetBlock-256

×2

concatenate

5×5-128×128-ConvTrans-128, Relu
255×255-ResnetBlock-128

B. Conditional GAN block

×2

×2

5×5-255×255-ConvTrans-64, Relu
509×509-ResnetBlock-64
6×6-509×509-ConvTrans-1, Relu
Reconstruction results
Fig. 2. The structure of generator. The parameters of each layer are separated
by the notation ’-’, e.g., 3 × 3 means the kernel size is 3, 32 × 32 is the
size of the feature, ConvTrans represents transposed convolution, 256 is the
channel number and Relu is the activation function.

generator can be denoted as:
LG = Lsrec (xs , G(xs )) + Ltrec (xt , G(xt )) + Lsadv (D(G(xs )),
s-real) + Ltadv (D(G(st )), t-real)
(1)
The structure of discriminator is shown in Figure 3. It
takes xs , xt , G(xs ), G(xt ) as input, then the input is sent
to a series of down-sampling and residual layers. Every three
residual layers are followed by a pooling layer to reduce
the resolution. Note that, unlike from the conventional GAN
model, there are two final outputs in our discriminator. The
first is a four channel 64 × 64 probability map, in which
the value of every pixel represents the possibility of each
patch in input belonging to real or fake across source domain
and target domain. In our work, all adversarial losses are
calculated using the cross-entropy loss [38] between the output
probability map and the domain-real/fake label. Compared
with the normal discriminator whose output is only a real
number, our discriminator is more helpful to retain the detailed

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

The parameters of the segmentation block are updated by the
following function:
LS = Lsseg (y s , ŷ s ) + αLsaux + β[Lsadv (D(G(xs ), t-real)+

4×4-512×512-Conv-128, Relu
257×257-ResnetBlock-128

×3

Pooling

128×128-ResnetBlock-128

×3

Pooling

64×64-ResnetBlock-128

×3

3×3-64×64-Conv-4

3×3-64×64-Conv-3

Probability map

Auxiliary classification map

Fig. 3. The structure of discriminator. Conv represents the convolution layer.

information. The second output is a three channel 64 × 64
classification map (total three classes: ground-glass opacity,
consolidation, and lung), introduced to calculate auxiliary
loss Lsaux , which has been used in standard segmentation
networks such as FCN [39] for stable training. The auxiliary
loss is the cross-entropy between the output classification map
and corresponding down-sampled classification label. Because
we only have the pixel-wise annotations for source images
when training, the auxiliary loss is only calculated in the
source domain. The loss function for the discriminator is the
combination of adversarial and auxiliary loss, given by:
LD = Lsadv (D(xs ), s-real) + Lsadv (D(G(xs )), s-f ake)+
Ltadv (D(xt ), t-real) + Ltadv (D(G(xt )), t-f ake) + Lsaux
(2)

C. Segmentation block
The segmentation block is based on the U-Net [40] network,
which was designed for biomedical image segmentation and
has shown significant effects in dense prediction tasks. The
contracting path encodes the image into a small and deep
representation. The encoding is then decoded to the original
size of the image via a stack of upsampling layers. Additional
skip connections are included between layers at the same
hierarchical level in the encoder and decoder in order to
feed different levels of features to the decoder to gain finegrained segmentation details. In this work, to overcome the
domain mismatch between the synthetic and real CT data,
we employ several embeddings from the U-Net encoder and
share them with generator. In addition to the conditional GAN
block, the parameters of segmentation block are also updated
via adversarial training. Thus the features extracted by the
encoder are transferable for the source domain and target
domain, which helps to improve segmentation performance.

Ltadv (D(G(xt ), s-real)]
(3)
where Lsseg is the pixel-wise binary cross-entropy loss. Because there is no label available for the target domain, this
loss is applied to the source domain only. The last two terms
are the cross-domain adversarial loss for domain adaptation.
During the discriminator updating process discussed above,
the within-domain adversarial loss in Equation (2) causes
discriminator to classify the real-source (real-target) input as
real, and the fake-source (fake-target) generated by generator
as fake. All adversarial loss terms are calculated within the
domain. To overcome the domain mismatch problem, we introduce cross-domain adversarial loss Lsadv (D(G(xs ), t-real),
Ltadv (D(G(xt ), s-real) to gain the gradients from discriminator that lead to a reversal in domain classification. That is, the
embeddings corresponding to source domain are classified as
coming from the target domain, and the embeddings corresponding to the target domain are classified as coming from
the source domain. Such gradients of discriminator are used to
update the segmentation block. Through this constraint domain
alignment, the learned features from the two domains will be
closer, thus the network can learn the common representations
of the two domains.
D. Training and testing procedure
In Figure 4, we illustrate the training process of our network
with the direction of the data flow and gradient flow across different blocks. For every iteration, randomly sampled {xs , y s }
and {xt } are sent to the whole network. The conditional GAN
block and segmentation block are then iteratively updated in
turn. Note that, unlike the update of the conditional GAN
block, the adversarial loss used to update the segmentation
module is the cross-domain adversarial loss. Except for pixelwise classification loss Lseg , all of the other losses shown
in Figure 4 are calculated in the source domain and target
domain. During the testing process, we only utilize the trained
segmentation block. As shown at the bottom of Figure 1,
the input is real CT images of confirmed COVID-19 cases.
After passing it through the segmentation block, the predicted
segmentation map is generated, which is the final output of
the test process.
IV. E XPERIMENTS
A. Experimental settings
Dataset. All the target data in our quantitative and qualitative
study comes from the COVID-19 CT Segmentation dataset
[41], which was collected by the Italian Society of Medical and
International Radiology. This dataset consists of 829 images
and corresponding pixel-wise annotations from nine COVID19 patients. There are three categories in the annotations maps:
ground-glass opacity, consolidation, and lung. The first two
categories are used to represent infections and have been

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

G

Enc

D
Dec
1

Step 1: Update G

G

Enc

D

Dec
0

1

Step 2: Update D

Enc

G

D

Dec
Step 3: Update segmentation block

Fig. 4. Training process for our network. The black solid lines indicate the
data flow and the thin dashed lines indicate the gradient flow. The dashed
lines with arrows on both ends denote losses. Enc, Dec, G, and D denote
the encoder, decoder, generator, and discriminator respectively. x represents
the input CT images from the source and target domain, while y is the
segmentation ground truth corresponding to input from the source domain.

used as the two most common characteristics for COVID19 diagnosis in lung CT imaging. In this work, we select
373 slices from the 829 CT slices that contain clear areas
of infections in the images. We randomly choose 300 (80%)
images from the selected slices as training data for the target
domain. To realize the full potential of the proposed deep
network, we apply data augmentation (e.g., flipping, rotating,
shifting, brightness adjustment, and zooming) to these images, to produce 12,000 target images. Independent testing
is performed on the remaining 73 (20%) images. For the
source domain data, we follow our previous work [8], which
is designed for realistic COVID-19 lung CT image synthesis,
to obtain 10,220 synthetic CT images and corresponding
annotations for training.
Network architecture. We use U-Net [40] as the base segmentation architecture. To better capture the high-level global
features and the low-level fine-grained details of the infection,
we take the output features maps of the 3rd, 4th, and 5th downsampling steps in the encoder as the selected embeddings.
Note that, unlike the original U-Net, we set padding = 1
for DoubleConv so that the size of the selected feature maps
is consistent with the input of the generator. The size of
the selected embeddings are 32 × 32, 64 × 64, 128 × 128.

The architecture of generator and discriminator introduced in
Section III is loosely motivated by LSD-Seg [42], which was
designed for city landscape domain adaptation.
Implementation details. In this work, all images and annotations are resized and cropped to 512 × 512. We trained
the proposed network for 10 epochs with a batch size of 1.
Adam optimization [43] is adopted to our network. The initial
learning rate is 10−5 for the the segmentation block, 10−4
for the conditional GAN block, and multiplied by 0.1 every
five epochs for lr schedule. The hyperparameters are set as
α = 0.1 and β = 0.1 according to experience, these are
weights associated with different loss terms. Our network is
implemented in PyTorch deep learning framework [44] and
accelerated with an NVIDIA RTX 2080Ti GPU. To evaluate
the COVID-19 lung infections segmentation performance, we
compare the results of our proposal the baseline, and the stateof-art methods MinEnt [20], AdvEnt [20], IntraDA [22]. The
source-only model corresponds to the no adaptation case, that
is training the segmentation block using the fully-annotated
synthetic data.
Evaluation metrics. In this work, we evaluate COVID-19
lung infections segmentation performance using three widely
used evaluation metrics in medical imaging analysis, i.e., dice
similarity coefficient (Dice.), sensitivity (Sen.), and specificity
(Spec.) [45], [46]. The dice similarity coefficient is an overlap
index that can represent the degree of similarity between predicted infections area of the lung and the corresponding ground
truth. Sensitivity measures the proportion of the ground truth
infections area that is correctly predicted as such. Specificity
represents the degree to which the ground truth background
areas are predicted as background. These metrics are defined
as follows:
2 × TP
(4)
Dice. =
2 × TP × FN + FN
Sen. =

TP
TP + FN

(5)

TN
(6)
TN + FP
where TP , FP , TN , and FN indicate the number of true
positive, false positive, true negative, and false negative pixels
in the prediction respectively. The range of these metrics is
between 0 and 1, with a larger value representing the better
model performance.
Spec. =

B. Quantitative results
In this sub-section, we discuss the quantitative performance
of the proposed method. As an assistant diagnostic tool, the
model is expected to provide more detailed information on the
infected areas and the lung. Therefore, we extend our method
to multi-class tasks and compare them with other methods
on the segmentation results for the three categories (groundglass opacity, consolidation, and lung). It should be noted that
every metric for each category is calculated by taking the other
categories as background. That is, even though the network
is trained as multi-class segmentation task, we employ two
classes (object and background) when calculating the metrics.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

TABLE I
QUANTITATIVE SEGMENTATION RESULTS OF GROUND-GLASS OPACITY AND
CONSOLIDATION FOR CT IMAGES OF COVID-19 CASES
(T HE BEST EVALUATION SCORE IS MARKED IN BOLD . ↑ MEANS HIGHER NUMBER IS BETTER .)
Method
Source-only
MinEnt [20]
AdvEnt [20]
IntraDA [22]
Proposed method
Target-only

Ground-glass opacity
Dice.(%) ↑
Sen.(%) ↑
Spe.(%) ↑
79.16±0.56 73.65±0.41 99.81±0.01
79.72±0.42 71.83±0.48 99.87±0.01
81.99±0.38 76.68±0.45 99.83±0.01
79.30±0.34 69.17±0.35 99.88±0.01
86.31±0.27 85.37±0.26 99.81±0.01
87.54±0.27 86.83±0.34 99.82±0.01

Dice.(%) ↑
61.42±0.45
75.33±0.41
64.07±0.74
62.33±0.88
74.55±0.30
84.88±0.42

Consolidation
Sen.(%) ↑
57.54±0.67
67.23±0.68
54.18±0.98
57.80±1.00
67.44±0.32
82.79±0.62

Spe.(%) ↑
99.82±0.01
99.97±0.01
99.95±0.01
99.97±0.01
99.95±0.01
99.96±0.01

TABLE II
QUANTITATIVE SEGMENTATION RESULTS OF INFECTIONS (GROUND-GLASS OPACITY +
CONSOLIDATION) AND THE LUNG FOR CT IMAGES OF COVID-19 CASES
(T HE BEST EVALUATION SCORE IS MARKED IN BOLD . ↑ MEANS HIGHER NUMBER IS BETTER .)
Method
Source-only
MinEnt [20]
AdvEnt [20]
IntraDA [22]
Proposed method
Target-only

Dice.(%) ↑
76.98±0.30
80.91±0.27
81.12±0.28
77.34±0.32
86.15±0.29
89.55±0.35

Infections
Sen.(%) ↑
70.92±0.47
72.61±0.30
74.55±0.35
67.76±0.43
84.29±0.31
88.57±0.29

Spe.(%) ↑
99.66±0.01
99.86±0.01
99.82±0.01
99.89±0.01
99.81±0.01
99.82±0.01

Dice.(%) ↑
88.54±0.32
95.55±0.01
95.69±0.06
95.27±0.07
96.13±0.07
97.12±0.13

Lung
Sen.(%) ↑
93.47±0.16
95.62±0.01
95.41±0.05
95.01±0.06
94.61±0.09
97.04±0.18

Spe.(%) ↑
97.41±0.08
99.33±0.01
99.41±0.01
99.35±0.01
99.67±0.01
99.59±0.01

Tables I and II show the quantitative evaluation results
on real CT images from confirmed COVID-19 cases. The
results are reported with the format as mean ± error interval
(calculated from 95% confidence interval). The target-only
method represents the network trained on real fully-annotated
CT image data (supervised learning). It is served as the up
boundary for the performance of domain adaptation. The
results show that our domain adaptation based infections
segmentation method produces a competitive performance on
ground-glass opacity in most evaluation metrics. For more
challenging consolidation, the proposed method also performs
better than other state-of-art methods by a large margin. Compared with the source-only method without domain adaptation
(the baseline), our method exhibits an improvement of 9.17%
in the dice similarity coefficient for infections segmentation,
which further proves the effectiveness of adversarial training.
Even without access to the annotation of real CT images, the
proposed method produces the results that are comparable to
the target-only method (i.e., supervised learning). Moreover,
the proposed method achieves the highest scores for lung
segmentation, which indicates that our network is not only
suitable for small areas of infection, but also for large-area
tissues or organ segmentation.

and intensity between the synthetic data and real data lead
the network to perform poorly when tested on real lung CT
images. MinEnt [20] and AdvEnt [20] attempt to minimize
the target entropy value of the model output directly or
using adversarial learning to overcome the domain gap, as a
result, these methods produce relatively good results overall.
However, compared with our proposed method, MinEnt cannot
capture the fine-grained details of the infections. For example,
in cases (d) and (e), MinEnt cannot segment all of the groundglass opacity areas, but our proposed method can distinguish
the subtle differences between different infections types. AdvEnt is quite sensitive to the influence of irrelevant areas,
e.g., there is obvious noise in the results of AdvEnt in cases
(c) and (g). The performance of MinEnt and AdvEnt heavily
relies on the assumption that the segmentation maps of the
target-like images have a higher entropy than the predictions
of source-like images. This hypothesis may not hold true
in CT images with high similarity among different kinds of
infections. IntraDA [22] has a limited effect on infections and
lung segmentation. For example, it cannot separate groundglass opacity and consolidation in cases (c) and (d). This
is mainly because it attempts to decrease the intra-domain
gap using adversarial training, but the training performance
is limited by the pseudo labels from the generator.

C. Qualitative results

Our proposed domain adaptation based segmentation network clearly outperforms the baseline method and other stateof-art methods. Specifically, it yields a performance that is
close to the ground truth with less mis-segmented infection areas, especially for consolidation, which is relatively small and
challenging to segment, e.g. cases (c) and (g). The success of
the proposed method is due to our adversarial training strategy,
through which the true features from the target domain can be
learned from the constraint of cross-domain adversarial loss.
This strategy allows our network to more clearly distinguish

For the purpose of demonstration, we also visualize some
segmentation results on real COVID-19 CT images for qualitative comparison. In Figure 5, the lung, ground-glass opacity,
and consolidation areas in the segmentation maps are marked
in red, blue, and green, respectively. The source-only method
(baseline) produces unsatisfactory results, with a large number
of mis-segmented infections and the complete lung structure
unable to be predicted. This is because the baseline is trained
only on synthetic realistic images, the differences in texture

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

Original CT
images

Segmentation
Map

Source Only

MinEnt [20]

AdvEnt [20]

IntraDA [22]

Ours

Fig. 5. COVID-19 CT lung infections segmentation results generated by the proposed method, source-only method, and other three competitive state-of-art
domain adaptation segmentation approaches. The first column shows the original CT images of confirmed COVID-19 cases. The second column shows the
ground truth segmentation map including the lung (red), ground-glass opacity (blue), and consolidation (green) areas. The results of the compared methods
are shown in the following columns and the last column shows the result of our proposed method.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

TABLE III
COMPARISION OF DIFFERENT EMBEDDING COMBINATIONS
(T HE BEST EVALUATION SCORE IS MARKED IN BOLD . ↑ MEANS HIGHER
NUMBER IS BETTER . GGO: GROUND-GLASS OPACITY)

1
X
×
×

Embeddings
Dice.(%) ↑
2 3 4 5
GGO
Consolidation Infections
Lung
X X × × 83.43±0.40 67.35±0.36 83.30±0.23 95.69±0.07
X X X × 85.36±0.28 74.83±0.40 84.24±0.16 96.11±0.08
× X X X 86.31±0.27 74.55±0.30 86.15±0.29 96.13±0.07

TABLE IV
COMPARISION OF DIFFERENT NETWORK CONFIGURATIONS
(T HE BEST EVALUATION SCORE IS MARKED IN BOLD . ↑ MEANS HIGHER
NUMBER IS BETTER . GGO: GROUND-GLASS OPACITY)
Network
Dice.(%) ↑
Configuration
GGO
Consolidation Infections
Lung
w/o adversarial training 79.16±0.56 61.42±0.45 79.98±0.30 88.54±0.32
w/o skip connections 78.56±0.48 61.71±0.63 78.81±0.24 94.11±0.01
Feature space training 81.87±0.33 52.61±0.43 79.52±0.41 92.33±0.22
w/o auxiliary loss
85.94±0.55 72.33±0.52 85.58±0.14 95.86±0.08
Proposed
86.31±0.27 74.55±0.30 86.15±0.29 96.13±0.07

the real features of ground-glass opacity and consolidation
even without access to ground truth annotations for the target
data. In addition, we use U-Net as our segmentation block
to extract the features of the images. The skip connections
allow the proposed network to capture the details of medical
images well. In addition, our proposed method also performs
best with lung segmentation, which proves that this method
can be generalized. That is, it can be used not only for the
segmentation of lung infections but also other parts.
D. Ablation analysis
Table III presents the comparison results of taking different
combinations of embeddings as input for the generator. As
described above, the conditional generator takes three feature
embeddings from the encoder of the segmentation block as
input and maps them to the image space. The output of the
generator is then used to calculate the reconstruction loss
and the adversarial loss. In our network, the realization of
domain adaptation depends on the cross-domain adversarial
loss. Therefore, different choices of embeddings affect the
performance of the entire network. Because there are four
down-sampling operations in the U-Net feature encoder, there
are five scales including the size of the original image. We
choose three different combinations to carry out the experiments. We can see that selecting the feature layers at the
highest three levels as the input of the generator can achieve
the best overall performance. It shows that, compared with
the rich details extracted by low-level down-sampling, the
semantic information in high-level down-sampling is more
helpful for domain adaptation, so we adopt this setting in our
network.
Table IV shows how different network configurations affect
the final performance of the network. The bold line corresponds to our proposed network, while the others correspond
to models that differ from the proposed method in some
way. (1) w/o adverse training: Our source-only baseline corresponds to α=β=0. At this time, the segmentation block is

9

not updated with the adversarial loss and auxiliary loss, and
only the segmentation block works in the network. That is,
the segmentation block is trained with the source data and
tested on the target data. (2) w/o skip connections: Here,
the skip connections between the encoder and decode of
the segmentation block are removed, which are essential for
preserving the fine-grained details in the segmentation task. (3)
Feature space training: In this case, we remove the conditional
GAN block and calculate pixel-level adversarial loss for the
feature embeddings, and update the segmentation block with
this loss. (4) w/o auxiliary loss: Corresponding to α=0, the
discriminator only performs the task of classifying the input
as real or fake.
V. C ONCLUSION
In this paper, we proposed a novel domain adaptation
based method for COVID-19 lung infections segmentation. We
considered a very difficult case in which abundant synthetic
annotated medical images (source data) are available, but no
annotations are available for real COVID-19 lung CT scans
images (target data). We introduced unsupervised adversarial
domain training that correlates the features between the real
CT images and the synthetic images. At the same time, the
adversarial loss enforces the learned embeddings of the segmentation block from the two domains closer, thus the network
can learn the common representations of the two domains
to maintain the diagnostic information (i.e., the features of
COVID-19 lung infections), then these embeddings are utilized
for segmentation. Experiments on real CT images of COVID19 cases demonstrated that the proposed method outperforms
baseline and state-of-art approaches. We also demonstrated
the effectiveness of our network in lung segmentation. Our
proposed method has great potential in diagnosing of COVID19 by quantifying the infected areas.
R EFERENCES
[1] Esteban Ortiz-Ospina Hannah Ritchie et al. Mortality risk of covid-19.
https://ourworldindata.org/mortality-risk-covid.
[2] Michael Chung, Adam Bernheim, Xueyan Mei, Ning Zhang, Mingqian
Huang, Xianjun Zeng, Jiufa Cui, Wenjian Xu, Yang Yang, Zahi A Fayad,
et al. Ct imaging features of 2019 novel coronavirus (2019-ncov).
Radiology, 295(1):202–207, 2020.
[3] Ophir Gozes, Maayan Frid-Adar, Hayit Greenspan, Patrick D Browning,
Huangqi Zhang, Wenbin Ji, Adam Bernheim, and Eliot Siegel. Rapid
ai development cycle for the coronavirus (covid-19) pandemic: Initial
results for automated detection & patient monitoring using deep learning
ct image analysis. arXiv preprint arXiv:2003.05037, 2020.
[4] Fei Shan, Yaozong Gao, Jun Wang, Weiya Shi, Nannan Shi, Miaofei
Han, Zhong Xue, and Yuxin Shi. Lung infection quantification of covid19 in ct images with deep learning. arXiv preprint arXiv:2003.04655,
2020.
[5] Cong Shen, Nan Yu, Shubo Cai, Jie Zhou, Jiexin Sheng, Kang Liu,
Heping Zhou, Youmin Guo, and Gang Niu. Quantitative computed
tomography analysis for stratifying the severity of coronavirus disease
2019. Journal of Pharmaceutical Analysis, 2020.
[6] Feng Shi, Jun Wang, Jun Shi, Ziyan Wu, Qian Wang, Zhenyu Tang, Kelei
He, Yinghuan Shi, and Dinggang Shen. Review of artificial intelligence
techniques in imaging data acquisition, segmentation and diagnosis for
covid-19. IEEE Reviews in Biomedical Engineering, 2020.
[7] Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G
Schwarz, Matthew L Senjem, Jeffrey L Gunter, Katherine P Andriole,
and Mark Michalski. Medical image synthesis for data augmentation and
anonymization using generative adversarial networks. In International
workshop on simulation and synthesis in medical imaging, pages 1–11.
Springer, 2018.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[8] Yifan Jiang, Han Chen, Murray Loew, and Hanseok Ko. Covid-19 ct
image synthesis with a conditional generative adversarial network. arXiv
preprint arXiv:2007.14638, 2020.
[9] Yang Zhang, Philip David, and Boqing Gong. Curriculum domain
adaptation for semantic segmentation of urban scenes. In Proceedings
of the IEEE International Conference on Computer Vision, pages 2020–
2030, 2017.
[10] German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and
Antonio M Lopez. The synthia dataset: A large collection of synthetic
images for semantic segmentation of urban scenes. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages
3234–3243, 2016.
[11] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang,
and Chunfang Liu. A survey on deep transfer learning. In International
conference on artificial neural networks, pages 270–279. Springer, 2018.
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems, pages 2672–2680, 2014.
[13] Vishal M Patel, Raghuraman Gopalan, Ruonan Li, and Rama Chellappa.
Visual domain adaptation: A survey of recent advances. IEEE signal
processing magazine, 32(3):53–69, 2015.
[14] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard
Schölkopf, and Alexander Smola. A kernel two-sample test. The Journal
of Machine Learning Research, 13(1):723–773, 2012.
[15] Muhammad Ghifary, W Bastiaan Kleijn, and Mengjie Zhang. Domain
adaptive neural networks for object recognition. In Pacific Rim international conference on artificial intelligence, pages 898–904. Springer,
2014.
[16] Hongliang Yan, Yukang Ding, Peihua Li, Qilong Wang, Yong Xu, and
Wangmeng Zuo. Mind the class weight bias: Weighted maximum mean
discrepancy for unsupervised domain adaptation. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pages
2272–2281, 2017.
[17] Philip Haeusser, Thomas Frerix, Alexander Mordvintsev, and Daniel
Cremers. Associative domain adaptation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 2765–2773, 2017.
[18] Hongmin Li, Doina Caragea, Cornelia Caragea, and Nic Herndon.
Disaster response aided by tweet classification with a domain adaptation
approach. Journal of Contingencies and Crisis Management, 26(1):16–
27, 2018.
[19] Yang Zou, Zhiding Yu, BVK Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via classbalanced self-training. In Proceedings of the European conference on
computer vision (ECCV), pages 289–305, 2018.
[20] Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and
Patrick Pérez. Advent: Adversarial entropy minimization for domain
adaptation in semantic segmentation. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 2517–
2526, 2019.
[21] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 7167–
7176, 2017.
[22] Fei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, and In So
Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 3764–
3773, 2020.
[23] Markus A Degel, Nassir Navab, and Shadi Albarqouni. Domain and
geometry agnostic cnns for left atrium segmentation in 3d ultrasound. In
International Conference on Medical Image Computing and ComputerAssisted Intervention, pages 630–637. Springer, 2018.
[24] Yue Zhang, Shun Miao, Tommaso Mansi, and Rui Liao. Task driven
generative modeling for unsupervised domain adaptation: Application
to x-ray image segmentation. In International Conference on Medical
Image Computing and Computer-Assisted Intervention, pages 599–607.
Springer, 2018.
[25] Christian S Perone, Pedro Ballester, Rodrigo C Barros, and Julien
Cohen-Adad. Unsupervised domain adaptation for medical imaging
segmentation with self-ensembling. NeuroImage, 194:1–11, 2019.
[26] Jian Ren, Ilker Hacihaliloglu, Eric A Singer, David J Foran, and
Xin Qi. Adversarial domain adaptation for classification of prostate
histopathology whole-slide images. In International Conference on
Medical Image Computing and Computer-Assisted Intervention, pages
201–209. Springer, 2018.

10

[27] Konstantinos Kamnitsas, Christian Baumgartner, Christian Ledig, Virginia Newcombe, Joanna Simpson, Andrew Kane, David Menon, Aditya
Nori, Antonio Criminisi, Daniel Rueckert, et al. Unsupervised domain
adaptation in brain lesion segmentation with adversarial networks. In
International conference on information processing in medical imaging,
pages 597–609. Springer, 2017.
[28] Chuansheng Zheng, Xianbo Deng, Qing Fu, Qiang Zhou, Jiapei Feng,
Hui Ma, Wenyu Liu, and Xinggang Wang. Deep learning-based
detection for covid-19 from chest ct using weak label. medRxiv, 2020.
[29] Lu Huang, Rui Han, Tao Ai, Pengxin Yu, Han Kang, Qian Tao, and
Liming Xia. Serial quantitative chest ct assessment of covid-19: Deeplearning approach. Radiology: Cardiothoracic Imaging, 2(2):e200075,
2020.
[30] Xiaolong Qi, Zicheng Jiang, Qian Yu, Chuxiao Shao, Hongguang Zhang,
Hongmei Yue, Baoyi Ma, Yuancheng Wang, Chuan Liu, Xiangpan
Meng, et al. Machine learning-based ct radiomics model for predicting
hospital stay in patients with pneumonia associated with sars-cov-2
infection: A multicenter study. medRxiv, 2020.
[31] Yu Qiu, Yun Liu, and Jing Xu. Miniseg: An extremely minimum network
for efficient covid-19 segmentation. arXiv preprint arXiv:2004.09750,
2020.
[32] Issam Laradji, Pau Rodriguez, Oscar Manas, Keegan Lensink, Marco
Law, Lironne Kurzman, William Parker, David Vazquez, and Derek
Nowrouzezahrai. A weakly supervised consistency-based learning
method for covid-19 segmentation in ct images. arXiv preprint
arXiv:2007.02180, 2020.
[33] Mehdi Mirza and Simon Osindero. Conditional generative adversarial
nets. arXiv preprint arXiv:1411.1784, 2014.
[34] Hyojin Park, Youngjoon Yoo, and Nojun Kwak. Mc-gan: Multiconditional generative adversarial network for image synthesis. arXiv
preprint arXiv:1805.01123, 2018.
[35] Dwarikanath Mahapatra, Behzad Bozorgtabar, Jean-Philippe Thiran, and
Mauricio Reyes. Efficient active learning for image classification and
segmentation using a sample selection and conditional generative adversarial network. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 580–588. Springer,
2018.
[36] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.
Semantic image synthesis with spatially-adaptive normalization. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2337–2346, 2019.
[37] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt
Schiele, and Honglak Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.
[38] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for
training deep neural networks with noisy labels. In Advances in neural
information processing systems, pages 8778–8788, 2018.
[39] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
networks for semantic segmentation. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 3431–
3440, 2015.
[40] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International
Conference on Medical image computing and computer-assisted intervention, pages 234–241. Springer, 2015.
[41] http://medicalsegmentation.com/covid19/.
[42] Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and
Rama Chellappa. Learning from synthetic data: Addressing domain shift
for semantic segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 3752–3761, 2018.
[43] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
[44] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward
Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga,
and Adam Lerer. Automatic differentiation in pytorch. In In NIPS
Workshop, 2017.
[45] Araon Fenster and Bernard Chiu. Evaluation of segmentation algorithms
for medical imaging. In 2005 IEEE Engineering in Medicine and
Biology 27th Annual Conference, pages 7186–7189. IEEE, 2006.
[46] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net:
Fully convolutional neural networks for volumetric medical image
segmentation. In 2016 Fourth International Conference on 3D Vision
(3DV), pages 565–571. IEEE, 2016.

