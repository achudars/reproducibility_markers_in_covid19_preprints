arXiv:2005.07503v1 [cs.CL] 15 May 2020

COVID-T WITTER -BERT: A NATURAL L ANGUAGE
P ROCESSING M ODEL TO A NALYSE COVID-19 C ONTENT ON
T WITTER

Martin Müller
Digital Epidemiology Lab
EPFL
Geneva, Switzerland
martin.muller@epfl.ch

Marcel Salathé
Digital Epidemiology Lab
EPFL
Geneva, Switzerland
marcel.salathe@epfl.ch

Per E Kummervold
FISABIO-Public Health
Vaccine Research Department
Valencia, Spain
per@capia.no

May 18, 2020

A BSTRACT
In this work, we release COVID-Twitter-BERT (CT-BERT), a transformer-based model, pretrained
on a large corpus of Twitter messages on the topic of COVID-19. Our model shows a 10–30%
marginal improvement compared to its base model, BERT-L ARGE, on five different classification
datasets. The largest improvements are on the target domain. Pretrained transformer models, such
as CT-BERT, are trained on a specific target domain and can be used for a wide variety of natural
language processing tasks, including classification, question-answering and chatbots. CT-BERT is
optimised to be used on COVID-19 content, in particular from social media.
Keywords Natural Language Processing · COVID-19 · Language Model · BERT

1

Introduction

directional transformer models on huge unlabelled text corpuses [1, 2, 3, 4]. This process is done using methods such
as mask language modelling (MLM), next sentence predicTwitter has been a valuable source of news and a public tion (NSP) and sentence order prediction (SOP). Different
medium for expression during the COVID-19 pandemic. models vary slightly in how these methods are applied,
However, manually classifying, filtering and summarising but in general, all training is done in a fully unsupervised
the large amount of information available on COVID-19 manner. This process generates a general language model
on Twitter is impossible and has also been a challenging that is then used as input for a supervised finetuning for
task to solve with tools from the field of machine learning specific language processing tasks, such as classification,
and natural language processing (NLP). To improve our question-answering models, and chatbots.
understanding of Twitter messages related to COVID-19
content as well as the analysis of this content, we have Our model is based on the BERT-L ARGE (English, untherefore developed a model called COVID-Twitter-BERT cased, whole word masking) model. BERT-L ARGE is
trained mainly on raw text data from Wikipedia (3.5B
(CT-BERT)1 .
words) and a free book corpus (0.8B words) [2]. Whilst
Transformer-based models have changed the landscape this is an impressive amount of text, it still contains litof NLP. Models such as BERT, RoBERTa and AL- tle information about any specific subdomain. To imBERT are all based on the same principle – training bi1

https://github.com/digitalepidemiologylab/covid-twitter-bert

prove performance in subdomains, we have seen numerous
transformer-based models trained on specialised corpuses.
Some of the most popular ones are B IO BERT [5] and
S CI BERT [6]. These models are trained using the exact
same unsupervised training techniques as the main models
(MLM/NSP/SOP). They can be trained from scratch, but
this requires a very large corpus, so a more common approach is to start with the trained weights from a general
model. In this study, this process is called domain-specific
pretraining. When trained, such models can be used as
replacements for general language models and be trained
for downstream tasks.

2

sets. Three of them are publicly available datasets, and two
are from internal projects not yet published. All datasets
consist of Twitter-related data.
2.1.1

COVID-19 Category (CC)

This dataset is a subsample of the data used for training
CT-BERT, specifically for the period between January
12 and February 24, 2020. Annotators on Amazon Turk
(MTurk) were asked to categorise a given tweet text into
either being a personal narrative (33.3%) or news (66.7%).
The annotation was performed using the Crowdbreaks platform [7].

Method

2.1.2

Vaccine Sentiment (VS)

This dataset contains a collection of measles- and
vaccination-related US-geolocated tweets collected between March 2, 2011 and October 9, 2016. The dataset was
first used by Pananos et al. [9], but a modified version from
Müller et al. [7] was used here. The dataset contains three
classes: positive (towards vaccinations) (51.9%), negative
(7.1%) and neutral/others (41.0%). The neutral category
was used for tweets which are either irrelevant or ambiguous. Annotation was performed on MTurk.

The CT-BERT model is trained on a corpus of 160M
tweets about the coronavirus collected through the Crowdbreaks platform [7] during the period from January 12 to
April 16, 2020. Crowdbreaks uses the Twitter filter stream
API to listen to a set of COVID-19-related keywords2 in the
English language. Prior to training, the original corpus was
cleaned for retweet tags. Each tweet was pseudonymised
by replacing all Twitter usernames with a common text
token. A similar procedure was performed on all URLs to
web pages. We also replaced all unicode emoticons with
textual ASCII representations (e.g. :smile: for ,) using
the Python emoji library3 . In the end, all retweets, duplicates and close duplicates were removed from the dataset,
resulting in a final corpus of 22.5M tweets that comprise
a total of 0.6B words. The domain-specific pretraining
dataset therefore consists of 1/7th the size of what is used
for training the main base model. Tweets were treated as
individual documents and segmented into sentences using
the spaCy library [8].

2.1.3

Maternal Vaccine Stance (MVS)

The dataset is from a so far unpublished project related to
the stance towards the use of maternal vaccines. Experts
in the field annotated the data into four categories: neutral
(41.0%), discouraging (25.3%), promotional (43.9%) and
ambiguous (14.3%). Each tweet was annotated threefold,
and disagreement amongst the experts was resolved in each
case by using a common scoring criterion.

All input sequences to the BERT models are converted
to a set of tokens from a 30 000-word vocabulary. As all
Twitter messages are limited to 280 characters, this allows
us to reduce the sequence length to 96 tokens, thereby
increasing the training batch sizes to 1024 examples. We
use a dupe factor of 10 on the dataset, resulting in 285M
training examples and 2.5M validation examples. A constant learning rate of 2e-5, as recommended on the official
BERT GitHub4 when doing domain-specific pretraining.

2.1.4

Twitter Sentiment SemEval (SE)

This is an open dataset from SemEval-2016 Task 4: Sentiment Analysis in Twitter [10]. In particular, we used
the dataset for subtask A, a dataset annotated fivefold into
three categories: negative (15.7%), neutral (45.9%) and
positive (38.4%). We make a small adjustment to this
dataset by fully anonymising links and usernames.
2.1.5

Loss and accuracy was calculated through the pretraining
procedure. For every 100 000 training steps, we therefore
save a checkpoint and finetune this towards a variety of
downstream classification tasks. Distributed training was
performed using Tensorflow 2.2 on a TPU v3-8 (128GB
of RAM) for 120 h.

Stanford Sentiment Treebank 2 (SST-2)

SST-2 is a public dataset consisting of binary sentiment
labels, negative (44.3%) and positive (55.7%), within sentences [11]. Sentences were extracted from a dataset of
movie reviews [12] and did not originate from Twitter,
making SST-2 our only non-Twitter dataset.

The dataset split size is predefined for the SST-2 and SE
datasets. For the SST-2 dataset, the test dataset is not
To assess the performance of our model on downstream released. For the other datasets, we aimed at a split of
classification tasks, we selected five independent training around 50%-30% between the training and development
2.1

Evaluation

2

wuhan, ncov, coronavirus, covid, sars-cov-2
https://pypi.org/project/emoji/
4
https://github.com/google-research/bert
3

2

Dataset

Classes

Train

Dev

COVID-19 Category (CC)

2

3094

1031

Vaccine Sentiment (VC)

3

5000

3000

Maternal Vaccine Stance (MVS)

4

1361

817

Stanford Sentiment Treebank 2 (SST-2)

2

67 349

872

Twitter Sentiment SemEval (SE)

3

6000

817

Labels
News

Personal
N

Neutral

Positive

A N

Promotional

Disc

Negative
Neg

Neutral

Positive
Positive

1.9
1.7
1.5
0.4
0.2
0.0

0.700
0.675
0.650
0.625
1.000
0.975
0.950
0.925
100000 200000 300000 400000 500000
Pretraining step

NSP accuracy MLM accuracy

NSP loss

MLM loss

Table 1: Overview of the evaluation datasets. All five evaluation datasets are multi-class datasets with sometimes
strong label imbalance, visualised by the proportional bar width in the label column. N and Neg stand for negative;
Disc and A stand for discouraging and ambiguous, respectively.

Figure 1: Evaluation metrics for the domain-specific pretraining of CT-BERT. Shown are the loss and accuracy of
masked language modelling (MLM) and next sentence prediction (NSP) tasks.
sets, leaving a test set of 20% which was not used in this
work. Our intention was not to optimise the finetuned
models but to thoroughly evaluate the performance of the
domain-specific CT-BERT-model. We experimented with
different numbers of epochs for each training dataset for
BERT-L ARGE (i.e. checkpoint 0 of CT-BERT) and selected the optimal one. We then used this number in sub-

sequent experiments on the respective dataset. We ended
with three epochs for SST-2, CC and SE, five epochs for
VC and 10 epochs for MVC, all with a learning rate of
2e-05. The number of epochs was dependent on both the
size and balance of the categories. Larger and unbalanced
sets require more epochs.

3

value of 1.48. The NSP task improves only marginally,
as it already performs very well initially. Training was
stopped at 500 000, an equivalent of 512M training examples, which we consider as our final model. This corresponds to roughly 1.8 training epochs. All metrics for the
MLM and NLM tasks improve steadily throughout training. However, using loss/metrics for these tasks to evaluate
the correct time to stop training is difficult.

3.1

Results
Domain-sepcific pretraining

Figure 1 shows the progress of pretraining CT-BERT at intervals of 25k training steps and the evaluation of 1k steps
on a held-out validation dataset. All metrics considered improve throughout the training process. The improvement
on the MLM loss task is most notable and yields a final

3.2

Evaluation on classification datasets

the number of training epochs for each dataset according
to its size in order to have a similar number of training
steps for each dataset. Our final model shows higher perforTo assess the performance of our model properly, we com- mance on all datasets (a mean F1 score of 0.833) compared
pared the mean F1 score of CT-BERT with that of BERT- with BERT-L ARGE (a mean F1 score of 0.802). As the
L ARGE on five different classification datasets. We adapted
3

BERT-L ARGE

CT-BERT

∆MP

COVID-19 Category (CC)
Vaccine Sentiment (VC)
Maternal Vaccine Stance (MVS)
Stanford Sentiment Treebank 2 (SST-2)
Twitter Sentiment SemEval (SE)

0.931
0.824
0.696
0.937
0.620

0.949
0.869
0.748
0.944
0.654

25.88%
25.27%
17.07%
10.67%
8.97%

Average

0.802

0.833

17.57%

Dataset

Table 2: Comparison of the final model performance with BERT-L ARGE. CT-BERT shows improvements on all
datasets. The marginal improvement is the highest on the COVID-19-related dataset (CC) and lowest on the SST-2 and
SemEval datasets.

initial performance varies widely across datasets, we com- From this metric, we can observe the largest improvement
pute the relative improvement in marginal performance of our model on the COVID-19-specific dataset (CC), with
(∆MP) for each dataset. ∆MP is calculated as follows:
a ∆MP value of 25.88%. The marginal improvement is
also high on the Twitter datasets related to vaccine sentiment (MVS). Our model likewise shows some improvements on the SST-2 and SemEval datasets, but to a smaller
F1, BERT-L ARGE − F1, CT-BERT
∆MP =
extent.
1 − F1, BERT-L ARGE
3.3

Evaluation on intermediary pretraining
checkpoints

ment afterwards. The loss curve, on the other hand, shows
a gradual increase even after step 200k. We also note that
for the COVID-19-related dataset, most of the marginal
improvement occurred after 100k pretraining steps. SST-2,
So far, we have seen improvements in the final CT-BERT the only non-Twitter dataset, improves much more slowly
model on all evaluated datasets. To understand whether and reaches its final performance only after 200k pretrainthe observed decrease in loss during pretraining linearly ing steps.
translates into performance on downstream classification
tasks, we evaluated CT-BERT on five intermediary ver- Amongst runs on the same model and dataset, some degree
sions (checkpoints) of the model and on the zero check- of variance in performance was observed. This variance
point, which corresponds to the original BERT-L ARGE is mostly driven by runs with a particularly low performodel. At each intermediary checkpoint, 10 repeated train- mance. We observe that the variance is dataset dependent,
ing runs (finetunings) for each of the five datasets were but it does not increase throughout different pretraining
performed, and the mean F1 score was recorded. Fig- checkpoints and is comparable to the variance observed
ure 2 shows the marginal performance increase (∆MP) on BERT-L ARGE (pretraining step zero). The most stable
at specific pretraining steps. Our experiments show that training seems to be on the SemEval training set, and the
downstream performance increases fast up to step 200k in least stable one is on SST-2, but most of this difference is
the pretraining and only demonstrates marginal improve- within the error margins.

4

Discussion

formance gains even when working with general Twitter
messages (SemEval dataset) or with a non-Twitter dataset
The most accurate way to evaluate the performance of (SST-2).
a domain-specific model is to apply it on specific down- Our results show that the MLM and NSP metrics during
stream tasks. CT-BERT is evaluated on five different the pretraining align to some degree with downstream perTwitter-based datasets. Compared to BERT-L ARGE, it formance on classification tasks. However, compared with
improves significantly on all datasets. However, the im- COVID-19 or health-related content, out-of-domain text
provement is largest in datasets related to health, particu- might require longer pretraining to achieve a similar perlarly in datasets related to COVID-19. We therefore expect formance boost.
CT-BERT to perform similarly well on other classification
problems on COVID-19-related data sources, but particu- Whilst we have observed an improvement in performance
on classification tasks, we did not test our model on other
larly on text derived from social media platforms.
natural language understanding tasks. Furthermore, at the
Whilst it is expected that the benefit of using CT-BERT time of this paperâĂŹs writing, we only had access to
instead of BERT-L ARGE is greatest when working with one COVID-19-related dataset. The general performance
Twitter COVID-19 text, it is reasonable to expect some per4

Marginal performance increase MP

0.30

Evaluation dataset
Maternal Vaccine Stance (MVS)
COVID-19 Category (CC)
SemEval 2016 (SE)
Vaccine Sentiment (VS)
Stanford Sentiment Treebank (SST-2)
Average

0.25
0.20
0.15
0.10
0.05
0.00
0

100000

200000
300000
Pretraining step

400000

500000

Figure 2: Marginal performance increase in the F1 score (∆MP) on finetuning on various classification tasks at
increasing steps of pretraining. Zero on the x-axis corresponds to the base model, which is BERT-L ARGE in this case.
Our model improves on all evaluated datasets, with the biggest relative improvement being in the COVID-19 category
dataset. The bands show the standard error of the mean (SEM) out of 10 repeats.

of our model might be improved further by considering
pretraining under different hyperparameters, particularly
modifications to the learning rate schedules, training batch
sizes and optimisers. Future work might include evaluation
on other datasets and the inclusion of more recent training
data.

not to necessarily optimise the finetuning. The number of
finetuning epochs and the learning rate are, for instance,
have been optimised for BERT-L ARGE, not for CT-BERT.
This means that there is still great room for optimisation
on the downstream task.

The best way to evaluate pretrained transformer models is
to finetune them on downstream tasks. Finetuning a classifier on a pre-trained model is considered computationally
cheap. The training time is usually done in an hour or
two on a GPU. Using this method for evaluation is more
expensive, as it requires evaluating multiple checkpoints
to monitor improvement and on several varied datasets to
show robustness. As finetuning results vary between each
run, each experiment must be performed multiple times
when the goal is to study the pretrained model. In this
case, we repeated the training for six checkpoints, 10 runs
for each checkpoint on all the five datasets. A total of
300 evaluation runs were performed. The computational
cost for evaluation is therefore on par with the pretraining.
Large and reliable training and validation sets make this
task easier, as the number of repetitions can be reduced.

5

Data Availability

The model, code and public datasets are available
in our GitHub repository: https://github.com/
digitalepidemiologylab/covid-twitter-bert.

6

Funding

PK received funding from the European Commission for
the call H2020-MSCA-IF-2017 and the funding scheme
MSCA-IF-EF-ST for the VACMA project (grant agreement ID: 797876).

MM and MS received funding through the Versatile Emerging infectious disease Observatory grant as a part of the
European CommissionâĂŹs Horizon 2020 framework proAll the tests are done on categorisation tasks, as this task gramme (grant agreement ID: 874735).
is easier in terms of both data access and evaluation. However, transformer-based models can be used for a wide The research was supported with Cloud TPUs from
range of tasks, such as named entity recognition and ques- Google’s TensorFlow Research Cloud and Google Cloud
tion answering. It is expected that CT-BERT can also be credits in the context of COVID-19-related research.
used for these kinds of tasks within our target domain.

7

Our primary goal in this work was to obtain stable results
on the finetuning in order to evaluate the pre-trained model,

Conflicts of Interest

The authors have no conflicts of interest to declare.
5

References
[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages
5998–6008, 2017.
[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[3] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692, 2019.
[4] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A
lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.
[5] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,
36(4):1234–1240, 2020.
[6] Iz Beltagy, Arman Cohan, and Kyle Lo. Scibert: Pretrained contextualized embeddings for scientific text. arXiv
preprint arXiv:1903.10676, 2019.
[7] Martin M Müller and Marcel Salathé. Crowdbreaks: Tracking health trends using public social media data and
crowdsourcing. Frontiers in public health, 7, 2019.
[8] Matthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom embeddings,
convolutional neural networks and incremental parsing. To appear, 7(1), 2017.
[9] A Demetri Pananos, Thomas M Bury, Clara Wang, Justin Schonfeld, Sharada P Mohanty, Brendan Nyhan, Marcel
Salathé, and Chris T Bauch. Critical dynamics in population vaccinating behavior. Proceedings of the National
Academy of Sciences, 114(52):13762–13767, 2017.
[10] Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin Stoyanov. Semeval-2016 task 4:
Sentiment analysis in twitter. arXiv preprint arXiv:1912.01973, 2019.
[11] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher
Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the
2013 conference on empirical methods in natural language processing, pages 1631–1642, 2013.
[12] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect
to rating scales. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages
115–124. Association for Computational Linguistics, 2005.

6

