Estimating the Efficiency Gain of Covariate-Adjusted Analyses in
Future Clinical Trials Using External Data

arXiv:2104.14752v1 [stat.ME] 30 Apr 2021

Xiudi Li* , Sijia Li* , and Alex Luedtke†
*

Department of Biostatistics, University of Washington
†

Department of Statistics, University of Washington

Abstract
We present a general framework for using existing data to estimate the efficiency gain from using a
covariate-adjusted estimator of a marginal treatment effect in a future randomized trial. We describe
conditions under which it is possible to define a mapping from the distribution that generated the
existing external data to the relative efficiency of a covariate-adjusted estimator compared to an
unadjusted estimator. Under conditions, these relative efficiencies approximate the ratio of sample
size needed to achieve a desired power. We consider two situations where the outcome is either fully
or partially observed and several treatment effect estimands that are of particular interest in most
trials. For each such estimand, we develop a semiparametrically efficient estimator of the relative
efficiency that allows for the application of flexible statistical learning tools to estimate the nuisance
functions and an analytic form of a corresponding Wald-type confidence interval. We also propose a
double bootstrap scheme for constructing confidence intervals. We demonstrate the performance of
the proposed methods through simulation studies and apply these methods to data to estimate the
relative efficiency of using covariate adjustment in Covid-19 therapeutic trials.

1

Introduction

The aim of most clinical trials is to estimate a marginal treatment effect that contrasts outcomes in a
treatment group to those in a control group. In addition to the treatment assignment and outcome, data
1

on prognostic baseline covariates are often available. In the case of continuous outcomes, the U.S. Food
and Drug Administration [10] recommends adjusting for these baseline covariates through ANCOVA or
linear regression models. However, such covariate-adjusted analyses are often underutilized in practice,
especially with ordinal or time-to-event data [2].
Compared with unadjusted analyses, analyses that adjust for baseline covariates have several benefits. First, adjusted analyses can lead to consistent estimators of the treatment effect under weaker
assumptions. One such example arises when right-censoring is present in a trial with a time-to-event
outcome. Adjusted analyses often give consistent estimates provided that the censoring and survival
times are independent given treatment and covariates [22]. This condition is more plausible in many
trial settings than is the requirement made in unadjusted analyses that the censoring and survival times
are independent given treatment alone. Second, adjusting for covariates that are predictive of the outcome can improve precision, and thus a smaller sample size can be required to achieve a desired power.
Such precision gain is generally expected when the outcome is fully observed, and also applies in certain
cases where the outcome is only partially observed — some exceptions occur, for example, when the
covariates are highly predictive of the censoring time but are only weakly predictive of the survival time.
Despite these potential benefits, covariate adjustment is underutilized in analyzing clinical trial data.
This is partly because, at the trial planning stage, there is typically little prior knowledge about the
amount of precision gain that should be expected to result from using covariate adjustment.
To address this problem, many previous works have aimed to estimate this precision gain using external datasets. In particular, some works have demonstrated the potential precision gain of covariate
adjustment in clinical trial settings by comparing the standard errors of adjusted and unadjusted estimators on existing clinical trial datasets [e.g., 26, 34]. When the data-generating mechanism that gave rise
to one of these existing datasets is reflective of the corresponding mechanism that is anticipated in an
upcoming trial, these point estimates may yield a reasonable estimate of the precision gain anticipated in
these future trials. It is worth noting, however, that the sampling variability in the existing trial dataset
induces uncertainty in this point estimate. Other works have used an existing trial dataset to design a
simulation study that can be used to estimate the precision gain [e.g., 16, 7, 26]. However, even when
these simulation studies involve many repetitions, so that the Monte Carlo error is negligible, there is
still uncertainty associated with these precision gain estimates that arises due to sampling variability in
the existing trial dataset. In many cases, there may not be data available from a clinical trial that is
2

reflective of the upcoming trial. An alternative approach, which does not require access to such data
but can leverage it when it is available, is to conduct a simulation based on an external dataset that
may be reflective of the covariate and outcome distributions that will be seen in the control arm of the
upcoming trial [3]. This data may, for example, be derived from a pilot study or an observational study.
Treatment arm data can then be simulated under a sharp null of no effect or as a user-defined shift of the
conditional distribution of the outcome given covariates in the pilot study. As for the earlier simulation
approach, a point estimate of the precision gain can be easily obtained, but there is still uncertainty in
this point estimate that arises from the sampling variability in the dataset upon which the simulation is
based.
It can be challenging to be confident that a favorable estimated precision gain is not due to random
noise, especially when the external dataset is small. Consequently, some clinical trialists may be cautious
when making decisions about using covariate adjustment in future clinical trials based on a point estimate
alone, even if the external dataset upon which it is based is known to be reflective of the data that will be
seen in the trial. In other statistical estimation problems, the lack of interpretability of point estimates is
often addressed by reporting a confidence interval alongside each point estimate. However, to the best of
our knowledge, the problem of making statistical inferences about the precision gains of covariate-adjusted
estimators has not been formally investigated. In this work, we aim to fill this critical knowledge gap.
When doing so, we focus on the most general case described above, namely that data from an external
study that is reflective of the covariate and control arm outcome distributions are available. Special cases
of this setting include the case where data are available from a previous trial and the control arm data
are used for the external study, and also the case where covariate and outcome data are available from
an observational study.
We primarily consider treatment effect estimands that can be written as contrasts of the distributions of the outcomes within each treatment arm. Most commonly, investigators perform an unadjusted
analysis that uses the empirical distribution to estimate these two arm-specific distributions. One approach to covariate adjustment involves instead estimating these distributions with possibly-misspecified
working models. Specifically, this involves fitting a working parametric model within each arm that conditions on covariates, and then marginalizing over the arm-pooled empirical distribution of the covariates
[21, 3]. In many cases, this approach can result in consistent and asymptotically normal estimators of
the marginal effect of interest, even if the working model is misspecified. Nevertheless, these approaches
3

are typically inefficient when the model is not correct, in the sense that they fail to achieve the asymptotic efficiency bound within a model that only imposes that treatment is randomized [6]. In contrast,
many covariate-adjusted estimators have been proposed recently that achieve the efficiency bound under
appropriate regularity conditions (see, for example, [33, 9] for ordinal outcomes; and [22, 27, 8] for survival outcomes). These approaches usually involve estimating nuisance parameters such as the treatment
mechanism and the outcome regression functions. While being more efficient, these estimators are often
more difficult for practitioners to understand because they cannot typically be framed as corresponding
to a commonly used estimator within a parametric working model. In this paper, we consider both
of the above-described strategies for covariate adjustment, which we refer to as working-model-based
approaches and fully adjusted approaches, respectively.
Our main contributions are as follows:
1. we provide a framework for using external data to identify the efficiency gain in terms of percentage
reduction in sample size needed to achieve a desired power from using covariate-adjusted rather
than unadjusted estimation methods on future clinical trial data;
2. we introduce efficient estimators of this quantity that allow for the incorporation of flexible statistical learning tools to estimate the needed nuisance functions;
3. we present statistical inference procedures to accompany the proposed estimators, namely a Waldtype procedure that requires knowledge of their influence functions but is widely applicable and a
bootstrap procedure that applies only to working-model-based estimators but is easy to implement;
and
4. we evaluate the performance of the proposed methods in a simulation study and an application to
a dataset of Covid-19 patients hospitalized at the University of Washington Medical Center.
This paper is organized as follows. In Section 2, we provide background on efficient estimation in
semiparametric models and describe the relevance of the relative efficiency and local alternatives to
clinical trial settings. In Section 3, we introduce the framework to identify and estimate the efficiency
gain when the outcome is fully observed. We also propose efficient estimators and develop analytical
and bootstrap inference procedures for estimands that are of frequent interest in the cases of continuous
and ordinal outcomes. In Section 4, we study the case where the outcome is partially observed and
4

consider time-to-event outcomes with right-censoring as an example. In Section 5, we demonstrate the
performance of the proposed methods through simulation experiments and an analysis of a real dataset.
Section 6 concludes with a discussion.

2

Review of efficiency theory and its relevance to clinical trial
settings

2.1

Pathwise differentiability and regular and asymptotically linear estimators

The theory of efficient estimation in nonparametric and semiparametric models was described in Pfanzagl
[23] and Bickel et al. [6]. Here we give a brief review of the relevant concepts. Let X denote a generic
data unit with distribution P and let L20 (P ) := {f : EP [f (X)] = 0, varP [f (X)] < ∞}. Let M denote
a statistical model, that is, a collection of distributions of X. We suppose that M contains P . Let
M(P ) denote the collection of all one-dimensional submodels {Pǫ : ǫ ∈ R} ⊆ M that are quadratic
mean differentiable [31] at ǫ = 0 and are such that Pǫ=0 = P . Let SM (P ) denote the collection of all
functions s : X → R for which s is a score function at ǫ = 0 for some submodel contained in M(P ),
and let TM (P ) denote the L20 (P )-closure of the linear span of SM (P ). The subspace TM (P ) of L20 (P )
is referred to as the tangent space. A parameter ψ : M → R is called pathwise differentiable at P in
M if there exists a function D ∈ L20 (P ) such that, for all submodels {Pǫ : ǫ ∈ R} ∈ M(P ), it holds that
∂
∂ǫ ψ(Pǫ )|ǫ=0

= EP [D(X)s(X)], where s is the score function of {Pǫ : ǫ ∈ R} at ǫ = 0. Any such function

D is called a gradient of ψ with respect to M at P . The canonical gradient D∗ is the gradient that lies
in the tangent space TM (P ) — it can be shown that this gradient is unique. We note that the X → R
functions D and D∗ both depend on P .
We refer to an estimator ψ̂ of ψ as a random variable that is a function of an independent and
identically distributed (iid) sample X := {X1 , . . . , Xn } drawn from some distribution. An estimator
ψ̂ is called regular if there exists a real-valued probability distribution L such that, for all submodels
{Pǫ : ǫ ∈ R} in M(P ) and all c ∈ R,
o P −1/2
√ n
n ψ̂ − ψ (Pcn−1/2 ) −−cn−−−→ L.
5

(1)

Importantly, note that, if an estimator is regular, then the distribution L above does not depend on the
choice of submodel in M(P ).
An estimator ψ̂ of ψ(P ) is called asymptotically linear if there exists some function ξP ∈ L20 (P ) such
that

n

ψ̂ − ψ(P ) =

1X
ξP (Xi ) + oP (n−1/2 ).
n i=1

The function ξP is referred to as the influence function of ψ̂. Asymptotically linear estimators are
consistent and asymptotically normal, in the sense that
√
d
→ N (0, σP2 ),
n{ψ̂ − ψ(P )} −

(2)

where σP2 is the variance of ξP (X) when X ∼ P . If ψ̂ is asymptotically linear and ψ is pathwise
differentiable, then ψ̂ is regular if, and only if, ξP is a gradient of ψ with respect to M at P . Among
the collection of gradients, the canonical gradient D∗ has the smallest variance, and is also called the
efficient influence function (EIF). This variance characterizes the efficiency bound of estimating ψ given
the model M with a regular and asymptotically linear (RAL) estimator. An estimator is called efficient
if it is RAL and its influence function is the same as the EIF.
Suppose that we have available an initial estimator P̂ of the distribution. A plug-in estimator is
√
defined as ψ(P̂ ). However, such estimators may not be n-consistent due to the potential bias in the
initial estimators. One way to construct a RAL estimator with influence function D is through onestep estimation [15, 5, 24], which corrects for this bias by using ψ̂ = ψ(P̂ ) + Pn D(P̂ ) where Pn (·) is
the empirical mean. Estimating equations [29, 28] and targeted minimum loss-based estimation [30]
are alternative approaches. These techniques are often used to construct efficient covariate-adjusted
estimators of a treatment effect. Later, we will also use them to estimate the relative efficiency of two
estimators based on external data.

2.2

Local alternatives, relative efficiency, and their relevance to clinical trial
settings

In the context that we consider in this paper, the treatment effect measure that will be estimated with the
future clinical trial data will often correspond to an evaluation ψ(P ) of a pathwise differentiable parameter

6

ψ. In many cases, a primary objective of the forthcoming trial will be to test the null hypothesis that
this quantity is equal to zero against a one-sided alternative, for example, that this quantity is positive.
Suppose that a level α Wald test is performed, which corresponds to evaluating whether zero is smaller
than ψ̂ − n−1/2 z1−α σ̂P based on a RAL estimator ψ̂, where z1−α is the (1 − α)-quantile of a standard
normal distribution and σ̂P is a consistent estimator of σP , as defined in (2). Fix an arbitrary c 6= 0.
As ψ is pathwise differentiable, for any {Pǫ : ǫ ∈ R} in M(P ) with score function s at ǫ = 0, it holds
that ψ(Pcn−1/2 ) = ψ(P ) + cn−1/2 µP,s + o(n−1/2 ), where µP,s := EP [D∗ (X)s(X)]. If P is such that the
null that ψ(P ) = 0 holds, then this shows that n1/2 ψ(Pcn−1/2 ) → cµP,s as n → ∞. If s is such that
µP,s > 0, then we call {Pcn−1/2 }∞
n=1 a sequence of local alternatives — this name is natural given that
ψ(Pcn−1/2 ) > 0 for all n large enough (and so the alternative holds for all n large enough), while also
ψ(Pcn−1/2 ) → 0 as n → ∞ (and so these alternatives are local to the null hypothesis). Because ψ̂ is
RAL, combining (1) and (2) with Slutsky’s theorem implies that

P −1/2
√
nψ̂ −−cn
−−−→ N cµP,s , σP2 ,

(3)

where σP2 is as defined below (2). Let β denote the power for rejecting the null that ψ(P ) = 0 under
sampling from Pcn−1/2 — that is, let
o
n
β := lim Pcn−1/2 0 < ψ̂ − n−1/2 z1−α σ̂P .
n→∞

(4)

Letting Φ(·) denote the cumulative distribution function (CDF) of the standard normal distribution, (3)
implies that β = 1 − Φ(z1−α − cµP,s /σP ) which lies in (α, 1) when the shift in the mean of the limit
normal distribution cµP,s is positive. This gives us a way to quantify the power of the test in a range
of settings where the effect size is small. Hence, effect sizes scaling as n−1/2 are interesting in general
testing problems, given that it is exactly at these effect sizes that the problem is neither asymptotically
trivial (power converging to 1) or impossible (power converging to α). Nevertheless, in many statistical
problems, there may be no a priori reason to believe that the effect size will be of the order n−1/2 .
The setup is quite different in randomized trials. Indeed, these local alternatives are natural to think
about in these settings because, under sampling from such a sequence of alternatives, the asymptotic
power takes some intermediate value between α and 1, which reflects the fact that the sample size in most

7

trials is specified so that a test of the null will have a chosen power β ∗ ∈ (α, 1). To be more concrete,
suppose that {ψ (k) }∞
k=1 is a decreasing sequence of effect sizes that satisfy the alternative hypothesis,
that is, that are such that ψ (k) ↓ 0 as k → ∞. We suppose that these effect sizes arise from some
sequence of distributions {P (k) }∞
k=1 that belong to some submodel M1 := {Pǫ : ǫ ∈ R} ∈ M(P ), so that
ψ(P (k) ) = ψ (k) for each k. Our objective is to establish an expression for the sequence of sample sizes
∗
(k)
{n(k) }∞
iid
k=1 so that, as k → ∞, the power for rejecting the null hypothesis converges to β when n

observations are drawn from P (k) . Let s denote the score of ǫ at 0 in the submodel M1 , and suppose
that µP,s 6= 0. To derive the sequence {n(k) }∞
k=1 , it will be helpful to first find a c such that, when n
iid observations are drawn from Pcn−1/2 , the power converges to the desired β ∗ as n → ∞. Because
(k)
{P (k) }∞
such that P (k) ≈ Pc/√n(k) . As {n(k) }∞
k=1 is a
k=1 ⊆ M1 , it will also be possible to find an n

subsequence of {n}∞
n=1 , it will then be reasonable to expect that, when a sequence of tests is conducted
based on n(k) iid observations sampled from each P (k) , the power for rejecting the null hypothesis will
converge to β ∗ as k → ∞.
We now find the expressions for c and n(k) that were described in the last paragraph. Recalling (4)
and the alternative expression for the power given below that display, we see that, when c = σP (z1−α −
z1−β ∗ )/µP,s , it holds that
n
o
lim Pcn−1/2 0 < ψ̂ − n−1/2 z1−α σ̂P = 1 − Φ(z1−α − cµP,s /σP ) = β ∗ .

n→∞

To find the expression for n(k) , we note that, as ψ is pathwise differentiable, ψ(Pcn−1/2 ) = cn−1/2 µP,s +
o(n−1/2 ) when n is large — here, the little-oh term describes behavior as n → ∞. Hence, P (k) ≈ Pc/√n(k) ,
where n(k) = ⌈(cµP,s /ψ (k) )2 ⌉ = ⌈σP2 (z1−α −z1−β ∗ )2 /(ψ (k) )2 ⌉. Thus, to achieve asymptotic power β ∗ when
sampling n(k) iid observations from P (k) , n(k) must scale proportionally with σP2 . These calculations also
provide a means to compare the sample sizes needed to achieve the same power based on two different
RAL estimators. In particular, suppose that a second RAL estimator is available and its asymptotic
variance is equal to σ̃P2 ≤ σP2 . In this case, the proportional reduction in sample size needed to achieve
power β ∗ when using this estimator rather than the estimator with variance σP2 is approximately equal
to 1 − σ̃P2 /σP2 when k is large. In fact, σ̃P2 /σP2 is often referred to as the relative efficiency of the
RAL estimator with variance σ̃P2 versus the RAL estimator with variance σP2 , and so this proportional
reduction is exactly equal to one minus the relative efficiency of these two estimators.

8

3

When the outcome is fully observed

3.1

Framework to identify the relative efficiency

We now propose a general framework to identify the relative efficiency of covariate-adjusted estimators.
We start by defining notation that we will use to describe the data that will arise in the future clinical
trial. Let A denote the binary treatment, W denote the d-dimensional covariate vector and Y denote the
outcome. We will use superscript t to denote random variables in a future clinical trial. Let P1 be the
conditional distribution of Y t |A = 1 and P0 be the conditional distribution of Y t |A = 0. The treatment
effect is often a functional f of these distributions, i.e., ψ = f (P1 , P0 ) for some f . An example is the
average treatment effect for continuous outcome, where f (P1 , P0 ) = E[Y t |A = 1] − E[Y t |A = 0]. The
observation unit in a trial is X t = (Y t , A, W t ). Randomization implies that A and W t are independent,
and thus that the distribution of X t , denoted by ν, is determined by the marginal distribution of A,
the conditional distribution of Y t given (A = 0, W t = w), the conditional distribution of Y t given
(A = 1, W t = w), and the marginal distribution of W t — we denote these distributions by Π, P0t , P1t ,
and PW , respectively. Let MX consist of all distributions for which A and W t are independent. In
the randomized trial settings that we consider, ν ∈ MX . The adjusted analysis uses X t , while the
unadjusted analysis ignores the covariate W t . Let ψ̂u , ψ̂a and ψ̂m be a specified unadjusted estimator,
fully adjusted estimator and working-model-based adjusted estimator of ψ, respectively. Further suppose
that these estimators are regular and asymptotically linear with influence functions Du , Da and Dm ,
respectively. We note that these influence functions all depend on the underlying distribution ν, but we
will omit this dependence when it is clear from the context.
We assume the following regularity condition holds throughout, which guarantees that the treatment
effects of interest can be estimated using strategies typically employed in randomized trial settings.
Although some of the conditions can in principle be relaxed, they cover most realistic clinical trial
settings.
Condition A1. Treatment is independent of covariates (A ⊥ W t ), the treatment probability Π(A = 1)
falls in (0, 1), and Y and W t both have bounded support under sampling from ν.
Let W ⊆ Rd and Y ⊆ R be bounded and convex sets that contain the support of W t and Y ,
respectively.

9

Our objective is to quantify the relative precision of the specified adjusted and unadjusted estimators.
To do this, we will consider the relative efficiency of these two estimators, defined as the ratio between
the asymptotic variances of the adjusted estimator and the unadjusted estimator under a sharp null
distribution. Though we will focus on the sharp null when introducing these relative efficiencies, these
quantities also correspond to the relative efficiencies under local alternatives (see Section 2.2). Consequently, our apparent restriction to the sharp null setting will in fact not be restrictive at all. Indeed,
from this sharp null setting, we can typically approximate the reduction in sample size needed to achieve
a desired power at all local alternatives that are consistent with the design alternative used to size the
trial (ibid.).
We now define these relative efficiencies. Consider a trial where the sharp null holds, that is, the
treatment has no effect and P1t = P0t . In this case, let P denote the joint distribution of (Y t , W t ),
determined by the pair (P1t , PW ). Under Condition A1, ν is equal to the product measure P Π in
this sharp null setting. The relative efficiency of the fully adjusted estimator compared to that of the
unadjusted estimator is defined as

φa (P ) :=

EP Π [Da (P Π)(X t )2 ]
Eν [Da (ν)(X t )2 ]
=
,
t
2
EP Π [Du (P Π)(X ) ]
Eν [Du (ν)(X t )2 ]

(5)

whereas the analogous quantity for the working-model-based estimator is defined as

φm (P ) :=

Eν [Dm (ν)(X t )2 ]
EP Π [Dm (P Π)(X t )2 ]
=
.
t
2
EP Π [Du (P Π)(X ) ]
Eν [Du (ν)(X t )2 ]

(6)

Although in general ν depends on both Π and P , we define relative efficiencies as functions of P only.
As we will show, in many cases that are of interest in practice, the relative efficiency does not depend
on Π. Even in cases where it does depend on Π, the investigator in a trial would have control over the
treatment distribution Π in the trial setting, and the only unknown component would still be P .
The local alternatives we consider allow for a variety of perturbations to the underlying distribution.
The direction of these perturbations is described by their scores, which belong to the tangent space
TMX (ν). This tangent space decomposes into three subspaces, corresponding to the marginal distribution
of W t , the distribution of treatment A, and the conditional distribution of Y t |A, W t . The score in a
smooth submodel can lie in one or more subspaces, which means that the local alternative can perturb

10

one or more of the four components of ν, namely Π, P0t , P1t , and PW . For an example of how these
perturbations may impact ν, consider the special case of the average treatment effect ψ = E[Y t |A =
1] − E[Y t |A = 0] that was introduced at the beginning of this section. We note that ψ = EPW [c(W t )],
where c(w) := E[Y t |A = 1, W t = w] − E[Y t |A = 0, W t = w] is the conditional average treatment effect
function. Here, ψ will be zero when this function is zero for all values of the covariates. Now, for any
L2 (PW ) integrable function f , there exists a sequence of distributions {νn }∞
n=1 along a smooth submodel
whose score perturbs the conditional distribution of Y t |A, W t in such a way that the conditional average
treatment effect function of νn is equal to n−1/2 f (w). This sequence of distributions will constitute a
local alternative whenever EPW [f (W t )] > 0. There are also local alternatives that perturb the covariate
distribution. For example, consider the case where the conditional average treatment effect c(w) is not
everywhere zero but is such that EPW [c(W t )] = 0. In this case, there are local alternatives that perturb
the marginal distribution PW but do not modify the conditional average treatment effect.
We now describe conditions that we use to identify the relative efficiency φa (P ) and φm (P ) using the
external data that are available at the trial planning stage. When doing this, we assume that P ∈ M,
where M is a locally nonparametric model of all distributions of (Y t , W t ), that is, a model where the
tangent space at P is L20 (P ). Let X = (Y, W ) be the data unit in the external dataset, which we assume
consists of n iid draws from some distribution. The identifiability condition that we consider imposes
that the external data should accurately reflect the distribution of covariate and outcomes in future trials
where treatment has no effect.
Condition A2. A random variate X = (Y, W ) from the external dataset has distribution P .
Under this condition, the relative efficiencies in (5) and (6) can be estimated based on the external
data.

3.2

Estimating the relative efficiency

We now consider estimating relative efficiency for certain treatment effect estimands that are of particular
interest in many clinical trials. We focus primarily on continuous and ordinal outcomes in this section.
For the examples we consider, the asymptotic variances of the adjusted and unadjusted estimators
factorize into a product of two terms, one that depends on Π only and another that depends on P only.
Moreover, the term that depends only on Π is the same for both the adjusted and unadjusted estimators,
11

and the relative efficiency is a function of P only. In particular, (5) and (6) now take the following forms,
2
φa (P ) = σa2 (P )/σu2 (P ), φm (P ) = σm
(P )/σu2 (P ).

(7)

The exact forms of σa , σm and σu depend on the treatment effect estimand and the specified estimators
but do not depend on Π. Some specific examples are presented in the remainder of this section.
3.2.1

Continuous outcomes and average treatment effect

To illustrate the idea, we start with a simple example where the outcome is continuous and we are
interested in the average treatment effect, defined as ψ = E[Y t |A = 1] − E[Y t |A = 0]. Let nt denote the
sample size of the future trial dataset.
The unadjusted estimator that we consider corresponds to the difference between the arm-specific
means, namely
t

ψ̂u =

n
X
i=1

t

t

Ai Yit /

n
X
i=1

Ai −

n
X
i=1

t

(1 −

Ai )Yit /

n
X
(1 − Ai ).
i=1

For the fully adjusted estimator, we consider the augmented inverse probability weighted (AIPW) estimator, namely

nt 
1 X Ai {Yit − r̂1 (Wit )} (1 − Ai ){Yit − r̂0 (Wit )}
t
t
−
+ r̂1 (Wi ) − r̂0 (Wi ) ,
ψ̂a = t
n i=1
π̂(Wit )
1 − π̂(Wit )

(8)

where r̂a (w) is an estimator of the conditional mean function ra (w) := E[Y t |A = a, W t = w] and π̂ is an
estimator of the treatment mechanism π(w) := P (A = 1|W t = w). In randomized trials, the treatment
√
mechanism can be estimated with π̂, the empirical marginal of A, which is n-consistent, and the AIPW
estimator is efficient provided that r̂a is consistent and satisfies appropriate conditions. The estimator
will be consistent and asymptotically normal even if r̂a is inconsistent but has an appropriately defined
limit.
For the working-model-based adjusted estimator, we consider linear models, which are commonly
used by practitioners for continuous outcomes [10]. Specifically, we fit an arm-specific linear model for
the outcome regression, which assumes that
E[Y t |A = a, W t = w] = αa + βa⊤ w,
12

and denote by α̂a , β̂a the fitted coefficients. To estimate the average treatment effect, we marginalize the
fitted values over all covariates and take the difference between treatment arms,
t

⊤

ψ̂m = α̂1 − α̂0 + (β̂1 − β̂0 )

n
X

Wit /nt .

i=1

We note again that the consistency and asymptotic normality of this estimator does not rely on the
arm-specific linear models being correct.
The following lemma gives the forms of the relevant variances in the definition of relative efficiency
in this setting.
Lemma 1. Let (Y, W ) ∼ P . Suppose that the appropriate regularity conditions hold such that the AIPW
estimator φ̂a is efficient. Then, for the above ψ̂u , ψ̂a and ψ̂m , we have that σu2 (P ) = varP (Y ), σa2 (P ) =


2
EP [varP (Y |W )] and σm
= EP (Y − α∗ − W ⊤ β ∗ )2 , where (α∗ , β ∗ ) is the minimizer of EP [(Y − α −
W ⊤ β)2 ] over (α, β) ∈ R × Rd .

We now estimate these variances using external data {(Yi , Wi ), i = 1, . . . , n}. Specifically, we use the
Pn
sample variance for the unadjusted variance, σ̂u2 = i=1 (Yi − Ȳ )2 /n, where Ȳ is the overall mean of the

outcome. Let r̂(w) be an estimator of r(w) := EP [Y |W = w], then we estimate the adjusted variance by
Pn
Pn
2
2
is σ̂m
= i=1 (Yi − α̂ − Wi⊤ β̂)2 /n,
σ̂a2 = i=1 {Yi − r̂(Wi )}2 /n. Finally, a natural plug-in estimator of σm

where (α̂, β̂) are the coefficients in the linear regression of Y on W . We estimate the relative efficiencies
2
by φ̂a = σ̂a2 /σ̂u2 and φ̂m = σ̂m
/σ̂u2 .

For a generic W → R function f , define its (squared) L2 (PW ) norm as kf k2L2 (PW ) :=

R

f (w)2 dPW (w).

The following theorem establishes the asymptotic linearity of φ̂a and φ̂m under appropriate conditions.
Theorem 1. Suppose that Conditions A1 and A2 hold. Suppose, in addition, that the random function
r̂ : W → Y is such that kr̂ − rkL2 (PW ) = oP (n−1/4 ) and belongs to some fixed P -Donsker class F of
functions with probability tending to one. Then, φ̂a is an efficient estimator of φa and φ̂m is an efficient
estimator of φm .
3.2.2

Ordinal outcomes

Now suppose that the outcome is ordinal and takes value in {1, 2, . . . , K}. Dichotomous outcomes
correspond to the special case where K = 2. Let Fa (k) := P (Y t ≤ k|A = a) denote the treatment-specific
13


CDF. The treatment effect estimands we consider can all be written as ψ = g {F0 (k), F1 (k)}K−1
for
k=1

some real-valued function g.

The proportional odds model [19] is a commonly used parametric model for ordinal outcomes. Here we
use a treatment-specific proportional odds model as our working parametric model. For k ∈ {1, . . . , K −
1}, the model assumes that P (Y t ≤ k|A = a, W t = w) = θαa ,βa (k, w), where
logit θαa ,βa (k, w) = αa (k) + βa⊤ w.
The above reduces to a logistic regression when the outcome is dichotomous. Let (α̂a , β̂a ) be the coefficients, fitted by minimizing the following empirical risk function:

Ln,a (α, β) = −

K−1
nt
XX
k=1 i=1

h

I{Ai = a} I{Yit ≤ k} log θα,β (k, Wit )

i

+ I{Yit > k} log 1 − θα,β (k, Wit ) ,

(9)

Pnt
where I{·} is the indicator function. In the special case that i=1 I{Ai = a, Yit ≤ k} = 0 for some k, we
Pnt
let α̂a (k) = −∞. Similarly, in the case that i=1 I{Ai = a, Yit > k} = 0 for some k, we let α̂a (k) = ∞.
For such cases we use the conventions that logit−1 (−∞) = 0, logit−1 (∞) = 1, and 0 log(0) = 0. The
Pnt
treatment-arm-specific CDFs are estimated by F̂a (k) = i=1 θα̂a ,β̂a (k, Wit )/nt . In addition, we define
(α∗a , βa∗ ) as the minimizer of E[Ln,a (α, β)] over RK−1 × Rd .

We first establish the RAL property of F̂a (k), which holds even when the proportional odds model
is misspecified. Let θa (k, w) := P (Y t ≤ k|A = a, W t = w) be the true outcome regression, and let
θa∗ (k, w) = θα∗a ,βa∗ (k, w) be the best model approximation to the true outcome regression according to the
population analogue of the risk in (9). Note that θa∗ (k, w) can be different from θa (k, w) in the presence
of misspecification.
Lemma 2. Suppose that Condition A1 holds and that (α̂a , β̂a ) is estimated by minimizing (9), then
F̂a (k) is an asymptotically linear estimator of Fa (k), for k ∈ {1, . . . , K − 1}. Its influence function is
given by
IFFa (k) (y t , ã, wt ) =


I{ã = a} 
I{y t ≤ k} − θa∗ (k, wt ) + θa∗ (k, wt ) − Fa (k).
Π(A = a)

Following [3], we focus on three treatment effect estimands that are often of interest.
14

Difference in mean (DIM) is defined as ψ = E [u(Y t )|A = 1] − E [u(Y t )|A = 0] for a pre-specified monotone transformation u(·). This reduces to the average treatment effect when u(·) is the identity function.
The unadjusted estimator is the difference between the arm-specific sample means,
t

ψ̂u =

n
X

u(Yit )Ai

nt
.X
i=1

i=1

t

Ai −

n
X
i=1

u(Yit )(1

nt
.X
(1 − Ai ).
− Ai )
i=1

Instead of using sample means, the adjusted estimator based on proportional odds model computes
means with respect to the estimated CDFs F̂a .

ψ̂m =

K−1
X
k=1

{u(k) − u(k + 1)}{F̂1 (k) − F̂0 (k)}.

Finally, we define an AIPW estimator similarly to (8), but with Y t replaced by u(Y t ). We denote this
estimator as ψ̂a . As in the previous section, this estimator achieves the semiparametric efficiency bound
when the treatment mechanism is estimated with the marginal proportion of treatment and the outcome
regression is consistently estimated. For the above three estimators, the variances in (7) are given in the
following lemma.
Lemma 3. Let (Y, W ) ∼ P . Suppose that the appropriate regularity conditions hold such that the
AIPW estimator φ̂a is efficient. Then, for the above ψ̂u , ψ̂a and ψ̂m , we have that σu2 (P ) = varP [u(Y )],
σa2 (P ) = EP [varP (u(Y )|W )], and


2
σm
(P ) = EP 

K−1
X
k=1

{u(k) − u(k + 1)} [I{Y ≤ k} − θ∗ (k, W )]

!2 

,

where θ∗ (k, w) = θα∗ ,β ∗ (k, w) and (α∗ , β ∗ ) maximizes the following objective:

EP

"K−1
X
k=1

#

I{Y ≤ k} log {θα,β (k, W )} + I{Y > k} log {1 − θα,β (k, W )} .

(10)

We now propose estimators of these quantities for settings where external data are available. Let r̂(w)
be an estimator of r(w) := EP [u(Y )|W = w], the conditional mean of u(Y ), and let ūn be the sample

15

mean of u(Y ). We estimate the unconditional and conditional variances by
n

σ̂u2 =

n

1X
1X
{u(Yi ) − ūn }2 , σ̂a2 =
{u(Yi ) − r̂(Wi )}2 .
n i=1
n i=1

2
To estimate σm
, we fit the proportional odds model by maximizing the empirical counterpart of (10),

and let (α̂, β̂) denote the fitted coefficients. We then construct a plug-in estimator
n

2
σ̂m

1X
=
n i=1

K−1
X
k=1

h
i
{u(k) − u(k + 1)} I{Yi ≤ k} − θα̂,β̂ (k, Wi )

!2

.

2
Finally, we estimate the relative efficiency by φ̂a = σ̂a2 /σ̂u2 and φ̂m = σ̂m
/σ̂u2 .

In the upcoming theorem, we let u(Y) denote the convex hull of {u(y) : y ∈ Y}.
Theorem 2. Suppose that Conditions A1 and A2 hold. Suppose, in addition, that the random function
r̂ : W → u(Y) is such that kr̂ − rkL2 (PW ) = oP (n−1/4 ) and belongs to some fixed P -Donsker class F of
functions with probability tending to one. Then φ̂a is an efficient estimator of φa . Moreover, φ̂m is an
efficient estimator of φm .
Exact forms of the influence functions of φ̂a and φ̂m are given in Appendix B.

The Mann-Whitney estimand (MW) is defined as ψ = P (Y1t > Ỹ0t ) + P (Y1t = Ỹ0t )/2, for two independent
variables Y1t ∼ P1 and Ỹ0t ∼ P0 . It is the probability that a randomly chosen individual’s outcome under
treatment is larger than another randomly chosen individual’s outcome under control plus one half times
the probability that the two outcomes are equal. Define h(x, y) = I{x > y} + I{x = y}/2. Then the
RR
Mann-Whitney parameter can be alternatively written as ψ =
h(x, y)dP1 (x)dP0 (y). This alternative

definition suggests the following unadjusted estimator
t

ψ̂u =

t

n X
n
X
i=1 j=1




nt
nt

. X
X

Aj  ,
Ai (1 − Aj )h(Yit , Yjt )
Ai  nt −


i=1

and the following working-model-based estimator

ψ̂m =

Z Z

h(x, y)dP̂1 (x)dP̂0 (y),

16

j=1

where P̂a is the distribution with CDF F̂a (k). In addition, let ψ̂a be the covariate-adjusted estimator in
Vermeulen et al. [33] for the MW parameter, which is efficient under appropriate regularity conditions.
Lemma 4. Let (Y, W ) ∼ P , and define ηP (k) = P (Y < k) + P (Y = k)/2 and pk = P (Y = k).
Suppose that the appropriate regularity conditions hold such that φ̂a is an efficient estimator of the MW
P
3
parameter. Then, for the above ψ̂u , ψ̂a and ψ̂m , we have that σu2 = varP [ηP (Y )] = (1 − K
k=1 pk )/12;

σa2 = EP [varP (ηP (Y )|W )]; and



2
σm
(P ) = EP 

K−1
X
k=1

{ηP (k) − ηP (k + 1)} [I{Y ≤ k} − θ∗ (k, W )]

!2 

.

We now propose estimators for these quantities. Unlike in the case of the DIM estimand, ηP (·)
depends on the unknown marginal distribution of Y and needs to be estimated from the external data.
Pn
Pk
A simple estimator is based on the empirical distribution, p̂k = i=1 I{Yi = k}/n and η̂(k) = j=1 p̂j −
PK
p̂k /2. The unadjusted variance can be estimated via the plug-in estimator σ̂u2 = (1 − k=1 p̂3k )/12. Let

r̂(w) be an estimator of the conditional mean r(w) := EP [ηP (Y )|W = w], and we estimate the adjusted
Pn
2
variance by σ̂a2 = i=1 {η̂(Yi ) − r̂(Wi )}2 /n. Finally, a natural plug-in estimator for σ̂m
is given by
2
σ̂m

=

n
X
i=1

K−1
Xn
k=1

h
io
[η̂(k) − η̂(k + 1)] I{Yi ≤ k} − θα̂,β̂ (k, Wi )

!2

/n,

where (α̂, β̂) is again the fitted coefficients from the proportional odds model, by maximizing the sample
2
counterpart of (10). The relative efficiency can be estimated as φ̂a = σ̂a2 /σ̂u2 and φ̂m = σ̂m
/σ̂u2 .

The next theorem establishes the asymptotic properties of these estimators.
Theorem 3. Suppose that Conditions A1 and A2 hold. Suppose, in addition, that the random function
r̂ : W → R is such that kr̂ − rkL2 (PW ) = oP (n−1/4 ) and belongs to some fixed P -Donsker class F of
functions with probability tending to one. Then, φ̂a is an efficient estimator of φa and φ̂m is an efficient
estimator of φm .
The influence functions of ψ̂a and ψ̂m are given in Appendix B.

The log odds ratio (LOR) is defined as ψ =

PK−1
k=1

{logit F1 (k) − logit F0 (k)}/(K − 1), which is an

average of the cumulative log odds ratios [9]. In general, one can also consider a weighted average.
17

Employing this definition for the LOR ensures that the LOR is well-defined even in settings where a
proportional odds assumption fails.
The unadjusted estimator is given by
Pnt
K−1
o
I{Yit ≤ k, Ai = a}
1 Xn
.
logit F̃1 (k) − logit F̃0 (k) , where F̃a (k) = i=1
ψ̂u =
Pnt
K −1
I{Ai = a}
i=1

k=1

The working-model-based adjusted estimator ψ̂m replaces F̃a (k) with the proportional odds model-based
estimator F̂a (k) that was defined earlier. Finally, let ψ̂a be the covariate-adjusted estimator proposed
in Dı́az et al. [9], which achieves the semiparametric efficiency bound under regularity conditions. The
following lemma gives the forms of the relevant variances.
Lemma 5. Let (Y, W ) ∼ P , F (k) := P (Y ≤ k), and
ζ(Y ) :=

K−1
X
1
I{Y ≤ k}
.
K −1
F (k){1 − F (k)}
k=1

Suppose that the appropriate regularity conditions hold such that φ̂a is an efficient estimator of the LOR.
Then, for the above ψ̂u , ψ̂a and ψ̂m , we have that
σu2 (P ) = varP [ζ(Y )], σa2 (P ) = EP [varP (ζ(Y )|W )] ,

!2 
K−1
∗
X
θ
(k,
W
)
1
2
.
σm
(P ) = EP  ζ(Y ) −
K −1
F (k){1 − F (k)}
k=1

Let θ̂(k, w) be an estimator of the true conditional distribution function θ(k, w) := P (Y ≤ k|W = w)
in the setting where external data are available. We estimate the relative efficiencies by φ̂a = σ̂a2 /σ̂u2
2
and φ̂m = σ̂m
/σ̂u2 , with the variance estimators all taking the following form with certain choice of the

estimator θ̂c :

where F̃ (k) :=

"K−1
#2
n
1 X X I{Yi ≤ k} − θ̂c (k, Wi )
,
n i=1
F̃ (k){1 − F̃ (k)}
k=1
Pn

i=1

I{Yi ≤ k}/n. Specifically, for σ̂a2 , θ̂c (k, w) is replaced with θ̂(k, w); for σ̂u2 , we use

2
F̃ (k); and, for σ̂m
, we take θ̂c (k, w) = θα̂,β̂ (k, w).

Theorem 4. Suppose that Conditions A1 and A2 hold and that there exists a constant δ > 0 such that
18

δ < F (k) < 1 − δ for all k ∈ {1, . . . , K − 1}. Suppose, in addition, that, for all k ∈ {1, . . . , K − 1}, the
random function θ̂(k, ·) : W → R is such that kθ̂(k, ·) − θ(k, ·)kL2 (PW ) = oP (n−1/4 ) and belongs to some
fixed P -Donsker class F of functions with probability tending to one. Then, φ̂a is an efficient estimator
of φa . Moreover, φ̂m is an efficient estimator of φm .
For all of the aforementioned treatment effect estimands, φa ∈ (0, 1], since the fully adjusted estimator
achieves the semiparametric efficiency bound. However, φm might be larger than 1 if the proportional
odds model is far from the truth.
Wald-type intervals are a standard approach for constructing confidence intervals when an asymptotically linear estimator φ̂ of φ is available. Specifically, a (1 − α)-CI is given by φ̂ ± n−1/2 z1−α/2 Pn τ̂ 2 ,
where z1−α/2 is the (1 − α/2)-quantile of a standard normal distribution and τ̂ is the influence function
of φ̂ except that we replace unknown quantities with consistent estimates. However, there are certain
cases where, even though such consistent estimates are used, the Wald-type confidence interval will not
provide asymptotically valid coverage. A key time when this challenge arises occurs when the limiting
distribution of a RAL estimator is degenerate because the influence function is almost everywhere zero,
which will often occur in our setting when the relative efficiency is one. One such example arises in
estimating the relative efficiency of the fully adjusted estimator to the unadjusted estimator for the
ATE or DIM estimands. In this case, the influence function of φ̂a is almost everywhere zero when
E[u(Y )|W = w] = E[u(Y )] for almost all w. While a Wald-type interval will typically achieve asymptotically valid coverage outside of these degenerate cases, there is no way to know in advance whether or not
P is such that degeneracy will occur. To overcome this challenge, we propose an alternative approach
that yields a confidence set that achieves the desired coverage regardless of whether degeneracy occurs.
Most importantly, the resulting confidence sets are valid regardless of whether or not the true relative
efficiency is, in fact, one.
The proposed confidence set is constructed as follows. Suppose that we have available a valid level α
test of the null hypothesis H0 : φa = 1 (φm = 1) — one such test based on sample splitting is given in
Appendix E. Denote the Wald confidence interval by Iwald . We first test this null hypothesis. If we do
not reject it, we take the 1 − α confidence set to be Iwald ∪ {1}. If, instead, the null hypothesis is rejected,
we take the confidence set to be Iwald . At first glance, it seems that the proposed approach may fail to
achieve valid coverage given that it uses the same data twice — once to test the null hypothesis and a

19

second time to form the Wald-type interval. Nevertheless, as we show in Appendix E, the confidence set
resulting from this procedure in fact has at least 1 − α asymptotic probability of covering the truth. It
may happen that Iwald and {1} are disjoint. In this case, a disconnected confidence set Iwald ∪ {1} can
be reported or, if this is considered undesirable, the convex hull of this confidence set can be taken to
form a confidence interval.

3.3

Bootstrap procedure for working-model-based estimators

The inferential procedures described in the preceding subsection are based on closed-form expressions
for the relative efficiency parameter in several problems and knowledge of the corresponding efficient
influence functions. On the one hand, now that these expressions have been calculated, the estimators
that we have presented can be used in any problems in which the relative efficiency takes this form. On
the other hand, if a new effect estimand or working parametric model is of interest in a future setting,
then new analytical calculations will need to be conducted to derive the closed-form expression for the
relative efficiency parameter and develop corresponding estimators and confidence intervals. Here, we
propose an automated double bootstrap procedure that avoids the need to perform these potentiallytedious analytic calculations. When doing so, we focus on the case where the goal is to infer about the
relative efficiency of a new working-model-based adjusted estimator, that is, we focus on φm . The reason
for this choice is discussed at the end of this subsection.
Before describing this procedure, we first investigate the applicability of a more traditional, one-layer
bootstrap procedure. Suppose that the relative efficiency parameter Φm : M → R+ is sufficiently smooth
so that a plug-in estimator of Φm (P ) based on the empirical distribution is asymptotically linear [see,
e.g., Theorem 20.8 in 31]. In this case, we can construct a plug-in estimator based on the empirical
distribution Pn of a sample of iid observations. We denote this plug-in estimator by Φm (Pn ) and note
that all the estimators proposed so far for φm correspond to plug-in estimators of this form. In a
traditional setting where the bootstrap would be applied, a closed-form expression for the functional Φm
would be available and so would be the plug-in estimator Φm (Pn ), and the goal would be to derive a
corresponding confidence interval. In particular, let the n entries of X = (Xk )nk=1 correspond to an iid
sample of external data, where Xk = (Yk , Wk ). We sample from X with replacement B1 times, to form
the bootstrap resamples X ∗i of size n for i = 1, . . . , B1 . Letting P∗n,i denote the empirical distribution of

20

the observations in X ∗i , we could then use the empirical standard deviation of Φm (P∗n,i ), i = 1, . . . , B1 , as
the standard error estimate used to construct a confidence interval centered around Φm (Pn ). Though this
traditional bootstrap approach is useful in that it avoids explicitly computing the influence function of
Φm (Pn ), it does not fully avoid the aforementioned analytic calculations. Indeed, in many cases, deriving
the plug-in estimator will itself require deriving the explicit form of the relative efficiency parameter,
which in turn relies on computing inefficient and efficient gradients of the treatment effect estimand in
the model MX . Computing these gradients requires specialized calculations that are unfamiliar to many
practitioners.
To avoid this challenge, we approximate the plug-in estimator with an alternative estimator φ̃ that
can be obtained in a fully automated fashion. Specifically, we propose to use an additional layer of
resampling to approximate φm . Evaluating the resulting estimation strategy only requires having access
to the external data and the treatment effect estimator that will be used to analyze data from the future
clinical trial.
∗
1
Again let {X ∗i }B
i=1 be the first layer resamples, and in addition define X 0 := X. For each i ≥ 0, we

then let X̃ ij , j = 1, . . . , B2 , denote an iid sample of size N from the product measure P∗n,i Π, where Π is
the known distribution of treatment. To simulate X̃ ij , we first draw an iid sample of size N from the
empirical distribution P∗n,i and then append a random draw of the treatment vector, which is a tuple
consisting of N iid draws from a Bernoulli(π) distribution.
For each X ∗i , we will construct an estimator φ̃(X ∗i ) using the collection of second-layer resamples.
Specifically, for each (i, j), we compute the adjusted and unadjusted estimators based on the sample
P 2 ij
PB2 ij
i
ij
i
= B
and ψ̂uij , respectively. Define ψ̄m
X̃ ij , which we denote as ψ̂m
j=1 ψ̂m /B2 and ψ̄u =
j=1 ψ̂u /B2 .
A stochastic approximation of the parameter evaluation Φm (P∗n,i ) is then given by

φ̃n (X ∗i ) =

B2
X
j=1

ij
i 2
(ψ̂m
− ψ̄m
)

B2
.X
j=1

(ψ̂uij − ψ̄ui )2 .

(11)

2
We note that φ̃n (X ∗i ) depends on {X̃ ij }B
j=1 , and therefore also on B2 and N — we omit these depen-

dencies in the notation. A Wald-type bootstrap confidence interval can be constructed by using the
empirical standard deviation of φ̃n (X ∗i ) over i = 1, . . . , B1 as the standard error and φ̃n (X) := φ̃n (X ∗0 )
as the center. This double bootstrap procedure is summarized in Algorithm 1 in Appendix F.
We now provide some intuition behind why the above-described double bootstrap procedure is ex21

pected to work. We then provide a theorem that formalizes these arguments. First, we observe that
the double bootstrap procedure is analogous to a traditional single-layer bootstrap, except that we replace the plug-in estimator with an estimator φ̃n , which itself is defined through an additional layer of
bootstrap. Intuitively, if φ̃n (X ∗i ) is close enough to the plug-in estimator Φm (P∗n,i ) on all X ∗i , we would
expect that using the stochastic approximation instead of the plug-in makes little difference and the
procedure works similarly as the traditional bootstrap works. We now give heuristic arguments showing
that φ̃n (X ∗i ) and Φm (P∗n,i ) should indeed be close, in the sense that
φ̃n (X ∗i ) − Φm (P∗n,i ) = oP∗n,i (n−1/2 ).
To see this, for an arbitrary j ∈ {1, . . . , B2 }, consider a general asymptotically linear estimator ψ̂ of the
treatment effect that satisfies

ψ̂(X̃ ij ) −

ψ(P∗n,i Π)

N
l
1 X
D(P∗n,i Π)(X̃ ij ) + Remi ,
=
N

(12)

l=1

l

where X̃ ij is the l-th observation in X̃ ij . In our upcoming theorem, we will assume that Rem is
negligible in an appropriate sense. Suppose that we take sufficiently many samples from P∗n,i Π — that
is, that B2 is sufficiently large — so that the Monte-Carlo error from the second bootstrap layer is
√
negligible. We can then accurately approximate the sampling distribution of N {ψ̂(X̃ ij ) − ψ(P∗n,i Π)}
2
under P∗n,i by the empirical distribution of {ψ̂(X̃ ij )}B
j=1 . Applying these arguments at ψ̂u and ψ̂m
√
2
2
2
2
suggests that φ̃n (X ∗i ) accurately approximates σ̃m,i
/σ̃u,i
, where σ̃u,i
= varP∗n,i [ N ψ̂u (X̃ ij )] and σ̃m,i
=
√
varP∗n,i [ N ψ̂m (X̃ ij )] are the variances of the sampling distributions where X̃ ij is an iid sample from

P∗n,i Π. In addition, provided that N ≫ n so that the remainders in the above linear expansion (12)
2
2
are sufficiently small when ψ̂ is equal to ψ̂u and ψ̂m , the ratio between these variances σ̃m,i
/σ̃u,i
is
2
approximately EP∗n,i [Dm
(P∗n,i Π)]/EP∗n,i [Du2 (P∗n,i Π)] = Φm (P∗n,i ). As a result, we expect φ̃n (X ∗i ) to be

reasonably close to the plug-in estimator Φm (P∗n,i ).
The upcoming theorem formalizes the heuristic argument given in the previous paragraph. Before
giving this result, we define a key differentiability concept that is useful for establishing theoretical
guarantees for bootstrap procedures. Let D denote the space of càdlàg Rd+1 → R functions equipped
with the uniform norm. Let ρ be the operator that takes as input a CDF on Rd+1 and outputs the

22

corresponding distribution on Rd+1 . Also let DM := {ρ−1 (P ) : P ∈ M}, where ρ−1 (P ) denotes the CDF
of P . In what follows, we will call a parameter φ : M → R Hadamard differentiable if the composition
φ ◦ ρ : DM → R, defined on the subset DM of the normed space D, is Hadamard differentiable in the
sense defined in Chapter 20.2 of [31].
We will assume that the following conditions hold:
2
Condition B1. Both σu2 (·) and σm
(·) are Hadamard differentiable;

Condition B2. There exists a γ ∈ (1/2, ∞) such that the remainder Rem1 in Eq. 12 is such that
E[varP∗n,1 (N γ Rem1 )] is uniformly bounded in n, where the expectation is over the draw of the bootstrap
sample P∗n,1 and X1 , X2 , . . .;
p

2
2
Condition B3. B2 grows with n in such a way that n1/2 {σ̃m,1
/σ̃u,1
− φ̃n (X ∗1 )} → 0 given (X1 , X2 , . . .) =

(x1 , x2 , . . .) for almost every (x1 , x2 , . . .);
Condition B4. N ≫ n1/(2γ−1) in the sense that n1/(2γ−1) /N → 0 as n → ∞.
We are now ready to state the theorem.
Theorem 5. Under Conditions A1-A2 and Conditions B1-B4, we have that

√
n{φ̃n (X ∗1 ) − Φm (Pn )}

converges in distribution to Φ′m (G), given X1 , X2 , . . ., in probability, where Φ′m is the Gâteaux derivative
of the functional Φm and G is a mean-zero Gaussian process with covariance cov(Gf1 , Gf2 ) = P (f1 f2 ) −
P f1 P f2 .
The proof is a modification of the proof of Theorem 23.9 in Van der Vaart [31], and is given in
Appendix B. To approximate the limiting distribution Φ′m (G) given in the above theorem, Algorithm 1
√
uses the empirical distribution of n{φ̃n (X ∗i )− φ̃n (X)} across the B1 bootstrap replicates X ∗1 , . . . , X ∗B1 .
We now discuss the conditions of Theorem 5. Condition B1 ensures the Hadamard differentiability
of the relative efficiency parameter Φm (·), which is used in most standard sets of sufficient conditions
for the validity of bootstrap methods. We can establish these Hadamard differentiability conditions by
noting that the variance is essentially the mean of a function indexed by nuisance parameters, which
themselves are transformations of some population means. We use the Mann-Whitney estimand in the
ordinal outcome case as an example. The variance of the adjusted estimator takes the form


EP 

K−1
X
k=1

[bP (k){I{Y ≤ k} − θα∗ P ,βP
23

!2 
(k, W )}]  .

Here b, α, β are nuisance parameters, which are defined, either explicitly or implicitly, with a set of
population means. The mean functional is Hadamard differentiable, for example, when the support is
bounded. One can then apply the chain rule of Hadamard differentiability as in [14].
Condition B2 ensures that the remainder term in the asymptotic linear expansion is sufficiently small.
For the examples we have considered, it is possible to show that we can take γ = 1 under mild conditions.
Conditions B3 and B4 require that the user selects sufficiently large values for B2 and N . Condition B3
2
2
places a restriction on the Monte Carlo approximation φ̃n (X ∗1 ) of σ̃m,1
/σ̃u,1
. In most cases, this condition

will hold provided the number of second-layer bootstrap samples goes to infinity faster than does n, that
is, so that n/B2 → 0. Condition B4 places a restriction on the sample size N of each second-layer
bootstrap sample. When γ = 1, this condition requires that these samples be of a larger order than
the original sample size n. Taken together, Conditions B3 and B4 impose that sufficient computing
power must be available to compute the estimator ψ̂ approximately B1 B2 times on samples of size N
— in contrast, the analytic method in the previous section only required fitting the estimator φ̂m (and
estimating its standard error) once on a sample of size n.
We conclude by noting that we can define a double bootstrap procedure analogous to Algorithm 1 for
the estimation of the relative efficiency of a fully adjusted estimator φa . However, our arguments cannot
generally be used to establish the validity of double bootstrap confidence intervals for φa . The problem
arises because the asymptotic variance of the fully adjusted estimator often involves a regression function
of the outcome against the covariate. Because the statistical model is nonparametric up to knowledge of
the treatment probability, this dependence will often make it so that the parameter σa2 (·) is not Hadamard
differentiable, and so the theoretical guarantee presented above for our double bootstrap procedure may
not apply. It is therefore an open question as to whether the double bootstrap will yield valid confidence
intervals for the relative efficiency of fully adjusted estimators.

4

When the outcome is partially observed

We now consider settings where the outcome in the trial is only partially observed. For this purpose, we
use the notion of coarsening-at-random [12, 13]. Let Z t = (T t , A, W t ) be the full data unit in the trial,
C t be a coarsening variable, and X t = Ga (Z t , C t ) be the observation unit in the trial where Ga (·, ·) is
some many-to-one function. We further assume that, under Ga , the covariate W t is fully observed. The

24

adjusted analysis estimates the treatment effect ψ based on X t . We can write X t as (X̃ t , W t ), where
X̃ t represents the components in X t that are not covariates and W t is the covariate vector. Define a
function c(·) such that c(X t ) := X̃ t , and write Gu to denote the composition c ◦ Ga . The unadjusted
analysis ignores the covariate information, which is equivalent to working with the observation unit
Xut = Gu (Z t , C t ) rather than with X t . The relative efficiency, defined in terms of the variances of the
unadjusted and adjusted estimators, is interesting only when both analyses give consistent estimators.
Thus, we will assume that both conditional distributions Ga (Z t , C t )|Z t and Gu (Z t , C t )|Z t satisfy the
coarsening-at-random assumption, so that both the unadjusted and adjusted analyses are asymptotically
unbiased for the treatment effect.
Let ν denote the distribution of X t . We again define the relative efficiencies by focusing on trials
under the sharp null, that is, the conditional distributions T t |A = 0, W t and T t |A = 1, W t are the
same. We let G(·|a, w) denote the conditional distribution of C t given that (A, W t ) = (a, w). Under the
sharp null, ν is fully characterized by the treatment distribution Π, the conditional distribution of C t
characterized by G, and the joint distribution of (T t , W t ) denoted by P — when we wish to emphasize
this dependence, we write νΠ,G,P . We define the relative efficiencies as

Φa,Π,G (P ) =

EνΠ,G,P [Da (νΠ,G,P )(X t )2 ]
EνΠ,G,P [Dm (νΠ,G,P )(X t )2 ]
,
Φ
(P
)
=
,
m,Π,G
EνΠ,G,P [Du (νΠ,G,P )(Xut )2 ]
EνΠ,G,P [Du (νΠ,G,P )(Xut )2 ]

(13)

where Du , Da and Dm are the influence functions of the unadjusted, fully adjusted and working-modelbased adjusted estimators, respectively. We will often suppress the dependence of these relative efficiencies on Π and G in the notation by writing Φa (P ) and Φm (P ).
We aim to identify and estimate these relative efficiencies from external data available at the trial
planning stage. Like the future trial data, the external data can be subject to coarsening. Let C be the
coarsening variable, and Γ(·, ·) be a many-to-one function. The full data unit in the external dataset is
Z = (T, W ), and the observed data unit is X = Γ(Z, C). Let Q be the distribution of X, induced by
the joint distribution of (Z, C) and the many-to-one function Γ. To identify the relative efficiencies from
the observed external data X, we assume the following condition holds throughout this section. This
condition is similar to Condition A2 and assumes in addition that coarsening-at-random holds in the
external data.
Condition A3. A full data unit in the external data Z = (T, W ) has distribution P , and the conditional
25

distribution Γ(Z, C) | Z satisfies the coarsening-at-random assumption.
Under this condition, it is possible to identify the relative efficiencies in (13) as parameters of the
distribution of the observed external data, and also to show that, under reasonable conditions, these
parameters will be smooth enough so that it should be possible to develop regular and asymptotically
linear estimators based on the external data [Theorem 1.3 in 29].
The external data might be obtained from various settings including observational studies, some
of which are distinct from randomized clinical trials. Consequently, the reasons for coarsening can
be much different from those in the future trial. For example, for time-to-event data, administrative
censoring may account for a large proportion of right censoring in clinical trials, but a lesser proportion
for observational data. Thus, it is often not plausible to assume that we can identify G from the external
data. To overcome this issue, we define the relative efficiencies for a particular G, and the user can
choose a coarsening mechanism that is expected to reflect the setting of a future trial.
In Appendix A, we use the identifiability result stemming from Condition A3 to develop estimators
and confidence intervals for the relative efficiency in settings where there are time-to-event outcomes
with right censoring. In this case, T t is the time to some event of interest and C t is the censoring
time in the trial. The full data unit Z t is (T t , A, W t ), and the observation unit X t is (Y t , ∆t , A, W t ),
where Y t = min{T t , C t } and ∆t = I{T t ≤ C t }. The mapping that gives rise to this observation unit
is given by Ga (z t , ct ) = (min{tt , ct }, I{tt ≤ ct }, a, wt ). The validity of the unadjusted analysis relies
on the condition that T t ⊥ C t |A, while the validity of the adjusted analysis relies on the condition
that T t ⊥ C t |(W t , A). It is worth noting that, although the condition for the validity of the adjusted
analysis can be more plausible in many settings, neither of these conditions implies the other — this
is a consequence of the fact that conditional independence does not imply marginal independence and
marginal independence does not imply conditional independence. The external data consist of X =
(Y, ∆, W ) where Y = min{T, C} and ∆ = I{T ≤ C}. Here C is the censoring time in the external
dataset. Letting Γ(z, c) = (min{t, c}, I{t ≤ c}, w), we see that the observed external data X is equal
to Γ(Z, C). We consider three estimands of treatment effect, which are all functionals of the treatmentarm-specific survival function Sa (t) := P (T t > t|A = a). In particular, we develop estimators and
confidence intervals for the risk difference (RD), the relative risk (RR), and the restricted mean survival
time (RMST) — see Appendix A for details.

26

Table 1: Age distribution and probability of outcomes within age groups, among hospitalized
Covid-19 patients [4]. “ICU” represents ICU admission.
age
0-19
20-44
45-54
55-64
65-74
75-84
≥ 85

5

P(age)
0.01
0.09
0.12
0.13
0.18
0.22
0.25

P(death | age)
0.00
0.01
0.03
0.08
0.11
0.17
0.37

P(ICU & survived | age)
0.00
0.18
0.32
0.31
0.37
0.47
0.35

P(no ICU & survived | age)
1.00
0.81
0.65
0.61
0.52
0.36
0.28

Experiments

5.1

Simulations

For the ordinal outcome case, we generate data based on a CDC report describing the age distribution
and probabilities of various outcomes within age groups for hospitalized Covid-19 patients [4], which are
also presented in Table 1. The ordinal outcome is assigned the value 1, 2, or 3 for “death”, “ICU and
survived”, or “no ICU and survived”, respectively. Age category is the only covariate we adjust for.
We consider both the fully adjusted and working-model-based estimators. The relative efficiency of
fully adjusted estimators is estimated with the analytical approach, while for the working-model-based
estimators, we use both the analytical and the bootstrap approaches. We consider three estimands of
the treatment effect: difference in mean, Mann-Whitney, and average log odds ratio.
As the covariate is ordinal as well, the nuisance conditional mean functions are estimated by sample
averages within each age group. In the analytical approach, we build Wald-type confidence intervals on
the logit scale first and transform them. For the bootstrap, we take the number of bootstrap resamples
in the two layers to be 100 and 500. Though these resample sizes are small compared to those used in
typical applications of the bootstrap, we use them to reduce the computational cost in this Monte Carlo
simulation. We do 1,000 replications for the analytical approach and 200 for the bootstrap.
The simulation results for sample size 1,000 are presented in Table 2. We observe that despite
the small number of resamples, the bootstrap procedure gives approximately 95% coverage, but that
this estimator has larger variance than does the analytical estimator across all settings considered. We
expect the performance to improve as the number of resamples increases. The coverage of the analytical
approach is close to the nominal level. Additional results for sample sizes 200 and 500 are given in

27

Table 2: Simulation results for ordinal outcome. We consider relative efficiency of fully adjusted and working-model-based estimators for DIM, MW and LOR. In the bootstrap approach, we take B1 = 100 and B2 = 500. Results are based on 1000 replications for analytic
approach, 200 for bootstrap. “F” stands for the fully adjusted estimator, and “W” stands for
the working-model-based estimator.
DIM (F)
DIM (W)

truth
0.837
0.840

MW (F)
MW (W)

0.842
0.845

LOR (F)
LOR (W)

0.838
0.842

method
analytic
analytic
bootstrap
analytic
analytic
bootstrap
analytic
analytic
bootstrap

bias
0.000
-0.004
0.000
0.006
0.002
0.001
0.003
-0.000
0.000

MSE
0.000
0.000
0.002
0.000
0.000
0.002
0.000
0.000
0.001

%RMSE
0.025
0.025
0.047
0.026
0.025
0.048
0.026
0.024
0.045

coverage
0.957
0.943
0.940
0.949
0.957
0.935
0.954
0.958
0.945

CI width
0.084
0.082
0.154
0.084
0.083
0.160
0.085
0.081
0.147

Appendix D. We note that, as the true relative efficiency is strictly less than 1, the confidence sets
constructed using the two-step approach detailed in Section 3.2 have the same coverage.
For survival outcomes, we only consider the relative efficiency of the fully adjusted estimators. We
generate a univariate covariate W ∼ Uniform(0, 1), and the survival time follows an exponential distribution Y |W ∼ Exp{(1 + 9W )/10}. The censoring time in the external data C is generated from an
Exp(0.1) distribution independent of W . The user-specified censoring mechanism in the trial is taken
to be the same, that is, Exp(0.1). We again consider three estimands: risk difference (RD), relative risk
(RR), and restricted mean survival time (RMST). The relative efficiency for RD and RR are the same
under the null. For RMST, we discretize time with a 0.2 interval to reduce computation time and also
mimic a setting where there are fixed follow-up times.
With continuous time, the nuisance functions are estimated using a sequence of Cox proportional
hazard models with polynomials of the covariate. We select the best model based on BIC. For discrete
time, we use a proportional odds model instead, which slightly outperforms the Cox model in the
simulations. Results for sample size 1,000 are presented in Table 3. The coverage of the confidence
intervals is close to the nominal level across all settings. The uncertainty in the estimates becomes larger
as time (t) increases, due to the reduced size of the risk set.
The R scripts for all the simulation experiments are available as supplementary files.

28

Table 3: Simulation results for survival outcome. We consider relative efficiency of fully
adjusted estimators for RD at time 1, 2, and 3 (the relative efficiency is the same for RR) and
RMST at time 3. Results are based on 1000 replications.
RD
RD
RD
RMST

5.2

(t = 1)
(t = 2)
(t = 3)
(t = 3)

truth
0.903
0.847
0.819
0.820

bias
0.000
0.000
0.002
-0.002

MSE
0.000
0.001
0.001
0.001

%RMSE
0.020
0.027
0.034
0.028

coverage
0.949
0.954
0.941
0.952

mean width
0.071
0.091
0.106
0.091

Application to Covid-19 data

We apply the proposed methods to assess the efficiency gain of covariate-adjustment using Covid-19 data.
The data contains information on 345 non-pregnant patients (≥ 18 years old) admitted to University of
Washington Medical Center through 6/15/2020. Among these patients, 40 were admitted twice and 3
were admitted three times. The following demographic and clinical features were measured at baseline:
gender, age at admission, race (White, Asian, Black or African American, American Indian or Alaska
Native and Native Hawaiian or other Pacific Islander), body mass index (kg/m2 ), type I diabetes (yes/no),
type II diabetes (yes/no), cardiovascular disease (CVD) (yes/no), hypertension (HTN) (yes/no), chronic
kidney disease (yes/no), whether are on cholesterol medications (yes/no) and whether are on HTN
medications (yes/no). Since only 4 patients have type I diabetes, we combine type I and type II diabetes
as one single baseline feature and therefore have 10 baseline covariates in total. We discretize age into
7 groups ( <30, 30-40, 40-50, 50-60, 60-70, 70-80, > 80). This is an observational dataset and there is
no treatment information. The minimum of the censoring time and the times to each of the following
events were measured: discharge, intubation, ventilation, and death. Time of hospital admission was
treated as time zero.
Ordinal Outcome. We use the following mutually exclusive ordinal outcome based on the severity
of a patient’s Covid-19 status: (1) censor or discharge, (2) intubation or ventilation, and (3) death.
Among 40 patients who had been admitted twice, only 14 patients had different outcomes between
the two visits (9 patients were classified as 2 during first admission and as 1 in the second admission
while 5 patients transited from 1 to 2). Among 3 patients who had been admitted three times, only 1
patient had different ordinal outcomes between 3 visits that he was classified a 1, 2, and 1 respectively.
For all patients who had been admitted more than once, there was no death. To deal with duplicated
observations for these patients, we only include the observations with a more severe outcome. As a result

29

of the above classification, there are 207 (60%) censor/discharges, 59 (17%) intubation or ventilation,
and 79 (23%) deaths. We consider three estimands of treatment effects: difference in mean (DIM),
Mann-Whitney (MW), and average log odds ratio (LOR). To estimate the nuisance functions for fully
adjusted estimators, we fit a series of polynomial regressions from order 1 to 5 and then select the optimal
model based on BIC score for DIM and MW. For LOR, these nuisance functions are estimated by fitting
proportional odds models with polynomials of order 1 to 5 and selecting the best model based on BIC.
We present the relative efficiency of covariate-adjusted estimators that adjust for all the covariates in
Table 4. The estimated efficiency gain is about 7% for the fully adjusted estimator, whereas for the
working-model-based estimators we do not see evidence of a significant efficiency gain. In contrast,
adjusting for a single baseline covariate gives an estimated efficiency gain ranging from 1% to 5%, and
the difference between using fully adjusted and working-model-based estimators is negligible when only
adjusting for one covariate. We leave the details to Appendix D.
Survival Outcome. We choose the time point of interest to be t = 350 hours, where the overall survival
is around 70%, and assess the relative efficiency for survival outcomes. We consider three estimands of
treatment effects: risk difference (RD), relative risk (RR), and restricted mean survival time (RMST). We
use elastic net [11] for variable selection, where the tuning penalty parameter is selected via 5-fold cross
validation. In particular, we select those variables with nonzero coefficients. To estimate the nuisance
functions, we then fit a sequence of Cox proportional hazards models with polynomials of orders 1 to
7 of the selected variables and select the model with the smallest BIC score. The results are shown
in Table 5. Adjusting for a single baseline covariate gives an efficiency gain ranges from 1% to 9%
in estimating RD or RR, with age being the most prognostic factor. A similar trend is observed for
RMST. Using elastic net, we select the following 4 baseline factors: age, CVD, chronic kidney disease,
and cholesterol medications. Adjusting for these four factors gives an 11% efficiency gain in estimating
RD or RR and RMST.

6

Discussion

In this paper, we presented a framework to use external data to infer about the relative efficiency of
covariate-adjusted analyses in a future clinical trial. We also exhibited the applicability of our framework for a variety of treatment effect estimands of particular interest. For each of these estimands,

30

Table 4: Relative efficiency (95% CI) of fully adjusted and working-model-based estimators
that adjust for all baseline covariates for estimating DIM, MW and LOR in the Covid-19
dataset. “F” stands for the fully adjusted estimator, and “W” stands for the working-modelbased estimator.
DIM
MW
LOR

F
0.93 (0.88, 0.97)
0.94 (0.92, 0.97)
0.93 (0.89, 0.98)

W
1.02 (0.95, 1.10)
1.05 (0.98, 1.14)
1.01 (0.94, 1.08)

Table 5: Relative efficiency (95% CI) for estimating RD, RR and RMST in time-to-event
setting in the Covid-19 dataset. Note under the null, relative efficiency of RD and RR are the
same and therefore only the one for RD is presented. Selected variables include age, CVD,
chronic kidney disease and cholesterol medications.
age
gender
race
CVD
HTN
diabetes
kidney disease
cholesterol meds
HTN meds
BMI
selected

0.91
1.00
1.00
0.98
1.00
1.00
0.96
0.98
1.00
0.99
0.89

RD
(0.79,
(0.88,
(0.88,
(0.85,
(0.88,
(0.88,
(0.84,
(0.86,
(0.88,
(0.87,
(0.76,

31

1.06)
1.14)
1.14)
1.13)
1.14)
1.14)
1.10)
1.12)
1.14)
1.14)
1.04)

0.92
1.00
1.00
0.99
1.00
1.00
0.97
0.98
1.00
0.99
0.89

RMST
(0.87, 0.97)
(1.00, 1.00)
(0.99, 1.00)
(0.97, 1.01)
(1.00, 1.01)
(0.99, 1.00)
(0.93, 1.00)
(0.96, 0.99)
(1.00, 1.00)
(0.97, 1.00)
(0.84, 0.95)

we introduced a consistent and asymptotically normal estimator of the relative efficiency and provided
an analytic means to develop Wald-type confidence intervals. We also introduced a double bootstrap
scheme that enables confidence interval construction in certain problems even when an analytic form for
the standard error is not available.
When the outcome is only partially observed, standard unadjusted and adjusted analyses typically
provide consistent estimators of the treatment effect under different assumptions on the coarsening
mechanism. In our view, the choice between adjusted and unadjusted estimator should first and foremost
be based on the plausibility of these assumptions. In settings where both sets of assumptions are
plausible, the relative efficiency of the two estimators represents a natural criterion upon which to make
this choice. Interestingly, unlike for fully adjusted estimators in uncoarsened settings, it is possible that
the unadjusted estimator will, in fact, be more efficient than the adjusted estimator when both estimators
are consistent. As a specific example, in the survival setting, our results in Theorem 8 show that the
asymptotic variance of the adjusted estimator is smaller than that of the unadjusted estimator if the
covariates are only predictive of the survival time, but is larger if the covariates are only predictive of
the censoring time.
The relative efficiency we considered is based on a sharp null setting where the treatment has no
effect. As a consequence, we do not need to specify the full distribution of Y t |A, W t expected in the
trial. Moreover, if the treatment effect estimator is regular, which is the case for all those that we
considered, then the relative efficiency at this sharp null also serves as an accurate approximation to the
relative efficiency under a variety of local alternatives. Though accurate in such settings, we expect that
this approximation may be poor when the treatment is extremely beneficial in some subgroups while
being quite harmful in some others. While a subgroup analysis might be able to detect this after the trial
is completed, it is not generally possible to know a priori whether this kind of subgroup effect exists. An
alternative approach would involve specifying a particular alternative distribution that the investigator
is interested in. In this case, the relative efficiency under that alternative can be derived and estimated.
Our framework for estimating relative efficiencies based on external data can be easily modified for this
setting.
Observational settings and clinical trials can be quite different in terms of coarsening, and thus we
define relative efficiency for a user-specified coarsening mechanism that approximates that of the future
trial. This also extends to the case where the covariate distribution is different between the external
32

data and the future clinical trial due to, for example, trial eligibility criteria. In such cases, a particular
covariate distribution for the future trial can be imposed when defining the relative efficiency, and the
external data can then be used to estimate the distribution of the outcome conditional on covariates.

Appendix
This appendix is organized as follows. In Appendix A, we develop estimators and confidence intervals
for the relative efficiency in settings where there are time-to-event outcomes with right censoring. In
Appendix B, we prove lemmas and theorems in Section 3 on continuous and ordinal outcomes. In
Appendix C, we prove lemmas and theorems in Appendix A on time-to-event outcomes with right
censoring. In Appendix D, we show some additional experiment results. In Appendix E, we develop
a two-step procedure with sample splitting to construct confidence intervals, and show that it achieves
nominal coverage. In Appendix F, we give the pseudocode for the double bootstrap scheme presented in
Section 3.

A

Estimation of relative efficiencies for time-to-event outcome
with right censoring

We consider three estimands of treatment effect, which are all functionals of the treatment-arm-specific
survival function Sa (t) := P (T t > t|A = a). For the unadjusted analysis, we consider plug-in estimators
based on the treatment-arm-specific Kaplan-Meier estimator [17], which we denote as S̃a (t). Such plug-in
estimators are consistent and asymptotically linear provided that T t ⊥ C t |A [see, for example, 8].
In contrast, the consistency of covariate-adjusted estimators often relies on the assumption that
T t ⊥ C t |(W t , A). In fact, many recently proposed adjusted estimators are based on the efficient influence
function of the treatment effect estimand in a model where the only assumption is that T t ⊥ C t |(W t , A)
[e.g., 22, 27, 8]. Under regularity conditions, these estimators achieve the semiparametric efficiency
bound in this model. Constructing these estimators often requires estimation of nuisance functions
such as the conditional hazard function ha (t, w) = P (T t = t|T t ≥ t, A = a, W t = w), the conditional
survival function Sa (t, w) = P (T t > t|A = a, W t = w), the conditional distribution of censoring time
Ga (t, w) := P (C t ≥ t|A = a, W t = w) or the treatment mechanism π(w) = P (A = 1|W t = w). We call
33

these estimators “fully adjusted”.
As discussed in Section 4, the efficiency of an adjusted estimator relative to that of an unadjusted
estimator is relevant only when both estimators are consistent — as noted earlier, a sufficient condition
for this to hold is that the observed data arises from a distribution in the intersection model consisting of
all distributions of (Z t , C t ) for which T t ⊥ C t |(W t , A) and T t ⊥ C t |A. Notably, there is not generally any
guarantee that a fully adjusted estimator will be efficient relative to the observed data model consisting
of the distributions of Ga (Z t , C t ) generated by sampling (Z t , C t ) from a distribution in this intersection
model. Stated more plainly, if it is known in advance that both the adjusted and unadjusted survival
function estimators are consistent, then, in certain cases, there may exist a more efficient estimator of
this survival function.
Unlike the cases of continuous or ordinal outcomes that we considered in Section 3, we are not aware
of a parametric working model for the conditional distribution of T t |A, W t that yields a RAL estimator
of S0 and S1 when marginalized over the distribution of the covariate W t . Nevertheless, it is possible
to define adjusted estimators based on working models in this setting. To see this, note that many of
the aforementioned fully adjusted estimators do have the doubly robust property: they are consistent
if either (S0 , S1 ) or (G, π) is correctly specified, and are efficient if both are correctly specified. This
allows us to use potentially misspecified parametric working models to estimate (S0 , S1 ) as long as we
estimate the distribution of censoring time using a correctly specified semiparametric or nonparametric
model — this is the case, for example, if we estimate the censoring distribution via a correctly specified
arm-specific Kaplan-Meier estimator. Such estimators are rarely used in practice. We, therefore, focus
on computing the relative efficiency of fully adjusted estimators, which see more use, as compared to
that of unadjusted estimators.

A.1

Estimation of relative efficiency

As in previous works [22, 27, 8], we assume that survival and censoring time are discrete, and take values
in {t1 , t2 , . . . , tK }. We let t0 = 0 be the baseline time. We expect similar derivations can be done for
continuous time, and in the simulation studies we empirically validate the performance of our proposed
methods when time is measured on a continuous scale.
The first two estimands we consider focus on survival functions at a specific time point. The risk

34

difference (RD) is defined as S0 (tk ) − S1 (tk ) for a time tk of interest. The relative risk (RR) is defined as
{1 − S1 (tk )}/{1 − S0(tk )} for a time tk of interest. We consider the unadjusted estimator S̃0 (tk ) − S̃1 (tk )
for RD and {1 − S̃1 (tk )}/{1 − S̃0 (tk )} for RR, where S̃a is the Kaplan-Meier estimator within each
treatment group. Let Ŝa denote the efficient adjusted estimator proposed in Moore and van der Laan
[20]. For each of the two estimands under consideration, we refer to the estimator that replaces S̃a in
the unadjusted estimator with Ŝa as the fully adjusted estimator.
Recall that Ŝa is a consistent estimator of Sa when T t ⊥ C t |(A, W t ). Under additional regularity
conditions given in Theorem 1 in Moore and van der Laan [22], for each a ∈ {0, 1} and k ∈ {1, . . . , K},
Ŝa (tk ) is an asymptotically linear estimator of Sa (tk ) with influence function

(y t , δ t , ã, wt ) 7→

k
X
j=1

−


I{ã = a}Sa (tk , wt )  t
δ I{y t = tj } − I{y t ≥ tj }ha (tj , wt )
πa Sa (tj , wt )Ga (tj , wt )

+ Sa (tk , wt ) − Sa (tk ).

(14)

Moreoever, for each a ∈ {0, 1} and k ∈ {1, . . . , K}, S̃a (tk ) is a RAL estimator of Sa (tk ) when T t ⊥ C t |A
with influence function [see, e.g., 8]

(y t , δ t , ã, wt ) 7→

k
X
j=1

−


I{ã = a}Sa (tk )  t
δ I{y t = tj } − ha (tj )I{y t ≥ tj } .
Sa (tj )Ga (tj )πa

(15)

Here, ha (t) is the hazard corresponding to Sa at time t and Ga (t) := P (C t ≥ t|A = a). The influence
functions of the fully adjusted and unadjusted estimators of the treatment effect estimand, which we
denote as Da and Du , respectively, can then be derived via the delta method.
As in Section 4, we define the relative efficiency as the ratio between the variances of Da and Du
under the sharp null. In such cases, the distribution of the observed data in the trial is characterized by
the marginal distribution of A, denoted by Π, the joint distribution of (T t , W t ), denoted by P , and the
conditional distribution of C t given (A, W t ). In particular, this implies that S1 (t, w) = S0 (t, w) = S(t, w)
for all (t, w), where S(t, w) := P (T t > t|W t = w) is the conditional survival function under P , and
also that S1 (t) = S0 (t) = S(t) for all t, where S(t) := P (T t > t) is the marginal survival function
under P . To simplify the presentation, we suppose additionally that C t ⊥ A|W t = w, and write
G(t, w) := P (C t ≥ t|W t = w) and G(t) := P (C t ≥ t). For given G and Π, the relative efficiency

35

parameter is a functional of P .
Before presenting the form of the relative efficiency, we introduce some additional needed notation.
For (T, W ) ∼ P , let h(t, w) := P (T = t|T ≥ t, W = w) and h(t) := P (T = t|T ≥ t) be the conditional
and marginal hazard functions under P , respectively. We define the following quantities, which will be
useful throughout this section:
S(tk )S(tl ){S(tj−1 ) − S(tj )}
, and
S(tj )S(tj−1 )
S(tk , w)S(tl , w){S(tj−1 , w) − S(tj , w)}
fjkl (w) =
.
S(tj , w)S(tj−1 , w)
skl
j =

(16)

Interestingly, in the null case that we consider, the relative efficiencies are the same for the RD and RR
estimands.
Lemma 6. Suppose that Conditions A1 and A3 hold and, in addition, that S(tk ) < 1. Suppose that
Ŝa and S̃a are asymptotically linear with influence functions given in (14) and (15), respectively. For
both the RR and RD estimand, the relative efficiency of the fully adjusted estimator as compared to the
unadjusted estimator is given by Φa (P ) = σa2 (P )/σu2 (P ), where


σa2 (P ) = EP 

k
X
j=1



fjkk (W )/G(tj , W ) , σu2 (P ) =

k
X

skk
j /G(tj ).

(17)

j=1

In what follows, we will often write φa for Φa (P ).
To estimate φa from the external data (Y, ∆, W ), we estimate σu2 and σa2 separately. We observe that
kl
σu2 is a transformation of S(t), and hence we construct a plug-in estimator ŝkl
j of sj using covariatePk
adjusted estimator of S(t) given in Moore and van der Laan [20] and estimate σu2 by σ̂u2 = j=1 ŝkk
j /G(tj ).

We estimate σa2 using one-step estimation based on its EIF. Recall that C is the censoring time in the
external data. We define H(t, w) := P (C ≥ t|W = w). For notational convenience, we define the
following function, which appears multiple times in the EIF of σa2 :

gjkl (y, δ, w) =



S(tk , w)
S(tk , w)
−
S(tj , w)
S(tj−1 , w)




S(tl , w)
S(tl , w)
τk (y, δ, w)
−
S(tj , w) S(tj−1 , w)
S(tk , w)S(tl , w)
S(tk , w)S(tl , w)
−
τj (y, δ, w) +
τj−1 (y, δ, w),
S 2 (tj , w)
S 2 (tj−1 , w)
τl (y, δ, w) +

36



where
τl (y, δ, w) =

X
u≤l

−

S(tl , w)
[I{y = tu , δ = 1} − h(tu , w)I{y ≥ tu }] .
S(tu , w)H(tu , w)

(18)

The efficient influence function of σa2 relative to the observed data model is

IFa (y, δ, w) =

k
X
j=1

 kk
1
g (y, δ, w) + fjkk (w) − σa2 .
G(tj , w) j

(19)

The derivation of this expression is deferred to Appendix C. Let Ŝ(t, w) and ĥ(t, w) be estimators of the
conditional survival and hazard functions, respectively. Let Ĥ(t, w) be an estimator of the conditional
censoring distribution. Define ĝjkk , fˆjkk with these estimates. We estimate σa2 with

σ̂a2 =

n
k
n
o
1 XX
1
ĝjkk (Yi , ∆i , Wi ) + fˆjkk (Wi ) .
n
G(tj , Wi )
i=1 j=1

We then estimate φa by φ̂ = σ̂a2 /σ̂u2 . The properties of φ̂ are given in the following theorem.
Theorem 6. Suppose that (1) Conditions A1 and A3 hold; (2) Ŝ(t, w), Ĥ(t, w), S(t, w), H(t, w) and
G(t, w) are all uniformly bounded away from 0, and Ŝ(t, w), ĥ(t, w) and h(t, w) are uniformly bounded
above; (3) for all t ∈ {t1 , . . . , tK }, the random functions Ĥ(t, ·) : W → R, Ŝ(t, ·) : W → R and
ĥ(t, ·) : W → R are such that kĤ(t, ·)−H(t, ·)kL2 (PW ) = oP (n−1/4 ), kŜ(t, ·)−S(t, ·)kL2 (PW ) = oP (n−1/4 ),
kĥ(t, ·) − h(t, ·)kL2 (PW ) = oP (n−1/4 ) and they all belong to a certain fixed Q-Donsker class F of functions
with probability tending to one. Then, φ̂ is an efficient estimator of φa .
The influence function of φ̂ is given in Appendix C.
The final treatment effect estimand we consider is the restricted mean survival time (RMST), defined
Pk
as ψ = j=1 {S1 (tj ) − S0 (tj )}. We again consider two plug-in estimators: the unadjusted KM-based
Pk
Pk
estimator ψ̂u = j=1 {S̃1 (tj ) − S̃0 (tj )} and the fully-adjusted estimator ψ̂a = j=1 {Ŝ1 (tj ) − Ŝ0 (tj )}.

Lemma 7. Suppose that Conditions A1 and A3 hold. Then, the relative efficiency φa takes the following

37

form

φa = σa2 (P )/σu2 (P ), where
σa2 (P )

=

k X
k min(j,l)
X
X
j=1 l=1

EP [fujl (W )/G(tu , W )],

σu2 (P )

=

u=1

k X
k min(j,l)
X
X
j=1 l=1

sjl
u /G(tu ).

u=1

We construct σ̂u2 in a similar way as in the case of the risk difference, namely by plugging in an
efficient adjusted estimator of S(·). As for σa2 , its EIF can be derived in a similar fashion as in the case
of RD, and we defer the details to Appendix C. We propose the following estimator

σ̂a2 =

n
k
k min(j,l)
o
1 X X X X n jl
ĝu (Yi , ∆i , Wi ) + fˆujl (Wi ) .
n i=1 j=1
u=1

(20)

l=1

The relative efficiency is estimated by φ̂ = σ̂a2 /σ̂u2 .
Based on (20), it appears that computing the quadruple sum used to define σ̂a2 will take order nk 3
time. As it turns out, these sums can be computed much more efficiently. In Appendix C, we show
that for a given i and given estimates of τ and S, the inner three sums can be computed in O(k) time,
resulting in an O(nk) complexity for computing the above quadruple sum.
Theorem 7. Under the same conditions as in Theorem 6, φ̂ is an efficient estimator of φa .
Again, the specific form of its influence function is given in Appendix C.
Remark 1. As discussed at the end of Section 3.2, the influence function of φ̂ is identically 0 in certain
special cases. One such case arises when T ⊥ W under P and the mapping G does not depend on W .
In these cases, a two-step procedure can be considered for inference — see the end of Section 3.2 for a
description of such an approach in a similar setting.
As noted earlier, for it to be interesting to compare the efficiency of the adjusted and unadjusted
estimators, it must be the case that both are asymptotically linear. In such settings, we now characterize
cases in which the adjusted estimator will be more efficient than will the unadjusted estimator. Moreover,
unlike in the uncoarsened data setting, there are also settings where the adjusted estimator may be less
efficient than the unadjusted estimator. We also characterize these cases.
Theorem 8. For all three estimands considered:
38

1. If the conditions in Lemma 6 hold and P is such that T ⊥ W , then σu2 (P ) ≤ σa2 (P ), that is, the
unadjusted estimator is at least as efficient as the adjusted estimator. Moreover, if varP [G(tj , W )] >
0 for some j ∈ {1, . . . , k}, then the inequality is strict: σu2 (P ) < σa2 (P ).
2. If the conditions in Lemma 6 hold and G(·, ·) is such that G(t, w1 ) = G(t, w2 ) for all t ≤ tk and
w1 , w2 ∈ W, then σa2 (P ) ≤ σu2 (P ), that is, the adjusted estimator is at least as efficient as the
unadjusted estimator.

B

Proofs of results in the case where the outcome is fully observed

B.1

Supporting lemmas for proofs in Section 3

Lemma 8 (EIF of mean conditional variance). For a given function uP (·) that can be written as uP (y) =
R
h(x, y)dP (x) for some function h, the canonical gradient of σa2 = EP [var(uP (Y )|W )] is given by
2

Da (y, w) = {uP (y) − fP (w)} + 2

Z

{uP (ỹ) − fP (w̃)}h(y, ỹ)dP (ỹ, w̃) − 3σa2 ,

(21)

where f (w) = EP [uP (Y )|W = w].
Proof. We prove this lemma by directly applying the definition of a gradient. We consider the onedimensional submodel {Pǫ : |ǫ| ≤ 1} with density
pǫ (y, w) = p(y|w){1 + ǫs1 (y|w)}p(w){1 + ǫs2 (w)},

where the range of s1 and s2 falls in [−1, 1] and these functions satisfy EP [s1 (Y |W )|W ] = 0 P -almost
surely and EP [s2 (W )] = 0. Let fP (w) = EP [uP (Y )|W = w] and fPǫ (w) = EPǫ [uPǫ (Y )|W = w]. We

39

have that
σa2 (Pǫ ) =
=

Z

{uPǫ (y) − fPǫ (w)}2 {1 + ǫs1 (y|w)}{1 + ǫs2 (w)}dP (y|w)dP (w)

Z

{uPǫ (y) − fPǫ (w)}2 dP (y, w)
Z
+ {uPǫ (y) − fPǫ (w)}2 {ǫs1 (y|w) + ǫs2 (w) + ǫ2 s1 (y|w)s2 (w)}dP (y, w).

The second term on the right has the following derivative with respect to ǫ at ǫ = 0
Z

{uP (y) − fP (w)}2 {s1 (y|w) + s2 (w)}dP (y, w),

and so will contribute {uP (Y ) − fP (W )}2 − σa2 to the gradient. We now focus on the first term, which
re-writes as
Z Z

2
h(x, y){1 + ǫs1 (x|w)}{1 + ǫs2 (w)}dP (x|w)dP (w) − fPǫ (w) dP (y, w)
2
Z
#
Z "Z
2
h(x, y)dP (x, w) − fPǫ (w) + c(ǫ) + 2c(ǫ)
=
dP (y, w),
h(x, y)dP (x, w) − fPǫ (w)

where c(ǫ) =

R

h(x, y){ǫs1 (x|w) + ǫs2 (w) + ǫ2 s1 s2 }dP (x, w). The first term in the above display has

derivative 0 with respect to ǫ at ǫ = 0, as fP is the true conditional mean. The second term also has
derivative 0 at ǫ = 0, as it is quadratic in ǫ. At ǫ = 0, the third term has derivative
Z Z

 Z

2
h(x, y){s1 (x|w) + s2 (w)}dP (x, w)
h(x, y)dP (x, w) − fP (w) dP (y, w)


Z Z Z
=2
h(x, y)dP (x, w) − fP (w) [h(x, y){s1 (x|w̃) + s2 (w̃)}] dP (x, w̃)dP (y, w)

Z Z
=2
{uP (y) − fP (w)} h(x, y)dP (y, w) {s1 (x|w̃) + s2 (w̃)}dP (x, w̃).
The inner integral has mean
Z Z

{uP (y) − fP (w)} h(x, y)dP (y, w)dP (x, w̃) =

40

Z

{uP (y) − fP (w)}uP (y)dP (y, w) = σa2 .

Therefore the following is a gradient:
Da (y, w) = {uP (y) − fP (w)}2 + 2

Z

{uP (ỹ) − fP (w̃)}h(y, ỹ)dP (ỹ, w̃) − 3σa2 .

Since we are working within a locally nonparametric model, the above is also the canonical gradient.
Lemma 9 (EIF of mean conditional covariance). Consider a locally nonparametric model of distributions
of (Y, W ). For given functions u(·) and v(·), the canonical gradient of σuv = EP [covP (u(Y ), v(Y )|W )]
is
Dcov (y, w) = {u(y) − fu (w)}{v(y) − fv (w)} − σuv ,

(22)

where fu (w) = EP [u(Y )|W = w] and fv (w) = EP [v(Y )|W = w].
Proof. As in the proof of Lemma 8, we consider a one-dimensional submodel {Pǫ : ǫ} with density
pǫ (y, w) = p(y, w){1 + ǫh(y, w)}. Let fP,u (w) = EP [u(Y )|W = w] and fP,v (w) = EP [v(Y )|W = w].
Note that

σuv (Pǫ ) =

Z

{u(y) − fPǫ ,u (w)} {v(y) − fPǫ ,v (w)} {1 + ǫh(y, w)}dP (y, w).

Also, because
Z 

{u(y) − fP,u (w)}

d
fP ,v (w)
dǫ ǫ

ǫ=0

+ {v(y) − fP,v (w)}

d
fP ,u (w)
dǫ ǫ

ǫ=0



dP (y, w) = 0,

it holds that
d
σuv (Pǫ )
dǫ

ǫ=0

=

Z

{u(y) − fP,u (w)} {v(y) − fP,v (w)} h(y, w)dP (y, w).

This shows that Dcov is a gradient, and, because the model is locally nonparametric, Dcov must therefore
be the canonical gradient.
Let X be a generic random variate with distribution P ∈ M with support in a bounded set X ⊂ Rd .
Let Uα : X −
→ Rm be a function indexed by α ∈ Rm . Suppose that P Uα = 0 has a unique solution in
α, and we denote this solution by ψ1 = ψ1 (P ). In general, we can regard ψ1 (P ) as a parameter defined

41

implicitly through the estimating equation. The following lemma establishes the pathwise differentiability
of this and a related parameter, under appropriate conditions.
Lemma 10 (Pathwise differentiability of parameters defined via estimating equations). Let ψ1 : M −
→
Rm be such that ψ1 (P ) is the unique solution in α to the estimating equation P Uα = 0. Suppose that, for
each x ∈ X , Uα (x) is continuously differentiable in α, with derivative U̇α (x) :=

∂
∂ α̃ Uα̃ (x)|α̃=α .

Suppose

in addition that Uψ1 , U̇ψ1 ∈ L2 (P ) and that P U̇ψ1 is invertible. Then, ψ1 is pathwise differentiable and
its gradient relative to any locally nonparametric model is given by
−1

D1 (x) = − P U̇ψ1
Uψ1 (x).
Moreover, for each α ∈ Rm , let gα : X −
→ R be a function, and suppose that α 7→ gα (x) is differentiable
for all x ∈ X . For each P ∈ M, define ψ2 := P gψ1 . Then ψ2 : M → Rm is pathwise differentiable with
gradient
D2 (x) = gψ1 (x) + P



∂
gα (x)|α=ψ1
∂α

⊤

D1 (x) − ψ2 .

Proof. For h ∈ L20 (P ) whose range is contained in [−1, 1], consider the one-dimensional submodel {Pǫ : ǫ},
where each Pǫ has density pǫ (x) = {1 + ǫh(x)}p(x). This submodel has score h at ǫ = 0. By definition,
R
R
we have that Pǫ Uψ1 (Pǫ ) = Uψ1 (Pǫ ) (x){1 + ǫh(x)}p(x)dx = 0. Define a function f : (α, ǫ) 7→ Uα (x){1 +
ǫh(x)}p(x)dx, which is linear in ǫ. Thus, the continuous differentiability of U as a function of α implies
that f is also continuously differentiable. Then the implicit function theorem implies that
n
o−1
∂
ψ1 (Pǫ )|ǫ=0 = − P U̇ψ1 (P )
∂ǫ

Z

Uψ1 (P ) (x)h(x)dP (x) = hD1 , hiP .

We note that D1 ∈ L20 (P ). Hence by definition ψ1 is pathwise differentiable with gradient D1 , which is
also the only gradient in any locally nonparametric model.

42

Also, noting that ψ2 (Pǫ ) =

R

gψ1 (Pǫ ) (x){1 + ǫh(x)}dP (x), we see that


⊤ 
∂
∂
gψ1 (x)h(x)dP (x) +
gα (x)|α=ψ1 dP (x)
ψ1 (Pǫ )|ǫ=0
∂α
∂ǫ
"
#

⊤
Z
∂
gψ1 (x) + P
=
gα (x)|α=ψ1
D1 (x) h(x)dP (x)
∂α

∂
ψ2 (Pǫ )|ǫ=0 =
∂ǫ

Z

Z

= hD2 , hiP
Thus, ψ2 is pathwise differentiable and D2 is the gradient in any locally nonparametric model.

B.2

Results in Section 3

Proof of Lemma 1. For notational convenience, we define µa = E[Y t |A = a], for a ∈ {0, 1}.
We first establish properties of the working-model-based estimator of µa , given by µ̂a = α̂a + β̂a⊤ W̄ t .
To do this, we note that the fitted coefficients from arm-specific linear regression (α̂a , β̂a⊤ ) satisfy the
following first-order conditions:
t

n
X
i=1

I{Ai = a}(Yit − α̂a − β̂a⊤ Wit ) = 0;

n
X
i=1

I{Ai = a}Wit (Yit − α̂a − β̂a⊤ Wit ) = 0.

Let (α∗a , βa∗ ) be the large-sample limit of (α̂a , β̂a ), defined implicitly as the solution to
E[I{A = a}(Y t − αa − βa⊤ W t )] = 0;

E[I{A = a}W t (Y t − αa − βa⊤ W t )] = 0.

The efficient influence function of µa when the treatment is randomized is given by
Da∗ (y t , at , wt ) =

I{at = a} t
{y − ra (wt )} + ra (wt ) − µa ,
πa

where πa = Π(A = a). We claim that Da defined below is also a gradient of µa in this model.
Da (y t , at , wt ) =

I{at = a}  t
y − α∗a − (βa∗ )⊤ wt + α∗a + (βa∗ )⊤ wt − µa .
πa

To show this, we will show that Da∗ − Da lies in the orthogonal complement of the tangent space. First,

43

define
L20,A (ν) = {s ∈ L20 (v) : s(y, a, w) = s(y ′ , a, w′ ) ∀(y, w), (y ′ , w′ )};
L20,W t (ν) = {s ∈ L20 (v) : s(y, a, w) = s(y ′ , a′ , w) ∀(y, a), (y ′ , a′ )};
L20,Y t |A,W t (ν) = {s ∈ L20 (v) : E[s(Y t , A, W t )|A = a, W t = w] = 0 ∀a, w}.
The tangent space decomposes as L20,A (ν)

L

L20,W t (ν)

Da (Y t , A, W t ) − Da∗ (Y t , A, W t ) =



L

L20,Y t |A,W t (ν). Next,


I{A = a}
− 1 {ra (W t ) − α∗a − βa∗ W t }.
πa

We note that each individual factor in the above display has mean 0. This, together with the independence
between A and W t , implies that hs, Da − Da∗ iν = 0 for any s in L20,A (ν), L20,W t (ν) or L20,Y t |A,W t (ν). This
implies that the difference is orthogonal to each component of the tangent space, and hence the tangent
space itself.
Next, we note that µ̂a re-writes as a one-step estimator based on the gradient Da . Let ν̂ be a
distribution of (Y t , A, W t ) such that Eν̂ [Y t |A = a, W t = w] = α̂a +β̂a⊤ w and Eν̂ [I{A = a}|W t = w] = π̂a ,
the sample proportion of A = a. The remainder is given by


I{A = a} t
(Y − α̂a − β̂a⊤ W t ) + α̂a + β̂a⊤ W t − ra (W t )
ψ(ν̂) − ψ(ν) + Pν Da (ν̂) = Pν
π̂a

 n
o
πa
=
− 1 Pν ra (W t ) − α̂a − β̂a⊤ W t ,
π̂a
which is oP (n−1/2 ) as the first term is OP (n−1/2 ) and the second term is oP (1). We note that Y and
W both have bounded support and πa ∈ (0, 1). Thus, kDa (ν̂) − Da kL2 (ν) = oP (1), which follows from
the convergence of α̂a , β̂a and π̂a to their population counterparts. Let γ ∗ := (πa , α∗a , βa∗ , µa ) be the
true parameter value and γ = (π, α, β, µ) be a generic parameter value, and define a class of functions

F := {fγ : (y, ã, w) 7→ I{ã = a} y − α − β ⊤ w /π + α + β ⊤ w − µ, γ ∈ B(γ ∗ )} where B(γ ∗ ) denotes a

small neighborhood of γ ∗ . We note that Da (ν̂) ∈ F with probability tending to 1. The boundedness of
Y t , W t and πa implies that this class of functions is Lipschitz in γ, and Example 19.7 in Van der Vaart
[31] implies that F is a Donsker class.
Therefore, µ̂a is asymptotically linear with influence function Da , and so is ψ̂m with influence function

44

Dm = D1 − D0 . Now focusing on ν where the sharp null holds, Dm simplifies to
Dm (y t , at , wt ) =



at
1 − at
−
π1
π0



(y t − α∗ − β ∗ wt )

2
Due to the independence of A and (Y t , W t ) under the sharp null, it has variance σm
/(π1 π0 ) under the

sharp null.
Now, we derive the asymptotic variances of the unadjusted estimator and the fully adjusted estimator.
In doing so, we consider a more general parameter ψ = E[u(Y t )|A = 1] − E[u(Y t )|A = 0] for a given
function u. The average treatment effect corresponds to the special case of u being the identity function.
Recall that the unadjusted estimator is given by the difference between arm-specific means

ψ̂u =

Pnt

t
i=1 u(Yi )Ai
Pnt
i=1 Ai

−

Pnt

u(Yit )(1 − Ai )
.
Pnt
i=1 (1 − Ai )

i=1

Applying the delta method, we see that ψ̂u is asymptotically linear with influence function
Du (y t , at , wt ) =



1 − at
at
−
π1
π0



 t
u(y ) − E[u(Y t )|A = at ] .

Under the sharp null, it simplifies to
t

t

t

Du (y , a , w ) =



1 − at
at
−
π1
π0





u(y t ) − E[u(Y t )] ,

with variance under the sharp null varP (u(Y ))/(π1 π0 ), due to the independence between A and (Y t , W t )
under the sharp null.
The fully adjusted estimator we consider is the AIPW estimator given by

nt 
1 X Ai {u(Yit ) − r̂1 (Wit )} (1 − Ai ){u(Yit ) − r̂0 (Wit )}
t
t
ψ̂a = t
−
+ r̂1 (Wi ) − r̂0 (Wi ) ,
n i=1
π̂(Wit )
1 − π̂(Wit )
where r̂1 (w) and r̂0 (w) are estimators of r1 (w) = E[u(Y t )|A = 1, W t = w] and r0 (w) = E[u(Y t )|A =
0, W t = w]. The influence function of this estimator ψ̂a is given by
D∗ (y t , at , wt ) =

{u(y t ) − r1 (wt )}at
{u(y t ) − r0 (wt )}(1 − at )
+ r1 (wt ) −
− r0 (wt ) − ψ.
π1
π0
45

Under the sharp null, it simplifies to
D∗ (y t , at , wt ) =



at
1 − at
−
π1
π0



 t
y − E[u(Y t )|W t = wt ] ,

with variance under the sharp null EP [varP (u(Y )|W )]/(π1 π0 ), where we again use the independence
between A and (Y t , W t ) under the sharp null.
2
Proof of Theorem 1. First, we consider the estimation of σm
. Define a parameter P 7→ (α∗ (P ), β ∗ (P )),

where (α∗ (P ), β ∗ (P )) is the minimizer of EP [(Y − α − β ⊤ W )2 ] in (α, β). Define a set of estimating
functions U (α, β) = (y −α−β ⊤w, w(y −α−β ⊤ w)). The first-order condition implies that (α∗ (P ), β ∗ (P ))
is the unique solution to the estimating equation P U (α, β) = 0. This solution can be written as a
differentiable transformation of EP [Y ], EP [W ], EP [Y W ] and EP [W W ⊤ ]. Hence, by the chain rule,
(α∗ (P ), β ∗ (P )) is pathwise differentiable.
In the remainder of this proof, we will often write (α∗ (P ), β ∗ (P )) as (α∗ , β ∗ ). The fitted coefficients
(α̂, β̂) solve the empirical estimating equation Pn U (α, β) = 0, and are asymptotically linear estimators
of (α∗ , β ∗ ) with influence function


⊤

−1 

E[W ] 
 1
IFab (y, w) = 

E[W ] E[W W ⊤ ]

∗

⊤ ∗



 y−α −w β 

.
w(y − α∗ − w⊤ β ∗ )

Lemma 10 implies that this influence function is also the canonical gradient of (α∗ , β ∗ ) in any locally
nonparametric model, and therefore (α̂, β̂) is also regular [Proposition 2.3.i, 23]. Define the estimating
2
function UAT E (α, β, σ 2 ) := (y − α − β ⊤ w)2 − σ 2 . We note that σm
is the solution in σ 2 to the estimating
2
equation P UAT E (α∗ , β ∗ , σ 2 ) = 0 and σ̂m
solves its empirical counterpart Pn UAT E (α̂, β̂, σ 2 ) = 0. Hence,

2
2
0 = Pn UAT E (α̂, β̂, σ̂m
) − P UAT E (α∗ , β ∗ , σm
)
n
o
2
2
2
= (Pn − P )UAT E (α∗ , β ∗ , σm
) + P UAT E (α̂, β̂, σ̂m
) − UAT E (α∗ , β ∗ , σm
)
n
o
2
2
+ (Pn − P ) UAT E (α̂, β̂, σ̂m
) − UAT E (α∗ , β ∗ , σm
) .
2
The consistency of (α̂, β̂) implies that σ̂m
is also consistent. This together with the bounded support
2
2
of W implies that kUAT E (α̂, β̂, σ̂m
) − UAT E (α∗ , β ∗ , σm
)kL2 (P ) = oP (1). Furthermore, we can show that

46

2
2
UAT E is a Lipschitz function of (α, β, σ 2 ) in a neighborhood of (α∗ , β ∗ , σm
), and thus UAT E (α̂, β̂, σ̂m
)

belongs to a Donsker class with probability tending to 1 [Example 19.7, 31]. Lemma 19.24 in Van der
Vaart [31] implies that the last term in the above display is oP (n−1/2 ). Applying a Taylor expansion to
the second term, we have
2
2
2
σ̂m
− σm
= (Pn − P )UAT E (α∗ , β ∗ , σm
)+

+



∂
P UAT E
∂β

⊤




∂
P UAT E |(α∗ ,β ∗ ) (α̂ − α∗ )
∂α

|(α∗ ,β ∗ ) (β̂ − β ∗ ) + oP (n−1/2 ).

In particular, we have
∂UAT E
∂UAT E
(y, w) = −2(y − α − β ⊤ w),
(y, w) = −2w(y − α − β ⊤ w),
∂α
∂β
2
both of which have mean 0 at (α∗ , β ∗ ) by the first-order condition of (α∗ , β ∗ ). This implies that σ̂m
is
2
asymptotically linear with influence function IFm (y, w) = (y − α∗ − w⊤ β ∗ )2 − σm
. Lemma 10 implies
2
2
is also regular.
that this is also the canonical gradient of σm
, and hence σ̂m

Next we estimate σa2 . The proposed estimator is a one-step estimator based on the canonical gradient.
Applying Lemma 8 with h(x, y) = y, we obtain the canonical gradient IFa (y, w) = {y − r(w)}2 − σa2
where r(w) = EP [Y |W = w]. Let P̂ be a distribution of (Y, W ) such that the conditional mean of Y
given W is r̂(w) and the marginal distribution of W is the empirical distribution of W . Then,
σ̂a2 − σa2 = σa2 (P̂ ) + Pn IFa (P̂ ) − σa2
= (Pn − P )IFa (P ) + (Pn − P ){IFa (P̂ ) − IFa (P )} + R(P̂ , P ),
where
R(P̂ , P ) = σa2 (P̂ ) − σa2 (P ) + P {IFa (P̂ )}
Z
= {r̂(w) − r(w)}2 dP (w) = oP (n−1/2 ).
As r̂(w) is uniformly bounded and belongs to a Donsker class F with probability tending to 1 and
Y has bounded support, Theorem 2.10.6 in Van Der Vaart and Wellner [32] implies that IFa (P̂ ) =

47

{y − r̂(w)}2 − σ̂a2 belongs to a Donsker class F̃ that is also bounded. The difference between IFa (P̂ ) and
IFa (P ) is given by
{y − r̂(w)}2 − σ̂a2 − {y − r(w)}2 + σa2 = {2y − r̂(w) − r(w)}{r(w) − r̂(w)} − (σ̂a2 − σa2 ).
Therefore,

kIFa (P̂ ) − IFa (P )kL2 (P ) ≤

Z

2

2

{2y − r̂(w) − r(w)} {r(w) − r̂(w)} dP

1/2

+ |σ̂a2 − σa2 |

≤ M kr − r̂kL2 (P ) + |σ̂a2 − σa2 |,
for some M as the support of Y and r̂ are bounded. The first term in the last line is oP (1) by the
assumption that kr̂ − rkL2 (P ) = oP (n−1/4 ). Thus, kIFa (P̂ ) − IFa (P )kL2 (P ) = oP (1) provided that σ̂a2 is
a consistent estimator of σa2 . Suppose for now that this is indeed the case, then Lemma 19.24 in Van der
Vaart [31] implies that (Pn − P ){IFa (P̂ ) − IFa (P )} is oP (n−1/2 ). This shows that σ̂a2 is regular and
asymptotically linear with influence function IFa .
We now show that σ̂a2 is indeed a consistent estimator of σa2 . Note that


σ̂a2 − σa2 = Pn {y − r̂(w)}2 − σa2




= Pn {y − r(w)}2 − σa2 + Pn {r(w) − r̂(w)}2 + 2{y − r(w)}{r(w) − r̂(w)}


≤ Pn {y − r(w)}2 − σa2 + M1 Pn |r(w) − r̂(w)|,
for some constant M1 . The first term in the last line is oP (1) by the law of large number. As for the
second term, we rewrite it as

Pn |r(w) − r̂(w)| = (Pn − P )|r̂(w) − r(w)| + P |r̂(w) − r(w)|.
Lemma 19.24 in Van der Vaart [31] implies that (Pn − P )|r̂(w) − r(w)| = oP (1) as |r̂(w) − r(w)| lies in
a Donsker class with probability tending to 1 and kr̂ − rkL2 (P ) = oP (1). In addition, P |r̂(w) − r(w)| ≤
kr̂ − rkL2 (P ) = oP (1). This establishes the consistency of σ̂a2 .
Finally, we estimate σu2 with the sample variance of Y , which is regular and asymptotically linear

48

with influence function IFu (y, w) = {y − EP [Y ]}2 − σu2 .
Theorem 2 then follows by applying the delta method. Specifically, the influence function of φ̂a
is given by (IFa − φa IFu )/σu2 , and similarly the influence function of φ̂m is (IFm − φm IFu )/σu2 . Both
estimators are efficient as we work in a locally nonparametric model.
Proof of Lemma 2. First recall that θa∗ (k, w) is the best approximation to the true outcome regression
θa (k, w) within the proportional odds model. The coefficients α∗a and βa∗ satisfy the following first-order
condition
E





I{Ai = a} 
I{Yit ≤ k} − θα∗a ,βa∗ (k, Wit ) = 0,
πa

which implies that




Fa (k) = E θa (k, W t ) = E θa∗ (k, W t ) .
We claim that when the treatment is randomized, the following is a gradient of Fa (k),
IFFa (k) (y t , at , wt ) =


I{at = a} 
I{y t ≤ k} − θa∗ (k, wt ) + θa∗ (k, wt ) − Fa (k).
πa

To show this, we will show that the difference between IFFa (k) and the canonical gradient of Fa (k) lies
in the orthogonal complement of the tangent space. The canonical gradient is given by
Da∗ (y t , at , wt ) =


I{at = a} 
I{y t ≤ k} − θa (k, wt ) + θa (k, wt ) − Fa (k).
πa

Hence,

IFFa (k) (Y t , A, W t ) − Da∗ (Y t , A, W t ) = θa∗ (k, W t ) − θa (k, W t ) [1 − I{A = a}/πa ] .
As each individual factor has mean 0 and A ⊥ W t , the difference is orthogonal to each component of
the tangent space and hence the tangent space itself.
Finally, F̂a re-writes as a one-step estimator based on the gradient IFFa (k) . In particular, let ν̂ be a
distribution with outcome regression θα̂a ,β̂a and treatment mechanism π̂1 , then
F̂a (k) − Fa (k) = (Pv̂ − Pν )IFFa (k) (ν) + (Pν̂ − Pν ){IFFa (k) (ν̂) − IFFa (k) (ν)} + R(ν̂, ν),

49

where

R(ν̂, ν) = Fa (k)(ν̂) − Fa (k)(ν) + Pν IFFa (k) (ν̂)
o
πa − π̂a n
=
Pν θa (k, W t ) − θa∗ (k, W t ) + θa∗ (k, W t ) − θα̂a ,β̂a (k, W t )
π̂a
o
πa − π̂a n ∗
Pν θa (k, W t ) − θα̂a ,β̂a (k, W t ) .
=
π̂a
First we note that R(ν̂, ν) is oP (n−1/2 ) as the first term in the last line is OP (n−1/2 ) and the second term
is oP (1). Also given the bounded support of W t and the fact that π1 and π0 are bounded away from 0,
we can show that the convergence of π̂a , α̂a and β̂a implies that kIFFa (k) (ν̂) − IFFa (k) (ν)kL2 (ν) = oP (1).
Example 19.7 in Van der Vaart [31] shows that IFFa (k) (v̂) lies in a Donsker class with probability
tending to 1, as IFFa (k) is Lipschitz in its indexing parameters in a neighborhood of the true parameter
value, again due to the boundedness of W t and πa . Lemma 19.24 in Van der Vaart [31] implies that
(Pν̂ − Pν ){IFFa (k) (ν̂) − IFFa (k) (ν)} = oP (n−1/2 ).
Thus F̂a (k) is asymptotically linear with influence function IFFa (k) .
Proof of Lemma 3. First we consider the variance of the unadjusted and fully adjusted estimators. Recall
that, in proving Lemma 1, we considered general functions u(·). Although the outcome is now ordinal,
the same arguments as in the proof of Lemma 1 applies here, and we can show that σu2 = EP [u(Y )] and
σa2 = EP [varP (u(Y )|W )].
We now derive the influence function of the adjusted estimator based on the working proportional
PK−1
odds model. For the ease of notation, let bk := u(k)−u(k+1). Recall that ψ̂m = k=1 bk {F̂1 (k)− F̂0 (k)},
and thus, by Lemma 2, it is asymptotically linear with influence function

Dm (y t , at , wt ) =

K−1
X
k=1


bk IFF1 (k) (y t , at , wt ) − IFF0 (k) (y t , at , wt ) − ψ.

Under the sharp null, the above display simplifies to

t

t

t

Dm (y , a , w ) =



at
1 − at
−
π1
π0

Under the sharp null, its variance is EP


PK−1
k=1

 K−1
X
k=1



bk I{y t ≤ k} − θ∗ (k, wt ) .
∗

bk [I{Y ≤ k} − θ (k, W )]

50

2 

/(π1 π0 ) due to the inde-

pendence between A and (Y t , W t ) under the sharp null.
2
Proof of Theorem 2. We first consider estimating σm
. To start, we show that (α̂, β̂), the maximizer of

the empirical version of (10), is a RAL estimator of (α∗ , β ∗ ). The first-order conditions of this maximization imply that (α̂, β̂) solves a set of estimating equations, as the parameter space is unconstrained.
Specifically, define U (α, β) = (U1 (α, β), . . . , UK−1 (α, β), UK (α, β)) with
exp(αk + β ⊤ w)
, k = 1, . . . , K − 1,
1 + exp(αk + β ⊤ w)

K−1
X
exp(αk + β ⊤ w)
.
UK (α, β)(y, w) = w
I{y ≤ k} −
1 + exp(αk + β ⊤ w)
Uk (α, β)(y, w) = I{y ≤ k} −

k=1

Then we have that Pn Uk (α̂, β̂) = 0 for all k = 1, . . . , K. We can show, through the usual arguments
used to study estimating equations [Chapter 5, 31], that the influence function of (α̂, β̂) is
IFab (y, w) = −{P U̇(α∗ , β ∗ )}−1 U (α∗ , β ∗ )(y, w).

U̇αα
In particular, the derivative matrix U̇ can be partitioned into 
U̇αβ



⊤
U̇αβ


U̇ββ

, with

U̇αα (y, w) = −Diag [θ∗ (k, w){1 − θ∗ (k, w)}] ,
⊤

U̇αβ (y, w) = − [wθ∗ (1, w){1 − θ∗ (1, w)}, · · · , wθ∗ (K − 1, w){1 − θ∗ (K − 1, w)}] ,
U̇ββ (y, w) = −

K−1
X
k=1

ww⊤ θ∗ (k, w){1 − θ∗ (k, w)}.

Lemma 10 implies that IFab is the canonical gradient of (α∗ , β ∗ ) in a locally nonparametric model, and
thus (α̂, β̂) is also regular [Proposition 2.3.i, 23].
Next, we define the following estimating equation:

2

UDIM (α, β, σ )(y, w) =

K−1
X
k=1

bk [I{y ≤ k} − θα,β (k, w)]

!2

− σ2 .

2
2
is the unique solution in σ 2 to the equation P UDIM (α∗ , β ∗ , σ 2 ) = 0, and σ̂m
solves its
By definition, σm

51

empirical counterpart Pn UDIM (α̂, β̂, σ 2 ) = 0. Hence, we have
2
2
0 = Pn UDIM (α̂, β̂, σ̂m
) − P UDIM (α∗ , β ∗ , σm
)
n
o
2
2
2
= (Pn − P )UDIM (α∗ , β ∗ , σm
) + P UDIM (α̂, β̂, σ̂m
) − UDIM (α∗ , β ∗ , σm
)
n
o
2
2
+ (Pn − P ) UDIM (α̂, β̂, σ̂m
) − UDIM (α∗ , β ∗ , σm
) .
2
The consistency of (α̂, β̂) implies that σ̂m
is also consistent. Combining this with the bounded support
2
2
of W , we see that kUDIM (α̂, β̂, σ̂m
) − UDIM (α∗ , β ∗ , σm
)kL2 (P ) = oP (1). Furthermore, it can be shown
2
that UDIM is a Lipschitz transformation of (α, β, σ 2 ) in a neighborhood of (α∗ , β ∗ , σm
), and thus that
2
UDIM (α̂, β̂, σ̂m
) belongs to a Donsker class with probability tending to 1 [Example 19.7, 31]. Lemma

19.24 in Van der Vaart [31] implies that the last term in the above display is oP (n−1/2 ). Thus,
2
2
2
)+
= (Pn − P )UDIM (α∗ , β ∗ , σm
σ̂m
− σm

+



∂
P UDIM
∂β



(α∗ ,β ∗ )



∂
P UDIM
∂α



(α∗ ,β ∗ )

(α̂ − α∗ )

(β̂ − β ∗ ) + oP (n−1/2 ).

The partial derivatives are given by
∂
UDIM |(α∗ ,β ∗ ) (y, w) = −2bk
∂αk
∂
UDIM |(α∗ ,β ∗ ) (y, w) = −2
∂β

K−1
X
l=1

K−1
X
k=1

!

bk [I{y ≤ l} − θ∗ (l, w)] θ∗ (k, w){1 − θ∗ (k, w)},
∗

bk [I{y ≤ k} − θ (k, w)]

! "K−1
X
k=1

∗

∗

#

bk wθ (k, w){1 − θ (k, w)} .

2
2
Combining all the results above, we see that σ̂m
is an asymptotically linear estimator of σm
with

influence function IFm , where
2
IFm (y, w) = UDIM (α∗ , β ∗ , σm
)(y, w) +



∂P UDIM
|(α∗ ,β ∗ )
∂(α, β)

⊤

IFab (y, w).

2
Lemma 10 implies that IFm is the canonical gradient of σm
in any locally nonparametric model, and
2
thus that σ̂m
is regular [Proposition 2.3.i, 23].

We now consider the estimation of σa2 . The proposed estimator is a one-step estimator based on
the canonical gradient, and the proof is very similar to that of Theorem 1. Applying Lemma 8 with

52

h(x, y) = u(y), we obtain the canonical gradient IFa (y, w) = {u(y)− r(w)}2 − σa2 . Let P̂ be a distribution
of (Y, W ) such that the conditional mean of u(Y ) given W is r̂(w), and that the marginal distribution
of W is the empirical distribution of W . We have that
σ̂a2 − σa2 = σa2 (P̂ ) + Pn IFa (P̂ ) − σa2
= (Pn − P )IFa (P ) + (Pn − P ){IFa (P̂ ) − IFa (P )} + R(P̂ , P ),
where
R(P̂ , P ) = σa2 (P̂ ) − σa2 (P ) + P {IFa (P̂ )}
Z
= {r̂(w) − r(w)}2 dP (w) = oP (n−1/2 ),
where the latter equality holds by assumption. As r̂(w) is uniformly bounded and belongs to a Donsker
class F with probability tending to 1 and Y has bounded support, Theorem 2.10.6 in Van Der Vaart
and Wellner [32] implies that IFa (P̂ ) = {u(y) − r̂(w)}2 − σ̂a2 belongs to a transformed Donsker class F̃
that is also bounded. The difference between IFa (P̂ ) and IFa (P ) is given by
{u(y) − r̂(w)}2 − σ̂a2 − {u(y) − r(w)}2 + σa2 = {2u(y) − r̂(w) − r(w)}{r(w) − r̂(w)} − (σ̂a2 − σa2 ).
Therefore,

kIFa (P̂ ) − IFa (P )kL2 (P ) ≤

Z

2

2

{2u(y) − r̂(w) − r(w)} {r(w) − r̂(w)} dP

1/2

+ |σ̂a2 − σa2 |

≤ M kr − r̂kL2 (P ) + |σ̂a2 − σa2 |,
for some M as the support of Y and r̂ are bounded. The first term in the last line is oP (1) by the
assumption that kr̂ − rkL2 (P ) = oP (n−1/4 ). Thus, kIFa (P̂ ) − IFa (P )kL2 (P ) = oP (1) provided that σ̂a2
is a consistent estimator of σa2 . Suppose for now that this is indeed the case, then Lemma 19.24 in
Van der Vaart [31] implies that (Pn − P ){IFa (P̂ ) − IFa (P )} is oP (n−1/2 ). Hence, if we show that σ̂a2 is
a consistent estimator of σa2 , then we will have shown that σ̂a2 is regular and asymptotically linear with
influence function IFa .

53

We now show that σ̂a2 is indeed a consistent estimator of σa2 . Note that


σ̂a2 − σa2 = Pn {u(Y ) − r̂(W )}2 − σa2




= Pn {u(Y ) − r(W )}2 − σa2 + Pn {r(W ) − r̂(W )}2 + 2{u(Y ) − r(W )}{r(W ) − r̂(W )}


≤ Pn {u(Y ) − r(W )}2 − σa2 + M1 Pn |r(W ) − r̂(W )|,
for some constant M1 , where we used the fact that both Y and W have bounded support and r̂ is
uniformly bounded. The first term in the last line is oP (1) by the weak law of large numbers. As for the
second term, we see that it is equal to

Pn |r(W ) − r̂(W )| = (Pn − P )|r̂(W ) − r(W )| + P |r̂(W ) − r(W )|.
Lemma 19.24 in Van der Vaart [31] implies that (Pn − P )|r̂ − r| = oP (1) as |r̂ − r| lies in a Donsker class
with probability tending to 1 and kr̂ − rkL2 (P ) = oP (1). In addition, P |r̂ − r| ≤ kr̂ − rkL2 (P ) = oP (1).
This establishes the consistency of σ̂a2 .
Finally, we estimate σu2 with the sample variance of u(Y ), which is regular and asymptotically linear
with influence function IFu = {u(y) − E[u(Y )]}2 − σu2 .
Theorem 2 then follows by applying the delta method. Specifically, the influence function of φ̂a is
given by (IFa − φa IFu )/σu2 , and, similarly, the influence function of φ̂m is (IFm − φm IFu )/σu2 .
Proof of Lemma 4. Recall that the unadjusted estimator is

ψ̂u =

 t t
n X
n
X


i=1 j=1


  t
nt
n

 X
X
Aj ) .
Ai )(nt −
Ai (1 − Aj )h(Yit , Yjt )
(

 
i=1

j=1

First we introduce some notation. Let νn be the empirical distribution of X t in the future trial data.
Define the functions ηa (·), a ∈ {0, 1}, analogously to η(·) but within each treatment arm as ηa (k) :=
P t P t
Pa (Y < k) + Pa (Y = k)/2 for a ∈ {0, 1}, and define ψ̂u1 := ni=1 nj=1 Ai (1 − Aj )h(Yit , Yjt )/(nt )2 .

We note that ψ̂u1 is a V-statistic with symmetric kernel h̃(X1t , X2t ) := {A1 (1 − A2 )h(Y1t , Y2t ) + A2 (1 −
RR
A1 )h(Y2t , Y1t )}/2. For a generic distribution Q of X t , we define Q2 h̃ :=
h̃(xti , xtj )dQ(xti )dQ(xtj ). With

54

this notation, ψ̂u1 = νn2 h̃. Note that
ψ̂u1 − π1 π0 ψ = νn2 h̃ − ν 2 h̃ = 2(νn − ν)(ν h̃) + (νn − ν)2 h̃.
To establish the asymptotic linearity of ψ̂u1 , we first show that (νn − ν)2 h̃ is oP (n−1/2 ). To start, define
a class of functions H̃ := {x 7→ h̃(x, x2 ) : x2 ∈ X } where X is the support of X t . Each function in H̃
is a weighted sum of 4 binary terms, with weights being either 1 or 1/2. Each term is indexed by a2
and y2t , and can be computed with 3 arithmetic operations and 1 comparison. Theorem 8.4 in Anthony
and Bartlett [1] implies that each binary term belongs to a VC-class with VC dimension at most 64,
and Lemma 19.15 in Van der Vaart [31] in turn implies that this class is Donsker. Theorem 2.10.6 in
Van Der Vaart and Wellner [32] then implies that H̃ is a Donsker class (hence also Glivenko-Cantelli).
R
Define h̃1n : x 7→ h̃(x1 , x)d(νn − ν)(x1 ), then ν h̃21n ≤ {supx∈X |h̃1n (x)|}2 = oP (1). Next, note that
R
(νn − ν)2 h̃ = (νn − ν)h̃1n . The function x 7→ h̃(x, x2 )dνn (x2 ) is in the closure of the convex hull of
R
the Donsker class H̃, and x 7→ h̃(x, x2 )dν(x2 ) is a fixed function. This together with the symmetry of
h̃ implies that h̃1n lies in a Donsker class. Lemma 19.24 then implies that (νn − ν)2 h̃ = (νn − ν)h̃1n =

oP (n−1/2 ).
Next we note that ν h̃(xt , ·) : xt 7→ [aπ0 η0 (y t ) + (1 − a)π1 {1 − η1 (y t )}]/2. Combining this with the
previous results, we have that
t

n

1 X
ψ̂u1 − π1 π0 ψ = t
Ai π0 η0 (Yit ) + (1 − Ai )π1 {1 − η1 (Yit )} − 2π1 π0 ψ + oP (n−1/2 ).
n i=1

Applying the delta method, we see that ψ̂u is asymptotically linear with influence function
Du (y t , at , wt ) =

at {η0 (y t ) − ψ} (1 − at ){1 − η1 (y t ) − ψ}
+
,
π1
π0
t

t

1
t
t
which, under the sharp null, simplifies to ( πa1 − 1−a
π0 ){η(y )− 2 }. The variance of η(Y ) can be calculated
PK
exactly, and equals to (1 − k=1 p3k )/12.

Next we look at the fully adjusted estimator. The efficient influence function of ψ was given in, for

55

example, Mao [18],


at 
η0 (y t ) − E η0 (Y t )|A = 1, W t = wt
π1


1 − at 
−
η1 (y t ) − E η1 (Y t )|A = 0, W t = wt
π0




+ E η0 (Y t )|A = 1, W t = wt − E η1 (Y t )|A = 0, W t = wt ,

D∗ (y t , at , wt ) = 1 − 2ψ +

which, under the sharp null, simplifies to



at
π

−

1−at
π0



{η(y t ) − E [η(Y t )|W t = wt ]} with variance σa2 /(π1 π0 ),

where we use the independence between A and (Y t , W t ) under the sharp null.
Finally we consider the estimator based on proportional odds model. Let P̂a be a distribution with
RR
CDF F̂a (k) for a = 0, 1. Recall that ψ̂m =
h(x, y)dP̂1 (x)dP̂0 (y). Since F̂ ’s are asymptotically linear,

we have

Z Z
h(x, y)dP1 (x)d(P̂0 − P0 )(y) +
h(x, y)dP0 (y)d(P̂1 − P1 )(x) + oP (n−1/2 )
Z
Z
= {1 − η1 (y)}d(P̂0 − P0 )(y) + η0 (x)d(P̂1 − P1 )(x) + oP (n−1/2 ).

ψ̂m − ψ =

Z Z

The first term can be alternatively written as

PK−1
k=1

−b1 (k){F̂0 (k) − F0 (k)} where b1 (k) = η1 (k) − η1 (k +

1), similarly for the second term. Then Lemma 2 implies that ψ̂m is asymptotically linear with influence
function
Dm (y t , at , wt ) =

K−1
X
k=1


−b1 (k)IFF0 (k) (y t , at , wt ) + b0 (k)IFF1 (k) (y t , at , wt ) ,

which, under the sharp null, simplifies to


at
1 − at
−
π1
π0

 K−1
X


{η(k) − η(k + 1)} I{y t ≤ k} − θ∗ (k, wt ) .
k=1

2
The variance of Dm under the sharp null is σm
/(π1 π0 ), where we used the independence between A and

(Y t , W t ) under the sharp null. Lemma 4 follows by the definition of relative efficiency.
2
Proof of Theorem 3. We first study σ̂m
based on estimating equations. The proof is similar to that of

Theorem 2 except that we need to estimate the marginal distribution of Y in addition. We use the
sample proportion p̂k , which is asymptotically linear with influence function IFpk (y, w) = I{y = k} − pk .

56

Consider the following estimating equation

2

UMW (α, β, p̃, σ )(y, w) =

K−1
X
k=1

1
− (p̃k + p̃k+1 )[I{y ≤ k} − θα,β (k, w)]
2

!2

− σ2 .

2
2
By definition, σm
is the unique solution to the equation P UMW (α∗ , β ∗ , p, σ 2 ) = 0, and σ̂m
solves its

empirical counterpart. Hence, we have
2
2
0 = Pn UMW (α̂, β̂, p̂, σ̂m
) − P UMW (α∗ , β ∗ , p, σm
)
n
o
2
2
2
= (Pn − P )UMW (α∗ , β ∗ , p, σm
) + P UMW (α̂, β̂, p̂, σ̂m
) − UMW (α∗ , β ∗ , p, σm
)
n
o
2
2
+ (Pn − P ) UMW (α̂, β̂, p̂, σ̂m
) − UMW (α∗ , β ∗ , p, σm
) .
2
Consistency of (α̂, β̂) and p̂ implies that σ̂m
is also consistent. This together with the bounded sup2
2
port of W implies that kUMW (α̂, β̂, p̂, σ̂m
) − UMW (α∗ , β ∗ , p, σm
)kL2 (P ) = oP (1). Furthermore, we can
2
show that UMW is a Lipschitz function of (α, β, p̃, σ 2 ) in a neighborhood of (α∗ , β ∗ , p, σm
), and thus
2
UMW (α̂, β̂, p̂, σ̂m
) belongs to a Donsker class with probability tending to 1 [Example 19.7, 31]. Lemma

19.24 in Van der Vaart [31] implies that the last term in the above display is oP (n−1/2 ).
Applying a Taylor expansion, we have that


∂
2
2
2
σ̂m
− σm
= (Pn − P )UMW (α∗ , β ∗ , p, σm
)+
P UMW |(α∗ ,β ∗ ,p) (α̂ − α∗ )
∂α




∂
∂
∗
−1/2
+
).
P UMW |(α∗ ,β ∗ ,p) (β̂ − β ) +
P UMW |⊤
(α∗ ,β ∗ ,p) (p̂ − p) + oP (n
∂β
∂ p̃

57

Note that
∂
1
UMW |(α∗ ,β ∗ ,p) (y, w) = −
∂αk
2

(K−1
)
X
∗
(pl + pl+1 ) [I{y ≤ l} − θ (l, w)]
l=1

× (pk + pk+1 )θ∗ (k, w) {1 − θ∗ (k, w)}
(K−1
)
1 X
∂
∗
(pl + pl+1 ) [I{y ≤ l} − θ (l, w)]
UMW |(α∗ ,β ∗ ,p) (y, w) = −
∂β
2
l=1
"K−1
#
X
∗
∗
×
(pk + pk+1 )W θ (k, w) {1 − θ (k, w)}
k=1

(K−1
)
1 X
∂
∗
UMW |(α∗ ,β ∗ ,p) (y, w) =
(pl + pl+1 ) [I{y ≤ l} − θ (l, w)]
∂ p̃k
2
l=1

× [I{y ≤ k} − θ∗ (k, w) + I{Y ≤ k − 1} − θ∗ (k − 1, w)] .
2
2
2
Hence, σ̂m
satisfies σ̂m
− σm
= (Pn − P )IFm + oP (n−1/2 ), where

∗

IFm (y, w) = UMW (α , β
+

∗

2
, p, σm
)(y, w)

K
X
∂P UMW

k=1

∂ p̃k

+



∂P UMW
|(α∗ ,β ∗ ,p)
∂(α, β)

⊤

IFab (y, w)

|(α∗ ,β ∗ ,p) IFpk (y, w).

We note that using similar arguments as Lemma 10, we can show that IFm is the canonical gradient of
2
2
σm
, and therefore σ̂m
is a regular estimator [Proposition 2.3.i, 23].

We estimate the unadjusted variance by plugging in p̂k . By the delta method, σ̂u2 is asymptotically
P
2
linear with influence function IFu = − K
k=1 pk IFpk /4.
Next we establish the asymptotic linearity of σ̂a2 . Applying Lemma 8 with h(ỹ, y) = I{ỹ < y}+ 12 {ỹ =

y}, we obtain the EIF of σa2 as follows
2

IFa (y, w) = {η(y) − r(w)} + 2

Z

{η(ỹ) − r(w̃)} h(y, ỹ)dP (ỹ, w̃) − 3σa2 .

We show through direct linearization that σ̂a2 is indeed asymptotically linear with influence function IFa .

58

To start, we note that σ̂a2 = Pn (η̂ − r̂)2 and σa2 = P (η − r)2 . Thus,
σ̂a2 − σa2 = Pn (η̂ − r̂)2 − P (η − r)2
= Pn (η̂ − r̂)2 − Pn (η − r̂)2 + Pn (η − r̂)2 − P (η − r)2 .
|
{z
} |
{z
}
term 1

We analyze term 2 first.

term 2

Pn (η − r̂)2 − P (η − r)2

(23)



= (Pn − P )(η − r)2 + (Pn − P ) (η − r̂)2 − (η − r)2 + P (η − r̂)2 − (η − r)2
The first term on the right-hand side is already linear and contributes the first term to the influence
function. By the assumption that kr̂ − rkL2 (PW ) = oP (n−1/4 ), the third term is negligible because

P (η − r̂)2 − (η − r)2


= P 2(η − r)(r − r̂) + (r − r̂)2

= P (r − r̂)2 = oP (n−1/2 ).

We now turn to the second term on the right-hand side of (23). By Theorem 2.10.6 in Van Der Vaart
and Wellner [32], the fact that r̂ belongs to a bounded P -Donsker class with probability tending to 1
implies that (η − r̂)2 − (η − r)2 also belongs to a P -Donsker class with probability tending to 1, as η and
r are both fixed and bounded functions. Furthermore,
k(η − r̂)2 − (η − r)2 kL2 (P ) = k(2η − r̂ − r)(r̂ − r)kL2 (P ) ≤ M kr̂ − rkL2 (P ) = oP (1),
for some constant M . Lemma 19.24 in Van der Vaart [31] implies that the second term is also oP (n−1/2 ).
We now analyze term 1. Note that


term 1 = (Pn − P ) (η̂ − r̂)2 − (η − r̂)2 + P (η̂ − r̂)2 − (η − r̂)2

= (Pn − P ) (η̂ − r̂)2 − (η − r̂)2 + P {(η̂ + η − 2r̂)(η̂ − η)}
= P {(2η − 2r)(η̂ − η)} + P {(η̂ − η + 2r − 2r̂)(η̂ − η)}

+ (Pn − P ) (η̂ − r̂)2 − (η − r̂)2 .

59

We note that η̂ belongs to a bounded P -Donsker class, as the empirical distribution function lies in the
closure of the convex hull of a P -Donsker class. Theorem 2.10.6 in Van Der Vaart and Wellner [32] again
implies that (η̂ − r̂)2 − (η − r̂)2 belongs to a P -Donsker class with probability tending to 1. Moreover,
k(η̂ − r̂)2 − (η − r̂)2 kL2 (P ) = k(η̂ − η)(η̂ + η − 2r̂)kL2 (P ) ≤ M kη̂ − ηkL2 (P ) = oP (1).
Thus the third term is oP (n−1/2 ). The second term is also oP (n−1/2 ) by our assumption on the convergence rate of r̂ and the convergence of η̂. Finally, the first term is the linear term that contributes to the
rest of IFa . To see this, we write it in the integral form.
Z

2{η(ỹ) − r(w̃)}{η̂(ỹ) − η(ỹ)}dP (ỹ, w̃)
Z

Z
= 2{η(ỹ) − r(w̃)}
h(y, ỹ)d(Pn − P )(y) dP (ỹ, w̃)
Z
= 2(Pn − P ) {η(ỹ) − r(w̃)}h(·, ỹ)dP (ỹ, w̃).

R
Note that Lemma 8 implies that P {η(ỹ) − r(w̃)}h(·, ỹ)dP (ỹ, w̃) = σa2 .

Theorem 3 now follows by applying the delta method. Specifically, the influence function of φ̂a is

given by (IFa − φa IFu )/σu2 , and similarly the influence function of φ̂m is (IFm − φm IFu )/σu2 . These
estimators are RAL in any locally nonparametric model.
Proof of Lemma 5. Recall that the unadjusted estimator is given by
Pnt
K−1
o
Xn
I{Yit ≤ k, Ai = a}
1
ψ̂u =
.
logit F̃1 (k) − logit F̃0 (k) , F̃a (k) = i=1
Pnt
K −1
I{Ai = a}
i=1

k=1

Applying the delta method shows that its influence function is given by

Du (y t , at , wt ) =

K−1
X  at [I{y t ≤ k} − F1 (k)] (1 − at ) [I{y t ≤ k} − F0 (k)] 
1
,
−
K −1
π1 F1 (k){1 − F1 (k)}
π0 F0 (k){1 − F0 (k)}
k=1

which, under the sharp null, simplifies to


K−1
X  at
1 − at
I{y t ≤ k} − F (k)
1
.
−
K −1
π1
π0
F (k){1 − F (k)}
k=1

60

Due to the independence of A and (Y t , W t ) under the sharp null, the variance of Du (Y t , A, W t ) under
the sharp null is σu2 /(π1 π0 ).
The working-model-based adjusted estimator ψ̂m replaces F̃a with F̂a . By Lemma 2 and the delta
method, the influence function of ψ̂m is
K−1
X  IFF1 (k) (y t , at , wt ) IFF0 (k) (y t , at , wt ) 
1
Dm (y , a , w ) =
,
−
K −1
F1 (k){1 − F1 (k)} F0 (k){1 − F0 (k)}
t

t

t

k=1

which under the sharp null becomes

Dm (y t , at , wt ) =



1 − at
at
−
π1
π0



K−1
X I{y t ≤ k} − θ∗ (k, wt )
1
,
K −1
F (k){1 − F (k)}
k=1

2
The variance of Dm (Y t , A, W t ) under the sharp null is σm
/(π1 π0 ).

Finally the efficient influence function can be obtained by projecting Du onto the tangent space.

D∗ (y t , at , wt ) =

K−1
X  at [I{y t ≤ k} − θ1 (k, wt )]/π1 + θ1 (k, wt ) − F1 (k)
1
K −1
F1 (k){1 − F1 (k)}
k=1

−

(1 − at )[I{y t ≤ k} − θ0 (k, wt )]/π0 + θ0 (k, wt ) − F0 (k) 
.
F0 (k){1 − F0 (k)}

Under the sharp null, it simplifies to


K−1
X  at
1 − at I{y t ≤ k} − θ(k, wt )
1
,
−
D (y , a , w ) =
K −1
π
π0
F (k){1 − F (k)}
∗

t

t

t

k=1

The variance of D∗ (Y t , A, W t ) under the sharp null is σa2 /(π1 π0 ).
Proof of Theorem 4. We first consider estimating σu2 . Define the following estimating equation
"

K−1
1 X I{y ≤ k} − F̌ (k)
ULOR,u (F̌ , σ )(y, w) =
K −1
F̌ (k){1 − F̌ (k)}
k=1
2

#2

− σ2 .

(24)

Then, σu2 is the unique solution in σ 2 to the equation P ULOR,u (F , σ 2 ) = 0; and σ̂u2 solves Pn ULOR,u (F̃ , σ 2 ) =
0, with F̃ being the CDF of the empirical distribution of Y . We can apply similar estimating equation

61

arguments as in the proof of Theorems 2 and 3 to show that σ̂u2 − σu2 = (Pn − P )IFu + oP (n−1/2 ), where
IFu (y, w) = ULOR,u (F , σu2 )(y, w) +

K
X
∂P ULOR,u

k=1

∂ F̌ (k)

F

[I{y ≤ k} − F (k)] .

This implies that σ̂u2 is asymptotically linear with influence function IFu . In particular,
∂ULOR,u
∂ F̌ (k)

2
(y, w) = −
(K − 1)2
F

#
"K−1
X I{y ≤ l} − F (l)  I{y ≤ k} − F (k) 2
.
F (l){1 − F (l)}
F (k){1 − F (k)}
l=1

2
Next we consider the estimation of σm
. Define the following estimating equation

"

K−1
X I{y ≤ k} − θα,β (k, w)
1
ULOR (α, β, F̌ , σ )(y, w) =
K −1
F̌ (k){1 − F̌ (k)}
k=1
2

#2

− σ2 .

2
2
Then, σm
is the solution in σ 2 to P ULOR (α∗ , β ∗ , F , σ 2 ) = 0, while σ̂m
is the solution to Pn ULOR (α̂, β̂, F̃ , σ 2 ) =

0. We can again apply estimating equation arguments as in the proof of Theorems 2 and 3 to show that
2
2
σ̂m
− σm
= (Pn − P )IFm (y, w) + oP (n−1/2 ),

where

2
)(y, w) +
IFm (y, w) = ULOR (α∗ , β ∗ , F , σm



∂P ULOR
∂(α, β)

(α∗ ,β ∗ ,F )

+

⊤

IFab (y, w)

K
X
∂P ULOR
k=1

∂ F̌ (k)

(α∗ ,β ∗ ,F )

[I{y ≤ k} − F (k)] .

2
This implies that σ̂m
is asymptotically linear with influence function IFm . Here,

#
"K−1
X I{y ≤ l} − θ∗ (l, w) θ∗ (k, w){1 − θ∗ (k, w)}
,
F (l){1 − F (l)}
F (k){1 − F (k)}
l=1
#" K
#
"K−1
X I{y ≤ l} − θ∗ (l, w) X
∂ULOR
wθ∗ (l, w){1 − θ∗ (l, w)}
2
,
(y, w) = −
∂β (α∗ ,β ∗ ,F )
(K − 1)2
F (l){1 − F (l)}
F (l){1 − F (l)}
l=1
l=1
#
"K−1
X I{y ≤ l} − θ∗ (l, w) [I{y ≤ k} − F (k)]{1 − 2F (k)}
2
∂ULOR
(y, w) = −
.
(K − 1)2
F (l){1 − F (l)}
F 2 (k){1 − F (k)}2
∂ F̌ (k) (α∗ ,β ∗ ,F )
l=1
∂ULOR
∂αk

2
(y, w) = −
∗
∗
(K − 1)2
(α ,β ,F )

62

Finally we consider the estimation of σa2 . We define σkl := EP [cov(I{Y ≤ k}, I{Y ≤ l}|W )]. Then
σa2 can be equivalently written as
σa2 (P ) =

K−1
X K−1
X
1
σkl
.
2
(K − 1)
F (k)F (l){1 − F (k)}{1 − F (l)}
k=1 l=1

We estimate F (k) with F̃ (k), which is an asymptotically linear estimator. By Lemma 9, the EIF of σkl
is given by
Dkl (y, w) = [I{y ≤ k} − θ(k, w)] [I{y ≤ l} − θ(l, w)] − σkl .
We show that the estimator σ̂kl = n−1

Pn

i=1 [I{Yi

≤ k} − θ̂(k, Wi )][I{Yi ≤ l} − θ̂(l, Wi )] is asymp-

totically linear with influence function Dkl . For the ease of notation, we use Ik as shorthand for the
function y 7→ I{y ≤ k}, θk for the function w 7→ θ(k, w) and θ̂k for the function w 7→ θ̂(k, w), for
k ∈ {1, . . . , K − 1}. Then we have that σ̂kl = Pn {(Ik − θ̂k )(Il − θ̂l )} and σkl = P {(Ik − θk )(Il − θl )}.
Therefore,
o
n
σ̂kl − σkl = Pn (Ik − θ̂k )(Il − θ̂l ) − P {(Ik − θk )(Il − θl )}
n
o
= (Pn − P ) {(Ik − θk )(Il − θl )} + P (Ik − θ̂k )(Il − θ̂l ) − (Ik − θk )(Il − θl )
n
o
+ (Pn − P ) (Ik − θ̂k )(Il − θ̂l ) − (Ik − θk )(Il − θl ) .
The first term is exactly (Pn − P )Dkl . For the second term, we note that
n
o
P (Il − θl )(θk − θ̂k ) + (Ik − θk )(θl − θ̂l ) + (θk − θ̂k )(θl − θ̂l )
n
o
= P (θk − θ̂k )(θl − θ̂l ) ≤ kθ̂k − θk kL2 (PW ) kθ̂l − θl kL2 (PW ) = oP (n−1/2 ).
Now we turn to the third term. By Theorem 2.10.6 in Van Der Vaart and Wellner [32], the fact that θ̂k
and θ̂l belong to fixed P -Donsker classes with probability tending to 1 implies that (Ik − θ̂k )(Il − θ̂l ) −
(Ik − θk )(Il − θl ) also belongs to a fixed P-Donsker with probability tending to 1, as the support of W
is bounded and Ik , Il , θk and θl are all fixed and bounded functions. In addition, k(Ik − θ̂k )(Il − θ̂l ) −
(Ik − θk )(Il − θl )kL2 (P ) = oP (1). Thus, Lemma 19.24 implies that the third term is oP (n−1/2 ). Hence
σ̂kl has influence function Dkl .

63

Applying the delta method, we see that σ̂a2 is asymptotically linear with influence function
K−1
X K−1
X
1
IFa (y, w) =
(K − 1)2
k=1 l=1

Dkl (y, w)
F (k)F (l){1 − F (k)}{1 − F (l)}

!
σkl {1 − 2F (k)}[I{y ≤ k} − F (k)]
σkl {1 − 2F (l)}[I{y ≤ l} − F (l)]
− 2
.
− 2
F (k){1 − F (k)}2 F (l){1 − F (l)}
F (l){1 − F (l)}2 F (k){1 − F (k)}

Theorem 4 then follows by applying the delta method. Specifically, the influence function of φ̂a is
given by (IFa − φa IFu )/σu2 , and similarly the influence function of φ̂m is (IFm − φm IFu )/σu2 . Regularity
2
of σ̂u2 and σ̂m
can be established using arguments similar to Lemma 10. Finally, as we are working within

a locally nonparametric model, all of these estimators are efficient.
Proof of Theorem 5. First we introduce some notations. Let P∗n denote the empirical distribution of a
generic first-layer bootstrap resample from the external data X. Let X̃ denote a generic second-layer
sample from P∗n Π, where Π is the (known) distribution of the treatment. In what follows we consider a
generic estimator such that, for any distribution Q of (Y, W ), the estimator is asymptotically linear with
influence function DQΠ in the trial with distribution QΠ. We define σ 2 (Q) := varQΠ [DQΠ (Y, A, W )], and
we recall that σ̃ 2 (P∗n ) denotes N varP∗n [ψ̂(X̃)]. Because Π is fixed, we omit the dependences of σ 2 and σ̃ 2
on this quantity.
The proof below is a modification of the proof of Theorem 23.9 in Van der Vaart [31]. Let BL1 (R)
be the set of all functions h : R −
→ [−1, 1] that are uniformly Lipschitz. We use subscript M to denote
taking expectation conditionally on the external data X1 , X2 , . . .. Let (σ 2 )′ be the Gâteaux derivative
of the functional σ 2 : Q 7→ σ 2 (Q) = varQΠ [DQΠ (Y, A, W )]. To start, we note that

EM h

sup
h∈BL1 (R)

≤

sup

EM h

h∈BL1 (R)

|



√  2 ∗
n σ̃ (Pn ) − σ 2 (Pn ) − Eh (σ 2 )′ (G)



√  2 ∗
n σ (Pn ) − σ 2 (Pn ) − Eh (σ 2 )′ (G)
{z

}

term 1

+

sup
h∈BL1 (R)

|

EM h



√ 
√  2 ∗
n σ̃ (Pn ) − σ 2 (Pn ) − EM h n σ 2 (P∗n ) − σ 2 (Pn )
.
{z

term 2

64

}

We study term 2 first. By the Lipschitz property of h, term 2 is bounded by

sup

EM

h∈BL1 (R)

We now show that EM

√  2 ∗
n σ̃ (Pn ) − σ 2 (P∗n )

= EM

√

n σ̃ 2 (P∗n ) − σ 2 (P∗n ) .

 P
√
n σ̃ 2 (P∗n ) − σ 2 (P∗n ) → 0. Recall the asymptotic linear expansion (12),
N
o
√ n
√
1 X
N ψ̂(X̃) − ψ(P∗n Π) = √
DP∗n Π (X̃l ) + N Rem.
N l=1

It then follows that
σ̃ 2 (P∗n ) = σ 2 (P∗n ) + 2covP∗n

!
N
√
1 X
√
DP∗n Π (X̃l ), N Rem + N varP∗n (Rem).
N l=1



We note that N varP∗n (Rem) = N 1−2γ varP∗n (N γ Rem). Therefore, Condition B2 implies that E N varP∗n (Rem) ≤
LN 1−2γ for some constant L. In addition, the boundedness of the support of X implies that varP∗n [DP∗n Π (X̃)]
is also bounded. Thus, by the Cauchy Schwartz inequality and Jensen’s inequality,

E

"

covP∗n

≤E

"r

N
√
1 X
√
DP∗nΠ (X̃l ), N Rem
N l=1

!#

h
i
varP∗n DP∗n Π (X̃) N 1−2γ varP∗n (N γ Rem)

#

≤

p
L1 N 1−2γ ,

√
 √
for some constant L1 . As a result, we have that E n σ̃ 2 (P∗n ) − σ 2 (P∗n ) ≤ L2 nN 1−2γ for some con
√
 √
stant L2 . Or equivalently, E EM
n σ̃ 2 (P∗n ) − σ 2 (P∗n ) ≤ L2 nN 1−2γ , where the outer expectation

is over X1 , X2 , . . . Markov’s inequality and Condition B4 then imply that, for all ǫ > 0,

P (EM

This shows that EM


√
n σ̃ 2 (P∗n ) − σ 2 (P∗n ) > ǫ) ≤

√

n σ̃ 2 (P∗n ) − σ 2 (P∗n )



P

→ 0.

65

√
L2 nN 1−2γ
→ 0,
ǫ

as n → ∞.

We now turn to term 1, which further decomposes into

sup
h∈BL1 (R)

|

+



√
√  2 ∗
n σ (Pn ) − σ 2 (Pn ) − EM h (σ 2 )′ ( n(P∗n − Pn ))
{z

term 1.1

sup

h∈BL1 (R)

|

EM h



√
EM h (σ 2 )′ ( n(P∗n − Pn )) − Eh (σ 2 )′ (G) .
{z

}

}

term 1.2

Theorem 23.7 and Equation 23.8 in Van der Vaart [31] imply that term 1.2 converges to 0 in probability. For term 1.1, the same argument as was used in the proof of Theorem 23.9 in Van der Vaart [31]
shows that this term converges to 0 in probability.
Combining these steps as in the proof of Theorem 23.9 in Van der Vaart [31], we see that

√
n{σ̃ 2 (P∗n )−

σ 2 (Pn )} converges conditionally in distribution to (σ 2 )′ (G), given X1 , X2 , . . ., in probability.
In particular, we can apply the above argument to the variance of the unadjusted estimator σu2
√
2
and the variance of the working-model-based estimator σm
to show that (i) n{σ̃u2 (P∗n ) − σu2 (Pn )} con√
2
2
verges conditionally in distribution to (σu2 )′ (G), and (ii) n{σ̃m
(P∗n ) − σm
(Pn )} converges condition2 ′
ally in distribution to (σm
) (G), both given X1 , X2 , . . ., in probability. The delta method implies that
√
2
n{σ̃m
(P∗n )/σ̃u2 (P∗n ) − φm (Pn )} converges conditionally in distribution to (φm )′ (G), given X1 , X2 , . . ., in

probability. Finally, Theorem 3.10 follows by Condition B3 and Slutsky’s theorem.

C

Proofs of results for time-to-event outcomes with right censoring

All three estimands of the treatment effect we consider are transformations of the arm-specific survival
functions S1 (t) and S0 (t). Recall that for the unadjusted analysis we plug in S̃1 (t) and S̃0 (t), the armspecific KM estimators, and, for the adjusted analysis, we plug in Ŝ1 (t) and Ŝ0 (t), the efficient adjusted
estimators for the arm-specific survival function.
The influence function of the KM estimator was derived, for example, in Reid [25]. In particular, if

66

T t ⊥ C t |A, then S̃a (tk ) is an asymptotically linear estimator of Sa (tk ) with influence function
ηa,k (y t , δ t , at , wt ) =

k
X
j=1

−


I{at = a}Sa (tk ) 
I{y t = tj , δ t = 1} − ha (tj )I{y t ≥ tj } ,
Sa (tj )Ga (tj )πa

for a = 0 or 1, and k ∈ {1, . . . , K}. Here ha (t) is the hazard corresponding to Sa at time t, Ga (t, w) :=
P (C t ≥ t|A = a, W t = w) and Ga (t) := P (C t ≥ t|A = a).
Under the assumption that T t ⊥ C t |(A, W t ) and other regularity conditions, Ŝa (tk ) is asymptotically
linear with influence function given in Moore and van der Laan [20]

λa,k (y t , δ t , at , wt ) =

k
X
j=1

−


I{at = a}Sa (tk , wt ) 
I{y t = tj , δ t = 1} − I{y t ≥ tj }ha (tj , wt )
t
t
πa Sa (tj , w )Ga (tj , w )

+ Sa (tk , wt ) − Sa (tk ),
for a = 0 or 1, and k ∈ {1, . . . , K}.

C.1

Supporting lemmas for proofs in Section C.2

Lemma 11. (EIF of the variance of the fully-adjusted estimators) For (k, l) ∈ {1, . . . , K}2 and j ∈
{1, . . . , min(k, l)}, let Djkl be defined as in (25). When we measure the treatment effect with the risk
Pk
difference S0 (tk ) − S1 (tk ) or the relative risk {1 − S1 (tk )}/{1 − S0 (tk )}, the EIF of σa2 is j=1 Djkk .
Pk
When we use restricted mean survival time j=1 {S1 (tj ) − S0 (tj )} as the treatment effect estimand, the
Pk Pk Pmin(j,l) jl
EIF of σa2 is j=1 l=1 u=1
Du .

Proof. Recall that we define fjkl (·) as

fjkl (w) = S(tk , w)S(tl , w)



1
1
−
S(tj , w) S(tj−1 , w)



.

When we wish to emphasize the dependence of fjkl on P through its survival function, we instead denote
kl
kl
this function by fj,P
. First we define a parameter σjkl : M → R such that σjkl (P ) := EP [fj,P
(W )/G(tj , W )].

We consider the efficient influence function of σjkl in the full data model, that is, the model where we
observe (T, W ) and there is no censoring. Let p(t, w) = p(t|w)p(w) be the density of the joint distribution.
We consider the one-dimensional submodel {Pǫ : |ǫ| ≤ 1} with density p(t|w){1 + ǫs1 (t|w)}p(w){1 +
67

ǫs2 (w)}, where the range of s1 and s2 falls in [−1, 1] and these functions satisfy EP [s1 (Y |W )|W ] = 0
kl
P -almost surely and EP [s2 (W )] = 0. Let Sǫ be the survival function corresponding to Pǫ and define fj,ǫ

similarly to fjkl but with S replaced by Sǫ . We omit the subscript “j” and superscript “kl” below when
it is clear from the context that we are focusing on fjkl .
Note that
Z
d
d
Sǫ (tk , w) =
I{t > tk }{1 + ǫs1 (t|w)}dP (t|w)
dǫ
dǫ
Z
= [I{t > tk } − S(t, w)]s1 (t|w)dP (t|w)
Z
= [I{t > tk } − S(t, w)]{s1 (t|w) + s2 (w)}dP (t|w).
Because
σjkl (Pǫ )

=

Z

Sǫ (tk , w)Sǫ (tl , w)
G(tj , w)



1
1
−
Sǫ (tj , w) Sǫ (tj−1 , w)



p(w){1 + ǫs2 (w)}dw,

we then see that
d kl
σ (Pǫ )
dǫ j

ǫ=0

=

Z

f (w)
s2 (w)dP (w) +
G(tj , w)

Z

1
d
fǫ (w)
G(tj , w) dǫ

ǫ=0

dP (w).

Let τf ull,k (t, w) = I{t > tk } − S(tk , w). By definition, the gradient is given by
n
Df ull (t, w) = f (w) + dl (w)τf ull,l (t, w) + dk (w)τf ull,k (t, w) + dj (w)τf ull,j (t, w)
o
+ dj−1 (w)τf ull,(j−1) (t, w) /G(tj , w) − E[f (w)/G(tj , w)],
where the partial derivatives dl , dk , dj , and dj−1 are given by
S(tk , w)
S(tk , w)
S(tl , w)
S(tl , w)
−
, dk (w) =
−
,
S(tj , w)
S(tj−1 , w)
S(tj , w) S(tj−1 , w)
S(tk , w)S(tl , w)
S(tk , w)S(tl , w)
, dj−1 (w) =
.
dj (w) = −
S 2 (tj , w)
S 2 (tj−1 , w)
dl (w) =

This is the EIF in the full data model, as we work with a locally nonparametric model.
The observed data unit (with censoring) is (Y, ∆, W ). To find the EIF in the observed data model,

68

we apply Theorem 10.1 of Tsiatis [28] to show that the following is an observed data influence function
of σjkl :
n
Djkl (y, δ, w) = fjkl (w) + dl (w)τl (y, δ, w) + dk (w)τk (y, δ, w) + dj (w)τj (y, δ, w)
o
+ dj−1 (w)τj−1 (y, δ, w) /G(tj , w) − E[fjkl (w)/G(tj , w)]

= gjkl (y, δ, w) + fjkl (w) /G(tj , w) − E[fjkl (w)/G(tj , w)].

(25)

Moreover, as our observed data model is locally nonparametric, Djkl is the efficient observed data influence
function of σjkl . Lemma 11 then follows since the variances are linear combinations of E[fjkl (W )/G(tj , W )].

Lemma 12 (Computation time of σ̂a2 with RMST). For given τ̂ and Ŝ, the function
k X
k min(j,l)
o
X
X n
ĝujl (y, δ, w) + fˆujl (w)
(y, δ, w) 7→
j=1 l=1

u=1

can be computed in O(k) time.
Proof. There are 5 terms inside the sums, and we show that the sum of each term can be computed in
O(k) time.
To start, we note that
k X
k min(j,l)
X
X
j=1 l=1

u=1

fˆujl (w) =

k X
k min(j,l)
X
X

Ŝ(tj , w)Ŝ(tl , w)

(

1

1

)

−
Ŝ(tu , w) Ŝ(tu−1 , w)


(
)
k
X
 X

X
1
1

=
−
Ŝ(tj , w)
Ŝ(tl , w)  .


Ŝ(tu , w) Ŝ(tu−1 , w) j≥u
u=1
l≥u
j=1 l=1

By taking a cumulative sum, {

P

l≥u

u=1

Ŝ(tl , w)}ku=1 can be pre-computed in O(k) time. Thus, the above

display can be computed in O(k) time as we sum over u.

69

The terms in g are of two types. The first of these is
k X
k min(j,l)
X
X

(

1

1

)

τ̂l (y, δ, w)
−
Ŝ(tu , w) Ŝ(tu−1 , w)


(
)
k
X
 X

X
1
1

=
−
τ̂l (y, δ, w)
Ŝ(tj , w)  .


Ŝ(tu , w) Ŝ(tu−1 , w)  l≥u
u=1
j≥u
Ŝ(tj , w)

u=1

j=1 l=1

P
By taking a cumulative sum, ( l≥u τ̂l )ku=1 can be pre-computed in O(k) time and summing over u takes

another O(k) steps. The second type of term is
k X
k min(j,l)
X
X Ŝ(tj , w)Ŝ(tl , w)
j=1 l=1

u=1

Ŝ 2 (tu , w)

τ̂u (y, δ, w) =


(
)




X
X
 τ̂u (y, δ, w)
Ŝ(tl , w)
Ŝ(tj , w)  .


Ŝ 2 (tu , w)  l≥u
u=1
j≥u
k
X

Again with the sum over j and l pre-computed for all u, the summation over u takes O(k) time.

C.2

Results in Appendix A

Proof of Lemma 6. First we note that the (conditional) independencies A ⊥ W t , T t ⊥ A|W t , C t ⊥ T t |A
and C t ⊥ T t |(A, W t ) together imply that T t ⊥ C t and T t ⊥ C t |W t . This result will be useful when we
compute the variances of the adjusted and unadjusted estimators.
We consider the risk difference first. The unadjusted estimator, namely ψ̂u = S̃0 (tk ) − S̃1 (tk ), has
influence function Du,k = η0,k − η1,k . Under the sharp null where the treatment has no effect and the
assumption that C t ⊥ A|W t , the influence function simplifies to
t

t

t

t

Du,k (y , δ , a , w ) =



1 − at
at
−
π1
π0

X
k
j=1

S(tk )
Ij (y t , δ t ), where
S(tj )G(tj )

Ij (y t , δ t ) = I{y t = tj , δ t = 1} − h(tj )I{y t ≥ tj }.
Noting that


E Ij (Y t , ∆t )Il (Y t , ∆t ) = δjl P (Y = tj )G(tj ){1 − h(tj )}, where δjl = I{j = l},
70

(26)

we see that

var(Du,k ) =

k
1 X S 2 (tk )S(tj−1 )h(tj ){1 − h(tj )}
π1 π0 j=1
S 2 (tj )G(tj )

k
σ2
1 X S 2 (tk ){S(tj−1 ) − S(tj )}
= u .
=
π1 π0 j=1
S(tj )G(tj )S(tj−1 )
π1 π0

The adjusted estimator, namely ψ̂a = Ŝ0 (tk ) − Ŝ1 (tk ), has influence function Da,k = λ0,k − λ1,k . Under
the sharp null and the assumption that C t ⊥ A|W t , the influence function becomes
t

t

t

t

Da,k (y , δ , a , w ) =



1 − at
at
−
π1
π0
k
X
j=1



×



S(tk , wt )
I{y t = tj , δ t = 1} − I{y t ≥ tj }h(tj , wt ) . (27)
S(tj , wt )G(tj , wt )

The variance of Da,k can be calculated in a similar way as was done for the unadjusted estimator, except
that in taking expectation of the indicators, we condition on W t first.


k
2
2
X
S
(t
,
W
){S(t
,
W
)
−
S(t
,
W
)}
1
k
j−1
j
 = σa .
EP 
var(Da,k ) =
π1 π0
S(tj , W )S(tj−1 , W )G(tj , W )
π1 π0
j=1

Hence the relative efficiency is given by σa2 /σu2 , which depends only on the distribution of survival time
and the covariate, for a user-specified mapping G.
Next we consider the relative risk. The unadjusted estimator, namely ψ̂u =
function

1
1−S0 (tk ) (−η1,k

1−S̃1 (tk )
,
1−S̃0 (tk )

has influence

+ ψη0,k ), which becomes Du,k /{1 − S(tk )} under the sharp null. Similarly, the

influence function of the adjusted estimator ψ̂a =

1−Ŝ1 (tk )
1−Ŝ0 (tk )

simplifies to Da,k /{1 − S(tk )} under the sharp

null and the assumption that C t ⊥ A|W t . Hence the relative efficiency is again σa2 /σu2 .
Proof of Theorem 6. Recall that Ŝ(tk ) is the efficient adjusted estimator of the survival probability at
time tk using the external data. Hence, this estimator is asymptotically linear with influence function

IFk (y, δ, w) = τk (y, δ, w) + S(tk , w) − S(tk ),

71

where τ is defined in (18). Furthermore, define IF0 (y, δ, w) := 0. Recall that
ŝjl
u =

Ŝ(tj )Ŝ(tl ){Ŝ(tu−1 ) − Ŝ(tu )}
Ŝ(tu )Ŝ(tu−1 )

.

jl
Applying the delta method, we have that ŝjl
u is an asymptotic linear estimator of su with influence

function

ξujl (y, δ, w) =



S(tj )
S(tj )
−
S(tu ) S(tu−1 )



IFl (y, δ, w) +




S(tl )
S(tl )
IFj (y, δ, w)
−
S(tu ) S(tu−1 )
S(tj )S(tl )
S(tj )S(tl )
−
IFu (y, δ, w) + 2
IFu−1 (y, δ, w).
S 2 (tu )
S (tu−1 )

Also, σ̂u2 is a linear combination of ŝkk
j , and its influence function is given by

IFu (y, δ, w) =

k
X

ξjkk (y, δ, w)/G(tj ).

j=1

We now consider estimating σa2 . Its efficient influence function is derived in Lemma 11, and also given
P
in (19), which is of the form IFa = kj=1 Djkk . The proposed σ̂a2 is a one-step estimator based on the
EIF.

Let Q be the distribution of the observed data unit (Y, ∆, W ) in the external dataset, induced by
the joint distribution P of (T, W ), the conditional distribution of the censoring time H and the function
Γ. Let P̂ be a joint distribution of (T, W ) such that the condtional hazard function is given by ĥ(t, w),
the conditional survival function is given by Ŝ(t, w) and the distribution of W is given by its empirical
distribution. Let Q̂ be the observed data distribution induced by P̂ , Ĥ and Γ. Then, we have that
σ̂a2 = Qn IFa (Q̂) + σa2 (P̂ ), where Qn is the empirical distribution of (Y, ∆, W ) in the external dataset.
Hence,

σ̂a2 − σa2 = (Qn − Q)IFa (Q) + (Qn − Q){IFa (Q̂) − IFa (Q)} + R(Q̂, Q),
where R(Q̂, Q) = σa2 (P̂ ) − σa2 (P ) + Q{IFa (Q̂)}.
Our first step is to show that the remainder term R(Q̂, Q) is oP (n−1/2 ). As the influence function
IFa and the variance σa2 itself can both be written as a sum of k terms, it is easy to see that we can write
72

R(Q̂, Q) as

Pk

j=1

Rj (Q̂, Q), where

Rj (Q̂, Q) = Q



n
o
1
ĝj (y, δ, w) + fˆj (w) − fj (w) .
G(tj , w)

Here we omit the superscript “kk” in f and g. We examine the terms coming from f and g separately.
First, we study

Q

(

fˆj (w) − fj (w)
G(tj , w)

)
=Q

1
G(tj , w)

"(

Ŝ 2 (tk , w)
Ŝ(tj , w)

−

Ŝ 2 (tk , w)
Ŝ(tj−1 , w)

)

−



S 2 (tk , w)
S 2 (tk , w)
−
S(tj , w)
S(tj−1 , w)

#!

.

Define

Rf 2
Rf 3






1
2S(tk , w)
2S(tk , w)
{Ŝ(tk , w) − S(tk , w)} ,
−
G(tj , w) S(tj , w)
S(tj−1 , w)


2
S (tk , w)
1
{
Ŝ(t
,
w)
−
S(t
,
w)}
,
=Q −
j
j
G(tj , w) S 2 (tj , w)


1
S 2 (tk , w)
=Q
{
Ŝ(t
,
w)
−
S(t
,
w)}
.
j−1
j−1
G(tj , w) S 2 (tj−1 , w)

Rf 1 = Q

Then, we apply a second-order Taylor expansion to the function (x, y, z) 7→ x2 (1/y − 1/z). The relevant
second-order derivatives are bounded by some constant M when Ŝ(t, w), S(t, w) and G(t, w) are all
uniformly bounded away from 0. Then,

Q

(

fˆj (w) − fj (w)
G(tj , w)

)

n
≤ Rf 1 + Rf 2 + Rf 3 + M kŜ(tk , ·) − S(tk , ·)k2L2 (PW )
+ kŜ(tj , ·) − S(tj , ·)k2L2 (PW ) + kŜ(tj−1 , ·) − S(tj−1 , ·)k2L2 (PW )
= Rf 1 + Rf 2 + Rf 3 + oP (n−1/2 ),

o

since kŜ(t, ·) − S(t, ·)kL2 (PW ) = oP (n−1/4 ) for all t.
Next, we study the terms in the remainder resulting from g. To do this, we define δhl (w) :=
S(tl−1 , w){h(tl , w) − ĥ(tl , w)}/Ŝ(tl , w). Then,
Q {ĝj (y, δ, w)/G(tj , w)} = Rg1 + Rg2 + Rg3 ,
73

where

Rg1

Rg2

Rg3


(
)

X Ŝ(t , w)H(t , w)
2
Ŝ(t
,
w)
2
Ŝ(t
,
w)
1
k
k
k
l
= Q
−
−
δhl (w)  ,

G(tj , w)
Ŝ(tj , w)
Ŝ(tj−1 , w)  l≤k
Ĥ(tl , w)



X Ŝ(t , w)H(t , w)

2
Ŝ
(t
,
w)
1
k
j
l
= Q −
−
δhl (w)  ,

G(tj , w) Ŝ 2 (tj , w) 
Ĥ(tl , w)
l≤j



 X Ŝ(t , w)H(t , w)

2
1
Ŝ
(t
,
w)
j−1
l
k
−
δhl (w)  .
= Q

G(tj , w) Ŝ 2 (tj−1 , w) 
Ĥ(tl , w)


l≤j−1

Noting that S(tk , w) =

Q

l≤k {1 − h(tl , w)}

and Ŝ(tk , w) =

between Ŝ(tk , w) and S(tk , w) as

Ŝ(tk , w) − S(tk , w) =

X

Q

l≤k {1 − ĥ(tl , w)},

we can write the difference

Ŝ(tk , w)δhl (w).

l≤k

Hence,

Rf 1 + Rg1

"

2S(tk , w)
1
2S(tk , w) X Ĥ(tl , w) − H(tl , w)
=Q
−
Ŝ(tk , w)δhl (w)
G(tj , w)
S(tj , w)
S(tj−1 , w)
Ĥ(t
,
w)
l
l≤k
)
#!
(
2S(tk , w) X Ŝ(tk , w)H(tl , w) l
2Ŝ(tk , w)
2S(tk , w)
2Ŝ(tk , w)
+
−
−
−
δh (w)
+
S(tj , w)
S(tj−1 , w)
Ŝ(tj , w)
Ŝ(tj−1 , w)
Ĥ(tl , w)
l≤k

The term in the first line on the right-hand side is oP (n−1/2 ) because G, S, Ŝ and Ĥ are uniformly
bounded away from 0; Ŝ is uniformly bounded above; and k{ĥ(t, ·) − h(t, ·)}{Ĥ(t, ·) − H(t, ·)}kL2 (PW ) =
oP (n−1/2 ) for all t. The term in the second line is also oP (n−1/2 ). To see this, we apply a first-order
Taylor expansion to the first factor, which is very similar to the second-order Taylor expansion we
studied earlier and the derivative is again bounded by some constant M . In addition, we have that
k{Ŝ(t, ·) − S(t, ·)}{ĥ(t̃, ·) − h(t̃, ·)}kL2 (PW ) = oP (n−1/2 ) for all (t, t̃).
Similarly, we can show that Rf 2 + Rg2 and Rf 3 + Rg3 are both oP (n−1/2 ), and so is Rj (Q̂, Q) and
consequently so is R(Q̂, Q).
Our second step is to show that (Qn − Q){IFa (Q̂) − IFa (Q)} is oP (n−1/2 ). To do this, we again
use Lemma 19.24 in Van der Vaart [31]. We need to verify the following two conditions: (1) kIFa (Q̂) −
IFa (Q)kL2 (Q) = oP (1), and (2) IFa (Q̂) lies in a fixed Q-Donsker class with probability tending to 1.

74

We first establish condition (1).

IFa (Q̂)(y, δ, w) − IFa (Q)(y, δ, w) = σa2 (Q̂) − σa2 (Q)
+

k
X
j=1

n
o
1
ĝj (y, δ, w) − gj (y, δ, w) + fˆj (w) − fj (w) .
G(tj , w)

Using the triangle inequality, it suffices to show that for each j, k{ĝj − gj }/G(tj , ·)kL2 (Q) = oP (1) and
k{fˆj − fj }/G(tj , ·)kL2 (Q) = oP (1), and that σa2 (Q̂) − σa2 (Q) = oP (1). As G is uniformly bounded away
from 0, it suffices to show that kĝj − gj kL2 (Q) = oP (1) and kfˆj − fj kL2 (Q) = oP (1).
fˆj (w) − fj (w) =

(

Ŝ 2 (tk , w)
Ŝ(tj , w)

−

Ŝ 2 (tk , w)
Ŝ(tj−1 , w)

)

−



S 2 (tk , w)
S 2 (tk , w)
−
S(tj , w)
S(tj−1 , w)



.

Recall that when studying the remainder term, we applied a second-order Taylor expansion to the
function (x, y, z) 7→ x2 (1/y − 1/z). Here a first-order Taylor expansion suffices. As S is bounded away
from 0, there exists some constant M such that the first derivatives are bounded by M . Then,


kfˆj − fj kL2 (Q) ≤ M kŜ(tk , ·) − S(tk , ·)kL2 (PW )

+ kŜ(tj , ·) − S(tj , ·)kL2 (PW ) + kŜ(tj−1 , ·) − S(tj−1 , ·)kL2 (PW ) = oP (1).

We note that ĝj − gj consists of three terms that are of similar forms. We study one of them in details
and similar arguments apply to the other two, and we can then apply the triangle inequality to conclude
that kĝj − gj kL2 (Q) = oP (1). For notational convenience, we define
τ̂k (y, δ, w) =

X
l≤k

−

Ŝ(tk , w)
Ŝ(tl , w)Ĥ(tl , w)

h
i
I{y = tl , δ = 1} − ĥ(tl , w)I{y ≥ tl } .

75

We focus on the term in ĝj − gj that is given by
(

)


2S(tk , w)
2S(tk , w)
τk (y, δ, w)
−
τ̂k (y, δ, w) −
−
S(tj , w)
S(tj−1 , w)
Ŝ(tj , w)
Ŝ(tj−1 , w)
(
)
2Ŝ(tk , w)
2S(tk , w)
2Ŝ(tk , w)
2S(tk , w)
=
τ̂k (y, δ, w)
+
−
−
S(tj , w)
S(tj−1 , w)
Ŝ(tj , w)
Ŝ(tj−1 , w)
|
{z
}
term 1


2S(tk , w)
2S(tk , w)
{τ̂k (y, δ, w) − τk (y, δ, w)} .
+
−
S(tj , w)
S(tj−1 , w)
{z
}
|
2Ŝ(tk , w)

2Ŝ(tk , w)



term 2

The triangle inequality allows us to bound each term separately. Since Ŝ and Ĥ are uniformly bounded
away from 0 and Ŝ and ĥ are uniformly bounded above, there exists some constant M such that |τ̂k | ≤ M .
Ŝ(tk ,·)
2S(tk ,·)
2Ŝ(tk ,·)
k ,·)
2
Therefore, in term 1 it suffices to upper bound k 2Ŝ(t
− Ŝ(t
− 2S(t
S(tj ,·) + S(tj−1 ,·) kL (PW ) . As in our
,·)
,·)
j

j−1

analysis of kfˆj − fj kL2 (PW ) , we apply a first-order Taylor expansion, where the first order derivatives are
bounded by some constant M ′ . Then,
2Ŝ(tk , ·)
Ŝ(tj , ·)

−

2Ŝ(tk , ·)

Ŝ(tj−1 , ·)

−

2S(tk , ·)
2S(tk , ·)
+
S(tj , ·)
S(tj−1 , ·)

L2 (PW )


≤ M ′ kŜ(tk , ·) − S(tk , ·)kL2 (PW )

+ kŜ(tj , ·) − S(tj , ·)kL2 (PW ) + kŜ(tj−1 , ·) − S(tj−1 , ·)kL2 (PW ) = oP (1).
Thus, term 1 is indeed oP (1). As S is uniformly bounded away from 0,

2S(tk ,w)
S(tj ,w)

−

2S(tk ,w)
S(tj−1 ,w)

is bounded

above. Therefore, it suffices to show that kτ̂k − τk kL2 (Q) = oP (1). Note that
τ̂k (y, δ, w) − τk (y, δ, w)
=

X

−

Ŝ(tk , w)I{y ≥ tl } n

o
h(tl , w) − ĥ(tl , w)

Ŝ(tl , w)Ĥ(tl , w)
{z
}
term 2.1
)
(
X
Ŝ(tk , w)
S(tk , w)
[I{y = tl , δ = 1} − h(tl , w)I{y ≥ tl }] .
+
−
S(tl , w)H(tl , w) Ŝ(tl , w)Ĥ(tl , w)
l≤k
{z
}
|
l≤k

|

term 2.2

To see that Term 2.1 is oP (1), note that the first factor is bounded above and kĥ(t, ·) − h(t, ·)kL2 (PW ) =
oP (1). To see that Term 2.2 is oP (1), note that the second factor is bounded above and the first factor

76

is oP (1), which can be shown again by a Taylor expansion. This argument shows one of the three terms
in ĝj − gj is oP (1). A similar argument can be applied to show the other two terms are oP (1) as well.
Finally we show that σa2 (Q̂) − σa2 (Q) = oP (1). Note that
σa2 (Q̂) − σa2 (Q) =
=

k 
X
j=1

Qn fˆj − Qfj

k n
X
j=1



o
(Qn − Q)fj + Q(fˆj − fj ) + (Qn − Q)(fˆj − fj ) .

The term (Qn − Q)fj is oP (1) by the law of large numbers. We have shown that kfˆj − fj kL2 (Q) is
oP (1), which provided an upper bound on Q(fˆj − fj ). As we will show momentarily, fˆj lies in a
Q-Donsker class with probability tending to 1, so Lemma 19.24 in Van der Vaart [31] implies that
(Qn − Q)(fˆj − fj ) = oP (1). Combining these results, we have σa2 (Q̂) − σa2 (Q) = oP (1).
Next we establish condition (2), which says that IFa (Q̂) lies in a Q-Donsker class with probability
tending to 1. By Theorem 2.10.6 in Van Der Vaart and Wellner [32], it suffices to show that ĝj and fˆj
both lie in Q-Donsker classes with probability tending to 1, since G is bounded away from 0. This can
be shown again by Theorem 2.10.6 in Van Der Vaart and Wellner [32], as by assumption Ŝ, Ĥ and ĥ all
belong to fixed Q-Donsker classes with probability tending to 1 and all the functions involved in ĝj and
fˆj are uniformly bounded away from 0 and also bounded above.
Conditions (1) and (2) allows us to apply Lemma 19.24 in Van der Vaart [31] to conclude that
(Qn − Q){IFa (Q̂) − IFa (Q)} is oP (n−1/2 ).
Now, combining step 1, which showed that R(Q̂, Q) is oP (n−1/2 ), and step 2, which showed that
(Qn − Q){IFa (Q̂) − IFa (Q)} is oP (n−1/2 ), we see that σ̂a2 is asymptotically linear with influence function
IFa . Theorem 6 then follows by the delta method. Specifically, the influence function of φ̂ is given by
(IFa − φa IFu )/σu2 . This estimator is efficient as its influence function agrees with the EIF of φa .
Pk
Proof of Lemma 7. The unadjusted estimator, namely ψ̂u = j=1 {S̃1 (tj ) − S̃0 (tj )}, has influence funcPk
Pk
tion j=1 (η1,j − η0,j ). Under the sharp null, the form of the influence function simplifies to j=1 Du,j ,
where the definition of Du,j is given in (26). Since

E[Du,j Du,l ] =

min(j,l)
X
1
S(tu−1 ) − S(tu )
S(tj )S(tl )
,
π1 π0
S(tu−1 )S(tu )G(tu )
u=1

77

we have that



var 

k
X
j=1



Du,j  =

k
k min(j,l)
1 X X X S(tj )S(tl ){S(tu−1 ) − S(tu )}
.
π1 π0 j=1
S(tu−1 )S(tu )G(tu )
u=1
l=1

Pk
Now we consider the fully adjusted estimator ψ̂a = j=1 {Ŝ1 (tj ) − Ŝ0 (tj )}. Under the sharp null, the
Pk
influence function of ψ̂a is given by j=1 Da,j , where Da,j is defined in (27). As in the proof of Lemma
Pk
6, the variance of j=1 Da,j can be derived in the same way, except that we condition on W first when

taking expectations.


var 

k
X
j=1



Da,j  =



k
k min(j,l)
1 XX X
S(tj , W )S(tl , W ){S(tu−1 , W ) − S(tu , W )}
.
EP
π1 π0 j=1
S(tu−1 , W )S(tu , W )G(tu , W )
u=1
l=1

jl
2
Proof of Theorem 7. σu2 is a linear combination of sjl
u . The asymptotic linearity of ŝu implies that σ̂u is

asymptotically linear with influence function

IFu (y, δ, w) =

k X
k min(j,l)
X
X

ξujl (y, δ, w)/G(tu ).

u

j=1 l=1

Moreover, σ̂a2 is again a one-step estimator based on its EIF given in Lemma 11. Using the same approach
as was used in the proof of Theorem 6, it can be shown that the remainder term R(Q̂, Q) is oP (n−1/2 ),
and (Qn − Q){IFa (Q̂) − IFa (Q)} = oP (n−1/2 ). Due to their close similarity to earlier arguments, we omit
Pk Pk Pmin(j,l) jl
the details here. We can then conclude that σ̂a2 has influence function IFa = j=1 l=1 u=1
Du ,

where the definition of Dujl is given in (25) in Lemma B.1.

Applying the delta method, the influence function of φ̂ is given by (IFa − φa IFu )/σu2 . This estimator
is efficient as its influence function agrees with the EIF of φa .


Proof of Theorem 8. We prove the first claim by showing that EP fujl (W )/G(tu , W ) ≥ sjl
u /G(tu ), for

all (u, j, l) such that max{j, l} ≤ k and u ≤ min(j, l). To start, we note that T ⊥ W under P implies

that fujl (w) = sjl
u for all w ∈ W. Thus, it suffices to show that EP [1/G(tu , W )] ≥ 1/G(tu ), which follows
from the convexity of the function a 7→ 1/a for a > 0 and Jensen’s inequality. Strict inequality holds
when varP [G(tu , W )] > 0 for some u.
To prove the second claim, we note that C ⊥ W implies that G(tj , w) = G(tj ) for all w ∈ W and
78

2
j ∈ {1, . . . , k}. We focus on the case of RD andRR first. Consider
 a bivariate function (a, b) 7→ a /b for
−2a/b2
 2/b
(a, b) ∈ (0, 1)2 . The Hessian matrix is given by 
 with eigenvalues 0 and 2(a2 + b2 )/b3 ,
2
2 3
−2a/b
2a /b

both of which are non-negative. Therefore, this function is convex for (a, b) ∈ (0, 1)2 . We note that


k
X
2
2

S (tk , W )
σa = EP
j=1



1
1
−
S(tj , W ) S(tj−1 , W )

k−1

S(tk , W ) X S 2 (tk , W )
= EP 
−
G(tk )
S(tj , W )
j=1






1 
G(tj )

1
1
−
G(tj+1 ) G(tj )




S 2 (tk , W ) 
−
S(t0 , W )G(t1 )

S(tk ) X
−
G(tk ) j=1

k−1 

1
1
−
G(tj+1 ) G(tj )



EP

S(tk ) X
≤
−
G(tk ) j=1

k−1 

1
1
−
G(tj+1 ) G(tj )



S 2 (tk )
S 2 (tk )
−
S(tj )
S(t0 )G(t1 )

=

=

k
X
j=1

2

S (tk )



1
1
−
S(tj ) S(tj−1 )






 2

S 2 (tk , W )
1
S (tk , W )
−
EP
S(tj , W )
G(t1 )
S(t0 , W )

1
= σu2 ,
G(tj )

where the inequality follows from Jensen’s inequality on the function (a, b) 7→ a2 /b for (a, b) ∈ (0, 1)2 .
Pq
For the case of RMST, we consider a (q + 1)-variate function that maps (a1 , . . . , aq , b) to ( i=1 ai )2 /b
Pq
for ai ∈ (0, 1) and b ∈ (0, 1). The only non-zero eigenvalue of its Hessian matrix is 2{( i=1 ai )2 +qb2 }/b3 ,
which is positive. Therefore, this function is convex for any q ≥ 1. In the following argument, we will

79

take q ∈ {1, . . . , k}.
σa2 =

k X
k min(j,l)
X
X
j=1 l=1

EP [fujl (W )/G(tu , W )]

u=1



k X
k min{j,l}
X
X

= EP
S(tj , W )S(tl , W )
u=1

j=1 l=1

= EP

"

k X
k
X
j=1 l=1

(

S(tmax{j,l} , W )
−
G(tmin{j,l} )

1
1
−
S(tu , W ) S(tu−1 , W )

min{j,l}−1

X

u=1

S(tj , W )S(tl , W )
S(tu , W )

)#






1 
G(tu )

1
1
−
G(tu+1 ) G(tu )



S(tj , W )S(tl , W )
S(t0 , W )G(t1 )




k X
k
k−1
k
k
X
X X
X
S(tmax{j,l} )
1
1
S(t
,
W
)S(t
,
W
)
j
l

=
− EP 
−
G(t
)
S(t
,
W
)
G(t
)
G(t
)
u
u+1
u
min{j,l}
u=1 j=u+1 l=u+1
j=1 l=1


k
k
X X S(tj , W )S(tl , W )

− EP 
S(t
,
W
)G(t
)
0
1
j=1 l=1


nP
o2
k


k−1
k X
k
S(t
,
W
)
X
X
j
j=u+1
S(tmax{j,l} )
1
1


− EP 
−
=

G(t
)
S(t
,
W
)
G(t
)
G(t
)
u
u+1
u
min{j,l}
u=1
j=1
−

l=1

n


− EP 

o2 
S(t
,
W
)
j
j=1


S(t0 , W )G(t1 )

Pk

≤

k X
k
X
S(tmax{j,l} )

=

k X
k min{j,l}
X
X

j=1 l=1

j=1 l=1

G(tmin{j,l} )

u=1




−

k−1
X

u=1

S(tj )S(tl )



nP

k
j=u+1

o2
S(tj ) 

S(tu )

1
1
−
S(tu ) S(tu−1 )



where the inequality follows from Jensen’s inequality.

80

 n
o2 
Pk

S(t
)
j
j=1
1
1
 

−
−

G(tu+1 ) G(tu )
S(t0 )G(t1 )

1
= σu2 ,
G(tu )

Table 6: Simulation results for ordinal outcome. We consider relative efficiency of fully adjusted and working-model-based estimators for DIM, MW and LOR. In the bootstrap approach, we take B1 = 100 and B2 = 500. Results are based on 1000 replications for analytic
approach; 200 for bootstrap.

D
D.1

DIM (F)
DIM (P)

truth
0.837
0.840

MW (F)
MW (P)

0.842
0.845

LOR (F)
LOR (P)

0.838
0.842

DIM (F)
DIM (P)

truth
0.837
0.840

MW (F)
MW (P)

0.842
0.845

LOR (F)
LOR (P)

0.838
0.842

method
analytic
analytic
bootstrap
analytic
analytic
bootstrap
analytic
analytic
bootstrap
method
analytic
analytic
bootstrap
analytic
analytic
bootstrap
analytic
analytic
bootstrap

n = 200
bias MSE
0.008 0.002
-0.008 0.002
-0.001 0.003
0.011 0.003
-0.004 0.002
0.000 0.003
0.013 0.003
-0.002 0.002
-0.001 0.003
n = 500
bias MSE
0.003 0.001
-0.004 0.001
-0.004 0.002
0.008 0.001
0.001 0.001
-0.004 0.002
0.006 0.001
-0.000 0.001
-0.003 0.002

%RMSE
0.060
0.057
0.068
0.060
0.056
0.069
0.062
0.054
0.066

coverage
0.972
0.950
0.945
0.977
0.951
0.945
0.973
0.961
0.945

CI width
0.194
0.183
0.236
0.200
0.186
0.244
0.197
0.182
0.230

%RMSE
0.037
0.036
0.051
0.038
0.036
0.053
0.038
0.035
0.049

coverage
0.946
0.946
0.965
0.945
0.943
0.945
0.957
0.953
0.955

CI width
0.119
0.115
0.177
0.120
0.117
0.184
0.120
0.114
0.170

Additional Results from the Numerical Experiments
Additional simulation results

In this section, we present additional simulation results for sample sizes n = 200 and n = 500. The
simulation set-up is otherwise the same as described in Section 5. The results for ordinal outcomes are
presented in Table 6, and the results for survival outcomes are presented in Table 7.

D.2

Application to Covid-19 data: ordinal outcomes

In this section, we present additional results when applying our proposed methods to the Covid-19 dataset
with ordinal outcomes. In particular, we estimate the efficiency gain from using the fully adjusted and
working-model-based estimators that adjust for one of the covariates, for estimating three treatment
effect estimands: DIM (Table 8), MW (Table 9) and LOR (Table 10).

81

Table 7: Simulation results for survival outcomes. We only consider the analytical approach
and consider relative efficiency of fully adjusted estimators for RD at time 1, 2 and 3 (the
relative efficiency is the same for RR) and RMST at time 3. Sample size is 200 or 500, and
results are based on 1000 replications.

RD
RD
RD
RMST

(t = 1)
(t = 2)
(t = 3)
(t = 3)

truth
0.903
0.847
0.819
0.820

RD
RD
RD
RMST

(t = 1)
(t = 2)
(t = 3)
(t = 3)

truth
0.903
0.847
0.819
0.820

bias
0.003
0.004
0.007
0.001
bias
0.001
0.002
0.004
-0.001

n = 200
MSE %RMSE
0.002
0.045
0.003
0.061
0.004
0.074
0.003
0.061
n = 500
MSE %RMSE
0.001
0.028
0.001
0.039
0.002
0.048
0.001
0.043

coverage
0.948
0.933
0.908
0.936

mean width
0.165
0.196
0.212
0.202

coverage
0.953
0.949
0.916
0.930

mean width
0.103
0.126
0.140
0.128

Table 8: Relative efficiency (95% CI) of fully adjusted and working-model-based estimators
that adjust for one of the covariates for estimating DIM, in the Covid-19 dataset. “F” stands
for the fully adjusted estimator, and “W” stands for the working-model-based estimator.

age
gender
race
CVD
HTN
diabetes
kidney disease
cholesterol meds
HTN meds
BMI

0.97
0.99
0.99
0.95
1.00
0.99
0.99
1.00
0.99
1.00

(0.94,
(0.97,
(0.98,
(0.92,
(1.00,
(0.98,
(0.96,
(1.00,
(0.98,
(1.00,

82

DIM
F
1.00) 0.97
1.01) 0.99
1.01) 0.99
0.99) 0.95
1.00) 1.00
1.01) 0.99
1.01) 0.99
1.00) 1.00
1.01) 0.99
1.00) 1.00

(0.94,
(0.97,
(0.98,
(0.92,
(1.00,
(0.98,
(0.96,
(1.00,
(0.98,
(1.00,

W
1.00)
1.01)
1.01)
0.99)
1.00)
1.01)
1.01)
1.00)
1.01)
1.00)

Table 9: Relative efficiency (95% CI) of fully adjusted and working-model-based estimators
that adjust for one of the covariates for estimating MW, in the Covid-19 dataset. “F” stands
for the fully adjusted estimator, and “W” stands for the working-model-based estimator.

age
gender
race
CVD
HTN
diabetes
kidney disease
cholesterol meds
HTN meds
BMI

0.98
0.99
0.99
0.95
1.00
0.99
0.99
1.00
0.99
1.00

(0.96,
(0.97,
(0.98,
(0.92,
(0.99,
(0.98,
(0.97,
(0.99,
(0.97,
(1.00,

MW
F
1.01) 0.98
1.01) 0.99
1.01) 0.99
0.99) 0.95
1.00) 1.00
1.01) 0.99
1.01) 0.99
1.00) 1.00
1.01) 0.99
1.00) 1.00

(0.96,
(0.91,
(0.98,
(0.92,
(1.00,
(0.98,
(0.97,
(0.99,
(0.97,
(1.00,

W
1.01)
1.01)
1.01)
0.99)
1.00)
1.01)
1.01)
1.00)
1.01)
1.00)

Table 10: Relative efficiency (95% CI) of fully adjusted and working-model-based estimators
that adjust for one of the covariates for estimating LOR, in the Covid-19 dataset. “F” stands
for the fully adjusted estimator, and “W” stands for the working-model-based estimator.

age
gender
race
CVD
HTN
diabetes
kidney disease
cholesterol meds
HTN meds
BMI

0.97
0.99
1.00
0.96
1.00
0.99
0.99
1.00
0.99
1.00

(0.95,
(0.98,
(0.98,
(0.92,
(1.00,
(0.98,
(0.97,
(1.00,
(0.98,
(1.00,

83

LOR
F
0.99) 0.97
1.02) 0.99
1.01) 1.00
0.99) 0.96
1.00) 1.00
1.01) 0.99
1.01) 0.99
1.01) 1.00
1.01) 0.99
1.00) 1.00

(0.93,
(0.98,
(0.98,
(0.92,
(1.00,
(0.98,
(0.96,
(1.00,
(0.98,
(1.00,

W
1.00)
1.01)
1.01)
0.99)
1.00)
1.01)
1.01)
1.00)
1.01)
1.00)

E

Statistical Inference for the Relative Effciency when Y and
W are Independent under P

E.1

Hypothesis test based on sample splitting

2
We split the external data into two subsamples of size n/2, denoted by D1 and D2 . Let σ̂a,1
be the
2
proposed estimator of the adjusted variance, calculated from observations in D1 only; and let σ̂u,2
be

the proposed estimator for the unadjusted variance using D2 . By the asymptotic linearity of these
estimators, we have that
√
√
d
d
2
2
− σa2 ) −
→ N (0, 2E[IF2a ]),
− σu2 ) −
→ N (0, 2E[IF2u ]).
n(σ̂a,1
n(σ̂u,2
2
2
Moreover, as σ̂a,1
and σ̂u,2
are based on different observations, they are independent. Therefore, delta

method implies that
√
n

2
σ̂a,1
− φa
2
σ̂u,2

!

d

−
→N



2E[IF2a ] + 2φ2a E[IF2u ]
0,
σu4



.

A Wald test can be used to test the hypothesis H0 : φa = 1, and in fact a Wald confidence interval
can also be constructed directly although it might be wider than the one obtained from the proposed
two-step procedure. The same argument applies when we consider the working-model-based variance.
The sample splitting approach was also used in Williamson and Feng [35] to test a null hypothesis
that lies on the boundary of the parameter space.

E.2

Asymptotic coverage of the confidence set

Recall that the confidence set, which we denote as Its , is constructed using a two-step procedure. We
first test the null hypothesis H0 : φ = 1 using a level α test. If it is rejected, we take the confidence set
to be Iwald , the Wald confidence interval; otherwise the confidence set is taken to be Iwald ∪ {1}. We
argue that the asymptotic coverage of this confidence set is at least 1 − α.
First, consider the case where φ 6= 1 under P . This implies that the influence function of φ̂ without
sample splitting is not identically 0. Hence, asymptotic linearity in the form of (2) implies that P (φ ∈

84

Iwald ) → 1 − α. In addition, we have that P (φ ∈ Iwald ) ≤ P (φ ∈ Its ). Next, consider P such that φ = 1.
P (φ ∈ Its ) = P (φ ∈ Its | reject H0 )P ( reject H0 )
+ P (φ ∈ Its | do not reject H0 )P ( do not reject H0 )
= P (1 ∈ Iwald | reject H0 )P ( reject H0 )
+ P (1 ∈ Iwald ∪ {1}| do not reject H0 )P ( do not reject H0 )
≥ P ( do not reject H0 ) ≥ 1 − α,
where the last inequality follows because the test is level α and thus the probability of falsely rejecting
the null is at most α.

F

Pseudocode for double bootstrap scheme

Algorithm 1 Double bootstrap procedure

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

Input: external data X, treatment effect estimators ψ̂u and ψ̂m
Output: Estimate of and confidence interval for the relative efficiency
for k = 1, 2, . . . , B2 do
Resample Xk of size N from X with replacement
Randomly assign treatment to form X̃k
Compute ψ̂u,k = ψ̂u (X̃k ), ψ̂m,k = ψ̂m (X̃k )
end for
Let φ̃(X) = var(ψ̂m,k )/var(ψ̂u,k )
for i = 1, 2, . . . , B1 do
Resample Xi∗ of size n from X with replacement
for j = 1, 2, . . . , B2 do
∗∗
Resample Xij
of size N from Xi∗ with replacement
Randomly assign treatment to form X̃ij
ij
Compute ψ̂uij = ψ̂u (X̃ij ), ψ̂m
= ψ̂m (X̃ij )
end for
ij
φ̃(Xi∗ ) = var(ψ̂m
)/var(ψ̂uij )
end for
return φ̃(X) and φ̃(X) ± z1−α/2 sd(φ̃(Xi∗ ))

85

Acknowledgements
The authors gratefully acknowledge the support of the NIH through award number DP2-LM013340. The
content is solely the responsibility of the authors and does not necessarily represent the official views of
the NIH.

References
[1] Anthony, M. and P. L. Bartlett (2009). Neural network learning: Theoretical foundations. cambridge
university press.
[2] Austin, P. C., A. Manca, M. Zwarenstein, D. N. Juurlink, and M. B. Stanbrook (2010). A substantial
and confusing variation exists in handling of baseline covariates in randomized controlled trials: a
review of trials published in leading medical journals. Journal of clinical epidemiology 63 (2), 142–153.
[3] Benkeser, D., I. Diaz, A. Luedtke, J. Segal, D. Scharfstein, and M. Rosenblum (2020). Improving
precision and power in randomized trials for covid-19 treatments using covariate adjustment, for binary,
ordinal, and time-to-event outcomes. medRxiv .
[4] Bialek, S., E. Boundy, V. Bowen, N. Chow, A. Cohn, N. Dowling, S. Ellington, et al. (2020). Severe
outcomes among patients with coronavirus disease 2019 (COVID-19)—United States, February 12–
March 16, 2020. Morbidity and mortality weekly report 69 (12), 343–346.
[5] Bickel, P. J. (1982). On adaptive estimation. The Annals of Statistics, 647–671.
[6] Bickel, P. J., C. A. Klaassen, P. J. Bickel, Y. Ritov, J. Klaassen, J. A. Wellner, and Y. Ritov (1993).
Efficient and adaptive estimation for semiparametric models, Volume 4. Johns Hopkins University
Press Baltimore.
[7] Colantuoni, E. and M. Rosenblum (2015). Leveraging prognostic baseline variables to gain precision
in randomized trials. Statistics in medicine 34 (18), 2602–2617.
[8] Dı́az, I., E. Colantuoni, D. F. Hanley, and M. Rosenblum (2019). Improved precision in the analysis
of randomized trials with survival outcomes, without assuming proportional hazards. Lifetime data
analysis 25 (3), 439–468.
86

[9] Dı́az, I., E. Colantuoni, and M. Rosenblum (2016). Enhanced precision in the analysis of randomized
trials with ordinal outcomes. Biometrics 72 (2), 422–431.
[10] FDA (2019). Adjusting for covariates in randomized clinical trials for drugs and biologics with
continuous outcomes. draft guidance for industry. https://www.fda.gov/media/123801/download.
[11] Friedman, J., T. Hastie, and R. Tibshirani (2010). Regularization paths for generalized linear models
via coordinate descent. Journal of Statistical Software 33 (1), 1–22.
[12] Gill, R. D., M. J. Van Der Laan, and J. M. Robins (1997). Coarsening at random: Characterizations,
conjectures, counter-examples. In Proceedings of the First Seattle Symposium in Biostatistics, pp. 255–
294. Springer.
[13] Heitjan, D. F. and D. B. Rubin (1991). Ignorability and coarse data. The annals of statistics,
2244–2253.
[14] Hirose, Y. et al. (2016). On differentiability of implicitly defined function in semi-parametric profile
likelihood estimation. Bernoulli 22 (1), 589–614.
[15] Ibragimov, I. and R. Has’minskii (1981). Statistical estimation: asymptotic theory.
[16] Kahan, B. C., V. Jairath, C. J. Doré, and T. P. Morris (2014). The risks and rewards of covariate
adjustment in randomized trials: an assessment of 12 outcomes from 8 studies. Trials 15 (1), 139.
[17] Kaplan, E. L. and P. Meier (1958). Nonparametric estimation from incomplete observations. Journal
of the American statistical association 53 (282), 457–481.
[18] Mao, L. (2018). On causal estimation using-statistics. Biometrika 105 (1), 215–220.
[19] McCullagh, P. (1980). Regression models for ordinal data. Journal of the Royal Statistical Society:
Series B (Methodological) 42 (2), 109–127.
[20] Moore, K. and M. J. van der Laan (2009a). Application of time-to-event methods in the assessment
of safety in clinical trials. Design and Analysis of Clinical Trials with Time-to-Event Endpoints. Taylor
& Francis, 455–482.

87

[21] Moore, K. L., R. Neugebauer, T. Valappil, and M. J. van der Laan (2011). Robust extraction of covariate information to improve estimation efficiency in randomized trials. Statistics in medicine 30 (19),
2389–2408.
[22] Moore, K. L. and M. J. van der Laan (2009b). Increasing power in randomized trials with right
censored outcomes through covariate adjustment. Journal of biopharmaceutical statistics 19 (6), 1099–
1131.
[23] Pfanzagl, J. (1990). Estimation in semiparametric models. In Estimation in Semiparametric Models,
pp. 17–22. Springer.
[24] Pfanzagl, J. and W. Wefelmeyer (1985). Contributions to a general asymptotic statistical theory.
Statistics & Risk Modeling 3 (3-4), 379–388.
[25] Reid, N. (1981). Influence functions for censored data. The Annals of Statistics, 78–92.
[26] Steingrimsson, J. A., D. F. Hanley, and M. Rosenblum (2017). Improving precision by adjusting
for prognostic baseline variables in randomized trials with binary outcomes, without regression model
assumptions. Contemporary clinical trials 54, 18–24.
[27] Stitelman, O. M., V. De Gruttola, and M. J. van der Laan (2012). A general implementation of
tmle for longitudinal data applied to causal inference in survival analysis. The international journal
of biostatistics 8 (1).
[28] Tsiatis, A. (2007). Semiparametric theory and missing data. Springer Science & Business Media.
[29] Van der Laan, M. J., M. Laan, and J. M. Robins (2003). Unified methods for censored longitudinal
data and causality. Springer Science & Business Media.
[30] Van Der Laan, M. J. and D. Rubin (2006). Targeted maximum likelihood learning. The international
journal of biostatistics 2 (1).
[31] Van der Vaart, A. W. (2000). Asymptotic statistics, Volume 3. Cambridge university press.
[32] Van Der Vaart, A. W. and J. A. Wellner (1996). Weak convergence. In Weak convergence and
empirical processes, pp. 16–28. Springer.

88

[33] Vermeulen, K., O. Thas, and S. Vansteelandt (2015). Increasing the power of the mann-whitney
test in randomized experiments through flexible covariate adjustment. Statistics in medicine 34 (6),
1012–1030.
[34] Wang, B., E. L. Ogburn, and M. Rosenblum (2019). Analysis of covariance in randomized trials:
More precision and valid confidence intervals, without model assumptions. Biometrics 75 (4), 1391–
1400.
[35] Williamson, B. D. and J. Feng (2020). Efficient nonparametric statistical inference on population
feature importance using shapley values. arXiv preprint arXiv:2006.09481 .

89

