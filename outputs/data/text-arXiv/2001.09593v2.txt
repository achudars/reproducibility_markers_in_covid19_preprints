arXiv:2001.09593v2 [stat.ME] 8 Aug 2020

Shapley value confidence intervals for
attributing variance explained
Daniel Vidali Fryer∗,
School of Mathematics and Physics,
The University of Queensland, St Lucia, Australia
Inga Strümke†,
Simula Research Laboratory,
Pilestredet 52, Oslo, Norway
Hien Nguyen‡,
Department of Mathematics and Statistics,
La Trobe University, Melbourne, Australia

Abstract
The coefficient of determination, the R2 , is often used to measure the variance explained by an affine combination of multiple explanatory covariates. An attribution
of this explanatory contribution to each of the individual covariates is often sought
in order to draw inference regarding the importance of each covariate with respect to
the response phenomenon. A recent method for ascertaining such an attribution is via
the game theoretic Shapley value decomposition of the coefficient of determination.
Such a decomposition has the desirable efficiency, monotonicity, and equal treatment
properties. Under a weak assumption that the joint distribution is pseudo-elliptical, we
obtain the asymptotic normality of the Shapley values. We then utilize this result in
order to construct confidence intervals and hypothesis tests regarding such quantities.
Monte Carlo studies regarding our results are provided. We found that our asymptotic
confidence intervals are computationally superior to competing bootstrap methods and
are able to improve upon the performance of such intervals. In an expository application to Australian real estate price modelling, we employ Shapley value confidence
intervals to identify significant differences between the explanatory contributions of
covariates, between models, which otherwise share approximately the same R2 value.
These different models are based on real estate data from the same periods in 2019
and 2020, the latter covering the early stages of the arrival of the novel coronavirus,
COVID-19.
∗

daniel.fryer@uq.edu.au (Corresponding Author)

† inga@simula.no
‡ h.nguyen5@latrobe.edu.au

1

1

Introduction

The multiple linear regression model (MLR) is among the most commonly applied tools
for statistics inference; see [Seber and Lee, 2003] and [Greene, 2007, Part I] for thorough
introductions to MLR models. In the usual MLR setting, one observes an independent
and identically distributed (IID) sample of data pairs Zi> = Yi , Xi> ∈ Rd+1 , where
i ∈ [n] = {1, . . . , n}, and d, n ∈ N. The MLR model is then defined via the linear relationship
E (Yi |Xi = xi ) = β0 +

d
X

βj Xij ,

i=1

where β > = (β0 , . . . , βn ) ∈ Rd+1 , and Xi> = (Xi1 , . . . , Xid ). We shall also write Zi> =
>
(Zi0 , Zi1 , . . . , Zid ), when it is convenient to do so. Here, (·) is the transposition operator.
The usual nomenclature is to call the Yi and Xi elements of each pair, the response
(or dependent) variable and the explanatory (or covariate) vector, respectively. Here, the
jth element of Xi : Xij (j ∈ [d]), is referred to as the jth explanatory variable (or the jth
n
covariate). We may put the pairs of data into a dataset Zn = {Zi }i=1 .
Let Rjk (Zn ) denote the sample correlation coefficient


Pn
Zik − Z̄k
i=1 Zij − Z̄j
q
(1)
Rjk (Zn ) = qP
2 Pn
2 ,
n
i=1 Zij − Z̄j
i=1 Zik − Z̄k
Pn
for each j, k ∈ {0} ∪ [d]. Here Z̄j = n−1 i=1 Zij
 is the sample mean of variable j. Write
U ⊆ {0} ∪ [d] be a nonempty subset, where U = u1 , . . . , u|U | , where |U| is the cardinality
of U. We refer to the matrix of correlations between the variables in U as


1
Ru1 u2 (Zn ) · · · Ru1 u|U | (Zn )


1
· · · Ru2 u|U | (Zn ) 
 Ru2 u1 (Zn )

.
(2)
Cn (U) = 

..
..
..
..


.
.
.
.
Ru|U | u1 (Zn ) Ru|U | u2 (Zn ) · · ·
1
A common inferential task is to determine the degree to which the response can be explained
by the covariate vector, in totality. The usual device for addressing this question is via the
coefficient of determination (or squared coefficient of multiple correlation), which is defined
as
R2 (Zn ) = 1 −

|Cn ({0} ∪ [d])|
,
|Cn ([d])|

(3)

where |C| is the matrix determinant of C (cf. [Cowden, 1952]). Intuitively, the coefficient
of determination measures the proportion of the total variation in the response variable
that is explained by variation in the covariate vector. See [Seber and Lee, 2003, Sec. 4.4]
and [Greene, 2007, Sec. 3.5] for details regarding the derivation and interpretation of the
R2 (Zn ) coefficients.
A refinement to the question that is addressed by the R2 (Zn ) coefficient, is that of
eliciting the contribution of each of the covariates to the total value of R2 (Zn ).
2

In the past, this question has partially been resolved via the use of partial correlation
coefficients (see, e.g. [Greene, 2007, Sec. 3.4]). Unfortunately, such coefficients are only
able to measure the contribution of each covariate coefficient of determination, conditional
to the presence of other covariates that are already in the MLR model.
A satisfactory resolution to the question above, is provided by [Lipovetsky and Conklin, 2001],
[Israeli, 2007], and [Huettner and Sunder, 2012], who each suggested and argued for the use
of the Shapley decomposition of [Shapley, 1953]. The Shapley decomposition is a gametheoretic method for decomposing the contribution to the value of a utility function in the
context of cooperative games.
Let π > = (π1 , . . . , πd ) be a permutation of the set [d]. For each j ∈ [d], let
Sj (π) = {k : πk < πj , k ∈ [d]}
be the elements of [d] that appear before πj when [d] is permuted by π. We may define
2
RS2 j (π) (Zn ) and R{j}∪S
(Zn ) in a similar manner to (3), using the generic definition
j (π)
RS2 (Zn ) = 1 −

|Cn ({0} ∪ S)|
,
|Cn (S)|

(4)

for nonempty subsets S ⊆ [d], and R{2 } (Zn ) = 0 for the empty set.
Treating the coefficient of determination as a utility function, we may conduct a Shapley
partition of the R2 (Zn ) coefficient by computing the jth Shapley value, for each of the d
covariates, defined by
i
Xh
−1
2
Vj (Zn ) = |P|
R{j}∪S
(Zn ) − RS2 j (π) (Zn ) ,
(5)
j (π)
π∈P

where P is the set of all possible permutations of [d].
Compared to other decompositions of the coefficient of determination, such as those
considered in [Gromping, 2006] and [Gromping, 2007], the Shapley values, obtained from
the partitioning above, have the favorable axiomatic properties that were well exposed in
[Huettner and Sunder, 2012]. Specifically, the Shapley values have the efficiency, monotonicity, and, equal treatment properties, and the decomposition is provably the only method
that satisfies all three of these properties (cf. [Young, 1985, Thm. 2]). Here, in the context
of the coefficient of determination, efficiency, monotonicity, and equal treatment are defined
as follows:
Efficiency: The sum of the Shapley values across all covariates equates to the coefficient
of determination, that is
d
X
Vj (Zn ) = R2 (Zn ) .
j=1

Monotonicity: For pairs of samples Zm and Zn , of sizes m, n ∈ N,
2
2
R{j}∪S
(Zn ) − RS2 j (π) (Zn ) ≥ R{j}∪S
(Zm ) − RS2 j (π) (Zm ) ,
j (π)
j (π)

for every π ∈ P, implies that Vj (Zn ) ≥ Vj (Zm ), for each j ∈ [d].
3

Equal treatment: If covariates j, k ∈ [d] are substitutes in the sense that
2
2
(Zn ) ,
(Zn ) = R{k}∪S
R{j}∪S
j (π)
k (π)

for each π ∈ P such that k ∈
/ Sj (π) and j ∈
/ Sk (π), then the Shapley decomposition
is such that Vj (Zn ) = Vk (Zn ).
We note that equal treatment is also often referred to as symmetry in the literature.
The uniqueness of the Shapley decomposition in exhibiting the three described properties is
often used as the justification for its application. Furthermore, there are numerous sets of
axiomatic properties that lead to the Shapley value decomposition as a solution (see, e.g.,
[Feltkamp, 1995]). In the statistics literature, it is known that the axioms for decomposition
of the coefficient of determination that are proposed by [Kruskal, 1987] correspond exactly
to the Shapley values (cf. [Genizi, 1993]).
When conducting statistical estimation and computation, the assumption of randomness
of data necessitates that we address not only the problem of point estimation, but also
variability quantification. In [Huettner and Sunder, 2012], variability for the coefficient of
determination Shapley values were quantified via the use of bootstrap confidence intervals
(CIs). Combined with the usual computational intensiveness of bootstrap resampling (see,
e.g., [Efron, 1988] and [Baglivo, 2005, Ch. 12]), the combinatory nature of the computation
of (5) (notice that |P| = d!) compounds the time complexity of such a method, which
is already of order O(2d ). In this article, we seek to provide an asymptotic method for
computing CIs for the Shapley values.
Our approach uses the joint asymptotic normality result of the elements in a correlation matrix, under an elliptical assumption, via [Steiger and Browne, 1984], combined
with asymptotic normality results concerning the determinants of a correlation matrix, of
[Hedges and Olkin, 1983b] and [Olkin and Finn, 1995]. Using these results, we derive the
asymptotic joint distribution for the R2 (Zn ) Shapley values, which allows us to construct
CIs for each of the values and their contrasts. We assess the finite sample properties of our
constructions via a comprehensive Monte Carlo study and demonstrate the use of our CIs
via applications to real estate price data.
The remainder of the article proceeds as follows. In Section 2, we present our main
results regarding the asymptotic distribution of the coefficient of determination Shapley
values, and their CI constructions. In Section 3, we present a comprehensive Monte Carlo
study of our CI construction method. In Section 4, we demonstrate how our results can be
applied to real estate price data. Conclusions are lastly drawn in Section 5.

2
2.1

Main results
The correlation matrix

Let Z ∈ Rd+1 be a random variable with mean vector µ ∈ Rd+1 and covariance matrix
Σ ∈ R(d+1)×(d+1) . Then, we can define the coefficient of multivariate kurtosis [Mardia, 1970]
by
κ=

h
i2
1
>
E (Z − µ) Σ−1 (Z − µ) .
(d + 1) (d + 3)
4

Let ρjk = cor (Zj , Zk ), for j, k ∈ {0} ∪ [d] such that j 6= k. Assume that Z arises from an
elliptical distribution (cf. [Fang et al., 1990, Ch. 2]) and let Zn be an IID sample with the
same distribution as Z. Then, we may estimate ρjk using the sample correlation coefficient
(1). Upon writing acov to denote the asymptotic covariance, we have the following result
due to Corollary 1 of [Steiger and Browne, 1984].
Lemma 1. If Z arises from an elliptical distribution and has coefficient of multivariate
kurtosis κ, then the normalized coefficients of correlation ζjk = n1/2 (Rjk − ρjk ) (j, k ∈ {0}∪
[d]; j 6= k) converge to a jointly normal distribution with asymptotic mean and covariance
elements 0 and



acov(ζgh , ζjk ) = κ ρgh ρjk ρ2gj + ρ2hj + ρ2gk + ρ2hk /2 + ρgj ρhk + ρgk ρhj
−κ [ρgh (ρhj ρhk + ρgj ρgk ) + ρjk (ρgj ρhj + ρgk ρhk )] .

(6)

Remark 1. We note that the elliptical distribution assumption above can be replaced by a
broader pseudo-elliptical assumption, as per [Yuan and Bentler, 1999] and [Yuan and Bentler, 2000].
This is a wide class of distributions that includes some that may not be symmetric. Due
to the complicated construction of the class, we refer the interested reader to the source
material for its definition.
Remark 2. We may state a similar result that replaces the elliptical assumption by a fourth
moments existence assumption instead, using Proposition 2 of [Steiger and Browne, 1984].
In order to make practical use of such an assumption, we require the estimation of (d + 1)!/[(d − 3)!4!]
fourth order moments instead of a single kurtosis term κ. Such a result may be useful when
the number of fourth order moments is small, but become infeasible rapidly, as d increases.

Let V ⊆ {0} ∪ [d], where V = v1 , . . . , v|V| . Define Cn (V) in the same manner as (2),
and let


1
ρu1 u2 · · · ρu1 u|U |
 ρu2 u1
1
· · · ρu2 u|U | 


R (U) = 

..
..
..
..


.
.
.
.
ρu|U | u1 ρu|U | u2 · · ·
1
and


1

 ρ v2 v1

R (V) = 
..

.
ρv|V| v1

ρ v1 v2
1
..
.
ρv|V| v2

···
···
..
.
···

ρv1 v|V|
ρv2 v|V|
..
.
1




.


The following theorem is adapted from a result of [Hedges and Olkin, 1983b] (also appearing as Theorem 1 in [Hedges and Olkin, 1983a]). Our result expands upon the original
theorem, to allow for inference regarding elliptically distributed data, and not just normally distributed data. We further fix some typographical matters that appear in both
[Hedges and Olkin, 1983a] and [Hedges and Olkin, 1983b].
Lemma 2. Assume the same conditions as in (1). Then, the normalized covariance determinant δ(U) = n1/2 (|Cn (U)| − |R (U)|) (where U and V are nonempty subsets of {0} ∪ [d])
5

converges to a jointly normal distribution, with asymptotic mean and covariance elements 0
and
X X
acov(δ(U) , δ(V)) =
rU (g, h) rV (j, k) acov(ζgh , ζjk ) ,
(7)
g,h∈U j,k∈V

where acov(ζgh , ζjk ) is as

rU ( u1 , u1 )
 rU ( u2 , u1 )


..

.

rU u|U | , u1

per (6),
rU ( u1 , u2 ) · · ·
rU ( u2 , u2 ) · · ·
..
..
.
.

rU u|U | , u2
···


rU u1 , u|U | 
rU u2 , u|U | 

 = |R (U)| R−1 (U) ,
..

.

rU u|U | , u|U |

and rV (j, k) (j, k ∈ V) is defined similarly.
Proof. The result is due to an application of the delta method (see, e.g., [van der Vaart, 1998,
Thm. 3.1]) and the fact that for any matrix R, the derivative of its determinant is
∂ |R| /∂R = |R| R−> [Seber, 2008, Sec. 17.45]. Notice that we use the unconstrained
case of the determinant derivative, since we sum over each pair of coordinates, where g 6= h
or j 6= k, twice.


Remark 3. If R is symmetric, then ∂ |R| /∂R = |R| 2R−1 − diag R−1 [Seber, 2008,
Sec. 17.45]. Using this fact, we may write (7) in the alternative, and more computationally
efficient form
X X
rU∗ (g, h) rV∗ (j, k) acov(ζgh , ζjk ) ,
acov(δ(U) , δ(V)) =
g≤h j≤k
g,h∈U j,k∈V

where


rU∗ ( u1 , u1 )
 rU∗ ( u2 , u1 )


..

.

rU∗ u|U | , u1

rU∗ ( u1 , u2 ) · · ·
rU∗ ( u2 , u2 ) · · ·
..
..
.
.

···
rU∗ u|U | , u2


rU∗ u1 , u|U | 
rU∗ u2 , u|U | 

 =
..

.

rU∗ u|U | , u|U |
−

2 |R (U)| R−1 (U)


|R (U)| diag R−1 (U) ,

and rV∗ (j, k) (j, k ∈ V) is defined similarly.

2.2

The coefficient of determination

Let plim denote convergence in probability, so that for any sequence {Xn }, and any random
variable X, the statement
lim Pr(|Xn − X| > ε) = 0 ,
n→∞

for every ε > 0, can be written as plimn→∞ Xn = X.
Now, recall definition (4), and further let ρ2S = plimn→∞ RS2 (Zn ). We adapt from
and expand upon [Hedges and Olkin, 1983a, Thm. 2] in the following result. This result also fixes typographical errors that appear in the original theorem, as well as in
[Hedges and Olkin, 1983b].
6

Lemma 3. Assume the same conditions as in Lemma 1. Then, the normalize coefficient
of determination λ (S) = n1/2 RS2 (Zn ) − ρ2S (where S and T are nonempty subsets of [d])
converges to a jointly normal distribution, with asymptotic mean and covariance elements 0
and
acov(λ(S) , λ(T ))

1
acov(δ({0} ∪ S) , δ({0} ∪ T ))
|R (S)| |R (T )|
|R ({0} ∪ S)| |R ({0} ∪ T )|
acov(δ(S) , δ(T ))
+
2
2
|R (S)| |R (T )|
|R ({0} ∪ S)|
−
acov(δ(S) , δ({0} ∪ T ))
2
|R (S)| |R (T )|
|R ({0} ∪ T )|
−
2 acov(δ({0} ∪ S) , δ(T )) .
|R (S)| |R (T )|

=

(8)

Proof. We apply the delta method again, using the functional form (4), and using the fact
that

 

∂
x
1 x
1−
= − , 2 .
∂ (x, y)
y
y y

Remark 4. When S = T , (8) yields the usual form for the asymptotic variance of RS2 (Zn ):
2
4κρ2S 1 − ρ2S (cf. [Yuan and Bentler, 2000]).

2.3

The Shapley values

For every j ∈ [d] and S ⊆ Sj = [d] − {j}, there are |S|! (d − |S| − 1)! elements of π ∈ P
such that Sj (π) = S. Thus, we may write
h
i
X
2
Vj (Zn ) =
ω (S) R{j}∪S
(Zn ) − RS2 (Zn ) ,
(9)
S⊆Sj

where ω (S) = |S|! (d − |S| − 1)!/d!, and define vj = plimn→∞ Vj (Zn ). Using this functional
form (9), we may apply the delta method once more, in order to derive the following joint
asymptotic normal distribution result regarding the Shapley values Vj (Zn ), for j ∈ [d].
Remark 5. The form (9) is a useful computational trick that reduces the computational
time of form (5) and results in more efficient computations for fixed d. It is unclear whether
other formulations such as that of [Hart et al., 1988] can make the computation time even
faster. Unfortunately, however, there is no formulation that reduces the O(2d ) scaling, as d
increases.
Theorem 1. Assume the same conditions as in Lemma 1. Then, the normalized Shaply
values ξj = n1/2 (Vj (Zn ) − vj ) (where j, k ∈ [d]) converge to a jointly normal distribution,
with asymptotic mean and covariance elements 0 and acov(ξj , ξk ) = ajk + bjk − cjk − djk .
Here,
X X
ajk =
ω (S) ω (T ) acov(λ ({j} ∪ S) , λ ({k} ∪ T )) ,
S⊆Sj T ⊆Sk

7

bjk =

X X

ω (S) ω (T ) acov(λ (S) , λ (T )) ,

S⊆Sj T ⊆Sk

cjk =

X X

ω (S) ω (T ) acov(λ (S) , λ ({k} ∪ T )) ,

S⊆Sj T ⊆Sk

and
djk =

X X

ω (S) ω (T ) acov(λ ({j} ∪ S) , λ (T )) ,

S⊆Sj T ⊆Sk

where λ (S) is as defined in Lemma 3, for nonempty subsets S ⊆ [d].
Using the result above, we may apply the delta method again in order to construct
asymptotic CIs or hypothesis tests regarding any continuous function of the d Shapley
values for the coefficient of determination. Of particular interest is the asymptotic CI for
each of the individual Shapley values and the hypothesis test for the difference between two
Shapley values.
The asymptotic 100 (1 − α) % CI for the jth expected Shapley value vj has the usual
form
!
r
r

 avar (ξ )

 avar (ξ )
α
α
j
j
Vj (Zn ) − Φ−1 1 −
, Vj (Zn ) + Φ−1 1 −
,
2
n
2
n
where avar (ξj ) = acov(ξj , ξj ) denotes the asymptotic variance of ξj and Φ−1 is the inverse
cumulative distribution function of the standard normal distribution. The z-statistic for the
test of the null and alternative hypotheses
H0 : vj = vk and H1 : vj 6= vk ,

(10)

for j, k ∈ [d] such that j 6= k, is
√

n (Vj (Zn ) − Vk (Zn ))
,
avar (ξj ) + avar (ξk ) − 2acov(ξj , ξk )
where ∆n has an asymptotic standard normal distribution.
Remark 6. In practice, we do not know the necessary elements κ and ρjk (j, k ∈ {0} ∪ [d] ; j 6= k)
that are required in order to specify the asymptotic covariance terms in Lemma 1–Lemma 3
and Theorem 1. However, by Slutsky’s theorem, we have the usual result that any acov
(or avar) term can be replaced by the estimator acov
d n (or avar
d n ), which replaces κ by the
estimator of [Mardia, 1970]
>
i2
Pn h
Zi − Z̄ Σ̂−1
n ({0} ∪ [d]) Zi − Z̄
i=1
,
κ̂ (Zn ) =
n (d + 1) (d + 3)

and replaces ρjk by Rjk (Zn ), where Z̄ > = Z̄0 , . . . , Z̄d . Here,


Σ̂u1 u1 (Zn )
Σ̂u1 u2 (Zn ) · · · Σ̂u1 u|U | (Zn )


Σ̂u2 u2 (Zn ) · · · Σ̂u2 u|U | (Zn ) 
 Σ̂u2 u1 (Zn )

,
Σ̂n (U) = 
..
..
..
..

.


.
.
.
∆n = p

Σ̂u|U | u1 (Zn ) Σ̂u|U | u2 (Zn ) · · ·
8

Σ̂u|U | u|U | (Zn )

where Σ̂jk (Zn ) is the sample covariance between the jth and kth dimension of Z (j, k ∈
{0} ∪ [d]). For example, the estimated test statistic
√
n (Vj (Zn ) − Vk (Zn ))
ˆn = p
,
(11)
∆
avar
d n (ξj ) + avar
d n (ξk ) − 2acov
d n (ξj , ξk )
for the hypotheses (10), retains the property of having an asymptotically standard normal
distribution.

3

Monte Carlo studies and benchmarks

In each of the following three Monte Carlo studies, we simulate a large number N of random
(i)
samples Zn , i ∈ [N ], of size n, from a chosen distribution D. For each sample, we apply
Lemma 1 to calculate an asymptotic 95% CI for the first Shapley value v1 , producing a set
of N observed intervals IN = {[`i , ui ] : i ∈ [N ]}, as realisations of the CI [L, U ] for v1 . The
coverage probability covrv1 ([L, U ]) := Pr(v1 ∈ [L, U ]) is then estimated as the proportion of
intervals in IN containing the population Shapley value
covr
d v1 ([L, U ]) =

|{[`i , ui ] ∈ IN : v1 ∈ [`i , ui ]}|
.
N

(12)

Here, the population Shapley value v1 has the form:
h
i
X
2
v1 =
ω (S) R{1}∪S
− RS2 ,
S⊆S1

where RS2 is defined by replacing Cn in (4) by the known population correlation matrix
cor(Z), as determined by the chosen distribution D. In Studies A and B, this population
correlation matrix is the (d + 1) × (d + 1) matrix Σ, with diagonal elements equal to 1 and
off-diagonal elements equal to a constant correlation c ∈ [0, 1). That is,
Σ = c Jd+1 + (1 − c) Id+1 ,

(13)

where Jd+1 denotes a (d + 1) × (d + 1) matrix with all entries equal to 1. Note that, for
fixed variance, larger magnitudes of c map to larger regression coefficients. Thus, these
simulations can be viewed as assigning equal regression coefficients to each covariates that
are increasing for increasing values of c.
In Study C, we are concerned with covariance matrices with off-diagonal elements deviating from c. We aim to capture a case where the off-diagonal elements of cor(Z) are not
(i)
uniform, and where some may be negative. This is achieved by sampling Zn from a multivariate normal distribution Di , with a symmetric positive definite covariance matrix Σi
that is sampled at random from a Wishart distribution with scale matrix Σ; see Section 3.3.
(i)
Accordingly, the population Shapley value v1 is unique to sample i, and thus we adjust the
(i)
coverage estimator covr
d v1 ([L, U ]) by replacing v1 on the RHS of 12 by v1 .
To accompany our estimates of covr
d v1 ([L, U ]), we also provide Clopper-Pearson CIs
[Clopper and Pearson, 1934] for the coverage probability. We also report the average CI
9

widths, and middle 95% percentile intervals for the widths. For comparison, we estimate
coverage probabilities of non-parametric bootstrap confidence intervals in each of the three
studies. To obtain the bootstrap CIs, we set some large number Nb and take random
(r)
(i)
resamples Rn , r ∈ [Nb ], of size n, with replacement,nfrom
 Zn . From these
o resamples, we
(r)

calculate the set of estimated Shapley values Li = V1 Rn
: r ∈ [Nb ] . The ith 95%
bootstrap CI is then taken as the middle 95% percentile interval of L, and the coverage is
estimated as in 12.
To obtain results for each pair (n, c) ∈ N ×C, where N = {5, 10, . . . , 50}∪{100, 200, . . . , 2000}
and C = {0, 0.1, 0.2, 0.3, 0.6, 0.9, 0.99}, we performed 30 × 7 = 210 simulations for each of
the three studies. We use N = Nb = 1000 and d = 3, in all cases.

3.1

MC Study A
(i)

Here, we choose D = Nd+1 (0, Σ), so that each sample Zn , for i ∈ [N ], is drawn from a
multivariate normal distribution, with covariance Σ given in (13).
The simulation results in Figure 1 (and in Figure 12, Appendix A) show very similar
coverage and width performance between the two assessed CIs for moderate and high correlations c > 0.3. For lower correlations c ≤ 0.2, coverage convergence appears to be slower
in n than the bootstrap CI for large sample sizes (n ≥ 100). The opposite trend seems to
hold for small sample sizes (n ≤ 50), see the discussion under MC Study C. Also, for the
highest correlation c = 0.99, coverage performance of the asymptotic CI is overall slightly
better than the bootstrap CI.

10

c = 0.1
0.95
0.90
0.85

●
●

●

●

●

●

●

●

●

c = 0.1
●

●
●

●

●
●

●

●

0.20
0.15
0.10 ●
0.05 ●
0.00
100

●

●

●

100

500

1000

1500

2000

● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ●

500

c = 0.2
0.95
0.90

●

●
●

●

●

●

100

●

●

500

●

●

●

●

●
●

1000

●

●

●

●

●

●

1500

2000

●

● ●
● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ●

500

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

100

500

1000

1500

2000

c = 0.6
0.96
0.94
0.92

●

●
●

●

100

●

●

●

●

●

●

500

●
●

●

●

●

1000

●
●

●

1500

●

●

2000

0.20
0.15
0.10
0.05

0.96
0.94
0.92

●
●

●

●

●

●

●

●

●

●

●
●

500

●

● ●
● ● ●
● ● ● ● ● ● ●
● ● ● ● ● ●

100

500

●

●

100

●
●

●

1000

●

500

●

●

●
●

●

1000

1000

1500

bootstrap

2000
●

c = 0.6
0.20
0.15
0.10
0.05

no
yes

●
●

100

●

● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ● ●

500

●

●

●

●
●

1500

●

2000

0.08
0.06
0.04
0.02

1000

1500

2000

●
●

100

● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ● ● ●

500

c = 0.99
0.96
0.94
0.92

2000

c = 0.9

●

100

1500

●

c = 0.9
●

1000
c = 0.3

●

●

2000

●

100

mean width

coverage

●
●

●

1500

c = 0.2
0.20
0.15
0.10
0.05

c = 0.3
0.98
0.95
0.92
0.90
0.88

1000

●

1000

1500

2000

c = 0.99
●

●

●

●

1500

●

●

●
●

2000

n

0.01
0.00
0.00

●
●

100

● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ● ● ●

500

1000

1500

2000

n

Figure 1: Comparisons of coverage (left column) and mean width (right column), between
the bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study A. Rows represent correlations c, increasing from 0.1
on the top row to 0.99 on the bottom row. The horizontal axes display the sample sizes
n = 100, 200, . . . , 2000.

3.2

MC Study B

Here, we choose D = tν (0, Σ) where tν (µ, Σ) is the multivariate Student t distribution
with ν ∈ (0, ∞) degrees of freedom, mean vector µ, and scale matrix Σ. Specifically, the ith
(i)
sample Zn , for i ∈ [N ], we set ν = 100 degrees of freedom, and set Σ as the (d + 1) × (d + 1)
11

covariance matrix in 13.
For all sample sizes n and correlations c, coverage and width performances are similar to
MC Study A (see Figure 13 and Figure 14 in Appendix A). Of particular interest, in both
MC Studies A and B (but not in MC Study C), we observe that for c = 0, the estimated
coverage probability of the asymptotic CI is almost equal to 1, for all sample sizes greater
than 10, while the corresponding bootstrap CIs have estimated coverage equal to 0 (Figure 2
left). Despite this, the average CI widths, though large under small samples, are somewhat
smaller than those for bootstrap (Figure 2 right).
c=0
1.00
●

●

●

●

●

●

●

0.75
●

mean width

coverage

0.75

●

c=0
●

0.50

●

0.50

bootstrap
●

●

●

0.25

0.25

●
●

0.00

no
yes

●

●

●

●

●

0.00
5

10

20

30

40

50

5 10

n

20

30

40

50

n

Figure 2: Comparisons of coverage (left column) and mean width (right column) between
bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study B, for correlation c = 0. The horizontal axes display
the sample sizes n = 5, 10, . . . , 50.

3.3

MC Study C
(i)

Here, we set Di = Nd+1 (0, Σi ), so that the sample Zn , for i ∈ N , is drawn from a multivariate normal distribution, with covariance matrix Σi realised from a Wishart distribution
Wd+1 (Σ, ν) with scale matrix Σ and ν degrees of freedom. This set up is different from
Studies A and B in that the distributions Di , and therefore the population Shapley values
(i)
v1 , are allowed to differ between samples.
The distribution Wd+1 (Σ, ν) can be understood as the distribution of the sample covariance matrix of a sample of size ν+1 from the distribution Nd+1 (0, Σ) (cf [Fujikoshi et al., 2011]).
This implies that each covariance matrix Σi can have non-uniform and negative off-diagonal
elements, with variability between the off-diagonal elements increasing as ν decreases. For
this study, we set ν = 100.
Aside for the case c = 0, coverage and width statistics are again similar to MC Studies A
and B, for all n and c (see Appendix A). Interestingly, in all three Studies, for small sample
sizes (n ≤ 50), coverage is often higher than for the bootstrap CI, with slightly smaller

12

average widths, as seen in Figure 3 (and in Figure 12 and Figure 13 in Appendix A). For
the c = 0 case, the observed behaviour differs from MC Studies A and B, with bootstrap
performing comparatively well for large sample sizes (Figure 4).
c = 0.1
1.00
0.75
0.50
0.25

●

●

●

●

●

c = 0.1
●

●

●

●

0.75
0.50
0.25
0.00

●

5

10

20

30

40

50

●

●

●

5 10

●

20

c = 0.2
1.00
0.80
0.60
0.40

●

●

●

●

●

10

20

●

●

●

0.75
0.50
0.25
0.00

30

40

50

●

●

●

5 10

●

●

●

●

●

●

●

●

5

10

20

30

40

50

c = 0.6
0.90
0.80

●

●

●

●

●

●

●

●

●

●

0.70
5

10

20

30

40

0.75
0.50
0.25
0.00

●

●

●

●

0.90
0.80

●

●

●

●

20

●

●

●

30

●

●

●

●

●

40

10

20

30

●

●

●

40

●

50

●

●

●

●

●

●

30

40

50
●

0.80
0.60
0.40
0.20

●

●

●

5 10

0.60
0.40
0.20

50

●

●

20

●

no
yes

●

30

●

●

40

●

50

●

●

●

5 10

●

●

20

●

●

30

●

●

40

●

50

c = 0.99
●

●

●

●

●

5

●

30

20

c = 0.99
0.90
0.80
0.70

●

c = 0.9
●

0.70
10

●

50

c = 0.6

50

●

5

●

bootstrap
5 10

c = 0.9
●

●

40

c = 0.3
●

mean width

coverage

0.80
0.60
0.40

●

●

30

20

c = 0.3
●

●

c = 0.2
●

●

5

●

40

50

n

0.20
0.15
0.10
0.05
0.00

●

●

5 10

●

●

20

●

●

30

●

●

40

●

●

50

n

Figure 3: Comparisons of coverage (left column) and mean width (right column), between
the bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study C. Rows represent correlations c, increasing from 0.1
on the top row to 0.99 on the bottom row. The horizontal axes display the sample sizes
n = 5, 10, . . . , 50.

13

c=0
1.00

●

●

●

●

c=0
●

●

●

●

●

0.75
●

mean width

coverage

0.75

0.50

●

bootstrap

0.50

●

●

no
yes

●
●

0.25

●
●

0.25

●

●

●

●

0.00
5

10

20

30

40

50

5 10

20

n

30

40

50

n

Figure 4: Comparisons of coverage (left column) and mean width (right column) between
bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study C, for correlation c = 0. The horizontal axes display
the sample sizes n = 5, 10, . . . , 50.

c=0

c=0
0.20

●

0.94
●
●

●

●

●

●

●

●

●

●
●
●

●

●

●

●

mean width

coverage

●

●

●

0.90

0.15

●

0.10
●

0.05

0.86

bootstrap
no
yes
●
●

●

● ●
● ● ●
● ● ● ● ● ● ● ●
● ● ●

0.00
100

500

1000

1500

2000

n

100

500

1000

1500

2000

n

Figure 5: Comparisons of coverage (left column) and mean width (right column) between
bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study C, for correlation c = 0. The horizontal axes display
the sample sizes n = 100, 200, . . . , 2000.

14

3.4

Computational benchmarks

From Figure 6, we see that the memory usage (left) and mean execution time (right) for the
bootstrap CIs are both higher than that for the asymptotic CIs, and that the ratio increases
with sample size. As n increases, asymptotic CIs become increasingly efficient, compared to
the bootstrap CIs. On the other hand, as d increases, with n fixed, we expect an increase
in the relative efficiency of the bootstrap, since the complexity of calculating acov(ξj , ξk ) in
Theorem 1 grows faster in d than the complexity of the bootstrap procedure.
Table 1: Parameters for computational benchmarking.
Parameter

Value(s)

Number of features (d)
Sample sizes (n)
Number of bootstrap resamples (Nb )
Number of simulation repetitions (N )

3
1000, 5000, 10000
1000
1000

Mean execution time ratios

60

40

79.644
70.962

20

42.005

Bootstrap time / Normal time

Bootstrap memory use / Normal memory use

Memory usage ratios
80

0

40

53.097
46.792

20
27.306

0
1000

5000
Sample size (n)

10000

1000

5000
Sample size (n)

10000

Figure 6: Computational benchmark metric ratios of confidence interval estimation using
the naïve bootstrap over the asymptotic normality approach.

3.5

Summary of results and recommendations for use

Below follows a summary of the general tendencies and observations from our results, and
recommendations regarding when to use the asymptotic CIs.
• For all correlations c in all three Studies, the estimated coverage probability of asymptotic intervals is above 0.85 for all sample sizes n ≥ 10.
15

• For smaller correlations and sample sizes, in particular c ≥ 0.2 and n > 15, the lower
bound of the confidence interval for coverage never drops below 0.85.
• For all correlations c ≥ 0.3 and sample sizes n > 100, the lower bound of the confidence
interval for coverage never drops below 0.9.
• For small correlations, in particular c ≤ 0.1 and sample sizes 10 ≤ n ≤ 100, the lower
bound of the confidence interval for the coverage of the asymptotic CIs never drops
below 0.91.
• For c = 0 and n ≥ 15, the lower bound of the asymptotic CI for coverage never drops
below 0.95 in Studies A and B, while in Study C the lower bound is at least 0.88.
• For sample sizes 15 ≤ n ≤ 50, the coverage of the asymptotic CI tends to be higher
when c is closer to the boundaries of [0, 1], as shown in Figure 7.

●

0.94

0.96

●

●

●

n

0.92

●

0.88

0.90

estimated coverage

0.98

1.00

• The average asymptotic CI width is lower when c is nearer to the boundaries of [0, 1],
see Figure 8.

0.0

15
20
25
30
35
40
45
50

●
●
●

●

●

●
●

0.2

0.4

0.6

0.8

1.0

c

Figure 7: Estimated coverage probability versus correlation c, for small samples sizes n =
15, 20, . . . , 50, in MC Study A. The same patterns can be observed for Studies B and C.
We now make some general observations which apply to all three Studies. As sample
size increases, the estimated coverage initially increases rapidly, as can be seen, for example,
in the left column of Figure 3. For small sample sizes between n = 5 and n = 50, the
asymptotic CIs typically outperform bootstrap CIs, especially when c lies farther from 0.5;
there is a clear drop in coverage as c approaches 0.5, for small samples, as can be seen in
16

●

●

●

●

0.04

0.3

mean width

15
20
25
30
35
40
45
50

0.2

0.4

0.6

●

n

●

0.02

●

●

0.01

●

●
●

0.8

0.00

0.2
0.0

0.1

mean width

n
●

●
●

●

0.0

●

●

0.05

●

●

●

0.03

0.4

●
●

1.0

0.0

c

0.2

●

1000
1200
1400
1600
1800
2000

0.4

0.6

●

0.8

1.0

c

Figure 8: Average confidence interval width versus correlation c, for small sample sizes
n = 15, 20, . . . , 50 (left) and large sample sizes n = 1000, 1200, . . . , 2000 (right), in MC
Study A. The same patterns are present in MC Study B and C.
Figure 7. In many cases, the estimated coverage is above 0.9 for n ≥ 10. However, empirical
coverage does not appear to be an increasing function of sample size in general. On the top
row in the left column of Figure 1, we observe one example of a clear and extended dip in
coverage for n in [100, 1000]. This gives rise to the general observation that the asymptotic
intervals have preferable coverage statistics over bootstrap for small samples, but not for a
certain range of large samples, depending on c.
We further observe that, for all n, there is a general increase in the average CI width as c
approaches 0.5 from either direction, as in Figure 8. In all Studies, over all sample sizes and
correlations, the bootstrap CI average widths were smaller than the asymptotic CI widths
by at most 0.0289, and vice versa by at most 0.0667. In general, the asymptotic intervals
display favourable widths, though less so near c = 0.5.
Based on these observations, we recommend using asymptotic CIs over bootstrap CIs
under the following conditions:
(i) Computational time is relevant (e.g., estimating a large number of Shapley values).
(ii) The sample size is small (e.g., n ≤ 50).
(iii) The correlation between explanatory variables and the response variable is expected
to be beyond ±0.2 from 0.5, or when this is where the highest precision is desired.
We note that our observations are made from an incomplete albeit comprehensive set of
17

simulation scenarios. There are of course an infinite number of combinations of simulation
cases and thus we cannot guarantee that our observation applies to all possible DGPs.

4

Application: Melbourne real estate and COVID-19

For an interesting application of our methods, in this section we identify significant changes
in the behaviour of the local real estate market in Melbourne, Australia, within the period
from 1 February to 1 April, between the years 2019 and 2020. In 2020, this corresponds
to an early period of growing public concern regarding the novel coronavirus COVID-19.
We obtain the Shapley decomposition of the coefficient of multiple correlation R2 between
observed house prices and a number of property features. We also find significant differences
in behaviour between real estate near and far from the Central Business District (CBD),
where near is defined to be within 25 km of Melbourne CBD, and far is defined as non-near
(see Figure 11). Note that the nature of this investigation is exploratory and expository; our
conclusions are not intended to be taken as evidence for the purposes of policy or decision
making.
On 1 February the Australian government announced a temporary ban on foreign arrivals from mainland China, and by 1 April a number of social distancing measures were
in place. We scraped real estate data from the AUHousePrices website (https://www.
auhouseprices.com/), to obtain a data set of 13,291 (clean) house sales between 1 January
and 18 July in 2019 and 2020. We then reduced this date range to capture only the spike in
sales observed between 1 February and 1 April (see Figure 10), giving a remaining sample
size of 5110, which was partitioned into the four subgroups in Table 2. Within each of
the four subgroups we perform a Yeo-Johnson transformation [Yeo and Johnson, 2000] to
reduce any violation of the assumption of joint pseudo-ellipticity.
Table 2: The four subgroups and their sample sizes after partitioning by distance (where
near := within 25 km of CBD), and year of sale in the period 1 February to 1 April 2019
and 2020.
Subgroup:

near (2019)

far (2019)

near (2020)

far (2020)

Sample size:

1203

953

1824

1130

We decompose R2 amongst the covariates: distance to CBD (CBD); images used in
advertisement (images); property land size (land ); distance to nearest school (school ); distance to nearest station (station); and number of bedrooms + bathrooms + car ports (room);
along with the response variable, house sale price (price). We expect the room covariate to
act as a proxy for house size. Thus we decompose R2 for the linear model,
E(price) = β0 + β(CBD, images, land, school, station, room)T , β0 ∈ R, β ∈ R6 .

(14)

Fitted to each of the four subgroups, we obtain R2 = 0.37 for model (14), for each
subgroup except for the near (2020) subgroup, for which R2 = 0.39. The resulting Shapley
values and 95% confidence intervals are listed in Table 3, and shown graphically in Figure 9.
18

From those results we make the following observations regarding attributions of the total
variability in house prices explained by model (14):
(i) In both 2019 and 2020, the attribution for distance to CBD was significantly higher
for house sales near to the CBD, compared to house sales farther from the CBD.
Correspondingly, the attribution for roominess was significantly lower for house sales
near to the CBD.
(ii) Amongst sales that were near to the CBD, distances to the CBD received significantly
greater attribution than both land size and roominess in 2019. Unlike the case for sales
that were far from the CBD, these differences remained significant in 2020.
(iii) Distances to stations and schools, as well as images used in advertising, have apparently had little overall impact. In all four subgroups, the attribution for distance to
the nearest school is not significantly different from 0. However, distance to a station
does receive significantly more attribution amongst houses that are near to the CBD,
compared to those farther away.
(iv) Interestingly, while not a significant result, the number of images used in advertising
did appear to receive greater attribution amongst house sales that were far from the
CBD, compared to those near to it.
(v) Amongst sales that were far from the CBD, land size and roominess both received
significantly more attribution than distance to the CBD, in 2019. However, this difference vanished in 2020, with distances to the CBD apparently gaining more attribution,
while roominess and land size apparently lost some attribution, in a relative leveling
out of these three covariates.
Item (i) is perhaps unsurprising: distances were less relevant far from the city, where
price variability was influenced more by roominess and land size. Indeed, we can assume
we are less likely to find small and expensive houses far from the CBD. However, the
authors find Item (v) interesting: near the city, the behaviour didn’t change significantly
during the 2020 period. However, far from the city, the behaviour did change significantly,
moving toward the near-city behaviour. Distance to the city became more important for
explaining variability in price, while land size and roominess both became less important,
compared with their 2019 estimates. Our initial guess at an explanation was that near-city
buyers, with near-city preferences, were temporarily motivated by urgency to buy farther
out. However, according to Table 2, the observed ratio of near-city buyers to non-near
buyers actually increased in this period, from 1.26 in 2019 to 1.61 in 2020. We will not take
this expository analysis further here, but we hope that the interested reader is motivated
to take it further, and to this end we have made the data and R, python and Julia scripts
available at github.com/BSMLcode/shapley_confidence.

19

0.20

●

0.15

●

2019

0.10
0.05

Shapley

●

type

●

●

0.00

●
●

far
near

0.20
0.15
0.10

2020

●

●

●

0.05
●

0.00
CBD

images

land

room

●

●

school

station

feature
Figure 9: Shapley values and associated asymptotic 95% confidence intervals for each of
the 6 covariates, within the 4 real estate data subgroups (near/far and 2019/2020). Blue
bands and triangle markers represent the near subgroup (i.e., ≤ 25km from Central Business
District) and red bands with circular markers represent the far subgroup (> 25km).
Table 3: Shapley values and asymptotic CIs for the covariates of the real estate data.
Feature

near (2019)

far (2019)

near (2020)

far (2020)

CBD
images
land
school
station
room

0.19
0.00
0.09
0.00
0.02
0.07

0.03
0.01
0.14
0.00
0.00
0.19

0.20
0.06
0.09
0.00
0.03
0.07

0.10
0.02
0.13
0.00
0.00
0.12

( 0.15,0.23)
(-0.00,0.00)
( 0.06,0.12)
(-0.00,0.00)
( 0.01,0.04)
( 0.05,0.09)

( 0.02,0.05)
( 0.00,0.02)
( 0.10,0.18)
(-0.01,0.01)
(-0.00,0.00)
( 0.15,0.23)

20

( 0.17,0.23)
( 0.00,0.01)
( 0.07,0.11)
(-0.00,0.00)
( 0.02,0.04)
( 0.05,0.08)

( 0.06,0.12)
( 0.01,0.03)
( 0.10,0.17)
(-0.00,0.00)
( 0.00,0.01)
( 0.09,0.16)

5

Discussion

In Section 2, we showed that under an elliptical (or pseudo-elliptical) joint distribution
assumption, the game theoretic Shapley value decomposition of R2 (Zn ) is asymptotically
normal. Implementing this result, we produced asymptotic Shapley value CIs and hypothesis
tests.
In Section 3, we examined the coverage and width statistics of these asymptotic CIs
over a range of sample sizes, using Monte Carlo simulations. These simulations were conducted across three separate data generating processes: using a variety of correlations with
a compound symmetry covariance matrix under multivariate normal (i) and Student-t (ii)
distributions. The simulations were also conducted under a normal distribution data generating process, with random Wishart covariance matrix (iii). In all three cases, the coverage
and width statistics were compared to the corresponding statistics for the non-parametric
bootstrap CIs. The computation time and memory usage were also benchmarked against
the bootstrap CIs. In Section 3.5 we provided recommendations for when asymptotic CIs
should be preferred and used, over the bootstrap CIs. We found that the asymptotic CIs
have estimated coverage probabilities of at least 0.85 across all studies, are preferable over
the bootstrap CIs for small sample sizes (n ≤ 50), and are often (although not always)
favourable for large sample sizes. The asymptotic CIs are also far more computationally
efficient than bootstrap CIs (at least for the cases of three and five explanatory variables),
and show improved coverage and width when correlation is further from c = 0.5.
Finally, in Section 4, we demonstrated the application of our derived asymptotic CIs
to a data set consisting in house prices from Melbourne, to investigate a period of altered
consumer behaviour during the initial stages of the arrival of COVID-19 in Australia. Using
the CIs, we identified significant changes in model behaviour between 2019 and 2020, and
attributed these changes amongst features, highlighting a potentially interesting direction of
future research into the period. We also observed significant changes in behaviour between
houses classified as close to the city, and those far from the city.
We have made the computational implementations of our methods openly available for
use. These computational resources are implemented in the R and Julia programming
languages.
We are preparing R, Julia, and Python versions of our methods for release. Our code
and future progress regarding these implementations will be made available at github.com/
BSMLcode/shapley_confidence. Additionally, we aim to use what we have developed in
order to derive the asymptotic distributions of variance inflation factors and their generalizations [Fox and Monette, 1992], as well as the closely related Owen values decomposition
of the coefficient of determination [Huettner and Sunder, 2012].

21

A

Additional figures

200
2019

sales

100

0

200
2020

100

0
01−01

02−01

03−01

04−01

05−01

06−01

07−0107−18

date
Figure 10: Bar plot of sales per day between 1 January and 18 July in 2019 and 2020.
Vertical dashed red lines indicate 1 February and 1 April, between which a spike in sales is
observed.

22

n
300
200
100

Figure 11: Map of Melbourne suburbs included in this study, with greyscale shading to
represent sample sizes. Pink dashed polygon borders contain suburbs classified as near to
the Central Business District (i.e., ≤ 25km), and blue solid polygon borders represent those
classified as far (> 25km). The solid red diamond represents the location of the CBD.

23

c = 0.1
1.00
0.75
0.50
0.25

●

●

●

●

●

c = 0.1
●

●

●

●

0.75
0.50
0.25
0.00

●

5

10

20

30

40

50

●
●

●

5 10

●

20

c = 0.2
1.00
0.80
0.60
0.40
0.20

●

●

●

●

●

10

20

●

●

●

0.75
0.50
0.25
0.00

30

40

50

●

●

●

5 10

●

●

●

●

●

●

●

●

●

5

10

20

30

40

50

c = 0.6
0.90
0.80
0.70

●

●

●

●

●

●

●

●

●

0.75
0.50
0.25

●

●

●

20

30

40

●

0.90
0.80

●

●

●

●

20

●

●

●

30

●

●

●

●

●

40

●

●

●

5 10

0.60
0.40
0.20

50

10

20

30

●

40

●

50

●

●

●

●

●

●

30

40

50
no
yes

●

●

20

●

●

30

●

●

40

●

50

●

●

●

5 10

●

●

20

●

●

30

●

●

40

●

50

c = 0.99
●

●

●

●

●

5

30

●

0.75

c = 0.99
0.90
0.80
0.70

●

c = 0.9
●

0.70
10

●

●

50

●

5

●

50

c = 0.6

c = 0.9
●

●

20

0.25
10

●

bootstrap
5 10

0.50

●

5

●

40

c = 0.3
●

mean width

coverage

●

●

30

20

c = 0.3
1.00
0.80
0.60
0.40

●

c = 0.2
●

●

5

●

40

50

n

0.20
0.15
0.10
0.05
0.00

●

●

5 10

●

●

20

●

●

30

●

●

40

●

●

50

n

Figure 12: Comparisons of coverage (left column) and mean width (right column), between
the bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study A. Rows represent correlations c, increasing from 0.1
on the top row to 0.99 on the bottom row. The horizontal axes display the sample sizes
n = 5, 10, . . . , 50.

24

c = 0.1
1.00
0.75
0.50
0.25

●

●

●

●

●

c = 0.1
●

●

●

●

0.75
0.50
0.25
0.00

●

5

10

20

30

40

50

●
●

●

5 10

●

20

c = 0.2
1.00
0.80
0.60
0.40

●

●

●

●

●

10

20

●

●

●

0.75
0.50
0.25
0.00

30

40

50

●

●

●

5 10

●

●

●

●

●

●

●

●

●

5

10

20

30

40

50

c = 0.6
0.90

●

●

●

●

●

●

●

●

●

0.80
0.70

●

5

10

20

30

40

0.75
0.50
0.25

●

●

●

●

0.80

●

●

●

●

●

●

0.70
20

●

●

●

30

●

●

40

10

20

30

●

●

●

40

●

50

●

●

●

●

●

●

30

40

50
●

0.75
0.50
0.25

●

●

●

5 10

0.60
0.40
0.20

50

●

●

20

●

no
yes

●

30

●

●

40

●

50

●

●

●

5 10

●

●

20

●

●

30

●

●

40

●

50

c = 0.99
●

●

●

●

●

5

●

30

20

c = 0.99
0.90
0.80
0.70

●

c = 0.9
●

●

10

●

50

c = 0.6

50

●

5

●

bootstrap
5 10

c = 0.9
●

0.90

●

40

c = 0.3
●

mean width

coverage

●

●

30

20

c = 0.3
1.00
0.80
0.60
0.40

●

c = 0.2
●

●

5

●

40

50

n

0.20
0.15
0.10
0.05
0.00

●

●

5 10

●

●

20

●

●

30

●

●

40

●

●

50

n

Figure 13: Comparisons of coverage (left column) and mean width (right column), between
the bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study B. Rows represent correlations c, increasing from 0.1
on the top row to 0.99 on the bottom row. The horizontal axes display the sample sizes
n = 5, 10, . . . , 50.

25

c = 0.1
0.95
0.90
0.85

●
●

●
●

●

100

●

●

●

500

●

c = 0.1
●

●

●

●

1000

●

●

●

●

●

1500

●

0.20
0.15
0.10 ●
0.05 ●
0.00
100

●

2000

● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ●

500

c = 0.2
0.98
0.95
0.92
0.90
0.88

●
●

●

●

●

●

●

●

●

●

500

●

●

1000

●

●

●

●

●
●

●

1500

2000

●

● ●
● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ●

100

500

●
●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

100

500

1000

1500

2000

c = 0.6
0.96
0.94

●

●

●

●

●

●

●

0.92
100

●

●

●

●

●

●

500

1000

●

●

●

●
●

●

●

1500

2000

0.20
0.15
0.10
0.05

●
●

●

●

●

●

●

●

500

●

● ●
● ● ●
● ● ● ● ● ● ●
● ● ● ● ● ●

100

500

●

1000

●

●

100

●

●

●

●
●

●

●

●

●

●

●
●

1500

500

●

●

●

●

●

1000

1000

1500

bootstrap

2000
●

c = 0.6
0.20
●
0.15
●
0.10
0.05
100

●

no
yes

● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ● ●

500

2000

0.08
0.06
0.04
0.02

1000

1500

2000

●
●

100

● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ● ● ●

500

c = 0.99
0.97
0.96
0.95
0.94
0.93
0.92

2000

c = 0.9

●
●

1500

●

c = 0.9
0.97
0.96 ●
0.95
●
0.94
0.93
0.92
0.91
100

1000
c = 0.3

●

mean width

coverage

0.95
0.92
0.90

2000

●

c = 0.3
●

1500

c = 0.2
0.20
0.15
0.10
0.05

●

100

1000

1000

1500

2000

c = 0.99
●

●
●

●

1500

●
●

●

●

2000

n

0.01
0.01
0.00
0.00

●
●

100

● ●
● ● ●
● ● ● ● ● ● ●
● ● ● ● ● ●

500

1000

1500

2000

n

Figure 14: Comparisons of coverage (left column) and mean width (right column), between
the bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study B. Rows represent correlations c, increasing from 0.1
on the top row to 0.99 on the bottom row. The horizontal axes display the sample sizes
n = 100, 200, . . . , 2000.

26

c = 0.1
0.95
0.92
0.90

●

●

●

●

●

●

●

●

●

●

c = 0.1

●

●

●

●

●

●

0.20
0.15
0.10 ●
0.05 ●
0.00
100

●
●

●

●

100

500

1000

1500

2000

● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ●

500

c = 0.2
0.96
0.94
0.92
0.90

●
●

●

●

●

100

●

●

●

●

●

●

0.25
0.20
0.15 ●
0.10 ●
0.05
0.00
100

●
●

1000

●

●

●

●

1500

●

●

2000

500

●

●

●

●
●

●

●

●
●

●

●

●

●

●

500

1000

1500

2000

c = 0.6
0.96
0.94
0.92

●

●

●
●

●

●

●

●

●

●

●

100

●

●
●

500

1000

●

●

●
●

●

1500

mean width

coverage

●
●

●

2000

0.25
0.20
0.15 ●
0.10 ●
0.05
0.00
100

0.96
● ●
0.94
0.92
100

●

0.96
0.94
0.92

●

●

●

●

500

500

●

100

●

●

●

●

●

1000

●

●
●

●

●

●

●

●

●

●

●

1500

●
●

2000

1000

1500

bootstrap

2000
●

0.20
0.15
0.10
0.05

no
yes

●
●

100

● ●
● ● ●
● ● ● ● ● ●
● ● ● ● ● ● ●

500

1000

1500

2000

0.08
0.05
0.02

●
●

100

● ●
● ● ●
● ● ● ● ● ● ●
● ● ● ● ● ●

500

1000

1500

2000

c = 0.99
●

●

●
●

500

1000
c = 0.6

c = 0.99
●

2000

c = 0.9

●

●

●

1500

● ●
● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ●

c = 0.9
●

1000
c = 0.3

●
●

2000

● ●
● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ●

c = 0.3
0.96
0.94
0.92 ● ●
0.90
100

1500

c = 0.2

●

500

1000

●

●

1500

●

●
●

2000

n

0.01
0.01
0.00

●
●

100

● ●
● ● ● ●
● ● ● ● ● ● ● ● ●
● ● ●

500

1000

1500

2000

n

Figure 15: Comparisons of coverage (left column) and mean width (right column), between
the bootstrap CIs (dashed lines with triangular markers) and the asymptotic CIs (solid lines
with circular markers) in MC Study C. Rows represent correlations c, increasing from 0.1
on the top row to 0.99 on the bottom row. The horizontal axes display the sample sizes
n = 100, 200, . . . , 2000.

References
[Baglivo, 2005] Baglivo, J. A. (2005). Mathematica laboratories for mathematical statistics:
emphasizing simulation and computer intensive methods. SIAM, Philadelphia.

27

[Clopper and Pearson, 1934] Clopper, C. J. and Pearson, E. S. (1934). The use of confidence
or fiducial limits illustrated in the case of the binomial. Biometrika, 26(4):404–413.
[Cowden, 1952] Cowden, D. J. (1952). The multiple-partial correlation coefficient. Journal
of the American Statistical Association, 47:442–456.
[Efron, 1988] Efron, B. (1988). Computer-intensive methods in statistical regression. SIAM
Review, 30:421–449.
[Fang et al., 1990] Fang, K. T., Kotz, S., and Ng, K. W. (1990). Symmetric Multivariate
and Related Distributions. Chapman and Hall.
[Feltkamp, 1995] Feltkamp, V. (1995). Alternative axiomatic characterizations of the shapley and banzhaf values. International Journal of Game Theory, 24(2):179–186.
[Fox and Monette, 1992] Fox, J. and Monette, G. (1992). Generalized collinearity diagnostics. Journal of the American Statistical Association, 87(417):178–183.
[Fujikoshi et al., 2011] Fujikoshi, Y., Ulyanov, V. V., and Shimizu, R. (2011). Multivariate
statistics: High-dimensional and large-sample approximations, volume 760. John Wiley
& Sons.
[Genizi, 1993] Genizi, A. (1993). Decomposition of R2 in multiple regression with correlated
regressors. Statistica Sinica, pages 407–420.
[Greene, 2007] Greene, W. H. (2007). Econometric Analysis. Prentice Hall, New Jersey.
[Gromping, 2006] Gromping, U. (2006). Relative importance for linear regression R: the
package relaimpo. Journal of Statistical Software, 17:1–27.
[Gromping, 2007] Gromping, U. (2007). Estimators of relative importance in linear regression based on variance decomposition. American Statistician, 61:139–147.
[Hart et al., 1988] Hart, S., Mas-Colell, A., et al. (1988). The potential of the shapley value.
the Shapley value, pages 127–137.
[Hedges and Olkin, 1983a] Hedges, L. V. and Olkin, I. (1983a). Joint Distribution of Some
Indices Based on Correlation Coefficients. Technical Report MCS 81-04262, Stanford
University.
[Hedges and Olkin, 1983b] Hedges, L. V. and Olkin, I. (1983b). Joint distributions of some
indices based on correlation coefficients. In Karlin, S., Amemiya, T., and Goodman,
L. A., editors, Studies in Econometrics, Time Series, and Multivariate Analysis. Academic
Press.
[Huettner and Sunder, 2012] Huettner, F. and Sunder, M. (2012). Axiomatic arguments for
decomposiing goodness of fit according to Shapley and Owen values. Electronic Journal
of Statistics, 6:1239–1250.
[Israeli, 2007] Israeli, O. (2007). A Shapley-based decomposition of the R-square of a linear
regression. Journal of Economic Inequality, 5:199–212.
28

[Kruskal, 1987] Kruskal, W. (1987). Relative importance by averaging over orderings. The
American Statistician, 41:6–10.
[Lipovetsky and Conklin, 2001] Lipovetsky, S. and Conklin, M. (2001). Analysis of regression in game theory approach. Applied Stochastic Models in Business and Industry,
17:319–330.
[Mardia, 1970] Mardia, K. V. (1970). Measures of multivariate skewness and kurtosis with
applications. Biometrika, 57:519–530.
[Olkin and Finn, 1995] Olkin, I. and Finn, J. D. (1995). Correlations redux. Quantitative
Methods in Psychology, 118:155–164.
[Seber, 2008] Seber, G. A. F. (2008). A Matrix Handbook For Statisticians. Wiley, New
York.
[Seber and Lee, 2003] Seber, G. A. F. and Lee, A. J. (2003). Linear Regression Analysis.
Wiley, Hoboken.
[Shapley, 1953] Shapley, L. S. (1953). A value for n-person games. In Kuhn, H. W. and
Tucker, A. W., editors, Contributions to the Theory of Games. Princeton University Press,
Princeton.
[Steiger and Browne, 1984] Steiger, J. H. and Browne, M. W. (1984). The comparison of
interdependent correlations between optimal linear composites. Psychometrika, 49:11–24.
[van der Vaart, 1998] van der Vaart, A. (1998). Asymptotic Statistics. Cambridge University
Press, Cambridge.
[Yeo and Johnson, 2000] Yeo, I.-K. and Johnson, R. A. (2000). A new family of power
transformations to improve normality or symmetry. Biometrika, 87(4):954–959.
[Young, 1985] Young, H. P. (1985). Monotonic solutions of cooperative games. International
Journal of Game Theory, 14:65–72.
[Yuan and Bentler, 1999] Yuan, K.-H. and Bentler, P. M. (1999). On normal theory and
associated test statistics in covariance structure analysis under two classes of nonnormal
distributions. Statistica Sinica, 9:831–853.
[Yuan and Bentler, 2000] Yuan, K.-H. and Bentler, P. M. (2000). Inferences on correlation
coefficients in some classes of nonnormal distributions. Journal of Multivariate Analysis,
72:230–248.

29

