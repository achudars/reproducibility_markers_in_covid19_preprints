arXiv:2008.06178v2 [econ.EM] 27 Jan 2021

Bounding Infection Prevalence
by Bounding Selectivity and Accuracy of Tests:
With Application to Early COVID-19∗
Jörg Stoye†
January 29, 2021

Abstract
I propose novel partial identification bounds on infection prevalence from information
on test rate and test yield. The approach utilizes user-specified bounds on (i) test accuracy
and (ii) the extent to which tests are targeted, formalized as restriction on the effect of
true infection status on the odds ratio of getting tested and thereby embeddable in logit
specifications. The motivating application is to the COVID-19 pandemic but the strategy
may also be useful elsewhere.
Evaluated on data from the pandemic’s early stage, even the weakest of the novel
bounds are reasonably informative. Notably, and in contrast to speculations that were
widely reported at the time, they place the infection fatality rate for Italy well above the
one of influenza by mid-April.

∗

Thanks to Chuck Manski and Francesca Molinari for stimulating discussions and for sharing their data,
to Dan Sacks, Coady Wing, and Gabriel Ziegler for feedback and literature pointers, and to Cynthia Stoye for
special support in crazy times. Any and all errors are mine.
†
Department of Economics, Cornell University, stoye@cornell.edu.

1

1

Introduction

Prevalence of a novel infection like SARS-CoV-2 (the virus causing COVID-19 disease) is a
quintessential missing data problem. Only a small subset of the population has been tested,
this subset is almost certainly selective, we do not even know the accuracy of tests, and our
understanding of the pandemic is vague enough so that we might not want to overly rely on
heavily parameterized models. This is a natural application for partial identification analysis,
i.e. the analysis of bounds on parameter values that can be inferred from imperfect data and
weak but credible assumptions, without forcing statistical identifiability of a model.1 The
present paper proposes a general framework for analyzing partial identification of disease
prevalence, assuming that one has partially identifying information on the selectivity and
sensitivity of diagnostic tests.
The obvious precedent for this is Manski and Molinari (2021, MM henceforth). I agree
with that paper’s overall thrust but propose a considerably different implementation, refining
worst-case bounds by bounding test sensitivity and selectivity but not predictive value (all
terms will be defined later). These restriction are readily related to other literatures, and
–unlike with predictive values– nonvacuous prior bounds on them can be asserted without
implying informative prior bounds on prevalence itself. In the empirical application, bounds
that only restrict the direction of selectivity are considerably more informative than the
analogous bounds emphasized in MM, and yet I will argue that assumptions became more
compelling. The difference matters: The novel bounds on the infection fatality rate exclude
“flu-like” values, which were the subject of speculation at the time, at a rather early stage.
They become much tighter, though at the cost of reduced credibility, if one substantively
restricts selectivity.

2

The Identification Problem

2.1

Basic Setting and Worst-Case Bounds

Consider first the problem of bounding prevalence of an infection in a stylized example where
one has observed test rate and test yield for one population. I will call the disease COVID-19
henceforth but the ideas are obviously more general. For readability, I also follow common
parlance and loosely refer to COVID-19, though I strictly speaking investigate SARS-CoV-2
infection as opposed to COVID-19 disease.
Thus, let C indicate true infection status (with C = 1 indicating infection), T test status
(with T = 1 indicating having been tested), and R test result (with R = 1 a positive test
result; we observe R only conditionally on T = 1). In particular, define the testing rate
τ := Pr(T = 1) and the test yield γ := Pr(R = 1|T = 1). These objects are directly identified
1

See Manski (2003) for an early monograph and Molinari (2020) for an extensive survey.

[2]

from the data, and we initially assume that they are known; indeed, inference will turn out to
be a secondary concern. We also maintain the assumption that (PCR-)tests have specificity
(=true negative rate Pr(R = 0|T = 1, C = 0)) of 1; thus, Pr(C = 1|R = 1) = 1. Generalizing
away from this simplification would be straightforward.
Worst-case bounds on the true infection rate can then be derived from the Law of Total
Probability and the logical bound of [0, 1] on any unknown probability. In particular, write
Pr(C = 1) = Pr(C = 1|R = 1) Pr(R = 1) + Pr(C = 1|R = 0) Pr(R = 0)
and observe that Pr(C = 1|R = 0) ∈ [0, 1], whereas Pr(R = 1) = γτ and Pr(C = 1|R = 1) = 1
by maintained assumption. Thus, without any further assumption,
Pr(C = 1) ∈ [γτ, 1].

(2.1)

These bounds go back to Manski (1989) in spirit and are also the starting point of MM. I
next lay out novel ways to refine them.

2.2

Introducing Bounds on Sensitivity and Selectivity of Tests

Consider injecting prior information on test sensitivity (i.e., true positive rate Pr(R = 1|C =
1)) and on test selectivity (i.e., the relation of Pr(T = 1|C = 1) to Pr(T = 1|C = 0)
but not either of these two probabilites by itself). I do not claim that any of these are
context-independent, much less known; hence, the prior information will itself be in the
form of bounds. However, test sensitivity relates directly to a large medical literature, and
test selectivity is readily related to statistical models of binary response. I next explain the
approach and work out its implications.
Refinement: Allow for measurement error through bounding sensitivity.

Test

sensitivity is the target parameter in much research on COVID-19 (and, of course, more
generally). Thus, consider:
Assumption 1: Sensitivity of the test is bounded by
Pr(R = 1|C = 1, T = 1) =: π ∈ [π, π].

(2.2)

The assumption takes a notational shortcut: It seemingly implies that π is constant
across true prevalence and test rates. This textbook view (Zhou et al., 2002, chapter 2) has
been challenged (Leeflang et al., 2008). In the specific case of COVID-19, both prevalence
and testing rate might influence sensitivity through the distribution of viral load among the
tested and infected; the corresponding conjecture that asymptomatic surveillance might be
characterized by lower sensitivity than symptomatic surveillance has some empirical corrob[3]

oration (Mohammadi et al., 2020; Zhang et al., 2021). However, bounds derived below do
not exploit constancy of π beyond the fact that π ∈ [π, π]. So this is best thought of as
a shortcut to avoid further subscripts, though users should keep the consideration in mind
when specifying (π, π).
The effect of Assumption 1 on prevalence bounds is easily calculated.
Proposition 1: Suppose Assumption 1 holds. Then prevalence is sharply bounded by
ρ ∈ [γτ /π, γτ /π + 1 − τ ].

(2.3)

Proof. Write
Pr(C = 1) = Pr(C = 1|T = 1) Pr(T = 1) + Pr(C = 1|T = 0) Pr(T = 0).

(2.4)

While no informative bound on Pr(C = 1|T = 0) is available, we have
Pr(R = 1|T = 1)
= Pr(R = 1|C = 1, T = 1) Pr(C = 1|T = 1) + Pr(R = 1|C = 0, T = 1) Pr(C = 0|T = 1)
|
{z
}
=0 by assumption

= Pr(R = 1|T = 1, C = 1) Pr(C = 1|T = 1),
{z
}
|
=π

implying (in the notation introduced above) that Pr(C = 1|T = 1) = γ/π ∈ [γ/π, γ/π]. The
bounds follow by substituting into (2.4).
Remark 2.1: This result is easily extended to allow for specificity (=true negative rate
Pr(R = 0|C = 0, T = 1)) to differ from 1. Indeed, the bounds simply adjust prevalence
in the tested population through the well-known formula “prevalence=(yield+specificity1)/(sensitivity+specificity-1)” and leave prevalence in the untested population unconstrained.
This is not worked out to economize on notation.
Refinement: A “logit bound” on test selectivity.

Consider also the following:

Assumption 2: The factor κ in
Pr(T = 1|C = 1)
Pr(T = 1|C = 0)
=κ
1 − Pr(T = 1|C = 1)
1 − Pr(T = 1|C = 0)
can be bounded as κ ∈ [κ, κ].
Assumption 2 resembles sensitivity analysis for treatment effects in Rosenbaum (2002). It
bounds the relative odds ratio of being tested between true positives and true negatives. Of
course, this is only one of many possible ways to constrain how targeted tests are. However,
[4]

it is easily related to standard models of selection as binary response. In particular, bounding
κ in the above is equivalent to bounding it in the logit model
Pr(T = 1|C = c) =

exp(α + κc)
.
1 + exp(α + κc)

Logit models are well understood in econometrics and medical statistics, so this connection
generates an interface to natural estimation strategies and maybe researcher intuitions about
plausible parameter values.
For example, Canning et al. (2020) model the age-dependent effect of COVID-19 syptoms
on social distancing behavior through a logit; a similar model could in principle be used to
model self-selection into symptomatic surveillance. If such a model were applied to the
propensity to get tested, true infection status could be treated as hidden covariate. If one
were furthermore willing to bound the coefficient on this covariate –where the bounds may
depend on the values of other covariates– then conditionally on any realization of observed
covariates, one is in the setting of Assumption 2. In a setting of symptomatic surveillance, the
propensity of noninfected subjects to get tested could, for example, relate to the age-specific
frequency of influenza-like symptoms.
The selectivity factor κ could be bounded from both above and below. For this paper’s application, I will impose throughout that κ ≥ 1, thus there is at least weak selection of infected
subjects into testing, and I will consider values of κ that force strict selection. Bounding selectivity from above, or also allowing for a lower bound below 1, may be interesting in other
contexts, for example, if getting tested is stigmatized or tests are targeted but not at the
at-risk population.
The implications of bounding κ are slightly more involved.
Proposition 2: Suppose that Assumptions 1 and 2 hold. Then prevalence is sharply
bounded by

π + (κ − 1)τ (π − γ) γ
γ
π + (κ − 1)τ (π − γ)
×
, ×
.
Pr(C = 1) ∈
π
κ(π − γ) + γ
π
κ(π − γ) + γ


(2.5)

including by the corresponding limiting expressions as κ → −∞ or κ → ∞. In particular, if
κ = ∞ as in the empirical application, we have

π + (κ − 1)τ (π − γ)
τγ γ
, ×
.
P r(C = 1) ∈
π π
κ(π − γ) + γ


(2.6)

Proof. To keep algebra transparent, introduce new notation τc ≡ Pr(T = 1|C = c). Recall
also the notation ρ = Pr(C = 1). Write
γ=

Pr(R = 1, T = 1)
ρτ1 π
γτ
=
=⇒ τ1 =
.
Pr(T = 1)
τ
ρπ
[5]

(2.7)

Substituting
Pr(T = 1|C = 0)
τ1
Pr(T = 1|C = 1)
=κ
=⇒ τ0 =
1 − Pr(T = 1|C = 1)
1 − Pr(T = 1|C = 0)
τ1 + κ(1 − τ1 )
into the accounting identity τ = ρτ1 + (1 − ρ)τ0 yields
τ = ρτ1 + (1 − ρ)

τ1
.
τ1 + κ(1 − τ1 )

Substituting for τ1 from (2.7) yields the following algebra:
γτ

γτ
ρπ


+ (1 − ρ)
τ=
γτ
γτ
π
+
κ
1
−
ρπ
ρπ
γπ
⇐⇒ π = γ + (1 − ρ)
κρπ − (κ − 1)γτ
⇐⇒ (κρπ − (κ − 1)γτ )(π − γ) = (1 − ρ)γπ
γ
π + (κ − 1)τ (π − γ)
⇐⇒ ρ = ×
.
π
κ(π − γ) + γ
By taking derivatives, one can verify that the r.h. fraction in the last expression, and therefore
the entire expression, decreases in both π and κ. Bounds follow by evaluating it at (π, κ) =
(π, κ) respectively (π, κ) = (π, κ).
The bounds effectively multiply sample prevalence by an adjustment factor that reflects
test selectivity. As would be expected, the implied prevalence decreases in selectivity κ and
sensitivity π. Note also (again as expected) that the adjustment factor simplifies to ρ = γ/π
at κ = 1 (no selectivity would mean we estimate prevalence by prevalence in the tested
subpopulation), to ρ → τ γ/π as κ → ∞ (perfect targeting means we impute zero prevalence
in the untested population; compare (2.6)) and also, for the record, ρ → 1−τ +τ γ/π as κ → 0
(perfectly wrong targeting means we impute complete prevalence in the untested population).
Remark 2.2: While I present their cumulative impact, Assumptions 1 and 2 are in principle easily separated: The first one restricts the relation between test yield and prevalence
in the tested population, the second one restricts prevalence across tested and untested populations. Readers are encouraged to “pick and choose” and, of course, also to propose other
approaches. For example, sensitivity adjustment could be combined with MM’s suggestion
to restrict the rate of asymptomatic infections.

2.3

Bounds on the Negative Predictive Value

The negative predictive value NPV = Pr(C = 0|R = 0, T = 1) is the probability that
a negative test result is accurate. It is of great importance in medical decision making
(Eng and Bluemke, 2020; Manski, 2020; Watson et al., 2020). It can be bounded as follows:
[6]

Proposition 3: Suppose Assumption 1 holds. Then sharp bounds on the NPV are given
by




1 − γ/π 1 − γ/π
NPV ∈
,
.
1−γ
1−γ

(2.8)

Proof. Also for later use, denote the NPV as η := Pr(C = 0|R = 0, T = 1), then
η =
=
=

Pr(C = 0, R = 0|T = 1)
Pr(C = 0, R = 0|T = 1) + Pr(C = 1, R = 0|T = 1)
1 − Pr(C = 1|T = 1)
1 − Pr(C = 1|T = 1) + (1 − π) Pr(C = 1|T = 1)
1 − γ/π
1 − γ/π
=
,
1 − γ/π + (1 − π)γ/π
1−γ

where γ = π Pr(C = 1|T = 1) was used. This obviously decreases in π.
The result is easy but will be used momentarily. Note that the expression derived in the
proof has an easy intuition: The numerator is the fraction of true negatives, i.e. adjusting
yield by sensitivity, whereas the denominator is the proportion of measured negatives, in the
tested population. The result could again be easily generalized to also allow for specificity
of less than 1. In that case, there would also be nondegenerate bounds on the positive
predictive value Pr(C = 1|R = 1, T = 1), which equals 1 here because of the assumption of
perfect specificity.

2.4

Comparison to Bounds that Start from NPV

Assumption 1 contrasts with MM’s strategy of inputting ex ante bounds on the NPV. In each
case, bounds on the respective other quantity become an output of the model, so the direction
of logical inference is reversed. Notating the input bounds as η ∈ [η, η], MM establish that
Pr(C = 1) ∈ [τ (γ + (1 − γ)η), γ + (1 − γ)η].

(2.9)

I will add to this the observation that in conjunction with empirical test yield, prior bounds
on NPV restrict test sensitivity. Specifically, simple algebra building on Proposition 3 yields



γ
γ
π∈
,
.
1 − (1 − γ)η 1 − (1 − γ)η

(2.10)

The following are some methodological considerations as to why one might want to rather
start from sensitivity and selectivity.
• By bounding the NPV, one necessarily directly restricts prevalence in the tested population. This is because Pr(C = 1|T = 1) = γ + (1 − γ)(1 − η), so the lower and upper
bound on Pr(C = 1|T = 1) necessarily exceed the corresponding bound on (1 − η).

[7]

Since also the upper bound on overall prevalence is just the upper bound on prevalence
in the tested population, the effect can be large.
In addition, (2.10) reveals that bounds on NPV do (in conjunction with test yield, which
is observable) imply bounds on sensitivity. But because these are not made explicit, an
opportunity to check empirical plausbility of assumptions is missed.
To see how all of this can play out, consider MM’s prior bounds of [.6, .9] on the NPV.
With this input, no data can move the upper bound on prevalence below .4 and no
data –including a test yield of 0– can reduce the lower bound to 0. For example, if
test yield is .1, then prevalence in the tested population is bounded by [.19, .46]; the
upper bound of .46 also applies to overall prevalence; and test sensitivity is restricted
to [.22, .53], far below any plausible range of values. This example is stark but not
hypothetical; compare the first entry in Table 2 in MM, replicated in the first line of
Table 1 below. About half of the upper bounds in that table are below .5, so it is
important to understand that they cannot be below .4 by construction.2
In contrast, prior bounds on sensitivity do not directly restrict prevalence. This does
not mean that plausible bounds on the former are necessarily independent of the latter;
see the discussion after Assumption 1. However, one can assert bounds that are plausible over a wide range of prevalences, and not directly restricting the latter matters.
Indeed, because any value of NPV strictly below 1 bounds prevalence away from 0 by
assumption, I submit that in the early stages of a pandemic, any such bound runs the
risk of injecting “incredible certitude” (Manski, 2011). The above, implied bounds on
sensitivity are arguably a case in point.
• Inputting sensitivity (and possibly specificity) generates an interface with the literature on diagnostic tests because it is the focus of this literature. For a general example,
see Table 1 in Paules and Subbarao (2017). With regard to COVID-19, practitioners’ guides (Eng and Bluemke, 2020; Watson et al., 2020) emphasize the importance
of NPV for decisions but treat sensitivity and specificity as scientific input and NPV
as jointly determined by those and prevalence. The literature on diagnostic testing
(Arevalo-Rodriguez et al., 2020; Yang et al., 2020) is explicitly about sensitivity.
MM seem to disagree when they write: “Medical experts have been cited as believing
that the rate of false-negative test findings is at least 0.3. However, it is not clear
whether they have in mind one minus the NPV or one minus test sensitivity.” The
technical definition of false-negative rate is not in doubt, so the concern is about informal usage. This may be a valid point in general, especially as conflation of the two
2

That bounds on NPV presuppose bounds on prevalence is clearly expressed in Manski (2020), who reverses
the direction of logical inference and bounds the NPV of serological tests by inputting MM’s prevalence bounds
which, in turn, inputted assumed (albeit for PCR tests) NPV bounds.

[8]

corresponds to base-rate neglect, but it did not occur to me with regard to the literature
on COVID-19.3
• Asserting bounds on NPV without taking targeting of tests into account may ignore
constraining information that could lead to tighter bounds. Specifically, relatively low
values of the NPV (i.e., a large fraction of negative test results being false) will be more
plausible if one believes the test to be efficiently targeted. But in that same case one
would conclude that the constraint Pr(C = 1|T = 1) ≥ Pr(C = 1|T = 0) is far from
binding. Therefore, the degree of targeting informally enters NPV-based bounds twice,
in different directions, but derivation of (2.9) does not force the value to be the same
in both appearances. Assumption 2 is intended to allow for targeting of tests to affect
bounds in a disciplined manner.

3

Empirical Application

These bounds are mainly designed to process the information that is available early in a
pandemic. For this reason and to highlight some important differences, I illustrate them on
MM’s data, i.e. daily counts of tests, test results, and fatalities for Illinois, New York, and
Italy in March and April, and extend them only to early analysis of subsequent hot spots.
For credible application to prevalence data at a much later stage of a pandemic, one would
have to take into account multiple testing and other factors.
The leftmost columns of Tables 1-2 present bounds that set κ ∈ [1, ∞), that is, they only
restrict the direction of selectivity. I also restrict sensitivity to be in [.7, .95]. This is the same
sensitivity interval that was used by Frazier et al. (2020) in the analysis on which Cornell’s
Fall reopening plans were based and was supported by the medical literature at the time.4 For
comparison, the third column of Table 1 presents bounds that restrict NPV to be in [.6, .9] as
well as the direction of selectivity. These replicate MM’s Table 2.5 Table 2 extends these same
bounds to new data.6 To facilitate further comparisons, the tables also illustrate the bounds
on NPV implied by bounds on sensitivity (second column, corresponding to Proposition 3)
3

The footnote accompanying the cited sentence links to a news piece that attributes an estimated falsenegative rate of .3 to Yang et al. (2020). While the news piece has vague language, Yang et al. (2020) unambiguously estimates one minus sensitivity.
4
UCSF (2020) base medical advice on a point estimate of .8. Watson et al. (2020) give .7 as “lower end of
current estimates from systematic reviews.” Frazier et al. (2020) use a preferred point estimate of .9. Recall
in particular that the data analyzed here were overwhelmingly generated by testing of symptomatic subjects.
5
MM’s results were independently replicated from their original data. MATLAB code generating all tables
is available from the author. To keep the presentation succinct, the tables only show one day per week of data
and the last day.
MM refine these bounds by imposing time monotonicity; that is, prevalence (and therefore both bounds on
it) cannot decrease over time. I agree with that restriction and can provide tables that implement it. It is
dropped here solely because those tables have many identical rows.
6
The reader should keep in mind that MM might not have asserted the same bounds on NPV there, so the
bounds may not be what they would have proposed. On the other hand, it is a feature that one can (in this
author’s view) comfortably impose the same bounds on π across these contexts.

[9]

and the converse bounds if one starts from restricting NPV (last column, corresponding to
(2.10)).
The new upper bounds are considerably more restrictive and the lower bounds are slightly
less so; in sum, all bounds move down. For the first day of data in Illinois, the comparison is
between upper bounds of 13% versus 46%. The effect on lower bounds is less pronounced but
is not negligible in relative terms; for example, for Illinois on 3/23, rounding error obscures
that the NPV-based lower bound is 1.5 times the sensitivity-based one. The difference is also
substantively meaningful. The new bounds would rather clearly have ruled out speculation
of saturation being “around the corner” at the time. Consider also the implied bounds on the
infection fatality rate (i.e., fatalities divided by true infections; IFR henceforth). The most
informative NPV-based lower bounds (i.e., evaluated on 4/24) equal .0003 for Illinois, .0013
for New York, and .0010 for Italy. This is close to “flu-like” numbers that were the subject
of speculation at the time, and so it might have appeared that credible partial identification
analysis did not exclude such speculations. Yet it did: For the same data, the bounds from
Proposition 3 are .0005, .0016, and .0026; for Italy, the lower bound is above .001 starting on
3/29. In places where the data admittedly spoke very loudly, these numbers would have cast
strong doubts on “just the flu” conjectures in real time.
Tighter bounds are an unambiguous improvement only if assumptions did not become
less credible. I would indeed argue that credibility might, if anything, have improved. A
symptom of this is that the NPV-based bounds frequently imply sensitivity below .7, whereas
the sensitivity-based bounds imply NPV mostly close to, and frequently above, .9. The former
number seems out of step with expert opinion, including at the time, whereas the latter one
would not have raised any eyebrows.7 Also, Table 1 reveals that according to NPV-based
bounds, sensitivity increased in New York (the bounds fail to overlap). This might have
happened, but forcing it by assumption is arguably against the spirit of weak and credible
partial identification assumptions. I would argue that this illustrates methodological qualms,
notably the first bullet in Section 2.4. As discussed there, the implausibilities could be avoided
by relaxing prior bounds on NPV to [.6, 1]. However, the upper bounds would then just be
worst-case bounds, all bounds would be wider than the new ones in the present data, and
the intriguing feature that MM are able to exclude the crude case fatality rate (i.e., observed
fatalities divided by observed cases) as true IFR would be lost.
Table 2 repeats the exercise for data from subsequent hot spots of the pandemic.8 I delib7

As part of a recent partial identification analysis, Sacks et al. (2020) provide an empirically informed
NPV estimate for Indiana of .995. This comes with caveats: It corresponds to obviously lower prevalence
than in the data considered here, so that MM would presumably have inputted different NPV bounds; also,
it operationalizes NPV as test-retest validity. UCSF (2020) gives NPV as .972 for symptomatic and .998 for
asymptomatic cases in the Bay Area, though using the sort of point-identifying assumptions that we seek to
avoid here.
8
Test counts and results were retrieved from the COVID tracking project. State populations are U.S.
Census estimates for 7/1/19.

[10]

Date

New Bounds (κ = ∞)
Prevalence
NPV

NPV-based Bounds
Prevalence Sensitivity

Illinois

3/16
3/23
3/30
4/06
4/13
4/20
4/24

[0.000,0.131]
[0.000,0.186]
[0.000,0.237]
[0.001,0.279]
[0.002,0.297]
[0.003,0.303]
[0.003,0.299]

[0.957,0.995]
[0.936,0.992]
[0.915,0.990]
[0.896,0.987]
[0.887,0.986]
[0.885,0.986]
[0.887,0.986]

[0.000,0.455]
[0.000,0.478]
[0.000,0.500]
[0.001,0.517]
[0.002,0.525]
[0.003,0.527]
[0.004,0.525]

[0.202,0.503]
[0.272,0.599]
[0.332,0.666]
[0.377,0.708]
[0.396,0.724]
[0.402,0.729]
[0.398,0.725]

New York

3/16
3/23
3/30
4/06
4/13
4/20
4/24

[0.000,0.191]
[0.001,0.400]
[0.004,0.527]
[0.007,0.583]
[0.011,0.579]
[0.013,0.554]
[0.015,0.519]

[0.934,0.992]
[0.833,0.980]
[0.749,0.969]
[0.705,0.964]
[0.708,0.964]
[0.728,0.967]
[0.756,0.970]

[0.000,0.480]
[0.002,0.568]
[0.005,0.621]
[0.008,0.645]
[0.012,0.643]
[0.015,0.633]
[0.017,0.618]

[0.279,0.607]
[0.493,0.795]
[0.594,0.854]
[0.633,0.873]
[0.630,0.872]
[0.613,0.864]
[0.588,0.851]

Italy

3/16
3/23
3/30
4/06
4/13
4/20
4/24

[0.000,0.290]
[0.001,0.331]
[0.002,0.304]
[0.002,0.263]
[0.003,0.217]
[0.003,0.186]
[0.003,0.169]

[0.891,0.987]
[0.871,0.984]
[0.884,0.986]
[0.903,0.988]
[0.923,0.991]
[0.936,0.992]
[0.943,0.993]

[0.001,0.522]
[0.002,0.539]
[0.002,0.528]
[0.003,0.510]
[0.004,0.491]
[0.005,0.478]
[0.006,0.471]

[0.389,0.718]
[0.430,0.751]
[0.404,0.730]
[0.361,0.693]
[0.309,0.642]
[0.272,0.599]
[0.251,0.572]

Table 1: Bounds from Proposition 2 (κ = ∞) and 3 assuming that sensitivity is bounded by
[.7, .95]. For comparison, the right-hand columns are bounds on prevalence and sensitivity
are generated by bounding negative predictive value to be in [.6, .9].

Arizona
California
Florida
Texas

Date

New Bounds (κ = ∞)
Prevalence
NPV

NPV-based Bounds
Prevalence Sensitivity

8/13
8/13
8/13
8/13

[0.028,0.258]
[0.016,0.090]
[0.027,0.193]
[0.019,0.173]

[0.038,0.508]
[0.037,0.438]
[0.043,0.481]
[0.031,0.473]

[0.906,0.988]
[0.971,0.996]
[0.933,0.992]
[0.941,0.993]

[0.355,0.687]
[0.143,0.401]
[0.281,0.610]
[0.257,0.580]

Table 2: Extension of Table 1 to U.S. pandemic hot spots later in 2020.

[11]

erately restrict attention to states with high test yield because it seems that MM calibrated
their input bounds to such places, and also to states that were in their first wave because
that is what the bounds are designed for. NPV-based bounds continue to allow for very high
prevalence but also force test sensitivity to be relatively low. Sensitivity-based upper bounds
are again at most half – often much less – than their NPV-based counterparts, and other
implications of respective bounds are roughly as before.
Table 3 shows the effect of increasingly restricting test selectivity through Assumption
2. Reading it from right to left (meant to evoke “outside in”), it starts with κ = 1, i.e.
test monotonicity, and progresses through arguably weak restrictions up to κ = 5, which is
restrictive and may be more in the spirit of a sensitivity parameter. Upper bounds respond
strongly. This is reflected in the implied lower bounds on the IFR; for Italy, these increase to
(in order) .0036, .0046, .0065, and .0102. Of course, these numbers should not be compared to
MM’s Table 2; to the contrary, MM reach similar conclusions when restricting the proportion
of asymptomatic infections. The same exercise but for current hot spots is displayed in Table
4.
The lower bounds are driven by the possibility that all true positives got tested. While
this paper focuses on upper bounds, one could also use Assumption 2 to refine lower bounds
away from that scenario. For the record, restricting κ ≤ 100 [κ ≤ 10] would refine the (last
period) lower bound on prevalence for Italy from .0034 to .0047 [.0325]. The upper bound on
IFR would be refined from .1278 to .0908 [.0132], a not completely vacuous restriction.
Inference on these bounds has at least two nonstandard aspects: As MM point out,
one might think of states and regions as populations of interest rather than samples from
meta-populations; at the very least, one might be interested in inference conditionally on the
realized population. In that sense, conventional sampling theory might not apply (and is
omitted in MM for that reason). However, whether a given subject is tested and the result
of that test are realizations of well-defined random variables, opening a clear avenue for
statistical inference. Separately, such inference might be complicated if the variables interact
(e.g., the marginal tested subject is asymptomatic) and will also involve small probabilities, so
that Central Limit Theorem-based approximations (including many forms of the bootstrap)
would not apply. Questions like this inform an exciting strand of current research (Rothe,
2020; Toulis, 2021). However, they are orthogonal to the thrust of this paper and also less
salient in the application because sample sizes are so large, and identified intervals so long,
that estimation uncertainty is dwarfed by identification issues.9
9

For a paper-and-pencil computation, approximate the distribution of (τ̂ , γ̂) as independently normal.
Then the distribution of estimated bounds follows easily. Because these bounds are ordered by construction,
inference would be a direct application of Imbens and Manski (2004) and in particular Stoye (2009, Lemma 3)
and would practically amount to intersecting one-sided confidence intervals. Simple calculations show that the
standard errors on (γ̂, τ̂ ) would be at least two orders of magnitude smaller than the estimators; consequently,
the difference between these confidence intervals and the estimated bounds is at most comparable to the tables’
rounding errors.

[12]

Date

Lower Bound
κ≥5

Upper Bound with
κ ≥ 3 κ ≥ 2 κ ≥ 1.5

κ≥1

Illinois

3/16
3/23
3/30
4/06
4/13
4/20
4/24

0.000
0.000
0.000
0.001
0.002
0.003
0.003

0.036
0.054
0.072
0.089
0.097
0.100
0.099

0.048
0.071
0.094
0.115
0.125
0.129
0.127

0.0700
0.102
0.135
0.162
0.175
0.180
0.177

0.092
0.132
0.172
0.205
0.220
0.226
0.222

0.131
0.186
0.237
0.279
0.297
0.303
0.299

New York

3/16
3/23
3/30
4/06
4/13
4/20
4/24

0.000
0.001
0.004
0.007
0.011
0.013
0.015

0.056
0.144
0.221
0.264
0.264
0.248
0.224

0.073
0.183
0.274
0.322
0.321
0.302
0.274

0.106
0.251
0.360
0.414
0.411
0.389
0.357

0.136
0.308
0.427
0.484
0.480
0.457
0.422

0.191
0.400
0.527
0.583
0.579
0.554
0.519

Italy

3/16
3/23
3/30
4/06
4/13
4/20
4/24

0.000
0.001
0.002
0.002
0.003
0.003
0.003

0.093
0.111
0.100
0.084
0.067
0.057
0.051

0.120
0.143
0.129
0.108
0.087
0.073
0.066

0.170
0.199
0.180
0.153
0.123
0.104
0.094

0.214
0.249
0.226
0.193
0.157
0.133
0.120

0.290
0.331
0.304
0.263
0.217
0.186
0.169

Table 3: Change in upper bounds from Table 1 as test selectivity is increasingly restricted.

Date
Arizona
California
Florida
Texas

8/13
8/13
8/13
8/13

Lower Bound
0.028
0.016
0.027
0.019

κ≥5

Upper Bound with
κ ≥ 3 κ ≥ 2 κ ≥ 1.5

κ≥1

0.093
0.036
0.074
0.060

0.126
0.046
0.097
0.081

0.258
0.090
0.193
0.173

0.164
0.057
0.123
0.106

0.198
0.068
0.148
0.130

Table 4: Change in upper bounds from Table 2 as test selectivity is increasingly restricted.

[13]

4

Conclusion

This paper proposes new methods to bound prevalence of a disease from partially identifying
data and assumptions. It is to some extent intended as “think piece” to alert researchers
to the possibly fruitful application of partial identification methods. I have no doubt that
domain knowledge may inform further, and better, iterations.
Options for refinement and subsequent analysis abound. Users who are comfortable with
injecting more prior information may also refine bounds by placing priors on unidentified parameters (Bollinger and van Hasselt, 2020). Users who find the present paper’s simple input
bounds too coarse but do not want to commit to a prior could also interpolate between these
approaches with sets of priors, i.e. in the spirit of Robust Bayesian Analysis (Ruggeri et al.,
2005). The analysis can also be used as input for decision recommendations. Here, a partially
identifying analysis will typically only partially identify optimal actions, and a point valued
decision may require to commit to a specific optimality criterion under ambiguity (Manski,
2000; Stoye, 2012).
The conceptual innovation is to think of test accuracy as (unknown, not necessarily constant, and possibly not even identifiable) technological parameter and of test selectivity as
something that econometric or epidemiological models can speak to. Bounds are therefore
constructed with these as starting points, deriving bounds on predictive values by implication
and not imposing any prior bound on prevalence in the tested population. In the empirical
application, it turns out that some of the more audacious speculations floated at the time
contradicted credible partial identification analysis even then. This illustrates the potential
utility of such analysis in early stages of a pandemic.
That said, many of this paper’s simplifications are a stretch in the current, more advanced
stage of the pandemic. For example, one should distinguish between current and past infection
and take multiple testing into account. I leave further exploration of such extensions to future
work, but would recommend to use restrictions on test sensitivity as primitive of the analysis.
Once again, my main hope is to stimulate further research on partial identification in, or in
collaboration with, epidemiology.

References
Arevalo-Rodriguez, I., D. Buitrago-Garcia, D. Simancas-Racines, P. ZambranoAchig, R. del Campo, A. Ciapponi, O. Sued, L. Martinez-Garcia, A. Rutjes,
N. Low, J. A. Perez-Molina, and J. Zamora (2020): “False-Negative Results of
Initial RT-PCR Assays for COVID-19: A Systematic Review,” PLoS One, 15.
Bollinger, C. R. and M. van Hasselt (2020): “Estimating the cumulative rate of SARSCoV-2 infection,” Economics Letters, 197, 109652.

[14]

Canning, D., K. Mahesh, D. Rashmi, C. Muqi, and D. Bloom (2020): “The association
between age, COVID-19 symptoms, and social distancing behavior in the United States,”
Working Paper. Harvard T.H. Chan School of Public Health, http://nrs.harvard.edu/urn3:HUL.InstRepos:42659941.
Eng, J. and D. A. Bluemke (2020): “Imaging Publications in the COVID-19 Pandemic:
Applying New Research Results to Clinical Practice,” Radiology, pMID: 32324100.
Frazier, P., S. Henderson, D. Shmoys, J. M. Cashore, N. Duan, A. Janmohamed,
J. Wan, and Y. Zhang (2020): “COVID-19 Mathematical Modeling for Cornell’s Fall
Semester,” Working Paper. Cornell University.
Imbens, G. W. and C. F. Manski (2004): “Confidence Intervals for Partially Identified
Parameters,” Econometrica, 72, 1845–1857.
Leeflang, M., P. Bossuyt, and L. Irwig (2008): “Diagnostic test accuracy may vary
with prevalence: implications for evidence-based diagnosis.” Journal of clinical epidemiology, 62, 5–12.
Manski, C. F. (1989): “Anatomy of the Selection Problem,” Journal of Human Resources,
24, 343–360.
——— (2000): “Identification problems and decisions under ambiguity: Empirical analysis of
treatment response and normative analysis of treatment choice,” Journal of Econometrics,
95, 415 – 442.
——— (2003): Partial Identification of Probability Distributions, Springer.
——— (2011): “Policy Analysis with Incredible Certitude,” The Economic Journal, 121,
F261–F289.
——— (2020): “Bounding the Accuracy of Diagnostic Tests, with Application to COVID-19
Antibody Tests,” Working Paper. Northwestern University.
Manski, C. F. and F. Molinari (2021): “Estimating the COVID-19 infection rate:
Anatomy of an inference problem,” Journal of Econometrics, 220, 181 – 192.
Mohammadi, A., E. Esmaeilzadeh, Y. Li, R. J. Bosch, and J. Z. Li (2020): “SARSCoV-2 detection in different respiratory sites: A systematic review and meta-analysis,”
EBioMedicine, 59, 102903.
Molinari, F. (2020): “Microeconometrics with Partial Identification,” Handbook of econometrics, forthcoming.
Paules, C. and K. Subbarao (2017): “Influenza,” The Lancet, 390.
[15]

Rosenbaum, P. R. (2002): Observational Studies, Springer.
Rothe, C. (2020): “Combining Population and Study Data for Inference on Event Rates,
with an Application to the Infection Fatality Rate of SARS-CoV-2,” Working Paper. University of Mannheim.
Ruggeri, F., D. Rı́os Insua, and J. Martı́n (2005): “Robust Bayesian Analysis,” in
Bayesian Thinking, ed. by D. Dey and C. Rao, Elsevier, vol. 25 of Handbook of Statistics,
623 – 667.
Sacks, D. W., N. Menachemi, P. Embi, and C. Wing (2020): “What can we learn
about SARS-CoV-2 prevalence from testing and hospital data?” Working Paper. Indiana
University, arxiv:2008.00298.
Stoye, J. (2009): “More on Confidence Intervals for Partially Identified Parameters,” Econometrica, 77, 1299–1315.
——— (2012): “New Perspectives on Statistical Decisions Under Ambiguity,” Annual Review
of Economics, 4, 257–282.
Toulis, P. (2021): “Estimation of Covid-19 prevalence from serology tests: A partial identification approach,” Journal of Econometrics, 220, 193 – 213.
UCSF (2020): “COVID-19 Diagnostic Testing,” UCSF Health Hospital Epidemiology and
Infection Prevention,

https://infectioncontrol.ucsfmedicalcenter.org/sites/g

/files/tkssra4681/f/COVID-19-testing-reference-sheet.pdf.
Watson, J., P. F. Whiting, and J. E. Brush (2020): “Interpreting a covid-19 test
result,” BMJ, 369.
Yang, Y., M. Yang, C. Shen, F. Wang, J. Yuan, J. Li, M. Zhang, Z. Wang, L. Xing,
J. Wei, L. Peng, G. Wong, H. Zheng, M. Liao, K. Feng, J. Li, Q. Yang, J. Zhao,
Z. Zhang, L. Liu, and Y. Liu (2020): “Evaluating the accuracy of different respiratory
specimens in the laboratory diagnosis and monitoring the viral shedding of 2019-nCoV
infections,” medRxiv.
Zhang, Z., Q. Bi, S. Fang, L. Wei, X. Wang, J. He, Y. Wu, X. Liu, W. Gao,
R. Zhang, W. Gong, Q. Su, A. S. Azman, J. Lessler, and X. Zou (2021): “Insight
into the practical performance of RT-PCR testing for SARS-CoV-2 using serological data:
a cohort study,” The Lancet Microbe.
Zhou, X., N. Obuchowski, and D. McClish (2002): Statistical Methods in Diagnostic
Medicine, Second Edition, Wiley.

[16]

