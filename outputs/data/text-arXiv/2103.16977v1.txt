Solving Heterogeneous General Equilibrium Economic Models
with Deep Reinforcement Learning
Preprint

arXiv:2103.16977v1 [econ.GN] 31 Mar 2021

Edward Hill 1 Marco Bardoscia 1 2 Arthur Turrell 1 3

Abstract
General equilibrium macroeconomic models are
a core tool used by policymakers to understand
a nation’s economy. They represent the economy as a collection of forward-looking actors
whose behaviours combine, possibly with stochastic effects, to determine global variables (such as
prices) in a dynamic equilibrium. However, standard semi-analytical techniques for solving these
models make it difficult to include the important
effects of heterogeneous economic actors. The
COVID-19 pandemic has further highlighted the
importance of heterogeneity, for example in age
and sector of employment, in macroeconomic outcomes and the need for models that can more
easily incorporate it. We use techniques from
reinforcement learning to solve such models incorporating heterogeneous agents in a way that
is simple, extensible, and computationally efficient. We demonstrate the method’s accuracy and
stability on a toy problem for which there is a
known analytical solution, its versatility by solving a general equilibrium problem that includes
global stochasticity, and its flexibility by solving
a combined macroeconomic and epidemiological
model to explore the economic and health implications of a pandemic. The latter successfully
captures plausible economic behaviours induced
by differential health risks by age.

1

Bank of England 2 University College London, Department of Computer Science 3 Data Science Campus, Office
for National Statistics. Correspondence to: Edward Hill
<ed.hill@bankofengland.co.uk>.
The views expressed are solely those of the authors, and
do not represent the views of the Bank of England (BoE) or Office
for National Statistics (ONS) or state the policies of the BoE or
ONS.

1. Introduction
One of the core problems in macroeconomics is to create
models that capture how the self-interested actions of individuals and firms combine to drive the aggregate behaviour
of the economy. These models can provide a guide for policymakers as to what actions they should take in any particular circumstance. Historically, macroeconomic models have
tended to be simple because of the need for interpretability,
but also because of a heavy reliance on solution methods
that are semi-analytical. Such methods allow for the solution of a wide range of important macroeconomic problems.
However, events such as the Great Financial Crisis and the
COVID-19 crisis have shown that the ability to solve more
general problems that include multiple, discrete agents and
complex state spaces is desirable. We propose a way to
use reinforcement learning to extend the frontier of what
is possible in macroeconomic modelling, both in terms of
the model assumptions that can be used and the ease with
which models can be changed.
Specifically, we show that reinforcement learning can solve
the ‘rational expectations equilibrium’ (REE) models that
are ubiquitous in macroeconomics where choice variables
are continuous and may have time-dependency, and where
there are global constraints that bind agents’ collective actions. Importantly, we show how to solve rational expectations equilibrium models with discrete heterogeneous agents
(rather than a continuum of agents or a single representative
agent).
We apply reinforcement learning to solve three REE models: precautionary saving; the interaction between a pandemic and the macroeconomy (an ‘epi-macro’ model), with
stochasticity in health statuses; and a macroeconomic model
which has global stochasticity, i.e. where the background
is changing in a way that the agents are unable to predict.
With these three models, we show that we can capture a
macroeconomy that has rational, forward-looking agents,
that is dynamic in time, that is stochastic, and that attains
‘general equilibrium’ between the supply and demand of
goods or services in different markets.

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

2. Background
Macroeconomic models seek to explain the behaviour of
economic variables such as wages, hours worked, prices,
investment, interest rates, the consumption of goods and
services, and more, depending on the level of complexity. They do this through ‘microfoundations’, that is describing the behaviour of individual agents and deriving
the system-wide behaviour based on how those atomic behaviours aggregate. An important class of these models
is used to describe how variables co-move in time when
supply and demand are balanced (in general equilibrium),
and when some variables are subject to stochastic noise (aka
‘shocks’). A typical macroeconomic rational expectations
model with general equilibrium is a representation of an
economy populated by households, firms, and public institutions (such as the government). The choices made by
these distinct agents are framed as a dynamic programming
problem in which households
maximise their discounted
P∞
future utility U = E t=1 β t u (st , at ) with u per-period
utility, β a discount factor, st ∈ S a vector of state variables,
at ∈ a(st ) a vector of choice variables, and st evolving as
st+1 = h(st , at ). E(·) represents an expectation operator,
usually assumed to be ‘rational’ in the sense of being the
households’ best possible forecast given the available information (and implying that any deviations from perfect
foresight are random). For household agents, u is monotonically increasing in consumption, ct , and decreasing in
hours worked, nt (both choice variables). Extra conditions
are imposed via other equations, for example, a budget
constraint of the form (1 + rt )bt−1 + wt nt ≥ pt ct + bt
with pt price, wt wages, and r the interest rate. bt captures savings, typically in the form of a risk-free bond or
other investment. If bt < 0 is permitted (i.e. debt) then
bt usually satisfies a ‘no Ponzi’ condition that rules out
unlimited borrowing and effectively imposes the rule that
bT = 0 (t ∈ 0, . . . , T ). Consumers take prices, wages,
and interest rate as given; these are state variables. Firms
maximise profits Πt = pt Yt − wt Nt (possibly including a
−rt Kt term if savings are invested) subject to a ‘production
constraint’, Yt , that turns labour, Nt , and capital, Kt into
consumption goods. Typically, Yt = At f (Kt , Nt ) where
f is a monotonically increasing function of its inputs and
At is either predetermined or follows a log-autoregressive
process ln At = ρA ln At−1 + t ; t ∼ N (0, σA ) is known
as a technology ‘shock’. Governments perform functions
such as the collection and redistribution of taxes. Firms are
assumed to be perfectly competitive, meaning that each firm
takes prices and wages as given.
Prices, wages, and interest rates are determined by market
clearing for goods, labour, and savings respectively in which
supply and demand are balanced in each market. These
‘general equilibrium’ conditions bind agents and the environment together, and are atypical in reinforcement learning.

The competitive equilibrium is defined by a vector of state
variables, and by consumption and production plans for the
agents that maximise utility. Often, the optimal policies
of all agents are solved analytically by Lagrangian methods: the equilibrium conditions are substituted in and the
system of equations simplified, usually by log-linearising
the model around an assumed steady state. We now review some macroeconomic models before briefly discussing
multi-agent models more generally.
The Representative agent with rational expectations is an
important class of macroeconomic model, most well-known
is the representative agent dynamic stochastic general equilibrium (DSGE) model. The canonical model is the representative agent New Keynesian (RANK) model (Smets &
Wouters, 2007). Continuum rational expectations models
overcome some of the heterogeneity-related shortcomings of
those models by replacing the representative household with
a continuum of households that are ex ante differentiated by
their assets and labour productivity. The canonical example
is the heterogeneous agent New Keynesian (HANK) model
(Kaplan et al., 2018). Macroeconomic agent-based models
differ in that they simulate agents as discrete entities but also
typically make very different assumptions to, say, RANK or
HANK models; the most important being that they tend not
to assume rational expectations/perfect foresight and they
may not necessarily have competitive markets. Importantly,
they allow for heterogeneity in multiple dimensions simultaneously (Haldane & Turrell, 2019). Agent-based models
(ABMs) are also extensively used in epidemiology (Tracy
et al., 2018), sometimes under the name ‘individual-based
models’. At the start of the coronavirus crisis, UK government policy was heavily informed by such models, most
notably that of Ferguson et al. (2020), and there are several
ABMs modelling the coronavirus pandemic (Hoertel et al.,
2020; Kerr et al., 2020). These epi-ABMs do not capture
economic effects.
Epi-macro models attempt to combine macroeconomic and
epidemiological effects, and their interaction. The canonical
examples combining epidemiology and a REE representative agent model are Eichenbaum et al. (2020a) and Eichenbaum et al. (2020b) who link the two by assuming that, in
addition to the usual Susceptible, Infected, Recovered (SIR)
model transmission mechanism as posed by Kermack &
McKendrick (1927), a household agent may be infected at
work or while engaging in consumption. Market clearing
is also assumed. Building on many of the same assumptions as HANK, the canonical continuum agent epi-macro
model with REE is by Kaplan et al. (2020). Agents are
differentiated by their assets, productivity, occupation, and
health status. There are three types of good: regular, social,
and home-produced; and three types of work: workplace,
remote, and home. The epi-macro link is achieved through
a transmissibility of infection that is modified to include

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

terms proportional to hours worked and amount consumed,
with avoidance of infection captured through a disutility of
death. Market clearing is assumed.
Finally, recent work has seen reinforcement learning be
applied to multi-agent systems of relevance to economics in
the case of bidding in auctions under constraints (Feng et al.,
2018; Dütting et al., 2019), and deciding on behaviours
for both agents and a social planner in a gather-and-build
economy (Zheng et al., 2020).

allow θ (the utility of work) to vary, and introduce four
discrete states, indexed by d, that the household can be in
at any given time. While making no economic difference,
the expansion of the action and state spaces means that
the hyperparameters we find are transferable to the more
realistic problems we will come to. It also allows us to test
that training the parameters of a single network to provide
the value function for any agent is a good approximation
to training a network for each agent individually, which is
significantly more computationally expensive.

In the rest of this paper, we show how to use reinforcement
learning to solve typical rational expectations macroeconomic models while also incorporating discrete agent heterogeneity and, potentially, stochasticity; demonstrating that
all three can be combined is by far our major contribution
and has applications for a wide class of economic problems.

The analytical solution for the consumption and hours paths
for the household are given by ct = λ−1 β t /I and nt =
wt θ−1 λβ −t where λ is a Lagrange multiplier determined
from the no Ponzi condition. Computationally, we start
from the Bellman equation:

3. Model and Experiments

U (t, s, S) =

max ut (s, S, a) + β

3.1. Precautionary Savings
A typical rational expectations equilibrium problem is that
of precautionary saving, in which agents anticipate a change
in circumstances that will adversely affect their utilities, in
this case a reduction in wages, and respond in advance
in order to smooth their consumption. Such behaviour is
typical of the agents in a REE model. The simplest version
of this problem has a known analytical solution. We solve
this model using reinforcement learning so that we may
compare it to the analytical solution, and we also use it as
a way to demonstrate many of the challenges of using RL
for this class of problems; notably the speed and accuracy
of convergence given the sensitivity to the estimate of the
value function; the continuous action and state spaces; and
the enforcement of the ‘no Ponzi’ condition.
3.1.1. M ODEL
We assume that there is a single household agent with rational expectations. There are I = 2 firms, with the firms and
the good each firm produces indexed by i. The household
agent is employed by one of these firms,
P which we will
denote e, and has per period utility ut = i∈I ln cit − θ2 n2t
with action (choice variables) cit consumption and nt hours
worked. 0 ≤ t < T is the discrete timestep. The price
vector is fixed to pit = 1, and the interest rate is fixed
to 0. The wage is imposed as w = 1 for t < T /2
and w = 0.5 afterwards, a fall that is anticipated. The
household agent is subject
P to a budget constraint such that
bt+1 = bt + wt nt − i∈I pit cit . The no Ponzi condition
is imposed via bT = 0, which prevents unlimited borrowing by P
the household. The agent maximises its discounted
utility 0≤t<T β t ut with β = 0.97 and T = 20.
We use I = 2 firms rather than a single representative firm,

a


E

st+1 ,St+1 |a,s,S

U (t + 1, st+1 , St+1 )
(1)

where s is the agent’s state, a is the action vector. E is
the expectation operator, and S is the global state (which
includes t, but we make t explicit for clarity). For the current problem,
we drop the global state to obtain U (t, s) =

maxa ut (s, a) + β Est+1 |a,s U (t + 1, st+1 ) .
The optimal action vector under local and global constraints,
a∗t (s, S), is computed using the method of Lagrange multipliers; this requires accurate values of the gradient of U
with respect to the state variables.
We use a deep neural network to approximate U (t, s) =
U (t, s = (e, θ, d; bt )); however, direct approximation is
problematic because ∂s U (t, s) is slow to converge and is
highly sensitive to initialisation and hyperparameter choice.
To mitigate this, we find D(t, s) = ∂s U (t, s) explicitly by
solving
D(t, s) = ∂s ut (s, a∗ )
X
+β
∂s P(a∗ (s) → st+1 |s, a∗ )U (t + 1, st+1 )
st+1

+β

E

st+1 |s,a∗

∂s (st+1 )D(t + 1, st+1 ) (2)

That a directly learnt estimate of the ∂s U (t, s) aids stability
and convergence has been noted in both deterministic (Balduzzi & Ghifary, 2015) and stochastic (Heess et al., 2015)
continuous control problems.
In the case of precautionary saving, ∂s P(s → st+1 |s, a∗ ) =
0 and P(s → st+1 |s, a∗ ) = δsst+1 , (with δ the Krot+1
necker delta) so that D(t, s) = ∂s ut (s, a∗ ) + β ∂s∂s
D(t +
∗
1, a (s)). The values of ∂a U (t + 1, st+1 ) that are required
t+1
for finding a∗ are then written as ∂s∂a
D(t + 1, st+1 ). U
and D need not be consistent.

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

3.1.2. S PECIFICATION
System All timings use a laptop with a 4-core CPU (an i76700HQ) without GPU acceleration. The code is written
in P YTHON 3 using P Y T ORCH (Paszke et al., 2019) for the
neural network.
Neural Network and Training The networks for U and D
are identical with 5 layers of 50 Softsign neurons, followed
by a single linear layer. The inputs are normalised to the
typical scales in the problem, for example t → (2t − 1)/2T ,
and the outputs of each network are normalised to the scale
of the problem by an additive and multiplicative factor. The
networks are wrapped in a caching and linear interpolation
function to reduce network evaluations.
The networks are trained using the Adam optimiser (Kingma
& Ba, 2014) with a decaying learning rate over epochs,
E of lr = max(5 × 10−3 × 0.8−E , 10−5 ). Each epoch
contains 160 experiences of U (t, s) and D(t, s), which are
trained with a replay buffer (Mnih et al., 2015). Initially the
buffer is emptied after each epoch, but once lr ≤ 10−4 it
retains a fraction of its contents. This provides swift initial
learning followed by good coverage of experiences later in
the process (Fedus et al., 2020).
The experiences are created using n = 4-step learning (e.g.
Sutton & Barto, 2018), recorded by running an agent forward taking its current optimal actions. These are run independently, in parallel. n/(n + 1) of forward runs apply a
perturbation to the state every n steps, allowing for exploitation and exploration. In early epochs, where the predicted
solution is less accurate, Double DQN (Van Hasselt et al.,
2016) and target clipping (Schulman et al., 2017) provide
stability.
3.1.3. R ESULTS
We assess the goodness of the model by defining its error as
the mean absolute fractional difference between the analytic
consumption from the equations above and the simulated
consumption for an agent beginning at t = 0 with b0 = 0 for
a number of values of θ. This is a stringent and appropriate
test of the model since the value of consumption at each
timestep depends on current savings, which are themselves
determined by the time-histories of n and ci . Note that this
means errors in the observable values compound over time,
as they will in similar models in later sections. The model
successfully converges to the analytical solution with error
of ≈ 0.01 in ≈ 25 epochs, with each epoch containing 160
experiences. This takes ≈ 2 minutes, about 1 GFLOPs-hour.

3.2. A General Equilibrium Rational Expectations
Epi-Macro Model with Stochasticity in Agents’
States
We now build on the previous example to demonstrate the
solution of a rational expectations general equilibrium epimacro model, with SIRD health states. We find the ‘decentralised equilibrium’ in which each household agent behaves
optimally according to its choice variables. Agents are motivated to change their behaviour due to fear of dying from
the disease and, because consumption carries with it a risk
of contracting the disease, their patterns of consumption
change, in turn altering their risk of infection – this risk
therefore connects the macroeconomic and epidemiological aspects of the model. These consumption changes are
differentiated by age as there is an exponentially increasing
risk of death according to age once infected.
3.2.1. T HE MODEL
The model combines features of both agent-based macro
models and rational expectations models. Agents are rational, forward-looking, and discrete. Let household agents be
indexed by j, while sectors (the analogue of firms), and the
goods that each sector produces, are indexed by i. Household are ex ante differentiated by their age and the sector that
employs them. Let Ei be the set of households employed
by sector i.
The model is a real business cycle (RBC) model (Kydland & Prescott, 1982), with no technology shock. In
each period household agents engage in consumption, c,
and work, captured by hours worked, n. Time-t utility
of household
infected, or recovered
P j when susceptible,
1
2
is uj =
ln
c
−
where
θ = 1 is a disutility
θn
ji
j
i
2
of working. Households face a time-t budget constraint
balancing their income from work and interest on the capital they have loaned to industry, against their consumption
P and investment in new capital, vj : wj nj + rkj,t =
i pi cji + vj,t , with kj,t+1 = kj,t + vj,t . Sector i produces
a quantity of goods Yi using household labour such that
Yi = ANiα Ki1−α where Ni are the total hours worked in
sector i and Ki = Ke + K̂i is its total capital. Ke is an
initial endowment of capital and K̂i is that derived from
consumers’ investments. We set A = 1, α = 2/3. Sectors
are profit-maximising so ∂Ni Πi = 0 and ∂Ki Πi = 0 where
the profit Πi = pi Yi − wi Ni − (r + δ)Ki , with δ the depreciation rate of capital. For our form of Yi this implies
Πi = 0. All consumers and firms take pi , wi and r as given,
and
P these are adjusted to clear the markets
P for hours worked:
P
g
n
=
N
∀i
and
capital:
i
j∈Ei j j
j gj kj =
i K̂i ,
−1
where agent weights gj = J ∀j. The utility of death is
−200, and the discount rate is β = 0.97. See Appendix I
for a more detailed description.
This type of model, which is common, is used only to

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

demonstrate the approach; the solution method is applicable to a wide range of models. Also, note that there is
no need for linearisation around a steady-state, which is a
common solution technique for models of this type.
Household agents have four possible health states: susceptible, infected, recovered, and deceased (SIRD), with
the probability of transition between states given by, e.g.,
P(S → I) for going from susceptible to infected. In what
follows, epidemiological parameters are noted with tildes.
At each time-step (a day),
X
cji
X ρ̃i cji Infected j
P
Pj (S → I) = β̃
cji,t=0
j cji
i
so that there is a ‘shopping risk’ of acquiring the infection if
many infected are consuming the same goods.
P ρ̃i is a vector
of relative consumption risks such that i ρ̃i = 1. P(I →
˜ for each household j
R) = γ̃, and Pj (I → D) = ξ(j)
˜
where ξ(j) is an exponentially increasing function of age
rising from 0.006 at age 40 through 0.024 at 55 to 0.165 for
a 70 year old, then flattening at age 80. β̃ = 0.56, and γ̃ =
0.2 from Lin et al. (2020). The relative risk of consuming
each sector’s product is ρ̃i = {0.8, 1.2}/2, and we refer
to these as ‘remote’ and ‘social’ consumption respectively.
At time t = 0, kj = 15 = Ke /2 ∀j, and at time t = 2
we infect the youngest 10% of the population. On death,
any investments are redistributed across living agents and
deceased agents are not replaced. We use J = 100 agents,
with a distribution of ages given by Age(j) ∼ U(20, 95).
Households are evenly distributed across employers, and
cannot change employer.
Aside from using a reasonable distribution of the death rates,
this model is entirely uncalibrated.
3.2.2. S OLVING THE MODEL WITH MULTIPLE AGENTS
We run a number of simulations indexed by τ . Solving
the model means finding a history Hτ =∞ = {St }t∈0,...,T
and agent behaviours Uτ =∞ (t, kt , St ) that are consistent.
We begin with Hτ =0 and Uτ =0 . We use H̄τ , an average
created from {Hτ 0 }τ 0 ≤τ to re-train Uτ , obtaining Uτ +1 . A
multi-agent simulation is run with agent behaviours governed by Uτ +1 to obtain Hτ +1 in general equilibrium. As is
standard in iterative methods, we terminate at a sufficiently
large τmax in order to provide a good approximation to the
values at τ = ∞; the results we present use τmax = 50. In
the limit of large J, each household’s behaviour makes no
difference to the system, so despite the stochastic transitions
of health statuses, we are able to obtain a unique history
of the variables characterising the global system (including
the I = 2 sectors). The observed Ĥτ can be seen as noisy
¯
observations of that Hτ , and Ĥτ as an estimate of H̄τ where
τ →∞
¯
the averaging is chosen such that H̄τ −−−−→ Hτ and Ĥτ

has significantly less noise than Ĥτ . We use the average
0
of {Hτ 0 }τ 0 ≤τ weighted by γ τ −τ with γ ≤ 1. While in
general Hτ is therefore produced solely as a function of
Uτ −1 , we modify this general scheme
P for this specific
P case
by providing the infection fraction Infected j cji / j cji in
multi-agent simulation τ from H̄τ −1 . This counteracts the
propagation of the high levels of noise created by the initiation of the pandemic through to later times of the simulation,
and could be avoided by using a larger number of agents.
At each timestep, we iteratively solve to find the values of the prices (pi , wi , r) for which the markets
for labour and capital clear by gradient descent using
scipy.optimize.least squares with the default
parameters, initialised from the previous timestep, for t > 0.
We then advance both the capital and epidemiological state
to proceed to the next timestep.
3.2.3. R E - TRAINING AGENT BEHAVIOURS
Uτ +1 is obtained from Uτ by continuing training using H̄τ .
The network parameters are the same as in §3.1, however
we use a gentler decay of the learning rate. There are no
problems observed with convergence, and the adherence to
the no Ponzi condition is a good test of this, since achieving
it is sensitive to the entire time history of the simulation.
As in the multi-agent model, consumers take prices (pi ,wi
and r) as given, find their optimal consumptions, hours
worked and investments before advancing their state using
the budget constraint and the probabilities of their SIRD
state changing.
3.2.4. R ESULTS
We examine two cases: a ‘heterogeneous’ case as described
above, and a ‘homogeneous’ case without age heterogeneity
but with the same mean death rate.
Figure 1 shows the percentages of susceptible, infected,
recovered and deceased agents as the pandemic progresses.
Each line is an average over the results of 3 simulations,
and each simulation’s result is an average over 20 histories.
As can be seen from the 95% confidence intervals in the
figure, the behaviour is similar across simulations. In the
homogeneous-age case, more people are infected over the
course of the pandemic, but there are fewer deaths in total.
Figure 2 shows the agents’ consumptions. We bin the uniform age distribution into young (< 40), old (> 70) and
middle-age groups; we find considerable differences in behaviour between them. After consuming the most before
the pandemic since they anticipate the coming opportunity
to save, the old strongly reduce consumption in response to
infection risk, and reduce consumption of the riskier ‘social’
good more. The young, conversely, are unlikely to die and
so their consumption is relatively unchanged, governed by

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

Homogeneous
Heterogeneous

Percent of agents

80
60

Recovered

40
Deceased

20
0

Infected

0

10

20
Simulation Time

30

Age Group
Under 40
40 to 70
70+

15

Age distribution

40

10
5
0

Peak of infections

Susceptible

Investment deviation from global average

100

5
10
15
20
0

5

10

15
20
25
Simulation Time

30

35

40

Figure 3. Average investments of living agents binned into three
age groups. The vertical line shows the time of peak infections.

Figure 1. Percentage of agents who are susceptible, infected, recovered, or deceased as the simulation progresses.

Age Group = Under 40

Age Group = 40 to 70

Remote
Social

Consumption per living agent

8

Age Group = 70+

6
4
2

0

10

20

Simulation Time

30

0

10

20

Simulation Time

30

0

10

20

Simulation Time

30

and has shown consistent stability and convergence. This
exercise has also shown the sensitivity of the model’s conclusions to those parameters, emphasising the importance
of calibration in all aspects of the model if it were used as
more than a test case of the methodology.
3.2.5. S PECIFICATION

Figure 2. Average consumption of living agents from the ‘remote’
and ‘social’ sectors binned into three age groups.

the decrease in the size of the economy.
Figure 3 shows the mean investments per agent in each age
group, normalised to the salary (product of wage and hours
worked) of an agent in the same system with no pandemic
and no saving. The young anticipate the pandemic and save
before it in order to spend when infection rates are higher,
with the converse behaviour for the old. It also shows the
no Ponzi condition holds with good accuracy for all age
groups.
Finally, Figure 4 shows the total consumption for both
heterogeneous-age and homogeneous-age cases. The inclusion of distributional effects causes significant changes
to bulk macroeconomic quantities.
Together these results show that in this uncalibrated model
the inclusion of age heterogeneity makes a substantial difference to both the epidemiological and economic progress of a
pandemic. This model and solution method has been tested
with a range of epidemiological and economic parameters

The hardware, software, and parameters are identical to
§3.1 with the exception of the learning rate decay which
is slower here to allow for the longer time history. Scaling
is linear with J, since the number of iterations of the least
squares optimisation seems to scale very weakly with J for
this problem. Each history calculation followed by an RL
update takes ∼ 30 minutes on the reference machine, so a
single simulation takes ∼ 24 hours, ∼ 720 GFLOPs-hour.
3.3. Stochasticity of global variables: A toy wage-shock
problem
In the previous section, only the agents’ state was stochastic
and, because of being in the limiting case of large numbers
of agents, each individual agent’s state did not affect the
global state. We now return to the model in §3.1, but instead
of having a deterministic drop in wages, wages now follow
a log-autoregressive stochastic process.
3.3.1. T HE MODEL
We return to the Bellman equation, (1). We assume there
is no stochasticity in health states so the Es0 |s,a (·) are no
longer present, however the ESt+1 |S (·) remain.

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

7

work, θ ∈ [0.6, 1.4], and employer, e; and dynamically
heterogeneous in savings. Again, the multiple employers
and the treatment of heterogeneity in θ are introduced so that
demonstrations of convergence and hyperparameter choice
are relevant to the problem in the next section.

Age distribution
Homogeneous
Heterogeneous

Consumption per living agent

6
5
4

Social
Social

3

Remote
Remote

2

0

10

20

Simulation Time

30

Figure 4. A comparison of total consumption per living agent by
sector in two separate simulations with agents who are heterogeneous or homogeneous in age.

U (t, s, S) =

max ut (s, S, a) + β
a

E

St+1 |S


U (t + 1, st+1 , St+1 )

(3)

where st+1 = a(s) advances deterministically. We change
the method to solve for Ũ and D̃, defining
Ũ (t, S, st+1 ) = β

U (t + 1, St+1 , st+1 )

(4)

n
o
U (t, S, s) = max ut (S, s, a) + β Ũ (t, S, st+1 )

(5)

E

St+1 |S

and
a

where the value of a which attains the maximum defines a∗ .
Again, we will find the maximum using Lagrange’s method
and the auxiliary quantity D̃,
D̃(t, S, st+1 ) =

E

St+1 |S

[D(t + 1, St+1 , st+1 )]

(6)

and so
∂a L(t, S, s, a) = ∂a ut (S, s, a)
+ β(∂a st+1 )D̃(t + 1, S, st+1 ) (7)
As is standard in reinforcement learning, the expectations
are approximated by using a large number of global state
histories to update Ũ and D̃.
Excepting the wage history, the set-up is as in §3.1. The
agents are statically heterogeneous in their propensity to

In this toy problem, we compare two types of agent: one
a current-time wage-observing agent, whose future utility is a function of the wages at the current timestep, wt :
U (t, S = (t, wt ), s = (θ, e, bt )); the other a non-wageobserving agent with U (t, S = (t), s = (θ, e, bt )). Both
are able to optimise by testing how their strategies play out
within the histories they have seen, however they only have
partial visibility of the global state. In the previous case
that had deterministic global state, a knowledge of the time
determined all other global variables, but here the mapping
from observable values to global state is one-to-many.
The wage follows a log-autoregressive process, ln wt =
ρw ln wt−1 + t ; t ∼ N (0, σw ); where we use ρw = 0.97,
σw = 0.1, and wt=0 = 1. Since the wage is autoregressive, knowledge of the current wage adds information about the wage in the future. The agent is trained
on #T ∈ {100, 1000} training histories.
We parametrise wage histories by their mean absolute
fractional deviation, dh , from the mean path of the auto0
σ2 P
regressive process wmean,t = exp 2w t0 <t ρ2t
w . Of the
50 test histories, which have dh ∈ [0.07, 0.55], we consider
the 27 with dh > 0.2 to represent those with significant
deviation from the mean path.
We judge the success of this model by calculating the average total utility an agent with θ = 1 attains over these
previously unseen wage histories, {wh,t }0≤t<T , where the
average over histories with dh > x is denoted ūdh >x . Table 1 compares the average utilities of agents with different training setups and wage visibility to an analytic approximation found by defining the action at time t in history h to be the values obtained from the formulae in §3.1
for a wage history beginning at time t: ch,t = 1/λ̄ and
nh,t = wh,t θ−1 λ̄, obtaining λ̄ from thenno Ponzi condio
P
P
0
0
2
tion as I −1 t0 ≥t β t −t = λ̄2 θ−1 t0 ≥t β t−t Ep wp,t
.
0
2
Ep wp,t
0 is evaluated analytically over all possible wage
paths that have wp,t = wh,t using the second moment of
the log-normal distribution.

We implement prioritised experience replay (Schaul et al.,
2015) by retaining experiences with a larger error for a
larger number of training epochs. This increases the agents’
performance relative to a base agent trained as in previous
sections, particularly on the dh > 0.2 histories that deviate
significantly from the mean path. This is expected since
the training examples are sparser and more varied at higher
dh . The wage-observing agent outperforms the non-wage-

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning
Table 1. Total utilities achieved by different agents relative to the
analytic approximation for different visibility of the wage, numbers
of training histories #T and including prioritised experience replay
(PER).
Agent

#T

ūdh ≥0

ūdh >0.2

Wage-observing + PER
Wage-observing
Wage-observing + PER
Analytical approximation
Non-wage-observing + PER

1000
1000
100
1000

+0.97
+0.54
+0.27
0.0
-0.39

+1.21
+0.29
+0.17
0.0
-1.43

Technology shock

2
1

0

10

0

10

3.3.2. S PECIFICATION
The specification is identical to §3.1.2 except that the rate
of decay of the learning rate is decreased to allow averaging,
lr = e−0.01E /(1 + E) for epoch E; and, as discussed,
prioritised experience replay is used.
3.4. Stochasticity of global variables: A general
equilibrium with stochastic technology shocks
Finally we demonstrate a general equilibrium model that has
stochastic global variables, specifically a log-autoregressive
process in the technology. This is given by ln Ait =
ρA ln Ai,t−1 + t ; t ∼ N (0, σA ) and affects the produc1−α α
tion, given by Yit = Ait Kit
Nit , of a sector. α = 2/3,
ρA = 0.97, σA = 0.1, and Ai0 = 1. The agents are as in
§3.3, being statically heterogeneous in propensity to work,
θ and employer e, and dynamically heterogeneous in investment, kt ; however now have visibility of all prices, not
just wages. The global model that couples the agents is
the same RBC model described in §3.2 and Appendix I,
but without infections. As the agents’ internal states are no
longer stochastic, a smaller number (J = 10) of agents can
be used.
As in §3.3, we compare two types of agents, one that ‘sees’
realisations of the prices and another than does not. Since
differing values of the technology shock move the general
equilibrium, changing the prices, observing them gives the
agent information about the state of the underlying stochastic process. Figure 5 shows the paths of hours worked,
nj , for 5 agents with a range of values of θ, all of whom
work in the same sector given by i = 0. As expected,

1

0

10

10

1.0

0

10

1.0

0

10

0

10
Simulation Time

1.5

2
1

0

1.5

2

observing agent. In addition to the solution becoming stable
and the no Ponzi condition being satisfied, that the utilities
for the wage-observing agent are consistently higher than
those for the analytical approximation gives us confidence
that the answers converged to are accurate. We record the
utilities after 100 epochs which equates to 40, 000 experiences or 10 minutes on the reference machine. A degradation in performance is seen if an insufficient number of
training histories are used.

1.0
1.5

2
1

Hours worked

1.5

0

10
Simulation Time

1.0

Figure 5. Each row represents one of 4 randomly chosen simulations. Left column The technology shock for sectors 0 (black,
solid) and 1 (red, dashed). Right column Paths of hours worked for
agents who can (orange, dashed) and cannot (blue, solid) observe
realisation of prices when choosing their action; all agents work
for sector 0 and have θ = {0.76, 0.88, 1.0, 1.12, 1.24} from top
to bottom.

the paths of agents who can observe realisations of prices
have smaller fluctuations, which is also true of the paths
of other quantities. Averaging over 256 runs, the mean unsigned curvature of the paths drops from κ̄non−obs = 0.44
to κ̄obs = 0.27; this fall is also true of the curves in the
figure where κ̄non−obs = 0.42 and κ̄obs = 0.19. This
difference is because agents who can observe realisations
are better able to adjust their behaviour to the current and
(since it is an autoregressive process) future values of the
technology shock.
3.4.1. S PECIFICATION
The specification of the neural network and learning remains
unchanged from the previous section. Calculation of the
multiple histories is parallelised; we use 8 threads. For each
simulation epoch E, 8(4 + E) histories are found, followed
by 20 RL training epochs. In total, there are 12 simulation
epochs and a total of ≈1000 histories and 240 RL training
epochs, each of which samples from the most recent 50%
of the histories. The number of histories and epochs is
informed by the convergence properties from the previous
section. A total of ≈100, 000 experiences are recorded
during the whole simulation which takes ≈6 hours.

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

4. Discussion and Conclusion
This work shows that reinforcement learning can be used
to solve a wide range of important macroeconomic rational expectations models in a way that is simple, flexible,
and computationally tractable. Furthermore, these methods can be immediately applied to previously intractable
problems with multiple degrees of discrete heterogeneity
and stochasticity. Being highly relevant to real world phenomena, such as climate change and disease transmission,
these capabilities are of great value to policymakers and can
be developed into serious tools to aid decision making in
complex scenarios.
Finally, by linking to reinforcement learning, this work
provides the potential to apply its extensive toolkit of techniques, many of which have direct relevance to economic
questions: examples include accessing larger state and action spaces (e.g. Lillicrap et al., 2015), including bounded
rationality, or applying inverse reinforcement learning to
deduce agents’ objectives and rewards from observed microand macro-economic behaviours. Additionally, we can harness improvements in implementation such as GPU/TPU
acceleration (Paszke et al., 2019) and distributed computing
(Mnih et al., 2016).

Acknowledgements
We would like to thank Federico Di Pace for useful discussions.

References
Balduzzi, D. and Ghifary, M. Compatible value gradients
for reinforcement learning of continuous deep policies.
arXiv preprint arXiv:1509.03005, 2015.
Dütting, P., Feng, Z., Narasimhan, H., Parkes, D., and Ravindranath, S. S. Optimal auctions through deep learning.
In International Conference on Machine Learning, pp.
1706–1715. PMLR, 2019.
Eichenbaum, M. S., Rebelo, S., and Trabandt, M. The
macroeconomics of epidemics. Working Paper 26882,
National Bureau of Economic Research, March 2020a.
Eichenbaum, M. S., Rebelo, S., and Trabandt, M. Epidemics
in the neoclassical and new keynesian models. Working
Paper 27430, National Bureau of Economic Research,
June 2020b.
Fedus, W., Ramachandran, P., Agarwal, R., Bengio, Y.,
Larochelle, H., Rowland, M., and Dabney, W. Revisiting
fundamentals of experience replay. In International Conference on Machine Learning, pp. 3061–3071. PMLR,
2020.

Feng, Z., Narasimhan, H., and Parkes, D. C. Deep learning
for revenue-optimal auctions with budgets. In Proceedings of the 17th International Conference on Autonomous
Agents and Multiagent Systems, pp. 354–362, 2018.
Ferguson, N., Laydon, D., Nedjati-Gilani, G., Imai, N.,
Ainslie, K., Baguelin, M., Bhatia, S., Boonyasiri, A.,
Cucunubá, Z., Cuomo-Dannenburg, G., et al. Report
9: Impact of non-pharmaceutical interventions (NPIs)
to reduce COVID-19 mortality and healthcare demand.
Imperial College COVID-19 Response Team, 2020.
Haldane, A. G. and Turrell, A. E. Drawing on different
disciplines: macroeconomic agent-based models. Journal
of Evolutionary Economics, 29(1):39–66, 2019.
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and
Erez, T. Learning continuous control policies by stochastic value gradients. arXiv preprint arXiv:1510.09142,
2015.
Hoertel, N., Blachier, M., Blanco, C., Olfson, M., Massetti,
M., Rico, M. S., Limosin, F., and Leleu, H. A stochastic agent-based model of the SARS-CoV-2 epidemic in
france. Nature Medicine, 26(9):1417–1421, 2020.
Kaplan, G., Moll, B., and Violante, G. L. Monetary policy
according to hank. American Economic Review, 108(3):
697–743, 2018.
Kaplan, G., Moll, B., and Violante, G. L. The great lockdown and the big stimulus: Tracing the pandemic possibility frontier for the u.s. Working Paper 27794, National
Bureau of Economic Research, September 2020.
Kermack, W. O. and McKendrick, A. G. A contribution to
the mathematical theory of epidemics. Proceedings of
the Royal Society A: Mathematical, Physical and Engineering Sciences, 115(772):700–721, 1927.
Kerr, C. C., Stuart, R. M., Mistry, D., Abeysuriya, R. G.,
Hart, G., Rosenfeld, K., Selvaraj, P., Nunez, R. C.,
Hagedorn, B., George, L., et al. Covasim: an agentbased model of COVID-19 dynamics and interventions. medRxiv, 2020. URL https://doi.org/10.
1101/2020.05.10.20097469.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
Kydland, F. E. and Prescott, E. C. Time to build and aggregate fluctuations. Econometrica: Journal of the Econometric Society, pp. 1345–1370, 1982.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

Lin, Q., Zhao, S., Gao, D., Lou, Y., Yang, S., Musa, S. S.,
Wang, M. H., Cai, Y., Wang, W., Yang, L., et al. A conceptual model for the outbreak of coronavirus disease
2019 (COVID-19) in Wuhan, China with individual reaction and governmental action. International journal of
infectious diseases, 2020.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529–533, 2015.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In
International Conference on Machine Learning, pp. 1928–
1937. PMLR, 2016.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,
L., Bai, J., and Chintala, S. Pytorch: An imperative
style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d’Alché Buc,
F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran
Associates, Inc., 2019.

Zheng, S., Trott, A., Srinivasa, S., Naik, N., Gruesbeck,
M., Parkes, D. C., and Socher, R. The AI economist:
Improving equality and productivity with AI-driven tax
policies. arXiv preprint arXiv:2004.13332, 2020.

Appendix I : The Real Business Cycle model
We use a standard real business cycle model, however we
adopt notation and variables common in reinforcement learning, in particular, an emphasis on state variables (capital)
rather than action variables (consumption, hours worked),
and the inclusion of an action-dependent expectation. A
baseline RBC model would use Equations 9, 12, 13, 14, 15,
16, and 19. We use Equations 9, 10, 11, 14, 15, 16, and 19,
but note that 10 and 12, and 11 and 13, are the same up to
algebraic manipulation, expressing the future behaviour in
terms of U (k) and U 0 (k), functions of the state, rather than
in terms of consumption or other actions as is usually seen.
We work in real quantities, using pi=0 = 1 as the numéraire
and other quantities, including pi6=0 6= 1 defined relative to
this.
Notation
j is an index that runs over the J consumer-workers. i runs
over the I consumption goods, each of which is produced
by a different sector/firm.

Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. arXiv preprint arXiv:1511.05952,
2015.

For consumers, nj is hours worked, cji is consumption, kj
is capital held by the consumer, vj is their investment in
capital in that timestep, θj > 0 is the weight given to hours
in the utility function. Ei denotes the set of agents employed
at firm i, e(j) is the index, i, of the employer of j.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.

For firms, Ni is the number of hours worked at the firm, Ki
is its capital, Ai is the firm’s technology, Yi is the production
function, Ci is the consumption of the firm’s goods.

Smets, F. and Wouters, R. Shocks and frictions in us business cycles: A bayesian dsge approach. American Economic Review, 97(3):586–606, 2007.

r is the real interest rate, wi are wages, and pi are the prices
of goods.
Consumers

Sutton, R. S. and Barto, A. G. Reinforcement learning:
An introduction. MIT Press, Cambridge, Massachusetts,
2018.

For consumers, the time-t utility is
uj ({cji }, nj ; θj ) =

Tracy, M., Cerdá, M., and Keyes, K. M. Agent-based modeling in public health: Current applications and future
directions. Annual Review of Public Health, 39(1):77–94,
2018.
Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, pp. 2094—
-2100, 2016.

X
i

1
ln cji − θj n2j
2

and their total utility from time t onward is
Uj,t,S (kt,j ) =
(
max

cji ,nj

uj (cji , nj ) + β

)
X

PS→S 0 (cji , nj )Uj,t+1,S 0 (kt+1,j )

S0

(8)

Solving Heterogeneous General Equilibrium Economic Models with Deep Reinforcement Learning

The technology shock

Their budget constraint is,
we(j) nj + rkj,t =

X

pi cji + vj

∀j

(9)

i

The technology shock is log-autoregressive
ln Ait = ρA ln Ai,t−1 + t

t ∼ N (0, σA )

(19)

kj,t+1 = kj,t + vj
Let Uj0 = ∂kt+1,j U (kt+1,j ); expectations are over a distribution of probabilities PS→S 0 , and so E ∂cji ln Pj =
P
S→S 0 ∂cji Pj . Consumers take prices (pi , wi and r) as
given and use their first order conditions

0
0 = c−1
(10)
ji + β E Uj ∂cji ln Pj − βpi Uj
0 = −θj nj + we(j) β E Uj0

(11)

to find cji and nj ; this is solved iteratively since U is a
function of kt+1 and thus cji and nj . In the reinforcement
learning training we add an additional reward for individually achieving a no-Ponzi condition at the final time-step.
If the probabilities were independent of the action, ∂cji Pj =
0, as would be the case in a standard RBC model, then that
term can be removed and the E Uj0 eliminated obtaining
nj =

we(j)
∀i
θcji pi

(12)

A small amount of work, with care taken as to the maximisation over a in the definition of the utility, shows that
E Uj0 = (1 + rt+1 )(pi,t+1 cji,t+1 )−1 , and so Equation 10
reduces to the Euler equation


pi
−1
(1
+
r
)c
(13)
c−1
=
β
E
t+1
ji,t+1
ji
pi,t+1
where the pi remains due to the multiple goods with pi6=0 6=
1.
Firms
Firms are profit maximising with production function
Yi = Ai Ki1−α Niα

(14)

Since profit Πi = pi Yi − wi Ni − (r + δ)Ki , then taking
prices (pi , wi , r) as given
∂Ni Πi = 0
∂Ki Πi = 0

pi αYi − wi Ni = 0 ∀i

(15)

pi (1 − α)Yi − (r + δ)Ki = 0 ∀i (16)

and therefore Πi = 0.
Ki,t+1 = Ki,t (1 − δ) + (Yi − Ci )

(17)

We split Ki = Ke + K̂i where Ke is an endowment and K̂i
is provided by investment from the consumers.
K̃i,t+1 = K̃i,t (1 − δ) + (Yi − Ci ) − (r + δ)Ke

(18)

Market clearing conditions
Wages are set by market clearing for hours worked
X
Ni =
gj nj ∀i

(20)

j∈Ei

and the real interest rate is set by market clearing for capital
X
X
K̃i =
gj kj
(21)
i

j

where gj is the weight of each agent, with

P

j

gj = 1.

