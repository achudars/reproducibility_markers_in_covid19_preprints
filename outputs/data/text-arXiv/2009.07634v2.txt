arXiv:2009.07634v2 [stat.ME] 9 Mar 2021

Time-varying auto-regressive models for count
time-series
Arkaprava Roy, Sayar Karmakar
University of Florida
March 10, 2021

Abstract
Count-valued time series data are routinely collected in many application areas. We are
particularly motivated to study the count time series of daily new cases, arising from COVID19 spread. First, we propose a Bayesian framework to study time-varying semiparametric
AR(p) model for count and then extend it to propose a time-varying INGARCH model
considering the rapid changes in the spread. We calculate posterior contraction rates of
the proposed Bayesian methods with respect to average Hellinger metric. Our proposed
structures of the models are amenable to Hamiltonian Monte Carlo (HMC) sampling for
efficient computation. We substantiate our methods by simulations that show superiority
compared to some of the close existing methods. Finally we analyze the daily time series
data of newly confirmed cases to study its spread through different government interventions.

Keywords: Autoregressive model, B-splines, COVID-19, Count-valued time series, Hamiltonian
Monte Carlo (HMC), INGARCH, Posterior Contraction Rates, Non-stationary, Poisson Regression

1

Introduction

Modeling count time series is important in many applications such as disease incidence, accident
rates, integer financial datasets such as price movement, etc. This relatively new research stream
was introduced in Zeger (1988) and interestingly he analyzed another outbreak namely the US
1970 Polio incidence rate. This stream was furthered by Chan and Ledolter (1995) where Poisson
generalized linear models (GLM) with an autoregressive latent process in the mean are discussed.
A wide range of dependence was explored in Davis et al. (2003) for simple autoregressive (AR)
structure and external covariates. On the other hand, a different stream explored integer-valued
time series counts such as ARMA structures as in (Brandt and Williams, 2001; Biswas and Song,
1

2009) or INGARCH structure as done in Zhu (2011, 2012c,a,b). However, from a Bayesian perspective, the only work to the best of our knowledge is that of Silveira de Andrade et al. (2015)
where the authors discussed an ARMA model for different count series parameters. However,
their treatment of ignoring zero-valued data or putting the MA structure by demeaned Poisson
random variable remains questionable. None of these works focused on the time-varying nature
of the coefficients except for a brief mention in Karmakar et al. (2020+).
Our goals are motivated by both the application and methodological development. To the
best of our knowledge, ours is the first attempt to model possibly autoregressive count time series
with time-varying coefficients which can be regarded as the time-varying analog of Fokianos et al.
(2009). We consider a linear link based GLM route instead of the traditional exponential link
(Fokianos and Tj√∏stheim, 2011), since linear link helps in better interpretability of the coefficient
functions. Linear link however requires more stringent shape restrictions on the functions. We
impose those by putting constraints on the B-spline coefficients while modeling those coefficient
functions. However, it is possible to extend all the computations of the current paper to an
exponential link based GLM framework. The mean function stands for the overall spread and the
autoregressive coefficients stand for the effect of different lags. We are particularly motivated to
study the spread of COVID-19 in New York City (NYC) from 23rd January to 14th July using
the daily count data of new cases. In terms of our motivating data application, we wish to identify
which lags are significant in our model which can be directly linked to the period of time symptoms
did not show up. We find that some higher-order lags like 6, 7, and 8 are also significant. These
findings are in-line with several research articles discussing the incubation length for the novel
coronavirus with a median of 6-7 days and 98% below 11 days. For example, see Lauer et al.
(2020b). We also find that after the lockdown or stay-at-home orders it takes about 12-16 days
to reach the peak and then the intercept coefficient function starts decreasing. This is also an
interesting find which characterizes the fact that the number of infected but asymptomatic cases
is large compared to the new cases reported. Additional to the time-varying AR model proposal,
we also offer an analysis via a time-varying Bayesian integer-valued generalized autoregressive
conditional heteroscedasticity (TVBINGARCH) model that assumes an additional recursive term
in the conditional expectation (cf. (2.1)). This extension offers some more comprehensiveness
in the modeling part as even BINGARCH with small orders can help us get rid of choosing an

2

appropriate maximum lag value. Since for a Poisson model, the mean is the same as the variance,
this can also be thought of as an extension of the GARCH model in the context of count data.
First introduced by Ferland et al. (2006), these models were thoroughly analyzed in Zhu (2012c,a,
2011, 2012b); Ahmad and Francq (2016). Our proposal for the time-varying TVBINGARCH model
adapts to the non-stationarity theme and also can be viewed as a new contribution. Finally, we
contrast the time-varying AR and the GARCH for both simulations and real-data applications
under different metrics of evaluation. Our semiparametric time-varying model provides better
estimates.
Regression models with varying coefficient were introduced by Hastie and Tibshirani (1993).
They modeled the varying coefficients using cubic B-splines. Later, these models has been further
explored in various directions Gu and Wahba (1993); Biller and Fahrmeir (2001); Fan and Zhang
(2008); Franco-Villoria et al. (2019); Yue et al. (2014). Spline bases have been routinely used to
model the time-varying coefficients within non-linear time series models (Cai et al., 2000; Huang
et al., 2002; Huang and Shen, 2004; Amorim et al., 2008). We also consider the B-spline series
based priors to model the time-varying coefficient functions. We develop efficient computational
algorithms for the proposed models.
Apart from developing a computationally tractable hierarchical model, we also establish posterior contraction rates of the proposed models. Ghosal et al. (2007) established posterior contraction for a general stationary Markov chain with much stricter conditions. However, they relaxed
some of those conditions in Theorem 8.29 of Ghosal and Van der Vaart (2017). To the best of
our knowledge, the posterior contraction rate result of this paper is the first for the time-varying
Markov model based on minimal assumptions under Poisson-link. We also consider the strategy,
used to relax the conditions in Theorem 8.29 of Ghosal et al. (2007). Our posterior contraction
rate is with respect to the average Hellinger metric. The primary theoretical hurdle is to construct
exponentially consistent tests in a time-varying Markov setup. Our proposed test construction is
inspired by Jeong et al. (2019); Ning et al. (2020). We construct the test relying on the NeymanPearson lemma with respect to negative average log affinity distance and calculate contraction
rates. Then we show that the same rate holds for the average Hellinger metric as well. We also
discuss a pointwise inferential tool by drawing credible intervals. Such tools are important to
keep an objective perspective in terms of the evolution of the time-varying coefficients without

3

restricting it to some specific trend models. See Karmakar et al. (2020+) (Karmakar (2018) for
an earlier version) for a comprehensive discussion on time-varying models and their applications.
The rest of the paper is organized as follows. Section 2 describes the proposed Bayesian
models in detail. Section 3 discusses an efficient computational scheme for the proposed method.
We calculate posterior contraction rates in Section 4. The performance of our proposed method in
capturing true coefficient functions are studied in Section 5 and we show excellent performance over
other existing methods. Section 6 deals with an application of the proposed method on COVID19 spread for NYC. Then, we end with discussions and possible future directions in Section 7.
Section 8 contains detail theoretical proofs.

2

Modeling

Given the rapidly evolving nature of the pandemic, the patterns and number of new affected
cases were changing rapidly over different geographical regions. The rapid change in the observed
counts make all earlier time-constant analysis inappropriate and builds a path where we can explore
methodological and inferential development in tracking down the trajectory of this spread. Thus,
we propose two novel semiparametric time-varying autoregressive models for counts to study the
spread and examine the effects of these interventions in the spread based on the time-varying
coefficient functions. We first consider the most general case where we model the data using
a time-varying Bayesian integer-valued generalized autoregressive conditional heteroscedasticity
(TVBINGARCH) model where the conditional mean depends on the past observations as well
as past conditional means. However, the relatively simpler process consisting of a time-varying
mean/intercept function along with time-varying autoregressive coefficient functions upto lag-p
is also important keeping in mind the scope of application to real data and its interpretation.
For example, the particular lags in an AR(p) model for the COVID-19 count data can crave an
interesting phenomenon in the lag-dynamics of the spread. This might be lost if we model the
same using a TVBINGARCH(1,1) model since typically for GARCH type models it is standard
practice to only consider smaller orders.

4

2.1

Time-varying generalized autoregressive conditional heteroscedasticity model for counts

Ferland et al. (2006) proposed integer valued analogue of generalized autoregressive conditional
heteroscedasticity model (GARCH) after observing that the variability in number of cases of
campylobacterosis infections also changes with level. We consider here a time-varying analog of
such process. The conditional distribution for count-valued time-series Xt given Ft‚àí1 = {Xi : i ‚â§
(t ‚àí 1)} and Gt‚àí1 = {Œªi : i ‚â§ (t ‚àí 1)} is,
p
X

Xt |Ft‚àí1 , Gt‚àí1 ‚àºPoisson(Œªt ) where Œªt = ¬µ(t/T ) +

ai (t/T )Xt‚àíi +

i=1

q
X

bj (t/T )Œªt‚àíj .

(2.1)

j=1

We call our method time-varying Bayesian Integer valued Generalized Auto Regressive Conditional
Heteroscedastic (TVBINGARCH) model. We impose following constraints on the parameter space
similar to Ferreira et al. (2017),
P1 = {¬µ, ai : 0 < ¬µ(x) < ‚àû, sup
x

X

(ai (x) + bj (x)) < 1}.

(2.2)

i,j

This constraint ensure a unique solution of the time-varying GARCH process as discussed in Davis
and Mikosch (2009); Rohan and Ramanathan (2013); Ferreira et al. (2017). Now, we put priors
on the functions ¬µ(¬∑), ai (¬∑) and bj (¬∑) such that they are supported in P1 . Using the B-spline bases,
we put following hierarchical prior on the unknown functions,
¬µ(x) =

K1
X

Œ±j Bj (x)

(2.3)

j=1

ai (x) =

K2
X

Œ∏ij Mi Bj (x),

0 ‚â§ Œ∏ij ‚â§ 1, 1 ‚â§ i ‚â§ p,

(2.4)

j=1

bk (x) =

K3
X

Œ∑kj Mk+p Bj (x),

0 ‚â§ Œ∑kj ‚â§ 1, 1 ‚â§ k ‚â§ q,

(2.5)

j=1

œÑi
Mi = Pp

k=0 œÑk

,

i = 1, . . . , p + q,

(2.6)

Œ∏ij ‚àºU (0, 1) for 1 ‚â§ i ‚â§ p, 1 ‚â§ j ‚â§ K2 ,

(2.7)

Œ∑kj ‚àºU (0, 1) for 1 ‚â§ k ‚â§ q, 1 ‚â§ j ‚â§ K3 ,

(2.8)

Œª0 ‚àºInverse-Gamma(d1 , d1 ),

(2.9)

5

where Œª0 is the rate parameter for X0 . The specification for the density of X0 is required for
computation. Otherwise we need to assume Œª0 to be known which is not reasonable for a real data
application. We primarily focus on the special case where p = 1, q = 1. Based on the constraints
on the parameter space we consider following prior for Œ±j ‚Äôs and œÑi ‚Äôs,
Œ±j ‚àº TN(0, c21 , 0, ‚àû),

œÑi ‚àº U (0, 1),

(2.10)

where TN stands for the truncated normal with mean 0, variance c21 and truncated to [0, ‚àû). In
P
Pp+q
above construction, Pj=0 Mj = 1. Thus j=1
Mj < 1 if M0 > 0. As Œ†(M0 > 0) = 1, we have
Pp+q
Œ†( j=1 Mj < 1) = 1. Since 0 ‚â§ Œ∏ij ‚â§ 1, we have supx ai (x) ‚â§ Mi , and supx bj (x) ‚â§ Mp+j . Thus
P
P
P
Pp+q
supx pi=1 ai (x) + qj=1 bj (x) ‚â§ p+q
M
<
1.
We
have
i
i=1
j=1 Mj = 1 if and only if œÑ0 = 0, which
has zero prior probability. On the other hand, we also have ¬µ(¬∑) ‚â• 0 as we have Œ±j ‚â• 0. Thus,
the induced priors, described in (2.3)‚àí (2.9) are well supported in P1 .

2.2

Time-varying auto-regressive model for counts

Although our previous modeling framework is more general, one may only wish to study higher
order lag dependence from the past observations. Thus we consider a simplified model in this
subsection. The linear Poisson autoregressive model (Zeger, 1988; Brandt and Williams, 2001)
is popular in analyzing higher order lag-dependence in count valued time series. Due to the
assumed non-stationary nature of the data, we propose a time-varying version of this model. The
conditional distribution for count-valued time-series Xt given Ft‚àí1 = {Xi : i ‚â§ (t ‚àí 1)} is,
Xt |Ft‚àí1 ‚àºPoisson(Œªt ) where Œªt = ¬µ(t/T ) +

p
X

ai (t/T )Xt‚àíi .

(2.11)

i=1

We call our method time-varying Bayesian Auto Regressive model for Counts (TVBARC). The
rescaling of the time-varying parameters to the support [0,1] is usual for in-filled asymptotics.
Due to the Poisson link in (2.11), both conditional mean and conditional variance depend on the
past observations. The conditional expectation of Xt in the above model (2.11) is E(Xt |Ft‚àí1 ) =
P
¬µ(t/T ) + pi=1 ai (t/T )Xt‚àíi , which needs to be positive-valued. To ensure that, we impose the
following constraints on parameter space for the time-varying parameters,
P2 = {¬µ, ai : 0 < ¬µ(x) < ‚àû, sup
x

6

X
k

ak (x) < 1}.

(2.12)

Note that, the conditions imposed (2.12) on the parameters are somewhat motivated by the
stationarity conditions for the time-constant versions of these models. This is not uncommon
in time-varying AR literature. See Dahlhaus and Subba Rao (2006); Fryzlewicz, Sapatinas and
Subba Rao (2008); Karmakar et al. (2020+) for example. Even though the condition on ¬µ(¬∑) seem
restrictive in the light of what we need for invertible time-constant AR(p) process with Gaussian
error, it is not unusual when it is used to model variance parameters to ensure positivity; it was
unanimously imposed for all the literature mentioned above. Additionally, the above references
heavily depend on local stationarity: namely, for every rescaled time 0 < t < 1, they assume
the existence of an XÃÉi process which is close to the observed process. One key advantage of our
proposal is it is free of any such assumption. Our assumption of only the first moment is also
very mild for theoretical exploration in Section 4. Moreover, except for a very general linear
model discussed in (Karmakar et al., 2020+), to the best of our knowledge, this is the very first
analysis of the time-varying parameter for count time-series modeled by Poisson regression. Thus
we choose to focus on the methodological development rather than proving the optimality of these
conditions. When p = 0, our proposed model reduces to routinely used nonparametric Poisson
regression model as in Shen and Ghosal (2015).
To proceed with Bayesian computation, we put priors on the unknown functions ¬µ(¬∑) and ai (¬∑)‚Äôs
such that they are supported in P2 . The prior distributions on these functions are induced through
basis expansions in B-splines. Suitable constraints on the coefficients are imposed to ensure the
shape constraints as in P2 . Detailed description of the priors are given below,
¬µ(x) =

K1
X

Œ±j Bj (x)

(2.13)

j=1

ai (x) =

K2
X

Œ∏ij Mi Bj (x),

0 ‚â§ Œ∏ij ‚â§ 1,

(2.14)

j=1

œÑi
Mi = Pp

k=0 œÑk

,

i = 1, . . . , p,

Œ∏ij ‚àºU (0, 1) for 1 ‚â§ i ‚â§ p, 1 ‚â§ j ‚â§ K2 .

(2.15)
(2.16)

Here Bj ‚Äôs are the B-spline basis functions. The parameters Œ¥j ‚Äôs are unbounded. Based on the
constraints on the parameter space we consider following prior for Œ±j ‚Äôs and œÑi ‚Äôs,
Œ±j ‚àº TN(0, c21 , 0, ‚àû),
7

œÑi ‚àº U (0, 1),

(2.17)

where TN stands for the truncated normal distribution with mean 0, variance c21 and truncated
in [0, ‚àû). The priors induced by above construction are P2 -supported. The verification is very
straightforward and similar to the previous subsection.

2.3

Model properties

In this paper, we only consider TVBINGARCH(1,1) which is commonly used for the GARCH
class of models. One drawback of TVBARC is proper selection of lag. To alleviate this, one may
then consider the TVBINGARCH framework. As in the stationary case, TVBINGARCH(1,1) can
be viewed as TVBARC with infinite order. Then the higher values in b1 (¬∑)‚Äôs is an indication that
there might be important higher lags in TVBARC. Besides, to infer about higher lag dependence
TVBARC is more suitable than TVBINGARCH. In our real data illustration, we find that the
TVBARC model identifies three important higher order lags 6,7 and 8 in COVID-19 spread. Such
inference is difficult to obtain from TVBINGARCH. If the CH coefficient b1 (¬∑) is uniformly zero,
TVBINGARCH(1,1) reduces to TVBARC(1). However, the computational steps for TVBARC(1)
does not easily follow from TVBINGARCH(1,1). Furthermore, our theoretical result of TVBINGARCH requires a lower bound for the true CH coefficient which is standard for time varying
GARCH class of models. Thus the theoretical result of TVBARC does not easily follow from
TVBINGARCH.
Towards writing the likelihood, note that our proposed models are non-stationary since the
coefficient functions ai (¬∑), bj (¬∑) are possibly not constant. However, we still take a simple product of
individual conditional likelihoods for Xt ‚Äôs rather than first locally approximating it by a stationary
process. The latter approach is more prominent in the frequentist framework and this phenomenon
is known by ‚Äòlocally stationary approximation‚Äô. This was introduced in some seminal papers by
Dahlhaus et al. (1997, 2000) and were later used in many time-varying literature. See Dahlhaus
and Subba Rao (2006); Dahlhaus (2012); Truquet et al. (2019) among many others. Towards
the Bayesian approach of modelling such approximating phenomenon, interested readers can refer
to Rosen et al. (2009, 2012). However, the assumption of existence of such an approximating
stationary process is somewhat stringent and is probably not required in Bayesian paradigm. For
example, see DeYoreo and Kottas (2017) where the likelihood is formed by taking product of
individual conditional likelihoods for a non-stationary time-series. Other approaches can be found

8

in Hadj-Amar et al. (2020); Yang and Bradley (2020) where the likelihoods for the proposed nonstationary processes were computed without any local stationary approximation. Moreover, note
that such approximating stationary processes can be shown to exist under the general smoothness
conditions as outlined in Theorem 1 in Dahlhaus and Subba Rao (2006) (for tvARCH case) or
Proposition 2.3 in Rohan and Ramanathan (2013)(for tvGARCH case). These are easily extendible
to the Poisson setting and for more general Holder smooth coefficient functions with probably
an amended approximation rate. So in a sense, our smoothness assumption and the parameter
restriction as (2.2) or (2.12)implies existence of such stationary processes without us implicitly
putting additional assumption.

3

Posterior computation

In this section, we discuss the Markov Chain Monte Carlo (MCMC) sampling method for posterior
computation. Our proposed sampling is dependent on the gradient-based Hamiltonian Monte
Carlo (HMC) sampling algorithm (Neal et al., 2011). Hence, we show the gradient computations
of the likelihood with respect to different parameters for TVBARC(p) and TVBINGARCH(p, q)
in the following two subsections.
We obtain the likelihood from the joint density of the data based on our Poisson error model.
Since the joint density can be written as product of conditionals, we can thus write the joint
likelihood of the data as product of conditional densities. Detail expressions for each case are
separately presented below. The likelihoods of the two models are constructed differently, thus we
present them separately.

3.1

TVBINGARCH structure

We only derive the computational steps for TVBINGARCH(1,1) which is the frequent choice
among GARCH-type models. While fitting this model, we assume for any t < 0 Xt = 0, Œªt = 0.
The expression for Œª1 also involves Œª0 . Thus, we need to additionally estimate the parameter
Œª0 , the Poisson rate parameter for X0 . Here the likelihood for TVBINGARCH(1,1) is given by
Q
P (X0 ) Tt=1 P (Xt |Ft‚àí1 ). We assume that the marginal distribution of X0 is Poisson(Œª0 ) and the
prior for Œª0 is Inverse-Gamma(d1 , d2 ) as described in Section 2.1. The complete likelihood L2 of

9

the propose Bayesian method of (2.1) is given by
L2 ‚àù exp

X
T




‚àí {¬µ(t/T ) + a1 (t/T )Xt‚àí1 + b1 (t/T )Œªt‚àí1 + Xt log ¬µ(t/T )

t=1
K1
 X
Œ±j2 /(2c21 )
+ a1 (t/T )Xt‚àí1 + b1 (t/T )Œªt‚àíi } ‚àí
j=1


‚àí (d1 + 1) log Œª0 ‚àí d1 /Œª0 10‚â§Œ∏11 ,Œ∑ij ‚â§1,,0‚â§œÑi ‚â§1,Œ±j ‚â•0 ,
We calculate the gradients of negative log-likelihood (‚àí log L2 ) with respect to the parameters Œ≤,
Œ∏, Œ∑ and Œ¥. The gradients are given below,
‚àí

d log L2
Œ±
1 X
= 1‚àí


B1 (t/T )Xt‚àíj
+ Œ±j /(2c21 ),
(¬µ(t/T
)
+
a
(t/T
)X
)
+
b
(t/T
)Œª
)
1
t‚àíj
1
t‚àí1
t


X
d log L2
B1 (t/T )Xt‚àíj
= Mi 1 ‚àí
‚àí
,
Œ∏11
(¬µ(t/T
)
+
a
(t/T
)X
)
+
b
(t/T
)Œª
)
j
t‚àíj
k
t‚àí1
 tX

B1 (t/T )Œªt‚àíj
d log L2
= Mp+k 1 ‚àí
,
‚àí
Œ∑kj
(¬µ(t/T
)
+
a
(t/T
)X
)
+
b
(t/T
)Œª
)
j
t‚àíj
k
t‚àí1
t
d log L2 X
(Mj 1{j=k} ‚àí Mj Mk )√ó
‚àí
=
œÑj
k
"


X
X
Bj (t/T )Xt‚àíj
Œ∏ij Bj (x) 1 ‚àí
1{j‚â§p} +
(¬µ(t/T
)
+
a
(t/T
)X
)
+
b
(t/T
)Œª
)
j
t‚àíj
1
t‚àí1
t
i‚â§p
#


X
X
Bj (t/T )Œªt
Œ∑kj Bj (x) 1 ‚àí
1{j>p} .
(¬µ(t/T ) + aj (t/T )Xt‚àí1 ) + b1 (t/T )Œªt‚àí1 )
t
1‚â§k‚â§q
The derivative of the likelihood concerning Œª0 is calculated numerically by differentiating from the
first principles. Hence, it is sampled using the HMC algorithm too.

3.2

TVBARC structure

Since we do not have any information of the process for t < 0, our computation for TVBARC(p)
Q
is based on the likelihood Tt=p P (Xt |Ft‚àí1 ). This likelihood may thus be regarded as a quasilikelihood as we are looking at the joint density of last T ‚àí p + 1 time points given the first p
observations and it is similar to the likelihood from DeYoreo and Kottas (2017). This likelihood
also shares some commonality with the objective functions used for computation in Dahlhaus and
10

Subba Rao (2006); Fryzlewicz, Sapatinas, Rao et al. (2008). The complete posterior likelihood L1
of the proposed Bayesian method in (2.11) is given by
L1 ‚àù exp

X
T



‚àí {¬µ(t/T ) +

t=p
p

+

X


ai (t/T )Xt‚àíi } ‚àí

i=1

where we have ¬µ(x) =

PK1

j=1

p
X


ai (t/T )Xt‚àíi + Xt log ¬µ(t/T )

i=1
K1
X



Œ±j2 /(2c21 )

10‚â§Œ∏ij ‚â§1,0‚â§œÑi ‚â§1,Œ±j ‚â•0 ,

j=1

exp(Œ≤j )Bj (x), ai (x) =

PK2

j=1 Œ∏ij Mi Bj (x)

and Mj =

exp(Œ¥j )
Pp
.
k=0 exp(Œ¥k )

We

develop efficient MCMC algorithm to sample the parameter Œ≤, Œ∏ and Œ¥ from the above likelihood.
The derivatives of above likelihood with respect to the parameters are easily computable. This
helps us to develop an efficient gradient-based MCMC algorithm to sample these parameters. We
calculate the gradients of negative log-likelihood (‚àí log L1 ) with respect to the parameters Œ≤, Œ∏
and Œ¥. The gradients are given below,


X
Bj (t/T )Xt
d log L1
P
= 1‚àí
+ Œ±j /(2c21 ),
‚àí
Œ±j
(¬µ(t/T ) + j aj (t/T )Xt‚àíj )
 tX

d log L1
Bj (t/T )Xt
P
‚àí
= Mi 1 ‚àí
,
Œ∏ij
(¬µ(t/T
)
+
j aj (t/T )Xt‚àíj )
t


X
X
X
Bj (t/T )Xt‚àíj
d log L1
P
(Mj 1{j=k} ‚àí Mj Mk )
Œ∏ij Bj (x) 1 ‚àí
=
‚àí
,
œÑj
(¬µ(t/T
)
+
j aj (t/T )Xt‚àíj )
t
i
k
where 1{j=k} stands for the indicator function which takes the value one when j = k.
As the parameter spaces of Œ∏ij ‚Äôs and Œ∑kj ‚Äôs have bounded support, we map any Metropolis candidate, falling outside of the parameter space back to the nearest boundary point of the parameter
space. To obtain a good acceptance rate, we tune our HMC sampler periodically. There are two
tuning parameters in HMC namely the leapfrog step, and the step size parameter. The step size
parameter is tuned to maintain an acceptance rate within the range of 0.6 to 0.8. The step size
is reduced if the acceptance rate is less than 0.6 and increased if the rate is more than 0.8. This
adjustment is done automatically after every 100 iterations. However, we choose to pre-specify
the leapfrog step at 30 and obtain good results. Due to the increasing complexity of the parameter
space in TVBINGARCH, we consider updating all the parameters involved in ai (¬∑)‚Äôs, bk (¬∑)‚Äôs, and
Œª0 together.

11

4

Large-sample properties

In this section we obtain posterior contraction rates for the two proposed models. Posterior
contraction measures the speed at which we can recover the true parameter from the posterior
distribution with increasing sample size. The notion of recovery is specified by a semimetric d.
Definition (Ghosal and Van der Vaart, 2017): The posterior contraction rate at the true parameter Œ∫0 ‚àà A with respect to the semimetric d on A is a sequence T ‚Üí 0 such that PŒ∫0 Œ†(Œ∫ :
d(Œ∫, Œ∫0 ) > MT T |X (T ) ) ‚Üí 0 for every MT ‚Üí ‚àû, where A denotes the parameter space of Œ∏0 . Here
X (T ) stands for the complete dataset.
Although TVBINGARCH(1,1) may reduce to TVBARC(1) assuming b1 (x) = 0 for all x ‚àà
[0, 1], the required technical assumptions do not allow us to derive the results for TVBARC as
a special case for TVBINGARCH. For clarity in presenting the assumptions under which the
respective results are established, we will make the conditions in (2.12) and (2.2) more specific.
Since TVBARC is a simpler model, we first develop the theoretical results for this model and then
make modifications to obtain the results for TVBINGARCH.

4.1

TVBARC structure

We start by studying large sample properties of the simpler AR model in (2.11). For simplicity,
we fix order p at p = 1 for this section however the results are easily generalizable for any fixed
order p with some additional assumptions. The posterior consistency is studied in the asymptotic
regime of increasing sample size T . Let Œ∫ = (¬µ, a1 ) stands for the complete set of parameters. For
sake of generality of the method, we put a prior on K1 and K2 with probability mass function
given by,
Œ†(Ki = k) = bi1 exp[‚àíbi2 k(log k)bi3 ],

(4.1)

with bi1 , bi2 > 0 and 0 ‚â§ bi3 ‚â§ 1 for i = 1, 2. Poisson and geometric probability mass functions
appear as special cases of the above prior density for bi3 = 1 or 0 respectively. These priors
have not been considered while fitting the model as it would require computationally expensive
reversible jump MCMC strategy. We study the posterior consistency with respect to the average
Hellinger distance on the coefficient functions which is
Z p
p
1
1 2
2
( f1 ‚àí f2 )2 ,
d1,T = dH (Œ∫1 , Œ∫2 ) =
T
T
12

where f1 =

QT

t=1

PŒ∫1 (Xt |Xt‚àí1 ). Here P stand for the conditional Poisson density defined in (2.11).

The contraction rate will depend on the smoothness of true coefficient functions ¬µ and a and the
parameters b13 and b23 from the prior distributions of K1 and K2 . Let Œ∫0 = (¬µ0 , a10 ) be the truth
of Œ∫.
Assumptions (A): There exists constants 0 < M¬µ < MX such that,
(A.1) At time t = 0, EŒ∫0 (X0 ) < MX .
(A.2) The coefficient functions supx‚àà[0,1] ¬µ0 (x) < M¬µ and supx‚àà[0,1] a10 (x) < 1 ‚àí M¬µ /MX .
(A.3) inf x‚àà[0,1] min(¬µ0 (x), a10 (x)) > œÅ for some small œÅ > 0.
Assumptions (A.1), (A.2) ensure


M¬µ
EŒ∫0 (Xt ) = EŒ∫0 (EŒ∫0 (Xt |Xt‚àí1 )) < M¬µ + 1 ‚àí
MX


MX < MX

by recursion. Assumption (A.3) is imposed to ensure strict positivity of parameters and is standard
in time-varying literature that deals with such constrained parameters.
Posterior consistency theory studies recovery of the ‚Äòtrue‚Äô parameter Œ∫0 with increasing sample
size when the data is sampled from the distribution characterized by Œ∫0 . Our notion of recovery
is based on the average Hellinger metric d21,T defined above.
Theorem 1. Under assumptions (A.1)-(A.3), let the true functions ¬µ0 (¬∑) and a10 (¬∑) be HoÃàlder
smooth functions with regularity level Œπ1 and Œπ2 respectively, then the posterior contraction rate
with respect to the distance d21,T is


‚àíŒπ1 /(2Œπ1 +1)
Œπ1 /(2Œπ1 +1)+(1‚àíb13 )/2
‚àíŒπ2 /(2Œπ2 +1)
Œπ2 /(2Œπ2 +1)+(1‚àíb23 )/2
max T
(log T )
,T
(log T )
.
where bij are specified in (4.1). For the proof, the first step is to calculate posterior contracR 1/2 1/2
tion rate with respect to average log-affinity rT2 (f1 , f2 ) = ‚àí T1 log f1 f2 and then show that
rT2 (f1 , f2 ) . 2T implies

1 2
d (f , f )
T H 1 2

. 2T . The average log-affinity provides a unique advantage to

construct exponentially consistent tests leveraging on the famous Neyman-Pearson Lemma as has
also been used in Ning et al. (2020) for a multivariate linear regression setup under group sparsity.
The proof is postponed to Section 8. The proof is based on the general contraction rate result
from Ghosal and Van der Vaart (2017) and some results on B-splines based finite random series.
13

4.2

TVBINGARCH structure

Next, we discuss the more comprehensive tvBINGARCH model (2.1). To maintain simplicity in
the proof, we again assume p = 1, q = 1. Similar to the previous subsection, we put a prior on the
number of Bspline bases, Ki with probability mass function given by,
Œ†(Ki = k) = bi1 exp[‚àíbi2 k(log k)bi3 ],
with bi1 , bi2 > 0 and 0 ‚â§ bi3 ‚â§ 1 for i = 1, 2, 3. Let us assume that œà = (¬µ, a1 , b1 ) be the complete
set of parameters. We study the posterior consistency with respect to the Hellinger distance on
the coefficient functions which is
d22,T
where f1 = PœÜ1 (X0 )

QT

t=1

1
1
= d2H (œà1 , œà2 ) =
T
T

Z p
p
( f1 ‚àí f2 )2 ,

Pœà1 (Xt |Xt‚àí1 , Œªt‚àí1 ). Here P stands for the conditional Poisson den-

sity defined in (3) and the marginal density of X0 , PœÜ1 (X0 ) is Poisson(Œª10 ) as described in our
computational steps.
For this structure, we modify the assumptions as
Assumptions(B): There exists constants 0 < M¬µ < MX such that,
(B.1) At time t = 0, Eœà0 (X0 ), Œª0 < MX .
(B.2) The coefficient functions supx‚àà[0,1] ¬µ0 (x) < M¬µ and supx‚àà[0,1] (a10 (x) + b10 (x)) < 1 ‚àí M¬µ /MX .
(B.3) inf x‚àà[0,1] min(¬µ0 (x), a10 (x), b10 (x)) > œÅ for some small œÅ > 0.
Assumptions (B.1), (B.2) ensure


M¬µ
Eœà0 (Xt ) = Eœà0 (Eœà0 (Xt |Xt‚àí1 , Œªt‚àí1 )) < M¬µ + 1 ‚àí
MX


MX < MX

by recursion. Thus we have, by Assumption (B.1-B.2)
Eœà0 (Xt ) < MX ,

Eœà0 (Œªt ) = Eœà0 (Xt |Xt‚àí1 , Œªt‚àí1 ) = Eœà0 (Xt ) < MX .

Assumption (B.3) is imposed to ensure strict positivity of parameters and is standard in timevarying literature that deals with such constrained parameters. Now we present our posterior
contraction rate theorem below. The definition of the contraction rate is the same as before.
14

Theorem 2. Under assumptions (B.1)-(B.3), let the true functions ¬µ0 (¬∑), a10 (¬∑) and b10 (¬∑) be
HoÃàlder smooth functions with regularity level Œπ1 , Œπ2 and Œπ3 respectively, then the posterior contraction rate with respect to the distance d22,T is

max T ‚àíŒπ1 /(2Œπ1 +1) (log T )Œπ1 /(2Œπ1 +1)+(1‚àíb13 )/2 , T ‚àíŒπ2 /(2Œπ2 +1) (log T )Œπ2 /(2Œπ2 +1)+(1‚àíb23 )/2 ,

‚àíŒπ3 /(2Œπ3 +1)
Œπ3 /(2Œπ3 +1)+(1‚àíb33 )/2
.
T
(log T )
The proof follows from a similar strategy as in Theorem 1. An outline of the proof can be found
in the Section 8.

5

Simulation studies

In this section, we study the performance of our proposed Bayesian method in capturing the
true coefficient functions. We compare both TVBARC and TVBINGARCH methods with some
other competing models. It is important to note that, this is to the best of our knowledge first
work in Poisson autoregression with a time-varying link. Thus, we compare our method with the
existing time-series models with time-constant coefficients for count data and time-varying AR
with Gaussian error. We also examine the estimation accuracy of the coefficient functions for
estimating the truth.
The hyperparameter c1 of the truncated normal prior is set to 10 to ensure weak informativeness. The hyperparameters for Inverse-Gamma prior d1 = 0.1, which is also weakly informative.
We consider 6 equidistant knots for the B-splines based on comparing the AMSE scores. We
choose the knot number after which the AMSE score does not change significantly. We collect
10000 MCMC samples and consider the last 5000 as post-burn-in samples for inferences. In absence of any alternative method for time-varying AR(p) model of count-valued data, we compare
the estimated functions with the true functions in terms of the posterior estimates of functions
along with its 95% pointwise credible bands. The credible bands are calculated from the MCMC
samples at each point t = 1/T, 2/T, . . . , 1. We also compare different competing methods in terms
of average MSE (AMSE) score using the INGARCH method of tsglm from R package tscount,
GARMA using tscount as well, tvAR and our proposed Bayesian methods. The AMSE is defined

15

as

1
T

P

t (Xt

‚àí ŒªÃÇt )2 . We estimate this in terms of the posterior mean of AMSEs across MCMC as
5000
1 X1X
AM SE =
(Xt ‚àí ŒªÃÇSt )2 ,
5000 S=1 T t

where ŒªÃÇSt is the posterior estimate of Œªt at S-th postburn sample.

5.1

Case 1: TVBARC structure

Here, we consider two model settings p = 1; Xt ‚àº Poisson(¬µ(t/T ) + a1 (t/T )Xt‚àí1 ) and p = 2; Xt ‚àº
Poisson(¬µ(t/T ) + a1 (t/T )Xt‚àí1 + a2 (t/T )Xt‚àí2 ) for t = 1, . . . , T . Three different choices for T have
been considered, T = 100, 500 and 1000. The true functions are for x ‚àà [0, 1],

¬µ0 (x) =10 exp ‚àí (x ‚àí 0.5)2 /0.1 ,
a10 (x) =0.3(x ‚àí 1)2 + 0.1,
a02 (x) =0.4x2 + 0.1.
We compare the estimated functions with the truth for sample size 1000 in Figures 1 and
Figure 2 for the models p = 1 and p = 2 respectively. Tables 1 and 2 illustrate the performance
of our method with respect to other competing methods.
Table 1: AMSE comparison for different sample sizes across different methods when the true
model is (2.11) with p = 1.

5.2

INGARCH(1,0)

GARMA(1,0)

TVAR(1)

TVBARC(1)

T = 100

11.60

11.18

11.41

8.65

T = 500

11.35

11.04

11.24

8.12

T = 1000

11.05

10.73

10.94

7.02

Case 2: TVBINGARCH structure

For the tvBINGARCH case, we only consider one simulation settings p = 1, q = 1; Xt ‚àº Poisson(¬µ(t/T )+
a1 (t/T )Xt‚àí1 + b1 (t/T )Œªt‚àí1 ). Two different choices for T have been considered, T = 100 and 200

16

(a) ¬µ()

(b) a1 ()

Figure 1: Estimated mean function in 1st column and estimated AR(1) coefficient function in
the 2nd column for the case p = 1 and sample size 1000. Red is the true function, black is the
estimated curve along with the 95% pointwise credible bands in green.

Figure 2: Estimated coefficient functions for the simulation case p = 2 and sample size 1000. Red
is the true function, black is the estimated curve along with the 95% pointwise credible bands in
green.

17

Table 2: AMSE comparison for different sample sizes across different methods when the true
model is (2.11) with p = 2.
INGARCH(2,0)

GARMA(2,0)

TVAR(2)

TVBARC(2)

T = 100

18.02

17.28

13.04

11.01

T = 500

16.42

15.86

12.61

10.79

T = 1000

15.79

15.25

12.75

10.61

Figure 3: Estimated coefficient functions for the TVBINGARCH(1,1) and sample size 1000. Red
is the true function, black is the estimated curve along with the 95% pointwise credible bands in
green.
and for x ‚àà [0, 1] the coefficient functions are,

¬µ0 (x) =25 exp ‚àí (x ‚àí 0.5)2 /0.1 ,
a1 (x) =0.3(x ‚àí 1)2 + 0.1,
b1 (x) =0.1x1.5 + 0.1
Figure 3 compares the estimated functions with the truth for sample size 200 for the model in (2.1)
with p = 1, q = 1. The performance of our method is compared to other competing methods in
Tables 3.
Table 3: Average MSE comparison for different sample sizes across different methods when the
true model is (2.1) with p = 1, q = 1.
INGARCH(1,1)

GARMA(1,1)

tvAR(10)

TVBINGARCH(1,1)

T = 100

27.38

27.60

24.50

22.83

T = 500

24.02

24.07

22.90

21.23

T = 1000

23.23

23.32

22.93

21.19

18

Figure 1 to 3 shows that our proposed Bayesian method captures the true functions quite well
for both of the two simulation experiments. We find that the estimation accuracy improves as the
sample size increases. As the sample size grows, the 95% credible bands are also getting tighter,
implying lower uncertainty in estimation. This gives empirical evidence in favor of the estimation
consistency which has also been verified theoretically in Section 4. The average mean square error
(AMSE) is always the lowest for our method in Tables 1, 2 and 3.

6

COVID-19 spread at NYC

We collect the data of new affected cases for every day from 23rd January to 14th July from an

open-source platform {https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset}.
The end date 14th July is chosen as around that time NYC started the process of re-opening. The
data on daily new cases are illustrated in Figure 4. We were particularly interested in NYC
data as this city remained an epicenter in US for about a month. With the help of government
interventions and sustained lock-down, the recovery was significant in about 3 months. Such a
time-varying nature of the data motivated us to retrospect as how the mean trend and AR trend
behave which can also shed some insight about effects of lockdown or the contagious spread.
Based on the findings on the incubation of the virus in Lauer et al. (2020a) and others, it is
understood that the symptoms often take some time after the virus affects through contagion. Our
idea is to consider different models with varying number of lags for this. We consider TVBARC(1),
TVBARC(10) and TVBINGARCH(1,1) here. The results for the TVBARC(1) are illustrated in
Figure 5. We see that during the spike in daily new cases the function a1 (¬∑) is the highest. Figure 6
depicts the estimated mean and coefficient functions from a TVBARC(10) model. We find that
the estimated a1 (¬∑) functions show a similar trend. On top of that, we see that a6 (¬∑), a7 (¬∑) and
a8 (¬∑) have also some effect. Finally we fit our TVBINGARCH(1,1) which might be considered
TVBARC with infinite order. Figure 7 depicts the estimated functions, the mean {¬µ(¬∑)}, AR(1)
{a1 (¬∑)} and CH(1) {b1 (¬∑)} coefficient functions. In Table 4, we compare the AMSE scores across
different models. For all the models, we consider 12 equidistant knots based on the AMSE scores
as discussed in Section 5.
Figure 6 suggests that even lag 6, 7, and 8 have some significant contribution. The effect of
this lag is suppressed in Figure 5 and is expressed in terms of b1 (¬∑) of Figure 7. The estimated
19

Figure 4: Daily new COVID-19 cases from 31st January to 14th of July recorded at NYC.
mean functions also behave similarly for all the three cases. It shows a spike during the rise of
daily new cases. After that, it decreases which can talk about successful containment strategies
in NYC. More specifically it decreases after around 15 days since the strict implementation of
statewide lockdown on 20-th March. This is consistent with what was found in our unsubmitted
preprint (Roy and Karmakar, 2020) through an empirical early-stage analysis of the spread in
different cities and countries.
The effect of Lag 6, 7, and 8 can be attributed to the incubation period of the virus. It
can also lead to the finding that there was a weekly periodicity which is probably due to shorter
testing/administrative facilities being available during the weekend. Note that our choice of fitting
an TVBARC(10) model is more general than separately fitting a seasonal/periodic time-series
model. Another important finding is coming from the overall trend of a1 (¬∑). It starts to decrease
when the number of cases starts going down. However later on it varies around 0.6 can be
attributed to the fact that the number of new cases did not vary much and remained around the
same level from the middle of May. The credible bands look very small around the mean function
which is probably due to the large magnitude of the estimated function.

20

(a) NYC-¬µ(¬∑) function

(b) NYC-a1 (¬∑) function

Figure 5: Estimated mean functions in 1st column and estimated AR coefficient functions in
the 2nd column for NYC using TVBARC(1). Black is the estimated curve along with the 95%
pointwise credible bands in green for the mean and AR(1) function.

(a) NYC-¬µ(¬∑) function

(b) NYC-a(¬∑) functions

Figure 6: Estimated mean functions in 1st column and estimated AR coefficient functions in
the 2nd column for NYC using TVBARC(10). Black is the estimated curve along with the 95%
pointwise credible bands in green for the mean function.

21

Figure 7: Estimated coefficient functions for the TVBINGARCH(1,1) on NYC data. Black is the
estimated curve along with the 95% pointwise credible bands in green.
Table 4: Average MSE comparison for different methods on NYC data.

7

Method

AMSE

Method

INGARCH(1,1)

318056.3

GARMA(1,1)

329610.1

tvAR(1)

AR(10,0)

1376133.7

tvAR(10)

AMSE

GARMA(10,0) 1682976.1
338970.6

Method

AMSE

TVBARC(1) 210258.9
TVBARC(10) 185777.9

274913.7 TVBINGARCH(1,1) 212168.1

Discussion

We propose a time-varying Bayesian autoregressive model for counts (TVBARC) and time-varying
Bayesian integer-valued generalized autoregressive conditional heteroskedastic model (TVBINGARCH) with linear link function within Poisson error to study the time series of daily new
confirmed cases of COVID-19. We develop a novel hierarchical Bayesian model that satisfies
the stability condition for the respective time-varying models and propose an HMC algorithm
based MCMC sampling scheme. We also establish posterior contraction rate results of the proposed Bayesian methods.

The ‚ÄòR‚Äô function with an example code can be found at https:

//github.com/royarkaprava/TVBARC. Relying on the proposed hierarchical Bayesian model,
one can develop a time-varying Bayesian model for positive-valued time-series data too. Our analysis of NYC data shows that there is a time-varying effect of Lag 6, 7, and 8. Some preliminary
analysis on COVID data using our model based on the data until April 24 are archived in our
unpublished pre-print Roy and Karmakar (2020). There are some more interesting findings related
to significant lags for different countries.
The definition of posterior contraction rate we followed involves an diverging sequence MT ‚Üí
22

‚àû. If it is possible to replace MT by a large constant M without changing T , the contraction
rate then holds in a slightly stronger sense (see Chapter 8, Ghosal and Van der Vaart (2017)).
Local stationary approximation of the proposed nonstationary process is expected help to establish
such result. Establishing Bernstein von-Mises type theorem to ensure asymptotic normality of the
posterior distribution will also be interesting. However, such results are not yet available for the
corresponding stationary cases. Nevertheless, it is also important direction of future research.
As future work, it will be interesting to include some country-specific information such as
demographic information, geographical area, the effect of environmental time-series, etc in the
model. These are usually important factors for the spread of any infectious disease. We can
also categorize the different types of government intervention effects to elaborate more on the
specific impacts of the same. In the future we wish to analyze the number of deaths, number of
recovered cases, number of severe/critical cases, etc. for these diseases as those will hopefully have
different dynamics than the one considered here and can provide useful insights about the spread
and measures required. For computational ease, we have considered the same level of smoothness
for all the coefficient functions. Fitting this model with different levels of smoothness might
be able to provide more insights. Lag selection is a difficult task for time-varying auto-regressive
models. One potential future direction would be to put sparsity inducing prior to the time-varying
coefficient functions in TVBARC for automatic lag detection. Other than building time-varying
autoregressive models for count-valued data using the hierarchical structure from this article, one
interesting future direction is to extend this model for vector-valued count data. In general, it is
difficult to model multivariate count data. There are only a limited number of methods to deal
with multivariate count data (Besag, 1974; Yang et al., 2013; Roy and Dunson, 2019). Building
on these multivariate count data models, one can extend our time-varying univariate AR(p) to a
time-varying vector-valued AR(p). On the same note, even though we imposed Poisson assumption
for increased model interpretation, in the light of the upper bounds for the KL distance, it is not
a necessary criterion and can be applied to a general multiple non-stationary count time-series.
Extending some of the continuous time-series invariance results for nonlinear non-stationary and
multiple series from Karmakar and Wu (2020) to a count series regime will be an interesting
challenge. Finally, we wish to undertake an autoregressive estimation of the basic reproduction
number with the time-varying version of compartmental models in epidemiology.

23

8

Proof of Theorems

We study the frequentist property of the posterior distribution is increasing T regime assuming that
the observations are coming from a true density f0 characterized by the parameter Œ∫0 . We follow
the general theory of Ghosal et al. (2000) to study the posterior contraction rate for our problem.
In the Bayesian framework, the density f is itself a random measure and has distribution Œ† which is
the prior distribution induced by the assumed prior distribution on Œ∫. The posterior distribution of
a neighborhood UT = {f : d(f, f0 ) < T } around f0 given the observation X (T ) = {X0 , X1 , . . . , XT }
is

R
Œ†T (UTc |X (T ) )

8.1

UTc

= R

f (X (T ) )dŒ†(Œ∫)

f (X (T ) )dŒ†(Œ∫)

General proof strategy

The posterior consistency would hold if above posterior probability almost surely goes to zero in
(T )

(T )

FŒ∫0 probability as T goes to ‚àû, where FŒ∫0 is the true distribution of X (T ) . Recall the definition
(T )

of posterior contraction rate; for a sequence T if Œ†T (d(f, f0 )|X (T ) ‚â• MT T |X (T ) ) ‚Üí 0 in FŒ∫0 probability for every sequence MT ‚Üí ‚àû, then the sequence T is called the posterior contraction
rate. If the assertion is true for a constant MT = M , then the corresponding contraction rate
becomes slightly stronger.
Note that for two densities f0 , f characterized by Œ∫0 and Œ∫ respectively, the Kullback-Leibler
divergences are given by
Z
KL(Œ∫0 , Œ∫) =

#
"
Q
PQŒ∫0 (X0 ) Tt=1 PŒ∫0 (Xt |Ft‚àí1 , Œª0 )
f0
.
f0 log
= EŒ∫0 log
Q
f
PQŒ∫ (X0 ) Tt=1 PŒ∫ (Xt |Ft‚àí1 , Œª0 )

Assume that there exists a sieve in parameter space such that Œ†(WTc ) ‚â§ exp(‚àí(CT + 2)T 2T ) and
we have tests œáT such that
2

EŒ∫0 (œáT ) ‚â§ e‚àíLT T T /2
for some LT > CT + 2.

sup
Œ∫‚ààWT :d2 (f,f0 )>LT 2T

2

EŒ∫ (1 ‚àí œáT ) . e‚àíLT T T

R
Say UT = {f : d2 (f, f0 ) ‚â§ LT 2T } and ST = {

24

f (X T )
dŒ†(Œ∫)
f0 (X T )

‚â•

Œ†T ( T1 KL(Œ∫0 , Œ∫) < T ) exp(‚àíCT T 2T )}. We can bound the posterior probability from above by,
R
f (X T )dŒ†(Œ∫)
UTc
(T )
Œ†T (d(f, f0 ) ‚â• MT T |X ) ‚â§ œáT + (1 ‚àí œáT ) R
f (X (T ) )dŒ†(Œ∫)
R f (X (T ) )
dŒ†(Œ∫)
U c f0 (X (T ) )
= œáT + (1 ‚àí œáT ) R T f (X
(T ) )
dŒ†(Œ∫)
f0 (X (T ) )
R f (X (T ) )
dŒ†(Œ∫)
UTc f0 (X (T ) )
‚â§ œáT + 1{STc } + (1 ‚àí œáT )
exp(‚àíCT T 2T )Œ†T { T1 KL(Œ∫0 , Œ∫) < T }
R
2
f (X (T ) )
exp(C
T

)
UTc
T
T
c
dŒ†(Œ∫)
‚â§ œáT + 1{ST } +
(1 ‚àí œáT )
f0 (X (T ) )
Œ†T { T1 KL(Œ∫0 , Œ∫) < T }
(8.1)
Taking expectation with respect to Œ∫0 , first term go to zero by construction of œáT . The second
term EŒ∫0 1{STc } goes to zero due to Lemma 8.21 of Ghosal and Van der Vaart (2017) for any
sequence CT ‚Üí ‚àû. We would require that Œ†T { T1 KL(Œ∫0 , Œ∫) < T } ‚â• exp(‚àíT 2T ). Then for the
third term,
R
EŒ∫0 exp((CT +

1)T 2T )(1

‚â§ exp(CT + 1)T 2T )

‚àí œáT )

"Z

UTc

f0

f (X (T ) )
(X (T ) )

dŒ†(Œ∫) = exp((CT +

1)T 2T )

Z

f (X (T ) )(1 ‚àí œáT )dŒ†(Œ∫)

UTc

#

UTc ‚à©WT

f (X (T ) )(1 ‚àí œáT )dŒ†(Œ∫) + Œ†(WTc )

"

#

= exp((CT + 1)T 2T )

sup
Œ∫‚ààWT

:d2 (f,f

2
0 )>LT T

EŒ∫ (1 ‚àí œáT ) + Œ†(WTc ) . exp(‚àíT 2T ).

(8.2)

Thus we need three things to calculate posterior contraction rate.
(i) (Prior mass Condition) We would require Œ†T { T1 KL(Œ∫0 , Œ∫) < T } ‚â• exp(‚àíT 2T ),
(ii) (Sieve) construct the sieve WT such that Œ†(WTc ) ‚â§ exp(‚àí(CT + 2)T 2T ) and
(iii) (Test construction) exponentially consistent tests œáT .
We first study the contraction properties with respect to d2 (f, f0 ) = rT2 (f, f0 ) = ‚àí T1 log
and then show that the same rate holds for average Hellinger

1 2
d (f, f0 ).
T H

R‚àö

f f0

Note that LT can

be taken as LT = MT2 . With the above general structure, we now proceed to prove individual
theorems focusing on the TVBARC and the TVINGARCH cases.
25

8.2

Proof of Theorem 1

For the sake of technical convenience we show our proof for time-varying AR model with 1 lag
only. All the proofs go through for higher lags with the same technical tools.
8.2.1

KL Support

The likelihood based on the parameter space Œ∫ is given, PŒ∫ (X0 )

QT

t=1

PŒ∫ (Xt |Xt‚àí1 ). Let QŒ∫,t (Xt )

be the distribution of Xt with parameter space Œ∫.
We have
QT
PŒ∫0 (Xt |Ft‚àí1 , Œª0 )
R = log Qt=1
T
t=1 PŒ∫ (Xt |Ft‚àí1 , Œª0 )
T
X
=
[‚àí{¬µ0 (t/T ) ‚àí ¬µ(t/T )} ‚àí {a01 (t/T ) ‚àí a1 (t/T )}Xt‚àí1 + Xt {log(¬µ0 (t/T ) + a01 (t/T )Xt‚àí1 )
t=1

‚àí log(¬µ(t/T ) + a1 (t/T )Xt‚àí1 )}]

(8.3)

Then KL(Œ∫0 , Œ∫) = EŒ∫0 (R). We have in light of MVT,
|R| ‚â§

T
X

[|¬µ(t/T ) ‚àí ¬µ0 (t/T )| + |a1 (t/T ) ‚àí a01 (t/T )|Xt‚àí1

(8.4)

t=1

+

Xt
{|¬µ(t/T ) ‚àí ¬µ0 (t/T )| + |a1 (t/T ) ‚àí a01 (t/T )|Xt‚àí1 }]
¬µ‚àó (t/T ) + a1‚àó (t/T )Xt‚àí1
X
X
X
‚â§ T k¬µ ‚àí ¬µ0 k‚àû + ka1 ‚àí a01 k‚àû
Xt‚àí1 + k¬µ ‚àí ¬µ0 k‚àû /œÅ
Xt + ka1 ‚àí a01 k‚àû /œÅ
Xt ,
t

t

t

(8.5)
under the assumption that Œ∫(¬∑) = (¬µ(¬∑), a1 (¬∑), b1 (¬∑)) and Œ∫0 (¬∑) = (¬µ0 (¬∑), a10 (¬∑), b10 (¬∑)) are close and
also Œ∫‚àó is close to both and also in conjunction with Assumption (A.3) to imply inf t a1‚àó (t/T ) > œÅ
and Assumption (A.2) which implies E(Xt ) < MX . Then for the first term we use the bound
¬µ‚àó (t/T ) + a1‚àó (t/T )Xt‚àí1 > œÅ and for the second term the bound ¬µ‚àó (t/T ) + a1‚àó (t/T )Xt‚àí1 > œÅXt‚àí1
is used to have

|¬µ(t/T )‚àí¬µ0 (t/T )|+|a(t/T )‚àía(t/T )|Xt‚àí1
¬µ‚àó (t/T )+a1‚àó (t/T )Xt‚àí1

‚â§ k¬µ ‚àí ¬µ0 k‚àû /œÅ + ka1 ‚àí a01 k‚àû /œÅ for all t. Thus,

1
E(R) . k¬µ ‚àí ¬µ0 k‚àû + ka1 ‚àí a01 k‚àû .
T
8.2.2

(8.6)

Posterior contraction in terms of average negative log-affinity

In this section, we focus on the requirements to calculate posterior contraction rate as in Section 8.1.We first show posterior consistency in terms of average negative log-affinity which is de26

fined as rT2 (f1 , f2 ) = ‚àí T1 log

R

1/2 1/2

f1 f2

between f1 and f2 . Here, we have f1 =

QT

i=1

PŒ∫1 (Xi |Xi‚àí1 ).

Then we show that, having rT2 (f1 , f0 ) . 2n implies that our distance metric d22,T (f1 , f0 ) . 2n .
Proceeding with the rest of the proof of Theorem 1, we use the results of B-Splines, k¬µ‚àí¬µ0 k‚àû ‚â§
kŒ± ‚àí Œ±0 k‚àû , where Œ± = {Œ±j } and ka1 ‚àí a10 k‚àû ‚â§ kŒ≥ ‚àí Œ≥0 k‚àû , where Œ≥j = Œ∏1j M1 , such that Œ≥j < 1.
The HoÃàlder smooth functions with regularity Œπ can be approximately uniformly up to order K ‚àíŒπ
‚àíŒπ1
‚àíŒπ2
with K many B-splines. Thus we have T & max{K1T
, K2T
}.

We need to lower bound the prior probability as required by (i). We have the result (8.6) and
1T +K2T
the prior probabilities Œ†(kŒ± ‚àí Œ±0 k‚àû . T , kŒ≥ ‚àí Œ≥0 k‚àû . T ) & K
based on the discussion of
T

A2 from Shen and Ghosal (2015). The rate of contraction cannot be better than the parametric
rate T ‚àí1/2 , and so log(1/T ) . log T . Thus (i) requires that in terms of pre-rate ¬ØT , we need
(K1T + K2T ) log T . T ¬Ø2T .
In our problem, we consider following sieve as required by (ii)
WT = {K1 , K2 , Œ±, Œ≥ : K1 ‚â§ K1T , K2 ‚â§ K2T , kŒ±k‚àû ‚â§ AT , min(Œ±, Œ≥) > œÅT , Œ≥ ‚â§ 1 ‚àí AT /BT ,
Œª0 ‚â§ BT , AT < BT },

(8.7)

where AT , BT are at least polynomial in T and Œª0 is the mean of X0 and KT = max{K1T , K2T }.
We take œÅT  T ‚àía with a < 1, AT  T a1 , BT  T a2 with a2 > a1 for technical need. Note
that, for Œ∫ ‚àà WT , we have EŒ∫ (Xt ) < BT . We need to choose these bounds carefully so that
we have Œ†(WTc ) ‚â§ exp(‚àí(1 + C1 )T 2T ), which depend on tail properties of the prior. We have,
Œ†(WTc ) = Œ†[K1 > K1T , K2 > K2T , Œ±K1T ‚àà {x : inf x > œÅT , sup x < AT , Œ≥K2T ‚àà
/ {x : inf x >
œÅT , sup x < 1 ‚àí
h

AT
BT

}, Œª0 > BT ].

Hence we have, Œ†(WTc ) ‚â§ Œ†(K1 > K1T ) + Œ†(K2 > K2T ) + Œ†{Œ±K1T ‚àà
/ [œÅT , AT ]K1T } + Œ†{Œ≥K2T ‚àà
/
iK2T
AT
œÅT , 1 ‚àí B
} + Œ†{Œª0 > BT } where Œ±K1T is the vector of full set of coefficients of length
T

K1T and Œ≥K2T is the vector of coefficients of length K2T . The quantity Œ†[Œ±K1T ‚àà
/ [œÅT , AT ]K1T
can be further upper bounded by K1T Œ†(Œ±1 ‚àà
/ [œÅT , AT )]) ‚â§ K1T exp{‚àíR1 T a3 }, for some constant
R1 , a3 > 0 which can be verified from the discussion of the assumption A.2 of Shen and Ghosal
h
iK2T
AT
‚â§
(2015) for our choice of prior which exponential. On the other hand, Œ†{Œ≥K2T ‚àà
/ œÅT , 1 ‚àí BT
K2T Œ†(Œ≥1 ‚àà
/ [œÅT , 1 ‚àí

AT
BT

]) ‚â§ K2T exp{‚àíR2 T a4 } for some constant R2 , a4 > 0 which can be verified

from the proof of Roy et al. (2018). The inverse-gamma prior of Œª0 has exponential tail similar
to Œ±1 and thus can be ignored as K1T grows with T . Since BT > AT , the tail of Œª0 can be upper
bounded by tail of Œ±1
27

Hence, Œ†(WTc ) . F1 (K1T ) + F2 (K2T ) + (K1T + K2T ) exp{‚àíRT a5 }. The two functions F1 and F2
in the last expression stand for the tail probabilities of the prior of K1 and K2 . We can calculate
their asymptotic order as, F1 (x) = Œ†(K1 > x)  exp{‚àíx(log x)b13 } and F2 (x) = Œ†(K2 > x) 
exp{‚àíx(log x)b23 }. We need Œ†(WTc ) . exp{‚àí(1 + CT )T 2T }. Hence, we calculate pre-rate from the
following equation for some sequence HT ‚Üí ‚àû,

K1T (log T )b13 + K2T (log T )b23 & HT T ¬Ø2T ,

log(K1T + K2T ) + HT T ¬Ø2T . T a5 .

(8.8)

Now, we construct test œáT such that
2

EŒ∫0 (œáT ) ‚â§ e‚àíLT T T /2

sup
2 (Œ∫,Œ∫ )>L 2
Œ∫‚ààWT :rT
0
T T

2

EŒ∫ (1 ‚àí œáT ) . e‚àíLT T T

for some LT > CT + 2.
To construct the test as required in (iii), we first construct the test for point alternative
H0 : Œ∫ = Œ∫0 vs H1 : Œ∫ = Œ∫1 . The most powerful test for such problem is Neyman-Pearson test
œÜ1T = 1{f1 /f0 ‚â• 1}. For rT2 > LT 2T , we have
EŒ∫0 œÜ1T

Z p
p
= EŒ∫0 ( f1 /f0 ‚â• 1) ‚â§
f1 f0 ‚â§ exp(‚àíLT T 2T ),

Z p
p
f0 f1 ‚â§ exp(‚àíLT T 2T ).
EŒ∫1 (1 ‚àí œÜ1T ) = EŒ∫1 ( f0 /f1 ‚â• 1) ‚â§
It is natural to have a neighborhood around Œ∫1 such the Type II error remains exponentially
small for all the alternatives in that neighborhood under the test function œÜ1T . By Cauchy-Schwarz
inequality, we can write that
EŒ∫ (1 ‚àí œÜ1T ) ‚â§ {EŒ∫1 (1 ‚àí œÜ1T )}1/2 {EŒ∫1 (f /f1 )2 }1/2 .
In the above expression, the first factor already exponentially decaying. The second factor can be
2

allowed to grow at most of order ecT T for some positive small constant c. We show that EŒ∫1 (f /f1 )2
is bounded for every Œ∫ such that
k¬µ ‚àí ¬µ1 k‚àû

‚àö
‚àö
œÅT
œÅT
‚â§ ‚àö , ka ‚àí a1 k‚àû ‚â§ ‚àö
.
T BT
T

28

We have, in the light of AM-GM inequality,
2

Z

EŒ∫1 (f /f1 ) =

f2
f1 =
f12

Z


T
T
Y
f
f
f (Xt |Xt‚àí1 )
1X
f (Xt |Xt‚àí1 )
f = EŒ∫ = EŒ∫
‚â§
EŒ∫
f1
f1
f (Xt |Xt‚àí1 )
T t=1
f1 (Xt |Xt‚àí1 )
t=1 1

Towards uniformly bounding the summand in the above display, we write

EŒ∫

f (Xt |Xt‚àí1 )
f1 (Xt |Xt‚àí1 )

T

‚àû
X
{f (Xt |Xt‚àí1 )}T
= EXt‚àí1 ,Œ∫
f (Xt |Xt‚àí1 )
{f1 (Xt |Xt‚àí1 )}T
Xt =0
‚àû  T +1 Xt
X
Œª
/Xt !
= EXt‚àí1 ,Œ∫ exp[‚àíT (Œª ‚àí Œª1 ) ‚àí Œª]
T
Œª
1
X =0
t

ŒªT +1
= EXt‚àí1 ,Œ∫ exp[‚àíT (Œª ‚àí Œª1 ) ‚àí Œª + T ]
Œª1
"
= EXt‚àí1 ,Œ∫ exp ‚àí T {¬µ(t/T ) ‚àí ¬µ1 (t/T )} ‚àí T {a1 (t/T ) ‚àí a11 (t/T )}Xt‚àí1
T +1

‚àí ¬µ(t/T ) ‚àí a1 (t/T )Xt‚àí1 +

#

(¬µ(t/T ) + a1 (t/T )Xt‚àí1 )
. (8.9)
(¬µ1 (t/T ) + a11 (t/T )Xt‚àí1 )T

where, Œª = ¬µ(t/T ) + a1 (t/T )Xt‚àí1 ,Œª1 = ¬µ1 (t/T ) + a11 (t/T )Xt‚àí1 and EXt‚àí1 ,Œ∫ denotes unconditional
expectation over Xt‚àí1 under the density f with parameter Œ∫. Let us define r1 = k¬µ ‚àí ¬µ1 k‚àû and
r2 = ka1 ‚àí a11 k‚àû
Assuming ¬µ(t/T ) ‚àí ¬µ1 (t/T ) and a1 (t/T ) ‚àí a11 (t/T ) very small, we can write


(¬µ(t/T ) + a1 (t/T )Xt‚àí1 )T +1
(¬µ1 (t/T ) + a11 (t/T )Xt‚àí1 )T
T

¬µ(t/T ) ‚àí ¬µ1 (t/T ) + (a1 (t/T ) ‚àí a11 (t/T ))Xt‚àí1
= 1+
(¬µ(t/T ) + a1 (t/T )Xt‚àí1 )
¬µ1 (t/T ) + a11 (t/T )Xt‚àí1


¬µ(t/T ) ‚àí ¬µ1 (t/T ) + (a1 (t/T ) ‚àí a11 (t/T ))Xt‚àí1
‚âà 1+T
(¬µ(t/T ) + a1 (t/T )Xt‚àí1 )
¬µ1 (t/T ) + a11 (t/T )Xt‚àí1
For the above approximation to hold, we need

¬µ(t/T )‚àí¬µ1 (t/T )+(a1 (t/T )‚àía11 (t/T ))Xt‚àí1
¬µ1 (t/T )+a11 (t/T )Xt‚àí1

(8.10)

to be small. To

verify that, observe that
¬µ(t/T ) ‚àí ¬µ1 (t/T ) + (a1 (t/T ) ‚àí a11 (t/T ))Xt‚àí1
r1
r2
1
1
‚â§
+
=‚àö
(1 + ‚àö ).
¬µ1 (t/T ) + a11 (t/T )Xt‚àí1
œÅT
œÅT
T œÅT
BT
As we have œÅT = T ‚àía with a < 1, it follows directly. Thus (8.9) before EXt‚àí1 ,Œ∫ applying on (8.10)

29

becomes


[T {¬µ( Tt ) ‚àí ¬µ1 ( Tt )} + T {a1 ( Tt ) ‚àí a11 ( Tt )}Xt‚àí1 ][{¬µ( Tt ) ‚àí ¬µ1 ( Tt )} + {a1 ( Tt ) ‚àí a11 ( Tt )}Xt‚àí1 ]
exp
¬µ1 ( Tt ) + a11 ( Tt )Xt‚àí1
‚â§ exp[T r12 /œÅT + 2T r1 r2 /œÅT + T r22 Xt‚àí1 /œÅT ]

(8.11)

The bound in (8.11) is obtained by applying a combination of the following inequalities ¬µ(t/T ) +
a1 (t/T )Xt‚àí1 > œÅT or > œÅT Xt‚àí1 , |¬µ(t/T ) ‚àí ¬µ1 (t/T )| < r1 and |a1 (t/T ) ‚àí a11 (t/T )| < r2 . Taking
q = T r22 /œÅT , last part becomes E(eqXt‚àí1 ) after taking exectation over (8.11). We have E(eqX0 ) =
q ‚àí1)

eŒª0 (e

q ‚àí1)

< eBT (e

= eQ for Q = BT (eq ‚àí 1) =‚áí (eq ‚àí 1) = Q/BT , BT is the upper bound for Œª0

in the sieve). We will show E(eqX1 ) < Q under the above choice of r1 and r2 . Then by recursion
it holds for all t. We use the result eq ‚àí 1 ‚â§ 2q for q < 1.
With Œª1 (X0 ) = ¬µ(1) + a1 (1)X0 , we have
q ‚àí1)

E(eqX1 ) = E(E(eqX1 |X0 )) = E(eŒª1 (X0 )(e

) = e(e

q ‚àí1)¬µ(1)

(eq ‚àí1)a1 (1) ‚àí1)

eŒª0 (e

Then choose sieve parameters such that Qa1 /BT = a1 (1)(eq ‚àí 1) ‚â§ 2a1 (1)q is very small which
is ensured as q is very small. Then ¬µ(1)Q/BT + Œª0 (eQa1 (1)/BT ‚àí 1) ‚âà Q¬µ(1)/BT + Œª0 ( QaB1T(1) ) ‚â§
Q{¬µ(1)/BT + a1 (1)} < Q as within the sieve ¬µ(1)/BT + a1 (1) < AT /BT + (1 ‚àí AT /BT ) = 1.
Hence, E(eqX1 ) < eQ . Recursively, for all t, we can show E(eqXt ) < eQ .
Our primary goal of showing EŒ∫1 (f /f1 )2 < ‚àû can be fulfilled if Q is a constant, independent
of T . To ensure Q is independent of T we need BT (eq ‚àí 1) is constant. It suffices to make qBT
constant as qBT < BT (eq ‚àí 1) < 2qBT . Thus, for r2 ‚â§
 2
we have EŒ∫1 ff1 bounded.

‚àö
œÅ
‚àö T
T BT

and in the light of (8.11) r1 ‚â§

‚àö
œÅ
‚àöT
T

The test function œáT satisfying exponentially decaying Type I and Type II probabilities is
then obtained by taking maximum over all tests œÜjT ‚Äôs for each ball, having above radius. Thus
P
œáT = maxj œÜjT . Type I and Type II probabilities are given by P0 (œáT ) ‚â§ j P0 œÜjT ‚â§ DT P0 œÜjT
and supŒ∫‚ààWT :rT2 (Œ∫,Œ∫0 )>LT 2T P (1 ‚àí œáT ) ‚â§ exp(‚àíT LT 2T ). Hence, we need to show that log DT . T 2T ,
where DT is the required number of balls of above radius needed to cover our sieve WT . We have
log DT ‚â§ log D(r1 , kŒ±k‚àû ‚â§ AT , min(Œ±) > œÅT , k ¬∑ k‚àû ) + log D(r2 , kŒ≥k‚àû ‚â§ 1 ‚àí
‚â§ K1T log(3K1T AT /r1 ) + K2T log(3K2T /r2 )

AT
, min(Œ≥) > œÅT , k ¬∑ k‚àû )
BT
(8.12)

Given our choices of AT , BT and œÅT , the two radii r1 and r2 are some fractional polynomials in T .
30

Thus log DT . (K1T + K2T ) log T , which is required to be . T 2T as in the prior mass condition
due to (i).
1/(2Œπ1 +1)
Based on (8.8), we have
(log T )‚àí1/(2Œπ1 +1) , K2T  T 1/(2Œπ2 +1)
(log T )‚àí1/(2Œπ2 +1)
 KÃÑ1T  T
and a pre-rate ¬ØT = max T ‚àíŒπ1 /(2Œπ1 +1) (log T )Œπ1 /(2Œπ1 +1) , T ‚àíŒπ2 /(2Œπ2 +1) (log T )Œπ2 /(2Œπ2 +1) . The actual

rate will be slower that pre-rate. Now, the covering number condition, prior mass conditions
‚àíŒπ1
‚àíŒπ2
and basis approximation result give us (K1T + K2T ) log T . T 2T and T & max{K1T
, K2T
}.

Combining all these conditions, we would require K1T  T 1/(2Œπ1 +1) (log T )2Œπ1 /(2Œπ1 +1)‚àíb13 , K2T 
T 1/(2Œπ2 +1) (log T )2Œπ2 /(2Œπ2 +1)‚àíb23 . Hence we calculate the posterior contraction rate as T equal to


‚àíŒπ1 /(2Œπ1 +1)
Œπ1 /(2Œπ1 +1)+(1‚àíb13 )/2
‚àíŒπ2 /(2Œπ2 +1)
Œπ2 /(2Œπ2 +1)+(1‚àíb23 )/2
.
max T
(log T )
,T
(log T )
8.2.3

Posterior contraction in terms of average Hellinger

We can write Reyni divergence as rT2 = ‚àí T1 log

R‚àö

f0 f1 = ‚àí T1 log EŒ∫0

q

f1
.
f0

We need to show

rT2 . 2T implies that d22,T (Œ∫0 , Œ∫) . 2T as T goes to zero.
 q ‚àí1/T
 q 1/T
If rT2 ‚â§ 2T , we have EŒ∫0 ff10
‚â§ exp(2T ) which implies for small 2T , we have EŒ∫0 ff10
‚â•

R
R
R
‚àö
2
f0 f1 ‚â§ f0 f = 1. Thus we have,
1 ‚àí 2T . By Cauchy-Squarz inequality
s
1 ‚àí 2T ‚â§
Since d2H (f1 , f0 ) = 2(1 ‚àí EŒ∫0
s
EŒ∫0
Thus

8.3

1 2
d (f , f )
T H 1 0

f1
f0

q

!1/T

EŒ∫0

f1
f0

!1/T
‚â§ 1,

f1
)
f0

s

(
=

1‚àí

1 ‚àí EŒ∫0

f1
f0

!)1/T
‚âà1‚àí

1 2
d (f1 , f0 ).
2T H

. 2T . Thus it is consistent under average Hellinger distance.

Proof of Theorem 2

The proof will follow similar path as in the previous section. Thus we just specifically touch upon
the parts that require different treatment. We can rewrite history of the INGARCH process as
{Ft‚àí1 , Gt‚àí1 } = {Ft‚àí1 , Œª0 }. For the INGARCH case, the likelihood based on the parameter space
Q
Œ∫ is different from above and is given by, Pœà0 (X0 , Œª0 ) Tt=1 Pœà (Xt |Ft‚àí1 , Œª0 ). Since all the steps are

31

similar for the proof of Theorem 2, we only provide a outline. First to bound KL by the sup norm
distances among functions, we need to tackle |b11 (t/T )Œª1t ‚àí b01 (t/T )Œª0t |. For this term we have
|b11 (t/T )Œª1t ‚àí b01 (t/T )Œª0t | ‚â§ Œª0t kb11 ‚àí b01 k‚àû + max b11 (t)|Œª1t ‚àí Œª0t |.
t

(8.13)

When œà1 is near œà0 , we have for all t
|Œª1t ‚àí Œª0t | ‚â§ k¬µ1 ‚àí ¬µ0 k‚àû + Xt‚àí1 ka11 ‚àí a01 k‚àû + (1 ‚àí
as we can upper bound maxt b11 (t) by (1 ‚àí

M¬µ
)
MX

M¬µ
)|Œª1,t‚àí1 ‚àí Œª0,t‚àí1 | + Œª0,t‚àí1 |b11 ‚àí b01 |‚àû
MX

since œà1 is close to œà0 . We have

T ‚àí1
X
M¬µ
|Œª1t ‚àí Œª0t | + |Œª1T ‚àí Œª0T |
M
X
t=1
X
X
M¬µ
‚â§ T k¬µ1 ‚àí ¬µ0 k‚àû +
Xt‚àí1 ka11 ‚àí a01 k‚àû + (1 ‚àí
)|Œª10 ‚àí Œª00 | +
Œª0,t‚àí1 |b11 ‚àí b01 |‚àû
M
X
t
t

As M¬µ < MX ,
T
X

|Œª1t ‚àí Œª0t | ‚â§

t=1

X
MX
Xt‚àí1 ka11 ‚àí a01 k‚àû
{T k¬µ1 ‚àí ¬µ0 k‚àû +
M¬µ
t
X
M¬µ
+ (1 ‚àí
Œª0,t‚àí1 |b11 ‚àí b01 |‚àû }.
)|Œª10 ‚àí Œª00 | +
MX
t

which implies,
E

T
X

|Œª1t ‚àí Œª0t | ‚â§

t=1

MX
{T k¬µ1 ‚àí ¬µ0 k‚àû + T MX ka11 ‚àí a01 k‚àû
M¬µ
+ (1 ‚àí

M¬µ
)|Œª10 ‚àí Œª00 | + T MX |b11 ‚àí b01 |‚àû }.
MX

Using the definition of R as in (8.3), we have

T 
X
Xt
|Œª1t ‚àí Œª0t |
|R| ‚â§
|Œª1t ‚àí Œª0t | +
¬µ
(t/T
)
+
a
(t/T
)X
‚àó
1‚àó
t‚àí1
t=1

(8.14)

(8.15)

The first part follows directly. For the second part as œà1 and œà0 are close
 X
X   Xt
MX
MX X
E E
|Œª1t ‚àí Œª0t | Ft
‚â§
E(|Œª1t ‚àí Œª0t |) =
E(
|Œª1t ‚àí Œª0t |).
Œª‚àót
œÅ
œÅ
t
t
t

|

Thus E( R
) can again be bounded by sup-norm differences in functions as before and |Œª10 ‚àí Œª00 |
T
using (8.14). Next, we need to construct a sieve and construct tests. We consider similar sieve
WT = {K1 , K2 , K3 Œ±, Œ≥1 , Œ≥2 : K1 ‚â§ K1T , K2 ‚â§ K2T , K3 ‚â§ K3T , kŒ±k‚àû ‚â§ AT , min(Œ±, Œ≥1 , Œ≥2 ) > œÅT ,
max Œ≥1 + max Œ≥2 ‚â§ 1 ‚àí AT /BT , Œª0 ‚â§ BT },
32

(8.16)

as in the previous problem. Within the sieve, we have EEt‚àí1 (max(Xt , Œªt )) < BT . Here the extra
terms such as K3 stands for number of basis in b1 (t) and the vectors Œ≥1 and Œ≥2 correspond to
the B-spline coefficients of the functions a1 (t) and b1 (t) respectively. Also note that we now
have a lower bound for AT for technical need. We take œÅT ‚âà T ‚àía with a < 1, AT = BT (1 ‚àí
exp(log T /T )œÅT ), BT ‚âà T a2 for sufficiently large T such that exp(log T /T )œÅT < 1. Within the
sieve again we use a variant of above inequality. Note that within the sieve E(Xt ) ‚â§ BT and
E(Œªt ) ‚â§ BT .
We have that,
|Œª1t ‚àí Œªt | ‚â§ k¬µ1 ‚àí ¬µk‚àû + Xt‚àí1 ka11 ‚àí a1 k‚àû + (1 ‚àí

AT
)|Œª1,t‚àí1 ‚àí Œªt‚àí1 | + Œªt‚àí1 |b11 ‚àí b01 |‚àû
BT

(8.17)

and also,

|Œªt ‚àí Œª1t |
1
1
1 ‚àí AT /BT |Œªt‚àí1 ‚àí Œª1,t‚àí1 |
1
‚â§
k¬µ ‚àí ¬µ1 k‚àû + ka1 ‚àí a11 k‚àû +
+ kb1 ‚àí b11 k‚àû
Œªt
œÅT
œÅT
œÅT
Œªt‚àí1
œÅT
By recursion,
|Œªt ‚àí Œª1t |
GtT ‚àí 1
Gt‚àí1
‚â§
[k¬µ ‚àí ¬µ1 k‚àû + ka1 ‚àí a11 k‚àû + kb1 ‚àí b11 k‚àû ] + T |Œª0 ‚àí Œª01 |,
Œªt
(GT ‚àí 1)œÅT
œÅT
where GT =

1‚àíAT /BT
œÅT

(8.18)

> 1. Since RHS is increasing in t and we only need to find a bound for t = T .

If AT , BT and œÅT are chosen in such a way that GT  exp(log T /T ), then GTT  T . Based on that
r1 , r2 , r3 and r4 can be chosen. For sufficiently large T (> 1/a) we have (1 ‚àí exp(log T /T )œÅT ) < 1.
Let us assume that k¬µ ‚àí ¬µ1 k‚àû = r1 , ka ‚àí a1 k‚àû = r2 , kb ‚àí b1 k‚àû = r3 , |Œª0 ‚àí Œª01 | = r4 . Then for
œÅT
,
T 1+a3

|Œªt ‚àíŒª1t |
Œªt

‚â§ 1/T a3 for all t with a3 > 0. The choice of a3 is shown later.
 2
Next goal is to find the radii for which Eœà ff1 is bounded. Similar steps as before first give us
 2

T
P
t |Ft‚àí1 ,Œª0 )
Eœà1 ff1 ‚â§ T1 t=1 Eœà ff1(X
and then the following,
(Xt |Ft‚àí1 ,Œª0 )
ri ‚â§



we have that

T

T (Œª1t ‚àí Œªt )(Œª1t ‚àí Œªt )
Œªt
3
‚â§ Eœà exp(T 1‚àía |Œª1t ‚àíŒªt |) ‚â§ Eœà exp 2a3 ‚àí1 .
Œªt
T




We have by Jensen‚Äôs inequality, Eœà exp T 2aŒª3t ‚àí1 ‚â§ Eœà exp T 2aX3t‚àí1 as Œªt = Eœà (Xt |Ft‚àí1 , Œª0 ). We
Eœà

f (Xt |Ft‚àí1 , Œª0 )
f1 (Xt |Ft‚àí1 , Œª0 )

‚âà Eœà exp

can again show by induction that within the sieve E(eqXt ) < eQ for some constant Q following
similar argument with q = T 1‚àí2a3 . We again need qBT independent of T . Hence our choice for a3

33

will be a3 =

1+a2
2

> 1/2. Thus q is small for sufficiently large T and hence eq ‚àí 1 ‚âà q. We have

from MGF of Poisson,
E(eqXt ) = E(exp{Œªt (eq ‚àí 1))) ‚âà E(exp(¬µ(t)q + a1 (t)Xt‚àí1 q) + b1 (t)Œªt‚àí1 q}
= E(Et‚àí1 (exp{¬µ(t)q + a1 (t)Xt‚àí1 q})(exp{b1 (t)Et‚àí1 (Xt‚àí1 )q}))
‚â§ E(Et‚àí1 (exp{¬µ(t)q + a1 (t)Xt‚àí1 q})Et‚àí1 (exp{b1 (t)Xt‚àí1 q}))
‚â§ E(Et‚àí1 (exp{¬µ(t)q + a1 (t)Xt‚àí1 q + b1 (t)Xt‚àí1 q}))
= E(exp{¬µ(t)q + (a1 (t) + b1 (t))Xt‚àí1 q}),

(8.19)

by first Jensen‚Äôs inequality as Œªt = Eœà (Xt |Ft‚àí1 , Œª0 ) and positive correlation between exp{a1 (t)Xt‚àí1 q}
and exp{b1 (t)Xt‚àí1 q} under the expectation Et‚àí1 . For two positively correlated random variables
Y and Z under the sample space, we have E(Y Z) > E(Y )E(Z). Now using this recurrence result
(8.19) of E(eqXt ), we again arrive at similar type of bounds for r1 ‚â§

‚àö
œÅ
‚àöT
T

, r2 ‚â§

‚àö
œÅ
‚àö T
T BT

to ensure that

E(eqXt ) < eQ for some constant Q for all t. We also need that r4  r1 , r3  r2 , where  means
‚àö

œÅ

‚àö

œÅ

œÅT
œÅT
asymptotically equivalent. Finally we need r1 ‚â§ min{ ‚àöTT , T 1+a
} and r2 ‚â§ min{ ‚àöT BTT , T 1+a
} and
3
3

r4  r1 , r3  r2 . These radii are also of polynomial order in T . Rest of the pieces of the proof
follow similar arguments as before.

References
Ahmad, A., and Francq, C. (2016), ‚ÄúPoisson QMLE of count time series models,‚Äù Journal of Time
Series Analysis, 37(3), 291‚Äì314. 3
Amorim, L. D., Cai, J., Zeng, D., and Barreto, M. L. (2008), ‚ÄúRegression splines in the timedependent coefficient rates model for recurrent event data,‚Äù Statistics in medicine, 27(28), 5890‚Äì
5906. 3
Besag, J. (1974), ‚ÄúSpatial interaction and the statistical analysis of lattice systems,‚Äù Journal of
the Royal Statistical Society. Series B (Methodological), pp. 192‚Äì236. 23
Biller, C., and Fahrmeir, L. (2001), ‚ÄúBayesian varying-coefficient models using adaptive regression
splines,‚Äù Statistical Modelling, 1(3), 195‚Äì211. 3

34

Biswas, A., and Song, P. X.-K. (2009), ‚ÄúDiscrete-valued ARMA processes,‚Äù Statistics & probability
letters, 79(17), 1884‚Äì1889. 1
Brandt, P. T., and Williams, J. T. (2001), ‚ÄúA linear Poisson autoregressive model: The Poisson
AR (p) model,‚Äù Political Analysis, 9(2), 164‚Äì184. 1, 6
Cai, Z., Fan, J., and Yao, Q. (2000), ‚ÄúFunctional-coefficient regression models for nonlinear time
series,‚Äù Journal of the American Statistical Association, 95(451), 941‚Äì956. 3
Chan, K., and Ledolter, J. (1995), ‚ÄúMonte Carlo EM estimation for time series models involving
counts,‚Äù Journal of the American Statistical Association, 90(429), 242‚Äì252. 1
Dahlhaus, R. (2012), ‚ÄúLocally stationary processes,‚Äù , 30, 351‚Äì413. 8
Dahlhaus, R., and Subba Rao, S. (2006), ‚ÄúStatistical inference for time-varying ARCH processes,‚Äù
Ann. Statist., 34(3), 1075‚Äì1114. 7, 8, 9, 10
Dahlhaus, R. et al. (1997), ‚ÄúFitting time series models to nonstationary processes,‚Äù Annals of
Statistics, 25(1), 1‚Äì37. 8
Dahlhaus, R. et al. (2000), ‚ÄúA likelihood approximation for locally stationary processes,‚Äù The
Annals of Statistics, 28(6), 1762‚Äì1794. 8
Davis, R. A., Dunsmuir, W. T., and Streett, S. B. (2003), ‚ÄúObservation-driven models for Poisson
counts,‚Äù Biometrika, 90(4), 777‚Äì790. 1
Davis, R. A., and Mikosch, T. (2009), ‚ÄúExtreme value theory for GARCH processes,‚Äù , pp. 187‚Äì
200. 5
DeYoreo, M., and Kottas, A. (2017), ‚ÄúA Bayesian nonparametric Markovian model for nonstationary time series,‚Äù Statistics and Computing, 27(6), 1525‚Äì1538. 8, 10
Fan, J., and Zhang, W. (2008), ‚ÄúStatistical methods with varying coefficient models,‚Äù Statistics
and its Interface, 1(1), 179. 3
Ferland, R., Latour, A., and Oraichi, D. (2006), ‚ÄúInteger-valued GARCH process,‚Äù Journal of
Time Series Analysis, 27(6), 923‚Äì942. 3, 5
35

Ferreira, G., Navarrete, J. P., Rodrƒ±ÃÅguez-CorteÃÅs, F. J., and Mateu, J. (2017), ‚ÄúEstimation and prediction of time-varying GARCH models through a state-space representation: a computational
approach,‚Äù Journal of Statistical Computation and Simulation, 87(12), 2430‚Äì2449. 5
Fokianos, K., Rahbek, A., and Tj√∏stheim, D. (2009), ‚ÄúPoisson autoregression,‚Äù Journal of the
American Statistical Association, 104(488), 1430‚Äì1439. 2
Fokianos, K., and Tj√∏stheim, D. (2011), ‚ÄúLog-linear Poisson autoregression,‚Äù Journal of Multivariate Analysis, 102(3), 563‚Äì578. 2
Franco-Villoria, M., Ventrucci, M., Rue, H. et al. (2019), ‚ÄúA unified view on Bayesian varying
coefficient models,‚Äù Electronic Journal of Statistics, 13(2), 5334‚Äì5359. 3
Fryzlewicz, P., Sapatinas, T., Rao, S. S. et al. (2008), ‚ÄúNormalized least-squares estimation in
time-varying ARCH models,‚Äù Annals of Statistics, 36(2), 742‚Äì786. 11
Fryzlewicz, P., Sapatinas, T., and Subba Rao, S. (2008), ‚ÄúNormalized least-squares estimation in
time-varying ARCH models,‚Äù Ann. Statist., 36(2), 742‚Äì786. 7
Ghosal, S., Ghosh, J. K., Van Der Vaart, A. W. et al. (2000), ‚ÄúConvergence rates of posterior
distributions,‚Äù Annals of Statistics, 28(2), 500‚Äì531. 24
Ghosal, S., and Van der Vaart, A. (2017), ‚ÄúFundamentals of nonparametric Bayesian inference,‚Äù
, 44. 3, 12, 13, 23, 25
Ghosal, S., Van Der Vaart, A. et al. (2007), ‚ÄúConvergence rates of posterior distributions for noniid
observations,‚Äù Annals of Statistics, 35(1), 192‚Äì223. 3
Gu, C., and Wahba, G. (1993), ‚ÄúSmoothing spline ANOVA with component-wise Bayesian ‚Äúconfidence intervals‚Äù,‚Äù Journal of Computational and Graphical Statistics, 2(1), 97‚Äì117. 3
Hadj-Amar, B., Rand, B. F., Fiecas, M., LeÃÅvi, F., and Huckstepp, R. (2020), ‚ÄúBayesian Model
Search for Nonstationary Periodic Time Series,‚Äù Journal of the American Statistical Association,
115(531), 1320‚Äì1335. 9
Hastie, T., and Tibshirani, R. (1993), ‚ÄúVarying-coefficient models,‚Äù Journal of the Royal Statistical
Society: Series B (Methodological), 55(4), 757‚Äì779. 3
36

Huang, J. Z., and Shen, H. (2004), ‚ÄúFunctional coefficient regression models for non-linear time
series: a polynomial spline approach,‚Äù Scandinavian journal of statistics, 31(4), 515‚Äì534. 3
Huang, J. Z., Wu, C. O., and Zhou, L. (2002), ‚ÄúVarying-coefficient models and basis function
approximations for the analysis of repeated measurements,‚Äù Biometrika, 89(1), 111‚Äì128. 3
Jeong, S. et al. (2019), ‚ÄúFrequentist Properties of Bayesian Procedures for High-Dimensional
Sparse Regression.,‚Äù , . 3
Karmakar, S. (2018), Asymptotic Theory for Simultaneous Inference Under Dependence,, Technical report, University of Chicago. 4
Karmakar, S., Richter, S., and Wu, W. B. (2020+), ‚ÄúSimultaneous inference for time-varying
models,‚Äù In revision, https: // sayarkarmakar. github. io/ publications/ sayar1. pdf , .
2, 4, 7
Karmakar, S., and Wu, W. B. (2020), ‚ÄúOptimal Gaussian Approximation for Multiple Time
Series,‚Äù Statistica Sinica, 30(3), 1399‚Äì1417. 23
Lauer, S. A., Grantz, K. H., Bi, Q., Jones, F. K., Zheng, Q., Meredith, H. R., Azman, A. S., Reich,
N. G., and Lessler, J. (2020a), ‚ÄúThe incubation period of coronavirus disease 2019 (COVID19) from publicly reported confirmed cases: estimation and application,‚Äù Annals of internal
medicine, . 19
Lauer, S. A., Grantz, K. H., Bi, Q., Jones, F. K., Zheng, Q., Meredith, H. R., Azman, A. S.,
Reich, N. G., and Lessler, J. (2020b), ‚ÄúThe Incubation Period of Coronavirus Disease 2019
(COVID-19) From Publicly Reported Confirmed Cases: Estimation and Application,‚Äù Annals
of Internal Medicine, .
URL: https://doi.org/10.7326/M20-0504 2
Neal, R. M. et al. (2011), ‚ÄúMCMC using Hamiltonian dynamics,‚Äù Handbook of Markov Chain
Monte Carlo, 2(11), 2. 9
Ning, B., Jeong, S., Ghosal, S. et al. (2020), ‚ÄúBayesian linear regression for multivariate responses
under group sparsity,‚Äù Bernoulli, 26(3), 2353‚Äì2382. 3, 13

37

Rohan, N., and Ramanathan, T. (2013), ‚ÄúNonparametric estimation of a time-varying GARCH
model,‚Äù Journal of Nonparametric Statistics, 25(1), 33‚Äì52. 5, 9
Rosen, O., Stoffer, D. S., and Wood, S. (2009), ‚ÄúLocal spectral analysis via a Bayesian mixture of
smoothing splines,‚Äù Journal of the American Statistical Association, 104(485), 249‚Äì262. 8
Rosen, O., Wood, S., and Stoffer, D. S. (2012), ‚ÄúAdaptSPEC: Adaptive spectral estimation for
nonstationary time series,‚Äù Journal of the American Statistical Association, 107(500), 1575‚Äì
1589. 8
Roy, A., and Dunson, D. B. (2019), ‚ÄúNonparametric graphical model for counts,‚Äù arXiv preprint
arXiv:1901.00886, . 23
Roy, A., Ghosal, S., Choudhury, K. R. et al. (2018), ‚ÄúHigh Dimensional Single-Index Bayesian
Modeling of Brain Atrophy,‚Äù Bayesian Analysis, . 27
Roy, A., and Karmakar, S. (2020), ‚ÄúBayesian semiparametric time varying model for count data
to study the spread of the COVID-19 cases,‚Äù arXiv preprint arXiv:2004.02281, . 20, 22
Shen, W., and Ghosal, S. (2015), ‚ÄúAdaptive Bayesian procedures using random series priors,‚Äù
Scandinavian Journal of Statistics, 42(4), 1194‚Äì1213. 7, 27
Silveira de Andrade, B., Andrade, M. G., and Ehlers, R. S. (2015), ‚ÄúBayesian GARMA models
for count data,‚Äù Communications in Statistics: Case Studies, Data Analysis and Applications,
1(4), 192‚Äì205. 2
Truquet, L. et al. (2019), ‚ÄúLocal stationarity and time-inhomogeneous Markov chains,‚Äù Annals of
Statistics, 47(4), 2023‚Äì2050. 8
Yang, E., Ravikumar, P. K., Allen, G. I., and Liu, Z. (2013), On Poisson graphical models,, in
Advances in Neural Information Processing Systems, pp. 1718‚Äì1726. 23
Yang, H.-C., and Bradley, J. R. (2020), ‚ÄúBayesian inference for big spatial data using nonstationary spectral simulation,‚Äù arXiv preprint arXiv:2001.06477, . 9
Yue, Y. R., Simpson, D., Lindgren, F., Rue, H. et al. (2014), ‚ÄúBayesian adaptive smoothing splines
using stochastic differential equations,‚Äù Bayesian Analysis, 9(2), 397‚Äì424. 3
38

Zeger, S. L. (1988), ‚ÄúA regression model for time series of counts,‚Äù Biometrika, 75(4), 621‚Äì629. 1,
6
Zhu, F. (2011), ‚ÄúA negative binomial integer-valued GARCH model,‚Äù Journal of Time Series
Analysis, 32(1), 54‚Äì67. 2, 3
Zhu, F. (2012a), ‚ÄúModeling overdispersed or underdispersed count data with generalized Poisson integer-valued GARCH models,‚Äù Journal of Mathematical Analysis and Applications,
389(1), 58‚Äì71. 2, 3
Zhu, F. (2012b), ‚ÄúModeling time series of counts with COM-Poisson INGARCH models,‚Äù Mathematical and Computer Modelling, 56(9-10), 191‚Äì203. 2, 3
Zhu, F. (2012c), ‚ÄúZero-inflated Poisson and negative binomial integer-valued GARCH models,‚Äù
Journal of Statistical Planning and Inference, 142(4), 826‚Äì839. 2, 3

39

