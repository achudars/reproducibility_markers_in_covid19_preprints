arXiv:2009.04607v2 [cs.LG] 12 Sep 2020

Multi-Objective Reinforcement Learning for
Infectious Disease Control with Application to
COVID-19 Spread

Runzhe Wan, Xinyu Zhang, Rui Song
Department of Statistics, North Carolina State University
rwan@ncsu.edu, xzhang97@ncsu.edu, rsong@ncsu.edu

Abstract
Severe infectious diseases such as the novel coronavirus (COVID-19) pose a huge
threat to public health. Stringent control measures, such as school closures and stayat-home orders, while having significant effects, also bring huge economic losses.
A crucial question for policymakers around the world is how to make the trade-off
and implement the appropriate interventions. In this work, we propose a MultiObjective Reinforcement Learning framework to facilitate the data-driven decision
making and minimize the long-term overall cost. Specifically, at each decision
point, a Bayesian epidemiological model is first learned as the environment model,
and then we use the proposed model-based multi-objective planning algorithm to
find a set of Pareto-optimal policies. This framework, combined with the prediction
bands for each policy, provides a real-time decision support tool for policymakers.
The application is demonstrated with the spread of COVID-19 in China.

1

Introduction

The novel coronavirus (COVID-19) has spread rapidly and posed a tremendous threat to the global
public health (Organization et al. 2020). Among the efforts to contain its spread, several strict
control measures, including school closures and workplace shutdowns, have shown high effectiveness
(Anderson et al. 2020). Nevertheless, these measures always bring enormous costs to economies
and other public welfare aspects (Eichenbaum, Rebelo, and Trabandt 2020). For example, an
unprecedented unemployment rate in the United States partially caused by some COVID-19 control
measures is anticipated by economists (Gangopadhyaya and Garrett 2020). In the face of an emerging
infectious disease, we usually have multiple objectives conflicting with each other, and either
overreaction or under-reaction may result in a substantial unnecessary loss. A crucial question for
policymakers around the world is how to make the trade-off and intervene at the right time and in the
right amount to minimize the overall long-term cost to the citizens.
Contribution. This paper aims to provide a real-time data-driven decision support tool for policymakers. We formalize the problem under the multi-objective Markov decision process (MOMDP)
framework. Our contributions are multi-fold. First, as our transition model, we generalize the
celebrated Susceptible-Infected-Removal (SIR) model (Kermack and McKendrick 1927) to allow
simultaneous estimation of the infectious ability of this disease and evaluation for the effectiveness of
different control measures, in an online fashion. There is a vast literature on modeling and prediction
for infectious diseases; see (Keeling and Rohani 2011) for an overview. Among these work, compartmental models such as the SIR model are widely used; see, e.g., (Song et al. 2020) and (Sun et al.
2020) for applications to COVID-19. During an outbreak, the knowledge about the infectious ability
and the effectiveness of interventions usually change a lot, and a quantitative decision making support
tool should utilize the update-to-date information timely. However, none of these works considers
Preprint. Under review.

online parameter updating and intervention effect estimation at the same time. We aim to achieve this
goal via an online Bayesian framework.
Second, we propose a novel online planning framework to assist policymakers in making decisions to
minimize the overall long-term cost. In reality, policymakers generally group different interventions
into several ordered levels with increasing strictness and then choose among them (e.g., states in the
U.S. (Korevaar et al. 2020) and New Zealand (Wilson 2020)). This paper will focus on selecting
among such a set of ordinal actions. Specifically, at each decision point, on the ground of the
estimated generalized SIR model, we propose a model-based planning algorithm to find the optimal
intervention policy among an interpretable class, for each given weight between the two competing
objectives, the epidemiological cost and the economic cost. We then extend the algorithm to obtain a
representative set of Pareto-optimal policies, i.e., policies that cannot be improved for one objective
without sacrificing another. This framework, combined with the prediction bands for each policy,
achieves the goal of supporting multi-objective decision making. Compared with the huge literature
on infectious disease modeling and prediction, the optimal decision-making problem is much less
studied. Most works either focus on the evaluation of several fixed interventions (Tildesley et al.
2006; Ferguson et al. 2020; Hellewell et al. 2020) or study the optimal control problem with a
deterministic model (Ledzewicz and Schättler 2011; Elhia, Rachik, and Benlahmar 2013). None
of these works allows sequential decision making with online updated parameter estimation, which
enables selecting the appropriate intervention according to the current state and available data. In
addition, the long-term effect is particularly important in this application due to the spread nature
of the pandemic. Reinforcement learning (RL) is particularly suitable for these purposes, while
its application in infectious disease control is relatively new to the literature. The existing works
(e.g., (Probert et al. 2018), (Laber et al. 2018), (Zlojutro, Rey, and Gardner 2019), etc.) are mainly
concerned with the problem caused by limited resources. In contrast, we focus on another aspect,
the multi-objective problem caused by the huge costs of stringent control measures. This approach
provides us with a clearer view of the trade-off. To our knowledge, this is the first work about
applications of multi-objective RL to infectious disease control.
Third, an application of our method to control the outbreak of COVID-19 in China is presented as
an example. This application is important in its own right. Our proposed framework is generally
applicable to infectious disease pandemics.
Related work. In addition to the literature on infectious disease-related modeling, prediction, and
decision making, our methodology also belongs to the field of reinforcement learning. Our problem
is closely related to the line of research on MOMDPs (see (Roijers et al. 2013) or (Liu, Xu, and Hu
2014) for a survey), which studies problems with multiple competing objectives. When the weight
between objectives is unknown, the literature focuses on approximately obtaining the whole class of
Pareto-optimal policies (Castelletti, Pianosi, and Restelli 2012; Barrett and Narayanan 2008; Parisi,
Pirotta, and Peters 2017; Pirotta, Parisi, and Restelli 2015). Because of the online planning nature in
our case, it suffices to adopt a simpler approach, by performing multiple runs of policy search over a
representative set of weights (Van Moffaert, Drugan, and Nowé 2013; Natarajan and Tadepalli 2005).
Besides, RL algorithms are commonly classified as model-based methods (e.g., MBVE (Feinberg
et al. 2018), MCTS (Browne et al. 2012), etc.), which directly learn a model of the system dynamics,
and model-free methods (e.g., fitted-Q iteration (Riedmiller 2005), deep-Q network (Mnih et al.
2015), actor-critic (Konda and Tsitsiklis 2000), etc.), which do not. In the case of emerging infectious
disease control, on one hand, the algorithm needs to generalize to unseen transitions (e.g., the end of
the epidemic), where the model-free approach is typically not applicable (van Hasselt, Hessel, and
Aslanides 2019); on the other hand, some epidemiological models have demonstrated satisfactory
prediction power. Therefore, the model-based approach is adopted in this paper. Finally, we note
that some efforts have been made in the literature to study the optimal control policy for COVID-19
(Alvarez, Argente, and Lippi 2020; Piguillem and Shi 2020; Eftekhari et al. 2020). As discussed
above, these works did not consider the multi-objective problem but assigned a fixed price for each
life to scalarize the objective, and in addition, they focus on solving a one-time planning problem to
decide all future actions instead of providing a real-time decision-making framework.
Outline. The remainder of this paper is structured as follows: we first outline the proposed decision
making workflow in Section 2, and then discuss the details of several components in Section 3. The
numerical experiments and results are presented in Section 4. We conclude the paper with discussions
and possible extensions in Section 5.

2

2
2.1

Framework of the Multi-Objective Decision Support Tool
Preliminaries

A Markov decision process (MDP) is a sequential decision making model which can be represented
by a tuple hS, A, c, f i, where S is the state space, A is the action space, c : S × A → R is the
expected cost function, and f : S 2 × A → R is a Markov transition kernel. Throughout the paper we
use cost instead of reward. A multi-objective MDP (MOMDP) is an extension of the MDP model
when there are several competing objectives. An MOMDP can be represented as a tuple hS, A, c, f i,
where the other components are defined as above and c = (c1 , . . . , cK )T is a vector of K cost
functions for K different objectives, respectively.
In this paper, we consider the finite horizon setting with a pre-specified horizon T , which can simply
be selected as a large enough number without loss of generality, since the costs we consider will be
close to zero after the disease being controlled. In addition, we will focus on deterministic policies
for realistic consideration. For a deterministic policy π, we define its k-th value function at time t0 as
PT
π
Vk,t
(s) = Eπ ( t=t0 ck (St , At )|St0 = s), where Eπ denotes the expectation assuming At = π(St )
0
for every t ≥ t0 . In the MDP setting, the objective is typically to find an optimal policy π ∗ that
minimizes the expected cumulative cost among a policy class F. However, in an MOMDP, there may
not be a single policy that minimizes all costs. Instead, we consider the set of Pareto-optimal policies
0
with linear preference functions Πt0 = {π ∈ F : ∃ω ∈ Ω s.t. ω T Vtπ0 (s) ≤ ω T Vtπ0 (s), ∀π 0 ∈
T
π
π
π
T
F, s ∈ S}, where Ω = {ω ∈ RK
+ : ω 1 = 1} and Vt0 = (V1,t0 , . . . , VK,t0 ) . A policy is
called Pareto-optimal if it can not be improved for one objective without sacrificing the others. The
dependency on t0 is to be consistent with the online planning setting considered in this paper.
2.2

The generalized SIR model

The SIR model is one of the most widely applied models to describe the dynamics of infectious
diseases (Brauer 2008). Suppose at time t, the infectious disease is spread within Nt regions. Denote
the total population of the l-th region as Ml , for l = 1, . . . , Nt . With the SIR model, we divide the
total population into three groups: the individuals who can infect others, who have been removed
from the infection system, and who have not been infected and are still susceptible. Denote the count
I
R
S
I
R
for each group in region l at time t as Xl,t
, Xl,t
, and Xl,t
= Ml − Xl,t
− Xl,t
, respectively. We
will discuss how to construct these variables using observational data in Section 3.1. The standard
deterministic SIR model can then be written as a system of difference equations:
S
S
S
I
Xl,t+1
= Xl,t
− βXl,t
Xl,t
/Ml ;
I
I
S
I
I
Xl,t+1
= Xl,t
+ βXl,t
Xl,t
/Ml − γXl,t
;
R
R
I
Xl,t+1
= Xl,t
+ γXl,t
,
where β and γ are constants representing the infection and removal rate, respectively.

There are two limitations with the above SIR model. First, the infection rate heavily depends on the
control measure being taken. As discussed in Section 1, in this paper we focus on an ordinal set of
actions A = {1, 2, . . . , J}, with level 1 standing for no official measures. Second, a stochastic model
with proper distribution assumptions is required to fit real data with randomness. Motivated by the
discussions above, we propose the generalized SIR (GSIR) model and use it as our transition model:
S
S
S
Xl,t+1
= Xl,t
− eS
l,t , el,t ∼ Poisson(

J
X

S
βj I(Al,t = j)Xl,t

j=1
R
R
R
I
Xl,t+1
= Xl,t
+ eR
l,t , el,t ∼ Binomial(Xl,t , γ);

I
Xl,t
);
Ml

(1)

I
S
R
Xl,t+1
= Ml − Xl,t+1
− Xl,t+1
,

where I(·) is the indicator function, Al,t is the action taken by region l at time t, and βj denotes
the infection rate under action j. We assume β1 ≥ β2 ≥ · · · ≥ βJ ≥ 0 to represent the increasing
effectiveness. The estimation of θ = (γ, β1 , . . . , βJ )T is deferred to Section 3.2.
2.3

Sequential decision making

In this work, we focus on the intervention decision of each region: in the model estimation step, data
from several similar regions are aggregated together to share information and mediate the issue that
3

Figure 1: Decision making process for region l at time t0
data is typically noisy, scarce, and single-episode in a pandemic; while in the decision making step,
each region chooses its own action.
MOMDP definition. The intervention decision making problem can be naturally formalized as an
MOMDP. For each region l, at each decision point t, according to the estimated transition model, the
S
I
R T
current state Sl,t = (Xl,t
, Xl,t
, Xl,t
) , and their judgment, the policymakers determine a policy πl,t
with the objective of minimizing the overall long-term cost, and choose the action Al,t to implement
according to the policy. This action, assuming being effectively executed, will affect the infection rate
during (t, t + 1] and hence the conditional distribution of Sl,t+1 . Let f (·|·, ·; θ) be the conditional
density for Sl,t+1 given Sl,t and Al,t in model (1). In this work, we consider two cost variables, the
A
E
E
and the action cost Cl,t
. Cl,t
can be naturally chosen as the number of
epidemiological cost Cl,t
S
S
A
new infections Xl,t − Xl,t+1 . Let Cl,t = cl (Zl,t , Al,t , Zl,t+1 ) ∈ R+ for a time-varying variable Zl,t
and a stochastic function cl . Both cl and Zl,t should be chosen by domain experts. For example,
A
with Zl,t representing the unemployment rate, Cl,t
= Zl,t+1 − Zl,t can represent its change due to
Al,t . Since the modeling for action cost and its transition is a separate question, for simplicity, we
A
focus on the case that Cl,t
= cl (Al,t ) for a pre-specified stochastic function cl (·) in this paper. cl (·)
is set to be region-specific to incorporate local features, such as the economic conditions. With a
E
A
given weight ωl,t ∈ R+ , the overall cost Cl,t is then defined as Cl,t
+ ωl,t Cl,t
. The expected cost
functions can then be derived from these definitions, and for decision-making purposes, the overall
cost is equivalent with the weighted cost defined in Section 2.1 when K = 2.
Online planning. For each region l, the online planning workflow is described as follows. We
consider a sequence of decision points T ⊂ {1, . . . , T } to reduce the action switch cost in real
applications. At time t0 ∈
/ T , the region keeps the same action with time t0 − 1. At time t0 ∈ T ,
the policymakers choose an action according to the decision making workflow displayed in Figure 1,
which is summarized as follows: we first estimate the posterior of θ as ρt0 , using accumulated data
Dt0 = {Sl,t , Al,t }1≤l≤Nt0 ,1≤t≤t0 −1 ∪ {Sl,t0 }1≤l≤Nt0 and priors selected with domain knowledge.
The Bayesian approach is proffered here because (i) typically there is important domain knowledge
available, (ii) we need to make decisions before accumulating sufficient data, and (iii) the model
uncertainty should be emphasized in this case. Next, the policymakers choose the trade-off weight
ωl,t0 , learn a deterministic policy π̂l,t0 (·; ωl,t0 ) by planning, and implement Al,t0 = π̂l,t0 (Sl,t0 ; ωl,t0 ).
Formally, we solve the following optimization problem to obtain π̂l,t0 (·; ωl,t0 ):
π̂l,t0 (·; ωl,t0 ) = argmin Eπ,ρt0 (
π∈F

T
X

E
A
(Cl,t
+ ωl,t0 Cl,t
)),

(2)

t=t0

A
where Eπ,ρt0 denotes the expectation assuming Cl,t
∼ cl (Al,t ), P(Sl,t+1 = sl,t+1 |Sl,t = sl,t , Al,t =
al,t ) = f (sl,t+1 |sl,t , al,t ; θ) with θ ∼ ρt0 , and Al,t = π(Sl,t )I(t ∈ T ) + Al,t−1 , I(t ∈
/ T ), for every
t ≥ t0 . The specification of F and the policy search algorithm for solving (2) will be discussed in
Section 3.3.

2.4

Multiple objectives and the Pareto-optimal policies

In the discussion above, we assume the tradeoff weight ωl,t0 is easily specified at each decision point.
A
This is feasible when the two objectives share the same unit, for example, when Cl,t
represents the
damage to public health due to economic losses. In general settings, properly choosing the weight
is not easy and sometimes unrealistic. Therefore, we aim to assemble a decision support tool that
4

provides a comprehensive picture of the future possibilities associated with different weight choices
and hence makes the multi-objective decision making feasible.
Solving the whole set of Pareto-optimal policies is typically quite challenging. While in our online
planning setting, for each decision point t0 , we only need to make a one-time decision among a few
available actions. For this purpose, it is not necessary to generate all such policies. We can simply
solve problem (2) for a representative set of weights {ωb }B
b=1 to find the corresponding Pareto-optimal
policies, and then apply Monte Carlo simulation to obtain the corresponding prediction bands for
the potential costs following each policy. The policymakers can then compare all these possible
trajectories, select among them, and hence choose the action Al,t0 . We summarize this tool in
Algorithm 1.
Algorithm 1: Pareto-optimal Policies and Prediction Bands
Input: weights {ωb }B
b=1 , number of replications K, significance level α, action cost function cl (·),
decision points T , Dt0 , ρt0 , T .
for b = 1, . . . , B do
apply a policy search algorithm (e.g., Algorithm 3) to find the optimal policy π̂ b for weight ωb .
for k = 1, . . . , K do
set the cumulative cost Vtk,E
and Vtk,A
as 0
0 −1
0 −1
set Stk0 ,b = St0
for t = t0 , . . . , T do
k
choose action Akt,b = π̂ b (St,b
)I(t ∈ T ) + Akt−1,b I(t ∈
/ T)
k,A
k
k
k
k
k
sample Ct,b ∼ cl (At,b ), θt,b ∼ ρt0 , and St+1,b ∼ f (·|St,b
, Akt,b ; θt,b
)
k,E
k,E
k,S
k,S
k,A
k,A
k,A
calculate Vt,b
= Vt−1,b
+ Xt,b
− Xt+1,b
and Vt,b
= Vt−1,b
+ Ct,b
k,E K
for t ∈ {t0 , . . . , T }, calculate the upper and lower α/2-th quantile and the mean of {Vt,b
}k=1
b,E
b,E
b,A
b,A
as Vu,t
, Vl,t
, and V̄tb,E , respectively; similarly calculate Vu,t
, Vl,t
, and V̄tb,A .
1
B
Result: optimal policies {π̂ b }B
b=1 , recommended actions {At0 ,b }b=1 , prediction bands for costs
b,E
b,E
b,E
b,A
b,A
b,A T
{{(Vl,t , Vu,t , V̄t ), (Vl,t , Vu,t , V̄t )}t=t0 }B
b=1

3
3.1

Details of the Decision Support Tool
State construction

In this section, we discuss how to construct the state variables with surveillance data. The data
usually available to policymakers is the cumulative count of confirmed cases until time t, denoted as
I
R
I
Ol,t
. We can naturally set Xl,t
as Ol,t
, since the individuals counted in Ol,t generally either have
been confirmed and isolated, or have recovered or died. For infectious diseases, there is typically a
time delay between being infectious and getting isolated, the length of which is treated as a random
I
variable with expectation D. Therefore, the count of the infectious Xl,t
is usually not immediately
observable at time t, but will be gradually identified in the following days. Following the existing
works on infectious disease modelling (Zhang et al. 2005; Chen et al. 2020), we treat this issue as a
I
I
delayed observation problem and use Ol,t+D
− Ol,t
, the new confirmed cases during (t, t + D], as a
I
proxy for Xl,t . In the planning step, following the literature on delayed MDPs (Walsh et al. 2009),
we apply Algorithm 2 (Model-based Simulation, MBS) to generate a belief state, and then choose the
action according to it. We note that although this issue is not obvious in prediction, it is unavoidable
I
in decision making because Al,t works directly on Xl,t
. It also reminds us that our control decisions
should be based on the latent state instead of only the confirmed cases. The performance of such an
approximation is examined with numerical experiments in Section 4.
3.2

Estimation of the transition model

At each time t0 ∈ T , we need to first obtain the posterior of θ in the transition model (1). Notice
S
I
R
that the last equation in (1) is redundant under the constraint Xl,t
+ Xl,t
+ Xl,t
= Ml for all t. With
5

Algorithm 2: Model-based Simulation (MBS)
I t0
0 −1
Data: {Ol,t
}t=t0 −D , {Al,t }tt=t
, ρt0 , and D
0 −D
I,G
I
I
set Xl,t
=
O
−
O
and
(γ̂t0 , β̂1,t0 , . . . , β̂J,t0 )T = E(ρt0 ).
l,t0
l,t0 −D
0 −D
for t = t0 − D, . . . , t0 − 1 do
PJ
I,G
I,G
I,G
I,G
I
I
I
Xl,t+1
= Xl,t
+ j=1 β̂j,t0 I{Al,t =j} (Ml − Ol,t
− Xl,t
)Xl,t
/Ml − (Ol,t+1
− Ol,t
)
I,G
I,G
I
I
Result: the belief state Sl,t0 = (Ml − Xl,t
− Ol,t
, Xl,t
, Ol,t
)T
0
0
0
0

data Dt0 , the Markov property reduces the estimation problem to J + 1 Bayesian generalized linear
models with the identity link function. With proper choices of the conjugate priors, the posterior
distributions have explicit forms. To save space, we postpone the derivations to Section A in the
supplement.
Below, we introduce a way to specify the prior parameters: (i) β1 and γ are both features of this
disease without any interventions, and we can first set their priors with the estimates of similar
diseases, and update them when additional biochemical findings are available; (ii) for j ≥ 2, βj
indicates the infection rate under action level j. Suppose we have a reasonable estimate of the
intervention effect uj = βj /β1 as ûj and that of β1 as β̂1 , then the prior of βj can be set as a
distribution with expectation ûj β̂1 .
3.3

Policy search

At each decision point t0 ∈ T , for each region l and a given weight ωl,t0 , we need to find the optimal
policy π̂l,t0 (·; ωl,t0 ) ∈ F by solving the model-based planning problem (2). F is supposed to be a
general and interpretable policy class, since interpretability is of great importance in this application.
In this work, we focus on the following observations from the pandemic control decision making
process in real life: (i) the decision should be based on the spread severity, which we interpret as
I
Xl,t
, the number of infectious individuals; (ii) the policy should also be based on the current estimate
of disease features ρt0 , the current state Sl,t0 , the trade-off weight ωl,t0 , and the potential cost cl (·),
which have all been incorporated in the objective function of (2); (iii) the more serious the situation,
the more stringent the intervention should be. Motivated by these observations, we consider the
following policy class:
F = {π : π(Sl,t ; λ) =

J
X

I
jI(λj ≤ Xl,t
< λj+1 ),

j=1

0 = λ1 ≤ λ2 ≤ · · · ≤ λJ+1 = Ml , λJ ≤ λM },
I
where λ = (λ2 , . . . , λJ )T . Notice that Xl,t
is generally a number much smaller than Ml , we
introduce the pre-specified tolerance parameter λM ∈ (0, Ml ) to reduce the computational cost by
deleting unrealistic policies.

In this paper, we use the rollout-based direct policy search to solve Problem (2). Direct policy search
algorithms generally apply optimization algorithms to maximize the value function approximated
via Monte Carlo rollouts (Gosavi et al. 2015). Since the state transition and the action cost are both
computationally affordable to sample in our case, when J is not large, we can simply apply the grid
search algorithm to efficiently and robustly find the optimal policy. The example for J = 3, which is
the case in our experiment, is described in Algorithm 3. When J is large, many other optimization
algorithms can be used and a simultaneous perturbation stochastic approximation algorithm (Sadegh
1997) is provided in Section B.1 of the supplement. The computational complexity of our algorithm
is discussed in Section B.2 of the supplement.

4

Application to COVID-19

In this section, we apply our framework to some COVID-19 data in China for illustration. China
has passed the first peak, which provides data with good quality for validation purposes. Moreover,
6

Algorithm 3: Policy Search with Grid Search
Input: bounds of the search space u2 , u3 , U2 , U3 ; step sizes ξ2 , ξ3 ; number of replications M ; data
I t0
0 −1
{Ol,t
}t=t0 −D and {Al,t }tt=t
; other parameters D, ρt0 , wl,t0 , t0 , T, T , cl (·)
0 −D
t0 −1
∗
R t0
R
I
set λ2 = u2 , V = +∞, Sl,t0 = M BS({Xl,t
}t=t0 −D , {Al,t }t=t
, ρt0 , D), and Xl,t
= Ol,t
for
0 −D
t ∈ {t0 − D, . . . , t0 }
while λ2 ≤ U2 do
set λ3 = max(λ2 , u3 )
while λ3 ≤ U3 do
set λ = (λ2 , λ3 )T and the overall value V = 0
for m = 1, . . . , M do
for t0 = t0 , . . . , T do
t0 −1
G
R t0
generate Sl,t
0 = M BS({Xl,t }t=t0 −D , {Al,t }t=t0 −D , ρt0 , D)
G
0
0
choose action Al,t0 = π(Sl,t
/ T)
0 ; λ)I(t ∈ T ) + Al,t0 −1 I(t ∈
A
0
0
0
sample θ ∼ ρt0 , Cl,t
∼
c
(A
)
,
and
S
∼
f
(·|S
0
l
l,t
l,t +1
l,t , Al,t0 ; θ)
S
S
A
calculate V = V + (Xl,t0 − Xl,t0 +1 + ωl,t0 Cl,t0 )
if V < V ∗ then update V ∗ = V and λ∗ = λ
set λ3 = λ3 + ξ3
set λ2 = λ2 + ξ2
Output: optimal policy π̂l,t0 (·; ωl,t0 ) = π(·; λ∗ )

COVID-19 is still spreading worldwide, and we hope this framework can provide some informative
suggestions to regions in the COVID-19 cycle.
4.1

Data description and hyper-parameters

We collect data for six important cities in China from 01/15/2020 to 05/13/2020, and index these
cities by l ∈ {1, . . . , 6} and dates by t ∈ {1, . . . , 120}, with T = 120. More details about the dataset
and hyper-parameters can be found in Section C.1 in the supplement.
State variables and region-specific features: the counts of confirmed cases for each city are collected from (Lab 2020). For each region l, we collect its annual gross domestic product
(GDP) Gl and population Ml from a Chinese demographic dataset 1 .
Action: three levels of intervention measures implemented in China during COVID-19 are considered
and data are collected from the news: level 1 means no or few official policies claimed;
level 2 means the public health emergency response; level 3 means the stringent closed-off
management required by the government.
Cost: we use rl,t , the observed ratio of human mobility loss in city l on day t compared with year
2019, to construct a proxy for its GDP loss and calibrate the action cost function cl (·). The
data is collected from the online platform Baidu Migration2 and it is collected until day
61 due to availability. we first fit a normal distribution N (µj , σj2 ) to {rl,t |Al,t = j, 1 ≤
P3
l ≤ 6, 1 ≤ t ≤ 61} for j ∈ {2, 3}, and then define cl (a) as j=1 Cj I(a = j)Gl /365,
where C1 = 0 and Cj ∼ N (µj , σj2 ) for j ∈ {2, 3}. We note that this is only for illustration
purposes. In real applications, policymakers need to carefully design and measure the
potential costs with domain experts.
Hyper-parameters: the parameter estimates for a similar pandemic SARS (Mkhatshwa and Mummert 2010) are used as priors of γ and β1 . Similar to (Ferguson et al. 2020), we assume
that action 2 and 3 can reduce the infection rate by 80% and 90%, respectively, and set the
priors for β2 and β3 accordingly. D is chosen as 9 according to (Sun et al. 2020) and (Pellis
et al. 2020). All the above hyper-parameters are chosen according to domain knowledge,
and sensitivity analysis provided in the supplement show that the performance is robust.
1
2

https://www.hongheiku.com
http://qianxi.baidu.com

7

4.2

Estimation and validation of the transition model

The performance of our learned policies and the choices of weights both rely on the prediction
accuracy of the estimated GSIR model, and we aim to examine this point via temporal validation.
Specifically, We first estimate the GSIR model using all data until day 12, and then for each city
R
l, we predict Xl,t
, the count of cumulative confirmed cases, from day 13 to 120 following the
observed actions in data via forward sampling with the estimated GSIR model. The results over 1000
replications are plotted in Figure 2 and the prediction bands successfully cover the observed counts
in most cases.

Figure 2: Validation results for the six important cities in China. The solid lines are the observed
counts of the cumulative infected cases, and the red dotted lines represent the mean predicted numbers.
The shaded areas indicate the 99% prediction bands. When a different action was taken, we annotate
the new action level on the change point.
To provide more insights into the intervention effects, in Table 1, we present the estimated parameters
using all data until day 61, since the new cases afterwards are sparse. Here, R0j = βj /γ is the basic
reproduction number under action level j, which plays an essential role in epidemiology analysis
(Delamater et al. 2019). Roughly speaking, R0j < 1 implies the pandemic will gradually diminish
under action j. The small value of R03 indicates that action 3 has a significant effect on controlling
the pandemic; measure 2 is also effective and is more like a mitigation strategy; the estimate of R01 is
consistent with the existing results (Alimohamadi, Taghdir, and Sepandi 2020), and it emphasizes
that a lack of intervention will lead to a disaster. We also reported the estimates used to make the
predictions in Figure 2. Although the estimation is not perfect due to data scarcity in the early stage,
the prediction still captures the rough trend under a reasonable policy and we expect it will not affect
the decision making significantly.
Table 1: The posterior means and standard deviations (in the parentheses) obtained using data for all
the six cities until day t0 .
t0 R01
R02
R03
γ
β1
β2
β3
12
61
4.3

4.13 (.34)
2.32 (.09)

0.74 (.09)
0.70 (.03)

0.37 (.06)
0.38 (.03)

0.07 (.005)
0.11 (.002)

0.29 (.011)
0.25 (.007)

0.05 (.005)
0.07 (.003)

0.03 (.004)
0.04 (.003)

Evaluation of the Pareto-optimal policies

In this section, we conduct a simulation experiment to compare the performance of our proposed
method with several competing policies on curbing the spread of COVID-19. Specifically, for the
six cities, we start from day 12 together and follow the actions recommended by a specific policy
until day 120, with the cumulative costs recorded. The state transitions are generated from the GSIR
model with E(ρ61 ) as the parameter, and the observations before day 12 are kept the same with the
real data. The validity of E(ρ61 ) as the environment parameter is examined via cross-validation in
Section C.2 in the supplement.
Motived by the real life observations and the candidate policies considered in the literature (Merl
et al. 2009; Lin, Muthuraman, and Lawley 2010; Ludkovski and Niemi 2010), we make comparisons
among the following policies:
1. Our proposed Pareto-optimal policies πk for k ∈ {−2, 0, . . . , 6}: we fix the weight as ek /10
across all cities and time, and follow the workflow proposed in Section 2.3. T is set as every
seven days.
8

M
2. Occurrence-based mitigation policy πm
for m ∈ {4, 6 . . . , 12}: a city implements action 2
when there are new cases confirmed in the past m days, and action 1 otherwise.
S
for m ∈ {4, 6, . . . , 10}: a city begins to implement
3. Occurrence-based suppression policy πm
action 2 after m days from its first confirmed case and strengthens it to level 3 after m more
days. Afterwards, the city weakens the action to level 2 when there have been no new cases
for m days and level 1 for 2m days.
TB
for m ∈ {5, 10, . . . , 50}: a city implements action 3
4. Count threshold-based policy πm
when the count of new cases on the day before exceeds m, action 1 when it is zero, and
action 2 otherwise.
5. Behaviour policy π B : the observed trajectories in the dataset.

We run experiments on a c5d.24xlarge instance on the AWS EC2 platform, with 96 cores and
192GB RAM, and it takes roughly 6 hours to complete. For each policy except for π B , we run 100
replications and present the average costs in Figure 3. The standard errors are small and reported in
the supplement. We can see that the proposed method provides a clear view of the tradeoff between
the two objectives and its performance is generally better than the competing policies. The behaviour
policy by Chinese local governments is quite strict in the later period, and we interpret part of this
effort as the attempt to curb the cross-border spread. The other policies are not Pareto-optimal and also
are not adaptive to different situations in different cities. The clear trend among the occurrence-based
suppression policies emphasizes the importance of intervening as promptly as possible, which can
reduce both costs.

Figure 3: Cumulative epidemiological costs and economic costs following different policies, averaged
over 100 replications. The closer to the left bottom corner, the better.

5

Discussion

This work is motivated by the ongoing COVID-19 pandemic, where it is witnessed that the decision
making can be impeded by the huge intervention costs and the uncertainty. We propose a novel
model-based multi-objective reinforcement learning framework to assist policymakers in real-time
decision making with the objective of minimizing the overall long-term cost. The method shows
promising performance in numerical studies.
The overall framework is generally applicable to infectious disease pandemics and there are several
components that can be extended: (i) other epidemiology models than the SIR model can also be used
as the transition model, with the estimation method and policy class modified correspondingly; (ii)
more than two objectives can be similarly formalized and resource limits can also be considered by
including constraints in the specification of the policy class; (iii) other policy classes can be similarly
formalized depending on the information required for decision making. As future directions, the
spreads among multiple regions can be incorporated under the multi-agent reinforcement learning
framework, the partially observable MDPs can be considered to deal with the delayed observation
9

problem, and a multidimensional action space with the combinations of different interventions is a
meaningful next step.

Ethics Statement
We would like to emphasize that taking the economy as more important than human lives is not a
motivation or an outcome of this framework. On one hand, economic losses can also cause health
damage to many people, probably not less than the direct damage from the disease; on the other
hand, the estimated Pareto-optimal policies aim to help policymakers reduce the cost on one objective
without sacrificing the other, as illustrated in Section 4.3.
The framework is designed with the overall welfare of all people in mind. We acknowledge that the
degree of interest loss for different groups may vary due to different choices among the Pareto-optimal
policies, which is a tricky and unavoidable ethical question. For this purpose, an online interactive
tool and an open-sourced software are under development to facilitate the real-time dissemination
of results. Everybody is welcome to utilize these tools to have a better understanding of the current
situation and participates in the discussion.

References
Alimohamadi, Y.; Taghdir, M.; and Sepandi, M. 2020. The estimate of the basic reproduction number
for novel coronavirus disease (COVID-19): a systematic review and meta-analysis. Journal of
Preventive Medicine and Public Health .
Alvarez, F. E.; Argente, D.; and Lippi, F. 2020. A simple planning problem for covid-19 lockdown.
Technical report, National Bureau of Economic Research.
Anderson, R. M.; Heesterbeek, H.; Klinkenberg, D.; and Hollingsworth, T. D. 2020. How will
country-based mitigation measures influence the course of the COVID-19 epidemic? The Lancet
395(10228): 931–934.
Barrett, L.; and Narayanan, S. 2008. Learning all optimal policies with multiple criteria. In
Proceedings of the 25th international conference on Machine learning, 41–47.
Brauer, F. 2008. Compartmental models in epidemiology. In Mathematical epidemiology, 19–79.
Springer.
Browne, C. B.; Powley, E.; Whitehouse, D.; Lucas, S. M.; Cowling, P. I.; Rohlfshagen, P.; Tavener,
S.; Perez, D.; Samothrakis, S.; and Colton, S. 2012. A survey of monte carlo tree search methods.
IEEE Transactions on Computational Intelligence and AI in games 4(1): 1–43.
Castelletti, A.; Pianosi, F.; and Restelli, M. 2012. Tree-based fitted Q-iteration for multi-objective
Markov decision problems. In The 2012 International Joint Conference on Neural Networks (IJCNN),
1–8. IEEE.
Chen, B.; Shi, M.; Ni, X.; Ruan, L.; Jiang, H.; Yao, H.; Wang, M.; Song, Z.; Zhou, Q.; and Ge,
T. 2020. Data Visualization Analysis and Simulation Prediction for COVID-19. arXiv preprint
arXiv:2002.07096 .
Delamater, P. L.; Street, E. J.; Leslie, T. F.; Yang, Y. T.; and Jacobsen, K. H. 2019. Complexity of the
basic reproduction number (R0). Emerging infectious diseases 25(1): 1.
Eftekhari, H.; Mukherjee, D.; Banerjee, M.; and Ritov, Y. 2020. Markovian And Non-Markovian
Processes with Active Decision Making Strategies For Addressing The COVID-19 Epidemic. arXiv
preprint arXiv:2008.00375 .
Eichenbaum, M. S.; Rebelo, S.; and Trabandt, M. 2020. The macroeconomics of epidemics. Technical
report, National Bureau of Economic Research.
Elhia, M.; Rachik, M.; and Benlahmar, E. 2013. Optimal control of an SIR model with delay in state
and control variables. ISRN Biomathematics 2013.
10

Feinberg, V.; Wan, A.; Stoica, I.; Jordan, M. I.; Gonzalez, J. E.; and Levine, S. 2018. Model-based
value estimation for efficient model-free reinforcement learning. arXiv preprint arXiv:1803.00101 .
Ferguson, N.; Laydon, D.; Nedjati-Gilani, G.; et al. 2020. Impact of non-pharmaceutical interventions
(NPIs) to reduce COVID-19 mortality and healthcare demand. Imperial College COVID-19 Response
Team.
Gangopadhyaya, A.; and Garrett, A. B. 2020. Unemployment, Health Insurance, and the COVID-19
Recession. Health Insurance, and the COVID-19 Recession (April 1, 2020) .
Gosavi, A.; et al. 2015. Simulation-based optimization. Springer.
Hellewell, J.; Abbott, S.; Gimma, A.; Bosse, N. I.; Jarvis, C. I.; Russell, T. W.; Munday, J. D.;
Kucharski, A. J.; Edmunds, W. J.; Sun, F.; et al. 2020. Feasibility of controlling COVID-19 outbreaks
by isolation of cases and contacts. The Lancet Global Health .
Keeling, M. J.; and Rohani, P. 2011. Modeling infectious diseases in humans and animals. Princeton
University Press.
Kermack, W. O.; and McKendrick, A. G. 1927. A contribution to the mathematical theory of
epidemics. Proceedings of the royal society of london. Series A, Containing papers of a mathematical
and physical character 115(772): 700–721.
Konda, V. R.; and Tsitsiklis, J. N. 2000. Actor-critic algorithms. In Advances in neural information
processing systems, 1008–1014.
Korevaar, H. M.; Becker, A. D.; Miller, I. F.; Grenfell, B. T.; Metcalf, C. J. E.; and Mina, M. J. 2020.
Quantifying the impact of US state non-pharmaceutical interventions on COVID-19 transmission.
medRxiv .
Lab, C. D. 2020. China COVID-19 Daily Cases with Basemap. doi:10.7910/DVN/MR5IJN. URL
https://doi.org/10.7910/DVN/MR5IJN.
Laber, E. B.; Meyer, N. J.; Reich, B. J.; Pacifici, K.; Collazo, J. A.; and Drake, J. M. 2018. Optimal
treatment allocations in space and time for on-line control of an emerging infectious disease. Journal
of the Royal Statistical Society: Series C (Applied Statistics) 67(4): 743–789.
Ledzewicz, U.; and Schättler, H. 2011. On optimal singular controls for a general SIR-model with
vaccination and treatment. Discrete and continuous dynamical systems 2: 981–990.
Lin, F.; Muthuraman, K.; and Lawley, M. 2010. An optimal control theory approach to nonpharmaceutical interventions. BMC infectious diseases 10(1): 32.
Liu, C.; Xu, X.; and Hu, D. 2014. Multiobjective reinforcement learning: A comprehensive overview.
IEEE Transactions on Systems, Man, and Cybernetics: Systems 45(3): 385–398.
Ludkovski, M.; and Niemi, J. 2010. Optimal dynamic policies for influenza management. Statistical
Communications in Infectious Diseases 2(1).
Merl, D.; Johnson, L. R.; Gramacy, R. B.; and Mangel, M. 2009. A statistical framework for the
adaptive management of epidemiological interventions. PloS One 4(6): e5807.
Mkhatshwa, T.; and Mummert, A. 2010. Modeling super-spreading events for infectious diseases:
case study SARS. arXiv preprint arXiv:1007.0908 .
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.;
Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep
reinforcement learning. Nature 518(7540): 529–533.
Natarajan, S.; and Tadepalli, P. 2005. Dynamic preferences in multi-criteria reinforcement learning.
In Proceedings of the 22nd international conference on Machine learning, 601–608.
Organization, W. H.; et al. 2020. Coronavirus disease 2019 (COVID-19): situation report, 72 .
11

Parisi, S.; Pirotta, M.; and Peters, J. 2017. Manifold-based multi-objective policy search with sample
reuse. Neurocomputing 263: 3–14.
Pellis, L.; Scarabel, F.; Stage, H. B.; Overton, C. E.; Chappell, L. H.; Lythgoe, K. A.; Fearon, E.;
Bennett, E.; Curran-Sebastian, J.; Das, R.; et al. 2020. Challenges in control of Covid-19: short
doubling time and long delay to effect of interventions. arXiv preprint arXiv:2004.00117 .
Piguillem, F.; and Shi, L. 2020. Optimal COVID-19 quarantine and testing policies .
Pirotta, M.; Parisi, S.; and Restelli, M. 2015. Multi-objective reinforcement learning with continuous
pareto frontier approximation. In Twenty-Ninth AAAI Conference on Artificial Intelligence.
Probert, W. J.; Jewell, C. P.; Werkman, M.; Fonnesbeck, C. J.; Goto, Y.; Runge, M. C.; Sekiguchi, S.;
Shea, K.; Keeling, M. J.; Ferrari, M. J.; et al. 2018. Real-time decision-making during emergency
disease outbreaks. PLoS computational biology 14(7): e1006202.
Riedmiller, M. 2005. Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning, 317–328. Springer.
Roijers, D. M.; Vamplew, P.; Whiteson, S.; and Dazeley, R. 2013. A survey of multi-objective
sequential decision-making. Journal of Artificial Intelligence Research 48: 67–113.
Sadegh, P. 1997. Constrained optimization via stochastic approximation with a simultaneous perturbation gradient approximation. Automatica 33(5): 889–892.
Song, P. X.; Wang, L.; Zhou, Y.; He, J.; Zhu, B.; Wang, F.; Tang, L.; and Eisenberg, M. 2020. An
epidemiological forecast model and software assessing interventions on COVID-19 epidemic in
China. medRxiv .
Sun, H.; Qiu, Y.; Yan, H.; Huang, Y.; Zhu, Y.; and Chen, S. X. 2020. Tracking and Predicting
COVID-19 Epidemic in China Mainland. medRxiv .
Tildesley, M. J.; Savill, N. J.; Shaw, D. J.; Deardon, R.; Brooks, S. P.; Woolhouse, M. E.; Grenfell,
B. T.; and Keeling, M. J. 2006. Optimal reactive vaccination strategies for a foot-and-mouth outbreak
in the UK. Nature 440(7080): 83–86.
van Hasselt, H. P.; Hessel, M.; and Aslanides, J. 2019. When to use parametric models in reinforcement learning? In Advances in Neural Information Processing Systems, 14322–14333.
Van Moffaert, K.; Drugan, M. M.; and Nowé, A. 2013. Scalarized multi-objective reinforcement
learning: Novel design techniques. In 2013 IEEE Symposium on Adaptive Dynamic Programming
and Reinforcement Learning (ADPRL), 191–199. IEEE.
Walsh, T. J.; Nouri, A.; Li, L.; and Littman, M. L. 2009. Learning and planning in environments with
delayed feedback. Autonomous Agents and Multi-Agent Systems 18(1): 83.
Wilson, S. 2020. Pandemic leadership: Lessons from New Zealand’s approach to COVID-19.
Leadership 1742715020929151.
Zhang, J.; Lou, J.; Ma, Z.; and Wu, J. 2005. A compartmental model for the analysis of SARS
transmission patterns and outbreak control measures in China. Applied Mathematics and Computation
162(2): 909–924.
Zlojutro, A.; Rey, D.; and Gardner, L. 2019. A decision-support framework to optimize border
control for global outbreak mitigation. Scientific reports 9(1): 1–14.

12

