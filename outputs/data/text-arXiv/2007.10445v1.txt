PanRep: Universal node embeddings for
heterogeneous graphs

arXiv:2007.10445v1 [cs.LG] 20 Jul 2020

Vassilis N. Ioannidis1 , Da Zheng1 , and George Karypis1
1

AWS Deep Learning Palo Alto, CA
July 22, 2020
Abstract

Learning unsupervised node embeddings facilitates several downstream
tasks such as node classification and link prediction. A node embedding is
universal if it is designed to be used by and benefit various downstream
tasks. This work introduces PanRep, a graph neural network (GNN) model,
for unsupervised learning of universal node representations for heterogenous
graphs. PanRep consists of a GNN encoder that obtains node embeddings
and four decoders, each capturing different topological and node feature
properties. Abiding to these properties the novel unsupervised framework
learns universal embeddings applicable to different downstream tasks.
PanRep can be furthered fine-tuned to account for possible limited labels.
In this operational setting PanRep is considered as a pretrained model
for extracting node embeddings of heterogenous graph data. PanRep
outperforms all unsupervised and certain supervised methods in node
classification and link prediction, especially when the labeled data for the
supervised methods is small. PanRep-FT (with fine-tuning) outperforms all
other supervised approaches, which corroborates the merits of pretraining
models. Finally, we apply PanRep-FT for discovering novel drugs for
Covid-19. We showcase the advantage of universal embeddings in drug
repurposing and identify several drugs used in clinical trials as possible
drug candidates.

1

Introduction

Learning node representations from heterogeneous graph data powers the success
of many downstream machine learning tasks such as node classification [29], and
link prediction [47]. Graph neural networks (GNNs) learn node embeddings
by applying a sequence of nonlinear operations parametrized by the graph
adjacency matrix and achieve state-of-the-art performance in the aforementioned
downstream tasks. The era of big data provides an opportunity for machine
learning methods to harness large datasets [49]. Nevertheless, typically the
1

labels in these datasets are scarce due to either lack of information or increased
labeling costs [9]. The lack of labeled data points hinders the performance
of supervised algorithms, which may not generalize well to unseen data and
motivates unsupervised learning.
Unsupervised node embeddings may be used for downstream learning tasks,
while the specific tasks are typically not known a priori. For example, node
representations of the Amazon book graph can be employed for recommending
new books as well as classifying a book’s genre. This work aspires to provide
universal node embeddings, which will be applied in multiple downstream tasks
and achieve comparable performance to their supervised counterparts.
Although unsupervised learning has numerous applications, limited labels
of the downstream task may be available. Refining the unsupervised universal
representations with these labels could further increase the representation power
of the embeddings. This can be achieved by fine-tuning the unsupervised model.
Natural language processing methods have achieved state-of-the-art performance
by applying such a fine-tuning framework [12]. Fine-tuning pretrained models is
beneficial compared to end-to-end supervised learning since the former typically
generalizes better especially when labeled data are limited [16] and decreases the
inference time since typically just a few fine-tuning iterations typically suffice
for the model to converge [12].

1.1

Contributions

This work introduces a framework for unsupervised learning of universal node
representations on heterogenous graphs termed PanRep1 . It consists of a GNN
encoder that maps the heterogenous graph data to node embeddings and four
decoders, each capturing different topological and node feature properties. The
cluster and recover (CR) decoder exploits a clustering prior of the node attributes.
The motif (Mot) decoder captures structural node properties that are encoded
in the network motifs. The meta-path random walk (MRW) decoder promotes
embedding similarity among nodes participating in a MRW and hence captures
intermediate neighborhood structure. Finally, the heterogeneous information
maximization (HIM) decoder aims at maximizing the mutual information among
node local and the global representations per node type. These decoders model
general properties of the graph data related to node homophily [19, 30] or node
structural similarity [37, 15]. PanRep is solely supervised by the decoders and has
no knowledge of the labels of the downstream task. The universal embeddings
learned by PanRep are employed as features by models such as SVM [42] or
DistMult [52] to be trained for the downstream tasks. To further accommodate
the case where limited labels are available for some downstream tasks we propose
fine-tuning PanRep (PanRep-FT). In this operational setting, PanRep-FT is
optimized adhering to a task-specific loss. PanRep can be considered as a
pretrained model for extracting node embeddings of heterogenous graph data.
Figure 1 illustrates the two novel models. The contribution of this work is
1 Pan:

Pangkosmios (Greek for universal) and Rep: Representation

2

Figure 1: Illustration of the PanRep (left) and PanRep-FT (right) models. The
GNN encoder processes the node features X to obtain the embeddings H. The
decoders facilitate unsupervised learning of H. On the other hand, PanRep-FT
is further fine-tuned for a few iterations by the task specific loss.
threefold.
C1. We introduce a novel problem formulation of universal unsupervised learning and design a tailored learning framework termed PanRep. We identify
the following general properties of the heterogenous graph data: (i) the
clustering of local node features, (ii) structural similarity among nodes, (iii)
the local and intermediate neighborhood structure, (iv) and the mutual
information among same-type nodes. We develop four novel decoders to
model the aforementioned properties.
C2. We adjust the unsupervised universal learning framework to account for
possible limited labels of the downstream task. PanRep-FT refines the
universal embeddings and increases the model generalization capability.
C3. We compare the proposed models to state-of-the-art supervised and unsupervised methods for node classification and link prediction. PanRep
outperforms all unsupervised and certain supervised methods in node
classification, especially when the labeled data for the supervised methods
is small. PanRep-FT outperforms even supervised approaches in node classification and link prediction, which corroborates the merits of pretraining
models. Finally, we apply our method on the drug-repurposing knowledge
graph for discovering drugs for Covid-19 and identify several drugs used in
clinical trials as possible drug candidates.

2

Related work

Unsupervised learning. Representation learning amounts to mapping nodes
in an embedding space where the graph topological information and structure is
preserved [22]. Typically, representation learning methods follow the encoderdecoder framework advocated by PanRep. Nevertheless, the decoder is typically
attuned to a single task based on e.g., matrix factorization [43, 4, 8, 10, 33],
random walks [21, 34], or kernels on graphs [41]. Recently, methods relying on
GNNs are increasingly popular for representation learning tasks [50]. GNNs
3

typically rely on random walk-based objectives [21, 22] or on maximizing the
mutual information among node representations [45]. Relational GNNs methods
extend representation learning to heterogeneous graphs [14, 40, 39]. Relative
to these contemporary works PanRep introduces multiple decoders to learn
universal embeddings for heterogeneous graph data capturing the clustering of
local node features, structural similarity among nodes, the local and intermediate
neighborhood structure, and the mutual information among same-type nodes.
Supervised learning. Node classification is typically formulated as a semisupervised learning (SSL) task over graphs, where the labels for a subset of
nodes are available for training [7]. GNNs achieve state-of-the-art performance
in SSL by utilizing regular graph convolution [29] or graph attention [44], while
these models have been extended in heterogeneous graphs [38, 17, 48]. Similarly,
another prominent supervised downstream learning task is link prediction with
numerous applications in recommendation systems [47] and drug discovery [56,
26]. Knowledge-graph (KG) embedding models rely on mapping the nodes and
edges of the KG to a vector space by maximizing a score function for existing
KG edges [47, 52, 55]. RGCN models [38] have been successful in link prediction
and contrary to KG embedding models can further utilize node features. The
universal embeddings extracted from PanRep without labeled supervision offer
a strong competitive to these supervised approaches for both node classification
and link prediction tasks.
Pretraining. Pretraining models provides a significant performance boost
compared to traditional approaches in natural language processing [12, 35, 36,
32] and computer vision [13, 18] . Pretraining offers increased generalization
capability especially when the labeled data is scarce and increased inference
speed relative to end-to-end training [12]. Recently, [25] introduced a framework
for pretraining GNNs for graph classification. Different than [25] that focuses
on graph representations, PanRep aims at node prediction tasks and obtains
node representations via capturing properties related to node homophily [19, 30]
or node structural similarity [37]. PanRep is a novel pretrained model for node
classification and link prediction that requires significantly less labeled points to
reach the performance of its fully supervised counterparts.

3

Definitions and Problem formulation

A heterogeneous graph with T node types and R relation types is defined as
G := {{Vt }Tt=1 , {Er }R
r=1 }. The node types represent the different entities and the
relation types represent how these entities are semantically associated to each
other. For example, in the IMDB network, the node types correspond to actors,
directors, movies, etc., whereas the relation types correspond to directed-by
and played-in relations. The number of nodes of type t is denoted by Nt and
t
its associated nodal set by Vt := {nt }N
n=1 . The total number of nodes in G is
Pt
N := t=1 Nt . The rth relation type, Er := {(nt , n0t0 ) ∈ Vt × Vt0 }, holds all
4

interactions of a certain type among Vt and Vt0 and may represent that a movie
is directed-by a director. Heterogenous graphs are typically used to represent
knowledge graphs [47]. Each node nt is also associated with an F × 1 feature
vector xnt . This feature may be a natural language embedding of the title of a
movie. The nodal features are collected in a N × F matrix X. Note that certain
node types may not have features and for these we use an embedding layer to
represent their features.
Unsupervised learning. Given G and X, the goal of representation learning
is to estimate a function g such that H := g(X, G), where H ∈ RN ×D represents
the node embeddings and D is the size of the embedding space.Note that in
estimating g, no labeled information is available.
Universal representation learning. The universal representations H should
perform well on different downstream tasks. Different node classification and
link prediction tasks may arise by considering different number of training nodes
and links and different label types, e.g., occupation label or education level label.
Consider I downstream task, for the universal representations H it holds that
L(i) (f (i) (H), T (i) ) ≤ , i = 1, . . . , I,

(1)

where L(i) , f (i) , and T (i) represent the loss function, learned classifier, and
training set (node labels or links) for task i, respectively and  is the largest error
for all tasks. The goal of unsupervised universal representation learning is to
learn H such that  is small. While learning H, PanRep does not have knowledge
of {L(i) , f (i) , T (i) }i . Nevertheless, by utilizing the novel decoder scheme PanRep
achieves superior performance even compared to supervised approaches across
tasks.

4

PanRep

Our universal representation learning framework aims at embedding nodes in a
low-dimensional space such that the representations are discriminative for node
classification and link prediction. Methods for learning over graphs typically
rely on modeling homophily of nodes that postulates neighboring vertices to
have similar attributes [41, 53, 19, 30] or structural similarity among nodes [37],
where vertices involved in similar graph structural patterns possess related
attributes [15]. Motivated by these methods we identify related properties
encoded in the graph data. Clustering nodes based on their attributes provides a
strong signal for node homophily [31]. Network motifs reveal the local structure
information for nodes in the graph [5]. Metapaths encode the heterogeneous
graph neighborhood and indicate the local connectivity [14]. Finally, maximizing
the mutual information among embeddings declusters node representations and
provides further discriminative information [45].

5

4.1

Universal supervision signals

In order to capture the aforementioned properties we develop four novel universal
decoders.
Cluster and recover supervision. Node attributes may reveal interesting
properties of nodes, such as clusters of customers based on their buying power
and age. This is important in recommendation systems, where traditional matrix
factorization approaches [31] rely on revealing clusters of similar buyers. To
capitalize such information we propose to supervise the universal embeddings
by such cluster representations. Specifically, we cluster the node attributes via
K-means [27] and then design a model that decodes H to recover the original
clusters. The CR-decoder is modeled as a two layer MLP and is supervised by
Lcr := −

N X
K
X

Cnk ln Ĉnk ,

(2)

n=1 k=1

where the cluster assignment Cnk is 1 if node n belongs in class k and the
predicted cluster assignment Ĉnk is the output of the CR-decoder. Such a
supervision signal will enrich the universal embeddings H with information
based on the clustering of local node features.
Motif supervision. Network motifs are sub-graphs where the nodes have
specific connectivity patterns. Typical size-3 motifs for example, are the triangle
and the star motifs. Each of these sub-graphs is identified by a particular pattern
of interactions among nodes, and reveals important properties for the participating nodes. In gene regulatory networks for example, motifs are associated
with certain biological properties [6]. The work in [5] develops efficient parallel
implementations for extracting network motifs. We aspire to capture structural
similarity among nodes by predicting their motif information. The motivation
is that nodes which might be distant in the graph may have similar structural
properties as described by their motifs.
Using the method in [5] we extract a frequency vector µn per node that shows
how many times n participates to graph motifs up to size 4. This information
reveals the structural role of nodes such as star-center, star-edge nodes, or bridge
nodes [37, 23]. The motif decoder predicts this vector for all nodes using the
universal representation H. This allows for information sharing among nodes
which are far away in the graph but have similar motif frequency vectors. The
novel motif decoder is modeled as a two-layer MLP and is supervised by the
following loss function
Lmot :=

N
X

kµn − µ̂n k22

(3)

n=1

where µ̂n is the output of the Mot-decoder for the nth node. Using the Motdecoder PanRep enhances the universal embeddings by structural information
encoded in the node motifs.
6

Metapath RW supervision. Metapaths are sequences of edges of possibly
different type that connect nodes in a KG [14]. A metapath random walk (MRW)
is a specialized RW that follows different edge-types; see e.g., [14].
We aspire to capture local connectivity patterns by promoting nodes participating in a MRW to have similar embeddings. Consider all node pairs for
nodes (nt , n0 t0 ) participating in a MRW, the following criterion maximizes the
similarity among these nodes as follows
Lmrw := log(1 + exp(−y × h>
nt diag(rt,t0 )hn0 t0 )),

(4)

where hnt and hn0 t0 are the universal embeddings for nodes nt and n0 t0 , respectively, rt,t0 is an embedding parametrized on the pair of node-types and y is 1
if nt and n0 t0 co-occur in the MRW and -1 otherwise. Negative examples are
generated by randomly selecting tail nodes for a fixed head node with ratio 5
negatives per positive example. Link prediction is indeed a special case of the
MRW supervision that considers MRWs of length 1. However, metapaths convey
more information than regular links since the former can be defined to promote
certain prior knowledge. For example, in predicting the movie genre in IMDB
the metapath configured by the edge types (played by, played in) among node
types (movie, actor, movie) will potentially connect movies with same genre
and hence it is desirable. The embedding per node-type pair rt,t0 allows the
MRW-decoder to weight the similarity among node embeddings from different
node types accordingly. The length of the MRW controls the radius of the
graph neighborhood considered in equation (4) and it can vary from local to
intermediate.
Heterogenous information maximization. The aforementioned supervision signals capture clustering affinity, structural similarity and local and intermediate neighborhood of the nodes. Nevertheless, further information can be
extracted by the representations by maximizing the mutual information among
node representations. Such an approach for homogeneous graphs is detailed
in [45], where the mutual information between node representations and the
global graph summaries is maximized [24].
Towards further refining the universal embeddings, we propose an adaptation
of [45] for heterogeneous graphs. We consider a global summary vector per t as
PN
st := ntt=1 hnt that captures the average tth node representation. We aspire
to maximize the mutual information among st and the corresponding nodes in
Vt . The proposed HIM decoder is supervised by the following contrastive loss
function

Nt
T  X
X


>
Lhim :=
log σ(h>
Ws
)
+
log
1
−
σ(
h̃
Ws
)
(5)
t
t
nt
nt
t=1

nt =1

where the bilinear scoring function [52] σ(h̃>
nt Wst ) captures how close is hnt to
the global summary, W is a learnable matrix and h̃nt represents the negative
example used to facilitate training. Designing negative examples is a cornerstone
7

property for training contrastive models [45]. We generate the negative examples
in (5) by shuffling node attributes among nodes of the same type. The HIM
decoder maximizes the mutual information across nodes and complements the
former decoders.
Putting everything together. PanRep’s overall loss function is the linear
unweighted combination of (2)-(5) and can be considered in the framework of
deep multitask learning [54], since the GNN encoder is shared across the multiple
supervision tasks and PanRep makes multiple inferences in one forward pass.
Such networks are not only scalable, but the shared features within these networks
can induce more robust regularization and possibly boost performance [11]. A
future direction of PanRep is to introduce adaptive weights per decoder to control
its learning rate [11].

4.2

PanRep Encoder

Although the PanRep framework can utilize any GNN model as an encoder [50],
in this paper PanRep uses a relational (R)GCN encoder [38]. RGCNs extend
the graph convolution operation [29] to heterogenous graphs. An RGCN model
is comprised by a sequence of RGCN layers. The lth layer computes the nth
(l+1)
node representation hn
as follows


R
X
X (l)
h(l+1)
:= σ 
hn0 Wr(l)  ,
(6)
n
r=1 n0 ∈Nnr

where Nnr is the neighborhood of node n under relation r, σ the rectified linear
(l)
unit non linear function, and Wr is a learnable matrix associated with the rth
relation. Essentially, the output of the RGCN layer for node n is a nonlinear
combination of the hidden representations of neighboring nodes weighted based
on the relation type.
Several works consider designing possible more general GNN encoders that
utilize attention mechanism [48, 44] or graph isomorphism networks [51]. This
paper proposes novel supervision signals for unsupervised learning that capture
general properties of the graph data. Designing a universal encoder based on
these contemporary GNN models is an interesting future direction of PanRep.

4.3

PanRep-FT

In certain cases a very small subset of labels may be known a priori for the
downstream task. In such cases it is beneficial to fine-tune PanRep’s model to
obtain refined node representations. In this context, PanRep can be considered
as pretrained model and a downstream task specific loss may be applied to
supervise PanRep. BERT models in natural language processing have reported
state of the art results by considering such a pretrain and fine-tune framework [12]. PanRep-FT can be considered a counterpart of BERT for extracting
8

Table 1: Node classification results.
Train %
Mac-F1
IMDB
Mic-F1

Mac-F1
OAG
Mic-F1

40%
60%
80%
40%
60%
80%
40%
60%
80%
40%
60%
80%

Unsupervised
node2vec
HIM
PanRep
50.63
55.21
56.04
51.65
57.66
58.51
51.49
57.89
60.23
51.77
55.11
55.92
52.79
56.57
58.41
52.72
57.79
60.14
56.37
50.54
57.76
57.01
51.98
59.72
58.05
53.25
63.03
70.17
71.91
75.50
70.95
73.89
77.39
72.24
75.31
79.76

HAN
56.15
57.29
58.51
57.32
58.42
59.24
63.99
64.25
64.37
73.95
75.32
75.24

Semi-supervised
MAGNN
RGCN
PanRep-FT
60.27
58.48
59.49
60.66
58.42
59.86
61.44
58.76
61.49
60.50
58.64
59.67
60.88
58.55
59.75
61.53
58.89
61.59
63.31
64.68
64.72
63.42
65.96
66.99
63.89
67.67
67.90
72.74
81.92
80.36
72.75
81.39
81.78
73.43
82.38
83.17

information from heterogenous graph data. PanRep-FT combines the benefit
of universal unsupervised learning and task specific information and achieves
greater generalization capacity especially when labeled data are scarce [16].

5

Experiments

The proposed universal represention learning techniques are compared with
state-of-the-art methods. The models are developed in the efficient deep graph
learning library [46]. For node classification the following contemporary methods
are considered RGCN [38], HAN [48], MAGNN [17], node2vec [21], meta2vec [14]
and an adaptations of the work in [45] for heterogenous graphs termed HIM. For
link prediction the baseline models is RGCN [38] with DistMult supervision [52]
that uses the same encoder as PanRep. The Mot-decoder and RC-decoder
employ a 2-layer MLP. For the MRW-decoder we use length-2 MRWs. The
parameters for all methods considered are optimized using the performance on
the validation set.
Datasets. We consider a subset of IMDB dataset [1] containing 11,616 nodes
with 3 node-types and 17,106 edges from 6 edge-types. Each movie is associated
with a label representing its genre and with a feature vector capturing its
keywords. We also use a subset of the OAG dataset [2] with 23,696 with 4 node
types (authors, affiliations, papers, venues) and 90,183 edges from 14 edge-types.
In OAG we did not use mot supervision since [5] is not applicable. Each paper
is associated with a label denoting the scientific area and with an embedding
of the papers’ text. Finally, we utilize the drug-repurposing knowledge graph
(DRKG) constructed in [26]. DRKG contains 5,874,261 biological interactions
belonging to 107 edge-types among 97,238 biological entities from 13 entitytypes. For further details on the datasets and configuration of methods see the
supplementary material.

9

IMDB

PanRep

RGCN

85
80

PanRep-FT

PanRep

95
90

60
50

60
70
80
90
Training link percentage

PanRep-FT

PanRep

RGCN

90
85
80

75
60
70
80
90
Training link percentage

RGCN

70
MRR

Hit-10

90

OAG

OAG

PanRep

100

95
MRR

PanRep-FT

RGCN

Hit-10

IMDB

PanRep-FT

60
70
80
90
Training link percentage

60
70
80
90
Training link percentage

Figure 2: MRR and Hit-10 for link prediction across different percentages of
testing links.

5.1

Node classification

We split the labeled nodes in 10% training, 5% validation, and 85% testing
sets. In this experiment we compare supervised and unsupervised methods for
classification. First, the methods learn embeddings for the labeled nodes with
or without labeled supervision. We then obtain the embeddings corresponding
to the 85% testing nodes as calculated from the unsupervised and supervised
methods and further split these nodes to training and testing sets and train a
linear SVM. This evaluation setting allows us to directly compare the different
supervised and unsupervised approaches.
We report the Macro and Micro F1 accuracy for different training percentages
of the 85% nodes fed to the SVM classifier in Table 1. It is observed that
PanRep outperforms significantly other unsupervised approaches as well as some
supervised approaches. In certain splits, PanRep outperforms its supervised
counterpart RGCN that uses node labels for supervision. Metapath2vec [14]
reports competitive performance for OAG in Macro-F1 score but unperformed in
Micro-F1. This result is also in par with the Table 3 where the strongest signal
for PanRep is given by the MRW decoder. PanRep-FT outperforms significantly
RGCN that uses the same encoder, which is a testament to the power of
pretraining models. Finally, PanRep-FT matches and outperforms in certain
splits the state-of-the-art MAGNN that uses a more expressive encoder. PanRep’s
universal decoders enhance the embeddings with additional discriminative power
that results to improved performance in the downstream tasks.
Table 2 reports the accuracy of the PanRep-FT and the encoder RGCN that
is only trained for the semi-supervised learning task for different splits of the
training labels used to obtain the encodings. PanRep outperforms the supervised
RGCN embeddings for 5% training labels. PanRep-FT consistently outperforms
RGCN across most SVM splits. This demonstrates the importance of using
PanRep as a pretraining method. RGCN reports similar performance across
SVM splits, whereas PanRep-FT increases with more supervision. These results
suggest that PanRep-FT’s embeddings have higher generalization capacity.
Table 3 reports an ablation study by using different decoder subsets. PanRep
that uses all signals obtain the best performance. The decoders him and mrw and
the their combination exhibit the best performance after PanRep. Nevertheless,
the full supervision in PanRep leads to the superior performance.

10

Table 2: Node classification results for different labeled supervision splits.
Datasets

Train embeddings %
Train%
PanRep
40%
56.04
60%
58.51
80%
60.23
40%
55.92
60%
58.41
80%
60.14
40%
57.76
60%
59.72
80%
63.03
40%
75.50
60%
77.39
80%
79.76

Metrics
Macro-F1

IMDB
Micro-F1

Macro-F1
OAG
Micro-F1

RGCN
55.12
55.20
55.55
55.27
55.39
55.62
55.51
55.99
56.36
78.00
78.07
78.44

5%
PanRep-FT
56.85
59.39
61.27
56.92
59.45
61.32
64.99
66.62
68.94
80.19
81.36
82.52

RGCN
58.48
58.42
58.76
58.64
58.55
58.89
64.68
65.96
66.10
81.92
81.39
82.38

10%
PanRep-FT
59.49
59.86
61.49
59.67
59.75
61.39
64.72
66.99
68.60
80.36
81.78
83.17

RGCN
61.30
60.98
61.10
61.49
61.17
61.30
67.07
67.58
67.67
82.57
81.74
82.20

20%
PanRep-FT
63.14
62.91
62.72
63.17
62.89
62.75
65.31
66.25
67.90
81.17
81.34
82.31

Table 3: Ablation study for different supervision signals.
Datasets

Metrics
Macro-F1

IMDB
Micro-F1

Macro-F1
OAG
Micro-F1

5.2

Train %
40%
60%
80%
40%
60%
80%
40%
60%
80%
40%
60%
80%

him
55.21
57.66
57.89
55.11
56.57
57.79
50.54
51.98
53.25
71.91
73.89
75.31

mrw
54.54
56.12
56.64
54.36
55.91
56.49
55.92
58.40
60.61
74.39
76.76
78.90

him+mrw
55.32
57.24
57.74
55.53
56.25
58.42
56.11
58.91
61.74
74.65
76.33
78.71

mot
42.28
43.41
44.31
43.66
44.89
45.65
-

cr
39.68
40.59
41.30
41.28
42.08
42.80
15.46
15.48
15.54
63.06
63.14
63.59

PanRep
56.04
58.51
60.23
55.92
58.41
60.14
57.76
59.72
63.03
75.50
77.39
79.76

Link prediction

Our universal embedding framework is further evaluated for link prediction
using the IMDB and OAG datasets. The MRW decoder is used to evaluate
the performance of PanRep in link prediction. Figure 2 reports the MRR, and
Hit-10 scores of the baseline methods along with the PanRep and PanRep-FT
methods. We report the performance of the methods for different percentages
of links used for training. Observe that PanRep-FT consistently outperforms
the competing methods and the performance gain increases as the percentage of
training links decreases. This corroborates the advantage of pretraining GNNs

Table 4: Drug inhibits gene scores for Covid-19.
Drug name
Losartan
Chloroquine
Deferoxamine
Ribavirin
Methylprednisolone

PanRep-FT
# hits
Drug name
232
198
104
101
44

Thalidomide
Hydroxychloroquine
Tetrandrine
Eculizumab
Tocilizumab

# hits

Drug name

# hits

41
19
13
10
9

Chloroquine
Colchicine
Tetrandrine
Oseltamivir
Azithromycin

69
41
40
37
36

RGCN
Drug name
Tofacitinib
Ribavirin
Methylprednisolone
Deferoxamine
Thalidomide

# hits
33
32
30
30
25

We retain the top-5 drugs based on their number of hits for each method. Note that a random classifier will result to approximately 5.3 per drug. This
suggests that the reported predictions are significantly better than random.

11

for link prediction. Note that PanRep reports similar performance with RGCN
that is trained solely in link prediction. This result confirms the success of the
universal embeddings in link prediction.

5.3

Drug repurposing

Drug-repurposing aims at discovering the most effective existing drugs to treat
a certain disease. Using the Drug Repurposing Knowledge Graph (DRKG) [26],
we compare the drug repurposing results in Covid-19 among PanRep-FT that
is finetuned in link prediction and the baseline RGCN [38]. We employ L = 1
hidden layer with D = 600 and train for 800 epochs both networks. Drugrepurposing can be formulated as predicting direct links in the DRKG. Here,
we attempt to predict whether a drug inhibits a certain gene, which is related
to the target disease. We identify 442 genes that are related with the Covid-19
disease [20, 56]. We select 8,104 FDA-approved drugs in the DRKG as candidates;
see also [26]. To validate our predictions we use 32 Covid-19 clinical trial drugs
from [3].
For each gene node we calculate with RGCN and PanRep-FT an inhibit
link score associated with every drug. Next, we score all ‘drug-inhibits-gene’
triples and rank them per target gene. We obtain in this way 442 ranked lists
of drugs, one per gene node. Finally, to assess whether our prediction is in
par with the drugs used for treatment, we check the overlap among the top
100 predicted drugs and the drugs used in clinical trials per gene. Table 4
lists the clinical drugs included in the top-100 predicted drugs across all the
genes with their corresponding number of hits for the RGCN and PanRepFT. It can be observed, that several of the widely used drugs in clinical trials
appear high on the predicted list in both prediction. Furthermore, PanRep-FT
reports a higher hit rate than RGCN, which corroborates the benefit of using
the universal pretraining decoders. The universal representation endows PanRep
with increased generalization power that allows for accurate link prediction
performance when training data are extremely scarce as is the case of Covid-19.
While this study, does not recommend specific drugs, it demonstrates a powerful
deep learning methodology to prioritize existing drugs for further investigation,
which holds the potential of accelerating therapeutic development for Covid-19.

6

Conclusion

This paper develops a novel framework for unsupervised learning of universal
node representations on heterogenous graphs termed. PanRep supervises the
GNN encoder by decoders attuned to model the clustering of local node features,
structural similarity among nodes, the local and intermediate neighborhood
structure, and the mutual information among same-type nodes. To further
facilitate cases where limited labels are available we implement PanRep-FT.
Experiments in node classification and link prediction corroborate the competitive performance of the learned universal node representations compared to

12

unsupervised and supervised methods. Experiments on the DRKG showcase the
advantage of the universal embeddings in drug repurposing.

References
[1] www.imdb.com, 2020.
[2] www.openacademic.ai/oag/, 2020.
[3] http://www.covid19-trails.com/, 2020.
[4] Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski,
and Alexander J Smola. Distributed large-scale natural graph factorization.
In Proceedings of the 22nd international conference on World Wide Web,
pages 37–48, 2013.
[5] Nesreen K Ahmed, Jennifer Neville, Ryan A Rossi, Nick G Duffield, and
Theodore L Willke. Graphlet decomposition: Framework, algorithms, and
applications. Knowledge and Information Systems, 50(3):689–722, 2017.
[6] M Madan Babu, Nicholas M Luscombe, L Aravind, Mark Gerstein, and
Sarah A Teichmann. Structure and evolution of transcriptional regulatory
networks. Current opinion in structural biology, 14(3):283–291, 2004.
[7] M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised
learning on large graphs. In Proc. Annual Conf. Learning Theory, volume
3120, pages 624–638, Banff, Canada, Jul. 2004. Springer.
[8] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral
techniques for embedding and clustering. In Advances in neural information
processing systems, pages 585–591, 2002.
[9] Yoshua Bengio, Aaron C Courville, and Pascal Vincent. Unsupervised
feature learning and deep learning: A review and new perspectives. CoRR,
abs/1206.5538, 1:2012, 2012.
[10] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM
international on conference on information and knowledge management,
pages 891–900, 2015.
[11] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
Gradnorm: Gradient normalization for adaptive loss balancing in deep
multitask networks. arXiv preprint arXiv:1711.02257, 2017.
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
Pre-training of deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805, 2018.

13

[13] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric
Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature
for generic visual recognition. In International conference on machine
learning, pages 647–655, 2014.
[14] Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec:
Scalable representation learning for heterogeneous networks. In Proceedings
of the 23rd ACM SIGKDD international conference on knowledge discovery
and data mining, pages 135–144, 2017.
[15] Claire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning
structural node embeddings via diffusion wavelets. In Proc. Intl. Conf. on
Knowledge Disc. and Data Mining (KDD), 2018.
[16] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol,
Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help
deep learning? Journal of Machine Learning Research, 11(Feb):625–660,
2010.
[17] Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. Magnn: Metapath
aggregated graph neural network for heterogeneous graph embedding. In
Proceedings of The Web Conference 2020, pages 2331–2341, 2020.
[18] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich
feature hierarchies for accurate object detection and semantic segmentation.
In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 580–587, 2014.
[19] David F Gleich. Pagerank beyond the web. SIAM Review, 57(3):321–363,
2015.
[20] David E Gordon, Gwendolyn M Jang, Mehdi Bouhaddou, Jiewei Xu, Kirsten
Obernier, Matthew J O’meara, Jeffrey Z Guo, Danielle L Swaney, Tia A
Tummino, Ruth Huttenhain, et al. A sars-cov-2-human protein-protein
interaction map reveals drug targets and potential drug-repurposing. Nature,
2020.
[21] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning
for networks. In Proceedings of the 22nd ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 855–864, 2016.
[22] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning
on graphs: Methods and applications. arXiv preprint arXiv:1709.05584,
2017.
[23] Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghang Tong,
Sugato Basu, Leman Akoglu, Danai Koutra, Christos Faloutsos, and Lei Li.
Rolx: structural role extraction & mining in large graphs. In Proceedings of
the 18th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 1231–1239, 2012.
14

[24] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal,
Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. arXiv
preprint arXiv:1808.06670, 2018.
[25] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay
Pande, and Jure Leskovec. Strategies for pre-training graph neural networks.
In International Conference on Learning Representations, 2019.
[26] Vassilis N. Ioannidis, Xiang Song, Saurav Manchanda, Mufei Li, Xiaoqin
Pan, Da Zheng, Xia Ning, Xiangxiang Zeng, and George Karypis. Drkg
- drug repurposing knowledge graph for covid-19. https://github.com/
gnn4dr/DRKG/, 2020.
[27] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D Piatko, Ruth Silverman, and Angela Y Wu. An efficient k-means clustering
algorithm: Analysis and implementation. IEEE transactions on pattern
analysis and machine intelligence, 24(7):881–892, 2002.
[28] Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic
optimization. In Proc. Int. Conf. on Learn. Represantions, San Diego, CA,
USA, May 2015.
[29] Thomas N Kipf and Max Welling. Semi-supervised classification with graph
convolutional networks. In Proc. Int. Conf. on Learn. Represantions, Toulon,
France, Apr. 2017.
[30] Kyle Kloster and David F Gleich. Heat kernel based community detection.
In Proc. Intl. Conf. on Knowledge Disc. and Data Mining (KDD), pages
1386–1395, 2014.
[31] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization
techniques for recommender systems. Computer, 42(8):30–37, 2009.
[32] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning
of language representations, 2019.
[33] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. Asymmetric transitivity preserving graph embedding. In Proceedings of the 22nd
ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 1105–1114, 2016.
[34] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online
learning of social representations. In Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining, pages
701–710, 2014.

15

[35] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word
representations. arXiv preprint arXiv:1802.05365, 2018.
[36] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language models are unsupervised multitask learners. 2019.
[37] Ryan A Rossi and Nesreen K Ahmed. Role discovery in networks. IEEE
Transactions on Knowledge and Data Engineering, 27(4):1112–1131, 2014.
[38] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg,
Ivan Titov, and Max Welling. Modeling relational data with graph convolutional networks. In European Semantic Web Conference, pages 593–607.
Springer, 2018.
[39] Jingbo Shang, Meng Qu, Jialu Liu, Lance M Kaplan, Jiawei Han, and
Jian Peng. Meta-path guided embedding for similarity search in large-scale
heterogeneous information networks. arXiv preprint arXiv:1610.09769, 2016.
[40] Chuan Shi, Binbin Hu, Wayne Xin Zhao, and S Yu Philip. Heterogeneous
information network embedding for recommendation. IEEE Transactions
on Knowledge and Data Engineering, 31(2):357–370, 2018.
[41] A. J. Smola and R. I. Kondor. Kernels and regularization on graphs. In
Learning Theory and Kernel Machines, pages 144–158. Springer, 2003.
[42] Johan AK Suykens and Joos Vandewalle. Least squares support vector
machine classifiers. Neural processing letters, 9(3):293–300, 1999.
[43] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu
Mei. Line: Large-scale information network embedding. In Proceedings of
the 24th international conference on world wide web, pages 1067–1077, 2015.
[44] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
Pietro Lio, and Yoshua Bengio. Graph attention networks. In Proc. Int.
Conf. on Learn. Represantions, 2018.
[45] Petar Veličković, William Fedus, William L Hamilton, Pietro Liò, Yoshua
Bengio, and R Devon Hjelm. Deep graph infomax. arXiv preprint
arXiv:1809.10341, 2018.
[46] Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li,
Jinjing Zhou, Qi Huang, Chao Ma, et al. Deep graph library: Towards efficient and scalable deep learning on graphs. arXiv preprint arXiv:1909.01315,
2019.
[47] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph
embedding: A survey of approaches and applications. IEEE Transactions
on Knowledge and Data Engineering, 29(12):2724–2743, 2017.

16

[48] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and
Philip S Yu. Heterogeneous graph attention network. In The World Wide
Web Conference, pages 2022–2032, 2019.
[49] Xindong Wu, Xingquan Zhu, Gong-Qing Wu, and Wei Ding. Data mining
with big data. IEEE transactions on knowledge and data engineering,
26(1):97–107, 2013.
[50] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang,
and S Yu Philip. A comprehensive survey on graph neural networks. IEEE
Transactions on Neural Networks and Learning Systems, 2020.
[51] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful
are graph neural networks? In Proc. Int. Conf. on Learn. Represantions,
2019.
[52] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng.
Embedding entities and relations for learning and inference in knowledge
bases. arXiv preprint arXiv:1412.6575, 2014.
[53] Guangchao Yuan, Pradeep K Murukannaiah, Zhe Zhang, and Munindar P
Singh. Exploiting sentiment homophily for link prediction. In Proceedings
of the 8th ACM Conference on Recommender systems, pages 17–24, 2014.
[54] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial
landmark detection by deep multi-task learning. In European conference on
computer vision, pages 94–108. Springer, 2014.
[55] Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao
Xiong, Zheng Zhang, and George Karypis. Dgl-ke: Training knowledge
graph embeddings at scale. arXiv preprint arXiv:2004.08532, 2020.
[56] Yadi Zhou, Yuan Hou, Jiayu Shen, Yin Huang, William Martin, and
Feixiong Cheng. Network-based drug repurposing for novel coronavirus
2019-ncov/sars-cov-2. Cell discovery, 6(1):1–18, 2020.

A

Broader Impact

This work contributes to improving the performance of machine learning approaches on large linked datasets (known as learning on graphs ML tasks),
which are used to enable applications in various domains. Examples of such
applications include recommendations, link prediction, node/entity classification,
clustering, entity resolution, drug-protein binding prediction, personalized treatment, and inter-atomic potential estimation. Examples of such domains include
(e-)commerce; life, physical, and social sciences; engineering and manufacturing;
law enforcement and (information) security; healthcare; and finance. The source
of this paper’s improved performance is that it reduces the amount of labeled
data that is required to achieve a certain level of performance. When it operates
17

as an unsupervised representation learning approach (PanRep), it requires no
labeled data; whereas when it operates in its fine-tuning mode (PanRep-FT), it
can achieve considerable improvements with limited amount of labeled data.
This paper’s potential positive impact to society stem from the fact that it
can lower the effort required to achieve the desired performance for learningon-graphs applications and consequently enable them to larger segments of the
society. This will allow us to recommend the appropriate course to a student,
show an enjoyable movie to a customer, find a drug candidate with no adverse
side effects, detect money laundry transactions, decrease the time and hence
the energy required by a supercomputing-based materials science numerical
simulation. Note that the lower effort is a direct consequence of the need for
none or fewer labeled data.
At the same time, this work can also have some negative consequences. It
can result in job losses by automating tasks that are currently done by people. It can make it easier for bad actors or undemocratic regimes to infer
protected/sensitive/private information by leveraging learning on graphs approaches and smaller labeled sets. The unsupervised nature of PanRep can fail
to compute high-quality representations for infrequently occurring nodes, which
depending on the dataset, it can potentially discriminate against some protected
groups, if those groups are not well represented. Nevertheless, some of these
adverse consequences can be averted by adjusting PanRep with appropriate
regularization or constraints accounting for privacy or fairness concerns.

B

Implementation framework

The methods presented in this paper are implemented in the efficient deep graph
learning (DGL)2 library [46]. PanRep is implemented using the mini-batch
training framework that facilitates training for very large graphs even with
limited computational resources 3 . The competing methods RGCN, MAGNN
and HAN are also implemented using the DGL. PanRep experiments are executed
on an AWS P3.8xlarge4 instances with 8 GPUs each having 16GB of memory.

C

Methods

Different competing methods include RGCN [38], HAN [48], MAGNN [17],
node2vec [21], meta2vec [14] and an adaptation of the work in [45] for heterogenous graphs termed HIM. For link prediction the baseline model is RGCN [38]
with DistMult supervision [52] that uses the same encoder as PanRep. The
Mot-decoder and RC-decoder employ a 2-layer MLP. For the MRW-decoder we
use length-2 MRWs. The parameters for all methods considered are optimized
using the performance on the validation set. For the majority of the experiments
2 https://www.dgl.ai/
3 https://github.com/dmlc/dgl/blob/master/examples/pytorch/rgcn-hetero
4 https://aws.amazon.com/ec2/instance-types/p3/

18

Table 5: Statistics of datasets.
Node

Dataset
IMDb

OAG

Edge

# movie (M): 4,278
# director (D): 2,081
# actor (A): 5,257

# M-directed by-D: 4,278 , D-directed-M: 4,278
# M-played by-A: 12,828, A-played-M: 12,828

# author (A): 13,720
# paper (P): 7,326
# affiliation (Af): 2,290
# venue (V): 782

# P-in journal-V: 3941, V-journal has-P: 3941
# P-conference-V: 3368, V-conference has-P: 3368
# P-cites-P: 3327, P-cited by-P: 3327
# A-writes as last-P: 4522, P-written by last-A: 4522
# A-writes as other-P: 7769, P-written by other-A: 7769
# A-writes as first-P: 4795, P-written by first-A: 4795
# A-affiliated with-Af: 17035, Af-affiliated with-A: 17035

PanRep uses a hidden dimension of 300, 1 hidden layer, 800 epochs of model
training, 100 epochs for finetuning, and an ADAM optimzer [28] with a learning
rate of 0.001. For link prediction finetuning PanRep uses a DistMult model [52]
whereas for node classification it uses a logistic loss.

D
D.1

Datasets
DRKG

The Drug Repurposing Knowledge Graph (DRKG) contains 97055 entities belonging to 13 entity-types [26]. The type-wise distribution of the entities is
shown in Table 6. DRKG contains a total of 5869294 triplets belonging to 107
edge-types. Table 7 shows the number of triplets between different entity-type
pairs for DRKG and various data sources. The DRKG is publicly available.5

D.2

IMDB and OAG

IDMB [1] is a movie database including information about the cast, production
crew, and plot summaries. A subset of IMDb is used after data preprocessing
in Table 5. Movies are labeled as one of three classes (Action, Comedy, and
Drama) based on their genre information. Each movie is also described by a
bag-of-words representation of its plot keywords.
OAG [2] is bibliography website. We preprocess the data and retain the
subgraph in Table 5. The papers are divided into 6 research areas. Each paper
is described by a BERT embedding of the paper’s title.

5 https://github.com/gnn4dr/DRKG/

19

Table 6: Number of nodes per node type in the DRKG and the data sources.
Entity type
Anatomy
Atc
Biological Process
Cellular Component
Compound
Disease
Gene
Molecular Function
Pathway
Pharmacologic Class
Side Effect
Symptom
Tax
Total

Drugbank

GNBR

Hetionet

STRING

IntAct

DGIdb

Bibliography

Total Entities

4, 048
9, 708
1, 182
4, 973
-

11, 961
4, 746
27, 111
215

400
11, 381
1, 391
1, 538
257
19, 145
2, 884
1, 822
345
5, 701
415
-

18, 316
-

153
16, 321
-

6, 348
2, 551
-

6, 250
33
3, 181
-

400
4, 048
11, 381
1, 391
24, 313
5, 103
39, 220
2, 884
1, 822
345
5, 701
415
215

19, 911

44, 033

45, 279

18, 316

16, 474

8, 899

9, 464

97, 238

Table 7: Number of interactions in the DRKG and the data sources.
Entity-type pair

Drugbank

GNBR

Hetionet

STRING

IntAct

DGIdb

Bibliography

Total interactions

(’Gene’, ’Gene’)
(’Compound’, ’Gene’)
(’Disease’, ’Gene’)
(’Atc’, ’Compound’)
(’Compound’, ’Compound’)
(’Compound’, ’Disease’)
(’Gene’, ’Tax’)
(’Biological Process’, ’Gene’)
(’Disease’, ’Symptom’)
(’Anatomy’, ’Disease’)
(’Disease’, ’Disease’)
(’Anatomy’, ’Gene’)
(’Gene’, ’Molecular Function’)
(’Compound’, ’Pharmacologic Class’)
(’Cellular Component’, ’Gene’)
(’Gene’, ’Pathway’)
(’Compound’, ’Side Effect’)

24, 801
15, 750
1, 379, 271
4, 968
-

667, 22
80, 803
95, 399
77, 782
14, 663
-

474, 526
51, 429
27, 977
6, 486
1, 145
559, 504
3, 357
3, 602
543
726, 495
97, 222
1, 029
73, 566
84, 372
138, 944

1, 496, 708
-

254, 346
1, 805
-

26, 290
-

58, 629
25, 666
461
-

2, 350, 931
210, 794
123, 837
15, 750
1, 385, 757
83, 895
14, 663
559, 504
3, 357
3, 602
543
726, 495
97, 222
1, 029
73, 566
84, 372
138, 944

Total

1, 424, 790

335, 369

2, 250, 197

1, 496, 708

256, 151

26, 290

84, 756

5, 874, 261

20

