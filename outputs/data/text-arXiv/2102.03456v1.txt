BinaryCoP: Binary Neural Network-based COVID-19 Face-Mask Wear
and Positioning Predictor on Edge Devices

arXiv:2102.03456v1 [cs.CV] 6 Feb 2021

Nael Fasfous1* , Manoj-Rohit Vemparala2* , Alexander Frickenstein2* , Lukas Frickenstein1 , Walter Stechele1
1 Technical University of Munich (<first name>.<last name>@tum.de)
2 BMW Group (<first name>.<last name>@bmw.de)

Abstract— Face masks have long been used in many areas of
everyday life to protect against the inhalation of hazardous
fumes and particles. They also offer an effective solution
in healthcare for bi-directional protection against air-borne
diseases. Wearing and positioning the mask correctly is essential
for its function. Convolutional neural networks (CNNs) offer
an excellent solution for face recognition and classification
of correct mask wearing and positioning. In the context of
the ongoing COVID-19 pandemic, such algorithms can be
used at entrances to corporate buildings, airports, shopping
areas, and other indoor locations, to mitigate the spread of
the virus. These application scenarios impose major challenges
to the underlying compute platform. The inference hardware
must be cheap, small and energy efficient, while providing
sufficient memory and compute power to execute accurate
CNNs at a reasonably low latency. To maintain data privacy
of the public, all processing must remain on the edge-device,
without any communication with cloud servers. To address
these challenges, we present BinaryCoP, a low-power binary
neural network classifier for correct facial-mask wear and
positioning. The classification task is implemented on an embedded FPGA accelerator, performing high-throughput binary
operations. Classification can take place at up to ∼6400 framesper-second, easily enabling multi-camera, speed-gate settings
or statistics collection in crowd settings. When deployed on a
single entrance or gate, the idle power consumption is reduced
to 1.6W, improving the battery-life of the device. We achieve
an accuracy of up to 98% for four wearing positions of the
MaskedFace-Net dataset. To maintain equivalent classification
accuracy for all face structures, skin-tones, hair types, and mask
types, the algorithms are tested for their ability to generalize
the relevant features over all subjects using the Grad-CAM
approach.

I. I NTRODUCTION
Convolutional neural networks (CNNs) have been applied
to real-world problems since the early days of their conception [1]. In current times, the ongoing COVID-19 pandemic
presents new challenges, which can be solved with the help
of state-of-the-art computer vision algorithms [2], [3]. One
of the most simple ways of mitigating the spread of the
COVID-19 disease is wearing a face-mask, which can protect
the wearer from direct exposure to the virus through the
mouth and nasal passages. A correctly worn mask can also
protect other people, in case the wearer is already infected
with the disease. This bi-directional protection makes masks
highly effective in crowded and/or indoor areas. Although
face-masks have become a mandatory requirement in many
public areas, it is difficult to ensure the compliance of the
∗

Equally contributed

general public. More specifically, it is difficult to assert that
the masks are worn correctly as intended, i.e. completely
covering the nose, mouth and chin [4].
CNNs are the current state-of-the-art in face detection
applications. Compared to classical computer vision algorithms, CNNs can provide better accuracy on problems with
diverse features without having to manually extract said
features [5]. This holds true only when the training dataset
has a fair distribution of samples. Correctly identifying a
mask on a person’s face is a relatively simple task for these
powerful algorithms. However, a more precise classification
of the exact positioning of the mask and identifying the
exposed region of the face is more challenging. To maintain
equivalent classification accuracy for all face structures, skintones, hair types, and mask types, the algorithms must be able
to generalize the relevant features over all subjects.
The deployment scenarios for the CNN should also be
taken into consideration. A face-mask detector can be set at
the entrance of corporate buildings, shopping areas, airport
checkpoints, and speed gates. These distributed settings require cheap, battery-powered, edge devices which are limited
in memory and compute power. To maintain security and
data privacy of the public, all processing must remain on the
edge-device without any communication with cloud servers.
Minimizing power and resource utilization while maintaining a high classification accuracy is a design challenge which
necessitates hardware-software co-design. In this context,
we propose Binary-CoP (Binary COVID-mask Predictor), an
efficient binary neural network (BNN) classifier for real-time
classification of correct face-mask wear and positioning. The
challenges of the described application are tackled through
the following contributions:
• Training BNNs on synthetically generated data [6] to
cover a wide demographic and generalize relevant taskrelated features. A high accuracy of ∼98% is achieved
for a 4-class problem of mask wear and positioning.
• Deploying BNNs on a low-power, real-time embedded
FPGA accelerator based on the Xilinx FINN architecture [7]. The accelerator can operate at a low-power
of ∼1.6W on single entrances and gates or at highperformance (∼6400 frames-per-second) in crowded
settings.
• The BNNs are analyzed through Gradient-weighted
Class Activation Mapping (Grad-CAM) to improve interpretability and study the features being learned.

II. R ELATED W ORK
A. COVID-19 Face-Mask Wear and Positioning
Correctly worn masks play a pivotal role in mitigating
the spread of the COVID-19 disease during the ongoing
pandemic [8]. Members of the general public often underestimate the importance of this simple yet effective method
of disease prevention and control. Researchers and data
scientists in the field of computer vision have collected
data to train and deploy algorithms which help in automatically regulating masks in public spaces and indoor
locations [9], [10]. Although large-scale natural face datasets
exist, the number of real-world masked images is limited [9].
Wang et al. [10] extended their masked-face dataset with
a Simulated Masked Face Recognition Dataset (SMFRD),
which is synthetically generated by applying virtual masks
to existing natural face datasets. Cabani et al. [6] improved
the generation of synthetically masked-faces by applying a
deformable mask-model onto natural face images with the
help of automatically detected facial key-points. The keypoints of the deformable mask-model can be matched to the
key-points of the face, allowing the application of the mask in
a variety of ways. This allows the dataset generation process
to further generate examples of incorrectly worn masks, such
as chin exposed, nose exposed or nose and mouth exposed.
B. Binary Neural Networks
The memory footprint of neural networks and the complexity of their arithmetic operations on inference hardware
can be reduced through parameter quantization. In the most
extreme case, binarizing neural networks constrains their
weights and activations to {−1, 1}, such that their memory
footprint is theoretically reduced by ×32 compared to a
float-32 CNN [11]. Additionally, simple XNOR and popcount
operations can be used to implement multiply-accumulate
(MAC) operations on inference hardware [12]. Specialized
training schemes have been proposed to mitigate the loss
in information capacity introduced by the low-bitwidth representation of BNNs [11], [13], [14], [15], [12]. In some
cases, the low information capacity due to binarization can
have a regularization effect which improves feature generalization [13]. This is helpful in improving the classification
performance on real-world data, particularly when training
on synthetically generated data [16]. In [13], Courbariaux et
al. introduced a scheme to train neural networks with binary
weights during forward propagation while maintaining latent
full-precision values during back propagation. This ensures
proper gradient flow and fine adjustments through the gradients. This approach is later extended by the binarization
of activations [11]. Rastegari et al. [12] proposed XNORNet, where both weights and activations are binarized such
that the convolutions of input feature maps and weights
can be approximated by a combination of XNOR operations
and popcounts, followed by a multiplication with scaling
factors. The introduction of scaling factors improves the
information capacity of the network at the cost of more
trainable parameters for each layer. This adds to the compu-

tational complexity of XNOR-Net at deployment time. For
the task of face-mask detection with low scene complexity,
more efficient forms of BNNs [11] can be applied.
C. BNN Hardware Accelerators
Several accelerators have been designed to exploit the
benefits of BNNs [17], [18], [7], [19], [20], [21], [22].
The Xilinx FINN [7] framework was developed to accelerate BNNs efficiently on FPGA platforms. The framework
compiles high level synthesis (HLS) code from a BNN
description to create a hardware design for the network.
The generated streaming architecture consists of a pipeline
of individual hardware components instantiated for each
layer of the BNN. In this work, we deploy Binary-CoP on
FINN-based hardware architectures to achieve an efficient
acceleration of the masked-face inference on embedded
FPGAs. We parameterize and synthesize accelerators with
different hardware requirements, geared towards individual
COVID-19 mask recognition (low-power) or crowd statistics
collection (high-performance).
III. M ETHOD
This section describes the building blocks of Binary-CoP
for the classification of correct mask wear and positioning.
A. Training and Inference of Binary Neural Networks
The BNN method proposed by Courbariaux et al. [11]
serves as our foundation to efficiently approximate weights
and activations to single bit precision at inference time,
such that the neural network’s arithmetic operations can
be executed by simple logic operations. Smooth model
training and convergence is ensured by relying on fullprecision latent weights W during training time [23]. In
detail, the activation tensor Al−1 ∈ RXi ×Yi ×Ci , with its
dimensions of Xi width, Yi height, and Ci channels, serves
as the input to the convolutional layer l ∈ [1, ..., L]. Here,
A0 and AL represent the input image and the network’s
prediction, respectively. The trainable parameters of the
2D-convolutional layers are composed of the latent weight
matrix W ∈ RK×K×Ci ×Co required for training, with kernel
dimension K, input channels Ci , and output channels Co . As
previously stated, the latent weights are mapped to {−1, +1}
during the forward pass for loss calculation or deployment,
resulting in the binarized b ⊂ B ∈ BK×K×Ci ×Co . In the
hardware implementation, −1 is expressed as a binary 0
to perform multiplications as XNOR logic operations. The
sign() function in Eq. 1 is used to binarize the input feature
maps and weights.

1
if w ≥ 0,
b = sign(w) =
.
(1)
−1 otherwise
The derivative of the sign() function is almost always zero,
resulting in insufficient gradient flow during training and
back-propagation. This necessitates gradient flow approximation using a straight-through estimator (STE) [23].
Particularly for BNNs, it is of crucial importance to adjust
the input elements al−1 ⊂ Al−1 , before the approximation

Camera

Binary CoP-Net FPGA-based Accelerator

Binary CoP-Net

Prediction
>98% Accuracy

Application Scenario
Power-Saving

Binary convolutional layer:

Input
BatchNorm

1

1

-1

-1

sign

sign

Crowd Feature

Uncovered Nose

Synthesis
MVTU:
• PEs
• SIMD
• Hl-1, Bl

• Pipeline
Buffers
• Sliding Window
Unit (SWU)

• Pipeline
Buffers
• SWU

Performance
up to 6400
Frames per Second

Uncovered Nose & Mouth

Power
~1.6 W

Processing Element (PE)

Uncovered Chin

XNOR

Threshold
Memory

Weight
Memory

Accumulator

Write to PEs

Binary
Conv

Binary
Activations
and
Weights
(<15k Byte)

Correctly Masked

High-Performance

Battery-powered Device

PopCnt

+

>>

Fig. 1: Schematic representation of the Binary-CoP accelerator. A camera captures images to be classified by the neural
network. The BNN accelerator is tailored for the application scenario (single gate prediction or crowd statistics collection).
Binary tensors are processed in the PEs of the FPGA-based accelerator using XNOR operations. The classification of the
input data is available after completion of the computations at low-latency or low-power.
into the binary representation hl−1 ⊂ H l−1 ∈ BXi ×Yi ×Ci
by means of batch normalization to zero mean and unit
variance. An advantage of BNNs is that the result of the
batch-norm operation is followed by sign() (see Fig. 1).
Since the result after applying both functions is simply
{−1, 1}, the precise calculation of the batch-norm is wasteful
on embedded hardware. Based on the batch-norm statistics
collected at training time, a threshold point τ is defined,
wherein an activation value al−1 ≥ τ results in 1, otherwise
-1 [7]. This allows the implementation of the typically costly
batch-norm operation as a simple magnitude comparison
operation on hardware.
Next, the binary convolution follows as:
H l−1 = sign(BatchNorm(Al−1 )); B l = sign(W l )

(2)

Al = BinConv(H l−1 , B l ) = PopCnt(XNOR(H l−1 , B l )),
(3)
which results in the output feature map Al ∈ RXo ×Yo ×Co .
B. Hardware Architecture
The trained BNNs are conditioned for deployment on the
Xilinx FINN framework [7]. The pipelined architecture offers
several advantages on embedded devices, most importantly,
the reduction in on-chip to off-chip memory transfers of
the BNN parameters B l and intermediate activations Al and
H l . This is mainly feasible due to the binary format, which
results in highly compact neural networks that can fit on the
on-chip memory units of embedded devices. The number of
processing elements (PEs), single-instruction-multiple-data
(SIMD)-lanes, and other parameters can be optimized by the

designer to suit the acceleration of the trained BNN. The
final design is synthesized and implemented on an embedded
FPGA.
For each convolutional or fully-connected layer in the
BNN, a matrix-vector-threshold unit (MVTU) is instantiated,
which executes the XNOR, popcount and threshold operations
mentioned in Sec. III-A. Each MVTU in the pipeline can be
dimensioned for the number of PEs and SIMD lanes, which
have a significant impact on hardware resource utilization,
latency and the effective throughput of the pipeline. Based
on the compute complexity of each layer, the available
hardware resources need to be distributed over the corresponding MVTUs, such that all parts of the pipeline have
a matched-throughput. A single under-dimensioned MVTU
could throttle the entire pipeline, resulting in sub-optimal
throughput. A single MVTU of the pipeline is shown in
Fig. 1, and a corresponding PE is detailed.
For convolutional layers, an additional sliding-window
unit (SWU) reshapes the binarized activation maps to create a
single, wide input feature map memory, which can efficiently
be accessed by the corresponding MVTU. Max-pool layers
are implemented as boolean OR operations, since a single
binary “1” value suffices to make the entire pool window
output equal to 1.
C. BNN Interpretability with Grad-CAM
The output of the convolutional layers in a CNN contains
localized information of the input image, without any prior
bias on the location of objects and features during training.
This information can be captured using Class Activation

Mapping (CAM) [24] and Gradient-weighted Class Activation Mapping (Grad-CAM) [25] techniques. To apply CAM,
the model must end with a global average pooling layer
followed by a fully-connected layer, providing the logits of a
particular input. The BNN models investigated in this work
operate on a small input resolution of 32×32, and achieve a
high reduction of spatial information without incorporating a
global average pooling layer. For this reason, the Grad-CAM
approach is better-suited to obtain visual interpretations of
Binary-CoP’s attention and determine the important regions
for its predictions of different inputs and classes.
To obtain the class-discriminative localization map, we
consider the activations and gradients for the output of the
conv2 2 layer, which has spatial dimensions of 5×5. We
use average pooling for the corresponding gradients and
reduce the channels by performing Einstein summation as
specified in [25]. With this approach the base networks do not
need any modifications or retraining. Due to the synthetically
generated dataset used for training, we expect Binary-CoP
models to generalize well against domain shifts.
IV. R ESULTS AND D ESIGN S PACE E XPLORATION
A. Experimental Setup
Binary-CoP is able to detect the presence of a mask, as
well as its position and correctness. This level of classification detail is possible through the more detailed split
of the MaskedFace-Net dataset [6] from 2 classes, namely
Correctly Masked Face Dataset (CMFD) and Incorrectly
Masked Face Dataset (IMFD), to 4 classes of CMFD, IMFD
Nose, IMFD Chin, and IMFD Nose and Mouth. The dataset
suffers from high imbalance in the number of samples per
class. From the total 133,783 samples, roughly 5% of the
samples are IMFD Chin, and another 5% samples are IMFD
Nose and Mouth. CMFD samples make up 51% of the total
dataset while IMFD Nose makes up 39%. The dataset in
its raw distribution would heavily bias the training towards
the two dominant classes. To counter this, we randomly
sample the larger classes CMFD and IMFD Nose to collect
a comparable number of examples to the two remaining
classes, IMFD Chin and IMFD Nose and Mouth. The evenly
balanced dataset is then randomly augmented with a varying
combination of contrast, brightness, gaussian noise, flip and
rotate operations. The final size of the balanced dataset is
110K train and validation examples and 28K test samples.
The images are resized to 32×32 pixels, similar to the
CIFAR-10 [26] dataset. The BNNs are trained up to 300
epochs, unless learning saturates earlier. The full-precision
(FP32) variant used for the Grad-CAM comparison is trained
for 175 epochs due to early learning saturation (98.6% final
test accuracy). We trained the BNN architectures shown
in Tab. I according to the method described in Sec.III-A.
Each convolutional (Conv) and fully connected (FC) layer
is followed by batch-norm and activation layers except for
the final layer. Conv groups 1 and 2 are followed by a maxpool layer. The target System-on-Chip (SoC) platform for
the experiments is the Xilinx XC7Z020 (Z7020) chip. The µCNV design can also be synthesized for the more constrained

XC7Z010 (Z7010) chip, when XNOR operations are offloaded
to the DSP blocks as described in [27]. Power and throughput
measurements are taken directly on a running system. The
power is measured at the power supply of the board (includes
both PS and PL). The throughput reported is the classification
rate when the accelerator’s pipeline is full.
TABLE I: Network architectures and hardware dimensioning.
Network
Arch.
L | [Ci , Co ]
K = 3 ∀ Conv

PE Count
SIMD lanes

n-CNV

CNV
Conv
Conv
Conv
Conv
Conv
Conv
FC 1
FC 2
FC 3

1 1 | [3, 64]
1 2 | [64, 64]
2 1 | [64, 128]
2 2 | [128, 128]
3 1 | [128, 256]
3 2 | [256, 256]
| [512]
| [512]
| [4]

16, 32, 16, 16, 4, 1, 1, 1, 4
3, 32, 32, 32, 32, 32, 4, 8, 1

Conv
Conv
Conv
Conv
Conv
Conv
FC 1
FC 2
FC 3

1 1 | [3, 16]
1 2 | [16, 16]
2 1 | [16, 32]
2 2 | [32, 32]
3 1 | [32, 64]
3 2 | [64, 64]
| [128]
| [128]
| [4]

16, 16, 16, 16, 4, 1, 1, 1, 1
3, 16, 16, 32, 32, 32, 4, 8, 1

µ-CNV
Conv
Conv
Conv
Conv
Conv
FC 1
FC 2

1 1 | [3,16]
1 2 | [16, 16]
2 1 | [16, 32]
2 2 | [32, 32]
3 1 | [32, 64]
| [128]
| [4]

4, 4, 4, 4, 1, 1, 1
3, 16, 16, 32, 32, 16, 1

B. Design Space Exploration
We evaluate three Binary-CoP prototypes, namely CNV,
n-CNV and µ-CNV. The CNV network is based on the
architecture in [7] inspired by VGG-16 [28] and BinaryNet [11]. n-CNV is a downsized version for a smaller
memory footprint, and µ-CNV has fewer layers to reduce the
size of the synthesized design. All designs are synthesized
with a target clock frequency of 100MHz.
In Tab. II, the hardware utilization for the Binary-CoP
prototypes is provided. With µ-CNV, a significant reduction
in LUTs is achieved, which makes the design synthesizable
on the heavily constrained Z7010 SoC. The trade-off is a
slight increase in the memory footprint of the BNN, as
the shallower network has a larger spatial dimension before
the fully-connected layers, increasing the total number of
parameters after the last convolutional layer. The choice of
PE count and SIMD lanes for the n-CNV prototype allow
it to reach a maximum throughput of ∼6400 classifications
per second when its pipeline is full. This high-performance
can be used to split large crowd images and classify them at
a high-rate to detect uncovered faces in a scene. Conversely,
for single entrance/gate classifications, all prototypes have an
idle power of around 1.6W, which is required mostly by the
soft-core on the SoC. In this setting, a classification needs
to be triggered only when a subject is attempting to pass
through the entrance where Binary-CoP is deployed.
C. Grad-CAM Analysis
The confusion matrix in Fig. 2 shows the generalization of
Binary-CoP-CNV on all classes after balancing the dataset.
We further analyze the output heat maps generated by GradCAM to interpret the predictions of our BNNs with respect
to the diverse attributes of the MaskedFace-Net dataset. In
Fig. 3 - Fig. 9, column 1 and 2 indicate the label and input
image respectively. Columns 3, 4 and 5 highlight the heat
maps obtained from the Grad-CAM output of Binary-CoPCNV, Binary-CoP-n-CNV and a full-precision version of
CNV with float-32 parameters (FP32). The heat maps are
overlayed on the raw input images for better visualization.
All raw images chosen have been classified correctly by all

TABLE II: Hardware results of design space exploration.
Configuration

LUT

BRAM

DSP

Acc.

CNV

26060

124

24

98.10

n-CNV

20425

10.5

14

93.94

µ-CNV

11738

14

27

93.78

7125
98%

41
1%

1
0%

90
1%

Nose

26
0%

7042
98%

94
2%

26
0%

N+M

4
0%

79
1%

5651
98%

9
0%

True Class

Correct

Label

Raw

BCoP
CNV

BCoP
n-CNV

FP32

Nose
Exposed
Nose
Exposed
Nose
Exposed

Fig. 4: Grad-CAM results for the nose-exposed class.
Label
Chin

107
1%

41
1%

7
0%

7363
98%

Correct

Nose

N+M

Chin

Fig. 2: Confusion matrix of Binary-CoP-CNV on the test set.

Nose
Mouth
Exposed

the networks, for fair interpretation of feature-to-prediction
correlation.
In Fig. 3, we analyze the Region of Interest (RoI) for
the correctly masked class. Binary-CoP’s learning capacity
allows it to focus on key facial lineaments of the human
wearing the mask, rather than the mask itself. This potentially
helps in generalizing on other mask types. For the child
example shown in the first row, the focus of Binary-CoP
variants lies on the nose, making sure that it is fully covered
by the mask. Similarly, for the adult in row 2, BinaryCoP-CNV focuses on the exposed cheekbones, to assert its
correct mask prediction. This also holds for our small version
of Binary-CoP, with significantly reduced learning capacity.
The RoI curves finely above the mask, tracing the exposed
region of the face. In the third row example, Binary-CoPCNV falls back to focusing on the mask, whereas Binary-

Nose
Mouth
Exposed

Raw

BCoP
CNV

BCoP
n-CNV

BCoP
CNV

BCoP
n-CNV

FP32

Nose
Mouth
Exposed

Predicted Class

Label

Raw

FP32

Correctly
Masked
Correctly
Masked
Correctly
Masked

Fig. 3: Grad-CAM results for the correctly-masked class.

Fig. 5: Grad-CAM results for the nose and mouth-exposed
class.

CoP-n-CNV continues to focus on the exposed features.
Both models achieve the same prediction by focusing on
different parts of the raw image. In contrast to the BinaryCoP variants, the full precision FP32 model seems to focus
on a combination of several different features. This can be
attributed to its larger learning capacity.
In Fig. 4, we analyze the Grad-CAM output of the uncovered nose class. Binary-CoP-CNV and Binary-CoP-n-CNV
focus specifically on two regions, namely the nose and the
straight upper edge of the mask. These clear characteristics
cannot be observed with the oversized FP32 CNN. In Fig 5,
the results show the RoI for predicting the exposed mouth
and nose class. All models seem to distribute their attention
onto several exposed features of the face.
Fig. 6 shows Grad-CAM results which predict the chin
being exposed. The top region of the mask points upwards,
similar to the correctly worn mask. Therefore, the BNNs pay
less attention to this region and instead focus on the neck
and chin. With the full precision FP32 model, it is difficult
to interpret the reason for the correct classification, as little
to no focus is given to the chin region.
Beyond studying the BNNs’ behavior on different class
predictions, we can use the attention heat maps to understand
the generalization behavior of the classifier. In Fig. 7 - Fig. 9,

Label

Raw

BCoP
CNV

BCoP
n-CNV

FP32

Label

Chin
Exposed

Correctly
Masked

Chin
Exposed

Correctly
Masked

Chin
Exposed

Nose
Exposed

Fig. 6: Grad-CAM results for the chin-exposed class.
Label

Raw

BCoP
CNV

BCoP
n-CNV

FP32

Label

Correctly
Masked

Correctly
Masked

Correctly
Masked

Correctly
Masked

Chin
Exposed

Fig. 7: Grad-CAM results for age generalization.

Nose
Mouth
Exposed

we test Binary-CoP’s generalization over ages, hair colors
and head gear, as well as complete face manipulation with
double-masks, face paint and sunglasses. In Fig 7, we see that
the smaller eyes of infants and elderly do not hinder BinaryCoP’s ability to focus on the top region of the correctly
worn masks. In Fig. 8, Binary-CoP-CNV shows resilience
to differently colored hair and head-gear, even when having
a similar light-blue color as the face-masks (row 2 and
3). In contrast, the FP32 model’s attention seems to shift
towards the hair and head-gear for these cases. Finally, in
Fig. 9, both Binary-CoP variants focus on relevant features
of the corresponding label, irrespective of the obscured or
manipulated faces. This empirically shows that the complex
training of BNNs, along with their lower information capacity, constrains them to focus on a smaller set of relevant
features, thereby generalizing well for unprecedented cases.

Nose
Mouth
Exposed

V. C ONCLUSION

BCoP
CNV

BCoP
n-CNV

FP32

Fig. 8: Grad-CAM results for hair/headgear generalization.

Correctly
Masked

In this paper, we apply binary neural networks to the
task of classifying the correctness of face-mask wear and
positioning. In the context of the ongoing COVID-19 pandemic, such algorithms can be used at entrances to corporate
buildings, airports, shopping areas, and other indoor locations
to mitigate the spread of the virus. Applying BNNs to this
application solves several challenges such as (1) Maintaining
data privacy of the public by processing data on the edge-

Raw

Raw

BCoP
CNV

BCoP
n-CNV

FP32

Fig. 9: Grad-CAM results for face manipulation with doublemasks, face paint and sunglasses.

device, (2) Deploying the classifier on an efficient XNORbased accelerator to achieve low-power computation, and
(3) Minimizing the neural network’s memory footprint by
representing all parameters in the binary domain, enabling
deployment on low-cost, embedded hardware. The accelerator requires only ∼1.6W of power for operation on single
gates/entrances. Alternatively, high-performance is possible,
providing fast batch classification for statistics collection in
crowded settings at ∼6400 frames-per-second. We achieve
an accuracy of up to 98% for four wearing positions of the
MaskedFace-Net dataset. The Grad-CAM approach is used
to study the features learned by the proposed Binary-CoP
classifier. The results show the classifier’s high generalization ability, allowing it to perform well on different face
structures, skin-tones, hair types, and age groups.
R EFERENCES
[1] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,

vol. 86, no. 11, pp. 2278–2324, 1998.
[2] L. Wang and A. Wong, “Covid-net: A tailored deep convolutional
neural network design for detection of covid-19 cases from chest xray images,” 2020.
[3] A. I. Khan, J. L. Shah, and M. M. Bhat, “Coronet: A
deep neural network for detection and diagnosis of covid-19
from chest x-ray images,” Computer Methods and Programs in
Biomedicine, vol. 196, p. 105581, Nov 2020. [Online]. Available:
http://dx.doi.org/10.1016/j.cmpb.2020.105581
[4] “When and how to use masks.” [Online]. Available:
https://www.who.int/emergencies/diseases/novel-coronavirus-2019/
advice-for-public/when-and-how-to-use-masks
[5] N. O’Mahony, S. Campbell, A. Carvalho, S. Harapanahalli, G. V.
Hernandez, L. Krpalkova, D. Riordan, and J. Walsh, “Deep learning vs.
traditional computer vision,” in Advances in Computer Vision, K. Arai
and S. Kapoor, Eds. Cham: Springer International Publishing, 2020,
pp. 128–144.
[6] A. Cabani, K. Hammoudi, H. Benhabiles, and M. Melkemi,
“Maskedface-net – a dataset of correctly/incorrectly masked
face images in the context of covid-19,” Smart Health, 2020.
[Online]. Available: http://www.sciencedirect.com/science/article/pii/
S2352648320300362
[7] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong,
M. Jahre, and K. Vissers, “Finn: A framework for fast, scalable
binarized neural network inference,” in Proceedings of the 2017
ACM/SIGDA International Symposium on Field-Programmable Gate
Arrays, ser. FPGA ’17. New York, NY, USA: ACM, 2017, pp. 65–74.
[Online]. Available: http://doi.acm.org/10.1145/3020078.3021744
[8] T. Mitze, R. Kosfeld, J. Rode, and K. Wälde, “Face masks
considerably reduce covid-19 cases in germany,” Proceedings of the
National Academy of Sciences, vol. 117, no. 51, pp. 32 293–32 301,
2020. [Online]. Available: https://www.pnas.org/content/117/51/32293
[9] S. Ge, J. Li, Q. Ye, and Z. Luo, “Detecting masked faces in the wild
with lle-cnns,” in 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017, pp. 426–434.
[10] Z. Wang, G. Wang, B. Huang, Z. Xiong, Q. Hong, H. Wu, P. Yi,
K. Jiang, N. Wang, Y. Pei, H. Chen, Y. Miao, Z. Huang, and J. Liang,
“Masked face recognition dataset and application,” 2020.
[11] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv,
and Y. Bengio, “Binarized neural networks,” in Advances
in Neural Information Processing Systems 29.
Curran
Associates, Inc., 2016, pp. 4107–4115. [Online]. Available:
http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf
[12] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “XNORNet: ImageNet Classification Using Binary Convolutional Neural
Networks,” in The European Conference on Computer Vision (ECCV).
Cham: Springer International Publishing, 2016, pp. 525–542.
[13] M. Courbariaux, Y. Bengio, and J.-P. David, “Binaryconnect: Training deep neural networks with binary weights during propagations,”
in Advances in Neural Information Processing Systems (NeurIPS),
C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,
Eds. Curran Associates, Inc., 2015, pp. 3123–3131.
[14] S. Darabi, M. Belbahri, M. Courbariaux, and V. P. Nia, “BNN+:
improved binary network training,” CoRR, vol. abs/1812.11800, 2018.
[Online]. Available: http://arxiv.org/abs/1812.11800
[15] X. Lin, C. Zhao, and W. Pan, “Towards accurate binary convolutional
neural network,” in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc.,
2017, pp. 345–353. [Online]. Available: http://papers.nips.cc/paper/
6638-towards-accurate-binary-convolutional-neural-network.pdf
[16] A. Frickenstein, M.-R. Vemparala, J. Mayr, N.-S. Nagaraja, C. Unger,
F. Tombari, and W. Stechele, “Binary DAD-Net: Binarized Drivable
Area Detection Network for Autonomous Driving,” in International
Conference on Robotics and Automation (ICRA), Paris, France, 2020.
[17] R. Andri, L. Cavigelli, D. Rossi, and L. Benini, “Yodann: An architecture for ultralow power binary-weight cnn acceleration,” IEEE
Transactions on Computer-Aided Design of Integrated Circuits and
Systems, vol. 37, no. 1, pp. 48–60, Jan 2018.
[18] K. Ando, K. Ueyoshi, K. Orimo, H. Yonekawa, S. Sato, H. Nakahara,
S. Takamaeda-Yamazaki, M. Ikebe, T. Asai, T. Kuroda, and M. Motomura, “Brein memory: A single-chip binary/ternary reconfigurable
in-memory deep neural network accelerator achieving 1.4 tops at 0.6
w,” IEEE Journal of Solid-State Circuits, vol. 53, no. 4, pp. 983–994,
April 2018.

[19] S. Liang, S. Yin, L. Liu, W. Luk, and S. Wei, “Fp-bnn,”
Neurocomput., vol. 275, no. C, p. 1072–1086, Jan. 2018. [Online].
Available: https://doi.org/10.1016/j.neucom.2017.09.046
[20] P. Guo, H. Ma, R. Chen, P. Li, S. Xie, and D. Wang, “Fbna: A
fully binarized neural network accelerator,” in 2018 28th International
Conference on Field Programmable Logic and Applications (FPL),
2018, pp. 51–513.
[21] E. Nurvitadhi, D. Sheffield, Jaewoong Sim, A. Mishra, G. Venkatesh,
and D. Marr, “Accelerating binarized neural networks: Comparison of
fpga, cpu, gpu, and asic,” in 2016 International Conference on FieldProgrammable Technology (FPT), 2016, pp. 77–84.
[22] C. Fu, S. Zhu, H. Su, C.-E. Lee, and J. Zhao, “Towards fast
and energy-efficient binarized neural network inference on fpga,” in
Proceedings of the 2019 ACM/SIGDA International Symposium on
Field-Programmable Gate Arrays, ser. FPGA ’19. New York, NY,
USA: Association for Computing Machinery, 2019, p. 306. [Online].
Available: https://doi.org/10.1145/3289602.3293990
[23] Y. Bengio, N. Léonard, and A. C. Courville, “Estimating or
propagating gradients through stochastic neurons for conditional
computation,” CoRR, vol. abs/1308.3432, 2013. [Online]. Available:
http://arxiv.org/abs/1308.3432
[24] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba,
“Learning deep features for discriminative localization,” in IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), June
2016, pp. 2921–2929.
[25] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via
gradient-based localization,” in Proceedings of the IEEE International
Conference on Computer Vision (ICCV), Oct 2017.
[26] A. Krizhevsky, “Learning multiple layers of features from tiny images,” University of Toronto, 2009.
[27] N. Fasfous, M. R. Vemparala, A. Frickenstein, and W. Stechele, “Orthruspe: Runtime reconfigurable processing elements for binary neural
networks,” in 2020 Design, Automation Test in Europe Conference
Exhibition (DATE), 2020, pp. 1662–1667.
[28] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” in International Conference on
Learning Representations, 2015.

