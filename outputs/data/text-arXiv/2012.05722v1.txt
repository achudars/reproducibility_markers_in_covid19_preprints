arXiv:2012.05722v1 [cs.LG] 7 Dec 2020

Using Differentiable Programming for Flexible
Statistical Modeling
Maren Hackenberg, Marlon Grodd, Clemens Kreutz
Institute of Medical Biometry and Statistics,
Faculty of Medicine and Medical Center, University of Freiburg, Germany
and
Martina Fischer, Janina Esins, Linus Grabenhenrich
Robert-Koch-Institut, Berlin, Germany
and
Christian Karagiannidis
Department of Pneumology and Critical Care Medicine, Cologne-Merheim Hospital,
ARDS and ECMO Center, Kliniken der Stadt Köln,
Witten/Herdecke University Hospital, Cologne, Germany
and
Harald Binder
Institute of Medical Biometry and Statistics,
Faculty of Medicine and Medical Center, University of Freiburg, Germany
December 11, 2020
MH acknowledges funding by the DFG (German Research Foundation) – 322977937/GRK2344.
The authors thank Gerta Rücker for her thoughtful comments on the manuscript.
Abstract
Differentiable programming has recently received much interest as a paradigm that
facilitates taking gradients of computer programs. While the corresponding flexible
gradient-based optimization approaches so far have been used predominantly for deep
learning or enriching the latter with modeling components, we want to demonstrate
that they can also be useful for statistical modeling per se, e.g., for quick prototyping
when classical maximum likelihood approaches are challenging or not feasible.
In an application from a COVID-19 setting, we utilize differentiable programming
to quickly build and optimize a flexible prediction model adapted to the data quality
challenges at hand. Specifically, we develop a regression model, inspired by delay differential equations, that can bridge temporal gaps of observations in the central German
registry of COVID-19 intensive care cases for predicting future demand. With this exemplary modeling challenge, we illustrate how differentiable programming can enable
simple gradient-based optimization of the model by automatic differentiation. This
allowed us to quickly prototype a model under time pressure that outperforms simpler benchmark models. We thus exemplify the potential of differentiable programming
also outside deep learning applications, to provide more options for flexible applied
statistical modeling.

Keywords: Differential Equations; Machine Learning; Optimization; Workflow
1

1

Introduction

Recently, differentiable programming (e.g., Innes et al. 2019) has become a prominent concept in the machine learning community, in particular for deep neural networks. Parameter
estimation for the latter has long been conceptualized via backpropagation of gradients
through a chain of matrix operations. The paradigm of differentiable programming now
allows to view artificial neural networks more generally as non-linear functions specified via
computer programs, while still providing gradients for parameter estimation via automatic
differentiation. For example, this has allowed to incorporate other modeling approaches,
such as differential equations, that can better reflect the problem structure in the task at
hand (Rackauckas et al. 2020). Given that differentiable programming now enables such a
shift of neural networks towards modeling, we conjecture that it might also be useful for
enhancing statistical modeling per se. In the following, we describe a modeling challenge,
where differentiable programming allowed us to quickly prototype a prediction model for
COVID-19 intensive care unit (ICU) data from the central German registry, and also more
generally introduce and illustrate differentiable programming for statistical modeling.
While statistical modeling is sometimes considered as distinct from black box algorithmic modeling in machine learning (Breiman 2001), there are many examples where both
fields could benefit from the exchange and joint development of techniques (e.g., Friedman
et al. 2000). In particular, Efron (2020) provides a compelling perspective on how pure
prediction algorithms relate to traditional methods with respect to the central statistical
tasks of prediction, estimation and attribution, and points out directions for combining their
respective advantages and thus enriching statistical methods development. In this spirit,
we present an illustrative example that integrates concepts of differential equations and
differentiable programming with regression modeling.
Traditional regression-based methods assume a "surface plus noise" formulation (Efron
2020): an underlying structural component describes the true process we are interested
in but can only be accessed by noisy observations. Parameter estimation in such models typically is based on maximum likelihood techniques, e.g., using Fisher scoring as an
iterative approach. An overview on many different types of corresponding regression modeling approaches is provided, for example, in Fahrmeir & Tutz (2001). Machine learning
approaches, such as deep neural networks, typically rely on some loss function, which can
be based on a likelihood but does not have to. Optimization then is typically performed
using a gradient-descent algorithm. In contrast to Fisher scoring, this does not require
specification of second derivatives, which makes it more difficult to determine step sizes
and quantify uncertainty, but is more generally applicable. In particular, this enables the
paradigm of differentiable programming, where loss functions can be specified as computer
programs and optimized via automatic differentiation. While it is a well-established, pervasive tool in scientific computing (see, e.g., Griewank 1989), the field of deep learning has
only recently more broadly embraced the applicability of automatic differentiation tools to
any prediction model that relies on the gradient-based optimization of a differentiable loss
function. Furthermore, automatic differentiation frameworks enable to differentiate through
computer programs, supporting control flow and recursion (Baydin et al. 2017, Innes et al.
2019), e.g., allowing to incorporate solvers of differential equations into neural networks to
reflect structural assumptions (Chen et al. 2018). Differentiable programming emerged as
a descriptive term for this resulting new paradigm (LeCun 2018). Its core idea is to use
automatic differentiation to allow for flexible models that seamlessly integrate elements of
2

deep learning as well as mechanistic modeling (Rackauckas et al. 2020).
We are convinced that differentiable programming can also be useful for applied statistical modeling, e.g., for quick prototyping when other parameter optimization techniques like
maximum likelihood Fisher scoring are challenging. Such an approach also more generally
has the potential for combining the two optimization paradigms of machine learning and
regression-based modeling and thereby opens up new routes for model building. To exemplify this potential, we present a COVID-19 application, where we employ differentiable
programming to predict prevalent COVID-19 cases in ICUs with regression models, flexibly
adapted to handle temporal gaps in longitudinal observations. In particular, we design a
statistical model that is inspired by differential equations, introducing dependence of the
model on its own past (similar to delay differential equations), and show how differentiable
programming allows to solve the corresponding non-linear optimization problem.
In the following, we briefly describe the data setting of our application. We then introduce our model and the differentiable programming framework we employ to optimize it.
We present results on prediction performance, investigate the sensitivity of the model with
respect to time intervals, and explore different model variants. The closing discussion also
provides further remarks on the more general applicability of differential programming in
statistical modeling.

2

An exemplary application

To illustrate how differentiable programming can be used to estimate parameters of a statistical model with prediction capabilities in a setting where quality issues in the data make
straightforward regression modeling challenging, we present an application in a COVID-19
context.
Here, the aim is to predict future ICU demand, based on past daily reports of prevalent
cases in hospitals and numbers of new infections in the population. During the first wave
of COVID-19 infections in Germany, however, many hospitals did not yet report prevalent
cases daily, such that the data is characterized by a large amount of missing values, often
also over longer periods of time. Any prediction modeling approach will have to deal with
these missing values. While these could potentially be imputed, any imputation scheme
should ideally incorporate typical temporal patterns, which in turn only are obtained after
modeling. As a further challenge for modeling, in particular during the first wave of infections, the pandemic dynamics differed widely between regions as well as between larger and
smaller hospitals, requiring individual models for each hospital. Yet, with not many time
points available as data collections had only just been set up, this implies restrictions on the
number of parameter that can be estimated in these hospital-specific models. Nonetheless,
an approach to explicitly model the course of the prevalent ICU cases over time and to make
day-to-day predictions for future demand was urgently needed in the pandemic situation.
Thus, we developed a model that can flexibly deal with the missingness patterns in the data
without requiring any imputation and that can predict future ICU demand by modeling the
increments of prevalent ICU cases over time individually for each hospital. As we will elaborate further in the next section, here we can take advantage of differentiable programming
to quickly prototype a flexible prediction model and estimate its parameters, even though
classical maximum likelihood techniques are challenging due to the specific data scenario.
Specifically, our model is based on data from the DIVI intensive care registry, a joint

3

3

2

1

0
Fri 17

4

3

2

1

0
Fri 17

Tue 21

5

4

3

2

1

Number of prevalent COVID-19 cases

4

5

Number of prevalent COVID-19 cases

Number of prevalent COVID-19 cases

Number of prevalent COVID-19 cases

5

Prevalent cases reported?
Prevalent cases reported?
missing

5

Prevalent cases reported?
missing
reported

4

reported

missing
reported

Prevalent cases reported?
missing
reported

3

2

1

0 03 Thu 07 Mon 11
Tue021
Sat 25 Wed 29 May
Fri 15
Tue 19
Sat 23 Wed 27 May 31 Wed 03 Jun 07
Thu 11 Mon 15
Fri 19
Tue 23
Wed
29 May
0725 Mon
11 29Fri
Tue
2329
Wed
May
31 07
Wed
07Fri
ThuWed
11Tue
Mon
15Sat31
Fri
19 Wed
23 May
FriWed
17
Tue15
21 03
Sat19
25 Sat
Wed
May
Thu
Mon
11Sat
19 May
23
27Jun
Fri 17
Tue0321 Thu
Sat
May
Thu
07
Mon
11 2703
Fri
15
Tue03
19 Jun
2315
27
Wed
03Tue
07 31 Wed
Thu 03
11 Jun
Mon07
15 Thu
Fri 11
19 Mon
Tue1523 Fri 19
Date
Date
Date
Date

Sat 25

Tue 23

Figure 1: Exemplary daily numbers of prevalent ICU cases of a hospital from the DIVI
registry between April 16, 2020, and June 24, 2020. Reported values are shown in blue,
missing values imputed by a last-observation-carried-forward scheme are shown in red.
project of the German Interdisciplinary Association for Intensive and Emergency Medicine
(DIVI) and the Robert Koch Institute (RKI) that documents capacities for intensive care
and records numbers of COVID-19 cases currently treated in ICUs of participating hospitals.
We consider data from April 16, 2020, onwards, when a regulation came into effect that
made daily reporting compulsory for all German acute care hospitals with ICUs. However,
hospitals only gradually conformed to the regulation, with many joining the registry later
or failing to report on a regular basis. This leads to a substantial amount of time intervals
without reports for some hospitals in the registry: Until June 24, 2020, 1281 hospitals have
been participating in the registry with a total of 6.41% missing daily reports for prevalent
COVID-19 cases. For 463 (36.14%) hospitals, all daily reports are available, while for 187
(14.60%) hospitals, more than 10% of the daily reports are missing. Additionally, some
hospitals have large gaps in their reporting, with e.g. 7.11% of hospitals having more than
25% missing daily reports.
Figure 1 exemplarily depicts the daily numbers of reported prevalent COVID-19 ICU
cases in a hospital from the DIVI registry, with missing observations imputed for graphical
display using a last-observation-carried-forward principle.
As a further input for predicting the future number of prevalent ICU cases individually
for each hospital, we incorporate predicted COVID-19 incidence based on data from the
RKI. For these predictions, an ordinary differential equation model is employed to describe
the temporal changes of the number of susceptible, exposed, infected and removed (SEIR)
people (Allen et al. 2008). To account for containment measures, a time dependent infection
rate is assumed that is described by a smoothing spline (Schelker et al. 2012). All model
parameters are estimated by maximum likelihood at the level of the 16 German federal
states. The local dynamics at the level of the 412 counties are then estimated by fitting a
scaling parameter and an observation error parameter to the reported new infections in each
county. The resulting model predictions provide daily information on the local pressure of
new cases that could potentially turn into ICU cases. As these predictions just serve as an
input, we do not provide further details on the underlying model.

4

3
3.1

Methods
Developing a flexible regression model to predict the increments of
prevalent COVID-19 ICU cases

Numbers of COVID-19 patients vary between hospitals due to regional differences in infection rates and regulating mechanisms of hospitals within one area (e.g., in a city like
Freiburg with a large university medical center and several smaller hospitals, COVID-19
patients tend to be transferred to the large center), meaning that predicted county-level
COVID-19 incidences translates into a different number of ICU cases for each hospital.
Therefore, we fit an individual model for each of the 1281 participating hospitals from the
DIVI register. We denote with y = (y1 , . . . , yT )> ∈ RT the numbers of prevalent ICU cases,
where T ∈ N is the number of observations days in the registry from April 16, 2020 onwards,
and aim at modeling the increments (yt − yt−1 ) for t = 2, . . . , T .
For each hospital, we employ a simple linear regression model with the absolute numbers
of prevalent COVID-19 ICU cases y and the incident county-level COVID-19 cases z =
(z1 , . . . , zT )> ∈ RT in the county of the hospital as covariates, representing the potential
for decrease (e.g., due to discharge) and increase (through conversion to ICU cases) with
respect to the future number of prevalent ICU cases in a hospital, respectively:
(yt − yt−1 ) = β1 + β2 · yt−1 + β3 · zt−1 ,

t = 2, . . . , T.

(1)

If the numbers of prevalent cases y were a smooth function y(t) ∈ C([t0 , T ]; R) defined on
the observed time interval, the increments of the prevalent ICU cases could be viewed as a
crude approximation to its derivative. Including the absolute numbers of the prevalent ICU
cases yt−1 at the previous day t − 1 as a covariate in (1) then resembles a dependence of the
approximate derivate on the history of the function itself, like in delay differential equations.
Note that this is to be understood as a loose analogy for motivating our approach.
In the following, we describe how this model formulation allows to account for missing
daily reports when defining a squared-error loss function for subsequently estimating the
model parameters by gradient descent. We encode the information of whether a specific
hospital reported their number of prevalent ICU cases at a given time point as a binary vector
r ∈ {0, 1}T , where rt = 1 if yt is reported, and rt = 0 otherwise, for t = 1, . . . , T . Whenever
the prevalent ICU cases at a certain day have actually been reported, this observation
contributes to the loss function, by including the squared difference between the model
prediction with the current parameter estimates and the true observed value. However, if
a daily report is missing, there is no true value to compare a prediction to, so we use the
current state of the model to predict a value for the respective day and move on to the next
time point, where the procedure is repeated. In this way, we carry the model prediction
forward in time (like a differential equation), until a new observation becomes available that
we can include in the loss function.
More formally, defining dyt := yt − yt−1 , the predicted increments of prevalent ICU cases
for a specific hospital are given by
(
yt ,
if rt = 1,
c
dy
et + β3 · zt , where yet =
(2)
t+1 = β1 + β2 · y
c
yet−1 + dy t , else.
We can thus define a loss function that includes only the predictions for those time points
5

where a daily report is available:
1

L(y, z, r, β) = PT

t=2 rt

T
X

c − (e
rt · (dy
yt − yet−1 ))2
t

(3)

t=2

Finding optimal model parameters then amounts to minimizing the loss function with respect to the regression coefficients.
If there are missing daily reports at the beginning of the time interval, i.e., r1 , . . . , rs =
0 for some s < T , those values are skipped and the first non-missing value ytstart =
yet start , tstart = mint∈{1,...,T } {rt | rt = 1} is used to calculate the first prediction dytstart+1 .

3.2

Differentiable programming

Differentiable programming is a paradigm that allows to take gradients of computer programs for gradient-based parameter estimation, using automatic differentiation without requiring user intervention or refactoring. Several techniques have been used in the past to
access the gradients that are at the core of differentiable programming (Baydin et al. 2017):
While manually working out analytical derivatives is time-consuming, prone to errors and
not always feasible (if no closed-form expression is available), numerical differentiation by
finite difference approximations is easy to implement but can incur rounding and truncation errors. Symbolic differentiation quickly results in long, complex expressions, while
also requiring a closed-form expression. In contrast, automatic differentiation allows for
exact evaluation of derivatives at machine precision and can be applied to regular computer
programs without requiring closed-form expressions, thus maintaining maximal expressivity.
For our application, we use a differentiable programming system implemented in the
Zygote.jl package (Innes et al. 2019) for the Julia programming language (Bezanson
et al. 2017). A key advantage of Julia with respect to other dynamic languages, such as
Python or R, is the broad ecosystem of packages written entirely in Julia itself, making
them accessible for automatic differentiation.
Many state-of-the-art reverse-mode automatic differentiation tools employ tracing methods that evaluate derivatives only at specific points in the program space which requires to
unroll all control flow and re-compile for every new input value (e.g., Johnson et al. 2018,
Abadi et al. 2016, Innes et al. 2018). In contrast, Zygote.jl applies a source-to-source
transformation (Innes et al. 2019), i.e., generates a single derivative function from the original code which can then be compiled, heavily optimized, and re-used for all input values
(Baydin et al. 2017). This transformation supports language constructs, such as control
flow and recursion, as well as user defined data types in a memory-efficient form. With respect to prior work on source-to-source automatic differentiation (e.g., Bischof et al. 1996),
the main novelty of Zygote.jl is its adaptation for a full, high level dynamic language
such as Julia (Innes et al. 2019). In addition, the source-to-source automatic differentiation
system of Zygote.jl seamlessly integrates with existing Julia packages without requiring
any user intervention of refactoring.
Following Innes et al. (2019), we briefly describe the differentiable programming system
of Zygote.jl, which also corresponds to a common, more general strategy (e.g., Pearlmutter & Siskind 2008, Wang et al. 2018). At its core is a differential operator J that is

6

defined as follows:
J : C → C;
f 7→ (x 7→ (f (x), z 7→ Jf (x) · z)),

(4)

∂fi
where C denotes the space of functions f : Rn → Rk and Jf (x) = ( ∂x
)i=1,...,k,j=1,...,n ∈ Rk×n
j

the Jacobian of f (x). With the operator J , the gradient of a scalar function g : Rk → R
can be obtained by selecting the second element of the tuple J (g)(x), corresponding to the
map z 7→ Jg (x) · z and evaluating it at z = 1, yielding ∇g(x).
Furthermore, with the operator J we can express the chain rule as
J (g ◦ f ) = x 7→ (g(f (x)), z 7→ Jf (x) · Jg (f (x)) · z)

(5)

which can be implemented using a local recursive transformation. It can then be hard
coded how J operates on a set of primitive functions and subsequently, differentials of all
other functions can be generated by repeatedly applying the chain rule (5). The specific
characteristics of the Julia implementation in Zygote.jl are described in Innes (2018a).

3.3

Fitting the model

In the following, we illustrate how the parameters of the increment model introduced in
Section 3.1 can be optimized by differentiable programming.
Note that the model covariate ye from (2) depends on the model prediction from previous
time steps, such that for any rt = 0, it holds that
c
dy
et + β3 · zt
t+1 = β1 + β2 · y
c ) + β3 · zt
= β1 + β2 · (e
yt−1 + dy
t

(6)

= β1 + β2 · (e
yt−1 + (β1 + β2 · yet−1 + β3 · zt−1 )) + β3 · zt ,
introducing higher order terms β2 β1 , β22 , β2 β3 of the parameters. Thus, we cannot apply
Fisher scoring to iteratively obtain a maximum likelihood estimate. Yet, we can find optimal values for the model parameters by using gradient descent and applying differential
programming. To do this, we initialize β (0) = 0 and update it by
β (s) = β (s−1) − η · ∇β L(y, z, r, β (s−1) ),

s = 1, . . . , S,

(7)

where the number of steps S ∈ N and the stepsize η ∈ R3 are hyperparameters.
To illustrate how even rather complex functions can be incorporated into differentiable
functions in a straightforward way, we show the original Julia source code corresponding to
the loss function, including a loop and control flow elements:
function loss (y, z, r, beta)
sqerror = 0.0 # squared error
firstseen = false # set to true after skipping potential missings
last_y = 0.0 # prevalent cases from previous time point
contribno = 0.0 # number of non-missing observations
for t = 1:(length(y))
# skip missings at the start until first reported value
if !firstseen

7

if r[t] == 1
firstseen = true
last_y = y[t]
else
continue
end
else # make a prediction for the current increment
pred_dy = beta[1] + beta[2] * last_y + beta[3] * z[t-1]
if r[t] == 1
dy = y[t] - last_y
sqerror += (dy - pred_dy)ˆ2
contribno += 1.0
last_y = y[t]
else
last_y += pred_dy
end
end
end
return sqerror/contribno # return MSE over all reported time points
end

The gradient descent algorithm with the parameter update from (7) can then be implemented in just a few lines:
beta = [0.0; 0.0; 0.0]
for steps = 1:nsteps
gradloss = gradient(arg -> loss(y, z, r, arg), beta)[1]
beta .-= eta .* gradloss
end

With the source-to-source automatic differentiation implementation of the Zygote.jl
differentiable programming framework, an exact gradient ∇β L is obtained automatically in
each step, without requiring any refactoring on the user-defined loss function.
This flexible optimization of a customized loss function also allows to easily integrate
additional constraints and to adapt the model. For example, to prevent overfitting with a
smaller number of time points, a L2 regularization term can be added by replacing (7) with
β (s) = β (s−1) − η · ∇β (L(y, z, r, β (s−1) ) + λ · kβ (s−1) k22 ),

s = 1, . . . , S

(8)

and exploiting the flexibility of the differentiable programming framework to automatically
differentiate through the new loss function for optimization.
Yet, due to the form of the model predictions (6), estimating the curvature of the
loss function is not straightforward, and we cannot use the Fisher information matrix to
determine an optimal step size. This makes the choice of η a critical one, which we will
further discuss in the next section.

8

4
4.1

Results
Implementation

We implement the model in the Julia programming language (v1.4.2), using the differentiable programming framework provided by the Zygote.jl package (v0.4.22). Despite our
customized, problem-specific model, this implementation allows us to quickly explore model
variations and extensions. Also, the Zygote.jl differentiable programming framework is
tightly integrated with the Julia machine learning package Flux.jl (Innes 2018b), which
provides a full stack of optimizers for gradient-based learning.
Finding the right step size in gradient descent is crucial. In our gradient-descent algorithm, we set the step size for the update of β3 an order of magnitude smaller than the
others to scale down the incidences predicted at the level of counties and thus account for
the fact that they are usually considerably larger than individual hospital catchment areas.
In contrast, the adaptive optimizers from Flux.jl determine an optimal step size purely
data-based. While some optimizers, in particular versions of the ADAM optimizer (Kingma
& Ba 2015), also yield better predictions than baseline models, the simple gradient-descent
algorithm with manually chosen step size worked well in the present application.

4.2

Individual models

To evaluate the model, we separately optimize the parameters for each hospital k = 1, . . . , K =
1291, based on all time points available for that hospital except for the last. The prediction
for this withheld last time point is then compared to three benchmark models across all
hospital in the DIVI registry.
As a first benchmark, we consider the zero model
c zero := 0,
dy
k,T

k = 1, . . . , K.

This benchmark model is motivated by the knowledge that the numbers of COVID-19
infections towards the end of the considered time interval, when the first wave of infections
is over, are consistently low. As a second benchmark, we consider a mean model
c mean :=
dy
k,T

T −1
1 X
(yk,t − yk,t−1 ),
T −2

k = 1, . . . , K,

t=2

where missing observations are imputed using a last-observation-carried-forward principle.
Third, we evaluate a modified version of the mean model
(
modmean
0,
if (yk,T −1 − yk,T −2 ) = 0,
c
dy
:=
k = 1, . . . , K.
mean
k,T
c
dy
k,T , else,
When calculating predictions from our proposed model, we have to take hospitals into
account where estimation does not converge. In these cases we use the mean model for
prediction. Anticipating that the zero models will perform better than the mean models,
as they incorporate additional knowledge, this is a conservative choice for evaluating our
approach.

9

For each of the four models, we sum the squared prediction errors, i.e., the differences
to the true increments at the last time point, over all DIVI hospitals where the observation
at T is not missing, i.e., for all k with rk,T = 1:
errmodel =

K
X

c model −(yk,T −yk,T −1 ))2 ,
rk,T ·(dy
k,T

model ∈ {zero, mean, modmean, increment}.

k=1

(9)
The results are summarized in Table 1, showing that the increment model optimized
via differentiable programming provides the best prediction performance. As expected,
the models incorporating knowledge on the large number of zeros perform better than the
mean model. Still, our proposal, which uses the mean model as a fallback in cases of
non-convergence, outperforms the zero-based benchmark models.

4.3

Global vs. individual parameters

Additionally, we investigate whether an improvement over individual prediction models for
each hospital in the DIVI registry, as presented in Section 3.1, can be achieved by sharing
some of the parameters globally, i.e., using the same parameters values for all hospitals.
To this end, we implement several versions of the model that consider all possible combinations of local and global estimation of individual parameters. Technically, we use an
approach where in each step individual parameters are fitted for every hospital, but the
mean of those individual parameters is taken as a global parameter before proceeding to
the next step, such that in step s,
β (s,k) = β (s−1,mean) − η · ∇L(yk , zk , rk , β (s−1,mean) ),

k = 2, . . . , K,

(10)

with β (s,k) 6= β (s,l) for k, l ∈ {1, . . . , K}, k 6= l. After obtaining parameters β (s,k) for all
k = 1, . . . , K, we update the global parameter
β (s,mean) =

K
1 X (s,k)
β
K

(11)

k=1

and proceed to the next step s + 1.
To explore different combinations of local and global parameters, the update (11) can be
(n,mean)
applied only in specified dimensions such that for any given subset I ⊂ {1, 2, 3}, βI
=
(n,k)
(n,k)
1 PK
while for all j ∈
/ I, βj
remains unchanged.
k=1 βI
K
From the results summarized in Table 1, it can be seen that the model with all individual
parameters still performs best. However, individual estimation of the parameter for the
prevalent cases, which reflects internal processes of the hospital, more strongly improves
the prediction performance than individual estimation of the intercept parameter or of the
parameter for the incidences, representing an external pressure.

4.4

Sensitivity Analysis

The proposed increment model is based on the assumption that the parameters for a hospital
are constant in the course of time. To investigate how critical this assumption is, we perform
a sensitivity analysis by using different temporal subsets of the data for model fitting, which
10

Table 1: Prediction performance of benchmark models and the proposed increment model
with different combinations of global and local parameters
Prediction model
Zero model
Mean model
Modified mean model
Increment model
with global parameter β2 for prevalent cases
global β1 ; global β3
individual β1 ; global β3
global β1 ; individual β3
individual β1 ; individual β3 ;
Increment model
with individual parameter β2 for prevalent cases
global β1 ; global β3
individual β1 ; global β3
global β1 ; individual β3
individual β1 ; individual β3 ;

Squared error
(sum over all DIVI hospitals)
86.0
89.2
84.3

238.0
219.1
98.8
94.7

79.0
77.7
75.7
74.7

is also useful for judging variability and potential bias more generally. Specifically, we
construct time intervals that are only half as long as the available observation interval of
length 70, starting at each day tstart = 1, . . . , 36 one after another, and defining the intervals
Itstart = [tstart , tstart + 34], such that I1 = [1, 35], I2 = [2, 36], . . . , I36 = [36, 70]. On each
interval, we evaluate the prediction of the baseline models defined in Section 4.2 and fit
models for all combinations of individual and global parameters, as in Section 4.3.
Since at the beginning of the observation period, numbers of prevalent cases are globally
higher and then decrease until mainly zeros dominate the dataset, the prediction errors for
the earlier time intervals are also generally higher. Thus, the absolute values of prediction
errors are not directly comparable between different time intervals. For a first overview
of the model behavior across different time intervals, we therefore evaluate the relative
improvement of the model over the mean model, i.e., the difference in prediction error
(errmean − errincrement ). Larger values correspond a smaller error of the proposed model
compared to the mean model and thus better prediction performance. In Table 2, we report
the first and third quartile as well as the median of the results over all 36 time intervals.
For most combinations, the proposed increment model yields a better prediction performance than the simple mean model on the majority of data subsets, with only one
combination with a large negative 1st quartile and median only slightly above 0. As in Section 4.3, we also generally observe a better prediction performance of the increment model
compared to the mean model for all model versions with an individual estimation of the
parameter for the prevalent cases. Overall, the results from the previous sections generalize
to models fitted only on subsets of the data, indicating general robustness of our model and
optimization method.
11

Table 2: Quantiles of improvement in prediction error of the proposed increment model over
the mean model, obtained across all time intervals half as long as the entire time period.
Increment model parameters

Differences of squared error
(mean model - increment model)
1st quartile median 3rd quartile

global parameter β2
global β1 ; global β3
individual β1 ; global β3
global β1 ; individual β3
individual β1 ; individual β3 ;
individual parameter β2
global β1 ; global β3
individual β1 ; global β3
global β1 ; individual β3
individual β1 ; individual β3 ;

1.569
-0.551
-15.683
1.606

6.041
6.108
0.316
6.319

10.941
13.119
4.475
12.276

-0.620
4.352
1.060
2.850

8.201
10.517
11.263
12.688

14.860
19.317
18.388
19.666

Additionally, we take a look at exemplary predictions of the proposed model for larger
gaps of missing daily reports that originally motivated our method development. Again,
we evaluate these predictions based on the data from the entire time span and all data
subsets from the 36 shorter time intervals and for different combinations of individual and
global parameters. We exemplarily illustrate the results for the hospital in the DIVI register
depicted in Figure 1 from Section 2 with its larger gaps in daily reporting.
In Figure 2, we show exemplary predictions for missing reports of prevalent COVID-19
cases obtained from the increment model with different combinations of individual and global
parameters. In the models corresponding to panels A-C, the intercept parameter is estimated
globally. Panel A represents the all-global model, while in panel B, only the parameter
for the absolute value of the prevalent cases is estimated individually, thus corresponding
to a model with individually estimated internal processes and globally estimated external
pressure. In panel C, only the intercept parameter is estimated globally.
As a first observation, the variance of the predictions based on the data subsets (indicated
by how wide the light green dots spread out) increases when parameters are estimated
individually, as would be expected. On the other hand, global estimation can introduce
a bias, as can, e.g., be seen from the predictions for missing values between May, 20 and
May, 25: Based on the observations before and after that gap, the true development in
this period has to be an increase in prevalent cases. Increases are related to higher external
pressure due to more incident cases in the population and are thus modeled by the parameter
β3 . During that particular period, numbers of cases are however globally falling, and the
increase in prevalent cases is specific to the considered hospital. Thus, global estimation of
the corresponding parameter fails to model that development, introducing a bias in panels
A and B. When estimating that parameter individually, however, the increase in prevalent
cases is captured, as can be seen in panels C and D, in particular when the intercept is also
estimated individually.
Finally, the trajectory of the exemplary hospital exhibits distinct phases of rising and
falling numbers of cases. This more generally applies to the overall development across
12

Figure 2: Prevalent cases of one DIVI hospital with observed values shown in blue and
model predictions for the missing observations shown in green. The panels depict four
combinations of global and individual parameters of the proposed model. In each panel,
in addition to the observed values (blue dots connected with blue line), we show both the
predicted values based on the respective model optimized on the entire dataset (dark green
dots connected with blue line) and the predictions based on all 36 data subsets obtained
from the shorter time intervals (lighter green dots).
hospitals, implying that our modeling assumption of constant parameters may be (overly)
simplifying and time-dependent parameters would be better suited to capture these phases.

5

Discussion

Differentiable programming is part of a current paradigm shift in the field of deep neural
networks towards incorporating model components, such as differential equations, that better formalize knowledge on the problem at hand. We described an exemplary application of
differentiable programming for statistical modeling per se, to illustrate how such techniques
could also be useful in applied statistical work, e.g., for quick prototyping. In an exemplary scenario in a COVID-19 setting, characterized by a substantial amount of missing
daily reports, we have adapted a regression model for the increments of prevalent ICU cases
to flexibly handle missing observations in the data, inspired by differential equations. Using a differentiable programming framework allowed for tailoring the model to the specific
requirements of the data, yet optimizing it efficiently.
Specifically, we have shown that our modeling strategy allows for more accurate predictions on the given dataset compared to simple benchmark models. Moreover, the differentiable programming framework provided a convenient tool to quickly explore variations of
the model with respect to global vs. individual estimation of parameters and to investigate
factors predominantly influencing prediction performance. It thus facilitated exploring the
effect of model components and ultimately a more in-depth understanding of the model
properties and predictions.
Using data subsets defined by sliding time intervals for model fitting, we have investigated the robustness of our method and optimization procedure and have found our results
with respect to prediction performance and global vs. individual parameter estimation to
13

roughly generalize across subsets. Additionally, we have exemplarily illustrated the ability
of the model to bridge gaps of missing observations and have discussed the effect of different
global and individual parameters estimates on the prediction bias and variance.
With respect to the application setting of the first COVID-19 wave, the dynamic situation of the pandemic exemplifies a scenario where we can benefit from differentiable
programming as it allowed us to quickly prototype a model under time pressure and flexibly
adapt it to the challenging data scenario with many missing values. Such an approach can
be more generally useful in future epidemic scenarios, when data collection is still ongoing
and incomplete, yet interpretable models with predictive capabilities that can be flexibly
adapted to the challenge at hand are urgently needed.
In our model, the predictions for the missing values generally allow for an intuitive interpretation and the identification of external pressure and internal processes for individual
hospitals when comparing models with different combinations of global and individual parameters. Yet, our analyses have shown that time-dependent parameters would be more
suitable to accurately capture distinct phases of development in the data, which yet has to
be addressed in future research. Subsetting the data with a sliding window approach as
discussed for the sensitivity analyses and interpolating the resulting parameters could be a
first step in that direction. Furthermore, the discussed mixtures of individual and global
parameters currently rely on a coarse procedure of taking a global mean across all hospitals after each step. For further model refinement, groups of hospitals could be modeled
together to uncover cluster structures within the dataset and identify groups of hospitals
sharing common developments. Such similarity could be determined either based on the
regression parameters or the predicted trajectories, and potentially also take into account
structural characteristics of hospitals.
Despite these present limitations, our method could also be more broadly applicable to
general time-series data with missing values from an incomplete data collection. Yet, our
main aim was to provide an example of how differentiable programming allows to flexibly
combine different tools and modeling strategies led by the requirements of the specific modeling task, and to optimize models with a gradient-based approach in an easy-to-implement
and efficient way. Differentiable programming can thus aid statistical modeling where established approaches, such as maximum likelihood Fisher scoring, are not applicable and prove
beneficial particularly in application-driven settings with a focus on prediction performance.
In short, integrating differentiable programming into statistical modeling can enrich the
statistician’s toolbox and be a step towards uniting the estimation-focused data modeling
and the prediction-oriented algorithmic culture and their respective tools, allowing the fields
to interact and mutually inspire each other.

References
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S.,
Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G.,
Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y. & Zheng, X. (2016),
Tensorflow: A system for large-scale machine learning, in ‘12th USENIX Symposium
on Operating Systems Design and Implementation (OSDI 16)’, USENIX Association,
Savannah, GA, pp. 265–283.

14

Allen, L. J., Brauer, F., Van den Driessche, P. & Wu, J. (2008), Mathematical epidemiology,
Vol. 1945, Springer.
Baydin, A. G., Pearlmutter, B. A., Radul, A. A. & Siskind, J. M. (2017), ‘Automatic
differentiation in machine learning: A survey’, J. Mach. Learn. Res. 18(1), 5595–5637.
Bezanson, J., Edelman, A., Karpinski, S. & Shah, V. B. (2017), ‘Julia: A fresh approach to
numerical computing’, SIAM Review 59(1), 65–98.
Bischof, C., Khademi, P., Mauer, A. & Carle, A. (1996), ‘Adifor 2.0: automatic differentiation of Fortran 77 programs’, IEEE Computational Science and Engineering 3(3), 18–32.
Breiman, L. (2001), ‘Statistical modeling: The two cultures (with comments and a rejoinder
by the author)’, Statistical Science 16(3), 199–231.
Chen, T. Q., Rubanova, Y., Bettencourt, J. & Duvenaud, D. (2018), Neural ordinary differential equations, in ‘Advances in Neural Information Processing Systems’.
Efron, B. (2020), ‘Prediction, estimation, and attribution’, Journal of the American Statistical Association 115(530), 636–655.
Fahrmeir, L. & Tutz, G. (2001), Multivariate Statistical Modelling Based on Generalized
Linear Models, Springer Series in Statistics, 2 edn, Springer-Verlag New York.
Friedman, J., Hastie, T. & Tibshirani, R. (2000), ‘Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)’, Ann. Statist.
28(2), 337–407.
Griewank, A. (1989), On automatic differentiation, in ‘Mathematical Programming: Recent
Developments and Applications’, M. Iri and K. Tanabe, pp. 83–108.
Innes, M. (2018a), ‘Don’t unroll adjoint: Differentiating SSA-form programs’.
preprint: https://arxiv.org/abs/1810.07951.

arXiv

Innes, M. (2018b), ‘Flux: Elegant machine learning with Julia’, Journal of Open Source
Software 3(25), 602.
Innes, M., Edelman, A., Fischer, K., Rackauckas, C., Saba, E., Shah, V. B. & Tebbutt, W.
(2019), ‘A differentiable programming system to bridge machine learning and scientific
computing’. arXiv preprint: https://arxiv.org/abs/1907.07587.
Innes, M., Saba, E., Fischer, K., Gandhi, D., Rudilosso, M. C., Joy, N. M., Karmali,
T., Pal, A. & Shah, V. B. (2018), ‘Fashionable modelling with Flux’. arXiv preprint:
https://arxiv.org/abs/1811.01457.
Johnson, M., Frostig, R., Maclaurin, D. & Leary, C. (2018), ‘JAX; autograd and xla’,
https://github.com/google/jax. Accessed on December 4, 2020.
Kingma, D. P. & Ba, J. (2015), Adam: A method for stochastic optimization, in Y. Bengio
& Y. LeCun, eds, ‘3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings’.

15

LeCun, Y. (2018), ‘Deep Learning est mort. Vive Differentiable Programming!’,
https://www.facebook.com/yann.lecun/posts/10155003011462143. Accessed on December 4, 2020.
Pearlmutter, B. A. & Siskind, J. M. (2008), ‘Reverse-mode ad in a functional framework:
Lambda the ultimate backpropagator’, ACM Trans. Program. Lang. Syst. 30(2).
Rackauckas, C., Ma, Y., Martensen, J., Warner, C., Zubov, K., Supekar, R., Skinner, D.
& Ramadhan, A. (2020), ‘Universal differential equations for scientific machine learning’.
arXiv preprint: https://arxiv.org/abs/2001.04385.
Schelker, M., Raue, A., Timmer, J. & Kreutz, C. (2012), ‘Comprehensive estimation of input
signals and dynamics in biochemical reaction networks’, Bioinformatics 28(18), i529–i534.
Wang, F., Wu, X., Essertel, G. M., Decker, J. M. & Rompf, T. (2018), ‘Demystifying
differentiable programming: Shift/reset the penultimate backpropagator’. arXiv preprint:
https://arxiv.org/abs/1803.10228.

16

