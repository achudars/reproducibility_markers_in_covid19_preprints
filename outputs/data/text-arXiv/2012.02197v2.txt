A DDRESSING MACHINE LEARNING CONCEPT DRIFT REVEALS
DECLINING VACCINE SENTIMENT DURING THE COVID-19
PANDEMIC

arXiv:2012.02197v2 [cs.SI] 7 Dec 2020

A P REPRINT
Martin Müller
Digital Epidemiology Lab
Ecole polytechnique fédérale de Lausanne (EPFL)
1202 Geneva, Switzerland
martin.muller@epfl.ch

Marcel Salathé
Digital Epidemiology Lab
Ecole polytechnique fédérale de Lausanne (EPFL)
1202 Geneva, Switzerland
marcel.salathe@epfl.ch

December 8, 2020

A BSTRACT
Social media analysis has become a common approach to assess public opinion on various topics,
including those about health, in near real-time. The growing volume of social media posts has led to an
increased usage of modern machine learning methods in natural language processing. While the rapid
dynamics of social media can capture underlying trends quickly, it also poses a technical problem:
algorithms trained on annotated data in the past may underperform when applied to contemporary
data. This phenomenon, known as concept drift, can be particularly problematic when rapid shifts
occur either in the topic of interest itself, or in the way the topic is discussed. Here, we explore the
effect of machine learning concept drift by focussing on vaccine sentiments expressed on Twitter, a
topic of central importance especially during the COVID-19 pandemic. We show that while vaccine
sentiment has declined considerably during the COVID-19 pandemic in 2020, algorithms trained on
pre-pandemic data would have largely missed this decline due to concept drift. Our results suggest
that social media analysis systems must address concept drift in a continuous fashion in order to
avoid the risk of systematic misclassification of data, which is particularly likely during a crisis when
the underlying data can change suddenly and rapidly.
Keywords Concept drift · Vaccine sentiment · Text classification · COVID-19 · Twitter

1

Introduction

Supervised and semi-supervised Machine Learning algorithms are now ubiquitous in the analysis of social media
data. At the core of these algorithms is their ability to
make sense of a vast amount of semi-structured real-time
data streams, allowing them to automatically categorize or
filter new data examples into, usually pre-defined, classes.
Multi-class text classification has been successfully used
in public health surveillance, election monitoring, or vaccine stance prediction [1, 2, 3]. In recent years such algorithms have also been developed to mitigate the negative
effects of social media, such as in the detection of cyber-

bullying, hate speech, misinformation, and automated accounts (bots) [4, 5, 6, 7].
The microblogging service Twitter has played a central
role in these efforts, as it serves as a public medium and
provides easy access to real-time data through its public
APIs, making it the primary focus of this work. Twitter is
well described as a classical example of a non-stationary
system with frequently emerging and disappearing topical
clusters [8]. This poses problems for the aforementioned
applications, as the underlying data distribution is different
between training time and the time of the algorithm’s application in the real world. This phenomenon is known as

A PREPRINT - D ECEMBER 8, 2020

concept drift [9] and can lead to a change in performance
of the algorithm over time.

important to ask how much we should be worried about
concept drift: even if model performance were to decrease,
the real impacts on our analysis or interpretation might be
It is important to distinguish concept drift from other reanegligible.
sons for performance differences between training and
testing, such as random noise due to sampling biases or The consequences of concept drift are task-, environment-,
differences in data preprocessing [10, 11]. A classic ex- and model-dependent [19]. Here, we will address conample of concept drift is the change in the meaning of cept drift in the specific case of vaccine stance classificaclasses, which requires an update of the learned class de- tion. Vaccine stance classification on Twitter data has been
cision boundaries in the classifier. This is sometimes also widely studied and has shown promising links to vaccinareferred to as real concept drift. Often, however, an ob- tion decision making and vaccine uptake rates in different
served performance change is a consequence of a change in countries [1, 20]. The COVID-19 pandemic further emphathe underlying data distribution, leading to what is known sizes its importance, as evolving concerns about vaccines
as virtual drift [12, 13]. Virtual drift can be overcome by may significantly influence their effect [21, 22].
supplemental learning, i.e. collecting training data from
To the best of our knowledge, only one study directly adthe new environment. A good example are periodic seadressed concept drift in vaccine stance classification. In
sonality effects, which may not be fully represented in
this study [23] on tweets posted between September 2016
the initial training data and only become fully visible over
and January 2017 in Italian language, the authors did not
time. However, in practice it is usually very difficult (if
find a substantial improvement of their model from innot impossible) to disentangle virtual from real concept
cremental re-training before specific events. Re-training
drift, and as a consequence they are treated as the same
was performed on 60 newly annotated tweets from seven
effect [10].
manually selected events. The authors conclude that either
On Twitter concept drift might appear on very different their original algorithm was already quite robust towards
time scales and at different rates. Sudden shifts in a debate concept change, or that the newly collected training data
might be triggered by a quickly evolving news cycle or was too small to see an effect.
a catastrophic event. Concept drift may also be a slow
Here, we use FastText [24] and BERT (Bidirectional Enprocess in which the way a topic is discussed gradually
coder Representations from Transformers) [25], two comchanges over time. A substantial amount of work has been
monly used models in social media text classification.
dedicated to detecting and overcoming concept drift [12,
Most work on the topic of concept drift was conducted
10, 14]. Three basic re-training procedures for overcomusing classical machine learning models, to which also
ing concept drift have been proposed: (i) a time-window
FastText belongs. These types of models are very reliant
approach, (ii) an incremental model, and (iii) an ensemble
on high-quality annotation data. More recently, models of
model [8]. In the time-window approach, a sliding window
the transformer family, such as BERT [25], have been proof recent training examples is used to train an algorithm. In
posed, which require significantly less annotation data. In
this approach, the algorithm ignores training data collected
what follows, we will examine whether these two models
outside of that time window. The incremental model, in
also share different concept drift characteristics.
contrast, uses all previously collected training examples
to re-train the model. Lastly, the ensemble model trains The goal of this work is to emulate a typical social media
a model for each time window and uses the consensus analysis study, in which data is collected for a certain peof all previous models for future predictions. As found riod of time, and a supervised machine learning model is
in [8], in the case of hashtag prediction on Twitter data, the trained on a subset of annotated data. The model is then
incremental method gave the best results.
published and used to predict newly collected data. First,
we will try to answer whether or not concept drift can be
Although sophisticated methods have been proposed to
observed, and if so, at what rate it occurs. Second, we
estimate concept drift in an unsupervised way [15, 16], in
will investigate the influence of the study duration and the
practice, a certain amount of re-annotation for both the
amount of annotation data used. Lastly, we will examine
detection and re-training of models seems unavoidable.
to what extent concept drift influences the final analysis
The decision about which of the newly collected data to
outcomes, in this case a sentiment index.
annotate points to an exploration-exploitation dilemma,
which is usually addressed in the context of an active learning framework [17]. The Crowdbreaks platform [18] is 2 Results
an example of such a framework and has been built with
the goal of exploring optimal solutions to this problem in 2.1 Observing concept drift
order to overcome concept drift.
Throughout the 1188 day observation period, starting on
A change in the underlying data distribution might not
July 1st, 2017 and ending on October 1st, 2020, a total of
necessarily have a negative impact on classifier perfor57.5M English vaccination-related tweets were collected.
mance. It is conceivable, for example, that a polarisation
A random subset of 11,893 tweets were annotated with
in a debate on Twitter about a topic could even lead to
respect to stance towards vaccines, which resulted in 5482
an improvement in classifier performance. It is therefore
(46%) positive, 4270 neutral (36%), and 2141 negative
2

Number of annotations

A PREPRINT - D ECEMBER 8, 2020

1200

training window
for b1

b0

800

b8

b1

400
0

2018-01

2019-01

Annotation type
Train
Eval
Training data b1
Eval datasets b1
Unused

2020-01

Figure 1: Training and evaluation datasets. Each 90 day bin consists of 400 samples of training data (blue) and 150
samples of evaluation data (red). Each trained model is using the most recent 1600 samples for training, which is an
equivalent of 4 bins or 360 days. For illustration purposes, the training data for the second bin b1 is indicated as blue
with white stripes. The b1 model is then evaluated on all future evaluation datasets, indicated as red with white stripes.
(18%) labels (for further details see methods section 4.2). Comparing the performance scores on future evaluation
The dataset therefore bears clear label imbalance.
sets (circles) between models, we observe that the oldest models (black) generally perform worse than newer
In order to observe whether classifiers experience drift
models (yellow) and that the ordering between models is
in our dataset, we analysed the performance change of
preserved at all times. However, in order to disentangle
a model when predicting newly collected labelled data.
this effect from the variability in initial performance, we
For this we used a sliding time window approach, as first
compute the relative change in performance with respect
proposed in [8]. We dissected the collected 11,893 annoto performance at training time (lower left panel). Starting
tations into 13 bins of 90 days each. From each bin we
from zero, the first model’s performance drops quickly
sampled 550 examples and split them into a train (n = 400,
by around 5 %–10 %, followed by a rebound to initial per72%) and evaluation (n = 150, 27%) set (see Figure 1).
formance in fall 2019, and ending in a sudden drop of
Each model was trained on a window of 4 bins of training
approximately −20% in early 2020. The last drop indicates
data, which is equivalent to 1600 samples and a time span
a very abrupt shift in concepts, twice as strong as during a
of 360 days. The models are subsequently evaluated on
comparable time window in 2019. In fall 2019, changes
the evaluation set corresponding to the bin at the end of
in the data distribution allowed all models to rebound to
their training window as well as on all future evaluation
initial performance, with some even “over-performing” by
sets. We repeat the process of binning, splitting, training
5% compared to training time. This is a sign that the data
and evaluating 50 times in order to yield a measure of
distribution was particularly easy to predict.
confidence to our results.
Further investigation of the F1-scores by class reveals that
Figure 2 shows the classifier performance at training time
concept drift is especially impactful on the negative class,
(square symbol) and the performance at each future evalwhereas the positive and neutral classes do not experience
uation dataset (circle symbol) for classifiers trained on
a significant drift (see Figure S1). This could either indifferent training windows (color). The upper left panel
dicate that anti-vaccine concepts are changing faster than
shows the results of these experiments for the FastText
pro-vaccine concepts or that the negative class is harder to
models. We will first compare the initial performance in
learn due to label imbalance (cf. Figure S4) and might, as
terms of F1-macro score (i.e. the arithmetic mean of the
a consequence, be more affected by virtual drift. We will
class-level F1 scores) of the classifiers on a test dataset
further investigate this difference in the next section.
which was sampled from the last bin of the corresponding training window (square symbols). The initial perfor- Comparing these results to the BERT models (upper right
mance of the first model is at 0.42, the subsequent models panel), the models show higher absolute performance but
plateau at around 0.50, followed by a peak in fall 2019 they experience a similar level of relative performance loss
with an abrupt decline in January 2020. This variability in and similar drift patterns. This confirms that the observathe initial performance of models points to considerable tions are not model-specific but are likely to be observed in
differences between training datasets over time. The per- state-of-the-art semi-supervised machine learning models.
formance of the FastText models is quite low in general,
As previously stated, each model was trained on 1600 trainwhich may be a consequence of the relatively small training
ing examples over the previous 360 days. Experiments
dataset of 1600 examples and the lack of hyperparameter
were conducted under fewer training examples (Figure S2)
tuning.
and smaller training windows (Figure S3) for FastText.

3

A PREPRINT - D ECEMBER 8, 2020

FastText

BERT

Trained on data up to
2018-08-11
2018-11-09
2019-02-07
2019-05-08
2019-08-06
2019-11-04
2020-02-02
2020-05-02
2020-07-31

F1-macro

0.5

% Relative
performance change

0.6

10

0.4

0
10
20
2019-01

2020-01

2019-01

2020-01

Train & evaluate
Evaluate

Figure 2: Model performance over time. The top row shows absolute performance (in terms of F1-macro), and
the bottom row shows the relative performance change of models compared to training time. The columns show the
result for the two different classifier types FastText and BERT. The square indicates the performance at training time,
the circles correspond to performance of that same model on future evaluation sets (compare with Figure 1). Bands
correspond to bootstrapped 95% confidence intervals resulting from 50 repeats.
As expected, training on fewer training examples leads
to lower model performance, but we find the same drift
patterns irrespective of the training data size. Reducing
the training window while keeping the number of training examples constant does seem to have an impact on
performance or drift patterns.
2.2

performance in 2020, we conclude that label imbalance
alone does not explain the observed variability in initial
performance.
Annotator agreement
We measure annotator agreement by computing the Fleiss’
Kappa [26] values for each dataset. Annotator agreement
is initially low at 0.37, then increases to almost 0.45 and
drops again to 0.36 in mid-2020. This overlaps very well
with the initial performance trend observed in Figure 2.
Variation in inter-annotator agreement may be a consequence of differences in annotation quality or difficulty of
the annotation task, possibly hinting at semantic ambiguity
of the text, as discussed next.

Explaining concept drift

Next, we will try to explain both the variance in initial performance, as well as the different rates of drift observed.
We will investigate the effects of label imbalance, annotator
agreement, and corpus variability on initial performance
of models (Figure 3a-c). Additionally, we compare corpus
similarity over time and discuss it in the context of concept
drift (Figure 3d). In particular, we consider the first sampling (repeat) of the combined training (n = 1600) and
first evaluation set (n = 150) for each training window.
The provided measures therefore correspond to what the
model “saw” during training and in the first bin of evaluation. Figure S3 shows the equivalent metrics when limited
to only the individual 90 day bins.

Corpus variability
We use the BERT-large-uncased model to generate a 1024dimensional sentence embedding vector (i.e. the vector of
the CLS token) for each tweet text in the datasets. Note
that this BERT model has not been trained on any of our
datasets, but it is able to generate rich sentence embeddings due to having been pre-trained on large amounts of
English text. Figure 3c shows the variance in the generated
sentence embeddings across time. We note that overall,
corpus variability is highest in the beginning of our observation period, and then decreases towards the end. Also,
when considering the corpus variability by label class, we
observe that negative samples have consistently lower variability compared to text labelled as positive. The neutral
class seems to undergo a shift from high to low variability. In general, we may hypothesize that a lower variability

Label imbalance
Although the used training datasets are always of equal
absolute size, they vary in the number of examples per
class over time (see Figure 3a). It is commonly known that
label imbalance can negatively impact model performance,
which is also observed here (see Figure S1). However, we
note that label imbalance was highest in the very beginning
of the observation period and continuously decreased towards a more balanced situation. Given the drop in initial
4

A PREPRINT - D ECEMBER 8, 2020

A

B

C

D

Figure 3: Properties of the combined training data and first evaluation dataset for each trained model. A. Distribution
in the number of labels per class. B. Annotator agreement, measured by Fleiss’ Kappa. C. Corpus variability in terms
of the variance of sentence embeddings within a corpus. Variability is shown for the full corpus as well by class. D.
Normalized cosine similarity between the mean corpus vectors (i.e. the mean of all sentence vectors in each corpus) for
all data as well as by class.

5

A PREPRINT - D ECEMBER 8, 2020

points to lower separability in embeddings space, and there- model was in agreement with the updated models at the
fore lower model performance. This hypothesis aligns with time of the first drop in 2019, but then started to diverge.
the observations made in terms of initial performance.
We can therefore conclude that due to their higher overall
performance, BERT models will have less severe deviations, but are not immune to effects of concept drift in the
Corpus similarity
long run. We also note a large difference in the extent
Similarity was measured by calculating the cosine similar- of positive and negative spikes between the legacy and
ity between the mean vectors for each corpus. Low cosine re-trained models. Drift may therefore not only affect the
similarity points to large semantic differences between mean sentiment trend but also sensitivity on shorter time
datasets, which in turn could be an indicator for concept scales, which could be problematic for real-time event or
drift. In the top left panel (“all”), the datasets are compared anomaly detection.
with each other. We observe that over time, corpus vectors As previously stated, the sentiment trend of both the upare moving further away from each other. The biggest dated BERT and FastText models show a negative trend of
difference was observed between the two datasets furthest the vaccine sentiment. Given the current debate surroundfrom each other in time (2018-08-11 and 2020-07-31). We ing novel vaccines for the Sars-CoV-2 virus, this finding
also observe a bright area in the middle of the heatmap, is concerning from an epidemiological perspective. Note
which reveals that datasets between February 2019 and however, that the BERT models used for these predictions
February 2020 are more similar to each other compared to are of mediocre performance and future studies will be
datasets before (2018) or after (May & July 2020). This needed to confirm and interpret these trends.
aligns well with the results in Figure 2: Most of the concept
drift was observed in 2018 and following 2020, whereas
models in 2019 didn’t drift by a lot. When considering the 3 Discussion
corpus similarity by class, we can attribute most of these
effects to the neutral and negative class. We therefore show In this work, we investigated the effects of concept drift
that anti-vaccine content “drifts” faster than pro-vaccine in vaccination-related Twitter data streams over a duration
content.
of three years. Using a sliding time window approach,
we
emulate a social media study in which (i) data is colIn conclusion, our observations point to the fact that the
differences in initial performance of models are likely a lected for one year, (ii) an algorithm is trained, and (iii)
consequence of low annotator agreement. The reason for the algorithm is used in real-time monitoring of new data.
this low agreement could be rooted in semantic ambiguity, While this may correspond to a common setup in social
as expressed by annotator agreement and corpus variabil- media analytics, we demonstrate here that without taking
ity. The degree of concept drift on the other hand is best concept drift into account, the quality of the results will
decay. Using a vaccine-related dataset from 2018–2020,
explained by our measure of corpus similarity.
we demonstrate how failing to take concept drift into account would have largely missed a rather dramatic decay
2.3 Consequences of concept drift on real-time
in vaccine sentiment during the COVID-19 pandemic in
monitoring
2020.
Lastly, but perhaps most importantly, we highlight the impact of concept drift on the inference of the previously
trained models when used for real-time monitoring of new
data. We compare the predictions of a legacy model, which
was trained in August 2018 and used for the two subsequent years, to a model we update (re-train) every 90 days.
We compute the sentiment index s, which corresponds to
the weekly mean of positive, neutral and negative predictions, when mapped to the numerical values of 1, 0 and
−1, respectively. Figure 4 shows these sentiment trends for
both the FastText and BERT model variants. We observe
that, in the case of FastText, the sentiment predicted by
the legacy model increased slightly until 2019 and then
remained static. The updated models, however, show a
downwards trend starting in mid-2019 and dropping further in 2020. By the end of our observation period the
legacy model predicts a 0.3 points higher sentiment than
the up-to-date models, while completely missing out on
the downwards trend.

We find that overall, concept drift indeed occurred, which
led to a decline in model performance of over 20% in the
course of three years. However, most of this decline happened in only ten months. Concept drift therefore affected
model performance at different rates throughout the observation period. Furthermore, the relative performance loss
was not consistently negative but reverted to initial levels,
or even slightly above that. These findings are consistent
with the various ways real and virtual concept drift can occur. Although BERT models yielded higher performance
scores, they are not immune to issues related to concept
drift. On a relative scale, BERT models show the same
degree of drift as the much less sophisticated FastText
models.

In order to better understand the reasons for these phenomena, we investigate the properties of the used datasets.
We can explain the large differences in initial performance
of models with differences in semantic ambiguity of the
text, as indicated by low inter-annotator agreement and
BERT models show a similar but smaller error, which is in low corpus variability. Occurrence of concept drift could
agreement with our previous analysis. The legacy BERT be linked to differences in corpus similarity. In particular,
6

A PREPRINT - D ECEMBER 8, 2020

0.8

FastText

0.6

Trained at
2018-08-11
2018-11-09
2019-02-07
2019-05-08
2019-08-06
2019-11-04
2020-02-02
2020-05-02
2020-07-31

0.4
Sentiment index s

0.2
0.0
0.8

BERT

0.6
0.4
0.2
0.0

2018-01 2018-07 2019-01 2019-07 2020-01 2020-07

Figure 4: Impact of concept drift on the predictions made by FastText and BERT models. Each panel shows the
comparison of a model which was trained in August 2018 (black) to a model which was continuously updated every 90
days (colored).
we find that the negative class is responsible for most of
the decay in performance over time and also shows the
strongest signs of drift. Anti-vaccine content may therefore change topics at an increased rate compared to both
positive or neutral content.

account. This is especially true for applications which are
intended to be used exactly in such circumstances.
Although our work focused on the singular task of vaccine stance prediction, we believe that these results stress
the general importance of addressing concept drift in any
real-time social media monitoring project. Overcoming
concept drift is a complex task, and many algorithmic solutions have been proposed. However, in order to succeed
in practice, a tightly coordinated and fine-tuned framework for both the annotation and retraining of models is
required. The Crowdbreaks platform [18] was built with
the intention to address this issue and provide solutions for
it.

A caveat of this study is that the results are based on classifiers of mediocre performance. Given the fact that the
negative class was most affected by concept drift and is at
the same time also the smallest class in our dataset, it is a
fair question to ask whether concept drift would disappear
given more annotation data and higher performance of
models. It is conceivable that more annotation data would
lead to a better representation of the training window. However, as results in a study on automated geo-location of
tweets show [27], concept drift will still occur also under
vast amounts of annotated data and adaptive re-training on
even a relatively small corpus can overcome this drift.

4

Materials and methods

Our results do not overlap with a previous study on
vaccination-related Twitter data [23], which did not find
concept drift in an observation period between September
2016 and January 2017 in Italian language. The reason for 4.1 Data collection
this could be that the time scale analysed was too small to
see an effect, or that concept drift was much smaller in that
This study is based on Twitter data collected through
particular dataset.
the Crowdbreaks platform [18]. Between July 1st, 2017
It is safe to assume that the COVID-19 pandemic led to and October 1st, 2020 a total of 57.5M tweets (including
severe topical shifts in the vaccine debate, which ultimately 39.7M retweets) in English language by 9.9M unique users
translated into strong concept drift and model performance were collected using the public filter stream endpoint of the
loss. Based on these results, it can be expected that future Twitter API. The tweets matched one or more of the keycrisis situations would lead to similarly strong concept words “vaccine”, “vaccination”, “vaxxer”, “vaxxed”, “vacdrift, thereby severely undermining the utility of social cinated”, “vaccinating”, “vacine”, “overvaccinate”, “unmedia monitoring tools that do not take concept drift into dervaccinate”, “unvaccinated”. The data can be considered
complete with respect to these keywords.
7

A PREPRINT - D ECEMBER 8, 2020

4.2

Annotation data

Funding. This work received funding through the Versatile Emerging infectious disease Observatory (VEO)
Human annotation of a subset of tweets was performed grant as a part of the European Commission’s Horizon
through the Crowdbreaks platform [18]. Tweets were 2020 framework programme (grant agreement ID: 874735).
anonymized by replacing user mentions and URLs with Compute resources (Cloud TPUs) were provided through
placeholders. Tweets between February 2nd 2018 and Google’s TensorFlow Research Cloud and the work was
November 11th 2020 were sampled for annotation if they supported through Google Cloud credits in the context of
contained at least 3 words. Exact duplicates were removed. COVID-19-related research.
Annotators were asked the question “What is the attitude of
the author of the tweet regarding vaccines?” and given the
three options “negative”, “neutral”, and “positive”. Anno- References
tation was performed both on Amazon Turk (mTurk) and,
to a smaller extent (roughly 1% of all annotations) by pub- [1] Marcel Salathé and Shashank Khandelwal. “Assessing vaccination sentiments with online social melic users on the Crowdbreaks website. We yield a dataset
dia: implications for infectious disease dynamics
of 44,843 annotations (Fleiss’ kappa of 0.30), which reand control”. In: PLoS Comput Biol 7.10 (2011),
sulted in 11,893 three-fold annotated tweets. Tweets with
e1002199.
less than two-third agreement were excluded and conflicts
were decided through majority vote.
[2] Adam Bermingham and Alan Smeaton. “On using
Twitter to monitor political sentiment and predict
election results”. In: Proceedings of the Workshop
4.3 Training of classifiers
on Sentiment Analysis where AI meets Psychology
(SAAIP 2011). 2011, pp. 2–10.
In this work we leverage two different classifiers: FastText [24] and BERT [25]. For both models, hyperparam- [3] John S Brownstein, Clark C Freifeld, and Lawrence
C Madoff. “Digital disease detection—harnessing
eters were first tuned on the full annotation data to yield
the Web for public health surveillance”. In: The
optimal performance and then fixed for further experiNew England journal of medicine 360.21 (2009),
ments. For FastText we used 10 dimensions, 500 epochs,
p. 2153.
a learning rate of 0.01, and using 1-gram embeddings.
Optimal results were yielded by lower casing texts, con- [4] Kelly Reynolds, April Kontostathis, and Lynne Edverting them to ASCII and using the tags “user” and “url”
wards. “Using machine learning to detect cyberbulfor anonymization. BERT models of the type bert-largelying”. In: 2011 10th International Conference on
uncased (pretrained in English language) were trained for
Machine learning and applications and workshops.
20 epochs, training batch size of 32, and a learning rate
Vol. 2. IEEE. 2011, pp. 241–244.
2 × 10−5 (using 10% warmup with linear decay to zero), as [5] Thomas Davidson et al. “Automated hate speech
recommended in recent literature [28, 29]. FastText moddetection and the problem of offensive language”.
els were trained on a university cluster using the CrowdIn: arXiv preprint arXiv:1703.04009 (2017).
breaks TEXT- CLASSIFICATION library1 and BERT mod- [6] Kai Shu et al. “Fake news detection on social meels were trained using Google Cloud v3-8 TPUs and the
dia: A data mining perspective”. In: ACM SIGKDD
COVID-T WITTER -BERT library2 [30]. For the purpose
explorations newsletter 19.1 (2017), pp. 22–36.
of predictions, text was preprocessed using the respective
Clayton Allen Davis et al. “Botornot: A system to
[7]
preprocessing approach.
evaluate social bots”. In: Proceedings of the 25th
international conference companion on world wide
Data availability. All data and code can be found on
web. 2016, pp. 273–274.
our public GitHub repository https://github.com/ [8] Joana Costa et al. “Concept drift awareness in twitdigitalepidemiologylab/concept_drift_paper.
ter streams”. In: 2014 13th International Conference
on Machine Learning and Applications. IEEE. 2014,
Author contributions. M.M. collected the data, depp. 294–299.
signed the experiments and analysed the data. M.M. and
[9] Jeffrey C Schlimmer and Richard H Granger. “InM.S. conceptualized the work and wrote the manuscript.
cremental learning from noisy data”. In: Machine
learning 1.3 (1986), pp. 317–354.
Acknowledgments. The authors would like to acknowl- [10] Indrė Žliobaitė. “Learning under concept drift:
edge Dr. Per Egil Kummervold and Dr. Burcu Tepekule
an overview”. In: arXiv preprint arXiv:1010.4784
for their valuable comments and discussions.
(2010).
[11] Geoffrey I Webb et al. “Characterizing concept
Competing interests. The authors declare no competing
drift”. In: Data Mining and Knowledge Discovery
interests.
30.4 (2016), pp. 964–994.
1
2

https://github.com/crowdbreaks/text-classification
https://github.com/digitalepidemiologylab/covid-twitter-bert

8

A PREPRINT - D ECEMBER 8, 2020

[12] Gerhard Widmer and Miroslav Kubat. “Learning in
the presence of concept drift and hidden contexts”.
In: Machine learning 23.1 (1996), pp. 69–101.
[13] Alexey Tsymbal. “The problem of concept drift:
definitions and related work”. In: Computer Science
Department, Trinity College Dublin 106.2 (2004),
p. 58.
[14] Ryan Elwell and Robi Polikar. “Incremental learning of concept drift in nonstationary environments”.
In: IEEE Transactions on Neural Networks 22.10
(2011), pp. 1517–1531.
[15] Ioannis Katakis, Grigorios Tsoumakas, and Ioannis Vlahavas. “Tracking recurring contexts using
ensemble classifiers: an application to email filtering”. In: Knowledge and Information Systems 22.3
(2010), pp. 371–391.
[16] Ying Yang, Xindong Wu, and Xingquan Zhu. “Conceptual equivalence for contrast mining in classification learning”. In: Data & Knowledge Engineering
67.3 (2008), pp. 413–429.
[17] Burr Settles. Active learning literature survey. Tech.
rep. University of Wisconsin-Madison Department
of Computer Sciences, 2009.
[18] Martin M Müller and Marcel Salathé. “Crowdbreaks: Tracking health trends using public social
media data and crowdsourcing”. In: Frontiers in
public health 7 (2019), p. 81.
[19] Indrė Žliobaitė, Mykola Pechenizkiy, and Joao
Gama. “An overview of concept drift applications”.
In: Big data analysis: new algorithms for a new
society. Springer, 2016, pp. 91–114.
[20] Gema Bello-Orgaz, Julio Hernandez-Castro, and
David Camacho. “Detecting discussion communities on vaccination in twitter”. In: Future Generation Computer Systems 66 (2017), pp. 125–136.
[21] Neil F Johnson et al. “The online competition between pro-and anti-vaccination views”. In: Nature
(2020), pp. 1–4.

9

[22]

Talha Burki. “The online anti-vaccine movement
in the age of COVID-19”. In: The Lancet Digital
Health 2.10 (2020), e504–e505.

[23]

Eleonora D’Andrea et al. “Monitoring the public
opinion about the vaccination topic from tweets
analysis”. In: Expert Systems with Applications 116
(2019), pp. 209–226.

[24]

Armand Joulin et al. “Bag of tricks for efficient text
classification”. In: arXiv preprint arXiv:1607.01759
(2016).

[25]

Jacob Devlin et al. “Bert: Pre-training of deep bidirectional transformers for language understanding”.
In: arXiv preprint arXiv:1810.04805 (2018).

[26]

Joseph L Fleiss. “Measuring nominal scale agreement among many raters.” In: Psychological bulletin 76.5 (1971), p. 378.

[27]

Mark Dredze, Miles Osborne, and Prabhanjan Kambadur. “Geolocation for twitter: Timing matters”. In:
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies.
2016, pp. 1064–1069.

[28]

Marius Mosbach, Maksym Andriushchenko, and
Dietrich Klakow. “On the Stability of Fine-tuning
BERT: Misconceptions, Explanations, and Strong
Baselines”. In: arXiv preprint arXiv:2006.04884
(2020).

[29]

Jesse Dodge et al. “Fine-tuning pretrained language
models: Weight initializations, data orders, and early
stopping”. In: arXiv preprint arXiv:2002.06305
(2020).

[30]

Martin Müller, Marcel Salathé, and Per E Kummervold. “COVID-Twitter-BERT: A Natural Language
Processing Model to Analyse COVID-19 Content
on Twitter”. In: arXiv preprint arXiv:2005.07503
(2020).

A PREPRINT - D ECEMBER 8, 2020

S UPPLEMENTARY M ATERIAL
A DDRESSING MACHINE LEARNING CONCEPT DRIFT REVEALS DECLINING VACCINE
SENTIMENT DURING THE COVID-19 PANDEMIC
1

1

Martin Müller1 , Marcel Salathé1
Digital Epidemiology Lab, Ecole polytechnique fédérale de Lausanne (EPFL), 1202 Geneva, Switzerland

Supplementary figures

F1 positive

0.5

F1 neutral

0.5

F1 negative

FastText

0.5

BERT

0.0

0.0

0.0

2019-01

2020-01

2019-01

2020-01

Trained on data up to
2018-08-11
2018-11-09
2019-02-07
2019-05-08
2019-08-06
2019-11-04
2020-02-02
2020-05-02
2020-07-31

Train & evaluate
Evaluate

Figure S1: Performance scores by class for FastText and BERT models. For an explanation of the Figure, please refer
to Figure 2 in the main text. Unlike for the negative class, performance between FastText and BERT is comparable for
the neutral and positive class. The “negative” class shows the strongest effects due to concept drift.

10

A PREPRINT - D ECEMBER 8, 2020

1600

% Performance
change
F1-macro

0.6

Number of training samples
800

Trained on data up to
2018-08-11
2018-11-09
2019-02-07
2019-05-08
2019-08-06
2019-11-04
2020-02-02
2020-05-02
2020-07-31

400

0.4
0.2
20
0
20

2019

2020

2019

2020

2019

2020

Train & evaluate
Evaluate

% Performance
change
F1-macro

Figure S2: Drift of FastText models depending on size of training data. The plots of the first column are identical to
the FastText plots in Figure 2. For all experiments a training window length of 360 days was used. Initial performance
is decreasing with a decreasing number of training samples. Overcoming concept drift is increasingly difficult, and is
barely visible at 400 training samples.

Training window length (days)

180

0.5

270

Trained on data up to
2018-08-11
2018-11-09
2019-02-07
2019-05-08
2019-08-06
2019-11-04
2020-02-02
2020-05-02
2020-07-31

360

0.4
0.3
0
20
2019

2020

2019

2020

2019

2020

Train & evaluate
Evaluate

Figure S3: Drift of FastText models depending on the length of the training window. Each model was trained on
an equal number of 800 training examples, but distributed over 180, 270 or 360 days. A shorter training window is
occasionally associated with slightly higher initial performance and slightly faster relative performance decrease on
average.

11

A PREPRINT - D ECEMBER 8, 2020

A

B

C

D

Figure S4: This figure is equivalent to Figure 3 in the main text, except for the different datasets that were used. In
Figure 3, we show the used training and evaluation set in the full time window. This figure shows the newly added
training and evaluation data for each 90 day bin. For a detailed description of this figure please refer to Figure 3 in the
main text.

12

