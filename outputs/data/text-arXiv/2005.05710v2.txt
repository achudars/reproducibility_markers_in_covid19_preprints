Highlights
An Exploratory Study of COVID-19 Misinformation on Twitter
Gautam Kishore Shahi, Anne Dirkson, Tim A. Majchrzak
• A very timely study on misinformation on the COVID-19 pandemic
• Synthesis of social media analytics methods suitable for analysis of the infodemic
• Explaining what makes COVID-19 misinformation distinct from other tweets on COVID-19

arXiv:2005.05710v2 [cs.SI] 24 Aug 2020

• Answering where COVID-19 misinformation originates from, and how it spreads
• Developing crisis recommendations for social media listeners and crisis managers

An Exploratory Study of COVID-19 Misinformation on Twitter⋆
Gautam Kishore Shahia , Anne Dirksonb and Tim A. Majchrzakc,∗∗
a University

of Duisburg-Essen, Germany
Leiden University, Netherlands
c University of Agder, Norway
b LIACS,

ARTICLE INFO

ABSTRACT

Keywords:
Misinformation
Twitter
Social media
COVID-19
Coronavirus
Diffusion of information

During the COVID-19 pandemic, social media has become a home ground for misinformation. To
tackle this infodemic, scientific oversight, as well as a better understanding by practitioners in crisis
management, is needed. We have conducted an exploratory study into the propagation, authors and
content of misinformation on Twitter around the topic of COVID-19 in order to gain early insights.
We have collected all tweets mentioned in the verdicts of fact-checked claims related to COVID-19
by over 92 professional fact-checking organisations between January and mid-July 2020 and share
this corpus with the community. This resulted in 1 500 tweets relating to 1 274 false and 276 partially
false claims, respectively. Exploratory analysis of author accounts revealed that the verified twitter
handle(including Organisation/celebrity) are also involved in either creating (new tweets) or spreading
(retweet) the misinformation. Additionally, we found that false claims propagate faster than partially
false claims. Compare to a background corpus of COVID-19 tweets, tweets with misinformation are
more often concerned with discrediting other information on social media. Authors use less tentative
language and appear to be more driven by concerns of potential harm to others. Our results enable us
to suggest gaps in the current scientific coverage of the topic as well as propose actions for authorities
and social media users to counter misinformation.

1. Introduction
The COVID-19 pandemic is currently spreading across
the world at an alarming rate [93]. It is considered by many
to be the defining global health crisis of our time [76]. As
WHO Director-General Tedros Adhanom Ghebreyesus proclaimed at the Munich Security Conference on 15 February
2020, "We’re not just fighting an epidemic; we’re fighting an
infodemic ". [97]. It has even been claimed that the spread
of COVID-19 is supported by misinformation [27]. The actions of individual citizens guided by the quality of the information they have at hand are crucial to the success of the
global response to this health crisis. By 18 July 2020, the International Fact-Checking Network (IFCN) [62] uniting over
92 fact-checking organisations unearthed over 7 623 unique
fact checked articles regarding the pandemic. However, misinformation does not only contribute to the spread: misinformation might bolster fear, drive societal disaccord, and could
even lead to direct damage – for example through ineffective
(or even directly harmful) medical advice or through over(e.g. hoarding) or underreaction (e.g. deliberately engaging
in risky behaviour) [57].
Misinformation on COVID-19 appears to be spreading
rapidly on social media [97]. Similar trends were seen during other epidemics, such as the recent Ebola [55], yellow
fever [54] and Zika [51] outbreaks. This is a worrying de⋆

The work presented in this document results from the Horizon 2020
Marie SkÅĆodowska-Curie project RISE_SMA funded by the European
Commission.
∗∗ Corresponding author
gautamshahi16@gmail.com (G.K. Shahi);
a.r.dirkson@liacs.leidenuniv.nl (.A. Dirkson); timam@uia.no (.T.A.
Majchrzak)
ORCID (s): 0000-0001-6168-0132 (G.K. Shahi); 0000-0002-4332-0296
(.A. Dirkson); 0000-0003-2581-9285 (.T.A. Majchrzak)

Shahi et al.: Preprint submitted to Elsevier

velopment as even a single exposure to a piece of misinformation increases its perceived accuracy [56]. In response
to this infodemic, the WHO has set up their own platform
MythBusters that refutes misinformation [92] and is urging
tech companies to battle fake news on their platforms [9].1
Fact-checking organisations have united under the IFCN to
counter misinformation collaboratively, as individual factcheckers like Snopes are being overwhelmed [14].
There are many pressing questions in this uphill battle.
So far2 , five studies have investigated the magnitude or spread
of misinformation on Twitter regarding the COVID-19 pandemic [18, 39, 26, 73, 96]. However, two of these studies
either investigated a very small subset of claims [73] or manually annotated a small subset of Twitter data [39]. The
remaining studies used the reliability of the cited sources
to identify misinformation automatically [26, 18, 96]. Although such source-based approaches are popular and allow
for a large-scale analysis of Twitter data [6, 99, 10, 30, 72],
the reliability of news sources remains a subject of considerable disagreement [90, 60]. Moreover, source-based classification misclassifies misinformation propagated by generally
reliable mainstream news sources [6] and misses misinformation generated by individuals, e.g. by Donald Trump or
unofficial sources such as recently emerging web sites. According to a recent report for the European Council by Wardle and Derakhshan [91], the latter is increasingly prevalent.
1 At the same time, the WHO itself faces criticism regarding how it
handles the crisis, among others regarding the dissemination of information
from member countries [82].
2 The number of articles taking COVID-19 as an example, case study,
or even directly as the main theme is steadily growing. In particular, there is
a high number of preprints; how many of those will eventually make it to the
scientific body of knowledge remains to be seen. Any mentioning of closely
related work in such articles – including ours – must be seen as a snapshot,
as moments after submission likely more work is uploaded elsewhere.

Page 1 of 20

COVID-19 Misinformation on Twitter

In our study, we employ an alternative, complementary
approach also used by [90, 52, 68]; We rely on the verdicts
of professional fact-checking organisations which manually
check each claim. This does limit the scope of our analysis
due to the bottleneck of verifying claims but avoids the limitations of a source-based approach, thereby complementing
previous work. Furthermore, none of the previous studies
has investigated how the language use of COVID-19 misinformation differs from other COVID-19 tweets or which
Twitter accounts are associated with the spreading of COVID19 misinformation. Although there have already been some
indications that bots might be involved [26, 24, 96], the majority of posts is generated by accounts that are likely to be
human [96].
We thus conduct an exploratory analysis into (1) the Twitter accounts behind COVID-19 misinformation, (2) the propagation of COVID-19 misinformation on Twitter, and (3) the
content of incorrect claims on COVID-19 that circulate on
Twitter. We decided to work exploratory because too little is
known about the topic at hand to tailor either a purely quantitative or a purely qualitative study.
The exploration of the phenomena with the aim of rapid
dissemination of results combined with the demand for academic rigour make our article somewhat uncommon in nature. We, therefore, explicate our three contributions. First,
we present a synthesis of social media analytics techniques
suitable for the analysis of the COVID-19 infodemic. We
believe this to be a starting point for a more structured, goaloriented approach to mitigate the crisis on the go – and to
learn how to decrease negative effects from misinformation
in future crisis as they unfold. Second, we contribute to
the scientific theory with first insights into how COVID-19
misinformation differs from other COVID-19 related tweets,
which it originates from, and how it spreads. This should
pose the foundation for drawing a research agenda. Third,
we provide the first set of recommendations for practice. They
ought to directly help social media managers of authorities,
crisis managers, and social media listeners in their work.
In Section 2, we provide the academic context of our
work in the field of misinformation detection and propagation. In Sections 4 and 3, we elaborate on our data collection process and methodology, respectively. We then present
the experimental result in Section 5, followed by discussing
these results and providing recommendations for organisations targeting misinformation in Section 6. Finally, we draw
a conclusion in Section 7.

2. Background
In this section, we describe the background of misinformation, propagation of misinformation, rumours detection,
and the impact of fact-checking.

2.1. Defining misinformation
Within the field there is no consensus on the definition
for misinformation [60]. We define misinformation broadly
as circulating information that is false [100]. The term misinformation is more commonly used to refer specifically to
Shahi et al.: Preprint submitted to Elsevier

when false information is shared accidentally, whereas disinformation is used to refer to false information shared deliberately [32]. In this study, we do not make claims about the
intent of the purveyors of information, whether accidental
or malicious. Therefore, we pragmatically group false information regardless of intent. In line with recommendations
by Wardle and Derakhshan [91], we avoid the polarised and
inaccurate term fake news.
Additionally, there is no consensus on when a piece of
information can be considered false. According to seminal work by del Vicario et al. [20] it is “the possibility of
verification rather than the quality of information” that is
paramount. Thus, verifiability should be considered key to
determining falsity. To complicate matters further, claims
are not always wholly false or true, but there is a scale of
accuracy [91]. For instance, claims can be mostly false with
elements of truth. Two examples in this category are images
that are miscaptioned and claims to omit necessary background information. In our work, we name such claims partially false.
We rely on the manual evaluations of fact-checking organisations to determine which information is (partially) false
(see Section 3.2 for details on the conversion of manual evaluations from fact-checkers). We make the distinction between false and partially false for two reasons. First, other
researchers have proposed a scale over a hard boundary between false and not false, as illustrated above. Second, it
needs to be assessed, whether completely and partially false
information is perceived differently. We expect the believability of partially false information to be higher. It may be
more challenging for users to recognise claims as false when
they contain elements of truth, as this has found to be the
case even for professional fact-checkers [43].
The comparison between partially and completely false
claims thus enables us to attain better insight into differences in their spread. It is crucial for fact-checking organisations and governments battling misinformation to understand better how to sustain information sovereignty [47]. In
an ideal setting, people would always check facts and employ scientific methods. In a realistic setting, they would
at least be mainly drawn to information coming from factbased sources which work ethically and without a hidden
agenda. Authorities such as cities (local governments) ought
to be such sources [48].

2.2. Identifying rumours on Twitter
Rumours are “circulating pieces of information whose
veracity is yet to be determined at the time of posting” [100].
Misinformation is essentially a false rumour that has been
debunked. Research on rumours is consequently closely related, and the terms are often used interchangeably.
Rumours on social media can be identified through topdown or bottom-up sampling [100]. A top-down strategy
use rumours which have already been identified and factchecked to find social media posts related to these rumours.
This has the disadvantage that rumours that have not been included in the database are missed. Bottom-sampling stratePage 2 of 20

COVID-19 Misinformation on Twitter

gies have emerged more recently and are aimed at collecting a wider range of rumours often prior to fact-checking.
This method was first employed by by [101]. However, manual annotation is necessary when using a bottom-up strategy. Often journalists with expertise in verification are enlisted since crowd-sourcing will lead to credibility perceptions rather than ground truth values. The exhaustive verification may be beyond their expertise [100].
In this study, we employ a top-down sampling strategy
relying on the work of on Snopes.com and over 91 different fact-checking organisations organised under the CoronaVirusFacts/ DatosCoronaVirus alliance run by the Poynter Institute. We included all misinformation (see Section
3.2) around the topic of COVID-19, which include a Tweet
ID. A similar approach was used by Jiang et al. [35] with
Snopes.com and Politifact and by [90] using six independent
fact-checking organisations.

2.3. Misinformation propagation
To what extent information goes viral is often modelled
using epidemiological models originally designed for biological viruses [28, 69]. The information is represented as
an ‘infectious agent’ that is spread from ‘infectives’ to ‘susceptibles’ with some probability. This method was also employed by [18] for the propagation of information to study
how infectious information on COVID-19 is on Twitter. They
found that the basic reproductive number 𝑅0 , i.e. the number of infections due to one infected individual for a given
period, is between 4.0 to 5.1 on Twitter, indicating a high
level of ‘virality’ of COVID-19 information in general.3 Additionally, they found the overall magnitude of COVID-19
misinformation on Twitter to be around 11%. They also investigated the relative amplification of reliable and unreliable information on Twitter and found it to be roughly equal.
Similarly, Yang et al. [96] found that the volume of tweets
linking to low-credibility information was compared to the
volume of links to the New York Times and Centre for Disease Control and Prevention (CDC).
Other researchers have modelled information propagation on Twitter using the retweet (RT) trees, i.e. asking who
retweets whom? Various network metrics can then be applied to quantify the spread of information such as the depth
(number of retweets by unique users over time), size (number of total users involved) or breadth (number of users involved as a certain depth) [90]. These measures can also be
considered over time to understand how propagation fluctuates. Additionally, these networks can be used to investigate
the role of bots in the spreading of information [72]. Recent
studies [52, 60] have also shown the promise of propagationbased approaches for precise discrimination of fake news on
social media. In fact, it appears that aspects of tweet content
can be predicted from the collective diffusion pattern [68].
An advantage of this approach compared to epidemiological modelling, is that it does not rely on the implicit assumption that propagation is driven largely if not exclusively
3 This,

curiously, means that misinformation on the new coronavirus
has higher infectivity than the virus itself [45, 98].

Shahi et al.: Preprint submitted to Elsevier

by peer-to-peer spreading [28]. However, viral spreading is
not the only mechanism by which information can spread:
Information can also be spread by broadcasting, i.e. a large
number of individuals receive information directly from one
source. Goel et al. [28] introduced the measure of structural
virality to quantify to what extent propagation relies on both
mechanisms.

2.4. The impact of fact-checking
Previous research on the efficacy of fact-checking reveals
the corrections often do not have the desired effect and misinformation resists debunking [99]. Although the likelihood
of sharing does appear to drop after a fact-checker adds a
comment revealing this information to be false, this effect
does not seem to persist on the long run [25]. In fact, 51.9%
of the re-shares of false rumours occur after this debunking comment. This may, in part, be due to readers not reading all the comments before re-sharing. Complete retractions of the misinformation are also generally ineffective, despite people believing, understanding and remembering the
retraction [41]. Social reactance [11] may also play a role
here: people do not like being told what to think and may reject authoritative retractions. Three factors that do increase
their effectiveness are (a) repetition, (b) warnings at the initial exposure and (c) corrections that tell an alternate story
that does not leave behind an unexplained gap [41].
Twitter users also engage in debunking rumours. Overall, research supports the idea that the Twitter community
debunks inaccurate information through self-correction [100,
49]. However, self-correction can be slow to take effect [63]
and interaction with debunking posts can even lead to an increasing interest in conspiracy-like content [99]. Moreover,
it appears that in the earlier stages of a rumour circulating
Twitter users have problems differentiating between true and
false rumours [101]. This includes users of the high reputation such as news organisations who may issue corrective
statements at a later date if necessary. This underscores the
necessity of dealing with newly emerging rumours around
crises like the outbreak of COVID-19.
Yet, these corrections also do not always have the desired effect. Fact-checking corrections are most likely to be
tweeted by strangers but are more likely to draw user attention and responses when they come from friends [31]. Although such corrections do elicit more responses from users
containing words referring to facts, deceit (e.g. fake) and
doubt, there is an increase in the number of swear words [35],
too. Thus, on the one hand, users appear to understand and
possibly believe the rumour is false. On the other hand,
swearing likely indicates backfire [35]: an increase in negative emotion is symptomatic of individuals clinging to their
own worldview and false beliefs. Thus, corrections have
mixed effects that may depend in part on who is issuing the
correction.

3. Data collection & preprocessing
In this section, we describe the steps involved in the data
collection and filtering the tweets for analysis. We have used
Page 3 of 20

COVID-19 Misinformation on Twitter

Figure 1: Illustration of data collection method- Extraction of social media link (Tweet
Link) on the fact checked article and fetching the relevant tweets from Twitter (screenshots
from [34, 21]).

two datasets for our study. The first dataset are the tweets
which have been mentioned by fact-checking websites and
are classified as false or partially false and the second dataset
consists of COVID-19 tweets collected from publicly available corpus TweetsCOV19 4 and in-house crawling from MayJuly 2020. A detailed description of data collection process
explain in section 3.1.

3.1. Data collection
For our study, we gathered the data from two different
sources. The first data set consists of false or partially false
tweets from the fact-checking websites. The second is a random sample of tweets related to COVID-19 from the same
period.

3.1.1. Dataset I – Misinformation Tweets
We used an automated approach to retrieve tweets with
misinformation. First, we collected the list of fact-checked
news articles related to the COVID-19 from Snopes [74] and
Poynter [61] from 04-01-2020 to 18-07-2020. We collected
7 623 fact-checked articles using the approach mentioned in
[70]. We used Beautifulsoup [65] to crawl the content of
the news articles and prepared a list of news articles which
collected the information like title, the content of the news
article, name of the fact-checking website, location, category
(e.g. False, Partially False) of fact-checked claims.
To find the misleading posts on COVID-19 on Twitter,
we crawled the content of the news article using Beautifulsoup and looked for the article, which is referring to Twitter.
In the HTML Document Object Model(DOM), we looked
for all anchor tags <a> which defines a hyperlink. We filter
the anchor tag which contains keyword ‘twitter’ and ‘sta4 https://data.gesis.org/tweetscov19/

(January-April 2020)

Shahi et al.: Preprint submitted to Elsevier

tus’ because each tweet message is linked with the Uniform
Resource Locator(URL) in the form of https://twitter.com/
statuses/ID. From the collected URLs, we fetched the ID,
where the ID is the unique identifier for each tweet. An illustration of the overall workflow for fetching tweets mentioned in the fact-checked articles is shown in Figure 1. We
collected a total of 3 053 Tweet IDs from 7 623 news articles.
The timestamps of these tweets are between 14-01-2020 and
10-07-2020. After removing the duplicates tweets, we got
1 565 tweets for our analysis which is further filtered based
on its category as discussed in section 3.2. We further categorise the tweet ID into four different classes as mentioned
in section 3.2.
From the Tweet ID generated in the above step, we used
tweepy [66], a python library for accessing the Twitter API.
Using the library, we fetched the tweet and its description
such as created_at, like, screen name, description, and followers.

3.1.2. Dataset II – Background corpus of COVID-19
Tweets
To understand how the misinformation around COVID19 is distinct from the other tweets on this topic, we created
a background corpus of 163 096 English tweets spanning the
same time period (14 January until 10 July) as our corpus of
misinformation. We randomly selected 1 000 tweets per day
and all tweets if fewer than 1 000 tweets were available. For
January until April, we used the publicly available corpus
TweetsCOV195
5 https://data.gesis.org/tweetscov19/. We attempted to retrieve tweet
content using the Twitter API. As some tweets were no longer available, this
resulted in 92 095 tweets. TweetsCOV19 spans until April 2020 so, for May
to July, we used our own keyword-based crawler using Twitter4J, resulting
in a total of 71 000 tweets for this time span. Specifically, we used sev-

Page 4 of 20

COVID-19 Misinformation on Twitter

3.1.3. Retweets and Account details
In this section, we describe the methods used for the
crawling of retweets and the retrieval of details of author accounts.

Retweets Usually, Fake news spreads on social media immediately after sharing the post on social media. We wanted
to analyse the difference in the propagation of false and partially false tweets. We fetched all the retweet using the python
library Twarc [79]. Twarc is a command-line tool for collecting Twitter data in JSON6 format.
Our main goal to detect the difference in the propagation
speed for false and partially false tweets, so we crawled the
retweets for the data set I only.
User account details From the Twitter API, we also gath-

ered the account information: favourites count (number of
likes gained), friends count (number of accounts followed by
the user), follower count (number of followers this account
currently has), account age (number of days from account
creation date to 31-12-2020, the time when discussion about
COVID-19 started around the world), a profile description,
and user location. We used this information for both classifying the popular accounts and for bot detection.

3.2. Defining classes for misinformation
Discounting differences in capitalisation, our data originally contained 18 different verdict classes provided by the
92 fact-checking websites, i.e. Snopes and 91 organisations
in the International Fact Checking Network (IFCN). In Table 1, we provide an overview of the verdict categories that
were included or excluded in our study along with our categorisation and the original, more granular categorisation by
fact-checkers. Since each fact-checking organisation has its
own set of verdicts and Poynter has not normalised these,
manual normalisation is necessary. Following the practice
of [90], we normalised verdicts by manually mapping them
to a score of 1 to 4 (1=‘False’, 2=‘Partially False’, 3=‘True’,
4=‘Others’) based on the definitions provided by the factchecking organisations. Our definition for the four categories
are as followsFalse: Claims of an article are untrue.
Partially False: Claims of an article are a mixture of true
and false information. The article contains partially true and
partially false information, but it can not be considered as
100% true. It includes articles of type, partially false, partially true, mostly true, miscaptioned, misleading etc.
True: This rating indicates that the primary elements of a
claim are demonstrably true.
Other: An article that cannot be categorised as true, false or
partially false due to lack of evidence about its claims. This
category includes articles in dispute and unproven articles.
eral hashtags, including #Coronavirus, #nCoV2019, #WuhanCoronovirus,
#WuhanVirus, #Wuhan, #CoronavirusOutbreak,
#Ncov2020, #coronaviruschina, #Covid19, #covid19,
#covid_19, #sarscov2, #covid, #cov, and #corona. We merged both data
sets for our study.
6 https://www.json.org/

Shahi et al.: Preprint submitted to Elsevier

As we are specifically interested in misinformation, we
considered only the false and partially false category. We
also excluded claims with verdicts that did not conform to
this scale, e.g. sarcasm, unproven claims and disputed claims.
From 1 565 tweets collected, 1 500 are used for our study –
1 274 false and 226 partially false claims. The data used in
our work is available through GitHub7 .

3.3. Examples of misinformation
Figure 2 and Figure 3 display two randomly chosen examples of misinformation in the false and partially false category, respectively. The first is an example of a false claim,
namely a tweet about the rumour that Costco had issued a
recall of their toilet paper because they feared that it might
contain COVID-19 [53]. The author states that people were
running to the store to buy and then return the toilet paper
after hearing the news. Later the claim was fact-checked by
Snopes and found to be false, and no such recall had been
announced by Costco [75]. There were several other tweets
making similar false claims.
An example of a tweet [8] containing partially false information was posted by the news company, ANI. It claimed
that people quarantined from Tablighi Jamaat [5] misbehaved
towards health workers and police staff. They were not following the rules of the quarantine centre and misbehaved the
police. AFP [1] found the claim to be misleading. The misleading claim is one subset of the claim that is normalised
to partially false. The incident used in the claim was used
from a past event in Mumbai during February 2020. Different Twitter handles circulated this misinformation. The
claim was retweeted and liked by several users on Twitter.
The first tweet about toilet paper is false because the
tweet with old video was circulated with false information
about the recall of the toilet paper. The second tweet about
Tablighi Jamaat is considered partially false as the incident
portraying a true incident that people of Tablighi Jamaat are
not following the protocol, but it was re-purposed with a different video to support a false claim. So, if a claim is based
on a false incident or information its called a false claim
while if it’s based on the true incident with a false message
then its partially false.

3.4. Preprocessing of tweets
Originally, the data contained 32 known languages (according to Twitter – see Figure 4). We use the Google Translate API8 to automatically detect the correct language and
translate to English. Hereafter, tweets were lowercased and
tokenised using NLTK[46]. Emojis were identified using the
emoji package [37] and were removed for subsequent analyses. Mentions and URLs were also removed using regular
expressions. Hashtags were not removed, as they are often
used by twitter users to convey essential information. Additionally, sometimes they are used to replace regular words in
the sentence (e.g. ‘I was tested for #corona’) and thus omitting them would remove essential words from the sentence.
7 https://github.com/Gautamshahi/Misinormation_COVID-19
8 https://cloud.google.com/translate/docs

Page 5 of 20

COVID-19 Misinformation on Twitter
Table 1
Normalisation of original categorisation by the fact checking web sites
Included Our rating
(y/n)

Fact-checker
rating

Definition given by fact-checker

y
y

False
Partially false

False
Miscaptioned

y

Partially false

Misleading

n

Others

Unsupported/
Unproven

y

Partially false

Partially false

y

False

y
n

Partially false
True

Pants on fire/
Two pinocchios
Mostly false
True

The checkable claims are all false.
This rating is used with photographs and videos that are âĂĲrealâĂİ (i.e., not
the product, partially or wholly, of digital manipulation) but are nonetheless
misleading because they are accompanied by explanatory material that falsely
describes their origin, context, and/or meaning.
Offers an incorrect impression on some aspect(s) of the science, leaves the
reader with false understanding of how things work, for instance by omitting
necessary background context.
This rating indicates that insufficient evidence exists to establish the given claim
as true, but the claim cannot be definitively proved false. This rating typically
involves claims for which there is little or no affirmative evidence, but for which
declaring them to be false would require the difficult (if not impossible) task
of our being able to prove a negative or accurately discern someone elseâĂŹs
thoughts and motivations.
[Translated] Some claims appear to be correct, but some claims can not be
supported by evidence.
The statement is not accurate and makes a ridiculous claim.

n

Others

Labeled Satire

n

Others

Explanatory

y

Partially false

Mixture

y
y

Partially false
Partially false

n

Others

Mostly true
Misinformation/
Misattributed
In dispute

y
n
y
y

False
Others
Partially false
Partially false

Fake
No rating
Partially True
Manipulations

Mostly false with one minor element of truth.
This rating indicates that the primary elements of a claim are demonstrably
true.
This rating indicates that a claim is derived from content described by its
creator and/or the wider audience as satire. Not all content described by its
creator or audience as âĂŸsatireâĂŹ necessarily constitutes satire, and this
rating does not make a distinction between ’real’ satire and content that may
not be effectively recognized or understood as satire despite being labelled as
such.
"Explanatory" is not a rating for a checked article, but an explanation of a fact
on its own
This rating indicates that a claim has significant elements of both truth and
falsity to it such that it could not fairly be described by any other rating.
Mostly accurate, but there is a minor error or problem.
This rating indicates that quoted material (speech or text) has been incorrectly
attributed to a person who didn’t speak or write it.
One can see the duelling narratives here, neither entirely incorrect. For that
reason, we will leave this unrated.
[Rewritten generalized] Claims of an article are untrue
Outlet decided to not apply any rating after doing a fact checking.
Leaves out important information or is made out of context.
[Translated] Article only showed part of an interview answer, and interview
question has been phrased in a way that makes it easy to manipulate the
answer.

Therefore, we only remove the # symbol from the hashtags.

4. Method
In this section, we present our method for analysis and
illustration of the extracted data. We follow a two-way approach. In the first, we analyse the details of the user accounts involved in the spread of misinformation and propagation of misinformation (false or partially false data). In
the second, we analyse the content. With both we investigate
the propagation of misinformation on social media.

Shahi et al.: Preprint submitted to Elsevier

4.1. Account categorisation
In order to gain a better understanding of who is spreading misinformation on Twitter, we investigated the Twitter
accounts behind the tweets. First, we analyse the role of bots
in spreading misinformation by using a bot detection API to
automatically classify the accounts of authors. Similarly, we
analyse whether accounts are brands using an available classifier. Third, we investigate some some characteristics of the
accounts that reflect their popularity (e.g. follower count).

Page 6 of 20

COVID-19 Misinformation on Twitter

Figure 4: The language distribution of tweets with misinformation prior to translation of tweets

dia such as "newsbots", "spambots", "malicious bot". Sometimes, newsbots or malicious bots are trained to spread the
misinformation. Caldarelli et al. [15] discuss the role of bots
in Twitter propaganda. To analyse the role of bots, we examined each account by using a bot detection API [19].

Figure 2: An example of misinformation of false category

Figure 3: An example of Misinformation of partially false category

4.1.1. Bot detection
A Twitter bot is a type of bot program which operate
a Twitter account via the Twitter API. The pre-programmed
bot autonomously performs some work such as tweeting, unfollowing, re-tweeting, liking, following or direct messaging
other accounts. Shao et al. [71] discussed the role of social bots in spreading the misininformation. Previous studies
show there are several types of bots involved in social meShahi et al.: Preprint submitted to Elsevier

4.1.2. Type of account (brand or non-brand)
Social media, such as microblogging websites, used for
sharing information and gathering opinion on the trending
topic. Social media has different types of user, organisation, celebrity or an ordinary user. We consider organisation, celebrity as a brand which has a big number of followers and catches more attention public attention. The brand
uses a more professional way of communication, gets more
user attention [77] and have high reachability due to bigger
follower network and retweet count. With a large network,
a piece of false or partially false information spread faster
compared to a normal account. We classify the account as
a brand or normal users using a modified of TwiRole [42]
a python library. We use profile name, picture, latest tweet
and account description to classify the account.
4.1.3. Popularity of account
Popular accounts get more attention from users, so we
analyse the popularity of the account; we considered the
parameter number of followers, verified account. Twitter
gives an option to "following"; users can follow another user
by clicking the follow button, and they becomes followers.
When a tweet is posted on Twitter, then it is visible to all
of his/her followers. Twitter verifies the account, and after
doing a verification, Twitter provides the user to receive a
blue checkmark badge next to your name. From 2017 the
service is paused by Twitter, and it is limited to only a few
accounts chosen by the Twitter developer. Hence, the verified account is a kind of authentic account. We investigate
several characteristics that are associated with popular accounts, namely: Favourites count, follower count, account
age and verified status. If a popular user spread false or parPage 7 of 20

COVID-19 Misinformation on Twitter

tially false news, then it is more likely to attract more attention from other users compared to the non-popular twitter
handle.

4.2. Information diffusion
To investigate the diffusion of misinformation i.e, false
and partially false tweets, we explore the timeline of retweets
and calculate the speed of retweets as a proxy for the speed
of propagation. A retweet is a re-posting of a tweet, which a
Twitter user can do with or without an additional comment.
Twitter even provides a retweet feature to share the tweet
with your follower network quickly. For our analysis, We
only considered the retweet of tweet.

Propagation of misinformation We define the average

speed of propagation of tweet as the total number of retweet
done for a tweet divided by the total number of days the tweet
is getting retweets. The formula to calculate the propagation
speed is defined in Equation 1.
∑𝑑
𝑃𝑠 =

𝑛=1 𝑟𝑐

𝑁𝑑

(1)

Where Ps is the propagation speed, rc is retweet count
per day and Nd is the total number of days.
We calculated the speed of propagation over three different periods. The first metric Ps_a is the average overall propagation speed: the speed of retweets from the 1st retweet to
the last retweet of a tweet in our data. The second metric is
the propagation speed during the peak time of the tweet, denoted by Ps_pt . After a time being, the tweet does not get any
retweet, but again some days again start getting user attention and retweet. So, We define the peak time of the tweet
as the time (in days) from the retweet start till retweet goes
to zero for the first time. The third metric Ps_pcv is the propagation speed calculated during a first peak time of the crisis, i.e., from 15-03-2020 to 15-04-2020. We decided the
peak time according to the timeline propagation of retweet,
as shown in 5, which is maximum during the mid-March and
mid-April.
Although we are aware that misinformation gets spread
on Twitter as soon as it is shared, in the current situation
it is not possible us to detect fake tweets in real-time because fact-checking websites take a few days to verify the
claim. For example, the fake news on "Has Russia’s Putin
released lions on streets to keep people indoors amid coronavirus scare?" was first seen on the 22nd March on Twitter but the first fact-checked article published on late 23rd
March 2020 [84] and later by other fact-checking websites.
With propagation speed, our aim is to measure the speed of
propagation speed among false and partially false tweets.

4.3. Content analysis
In order to attain a better understanding of what misinformation around the topic of COVID-19 is circulating on Twitter, we investigate the content of the tweets. Due to the relatively small number of partially false claims, we combined
Shahi et al.: Preprint submitted to Elsevier

the data for these analyses. First, we analyse the most common hashtags and emojis. Second, we investigate the most
distinctive terms in our data to gain a better understanding of
how COVID-19 misinformation differs from other COVID19 related content on Twitter. To this end, we compare our
data to a background corpus of all English COVID-19 tweets
from 14-01-2020 to 10-07-2020 (See Section 3.1.2). This
enables us to find the most distinctive phrases in our corpus:
Which topics are discussed in misinformation that are not
discussed in other COVID-19 related tweets? These topics
may be of special interest, as there may be little correct information to balance the misinformation circulating on these
topics. Third, we make use of the language used in the circulating misinformation to gauge the emotions and underlying
psychological factors authors display in their tweets. The
latter may be able to give us a first insight into why they are
spreading this information. Again the prevalence of emotional and psychological factors is compared to their prevalence in a background corpus in order to uncover how false
tweets differ from the general chatter on COVID-19.

4.3.1. Hashtags and emojis
Hashtags are brief keywords or abbreviations prefixed
by the hash sign # that are used on social media platforms
to make tweets more easily searchable [13]. Hashtags can
be considered self-reported topics that the author believes
his or her tweet links to. Emoji are standardised pictographs
originally designed to convey emotion between participants
in text-based conversation [36]. Emojis can thus be considered a proxy for self-reported emotions by the author of the
tweet.
We analyse the top 10 hashtags by combining all terms
prefixed by a #. For # symbols that are stand-alone, we take
the next unigram to be the hashtag. We identify emojis using
the package emoji [37].
4.3.2. Analysis of distinctive terms
To investigate the most distinctive terms in our data, we
used the pointwise Kullback Leibner divergence for Informativeness and Phraseness (KLIP) [85] as presented in [89]9
for unigrams, bigrams and trigrams. KullbackâĂŞLeibler
divergence is a measure from information theory that estimates the difference between two probability distributions.
The informativeness component (KLI) of KLIP compares
the probability distribution of the background corpus to that
of the candidate corpus to estimate the expected loss of information for each term. The terms with the largest loss are the
most informative. The phraseness component (KLP) compares the probability distribution of a candidate multi-word
term to the distributions of the single words it contains. The
terms for which the expected loss of information is largest
are those that are the strongest phrases. We set the parameter 𝛾 to 0.8 as recommended for English text. 𝛾 determines
the relative weight of the informativeness component KLI
versus the phraseness component KLP. Additionally, for this
analysis, tweets that are duplicated after preprocessing are
9 See https://github.com/suzanv/termprofiling

for an implementation.

Page 8 of 20

COVID-19 Misinformation on Twitter

removed; these are slightly different tweets concerning the
same misinformation and would bias our analysis of distinctive terms towards a particular claim.

4.3.3. Analysis of emotional and psychological
processes
The emotional and psychological processes of authors
can be studied by investigating their language use. A wellknown method to do so is the Linguistic Inquiry and Word
Count (LIWC) method [80]. We made use of the LIWC 2015
version and focused on the categories: Emotions, Social Processes, Cognitive Processes, Drives, Time, Personal Concerns and Informal Language. In short, the LIWC counts
the relative frequency of words relating to these categories
based on manually curated word lists. All statistical comparisons were made with Mann-Whitney U tests.

5. Results
This section describes the result obtained from our analysis for both datasets I, i.e., 1 500 tweets, which classified
as misinformation and dataset II, i.e., 163 096 COVID-19
tweets as the background tweets. A detailed comparison of
two datasets is shown in table 2.

5.1. Account categorisation
From 1 500 tweets, we filter 1 187 unique accounts and
performed categorisation of the account using the method
mentioned in Section 4.1. The summary of the result obtained is discussed as follow.

Bot detection From BotoMeter API, we use the Complete

Automation Probability (CAP) score to classify the bot. CAP
is the probability of the account being a bot according to the
model used in the API. We choose the CAP score of more
than 0.65. We discovered that there are 24 bot accounts out
1 187 unique user accounts; user IDs 1025102081265360896
and 1180933034423529473, for instance, are classified as
bots.

Brand detection For Brand detection, we used the TwiRole API to categorised the accounts as a brand, male and
female. We also randomly checked the category of the account. We have got 792 accounts as a brand. For instance,
user ID 18815507 is an organisation account while user ID
2161969770 is a representative of the UNICEF.
Popularity of account For measuring the popularity, we

gathered the information about favourite counts gained by
the accounts, followers count, friends accounts, and the age
of the accounts using the Twitter API. We represented the
median of Favourite Count, account age and followers count,
as shown in Table 2.

5.2. Information diffusion
In this section, we describe the propagation of misinformation with timeline analysis and speed of propagation.

Shahi et al.: Preprint submitted to Elsevier

Table 2
Description of twitter accounts and tweets from Dataset
I(Misinformation) and Dataset II (Background corpus)
Dataset:
Number of Tweets
Unique Account
Verified Account
Distinct Language
Organisation/Celebrity
Bot Account
Tweet without Hashtags
Tweet without mentions
Tweet with Emoji
Median Retweet Count
Median Favourite Count
Median Followers Count
Median Friends Count
Median Account Age (d)

I

II

1 274/276(1 500)
964/198(1 117)
727/131(858)
31/21(33)
698/135(792)
22/2(24)
919/147(1 066)
1019/176/(1 195)
168/20/(188)
165/169(165)
2 446/3 381(2 744)
74 632/69 725(74 131)
526/614(531)
82/80(82)

163 096
143 905
16 720
1(en)
16 324
1 206
134 242
71 316
14 021
8
9 695
935
654
108

Timeline of misinformation tweets We presented the

timeline of the misinformation tweets created during January 2020 to mid-July 2020 in Figure 5. The blue colour
indicates the propagation of the false category, whereas orange colour indicates the partially false category. The timeline plot of tweet shows that the spread of misinformation of
false category is faster than the partially false category. During the peak time of the COVID-19, i.e., mid-March to midApril, the number of false and partially false tweets were
maximum.
The diffusion of misinformation tweets can be analysed
in terms of likes and retweet [78]. Likes indicate how many
times a user clicked the tweet as a favourite while retweet is
when a user retweets a tweet or retweet with comment. We
have visualised the number of likes and retweet gained by
each misinformation tweet with the timeline. There is considerable variance in the count of retweet and likes, so we
decided to normalise the data. We normalise the count of
likes and retweet using Min-Max Normalization in the scale
of [0, 1]. We normalised the count of retweet and likes for
the overall month together and plotted the normalised count
of retweet and liked for both false and partially false and plotted it for each month. In Figure 6 (p. 11), we presented our
result from January to July 2020, one plot for each month.
The blue colour indicates the retweet of the false category,
whereas orange colour indicates the retweet of the partially
false category. Similarly, the green colour shows the likes of
the false category, whereas red colour shows the likes of the
partially false category.
The timeline analysis of normalised retweet shows that
misinformation(false category) gets more likes than the partially false category, especially during mid-March to midApril 2020. The spread of misinformation was at a peak
from 16th March to 23rd April 2020, as shown in Figure 5.
However, for the retweet, there is no uniform distinction between false and partially false category. Although, for overall misinformation tweets, the number of likes is comparatively more than the retweet.
Page 9 of 20

COVID-19 Misinformation on Twitter

Figure 5: Timeline of misinformation tweets created during January 2020 to mid-July 2020

Propagation of misinformation We calculated the three

variant of propagation speed of tweet as discussed in Section 4.2. Results for Ps_a , Ps_pt and Ps_pcv are describe in
Table 3. We have observed that the speed of propagation is
higher for the false category and it was the highest during
the peak time of tweet (time duration from the beginning to
the day tweet not getting new retweet).
We performed a chi-square test on the propagation speed
shown in table 3. The analysis showed that there is a difference in the speed of propagation in tweets, between false
and partially false by performing (X2 (3, N = 1500) = 10.23,
p <.001). The speed of propagation for the false tweet was
more than a partially false tweet, which means the false tweets
speed faster. In particular, the propagation speed was maximum during the peak time of the COVID-19 according to
Figure 5.
Table 3
Propagation speed of retweet for misinformation tweets

Ps_a
Ps_pt
Ps_pcv

False(𝜎)

Partially False(𝜎)

Overall(𝜎)

365(15.6)
526(26.6)
418(17.4)

260(6.5)
394(14.6)
357(9.7)

209(13.6)
376(22.9)
329(15.2)

5.3. Content analysis
This section discusses the result obtained after doing the
content analysis of tweets discussing false and partially false
claims.

5.3.1. Hashtag & emoji analysis
Hashtag analysis As illustrated in Figure 7, many of the
most commonly used hashtags in COVID-19 misinformation concern the corona virus itself (i.e. #covid19 and #coronavirus) or stopping the virus (i.e. #stopcorona and #komeshacorona). Since we did not use any hashtags in the data
collection of our corpus of COVID-19 misinformation (See
Section 3.1), this confirms that our method managed to capture misinformation related to the corona crisis. Additionally, the hashtags #fakenews, #stopbulos and #bulo stand
out; the author appear to be calling out against other misinformation or hoaxes. The term fake news is widely used to
Shahi et al.: Preprint submitted to Elsevier

refer to inaccurate information [100] or more specifically to
“fabricated information that mimics news media content in
form but not in organisational process or intent” [40]. It appears that some authors are discrediting information spread
by others. Yet, we are unable to determine based on this
analysis who they are discrediting. Furthermore, three locations can be discerned from the hashtags: Madagascar,
Mérida in Venezuela, and Daraq in Iran. Manual analysis
revealed that various false claims are linked to each location.
For example, there was misinformation circulating on Facebook that the Madagascan president was calling up African
states to leave the WHO [22] and the Madagascan government has been promoting the disputed COVID-19 organics,
a herbal drink that could supposedly cure an infection [58].
Mérida is mainly mentioned in relation to a viral image in
which one can see a lot of money lying on the street. On social media, people claimed that Italians were throwing money
onto the streets to demonstrate that health is not bought with
money, while in reality bank robbers had dropped money on
the streets in Mérida, Venezuela [3]. Iran has also been central to various false claims, such as the claim by the head of
judiciary in Iran that the first centres that volunteered to suspend activities in response to the pandemic were religious
institutions [23], implying that the clergy were pioneers in
trying to halt the spread

Emoji analysis Emojis are used on Twitter to convey emo-

tions. We analysed the most prevalent emojis used by authors of COVID-19 misinformation on Twitter (see Figure 8).
It appears authors make use of emojis to attract attention
to their claim (loudspeaker, red circle) and to convey distrust or dislike (down-wards arrow, cross) or danger (warning sign, police light). In our data set, the thinking emoji
is mostly used as an expression of doubt, frequently relating
to whether something is fake news or not. Tweets with the
laughing emoji are either making fun of someone (e.g. the
WHO director-general) or laughing at how dumb others are.
Both the play button and the tick are often used for check
lists, although the latter is occasionally also conveying approval.

Page 10 of 20

COVID-19 Misinformation on Twitter

Figure 6: Frequency distribution of retweet(time window of 3 hours) for false (blue) and
partially false (orange) claims for each month- 6(a) January, 2020, 6(b) February, 2020,
6(c) March, 2020, 6(d) April, 2020, 6(e) May, 2020, 6(f) June, 2020, 6(g) July, 2020

Shahi et al.: Preprint submitted to Elsevier

Page 11 of 20

COVID-19 Misinformation on Twitter
Percentage of hashtags
14

12

10

8

6

4

2

0
#covid19

#coronavirus

#fakenews

#madagascar

#komeshacorona

stop corona
#stopbulos

stop hoaxes
#bulo

hoax

#

corona daraq
#mérida

#stopcovid

Figure 7: Top 10 hashtags used in the tweets with misinformation. (Translation provided in blue where necessary.)

10

Percent age of em ojis

8

6

4

2

0

Figure 8: Top 10 emojis used in the tweets

5.3.2. Most distinctive terms in COVID-19
misinformation
Analysing the most distinctive terms in our corpus compared to a corpus of general COVID-19 tweets can reveal
which topics are most unique. The more unique a topic is
to the misinformation corpus, the more likely it is that for
this topic there is a larger amount of misinformation than
correct information circulating on Twitter. We can see in
Table 4 which phrases have the highest KLIP score and thus
are most distinct to our corpus of COVID-19 misinformation when compared to the background corpus of COVID-19
tweets. First, we find that misinformation more often concerns discrediting information circulating on social media
(‘fake news’, ‘circulating on social’, ‘social network’, ‘soShahi et al.: Preprint submitted to Elsevier

Table 4
Top 10 most informative terms in misinformation tweets compared to COVID-19 background corpus
False Claims

Partially False Claims

social medium
circulating on social
social network
fake news
not
corona virus
medium briefing
world health organization
ministry of health
circulating

fake news
mortality rate
mild criticism
join the homage
several voitur
human-to-human transmission
bay area
santa clara
latest information
situation report

cial media’ and ‘circulating’). Second, compared to general
COVID-19 tweets, completely false misinformation more often mentions governing bodies related to health (‘world health
organisation’ and ‘ministry of health’) and their communication to the outside world (‘medium briefing’). Conversely,
partially false misinformation appears more concerned with
human-to-human transmission, mortality rates and running
updates on a situation (‘latest information’ and ‘situation report’) than the average COVID-19 tweet. It is interesting
that also the term ‘mild criticism’ is distinctive of partially
false claims; It is congruent with the idea that these authors
agree with some and disagree with other correct information.
Manual inspection reveals that both the terms ‘bay area’ and
‘santa clara’ refer to a serology (i.e. antibody prevalence)
study on COVID-19 that was done in Santa Clara, California. Join the homage, and several voitur most likely have a
high KLIP score due to the combination of translation errors and the fact that non-English words occur only in the
misinformation corpus (also see Section 6.3).

5.3.3. Psycho-linguistic analysis
According to Wardle and Derakhshan [91], the most successful misinformation plays into people’s emotions. To investigate the emotions and psychological processes portrayed
by authors in their tweets, we use the LIWC to estimate the
relative frequency of words relating to each category [80].
LIWC is a good proxy for measuring emotions in tweets:
In a recent study of emotional responses to COVID-19 on
Twitter, Kleinberg et al. [38] found that the measures of the
LIWC correlate well with self-reported emotional responses
to COVID-19.
First, we compared the false with the partially false misinformation, but there do not seem to be significant differences in language use. Second, we compared all misinformation on COVID-19 with a background corpus of tweets
on COVID-19. Both positive and negative emotions are significantly less prevalent in tweets with COVID-19 misinformation than in COVID-19 related tweets in general (𝑝 <
0.0001) (Figure 9). This is also the case for specific negative emotions such as anger and sadness (𝑝 < 0.0001 and
𝑝 = 0.0002 resp.). The levels of anxiety expressed are not
significantly different, however.
Page 12 of 20

EMOTIONS

COVID-19 Misinformation on Twitter
Positive emotions

Data
COVID-19 Tweets
Misinformation

Negative emotions
Anxiety
Anger
Sadness
Family
Insight
Cause
Discrepancy
Tentative
Certainty
Differentiation
Affiliation
Achievement
Power
Reward
Risk
Focus on past
Focus on present
Focus on future
Work

PERSONAL
CONCERNS

TIME

DRIVES

LIWC category

COGNITIVE
PROCESSES

Friends

Leisure
Home
Money
Religion

INFORMAL
LANGUAGE

Death
Swearing
Netspeak
Assent
Nonfluencies
Fillers
0

2

4
6
Mean frequency

8

10

Figure 9: LIWC results comparing COVID-19 background corpus to COVID-19 misinformation

When we consider cognitive processes that can be discerned from language use, we see that authors of tweets containing misinformation are significantly less tentative in what
they say (𝑝 < 0.0001) although they also contain less words
that reflect certainty (𝑝 < 0.0001). Moreover, they contain
less words relating to the discrepancy between the present
(i.e. what is now) and what could be (i.e. what would, should
or could be) (𝑝 < 0.0001).
We also consider indicators of what drives authors. It appears that authors posting misinformation are driven by affiliations to others (𝑝 = 0.003) significantly more often than
authors of COVID-19 tweets in general and significantly less
often by rewards (𝑝 < 0.0001) or achievements (𝑝 = 0.001).
In line with the lack of focus on rewards, money concerns
also appear to be discussed less (𝑝 < 0.0001). Thus, authors
posting misinformation appear to be driven by their desire
to prevent people they care about from coming to harm. Interestingly, tweets on misinformation are significantly less
likely to discuss family (𝑝 < 0.0001) although not less likely
to discuss friends.
Misinformation is also less likely to discuss certain perShahi et al.: Preprint submitted to Elsevier

sonal concerns, such as matters concerning the home (𝑝 =
0.002) or money (𝑝 < 0.0001) but also death (𝑝 = 0.03).
Yet, they are more likely to discuss personal concerns relating to work (𝑝 < 0.0001) and religion (𝑝 < 0.0001).
Furthermore, COVID-19 tweets appear to have a focus
on the present. COVID-19 misinformation seems to also focus on the present but to a significantly lesser degree (𝑝 <
0.0001). Tweets containing misinformation are also less focused on the future (𝑝 < 0.0001), whereas the focus on past
does not differ. Lastly, although both corpora are from Twitter, the COVID-19 tweets containing misinformation use relatively less informal language with less so-called netspeak
(e.g. lol and thx) (𝑝 < 0.0001), swearing (𝑝 < 0.0001) and
assent (e.g. OK) (𝑝 < 0.0001). The smaller amount of assent
words might also indicate that these tweets are expressing a
disagreement with circulating information, in line with the
results from the emoji analysis.

Page 13 of 20

COVID-19 Misinformation on Twitter

6. Discussion
Based on our analysis, we discuss our findings. We first
look at lessons learned from using Twitter in an ongoing crisis before deriving recommendations for practice. We then
scrutinise the limitations of our work, which form the basis
for our summary of open questions.

6.1. Lessons learned
While conducting this research, we encountered several
issues concerning the use of Twitter data to monitor misinformation in an ongoing crisis. We wanted to point these out
in order to stimulate a discussion on these topics within the
scientific community.
The first issue is that the Twitter API severely limits the
extent to which the reaction to and propagation of misinformation can be researched after the fact. One of the major
challenges with collecting Twitter data is the fact that the
Twitter API does not allow for retrieval of tweet replies over
7 days old and limits the retrieval of retweets. As it typically takes far longer for a fact-checking organisation to verify or discount a claim, this means early replies cannot be
retrieved in order to gauge the public reaction before factchecking. Recently, Twitter has created an endpoint specifically for retrieving COVID-19 related tweets in real-time for
researchers [86]. Although we welcome this development,
this does not solve the issue at hand. Although large data
sets of COVID-19 Twitter data are increasingly being made
publicly available [16], as far as we are aware, these do not
include replies or retweets either.
The second issue is that there is an inherent tension between the speed at which data analysis can be done to aid
practitioners combating misinformation and the magnitude
of Twitter data that can be included. In a crisis where speed
is of the essence, this is not trivial. Our data was limited by
the number of claims that included a tweet (for more on data
limitations see Section 6.3), causing a loss of around 90% of
the claims we collected from fact-checking websites. This
problem could be mitigated to some extent by employing
similarity matching to map misinformation verified by factchecking organisations to tweets in COVID-19 Twitter data
[16]. However, this would be computationally intensive and
require the creation of a reliable matching algorithm, making this approach far slower. Moreover, automatic methods
for creating larger data sets will also lead to more noisy data.
Thus, such an approach should rather be seen as complementary to our own. Probably, social media analytics support can
draw from lessons learned on crisis management decision
making under deep uncertainty [64]. Eventually, more work
and scientific debate on this topic are necessary. Additionally, as an academic community, it is important to explicitly
convey what can and what cannot be learned from the data so
as to prevent practitioners from drawing unfounded conclusions. The other way around, we deem it necessary to “look
over the shoulder” over practitioners to learn about their way
of handling the dynamics of social media, eventually leading
to a better theory.
A third point that must be considered by the academic
Shahi et al.: Preprint submitted to Elsevier

community researching this subject is the risk of profiling
Twitter users. There have been indications that certain user
characteristics such as gender [17] and affiliation with the
alt-right community [81] may be related to the likelihood
of spreading misinformation. Systematic analyses of these
characteristics could prove valuable to practitioners battling
this infodemic but simultaneously raises serious concerns related to discrimination. In this article, we did not analyse
such characteristics, but we urge the scientific community to
consider how this could be done in an ethical manner. Better
understanding which kind of people create, share and succumb to misinformation would much help to mitigate their
negative influence.
Fourth, relying on automatic detection of fact-checked
articles can lead to false results. The method used by factcheckers is often confusing and messyâĂŤ it is a muddle
of claims, news articles and social media posts. Additionally, each fact-checker appears to have its own process of
debunking and set of verdicts. We even encountered cases
where fact-checkers discuss multiple claims in one go, resulting in additional confusion. Moreover, fact-checkers do
not always explicitly specify the final verdict or class (false
or not) of the claim. For example, in a fact check performed
by Pesa Check [59] the claim "Chinese woman was killed in
Mombasa over COVID-19 fears" is described and the article
links various news sources. Then, abruptly in the bottom, a
tweet message about a mob lynching of a man is embedded,
and no specification of the class (false or not) of the article
is mentioned.

6.2. Recommendations
Research on a topic that relates to crisis management offers the chance to not only contribute to the scientific body
of knowledge but directly (back) to the field. Our findings
allow us to draw the first set of recommendations for public
authorities and others with an official role in crisis communication. Ultimately, these also could be helpful for all critical
users of social media, and especially those who seek to debunk misinformation. While we are well aware that we are
unable to provide sophisticated advice well-backed by theory, we believe our recommendations may be valuable for
practitioners and can lead the way to contributions to theory.
In fact, public authorities seek for such advice based on scientific work, which can be the foundation for local strategies
and aid in arguing for measurements taken (cf. e.g. with the
work presented by Grimes et al. [29], Majchrzak et al. [47]).
First, and rather unsurprisingly, closely watching social
media is recommended (cf. e.g. with [4, 94, 88]). COVID19 has sparked much misinformation, and it quickly propagates. Our work indicates that this is not an ephemeral phenomenon. For the john doe user, our findings suggest to always be critical, even if alleged sources are given, and even
if tweets are rather old or make the reference of old tweets.
Second, our results serve as proof that brands (organisations or celebrities) are involved in approximately 70%
of false category and partially false category of misinformation. They either create or circulate misinformation by
Page 14 of 20

COVID-19 Misinformation on Twitter

performing activities such as liking or retweeting. This is
in line with work by researchers from the Queensland University of Technology who also found that celebrities are
so-called “super-spreaders” of misinformation in the current
crisis [12]. Thus, we recommend close monitoring of celebrities and organisations that have been found to spread misinformation in order to catch misinformation at an early stage.
For users, this means that they should be cautious, even if a
tweet comes from their favourite celebrity.
Third, we recommend close monitoring of tags such as
#fakenews that are routinely associated with misinformation.
For Twitter users, this also means that they have chances to
check if a tweet might be misinformation by checking replies
to it – these replies being tagged with for instance #fakenews
would be an indicator of suspicion 10 .
Fourth, we advise to particularly study news that are partially false – despite an observed slower propagation, it might
be more dangerous to those not routinely resorting to information that provides the desired reality rather than facts.
As mentioned before, it may be more challenging for users
to recognise claims as false when they contain elements of
truth, as this has found to be the case even for professional
fact-checkers [43]. It is still an open question whether there
is less partially false than false information circulating on
Twitter or whether fact-checkers are more likely to debunk
completely false claims.
Fifthly, we recommend authorities to carefully tailor their
online responses. We found that for spreading fake news,
emojis are used to appeal to the emotions. One the one hand,
you would rather expect a trusted source to have a neutral,
non-colloquial tone. On the other hand, it seems to advisable
to get to the typical tone in social media, e.g. by also using
emojis to some degree. We also advise authorities to employ
tools of social media analytics. This will help them to keep
updated on developing misinformation, as we found that for
example, psycho-linguistic analysis can reveal particularities
that differ in misinformation compared to the “usual talk” on
social media. Debunking fake news and keeping information
sovereignty is an arms race – using social media analytics to
keep pace is therefore advisable. In fact, we would recommend authorities to employ methods such as the ones discussed in this paper as they work not only ex-post but also
during an ongoing infodemic. However, owing to the limitation of data analysis and regarding API usage (cf. with
the prior and with the following section), we recommend
making social media monitoring part of the communication
strategy, potentially also manually monitoring it. This advice, in general, applies to all Twitter users: commenting
something is like shouting out loudly on a crowded street,
just that the street is potentially crowded by everyone on the
planet with Internet access. Whatever is tweeted might have
consequences that are unsought for (cf. e.g. [83]).
Lastly, we recommend working timely, yet calmly, and
with an eye for the latest developments. During our analy10 This also applies the other way around: facts might be commented
ironically or deliberately provocative by using the tag #fakenews to create
confusion and to make trustworthy sources appear biased.

Shahi et al.: Preprint submitted to Elsevier

sis, we encountered much bias – not only on Twitter but also
on media and even in science. Topics such as the justification of lockdown measurement spark heated scientific debate already and offer much controversy. Traditional media,
which supposedly should have well-trained science journalists, will cite vague and cautiously phrases ideas from scientific preprints as seeming facts, ignoring that that ongoing
crisis mandates them to be accessible before peer review.
Acting cautiously on misinformation will not only likely create more misinformation, but it may erode trust. Our final
recommendation for officials is, thus, to be the trusty source
in an ocean of potential misinformation.
These recommendations must not be mistaken for definite guidelines, let alone a handbook. They should offer
some initial aid, though. Moreover, formulating them supports the identification of research gaps, as will be discussed
along with the limitations in the following two subsections.

6.3. Limitations
Due to its character as complete yet early research, our
work is bound to several limitations. Firstly, we are aware
that there may be a selection bias in the collection of our
data set as we only consider rumours that were eventually
investigated by a fact-checking organisation. Thus, our data
probably excluded less viral rumours. Additionally, we limited our analysis to Twitter, based on prior research by [18]
that found that out of the mainstream media, it was most susceptible to misinformation. Nonetheless, this does limit our
coverage of online COVID-19 misinformation.
We are also aware that we introduce another selection
bias through our data collection method, as we only include
rumours for which the fact-checking organisation refers to a
specific tweet id in its analysis of the claim. Furthermore, we
cannot be certain that this tweet id refers to the tweet spreading misinformation, as it could also refer to a later tweet refuting this information or an earlier tweet spreading correct
information that was later re-purposed for spreading misinformation. Two examples of this are: (1) “Carnival in Bahia
- WHO WILL ? -URL-” which refers to a video showing
Carnival in Bahia but of which some claim it shows a gay
party in Italy shortly before the COVID-19 outbreak [2] and
(2) “ i leave a video of what happened yesterday 11/03 on a
bicentennial bank in merida. Yes, these are notes.” which
is the correct information for a video from 2011 that was repurposed to wrongly claim that Italians were throwing cash
away during the corona crisis [3].
Second, our interpretation of both hashtag and emoji usage by authors of misinformation is limited by our lack of
knowledge of how the authors intended them. Both are culturally and contextually bound, as well as influenced by age
and gender [33] and open to changes in their interpretation
over time [50].
Thirdly, despite indications of a rapid spread of COVID19 related misinformation on Twitter, a recent study by Allen
et al. [6] found that exposure to fake news on social media
is rare compared to exposure to other types of media and
news content, such as television news. They do concede that
Page 15 of 20

COVID-19 Misinformation on Twitter

it may have a disproportionate impact, as the impact was
not measured. Regardless, further research not only into the
prevalence but also into the exposure of people to COVID19 misinformation is necessary. Our study does not allow
for any conclusions to be drawn on this matter.
A fourth limitation stems from the selection of our background corpus. Although the corpora span the same time
period, COVID-19 misinformation was allowed to be written in another language than English, whereas we limited our
background corpus to English tweets. Therefore, the range
of topics discussed in both corpora may also differ for this
reason. Consequently, we need to remain critical of informative terms of the COVID-19 misinformation corpus that refer to non-English speaking countries or contain non-English
words (e.g. voitur).
However, none of these limitations impairs the originality and novelty of our work; in fact, we gave first recommendations for practitioners and are now able to propose directions for future research.

6.4. Open questions
On the one hand, work on misinformation in social media is no new emergence. On the other hand, the current
crisis has made it clear how harmful misinformation is. Obviously, strategies to mitigate the spread of misinformation
are needed. This leads to open research questions, particularly in light of the limitations of our work. Open questions
can be divided into four categories.
First, techniques, tools and theory from social media analytics must be enhanced. It should become possible – ideally in half- or fully-automated fashion – to assess the propagation of misinformation. Understanding where misinformation originates, in which networks in circulates, how it is
spread, when it is debunked, and what the effects of debunking are ought to be researched in detail. As we already set
out in this paper, it would be ideal for providing as much discriminatory power as possible, for example by distinguishing misinformation that is completely and partly false; misinformation that is spread intentionally and by accident (possibly even with good intention, but not knowing better); and
misinformation that is shared only in silos versus misinformation that leaves such silos and propagates further. Not
only such a typology (maybe even taxonomy) would make
a valuable contribution to theory but also in-depth studies
of the propagation by type. Such insights would also aid
fact-checkers, who would, for example, learn when it makes
sense to debunk facts, and whether there is a “break-even”
point after which it is justified to invest the effort for debunking.
Second, since a holistic approach is necessary to effectively tackle misinformation, it is important to investigate
how our results – and future results on the propagation of
misinformation on Twitter – relate to other social media.
While Twitter is attractive for study and important for misinformation due to the brevity and speed, other social media should also be researched. COVID-19 misinformation is
not necessarily restricted to a single platform and may thus
Shahi et al.: Preprint submitted to Elsevier

be spread from one platform to another. Consequently, the
fact-checking organisation may not mention any tweets despite a claim also being present on Twitter. Especially if
the origin of the claim was another platform, there might be
several seeds on Twitter as people forward links from other
platforms. As part of this, the spread of fake news through
closed groups and messages would make an interesting object of study.
Third, the societal consequences of fake news ought to
be investigated. There is no doubt society is negatively impacted, but to which extent these occur, whom they affect,
and how the offline spread of misinformation can be mitigated remain open research questions. Again, achieving high
discriminatory power would be much helpful to counter misinformation. For example, it would be worthwhile to investigate how the diffusion of misinformation about COVID19 differs per country. In this regard, specifically, the relation between trust and misinformation is a topic that requires
closer investigation. In order for authorities to maintain information sovereignty, users – in this case typically citizens
– need to trust the authorities. Such trust may vary widely
from country to country. In general, a high level of trust, as
achieved in the Nordic countries [44, 7], should help mitigating misinformation. Thus, a better understanding of how
authorities can gain and maintain a high level of trust could
greatly benefit effective crisis management.
Fourth, researching synergetically between the fields of
social media analytics and crisis management could benefit
both fields. On the one hand, social media analytics could
benefit from the expertise of crisis managers and researchers
in the field of crisis management in order to better interpret
their findings and to guide their research into worthwhile directions. On the other hand, researchers in crisis management could make use of novel findings on the propagation
of misinformation during the crisis to improve their existing
theoretical models to provide holistic approaches to information dissemination throughout the crisis. Crisis management
in practice needs a set of guidelines. What we provided here
is just a starting point; an extension requires additional quantitative and especially qualitative research as well as validation by practitioners. Further collaboration of these fields is
necessary.

7. Conclusion
In this article, we have presented work on COVID-19
misinformation on Twitter. We have analysed tweets that
have been fact-checked by using techniques common to social media analytics. However, we decided on an exploratory
approach to cater to the unfolding crisis. While this brings
severe limitations with it, it also allowed us to gain insights
otherwise hardly possible. Therefore, we have presented rich
results, discussed our lessons learned, have first recommendations for practitioners, and raised many open questions.
That there are so many questions – and thereby research gaps
– is not surprising, as the COVID-19 crisis is among few
stress-like disasters where misinformation is studied in dePage 16 of 20

COVID-19 Misinformation on Twitter

tail11 – and we are just at the beginning. Therefore, it was
our aspiration to contribute to a small degree to mitigating
this crisis.
We hope that our work can stimulate the discussion and
lead to discoveries from other researchers that make social
media a more reliable data source. Some of the questions
raised will also be on our future agendas. We intend to continue the very work of this paper, even though in a less exploratory fashion. Rather, we will seek to verify our early
findings quantitatively with much larger data sets. We will
seek collaboration with other partners to gain access to historical Twitter data in order to investigate all replies and
retweets to the tweets on our corpus. This extension should
cover not only additional misinformation but also full sets
of replies and retweets. Moreover, it would be valuable to
longitudinally study how misinformation propagates as the
crisis develops. Regarding COVID-19, medical researchers
warn of the second wave [95], and maybe consecutive further ones. Will misinformation also come in waves, possibly
in conjunction with societal discussion, political measurements, or other influencing factors? Besides an extension
of the data set, our work will be extended methodologically.
For example, we seek to stance detection methods to determine the position of replies towards the claim. At the same
time, we would like to qualitatively explore the rationale behind our observation.
Right as we concluded our work on this article, “doctors, nurses and health expert [. . . ] sound the alarm” over a
“global infodemic, with viral misinformation on social media threatening lives around the world” [87]. They target
tech companies, specifically those that run social media platforms. We take their letter as encouragement. The companies might be able to filter much more misinformation
than they do now, but to battle, this infodemic much more
is needed. We hope we could help to arm those that seek for
truth!

CRediT authorship contribution statement
Gautam Kishore Shahi: Data Collection, Data curation, Investigation, Methodology, Software, Visualization,
Writing – original draft, Writing – review & editing. Anne
Dirkson: Data curation, Investigation, Methodology, Software, Visualization, Writing – original draft, Writing – review & editing. Tim A. Majchrzak: Conceptualization,
Funding acquisition, Methodology, Project administration,
Writing – original draft, Writing – review & editing.

References
[1] AFP Fact Check, 2020a.
This video has circulated in reports since at least February 2020, weeks before the incident at a coronavirus quarantine facility in India.
URL:
https://factcheck.afp.com/video-has-circulated-reports-leastfebruary-2020-weeks-incident-coronavirus-quarantine-facility.
11 One other is the European refugee crisis [67]. However, due to the
limited time of onset, with regard to crisis management particularly shocks
– such as earthquakes and floods – are studied.

Shahi et al.: Preprint submitted to Elsevier

[2] AFP Fact Check, 2020b. This video shows a Brazil carnival in 2018,
not a party in Italy. URL: https://factcheck.afp.com/video-showsbrazil-carnival-2018-not-party-italy.
[3] Africa Check, 2020. Italians throwing away cash in coronavirus
crisis? No, photos of old Venezuelan currency dumped by robbers. URL: https://africacheck.org/fbcheck/italians-throwingaway-cash-in-coronavirus-crisis-no-photos-of-old-venezuelancurrency-dumped-by-robbers/.

[4] Alexander, D.E., 2014. Social media in disaster risk reduction and
crisis management. Science and engineering ethics 20, 717–733.
[5] Ali, J., 2003. Islamic revivalism: The case of the tablighi jamaat.
Journal of Muslim Minority Affairs 23, 173–181.
[6] Allen, J., Howland, B., Mobius, M., Rothschild, D., Watts, D.J.,
2020. Evaluating the fake news problem at the scale of the information ecosystem. Science Advances 6, eaay3539. URL: http:
//advances.sciencemag.org/, doi:10.1126/sciadv.aay3539.
[7] Andreasson, U., 2017. Trust–The Nordic Gold. Nordic Council of
Ministers.
[8] ANI, 2020. Occupants were unruly since morning [...]. URL: https:
//twitter.com/ANI/status/1245349439453253632.
[9] BBC, 2020. WHO says fake coronavirus claims causing ’infodemic’.
URL: https://www.bbc.com/news/technology-51497800.
[10] Bovet, A., Makse, H.A., 2019. Influence of fake news in Twitter during the 2016 US presidential election. Nature Communications 10, 1–14. URL: https://doi.org/10.1038/s41467-018-07761-2,
doi:10.1038/s41467-018-07761-2, arXiv:1803.08491.
[11] Brehm, S.S., Brehm, J.W., 1981. Psychological reactance: A theory
of freedom and control. Academic Press.
[12] Brisbane Times, 2020.
Celebrities ’super-spreaders’ of
fake news, Queensland researchers say.
URL: https:
//www.brisbanetimes.com.au/national/queensland/celebritiessuper-spreaders-of-fake-news-queensland-researchers-say20200423-p54mpu.html.

[13] Bruns, A., Stieglitz, S., 2013. Towards more systematic Twitter analysis: metrics for tweeting activities. International Journal of Social Research Methodology 16, 91–108. doi:10.1080/
13645579.2012.756095.
[14] Business Insider, 2020. One of the internetâĂŹs oldest factchecking organizations is overwhelmed by coronavirus misinformation âĂŤ and it could have deadly consequences.
URL:
https://www.businessinsider.nl/coronavirus-snopesmisinformation-fact-checking-overwhelmed-deadly-consequences2020-3?international=trueŹr=US.

[15] Caldarelli, G., De Nicola, R., Del Vigna, F., Petrocchi, M., Saracco,
F., 2019. The role of bot squads in the political propaganda on twitter.
arXiv preprint arXiv:1905.12687 .
[16] Chen, E., Lerman, K., Ferrara, E., 2020. Covid-19: The first public
coronavirus twitter dataset. arXiv:2003.07372.
[17] Chen, X., Sin, S.C.J., Theng, Y.L., Lee, C.S., 2015. Why students share misinformation on social media: Motivation, gender, and
study-level differences. The Journal of Academic Librarianship 41,
583–592.
[18] Cinelli, M., Quattrociocchi, W., Galeazzi, A., Valensise, C.M.,
Brugnoli, E., Schmidt, A.L., Zola, P., Zollo, F., Scala, A., 2020
(Preprint). The COVID-19 Social Media Infodemic. arXiv URL:
https://arxiv.org/abs/2003.05004, arXiv:2003.05004v1.
[19] Davis, C.A., Varol, O., Ferrara, E., Flammini, A., Menczer, F., 2016.
Botornot: A system to evaluate social bots, in: Proceedings of the
25th international conference companion on world wide web, pp.
273–274.
[20] Del Vicario, M., Bessi, A., Zollo, F., Petroni, F., Scala, A.,
Caldarelli, G., Stanley, H.E., Quattrociocchi, W., 2016. The
spreading of misinformation online.
Proceedings of the National Academy of Sciences of the United States of America
113, 554–559. URL: www.pnas.org/cgi/doi/10.1073/pnas.1517441113,
doi:10.1073/pnas.1517441113.
[21] Evon, D., 2020. Was Coronavirus Predicted in a 1981 Dean
Koontz Novel?
URL: https://www.snopes.com/fact-check/dean-

Page 17 of 20

COVID-19 Misinformation on Twitter
koontz-predicted-coronavirus/?collection-id=243544.
[22] Factcheck AFP, 2020.
Madagascan president has not called
on african states to quit who. URL: https://factcheck.afp.com/
madagascan-president-has-not-called-african-states-quit-who.
[23] Factnameh, 2020.
Did the clergy agree or oppose the closure of religious sites due to the spread of the corona? (translated).
URL: https://factnameh.com/fact-checks/2020-03-11religious-sites-coronavirus.html.
[24] Ferrara, E., 2020. #COVID-19 on Twitter: Bots, Conspiracies and
Social Media Activism. preprint doi:10.1080/09668136.2012.671567.3.
[25] Friggeri, A., Adamic, L.A., Eckles, D., Cheng, J., 2014. Rumor cascades. Proceedings of the 8th International Conference on Weblogs
and Social Media, ICWSM 2014 , 101–110.
[26] Gallotti, R., Valle, F., Castaldo, N., Sacco, P., De Domenico, M.,
2020. Assessing the risks of "infodemics" in response to COVID19 epidemics. preprint URL: http://arxiv.org/abs/2004.03997,
arXiv:2004.03997.
[27] Garrett, L., 2020. COVID-19: the medium is the message. The
Lancet 395, 942–943. doi:10.1016/s0140-6736(20/30600-0.
[28] Goel, S., Anderson, A., Hofman, J., Watts, D.J., 2016. The structural virality of online diffusion. Management Science 62, 180–196.
doi:10.1287/mnsc.2015.2158.
[29] Grimes, C., Sakurai, M., Latinos, V., Majchrzak, T.A., 2017. Cocreating communication approaches for resilient cities in europe: the
case of the eu project smr, in: Comes, T., BÃľnaben, F. (Eds.), 14th
Proceedings of the International Conference on Information Systems
for Crisis Response and Management (ISCRAM)), ISCRAM Association. pp. 353–362.
[30] Grinberg, N., Joseph, K., Friedland, L., Swire-Thompson, B.,
Lazer, D., 2019. Political science: Fake news on Twitter during the 2016 U.S. presidential election. Science 363, 374–378.
URL: https://science.sciencemag.org/content/363/6425/374https:
//science.sciencemag.org/content/363/6425/374.abstract,
doi:10.1126/science.aau2706.
[31] Hannak, A., Margolin, D., Keegan, B., Weber, I., 2014. Get Back!
You Don’t Know Me Like That: The Social Mediation of Fact
Checking Interventions in Twitter Conversations, in: Proceedings
of the eighth international AAAI conference on weblogs and social
media., pp. 187–196. URL: www.aaai.org.
[32] Hernon, P., 1995. Disinformation and misinformation through the
internet: Findings of an exploratory study. Government information
quarterly 12, 133–139.
[33] Herring, S.C., Dainas, A.R., 2020. Gender and Age Influences on Interpretation of Emoji Functions. ACM Trans. Soc. Comput 3. URL:
https://doi.org/10.1145/3375629, doi:10.1145/3375629.
[34] Hinton, N., 2020. A Dean Koontz novel [...]. URL: https://
twitter.com/NickHintonn/status/1228896027987660800.
[35] Jiang, S., Wilson, C., 2018. Linguistic signals under misinformation and fact-checking: Evidence from user comments on social media. Proceedings of the ACM on Human-Computer Interaction 2.
doi:10.1145/3274351.
[36] Kelly, R., Watts, L., 2015. Characterising the inventive appropriation of emoji as relationally meaningful in mediated close personal
relationships. Experiences of technology appropriation: unanticipated users, usage, circumstances, and design 20. Experiences of
Technology Appropriation: Unanticipated Users, Usage, Circumstances, and Design ; Conference date: 20-09-2015 Through 20-092015.
[37] Kim, T., Wurster, K., 2020. Emoji 0.5.4. URL: https://pypi.org/
project/emoji/.
[38] Kleinberg, B., van der Vegt, I., Mozes, M., 2020. Measuring Emotions in the COVID-19 Real World Worry Dataset. preprint URL:
http://arxiv.org/abs/2004.04225, arXiv:2004.04225.
[39] Kouzy, R., Abi Jaoude, J., Kraitem, A., El Alam, M.B., Karam, B.,
Adib, E., Zarka, J., Traboulsi, C., Akl, E., Baddour, K., 2020. Coronavirus Goes Viral: Quantifying the COVID-19 Misinformation Epidemic on Twitter. Cureus 12, e7255. doi:10.7759/cureus.7255.
[40] Lazer, D.M.J., Baum, M.A., Benkler, Y., Berinsky, A.J., Green-

Shahi et al.: Preprint submitted to Elsevier

hill, K.M., Menczer, F., Metzger, M.J., Nyhan, B., Pennycook,
G., Rothschild, D., Schudson, M., Sloman, S.A., Sunstein,
C.R., Thorson, E.A., Watts, D.J., Zittrain, J.L., 2018. The science
of fake news. Science 359, 1094–1096. doi:10.1126/science.aao2998,

arXiv:https://science.sciencemag.org/content/359/6380/1094.full.pdf.

[41] Lewandowsky, S., Ecker, U.K., Seifert, C.M., Schwarz, N., Cook,
J., 2012. Misinformation and Its Correction: Continued Influence and Successful Debiasing. Psychological Science in the Public Interest. Psychological Science in the Public Interest, Supplement 13, 106–131. URL: http://journals.sagepub.com/doi/10.1177/
1529100612451018, doi:10.1177/1529100612451018.
[42] Li, L., Song, Z., Zhang, X., Fox, E.A., 2018. A hybrid model
for role-related user classification on twitter.
arXiv preprint
arXiv:1811.10202 .
[43] Lim, C., 2018. Checking how fact-checkers check. Research and
Politics 5. doi:10.1177/2053168018786848.
[44] Listhaug, O., Ringdal, K., 2008. Trust in political institutions.
Nordic social attitudes in a European perspective , 131–151.
[45] Liu, Y., Gayle, A.A., Wilder-Smith, A., Rocklöv, J., 2020. The reproductive number of covid-19 is higher compared to sars coronavirus.
Journal of travel medicine .
[46] Loper, E., Bird, S., 2002. Nltk: the natural language toolkit. arXiv
preprint cs/0205028 .
[47] Majchrzak, T.A., Sakurai, M., Serrano, N., 2018. Conceptualizing
and designing a resilience information portal, in: Proceedings 51th
Hawaii International Conference on Systems Science (HICSS-51),
AIS Electronic Library (AISeL). pp. 45–54.
[48] Marana, P., Eden, C., Eriksson, H., Grimes, C., Hernantes, J., Howick, S., Labaka, L., Latinos, V., Lindner, R., Majchrzak, T.A., Pyrko,
I., Radianti, J., Rankin, A., Sakurai, M., Sarriegi, J., Serrano, N.,
2019. Towards a resilience management guideline – cities as a starting point for societal resilience. Sustainable Cities and Society 48.
doi:10.1016/j.scs.2019.101531.
[49] Mendoza, M., Poblete, B., Castillo, C., 2010. Twitter under crisis:
Can we trust what we RT?, in: SOMA 2010 - Proceedings of the
1st Workshop on Social Media Analytics, pp. 71–79. doi:10.1145/
1964858.1964869.
[50] Miller, H., Thebault-Spieker, J., Chang, S., Johnson, I., Terveen, L.,
Hecht, B., 2016. "blissfully happy" or "ready to fight": Varying
interpretations of emoji. Proceedings of the 10th International Conference on Web and Social Media, ICWSM 2016 , 259–268.
[51] Miller, M., Banerjee, T., Muppalla, R., Romine, W., Sheth, A., 2017.
What are people tweeting about zika? an exploratory study concerning its symptoms, treatment, transmission, and prevention. JMIR
public health and surveillance 3, e38. doi:10.2196/publichealth.7157.
[52] Monti, F., Frasca, F., Eynard, D., Mannion, D., Bronstein, M.M.,
Ai, F., Lugano, U., 2019. Fake News Detection on Social Media using Geometric Deep Learning. ArXiv preprint URL: https:
//www.snopes.com/, arXiv:1902.06673v1.
[53] Officer Bandit, 2020. I feel like this is the video that [...]. URL:
https://twitter.com/OfficerBandit/status/1238215769445388288.
[54] Ortiz-Martínez, Y., Jiménez-Arcia, L.F., 2017. Yellow fever outbreaks and Twitter: Rumors and misinformation. American Journal
of Infection Control 45, 816–817. doi:10.1016/j.ajic.2017.02.027.
[55] Oyeyemi, S.O., Gabarron, E., Wynn, R., 2014. Ebola, Twitter, and
misinformation: A dangerous combination? doi:10.1136/bmj.g6178.
[56] Pennycook, G., Cannon, T.D., Rand, D.G., 2018. Prior exposure
increases perceived accuracy of fake news. Journal of experimental
psychology: general 147, 1865.
[57] Pennycook, G., Rand, D.G., 2020. Who falls for fake news?
The roles of bullshit receptivity, overclaiming, familiarity, and analytic thinking. Journal of Personality 88, 185–200. doi:10.1111/
jopy.12476.
[58] Pesacheck, . Faux: JÃľrÃťme munyangi nâĂŹa pas dÃľcouvert
le covid-organics.
URL: https://pesacheck.org/faux-j%C3%A9r%
C3%B4me-munyangi-na-pas-d%C3%A9couvert-le-covid-organics53120ff5dfad.

[59] Pesacheck, 2020. HOAX: Reports that a Chinese woman was killed

Page 18 of 20

COVID-19 Misinformation on Twitter
[...]. URL: https://pesacheck.org/hoax-reports-that-a-chinesewoman-was-killed-in-mombasa-over-covid-19-fears-are-falsec7d6913ce5ae.

[60] Pierri, F., Piccardi, C., Ceri, S., 2020. Topology comparison of Twitter diffusion networks effectively reveals misleading information.
Scientific Reports 10, 1–9. URL: www.nature.com/scientificreports,
doi:10.1038/s41598-020-58166-5, arXiv:1905.03043.
[61] Poynter Institute, 2019. HereâĂŹs what to expect from fact-checking
in 2019. URL: https://www.poynter.org/fact-checking/2018/hereswhat-to-expect-from-fact-checking-in-2019/.
[62] Poynter Institute, 2020. The International Fact-Checking Network.
URL: https://www.poynter.org/ifcn/.
[63] Procter, R., Vis, F., Voss, A., 2013.
Reading the riots
on Twitter: methodological innovation for the analysis of big
data.
International Journal of Social Research Methodology
16, 197–214. URL: http://www.tandfonline.com/doi/abs/10.1080/
13645579.2013.774172, doi:10.1080/13645579.2013.774172.
[64] Rahman, M.T., Comes, T., Majchrzak, T.A., 2019. Deep uncertainty
in humanitarian logistics operations: Decision-making challenges in
responding to large-scale natural disasters. International Journal of
Emergency Management 15, 276–297.
[65] Richardson, L., 2007. Beautiful soup documentation. April .
[66] Roesslein, J., 2020.
http://www.tweepy.org.
URL: http://
www.tweepy.org.
[67] Roozenbeek, J., Van Der Linden, S., 2019. The fake news game:
actively inoculating against the risk of misinformation. Journal of
Risk Research 22, 570–580.
[68] Rosenfeld, N., Szanto, A., Parkes, D.C., 2020. A Kernel of
Truth: Determining Rumor Veracity on Twitter by Diffusion Pattern Alone, in: The Web Conference (WWW) 2020, pp. 1018–
1028. URL: https://doi.org/10.1145/3366423.3380180, doi:10.1145/
3366423.3380180, arXiv:2002.00850v2.
[69] Serrano, E., Iglesias, C.A., Garijo, M., 2015. A survey of twitter
rumor spreading simulations, in: Computational Collective Intelligence. Springer, pp. 113–122.
[70] Shahi, G.K., Nandini, D., 2020. Fakecovid–a multilingual crossdomain fact check news dataset for covid-19. arXiv preprint
arXiv:2006.11343 .
[71] Shao, C., Ciampaglia, G.L., Varol, O., Flammini, A., Menczer, F.,
2018a. The spread of low-credibility content by social bots. Nature
Communications 9, 4787. doi:10.1038/s41467-018-06930-7.
[72] Shao, C., Hui, P.M., Wang, L., Jiang, X., Flammini, A., Menczer, F.,
Luca Ciampaglia, G., 2018b. Anatomy of an online misinformation
network. PLoS ONE 13, e0196087. URL: https://doi.org/10.1371/
journal.pone.0196087, doi:10.1371/journal.pone.0196087.
[73] Singh, L., Bansal, S., Bode, L., Budak, C., Chi, G., Kawintiranon, K., Padden, C., Vanarsdall, R., Vraga, E., Wang, Y., 2020.
A first look at COVID-19 information and misinformation sharing on Twitter. preprint URL: http://arxiv.org/abs/2003.13907,
arXiv:2003.13907.
[74] Snopes, 2020.
Collections archive.
URL: www.snopes.com/
collections.
[75] Snopes, 2020. Did Costco Issue a Recall Notice for Toilet Paper? URL: https://www.snopes.com/fact-check/costco-bath-tissuerecall/?collection-id=242229.
[76] Sohrabi, C., Alsafi, Z., OâĂŹNeill, N., Khan, M., Kerwan, A., AlJabir, A., Iosifidis, C., Agha, R., 2020. World health organization
declares global emergency: A review of the 2019 novel coronavirus
(covid-19). International Journal of Surgery .
[77] Sook Kwon, E., Kim, E., Sung, Y., Yun Yoo, C., 2014. Brand followers: Consumer motivation and attitude towards brand communications on twitter. International Journal of Advertising 33, 657–680.
[78] Stieglitz, S., Dang-Xuan, L., 2013. Emotions and information diffusion in social mediaâĂŤsentiment of microblogs and sharing behavior. Journal of management information systems 29, 217–248.
[79] Summers, E., van Kemenade, H., Binkley, P., Ruest, N., recrm,
Costa, S., Phetteplace, E., Badger, T.G., Matienzo, M.A., Blakk,
L., Chudnov, D., Nelson, C., 2015. Twarc: v0.3.4. URL: doi:

Shahi et al.: Preprint submitted to Elsevier

10.5281/zenodo.31919.
[80] Tausczik, Y.R., Pennebaker, J.W., 2010. The psychological meaning
of words: Liwc and computerized text analysis methods. Journal of
language and social psychology 29, 24–54.
[81] The Guardian, 2020a.
Disinformation and blame: how
America’s far right is capitalizing on coronavirus.
URL:
https://www.theguardian.com/world/2020/mar/19/america-farright-coronavirus-outbreak-trump-alex-jones.
[82] The Guardian, 2020b. The WHO v coronavirus: why it can’t handle
the pandemic. URL: https://www.theguardian.com/news/2020/apr/
10/world-health-organization-who-v-coronavirus-why-it-canthandle-pandemic.

[83] Theocharis, Y., BarberÃą, P., Fazekas, Z., Popa, S.A., Parnet,
O., 2016. A Bad Workman Blames His Tweets: The Consequences of Citizens’ Uncivil Twitter Use When Interacting With
Party Candidates. Journal of Communication 66, 1007–1031.
URL: https://doi.org/10.1111/jcom.12259, doi:10.1111/jcom.12259,

arXiv:https://academic.oup.com/joc/article-pdf/66/6/1007/22321332/jjnlcom1007.p

[84] Timesnownews, 2020. Has RussiaâĂŹs Putin released lions on
streets [...]. URL: https://www.timesnownews.com/international/
article/has-russia-s-putin-released-lions-on-streets-to-keeppeople-indoors-amid-coronavirus-scare-fake-news-alert/568251.

[85] Tomokiyo, T., Hurst, M., 2003. A language model approach to
keyphrase extraction, in: Proceedings of the ACL 2003 Workshop
on Multiword Expressions: Analysis, Acquisition and Treatment,
Association for Computational Linguistics, Sapporo, Japan. pp. 33–
40. URL: https://www.aclweb.org/anthology/W03-1805, doi:10.3115/
1119282.1119287.
[86] Twitter, 2020.
COVID-19 stream.
URL: https:
//developer.twitter.com/en/docs/labs/covid19-stream/overview.
[87] Various authors, 2020.
Doctors sound alarm over social
media infodemic. URL: https://secure.avaaz.org/campaign/en/
health_disinfo_letter/.
[88] Veil, S.R., Buehner, T., Palenchar, M.J., 2011. A work-in-process
literature review: Incorporating social media in risk and crisis communication. Journal of contingencies and crisis management 19,
110–122.
[89] Verberne, S., Sappelli, M., Hiemstra, D., Kraaij, W., 2016. Evaluation and analysis of term scoring methods for term extraction. Inf
Retrieval J 19, 510–545. doi:10.1007/s10791-016-9286-2.
[90] Vosoughi, S., Roy, D., Aral, S., 2018. The spread of true and false
news online. Science 359, 1146–1151. doi:10.1126/science.aap9559.
[91] Wardle, C., Derakhshan, H., 2017. Information Disorder: Toward
an interdisciplinary framework for research and policy making Information Disorder Toward an interdisciplinary framework for research
and policymaking. Technical Report. Council of Europe. URL:
www.coe.int.
[92] WHO, 2020. Coronavirus disease (COVID-19) advice for the public: Myth busters. URL: https://www.who.int/emergencies/diseases/
novel-coronavirus-2019/advice-for-public/myth-busters.
[93] World Health Organization and others, 2020. Coronavirus disease
2019 (COVID-19): situation report, 72. Technical Report. World
Health Organization.
[94] Wukich, C., et al., 2015. Social media use in emergency management. Journal of Emergency Management 13, 281–294.
[95] Xu, S., Li, Y., 2020. Beware of the second wave of COVID-19. The
Lancet .
[96] Yang, K.C., Torres-Lugo, C., Menczer, F., 2020.
Prevalence of Low-Credibility Information on Twitter During the
COVID-19 Outbreak. ArXiv preprint URL: http://arxiv.org/abs/
2004.14484http://dx.doi.org/10.36190/2020.16, doi:10.36190/2020.16,
arXiv:2004.14484.
[97] Zarocostas, J., 2020. World Report How to fight an infodemic. The
Lancet 395, 676. doi:10.1016/S0140-6736(20/30461-X.
[98] Zhang, S., Diao, M., Yu, W., Pei, L., Lin, Z., Chen, D., 2020. Estimation of the reproductive number of novel coronavirus (covid-19)
and the probable outbreak size on the diamond princess cruise ship:
A data-driven analysis. International Journal of Infectious Diseases

Page 19 of 20

COVID-19 Misinformation on Twitter
93, 201–204.
[99] Zollo, F., Bessi, A., Del Vicario, M., Scala, A., Caldarelli, G., Shekhtman, L., Havlin, S., Quattrociocchi, W.,
2017.
Debunking in a world of tribes.
PLOS ONE 12,
e0181821. URL: https://dx.plos.org/10.1371/journal.pone.0181821,
doi:10.1371/journal.pone.0181821.
[100] Zubiaga, A., Aker, A., Bontcheva, K., Liakata, M., Procter, R.,
2017. Detection and Resolution of Rumours in Social Media: A Survey. preprint 51. URL: http://arxiv.org/abs/1704.00656{%}0Ahttp://
dx.doi.org/10.1145/3161603, doi:10.1145/3161603, arXiv:1704.00656.
[101] Zubiaga, A., Liakata, M., Procter, R., Wong, G., Hoi, S., Tolmie,
P., 2016. Analysing How People Orient to and Spread Rumours in Social Media by Looking at Conversational Threads.
PLoS ONE 11. URL: https://figshare.com/articles/, doi:10.1371/
journal.pone.0150989.

Author biographies
Gautam Kishore Shahi is a PhD student in the Research Training Group User-Centred Social Media
at the research group for Professional Communication in Electronic Media/Social Media(PROCO) at
the University of Duisburg-Essen, Germany. His
research interests are Web Science, Data Science,
and Social Media Analytics. He has a background
in computer science, where he has gained valuable
insights in India, New Zealand, Italy and now Germany. Gautam received a Master’s degree from
the University of Trento, Italy and Bachelor Degree
from BIT Sindri, India. Outside of academia, He
worked as an Assistant System Engineer for Tata
Consultancy Services in India.

Anne Dirkson is a PhD student at the Leiden Institute of Advanced Computer Science (LIACS) of
the University of Leiden, the Netherlands. Her
PhD focuses on knowledge discovery from healthrelated social media and aims to empower patients
by automatically extracting information about their
quality of life and knowledge that they have gained
from experience from their online conversations.
Her research interests include natural language
processing, text mining and social media analytics.
Anne received a BA in Liberal Arts and Sciences
at the University College Maastricht of Maastricht
University and a MSc degree in Neuroscience at
the Vrije Universiteit, Amsterdam.

Tim A. Majchrzak is professor in Information Systems at the University of Agder (UiA) in Kristiansand, Norway. He also is a member of the Centre for Integrated Emergency Management (CIEM)
at UiA. Tim received BSc and MSc degrees in Information Systems and a PhD in economics (Dr.
rer. pol.) from the University of MÃĳnster,
Germany. His research comprises both technical
and organizational aspects of Software Engineering, typically in the context of Mobile Computing. He has also published work on diverse interdisciplinary Information Systems topics, most notably targeting Crisis Prevention and Management.
Tim’s research projects typically have an interface
to industry and society. He is a senior member of
the IEEE and the IEEE Computer Society, and a
member of the Gesellschaft fÃĳr Informatik e.V.

Shahi et al.: Preprint submitted to Elsevier

Page 20 of 20

