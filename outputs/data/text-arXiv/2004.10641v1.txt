Automatic Detection of Coronavirus Disease (COVID-19) in
X-ray and CT Images: A Machine Learning Based Approach
Sara Hosseinzadeh Kassania,∗, Peyman Hosseinzadeh Kassasnib , Michal J. Wesolowskic ,
Kevin A. Schneidera , Ralph Detersa
a Department

of Computer Science, University of Saskatchewan, Saskatchewan, Canada
of Neurology and Neurological, University of Stanford, California, United States
c Department of Medical Imaging, University of Saskatchewan, Saskatchewan, Canada

arXiv:2004.10641v1 [eess.IV] 22 Apr 2020

b Department

Abstract
The newly identified Coronavirus pneumonia, subsequently termed COVID-19, is highly transmittable and pathogenic with no clinically approved antiviral drug or vaccine available for
treatment. The most common symptoms of COVID-19 are dry cough, sore throat, and fever.
Symptoms can progress to a severe form of pneumonia with critical complications, including
septic shock, pulmonary edema, acute respiratory distress syndrome and multi-organ failure.
While medical imaging is not currently recommended in Canada for primary diagnosis of
COVID-19, computer-aided diagnosis systems could assist in the early detection of COVID-19
abnormalities and help to monitor the progression of the disease, potentially reduce mortality
rates. In this study, we compare popular deep learning-based feature extraction frameworks for
automatic COVID-19 classification. To obtain the most accurate feature, which is an essential
component of learning, MobileNet, DenseNet, Xception, ResNet, InceptionV3, InceptionResNetV2, VGGNet, NASNet were chosen amongst a pool of deep convolutional neural networks.
The extracted features were then fed into several machine learning classifiers to classify subjects as either a case of COVID-19 or a control. This approach avoided task-specific data
pre-processing methods to support a better generalization ability for unseen data. The performance of the proposed method was validated on a publicly available COVID-19 dataset of
chest X-ray and CT images. The DenseNet121 feature extractor with Bagging tree classifier
achieved the best performance with 99% classification accuracy. The second-best learner was
a hybrid of the a ResNet50 feature extractor trained by LightGBM with an accuracy of 98%.
Keywords: Coronavirus Disease, Lung Opacity, Computer-Aided Diagnosis, Deep Learning,
Feature Extraction, Transfer Learning

1. Introduction
A series of pneumonia cases of unknown etiology occurred in December 2019, in Wuhan,
Hubei province, China. On December 31, 2019, 27 unexplained cases of pneumonia were
identified and found to be associated with so called “wet markets” which sell fresh meat and
seafood from a variety of animals including bats and pangolins. The pneumonia was found to
be caused by a virus identified as ”severe acute respiratory syndrome coronavirus 2” (SARSCoV-2), with the associated disease subsequently termed coronavirus disease 2019 (COVID-19)
∗ Corresponding

author.
Email addresses: sara.kassani@usask.ca (Sara Hosseinzadeh Kassani), peymanhk@stanford.edu
(Peyman Hosseinzadeh Kassasni)

1

by the World Health Organization (WHO) [1] [2]. Genomic analysis showed that COVID-19
is phylogenetically related to SARS-like bat viruses. Hence, bats could be the possible source
of the viral replication [1]. Pangolins have also been identified as a potential intermediate
host of COVID-19 [3]. This newly identified virus is highly transmittable and pathogenically
different from SARS-CoV, MERS-CoV, avian influenza, influenza, and other common respiratory viruses. Concerning the outbreak of COVID-19, on January 30, 2020, the WHO declared
the outbreak of the novel Coronavirus disease as a Public Health Emergency of International
Concern (PHEIC) [4]. The rapid worldwide spread of disease resulted in a global pandemic declaration on March 11, 2020. Clinical symptoms of patients infected with COVID-19 are similar
to other viral upper respiratory diseases such as Influenza, respiratory syncytial virus (RSV),
and bacterial pneumonia. The most common presenting symptoms experienced by patients
include dry cough, sore throat, fever, dyspnea, diarrhea, myalgia, shortness of breath and bilateral lung infiltrates, observable on clinical imaging such as chest X-ray. Other symptoms are
headache, vomiting, pleurisy, sneezing, rhinorrhea, and nasal congestion. Patients with more
severe COVID-19 have developed critical complications, including septic shock, pulmonary
edema, cardiac injury, acute kidney injury, Acute Respiratory Distress Syndrome (ARDS) and
even Multi-Organ Failure (MOF) [4] [5]. At present, there is no clinically approved antiviral
drug or vaccine available to treat COVID-19. The reproduction number (R0), defined as the
expected number of susceptible cases directly generated by one infectious case of COVID-19
infection, is estimated to 3.77 [6] [7]. Despite global efforts of travel restrictions and quarantine,
while the epidemic continues to decline in China, the incidence of novel COVID-19 continues
to rise globally, with over 1.6 million confirmed cases and over 100,000 deaths worldwide, at
the time of this writing [8]. As of April 2020, substantial new incidence of COVID-19 cases
have been reported in 211 countries with significant confirmed cases in South Korea, Italy,
Iran, Japan, Germany, and France [9]. The early spread of new COVID-19 cases was associated with recent travel to China; however, community spread is now common globally. The
greatest number of new cases occur through close contact human-to-human transmission (approximately 6 feet) by respiratory droplets [5]. Contamination also can occur through infected
surfaces with subsequent contact with the eyes, nose, or mouth.
The genetic characteristics of the Coronavirus should be well understood to fight against
this virus. Coronavirus is a single-stranded RNA virus consisting of approximately 27–32
kb with particle size ranged from 65-125nm in diameter [1]. An illustration of COVID-19 is
shown in Figure 1. A transmission electron microscopic image of a case of COVID-19 is also
demonstrated in Figure 2.
In light of this, it is evident that early detection of COVID-19 is necessary to interrupt
the spread of COVID-19 and prevent transmission by early isolation of patients, trace and
quarantine of close contacts. In patients with COVID-19, accurate monitoring of the disease progression is a critical component of disease management. While not recommended for
primary diagnosis of COVID-19 in Canada, medical imaging modalities such as chest X-ray
and Computed Tomography (CT) play an important role in confirming diagnosis of positive
COVID-19 pneumonia as well as monitoring the progression of the disease. These types of
images show an extent of irregular ground-glass opacities that progress rapidly after COVID19 symptom onset. These abnormalities peaked during days 6-11 of the illness. The second
most predominant pattern of lung opacity abnormalities peak during days 12-17 of the illness [12]. Computer-Aided Diagnosis (CAD) systems that incorporate X-ray and CT image

2

Figure 1: The illustration of COVID-19, created at the Centers for Disease Control and Prevention (CDC) [10].
The protein particles E, S, and M are located on the outer surface of the virus particle.

Figure 2: Transmission electron microscopic image of a case of COVID-19. The spherical viral particles,
colorized blue, contain cross-sections through the viral genome, seen as black dots [11].

processing techniques and deep learning algorithms could assist physicians as diagnostic aides
for COVID-19 and help provide a better understanding of the progression the disease.
1.1. Related Research
Hemdan et al. [13] developed a deep learning framework, COVIDX-Net, to diagnose COVID19 in X-Ray Images. A comparative study of different deep learning architectures including VGG19, DenseNet201, ResNetV2, InceptionV3, InceptionResNetV2, Xception and MobileNetV2 is provided by authors. The public dataset of X-ray images was provided by Dr.
Joseph Cohen [14] and Dr. Adrian Rosebrock [15]. The provided dataset included 50 X-ray
images, divided into two classes as 25 normal cases and 25 positive COVID-19 images. Hemdan’s results demonstrated VGG19 and DenseNet201 models achieved the best performance
scores among counterparts with 90.00% accuracy.
Barstugan et al. [16] proposed a machine learning approach for COVID-19 classification
from CT images. Patches with different sizes 16×16, 32×32, 48×48, 64×64 were extracted
from 150 CT images. Different hand-crafted features such as Grey Level Co-occurrence Matrix
(GLCM), Local Directional Pattern (LDP), Grey Level Run Length Matrix (GLRLM), GreyLevel Size Zone Matrix (GLSZM), and Discrete Wavelet Transform (DWT) algorithms were
3

employed. The extracted features were fed into a Support Vector Machine (SVM) [17] classifier
on 2-fold, 5-fold and 10-fold cross-validations. The best accuracy of 98.77% was obtained by
GLSZM feature extractor with 10-fold cross-validation.
Wang and Wong [18] designed a tailored deep learning-based framework, COVID-Net,
developed for COVID-19 detection from chest X-ray images. The COVID-Net architecture
was constructed of combination of 1×1 convolutions, depth-wise convolution and the residual
modules to enable design deeper architecture and avoid the gradient vanishing problem. The
provided dataset consisted of s a combination of COVID chest X-ray dataset provided by Dr.
Joseph Cohen [14], and Kaggle chest X-ray images dataset [19] for a multi-class classification of
normal, bacterial infection, viral infection (non-COVID) and COVID-19 infection. Obtained
accuracy of this study was 83.5%.
In a study conducted by Maghdid et al. [20], a deep learning-based method and transfer
learning strategy were used for automatic diagnosis of COVID-19 pneumonia. The proposed
architecture is a combination of a simple convolutional neural network (CNN) architecture (one
convolutional layer with 16 filters followed by batch normalization, rectified linear unit (ReLU),
two fully-connected layers) and a modified AlexNet [21] architecture with the feasibility of
transfer learning. The proposed modified architecture achieved an accuracy of 94.00%.
Ghoshal and Tucker [22] investigated the diagnostic uncertainty and interpretability of deep
learning-based methods for COVID-19 detection in X-ray images. Dropweights based Bayesian
Convolutional Neural Networks (BCNN) were used to estimate uncertainty in deep learning
solutions and provide a level of confidence of a computer-based diagnosis for a trusted clinician
setting. To measure the relationship between accuracy and uncertainty, 70 posterioranterior
(PA) lung X-ray images of COVID-19 positive patients from the public dataset provided by Dr.
Joseph Cohen [14] were selected and balanced by Kaggle’s Chest X-Ray Images dataset [19].
To prepare the dataset, all images were resized to 512×512 pixels. A transfer learning strategy
and real-time data augmentation strategies were employed to overcome the limited size of the
dataset. The proposed Bayesian inference approach obtained the detection accuracy of 92.86%
on X-ray images using VGG16 deep learning model.
Hall et al. [23] used a VGG16 architecture and transfer learning strategy with 10-fold crossvalidation trained on the dataset from Dr. Joseph Cohen [14]. All images were rescaled to
224×224 pixels and a data augmentation strategy was employed to increase the size of dataset.
The proposed approach achieved an overall accuracy 96.1% and overall Area Under Curve
(AUC) of 99.70% on the provided dataset.
Farooq and Hafeez [24] proposed a fine-tuned and pre-trained ResNet-50 architecture,
COVID-ResNet, for COVID-19 pneumonia screening. To improve the generalization of the
training model, different data augmentation methods including vertical flip, random rotation
(with angle of 15 degree), along with the model regularization were used. The proposed method
achieved the accuracy of 96.23% on a multi-class classification of normal, bacterial infection,
viral infection (non-COVID-19) and COVID-19 infection dataset.
1.2. Motivation and contributions
The main motivation of this study is to present a generic feature extraction method using
convolutional neural networks that does not require handcrafted or very complex features from
input data while being easily applied to different modalities such as X-ray and CT images.
Another primary goal is to reduce the generalization error while achieving a more accurate
diagnosis. The contributions are summarized as follows:
4

• Deep convolutional feature representation [25, 26, 27] is used to extract highly representative features using state-of-the-art deep CNN descriptors. The employed approach is
able to discriminate between COVID-19 and healthy subjects from chest X-ray and CT
images and hence produce higher accuracy in comparison to other works presented in the
literature. To the best of our knowledge, this research is the first comprehensive study
of the application of machine learning (ML) algorithms (15 deep CNN visual feature
extractor and 6 ML classifier) for automatic diagnoses of COVID-19 from X-ray and CT
images.
• To overcome the issue of over-fitting in deep learning due to the limited number of training
images, a transfer-learning strategy is adopted as the training of very deep CNN models
from scratch requires a large number of training data.
• No data augmentation or extensive pre-processing methods are applied to the dataset
in order to increase the generalization ability and also reduce bias toward the model
performance.
• The proposed approach reduces the detection time dramatically while achieving satisfactory accuracy, which is a superior advantage for developing real or near real-time
inferences on clinical applications.
• With extensive experiments, we show that the combination of a deep CNN with Bagging
trees classifier achieves very good classification performance applied on COVID-19 data
despite the limited number of image samples.
• Finally, we developed an end to end web-based detection system to simulate a virtual
clinical pipeline and facilitate the screening of suspicious cases.
The rest of this paper is organized as follows. The proposed methodology for automatically
classifying COVID-19 and healthy cases is explained in Section 2. The dataset description,
experimental settings and performance metrics are given in Section 3. A brief discussion and
results analysis are provided in Section 4, and finally, the conclusion is presented in Section 5.
2. Proposed Methodology
Few studies have been published on the application of deep CNN feature descriptors to
X-ray and CT images. Each of the CNN architectures is constructed by different modules and
convolution layers that aid in extracting fundamental and prominent features from a given
input image. Briefly, in the first step, we collect available public chest X-ray and CT images.
In the next step, we pre-processed the provided dataset using standard image normalization
techniques to improve the quality of visual information of the input data. Once input images
are prepared, we fed them into the feature extraction phase with the state-of-the-art CNN descriptors to extract deep features from each input image. For the training phase, the generated
features are then fed into machine learning classifiers such as Decision Tree (DT) [28], Random
Forest (RF) [29], XGBoost [30], AdaBoost [31], Bagging classifier [32] and LightGBM [33].
Finally, the performance of the proposed approach is evaluated on test images.

5

2.1. Feature extraction using transfer learning
The concept of transfer learning has been introduced for solving deep learning problems
arising from insufficiently labeled data, or when the CNN model is too deep and complex.
Aiming to tackle these challenges, studies in a variety computer vision tasks demonstrated the
advantages of transfer learning strategies from an auxiliary domain in improving the detection
rate and performance of a classifier [34] [35] [36]. In a transfer learning strategy, we transfer
the weights already learned on a cross-domain dataset into the current deep learning task
instead of training a model from scratch. With the transfer learning strategy, the deep CNN
can obtain general features from the source dataset that cannot be learned due to the limited
size of the dataset in the current task. Transfer learning strategies have various advantages,
such as avoiding the overfitting issue when the number of training samples is limited, reducing
the computational resources, and also speeding up the convergence of the network [37] [38].
2.2. CNN Descriptor
Effective feature extraction is one of the most important steps toward learning rich and
informative representations from raw input data to provide accurate and robust results. The
small or imbalanced size of the training samples poses a significant challenge for the training
of a deep CNN where data dimensionality is much larger than the number of samples leading
to over-fitting. Although various strategies, e.g. data augmentation [39], transfer learning [40]
and fine-tuning [41], may reduce the problem of insufficient or imbalance training data, the
detection rate of the CNN model may degrade due to the over-fitting issue. Since the overall
performance obtained by a fine-tuning method in the initial experiments for this study was
not significant, we employed a different approach inspired by [25] [26] [27] known as deep
convolutional feature representation. In this method, we used pre-trained well-established
CNN models as a visual feature extractor to encode the input images into a feature vector of
sparse descriptors of low dimensionality. Then the computed encoded feature vectors produced
by CNN architectures are fed into different classifiers, i.e. machine learning algorithms, to yield
the final prediction. This lower dimension vector significantly reduces the risk of over-fitting
and also the training time. Different robust CNN architectures such as MobileNet, DenseNet,
Xception, InceptionV3, InceptionResNetV2, ResNet, VGGNet, NASNet are selected for feature
extraction with the possibility of transfer learning advantage for limited datasets and also their
satisfying performances in different computer vision tasks [42, 43, 44, 45]. Figure 3. illustrates
the visual features extracted by VGGNet architecture from an X-ray image of a COVID-19
positive patient.
3. Experiments
3.1. Dataset description
In order to evaluate the performance of our feature extracting and classifying approach, we
used the public dataset of X-ray images provided by Dr. Joseph Cohen available from a GitHub
repository [14]. We used the available 117 chest X-ray images and 20 CT images (137 images
in total) of COVID-19 positive cases. We also included 117 images of healthy cases of X-ray
images from Kaggle Chest X-Ray Images (Pneumonia) dataset available at [19] and 20 images
of healthy cases of CT images from Kaggle RSNA Pneumonia Detection dataset available
at [46] to balance the dataset with both positive and normal cases. Figure 4 shows examples

6

Figure 3: General framework of the proposed method with VGGNet as feature extractor.

of confirmed COVID-19 images extracted from the provided dataset. The X-ray images of
confirmed COVID-19 infection demonstrate different shapes of “pure ground glass” also known
as hazy lung opacity with irregular linear opacity depending the disease progress [12].

(a)

(c)

(b)

(d)

Figure 4: Chest X-ray images of four confirmed COVID-19 pneumonia. (a) 52-year old female, presenting
diffuse infiltrates in the bilateral lower lungs. (b) 59-year old female, demonstrating right infahilar airspace
opacities. (c) 35-year old male, presenting stable streaky opacities in the lung bases, indicating likely atypical
pneumonia; the opacities have steadily increased in density over time. (d) 42-year old male, presenting opacities
in the left lower and right upper lobes on day 7 after the onset of symptoms.

3.2. Data pre-processing
The images within the dataset were collected from multiple imaging clinics with different
equipment and image acquisition parameters; therefore, considerable variations exist in images’
intensity. The proposed method in this study avoids extensive pre-processing steps to improve
the generalization ability of the CNN architecture. This helps to make the model more robust
to noise, artifacts and variations in input images during feature extraction phase. Hence, we
only employed two standard pre-processing steps in training deep learning models to optimize
the training process.
• Resizing: The images in this dataset vary in resolution and dimension, ranging from
365×465 to 1125×859 pixels; therefore, we re-scaled all images of the original size to the
size of 600×450 pixels to obtain a consistent dimension for all input images. The input
images were also separately resized to 331×331 pixels and 224×224 pixels as required for
NASNetLarge and NASNetMobile architectures, respectively.
7

• Image normalization: For image normalization, first, we re-scaled the intensity values
of the pixels using ImageNet mean subtraction as a pre-processing step. The ImageNet
mean is a pre-computed constant derived from the ImageNet database [21]. Another
essential pre-process step is intensity normalization. To accomplish this, we normalized
the intensity values of all images from [0, 255] to the standard normal distribution by
min-max normalization to the intensity range of [0, 1], which is computed as:

xnorm =

x − xmin
xmax − xmin

(1)

where x is the pixel intensity. xmin and xmax are minimum and maximum intensity values
of the input image in equation 1. This operation helps to speed up the convergence of the
model by removing the bias from the features and achieve a uniform distribution across the
dataset.
3.3. Evaluation criteria
To measure the prediction performance of the methods in this study, we utilized common
evaluation metrics such as recall, precision, accuracy and f1-score. According to equations (2–5)
True positive (TP) is the number of instances that correctly predicted; false negative (FN) is the
number of instances that incorrectly predicted. True negative (TN) is the number of negative
instances that predicted correctly, while false positive (FP) is the number of negative instances
incorrectly predicted. Given TP, TN, FP and FN, all evaluation metrics were calculated as
follows:
Recall or sensitivity is the measure of COVID-19 cases that are correctly classified. Recall
is critical, especially in the medical field and is given by:
TP
(2)
TP + FN
Precision or positive predictive value is defined as the percentage of correctly classified
labels in truly positive patients and is given as:
Recall =

TP
(3)
TP + FP
Accuracy shows the number of correctly classified cases divided by the total number of test
images, and is defined as:
P recision =

TP + TN
(4)
TP + TN + FP + FN
F1-score, also known as F-measure, is defined as the weighted average of precision and
recall that combines both the precision and recall together. F-measure is expressed as:
Accuracy =

F 1 − score = 2 ×

Recall × P recision
Recall + P recision

(5)

4. Discussion
Diagnostic imaging modalities, such as chest radiography and CT are playing an important
role in confirming the primary diagnosis from the Polymerase Chain Reaction (PCR) test for
COVID-19. Medical imaging is also playing a critical in monitoring the progression of the

8

disease and patient care. Extracting features from radiology modalities is an essential step in
training machine learning models since the model performance directly depends on the quality
of extracted features. Motivated by the success of deep learning models in computer vision,
the focus of this research is to provide an extensive comprehensive study on the classification
of COVID-19 pneumonia in chest X-ray and CT imaging using features extracted by the stateof-the-art deep CNN architectures and trained on machine learning algorithms. The 10-fold
cross-validation technique was adopted to evaluate the average generalization performance of
the classifiers in each experiment. For all CNNs, the network weights were initialized from
the weights trained on ImageNet. The Windows based computer system used for this work
had an Intel(R) Core(TM) i7-8700K 3.7 GHz processors with 32 GB RAM. The training and
testing process of the proposed architecture for this experiment was implemented in Python
using Keras package with Tensorflow backend as the deep learning framework backend and run
on Nvidia GeForce GTX 1080 Ti GPU with 11GB RAM.
Table 1: Comparison of classification performance (µ ± σ) of different machine learning models measured by
accuracy. The bold value indicates the best result; underlined value represents the second-best result of the
respective category.
Decision Tree

Random Forest

XGBoost

AdaBoost

Bagging

LightGBM

MobileNet

83.00 ± 0.26

93.00 ± 0.23

95.00 ± 0.16

80.00 ± 0.17

96.00 ± -0.11

82.00 ± 0.28

DesnseNet121

92.00 ± 0.15

90.00 ± 0.21

94.00 ± 0.16

92.00 ± 0.19

99.00 ± 0.07

96.00 ± 0.11

DenseNet201

84.00 ± 0.26

90.00 ± 0.24

90.00 ± 0.18

87.00 ± 0.25

96.00 ± 0.11

87.00 ± 0.17

Xception

95.00 ± 0.17

90.00 ± 0.19

96.00 ± 0.11

93.00 ± 0.20

96.00 ± 0.11

96.00 ± 0.11

InceptionV3

82.00 ± 0.22

84.00 ± 0.29

88.00 ± 0.15

80.00 ± 0.12

95.00 ± 0.12

84.00 ± 0.16

InceptionResNetV2

84.00 ± 0.31

93.00 ± 0.16

93.00 ± 0.19

87.00 ± 0.33

94.00 ± 0.12

88.00 ± 0.21

ResNet50

89.00 ± 0.17

90.00 ± 0.15

93.00 ± 0.16

94.00 ± 0.12

93.00 ± 0.16

98.00 ± 0.09

ResNet152

93.00 ± 0.12

92.00 ± 0.16

93.00 ± 0.16

94.00 ± 0.17

91.00 ± 0.22

93.00 ± 0.20

VGG16

90.00 ± 0.19

91.00 ± 0.19

88.00 ± 0.19

90.00 ± 0.19

90.00 ± 0.19

85.00 ± 0.19

VGG19

90.00 ± 0.19

87.00 ± 0.21

88.00 ± 0.19

90.00 ± 0.19

90.00 ± 0.19

85.00 ± 0.25

NASNetLarge

82.00 ± 0.23

88.00 ± 0.19

89.00 ± 0.17

81.00 ± 0.23

93.00 ± 0.19

82.00 ± 0.26

NASNetMobile

87.00 ± 0.17

88.00 ± 0.22

94.00 ± 0.19

87.00 ± 0.17

93.00 ± 0.19

89.00 ± 0.17

ResNet50V2

87.00 ± 0.12

96.00 ± 0.11

92.00 ± 0.19

90.00 ± 0.18

95.00 ± 0.12

88.00 ± 0.10

ResNet101V2

79.00 ± 0.32

89.00 ± 0.24

89.00 ± 0.28

76.00 ± 0.32

95.00 ± 0.12

78.00 ± 0.26

ResNet152V2

90.00 ± 0.27

86.00 ± 0.26

93.00 ± 0.16

89.00 ± 0.20

96.00 ± 0.11

88.00 ± 0.28

Table 1 and Figure 5 summarize the accuracy performance of six machine learning algorithms, namely, DT, RF, XGBoost, AdaBoost, Bagging classifier and LightGBM on the
feature extracted by deep CNNs. Each entry in Table 1, is in the format (µ ± σ) where µ is
the average classification accuracy and σ is standard deviation. Analyzing Table 1 the topmost
result was obtained by Bagging classifier with a maximum of 99.00% ± 0.09 accuracy on features extracted by DesnseNet121 architecture (with feature extraction time of 9.306 seconds
and training time of 30.748 seconds in Table 5), which is the highest result reported in the
literature for COVID-19 classification of this dataset. It is also inferred from Table 1 that
the second-best result obtained by ResNet50 feature extractor and LightGBM classifier (with
feature extraction time of 0.960 seconds and training time of 10.206 seconds in Table 5) with
an overall accuracy of 98.00 ± 0.09. Comparing the first and second winners among all combinations, the classification accuracy of DenseNet121 with Bagging is slightly better (1%) than
9

Table 2: Comparison of classification precision metric of different machine learning models. The bold value
indicates the best result; underlined value represents the second-best result of the respective category.
Decision Tree

Random Forest

XGBoost

AdaBoost

Bagging Classifier

LightGBM

MobileNet

89.00%

88.00%

93.00%

85.00%

99.00%

90.00%

DesnseNet121

96.00%

97.00%

98.00%

95.00%

96.00%

95.00%

DenseNet201

94.00%

94.00%

95.00%

94.00%

98.00%

94.00%

Xception

92.00%

95.00%

90.00%

89.00%

98.00%

93.00%

InceptionV3

85.00%

85.00%

96.00%

85.00%

99.00%

82.00%

InceptionResNetV2

88.00%

96.00%

95.00%

90.00%

95.00%

93.00%

ResNet50

95.00%

89.00%

94.00%

96.00%

95.00%

94.00%

ResNet152

90.00%

91.00%

95.00%

91.00%

93.00%

89.00%

VGG16

94.00%

93.00%

94.00%

89.00%

92.00%

89.00%

VGG19

94.00%

93.00%

94.00%

89.00%

92.00%

89.00%

NASNetLarge

89.00%

91.00%

94.00%

90.00%

95.00%

91.00%

NASNetMobile

89.00%

87.00%

95.00%

88.00%

93.00%

88.00%

ResNet50V2

92.00%

89.00%

94.00%

88.00%

96.00%

91.00%

ResNet101V2

87.00%

89.00%

94.00%

86.00%

96.00%

78.00%

ResNet152V2

91.00%

94.00%

96.00%

91.00%

97.00%

91.00%

Table 3: Comparison of classification recall metric of different machine learning models. The bold value indicates
the best result; underlined value represents the second-best result of the respective category.
Decision Tree

Random Forest

XGBoost

AdaBoost

Bagging Classifier

LightGBM

MobileNet

89.00%

88.00%

93.00%

84.00%

99.00%

90.00%

DesnseNet121

96.00%

96.00%

98.00%

95.00%

96.00%

95.00%

DenseNet201

94.00%

94.00%

95.00%

94.00%

98.00%

94.00%

Xception

90.00%

95.00%

90.00%

89.00%

98.00%

93.00%

InceptionV3

85.00%

85.00%

96.00%

85.00%

99.00%

82.00%

InceptionResNetV2

88.00%

95.00%

95.00%

90.00%

95.00%

93.00%

ResNet50

95.00%

89.00%

94.00%

96.00%

95.00%

94.00%

ResNet152

89.00%

91.00%

95.00%

90.00%

93.00%

89.00%

VGG16

94.00%

93.00%

94.00%

89.00%

91.00%

89.00%

VGG19

94.00%

93.00%

94.00%

89.00%

91.00%

89.00%

NASNetLarge

88.00%

91.00%

94.00%

90.00%

95.00%

90.00%

NASNetMobile

89.00%

87.00%

95.00%

88.00%

93.00%

85.00%

ResNet50V2

91.00%

89.00%

94.00%

88.00%

96.00%

91.00%

ResNet101V2

87.00%

88.00%

94.00%

85.00%

96.00%

77.00%

ResNet152V2

90.00%

94.00%

96.00%

90.00%

96.00%

91.00%

ResNet50 with LightGBM, while the training time of the second winner is tempting, almost
30 times better than the first winner in terms of accuracy. Although Bagging is a slow learner,
it has the lowest standard deviation and hence is more stable than other learners.
The results also demonstrate that the detection rate is worst on the features extracted by
ResNet101V2 trained by the AdaBoost classifier with 76.00 ± 0.32 accuracy. Figure 5 and
Figure 6 demonstrate box-plot distributions of deep CNNs feature extractors and classification
accuracy from the 10-fold cross-validation. Circles in Figure 5 represent outliers. In Tables 2, 3

10

Table 4: Comparison of classification f1-score metric of different machine learning models. The bold value
indicates the best result; underlined value represents the second-best result of the respective category.
Decision Tree

Random Forest

XGBoost

AdaBoost

Bagging Classifier

LightGBM

MobileNet

89.00%

88.00%

93.00%

84.00%

99.00%

91.00%

DesnseNet121

96.00%

96.00%

98.00%

95.00%

96.00%

95.00%

DenseNet201

94.00%

94.00%

95.00%

94.00%

98.00%

94.00%

Xception

90.00%

95.00%

90.00%

89.00%

98.00%

93.00%

InceptionV3

85.00%

85.00%

96.00%

85.00%

99.00%

82.00%

InceptionResNetV2

88.00%

95.00%

95.00%

90.00%

95.00%

93.00%

ResNet50

95.00%

89.00%

94.00%

96.00%

95.00%

94.00%

ResNet152

89.00%

91.00%

95.00%

90.00%

93.00%

89.00%

VGG16

94.00%

93.00%

94.00%

89.00%

91.00%

89.00%

VGG19

94.00%

93.00%

94.00%

89.00%

91.00%

89.00%

NASNetLarge

88.00%

91.00%

94.00%

90.00%

95.00%

90.00%

NASNetMobile

89.00%

87.00%

95.00%

88.00%

93.00%

85.00%

ResNet50V2

91.00%

89.00%

94.00%

88.00%

96.00%

91.00%

ResNet101V2

87.00%

88.00%

94.00%

85.00%

96.00%

77.00%

ResNet152V2

90.00%

94.00%

96.00%

90.00%

96.00%

91.00%

Table 5: The time for feature extraction of deep CNN models and training on ML algorithms using Intel(R)
Core (TM) i7-8700K 3.7 GHz processors with 32 GB RAM, Nvidia GeForce GTX 1080 Ti GPU with 11GB
RAM.
Extraction Time (s)

DT (s)

RF (s)

XGBoost (s)

AdaBoost (s)

Bagging Classifier (s)

LightGBM (s)

MobileNet

8.803

0.022

0.008

0.438

0.023

33.535

1.097

DesnseNet121

9.306

0.017

0.009

0.362

0.021

30.748

0.897

DenseNet201

38.227

0.035

0.009

0.684

0.034

33.446

1.573

Xception

10.819

0.042

0.009

0.787

0.044

35.144

1.612

InceptionV3

11.825

0.045

0.009

0.86

0.048

37.54

1.98

InceptionResNetV2

14.151

0.035

0.009

0.575

0.035

33.562

1.169

ResNet50

10.206

0.034

0.009

0.694

0.04

33.232

0.96

ResNet152

15.769

0.031

0.01

0.653

0.031

32.347

1.114

VGG16

14.746

0.009

0.008

0.2

0.012

29.51

0.498

VGG19

14.359

0.01

0.008

0.2

0.013

29.336

0.494

NASNetLarge

13.131

0.066

0.01

1.409

0.067

38.337

2.542

NASNetMobile

7.786

0.024

0.009

0.429

0.024

32.782

0.93

ResNet50V2

10.204

0.044

0.009

0.691

0.045

34.369

1.798

ResNet101V2

12.435

0.047

0.009

0.776

0.048

0.9634

1.577

ResNet152V2

16.67

0.031

0.009

0.73

0.032

34.56

1.514

and 4, the obtained precision, recall, and F1-score of the features extracted by deep CNN architectures trained by different learners are presented respectively. As given in these tables,
the highest precision, recall, and F1-score rates are achieved by MobileNet and InceptionV3
feature vector trained on Bagging tree classifier with value of 99.00% precision, recall, and
F-score. The XGBoost and Bagging classifiers also yielded the second-best results with values of (98.00, 98.00, 98,00)% precision, recall, and F-score rates with features extracted by
DesnseNet121, DenseNet201 and Xception architectures. Similar conclusions can be drawn for
other models. The experimental results indicate that the performance of the deep CNNs using
DenseNet121, DenseNet201, MobileNet, Xception and InceptionV3 models trained by Bagging
11

Figure 5: Performance of different ML classifiers on the COVID-19 pneumonia classification.

Figure 6: Performance of the deep CNNs feature extractors and Bagging classifier on the COVID-19 pneumonia
classification.

tree and XGBoost classifiers yield satisfactory results and outperforms other state-of-the-art
CNNs and learners in COVID-19 classification. Based on the obtained results, we believe that
by discarding the irrelevant features using sparse descriptors of low dimensionality features
extracted by deep CNN models instead of training a deep CNNs model can be considered as
a successful improvement of the performance of a machine learning algorithms. Our obtained
results agree with the top-performing ML classifiers of Bagging and LightGBM. The best pre-

12

Figure 7: Feature extraction time and accuracy of different deep CNN feature extractors on COVID-19 classification.

Figure 8: Examples of miss-classified cases of COVID-19 dataset.

trained visual feature extractor so far was DesnseNet121, MobileNet and InceptionV3 rather
than counterpart architectures for COVID-19 image classification.
Although the approach presented here shows satisfying performance, it also has limitations
classifying more challenging instances with vague, low contrast boundaries, and the presence of
artifacts. Some examples of these cases are illustrated in Figure 7. Finally, comparison of the
feature extraction time using deep CNN models and training with ML algorithms are shown in
Table 5 and Figure 7. The extraction time of the DenseNet201 architectures on the total of 274
images was computed with 38.227 seconds (about 0.13 second per image) was the longest visual
feature extractor and NASNetMobile was the fastest visual feature extractor by 7.786 seconds
(about 0.028 second per image). DesnseNet121 architecture as the best model took 9.306

13

seconds (about 0.03 second per image) for feature extraction phase and 30.748 seconds (about
0.11 second per image) for the training phase on Bagging tree classifier. ResNet50 architecture
as the second-best visual feature extractor took 10.206 seconds (about 0 0.03 second per image)
for feature extraction phase and the training time of 0.960 seconds (about 0.003 second per
image) on LightGBM classifier. In conclusion, the extraction and training time of the proposed
approach is considerate significantly low in comparison with training a deep CNN model from
scratch which implies faster computation time and lower resource consumption.
After training a model, the pre-trained weights and models can be used as predictive engine
for CAD systems to allow an automatic classification of new data. A web-based application was
implemented using standard web development tools and techniques such as Python, JavaScript,
HTML, and Flask web framework. Figure 9 shows the output of our web-based application
for COVID-19 pneumonia detection. This web application could help doctors benefit from
our proposed method by providing an online tool that only requires uploading an X-ray or
CT image. The application then provides the physician with a simple COVID-19 Positive, or
COVID-19 Negative observation. It should be noted that this application has yet to be clinically
validated, is not yet approved for diagnostic use and would simply serve as a diagnostic aid for
the medical imaging specialist.

Figure 9: Web-based application for automatic detection of COVID-19 pneumonia.

The proposed method is generic as it does not need handcrafted features and can be easily
adapted, requiring minimal pre-processing. The provided dataset is collected across multiple
sources with different shape, textures and morphological characteristics. The transfer learning
strategy has successfully transferred knowledge from the source to the target domain despite
the limited dataset size of the provided dataset. During the proposed approach, we observed
that no overfitting occurs to impact the classification accuracy adversely. However, our study
has some limitations. The training data samples are limited. Extending the dataset size by
additional data sources can provide a better understanding on the proposed approach. Also,
employing pre-trained networks as feature extractors requires to rescale the input images to a
certain dimension which may discard valuable information. Although the proposed methodology achieved satisfying performance with an accuracy of 99.00%, the diagnostic performance of
the deep learning visual feature extractor and machine learning classifier should be evaluated
on real clinical study trials.

14

5. Conclusion
The ongoing pandemic of COVID-19 has been declared a global health emergency due to
the relatively high infection rate of the disease. As of the time of this writing, there is no
clinically approved therapeutic drug or vaccine available to treat COVID-19. Early detection
of COVID-19 is important to interrupt the human-to-human transmission of COVID-19 and
patient care. Currently, the isolation and quarantine of the suspicious patients is the most
effective way to prevent the spread of COVID-19. Diagnostic modalities such as chest Xray and CT are playing an important role in monitoring the progression and severity of the
disease in COVID-19 positive patients. This paper presents a feature extractor-based deep
learning and machine learning classifier approach for computer-aided diagnosis of COVID-19
pneumonia. Several ML algorithms were trained on the features extracted by well-established
CNNs architectures to find the best combination of features and learners. Considering the high
visual complexity of image data, proper deep feature extraction is considered as a critical step
in developing deep CNN models. The experimental results on available chest X-ray and CT
dataset demonstrate that the features extracted by DesnseNet121 architecture and trained by
a Bagging tree classifier generates very accurate prediction of 99.00% in terms of classification
accuracy.
References
[1] M. A. Shereen, S. Khan, A. Kazmi, N. Bashir, R. Siddique, Covid-19 infection: Origin, transmission, and characteristics of human coronaviruses, Journal of Advanced
Research 24 (2020) 91 – 98. URL: http://www.sciencedirect.com/science/article/
pii/S2090123220300540. doi:https://doi.org/10.1016/j.jare.2020.03.005.
[2] G. Lippi, M. Plebani, B. M. Henry, Thrombocytopenia is associated with severe coronavirus disease 2019 (covid-19) infections: A meta-analysis, Clinica Chimica Acta
506 (2020) 145 – 148. URL: http://www.sciencedirect.com/science/article/pii/
S0009898120301248. doi:https://doi.org/10.1016/j.cca.2020.03.022.
[3] T. Zhang, Q. Wu, Z. Zhang, Probable pangolin origin of sars-cov-2 associated with
the covid-19 outbreak,
Current Biology 30 (2020) 1346 – 1351.e2. URL: http:
//www.sciencedirect.com/science/article/pii/S0960982220303602. doi:https://
doi.org/10.1016/j.cub.2020.03.022.
[4] H. Guo, Y. Zhou, X. Liu, J. Tan, The impact of the covid-19 epidemic on the utilization of emergency dental services, Journal of Dental Sciences (2020). URL: http:
//www.sciencedirect.com/science/article/pii/S1991790220300209. doi:https://
doi.org/10.1016/j.jds.2020.02.002.
[5] S. Chavez, B. Long, A. Koyfman, S. Y. Liang, Coronavirus disease (covid-19): A primer for
emergency physicians, The American Journal of Emergency Medicine (2020). URL: http:
//www.sciencedirect.com/science/article/pii/S0735675720301789. doi:https://
doi.org/10.1016/j.ajem.2020.03.036.
[6] H. A. Rothan, S. N. Byrareddy, The epidemiology and pathogenesis of coronavirus disease (covid-19) outbreak, Journal of Autoimmunity 109 (2020) 102433. URL: http:

15

//www.sciencedirect.com/science/article/pii/S0896841120300469. doi:https://
doi.org/10.1016/j.jaut.2020.102433.
[7] H. Liu, F. Liu, J. Li, T. Zhang, D. Wang, W. Lan, Clinical and ct imaging features of the covid-19 pneumonia: Focus on pregnant women and children, Journal of Infection (2020). URL: http://www.sciencedirect.com/science/article/pii/
S0163445320301183. doi:https://doi.org/10.1016/j.jinf.2020.03.007.
[8] WHO, Coronavirus disease (COVID-19) Pandemic, 2020. URL: https://www.who.int/
emergencies/diseases/novel-coronavirus-2019.
[9] E. Shim, A. Tariq, W. Choi, Y. Lee, G. Chowell,
Transmission potential and
severity of covid-19 in south korea,
International Journal of Infectious Diseases
93 (2020) 339 – 344. URL: http://www.sciencedirect.com/science/article/pii/
S1201971220301508. doi:https://doi.org/10.1016/j.ijid.2020.03.031.
[10] CDC, Coronavirus Infections, 2020. URL: https://phil.cdc.gov/Details.aspx?pid=
23313.
[11] CDC, Coronavirus Infections - Transmission electron microscopic image, 2020. URL:
https://phil.cdc.gov/Details.aspx?pid=23354.
[12] Y. Wang, C. Dong, Y. Hu, C. Li, Q. Ren, X. Zhang, H. Shi, M. Zhou, Temporal changes
of ct findings in 90 patients with covid-19 pneumonia: a longitudinal study, Radiology
(2020) 200843.
[13] E. E.-D. Hemdan, M. A. Shouman, M. E. Karar, Covidx-net: A framework of deep
learning classifiers to diagnose covid-19 in x-ray images, arXiv preprint arXiv:2003.11055
(2020).
[14] J. P. Cohen, P. Morrison, L. Dao, Covid-19 image data collection, arXiv 2003.11597
(2020). URL: https://github.com/ieee8023/covid-chestxray-dataset.
[15] Adrian Rosebrock, Detecting COVID-19 in X-ray images with Keras, TensorFlow,
and Deep Learning, 2020. URL: https://www.pyimagesearch.com/2020/03/16/
detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/.
[16] M. Barstugan, U. Ozkaya, S. Ozturk, Coronavirus (covid-19) classification using ct images
by machine learning methods, arXiv preprint arXiv:2003.09424 (2020).
[17] N. Cristianini, J. Shawe-Taylor, et al., An introduction to support vector machines and
other kernel-based learning methods, Cambridge university press, 2000.
[18] L. Wang, A. Wong, Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest radiography images, arXiv preprint arXiv:2003.09871
(2020).
[19] Kaggle, Kaggle’s Chest X-Ray Images (Pneumonia) dataset, 2020. URL: https://www.
kaggle.com/paultimothymooney/chest-xray-pneumonia.
[20] H. S. Maghdid, A. T. Asaad, K. Z. Ghafoor, A. S. Sadiq, M. K. Khan, Diagnosing covid-19
pneumonia from x-ray and ct images using deep learning and transfer learning algorithms,
arXiv preprint arXiv:2004.00038 (2020).
16

[21] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional
neural networks, in: Advances in neural information processing systems, 2012, pp. 1097–
1105.
[22] B. Ghoshal, A. Tucker, Estimating uncertainty and interpretability in deep learning for
coronavirus (covid-19) detection, arXiv preprint arXiv:2003.10769 (2020).
[23] L. O. Hall, R. Paul, D. B. Goldgof, G. M. Goldgof, Finding covid-19 from chest x-rays
using deep learning on a small dataset, arXiv preprint arXiv:2004.02060 (2020).
[24] M. Farooq, A. Hafeez, Covid-resnet: A deep learning framework for screening of covid19
from radiographs, arXiv preprint arXiv:2003.14395 (2020).
[25] Y.-L. Boureau, J. Ponce, Y. LeCun, A theoretical analysis of feature pooling in visual
recognition, in: Proceedings of the 27th international conference on machine learning
(ICML-10), 2010, pp. 111–118.
[26] A. Rakhlin, A. Shvets, V. Iglovikov, A. A. Kalinin, Deep convolutional neural networks
for breast cancer histology image analysis, in: International Conference Image Analysis
and Recognition, Springer, 2018, pp. 737–744.
[27] Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, M. S. Lew, Deep learning for visual
understanding: A review, Neurocomputing 187 (2016) 27–48.
[28] J. R. Quinlan, Induction of decision trees, Machine learning 1 (1986) 81–106.
[29] L. Breiman, Random forests, Machine learning 45 (2001) 5–32.
[30] T. Chen, C. Guestrin, XGBoost, in: Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining - KDD ’16, ACM Press, New York,
New York, USA, 2016, pp. 785–794. URL: http://dl.acm.org/citation.cfm?doid=
2939672.2939785. doi:10.1145/2939672.2939785.
[31] Y. Freund, R. E. Schapire, A desicion-theoretic generalization of on-line learning and
an application to boosting, in: European conference on computational learning theory,
Springer, 1995, pp. 23–37.
[32] L. Breiman, Bagging predictors, Machine learning 24 (1996) 123–140.
[33] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y. Liu, Lightgbm:
A highly efficient gradient boosting decision tree, in: Advances in neural information
processing systems, 2017, pp. 3146–3154.
[34] S. H. Kassani, P. H. Kassani, M. J. Wesolowski, K. A. Schneider, R. Deters, Breast cancer
diagnosis with transfer learning and global pooling, arXiv preprint arXiv:1909.11839
(2019).
[35] S. Khan, N. Islam, Z. Jan, I. U. Din, J. J. C. Rodrigues, A novel deep learning based
framework for the detection and classification of breast cancer using transfer learning,
Pattern Recognition Letters 125 (2019) 1–6.
[36] R. Mehra, et al., Breast cancer histology images classification: Training from scratch or
transfer learning?, ICT Express 4 (2018) 247–254.
17

[37] S. Lu, Z. Lu, Y.-D. Zhang, Pathological brain detection based on alexnet and transfer
learning, Journal of computational science 30 (2019) 41–47.
[38] S. H. Kassani, P. H. Kassani, M. J. Wesolowski, K. A. Schneider, R. Deters, Classification
of histopathological biopsy images using ensemble of deep learning networks, in: Proceedings of the 29th Annual International Conference on Computer Science and Software
Engineering, CASCON ’19, IBM Corp., USA, 2019, p. 92–99.
[39] Z. Liu, Y. Cao, Y. Li, X. Xiao, Q. Qiu, M. Yang, Y. Zhao, L. Cui, Automatic diagnosis of
fungal keratitis using data augmentation and image fusion with deep convolutional neural
network, Computer Methods and Programs in Biomedicine 187 (2020) 105019.
[40] S. Liu, G. Tian, Y. Xu, A novel scene classification model combining resnet based transfer
learning and data augmentation with a filter, Neurocomputing 338 (2019) 191–206.
[41] P. Sridar, A. Kumar, A. Quinton, R. Nanan, J. Kim, R. Krishnakumar, Decision fusionbased fetal ultrasound image plane classification using convolutional neural networks,
Ultrasound in medicine & biology 45 (2019) 1259–1273.
[42] W. Zhang, J. Zhong, S. Yang, Z. Gao, J. Hu, Y. Chen, Z. Yi, Automated identification
and grading system of diabetic retinopathy using deep neural networks, Knowledge-Based
Systems 175 (2019) 12–25.
[43] C. M. Dourado Jr, S. P. P. da Silva, R. V. M. da Nóbrega, A. C. d. S. Barros, P. P.
Reboucas Filho, V. H. C. de Albuquerque, Deep learning iot system for online stroke
detection in skull computed tomography images, Computer Networks 152 (2019) 25–39.
[44] A. Çinar, M. Yıldırım, Detection of tumors on brain mri images using the hybrid convolutional neural network architecture, Medical Hypotheses (2020) 109684.
[45] T. Cogan, M. Cogan, L. Tamil, Mapgi: Accurate identification of anatomical landmarks
and diseased tissue in gastrointestinal tract using deep learning, Computers in biology
and medicine 111 (2019) 103351.
[46] Kaggle, RSNA Pneumonia Detection Challenge, 2020. URL: https://www.kaggle.com/
c/rsna-pneumonia-detection-challenge.

18

