End-to-End QA on COVID-19: Domain Adaptation with Synthetic Training
Revanth Gangi Reddy,1* Bhavani Iyer,2 Md Arafat Sultan,2 Rong Zhang,2
Avi Sil,2 Vittorio Castelli,2 Radu Florian,2 Salim Roukos2

arXiv:2012.01414v1 [cs.CL] 2 Dec 2020

1
2
University of Illinois, Urbana Champaign
IBM Research AI, New York
revanth3@illinois.edu, {bsiyer,zhangr,avi,vittorio,raduf,roukos}@us.ibm.com,
arafat.sultan@ibm.com

Abstract
End-to-end question answering (QA) requires both information retrieval (IR) over a large document collection and machine reading comprehension (MRC) on the retrieved passages. Recent work has successfully trained neural IR systems using only supervised question answering (QA) examples from open-domain datasets. However, despite impressive
performance on Wikipedia, neural IR lags behind traditional
term matching approaches such as BM25 in more specific and
specialized target domains such as COVID-19. Furthermore,
given little or no labeled data, effective adaptation of QA systems can also be challenging in such target domains. In this
work, we explore the application of synthetically generated
QA examples to improve performance on closed-domain retrieval and MRC. We combine our neural IR and MRC systems and show significant improvements in end-to-end QA on
the CORD-19 collection over a state-of-the-art open-domain
QA baseline.

Introduction
In early 2020, the novel coronavirus SARS-CoV-2 was
circulating in the U.S. state of New York, unknowingly to
its residents, later causing tens of thousands of deaths in
the state. The lack of information available at those early
stages of the virus’ spread—first about the spread itself, and
later about its mitigation including the importance of wearing masks—contributed to more deaths per capita in several
northeastern U.S. states than the rest of the country. This
unfortunate occurrence underscores the criticality of early
discovery and dissemination of key information in the fight
against pandemics such as COVID-19.
End-to-end question answering (QA) systems (Lee,
Chang, and Toutanova 2019; Karpukhin et al. 2020) can
be an effective tool for information dissemination in such
events. Given a natural language question, these systems
mine related passages from large collections of texts and
then extract a short specific answer from the retrieved passages. Hence they must include both an information retrieval
(IR) and a machine reading comprehension (MRC) component.
Typical approaches to open-domain QA use traditional IR
methods—such as TF-IDF matching (Chen et al. 2017) or
BM25 term weighting (Robertson and Zaragoza 2009)—to
* Work done during AI Residency at IBM Research.

retrieve evidence from a corpus. Such IR methods represent
text as high-dimensional sparse vectors and rely on inverted
indices for efficient search. However, these methods cater almost exclusively to keyword search queries. Recently, with
the advent of large pre-trained language models (Radford
et al. 2019; Devlin et al. 2019), using dense semantic representations for IR has emerged as a promising approach.
Concurrently, tools like FAISS (Johnson, Douze, and Jégou
2019) have been created, which use special in-memory data
structures and indexing schemes to provide highly efficient
search in a dense vector space. Karpukhin et al. (2020) note
that when queried on an index with 21 million passages,
FAISS processes 995.0 questions per second, returning top
100 passages per question. In contrast, BM25 processes 23.7
questions per second per CPU thread in a similar setting.
Karpukhin et al. (2020) have shown that neural IR systems trained using human-annotated open-domain MRC
data (Kwiatkowski et al. 2019; Joshi et al. 2017) can yield
high retrieval performance on a large collection (Wikipedia,
21 million documents). While such systems have been
shown to perform better than BM25 in an open-domain
setup, it is unclear how well they would perform in specialized domains where the content and terminology are very
specific. Also, such domains often do not have an adequate
amount of high-quality annotated data on which neural IR
and MRC models can be trained. For example, there is very
little annotated QA data on COVID-19 although considerable amount of raw text (Wang et al. 2020) is available.
In this work, we show that automatically generated synthetic training examples from in-domain raw text can yield
effective end-to-end QA systems for new target domains,
specifically, COVID-19. Our novel synthetic example generator creates training labels for both the IR and the MRC
component of the system. Using distant supervision from
synthetic question-passage pairs, we fine-tune a pre-trained
open-domain neural IR system for the target domain. Synthetic question-passage-answer triples are used to also train
a target domain MRC system. In addition, we adopt existing techniques from the question generation literature such
as top-p top-k sampling (Sultan et al. 2020) and roundtrip
consistency check (Alberti et al. 2019) to further improve
the quality of generated examples.
In our experiments, we use the scientific articles from
the CORD-19 collection (Wang et al. 2020) as the target

domain text. While recent work (Möller et al. 2020; Tang
et al. 2020; Lee et al. 2020a) has released some labeled QA
datasets on COVID-19, they are small in size and can only be
used to evaluate systems in a zero-shot setting. This scarcity
of human-labeled QA data makes our approach of domain
adaptation using synthetic examples especially relevant for
this domain. However, the techniques proposed in this paper
are not specific to COVID-19 and can potentially improve
end-to-end QA performance in other domains as well.
Overall, our main contributions in this paper are as follows:
• We propose a novel approach to generating synthetic
training examples for both IR and MRC in target domains
where human-annotated data is scarce.
• We utilize the synthetic examples in a zero-shot domain
adaptation setting to separately improve neural IR and
MRC performance in the target domain.
• Finally, combining our improved IR and MRC systems,
we show significant improvements in end-to-end QA on
multiple COVID-19 datasets over a state-of-the-art opendomain QA system. To our knowledge, our work is the
most extensive evaluation of end-to-end QA on a diverse
set of QA datasets related to COVID-19.

Related Work
End-to-end QA works in a pipeline where the first important step is to retrieve documents, which potentially contain the answer to an input question, from a large corpus.
Classical sparse vector methods like TF-IDF and BM25 remain strong baselines for IR, and have been used in end-toend open-domain QA systems (Chen et al. 2017). More recently, approaches based on dense text representations have
emerged as strong alternatives, as they enable modeling of
textual similarity at a semantic level. Seo et al. (2019) propose to combine dense and sparse representations of phrases
for real-time question answering. Unsupervised training
schemes have also been proposed to latently learn neural
retrieval models. Lee, Chang, and Toutanova (2019) propose an inverse cloze task to train a retriever while Guu
et al. (2020) augment masked language model pre-training
with a latent knowledge retriever. Karpukhin et al. (2020)
introduce a “dense passage retriever” (DPR) and show that
high performance neural IR models can be trained using labeled MRC data. Later works (Lewis et al. 2020b; Izacard
and Grave 2020) have used DPR to obtain competitive opendomain QA performance. We also adopt DPR as our base IR
model and further fine-tune it with target domain examples.
Domain adaptation is an active area of research in natural language processing. Previous work has tackled sourcetarget domain mismatch via instance weighting (Jiang and
Zhai 2007) and training data selection strategies (Liu et al.
2019a). Wiese, Weissenborn, and Neves (2017) have used
transfer learning from open-domain datasets. For pre-trained
language models, domain adaptation involves further training the model (Gururangan et al. 2020) on unlabeled text
in the target domain, e.g., scientific articles (Beltagy, Lo,
and Cohan 2019) and biomedical text (Lee et al. 2020b;
Alsentzer et al. 2019). Here, in addition to fine-tuning on

target domain raw text, we generate labeled target domain
training data using a synthetic example generator model.
COVID-19 is a low-resource domain with abundant unlabeled text (Wang et al. 2020). A number of organizations
have made their demos1 available for search over scientific
articles on COVID-19. Zhang et al. (2020) build a retrieval
system that uses neural ranking models on top of traditional term matching methods. Tang et al. (2020) use transfer
learning from out-of-domain datasets to improve MRC performance. None of the two systems have been evaluated in
an end-to-end QA setting. We implement a complete QA
pipeline for COVID-19 which we evaluate on end-to-end
QA and also separately on IR and MRC.
In recent times, pre-trained transformer models have been
instrumental in the generation of high quality text across a
wide range of applications (Radford et al. 2019; Lewis et al.
2020a; Raffel et al. 2019). In QA, such models as well as
older LSTM-based generators have been applied to answeraware question generation, where the model learns to generate questions from passage-answer pairs (Du and Cardie
2018; Dong et al. 2019; Sultan et al. 2020; Tuan, Shah, and
Barzilay 2020). We take a similar approach and fine-tune a
pre-trained transformer model (Lewis et al. 2020a) to generate question-answer pairs from passages. We also utilize
state-of-the-art techniques such as diversity-promoting sampling (Sultan et al. 2020) and roundtrip consistency check
(Alberti et al. 2019) to further improve performance.

COVID-19 Datasets
In this section, we describe our corpus of COVID-19
documents and the annotated datasets we use for zero-shot
evaluation of all IR and MRC systems.
CORD-192 (Wang et al. 2020), or the COVID-19 Open
Research Dataset, is a collection of documents taken from
the scientific literature on SARS-CoV-2 and other related
coronaviruses. The collection is updated on a daily basis; in
our experiments, we use 74,059 full text PDFs from its June
22 version.
COVID-QA-20193 (Möller et al. 2020) is a question answering dataset consisting of 2,019 question-article-answer
triples. These were created by volunteer biomedical experts
from scientific articles related to COVID-19. This dataset
differs from traditional open-domain MRC datasets such as
SQuAD (Rajpurkar et al. 2016) in that the examples come
from longer contexts (more than 6k words on average vs.
SQuAD’s 153.2) and answers are also generally longer
(13.9 vs. 3.2 words).
COVID-QA-1474 (Tang et al. 2020) is a QA dataset
obtained from Kaggle’s CORD-19 challenge5 . Overall, the
dataset contains 147 question-article-answer triples with 27
1

Amazon, Google, IBM, Vespa
https://www.semanticscholar.org/cord19
3
https://github.com/deepset-ai/COVID-QA
4
http://covidqa.ai/
5
Kaggle CORD-19
2

Passage
... Since December 2019, when the first patient with a confirmed case of
COVID-19 was reported in Wuhan, China, over 1,000,000 patients with
confirmed cases have been reported worldwide. It has been reported that
the most common symptoms include fever, fatigue, dry cough, anorexia,
and dyspnea. Meanwhile, less common symptoms are nasal congestion ...
... As with any research, this study is also not without its limitations. First,
is the issue of low response rate despite concerted efforts by the research
team to contact key informants multiple times. Scholars have argued that
such research is often perceived as opportunistic, by the respondents and
this perceived lack of trust is likely to have impacted response rates ...

Synthetic Question-Answer pairs
Q: What are the most common symptoms of COVID-19?
A: fever, fatigue, dry cough, anorexia, and dyspnea
Q: How many people have been diagnosed with COVID-19?
A: over 1,000,000
Q: What is the main limitation of this study?
A: low response rate
Q: Why was there a low response rate?
A: perceived lack of trust

Table 1: Synthetic MRC examples generated by our generator from two snippets in the CORD-19 collection.

COVID-QA-1116 (Lee et al. 2020a) contains queries gathered from multiple sources including Kaggle and the FAQ
sections of the CDC7 and the WHO8 . The dataset contains 111 question-answer pairs with 53 interrogative and
58 keyword-style queries. Since questions are not aligned to
passages in this dataset, we use it for zero-shot evaluation of
only the IR and end-to-end QA systems.

disambiguates s in p. In essence, g learns to identify a sentence s in p likely to contain an answer span, extract span a
from s, and generate the corresponding question q, all in a
single generative model.
Recently, Sultan et al. (2020) have shown that samplingbased question generation leads to better MRC training than
greedy or beam search decoding due to increased sample diversity. We adopt a diversity-promoting top-p top-k sampler
for our modified generation task. Moreover, our design enables the diversification of question-answer pairs as a whole
instead of just questions. Table 1 shows some examples of
generated QA pairs from two CORD-19 passages.

Generating Synthetic Training Examples

Information Retrieval

We fine-tune BART (Lewis et al. 2020a)—a transformerbased denoising autoencoder with a bidirectional encoder
and a causal decoder—to generate synthetic training examples for both IR and MRC. An MRC example is a triple
(p, q, a) comprising a paragraph p, a natural language question q, and a short answer a (e.g., a named entity) in p. IR
training examples consist of only q and p. Previous work has
fine-tuned similar transformer models to generate questions
from (p, a) pairs, where a separate model extracts a candidate answer a from p prior to question generation (Du and
Cardie 2018; Alberti et al. 2019; Sultan et al. 2020).
In this work, we train a model to generate both q and a
given p in a single pass. To achieve this, we create training
examples for the generator from existing MRC datasets such
as SQuAD (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang
2018). For each MRC example (p, q, a) where p is the paragraph, q is the question, and a is its human-annotated answer
in p, we first use a sentence tokenizer to segment p into sentences and locate the sentence s that contains a. Then we
create a training example consisting of p as the source and
the ordered sequence s, a, q as the target; special separator
tokens separate the three target segments. A BART autoencoder is then fine-tuned on all such examples to train a generator g with parameters θg , which learns to maximize the
joint probability P (s, a, q|p; θg ). To speed up generation, we
only include the first and the last word of s in the generated
version of s, which in a vast majority of the cases uniquely

We adopt a state-of-the-art neural IR model called the Dense
Passage Retriever (DPR) (Karpukhin et al. 2020) as our base
retrieval model. For COVID-19 domain adaptation, we further fine-tune it on synthetic examples generated from the
CORD-19 collection.

unique questions and 104 unique articles. Due to the small
number of questions, we only use this dataset for zero-shot
evaluation of MRC models on the 147 question-answer
pairs.

6

https://github.com/dmis-lab/covidAsk
CDC FAQ
8
WHO FAQ
7

Dense Passage Retriever (DPR)
Given a collection of passages, the DPR model creates an
index in a continuous space to retrieve passages relevant to
an input question. It uses a Siamese neural network (Koch,
Zemel, and Salakhutdinov 2015) (aka. dual encoder) model
with separate dense encoders EQ (.) and EP (.) for the question and passage, respectively. Each encoder is a BERT (Devlin et al. 2019) (base, uncased) model that produces the hidden representation of the [CLS] token as output. The similarity between a question and a passage is the dot product of
their encoder outputs:
sim(q, p) = EQ (q)T EP (p)

(1)

Since Eq. 1 is decomposable, representations of all passages
in the collection are pre-computed and stored in an index using FAISS (Johnson, Douze, and Jégou 2019). Given an input question q, the top k passages with representations close
to EQ (q) are then retrieved.
Karpukhin et al. (2020) show that training examples for
such a dual-encoder model can be obtained from existing
−
−
MRC datasets. Each training instance (qi , p+
i , pi,1 , ..., pi,n )
contains a question qi , one positive passage p+
i and n negative passages p−
.
The
training
loss
is
the
negative logi,j

LM serves as the starting point for the later MRC fine-tuning
steps.

likelihood of the positive passage:
+

L = − log

esim(qi ,pi )
−
Pn
+
esim(qi ,pi ) + j=1 esim(qi ,pi,j )

(2)

While negative passages for a given question can be simply sampled from the collection, Karpukhin et al. (2020)
show that having a top passage returned by BM25 among the
negatives helps improve performance. To make the training
process more efficient, the trick of in-batch negatives (Yih
et al. 2011; Gillick et al. 2019) is also used. Thus, for each
question in a training mini-batch, the following passages are
used as negatives: (1) a passage returned by BM25 that is
not labeled positive, (2) positive passages as well as BM25retrieved negatives for other questions in the mini-batch. In
open-domain QA, DPR outperforms a strong Lucene-BM25
system by 9-19% top-20 passage retrieval accuracy on a
wide range of benchmark datasets.

Synthetic Training
We train our synthetic example generator on existing opendomain MRC data and give it target domain passages as input to produce multiple question-answer pairs per passage.
For IR, we discard the generated answers to first construct
positive question-passage pairs. To create negative examples, for each question, we select a BM25-retrieved passage which does not contain the answer text but has a high
lexical overlap with the question. For each generated question, we create an IR training example by aggregating the
question, the positive and the negative passage. During finetuning of the DPR model, at each iteration, a set of questions is randomly sampled from the generated dataset. Following (Karpukhin et al. 2020), we use in-batch negatives
while training. We call this final model the Adapted DPR
model.
Prior to retrieval using Adapted DPR, we pre-compute
the passage representations for the entire retrieval corpus,
which for the work presented in this paper was obtained
from the CORD-19 collection. The embeddings are indexed
using FAISS for efficient run-time retrieval. Finally, given
a question, the same inference procedure as in DPR is followed for retrieval.

MRC Model Architecture
Before giving the details of MRC fine-tuning, we briefly discuss our MRC model architecture. We use a standard extractive MRC model (Devlin et al. 2019) that extracts a short answer from a passage given a question. The network uses two
classification heads on top of a pre-trained RoBERTa LM,
which point to the start and end positions of the answer span.
For unanswerable examples, the classification heads point to
the position of the [CLS] token. Let start(.) and end(.) be
the outputs of the start and end classification heads. Then the
MRC score of an answer span (s, e), where s is the start and
e is the end token, is defined as:
Score(s, e) =
(3)
start(s) + end(e) − start([CLS]) − end([CLS])

Synthetic Training with Roundtrip Consistency
To fine-tune the CORD-19 LM for the MRC task, we use
both human-annotated data from two open-domain MRC
datasets—SQuAD2.0 (Rajpurkar, Jia, and Liang 2018) and
Natural Questions (NQ) (Kwiatkowski et al. 2019)—and the
synthetic question-answer pairs generated by our example
generator from CORD-19 passages.
For the synthetic training examples, we use a roundtrip
consistency (Alberti et al. 2019) filter to remove noisy examples from the generated data. It utilizes a pre-trained MRC
model to evaluate the quality of automatically generated
question-answer pairs. Specifically, following Alberti, Lee,
and Collins (2019), we fine-tune a RoBERTa LM first on
SQuAD2.0 and then on NQ. Given a synthetic question, this
MRC model first computes the MRC scores of candidate answer spans (Eq. 3) in the passage. We take the highest score
over all candidate spans as the answerability score of the
synthetic question, and filter the example out if this score is
lower than a threshold (tuned on our dev fold of the COVIDQA-2019 dataset). This filter uses the answerability score of
the question as a measure of noise in the generated example,
since the question generated from the passage is expected to
be answerable.

Fine-Tuning Sequence

Machine Reading Comprehension
Starting from a pre-trained masked language model (LM),
we perform a series of fine-tuning steps to adapt an opendomain MRC system for question answering on CORD-19.
Here we discuss the individual steps in detail as well as the
order in which they are applied to construct our final MRC
model.

We adapt the CORD-19 LM to the final MRC task using the
following sequence of fine-tuning steps. First, we fine-tune
on SQuAD2.0 examples, then on the roundtrip-consistent
synthetic examples from the CORD-19 passages, and finally
on the NQ examples. We experimented with other sequence
orders too but found the above to yield the best performance
on our dev fold of the COVID-QA-2019 (Möller et al. 2020)
dataset. We call this final model the Adapted MRC model.

CORD-19 Language Modeling
Gururangan et al. (2020) have shown that adapting a pretrained open-domain LM to unlabeled text in a target domain
before task-specific fine-tuning can be beneficial for the target task. We begin with a pre-trained RoBERTa-large LM
(Liu et al. 2019b) and continue masked LM training (Devlin
et al. 2019) on the CORD-19 documents. This target domain

Experimental Setup
Datasets
We use documents from the CORD-19 collection (Wang
et al. 2020) to create our retrieval corpus. We split the abstract and the main body of text of each article into passages that (a) contain no more than 120 words, and (b) align

Model

BM25
DPR-Multi
ICT
Adapted DPR
BM25 + DPR-Multi
BM25 + Adapted DPR

Open-COVID-QA-2019
M@20
22.4
14.4
16.6 (1.3)
28.0 (1.8)
23.4
31.8 (0.0)

Dev
M@40
24.9
18.4
21.6 (1.3)
31.8 (0.8)
27.9
36.0 (0.6)

M@100
29.9
22.9
25.5 (0.6)
39.0 (0.5)
32.3
42.6 (0.6)

M@20
29.9
13.8
18.1 (0.4)
34.8 (0.3)
29.5
43.2 (0.3)

Test
M@40
33.4
17.5
23.0 (0.1)
40.4 (0.2)
33.2
48.2 (0.3)

M@100
39.7
21.4
29.6 (0.2)
47.2 (0.2)
38.9
53.7 (0.3)

COVID-QA-111
Test
M@20
M@40
M@100
48.7
60.4
64.9
51.4
57.7
66.7
52.8 (0.4) 59.8 (1.1) 67.6 (2.2)
58.6 (0.0) 64.6 (1.5) 74.2 (1.9)
58.6
65.8
69.4
60.4 (1.3) 68.2 (0.4) 76.9 (0.8)

Table 2: Performance of different IR systems on (a) the open version of COVID-QA-2019, and (b) COVID-QA-111.
with sentence boundaries. This leads to an inference-time
retrieval corpus of around 3.5 million passages.
To create the passages from which we generate synthetic
training examples, we split the CORD-19 collection into
larger chunks of at most 288 wordpieces using the BERT
tokenizer, which results in about 1.8 million passages. This
setup provides longer contexts for diverse example generation and also facilitates faster experiments due to a smaller
number of passages.
For our MRC experiments, we split the COVID-QA-2019
dataset into dev and test subsets of 203 and 1,816 examples, respectively. Additionally for retrieval and end-to-end
QA experiments, we create an open version (Open-COVIDQA-2019 henceforth) wherein duplicate questions are deduplicated and different answers to the same question are
all included in the set of correct answers. This leaves 201
dev and 1,775 test examples in the open version. Finally, we
use the COVID-QA-2019 dev set for all hyperparameter tuning experiments, COVID-QA-147 for MRC evaluation, and
COVID-QA-111 for evaluating IR and end-to-end QA.

size of 128. The resulting model is our Adapted DPR model.
We also use a second neural IR baseline based on the Inverse Cloze Task (ICT) method proposed in (Lee, Chang,
and Toutanova 2019). ICT is an unsupervised training procedure wherein a sentence is randomly masked out from the
passage with a probability p and used as the query to create
a query-passage synthetic training pair. We adopt ICT as an
alternative approach to generating synthetic training examples. We set p=0.9, which Lee, Chang, and Toutanova (2019)
have shown to work best, and use the 288 wordpiece passages from the CORD-19 collection to create 1.8 million
training examples. We train for 6 epochs using these ICT
examples with a learning rate of 1e-5 and a batch size of
128. We then follow Lee, Chang, and Toutanova (2019) to
do a final round of fine-tuning wherein only the question encoder is trained for 10 epochs using questions from the open
version of NQ. Since the above technique does not require
any in-domain labeled data, we use it as a baseline domain
adaptation approach. We call this model the ICT model.

Synthetic Example Generation

The baseline MRC system fine-tunes a pre-trained
RoBERTa-large model for 3 epochs on SQuAD2.0 and then
for 1 epoch on Natural Questions (NQ) training examples.
It achieves a short answer EM of 59.4 on the NQ dev set,
which is competitive with numbers reported in (Liu et al.
2020). We use the Transformers library (Wolf et al. 2019)
for all our MRC experiments.
For masked LM fine-tuning of the pre-trained RoBERTalarge LM on the CORD-19 collection, we use approximately
1.5GB of text containing 225 million tokens. We train for
8 epochs with a learning rate of 1.5e-4 using the Fairseq
toolkit (Ott et al. 2019). For the downstream fine-tuning
of this LM to the MRC task, we train for 3 epochs on
SQuAD2.0, 1 epoch each on the filtered synthetic MRC examples and the NQ dataset. During roundtrip consistency filtering, we use a high answerability score threshold of t=7.0,
and are left with around 380k synthetic MRC examples after
filtering.

To train the synthetic example generator, we use the MRC
training examples of SQuAD1.1 (Rajpurkar et al. 2016). We
fine-tune BART for 3 epochs with a learning rate of 3e-5.
Using this model, we generate 5 MRC examples from each
of the 1.8 million passages in the CORD-19 collection. For
top-p top-k sampling, we use p=0.95 and k=10. Since it is a
generative model, we see cases where the answer text output
by the model is not in the input passage. We discard such examples. Overall, the model generates about 7.9 million synthetic examples. We store these as passage-question-answer
triples.

Neural IR
As our neural IR baseline, we use the DPR-Multi system—a
state-of-the-art neural IR model—from the publicly available implementation9 provided by Karpukhin et al. (2020).
This system comes pre-trained on the open versions of
multiple MRC datasets: Natural Questions (Kwiatkowski
et al. 2019), WebQuestions (Berant et al. 2013), CuratedTrec (Baudiš and Šedivỳ 2015) and TriviaQA (Joshi et al.
2017).
We fine-tune the DPR-Multi system for 6 epochs using the
synthetic examples with a learning rate of 1e-5 and a batch
9

Code available at https://github.com/facebookresearch/DPR

Machine Reading Comprehension

Metrics
We evaluate the IR models using the recall-based
Match@20, Match@40 and Match@100 metrics, similar to
(Karpukhin et al. 2020). These metrics measure the top-k retrieval accuracy, which is the fraction of questions for which
the top k retrieved passages contain a span that answers the
question. For the MRC models, we use the standard Exact

Example
Q: What was the fatality rate
for SARS-CoV?
A: 10%
Q: What is the molecular structure of the Human
metapneumovirus (HMPV)?
A: single-stranded RNA
virus

Adapted DPR
The case fatality rate (CFR) of COVID-19 was
2.3% (44/1023), much lower than that of SARS
(10%) and MERS (36%) (de Wit et al. 2016; Wu
and McGoogan 2020). Suspected COVID-19 patients (with symptoms) could be diagnosed ...
Human bocavirus: hMPV is a paramyxovirus first
discovered by van den Hoogen and colleagues15 in
2001. Similar to RSV, hMPV is a single-stranded
RNA virus belonging to the Pneumoviridae subfamily, and causes many of the same symptoms ...

BM25
The analysis estimated that the case-fatality rate
of COVID-19 in Europe would range between 4%
and 4.5%. The case-fatality rate of SARS-COV,
which was a similar outbreak, was 10%, while the
case-fatality rate of MERS-CoV was over 35% ...
... in specimens from 1976 to 2001. Collectively,
these studies show that HMPV has been circulating undetected for many decades. Genome organization and structure: HMPV is a negative-sense,
non-segmented, single-stranded RNA virus.

Table 3: Examples where Adapted DPR and BM25 both retrieve passages that are not returned by the other system (in the top
100 results).
Match (EM) and F1 score for evaluation. Finally, we evaluate the end-to-end QA systems on Top-1 F1 and Top-5 F1.

Results and Discussion
In this section, we first report results separately for our IR
and MRC systems. Then we evaluate the end-to-end QA system which combines the IR and the MRC component and
uses the entire CORD-19 collection to find an answer given
an input question. Reported numbers for all our trained models are averages over three seeds.

Information Retrieval
We evaluate our proposed system, Adapted DPR, against a
number of traditional term matching and neural IR baselines.
Specifically, we use BM2510 as the term matching baseline,
the DPR-Multi system as the zero-shot open-domain baseline and ICT as a domain adaptation baseline. Further, we
also evaluate models that combine term matching and neural approaches. We take the top 2,000 passages retrieved by
BM25 and neural models separately, and score each passage using a convex combination of its BM25 and neural
IR scores after normalization (weight is tuned on the OpenCOVID-QA-2019 dev set). We create two such combined
systems: BM25+DPR-Multi and BM25+Adapted DPR.
Results Table 2 shows the performance of different IR
systems on the Open-COVID-QA-2019 and COVID-QA111 datasets. BM25 shows strong performance on both
datasets, demonstrating the robustness of such term matching methods. While the neural DPR-multi system is competitive with BM25 on COVID-QA-111, it is considerably
behind on the larger Open-COVID-QA-2019 dataset. The
ICT model improves over DPR-multi, showing that domain
adaption using such unsupervised techniques can be beneficial.
Our Adapted DPR system achieves the best single system
results on both datasets, demonstrating the effectiveness of
using our synthetic example generation for domain adaptation. On the Open-COVID-QA-2019 test set, our model improves over the baseline DPR-Multi system by more than
10

Lucene Implementation. BM25 parameters b = 0.75 (document length normalization) and k1 = 1.2 (term frequency scaling)
worked best.

100%. Finally, we see that a combination of BM25 and the
neural approaches can give considerable performance improvements. Combining DPR-Multi with BM25 does not
lead to any gains on Open-COVID-QA-2019 likely due to
the fact that DPR-Multi performs poorly on this dataset.
However, we see large gains from combining BM25 and our
Adapted DPR system, as both perform well individually on
the two datasets. Our final BM25+Adapted DPR system is
better than the next best baseline by about 14 points across
all metrics on the test set of Open-COVID-QA-2019 and up
to 7 points on Match@100 on COVID-QA-111.
Analysis On a closer look at the passages retrieved individually by BM25 and Adapted DPR, we observe that the
two sets of retrieved passages are very different. For the
Open-COVID-QA-2019 dataset, only 5 passages are common on average among the top 100 passages retrieved separately by the two systems. This difference is also visible in
the relevant passages (passages that contain an answer to the
question) that are returned by the two systems. We observe
many cases where the two systems retrieve mutually exclusive relevant passages in the top 100 retrieved results. Table
3 shows two such examples. This diversity in retrieval results
demonstrates the complementary nature of the two systems
and also explains why their combination leads to improved
IR performance.
Model
NQ-style SynQ
Squad-style SynQ

M@20
20.4
28.0

M@40
23.9
31.8

M@100
27.9
39.0

Table 4: Retrieval performance on the dev fold of OpenCOVID-QA-2019.
To further investigate synthetic example generation, besides SQuAD, we also train the generator on a second
MRC dataset, namely, the Natural Questions (NQ) dataset
(Kwiatkowski et al. 2019). NQ contains information seeking questions from real-life users of Google search whereas
SQuAD contains well-formed questions created by annotators after looking at the passage. Thus these two datasets
contain distinct question styles; we explore which one yields
a better synthetic example generator for our application. Table 4 compares the performance of the NQ-style synthetic
examples with the SQuAD-style examples while adapting

Model
BM25 → Baseline MRC
(BM25 + DPR-Multi) → Baseline MRC
(BM25 + Adapted DPR) → Baseline MRC
(BM25 + Adapted DPR) → Adapted MRC

Open-COVID-QA-2019
Dev
Test
Top-1
Top-5
Top-1
Top-5
21.7
31.8
27.1
38.7
21.4
30.9
25.2
37.2
24.2 (0.9) 35.6 (0.3) 29.5 (0.1) 44.2 (0.2)
27.2 (0.9) 37.2 (0.2) 30.4 (0.3) 44.9 (0.1)

COVID-QA-111
Test
Top-1
Top-5
24.1
39.3
24.4
43.2
25.0 (0.2) 45.9 (0.8)
26.5 (0.5) 47.8 (0.8)

Table 5: End-to-end question answering F1 scores on the open version of COVID-QA-2019 and COVID-QA-111.
the DPR-Multi model. We can see from the results that using questions from a SQuAD-trained synthetic generator is
considerably better.

Machine Reading Comprehension
Table 6 shows results on the test sets of different MRC
datasets. Input to the models is a question and an annotated
document that contains an answer. The Adapted MRC model
incorporates both language modeling on the CORD-19 collection and synthetic MRC training. Over our state-of-theart open-domain MRC baseline, we see 2 and 3.7 F1 improvements on test sets of COVID-QA-2019 and COVIDQA-147, respectively.
Model
Baseline MRC
Adapted MRC

COVID-QA-2019
EM
F1
34.7
62.7
37.2 (0.4) 64.7 (0.1)

COVID-QA-147
EM
F1
8.8
31.0
11.3 (0.6) 34.7 (1.1)

Table 6: MRC performance on the test folds of two datasets.
To further demonstrate the improvements from language
modeling and using synthetic examples, we present in Table
7 the results on the COVID-QA-2019 dev set from incrementally applying the two domain adaptation strategies. We
see that both strategies yield performance gains. However,
it can be seen that using the synthetic MRC examples from
our generator contributes more, with 3.1 EM and 2.6 F1 increase vs 1.5 EM and 0.8 F1 improvement from language
modeling.
Model
Baseline MRC
+ Language Modeling on CORD-19
+ Adding SynQ during MRC training

EM
34.0
35.5
38.6

F1
59.4
60.2
62.8

Table 7: Machine reading comprehension performance on
the dev split of COVID-QA-2019.

End-to-End Question Answering
Finally, we combine different IR and MRC systems to create end-to-end QA systems. We measure the improvements
from our domain adaptation strategy in this setting, where
only the question is given as input for QA over the entire
corpus.
In the retrieval phase, we take the top K passages (K
tuned on dev) from the IR system. Each passage is then

passed to the MRC model to get the top answer and its MRC
score. Finally, we normalize the IR and MRC scores and
combine via a convex combination (IR weight = 0.7, tuned
on dev). We observe that using K=100 works best when IR
is BM25 only and K=40 works best for BM25 + Neural IR
systems. Table 5 shows the end-to-end F1 performance of
the combination of IR and MRC systems. We see that both
having a better retriever (BM25+Adapted DPR) and a better
MRC (Adpated MRC) model contribute to improvements in
end-to-end QA performance.
To verify the statistical significance of our end-to-end QA
results, we perform a paired t-test (Hsu and Lachenbruch
2005) on the Top-5 F1 scores for both datasets. Our final
end-to-end QA system is significantly better than the baseline system at p < 0.01.

Conclusion
We present an approach for zero-shot adaptation of an opendomain end-to-end question answering system to a target
domain, in this case COVID-19. We propose a novel example generation model that can produce synthetic training examples for both information retrieval and machine reading
comprehension. Importantly, our generation model, trained
using open-domain supervised QA data, is used to generate synthetic question-answer pairs in the target domain. By
running extensive evaluation experiments, we show that our
end-to-end QA model as well as its individual IR and MRC
components benefit from the synthetic examples.
Low-resource target domains can present significant challenges for natural language processing systems. Our work
shows that synthetic generation can be an effective domain
adaptation approach for QA. Future work will explore semisupervised and active learning approaches to determine how
a small amount supervision can further improve results.

References
Alberti, C.; Andor, D.; Pitler, E.; Devlin, J.; and Collins, M.
2019. Synthetic QA corpora generation with roundtrip consistency. arXiv preprint arXiv:1906.05416 .
Alberti, C.; Lee, K.; and Collins, M. 2019. A bert baseline
for the natural questions. arXiv preprint arXiv:1901.08634 .
Alsentzer, E.; Murphy, J.; Boag, W.; Weng, W.-H.; Jindi, D.;
Naumann, T.; and McDermott, M. 2019. Publicly Available
Clinical BERT Embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, 72–78.

Baudiš, P.; and Šedivỳ, J. 2015. Modeling of the question
answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, 222–228. Springer.
Beltagy, I.; Lo, K.; and Cohan, A. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLPIJCNLP), 3606–3611.
Berant, J.; Chou, A.; Frostig, R.; and Liang, P. 2013. Semantic parsing on freebase from question-answer pairs. In
Proceedings of the 2013 conference on empirical methods in
natural language processing, 1533–1544.
Chen, D.; Fisch, A.; Weston, J.; and Bordes, A. 2017. Reading Wikipedia to Answer Open-Domain Questions. In Association for Computational Linguistics (ACL).
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186.
Dong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang,
Y.; Gao, J.; Zhou, M.; and Hon, H.-W. 2019. Unified
Language Model Pre-training for Natural Language Understanding and Generation. In Proceedings of NeurIPS.
Du, X.; and Cardie, C. 2018. Harvesting Paragraph-level
Question-Answer Pairs from Wikipedia. In Proceedings of
the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1907–1917. Association for Computational Linguistics.
Gillick, D.; Kulkarni, S.; Lansing, L.; Presta, A.; Baldridge,
J.; Ie, E.; and Garcia-Olano, D. 2019. Learning Dense Representations for Entity Retrieval. In Proceedings of the 23rd
Conference on Computational Natural Language Learning
(CoNLL), 528–537.
Gururangan, S.; Marasović, A.; Swayamdipta, S.; Lo, K.;
Beltagy, I.; Downey, D.; and Smith, N. A. 2020. Don’t Stop
Pretraining: Adapt Language Models to Domains and Tasks.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 8342–8360.
Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.W. 2020. Realm: Retrieval-augmented language model pretraining. arXiv preprint arXiv:2002.08909 .
Hsu, H.; and Lachenbruch, P. A. 2005. Paired t test. Encyclopedia of Biostatistics 6.
Izacard, G.; and Grave, E. 2020. Leveraging Passage Retrieval with Generative Models for Open Domain Question
Answering. arXiv preprint arXiv:2007.01282 .
Jiang, J.; and Zhai, C. 2007. Instance weighting for domain
adaptation in NLP. In Proceedings of the 45th annual meeting of the association of computational linguistics, 264–271.

Johnson, J.; Douze, M.; and Jégou, H. 2019. Billion-scale
similarity search with GPUs. IEEE Transactions on Big
Data .
Joshi, M.; Choi, E.; Weld, D. S.; and Zettlemoyer, L. 2017.
TriviaQA: A Large Scale Distantly Supervised Challenge
Dataset for Reading Comprehension. In Proceedings of the
55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 1601–1611.
Karpukhin, V.; Oğuz, B.; Min, S.; Wu, L.; Edunov, S.;
Chen, D.; and Yih, W.-t. 2020. Dense Passage Retrieval
for Open-Domain Question Answering. arXiv preprint
arXiv:2004.04906 .
Koch, G.; Zemel, R.; and Salakhutdinov, R. 2015. Siamese
neural networks for one-shot image recognition. In ICML
deep learning workshop, volume 2. Lille.
Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.;
Parikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin,
J.; Lee, K.; et al. 2019. Natural questions: a benchmark for
question answering research. Transactions of the Association for Computational Linguistics 7: 453–466.
Lee, J.; Yi, S. S.; Jeong, M.; Sung, M.; Yoon, W.; Choi,
Y.; Ko, M.; and Kang, J. 2020a. Answering Questions on
COVID-19 in Real-Time. arXiv preprint arXiv:2006.15830
.
Lee, J.; Yoon, W.; Kim, S.; Kim, D.; Kim, S.; So, C.;
and Kang, J. 2020b. BioBERT: a pre-trained biomedical
language representation model for biomedical text mining.
Bioinformatics (Oxford, England) 36(4): 1234.
Lee, K.; Chang, M.-W.; and Toutanova, K. 2019. Latent Retrieval for Weakly Supervised Open Domain Question Answering. In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, 6086–6096.
Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.; Stoyanov, V.; and Zettlemoyer, L.
2020a.
BART: Denoising Sequence-to-Sequence Pretraining for Natural Language Generation, Translation, and
Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 7871–
7880. Association for Computational Linguistics.
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,
V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.;
Rocktäschel, T.; et al. 2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint
arXiv:2005.11401 .
Liu, D.; Gong, Y.; Fu, J.; Yan, Y.; Chen, J.; Jiang, D.;
Lv, J.; and Duan, N. 2020. RikiNet: Reading Wikipedia
Pages for Natural Question Answering. arXiv preprint
arXiv:2004.14560 .
Liu, M.; Song, Y.; Zou, H.; and Zhang, T. 2019a. Reinforced training data selection for domain adaptation. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, 1957–1968.
Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,
O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019b.

Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692 .
Möller, T.; Reina, G. A.; Jayakumar, R.; and Livermore,
L. 2020. COVID-QA: A Question Answering Dataset for
COVID-19 .
Ott, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;
Grangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensible
Toolkit for Sequence Modeling. In Proceedings of NAACLHLT 2019: Demonstrations.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and
Sutskever, I. 2019. Language Models are Unsupervised
Multitask Learners .
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2019. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683 .
Rajpurkar, P.; Jia, R.; and Liang, P. 2018. Know What You
Don’t Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), 784–
789.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
SQuAD: 100,000+ Questions for Machine Comprehension
of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2383–2392.
Robertson, S.; and Zaragoza, H. 2009. The Probabilistic
Relevance Framework: BM25 and Beyond. Foundations
and Trends in Information Retrieval 3(4): 333–389.
Seo, M.; Lee, J.; Kwiatkowski, T.; Parikh, A.; Farhadi, A.;
and Hajishirzi, H. 2019. Real-Time Open-Domain Question
Answering with Dense-Sparse Phrase Index. In Proceedings
of the 57th Annual Meeting of the Association for Computational Linguistics, 4430–4441.
Sultan, M. A.; Chandel, S.; Fernandez Astudillo, R.; and
Castelli, V. 2020. On the Importance of Diversity in Question Generation for QA. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics,
5651–5656. Association for Computational Linguistics.
Tang, R.; Nogueira, R.; Zhang, E.; Gupta, N.; Cam, P.;
Cho, K.; and Lin, J. 2020. Rapidly Bootstrapping a Question Answering Dataset for COVID-19. arXiv preprint
arXiv:2004.11339 .
Tuan, L. A.; Shah, D. J.; and Barzilay, R. 2020. Capturing
Greater Context for Question Generation. In Proceedings of
AAAI.
Wang, L. L.; Lo, K.; Chandrasekhar, Y.; Reas, R.; Yang, J.;
Eide, D.; Funk, K.; Kinney, R.; Liu, Z.; Merrill, W.; et al.
2020. CORD-19: The Covid-19 Open Research Dataset.
ArXiv .
Wiese, G.; Weissenborn, D.; and Neves, M. 2017. Neural
Domain Adaptation for Biomedical Question Answering. In
Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), 281–289.

Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;
Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu,
J.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest,
Q.; and Rush, A. M. 2019. HuggingFace’s Transformers: State-of-the-art Natural Language Processing. ArXiv
abs/1910.03771.
Yih, W.-t.; Toutanova, K.; Platt, J. C.; and Meek, C. 2011.
Learning Discriminative Projections for Text Similarity
Measures. CoNLL-2011 247.
Zhang, E.; Gupta, N.; Tang, R.; Han, X.; Pradeep, R.; Lu,
K.; Zhang, Y.; Nogueira, R.; Cho, K.; Fang, H.; et al. 2020.
Covidex: Neural Ranking Models and Keyword Search
Infrastructure for the COVID-19 Open Research Dataset.
arXiv preprint arXiv:2007.07846 .

Appendix
Here we describe the open-domain datasets we use and also
list out the hyperparameter values for all of our experiments.

Open-Domain Datasets
We use SQuAD1.1 to train the synthetic example generator. SQuAD2.0 and Natural Questions are used to fine-tune
the machine reading comprehension (MRC) model. Finally,
we use Open-NQ to fine-tune the information retrieval (IR)
model.
Natural Questions NQ (Kwiatkowski et al. 2019) is an
English MRC benchmark which contains questions from
Google users, and requires systems to read and comprehend
entire Wikipedia articles. The dataset contains 307,373 instances in the train set, 7,830 examples in the dev set and
7842 in a blind test set. Lee, Chang, and Toutanova (2019)
create an open version of this dataset, called Open-NQ,
wherein they only keep questions with short answers and
discard the given evidence document. This open version
contains 79,168, 8,757 and 3,610 examples in the train, dev
and test set, respectively.

Hyperparameter
Learning rate
Epochs
Batch size
Max sequence length
Masking rate

Value
1.5e-4
8
256
512
0.15

Table 10: Hyperparameter settings during masked language
modeling on the CORD-19 collection.

SQuAD SQuAD1.1 (Rajpurkar et al. 2016) is an extractive MRC dataset containing questions posed by crowdworkers on a set of Wikipedia articles. All the questions are
answerable, with 87,599, 10,570 and 9,533 examples in the
train, dev and test set, respectively. SQuAD2.0 (Rajpurkar,
Jia, and Liang 2018) combines the 100,000+ questions in
SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. It contains 130,319, 11,873 and 8,862 examples
in the train, dev and test set, respectively.

Hyperparameters
Tables 8, 9, 10, 11 list the hyperparameters for training
the example generation, IR, masked language modeling and
MRC models, respectively.
Hyperparameter
Learning rate
Epochs
Batch size
Max source + target sequence length

Value
3e-5
3
24
1024

Table 8: Hyperparameter settings during training the synthetic example generator on SQuAD1.1.

Hyperparameter
Learning rate
Epochs
Batch size
Warm-up steps
Max sequence length

ICT
1e-5
6
128
1237
350

Adapted DPR
1e-5
6
128
1237
350

Table 9: Hyperparameter settings for the IR experiments in
fine-tuning the DPR model with different adaption strategies.

Hyperparameter
Learning rate
Epochs
Batch size
Max sequence length
Max question length
Document stride

SQuAD2.0
3e-5
2
8
384
64
128

SynQ
1.6e-5
1
48
512
18
192

NQ
1.6e-5
1
48
512
18
192

Table 11: Hyperparameter settings during MRC fine-tuning
of the language model.

