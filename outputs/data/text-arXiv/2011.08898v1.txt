Probing Fairness of Mobile Ocular Biometrics
Methods Across Gender on VISOB 2.0 Dataset

arXiv:2011.08898v1 [cs.CV] 17 Nov 2020

Anoop Krishnan∗ , Ali Almadan? , and Ajita Rattani??
Department of Electrical Eng. and Computer Science
Wichita State University, USA
{axupendrannair,aaalmadan}@shockers.wichita.edu;
ajita.rattani@wichita.edu

Abstract. Recent research has questioned the fairness of face-based
recognition and attribute classification methods (such as gender and
race) for dark-skinned people and women. Ocular biometrics in the visible spectrum is an alternate solution over face biometrics, thanks to its
accuracy, security, robustness against facial expression, and ease of use
in mobile devices. With the recent COVID-19 crisis, ocular biometrics
has a further advantage over face biometrics in the presence of a mask.
However, fairness of ocular biometrics has not been studied till now.
This first study aims to explore the fairness of ocular-based authentication and gender classification methods across males and females. To this
aim, VISOB 2.0 dataset, along with its gender annotations, is used for
the fairness analysis of ocular biometrics methods based on ResNet-50,
MobileNet-V2 and lightCNN-29 models. Experimental results suggest
the equivalent performance of males and females for ocular-based mobile user-authentication in terms of genuine match rate (GMR) at lower
false match rates (FMRs) and an overall Area Under Curve (AUC). For
instance, an AUC of 0.96 for females and 0.95 for males was obtained for
lightCNN-29 on an average. However, males significantly outperformed
females in deep learning based gender classification models based on
ocular-region.
Keywords: Fairness and Bias in AI · Mobile Ocular Biometrics · Deep
Learning.

1

Introduction

With AI and computer vision reaching an inflection point, face biometrics is
widely adopted for recognizing identities, surveillance, border control, and mobile
user authentication with Apple introducing Face ID moniker in iPhone X1 . The
wide-scale integration of biometrics technology in mobile devices facilitate enhanced security in a user login, payment transaction, and eCommerce. Over the
?
??
1

Both the authors contributed equally.
Corresponding author.
https://www.apple.com/iphone/

2

Corresponding author: Ajita Rattani (ajita.rattani@wichita.edu).

last few years, fairness of these automated face-based recognition [1,12,5,23] and
gender classification methods have been questioned [11,4,14] across demographic
variations. Fairness is defined as the absence of any prejudice or favoritism toward a group based on their inherent or acquired characteristics. Specifically,
the majority of these studies raise the concern of higher error rates of face-based
recognition and gender2 classification methods3 for darker-skinned people like
African-American, and for women.
Speculated causes of the difference in the accuracy rates are skin-tone, makeup, facial expression change rate, pose, and illumination variations for face biometrics. Further, there has been a recent push for alternate solutions for face
biometrics due to a significant drop in its performance in the presence of occlusion, such as mask amid COVID-19 [6]. Recent 2020 NIST study [15] suggests
the presence of a mask could cause a face recognition system to fail up to 50%.

Fig. 1: An ocular image labeled with vasculature pattern, eyebrow, eyelids, eyelashes, and periocular skin texture.

Ocular biometrics in the visible spectrum offers a perfect alternate solution
over the face and can be acquired using the front-facing RGB camera already
installed in the mobile device [19,20,17]. It comprises of scanning regions in
the eye and those around it, i.e., iris, conjunctival and episcleral vasculature
and periocular region for person authentication. Figure 1 shows an ocular image
labeled with vasculature pattern, eyebrow, eyelids, eyelashes, and periocular skin
texture. It has obtained significant attention from the research community due
to its accuracy, security, robustness against facial expressions, and ease of use in
mobile device. The use of ocular biometrics technology in the mobile device is
termed as mobile ocular biometrics [19,13].
With advances in deep learning, deeply coupled autoencoders and convolutional neural networks (CNNs) have been trained from scratch and re-purposed
for mobile ocular recognition4 [21,19]. Thorough evaluation of fine-tuned CNNs
2

3
4

The term “sex” would be more appropriate, but in consistency with the existing
studies, the term “gender” is used in this paper.
The term “methods”, “algorithms” and “models” are used interchangeably.
The term “recognition” and “user authentication” are used interchangeably.

Fairness of Mobile Ocular Biometrics

3

suggests efficacy of ResNet-50, LightCNN and MobileNet in mobile ocular recognition [21]. Datasets such as MICHE-I [7] (92 subjects) and VISOB 1.0 [16] (550
subjects) have been assembled for ocular recognition in mobile devices. VISOB
1.0 dataset was used in the IEEE 2016 ICIP international competition for mobile ocular biometrics. Studies in [18,3,19] also suggested the efficacy of deep
learning-based methods for gender classification from the ocular region in the
visible spectrum acquired using a mobile device. The reported results obtained
from fine-tuned CNNs suggest that equivalent performance could be obtained
in gender classification (with an accuracy of about 85%) from the ocular region
over face biometrics.
Recent interest has been in using subject-independent evaluation of these
ocular recognition methods where subjects do not overlap between the training
and testing set to simulate realistic scenarios. To this front, VISOB 2.0 competition [16] in IEEE WCCI 2020 conference has been organized using VISOB 2.0
database. VISOB 2.0 [16] is a new version of the VISOB 1.0 dataset where the
region of interest is extended from the eye (iris, conjunctival, and episcleral vasculature) to periocular (a region encompassing the eye). Further, the evaluation
protocol followed is subject-independent, over subject-dependent evaluation in
IEEE ICIP VISOB 1.0 competition [20]. Furthermore, instead of a single frame
eye image in VISOB 1.0 [20], the data sample consists of a stack of five images
captured in burst mode to facilitate multi-frame analysis.
However, to date, the fairness of these deep learning based mobile ocular biometrics analysis models (such as ResNet-50, LightCNN and MobileNet) has not
been evaluated. It is not known whether ocular biometrics also have an advantage over face biometrics in terms of performance across demographic variations.
The aim of this paper is to evaluate the fairness of ocular-based recognition and
gender classification models across males and females from images acquired using mobile devices. In the context of this study, fairness is defined as equivalent
error rates (or accuracy rates) for user authentication and gender classification
across males and females. To the best of our knowledge, this is the first study of
its kind. To this aim, the contributions of this paper are as follows:
– Evaluation of the fairness of deep learning-based methods for mobile ocularbased user authentication across males and females. To this front, performance of the fine-tuned version of ResNet-50 [8] and lightCNN-29 [24] have
been evaluated using Softmax and cosine loss functions (ArcFace, CosFace,
SphereFace, and AdaCos [2]) in three lighting conditions (office, dark and
daylight condition).
– Evaluation of the fairness of gender classification methods based on ocular
region across males and females. The performance of fine-tuned ResNet50 [8], MobileNet-V2 [9,22] and their ensemble have been evaluated in three
lighting conditions across gender.
All the experiments are conducted on VISOB 2.0 dataset [16], which facilitates subject-independent evaluation across three lighting conditions; office,
daylight, and dark light. This paper is organized as follows: section 2 details deep
learning architectures used in this study for ocular analysis. Section 3 discusses

4

Corresponding author: Ajita Rattani (ajita.rattani@wichita.edu).

the VISOB 2.0 training and testing dataset. Sections 4 and 5 discuss implementation details, and the obtained results on the fairness of the mobile ocular-based
user authentication and gender classification methods, respectively, across males
and females. Conclusions are drawn in section 6.

2

Convolutional Neural Networks (CNN) Models Used

We used the popular ResNet [8], mobile friendly lightCNN [24] and MobileNet [22]
based ocular analysis models for our evaluation. Efficacy of these models have already been established for mobile user recognition [21] and gender classification
from ocular region [18,3,19]. Experimental results are reported for only two best
models for user authentication and gender classification for the sake of space.
These models (networks) are described below as follows:
– ResNet: ResNet [8] is a short form of residual network based on the idea of
“identity shortcut connection” where input features may skip certain layers.
The residual or shortcut connections introduced in ResNet allow for identity
mappings to propagate around multiple nonlinear layers, preconditioning the
optimization and alleviating the vanishing gradient problem. In this study,
we used ResNet-50 model, which has 23.5M parameters.
– LightCNN: This model extensively uses the Max-Feature-Map (MFM) operation instead of ReLu activation, which acts as a feature filter after each
convolutional layer [24]. The operation takes two feature maps, eliminates
the element-wise minimums, and returns element-wise maximums. By doing so across feature channels, only 50% of the information-bearing nodes
from each layer reach the next layer. Consequently, during training, each
layer is forced to preserve only compact feature maps. Therefore, model parameters and the extracted features are significantly reduced. We used the
lightCNN-29 model consisting of 12K parameters in this study.
– MobileNet: MobileNet [9,22] is one of the most popular mobile-centric deep
learning architectures, which is not only small in size but also computationally efficient while achieving high performance. The main idea of MobileNet
is that instead of using regular 3 × 3 convolution filters, the operation is split
into depth-wise separable 3 × 3 convolution filters followed by 1 × 1 convolutions. While achieving the same filtering and combination process as a
regular convolution, the new architecture requires less number of operations
and parameters. In this study, we used MobileNet-V2 [22] which consist of
3.4M parameters.

3

VISOB 2.0 Dataset

In this section, we discuss VISOB 2.0 dataset along with the experimental protocol.
VISOB 2.0 [16] is the 2nd version of VISOB 1.0 dataset used in IEEE WCCI
competition 2020. This publicly available dataset consists of a stack of eye images

Fairness of Mobile Ocular Biometrics

5

captured using the burst mode via two mobile devices: Samsung Note 4 and
Oppo N1. During the data collection, the volunteers were asked to take their
selfie images in two visits, 2 to 4 weeks apart from each other. At each visit,
the selfie-like images were captured using the front-facing camera of the mobile
devices under three lighting conditions (daylight, office light, and dark light)
and two sessions (about 10 to 15 minutes apart). The stack consisting of five
consecutive eye images were extracted from the stack of full-face frames selected
such that the correlation coefficient between the center frame and the remaining
four images is greater than 90%. The face and eye landmarks are detected using
the Dlib library [10]. The eye crops were generated such that the width and
height of the crop are 2.5× that of eye width.
Training and testing subset: The subset of the VISOB 2.0 dataset consisting
of 150 subjects each for left and right eye (ocular regions) from two visits are
used as the training set. This set was provided to the participants at the IEEE
WCCI competition 2020. All the images from visit 1 and visit 2 (2-4 weeks apart)
under three lighting conditions are included in this training set.
In order to evaluate the submission for real-life scenarios, 100 subjects each
for left and right eye images are used as the testing set. All the stack of five
images per sample from two visits across three lighting conditions is available in
this set as well. We used a gender-balanced subset of the dataset for training and
testing the models for user-authentication and gender classification (detailed in
sections 4 and 5). This is in order to mitigate the impact of training and testing
set imbalance on the fairness of the models.

4

Fairness of Mobile Ocular Recognition Methods Across
Gender

In this section, we discuss the implementation of the models (networks) for
ocular-based user authentication evaluated across males and females. All the
implementations are done using Pytorch library (https://pytorch.org/).
4.1

Network Training and Implementation Details

ResNet-50 [8] and lightCNN-29 [24] are fine-tuned on the training subset of the
VISOB 2.0 dataset [16] using five different loss functions; Softmax and cosinebased (ArcFace, CosFace, SphereFace, and AdaCos [2]) for the first time for
ocular recognition. For ResNet-50 based on cosine loss functions (ArcFace, CosFace, SphereFace, and AdaCos [2]), batch normalization, drop-out, and fully
connected layers of 2048 and 512 are added after the last convolutional layer.
This is followed by the final output layer. In case of lightCNN-29, the layers
added after the last pooling layer: batch normalization, drop-out, and a fully
connected layer of 128 x 128 x 8 and 512, and followed by the output layer.
The angular margins are set to 0.50, 0.40, and 4.0 for ArcFace, CosFace, and
SphereFace. AdaCos adjusts its scale parameter automatically. SphereFace obtained equivalent performance with AdaCos and results are not included due to

6

Corresponding author: Ajita Rattani (ajita.rattani@wichita.edu).

space constraints. The ResNet-50 network is trained using Adam optimizer with
a batch size of 128 for 15 iterations, and lightCNN-29 using stochastic gradient
decent optimizer with a batch size of 64 and the same number of iterations as
ResNet-50. The learning rate was set to 0.001.
We used a gender-balanced subset of the training set of these models. Following IEEE WCCI competition protocol, left and right eye images are treated
as different identities. To this aim, 288 subjects consisting of left and right eye
individually (with 50% male and 50% female distribution) are randomly chosen.
The training set consists of 64K ocular images from all the lighting conditions
and the two visits. In particular, the number of samples from visit 1 is around
32K and 40K from visit 2 where each subject has 500 images. The models are
trained and validated on a split of 80/20 using samples from both the visits and
across all the lighting conditions for left and right eyes5 are used for training the
models.
The trained models are evaluated using a subject-independent testing set of
VISOB 2.0. For the purpose of this study, we used a gender-balanced version of
the test set as well. This results in a total of 21K ocular images for each gender
for Oppo device, and 15K images for Note-4 from 86 subjects for all the three
lighting conditions. The deep features of size 512−D are extracted from the fully
connected layers of these trained models for the evaluation. The deep features
from the samples in visit 1 and visit 2 are chosen as the template and query
pairs, respectively. The scores are computed in a pairwise fashion over a stack
of images and are averaged per template-query pair. Cosine similarity is used to
compute scores between deep features from a pair of template-query pair.
4.2

Experimental Results

In this section, we compare the verification performance of the ResNet-50 and
lightCNN-29 models trained using multiple loss functions on the gender-balanced
subset. Both models are evaluated in the same lighting conditions for both left
and right eye regions. Table 1 and 2 compare the Equal Error Rate (EER)
and Genuine Match Rate (GMR) across males and females at 1−4 , 1−3 , and 1−2
FMRs.
Tables 1 and 2 shows Equal Error Rates (EER) and Genuine Match Rates
(GMR) at three different False Match Rates (FMR). In general, both models
performed better for females than males. For instance, lightCNN-29 obtained an
average EER of 9.94 for females and 11.16 for males, respectively. ResNet-50 obtained higher EER of 17.47 for females and 20.15 for males over ResNet-50. However, both the genders obtained similar Genuine Match Rate (GMR) averaged
over both the models. For instance, lightCNN-29 obtained 36.94 and 37.69 for
females and males at GMR@1−4 FMR, respectively. At the same FMR, ResNet50 obtained GMR of 6.43 and about 5.44 for females and males, respectively.
However, females tends to outperform males remarkably at GMR@1−2 FMR.
This can be noticed for lightCNN-29, which obtained an average of 71.00 GMR
5

The term “eye” and “ocular region” are used interchangeably.

Fairness of Mobile Ocular Biometrics

7

on females compared to 66.87 on males. Similarly, a difference of about 7% was
obtained for ResNet-50 across females and males.
Across lighting conditions, females obtained equivalent EERs across dark
and office lighting conditions. For lightCNN model, females obtained an average
EER of 9.94 for dark and 9.43 for office light, respectively. For ResNet-50, EER
of 16.04 and 16.54 for dark and office light, respectively. On the other hand,
males performed the best in dark conditions for both models compared to other
lighting conditions. This can be observed as the EERs increased by about 4.5%
for daylight and 3% for office light compared to dark lighting condition.
The performance across gender for different loss functions varied depending
on the lighting conditions and the CNN model. AdaCos loss function results
in lower EER for females than males for both the models and across all the
lighting conditions. For instance, in dark condition, an overall increase in EER
of 0.964 for lightCNN-29 and 2.27 for ResNet-50 was observed. ArcFace also
performed better for females than males except in the case of lightCNN-29 in
dark conditions where the EER of females increased by 0.88. For other lighting
conditions, males obtained an average increase of 2.67 in EER over females. At
GMR@1−2 FMR, females obtained higher performance across many loss functions and lighting conditions, yet the performance for females dropped uniquely
across all loss functions in the dark conditions for lightCNN-29, except for AdaCos. The average drop at GMR@1−2 FMR was about 2.79% for ArcFace, CosFace, SphereFace, and Softmax where AdaCos declined for males by only 1.14%.
In daylight and office light conditions, a constant increase in GMR@1−2 FMR
for females was noticed.
Generally, the average AUC of both genders were equivalent for lightCNN29 (0.96 for females and 0.95 for males). In the case of ResNet-50, the average
AUC for females was 0.90, whereas males obtained an AUC of 0.87.

5

Fairness of Mobile Ocular-based Gender Classification
Methods

In this section, we evaluate the fairness of the gender classification models based
on the ocular region. Following the studies in [18,3,19], we fine-tuned ResNet-50,
MobileNet-v2 and their ensemble for gender classification. Next, we discuss the
implementation details and the obtained results.
5.1

Network training and Implementation details

ResNet-50 and MobileNet-V2 CNN models are fine-tuned on training subset of
VISOB 2.0 dataset. We also evaluated ensemble of ResNet-50 and MobileNetV2 models. For fine-tuning ResNet-50 and MobileNet-V2, fully connected layers
of 512 and 512 were added after the last convolutional layer, followed by the
final output layer. Ensemble of ResNet-50 and MobileNet-V2 was obtained by
concatenating their first fully connected layers (of size 1024), followed by the

8

Corresponding author: Ajita Rattani (ajita.rattani@wichita.edu).

final output layer. The above models were trained using an Adamax optimizer6
on a batch size of 128 for 100 epochs using an early stopping mechanism on the
validation set (80-20 split of training and validation). The learning rate was set
equal to 1e-4 and decay of 5e-4. In order to mitigate the impact of imbalanced
training and evaluation set on the fairness of the models. We used an almost
gender balanced subset of the VISOB 2.0 training set (shown in Table 3) for
these models training. Samples across both the visits (1 and 2) and all three
lighting conditions are used all together to train the models for left and right
eye, individually. Validation accuracy of about 90% was obtained for most of
the cases. The trained models are evaluated on a subject independent genderbalanced testing subset of the VISOB 2.0 dataset shown in Table 4. Results are
reported in terms of accuracy values across gender and lighting conditions for
the left and right eye, individually. Further, false positive rate (FPR), indicating
females misclassified as males, and false negative rate (FNR), indicating males
misclassified as females, are also reported for further insight.

5.2

Experimental Results

In this section, we report the gender classification accuracy of the ocular-based
models across males and females.
Tables 5, 6, 7, 8 shows the accuracy of the fine-tuned ResNet-50, MobileNetV2, and their ensemble across gender and lighting conditions for left and right
ocular images acquired using Note-4 and Oppo, individually. FPR and FNR
are also reported in these tables. For Note-4, the average gender classification
accuracy across lighting conditions is 79.72%, 81.62%, and 84.02% for ResNet50, MobileNet-V2 and their ensemble, respectively, when trained and tested on
the left ocular region (as can be seen from Table 5). Similarly, for the right
ocular region, Resnet-50 has the highest average accuracy of 83.98%, followed
by ensemble with an accuracy of 82.46% and MobileNet-v2 with an average
accuracy of 81.02% (as can be seen from Table 7).
Across gender for left ocular region acquired using Note4; males obtained the
highest average accuracy of 94.07% and the lowest of 89.4% for ResNet-50 and
MobileNet-V2, respectively. However, females obtained the highest of 75.03%
and the lowest of 64.2% for Ensemble and ResNet-50, respectively, averaged
over three different lighting conditions (Table 5). Similarly, for the right ocular
region, males obtained the highest average accuracy of 94.9% and the lowest
of 89.27% for ResNet-50 and Ensemble, respectively. However, females obtained
the highest of 77.93% and the lowest of 65.83% for MobileNet-v2 and ResNet-50,
respectively (see Table 7).
Average difference in the accuracy between males and females is 21.21% for
left ocular images acquired using Note-4. The average difference in the accuracy
between males and females is 21.75% for right ocular images acquired using
Note-4.
6

https://pytorch.org/docs/stable/optim.html

Fairness of Mobile Ocular Biometrics

9

For Oppo, the average gender classification across different lighting conditions
is 79.17%, 77.8%, and 80.42% for ResNet-50, MobileNet-V2 and their ensemble,
respectively, when trained and tested on left ocular region (as can be seen from
Table 6). Similarly for right ocular region, ResNet-50, MobileNet-v2 and their
ensemble obtained 80.49%, 84.89% and 83.09%, respectively (see Table 8).
Across gender for left ocular region acquired using Oppo; males obtained the
highest average accuracy of 95.4% and the lowest of 94.54% for MobileNet-V2
and ResNet-50, respectively. However, females obtained the highest of 65.13%
and the lowest of 59.19% for Ensemble and MobileNet-V2, respectively, averaged
over three different lighting conditions (refer Table 6). Similarly, for the right
ocular region, males obtained the highest average accuracy of 93.19% and the
lowest of 86.45% for ResNet-50 and MobileNet-v2, respectively. However, females
obtained the highest of 86.38% and the lowest of 67.23% for MobileNet-v2 and
ResNet-50, respectively (see Table 8). Better classification accuracy for Oppo
device is due to the higher resolution images of better quality compared to Note4. Also, in general, higher accuracy rates are obtained for samples acquired in
controlled lighting conditions, i.e., office light.
Average difference in the accuracy between males and females is 32.7% for
left ocular images acquired using Oppo. The average difference in the accuracy
between males and females is 14.68% for right ocular images for Oppo. Lowest
FPR (15.49%) and FNR (13.78%) are obtained for left ocular images under
dark lighting conditions acquired using Oppo. The lowest FPR (18.2%) and
FNR (6.8%) are obtained for left ocular images under office lighting conditions
for Note-4. Our results are in contrary to those obtained in [3] where females
outperformed males in gender classification based on ocular region. However, in
this study [3] ocular regions are cropped from Labeled Faces in the Wild dataset.
Further, based on manual inspection, we observed that covariates such as
eye-gazing, eyeglasses, obstructions, the presence of hair, and low lighting to be
the major factors contributing to the error rate of the gender classifier especially
for females. Figure 2 shows some of the female sample eye images misclassified
by the gender classification models.

6

Conclusion

This paper evaluates the fairness of the mobile user authentication and gender classification algorithms based on ocular region across males and females.
In contrary to the existing studies on face recognition, we obtained equivalent
authentication performance for males and females based on the ocular region at
lower FMR points (1−4 ) and an overall Area Under Curve (AUC). The reason
could be the robustness of the subject-specific templates of ocular region to facial expression change, make-up, and facial morphological differences over face
biometrics. However, males outperformed females by a significant difference of
22.58% in gender classification. This error rate was mainly due to the presence
of covariates such as hair, eyeglasses, motion blur, and eye gazing. As a part of
future work, experiments will be extended on other ocular biometric datasets

10

Corresponding author: Ajita Rattani (ajita.rattani@wichita.edu).

(a) Obstruction

(b) Closed eyelid

(c) Gazing

(d) Poor lighting

(e) Eyeglasses

(f) Motion blur

Fig. 2: Example of covariates in ocular images of females, commonly available in
mobile environment, and attributing to the error rate of the gender classifiers.

captured in the near-infrared and visible spectrum across gender, race and age.
The impact of the covariates and multi-frame fusion in unequal accuracy rates
of the ocular-based gender classifiers will be quantified.

7

Acknowledgment

Rattani is the co-organizer of the IEEE ICIP 2016 VISOB 1.0 and IEEE WCCI
2020 VISOB 2.0 mobile ocular biometric competitions. Authors would like to
thank Narsi Reddy and Mark Nguyen for their assistance in dataset processing.

References
1. Albiero, V., Zhang, K., Bowyer, K.W.: How does gender balance in training data
affect face recognition accuracy? (2020)
2. Almadan, A., Krishnan, A., Rattani, A.: Bwcface: Open-set face recognition using body-worn camera. In: 19th IEEE Intl Conference on Machine Learning and
Applications. pp. 1–8. Miami, Florida (2020)
3. Alonso-Fernandez, F., Diaz, K.H., Ramis, S., Perales, F.J., Bigun, J.: Softbiometrics estimation in the era of facial masks. In: 2020 IEEE BIOSIG. pp. 1–6
(2020)
4. Buolamwini, J., Gebru, T.: Gender shades: Intersectional accuracy disparities in
commercial gender classification. In: ACM Conference on Fairness, Accountability,
and Transparency. p. 77–91 (June 2018)
5. Cavazos, J.G., Phillips, P.J., Castillo, C.D., O’Toole, A.J.: Accuracy comparison
across face recognition algorithms: Where are we on measuring race bias? (2019)

Fairness of Mobile Ocular Biometrics

11

6. Damer, N., Grebe, J.H., Chen, C., Boutros, F., Kirchbuchner, F., Kuijper, A.: The
effect of wearing a mask on face recognition performance: an exploratory study.
arXiv preprint arXiv:2007.13521 (2020)
7. De Marsico, M., Nappi, M., Riccio, D., Wechsler, H.: Mobile iris challenge evaluation (miche)-i, biometric iris dataset and protocols. Pattern Recognition Letters
57, 17–23 (2015)
8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition
(2015)
9. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)
10. King, D.E.: Dlib-ml: A machine learning toolkit. J. Mach. Learn. Res. 10, 1755–
1758 (Dec 2009), http://dl.acm.org/citation.cfm?id=1577069.1755843
11. Krishnan, A., Almadan, A., Rattani, A.: Understanding fairness of gender classification algorithms across gender-race groups. In: 19th IEEE Intl Conference on
Machine Learning and Applications. pp. 1–8. Miami, Florida (2020)
12. Krishnapriya, K.S., Albiero, V., Vangara, K., King, M.C., Bowyer, K.W.: Issues
related to face recognition accuracy varying based on race and skin tone. IEEE
Transactions on Technology and Society 1(1), 8–20 (2020)
13. Lovisotto, G., Malik, R., Sluganovic, I., Roeschlin, M., Trueman, P., Martinovic,
I.: Mobile biometrics in financial services: A five factor framework. University of
Oxford, Oxford, UK (2017)
14. Muthukumar, V.: Color-theoretic experiments to understand unequal gender classification accuracy from face image. In: IEEE CVPRW (2019)
15. Ngan, M.L., Grother, P.J., Hanaoka, K.K.: Ongoing face recognition vendor test
(frvt) part 6a: Face recognition accuracy with masks using pre-covid-19 algorithms
(2020)
16. Nguyen, H., Reddy, N., Rattani, A., Derakhshani, R.: VISOB 2.0 - second international competition on mobile ocular biometric recognition. In: IAPR ICPR.
pp. 1–8. Rome, Italy (2020)
17. Raja, K., Ramachandra, R., Busch, C.: Collaborative representation of blur invariant deep sparse features for periocular recognition from smartphones. Image and
Vision Computing 101, 103979 (2020)
18. Rattani, A., Reddy, N., Derakhshani, R.: Convolutional neural networks for gender
prediction from smartphone-based ocular images. IET Biometrics 7(5), 423–430
(2018)
19. Rattani, A., Derakhshani, R., Ross, A.: Selfie biometrics. Advances and Challenges.
Cham: Springer Nature (2019)
20. Rattani, A., Derakhshani, R., Saripalle, S.K., Gottemukkula, V.: ICIP 2016 competition on mobile ocular biometric recognition. In: 2016 IEEE international conference on image processing (ICIP). pp. 320–324. IEEE (2016)
21. Reddy, N., Rattani, A., Derakhshani, R.: Comparison of deep learning models
for biometric-based mobile user authentication. In: 2018 IEEE 9th International
Conference on BTAS. pp. 1–6 (2018)
22. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.: Mobilenetv2: Inverted
residuals and linear bottlenecks. In: IEEE Conference on CVPR. pp. 4510–4520
(2018)
23. Singh, R., Agarwal, A., Singh, M., Nagpal, S., Vatsa, M.: On the robustness of face
recognition algorithms against attacks and bias (2020)
24. Wu, X., He, R., Sun, Z., Tan, T.: A light cnn for deep face representation with
noisy labels. IEEE TIFS 13(11), 2884–2896 (2018)

12

Corresponding author: Ajita Rattani (ajita.rattani@wichita.edu).

Table 1: EER(%), GMR@1−4 FMR, GMR@1−3 FMR, and GMR@1−2 FMR for
lightCNN-29 model (trained on gender balanced subset of VISOB 2.0) for five loss
functions and evaluated in different light conditions for males (M) and females (F) for
mobile user authentication. Gender-balanced training and testing subset of VISOB 2.0
are used.
Loss
Light
Function Condition

Eye

EER(%)

GMR(/%)@1-4FMR GMR(%)@1-3FMR GMR(%)@1-2FMR

L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R

M
7.73
4.26
4.93
4.66
6.39
5.54
17.21
13.12
9.01
4.53
11.10
6.37
10.37
9.36
11.38
6.16
22.93
17.71
11.93
11.38
5.95
6.49
3.14
4.44
8.69
3.35

F
3.43
4.15
3.19
2.84
2.20
2.61
14.25
11.04
5.34
6.16
7.87
9.63
11.53
7.93
8.81
8.22
19.96
16.45
9.56
10.57
9.27
3.92
5.02
2.60
8.01
6.90

M
29.04
67.25
55.96
76.40
38.99
72.22
11.84
39.15
39.38
64.48
17.25
53.03
49.17
60.18
51.65
60.37
8.35
25.96
31.56
45.87
47.19
64.61
72.29
73.16
34.96
73.16

L
R
Daylight L
R
Dark
L
R
ArcFace Office
L
R
Daylight L
R
Dark
L
R
CosFace Office
L
R
Daylight L
R
Dark
L
R
Softmax Office
L
R
Daylight L
R
Dark
L
R

9.68
14.32
12.42
15.51
16.10
13.39
21.24
20.42
16.42
16.71
11.93
12.03
13.29
12.25
14.12
11.64
20.21
15.22
18.14
12.90
9.93
10.48
6.38
8.13

10.46
11.08
10.71
11.87
10.98
12.22
18.72
19.70
14.05
13.57
7.97
8.48
9.39
7.67
10.55
10.99
15.50
13.08
15.58
9.31
8.56
7.24
8.84
9.17

28.31
17.99
23.82
21.20
28.58
25.80
12.53
9.36
19.83
11.74
21.36
39.81
22.23
49.53
30.37
53.57
15.09
19.20
27.70
41.43
34.41
32.50
20.69
40.44

AdaCos

Office
Daylight
Dark

ArcFace Office
Daylight
Dark
CosFace Office
Daylight
Dark
Softmax Office
Daylight
Dark
Dark

AdaCos

Office

lightCNN-29 - Note-4
F
M
F
50.33
46.25
58.89
56.76
76.56
69.48
65.42
71.90
78.39
64.79
81.69
80.12
53.36
56.43
65.61
79.85
77.82
86.75
12.85
25.65
28.52
11.71
48.30
29.85
31.49
59.12
39.00
55.82
71.27
68.27
21.84
38.26
40.77
39.94
60.28
65.16
34.02
61.38
46.30
54.58
70.92
73.75
34.30
58.26
40.49
58.73
71.10
65.82
17.53
14.40
25.40
10.01
33.49
25.21
13.03
44.13
34.68
39.66
60.64
57.88
30.96
62.45
44.35
64.26
71.54
71.00
45.10
81.71
60.51
78.29
81.28
84.56
33.36
43.51
54.97
61.76
79.11
70.53
lightCNN-29 - Oppo
27.10
36.20
47.07
28.10
27.04
43.57
21.04
41.44
42.17
35.32
30.35
43.68
20.04
38.90
39.62
22.41
35.37
36.65
12.12
18.91
26.17
10.22
17.68
22.25
18.14
29.12
31.44
17.09
19.23
28.63
32.26
37.21
56.92
33.17
49.35
57.27
55.53
39.81
66.19
40.47
62.43
50.93
45.86
41.93
58.66
44.15
60.95
55.69
8.28
19.09
26.20
22.24
26.59
32.46
47.25
39.63
58.34
43.56
50.86
53.47
29.67
47.85
41.97
42.84
46.94
52.25
44.46
42.74
60.23
39.50
53.73
57.20

M
72.06
87.53
84.29
90.37
78.93
85.79
49.17
64.64
78.93
90.21
63.39
77.98
74.95
82.39
72.75
86.97
30.09
51.93
64.31
74.50
78.35
90.37
91.77
90.58
77.06
90.37

F
87.02
87.15
92.42
93.84
93.15
95.38
44.60
54.08
82.15
79.52
76.76
77.05
68.42
82.53
74.79
85.46
42.92
48.91
68.98
76.96
75.54
87.38
82.27
93.42
77.56
81.66

63.16
51.96
60.02
46.90
52.30
60.39
30.09
32.01
49.43
32.57
59.98
65.31
61.53
71.69
60.99
71.76
34.40
47.98
52.27
64.48
73.88
61.07
77.51
77.10

71.48
70.02
69.51
63.21
70.83
60.71
43.13
44.22
54.34
49.23
78.64
79.59
79.11
74.08
77.61
70.83
53.23
54.82
71.30
70.31
70.33
74.98
74.45
76.55

Fairness of Mobile Ocular Biometrics

13

Table 2: EER(%), GMR@1−4 FMR, GMR@1−3 FMR, and GMR@1−2 FMR for ResNet50 model using five loss functions and evaluated in different light conditions across
males (M) and females (F) for mobile user authentication. Gender-balanced training
and testing subset of VISOB 2.0 are used.
Loss
Light
Function Condition

Eye

AdaCos

L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R

M
19.65
10.45
24.80
19.36
17.54
18.29
25.36
19.02
21.83
18.94
21.64
15.27
19.42
12.31
24.60
21.96
15.58
15.19
20.44
10.90
15.59
12.66
16.12
8.66

F
12.38
8.45
16.24
13.69
13.61
14.65
18.71
11.24
15.87
16.52
17.35
11.99
11.73
12.38
17.06
19.64
19.45
16.22
7.97
9.24
18.27
13.31
10.11
10.97

M
0.00
9.87
0.28
5.41
4.65
5.19
1.10
12.23
1.10
0.18
5.41
4.55
2.68
0.95
3.76
1.19
0.65
10.82
8.13
15.63
11.28
33.21
19.16
20.02

L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R
L
R

22.00
27.25
28.76
23.05
18.43
17.70
24.21
25.24
27.61
23.27
22.50
21.39
22.99
24.21
31.41
25.50
19.62
22.40
21.40
23.69
20.10
20.82
11.56
17.30

20.65
19.59
22.31
25.29
17.48
17.14
23.91
20.93
28.74
24.49
18.66
16.46
24.08
22.34
23.50
26.40
20.92
20.55
17.24
17.95
16.01
16.80
12.80
10.92

1.46
6.10
0.68
1.62
4.11
0.56
7.29
13.19
1.19
3.46
6.10
7.86
0.81
7.91
2.09
2.92
5.98
0.36
8.81
6.98
8.29
7.78
13.04
10.71

Office
Daylight
Dark

ArcFace Office
Daylight
Dark
CosFace Office
Daylight
Dark
Softmax Office
Daylight
Dark

AdaCos

Office
Daylight
Dark

ArcFace Office
Daylight
Dark
CosFace Office
Daylight
Dark
Softmax Office
Daylight
Dark

EER(%)

GMR(/%)@1-4FMR GMR(%)@1-3FMR GMR(%)@1-2FMR

ResNet-50 - Note-4
F
M
4.48
4.74
1.00
28.57
16.49
3.67
0.00
18.35
5.24
7.36
8.54
17.97
2.24
5.21
3.15
18.94
6.00
6.33
6.70
7.16
4.64
10.17
5.02
20.89
8.83
5.84
20.62
14.44
10.31
9.27
3.78
2.75
0.90
4.44
6.97
18.40
13.11
20.36
7.70
31.49
21.37
18.90
26.63
38.81
4.94
28.68
20.92
33.87
ResNet-50 - Oppo
8.31
11.78
3.00
10.91
3.25
1.66
2.73
10.59
3.87
14.19
3.42
13.73
4.20
19.61
3.11
18.56
1.39
7.89
4.40
10.66
3.83
13.32
2.41
14.09
0.02
1.24
0.72
12.51
2.54
9.55
1.55
15.13
3.71
15.43
2.37
7.38
9.01
17.61
9.20
10.96
13.48
21.65
8.13
18.88
14.87
26.36
18.10
22.10

F
12.38
15.66
24.27
2.27
14.88
17.87
7.18
26.97
13.50
27.10
11.52
19.20
15.68
31.79
18.93
9.92
3.89
21.00
32.54
20.88
29.80
38.81
19.37
32.84

M
27.86
58.41
19.08
41.93
19.05
51.62
22.65
38.12
34.59
30.83
29.55
46.43
19.10
55.56
26.61
26.97
27.38
48.70
43.96
59.19
45.50
55.87
44.70
65.48

F
41.63
59.84
48.55
31.35
40.39
46.87
25.76
61.18
45.36
50.90
30.59
48.12
49.47
56.43
44.89
38.05
19.00
41.85
62.12
60.84
47.80
65.72
52.88
58.86

18.39
14.86
12.05
12.45
9.82
5.11
15.42
12.66
8.20
16.45
11.49
9.01
0.70
2.42
9.75
4.44
23.96
14.92
18.81
20.09
26.91
18.31
27.55
33.39

32.52
23.32
19.31
30.08
43.14
32.62
36.36
30.92
21.90
32.13
27.31
29.64
29.07
26.99
24.03
31.81
36.04
27.82
38.74
25.59
40.85
42.76
49.72
43.49

36.09
38.69
28.97
35.16
42.42
32.18
33.72
36.41
26.28
34.68
31.50
37.05
16.79
34.56
28.38
32.58
43.56
35.96
41.59
44.10
48.55
42.73
50.00
53.70

14

Corresponding author: Ajita Rattani (ajita.rattani@wichita.edu).

Table 3: Almost gender-balanced subset of VISOB 2.0 dataset subset used for
training gender classification models.
Lighting
Condition

Left Eye

Right Eye

M
F
M
F
Dark
35,917 31,023 35,917 31,023
Daylight 36,095 30,742 36,095 30,742
Office
44,669 38,424 44,669 38,424

Table 4: Gender-balanced subject independent testing subset of VISOB 2.0
dataset used for gender classification model evaluation.
NOTE4
Oppo
Lighting
Left Eye Right Eye Left Eye Right Eye
Condition
M F M
F
M F M
F
Dark
1019 1020 1020 1020 1525 1525 1525 1525
Daylight
1300 1300 1300 1300 1590 1590 1590 1590
Office
1485 1485 1485 1485 2515 2515 2515 2515

Table 5: Gender classification accuracy rates of ResNet-50, MobileNet-V2 and
their ensemble across males (M) and females (F) in different lighting conditions,
when trained and test on left eye images acquired using Note-4.
ResNet-50
MobileNet-V2
Ensemble
M F Overall Acc. M F Overall Acc. M F Overall Acc.
Dark
98 59
78.71
83 79
80.88
93 74.5
83.93
Daylight 90.2 69
80.76
91.7 63
78.56
89.6 71.7
81.12
Office
94 64.6
79.69
93.5 76.6
85.43
94.2 78.9
87.01
FPR
FNR
FPR
FNR
FPR
FNR
Dark
29.4
3.2
20.3
17.8
21.4
8
Daylight 25.6
12.4
28.9
11.7
24
12.6
Office
27.3
8.5
20
7.8
18.2
6.8

Fairness of Mobile Ocular Biometrics

15

Table 6: Gender classification accuracy rates of ResNet-50, MobileNet-V2 and
their ensemble across males (M) and females (F) in different lighting conditions,
when trained and tested on left eye images acquired using Oppo.
ResNet-50
MobileNet-V2
Ensemble
M
F Overall Acc. M
F Overall Acc. M
F Overall Acc.
Dark
96.26 66.67
81.6
95.74 63.34
79.64
96.6 70
83.4
Daylight 91.76 60.75
77.31
93.2 58.05
76.71
92.07 62.45
77.41
Office
95.6 61.07
78.61
97.25 56.18
77.11
97.23 62.94
80.47
FPR
FNR
FPR
FNR
FPR
FNR
Dark
25.7
5.3
27.69
6.3
23.72
4.64
Daylight
29.96
11.9
31.0 3
0.104
28.96
11.26
Office
28.94
6.7
31.05
4.65
27.58
4.11

Table 7: Gender classification accuracy rates of ResNet-50, MobileNet-V2 and
their ensemble across males (M) and females (F) in different lighting conditions,
when trained and tested on right eye images acquired using Note-4.
ResNet-50
MobileNet-V2
Ensemble
M F Overall Acc. M F Overall Acc. M F Overall Acc.
Dark
91.3 70.3
80.83
80.5 81.6
81.02
89.7 76.2
82.94
Daylight 95 57
77.59
92 74
84.03
95.5 61.7
79.32
Office
98.4 70.2
84.64
95.3 78.2
86.89
98.2 71
85.12
FPR
FNR
FPR
FNR
FPR
FNR
Dark
24.6
11
18.6
19.3
21
12
Daylight 31.3
8.55
21.9
9.6
28.6
6.7
Office
23.2
2.2
18.6
5.7
22.8
2.5

Table 8: Gender classification accuracy rates of ResNet-50, MobileNet-V2 and
their ensemble across males (M) and females (F) in different lighting conditions,
when trained and tested on right eye images acquired using Oppo.
ResNet-50
MobileNet-V2
Ensemble
M
F Overall Acc. M
F Overall Acc. M
F Overall Acc.
Dark
92.45 67.47
80.04
81.97 90.82
86.33
90.36 77.9
84.19
Daylight 91.26 66.3
79.22
85.03 81.44
83.7
89.94 72.96
81.49
Office
95.86 67.91
82.21
92.36 76.38
84.63
95.6 71
83.59
FPR
FNR
FPR
FNR
FPR
FNR
Dark
26.02
10.05
10.07
16.57
19.65
11.01
Daylight
26.97
11.65
17.91
15.52
23.11
12.12
Office
25.07
5.74
20.34
9.086
23.3
5.85

