An artificial intelligence system for predicting the
deterioration of COVID-19 patients in the emergency
department

arXiv:2008.01774v2 [cs.LG] 4 Nov 2020

Farah E. Shamout1,∗ , Yiqiu Shen2,∗ , Nan Wu2,∗ , Aakash Kaku2,∗ , Jungkyu Park3,8∗ ,
Taro Makino3,2,∗ , Stanisław Jastrz˛ebski3,4,2 , Jan Witowski3,4 , Duo Wang5 , Ben Zhang5 ,
Siddhant Dogra3 , Meng Cao6 , Narges Razavian5,3,2 , David Kudlowitz6 , Lea Azour3 ,
William Moore3 , Yvonne W. Lui3,4 , Yindalon Aphinyanaphongs5 ,
Carlos Fernandez-Granda2,7 , Krzysztof J. Geras3,4,2,
1

Engineering Division, NYU Abu Dhabi
Center for Data Science, New York University
3
Department of Radiology, NYU Langone Health
4
Center for Advanced Imaging Innovation and Research, NYU Langone Health
5
Department of Population Health, NYU Langone Health
6
Department of Medicine, NYU Langone Health
7
Department of Mathematics, Courant Institute, New York University
8
Vilcek Institute of Graduate Biomedical Sciences, NYU Grossman School of Medicine
∗
Equal contribution
k.j.geras@nyu.edu
2

Abstract
During the coronavirus disease 2019 (COVID-19) pandemic, rapid and accurate
triage of patients at the emergency department is critical to inform decision-making.
We propose a data-driven approach for automatic prediction of deterioration risk
using a deep neural network that learns from chest X-ray images and a gradient
boosting model that learns from routine clinical variables. Our AI prognosis system,
trained using data from 3,661 patients, achieves an area under the receiver operating
characteristic curve (AUC) of 0.786 (95% CI: 0.745-0.830) when predicting deterioration within 96 hours. The deep neural network extracts informative areas of
chest X-ray images to assist clinicians in interpreting the predictions and performs
comparably to two radiologists in a reader study. In order to verify performance
in a real clinical setting, we silently deployed a preliminary version of the deep
neural network at New York University Langone Health during the first wave of
the pandemic, which produced accurate predictions in real-time. In summary, our
findings demonstrate the potential of the proposed system for assisting front-line
physicians in the triage of COVID-19 patients.

1

Introduction

In recent months, there has been a surge in patients presenting to the emergency department (ED)
with respiratory illnesses associated with the coronavirus disease 2019 (COVID-19) [1, 2]. Evaluating
the risk of deterioration of these patients to perform triage is crucial for clinical decision-making
and resource allocation [3]. While ED triage is difficult under normal circumstances [4, 5], during
a pandemic, strained hospital resources increase the challenge [2, 6]. This is compounded by our
incomplete understanding of COVID-19. Data-driven risk evaluation based on artificial intelligence
(AI) could, therefore, play an important role in streamlining ED triage.

As the primary complication of COVID-19 is pulmonary disease, such as pneumonia [7], chest X-ray
imaging is a first-line triage tool for COVID-19 patients [8]. Although other imaging modalities,
such as computed tomography (CT), provide higher resolution, chest X-ray imaging is less costly,
inflicts a lower radiation dose, and is easier to obtain without incurring the risk of contaminating
imaging equipment and disrupting radiologic services [9]. In addition, abnormalities in the chest
X-ray images of COVID-19 patients have been found to mirror abnormalities in CT scans [10].
Although the knowledge of the disease is rapidly evolving, the understanding of the correlation
between pulmonary parenchymal patterns visible in the chest X-ray images and clinical deterioration
remains limited. This motivates the use of machine learning approaches for risk stratification using
chest X-ray imaging, which may be able to learn such correlations automatically from data.
The majority of related previous work using imaging data of COVID-19 patients focus more on
diagnosis than prognosis [11, 12, 13, 14, 15, 16, 17, 18]. Prognostic models used for predicting
mortality, morbidity and other outcomes related to the disease course have a number of potential reallife applications, such as: consistently defining and triaging sick patients, alerting bed management
teams on expected demands, providing situational awareness across teams of individual patients,
and more general resource allocation [11]. Prior methodology for prognosis of COVID-19 patients
via machine learning mainly use routinely-collected clinical variables [2, 19] such as vital signs
and laboratory tests, which have long been established as strong predictors of deterioration [20, 21].
Some studies have proposed scoring systems for chest X-ray images to assess the severity and
progression of lung involvement using deep learning [22], or more commonly, through manual
clinical evaluation [7, 23, 24]. In general, the role of deep learning for the prognosis of COVID-19
patients using chest X-ray imaging has not yet been fully established. Using both the images and
the clinical variables in a single AI system also has not been studied before. We show that they
both contain complimentary information, which opens a new perspective on building prognostic AI
systems for COVID-19.
In this work, we present an AI system that performs an automatic evaluation of deterioration risk,
based on chest X-ray imaging, combined with other routinely collected non-imaging clinical variables.
The goal is to provide support for critical clinical decision-making involving patients arriving at the
ED in need of immediate care [2, 25], based on the need for efficient patient triage. The system is
based on chest X-ray imaging, while also incorporating other routinely collected non-imaging clinical
variables that are known to be strong predictors of deterioration.
Our AI system uses deep convolutional neural networks to perform risk evaluation from chest X-ray
images. In particular, we designed our imaging-based classifier based on the Globally-Aware Multiple
Instance Classifier (GMIC) [26, 27], denoted as COVID-GMIC, aiming for accurate performance and
interpretability. The system also learns from routinely collected clinical variables using a gradient
boosting model (GBM) [28], denoted as COVID-GBM. Both models were trained using a dataset
of 3,661 patients admitted to NYU Langone Health between March 3, 2020, and May 13, 2020. To
learn from both modalities, we combined the output predictions of COVID-GMIC and COVID-GBM
to predict each patient’s overall risk of deterioration over different time horizons, ranging from 24
to 96 hours. In addition, the system includes a model that predicts how the risk of deterioration is
expected to evolve over time by computing deterioration risk curves (DRC), in the spirit of survival
analysis [29], denoted as COVID-GMIC-DRC.
Our system is able to accurately predict the deterioration risk on a test set of new patients. It
achieves an area under the receiver operating characteristic curve (AUC) of 0.786 (95% CI: 0.7450.830), and an area under the precision-recall curve (PR AUC) of 0.517 (95% CI: 0.429-0.600) for
prediction of deterioration within 96 hours. Additionally, its estimated probability of the temporal risk
evolution discriminates effectively between patients, and is well-calibrated. The imaging-based model
achieves a comparable AUC to two experienced chest radiologists in a reader study, highlighting
the potential of our data-driven approach. In order to verify our system’s performance in a real
clinical setting, we silently deployed a preliminary version of it at NYU Langone Health during
the first wave of the pandemic, demonstrating that it can produce accurate predictions in real-time.
Overall, these results strongly suggest that our system is a viable and valuable tool to inform
triage of COVID-19 patients. For reproducibility, we published our code and the trained models at
https://github.com/nyukat/COVID-19_prognosis.

2

Figure 1: Overview of the AI system and the architecture of its deep learning component. a, Overview of
the AI system that assesses the patient’s risk of deterioration every time a chest X-ray image is collected in the
ED. We design two different models to process the chest X-ray images, both based on the GMIC neural network
architecture [26, 27]. The first model, COVID-GMIC, predicts the overall risk of deterioration within 24, 48,
72, and 96 hours, and computes saliency maps that highlight the regions of the image that most informed its
predictions. The predictions of COVID-GMIC are combined with predictions of a gradient boosting model [28]
that learns from routinely collected clinical variables, referred to as COVID-GBM. The second model, COVIDGMIC-DRC, predicts how the patient’s risk of deterioration evolves over time in the form of deterioration risk
curves. b, Architecture of COVID-GMIC. First, COVID-GMIC utilizes the global network to generate four
saliency maps that highlight the regions on the X-ray image that are predictive of the onset of adverse events
within 24, 48, 72, and 96 hours respectively. COVID-GMIC then applies a local network to extract fine-grained
visual details from these regions. Finally, it employs a fusion module that aggregates information from both the
global context and local details to make a holistic diagnosis.

3

a

b

Raw dataset
n=19,957
p=4,772
Chest X-ray images linked to radiology
reports & encounter information
n=19,165
p=4,625
Discharged patients
(including patients who had died in the
hospital)
n=13,952
p=4,316

Example 1

Example 2

Example 3

Chest X-rays collected prior to any
adverse events
n=7,692
p=4,241
Non-intubated based on manual
annotation
n=7,505
p=4,204

Example 4

Example 5

Training set
(images collected in the emergency
department and during inpatient
encounters)
n=5,224
p=2,943

Example 6

Test set
(images collected in the emergency
department only)
n=770
p=718

Figure 2: Illustrations of the dataset and the dataset flowchart. a, Examples of chest X-ray images in our
dataset. Example 1: Patient was discharged and experienced no adverse events (44 years old male). Example 2:
Patient was transferred to the ICU after 95 hours (71 years old male). Example 3: Patient was intubated after 72
hours (66 years old male). Example 4: Patient was transferred to the ICU after 48 hours (99 years old female).
Example 5: Patient was intubated after 24 hours (74 years old male). Example 6: Patient was transferred to
the ICU in 30 minutes (73 years old female). It is important to note that the extent of parenchymal disease
does not necessarily have a direct correlation with deterioration time. For example, Example 5 has less severe
parenchymal findings than Examples 3 and 4, but deteriorated faster. b, Flowchart showing how the inclusion
and exclusion criteria were applied to obtain the final training and test sets, where n represents the number of
chest X-ray exams, and p represents the number of unique patients. Specifically, we excluded 783 exams that
were not linked to any radiology report, nine exams that had missing encounter information, and 5,213 exams
from patients who were still hospitalised by May 13, 2020. To ensure that our system predicts deterioration
prior to its occurrence, we excluded 6,260 exams that were collected after an adverse event and 187 exams of
already intubated patients. The final dataset consisted of 7,502 chest X-ray exams corresponding to 4,204 unique
patients. We split the dataset at the patient level such that exams from the same patient exclusively appear either
in the training or test set. In the training set, we included exams that were collected both in the ED and during
inpatient encounters. Since the intended clinical use of our model is in the ED, the test set only includes exams
collected in the ED. This resulted in 5,224 exams (5,617 images) in the training set and 770 exams (832 images)
in the test set. We included both frontal and lateral images, however there were less than 50 lateral images in the
entire dataset.

2

Results

Dataset. Our AI system was developed and evaluated using a dataset of 19,957 chest X-ray exams
collected from 4,722 patients at NYU Langone Health between March 3, 2020 and May 13, 2020.1
The dataset consists of chest X-ray images collected from patients who tested positive for COVID-19
using the polymerase chain reaction (PCR) test, along with the clinical variables recorded closest
to the time of image acquisition (e.g. vital signs, laboratory test results, and patient characteristics).
Figure 2.a shows examples of chest X-ray images collected from different patients. We applied
inclusion and exclusion criteria that were defined in collaboration with clinical experts, as shown in
Figure 2.b. The training set consisting of 2,943 patients and 5,617 chest X-ray images was used for
model development and hyperparameter tuning, while the test set consisting of 718 patients and 832
images was used to report the final results. The training and the test sets were disjoint, with no patient
overlap. Table 1 summarizes the overall demographics and characteristics of the patient cohort in the
training and test sets, including distributions of the included clinical variables. The raw laboratory
test variables were further processed to extract the minimum and maximum statistics.
We define deterioration, the target to be predicted by our models, as the composite outcome of one of
three adverse events: intubation, admission to the intensive care unit (ICU), and in-hospital mortality.
If multiple adverse events occurred, we only consider the time of the first event.
Evaluation metrics. Throughout the paper we used AUC (area under the receiver operating characteristic curve) and PR AUC (area under the precision-recall curve), which offer a complimentary
view on the performance of our models. These metrics integrate the performance of the evaluated
1

This study was approved by the Institutional Review Board, with ID# i20-00858.

4

Table 1: Description of the characteristics of the patient cohort included in the training and test sets and the
mean and interquartile range statistics of the raw vital signs and laboratory test results used for COVID-GBM.
Note that n represents a counting unit. The training and test sets are similar in terms of age, BMI, and proportion
of females. We note that there is a higher proportion of chest X-ray images associated with deterioration across
all time windows in the test set compared to the training set. This implies that there is a higher incidence of
adverse events amongst ED patients than inpatients, since the test set only includes chest X-ray images collected
from ED patients, while the training set also includes inpatients.
Characteristic

Training set

Test set

Patients, n
Admissions, n
Females, n (%)
Age (years), mean (SD)
BMI (kg/m2 ), mean (SD)
Survived
Adverse events, n
Intubation, n
ICU admission, n
Mortality, n
Composite outcome, n
Chest X-ray exams, n
Composite outcome within 24 hours, n (%)
Composite outcome within 48 hours, n (%)
Composite outcome within 72 hours, n (%)
Composite outcome within 96 hours, n (%)
Total number of images, n
Vital signs, units
Heart rate, beats per minute
Respiratory rate, breaths per minute
Temperature, °F
Systolic blood pressure, mmHg
Diastolic blood pressure, mmHg
Oxygen saturation, %
Laboratory tests, units
Albumin, g/dL
ALT, U/L
AST, U/L
Total bilirubin, mg/dL
Blood urea nitrogen, mg/dL
Calcium, mg/dL
Chloride, mEq/L
Creatinine, mg/dL
D-dimer, ng/mL
Eosinophils, %
Eosinophils, n
Hematocrit, %
LDH, U/L
Lymphocytes, %
Lymphocytes, n
Platelet volume, fL
Neutrophils, n
Neutrophils, %
Platelet, n
Potassium, mmol/L
Procalcitonin, ng/mL
Total protein, g/dL
Sodium, mmol/L
Troponin, ng/mL

2,943
3,175
1,206 (41.0)
62.9 (17.2)
29.4 (7.0)
2,405
1,311
386
387
538
730
5,224
349 (6.7%)
553 (10.6%)
735 (14.1%)
876 (16.8%)
5,617

718
764
305 (42.5)
64.9 (17.2)
29.5 (8.6)
559
594
97
113
159
225
770
74 (9.6%)
101 (13.1%)
130 (16.9%)
156 (20.3%)
832

93.7 (25.0)
22.4 (7.0)
99.4 (1.9)
130.7 (30.0)
75.9 (17.0)
94.1 (4.0)

93.5 (27.0)
23.4 (7.0)
99.4 (1.9)
129.8 (29.3)
76.0 (18.0)
93.8 (5.0)

3.5 (0.9)
49.8 (32.0)
67.3 (37.0)
0.7 (0.4)
25.9 (17.0)
8.7 (0.8)
101.1 (7.0)
1.6 (0.7)
1,321.6 (535.5)
0.4 (0.0)
0.03 (0.00)
38.9 (7.3)
412.8 (207.0)
14.1 (10.0)
1.0 (0.7)
10.6 (1.4)
6.4 (4.0)
76.6 (14.0)
226.1 (114.0)
4.2 (0.8)
1.9 (0.3)
7.1 (1.1)
136.2 (6.0)
0.2 (0.1)

3.5 (0.9)
52.2 (36.0)
69.7 (43.0)
0.7 (0.4)
26.4 (18.0)
8.7 (0.8)
101.6 (7.0)
1.6 (0.7)
1,146.3 (618.5)
0.4 (0.0)
0.03 (0.00)
38.9 (7.5)
404.0 (213.0)
14.9 (11.0)
1.0 (0.7)
10.6 (1.4)
6.3 (3.8)
75.9 (13.0)
223.7 (103.0)
4.2 (0.8)
1.9 (0.4)
7.2 (1.0)
136.6 (7.0)
0.2 (0.1)

models over all possible thresholds for predictions to be considered positive. As there are no available
guidelines on how to select the threshold, we prefer these metrics to metrics that are computed for a
fixed threshold (i.e., F1 score or classification accuracy).
5

Model performance. Table 2 summarizes the performance of all the models in terms of the AUC
and PR AUC for the prediction of deterioration within 24, 48, 72, and 96 hours from the time of
the chest X-ray exam. The receiver operating characteristic curves and precision-recall curves can
be found in Supplementary Figure 4. For comparison, a logistic regression model achieves 0.698,
0.699, 0.712, and 0.728 AUC and 0.214, 0.266, 0.339, and 0.436 PR AUC across the 24, 48, 72,
and 96 hours windows, respectively. Our ensemble model consisting of COVID-GMIC and COVIDGBM achieves the best AUC performance across all time windows compared to COVID-GMIC and
COVID-GBM individually. This highlights the complementary role of chest X-ray images and routine
clinical variables in predicting deterioration. The weighting of the predictions of COVID-GMIC and
COVID-GBM was optimized on the validation set, as shown in Supplementary Figure 2.b. Similarly,
the ensemble of COVID-GMIC and COVID-GBM outperforms all models across all time windows
in terms of the PR AUC, except for the 96 hours window. The consistent advantage of the ensemble
model in our results is especially encouraging. Investigating more complex strategies for fusion of
information from these two modalities could further improve the results and this will be a subject of
our future research. Sample learning curves of COVID-GMIC are shown in Supplementary Figure 6
for reference.
Table 2: Performance of the outcome classification task on the held-out test set, and on the subset of the
test set used in the reader study. We include 95% confidence intervals estimated by 1,000 iterations of the
bootstrap method [30]. The optimal weights assigned to the COVID-GMIC prediction in the COVID-GMIC
and COVID-GBM ensemble were derived through optimizing the AUC on the validation set as described in
Supplementary Figure 2.b. The ensemble of COVID-GMIC and COVID-GBM, denoted as ‘COVID-GMIC +
COVID-GBM’, achieves the best performance across all time windows in terms of the AUC and PRAUC, except
for the PR AUC in the 96 hours task. In the reader study, our main finding is that COVID-GMIC outperforms
radiologists A & B across time windows longer than 24 hours, with 3 and 17 years of experience, respectively.
Note that the radiologists did not have access to clinical variables and as such their performance is not directly
comparable to the COVID-GBM model; we include it only for reference. The area under the precision-recall
curve is sensitive to class distribution, which explains the large differences between the scores on the test set and
the reader study subset.
Test set (n=832)
AUC

PR AUC

24 hours

48 hours

72 hours

96 hours

24 hours

48 hours

72 hours

96 hours

COVID-GBM

0.747
(0.698, 0.802)

0.739
(0.69, 0.795)

0.750
(0.703, 0.799)

0.770
(0.727, 0.813)

0.230
(0.139, 0.296)

0.325
(0.229, 0.396)

0.408
(0.317, 0.479)

0.523
(0.433, 0.6)

COVID-GMIC

0.695
(0.636, 0.763)

0.716
(0.666, 0.771)

0.717
(0.668, 0.773)

0.738
(0.695, 0.785)

0.200
(0.119, 0.260)

0.302
(0.209, 0.379)

0.374
(0.283, 0.452)

0.439
(0.346, 0.515)

COVID-GBM +
COVID-GMIC

0.765
(0.712, 0.817)

0.749
(0.700, 0.798)

0.769
(0.724, 0.818)

0.786
(0.745, 0.830)

0.243
(0.150, 0.299)

0.332
(0.237, 0.41)

0.439
(0.345, 0.527)

0.517
(0.429, 0.600)

Reader study dataset (n=200)
AUC

PR AUC

24 hours

48 hours

72 hours

96 hours

24 hours

48 hours

72 hours

96 hours

Radiologist A

0.613
(0.519, 0.705)

0.645
(0.571, 0.731)

0.691
(0.618, 0.77)

0.740
(0.674, 0.814)

0.346
(0.217, 0.441)

0.490
(0.367, 0.599)

0.640
(0.536, 0.745)

0.742
(0.657, 0.834)

Radiologist B

0.637
(0.547, 0.73)

0.636
(0.552, 0.716)

0.658
(0.588, 0.738)

0.713
(0.649, 0.786)

0.365
(0.229, 0.462)

0.460
(0.335, 0.56)

0.590
(0.492, 0.701)

0.704
(0.616, 0.805)

Radiologist A +
Radiologist B

0.642
(0.555, 0.729)

0.663
(0.589, 0.746)

0.692
(0.621, 0.766)

0.741
(0.678, 0.809)

0.403
(0.272, 0.52)

0.499
(0.380, 0.613)

0.609
(0.492, 0.711)

0.740
(0.650, 0.831)

COVID-GMIC

0.642
(0.554, 0.734)
0.704
(0.632, 0.784)

0.701
(0.627, 0.781)
0.719
(0.648, 0.794)

0.751
(0.685, 0.821)
0.750
(0.684, 0.821)

0.808
(0.75, 0.87)
0.787
(0.727, 0.850)

0.381
(0.235, 0.480)
0.411
(0.259, 0.518)

0.546
(0.421, 0.657)
0.537
(0.394, 0.64)

0.676
(0.564, 0.780)
0.668
(0.558, 0.77)

0.789
(0.699, 0.880)
0.804
(0.738, 0.884)

0.708
(0.637, 0.799)

0.702
(0.633, 0.775)

0.778
(0.719, 0.851)

0.819
(0.763, 0.885)

0.411
(0.279, 0.517)

0.500
(0.364, 0.601)

0.705
(0.599, 0.806)

0.808
(0.735, 0.898)

COVID-GBM
COVID-GBM +
COVID-GMIC

To illustrate the interpretability of COVID-GMIC, we show in Figure 3 the saliency maps for all time
windows (24, 48, 72, and 96 hours) computed for four examples from the test set. Across all four
examples, the saliency maps highlight regions that contain visual patterns such as airspace opacities
and consolidation, which are correlated with clinical deterioration [22, 24]. These saliency maps
are utilized to guide the extraction of six regions of interest (ROI) patches cropped from the entire
image, which are then associated with a score that indicates its relevance to the prediction task. We
also note that in the last example, the saliency maps highlight right mid to lower paramediastinal and
left mid-lung periphery, while missing the dense consolidation in the periphery of the right upper
lobe. This suggests that COVID-GMIC emphasizes only the most informative regions in the image,
6

while human experts can provide a more holistic interpretation covering the entire image. It might,
therefore, be useful to enhance GMIC through a classifier agnostic mechanism [31], which finds all
the useful evidence in the image, instead of solely the most discriminative part. We leave this for
future work.

Figure 3: Explainability of COVID-GMIC. From left to right: the original X-ray image, saliency maps for
clinical deterioration within 24, 48, 72, and 96 hours, locations of region-of-interest (ROI) patches, and ROI
patches with their associated attention scores. All four patients were admitted to the intensive care unit and were
intubated within 48 hours. In the first example, there are diffuse airspace opacities, though the saliency maps
primarily highlight the medial right basilar and peripheral left basilar opacities. Similarly, the two ROI patches (1
and 2) on the basilar region demonstrate comparable attention values, 0.49 and 0.46 respectively. In the second
example, the extensive left mid to upper-lung abnormality (ROI patch 1) is highlighted, which correlates with
the most extensive area of parenchymal consolidation. In the third example, the saliency maps highlight the left
mid lung (ROI patch 1) and right hilar/infrahilar regions (ROI patch 2) which show groundglass opacities. In the
last example, saliency maps highlight the right mid to lower paramediastinal (ROI patch 1 and 6) and left mid
lung periphery (ROI patch 2) as regions predictive of clinical deterioration within 96 hours.

Comparison to radiologists. We compared the performance of COVID-GMIC with two chest
radiologists from NYU Langone Health (with 3 and 17 years of experience) in a reader study with a
sample of 200 frontal chest X-ray exams from the test set. We used stratified sampling to improve
the representation of patients with a negative outcome in the reader study dataset. Specifically, we
randomly sampled the first 100 exams from patients that had an adverse event in the next 96 hours
from the time the exam was taken. The remaining 100 exams came from the complement of the test
set. We describe the design of the reader study in more detail in the Methods section.
As shown in Table 2, our main finding is that COVID-GMIC achieves a comparable performance to
radiologists across all time windows in terms of AUC and PR AUC, and outperforms radiologists for
48, 72, and 96 hours. For example, COVID-GMIC achieves AUC of 0.808 (95% CI, 0.746-0.866)
compared to AUC of 0.741 average AUC of both radiologists in the 96 hours prediction task. We
hypothesize that COVID-GMIC outperforms radiologists on this task due to the currently limited
clinical understanding of which pulmonary parenchymal patterns predict clinical deterioration, rather
than the severity of lung involvement [24]. Supplementary Figure 5 shows AUC and PR AUC curves
across all time windows.
Deterioration risk curves. We use a modified version of COVID-GMIC, referred to hereafter as
COVID-GMIC-DRC, to generate discretized deterioration risk curves (DRCs) which predict the
evaluation of the deterioration risk based on chest X-ray images. Figure 4.a shows the DRCs for all
the patients in the test set. The DRC represents the probability that the first adverse event occurs
before time t, where t is equal to 3, 12, 24, 48, 72, 96, 144, and 192 hours. The mean DRCs of
7

patients who deteriorate (red bold line) is significantly higher than the mean DRCs of patients who are
discharged without experiencing any adverse events (blue bold line). We evaluate the performance of
the model using the concordance index, which is computed on patients in the test set who experienced
adverse events. For a fixed time t the index equals the fraction of pairs of patients in the test data
for which the patient with the higher DRC value at t experiences an adverse event earlier. For t
equal to 96 hours, the concordance index is 0.713 (95% CI: 0.682-0.747), which demonstrates that
COVID-GMIC-DRC can effectively discriminate between patients. Other values of t yield similar
results, as reported in Supplementary Table 4. Sample learning curves of COVID-GMIC-DRC are
shown in Supplementary Figure 7 for reference.
Figure 4.b shows a reliability plot, which evaluates the calibration of the probabilities encoded in
the DRCs. The diagram compares the values of the estimated DRCs for the patients in the test set
with empirical probabilities that represent the true frequency of adverse events. To compute the
empirical probabilities, we divided the patients into deciles according to the value of the DRC at each
time t. We then computed the fraction of patients in each decile that suffered adverse events up to t.
The fraction is plotted against the mean DRC of the patients in the decile. The diagram shows that
these values are similar across the different values of t, meaning the model is well-calibrated (for
comparison, perfect calibration would correspond to the diagonal black dashed line).

Figure 4: Deterioration risk curves (DRCs) and reliability plot for COVID-GMIC-DRC. a, DRCs generated by the COVID-GMIC-DRC model for patients in the test set with (faded red lines) and without adverse
events (faded blue lines). The mean DRC for patients with adverse events (red dashed line) is higher than the
DRC for patients without adverse events (blue dashed line) at all times. The graph also includes the ground-truth
population DRC (black dashed line) computed from the test data. b, Reliability plot of the DRCs generated by
the COVID-GMIC-DRC model for patients in the test set. The empirical probabilities are computed by dividing
the patients into deciles according to the value of the DRC at each time t. The empirical probability equals
the fraction of patients in each decile that suffered adverse events up to t. This is plotted against the predicted
probability, which equals the mean DRC of the patients in the decile. Perfect calibration is indicated by the
diagonal black dashed line.

Prospective silent validation in a clinical setting. Our long-term goal is to deploy our system in
existing clinical workflows to assist clinicians. The clinical implementation of machine learning
models is a very challenging process, both from technical and organizational standpoints [32]. To test
the feasibility of deploying the AI system in the hospital, we silently deployed a preliminary version
of our AI system in the hospital system and let it operate in real-time beginning on May 22, 2020.
The deployed version includes 15 models that are based on DenseNet-121 architectures, and use only
chest X-ray images. The models were developed to predict deterioration within 96 hours using a
subset of our data collected prior to deployment from 3,425 patients. The models were serialized
and served with TensorFlow Serving components [33] on an Intel(R) Xeon(R) Gold 6154 CPU @
3.00GHz; no GPUs were used. Images are preprocessed as explained in the Methods section. Our
system produces predictions essentially in real-time - it takes approximately two seconds to extract
an image from the PACS system, apply the image preprocessing steps, and get the prediction of a
model as a TensorFlow [33] output.
A total of 375 exams were collected between May 22, 2020 and June 24, 2020. Of the 375 exams
collected between May 22, 2020 and June 24, 2020, 38 exams were associated with a positive 96 hour
deterioration outcome. An ensemble of the deployed models, obtained by averaging their predictions,
achieved an AUC of 0.717 (95% CI: 0.622-0.801) and a PR AUC of 0.289 (95% CI: 0.181-0.465).
These results are comparable to those obtained on a retrospective test set used for evaluation before
deployment, which are 0.748 (95% CI: 0.708-0.790) AUC and 0.365 (95% CI: 0.313-0.465) PR
8

AUC. The decrease in accuracy is expected and may indicate changes in the patient population and
treatment guidelines as the pandemic progressed. When practically deployed, our system would still
need periodical retraining with the latest data.

3

Discussion

In this work, we present an AI system that is able to predict deterioration of COVID-19 patients
presenting to the ED, where deterioration is defined as the composite outcome of mortality, intubation,
or ICU admission. The system aims to provide clinicians with a quantitative estimate of the risk
of deterioration, and how it is expected to evolve over time, in order to enable efficient triage and
prioritization of patients at the high risk of deterioration. The tool may be of particular interest for
pandemic hotspots where triage at admission is critical to allocate limited resources such as hospital
beds.
Recent studies have shown that chest X-ray images are useful for the diagnosis of COVID-19 [12, 13,
15, 19, 34]. Our work supplements those studies by demonstrating the significance of this imaging
modality for COVID-19 prognosis. Additionally, our results suggest that chest X-ray images and
routinely collected clinical variables contain complementary information, and that it is best to use
both to predict clinical deterioration. This builds upon existing prognostic research, which typically
focuses on developing risk prediction models using non-imaging variables extracted from electronic
health records [19, 35]. In Supplementary Table 3, we demonstrate that our models’ performance
can be improved by increasing the dataset size. The current dearth of prognosis models that use both
imaging and clinical variables may partly be due to the limited availability of large-scale datasets
including both data types and outcome labels, which is a key strength of our study. In order to assess
the clinical benefits of our approach, we conducted a reader study, and the results indicate that the
proposed system can perform comparably to radiologists. This highlights the potential of data-driven
tools for assisting the interpretation of X-ray images.
The proposed deep learning model, COVID-GMIC, provides visually intuitive saliency maps to help
clinicians interpret the model predictions [36].Existing work on COVID-19 often use external gradientbased algorithms, such as gradCAM [37], to interpret deep neural network classifiers [38, 39, 40].
However, visualizations generated by gradient-based methods are sensitive to minor perturbation in
input images, and could yield misleading interpretations [41]. In contrast, COVID-GMIC has an
inherently interpretable architecture that better retains localization information of the more informative
regions in the input images.
We performed prospective validation of an early version of our system through silent deployment
in an NYU Langone Health hospital. The results suggest that the implementation of our AI system
in the existing clinical workflows is feasible. Our model does not incur any overhead operational
costs on data collection, since chest X-ray images are routinely collected from COVID-19 patients.
Additionally, the model can process the image efficiently in real-time, without requiring extensive
computational resources such as GPUs. This is an important strength of our study, since very few
studies have implemented and prospectively validated risk prediction models in general [42].
Our approach has some limitations that will be addressed in future work. The silent deployment
was based only on the model that processes chest X-ray exams, and did not include routine clinical
variables, nor any interventions. In addition, further validation is required to assess whether the
system can improve key performance measures, such as patient outcomes, through prospective and
external validation across different hospitals and electronic health records systems.
Our system currently considers two data types, which are chest X-ray images and clinical variables.
Incorporating additional data from patient health records may further improve its performance. For
example, the inclusion of presenting symptoms using natural language processing has been shown to
improve the performance of a risk prediction model in the ED [25]. Although we focus on chest X-ray
images because pulmonary disease is the main complication associated with COVID-19, COVID-19
patients may also suffer poor outcomes due to non-pulmonary complications such as: non-pulmonary
thromboembolic events, stroke, and pediatric inflammatory syndromes [43, 44, 45]. This could
explain some of the false negatives incurred by our system; therefore, incorporating other types of
data that reflect non-pulmonary complications may also improve prognostic accuracy.
9

Our system was developed and evaluated using data collected from the NYU Langone Health in New
York, USA. Therefore, it is possible that our models overfit to the patient demographics and specific
configurations in the imaging acquisition devices of our dataset.
Our findings show the promise of data-driven AI systems in predicting the risk of deterioration for
COVID-19 patients, and highlights the importance of designing multi-modal AI systems capable of
processing different types of data. We anticipate that such tools will play an increasingly important
role in supporting clinical decision-making in the future.

4

Methods

Outline. In this section, we first introduce our image preprocessing pipeline then formulate the
adverse event prediction task and present our multi-modal approach which utilizes both chest X-ray
images and clinical variables. Next, we formally define deterioration risk curve (DRC) and introduce
our X-ray image-based approach to estimate DRC. Subsequently, we summarize the technical details
of model training and implementation. Lastly, we describe the design of the reader study.
Image Preprocessing. After extracting the images from DICOM files, we applied the following
preprocessing procedure. We first thresholded and normalized pixel values, and then cropped the
images to remove any zero-valued pixels surrounding the image. Then, we unified the dimensions of
all images by cropping the images outside the center and rescaling. We performed data augmentation
by applying random horizontal flipping (p = 0.5), random rotation (-45 to 45 degrees), and random
translation. Supplementary Figure 1 shows the distribution of the size of the images prior to data
augmentation, as well as examples of images before and after preprocessing.
Adverse event prediction. Our main goal is to predict clinical deterioration within four time
windows of 24, 48, 72, and 96 hours. We frame this as a multi-label classification task with binary
labels y = [y 24 , y 48 , y 72 , y 96 ] indicating clinical deterioration of a patient within the four time
windows. The probability of deterioration is estimated using two types of data associated with the
patient: a chest X-ray image, and routine clinical variables. We use two different machine learning
models for this task: COVID-GMIC to process chest X-ray images, and COVID-GBM to process
clinical variables. For each time window t ∈ Ta = {24, 48, 72, 96}, both models produce probability
t
t
estimates of clinical deterioration, ŷCOVID-GMIC
, ŷCOVID-GBM
∈ [0, 1].
In order to combine the predictions from COVID-GMIC and COVID-GBM, we employ the technique
of model ensembling [46]. Specifically, for each example, we compute a multi-modal prediction
ŷENSEMBLE as a linear combination of ŷCOVID-GMIC and ŷCOVID-GBM :
ŷENSEMBLE = λŷCOVID-GMIC + (1 − λ)ŷCOVID-GBM ,

(1)

where λ ∈ [0, 1] is a hyperparameter. We selected the best λ by optimizing the average of the AUC
and PR AUC on the validation set. In Supplementary Figure 2.b, we show the validation performance
of ŷENSEMBLE for varying λ.
Clinical variables model. The goal of the clinical variables model is to predict the risk of deterioration when the patient’s vital signs are measured. Thus, each prediction was computed using a set of
vital sign measurements, in addition to the patient’s most recent laboratory test results, age, weight,
and body mass index (BMI). The laboratory test results were represented as maximum and minimum
statistics of all values collected within 12 hours prior to the time of the vital sign measurement.
The feature sets of vital signs and laboratory tests were then processed using a gradient boosting
model [28] which we refer to as COVID-GBM. For the final ensemble prediction, ŷENSEMBLE , we
combined the COVID-GMIC prediction with the COVID-GBM prediction computed using the most
recently collected clinical variables prior to the chest X-ray exam. In cases where there were no clinical variables collected prior to the chest X-ray, we performed a mean imputation of the predictions
assigned to the validation set.
Chest X-ray image model. We process chest X-ray images using a deep convolutional neural
network model, which we call COVID-GMIC, based on the GMIC model [26, 27]. COVID-GMIC
has two desirable properties. First, COVID-GMIC generates interpretable saliency maps that highlight
regions in the X-ray images that correlate with clinical deterioration. Second, it possesses a local
10

module that is able to utilize high-resolution information in a memory-efficient manner. This
avoids aggressive downsampling of the input image, a technique that is commonly used on natural
images [47, 48], which may distort and blur informative visual patterns in chest X-ray images such
as basilar opacities and pulmonary consolidation. In Supplementary Table 1, we demonstrate that
COVID-GMIC achieves comparable results to DenseNet-121, a neural network model that is not
interpretable by design, but is commonly used for chest X-ray analysis [49, 50, 51, 52].
The architecture of COVID-GMIC is schematically depicted in Figure 1.b. COVID-GMIC processes
an X-ray image x ∈ RH,W (H and W denote the height and width) in three steps. First, the global
module helps COVID-GMIC learn an overall view of the X-ray image. Within this module, COVIDGMIC utilizes a global network fg to extract feature maps hg ∈ Rh,w,n , where h, w, and n denote
the height, width, and number of channels of the feature maps. The resolution of the feature maps is
chosen to be coarser than the resolution of the input image. For each time window t ∈ Ta , we apply
a 1×1 convolution layer with sigmoid activation to transform hg into a saliency map At ∈ Rh,w that
highlights regions on the X-ray image which correlate with clinical deterioration.2 Each element
Ati,j ∈ [0, 1] represents the contribution of the spatial location (i, j) in predicting the onset of adverse
events within time window t. In order to train fg , we use an aggregation function fagg : Rh,w 7→ [0, 1]
to transform all saliency maps At for all time windows t into classification predictions ŷglobal :
X
1
Ati,j ,
(2)
fagg (At ) =
+
|H |
+
(i,j)∈H

+

where H denotes the set containing the locations of the r% largest values in At , and r is a
hyperparameter.
The local module enables COVID-GMIC to selectively focus on a small set of informative regions.
As shown in Figure 1, COVID-GMIC utilizes the saliency maps, which contain the approximate
locations of informative regions, to retrieve six image patches from the input X-ray image, which
we call region-of-interest (ROI) patches. We refer the readers to supplementary note 6 for more
details about the ROI retrieval algorithm. Figure 3 shows some examples of ROI patches. To utilize
high-resolution information within each ROI patch x̃ ∈ R224,224 , COVID-GMIC applies a local
network fl , parameterized as a ResNet-18 [47], which produces a feature vector h̃k ∈ R512 from
each ROI patch. The predictive value of each ROI patch might vary significantly. Therefore, we
utilize the gated attention mechanism [53] to compute an attention score αk ∈ [0, 1] that indicates
the relevance of each ROI patch x̃ for the prediction task. To aggregate information from all ROI
patches, we compute an attention-weighted representation:
z=

6
X

αk h̃k .

(3)

k=1

The representation z is then passed into a fully connected layer with sigmoid activation to generate a
prediction ŷlocal . We refer the readers to Shen et al. [27] for further details.
The fusion module combines both global and local information to compute a final prediction. We
apply global max pooling to hg , and concatenate it with z to combine information from both saliency
maps and ROI patches. The concatenated representation is then fed into a fully connected layer with
sigmoid activation to produce the final prediction ŷfusion .
In our experiments, we chose H = W = 1024. Supplementary Table 1 shows that COVID-GMIC
achieves the best validation performance for this resolution. We parameterize fg as a ResNet-18 [47]
which yields feature maps hg with resolution h = w = 32, and number of channels n = 512. During
training, we optimize the loss function:
1 X
t
t
t
l(y, ŷglobal , ŷlocal , ŷfusion ) =
BCE(yt , ŷglobal
)+BCE(yt , ŷlocal
)+BCE(yt , ŷfusion
)+β|At |,
|Ta |
t∈Ta
(4)
where BCE denotes binary cross-entropy and β is a hyperparameter representing the relative weights
on an `1 -norm regularization term that promotes sparsity of the saliency maps. During inference, we
use ŷfusion as the final prediction generated by the model.
2

For visualization purposes, we apply nearest neighbor interpolation to upsample the saliency maps to match
the resolution of the original image.

11

Estimation of deterioration risk curves. The deterioration risk curve (DRC) represents the evolution of the deterioration risk over time for each patient. Let T denote the time of the first adverse
event. The DRC is defined as a discretized curve that equals the probability P (T ≤ ti ) of the first
adverse event occurring before time ti ∈ {ti |1 ≤ i ≤ 8}, where t1 = 3, t2 = 12, t3 = 24, t4 = 48,
t5 = 72, t6 = 96, t7 = 144, t8 = 192 (all times are in hours).
Following recent work on survival analysis via deep learning [54], we parameterize the DRC using a
vector of conditional probabilities p̂ ∈ R8 . The ith entry of this vector, p̂i , is equal to the conditional
probability of the adverse event happening before time ti given that no adverse event occurred before
time ti−1 , that is:3

P (T ≤ t1 ),
i = 1,
p̂i =
(5)
P (T ≤ ti | T > ti−1 ), 2 ≤ i ≤ 8.
Given an estimate of p̂, the DRC can be computed applying the chain rule:
DRC(ti ) = P (T ≤ ti )
= 1 − P (T > ti )
=1−

i
Y

P (T > tj | T > tj−1 )

j=1

=1−

i
Y

(1 − p̂j ).

(6)

j=1

We use the GMIC model to estimate the conditional probabilities p̂ from chest X-ray images. We
refer to this model as COVID-GMIC-DRC. As explained in the previous section, the GMIC model
has three different outputs corresponding to the global module, local module and fusion module.
When estimating conditional probabilities for the eight time intervals, we denote these outputs by
p̂global , p̂local , and p̂fusion . During inference, we use the output of the fusion module, p̂fusion , as the
final prediction of the conditional-probability vector p̂. We use an input resolution of H = W = 512
and parameterize fg as ResNet-34 [47]. The resulting feature maps hg have resolution h = w = 16
and number of channels n = 512. The results of an ablation study that evaluates the impact of input
resolution and compares COVID-GMIC-DRC to a model based on the Densenet-121 architecture,
are shown in the Supplementary Tables 1 and 4. During training, we minimize the following loss
function defined on a single example:
l(T, p̂global , p̂local , p̂fusion ) = ls (T, p̂global ) + ls (T, p̂local ) + ls (T, p̂fusion ) +

8
X

β|Am |,

(7)

m=0

where ls is the negative log-likelihood of the conditional probabilities. For a patient who had an
adverse event between ti−1 and ti (where t0 = 0), this negative log-likelihood is given by
ls (T, p̂) = − ln P (ti−1 ≤ T ≤ ti )
= − ln

i−1
Y

P (T > tj | T > tj−1 )P (T ≤ tj | T > ti−1 )

j=1

=−

i−1
X

ln(1 − p̂j ) − ln p̂i .

(8)

j=1

The framework can easily incorporate censored data corresponding to patients whose information is
not available after a certain point. The negative log-likelihood corresponding to a patient, who has no
3
The parameters in our implementation are the complementary probabilities q̂ = 1 − p̂, which is a
mathematically equivalent parameterization. We also include an additional parameter to account for patients
whose first adverse event occurs after 192 hours.

12

information after ti and no adverse events before ti , equals
ls (T, p̂) = − ln P (T > ti )
= − ln

i
Y

P (T > tj | T > tj−1 )

j=1

=−

i
X

ln(1 − p̂j ).

(9)

j=1

Note that each p̂i is estimated only using patients that have data available up to ti . The total negative
log-likelihood of the training set is equal to the sum of the individual negative log-likelihoods
corresponding to each patient, which makes it possible to perform minimization efficiently via
stochastic gradient descent. In contrast, deep learning models for survival analysis based on Cox
proportional hazards regression [55] require using the whole dataset to perform model updates [56,
57, 58], which is computationally infeasible when processing large image datasets.
Model training and selection. In this section, we discuss the experimental setup used for COVIDGMIC, COVID-GMIC-DRC, and COVID-GBM. The chest X-ray image models were implemented
in PyTorch [59] and trained using NVIDIA Tesla V100 GPUs. The clinical variables models were
implemented using the Python library LightGBM [28].
We initialized the weights of COVID-GMIC and COVID-GMIC-DRC by pretraining them on
the ChestX-ray14 dataset [60] (Supplementary Table 2 compares the performance of different
initialization strategies). We used Adam [61] with a minibatch size of eight to train the models on
our data. During the training and test stages, we applied a set of data transformations to the inputs
in order to make the model more robust to rotation and spatial translation. During the test stage,
we applied ten different augmentations to each image and used the average of their predictions in
order to further improve performance. We did not apply any data augmentation during the validation
stage since it introduces randomness, which can be confounding when determining whether or not
validation performance is improving.
We optimized the hyperparameters using random search [62]. For COVID-GMIC, we searched
for the learning rate η ∈ 10[−6,−4] on a logarithmic scale, the regularization hyperparameter β ∈
4 × 10[−6,−3] on a logarithmic scale, and the pooling threshold r ∈ [0.2, 0.8] on a linear scale. For
COVID-GMIC-DRC, based on the preliminary experiments, we fixed the learning rate to 1.25 ×
10−4 . We searched for the regularization hyperparameter, β ∈ 10[−6,−4] on a logarithmic scale,
and the pooling threshold r ∈ {0.2, 0.5, 0.8}. For COVID-GBM, we searched for the learning rate
η ∈ 10[−2,−1] on a logarithmic scale, the number of estimators e ∈ 10[2,3] on a logarithmic scale,
and the number of leaves l ∈ [5, 15] on a linear scale. For each hyperparameter configuration, we
performed Monte Carlo cross-validation [63] (we sampled 80% of the data for training and 20% of
the data was used for validation). We performed cross-validation using three different random splits
for each hyperparameter configuration. We then selected the top three hyperparameter configurations
based on the average validation performance across the three splits. Finally, we combined the nine
models from the top three hyperparameter configurations by averaging their predictions on the
held-out test set to evaluate the performance. This procedure is formally described in Supplementary
Algorithm 1.
Design of the reader study The reader study consists of 200 frontal chest X-ray exams from the
test set. We selected one exam per patient to increase the diversity of exams. We used stratified
sampling to ensure that a sufficient number of exams in the study corresponded to the least common
outcome (patients with adverse outcomes in the next 24 hours). In more detail, we oversampled
exams of patients who developed an adverse event by sampling the first 100 exams only from patients
from the test set that had an adverse outcome within the first 96 hours. The remaining 100 exams
came from the remaining patients in the test set. The radiologists were asked to assign the overall
probability of deterioration to each scan across all time windows of evaluation.
13

Acknowledgements
The authors would like to thank Mario Videna, Abdul Khaja and Michael Constantino for supporting
our computing environment, Philip P. Rodenbough (the NYUAD Writing Center) and Catriona C.
Geras for revising the manuscript, and Boyang Yu, Jimin Tan, Kyunghyun Cho and Matthew Muckley
for useful discussions. We also gratefully acknowledge the support of Nvidia Corporation with the
donation of some of the GPUs used in this research. This work was supported in part by grants from
the National Institutes of Health (P41EB017183, R01LM013316), the National Science Foundation
(HDR-1922658, HDR-1940097), and NYU Abu Dhabi.

Author Contributions
FES, YS, NW, AK, JP and TM designed and conducted the experiments with neural networks. FES,
NW, JP, SJ, TM and JW built the data preprocessing pipeline. FES, NR and BZ designed the clinical
variables model. SJ conducted the reader study and analyzed the data. SD and MC conducted
literature search. YL, DW, BZ and YA collected the data. DK, LA and WM analyzed the results from
a clinical perspective. YA, CFG and KJG supervised the execution of all elements of the project. All
authors provided critical feedback and helped shape the manuscript.

Competing Interests
The authors declare no competing interests.

Data Availability
The ImageNet dataset is available at http://www.image-net.org/. The ChestX-ray8 dataset is
available at https://nihcc.app.box.com/v/ChestXray-NIHCC. The COVID-19 X-ray images
and associated clinical variables from NYU Langone Health are not publicly available, but we provide
sample patients in our source code repository.

Code Availability
The code of the models in this study, along with their trained weights, are available at https:
//github.com/nyukat/COVID-19_prognosis.

References
[1] Baugh, J. J. et al. Creating a COVID-19 surge clinic to offload the emergency department. Am.
J. Emerg. Med. 38, 1535–1537 (2020).
[2] Debnath, S. et al. Machine learning to assist clinical decision-making during the COVID-19
pandemic. Bioelectron. Med. 6, 1–8 (2020).
[3] Whiteside, T., Kane, E., Aljohani, B., Alsamman, M. & Pourmand, A. Redesigning emergency
department operations amidst a viral pandemic. Am. J. Emerg. Med. 38, 1448–1453 (2020).
[4] Dorsett, M. Point of no return: COVID-19 and the us health care system: An emergency
physician’s perspective. Sci. Adv. 6 (2020).
[5] McKenna, P. et al. Emergency department and hospital crowding: causes, consequences, and
cures. Clin. Exp. Emerg. Med. 6, 189 (2019).
[6] Warner, M. A. Stop doing needless things! Saving healthcare resources during COVID-19 and
beyond. J. Gen. Intern. Med. 35, 2186–2188 (2020).
[7] Cozzi, D. et al. Chest X-ray in new coronavirus disease 2019 (COVID-19) infection: findings and correlation with clinical outcome. Radiol. Med. https://doi.org/10.1007/
s11547-020-01232-9 (2020).
14

[8] Rubin, G. D. et al. The role of chest imaging in patient management during the COVID-19
pandemic: a multinational consensus statement from the fleischner society. Chest 158, 106–116
(2020).
[9] American College of Radiology. ACR recommendations for the use of chest radiography and
computed tomography (CT) for suspected COVID-19 infection. https://www.acr.org/Advocacyand-Economics/ACR-Position-Statements/Recommendations-for-Chest-Radiography-andCT-for-Suspected-COVID19-Infection (2020).
[10] Wong, H. Y. F. et al. Frequency and distribution of chest radiographic findings in COVID-19
positive patients. Radiology https://doi.org/10.1148/radiol.2020201160 (2020).
[11] Kundu, S., Elhalawani, H., Gichoya, J. W. & Kahn Jr, C. E. How might ai and chest imaging
help unravel COVID-19’s mysteries? Radiol. Artif. Intell. 2, e200053 (2020).
[12] Khan, A. I., Shah, J. L. & Bhat, M. M. CoroNet: A deep neural network for detection and
diagnosis of COVID-19 from chest X-ray images. Comput. Meth. Prog. Bio. 196, 105581
(2020).
[13] Ucar, F. & Korkmaz, D. COVIDiagnosis-net: Deep Bayes-SqueezeNet based diagnostic of
the coronavirus disease 2019 (COVID-19) from X-ray images. Med. Hypotheses 140, 109761
(2020).
[14] Li, L. et al. Artificial intelligence distinguishes COVID-19 from community acquired pneumonia
on chest ct. Radiology https://doi.org/10.1148/radiol.2020200905 (2020).
[15] Ozturk, T. et al. Automated detection of COVID-19 cases using deep neural networks with
X-ray images. Comput. Biol. Med. 121, 103792 (2020).
[16] Wang, S. et al. A fully automatic deep learning system for COVID-19 diagnostic and prognostic
analysis. Eur. Respir. J. https://doi.org/10.1183/13993003.00775-2020 (2020).
[17] Zhang, K. et al. Clinically applicable AI system for accurate diagnosis, quantitative measurements, and prognosis of COVID-19 pneumonia using computed tomography. Cell 181,
1423–1433.e11 (2020).
[18] Singh, D., Kumar, V. & Kaur, M. Classification of COVID-19 patients from chest ct images
using multi-objective differential evolution–based convolutional neural networks. Eur. J. Clin.
Microbiol. 39, 1379–1389 (2020).
[19] Wynants, L. et al. Prediction models for diagnosis and prognosis of COVID-19 infection:
systematic review and critical appraisal. BMJ 369, m1328 (2020).
[20] Royal College of Physicians. National early warning score (news) 2: Standardising the assessment of acute-illness severity in the nhs. report of a working party. https://www.rcplondon.
ac.uk/projects/outputs/national-early-warning-score-news-2 (2017).
[21] Shamout, F. E., Zhu, T., Sharma, P., Watkinson, P. J. & Clifton, D. A. Deep interpretable early
warning system for the detection of clinical deterioration. IEEE J. Biomed. Health 24, 437–446
(2019).
[22] Li, M. D. et al. Automated assessment of COVID-19 pulmonary disease severity on chest
radiographs using convolutional siamese neural networks. Preprint at https://www.medrxiv.
org/content/10.1101/2020.05.20.20108159v1 (2020).
[23] Borghesi, A. & Maroldi, R. COVID-19 outbreak in italy: experimental chest X-ray scoring
system for quantifying and monitoring disease progression. Radiol. Med. 125, 509–513 (2020).
[24] Toussie, D. et al. Clinical and chest radiography features determine patient outcomes in young
and middle age adults with COVID-19. Radiology https://doi.org/10.1148/radiol.
2020201754 (2020).
[25] Fernandes, M. et al. Clinical decision support systems for triage in the emergency department
using intelligent systems: a review. Artif. Intell. Med. 102, 101762 (2020).
[26] Shen, Y. et al. Globally-aware multiple instance classifier for breast cancer screening. In
International Workshop on Machine Learning in Medical Imaging, 18–26 (2019).
[27] Shen, Y. et al. An interpretable classifier for high-resolution breast cancer screening images
utilizing weakly supervised localization. Preprint at https://arxiv.org/abs/2002.07613
(2020).
15

[28] Ke, G. et al. Lightgbm: A highly efficient gradient boosting decision tree. In Adv. Neur. In.,
3146–3154 (2017).
[29] Miller Jr, R. G. Survival Analysis, vol. 66 (John Wiley & Sons, New York, 2011).
[30] Efron, B. & Tibshirani, R. J. An introduction to the bootstrap (CRC press, 1994).
[31] Żołna, K., Geras, K. J. & Cho, K. Classifier-agnostic saliency map extraction. Comput. Vis.
Image Und. 196, 102969 (2020).
[32] Baier, L., Jöhren, F. & Seebacher, S. Challenges in the deployment and operation of machine
learning in practice. In Proceedings of the 27th European Conference on Information Systems
(ECIS) (2019).
[33] Martín, A. et al. TensorFlow: Large-scale machine learning on heterogeneous distributed
systems. Preprint at https://arxiv.org/abs/1603.04467 (2015).
[34] Narin, A., Kaya, C. & Pamuk, Z. Automatic detection of coronavirus disease (COVID-19) using
X-ray images and deep convolutional neural networks. Preprint at https://arxiv.org/abs/
2003.10849 (2020).
[35] Shamout, F. E., Zhu, T. & Clifton, D. A. Machine learning for clinical outcome prediction.
IEEE Rev. Biomed. Eng. https://doi.org/10.1109/RBME.2020.3007816 (2020).
[36] Ahmad, M. A., Eckert, C. & Teredesai, A. Interpretable machine learning in healthcare. In
Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational
Biology, and Health Informatics, 559–560 (2018).
[37] Selvaraju, R. R. et al. Grad-cam: Visual explanations from deep networks via gradient-based
localization. In Proceedings of the IEEE International Conference on Computer Vision, 618–626
(2017).
[38] Song, L. et al. Exploring the active mechanism of berberine against hcc by systematic pharmacology and experimental validation. Mol. Med. Rep. 20, 4654–4664 (2019).
[39] Brunese, L., Mercaldo, F., Reginelli, A. & Santone, A. Explainable deep learning for pulmonary
disease and coronavirus COVID-19 detection from X-rays. Comput. Meth. Prog. Bio. 196,
105608 (2020).
[40] Paul, H. Y., Kim, T. K. & Lin, C. T. Generalizability of deep learning tuberculosis classifier
to COVID-19 chest radiographs: New tricks for an old algorithm? J. Thorac. Imag. 35,
W102–W104 (2020).
[41] Adebayo, J. et al. Sanity checks for saliency maps. In Adv. Neur. In., 9505–9515 (2018).
[42] Brajer, N. et al. Prospective and external evaluation of a machine learning model to predict
in-hospital mortality of adults at time of admission. JAMA Netw. Open 3, e1920733–e1920733
(2020).
[43] Lodigiani, C. et al. Venous and arterial thromboembolic complications in COVID-19 patients
admitted to an academic hospital in Milan, Italy. Thromb. Res. (2020).
[44] Oxley, T. J. et al. Large-vessel stroke as a presenting feature of COVID-19 in the young. New
Engl. J. Med. 382 (2020).
[45] Viner, R. M. & Whittaker, E. Kawasaki-like disease: emerging complication during the
COVID-19 pandemic. Lancet 395, 1741–1743 (2020).
[46] Dietterich, T. G. Ensemble methods in machine learning. In International Workshop on Multiple
Classifier Systems, 1–15 (2000).
[47] He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–778
(2016).
[48] Huang, G., Liu, Z., Van Der Maaten, L. & Weinberger, K. Q. Densely connected convolutional
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
4700–4708 (2017).
[49] Rajpurkar, P. et al. CheXNet: Radiologist-level pneumonia detection on chest X-rays with deep
learning. Preprint at https://arxiv.org/abs/1711.05225 (2017).
[50] Allaouzi, I. & Ahmed, M. B. A novel approach for multi-label chest X-ray classification of
common thorax diseases. IEEE Access 7, 64279–64288 (2019).
16

[51] Liu, H. et al. Sdfn: Segmentation-based deep fusion network for thoracic disease classification
in chest X-ray images. Comput. Med. Imag. Grap. 75, 66–73 (2019).
[52] Guan, Q. & Huang, Y. Multi-label chest X-ray image classification via category-wise residual
attention learning. Pattern Recogn. Lett. 130, 259–266 (2020).
[53] Ilse, M., Tomczak, J. M. & Welling, M. Attention-based deep multiple instance learning.
Preprint at https://arxiv.org/abs/1802.04712 (2018).
[54] Gensheimer, M. F. & Narasimhan, B. A scalable discrete-time survival model for neural
networks. Preprint at https://arxiv.org/abs/1805.00917 (2018).
[55] Cox, D. R. & Oakes, D. Analysis Of Survival Data, vol. 21 (CRC Press, Boca Raton, 1984).
[56] Ching, T., Zhu, X. & Garmire, L. X. Cox-nnet: an artificial neural network method for prognosis
prediction of high-throughput omics data. PLoS Comput. Biol. 14, e1006076 (2018).
[57] Katzman, J. L. et al. DeepSurv: personalized treatment recommender system using a cox
proportional hazards deep neural network. BMC Med. Res. Methodol. 18, 24 (2018).
[58] Liang, W. et al. Early triage of critically ill COVID-19 patients using deep learning. Nat.
Commun. 11, 1–7 (2020).
[59] Paszke, A. et al. PyTorch: An imperative style, high-performance deep learning library. In Adv.
Neur. In., 8026–8037 (2019).
[60] Wang, X. et al. ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weaklysupervised classification and localization of common thorax diseases. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (2017).
[61] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. In Proceedings of the 3rd
International Conference on Learning Representations (2015).
[62] Bergstra, J. & Bengio, Y. Random search for hyper-parameter optimization. J. Mach. Learn.
Res. 13, 281–305 (2012).
[63] Xu, Q. & Liang, Y. Monte Carlo cross validation. Chemometr. Intell. Lab. 56, 1–11 (2001).
[64] Liu, K., Chen, Y., Lin, R. & Han, K. Clinical features of COVID-19 in elderly patients: A
comparison with young and middle-aged patients. J. Infection. 80, e14–e18 (2020).
[65] Krizhevsky, A. Learning multiple layers of features from tiny images. Tech. Rep., University of
Toronto (2009).
[66] Deng, J. et al. ImageNet: A Large-Scale Hierarchical Image Database. In 2009 IEEE Conference
on Computer Vision and Pattern Recognition, 248–255 (2009).
[67] Geras, K. J. et al. High-resolution breast cancer screening with multi-view deep convolutional
neural networks. Preprint at https://arxiv.org/abs/1703.07047 (2017).
[68] Pan, S. J. & Yang, Q. A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 22,
1345–1359 (2009).
[69] Yosinski, J., Clune, J., Bengio, Y. & Lipson, H. How transferable are features in deep neural
networks? In Adv. Neur. In., 3320–3328 (2014).
[70] He, K., Zhang, X., Ren, S. & Sun, J. Delving deep into rectifiers: surpassing human-level
performance on ImageNet classification. In Proceedings of the IEEE International Conference
on Computer Vision, 1026–1034 (2015).

17

Supplementary Information
Supplementary Note 1: Image preprocessing
In Supplementary Figure 1.a, we show the heights and widths of the images prior to data augmentation.
In Supplementary Figure 1.b, we show an example of a raw image and the final image after applying
the preprocessing steps in Figure 1.c.
a Raw image sizes

b Raw image

c Preprocessed image

Supplementary Figure 1: (a) Heights and widths (in pixels) of images prior to data augmentation. (b) An
example raw image. (c) To ensure that the inputs to the model have a consistent size, we perform center cropping
and rescaling. In addition, we apply random horizontal flipping, rotation, and translation to augment the training
dataset.

18

Supplementary Note 2: Clinical variables modeling
The average importance of the top ten features computed by the COVID-GBM models are shown
in Supplementary Figure 2.a. The importance of a feature is measured by the numbers of times the
feature is used to split the data across all trees in a single COVID-GBM model. Age is amongst the
top ten features across all time windows, which is consistent with existing findings that mortality
is more common amongst elderly COVID-19 patients than younger patients [64]. The inclusion of
the vital sign variables, amongst the top ten features across all models, is also aligned with existing
research suggesting that they are strong indicators of deterioration [20].
a

b

c

Supplementary Figure 2: Additional results for COVID-GBM and the ensemble of COVID-GBM and
COVID-GMIC. a, The average importance of the top ten features computed by the nine COVID-GBM ensemble
models for 24, 48, 72, and 96 hours. The importance of a feature is measured by the numbers of times the feature
is used to split the data across all trees in a model. b, The effect of varying λ, the weight on the COVID-GMIC
prediction, in combining the predictions of COVID-GMIC and COVID-GBM when using AUC, PR AUC and
the average AUC and PR AUC on the validation set. For the average AUC and PR AUC, the optimal λ was 0.74
for 24 hours, 0.81 for 48 hours, 0.62 for 72 hours, and 0.64 for 96 hours. c, the optimal values of λ selected
through the validation set in b are shown for the test set.

19

Supplementary Note 3: Ablation studies
DenseNet-121-based models. DenseNet [48] is a deep neural network architecture which consists
of dense blocks in which layers are directly connected to every other layer in a feed-forward fashion.
It achieves strong performance on benchmark natural images dataset, such as CIFAR10/100 [65]
and ILSVRC 2012 (ImageNet) dataset [66] while being computationally efficient. Here we compare
COVID-GMIC to a specific variant of DenseNet, DenseNet-121, which has been applied to process
chest X-ray images in the literature [49, 50, 51, 52].
The model assumes an input size of 224×224. We applied DenseNet-121-based models to predict
deterioration and also to compute deterioration risk curves. We initialized the models with weights
pretrained on the ChestX-ray14 dataset [60], provided at https://github.com/arnoweng/CheXNet. We
used weight decay in the optimizer. To perform hyperparameter search, we sampled the learning
rate and the rate of weight decay per step uniformly on a logarithmic scale between 10[−6,−1] and
10[−6,−3] .
For adverse event prediction, the DenseNet-121 based model yielded test AUCs of 0.687 (95% CI:
0.621 - 0.749), 0.709 (95% CI: 0.653 - 0.757), 0.710 (95% CI: 0.660 - 0.763), and 0.736 (95% CI:
0.691 - 0.782), and PRAUCs of 0.216 (95% CI: 0.155 - 0.317), 0.315 (95% CI: 0.239 - 0.419),
0.373 (95% CI: 0.300 - 0464), and 0.454 (95% CI: 0.384 - 0.542) for 24, 48, 72, and 96 hours.
The deterioration risk curves produced by the DenseNet-121 based models and the corresponding
reliability plot are presented in Figure 3.

Supplementary Figure 3: Deterioration risk curves (DRCs) and reliability plot for DenseNet-121. Compare to Figure 4, which shows analogous graphs for COVID-GMIC-DRC. a, DRCs generated by DenseNet-121
model for patients in the test set with (faded red lines) and without adverse events (faded blue lines). The mean
DRC for patients with adverse events (red dashed line) is higher than the DRC for patients without adverse
events (blue dashed line) at all times. The graph also includes the ground-truth population DRC (black dashed
line) computed from the test data. b, Reliability plot of the DRCs generated by DenseNet-121 model for patients
in the test set. The empirical probabilities are computed by dividing the patients into deciles according to the
value of the DRC at each time t. The empirical probability equals the fraction of patients in each decile that
suffered adverse events up to t. This is plotted against the predicted probability, which equals the mean DRC of
the patients in the decile. The diagram shows that these values are similar across the different values of t, and
hence the model is well-calibrated (for comparison, perfect calibration would correspond to the diagonal black
dashed line).

Impact of input image resolution. Prior work on deep learning for medical images [67] report
that using high resolution input images can improve performance. In this section, we analyze the
impact of image resolution on our tasks of interest. We consider the following image sizes: 128×128,
256×256, 512×512, and 1024×1024. We pretrain all models on the ChestX-ray14 dataset [60] and
then fine-tune them on our dataset. Results on the test set are reported in Supplementary Table 1.
The DenseNet-121 based model achieves the best AUCs when using an image size of 256 × 256,
and the best concordance index for 512×512. Further increasing the resolution does not improve
performance. COVID-GMIC achieves the best performance for the highest input image resolution of
1024×1024, while achieving the best concordance index for 512×512. While a further increase in
performance may be possible, we did not consider any larger image sizes resolutions because the
computational cost would become prohibitively high.
20

Supplementary Table 1: Model performance with 95% confidence intervals when using input images of sizes
of 128×128, 256×256, 512×512, and 1024×1024. For COVID-GMIC, we started with a size of 256×256
since an image with resolution of 128×128 pixels results in saliency maps that are too small to generate
meaningful ROI patches. We report AUCs for predicting the risk of deterioration within 24, 48, 72, and 96 hours.
When evaluating the deterioration risk curves, we report the concordance index with a reference time of 96 hours,
as well as the average of the index over all possible reference times (3, 12, 24, 48, 72, 96, 144, and 192 hours).
AUC / PR AUC
DenseNet-121

128×128
256×256
512×512
1024×1024

COVID-GMIC

256×256
512×512
1024×1024

Concordance index

24 hours

48 hours

72 hours

96 hours

96 hours

0.663 (0.602, 0.733) /
0.214 (0.119, 0.284)
0.698 (0.632, 0.763) /
0.218 (0.153, 0.310)
0.682 (0.617, 0.749) /
0.208 (0.111, 0.267)
0.680 (0.619, 0.742) /
0.180 (0.101, 0.230)

0.688 (0.633, 0.749) /
0.300 (0.198, 0.376)
0.721 (0.668, 0.778) /
0.310 (0.207, 0.382)
0.710 (0.658, 0.764) /
0.318 (0.238, 0.422)
0.709 (0.657, 0.763) /
0.278 (0.185, 0.344)

0.700 (0.649, 0.753) /
0.370 (0.279, 0.448)
0.719 (0.670, 0.773) /
0.390 (0.318, 0.486)
0.709 (0.656, 0.764) /
0.383 (0.286, 0.459)
0.716 (0.666, 0.766) /
0.369 (0.269, 0.442)

0.728 (0.685, 0.781) /
0.453 (0.364, 0.533)
0.748 (0.701, 0.795) /
0.469 (0.392, 0.562)
0.732 (0.686, 0.780) /
0.441 (0.353, 0.516)
0.739 (0.694, 0.787) /
0.441 (0.353, 0.516)

0.700 (0.667, 0.734)

0.700 (0.672, 0.736)

Average

0.701 (0.666, 0.738)

0.698 (0.663, 0.734)

0.664 (0.593, 0.734) /
0.202 (0.101, 0.260)
0.700 (0.635, 0.765) /
0.210 (0.154, 0.298)
0.695 (0.630, 0.763) /
0.200 (0.121, 0.258)

0.688 (0.630, 0.747) /
0.263 (0.172, 0.326)
0.714 (0.661, 0.769) /
0.300 (0.205, 0.370)
0.716 (0.661, 0.767) /
0.302 (0.230, 0.394)

0.699 (0.651, 0.750) /
0.342 (0.253, 0.414)
0.714 (0.671, 0.766) /
0.389 (0.314, 0.481)
0.717 (0.663, 0.764) /
0.374 (0.289, 0.447)

0.728 (0.684, 0.774) /
0.424 (0.343, 0.492)
0.733 (0.690, 0.780) /
0.443 (0.354, 0.515)
0.738 (0.692, 0.780) /
0.439 (0.368, 0.522)

0.705 (0.673,0.739)

0.701 (0.669,0.735)

0.701 (0.668, 0.734)

0.696 (0.664, 0.729)

0.712 (0.679, 0.744)

0.707 (0.675, 0.741)

0.713 (0.679,0.748)

0.708 (0.675, 0.742)

0.686 (0.650, 0.720)

0.685 (0.648, 0.717)

Impact of different transfer learning strategies. In data-scarce applications, it is crucial to
pretrain deep neural networks on a related task for which a large dataset is available, prior to finetuning on the task of interest [68, 69]. Given the relatively small number of COVID-19 positive cases
in our dataset, we investigate the impact of different weight initialization strategies on our results.
Specifically, we compare three strategies: 1) initialization by He et al. [70], 2) initialization with
weights from models trained on natural images (ImageNet [66]), and 3) initialization with weights
from models trained on chest X-ray images (ChestX-ray14 dataset [60]). We apply the initialization
procedure to all layers except the last fully connected layer, which is always initialized randomly. We
then fine-tune the entire network on our COVID-19 task.
Based on results shown in Supplementary Table 2, fine-tuning the network from weights pretrained
on the ChestX-ray14 dataset is the most effective strategy for COVID-GMIC. This dataset contains
over 100,000 chest X-ray images from more than 30,000 patients, including many with advanced lung
disease. The images are paired with labels representing fourteen common thoracic observations: atelectasis, cardiomegaly, effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation,
edema, emphysema, fibrosis, pleural thickening, and hernia. By pretraining a model to detect these
conditions, we hypothesize that the model learns a representation that is useful for our downstream
task of COVID-19 prognosis.
Supplementary Table 2: Model performance with 95% confidence intervals across three different initialization
strategies: random initialization, initialization with the weights of the model pretrained on ImageNet [66] and
initialization with the weights of the model pretrained model on the ChestX-ray14 dataset [60]. We report AUCs
for each time window in the outcome classification task. When evaluating the deterioration risk curves, we
report the concordance index with a reference time of 96 hours, as well as the average of the index over all
discretized times (3, 12, 24, 48, 72, 96, 144, and 192 hours).
AUC / PR AUC
DenseNet-121

Random
ImageNet
ChestX-ray14

COVID-GMIC

Random
ImageNet
ChestX-ray14

24 hours

48 hours

0.687 (0.625, 0.753) /
0.178 (0.105, 0.222)
0.701 (0.639, 0.761) /
0.206 (0.117, 0.260)
0.687 (0.616, 0.755) /
0.216 (0.155, 0.317)

0.699 (0.648, 0.754) /
0.258 (0.177, 0.315)
0.722 (0.668, 0.776) /
0.299 (0.197, 0.366)
0.709 (0.651, 0.765) /
0.315 (0.239, 0.419)

0.675 (0.609, 0.743) /
0.174 (0.101, 0.223)
0.694 (0.635, 0.757) /
0.195 (0.110, 0.252)
0.695 (0.626, 0.757) /
0.200 (0.142, 0.283)

0.671 (0.614, 0.725) /
0.227 (0.146, 0.277)
0.709 (0.657, 0.761) /
0.258 (0.165, 0.319)
0.716 (0.659, 0.768) /
0.302 (0.228, 0.400)

Concordance index
72 hours

96 hours

96 hours

0.693 (0.642, 0.747) /
0.326 (0.236, 0.388)
0.719 (0.670, 0.772) /
0.365 (0.264, 0.436)
0.710 (0.657, 0.760) /
0.373 (0.300, 0.464)

0.705 (0.660, 0.752) /
0.386 (0.298, 0.449)
0.745 (0.701, 0.789) /
0.444 (0.349, 0.513)
0.736 (0.690, 0.781) /
0.454 (0.384, 0.542)

0.649 (0.614, 0.686)

0.648 (0.613, 0.685)

Average

0.686 (0.652, 0.720)

0.683 (0.651, 0.715)

0.705 (0.673,0.739)

0.701 (0.669,0.735)

0.686 (0.640, 0.732) /
0.290 (0.214, 0.345)
extbf0.724 (0.673, 0.769) /
0.347 (0.263, 0.416)
0.717 (0.672, 0.769) /
0.374 (0.302, 0.463)

0.708 (0.668, 0.752) /
0.352 (0.276, 0.410)
0.737 (0.696, 0.782) /
0.433 (0.354, 0.506)
0.738 (0.690, 0.783) /
0.439 (0.368, 0.532)

0.643 (0.606, 0.678)

0.640 (0.604, 0.673)

0.684 (0.652, 0.717)

0.680 (0.649, 0.711)

0.713 (0.679,0.748)

0.708 (0.675,0.742)

Impact of training set size. We evaluated the impact of the sample size used for training our
machine learning models. Specifically, we evaluated our models on a subset of the training data,
obtained by randomly sampling 12.5%, 25%, and 50% of the exams. Table 3 presents the AUCs and
PR AUCs and the concordance indices achieved on the test set. It is evident that the performance
of COVID-GMIC and COVID-GBM improve when increasing the number of images and clinical
variables used for training, which highlights the importance of using a large dataset.
21

Supplementary Table 3: Model performance with 95% confidence intervals when using 12.5%, 25%, 50%,
and 100% of the training data. We report AUCs for each time window in the adverse event prediction task. When
evaluating the deterioration risk curves, we report the concordance index with a reference time of 96 hours, as
well as the average of the index over all discretized times (3, 12, 24, 48, 72, 96, 144, and 192 hours).
AUC / PR AUC
DenseNet-121

12.5%
25%
50%
100%

COVID-GMIC

12.5%
25%
50%
100%

COVID-GBM

12.5%
25%
50%
100%

Concordance index

24 hours

48 hours

72 hours

96 hours

96 hours

Average

0.608 (0.530, 0.678) /
0.182 (0.094, 0.241)
0.638 (0.570, 0.708) /
0.174 (0.090, 0.227)
0.672 (0.605, 0.737) /
0.214 (0.109, 0.278)
0.687 (0.621, 0.753) /
0.216 (0.154, 0.317)

0.653 (0.594, 0.711) /
0.265 (0.177, 0.332)
0.678 (0.621, 0.737) /
0.266 (0.170, 0.327)
0.699 (0.644, 0.752) /
0.303 (0.209, 0.373)
0.709 (0.654, 0.763) /
0.315 (0.239, 0.417)

0.672 (0.617, 0.722) /
0.336 (0.248, 0.401)
0.682 (0.628, 0.734) /
0.327 (0.239, 0.393)
0.698 (0.646, 0.747) /
0.351 (0.265, 0.417)
0.710 (0.658, 0.761) /
0.373 (0.298, 0.475)

0.703 (0.654, 0.749) /
0.415 (0.330, 0.486)
0.711 (0.662, 0.758) /
0.408 (0.321, 0.475)
0.725 (0.679, 0.769) /
0.433 (0.349, 0.501)
0.736 (0.689, 0.781) /
0.454 (0.377, 0.552)

0.675 (0.640, 0.708)

0.670 (0.636, 0.703)

0.676 (0.641, 0.709)

0.671 (0.637, 0.704)

0.694 (0.660, 0.728)

0.691 (0.657, 0.725)

0.705 (0.673,0.739)

0.701 (0.669,0.735)

0.640 (0.577, 0.703) /
0.145 (0.084, 0.180)
0.661 (0.598, 0.724) /
0.177 (0.091, 0.229)
0.646 (0.576, 0.715) /
0.164 (0.090, 0.212)
0.695 (0.626, 0.753) /
0.200 (0.142, 0.276)

0.672 (0.621, 0.726) /
0.231 (0.146, 0.283)
0.672 (0.616, 0.726) /
0.254 (0.162, 0.312)
0.681 (0.624, 0.740) /
0.266 (0.172, 0.333)
0.716 (0.663, 0.769) /
0.302 (0.230, 0.395)

0.677 (0.631, 0.728)
0.318 (0.230, 0.387)
0.677 (0.627, 0.723) /
0.327 (0.238, 0.388)
0.687 (0.635, 0.742) /
0.351 (0.257, 0.428)
0.717 (0.667, 0.767) /
0.374 (0.297, 0.461)

0.695 (0.652, 0.738) /
0.384 (0.294, 0.449)
0.693 (0.649, 0.738) /
0.395 (0.313, 0.461)
0.716 (0.669, 0.764) /
0.424 (0.332, 0.502)
0.738 (0.693, 0.782) /
0.439 (0.363, 0.521)

0.673 (0.640, 0.706)

0.668 (0.635, 0.701)

0.689 (0.655, 0.723)

0.680 (0.646, 0.714)

0.699 (0.664, 0.733)

0.690 (0.657, 0.722)

0.713 (0.679,0.748)

0.708 (0.675,0.742)

0.674 (0.609, 0.736) /
0.262 (0.153, 0.344)
0.688 (0.628, 0.740) /
0.175 (0.102, 0.220)
0.743 (0.699, 0.796) /
0.210 (0.119, 0.263)
0.747 (0.692, 0.798) /
0.230 (0.167, 0.322)

0.699 (0.647, 0.753) /
0.297 (0.199, 0.366)
0.716 (0.666, 0.765) /
0.319 (0.227, 0.401)
0.752 (0.702, 0.797) /
0.325 (0.252, 0.425)
0.739 (0.687, 0.793) /
0.325 (0.253, 0.425)

0.710 (0.666, 0.761) /
0.395 (0.310, 0.472)
0.733 (0.689, 0.778) /
0.385 (0.304, 0.461)
0.749 (0.706, 0.795) /
0.418 (0.326, 0.495)
0.750 (0.704, 0.794) /
0.408 (0.334, 0.502)

0.708 (0.663, 0.755) /
0.439 (0.361, 0.516)
0.739 (0.695, 0.784) /
0.476 (0.402, 0.545)
0.751 (0.711, 0.796) /
0.482 (0.396, 0.557)
0.770 (0.728, 0.811) /
0.523 (0.439, 0.611)

22

Supplementary Note 4: Additional results on the test sets

We visualize the receiver operating characteristic (ROC) and precision-recall (PR) curves on the test
set in Supplementary Figure 4. In a, we group the results based on the predictive models (COVIDGMIC, COVID-GBM, and the ensemble of both), while in b, we group the performances based on
the time window of the task (i.e., 24, 48, 72, and 96 hours). In Supplementary Figure 5, we visualize
the ROC and PR curves on the test set considered in the reader study.

a

b

Supplementary Figure 4: Receiver operating characteristic (ROC) and Precision-Recall (PR) curves for
predicting the onset of adverse events within 24, 48, 72, and 96 hours evaluated on the test set. a, ROC
and PR curves are grouped by predictive models. Ensembling COVID-GMIC and COVID-GBM improves
performance in almost all cases. b, ROC and PR curves are grouped by time window of the task. The AUC and
PR AUC improve as the length of the time window increases, which is consistence across models. Numerical
values of AUCs and PR AUCs can be found in Table 2.
23

Supplementary Figure 5: Test set ROC (top) and PR (bottom) curves of COVID-GMIC and the radiologists
for predicting the risk of deterioration over 24, 48, 72, and 96 hours. These results suggest that COVID-GMIC
performs comparably to the radiologists. Numerical values of AUCs and PR AUCs can be found in Table 2.

In Supplementary Table 4, we show the concordance index results across all time intervals for the
best DenseNet-121 and COVID-GMIC-DRC models.
Supplementary Table 4: Concordance index (with 95% confidence intervals) of the DRC curves generated by
the best DenseNet-121 and COVID-GMIC-DRC models. Both models use input images of size 512×512 and
are pretrained on the ChestX-ray14 dataset [60]. The results shows that the concordance index does not change
much with the choice of time reference.
Concordance index
Time (in hours)

3

12

24

48

72

96

144

192

Ave.

DenseNet-121

0.681
(0.647, 0.714)

0.694
(0.658, 0.727)

0.701
(0.666, 0.735)

0.702
(0.667, 0.735)

0.703
(0.668, 0.734)

0.705
(0.671, 0.737)

0.706
(0.672, 0.739)

0.705
(0.67, 0.737)

0.701
(0.667, 0.733)

COVID-GMIC-DRC

0.692
(0.661, 0.734)

0.698
(0.664, 0.736)

0.706
(0.672, 0.74)

0.710
(0.677, 0.746)

0.712
(0.676, 0.745)

0.713
(0.678, 0.747)

0.716
(0.681, 0.748)

0.715
(0.68, 0.748)

0.708
(0.674, 0.741)

24

Supplementary Note 5: Model selection
We describe our model selection procedure used throughout the paper in Algorithm 1. For the ablation
study in Supplementary Table 3, we control the size of the dataset by setting the parameter u to 12.5,
25 and 50. Specifically, in that experiment, we randomly sampled u% of the training set Dt as the
“universe” U that our model used for training and validation.
Algorithm 1 Model selection
Input: training set Dt , test set Ds , universe fraction u ∈ [0, 100], and a predictive model M
Output: a∗ performance of M evaluated on Ds
1: U = randomly sample u% of data from Dt
2: Φ = 30 randomly sampled configuration of hyperparameters of the M
3: for each hyperparameter configuration φi ∈ Φ do
4:
for j ∈ {1, 2, 3} do
5:
draw a random seed rj
6:
Utj , Uvj = universe U split into training and validation subset using the random seed rj
7:
Mij = trained M using hyperparameter configuration φi on Utj
8:
aij = performance of Mij evaluated on Uvj
9:
end forP
3
10:
ai = 13 j=1 aij
11: end for
12: A = {ai | ∀φi ∈ Φ}
13: B = {Mij | ∀ai ∈ top-3(A)}
14: M∗ = an equally weighted ensemble of all models in B
15: a∗ = performance of M∗ on Ds
16: return a∗

25

Supplementary Note 6: ROI retrieval algorithm
We describe the algorithm used to retrieve the ROI patches in Algorithm 2. In all experiments, we set
H = W = 1024, h = w = 32, hc = wc = 256, Ta = {24, 48, 72, 96}, and K = 6.
Algorithm 2 ROI retrival
Input: chest X-ray image x ∈ RH,W , saliency maps A ∈ Rh,w,|Ta | , number of ROI patches K
Output: a set of retrieved ROI patches O = {x̃k |x̃k ∈ Rhc ,wc }
1: O = ∅
2: for each time window t ∈ Ta do
3:
Ãt = min-max-normalization(At )
4: end forP
5: A∗ = t∈Ta Ãt
h
w
∗
6: l denotes an arbitrary
P hc H × wc W rectangular patch on A
7: criterion(l, A∗ ) = (i,j)∈l A∗ [i, j]
8: for each 1, 2, ..., K do
9:
l∗ = argmaxl criterion(l, A∗ )
10:
L = position of l∗ in x
11:
O = O ∪ {L}
12:
∀(i, j) ∈ l∗ , set A∗ [i, j] = 0
13: end for
14: return O

26

Supplementary Note 7: Learning curves
COVID-GMIC (96 hrs)

1.0

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0 0

0.9
AUC

PR AUC

0.8
0.7
0.6
0.5 0

25

1.00

50

validation
training
175 200

75 100 125 150
epoch number
DenseNet121 (96 hrs)

validation
training

0.95

25

50

validation
training

0.9

75 100 125 150
epoch number
DenseNet121 (96 hrs)

validation
training
175 200

0.8

0.90

0.7

AUC

PR AUC

0.85

0.6

0.80
0.75

0.5

0.70

0.4

0.65

COVID-GMIC (96 hrs)

0.3
0

2

4

6
8
10
epoch number

12

0

14

2

4

6
8
10
epoch number

12

14

Supplementary Figure 6: AUC and PR AUC for predicting clinical deterioration within 96 hours during
training achieved by a selected COVID-GMIC model and a selected DenseNet121 model on the training and
validation set. We select the best epoch (marked in red) in which the model achieves highest AUC on the
validation set. Our model selection mechanism ensures that the selected model is sufficiently trained and does not
lie in the overfitted regime. We observe similar trends in the learning curves for predicting clinical deterioration
within 24, 48, and 72 hours.

DenseNet121-DRC

0.72
0.70
0.68
0.66
5

10

15

20
Epoch

25

30

Validation
Training
35
40

Mean Concordance Index

Mean Concordance Index

0.74

0

COVID-GMIC-DRC

0.72

0.76

0.70
0.68
0.66
0.64
0

5

10

15

20
Epoch

25

30

Validation
Training
35
40

Supplementary Figure 7: Training curves showing mean concordance index for DenseNet121 and COVIDGMIC-DRC models. We select the best epoch (marked in red) in which the model achieves highest mean
concordance index on the validation set. Our model selection mechanism ensures that the selected model is
sufficiently trained and does not lie in the overfitted regime.

27

