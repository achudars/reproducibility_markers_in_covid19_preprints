Accessible Data Curation and Analytics for
International-Scale Citizen Science Datasets
Benjamin Murray1, * , Eric Kerfoot1 , Mark S. Graham1 , Carole H. Sudre2 , Erika Molteni1 ,
Liane S. Canas1 , Michela Antonelli1 , Kerstin Klaser1 , Alessia Visconti3 , Andrew T. Chan4 ,
Paul W. Franks5 , Richard Davies6 , Jonathan Wolf6 , Tim Spector3 , Claire J. Steves3 , Marc
Modat1, † , and Sebastien Ourselin1, †
1 King’s

College London, School of Biomedical Engineering & Imaging Sciences, London, SE1 7EU, United Kingdom
College London, MRC Unit for Lifelong Health and Ageing, Department of Population Health Sciences,
London, WC1E 7HB, United Kingdom
3 King’s College London, Department of Twin Research and Genetic Epidemiology, Westminster Bridge Road,
London, SE1 7EH, United Kingdom
4 Massachusetts General Hospital, 55 Fruit Street, GRJ 825C, Boston, MA 02116, United States
5 Lund University, Diabetes Centre, CRC, SUS Malmö, Jan Waldenströms gata 35, House 91:12. SE-214 28 Malmö,
Sweden
6 Zoe Global Limited, 164 Westminster Bridge Road, London, SE1 7RW, United Kingdom
* corresponding author(s): Benjamin Murray (benjamin.murray@kcl.ac.uk)
† Equal contribution

arXiv:2011.00867v2 [cs.DB] 17 Feb 2021

2 University

ABSTRACT
The Covid Symptom Study, a smartphone-based surveillance study on COVID-19 symptoms in the population, is an exemplar
of big data citizen science. Over 4.7 million participants and 189 million unique assessments have been logged since its
introduction in March 2020. The success of the Covid Symptom Study creates technical challenges around effective data
curation for two reasons. Firstly, the scale of the dataset means that it can no longer be easily processed using standard
software on commodity hardware. Secondly, the size of the research group means that replicability and consistency of key
analytics used across multiple publications becomes an issue. We present ExeTera, an open source data curation software
designed to address scalability challenges and to enable reproducible research across an international research group for
datasets such as the Covid Symptom Study dataset.

Introduction
Mobile applications have enabled citizen science 1–4 projects that can collect data from millions of individuals. The Covid
Symptom Study 5 , a smartphone-based surveillance study on self-reported COVID-19 symptoms started in March 2020, is an
exemplar of citizen science. As of 14th October 2020, the study contains 189 million self-assessments collected from more than
4.7 million individuals, provided as daily comma separated value (CSV) snapshots, that are made available to both academic
and non-academic researchers to facilitate COVID-19 research by the wider community.
The primary challenge in curating and analysing the data is its scale. Scale adds complexity to otherwise simple operations.
Python-based scientific computing libraries such as Numpy 6, 7 and Pandas 8 are ubiquitous in the academic community, but
they are not designed to scale to datasets larger than the amount of Random Access Memory (RAM) on a given machine.
Commodity hardware, as of 2020, is typically equipped with 16 to 32 GB of memory. Although it is possible to procure
hardware with more RAM, this involves server-grade pricing above 64 GB of memory, and doubling memory only doubles the
size of the data that can be handled.
Alternatively, data can be moved to a datastore; either traditional, relational databases such as PostgreSQL or other types of
datastore including key-value stores such as CouchBASE. Each type of datastore comes with its own design philosophy and set
of performance trade-offs 9 . Datastores are capable of running queries on larger-than-RAM datasets, but all come with the
need to learn either a new language or application programming interface (API), and an installation and maintenance burden,
whether installed locally or deployed in a cloud.
Although most of the principal Python libraries that support data science are not designed to work with datasets that are
significantly larger than RAM, they provide a rich set of functionality that can be built on top of, as well as demonstrating
design choices that have proven successful with the community. By focusing on the provision of key algorithms that are critical

for scaling beyond RAM, whilst still leveraging the existing Python ecosystem it is possible to create a highly scalable data
analysis software with an API that is familiar to users of the Python ecosystem.
Aside from scale, data curation includes other significant engineering challenges.
Raw datasets are typically noisy and may contain conflicting and inconsistent values. Erroneous values, changing schemas
and multiple contemporary app versions all add complexity to the task of cleaning and consolidating datasets for downstream
analysis.
Another major challenge is reproducibility of analyses, especially across a large research team. When data cleaning and
generation of analytics is done in an ad-hoc fashion, it is easy to generate subtly different derivations of the same measure,
causing difficulties in reconciling research efforts across multiple groups and research outputs. Full reproducibility requires
algorithms to be treated as immutable, so that the application of a particular algorithm to a particular snapshot of data guarantees
identical results, hardware notwithstanding.
Public datasets are often delivered as a series of timestamped snapshots. The snapshots are typically memory-less; if two
snapshots have corresponding entries that are modified over time, one can only determine the delta by comparing the snapshots,
which exacerbates scaling issues. Updating an analysis from one snapshot to another without such comparison compromises
the interpretability of the updated results in a way that is not visible when working with individual snapshots.
To address the above challenges, we have created ExeTera; a software that enables sophisticated, auditable, and reproducible
data curation and analytics by distributed research teams, through a Python API designed to be familiar to data analysts and
researchers who use Python’s scientific computing ecosystem. ExeTera has initially been implemented to provide data curation
and analytics for the Covid Symptom Study. In this work, we evaluate ExeTera’s performance on this dataset and demonstrate
that it is suitable for analysis of tabular datasets approaching terabyte scale in the general case.

Results
ExeTera’s performance has been benchmarked against a combination of artificial data and data from the Covid Symptom Study.
We examine performance and scalability of ExeTera for key operations relative to Pandas, the most popular Python library
for analysing tabular data. We also demonstrate Exetera’s ability to generate a journaled dataset from snapshots allowing
longitudinal analysis that is able to account for destructive changes between corresponding rows of the different snapshots.
Finally, we present an example of ExeTera’s analytic capabilities.
Performance was measured on an AMD Ryzen Threadripper 3960x 24-core processor with 256 GB of memory. All
processes were limited to 32GB of memory. The data was read from and written to a 1TB Corsair Force MP600, M.2 (2280)
PCIe 4.0 NVMe SSD.
Importing to ExeTera
ExeTera has an import function that is used to convert a set of source CSV files to HDF5 10 , given a JSON (JavaScript Object
Notation) schema document describing the datatypes. This operation streams the CSV files, transforming the string source
field data into one or more target fields in the HDF5 dataset. Table 1 shows the time taken and resulting table row counts for
importing the Covid Symptom Study snapshot from the 8th October 2020.
Comparison of ExeTera and Pandas for key operations
We compare ExeTera with Pandas for two key performance and scalability tests; the ability to load subsets of a dataset and the
ability to perform joins across different tables in a dataset.
Data subset reading

In the general case, a typical piece of analysis will only involve a fraction of the available fields in a dataset. We test the ability
of ExeTera and Pandas to load subsets of the Covid Symptom Study dataset, from the patient and assessment tables respectively.
To generate the results in Table 2, we load 1, 2, 4, 8, 16, and 32 fields respectively
from the patient table of the Covid Symptom Study dataset, containing approximately 4.7 million entries. Table 2 shows that
ExeTera (given its use of HDF5 as its serialized format) is orders of magnitude faster than CSV due to each entire field being
loadable with effectively zero parsing overhead and as monolithic reads. CSV on the other hand, must be parsed in its entirety
and the resulting collections incrementally built. CSV files are loaded through Panda’s read_csv function.
Covid Symptom Study Patient Table

To generate the results in Table 3, we load 1, 2, 4, 8, 16, and 32 fields respectively
from the assessment table of the Covid Symptom Study dataset, containing approximately 189 million entries. The results
in Table 3 show that Pandas is unable to load multiple fields at this row count. Again, CSV files are loaded through Panda’s
read_csv function.
Covid Symptom Study Assessment Table

2/16

Left join operation

The join operation is the most memory-intensive operation that is typically carried out on tabular datasets. ExeTera implements
highly scalable versions of left, right and inner joins; left join is the most typical join performed so we focus on its performance
here.
A left join operation involves the mapping of data from one table onto another table, based on the key relationships between
the two tables. Values in the right table are mapped to values in the left table. For example, patient data may be joined to
assessment data so that assessments can be processed with patient-level features such as age or BMI (body mass index). An
illustrated example of a left join can be seen in Figure 1.
To test the performance of the join operator when ExeTera and Pandas are used, we generate a dataset composed of a left
primary key (int64), a right foreign key (int64) and 1, 2, 4, 8, 16, and 32 fields respectively of random numbers corresponding
to entries in the right table (int32). The first ten rows of the join dataset in the two field case is shown in Table 4. The resulting
join performance for both is shown in Table 5. The left and right tables have the same number of rows for each test, listed in the
first column.
It should be noted that ExeTera organises data into groups, which may be considered logical tables, but the user typically
loads the data on a field by field basis when working with it. ExeTera’s merging API allows the user to think in a tabular
fashion by accepting tuples of ExeTera fields, which only load their contents when specifically requested. ExeTera achieves its
scalability through two techniques; firstly, by calculating the map and then applying it to each field in turn, and secondly, by
exploiting the natural sorted order to convert the mapping into a streaming merge. The result is a merge operation that is fast
and scales to billions of rows.
Journaling operation
ExeTera can combine snapshots of datasets to create a journaled dataset, keeping multiple, timestamped copies of otherwise
destructive changes to corresponding records between the snapshots. Table 6 shows the results of journaling together shapshots
of the Covid Symptom Study from the August 1st 2020 and September 1st 2020, and the time taken to do so.
Analytics
ExeTera provides the ability to load data very efficiently, as seen in Table 2 and 3. Once loaded, analytics can be performed
through use of libraries such as Numpy and Matplotlib 11 , using tools that researchers are familiar with, such as Jupyter
Notebook12 . Figure 2 shows a histogram of healthy and unhealthy assessment logs bucketed into seven day periods that must
parse 189 million assessments to generate its results.

Discussion
In this work, we present ExeTera, a data curation and analytics tool designed to provide users with a low complexity solution
for working on datasets approaching terabyte scale, such as national / international-scale citizen science datasets like the Covid
Symptom Study. ExeTera makes this possible without the additional complexity of server-based datastores, and thus simplifies
access to such datasets for both academic and independent researchers who are versed in the Python scientific programming
ecosystem.
ExeTera provides features for cleaning, journaling, and generation of reproducible processing and analytics, enabling large
research teams to work with consistent measures and analyses that can be reliably recreated from the base data snapshots. Its
ability to store multiple snapshots in a journaled format enables researchers to perform full longitudinal analysis on otherwise
unjournaled datasets and facilitates the ability to move between snapshots whilst being able to properly explore the impact of
doing so on analyses.
ExeTera has been a key part in enabling analysis of the Covid Symptom Study dataset, including being used for analysis in
the following manuscripts 13–16 .
Although ExeTera was developed to provide data curation for researchers working on the Covid Symptom Study, this
software is being developed to be generally applicable to large-scale relational datasets for researchers who work in Python. As
such, ExeTera core functionality is provided as a python package that is imported by ExeTeraCovid, the collection of algorithms,
scripts, and other resources specific to the Covid Symptom Study.
While our goal is to closely emulate a Pandas-style API, additional work is still required to ensure this is available to the
user across the whole API. The highly scalable sorting API and journaling API, for example, are accessed through the low-level
operations API rather than through abstractions to more closely emulate Pandas, and work is ongoing to make the API more
consistent in the presentation and manipulation of the data as logical tables.
ExeTera is currently built on top of Numpy and Pandas functionality. The ability to scale is provided through implementations
of key operations that can stream large collections from drive. This enables processing of large datasets, but, at present, ExeTera
doesn’t take advantage of multiple cores or processors, nor is it able to process across a cluster. Future development will make
3/16

use of Dask 17 a library designed to convert operations on Numpy and Pandas to a directed acyclic graph of sub-operations
that can be distributed to multiple cores and nodes, and focus on provision of the elements of scalable processing, such as
multi-key argsorts and tableless merges that Dask lacks. Dask is also integrated with more specialised back-ends such as
Nvidia’s RAPIDs 18 , which enables execution of distributed graph processing across GPU clusters. The integration of Dask
should allow ExeTera to execute over datasets well into the multi-terabyte range.
Reproducibility relies on immutability of the algorithms deployed during analysis. At present, ExeTera achieves this
through a convention that algorithms are treated as immutable once implemented and deployed. A more robust system is being
designed to provide algorithmic immutability without this constraint.
HDF5 has proven to be fragile to interruptions while data is being written to it; an interrupted write is capable of rendering
the entire dataset unreadable and so all writes must be protected from user interrupts and other exceptions. Additionally, HDF5
does not allow the space from deleted fields to be properly reclaimed from the dataset. This must be done separately through
use of the ‘h5repack’ tool. The primary benefit of HDF5 to ExeTera is its ability to flexibly store contiguous fields of data for
rapid reading. This can also be achieved through use of alternative columnar data formats such as ORC 19 or Parquet 19, 20
or, alternatively, through use of the file system as a datastore. Storing individual fields as serialized Numpy arrays and field
metadata in JSON allows for transparent, robust dataset serialization that can be explored on the file system.
ExeTera is made available as open source software released under Apache License 2.0.

Methods
The Covid Symptom Study dataset
ExeTera has been developed by King’s College London (KCL) to provide data curation for the Covid Symptom Study dataset.
The dataset is collected using the Covid Symptom Study app, developed by Zoe Global Ltd with input from King’s College
London, the Massachusetts General Hospital, Lund University Sweden, and Uppsala University, Sweden. It is a response to
the COVID-19 pandemic based on epidemiological surveillance via smartphone-based self-reporting. It asks citizens from
the UK, US, and Sweden to use a mobile application to log symptoms, record COVID-19 test results and answer lifestyle
and occupational questions. The Covid Symptom Study dataset has generated insights into COVID-19 that have gone on to
inform governmental policies for handling of the disease 5, 21–23 . In the UK, the App Ethics has been approved by KCL ethics
Committee REMAS ID 18210, review reference LRS-19/20-18210 and all subscribers provided consent. In Sweden, ethics
approval for the study is provided by the central ethics committee (DNR 2020-01803).
As of the 14th October, 2020, the dataset is composed of four tables:
Patients: 4.701 million patients with 224 data fields. Patient records store data such as the patients’ physiological statistics,
long-term illnesses, lifestyle factors, location and other data that only occasionally changes, at the patient level.
Assessments: 189.2 million assessments with 66 fields. Patients are asked to give regular assessments through the app that
cover their current health status and symptoms, aspects of their lifestyle such as potential exposure to COVID-19, and, in early
versions of the schema, any COVID-19 tests that they have had.
Tests: 1.455 million tests, with 26 fields. Test records are kept for each COVID-19 test that a patient has had along with the
evolving status of that test (typically ‘waiting’ to some result).
Diet: 1.563 million diet study questionnaires with 104 fields. These ask people at several time points about their dietary
and lifestyle habits.
Assessments, tests and diet study questionnaires are mapped to patients via ids that serve as foreign keys.
This dataset is delivered as daily snapshots in CSV format. As of 14th October 2020, the daily snapshot is 42.2 GB in size,
and the accumulated daily snapshots are over 3TB in size. The dataset, excepting fine-grained geolocation data, is publicly
available at https://healthdatagateway.org.
Scalability as prerequisite
In order to successfully curate the Covid Symptom Study data, it is necessary to be able to handle data that cannot fit into RAM.
Data size and structure, and the set of operations needed to handle the dataset have to be addressed. We can define three scale
domains that necessitate a change of approach.
RAM Scale (1GB to 16 GB)

This is the scale at which the dataset entirely fits in the computer’s RAM. Commodity laptops and desktops used by researchers
typically have between 16 and 32 GB of RAM. Loading the data can inflate its memory footprint depending on the datatypes
used, and operations can multiply memory requirements by a small constant factor, but provided peak memory usage does not
dramatically exceed RAM, researchers can make use of programming languages with numerical / scientific libraries such as
Numpy or Pandas to effectively analyse the data.
4/16

Drive Scale (16 GB to 1 TB)

At drive scale, only a portion of the dataset can fit into RAM at a given time, so specific solutions are required to effectively
stream the dataset from drive to memory. Datastores become a more compelling option at this scale, as they already have
memory efficient, streaming versions of the operations that they support, but their usage may not be desirable due to the need
to learn a new language or API, and the installation and maintenance burden they represent. This is the scale of dataset that
ExeTera currently targets.
Distributed Scale (> 1 TB )

At distributed scale, the use of server-based datastores is typically mandatory. It becomes necessary to redesign operations to
exploit distributed computing across many nodes. Selection of appropriate datastore technology becomes critical, with specific
datastore technologies addressing different roles within the overall system. This scale will be targeted by ExeTera in future
development.
A model data curation pipeline
The ExeTera software provides functionality that enables a data curation pipeline incorporating data curation best practice. The
pipeline has the following steps:
• Import / preliminary data cleaning and filtering
• Journaling of snapshots into a consolidated dataset
• Generation of derived data and analytics
The first two stages are generic operations that apply to any tabular dataset being imported into ExeTera. The third stage is
specific to a given dataset, such as the Covid Symptom Study.
Import / Preliminary Cleaning and Filtering

The import process converts CSV data to a binary, columnar format, discussed further in the ‘Implementation’ subsection, that
is many orders of magnitude more time efficient for querying in most cases. As part of this process, the data is converted from
strings to data types defined by a JSON schema file.
The mapping of data types is from a single CSV field to one or more strongly typed fields. How this is done is determined
by the JSON schema and the type specified in the schema.
Fixed string fields contain string data where each entry is guaranteed to be no longer than the length
specified by the field. Fixed string fields can handle UTF8 unicode data, but this is encoded into bytes and so the specified
length must take into account the encoding of the string to a byte array.
Fixed string fields

Indexed string fields are used for string data where the strings may be of arbitrary length. The data is
stored as two arrays; a byte string of all of the strings concatenated together, and an array of indices indicating the offset to each
entry.
Indexed string fields

Numeric / logical fields Fields which contain a combination of strings to be converted to strongly typed values and empty
values, for example “”, “False”, “True” are converted to the appropriate numeric / logical dataset and a corresponding filter field
indicating whether a value is present for a given row of the field. Fields containing string values can be processed as categorical
fields if specified as such.

Categorical fields map a limited set of string values to a corresponding numeric value. A key is stored
along with the field providing a mapping between string and number. A categorical field can also be specified as leaky, in the
case that the field is a mixture of categorical values and free text. In this case, a value is reserved to indicate that a given row
doesn’t correspond to a category, and an indexed string field is created for free text entries.
Categorical fields

Datetime fields store date times as posix timestamps in double precision floating point format. The
schema can also specify the generation of a ‘day’ field quantising the timestamp to the nearest day, and can also specify whether
the field contains empty values, in which case as filter is also generated, as with numeric fields.
Datetime / date fields

Importing data from CSV requires a schema file that describes the fields and the type conversions that
they should undergo. The ExeTera schema file format is a JSON format. Each table is described by entry inside of a JSON
dictionary labeled schema. Each entry in this dictionary is the name of the table followed by the table descriptor. This has up
to three entries. The first is primary_keys, which lists zero or more fields for the dataset that together represent the primary
key for the table. The second is fields and contains all the field descriptors for the table. The third is foreign_keys and
contains the names of foreign keys in the table and which other tables they relate to.
Schema file format

5/16

The schema file entries themselves contain at minimum a field_type entry, and depending
on the specific field type, require additional entries. Figure 4 shows an illustrative, minimal example of a schema file. A full
specification can be found in the ExeTera github wiki, as of the time of writing.
Schema file field entries

Journaling of snapshots into a consolidated dataset

Data for the Covid Symptom Study project is delivered as a series of timestamped snapshots. The unanonymised data generated
by the Covid Symptom Study app is stored in a relational database or similar datastore, that is not accessible to query by the
broader research community. Instead, the data needs to first be anonymised and bulk exported to CSV format. The database
is a live view of the dataset, however; users can update data through the app, and, unless the database is explicitly journaled
and each entry made immutable, the prior states are erased. As such, a row corresponding to a given entity in two different
snapshots can be contain conflicting values.
When each snapshot is large, the scaling problem is exacerbated by having to reconcile multiple snapshots. The Covid
Symptom Study dataset does not have a field that reliably indicates whether the contents of a given row have changed and so
determining whether a row at time t has changed relative to a row at time t + 1 requires a full comparison of all common fields.
An example of this can be seen in Figure 3.
Generation of derived data and analytics

In addition to the initial data cleaning performed during import, it is useful to perform application specific cleaning and generate
ancillary fields that are widely used for downstream analyses. This helps to ensure consistency across analytics and reduces
scripting complexity for new users.
Covid Symptom Study-Specific Cleaning and Processing
The Covid Symptom Study data schema has seen rapid iteration since its inception, due to a number of factors. Firstly, the initial
app was rapidly released to allow users to contribute as soon as possible after the pandemic was declared. Secondly, the evolving
nature of the pandemic, particularly around prevalence in the population and availability and type of tests has necessitated
structural changes to the schema. Thirdly, this dataset is novel in terms of its scale and deployment for Epidemiological analysis,
and the initial wave of papers published by the research group has fed back into the schema.
Public health surveillance campaigns such as the Covid Symptom Study impose time constraints to software development,
with frequent changes in database structure and intense versioning to accommodate iterative refinements. The evolving
epidemiology of COVID-19, the response of governments and populations to the pandemic, and academic responses to papers
based on the dataset all shape the questions that are added to or removed from the app over time.
The dataset is only minimally validated at source. The fields often contain data of mixed type, and different fields can be in
mutual contradiction. Numeric values are only validated for type rather than sensible value ranges. Furthermore, the dataset
contains multiple competing schema for the same underlying data, and the app version is tied to the schema version, so users
who are using older versions of the app are still contributing to otherwise retired schema elements. As such, a considerable
amount of data cleaning and processing is required in order to extract data suitable for analysis.
Schema changes

The handling of COVID-19 tests in the dataset is an example of the complexity created by changes to the schema. Testing was
initially reported as an assessment logging activity, but this came with a number of problems. Firstly, a test needed to be logged
on the day it was taken for the assessment date to be treatable as the test date. Secondly, some users interpreted the test field as
something to be logged only when they took a test or received the result, whilst other users filled in intermediate assessments
with the pending status. Thirdly, this system did not allow for users to enter multiple tests unambiguously. Whilst this was not a
problem in the initial months of the pandemic, the ramping up of test availability necessitated a solution.
A new test table was introduced in June 2020, giving each test a unique id to allow multiple tests for each patient. However,
existing tests recorded in the old schema were not connected with new test entries, although many users re-entered old test
results in the new test format. Furthermore, new tests continued to be added by users in the old, assessment-based schema
format, logging on previous versions of the app. As such, there is no unambiguous way of determining whether tests in the old
format are replicated by tests in the new format. This is an example of a postprocessing activity with no unambiguously correct
output, which therefore requires at least a single, agreed upon algorithm to be consistently deployed to avoid inconsistencies
between related analyses.
Validation of user-entered values (weight, height, BMI)

In the Covid Symptom Study app, user-entered numeric values are only validated to ensure that they are numeric, as of the
time of writing. There are no validations of sensible ranges given the user-selected units of measurement. Some users enter
incorrect values, and some users enter values that appear sensible but only in some other unit (1.8 is a plausible height if the
user is entering height in metres, for example).
6/16

Quality metrics for test mechanism

The covid test table has a ‘mechanism’ field where the user is free to either select a categorical value indicating the test
mechanism, or enter free text relating to the test mechanism. Some free text clearly indicates the test type, whereas other free
text entries only infer the test type weakly, through inference such as ‘home test kit’. As such, a set of gradated flags are
generated that indicate the quality of the categorisation.
Generation of daily assessments

In case of multiple daily entries by the users, these assessments can optionally be quantised into a single daily assessment
that, for symptoms, corresponds to the maximum value for each symptom that the user reported in the day. This considerably
simplifies many downstream analyses.
Generation of patient-level assessment and test metrics

Analysis often involves the filtering of patients that are categorised by aspects of the assessments and tests that they have logged.
These include metrics such as whether the patient logged as being initially healthy, or whether they have ever logged a positive
test result.
Reproducibility and algorithm immutability
Reproducibility depends on the ability to reproduce a given analysis from a version of the dataset and a set of algorithms run on
the dataset. For this to be possible, algorithms must be considered immutable once implemented. This allows any subsequent
version of the software to generate results consistent with those of the software version in which the algorithm was introduced.
ExeTera does this by requiring that a version of any given algorithm that is created is treated as immutable in the code
base. This means that any target script is guaranteed to exhibit the same behaviour, provided that the following conditions
hold. Firstly, any algorithms written for ExeTera are explicitly versioned. Secondly, any randomness introduced must be given
consistent random seeds and, ideally, multiple sources of randomness should be given different random number generators.
Once an algorithm is used in analysis, it may no longer be altered in the codebase, even if it subsequently shown to contain
errors. This enables researchers to run multiple versions of the same algorithm as part of their analytics and understand how
sensitive their results are to changes and corrections. An example for this is the multiple versions of height / weight / body
mass index (BMI) cleaning that have been devised over the course of the project; each is available as separate versions of the
algorithm for reproducibility.
Implementation
ExeTera is implemented in the Python programming language. Python has two aspects that make it suitable for writing software
that performs data analytics and numerical analysis. Firstly, it is dynamically typed, which reduces code complexity and
verbosity 24 . Secondly, it has a strong ecosystem of scientific libraries and tools to mitigate the performance and memory
penalties that come with using a dynamically typed, byte-code interpreted language and runtime.
Working with Python

Code that is compiled and run directly in CPython (the reference Python implementation) executes in the Python interpreter.
The Python interpreter is extremely slow relative to optimised code such as that generated by compiled, optimised C/C++; in
many cases it is orders of magnitude slower. Python is dynamically typed, but its type system does not provide light-weight
objects to represent primitive types. Even numeric values such as integers and floats are stored as full objects, and typically
take up 28 bytes for a 4 byte integer value. This overhead precludes efficient memory usage when iterating over large numbers
of values.
Numpy 6, 7 is the Python community’s main tool for circumventing such time and space inefficiencies. Amongst other
features, it provides a library for space-efficient representations of multi-dimensional arrays, and a large library of time-efficient
operations that can be carried out on arrays.
The performance of such operations can be orders of magnitude faster than native CPython, but this is conditional on
minimising the number of transitions between Python code and the internal compiled code in which the operations are
implemented.
Not all code can be easily phrased to avoid transitions between CPython and Numpy internals. Where this is not possible,
Numba 25 is used to compile away the dynamic typing and object overhead, resulting in functions that execute at near optimised
C performance levels.
Serialised data representations

CSV is a very common way to portably represent large datasets, but it comes with many drawbacks, including a lack of strong
typing and an inability to rapidly index to a given location in the dataset. These issues become severely problematic at scale,
and so an alternative serialised data representation is required.
7/16

Data storage formats can be classified as primarily rowlocal or primarily column-local. This choice has key implications for analytics software. Row-local data formats store groups
of related fields for a given data entry together in memory. Column-local data formats store a specific value for a group of data
entries together in memory.
Data representation: row-local vs. column-local data formats

CSV format nearly always used in a row-local fashion, i.e. rows are data entries and columns
are fields. This typically makes CSV very slow to parse for a subset of the data; with CSV this is exacerbated because
escape-sequenced line-breaks mean that the entirety of each line must be parsed to determine the start of the next row. Even
without this issue, row-local data formats suffer from locality of reference issues 26 .
Row-local data format

Column-local data storage enables very efficient access to a given field. All the entries for a given
field are (effectively) contiguous in memory and so loading a single field can be done in an asymptotically optimal fashion.
When a dataset has many fields and a given operation operates on a smaller number of those fields, scaling the operation is a far
simpler proposition, technically 9, 27 .
The ability to load specific subsets of the data with maximal efficiency benefits both the ability to scale and the latency with
which operations can be performed on a small subset of the dataset. As such, a column-local data format is a preferable format.
Column-local data format

HDF5 10 is a data format for storing keys and their associated values in a
hierarchically organised, nested collection. In contrast with CSV, HDF5 stores data in a column-local format. It also allows
for data to be stored as binary, concrete data types. HDF5 permits a user to explore the overall structure of the data without
loading fields. Fields are loaded at the point that a user specifically requests the contents of a given field. This can be a direct
fetch of the entire field or an iterator over the field. This makes it a suitable initial data format for ExeTera, although alternative
columnar data storage formats are being considered to replace HDF5 for future development due primarily to issues of dataset
fragility and shortcomings relating to concurrent reading / writing.
HDF5 as the ExeTera serialised data format

Space-efficient operations

Most analysis of tabular data is performed through a combination of joins, sorts, filters and aggregations. ExeTera operates on
arrays of effectively unlimited length, particularly when certain preconditions are met, using the following techniques.
Sorting

Sorting is one of the key operations that must scale in order to process large datasets, as imposition of a sorted order enables
operations such as joins to scale. ExeTera uses several techniques to provide highly scalable sorting.
Rather than sorting data directly, ExeTera generates a sorted index that is a permutation of the
original order. This is used to scale related sort operations, and implement a soft sort, where fields are stored in a natural sorted
order and the permuted index applied when the field is read.
Generation of a sorted index

Multi-key sorts are memory intensive when keys are large, and expensive due to the
internal creation of tuples in the inner loops of sorts. Multi-key sorts in ExeTera are rephrased as a series of sorts on individual
keys from right to left, where the output of each sort step is a sorted index that is the input to the next sort step, using a stable
sort. Figure 5 shows pseudocode for this operation.
Scaling multi-key sorts on long arrays

ExeTera has a second sorting algorithm that can be selected if an array is too large to fit
into memory in its entirety. Such arrays are sorted via a two-phase approach in which the array is divided into subsets; each
subset is sorted, and the sorted subsets are merged together by maintaining a heap of views onto the sorted subsets. A separate
index is generated and maintained with the sorted chunks, so that the merge phase is stable. Figure 6 shows pseudocode for this
operation
Scaling sorts on very long arrays

The sorts described above, that produce a permutation of the original order, can be used to sort
multiple fields in a space-efficient fashion. For large arrays, the array can be permuted in turn and written back to disk, or the
permuted order maintained and reapplied when needed. ExeTera scales to provide this functionality even for very large arrays.
Sorting multiple fields

A number of operations become merges with various predicates when performed on fields
that have been sorted by the key field and can be performed in O(m + n) time where m and n are the lengths of the fields to
be merged. This includes joins and aggregations. ExeTera performs these operations as merges when the key field is sorted.
Importantly, arbitrarily large fields can be operated on in this way.
Operations on sorted fields

Joining
Generation of join maps

Rather than performing the join on the fields themselves, ExeTera first generates primary key and
foreign key index maps, which are then subsequently applied to the fields to be joined.
8/16

As with sorts above, once the mapping indices have been calculated, they are applied to each field on
the left side to map to the right side of the join, or vice-versa.
Joining multiple fields

Joining on sorted keys

When the data is sorted on the keys of the respective fields, ExeTera rephrases joins as ordered

merge operations.
Aggregation
Generation of aggregation maps / spans

Aggregation is another operation that ExeTera optimises through use of pregenerated indices, particularly in the case that the data is sorted in aggregation key order.
As with joins, when the data is sorted on the keys of the aggregated fields, joins are performed
by ExeTera in a very scalable and efficient fashion by precomputing spans representing ranges of the key field with the same
key value. This can be iterated over, and aggregations performed in a streaming fashion.
Aggregating on sorted keys

Data availability
The Covid Symptom Study dataset is available at https://healthdatagateway.org, by searching for “COVID-19 Symptom
Tracker Dataset”, or can be directly accessed at https://web.www.healthdatagateway.org/dataset/fddcb382-3051-4394-8436b92295f14259, at the time of writing.
Code for generating synthetic data is detailed in the Code availability section.

Code availability
All source code for ExeTera is made available through github under the Apache 2.0 license, at the time of writing. The code is
split up into three separate projects.
The core functionality for ExeTera is hosted at https://github.com/KCL-BMEIS/ExeTera.git and is available through
pypi via pip install exetera.
ExeTera

ExeTeraCovid The functionality for the Covid Symptom Study dataset, (algorithms, scripts and other resources) is hosted
at https://github.com/KCL-BMEIS/ExeTeraCovid.git and is available through pypi via pip install exeteracovid.
Installing exeteracovid installs exetera.

Code for creating evaluation datasets, including the dataset used to evaluate join operations, is hosted at
https://github.com/KCL-BMEIS/ExeTeraEval.git.
ExeTeraEval

Contribution statement
BM, EK, MSG, and AV contributed to the software. MSG, CHS, EM, LSC, and MA provided feedback on the application
programmer interface. RJ, JW and TS created the Covid Symptom Study that this software was created to provide data curation
for. BM and MM wrote the manuscript. BM, EK, MSG, CHS, EM, LSC, MA, AV, ATC, PWF, RD, JW, TS, CJS, MM and SO
reviewed and edited the draft. MM and SO supervised the project.

References
1. Silvertown, J. A new dawn for citizen science. Trends ecology & evolution 24, 467–471 (2009).
2. Newman, G. et al. The future of citizen science: emerging technologies and shifting paradigms. Front. Ecol. Environ. 10,
298–304 (2012).
3. Follett, R. & Strezov, V. An analysis of citizen science based research: usage and publication patterns. PloS one 10,
e0143687 (2015).
4. Heigl, F., Kieslinger, B., Paul, K. T., Uhlik, J. & Dörler, D. Opinion: Toward an international definition of citizen science.
Proc. Natl. Acad. Sci. 116, 8089–8092 (2019).
5. Drew, D. A. et al. Rapid implementation of mobile technology for real-time epidemiology of covid-19. Science 368,
1362–1367 (2020).
6. Walt, S. v. d., Colbert, S. C. & Varoquaux, G. The numpy array: a structure for efficient numerical computation. Comput.
science & engineering 13, 22–30 (2011).
7. Harris, C. R. et al. Array programming with numpy. Nature 585, 357–362 (2020).
9/16

8. Wes McKinney. Data Structures for Statistical Computing in Python. In Stéfan van der Walt & Jarrod Millman (eds.)
Proceedings of the 9th Python in Science Conference, 56 – 61 (2010).
9. Stonebraker, M. Sql databases v. nosql databases. Commun. ACM 53, 10–11 (2010).
10. The HDF Group. Hierarchical data format version 5 (2000-2010).
11. Hunter, J. D. Matplotlib: A 2D graphics environment. Comput. Sci. & Eng. 9, 90–95 (2007).
12. Kluyver, T. et al. Jupyter notebooks – a publishing format for reproducible computational workflows. In Loizides, F. &
Schmidt, B. (eds.) Positioning and Power in Academic Publishing: Players, Agents and Agendas, 87 – 90 (2016).
13. Ni Lochlainn, M. et al. Key predictors of attending hospital with covid19: An association study from the covid symptom
tracker app in 2,618,948 individuals. medRxiv (2020).
14. Costeira, R. et al. Estrogen and covid-19 symptoms: associations in women from the covid symptom study. medRxiv
(2020).
15. Bowyer, R. et al. Geo-social gradients in predicted covid-19 prevalence and severity in great britain: results from 2,266,235
users of the covid-19 symptoms tracker app. medRxiv (2020).
16. Bataille, V. et al. Diagnostic value of skin manifestation of sars-cov-2 infection. medRxiv (2020).
17. Dask Development Team. Dask: Library for dynamic task scheduling (2016).
18. RAPIDS Development Team. RAPIDS: Collection of Libraries for End to End GPU Data Science (2018).
19. Floratou, A. Columnar Storage Formats, 1–6 (Springer International Publishing, 2018).
20. Vohra, D. Apache parquet. In Practical Hadoop Ecosystem, 325–335 (2016).
21. Menni, C. et al. Real-time tracking of self-reported symptoms to predict potential covid-19. Nat. medicine 26, 1037–1040
(2020).
22. Nguyen, L. H. et al. Risk of covid-19 among front-line health-care workers and the general community: a prospective
cohort study. The Lancet Public Heal. 5, e475–e483 (2020).
23. Zazzara, M. B. et al. Delirium is a presenting symptom of covid-19 in frail, older adults: a cohort study of 322 hospitalised
and 535 community-based older adults. medRxiv (2020).
24. Delorey, D. P., Knutson, C. D. & Chun, S. Do programming languages affect productivity? a case study using data
from open source projects. In First International Workshop on Emerging Trends in FLOSS Research and Development
(FLOSS’07: ICSE Workshops 2007), 8–8 (2007).
25. Lam, S. K., Pitrou, A. & Seibert, S. Numba: A llvm-based python jit compiler. In Proceedings of the Second Workshop
on the LLVM Compiler Infrastructure in HPC, LLVM ’15 (Association for Computing Machinery, New York, NY, USA,
2015).
26. Drepper, U. What every programmer should know about memory (2007).
27. Stonebraker, M. et al. C-store: A column-oriented dbms. In Proceedings of the 31st International Conference on Very
Large Data Bases, VLDB ’05, 553–564 (2005).

Figures & Tables

Row count
Time to import (seconds)

Patients
4,684,218
386.3

Assessments
184,045,890
3849.6

Tests
1,392,782
20.1

Diet
1,553,651
52.9

Table 1. Import performance for the Covid Symptom Study snapshot dated 8th October, 2020.

10/16

Data source
Pandas read time (seconds)
Time to import (seconds)

N=1
10.27
0.0242

N=2
11.49
0.0315

Patient fields read
N=4
N=8
N=16
12.84
13.92
15.53
0.0465 0.0558 0.0787

N=32
18.83
0.0928

Table 2. Reading fields from the Covid Symptom Study 2020/10/08 snapshot, patient table. Times (in seconds) to load the
first 1, 2, 4, 8, 16, and 32 fields respectively.

Figure 1. A left join of a simplified dummy patent and assessment dataset. The left join is from patients on the right to
assessments on the left to generate a patient_age field in assessment space.

Data source
Pandas read time (seconds)
Time to import (seconds)

N=1
144.1
0.897

N=2
X
1.810

Patient fields read
N=4
N=8 N=16
X
X
X
2.292 2.878 3.296

N=32
X
4.056

Table 3. Reading fields from the Covid Symptom Study 2020/10/08 snapshot, assessment table. Times (in seconds) to load
the first 1, 2, 4, 8, 16, and 32 fields respectively. X means that the operation could not be completed due to lack of memory.
left_fk_ids
0
1
1
2
4
5
5
6
8
9

right_ids
0
1
2
3
4
5
6
7
8
9

right_data_0
51
98
31
4
49
80
43
47
97
56

right_data_1
36
34
47
43
18
85
20
71
87
64

Table 4. The first ten rows of the dataset used to evaluate joins, in the two field case. The right_ids field is the primary
key for the right hand side and the left_fk_ids field is the foreign key on the left hand side. This data is generated by
running the scripts provided in the ExeTeraEval github project, detailed in the code availability section.

11/16

Row count
1,048,576 (220 )
2,097,152 (221 )
4,194,304 (222 )
8,388,608 (223 )
16,777,216 (224 )
33,554,432 (225 )
67,108,864 (226 )
134,217,728 (227 )
268,435,456 (228 )
536,870,912 (229 )
1,073,741,824 (230 )
2,147,483,648 (231 )

Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera
Pandas
ExeTera

N=1
0.152
0.249
0.281
0.266
0.710
0.297
1.66
0.359
3.96
0.474
8.17
0.701
18.9
1.17
40.7
2.09
X
3.98
X
7.72
X
15.3
X
30.2

Time to left join fields (seconds)
N=2
N=4
N=8 N=16
0.152 0.153 0.153 0.164
0.254 0.265 0.291 0.335
0.283 0.285 0.291 0.314
0.278 0.300 0.349 0.439
0.721 0.727 0.741 0.773
0.322 0.369 0.460 0.646
1.67
1.68
1.71
1.78
0.404 0.497 0.674 1.05
3.96
3.99
4.06
4.18
0.563 0.746 1.09
1.81
8.33
8.27
8.46
8.65
0.879 1.23
1.93
3.35
19.1
19.0
19.2
19.8
1.52
2.21
3.62
6.40
41.0
41.3
41.4
42.7
2.80
4.23
6.96
12.6
X
X
X
X
5.35
8.18
13.8
24.9
X
X
X
X
10.4
16.2
26.9
50.3
X
X
X
X
21.1
32.8
55.3 100.8
X
X
X
X
42.8
65.6 112.1 241.8

N=32
0.180
0.422
0.347
0.617
0.837
1.02
1.91
1.81
4.44
3.27
9.22
6.20
20.8
12.2
X
23.7
X
48.2
X
95.9
X
197.7
X
602.3

Table 5. Left join pandas merge vs. ExeTera ordered_merge_left: row count (rows) and fields joined count
(columns). All times are in seconds. X means that the operation could not be completed due to lack of memory. Bold entries
indicate the fastest join.

Data source
August 1st row count
September 1st row count
Rows only in old
Rows only in new
Rows updated
Rows not updated
Journaled row count
Time to import (seconds)

Patients
4,402,930
4,480,270
2,519
86,301
1,632,849
2,761,120
6,122,080
145.1

Journaling dataset snapshots
Assessments
Tests
Diet
129,423,329
749,937
659
153,655,115
991,128
1,291,237
108,231
485
0
243,40,017
241,676
1,290,578
702
18,169
630
129,314,396
731,283
29
153,764,048 1,009,782 1,291,867
2273
6.616
8.179

Table 6. Journaling Covid Symptom Study snapshots from 1st August 2020 and 1st September 2020. This table shows results
in terms of row counts and the time taken to perform the journaling.

12/16

2020-10-06
2020-09-29
2020-09-22
2020-09-15
2020-09-08
2020-09-01
2020-08-25
2020-08-18
2020-08-11
2020-08-04
2020-07-28
2020-07-21
2020-07-14
2020-07-07
2020-06-30
2020-06-23
2020-06-16
2020-06-09
2020-06-02
2020-05-26
2020-05-19
2020-05-12
2020-05-05
2020-04-28
2020-04-21
2020-04-14
2020-04-07
2020-03-31
2020-03-24
0.00

0.20

0.15

0.10

0.05
'Unhealthy' assessment fraction

'Unhealthy' assessments as a fraction of assessments by week

Positive test fraction
0.25

2020-10-06
2020-09-29
2020-09-22
2020-09-15
2020-09-08
2020-09-01
2020-08-25
2020-08-18
2020-08-11
2020-08-04
2020-07-28
2020-07-21
2020-07-14
2020-07-07
2020-06-30
2020-06-23
2020-06-16
2020-06-09
2020-06-02
2020-05-26
2020-05-19
2020-05-12
2020-05-05
2020-04-28
2020-04-21
2020-04-14
2020-04-07
2020-03-31
2020-03-24
0

7

6

5

4

3
Million tests per week

Assessment counts by week

'Healthy'
'Unhealthy'
9

8

2

1

Week starting

Week starting

Figure 2. Seven day summary of assessments from the Covid Symptom Study snapshot dated 14th October 2020. The upper
chart shows the number of assessments, coloured by whether the patient logged as healthy or unhealthy. The lower chart shows
the assessments logged as unhealthy as a fraction of assessments logged for that seven day period

13/16

Figure 3. Construction of a journaled dataset Two snapshots of a simplified dummy dataset representing COVID-19 tests, one
from 2020/08/22 and one from 2020/08/29 are used to construct a journaled dataset.

14/16

{
" exetera ": {
" version ": "1.0.0"
},
" schema " : {
" alpha ": {
" p r i m a r y _ k e y s " : [ " a_pk " ] ,
" fields ": {
" a_pk " : { " f i e l d _ t y p e " : " n u m e r i c " , " v a l u e _ t y p e " : " i n t 3 2 " } ,
" field_x ": { " field_type ": " fixed_string " , " length ": 5 } ,
" field_dt ": { " field_type ": " datetime " }
}
},
" beta ": {
" p r i m a r y _ k e y s " : [ " b_pk " ] ,
" foreign_keys ": {
" a _ f k " : { " s p a c e " : " a l p h a " , " key " : " i d " }
},
" fields ": {
" b_pk " : { " f i e l d _ t y p e " : n u m e r i c , " v a l u e _ t y p e " : " i n t 6 4 " } ,
" a_fk " : { " f i e l d _ t y p e " : numeric , " v a l u e _ t y p e " : " i n t 3 2 " } ,
" field_m ": {
" field_type ": " categorical " ,
" categorical ": {
" value_type ": " int8 " ,
" s t r i n g s _ t o _ v a l u e s " : { " " : 0 , " no " : 1 , " y e s " : 2 }
}
},
" field_n ": {
" field_type ": " categorical ": ,
" categorical ": {
" value_type ": " int8 " ,
" strings_to_values ": { " left ": 0 , " right ": 1 } ,
" out_of_range ": " f r e e t e x t "
}
},
" f i e l d _ d t " : {" f i e l d _ t y p e " : " d a t e t i m e " , " o p t i o n a l " : " True "}
}
}
}
}
Figure 4. An illustrative, minimal example of an ExeTera JSON schema for use when importing data from CSV.

15/16

F : a l i s t o f u n s o r t e d d a t a f i e l d s t h a t a r e b e i n g s o r t e d on
Du : t h e u n s o r t e d d a t a f o r a g i v e n f i e l d
Ds : t h e s o r t e d d a t a f o r a g i v e n f i e l d
I : I n d e x from s o r t i n g d a t a w i t h an ’ a r g s o r t ’ f u n c t i o n
A: Accumulated s o r t e d i nd e x
g e t d a t a : a f u n c t i o n t h a t f e t c h e s t h e d a t a from a f i e l d
range : a f u n c t i o n t h a t r e t u r n s i n t e g e r v a l u e s between s t a r t ( i n c l u s i v e )
and end ( e x c l u s i v e )
a r g s o r t : a f u n c t i o n t h a t r e t u r n s t h e p e r m u t a t i o n o f i n d i c e s by s o r t i n g a
vector of data
p e r m u t e : a f u n c t i o n t h a t t a k e s a d a t a f i e l d and an i n d e x and a p p l i e s t h e
l a t t e r to the former
Du = g e t d a t a ( F [ 0 ] )
Ds = p e r m u t e ( Du , A)
I = a r g s o r t ( Ds )
A = p e r m u t e (A, I )
for i in range (1 , n ) :
Du = g e t d a t a ( F [ i ] )
Ds = p e r m u t e ( Du , A)
I = a r g s o r t ( Ds )
A = p e r m u t e (A, I )
r e t u r n A_n
Figure 5. Pseudocode for a multi-key sort that outputs a sorted index for subsequent application to many fields.
D : t h e d a t a v e c t o r t o be s o r t e d
Ds : t h e s t h
subset of D
I : a s e t o f u n p e r m u t e d i n d i c e s ( t h e i n d i c e s o f e a c h e l e m e n t o f D)
I s : the sth subset of I
s u b s e t s : a f u n c t i o n t h a t s p l i t s a v e r y l a r g e s e q u e n c e up i n t o s m a l l e r s u b s e t s
s o r t _ a n d _ a p p l y : a f u n c t i o n t h a t s o r t s a d a t a v e c t o r and p e r m u t e s an i n d e x
vector accordingly
c u r s o r : a view o n t o a v e c t o r t h a t a m o r t i s e s t h e c o s t o f r e a d i n g i n d i v i d u a l
e l e m e n t s o f t h a t v e c t o r from a d r i v e
b e s t : a f u n c t i o n t h a t s e l e c t s t h e n e x t e l e m e n t from a c o l l e c t i o n o f d a t a and
index cursors
next : a function t h a t increments a cursor
Ic = l i s t ()
Dc = l i s t ( )
F o r s i n s u b s e t s (D ) :
Dis , Dps = s o r t _ a n d _ a p p l y ( Ds , I s )
I c . append ( c u r s o r ( Dis ) )
Dc . a p p e n d ( c u r s o r ( Dps ) )
Dr = [ ]
F o r i i n l e n g t h (D ) :
j = b e s t ( I c , Dc )
Dr . a p p e n d ( Dc [ j ] )
next ( Ic [ j ] )
n e x t ( Dc [ j ] )
Figure 6. Pseudocode for a streaming sort that outputs a sorted index for subsequent application to many fields.

16/16

