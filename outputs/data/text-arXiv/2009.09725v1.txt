arXiv:2009.09725v1 [eess.IV] 21 Sep 2020

Improving Automated COVID-19 Grading with
Convolutional Neural Networks in Computed Tomography
Scans: An Ablation Study
Coen de Vente, Luuk H. Boulogne ∗, Kiran Vaidhya Venkadesh, Cheryl Sital,
Nikolas Lessmann, Colin Jacobs, Clara I. Sánchez, Bram van Ginneken †
Abstract
Amidst the ongoing pandemic, several studies have shown that COVID-19 classification and grading using computed
tomography (CT) images can be automated with convolutional neural networks (CNNs). Many of these studies
focused on reporting initial results of algorithms that were assembled from commonly used components. The choice
of these components was often pragmatic rather than systematic. For instance, several studies used 2D CNNs even
though these might not be optimal for handling 3D CT volumes. This paper identifies a variety of components
that increase the performance of CNN-based algorithms for COVID-19 grading from CT images. We investigated
the effectiveness of using a 3D CNN instead of a 2D CNN, of using transfer learning to initialize the network, of
providing automatically computed lesion maps as additional network input, and of predicting a continuous instead
of a categorical output. A 3D CNN with these components achieved an area under the ROC curve (AUC) of 0.934
on our test set of 105 CT scans and an AUC of 0.923 on a publicly available set of 742 CT scans, a substantial
improvement in comparison with a previously published 2D CNN. An ablation study demonstrated that in addition
to using a 3D CNN instead of a 2D CNN transfer learning contributed the most and continuous output contributed
the least to improving the model performance.

Keywords:

1

deep learning, 3D convolutional neural network, COVID-19, CO-RADS, medical imaging.

Introduction

Imaging of COVID-19 with chest computed tomography (CT) has been found helpful for diagnosis of this
disease in the current pandemic [36]. With the aim to reduce the workload of radiologists, various machine
learning techniques have been proposed to automatically grade and classify the presence of COVID-19 in CT
images [5, 6, 8, 15, 19–23, 28–30, 32–34, 37]. Automatic COVID-19 classification methods have already been
deployed in several medical centers [15].
The most common technique for automatic COVID-19 classification from CT images is the Convolutional
Neural Network (CNN) [14, 18], which is the current state-of-the-art for image classification [17]. The works
that use this approach can be divided into those that use 2D CNNs [6, 8, 20, 22, 29, 33] and those that
use 3D CNNs [19, 21–23, 32, 34, 37]. While 3D CNNs are directly capable of exploiting 3D information
present in CT volumes, 2D CNNs can only indirectly use 3D information by aggregating the output of 2D
networks for individual slices of the image to produce an image level prediction. 3D CNNs are typically more
memory intensive than 2D CNNs, but Graphics Processing Units (GPUs) with sufficient memory to train
∗ Coen

de Vente and Luuk H. Boulogne contributed equally.
(Corresponding author: L. H. Boulogne, e-mail:
luuk.boulogne@radboudumc.nl)
† All authors are with the Radboud university medical center, Radboud Institute for Health Sciences, Department of Radiology and Nuclear Medicine, Nijmegen, The Netherlands.

1

b) classiﬁcation network
a) input

3D

CT scan

c) CO-RADS output

continuous
70%

1

segmentation
network

2D

1

2

3

4

5

categorical
4%

shared weights

max pooling

5%

lesion
segmentation

8%
73%

1
2

10%

Fig. 1: Summary of the processing pipeline. (a) The input CT scan is fed into a lesion segmentation network.
The CT and the lesion segmentation are used as separate input channels to the classification network.
In one of the ablation study experiments, we left out this lesion segmentation input. (b) We compared
a 3D (top) with a 2D approach (bottom). The 3D network takes as input the full volume. The 2D
network uses individual slices as input. (c) We compared a continuous output to a categorical output
in the ablation study.
3D models are becoming increasingly available. Moreover, radiologists are specifically instructed to take 3D
information into account by inspecting different orthogonal views for assessing the suspicion of COVID-19
in CT scans [24]. This indicates that 3D information is essential for radiologists in assessing the patterns
indicative for COVID-19. Additionally, the slice thickness of CT scans increasingly becomes smaller [31] so
that the scans contain more detailed 3D information. We therefore hypothesize that 3D CNNs are more
suitable for COVID-19 classification from CT scans than 2D CNNs.
In order to determine the optimal direction for the development of automatic COVID-19 classification
systems, comparative studies need to be performed that evaluate different approaches. To indicate the
generalization capabilities of automatic COVID-19 classification systems, some methods have been validated
on data from different centers than the data that were used for training [19,23,34]. Also, the same validation
methods, such as Receiver Operating Characteristic (ROC) curves and the area under the ROC curve
(AUC), have been reported across different studies [15,19,20,22,23,29,32–34,37]. However, since each study
used different datasets for training and for validation, a fair direct comparison of the performance of these
algorithms is still not possible. However, the “CT images and clinical features for COVID-19” (iCTCF)
dataset was recently made publicly available and does allow for a fair comparison of COVID-19 classification
methods [1].
This paper compares a slice-based 2D CNN approach for COVID-19 classification to a 3D CNN approach.
We trained and evaluated the approaches on the same internal dataset and additionally tested them on
the iCTCF dataset to allow for a fair comparison between these algorithms. Moreover, we investigated
performance changes due to 1) using transfer learning for 2D and 3D COVID-19 classification models, 2)
using prior information in the form of COVID-19 related lesion segmentations as additional input to the
network, 3) replacing the categorical output of our model with a continuous output.

2

We furthermore created an open grand challenge [3] for evaluating and comparing different COVID-19
classification algorithms. In this challenge, algorithms are evaluated on the iCTCF dataset that we used
in this paper and can be compared to the methods presented in this paper, as well as to other COVID-19
grading and classification algorithms that are submitted to the challenge.

2

Related Work

3D CNNs were initially proposed for processing video data [14], where the third dimension of the convolutional layers dealt with the temporal dimension. In later work, 3D CNN architectures were derived from 2D
CNN architectures by expanding the 2D filters into 3D [7]. Methods based on these inflated 3D CNNs, in
particular the Inflated Inception V1 (I3D) model, have recently been successfully employed for lung nodule
detection and scan-level classification tasks from thorax CT scans [4, 12].
Transfer learning (TL) is widely used in research on deep learning in medical imaging [25]. With TL,
models are initialized with pre-trained weights from models trained on a different task or dataset. They are
commonly pre-trained on the ImageNet [11] dataset that contains a large variety of 2D natural images. TL
speeds up training and can offer performance gains for large models [25]. It has been used in several 2D CNN
COVID-19 classification methods [20, 29, 33]. Pre-trained weights have also been used for 3D CNN-based
methods. Wang et al. [34] applied TL by pre-training a model for COVID-19 classification on a large number
of CT scans from lung cancer patients. I3D models can conveniently be initialized with inflated 2D weights.
2D weights have been used to pre-train I3D models for video classification [7] and chest CT classification [4]
tasks.
Before presenting CT images to the CNN, they are often pre-processed by extracting the lung region
using lung or lobe segmentation algorithms. These lung regions are then used either for cropping around
and centering to the lungs [19, 21, 29, 34] and/or by suppressing non-lung tissue [6, 15, 20, 23, 29, 32, 34, 37].
Lessmann et al. [19] also added a lesion segmentation to the input of their model.
Previous works have trained models to discern between COVID-19 positive and negative patients [5, 28,
29, 37], COVID-19 positive patients and patients with other types of pneumonia [23, 33, 34], and between
all three [6, 20, 32]. In this work, we followed Lessmann et al. [19] and trained our models to produce CORADS [24] scores on chest CT scans of suspected COVID-19 patients. The CO-RADS score denotes the
suspicion of COVID-19 on a scale from 1 to 5 and was developed for standardizing reporting of CT scans
of patients suspected with COVID-19 [24]. Scoring systems, like CO-RADS, have been advocated for better
communication between radiologists and other healthcare providers [19, 24].

3
3.1

Data
Training and internal test data

The internal dataset contained CT scans from consecutive patients who presented at the emergency wards of
the Radboud University Medical Center, the Netherlands in March, April and May 2020 and were referred
for CT imaging because of moderate to severe COVID-19 suspicion. Medical ethics committee approval was
obtained prior to the study. Further details such as imaging parameters can be found elsewhere [19].
CO-RADS scores were reported by a radiologist as part of routine interpretation of the scans. CO-RADS
1 was used for normal or non-infectious etiologies, having a very low level of suspicion. CO-RADS 2 was used
if the CT-scan was typical for other infections than COVID-19, indicating a low level of COVID-19 suspicion.
CO-RADS 3 implies equivocal findings and features compatible with COVID-19, but characteristics of other
diseases are also found. CO-RADS 4 and 5 indicate a high and very high level of COVID-19 suspicion,
respectively.
We randomly split the dataset into a development set with 616 patients and an internal test set of 105
patients. The patients in the development set were split into 75% for training and 25% for validation using
data stratification based on the CO-RADS scores. The distribution of CO-RADS scores over the different

3

Tab. 1: Number of CT Images in Internal Dataset.
CO-RADS
1

2

3

4

5

Total

Neg

Pos

Development set
Training
253
Validation
81
Internal test set 20

71
24
10

78
26
19

37
11
17

73
23
39

512
165
105

324
105
30

188
60
75

Total

105

123

65

135

782

459

323

354

Tab. 2: Number of CT Images in External Dataset.
Grade [22]
Control

Mild

Regular

Severe

Critically ill

Total

Neg

Pos

207

23

363

117

32

742

207

535

splits is displayed in Table 1. All data splits were made such that all scans from a patient with multiple
visits ended up in the same split.

3.2

External test data

For external evaluation, we used the publicly available CT images and clinical features for COVID-19 dataset
(iCTCF) dataset [1,22]. Since we focused on comparing architectures for CT image processing for COVID-19
classification, we did not incorporate the clinical features from this dataset into the input for our models. In
iCTCF, patients were categorized with a Chinese grading system that distinguishes the classes as Control,
Mild, Regular, Severe, Critically ill and Suspected. Since there was no etiological evidence available for the
presence of COVID-19 in Suspected cases [22], we did not use them for testing our models. The distribution
of the other classes is displayed in Table 2. The grading system uses etiological laboratory confirmation and
other factors such as clinical features and CT imaging [22]. The control cases include both healthy patients
and patients with community acquired pneumonia. Most of the iCTCF data has been made publicly available,
but some CT scans were not available at the time of conducting this study. We validated our models with
all available data from the first iCTCF cohort for which etiological evidence for the presence of COVID-19
was available [1].

4
4.1

Methods
2D and 3D architectures

We compared 2D CNNs to 3D CNNs for the task of COVID-19 classification from CT scans as summarized
in Fig. 1. Since we used scan-level labels for training and testing our models, the 2D architecture required
the integration of a slice-wise reduction step, while the 3D architecture did not. Therefore, we could not
simply consider a 3D architecture and compare it with the same model in which the 3D convolutions were
replaced with 2D convolutions. We instead compared two architectures that had been trained and evaluated
before for the task of CT scan classification.
The 3D model we trained was 3D inflated Inception V1 (I3D) [7], which was previously used as part
of a system called CORADS-AI [19]. The 2D model we trained was COVNet [20], which extracts features
of individual axial slices using a 2D ResNet-50 architecture [13]. A global max pooling step reduces these
features to a 1D vector, to which a fully connected layer is applied with an output size equal to the number

4

of classes. The code of COVNet has been made publicly available by the authors so that we were able to
use the original implementation [2].

4.2

Lesion map as prior information

To aid the model in localizing COVID-19 related parenchymal lesions, we provided a lesion segmentation map
as additional input in an extra input channel. A 3D U-net which segments ground-glass opacities (GGOs)
and consolidations [19] provided these segmentations. GGOs and consolidations are biomarkers with major
importance in diagnosing COVID-19 [24].

4.3

Pre-training

For both the 2D and 3D CNNs, we used TL from natural image classification tasks. Following Lessmann et
al. [19], the 3D model was initialized with a checkpoint from Carreira and Zisserman [7], who first trained
a 2D Inception V1 network on ImageNet [11] and afterwards inflated the model to 3D by replacing 2D
convolution kernels with 3D kernels. Subsequently, this 3D model was further trained on RGB video data
from the Kinetics dataset [16]. Following Li et al. [20], the ResNet-50 [13] backbone of the 2D CNN that we
applied to individual axial slices was also pre-trained on ImageNet.

4.4

Continuous output

The standard output format of CNNs used for categorical classification does not capture the ordinal nature
of the CO-RADS scoring system. Furthermore, although the CO-RADS scoring system allows for a higher
level of interpretability than a binary system, the fact that a CO-RADS suspicion score of 3 indicates that
it is unclear whether COVID-19 is present makes it difficult to decide on the onset of the positive class for
the predicted scores in ROC analyses. For these reasons, we considered the CO-RADS classification to be a
regression task. Hence, the model had one output node that was forced to the range (0, 1) using the sigmoid
function. CO-RADS scores were mapped to target values in the range [0, 1] with a uniform spacing between
CO-RADS classes such that CO-RADS scores of 1 and 5 were assigned target values of 0 and 1, respectively.
As the network had one output node, binary cross-entropy was used as loss function. With this method,
unlike a standard categorical approach with a softmax layer and categorical cross-entropy loss, predictions
that are further off from the target are penalized more heavily than predictions that are closer. To obtain
a CO-RADS score during inference, the sigmoid output was multiplied by 4, rounded to the nearest integer
and added to 1. De Vente et al. [10] explored this approach for prostate cancer grading and found that it
outperformed other regression and categorical output methods.

4.5

Pre-processing

The CT scans were clipped between -1100 and 300 Hounsfield units, normalized between 0 and 1, and
resampled to a voxel spacing of 1.5 mm3 using linear interpolation. The scans were further pre-processed
using a lung segmentation algorithm that was trained on data from patients with and without COVID19 [35]. More specifically, any slices with a distance of 10 mm or more to the lung mask were discarded and
the remaining slices were cropped to 240 × 240 pixels around the center of the mask. Following previous
research with I3D models [4,7,12], we trained our models with a fixed 3D input size. To achieve this without
adding extra slices that do not contain information regarding the presence of COVID-19, we uniformly
sampled 128 axial slices along the z-axis.

4.6

Training

We trained all networks with a batch size of 2, the Adam optimizer with β1 = 0.9, β2 = 0.999, and a learning
rate of 10−4 . Data augmentation consisted of zooming, rotation, shearing and elastic deformations in the
axial plane, translation in all directions, and additive Gaussian noise. To correct for the class imbalance,

5

we monitored the performance on the validation data in the development set during training with balanced
samples based on the distribution of CO-RADS classes in the training set. We used early stopping with a
patience of 10 000 training batches and the quadratic weighted kappa (QWK) on the validation set for the
stopping criterion. Gradient checkpointing [9] was used to enable a batch size of 2 for the 2D model.
To rule out the possibility that performance differences between the 3D and 2D approach were due to
other factors such as pre-processing or data augmentation, we only took the architecture implementation
from the publicly available code for the 2D model [20] and kept all other hyperparameters the same during
training. Nevertheless, we also trained 2D models with the pipeline implemented by Li et al. [20]. Unless
otherwise specified, the results in Section 5 were obtained with the prepossessing and training pipeline
described here and not with the original pipeline.
Each model was trained on a single GPU, using NVIDIA GeForce GTX TITAN X, GeForce GTX 1080,
GeForce GTX 1080 Ti, GeForce RTX 2080 Ti, and TITAN Xp cards.

4.7

Ensembling

The models were sensitive to the randomness of the training process introduced by initialization of weights
without pre-training, sample selection, and data augmentation. In order to enable stable comparisons, we
obtained ensembles by training 10 instances of the same model with different random seeds. The ensemble
output was obtained by taking the mean of the individual model outputs. For categorical model ensembles,
the output was the mean of the probability output vectors of the individual models. All results presented in
Section 5 were obtained from ensembles unless stated otherwise.

4.8

Evaluation

We evaluated the CO-RADS scoring performance using the QWK score. This measure accounts for the
ordinal nature of the CO-RADS score by weighting mismatches between true and predicted labels differently
based on the magnitude of the error. Following previous works on COVID-19 classification and grading
[15, 19, 20, 22, 23, 29, 32–34, 37], diagnostic performance was evaluated using the AUC and ROC curves.
We calculated 95% confidence intervals (CIs) with non-parametric bootstrapping and 1000 iterations [26].
Statistical significance was computed with the same bootstrapping method [27].
The AUCs that our models achieved on the external test set are additionally listed on the Grand Challenge
platform [3] to allow for a direct comparison between our and future COVID-19 grading and classification
solutions.

5
5.1

Results
2D vs. 3D CNNs

On the internal dataset, both the AUC and the QWK scores were significantly higher for the full 3D model
(with transfer learning, lesion maps and continuous output) than for the full 2D model (p < .001 for both
metrics). Figures 2 and 5 show the corresponding CIs and ROC analyses respectively. Fig. 3 shows prediction
examples from the full 3D, full 2D and ablated 3D models in blue, yellow, and black respectively.
When training an ensemble with the pipeline from Li et al. [20] we obtained a lower performance on the
internal test set than when we applied the 2D model in our own pipeline. With the pipeline from Li et al., we
obtained a QWK of 0.567 (95% CI: 0.411-0.703, p = .054) and a lower AUC of 0.828 (95% CI: 0.741-0.906,
p = .206).
Fig. 4 shows confusion matrices for the two dimensionalities. For 13 scans, the full 3D approach had
predictions that were more than one CO-RADS category off. For the full 2D approach this was the case for
24 scans. Furthermore, the full 3D approach had no cases that were further off than 2 categories, while the
full 2D approach did have two of these cases.
On average, training of the individual 3D models took approximately 16 hours (21 100 iterations), while
it took about 30 hours (30 000 iterations) for the 2D models.
6

5.2

Ablation study

The results of an ablation study to investigate the effect of each of the additional components added to the
3D CNN are shown in Fig. 2. Removing any of the additions had a negative effect on both the AUC and
the QWK, which were 0.934 and 0.813 for the 3D model without ablations respectively. The biggest drop
in performance occurred when removing pre-training from the full model, which reduced the AUC to 0.895
(p = .004) and the QWK to 0.751 (p = .011). Removing the lesion segmentation input from it reduced
the AUC to 0.909 (p = .116) and the QWK to 0.789 (p = .220). Replacing the regression approach with
a categorical target had the smallest effect on performance, reducing the AUC to 0.928 (p = .237) and the
QWK to 0.807 (p = .397). Fig. 3 shows prediction examples from the ablation study models in black. The
models without pre-training and without lesion input needed more iterations than the 3D model, which
required 21 100 iterations on average. Their mean numbers of iterations were 26 050 and 26 300, respectively.
The categorical output, however, only needed 12 950 iterations to finish training on average.

Quadratic weighted
Dimensionality

Ablation study

1.00

0.928
(0.876, 0.974)

0.909
(0.851, 0.957)

0.895
(0.818, 0.957)

rai

3D

wi

wi

tho

ut

pre
t
ut
tho
wi
3D

le
th
cat sion
ma
eg
ori
p
cal
ou
tpu
t

nin
g

0.858
(0.777, 0.928)

2D

rs)
(ou

ut

wi
3D

3D

0.807
(0.735, 0.872)

0.789
(0.711, 0.856)

Performance

3D

rai

tho

wi

ut

3D

tho
wi
3D

0.80

0.65

pre
t

3D

0.85

0.70

le
th
cat sion
ma
eg
ori
p
cal
ou
tpu
t

rs)

2D

0.4

0.90

0.75

nin
g

0.5

(ou

0.751
(0.661, 0.829)

0.670
(0.567, 0.764)

0.813
(0.743, 0.873)

Performance

0.6

Ablation study

0.95

0.8
0.7

Dimensionality

0.934
(0.876, 0.981)

0.9

AUC

Fig. 2: Comparison of 2D and 3D CNNs and ablation study for automatic COVID-19 grading from CT
images. The analysis was performed on the internal test set. The error bars indicate the 95% CIs.
The AUC was computed with CO-RADS 1-2 as the negative class (30 scans) and CO-RADS 3-5 as
the positive class (75 scans).

7

5
CO-RADS

4
3
2

ut
pr 2D
tho etrai
nin
ut
wi
g
th
l
cat esio
nm
eg
ori
a
cal p
ou
tpu
t
3D

3D

tho
wi
3D

wi

rs)
(ou

3D

Ra

dio

log

ist

1

(a)

5
CO-RADS

4
3
2

tho
2D
3D ut p
r
wi
tho etrai
3D
n
ut
wi
ing
th
l
cat esio
n
eg
ori map
cal
ou
tpu
t

(ou
rs)
3D

wi

3D

Ra
d

iol

og
ist

1

(b)

5
CO-RADS

4
3
2

3D

wi

Ra
dio
log
is
3D t
(ou
rs)

tho
2D
3D ut p
r
wi
tho etrai
3D
n
ut
wi
ing
th
l
cat esio
n
eg
ori map
cal
ou
tpu
t

1

(c)

Fig. 3: Example input-output pairs of the internal test set for the trained ensembles. Input examples are
shown on the left. Top row: Coronal slices of an input CT scan. Bottom row: Lung segmentation used
for centering and cropping are displayed with colored overlays. Delineations of the lesion masks that
were used as a separate input channel are depicted as black lines. Output examples of the ensembles
(wide, light bars) as well as the individual models these ensembles are composed of (narrow, dark
bars) are shown on the right. (a) Radiology report: ”GGO and consolidations especially lower
lobes and posterior. Has had prior lung carcinoma. COVID-19 is probable, but other infection
intrapulmonal is also possible.” (b) Radiology report: ”COVID-19 not probable, but also not ruled
out. Known post-traumatic thorax, persistent pleura fluid, slice pneumothorax. Small amount of
GGO and consolidation (left). Some pneumonia at thorax trauma, post-traumatic deviations.” (c)
Consolidation and GGO in all lobes. According to radiologist: ”Very suggestive for COVID. Also
positive PCR. Proven comorbidity.”

8

5.3

External validation

Fig. 6 shows the ROC curves of the full 3D and the full 2D model for the external iCTCF test set. The 3D
approach outperformed the 2D approach in terms of AUC (p = .001). An ablation experiment confirmed
that the full 3D model performed better than the models without pre-training, without lesion map input,
and with categorical output, which achieved AUCs of 0.923 (95% CI: 0.901 - 0.942, p = .440), 0.920 (95%
CI: 0.898 - 0.940, p = .343), and 0.910 (95% CI: 0.889 - 0.930, p = .0140), respectively.

6

Discussion

In this paper, we identified and tested components of CNN based automated COVID-19 grading models.
More specifically, we investigated how the performance of such a model is affected by using 3D CNNs instead
of 2D CNNs, using transfer learning, using automatically computed lesion maps as additional network input,
and predicting a continuous output instead of a categorical output. We evaluated all models with the same
datasets to allow for a fair comparison between models.
The full 3D model (with transfer learning, lesion maps and continuous output) outperformed the full 2D
model in terms of AUC and QWK score on the internal test set for COVID-19 classification and CO-RADS
grading.
For the 2D architecture we used COVNet, an architecture previously used in a similar COVID-19 classification task in CT [20], for which the authors reported an AUC of 0.96 for differentiating between COVID-19
positive and negative patients. The substantial difference between this result and our observations with
COVNet, both when using only their architecture (AUC = 0.858) and when using both their architecture
and their full training pipeline (AUC = 0.828), illustrates the importance of using the same dataset when
comparing different approaches.
The 3D model with categorical output obtained an AUC of 0.928 (95% CI: 0.876 - 0.974) on the internal
test set. This model is similar to the CO-RADS scoring component of CORADS-AI [19], which obtained an
AUC of 0.95 on the same test set. This AUC is slightly higher, but lies within the 95% CI stated above.
However, Lessmann et al. [19] computed AUC using only the CO-RADS 5 probability, while in our work

25

1 11

9

0

0

0

2 1

7

0

2

0

3 3

6

6

3

1

4 0

2

3

6

6

5 0
1

0

4

10

25

2 3 4
Neural network
(CO-RADS)

5

2D

20
15
10

Radiologist
(CO-RADS)

Radiologist
(CO-RADS)

3D (ours)

5
0

1 10

6

3

1

0

2 1

6

1

2

0

3 3

9

4

3

0

4 0

4

3

8

2

5 0
1

1

8

17

13

2 3 4
Neural network
(CO-RADS)

5

15.0
12.5
10.0
7.5
5.0
2.5
0.0

Fig. 4: Confusion matrices for CO-RADS scoring of the 2D and 3D model predictions on the internal test
set. These models were trained with transfer learning, lesion maps and produced continuous output.
The true label reference is from the radiology report. Cells contain the number of CT scans.
9

1.0

sensitivity

0.8
0.6
0.4
0.2
3D CNN: AUC=0.934 (95% CI: 0.876 - 0.981)
2D CNN: AUC=0.858 (95% CI: 0.777 - 0.928)

0.0
0.0

0.2

0.4

0.6

1 - specificity

0.8

1.0

Fig. 5: ROC analysis for the internal test set from Radboudumc (105 CT scans). The analysis was performed
with CO-RADS 1 and 2 as the negative class (30 scans) and CO-RADS 3-5 as the positive class (75
scans). It was performed for the full 2D and 3D models trained with transfer learning, lesion maps
and continuous output.
we used CO-RADS 1 and 2 as the negative class and CO-RADS 3-5 as the positive class. Furthermore,
Lessmann et al. did not use ensembling and used a slightly different pre-processing method. Therefore, this
may not be a fair comparison.
We also observed a better diagnostic performance for COVID-19 classification by the 3D model on the
external test set, where the AUC was 0.923 for the full 3D model, while it was 0.901 for the full 2D model.
Ning et al. [22] developed a 2D model with slice-level annotations indicating if the slice was COVID-19
positive, negative or non-informative. Using a superset of the external set used in this paper for evaluation
an AUC of 0.919 was obtained, which is lower than the AUC of our proposed 3D model. This further
underlines the importance of using 3D rather than 2D models.
A possible explanation for why adding the extra dimension to the convolutions improves the performance
is that it allows the CNN to take into account the 3D structure and full volume of individual lesions. This
explanation is in line with the fact that radiologists typically use both the axial and coronal views to visualize
the spread of COVID-19 related lesions across the lungs in CT scans, such as GGOs [24].

10

1.0

sensitivity

0.8
0.6
0.4
0.2
3D CNN: AUC=0.923 (95% CI: 0.904 - 0.942)
2D CNN: AUC=0.901 (95% CI: 0.878 - 0.922)

0.0
0.0

0.2

0.4

0.6

1 - specificity

0.8

1.0

Fig. 6: ROC analysis for the external iCTCF test set (742 CT scans). The analysis was performed with 207
COVID-19 negative (Control) cases and 535 positive (Mild, Regular, Severe, Critically ill) cases.
We could not directly compare the CO-RADS classification performance on the external set, since CORADS labels were not available. Moreover, the CO-RADS grading cannot be directly translated to the
system used in the iCTCF dataset, since the former measures the probability of COVID-19 presence, while
the latter quantifies the severity of the disease.
The internal test set was comprised of data from the same population as the data the model was trained
on, while the external test set was comprised of data from a different population. For the full 2D model,
a lower AUC was obtained on the internal test set than on the external test set. This difference might be
due to population differences between the internal and external test set, or due to the different definitions of
the positive class, which were presence of COVID-19 and high suspicion of COVID-19 for the internal and
external test sets respectively.
On the external test set, the full 3D model outperformed the full 2D model by a smaller margin in terms
of AUC than on the internal dataset. This difference could be partly due to the different definitions of the
positive class. However, we also found that it partly arises from the larger overall slice thickness in the
external test set. All scans in the internal test set had a slice thickness of 0.5mm. In contrast, 207 scans
(40 COVID-19 positive, 167 negative scans) in the external test set had a slice thickness larger than 1.5mm,

11

which was the input resolution in our training and testing pipeline. When evaluating only on these scans,
we obtained an AUC of 0.886 (95% CI: 0.836-0.929) for the full 3D model and an AUC of 0.873 (95% CI:
0.819-0.922) for the full 2D model. The external test set contained 535 scans (167 COVID-19 positive, 368
negative) with a slice thickness smaller than or equal to 1.5mm. On these scans we obtained an AUC of
0.931 (95% CI: 0.907-0.951) for the full 3D model and an AUC of 0.905 (95% CI: 0.879-0.929) for the full
2D model. The performance of both models is lower for scans with a large slice thickness, but this effect is
more apparent for the 3D model.
The ablation study on the internal test set showed that our proposed additions to the network and training
procedure have a positive effect on the performance. On the external test set, the ablation study showed
only small improvements in AUC due to these additions. These performance increases were smaller than
the performance increases obtained for these components on the internal test set. As with the difference
in performance of the 2D model on the internal and external test set, these differences could be due to
population differences between the internal and external test set and the different definitions of the positive
class in these datasets.
Although the ablation studies for the internal and external test sets both showed an increase in performance for each component, not all performance increases were statistically significant. For a significance level
of 0.05, the increase in QWK was insignificant for both adding the lesion segmentation input and replacing
the categorical output with the continuous output. The increase in AUC was insignificant for replacing the
categorical output with the continuous output on the internal test set, for adding pre-training on the external
test set, and for adding lesion maps as input for both test sets. Except for using lesion maps as input, all
components thus resulted in a significant performance increase for at least one of the test sets. Regardless of
performance increases, using a continuous output removes the disadvantage of having to decide on the onset
of the positive class for the predicted CO-RADS scores. Adding lesion maps as input might be ineffective
for automated CNN based COVID-19 grading methods.
Our results show that TL from natural 2D images and video data is beneficial for COVID-19 classification.
However, other pre-training schemes can be explored in future work. For example, employing TL by pretraining with another dataset with large amounts of thorax CT scans as was done in [34] might lead to a
further increase in performance.
We did not use clinical features available for the external dataset as input to the models trained in this
work, since the main goal of this paper was to demonstrate the effect on performance of different COVID19
grading and classification algorithm components. In future work, however, clinical data could be combined
with our proposed 3D approach.

7

Conclusion

We compared 2D and 3D Convolutional Neural Network (CNN) architectures for COVID-19 classification
from computed tomography scans and found that the 3D architecture outperforms the 2D architecture on
an internal and an external dataset. We investigated how the performance of our model was affected by
including COVID-19 related lesion segmentations as additional input, using transfer learning, and replacing
the categorical output of our model with a scalar continuous output.
The models and the automatic evaluation method we used in this paper have been made available on
the online Grand Challenge platform [3]. This allows researchers to obtain and compare the performance of
their COVID-19 grading and classification solutions to other solutions on the platform.
Our findings advance the performance of automated COVID-19 grading systems and provide insight
in the performance benefits of several of their components. These insights primarily indicate that future
research and clinical applications should move towards using 3D CNNs for COVID-19 grading in CT scans.

References
[1] CT images and clinical features for COVID-19. http://ictcf.biocuckoo.cn/HUST-19.php Accessed: 202005-20.

12

[2] GitHub - bkong999/COVNet: Artificial intelligence distinguishes COVID-19 from community acquired
pneumonia on chest CT. https://github.com/bkong999/COVNet Accessed: 2020-08-24.
[3] Grand Challenge - COVID-19 CT Classification challenge. https://covid19.grand-challenge.org/ Accessed: 2020-09-01.
[4] D. Ardila, A. P. Kiraly, S. Bharadwaj, B. Choi, J. J. Reicher, L. Peng, D. Tse, M. Etemadi, W. Ye,
G. Corrado, D. P. Naidich, and S. Shetty. End-to-end lung cancer screening with three-dimensional
deep learning on low-dose chest computed tomography. Nature medicine, 25:954–961, 2019.
[5] M. Barstugan, U. Ozkaya, and S. Ozturk. Coronavirus (COVID-19) classification using CT images by
machine learning methods. arXiv:2003.09424, 2020.
[6] C. Butt, J. Gill, D. Chun, and B. A. Babu. Deep learning system to screen coronavirus disease 2019
pneumonia. Applied Intelligence, page 1, 2020.
[7] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the Kinetics Dataset.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299–6308,
2017.
[8] J. Chen, L. Wu, J. Zhang, L. Zhang, D. Gong, Y. Zhao, S. Hu, Y. Wang, X. Hu, B. Zheng, et al. Deep
learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed
tomography: a prospective study. medRxiv, 2020.
[9] T. Chen, B. Xu, C. Zhang, and C. Guestrin.
arXiv:1604.06174, 2016.

Training deep nets with sublinear memory cost.

[10] C. de Vente, P. Vos, M. Hosseinzadeh, J. Pluim, and M. Veta. Deep learning regression for prostate
cancer detection and grading in bi-parametric MRI. IEEE Transactions on Biomedical Engineering,
2020.
[11] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Computer Vision and Pattern Recognition, pages 248–255, 2009.
[12] I. W. Harsono, S. Liawatimena, and T. W. Cenggoro. Lung nodule detection and classification from
thorax CT-scan using RetinaNet with transfer learning. Journal of King Saud University-Computer
and Information Sciences, 2020.
[13] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[14] S. Ji, W. Xu, M. Yang, and K. Yu. 3d convolutional neural networks for human action recognition.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:221–231, 2013.
[15] S. Jin, B. Wang, H. Xu, C. Luo, L. Wei, W. Zhao, X. Hou, W. Ma, Z. Xu, Z. Zheng, et al. Ai-assisted ct
imaging analysis for COVID-19 screening: Building and deploying a medical AI system in four weeks.
medRxiv, 2020.
[16] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green,
T. Back, P. Natsev, et al. The Kinetics human action video dataset. arXiv:1705.06950, 2017.
[17] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems 25, pages 1097–1105, 2012.
[18] Y. LeCun, B. E. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. E. Hubbard, and L. D. Jackel.
Handwritten digit recognition with a back-propagation network. In Advances in neural information
processing systems, pages 396–404, 1990.
13

[19] N. Lessmann, C. I. Sanchez, L. Beenen, L. H. Boulogne, M. Brink, E. Calli, J.-P. Charbonnier, T. Dofferhoff, W. M. van Everdingen, P. K. Gerke, B. Geurts, H. A. Gietema, M. Groeneveld, L. van Harten,
N. Hendrix, W. Hendrix, H. J. Huisman, I. Isgum, C. Jacobs, R. Kluge, M. Kok, J. Krdzalic, B. LassenSchmidt, K. van Leeuwen, J. Meakin, M. Overkamp, T. van Rees Vellinga, E. M. van Rikxoort, R. Samperna, C. Schaefer-Prokop, S. Schalekamp, E. T. Scholten, C. Sital, L. Stöger, J. Teuwen, K. Vaidhya Venkadesh, C. de Vente, M. Vermaat, W. Xie, B. de Wilde, M. Prokop, and B. van Ginneken.
Automated assessment of co-rads and chest ct severity scores in patients with suspected covid-19 using
artificial intelligence. Radiology, 2020.
[20] L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, J. Bai, Y. Lu, Z. Fang, Q. Song, et al. Artificial
intelligence distinguishes COVID-19 from community acquired pneumonia on chest CT. Radiology,
2020.
[21] Y. Li, W. Dong, J. Chen, S. Cao, H. Zhou, Y. Zhu, J. Wu, L. Lan, W. Sun, T. Qian, K. Ma, H. Xu,
and Y. Zheng. Efficient and effective training of COVID-19 classification networks with self-supervised
dual-track learning to rank. IEEE Journal of Biomedical and Health Informatics, 2020.
[22] W. Ning, S. Lei, J. Yang, Y. Cao, P. Jiang, Q. Yang, J. Zhang, X. Wang, F. Chen, Z. Geng, et al.
iCTCF: an integrative resource of chest computed tomography images and clinical features of patients
with COVID-19 pneumonia. Research Square, 2020.
[23] X. Ouyang, J. Huo, L. Xia, F. Shan, J. Liu, Z. Mo, F. Yan, Z. Ding, Q. Yang, B. Song, et al. Dualsampling attention network for diagnosis of COVID-19 from community acquired pneumonia. IEEE
Transactions on Medical Imaging, 2020.
[24] M. Prokop, W. van Everdingen, T. van Rees Vellinga, J. Quarles van Ufford, L. Stoger, L. Beenen,
B. Geurts, H. Gietema, J. Krdzalic, C. Schaefer-Prokop, B. van Ginneken, M. Brink, and COVID-19
Standardized Reporting Working Group of the Dutch Radiological Society. CO-RADS - a categorical
CT assessment scheme for patients with suspected COVID-19: definition and evaluation. Radiology,
page 201473, 4 2020.
[25] M. Raghu, C. Zhang, J. Kleinberg, and S. Bengio. Transfusion: Understanding transfer learning for
medical imaging. In Advances in Neural Information Processing Systems, pages 3342–3352, 2019.
[26] C. M. Rutter. Bootstrap estimation of diagnostic accuracy with patient-clustered data. Academic
Radiology, 7:413–419, 2000.
[27] F. Samuelson, N. Petrick, and S. Paquerault. Advantages and examples of resampling for CAD evaluation. In IEEE International Symposium on Biomedical Imaging, pages 492 –495, 2007.
[28] D. Singh, V. Kumar, and M. Kaur. Classification of COVID-19 patients from chest CT images using
multi-objective differential evolution-based convolutional neural networks. European Journal of Clinical
Microbiology & Infectious Diseases, pages 1–11, 2020.
[29] Y. Song, S. Zheng, L. Li, X. Zhang, X. Zhang, Z. Huang, J. Chen, H. Zhao, Y. Jie, R. Wang, et al.
Deep learning enables accurate diagnosis of novel coronavirus (COVID-19) with CT images. medRxiv,
2020.
[30] L. Sun, Z. Mo, F. Yan, L. Xia, F. Shan, Z. Ding, B. Song, W. Gao, W. Shao, F. Shi, H. Yuan, H. Jiang,
D. Wu, Y. Wei, Y. Gao, H. Sui, D. Zhang, and D. Shen. Adaptive feature selection guided deep forest
for COVID-19 classification with chest CT. IEEE Journal of Biomedical and Health Informatics, 2020.
[31] B. van Ginneken, S. G. Armato, B. de Hoop, S. van de Vorst, T. Duindam, M. Niemeijer, K. Murphy,
A. M. R. Schilham, A. Retico, M. E. Fantacci, N. Camarlinghi, F. Bagagli, I. Gori, T. Hara, H. Fujita,
G. Gargano, R. Belloti, F. D. Carlo, R. Megna, S. Tangaro, L. Bolanos, P. Cerello, S. C. Cheran,
E. L. Torres, and M. Prokop. Comparing and combining algorithms for computer-aided detection of
14

pulmonary nodules in computed tomography scans: the ANODE09 study. Medical Image Analysis,
14:707–722, 12 2010.
[32] J. Wang, Y. Bao, Y. Wen, H. Lu, H. Luo, Y. Xiang, X. Li, C. Liu, and D. Qian. Prior-attention residual
learning for more discriminative COVID-19 screening in CT images. IEEE Transactions on Medical
Imaging, 2020.
[33] S. Wang, B. Kang, J. Ma, X. Zeng, M. Xiao, J. Guo, M. Cai, J. Yang, Y. Li, X. Meng, et al. A deep
learning algorithm using CT images to screen for corona virus disease (COVID-19). medRxiv, 2020.
[34] S. Wang, Y. Zha, W. Li, Q. Wu, X. Li, M. Niu, M. Wang, X. Qiu, H. Li, H. Yu, et al. A fully automatic
deep learning system for COVID-19 diagnostic and prognostic analysis. European Respiratory Journal,
2020.
[35] W. Xie, C. Jacobs, J.-P. Charbonnier, and B. van Ginneken. Relational modeling for robust and efficient
pulmonary lobe segmentation in CT scans. IEEE Transactions on Medical Imaging, 39:2664–2675, 2020.
[36] W. Yang, A. Sirajuddin, X. Zhang, G. Liu, Z. Teng, S. Zhao, and M. Lu. The role of imaging in 2019
novel coronavirus pneumonia (COVID-19). European Radiology, pages 1–9, 2020.
[37] C. Zheng, X. Deng, Q. Fu, Q. Zhou, J. Feng, H. Ma, W. Liu, and X. Wang. Deep learning-based
detection for COVID-19 from chest CT using weak label. medRxiv, 2020.

15

