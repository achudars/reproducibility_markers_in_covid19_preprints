R ETINA FACE M ASK : A FACE M ASK D ETECTOR

arXiv:2005.03950v2 [cs.CV] 8 Jun 2020

Mingjie Jiang∗
Department of Electrical Engineering
City University of Hong Kong
Hong Kong
minjiang5-c@my.cityu.edu.hk

Xinqi Fan*
Department of Electrical Engineering
City University of Hong Kong
Hong Kong
xinqifan2-c@my.cityu.edu.hk

Hong Yan
Department of Electrical Engineering
City University of Hong Kong
Hong Kong
h.yan@cityu.edu.hk

June 9, 2020

Abstract
Coronavirus disease 2019 has affected the world seriously. One major protection method for people is to wear
masks in public areas. Furthermore, many public service providers require customers to use the service only
if they wear masks correctly. However, there are only a few research studies about face mask detection based
on image analysis. In this paper, we propose RetinaFaceMask, which is a high-accuracy and efficient face
mask detector. The proposed RetinaFaceMask is a one-stage detector, which consists of a feature pyramid
network to fuse high-level semantic information with multiple feature maps, and a novel context attention
module to focus on detecting face masks. In addition, we also propose a novel cross-class object removal
algorithm to reject predictions with low confidences and the high intersection of union. Experiment results
show that RetinaFaceMask achieves state-of-the-art results on a public face mask dataset with 2.3% and 1.5%
higher than the baseline result in the face and mask detection precision, respectively, and 11.0% and 5.9%
higher than baseline for recall. Besides, we also explore the possibility of implementing RetinaFaceMask with a
light-weighted neural network MobileNet for embedded or mobile devices.

Keywords Coronavirus, Context Attention, Face Mask Detection, Feature Pyramid Network

1

Introduction

The situation report 96 of world health organization (WHO) [1] presented that coronavirus disease 2019 (COVID-19)
has globally infected over 2.7 million people and caused over 180,000 deaths. In addition, there are several similar large
scale serious respiratory diseases, such as severe acute respiratory syndrome (SARS) and the Middle East respiratory
syndrome (MERS), which occurred in the past few years [2, 3]. Liu et al. [4] reported that the reproductive number of
COVID-19 is higher compared to the SARS. Therefore, more and more people are concerned about their health, and
public health is considered as the top priority for governments [5]. Fortunately, Leung et al. [6] showed that the surgical
face masks could cut the spread of coronavirus. At the moment, WHO recommends that people should wear face masks
if they have respiratory symptoms, or they are taking care of the people with symptoms [7]. Furthermore, many public
service providers require customers to use the service only if they wear masks [5]. Therefore, face mask detection has
become a crucial computer vision task to help the global society, but research related to face mask detection is limited.
Face mask detection refers to detect whether a person wearing a mask or not and what is the location of the face [9].
The problem is closely related to general object detection to detect the classes of objects and face detection is to detect
a particular class of objects, i.e. face [10, 11]. Applications of object and face detection can be found in many areas,
∗

These two authors contributed equally to this paper

A PREPRINT - J UNE 9, 2020

Figure 1: Examples of Images in the Face Mask Dataset [8]

such as autonomous driving [12], education [13], surveillance and so on [10]. Traditional object detectors are usually
based on handcrafted feature extractors. Viola Jones detector uses Haar feature with integral image method [14], while
other works adopt different feature extractors, such as histogram of oriented gradients (HOG), scale-invariant feature
transform (SIFT) and so on [15]. Recently, deep learning based object detectors demonstrated excellent performance and
dominate the development of modern object detectors. Without using prior knowledge for forming feature extractors,
deep learning allows neural networks to learn features with an end-to-end manner [16]. There are one-stage and
two-stage deep learning based object detectors. One-stage detectors use a single neural network to detect objects, such
as single shot detector (SSD) [17] and you only look once (YOLO) [18]. In contrast, two-stage detectors utilize two
networks to perform a coarse-to-fine detection, such as region-based convolutional neural network (R-CNN) [19] and
faster R-CNN [20]. Similarly, face detection adopts similar architecture as general object detector, but adds more face
related features, such as facial landmarks in RetinaFace [21], to improve face detection accuracy. However, there is rare
research focusing on face mask detection.
In this paper, we proposed a novel face mask detector, RetinaFaceMask, which is able to detect face masks and
contribute to public healthcare. To the best of our knowledge, RetinaFaceMask is one of the first dedicated face mask
detectors. In terms of the network architecture, RetinaFaceMask uses multiple feature maps and then utilizes feature
pyramid network (FPN) to fuse the high-level semantic information. To achieve better detection, we propose a context
attention detection head and a cross-class object removal algorithm to enhance the detection ability. Furthermore,
since the face mask dataset is a relatively small dataset where features may be hard to extract, we use transfer learning
to transfer the learned kernels from networks trained for a similar face detection task on an extensive dataset. The
proposed method is tested on a face mask dataset [8], whose examples can be found in Fig. 1. The dataset covers a
various masked or unmasked faces images, including faces with masks, faces without masks, faces with and without
masks in one image and confusing images without masks. Experiment results shows that RetinaFaceMask achieves
state-of-the-art results, which is 2.3% and 1.5% higher than the baseline result in face and mask detection precision
respectively, and 11.0% and 5.9% higher than baseline for recall.
The rest of this paper is organized as follows. In Section II, we review related works on object detection and neural
networks. The proposed methodology is presented in Section III. Section IV describes datasets, experiment settings,
evaluation metrics, results, and ablation study. Finally, Section V concludes the paper and discusses the future work.

2
2.1

Related Work
Object Detection

Traditional object detection uses a multi-step process [22]. A well-known detector is the Viola-Joins detector, which
is able to achieve real-time detection [14]. The algorithm extracts feature by Haar feature descriptor with an integral
image method, selects useful features, and detects objects through a cascaded detector. Although it utilizes integral
image to facilitate the algorithm, it is still very computationally expensive. In [23] for human detection, an effective
feature extractor called HOG is proposed, which computes the directions and magnitudes of oriented gradients over
image cells. Later on, deformable part-based model (DPM) detects objects parts and then connects them to judge
classes that objects belong to [15].
Rather than using handcrafted features, deep learning based detector demonstrated excellent performance recently,
due to its robustness and high feature extraction capability [22]. There are two popular categories, one-stage object
detectors and two-stage object detectors. Two-stage detector generates region proposals in the first stage and then
2

A PREPRINT - J UNE 9, 2020

Figure 2: Architecture of RetinaFaceMask

fine-tune these proposals in the second stage. The two-stage detector can provide high detection performance but with
low speed. The seminal work R-CNN is proposed by R. Girshick et al. [19]. R-CNN uses selective search to propose
some candidate regions which may contain objects. After that, the proposals are fed into a CNN model to extract
features, and a support vector machine (SVM) is used to recognize classes of objects. However, the second-stage of
R-CNN is computationally expensive, since the network has to detect proposals on a one-by-one manner and uses
a separate SVM for final classification. Fast R-CNN solves this problem by introducing a region of interest (ROI)
pooling layer to input all proposal regions at once [24]. Finally, a region proposal network (RPN) is proposed in faster
R-CNN to take the place of selective search, which limits the speed of such detectors [20]. Faster R-CNN integrates
each individual detection components, such as region proposal, feature extractor, detector into an end-to-end neural
network architecture. One-stage detector utilizes only a single neural network to detect objects. In order to achieve this,
some anchor boxes which specifies the ratio of width and heights of objects should be predefined [22]. Rather than the
two-stage detector, one-stage detectors scarify the performance slightly to improve the detection speed significantly. In
order to achieve the goal, YOLO divided the image into several cells and then tried to match the anchor boxes to objects
for each cell, but this approach is not good for small objects [18]. The researchers found that one-stage detector does
not perform well by using the last feature output only, because the last feature map has fixed receptive fields, which can
only observe certain areas on original images. Therefore, multi-scale detection has been introduced in SSD, which
conducts detection on several feature maps to allow to detect faces in different sizes [17]. Later on, in order to improve
detection accuracy, Lin et. al [25] proposes Retina Network (RetinaNet) by combining an SSD and FPN architecture,
which also include a novel focal loss function to mitigate class imbalance problem.

2.2

Convolutional Neural Network

CNN plays an important role in computer vision related pattern recognition tasks, because of its superior spatial feature
extraction capability and fewer computation cost [26]. CNN uses convolution kernels to convolve with the original
images or feature maps to extract higher-level features. However, how to design better convolutional neural network
architectures still remains as an opening question. Inception network proposed in [27] allows the network to learn the
best combination of kernels. In order to train much deeper neural networks, K. He et al. propose the Residual Network
(ResNet) [28], which can learn an identity mapping from the previous layer. As object detectors are usually deployed on
mobile or embedded devices, where the computational resources are very limited, Mobile Network (MobileNet) [29] is
proposed. It uses depth-wise convolution to extract features and channel wised convolutions to adjust channel numbers,
so the computational cost of MobileNet is much lower than networks using standard convolutions.
3

A PREPRINT - J UNE 9, 2020

2.3

Attention Mechanism

Attention mechanism is used to mimic human attention, which can focus on important information. Attention is first
used in recurrent neural network (RNN) with a decoder-encoder mechanism is introduced in [30]. In the convolutional
block attention module (CBAM), a simpler but effective convolutional attention mechanism is proposed, which contains
spatial and channel attention [31]. Spatial attention uses the max and average pooling to learn a spatial attention map.
Channel attention aims to learn a set of channel attention maps by training the max and average pooling output into a
multi-layer perceptron. Then, the spatial attention map and channel attention map can multiply with the original feature
maps with the element-wise operation to produce attention feature maps.

3
3.1

Methodology
Network Architecture

The architecture of proposed RetinaFaceMask is shown in Fig. 2. In order to design an effective network for face
mask detection, we adopt the object detector framework proposed in [32], which suggests a detection network with
a backbone, a neck and heads. The backbone refers to a general feature extractor made up of convolutional neural
networks to extract information in images to feature maps. In RetinaFaceMask, we adopt ResNet as a standard backbone,
but also include MobileNet as a backbone for comparison and for reducing computation and model size in deployment
scenarios with limited computing resources. In terms of the neck, it is an intermediate component between a backbone
and heads, and it can enhance or refine original feature maps. In RetinaFaceMask, FPN is applied as a neck, which can
extract high-level semantic information and then fuse this information into previous layers’ feature maps by adding
operation with a coefficient. Finally, heads stand for classifiers, predictors, estimators, etc., which are able to achieve
the final objectives of the network. In RetinaFaceMask, we adopt a similar multi-scale detection strategy as SSD to
make a prediction with multiple FPN feature maps, because it can have different receptive fields to detect various sizes
of objects. Particularly, RetinaFaceMask utilizes three feature maps, and each of them is fed into a detection head.
Inside each detection head, we further add a context attention module to adjust the size of receptive fields and focus on
specific areas, which is similar to Single Stage Headless (SSH) [33] but with an attention mechanism. The output of the
detection head is through a fully convolutional network rather than a fully connected network to further reduce the
number of parameters in the network. We name the detector as RetinaFaceMask, since it follows the architecture of
RetinaNet, which consists of a SSD and a FPN, and able to detect small face masks as well.
3.2

Transfer Learning

Due to the limited size of the face mask dataset, it is difficult for learning algorithms to learn better features. As deep
learning based methods often require larger dataset, transfer learning is proposed to transfer learned knowledge from a
source task to a related target task. According to [34], transfer learning has helped with the learning in a significant way
as long as it has a close relationship. In our work, we use parts of the network pretrained on a large scale face detection
dataset - Wider Face, which consists of 32,203 images and 393,703 annotated faces [35]. In RetinaFaceMask, only the
parameters of the backbone and neck are transferred from Wider Face, the heads are initialized by Kaiming’s method.
In addition, pretrained Imagenet [26] weights are considered as a standard initialization of our backbones in basic cases.
3.3

Context Attention Module

To improve the detection performance for face masks, RetinaFaceMask proposes a novel context attention module
as its detection heads (Fig. 3). Similar to the context module in SSH, we utilize different sizes of kernels to form an
Inception-like block. It is able to obtain different receptive fields from the same feature map, so it would be able to
incorporate more different sizes of objects through concatenation operations. However, the original context module does
not take into faces or masks into account, so we simply cascade an attention module CBAM after the original context
module to allow RetinaFaceMask to focus on face and masks features. The context-aware part has three subbranches,
including one 3 × 3, two 3 × 3, and three 3 × 3 kernels in each branch individually. Then, the concatenated feature
maps are fed into CBAM through a channel attention to select useful channels by a multi-layer perceptron and then a
spatial attention to focus on important regions.
3.4

Loss Function

RetinaFaceMask yields two outputs for each input image, localization offset prediction, Ybloc ∈ Rp×4 and classification
prediction, Ybc ∈ Rp×c , where p and c denote the number of generated anchors and the number of classes. We also have
4

A PREPRINT - J UNE 9, 2020

Figure 3: Context Attention Detection Head
Algorithm 1 Object removal cross classes
0
0
Require: selected face: Df0 , Cf0 ; selected mask Dm
, Cm
0
for pf in predictive face detection Df do
0
for pm in predictive mask detection Dm
do
if IoU (pf , pm ) > thresh then
remove the object of lower confidence
end if
end for
end for
the default anchors P ∈ Rp×4 , the ground truth boxes, Yloc ∈ Ro×4 and the classification label Yc ∈ Ro×1 , where o
refers to the number of object.
First, we matched the default anchors P with the ground truth boxes Yloc and the classification label Yc to obtain
Pml ∈ Rp×4 and Pmc ∈ Rp×1 , where each row in Pml and Pmc denote the offsets and top classification label for each
default anchor, respectively.
+
+
∈ Rp+ ×4 and Pml
Here we defined positive localization prediction and positive matched default anchors, Ybloc
∈ Rp+ ×1 ,
where p+ denotes the number of default anchors whose top classification label is not zero. Then we computed the
+
+
+
+
smooth loss between Ybloc
and Pml
, Lloc (Ybloc
, Pml
).
−
Then we perform hard negative mining [36], and the sampling negative default and predictive anchors, Pmc
∈ Rp− ×1
−
p
×1
and Ybc ∈ R − , where p− is the number of sampling negative anchors. We computed the confidence loss by
Lconf (Yb − , P − ) + Lconf (Yb + , P + ).
c

mc

c

mc

Therefore, the entire loss function is
1
+
+
−
+
(Lconf (Ybc− , Pmc
) + Lconf (Ybc+ , Pmc
) + αLloc (Ybloc
, Pml
)),
N
where N is the number of matched default anchors.
L=

3.5

Inference

In inference, the model produced the object localization D ∈ Rp×4 and object confidence C ∈ Rp×3 , where the second
column of C , Cf ∈ Rp×1 , was the confidence of face; the third column of C, Cm ∈ Rp×1 , was the confidence of mask.
We removed the object with the confidence being lower than tc and then performed the non maximum suppression
0
0
(NMS) to produce Df0 ∈ Rf ×4 , Cf0 ∈ Rf ×1 and Dm
∈ Rm×4 , Cm
∈ Rm×1 , where f and m denote the number of
selected faces and masks, respectively. Here, we proposed a novel method, object removal cross classes (ORCC), to
remove the object which had high intersect-over-union (IoU) with other objects and lower confidence. The concrete
algorithm is given in Algorithm 1.

4
4.1

Experiment results
Dataset

Face Mask Dataset [8] contains 7959 images, and its faces are annotated with either with a mask or without a mask.
However, Face Mask Dataset a combined dataset made up of Wider Face [35] and MAsked FAces (MAFA) dataset [37].
Wider Face contains 32,203 images with 393,703 normal faces with various illumination, pose, occlusion etc. MAFA
contains 30,811 images and 34,806 masked faces, but some faces are masked by hands or other objects rather than
5

A PREPRINT - J UNE 9, 2020

Figure 4: Examples of Detection Results
Table 1: Detection Accuracy of RetinaFaceMask
Face
Precision Recall

Model
baseline [8]
RetinaFaceMask+MobileNet
RetinaFaceMask+ResNet

89.6%
83.0%
91.9%

85.3%
95.6%
96.3%

Mask
Precision Recall
91.9%
82.3%
93.4%

88.6%
89.1%
94.5%

physical masks, which brings advantages to the dataset to improves variants of Face Mask Dataset. Some samples are
shown in Fig. 1, including faces with masks, faces without masks, faces with and without masks in one image and
confusing images without masks.
4.2

Experiment Setup

In the experiments, we employed stochastic gradient descent (SGD) with learning rate α = 10−3 , momentum β = 0.9
and 250 epochs as optimization algorithm. We trained the models on a NVIDIA GeForce RTX 2080 Ti. The dataset has
been split into a train, validation and test set with 4906, 1226 and 1839 images individually. The algorithm is developed
with PyTorch [38] deep learning framework and the implementation is also based on RetinaFace [21]. Each experiment
operates 250 epochs. For ResNet backbone, the input image size is 840 × 840 with batch size 2; for MoileNet backbone,
the input image size is 640 × 640 with batch size 32.
4.3

Evaluation Metrics

We employed precision and recall as metrics, they are defined as follows.
TP
TP + FP
TP
recall =
,
TP + FN

precision =

(1)

where T P , F P and F N denoted the true positive, false positive and false negative, respectively.
4.4

Result and Analysis

The performance of RetinaFaceMask is compared with a public baseline result published by the creator of the dataset [8].
Due to the limited methods proposed for this dataset, we also used ResNet and MobileNet as different backbones for
comparison. The accuracy is given in Table 1.
Table 1 showed that RetinaFaceMask with ResNet-backbone achieved higher accuracy than with MobileNet-backbone
by around 10% in precision and recall respectively. In addition, RetinaFaceMask with ResNet-backbone achieves the
6

A PREPRINT - J UNE 9, 2020
Table 2: Ablation study of RetinaFaceMask
Backbone

Transfer
Learning
Imagenet

MobileNet
Wider Face
Imagenet
ResNet
Wider Face

Attention

Face
Precision Recall

3
7
3
7
3
7
3
7

80.5%
79.0%
83.0%
82.5%
91.0%
91.5%
91.9%
91.9%

93.0%
92.8%
95.6%
95.4%
95.8%
95.6%
96.3%
95.7%

Mask
Precision Recall
82.8%
78.9%
82.3%
82.4%
93.2%
93.3%
93.4%
92.9%

89.0%
89.1%
89.1%
89.3%
94.4%
94.4%
94.5%
94.8%

state-of-the-art result compared to the baseline model. Particularly, RetinaFaceMask is 2.3% and 1.5% higher than the
baseline result in face and mask detection precision respectively, and 11.0% and 5.9% higher than baseline for recall.
Some Experiment results are shown in Fig. 4, where the red and green boxes refer to the face and mask predictions,
respectively. Figure 4 indicates that RetinaFaceMask can recognize the confusing face without mask.
4.5

Ablation Study

We perform ablation study on transfer learning, attention mechanism and backbones. The Experiment results are
summarized in Table 2 and details are given as follows.
Transfer Learning. All experiments adopt pretained weights from the Imagenet dataset [26] for backbones. This
setup is considered as a baseline result in our comparison. The ablation study of transfer learning means that the
network uses pretrained weights from a similar task - face detection, and trained on a specific dataset Wider Face. The
results using MobileNet backbone show that transfer learning can dramatically increase the detection performance by
3 − 4% in both face and mask detection results. One possible reason is that the enhanced feature extraction ability is
enhanced by utilizing pretrained weights from a closely related task.
Attention Mechanism. We perform experiments to evaluate the performance of the proposed context attention
detection head. It can be shown that, by using attention mechanism, there is a 1.5% increase in face detection precision
and 3.9% increase in mask detection precision with MoileNet backbone. These results demonstrate that the attention
modules are able to focus on the desired face and mask features to improve the final detection performance.
Backbone. Although different network components are able to increase the detection performance, the biggest
accuracy improvement is achieved by ResNet backbone. From Table 2, it can be seen that the precision of ResNet
with Imagenet pretrain is more than 10% compared to that with MobileNet backbone in both face and mask detection
precision. However, other components do not work well on ResNet, but this may be due to that the network is achieving
the limitation of the dataset performance.

5

Conclusion

In this paper, we have proposed a novel face mask detector, namely RetinaFaceMask, which can possibly contribute to
public healthcare. The architecture of RetinaFaceMask consists of ResNet or MobileNet as the backbone, FPN as the
neck, and context attention modules as the heads. The strong backbone ResNet and light backbone MobileNet can be
used for high and low computation scenarios, respectively. In order to extract more robust features, we utilize transfer
learning to adopt weights from a similar task face detection, which is trained on a very large dataset. Furthermore, we
have proposed a novel context attention head module to focus on the face and mask features, and a novel algorithm
object removal cross class, i.e. ORCC, to remove objects with lower confidence and higher IoU. The proposed method
achieves state-of-the-art results on a public face mask dataset, where we are 2.3% and 1.5% higher than the baseline
result in the face and mask detection precision respectively, and 11.0% and 5.9% higher than baseline for recall.

Acknowledgment
This work is supported by Innovation and Technology Commission, and City University of Hong Kong (Project
9410460).

References
[1] W. H. Organization et al., “Coronavirus disease 2019 (covid-19): situation report, 96,” 2020.
7

A PREPRINT - J UNE 9, 2020

[2] P. A. Rota, M. S. Oberste, S. S. Monroe, W. A. Nix, R. Campagnoli, J. P. Icenogle, S. Penaranda, B. Bankamp,
K. Maher, M.-h. Chen et al., “Characterization of a novel coronavirus associated with severe acute respiratory
syndrome,” science, vol. 300, no. 5624, pp. 1394–1399, 2003.
[3] Z. A. Memish, A. I. Zumla, R. F. Al-Hakeem, A. A. Al-Rabeeah, and G. M. Stephens, “Family cluster of middle
east respiratory syndrome coronavirus infections,” New England Journal of Medicine, vol. 368, no. 26, pp.
2487–2494, 2013.
[4] Y. Liu, A. A. Gayle, A. Wilder-Smith, and J. Rocklöv, “The reproductive number of covid-19 is higher compared
to sars coronavirus,” Journal of travel medicine, 2020.
[5] Y. Fang, Y. Nie, and M. Penny, “Transmission dynamics of the covid-19 outbreak and effectiveness of government
interventions: A data-driven analysis,” Journal of medical virology, vol. 92, no. 6, pp. 645–659, 2020.
[6] N. H. Leung, D. K. Chu, E. Y. Shiu, K.-H. Chan, J. J. McDevitt, B. J. Hau, H.-L. Yen, Y. Li, D. KM, J. Ip et al.,
“Respiratory virus shedding in exhaled breath and efficacy of face masks.”
[7] S. Feng, C. Shen, N. Xia, W. Song, M. Fan, and B. J. Cowling, “Rational use of face masks in the covid-19
pandemic,” The Lancet Respiratory Medicine, 2020.
[8] D. Chiang., “Detect faces and determine whether people are wearing mask,” https://github.com/AIZOOTech/
FaceMaskDetection, 2020.
[9] Z. Wang, G. Wang, B. Huang, Z. Xiong, Q. Hong, H. Wu, P. Yi, K. Jiang, N. Wang, Y. Pei et al., “Masked face
recognition dataset and application,” arXiv preprint arXiv:2003.09093, 2020.
[10] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep learning: A review,” IEEE transactions on
neural networks and learning systems, vol. 30, no. 11, pp. 3212–3232, 2019.
[11] A. Kumar, A. Kaur, and M. Kumar, “Face detection techniques: a review,” Artificial Intelligence Review, vol. 52,
no. 2, pp. 927–948, 2019.
[12] D.-H. Lee, K.-L. Chen, K.-H. Liou, C.-L. Liu, and J.-L. Liu, “Deep learning and control algorithms of direct
perception for autonomous driving,” arXiv preprint arXiv:1910.12031, 2019.
[13] K. Savita, N. A. Hasbullah, S. M. Taib, A. I. Z. Abidin, and M. Muniandy, “How’s the turnout to the class? a face
detection system for universities,” in 2018 IEEE Conference on e-Learning, e-Management and e-Services (IC3e).
IEEE, 2018, pp. 179–184.
[14] P. Viola and M. Jones, “Rapid object detection using a boosted cascade of simple features,” in Proceedings of the
2001 IEEE computer society conference on computer vision and pattern recognition. CVPR 2001, vol. 1. IEEE,
2001, pp. I–I.
[15] P. Felzenszwalb, D. McAllester, and D. Ramanan, “A discriminatively trained, multiscale, deformable part model,”
in 2008 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2008, pp. 1–8.
[16] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen, “Deep learning for generic object
detection: A survey,” International journal of computer vision, vol. 128, no. 2, pp. 261–318, 2020.
[17] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, “Ssd: Single shot multibox
detector,” in European conference on computer vision. Springer, 2016, pp. 21–37.
[18] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” in
Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779–788.
[19] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and
semantic segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014,
pp. 580–587.
[20] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal
networks,” in Advances in neural information processing systems, 2015, pp. 91–99.
[21] J. Deng, J. Guo, Y. Zhou, J. Yu, I. Kotsia, and S. Zafeiriou, “Retinaface: Single-stage dense face localisation in the
wild,” arXiv preprint arXiv:1905.00641, 2019.
[22] Z. Zou, Z. Shi, Y. Guo, and J. Ye, “Object detection in 20 years: A survey,” arXiv preprint arXiv:1905.05055,
2019.
[23] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,” in 2005 IEEE computer society
conference on computer vision and pattern recognition (CVPR’05), vol. 1. IEEE, 2005, pp. 886–893.
[24] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international conference on computer vision, 2015, pp.
1440–1448.
8

A PREPRINT - J UNE 9, 2020

[25] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for dense object detection,” 2017.
[26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,”
in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248–255.
[27] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going
deeper with convolutions,” in Proceedings of the IEEE conference on computer vision and pattern recognition,
2015, pp. 1–9.
[28] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 770–778.
[29] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, “Mobilenets:
Efficient convolutional neural networks for mobile vision applications,” arXiv preprint arXiv:1704.04861, 2017.
[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention
is all you need,” in Advances in neural information processing systems, 2017, pp. 5998–6008.
[31] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, “Cbam: Convolutional block attention module,” 2018.
[32] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng, Z. Liu, J. Xu et al., “Mmdetection: Open
mmlab detection toolbox and benchmark,” arXiv preprint arXiv:1906.07155, 2019.
[33] M. Najibi, P. Samangouei, R. Chellappa, and L. S. Davis, “Ssh: Single stage headless face detector,” in Proceedings
of the IEEE International Conference on Computer Vision, 2017, pp. 4875–4884.
[34] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese, “Taskonomy: Disentangling task transfer
learning,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.
3712–3722.
[35] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “Wider face: A face detection benchmark,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 5525–5533.
[36] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based object detectors with online hard example
mining,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 761–769.
[37] S. Ge, J. Li, Q. Ye, and Z. Luo, “Detecting masked faces in the wild with lle-cnns,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2017, pp. 2682–2690.
[38] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga
et al., “Pytorch: An imperative style, high-performance deep learning library,” in Advances in Neural Information
Processing Systems, 2019, pp. 8024–8035.

9

