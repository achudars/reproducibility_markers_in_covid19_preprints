C OMPARATIVE PERFORMANCE ANALYSIS OF THE R ES N ET
BACKBONES OF M ASK RCNN TO SEGMENT THE SIGNS OF
COVID-19 IN CHEST CT SCANS

arXiv:2008.09713v1 [eess.IV] 21 Aug 2020

Muhammad Aleem
Ghulam Ishaq Khan Institute of Engineering
Sciences and Technology
Topi, KPK 23640
aleemm790@gmail.com

Rahul Raj
Ghulam Ishaq Khan Institute of Engineering
Sciences and Technology
Topi, KPK 23640
rahule.lohana97@gmail.com

Arshad Khan, PhD ∗
Independent Researcher
Machine Learning in NLP
and Computer Vision, Data Science professional
Southampton, UK
arshadkhanphd@gmail.com

August 25, 2020

A BSTRACT
COVID-19 has been detrimental in terms of the number of fatalities and rising number of critical
patients across the world. According to the UNDP (United National Development Programme)
Socio-Economic programme, aimed at the COVID-19 crisis, the pandemic is far more than a health
crisis: it is affecting societies and economies at their core 2 . There has been greater developments
recently in the chest X-ray-based imaging technique as part of the COVID-19 diagnosis especially
using Convolution Neural Networks (CNN) for recognising and classifying images. However, given
the limitation of supervised labelled imaging data, the classification and predictive risk modelling
of medical diagnosis tend to compromise. This paper aims to identify and monitor the effects of
COVID-19 on the human lungs by employing Deep Neural Networks on axial CT (Chest Computed
Tomography) scan of lungs. We have adopted Mask RCNN, with ResNet50 and ResNet101 as its
backbone, to segment the regions, affected by COVID-19 coronavirus. Using the regions of human
lungs, where symptoms have manifested, the model classifies condition of the patient as either "Mild"
or "Alarming". Moreover, the model is deployed on the Google Cloud Platform (GCP) to simulate
the online usage of the model for performance evaluation and accuracy improvement. The ResNet101
backbone model produces an F1 score of 0.85 and faster prediction scores with an average time of
9.04 seconds per inference.

1

Introduction

With its first emergence in Wuhan, city of China, in December 2019, the novel coronavirus, COVID-19, has engulfed
the entire world rapidly, infecting more than 12 million people so far. The virus was declared as a pandemic in March
2020 by the World Health Organisation (WHO), given the fast rate at which it had been infecting people across the
world. The pandemic has brought the whole world to a standstill, significantly affecting daily lives, public health,
socio-economic activities and global trade.
∗
2

Corresponding author: https://orcid.org/0000-0002-6062-0098
https://www.undp.org/content/undp/en/home/coronavirus/socio-economic-impact-of-covid-19.html

It is, therefore, critically important to detect those infected with the virus as early as possible so as to slow down and stop
the spread of this deadly epidemic. This can only be achieved by treating the affected patients right away before they
end up in the critical care of hospitals and healthcare facilities where the chances of succumbing to the virus increase
manifold. Artificial Intelligence (AI) researchers, medical experts, virologists, biologists and academic researchers
across the world have put their heads together to find a cure to the deadly virus. Clinical trials for various vaccines
and clinical remedies are already underway but alongside all those efforts, AI has forged solutions based on medical
images and CT scans. It is imperative to develop, test and deploy some supplementary diagnostic tools, based on AI
algorithms, which could tackle the spread of the virus to complement the clinical and biological tool-kits. Such tools
and applications will aid clinicians at the time of triage assessment, clinical investigations, and diagnosis determination
Recent research [1], employing radiology imaging techniques, for instance, suggests that such images contain salient
features about the COVID-19 coronavirus. Application of the advanced AI algorithms and techniques, coupled with
radiological imaging, can play pivotal role in the accurate detection of virus-infected individuals. Moreover, AI-based
technologies also have the potential of overcoming the problem of insufficient healthcare practitioners or specialist
physicians operating in remote villages and socio-economically deprived areas [2].

1.1

Are lock-downs and social distancing enough?

COVID-19, has claimed over 625,000 3 lives so far across the world with many more still under active treatment in
hospitals, care homes and doctor surgeries. With active cases well above 15,000,000 globally, COVID-19 has spread
out to 206 countries so far, prompting the respective governments all over the world to impose lock-downs, curfews
and social distancing measures. So far, the only means to fight against the deadly virus has been to initially screen
patients showing symptoms of COVID-19 and isolate them in order to prevent the exponential spread of the virus.
The major screening method, being used worldwide, is Reverse Transcription-Polymerase Chain Reaction (RT-PCR)
which detects COVID-19 from respiratory specimens. Under the current standard cycling conditions, a typical run takes
approximately 20 minutes which limits the number of tests done in 24 hours time period. In addition to that, RT-PCR
procedure suffers from the high false negative rates [3]
According to a new study [4], researchers found mild to significant lung abnormalities, on the CT images of 94 percent
of patients, diagnosed with COVID-19, at the time of discharge from the hospital. Such and related research studies,
necessitate a follow-up monitoring of patients to prevent or slow down the spread of infection. The researchers in [4]
performed a longitudinal study to analyze the serial CT findings over time in patients, infected with COVID-19. From
January 16 to February 17, 2020, 90 patients with COVID-19 pneumonia were prospectively enrolled and followed up
until they were either discharged or died until the the study was concluded. Radiologists, then, reviewed 366 CT scans
for: (a) discovering potential patterns and distribution of lung abnormalities, (b) calculating the total CT scores and (c)
the number of zones involved.
Following on, those features were analyzed for temporal change. The extent of CT abnormalities progressed rapidly
after the onset of symptoms and peaked during the illness at day 6-11. The predominant pattern of abnormalities, after
the appearance of symptoms, was Ground-Glass Opacity (GGO), where up to 62 percent CT scans of patients, showed
the presence of GGOs within 5 days of symptoms’ manifestation. Radiography assessment does produce results faster
and offers a greater accessibility given that most countries have good health care system, thus, making them a good
complement to RT-PCR testing. However, one of the major setbacks in developing countries is the lack of availability
of expert doctors who can interpret radiography images due to the subtlety of results and indicators. To capitalise on
this problem and fill up the gaps, our AI-based CT scan-based diagnostic systems can aid doctors and radiologists in
interpreting the results from CT-Scans within seconds.
This paper presents an AI-based deep learning model which outputs the location of the area, affected by the COVID-19
coronavirus and the intensity of its spread by (a) segmenting the area of lungs and (b) calculating the ratio of the area of
the bounding boxes to the area of a human lung. This research also aims at reducing the prediction time of the model
by harnessing the power of the Google Cloud Platform (GCP). The approach, adopted in this research, ensures that
the the results are computed within a matter of seconds thus saving a good deal of doctors’ and clinicians’ time while
dealing with COVID-19 patients. An introductory video of this research has been uploaded, as an unlisted video, on the
YouTube website4 .

3
4

https://www.worldometers.info/coronavirus/
https://www.youtube.com/watch?v=5HNRSdJLczs&feature=youtu.be

2

2

Literature Review

Currently, the widely practised approaches to detect COVID-19 are through the use of RT-PCR, which basically detects
the presence of viral nuclei acid. One of the problems with this method is the likelihood of getting a high number
of false negatives. However, a recent research, published by [5], reported that many patients showed the presence of
GGO and collateral expansion which can be used to treat COVID-19. Out of the 1014 cases, explored by this research,
chest CT scans diagnosis showed an accuracy of about 68% and sensitivity remained 97%. Moreover, researchers in
[6] studied 1099 cases and reported that 86% of the patients showed the presence of GGO, local patchy shadowing,
bilateral patchy shadowing, or interstitial abnormalities. Only 2.9% patients with severe disease did not show these
symptoms. To improve upon the accuracy of chest CT diagnosis of COVID-19, several AI experts have incorporated
AI into COVID-19 diagnosis due to the following reasons: (1) AI can remove human error (2) AI-based tools do not
necessarily need the availability of a doctor thus addressing the issue of insufficient healthcare experts (3) AI-based
approaches can aid in the post-treatment monitoring of patients which is valid, for instance, in the case of COVID-19.
Several other researchers have published their work highlighting the CT scan-based detection of the novel coronavirus.
[7] studied 618 CT scans of which 35% were COVID-19 patients, 36% were Influenza-A-viral-pneumonia patients and
28% were healthy patients. The symptoms of each disease were manually annotated by expert radiologists and the
results were fed into a 3D-CNN model. This network was a simple ResNet18 network that receives the chest CT scans
as input and the output was flattened into a Fully Connected Layer (FCN). Furthermore, other information, related to
the location of the symptoms, were fed into the nodes and subsequently communicated to the final softmax layer in
order to predict the three target classes. The research achieved about 86.7% of accuracy on the test samples. Another
recently published study [8] conducted a critical evaluation of methods, used for automatically detecting COVID-19,
from X-Ray images. They argue that by removing the biasness from the usual COVID-19 testing protocols, which
revolves around the mere presence of COVID-19 in X-Ray images, they can improve the learning pattern of a neural
network model.
Moreover, [9] trained a VB-Net architecture (a custom network that combined V-Net with bottleneck structure) on a
total of 249 CT scans of COVID-19 patients and used a novel Human-In-The-Loop (HITL) method to achieve higher
accuracy. As part of the process, an expert radiologist identifies the symptoms himself/herself and then feeds the
outcome into the VB-Net application to output the predicted annotations. The predicted annotations are corrected by a
human and re-fed into the network to achieve better predicted annotation. The system was validated on 300 CT scans
of COVID-19 patients and achieved Dice similarity coefficients5 of 91.6% +/- 10.0% between automatic and manual
annotations.
[11] went further ahead by not just taking CT scans as input but also included into the input vector the cough voice
samples, fever level, nausea level, fatigue level and other symptoms. The samples were subsequently passed on to the
CNN and RNN layers in order to make a prediction on the existence of COVID-19. Furthermore, the end-to-end system
was integrated on a mobile platform to make it a widely accessible tool.

3

Methodology

This research paper draws its motivation from the current research initiatives, being carried out by various research
institutions in the world relating to AI Diagnosis for COVID-19 detection and treatment with a view to integrating it on
to the IoT platform. This paper presents a diagnosis that was implemented using Mask RCNN with ResNet101 and
ResNet50 as backbone models and were trained on 10 and 100 epochs respectively to cross-compare the behaviour of
the said models. The model outputs the location of the symptom and the intensity of the symptoms by segmenting the
area of patient lungs. Furthermore, it calculates the ratio of the area of the bounding boxes and the areas of interest
of the lungs in question. The best result, achieved by the model, was an accuracy of 83% with a sensitivity of 98%
and specificity of 63%. Moreover, the system was integrated on the GCP platform to simulate how doctors can use the
system globally. The system was tested to have an average inference time of 9.04 seconds while the fastest inference
was calculated in 5.36 seconds.
3.1

Data Collection and preparation

In this research, we use a total of 669 CT scans of which 313 are of positive COVID-19 cases and 356 are patients who
have either common pneumonia, lung cancer or are healthy people grouped together as non-COVID cases. The data
5
The Dice similarity coefficient (DSC) was used as a statistical validation metric to evaluate the performance of both the
reproducibility of manual segmentation and the spatial overlap accuracy of automated probabilistic fractional segmentation of MR
images[10]

3

was split, based on the 70-15-15 method, in which 70% (n=469) CT scans are for training, 15% (n=100) for validation
and the remaining 15% (n=100) is for testing. The data was obtained from [12], their established GitHub repository6 ,
and Caristica Radiological COVID-19 database7 and other classified confidential sources. The final dataset was kept
fairly unbiased using the distribution given in Table 1.

Table 1: Dataset distribution

Train
Validation
Test

Dataset
COVID-19
213
55
45

Non-COVID
256
45
55

Moreover, Figure 1(a) shows a sample of non-COVID case and Figure 1(b) shows a sample of COVID-19 case, taken
from the final dataset.

(a) CT scan sample of a healthy patient

(b) CT scan sample of a COVID-19 affected
patient

Figure 1: CT scan samples of healthy and affected patients

3.2

Data Pre-processing

The images were first resized to (1024,1024, 3) and manually annotated using Oxford’s annotation tool8 . To cope
up with the lack of data, augmentation techniques were applied such as rotation of 15 degrees, horizontal flipping,
horizontal and vertical translations and Gaussian blur. Figure 2(a) shows a sample image and Figure 2(b) shows the
same image but in rotated form.

6

https://github.com/ieee8023/covid-chestxray-dataset
https://www.sirm.org/category/senza-categoria/covid-19/
8
http://www.robots.ox.ac.uk/~vgg/software/via/)
7

4

(a) Original CT scan

(b) Rotated CT scan

Figure 2: Original vs. rotated CT scan comparison

3.3

Neural Network

The input images are then fed into Mask RCNN. This network uses Region Proposal Network (RPN) to generate Regions
of Interest (ROIs). On the CT scans, 1000 ROIs are proposed which contain even the smallest Ground Glass Opacities.
These ROIs are then fed into an ROIPool layer that extracts the features and infers the bounding box coordinates from
these ROIs. The “backbone” of Mask RCNN is a neural network that is at the heart of both aforementioned processes.
The backbone models, used in this research, are ResNet50 and ResNet101, which are 50 layers and 101 layers deep
respectively. No pre-trained weights were used for this purpose because the CT scans are not similar to the classes on
which the networks was fine-tuned. A typical Mask RCNN network, given in [13] is shown in Figure 3.

Figure 3: Network architecture of Mask RCNN

In this research, these models were trained on 10 and 100 epochs respectively and then accuracy was evaluated using
various metrics. The area of the output bounding box is then calculated, which is divided by the area of the lungs to get
the Intensity of the symptoms. The intensity is then used to classify the symptoms as either “Mild” or “Alarming”. In
this study, an Intensity of 0.15 was set as the threshold to classify an image as “Alarming”. Moreover, to be classified as
a COVID-19 class, the model has to output at least one bounding box.

4

Results and Discussion

The graphs of training loss and validation loss for ResNet50 and ResNet101 are shown in Figure 4(a) and Figure 4(b).
It can be seen that both of these models show a decreasing trend in the training loss as the number of epochs increases.
However, for both models, validation loss decreased for a certain time before starting to increase after epoch 20 for both
the models. This shows that the models are over-fitting.
5

(a) Training results based on ResNet50

(b) Training results based on ResNet101

Figure 4: Graphs showing training and validation losses for ResNet50 and ResNet101 from left to right
This can also be seen through the accuracy figures for each model. Also, the accuracy and F1 scores show that the
ResNet101 is a superior model because it is 101 layers deep and able to better learn the less evident symptoms, present
in the lungs. ResNet101, trained on 10 epochs, is therefore a better choice for the purpose in question because it has
produced better results; and the gap between the validation loss and training loss is minimal, meaning that the model
is a good fit. The accuracy figures of both models show that the ResNet101 is better suited for this task because the
complex model is comparatively better able to learn the small complex patterns of GGO in patients’ lungs compared
to the ResNet50. In terms of accuracy figures. ResNet101 backbone gave an accuracy of 87%, which is the highest
achieved after tuning the hyper-parameters. Compared to that, ResNet50 scored an accuracy of 73%, which is way
lower than that of ResNet101.
We have also presented the Precision, Recall and F1-scores, produced by ResNet101 and ResNet50 backbones, in Table
2 and Table 3 respectively.
Table 2: Results produced by ResNet101

Non-COVID
COVID-19

Precision
0.90
0.85

ResNet101
Recall
0.80
0.93

F1-score
0.85
0.89

Table 3: Results produced by ResNet50

Non-COVID
COVID-19

Precision
0.70
0.74

ResNet50
Recall
0.67
0.78

F1-score
0.69
0.76

As evident in the tables, ResNet101 backbone was able to achieve a 93% recall for COVID-19 cases as compared to
80% recall for normal cases or cases with no COVID symptoms exhibited in the CT scans of patient lungs. Whereas,
for ResNet50, the recall was 78% for COVID-19 and 67% for normal cases. Another important variable is the F-score
which helps to measure recall and precision at the same time. Since it uses harmonic mean in place of arithmetic mean,
it penalises the extreme values more than the others. This research achieved an F1 score of 89% for COVID-19 in
ResNet101 and 76% in ResNet50. Hence, it is clear that the fine-tuned Resnet101 model is better for inferencing as
compared to the ResNet50 model. The results show the efficiency of Resnet101 in detection of lesions and ground-glass
opacities of infected COVID-19 patients outweighs that of ResNet50. We understand that this will aid the radiologists
and clinicians at the time of testing as well as triaging. That in turn, will lead to taking the pressure of the respective
healthcare systems and hospitals at the time of crisis.
6

4.1

Confidence intervals

We also created a sample distribution of predictions from our model, and consequently used the distribution to calculate
the confidence intervals of the original predictions. Furthermore, upper and lower limits were calculated from these
predictions, for both Covid and Non-Covid cases. Table 4 shows the confidence intervals estimates, using the Wilson
confidence interval9 formula, given in Equation 1:
1
2
1 + zn



z2
p̂ +
2n



z
±
2
1 + zn

r

p̂ (1 − p̂)
z2
+ 2
n
4n

(1)

where p̂ is our metric (precision or recall), n is the sample size and z is the constant 1.96 for 95% confidence interval.
Table 4: Confidence intervals, based on predictions from ResNet backbones of MASK RCNN

ResNet50
ResNet101

4.2

Precision
Covid
Non-Covid
0.72+-0.13 0.68+-0.12
0.82+-0.11 0.87+-0.08

Recall
Covid
Non-Covid
0.75+-0.13 0.65+-0.12
0.89+-0.08 0.77+-0.11

Cloud Deployment

To simulate the environment of an online system where this model will be practically used (such as in a hospital), the
ResNet101 model, trained on 10 epochs, was deployed on the GCP (Google Cloud Platform) using a Tesla K80 GPU.
The user interface was designed with simplicity and practicality in mind which takes CT scans as as input before passing
them on to the trained model. The data is then sent to the trained model for inference using a Flask server. The average
inference time for each image was recorded as 9.04 seconds and the fastest inference was clocked at 5.36 seconds.
4.2.1

Inference on GCP platform

Figure 5 shows the ROIs, proposed on a sample image of healthy lung. The ROIs have been restricted to 50 for better
visualization and interpretation.

Figure 5: ROIs generated by ResNet101
9

https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval

7

The ROIPool layer then selects the bounding boxes with the highest probability, as shown in Figure 6, on a sample
image of a positive COVID-19 case.

Figure 6: Bounding boxes with the highest probabilities
Furthermore, another algorithm is implemented that removes the smaller bounding box if the intersection between the
two boxes is 1. Afterwards, the ratio of the area of the bounding boxes and the area of the lungs are computed to (a) get
the intensity and (b) determine the condition of the patient, as shown in Figure 7.

Figure 7: Bounding boxes with the highest probabilities
4.2.2

Software as a Service (SaaS)

Using the trained model, running at the back-end, an MVP (Minimal Viable Product) was developed as a monitoring
system, to enable hospitals and clinics track the COVID-19 patients. It enabled secure and faster patient management
through the GCP services without having to invest in expensive software platforms. This approach also suits the IT
environment in hospitals and healthcare facilities where integrating a software product into the existing infrastructure is
time consuming and error-prone.
8

Figure 8: Login Page

The MVP service uses the trained models at the back-end to do inferencing in the cloud and produces results on the
purpose-built dashboard. Back-end for the MVP product was developed from scratch using Python programming
language and the Flask framework. Endpoints for the application were made according to the RESTful routing protocol.
The front-end for the MVP product was developed on React10 and Material UI11 . It allowed the clinicians and other
authorised users to easily login to the application and manage their patients in real-time. PostgreSQL was used as
the Relational Database Management System (RDBMS) to store all the data pertaining to the hospital, patients and
the clinicians. CT Scans were stored in the Google Cloud Bucket which allowed faster processing of the previously
processed data regarding each patient. JSON Web Tokens12 were used for authentication so as to make the data
processing and transmission more secure. Such JWT-based authentication was also needed to allow data authentication,
based on user type and access level, to ensure role-based access to the data in question.
The MVP was deployed using the GCP with Load Balancing enabled, which allowed for high availability requirements
when the service was being used simultaneously; it ensured automatic scaling of the desired computing resources
on as-needed basis. Since the MVP is a SaaS Platform, it allows the users to access the portal using the dedicated
domain name. Doctors or other authorized users could easily login using his or her email address and password. After a
successful login, the dashboard is displayed, where the user can explore features using the navigational menu to (1)
access existing patients, (2) add further predictions to existing patients, (3) view the previous inferences of each patient,
(4) check out the progression of the disease under the Reports tab, which shows analytical graphs in real-time.
Clinicians can also add in new patients and can also explore the FAQ’s section for support and help. When a new
patient option is selected, the user or clinician is prompted to add in the necessary details regarding the patient. A
unique identifier is automatically created for each new user which is then stored in the PostgreSQL database at the
back-end. Results, produced after the inferencing on each CT Scan of a patient, are stored inside the Google Bucket in
an encrypted format. Unique URLs are then served through the cloud to the previous and current inferences (spanning
over several days), which adds an additional layer of security so that only the clinicians or experts, having the desired
access to the patient data, can view their respective patients’ records and reports. A screen shot of the dashboard,
showing patient records, can be seen in Figure 9.

10

React is an open-source JavaScript library that is used for building user interfaces specifically for single-page applications https://reactjs.org/
11
The Material-UI Grid component is used to display the courses in a grid layout which is responsive to the screen sizehttps://material-ui.com/
12
JSON Web Token (JWT) is a toolkit for facilitating secure client-server authorization - https://jwt.io/introduction/

9

Figure 9: Patient Records of the existing patients

5

Conclusion

Alternative methods of detecting COVID-19 are essential not only in the developed countries but also in developing ones
where insufficient and under-resourced healthcare facilities significantly slows down the pace of testing, triaging and
treating COVID-19 patients. The application of AI-based tools and algorithms have become common place in detecting
COVID-19 and the contribution, we have made through this research, is part of such efforts. We have used Mask RCNN
with ResNet50 and ResNet101 backbones in order to segment the signs of COVID-19 and determine the intensity of
the disease. However, we understand that the availability of more data for this research can further consolidate the
performance and accuracy of our model. It is also part of further work to evaluate the model performance, in terms
of better bounding box predictions, using both RoIPooling and ROIAlign operations with fix-sized features mapping
and bi-linear interpolation respectively. Moreover, it is also important to highlight that the real-time feedback from
clinicians in terms of refining ROIs detection in the scan will further improve the model performance at the time of
training and inferencing. These kind of practices and iterative feedback mechanism can be implemented on the GCP
platform subject to wider adoption of the system in healthcare facilities.

References
[1] Feng Pan, Tianhe Ye, Peng Sun, Shan Gui, Bo Liang, Lingli Li, Dandan Zheng, Jiazheng Wang, Richard L.
Hesketh, Lian Yang, and Chuansheng Zheng. Time course of lung changes at chest ct during recovery from
coronavirus disease 2019 (covid-19). Radiology, 295(3):715–721, 2020. PMID: 32053470.
[2] Siddique Latif, Junaid Qadir, Shahzad Farooq, and Muhammad Ali Imran. How 5g wireless (and concomitant
technologies) will revolutionize healthcare? Future Internet, 9(4):93, 2017.
[3] Jian-Long He, Lin Luo, Zhen-Dong Luo, Jian-Xun Lyu, Ming-Yen Ng, Xin-Ping Shen, and Zhibo Wen. Diagnostic
performance between ct and initial real-time rt-pcr for clinically suspected 2019 coronavirus disease (covid-19)
patients outside wuhan, china. Respiratory Medicine, 168:105980, 2020.
[4] Yuhui Wang, Chengjun Dong, Yue Hu, Chungao Li, Qianqian Ren, Xin Zhang, Heshui Shi, and Min Zhou.
Temporal changes of ct findings in 90 patients with covid-19 pneumonia: a longitudinal study. Radiology, page
200843, 2020.
[5] Tao Ai, Zhenlu Yang, Hongyan Hou, Chenao Zhan, Chong Chen, Wenzhi Lv, Qian Tao, Ziyong Sun, and Liming
Xia. Correlation of chest ct and rt-pcr testing in coronavirus disease 2019 (covid-19) in china: a report of 1014
cases. Radiology, page 200642, 2020.
[6] Susan C Shelmerdine, Jovan Lovrenski, Pablo Caro-Domínguez, Seema Toso, et al. Coronavirus disease 2019
(covid-19) in children: A systematic review of imaging findings. 2020.
[7] Xiaowei Xu, Xiangao Jiang, Chunlian Ma, Peng Du, Xukun Li, Shuangzhi Lv, Liang Yu, Qin Ni, Yanfei Chen,
Junwei Su, et al. A deep learning system to screen novel coronavirus disease 2019 pneumonia. Engineering, 2020.
[8] Gianluca Maguolo and Loris Nanni. A critic evaluation of methods for covid-19 automatic detection from x-ray
images. arXiv preprint arXiv:2004.12823, 2020.
[9] Fei Shan, Yaozong Gao, Jun Wang, Weiya Shi, Nannan Shi, Miaofei Han, Zhong Xue, and Yuxin Shi. Lung
infection quantification of covid-19 in ct images with deep learning. arXiv preprint arXiv:2003.04655, 2020.
10

[10] Kelly H Zou, Simon K Warfield, Aditya Bharatha, Clare MC Tempany, Michael R Kaus, Steven J Haker,
William M Wells III, Ferenc A Jolesz, and Ron Kikinis. Statistical validation of image segmentation quality based
on a spatial overlap index1: scientific reports. Academic radiology, 11(2):178–189, 2004.
[11] Halgurd S Maghdid, Kayhan Zrar Ghafoor, Ali Safaa Sadiq, Kevin Curran, and Khaled Rabie. A novel ai-enabled
framework to diagnose coronavirus covid 19 using smartphone embedded sensors: Design study. arXiv preprint
arXiv:2003.07434, 2020.
[12] Joseph Paul Cohen, Paul Morrison, Lan Dao, Karsten Roth, Tim Q Duong, and Marzyeh Ghassemi. Covid-19
image data collection: Prospective predictions are the future. arXiv preprint arXiv:2006.11988, 2020.
[13] Than Le. Mask r-cnn with data augmentation for food detection and recognition. 2020.

11

