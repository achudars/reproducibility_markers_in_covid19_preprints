arXiv:2102.04205v1 [cs.IR] 5 Feb 2021

How Pandemic Spread in News: Text Analysis
Using Topic Model
Minghao Wang

Paolo Mengoni, IEEE Member

School of Communication
Hong Kong Baptist University
Hong Kong SAR, China
wangminghao@hkbu.edu.hk

Department of Journalism
Hong Kong Baptist University
Hong Kong SAR, China
pmengoni@hkbu.edu.hk

Abstract—COVID-19 pandemic has made tremendous impact
on the whole world, both the real world and the media atmosphere. Our research conducted a text analysis using LDA
topic model. We first scraped 1127 articles and 5563 comments
on SCMP covering COVID-19 from Jan 20 to May 19, then
we trained the LDA model and tuned parameters based on
the Cv coherence as the model evaluation method. With the
optimal model, dominant topics, representative documents of
each topic and the inconsistency between articles and comments
are analyzed. Some factors of the inconsistency are discussed at
last.
Index Terms—text analysis, topic modeling, COVID-19

I. I NTRODUCTION
A. Background
The World Health Organization (WHO) defined the SARSCoV-2 virus outbreak as a severe global threat. The COVID-19
pandemic (hereinafter referred to as the pandemic) has made
tremendous impact on the whole world, including medical system, livelihood, employment and political issues. These issues
have been projected to the media atmosphere significantly as
well, which can be regarded as a media infodemic [1]. It is
a precious chance to observe the topics that people are most
interested in among the media during the pandemic using text
analysis [2]–[4] and topic modeling techniques.
B. Related Works
1) Topic Model: Topic model is a key technique to discover
the latent semantic structure from text contents. This technique
can be traced back to the Latent Semantic Analysis (LSA)
[5], which used singular-value decomposition to index and
retrieve the semantic structure automatically. However, due to
some basic statistical problems, a new approach called Latent
Semantic Indexing (LSI) was proposed by Thomas Hofmann,
which used statistical latent class model [6]. Latent Dirichlet
allocation was proposed then by Blei, Ng and Jordan [7] as
an extension of PLSA, which added Dirichlet priors for the
document-specific topic mixtures [8] that has improved the
model’s generalization ability on unseen documents.
In the LDA model, a topic was defined as a distribution
over a fixed vocabulary [9]. For example, the pandemic topic
has words about pandemic (e.g. mask, vaccine) with high
probability, and the politics topic has words (e.g. election,

regime, lie) about politics with high probability. These topics
are determined before the document has been formed. The
basic idea behind LDA was that documents usually bear
multiple topics in different proportions. Each topic consists of
multiple words in different proportions as well. As a posterior
distribution probabilistic model, the LDA model can capture
documents’ hidden structures (i.e. the topics, per-document
topic distributions, per-document per-word topic assignments).
It is also a relatively simple model. Thus, we would apply LDA
model in our research.
2) gensim LDA Model and Evaluation Methods: As a
collection of natural language processing tools, gensim has
a handy implementation of the LDA model. There are mainly
4 parameters in the gensim LDA model could be adjusted,
namely the number of topics, iteration, chunk size and passes.
The number of topics is the most important parameter of
a topic model, since too few topics will make the model
imprecise (underfitting) while too many will lead to overfitting.
The iteration refers to the maximum number to iterate through
each document in the corpus to calculate the probability of
every topic. Since the model is trained on one data chunk at
a time, the chunk size refers to the number of documents in
every training chunk. The passes refers to how many times the
whole corpus would be trained on. The model would be more
precise with bigger iterations and passes, while it would take
longer time to train the model as well. So we would trade
that off by choosing the minimum of them meeting enough
precision.
Several commonly used evaluation methods were considered. As addressed by Blei, Ng, and Jordan [7], perplexity
was first applied to evaluate the LDA model performance.
Perplexity reflects how uncertain that a word belongs to a
specific topic category. The smaller the perplexity, the more
precise is the model. However, some up-to-date experiments
and research have revealed that perplexity is confused and lack
of interpretability [10]. Even Dr. Radim Řehůřek, the author
of gensim, also thought there were some problems to measure
LDA with perplexity. In this case, topic coherence measures
like NPMI or UMass have been widely deployed in many
cases, while the Cv topic coherence was proved to be the best
performing one according the research by Röder, Both and
Hinneburg, which showed results of evaluating topic models

the closest to human evaluation [10]. Cv topic coherence is
essentially an index measures the co-occurrence of the words
extracted by the topic model. If those words from the same
topic co-occur very often (i.e. the Cv coherence value is high),
then the model is well performed. As a result, our research
would apply Cv coherence as the evaluation measurement of
our topic model.
3) Text Analysis Using Topic Model: In the field of text
analysis, more and more studies are using topic model to mine
data from media contents [11]. Compared with traditional text
analysis methods, topic model is effective on discovering the
latent message from especially short texts, which may convey
rich meaning via self-defined hashtags or slangs. It is also
effective on discovering the themes that run through the texts,
how those themes are connected to each other, and how they
change over time [9].

C. Research Questions
Inspired by previous studies, we are curious about what
media content that most people are interested in during the
pandemic. Rather than purely analyze user-generated content
(UGC) on platforms like Twitter or Weibo, we are more
interested in professional news websites (e.g. SCMP, BBC
News, CNN) for mainly 2 reasons: first, long news articles
offer richer corpus and that will help to train a more precise
topic model and get more precise analysis results; second,
comparing news articles together with comments may reveal
the effect of information diffusion and agenda setting.
In this way, we should train and fine-tune an LDA model
that suits the corpus the best.
RQ1: What is the optimal LDA model like based on the
corpus of news articles and comments?
Specifically, we are curious about what contents appear
the most frequently among the articles and comments? What
people talk about when people talk about the pandemic?
RQ2: What is the distribution of different topics among
both the news articles and comments covering the pandemic?
Moreover, during surfing on these news websites, we noticed that some people are talking about B when the article is
about A. An example is shown in Fig. 1 and Fig. 2. In this case,
the article reported some recent situation about the pandemic,
including where did the coronavirus come from, what is the
riskiest way of contagion, and who are the most susceptible
among the crowd. However, among the 4 comments (since
there is one duplicate), 2 of them are accusing Wuhan for
the pandemic and laughing at Chinese people, and 1 of
them is suspecting the credibility of media when they diffuse
pandemic messages. This inconsistency between news articles
and comments can reflect the effects of information diffusion
and the interaction between information sender and receiver.
RQ3: How inconsistent are news articles and their comments? And what factors contribute to the inconsistency?

Fig. 1. An article from SCMP that reported some recent situation about the
pandemic, including the source of coronavirus, the contagion ways, and the
susceptible people.

Fig. 2. Corresponding comments to the article above. Compared with the
article, many of the comments are concerned with political issue.

II. M ETHOD
A. Data Collection
South China Morning Post (hereinafter referred to as
SCMP) was selected as the data source of this research
for mainly 3 reasons compared with other news websites
accessible: first, it has comments posted, which takes a half
of the data we need; second, the news on SCMP is inclusive,
which will give us a relatively comprehensive global view of
the pandemic information; third, SCMP is popular [12], and
considered credible1 .
We chose the Python library BeautifulSoup to scrape articles
and comments from SCMP’s website.
The whole dataset consisted of 1127 news articles and 5563
pieces of comments from Jan 20, 2020 to May 19, 2020 after
dropping null values (e.g. unparseable pages). Six attributes of
the articles were collected, namely the news id, the title, the
article text, the release time of the article, the collecting date,
and the URL it came from. Seven attributes of the comments
were collected, namely the username, the raw comment, the
cleaned comment, the date of that comment, the news id, the
status that if this comment was a reply, and the collecting date.
B. Experiments
1) Data Preprocessing: Proper data preprocessing is of
great significance to train a precise topic model. With the help
of Python library nltk, we first tokenized the documents (i.e.
both news articles and comments) by any space or punctuation.
Then, all tokens regarded as stop words would be blocked
out from the token list, which would be made into dictionary
and corpus later. Besides all default stop words from nltk,
1042 new stop words (including integer numbers from 1 to
999) were added according to preliminary topic modeling
experiments.
Stemmer was not utilized in this research due to mainly 2
reasons: first, after trying both “porter” stemmer and “snowball” stemmer in nltk, words would become hard to read after
being stemmed (e.g. “china” is stemmed as “chin”, which is
the same word means “jaw”), which would lead to difficulties
and misunderstandings in interpreting topic modeling results;
second, another stemmer, “lancaster” seemed not available at
this moment. No proper stemmer did we find so far.
After building preprocessed articles and comments into
dictionary then corpus using Python library gensim, two data
frames were formed for different purposes. The first one was
following the order of original data, consisting of news id,
document (both news articles and comments), release time,
token and type (news articles are marked as type 0 and
comments are marked as type 1). This data frame was made for
future indexing and text analysis. The other one was shuffled
in order to train topic model. Training set took 90% of the
shuffled data because the amount of our data was relatively
small and more data were needed for training. The other 10%
was remained as test set.
1 Public

evaluation on media credibility. Centre for Communication and
Public Opinion Survey, The Chinese University of Hong Kong, Hong Kong,
2019.

2) LDA Model Training, Fine-Tuning and Evaluation: As
explained in I-B2, we would adjust 4 parameters in the gensim
LDA model, namely the number of topics, iteration, chunk
size, and passes. In order to control variable, we assume these
four parameters are decoupled, which means adjusting them
separately will not affect other parameters. We will prove this
assumption later.
To save training time, we firstly tried topic number from 2 to
50, 10 a step (iterations = 10, chunksize = 100, passes =
10). As shown in Fig. 3, the Cv coherence value grows up
mainly within 12 topics. Then, we tried topic number from 2
to 17, 1 a step (iterations = 10, chunksize = 100, passes =
10). As shown in Fig. 4, the Cv coherence value grows up
mainly within 7 topics, then it keeps relatively stable (there is
a drop when topics = 8). The Cv coherence value of 7 topics
is 0.477.
Different iterations were tested (i.e. 10, 100, 500, 1000) together with 7 topics. While the Cv coherence value did not
grow up as the iterations added. As for the chunk size and
passes, no apparent grow as the parameters added. Results are
shown in Table I.
With these tuned parameters, we tested the model on the
test set and got satisfactory results (Cv = 0.410).
About the decoupling assumption, we have also tested the
same chunk size and passes with 12 topics, and then we calculated the Pearson correlation between results, and they behaved
with strong correlations (rchunk size = 0.857, rpasses =
0.735), which has proved the assumption.

Fig. 3. The Cv coherence value with number of topics ∈ {2, 12, ..., 42}.

Fig. 4. The Cv coherence value with number of topics ∈ {2, 3, ..., 17}.

3) Topic Analysis Method: With the well-tuned model, we
delivered a visualization work to give an overview of the

TABLE I
T HE Cv COHERENCE VALUE WITH DIFFERENT ITERATIONS , CHUNK SIZE
AND PASSES .
parameter
iterations

chunksize

passes

parameter value
10
100
500
1000
10
100
500
1000
5
10
50
100

c v coherence
0.439
0.426
0.403
0.366
0.381
0.460
0.422
0.425
0.426
0.426
0.432
0.425

other parameters
numtopics=7, chunksize=100, passes=10

numtopics=7, chunksize=100, passes=10

numtopics=7, iterations=100, chunksize=100

TABLE II
C OMPARISON OF D IFFERENT C ORRELATION M EASURES ON 2 PAIRS OF
S IMILAR T OPIC L ISTS .
topic
topic
topic
topic

list1
list2
list3
list4

[1,
[2,
[1,
[6,

2,
1,
6,
1,

0,
0,
0,
0,

6,
6,
2,
2,

3,
3,
3,
3,

4,
4,
4,
4,

5]
5]
5]
5]

Spearman cor=0.964, Kendall tau=0.905, cosine sim=0.989
Spearman cor=0.107, Kendall tau=0.143, cosine sim=0.725

whole dataset using Python library pyLDAvis first. Then,
we calculated the dominant topic (i.e. the topic with the
highest probability) of each document. We gathered those
dominant topics and calculated the proportion of them. We
also extracted the most representative documents of each topic
by gathering the dominant topics separately and then picked
up the document with the highest probability within that topic.
4) Topic Inconsistency Evaluation Method: As addressed in
I-C, we are going to compare the difference between articles
and comments of the same news id, so an effective method to
measure the inconsistency between them is required. Considering every document had a list of topics (usually 7 digits) after
applying that document with the optimal model, we delivered
a small experiment on 3 measures (i.e. Spearman’s rank
correlation coefficient, Kendall rank correlation coefficient,
and cosine similarity) to decide which measure to apply. We
created 2 pairs of topic lists, which is shown as Table II. Each
pair has only the first two digits different, and they should be
considered very similar in the topic distribution. However, the
results show that both Spearman’s and Kendall rank correlation
coefficient are overly sensitive to the digits, while the cosine
similarity is more stable that different digits share the similar
ranks. In this way, cosine similarity is selected as the measure
to evaluate the topic differences. The measure scheme is shown
in Fig. 5.

C. Results
The optimal model has following parameters: number of
topics = 7, iterations = 10, chunk size = 100, passes = 5.
1) Overview: Table III shows the top 7 topic words within
7 topics. Words under each topic is described manually. As
we can see from those topic words, the difference between
topics are relatively obvious. Most of the words have actual
meanings. It reflects the main agendas in SCMP covering
COVID-19 from January to May. Interestingly, although this
is an issue about health, topics about health only take less than
a half of the total.
An overview graph of the topic distribution is shown in
Fig. 6 and Fig. 7. Notice that the numbers in the circles are
different from the topic numbers. The overlapping of circles
shows the distance (or closeness) between topics. The square
of a circle refers to the total amount of that topic. We can find
that circle 2, 3, 4, corresponds with topic 1 (Commercial), 5
(Politics), 6 (China/US) respectively, are closely tied, which is
consistent with our common sense. One word usually belongs
to several topics. Some frequently applied words during the
pandemic and their topic lists (sorted by topic probability) are
shown in Table IV.

Fig. 6. An intertopic distance map illustrates the topic distribution. The
numbers in the circles are different from the topic numbers.

Fig. 5. A schematic diagram of the method to calculate the inconsistency
between news articles and comments of the same news ID.

2) Dominant Topic Analysis: As shown in Fig. 8, topic
5 (Politics) becomes the most popular one of the pandemic
report in SCMP from January to May, which takes more than
a half of the agendas. Topic1 (Commercial) takes the second
place among 7 topics with nearly 1/5 of the agendas. Topic

TABLE III
T HE T OP 7 W ORDS T OGETHER W ITH T HEIR P ROBABILITY W ITHIN 7 T OPICS . THE D ESCRIPTIONS A RE M ADE M ANUALLY.
topic number
0
1
2
3
4
5
6

0.016”patients”
0.011”china”
0.016”workers”
0.043”us”
0.012”cases”
0.029”china”
0.037”china”

0.015”covid”
0.009”$”
0.016”hong”
0.015”china”
0.012”new”
0.026”\”
0.023”us”

0.013”disease”
0.007”economy”
0.015”kong”
0.014”taiwan”
0.010”coronavirus”
0.013”world”
0.017”chinese”

top 7 topic words
0.012”wuhan”
0.010”vaccine
0.007”economic” 0.006”us”
0.012”home”
0.010”india”
0.012”war”
0.010”scmp”
0.009”000”
0.008”health”
0.009”trump”
0.008”chinese”
0.011”beijing”
0.011”pandemic”

0.010”coronavirus”
0.006”companies”
0.009”singapore”
0.009”comment”
0.007”city”
0.007”even”
0.010”world”

0.009”found”
0.005”million”
0.008”family”
0.008”military”
0.007”government”
0.006”email”
0.009”coronavirus”

description
Treatment
Commercial
International Livelihood
Conflict
Spread
Politics
China/US

6 (China/US) takes nearly 10% of the agendas which is in
the third place. Interestingly, articles and comments about
treatment and virus itself merely take 0.5%. At the same time,
very polarized conflict contents only appear 0.5% as well.

Fig. 8. The proportion of each topic as the dominant (i.e. the topic with the
highest probability of a document).

Table V shows the most representative contents of each
topic. All the contents are happened to be comments.

Fig. 7. A bar chart of top-30 most salient terms. Notice that in the interaction
map, the relevance metric ∈ [0, 1].

TABLE IV
F REQUENTLY A PPLIED W ORDS D URING THE PANDEMIC T OGETHER
W ITH T HEIR B ELONGED T OPICS .
keyword
coronavirus
vaccine
infections
symptoms
respiratory
government
lockdown
outbreak
wuhan
migrant
huawei
trade

topic list (sorted)
4
0
6
1
0
6
4
0
0
4
0
4
1
5
6
2
4
2
1
6
4
1
0
0
5
4
6
2
3
6
1
3

3) Topic Inconsistency Analysis: As shown in Fig. 9, articles and comments shared a relatively good inconsistency
in general. News ids with similarity lower than 60% only
took 34.3%. Within the news ids of low similarity (i.e. of
high inconsistency), the distribution of their dominant topics is
shown in Fig. 10. In order to examine if there was any specific
topic contribute to the inconsistency more, we calculated
Pearson correlation coefficient between this distribution and
the dominant topic distribution among all documents. High
correlation (r = 0.957) exists, which means no specific topic
leads to the inconsistency. We selected several articles and
their corresponding comments then tried to get some clues
about the reason for inconsistency. Fig. 11 shows the article
and comment with the lowest similarity (0.418). The article
was talking about the “slump” occurred due to the pandemic,
and trading volume and trend were discussed as well. While
the comment was concerned with the home office fashion.
Essentially, this concern was stemmed from the economic
“slump”: besides the lockdown, many people work from home
during this period to save office rental. In this case, the article
and the comment were potentially related.
Fig. 12 shows a relative obvious inconsistency case. This
article was purely about the characteristics of the virus and
some possible treatments. However, political accusation took
a half part of the comments. Within all 4 comments, there was
also one comment criticized media’s spreading panic emotion.
Only one comment was concerned with the article’s words.

TABLE V
T HE MOST REPRESENTATIVE CONTENTS OF EACH TOPIC . A LL THE CONTENTS ARE HAPPENED TO BE COMMENTS .
TOPIC0 - 0.378, 2020-04-25
Guess their bank account is deep enough to cover the cost of their extended trip, as they seem not to be too concerned with a little pandemic-related hiccup in their RTW itinerary.
TOPIC1 - 0.573, 2020-05-03
@Werner Ziegler Actually no.... International scientists determined ”Type A” were mostly found in the US and Australia. China is mainly Type B.
TOPIC2 – 0.523, 2020-04-29
The CCP is laughable. The rest of the world can see through its lies. and responsibility. avoidance immediately. What a joke the CCP is.
TOPIC3 – 0.379, 2020-04-20
how can you be cutting trips even primary sch children know that we are fighting dangerous contengious virus yet they cut trips.....?
TOPIC4 – 0.491, 2020-05-06
Dr. Fauci is the only credible person on that podium. Americans trust Fauci more than they trust Trump or Pompeo.
TOPIC5 – 0.701, 2020-05-18
”’They told us it wasn’t contagious’: Chinese blogger Fang Fang’s forbidden diary reveals how Wuhan authorities told people coronavirus could NOT be passed between people” Fang FangSEARCH ”Fang Fang Daily Mail”
TOPIC6 - 0.506, 2020-03-23
Trump is using China as a target to shift the public’s attention because the government, under his leadership is failing the American people big time!

Fig. 9. The distribution of topic similarity within different ranges.

Fig. 10. The proportion of inconsistent documents grouped by their dominant
topics.

III. D ISCUSSION
As we can see from the results, articles and comments
on SCMP covering COVID-19 from January to May are
mainly concerned with topics like “politics” and “commercial”
stemmed from the pandemic rather than the pandemic itself,
and “politics” has attracted the most attention. Among topics
related to the pandemic itself, people have paid much attention
on the spreading situation while little on the treatment-related
messages like “vaccine” or “ventilator”. We can describe these
two features as people tend to “complain” and “panic”. Some
people are passionate with accusing China as the “source of
evil” and laughing at China’s “telling lies”.
Topic inconsistency between articles and comments existed
to a normal extent, and no obvious topic was proved to lead the
inconsistency. Some inconsistency was due to the very deep
relationships between the article and comment, that means
people may be associative in some cases. Meanwhile, we also
found some inconsistency due to the emotional expression.
When the news was talking about facts, people were often
caring about political issues.

Fig. 11. A case of topic inconsistency (comment). Essentially, this comment
is stemmed from this article.

IV. C ONCLUSION AND F UTURE W ORKS
Inspired by previous studies, our research applied LDA
topic model to analyze the SCMP news articles and their
comments covering the COVID-19 pandemic from January
20, 2020 to May 19, 2020. Model parameters were tuned
on the training set after a series of experiments then tested
on the test set. Cv topic coherence, which was used to
measure the model performance, was good with the set of
optimal parameters. Using the optimal model, the dominant
topics from Jan to May was counted, the most representative
contents were also analyzed, meanwhile several visualization
works were conducted. Topic inconsistency phenomenon was
measured with cosine similarity and the factors lead to this
phenomenon were discussed.
There is still space for improvement in the future. First,
the SCMP provided news categorized by regions like “Greater
China”, “North America”, and “Europe”. These news articles

[3] P. Mengoni, A. Milani, and Y. Li. Multi-term semantic context elicitation
from collaborative networks. In 2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),
pages 234–238, 2018.
[4] S. W. Chan, V. Franzoni, P. Mengoni, and A. Milani. Context-based
image semantic similarity for prosthetic knowledge. In 2018 IEEE
First International Conference on Artificial Intelligence and Knowledge
Engineering (AIKE), pages 254–258, 2018.
[5] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K
Landauer, and Richard Harshman. Indexing by latent semantic analysis.
Journal of the American society for information science, 41(6):391–407,
1990.
[6] Thomas Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on
Research and development in information retrieval, pages 50–57, 1999.
[7] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet
allocation. Journal of machine Learning research, 3(Jan):993–1022,
2003.
[8] Xueqi Cheng, Xiaohui Yan, Yanyan Lan, and Jiafeng Guo. Btm: Topic
modeling over short texts. IEEE Transactions on Knowledge and Data
Engineering, 26(12):2928–2941, 2014.
[9] David M Blei. Probabilistic topic models. Communications of the ACM,
55(4):77–84, 2012.
[10] Michael Röder, Andreas Both, and Alexander Hinneburg. Exploring
the space of topic coherence measures. In Proceedings of the eighth
ACM international conference on Web search and data mining, pages
399–408, 2015.
[11] Liangjie Hong and Brian D Davison. Empirical study of topic modeling
in twitter. In Proceedings of the first workshop on social media analytics,
pages 80–88, 2010.
[12] Huang Yu Raymond Li. Hong kong digital media report 2018. Report,
School of Communication, Hong Kong Baptist University, Hong Kong,
2018.

Fig. 12. Another case of topic inconsistency. Political issues are the most
concerned with the comments, while the article itself is merely about medical
issues.

have been already categorized by the editor, and different
topic distribution among different regions could be discovered.
Second, about the data preprocessing, we are using unigram
as our language model, and precision may be sacrificed for
the speed. Bigram or trigram models will be tried in data
preprocessing. Finally, cosine similarity was applied in this
research to measure the inconsistency between articles and
comments, but it could be overly sensitive to the topic number
to some extent. In the future, we will develop a model with a
measure that will be a better fit for the task.
ACKNOWLEDGMENT
This work was partly supported by the “Teaching Development Grant” - Department of Journalism, School of Communication, Hong Kong Baptist University, Kowloon Tong, Hong
Kong, China
R EFERENCES
[1] Matteo Cinelli, Walter Quattrociocchi, Alessandro Galeazzi,
Carlo Michele Valensise, Emanuele Brugnoli, Ana Lucia Schmidt,
Paola Zola, Fabiana Zollo, and Antonio Scala. The covid-19 social
media infodemic. arXiv preprint arXiv:2003.05004, 2020.
[2] Valentina Franzoni, Yuanxi Li, Paolo Mengoni, and Alfredo Milani.
Clustering facebook for biased context extraction. In Computational
Science and Its Applications – ICCSA 2017, pages 717–729, Cham,
2017. Springer International Publishing.

