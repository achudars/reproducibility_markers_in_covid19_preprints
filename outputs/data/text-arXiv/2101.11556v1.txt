Powering COVID-19 community Q&A with Curated Side
Information
Manisha Verma, Kapil Thadani, and Shaunak Mishra
Yahoo! Research, VerizonMedia, New York
manishav,thadani,shaunakm@verizonmedia.com

arXiv:2101.11556v1 [cs.IR] 27 Jan 2021

ABSTRACT
Community question answering and discussion platforms such as
Reddit, Yahoo! answers or Quora provide users the flexibility of
asking open ended questions to a large audience, and replies to such
questions maybe useful both to the user and the community on certain topics such as health, sports or finance. Given the recent events
around COVID-19, some of these platforms have attracted 2000+
questions from users about several aspects associated with the disease. Given the impact of this disease on general public, in this
work we investigate ways to improve the ranking of user generated
answers on COVID-19. We specifically explore the utility of external technical sources of side information (such as CDC guidelines
or WHO FAQs) in improving answer ranking on such platforms.
We found that ranking user answers based on question-answer
similarity is not sufficient, and existing models cannot effectively
exploit external (side) information. In this work, we demonstrate
the effectiveness of different attention based neural models that can
directly exploit side information available in technical documents or
verified forums (e.g., research publications on COVID-19 or WHO
website). Augmented with a temperature mechanism, the attention
based neural models can selectively determine the relevance of side
information for a given user question, while ranking answers.
ACM Reference Format:
Manisha Verma, Kapil Thadani, and Shaunak Mishra. 2019. Powering COVID19 community Q&A with Curated Side Information. In Proceedings of ACM
Conference (Conference’17). ACM, New York, NY, USA, 8 pages. https://doi.
org/10.1145/nnnnnnn.nnnnnnn

1

Figure 1: Illustrative example of COVID-19 community answer ranking powered by side information in the form of research papers, and information from verified sources (such
as CDC, WHO, and NHS).

INTRODUCTION

Question answering systems are key to finding relevant and timely
information about several issues. Community question answering (cQ&A) platforms such as Reddit, Yahoo! answers or Quora
have been used to ask questions about wide ranging topics. Most of
these platforms let users ask, answer, vote or comment on questions
present on the platform. However, question answering platforms
are useful not only for getting public opinions or votes about areas
such as entertainment or sports but can also serve as information
hot-spots for more sensitive topics such as health, injuries or legal
topics. Thus, it is imperative that when the user visits sensitive
topics content, answer ranking also takes into account curated
side information from reliable (external) sources. Most prior work
on cQ&A has focused on incorporating question-answer similarity [17, 19], user reputation [3, 6, 22], integration of multi-modal
content [25], community interaction features [7] associated with
Conference’17, July 2017, Washington, DC, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

answers or just the question answering network [8] on the platform. However, there is very limited work on incorporating curated
content from external sources. Existing work only exploits knowledge bases [25] that consist of different entities and relationships
between these entities to score answers. However, there are some
limitations of knowledge bases that would make it difficult to use
them for community Q&A for rapidly evolving topics such as disease outbreaks (e.g. ebola, COVID-19), wild-fires or earthquakes.
Firstly, knowledge bases contain information about established entities, and do not rapidly evolve to incorporate new information
which makes them unreliable for novel disease outbreaks such as
COVID-19 where information rapidly changes and its verification
is time sensitive. Secondly, it may be hard to determine what even
constitutes an entity as new information arrives about the topic.
To overcome these limitation in this work, we posit that external
curated free-text or semi-structured informational sources can also
be used effectively for cQ&A tasks.

Conference’17, July 2017, Washington, DC, USA

In this work, we demonstrate that free text or semi structured
external information sources such as CDC1 , WHO2 or NHS3 can
be very useful for ranking answers on community Q&A platforms
since they contain frequently updated information about several
topics such as ongoing disease outbreaks, vaccines or resources
about other topics such as surgeries, birth control or historical
numerical data about diseases across the world.
We argue that for sensitive topics such as COVID-19, it is useful to use publicly available vetted information for improving our
ranking systems. In this work, we explore the utility of publicly
available information for ranking answers for questions associated
with COVID-19. We specifically focus on ranking answers for questions in two publicly available primary Q&A datasets: a) Yahoo!
Answers4 and b) recently released annotated Q&A dataset [13] in
presence of two external semi-structured curated sources: a) TRECCOVID [20] and b) WHO questions and answers 5 on COVID-19.
We explore the utility of deep learning models with attention in
this work to improve upon existing state-of-the-art systems.
More specifically, we propose a temperature regulated attention
mechanism to rank answers in presence of external (side) information. Our experiments on 10K+ questions from both source datasets
on COVID-19 show that our models can improve ranking quality
by a significant margin over question-answer matching baselines in
presence of external information. Figure 1 demonstrates the overall
design of our system. We specifically use attention based neural
architecture with temperature to automatically determine which
components in the external information are useful for ranking user
answers with respect to a question. Ranking performance, when
evaluated with three metrics shows that precision and recall for
correct answer retrieval improves by ∼17% and ∼9% for both source
datasets respectively over several other cQ&A models.

2

RELATED WORK

Community Question and Answering (cQ&A) systems is a well
researched sub-field both in information retrieval and NLP communities. Several systems have been proposed to rank user submitted
answers to questions on community platforms such as Yahoo! answers, Reddit and Quora.
Ranking user submitted answers on community question-answering
platforms has been addressed with several approaches. Primary
method is to determine the relevance of the answer given an input
question. Text based matching is one of the most common approaches to rank answers. Researchers have used several methods
to compute similarity between a question and user generated answers to determine relevance. For instance, feature based questionanswer matching is used in [19] with 17 features extracted from
unigrams, bigrams and web correlation features using unstructured
user search logs to rank answers. It is worth noting that user features and community features when incorporated may still yield
further improvements in the performance of these models but this
is not the focus of our work. The authors in [19] used questions
1 https://www.cdc.gov/
2 https://www.who.int/
3 https://www.nhs.uk/
4 https://answers.yahoo.com/
5 https://www.who.int/emergencies/diseases/novel-coronavirus-2019/question-and-

answers-hub

Manisha Verma, Kapil Thadani, and Shaunak Mishra

extracted from Yahoo! answers for their experiments. Researchers
have used different approaches such representation learning, for
instance, in [2, 14] authors use LSTM to represent questions and
answers respectively. Convolutional networks have also been used
in [22, 26] to rank answers. Other approaches such as doc2vec [12],
tree-kernels [15], adversarial learning [23], attention [9, 14, 24, 25]
or deep belief networks [21] have been used to score question and
answer pairs. There have also been studies exploring community,
user interaction or question based features [3, 6, 7, 22] to rank
answers. While these approaches are relevant, it is not always evident how one can incorporate external information when it is
either in free-text or semi-structured format into these systems.
We explore some question-answer based matching approaches as
baselines in this work and show that for rapidly evolving topics
such as COVID-19, inclusion of external curated information can
boost model performance.
The line of work most closely related to ours is incorporation of
knowledge bases in Q&A systems. Existing work [10, 16, 25], however, approaches different tasks. For instance, authors in [10, 16]
focus on finding factual answers to questions using a knowledge
base. This does not extend easily to cQ&A where neither the questions nor the answers may request or refer to any facts. Most recent
work is [25] on incorporating medical KB for ranking answers on
medical Q&A platforms. They propose to learn path based representation of entities (from KB) present in question and answers posted
by users. This approach relies on reliable detection of entities first,
which may be absent for emerging topics such as COVID-19 pandemic. Another limitation of this work is that external knowledge
may not always be present in a structured format. For example,
CDC guidelines are usually simple question-answer pairs posted on
the website. This makes it difficult to apply their approach to our
problem. The proposed approach in this work incorporates semistructured information directly with help of temperature regulated
attention.
Finally, with the rise of COVID-19, researchers across disciplines
are actively publishing information and datasets to share understanding of the virus and its impact on people. Researchers routinely
organize dedicated challenges such as SemEval [11] with tasks such
as ranking answers on QA forums. One such initiative is TRECCOVID track [20] which released queries, documents and manual
relevance judgements to power search for COVID related information. Authors in [18] also released COVID-19 related QA dataset
with 100+ questions and answers pairs extracted from TREC COVID
6 initiative. These questions/answer pairs are not user generated
content, hence, do not reflect real user questions. We also rely on
recently released Q&A dataset from [13] for our task. We also compile a dataset of 2000+ COVID-19 questions with 10K+ answers all
submitted by users on Yahoo! answers for this work.

3 METHOD
3.1 Problem formulation
In this work, we focus on ranking answers for 𝑛 questions 𝑞 1, . . . , 𝑞𝑛
related to an emerging topics such as COVID. Each 𝑞𝑖 is associated
with a set of two or more answers 𝐴𝑖 = {𝑎𝑖 𝑗 : 𝑗 ≥ 2} and corresponding labels 𝑌𝑖 = {𝑦𝑖 𝑗 : 𝑗 ≥ 2} representing answer relevance.
6 https://ir.nist.gov/covidSubmit/data.html

Powering COVID-19 community Q&A with Curated Side Information

Conference’17, July 2017, Washington, DC, USA

Answer Encoding: Each word 𝑤 𝑎𝑗 in the answer is also represented
as a 𝐾 dimensional vector with pre-trained word embeddings. LSTM
takes each token embedding as input and updates hidden state ℎ𝑎𝑗 .
We also reduce the dimension of answer encoding with a feed
forward layer with dimension 𝐹 < 𝐾 as follows:
ℎ𝑎𝑗 = 𝐿𝑆𝑇 𝑀 (ℎ𝑎𝑗−1, 𝑤 𝑎𝑗 ), 𝑓 𝑗𝑎 = 𝑅𝐸𝐿𝑈 (ℎ𝑎𝑗𝑊𝑎 + 𝑏𝑎 )

(2)

We concatenate the question and answer representations for further
processing.
𝑞
𝑓𝑖 𝑗 = [𝑓𝑖 , 𝑓 𝑗𝑎 ]
(3)
External source encoding: External sources of information can
vary from task-to-task. We encode each segment of data individually. For instance, if there are two segments in the source (e.g.
question/answer or query/document), our system encodes both
segments individually. We use the same encoding architecture used
for primary source question/answer encoding above. Encoding
example for two segment external source is given below.
Figure 2: External source augmentation model

𝑒𝑞

We use binary indicator for relevance where relevant judgments
(e.g., favorite, upvoted) are provided by the user, i.e., 𝑦𝑖 𝑗 ∈ {0, 1}
respectively.
We attempt to model the relevance of each answer 𝑎𝑖 𝑗 to its
corresponding question using an external source which may contain free text or semi-structured information. For example, the
external source could consist of information-seeking queries or
questions 𝑒𝑞 1, . . . , 𝑒𝑞𝑚 related to a topic, with each 𝑒𝑞𝑘 linked to a
set of relevant scientific articles or answers 𝐸𝐷𝑘 , where each answer/document 𝑒𝑑 1, . . . , 𝑒𝑑𝑝 may be judged for relevance by human
judges [20] or some experts.
We hypothesize that this semi-structured or free-text information may be valuable in identifying user answer quality for certain
kinds of questions, although not all. We investigate this with our
model to recover the true labels 𝑦𝑖 𝑗 for each user answer 𝑎𝑖 𝑗 ∈ 𝐴𝑖
given its question 𝑞𝑖 , category information, and information from
𝑚 .
the external source ⟨𝑒𝑞𝑘 , 𝐸𝐷𝑘 ⟩𝑘=1

3.2

Proposed Model

In this work, we explore token-level matching mechanism to determine the relevance of information in the external source that may
inform the label prediction task. Our model (𝜏-att) aims to match a
given user question with all the submitted answers in the presence
of external information about the same domain. First, the question
𝑞𝑖 , an answer 𝑎𝑖 𝑗 and additional metadata can be encoded into a
𝑑-dimensional vector 𝑥𝑖 using a text encoder 𝑓input . We use LSTM
based encoder for both question and answer in the primary source
which can handle input sequences of variable length.
𝑞

Question Encoding: Each word 𝑤𝑖 in a question is represented as
a 𝐾 dimensional vector with pre-trained word embeddings. LSTM
𝑞
takes each token embedding as input and updates hidden state ℎ𝑖
𝑞
based on previous state ℎ𝑖−1 . Finally, the hidden state is input to
a feed forward layer with smaller dimension 𝐹 < 𝐾 to compress
question encoding as follows:
𝑞

𝑞

𝑞

𝑞

𝑞

ℎ𝑖 = 𝐿𝑆𝑇 𝑀 (ℎ𝑖−1, 𝑤𝑖 ), 𝑓𝑖 = 𝑅𝐸𝐿𝑈 (ℎ𝑖 𝑊𝑞 + 𝑏𝑞 )

𝑒𝑞

𝑒𝑞

𝑒𝑞

ℎ𝑡 = 𝐿𝑆𝑇 𝑀 (ℎ𝑡 −1, 𝑤𝑡 ), 𝑓𝑡

(1)

𝑒𝑞

= 𝑅𝐸𝐿𝑈 (ℎ𝑡 𝑊𝑒𝑞 + 𝑏𝑒𝑞 )

𝑒𝑑
𝑒𝑑
𝑒𝑑
𝑒𝑑
ℎ𝑒𝑑
𝑡 = 𝐿𝑆𝑇 𝑀 (ℎ𝑡 −1 , 𝑤 𝑡 ), 𝑓𝑡 = 𝑅𝐸𝐿𝑈 (ℎ𝑡 𝑊𝑒𝑑 + 𝑏𝑒𝑑 )

(4)

We incorporate external source encoding with a temperature
(𝜏) based variant of scaled dot-product attention, which provides a
straightforward conditioning approach over a set of query-document
pairs. Question encoding vector 𝑓𝑖 𝑗 serves as a query over keys
𝑒𝑞
𝑓𝑡 . If two segments are present in the external source such as
query/document, the model uses the attention weights over first
segment (e.g. query) to determine the importance of the second
segment (e.g. document) respectively. It is easy to extend this framework to external sources with multiple segments. The two segment
attention is described below.
𝑒𝑞
∑︁
𝑓𝑖⊤𝑗 𝑓𝑡
𝑒 𝑧𝑖𝑡 /𝜏 ′
=
𝛼𝑖𝑡 𝑓𝑡𝑒𝑑
(5)
√ 𝛼𝑖𝑡 = Í 𝑧 /𝜏 𝑠𝑖𝑡𝑑
𝑙
𝑑
𝑙𝑒
𝑑
To summarize, temperature (𝜏) based attention helps determine
𝑒𝑞
the relevance of each 𝑓𝑡𝑒𝑑 corresponding 𝑓𝑡 with respect to the
question encoding. Temperature (𝜏) parameter helps us control the
uniformity of attention weights 𝛼𝑖𝑡 . Finally, labels are predicted
using a multi-layer perceptron over the input vector 𝑓𝑖 𝑗 and the
′ . We use binary
learned weighted average of side information 𝑠𝑖𝑡𝑑
cross entropy loss to train the proposed model.

𝑧𝑖𝑡 =

′
𝑦ˆ𝑖 𝑗 = 𝐹 output ([𝑓𝑖 𝑗 ; 𝑠𝑖𝑡𝑑
])

(6)

where 𝐹 output uses sigmoid activation function. Since community
questions may often be entirely unrelated to external sources, a key
aspect of this approach is determining whether the external source
is useful, not merely attending to its entries that are most relevant.
Temperature based attention mechanism is useful in controlling
which external source entries are useful for user questions. It is
worth noting that one will have to experiment and tune the value
of temperature 𝜏 such that ranking performance improves.

4

EXPERIMENTAL SETUP

Given the model architecture, in this section, we provide a detailed
overview of different datasets, metrics and baselines used in our
experiments.

Conference’17, July 2017, Washington, DC, USA

Source
Yahoo! Ans

Infobot

Manisha Verma, Kapil Thadani, and Shaunak Mishra

Question
I am really scared to go
places for St. Patrick’s
day because of the coronavirus. what do I do?

Rel answer
Non-rel answer
Unfortunately, there’s not enough people that care and
will still go out and party despite the coronavirus epi- Stop being scared of viruses. What’s the
demic. I’m proud of you in that you’re taking extra problem?
precautions ... Good for you!
The risk is quite low for one to
become infected with COVID19
Can corona live on card- A recent study shows that the virus can live in the air ...
through mail/packages - especially
On cardboard, it can live up to 24 hours (1 day)
board?
because...(over a period of a few
days/weeks).
Table 1: Sample rel/non-rel answers from both sources

(a) Yahoo! ques length

(c) Infobot ques length

(b) Yahoo! ans length

(d) Infobot ans length

Figure 3: Token distribution in different sources

4.1

Data

We compiled two question answering datasets. The first was collected from Yahoo! answers and the second was recently released
in [13] where both datasets have questions raised by real users.
In this work we focus specifically on questions associated with
COVID-19. Different statistics about the train and test split of both
q&a datasets are given in Table 2 respectively. A pair of relevant
and non-relevant answers for a question in both datasets is also
shown in Table 1 for reference. More details about them is given
below.
Yahoo! Dataset. : We crawled COVID-19 related questions from
Yahoo! answers 7 using several keywords such as ‘coronavirus’,
‘covid-19’, ‘covid’, ‘sars-cov2’ and ‘corona virus’ between the period
of Jan 2020 to July 2020 to ensure we gather all possible questions
for our experiments. We keep only those questions have two or
more answers. In total, we obtained 1880 questions with 11500
answers. We used favorite answers as positive labels (similar to
previous work [19]), assuming that users, over time rate answers
(with upvotes/downvotes) that are most relevant to the submitted
question. We normalized the question and answer text by removing
7 https://answers.search.yahoo.com/search?p=coronavirus

Stat
Yahoo! Ans Infobot
Train Q-A
9341
6354
Train ans/q
6.25±2.9
4.40±0.77
Train #qwords 12.71±5.8
6.55±3.93
Train #awords 36.31±93.59 92.17±59.27
Test Q-A
2232
1592
Test ans/q
5.96±2.87
4.41±0.76
Test #qwords
13.07±5.89
6.21±2.94
Test #awords
35.64±80.31 92.39±59.47
Table 2: Train and test data from primary sources

a small list of stop words, numbers, links or any symbols. Figure
3a and 3b show the distribution of question and answer lengths
respectively. Questions contain 12.7 ± 5.8 (qwords) words and answers consist of 36.3 ± 93.5 (mean±std) words (awords) respectively
which indicates that user submitted answers can vary widely on
Yahoo! answers. On average, a question has about 6 answers (ans/q)
in Yahoo! ans dataset. We spilt the data into three sets: train (64%,
1196 questions, 7435 answers), validation (16%, 298 questions, 1858
answers) and test (20%, 374 questions, 2310 answers) set where
questions for each set were uniformly sampled.
Infobot Dataset [13]. : Researchers at JHU [13] have recently
compiled a list of user submitted questions on different platforms
and manually labeled 22K+ question-answer pairs. We cleaned this
set by removing questions with less than two answers or no relevant
answers. In total, our dataset contains 8000+ question answer pairs
where each question may have multiple relevant answers which is
not the same as Yahoo! answers dataset. Figure 3c and 3d show the
distribution of question and answer lengths respectively.
4.1.1 External sources. We use two external datasets to rank answers. Details of each dataset are given below:
TREC COVID [20]: We use recently released TREC COVID-19
track data with 50 queries which also contain manually drafted
query descriptions and narratives. Expert judges have labeled over
5000 scientific documents for these 50 queries from the CORD-19
dataset 8 . These documents contain coronavirus related research.
Given the documents are scientific literature, we initialize document
embeddings using SPECTER [1].
8 https://www.semanticscholar.org/cord19

Powering COVID-19 community Q&A with Curated Side Information

Conference’17, July 2017, Washington, DC, USA

WHO:. We use data released on question and answer hub of
WHO website9 to create a list of question-answer pairs. There
are 147 question and answer pairs in this dataset where questions
contain 13.28±5.36 words and answers contain 133.2±100.9 words
respectively.

4.2

Baselines

We evaluated our model against embedding similarity baseline. We
computed four baselines as follows:
Random: An answer is chosen at random as relevant for a user
question. This is expected to provide a lower bound on retrieval
performance.

Yahoo! Ans
Infobot
Model
P@1
R@3 MRR P@1 R@3 MRR
𝜏-att
0.393
0.644 0.598 0.673 0.868 0.802
𝜆-sim
0.3743 0.633 0.578 0.551 0.817 0.7207
bert-sl256 0.406
0.657 0.615 0.581 0.803 0.744
bert-sl128 0.363
0.604 0.589 0.557 0.799 0.731
att
0.377
0.645 0.589 0.567 0.821 0.739
qasim
0.318
0.608 0.546 0.551 0.817 0.720
random
0.21
0.239 Table 3: Evaluation with WHO external data

It is defined as follows:

Linear Attention ( att). : When 𝜏 = 1.0, our model defaults
to simple linear attention over all the information present in the
external sources. This gives an indication of how well the model performs when its forced to look at all the information in the external
source.

𝑃𝑟𝑒𝑐@𝑘 =

(7)

𝑡𝑞

where 𝑦𝑎, 𝑦𝑞 and 𝑡𝑞 are Yahoo! answer, question and concatenated
trec query, narrative and description embeddings respectively. This
is a more crude version of temperature attention where 𝜆 controls
the contribution of each component directly. We vary 𝜆 to determine
the optimal combination. Question-Answer similarity (qasim) is
similarity between question and answer embedding i.e. 𝜆 = 1. Both
question and answer embeddings are obtained by averaging over
their individual token embeddings.
BERT Q&A ( bert). : Large scale pre-trained transformers [4]
are widely popular for NLP tasks. BERT like models have shown
effectiveness on Q&A datasets such as SQUAD 10 . We fine-tune
BERT base model with two different answer lengths a) 128 (bertsl128) and b) 256 tokens (bert-sl256) respectively. The intuition is
that large scale pre-trained models are adept at language understanding and can be fine-tuned for new tasks with small number
of samples. We finetune BERT for both datasets Yahoo! ans and
Infobot respectively. It is non-trivial to include external information
in BERT and we leave this for future work.

4.3

Evaluation Metrics

We evaluate the performance of our model using three popular
ranking metrics, mainly Precision (P@1), Mean Reciprocal Rank
(MRR), and Recall (R@3). Each metric is described below:
• Precision (P@k): Precision at position 𝑘 evaluates the fraction of relevant answers retrieved until position k. For, both
datasets Yahoo! ans and Infobot [13], we evaluate whether
the top answer i.e. (𝑘 = 1) in the ranked list is indeed correct.
9 https://www.who.int/emergencies/diseases/novel-coronavirus-2019/question-and-

𝑗=1 I{𝑟𝑒𝑙𝑖 𝑗

= 1}
(8)

𝑘

where |𝑟𝑒𝑙𝑖 | is the number of relevant answers for the 𝑖th
question.
• MRR (MRR): evaluates the average of the reciprocal ranks
corresponding to the most relevant answer for the questions
in test set, which is given by:
𝑀𝑅𝑅 =

|𝑄 |
1 ∑︁ 1
|𝑄 | 𝑖=1 𝑟𝑎𝑛𝑘𝑖

(10)

where |𝑄 | indicates the number of queries in the test set
and 𝑟𝑎𝑛𝑘𝑖 is the rank of the first relevant answer for the 𝑖 𝑡ℎ
query.

4.4

Parameter Settings

Both primary datasets, Yahoo! ans and Infobot, were divied into
three parts: train (∼60%), validation and test (20%) respectively. The
baseline models 𝜆-sim and 𝑎𝑡𝑡 are initialized with glove embeddings
11 of 100 dimensions. We performed a parameter sweep over 𝜆 and
𝜏 for 𝜆-sim and 𝜏-att models with step size of 0.1 between {0, 1.0}
respectively. We used base uncased model for 𝑏𝑒𝑟𝑡 implementation.
We fine-tuned the model between 1-10 epochs and found that 3
epochs gave the best result on validation set. We used LSTM with 64
hidden units to represent question, answer and all the information
in external datasets. We experimented with higher embedding size
and hidden units, but the performance degraded significantly as
the model tends to overfit on training data. Lastly we used batch
size of 64 and trained the model for 30 epochs with early stopping.

answers-hub
10 https://rajpurkar.github.io/SQuAD-explorer/

Í𝑘

where I{𝑟𝑒𝑙𝑖 𝑗 = 1} indicates whether the answer at position
𝑗 is relevant to the 𝑖 𝑡ℎ question.
• Recall (R@k): Recall at position 𝑘 evaluates the fraction
of relevant answers retrieved from all the answers marked
relevant for a question. We report recall averaged for all the
queries in test set. For recall, we take a cutoff as (𝑘 = 3),
which evaluates whether the model is able to retrieve the
correct answers in top 3 positions. It is defined as follows:
|𝑄 | Í𝑘
1 ∑︁ 𝑗=1 I{𝑟𝑒𝑙𝑖 𝑗 = 1}
(9)
𝑅𝑒𝑐𝑎𝑙𝑙@𝑘 =
|𝑄 | 𝑖=1
|𝑟𝑒𝑙𝑖 |

Linear combination ( 𝜆-sim). : We linearly combine similarities between Yahoo! question-answer and Trec query-answer as
shown below:
𝜆-sim = 𝜆 𝑐𝑜𝑠 (𝑦𝑎, 𝑦𝑞) + (1 − 𝜆) max (𝑐𝑜𝑠 (𝑦𝑎, 𝑡𝑞))

|𝑄 |
1 ∑︁
|𝑄 | 𝑖=1

11 https://nlp.stanford.edu/projects/glove/

Conference’17, July 2017, Washington, DC, USA

Yahoo! Ans
Infobot
Model
P@1 R@3 MRR P@1 R@3 MRR
𝜏-att
0.532 0.778 0.715 0.606 0.842 0.766
𝜆-sim
0.326 0.616 0.555 0.556 0.813 0.722
bert-sl256 0.406 0.657 0.615 0.581 0.803 0.744
bert-sl128 0.363 0.604 0.589 0.557 0.799 0.731
att
0.291 0.495 0.494 0.601 0.833 0.762
qasim
0.318 0.608 0.546 0.551 0.817 0.720
random
0.21
0.239 Table 4: Evaluation with TREC-COVID external data

Manisha Verma, Kapil Thadani, and Shaunak Mishra

Category
𝜏-att 𝜆-sim qasim
Entertainment (47) 0.446 0.382 0.297
Health (62)
0.483 0.419 0.354
Politics (143)
0.45
0.300 0.272
Society (38)
0.28
0.157 0.236
Family (20)
0.6
0.350 0.40
Table 6: Precision@1 of models across categories

Temperature (𝜏) > 1.0
100 1000 10
100
1000
Src+ Ext
Prec@1
Recall@3
Yahoo! + TREC 0.46 0.38 0.38
0.73 0.644 0.64
Yahoo! + WHO 0.37 0.38 0.36
0.64 0.65
0.64
Infobot + TREC 0.44 0.59 0.39
0.72 0.81
0.75
Infobot + WHO 0.65 0.41 0.44
0.85 0.76
0.79
Table 7: Variation in P@1 and R@3 across different temperature values.
10

Category
𝜏-att 𝜆-sim qasim
Entertainment (47) 0.829 0.702 0.59
Health (62)
0.693 0.69
0.645
Politics (143)
0.727 0.629 0.587
Society (38)
0.578 0.473 0.42
Family (20)
0.85
0.750 0.65
Table 5: Recall@3 of models across categories

5

RESULTS

In this work, our focus is to evaluate the utility of external information in improving answer ranking for cQ&A task. Thus, we
performed experiments to answer three main research questions
listed below.
RQ1: Does external information improve answer ranking?
RQ2: How does temperature (𝜏) compare with 𝜆 parameter?
RQ3: What kind of queries/questions does the model attend to
when ranking relevant/non-relevant answers?
RQ1: Does external information improve answer ranking? We
evaluated different models for ranking answers in Yahoo! ans and
Infobot dataset in presence of TREC and WHO datasets respectively.
We found that temperature regulated attention models that incorporate external sources indeed outperform the baselines as shown
in Table 4 and Table 3 respectively. (𝜏-att) model beats bert models
by ∼30% in precision, ∼18% in recall and ∼16% in MRR respectively
on TREC data. However, (𝜏-att) does only marginally better than att
model in precision and MRR on Infobot data. We suspect that is due
to the large set of query-document pairs in TREC-COVID data compared to fewer number of question-answer pairs in Infobot dataset.
Our results also clearly suggest that embedding based matching of
question-answer pair (qasim) would not yield a good ranker, though
it is better than choosing an answer at random (random). When
WHO is used as an external dataset, we find that (𝜏-att) model is
slightly worse than bert. This suggests that not all sources would
equally benefit cQ&A task. Since attention is dependent on the
input query and key embedding lengths, it would be interesting
to scale the computation in our model to incorporate several open
external datasets to overcome this limitation in the future.
Yahoo! ans questions are also assigned categories by users. Category based breakdown of performance on test set is given in Table
6 and Table 5 respectively, where categories with largest number

of questions in test set are listed. In all the categories, our model
outperforms best 𝜆-sim and qasim model respectively. The largest
improvement happens for questions in Family category where our
model achieves an improvement of 71% over the 𝜆-sim model. It
seems that ranking answers for questions from society and politics are harder than other categories. All the models, however, are
able to rank the top answer in first three positions effectively as
Recall@3 is high for all the categories.
RQ2: How does temperature (𝜏) compare with 𝜆 parameter? We argued that linearly combining similarities between question-answer
in primary dataset and between question-external source may not
be sufficient to boost performance. We observe that in our results
too i.e. 𝜆-sim models do not perform better than (𝜏-att) models.
This clearly indicates that more sophisticated models can learn
to combine this information directly from training data. However,
our experiments indicate that optimal value of (𝜏) varies across
primary datasets and external sources of information. For instance,
(𝜏-att) model performed best when 𝜏 = 0.4 and 𝜏 = 0.9 for Yahoo! ans and Infobot dataset respectively when TREC was used
as external source. It performed best when 𝜏 = 0.1 and 𝜏 = 0.5 for
Yahoo! ans and Infobot dataset respectively when WHO was used
as external source. We also tried to vary 𝜏 beyond 1.0 to determine
whether it yielded a trend as shown in Table 7. Higher values of
temperature seem to degrade model performance. We found that
optimal temperature range is between [0.1−1]. Existing research in
model distillation [5] has also empirically found that lower values
of temperature yield better performance.
We also compared model performance in terms of precision
when 𝜆 and 𝜏 are varied for 𝜆-sim models and temperature based
models respectively as shown in Figure 4. Temperature based models peak at one value but do not have a clear trend indicating that
one needs to explore different 𝜏 values at the time of training for
better performance. On the other hand, we observe that adding
external information also helps the 𝜆-sim models until a certain

Powering COVID-19 community Q&A with Curated Side Information

(a) Yahoo!+TREC

Conference’17, July 2017, Washington, DC, USA

(b) Yahoo!+WHO

(c) Infobot + TREC

(d) Infobot + WHO

Figure 4: Temperature and 𝜆 variation impact on Prec@1

Figure 5: Y! ques, its rel and non-rel ans and questions with
𝜏-att model’s attention values for TREC queries.

threshold. Overall, both sets of models show that free-text external information can be incorporated to improve answer ranking
performance.
RQ3: What kind of queries/questions does the model attend to
when ranking relevant/non-relevant answers? Attention based models have a very unique feature: they can aid explaining the internal workings of neural network models. We inspect what kind of
queries/questions in external datasets does our model pay attention
to while ranking relevant or non-relevant answers. Figure 5 shows
one such example of Yahoo! question and incorporation of TREC
data. At the time of scoring relevant answer, the model gives higher
weight to some queries compared to others. In the example, for
instance, it assigns more weight to queries associated with masks
or COVID virus response to weather changes. We observe higher
attention weights for questions when relevant answers are ranked
than when non-relevant answers are scored. An example question,
a relevant and non-relevant answer along with model attention
weights on TREC queries are shown from the Infobot data in Figure
6 respectively. It shows a similar trend where attention weights
are high for external queries that are closely associated with the
question answer text.
Overall, our experiments show that curated external information is useful for improving community question answering task.
Our experiments also indicate that this external knowledge need
not always be structured text. However, it is worth noting that
curated and reliable external sources may not always be available
for all domains. We addressed a very niche task in this work, and
further research is required to extend it to incorporate multiple
external sources. We posit that with scalable attention mechanisms,
this work can be easily made tractable for large external sources
containing thousands or millions of entries in the future.

Figure 6: Infobot ques, its rel and non-rel ans and questions
with 𝜏-att model’s attention values for TREC queries.

6

CONCLUSION

Question answering platforms provide users with effective and easy
access to information. These platforms also provide content on
rapidly evolving sensitive topics such as disease outbreaks (such as
COVID-19) where it is also useful to use external vetted information
for ranking answers. Existing work only exploits knowledge bases
which have some limitations that makes it difficult to use them for
community Q&A for rapidly evolving topics such as wild-fires or
earthquakes. In this work, we tried to evaluate the effectiveness of
external (free text or semi-structured) information in improving
answer ranking models. We argue that simple question-answer text
matching may be insufficient and in presence of external knowledge,
but temperature regulated attention models can distill information
better which in turn yields higher performance. Our proposed
model with temperature regulated attention, when evaluated on two
public datasets showed significant improvements by augmenting
information from two external curated sources of information. In
future, we aim to expand these experiments to other categories
such as disaster relief and scale the attention mechanism to include
multiple external sources in one model.

REFERENCES
[1] A. Cohan, S. Feldman, I. Beltagy, D. Downey, and D. S. Weld. Specter: Documentlevel representation learning using citation-informed transformers. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, pages
2270–2282, 2020.
[2] D. Cohen and W. Croft. End to end long short term memory networks for
non-factoid question answering. pages 143–146, 09 2016.
[3] D. H. Dalip, M. A. Gonçalves, M. Cristo, and P. Calado. Exploiting user feedback
to learn to rank answers in q&a forums: A case study with stack overflow. In
Proceedings of the 36th International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’13, page 543–552, New York, NY,
USA, 2013. Association for Computing Machinery.
[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2019.

Conference’17, July 2017, Washington, DC, USA

[5] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network,
2015.
[6] L. Hong and B. D. Davison. A classification-based approach to question answering
in discussion boards. In Proceedings of the 32nd international ACM SIGIR conference
on Research and development in information retrieval, pages 171–178, 2009.
[7] J. Hu, S. Qian, Q. Fang, and C. Xu. Attentive interactive convolutional matching
for community question answering in social multimedia. In Proceedings of the
26th ACM International Conference on Multimedia, MM ’18, page 456–464, New
York, NY, USA, 2018. Association for Computing Machinery.
[8] J. Hu, S. Qian, Q. Fang, and C. Xu. Hierarchical graph semantic pooling network
for multi-modal community question answer matching. In Proceedings of the
27th ACM International Conference on Multimedia, MM ’19, page 1157–1165, New
York, NY, USA, 2019. Association for Computing Machinery.
[9] H. Huang, X. Wei, L. Nie, X. Mao, and X.-S. Xu. From question to text: Questionoriented feature attention for answer selection. ACM Transactions on Information
Systems, 37:1–33, 10 2018.
[10] B. Kratzwald, A. Eigenmann, and S. Feuerriegel. Rankqa: Neural question answering with answer re-ranking. CoRR, abs/1906.03008, 2019.
[11] P. Nakov, D. Hoogeveen, L. Màrquez, A. Moschitti, H. Mubarak, T. Baldwin, and
K. Verspoor. Semeval-2017 task 3: Community question answering. arXiv preprint
arXiv:1912.00730, 2019.
[12] L. Nie, X. Wei, D. Zhang, X. Wang, Z. Gao, and Y. Yang. Data-driven answer
selection in community qa systems. IEEE transactions on knowledge and data
engineering, 29(6):1186–1198, 2017.
[13] A. Poliak, M. Fleming, C. Costello, K. W. Murray, M. Yarmohammadi, S. Pandya,
D. Irani, M. Agarwal, U. Sharma, S. Sun, et al. Collecting verified covid-19 question
answer pairs. 2020.
[14] A. Rücklé and I. Gurevych. Representation learning for answer selection with
LSTM-based importance weighting. In IWCS 2017 — 12th International Conference
on Computational Semantics — Short papers, 2017.
[15] A. Severyn and A. Moschitti. Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th international ACM SIGIR conference
on Research and development in information retrieval, pages 741–750, 2012.
[16] Y. Shen, Y. Deng, M. Yang, Y. Li, N. Du, W. Fan, and K. Lei. Knowledge-aware
attentive neural network for ranking question answer pairs. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval,

Manisha Verma, Kapil Thadani, and Shaunak Mishra

[17]

[18]
[19]
[20]
[21]

[22]
[23]

[24]

[25]

[26]

SIGIR ’18, page 901–904, New York, NY, USA, 2018. Association for Computing
Machinery.
Y. Shen, W. Rong, Z. Sun, Y. Ouyang, and Z. Xiong. Question/answer matching
for cqa system via combining lexical and sequential information. In Proceedings
of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI’15, page
275–281. AAAI Press, 2015.
D. Su, Y. Xu, T. Yu, F. B. Siddique, E. J. Barezi, and P. Fung. CAiRE-COVID: A
question answering and multi-document summarization system for covid-19
research. arXiv 2005.03975, 2020.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to rank answers on large
online qa collections. In ACL, 2008.
E. Voorhees, T. Alam, S. Bedrick, D. Demner-Fushman, W. R. Hersh, K. Lo,
K. Roberts, I. Soboroff, and L. L. Wang. Trec-covid: Constructing a pandemic
information retrieval test collection, 2020.
B. Wang, X. Wang, C.-J. Sun, B. Liu, and L. Sun. Modeling semantic relevance
for question-answer pairs in web social communities. In Proceedings of the 48th
Annual Meeting of the Association for Computational Linguistics, pages 1230–1238,
2010.
L. Yang, Q. Ai, D. Spina, R.-C. Chen, L. Pang, W. B. Croft, J. Guo, and F. Scholer.
Beyond factoid qa: Effective methods for non-factoid answer sentence retrieval.
In European Conference on Information Retrieval, pages 115–128. Springer, 2016.
X. Yang, M. Khabsa, M. Wang, W. Wang, A. H. Awadallah, D. Kifer, and C. L. Giles.
Adversarial training for community question answer selection based on multiscale matching. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pages 395–402, 2019.
X. Zhang, S. Li, L. Sha, and H. Wang. Attentive interactive neural networks
for answer selection in community question answering. In Proceedings of the
Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, page 3525–3531.
AAAI Press, 2017.
Y. Zhang, S. Qian, Q. Fang, and C. Xu. Multi-modal knowledge-aware hierarchical
attention network for explainable medical question answering. In Proceedings of
the 27th ACM International Conference on Multimedia, MM ’19, page 1089–1097,
New York, NY, USA, 2019. Association for Computing Machinery.
X. Zhou, B. Hu, Q. Chen, B. Tang, and X. Wang. Answer sequence learning with
neural networks for answer selection in community question answering. arXiv
preprint arXiv:1506.06490, 2015.

