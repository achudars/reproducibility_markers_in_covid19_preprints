Does the `1-norm Learn a Sparse Graph under Laplacian
Constrained Graphical Models?
Jiaxi Ying

∗

José Vinı́cius de M. Cardoso

†

Daniel P. Palomar

‡

arXiv:2006.14925v1 [cs.LG] 26 Jun 2020

June 29, 2020

Abstract
We consider the problem of learning a sparse graph under Laplacian constrained Gaussian graphical
models. This problem can be formulated as a penalized maximum likelihood estimation of the precision
matrix under Laplacian structural constraints. Like in the classical graphical lasso problem, recent
works made use of the `1 -norm regularization with the goal of promoting sparsity in Laplacian structural
precision matrix estimation. However, we find that the widely used `1 -norm is not effective in imposing
a sparse solution in this problem. Through empirical evidence, we observe that the number of nonzero
graph weights grows with the increase of the regularization parameter. From a theoretical perspective,
we prove that a large regularization parameter will surprisingly lead to a fully connected graph. To
address this issue, we propose a nonconvex estimation method by solving a sequence of weighted `1 norm penalized sub-problems and prove that the statistical error of the proposed estimator matches the
minimax lower bound. To solve each sub-problem, we develop a projected gradient descent algorithm
that enjoys a linear convergence rate. Numerical experiments involving synthetic and real-world data
sets from the recent COVID-19 pandemic and financial stock markets demonstrate the effectiveness of
the proposed method. An open source R package containing the code for all the experiments is available
at https://github.com/mirca/sparseGraph.
Keywords: Graph Laplacian, Sparse graph learning, Graph signal processing, Laplacian constrained
Gaussian graphical model, Nonconvex optimization.

1

Introduction

Gaussian graphical models (GGM) have been widely used in a number of fields such as finance, bioinformatics,
and image analysis [3, 25, 38, 49]. Graph learning under GGM can be formulated to estimate the precision
matrix that captures the conditional dependency relations between random variables [10, 36]. In this paper,
the goal is to learn a sparse graph under the Laplacian constrained GGM, where the precision matrix obeys
the Laplacian structural constraints.
The general GGM has received broad interest in statistical machine learning, where the problem can
be formulated as a sparse precision matrix estimation. The authors Banerjee et al. [3], d’Aspremont et al.
[9], Yuan and Lin [70] proposed the `1 -norm penalized maximum likelihood estimation, also known as graphical lasso, to encourage sparsity in its entries. Numerous methods have been developed for solving this
optimization problem. To solve the primal problem, first-order methods including Nesterovs smooth gradient method [9], augmented Lagrangian method [54], and second-order methods like inexact interior point
method [39], and Newton’s method [11, 28, 48, 55, 64] have been proposed. To solve the dual problem,
∗ Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water
Bay, Hong Kong SAR; Email: jx.ying@connect.ust.hk.
† Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Clear Water
Bay, Hong Kong SAR; Email: jvdmc@connect.ust.hk.
‡ Department of Electronic and Computer Engineering, Department of Industrial Engineering and Data Analytics, The Hong
Kong University of Science and Technology, Clear Water Bay, Hong Kong SAR; Email: palomar@ust.hk.

1

block coordinate ascent method [3, 22], projected subgradient method [16], and accelerated gradient descent
method [43] have been explored. In addition, various extensions of graphical lasso and their theoretical
properties have also been studied [27, 45, 50, 57, 68, 69]. However, those methods mentioned above focus
on general graphical models and cannot be directly extended to Laplacian constrained GGM because of
the multiple constraints on the precision matrix. Moreover, unlike the case of GGM, this paper shows that
the `1 -norm is not effective in promoting sparsity in the penalized maximum likelihood estimation under
Laplacian constrained GGM.
In recent years, Laplacian constrained GGM has received increasing attention in signal processing and
machine learning over graphs [13, 33, 37, 47, 58]. Under Laplacian constrained GGM, graph Laplacian
learning can be formulated as Laplacian structural precision matrix estimation. Unlike general GGM with
a general positive definite precision matrix, the precision matrix in Laplacian constrained GGM enjoys
the spectral property that its eigenvalues and eigenvectors can be interpreted as spectral frequencies and
Fourier basis [58], which is very useful in computing graph Fourier transform in graph signal processing
[47, 58], and graph convolutional networks [5, 46, 53]. To learn a graph Laplacian, the authors Dong et al.
[12], Egilmez et al. [17], Gadde and Ortega [23], Zhang and Florêncio [71] formulated the graph signals as
random variables under the Laplacian constrained GGM. The learned graph under Laplacian constrained
GGM favours smooth graph signal representations [12], since the graph Laplacian quadratic term quantifies
the smoothness of graph signals [30, 31]. The authors Kumar et al. [31] proposed to learn structured graphs
under the Laplacian constrained GGM where a number of graph structures can be learned by imposing
different Laplacian spectral constraints. A regularized Laplacian constrained GGM was proposed for graph
structure discovery [33] and dimensionality reduction [37], where the precision matrix is a Laplacian matrix
plus a very small diagonal matrix. The underlying assumption in [33, 37] is that each data feature is
independent and identically distributed sampled from a regularized Laplacian constrained GGM. However,
sparse graph learning under Laplacian constrained GGM remains to be further explored. For example, how
to effectively and efficiently learn a sparse graph and how to characterize the statistical error of the estimation
under Laplacian constrained GGM are to be investigated.
Recent works [17, 31, 40, 74] introduced the `1 -norm penalized maximum likelihood estimation under
Laplacian constrained GGM with the goal of learning a sparse graph. However, this paper will show that
the `1 -norm is not effective in imposing a sparse solution in this problem. The contributions of this paper
are summarized as follows.
• First, we find an unexpected behavior of the `1 -norm in Laplacian constrained GGM. More specifically,
through empirical evidence, we observe that the number of nonzero graph weights grows as the regularization parameter increases. From a theoretical perspective, we prove that a large regularization
parameter of the `1 -norm will surprisingly lead to a solution representing a fully connected graph.
• Second, we propose a nonconvex estimation method by solving a sequence of weighted `1 -norm penalized sub-problems. We present the theoretical guarantees on estimation error of the proposed estimator,
in which the statistical error matches the minimax lower bound. To the best of our knowledge, this is
the first work to analyze the non-asymptotic optimization performance guarantees on both optimization
error and statistical error under Laplacian constrained GGM.
• Third, to solve each sub-problem efficiently, we simplify the Laplacian structural constraints and develop a projected gradient descent algorithm which enjoys a linear convergence rate.
• Finally, numerical experiments on both synthetic and real-world data sets from the recent COVID-19
pandemic and financial stock markets demonstrate the effectiveness of the proposed method in learning
sparse and interpretable graphs.
The remainder of the paper is organized as follows. Problem formulation and related work are provided in
Section 2. We present the proposed method and provide the theoretical results in Section 3. The experimental
results are provided in Section 4. The proofs of the theorems and technical lemmas are in Section 5 and 6,
respectively. We draw the conclusions in Section 7.

2

Notation Lower case bold letters denote vectors and upper case bold letters denote matrices. Both Xij
and [X]ij denote the (i, j)-th entry of the matrix X. X > denotes transpose of X. [p] denotes the set
{1, . . . , p}. Let supp+ (x) = {i ∈ [p]|xi > 0} for any x ∈ Rp . We use x ≥ 0 to denote that each element of
x is non-negative. dxe denotes the least integer greater than or equal to x. The all-zero and all-one vectors
or matrices are denoted by 0 and 1. kxk, kXkF and kXk2 denote Euclidean norm, Frobenius norm and
operator norm, respectively. Let
P λmax (X) denote the maximum eigenvalue of X. The inner product of two
vectors is defined as hx, yi = i xi yi . Let kxkmax = maxi |xi | and kxkmin = mini |xi |. For functions f (n)
p
p
and g(n), we use f (n) . g(n) if f (n) ≤ Cg(n) for some constant C ∈ (0, +∞). S+
and S++
denote the sets
of positive semi-definite and positive definite matrices with size p × p, respectively. R+ denotes the set of
positive real numbers. E c denotes the complement of the set E.

2

Problem Formulation and Related Work

We first present the definition of Laplacian constrained Gaussian Markov random fields and formulate the
problem of learning a graph under the Laplacian constrained GGM. After that, we discuss relevant related
work.

2.1

Laplacian constrained Gaussian Graphical Model

We define a weighted, undirected graph G = (V, E, W ), where V denotes the set of nodes, and the pair
{i, j} ∈ E if and only if there is an edge between node i and node j. W ∈ Rp×p is the weighted adjacency
matrix with Wij denoting the graph weight between node i and node j. The graph Laplacian L ∈ Rp×p ,
also known as combinatorial graph Laplacian, is defined as
L = D − W,

(1)

Pp

where D is a diagonal matrix with Dii = j=1 Wij . In this paper, we focus on the case of connected graphs,
implying that there is only one graph component. From spectral graph theory [8], the rank of the Laplacian
matrix for a connected graph with p nodes is p − 1. Then, the set of Laplacian matrices for connected graphs
can be formulated as
p
SL = {Θ ∈ S+
| Θij = Θji ≤ 0, ∀ i 6= j, Θ · 1 = 0, rank(Θ) = p − 1},

(2)

where 0 and 1 denote the constant zero and one vectors, respectively. Next, we will define Laplacian
constrained Gaussian Markov random fields, and without loss of generality we assume that the random
vector x has zero mean.
Definition 2.1. A zero-mean random vector x = [x1 , . . . , xp ]> ∈ V p−1 is called a Laplacian constrained
Gaussian Markov Random Fields (L-GMRF) with parameters (0, Θ) with Θ ∈ SL , if and only if its density
function qL : V p−1 → R follows
 1

p−1
1
(3)
qL (x) = (2π)− 2 det? (Θ) 2 exp − x> Θx ,
2
where det? denotes the pseudo determinant defined by the product of nonzero eigenvalues [26], and V p−1
is a (p − 1)-dimensional subspace of the coordinate space Rp defined by V p−1 := {x ∈ Rp | 1> x = 0}.
Note that we restrict x into a subspace because the L-GMRF does not have a density with respect to the
p-dimensional Lebesgue measure. According to the disintegration theorem, we can construct a conditional
probability measure defined on V p−1 and then the density of L-GMRF with this measure satisfies (3). In
this sense, the L-GMRF can be interpreted as a GMRF conditioned on the linear constraint 1> x = 0 and
thus each observation x(k) of an L-GMRF also satisfies 1> x(k) = 0. For convenience, we still denote Θ in (3)
as the precision matrix, though it formally does not exist [52]. Notice that the precision matrix in L-GMRF
has sign constraints on the entries, which can also be found in Gaussian distributions with multivariate total
positivity [18, 35, 59], where the precision matrix is a symmetric M -matrix.
3

Sparse graph learning under the Laplacian constrained Gaussian graphical model can be formulated as
the penalized maximum likelihood of the precision matrix with Laplacian structural constraints,
X
min − log det(Θ + J ) + tr (ΘS) +
hλ (Θij ),
(4)
Θ∈SL

i>j

where S is the sample covariance matrix, hλ is a regularizer, depending on a regularization parameter λ ≥ 0,
which serves to enforce sparsity, e.g., hλ (Θij ) = λ|Θij |. J = p1 1p×p is a constant matrix with each element
equal to p1 . Note that we replace det? (Θ) with det(Θ + J ) in (4) as done in [17], which follows from the fact
that the matrix J is rank one, and the nonzero eigenvalue of J is 1 whose eigenvector is orthogonal to the
row and column spaces of Θ ∈ SL .

2.2

Related Work

The `1 -norm regularized maximum likelihood estimation under GGM has been extensively studied [22, 28,
45, 51, 57, 70]. To reduce estimation bias, nonconvex regularizers have been introduced in estimating a
sparse precision matrix [4, 7, 34, 41, 56, 72]. Some popular nonconvex penalties include the smooth clipped
absolute deviation (SCAD) [19], minimax concave penalty (MCP) [72], and capped `1 -penalty [73]. Several
types of algorithms including local quadratic appoximation [19], minimization-maximization [29, 61], and
local linear approximation [75] were proposed to solve the nonconvex optimization. Recently, the authors Fan
et al. [20], Loh and Wainwright [41], Sun et al. [60] presented some theoretical analysis to characterize the
nonconvex estimators with desired statistical guarantees. However, all of those methods cannot be directly
extended to Laplacian constrained GGM because we aim to learn a precision matrix that must satisfy the
constraints in (2), while the learned precision matrix under GGM is a general positive definite matrix.
Recent works [17, 31, 40, 74] proposed the `1 -norm penalized maximum likelihood estimation under
Laplacian constrained GGM to learn a graph Laplacian. The authors Egilmez et al. [17], Zhao et al.
[74] designed a primal-dual algorithm that introduces additional variables to handle Laplacian structural
constraints. The authors Liu et al. [40] proposed a block coordinate descent method to solve the optimization
problem. The authors Kumar et al. [31] proposed a `1 -norm regularized maximum likelihood estimation
method with Laplacian spectral constrains to learn structured graphs such as k-component graphs. More
recently, the authors Kumar et al. [32] proposed a framework with re-weighted `1 -norm to learn structured
graphs by imposing spectral constraints on graph matrices. But, the proposed algorithms in [32, 31] have
to compute the eigenvalue decomposition in each iteration which is computationally expensive in the highdimensional regime. Note that all the methods mentioned above lack theoretical analysis on estimation error
and algorithm convergence rate. In this paper, we will show that the `1 -norm is not effective in promoting
sparsity in the Laplacian constrained GGM, and further propose a nonconvex estimator with theoretical
guarantees on estimation error. The approach of theoretical analysis in this paper may be extended to
characterize statistical properties of other estimators under the Laplacian constrained GGM.

3

Proposed Method

In this section, we first present an unexpected behavior of the `1 -norm in learning a sparse graph under Laplacian constrained GGM. Then, we propose a nonconvex penalized maximum likelihood estimation method by
solving a sequence of weighted `1 -norm sub-problems, and develop a projected gradient descent algorithm
to solve each sub-problem. Finally, we present the theoretical results in the analysis of the estimation error
and algorithm convergence of the proposed method.

3.1

`1 -norm Regularizer

Sparsity is often explored in high-dimensional Gaussian graphical models in order to reduce the number of
samples required. The effectiveness of the `1 -norm regularized maximum likelihood estimation, also known
as graphical lasso, has been widely demonstrated in a number of fields. One common rule of thumb for
4

graphical lasso is that the estimated graph will get sparser when a larger regularization parameter is used.
However, we find an unexpected behavior of the `1 -norm in Laplacian constrained GGM. We will prove that
a large regularization parameter of the `1 -norm will lead to a solution that represents a fully connected graph
instead of a sparse graph.
The `1 -norm regularized maximum likelihood estimation under the Laplacian constrained Gaussian
graphical model [17, 74] can be formulated as
X
min − log det(Θ + J ) + tr (ΘS) + λ
|Θij |,
(5)
Θ∈SL

i>j

where S is the sample covariance matrix with each sample independently sampled from L-GMRF, and λ is
the regularization parameter. It is easy to check that (5) is a convex optimization problem.
Theorem 3.1. Let Θ̂ ∈ Rp×p be the global minimum of (5) with p > 3.
√ Define s1 = maxk Skk and
s2 = minij Sij . If the regularization parameter λ in (5) satisfies λ ∈ [(2 + 2 2)(p + 1)(s1 − s2 ), +∞), then
the estimated graph weight Ŵij = −Θ̂ij obeys
Ŵij ≥

1
> 0,
(s1 − (p + 1)s2 + λ)p

∀ i 6= j.

Theorem 3.1 states that a large regularization parameter of the `1 -norm will force every graph weight to
be strictly positive, thus the estimated graph is fully connected. This theoretical result is consistent with
empirical observations depicted in Figure 1, which shows that the number of positive edges learned by (5)
grows along with the increase of λ, and finally the estimated graph in Figure 1 (d) is fully connected. Figure
2 illustrates the histograms of the nonzero graph weights (associated with the positive edges) learned in
Figure 1. Figure 2 (d) depicts the histogram for the graph in Figure 1 (d), where every weight is strictly
positive. It is further observed in Figure 2 (d) that all the graph weights are very small. Therefore, a large
regularization parameter will lead to a graph with every weight strictly positive and small. We can see that
the histogram in Figure 2 (d) is significantly different from the true histogram in Figure 2 (a), implying that
the estimated model fails to identify the true relationships among the data variables.
The unexpected behavior of the `1 -norm characterized in Theorem 3.1 is due to the Laplacian structural
constraints in theP
optimization (5). Because of the constrains
Θ · 1 and Θij = Θji ≤ 0 for any i 6= j, the
P
term tr (ΘS) + λ i>j |Θij | in (5) can be written as i>j (λ + [L∗ S]k )|Θij |, where L∗ is the adjoint of the
linear operator L defined in (7), and k = i − j + j−1
2 (2p − j). To intuitively understand the behavior of the
`1 -norm with a large regularization parameter imposed in (5), suppose that λ is sufficiently large such that
(5) can be approximated well by
X
min − log det(Θ + J ) + λ
|Θij |.
(6)
Θ∈SL

i>j

Let Θ̃ be the optimal solution of (6). By calculation, we obtain that W̃ij = −Θ̃ij =

2
pλ

for any i 6= j. Notice

that every estimated graph weight W̃ij is strictly positive, and a large λ will lead each weight to be very
small. Every W̃ij with i 6= j is equal with each other because the derivative of |Θij | is constant for Θij ≤ 0.
In the general GGM, however, the estimated precision matrix only needs to be positive definite and there
are no Laplacian structural constraints imposed. The resultant optimization problem for precision matrix
estimation in general GGM cannot be approximated well by (6). This is the reason why the `1 -norm behaves
differently in the Laplacian constrained GGM and general GGM.
To solve the issue of the `1 -norm in Laplacian constrained GGM, we propose to introduce the nonconvex
regularizer, and the effectiveness of the proposed method with the nonconvex regularizer has been demonstrated by numerical experiments in Section 4. In the next subsection, we will develop an efficient algorithm
to solve the penalized maximum likelihood estimation with the nonconvex regularizer under Laplacian constrained GGM.

5

(a) Ground-truth

(b) λ = 0

(c) λ = 0.1

(d) λ = 10

Figure 1: Graph learning using `1 -norm regularization with different regularization parameters. The number of
positive edges in (a), (b), (c) and (d) are 49, 135, 286 and 1225, respectively. The graph in (d) is fully connected.
The relative errors of the learned graphs in (b), (c) and (d) are 0.14, 0.64 and 0.99, respectively.
100
300

4

40
20

2
0
1

2

3

4

5

2

4

6

0.0

Positive graph weights value

(a) Ground-truth

0.5

1.0

1.5

2.0

Positive graph weights value

(b) λ = 0

200
100

0
0

Positive graph weights value

50
25

0
0

75

Counts

6

Counts

60

Counts

Counts

8

(c) λ = 0.1

0
0.0000

0.0025

0.0050

0.0075

Positive graph weights value

(d) λ = 10

Figure 2: Histograms of nonzero graph weights learned by the `1 -norm regularization method with different regularization parameters, corresponding to the graphs learned in Figure 1. The histograms count the number of nonzero
graph weights falling into each interval.

3.2

Proposed Algorithm

Problem (4) is a constrained optimization problem with Θ ∈ SL including multiple constraints. We first
simplify the Laplacian structural constraints in (2) as follows.
The constraints Θij = Θji and Θ · 1 = 0 in (2) are linear and there are only p(p−1)
free variables in Θ.
2
Therefore, we use a linear operator defined in [32] that maps a vector x ∈ Rp(p−1)/2 to a matrix Lx ∈ Rp×p
as below.
Definition 3.2. The linear operator L : Rp(p−1)/2 → Rp×p , x 7→ Lx, is defined by


i > j,
−xk
[Lx]ij =
[Lx]ji
i < j,

 P
− j6=i [Lx]ij
i = j,
where k = i − j +

j−1
2 (2p

(7)

− j).

A simple example is given below which illustrates the definition of the operator L. Let x ∈ R3 . Then we
have
 P

−x2
i=1,2 xi P −x1
.
−x1
Lx = 
i=1,3 xi P −x3
−x2
−x3
x
i
i=2,3
The adjoint operator L∗ of L is defined so as to satisfy hLx, Y i = hx, L∗ Y i, ∀x ∈ Rp(p−1)/2 and
Y ∈ Rp×p .
6

Definition 3.3. The adjoint operator L∗ : Rp×p → Rp(p−1)/2 , Y 7→ L∗ Y , is defined by
[L∗ Y ]k = Yi,i − Yi,j − Yj,i + Yj,j ,
where i, j ∈ [p] obeying k = i − j +

j−1
2 (2p

(8)

− j) and i > j.

By introducing the linear operator L, we can simplify the definition of SL in (2) as below.
Theorem 3.4. The Laplacian set SL defined in (2) can be written as
n
o
p
SL = Lx| x ≥ 0, (Lx + J ) ∈ S++
.

(9)

where J = p1 1p×p and x ≥ 0 means every entry of x is non-negative.
As a result of Theorem 3.4, we introduce the linear operator L defined in (7) and reformulate the
optimization (4) as
X
min − log det(Lw + J ) + tr (SLw) +
hλ (wi ).
(10)
w≥0

i
p
S++

in (10) compared with the constraint set in (9),
Notice that we remove the constraint (Lx + J ) ∈
p
because any w in the feasible setP
of (10) must obey (Lx + J ) ∈ S++
as shown as follows. Let F (w) =
− log det(Lw + J ) + tr (SLw) + i hλ (wi ). The feasible set of (10) is Sw = {w |w ≥ 0, w ∈ dom(F )},
where dom(F ) denotes the domain of the function F . One can verify that
dom(F ) = {w ∈ Rp(p−1)/2 | det(Lw + J ) ∈ R+ }.

(11)

The set Sw can be equivalently written as
p
Sw = {w ∈ Rp(p−1)/2 |w ≥ 0, (Lw + J ) ∈ S++
},

(12)

which is due to the reason that Lw + J must be positive semi-definite, because Lw is positive semi-definite
for any w ≥ 0 following from (39), while the matrix J is rank one, and the nonzero eigenvalue of J is 1
whose eigenvector is orthogonal to the row and column spaces of Lw. The condition det(Lw + J ) ∈ R+
in (11) implies that Lw + J is non-singular. The non-singularity and positive semi-definiteness of Lw + J
p
.
together lead to (Lw + J ) ∈ S++
To solve the problem (10), we follow the majorization-minimization framework [61], which consists of
two steps. In the majorization step, we design a majorized function f (w|ŵ(k−1) ) that locally approximates
the objective function F (w) at ŵ(k−1) satisfying
f (w|ŵ(k−1) ) ≥ F (w)

and f (ŵ(k−1) |ŵ(k−1) ) = F (ŵ(k−1) ).

(13)

Then in the minimization step, we minimize the majorized function f (w|ŵ(k−1) ). We assume hλ is concave
(refer to Assumption 3.5 for
Pmore details about the choices of hλ ). Here we find the majorized function
f (w|ŵ(k−1) ) by linearizing i hλ (wi ). Set fk (w) = f (w|ŵ(k−1) ) to simplify the notation and obtain
X
fk (w) = − log det(Lw + J ) + tr (SLw) +
h0λ (ŵi (k−1) )wi ,
(14)
i

By minimizing fk (w), we establish a sequence {ŵ(k) }k≥1 by
ŵ(k) = arg min − log det(Lw + J ) + tr (SLw) +
w≥0

X

(k−1)

zi

wi ,

(15)

i

P (k−1)
P (k−1)
(k−1)
where zi
= h0λ (ŵi (k−1) ), i ∈ [p(p − 1)/2]. We can see i zi
wi is equivalent to i |zi
wi | because
(k−1)
w ≥ 0 and zi
≥ 0 by Assumption 3.5. Thus the problem (15) can be viewed as a weighted `1 -norm
7

penalized maximum likelihood estimation under Laplacian constrained Gaussian graphical model. The final
estimator ŵ is obtained by solving a sequence of optimization problems (15). The iteration procedure is
summarized in Algorithm 1.
To solve the optimization problem in (15), we develop an algorithm based on projected gradient descent
(k)
with backtracking line search. To obtain ŵ(k) , the algorithm starts with w0 and then establishes the
(k)
(k)
sequence {wt }t≥0 by the projected gradient descent as below. In the t-th iteration, we update wt by


(k)
(k)
(k)
wt = P+ wt−1 − η∇fk (wt−1 ) ,
(16)
(k)

(k)

(k)

where P+ (a) = max(a, 0) and ∇fk (wt−1 ) = −L∗ (Lwt−1 + J )−1 + L∗ S + z (k−1) . The sequence {wt }t≥0
(k)
(k−1)
will converge to ŵ(k) . Here we set w0 = ŵ(k−1) which is the limit point of the sequence {wt
}t≥0 . The
algorithm is summarized in Algorithm 2. Note that the users do not need to tune the step size manually
because of the backtracking line search.
To establish the theoretical results in Section 3.3, the initial point ŵ(0) of Algorithm 1 is chosen such
(0)
that |supp+ (ŵ(0) )| ≤ s, where supp+ (ŵ(0) ) = {i ∈ [p(p − 1)/2]|ŵi > 0}, and s is the number of the nonzero
(0)
graph weights in the true graph. In other words, ŵ has no more than s positive elements. Through the
(k)
analysis of Algorithm 2 in Section 3.3, we will see that the sequence {wt }t≥0 can be guaranteed in the
feasible set of (10), implying that the estimated graph must be connected (refer to the proof of Theorem 3.10
for more details). Though the proposed algorithm has two loops, we prove in Section 3.3 that for the outer
loop, only d4 log(4αc−1
0 )e iterations in Algorithm 1 are needed to achieve the desired order of the estimation
error, where α and c0 are two constants, independent of the dimension size p and sample size n. For the
inner loop, the projected gradient descent in Algorithm 2 enjoys a linear convergence rate.
Algorithm 1 Nonconvex Graph Learning (NGL)
Input: Sample covariance S, λ, ŵ(0) ;
k ← 1;
1: while Stopping criteria not met do
(k−1)
(k−1)
2:
Update zi
= h0λ (ŵi
), for i = 1, . . . , p(p − 1)/2;
P (k−1)
(k)
3:
Update ŵ = arg minw≥0 − log det(Lw + J ) + tr (SLw) + i zi
wi ;
4:
k ← k + 1;
5: end while
Output: ŵ.

Algorithm 2 Update ŵ(k)
Input: Sample covariance S, λ, ŵ(k−1) , β;
(k)
w0 = ŵ(k−1) ;
t ← 1;
1: while Stopping criteria not met do
(k)
(k)
(k) 
2:
Update wt = P+ wt−1 − η∇fk (wt−1 ) ;
(k)

(k)

(k)

(k)

if fk (wt ) > fk (wt−1 ) + h∇fk (wt−1 ), wt
4:
η ← βη;
5:
Back to Step 2;
6:
end if
7:
t ← t + 1;
8: end while
Output: ŵ(k) .
3:

(k)

− wt−1 i +

8

1
2η

(k)

wt

(k)

− wt−1

2

then

3.3

Theoretical Results

We first list the assumptions needed for establishing our theorems, and then present the theoretical analysis
on estimation error and algorithm convergence of the proposed method.
We denote the true graph weights by w? ∈ Rp(p−1)/2 , which are non-negative, i.e., w? ≥ 0. Let
?
S = {i ∈ [p(p − 1)/2] |wi? > 0} be the support set of w? and s be the number of the nonzero weights,
i.e., |S ? | = s. This paper focuses on learning connected graphs and thus w? corresponds to the weights
of a connected graph, implying that Lw? ∈ SL in (2) and (9). We impose some mild conditions on the
sparsity-promoting function hλ in Assumption 3.5 and the true graph weights w? in Assumption 3.6.
Assumption 3.5. The function hλ : R → R satisfies the following conditions:
1. hλ (0) = 0, and h0λ (x) is monotone and Lipschitz continuous for x ∈ [0, +∞);
2. There exists a γ > 0 such that h0λ (x) = 0 for x ≥ γλ;
3. h0λ (x) = λ for x ≤ 0 and h0λ (cλ) ≥ λ/2, where c = (2 +

√

2)λ2max (Lw? ) is a constant.

Assumption 3.6. The minimal nonzero graph weight satisfies mini∈S ? wi? ≥ (c + γ)λ & λ, where c and γ
are defined in Assumption 3.5. There exists τ ≥ 1 such that
1/τ ≤ λ2 (Lw? ) ≤ λmax (Lw? ) ≤ τ,

(17)

where λ2 (Lw? ) and λmax (Lw? ) are the second smallest eigenvalue and maximum eigenvalue of Lw? , respectively. Note that the smallest eigenvalue of Lw? is 0.
Remark 3.7. In Assumption 3.5, the conditions on hλ (x) are mainly made over x ∈ [0, +∞) because of
the nonnegativity constraint in the optimization (15). In Assumption 3.5, the first two conditions are made
to promote sparsity and unbiasedness [42], and hold for a variety of nonconvex sparsity-promoting functions
including the popular choices SCAD [19] and MCP [72]. The two conditions also ensure that the derivative
of hλ (x) is not constant to avoid the issue in Theorem 3.1. In the third condition, we specify h0λ (x) for
x ≤ 0 only for theoretical analysis. The condition h0λ (cλ) ≥ λ/2 can always hold by tuning parameters due
to the conditions h0λ (0) = λ and h0λ (γλ) = 0. Therefore, it is flexible to design a sparsity-promoting function
?
satisfying the conditions stated in 3.5. In Assumption 3.6, the conditions on the
ptrue graph weights w are
mild. In our theorems, the regularization parameter λ is taken with the order log p/n that could be very
small when the sample size n increases. The assumptions on the minimal magnitude of signals are often
employed in the analysis of nonconvex optimization [20, 65]. The assumptions on the minimum nonzero
eigenvalue are frequently made [7, 44, 65] to establish a local contraction region.
3.3.1

Analysis of Estimation Error

We characterize the estimation error of the proposed estimator. In the following theorem, the choice of the
regularization parameter λ is set according to a user-defined parameter α > 2. A larger α yields a larger
probability with which the claims hold, but also leads to a more stringent requirement on the number of
samples.
q
Theorem 3.8. Under Assumptions 3.5 and 3.6, take the regularization parameter λ = 4αc−1
0 log p/n for
some α > 2. If the sample size n is lower bounded by
2
?
n ≥ max(94αc−1
0 λmax (Lw )s log p, 8α log p),

then with probability at least 1 − 1/pα−2 , the sequence ŵ(k) returned by Algorithm 1 satisfies
q
 3 k
√
√
ŵ(0) − w? ,
ŵ(k) − w? ≤ 2(3 2 + 4)λ2max (Lw? ) αc−1
0 s log p/n +
2
+
2
|
{z
} |
{z
}
Statistical error

where c0 = 1/ 8 L∗ (Lw? + J )−1

2
max



is a constant.
9

Optimization error

Theorem 3.8 holds with overwhelming probability. According to Theorem 3.8, the estimation error
between the estimated and true graph weights is bounded by two terms, i.e., the optimization error and
statistical error. The optimization error, ( 2+3√2 )k kŵ(0) − w? k, decays to zero at a linear rate with respect to
the iteration number k. The statistical error depends on the sample size n, and a large sample size will lead
to a small statistical error. We can see the statistical error is independent of k, and thus will not decrease
during iterations in the algorithm. Therefore, the estimation error of the output of the algorithm is mainly
from the statistical error.
Corollary 3.9. Under the same assumptions and conditions as stated in Theorem 3.8, the sequence ŵ(k)
returned by Algorithm 1 satisfies
q
 3 k
√
√
Lŵ(0) − Lw? F ,
s
log
p/n
Lŵ(k) − Lw? F ≤ 4(2 2 + 3)λ2max (Lw? ) αc−1
+
0
{z
} |2+ 2
|
{z
}
Statistical error

Optimization error

with probability at least 1 − 1/pα−2 . If k ≥ d4 log(4αc−1
0 )e, then the estimation error is dominated by the
statistical error and we further obtain
p
Lŵ(k) − Lw? F . s log p/n,
where c0 = 1/ 8 L∗ (Lw? + J )−1

2
max



is a constant.

Corollary 3.9 presents the estimation error between the estimated and true precision matrices in Laplacian
constrained GGM. Similar
to Theorem 3.8, the estimation error decays to zero at a linear rate. The order
p
of statistical error is s log p/n, which matches the order of minimax lower bound [6, 51, 50] in Gaussian
graphical model. Furthermore, the proposed estimation method can achieve the order of minimax lower
−1
bound by solving only d4 log(4αc−1
0 )e sub-problems. Notice that 4 log(4αc0 ) is independent of the dimension
size p and sample size n.
3.3.2

Analysis of Algorithm Convergence

In this subsection, we will establish the convergence rate of Algorithm 2 in solving the sub-problem (15).
In the theorem statement, the parameter δ > 1 is a user-defined parameter. A larger δ leads to a faster
convergence rate, but a larger number of samples are required.
q
Theorem 3.10. Under Assumptions 3.5 and 3.6, take the regularization parameter λ = 4αc−1
0 log p/n for
some α > 2. If the sample size n satisfies

2
4

−1 (δτ + 1)
n ≥ c−1
max
n
≥
max
840αc
sp
log
p,
8α
log
p
,
0
0
δ2 τ 2
(k)

then with probability at least 1 − 1/pα−2 , the sequence {wt }t≥1 returned by Algorithm 2 obeys
(k)

wt

− ŵ(k)

2

(k)

≤ ρt w0 − ŵ(k)

−1 2

2

,

∀ k ≥ 2,

)
∗
?
−1
where ρ = 1 − pτβ(1−δ
4 (1+δ −1 )2 < 1 with δ > 1 and β ∈ (0, 1), and c0 = 1/ 8 L (Lw + J )

2
max



is a constant.

Theorem 3.10 shows that the designed projected gradient descent in Algorithm 2 enjoys a linear convergence rate in solving sub-problems (15). Note that the claims in Theorem 3.10 may not hold for k = 1,
because we only set a very mild condition on the initial point ŵ(0) that |supp+ (ŵ(0) )| ≤ s. Thus ŵ(0) may
1
1
not be in the local region B(w? ; √2pδτ
) = {w|w ∈ B(w? ; √2pδτ
)∩Sw }, where fk (w) is µ-strongly convex and
L-smooth with µ =

2
(1+δ −1 )2 τ 2

and L =

2pτ 2
(1−δ −1 )2

according to Lemma 5.9. One may impose more conditions

10

1
on initial point such that ŵ(0) ∈ B(w? ; √2pδτ
), then the claims in Theorem 3.10 also holds for k = 1. Here
B(w? ; r) = {w | kw − w? k ≤ r}, and τ is defined in Assumption 3.6.
It is observed that the required sample size in Theorem 3.10 is larger than the one in Theorem 3.8.
(k)
Using the sample size in Theorem 3.8, one can still establish a local contraction region Bfk (w? ; τ, w0 ) =
(k)
(k)
(k)
{w |w ∈ BM (w? ; τ ) ∩ Cfk (w0 ) ∩ Sw } by introducing the lower level set Cfk (w0 ) = {w |fk (w) ≤ fk (w0 )}
(k)
such that Algorithm 2 also has a linear convergence. However, the region Bfk (w? ; τ, w0 ) is much larger
(k)
1
than B(w? ; √2pδτ ), and the smoothness parameter L of fk (w) may be very large in Bfk (w? ; τ, w0 ). Here
BM (w? ; r) = {w | kw − w? kM ≤ r}, where M is defined in Lemma 5.3.

4

Experimental Results

In this section, we conduct numerical experiments on both synthetic data and real-world data sets from the
recent COVID-19 pandemic and financial stock markets to verify the performance of the proposed method in
learning sparse and interpretable graphs. The compared methods include the state-of-the-art GLE-ADMM
algorithm [74] and the baseline projected gradient descent with `1 -norm. All the experiments are conducted
on a PC with a 2.8 GHz Inter Core i7 CPU and 16 GB RAM.
We use the relative error (RE) and F-score (FS) to evaluate the performance of the algorithms. Those
performance measures are defined as
RE =

Θ̂ − Θ?
kΘ? kF

F

,

and FS =

2tp
,
2tp + fp + fn

(18)

where Θ̂ = Lŵ and Θ? = Lw? denote the estimated and true precision matrices, respectively. The true
positive number is denoted as tp, i.e., the case that there is an actual edge and the algorithm detects it, the
false positive is denoted as fp, i.e., the case that there is no actual edge but algorithm detects one, and the
false negative is denoted as fn, i.e., the case that the algorithm failed to detect an actual edge. The F-score
takes values in [0, 1], where 1 indicates perfect structure recovery.
For the proposed method, we test two nonconvex penalties, MCP and SCAD, defined respectively by

λ
x ∈ [0, λ],



 (γλ − x)
x
λ
−
x
∈
[0,
γλ],
γ
h0MCP,λ (x) =
and h0SCAD,λ (x) =
x ∈ [λ, γλ],

0
x ∈ [γλ, +∞),
γ−1

 0
x ∈ [γλ, +∞),
where we define h0MCP,λ (x) and h0SCAD,λ (x) only for x ≥ 0 because of the nonnegativity constraint in (15).
We set γ equal to 1.01 and 2.01 in h0MCP,λ (x) and h0SCAD,λ (x) in all the experiments, respectively. Note
that the proposed method is not limited to using MCP and SCAD regularizers, and one can explore other
sparsity-inducing functions [2] for interest.

4.1

Synthetic Data

For experiments on synthetic data, we consider two types of graphs: Barabasi-Albert graphs of degree one
[1], also known as tree graphs, and modular graphs. Estimating tree graphs is a crucial task that has an
impact in the performance of many data intensive applications such as image segmentation [67]. Modular
graph consists of a connected graph which contains clusters of nodes that share similar properties. The
sparsity level in modular graphs are twofold: (1) intra-modular and (2) inter-modular. The first one refers
to the number of connections among nodes that belong to the same module, whereas the second one refers
to the number of connections among nodes in different modules. Modular graphs have been applied to a
myriad of social network tasks such as community detection [21].
We generate the data matrix X ∈ Rp×n with each column of X independently drawn from L-GMRF (See
Definition 2.1) with Θ = Lw? , where w? contains the true weights from Barabasi-Albert graphs of degree
11

(a)

(b)

(c)

Figure 3: Performance measures (a) Number of positive edges, (b) Relative error and (c) F-score as a function of
regularization parameter λ in learning the Barabasi-Albert graph. The true number of positive edges in (a) is 49 and
the sample size ratio is n/p = 100.

(a)

(b)

(c)

Figure 4: Performance measures (a) Number of positive edges, (b) Relative error and (c) F-score as a function of the
sample size ratio of n/p in learning the Barabasi-Albert graph. The true number of positive edges in (a) is 49. The
regularization parameter λ for each algorithm is fine-tuned.

one and modular graphs. The number of nodes in the Barabasi-Albert graph and modular graph is p = 50
and 100, respectively. The weights associated with positive edges in both Barabasi-Albert graph and modular
graph are uniformly sampled from U (2, 5). The modular graph is generated randomly with probabilities of
intra-module and inter-module connections 0.25 and 0.005, respectively. The sample covariance matrix is
constructed by S = n1 XX > , where n is the number of samples.
The curves in Figures 3, 4, 5 and 6 are the results of an average of 100 Monte Carlo realizations and the
shaded areas around the curves represent the one-standard deviation confidence interval.
Figure 3 presents the results of learning Barabasi-Albert graphs by GLE-ADMM, projected gradient
descent with `1 -norm, and the proposed method with the two regularizers. It is observed that the numbers
of positive edges learned by GLE-ADMM and projected gradient descent with `1 -norm rise along with the
increase of λ, implying that the learned graphs will get denser as λ increases. Figure 3 shows that both
GLE-ADMM and projected gradient descent with `1 -norm achieve the best performance in terms of sparsity,
relative error, and F-score when λ = 0, which defies the purpose of introducing the `1 -norm regularizer. In
contrast, the proposed NGL-SCAD and NGL-MCP enhance sparsity when λ increases. It is observed that,
with λ equal to 0.1 or 0.25, NGL-SCAD and NGL-MCP achieve the true number of positive edges, and
an F-score of 1, implying that all the connections and disconnections between nodes in the true graph are
correctly identified.

12

(a)

(b)

(c)

Figure 5: Performance measures (a) Number of positive edges, (b) Relative error and (c) F-score as a function of
regularization parameter λ in learning modular graphs. The true number of positive edges in (a) is 344 and the
sample size ratio is n/p = 500.

(a)

(b)

(c)

Figure 6: Performance measures (a) Number of positive edges, (b) Relative error and (c) F-score as a function of
the sample size ratio of n/p in learning modular graphs. The true number of positive edges in (a) is 344. The
regularization parameter λ for each algorithm is fine-tuned.

Figure 4 shows that the proposed NGL-SCAD and NGL-MCP always outperform both GLE-ADMM
and the baseline projected gradient descent in terms of sparsity, relative error, and F-score under different
samples size ratios.
Figure 5 presents the results of learning modular graphs by GLE-ADMM [74], projected gradient descent with `1 -norm, and the proposed NGL-SCAD and NGL-MCP. Similar to the phenomenon in learning
Barabasi-Albert graphs, it is observed that a larger regularization parameter λ for GLE-ADMM and projected gradient descent with `1 -norm will lead to a worse performance in terms of sparsity, relative error,
and F-score. On the contrary, NGL-SCAD and NGL-MCP enhance the sparsity and improve the relative
error and F-score as λ increases.
Figure 6 shows that the proposed method always leads to a better performance in learning modular
graphs in terms of sparsity, relative error, and F-score, than the compared methods under different sample
size ratios.
Figure 7 shows a sample result of learning a Barabasi-Albert graph via GLE-ADMM, NGL-SCAD and
NGL-MCP. It is observed that the learned graphs via NGL-SCAD and NGL-MCP present the connection
between any two nodes correctly, while there are many incorrect connections in the graph learned via GLEADMM. In addition, performance measures including sparsity, relative error, and F-score also indicate a
better performance of the proposed method.
13

(a) Ground-truth graph,
NE = 49

(b) NE = 135, RE = 0.14,
FS = 0.53

(c) NE = 49, RE = 0.08,
FS = 1

(d) NE = 49, RE = 0.08,
FS = 1

Figure 7: A sample result in learning a Barabasi-Albert graph of degree one by (b) GLE-ADMM [74], (c) NGL-SCAD
(proposed) and (d) NGL-MCP (proposed). The sample size ratio is n/p = 6. NE denotes the number of positive
edges in the graph. The regularization parameters for each method are set as λADMM = 0, λSCAD = λMCP = 0.5.

(a) Ground-truth graph,
NE = 344

(b) NE = 681, RE = 0.14,
FS = 0.66

(c) NE = 398, RE = 0.15,
FS = 0.85.

(d) NE = 453, RE = 0.14,
FS = 0.81.

Figure 8: A sample result of learning a modular graph by (b) GLE-ADMM [74], (c) NGL-SCAD (proposed) and (d)
NGL-MCP (proposed). The sample size ratio is n/p = 6. NE denotes the number of positive edges in the graph. The
regularization parameters for each method are set as λADMM = 0, λSCAD = λMCP = 0.1.

Figure 8 shows a sample result of learning a modular graph via GLE-ADMM, NGL-SCAD and NGLMCP. It is observed that the learned graphs via NGL-SCAD and NGL-MCP are much more representative
than the one learned via GLE-ADMM.

4.2

Real-world Data Sets

In this section, we compare the proposed method with the benchmark GLE-ADMM [74] on three real-world
data sets.
We first conduct numerical experiments on the COVID-19 data set1 from 98 anonymous Chinese patients
affected by the outbreak of COVID-19 on early February, 2020. The features include age (integer), gender
(categorical), and location (categorical). The label is a binary variable representing the life status of patients,
alive (green) or no longer alive (red). Our goal is to construct a graph from the data features. To this end,
we first pre-process the feature matrix so as to transform the categorical features into numerical ones via
one-hot-encoding. The pre-processed feature matrix X has the dimension 98 × 32, i.e., p = 98 and n = 32.
We then compute the sample covariance matrix and learn the graphs.
Figure 9 shows that the benchmark GLE-ADMM is unable to impose sparsity, diminishing interpretation
capabilities of the graph severely. On the other hand, the proposed NGL-SCAD and NGL-MCP obtain
sparse graphs with clearer connections. The learned graphs possibly provide guidance on priority setting
in health care because green nodes (patients alive) that have stronger connections with red nodes (patients
that passed away) may suffer a higher health risk.
1 2019-nCoV data is available in a queryable format via the R package nCov2019 which lives on GitHub: https://github.
com/GuangchuangYu/nCov2019.

14

(a)

(b)

(c)

Figure 9: The learned graphs using the COVID-19 data set from Chinese patients via (a) GLE-ADMM, (b) NGL-SCAD
(proposed), and (c) NGL-MCP (proposed). The computational time for GLE-ADMM, NGL-SCAD and NGL-MCP
are 2.9, 0.7 and 0.8 seconds, respectively. The regularization parameters are set as λADMM = 0, λSCAD = 0.6, and
λMCP = 1.2.

(a)

(b)

(c)

Figure 10: The learned graphs using the COVID-19 data set provided by the Israelite Hospital Albert Einstein via (a)
GLE-ADMM, (b) NGL-SCAD (proposed), and (c) NGL-MCP (proposed). The computational time for GLE-ADMM,
NGL-SCAD and NGL-MCP are 9.9, 55.8 and 57.9 seconds, respectively. The regularization parameters are set as
λADMM = 0, λSCAD = 0.1 and λMCP = 0.5.

We next perform experiments on the COVID-19 data set2 provided by the Israelite Hospital Albert
Einstein in Brazil. The data set contains anonymized data from patients who had samples collected to
perform the test for SARS-CoV-2. The features in the data set are mainly clinical coming from blood,
urine, and saliva exams, e.g., hemoglobin level, platelets, red blood cells, etc. The original data set contains
108 features from 558 patients. Due to the high number of missing values, we do not consider features
that were measured for at most 10 patients. In addition, a large number of patients had no record of any
features. Finally, we end up with a data matrix of 182 patients with 57 features, i.e., p = 182 and n = 57.
The remaining missing values were filled in with zeros. We then compare the proposed method with the
GLE-ADMM method on this data set. It is observed from Figure 10 that the proposed NGL-SCAD and
NGL-MCP output a more interpretable representation of the network, where blue and red nodes denote
patients who tested negative and positive for SARS-CoV-2, respectively.
The third data set is from stocks composing the S&P 500 index. We select log-returns from 181 stocks
from 4 sectors, namely: ”Industrials”, ”Consumer Staples”, ”Energy”, ”Information Technology”, during a
period of 4 years from January 1st 2016 to May 20th 2020, with a total of 1101 observations. Then the data
matrix X has a size of 181 × 1101. The graphs are learned on the basis of the sample correlation matrix.
2 The

data set is freely available at: https://www.kaggle.com/einsteindata4u/covid19.

15

(a)

(b)

(c)

Figure 11: Stock graph learned via (a) GLE-ADMM, (b) NGL-SCAD (proposed), and (c) NGL-MCP (proposed).
The computational time for GLE-ADMM, NGL-SCAD and NGL-MCP are 20.5, 1.3 and 90.3 seconds, respectively.
The regularization parameters are set as λADMM = 0, λSCAD = 0.25 and λMCP = 0.5.

Figure 11 shows that the graphs learned via NGL-SCAD and NGL-MCP are able to vividly display the
sectors modularity, whereas the graph learned via GLE-ADMM fails to do so.
Note that the real-world data sets may not exactly follow the Laplacian constrained Gaussian graphical
models. In this case, the log-determinant Bregman divergence regularized optimization with Laplacian
structural constraints can be interpreted as learning a graph such that the features are smooth on the
learned graph [12, 32].

5

Proofs of Theorems

This section includes the proofs of Theorems 3.1, 3.4, 3.8 and 3.10, and Corollary 3.9. Before proving the
theorems, we first present some technical lemmas.

5.1

Technical Lemmas

Lemma 5.1. Let f (w) = − log det(Lw + J ), with w ∈ Rp(p−1)/2 . Then for any x ∈ Rp(p−1)/2 , we have


−1
−1
x> ∇2 f (w)x = vec(Lx)> (Lw + J ) ⊗ (Lw + J )
vec(Lx).
p
Lemma 5.2. For any given w ∈ Rp(p−1)/2 satisfying (Lw + J ) ∈ S++
, there must exist an unique x ∈
p(p−1)/2
R
such that

1
−1
Lx + J = (Lw + bJ )
b

(19)

holds for any b 6= 0, where J = p1 1p×p , in which 1p×p ∈ Rp×p with each element equal to 1.
Lemma 5.3. Let G = L∗ L : Rp(p−1)/2 → Rp(p−1)/2 , x 7→ L∗ Lx. For any x ∈ Rp(p−1)/2 , Gx = M x with
p(p−1)
p(p−1)
M ∈ R 2 × 2 satisfying


l = k,
4
Mkl = 1
l ∈ (Ωi ∪ Ωj ) \k,


0
Otherwise,

16

where i, j ∈ [p] satisfying k = i − j +
(
Ωt :=

j−1
2 (2p

− j) and i > j, and Ωt is an index set defined by
)
X
l ∈ [p(p − 1)/2] |[Lx]tt =
xl , t ∈ [p].
l

Furthermore, we have λmin (M ) = 2 and λmax (M ) = 2p.
q
−1 2
?
Lemma 5.4. Take λ = 4αc−1
0 log p/n and suppose n ≥ 94αc0 λmax (Lw )s log p for some α > 2, where
c0 is a constant defined in Lemma 5.8. Let
ŵ = arg min − log det(Lw + J ) + tr (LwS) + z > w,
w≥0


−1
where z obeys 0 ≤ zi ≤ λ for i ∈ [p(p − 1)/2]. If L∗ (Lw? + J ) − S max ≤ λ/2 ≤ kzE c kmin holds with
the set E satisfying S ? ⊆ E and |E| ≤ 2s, then ŵ obeys

√
√
 
√
−1
kLŵ − Lw? kF ≤ 2 2λ2max (Lw? ) kzS ? k + L∗ (Lw? + J ) − S E
≤ 2(1 + 2)λ2max (Lw? ) sλ,
where S ? is the support of w? .
Lemma 5.5. Take λ =

q

−1 2
?
4αc−1
0 log p/n and suppose n ≥ 94αc0 λmax (Lw )s log p for some α > 2, where

c0 is a constant defined in Lemma 5.8. Define the set E (k) by
E (k) = {S ? ∪ S (k) },

with

(k−1)

S (k) = {i ∈ [p(p − 1)/2] |ŵi

≥ b},

(20)

√
where ŵ(k) for k ≥ 1 is defined in (15), S ? is the support of w? with |S ? | ≤ s and b = (2 + 2)λ2max (Lw? )λ
−1
is a constant. Under Assumption 3.5, if L∗ (Lw? + J ) − S max ≤ λ/2 holds and ŵ(0) satisfies
+
(0)
(k)
(k)
|supp (ŵ )| ≤ s, then E
obeys |E | ≤ 2s, for any k ≥ 1.
q

−1 2
?
4αc−1
0 log p/n and suppose n ≥ 94αc0 λmax (Lw )s log p for some α > 2, where c0

−1
is a constant defined in Lemma 5.8. Under Assumptions 3.5 and 3.6, if L∗ (Lw? + J ) − S max ≤ λ/2
holds and ŵ(0) satisfies |supp+ (ŵ(0) )| ≤ s, then for any k ≥ 1, ŵ(k) defined in (15) obeys

Lemma 5.6. Take λ =

ŵ(k) − w? ≤ 2λ2max (Lw? )

L∗ (Lw? + J )

−1

−S


S?

+

3
√ ŵ(k−1) − w? ,
2+ 2

and
Lŵ(k) − Lw?

F

√
≤ 2 2λ2max (Lw? )

L∗ (Lw? + J )

−1

−S


S?

+

3
√ Lŵ(k−1) − Lw?
2+ 2

F

.

q
Lemma 5.7. Take λ = 4αc−1
0 log p/n and suppose n ≥ 8α log p for some α > 2, where c0 is a constant
defined in Lemma 5.8. Then one has



−1
P L∗ (Lw? + J ) − S max ≤ λ/2 ≥ 1 − 1/pα−2 .
Lemma 5.8. Consider a zero-mean random vector x = [x1 , . . . , xp ]> ∈ Rp is a L-GMRF with precision
matrix Lw? ∈ SL . Given n i.i.d samples x(1) , . . . , x(n) , the associated sample covariance matrix S =
>
Pn
1
(k)
satisfies, for t ∈ [0, t0 ],
x(k)
k=1 x
n
 ∗


P [L S]i − L∗ (Lw? + J )−1 i ≥ t ≤ 2 exp(−c0 nt2 ), for i ∈ [p(p − 1)/2],

2
where t0 = L∗ (Lw? + J )−1 max and c0 = 1/ 8 L∗ (Lw? + J )−1 max are two constants.
17

Lemma 5.9. Let f (w) = − log det(Lw + J ). Define a local region of w? by
B(w? ; r) = {w|w ∈ B(w? ; r) ∩ Sw }.
p
where B(w? ; r) = {w ∈ Rp(p−1)/2 | kw − w? k ≤ r}, and Sw = {w |w ≥ 0, (Lw + J ) ∈ S++
}. Then, under
2

2pτ
2
? √ 1
Assumption 3.6, g(w) is (1+δ−1
)2 τ 2 -strongly convex and (1−δ −1 )2 -smooth in the region B(w ; 2pδτ ) where
1
τ is defined in (17) and δ > 1. In other words, for any w1 , w2 ∈ B(w? ; √2pδτ
), we have

1
pτ 2
2
2
kw2 − w1 k ≤ f (w2 ) − f (w1 ) − h∇f (w1 ), w2 − w1 i ≤
kw2 − w1 k .
−1
2
2
(1 + δ ) τ
(1 − δ −1 )2
Lemma 5.10. Let f (w) = − log det(Lw + J ). Define a local region of w? by
BM (w? ; r) = {w|w ∈ BM (w? ; r) ∩ Sw }.
1

where BM (w? ; r) = {w ∈ Rp(p−1)/2 | kw − w? kM ≤ r}, in which kxkM = hx, M xi 2 for any x ∈ Rp(p−1)/2
p
with M defined in Lemma 5.3, and Sw = {w |w ≥ 0, (Lw + J ) ∈ S++
}. Then for any w1 , w2 ∈ BM (w? ; r),
we have
−2

h∇f (w1 ) − ∇f (w2 ), w1 − w2 i ≥ (kLw? k2 + r)

2

kLw1 − Lw2 kF .

Lemma 5.11. [66] Suppose a positive matrix A ∈ Rp×p is diagonally scaled such that Aii = 1, i = 1, . . . , p,
and 0 < Aij < 1, i 6= j. Let y and x be the lower and upper bounds satisfying
0 < y ≤ Aij ≤ x < 1,

∀ i 6= j,

and define s by
x2 = sy + (1 − s)y 2 .
Then the inverse matrix of A exists and A is an inverse M -matrix if s−1 ≥ p − 2 with p > 3.
Lemma 5.12. [63] (Sub-exponential tail bound) Suppose X is sub-exponential with parameters (υ, α). Then
(
t2
2
e− 2υ2
t ∈ [0, υα ],
P[X − µ ≥ t] ≤
2
t
e− 2α
t ∈ ( υα , +∞).

5.2

Proof of Theorem 3.1

Proof. As a result of Theorem 3.4, the optimization (5) can be equivalently written as
min − log det(Lw + J ) + tr (LwS) + λ kwk1 .

w≥0

(21)

Due to the non-negativity constraint w ≥ 0, (21) can be further rewritten as
min − log det(Lw + J ) + hL∗ S + λ1, wi,

w≥0

(22)

where 1 = [1, . . . , 1]> .
We first prove that the optimization (22) has one global minimizer if λ > 0. Let f (w) = − log det(Lw +
p
J ) + hL∗ S + λ1, wi. The feasible set of (22) is Sw = {w ∈ Rp(p−1)/2 |w ≥ 0, (Lw + J ) ∈ S++
}, which is
18

the same with the feasible set of (10). For any w ∈ Sw , the minimum eigenvalue of ∇2 f (w) can be lower
bounded by

λmin ∇2 f (w) = inf x> ∇2 f (w)x
kxk=1


>
−1
−1
= inf (vec(Lx)) (Lw + J ) ⊗ (Lw + J )
vec(Lx)
kxk=1

≥ inf

(vec(Lx))

kxk=1

≥ λmin
= λmin

>


(Lw + J )−1 ⊗ (Lw + J )−1 vec(Lx)
>

(vec(Lx)) vec(Lx)

2
(Lw + J ) ⊗ (Lw + J )−1 · inf kLxkF
kxk=1

(Lw + J )−1 ⊗ (Lw + J )−1 · inf x> M x

2

· inf kLxkF
kxk=1

−1

kxk=1

= 2λmin (Lw + J )


−1 2

> 0,
where the second equality is due to Lemma 5.1; the third equality follows from Lemma 5.3; the last equality
follows from the property of Kronecker product that the eigenvalues of A ⊗ B are λi µj for i, j ∈ [p], where
λi and µj are the eigenvalues of A ∈ Rp×p and B ∈ Rp×p , respectively, and λmin (M ) = 2 following from
Lemma 5.3; the last inequality follows from the fact that w ∈ Sw . Therefore, the optimization (22) is strictly
convex, and thus (22) has at most one global minimizer.
The existence of minimizers of (22) can be guaranteed by the coercivity of f (w). The function f (w) can
be lower bounded by
!
p
Y
f (w) = − log
λi (Lw + J ) + hL∗ S + λ1, wi
= − log

i=1
p
Y

!
+ hL∗ S + λ1, wi

λi (Lw)

i=2

≥ −(p − 1) log
= −(p − 1) log

p
X
i=1
p
X

!
+ hL∗ S + λ1, wi + (p − 1) log(p − 1)

λi (Lw)
!
[Lw]ii

+ hL∗ S + λ1, wi + (p − 1) log(p − 1)

i=1



p(p−1)/2

= −(p − 1) log 2

X


wt  + hL∗ S + λ1, wi + (p − 1) log(p − 1)

t=1


≥ −(p − 1) log 

p(p−1)/2

X



p(p−1)/2

wt  + λ

t=1

X

wt + (p − 1) log

t=1

p−1
,
2

(23)

where the second equality follows from (56) with b = 1; the forth equality follows from Lw · 1 = 0; the last
inequality holds because w ≥ 0, and L∗ S ≥ 0, which follows from (92); the first inequality holds because
the smallest eigenvalue λ1 (Lw) = 0 and
√
a1 + a2 + . . . + an
≥ n a1 · a2 · · · an
n
holds for any non-negative real numbers of a1 , . . . , an . A function g : Ω → R ∪ {+∞} is called coercive over
Ω, if every sequence xk ∈ Ω with kxk k → +∞ obeys limk→∞ g(xk ) = +∞, where Ω ⊂ Rn . Let
h(z) = −(p − 1) log z + λz + (p − 1) log

19

p−1
.
2

A simple calculation yields limz→+∞ h(z) = +∞ if λ > 0. For any sequence wk ∈ cl(Sw ) with kwk k → +∞,
Pp(p−1)/2
Pp(p−1)/2
where cl(Sw ) is the closure of Sw , one has t=1
[wk ]t → +∞, because t=1
[wk ]t ≥ kwk k. Then
one obtains
p(p−1)/2

lim f (wk ) ≥ lim h(

k→∞

k→∞

X

[wk ]t ) = lim h(z) = +∞,
z→+∞

t=1

where the first inequality follows from (23). Hence, f (w) is coercive over cl(Sw ). Following from the Extreme
Value Theorem [14], if Ω ⊂ Rn is non-empty and closed, and g : Ω → R ∪ {+∞} is lower semi-continuous and
coercive, then the optimization minx∈Ω g(x) has at least one global minimizer. Therefore, by the coercivity
of f (w), (22) has at least one global minimizer in cl(Sw ).
p
Let ΩA = {w ∈ Rp(p−1)/2 |w ≥ 0} and ΩB = {w ∈ Rp(p−1)/2 |(Lw + J ) ∈ S++
}. ΩA is a closed set and
ΩB is an open set. Then Sw can be rewritten as Sw = ΩA ∩ ΩB . Consider the set V := cl(Sw ) \ Sw , we have
n
o n
o
V ⊆ cl(ΩA ) ∩ cl(ΩB ) \ ΩA ∩ ΩB = ΩA ∩ ∂ΩB ,
(24)
where ∂ΩB is the boundary of ΩB . Notice that every matrix on the boundary of the set of positive definite
matrices is positive semi-definite and has zero determinant. Hence, one has ∂ΩB = {w ∈ Rp(p−1)/2 |(Lw +
p
, det(Lw + J ) = 0}. As a result, for any wb ∈ cl(Sw ) \ Sw , f (wb ) = +∞. Therefore, (22) has
J ) ∈ S+
at least one global minimizer and the minimizer must belong to the set Sw . On the other hand, by the
strict convexity of f (w), (22) has at most one global minimizer in Sw . Totally, we conclude that (22) has
an unique global minimizer in Sw if λ > 0.
We prove the theorem through the KKT conditions. The Lagrangian of the optimization (22) is
L(w, υ) = − log det(Lw + J ) + hL∗ S + λ1, wi − υ > w,
where υ is a KKT multiplier. Let (ŵ, υ̂) be any pair of points that satisfies the KKT conditions. Then we
have

−L∗ (Lŵ + J )−1 + L∗ S + λ1 − υ̂ = 0;
(25)
ŵi υ̂i = 0, for i = 1, . . . , p(p − 1)/2;

(26)

ŵ ≥ 0, υ̂ ≥ 0;

(27)

As we know, for any convex optimization with differentiable objective and constraint functions, any point
that satisfies the KKT conditions (under Slater’s constraint qualification) must be primal and dual optimal.
Therefore, ŵ must obey ŵ = arg minw≥0 f (w). Note that the pair of points (ŵ, υ̂) that satisfies the KKT
conditions is unique. To prove the optimal solution ŵ > 0 holds for (22), we can equivalently prove that the
KKT conditions (25)-(27) hold for (ŵ > 0, υ̂ = 0). It is further equivalent to prove that

L∗ (Lŵ + J )−1 = L∗ S + λ1.
(28)
holds for ŵ > 0. Following from Lemma 5.2 with the fact that ŵ ∈ Sw , there must exist an unique x such
that
1
Lx + J = (Lŵ + bJ )−1
b

(29)


L∗ (Lŵ + J )−1 = L∗ (Lx + J ) = L∗ Lx,

(30)

holds for any b 6= 0. Thus one has

where the first equality follows from (29) with b = 1; the second equality holds because J ∈ N (L∗ ) where
N (L∗ ) is the null space of L∗ defined by N (L∗ ) := {X ∈ Rp×p | L∗ X = 0}. Combining (28) and (30) yields
x = (L∗ L)−1 (L∗ S + λ1),
20

(31)

where L∗ L is invertible according to Lemma 5.3. Recall that S is the sample covariance matrix defined by
S=

n
X

>

x(k) x(k) ,

k=1

where x(1) , . . . , x(n) are the samples independently drawn from L-GMRF in Definition 2.1. According to the
density function of L-GMRF defined in (3), we get 1> x(k) = 0 for k = 1, . . . , n. Therefore, S is symmetric
and obeys S · 1 = 0. It is easy to verify that S ∈ R(L), where R(L) is the range space of L defined
by R(L) := {Ly | y ∈ Rp(p−1)/2 }. Hence, there must exist a y ∈ Rp(p−1)/2 such that S = Ly. Thus
L∗ S = L∗ Ly. One further obtains
y = (L∗ L)−1 L∗ S.
Then a simple calculation yields
L(L∗ L)−1 L∗ S = Ly = S.

(32)

Next, we construct a matrix X = Lx + aJ with a > 0 and have
Lx + aJ = L(L∗ L)−1 (L∗ S + λ1) + aJ
= S + λL(L∗ L)−1 1 + aJ
λ
= S + L1 + aJ ,
2p

(33)

where the first equality follows from (31); the second equality follows from (32); the third equality holds
1
because L∗ L1 = 2p1 and then one has (L∗ L)−1 1 = 2p
1.
1

1

Let X̂ = D − 2 XD − 2 be the normalized matrix of X, where D is a diagonal matrix containing the
diagonals of X. Notice that each diagonal element of X̂ is 1. Next, we will prove that, under some
conditions, X̂ is an inverse M -matrix, that is, (X̂)−1 is a M -matrix. We say A ∈ Rp×p is an M -matrix if
A = sI − B,

(34)

where B ∈ Rp×p is an element-wise non-negative matrix and s > ρ(B), the spectral radius of B. According
to (33), one has
Xij = Sij −

λ
a
+ ,
2p p

for i 6= j,

and
Xii = Sii +

a
p−1
λ+ ,
2p
p

i = 1, . . . , p.

Define Seij = maxi6=j Sij , S̄ij = mini6=j Sij , Sekk = maxk Skk and S̄kk = mink Skk . By the definition of X̂,
the lower bound y and upper bound x of the elements off the diagonal of X̂ can be obtained as below,
X̂ij = p

≥

Xij
=q
Xii Xjj
Sii +
λ
a
2p + p
p−1
a
2p λ + p

S̄ij −
Sekk +

Sij −
p−1
2p λ

=: y,

21

+

a
p

λ
2p

+a
q p
· Sjj +

∀ i 6= j.

p−1
2p λ

+

a
p

(35)

and
X̂ij ≤

λ
a
2p + p
p−1
a
2p λ + p

Seij −
S̄kk +

∀ i 6= j.

=: x,

(36)

Define s by x2 = sy + (1 − s)y 2 . According to Lemma 5.11, if 0 < y ≤ x < 1 and s−1 ≥ p − 2 with p > 3,
1
then X̂ is an inverse M -matrix. We can see, provided that y = p+1
and the inequalities
√
(37)
0 < y ≤ x ≤ 2y < 1,
hold, then one has
y − y2
1−y
≥
> p − 2,
2
2
x −y
y
√
1
where the first inequality follows from x ≤ 2y. Therefore, if y = p+1
and (37) holds, then X̂ is an inverse
M -matrix.
√
1
Next, we will prove that if a = Sekk − (p + 1)S̄ij + λ with λ ≥ 2( 2 + 1)(p + 1)(Sekk − S̄ij ), then y = p+1
1
and (37) holds. Substituting a = Sekk − (p + 1)S̄ij + λ into (35) yields y = p+1 . Then it is clear that y > 0
√
and 2y < 1. By comparing x and y defined in (35) and (36), respectively, one has y ≤ x. A simple algebra
yields
s−1 =

x=

λ
a
2p + p
p−1
a
2p λ + p

Seij −
S̄kk +

≤

pS̄ij +

(p + 1)(Sekk − S̄ij ) +
=
Sekk − S̄ij + (p+1)λ
2

λ
2 +a
p−1
2 λ+a

pSekk −
λ
2

√

≤

√
2
= 2y,
p+1

p
where the first inequality follows from Seij ≤ Sekk and S̄kk ≥ S̄ij because S ∈ S+
and S · 1 = 0. It is easy to
√
e
verify that λ ≥ 2( 2 + 1)(p + 1)(Skk − S̄ij ) is large enough to establish the second inequality. Therefore, all
the inequalities in (37) hold.
Consequently, by Lemma 5.11, we conclude that X̂ is an inverse M -matrix when a = Sekk − (p + 1)S̄ij + λ
√
1
1
with λ ≥ 2( 2 + 1)(p + 1)(Sekk − S̄ij ). Therefore, X̂ −1 = D 2 X −1 D 2 is an M -matrix. Notice that the
elements off the diagonal of an M -matrix are non-positive according to (34). As a result, the elements off
1
1
the diagonal of D 2 X −1 D 2 are non-positive, implying that the elements off the diagonal of X −1 are also
non-positive, because X = Lx + aJ is positive definite and thus the diagonal elements of D are positive.
The application of (29) with b = a1 yields

1
X −1 = [Lx + aJ ]−1 = Lŵ + J .
a
One further obtains
1
1
[Lŵ + J ]ij = −ŵk +
≤ 0,
a
ap
where k = i − j +

j−1
2 (2p

∀ i 6= j,

− j). Therefore, we establish
ŵk ≥

1
1
=
> 0,
e
ap
(Skk − (p + 1)S̄ij + λ)p

∀ k,

concluding that (28) holds for ŵ > 0. Note that a = Sekk − (p + 1)S̄ij + λ > 0 because Sekk > 0, λ > 0,
and S̄ij ≤ 0, where S̄ij ≤ 0 following from S · 1 = 0 and each diagonal element Sii ≥ 0 since S is positive
semi-definite.

22

5.3

Proofs of Theorem 3.4

Proof. Let x ∈ Rp(p−1)/2 . According to the definition of L in (7), Lx must obey [Lx]ij = [Lx]ji , for any
i 6= j and (Lx) · 1 = 0.
Next, we will show that Lx is positive semi-definite for any
P x ≥ 0 by the Gershgorin circle theorem [62].
Given a matrix X ∈ Rp×p with entries Xij . Let Ri (X) = j6=i |Xij | be the sum of the absolute values of
the non-diagonal entries in the i-th row. Then a Gershgorin disc is the disc D(Xii , Ri (X)) centered at Xii
on the complex plane with radius Ri (X). Gershgorin circle theorem [24] shows that each eigenvalue of X
lies within at least one of the Gershgorin discs. For any x ≥ 0, Ri (Lx) = [Lx]ii holds for each i because
(Lx) · 1 = 0 and [Lx]ij ≤ 0 for any i 6= j. For any given eigenvalue λ of Lx, by Gershgorin circle theorem,
there must exist one Gershgorin disc D([Lx]ii , Ri (Lx)) such that
|λ − [Lx]ii | ≤ Ri (X) = [Lx]ii ,

(38)

indicating that λ ≥ 0. Note that the eigenvalues of Lx are real since Lx is symmetric. Therefore, one has
p
Lx ∈ S+
,

∀x ≥ 0.

(39)

p
Finally, we will prove that rank(Lx) = p − 1 ⇔ (Lx + J ) ∈ S++
, for any x ≥ 0. On one hand, if
rank(Lx) = p − 1, then Lx + J admits the eigenvalue decomposition U ΛU > , where U = [Us √1p 1] and
Λ is a diagonal matrix with the diagonal elements [λ2 , . . . , λp , 1]. Here λpi=2 are the nonzero eigenvalues of
Lx and Us is a p × (p − 1) matrix whose columns are the corresponding eigenvectors of Lx. Note that the
p
p
nonzero eigenvalue λpi=2 > 0 because Lx ∈ S+
. Therefore, one has (Lx + J ) ∈ S++
. On the other hand, if
p
(Lx + J ) ∈ S++ , then rank(Lx) ≥ rank(Lx + J ) − rank(J ) = p − 1 because Lx + J is full rank and J is rank
one. Furthermore, rank(Lx) ≤ p − 1 because (Lx) · 1 = 0. Therefore, we conclude that rank(Lx) = p − 1,
completing the proof.

5.4

Proofs of Theorem 3.8 and Corollary 3.9

q
Proof. We first prove Theorem 3.8. Take the regularization parameter λ = 4αc−1
0 log p/n for some α > 2,
and the sample size

2
?
n ≥ max 94αc−1
(40)
0 λmax (Lw )s log p, 8α log p ,
where c0 is a constant defined in Lemma 5.8. Notice that the sample size n in (40) satisfies the conditions
on the number of samples in Lemmas 5.4, 5.5, 5.6 and 5.7. Recall that the initial point ŵ(0) of Algorithm 1
satisfies |supp+ (ŵ(0) )| ≤s.

−1
−1
Define an event J = L∗ (Lw? + J ) −S max ≤ λ/2 . According to Lemma 5.7, L∗ (Lw? + J ) −

α−2
S max ≤ λ/2 holds with probability at least 1 − 1/p
. Under the event J , one applies Lemma 5.6 and
obtains, for any k ≥ 1,
ŵ(k) − w? ≤ 2λ2max (Lw? )

L∗ (Lw? + J )

−1

−S


S?

+

3
√ ŵ(k−1) − w? ,
2+ 2

By induction, if dk ≤ a0 + ρdk−1 for any k ≥ 1 with ρ ∈ [0, 1), then
1 − ρk
a0 + ρk d0 .
(41)
1−ρ

−1
(Lw? + J ) − S S ? , ρ = 2+3√2 and dk = ŵ(k) − w? , one obtains
dk ≤

Taking a0 = 2λ2max (Lw? )k L∗


√

−1
ŵ(k) − w? ≤ 2(3 2 + 4)λ2max (Lw? )k L∗ (Lw? + J ) − S S ? +

23

3 k (0)
√
ŵ − w? .
2+ 2

−1

L∗ (Lw? + J )

Under the event J ,

−S

L∗ (Lw? + J )

−1



can be bounded by

S?

−S


S?

≤

√

sλ/2 ≤

q
αc−1
0 s log p/n.

(42)

Therefore, under the event J , which holds with probability at least 1 − 1/pα−2 , one has
q
 3 k
√
√
ŵ(k) − w? ≤ 2(3 2 + 4)λ2max (Lw? ) αc−1
ŵ(0) − w? .
s
log
p/n
+
0
2+ 2
Next, we prove Corollary 3.9. Under the event J , one applies Lemma 5.6 and obtains
Lŵ(k) − Lw?

F

√
≤ 2 2λ2max (Lw? )

L∗ (Lw? + J )

√
holds for any k ≥ 1. Taking a0 = 2 2λ2max (Lw? )
Lŵ(k) − Lw?

F

Lŵ(k) − Lw?

−1

L∗

3
√ Lŵ(k−1) − Lw? F
(43)
2+ 2

−1
(Lw? + J ) − S S ? , ρ = 2+3√2 and dk =

−S



+

S?

, by (41) one has

F

√
≤ 4( 2 + 1)2 λ2max (Lw? )

−1

L∗ (Lw? + J )

−S


S?

+



3 k
√
Lŵ(0) − Lw?
2+ 2

F

.

Similarly, according to (42), one obtains,
Lŵ(k) − Lw?

F

q

√
≤ 4(3 + 2 2)λ2max (Lw? ) αc−1
0 s log p/n +

3 k
√
Lŵ(0) − Lw?
2+ 2

F

holds at least 1 − 1/pα−2 .
Alternative to (41), one obtains
dk ≤

1 − ρk−1
a0 + ρk−1 d1 ,
1−ρ

and correspondingly establishes
Lŵ(k) − Lw?

F

q

√
≤ 4(3 + 2 2)λ2max (Lw? ) αc−1
s
log
s/n
+
0

3 k−1
√
Lŵ(1) − Lw?
2+ 2

F

.

(44)
(0)

To apply Lemma 5.4, we first check the necessary conditions of the lemma. Let z (0) satisfy zi =
(0)
0
hλ (ŵi (0) ), i ∈ [p(p − 1)/2]. Notice that zi ∈ [0, λ] for i ∈ [p(p − 1)/2] by Assumption 3.5. According to

c
(0)
(20), E (1) = {S ? ∪ S (1) }, where S (1) = {i ∈ [p(p − 1)/2] |ŵi ≥ b}. For any i ∈ S (1) , one has
(0)

zi

(0)

= h0λ (ŵi ) ≥ h0λ (b) ≥

λ
,
2


c
(0)
where the first inequality holds because ŵi < b for any i ∈ S (1) , and h0λ is non-increasing according to
Assumption 3.5; the second inequality directly follows from Assumption 3.5. Hence one has
(0)

z E (1) c
{ }

(0)

min

≥ z S (1) c
{ }

≥
min

λ
.
2

One also obtains |E (1) | < 2s by Lemma 5.5, and S ? ⊆ E (1) by the definition of E (1) . Therefore, one can apply
Lemma 5.4 with E = E (1) and z = z (0) and obtains
√
√
Lŵ(1) − Lw? F ≤ 2(1 + 2)λ2max (Lw? ) sλ.

24

If t ≥ log 2+√2 (λ

p

n/ log p) = log

3

q

√
2+ 2
3 ,


4αc−1
/ log
0

a simple algebra yields

√
p
√
2(3 2 + 4) p
3 t−1
(1)
?
√
(45)
Lŵ − Lw F ≤
(λ n/ log p)−1 λ2max (Lw? ) sλ . s log p/n.
3
2+ 2
q

√
Taking k ≥ d4 log(4αc−1
)e
≥
log
4αc−1
/ log 2+3 2 , and combining (44) and (45) together, we can
0
0
conclude that
p
Lŵ(k) − Lw? F . s log p/n,


completing the proof.

5.5

Proof of Theorem 3.10

Proof. Take the regularization parameter λ =

q

4αc−1
0 log p/n for some α > 2, and the sample size

n ≥ max 840αc−1
0


(δτ 2 + 1)4
sp log p, 8α log p ,
δ2 τ 2

(46)

where c0 is a constant defined in Lemma 5.8. The sample size n in (46)
the conditions on the number
 satisfies
−1
L∗ (Lw? + J ) − S max ≤ λ/2 .
of samples in Lemmas 5.4, 5.5, 5.6 and 5.7. Define an event J =

−1
According to Lemma 5.7, L∗ (Lw? + J ) − S max ≤ λ/2 holds with probability at least 1 − 1/pα−2 .
Recall that the initial point ŵ(0) of Algorithm 1 satisfies |supp+ (ŵ(0) )| ≤ s.
Define a local region around w?

o
1  n
1 
B w? ; √
= w w ∈ B w? ; √
∩ Sw ,
2pδτ
2pδτ
where τ is defined in (17) and δ > 1, B(w? ; r) = {w ∈ Rp(p−1)/2 | kw − w? k ≤ r} and Sw = {w ∈
p
1
}. Notice that B(w? ; √2pδτ
Rp(p−1)/2 |w ≥ 0, (Lw+J ) ∈ S++
) must be nonempty because w? ∈ Sw according
to Theorem 3.4 together with the fact that w? contains the weights from a connected graph. According to
2
1
) with µ = (1+δ−1
Lemma 5.9, − log det(Lw + J ) is µ-strongly convex and L-smooth in B(w? ; √2pδτ
)2 τ 2 and
2

2pτ
(k−1)
L = (1−δ
, wi as defined in (14) is also
−1 )2 . Therefore, fk (w) = − log det(Lw + J ) + tr (SLw) + hz
1
µ-strongly convex and L-smooth in B(w? ; √2pδτ ), for any k ≥ 1.
We define another local region


 n
o

2
2
B ŵ(k) ; √
= w w ∈ B ŵ(k) ; √
∩ Sw .
3 2pδτ
3 2pδτ
Next, we will prove that under the event J ,



2
1 
B ŵ(k) ; √
⊆ B w? ; √
3 2pδτ
2pδτ
holds for any k ≥ 1, where ŵ(k) is defined in (15). Before applying Lemma 5.4, we first check the necessary
(k−1)
(k−1)
conditions of the lemma. Let z (k−1) satisfy zi
= h0λ (ŵi (k−1) ), i ∈ [p(p − 1)/2]. One has zi
∈ [0, λ]
(k)
?
(k)
for i ∈ [p(p
−
1)/2]
by
Assumption
3.5.
According
to
the
definition
of
E
in
(20),
one
has
S
⊆
E
. For

c
any i ∈ S (k) , one further has
(k−1)

zi

(k−1)

= h0λ (ŵi

25

) ≥ h0λ (b) ≥

λ
,
2


c
(k−1)
where the first inequality holds because ŵi
< b for any i ∈ S (k) , and h0λ is non-increasing by
Assumption 3.5; the second inequality follows from Assumption 3.5. Hence one obtains
(k−1)

z E (k) c
{ }

(k−1)

min

≥ z S (k) c
{
}

≥
min

λ
.
2

Under the event J , |E (k) | ≤ 2s holds for any k ≥ 1 following from Lemma 5.5. Therefore, all the conditions
in Lemma 5.4 are satisfied with E = E (k) and z = z (k−1) , and one has
√
√
√
2
1
(k)
?
Lŵ(k) − Lw? ≤ (2 + 2)λ2max (Lw? ) sλ ≤ √
ŵ − w ≤
,
(47)
2
F
3 2pδτ
q
where the first inequality follows from (91) and the third inequality is established by plugging λ = 4αc−1
0 log p/n
2

4

(δτ +1)
with n ≥ 840αc−1
0
δ 2 τ 2 sp log p. The (47) indicates that

1
)
ŵ(k) ∈ B(w? ; √
3 2pδτ

(48)

2
holds for any k ≥ 1. For any w ∈ B(ŵ(k) ; 3√2pδτ
),

kw − w? k ≤ w − ŵ(k) + w? − ŵ(k) ≤ √

1
,
2pδτ

2
1
indicating that B(ŵ(k) ; 3√2pδτ
) ⊆ B(w? ; √2pδτ
). Consequently, one has




2
1 
B ŵ(k) ; √
⊆ B w? ; √
,
3 2pδτ
2pδτ

∀ k ≥ 1.

(49)

2
), for any k ≥ 1.
Therefore, fk (w) is µ-strongly convex and L-smooth in B(ŵ(k) ; 3√2pδτ
(k)

Then we will establish that the whole sequence {wt }t≥0 } returned from Algorithm 2 is within the region
(k)
(k)
2
) for any k ≥ 2. We first prove that wt ∈ Sw for any k ≥ 2 and t ≥ 0. Any wt returned
B(ŵ(k) ; 3√2pδτ
(k)

from Algorithm 2 must obey wt ≥ 0 because of the projection P+ . Notice that Lw + J must be positive
semi-definite for any w ≥ 0, because Lw is positive semi-definite for any w ≥ 0 according to (39), and J
has an unique nonzero eigenvalue 1 with the eigenvector orthogonal to the row and column spaces of Lw.
(k)
Therefore, any wt ∈
/ Sw will lead Lw + J to be singular and positive semi-definite, and thus the objective
(k)
function fk (wt ) will go to infinity. On the other hand, one has
(k)

(k)

(k)

(k)

fk (wt ) ≤ fk (wt−1 ) + h∇fk (wt−1 ), wt

(k)

− wt−1 i +

1
(k)
(k)
wt − wt−1
2η

2

1 (k)
1
(k)
(k)
(k)
(k)
(k)
(k)
(k)
= fk (wt−1 ) + hwt − wt−1 + η∇fk (wt−1 ), wt − wt−1 i −
wt − wt−1
η
2η
2
1
(k)
(k)
(k)
(k)
≤ fk (wt−1 ) −
wt − wt−1 ≤ fk (wt−1 ),
2η

2

(50)

where the first inequality is established by the backtracking exit inequality in Algorithm 2; the second
inequality is established by
1 (k)
1
(k)
(k)
(k)
(k)
(k)
hwt − (wt−1 − η∇fk (wt−1 )), wt − wt−1 i = hP+ (w̃) − w̃, P+ (w̃) − wt−1 i ≤ 0,
η
η
(k)

(k)

(k)

(k)

(k)

where w̃ = wt−1 − η∇fk (wt−1 ). The equality follows from the updating rule wt = P+ (wt−1 − η∇fk (wt−1 )
(k)
and the inequality holds because of the projection theorem together with the fact that wt−1 ≥ 0. More
26

specifically, let S ⊆ Rn be a closed and convex set. Then PS (x) is the projection of x ∈ Rn on S, if and
only if one can establish
hPS (x) − x, PS (x) − zi ≤ 0, ∀ z ∈ S.
(51)
(k)

The (50) indicates that the objective function fk (wt ) is not increasing with the increase of t, and con(k)
(k)
(k)
(k)
sequently one has fk (wt ) ≤ fk (w0 ). Hence, if fk (w0 ) is upper bounded, then fk (wt ) is also upper
(k)
(k)
bounded and thus wt ∈ Sw . Notice that Algorithm 2 takes the initial point w0 = ŵ(k−1) , which is
the minimizer of the optimization (15). Thus, − log det(Lŵ(k−1) + J ) + tr SLŵ(k−1) is upper bounded.
P 0 (k−1) (k−1)
(k)
)ŵi
is also upper bounded following from Assumption 3.5. Therefore, fk (w0 ) is upper
i hλ (ŵi
bounded for any k ≥ 2. Then, we can conclude that
(k)

wt

∈ Sw ,

(52)

holds for any k ≥ 2 and t ≥ 0.
(k)
2
) for any k ≥ 2. For any
We will prove by induction that the sequence {wt }t≥0 } is in B(ŵ(k) ; 3√2pδτ
given k ≥ 2, when t = 0, one has
2
(k)
w0 − ŵ(k) = ŵ(k−1) − ŵ(k) ≤ ŵ(k−1) − w? + ŵ(k) − w? ≤ √
.
3 2pδτ
(k)

where the second inequality follows from (48). Together with (52), we conclude that w0

2
∈ B(ŵ(k) ; 3√2pδτ
).

(k)

2
) for some t ≥ 1. Then one obtains
Suppose wt−1 ∈ B(ŵ(k) ; 3√2pδτ
(k)

wt

− ŵ(k)

2

(k)

= (wt

(k)

= wt

(k)

(k)

− wt−1 ) − (ŵ(k) − wt−1 )
(k)

2

− wt−1

(k)

+ ŵ(k) − wt−1

2

2

(k)

− 2hwt

(k)

(k)

− wt−1 , ŵ(k) − wt−1 i.

(53)

On the other hand, one has
(k)

(k)

(k)

(k)

fk (wt ) − fk (ŵ(k) ) = fk (wt ) − fk (wt−1 ) + fk (wt−1 ) − fk (ŵ(k) )
2
1
(k)
(k)
(k)
(k)
(k)
≤ h∇fk (wt−1 ), wt − wt−1 i +
wt − wt−1
2η
2
µ
(k)
(k)
(k)
wt−1 − ŵ(k)
+ h∇fk (wt−1 ), wt−1 − ŵ(k) i −
2
2
2
µ
1
(k)
(k)
(k)
(k)
(k)
(k)
= h∇fk (wt−1 ), wt − ŵ i +
wt − wt−1 −
wt−1 − ŵ(k)
2η
2
2
1 (k)
µ
1
(k)
(k)
(k)
(k)
(k)
≤ hwt−1 − wt , wt − ŵ(k) i +
wt − wt−1 −
wt−1 − ŵ(k)
η
2η
2
2
1 (k)
1
µ
(k)
(k)
(k)
(k)
(k)
= hwt−1 − wt , wt−1 − ŵ(k) i −
wt − wt−1 −
wt−1 − ŵ(k)
η
2η
2

2

2

,

(54)

where the first inequality follows from the backtracking exit inequality, and fk (w) is µ-strongly convex in
(k)
(k)
2
2
the region B(ŵ(k) ; 3√2pδτ
) and wt−1 ∈ B(ŵ(k) ; 3√2pδτ
). The last equality is obtained by plugging η1 hwt−1 −
(k)

(k)

wt , wt

(k)

(k)

(k)

− ŵ(k) i = η1 hwt−1 − wt , wt−1 − ŵ(k) i −

1
η

(k)

wt

(k)

− wt−1

2

. The second inequality follows from

1 (k)
1 (k)
(k)
(k)
(k)
(k)
(k)
(k)
h∇fk (wt−1 ) − (wt−1 − wt ), wt − ŵ(k) i = hwt − (wt−1 − η∇fk (wt−1 )), wt − ŵ(k) i
η
η
1
= hP+ (w̃) − w̃, P+ (w̃) − ŵ(k) i
η
≤ 0,
27

(k)

(k)

(k)

where w̃ = wt−1 − η∇fk (wt−1 ) and wt = P+ (w̃). The second equality follows from the updating rule
(k)
(k)
(k)
wt = P+ wt−1 − η∇fk (wt−1 ) and the inequality is established by the projection theorem as shown in (51),
(k)
together with ŵ(k) ≥ 0. Substituting fk (wt ) − fk (ŵ(k) ) ≥ 0 into (54), one obtains
(k)

(k)

(k)

(k)

2hwt−1 − wt , wt−1 − ŵ(k) i − wt

(k)

2

− wt−1

(k)

− µη wt−1 − ŵ(k)

2

≥ 0.

(55)

Substituting (55) into (53) yields
(k)

wt

− ŵ(k)

2

(k)

≤ ŵ(k) − wt−1

2

(k)

= (1 − µη) wt−1 − ŵ(k)
(k)

indicating that wt
(k)

(k)

− µη wt−1 − ŵ(k)
2

2

(k)

< wt−1 − ŵ(k) ,
(k)

2
is within the region B(ŵ(k) ; 3√2pδτ
). Together with (52), we conclude that wt
(k)
{wt }t≥0

√2
),
3 2pδτ

(k)

∈

√2
),
3 2pδτ

completing the induction. Therefore, the whole sequence
is in B(ŵ ;
B(ŵ ;
for any k ≥ 2.
Finally, we will establish the lower bound of the step size η for each iteration. Since fk (w) is L-smooth
2
2
in B(ŵ(k) ; 3√2pδτ
), for any w1 and w2 in B(ŵ(k) ; 3√2pδτ
), one has
fk (w2 ) ≤ fk (w1 ) + h∇fk (w1 ), w2 − w1 i +

L
2
kw1 − w2 k .
2

Thus any η ∈ [0, L1 ] must satisfy the backtracking exit condition and the backtracking line search must
β
β
with β ∈ (0, 1), implying that η ≥ L
. Therefore, we establish
terminate when η ≤ L
(k)

wt
where ρ = 1 −

6

β(1−δ −1 )2
pτ 4 (1+δ −1 )2

− ŵ(k)

2

≤ 1−

βµ t
(k)
w0 − ŵ(k)
L

2

(k)

= ρt w0 − ŵ(k)

2

,

< 1, completing the proof.

Proof of Technical Lemmas

This section contains the proofs of the technical lemmas used in Section 5.

6.1

Proof of Lemma 5.1

Proof. The gradient of f (w) is ∇f (w) = −L∗ (Lw + J )−1 and its Hessian matrix is ∇2 f (w) with the k-th
column being


 2

∂(Lw + J )−1
∂(∇f (w))
∗
= −L
∇ f (w) :,k =
∂wk
∂wk


∗
−1 ∂(Lw + J )
−1
(Lw + J )
= L (Lw + J )
∂wk

= L∗ (Lw + J )−1 Ak (Lw + J )−1 ,
where Ak ∈ Rp×p is a matrix with [Ak ]ii = [Ak ]jj = 1, [Ak ]ij = [Ak ]ji = −1 and zeros for the other
2
elements, in which i, j ∈ Z+ obeying k = i − j + j−1
2 (2p − j) and i > j. Therefore, ∇ f (w) can be written as
∇2 f (w) = [L∗ B1 , L∗ B2 , . . . , L∗ Bp(p−1)/2 ],

28

where Bk = (Lw + J )−1 Ak (Lw + J )−1 , for k = 1, 2, . . . , p(p − 1)/2. Then one has
x> ∇2 f (w)x = x> [L∗ B1 , L∗ B2 , . . . , L∗ Bp(p−1)/2 ]x


p(p−1)/2
X
= x> L∗ 
xk Bk 
k=1





= x> L∗ (Lw + J )−1 



p(p−1)/2

X



xk Ak  (Lw + J )−1 

k=1
>

=x L

∗

(Lw + J )

−1

Lx(Lw + J )−1



= Lx, (Lw + J )−1 Lx(Lw + J )−1

= vec(Lx)> vec (Lw + J )−1 Lx(Lw + J )−1


−1
−1
= vec(Lx)> (Lw + J ) ⊗ (Lw + J )
vec(Lx),
where the forth equality follows from the definition of
 L in (7), and the last equality follows from the property
of Kronecker product that vec(ABC) = C > ⊗ A vec(B).

6.2

Proof of Lemma 5.2

p
Proof. Let X = Lw + bJ with any b 6= 0 and any given w ∈ Rp(p−1)/2 obeying (Lw + J ) ∈ S++
. It is easy
to verify that the column spaces as well as row spaces of Lw and J are orthogonal with each other. Hence
X admits the eigenvalue decomposition



 Λ 0 
>
U u
X = Lw + bJ = U u
,
(56)
0 b

where Lw = U ΛU > and bJ = buu> with u = √1p 1p , in which 1p ∈ Rp with each element equal to 1.
Notice that Λ is non-singular. X −1 admits the eigenvalue decomposition



 Λ−1 0 
>
1
−1
U u
X = U u
= U Λ−1 U > + J ,
1
0
b
b
It is easy to check that U Λ−1 U > is symmetric and U Λ−1 U > · 1p = 0. Therefore, there must exist a x
such that Lx = U Λ−1 U > . One further obtains xk = [U Λ−1 U > ]ij , for k ∈ [p(p − 1)/2], where i, j ∈ Z+
obeying k = i − j + j−1
2 (2p − j) and i > j. Hence such x is fixed and unique for a given w. Note that x is
−1
independent of b, and thus Lx + 1b J = (Lw + bJ ) holds for any b 6= 0, completing the proof.

6.3

Proof of Lemma 5.3

Proof. Define an index set Ωt by
(
Ωt :=

)

l ∈ [p(p − 1)/2] |[Lx]tt =

X

xl

,

t ∈ [p].

(57)

l

According to the definition of L in (7), for any x ∈ Rp(p−1)/2 ,


−xk
[Lx]ij =
[Lx]ji

P
l∈Ωi xl
29

one obtains that Lx ∈ Rp×p obeys
i > j,
i < j,
i = j,

(58)

∗
∗
p(p−1)/2
where k = i − j + j−1
2 (2p − j). By the definition of L in (8), one further obtains that L Lx ∈ R
satisfies
X
X
[L∗ Lx]k = [Lx]ii + [Lx]jj + 2xk =
xl + 2xk =
xl + 4xk ,
(59)
l∈Ωi ∪Ωj

where i, j ∈ [p] satisfying k = i − j +
X

j−1
2 (2p

X

xl = [Lx]ii + [Lx]jj = −

l∈Ωi ∪Ωj

l∈Ωi ∪Ωj \k

− j) and i > j. The last equality holds because
X

[Lx]im −

m6=i

X

[Lx]jm = 2xk −

m6=j

X

[Lx]im −

m6=i,j

[Lx]jm ,

m6=i,j

where the last equality follows from [Lx]ij = −xk according to (58).
p(p−1)
p(p−1)
According to (59), we conclude that there exists a matrix M ∈ R 2 × 2 such that L∗ Lx = M x
for any x ∈ Rp(p−1)/2 , and M obeys


l = k,
4
Mkl = 1
(60)
l ∈ (Ωi ∪ Ωj ) \k,


0
Otherwise,
where i, j ∈ [p] satisfying k = i−j+ j−1
2 (2p−j) and i > j. Note that we use the fact that {Ωi \k}∩{Ωj \k} = ∅
(2p
−
j).
with k = i − j + j−1
2
Finally, we will compute the minimum and maximum eigenvalues of M . To compute the minimum
eigenvalue of M , one has
λmin (M ) = inf

x6=0

x> M x
2

kxk

2

= inf

kLxkF
2

kxk

x6=0

= inf

2

Pp(p−1)/2
k=1

x2k +
kxk

x6=0

Pp

i=1 ([Lx]ii )

2

2

≥ 2.
p(p−1)

with equality when [Lx]11 = . . . = [Lx]pp = 0, which can be written as Qx = 0 with Q ∈ Rp× 2 .
Obviously, there must exist a nonzero solution to Qx = 0, and thus λmin (M ) = 2. To compute the
maximum eigenvalue of M , one has
λmax (M ) = sup
x6=0

= sup

x> M x
2

= sup

2

Pp(p−1)/2

2

4

Pp(p−1)/2
k=1

x2k +

kxk
Pp
1

t=1

2

kxk
4

Pp(p−1)/2

x2k +

4

Pp(p−1)/2

x2k

k=1

2
i,j∈Ωt , i6=j (xi

P

+ x2j )

2

Pp

t=1 (|Ωt |
2

− 1)

P

i∈Ωt

x2i

kxk

x6=0

= sup

2
t=1 ([Lx]tt )

kxk
kxk
x6=0
Pp(p−1)/2 2 Pp P
4 k=1
xk + t=1 i,j∈Ωt , i6=j xi xj

x6=0

= sup

Pp

x2k +

2

x6=0

≤ sup

k=1

k=1

+ (p − 2)
kxk

x6=0

Pp

t=1

P

i∈Ωt

x2i

2

= 2p,
with equality when each element of x is equal with each other, and thus λmax (M ) = 2p. The last second
equality is obtained by plugging |Ωt | = p − 1 with t ∈ [p], which is easy to verify according to the definition of
Pp P
Pp(p−1)/2 2
Ωt in (57); the last equality follows from t=1 i∈Ωt x2i = 2 k=1
xk , because for any k ∈ [p(p − 1)/2],
k ∈ Ωt only holds with t = {i, j}, where i, j ∈ [p] obeying k = i − j + j−1
2 (2p − j) and i > j.

30

6.4

Proof of Lemma 5.4

Proof. Take λ =

q

−1 2
?
4αc−1
0 log p/n and n ≥ 94αc0 λmax (Lw )s log p. Define a local region

BM (w? ; λmax (Lw? )) = {w |w ∈ BM (w? ; λmax (Lw? )) ∩ Sw },
2

2

where BM (w? ; r) = {w ∈ Rp(p−1)/2 | kw − w? kM ≤ r}, in which kxkM = hx, M xi = kLxkF with M  0
p
defined in Lemma 5.3, and Sw = {w ∈ Rp(p−1)/2 |w ≥ 0, (Lw + J ) ∈ S++
}. It is easy to check that
w? ∈ BM (w? ; λmax (Lw? )).
Recall that ŵ minimizes the optimization
min − log det(Lw + J ) + tr (LwS) + z > w,

(61)

w≥0

where 0 ≤ zi ≤ λ for i ∈ [p(p − 1)/2]. We can see the optimization problems (10) and (61) have the same
feasible set. Therefore, Sw is also the feasible set of (61) and thus ŵ ∈ Sw must hold.
Next, we will prove that ŵ ∈ BM (w? ; λmax (Lw? )). We first construct an intermediate estimator,
wt = w? + t(ŵ − w? ),
?

?

(62)
?

?

where t is taken such that kwt − w kM = λmax (Lw ) if kŵ − w kM ≥ λmax (Lw ), and t = 1 otherwise.
Hence kwt − w? kM ≤ λmax (Lw? ) always holds and t ∈ [0, 1]. One further has wt ∈ Sw because both
w? , ŵ ∈ Sw and Sw is a convex set as shown in (98). Therefore, we conclude that wt ∈ BM (w? ; λmax (Lw? )).
Applying Lemma 5.10 with w1 = wt , w2 = w∗ and r = λmax (Lw? ) yields
−1

−1

th−L∗ (Lwt + J )

−2

2

+ L∗ (Lw? + J ) , ŵ − w∗ i ≥ (2λmax (Lw? )) kLwt − Lw∗ kF .
(63)


Let q(a) = − log det L w? + a(ŵ − w? ) + J + ahL∗ (Lw? + J )−1 , ŵ − w? i and a ∈ [0, 1]. One has
−1

q 0 (a) = h−L∗ (Lwa + J )

+ L∗ (Lw? + J )

−1

, ŵ − w? i,

(64)

and
D 

E
−1
−1
q 00 (a) = L∗ (Lwa + J ) (Lŵ − Lw? ) (Lwa + J )
, ŵ − w? = tr (ABAB) ,
where wa = w? + a(ŵ − w? ), A = (Lwa + J )−1 and B = (Lŵ − Lw? ). Note that A is symmetric and
positive definite because wt ∈ Sw and B is symmetric. Let C = AB. According to Theorem 1 in Drazin and
Haynsworth [15], all the eigenvalues of a matrix X ∈ Rp×p are real if there exists a symmetric and positive
definite matrix Y ∈ Rp×p such that XY are symmetric. It is easy to check that the matrix CA is symmetric
with A symmetric and positive definite, and thus all the eigenvalues of C are real. Suppose
λ1 , . . . , λp are
Pp
the eigenvalues of C. Then the eigenvalues of CC are λ21 , . . . , λ2p . Therefore, q 00 (a) = i=1 λ2i ≥ 0, implying
that q 0 (a) is non-decreasing with the increase of a. Then one obtains
thL∗ (Lw? + J )

−1

−1

− L∗ (Lŵ + J )

, ŵ − w? i = tq 0 (1) ≥ tq 0 (t)
2

≥ (2λmax (Lw? ))−2 kLwt − Lw∗ kF .

(65)

where the first inequality holds because q 0 (a) is non-decreasing and t ≤ 1, and the second inequality follows
from (63).
The Lagrangian of the optimization (61) is
L(w, ν) = − log det(Lw + J ) + tr (LwS) + z > w − υ > w,
where υ is a KKT multiplier. Let (ŵ, υ̂) be the primal and dual optimal point. Then (ŵ, υ̂) must satisfy
the KKT conditions as below

−L∗ (Lŵ + J )−1 + L∗ S + z − υ̂ = 0;
(66)
ŵi υ̂i = 0, for i = 1, . . . , p(p − 1)/2;

(67)

ŵ ≥ 0, υ̂ ≥ 0;

(68)

31

According to (66), one has
− L∗ Lŵ + J

−1

+ L∗ S, ŵ − w? = hυ̂ − z, ŵ − w? i .

(69)

Substituting (69) into (65) yields



2
−1
kLwt − Lw? kF ≤ 4tλ2max (Lw? ) hυ̂ − z, ŵ − w? i + L∗ (Lw? + J ) − S , ŵ − w?

= 4tλ2max (Lw? ) hυ̂, ŵ − w? i − hz, ŵ − w? i
|
{z
} |
{z
}
term I
term II


−1
+ L∗ (Lw? + J ) − S , ŵ − w? .
|
{z
}

(70)

term III

Next we will bound term I, II and III, respectively. The term I can be directly bounded by
hυ̂, ŵ − w? i = −hυ̂, w? i ≤ 0.

(71)

where the equality follows from (67) and the inequality follows from υ̂ ≥ 0 in (68) and w? ≥ 0.
For term II, we separate the support of z into two parts, S ? and its complementary set {S ? }c , where S ?
is the support of w? with |S ? | ≤ s. Take a set E satisfying S ? ⊆ E and |E| ≤ 2s. A simple algebra yields
hz, ŵ − w? i = hzS ? , (ŵ − w? )S ? i + z{S ? }c , (ŵ − w? ){S ? }c
= hzS ? , (ŵ − w? )S ? i + z{S ? }c , ŵ{S ? }c
≥ − kzS ? k k(ŵ − w? )S ? k + z{S ? }c , ŵ{S ? }c
≥ − kzS ? k k(ŵ − w? )S ? k + hzE c , ŵE c i ,

(72)

where the first inequality follows from CauchySchwarz inequality and the second inequality follows from
z ≥ 0, ŵ ≥ 0 and E c ⊆ {S ? }c .

For term III, we separate the support of L∗ (Lw? + J )−1 − S into parts, E and E c . Then one has

L∗ (Lw? + J )−1 − S , ŵ − w? =

≤

−1

L∗ (Lw? + J )

, (ŵ − w? )E

+ L∗ (Lw? + J ) − S E c , (ŵ − w? )E c

L∗ (Lw? + J )−1 − S E (ŵ − w? )E

+ L∗ (Lw? + J )−1 − S E c , ŵE c .
−S



E
−1

Substituting (71), (72) and (73) into (70) yields


2
kLwt − Lw? kF ≤ 4tλ2max (Lw? )
L∗ (Lw? + J )−1 − S E (ŵ − w? )E


+ L∗ (Lw? + J )−1 − S E c − zE c , ŵE c + kzS ? k k(ŵ − w? )S ? k .

(73)

(74)

Notice that the inequality
L∗ (Lw? + J )−1 − S


Ec

− zE c , ŵE c ≤ 0

(75)

holds because ŵ ≥ 0, kzE c kmin ≥ λ/2 and
L∗ (Lw? + J )−1 − S


E c max

≤ L∗ (Lw? + J )−1 − S

32


max

≤

λ
,
2

where the last inequality follows from the conditions in Lemma 5.4. Combining (74) and (75) together yields



2
−1
kLwt − Lw? kF ≤ 4tλ2max (Lw? ) kzS ? k k(ŵ − w? )S ? k + L∗ (Lw? + J ) − S E k(ŵ − w? )E k

 
−1
≤ 4tλ2max (Lw? ) kzS ? k + L∗ (Lw? + J ) − S E kŵ − w? k ,
(76)
where the last inequality follows from kŵ − w? k ≥ k(ŵ − w? )E k ≥ k(ŵ − w? )S ? k.
On the other hand, one has
X
√
2  12
= 2t kŵ − w? k .
kLwt − Lw? kF = t kLŵ − Lw? kF ≥ t
[Lŵ − Lw? ]ij

(77)

i6=j

Combining (76) and (77) together yields

√
kLwt − Lw? kF ≤ 2 2λ2max (Lw? ) kzS ? k +
Recall that kzkmax ≤ λ and |S ? | ≤ s. Thus one has
kzS ? k ≤

√

−1

L∗ (Lw? + J )

−S

L

−1

?

(Lw + J )

−S




E

≤ |E| L

∗

E

.

sλ.

?

(78)

(79)

One also has
∗





−1

(Lw + J )

−S



2
max

 12

√
≤

Substituting (79) and (80) into (78) yields
√
√
kLwt − Lw? kF ≤ 2( 2 + 1)λ2max (Lw? ) sλ < λmax (Lw? ),

2√
sλ.
2

(80)

(81)

which implies that t = 1 in (62), i.e., wt = ŵ. The last inequality is established by plugging λ =
q
−1 2
?
4αc−1
0 log p/n with n ≥ 94αc0 λmax (Lw )s log p. Therefore, we conclude that

√
 
−1
kLŵ − Lw? kF ≤ 2 2λ2max (Lw? ) kzS ? k + L∗ (Lw? + J ) − S E
√
√
≤ 2(1 + 2)λ2max (Lw? ) sλ,
where the first inequality is established by (78) with t = 1, and the second inequality is established by
plugging (79) and (80).

6.5

Proof of Lemma 5.5

√
(k−1)
Proof. Recall that E (k) = {S ? ∪S (k) } and S (k) = {i ∈ [p(p−1)/2] |ŵi
≥ b} with b = (2+ 2)λ2max (Lw? )λ.
(0)
(0)
We prove |E (k) | ≤ 2s holds by induction. For k = 1, ∀i ∈
/ supp+ (ŵ(0) ), i.e., wi ≤ 0, one has wi < b,
implying that i ∈
/ S (1) . In other words, S (1) ⊆ supp+ (ŵ(0) ). Then one has
|E (1) | = |S ? ∪ S (1) | ≤ |S ? ∪ supp+ (ŵ(0) )| ≤ s + s = 2s.
Therefore, |E (k) | ≤ 2s holds for k = 1.
Assume |E (k−1) | ≤ 2s holds for some k ≥ 2. We separate the set E (k) into two parts, S ? and S (k) \S ? .
(k−1)
For any i ∈ S (k) \S ? , one has ŵi
≥ b, and further obtains
v

(k−1)
u X  (k−1) 2
q
ŵ(k−1) − w? S (k) \S ?
ŵS (k) \S ?
u
ŵ
i
|S (k) \S ? | ≤ t
=
=
b
b
b
(k)
?
i∈S

≤

\S

(k−1)
ŵE (k)

b

− w?

≤

ŵ(k−1) − w?
.
b
33

(82)

(k−2)

(k−2)

Let z (k−2) satisfy zi
= h0λ (ŵi (k−2) ), i ∈ [p(p − 1)/2]. By Assumption 3.5, one has zi

c
i ∈ [p(p − 1)/2]. For any i ∈ S (k−1) , one further has
(k−2)

zi

(k−2)

= h0λ (ŵi

) ≥ h0λ (b) ≥

λ
,
2

∈ [0, λ] for

(83)


c
(k−2)
where the first inequality holds because ŵi
< b for any i ∈ S (k−1) by the definition of S (k−1) , and
h0λ is non-increasing by Assumption 3.5; the second inequality follows from Assumption 3.5. Therefore, one
obtains
λ
(k−2)
(k−2)
≥ ,
≥ z S (k−1) c
c
{
{E (k−1) } min
} min
2
 (k−1) c  (k−1) c
where the first inequality follows from E
⊆ S
and the second inequality follows from (83).
One also has |E (k−1) | ≤ 2s and S ? ⊆ E (k−1) . Hence we can apply Lemma 5.4 with E = E (k−1) and z = z (k−2)
and obtain
√
√
√
2
(k−1)
?
Lŵ(k−1) − Lw? F ≤ (2 + 2)λ2max (Lw? ) sλ,
(84)
ŵ
−w ≤
2
z

where the first inequality holds with the proof similar to (91). Combining (82) and (84) together yields
√
√
q
(2 + 2)λ2max (Lw? ) sλ √
(k)
?
|S \S | ≤
= s,
b
√
where the last equality follows from b = (2 + 2)λ2max (Lw? )λ. Therefore, one gets
|E (k) | = |S ? ∪ S (k) \S ? | = |S ? | + |S (k) \S ? | ≤ s + s = 2s,
completing the induction.

6.6

Proof of Lemma 5.6

Proof. For any k ≥ 1, one has |E (k) | ≤ 2s by Lemma 5.5. According to the definition of E (k) in (20), one
(k−1)
(k−1)
has S ? ⊆ E (k) . Let zi
= h0λ (ŵi (k−1) ), i ∈ [p(p − 1)/2]. By Assumption 3.5, one has zi
∈ [0, λ] for
 (k) c
i ∈ [p(p − 1)/2]. For any i ∈ S
, one has
(k−1)

zi

(k−1)

= h0λ (ŵi

) ≥ h0λ (b) ≥

λ
,
2


c
(k−1)
where the first inequality holds because ŵi
< b for any i ∈ S (k) by the definition of S (k) in (20),
and h0λ is non-increasing by Assumption 3.5; the second inequality follows from Assumption 3.5. Therefore,
(k−1)

z E (k) c
{ }

(k−1)

min

≥ z S (k) c
{
}

≥ λ/2. Applying Lemma 5.4 with E = E (k) and z = z (k−1) yields
min

Lŵ(k) − Lw?

F


√
(k−1)
≤ 2 2λ2max (Lw? ) zS ?
+

L∗ (Lw? + J )

−1

−S


E (k)


.

(85)

(k−1)

We will show that the term zS ?
in (85) can be bounded in terms of ŵ(k−1) − w? . For any given
w ∈ Rp(p−1)/2 , if |wi? − wi | ≥ b, then one has
0 ≤ h0λ (wi ) ≤ λ ≤ λb−1 |wi? − wi |,
√
where b = (2 + 2)λ2max (Lw? )λ, and the first two inequalities follows from Assumption 3.5. Otherwise, one
has wi? − wi ≤ |wi? − wi | ≤ b, then 0 ≤ h0λ (wi ) ≤ h0λ (wi? − b) because h0λ is non-increasing. Totally, one has
h0λ (wi ) ≤ λb−1 |wi? − wi | + h0λ (wi? − b),
34

∀i ∈ [p(p − 1)/2].

(86)

Collecting the indices i ∈ S ? together and applying (86) with w = ŵ(k−1) yields
(k−1)

(k−1)

= h0λ (ŵS ?

zS ?

≤
(k−1)

where h0λ (ŵS ?
together yields

(k−1)

) = (h0λ (ŵi

Lŵ(k) − Lw?

F

) ≤

λ
(k−1)
ŵS ? − wS? ? k + h0λ (wS? ? − b)
b

λ
ŵ(k−1) − w? + h0λ (wS? ? − b) ,
b

(87)

))i∈S ? , and b = [b, . . . , b]> is a constant vector. Combining (85) and (87)


√
≤ 2 2λ2max (Lw? )

−1

L∗ (Lw? + J )

−S

+ h0λ (wS? ? − b)


E (k)



√ λ
+ 2 2 λ2max (Lw? ) ŵ(k−1) − w? .
b

(88)

By separating the set E (k) into two parts, S ? and S (k) \S ? , one has



−1
−1
−1
L∗ (Lw? + J ) − S E (k) ≤ L∗ (Lw? + J ) − S S ? + L∗ (Lw? + J ) − S S (k) \S ? .

−1
We will show that the term L∗ (Lw? + J ) − S S (k) \S ? can be bounded in terms of ŵ(k−1) − w? .
q


−1
−1
L∗ (Lw? + J ) − S S (k) \S ? ≤ |E (k) \S ? | L∗ (Lw? + J ) − S S (k) \S ? max
q

−1
≤ |E (k) \S ? | L∗ (Lw? + J ) − S max

1
−1
≤
ŵ(k−1) − w? L∗ (Lw? + J ) − S max
b
λ
ŵ(k−1) − w? ,
≤
2b
where the last second equality follows from (82). Thus one has
L∗ (Lw? + J )

−1

−S


E (k)

≤

L∗ (Lw? + J )

−1

−S


S?

+

λ
ŵ(k−1) − w? .
2b

(89)

Substituting (89) into (88) yields
√


−1
Lŵ(k) − Lw? F ≤ 2 2λ2max (Lw? ) L∗ (Lw? + J ) − S S ? + h0λ (wS? ? − b)
√ λ
+ 3 2 λ2max (Lw? ) ŵ(k−1) − w?
b
√
√ 2

3 2
−1
∗
?
?
√ ŵ(k−1) − w?
= 2 2λmax (Lw ) L (Lw + J ) − S S ? +
2+ 2
√

3
−1
√ Lŵ(k−1) − Lw? F ,
≤ 2 2λ2max (Lw? ) L∗ (Lw? + J ) − S S ? +
(90)
2+ 2
√
where the equality is established by plugging b = (2+ 2)λ2max (Lw? )λ and following from h0λ (wS? ? −b) = 0
because kwS? ? kmin − b ≥ γλ and h0λ (x) = 0 for any x ≥ γλ following from Assumption 3.6; the last inequality
follows from

 12
p(p−1)/2
p
X
X
√
(k)
Lŵ(k) − Lw? = 2
(ŵi − wi? )2 +
([Lŵ(k) − Lw? ]jj )2  ≥ 2 ŵ(k) − w? .
(91)
F

i=1

Similarly, one also obtains
√
2
(k)
?
ŵ − w ≤
Lŵ(k) − Lw?
2

j=1

F

≤ 2λ2max (Lw? )

−1

L∗ (Lw? + J )

35

−S


S?

+

3
√ ŵ(k−1) − w? .
2+ 2

6.7

Proof of Lemma 5.7

Proof. We apply Lemma 5.8 with t = λ/2 and union sum bound, then get

1
1
≥ λ/2 ≤ p(p − 1) exp(− c0 nλ2 ) ≤ p2 exp(− c0 nλ2 ),
4
4

2
L∗ (Lw? + J )−1 max and c0 = 1/ 8 L∗ (Lw? + J )−1 max . Take λ =



−1
P L∗ (Lw? + J ) − S

max

for any λ ≤ 2t0 , where t0 =
q
4αc−1
0 log p/n for some α > 2. To guarantee λ ≤ 2t0 , one takes n ≥ 8α log p. By calculation, we establish


−1
P L∗ (Lw? + J ) − S

max


1
≤ λ/2 ≥ 1 − p2 exp(− c0 nλ2 ) ≥ 1 − 1/pα−2 ,
4

completing the proof.

6.8

Proof of Lemma 5.8

Proof. The L-GMRF is a constrained GMRF model with 1> x = 0 and we can follow the method called
conditioning by Kriging [52] to sample L-GMRF. More specifically, to sample x for L-GMRF with precision
matrix Lw? ,we could first sample from a unconstrained GMRF x̃ ∼ N (0, (Lw? + J )−1 ) and then correct
for the constraint 1> x = 0 by
1
x(k) = x̃(k) − 11> x̃(k) ,
p

for k = 1, . . . , n.

For any i ∈ [p(p − 1)/2], one has
n
n
n
h 1 X
> i
1 X h ∗  (k) (k) > i
1 X (k)
(k) 2
[L∗ S]i = L∗
x(k) x(k)
xa − xb
=
L x
x
=
n
n
n
i
i
k=1
k=1
k=1


n
2




X
1
1 > (k)
1
(k)
=
x̃(k)
)a − x̃b − ( 11> x̃(k) )b
a − ( 11 x̃
n
p
p

=

1
n

k=1
n
X

(k) 2

x̃(k)
a − x̃b

,

(92)

k=1

where the indices a and b obey i = a − b + (b − 1)(2p − b)/2 and a > b.
Let Σ̃ = (Lw? + J )−1 and thus x̃ ∼ N (0, Σ̃). We first introduce two auxiliary random variables
(k)
(k)
2
. Together with (92), one has
Yk,i := x̃a − x̃b and Zk,i := Yk,i
n

1X
Zk,i = [L∗ S]i .
n

(93)

k=1

We can see Yk,i ∼ N (0, σi2 ) because of the fact that any linear combination of p components in x̃ has a
univariate normal distribution. The variance of Yk,i is



(k) 2 
σi2 = E (Yk,i − E(Yk,i ))2 = E (x̃(k)
a − x̃b )

= Σ̃aa + Σ̃bb − Σ̃ab − Σ̃ba = L∗ Σ̃ i .

(94)

Therefore Zk,i /σi2 ∼ χ2 (1) and E[Zk,i /σi2 ] = 1. We say a random variable X is sub-exponential if there are
non-negative parameters (υ, α) such that


υ 2 λ2
1
E exp λ(X − E[X]) ≤ exp(
), for all |λ| < .
2
α
36

(95)

By checking the condition in (95), one can conclude that Zk,i /σi2 is is sub-exponential with parameters (2, 4).
Furthermore,
if random variables {Yk }nk=1 are independent and sub-exponential with parameters (υk , αk ),
Pn
then k=1 Yk is still sub-exponential with parameters (υ∗ , α∗ ) where
v
u n
uX
υ∗ := t
υk2
and
α∗ := max αk .
k=1,...,n

k=1

Pn
√
Thus, k=1 Zk,i /σi2 is sub-exponential with parameters (2 n, 4). The application of the sub-exponential
tail bound in Lemma 5.12 yields
P

n
h X
k=1

i
 t2 
Zk,i /σi2 − n ≥ t0 ≤ 2 exp − 0 ,
8n

for t0 ∈ [0, n].

By taking t0 = nt/ maxi σi2 , one has
n
n
h 1X
i
h X
nt i
P
Zk,i /σi2 − n ≥ 2
Zk,i − σi2 ≥ t = P
n
σi
k=1

k=1

n
h X
Zk,i /σi2 − n ≥
≤P
k=1


≤ 2 exp −

nt2

nt i
maxi σi2


(96)

2

8 (maxi σi2 )


holds for t ∈ [0, maxi σi2 ]. Notice that σi2 = L∗ (Lw? + J )−1 i according to (94). Substituting (93) into (96)
yields



P [L∗ S]i − L∗ (Lw? + J )−1 i ≥ t ≤ 2 exp(−c0 nt2 ), for t ∈ [0, t0 ].
(97)
where t0 = L∗ (Lw? + J )−1

6.9

max

and c0 = 1/ 8 L∗ (Lw? + J )−1

2
max



, completing the proof.

Proof of Lemma 5.9

p
Proof. Recall that B(w? ; r) = {w ∈ Rp(p−1)/2 | kw − w? k ≤ r}, and Sw = {w |w ≥ 0, (Lw + J ) ∈ S++
}. It
?
is easy to verify that the set B(w ; r) is convex. For any x1 , x2 ∈ Sw , define xt = tx1 + (1 − t)x2 , t ∈ [0, 1].
p
It is clear that xt ≥ 0. Since S++
is a convex cone, one has
p
Lxt + J = t(Lx1 + J ) + (1 − t)(Lx2 + J ) ∈ S++
,

(98)

indicating that xt ∈ Sw and thus the set Sw is convex. Hence B(w? ; r) is a convex set. For any w1 ,
1
w2 ∈ B(w? ; √2pδτ
) with δ > 1, by Mean Value Theorem, we have
1
f (w2 ) = f (w1 ) + h∇f (w1 ), w2 − w1 i + hw2 − w1 , ∇2 f (wt )(w2 − w1 )i,
2

(99)

where wt = tw2 + (1 − t)w1 with t ∈ [0, 1]. Then one obtains

λmin ∇2 f (wt ) = inf x> ∇2 f (wt )x
kxk=1


>
−1
−1
vec(Lx)
= inf (vec(Lx)) (Lwt + J ) ⊗ (Lwt + J )
kxk=1

≥ inf

kxk=1

(vec(Lx))

>


(Lwt + J )−1 ⊗ (Lwt + J )−1 vec(Lx)
>

(vec(Lx)) vec(Lx)
37

2

· inf kLxkF
kxk=1

(vec(Lx))

= inf

>



(Lwt )† + J ⊗ (Lwt )† + J vec(Lx)

= 2 inf

(vec(Lx))

>

kxk=1

(vec(Lx)) vec(Lx)

(Lwt )† ⊗ (Lwt )† vec(Lx)
>

kxk=1

≥ 2λ22

· inf x> M x

>

kxk=1

(vec(Lx)) vec(Lx)


(Lwt )† ,

(100)


†

where λ2 (Lwt ) denotes the second smallest eigenvalue of (Lwt )† and (Lwt )† is the pseudo inverse of Lwt .
The second equality is according to Lemma 5.1; the third equality is established by (Lwt +J )−1 = (Lwt )† +J
because the row spaces as well as column spaces of Lwt and J are orthogonal with each other, and M is
>

defined in Lemma 5.3 with λmin (M ) = 2; the last equality follows from vec(Lx)
(Lwt )† ⊗J vec(Lx) = 0,
>


>
vec(Lx)
J ⊗ (Lwt )† vec(Lx) = 0, and vec(Lx) (J ⊗ J )vec(Lx) = 0 which are easy to verify; the last
inequality holds because of the property of Kronecker product that the eigenvalues of A⊗B are λi µj with the
corresponding eigenvector ai ⊗ bj , where λ1 , . . . , λp are the eigenvalues of A ∈ Rp×p with the corresponding
eigenvectors a1 , . . . , ap , and µ1 , . . . , µp are the eigenvalues of B ∈ Rp×p with the corresponding eigenvectors
b1 , . . . , bp . Notice that there is one and only one zero eigenvalue for Lwt because wt ∈ Sw . Assume λ1 , . . . , λp
are the eigenvalues of (Lwt )† with the corresponding eigenvectors a1 , . . . , ap . Without loss of generality, let
λ1 = 0 and then a1 = √1p 1. By calculation, one obtains
(vec(Lx))> vec(ai ⊗ a1 ) = 0,

(vec(Lx))> vec(a1 ⊗ ai ) = 0,

and

for any i = 1, . . . , p and any x ∈ Rp(p−1)/2 , indicating that vec(Lx) is orthogonal to all the eigenvectors of
†
†
†
(Lwt )† ⊗ (Lw
 t ) corresponding to zero eigenvalues. The smallest nonzero eigenvalue of (Lwt ) ⊗ (Lwt ) is
λ22 (Lwt )† , establishing (100). One further has

−2
−2
λ22 (Lwt )† = kLwt k2 = kLw1 + tL(w2 − w1 )k2
−2

≥ (kLw? k2 + (1 − t) kLw1 − Lw? k2 + t kLw2 − Lw? k2 )

−2
p
p
≥ kLw? k2 + (1 − t) 2p kw1 − w? k + t 2p kw2 − w? k

1 −2
1
,
(101)
≥ τ+
≥
2
δτ
(1 + δ −1 ) τ 2
√
1
where the second inequality follows from kLxk2 ≤ kLxkF ≤ (x> M x) 2 ≤ 2p kxk according to Lemma 5.3;
1
the third inequality holds because both w1 , w2 ∈ B(w? ; √2pδτ
), and kLw? k2 ≤ τ following from Assumption
1
−1
3.6; the last inequality follows from δτ ≤ δ τ , where τ > 1. Substituting (101) into (100) yields

λmin ∇2 g(wt ) ≥

2
2

(1 + δ −1 ) τ 2

.

(102)

Combining (99) and (102) together yields
f (w2 ) ≥ f (w1 ) + h∇f (w1 ), w2 − w1 i +

1
(1 +

2
δ −1 )

2

τ2

kw2 − w1 k .

Next, we will bound λmax (∇2 f (wt )). Similarly, one has

λmax ∇2 f (wt ) = sup x> ∇2 f (wt )x
kxk=1

= sup (vec(Lx))

>



−1

(Lwt + J )

⊗ (Lwt + J )

−1



vec(Lx)

kxk=1

≤ sup
kxk=1

(vec(Lx))

>



(Lwt )† + J ⊗ (Lwt )† + J vec(Lx)
>

(vec(Lx)) vec(Lx)
38

2

· sup kLxkF
kxk=1

>

= sup

(vec(Lx))


(Lwt )† ⊗ (Lwt )† vec(Lx)
>

(vec(Lx)) vec(Lx)

†
≤ λmax (Lwt ) ⊗ (Lwt )† · λmax (M )

= 2pλmax (Lwt )† ⊗ (Lwt )† ,
kxk=1

· λmax (M )

(103)

where the last equality follows from Lemma 5.3. One further obtains,


λmax (Lwt )† ⊗ (Lwt )† = λ2max (Lwt )† = λ−2
2 (Lwt ),

(104)

where λ2 (Lwt ) denotes the second smallest eigenvalue of Lwt , i.e., the minimum nonzero eigenvalue, which
could be bounded by
λ2 (Lwt ) =
≥

inf

x> (Lwt )x

inf

x> (Lw? )x + (1 − t)

kxk=1,x⊥1
kxk=1,x⊥1

inf

kxk=1,x⊥1

x> (Lw1 − Lw? )x + t

inf

kxk=1,x⊥1

x> (Lw2 − Lx? )x

≥ λ2 (Lw? ) − (1 − t) kLw1 − Lw? k2 − t kLw2 − Lw? k2
≥ λ2 (Lw? ) −

(1 − δ −1 )
1
≥
,
δτ
τ

(105)

where the first equality holds because Lwt has only one zero eigenvalue and its eigenvector is √1p 1, and the
eigvectors of Lwt associated with different eigenvalues are orthogonal with each other; the last inequality
follows from Assumption 3.6. Substituting (104) and (105) into (103) yields

λmax ∇2 f (wt ) ≤

2pτ 2
.
(1 − δ −1 )2

(106)

Combining (99) and (106) together yields
f (w2 ) ≤ f (w1 ) + h∇f (w1 ), w2 − w1 i +

pτ 2
2
kw2 − w1 k ,
(1 − δ −1 )2

completing the proof.

6.10

Proof of Lemma 5.10
2

2

Proof. Recall that BM (w? ; r) = {w ∈ Rp(p−1)/2 | kw − w? kM ≤ r}, where kxkM = hx, M xi = kLxkF with
p
M  0 defined in Lemma 5.3, and Sw = {w |w ≥ 0, (Lw + J ) ∈ S++
} We can see BM (w? ; r) is a convex
?
set because both BM (w ; r) and Sw are convex. It is easy to check that Sw is convex (See (98) for more
details). For any w1 , w2 ∈ BM (w? ; r), by Mean Value Theorem, one obtains
1
f (w2 ) = f (w1 ) + h∇f (w1 ), w2 − w1 i + hw2 − w1 , ∇2 f (wt )(w2 − w1 )i,
2
where wt = tw2 + (1 − t)w1 with t ∈ [0, 1]. For any nonzero x ∈ Rp(p−1)/2 , one has


>
−1
−1
x> ∇2 f (wt )x = (vec(Lx)) (Lwt + J ) ⊗ (Lwt + J )
vec(Lx)

>
(vec(Lx)) (Lwt + J )−1 ⊗ (Lwt + J )−1 vec(Lx)
2
=
· kLxkF
>
(vec(Lx)) vec(Lx)


>
(vec(Ly))
(Lwt )† + J ⊗ (Lwt )† + J vec(Ly)
2
≥ inf
· kLxkF
>
y
(vec(Ly)) vec(Ly)

2
≥ λ22 (Lwt )† · kLxkF ,
39

(107)

(108)


where λ2 (Lwt )† denotes the second smallest eigenvalue of (Lwt )† , and (Lwt )† is the pseudo inverse of
Lwt . The first equality follows from Lemma 5.1 and the last inequality is according to (100). Notice that
(108) also holds with x = 0. One further obtains

−2
−2
λ22 (Lwt )† ≥ (kLw? k2 + (1 − t) kLw1 − Lw? k2 + t kLw2 − Lw? k2 ) ≥ (kLw? k2 + r) ,
(109)
where the second inequality is established by kLxk2 ≤ kLxkF and the fact that both w1 , w2 ∈ BM (w? ; r).
Substituting (109) into (108) yields
−2

x> ∇2 f (wt )x ≥ (kLw? k2 + r)

2

· kLxkF .

(110)

Combining (107) and (110) yields
f (w2 ) ≥ f (w1 ) + h∇f (w1 ), w2 − w1 i +

1
−2
2
(kLw? k2 + r) kLw1 − Lw2 kF ,
2

(111)

f (w1 ) ≥ f (w2 ) + h∇f (w2 ), w1 − w2 i +

1
−2
2
(kLw? k2 + r) kLw1 − Lw2 kF ,
2

(112)

and

Combining (111) and (112), we establish
−2

h∇f (w1 ) − ∇f (w2 ), w1 − w2 i ≥ (kLw? k2 + r)

2

kLw1 − Lw2 kF ,

(113)

completing the proof.

7

Conclusions

In this paper, we have considered learning a sparse graph under the Laplacian constrained GGM. First we
have proved that a large regularization parameter of the `1 -norm leads to a fully connected graph. Then we
have proposed a nonconvex penalized maximum likelihood estimation method with theoretical guarantees on
the estimation error, by solving a sequence of weighted `1 -norm regularized sub-problems. We have proved
that the statistical error of the proposed estimator matches the minimax lower bound. A projected gradient
descent algorithm has designed to solve each sub-problem which enjoys a linear convergence rate. Numerical
results have demonstrated the effectiveness of the proposed method.
It is interesting to extend the proposed method to learn other structured graphs like k-component graphs,
and characterize the statistical properties. We leave this for the future research.

References
[1] Réka Albert and Albert-László Barabási. Statistical mechanics of complex networks. Reviews of Modern
Physics, 74(1):47, 2002.
[2] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with sparsityinducing penalties. Foundations and Trends in Machine Learning, 4(1):1–106, 2012.
[3] Onureena Banerjee, Laurent El Ghaoui, and Alexandre d’Aspremont. Model selection through sparse
maximum likelihood estimation for multivariate Gaussian or binary data. Journal of Machine Learning
Research, 9(Mar):485–516, 2008.
[4] Patrick Breheny and Jian Huang. Coordinate descent algorithms for nonconvex penalized regression,
with applications to biological feature selection. The Annals of Applied Statistics, 5(1):232, 2011.

40

[5] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun. Spectral networks and locally connected networks on graphs. In International Conference on Learning Representations, 2014.
[6] T Tony Cai, Weidong Liu, and Harrison H Zhou. Estimating sparse precision matrix: Optimal rates of
convergence and adaptive estimation. The Annals of Statistics, 44(2):455–488, 2016.
[7] Jinghui Chen, Pan Xu, Lingxiao Wang, Jian Ma, and Quanquan Gu. Covariate adjusted precision
matrix estimation via nonconvex optimization. In International Conference on Machine Learning, pages
921–930, 2018.
[8] Fan RK Chung. Spectral graph theory. Number 92. American Mathematical Soc., 1997.
[9] Alexandre d’Aspremont, Onureena Banerjee, and Laurent El Ghaoui. First-order methods for sparse
covariance selection. SIAM Journal on Matrix Analysis and Applications, 30(1):56–66, 2008.
[10] Arthur P Dempster. Covariance selection. Biometrics, pages 157–175, 1972.
[11] Quoc Tran Dinh, Anastasios Kyrillidis, and Volkan Cevher. A proximal Newton framework for composite
minimization: Graph learning without Cholesky decompositions and matrix inversions. In International
Conference on Machine Learning, pages 271–279, 2013.
[12] Xiaowen Dong, Dorina Thanou, Pascal Frossard, and Pierre Vandergheynst. Learning Laplacian matrix
in smooth graph signal representations. IEEE Transactions on Signal Processing, 64(23):6160–6173,
2016.
[13] Xiaowen Dong, Dorina Thanou, Michael Rabbat, and Pascal Frossard. Learning graphs from data: A
signal representation perspective. IEEE Signal Processing Magazine, 36(3):44–63, 2019.
[14] Pavel Drábek and Jaroslav Milota. Methods of nonlinear analysis: applications to differential equations.
Springer Science & Business Media, 2007.
[15] Michael P Drazin and Emilie V Haynsworth. Criteria for the reality of matrix eigenvalues. Mathematische Zeitschrift, 78(1):449–452, 1962.
[16] John Duchi, Stephen Gould, and Daphne Koller. Projected subgradient methods for learning sparse
gaussians. In Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence,
page 153160, 2008.
[17] Hilmi E Egilmez, Eduardo Pavez, and Antonio Ortega. Graph learning from data under laplacian and
structural constrints. IEEE Journal of Selected Topics in Signal Processing, 11(6):825–841, 2017.
[18] Shaun Fallat, Steffen Lauritzen, Kayvan Sadeghi, Caroline Uhler, Nanny Wermuth, Piotr Zwiernik,
et al. Total positivity in Markov structures. The Annals of Statistics, 45(3):1152–1184, 2017.
[19] Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle
properties. Journal of the American Statistical Association, 96(456):1348–1360, 2001.
[20] Jianqing Fan, Han Liu, Qiang Sun, and Tong Zhang. I-LAMM for sparse learning: Simultaneous control
of algorithmic complexity and statistical error. The Annals of Statistics, 46(2):814, 2018.
[21] Santo Fortunato. Community detection in graphs. Physics Reports, 486(3-5):75–174, 2010.
[22] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the
graphical lasso. Biostatistics, 9(3):432–441, 2008.
[23] Akshay Gadde and Antonio Ortega. A probabilistic interpretation of sampling theory of graph signals.
In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 3257–3261, 2015.

41

[24] Semyon Aranovich Gershgorin. Uber die abgrenzung der eigenwerte einer matrix. Bulletin of the Russian
Academy of Sciences, (6):749–754, 1931.
[25] Alexander J Hartemink, David K Gifford, Tommi S Jaakkola, and Richard A Young. Using graphical
models and genomic expression data to statistically validate models of genetic regulatory networks. In
Biocomputing 2001, pages 422–433. World Scientific, 2000.
[26] Andrew Holbrook. Differentiating the pseudo determinant. Linear Algebra and its Applications, 548:
293 – 304, 2018.
[27] Jean Honorio, Dimitris Samaras, Irina Rish, and Guillermo Cecchi. Variable selection for gaussian
graphical models. In Artificial Intelligence and Statistics, pages 538–546, 2012.
[28] Cho-Jui Hsieh, Mátyás A Sustik, Inderjit S Dhillon, and Pradeep Ravikumar. QUIC: quadratic approximation for sparse inverse covariance estimation. The Journal of Machine Learning Research, 15
(1):2911–2947, 2014.
[29] David R Hunter and Runze Li. Variable selection using MM algorithms. The Annals of Statistics, 33
(4):1617, 2005.
[30] Vassilis Kalofolias. How to learn a graph from smooth signals. In Artificial Intelligence and Statistics,
pages 920–929, 2016.
[31] Sandeep Kumar, Jiaxi Ying, Jose Vinicius de Miranda Cardoso, and Daniel P. Palomar. Structured
graph learning via Laplacian spectral constraints. In Advances in Neural Information Processing Systems, pages 11647–11658, 2019.
[32] Sandeep Kumar, Jiaxi Ying, Jose Vinicius de Miranda Cardoso, and Daniel P Palomar. A unified
framework for structured graph learning via spectral constraints. Journal of Machine Learning Research,
21(22):1–60, 2020.
[33] Brenden Lake and Joshua Tenenbaum. Discovering structure by learning sparse graphs. In Proceedings
of the 33rd Annual Cognitive Science Conference, pages 778–783, 2010.
[34] Clifford Lam and Jianqing Fan. Sparsistency and rates of convergence in large covariance matrix
estimation. The Annals of Statistics, 37(6B):4254, 2009.
[35] Steffen Lauritzen, Caroline Uhler, Piotr Zwiernik, et al. Maximum likelihood estimation in Gaussian
models under total positivity. The Annals of Statistics, 47(4):1835–1863, 2019.
[36] Steffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.
[37] Neil D Lawrence. A unifying probabilistic perspective for spectral dimensionality reduction: Insights
and new models. Journal of Machine Learning Research, 13(May):1609–1638, 2012.
[38] Hongzhe Li and Jiang Gui. Gradient directed regularization for sparse Gaussian concentration graphs,
with applications to inference of genetic networks. Biostatistics, 7(2):302–317, 2006.
[39] Lu Li and Kim-Chuan Toh. An inexact interior point method for L1-regularized sparse covariance
selection. Mathematical Programming Computation, 2(3-4):291–315, 2010.
[40] Tianyi Liu, Minh Trinh Hoang, Yang Yang, and Marius Pesavento. A block coordinate descent algorithm
for sparse Gaussian graphical model inference with Laplacian constraints. In IEEE 8th International
Workshop on Computational Advances in Multi-Sensor Adaptive Processing, pages 236–240, 2019.
[41] Po-Ling Loh and Martin J Wainwright. Regularized M-estimators with nonconvexity: Statistical and
algorithmic theory for local optima. The Journal of Machine Learning Research, 16(1):559–616, 2015.

42

[42] Po-Ling Loh, Martin J Wainwright, et al. Support recovery without incoherence: A case for nonconvex
regularization. The Annals of Statistics, 45(6):2455–2482, 2017.
[43] Zhaosong Lu. Smooth optimization approach for sparse covariance selection. SIAM Journal on Optimization, 19(4):1807–1827, 2009.
[44] Xiang Lyu, Will Wei Sun, Zhaoran Wang, Han Liu, Jian Yang, and Guang Cheng. Tensor graphical
model: Non-convex optimization and statistical inference. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2019.
[45] Rahul Mazumder and Trevor Hastie. The graphical lasso: New insights and alternatives. Electronic
Journal of Statistics, 6:2125, 2012.
[46] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks
for graphs. In International Conference on Machine Learning, pages 2014–2023, 2016.
[47] Antonio Ortega, Pascal Frossard, Jelena Kovavcević, José MF Moura, and Pierre Vandergheynst. Graph
¯
signal processing: Overview, challenges, and applications. Proceedings of the IEEE, 106(5):808–828,
2018.
[48] Figen Oztoprak, Jorge Nocedal, Steven Rennie, and Peder A Olsen. Newton-like methods for sparse
inverse covariance estimation. In Advances in Neural Information Processing Systems, pages 755–763,
2012.
[49] Youngsuk Park, David Hallac, Stephen Boyd, and Jure Leskovec. Learning the network structure of
heterogeneous data via pairwise exponential Markov random fields. Proceedings of Machine Learning
Research, 54:1302, 2017.
[50] Pradeep Ravikumar, Martin J Wainwright, Garvesh Raskutti, Bin Yu, et al. High-dimensional covariance estimation by minimizing `1 -penalized log-determinant divergence. Electronic Journal of Statistics,
5:935–980, 2011.
[51] Adam J Rothman, Peter J Bickel, Elizaveta Levina, Ji Zhu, et al. Sparse permutation invariant covariance estimation. Electronic Journal of Statistics, 2:494–515, 2008.
[52] Havard Rue and Leonhard Held. Gaussian Markov random fields: theory and applications. Chapman
and Hall/CRC, 2005.
[53] Luana Ruiz, Fernando Gama, Antonio Garcı́a Marques, and Alejandro Ribeiro. Invariance-preserving
localized activation functions for graph neural networks. IEEE Transactions on Signal Processing, 68:
127–141, 2019.
[54] Katya Scheinberg, Shiqian Ma, and Donald Goldfarb. Sparse inverse covariance selection via alternating
linearization methods. In Advances in Neural Information Processing Systems, pages 2101–2109, 2010.
[55] Mark Schmidt, Ewout Berg, Michael Friedlander, and Kevin Murphy. Optimizing costly functions with
simple constraints: A limited-memory projected quasi-newton algorithm. In Artificial Intelligence and
Statistics, pages 456–463, 2009.
[56] Xiaotong Shen, Wei Pan, and Yunzhang Zhu. Likelihood-based selection and sharp parameter estimation. Journal of the American Statistical Association, 107(497):223–232, 2012.
[57] Ali Shojaie and George Michailidis. Penalized likelihood methods for estimation of sparse highdimensional directed acyclic graphs. Biometrika, 97(3):519–538, 2010.
[58] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The
emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks
and other irregular domains. IEEE Signal Processing Magazine, 30(3):83–98, 2013.
43

[59] Martin Slawski and Matthias Hein. Estimation of positive definite M-matrices and structure learning
for attractive Gaussian Markov random fields. Linear Algebra and its Applications, 473:145–179, 2015.
[60] Qiang Sun, Kean Ming Tan, Han Liu, and Tong Zhang. Graphical nonconvex optimization via an
adaptive convex relaxation. In International Conference on Machine Learning, pages 4810–4817, 2018.
[61] Ying Sun, Prabhu Babu, and Daniel P. Palomar. Majorization-Minimization algorithms in signal processing, communications, and machine learning. IEEE Transactions on Signal Processing, 65(3):794–816,
2016.
[62] Richard S Varga. Geršgorin and his circles. Berlin, Germany: Springer-Verlag, 2004.
[63] Martin J. Wainwright. High-dimensional statistics: A non-asymptotic viewpoint. Cambridge Series in
Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
[64] Chengjing Wang, Defeng Sun, and Kim-Chuan Toh. Solving log-determinant optimization problems by
a Newton-CG primal proximal point algorithm. SIAM Journal on Optimization, 20(6):2994–3013, 2010.
[65] Lingxiao Wang, Xiang Ren, and Quanquan Gu. Precision matrix estimation in high dimensional gaussian
graphical models with faster rates. In Artificial Intelligence and Statistics, pages 177–185, 2016.
[66] Ralph A Willoughby. The inverse M-matrix problem. Linear Algebra and its Applications, 18(1):75–94,
1977.
[67] Zhenyu Wu and Richard Leahy. An optimal graph theoretic approach to data clustering: Theory and its
application to image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence,
15(11):1101–1113, 1993.
[68] Eunho Yang, Genevera Allen, Zhandong Liu, and Pradeep K Ravikumar. Graphical models via generalized linear models. In Advances in Neural Information Processing Systems, pages 1358–1366, 2012.
[69] Eunho Yang, Aurélie C Lozano, and Pradeep K Ravikumar. Elementary estimators for graphical models.
In Advances in Neural Information Processing Systems, pages 2159–2167, 2014.
[70] Ming Yuan and Yi Lin. Model selection and estimation in the Gaussian graphical model. Biometrika,
94(1):19–35, 2007.
[71] Cha Zhang and Dinei Florêncio. Analyzing the optimality of predictive transform coding using graphbased models. IEEE Signal Processing Letters, 20(1):106–109, 2012.
[72] Cun-Hui Zhang et al. Nearly unbiased variable selection under minimax concave penalty. The Annals
of Statistics, 38(2):894–942, 2010.
[73] Tong Zhang. Analysis of multi-stage convex relaxation for sparse regularization. Journal of Machine
Learning Research, 11(Mar):1081–1107, 2010.
[74] Licheng Zhao, Yiwei Wang, Sandeep Kumar, and Daniel P Palomar. Optimization algorithms for graph
Laplacian estimation via ADMM and MM. IEEE Transactions on Signal Processing, 67(16):4231–4244,
2019.
[75] Hui Zou and Runze Li. One-step sparse estimates in nonconcave penalized likelihood models. The
Annals of Statistics, 36(4):1509, 2008.

44

