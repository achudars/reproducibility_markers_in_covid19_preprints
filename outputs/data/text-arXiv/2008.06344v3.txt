COVID–19 MORTALITY ANALYSIS FROM SOFT–DATA
MULTIVARIATE CURVE REGRESSION AND MACHINE LEARNING

arXiv:2008.06344v3 [stat.ML] 27 Mar 2021

A. Torres–Signes, M. P. Frı́as and M. D. Ruiz–Medina

Abstract
A multiple objective space–time forecasting approach is presented involving cyclical curve log–regression, and multivariate time series spatial
residual correlation analysis. Specifically, the mean quadratic loss function is minimized in the framework of trigonometric regression. While,
in our subsequent spatial residual correlation analysis, maximization of
the likelihood allows us to compute the posterior mode in a Bayesian
multivariate time series soft–data framework. The presented approach is
applied to the analysis of COVID–19 mortality in the first wave affecting the Spanish Communities, since March, 8, 2020 until May, 13, 2020.
An empirical comparative study with Machine Learning (ML) regression,
based on random k–fold cross-validation, and bootstrapping confidence
interval and probability density estimation, is carried out. This empirical
analysis also investigates the performance of ML regression models in a
hard– and soft– data frameworks. The results could be extrapolated to
other counts, countries, and posterior COVID–19 waves.
Keywords COVID–19 analysis, Curve regression, Hard–data, Machine
Learning, Multivariate time series, Soft–data.
MSC code 62F40; 62F15; 62F10; 90B99

1

Introduction

Coronavirus disease 2019 (COVID–19) rapidly spreads around many other countries, since December 2019 when arises in China (see [55]; [61]; [69]). The effective allocation of medical resources requires the derivation of predictive techniques, describing the spatiotemporal dynamics of COVID-19 (see, e.g., [22];
[37]; [46]; [50], just to mention a few). Epidemiological models can contribute
to the analysis of the causes, dynamics, and spread of this pandemic (see, e.g,
[29]; [35]; [40], and the references therein). Short-term forecasts can be obtained
adopting the framework of compartmental SIR (susceptible-infectious-recovered)
models, based on ordinary differential equations (see, e.g. [6] [24]; [33]; [36];
1

[39]; [44]; [48]; [59]; [65]; [67]). An extensive literature is available, including
different versions of compartmental models, like SIR-susceptible (SIRS, [23]),
and delay differential equations (see [8]; [43]; [54]). Spatial extensions, based
on reaction-diffusion models, reflecting the infectious disease spread over a spatial region can be found, for instance, in [26] and [63]. SEIRD (susceptible,
exposed, infected, recovered, deceased) models, incorporating the spatial spread
of the disease with inhomogeneous diffusion terms are also analyzed (see [52]
and [53]). The stochastic version of SIR–type models intends to cover several
limitations detected regarding uncertainly in the observations, and the hidden dynamical epidemic process. Markov chain SIR based modelling (see [5]; [64]), and
some recent stochastic formulations involving complex networks (see [60]; [68])
or drug–resistant influenza (see [13]) constitute some alternatives. A Bayesian hierarchical statistical SIRS model framework is adopted in [1]; [2]; [5]; [25] taking
into account the observation error in the counts, and uncertainty in the parameter
space. Beyond SIR modeling, the multivariate survival analysis approach offers
a suitable modelling framework, regarding infection, incubation and recovering
random periods, affecting the containment of COVID-19 (see, e.g., [10] ; [34];
[47]; [62]).
In a first stage, most of the above referred models have been adapted and
applied to approximate the space/time evolution of COVID–19 incidence and
mortality. That is the case, for instance, of the three models presented in [51],
which were validated with outbreaks of other diseases different from COVID–19.
Alternative SEIR type models, involving stochastic components, are formulated
in [38]. A revised SEIR model has also been proposed in [66] (see also [28]). A θ–
SEIHRD model, able to estimate the number of cases, deaths, and needs of beds
in hospitals, is introduced in [31], adapted to COVID–19, based on the Be-CoDiS
model (see [32]). Due to the low quality of the records available, and the hidden
sample information, the most remarkable feature in this research area is the
balance between complexity and indentifiability of model parameters. Recently,
an attempt to simplify modelling strategies, applied to COVID-19 data analysis, is
presented in [49], in terms of θ–SEIHQRD model. Mitigation of undersampling is
proposed in [41], based on re-scaling of summary statistics characterizing sample
properties of the pandemic process, useful between countries with similar levels
of health care.
Nowadays ML models have established themselves as serious contenders to
classical statistical models in the area of forecasting. Research started in the
eighties with the development of the neural network model. Subsequently, research extended this concept to alternative models, such as support vector machines, decision trees, and others (see, e.g., [4]; [9]; [27]; [45]). In general, curve
regression techniques based on a function basis, usually in the space of square
integrable functions with respect to a suitable probability measure, allow short–
2

and long– term forecast. Thus, depending on our choice of the function basis,
and the probability measure selected, particle and field views could be combined.
Note that the classical stochastic diffusion models offer a particle rather than a
field view (see, e.g., [42]).
Linear regression, multilayer perceptron and vector autoregression methods
have been applied in [56]-[57] to predicting COVID-19 spread, anticipating the
potential patterns of COVID-19 effects (see also Section 2 of [56], on related
work). Early stage location of COVID-19 is addressed in [7], applying machine learning strategies actualized on stomach Computed Tomography pictures.
[15] evaluates association between meteorological factors and COVID–19 spread.
They concluded that average temperature, minimum relative humidity, and precipitation were better predictors, displaying possible non–linear correlations with
COVID–19 variables. These conclusions are crucial in the subsequent machine
learning regression based analysis.
This paper presents a multiple objective space–time forecasting approach,
where curve trigonometric log–regression is combined with multivariate time series spatial residual analysis. In our curve regression model fitting, we are interested on reflecting the cyclical behavior of COVID–19 mortality induced by the
hardening or relaxation of the containment measures, adopted to mitigate the
increase of infections and mortality. The trigonometric basis (sines and cosines)
is then selected in our spatial heterogeneous curve log–regression model fitting.
The ratio of the expected minimized empirical risk, and the corresponding expected value of the quadratic loss function at such a minimizer is considered for
model selection (see, e.g., [14]). Note that this selection procedure provides an
agreement between the expected minimum empirical risk, and the corresponding
expected theoretical loss function value.
The penalized factor proposed in [14], applied to our choice of the truncation
parameter, leads to the dimension of the subspace where our curve regression estimator is approximated at any spatial location. This model selection procedure
is asymptotically equivalent to Akaike correction factor. A robust modification
of the Akaike information criterion can be found, for example, in [3]. As an alternative, one can consider cross-validation criterion for selecting the best subset
of explanatory variables (see [58], where a mixed-integer optimization approach
is proposed in this context).
Beyond asymptotic analysis, model selection from finite sample sizes constitutes a challenging topic in our approach. To address this problem, a bootstrap
estimator of the ratio between the expected quadratic loss function and the expected training quadratic error, from different sets of explanatory variables, is
implemented. Bootstrap confidence intervals are also provided for the spatial
mean of the curve regression predictor, and for the expected training error of
the curve regression, and of the multivariate time–series residual predictor. The
3

bootstrap approximation of the probability distribution of these statistics is also
computed.
In our multivariate time series analysis of the regression residuals, a classical and Bayesian componentwise estimation of the spatial linear correlation is
achieved. The presented multiple objective forecasting approach is applied to
the spatiotemporal analysis of COVID–19 mortality in the first wave affecting
the Spanish Communities, since March, 8, 2020 until May, 13, 2020. Our results
show a remarkable qualitative agreement with the reported epidemiological data.
The spatiotemporal approach presented in this paper makes the fusion of
generalized random field theory, and our multiple–objective space–time forecasting, based on nonlinear parametric regression, and bayesian analysis of the spatiotemporal correlation structure. Regarding the site–specific or specificatory
knowledge bases (see [18]), in our approach, several information sources can be
incorporated in the description of the hidden epidemic process. Particularly, we
distinguish here between hard–data or hard measurements providing a satisfactory
level of accuracy for practical purposes, and soft–data displaying a non–negligible
amount of uncertainty. That is, in this second data category, we include missing
observations or imperfect observations, categorical data and fuzzy inputs (see
also [16]; [17]; [19], and the references therein). In this paper, we consider
hard–data sets given by numerical values of our count process at the Spanish
Communities analyzed. Our soft–data sample complements hard measures, in
terms of interpolated, smoothed, and spatial projected data. Particularly, spatial
correlations between regions are incorporated in terms of soft–data. Additional
information about the continuous functional nature of the underlying space–time
COVID–19 mortality process is also reflected in our soft–data set. This information helps the implementation of the proposed estimation methodology in the
framework of Functional Data Analysis (FDA) techniques.
As commented before, last advances in spatiotemporal mapping of epidemiological data incorporate ML regression models to improve and help the understanding of general or core knowledge bases. Thus, model fitting is achieved
according to epidemiological systems laws, population dynamics, and theoretical space–time dependence models (see [20], and the references therein). See
also [7], [15] and [56] in the hard–data context. It is well–known that the limited availability of hard–data affects space-time analysis. Hence, the incorporation of soft–data into ML regression models can help this analysis, providing a
global view of the available sample information (see, e.g., [18]). Particularly, in
our empirical comparative analysis, involving ML regression models and our approach, input hard– and soft–data information is incorporated. Cross–validation,
bootstrapping confidence intervals and probability density estimation support our
comparative study. Specifically, random k–fold (k = 5, 10) cross–validation first
evaluates the performance of the compared regression models from hard– and
4

soft–data, in terms of Symmetric Mean Absolute Percentage Errors (SMAPEs).
Bootstrap confidence intervals and probability density estimation of the spatially
averaged SMAPEs approximate the distributional characteristics of the random
k–fold cross–validation errors. Thus, a complete picture of SMAPEs supports
our evaluation of the predictive ability of the regression models tested, from the
analyzed hard– and soft–data sets.
From the empirical comparative analysis carried out, we can conclude that
almost the best performance in both, hard– and soft–data categories, is displayed by Radial Basis Function Neural Network (RBF), and Gaussian Processes
(GP). Both approaches are improved, when soft–data are incorporated into the
regression analysis. Slightly differences are observed in the performance of Support Vector Regression (SVR) and Bayesian Neural Networks (BNN). Multilayer
Perceptron (MLP) gets over GRNN, presenting better estimation results when
hard–data are analyzed. The sample values and distributional characteristics of
cross–validation SMAPEs, in Generalized Regression Neural Network (GRNN),
are similar to the ones obtained in trigonometric curve regression, when spatial
residual analysis is achieved in terms of empirical second–order moments. Note
that, GRNN is also favored by the soft–data category. In this category, BNN
and our approach show very similar performance, when trigonometric regression
is combined with Bayesian multivariate time series residual prediction. Indeed,
some slightly better bootstrapping distributional characteristics of our approach
respect to BNN are observed in the soft–data category.
The outline of the paper is the following. The modeling approach is introduced in Section 2. Section 3 describes the multiple objective forecasting
methodology. This methodology is applied to the spatiotemporal statistical analysis of COVID–19 mortality in Spain in Section 4. The empirical comparative
study with ML regression models is given in Section 5. Conclusions about our
data–driven model ranking can be found in Section 6. In the Supplementary
Material, a brief introduction to our implementation of ML models from hard–
and soft–data is provided. Additional numerical estimation results, based on the
complete sample, are also displayed. Particularly, the observed and predicted
mortality cumulative cases, and log–risk curves are displayed.

2

Data model

Let (Ω, A, P) be the basic probability space. Consider H = L2 (Rd ), d ≥ 2, the
space of square–integrable functions on Rd , to be the underlying real separable
Hilbert space. In the following, we denote by B d the Borel σ–algebra in Rd ,
d ≥ 1.
Let X = {Xt (z), z ∈ Rd , t ∈ R+ } be our spatiotemporal input hard–data
5

process on (Ω, A, P), satisfying E [kXt (·)k2H ] < ∞, for any time t ∈ R+ . The
input soft–data process over any spatial bounded set D ∈ B d is then defined as


Z
∞
Xt (z)h(z)dz, h ∈ C0 (D), t ∈ R+ ,
(1)
Xt (h) =
D

where C0∞ (D) denotes the space of infinite differentiable functions, with compact
support contained in D. For each bounded set D ∈ B d , define
Λ = {Λt (h) = exp (Xt (h)) , h ∈ C0∞ (D), t ∈ R+ }.
Assume that, for any finite positive interval T ∈ B, and bounded set D ∈ B d ,
Z
IT (h) 2 =
exp (Xt (h)) dt < ∞, ∀h ∈ C0∞ (D),
(2)
L (Ω,A,P )

where

=

L2 (Ω,A,P )

T

denotes the identity in the second–order moment sense. Let

{Nh : (Ω, A, P) × B −→ N, h ∈ H} be a family of random counting measures.
Given the observation {xt (h), t ∈ T } at the finite temporal interval T ∈ B
of the input soft–data process over the spatial h–window in D, the conditional
probability distribution of the number of random events RNh (T ) that occur in
T ∈ B is a Poisson probability distribution with mean T exp (xt (h)) dt, for
every h ∈ C0∞ (D) and D ∈ B d . We refer to IT (h) as the generalized cumulative
mortality risk random process over the interval T . Hence, the input hard–data
process X = {Xt (z), z ∈ Rd , t ∈ R+ } defines the spatiotemporal mortality
log–risk process.
From the sample values of our input soft–data process, the following observation model is considered in the curve regression model fitting
ln (Λt ) (ψp,$p ) = gt (ψp,$p , θ(p)) + εt (ψp,$p )
= gt (·, θ(p)), ψp,$p (·) H + εt (·), ψp,$p (·)

H

, t ∈ R+ , p = 1, . . . , P,(3)

where
Z
gt (ψp,$p , θ(p)) =

gt (z, θ(p))ψp,$p (z)dz
Dp

Z
hf, giH =

f (z)g(z)dz,

(4)

Rd

with {ψp,$p , p = 1, . . . , P } ⊂ H denoting a function family in H, whose elements have respective compact supports Dp , p = 1, . . . , P, defining the p small–
areas where the counts are aggregated, satisfying suitable regularity conditions.
6

For each p = 1, . . . , P, the vector $p contains the center and bandwidth parameters, defining the window selected in the analysis of the small–area p. For
each p ∈ {1, . . . , P }, θ(p) = (θ1 (p), . . . , θq (p)) ∈ Θ represents the unknown
parameter vector to be estimated at the p region, and Θ is the open set defining
the parameter space, whose closure Θc is a compact set in Rq . We assume that
gt is of the form (see, e.g., [30])
gt (θ(p)) =

N
X

(Ak (p) cos(ϕk (p)t) + Bk (p) sin(ϕk (p)t)) , p = 1, . . . , P,

t ∈ R+ ,

k=1

(5)
whose spatial–dependent parameters are given by the temporal scalings
(ϕ1 (·), . . . , ϕN (·)) , and the Fourier coefficients (A1 (·), B1 (·), . . . , AN (·), BN (·)) .
For simplifications purposes, we will consider that the scaling parameters ϕk ,
k = 1, . . . , N, are known, and fixed over the P spatial regions. Also, Ck2 (·) =
A2k (·)+Bk2 (·) > 0, for k = 1, . . . , N, where N denotes the truncation parameter,
that will be selected according to the penalized factor proposed in [14], as we
explain in more detail in Section 3. Thus,
θ(p) = (A1 (p), B1 (p), . . . , AN (p), BN (p)),

p = 1, . . . , P.

To analyze the spatial correlation between regions, a multivariate autoregressive model is considered for prediction of the regression residual term at each
region p ∈ {1, . . . , P }. Particularly, for any T ≥ 2, εt in equation (3) is assumed
to satisfy the state equation, for p = 1, . . . , P,
εt (ψp,$p ) =

P
X

ρ(ψq,$q )(ψp,$p )εt−1 (ψq,$q ) + νt (ψp,$p ),

(6)

q=1

where, for any t ∈ R+ , and p, q = 1, . . . , P,
Z
εt (ψp,$p ) =
εt (z)ψp,$p (z)dz
Dp
Z
νt (ψp,$p ) =
νt (z)ψp,$p (z)dz
Dp
Z
ρ(ψq,$q )(ψp,$p ) =
ρ(z, y)ψp,$p (z)ψq,$q (y)dydz.
Dp ×Dp



Here, νt (ψp,$p ), p = 1, . . . , P , t ∈ R+ , are assumed to be independent zero–
mean Gaussian P –dimensional vectors. For p, q ∈ {1, . . . P }, the projection
ρ(ψp,$p )(ψq,$q ) then keeps the temporal linear autocorrelation at each spatial
region for p = q, and the temporal linear cross-correlation between regions for
p 6= q of the regression error {εt (·), t ∈ R+ } (see, [11]).
7

3

Implementation of the curve regression model
and spatial residual analysis

Let D1 , . . . , DP be the small-areas, where the counts are aggregated, and {ψp,$p , $p =
(cp , ρp ), p = 1, . . . , P } ⊂ H be the functions with respective compact supports
D1 , . . . , DP . Particularly, we denote by cp , p = 1, . . . , P, the centers respectively allocated at the regions D1 , . . . , DP , and by ρ1 , . . . , ρP , the bandwidth
parameters providing the associated window sizes.
In practice, from the observation model (3), to find gt in (5) minimizing the
expected quadratic loss function, or expected risk, we look for the minimizer
bT (p) of the empirical regression risk
θ
T
1X
2
ln (Λt ) (ψp,$p ) − gt (θ(p)) .
inf c LT (θ(p)) = inf c
θ(p)∈Θ
θ(p)∈Θ T
t=1
(7)
Truncation parameter N is then selected to controlling the ratio between the exbT (p), and the expected value of the minimized
pected quadratic loss function at θ
empirical risk from the identity
i2
h
bT (p)
E ln (Λt ) (ψp,$p ) − gt (ψp,$p , θ
!
PN
−1
h
i
1/λi
bT (p)) 1 − N
= E LT (θ
,
(8)
1 + i=1
T
T

bT (p)) =
LT (θ

where, for i = 1, . . . , N, 1/λi denotes the inverse of the ith eigenvalue of the
matrix ΦT Φ, with Φ being a T × N matrix, whose elements are the values of
the N trigonometric basis functions selected at the time points t = 1, . . . , T.
Parameter N should be such that N << T. Note that, asymptotically, when
N → ∞, ΦT Φ goes to the identity matrix, and for i = 1, . . . , N, 1/λi ∼ 1. In
equation (8), we have considered the minimized empirical risk



bT (p)) = 1 R
e T (p) IT ×T − Φ ΦT Φ −1 ΦT R(p),
e
(9)
LT (θ
T
for each spatial region p = 1, . . . , P, where
e
R(p)
=

∞
X

!
(Ak (p) cos(ϕk t) + Bk (p) sin(ϕk t)) + εt (ψp,$p ), t = 1, . . . , T

k=N +1

Our regression predictor is then computed, for any t ∈ R+ , from the identity
\
bT (p)),
ln
(Λt )(ψp,$p ) = gt (θ
8

p = 1, . . . , P

(10)

.

(see Theorem 1 in [30] about conditions for the weak–consistency of (10)).
The regression residuals
n
o
b
Y = Yt (ψp,$p ) = ln (Λt ) (ψp,$p ) − gt (θ T (p)), t = 1, . . . , T, p = 1, . . . , P ,
and the empirical nuclear autocovariance and cross–covariance operators
T
1X
Y
b
Yt (ψp,$p )Yt (ψq,$q )
R0,T (ψp,$p )(ψq,$q ) =
T t=1
T −1

bY (ψp,$p )(ψq,$q ) =
R
1,T

1 X
Yt (ψq,$q )Yt+1 (ψp,$p ), p, q = 1, . . . , P,
T − 1 t=1
(11)

will be considered in the estimation of the spatial linear residual correlation (see
[11]). A truncation parameter k(T ) is also considered here to remove the ill–
posed nature of this estimation problem. Particularly, k(T ) must satisfy k(T ) →
∞, k(T )/T → 0, T → ∞. A suitable choice of k(T ) also ensures strong–
consistency of the estimator
k(T )

ρbk(T ) (ψp,$p )(ψq,$q ) =

X ψp,$p , φk,T

ψq,$q , φl,T
bY )
λk,T (R
0,T

k,l=1

H

H

Y
b1,T
R
(φk,T )(φl,T ),

(12)
for p, q = 1, . . . , P (see [11]). Here,
bY =
R
0,T

T
X

bY )[φk,T ⊗ φk,T ],
λk,T (R
0,T

(13)

k=1

bY ), k = 1, . . . , T } and {φk,T , k ≥ 1} denote the empirical
where {λk,T (R
0,T
bY , respectively. Particularly, we consider
eigenvalues and eigenvectors of R
0,T
k(T ) = ln(T ) (see [11]). The classical plug–in predictor is then computed,
for each p = 1, . . . , P, as
k(T )
Ybt (ψp,$p ) =

P
X

ρbk(T ) (ψq,$q )(ψp,$p )Yt−1 (ψq,$q ),

t ≥ 1.

(14)

q=1

Under the Gaussian distribution of νt , in the Bayesian estimation of ρ, from
(6), the likelihood function, defining the objective function, is given by, for each
9

p = 1, . . . , P,
ep (ε1p , . . . , εT p , ε0q , . . . , ε(T −1)q ρ(ψq,$q )(ψp,$p ), q = 1, . . . , P )
L

2 
PP
PT 
1
exp − 2σ2 t=1 εt (ψp,$p ) − q=1 εt−1 (ψq,$q )ρ(ψq,$q )(ψp,$p )
p
=
√ T
σp 2π
P
Y

apq −1
bpq −1
×
ρ(ψq,$q )(ψp,$p )
1 − ρ(ψq,$q )(ψp,$p )
q=1

I{0<ρ(ψq,$q )(ψp,$p )<1}
B(apq , bpq )
!
T
2
1
1 X
=
νt (ψp,$p )
√ T exp − 2
2σp t=1
σp 2π
×

P
Y

apq −1
bpq −1
×
ρ(ψq,$q )(ψp,$p )
1 − ρ(ψq,$q )(ψp,$p )
q=1

×

I{0<ρ(ψq,$q )(ψp,$p )<1}
,
B(apq , bpq )
(15)

where, for each p = 1, . . . , P, the beta probability distributions with shape
parameters apq and bpq , q = 1, . . . , P, respectively define the prior probability distributions of the independent random variables {ρ(ψq,$q )(ψp,$p ), q =
1, . . . , P }. Here, for each p = 1, . . . , P, εtp = εt (ψp,$p ) = εt , ψp,$p H , and
p
σp = E[εt (ψp,$p )]2 , for t = 0, . . . , T. As before, ψp,$p weights the spatial
sample information about the p small–area, for p = 1, . . . , P. As usual, I0<·<1
denotes the indicator function on the interval (0, 1), and B(apq , bpq ) is the beta
function,
Γ(apq )Γ(bpq )
B(apq , bpq ) =
.
Γ(apq + bpq )
From (15), the Bayesian predictor is obtained, for p = 1, . . . , P, as
εet (ψp,$p ) =

P
X

ρe(ψq,$q )(ψp,$p )εt−1 (ψq,$q ),

t ≥ 1,

(16)

q=1


with ρe(ψ1,$1 )(ψp,$p ), . . . , ρe(ψP,$P )(ψp,$p ) being computed by maximizing (15),
to find the posterior mode (see [12], where Bayesian estimation is introduced in
an infinite–dimensional framework). We refer to (16) as the Bayesian plug–
in predictor of the residual mortality log–risk process at the p small area, for
10

p = 1, . . . , P. In practice, equation (15) is approximated from the computed
values of the regression residual process.

4

Statistical analysis of COVID–19 mortality

Our analysis is based on daily records of COVID–19 mortality reported by the
Spanish Statistical National Institute, since March, 8 to May, 13, 2020, at the
17 Spanish Communities. We first describe the main steps of the proposed
estimation algorithm, referring to the inputs and outputs at different stages.
Step 1 Daily records of COVID–19 mortality are accumulated over the entire period at every Spanish Community. The resulting step cumulative curves are
interpolated at 265 temporal nodes, and cubic B–spline smoothed. Their
derivatives and logarithmic transforms are then computed.
Step 2 Our soft–input–data process is obtained from the spatial projection of
the outputs in Step 1 onto the compactly supported basis {ψp,$p , p =
1, . . . , P = 17}. We choose the tensorial product of Daubechies wavelet
bases. Here, for p = 1, . . . , 17, $p = (N (p), j(p), k(p)), whose components respectively provide the order of the Daubechies wavelet functions,
the resolution level, and the vector of spatial displacements, according to
the area occupied by each Spanish community (see, e.g., [21]).
Step 3 The choice N = 6 in (8) corresponds to 1.1304 value of the ratio between
the mean quadratic loss function and expected minimized empirical risk.
Hence 12 coefficients should be estimated. Note that the eigenvalues in
(8) are computed from the trigonometric basis.
Step 4 Under N = 6 in Step 3, the least–squares estimates of the 12 Fourier
coefficients are computed from (7), in terms of the soft–input–data process
obtained as output in Step 2.
Step 5 The regression residuals are then calculated from Step 4.
Step 6 The auto– and cross– covariance operators in (11) are computed from
the outputs of Step 5. The residual spatial linear correlation matrix is
then obtained from (12). The truncation scheme k(T ) = ln(T ) has been
adopted, with T = 265.
Step 7 The residual predictor (14) is computed from Step 6.
Step 8 100 bootstrap samples are generated from the empirical autocorrelation
projections. The bootstrap prior fitted suggests us to consider a scaled
beta probability density with shape parameters 14 and 13.
11

Step 9 Assuming a Gaussian scenario for our log–regression residuals, our constrained nonlinear multivariate objective function (15) is computed from
the prior proposed in Step 8.
Step 10 The maximize the objective function computed in Step 9, we implement an
hybrid genetic algorithm, constructed from ’gaoptimset’ MaLab function,
implemented with the ’HybridFcn’ option that handles to a function to
continuing optimization after the genetic algorithm terminates. This last
function applies quasi-Newton methodology in the optimization procedure,
involving an inverse Hessian matrix estimate.
Step 11 The soft–data based bayesian predictor (16) of the residual COVID–19
mortality log–risk is finally computed from the outputs in Step 10.
Step 12 Our multiple objective space–time predictor is obtained from Steps 4 and
11, by addition the regression and residual predictors, applying inverse
spatial wavelet transform.
bk (·) and B
bk (·), k =
Tables 1–2 below display the parameter estimates A
2π
has been considered, for k = 1, . . . , N = 6. In these
1, . . . , 6, where ϕk = 265
tables and below, the following Spanish Community (SC) codes appear: C1
for Andalucı́a; C2 for Aragón; C3 for Asturias; C4 for Islas Baleares; C5 for
Canarias; C6 for Cantabria; C7 for Castilla La Mancha; C8 for Castilla y León;
C9 for Cataluña; C10 for Comunidad Valenciana; C11 for Extremadura; C12 for
Galicia; C13 for Comunidad de Madrid; C14 for Murcia; C15 for Navarra; C16
for Paı́s Vasco, and C17 for La Rioja.
Bootstrap curve confidence intervals at confidence level 1 − α = 0.95, based
on 1000 bootstrap samples, are computed for the spatial mean, over the 17 Spanish Communities, of the curve regression predictors. Their construction is based
on the bias corrected and accelerated percentile method (I1 ); Normal approximated interval with bootstrapped bias and standard error (I2 ); basic percentile
method (I3 ), and bias corrected percentile method (I4 ) (see Figure 1 below).
b265 (p)), p = 1, . . . , 17, are
The minimized regression empirical risk values L265 (θ
displayed in Table 3.

12

bk (·), k = 1, . . . , 6, at the 17 SpanTable 1: Regression parameter estimates A
ish Communities
SC/PE
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12
C13
C14
C15
C16
C17

b1 (·)
A
3.6343
3.4345
3.2031
3.1445
3.1015
3.1347
4.0591
3.8032
4.5095
3.6321
3.2967
3.3454
4.8419
3.0941
3.2877
3.6870
3.2197

b2 (·)
A
-0.4814
-0.3923
-0.1364
-0.1118
-0.0693
-0.1397
-0.5487
-0.5500
-0.7435
-0.4685
-0.2274
-0.2122
-0.6790
-0.1037
-0.2598
-0.4302
-0.2071

b3 (·)
b4 (·)
b5 (·)
A
A
A
-0.0075 -0.0258 0.0189
0.0416 0.0265 -0.0709
-0.0088 0.0221 0.0430
0.0041 0.0337 0.0062
-0.0345 0.0352 -0.0112
0.0020 0.0300 -0.0061
-0.0907 0.0951 0.0992
-0.1007 0.0633 0.0139
-0.1134 0.1809 0.2231
-0.0540 0.0384 -0.0152
-0.0083 0.0553 0.0250
-0.0927 -0.0330 0.0724
-0.2455 0.0311 0.0554
0.0210 0.0141 -0.0016
-0.0524 0.0842 -0.0423
-0.0086 0.0078 -0.0027
0.0162 0.0079 0.0206

13

b6 (·)
A
0.0193
-0.0572
0.0289
0.0072
0.0003
-0.0002
0.0842
0.0277
0.2026
0.0011
0.0240
0.0679
0.0667
0.0041
-0.0348
-0.0017
0.0110

bk (·), k = 1, . . . , 6, at the 17 SpanTable 2: Regression parameter estimates B
ish Communities
SC/PE
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12
C13
C14
C15
C16
C17

b1 (·)
B
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

b2 (·)
B
-0.0052
-0.0367
-0.0531
-0.0074
0.0433
0.0018
-0.0365
0.0953
-0.1587
0.1118
0.0754
-0.1104
0.4654
0.0355
-0.0187
0.0025
0.0389

b3 (·)
B
-0.1330
-0.0998
-0.0074
-0.0284
-0.0438
-0.0174
-0.2451
-0.2389
-0.4054
-0.1579
-0.1138
-0.1338
-0.1302
-0.0560
-0.0021
-0.0707
-0.0270

14

b4 (·)
B
-0.0123
-0.0462
-0.0142
-0.0151
-0.0116
-0.0068
-0.1791
-0.0431
-0.2269
-0.0458
-0.0166
0.1330
-0.1602
0.0119
-0.0897
-0.0638
-0.0174

b5 (·)
B
0.0064
-0.0343
-0.0003
-0.0092
-0.0118
-0.0089
-0.0820
-0.0313
-0.1010
-0.0418
-0.0048
0.0761
-0.1061
0.0025
-0.0562
-0.0439
-0.0006

b6 (·)
B
-0.0195
-0.0107
0.0020
0.0012
0.0046
0.0000
0.0026
-0.0045
0.0047
-0.0220
0.0072
-0.0017
-0.0038
-0.0044
0.0134
-0.0267
0.0019

3500

3000

y-axis (Mean cumulative number of mortality cases)

3000

y-axis (Mean cumulative number of mortality cases)

2500

2500

2000
2000

1500
1500

1000

1000

500

500
0

0

−500
0

120

x-axis (Time. 265 temporal nodes)

x-axis (Time. 265 temporal nodes)

50

100

150

200

250

300

−500
0

100

y-axis (Mean mortality risk values)

100

50

100

150

200

250

300

y-axis (Mean mortality risk values)

80

80

60
60

40
40

20
20

0

0

x-axis (Time. 265 temporal nodes)

x-axis (Time. 265 temporal nodes)

−20
0
4

50

100

150

200

250

−20
0

300

4

y-axis (Mean log-risk values)

3

3

2

2

1

1

0

0

−1

−1

50

50

100

150

200

150

200

250

300

y-axis (Mean log-risk values)

x-axis (Time. 265 temporal nodes)

x-axis (Time. 265 temporal nodes)

−2
0

100

250

−2
0

300

50

100

150

200

250

300

Figure 1: At the top, COVID–19 mortality mean cumulative curve in Spain,
since March, 8, 2020 to May, 13, 2020 (continuous red line, 265 temporal
nodes), and bootstrap curve confidence intervals, at the left–hand–side, I1
(dashed blue lines) and I2 (dashed magenta lines), and at the right–hand–
side, I3 (dashed green lines) and I4 (dashed yellow lines). Plots at the
center and bottom reflect the same information respectively referred to the
mean intensity (spatial averaged COVID–19 mortality risk curve), and log–
intensity (spatial averaged COVID–19 mortality log–risk curve) curves in
Spain. All the confidence bootstrap intervals are computed at confidence
level 1 − α = 0.95, from 1000 bootstrap samples
15

b265 (p)), p = 1, . . . , 17
Table 3: Computed values L265 (θ
b265 (p))
L265 (θ
0.0155
0.0623
0.0559
0.0003

p=
0.0259
0.1642
0.1904
0.1238

1 . . . 17
0.0668
0.0883
0.0054

0.0408
0.2174
0.1602

0.0927
0.0313
0.1640

Figure 2 at the top displays the 1000 bootstrap sample values
P
1 X
b265 (p)),
L265 (ωi ) =
L265 (ωi , θ
P p=1

ωi ∈ Ω, i = 1, . . . , 1000,

of the spatial averaged minimized empirical quadratic risk in the trigonometric
regression. Note that the sample mean of these values is L = 0.0262, showing a good performance of the least–squares regression predictor, according to
the value T (265, 12) = 1.1304 obtained. The bootstrap histogram and the
corresponding approximation of the probability density function, computed from
L265 (ωi ), i = 1, . . . , 1000, are also plotted at the bottom of Figure 2.

Bootstrap confidence intervals for L265 have also been computed at level
1−α = 0.95, from 1000 and 10000 bootstrap samples. Table 4 displays these intervals respectively based on the bias corrected and accelerated percentile method
(I1 ); Normal approximated interval with bootstrapped bias and standard error
(I2 ); basic percentile method (I3 ); bias corrected percentile method (I4 ), and
Student–based confidence interval (I5 ).
The classical and Bayesian plug–in predictors of the residual COVID–19 mortality log–risk process at each one of the Spanish Communities are respectively
computed from equations (14) and (16) for P = 17.
Given the empirical spectral characteristics observed in the regularized approximation ρbk(T ) of ρ in (12), from the singular value decomposition of the
empirical operators in (11), our choice of the prior for the projections of ρ has
been a scaled, by factor 1/3, Beta prior with hyper–parameters apq = 14, and
bpq = 13, for p, q = 1, . . . , 17. The suitability of this data–driven choice, regarding localization of the mode, and the tails thickness, is illustrated in Figure
3. Specifically, at the right plot in Figure 3, both, the scaled Beta probability
density, with shape parameters 14 and 13 (red–square line), and the fitted probability density (blue–square line), from the generated bootstrap samples, based
16

0.14
y-axis (Sample values of spatially averaged minimum empirical regression risk)

0.13
0.12
0.11
0.1
0.09
0.08
0.07
0.06
0.05
x-axis (Number of the bootstrap sample)

0.04

0

100

200

300

400

500

600

800

900

1000

BOOTSTRAP PROBABILITY DENSITY

BOOTSTRAP HISTOGRAM
250

700

25

y-axis(Absolute frequencies of sample values of SAMERR)
x-axis (Sample values of SAMERR)

200

20

150

15

100

10

50

5

y-axis (Bootstrap probability density values)

x-axis (Sample values of SAMERR)

0
0.04

0.05

0.06

0.07

0.08

0.09

0.1

0.11

0.12

0.13

0
0.02

0.14

0.04

0.06

0.08

0.1

0.12

0.14

0.16

Figure 2: 1000 bootstrap samples have been generated of the spatially averaged minimum empirical regression risk (SAMERR). The corresponding
sample values are displayed at the top. The bootstrap histogram can be
found at the bottom–left–hand side. The bootstrap probability density is
plotted at the bottom–right–hand–side
Table 4: Bootstrap confidence intervals for L265 (confidence level 1−α = 0.95)
CI/S
I1
I2
I3
I4
I5

1000
[0.0593, 0.1222]
[0.0564, 0.1196]
[0.0584, 0.1215]
[0.0592, 0.1233]
[0.0484, 0.1281]

10000
[0.0594, 0.1236]
[0.0567, 0.1207]
[0.0579, 0.1217]
[0.0581, 0.1208]
[0.0494, 0.1215]

on the empirical projections of ρ, are displayed. Note that the observed range of
17

the empirical projections of ρ is well fitted, as one can see from the left plot in
Figure 3.

17X17=289 EMPIRICAL PROJECTIONS OF RHO

THEORETICAL AND EMPIRICAL PRIOR

0.75

7
AUTO− AND CROSS− CORRELATION (17 COMMUNITIES)

0.7

6

0.65

5

0.6

4

0.55

3

0.5

2

y-axis (Bootstrap and theoretical priors)

BASED ON 100 BOOTSRAT SAMPLES

0.45

1
x-axis (Sample values of the projections of the autocorrelation matrix)

0.4

0

50

100

150

200

250

0
0

300

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

x-axis (Sample values of the projections of the autocorrelation matrix)

Figure 3: At the left–hand side, empirical projections of the autocorrelation
operator ρ, reflecting temporal autocorrelation and cross–correlation between
the 17 Spanish Communities analyzed. At the right–hand side, the considered prior probability density (red squares) of a scaled, by factor 1/3, Beta
distributed random variable with shape parameters 14 and 13 is compared
with the bootstrap fitting of an empirical prior (blue squares)

Bootstrap confidence intervals I1 , . . . , I5 at level 1 − α = 0.95, for the
expected training standard error of the multivariate time series classical and
Bayesian residual COVID–19 mortality log–risk predictors, based on 1000 bootstrap samples, are displayed in Table 5:
Maps plotted in Figure 5 show the observed spatiotemporal evolution of
COVID–19 mortality risk, and its prediction, from the fitted curve trigonometric
regression model, and the subsequent classical and Bayesian time series analysis.

18

Table 5: Bootstrap confidence intervals for the expected training standard
error of the classical and Bayesian residual COVID–19 mortality log–risk
predictors (1 − α = 0.95)
CI/S
I1
I2
I3
I4
I5

day 10

day 13

day 16

day 19

Classical
[0.0474, 0.0597]
[0.0455, 0.0578]
[0.0463, 0.0588]
[0.0460, 0.0586]
[0.0421, 0.0563]

day 22

295

Bayesian
[0.0173, 0.0228]
[0.0167, 0.0220]
[0.0169, 0.0225]
[0.0172, 0.0226]
[0.0158, 0.0215]

day 10

day 13

day 16

day 19

day 22

80

80

40

day 25

day 28

day 31

day 34

40

day 37

day 25

day 28

day 31

day 34

day 37

20

day 40

day 43

day 46

day 49

day 55

day 58

day 61

day 64

day 52

20

10

day 40

day 43

day 46

day 49

day 55

day 58

day 61

day 64

day 52

5

day 13

day 16

day 19

day 22

0

295

day 10

day 13

day 16

day 19

day 22

80

day 28

day 31

day 34

40

day 37

day 25

day 28

day 31

day 34

day 37

20

day 40

day 43

day 46

day 49

day 55

day 58

day 61

day 64

day 52

295
80

40

day 25

10

5

0

day 10

295

20

10

day 40

day 43

day 46

day 49

day 55

day 58

day 61

day 64

5

day 52

10

5

0

0

Figure 4: COVID–19 mortality risk maps, since March, 8 to May, 13,
2020. Observed (left–hand–side) and estimated (right–hand side) maps, computed from trigonometric regression, combined with classical (first line) and
Bayesian (second line) residual predictors

5

An empirical comparative study

The ML regression models introduced in the Supplementary Material are applied to COVID–19 mortality analysis, and compared, via random k–fold cross–
validation and bootstrap estimators, with the multiple objective space–time forecasting approach presented. We distinguish two categories respectively referred
19

to the strong–sense (hard–data) and weak–sense (soft–data) definition of our
data set. Random k–fold (k = 5, 10) cross–validation, in terms of Symmetric
Mean Absolute Percentage Errors (SMAPEs), evaluates the performance of the
compared regression models, from hard– and soft–data. Bootstrap confidence
intervals, and probability density estimates of the spatially averaged SMAPEs are
also computed. Section 6 provides a data–driven model classification, based on
SMAPEs, in the two categories analyzed, from random k–fold cross–validation,
and the bootstrap estimation procedures applied.

5.1

Results from random k–fold cross–validation

After interpolation and cubic B-spline smoothing of our original data set, the logarithmic transform and linear scaling are applied. We held out the first ten points
and the last three, for each COVID–19 mortality log–risk curve, as an out of sample set. Our approach is implemented in the second–category from soft–data. In
this implementation, we consider N = 6, adopting the model selection criterion
given in Section 4 (see equation (8) and reference [14]). In the multivariate
time series classical and Bayesian prediction, our choice of k(T ) = k(265) = 8
provides a balance between k(T ) = [ln(T )]− = [ln(265)]− = 5, signing an agreement with the separation and velocity decay of the empirical eigenvalues of the
autocovariance operator, and the parameter value k(T ) = 9, controlling model
complexity according to the sample size T = 265. The random fluctuations observed at the k(T ) empirical projections of the spatial autocorrelation matrix ρ
are also well–fitted by our choice of the shape hyperparameters, characterizing
the prior Beta probability density.
Model fitting is evaluated in terms of the Symmetric Mean Absolute Percentage Errors (SMAPEs), given by, for P = 17, and T = 265,
\
T
ln(Λ
t )(ψp,ρp ) − ln(Λt )(ψp,ρp )
1X
 ,

T t=1 ln(Λ )(ψ ) + ln(Λ
\
/2
t
p,ρp
t )(ψp,ρp )

p = 1, . . . , P.

(17)

We have computed the mean of the SMAPEs obtained at each one of the
k iterations of the random k–fold cross–validation procedure. This validation
technique consists of random splitting the functional sample into a training and
validation samples at each one of the k iterations. Model fitting is performed
from the training sample, and the target outputs are defined from the validation
or testing sample. By running each model ten times and averaging SMAPEs,
we remove the fluctuations due to the random initial weights (for MLP and
BNN models), and the differences in the parameter estimation in all methods,
due to the random specification of the sample splitting in the random k–fold
cross–validation procedure.
20

The ten–running based random 10–fold cross–validation SMAPEs are displayed in Table 6, for the six ML techniques tested, GRNN, MLP, SVR, BNN,
RBF, and GP, when hard–data are considered (see also Table 3 of the Supplementary Material on random 5–fold cross–validation results). Table 7 provides
the ten–running based random 10–fold cross–validation results, from soft–data
category (see also Table 4 of the Supplementary Material on random 5–fold cross
validation results). The corresponding cross–validation results of the presented
approach from soft–data are displayed in Table 8.
ML model hyperparameter selection has been achieved by applying random
k–fold cross–validation (k = 5, 10). Our selection has been made from a suitable set of candidates. Specifically, the optimal numbers of hidden (NH) nodes
in the implementation of MLP and BNN have been selected from the candidate
sets [0, 1, 3, 5, 7, 9] and [1, 3, 5, 7, 9], respectively. The random cross–validation
results in both cases, k = 5, 10, lead to the same choice of the NH optimal value.
Namely, NH= 1 for MLP, and NH= 5 for BNN. The last one displays slight differences with respect to the values NH= 3, 7, in the random 10–fold cross–validation
implementation. In the same way, we have selected the respective spread β and
bandwidth h parameters in the RBF and GRNN procedures. Thus, after applying
random k–fold cross–validation, with k = 5, 10, the optimal values β = 2.5, and
h = 0.05 are obtained, from the candidate sets [2.5, 5, 7.5, 10, 12.5, 15, 17.5, 20]
and [0.05, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7], respectively (see Supplementary Material).
Better performance from hard–data is observed in linear SVR. In its implementation, automatic hyperparameter optimization from fitrsvm MatLab function is
applied. While, from the soft–data category, the best option corresponds to the
Gaussian kernel based nonlinear SVR model fitting (applying the same option
of automatic hyperparameter optimization, in the argument of fitrsvm MatLab
function). In the implementation of GP, we follow the same tuning procedure
for model selection. In this case, for both categories, we have selected Bayesian
cross-validation optimization (in the hyperparameter optimization argument of
the fitrgp MatLab function).
In all the results displayed, the SMAPE–MEAN (M.) and SMAPE–TOTAL
(T.) have been computed as performance measures, for comparing the ML models
tested, and our approach.

5.2

Bootstrap based classification results

For the ML regression models tested, in the hard– and soft–data categories,
bootstrap confidence intervals (1 − α = 0.95 confidence level) for the spatially
averaged SMAPEs, based on 1000 bootstrap samples, are constructed. Our approach requires the soft–data information to be incorporated. As before, the
computed bootstrap confidence intervals Ii , i = 1, . . . , 5, are respectively based
21

Table 6: Hard–data category . Averaged SMAPEs, based on 10 running
of random 10–fold cross–validation
SC(x10−2 ) GRNN
C1
0.1957
C2
0.6132
C3
0.1556
C4
0.0971
C5
0.2049
C6
0.1572
C7
0.4898
C8
0.0804
C9
0.7258
C10
0.2191
C11
0.1262
C12
0.5228
C13
0.3594
C14
0.1345
C15
0.6080
C16
0.2464
C17
0.0660
M.
0.2942
T.
5.0022

MLP
0.0777
0.1490
0.0473
0.0342
0.0457
0.0368
0.0698
0.0340
0.1976
0.0704
0.0530
0.1578
0.0647
0.0366
0.1523
0.0889
0.0370
0.0796
1.3528

SVR
0.0700
0.0663
0.0350
0.0135
0.0318
0.0177
0.0644
0.0171
0.0979
0.0556
0.0310
0.1341
0.0576
0.0209
0.1411
0.0709
0.0148
0.0553
0.9397

BNN
0.0594
0.0738
0.0303
0.0200
0.0370
0.0234
0.0590
0.0191
0.0812
0.0482
0.0395
0.1282
0.0579
0.0204
0.1141
0.0622
0.0222
0.0527
0.8959

RBF
0.0543
0.0680
0.0331
0.0182
0.0369
0.0233
0.0616
0.0211
0.0326
0.0471
0.0375
0.0940
0.0533
0.0194
0.0982
0.0568
0.0203
0.0456
0.7757

GP
0.0554
0.0654
0.0304
0.0211
0.0372
0.0247
0.0588
0.0177
0.0437
0.0463
0.0355
0.0993
0.0458
0.0207
0.1039
0.0594
0.0227
0.0463
0.7879

on the bias corrected and accelerated percentile method (I1 ); Normal approximated interval with bootstrapped bias and standard error (I2 ); basic percentile
method (I3 ); bias corrected percentile method (I4 ), and Student–based confidence interval (I5 ) (see Tables 9 and 10). The bootstrap histogram, and probability density of the spatially averaged SMAPEs are displayed in Figures 5 and
6, for the hard–data category, and in Figures 7, 8, and 9, for the soft–data category. The data–driven performance–based model classification results obtained
are discussed in Section 6.

22

Table 7: Soft–data category . Averaged SMAPEs, based on 10 running of
random 10–fold cross–validation
SC(x10−2 ) GRNN
C1
0.1545
C2
0.1844
C3
0.1029
C4
0.0432
C5
0.0610
C6
0.0260
C7
0.3750
C8
0.0764
C9
0.4894
C10
0.1680
C11
0.1537
C12
0.3689
C13
0.2848
C14
0.0367
C15
0.3618
C16
0.1773
C17
0.0884
M.
0.1854
T.
3.1524

MLP
0.0983
0.1730
0.1192
0.0286
0.0476
0.0217
0.2026
0.0482
0.3198
0.0815
0.0839
0.2558
0.1582
0.0226
0.2264
0.0835
0.0623
0.1196
2.0333

SVR
0.0666
0.0660
0.0481
0.0165
0.0258
0.0133
0.1095
0.0305
0.1753
0.0521
0.0436
0.1505
0.0968
0.0120
0.1201
0.0651
0.0210
0.0655
1.1129

23

BNN
0.0573
0.0749
0.0452
0.0158
0.0248
0.0140
0.0924
0.0300
0.1212
0.0462
0.0397
0.1249
0.0792
0.0143
0.1227
0.0545
0.0231
0.0577
0.9801

RBF
0.0234
0.0277
0.0273
0.0124
0.0144
0.0124
0.0307
0.0262
0.0229
0.0252
0.0199
0.0401
0.0240
0.0106
0.0317
0.0264
0.0125
0.0228
0.3877

GP
0.0312
0.0301
0.0274
0.0123
0.0149
0.0125
0.0399
0.0187
0.0372
0.0290
0.0219
0.0490
0.0320
0.0104
0.0522
0.0318
0.0136
0.0273
0.4642

Table 8: Our approach. Averaged SMAPEs, based on 10 running of random 10–fold cross–validation, for testing trigonometric regression combined
with Classical (C.) and Bayesian (B.) residual analysis
SC

C. k10

B. k10

C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
C11
C12
C13
C14
C15
C16
C17
M.
T.

0.0024
0.0019
0.0016
0.0017
0.0023
0.0018
0.0017
0.0016
0.0013
0.0019
0.0017
0.0016
0.0020
0.0026
0.0023
0.0015
0.0022
0.0019
0.0321

0.7106(10)−3
0.4003(10)−3
0.6797(10)−3
0.4367(10)−3
0.6530(10)−3
0.5854(10)−3
0.6341(10)−3
0.6593(10)−3
0.5979(10)−3
0.6954(10)−3
0.5444(10)−3
0.5016(10)−3
0.4832(10)−3
0.6544(10)−3
0.6616(10)−3
0.7134(10)−3
0.6781(10)−3
0.60524(10)−3
0.0103

24

Table 9: Hard–data category . Bootstrap confidence intervals (1 − α =
0.95) for the spatially averaged SMAPEs from 1000 bootstrap samples (T =
265, P = 17)
CI/ML
I1
I2
I3
I4
I5
CI/ML
I1
I2
I3
I4
I5
CI/ML
I1
I2
I3
I4
I5

GRNN
MLP
[2.1(10)−3 , 4.1(10)−3 ]
[0.5(10)−3 , 1(10)−3 ]
[2(10)−3 , 3.9(10)−3 ]
[0.4776(10)−3 , 0.9483(10)−3 ]
[2(10)−3 , 4(10)−3 ]
[0.4746(10)−3 , 0.9713(10)−3 ]
−3
−3
[2(10) , 4(10) ]
[0.5118(10)−3 , 0.9878(10)−3 ]
[1.7(10)−3 , 3.9(10)−3 ]
[0.2780(10)−3 , 0.9244(10)−3 ]
SVR
BNN
[0.3682(10)−3 , 0.7219(10)−3 ] [0.3720(10)−3 , 0.6659(10)−3 ]
[0.3516(10)−3 , 0.6763(10)−3 ] [0.3587(10)−3 , 0.6379(10)−3 ]
[0.3493(10)−3 , 0.6770(10)−3 ] [0.3668(10)−3 , 0.6509(10)−3 ]
[0.3508(10)−3 , 0.6865(10)−3 ] [0.3654(10)−3 , 0.6379(10)−3 ]
[0.3050(10)−3 , 0.6661(10)−3 ] [0.3099(10)−3 , 0.6335(10)−3 ]
RBF
GP
−3
−3
[0.3260(10) , 0.5310(10) ] [0.3243(10)−3 , 0.5350(10)−3 ]
[0.3155(10)−3 , 0.5159(10)−3 ] [0.3065(10)−3 , 0.5126(10)−3 ]
[0.3140(10)−3 , 0.5270(10)−3 ] [0.3095(10)−3 , 0.5188(10)−3 ]
[0.3247(10)−3 , 0.5338(10)−3 ] [0.3152(10)−3 , 0.5222(10)−3 ]
[0.2677(10)−3 , 0.5141(10)−3 ] [0.2505(10)−3 , 0.5046(10)−3 ]

25

Table 10: Soft–data category . Bootstrap confidence intervals (1 − α =
0.95) for the spatially averaged SMAPEs from 1000 bootstrap samples (T =
265, P = 17)
CI/ML
I1
I2
I3
I4
I5
CI/ML
I1
I2
I3
I4
I5
CI/ML
I1
I2
I3
I4
I5
CI/OA
I1
I2
I3
I4
I5

GRNN
[1.3(10)−3 , 2.6(10)−3 ]
[1.3(10)−3 , 2.6(10)−3 ]
[1.3(10)−3 , 2.7(10)−3 ]
[1.3(10)−3 , 2.6(10)−3 ]
[1(10)−3 , 2.7(10)−3 ]
SVR
[0.4096(10)−3 , 0.8221(10)−3 ]
[0.3764(10)−3 , 0.7763(10)−3 ]
[0.3900(10)−3 , 0.7889(10)−3 ]
[0.4108(10)−3 , 0.7805(10)−3 ]
[0.3105(10)−3 , 0.7818(10)−3 ]
RBF
[0.1794(10)−3 , 0.2478(10)−3 ]
[0.1754(10)−3 , 0.2474(10)−3 ]
[0.1785(10)−3 , 0.2485(10)−3 ]
[0.1743(10)−3 , 0.2494(10)−3 ]
[0.1616(10)−3 , 0.2542(10)−3 ]
Classical
[2.2(10)−3 , 3.7(10)−3 ]
[2.1(10)−3 , 3.3(10)−3 ]
[2.1(10)−3 , 3.3(10)−3 ]
[2.2(10)−3 , 3.4(10)−3 ]
[1.8(10)−3 , 3.2(10)−3 ]

26

MLP
[0.6(10)−3 , 1.3(10)−3 ]
[0.6(10)−3 , 1.3(10)−3 ]
[0.6(10)−3 , 1.3(10)−3 ]
[0.7(10)−3 , 1.3(10)−3 ]
[0.5(10)−3 , 1.3(10)−3 ]
BNN
[0.3588(10)−3 , 0.6177(10)−3 ]
[0.3433(10)−3 , 0.6053(10)−3 ]
[0.3454(10)−3 , 0.6037(10)−3 ]
[0.3559(10)−3 , 0.5988(10)−3 ]
[0.3003(10)−3 , 0.6129(10)−3 ]
GP
[0.2095(10)−3 , 0.3248(10)−3 ]
[0.2065(10)−3 , 0.3215(10)−3 ]
[0.2079(10)−3 , 0.3262(10)−3 ]
[0.2091(10)−3 , 0.3258(10)−3 ]
[0.1941(10)−3 , 0.3232(10)−3 ]
Bayesian
[0.2943(10)−3 , 0.5177(10)−3 ]
[0.2802(10)−3 , 0.4854(10)−3 ]
[0.2833(10)−3 , 0.4884(10)−3 ]
[0.2900(10)−3 , 0.5124(10)−3 ]
[0.2418(10)−3 , 0.4664(10)−3 ]

BOOTSTRAPPED PROBABILITY DENSITY GRNN

BOOTSTRAPPED HISTOGRAM GRNN
250

800

y-axis (Absolute frequencies)

y-axis (Bootstrap probability density values)

700

200
600
500

150

400

100

300
200

50
100
0

0
1.5

2

2.5

3

3.5

4

4.5

x-axis (Sample values of spatially averaged GRNN SMAPEs)

5

1

1.5

2.5

3

3.5

4

4.5

5

5.5
−3

x 10

x 10

BOOTSTRAPPED PROBABILITY DENSITY MLP

BOOTSTRAPPED HISTOGRAM MLP
250

2

x-axis (Sample values of spatially averaged GRNN SMAPEs)

−3

3500 y-axis (Bootstrap probability density values)

y-axis (Absolute frequencies)

3000
200
2500
150

2000
1500

100
1000
50
500

0

4

5

6

7

8

9

10

11

12

x-axis (Sample values of spatially averaged MLP SMAPEs)

0
2

13

4

6

8

10

12

14

x-axis (Sample values of spatially averaged MLP SMAPEs)

−4

x 10

−4

x 10

BOOTSTRAPPED PROBABILITY DENSITY SVR

BOOTSTRAPPED HISTOGRAM SVR
5000

250 y-axis (Absolute frequencies)

y-axis (Bootstrap probability density values)

4500
4000

200

3500
3000

150

2500
100

2000
1500

50

1000
500

0

2

3

4

5

6

7

x-axis (Sample values of spatially averaged SVR SMAPEs)

0
2

8
−4

x 10

3

4

5

6

7

8

x-axis (Sample values of spatially averaged SVR SMAPEs)

9

10
−4

x 10

Figure 5: Hard–data category . From 1000 bootstrap samples, spatially averaged SMAPEs histograms and probability densities are plotted,
for GRNN (top), MLP (center), and linear SVR (bottom)

27

BOOTSTRAPPED PROBABILITY DENSITY BNN

BOOTSTRAPPED HISTOGRAM BNN
250

6000

y-axis (Absolute frequencies)

y-axis (Bootstrap probability density values)

5000

200
4000

150
3000

100
2000

50

0

1000

0

2

3

4

5

6

7

x-axis (Sample values of spatially averaged BNN SMAPEs)

8

3

4

5

6

7

8

x-axis (Sample values of spatially averaged BNN SMAPEs)

−4

x 10

x 10

BOOTSTRAPPED PROBABILITY DENSITY RBF

BOOTSTRAPPED HISTOGRAM RBF
250

2

−4

8000

y-axis (Bootstrap probability density values)

y-axis (Absolute frequencies)

7000
200

6000
5000

150

4000
100

3000
2000

50

1000
0

3

4

5

x-axis (Sample values of spatially averaged RBF SMAPEs)

0

6

2

2.5

−4

x 10

3.5

4

4.5

5

5.5

6

6.5
−4

x 10

BOOTSTRAPPED PROBABILITY DENSITY GP

BOOTSTRAPPED HISTOGRAM GP
250

3

x-axis (Sample values of spatially averaged RBF SMAPEs)

7000

y-axis (Absolute frequencies)

y-axis (Bootstrap probability density values)

6000
200

5000
150

4000
3000

100

2000
50

1000
0

3

4

5

x-axis (Sample values of spatially averaged GP SMAPEs)

0

6
−4

x 10

2

2.5

3

3.5

4

4.5

5

5.5

x-axis (Sample values of spatially averaged GP SMAPEs)

6

6.5
−4

x 10

Figure 6: Hard–data category . From 1000 bootstrap samples, spatially
averaged SMAPEs histograms and probability densities are plotted, for BNN
(top), RBF (center), and GP (bottom)

28

BOOTSTRAPPED PROBABILITY DENSITY GRNN (SD)

BOOTSTRAPPED HISTOGRAM GRNN (SD)
250

1200 y-axis (Bootstrap probability density values)

y-axis (Absolute frequencies)

1000
200

800
150

600
100

400
50

200

0
0.5

1

1.5

2

2.5

0
0.5

3

x-axis (Sample values of spatially averaged GRNN SMAPEs)

BOOTSTRAPPED HISTOGRAM MLP (SD)
250

2500

200

2000

150

1500

100

1000

50

500

0.6

0.8

1

1.2

1.4

x-axis (Sample values of spatially averaged MLP SMAPEs)

2

2.5

3

3.5
−3

x 10

y-axis (Bootstrap probability density values)

0
0.2

1.6

0.4

−3

x 10

0.6

0.8

1

1.2

1.4

x-axis (Sample values of spatially averaged MLP SMAPEs)

1.6

1.8
−3

x 10

BOOTSTRAPPED PROBABILITY DENSITY SVR (SD)

BOOTSTRAPPED HISTOGRAM SVR (SD)
300

1.5

BOOTSTRAPPED PROBABILITY DENSITY MLP (SD)

y-axis (Absolute frequencies)

0
0.4

1

x-axis (Sample values of spatially averaged GRNN SMAPEs)

−3

x 10

4500

y-axis (Absolute frequencies)

y-axis (Bootstrap probability density values)

4000
250

3500
200

3000
2500

150

2000
100

1500
1000

50

500
0
3

4

5

6

7

8

9

x-axis (Sample values of spatially averaged SVR SMAPEs)

10

0
2

11
−4

x 10

3

4

5

6

7

8

9

10

x-axis (Sample values of spatially averaged SVR SMAPEs)

11

12
−4

x 10

Figure 7: Soft–data category . From 1000 bootstrap samples, spatially averaged SMAPEs histograms and probability densities are plotted, for GRNN
(top), MLP (center) and non–linear SVR (bottom)

29

BOOTSTRAPPED HISTOGRAM BNN (SD)
250

BOOTSTRAPPED PROBABILITY DENSITY BNN (SD)
6000

y-axis (Absolute frequencies)

y-axis (Bootstrap probability density values)

5000

200

4000

150
3000

100
2000

50

0

1000

2

3

4

5

6

7

0

8

x-axis (Sample values of spatially averaged BNN SMAPEs)

2

3

4

5

6

7

x-axis (Sample values of spatially averaged BNN SMAPEs)

−4

x 10

BOOTSTRAPPED HISTOGRAM RBF (SD)

8
−4

x 10

BOOTSTRAPPED PROBABILITY DENSITY RBF (SD)
4

300

2.5

y-axis (Absolute frequencies)

x 10

y-axis (Bootstrap probability density values)

250
2

200
1.5

150
1

100
0.5

50

0
1.4

1.6

1.8

2

2.2

2.4

2.6

0
1.2

2.8

x-axis (Sample values of spatially averaged RBF SMAPEs)

−4

x 10

1.6

1.8

2

2.2

2.4

2.6

x-axis (Sample values of spatially averaged RBF SMAPEs)

2.8

3
−4

x 10

BOOTSTRAPPED PROBABILITY DENSITY GP (SD)

BOOTSTRAPPED HISTOGRAM GP (SD)
250

1.4

14000
y-axis (Absolute frquencies)

y-axis (Bootstrap probability density values)

12000
200

10000
150

8000
6000

100

4000
50

2000
0
1.6

1.8

2

2.2

2.4

2.6

2.8

3

3.2

x-axis (Sample values of spatially averaged GP SMAPEs)

3.4

3.6
−4

x 10

0
1

2

3

x-axis (Sample values of spatially averaged GP SMAPEs)

−4

x 10

Figure 8: Soft–data category . From 1000 bootstrap samples, spatially
averaged SMAPEs histograms and probability densities are plotted, for BNN
(top), RBF (center) and GP (bottom)

30

CLASSICAL BOOTSTRAPPED PROBABILITY DENSITY (SD)

CLASSICAL BOOTSTRAPPED HISTOGRAM (SD)
300

1400

y-axis (Absolute frequencies)

y-axis (Bootstrap probability density values)

1200

250

1000
200

800
150

600
100

400
50

200

0
1.5

2

2.5

3

3.5

4

4.5

0
1.5

5

x-axis (Sample values of spatially averaged classical SMAPEs)

−3

x 10

2.5

3

3.5

4

4.5

x-axis (Sample values of spatially averaged classical SMAPEs)

5
−3

x 10

BAYESIAN BOOTSTRAPPED PROBABILITY DENSITY (SD)

BAYESIAN BOOTSTRAPPED HISTOGRAM (SD)
300

2

8000

y-axis (Absolute frequencies)

y-axis (Bootstrap probability density values)

7000
250

6000
200

5000
150

4000
3000

100

2000
50

1000
0

2

2.5

3

3.5

4

4.5

5

5.5

x-axis (Sample values of spatially averaged bayesian SMAPEs)

6

6.5
−4

x 10

0
1

2

3

4

5

6

x-axis (Sample values of spatially averaged bayesian SMAPEs)

7
−4

x 10

Figure 9: Soft–data category . From 1000 bootstrap samples, spatially averaged SMAPEs histograms and probability densities are plotted, for trigonometric regression, combined with empirical–moment based classical (top),
and Bayesian (bottom) residual prediction

31

6

Final comments

One can observe the agreement between the respective performance–based model
classification results, obtained from random k–fold cross–validation, and bootstrap estimation in Sections 5.1 and 5.2. In the hard–data category, the best
performance is displayed by RBF and GP. Similar bootstrapping characteristics
are observed for BNN and SVR, with slightly larger values of spatially averaged
SMAPEs, reflected in the location of the mode, in the histograms and probability
densities displayed in Figures 5 and 6. These four regression methodologies show
a similar degree of variability, regarding the spatially averaged SMAPEs sample
values. A higher variability than RBF, GP, BNN and SVR is displayed by the
bootstrap sample values of spatially averaged SMAPEs in MLP validation. MLP
bootstrapped mode is also slightly shifted to the right. The worst performance
corresponds to GRNN (see also Table 6). In the soft–data category, where our
approach is incorporated to the empirical comparative study, almost the same
empirical ML model ranking holds. Some differences are found in the bootstrap
confidence intervals, and histogram and probability densities computed. For instance, GRNN seems to be favored by soft–data category, while MLP displays
worse performance in this category. Hence, smaller differences between GRNN
and MPL are displayed in the soft–data category. A slightly improvement in the
soft–data category of BNN relative to SVR is observed, preserving almost the
same performance. RBF and GP display better performance in the soft–data category, being RFB a bit superior to GP in this category (see Table 10, and Figure
8). The trigonometric regression, and multivariate time series residual prediction approach based on the empirical moments displays similar results to GRNN,
with slightly better performance of GRNN, observed in the bootstrap intervals
and histogram/probability density (see Figures 7 and 9). However, as given in
Figures 8 and 9, the trigonometric regression and Bayesian residual prediction
presents almost the same ‘performance as BNN, with some slightly better probability distribution features of our approach respect to BNN (see also bootstrap
intervals). Our approach is less affected by the random splitting of the sample,
in the implementation of the random k–fold cross validation procedure, since a
dynamical spatial residual model is fitted in a second (objective) step. Thus,
the proposed multivariate time series classical and Bayesian regression residual
modeling fits the short–term spatial linear correlations displayed by the soft–data
category. However, the price we pay for increasing model complexity is reflected
in the resulting SMAPEs based random k–fold and bootstrap model classification
results obtained.
The spatial component effect is reflected in Tables 6 (hard–data), where spatial heterogeneities displayed by random 10-fold cross–validation
SMAPEs errors are observed (see also Table 3 in the Supplementary Material).
32

While Table 7 (see also Table 4 in the Supplementary Material) reveals the
benefits obtained in some of the ML regression models tested from soft–data
information. Particularly, in this category, possible spatial linear correlations are
incorporated to the analysis, in terms of soft–data.

Supplementary material
We briefly describe the implementation of ML models in the hard– and soft–data
categories.

Multilayer Perceptron (MLP)
MLP shares the philosophy of nonlinear regression, in terms of a link function g,
the hidden node output, defining the following approximation of the response:
yb = η0 +

NH
X

ηk g(βkT x),

(18)

k=1

from the input vector xT = (1, x) augmented with 1, and the weight vector βk associated with the kth hidden node, k = 1, . . . , N H, defining β =
1
T
T
(β1T , . . . , βN
H ) . Usually, the logistic function g(u) = 1+exp(−u) is considered.
The hidden node outputs are also weighted by the components of the vector
b is usually referred as the network output. MLP
(η0 , . . . , ηN H ). In this context, y
allows the approximation of any given continuous function on a compact set,
from a given network with a finite number of hidden nodes. Optimization algorithms are applied to obtain the weights from the least–squares loss function.
From (18), our implementation of MLP from soft–data is given by
ybm = yb(xm ) = ln (λ\
t+1 ) (φm ) = η0 +

NH
X

ηk g(βkT xm ),

(19)

k=1

where, for m = 1, . . . , M (T ),
xm = (ln (λt ) (φm ), . . . , ln (λt+1−j0 ) (φm ))T .

(20)

Parameter j0 refers to the number of temporal lags incorporated in the prediction.
The truncation parameter M (T ) plays a similar role to parameter k(T ) in the
paper. Here, T denotes the number of temporal training nodes.

33

Radial Basis Function Neural Network (RBF)
RBF works with node functions, depending on a center and scale parameters,
fitting the local smoothness of the response. Specifically, from an initial blank
network, the nodes are sequentially added, around the training pattern, until an
acceptable error is reached. All the output layer weights are then recomputed
using the least squares formula. Gaussian functions have been widely selected
as node functions. Particularly, in this case, our implementation from soft–data
has been achieved from the formula: For m = 1, . . . , M (T ),
ybt+1 (xm ) = ln (λ\
t+1 ) (φm ) =

NH
X


ηj exp

j=1

kxm − cj k2
β2


(21)

where, as usual, the weight parameters ηj , j = 1, . . . , N H, define the linear
combination of radial basis functions. Here, the scalar spread parameter β, and
the vector parameters cj , j = 1, . . . , N H, respectively provide the width, and
the centers of the node functions. The input vectors are defined as in (20).

Support Vector Regression (SVR)
SVR implementation involves a loss function leading to a balance between model
complexity and precision (accurate prediction). A bias parameter b is also considered in its formulation as reflected in the following equation:
y = f (x) = β T x + b

(22)

M
X
1
2
|ym − f (xm )| ,
L = kβk + C
2
m=1

(23)

where the loss function

is considered. Here, xm and ym respectively denote the mth training input vector
and the target output, for m = 1, . . . , M, and
|ym − f (xm )| = max {0, |ym − f (xm )| − } .
Thus, the errors below  are not penalized. The solution to the optimization
problem associated with the loss function (23) is obtained from the corresponding gradient of the Lagrangian function, involving Lagrange multipliers, that
determine the optimal weights from the training data points.

34

From equation (22), in the soft–data category, we solve the constrained optimization problem:
yt+1 (xm ) = f (xm ) = β T xm + b
M (T )
j0
X
X
1 T
ln(λt+1−j )(φm )βj − b
L= β β+C
ln (λt+1 ) (φm ) −
2
m=1
j=1



M (T )

X
1
= βT β + C
|ym − f (xm )| ,
2
m=1

(24)

where, for m = 1, . . . , M (T ), the imput vector xm is defined as in (20).

Bayesian Neural Network (BNN)
The design of BNN involves Bayesian estimation, and the concept of regularization. The network parameters or weights are considered random variables,
following a prior probability distribution. Smooth fits are usually favored in the
selection of the prior probability of the weights to reduce model complexity.
The posterior probability distribution of the weights is obtained after data are
observed. The network prediction is then computed. Specifically, the optimal
prediction is obtained by minimizing the following expression:
J = νEO + (1 − ν)EW ,

(25)

where EO is the sum of the square errors in the network output, based on the
posterior distribution of the parameters, EW represents the sum of the squares of
the weights or the network parameters, and ν ∈ (0, 1) denotes the regularization
parameter. Let L be the number of parameters. An L–dimensional Gaussian
prior probability distribution is usually assumed for the network parameters with
1
zero–mean and variance–covariance matrix 2(1−ν)
IL×L , with IL×L denoting the
L × L identity matrix. Thus,


1−ν
p(w) =
π

L/2
exp (−(1 − ν)EW (w)) .

(26)

This prior puts more weight onto small network parameter values close to zero.
The posterior probability density, given the observed data O = o, and the value
ν of the regularization parameter, is defined as
p(w/o, ν) =

p(o/w, ν)p(w/ν)
.
p(o/ν)
35

(27)

Considering that the errors are also Gaussian distributed, the conditional
probability of the data O given the parameters ν and w, is obtained as
p(o/w, ν) =

 ν M/2
π

exp (−νEO (o)) ,

(28)

where M denotes the number of training data points. From equations (26)–(28),
p(w/o, ν) = c exp (−J(w, o, ν)) ,

(29)

where c denotes the normalizing constant. The conditional probability of the
parameter ν given de observed data O = o is also computed under a Bayesian
framework as
P (o/ν)p(ν)
.
(30)
p(ν/o) =
p(o)
Equations (29) and (30) are maximized to obtain the optimal weights and
the regularization parameter ν, respectively.
In our soft–data implementation from (20), the corresponding optimization
problem is formulated by conditioning to the empirical spatial projections. Note
that, in the selected Gaussian prior probability framework, the error projections
are also Gaussian, and our choice of the function basis, diagonalizing the empirical autocovariance operator of the errors, leads to a projected error vector with
independent Gaussian components, suitable normalized by the empirical eigenvalues. Hence, optimization from equations (29) and (30) can be implemented
in a similar way to hard–data BNN.

Generalized Regression Neural Network (GRNN)
GRNN is based on kernel regression. The kernel estimator is computed from
the weighted sum of the observed responses, or target outputs associated with
the training data points in a neighborhood of the objective data point x, where
prediction must be computed. Thus, the training data points are selected in
the vicinity of the given objective point x. Specifically, the following formula is
applied in the approximation of the response value at the point x :


kx−xj k
T
K
X
h

 yj ,
(31)
yb(x) =
PT
kx−xl k
K
j=1
l=1
h
where yj is the target output for training data point xj , for j = 1, . . . , T, and
K is the kernel function. Usually an isotropic rapidly decreasing√kernel function
is considered, e.g., the Gaussian kernel K(u) = exp (−u2 /2) / 2π constitutes
36

a common choice. The bandwidth parameter h defines the smoothness of the
fit. Thus, h controls the size of the smoothing region. Hence, large values of
h correspond to a stronger smoothing than the smallest values allowing a larger
degree of local variation. In our soft–data implementation of (31), denote by
ΦM (T ) (H), the subspace of H obtained by projection of functions in H onto
T
{φ1 , . . . , φM (T ) }, and yt = ln(λt )(φ1 ), . . . , ln(λt )(φM (T ) ) , t ≥ 1, then,


kyt −yt−j kΦ
M (T ) (H)
K
j0 −1
h
X
 yt−j+1 ,

(32)
yd
=
t+1
kyt −yt−l kΦ
Pj0 −1
M (T ) (H)
j=1
l=1 K
h
where, as before, parameter j0 refers to the number of temporal lags incorporated
in the prediction.

Gaussian Processes (GP)
A good performance is usually observed in the implementation of GP regression,
based on the multivariate normal probability distribution assumption, characterizing the observed responses at the different training data points. Specifically,
we consider the observation model
Y = Z + ,

(33)

where the additive noise vector  is independent of Z, and has independent
and identically distributed zero–mean Gaussian components with variance σ2 .
A multivariate normal distribution of the random vector Z ∼ N (0, Σ), with
covariance matrix ΣZ (X, X) is assumed. This matrix provides the variances and
covariances between the function values Z(xi ), and Z(xj ), i, j = 1, . . . , N, at
the training data points X = (x1 , . . . , xN ). The conditional Gaussian distribution
of Z, given Y, leads to the solution to the inverse estimation problem (33). Thus,
the estimation of Z, for a given input vector x? , is obtained as


b x? = E [Z/y, x? , X] = ΣZ (x? , X) ΣZ (X, X) + σ 2 I −1 y.
Z
(34)

In the soft–data category, an alternative multivariate implementation is achieved
in terms of projection ΦM (T ) onto {φ1 , . . . , φM (T ) }. Hence, ΣZ is replaced by
the matrix covariance operator
 ?

ΦM (T ) R1,1 ΦM (T ) . . . Φ?M (T ) R1,T ΦM (T )
?
 Φ?

 M (T ) R2,1 ΦM (T ) . . . ΦM (T ) R2,T ΦM (T ) 
X,X
RZ = 
,
..
..


.
...
.
Φ?M (T ) RT,1 ΦM (T ) . . .
37

Φ?M (T ) RT,T ΦM (T )

associated with the T temporal training nodes considered, and the projection operator ΦM (T ) onto {φ1 , . . . , φM (T ) }, involved in the definition of our training soft–
data points X. Here, Ri,j = E [[ln(Λi ) − E[ln(Λi )]] ⊗ [ln(Λj ) − E[ln(Λj )]]] ,
i, j = 1, . . . , T, define the autocovariance and cross-covariance operators of the
functional values at the training data points. In this case, σ2 I becomes the diagonal matrix autocovariance operator diag (R0, ) of the H T –valued innovation
process  = (1 , . . . , T ). That is,


b x? = E Z/y, x? , X, ΦM (T )
Z
h
i−1
= RZx? ,X RZX,X + Φ?M (T ) diag (R0, ) ΦM (T )
ΦM (T ) (y).

Observed hard– and soft–data ML SMAPE sample values
The Symmetric Mean Absolute Percentage Errors (SMAPEs) associated with
ML estimation results, based on the overall functional sample, from hard– and
soft–data, are respectively displayed in Tables 11 and 12. See also Figures 10–
13 below, where the observed and estimated COVID–19 mortality log–risk and
cumulative cases curves are respectively displayed.

38

Andalucía

Aragón

Asturias

Islas Baleares
3.2

3.6

3.15

3.2

3.6

3.4

3.4

3.2

Cantabria

3.25

3.25

3.3

4
3.8

Canarias

3.25

3.8

3.1
3.1

3.2

3.2

3.15

3.15
3.1

3.1

3.05

3.05

3.05
20

40

60

20

Castilla La Mancha

40

60

20

Castilla y León

40

60

Cataluña

20

40

60

60

3.8
3.6

3.4

3.4
3.2

3.6
3.2

3.5

40

Galicia

3.6

3.8

4

20

Extremadura

4

4.5

4

3.5

60

4.2
5

4

40

C. Valenciana

4.5

4.5

20

3

3.4
3.5

20

40

60

20

C. Madrid

40

60

20

Murcia

40

60

20

Navarra

60

20

País Vasco

3.6

5.5

40

40

60

3.4

4

5

3.8

3.2

3.6

3.1
4.5

3.3
3.2
3.1

40

60

60

3.4

3
20

40

Log-int. obs.
GRNN est.
MLP est.
SVR est.
BNN est.
RBF est.
GP est.

3.2
3.4

20

La Rioja

20

40

60

20

40

60

20

40

60

20

40

60

Figure 10: Hard–data category . Observed and estimated COVID–19
mortality log–risk curves, from the implementation of Generalized Regression Neural Network (GRNN), Multilayer Perceptron (MLP), Support Vector
Regression (SVR), Bayesian Neural Network (BNN), Radial Basis Function
Neural Network (RBF), and Gaussian Processes (GP)

39

Andalucía

Aragón

Asturias

1000

Islas Baleares

300

800
600

100
100

100

500

50

100
200
0
20

40

60

0

Castilla La Mancha

20

40

60

0
0

Castilla y León

20

40

60

40

60

Extremadura

0

20

40

60

Galicia

600

800

300

600

200

400

200

400

1000

100

200

0
60

60

400

2000

40

40

1000

500

20

20

3000

1000

0

0
0

1200

4000
1000

20

C. Valenciana

5000
2000

0
0

Cataluña

1500

0

50

50

0
0

Cantabria
200
150

150

200

400

Canarias
150

200

0
0

C. Madrid

20

40

60

0

Murcia

20

40

60

0

Navarra

20

40

60

0
0

País Vasco

20

40

60

0

20

40

60

La Rioja

1400

8000
6000

300

1200

400
100

Cum. obs.
GRNN est.
MLP est.
SVR est.
BNN est.
RBF est.
GP est.

1000
200

800
4000

600

200

50

100

400

2000

200
0
0

20

40

60

0
0

20

40

60

0
0

20

40

60

0

20

40

60

0

20

40

60

Figure 11: Hard–data category . Observed and estimated COVID–19 mortality cumulative cases curves from the implementation of Generalized Regression Neural Network (GRNN), Multilayer Perceptron (MLP), Support
Vector Regression (SVR), Bayesian Neural Network (BNN), Radial Basis
Function Neural Network (RBF), and Gaussian Processes (GP)

40

Table 11: Hard–data. As indicated, displayed values must be multiplied
by 10−2
SC (x10−2 ) GRNN
C1
0.1964
C2
0.6117
C3
0.1565
C4
0.0969
C5
0.2044
C6
0.1571
C7
0.4889
C8
0.0808
C9
0.7231
C10
0.2185
C11
0.1255
C12
0.5216
C13
0.3584
C14
0.1342
C15
0.6064
C16
0.2456
C17
0.0655
M.
0.2936
T.
4.9916

MLP
0.0611
0.1172
0.0375
0.0314
0.0427
0.0319
0.0545
0.0273
0.1976
0.0526
0.0487
0.1666
0.0592
0.0286
0.1470
0.0681
0.0356
0.0710
1.2078

SVR
0.0665
0.0588
0.0328
0.0129
0.0290
0.0161
0.0619
0.0161
0.0850
0.0532
0.0298
0.1210
0.0548
0.0201
0.1307
0.0674
0.0147
0.0512
0.8707

41

BNN
0.0535
0.0678
0.0284
0.0191
0.0356
0.0230
0.0583
0.0180
0.1102
0.0446
0.0350
0.1022
0.0613
0.0202
0.0931
0.0573
0.0207
0.0499
0.8480

RBF
0.0483
0.0609
0.0300
0.0167
0.0334
0.0217
0.0569
0.0189
0.0318
0.0428
0.0338
0.0870
0.0490
0.0182
0.0864
0.0514
0.0181
0.0415
0.7053

GP
0.0490
0.0567
0.0273
0.0185
0.0328
0.0218
0.0503
0.0153
0.0352
0.0415
0.0316
0.0887
0.0412
0.0180
0.0927
0.0532
0.0200
0.0408
0.6936

Table 12: Soft–data. As indicated, displayed values must be multiplied by
10−2
SC (x10−2 ) GRNN
C1
0.1526
C2
0.1831
C3
0.1024
C4
0.0431
C5
0.0610
C6
0.0260
C7
0.3734
C8
0.0762
C9
0.4850
C10
0.1663
C11
0.1533
C12
0.3641
C13
0.2827
C14
0.0361
C15
0.3590
C16
0.1759
C17
0.0878
N.
0.1840
T.
3.1282

MLP
0.0857
0.1393
0.0804
0.0212
0.0362
0.0167
0.1301
0.0435
0.2467
0.0607
0.0540
0.2325
0.1350
0.0197
0.1843
0.0754
0.0352
0.0939
1.5966

SVR
0.0592
0.0619
0.0457
0.0157
0.0242
0.0125
0.0914
0.0290
0.1470
0.0460
0.0394
0.1320
0.0707
0.0117
0.1226
0.0566
0.0190
0.0579
0.9845

42

BNN
0.0501
0.0606
0.0374
0.0154
0.0240
0.0135
0.0737
0.0238
0.0818
0.0421
0.0383
0.0887
0.0699
0.0121
0.1049
0.0489
0.0219
0.0475
0.8070

RBF
0.0223
0.0248
0.0268
0.0117
0.0133
0.0118
0.0298
0.0244
0.0199
0.0236
0.0180
0.0369
0.0215
0.0102
0.0290
0.0243
0.0122
0.0212
0.3605

GP
0.0305
0.0285
0.0278
0.0120
0.0138
0.0121
0.0398
0.0180
0.0360
0.0276
0.0209
0.0480
0.0304
0.0104
0.0506
0.0302
0.0134
0.0265
0.4497

Andalucía

Aragón

Asturias

Islas Baleares

3.3

4
3.6

3.8

3.2
3.6

3.4

3.4

3.2

Canarias
3.25

3.2

3.2

3.2

3.15

3.15

3.15

3.1
3.1

Cantabria

3.25

3.25

3.8

3.1

3.1

3.05

3.05

3.05
20

40

60

20

Castilla La Mancha

40

60

20

Castilla y León

40

60

40

60

20

C. Valenciana

4.5

4.5

20

Cataluña

40

60

4.2

4

3.8

4

3.6

3.4

3.4

3.2
3.2

3.5

3.5

60

3.6

4
4.5

40

Galicia

3.6

5
4

20

Extremadura

3

3.4
3.5

20

40

60

20

C. Madrid

40

60

20

Murcia

40

60

20

Navarra
3.6

5.5

40

60

20

País Vasco

40

60

3.4

4

5

3.8

3.2

3.6

3.3
3.2

3.1
4.5

3.1
3.4

3
20

40

60

20

40

60

20

40

60

20

40

40

60

Log-int. obs.
GRNN est.
MLP est.
SVR est.
BNN est.
RBF est.
GP est.
B. est.
C. est.

3.2
3.4

20

La Rioja

60

20

40

60

Figure 12: Soft–data category . Observed and estimated COVID–19 mortality log–risk curves, from the implementation of Generalized Regression
Neural Network (GRNN), Multilayer Perceptron (MLP), Support Vector
Regression (SVR), Bayesian Neural Network (BNN), Radial Basis Function
Neural Network (RBF), Gaussian Processes (GP), and trigonometric regression combined with classical and Bayesian residual prediction

43

Andalucía

Aragón

Asturias

1000

Islas Baleares

300

800
600
400

100
100

100

500

50

100
200
0
20

40

60

0

Castilla La Mancha

20

40

60

0
0

Castilla y León

20

40

60

20

40

60

Extremadura

0

20

40

60

Galicia

600

800

300

600

200

400

200

400

1000

100

200

0
60

60

1000

2000

40

40

400

500

20

20

3000

1000

0

0
0

1200

4000
1000

0

C. Valenciana

5000
2000

0

Cataluña

1500

0

50

50

0
0

Cantabria
200
150

150

200

Canarias
150

200

0
0

C. Madrid

20

40

60

0

Murcia

20

40

60

0

Navarra

20

40

60

0
0

País Vasco

20

40

60

0

20

40

60

La Rioja

1400

8000
100

6000

300

1200

400

Cum. obs.
GRNN est.
MLP est.
SVR est.
BNN est.
RBF est.
GP est.
B. est.
C. est.

1000
200

800
4000

600

200

50

100

400

2000

200
0
0

20

40

60

0
0

20

40

60

0
0

20

40

60

0

20

40

60

0

20

40

60

Figure 13: Soft–data category . Observed and estimated COVID–19 mortality cumulative cases curves, from the implementation of Generalized Regression Neural Network (GRNN), Multilayer Perceptron (MLP), Support
Vector Regression (SVR), Bayesian Neural Network (BNN), Radial Basis
Function Neural Network (RBF), Gaussian Processes (GP), and trigonometric regression combined with classical and Bayesian residual prediction

44

Randon 5–fold cross–validation
The random 5–fold cross–validation SMAPEs obtained from implementation of
the six ML regression models tested are displayed in Table 13, for hard–data
category, and in Table 14, for soft–data category.
Table 13: Hard–data category . Averaged SMAPEs for 10 running of
random 5–fold cross–validation. (As indicated, displayed values must be
multiplied by 10−2 )
SC(x10−2 ) GRNN
C1
0.1962
C2
0.6150
C3
0.1541
C4
0.0984
C5
0.2065
C6
0.1585
C7
0.4957
C8
0.0804
C9
0.7280
C10
0.2208
C11
0.1273
C12
0.5237
C13
0.3637
C14
0.1359
C15
0.6105
C16
0.2479
C17
0.0667
M.
0.2958
T.
5.0293

MLP
0.0845
0.1531
0.0479
0.0388
0.0555
0.0423
0.0786
0.0352
0.2170
0.0724
0.0618
0.1706
0.0717
0.0388
0.1705
0.0931
0.0414
0.0867
1.4731

SVR
0.0890
0.0711
0.0438
0.0190
0.0378
0.0227
0.0771
0.0226
0.1061
0.0751
0.0373
0.1441
0.0701
0.0307
0.1592
0.0926
0.0195
0.0657
1.1177

BNN
0.0709
0.0805
0.0338
0.0226
0.0414
0.0257
0.0707
0.0215
0.0866
0.0577
0.0460
0.1449
0.0671
0.0235
0.1228
0.0753
0.0255
0.0598
1.0166

RBF
0.0635
0.0782
0.0371
0.0214
0.0429
0.0266
0.0712
0.0247
0.0421
0.0541
0.0451
0.1126
0.0605
0.0220
0.1101
0.0665
0.0241
0.0531
0.9026

GP
0.0592
0.0710
0.0325
0.0226
0.0397
0.0263
0.0587
0.0189
0.0475
0.0494
0.0385
0.1065
0.0480
0.0213
0.1121
0.0632
0.0246
0.0494
0.8400

Acknowledgements
This work has been supported in part by projects PGC2018-099549-B-I00 of the
Ministerio de Ciencia, Innovación y Universidades, Spain (co-funded with FEDER
funds), and by grant A-FQM-345-UGR18 cofinanced by ERDF Operational Programme 2014-2020, and the Economy and Knowledge Council of the Regional
Government of Andalusia, Spain.
45

Table 14: Soft–data category . Averaged SMAPEs, based on 10 running
of random 5–fold cross–validation. (As indicated, displayed values must be
multiplied by 10−2 )
SC(x10−2 ) GRNN
C1
0.1569
C2
0.1836
C3
0.1029
C4
0.0433
C5
0.0609
C6
0.0259
C7
0.3783
C8
0.0774
C9
0.4968
C10
0.1710
C11
0.1556
C12
0.3759
C13
0.2894
C14
0.0375
C15
0.3646
C16
0.1792
C17
0.0900
M.
0.1876
T.
3.1893

MLP
0.0958
0.1835
0.1309
0.0299
0.0524
0.0239
0.2136
0.0467
0.2926
0.0935
0.0914
0.2235
0.1599
0.0250
0.2410
0.0828
0.0724
0.1211
2.0589

SVR
0.0695
0.0697
0.0474
0.0171
0.0282
0.0133
0.1018
0.0315
0.1458
0.0541
0.0456
0.1509
0.0903
0.0124
0.1378
0.0677
0.0216
0.0650
1.1047

BNN
0.0604
0.0880
0.0491
0.0181
0.0264
0.0148
0.0963
0.0320
0.1329
0.0489
0.0441
0.1396
0.0775
0.0153
0.1297
0.0578
0.0256
0.0622
1.0567

RBF
0.0240
0.0286
0.0273
0.0133
0.0153
0.0130
0.0309
0.0292
0.0254
0.0277
0.0221
0.0402
0.0259
0.0114
0.0334
0.0292
0.0129
0.0241
0.4100

GP
0.0337
0.0329
0.0284
0.0130
0.0159
0.0129
0.0439
0.0207
0.0417
0.0316
0.0247
0.0560
0.0368
0.0109
0.0573
0.0344
0.0144
0.0299
0.5090

The subject of this paper was originally developed, in a first stage, under the
seminars hold in the Unidad de Transferencia del IMUS about Matemáticas y la
COVID. We also thank the organizer, Professor Emilio Carrizosa.

References
[1] O. O. Aalen, O. Borgan and H. K. Gjessing (2008). Survival and event history
analysis: a process point of view. Springer Science & Business Media, New–
York.
[2] C. Abboud, O. Bonnefon, E. Parent and S. Soubeyrand (2019). Dating and
localizing an invasion from post-introduction data and a coupled reaction–
diffusion–absorption model. Journal of Mathematical Biology 79, 765–789.
46

[3] C. Agostinelli (2001). Robust model selection in regression via weighted likelihood methodology. Statistics & Probability Letters 56, 289—300.
[4] E. Alpaydin (2004). Introduction to Machine Learning. MIT Press, Cambridge, MA.
[5] H. Anderson and T. Britton (2000). Stochastic epidemic models and their
statistical analysis. Springer–Verlag, New–York.
[6] J. Angulo, H.–L. Yu, A. Langousis, A. Kolovos, J. Wang, A. E. Madrid and G
Christakos (2013). Spatiotemporal infectious disease modeling: A BME-SIR
Approach. PLoS One 8(9): e72168.
[7] M. Barstugan, U. Ozkaya and S. Ozturk (2020). Coronavirus (COVID–19)
classification using ct images by machine learning methods. arXiv preprint
arXiv:2003.09424
[8] E. E. Beretta, T. Hara, W. Ma and Y. Takeuchi (2001). Global asymptotically
stability of an SIR epidemic model with distributed time delay. Nonlinear Anal
Theory Methods Appl 47, 4107–4115.
[9] R. Blanquero, E. Carrizosa, M. A. Jiménez–Cordero and B. Martı́n–Barragán
(2020). Selection of time instants and intervals with support vector regression for multivariate functional data. Computers & Operations Research 123
10.1016/j.cor.2020.105050.
[10] B. M. Bolker and B. Grenfell (1996). Impact of vaccination on the spatial
correlation and persistence of measles dynamics. Proceedings of the National
Academy of Sciences 93, 12648–12653.
[11] D. Bosq (2000). Linear processes in function spaces. Lecture notes in statistics 149. Springer, New–York.
[12] D. Bosq and M. D. Ruiz–Medina (2014). Bayesian estimation in a high
dimensional parameter framework. Electron J Statist 8, 1604–1640.
[13] D. L. Chao, J. D. Bloom, B. F. Kochin, R. Antia and I. M. Longini (2012).
The global spread of drug-resistant influenza. Journal of the Royal Society
Interface 9, 648–656.
[14] O. Chapelle, V. Vapnik and Y. Bengio (2002). Model selection for small
sample regression. Machine Learning 48, 9—23.

47

[15] L.–Ch. Chien and L.–W. Chen (2020). Meteorological impacts on the incidence of COVID-19 in the U.S. Stoch Environ Res Risk Assess 34, 1675—
1680.
[16] G. Christakos (2000). Modern spatiotemporal geostatistics. Oxford University Press, New–York, NY.
[17] G. Christakos (2002). On assimilation of uncertain physical knowledge bases:
Bayesian and non–Bayesian techniques. Adv. Water Resources 25: 1257–
1274.
[18] G. Christakos, P. Bogaert and M. L. Serre (2002). Advanced functions of
temporal GIS, Springer-Verlag, New York, N.Y.
[19] G. Christakos and D.T. Hristopulos (1998). Spatiotemporal environmental
health modelling: a Tractatus Stochaticus. Kluwer, Boston.
[20] G. Christakos (2008). Bayesian maximum entropy. In Advanced mapping of
environmental data: geostatistics, machine learning, and Bayesian maximum
entropy. Wiley, New York, NY, pp. 247–306.
[21] Daubechies, I. (1988). Orthonormal basis of compactly supported wavelets.
Comm. Pure Appl. Math. 41, 909–9996.
[22] Z. Du, X. Xu, Y. Wu, L. Wang, L. A. Cowling and B. J. Meyers (2020).
Serial interval of COVID-19 among publicly reported confirmed cases. Emerg.
Infect. Dis. 26(6).
[23] J. Dushoff, J. Plotkin, S. Levin and D. Earn (2004). Dynamical resonance
can account for seasonality of influenza epidemics. Proceedings of the National Academy of Sciences of the United States of America 101, 16915–
16916.
[24] M. Elhia, A. Laaroussi, M. Rachik, Z. Rachik and E. Labriji (2014). Global
stability of a susceptible–infected–recovered (SIR) epidemic model with two
infectious stages and treatment. Int J Sci Res 3, 114–121.
[25] T. R. Fleming and D. P. Harrington (1991). Counting processes and survival
analysis. Wiley Series in Probability and Mathematical Statistics: Applied
Probability and Statistics. John Wiley & Sons, Inc., New–York.
[26] L. N. Guin and P. K. Mandal (2014). Spatiotemporal dynamics of reaction–
diffusion models of interacting populations. Appl Math Model 38, 4417–
4427.
48

[27] T. Hastie, R. Tibshirani and J. Friedman (2001). The elements of statistical
learning. Springer Series in Statistics. Springer–Verlag, New–York.
[28] J. He , G. Chen, Y. Jiang , R. Jin , A. Shortridge, S. Agusti, M. Hea, J.
Wua, C.M. Duarte, G. Christakos (2020). Comparative infection modeling
and control of COVID-19 transmission patterns in China, South Korea, Italy
and Iran. Science of the Total Environment 747
[29] A. Huppert and G. Katriel (2013). Mathematical modelling and prediction
in infectious disease epidemiology. Clin Microbiol Infect 19, 999–1005.
[30] A.V. Ivanov, N.N. Leonenko, M.D. Ruiz Medina, and B.M. Zhurakovsky
(2015). Estimation of harmonic component in regression with cyclically dependent errors. Stastics: A Journal of Theoretical and Applied Statistics 49,
156–186.
[31] B. Ivorra, M.R. Ferrández, M. Vela-Pérez and A.M. Ramos (2020). Mathematical modeling of the spread of the coronavirus disease 2019 (COVID-19)
taking into account the undetected infections. The case of China. Commun
Nonlinear Sci Numer Simulat 88, 105–303.
[32] B. Ivorra, A.M. Ramos and D. Ngom (2015). Be-CoDiS: A mathematical
model to predict the risk of human diseases spread between countries. Validation and application to the 2014 ebola virus disease epidemic. Bull Math
Biol 77, 1668–1704.
[33] C. Ji, D. Jiang and N. Shi (2012). The behavior of an SIR epidemic model
with stochastic perturbation. Stoch Anal Appl. 30, 755–773.
[34] M.J. Keeling, D.A. Rand and A.J. Morris (1997). Correlation models for
childhood epidemics. Proceedings of the Royal Society of London 264, 1149–
1156.
[35] M.J. Keeling and P. Rohani (2008). Modeling infectious diseases in humans
and animals. Princeton University Press, Princeton.
[36] W. Kermack and A. McKendrick (1927). Contributions to the mathematical
theory of epidemics - I. Proceedings of the Royal Society of Edinburgh A 115,
700–721.
[37] M.A. Khan and A. Atangana (2020). Modeling the dynamics of
novel coronavirus (2019-nCov) with fractional derivative. Alex. Eng. J..
doi.org/10.1016/j.aej.2020.02.033.

49

[38] A.J. Kucharski, T.W. Russell, C. Diamond, Y. Liu, J. Edmunds and S. Funk
et al. (2020). Early dynamics of transmission and control of COVID-19: a
mathematical modelling study. Lancet Infect Dis. doi.org/10.1016/S14733099(20)30144-4.
[39] Y.A. Kuznetsov and C. Piccardi (1994). Bifurcation analysis of periodic
SEIR and SIR epidemic models. J Math Biol 32, 109–121.
[40] A.E. Laaroussi, M. Rachik and M. Elhia (2018). An optimal control problem
for a spatiotemporal SIR model Int. J. Dynam. Control 6, 384–397.
[41] A. Langousis and A.A. Carsteanu (2020). Undersampling in action and at
scale: application to the COVID-19 pandemic. Stoch Environ Res Risk Assess
34, 1281—1283.
[42] C. Malesios, N. Demiris, P. Kostoulas, K. Dadousis, T. Koutroumanidis
and Z. Abas (2016). Spatio-temporal modelling of foot-and-mouth disease
outbreaks. Epidemiol. Infect. 144, 2485–2493.
[43] C.C. McCluskey (2010). Complete global stability for an SIR epidemic model
with delay distributed or discrete. Nonlinear Anal Real World Appl 11, 55–59.
[44] F. A. Milner and R. Zhao (2008). SIR model with directed spatial diffusion.
Math Popul Stud 15, 160–181.
[45] M. Mohammady, H. Reza Pourghasemi, M. Amiri and J.P. Tiefenbacher
(2021). Spatial modeling of susceptibility to subsidence using machine learning techniques. https://doi.org/10.1007/s00477-020-01967-x
[46] H. Nishiura, N.M. Linton and A. R. Akhmetzhanov (2020). Serial interval
of novel coronavirus (COVID-19) infections. Int. J. Infect. Dis. 93, 284–286.
[47] D. Pak, K. Langohr, J. Ning, J. Cortés Martı́nez, G.Gómez–Melis and Y.
Shen (2020). Modeling the coronavirus disease 2019 incubation period: impact on quarantine policy doi.org/10.1101/2020.06.27.20141002.
[48] S. Pathak, A. Maiti and G. Samanta (2010). Rich dynamics of an SIR
epidemic model. Nonlinear Anal Model Control 15, 71–81.
[49] A.M. Ramosa, M.R. Ferrández, M. Vela-Pérez and B. Ivorra (2020). A
simple but complex enough θ–SIR type model to be used with COVID–19 real
data. Application to the case of Italy. doi.org/10.13140/RG.2.2.32466.17601.
[50] A. Remuzzi and G. Remuzzi (2020). COVID-19 and Italy: what next? The
Lancet. doi.org/10.1016/S0140-6736(20)30690-5
50

[51] K. Roosa, Y. Lee, R. Luo, A. Kirpich, R. Rothenberg, J. Hyman et al.
(2020). Real–time forecasts of the COVID-19 epidemic in China from February 5th to February 24th. Infect Dis Modell 5, 256–263.
[52] L. Roques and O. Bonnefon (2016). Modelling population dynamics in realistic landscapes with linear elements: A mechanistic-statistical reactiondiffusion approach. PloS One 11(3):e0151217.
[53] L. Roques, S. Soubeyrand and J. Rousselet (2011). A statistical-reactiondiffusion approach for analyzing expansion processes. J Theor Biol. 274,
43–51.
[54] M. Sekiguchi and E. Ishiwata (2010). Global dynamics of a discretized SIRS
epidemic model with time delay. J Math Anal Appl 371, 195–202.
[55] B. Sivakumar (2020). COVID-19 and water. Stoch Environ Res Risk Assess.
https://doi.org/10.1007/s00477-020-01837-6
[56] R. Sujath, J.M. Chatterjee and A.E. Hassanien (2020). A machine learning
forecasting model for COVID-19 pandemic in India. Stoch Environ Res Risk
Assess. 34, 959—972.
[57] R. Sujath, J.M. Chatterjee and A.E. Hassanien (2020). Correction to: A
machine learning forecasting model for COVID-19 pandemic in India. Stoch
Environ Res Risk Assess. https://doi.org/10.1007/s00477-020-01843-8
[58] Y. Takano and R. Miyashiro (2020). Best subset selection via cross–
validation criterion. TOP 28, 475—488.
[59] E. Tornatore, S.M. Buccellato and P. Vetro (2005). Stability of a stochastic
SIR system. Phys A Stat Mech Its Appl 354, 111–126.
[60] E. Volz (2008). SIR dynamics in random networks with heterogeneous connectivity. Journal of Mathematical Biology 56, 293–310.
[61] C. Wang, P.W. Horby, F. Hayden and G.F. Gao (2020). A novel coronavirus
outbreak of global health concern. Lancet 395, 470–473.
[62] R.K. Wasiur, B. Choiy, E. Kenahz and G.A. Rempa (2019). Survival dynamical systems for the population-level analysis of epidemics. arXiv.1901.00405.
[63] G. Webb (1981). A reaction-diffusion model for a deterministic diffusive
epidemic. J Math Anal Appl 84, 150–161.

51

[64] Y. Xu, L. Allena and A. Perelson (2007). Stochastic model of an influenza
epidemic with drug resistance. Journal of Theoretical Biology 248, 179–193.
[65] J. Yu, D. Jiang and N. Shi (2009). Global stability of two-group SIR model
with random perturbation. J Math Anal Appl. 360, 235–244.
[66] Wb. Zhang, Y. Ge, M. Liu et al. (2020) Risk assessment of the step-by-step
return-to-work policy in Beijing following the COVID-19 epidemic peak. Stoch
Environ Res Risk Assess. https://doi.org/10.1007/s00477-020-01929-3
[67] F. Zhang, Z. Li and F. Zhang (2008). Global stability of an SIR epidemic
model with constant infectious period. Appl Math Comput. 199, 285–291.
[68] T. Zhou, Z. Fu and B. Wang (2006). Epidemic dynamics on complex networks. Progress in Natural Science 16, 452–457.
[69] F. Zhou, T. Yu, R. Du, G. Fan, Y. Liu, Z. Liu, J. Xiang, Y. Wang, B. Song,
X. Gu, et al (2020). Clinical course and risk factors for mortality of adult
inpatients with COVID–19 in Wuhan, China: a retrospective cohort study.
The Lancet. doi.org/10.1016/S0140-6736(20)30566-3.

52

