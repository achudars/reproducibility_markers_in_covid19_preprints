CT-CAPS: FEATURE EXTRACTION-BASED AUTOMATED FRAMEWORK FOR COVID-19
DISEASE IDENTIFICATION FROM CHEST CT SCANS USING CAPSULE NETWORKS
Shahin Heidarian1 , Parnian Afshar2 , Arash Mohammadi2 , Moezedin Javad Rafiee, MD3 ,
Anastasia Oikonomou, MD4 , Konstantinos N. Plataniotis5 , and Farnoosh Naderkhani2
1

arXiv:2010.16043v1 [eess.IV] 30 Oct 2020

2

Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, Canada
Concordia Institute for Information Systems Engineering, Concordia University, Montreal, Canada
3
Department of Medicine and Diagnostic Radiology, McGill University, Montreal, QC, Canada
4
Department of Medical Imaging, Sunnybrook Health Sciences Centre, Toronto, Canada
5
Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada
ABSTRACT

The global outbreak of the novel corona virus (COVID-19) disease
has drastically impacted the world and led to one of the most challenging crisis across the globe since World War II. The early diagnosis and isolation of COVID-19 positive cases are considered as
crucial steps towards preventing the spread of the disease and flattening the epidemic curve. Chest Computed Tomography (CT) scan is
a highly sensitive, rapid, and accurate diagnostic technique that can
complement Reverse Transcription Polymerase Chain Reaction (RTPCR) test. Recently, deep learning-based models, mostly based on
Convolutional Neural Networks (CNN), have shown promising diagnostic results. CNNs, however, are incapable of capturing spatial relations between image instances and require large datasets. Capsule
Networks, on the other hand, can capture spatial relations, require
smaller datasets, and have considerably fewer parameters. In this paper, a Capsule network framework, referred to as the ”CT-CAPS”, is
presented to automatically extract distinctive features of chest CT
scans. These features, which are extracted from the layer before the
final capsule layer, are then leveraged to differentiate COVID-19
from Non-COVID cases. The experiments on our in-house dataset
of 307 patients show the state-of-the-art performance with the accuracy of 90.8%, sensitivity of 94.5%, and specificity of 86.0%.
Index Terms: COVID-19 Identification, Capsule Networks,
Pulmonary Infection, Chest CT Scan, Deep Learning
1. INTRODUCTION
Past months have been sadly tied up with the novel coronavirus
(COVID-19) outbreak resulting in long term damages to the society and people’s lives. As such, a global action plan has been established to prevent the spread of this highly contagious disease
including attempts to find vaccines and early diagnosis of the disease. The latter is highly important for the immediate isolation and
treatment of the COVID-19 positive cases. The gold standard diagnostic test for this disease is the Reverse Transcription Polymerase
Chain Reaction (RT-PCR), which is associated with a high false negative rate [1]. RT-PCR is also a time-consuming test, which may deprive health authorities from the opportunity of early isolation resulting in further spread of the virus. Recent studies have demonstrated
the strong capability of Chest Radiographs (CR) and Computed Tomography (CT) scans in providing distinctive patterns associated
with the COVID-19 infection such as bilateral prevalence of Ground
Glass Opacities (GGO) with peripheral and subpleural distribution

and lower lung zone predominance. Halo and reverse halo signs, focal areas of consolidations and interstitial thickening have also been
described [2,3]. COVID-19 lung manifestations, however, are highly
overlapped with the Community Acquired Pneumonia (CAP) findings making their identification challenging and complicated even
for experienced radiologists and healthcare professionals.
Recently, several research studies have developed deep learningbased approaches to identify COVID-19 disease from chest radiographs, some of which acquired promising performances [4–7]. Both
chest radiograph and CT scan provide a view of the structures inside the body by sending an electromagnetic radiation to the body
and capturing the radiation passed through the body on a detector.
However, chest radiographs are single 2D images that provide limited details of the lung infections. CT scans, on the other hand, consist of multiple cross-sectional images of the body (slices), which
are seen together to create a 3D representation of the body. Unlike
chest radiographs, CT scans can provide a comprehensive illustration of the specific structure of the lung abnormalities. Consequently,
CT has a higher sensitivity compared to CR especially in the case
of COVID-19 detection, where there is a high overlap with other
lung infections such as CAP and therefore a fast and accurate diagnosis is imperative. Due to the radiation exposure caused by CT
scans and the chance of virus spread during the scanning, CT scan is
mainly recommended as diagnostic tools in emergency settings and
to assist radiologists evaluating patients with suspected or confirmed
COVID-19 infection [8]. However, chest CT is widely used as a primary diagnostic imaging modality in many countries, especially in
those where RT-PCR test resources are low. Therefore, several studies have investigated possible deep solutions to identify COVID-19
from 2D and 3D CT images. Analyzing a series of CT scan slices,
however, is more challenging compared to a single chest radiograph
as we have to review several images and investigate the evidence of
infection over multiple slices.
Contributions: In this paper, we propose a fully automated framework based on Capsule Networks [9], referred to as the “CT-CAPS”,
to represent each slice of a CT scan by a small feature map to distinguish COVID-19 cases from non-COVID (CAP and normal) cases.
The capsule network is selected to address the failure of the commonly used CNN architectures [10] in recognizing spatial relations
between objects in an image and thus eliminate the need for large labeled datasets to reach a satisfying result. Large datasets are difficult
to obtain in the case of a novel and barely recognized disease such as
COVID-19. Furthermore, CNN architectures consist of millions of

parameters making their training process slow and complex. Medical images, in particular COVID-19 related ones, contain distinctive
spatial relations between image instances. For example, there is a
distinctive distribution of imaging findings in COVID-19 infection
on CT scans, which makes finding spatial relations in the image
highly important. Moreover, the superiority of Capsule Networks
over their counterparts is demonstrated in some of our previous research studies based on various imaging modalities such as magnetic
resonance imaging (MRI) and chest radiographs (CR) [4, 11–13].
In this study, a Capsule Network-based feature extractor is presented to detect specific characteristics of CT slices, followed by a
Max Pooling Layer to convert slice-level feature maps into patientlevel ones. Finally, a stack of fully connected layers are added to
make the final decision. The CT-CAPS is trained on a dataset of
CT slices labeled by three experienced radiologists [14] to determine slices demonstrating infection and slices without an evidence
of infection. The main focus of our labeling process was to detect
distinctive and obvious abnormalities in a CT scan not struggling
to identify minor findings of questionable infection. Therefore, CTCAPS framework benefits from a fast and timely labeling process,
which is highly valuable when we are facing an early emergence of
a new type of data. The experimental results on our in-house dataset
of 307 patients [14] demonstrate the capability of CT-CAPS model
to be trained on such a coarsely labeled dataset achieving the accuracy of 90.8%, sensitivity of 94.5%, specificity of 86.0%, and Area
Under the ROC Curve (AUC) of 0.93 using the cut-off probability
of 0.6. To improve on the explanability of the proposed CT-CAPS,
we incorporated the Grad-CAM localization mapping approach [15]
to determine the lung regions contributing the most to the final decision. The %95 Confidence Interval (CI) values are also calculated
for all performance matrics using the methods explained in [16, 17].
The trained CT-CAPS model and the testing code are available publicly for open access at https://github.com/ShahinSHH/CT-CAPS.
2. RELATED WORKS
Existing identification methods based on chest CT scans are generally divided into slice-level and patient-level methods. These
works can further be classified into segmentation-based or feature
extraction-based approaches. Segmentation-based methods [18–20]
aim to train a model on a large dataset of annotated lung lesions
to detect regions of infection and determine the disease severity
and type. Although the lung segmentation task has been wellstudied [21], infection segmentation requires an extensive collaboration with radiologists to perform the sophisticated infection and
abnormality annotation task making the training process too complicated and time-consuming. Moreover, in some cases [19], the overall
performance is low for scenarios with mild lung infections. As an
example of segmentation-based methods, Nguyen et al. [18] used a
semi-supervised method based on pre-trained existing segmentation
models to detect lung infected regions to be incorporated into a
CNN-based classifier via an attention mechanism to increase the
classification accuracy. The underlying approach reached the overall
accuracy of 88% in the binary slice-level classification (COVID19 and normal cases). In another study, Shi et al. [19] proposed
a model, which extracts handcrafted radiomics features from the
segmented lung and infected regions followed by a feature selection
mechanism to feed multi-stage random forest classifiers to classify
patients into four groups based on their infection size obtained from
the first step. Then a random forest model is trained for each group
as the final classifier achieving the accuracy of 87.0%, sensitivity
of 90.7%, and specificity of 83.3%. As another example, Zhang

Fig. 1. The segmented lung tissue using R231CovidWeb model.

et al. [20] proposed a two-stage method consisting of a sophisticated lung infection segmentation model, trained on a manually
annotated chest CT scan dataset, followed by a 3D ResNet18 classification model [22] to classify patients into COVID-19, CAP, and
normal cases. Their proposed method achieves the overall accuracy
of 92.49%.
With regard to the feature extraction-based approaches, different frameworks have been recently introduced, commonly utilizing
a CNN-based model. Such methods either use a 3D CNN to analyze the whole CT volume in a single stage, or apply 2D CNNs
on CT slices and aggregate slice-level results via an aggregation
mechanism. As an example, Wang et al. [23] fed a 3D CNN-based
classifier with lung regions, segmented by a pre-trained U-Net [24],
achieving accuracy of 90.1%, sensitivity of 84.0%, and specificity
of 98.2% on a dataset containing only COVID-19 and normal cases.
Hu et al. [25] extended patient-level labels into slice-level and used
the same label for all slices in a CT scan to train a deep model,
utilizing the intermediate CNN layers to obtain classification features. These features are then combined to make the final decision.
Their proposed method achieved the overall accuracy of 87.4%
in the three-way classification. It is worth mentioning that using
patient-level labels for all slices in a CT scan is not reasonable and
will add errors into the system as each volume of CT scan contains
many slices without any visible infection area. In another study, Li et
al. [26] proposed a fully automated framework using a ResNet50 as
the backbone to extract slice-level features followed by max pooling
and fully connected dense layers for the ultimate goal of patientlevel classification achieving the sensitivities of 90%, 87%, and 94%
for COVID-19, CAP, and normal cases, respectively. The aforementioned methods either require a carefully annotated data to segment
regions of infection, or extend patient-level labels to all slices resulting in an unexplainable and potentially lower results. Moreover,
some of the aforementioned works have only proposed slice-level
classifiers, which makes such methods partially automated.
3. CT-CAPS FRAMEWORK
In this section, the CT-CAPS framework is introduced in detail. First,
the in-house dataset of chest CT scans and the associated labeling and preprocessing steps used in this study are briefly described.
Next, the detailed description of the model structure and processing
pipeline is presented.
3.1. COVID-CT-MD Dataset
We used our recently released dataset of volumetric chest CT scans
referred to as the “COVID-CT-MD” [14], which includes 171 patients positive for COVID-19 infection, 60 patients with CAP, and

Fig. 2. The proposed CT-CAPS framework to identify COVID-19 and non-COVID cases from chest CT scans.

76 normal patients. A subset of 55 COVID-19, and 25 CAP cases in
the COVID-CT-MD are analyzed by three experienced radiologist to
detect slices with a distinctive evidence of infection. As mentioned
earlier, there exist many slices without evidence of infection in a
chest CT scan and commonly only a proportion of the lung is involved in the infection. The labeling process aims to specify slices
with distinctive disease manifestations in a timely manner rather than
those with minimal findings. The labeled subset of the data contains
4, 993 slices demonstrating infection and 18, 416 slices without evidence of infection. This labeled data is then randomly divided into
three groups, including 60%, 10%, and 30% independent parts of
the data, to train, validate, and test the feature extraction stage of the
CT-CAPS framework. The remaining data is split with the same proportion and used along with the labeled data to train and evaluate the
subsequent fully connected layer as the patient-level classifier. This
research work is performed based on the policy certification number
30013394 of Ethical acceptability for secondary use of medical data
approved by Concordia University. The COVID-CT-MD dataset is
available online through Figshare 1 . For more information about the
imaging settings and detailed description of the dataset, please refer
to Reference [14].
3.2. Lung Segmentation
In order to remove the unimportant components and artifacts existing
in a CT scan, we utilized a pre-trained U-Net-based lung region segmentation model [21] referred to as the “U-net (R231CovidWeb)”,
which has been fine-tuned specifically on the COVID-19 images. A
sample of lung region extracted by this model is illustrated in Fig. 1.
It is worth mentioning that unlike segmenting infected regions, lung
region segmentation is a well-studied topic and highly efficient models have been introduced so far. The input of the R231CovidWeb
model is a CT scan with the original slice size of 512 × 512.
The model returns the extracted lung tissues, which will further go
through some normalization and resizing steps. More specifically,
the output images will be normalized between 0 and 1 to help the
generalizability and effective convergence of the model. Following
the literature [20, 25], we down-sampled the output images from the
original 512 × 512 size to 256 × 256 to reduce the complexity and
memory requirements with negligible loss of information. Finally,
slices without visible lung tissues are excluded and the remaining
ones are saved to be used in the CT-CAPS framework.
3.3. CT-CAPS Architecture
The CT-CAPS’ pipeline is illustrated in Fig. 2. The first stage of
the CT-CAPS architecture consists of a 2D Capsule Network, which
aims to classify 2D CT slices into COVID-19 and non-COVID images. This network is initialized by a stack of four convolutional lay1 https://figshare.com/s/c20215f3d42c98f09ad0

ers along with a batch-normalization and one max pooling layer as
shown in Fig. 3. The last convolutional layer is then used to feed
the subsequent Capsule Layers to extract deeper and smaller feature maps by the routing by agreement process defined in [9]. Two
more capsule layers are subsequently added to the model, where the
amplitude of the last one represents the probability of the input image belonging to each target class. In the next step, we aggregate
slice-level features extracted by intermediate layers of the described
network to move on to the patient-level domain. In this regard, the
capsule layer before the last one is used as the representative feature
map of the slices. Experimental results (presented later in Section 4)
demonstrate the ability of this feature map to efficiently distinguish
between COVID-19 and non-COVID images.
A max pooling layer is then applied on the feature maps associated with slices of a patient and the result is fed to a stack of four
fully connected layers with the size of 256, 128, 32, and 2 respectively. Using this feature selection and max pooling, each 3D volume
CT scan is represented by a small 32 × 16 matrix. In addition, we
modified the loss function to compensate the relatively imbalanced
training dataset. We used a weighted version of the loss function,
such that a higher penalty rate is given to the less frequent class,
which is COVID-19 in this case. For the fully connected layers, however, the class weights are equal. The loss function of the Capsule
Network model is modified as follows
loss =

N+
N−
−
×
loss
+
× loss+ ,
N+ + N−
N+ + N−

(1)

where N + represents the number of COVID-19 samples, N − is the
number of non-COVID samples, loss+ signifies the loss value associated with COVID-19 samples, and loss− is the loss value associated with non-COVID samples.
4. EXPERIMENTAL RESULTS
The feature extraction part of the CT-CAPS is trained on a dataset
of 55 COVID-19, 25 CAP, and all 78 normal cases. The Adam optimizer with the initial learning rate of 1e−4, batch size of 16, and 100
epochs is used in the corresponding training step. For the fully connected patient-level classifier, the initial learning rate of 1e − 3, and
500 epochs are used. In each stage, the model with the lowest loss
value on the validation set is considered as the final model for evaluation. The evaluation results on the COVID-CT-MD dataset [14] are
presented in Table 4. We compared the CT-CAPS with its duplicate
but using the whole CT images without extracting the lung tissues.
We also replaced the Capsule Layers in another experiment with two
fully connected layers with the size of 128, while the rest of the architecture and parameters are kept the same. We then took the fully
connected dense layer before the last layer as the new feature map to
make a CNN-based alternative model for the comparison.

Fig. 3. The Capsule Network model used to extract slice-level features.
Table 1. CT-CAPS’s Patient-Level Classification Results. Values in parenthesis represent the 95% confidence interval.
Performance
Accuracy(%)
Sensitivity(%)
Specificity(%)
AUC
# Params.

CT-CAPS

CT-CAPS
(no lung)

CT-CNN

CT-Res50

89.8
(89.2, 90.4)
94.5
(93.7, 95.3)
83.7
(82.1, 85.4)
0.93
(0.88, 0.98)
0.5M

82.6
(81.8, 83.4)
87.3
(82.4, 85.0)
76.6
(74.7, 78.5)
0.86
(0.78, 0.93)
0.5M

78.6
(77.8, 79.4)
87.3
(86.1, 88.5)
67.4
(65.3, 69.5)
0.79
(0.70, 0.88)
243.9M

81.6
(80.8, 82.4)
96.4
(95.7, 97.1)
62.8
(60.6, 65.0)
0.82
(0.74, 0.90)
24M

In a similar experiment, Resnet50, which is the backbone of
many similar works such as Reference [26] is used in the feature
extraction stage. In this case, the fully connected layer with 2, 048
neurons before the last layer is taken as the feature map. The comparison results are presented in Table 4. The CT-CAPS achieves the
accuracy of 89.8%, high sensitivity of 94.5%, specificity of 83.7%,
and Area Under the ROC Curve (AUC) of 0.93 using the default
probability threshold of 0.5. It is worth mentioning that various cutoff probabilities can be used based on the radiologist’s preference.
A higher cut-off probability indicates the higher importance of the
specificity, while the lower value indicates the higher importance of
the sensitivity. In other words, radiologists can adjust this threshold
to change the range of required confidence to consider a patient as
a COVID-19 positive case or a non-COVID case. Experimental results show that increasing the probability threshold from 0.5 to 0.6
improves the accuracy to 90.8%, and the specificity to 86.0% while
the sensitivity remains the same. Table 4 presents performance of the
proposed CT-CAPS using different cut-off probabilities.

In addition to the aforementioned numerical results, we incorporated the GRAD-CAM localization mapping method to visualize the
distinctive patterns in a chest CT scan recognized by the last convolutional layer of the CT-CAPS. Fig. 4 illustrates the recognized
abnormal regions for two lung samples containing small evidences
of COVID-19 infection. In these two examples, we can observe that
the model correctly identified the regions of infection, which had the
highest contribution to final decision.

Table 2. Performance of the CT-CAPS using different cut-off probabilities.
Values in parenthesis represent the 95% confidence interval.

Cut-off
Probability
0.3
0.4
0.5
0.6
0.7

Accuracy(%)

Sensitivity(%)

Specificity(%)

86.7
(86.0, 87.4)
88.8
(88.2, 89.4)
89.8
(89.2, 90.4)
90.8
(90.2, 91.4)
89.8
(89.2, 90.4)

94.5
(93.7, 95.3)
94.5%
(93.7, 95.3)
94.5%
(93.7, 95.3)
94.5%
(93.7, 95.3)
90.9%
(89.9, 92, 0)

76.7
(74.8, 78.6)
81.4%
(79.6, 83.2)
83.7%
(82.0, 85.4)
86.0%
(84.4, 87.6)
88.4%
(86.9, 89.9)

5. CONCLUSION
In this paper, we proposed a fully automated Capsule network-based
framework, referred to as the “CT-CAPS”, which extracts distinctive
features of chest CT scans via Capsule Networks to identify COVID19 cases in a coarsely-labeled dataset of COVID-19, CAP, and normal cases. The experimental results indicate the capability of the
CT-CAPS to automatically analyze volumetric chest CT scans and
distinguish different cases with the accuracy of 90.8%, high sensitivity of 94.5%, and specificity of 86.0%, using far less number of
parameters and less sophisticated labeling process compared to its
existing counterparts. We also showed that Capsules in the layer before the last one can be a proper compact feature representative of

(COVID-19) using X-ray images,” Informatics in Medicine
Unlocked, vol. 20, pp. 100412, 2020.
[7] A. Altan and S. Karasu, “Recognition of COVID-19 disease
from X-ray images by hybrid model consisting of 2D curvelet
transform, chaotic salp swarm algorithm and deep learning
technique,” Chaos, Solitons & Fractals, vol. 140, pp. 110071,
nov 2020.
[8] C.E. Redmond, S. Nicolaou, F.H. Berger, A.M. Sheikh, and
M.N. Patlas, “Emergency Radiology During the COVID-19
Pandemic: The Canadian Association of Radiologists Recommendations for Practice,” Canadian Association of Radiologists Journal, vol. 71, no. 4, pp. 425–430, nov 2020.
[9] G. Hinton, S. Sabour, and N. Frosst, “Matrix capsules with EM
routing,” 6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings, pp. 1–29,
2018.
[10] R. Yamashita, M. Nishio, R.K.G. Do, and K. Togashi, “Convolutional neural networks: an overview and application in radiology,” Insights into Imaging, vol. 9, no. 4, pp. 611–629,
2018.
Fig. 4. The heat-maps created by the GRAD-CAM approach from the last
convolutional layer of the CT-CAPS framework for two sample images with
COVID-19-related evidences of infection.

CT scans. Moreover, the benefits of extracting lung tissues in the
CT-CAPS framework, and the flexibility of the model to be adjusted
based on radiologists’ preferences to achieve desired results have
been demonstrated by the experimental results. As a final note, we
will continue to further improve and validate our proposed method
upon receiving new datasets from our collaborators in medical centers to modify our model based on a multi-center dataset.
6. REFERENCES
[1] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, Q. Tao,
Z. Sun, and L. Xia, “Correlation of Chest CT and RT-PCR
Testing for Coronavirus Disease 2019 (COVID-19) in China:
A Report of 1014 Cases,” Radiology, vol. 296, no. 2, pp. E32–
E40, aug 2020.
[2] C. Hani, N.H. Trieu, I. Saab, S. Dangeard, S. Bennani, G. Chassagnon, and M.P. Revel, “COVID-19 pneumonia: A review
of typical CT findings and differential diagnosis,” Diagnostic
and Interventional Imaging, vol. 101, no. 5, pp. 263–268, may
2020.
[3] M. Carotti, F. Salaffi, P. Sarzi-Puttini, A. Agostini,
A. Borgheresi, D. Minorati, M. Galli, D. Marotto, and
A. Giovagnoni, “Chest CT features of coronavirus disease
2019 (COVID-19) pneumonia: key points for radiologists,” La
radiologia medica, vol. 125, no. 7, pp. 636–646, jul 2020.
[4] P. Afshar, S. Heidarian, F. Naderkhani, A. Oikonomou, K.N.
Plataniotis, and A. Mohammadi, “COVID-CAPS: A capsule network-based framework for identification of COVID-19
cases from X-ray images,” Pattern Recognition Letters, vol.
138, pp. 638–643, oct 2020.

[11] P. Afshar, K.N. Plataniotis, and A. Mohammadi, “BoostCaps:
A Boosted Capsule Network for Brain Tumor Classification,”
in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). 2020, pp.
1075–1079, IEEE.
[12] P. Afshar, K.N. Plataniotis, and A. Mohammadi, “A Bayesian
Approach to Brain Tumor Classification Using Capsule Networks,” Submitted to IEEE International Conference on Image
Processing (ICIP), 2020.
[13] P. Afshar, A. Oikonomou, F. Naderkhani, P.N. Tyrrell, K.N.
Plataniotis, K. Farahani, and A. Mohammadi, “3D-MCN: A
3D Multi-scale Capsule Network for Lung Nodule Malignancy
Prediction,” Scientific Reports, vol. 10, no. 1, pp. 7948, 2020.
[14] P. Afshar, S. Heidarian, N. Enshaei, F. Naderkhani, M. Javad
Rafiee, A. Oikonomou, F. Babaki Fard, K. Samimi, K.N. Plataniotis, and A. Mohammadi, “COVID-CT-MD: COVID-19
Computed Tomography (CT) Scan Dataset Applicable in Machine Learning and Deep Learning,” 2020.
[15] R. R. Selvaraju, A. Cogswell, M.and Das, R. Vedantam,
D. Parikh, and D. Batra, “Grad-CAM: Visual Explanations
from Deep Networks via Gradient-based Localization,” oct
2016.
[16] L.D. Brown, T.T. Cai, and A. Das Gupta, “Interval estimation
for a binomial proportion,” Statistical Science, vol. 16, no. 2,
pp. 101–117, 2001.
[17] J.A. Hanley and B.J. McNeil, “The meaning and use of the
area under a receiver operating characteristic (ROC) curve,”
Radiology, vol. 143, no. 1, pp. 29–36, 1982.
[18] D.M.H. Nguyen, D.M. Nguyen, H. Vu, B.T. Nguyen, F. Nunnari, and D. Sonntag, “Attention with Multiple Sources Knowledges for COVID-19 from CT Images,” sep 2020.

[5] L. Wang and A. Wong, “COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19
Cases from Chest X-Ray Images,” mar 2020.

[19] F. Shi, L. Xia, F. Shan, D. Wu, Y. Wei, H. Yuan, H. Jiang,
Y. Gao, H. Sui, and D. Shen, “Large-Scale Screening of
COVID-19 from Community Acquired Pneumonia using Infection Size-Aware Classification,” mar 2020.

[6] Md. Z. Islam, Md. M. Islam, and A. Asraf, “A combined deep
CNN-LSTM network for the detection of novel coronavirus

[20] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Zha,
W. Liang, C. Wang, K. Wang, L. Ye, M. Gao, Z. Zhou, L. Li,

J. Wang, Z. Yang, H. Cai, J. Xu, L. Yang, W. Cai, W. Xu, S. Wu,
W. Zhang, S. Jiang, L. Zheng, X. Zhang, L. Wang, L. Lu, J. Li,
H. Yin, W. Wang, O. Li, C. Zhang, L. Liang, T. Wu, R. Deng,
Y. Wei, K.and Zhou, T. Chen, J.Y.N. Lau, M. Fok, J. He, T. Lin,
W. Li, and G. Wang, “Clinically Applicable AI System for
Accurate Diagnosis, Quantitative Measurements, and Prognosis of COVID-19 Pneumonia Using Computed Tomography,”
Cell, vol. 181, no. 6, pp. 1423–1433.e11, jun 2020.
[21] F. Hofmanninger, J.and Prayer, J. Pan, S. Röhrich, H. Prosch,
and G. Langs, “Automatic lung segmentation in routine imaging is primarily a data diversity problem, not a methodology
problem,” European Radiology Experimental, vol. 4, no. 1, pp.
50, dec 2020.
[22] K. Hara, H. Kataoka, and Y. Satoh, “Learning Spatio-Temporal
Features with 3D Residual Networks for Action Recognition,”
in 2017 IEEE International Conference on Computer Vision
Workshops (ICCVW). oct 2017, pp. 3154–3160, IEEE.
[23] X. Wang, X. Deng, Q. Fu, Q. Zhou, J. Feng, H. Ma, W. Liu, and

C. Zheng, “A Weakly-Supervised Framework for COVID-19
Classification and Lesion Localization From Chest CT,” IEEE
Transactions on Medical Imaging, vol. 39, no. 8, pp. 2615–
2625, aug 2020.
[24] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Networks for Biomedical Image Segmentation,” may
2015.
[25] S. Hu, Y. Gao, Z. Niu, Y. Jiang, L. Li, X. Xiao, M. Wang, E.F.
Fang, W. Menpes-Smith, J. Xia, H. Ye, and G. Yang, “Weakly
Supervised Deep Learning for COVID-19 Infection Detection
and Classification From CT Images,” IEEE Access, vol. 8, pp.
118869–118883, 2020.
[26] L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, J. Bai, Y. Lu,
Z. Fang, Q. Song, K. Cao, D. Liu, Q. Wang, G.and Xu, X. Fang,
S. Zhang, J. Xia, and J. Xia, “Using Artificial Intelligence to
Detect COVID-19 and Community-acquired Pneumonia Based
on Pulmonary CT: Evaluation of the Diagnostic Accuracy,”
Radiology, vol. 296, no. 2, pp. E65–E71, aug 2020.

