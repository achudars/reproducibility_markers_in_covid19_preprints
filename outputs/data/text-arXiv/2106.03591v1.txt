Calibrating multi-dimensional complex ODE from
noisy datavia deep neural networks
Kexuan Li

arXiv:2106.03591v1 [stat.ML] 7 Jun 2021

Department of Mathematical Sciences, Worcester Polytechnic Institute
Fangfang Wang
Department of Mathematical Sciences, Worcester Polytechnic Institute
Ruiqi Liu
Department of Mathematics and Statistics, Texas Tech University
Fan Yang
Eli Lilly and Company
Zuofeng Shang
Department of Mathematical Sciences, New Jersey Institute of Technology
April 18, 2021

Abstract

Ordinary differential equations (ODEs) are widely used to model complex dynamics that
arises in biology, chemistry, engineering, finance, physics, etc. Calibration of a complicated
ODE system using noisy data is generally very difficult. In this work, we propose a two-stage
nonparametric approach to address this problem. We first extract the de-noised data and
their higher order derivatives using boundary kernel method, and then feed them into a
sparsely connected deep neural network with ReLU activation function. Our method is able to
recover the ODE system without being subject to the curse of dimensionality and complicated
ODE structure. When the ODE possesses a general modular structure, with each modular
component involving only a few input variables, and the network architecture is properly
chosen, our method is proven to be consistent. Theoretical properties are corroborated
by an extensive simulation study that demonstrates the validity and effectiveness of the
proposed method. Finally, we use our method to simultaneously characterize the growth rate
of Covid-19 infection cases from 50 states of the USA.
1

Keywords: Boundary kernel function; Sparsely connected deep neural networks; Nonlinear
ODE in high dimensions; ReLu activation function.

1

Introduction

The use of ordinary differential equations (ODEs) is prevalent in both social and natural sciences to study complex dynamic phenomena or dynamical systems. For instance,
linear ODEs are often used to describe population growth ([Henderson and Loreau, 2019]),
Lorenz equationâ€”a high-dimensional nonlinear ODEâ€”used to characterize chaos systems
([Talwar and Namachchivaya Sri, 1992]), and high-dimensional linear ODEs used to construct
a dynamic gene regulatory network ([Lu et al., 2011]). Therefore, calibrating complicated
ODE systems is of great interest and importance to both theorists and practitioners.
Owing to the superior performance of deep learning in modeling complicated data, deep
neural networks have been actively used to reproduce dynamical systems ([Weinan, 2017]).
Deep neural networks, such as residual network ([He et al., 2016]) and discrete normalizing
flows ([Kobyzev et al., 2020]), can be considered as discrete dynamical systems. Recently,
[Chen et al., 2018] propose a new family of continuous neural networks that extend the
traditional discrete sequence of hidden layers to continuous-depth by using an ODE to
parameterize the hidden units. [Lusch et al., 2018] and [Champion et al., 2019] consider
autoencoder-based architecture to understand and predict the complex dynamical systems.
Despite advances in deep learning, most of the existing methods lack interpretability
and their theoretical underpinnings are not well grounded. In this paper, we attempt to
fill the gap and provide statistical justification for calibrating a complex system which can
be characterized by multi-dimensional nonlinear ODEs. In particular, we are interested in
scenarios where data, collected from continuous-time nonlinear ODEs, are asynchronized,
irregularly spaced and are contaminated by measurement errors.
To start with, consider the following general multi-dimensional Î½th order ODE system:
dÎ½
x(t) = f0 (x(t), x(1) (t), Â· Â· Â· , x(Î½âˆ’1) (t)),
dtÎ½

0 â‰¤ t â‰¤ 1,

(1)

where x(t) = (x1 (t), . . . , xd (t))> âˆˆ Rd is a d-dimensional function of t, x(j) (t) represents the
jth derivative of x(t), and f0 (Â·) = (f0,1 (Â·), . . . , f0,d (Â·))> : Rr0 â†’ Rd is the ground-truth but
unknown, with r0 = Î½d. When Î½ = 1, (1) degenerates to a first-order ODE system:
dx(t)
= f0 (x(t)),
dt

0 â‰¤ t â‰¤ 1.

(2)

In general, f0 often implies unknown interactions among x(t), x(1) (t), Â· Â· Â· , and x(Î½âˆ’1) (t),
which is computationally difficult using conventional nonparametric techniques. Meanwhile,
in many applications, it is impossible to observe x(t) directly. Instead, we observe yji which
satisfy
yji = xj (tji ) + ji , i = 1, . . . , nj ,
(3)
at discrete time points 0 â‰¤ tj1 < tj2 < . . . < tjnj â‰¤ 1 for each component j, j = 1, . . . , d,
where ji âˆ¼ N (0, Ïƒj2 ) represents the measurement error at time tji . The data observed for each
2

component are allowed to be from different time stamps which might not be equally spaced,
thereby, yielding different sample sizes nj for different components. The statistical task is to
estimate the multi-dimensional nonlinear function f0 in (1) using the observed noisy data
y1 , . . . , yd , where yj = (yj1 , . . . , yjnj )> . To achieve this, we propose a two-stage procedure for
estimating f0 nonparametrically. In Stage 1, we use kernel estimation to filter out the noisy
b(t) and its high order derivative x
b(j) (t), j = 1, 2, . . . , Î½, which
in {y1 , . . . , yd } and obtain x
b(j) (t), respectively (see Section 3.1). In Stage
will be shown to be consistent for x(t) and x
b(Î½) (t) (See Section 3.2).
2, we adopt a ReLU feedforward neural network to approximate x
By assuming that the function f0 (Â·) enjoys a general modular structure with each modular
component involving only a few input variables, we establish the consistency of the ReLU
feedforward neural network estimator and derive its convergence rate. In particular, we prove
that the rate is not subject to the dimension of ODE system, but depends solely on the length
and width of the neural network fË†(Â·) and the smoothness of the function f0 (Â·). As such, our
method is computationally more tractable than conventional nonparametric methods.
In this paper all vectors are column vectors. Let kvk22 =
Terminologies and Notation:
R
v T v for any vector v. Let kf k22 = f (x)2 dx be the L2 norm of a real-valued function f (x). For
two positive sequences an and bn , we say an . bn if there exists a positive constant c such that
an â‰¤ cbn for all n, and an  bn if câˆ’1 an â‰¤ bn â‰¤ can for some constant c > 1 and a sufficiently
large n. Suppose x = (x1 , . . . , xd )> is a d-dimensional vector. P
Let |x| = (|x1 |, . . . , |xd |)>
d
denote the absolute value of x, |x|âˆž = maxi=1,...,d |xi |, |x|0 =
i=1 1(xi 6= 0). For two
dâˆ’dimensional vectors x, y, we say x . y if xi . yi , for i = 1, . . . , d. Let bxc be the largest
integer less than x and dxe be the smallest integer greater than x. Let kAkâˆž = maxij |aij |
be the max norm and kAk0 be the number of non-zero entries of a matrix A = (aij ), and
k|f |âˆž kâˆž be the sup-norm for a d-dimensional function f . We use a âˆ§ b and a âˆ¨ b to represent
the minimum and maximum of two numbers, respectively.

2

Related work

In a parametric/semi-parametric setting where f0 (Â·) is parameterized, the process of
calibrating the unknown parameters is the so-called inverse problem and has been widely
studied in the statistical literature. For example, if n1 = n2 = . . . = nd = n, t1i =
t2i = . . . = tdi , for i = 1, . . . , n, and suppose that (2) involves an unknown parameter Î¸,
i.e., dx(t;Î¸)
= f0 (x(t; Î¸); Î¸), then Î¸ can be estimated by the least square estimator (e.g., see
dt
[Benson, 1979], [Biegler et al., 1986]):
Î¸Ì‚LSE = argmin
Î¸

n X
d
X
(yji âˆ’ xj (tji ))2 ,

subject to

i=1 j=1

dx(t; Î¸)
= f0 (x(t; Î¸); Î¸).
dt

If the measurement errors ji are normally distributed, then Î¸Ì‚LSE coincides with the maxâˆš
imum likelihood estimator and is n-consistent. In most cases, Î¸Ì‚LSE has no closed-form
expression and the least square estimation tends to be computationally expensive. To overcome this problem, many other methods have been developed; see [Liang and Wu, 2008],
[Hall and Ma, 2014], [Bhaumik and Ghosal, 2015], [Wu et al., 2019], [Sun et al., 2020], among
3

many others. Nonetheless, they suffer from the curse of dimensionality and can only deal
with lower order derivatives.
In the cases where f0 can not be summarized by a few low-dimensional parameters, calibrating f0 becomes more demanding. Existing solutions attempt to impose extra assumptions
so as to simplify the structure of f0 . For instance, [Henderson and Michailidis, 2014] and
[Chen et al., 2017] assume an additive structure on f0 , while [Paul et al., 2016] consider a
case where f0 is positive. To well preserve the structure of f0 so as to align with what is
observed in practice, we suggest a two-stage deep learning method that estimates f0 nonparametrically by imposing a general modular structure on f0 with each modular component
involving only a few input variables. This method can deal with higher order derivatives and
is able to recover the ODE system without being subject to the curse of dimensionality.
Recently, deep neural network has been successfully applied in many fields, such as
computer vision, natural language processing, bioinformatics, recommendation systems,
etc. Investigating theoretical properties of deep neural network have also attracted many
researchersâ€™ interests. In the statistical literature, [Schmidt-Hieber, 2020] proves that using sparsely connected deep neural networks with ReLU activation function can achieve
the minimax rate of convergence in a nonparametric setting. [Farrell et al., 2021] achieve
similar convergence rates as [Schmidt-Hieber, 2020] under different regularity conditions.
Similarly, [Bauer and Kohler, 2019] show that multilayer feed-forward neural networks are
able to circumvent the curse of dimensionality if the regression function satisfies a generalized
hierarchical interaction model. Another point of view to theoretically understanding deep
learning is from approximation theory; e.g., see [ElbrÃ¤chter et al., 2021], [Lu et al., 2020],
[Hammer, 2000], [Li et al., 2020], [Li et al., 2019]. Unlike the traditional function approximation theory that uses the aggregation of simple functions to approximate complicated ones,
deep neural networks use the compositions of simple functions, which motivates us to assume
the underlining function f0 satisfies a compositional structure.

3

Methodology and Main Theorem

In this section, we detail the two-stage estimation procedure and provide its theoretical
underpinnings.

3.1

Stage 1: Kernel Estimator

Our goal in the first stage is to estimate x(t) = (x1 (t), . . . , xd (t))> and its Î½th derivative
x(Î½) (t), Î½ â‰¥ 1, on the basis of the â€œnoisy" observations {y1 , . . . , yd }. This is achieved via
kernel estimation by casting (3) as a nonparametric regression.
The classical kernel estimator of xj (t), j = 1, . . . , d, is given by
xÌƒj (t) =

nj
X
tji âˆ’ tj(iâˆ’1)
i=1

h

K(

t âˆ’ tji
) Â· yj (tji ),
h

where tj0 = 0 and h is a sequence of positive bandwidths satisfying h â†’ 0 and nj h â†’ âˆž as
4

nj â†’ âˆž, and where K(Â·) is a non-negative kernel function satisfying
Z âˆž
Z âˆž
K(x)dx = 1,
(K(x))2 dx < âˆž, and K(x) is Lipschitz continuous.
âˆ’âˆž

(4)

âˆ’âˆž

Despite that xÌƒj (t) is consistent for xj (t) for t âˆˆ (0, 1) (see [Priestley and Chao, 1972]), it
(Î½)
does not lead to a consistent estimator of xj (t). Therefore, we adopt the boundary kernel
function introduced in [Gasser and MÃ¼ller, 1984] here. We first estimate xj (t), j = 1, . . . , d,
by
nj Z
1 X si
tâˆ’u
x
bj (t) =
K(
)du Â· yj (tji ),
(5)
h i=1 siâˆ’1
h
where 0 = s0 â‰¤ s1 , . . . , â‰¤ snj = 1, si âˆˆ [tji , tj(i+1) ], i = 1, . . . , nj âˆ’ 1, h is a sequence of
positive bandwidths satisfying h â†’ 0 and nj h â†’ âˆž as nj â†’ âˆž, and where K(Â·) is a Î½ times
differentiable kernel function that has a compact support on [âˆ’Ï„, Ï„ ] with K(âˆ’Ï„ ) = K(Ï„ ) = 0
and fulfills (4) and its Î½th derivative, KÎ½ , meets the following requirements:
RÏ„
(i) The support of KÎ½ is [âˆ’Ï„, Ï„ ] and âˆ’Ï„ KÎ½ (x)dx = 1;
(ii) For constants Î² âˆˆ R and k â‰¥ Î½ + 2,
ï£±
Z Ï„
j = 0, . . . , Î½ âˆ’ 1, Î½ + 1, . . . , k âˆ’ 1,
ï£² 0
(âˆ’1)Î½ Î½!, j = Î½,
KÎ½ (x)xj dx =
ï£³
âˆ’Ï„
Î²,
j = k.

(6)

In order to obtain a consistent estimator of x(Î½) (t) for t âˆˆ [0, 1], i.e., to eliminate boundary
effects which become prominent when estimating derivatives, we introduce the modified kernel
KÎ½,q with support [âˆ’Ï„, qÏ„ ], for some q âˆˆ [0, 1], satisfying KÎ½,q â†’ KÎ½ as q â†’ 1. Moreover,
KÎ½,q (x) satisfies (6) with a uniformly bounded k-th moment for q, and the asymptotic variance
(Îº)
of KÎ½,q is bounded uniformly for q ([Gasser and MÃ¼ller, 1984]). Therefore, xj (t) is estimated
by
nj Z
tâˆ’u
1 X si
(Îº)
KÎº,q (
x
bj (t) = Îº+1
)du Â· yj (tji ), for Îº = 1, . . . , Î½.
(7)
h
h
i=1 siâˆ’1
[Gasser and MÃ¼ller, 1984] and [Gasser et al., 1985] have discussed the existence of such kernel
functions K(Â·) and KÎ½,q (x), and proved that if the sequence {si } satisfies maxi |si âˆ’ siâˆ’1 âˆ’
R 1 (Î½)
nâˆ’1 | = O(nâˆ’Î´ ) for some Î´ > 1, then 0 kb
x (t) âˆ’ f0 (x(t), x(1) (t), . . . , x(Î½âˆ’1) (t))k22 dt =
OP (nâˆ’2(kâˆ’Î½)/(2k+1) ).

3.2

Stage 2: Deep Neural Network Estimator

In Stage 2, we use a multilayer feedforward neural network to approximate the unknown
ground-truth f0 (Â·). We start off by introducing the definitions of HÃ¶lder smoothness and
compositional functions.

5

Definition 1. A function g : Rr0 â†’ R is said to be (Î², C)-HÃ¶lder smooth for some positive
constants Î² and C, if for every Î³ = (Î³1 , . . . , Î³r0 )> âˆˆ Nr0 the following two conditions hold:
sup
zâˆˆRr0

âˆ‚z1Î³1

âˆ‚ Îºg
Î³ (z) â‰¤ C,
. . . âˆ‚zr0r0

for Îº â‰¤ bÎ²c,

and
âˆ‚ Îºg
âˆ‚ Îºg
Î²âˆ’bÎ²c
(z)
âˆ’
z ) â‰¤ Ckz âˆ’ zek2
,
Î³
Î³ (e
âˆ‚z1Î³1 . . . âˆ‚zr0r0
âˆ‚z1Î³1 . . . âˆ‚zr0r0
P0
where Îº = ri=1
Î³i .

for Îº = bÎ²c and z, ze âˆˆ Rr0 ,

For convenience, we say g is (âˆž, C)-HÃ¶lder smooth if g is (Î², C)-HÃ¶lder smooth for all
Î² > 0. HÃ¶lder smoothness is commonly assumed for estimating regression functions in
the literature on nonparametric function estimation (see, for instance, [Stone, 1985] and
[Ferraty and Vieu, 2006]).
Definition 2. A function f : Rr0 â†’ R is said to have a compositional structure with
parameters (Lâˆ— , r, rÌƒ, Î², a, b, C) for Lâˆ— âˆˆ Z+ , r = (r0 , . . . , rLâˆ— +1 )> âˆˆ ZL+âˆ— +2 , with rLâˆ— +1 =
1, rÌƒ = (rÌƒ0 , . . . , rÌƒLâˆ— )> âˆˆ ZL+âˆ— +1 , Î² = (Î²0 , . . . , Î²Lâˆ— )> âˆˆ RL+âˆ— +1 , a = (a0 , . . . , aLâˆ— +1 )> , b =
(b0 , . . . , bLâˆ— +1 )> âˆˆ RLâˆ— +2 , and C = (C0 , . . . , CLâˆ— )> âˆˆ RL+âˆ— +1 , if
f (z) = gLâˆ— â—¦ . . . â—¦ g1 â—¦ g0 (z),

z âˆˆ [a0 , b0 ]r0

where gi = (gi,1 , . . . , gi,ri+1 )> : [ai , bi ]ri â†’ [ai+1 , bi+1 ]ri+1 for some |ai |, |bi | â‰¤ Ci , and the
functions gi,j : [ai , bi ]rÌƒi â†’ [ai+1 , bi+1 ] are (Î²i , Ci )-HÃ¶lder smooth only relying on rÌƒi variables.
Without loss of generality, we can always assume Ci > 1, i = 1, . . . , Lâˆ— . Denote by
CS(Lâˆ— , r, rÌƒ, Î², a, b, C) the class of compositional functions defined above. It is pointed out
by, for instance, [Schmidt-Hieber, 2020] that a compositional function can be approximated
by a neural network with any order of accuracy. Due to its popularity, it is also adopted by
[Bauer and Kohler, 2019], [Schmidt-Hieber, 2020], [Kohler and Langer, 2019], [Liu et al., 2020],
[Liu et al., 2019], [Wang et al., 2020] to study nonparametric regression problems.
By Definition 2, any function in CS(Lâˆ— , r, rÌƒ, Î², a, b, C) is composed of Lâˆ— layers, and in
the i-th layer, i = 0, . . . , Lâˆ— , there are only rÌƒi "active" variables. This implicitly assumes a
sparsity structure in each layer, which prevents us from the curse of dimensionality. The
intuition behind this definition comes from the structure of feedforward neural network (see
Definition 4). Clearly, a feedforward neural network belongs to CS(Lâˆ— , r, rÌƒ, Î², a, b, C), with
the i-th hidden layer in the network viewed as gi .
In Definition 2, functions in each layer gi,j : [ai , bi ]ri â†’ [ai+1 , bi+1 ] are (Î²i , Ci )-HÃ¶lder
smooth and it is not difficult to verify that the composed function f is also HÃ¶lder smooth.
To proceed, we need to introduce the following two definitions which capture the intrinsic
smoothness and dimension of the compositional functions.
Definition 3 (Intrinsic Smoothness and Intrinsic Dimension). For f âˆˆ CS(Lâˆ— , r, rÌƒ, Î², a, b, C),
the intrinsic smoothness and intrinsic dimension of f are defined as:
Î² âˆ— = Î²iâˆ—âˆ—

and
6

râˆ— = rÌƒiâˆ— ,

Q âˆ—
respectively, where Î²iâˆ— = Î²i Ls=i+1
(Î²s âˆ§ 1) for i = 0, . . . , Lâˆ— , and iâˆ— = argmin0â‰¤iâ‰¤Lâˆ— Î²iâˆ— /rÌƒi . We
QLâˆ—
will adopt the convention s=Lâˆ— +1 (Î²s âˆ§ 1) = 1 for convenience.
It is worth noting that the order of HÃ¶lder smoothness of a function f is less than
its intrinsic smoothness. Throughout the paper, we assume the true functions f0,j âˆˆ
CS(Lâˆ— , r, rÌƒ, Î², a, b, C), j = 1, . . . , d. This assumption is not restrictive, as the compositional structure covers a wide collection of functions. Here are some examples.
Example 1. (Homogeneous Linear ODE system) Consider the following dynamical system
ï£®
ï£¹ ï£®
ï£¹ï£®
ï£¹
x
(t)
x
(t)
a
.
.
.
a
1
1
1,1
1,Î½d
dÎ½ ï£¯ . ï£º ï£¯ . .
ï£º
..
.. ï£º ï£¯
.
.
.
=
ï£° . ï£» ï£° .
ï£».
.
.
. ï£»ï£°
dtÎ½
(Î½âˆ’1)
xd (t)
ad,1 . . . ad,Î½d
xd
(t)
(Î½)

In this example, each component xj has a compositional structure with Lâˆ— = 0, r = (Î½d, 1)> ,
rÌƒ = Î½d, Î² = âˆž. Therefore, Î² âˆ— = âˆž and râˆ— = Î½d.
Example 2. (Additive Model) Consider a more general case than Example 1. Define
P P
(Î½)
(k)
xi = gi ( dj=1 Î½âˆ’1
k=0 fj,k (xj )), where gi (Â·) is (Î²g , Cg )-HÃ¶lder smooth and fj,k (Â·) is (Î²f , Cf )(Î½)

(Î½)

HÃ¶lder smooth. Clearly, xi can be written as a composition of three functions xi = h2 â—¦h1 â—¦h0
P
(Î½âˆ’1)
(Î½âˆ’1)
with h0 (x1 , . . . , xd
) = (f1,0 (x1 ), . . . , fd,Î½âˆ’1 (xd
)), h1 (x1 , . . . , xÎ½d ) = Î½d
i=1 xi , and h2 (x) =
âˆ—
gi (x). Here Lâˆ— = 2, r = (Î½d, Î½d, 1, 1), rÌƒ = (1, Î½d, 1), Î² = (Î²h , âˆž, Î²g ), Î² = min(Î²h , Î²g ), and
râˆ— = 1.
For v = (v1 , . . . , vr )> âˆˆ Rr , define the shifted activation function Ïƒv (x) = (Ïƒ(x1 âˆ’
v1 ), . . . , Ïƒ(xr âˆ’ vr ))> , where Ïƒ(s) := max{0, s} and x = (x1 , . . . , xr ) âˆˆ Rr . We next introduce
the ReLU Feedforward Neural Network which is widely used in the deep learning literature.
Definition 4 (ReLU Feedforward Neural Network). A ReLU feedforward neural network
f (x; W, v) is defined as
f (x; W, v) := WL ÏƒvL . . . W1 Ïƒv1 W0 x,

x âˆˆ Rp0 ,

(8)

where W âˆˆ W := {(W0 , . . . , WL ) : Wl âˆˆ Rpl+1 Ã—pl , 0 â‰¤ l â‰¤ L} is the weight matrix, v âˆˆ V :=
{(v1 , . . . , vL ) : vl âˆˆ Rpl , 1 â‰¤ l â‰¤ L} is the bias term, and Ïƒv is the shifted activation function,
and p0 , . . . , pL+1 are positive integers.
In Definition 4, L is the number of hidden layers and the width vector p = (p0 , . . . , pL+1 )
specifies the number of units in each layer, i.e., the width of the network. The ReLU
feedforward neural network is parameterized by (Wj )j=0,...,L and (vj )j=1,...,L .
In this paper, we consider two subclasses of ReLU Feedforward neural networks:
F1 (L, p) := {f (x; W, v) of form (8) : max kWj kâˆž + |vj |âˆž â‰¤ 1},
j=0,...,L

(9)

where v0 is a vector of zeros, and
L
X
F2 (L, p, Ï„, F ) := {f (x; W, v) âˆˆ F1 (L, p) :
(kWj k0 + |vj |0 ) â‰¤ Ï„, k|f |âˆž kâˆž â‰¤ F }.
j=0

7

(10)

The subclass (9) comprises the fully connected networks with bounded parameters, and
it is not empty in that we can always divide all the weights by the maximum weight. In
practice, people tend to use dropout as a regularization technique to prevent overfitting,
i.e., randomly setting parts of neurons to zero. So it is natural to consider a sparse neural
network as specified in (10). The two subclasses of neural networks are also employed
in [Schmidt-Hieber, 2020] and [Wang et al., 2020] to nonparametrically estimate regression
function based on i.i.d. data and functional data.
Next we present the estimator of f0 (Â·) in (1) via a sparsely connected deep neural network.
(Î½)
(Î½)
(Î½)
b(t) = (b
b(Î½) (t) = (b
Let x
x1 (t), . . . , x
bd (t))> and x
x1 (t), . . . , x
bd (t))> , where x
bj (t) and x
bj (t)
are respectively from (5) and (7). The idea is to search for a member in F2 (L, p, Ï„, F )
to well approximate f0 (x(t), x(1) (t), Â· Â· Â· , x(Î½âˆ’1) (t)). Specifically, the "best" estimator of
f0 (x(t), Â· Â· Â· , x(Î½âˆ’1) (t)), or equivalently x(Î½) (t), is obtained by minimizing
Z 1
2
b(Î½) (t) âˆ’ f (b
b(1) (t), . . . , x
b(Î½âˆ’1) (t); W, v) 2 dt,
x
x(t), x
(11)
0

over all f (z; W, v) âˆˆ F2 (L, p, Ï„, F ) with p = (r0 , p1 , . . . , pL ) and z âˆˆ Rr0 . The resulting
c , vb). The entire two-stage estimation procedure is summarized
estimator is denoted by fb(z; W
below:
Algorithm 1
1:
2:
3:

4:
5:
6:
7:

Input: observed values {y1 , . . . , yd }.
for each component j, j = 1, . . . , d do
(Îº)
Estimate xj (t) and its higher order derivative xj (t), Îº = 1, . . . , Î½ through
Pnj R si
(Îº)
K( tâˆ’u
(5) and (7), respectively: x
bj (t) = h1 i=1
)du Â· yj (tji ) and x
bj (t) =
h
siâˆ’1
R
P
nj
si
tâˆ’u
1
for Îº = 1, . . . , Î½
i=1 siâˆ’1 KÎº,q ( h )du Â· yj (tji ),
hÎº+1
where the tuning parameter can be determined by cross validation.
end for
b(1) (t), . . . , x
b(Î½âˆ’1) (t)) as the input to fit x
b(Î½) (t) through (11).
Use (b
x(t), x
R 1 (Î½)
b (t) âˆ’ f (b
b(1) (t), . . . , x
b(Î½âˆ’1) (t); W, v)
x(t), x
Output: fb(z; WÌ‚ , vÌ‚) := argminf âˆˆF2 (L,p,Ï„,F ) 0 x

c , vb) as an estimator of
The following theorem establishes the consistency of fb(z; W
(Î½âˆ’1)
f0 (x(t), . . . , x
(t)) and its convergence rate.
Theorem 1. Suppose the true functions f0,j âˆˆ CS(Lâˆ— , r, rÌƒ, Î², a, b, C), j = 1, . . . , d. Consider
the subclass F2 (L, p, Ï„, F ). Further assume that F â‰¥ maxi=0,...,Lâˆ— (Ci , 1), N := mini=1,...,L pi â‰¥
6Î· maxi=0,...,Lâˆ— (Î²i + 1)rÌƒi âˆ¨ (CÌƒi + 1)erÌƒi ) where Î· = maxi=0,...,Lâˆ— (ri+1 (rÌƒi + dÎ²i e)), and Ï„ . LN .
Then we have
Z 1
c , vb) âˆ’ f0 (x(t), . . . , x(Î½âˆ’1) (t))k22 dt = OP (Ï‚n ),
kfb(x(t), . . . , x(Î½âˆ’1) (t); W
0

Z
0

1

c , vb) âˆ’ f0 (x(t), . . . , x(Î½âˆ’1) (t))k2 dt = OP (Ï‚n ),
b(Î½âˆ’1) (t); W
kfb(b
x(t), . . . , x
2
8

2
2

dt

where
Ï‚n = (1 + N L )nâˆ’2(kâˆ’Î½)/(2k+1) + (N 2âˆ’L )2

QLâˆ—

l=1

Î²l âˆ§1

2Î² âˆ—

+ N âˆ’ râˆ— +

LN log(n) + L2 N log(LN )
,
n

Î² âˆ— and râˆ— are the intrinsic smoothness and intrinsic dimension defined in Definition 3, and
CÌƒi â€™s are constants only depending on C, a, b.
c , vb) is guaranteed, if, for instance, L  (log n)1/2 and N 
The consistency of fb(z; W
1/4
e(log n) , as the latter implies Ï‚n â†’ 0 as n â†’ âˆž.

4

Numerical experiments

In this section, we carry out extensive simulations to assess the performance of the
proposed two-stage estimation procedure for two different designs of ODE systems.

4.1

Simulation Designs

In the first simulation (Design 1), we suppose that the true process is governed by the
following linear ODE system: x(1) (t) = Ax(t) + b, 0 â‰¤ t â‰¤ 1, where A is a d Ã— d sparse matrix
and b is a d-dimensional vector referred to as the initial value. Elements of b are sampled
from the uniform distribution U (0, 1). We further define a set J = {j1 , j2 , j3 , j4 , j5 }, where
jk , k = 1, . . . , 5, are randomly sampled from {1, . . . , d} without replacement. The sparse
matrix A = (aij ) is defined as, for i = 1, . . . , d,

=0
j 6âˆˆ J,
aij
âˆ¼ U (0, 1) j âˆˆ J.
In other words, each row of A has five non-zero values that are sampled from the uniform
distribution. It is easy to verify f0,j âˆˆ CS(Lâˆ— , r, rÌƒ, Î², a, b, C), j = 1, . . . , d, with Lâˆ— = 0,
r = (d, 1)> , rÌƒ = 5, Î² = âˆž, Î² âˆ— = âˆž, and râˆ— = 5. For simplicity, we set n1 = n2 = . . . = nd = n.
The "observed" data are generated from yji = xj (ti ) + ji , i = 1, . . . , n, j = 1, . . . , d, where
ti = i/n and ji are sampled from N (0, 1). The sample size n and dimension d are chosen to
be n = 100, 200, 500 and d = 10, 100, 1000.
In the second simulation (Design 2), we consider a second order nonlinear ODE system:
(2)
(1)
(2)
(1)
(2)
(2)
(1)
(2)
x1 (t) = 2 xx13 (t)
+ 4x4 (t) âˆ’ x3 (t)x4 (t), x2 (t) = âˆ’x4 (t), x3 (t) = 2, x4 (t) = x2 (t), x5 (t) =
(t)
(2)

(1)

(2)

(1)

(1)

(2)

(1)

x5 (t), x6 (t) = âˆ’x25 (t)x6 (t) + x6 (t), x7 (t) = x7 (t)x2 (t) âˆ’ x2 (t)x7 (t), x8 (t) = âˆ’(x8 (t))2 .
The noisy data are computed via yji = xj (ti ) + ji , for i = 1, . . . , n and j = 1, . . . , 8, with
ti = i/n and ji sampled independently from N (0, Ïƒ 2 ). We consider various noise levels:
Ïƒ = 0.2, 0.5, 0.8, and various sample sizes: n = 100, 200, 500.

9

Table 1: Simulation Design 1
(a) Metric M1

n = 100
n = 200
n = 500

d = 10
0.314
0.196
0.109

d = 100
0.345
0.188
0.116

(b) Metric M3

d = 1000
0.646
0.209
0.112

n = 100
n = 200
n = 500

(c) Metric M2

n = 100
n = 200
n = 500

d = 10
0.404
0.212
0.124

d = 100
0.478
0.223
0.146

d = 10
1.027
0.631
0.320

d = 100
2.191
1.014
0.638

d = 1000
6.740
1.779
0.892

(d) Metric M4

d = 1000
0.844
0.264
0.160

n = 100
n = 200
n = 500

d = 10
1.132
0.686
0.343

d = 100
2.342
1.034
0.643

d = 1000
7.203
1.834
0.947

Table 2: Simulation Design 2
(a) Metric M1

n = 100
n = 200
n = 500

Ïƒ = 0.2
0.041
0.037
0.030

Ïƒ = 0.5
0.067
0.047
0.037

(b) Metric M3

Ïƒ = 0.8
0.119
0.077
0.046

n = 100
n = 200
n = 500

(c) Metric M2

n = 100
n = 200
n = 500

Ïƒ = 0.2
0.053
0.047
0.042

Ïƒ = 0.5
0.103
0.084
0.072

Ïƒ = 0.2
0.182
0.172
0.159

Ïƒ = 0.5
0.243
0.190
0.173

Ïƒ = 0.8
0.412
0.262
0.192

(d) Metric M4

Ïƒ = 0.8
0.165
0.113
0.083

n = 100
n = 200
n = 500

10

Ïƒ = 0.2
0.194
0.179
0.177

Ïƒ = 0.5
0.329
0.268
0.228

Ïƒ = 0.8
0.541
0.360
0.260

4.2

Simulation results

In each simulation design, we employ four metrics to evaluate the performance of the
proposed two-stage estimator:
Z 1
M1 =
fb(x(t), x(1) (t), . . . , x(Î½âˆ’1) (t)) âˆ’ f0 (x(t), . . . , x(Î½âˆ’1) (t)) dt,
2
Z0 1
b(1) (t), . . . , x
b(Î½âˆ’1) (t)) âˆ’ f0 (x(t), . . . , x(Î½âˆ’1) (t)) dt,
M2 =
fb(b
x(t), x
2
0
Z 1
M3 = max
fË†j (x(t), x(1) (t), . . . , x(Î½âˆ’1) (t)) âˆ’ f0,j (x(t), . . . , x(Î½âˆ’1) (t)) dt, and
j=1,...,d 0
Z 1
b(1) (t), . . . , x
b(Î½âˆ’1) (t)) âˆ’ f0,j (x(t), . . . , x(Î½âˆ’1) (t)) dt.
M4 = max
fË†j (b
x(t), x
j=1,...,d

0

Metrics M1 and M3 measure the differences between the target and the second-stage deep
neural network estimator in the L2 norm and in the max norm, respectively, while M2 and
M4 gauge the overall performance of the proposed two-stage estimator in these two norms
respectively. A total of 100 independent replications are run for each design and the results
are summarized in Tables 1 and 2.
As expected, the deviations measured by M2 (M4 ) are greater than those by M1 (M3 )
unanimously. Our results further show that the accuracy of the estimator diminishes as the
dimension of the ODE grows and/or the noise-to-signal ratio increases. Moreover, a larger
sample size yields a smaller deviation across all cases, which is in line with the theoretical
results.

5

Real Data Analysis

In this section, we illustrate an application of our method to the Covid-19 infection cases.
The data are downloaded from New York Times. (2021) https://github.com/nytimes/
covid-19-data. This dataset contains daily new COVID-19 cases reported in individual
states across the United States during 03/23/2020â€“05/07/2021, with 411 observations for
each state. As an example, the orange curves in the four panels of Figure 1 display the
number of the recorded daily new cases in four states, respectively: California, Texas, New
York, and Florida.
Here, we attempt to use a system of ODEs to characterize the rate of changes of new cases
over the 50 states simultaneously. Specifically, we consider a 50-dimensional second-order
ODE system, as in (1) with x(t) = (x1 (t), . . . , x50 (t))> representing the number of Covid-19
infection cases at time t. With the daily new cases, we first calculate the accumulated
daily cases for each state, i.e., yji in (3). We then apply the proposed two-stage estimation
approach to estimate the function f0 . The filtered daily new cases are shown as blue curves
in Figure 1 for the four selected states, while the deep neural network estimator fb0 is shown
in green. The results demonstrate the effectiveness and foreseeability of our method: when
the estimated growth rate reaches the highest value, the daily new cases will peak roughly
one month later.
11

To further illustrate the strength of our approach, we compare the proposed method with
the traditional nonparametric estimation of the derivative of the regression function that
estimates the growth rate of each state separately. The top two panels in Figure 2 show the
growth rates that are estimated separately for three states in the western (Utah, Nevada, and
Idaho) and northeastern (Connecticut, Rhode Island, and Massachusetts) U.S., respectively.
The growth rates estimated using our methods are depicted in the bottom two panels. It is
apparent that the estimates based on the two methods are quite different, and our estimates
are more consistent with what is expected: geographically adjacent states should have strong
interactions and hence share similar growth rates. This further shows the superiority of our
method, in that it effectively incorporates the interactive processes among the neighboring
states.
1500

Estimated Daily New Cases
Observed Daily New Cases
Ì‚
f0,

1000

California
800
600

j

Texas

Estimated Daily New Cases
Observed Daily New Cases
Ì‚
f0,
j

400

500

200

0

0

âˆ’500

âˆ’200

âˆ’1000

âˆ’400
âˆ’600

âˆ’1500

Estimated Daily New Cases
Observed Daily New Cases
Ì‚
f0,

600

New York

800
600

j

400

Florida

Estimated Daily New Cases
Observed Daily New Cases
Ì‚
f0,
j

400

200

200

0

0

âˆ’200

âˆ’200

2

-0
020

5
2

-0
020

7
2

-0
020

9
2

-1
020

1
2

-0
021

1
2

-0
021

3
2

-0
021

âˆ’400

5

202

0-0

5

202

0-0

7

202

0-0

9

202

0-1

1

202

1-0

1

202

1-0

3

202

1-0

5

Figure 1: Green curve is the estimated growth rate for California, Texas, New York State
and Texas. Blue Curve is the estimated daily new cases and yellow curve is the observed
daily new cases.

12

Individual Estimation

1eâˆ’5
2

Connecticut
Massachusetts
Rhode Island

3
2

1

1

0

0

âˆ’1

âˆ’1

âˆ’2

2

Individual Estimation

1eâˆ’5
Nevada
Idaho
Utah

âˆ’2

Joint Estimation

1eâˆ’5

Joint Estimation

1eâˆ’5
Nevada
Idaho
Utah

1

Connecticut
Massachusetts
Rhode Island

2
1

0

0

âˆ’1

âˆ’1

âˆ’2

âˆ’2

âˆ’3
202

0-0

5

202

0-0

7

202

0-0

9

202

0-1

1

202

1-0

1

202

1-0

3

202

1-0

5

202

0-0

5

202

0-0

7

202

0-0

9

202

0-1

1

202

1-0

1

202

1-0

3

202

1-0

5

Figure 2: Comparison between individual estimation and joint estimation (the growth rates
are standardized by population size).Top two panels: estimate individually. Bottom two
panels: estimate jointly after calibrating using deep neural network. Left two panels: Western
States. Right two panels: Eastern States.

13

References
[Bauer and Kohler, 2019] Bauer, B. and Kohler, M. (2019). On deep learning as a remedy
for the curse of dimensionality in nonparametric regression. Ann. Statist., 47(4):2261â€“2285.
[Benson, 1979] Benson, M. (1979). Parameter fitting in dynamic models. Ecological Modelling,
6(2):97 â€“ 115.
[Bhaumik and Ghosal, 2015] Bhaumik, P. and Ghosal, S. (2015). Bayesian two-step estimation in differential equation models. Electron. J. Statist., 9(2):3124â€“3154.
[Biegler et al., 1986] Biegler, L. T., Damiano, J. J., and Blau, G. E. (1986). Nonlinear
parameter estimation: A case study comparison. AIChE Journal, 32(1):29â€“45.
[Champion et al., 2019] Champion, K., Lusch, B., Kutz, J. N., and Brunton, S. L. (2019).
Data-driven discovery of coordinates and governing equations. Proceedings of the National
Academy of Sciences, 116(45):22445â€“22451.
[Chen et al., 2018] Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K.
(2018). Neural ordinary differential equations. In Bengio, S., Wallach, H., Larochelle, H.,
Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information
Processing Systems, volume 31, pages 6571â€“6583. Curran Associates, Inc.
[Chen et al., 2017] Chen, S., Shojaie, A., and Witten, D. M. (2017). Network reconstruction
from high-dimensional ordinary differential equations. Journal of the American Statistical
Association, 112(520):1697â€“1707. PMID: 29618851.
[ElbrÃ¤chter et al., 2021] ElbrÃ¤chter, D., Perekrestenko, D., Grohs, P., and BÃ¶lcskei, H. (2021).
Deep neural network approximation theory. IEEE Transactions on Information Theory,
67(5):2581â€“2623.
[Farrell et al., 2021] Farrell, M. H., Liang, T., and Misra, S. (2021). Deep neural networks
for estimation and inference. Econometrica, 89(1):181â€“213.
[Ferraty and Vieu, 2006] Ferraty, F. and Vieu, P. (2006). Nonparametric functional data
analysis: theory and practice. Springer Science & Business Media.
[Gasser and MÃ¼ller, 1984] Gasser, T. and MÃ¼ller, H.-G. (1984). Estimating regression functions and their derivatives by the kernel method. Scandinavian Journal of Statistics,
11(3):171â€“185.
[Gasser et al., 1985] Gasser, T., MÃ¼ller, H.-G., and Mammitzsch, V. (1985). Kernels for
nonparametric curve estimation. Journal of the Royal Statistical Society. Series B (Methodological), 47(2):238â€“252.
[Hall and Ma, 2014] Hall, P. and Ma, Y. (2014). Quick and easy one-step parameter estimation in differential equations. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 76(4):735â€“748.
14

[Hammer, 2000] Hammer, B. (2000). On the approximation capability of recurrent neural
networks. Neurocomputing, 31(1):107 â€“ 123.
[He et al., 2016] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for
image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 770â€“778.
[Henderson and Michailidis, 2014] Henderson, J. and Michailidis, G. (2014). Network reconstruction using nonparametric additive ode models. PLOS ONE, 9(4):1â€“15.
[Henderson and Loreau, 2019] Henderson, K. and Loreau, M. (2019). An ecological theory
of changing human population dynamics. People and Nature, 1(1):31â€“43.
[Kobyzev et al., 2020] Kobyzev, I., Prince, S., and Brubaker, M. (2020). Normalizing flows:
An introduction and review of current methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence, pages 1â€“1.
[Kohler and Langer, 2019] Kohler, M. and Langer, S. (2019). On the rate of convergence
of fully connected very deep neural network regression estimates. arXiv e-prints, page
arXiv:1908.11133.
[Li et al., 2019] Li, Q., Lin, T., and Shen, Z. (2019). Deep learning via dynamical systems:
An approximation perspective. ArXiv, abs/1912.10382.
[Li et al., 2020] Li, Z., Han, J., E, W., and Li, Q. (2020). On the Curse of Memory in
Recurrent Neural Networks: Approximation and Optimization Analysis. arXiv e-prints,
page arXiv:2009.07799.
[Liang and Wu, 2008] Liang, H. and Wu, H. (2008). Parameter estimation for differential
equation models using a framework of measurement error in regression models. Journal of
the American Statistical Association, 103(484):1570â€“1583. PMID: 19956350.
[Liu et al., 2019] Liu, R., Boukai, B., and Shang, Z. (2019). Optimal Nonparametric Inference
via Deep Neural Network. arXiv e-prints, page arXiv:1902.01687.
[Liu et al., 2020] Liu, R., Shang, Z., and Cheng, G. (2020). On Deep Instrumental Variables
Estimate. arXiv e-prints, page arXiv:2004.14954.
[Lu et al., 2020] Lu, J., Shen, Z., Yang, H., and Zhang, S. (2020). Deep Network Approximation for Smooth Functions. arXiv e-prints, page arXiv:2001.03040.
[Lu et al., 2011] Lu, T., Liang, H., Li, H., and Wu, H. (2011). High-dimensional odes coupled
with mixed-effects modeling techniques for dynamic gene regulatory network identification.
Journal of the American Statistical Association, 106(496):1242â€“1258. PMID: 23204614.
[Lusch et al., 2018] Lusch, B., Kutz, J., and Brunton, S. (2018). Deep learning for universal
linear embeddings of nonlinear dynamics. Nat Commun, 9, 4950.

15

[Paul et al., 2016] Paul, D., Peng, J., and Burman, P. (2016). Nonparametric estimation of
dynamics of monotone trajectories. The Annals of Statistics, 44(6):2401â€“2432.
[Priestley and Chao, 1972] Priestley, M. B. and Chao, M. (1972). Non-parametric function
fitting. Journal of the Royal Statistical Society: Series B (Methodological), 34(3):385â€“392.
[Schmidt-Hieber, 2020] Schmidt-Hieber, J. (2020). Nonparametric regression using deep
neural networks with relu activation function. Ann. Statist., 48(4):1875â€“1897.
[Stone, 1985] Stone, C. J. (1985). Additive regression and other nonparametric models. The
annals of Statistics, pages 689â€“705.
[Sun et al., 2020] Sun, M., Zeng, D., and Wang, Y. (2020). Modelling temporal biomarkers
with semiparametric nonlinear dynamical systems. Biometrika, 108(1):199â€“214.
[Talwar and Namachchivaya Sri, 1992] Talwar, S. and Namachchivaya Sri, N. (1992). Control
of chaotic systems: Application to the lorenz equations. In Nonlinear Vibrations, American
Society of Mechanical Engineers, Design Engineering Division (Publication) DE, pages
47â€“58. Publ by ASME. Winter Annual Meeting of the American Society of Mechanical
Engineers ; Conference date: 08-11-1992 Through 13-11-1992.
[Wang et al., 2020] Wang, S., Cao, G., and Shang, Z. (2020). Estimation of the Mean Function
of Functional Data via Deep Neural Networks. arXiv e-prints, page arXiv:2012.04573.
[Weinan, 2017] Weinan, W. (2017). A proposal on machine learning via dynamical systems.
Communications in Mathematics and Statistics, 5(1).
[Wu et al., 2019] Wu, L., Qiu, X., xiang Yuan, Y., and Wu, H. (2019). Parameter estimation
and variable selection for big systems of linear ordinary differential equations: A matrixbased approach. Journal of the American Statistical Association, 114(526):657â€“667.

16

