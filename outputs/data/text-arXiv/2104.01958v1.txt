1

A Generalized Unscented Transformation for
Probability Distributions

arXiv:2104.01958v1 [stat.ME] 5 Apr 2021

Donald Ebeigbe, Tyrus Berry, Michael M. Norton, Andrew J. Whalen,
Dan Simon, Timothy Sauer, and Steven J. Schiff

Abstract—The unscented transform uses a weighted set of
samples called sigma points to propagate the means and covariances of nonlinear transformations of random variables. However, unscented transforms developed using either the Gaussian
assumption or a minimum set of sigma points typically fall
short when the random variable is not Gaussian distributed and
the nonlinearities are substantial. In this paper, we develop the
generalized unscented transform (GenUT), which uses adaptable
sigma points that can be positively constrained, and accurately
approximates the mean, covariance, and skewness of an independent random vector of most probability distributions, while being
able to partially approximate the kurtosis. For correlated random
vectors, the GenUT can accurately approximate the mean and
covariance. In addition to its superior accuracy in propagating
means and covariances, the GenUT uses the same order of
calculations as most unscented transforms that guarantee thirdorder accuracy, which makes it applicable to a wide variety of
applications, including the assimilation of observations in the
modeling of the coronavirus (SARS-CoV-2) causing COVID-19.
Index Terms—Unscented transform, Probability distributions,
Estimation, Kalman filtering, Infectious disease, COVID-19

I. I NTRODUCTION

T

HE Kalman filter provides the basis for most of the
popular state estimation techniques used for linear and
nonlinear dynamic systems. The linear Kalman filter works
by propagating the means and covariance of the state of
a dynamic system [1], [2]. Originally developed under the
Gaussian assumption for measurement and process noise, the
This work was supported by NIH Director’s Transformative Award
No. 1R01AI145057, and from the National Science Foundation DMS1723175, DMS-1854204, and DMS-2006808. (Corresponding author:
dee5127@psu.edu).
D. Ebeigbe and M. M. Norton are with the Center for Neural Engineering, Department of Engineering Science and Mechanics, Pennsylvania State University, University Park, PA, USA (email: dee5127@psu.edu;
mmn5439@psu.edu).
T. Berry and T. Sauer are with the Department of Mathematical Sciences, George Mason University, Fairfax, VA, USA (email: tberry@gmu.edu;
tsauer@gmu.edu ).
A. J. Whalen is with the Center for Neural Engineering, Department of
Engineering Science and Mechanics, Pennsylvania State University, University Park, PA, USA, and also with the Department of Neurosurgery,
Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA
(email: awhalen7@mgh.harvard.edu ).
D. Simon is with the Department of Electrical Engineering and Computer Science, Cleveland State University, Cleveland, OH, USA (email:
d.j.simon@csuohio.edu).
S. J. Schiff is with the Center for Neural Engineering, Department of
Engineering Science and Mechanics, Pennsylvania State University, University
Park, PA, USA, and also with the Department of Physics, Pennsylvania
State University, University Park, PA, USA, and also with the Department
of Neurosurgery, Penn State College of Medicine, Hershey, PA, USA (email:
sschiff@psu.edu).

Kalman filter is the optimal estimator when this assumption is
satisfied. Under non-Gaussian noise, the Kalman filter is the
optimal linear estimator but its performance can sometimes
deteriorate [1], [3].
For many dynamic systems in practice, linearity is a reasonable assumption. For others, system non-linearities cause
methods based on linear models to perform poorly. Most
nonlinear systems can behave approximately linearly over
small operation ranges. The extended Kalman filter (EKF)
is one of the most widely used Kalman filter for nonlinear
dynamic systems. The EKF employs a linear approximation of
the nonlinear system around a nominal state trajectory [1], [2].
However, for highly nonlinear systems, linear approximations
can introduce errors that can lead to divergence of the state
estimate.
To address the drawbacks of the EKF, several well-known
state estimators such as the ensemble Kalman filter [4]–[7],
the unscented Kalman filter (UKF) [8], [9], and the particle
filter [1], [10] have been developed. Although the particle filter
can give better performance than the UKF, this comes at the
cost of a higher computational effort. In some applications,
the improved performance might not be worth the additional
computational costs [1].
The UKF is a nonlinear filter that uses the unscented
transformation to approximate the mean and covariance of a
Gaussian random variable [8], [11]. The unscented transform
uses the intuition that with a fixed number of parameters
it should be easier to approximate a Gaussian distribution
than it is to approximate an arbitrary nonlinear function or
transformation [8]. It produces sets of vectors called sigma
points that capture the moments of the standard Gaussian
distribution. The UKF uses the generated sigma points to
obtain estimates of the states and the state estimation error
covariance. The UKF has been used to generate proposal
distributions which improve the performance of a particle
filter [12], [13]. It has also been employed to improve the
performance of the EnKF [14]. Despite the several types of
sigma points that exist in the literature [15], [16], a majority of
them that were not developed using the Gaussian assumption
do not try to match the skewness or kurtosis of a random
variable, thereby ensuring only second-order accuracy.
The need to effectively monitor, predict, and control the
spread of infectious disease has led to the application of
numerous state estimation techniques. The EKF [17], [18]
and the particle filter [19] have been used to estimate the
parameters of the measles virus transmission dynamics from
real data. The ensemble adjustment Kalman filter (EAKF) has

2

been employed in the forecasting of influenza [20] and dengue
fever [21]. Several infectious disease such as Ebola [22],
HIV [23], and neonatal sepsis [24] have seen implementation
of different Kalman filters. More recently, the outbreak of the
novel coronavirus (SARS-CoV-2) causing COVID-19 has led
to concerted efforts to properly understand its transmission
and offer policy guidelines that can mitigate its spread. Recent
efforts have employed the iterated EAKF to assimilate daily
observations in the modeling of COVID-19 [25]. Distributions
such as Poisson, negative-binomial, and binomial are typically
used for modeling infectious disease from count data. Additionally, the number of patients arriving at a hospital or a
testing center can be modeled by a Poisson distribution whose
rate is proportional to the infected population. Although the
use of standard Kalman filters in infectious disease estimation
and prediction under the Poisson assumption can be justified
with the fact that a Poisson distribution with a large rate can
be approximated by a Gaussian distribution of the same mean
and variance, the approximation breaks down when the rates
are small [26].
The usage of Kalman filters to assimilate data generated
by the transformation of random variables from different
probability distributions has revealed a fundamental mismatch
in the application of the filters – the accuracy of the filter is
reduced if the Gaussian assumption is not satisfied and the
nonlinearities are high. Given the mismatch between current
Kalman filter assumptions and modeling frameworks that
employ a wide range of transformed variables, we present
a new method that uses a fixed number of sigma points to
approximate an arbitrary probability distribution. The intuition
is that employing sigma points more suitable to the inherent
distributions of a random vector can lead to a more accurate
propagation of means and covariances. We develop the generalized unscented transform (GenUT) which is able to adapt
to the unique statistics of most probability distributions. We
show that for any random vector of an arbitrary probability
distribution, the GenUT gives accurate approximations up to
its second moment (second-order accuracy). Additionally, for
any independent random vector of an arbitrary probability
distribution, the GenUT gives accurate approximations up to
its third moment, while achieving partial accuracy in approximating the fourth moment.
In Section II, we discuss the problems that arise when the
Gaussian assumption is employed in the unscented transform.
In Section III, we develop the GenUT sigma points that can
capture certain properties of most probability distributions,
such as its mean, covariance, and skewness. In Section IV, we
show that these sigma points are accurate in approximating the
first three moments of an arbitrarily distributed independent
random vector, thus exceeding the performance of the original
unscented transform [8]. In Section V, we address positivity
constraints and develop additional sigma points that ensure that
such constraints are never violated while maintaining thirdorder accuracy. In Section VI, we evaluate the accuracy of the
GenUT sigma points in propagating the mean and covariance
of nonlinear transformations of arbitrarily distributed random
vectors and we give several examples that demonstrate its
effectiveness. We discuss the conclusions in Section VII.

II. T HE G AUSSIAN A SSUMPTION P ROBLEM
We analyze the performance of the unscented transform
developed in [8], [11]. We only focus the UT of [8], [11] as
opposed to other unscented transforms existing in the literature
because, to our best knowledge, a majority were developed
under the Gaussian assumption. The resulting problem even
extends to unscented transforms that were developed without
using the Gaussian assumption [27], where no probability
distribution was considered in the development of the sigma
points. We will show how linearization approximations, via
Taylor series expansion of a nonlinear transformation of a
random vector x evaluated about its mean x̄, introduces errors
in the propagation of means and covariances. We will see
that errors can be introduced in the propagation of means
and covariances beyond the second order when sigma points
developed under the Gaussian assumption [8], [11] are used
to approximate the nonlinear function λ(x) of a non-Gaussian
distributed random vector x. We note that the nonlinear
transformation y ∈ Rn is given by
y = λ(x)

(1)

Given the mean x̄ and covariance matrix P , the sample mean
and covariance of the nonlinear transformation of (1) can be
calculated as follows [8].
1) Calculate the 2n + 1 sigma points given by
κ
χ0 = x̄
w0 =
n+κ
p

1
χi = x̄ +
(2)
(n + κ)P
wi =
2(n + κ)
i
p

1
χi+n = x̄ −
(n + κ)P
wi =
2(n + κ)
i
p

where
(n + κ)P
is the ith row or column of
i
p
(n + κ)P , wi is the weight associated with the ith
sigma point, and κ is a free parameter. We typically
set κ = n − 3 to minimize the fourth-order moment
mismatch [8].
2) Pass the sigma points through the known nonlinear
function to get the transformed sigma points
Yi = λ(χi )

(3)

3) Evaluate the sample mean of the transformed sigma
points
ȳ =

2n
X

wi Yi

(4)

i=0

4) Evaluate the sample covariance of the transformed sigma
points
Py =

2n
X

wi (Yi − ȳ)(Yi − ȳ)T

(5)

i=0

A. Accuracy in Approximating the True Mean
Applying a Taylor series expansion of λ(x) about its mean
x̄, we show in Appendix A-A that the true mean of y = λ(x)
is given as

3


n
n
X
X
Pij ∂ 2 λ
∂3λ
Sijk
ȳ =λ(x̄) +
+

2! ∂xi ∂xj
3! ∂xi ∂xj ∂xk
i,j=1
i,j,k=1

n

X
Kijkl
∂4λ
+
4! ∂xi ∂xj ∂xk ∂xl 
i,j,k,l=1
x=x̄

 5
Dx̃ λ Dx̃6 λ
+
+ ···
(6)
+E
5!
6!
where the elements of the skewness tensor S are defined as
Sijk = E [(x − x̄)i (x − x̄)j (x − x̄)k ]

(7)

and the elements of the kurtosis tensor K are defined as
Kijkl = E [(x − x̄)i (x − x̄)j (x − x̄)k (x − x̄)l ]

(8)

Using the sigma points of (2), the analytical expression for
the approximated mean from [8] is given as
n
∂λ
1 X
Pij
ȳu = λ(x̄) +
2 i,j=1
∂xi ∂xj x=x̄

2n  4
X
Dσi λ Dσ6 i λ
1
+
+ ···
(9)
+
2(n + κ) i=1
4!
6!
Comparing the above equation with the true mean of (6),
we notice the following problems about the sigma points
developed using the Gaussian assumption
1) The odd-powered moments in the approximation of the
true mean are always zero due to their symmetry. This
introduces significant approximation errors in situations
where the odd-powered moments of the distribution of x
are non-zero and the transformation y = λ(x) is highly
nonlinear.
2) The fourth-order term fails to capture a part of the true
Kurtosis even when the optimal value of κ = n − 3 is
selected because of the Gaussian assumption.
We note that we can arrive at the same conclusion of errors
in approximating the mean beyond the second order when the
reduced sigma points existing in the literature are analyzed
– this is because they do not account for the skweness and
kurtosis of x when it is not Gaussian distributed.
B. Accuracy in Approximating the True Covariance Matrix
The true covariance matrix, which was evaluated in Appendix A-B, is given as

 2

n
 X
Sijk
∂ λ ∂λT
∂λ ∂ 2 λT
Py =ΛP ΛT +
+

2! ∂xi ∂xj ∂xk
∂xi ∂xj ∂xk
i,j,k=1

n
X
1
∂3λ
∂λT
+
Kijkl
3! ∂xi ∂xj ∂xk ∂xl
i,j,k,l=1

1 ∂λ
∂ 3 λT
1 ∂ 2 λ ∂ 2 λT
+
+
3! ∂xi ∂xj ∂xk ∂xl
4 ∂xi ∂xj ∂xk ∂xl


T 

n

X Pij ∂ 2 λ
 · · · 
+
+ ···
(10)

2 ∂xi ∂xj

i,j=1
x=x̄

where we have used the notation XX T = X[· · · ]T . The
analytical expression for the approximated covariance matrix
from [8] is given as
2n 
X
Dσi λ(Dσ3 i λ)T
1
Pu =ΛP ΛT +
2(n + κ) i
3!

T
3
2
2
D λ(Dσi λ)
D λ(Dσi λ)T
+ σi
+ σi
3!
2! × 2!
 T

n
2
X
1
∂
λ
 · · ·  + · · · (11)
+ 
Pij
2 i,j=1
∂xi ∂xj x=x̄
Comparing the above equation with the true covariance matrix
of (10), we notice similar issues that were pointed out in
approximating the mean – the approximation is only accurate
up to the second order when x is not Gaussian distributed. All
the odd-powered moments are zero because of the symmetric
nature of the sigma points, while the fourth-powered moment
is also inaccurate because of the Gaussian nature of the
sigma points. As with the mean approximation, errors in the
covariance matrix approximation are introduced beyond the
second order when the reduced sigma points existing in the
literature are used to approximate x because its skewness and
kurtosis are not captured.
III. G ENERALIZED U NSCENTED T RANSFORM
A third-order unscented transform uses a set of sigma points
to accurately capture the first three moments of a random
vector. Following [11], we define x ∈ Rn as a random vector
that can follow any probability distribution.
Theorem III.1. Given a random vector x with mean x̄ and
covariance P , a user-specified arbitrarily distributed random
vector z with zero mean and unit variance is defined as
z = C −1 (x − x̄)

(12)

where C is the matrix square root of P , CC T = P .
Proof. Taking the expected value of (12) gives the mean as
E [z] = C −1 E[x − x̄]
=0

(13)

The covariance of (12) is evaluated as




E zz T = C −1 E (x − x̄)(x − x̄)T C −1
= C −1 P C −1 = I

(14)

where I is the identity matrix.
To accurately capture the higher moments of x, we use the
following assumption
Assumption III.1. Let x ∈ Rn be an independent random
vector that can follow any probability distribution.
Assumption III.2. In addition to the availability of its mean x̄
and covariance P , the skewness (third central moment) Si =
E[(x − x̄)3i ] and the kurtosis (fourth central moment) Ki =
E[(x − x̄)4i ] of xi for i = 1, · · · , n are also available.

4

(a)

(b)
Fig. 1. (a) Samples chosen for a one-dimensional distribution for the GenUT.
The locations and weights of the sigma points are determined by the moments
of the probability distribution. (b) Symmetric samples chosen for a onedimensional Gaussian distribution [11].

Using the assumpion that x is an independent random
vector, the third central moment can be evaluated as
Sijk = E [(x − x̄)i (x − x̄)j (x − x̄)k ]
(
Si if i = j = k
=
0 otherwise

try to capture the fourth moment of z. To capture the first
three moments in a single dimension, three points are used:
the first point lies at the origin with a weight of w0 ; the second
point lies at a distance −s1 from the origin with a weight
of w1 ; the third point lies at a distance s2 from the origin
with a weight of w2 . A comparison between our sigma points
and the sigma points of [11] in one dimension is shown in
Fig. 1. In our sigma points, we have two points that have
their own adjustable weight and adjustable distance away from
their mean while the sigmas point of [11] have two points that
share the same weights and distance away from their mean.
Therefore, in one-dimension, we use the following 3 sigma
points
χ(0) = {0}, w0
χ(1) = {−s1 }, w1
χ(2) = {s2 }, w2
where w0 , w1 , and w2 are the weights for the respective sigma
points. Obeying the moments of z in (19)–(21) and the fact
that the sum of all weights should equal 1, we write

(15)

while the fourth central moment can be evaluated as
Kijkl

= E [(x − x̄)i (x − x̄)j (x − x̄)k (x − x̄)l ]


if i = j = k = l
Ki
= Pii Pjj if i = k 6= j = l


0
otherwise

(16)

where Pij is the ith element in the jth column of P . We
select the higher-order central moments of z in (12) to match
the standardized moments of x up to at least the third order.
This is done by selecting sigma point distributions that have
the flexibility to either be symmetric when x is symmetrically
distributed or be asymmetric when x is asymmetrically distributed. To aid in the selection of our sigma points, we define
the standardized moments S̃i and K̃i for i ∈ [1, n] such that
Si
σi3
Ki
K̃i = 4
σi
S̃i =

(17)
(18)

√
where σi = Pii is the standard deviation of xi . Let zi be the
ith element of the user-selected arbitrarily distributed random
vector z, and let z̄i be the mean of zi . The first four moments
are selected as
E [zi ] = 0


E (z − z̄)2i = 1


E (z − z̄)3i = S̃i


E (z − z̄)4i = K̃i

w0 + w1 + w2 = 1

(23)

−w1 s1 + w2 s2 = 0

(24)

w1 s21
−w1 s31

=1

(25)

= S̃

(26)

+
+

From (24), we see that w1 =
gives

w2 s22
w2 s32

s2
s1 w2 .

Rewriting (25) using (26)

w2 s2 (s1 + s2 ) = 1

(27)

w2 s2 (s22

(28)

−

s21 )

= S̃

We designate s1 as the free parameter while assuming that
s1 > 0. Using the fact that s22 − s21 = (s1 + s2 )(s2 − s1 ),
substituting (27) into (28) gives
s2 = s1 + S̃

(29)

From (23) and (27), we see that the weights are given as
w2 =

1
,
s2 (s1 + s2 )

w0 = 1 − w1 − w2

(30)

We note that the free parameter s1 can be selected to match
the fourth moment of z. We now attempt to satisfy the fourth
moment constraint given by
w1 s41 + w2 s42 = K̃
Eliminating w1 using w1 =

s2
s1 w2

(31)

gives

(19)

w2 s2 (s31 + s32 ) = K̃

(20)

Using the relationships w2 s2 (s1 + s2 ) = 1, s31 + s32 = (s1 +
s2 )(s21 + s22 − s1 s2 ), and s2 = s1 + S̃, the above equation
reduces to
s21 + S̃s1 + S̃ 2 − K̃ = 0

(21)
(22)

A. One-Dimensional Distribution
We develop sigma points that match the first three moments
of z in a single dimension, and then constrain those points to

The solution to the above quadratic equation is
i
p
1h
s1 =
−S̃ + 4K̃ − 3S̃ 2
2

(32)

(33)

5

distance s4 from the origin with a weight of w4 . In summary,
in each dimension, our sigma points are characterized by two
points that have their own adjustable weight and adjustable
distance away from their mean while the sigma points of [11]
are characterized by two points that share the same weight
and distance away from their mean. Therefore, our unscented
transform uses the following 2n + 1 sigma points
χ(0) = {0}, w0
χ(i) = {−si Ii }, wi
χ

(a)

(i+n)

i = 1, · · · , n

= {si+n Ii }, wi+n

i = 1, · · · , n

where 0 ∈ Rn is a vector of zeros and Ii is the ith column
of the n × n identity matrix. Obeying the moments of z, we
write
w0 +

2n
X

wi = 1

(34)

−wi si + wi+n si+n = 0

(35)

wi s2i
−wi s3i

=1

(36)

= S̃i

(37)

i=1

+
+

wi+n s2i+n
wi+n s3i+n

From (35), we see that wi =
(37) by eliminating wi gives

si+n
si wi+n .

Rewriting (36) and

wi+n si+n (si + si+n ) = 1
wi+n si+n (si + si+n )(si+n − si ) = S̃i

(38)
(39)

Selecting si > 0 for i ∈ [1, n] as the free parameters, we get
si+n = si + S̃i

(b)
Fig. 2. (a) Samples chosen for a two-dimensional distribution for the GenUT.
The locations and weights of the sigma points are determined by the moments
of the probability distribution. (b) Symmetric samples chosen for a twodimensional Gaussian distribution [11].

where s2 is given in (29). The equations for w1 , w2 , and w0
remain unchanged.
We note that the sigma points described above, which accurately capture the kurtosis when constrained, were designed
for when the state has a dimension of 1. In the next section,
we extend this to multiple dimensions.
B. Multi-Dimensional Distribution
We develop a set of sigma points that accurately capture
the first moment, second central moment, and third central
moment of an n-dimensional vector z. We then use the
developed sigma points, by constraining them, to try to capture
part of the kurtosis. The points whose distribution obey the
moments in (19)–(21) for a two-dimensional distribution are
shown in Fig. 2 where they are contrasted against the original
symmetric sigma points of [11]. Our first points lie at (0, 0)
with a weight of w0 . Our second points lie on the coordinate
axes a distance −s1 from the origin with a weight of w1 .
Our third points lie on the coordinate axes a distance −s2
from the origin with a weight of w2 . Our fourth points lie
on the coordinate axes a distance s3 from the origin with a
weight of w3 . Our fifth points lie on the coordinate axes a

(40)

Therefore, from (34) and (38), we see that
2n
X
1
, w0 = 1 −
wi
(41)
si+n (si + si+n )
i=1


To match the term E (z − z̄)4i , which is a part of the Kurtosis
of the n-dimensional vector z, we need to satisfy

wi+n =

wi s4i + wi+n s4i+n = K̃i

(42)

Solving the above equation results in constrained values for
si , such that


q
1
si =
−S̃i + 4K̃i − 3S̃i2
for i = 1, · · · , n (43)
2
It can be shown from (12) that after pre-multiplying by C,
the algorithm for selecting the 2n + 1 sigma points for any
random vector x is given in Algorithm 1.
We recall from (40) that si > 0. Applying this constraint
on (43), we see that


q
1
2
−S̃i + 4K̃i − 3S̃i > 0
2
q
4K̃i − 3S̃i2 > S̃i
K̃i >
where we have used the fact that S̃i =
K̃i =

Ki
σi4

Si2
σi2
Si
σi3

(44)
(see (17)) and

(see (18)). The inequality in (44) agrees with the

6

fact that for probability distributions, the standardized kurtosis
always exceeds the squared of the standardized skewness [28].
Therefore, we can conclude from (43) that si for i ∈ [1, n] is
always feasible.
There might be concerns that si+n in (40) might be negative
whenever the standardized skewness S̃i is negative. Substituting (43) into (40) gives


q
1
si+n =
−S̃i + 4K̃i − 3S̃i2 + S̃i
2


q
1
2
=
S̃i + 4K̃i − 3S̃i > 0
(45)
2
Irrespective of the sign of the standardized skewness S̃i , we
see from the above equation that si+n > 0 whenever (44) is
satisfied.
Algorithm 1 can be used to create sigma points that satisfy
some prescribed mean, variance, skewness, and kurtosis only
when (44) is satisfied. For example, let x̄ = 0.1, P = 0.2, and
2
S = −0.5. To satisfy (44), we require K > −0.5
0.2 = 1.25 so
we select K = 1.3. Using Algorithm 1, we see that w0 = 0.2,
w1 = 0.0286, w2 = 0.7714, s1 = 5.8055, and s2 = 0.2153.
The sample mean, sample covariance, sample skewness, and
sample kurtosis exactly match their true values. We show how
to calculate the sample statistics in Section IV.
C. Selecting the Parameters S̃i and K̃i
We use the moment generating function (MGF) M (t) to
evaluate the mean and higher-order central moments of a
probability distribution. For any random variable X [29], its
MGF and n-th moment are given by
M (t) = E[etX ],

E[X n ] =

∂nM
∂tn

We also use the gamma notation
Z ∞
Γ (k) =
xk−1 e−x dx

1

Make available the mean x̄, covariance P , skewness
Si for i ∈ [1, n], and kurtosis Ki for i ∈ [1, n]

2

Calculate the standardized skewness S̃i and kurtosis
K̃i ;
S̃i =

Si
,
σi3

K̃i =

Ki
σi4

where σi is the standard deviation of xi .
3
4

Choose the free parameters si > 0 for i ∈ [1, n];
Calculate the scaling parameter si+n , as well as the
weights wi , wi+n , and w0 ;
si+n = si + S̃i ,
wi =

5

si+n
wi+n ,
si

1
si+n (si + si+n )
2n
X
w0 = 1 −
wi

wi+n =

i=1

Calculate the 2n + 1 sigma points;
χ(0) = x̄

w0
√ 
P
wi
i ∈ [1, n]
χ(i) = x̄ − si
√ i 
P
wi+n i ∈ [1, n]
χ(i+n) = x̄ + si+n
i
√
where ( P )i is the ith column of the matrix
square root of P .
Note : To partially
 match
 select
q the Kurtosis,
1
2
si = 2 −S̃i + 4K̃i − 3S̃i in step 3.
Algorithm 1: Sigma Points for the Generalized Unscented Transform

(46)
√
Proof. We use the notation Ci = ( P )i , where CC T = P .
Evaluating the sample mean, we get

t=0

(47)

0

The first four moments, as well as the standardized moments
S̃i and K̃i for some probability distributions can be found in
Table I.
IV. ACCURACY OF S IGMA P OINT S AMPLE S TATISTICS
We demonstrate the accuracy of our sigma points in approximating any random vector X ∈ Rn .
Theorem IV.1. Let X ∈ Rn be any random vector with mean
X̄ and covariance matrix P > 0 and let the 2n + 1 sigma
points be defined as shown in Algorithm 1, then the following
statements are true:

P2n
1) The sample mean of χ(i) , wi , µχ = i=0 wi χ(i) , is
equal to the true mean of X.

2) The sample covariance matrix of χ(i) , wi , ΣP =
P2n
(i)
− µχ )(χ(i) − µχ )T , is equal to the true
i=0 wi (χ
covariance of X.

µχ =

2n
X

wi χ(i)

i=0

= w0 X̄ +

n
X

n

 X


wi X̄ − si Ci +
wi+n X̄ + si+n Ci

i=1

= X̄

2n
X

wi +

i=0

i=1
n
X

wi [(wi+n si+n − wi si )Ci ] = X̄ (48)

i=1

P2n
because
i=0 wi = 1 and wi+n si+n − wi si = 0. We see
that the sample mean equals the actual mean. Evaluating the
sample covariance matrix, we get
ΣP =

2n
X

wi (χ(i) − µχ )(χ(i) − µχ )T ,

i=0
n
n
X

 X


=
wi s2i Ci CiT +
wi+n s2i+n Ci CiT
i=1

i=1

n
X


=
(wi s2i + wi+n s2i+n )Ci CiT = P
i=1

(49)

Binomial N B(r, p)

Negative

Geometric GE(p)

Poisson P (λ)

Binomial B(n, p)

Beta BE(a, b)

Rayleigh R(σ)

Weibull W (a, b)

Gamma G(a, b)

Exponential E(λ)

Gaussian N (µ, σ 2 )

Random Variable

2πσ 2

1

−1
e 2




x−µ 2
σ
,

−x
e b ,

 b
− x
a
e
,

q
π
2

i

a i bi
2ζ
ζ0
1



2− π
2


r+k−1
r
r
p (1 − p) ,
k

k = 0, 1, 2, · · · , ∞


pi

i

3
+ 2Γ1b

q
π
2

i

r(pi − 1)(pi − 2)
p3
i

p2
i

(pi − 1)(pi − 2)
p3
i

λi

ni pi (1 − pi )(1 − 2pi )

2ai bi (bi −ai )
3ζ ζ
ζ0
1 2

σi3 (π − 3)

− 3Γ1b Γ2b )
i
i

3
ai (Γ3b

2ai b3
i

2
λ3
i

0

Skewness (Si )
E[(x − x̄)3
i]

ri (1 − pi )

p2
i

pi

ri (1 − pi )

(1 − pi )

(1 − pi )

k = 0, 1, 2, · · · , ∞

k
p(1 − p) , p ∈ (0, 1],

λi

ni pi (1 − pi )

σi2



2
a2 Γ2b − Γ1b
i
i

a i b2
i

1
λ2
i

σi2

Variance (Pii )
E[(x − x̄)2
i]

λi

ni pi

ai
ζ0

σi

aΓ1b

a i bi

1
λi

µi

Mean
E[xi ]

−λ
e
, λ > 0,
k!
k = 0, 1, 2, · · · , ∞

λk

p ∈ [0, 1], k = 0, 1, 2, · · · , n

 
n
k
n−k
p (1 − p)
,
k

ζk = ai + bi + k

x ∈ (0, 1), a > 0, b > 0

Γ (a)Γ (b)

a−1
b−1
x
(1 − x)
,

2
− x
e 2σ 2 , x ≥ 0

Γ (a + b)

σ2

x

a
a
x ≥ 0, a > 0, b > 0,
!
k
Γkb = Γ
+1
i
bi

b  x b−1

x ≥ 0, a > 0, b > 0

Γ (a)ba

xa−1

λe−λx , x ≥ 0, λ > 0

x ∈ (−∞, ∞)

p

Probability density
function

TABLE I
P ROBABILITY D ISTRIBUTIONS



32−3π 2
4



2

− 6p
− 3pr + 3r + 6)
p−4

r(1 − p)(p

p4
i

(1 − pi )(p2
i − 9pi + 9)

2
3λi + λi

ni pi (1 − pi )(1+
p(1 − pi )(3ni − 6))

3ai bi (2(bi −ai )2 +abζ2
4ζ ζ ζ
ζ0
1 2 3

σi4

2
+ 6Γ1b Γ2 )
i

− 4Γ1b Γ3b
i
i

4
4
a (Γ4b − 3Γ1b
i
i

3ai b4
i (ai + 2)

9
λ4
i

3σi4

Kurtosis (Ki )
E[(x − x̄)4
i]

p

1+pi
√
p i ri

(2−pi )
p
1−pi

−1
λ 2
i

1−2pi
ni pi (1−pi )

p
2(b−a) ζ1
√
ζ2 ab

√
2 π(π−3)
3
(4−π) 2

2
+ 2Γ1b )
i

3
2
Γ2b −Γ 2
1bi
i

(Γ3b − 3Γ1b Γ2b
i
i
i

√2
ai

2

0

GenUT
S̃i

−1
i

(3ri +6)(pi −1)−p2
i
ri (pi −1)

p2
i
+9
1−pi

3+λ

1−6pi (1−pi )
ni pi (1−pi )

6ζ1 (ai −bi )2 +3ai bi ζ1 ζ2
ai bi ζ2 ζ3

32−3π 2
(π−4)2

4
− 3Γ1b
i
i
− 4Γ1b Γ3b
i
i
2
+ 6Γ1b Γ2b )
i
i

2
Γ2b −Γ 2
1bi
i
(Γ4b

3+ 6
ai

9

3

GenUT
K̃i

7

8

Pn
because wi s2i + wi+n s2i+n = 1 and i=1 Ci CiT = P . We see
that the sample covariance matrix equals the actual covariance
matrix.
Theorem IV.2. Let X ∈ Rn be an independent random vector
with mean X̄ and covariance matrix P > 0. Let the skewness
and kurtosis for xi be given as Si and Ki respectively. Let
the 2n + 1 sigma points be defined as shown in Algorithm 1.
Then the following statements are true:

1) The sample skewness tensor of χ(i) , wi , ΣSjkl =
P2n
(i)
− µχ )j (χ(i) − µχ )k (χ(i) − µχ )l , is equal
i=1 wi (χ
to the skewness tensor of X.

2) The sample Kurtosis tensor of χ(i) , wi , ΣKjklm =
P2n
(i)
− µχ )j (χ(i) − µχ )k (χ(i) − µχ )l (χ(i) −
i=1 wi (χ
µχ )m , only approximates
a part of the
 Kurtosis tensor

q
1
2
of X if si = 2 −S̃i + 4K̃i − 3S̃i .
Proof. The true skewness and kurtosis are given in (15) and
(16) respectively. We evaluate the sample skewness tensor as
ΣSjkl =
=

2n
X

wi (χ(i) − µχ )j (χ(i) − µχ )k (χ(i) − µχ )l

(−wi s3i + wi+n s3i+n )Cji Cki Cli

i=1

(50)

because Cij = 0 whenever i 6= j and Cii√= σi . We note
that Cij is the ith row in the jth column of P . We see that
the sample skewness tensor equals the actual skewness tensor
of (15). Finally, we evaluate the sample Kurtosis tensor as
ΣKjklm =

n
X

(wi s4i + wi+n s4i+n )Cji Cki Cli Cmi

i=1

=

n
X
Ki
i=1

(
=

σi4

Kj
0

Cji Cki Cli Cmi
if i = j = k = l = m
otherwise

Several probability distributions, such as Poisson and negative binomial, only permit random variables that take positive
values. Although several sigma point transformations that exist
in the literature can be employed [9], [11], they sometimes give
sigma points that violate the positive assumption inherent to
some probability distributions despite being able to capture
the mean and covariance of the random variable. This might
make them inapplicable in situations/models that only permit
positive values. For example, in applications that assume a
Poisson distribution for the states, such as count data, the
states are usually positive by default and can never be negative.
When our sigma point of Algorithm 1 is applied, the positive
constraint on an independent random vector can be violated.
We demonstrate this using the following example.
Example V.1. We generate sigma points for an independent
Poisson random vector x with mean x̄ and covariance P such
that
 


1.5
1.5 0
x̄ =
, P =
1
0 1
Using Algorithm 1, we see that w0 = 0.3333, w1 = 0.2049,
w2 = 0.2129, w3 = 0.1284, w4 = 0.1204, s1 = 1.3713,
s2 = 1.3028, s3 = 2.1878, and s4 = 2.3028. The 2n + 1
sigma points in matrix form is


1.5000 −0.1794 1.5000 4.1794 1.5000
χ=
(52)
1.0000 1.0000 −0.3028 1.0000 3.3028

i=1
n
X

n
X
Si
=
Cji Cki Cli
σ3
i=1 i
(
Sj if i = j = k = l
=
0
otherwise

V. P OSITIVELY C ONSTRAINED S IGMA P OINTS

(51)

because Cij = 0 whenever i 6= j and Cii = σi . We see that,
for an independent random vector, the sample Kurtosis tensor
only matches a part of the true Kurtosis tensor of (16).
Theorem IV.1 shows that our sigma points in Algorithm 1
can accurately approximate the mean and covariance of any
random vector – this makes it applicable to a wide variety
of applications. In the special case of an arbitrarily distributed
independent random vector with known skewness and kurtosis
for each element, Theorem IV.2 shows that our sigma points
accurately approximates the skewness tensor. It also shows
that with proper selection of the proportionality constants, the
Kurtosis tensor can be partially approximated.

The sample mean and sample covariance are
 


1.5
1.5 0
µχ =
, ΣP =
1
0 1
We see from Example V.1 that despite the accuracy of the
sample mean and sample covariance, two sigma points χ(1)
and χ(2) in (52) had negative values. The negative sigma points
do not satisfy the non-negativity of Poisson draws. Hence,
we would like to come up with analytical expressions for
sigma points that do not violate the positive constraints, while
still maintaining accuracy in approximating the true mean and
covariance of a random vector. To do this, we employ the
following assumption about the sigma points in Algorithm 1.
Assumption V.1.
 mean x̄ is positive, the sigma points
√ The
P can either be positive or negative for
χ(i) = x̄ − si
i
√ 
i ∈ [1, n], and the sigma points χ(i+n) = x̄ + si+n
P
i
are always positive for i ∈ [1, n].
We analytically enforce positive constraints on the sigma
points
in
1 by ensuring that the inequality

√Algorithm

x̄ − si
P
≥ 0 will not be violated – we enforce this
i
constraint by redefining the free parameters si for i ∈ [1, n].
After calculating the sigma points using Algorithm 1, we select
the free parameter si such that

√  


si = min x̄
P
if min χ(i) < 0
√

i

√
where ( P )i is the ith column of P and the Hadamard division implies an element-wise division. Although the above

9

representation always guarantees that one of the elements of
the sigma point χ(i) attains a value of zero, some models
might not permit zero values for the sigma points. To address
this, we introduce a slack parameter k ∈ (0, 1] which is a user
selected constant. Using k, we now redefine the free parameter
si as


h

√  i
P
if min χ(i) < 0
(53)
si = k min x̄

Unconstrained

Truncated

Constrained

x2
3

2.5

2

1.5

i

1

where a value of k = 1 ensures that at least one of the elements
of χ(i) is zero. The sigma point gets farther away from 0 as
k → 0. Using a new value for si , we note that the equations
for si+n , wi+n , and wi are unchanged.
For an arbitrarily distributed independent random vector,
the positively constrained sigma points designed here are only
accurate up to the third order – the extra degree of freedom in
capturing a part of the kurtosis tensor is lost. The positively
constrained sigma point algorithm is given in Algorithm 2. We
now show a benefit of Algorithm 2 in the following example.

0.5

x1
0.5

1

1.5

2

2.5

3

3.5

4

(a)
True Mean
Unconstrained Mean

Truncated Mean
Constrained Mean

True Covariance
Unconstrained Covariance

Truncated Covariance
Constrained Covariance

1.6

1.4

1

Implement Algorithm 1

2
3

for i = 1 to n do

if min χ(i) < 0 then

4

Redefine si ;
h

si = k min x̄

x2

1.2

0.8

0.6

√  i
P
i

where the Hadamard division implies an
element-wise division. The user-selected
slack parameter k should be selected such
that 0 < k ≤ 1. A value of k = 1 yields a
zero sigma point. The sigma point gets
farther away from 0 as k → 0.
6

end
end

7

Repeat steps 4 and 5 of Algorithm 1;

5

1

Algorithm 2: Positively Constrained Sigma Points for
the General Unscented Transform
Example V.2. Using Algorithm 2 to generate positively constrained sigma points for the Poisson random vector, we
select k = 0.9. We see that w0 = −0.0576, w1 = 0.3003,
w2 = 0.3968, w3 = 0.1725, w4 = 0.1880, s1 = 1.1023,
s2 = 0.9000, s3 = 1.9188, and s4 = 1.9000. The 2n + 1
positive sigma points in matrix form is


1.5000 0.1500 1.5000 3.8500 1.5000
χ=
(54)
1.0000 1.0000 0.1000 1.0000 2.9000
while the corresponding sample mean and sample covariance
are
 


1.5
1.5 0
µχ =
, ΣP =
1
0 1
We see from Example V.2 that using Algorithm 2 ensures
that the sigma points are always positive while ensuring
accuracy in approximating the true mean and covariance of
a random vector. A graphical representation of Examples V.1

0.4
0.8

1

1.2

1.4

1.6

1.8

2

2.2

x1

(b)
Fig. 3. (a) Locations of sigma points for the unconstrained (Algorithm 1),
truncated, and constrained (Algorithm 2) sigma points. (b) Mean and covariance of the unconstrained (Algorithm 1), truncated, and constrained
(Algorithm 2) sigma points.

and V.2 is shown in Figure V.2 where we plot the sigma points
and the covariance. We note that the accuracy in approximating
the true mean and true covariance of a random vector is also
valid for any arbitrary distribution whenever the positively
constrained sigma points of Algorithm 2 are used.
VI. P ROPAGATION OF M EANS AND C OVARIANCES OF
N ONLINEAR T RANSFORMATIONS
We analyze the performance of our new sigma point algorithm when they undergo nonlinear transformations. We
will show how linearization approximations, via Taylor series
expansion of a nonlinear transformation of a random vector x
evaluated about its mean x̄, introduce errors in the propagation
of means and covariances. We evaluate the true mean and true
covariance of an independent random vector in Appendices
A-A and A-B respectively. We then evaluate the approximated
mean and approximated covariance in Appendices B-A and
B-B respectively. We see that errors are introduced beyond
the third order when approximating a nonlinear transformation
of an independent random vector. We also see that errors are
introduced beyond the second order when the random vector
is not independent.

10

where E[x] = x̄. We use the GenUT of Algorithm 1, which
guarantees fourth-order accuracy for random variables. Given
x̄, P , Si , and Ki , the sample mean and covariance of the
nonlinear transformation of (55) when the GenUT is used can
be calculated as follows.
1) Calculate the 2n + 1 sigma points using Algorithm 1.
2) Evaluate the sample mean and sample covariance using
(3)–(5).
For our comparison, we use the scaled unscented transform
of [30], which is denoted as UT for the remainder of this
paper. The sigma points of the UT is given as
1
χ0 = x̄
w0 =
2α
√ 
n
(56)
χi = x̄ +
αP
wi = 1 −
α
i
√ 
n
χi+n = x̄ −
αP
wi = 1 −
α
i
where α defines the scaling of the sigma points. A value
of α = 3 is typically recommended to partially match the
kurtosis of a random vector [30]. The sample mean and sample
covariance of the UT can also be evaluated using (3)–(5).

% error of mean
% error of covariance

(55)

30
20
10
0
300 Monte

25000 Monte

100000 Monte 400000 Monte

GenUT

UT

300 Monte

25000 Monte

100000 Monte 400000 Monte

GenUT

UT

40
30
20
10
0

Method

(a)
6
% error of mean

y = λ(x)

40

5
4
3
2
1
0

% error of covariance

We will see that errors can be introduced in the propagation
of means and covariances beyond the second order when sigma
points developed under the Gaussian assumption [8], [11], [30]
are used to approximate the nonlinear function λ(x) when x
is an independent random vector. We note that the nonlinear
transformation y ∈ Rn is given by

300 Monte

25000 Monte

100000 Monte 400000 Monte

GenUT

UT

300 Monte

25000 Monte

100000 Monte 400000 Monte

GenUT

UT

30

20

10

0

Method

(b)
Fig. 4. (a) Moments of y = sin(x) when x is a Poisson random variable.
(b) Moments of y = sin(x) when x is a Weibull random variable.

A. Case Study 1 – Transformation of Random Variables
Defining x as a random variable that can follow any of
the probability distributions given in Table I, we evaluate
the mean and covariance of two nonlinear transformations: a
quadratic function of the random variable y = 3x + 2x2 , and
a trigonometric function of the random variable y = sin(x).
We also use 100000 Monte Carlo draws from the different
probability distributions to evaluate the sample mean and
sample covariance of the nonlinear transformations. The true
mean and covariance of the quadratic function can be easily
evaluated using the raw moments of x up to its fourth
order. The true mean and covariance of the trigonometric
function can be evaluated using their characteristic functions.
A comparison between the accuracy of the GenUT, UT, and
100000 Monte Carlo draws in approximating the true mean
and true covariance of the nonlinear transformations for the
different probability distributions is shown in Tables II-V.
For the quadratic function, we see that the GenUT gave
an exact approximation of the true mean and true covariance
for all the probability distributions while the UT was only
accurate in approximating the true mean when the probability
distribution was not Gaussian. This is because the Taylor series
expansion of the mean is characterized by the existence of
second-order moments of x, the Taylor series expansion of
the covariance is characterized by the existence of fourth-order
moments of x, and the UT is only accurate up to the second
order when the distribution is not Gaussian. The GenUT

is accurate up fourth order because it is adaptable to the
unique skewness and kurtosis of most probability distributions.
Although the 100000 Monte Carlo draws gave relatively good
approximations, they were not as accurate as the GenUT.
For the trigonometric function, we see that the GenUT and
UT were unable to give exact approximations of the true mean
and true covariance in most cases because the Taylor series
expansion of the nonlinear transformation is characterized by
the existence of the central moments of x beyond the fourth
order. The GenUT was more accurate than the UT for all
the non-Gaussian probability distributions because the GenUT
is accurate up to the fourth order while the UT is accurate
up to the second order. The GenUT and UT gave similar
accuracy when x was Gaussian distributed because fourthorder accuracy was guaranteed by both methods. The 100000
Monte Carlo draws sometimes gave better accuracy than the
GenUT because of the random nature of its draws. A box
plot of the accuracy of the GenUT, UT, and several Monte
Carlo draws of different sizes is shown in Figure 4 for the
trigonometric function. We see that a significant number of
Monte Carlo draws is needed to achieve the accuracy of the
GenUT when approximating the mean. A significant number
of Monte Carlo draws gives better accuracy in approximating
the variance.

11

TABLE V
P ERCENTAGE ERROR IN P ROPAGATING THE COVARIANCE OF y = sin(x)
TABLE II
P ERCENTAGE ERROR IN P ROPAGATING THE MEAN OF y = 3x + 2x2

x

GenUT

UT

MC

N (1, 4)

0

0

0.015

E(2)

0

0

0.069

G(1, 2)

0

0

0.452

W (1, 2)

0

0

0.005

R(1)

0

0

0.097

BE(3, 4)

0

0

0.063

B(3, 0.3)

0

0

0.457

P (2)

0

0

0.270

GE(0.5)

0

0

1.251

N B(4, 0.67)

0

0

0.668

TABLE III
P ERCENTAGE ERROR IN P ROPAGATING THE COVARIANCE OF y = 3x + 2x2

x

GenUT

N (1, 4)
E(2)
G(1, 2)

UT

MC

0

0

0.029

0

49.057

0.249

0

64

1.889

W (1, 2)

0

15.003

0.310

R(1)

0

16.815

0.381

BE(3, 4)

0

2.307

0.613

B(3, 0.3)

0

16.380

0.359

P (2)

0

25.946

1.061

GE(0.5)

0

67.662

1.036

N B(4, 0.67)

0

43.224

2.356

x

GenUT

UT

MC

N (0.25, 0.1)

5.026

5.026

0.444
0.213

E(2)

23.499

72.557

G(0.5, 0.5)

20.749

61.391

0.372

W (1, 2)

4.862

31.760

0.043

R(1)

12.158

50.678

0.531

BE(3, 4)

0.031

0.940

0.225

B(3, 0.3)

11.033

24.806

0.060

P (0.1)

6.646

45.895

0.461

GE(0.7)

12.074

87.637

0.070

N B(0.4, 0.67)

39.068

135.783

0.366

B. Case Study 2 - Infectious Disease Models
We consider an SIR (susceptible-infectious-recovered) infectious disease model given by the difference equation [31]
βSk Ik
Sk+1 = Sk −
N
βSk Ik
− γIk
(57)
Ik+1 = It +
N
Rk+1 = Rk + γIk
where β is the infection rate, γ is the recovery rate, and N =
Sk +Ik +Rk . We examine the performance of the GenUT and
the UT in approximating the true mean and covariance of (57)
when its right hand side is linear in the random variables or
nonlinear in the random variables. We note that by defining
the state xk = [Sk , Ik , Rk ]T , we can write (57) in a more
compact form.
1) Linear in random variables: We modify (57) to be a
linear combination of independent Poisson random variables
such that
xk+1 = xk + AX

(58)

where

TABLE IV
P ERCENTAGE ERROR IN P ROPAGATING THE MEAN OF y = sin(x)

x

GenUT

UT

MC

N (0.25, 0.1)

0.001

0.001

0.012

E(2)

0.219

5.788

0.110

G(0.5, 0.5)

0.312

6.964

0.050

W (1, 2)

0.017

0.831

0.029
0.007

R(1)

0.049

0.912

BE(3, 4)

0

0.038

0.037

B(3, 0.3)

0.158

4.814

0.046

P (0.1)

0.275

18.305

0.531

GE(0.7)

2.416

32.906

0.138

N B(0.4, 0.67)

0.176

44.172

0.383


−1
A= 1
0


0
−1 ,
1

 βS
X = Poisson

k Ik
N
γIk



We note that although one of the Poisson rates is nonlinear
in the variables Sk and Ik , the difference equation of (58)
is comprised of linear combinations of the Poisson random
vector X. Variants of this particular formulaion can be found
in [25], [31] . Evaluating the mean and covariance of (58)
conditioned on the state at index k, the GenUT and UT will
both give the correct mean and correct covariance despite
the fact that the independent random vector X is Poisson
distributed. To better understand why they both give similar
values, we briefly analyze the Taylor series expansion of
(58) by using the results in (6) and (10). Assuming that
λ = xt + AX, we see that the following partial derivative
holds true.

T
∂2λ
= 0 0 0
for i, j ∈ [1, 2]
∂Xi ∂Xj

12

We see from the above equation that all second, third, and
fourth partial derivatives are zero. Therefore, only the terms
λ(X̄) and ΛP ΛT exist in the Taylor series expansion in (6)
and (10) respectively, which are accurately approximated by
the GenUT and UT.
2) Nonlinear in random variables – model 1: We deviate
from the more common representation in (58) by assuming
that the SIR dynamics is made up of Poisson draws of the
states such that

3) Nonlinear in random variables – model 2: Using the
conservation principle S + I + R = N , we reduce the model
of (57) to

xk+1 = xk + λ(X)

xk+1 = xk + λ(X)
(61)

I
where xk = k , and
Rk




Ik
β (N − X1 − Rk ) XN1 − X2
, X = Poisson
λ(X) =
γIk
X2

(59)

Ik+1 = It + β (N − Ik − Rk )

Ik
− γIk
N

(60)

Rk+1 = Rk + γIk
We now assume that the reduced dynamics of (60) is made
up of Poisson draws of the state Ik such that



where

− βXN1 X2
λ(X) =  βXN1 X2 − γX2  ,
γX2


 
S
X = Poisson k
Ik

The difference equation of (59) is comprised of nonlinear
combinations of the Poisson random vector X whose rates
depend on Sk and Ik . Evaluating the mean and covariance of
(59) conditioned on the state at index k, the GenUT and UT
will both give the same sample mean and sample covariance
for (59). We however note that although the sample mean
would be accurate, the sample covariance would be inaccurate
because of errors in approximating the fourth-order terms
in the Taylor series expansion of the covariance of (59).
To better understand why they both give the correct mean
and an inaccurate covariance matrix despite the fact that the
independent random vector was Poisson distributed, we briefly
analyze the Taylor series expansion of (59) using the results
in (6) and (10). We see that the following partial derivative of
λ(X) holds true.
h
iT

 −β β 0
2
if i 6= j
∂ λ
N
N
= h
for i, j ∈ [1, 2]
iT

∂Xi ∂Xj
 0 0 0
otherwise
From the above equation, we see that all partial derivatives
beyond the second order are zero. Therefore, from (6), we
can see that the GenUT and UT give the accurate mean
because they both accurately capture the second-order terms.
For the covariance matrix, we analyze the third-order and
2
λ
fourth-order terms. Although the term ∂X∂i ∂X
has some
j
nonzero elements if i 6= j, the term Sijk = 0 whenever
i = j = k is violated. Therefore, in the third-order terms,
the partial derivatives always equal zero when the skewness
exists while the skewness always equals zero when the partial
derivatives exist. This is why the GenUT and UT arrive at
the same value of zero for all third-order terms even though
the UT inaccurately approximates the skewness by always
guaranteeing zero skewness. For the fourth-order terms, only
2
∂ 2 λT
λ
6= 0 whenever i 6= j. From (10),
the term ∂X∂i ∂X
j ∂Xi ∂Xj
we can see that this term is multiplied by the kurtosis term
Kijkl which is defined in (16) as Kijkl = Pii Pjj whenever
i = k 6= j = l. However, we see from (51) that the GenUT
gives a value of Kijkl = 0 whenever i = k 6= j = l – the same
can be said about the UT. Therefore, the GenUT and UT both
arrive at zero values for the fourth-order terms thereby leading
to inaccurate approximations of the true covariance matrix.

Evaluating the mean and covariance of (59) conditioned on
the state at index k, the GenUT will accurately approximate
the mean and covariance matrix, while the UT will accurately
approximate the mean but inaccurately approximate the covariance matrix. To understand the performance of the GenUT
and UT, we briefly analyze the Taylor series expansion of (61)
using the results in (6) and (10). We see that the following
partial derivative of λ(X) holds true.
h
iT

 − 2β 0
2
if i = j = 1
∂ λ
N
for i, j ∈ [1, 2]
= h
iT

∂Xi ∂Xj
 0 0
otherwise
From the above equation, we see that all partial derivatives
beyond the second order are zero. Therefore, from (6), we can
see that the GenUT and UT give the accurate mean because
they both accurately capture the second-order terms. For the
covariance, using (10), we see that the third-order terms are
nonzero if i = j = k = 1 and only the GenUT is able to
accurately approximate Sijk , with the UT always giving a
value of zero for Sijk . Furthermore, from (10), we see that the
2
λ
∂ 2 λT
only nonzero fourth-order term is ∂X∂i ∂X
whenever
j ∂Xk ∂Xl
i = j = k = l = 1. However, only the GenUT is able to
accurately approximate Kijkl if i = j = k = l. This is why
the GenUT gives an accurate approximation of the covariance
matrix.
During the implementation of a Kalman filter on the reduced
dynamics of (61), using the GenUT and UT to estimate the
system noise covariance matrix will give filter performances
that might be indistinguishable from each other. This can
happen for two reasons: (i) Whenever the infected population
I is very small such that the Poisson random variable X1
can not be well approximated by a Gaussian of the same
mean and variance, the contribution of the third-order and
fourth-order terms in (10) will be very small because of the
β
term and the relatively small values of
presence of the N
β that are typically used in infectious disease modeling. The
resulting small difference in covariance matrix accuracy will
be made negligible during the measurement update step of the
Kalman filter. (ii) Whenever the infected population I is large
enough such that the Poisson random variable X1 can be well
approximated by a Gaussian of the same mean and variance,

13

β
due to the presence of the N
term and small β values, the
GenUT and UT will both give similar values for the thirdorder and fourth-order terms in (10).

VII. C ONCLUSION
In this paper we have developed the generalized unscented
transform (GenUT) that is capable of adapting to the unique
statistics of an arbitrarily distributed random variable. We
showed that due to its ability to match up to at least the
skewness of any random variable, the GenUT is preferable to
the unscented transforms that were either developed using the
Gaussian assumption or were developed without any probability distribution in mind. In terms of ease of implementation,
we demonstrated that like the unscented transform originally
developed in [11] which uses 2n + 1 sigma points, the GenUT
uses the same number of sigma points (2n+1) to achieve thirdorder accuracy for independent random vectors. In terms of
performance, the GenUT and unscented transforms developed
under the Gaussian assumption give the same performance
when the random variable is Gaussian distributed. We also
showed that the GenUT formulation makes it easy to enforce
positive constraints on the sigma points while still guaranteeing third-order accuracy, which makes it appealing in models
that permit only positive values for random variables.
When compared against many unscented transforms existing
in the literature, the GenUT gives better accuracy if the
nonlinearity of the transformations are significant and the
random variable is not Gaussian distributed, which makes it
suitable for numerous real world applications. For example,
several infectious disease models use linear combinations of
random variables such as Poisson or Negative Binomial [31] –
a linear combination of independent Poisson random variables
was used in the modeling and assimilation of daily COVID-19
infection data [25]. In such an application, for estimating the
noise covariance matrix, the use of the GenUT, UT, and other
unscented transforms existing in the literature that guarantee
at least a second-order accuracy will yield no noticeable
difference in performance. We showed in Section VI-B1 that
this is because higher order terms beyond the first order
in the Taylor series expansion of a linear transformation of
an independent random vector are all zero – this explains
why using the EKF, which uses first-order linearization, for
such applications might produce similar results. We showed
in Section VI-B2 that the GenUT and UT performance are
indistinguishable from each other if the each random variable
in the product is a monomial of degree 1. We showed in
Section VI-B3 that a noticeable increase in performance of the
GenUT will be observed when any of the random variable in
the product is a monomial of degree 2 or more – this implies
that terms beyond the third order exist in the Taylor series
expansion of the transformation.
For uncertainty quantification, estimation, or prediction applications, the GenUT can typically give better accuracy when
compared to other unscented transforms. This accuracy will
have more significant consequences if the nonlinearities are
strong.
In summary, the GenUT is a new form of adaptable
sigma points that can match a prescribed mean, variance, and

skewness of most independent random vectors, while being
able to partially match its kurtosis. In the case of a single
random variable, the GenUT can match up to its kurtosis. For
correlated random vectors, the GenUT can accurately match up
to its covariance. The GenUT can be applied to any filter that
uses linear or nonlinear transformations of random variables.
A PPENDIX A
T RUE M EAN AND C OVARIANCE OF N ONLINEAR
T RANSFORMATIONS
We derive analytical expressions for the true mean and
covariance when we take the Taylor series expansion of the
nonlinear function y = λ(x) where x is a random vector that
follows an arbitrary distribution.
A. True Mean of the Nonlinear Transformation
Applying Taylor series expansion around x̄, where x̃ = x −
x̄, we write the true mean of y as
ȳ = E [λ(x)]
D2 λ D3 λ D4 λ
= λ(x̄) + E Dx̃ λ + x̃ + x̃ + x̃ + · · ·
2!
3!
4!



(62)

where Dx̃ λ is the total differential of λ(x) when perturbed
around a nominal value x̄ by x̃. We note that
!k
n
X
∂
Dx̃k λ =
x̃i
λ(x)
(63)
∂xi
i=1
x=x̄

Using (63), we can evaluate the true mean of (62) as

n
n
X
X
Pij ∂ 2 λ
Sijk
∂3λ
ȳ =λ(x̄) +
+

2! ∂xi ∂xj
3! ∂xi ∂xj ∂xk
i,j=1
i,j,k=1

n

X
Kijkl
∂4λ
+
+ ···
(64)
4! ∂xi ∂xj ∂xk ∂xl 
i,j,k,l=1

x=x̄

where Pij = E[x̃i x̃j ], while Sijk , and Kijkl are defined in (7)
and (8) respectively.
B. True Covariance of the Nonlinear Transformation
The true covariance of y is given as


Py = E (y − ȳ)(y − ȳ)T

(65)

Evaluating the expression y − ȳ, we write
 2

Dx̃2 λ Dx̃3 λ
Dx̃ λ Dx̃3 λ
y − ȳT = Dx̃ λ +
+
−E
+
+ ···
2!
3!
2!
3!
(66)
Substituting (66) into (65) gives

D2 λ(Dx̃ λ)T
Dx̃ λ(Dx̃2 λ)T
Py = E Dx̃ λ(Dx̃ λ)T + x̃
+
2!
2!

3
T
3 T
2
D λ(Dx̃ λ)
Dx̃ λ(Dx̃ λ)
D λ(Dx̃2 λ)T
+ x̃
+
+ x̃
3!
3!
2! × 2!
 2   2 T
Dx̃ λ
Dx̃ λ
+E
E
+ ···
(67)
2!
2!

14

We note that we can write the first term in the above equation
as



 ∂λT
∂λ
E Dx̃ λ(Dx̃ λ)T =
E x̃x̃T
= ΛP ΛT
∂x x=x̄
∂x x=x̄
(68)
Using (63) and (68), we can rewrite the true covariance matrix
of (67) as

 2

n
 X
∂ λ ∂λT
∂λ ∂ 2 λT
Sijk
T
Py =ΛP Λ +
+

2! ∂xi ∂xj ∂xk
∂xi ∂xj ∂xk
i,j,k=1

n
X
∂3λ
∂λT
1
+
Kijkl
3! ∂xi ∂xj ∂xk ∂xl
i,j,k,l=1

1 ∂λ
∂ 3 λT
1 ∂ 2 λ ∂ 2 λT
+
+
3! ∂xi ∂xj ∂xk ∂xl
4 ∂xi ∂xj ∂xk ∂xl

T 


n

2
X
Pij ∂ λ 


···
+ ···
(69)
+

2 ∂xi ∂xj

i,j=1

P2n
(i) (i)
where i=1 wi x̃j x̃k = Pjk , while ΣSijk and ΣKijkl are
defined in (50) and (51) respectively.
We see that, for an independent random vector, the approximated mean of (72) correctly matches the true mean of (64)
up to the third order, and only partial accuracy is achieved for
the fourth order. This is because ΣΓijk = Sijk (see (15) and
(50)) and ΣKijkl only partially approximates Kijkl (see (16)
and (51)).
B. Approximation of the Covariance
The approximated covariance can be evaluated using the
expression
Pu =

Dx̃2(i) λ Dx̃3(i) λ
+
2!
3!


2n
X
Dx̃2(j) λ Dx̃3(j) λ
−
wi
+
+ ···
2!
3!
j=1

Y (i) − ȳu =Dx̃(i) λ +

where we have used the notation XX = X[· · · ]T .

A. Approximation of the Mean
The approximated mean is given as
ȳu =

2n
X
i=0
2n
X

Dx̃(i) λ(Dx̃2(i) λ)T
D3(i) λ(Dx̃(i) λ)T
+ x̃
2!
3!
#
Dx̃(i) λ(Dx̃3(i) λ)T
Dx̃2(i) λ(Dx̃2(i) λ)T
+
+
3!
2! × 2!

 T
2n
X
D2(j) λ
 · · ·  + · · ·
+ 
wj x̃
2!
j=1
+

(75)

For the first term in (75),
2n
X

wi Dx̃(i) λ(Dx̃(i) λ)T =
=

n X
2n
X

(i) (i)

wi x̃j x̃k

j,k=1 i=1
n
X
j,k=1

= ΛP Λ


D2(i) λ Dx̃3(i) λ
=
wi λ(x̄) + Dx̃(i) λ + x̃
+
+ ···
2!
3!
i=0


2n
X
Dx̃2(i) λ Dx̃3(i) λ
= λ(x̄) +
wi Dx̃(i) λ +
+
+ ···
2!
3!
i=1
(71)

(74)

Substituting (74) into (73) and multiplying out gives
"
2n
X
D2(i) λ(Dx̃(i) λ)T
Pu =
wi Dx̃(i) λ(Dx̃(i) λ)T + x̃
2!
i=1

i=1

wi λ(χ(i) )

(73)

From

x=x̄

x=x̄

h
ih
iT
wi Y (i) − ȳu Y (i) − ȳu

i=1

T

A PPENDIX B
A PPROXIMATION OF THE M EAN AND C OVARIANCE USING
G ENERALIZED U NSCENTED T RANSFORMATION
We analytically show the accuracy in capturing the true
mean and true covariance of y = λ(x) when using our
2n + 1 sigma points of Algorithm 1. We show that our sigma
point transformations are accurate up to the third order for
independent random vectors. We also show that our sigma
points can partially capture some of the fourth-order moments.
In this
√ section, we define the matrix square root of P as
C = P . We note that

k
2n
2n
n
X
X
X
∂ 
wi Dx̃k λ =
wi 
x̃j
λ(x)
(70)
∂x
j
i=1
i=1
j=1

2n
X

∂λ
∂xj

Pjk
x=x̄

T

∂λ ∂λT
∂xj ∂xk
∂λT
∂xk

x=x̄

x=x̄

(76)



Using (70), we can evaluate the approximated mean of (71)
as

n
n
X
X
ΣSijk
Pij ∂λ
∂3λ
ȳu =λ(x̄) +
+

2 ∂xi ∂xj
3! ∂xi ∂xj ∂xk
i,j=1
i,j,k=1

n

X
ΣKijkl
∂4λ
+
+ ···
(72)
4! ∂xi ∂xj ∂xk ∂xl 
i,j,k,l=1

x=x̄

Using (70) and (76), we can rewrite the approximated covariance matrix of (75) as

 2

n
 X
ΣSijk
∂ λ ∂λT
∂λ ∂ 2 λT
Pu =ΛP ΛT +
+

2!
∂xi ∂xj ∂xk
∂xi ∂xj xk
i,j,k=1

n
X
1
∂3λ
∂λT
+
ΣKijkl
3! ∂xi ∂xj ∂xk ∂xl
i,j,k,l=1

1 ∂λ
∂ 3 λT
1 ∂ 2 λ ∂ 2 λT
+
+
3! ∂xi ∂xj ∂xk ∂xl
4 ∂xi ∂xj ∂xk ∂xl


T 

n

X Pij ∂ 2 λ
 · · · 
+ 
+ ···
(77)

2 ∂xi ∂xj

i,j=1
x=x̄

15

where we have also used the notation XX T = X[· · · ]T . We
see that, for an independent random vector, the approximated
covariance matrix of (77) correctly matches the true covariance
matrix of (69) up to the third order, and only partial accuracy
is achieved for the fourth order.
R EFERENCES
[1] D. Simon, Optimal state estimation: Kalman, H infinity, and nonlinear
approaches. John Wiley & Sons, 2006.
[2] R. Kandepu, L. Imsland, and B. A. Foss, “Constrained state estimation
using the unscented kalman filter,” in 16th Mediterranean Conference
on Control and Automation, 2008, pp. 1453–1458.
[3] R. Izanloo, S. A. Fakoorian, H. S. Yazdi, and D. Simon, “Kalman
filtering based on the maximum correntropy criterion in the presence of
non-gaussian noise,” in 50th Annual Conference on Information Science
and Systems (CISS), 2016, pp. 530–535.
[4] G. Evensen, “Sequential data assimilation with a nonlinear quasigeostrophic model using monte carlo methods to forecast error statistics,” Journal of Geophysical Research: Oceans, vol. 99, no. C5, pp.
10 143–10 162, 1994.
[5] P. L. Houtekamer and H. L. Mitchell, “Data assimilation using an
ensemble kalman filter technique,” Monthly Weather Review, vol. 126,
no. 3, pp. 796–811, 1998.
[6] J. L. Anderson, “An ensemble adjustment kalman filter for data assimilation,” Monthly weather review, vol. 129, no. 12, pp. 2884–2903, 2001.
[7] T. Berry and T. Sauer, “Adaptive ensemble kalman filtering of non-linear
systems,” Tellus A: Dynamic Meteorology and Oceanography, vol. 65,
no. 1, p. 20331, 2013.
[8] S. J. Julier and J. K. Uhlmann, “A general method for approximating
nonlinear transformations of probability distributions,” Robotics Research Group, University of Oxford, Tech. Rep., 1996.
[9] ——, “Reduced sigma point filters for the propagation of means and
covariances through nonlinear transformations,” in Proceedings of the
American Control Conference, 2002, pp. 887–892.
[10] G. Kitagawa, “Monte carlo filter and smoother for non-gaussian nonlinear state space models,” Journal of computational and graphical
statistics, vol. 5, no. 1, pp. 1–25, 1996.
[11] S. J. Julier and J. K. Uhlmann, “Consistent debiased method for converting between polar and cartesian coordinate systems,” in Acquisition,
Tracking, and Pointing XI, vol. 3086, 1997, pp. 110–121.
[12] Y. Rui and Y. Chen, “Better proposal distributions: Object tracking using
unscented particle filter,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, vol. 2, 2001, pp. 786—-793.
[13] R. Van Der Merwe, A. Doucet, N. De Freitas, and E. A. Wan, “The
unscented particle filter,” in Advances in neural information processing
systems, 2001, pp. 584–590.
[14] X. Luo and I. M. Moroz, “Ensemble kalman filter with the unscented
transform,” Physica D: Nonlinear Phenomena, vol. 238, no. 5, pp. 549–
562, 2009.
[15] D. Simon, “Kalman filtering with state constraints: a survey of linear
and nonlinear algorithms,” IET Control Theory & Applications, vol. 4,
no. 8, pp. 1303–1318, 2010.
[16] Y. Cheng and Z. Liu, “Optimized selection of sigma points in the
unscented kalman filter,” in International Conference on Electrical and
Control Engineering. IEEE, 2011, pp. 3073–3075.
[17] E. Simons, M. Ferrari, J. Fricks, K. Wannemuehler, A. Anand, A. Burton, and P. Strebel, “Assessment of the 2010 global measles mortality
reduction goal: results from a model of surveillance data,” The Lancet,
vol. 379, no. 9832, pp. 2173–2178, 2012.
[18] S. Chen, J. Fricks, and M. J. Ferrari, “Tracking measles infection through
non-linear state space models,” Journal of the Royal Statistical Society:
Series C (Applied Statistics), vol. 61, no. 1, pp. 117–134, 2012.
[19] C. Bretó, D. He, E. L. Ionides, A. A. King et al., “Time series analysis
via mechanistic models,” The Annals of Applied Statistics, vol. 3, no. 1,
pp. 319–348, 2009.
[20] J. Shaman and A. Karspeck, “Forecasting seasonal outbreaks of influenza,” Proceedings of the National Academy of Sciences, vol. 109,
no. 50, pp. 20 425–20 430, 2012.
[21] T. K. Yamana, S. Kandula, and J. Shaman, “Superensemble forecasts of
dengue outbreaks,” Journal of The Royal Society Interface, vol. 13, no.
123, p. 20160410, 2016.
[22] D. Ndanguza, I. S. Mbalawata, H. Haario, and J. M. Tchuenche,
“Analysis of bias in an ebola epidemic model by extended kalman filter
approach,” Mathematics and Computers in Simulation, vol. 142, pp.
113–129, 2017.

[23] B. Cazelles and N. P. Chau, “Using the kalman filter and dynamic models
to assess the changing HIV/AIDS epidemic,” Mathematical Biosciences,
vol. 140, no. 2, pp. 131–154, 1997.
[24] D. Ebeigbe, T. Berry, S. J. Schiff, and T. Sauer, “A poisson kalman
filter to control the dynamics of neonatal sepsis and postinfectious
hydrocephalus,” Physical Review Research, vol. 2, no. 4, 2020.
[25] R. Li, S. Pei, B. Chen, Y. Song, T. Zhang, W. Yang, and J. Shaman,
“Substantial undocumented infection facilitates the rapid dissemination
of novel coronavirus (SARS-CoV-2),” Science, vol. 368, no. 6490, pp.
489–493, 2020.
[26] L. J. Curtis, “Simple formula for the distortions in a gaussian representation of a poisson distribution,” American Journal of Physics, vol. 43,
no. 12, pp. 1101–1103, 1975.
[27] H. M. Menegaz, J. Y. Ishihara, G. A. Borges, and A. N. Vargas, “A systematization of the unscented kalman filter theory,” IEEE Transactions
on automatic control, vol. 60, no. 10, pp. 2583–2598, 2015.
[28] K. Pearson, “Mathematical contributions to the theory of evolution.—XIX. Second supplement to a memoir on skew variation,” Philosophical Transactions of the Royal Society of London, Series A, vol.
216, no. 538-548, pp. 429–457, 1916.
[29] A. Papoulis and S. U. Pillai, Probability, random variables, and stochastic processes, 4th ed. Tata McGraw-Hill Education, 2002.
[30] S. J. Julier and J. K. Uhlmann, “Unscented filtering and nonlinear
estimation,” Proceedings of the IEEE, vol. 92, no. 3, pp. 401–422, 2004.
[31] M. J. Keeling and P. Rohani, Modeling infectious diseases in humans
and animals. Princeton university press, 2011.

