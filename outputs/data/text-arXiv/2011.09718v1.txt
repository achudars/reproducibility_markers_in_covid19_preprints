arXiv:2011.09718v1 [stat.AP] 19 Nov 2020

Variational Bayes method for ODE parameter
estimation with application to time-varying SIR model
for Covid-19 epidemic
Hyunjoo Yang and Jaeyong Lee
Department of Statistics
Seoul National University
November 20, 2020

Abstract
Ordinary differential equation (ODE) is a mathematical model for dynamical
systems. For its intuitive appeal to modelling, the ODE is used in many application areas such as climatology, bioinformatics, disease modelling and chemical
engineering. Despite ODE’s wide usage in modelling, there are difficulties in estimating ODE parameters from the data due to frequent absence of their analytic
solutions. The ODE model typically requires enormous computing time and shows
poor performance in estimation especially when the model has a lot of variables and
parameters. This paper proposes a Bayesian ODE parameter estimating algorithm
which is fast and accurate even for models with many parameters. The proposed
method approximates an ODE model with a state-space model based on equations
of a numeric solver. It allows fast estimation by avoiding computations of a whole
numerical solution in the likelihood. The posterior is obtained by a variational Bayes
method, more specifically, the approximate Riemannian conjugate gradient method

1

(Honkela et al. 2010), which avoids samplings based on Markov chain Monte Carlo
(MCMC). In simulation studies we compared the speed and performance of proposed
method with existing methods. The proposed method showed the best performance
in the reproduction of the true ODE curve with strong stability as well as the fastest
computation, especially in a large model with more than 30 parameters. As a realworld data application a SIR model with time-varying parameters was fitted to the
COVID-19 data. Taking advantage of our proposed algorithm, 30 parameters were
adequately fitted for each country.

1

Introduction

Ordinary differential equation (ODE) is a basic mathematical modeling tool describing
dynamical systems. By describing local changes, the ODE models the global system. With
strong interpretability, ODEs are used in many application areas such as climatology,
bioinformatics, disease modeling and chemistry.
Despite the usefulness of modelling, there are difficulties to estimate their parameters
from the data. In most cases, the solution of ODE cannot be calculated analytically. For
an ODE model (ODEM) which is a nonlinear regression model whose mean function is
a solution of ODE, repeated computations of numerical solutions in the likelihood make
the inference very slow. Aside from the speed, an accurate estimate itself is challenging
because the ODE solutions as regression curves make the likelihood surface extremely
messy.
From the frequentist side, the two step approach was proposed by Swartz & Bremermann (1975) and Varah (1982). In the first step, the observations are fitted with function
estimation method without considering the ODE, and in the second step, the parameters
of ODE are estimated by minimizing the difference between the estimated function and
the ODE.
Ramsay & Silverman (2005) proposed the penalized likelihood approach which penalizes deviation from the differential equation. The penalized likelihood is optimized

2

iteratively with respect to the parameters of the differential equation and the regression
function. Ramsay et al. (2007) proposed the generalized profiling method whose computation consists of outer and inner loops. In the outer loop, the parameters of the differential
equation are optimized for the likelihood function. Whenever the regression function required in the outer loop, it is obtained by minimizing the penalized likelihood, which is
called the inner loop.
Liang & Wu (2008) and Liang et al. (2010) proposed two step approaches where the
local polynomial regression is adopted in the observation fitting stage. Hall & Ma (2014)
proposed an one-step estimation method in which the tuning parameter and the parameters in the ODE are estimated simultaneously by minimizing an objective function.
Gelman et al. (1996) and Huang et al. (2006) fitted the nonlinear regression model for
the Bayesian pharmacokinetic model and hierarchical HIV dynamic model, respectively,
and Markov chain Monte Carlo (MCMC) samples were generated with numeric solver
of ODE. Campbell & Steele (2012) proposed smooth functional tempering, a population
MCMC method with differing tempering parameter. Bhaumik et al. (2015) proposed a
Bayesian version of two-step approach and proved the Bernstein-von Mises theorem holds.
Dass et al. (2017) proposed a combination of Laplace approximation and grid sampling
method. Lee et al. (2018) used particle filter method for the relaxed state space model. For
nonparametric Bayesian modelling, a gradient matching method using Gaussian Processes
(GPs) as data regression model was proposed by Calderhead et al. (2008). This methodology was later evolved into the adaptive gradient matching (AGM) of Dondelinger et al.
(2013) and the GP-ODE generative model of Wang & Barber (2014).
Despite these various methods, there are still limitations in that performance comparisons have been made only for small ODE models and that even for these small models,
they occasionally show a poor inference. In this paper, we propose a Bayesian ODE parameter estimating algorithm which is fast and accurate for even somewhat large model
with lots of parameters. The algorithm exploites a state-space model and a variational
Bayes approximation. Following Lee et al. (2018), ODE models are relaxed to a state-space
model based on equations of a numeric solver such as the Runge-Kutta method. It allows
3

fast estimation by avoiding computations of a whole numerical solution in a likelihood.
The posterior is obtained by the variational Bayes method. By employing the approximate
Riemannian conjugate gradient method of Honkela et al. (2010) which perfectly matches
with our model, the estimators are obtained fast. In addition to high speed, the proposed
method showed strong stability even in large models with more than 30 parameters in
simulation studies.
The rest of the paper is organised as follows. Section 2 describes the proposed method
in detail, along with the relaxed state-space model and some background of variational
Bayes. Simulation experiments comparing the speed and performance of our algorithm
with others are provided in Section 3. In Section 4, as an application to real world data,
a time-varying SIR model which replaced the parameters of the simple SIR model with a
time-varying parameters using B-spline basis was fitted to the COVID-19 data. We end
the paper with a discussion in Section 5. Some details for the algorithm implementation
are given in Appendix A & B.

2
2.1

Method
Ordinary Differential Equation (ODE) and State-Space Model

An ordinary differential equation represents changes of variables with their derivatives
and themselves. Let p-dimensional functions x(t) satisfy an ODE
ẋ(t) = f (x(t), t ; θ),

t ∈ [0, T ],

with the ODE parameters θ ∈ Θ ⊂ Rq . Observations are modelled with errors by
yi = xi + ε i ,

iid

εi ∼ N(0, λ−1 Ip ),

(1)

ẋ(t) = f (x(t), t ; θ).
Here, xi := x(ti ) for i = 0, 1, . . . , n and 0 ≤ t0 ≤ t1 ≤ · · · ≤ tn ≤ T . Given these data,
our goal is to estimate the ODE parameters θ quickly and accurately. Further, since a

4

solution of the ODE is determined by the initial values x0 as well as θ, the estimates of
x0 can be as important as those of θ.
In many cases, ODEs have no analytic solutions and numerically computed solutions
are exploited instead. Higher accuracy of numerical solutions requires a smaller stepsize,
which implies a higher cost of computation. Repeated computations of the numercical
solution in the likelihood, then, become a major cause for a slow inference of ODE models.
As a strategy to avoid these difficulties, we follow Lee et al. (2018) and approximate
the ODE with a state-space model. The original ODE model (1) can be relaxed to the
following state-space model:
yi = xi + ε i ,

iid

εi ∼ N(0, λ−1 Ip ),

xi+1 = g(xi , ti , θ) + η i ,

iid

i = 0, 1, . . . , n,

η i ∼ N(0, τ Ip ),

(2)
i = 0, 1, . . . , n − 1.

In the ODE state-space model (OSSM) (2), g(·) is an approximating function based on
a numeric method. Throughout this paper we choose the 4th-order Runge-Kutta method,
which is one of the most widely used numeric solvers of ODEs. The function g is expressed
as
1
g(x∗i , ti , θ) = x∗i + (Ki1 + 2Ki2 + 2Ki3 + Ki4 ),
6
Ki1 = hi+1 · f (x∗i , ti ; θ),
1
Ki2 = hi+1 · f (x∗i + Ki1 , ti +
2
1
Ki3 = hi+1 · f (x∗i + Ki2 , ti +
2

1
hi+1 ; θ),
2
1
hi+1 ; θ),
2

(3)

Ki4 = hi+1 · f (x∗i + Ki3 , ti + hi+1 ; θ),
where hi+1 = ti+1 −ti for i = 0, 1, . . . , n−1. The computation of g(·) is much simpler comparing to that of the whole numerical solution, because it needs just one-step numerical
calculations based on each interval hi from the observation time points. If a time interval hi is so large as to severely threaten the stability of the approximation, we can divide
each interval into m subintervals and apply the numerical method over the m subintervals
repeatedly.

5

The approximated model requires an error term η i . Here τ , the variance of the errors,
is a constant tuning parameter which determines the amount of allowable uncertainty.
When the selected numerical method has reliable performance, the smaller τ means that
the states x’s should follow the ODE solution more closely. As m → ∞ and τ → 0, the
OSSM approaches to the true ODE model (1). In fact, Lee et al. (2018) proved that under
some regularity conditions the posterior of OSSM (2) converges to that of the true ODE
model (1). For details, see Lee et al. (2018).
The parameters to be estimated are the ODE parameters θ and the initial states x0
which determine the solution of the ODE. The extent of the measurement noise, λ, and
the subsequent states xi in times t1 , . . . , tn are additionally inferred. Recall that the tuning
parameter τ is a constant.
The priors for the parameters in this paper are as follows:
λ ∼ Gamma(A0 , B0 ),
θk ∼ Unif(a0k , b0k ), for k = 1, . . . , q,

(4)

x0j ∼ Unif(c0j , d0j ), for j = 1, . . . , p.
The gamma prior for λ has mean A0 /B0 and variance A0 /B02 . The priors for ODE parameters and initial states are uniform distributions which are fully independent of each
other. The supports (a0k , b0k ) of the θ priors are chosen based on scientific knowledge and
sufficiently large to contain the true parameter values. For x0 , some reasonable bounds
(c0j , d0j ) can be considered from the observed data.

2.2

Posterior Inference with Variational Bayes Approximation

The variational inference or variational Bayes is a posterior computation method that
approximates a posterior density function using the Kullback-Leibler divergence. First
appearing in Jordan et al. (1999), it has gained large popularity as a posterior computation
method due to the high speed of computation.
The variational Bayes posits a specific family of densities, and the member of the family
which is closest to the true posterior in the Kullback-Leibler divergence is regarded as the
6

approximate posterior. When θ represents a vector including both unknown parameters
and latent variables, the joint density of θ and observations y is
p(θ, y) = p(θ)p(y|θ).
The posterior p(θ|y), often not easy to calculate, is approximated by a member of the predefined family Q using the Kullback-Leibler divergence (KL) as a measure of ‘closeness’.
The density with the minimum KL,
Z

∗

q (θ) = arg minDKL (q(θ)||p(θ|y)) = arg min
q(θ)∈Q

q(θ)∈Q

q(θ) log
θ

q(θ)
dθ
p(θ|y)

is selected as the approximate posterior. In practice, the cost function used is not DKL
but a variant
Z
CKL = DKL (q||p) − log p(y) =

q(θ) log
θ

q(θ)
dθ
p(θ, y)

(5)

which is more tractable and provides the equivalent result of minimization. For more
details about variational method, see Blei et al. (2017).
In the popular mean-field approximation, Q consists of distributions whose variables
are all mutually independent. This setting makes the calculation of the objective function
(5) relatively easy. With such an advantage we also take this family for the posterior of
(λ, θ, x) in OSSM, having the form:
q(λ, θ, x) = q(λ)q(θ)q(x),
q(λ) = Gamma(Aλ , Bλ ),
q(θ) =

q
Y

TN(a0k, ,b0k ) (θk ; µk , σk2 ),

k=1

q(x) = TN(c0 ,d0 ) (x0 ; m0 , V0 )

n
Y

(6)

N(xi ; mi , Vi ),

i=1

Vi = diag{Vi1 , . . . , Vip }.
The ODE parameters θ and the latent variables x follow normal distributions as marginal
distributions. To be exact, for θ and x0 which have the uniform priors in (4), truncated
7

normal distributions with the supports of (4) would be more rigorous. However, because
the effect of tails in normal distribution is generally negligible under small variances, subsequent calculations treat them as normal distributions. The ranges (ak0 , bk0 ), (c0j , d0j ) in
the uniform priors may be used as the bounds for optimization procedure of the variational
Bayes.
Now the cost function of formula (5) can be calculated with the priors (4) and the
family Q (6), as follows:




p(n + 1)
p(n + 1)
ψ(Aλ ) + A0 +
log Bλ
CKL = Aλ − A0 −
2
2
q
p
n
1X
1 XX
2
− Aλ − log Γ(Aλ ) −
log σk −
log Vij
2 k=1
2 i=0 j=1
p
n
n
1 XX
1 X
2
Eq kmi − g(xi−1 , ti−1 , θ)k +
Vij
+
2τ i=1
2τ i=1 j=1
"
#
p
n

1 XX
Aλ
B0 +
(mij − yij )2 + Vij
+
Bλ
2 i=0 j=1

+ constant terms.
CKL is a function of the variational parameters (Aλ , Bλ , µ, σ 2 , m, V). Letting the partial
derivatives of CKL with respect to Aλ and Bλ equal to zero gives the following equations:
p(n + 1)
,
2
p
n

1 XX
(mij − yij )2 + Vij .
Bλ = Bλ (m, V) = B0 +
2 i=0 j=1
Aλ = A0 +

By substituting the above expressions, we can eliminate Aλ and Bλ from CKL and obtain
p
n
1 XX
CKL (µ, σ , m, V) = Aλ log Bλ (m, V) +
Vij
2τ i=1 j=1
2

q

n

p

1X
1 XX
−
log σk2 −
log Vij
2 k=1
2 i=0 j=1
+

1
2τ

n
X

Eq kmi − g(xi−1 , ti−1 , θ)k2

i=1

+ constant terms.
8

(7)

The last expectation term in (7) is nearly impossible to find its explicit form because
of the nested structure of the function g(·). To compute the value and express the term as
a function of (µ, σ 2 , m, V), we use the Monte Carlo method. When M is the number of
(s)

(s)

M
Monte Carlo samples and {θ (s) , x0 , x1 , · · · , x(s)
n }s=1 are the Monte Carlo samples from

q(·),
Eq kmi − g(xi−1 , ti−1 , θ)k2 ≈

M
1 X
(s)
mi − g(xi−1 , ti−1 , θ (s) )
M s=1

2

.

Again, since the q(·) takes the mean-field assumption and (θ (s) , x(s) ) follow the normal distributions with the parameters (µ, σ 2 , m, V), we can also get the samples in the
following ways:
√
2
{θ (s) , x(s) }M
s=1 = { σ

(s)

√

Zθ + µ,

V

(s)

where Z(s) = (Z(s)
x , Zθ ) are samples from N(0, I) and the

M
Z(s)
x + m}s=1 ,

symbol denotes elementwise

multiplication. The covariance V in the last equation is regarded as vector of its diagonal
elements rather than matrix. Finally, the cost function has the form:
CKL (µ, σ 2 , m, V)
q
p
p
n
n
1 XX
1X
1 XX
2
log σk −
Vij −
log Vij
= Aλ log Bλ (m, V) +
2τ i=1 j=1
2 k=1
2 i=0 j=1
n
M
p
1 1 XX
mi − g( Vi−1
+
2τ M i=1 s=1

Z(s)
xi−1

√
+ mi−1 , ti−1 , σ 2

(s)

(8)
2

Zθ + µ)

+ constant terms.
Since the cost function should be a function of only the variational parameters (µ, σ 2 , m, V),
the samples Z(s) must be provided before starting an optimization algorithm. For a technique to make a good Monte Carlo approximation with a small sample size M , we use a
kind of quasi-Monte Carlo method. As is well known, when FZ (·) indicates the cumulative
distribution function (CDF) of the standard normal distribution,
iid

FZ−1 (U ) ∼ N(0, 1),

for
9

iid

U ∼ Unif(0, 1).

The idea is to use the most plausible samples from the uniform distribution to get balanced
Z samples. We can set {z (s) }M
s=1 to the followings:
z (s) = FZ−1 (u(s) ),

where u(s) =

s
1
−
for s = 1, · · · , M.
M
2M

(9)

Especially if M is an odd number, the center of the standard normal distribution 0 =
FZ−1 (0.5) corresponding to s = (M + 1)/2, would be included in the samples so that the
samples look “plausible” as schemed to be symmetric about the center and reasonably
well distributed. Once we obtain a sample set values {z (s) }M
s=1 according to (9) the order
(s)

(s)
(s)
of index s is randomly shuffled for each variables Zθ , Z(s)
x0 , Zx1 , · · · , Zxn to meet the

mutual independence condition (mean-field). These schemes actually showed good results
with a small sample size M , like 11, in the simulation studies.
The variational parameters are optimized using the approximate Riemannian conjugate gradient algorithm of Honkela et al. (2010). The algorithm, for fixed-form variational
Bayes, optimises the cost function using a conjugate gradient method that exploits the
Riemannian geometry of the space of the variational parameters. They applied the algorithm to a nonlinear state-space model which belongs to the non-conjugate exponential
family, and reported that their algorithm outperforms the existing gradient-based algorithms. For details, we refer to Honkela et al. (2010). The resulting algorithm for our model
is presented in Appendix A, and the Jacobian matrix computations which are required in
the algorithm are derived in Appendix B.
If numerical problems occur during the optimization procedure, such as failure in line
search, the optimization process is completely restarted at a new starting point based on
the prior, automatically. The run time of the algorithm surely includes all these restart
processes and nevertheless, it showed the fastest inference speed in the simulation study.

10

3

Simulation study

3.1

Competitors

A total of four estimators, including the proposed method, are compared in the simulation
study. The three methods to be compared with ours are as follows.
• Parameter cascade method (PC)
The parameter cascade method by Ramsay et al. (2007) is a representative of frequentist approach in the ODE parameter inference. The method estimates parameters with nested optimization:
p
n X
X
θ̂ = arg min
(Yij − x̂ij (θ))2
θ

x̂(θ) = arg min
x ∈ Bp

i=0 j=1
p
n X
X

2

(outer),

(Yij − xj (ti )) + λ

i=0 j=1

p Z
X

[ẋj (t) − fj (x(t); t, θ)]2 dt (inner).

j=1

where B is a B-spline basis expansion with degree of D having m knots at u0 ≤
· · · ≤ um−1 ,
B(t) =

m−D+2
X

cj Bj,D (t), t ∈ [uD , um−D−1 ]

j=0

when
(
Bj,0 (t) :=

1,

uj ≤ t < uj+1

0,

otherwise

,

j = 0, . . . , m − 2,

and for d > 0,




t − uj
uj+d+1 − t
Bj,d−1 (t) +
Bj+1,d−1 (t),
Bj,d (t) :=
uj+d − uj
uj+d+1 − uj+1

j = 0, . . . , m − d − 2.

To put it simply, it is a kind of least squares regression based on B-spline basis,
but whose basis coefficients remain loyal to the ODE as well as the observations.
They avoid the numeric ODE solver by using f (·) of the ODE for the measure of
loyalty. In all of our experiments, the smoothing parameter λ was chosen by forwards
prediction error (FPE) of Ellner (2007). The CollocInfer package in R was used
11

for implementation. The functions in the package implement their optimization with
‘nlminb’ function using C/C++.
• Delayed rejection & adaptive metropolis algorithm (DRAM)
The delayed rejection & adaptive metropolis algorithm by Haario et al. (2006) was
chosen as a general MCMC method which plays a major role in Bayesian framework.
It is a combination of the delayed rejection (DR) algorithm, which postpones the
rejection and provides more sample candidates at each update step, and the adaptive
metropolis (AM) algorithm, which periodically adjusts the covariance of the proposal
distribution based on the chain so far. For DR in all our experiments, we proposed
a maximum of 2 candidates at each update step. The FME package in R was used
for implementation. In the process, ‘lsoda’ function using C/C++ was exploited to
compute the ODE solutions.
• Relaxed DEM with extended Liu and West filter (RDEM)
The idea of relaxing an ODE model to a state-space model was devised by Lee et al.
(2018). They used a sequential Monte Carlo, the extended Liu and West filter of
Rios & Lopes (2013) for posterior computation. It is reported that RDEM is faster
than PC and DRAM empirically; see Lee et al. (2018). Since it has almost the same
approximation process to state-space model, we set its stepsize (m) and the tuning
parameter (τ ) the same as ours. The priors for the RDEM algorithm were
x0 |λ ∼ Np (y0 , λ−1 Ip ),
λ ∼ Gamma(1, 1)
in all the experiments hear, following the original article. These conjugate priors
facilitate the sampling algorithm and differ from ours, which aims to make the
cost function calculation easier. The method was implemented using C++ code via
Rcpp package.
Our method is denoted by SSVB (state-space model with variational Bayes) and
implemented using C++ code via Rcpp package too. For each ODE model, 100 data
12

sets are generated from one true model and the estimates based on the 4 methods are
compared. For the Bayesian methods, the mean of posterior is taken as a point estimator.
For a fair comparison, all methods begin their algorithms at the same starting point. In
other words, when the 4 methods need any starting point in their optimization or MCMC
chains, their starting points are different across the data sets, but the same across the
methods in each data set. The starting points of ODE parameters are drawn from uniform
distributions under the assumption that little prior information is available except for their
roughly possible ranges. The starting points of x0 are selected using the cubic B-spline
regression.

3.2

Lorenz-96 model

As a big ODE model for comparing the performance, we adopted Lorenz-96 model by
Lorenz (1995). The Lorenz-96 model is a toy model for unspecified meteorological quantity
such as temperature or concentration of a substance. The reason for choosing Lorenze-96
model as a testing model is that one can enlarge the model as desired by increasing the
number of variables p (≥ 3).
The p scalar variables at equally spaced sites around a latitude circle, as shown in
Figure 1, have the relationship given by
dXj
= (Xj+1 − Xj−2 )Xj−1 − Xj + F,
dt

for j = 1, . . . , p.

According to the circular structure, X−1 = Xp−1 , X0 = Xp , and Xp+1 = X1 . The quadratic
terms and the linear terms correspond to advection and dissipation respectively. The only
ODE parameter is constant terms F , corresponding to external forcing.
In fact, the above model is a reduced model letting the coefficients of the quadratic and
linear terms to be 1. In this simulation study, for the purpose of performance comparison
in a large model with many parameters and variables, we use the following model:
dXj
= θ1j (Xj+1 − Xj−2 )Xj−1 − θ2j Xj + θ3j ,
dt

for j = 1, . . . , p.

If the number of variable is p, the numbers of ODE parameters and the initial values are
13

Figure 1: The scalability of Lorenz-96 model.
q = 3p and p, respectively; thus total number of parameters to be estimated is q + p = 4p.
If the model has 10 variables, for example, a total of 40 parameters need to be inferred.
In this paper, we conducted experiments with p = 4 and p = 10 respectively. The true
parameter values are (θ1j , θ2j , θ3j ) = (1, 1, 8) for all j = 1, . . . , p in both cases, reflecting
the original model (θ1j = θ2j = 1) and guaranteeing the chaotic behaviour (θ3j = 8); see
Lorenz & Emanuel (1998).
The DRAM algorithm was excluded from the Lorenz-96 simulation due to its slow computational speed and poor performance. The chain failed to converge even with 100,000
iterations for the p = 4 model.

Figure 2: Lorenz-96 model with 4 variables of x0 = (1, 8, 4, 3)T . One of the data sets is
plotted.
14

3.2.1

Lorenz-96 model with 4 variables

First, we generated 100 data sets from the Lorenz-96 model with 4 variables. The error
variance was 1/λ = 1 and the initial values x0 = (1, 8, 4, 3)T were randomly selected.
Observations are made at the 51 equidistant time points t0 = 0, t1 = 0.1, · · · , t50 = 5.
The solution lines and one data set are shown in Figure 2. The total number of parameters
to be estimated is p + q = 4 + 12 = 16.
The priors for the SSVB were :
λ ∼ Gamma(1, 1),
θ j ∼ Unif {(0, 2) × (0, 2) × (0, 16)}, for j = 1, . . . , 4,
and the uniform priors for x0 were properly selected from the data set. With the tuning
parameters m = 2, τ = 0.14 and the number of Monte carlo samples M = 11 in the cost
function, the variational parameters were optimized.
For the other methods, the PC method’s smoothing parameter λ = 10, 000 was chosen
based on FPE of Ellner (2007). The RDEM algorithm was implemented with 50,000
particles.
Figure 3 shows the boxplots of the running times of each method for 100 data sets.
The SSVB showed the fastest inference speed with an average time of 6.38 seconds. It
was followed by the RDEM of 15.40 seconds and the PC of 33.36 seconds.
The inference results on 100 data sets are plotted in Figure 4. In the figure, the vertical
axis represents the indices of the data sets and the horizontal axis represents the resulting
estimates for the data set. The true parameter value was represented by a purple vertical
line in the center. Of the 16 parameters, only 8 results associated with X3 and X4 were
displayed but the other 8 parameters also showed similar patterns. The RDEM algorithm
tended to be biased in estimating the x0 . Some of the 100 data sets led the PC method to
make poor estimates. This shows that the PC method can fail depending on the given data
set and starting point of the algorithm. In the next experiment with 10 variables, we can
confirm that this tendency increases together with the number of estimated parameters.

15

Figure 3: The boxplots of the running time of the inference for the 100 data sets from the
Lorenz-96 model with 4 variables.

Figure 4: The resulting estimates from the 3 methods in the Lorenz-96 model with 4
variables. Red circle: SSVB, Blue triangle: PC, Green diamond : RDEM. Each plot was
centered on a true parameter value represented by a purlpe vertical line. Results were
shown for only 8 parameters, but the other 8 parameters also showed similar patterns.

16

More specifically, Table 1 shows the mean absolute bias (MAB) and the sample standard deviation (SSD) for the 100 estimates. The SSVB method had the smallest MAB
and the smallest SSD for all parameters. In the table, the last column ‘ratio’ denotes the
ratio of the SSVB to the best of the others. For example, for MAB of θ1,1 , the best of the
others is RDEM’s 0.560 and it is 1.2371 times that of the SSVB, 0.0460. Overall, we can
see that the SSVB provides a fairly stable estimate compared to the others.
Finally, we checked the estimated solution curves with their best/worst estimates
(θ̂, x̂0 ) from the 100 data sets. They were selected based on the deviation (sum of squares)
from the true solution curve at the observation times. In Figure 5 showing the curves for
X3 and X4 , all the methods provided their best curves that closely match the true solution curve. However, in the case of worst estimate, only the SSVB method produced
SSVB

PC

RDEM

min.

ratio.

SSVB

PC

RDEM

min.

ratio.

θ1,1

0.0460

0.0799

0.0569

SSVB

1.2371

θ1,1

0.0554

0.1589

0.0692

SSVB

1.2480

θ1,2

0.1172

0.1414

0.1425

SSVB

1.2059

θ1,2

0.1468

0.1980

0.1926

SSVB

1.3122

θ1,3

0.9275

1.0487

1.0879

SSVB

1.1307

θ1,3

1.0833

1.4475

1.3647

SSVB

1.2597

θ2,1

0.0382

0.0581

0.0478

SSVB

1.2506

θ2,1

0.0495

0.1161

0.0608

SSVB

1.2286

θ2,2

0.1659

0.1809

0.2392

SSVB

1.0906

θ2,2

0.1836

0.2379

0.2664

SSVB

1.2957

θ2,3

0.7625

0.9102

1.1903

SSVB

1.1937

θ2,3

0.9328

1.2679

1.3583

SSVB

1.3593

θ3,1

0.0511

0.0734

0.0598

SSVB

1.1716

θ3,1

0.0604

0.1054

0.0684

SSVB

1.1324

θ3,2

0.1699

0.2078

0.2013

SSVB

1.1852

θ3,2

0.2060

0.2639

0.2613

SSVB

1.2689

θ3,3

0.7544

0.9515

0.9462

SSVB

1.2542

θ3,3

0.9532

1.2086

1.1554

SSVB

1.2121

θ4,1

0.0423

0.0567

0.0533

SSVB

1.2598

θ4,1

0.0552

0.0993

0.0708

SSVB

1.2830

θ4,2

0.1542

0.1930

0.1871

SSVB

1.2132

θ4,2

0.1909

0.2861

0.2422

SSVB

1.2689

θ4,3

0.8622

0.9440

1.1372

SSVB

1.0949

θ4,3

1.0505

1.2149

1.3535

SSVB

1.1566

x01

0.4280

0.4712

1.3701

SSVB

1.1011

x01

0.4881

0.6063

0.9403

SSVB

1.2420

x02

0.3873

0.5338

0.7132

SSVB

1.3782

x02

0.4801

0.9362

0.8861

SSVB

1.8455

x03

0.6681

0.7406

0.8444

SSVB

1.1085

x03

0.8047

0.9389

1.0082

SSVB

1.1668

x04

0.4028

0.4852

1.2484

SSVB

1.2045

x04

0.5074

0.6228

0.9270

SSVB

1.2274

(a) Mean absolute bias

(b) Sample standard deviation

Table 1: MAB and SSD for the 100 estimates in the Lorenz-96 model with 4 variables.

17

curves that follow the true solution. Although omitted, the results for X1 and X2 also had
the same patterns. This shows that the SSVB method provides a relatively very stable
estimator and it can be confirmed more clearly in the following big model, the Lorenz-96
model with 10 variables.
3.2.2

Lorenz-96 model with 10 variables

As the number of parameters to be estimated reaches p+q = 10+30 = 40, the accuracy of
inference was expected to be worse than the Lorenz-96 model with 4 variables. Along the
51 time points t0 = 0, t1 = 0.1, · · · , t50 = 5, a total of 100 data sets were generated from
the true solution curve with x0 = (10, 4, 1, 0, 2, 8, 3, 10, 1, 5)T . As in the previous
simulation, the observation errors are added with variance 1/λ = 1. The true solution

Figure 5: The estimated solution curves for X3 and X4 with their best/worst estimates
(θ̂, x̂0 ) from the 100 data sets, of the Lorenz-96 model with 4 variables, for each method.
X1 and X2 also showed similar patterns.

18

curves and one data set are shown in Figure 6.
All the priors and settings for the SSVB method were the same as in the previous case
of 4 variables except for the step size m = 3. The smoothing parameter of the PC method
based on FPE of Ellner (2007) also have the same value λ = 10, 000 as before. The size of
particles of the RDEM algorithm was increased to 200,000 so as to afford the big model.
The running times of the methods are plotted in Figure 7. The mean time of the SSVB
was the fastest at 54.33 seconds, followed in turn by the RDEM of 219.82 seconds and
the PC of 358.97 seconds. As a results, the SSVB showed the fastest inference speed in
both experiments.
What we should notice in the big model is, in fact, the accuracy of the estimation
rather than the speed of computation. Figure 8 shows the resulting estimates from the
3 methods. The number of severe failures was greater than before in both the PC and
RDEM algorithm. On the other hand, the SSVB method tended to be relatively close to
the true parameter values, not significantly affected by the data set. These overall trends

Figure 6: Lorenz-96 model with 10 variables of x0 = (10, 4, 1, 0, 2, 8, 3, 10, 1, 5)T .
One of the data sets is plotted.

19

Figure 7: The boxplots of the running time of the inference for the 100 data sets from the
Lorenz-96 model with 10 variables.
were the same for the 32 omitted parameters.
Actually, the mean absolute biases of the SSVB were the smallest for all the parameters
except x09 as shown in Table 2. The ratio from the second place reached 2.8481 at the
maximum and the others also showed substantial differences. For the sample standard
deviation in which the SSVB showed the smallest values for all the parameters, the differences became more severe. Table 3 shows that the maximum ratio reached 4.5534 and
several other ratios exceeded 2.
The most important result in this paper is shown in Figure 9. Unlike the previous
experiment, the best estimates of both the PC and RDEM methods produced solution
curves that significantly deviate from the true solution curve, for all the 10 variables.
This indicates that virtually all the 100 estimates from the 100 different data sets cannot
provide the valid regression curves. Of course, some of the 40 parameters can be estimated
to be very close to the true parameters, but the problem is that the solution curve of
the ODE is drastically changed by all the parameters in the ODE, including the initial
values x0 . In other words, if the estimates of the parameters do not form an appropriate

20

Figure 8: The resulting estimates related to X5 and X6 from the 3 methods in the Lorenz96 model with 10 variables. Red circle: SSVB, Blue triangle: PC, Green diamond : RDEM.
Each plot was centered on a true parameter value represented by a purlpe vertical line.
Results were shown for only 8 parameters, but the other 32 parameters also showed similar
patterns.
combination, it is impossible to estimate the regression curve.
The strength of the SSVB is revealed here. Not only did the SSVB produce a solution
curve that closely matches the true curve from the best estimate, it also produced a regression curve that follows the true solution curve to some extent even from the worst
estimate. That is, in addition to estimating the parameters themselves properly, the solution curves derived from them were also estimated to be consistent with the data without
losing much of its stability even in the big model.
The results thought to be due to the concentrativeness of the mean field variational
method. As the ODE system becomes more complex and larger, the ODE curve determined by x0 and θ can more sensitively vary in shape. That is, the combination of x0
and θ becomes more important rather than the respective estimates. For the PC method,
though the ODE parameters θ are optimized under the interaction with the whole B-spline

21

SSVB

PC

RDEM

min.

ratio.

θ7,3

0.4689

0.9767

3.0383

SSVB

2.0829

1.4938

θ8,1

0.0418

0.0818

0.2128

SSVB

1.9562

SSVB

1.5849

θ8,2

0.1646

0.1890

0.3757

SSVB

1.1485

0.1937

SSVB

2.8481

θ8,3

0.3417

0.5310

2.9875

SSVB

1.5540

0.2527

0.3956

SSVB

1.5373

θ9,1

0.0462

0.0759

0.3280

SSVB

1.6431

0.5753

0.8469

2.6669

SSVB

1.4723

θ9,2

0.2121

0.2483

0.4184

SSVB

1.1704

θ3,1

0.0530

0.0937

0.2914

SSVB

1.7670

θ9,3

0.9793

1.2501

2.7519

SSVB

1.2765

θ3,2

0.1402

0.1505

0.4222

SSVB

1.0739

θ10,1

0.0498

0.0758

0.2698

SSVB

1.5212

θ3,3

0.8667

0.9371

2.6462

SSVB

1.0812

θ10,2

0.1099

0.1725

0.4179

SSVB

1.5692

θ4,1

0.0403

0.0539

0.2583

SSVB

1.3361

θ10,3

0.7858

0.8878

3.1258

SSVB

1.1298

θ4,2

0.0954

0.1237

0.3914

SSVB

1.2967

x01

0.3755

0.4585

1.9858

SSVB

1.2211

θ4,3

0.3466

0.4795

2.5656

SSVB

1.3833

x02

0.4065

0.4779

3.1649

SSVB

1.1758

θ5,1

0.0553

0.1002

0.2398

SSVB

1.8105

x03

0.5104

0.6412

1.8294

SSVB

1.2562

θ5,2

0.1444

0.2088

0.4074

SSVB

1.4463

x04

0.2480

0.3235

1.7746

SSVB

1.3044

θ5,3

0.4527

0.7136

2.4929

SSVB

1.5763

x05

0.3415

0.5114

1.9504

SSVB

1.4977

θ6,1

0.0474

0.0735

0.2483

SSVB

1.5511

x06

0.4469

0.5581

1.8704

SSVB

1.2489

θ6,2

0.1003

0.1538

0.3437

SSVB

1.5337

x07

0.5766

0.6889

2.3787

SSVB

1.1948

θ6,3

0.5557

0.9886

2.5301

SSVB

1.7791

x08

0.4319

0.5577

2.6107

SSVB

1.2912

θ7,1

0.0395

0.0549

0.2381

SSVB

1.3909

x09

0.5674

0.5565

2.0066

PC

0.9809

θ7,2

0.1166

0.1511

0.4100

SSVB

1.2958

x10

0.3330

0.4869

1.9900

SSVB

1.4619

SSVB

PC

RDEM

min.

ratio.

θ1,1

0.0361

0.0619

0.2182

SSVB

1.7148

θ1,2

0.1388

0.2074

0.4215

SSVB

θ1,3

0.4220

0.6689

2.8240

θ2,1

0.0413

0.1177

θ2,2

0.1644

θ2,3

Table 2: Mean absolute bias for the 100 estimates in the Lorenz-96 model with 10 variables.
basis, the initial states x0 , given θ, are only determined by some local basis coefficients
of the several basis near t = 0. For the MCMC based methods including RDEM, as the
model grows the problems in moving on the parameter space obstructed by their complex
dependencies between the parameters also get bigger. On the contrary, the mean field
variational Bayes approximates the density to the one with dense density in the center. In
other words the assumption of independence in the mean-field makes the approximated
density concentrated on the most representative combinations of x0 and θ. As a result the
reproduction of the ODE curves can be performed best through the proposed method.

22

SSVB

PC

RDEM

min.

ratio.

θ7,3

0.5868

1.3677

3.2604

SSVB

2.3308

1.6653

θ8,1

0.0535

0.1330

0.2689

SSVB

2.4861

SSVB

1.8833

θ8,2

0.2065

0.2472

0.4285

SSVB

1.1971

0.2261

SSVB

4.5534

θ8,3

0.4376

0.7454

3.4990

SSVB

1.7034

0.4800

0.4666

SSVB

2.2884

θ9,1

0.0571

0.1146

0.3010

SSVB

2.0072

0.7094

1.2944

3.2228

SSVB

1.8246

θ9,2

0.2651

0.3155

0.4677

SSVB

1.1903

θ3,1

0.0664

0.1299

0.3030

SSVB

1.9572

θ9,3

1.1698

1.4806

2.9127

SSVB

1.2657

θ3,2

0.1524

0.2047

0.4803

SSVB

1.3437

θ10,1

0.0620

0.1126

0.3362

SSVB

1.8170

θ3,3

1.0227

1.3611

3.2660

SSVB

1.3309

θ10,2

0.1364

0.2651

0.4915

SSVB

1.9437

θ4,1

0.0493

0.0727

0.3028

SSVB

1.4751

θ10,3

0.9393

1.1058

3.7625

SSVB

1.1772

θ4,2

0.1196

0.1733

0.4712

SSVB

1.4494

x01

0.4742

0.6116

2.5345

SSVB

1.2897

θ4,3

0.4413

0.6431

3.2410

SSVB

1.4573

x02

0.4971

0.6895

3.1750

SSVB

1.3871

θ5,1

0.0667

0.1647

0.2959

SSVB

2.4673

x03

0.6124

0.8285

2.2023

SSVB

1.3528

θ5,2

0.1736

0.3318

0.4782

SSVB

1.9111

x04

0.3072

0.4209

2.2970

SSVB

1.3700

θ5,3

0.5993

1.0250

3.0979

SSVB

1.7103

x05

0.4469

0.6323

2.2751

SSVB

1.4149

θ6,1

0.0593

0.1010

0.2890

SSVB

1.7016

x06

0.5602

0.7171

2.2786

SSVB

1.2800

θ6,2

0.1273

0.2168

0.4140

SSVB

1.7026

x07

0.7639

0.8550

2.2397

SSVB

1.1193

θ6,3

0.6933

1.4235

3.1694

SSVB

2.0532

x08

0.5345

0.7589

2.7780

SSVB

1.4197

θ7,1

0.0497

0.0795

0.1551

SSVB

1.5987

x09

0.6695

0.6696

2.4548

SSVB

1.0001

θ7,2

0.1456

0.1885

0.4699

SSVB

1.2949

x10

0.4239

0.6273

2.3603

SSVB

1.4797

SSVB

PC

RDEM

min.

ratio.

θ1,1

0.0441

0.0988

0.2727

SSVB

2.2382

θ1,2

0.1654

0.2755

0.4867

SSVB

θ1,3

0.4800

0.9040

3.4055

θ2,1

0.0496

0.2897

θ2,2

0.2039

θ2,3

Table 3: Sample standard deviation for the 100 estimates in the Lorenz-96 model with 10
variables.

4

Application to real data: COVID-19

4.1

SIR model with time-varying parameters

As an application to real-world data, we chose COVID-19 epidemic which is now the
world’s biggest issue. Since the first outbreak in Wuhan, China, lots of observational
data have been recorded daily. One important indicator is the number of infected people
because it can be used to determine the infectivity of the disease.
The SIR model of Kermack & McKendrick (1927) using ODEs is one of the most
23

Figure 9: The estimated solution curves for X5 and X6 with their best/worst estimates
(θ̂, x̂0 ) from the 100 data sets, of the Lorenz-96 model with 10 variables, for each method.
The other variables (X1 ∼ X4 and X7 ∼ X10 ) also showed similar patterns.
famous tool for dealing with it. It has 3 compartments: S (Susceptible), I (Infectious) and
R (Removed). S stands for the number of non-infected individuals who may be infected in
the future. I indicates the number of now infected people who can pass the infections to
S. Lastly R, meaning the removed people from the infectious relationships, is the number
of recovered or dead people from the epidemic. They are not infectious and also no longer
subject to infection because they have immunity or are already dead. The total population
is divided into these three compartments.

24

Although there are diverse variants, the basic SIR model is as follows:
βI(t)S(t)
dS(t)
=−
,
dt
N
dI(t)
βI(t)S(t)
=
− γI(t),
dt
N
dR(t)
= γI(t).
dt
The total population N = S(t) + I(t) + R(t) is assumed to be constant. The parameter
β is the average number of contagious contacts by one infected person per unit time.
Among a total of βI(t) who had contagious contacts with infectious people, as much as
the ratio S(t)/N of them can be newly infected and introduced into I(t). On the other
hand, an infected individual recovers or dies at an average rate of γ and move on to
the R(t) compartment. In other words, it takes an average of 1/γ days to die or recover
completely. It is known that the ratio of two parameters, the so-called basic reproductive
number R0 = β/γ, is used to evaluate the infectivity.
Unfortunately, however, it is difficult to apply the above model to real-world data as it
is. The model assumes that people’s contacts are in a simply random way, such as those
occurring in molecular motion. When a country with a population of tens of millions is
taken as the unit of analysis, this assumption can lead to significant gap from the realworld data. Actually, the forms of solutions that the model can have are particularly
limited. If you look at it briefly for example, it is easy to see that R(t) has the steepest
slope when I(t) is at its maximum value, since γ is a constant. And when you check the
actual data, you can see that this is rarely happening. The COVID-19 data cannot be
fitted by the simple SIR model.
Focusing on taking advantage of the SSVB algorithm at the same time as solving
the above problem, we can think of the following model that gives the parameters great

25

flexibility using the cubic B-spline basis:
β(t)I(t)S(t)
dS(t)
=−
,
dt
N
dI(t)
β(t)I(t)S(t)
=
− γ(t)I(t),
dt
N
dR(t)
= γ(t)I(t),
dt
nX
o
where β(t) := exp
cβ,i Bβ,i (t) ,
i

γ(t) := exp

nX

o
cγ,i Bγ,i (t) .

i

Here, Bβ,i (t) and Bγ,i (t) are the cubic B-spline basis functions defined by the range of
time t and some adequate knots. Since a B-spline curve is a piecewise polynomial function
and locally fitted to data, it can cope with well locally varying fluctuations. Especially
in the case of order 4, which represents piecewise cubic polynomial, it has a continuous
second derivative and forms a very smooth curve. The ODE parameters to estimate are
now the coefficients cβ,i and cγ,i of the basis functions, whose number can be increased by
adding more knots. In its practical application to COVID-19 data, it was modeled with
28 coefficients based on 14 basis each for β and γ. Lastly, the exponential function was
used to ensure positive values.

4.2

Fitting the COVID-19 data

The SIR model looks as if it has three variables, but in fact the actual variables are two
since it is assumed that the total population N = S(t)+I(t)+R(t) is constant. Therefore,
for practical implementation we used the following equations with the substitution S(t) =
N − I(t) − R(t):
dI(t)
β(t)I(t)(N − I(t) − R(t))
=
− γ(t)I(t),
dt
N
dR(t)
= γ(t)I(t).
dt
As mentioned above, the parameters β(t) and γ(t) were defined as linear combinations
of 14 basis functions, with equidistant knots as shown in Figure 10. The fda package in
26

Figure 10: The cubic B-spline basis functions for β(t) and γ(t).
R was used to calculate basis functions. We estimated the 28 ODE parameters and the 2
initial states for each country.
The priors for the SSVB were:
λ ∼ Gamma(0.01, 0.01),
θj ∼ Unif (−∞, ∞), for all j = 1, . . . , 18.
Since the scale of variable values is much larger than in the previous simulations, a more
flat prior was given for λ. For the SSVB algorithm, the tuning parameters m = 1, τ = 0.16
were chosen with the number of Monte carlo samples M = 7 in the cost function.
Because the variable values are large and the ODE model in hand is sensitive to
changes in parameters through exponential functions in β(t) and γ(t), it is possible that
the optimization process may fail to calculate gradients or fundamentally, take a long time
to find an appropriate starting values. Accordingly, a proper starting value calculated
from the data in a discretized way was provided and the learning rate of line search in
optimization was also given very small so that the algorithm could start successfully. The
termination conditions were also given more loosely to prevent unnecessary ticking-over.
Some results for South Korea, China and Japan are shown in Figure 11. The data on
the number of confirmed cases in each country were obtained from the GitHub repository
operated by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins
University, Dong et al. (2020). For the total population numbers N , we used the UN
27

Figure 11: The fitted results for South Korea, China and Japan. The four graphs on the
right side show the estimated β(t), α(t) and R0 (t) = β(t)/γ(t).
28

World Population Prospects 2019 provided by United Nations, Department of Economic
and Social Affairs, Population Division (2019). The graph on the left shows the fitted
results with the real data points, and the four graphs on the right show the estimated
β(t), α(t) and R0 (t) = β(t)/γ(t). For R0 (t), graphs ranging from 0 to 5 were added to see
the values in a relatively realistic range. In the case of Korea, the data in the early days
which are too flat to greatly violate the softness of the SIR model are not included for
the inference. In the case of Japan, too, a excessively flat period in the early days was
excluded from the estimation.
The SSVB algorithm properly estimated the 30 parameters within about 40 seconds
for each country. As expected, the rapid fluctuations of beta and gamma were required to
fit the COVID-19 data with the SIR model. This shows that the assumption of the simple
SIR model is very unrealistic at the national scale, and that much more sophisticated
models are required for the analysis of epidemic spread patterns in the real world. It is
meaningful that we directly verified this by estimating a number of parameters using the
proposed algorithm.

5

Discussion

We proposed an ODE parameter estimating method based on state-space model and
variational Bayes approximation. The conversion to the state-space model reduces the
amount of computation from the whole numerical solution to a few one-step calculations
at the observation time points. In addition, the posterior approximation by variational
Bayes makes the inference even faster.
The proposed method showed strength not only in the speed but also in the performance of reproducing the ODE curves. In the simulation studies, it was especially
noticeable as the number of parameters to be inferred increased. When the ODE model
is large, the approximation of the variational Bayes has some advantage in providing a
good combination of x̂0 and θ̂, even compared to the original posterior through sampling
method represented by the RDEM algorithm. When applied to actual data with somewhat
29

large variable values, it also showed decent performance in tens of seconds.
With good performance, however, the issue of underestimation of the variance indicated in the mean-field variational method remains to be improved. Several correction
strategies have been studied, including the method by Giordano et al. (2015), but due to
the nature of the state-space model which has a strong dependency between neighboring
latent variables, it didn’t work well in our model. Nevertheless, those can be seen as a
trade-off taken for better reproduction of the ODE curve. Considering the sensitivity of
the ODE curves, the accuracy of estimation can be more important than the corvariance
structure.

30

A

Appendix : Optimization algorithm

Algorithm 1 Optimization algorithm of SSVB
1. Update mean parameters u = (µ, m), when m = (m0 , . . . , mn )T .
< approximate Riemannian conjugate gradient learning >
p0 = 0q+p(n+1) , g̃0 = 1q+p(n+1)
for k = 1, 2, . . . do

. Repeat until convergence

g̃k ← diag(σ 2 , V)∇uk−1 CKL (uk−1 , · )
(∇uk−1 CKL )T (g̃k − g̃k−1 )

β←
T
g̃k−1
diag σ12 , V1 g̃k−1
pk ← −g̃k + βpk−1

. Riemannian gradient
. Polak-Ribiére formula
. Update direction

α ← arg min CKL (uk−1 + αpk , · )

. Line search

α

uk ← uk−1 + αpk

. Update

2. Update variance parameters s = (σ 2 , V), when V = (V0 , . . . , Vn )T as vectors.
< fixed-point iteration >
iteration

s−1
⇐= 2∇sk−1 Ffixed (s)
k

. Iterate until convergence

Ffixed (s)
p
n
1 XX
= Aλ log Bλ (m, V) +
Vij
2τ i=1 j=1
n
M
p
1 1 XX
mi − g( Vi−1
+
2τ M i=1 s=1

√
Z(s)
σ2
xi−1 + mi−1 , ti−1 ,

(s)

2

Zθ + µ)

3. Iterate 1 and 2 until convergence.

• For the line search procedure, we apply the quadratic interpolation method with three
points in Sun & Yuan (2006).
• As a skill to speed up the algorithm, we let the termination criteria of the early
part of updating mean parameters be lax and tighten it gradually up to the final
criterion ε. It can improve the speed about 2 to 4 times faster.
31

• The details for the computations are as follows. The notation Jg wrt θ (·) means the
Jacobian matrix of g(·) with respect to θ and the others are analogous to it.

A 1. Gradient calculations for the Riemannian gradient
The gradient vector with respect to the mean parameters consists of the sub-vectors:


∇ C
 µ KL 


 ∇m0 CKL 
.
∇u CKL = 


..


.


∇mn CKL
Using matrix calculus and the chain rule, we can obtain the first part,
#
"
n
M
√
2
1 X 1 X
(s)
(s)
mi − g(xi−1 , ti−1 , σ 2 Zθ + µ)
∇µ CKL =∇µ
2τ i=1 M s=1
n
M

T 

1 1 XX
(s)
(s)
=−
mi − g(xi−1 , ti−1 , θ (s) ) .
Jg wrt θ xi−1 , ti−1 , θ (s)
τ M i=1 s=1

In the same way,
∇m0 CKL =

Aλ
(m0 − y0 )
Bλ (m, V)
M

T 

1 1 X
(s)
(s)
−
Jg wrt x x0 , t0 , θ (s)
m1 − g(x0 , t0 , θ (s) ) ,
τ M s=1

for i = 1, · · · , n − 1,
∇mi CKL

M

Aλ
1 1 X
(s)
(s)
=
(mi − yi ) +
mi − g(xi−1 , ti−1 , θ )
Bλ (m, V)
τ M s=1
M

T 

1 1 X
(s)
(s)
(s)
(s)
−
Jg wrt x xi , ti , θ
mi+1 − g(xi , ti , θ ) ,
τ M s=1

∇mn CKL =

M

1 1 X
Aλ
(s)
(mn − yn ) +
mn − g(xn−1 , tn−1 , θ (s) ) .
Bλ (m, V)
τ M s=1

32

A 2. Gradient calculations for the fixed-point iteration
For the fixed-point iteration,


2∇σ2 Ffixed (s)







 2∇V0 Ffixed (s) 
.

2∇s Ffixed (s) = 

..


.


2∇Vn Ffixed (s)
In the same way as above, we use matrix calculus and the chain rule.

n
M
1 1 XX
1
2∇σ2 Ffixed (s) = −
diag
τ M i=1 s=1
σ
2∇V0 Ffixed (s) =

Aλ
1p
Bλ (m, V)

M
1 1 X
1
−
diag √
τ M s=1
V0

(s)
Zθ

Z(s)
x0


Jg wrt θ





Jg wrt x



Jg wrt x



(s)
xi−1 , ti−1 , θ (s)

(s)
x0 , t0 , θ (s)

T 

T 

mi −

m1 −



(s)
g(xi−1 , ti−1 , θ (s) )



(s)
g(x0 , t0 , θ (s) )

,

for i = 1, · · · , n − 1,
2∇Vi Ffixed (s) =

2∇Vn Ffixed (s) =

1
Aλ
1p + 1p
Bλ (m, V)
τ

M
1 1 X
1
−
diag √
τ M s=1
Vi

Z(s)
xi



(s)
xi , ti , θ (s)

T 

mi+1 −

(s)
g(xi , ti , θ (s) )

Aλ
1
1p + 1p .
Bλ (m, V)
τ

The calculations of Jacobian matrices in the formulas above are derived in Appendix B.

B

Appendix : Jacobian matrix

The proposed method needs the Jacobian matrix of the approximating function g(·) in
(2). We present the derivation of it from that of the ODE function f (·), Jf , in the case of

33



,

,

the 4th-order Runge-Kutta method:
1
g(x, t, θ) = x + (K1 + 2K2 + 2K3 + K4 )
6
K1 = h · f (x, t; θ)

1
K2 = h · f x + K 1 , t +
2

1
K3 = h · f x + K 2 , t +
2


1
h; θ
2

1
h; θ
2

(10)

K4 = h · f (x + K3 , t + h; θ),
which is selected in this paper. For simplicity, the subscripts are omitted and equally
spaced observation times (h = h1 = · · · = hn ) are assumed.

B 1. The case of step size m = 1
First, when the step size m = 1, the Jacobian of g(·) with respect to x can be obtained
by
1
Jg wrt x (x, t, θ) = Ip×p + (JK1 wrt x + 2JK2 wrt x + 2JK3 wrt x + JK4 wrt x )
6
JK1 wrt x = h · Jf
JK2 wrt x
JK3 wrt x
JK4 wrt x

wrt x (x, t, θ)



1
1
h, θ Ip×p + JK1 wrt x
2
2


1
1
h, θ Ip×p + JK2 wrt x
2
2
h
i
= h · Jf wrt x (x + K3 , t + h, θ) Ip×p + JK3 wrt x ,

= h · Jf wrt x x +

= h · Jf wrt x x +

1
K1 , t +
2
1
K2 , t +
2

from using the matrix chain rule.
On the other hand, with respect to θ, we should be cautious that K2 , K3 and K4
include the previous term which is a function of θ, in the position of x argument. Finally,
we get:
Jg wrt θ (x, t, θ) =

1
(JK1 wrt θ + 2JK2 wrt θ + 2JK3 wrt θ + JK4 wrt θ )
6

34

JK1 wrt θ = h · Jf

wrt θ (x, t, θ)




 1
JK1 wrt θ
1
1

Jf wrt θ x + K1 , t + h, θ  2
2
2
Iq×q






 1
J
K
wrt
θ
1
1
1
1
2

= h · Jf wrt x x + K2 , t + h, θ
Jf wrt θ x + K2 , t + h, θ  2
2
2
2
2
Iq×q


h
i JK wrt θ
3
.
= h · Jf wrt x (x + K3 , t + h, θ) Jf wrt θ (x + K3 , t + h, θ) 
Iq×q


JK2 wrt θ

JK3 wrt θ

JK4 wrt θ



1
1
= h · Jf wrt x x + K1 , t + h, θ
2
2

These Jacobian matrices (functions) become the base function of the next case.

B 2. The case of step size m ≥ 2
First, without considering the step size m, we can define some iterative functions like:
g(2) (x, t, θ) = g(g(x, t, θ), t + h, θ)
g(3) (x, t, θ) = g(g(2) (x, t, θ), t + 2h, θ)
..
.

g(m) (x, t, θ) = g g(m−1) (x, t, θ), t + (m − 1)h, θ .
Then, the Jacobian matrices with respect to x of the above functions can be recursively
computed from Jg wrt x (·) of B 1. as follows:
Jg(2) wrt x (x, t, θ) = Jg wrt x (g(x, t, θ), t + h, θ) Jg wrt x (x, t, θ)

Jg(3) wrt x (x, t, θ) = Jg wrt x g(2) (x, t, θ), t + 2h, θ Jg(2) wrt x (x, t, θ)
..
.

Jg(m) wrt x (x, t, θ) = Jg wrt x g(m−1) (x, t, θ), t + (m − 1)h, θ Jg(m−1) wrt x (x, t, θ).
For θ, again be cautious that each function includes the previous function which is a

35

function of θ, in the position of x argument. The chain rule of matrix calculus gives:


h
i Jg wrt θ (x, t, θ)

Jg(2) wrt θ (x, t, θ) = Jg wrt x (g(x, t, θ), t + h, θ) Jg wrt θ (g(x, t, θ), t + h, θ) 
Iq×q


h
i

 Jg(2) wrt θ (x, t, θ)
Jg(3) wrt θ (x, t, θ) = Jg wrt x g(2) (x, t, θ), t + 2h, θ
Jg wrt θ g(2) (x, t, θ), t + 2h, θ 
Iq×q
..
.
Jg(m) wrt θ (x, t, θ)
h



= Jg wrt x g(m−1) (x, t, θ), t + (m − 1)h, θ





i

 J (m−1)
g
wrt θ (x, t, θ)
.
Jg wrt θ g(m−1) (x, t, θ), t + (m − 1)h, θ 
Iq×q

Now to return to the main point, for given step size m ≥ 2 in the proposed method,
we can use Jg(m) wrt x (x, t, θ) and Jg(m) wrt θ (x, t, θ) that are computed with h/m instead
of h from the above formulas as the Jacobian matrix of g(·) in the algorithm of Appendix
A.

References
Bhaumik, P., Ghosal, S. et al. (2015). Bayesian two-step estimation in differential equation
models, Electronic Journal of Statistics 9(2): 3124–3154.
Blei, D. M., Kucukelbir, A. & McAuliffe, J. D. (2017). Variational inference: A review for
statisticians, Journal of the American Statistical Association 112(518): 859–877.
Calderhead, B., Girolami, M. & Lawrence, N. D. (2008). Accelerating bayesian inference over nonlinear differential equations with gaussian processes, Proceedings of the
21st International Conference on Neural Information Processing Systems, NIPS’08,
Curran Associates Inc., Red Hook, NY, USA, p. 217–224.
Campbell, D. & Steele, R. J. (2012). Smooth functional tempering for nonlinear differential equation models, Statistics and Computing 22(2): 429–443.

36

Dass, S. C., Lee, J., Lee, K. & Park, J. (2017). Laplace based approximate posterior
inference for differential equation models, Statistics and Computing 27(3): 679–698.
Dondelinger, F., Husmeier, D., Rogers, S. & Filippone, M. (2013). Ode parameter inference using adaptive gradient matching with gaussian processes, in C. M. Carvalho &
P. Ravikumar (eds), Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, Vol. 31 of Proceedings of Machine Learning Research,
PMLR, Scottsdale, Arizona, USA, pp. 216–228.
URL: http://proceedings.mlr.press/v31/dondelinger13a.html
Dong, E., Du, H. & Gardner, L. (2020). An interactive web-based dashboard to track
covid-19 in real time, The Lancet Infectious Diseases 20(5): 533 – 534.
URL: http://www.sciencedirect.com/science/article/pii/S1473309920301201
Ellner, S. (2007). Commentary on “parameter estimation for differential equations: a
generalized smoothing approach”, Journal of the Royal Statistical Society: Series B
(Statistical Methodology) 69(5): 741–796.
URL:

https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-

9868.2007.00610.x
Gelman, A., Bois, F. & Jiang, J. (1996). Physiological pharmacokinetic analysis using
population modeling and informative prior distributions, Journal of the American
Statistical Association 91(436): 1400–1412.
URL: https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476708
Giordano, R. J., Broderick, T. & Jordan, M. I. (2015). Linear response methods for accurate covariance estimates from mean field variational bayes., in Neural Information
Processing Systems pp. 1441 – 1449.
Haario, H., Laine, M., Mira, A. & Saksman, E. (2006). Dram: Efficient adaptive mcmc,
Statistics and Computing 16(4): 339–354.

37

Hall, P. & Ma, Y. (2014). Quick and easy one-step parameter estimation in differential
equations, Journal of the Royal Statistical Society: Series B (Statistical Methodology)
76(4): 735–748.
Honkela, A., Raiko, T., Kuusela, M., Tornio, M. & Karhunen, J. (2010). Approximate
riemannian conjugate gradient learning for fixed-form variational bayes, J. Mach.
Learn. Res. 11: 3235–3268.
URL: http://dl.acm.org/citation.cfm?id=1756006.1953035
Huang, Y., Liu, D. & Wu, H. (2006). Hierarchical bayesian methods for estimation of
parameters in a longitudinal hiv dynamic system, Biometrics 62(2): 413–423.
URL: https://www.ncbi.nlm.nih.gov/pubmed/16918905
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S. & Saul, L. K. (1999). An introduction to
variational methods for graphical models, Machine Learning 37(2): 183–233.
Kermack, W. O. & McKendrick, A. G. (1927). A contribution to the mathematical theory
of epidemics, Proceedings of the Royal Society A 115: 700–721.
Lee, K., Lee, J. & Dass, S. C. (2018). Inference for differential equation models using
relaxation via dynamical systems, Computational Statistics & Data Analysis 127: 116
– 134.
Liang, H., Miao, H. & Wu, H. (2010). Estimation of constant and time-varying dynamic
parameters of hiv infection in a nonlinear differential equation model, The annals of
applied statistics 4(1): 460.
Liang, H. & Wu, H. (2008). Parameter estimation for differential equation models using
a framework of measurement error in regression models, Journal of the American
Statistical Association 103(484): 1570–1583.
Lorenz, E. (1995). Predictability: a problem partly solved, Seminar on Predictability, 4-8
September 1995, Vol. 1, ECMWF, ECMWF, Shinfield Park, Reading, pp. 1–18.
URL: https://www.ecmwf.int/node/10829
38

Lorenz, E. N. & Emanuel, K. A. (1998). Optimal sites for supplementary weather observations: Simulation with a small model, Journal of the Atmospheric Sciences
55(3): 399–414.
URL: https://doi.org/10.1175/1520-0469(1998)055¡0399:OSFSWO¿2.0.CO;2
Ramsay, J. O., Hooker, G., Campbell, D. & Cao, J. (2007). Parameter estimation for
differential equations: a generalized smoothing approach, J. R. Stat. Soc. Ser. B
Stat. Methodol. 69(5): 741–796.
URL: http://dx.doi.org/10.1111/j.1467-9868.2007.00610.x
Ramsay, J. & Silverman, B. (2005). Functional Data Analysis, Springer Series in Statistics,
Springer.
URL: https://books.google.co.kr/books?id=mU3dop5wY 4C
Rios, M. P. & Lopes, H. F. (2013). The Extended Liu and West Filter: Parameter Learning
in Markov Switching Stochastic Volatility Models, Springer New York, New York, NY,
pp. 23–61.
Sun, W. & Yuan, Y.-x. (2006). Line Search, Springer US, Boston, MA, pp. 71–117.
URL: https://doi.org/10.1007/0-387-24976-1 2
Swartz, J. & Bremermann, H. (1975). Discussion of parameter estimation in biological
modelling: algorithms for estimation and evaluation of the estimates, Journal of
Mathematical Biology 1(3): 241–257.
United Nations, Department of Economic and Social Affairs, Population Division (2019).
World population prospects 2019, online edition. rev. 1.
URL: https://population.un.org/wpp/Download/Standard/CSV/
Varah, J. M. (1982). A spline least squares method for numerical parameter estimation
in differential equations, SIAM J. Sci. Statist. Comput. 3(1): 28–46.
URL: http://dx.doi.org/10.1137/0903003

39

Wang, Y. & Barber, D. (2014). Gaussian processes for bayesian estimation in ordinary differential equations, Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32, ICML’14, JMLR.org,
p. II–1485–II–1493.

40

