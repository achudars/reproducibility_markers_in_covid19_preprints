arXiv:2007.13847v2 [stat.AP] 19 Oct 2020

A Bayesian Hierarchical Network for Combining
Heterogeneous Data Sources in Medical Diagnoses

Claire Donnat
Department of Statistics
Stanford University
cdonnat@stanford.edu

Nina Miolane
Department of Statistics
Stanford University
nmiolane@stanford.edu

Jack Kreindler
Centre for Health and Human Performance
drjack@chhp.com

Frederick de Saint Pierre Bunbury
Carnegie Institution for Science
fbunbury@carnegiescience.edu

Abstract
Computer-Aided Diagnosis has shown stellar performance in providing accurate
medical diagnoses across multiple testing modalities (medical images, electrophysiological signals, etc.). While this field has typically focused on fully harvesting
the signal provided by a single (and generally extremely reliable) modality, fewer
efforts have utilized imprecise data lacking reliable ground truth labels. In this
unsupervised, noisy setting, the robustification and quantification of the diagnosis
uncertainty become paramount, thus posing a new challenge: how can we combine
multiple sources of information – often themselves with vastly varying levels of
precision and uncertainty – to provide a diagnosis estimate with confidence bounds?
Motivated by a concrete application in antibody testing, we devise a Stochastic
Expectation-Maximization algorithm that allows the principled integration of heterogeneous, and potentially unreliable, data types. Our Bayesian formalism is
essential in (a) flexibly combining these heterogeneous data sources and their corresponding levels of uncertainty, (b) quantifying the degree of confidence associated
with a given diagnostic, and (c) dealing with the missing values that typically
plague medical data. We quantify the potential of this approach on simulated data,
and showcase its practicality by deploying it on a real COVID-19 immunity study.

1

Introduction and Related Work

Current medical diagnoses are most often based on the combination of several data inputs by medical
experts, typically including (i) clinical history, interviews, and physical exams, (ii) laboratory tests,
(iii) electrophysiological signals, and medical images. Advances in the machine learning (ML)
community have highlighted the potential of ML to contribute to the field of Computer-Aided
Diagnosis (CAD), for which we distinguish two main classes of methods.
Single modality analysis. Most recent efforts in the ML community have focused on analyzing a
single medical data source — called a “modality". For instance, the parsing of electronic health
records (EHR) for clinical history, patients’ interviews, and physical exams has shown huge potential
for the diagnosis of a broad range of diseases, from coronary artery disease to rheumatoid arthritis
[1–5]. ML algorithms have been developed to process various types of laboratory results, from urine
steroid profiles to gene expression data [6–9]. Others have shown success in automatically processing
and classifying medical signals such as electrocardiograms (ECG) [10–13], or electroencephalograms
(EEG) [14]. Finally, a large body of work has focused on medical imaging analysis encompassing a
vast number of tasks, such as automatic extraction of diagnostic features [15, 16], segmentation of
anatomical structures [17–22], or direct diagnosis through image classification [23, 24].
Preprint. Under review.

While these works achieve record-breaking diagnostic accuracy, they often rely on supervised
learning approaches – requiring the diagnosis ground-truth to be available during training – and
on the acquisition of large datasets. Furthermore, they focus solely on a single type of data input
(EEGs, scans, etc.) – often acquired by clinicians using specialized, highly accurate equipment
– and do not harvest the potentially rich and complementary sources of information provided by
alternative medical modalities. Yet, with the development of at-home diagnostic tests (lateral flow
assays, questionnaire data for disease screening, mobile health apps, etc.), this paradigm shifts, and
diagnoses have to be established through the combination of multiple sources of cheaper, yet often
noisier and more imprecise data. In light of the uncertainty exhibited by these various inputs, it also
becomes indispensable to pair the provision of a diagnosis with a notion of a confidence interval,
thereby thoroughly characterizing our state of knowledge given the available data.
Multiple modality integration. Interestingly, diagnosis studies fusing different data sources seemed
to be more prominent in the first years of CAD. Bayesian networks [25] – also called belief networks
– have been used as a crucial decision tool for automatic diagnosis. Such networks provide a
biologically-grounded and interpretable statistical framework that integrates the probabilities of
contracting the disease given a patient’s clinical history, and the probability of developing symptoms
or observing positive test results in the presence or absence of the disease. The advantages of
these Bayesian networks are that they are (a) fully unsupervised and do not require ground-truth
information, and (b) able to provide meaningful results even in low sample regimes.
Bayesian networks have thus been implemented in a variety of contexts to integrate clinical data
and laboratory results, and diagnose conditions ranging from pyloric stenosis to acute appendicitis
[26–30]. They have also been used to combine clinical data with medical images, and subsequently
applied to assess venous thromboembolism [31], community-acquired pneumonia [32], head injury
prognostics [33], or to predict tumors [34–36]. As early as 1994, full integration of clinical data,
laboratory results and imaging features was performed to diagnose gallbladder disease [37].
However, contrary to our proposed setting, the uncertainty that these Bayesian networks integrate is
typically known and controlled. The medical conditions that they study are well characterized by
a set of specific questions and physiological exams (e.g. projectile vomiting, potassium levels and
ultrasounds in the case of Pyloric stenosis). Not only do these inputs provide a very strong signal for
the diagnosis, but the uncertainty arising from the different modalities can often be reliably informed
by a test manufacturer, extensive medical research or prior ML studies (for diagnostic inputs obtained
through classification algorithms). The uncertainty of a diagnostic method, however, is not always
well specified: whether it be (i) a physician’s assessment of the symptoms associated with a novel or
rare disease, (ii) predictions of an ML algorithm whose accuracy has not been fully characterized
outside of a curated research environment or reference datasets (such as MNIST, CIFAR), or simply
(iii) biological tests whose sensitivity still has to be determined.
Multiple noisy modality integration: a COVID-19 case study. To motivate this paper and further
understand the limitations of these previous approaches, let us consider a particular use case: COVID19 antibody testing. Lateral flow immunoassay (LFA) antibody tests for COVID-19 are one of the
manageable, affordable and easily deployable options to allow at-home testing for large population
and provide assessments of our immunity. Yet, studies have shown that the sensitivity of these tests
remains highly variable and highly contingent on the time of testing. The successful deployment
of LFAs thus depends on their augmentation with additional data inputs, such as user-specific risk
factors and self-reported symptoms. The provision of confidence scores is essential to flag potential
false negative or positive tests (requiring re-testing or closer scrutiny) and to assess local prevalence
levels – both pivotal for researchers and policymakers in the context of a pandemic.
Contributions and outline. This applications paper is geared towards the practical integration of
noisy sources of information for CAD. Our contribution is two-fold. From a methods perspective,
we account for the variability of the inputs by devising a two-level Bayesian hierarchical model. In
contrast to existing Bayesian methods for CAD, our model is deeper, and trained using a Stochastic
Expectation Maximization (EM) algorithm [38–40]. The Stochastic EM allows to overcome the
limitations of its non-stochastic counterpart [38], that is (a) its sensitivity to the starting position,
(b) its potential slow convergence rate, and (c) its possible convergence to a saddle position instead
of a local maximum. From an applications perspective, we gear this algorithm to enhance at-home
LFA testing in the context of the COVID-19 pandemic. In particular, we wish to (a) quantify the
benefit of multimodal data integration when the diagnostics are uncertain, and (b) show how our

2

method can benefit medical experts or researchers in real life. Section 3 presents the Bayesian model
for multimodal integration and the Stochastic Expectation-Maximization algorithm that performs
principled and scalable inference. Section 5 presents extensive tests of our model on simulated
datasets. Section 6 details the results obtained on the Covid-19 dataset and the impact of our method
for affordable and reliable at-home test kits.

2

Covid-19 Dataset

By way of clarifying the challenges that our algorithm proposes to overcome, we present here the
COVID Clearance Remote Recovery & Antibody Monitoring Platform study 1 , which motivated our
approach. The purpose of this study is to track the evolution of the immunity of a cohort of adult
participants in the UK with various COVID-19 exposure risks. This paper focuses on a subset of 117
healthcare workers, for which we have both questionnaire information and LFA test results.
Home-testing Immunoassay Data. Participants are issued packs of home-testing kits with written
instructions. These kits identify Immunoglobulin M (IgM) and/or Immunoglobulin G (IgG) specific
to SARS-CoV2 in blood samples using three gold-standard methods 2 3 4 . Pictures and typical
examples of LFA test results are provided in the supplementary materials. For the LFAs, participants
self-report a positive result if IgM and/or IgG are detected by the test in addition to a positive control
band. Through the collection of this data, the COV-CLEAR study aims to address questions relating
to (a) the quantification of the robustness of the antibody response, and (b) the durability of this
response. However, while affordable, we highlight that the LFA tests suffer from vastly varying levels
of sensitivity — registering a sensitivity as low as 70% on asymptomatic individuals.
Clinical Data: Questionnaire. Additionally, participants are asked to answer a questionnaire,
ideally before knowing the result of their LFA test. The form consists of questions related to
K = 14 exhibited symptoms (fever, cough, runny nose, etc.) and M = 2 subject-specific risk factors
(household size and proportion of members with a suspected or confirmed history of COVID-19).
The empirical distributions of the symptoms and the risk factors in this cohort are provided in the
supplementary materials.

3

Multimodal Data Integration: Statistical Model

In this section, we describe our principled integration of noisy diagnostic test results, with additional
clinical data such as symptom data and subject-specific risk factors. Our approach applies to general
heterogeneous medical data where the outputs are binary. The latter could be either self-reported
answers to questionnaires, clinician-reported physiological exams, outputs of a diagnostic based on
an image, abnormalities of laboratory results, etc., making this a widespread and general setting.
Thus, while we implement and showcase the method for the particular purpose of applying it to
antibody testing, this could be relevant to any medical diagnostic with binary inputs.
Denote D the true diagnosis of an individual (healthy/sick), T the outcome of the noisy diagnostic test
(positive/negative), S the symptomaticity (symptomatic/asymptomatic), X the symptoms exhibited
and Y the subject-specific risks factors. The underlying assumption is that given a true diagnosis D,
the symptoms X and the diagnostic test outcome T are independent. In other words, the probability
of the diagnostic test being a false negative P[T = 0|D = 1] is independent of the symptoms of a
truly infected individual P[X|D = 1]. Similarly, given a diagnosis D, the test outcome T and the
exhibited symptoms X are independent of the risk factors Y . We define:
•
•
•
•

P(D = 1|Y ) = πβ (Y ) the probability of contracting the disease given risk factors Y ,
P[(T = 1|D = 1) = x the sensitivity of the diagnostic test,
P[T = 0|D = 0] = 1 − y the specificity of the diagnostic test, i.e. y = P[T = 1|D = 0],
P(S = 1|D = 0) = p0 the probability of being symptomatic when whilst not having been
infected by that specific disease (the symptoms could be due to another illness for instance),

1

COV-CLEAR, www.cov-clear.com
Home-sampled (blood) Abbott ARCHITECT® Anti-SARS-CoV-2 IgG CMIA CE Marked; PHE Approved
3
Elecsys® Anti-SARS-CoV-2 Double Antigen, CE Marked, PHE Approved
4
Biopanda Ltd Product Number: RAPG-COV-019 version 1 In-Vitro LFT IgM IgG Antibody test. CE
certified for Professional Use Only.
2

3

• P(S = 1|D = 1) = p1 the probability of having been symptomatic upon infection,
• P(Xk = 1|S = 1, D = 0) = s0k the probability of exhibiting symptom k when not infected,
• P(Xk = 1|S = 1, D = 1) = s1k the probability of exhibiting symptom k upon infection.
In the above, the diagnostic test may be any biological test (e.g. LFA antibody tests), or output of a
medical imaging classification algorithm provided by a ML research team. We have here splitted
our binary variables into two classes according to their variability: (a) the test T , for which we have
provisional estimates for the sensitivity and specificity (given by the manufacturer) but which we
are still uncertain, and (b) the symptoms X, which carry additional uncertainty in that neither of
them is extremely specific to COVID-19 nor is their prevalence amongst COVID cohorts very well
established; Values for (x, y), p0 , p1 , s0 , s1 or β may be published, but challenged by complementary
research or field experience, or may be completely unavailable. We thus need to account for these
inputs’ variability and for the relative importance of different risk factors β. We propose using a
hierarchical Bayesian network (see Fig. 9) that is consequently deeper than the ones traditionally
implemented in the CAD literature [26–30].
The uncertainty in the test sensitivity x and specificity 1−y are expressed through Beta priors on x and
y with parameters (αx , βx ) and (αy , βy ), see Fig. 9. Similarly, the uncertainty on the probabilities of
symptomaticity and of the different symptoms are expressed via Beta priors with respective parameters
(αp0 , βp0 ), (αp1 , βp1 ), {(αs0k , βs0k ), (αs1k , βs1k )}k , see Fig. 9. When published estimates exist for
x, y, p0 , p1 , s0 and s1 (e.g. as provided by the LFA test manufacturer), we match the moments of the
Beta priors with those of the published distributions. If this information is unavailable, we use the
non-informative Beta prior with parameters (0.5, 0.5). In the COVID-19 example, we implement
a non-informative prior for the probabilities of symptomaticity and symptoms, as the etiology of
the disease remains unknown. These priors are then updated during learning as we aggregate the
information from the observed and imputed variables.
Lastly, we model the probability πβ (Y ) of contracting the disease with a logistic regression on the
M risk factors Y . In the case of the COVID-19 dataset, these include county data, profession, size
of household, etc. The coefficients β = {βm }m weight the relative importance of each factor Ym ,
m = 1...M . We express our uncertainty on the possible impact of a given factor m by introducing a
Gaussian prior: β ∼ N (0, σβ2 ), see Fig. 9.

σβm

M

∆

αpd , βpd

sdk

pd

βm
Weight
of risk factor m

Probability
of symptomaticity

Probability
of symptom k
Xik

Si
Yim

M

Risk factor m

K∆

αsdk , βsdk

Di

Symptomaticity

True
diagnosis

Ti

K

n

Symptom k

Test outcome
zd

∆

Sensitivity, Specificity
αzd , βzd
Figure 1: Hierarchial bayesian network integrating the accuracy uncertainty on the data sources,
to estimate the true diagnosis D. The index k = 1...K represents symptom k, while m = 1...M
represents risk factor m. The shaded cells represent observed variables or known hyper-parameters.
The dashed lines represent switches.

4

4

Inference in the Hierarchical Bayesian Network via Stochastic EM

At the subject level, we seek to compute the posterior distribution of the true diagnosis Di , informed by
the integration of the observed variables Ti , Si , Xi , Yi . This posterior yields an estimated diagnosis,
together with a credible interval that expresses the confidence associated with our prediction. In the
case of COVID-19, this provides individual estimates of each citizen’s immunity that may inform
their social behavior as lockdown is eased.
At the global level, we wish to learn: (i) the distributions of the sensitivity x and specificity y of
the diagnostic test as observed in the field; (ii) the distributions of the probabilities p0 , p1 of being
symptomatic, and s0 , s1 of exhibiting specific symptoms, within a population of interest; and (iii)
the distribution of the impact β of the risk factors for contracting the disease. In the context of the
COVID-19 pandemic, such aggregated figures are pivotal to understand the dynamic of the disease
and implement appropriate crisis policy.
Since the true diagnoses are hidden variables, the Expectation-Maximization algorithm [41] is an
appealing method to perform inference in our Bayesian model; hence to achieve the aforementioned
objectives. However, the EM algorithm requires the computation of an expectation over the posterior
of the hidden variables, which may be intractable depending on the probability distributions defined in
the model. To allow for flexibility in terms of model’s design within our multimodal data integration
framework, we offer to rely on the Stochastic EM algorithm (StEM) [42]. StEM effectively estimates
the conditional expectation in the EM using the “Stochastic Imputation Principle", i.e. approximating
the expectation by sampling once from the underlying distribution. This method allows us to carry out
inference with priors that are not necessarily the Beta distributions implemented in our experiments —
thus providing us with additional flexibility in modeling real data. Furthermore, StEM is more robust,
being less dependent on the parameters’ initialization than the EM – a definite advantage given our
very uncertain framework. Finally, StEM shows better asymptotic behavior: unlike the EM, StEM
always leads to maximization of a complete data log-likelihood in the M-step [43].
4.1

Stochastic E-step at iteration (j + 1)

Starting iteration j + 1, the current estimates of the model’s parameters are θ(j) . The stochastic
E-step computes the posterior distribution P(Di |Ti , Si , Xi , Yi , θ(j) ) of the hidden variable Di . We
ci from this posterior.
then sample a diagnosis D
Proposition 4.1. The odds of the posterior of the hidden variable Di at iteration (j + 1) writes:
P(Di = 1|Ti , Si , Xi , Yi , θ(j) )
P(Di = 0|Ti , Si , Xi , Yi , θ(j) )
(j) Si Xi

Ti

=

x(j) (1 − x(j) )(1−Ti ) × s1
T

(j) Si Xi

y (j) i (1 − y (j) )(1−Ti ) × s0

4.2

(j)

(1 − s1 )Si (1−Xi ) × π(Yi , β (j) ) × pS1 i (1 − p1 )(1−Si )

.
(j)
(1 − s0 )Si (1−Xi ) × (1 − π(Yi , β (j) )) × pS0 i (1 − p0 )(1−Si )
(1)

M-step at iteration (j + 1)

The following proposition shows the updates in the model’s parameters at iteration (j + 1), performed
in the M-step.
Proposition 4.2. The parameters updates write:
Pn
Pn
ci ) + αy − 1
ci + αx − 1
Ti D
Ti (1 − D
x(j+1) = Pn i=1
, y (j+1) = Pn i=1
c
c
i=1 Di + (αx + βx ) − 2
i=1 (1 − Di ) + (αy + βy ) − 2
Pn
P
n
ci )Si + αp − 1
ci Si + αp − 1
(1 − D
D
(j+1)
(j+1)
0
1
p0
= Pn i=1
, p1
= Pn i=1
c
c
i=1 (1 − Di ) + (αp0 + βp0 ) − 2
i=1 Di + (αp1 + βp1 ) − 2
Pn
Pn
c
c
i=1,s.t. Si =1 (1 − Di )Xi + αs0 − 1
i=1,s.t. Si =1 Di Xi + αs1 − 1
(j+1)
(j+1)
s0
= Pn
, s1
= Pn
c
c
i=1,s.t. Si =1 (1 − Di ) + (αs0 + βs0 ) − 2
i=1,s.t. Si =1 Di + (αs1 + βs1 ) − 2
β (j+1) = argminβ

n
X
ci − g(Yi β)||2
||D

2σ 2

i=1

+

||β||2
,
2σβ2
(2)

5

(B) Diagnosis Accuracy Improvement
.96

Specificity

Specificity

(A) Diagnosis Accuracy
.6
.7
.75
.8
.85
.87
.9
.93
.95
.97
.99

.92
.88
.84
.6 .7 .75 .8 .85.87 .9 .93.95.97.99

Sensitivity

.6
.7
.75
.8
.85
.87
.9
.93
.95
.97
.99

.20
.16
.12
.08
.04
.6 .7 .75 .8 .85.87 .9 .93.95.97.99

Sensitivity

Figure 2: Performance of the SEM algorithm for n=300 samples, σ = 0.5, and varying levels of
specificity and sensitivity. (A) Raw accuracy of the labels imputed via StEM. (B) Difference in
accuracy between the StEM imputed diagnostics and the observed test variable T .
where the minimization on β is performed through Newton-Raphson descent.
Algorithm Complexity. Our algorithm only relies on simple sequential updates of the distribution
parameters, as highlighted in Prop. B.1 and B.2. Denoting by L the number of such parameters, and
N the number of samples:
b T X , while those
• The updates for X and T only involve matrix multiplications of the form D
2
for β involve matrix multiplications of size O(M N ) — yielding a complexity of O(LN ).
• Prior updates rely solely on element-wise operations on the log-odds, with O(N ) complexity.
Denoting as B the maximum number of steps, the complexity is O(BLN ), and thus linear in N .
Fig 4(A) provides an example of the evolution of the number of steps as the number of samples
increases.

5

Validation on Synthetic Data

Since our approach is unsupervised, we begin by validating it on synthetic datasets where the ground
truth is known and controlled — thus allowing us to characterize the performance of the algorithm,
and to showcase the strength of combining multiple noisy modalities.
We assume the same generative model as in Fig. 9 and generate synthetic data for various pairs of
values of sensitivity and specificity ranging from 60% to 99% . In each case, we simulate N = 100
tuples of variables ((Yi , Xi , Ti )ni=1 , for n ∈ {100, 200, 500, 1000} participants. We also vary the
noise level σ ∈ {0.1, 0.5, 1.0} for the prior on D in Fig. 9. To mimic our COVID data, we simulate
K = 14 symptoms and M = 2 risk factors, for which we randomly select the parameters5 .
Improving upon the sole test T . Fig. 10 (A) and (B) show the Stochastic EM’s raw accuracy
and accuracy improvement over the sole test result T . Note that this difference is always positive,
highlighting that our method only improves upon single diagnostic inputs — even when one input
is more reliable than any of the others. As expected, the most substantial improvements upon T
are observed when the test specificity and/or sensitivity are low. For instance, for sensitivity and
specificity of 70%, our method provides an 86.6±(2.5)% accuracy for the diagnosis – or equivalently,
a 16% gain over T . We further quantify the algorithm’s performance in Table 1 of the supplementary
materials, for values of the specificity close to those observed on the field.
Benchmarking against other models. We now benchmark our algorithm against other approaches:
• Vanilla Classifier: using both the context Y and symptoms X as inputs, we fit a logistic regression to the test labels T . We choose the regularization parameter using 10-fold cross-validation,
and compute confidence intervals for the log-probability of (D|X, T ) by bootstrapping.
5

All of these parameters are provided with the code in the supplementary materials.

6

.90

Accuracy

Accuracy

.98

Sample
size
1000
500
200
100

.98

.82
0.5 0.6 0.7 0.8 0.9 1.0 1.1
(A)
Sensitivity

Sigma
0.1
0.5
1.0

.90

.82
0.5 0.6 0.7 0.8 0.9 1.0 1.1
(B)
Sensitivity

400

1.0
Accuracy

Convergence steps

Figure 3: Performance of the Stochastic EM for a fixed specificity of 80%, (A) as the number of
samples increases (and σ = 0.5) and (B) as the variance increases (and n = 300).

200
0
(A)

100 200 400
Sample size

0.9
0.8
0.7

StEM
EM-1
EM-2
Vanilla

0.5 0.6 0.7 0.8 0.9 1.0 1.1
(B)
Sensitivity

Figure 4: (A) Number of EM steps until convergence; and (B) Accuracy comparison against benchmarks: Stochastic EM (StEM), EM with data-informed priors on models’ parameters, not updated
during learning (EM-2), EM with uninformative priors on models’ parameters, not updated during
learning (EM-3), Vanilla logistic regression (Log. Reg.).
• Data-Agnostic EM: we implement a deterministic version of EM, providing uninformative priors
for the parameter (thereby reflecting our absence of knowledge of the truth), which are not
updated — i.e, an "uninformed” equivalent of the belief networks found in the literature.
• Data-Informed EM: similar to the Data-Agnostic EM, but choosing the priors (which then
remain fixed) based on the empirical data.
The results are shown in Fig. 4(B), and further completed in the supplementary materials. We
highlight the superiority of our deeper and more adaptive hierarchical model, yielding improvements
(for a reasonable tuple of specificity and sensitivity of 80%) of up to 4% over the Data-Informed EM,
8% over the Data-Agnostic one, and 9% over the Vanilla classifier.
Assessing the robustness of the Stochastic EM. For a fixed specificity 80%, Figure 3 shows the
accuracy of StEM for different values of σ (A) or as the number of samples increases (B). These
axes of variation seem to yield little impact on the model’s performance, providing evidence that our
algorithm is fairly robust, especially with respect to low-sample size.
Convergence. Finally, we assess the convergence properties of our algorithm. We examine the
distribution of the relative difference between recovered coefficients and ground truth (expressed as
a percentage of the ground-truth value). The plots, provided in the supplementary materials, show
deviations that are within a few percentages of the true value of the coefficients – thus highlighting
the ability of the model to converge to the ground-truth parameters, and making it relevant from a
medical perspective to characterize the disease’s etiology. Illustrations of the behavior of the number
of required EM steps are also provided in the supplementary materials.

6

Results on Real Data

We turn to the real dataset of n = 117 participants described in Section 2. The purpose of this
section is to show how our algorithm can be practically applied to process heterogeneous data types
and inform participants and researchers alike. At the individual level, we provide each user with
(a) the confirmation of the diagnostic, and (b) confidence intervals associated with the uncertainty
7

0

Diagnosis given
questionnaire

Diagnosis given
questionnaire and test

Diagnosis given
questionnaire

1

0

0

0.75

(A) Subject 111: Negative
questionnaire confirms negative test

1

Diagnosis given
questionnaire and test

0

(B) Subject 108: Positive
questionnaire infirms negative test

1

Figure 5: Posterior diagnosis distributions on two selected subjects. In each panel (A-B): the left plot
represents the posterior of the diagnosis, given the symptoms and risk factors data reported in the
questionnaire, while the right plot is the posterior of the diagnosis, given the symptoms, risk factors,
and reported result of the diagnostic test. The red dot represents the expectation.

1

Posteriors
Priors
1 - Spe. Sens. 1 - Spe. Sens.

0.6

0.9

D=0

D=1

(A)

Asymptomatic

0.1
(B)

D=0

D=1

0.3

0.5

0.4

0.6

Fever

0.0
(C)

Cough with sputum

Figure 6: (A) Posteriors of sensitivity and specificity compared to their priors, for asymptomatic
subjects. (B-C): Posteriors of the probability of exhibiting specific symptoms, for symptomatic
subjects with estimated negative (D = 0) or positive (D = 1) diagnosis.
to shed more light on the uncertainty associated with the diagnostic. At the global level, we
provide policymakers with (c) aggregated analysis of the herd immunity, and associated measures of
uncertainty.
At the subject-level. We present two examples, where our algorithm either confirms or infirms the
result of the test – thereby allowing for the potential flagging of false negatives. The first example
(Fig. 5 A) is a user that registers a negative test while being asymptomatic and with a limited number
of risk factors. In this case, we expect our model to confirm the result of the test, and provide a
narrower confidence interval as per the probability of immunity – as confirmed by Fig. 5 (A). The
second example showcases an instance where the questionnaire and the test disagree. Subject 108
is a user that registers a negative test, while exhibiting a wide number of known COVID symptoms
(dry cough, shortness of breath, fever), but took the test less than 10 days after his illness. While
the posterior does not reclassify the subjects’ diagnosis, the confidence interval associated with the
prediction of immunity reflects the uncertainty associated with this case, and flags it as a potential
false negative.
At the global level. At the global level, this framework allows to perform principled inference for
the disease and the population of interest. For this cohort of n = 117 healthcare workers in the
UK, at-home tests predict 35.8% of immunity. The posterior distributions of the model’s parameters
shed light on the actual accuracy of the LFA tests on our population. Fig. 6 (A) compares the
posteriors of the sensitivity and specificity to the priors built from values reported by manufacturers,
for asymptomatic subjects. Furthermore, our results provide information regarding the most prevalent
symptoms for COVID-19. For instance, among symptomatic participants, the probability of exhibiting
fever is higher for infected subjects (see Fig. 6 (B)) while cough with sputum does not seem to be
associated with COVID-19, being probably the sign of another infection (see Fig. 6 (C)). Similar
plots on additional symptoms are provided in the supplementary materials.

7

Conclusion

This applications paper provides a statistical framework for multimodal integration of noisy diagnostic
test results, self-reported symptoms, and demographic variables. Compared to previous approaches,
8

our work is more amenable to the handling of the different inputs’ uncertainty. While we have
focused here on binary symptoms, this adaptive Bayesian framework, together with the flexibility
provided by the Stochastic EM algorithm, pave the way for the integration of other continuous-valued
inputs (index of the severity of the disease, pain, etc.) as well as interactions – an extension that
we are currently in the process of implementing. The robustness of the Stochastic EM is crucial in
ensuring the reliability of the parameters given the medical stakes. Finally, we note that the generative
model allows for principled handling of missing data, thus allowing us to make diagnoses even in the
presence of incomplete data.

References
[1] Frank E. Harrell, Kerry L. Lee, Robert M. Califf, David B. Pryor, and Robert A. Rosati.
Regression modelling strategies for improved prognostic prediction. Statistics in Medicine, 3
(2):143–152, 1984. ISSN 10970258. doi: 10.1002/sim.4780030207.
[2] Imran Kurt, Mevlut Ture, and A. Turhan Kurum. Comparing performances of logistic regression,
classification and regression tree, and neural networks for predicting coronary artery disease.
Expert Systems with Applications, 34(1):366–374, 2008. ISSN 09574174. doi: 10.1016/j.eswa.
2006.09.004.
[3] Robert J. Carroll, Anne E. Eyler, and Joshua C. Denny. Naïve Electronic Health Record
phenotype identification for Rheumatoid arthritis. AMIA ... Annual Symposium proceedings /
AMIA Symposium. AMIA Symposium, 2011:189–196, 2011. ISSN 1942597X.
[4] Julia Hippisley-Cox and Carol Coupland. Predicting risk of emergency admission to hospital
using primary care data: Derivation and validation of QAdmissions score. BMJ Open, 3(8),
2013. ISSN 20446055. doi: 10.1136/bmjopen-2013-003482.
[5] Fatemeh Rahimian, Gholamreza Salimi-Khorshidi, Amir H. Payberah, Jenny Tran, Roberto
Ayala Solares, Francesca Raimondi, Milad Nazarzadeh, Dexter Canoy, and Kazem Rahimi.
Predicting the risk of emergency admission with machine learning: Development and validation
using linked electronic health records. PLoS Medicine, 15(11):1–18, 2018. ISSN 15491676.
doi: 10.1371/journal.pmed.1002695.
[6] Lei Huang and Tong Wu. Novel neural network application for bacterial colony classification.
Theoretical Biology and Medical Modelling, 15(1):1–16, 2018. ISSN 17424682. doi: 10.1186/
s12976-018-0093-x.
[7] Edmund H. Wilkes, Gill Rumsby, and Gary M. Woodward. Using machine learning to aid the
interpretation of urine steroid profiles. Clinical Chemistry, 64(11):1586–1595, 2018. ISSN
15308561. doi: 10.1373/clinchem.2018.292201.
[8] Ferhat Demirci, Pinar Akan, Tuncay Kume, Ali Riza Sisman, Zubeyde Erbayraktar, and
Suleyman Sevinc. Artificial neural network approach in laboratory test reporting: Learning
algorithms. American Journal of Clinical Pathology, 146(2):227–237, 2016. ISSN 19437722.
doi: 10.1093/ajcp/aqw104.
[9] Rupesh Agrahari, Amir Foroushani, T. Roderick Docking, Linda Chang, Gerben Duns,
Monika Hudoba, Aly Karsan, and Habil Zare. Applications of Bayesian network models in predicting types of hematological malignancies. Scientific Reports, 8(1):1–12, 2018.
ISSN 20452322. doi: 10.1038/s41598-018-24758-5. URL http://dx.doi.org/10.1038/
s41598-018-24758-5.
[10] U. Rajendra Acharya, Hamido Fujita, Oh Shu Lih, Yuki Hagiwara, Jen Hong Tan, and Muhammad Adam. Automated detection of arrhythmias using different intervals of tachycardia ECG
segments with convolutional neural network. Information Sciences, 405:81–90, 2017. ISSN
00200255. doi: 10.1016/j.ins.2017.04.012. URL http://dx.doi.org/10.1016/j.ins.
2017.04.012.
[11] Serkan Kiranyaz, Turker Ince, and Moncef Gabbouj. Real-Time Patient-Specific ECG Classification by 1-D Convolutional Neural Networks. IEEE Transactions on Biomedical Engineering,
63(3):664–675, 2016. ISSN 15582531. doi: 10.1109/TBME.2015.2468589.
9

[12] Ali Bahrami Rad, Trygve Eftestol, Kjersti Engan, Unai Irusta, Jan Terje Kvaloy, Jo KramerJohansen, Lars Wik, and Aggelos K. Katsaggelos. ECG-Based classification of resuscitation
cardiac rhythms for retrospective data analysis. IEEE Transactions on Biomedical Engineering,
64(10):2411–2418, 2017. ISSN 15582531. doi: 10.1109/TBME.2017.2688380.
[13] Pawe? Pławiak. Novel methodology of cardiac health recognition based on ECG signals
and evolutionary-neural system. Expert Systems with Applications, 92:334–349, 2018. ISSN
09574174. doi: 10.1016/j.eswa.2017.09.022.
[14] B. Richhariya and M. Tanveer. EEG signal classification using universum support vector
machine. Expert Systems with Applications, 106:169–182, 2018. ISSN 09574174. doi:
10.1016/j.eswa.2018.03.053. URL https://doi.org/10.1016/j.eswa.2018.03.053.
[15] Ricardo L. Thomaz, Pedro C. Carneiro, and Ana C. Patrocinio. Feature extraction using
convolutional neural network for classifying breast density in mammographic images. Medical
Imaging 2017: Computer-Aided Diagnosis, 10134:101342M, 2017. ISSN 16057422. doi:
10.1117/12.2254633.
[16] Yaniv Bar, Idit Diamant, Lior Wolf, Sivan Lieberman, Eli Konen, and Hayit Greenspan. Chest
pathology identification using deep feature selection with non-medical training. Computer
Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization, 6(3):259–
263, 2018. ISSN 21681171. doi: 10.1080/21681163.2016.1138324. URL http://dx.doi.
org/10.1080/21681163.2016.1138324.
[17] Pim Moeskops, Max A. Viergever, Adrienne M. Mendrik, Linda S. De Vries, Manon J.N.L.
Benders, and Ivana Isgum. Automatic Segmentation of MR Brain Images with a Convolutional
Neural Network. IEEE Transactions on Medical Imaging, 35(5):1252–1261, 2016. ISSN
1558254X. doi: 10.1109/TMI.2016.2548501.
[18] Wen Li, Fucang Jia, and Qingmao Hu. Automatic Segmentation of Liver Tumor in CT Images
with Deep Convolutional Neural Networks. Journal of Computer and Communications, 03(11):
146–151, 2015. ISSN 2327-5219. doi: 10.4236/jcc.2015.311023.
[19] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully Convolutional Networks for Semantic
Segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):640–651,
2017. ISSN 01628828. doi: 10.1109/TPAMI.2016.2572683.
[20] Fausto Milletari, Nassir Navab, and Seyed Ahmad Ahmadi. V-Net: Fully convolutional neural
networks for volumetric medical image segmentation. Proceedings - 2016 4th International
Conference on 3D Vision, 3DV 2016, pages 565–571, 2016. doi: 10.1109/3DV.2016.79.
[21] Jose Dolz, Christian Desrosiers, and Ismail Ben Ayed. 3D fully convolutional networks
for subcortical segmentation in MRI: A large-scale study. NeuroImage, 170(April 2017):
456–470, 2018. ISSN 10959572. doi: 10.1016/j.neuroimage.2017.04.039. URL https:
//doi.org/10.1016/j.neuroimage.2017.04.039.
[22] Patrick Ferdinand Christ, Mohamed Ezzeldin A. Elshaer, Florian Ettlinger, Sunil Tatavarty, Marc
Bickel, Patrick Bilic, Markus Rempfle, Marco Armbruster, Felix Hofmann, Melvin D?Anastasi,
Wieland H. Sommer, Seyed-Ahmad Ahmadi, and Bjoern H. Menze. Automatic Liver and
Lesion Segmentation in CT Using Cascaded Fully Convolutional Neural Networks and 3D
Conditional Random Fields. Urologic and Cutaneous Review, 23(8):435–460, 1919. ISSN
10636919. doi: 10.1007/978-3-319-46723-8.
[23] Marios Anthimopoulos, Stergios Christodoulidis, Lukas Ebner, Andreas Christe, and Stavroula
Mougiakakou. Lung Pattern Classification for Interstitial Lung Diseases Using a Deep Convolutional Neural Network. IEEE Transactions on Medical Imaging, 35(5):1207–1216, 2016. ISSN
1558254X. doi: 10.1109/TMI.2016.2535865.
[24] Yuma Miki, Chisako Muramatsu, Tatsuro Hayashi, Xiangrong Zhou, Takeshi Hara, Akitoshi
Katsumata, and Hiroshi Fujita. Classification of teeth in cone-beam CT using deep convolutional
neural network. Computers in Biology and Medicine, 80:24–29, 2017. ISSN 18790534. doi:
10.1016/j.compbiomed.2016.11.003. URL http://dx.doi.org/10.1016/j.compbiomed.
2016.11.003.
10

[25] Radford M Neal. Probabilistic Inference Using Markov Chain Monte Carlo Methods. Number September. 1993. ISBN 1095-9572 (Electronic)\r1053-8119 (Linking). doi: 10.1016/j.
neuroimage.2009.01.023.
[26] Sonia M. Alvarez, Beverly A. Poelstra, and Randall S. Burd. Evaluation of a Bayesian decision
network for diagnosing pyloric stenosis. Journal of Pediatric Surgery, 41(1):155–161, 2006.
ISSN 00223468. doi: 10.1016/j.jpedsurg.2005.10.019.
[27] Olivier Gevaert, Frank De Smet, Dirk Timmerman, Yves Moreau, and Bart De Moor. Predicting
the prognosis of breast cancer by integrating clinical and microarray data with Bayesian
networks. Bioinformatics, 22(14), 2006. ISSN 13674803. doi: 10.1093/bioinformatics/btl230.
[28] S. Sakai, K. Kobayashi, J. Nakamura, S. Toyabe, and Kohei Akazawa. Accuracy in the
diagnostic prediction of acute appendicitis based on the Bayesian network model. Methods of
Information in Medicine, 46(6):723–726, 2007. ISSN 00261270. doi: 10.3414/ME9066.
[29] J. J. González-López, M. García-Aparicio, D. Sánchez-Ponce, N. Muñoz-Sanz, N. FernandezLedo, P. Beneyto, and M. C. Westcott. Development and validation of a Bayesian network for
the differential diagnosis of anterior uveitis. Eye (Basingstoke), 30(6):865–872, 2016. ISSN
14765454. doi: 10.1038/eye.2016.64.
[30] Chaitawat Sangamuang, Peter Haddawy, Viravarn Luvira, Watcharapong Piyaphanee, Sopon
Iamsirithaworn, and Saranath Lawpoolsri. Accuracy of dengue clinical diagnosis with and
without NS1 antigen rapid test: Comparison between human and Bayesian network model
decision. PLoS Neglected Tropical Diseases, 12(6):1–14, 2018. ISSN 19352735. doi: 10.1371/
journal.pntd.0006573.
[31] JA Kline, AJ Novobilski, C Kabrhel, and DM Courtney. Derivation and Validation of a Bayesian
Network to Predict Pretest Probability of Venous Thromboembolism. Journal of Periodontology,
55(5):306–313, 1984. ISSN 0022-3492. doi: 10.1902/jop.1984.55.5.306.
[32] D. Aronsky and P. J. Haug. Diagnosing community-acquired pneumonia with a Bayesian
network. Proceedings / AMIA ... Annual Symposium. AMIA Symposium, (June 1997):632–636,
1998. ISSN 1531605X.
[33] G. C. Sakellaropoulos and G. C. Nikiforidis. Development of a Bayesian Network for the
prognosis of head injuries using graphical model selection techniques. Methods of Information
in Medicine, 38(1):37–42, 1999. ISSN 00261270. doi: 10.1055/s-0038-1634146.
[34] Xiao Hui Wang, Bin Zheng, Walter F. Good, Jill L. King, and Yuan Hsiang Chang. Computerassisted diagnosis of breast cancer using a data-driven Bayesian belief network. International Journal of Medical Informatics, 54(2):115–126, 1999. ISSN 13865056. doi:
10.1016/S1386-5056(98)00174-9.
[35] Charles E. Kahn, Linda M. Roberts, Katherine A. Shaffer, and Peter Haddawy. Construction of
a Bayesian network for mammographic diagnosis of breast cancer. Computers in Biology and
Medicine, 27(1):19–29, 1997. ISSN 00104825. doi: 10.1016/S0010-4825(96)00039-X.
[36] Sweta Sneha and Rajeev Agrawal. Towards enhanced accuracy in medical diagnostics - A
technique utilizing statistical and clinical data analysis in the context of ultrasound images.
Proceedings of the Annual Hawaii International Conference on System Sciences, pages 2408–
2415, 2013. ISSN 15301605. doi: 10.1109/HICSS.2013.566.
[37] Peter Haddawy, Charles E. Kahn, and Manonton Butarbutar. A Bayesian network model for
radiological diagnosis and procedure selection: Work-up of suspected gallbladder disease.
Medical Physics, 21(7):1185–1192, 1994. ISSN NA. doi: 10.1118/1.597400.
[38] Gilles Celeux, Didier Chauveau, and Jean Diebolt. On stochastic versions of the em algorithm.
1995.
[39] Gilles Celeux, Didier Chauveau, and Jean Diebolt. On stochastic versions of the em algorithm.
Biometrika, 88(1):281–286, 2001. ISSN 00063444. doi: 10.1093/biomet/88.1.281.
11

[40] Søren Feodor Nielsen et al. The stochastic em algorithm: estimation and asymptotic results.
Bernoulli, 6(3):457–489, 2000.
[41] Arthur Dempster, Natalie Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data
via the EM Algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39
(1):1–38, 1977.
[42] Gilles Celeux and Jean Diebolt. A random imputation principle : the stochastic EM algorithm.
Technical report, 1988.
[43] Soren Feodor Nielsen. The stochastic EM algorithm: Estimation and asymptotic results.
Bernoulli, 6(3):457–489, 2000. ISSN 13507265. doi: 10.2307/3318671.

A

COVID-19 Datasets

This section provides additional details on the COV-CLEAR dataset 6 . Figure 7 shows the sampling
distributions of selected symptoms and risk factors in the population of interest – grouped by the
result of the lateral flow immunoassay (LFA) test. Figure 8 illustrates the LFA test results as observed
by a given subject. Participants self-report a positive result if IgM and/or IgG are detected by the test.
The marker “C" stands for “Control".
(B)

Fever

Nausea

(C) Shortness of breath
20

30
Fever

10

0
1

0

T=0

Number of participants

(D)

15

0

T=1
Household size

20

Nausea
0
1

T=0

(E)
Household size
0
1
2
3

10

4
5
0

T=0

6

T=1

Short
breath
0

10

0

T=1

Number of participants

Number of participants

(A)
20

1

T=0

T=1

Symptomaticity

40
Symptomatic
0

20

0

1

T=0

T=1

Figure 7: (A-C) Distributions of selected symptoms in the COV-CLEAR dataset. (D) Distribution of
household size, a COVID-19 risk factor, in the COV-CLEAR dataset. (E) Distribution of symptomatic
subjects in the COV-CLEAR dataset. In each plot, the distributions are grouped with respect to the
result of the LFA test: T = 0 for a negative test, and T = 1 for a positive test.

Figure 8: Left: Example of test result. Right: Illustrations of a subset of possible test results.
6

www.cov-clear.com

12

B

Stochastic Expectation-Maximization

This section presents the derivation of the Stochastic Expectation-Maximization (StEM) algorithm,
specifically the proofs of Propositions 4.1 and 4.2 of the main paper. For the convenience of the
reader, we recall the Bayesian model of interest.
B.1

Bayesian generative model

Denote D the true diagnosis of an individual (healthy/sick), T the outcome of the noisy diagnostic test
(positive/negative), S the symptomaticity (symptomatic/asymptomatic), X the symptoms exhibited
and Y the subject-specific risks factors. The underlying assumption is that given a true diagnosis D,
the symptoms X and the diagnostic test outcome T are independent. In other words, the probability
of the diagnostic test being a false negative P[T = 0|D = 1] is independent of the symptoms of a
truly infected individual P[X|D = 1]. Similarly, given a diagnosis D, the test outcome T and the
exhibited symptoms X are independent of the risk factors Y . We define:
P(D = 1|Y ) = πβ (Y ) the probability of contracting the disease given risk factors Y ,
P[(T = 1|D = 1) = x the sensitivity of the diagnostic test,
P[T = 0|D = 0] = 1 − y the specificity of the diagnostic test, i.e. y = P[T = 1|D = 0],
P(S = 1|D = 0) = p0 the probability of being symptomatic when whilst not having been
infected by that specific disease (the symptoms could be due to another illness for instance),
• P(S = 1|D = 1) = p1 the probability of having been symptomatic upon infection,
• P(Xk = 1|S = 1, D = 0) = s0k the probability of exhibiting symptom k when not infected,
• P(Xk = 1|S = 1, D = 1) = s1k the probability of exhibiting symptom k upon infection.
•
•
•
•

The uncertainty in the test sensitivity and specificity is expressed by putting a prior on x and y, which
will be updated during training:
P[T = 1|D = 1] = x ∼ B(αx , βx ),
P[T = 1|D = 0] = y ∼ B(αy , βy ).

(3)

The uncertainty on the symptomaticity, as well as on the appearance of specific symptoms for infected
and healthy individuals is expressed with a prior on s0 and s1 , and updating it when we aggregate
more information:
P[S
P[S
P[X
P[X

= 1|D = 0] = p0
= 1|D = 1] = p1
= 1|D = 0] = s0
= 1|D = 1] = s1

∼ B(αp0 , βp0 ),
∼ B(αp1 , βp1 ).
∼ B(αs0 , βs0 ),
∼ B(αs1 , βs1 ).

(4)

Lastly, we model the probability for an individual to contract the disease depending on their risk
factors, by modeling the logodds:


π(Y )
log
= Y β +  where:  ∼ N (0, σ 2 ),
(5)
1 − π(Y )
where the parameters β weight the importance of the different components of the risk factors Y
(county data, profession, size of household) in contracting the disease. We express our uncertainty on
whether a given factor has an impact by putting a prior on β:
β ∼ N (0, σβ2 ).

(6)

In this model:


• ζ =
αx , βx , αy , βy , αp0 , βp0 , αp1 , βp1 , {αs0k , βs0k }k , {αs1k , βs1k }k , {σβm }m are
hyper-parameters, considered fixed and known,


• θ = x, y, p0 , p1 , {s0k , s1k }k , {βm }m are the parameters,
13

• D is a hidden random variable,
• Y, S, X, T are the observed variables.
For simplicity of notations, we write O = (S, X, T ) some of the observed variables. The Bayesian
model is represented in plate notations in Figure 9.

σβm

M

∆

αpd , βpd

sdk

pd

βm
Weight
of risk factor m

Probability
of symptomaticity

Probability
of symptom k
Xik

Si
Yim

M

Risk factor m

K∆

αsdk , βsdk

Di

Symptomaticity

True
diagnosis

Ti

K

n

Symptom k

Test outcome
zd

∆

Sensitivity, Specificity
αzd , βzd
Figure 9: Hierarchial bayesian network integrating the accuracy uncertainty on the data sources,
to estimate the true diagnosis D. The index k = 1...K represents symptom k, while m = 1...M
represents risk factor m. The shaded cells represent observed variables or known hyper-parameters.
The dashed lines represent switches.
Our objective is two-fold. At the patient level, we compute the posterior distribution of the true
diagnosis Di , informed by the integration of the observed variables Oi . This distribution gives us
an estimate of the true diagnosis, through Maximum a Posteriori, as well as a credible interval that
expresses our confidence in this diagnosis. At the global level, we learn the posterior distributions
of the parameters x, y, i.e. the sensitivity and specificity of the test - to be compared to the values
given by the providers. We also learn the posterior distributions of the parameters s0 , s1 , i.e. the
probability of symptoms with or without the disease - to be compared to the values given by the
medical specialists and the CDC. We also learn the posterior distribution of the parameter β, which
weights the impact of each risk factor for contracting the disease.
To fulfil this objective, we perform inference in the Bayesian model described in Figure 9 and in
the main paper. Since D are hidden variables, we proceed with the Expectation-Maximization (EM)
algorithm, specifically its stochastic version.
B.2

Stochastic Expectation-Maximization

We seek the Maximum a Posteriori (MAP) of the parameters θ. Given the parameters’ estimates, we
can compute the posterior distributions of the diagnosiss Di ’s. The stochastic EM algorithm allows
computing jointly the posterior distribution of the parameters and the hidden variables Di ’s, through
an iterative procedure described below.
We want to maximize the posterior distribution of the parameters θ:
P(θ|O1 , ...On , Y1 , ..., Yn ) =

P(O1 , ..., On |θ, Y1 , ..., Yn ) × P(θ)
∝ P(θ) × Πni=1 P(Oi |θ, Yi ) (7)
P(O1 , ..., On |Y1 , ..., Yn )
14

which translates into maximizing the expression:

`=
=

n
X
i=1
n
X

log P(Oi |θ, Yi ) + log P(θ)

i=1

=

n
X
i=1

≥

X

log

P(Oi , Di = di |θ, Yi ) + log P(θ)

di =0,1

P(Oi , Di = di |θ, Yi )

P(Di = di |Oi , θ̃, Yi )
+ log P(θ)
P(Di = di |Oi , θ̃, Yi )

P(Di = di |Oi , θ̃, Yi ) log

P(Oi , Di = di |θ, Yi )
+ log P(θ)
P(Di = di |Oi , θ̃, Yi )

X

log

di =0,1

n
X
X
i=1 di =0,1

=

(8)

n
X

h
P(Oi , Di = di |θ, Yi ) i
EDi |Oi ,θ̃,Yi log
+ log P(θ)
P(Di = di |Oi , θ̃, Yi )
i=1

In the above computations, the lower bound is obtained using Jensen inequality. It represents a
tangent lower bound of the posterior distribution: θ → `(θ) at the given set of parameters θ̃. On
the last line, the expectation is taken for Di distributed according to P(Di |Oi , θ̃, Yi ). Following
the literature on the stochastic EM algorithm, we compute this expectation by replacing it with its
ci according to P(Di |Oi , θ̃, Yi ). For simplicity of notation, we
Monte-Carlo estimate, sampling one D
denote:

wi = P(Di = di |Oi , θ̃, Yi ).

(9)

The approximate lower bound of the posterior of the parameters writes, after sampling:

`≥

n
X

log

i=1

ci |θ, Yi )
P(Oi , Di = D
+ log P(θ),
wi

(10)

and we only need to maximize the following function M in its parameters θ:

θ → M(θ) =

n
X

ci |θ, Yi ) + log P(θ).
log P(Oi , Di = D

i=1

15

(11)

ci is now fixed, we further decompose the right-hand side of the above inequality.
Since Di = D
n
X
ci |θ, Yi ) + log P(θ)
M(θ) =
log P(Ti , Si , Xi , Di = D
i=1

=

n
X

ci , θ, Yi )
log P(Ti , Si , Xi |Di = D

i=1
n
X

+
=

ci , θ, Yi ) + log P(Si , Xi |Di = D
ci , θ, Yi )
log P(Ti |Di = D

i=1
n
X

+
=

ci |θ, Yi ) + log P(θ)
log P(Di = D

i=1
n
X

ci |θ, Yi ) + log P(θ)
log P(Di = D

i=1
n
X

ci , θ, Yi ) + log P(Si , Xi |Di = D
ci , θ, Yi )
log P(Ti |Di = D

i=1
n
X

+

ci |θ, Yi ) + log P(θ)
log P(Di = D

i=1

=

n
X

ci , θ, Yi ) + log P(Xi |Si , Di = D
ci , θ, Yi ) + log P(Si |Di = D
ci , θ, Yi )
log P(Ti |Di = D

i=1
n
X

+

ci |θ, Yi ) + log P(θ)
log P(Di = D

i=1

(12)
using the conditional independence of (Si , Xi ) and Ti given Di . Using the dependency structure of
the variables relying on the Bayesian network from Figure 9, we get:
n
X
ci , x, y)
M(θ) =
log P(Ti |Di = D
i=1
n
X

+
+

i=1
n
X

ci , s0 , s1 ) + log P(Si |Di = D
ci , p0 , p1 )
log P(Xi |Si , Di = D

(13)

ci |β, Yi )
log P(Di = D

i=1

+ log P(x, y) + log P(p0 , p1 ) + log P(s0 , s1 ) + log P(β)
As a result, we find the maximum a posteriori of x, y, s0 , s1 and β separately, by maximizing
respectively:
n
X
ci , x, y) + log P(x, y),
M (x, y) =
log P(Ti |Di = D
i=1

M (p0 , p1 ) =
M (s0 , s1 ) =
M (β) =

n
X
i=1
n
X
i=1
n
X

ci , p0 , p1 ) + log P(p0 , p1 )
log P(Si |Di = D
(14)
ci , s0 , s1 ) + log P(s0 , s1 ),
log P(Xi |Si , Di = D
ci |β, Yi ) + log P(β).
log P(Di = D

i=1

To summarize, at iteration (j + 1) of the stochastic EM algorithm:
16

• Stochastic E-step: For each patient i:
– Compute the posterior distribution of their diagnosis P(Di |Oi , θ̃, Yi ),
ci from the posterior.
– Sample D
• M-step: Maximize the approximate lower-bound of the parameters’ posteriors in θ =
(x, y, p0 , p1 , s0 , s1 , β) by maximizing M (x, y), M (p0 , p1 ), M (s0 , s1 ) and M (β). This updates the parameters to:
(j+1)

θ(j+1) = (x(j+1) , y (j+1) , p0

(j+1)

, p1

(j+1)

, s0

(j+1)

, s1

, β (j+1) ).

(15)

until convergence in θ = (x, y, p0 , p1 , s0 , s1 , β). At each iteration, this algorithm maximizes a
(stochastic approximation) of a tangent lower-bound of the posterior distribution of the parameters.
Therefore, each iteration increases the posterior distribution of the parameters.
B.3

Auxiliary computation: joint log-likelihood

As an auxiliary computation, we provide the formula for the joint log-likelihood under our model,
which we will use in the E- and M-steps.
The joint log-likelihood writes:
P(O, D, θ|Y ) = P(T, S, X, D, x, y, p0 , p1 , s0 , s1 , β|Y )
= P(T, S, X, x, y, p0 , p1 , s0 , s1 |D, β, Y ) × P(D, β|Y )
= P(T, S, X, x, y, p0 , p1 , s0 , s1 |D) × P(D|Y, β) × P(β|Y )

(16)

Using the conditional independence of (S, X) and T given D, we write:
P(O, D, θ|Y )
= P(T, x, y|D) × P(S, X, p0 , p1 , s0 , s1 |D) × P(D|Y, β) × P(β|Y )
= P(T, x, y|D) × P(X, s0 , s1 |S, p0 , p1 , D) × P(S, p0 , p1 |D) × P(D|Y, β) × P(β|Y )
= P(T, x, y|D) × P(X, s0 , s1 |S, D) × P(S, p0 , p1 |D) × P(D|Y, β) × P(β|Y )
B.3.1

(17)

Auxiliary computations using the generative model

We separately compute the probabilities in the above formula, using the probability distributions
provided by the generative model. We get, for the term involving the immunoassay test T :
P(T, x, y|D) = P(T |D, x, y) × P(x, y|D)
= P(T |D, x, y) × P(x) × P(y)

(18)

= xT D (1 − x)(1−T )D y T (1−D) (1 − y)(1−T )(1−D)
× B(αx , βx )−1 xαx −1 (1 − x)βx −1 × B(αy , βy )−1 y αy −1 (1 − y)βy −1 .
For the term involving the symptoms X, we get:
P(X, s0 , s1 |S, D) = P(X|S, D, s0 , s1 ) × P(s0 , s1 |D)
= P(X|S, D, s0 , s1 ) × P(s0 ) × P(s1 )
S(1−D)X

= 0(1−S)X 1(1−S)(1−X) s0
α

× B(αs0 , βs0 )−1 s0 s0
= δS=0,X=0
(1−D)X

+ δS=1 × s0

−1

α

(1 − s0 )βs0 −1 × B(αs1 , βs1 )−1 s1 s1

−1

(1 − s1 )βs1 −1

D(1−X)
(1 − s0 )(1−D)(1−X) sDX
1 (1 − s1 )
α

× B(αs0 , βs0 )−1 s0 s0
×

(1 − s0 )S(1−D)(1−X) sSDX
(1 − s1 )SD(1−X)
1

−1

(1 − s0 )βs0 −1

α −1
B(αs1 , βs1 )−1 s1 s1 (1

− s1 )βs1 −1
(19)

17

For the term involving the symptomatic variable S, we get:
P(S, p0 , p1 |D) = P(S|p0 , p1 , D) × P(p0 ) × P(p1 )
(1−D)S

= p0

D(1−S)
(1 − p0 )(1−D)(1−S) × pDS
1 (1 − p1 )
α

−1

(1 − p0 )βp0 −1

α

−1

(1 − p1 )βp1 −1 .

× B(αp0 , βp0 )−1 p0 p0
× B(αp1 , βp1 )−1 p1 p1

(20)

For the terms involving the diagnosis D and the parameter β of the logistic regression, we get:
P(D|Y, β) = π(Y, β)D (1 − π(Y, β))1−D
P(β) = n(β; σβ2 ),

(21)

where we use the notations:
• π(Y, β) = g(Y β) with g the sigmoid function from the logistic regression,
• n(β; σβ2 ) the probability density function of the Gaussian N (0, σβ2 ).
B.3.2

Final formula for the joint log-likelihood

We plug the expressions of the probabilities in the joint log-likelihood, and get:
P(O, D, θ|Y )
= P(T, x, y|D) × P(X, s0 , s1 |S, D) × P(S, p0 , p1 |D) × P(D|Y, β) × P(β|Y )
∝ xT D (1 − x)(1−T )D y T (1−D) (1 − y)(1−T )(1−D) xαx −1 (1 − x)βx −1 y αy −1 (1 − y)βy −1
S(1−D)X

× 0(1−S)X 1(1−S)(1−X) s0
α

× s0 s0
(1−D)S

× p0

−1

(1 − s0 )S(1−D)(1−X) sSDX
(1 − s1 )SD(1−X)
1

α

(1 − s0 )βs0 −1 s1 s1

−1

(1 − s1 )βs1 −1

(22)

D(1−S)
(1 − p0 )(1−D)(1−S) × pDS
1 (1 − p1 )
α

× p 0 p0

−1

α

(1 − p0 )βp0 −1 p1 p1

−1

(1 − p1 )βp1 −1

× π(Y, β)D (1 − π(Y, β))1−D × n(β; σβ2 ),
where we omit the normalizing constants from the beta distributions.
B.4

Stochastic E-step: Compute posterior of the hidden variables Di

ci from the posterior distribution of the hidden variable
In the stochastic E-step, we aim to sample D
(j)
Di . Given the current estimate θ of the parameters, we compute the posterior of the diagnosis Di
for each patient i:
P(Di |Oi , θ(j) , Yi ) ∝ P(Di , Oi , θ(j) |Yi ),

(23)

where we plug the expression of the joint log-likelihood from Equation 51.
Omitting the coefficients that are shared in both formulae for the probability below, we have:
P(Di = 1|Oi , θ(j) , Yi )
Ti

(j) Si Xi

∝ x(j) (1 − x(j) )(1−Ti ) × s1

(j)

(1 − s1 )Si (1−Xi ) × π(Yi , β (j) ) × pS1 i (1 − p1 )(1−Si )

P(Di = 0|Oi , θ(j) , Yi )
Ti

(j) Si Xi

∝ y (j) (1 − y (j) )(1−Ti ) × s0

(j)

(1 − s0 )Si (1−Xi ) × (1 − π(Yi , β (j) )) × pS0 (1 − p0 )(1−Si ) ,
(24)

by identification in the expression of the joint log-likehood from Equation 51.
This shows the proposition:
18

Proposition B.1. The odds of the posterior of the hidden variable Di at iteration (j + 1) writes:
P(Di = 1|Ti , Si , Xi , Yi , θ(j) )
P(Di = 0|Ti , Si , Xi , Yi , θ(j) )
(j) Si Xi

Ti

x(j) (1 − x(j) )(1−Ti ) × s1

=

(j) Si Xi

T

y (j) i (1 − y (j) )(1−Ti ) × s0

(j)

(1 − s1 )Si (1−Xi ) × π(Yi , β (j) ) × pS1 i (1 − p1 )(1−Si )

.
(j)
(1 − s0 )Si (1−Xi ) × (1 − π(Yi , β (j) )) × pS0 i (1 − p0 )(1−Si )
(25)

Following the methodology of the stochast EM algorithm, we sample from this posterior to obtain
ci .
D
B.5

M-step: Update model’s parameters

In the M-step, we update the parameters of the generative model by maximizing the lower-bound of
their posterior distribution. We update each set of parameters separetely, using the expressions in
Equation 49.
B.5.1

Update Sensitivity and Specificity

ci ’s, we update x, y by maximizing:
Given D
M (x, y) =

n
X

ci , x, y) + log P(x, y).
log P(Ti |Di = D

(26)

i=1

We recognize in this expression the logarithm of the probability distribution of a product of beta
distributions in x and in y, omitting the normalization constants:


ci
c
c
c
ci , x, y) = Πni=1 xTi D
Πni=1 P(Ti |Di = D
(1 − x)(1−Ti )Di × y Ti (1−Di ) (1 − y)(1−Ti )(1−Di )
P(x, y) = xαx −1 (1 − x)βx −1 × y αy −1 (1 − y)βy −1 ,
(27)
such that M (x, y) writes:
n
n
h X
i
X
ci + αx ,
ci + βx
M (x, y) = log Bx
Ti D
(1 − Ti )D
i=1

h

+ log By

n
X

i=1

ci ) + αy ,
Ti (1 − D

i=1

n
X

(28)
ci ) + βy
(1 − Ti )(1 − D

i
.

i=1

The updated sensitivity and specificity, x(j+1) , y (j+1) , maximize M (x, y). They are the modes of
the beta distributions:
x(j+1) = argmax Bx
x

y

(j+1)

= argmax By
y

n
X

ci + αx ,
Ti D

i=1
n
X

n
X

ci + βx
(1 − Ti )D



i=1

ci ) + αy ,
Ti (1 − D

i=1

n

X
ci ) + βy .
(1 − Ti )(1 − D
i=1

19

(29)

If αx , βx > 1 and αy , βy > 1, the expression for the modes are:
Pn
c
(j+1)
i=1 Ti Di + αx − 1
x
= Pn
P
n
c
c
i=1 (1 − Ti )Di + βx − 2
i=1 Ti Di + αx +
Pn
ci + αx − 1
Ti D
= Pn i=1
c
i=1 Di + (αx + βx ) − 2
Pn
c
i=1 Ti (1 − Di ) + αy − 1
y (j+1) = Pn
P
n
c
c
i=1 Ti (1 − Di ) + αy +
i=1 (1 − Ti )(1 − Di ) + βy − 2
Pn
ci ) + αy − 1
Ti (1 − D
= Pn i=1
.
c
i=1 (1 − Di ) + (αy + βy ) − 2

(30)

We can have the case βx , βy < 1 if the sensitivity or the specificity are very close to 1. In this case,
we update the parameters using the expectation of the Beta distribution, instead of the mode.
B.5.2

Update Probabilities of being symptomatic

ci ’s, we update p0 , p1 by maximizing:
Given the D
n
X
ci , p0 , p1 ) + log P(p0 , p1 ).
M (p0 , s1 ) =
log P(Si |Di = D

(31)

i=1

We recognize in this expression the logarithm of the probability distribution of a product of beta
distributions in p0 and in p1 , omitting the normalization constants :


ci )Si
ci Si
c
c
ci , p0 , p1 ) = Πn p(1−D
Πni=1 P(Si |Di = D
(1 − p0 )(1−Di )(1−Si ) × pD
(1 − p1 )Di (1−Si )
i=1
0
1
α

P(p0 , p1 ) = p0 p0

−1

α

(1 − p0 )βp0 −1 × p1 p1

−1

(1 − p1 )βp1 −1
(32)

such that M (p0 , p1 ) writes:
n
n
h
X
i
X
c
ci )(1 − Si ) + βp
M (p0 , p1 ) = log Bp0
(1 − Di )Si + αp0 ,
(1 − D
0
i=1

h

+ log Bp1

n
X

i=1

ci Si + αp ,
D
1

i=1

n
X

ci (1 − Si ) + βp
D
1

(33)
i
.

i=1
(j+1)

The updated probabilities of being symptomatic, in absence or presence of the disease, p0
maximize M (p0 , p1 ). They are the modes of the beta distributions:
n
n
X

X
(j+1)
ci )Si + αp ,
ci )(1 − Si ) + βp
p0
= argmax Bx
(1 − D
(1
−
D
0
0
x

(j+1)
p1

= argmax By
y

i=1
n
X

i=1

ci Si + αp ,
D
1

i=1

n
X

(j+1)

, p1

,

(34)


ci (1 − Si ) + βp .
D
1

i=1

If αp0 , βp0 > 1 and αp1 , βp1 > 1, the expression for the modes are:
Pn
c
i=1 (1 − Di )Si + αp0 − 1
p0 (j+1) = Pn
P
n
c
c
i=1 (1 − Di )(1 − Si ) + βp0 − 2
i=1 (1 − Di )Si + αp0 +
Pn
ci )Si + αp − 1
(1 − D
0
= Pn i=1
c
i=1 (1 − Di ) + (αp0 + βp0 ) − 2
Pn c
(j+1)
i=1 Di Si + αp1 − 1
p1
= Pn
Pn c
c
i=1 Di Si + αp1 +
i=1 Di (1 − Si ) + βp1 − 2
Pn c
Di Si + αp1 − 1
= Pn i=1
.
c
i=1 Di + (αp1 + βp1 ) − 2
20

(35)

We can have the case βp0 , βp1 < 1 if the sensitivity or the specificity are very close to 1. In this case,
we update the parameters using the expectation of the Beta distribution, instead of the mode.
B.5.3

Update Symptoms Probabilities

ci ’s, we update s0 , s1 by maximizing:
Given D
M (s0 , s1 ) =

n
X

ci , s0 , s1 ) + log P(s0 , s1 ), .
log P(Xi |Si , Di = D

(36)

i=1

We recognize in this expression the logarithm of the probability distribution of a product of beta
distributions in s0 and in s1 , omitting the normalization constants:
ci , s0 , s1 )
Πni=1 P(Xi |Si , Di = D


ci )Xi
ci Xi
c
c
(1−D
= Πni=1 δSi =0,Xi =0 + δSi =1 × s0
(1 − s0 )(1−Di )(1−Xi ) sD
(1 − s1 )Di (1−Xi )
(37)
1


ci )Xi
ci Xi
c
c
(1−D
= Πni=1,s.t. Si =1 s0
(1 − s0 )(1−Di )(1−Xi ) sD
(1 − s1 )Di (1−Xi )
1
and:
α

P(s0 , s1 ) = s0 s0

−1

α

(1 − s0 )βs0 −1 × s1 s1

−1

(1 − s1 )βs1 −1

(38)

such that M (s0 , s1 ) writes:
h



n
X

h



i=1,s.t. Si =1
n
X

M (s0 , s1 ) = log Bs0
+ log Bs1

n
X

ci )Xi + αs ,
(1 − D
0

ci )(1 − Xi ) + βs
(1 − D
0

i

i=1,s.t. Si =1
n
X

ci Xi + αs ,
D
1

i=1,s.t. Si =1

ci (1 − Xi ) + βs
D
1

i
.

i=1,s.t. Si =1

(39)
The updated probabilities of exhibiting symptoms, in absence or presence of the disease,
(j+1) (j+1)
s0
, s1
, maximize M (s0 , s1 ). They are the modes of the beta distributions:
(j+1)

s0

(j+1)
s1

= argmax Bx



n
X



i=1,s.t. Si =1
n
X

x

= argmax By
y

n
X

ci )Xi + αs ,
(1 − D
0

ci )(1 − Xi ) + βs
(1 − D
0



i=1,s.t. Si =1
n
X

ci Xi + αs ,
D
1

i=1,s.t. Si =1

(40)


ci (1 − Xi ) + βs .
D
1

i=1,s.t. Si =1

If αs0 , βs0 > 1 and αs1 , βs1 > 1, the expression for the modes are:
Pn
c
i=1,s.t. Si =1 (1 − Di )Xi + αs0 − 1
(j+1)
s0
= Pn
P
n
c
c
i=1,s.t. Si =1 (1 − Di )Xi + αs0 +
i=1,s.t. Si =1 (1 − Di )(1 − Xi ) + βs0 − 2
Pn
c
i=1,s.t. Si =1 (1 − Di )Xi + αs0 − 1
= Pn
c
i=1,s.t. Si =1 (1 − Di ) + (αs0 + βs0 ) − 2
Pn
c
i=1,s.t. Si =1 Di X + αs1 − 1
(j+1)
s1
= Pn
P
n
c
c
i=1,s.t. Si =1 Di (1 − Xi ) + βs1 − 2
i=1,s.t. Si =1 Di Xi + αs1 +
Pn
c
i=1,s.t. Si =1 Di Xi + αs1 − 1
= Pn
.
ci + (αs + βs ) − 2
D
i=1,s.t. Si =1

1

(41)

1

We can have the case βs0 , βs1 < 1 if the probabilities of symptoms are very close to 1. In this case,
we update the parameters using the expectation of the Beta distribution, instead of the mode.
21

B.5.4

Update coefficients of the risk factors β

ci ’s, we update β by maximizing:
Given the D
M (β) =

n
X

ci |β, Yi ) + log P(β)
log P(Di = D

i=1

=−

n
X
ci − g(Yi β)||2
||D

2σ 2

i=1

(42)

||β||2
−
2σβ2

where we omit the normalization constant of the Gaussian distributions. We recognize the loss
function of the logistic regression, that we optimize using stochastic gradient descent, with update
rule:
hX
2i
ci )Yi + ||β||
β ←β−γ
(g(Yi β) − D
(43)
2σβ2
i
This gives the update in β.
B.5.5

Summary of the updates

This proposition summarizes the parameters updates.
Proposition B.2. The parameters updates write:
Pn
Pn
ci ) + αy − 1
c
Ti (1 − D
(j+1)
(j+1)
i=1 Ti Di + αx − 1
x
= Pn
, y
= Pn i=1
c
c
i=1 Di + (αx + βx ) − 2
i=1 (1 − Di ) + (αy + βy ) − 2
Pn
P
n
ci )Si + αp − 1
ci Si + αp − 1
(1 − D
D
(j+1)
(j+1)
0
1
p0
= Pn i=1
, p1
= Pn i=1
c
c
D
+
(α
(1
−
D
)
+
(α
+
β
+
β
)
−
2
i
p
i
p
p1 ) − 2
p
1
0
0
i=1
i=1
P
Pn
n
c
c
i=1,s.t. Si =1 Di Xi + αs1 − 1
i=1,s.t. Si =1 (1 − Di )Xi + αs0 − 1
(j+1)
(j+1)
, s1
= Pn
s0
= Pn
c
c
i=1,s.t. Si =1 (1 − Di ) + (αs0 + βs0 ) − 2
i=1,s.t. Si =1 Di + (αs1 + βs1 ) − 2
β (j+1) = argminβ

n
X
ci − g(Yi β)||2
||D
i=1

2σ 2

+

||β||2
,
2σβ2
(44)

where the minimization on β is performed through stochastic gradient descent.

C

Stochastic EM with missing T : truncated Bayesian network

We apply the stochastic EM algorithm in the Bayesian model truncated at T . In this model:


• ζ = αp0 , βp0 , αp1 , βp1 , {αs0k , βs0k }k , {αs1k , βs1k }k , {σβm }m are hyper-parameters,
considered fixed and known,


• θ = p0 , p1 , {s0k , s1k }k , {βm }m are the parameters,
• D is a hidden random variable,
• Y, S, X are the observed variables.
We now writ: O = (S, X).
We still wish to maximize the expression:
`=

n
X

h
P(Oi , Di = di |θ, Yi ) i
EDi |Oi ,θ̃,Yi log
+ log P(θ),
P(Di = di |Oi , θ̃, Yi )
i=1
22

(45)

under our new notations stated above. The approximate lower bound of the posterior of the parameters
ci according to P(Di |Oi , θ̃, Yi ):
still writes, after sampling one D

`≥

n
X

log

i=1

ci |θ, Yi )
P(Oi , Di = D
+ log P(θ),
wi

(46)

, using the notation: wi = P(Di |Oi , θ̃, Yi ). Again, we only need to maximize the following function
M in its parameters θ:
θ → M(θ) =

n
X

ci |θ, Yi ) + log P(θ).
log P(Oi , Di = D

(47)

i=1

ci is now fixed, we further decompose the right-hand side of the above inequality.
Since Di = D
M(θ) =

n
X

ci |θ, Yi ) + log P(θ)
log P(Si , Xi , Di = D

i=1

(48)

= M (p0 , p1 ) + M (s0 , s1 ) + M (β)
where:
M (p0 , p1 ) =

n
X

ci , p0 , p1 ) + log P(p0 , p1 )
log P(Si |Di = D

i=1

M (s0 , s1 ) =
M (β) =

n
X
i=1
n
X

ci , s0 , s1 ) + log P(s0 , s1 ),
log P(Xi |Si , Di = D

(49)

ci |β, Yi ) + log P(β).
log P(Di = D

i=1

are the same functions as in the case with observed Ti . The only difference is that D̂i is sampled
from P (Di |Oi , θ̃, Yi ) where Oi = (Si , Xi ) does not contain Ti .
C.1

Auxiliary computation: log-likelihood in truncated Bayesian model

The joint log-likelihood in the truncated Bayesian model writes:
P(O, D, θ|Y ) = P(S, X, D, x, y, p0 , p1 , s0 , s1 , β|Y )
= P(S, X, x, y, p0 , p1 , s0 , s1 |D) × P(D|Y, β) × P(β|Y )
= P(X, s0 , s1 |S, D) × P(S, p0 , p1 |D) × P(D|Y, β) × P(β|Y ),

(50)

which gives the same result as in the non-truncated case, but without the term P(T, x, y|D). We use
the computations from subsection B.3.1 to get the final expression of the loglikelihood: We plug the
expressions of the probabilities in the joint log-likelihood, and get:
P(O, D, θ|Y )
S(1−D)X

∝ 0(1−S)X 1(1−S)(1−X) s0
α

× s0 s0
(1−D)S

× p0

−1

(1 − s0 )S(1−D)(1−X) sSDX
(1 − s1 )SD(1−X)
1

α

(1 − s0 )βs0 −1 s1 s1

−1

(1 − s1 )βs1 −1

D(1−S)
(1 − p0 )(1−D)(1−S) × pDS
1 (1 − p1 )
α

× p0 p0

−1

α

(1 − p0 )βp0 −1 p1 p1

−1

(1 − p1 )βp1 −1

× π(Y, β)D (1 − π(Y, β))1−D × n(β; σβ2 ),
where we omit the normalizing constants from the beta distributions.
23

(51)

C.2

Stochastic E-step in the truncated model: Compute posterior of the hidden variables Di

ci from the posterior distribution of the hidden variable
In the stochastic E-step, we aim to sample D
(j)
Di . Given the current estimate θ of the parameters, we compute the posterior of the diagnosis Di
for each patient i:
P(Di |Oi , θ(j) , Yi ) = P(Di |Si , Xi , θ(j) , Yi ) ∝ P(Di , Si , Xi , θ(j) |Yi ),

(52)

where we plug the expression of the joint log-likelihood of the truncated model.
Omitting the coefficients that are shared in both formulae for the probability below, we have:
P(Di = 1|Oi , θ(j) , Yi )
(j) Si Xi

∝ s1

(j)

(1 − s1 )Si (1−Xi ) × π(Yi , β (j) ) × pS1 i (1 − p1 )(1−Si )

P(Di = 0|Oi , θ(j) , Yi )
(j) Si Xi

∝ s0

(53)

(j)

(1 − s0 )Si (1−Xi ) × (1 − π(Yi , β (j) )) × pS0 (1 − p0 )(1−Si ) ,

by identification.

D

Stochastic EM with missing T : Missing and hidden variables

We derive the Stochastic EM algorithm in the full Bayesian model, taking into account the missing
variables Ti ’s (missing for i = r + 1...n) and hidden variables Di ’s. We want to maximize the
posterior distribution of the parameters θ:
M
P(θ|O1 , ...Or , Or+1
, ..., OnM , Y1 , ..., Yn ) =

M
P(O1 , ..., Or , Or+1
, ..., OnM |θ, Y1 , ..., Yn ) × P(θ)
M , ..., O M |Y , ..., Y )
P(O1 , ..., Or , Or+1
1
n
n

∝ Πri=1 P(Oi |θ, Yi ) × Πni=r+1 P(OiM |θ, Yi ) × P(θ)
(54)
where we write Oi = (Si , Xi , Ti ) when Ti is available and OiM = (Si , Xi ) when Ti is missing.
This translates into maximizing the expression:
`=

r
X

log P(Oi |θ, Yi ) +

log P(OiM |θ, Yi ) + log P(θ)

i=r+1

i=1
r
X

n
X

P(Oi , Di = di |θ, Yi ) i
(55)
P(Di = di |Oi , θ̃, Yi )
i=1
n
h
X
P(OiM , Di = di , Ti = ti |θ, Yi ) i
+
+ log P(θ),
EDi ,Ti |OM ,θ̃,Yi log
i
P(Di = di , Ti = ti |OiM , θ̃, Yi )
i=r+1
Pr
where we use Jensen inequality to compute the tangent lower-bounds of `D = i=1 log P(Oi |θ, Yi )
Pn
and `T,D = i=r+1 log P(OiM |θ, Yi ) independently. This is still a valid lower-bound of ` at θ̃ as the
sum of the tangent-lower bounds is a tangent-lower bound of the sum.
≥

h

EDi |Oi ,θ̃,Yi log

After sampling, we need to maximize the following function of θ:
M IS

θ→M

(θ) =

r
X

ci = di |θ, Yi ) +
log P(Oi , D

i=1

= M(θ, n = r) +

n
X

ci = di , Tbi = ti |θ, Yi ) + log P(θ)
log P(OiM , D

i=r+1
n
X

ci = di , Tbi = ti |θ, Yi )
log P(OiM , D

i=r+1

(56)
where M(θ, n = r) is the cost function in the case without missing T .
24

We compute the second term of the lower-bound, which we denote MN EW :
n
X
N EW
ci = di , Tbi = ti |θ, Yi )
M
(θ) =
log P(OiM , D
=

=

i=r+1
n
X
i=r+1
n
X
i=r+1
n
X

+

ci = di , Tbi = ti |θ, Yi )
log P(Si , Xi , D
ci , θ, Yi ) + log P(Xi |Si , Di = D
ci , θ, Yi ) + log P(Si |Di = D
ci , θ, Yi )
log P(Ti = Tbi |Di = D
ci |θ, Yi )
log P(Di = D

i=1

(57)
which is M(θ, n = (n − r + 1), Ti → Tbi ). Therefore: mathcalM M IS (θ) is M(θ) where the
missing Ti ’s have been replaced by theirs imputed value.
We proceed with the Stochastic EM algorithm as follows:
• M-step: Compute the tangent-lower bound, which amounts to:
ci from P(Di |Oi , θ̃, Yi ),
– for i = 1, ..., r: sample D
ci , Tbi from P(Di , Ti |OM , θ̃, Yi ),
– for i = r + 1, ..., n: sample D
i

• E-step: Maximize the lower-bound in θ.
D.1

M-step

ci is performed as usual.
For i = 1, ..., r, sampling D
M
P(Di , Ti |Oi , θ̃, Yi ).

ci , Tbi from
We focus on sampling D

We compute the posterior of interest:
P(Di , Ti |OiM , θ̃, Yi ) ∝ P(Di , Ti , OiM , |θ̃, Yi )

(58)

We get:
P(Di = 0, Ti = 0|OiM , θ̃, Yi )
(j) Si Xi

∝ (1 − y (j) ) × s0

(j)

(1 − s0 )Si (1−Xi ) × (1 − π(Yi , β (j) )) × pS0 (1 − p0 )(1−Si )

P(Di = 0, Ti = 1|OiM , θ̃, Yi )
(j) Si Xi

∝ y (j) × s0

(j)

(1 − s0 )Si (1−Xi ) × (1 − π(Yi , β (j) )) × pS0 (1 − p0 )(1−Si )

P(Di = 1, Ti = 0|OiM , θ̃, Yi )
(j) Si Xi

∝ (1 − x(j) ) × s1

(59)

(j)

(1 − s1 )Si (1−Xi ) × π(Yi , β (j) ) × pS1 i (1 − p1 )(1−Si )

P(Di = 1, Ti = 1|OiM , θ̃, Yi )
(j) Si Xi

∝ x(j) × s1

(j)

(1 − s1 )Si (1−Xi ) × π(Yi , β (j) ) × pS1 i (1 − p1 )(1−Si ) .

ci , Tbi ) according to the posterior, for i = r + 1, .., n.
These allow to sample (D
D.2

E-step

The update are the same, except that the missing Ti s are replaced by their imputed values.
D.3

Enhancement

In addition, we can consider that we have a different prior on the imputed Ti ’s. Therefore, we create
a new class of sensivitivy/specificity pair, designed for the imputed Ti . The priors are initialized as
non-information Beta distribution with parameters (2, 2), and updated at training time.
25

.6
.7
.75
.8
.85
.87
.9
.93
.95
.97
.99

(B) Diagnosis Accuracy Improvement
over Data-Informed EM

.2

Specificity

Specificity

(A) Diagnosis Accuracy Improvement
over Agnostic EM

.12

.04
.6 .7 .75 .8 .85.87 .9 .93.95.97.99

Sensitivity

.6
.7
.75
.8
.85
.87
.9
.93
.95
.97
.99

.1

.06

.02
.6 .7 .75 .8 .85.87 .9 .93.95.97.99

Sensitivity

Figure 10: Performance of the StEM algorithm compared to benchmark versions of the EM for
n=300 samples, σ = 0.5, and varying levels of specificity and sensitivity. (A) Gain in accuracy with
respect to the Data-Agnostic EM described in Section 5. (B) Gain in accuracy with respect to the
Data-Informed EM described in Section 5.

E

Additional Plots for Validation on Synthetic Data

This section provides additional details on the validation of the StEM algorithm on synthetic data.
Improvement upon T . Table 1 shows the improvement in the diagnosis’ accuracy provided by our
method against the sole test T , and highlights the potential strength of harvesting multiple noisy
sources of information. The values that we have chosen here for the sensitivity are reflective of the
ones that have been reported for the LFA test in the COV-CLEAR study.

Sensitivity
Real-life
Value
Asymptomatic
70
2-10 days
80
11-20 days
93
21+ days
99.0

Specificity
80
93
12.8 ±4.0 8.42 ±3.2
9.27 ±3.1 4.82±2.9
6.4 ±2.5
2.3 ±2.1
5.7 ±2.0
2.0 ±1.4

70
16.2 ±3.9
12.3 ±3.2
8.5 ±2.6
8.7 ±2.7

99
5.64 ±3.2
97.0 ±2.3
0.03 ±1.4
0.01 ±0.0

Table 1: Gain in accuracy (mean and standard deviation) when using StEM over the sole test
Benchmarks. We complete here our discussion of the improvement that our method brings upon
Computer-Aided Diagnosis (CAD) standard methods. Fig. 10 compares — for various pairs of
sensitivity and specificity. — the diagnosis accuracies of the Stochastic EM algorithm (StEM)
and two variants of the EM algorithm: one variant where the parameters’ priors are agnostic, or
uninformative; and another variant where these priors are computed from the available data, as
described in Section 5. This figure demonstrates that learning the parameters’ posterior distributions
while estimating the diagnosis yields higher prediction accuracy.
Convergence. Fig. 11 shows the distribution of the relative difference between recovered coefficients
and ground truth (as a percentage of the ground truth value), showing deviations that are within a
few percentages of the true value of the coefficients — thus highlighting the ability of the model to
converge to the ground truth parameters.
Fig. 12 shows the average number of convergence steps for different values of sensitivity, specificity
and sample sizes. Interestingly, we note that for high values of the sensitivity/specificity, the rate of
convergence is much faster. To illustrate the complexity of the algorithm, we also display in Fig. 13
the time required per iteration as a function of the number of samples.
26

.2

.8

0.0

0.0

-.6

.5

.6 .7 .75 .8 .85 .87.9 .93.95.97.99

(A)
.6

Sensitivity
Probability of
symptom 0 if healthy

-.2

0.0

.6 .7 .75 .8 .85 .87.9 .93.95.97.99 -.4 .6 .7 .75 .8 .85 .87.9 .93.95.97.99

Sensitivity
Probability of
symptom 0 if sick

(B)

.6

0.0

-.6

(C)

Sensitivity
Coefficient of
risk factor 2

0.0

.6

.7 .75 .8

(D)

.85 .87 .9 .93 .95 .97 .99

Sensitivity
Sensitivity

-.6

.6

(E)

.7 .75 .8

.85 .87 .9 .93 .95 .97 .99

Sensitivity
1 - Specificity

Convergence steps

Figure 11: Relative difference between estimated parameters and their ground truth, for n = 300,
σ = 0.5 and different values of sensitivity (simulated data).

103
102
101
100

Specificity
Specificity
1.
1.
106
5
10
104
104
103
103
.8
.8
102
102
101
101
0
.6
.6
10
100
0.6 0.8 1.0
0.7 0.8 0.9 1.0
Sensitivity
Sensitivity
Sample
size
n
=
200
(C) Sample size n = 300
(B)

Specificity
1.
105

.85

0.7 0.8 0.9 1.0
Sensitivity
(A) Sample size n = 100

.7

Figure 12: Number of steps until convergence for σ = 0.5 and different values of sensitivity,
specificity and sample sizes (simulated data).

27

0.10

time per iteration

0.08
0.06
0.04
0.02
0.00
0.02
500 0

500 1000 1500 2000 2500

number of samples

Figure 13: Time (s) per iteration, as the number of samples increases.

F

Additional Plots on Real Data

At the subject level. Fig. 14 presents four examples, where our algorithm either confirms or infirms
the result of the test — thereby allowing for the potential flagging of false negatives or negatives.
The first example (Fig. 14 A) is a user that registers a positive test, together with a significant number
of symptoms and risk factors. The second user (Fig. 14 B) registers a negative test, while being
asymptomatic and with a limited number of risk factors. In both cases, we expect our model to
confirm the result of the test, and provide a narrower confidence interval as per the probability of
immunity — as confirmed by Fig. 14.
The third and fourth examples showcase instances where the our diagnostic posterior and the test
disagree. Subject 108 is a user that registers a negative test, while exhibiting a wide number of known
COVID symptoms (dry cough, shortness of breath, fever), but took the test less than 10 days after
his illness. Similarly, subject 92 exhibited less symptoms (in particular, no shortness of breath), but
lives in a household of three, where all the other members have also fallen sick. While the posteriors
reclassify the subjects’ diagnosis in each case, the confidence interval associated with the prediction
of immunity reflect the uncertainty that is associated with these cases, and flag them as potential false
negatives.
At the global level: LFA sensitivity and specificity. The posterior distributions of the model’s
parameters shed light on the actual accuracy of the LFA tests on our population. Fig. 15 compares the
posteriors of the sensitivity and specificity to the priors built from values reported by manufacturers,
for: (A) asymptomatic subjects, (B) symptomatic subjects who took the test between 2 to 10 days
after their first symptoms, and (C) symptomatic subjects who took the test between 11 to 20 days
after their first symptoms. We observe that the StEM has updated the prior distributions of sensitivity
and specificity, the most significant update being observed for the asymptomatic subjects.
At the global level: COVID-19 symptoms in the population of interest. Furthermore, our results
provide information regarding the most prevalent symptoms for COVID-19. The posterior probability
of exhibiting specific symptoms, among symptomatic subjects with positive or negative predicted
diagnostic is shown on Fig. 16. We emphasize that these probability distributions are relevant to our
population of n = 117 healthcare workers, and may vary for studies considering another population.

28

Diagnosis given
questionnaire

Diagnosis given
questionnaire and test

Diagnosis given
questionnaire

0

0.25

0

1

1

0

Diagnosis given
questionnaire and test

1

0

(C) Subject 108: Positive
questionnaire infirms negative test

1

0

0.75

(B) Subject 111: Negative
questionnaire confirms negative test

(A) Subject 83: Positive
questionnaire confirms positive test
Diagnosis given
questionnaire

Diagnosis given
questionnaire and test

Diagnosis given
questionnaire

1

0

Diagnosis given
questionnaire and test

1

0

(D) Subject 92: Positive
questionnaire infirms negative test

1

Figure 14: Posterior diagnosis distributions on four selected subjects. In each panel (A-D): the left
plot represents the posterior of the diagnosis, given the symptoms and risk factors data reported in the
questionnaire, while the right plot is the posterior of the diagnosis, given the symptoms, risk factors,
and reported result of the diagnostic test. The red dot represents the expectation of the corresponding
distribution.

1

Posteriors
Priors
1 - Spe. Sens. 1 - Spe. Sens.

1

Posteriors
Priors
1 - Spe. Sens. 1 - Spe. Sens.

0.9
0.6

0.8

0.4

0.7

(A)

Asymptomatic

(B)

1

Posteriors
Priors
1 - Spe. Sens. 1 - Spe. Sens.

0.9
0.8
2 - 10 days

0.7
(C)

11 - 20 days

Figure 15: Posteriors of sensitivity and specificity compared to their priors, for three regimes: (A)
Asymptomatic, (B) LFA test taken 2-10 days after first symptoms, (C) LFA test taken 11-20 days
after first symptoms.

29

0.9

D=0

D=1

D=0

D=1

0.1
(A)

0.0
(B)

Fever
D=0

D=1

Cough with sputum
D=0

D=1

0.3

0.5

(E)

0.1
(C)
0.7

Shortness of breath
D=0

D=1

0.4

0.1
Fatigue

D=1

0.4

0.5

0.8

D=0
0.7

0.3

0.5

0.2
(D)

0.6

Congested nose

0.1
(F)

Dry cough

Figure 16: Posterior distributions of the probability of exhibiting selected symptoms, for a symptomatic subject. The probability of exhibiting each symptom (A-F) is plotted for symptomatic
subjects with estimated negative (D = 0) or positive (D = 1) diagnosis.

30

