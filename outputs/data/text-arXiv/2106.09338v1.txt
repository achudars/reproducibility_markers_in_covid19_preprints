Investigating Misinformation Dissemination
on Social Media in Pakistan

Danyal Haroon

21100041@lums.edu.pk

Hammad Arif

21100305@lums.edu.pk

Ahmed Abdullah Tariq

21100075@lums.edu.pk

Fareeda Nawaz

19030045@lums.edu.pk

Dr. Ihsan Ayyub Qazi

ihsan.qazi@lums.edu.pk

Dr. Maryam mustafa

Maryam_mustafa@lums.edu.pk

2

Table of Contents
1.

Introduction ............................................................................................................................. 5
1.1.

Background ..................................................................................................................... 5

1.2.

What is Fake News?........................................................................................................ 6

1.3.

Research Questions ......................................................................................................... 6

2.

Literature Review.................................................................................................................... 7

3.

Methodology ......................................................................................................................... 14

4.

5.

3.1.

Data Collection through WhatsApp .............................................................................. 14

3.2.

Data Collection through Twitter ................................................................................... 16

3.3.

Data Collection through YouTube ................................................................................ 18

Findings................................................................................................................................. 20
4.1.

Misinformation on WhatsApp (RQ1-4) ........................................................................ 20

4.2.

A Comparison of WhatsApp, Twitter and YouTube (RQ5-7) ..................................... 33

Discussion and Future Work ................................................................................................. 45

References ..................................................................................................................................... 46

3

4

Chapter 1

1. Introduction
1.1.

Background

Fake news and misinformation are considered to be one of the most significant challenges
brought about by advances in communication technologies. From mob violence to rigged
elections, fake news has been linked to many unfortunate incidents around the world. This is
why researchers are studying fake news on social media and developing novel solutions to
counter its spread.
We chose to research the spread of fake news in Pakistan because of some unfortunate incidents
that took place during the summer of 2020. These included the downplaying of the severity of
the COVID-19 pandemic, and protests by right-wing political movements in Pakistan. We
observed that fake news and misinformation contributed significantly to these events and
especially affected low-literate and low-income populations.
We chose WhatsApp as the starting point of our research because it is the most widely used
social media application in Pakistan. WhatsApp is free, there are no ads present on it, and chats
are end-to-end encrypted which means messages between users are secure and unreadable to any
third party. However, WhatsApp’s reach and features make it a primary platform for the spread
of fake news and misinformation; one of WhatsApp’s largest and most pressing issues.
In the second phase of our project, we conducted a cross-platform comparison of misinformation
on WhatsApp, Twitter and YouTube in Pakistan. To the best of our knowledge, this is the first
attempt to compare misinformation on all three platforms in Pakistan. The comparison not only
helped us understand how misinformation spreads between social media platforms, but also gave
us an insight into the similarities and differences in the behaviour of users on the different
platforms.

5

1.2.

What is Fake News?

The terms fake news, misinformation and disinformation are used interchangeably despite
having distinct connotations [1].
▪

Misinformation is information which is false, inaccurate, or misleading, and is most often
shared by users unintentionally.

▪

Disinformation is false or misleading information which is created and shared with the
intention of deceit. It is often attributed to systematic political campaigns [1].

▪

Fake news is a generic term which was popularised during the 2016 US elections [1]. It
covers a range of misleading information including, but not limited to, misinformation
and disinformation.

Our research primarily focusses on misinformation, although the terms misinformation and fake
news are both used.

1.3.

Research Questions

The first phase of our research was based on the study of messages received in public WhatsApp
groups. More information on why and how we chose public WhatsApp groups is given in the
methodology section. We attempted to answer the following questions:
▪

RQ1: What are the characteristics of messages that go viral on WhatsApp?

▪

RQ2: To what extent is misinformation prevalent in WhatsApp groups?

▪

RQ3: What techniques are used to make misinformation believable?

▪

RQ4: How do users share and respond to misinformation in WhatsApp groups?

The second phase of our research involved a cross-platform comparison between WhatsApp,
Twitter and YouTube. Here, we attempted to answer the following questions:
▪

RQ5: To what extent does misinformation spread between social media platforms?

▪

RQ6: What type of misinformation is common and different across social media
platforms?

▪

RQ7: Is there a difference in the way users share and respond to misinformation on
different social media platforms?

6

Chapter 2

2. Literature Review
With a rapid increase in the number of people using social media as a primary source of news, a
lot of research has been conducted on how misinformation or fake news is propagated on social
media. Research conducted by Pezdek et al. relied on psychological theories to explain what
motivates people to believe and spread misinformation [2]. They identify factors such as the
confirmation bias (the tendency to accept information consistent with one’s beliefs and reject
information inconsistent with one’s beliefs) and the ‘truthiness effect’ (statements accompanied
by photos are perceived to be true) [2]. Other studies showed that belief in misinformation and
fake news is sometimes used as a coping mechanism [3]. Furthermore, research showed that
individuals believe in fake news spread during times of times of uncertainty to reduce stress and
make sense of the confusing situation. [3]. This theory has found further support because of the
ongoing coronavirus pandemic, the surge in the spread of coronavirus related misinformation,
and the relationship for people to cope with the stressful global situation while confined to their
homes.
Studies have also shown a relationship between fake news and peoples’ pre-existing biases and
prejudices. These studies state that misinformation has served as a powerful tool through which
existing prejudices are amplified, giving voice and legitimacy to hate [4]. These studies also
showed the relationship between hate speech, fear speech and fake news, particularly how fake
news is used to target the existing prejudices and hatred to incite violence against a particular
entity [4] [5] [6]. For example, a series of fake WhatsApp message led to violence against
Muslims in India in 2020, leading to fifty-three deaths [6]. This was the result of the belief that it
is a civic duty to use violence or threat against suspicious outsiders and to pass on information
about (even unverified) suspicious activities [1].
Similar examples exist for the use of carefully constructed disinformation campaigns that are
“technically correct”, by taking evidence and presenting it in a manner that eliminates any other
competing claim [4]. When institutional actors are ignored or tolerated, it acts as a verification of
7

the fake news or misinformation to supporters or uninformed observers, offering a powerful
boost to polarizing views and their subsequent consequences [4] [6].
Some studies have shown that while individuals with higher levels of education are not as
affected by these factors and are able to discern most fake news from real news, lower literacy
populations are more susceptible to believe fake news [7]. This can often have a detrimental
effect on these users, as studies have shown that belief in medical misinformation or conspiracy
theories has its impact upon individual behaviour [8] [9]. Research by Waszak et al. found that
anti-vaccine conspiracy claims significantly contributed to the reduction in vaccination rates and
made people more likely to avoid health care [8].
Fake news is so diverse in its contents and mediums that there is no single definition which can
completely describe the concept. Factually incorrect information is the most blatant form of fake
news, although it is arguably also the rarest. This diversity and carefully crafted mediums of fake
news is one of the primary reasons why fake news is believed. People often have difficulty in
discerning between fake and real news due to the mass exposure and continuous streaming of
fake and real news within our daily lives [5]. This includes newspapers, social media, websites,
etc. [5].
To identify fake news, we had to look beyond the facts by analysing the context, tone, and
motives. Roozenbeek et al. have defined six key elements which are most often present in fake
news and misinformation. These are: conspiracy, polarisation, emotion, impersonation, discredit,
and trolling [10]. Though Roozenbeek et al. were focusing primarily on fake news websites and
articles, we found that these elements were just as present in WhatsApp messages.

8

Figure 1: Six elements of fake news identified by Roozenbeek et al. and used in their
online game ‘Bad News’ [10].

Fake News on WhatsApp, Twitter and YouTube
WhatsApp, due to its nature as a free, unrestricted chat application has been a prime platform
through which fake news is spread. Within Pakistan, a study showed that 75% of the population
believed that Facebook and WhatsApp collectively are responsible for most fake news that the
participants of the study encountered [5]. Furthermore, platforms such as Twitter and Facebook,
have the ability to monitor content being posted and provide content moderation tools and
mechanisms such as the suspension of users and blocking of posts [6]. WhatsApp, on the other
hand, is an end-to-end encrypted platform, where messages can be seen only by the end users.
This makes the spread of any form of harmful content much easier [6].
Twitter is also regarded as a primary medium for fake news. Twitter is used due to the cascading
effect that the platform’s network and user interface creates [11]. A cascade begins on Twitter
when a user makes an assertion about a topic in a tweet, which could include written text, photos,
or links to articles online [11]. Others then propagate said tweet by retweeting it. This makes
verification of the source of the news difficult, for if a large individual retweet it, it can be
difficult to find the point of origin due to the cascade created. For example, an individual could

9

make a false claim or rumor cascade by tweeting a story or claim with an assertion in it, and
another individual could independently start a second cascade of the same rumour (pertaining to
the same story or claim) that is completely independent of the first cascade [11]. This would
make the point of origin ambiguous [11].
YouTube, akin to other social media sites, has become a vehicle for spreading fake news,
propaganda, conspiracy theories, and radicalizing content [12]. According to some studies,
Similarly, 1 in 10 YouTube users believe that the platform is a source of fake news [5]. Some
70% of watched content on YouTube is recommended content, in which YouTube algorithms
promote videos based on several factors including optimizing for user-engagement or view-time.
Because fake news and misinformation generally feature novel and provoking content, they tend
to yield higher than average engagement. The recommendation algorithms are thus vulnerable to
sparking a reinforcing feedback loop, in which more videos related to fake news are
recommended and consumed, thus aiding in the spread of fake news [12] [13]. The role of this
platform in the spread of fake news is made crucial in the light of the increasing use of YouTube
as a primary source of information, particularly among the youth and the nearly monopolistic
position of YouTube on its market [13].

What work has been done to curb the spread of Fake News?
According to Pezdek et al., social media companies, especially Twitter, Facebook and Whatsapp,
have been working on structural-level interventions to counter the spread of misinformation [2].
These include, for instance, placing warning labels on posts which potentially contain fake news.
Unfortunately, this has not been very effective. Pezdek et al. suggest that a more effective
solution would be one which intervenes at an individual level [2].
One such solution is to develop games to train people to identify fake news. Katsaounidou et al.
developed a board game for this purpose whereas Roozenbeek et al. developed a card game
which was later converted into an online game called Bad News [10]. In the online game, players
were told to actively create fake news stories using techniques most used by fake news
propagators on social media. These techniques included: impersonation, polarization, trolling,
10

conspiracies, discrediting facts, and evoking emotions [10]. Roosenbeek et al.’s research showed
that teenagers who played the game were more sceptical of the reliability of news and were less
persuaded by fake news [10] [14].
Social media sites are also experimenting with a crowd-powered procedure to reduce the spread
of fake news and misinformation: whenever a user is exposed to a post through the feed, they can
flag the story as misinformation and, if the post receives enough flags, it is sent to a trusted third
party for fact checking. If this party identifies the story as misinformation, it is marked as fake
news [15]. This methodology is used in a limited manner by Twitter.
Additionally, studies have been conducted to find a link between a person’s level of ‘media
literacy’ and their ability to distinguish fake news [16] [17]. Jones-Jang et al. claim that in recent
times many efforts have been made to improve people's media literacy, however only
information literacy - which is defined as the intellectual framework for understanding, finding,
evaluating, and using information - made a significant difference in participants' ability to
distinguish fake news [17]. Other studies observed that participants who were either older or
more liberal were better at distinguishing fake news [16] [14]. Furthermore, researchers have
made suggestion (that have been implemented by some platforms) that social media companies
should 'nudge' their users to check accuracy of news. This will make users more mindful and less
likely to share misinformation [18].
WhatsApp has also attempted several software patches to try and limit the spread of
misinformation. The primary patch being limiting the number of users per groups to prohibit the
creation of giant hubs to spread information through the network, however, is not able to prevent
a content to reach a large portion of entire platform [19]. Similarly, another method is to place a
limit on the number of users/groups message forwarding and broadcasting (e.g., up to five
forwards) can be done, offering a delay in the message propagation [19].
There have also been legislative attempts to aid in the curbing of fake news. Public authorities
have attempted to police the media environment by themselves. However, this approach has been
criticised insofar as it entails the creation of “Ministries of Truth”, special ministries for
monitoring social media and news outlets [20]. An additional, alternative form of State

11

intervention consists of imposing penalties to entities that engage, not just in content-creation but
even mere circulation of “illegal content”. A good example of this is the German Network
Enforcement Act. Under this controversial law, social media companies have 24 hours to remove
“obviously illegal and fake” content [20]. Similarly, there have been arguments that educators
should focus on formulating a dynamic curriculum framework that integrates collaborative
verification practices with an emphasis on misinformation, to aid in helping students identify
fake news early on [21].
Comparison of Our Work with Other Work Done:
Javed et al. used public political WhatsApp groups in Pakistan and Covid-19 related Tweets to
create a dataset [22]. Groups were joined by searching for group links (chat.whatsapp.com/…)
on the internet. Akbar et al. collected fake news stories from an archive in India maintained by
Tattle Civic Technologies. Their work was not focused on any particular social media platform
but on wider discourse on social media [4]. Banaji et al. tried to understand the link between
WhatsApp and mob violence in India. Their data came from qualitative interviews of community
members across different provinces of India, religious leaders, and social media experts [1]. Our
work relies on data collection over WhatsApp, Twitter, and YouTube. It is the first study which
compares all three platforms. We try, in our study, to understand the patterns and spread in the
fake news in these platforms individually as well as understand the link in the spread of fake
news across all three platforms. We have not limited our domain to any topic such as politics or
health. We have looked at the complete domain of discourse.
In terms of the nature of fake news being shared on WhatsApp, we observed messages with
sensational and polarizing content, often containing upsetting imagery such as scenes of
accidents or pornographic material. Additionally, we also observed content that encouraged
people to act against or encouraged violence against certain individuals or entities, based on
religious or political reasoning. Similar observations were made by Banaji et al. and Akbar et al.
in India who noted that fake news often exploited people’s biases and built upon pre-existing
stereotypes, such as those against Muslims, which led to communal violence within Indian when
such fake news spread [1] [4]. Techniques such as conspiracy and polarization which were

12

identified by Roozenbeek et al. regarding fake news websites in the West are also utilized by
fake news creators spreading fake news on social media [10].
Whereas most studies focus on one social media platform, our work gives an insight into the
different natures of several social media platforms in Pakistan. Our work also examines the
response of everyday users of these platforms to articles of misinformation that is spread on
these platforms to their perceptions and reactions upon encountering such content. This was
achieved with in person interviews and group activities with random and willing participants
encountered by roaming and approaching areas where our target populations work or frequent.
This is a very different methodology to collect data, distinct from the structured workshops and
interviews done in other works with similar populations. This gives us more natural responses
from willing and interested participants.
Each platform has its own unique user base, content, and environment. We observe the
similarities and differences in type of fake news shared on WhatsApp, Twitter, and YouTube, as
well as any reoccurring patterns or trends in the spread of such content, that would make
identification of such content easier. We also look at what mediums are used for fake news on
which platforms (e.g., text, images, and voice notes) and what mediums are common across all 3
platforms. Additionally, we try to find content that has been shared across all 3 platforms, within
a certain time frame, to find its point of origin, timeline of its spread and with platform
contributed most to the spread of such content. Finally, we study the sharing behavior and
response of users on different platforms. Our work also gives us an insight into how responsive
the three social media companies are in banning or preventing the spread of fake news and how
effective these methods have been.

13

Chapter 3
3. Methodology
3.1.

Data Collection through WhatsApp

To understand the spread of viral content on WhatsApp in Pakistan, we gathered data from
public WhatsApp groups over a period of eight months (September 2020 to April 2021). These
groups were joined using invite links of the form chat.whatsapp.com/ which we found through
searches on Google and Facebook. We also used invite links shared in the groups we initially
joined.
Having joined a total of 174 public WhatsApp groups, we shortlisted 74 public groups which
were most appropriate for our study. Groups which had low levels of activity, inorganic
conversations, and excessive advertising and adult content were discarded. The seventy-four
remaining groups primarily consisted of low-literate members and the content shared included
news, entertainment, poetry, travel, medical advice, and general discussions. The number of
members per group ranged from 12 to 256 (the limit set by WhatsApp). A majority of groups had
a single admin whereas some groups had two or more admins who worked together. Most groups
had had defined rules which were posted by the admin(s) in the group description.
20,000 text messages and 6,000 media files were collected from our WhatsApp groups. Content
posted or forwarded in three or more groups out of the seventy-four groups under analysis, or
was labelled ‘forwarded many times’ by WhatsApps was classified as viral. 909 messages
(approximately 3.5% of our total messages) were classified as viral. These included 309
URLs, 208 text messages, 196 images, 187 videos, and 9 voice notes.
To better analyse the viral content, we further separated it into two types: viral messages (which
contained text messages and URLs), and viral media (which contained images, videos, and
audios). This helped us categorise text messages separately using the Pandas library in Python.
Images were compared for similarity using the Perpetual Hashing (pHash) algorithm as done by
Melo et al. in their research on WhatsApp groups in Brazil and India [19]. Video and audio files
were sorted manually.

14

From the viral content gathered from these groups, we began to find and analyse content which
falls under the domain of fake news and misinformation which includes (but is not limited to) the
following [23]:
1.
2.
3.
4.
5.
6.
7.

Fabricated content: 100% false content, designed to deceive and do harm.
Manipulated content: genuine information/imagery manipulated to deceive.
Imposter content: when genuine sources are impersonated.
False context: genuine content shared with false contextual information.
Misleading content: misleading use of information to frame an individual/issue.
False connection: when headlines/visuals/captions do not match the content.
Satire/parody: no intention to cause harm but has the potential to fool.

Apart from a couple of services like AFP Fact Check and Soch Fact Check, there are currently
few fact-checking services which verify news in Pakistan. Therefore, we classified content as
fake news by means of manual verification through credible newspapers and news channels.
Google’s reverse image search greatly helped in checking whether an image has been posted
before in a different context. All content was checked for validity, correctness, context and
sources. Content that failed to meet the requirements and met any of the definitions stated above
was classified as fake news.
89 messages containing fake news (48 videos, 30 images, 4 text messages, 5 voice notes and 2
URLs) were identified. These messages were analysed for finding the characteristics of viral fake
news, to see if there exist any common pattern in these kinds of messages that will make them
identifiable among other messages.
We analysed the content, tone, visuals, audios and purpose of these messages in order to find
their characteristics. In many cases, the chat groups where these messages were shared were
viewed to see the responses and reactions to such messages. This was done to check their
believability and effectiveness.

15

3.2.

Data Collection through Twitter

Amidst the COVID-19 pandemic, Twitter is serving as a source of news and information for
many internet users in Pakistan. Politicians, journalists, public officials and religious leaders are
among some of the people who regularly tweet news and information for the public. We wanted
to examine the type of misinformation shared on Twitter in Pakistan and compare it to what we
observed on WhatsApp. This would help reveal the similarities and differences in the behaviour
of Twitter and WhatsApp users.
Based on messages found in our WhatsApp groups, we formed themes which encompassed
the majority of the viral content being shared. Though these themes do not completely
represent the content shared on WhatsApp, they cover topics which had the highest
number of messages. Using these themes, we searched for tweets to compare the nature of
discourse for similar topics on both platforms. Our themes included:

Table 1: Themes used to Extract Tweets
Theme

Sub-themes

Corona

Conspiracy Theories

Death and Violence
Protests and Demonstrations

▪
▪
▪
▪
▪
▪
▪
▪
▪

Symptoms
Vaccine
Hospitals
Conspiracies and propaganda by countries
(mainly America, Israel and France)
Conspiracies by certain religious minorities
Deaths of celebrities and leaders
Road accidents
Aurat March
Protests by TLP

Twint - a Python tool for Twitter scraping - was used to collect a dataset of 44,958 Tweets
spanning the period September 2020 to April 2021 (the same as our WhatsApp messages). Twint
uses Twitter's search operators to scrape Tweets from specific users, or Tweets related to certain
topics, hashtags and trends. We searched for specific topics by using strings of keywords which
we gave as input. The strings were generated by us manually combining words found commonly
in the WhatsApp messages corresponding to the themes mentioned in the table above. Twint
extracted the following data from tweets:
16

1. Username
2. Tweet message
3. Hashtags
4. Time
5. City
6. Language
7. Other meta data
Similar to what we did on WhatsApp, we set a criteria to narrow down tweets which were viral.
We set the criteria that if a tweet is retweeted more than average number of retweets (=5 retweets
in our dataset), then we consider it as a viral tweet. 4,250 or 9.5% of total tweets were labelled
as viral. The viral tweets with the largest number of retweets were by journalists, government
officials, activists and religious leaders with a large following. We noted that many accounts
associated with viral tweets were suspended by Twitter within days of our initial tweet
collection.
We filtered the viral tweets further by manually reviewing the content shared in them and
separating those which contained misinformation and hate speech. Our search was limited to
tweets in English, Urdu and Roman Urdu. Fact-checking was done using the same procedure we
used for the WhatsApp messages, relying primarily on double-checking information from
multiple online news sources and using Google’s reverse image search. A subset of 51 tweets
containing misinformation was identified and used as the basis for our comparison. Although
our team has attempted to remain as unbiased as possible during the classification of Tweets especially with regard to hate speech - some readers may disagree with our classifications based
on the subjective nature of the content itself.

17

3.3.

Data Collection through YouTube

During our study of WhatsApp messages, we observed a significant link between WhatsApp and
YouTube. 15% of all URLs shared in our seventy-four WhatsApp groups were links to YouTube
videos and channels. This motivated us to further explore the similarities and differences in the
content shared on both platforms. We scraped a total of 250 YouTube videos. Out of these, 20
videos contained fake news or misinformation.

Figure 2: Links Shared in our WhatsApp Groups

To gather content from YouTube, a scraper called ScrapeStorm was used which takes a
YouTube URL as input and extracts information from it. It allows users to specify what features
of a YouTube page are to be extracted. It further allows users to save this content as a csv or
excel file. However, to utilize this scraper efficiently and to avoid spending time examining and
searching for irrelevant content on YouTube, key strings needed to be created that would be used
to find relevant URLs to be given to the scraper.

18

To generate key strings, we followed the same protocol used to for Twitter. Relevant key words
associated with shortlisted topics found in the WhatsApp viral content were extracted to form
key strings (or phrases), that were used to search for videos.

These key strings were used as input in the YouTube search bar and the scraper was used to
extract the videos return by the search. To ensure that the search results were satisfactory, from
each search result, five videos per key string were manually examined. to check for relevant
content. If the key string searches did not return at least five relevant videos, that string was
discarded. For our study, the data extracted from the YouTube search page URLs included the
video title, link, number of views, channel name, upload date, description and duration.
From each successful key string search return, an average of twenty videos were extracted. The
reason for this was that beyond fifteen videos, the relevancy of the of video to the search results
begins to drop and beyond twenty videos it drops significantly. Therefore, it was decided to
extract twenty videos as this extracted the most relevant videos including any potential outliers
that would still have relevant content.
We watched the videos and separated the ones that contained fake news or misinformation. The
videos were classified as misinformation or containing fake news after reverse searching and
examining the claims or content within the video, similar to the procedure followed for
WhatsApp and Twitter. The videos labelled as fake news/misinformation were then further
examined by having their comments extracted using an online YouTube comment scraper
(https://youtubecommentsdownloader.com). The scraper works like the video scraper by using
URLs to find the video and extract their comments.

19

Chapter 4

4. Findings
4.1.

Misinformation on WhatsApp (RQ1-4)

The WhatsApp groups we conducted our study on contained groups on a diverse range of topics
including politics, news, poetry, travel, TV dramas, herbal medicine and many more. Since these
were publicly accessible groups, most of the members did not know each other personally. The
number of members per group ranged from 12 to 256 (the limit set by WhatsApp). A majority of
groups had a single admin whereas some groups had two or more admins who worked together.
Even though these groups were open to the public, most of the groups with a larger number of
members had defined rules which were posted by the admin(s) in the group description. The
rules would be in the form of bullet points and would - in addition to other things - require
members to limit their discussion to certain topics and refrain from sharing adult videos. If a
member did not adhere to the group rules, other members would alert the group admin to take
immediate action.

RQ1: What are the characteristics of messages that go viral on WhatsApp?
Out of the 20,000 text and 6,000 media messages we received on WhatsApp, 3.5% were shared
in three or more groups. The maximum spread of a single message was in twenty-five groups.
These viral messages included the following:
▪

309 URLs

▪

208 text messages

▪

196 images

▪

187 videos

▪

9 voice notes

20

This revealed that by far the most spread messages included URLs. An examination of their
content revealed that 36% of URLs contained marketing and promotional content such as links to
online earning websites and WhatsApp group invites. 31% contained entertainment material such
as movie download links and links to YouTube videos and channels (primarily focussed on
smartphone tutorials, news analysis and recipes). 12% of the links contained adult content such
as films and 11% contained news and information usually in the form of blog posts in both
English and Urdu. The following pie chart gives a complete breakdown of the viral URLs.

Figure 3: Categorisation of viral URLs on WhatsApp based on type of content.

We did a similar categorisation of text messages as well and found that the 54.5% of viral
messages contained poems and inspirational quotes on themes such as faith, trust and life, and
15% contained jokes and entertainment. Another 15% contained marketing and promotional

21

material such as job advertisements. The following pie chart gives a complete breakdown of the
viral text messages.

Figure 4: Categorisation of viral text messages on WhatsApp based on type of content .

The viral media content was divided into videos, images and voice notes, which were the three
main types of media shared in the WhatsApp groups. PDF documents were shared but did not go
viral. The media was analysed to find common elements, characteristics and patterns that would
provide information as to why this content went viral.
The data shows that the media content that went viral was most often religious (29.7%), political
(19.8%) or health-related (11%). These three types comprise approximately 60% of the viral

22

media being shared in our groups. The following pie chart gives a complete breakdown of the
viral media.

Figure 5: Categorisation of viral media on WhatsApp based on type of content.

Images: Viral images consisted of four main types: screenshots, photographs, custom graphics
or compiled images, each having specific characteristics that made them viral. Screenshots were
mainly of tweets, Facebook posts, news reports and articles, and chats of WhatsApp. These
screenshots usually contained information about various topics and were used to share this
information in the groups. All screenshots appeared authentic i.e., there was little to no editing
done on these screenshots.

23

Videos: The majority of viral videos were less than 60 seconds long and mainly contained clips
of people and objects. Animations were limited but there was use of graphics such as clip art and
accompanying text (usually in Urdu). Many videos contained shocking and disturbing scenes
with no prior warning for the viewer. The tone of voices in the videos conveyed panic, urgency
and the immediate need for action. Emotional appeals were a common way to grab the attention
of viewers.
Voice Notes: Viral voice notes were on average 65 seconds in length. They were recorded by
primarily male but also female voices. The tone of voices conveyed a sense of immediacy and
fear. All voice notes, at the end, prompted listeners to share them with as many people as
possible.

RQ2: To what extent is misinformation prevalent in WhatsApp groups?
Out of 909 viral messages, 89 were identified as containing fake news and misinformation. This
means that 9.7% of viral messages and 0.3% of total messages contained fake news and
misinformation. Out of the 89 messages there were 48 videos, 30 images, 4 text messages, 5
voice notes and 2 URLs. These messages were analysed for finding the characteristics of viral
fake news, to see if there is any common pattern to these kinds of messages that will make them
identifiable among other messages.
It was observed that the majority of fake news on WhatsApp related to medical advice (39.1%).
26.1% of the fake news related to politics and another 26.1% to religion. The pie chart below
shows the fake news messages sorted by type of content.

24

Figure 6: Categorisation of WhatsApp fake news based on type of content .

It was noted that the spread of fake news amplified during significant social and political
events. Examples of these include the PSL cricket tournament, the death of religious leader
Khadim Hussain Rizvi, the nation-wide power breakdown, the Aurat March, and protests by the
TLP political party. In the excitement and anxiety associated with these events, people were
more likely to forward news without double-checking its content or source.

25

RQ3: What techniques are used to make misinformation believable?
We analysed the content, tone, visuals, audios and purpose of these messages in order identify
common elements used to make fake news seem believable. From the analysis, these were the
common elements, more than one of which could be present in a single message:
1. Appeals to Emotion
Appeals to emotion were a very frequent technique used in fake news and around 50% of
our 89 samples contained some form of this. According to Roozenbeek et. al., fake news
creators use “basic emotions such as fear, anger, or empathy, in order to gain attention or
frame an issue in a particular way” [10]. We observed that fake news spread in our
WhatsApp groups utilized strong language and religious sentiments to instigate fear in
the receiver. An example of this can be seen in this screenshot of a video.

The caption translates to: “Dr Afia Siddiqui’s appeal to Imran Khan from an American
jail.” The woman in the video is not Dr Afia Siddiqui.

2. Conspiracy Theories
Conspiracy Theories were another common technique used and approximately 50% of
our 89 samples utilized this technique. Emotional appeals and conspiracy theories were
frequently used together. Conspiracy theories were primarily based on the alleged ulterior
motives of particular countries (mainly France, Israel, India and the US), as well as of

26

certain religious sects within and outside of Pakistan. There were also conspiracy theories
regarding the coronavirus, its vaccine and the various treatments administered in
hospitals.

3. Political and Religious Polarization
Approximately 40% of our fake news messages contained elements of political and
religious polarization which were frequently complimented by appeals to emotion and
conspiracy theories. Due to the nature of our WhatsApp groups, most groups had
members with similar beliefs and biases. Therefore, the groups acted like echo chambers
where certain political and religious beliefs could be reinforced through fake news and
misinformation. For instance, a group supporting a particular political party received a
constant stream of messages discrediting other political parties and little debate or
questioning occured.

4. False Context
This applies particularly to images and videos. 40% of our 89 messages contained
information presented in a false context. Images and videos were often used out of
context or were shared with little to no context. Using Google’s reverse image search
helped us determine the original context of images and even videos (by taking their
screenshot and reverse-searching it). The image on the right is showing the victim of a
bomb blast in Europe but is being used in a completely different content here.

27

The caption in the image translates to: “Good News! In America, the pathetic artist that
insulted the Holy Prophet (P.B.U.H), has died and turned into a charred mess after he
got electrocuted and burned by a short circuit. The American government wants that this
news is not made public. But you are obligated, as a member of the Holy Prophet
(P.B.U.H)’s ummah, that you share this with all your friends, so that people can bear
witness to the majesty of the Holy Prophet (P.B.U.H)!”

5. False Information
Only 20% of our fake news samples contained information which was outright false.
Those messages which did, however, also relied on other elements such as impersonation
or appeals to emotion to enhance believability. False information on its own was easily
identifiable by most users. The image on the right is a false news headline.

28

Headline in the image translates to: “Prime minister Imran Khan has died of corona virus
– sources.”

6. Impersonation
Some manipulated images and videos impersonated credible news providers such as TV
channels by using their logos and templates. Based on how people responded to these
images in the groups, it was difficult for them to determine their authenticity unless the
facts given in the image or video were blatantly false.

RQ4: How do users share and respond to misinformation in WhatsApp groups?
Due to the public nature of our WhatsApp groups, organic conversations were limited. There was
a large inflow of content but little in the way of responses and reactions to it. One of the reasons
for this could be that users’ phone numbers and profile pictures are visible to the other group
members, so they are unable to maintain the anonymity which other platforms such as Twitter
offer. Unwanted direct messages are common for public WhatsApp group users, especially
women, and we received a handful of such messages during the course of our research.
Nevertheless, we focussed on groups where relatively more organic conversations were taking
place and observed how users responded to fake news, misinformation and hate speech there.
Our first observation was that most users could identify information which was outright false.
This included incorrect facts presented through text, images, audio or video. Let us take the
example of the image below which is a manipulated version of the Geo News TV headlines. The
headlines make the false claim that Afghanistan has agreed to provide financial aid to Pakistan to
fight the coronavirus pandemic. This image was shared in three groups.

29

Translation: Screenshot from a news report. The headline has been edited. Translation:
“The government of Afghanistan, to help with the corona pandemic, has announced that
they will be providing 300 million dollars in aid to Pakistan.”

In one group, a user questioned whether this news was correct. Another user replied that this was
a manipulated image, explaining that he determined this by zooming-in and observing the image
closely. A third user endorsed him by observing the image up-close and confirming that the news
is indeed false.
The users realized that this was a manipulated image not because the manipulation was easily
visible, but because the news presented in it was clearly questionable. The seemingly
unbelievable news prompted users to observe the image more closely.
However, even after group members determined that news like the one above was fake, in the
majority of cases they did not confront the sender of the news, nor did they report the sender to
the group admin. If a person shared objectionable content such as adult videos, others would
often request the admin to remove the person in question from the group immediately. Yet this
was not the case with fake news.
Our second observation was that when factually incorrect information is infused with emotional
appeals and polarizing opinions, it becomes increasingly difficult for users to discern its truth

30

value. The users’ focus subconsciously shifts from the facts presented in a message to the tone
and sentiments used in it instead.
One such example is of the false news which spread about the TLP leader Khadim Hussain Rizvi
coming back to life after passing away. This news was spread in the form of audio messages
calling for prayers for Mr Rizvi, and screenshots of tweets by TLP leaders and journalists.
Initially, this news appeared in one of our WhatsApp groups minutes after Mr Rizvi passed
away. The group was managed by members of the TLP, and the messages containing the false
information were sent by admins.
Due to the emotional nature of the news, other group members did not question the source of the
news. Instead, they began sending in their prayers and forwarding the messages. Within a couple
of minutes, we noticed the forwarded messages appear in our other WhatsApp groups. The news
was proved false nearly an hour later by TLP officials.
Similarly, many videos surfaced during the Aurat March which took place on Women’s Day.
These videos included live reporting of the protest as well as commentary on its alleged
objectives. We noticed that many images within the videos were used out of context, however,
intense language coupled with religious and political sentiments motivated users to share these
videos in multiple groups. Several group members endorsed the claims in the videos, not because
they were factually sound, but because they resonated with peoples’ preconceived notions and
biases.
Banaji et. al. observed a similar trend when observing misinformation related to mob violence in
India. They noted that people are likely to forward messages which specifically target peoples’
sentiments and biases by building upon pre-existing stereotypes of religious and ethnic
minorities. Moreover, the forwarded messages contain images and videos depicting very
disturbing scenes of violence, death, sexual abuse, etc. which automatically ignite anger and
hatred in people, causing them to resort to violence against certain member of the community
[1].

31

To summarize, we noted that:
1. Most users could identify information which was outright false.
2. When factually incorrect information was infused with emotional appeals and polarizing
opinions, it became increasingly difficult for users to discern its truth value.
3. Users tended to positively respond to and forward messages which resonated with their
preconceived biases.
4. In the majority of cases, users did not confront the sender of the news, nor did they report
the sender to the group admin.

32

4.2.

A Comparison of WhatsApp, Twitter and YouTube (RQ5-7)

To understand the similarities and differences between the spread of fake news on WhatsApp,
Twitter and YouTube, we compared the fake news samples identified on all three platforms as
well as users’ responses to them.
The table below gives an overview of the messages, tweets and videos we studied during the
course of our research. It may appear that our sample of 250 total YouTube videos is small
compared to 26,000 WhatsApp messages and 44,958 Tweets. The reason for this is that the
amount of content uploaded on YouTube per day is small fraction of tweets posted and
WhatsApp messages sent every day.

Table 2: Data Collected and Studied During Our Research

WhatsApp

Twitter

YouTube

Total Content

Content Classified as

Content Classified as

Studied

Viral

Misinformation

26,000 Messages

44,958 Tweets

250 Videos

909 Messages
(3.5% of Total)

4,250 Tweets
(9.5% of Total)

-

89 Messages
(0.3% of Total,
9.7% of Viral)
51 Tweets
(0.1% of Total,
1.2% of Viral)
20 Videos
(8% of Total)

We classified 3.5% of WhatsApp messages, and 9.5% of Tweets as viral using the following
criteria:
1. Content posted or forwarded in three or more WhatsApp groups out of the seventy-four
groups, or was labelled ‘forwarded many times’ was classified as viral.

33

2. If a tweet was retweeted more than average number of retweets (=5 retweets in our
dataset), then we consider it as a viral tweet.
We did not subdivide YouTube videos as viral due to their relatively small sample size.

RQ5: To what extent does misinformation spread between social media platforms?

Table 3: Spread of Misinformation between WhatsApp and Twitter
WhatsApp → Twitter
▪

Sharing of images and videos on

Twitter → WhatsApp
▪

Twitter which originated from

Sharing of screenshots of Tweets in
WhatsApp groups.

WhatsApp.
These were mostly tweets by
These images and videos mostly

journalists and religious leaders which

contained eyewitness accounts of

were used to add credibility to news

significant events such as protests.

circulated on WhatsApp.

Whenever significant events – such as protests or lockdowns – took place in the country, we
observed an increase in the number of messages received on WhatsApp. People from around the
country shared news, opinions and eyewitness accounts in the form of images, videos, texts and
voice notes. For example, members of the TLP political party shared videos of their protests in
various cities. Similarly, many reporters present at the Aurat March in Lahore shared their
opinions and interviews of the protestors. Unlike stories published in the media or broadcast on
television, events reported through WhatsApp messages often had little to no context
information. The forwarding nature of WhatsApp is also such that when a video or image is
forwarded, WhatsApp automatically discards its caption.

34

We observed that these videos and images were picked up from WhatsApp and shared on
Twitter. Due to the lack of context, the same videos and images were Tweeted with multiple
different captions on Twitter, sometimes offering contradictory explanations. The screenshots
below show Tweets based on a video of army personnel passing through a crowd of protestors.
Two different explanations are given as to why the personnel were chanting slogans.

We compared the upload date and times for videos and images which were common on Twitter
and WhatsApp and found that in the majority of instances, the image or video appeared on
WhatsApp first. This is similar to what Qadir et. al. observed in their research on COVID-19
misinformation in Pakistan. [22]
On WhatsApp, we observed that screenshots of Tweets were shared in groups. These primarily
included news related to significant events which were taking place in the country and around
the world. Tweets by prominent journalists and political leaders where shared. It appeared that
this was done to enhance the credibility of ongoing discussions on WhatsApp. Two examples of
Tweets containing misinformation which were shared in our WhatsApp groups are shown below.

35

Translation: “Miracle of Allah. Baba Gee (Khadim Rizvi) has started breathing again. All
Muslims pray that the sickness instead affects us so he can be saved. ”

Translation: “Mubashir Luqman has stated on an Israeli television that Pakistan will
recognize Israel soon. Hakim Sayed was right. ”

Table 4: Spread of Misinformation between WhatsApp and YouTube
WhatsApp → YouTube
▪

We did not observe a significant

YouTube → WhatsApp
▪

Sharing of links to YouTube videos

transfer of misinformation from

containing misinformation in

WhatsApp to YouTube.

WhatsApp groups.
▪

Sharing of screenshots and clips of
YouTube videos containing
misinformation in WhatsApp groups.

36

We observed a flow of misinformation from YouTube to WhatsApp but not the other way
around. Links to YouTube videos and channels were common in our WhatsApp group and
comprised 15% of all links shared. Although the majority of YouTube videos and channels
shared on WhatsApp pertained to smartphone tutorials, news analysis and recipes, we observed
some videos containing fake news and misinformation in the form of conspiracy theories,
misreporting of events and religious hate speech.
Another interesting finding was the use of screenshots or clips of YouTube videos on WhatsApp.
YouTube videos contain imagery or information that when viewed without context, can have a
completely different meaning than what the uploader intended. We noticed that there were
certain cases where a screenshot from a YouTube video or a snippet from the video was shared
on WhatsApp, without the whole context of the actual video. This would be posted on WhatsApp
with context different from the actual video or with completely fabricated additional information,
thus misleading viewers who viewed these clips and videos. This was one explanation that can
explain why YouTube is such a common aid using which misinformation spread on WhatsApp.
Effectively, the content on YouTube is either directly fake news or modified via editing and then
shared on WhatsApp in a manner that it becomes misinformation.

RQ6: What type of misinformation is common and different across social media platforms?
At first glance it may appear that misinformation on Twitter and WhatsApp is similar,
particularly in terms of content. However, a closer inspection reveals that the two platforms
exhibit different trends in the methods used to spread misinformation. Misinformation on
WhatsApp comprised mainly conspiracy theories, polarizing propaganda and false information.
On the contrary, more than half of the misinformation we observed on Twitter revolved around
hate speech and propaganda.
One of the reasons could be that facts are easily debunked by other users on Twitter and Twitter
users are very active in responding to viral tweets. Hence it makes sense for fake news creators
to spread their message using tweets which include more hate speech and propaganda as opposed
to conspiracy theories or incorrect facts.

37

Even though hate speech was much more prevalent on Twitter than WhatsApp, we observed that
the popular targets of hate speech were common on both platforms. In addition, on both
platforms there were organised groups of people spreading misinformation and hate speech.
These groups belonged to political parties who are using social media to reach out to their
supporters as well as other members of the community.
We noted that on Twitter, several accounts belonging to political party members and supporters
were suspended after they shared Tweets containing hate speech and propaganda. However, on
WhatsApp, users have more impunity as their messages are encrypted and it is nearly impossible
to trace the origin of misinformation.
In addition to hate speech, Twitter also has a much more significant amount of imposter content
as compared to WhatsApp. Fake accounts on twitter with a large number of followers were
common sources of fake news and misinformation. The Tweet below is from a fake account of
the columnist Orya Maqbool Jan. Such accounts which have a significant number of followers
are hotbeds of fake news on Twitter because they have an enormous outreach. Due to the
restrictive nature of WhatsApp groups (which allow up to 256 members), a single WhatsApp
user does not have an outreach as vast as a single Twitter user.

Translation: “Some people are asking on what charge was Hafiz Sayed Hussain Rizvi
arrested? Remember that Tehreek-e-Labaik is guilty of only one thing and that is talking
about the respect of the Holy Prophet (P.B.U.H), so this is how the Qadianis and their
friends are gathering?”

38

We also observed that the majority of misinformation on Twitter was in the form of text whereas
on WhatsApp it was in the form of videos and images. This might give us a clue as to the
differences in the demographics of both platforms; Twitter users in Pakistan seem to be more
likely to have higher literacy that WhatsApp users. However, further investigation of both
platforms in needed to reach to determine the users’ demographics.
YouTube videos that were labelled as fake news or misinformation had patterns, trends, and
characteristics like the fake news/misinformation that was taken from WhatsApp. The videos
that were extracted were sorted into their respective categories of conspiracy, medical, religious,
political and hoaxes. This categorization was almost identical to the categorization of data that
was extracted on WhatsApp:

Figure 7: Categorization of Misinformation on YouTube

39

As seen from the pie chart above, another trend shared by misinformation on YouTube was the
high occurrence of content related to medical news. Medical related news and misinformation
had a high occurrence in the scraping results, including overlaps with the videos in the
conspiracy category. We hypothesize this large volume of medical misinformation is due to the
ongoing 3rd wave of the pandemic and vaccination drive. Additionally, the conspiracy categories
of videos also had 2 pandemic related conspiracies included within them, but due to the nature of
the content and lack of medical concepts being used, they were classified as conspiracy.
Regarding the more general characteristics of videos, it should be noted that most of the fake
news and misinformation, except for one video, were under 10 minutes long. Furthermore, 60%
of the videos were under 5 minutes long. This correlates to the observation we made during our
initial study on WhatsApp, that fake news and misinformation related video content was often
short in length compared to other videos, as it was meant to obtain and retain as much user
attention as possible. This is because a viewer is more likely to view and watch a video that is
short in length due to ease of viewing.
Furthermore, these types of videos often had a heavily edited or distinct video thumbnail
compared to other videos, that contained images or texts that are meant to invoke sentiments or
emotions in a user, prompting them to click the video. These sentiments can be either fear
inducing or invoke religious sentiments:

Translation (right): “What do Qadianis add in Shezan products? ”

40

Similarly, the titles and the descriptions of such videos often contained language to evoke higharousal emotions from the viewers. For example:
Video Title: “Allah Ka Mojza Khadim Hussain Rizvi Ki Sans Bahal Driver Ne Bta Dia”
Video Description: “#AllahKaMojza #MiracleofAllah #KhadimHussainRizvi
#KhadimHussainRizviLatest”
A characteristic that was unique to the misinformation YouTube was the names of the channels
that posted these videos. Misinformation containing videos were often posted by channels that
contain the words TV, official, news or a combination of the three within them. We hypothesized
that these users named their channels these types of names in the attempt to appear as an
authentic and verified content creator channel. This would also aid in giving credibility to the
content posted by these channels, thus making users more likely to believe the content being
posted. And if a user is likely to believe the content being posted, they are also likely to share.
Additionally, while YouTube has user verification like Twitter and Facebook, no current failsafe
method is in place to prevent users from having a channel name similar or exceptionally close to
an actual verified news channel. Therefore, a user can create a channel that can have a name that
is like an actual news channel’s name but with a minute difference and post content. This
channel would not be discernable for the low literate population from the actual channel.

41

Figure 8: Categorization of Fake News on WhatsApp, YouTube and Twitter

The figure above gives a comparison of fake news on all three social media platforms. We see
that WhatsApp and YouTube have a higher percentage of medical fake news as compared to
Twitter, which has more political fake news. Religious fake news is common across all three
platforms.

RQ7: Is there a difference in the way users share and respond to misinformation on
different social media platforms?
It was clear that users on Twitter engage in more healthy debate as compared to WhatsApp. We
observed that viral Tweets received thousands of responses by users in the form of likes, retweets
and replies. Twitter users expressed a diverse range of opinions on different subjects, especially
those which were trending. Users also retweeted content which they disagreed with, and they
explained their reasons for disagreement.

42

On WhatsApp, users in a particular group were more likely to express similar political and
religious opinions on issues. Most users forwarded only that content which they agreed with.
Users were also likely to exit groups where they received messages they did not like. In many of
our groups, new users would exit the group withing one or two days. Although many believe
Twitter to be an ‘echo-chamber’, we felt that WhatsApp groups played a bigger role in
reenforcing preconceived beliefs and opinions.
A similarity between both platforms is that users do not check the source of information before
forwarding (or retweeting) it. Even though it is very easy to see who the author of a Tweet is, we
noticed even journalists retweeting tweets from fake accounts without realizing it. Similarly on
WhatsApp, users did not actively question the original source of information in a message
regardless of whether the message was ‘forwarded’ or not.
The behaviour of users on Twitter and WhatsApp revealed that both platforms were used for
different purposes. Twitter is used both as a platform for news and updates, and a forum for
debate. Users on Twitter engage more actively in discourse by expressing their opinions on other
users’ tweets.
WhatsApp is primarily used for entertainment, with most users in our public groups acting as
passive observers. In fact, in many groups we frequently received messages from the admin(s)
requesting all users to actively share content. In some cases, users (including us) were forcefully
removed from groups for not participating actively.
On YouTube, user responses to videos that contained misinformation vary according to the topic
or the content of the videos. However, there are two primary types of user responses; users that
support the content and creator of the content and those who report/ridicule the content and
creator. For example, the following is a comment section from a video calling the coronavirus
pandemic a conspiracy, with the comments showing support for the claim are users who share a
similar viewpoint with the creator:

43

Social interactions are influential not only in determining which videos become successful but
also on the magnitude of that impact [24].
However, a key difference between YouTube and other platforms such as WhatsApp remain the
YouTube moderation teams and algorithms. If a user reports a video for its content, whether that
content is offensive, controversial or violates YouTube’s policies in any manner, that video is
likely to be taken down and the account will be removed from YouTube. A revisit to the video
mentioned above resulted in no search return, meaning that the video had likely been removed.
YouTube’s content moderation works using two methods; algorithms, which analyze the video
audio for flagged content and keywords as well as the comments of the video and a manual
moderator, who views the video for a final check when a video is flagged by the algorithm or if
the video has several complaints against it. Previous studies have shown that comments are more
likely to be moderated if the video channel is ideologically extreme, if the video content is false
and if the comments are offensive [25].

44

Chapter 5

5. Discussion and Future Work
This report provides an in-depth analysis into the nature of misinformation and fake news on
social media in Pakistan with a particular focus on public WhatsApp groups in Pakistan. It also
analyses cross-platform links between WhatsApp and Twitter and WhatsApp and YouTube.
The data collected over a span of eight months helped us identify fake news and misinformation
related to politics, religion and health, among other categories. Common elements which are
used by fake news creators in Pakistan to make false content seem believable include: appeals to
emotion, conspiracy theories, political and religious polarization, incorrect facts and
impersonation of credible sources.
Our research was an exploration into the diverse network of public WhatsApp groups in
Pakistan. More work is required to understand the demographics of the group members and how
that affects the way they process fake news. Qualitative interviews of WhatsApp users can help
reveal many new insights about their behaviour on social media.
The ultimate aim of this research is to help reduce the spread of harmful misinformation in
Pakistan. Fact-checking plays a very important role here. Independent fact-checking agencies
help debunk viral misinformation, thereby reducing its harmful impact. It is worth exploring how
community-based fact-checking agencies can be created in Pakistan to debunk fake news.
Social media companies are helping reduce misinformation by flagging content and blocking
users who post misinformation. However, researchers suggest that these methods are not scalable
enough. New and innovative solutions need to be deployed to counter misinformation.
One such approach is to train social media users to spot misinformation through online games
based on the theory of ‘active inoculation’ [14]. Players are asked to create fake news of their
own and during the process they learn to identify the common techniques used by fake news
creators. We would like to develop such a game, based on our findings, to educate social media
users in Pakistan to be more aware and cautious of fake news.

45

References

[1] S. Banaji and R. Bhat, “WhatsApp Vigilantes: An exploration of citizen reception and
circulation of WhatsApp misinformation linked to mob violence in India,” LSE, 2019.
[2] K. Pezdek, “Getting Science in the Picture: Digital Game Development to Curb the Spread
of Misinformation,” 2018.
[3] J. Pal and P. Chandra, “Rumors and Collective Sensemaking: Managing Ambiguity in an
Informal Marketplace.,” in CHI Conference on Human Factors in Computing Systems,
2019.
[4] S. Z. Akbar, A. Panda, D. Kukreti, A. Meena and J. Pal, “Misinformation as a Window into
Prejudice: COVID-19 and the Information Environment in India,” in ACM Human
Computer Interaction, 2020.
[5] A. Ghani and S. Khan, “Misinformation in the Public Eye,” Media Matters for Democracy,
2020.
[6] P. Saha, B. Mathew, K. Garimella and A. Mukherjee, “"Short is the Road that Leads from
Fear to Hate": Fear Speech in Indian WhatsApp Groups,” Cornell University, 2021.
[7] R. Greifeneder, M. E. Jaffé, E. J. Newman and N. Schwarz, The Psychology of Fake News,
London: Routledge, 2020.
[8] P. M. Waszak, W. Kasprzycka-Waszak and A. Kubanek, “The spread of medical fake news
in social media – The pilot quantitative study,” Health Policy and Technology, vol. 7, no. 2,
2018.
[9] F. Gowhar, “Politics of Fake News: How WhatsApp Became a Potent Propaganda Tool in
India,” Media Watch India, vol. 9, no. 1, 2019.
[10] J. Roozenbeek and S. v. d. Linden, “Fake news game confers psychological resistance
against online misinformation,” Palgrave Communications, no. 5, 2019.

46

[11] S. Vosoughi, D. Roy and S. Aral, “The spread of true and false news online,” Science, 9
March 2018.
[12] M. N. Hussain, S. Tokdemir, N. Agarwal and S. Al-Khateeb, “Analyzing disinformation
and crowd manipulation tactics on YouTube,” in IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining (ASONAM'18), 2018.
[13] M. Faddoul, G. Chaslot and H. Farid, “A Longitudinal Analysis of YouTube's Promotion of
Conspiracy Videos,” Cornell University, 2020.
[14] J. Roozenbeek and S. van der Linden, “The fake news game: actively inoculating against the
risk of misinformation,” Journal of Risk Research, vol. 22, no. 5, 2018.
[15] J. Kim, B. Tabibian, A. Oh, B. Schölkopf and M. Gomez-Rodriguez, “Leveraging the
Crowd to Detect and Reduce the Spread of Fake News and Misinformation,” in WSDM '18:
Eleventh ACM International Conference on Web Search and Data Mining, 2018.
[16] S. M. Jones-Jang, T. Mortensen and J. Liu, “Does Media Literacy Help Identification of
Fake News? Information Literacy Helps, but Other Literacies Don’t,” American Behavioral
Scientist, vol. 65, no. 2, 2021.
[17] L. D. Grace, “Factitious: Large Scale Computer Game to Fight Fake News and Improve
News Literacy,” in CHI 2019, 2019.
[18] G. Pennycook, J. McPhetres and Y. Zhang, “Fighting COVID-19 Misinformation on Social
Media: Experimental Evidence for a Scalable Accuracy-Nudge Intervention,” Psychological
Science, vol. 31, no. 7, 2020.
[19] P. Melo, C. Vieria, P. O. Vaz de Melo and F. Benevenuto, “Can WhatsApp Counter
Misinformation by Limiting Message Forwarding?,” UFMG Brazil, 2020.
[20] A. Alemanno, “How to Counter Fake News? A Taxonomy of Anti-fake News Approaches,”
Cambridge University Press, 2018.
[21] H. Bhaskaran, H. Mishra and P. Nair, “Contextualizing Fake News in Post-truth Era:

47

Journalism Education in India,” Asia Pacific Media Educator, vol. 27, no. 1, 2017.
[22] T. Javed, M. E. Shuja, M. Usama, J. Qadir, W. Iqbal, G. Tyson, I. Castro and K. Garimella,
“A First Look at COVID-19 Messages on WhatsApp in Pakistan,” in IEEE/ACM
International Conference on Advances in Social Networks Analysis and Mining (ASONAM),
2020.
[23] C. Wardle, “Fake News. It's Complicated.,” First Draft, 16 February 2017. [Online].
Available: https://firstdraftnews.org/latest/fake-news-complicated/.. [Accessed 7 May
2021].
[24] A. Susarla, J.-H. Oh and Y. Tan, “Social Networks and the Diffusion of User-Generated
Content: Evidence from YouTube,” Information Systems Research, vol. 23, no. 1, 2011.
[25] S. Jiang, R. E. Robertson and C. Wilson, “Bias Misperceived:The Role of Partisanship and
Misinformation in YouTube Comment Moderation,” in Thirteenth International AAAI
Conference on Web and Social Media, 2019.
[26] B. Southwell and V. Boudewyns, “Curbing the Spread of Misinformation: Insights,
Innovations, and Interpretations from the Misinformation Solutions Forum,” RTI
International, 2018.
[27] G. Resende, P. Melo, J. Messias and H. Sousa, “(Mis)Information Dissemination in
WhatsApp: Gathering, Analyzing and Countermeasures,” in 28th Web Conference
(WWW'19), San Francisco, 2019.

48

