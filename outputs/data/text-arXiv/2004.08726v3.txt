arXiv:2004.08726v3 [cs.CY] 4 Dec 2020

Automatically Characterizing Targeted Information
Operations Through Biases Present in Discourse on
Twitter
Autumn Toney1 , Akshat Pandey1 , Wei Guo1 , David Broniatowski2,3 and Aylin Caliskan1,3
1 Department of Computer Science, {autumntoney, apandey0123, weiguo, aylin}@gwu.edu
2 Department of Engineering Management and Systems Engineering, broniatowski@gwu.edu
3 Institute for Data, Democracy & Politics
George Washington University

Abstract—This paper considers the problem of automatically
characterizing overall attitudes and biases that may be associated
with emerging information operations via artificial intelligence.
Accurate analysis of these emerging topics usually requires
laborious, manual analysis by experts to annotate millions of
tweets to identify biases in new topics. We introduce adaptations
of the Word Embedding Association Test from Caliskan et al. to
a new domain [1]. Our practical and non-parametric method is
used to quantify biases promoted in information operations. We
validate our method using known information operation-related
tweets from Twitter’s Transparency Report. We perform a case
study on the COVID-19 pandemic to evaluate our method’s
performance on non-labeled Twitter data, demonstrating its
usability in emerging domains.

I. I NTRODUCTION
Formally defined, information operations are “actions taken
by governments or organized non-state actors to distort
domestic or foreign political sentiment, most frequently to
achieve a strategic and/or geopolitical outcome.” [2]. Injecting
false or manipulated information into online platforms is
a common information operation tactic [3]. Disinformation
(information known to be falsified) posted by fake user accounts
or fake news sources can cause misinformation (information
not known to be falsified) to circulate by real users who
are unaware of its inaccuracy [3]. Social media platforms
play a principal role in the rapid spread of user-generated
content and provide a platform for information operations
to disseminate among targeted groups of people [4], [5],
[6], [7]. Information operation detection remains an open
problem with the continuous high velocity and volume of social
media posts complicating identification, especially in emerging
domains [8]. To accurately characterize information operations,
manual identification and annotation of online content requires
specialized area expertise and does not scale [9].
This work applies adaptations of the the Word Embedding
Association Test (WEAT), a practical, non-parametric artificial
intelligence (AI) method, to emerging domains by examining
biases in word embeddings trained on tweets [1]. WEAT
quantifies human-like biases between two target groups and
two sets of polar attributes. Caliskan et al. use these tests on
word embeddings to replicate biased associations documented

by the Implicit Association Test (IAT) by using word sets
of targets and attributes in the IAT [10]. Word embeddings
are vector space representations of semantics learned via the
distributional hypothesis. Since information operations aim to
generate panic and sow distrust [11], we adapt the WEAT to
our research domain by creating two bias tests: Calm/Panic
and Trustworthy/Untrustworthy. Calm/Panic addresses the
extent to which text may express panic surrounding a target,
whereas Trustworthy/Untrustworthy addresses the extent to
which text might frame a target as untrustworthy; a target
can be an individual or a group. We include the original
Pleasant/Unpleasant bias test from Caliskan et al. to measure
general negative bias against an opposing target, and use word
sets from Kurdi et al. and Werntz et al. to represent the
polar extremes of calm, panic, trustworthy, and untrustworthy
sentiments for our new bias tests [12], [13].
Manual analysis of tweets linked to information operations
has shown that the discourse is clearly formulated to target
specific groups of people (e.g., left-leaning activists) and
enforce an ideology [14], [7], [15]. Using a subset of tweets that
are linked to an information operation, we can measure the bias
present in the discourse to identify the targeted group, topic, or
social movement being affected. Twitter’s Transparency Report
provides sets of tweets manually verified by experts as being
related to state-backed information operations[16]; we use the
Internet Research Agency (IRA) and Russia sets for groundtruth analysis. Using these historical sets of tweets, which are
linked to information operations, we explore known biases
against political and racial groups to validate our method. We
find that using our Calm/Panic and Trustworthy/Untrustworthy
WEAT tests, we can identify the strong biases targeting
individuals or social groups in information operations corpora.
Our findings showcase a novel AI tool, an adaptation of
the WEAT, which can be used to further guide research in
information operations.
Our technique can be applied to new trending topics,
including those reflecting suspected information operations,
when annotated data is not readily available. We include a
case study for emerging topics to validate the usability of our
method on non-annotated Twitter data by examining biases

in tweets surrounding the COVID-19 pandemic. We collect
tweets containing anti-Chinese hashtags, over the course of
one week to investigate potential biased associations. Since the
COVID-19 outbreak originated in Wuhan, China, we expected,
and found, indications of anti-Chinese biases. We further
demonstrate that these biases are associated with expressions
of fear, panic, and negative sentiment; surprisingly, Russia is
associated with expressions of calm and positive sentiment.

s(X, Y, A, B) =

1
1
x,A,B)− m
Σy
x∈X s(~
~∈Y
m Σ~

s(~
y ,A,B)

σw∈X∪Y
s(w,A,B)
~
~

Where s(w,
~ A, B) = Σ~a∈A cos(w,
~ ~
a) − Σ~b∈B cos(w,
~ ~b), and σ
denotes standard deviation. Cosine similarity is the metric
of association between the word embeddings. The one-sided
permutation test (p-value) measures the unlikelihood of the null
hypothesis, which is the probability that a random permutation
of the attribute words would produce the observed difference
II. R ELATED W ORK
in sample means [1]. We use our generated word embeddings
Automatically identifying emerging information operations from domains of interest in our adaptation of the WEAT to
on Twitter has been explored using supervised machine automatically discover biases in information operations.
We adapt the original WEAT implementation using the
learning models [8], [17], [18], [19]. These approaches require
datasets manually annotated by experts, which is a significant word sets in Table I. The calm, panic, pleasant, unpleasant,
time-consuming limitation when analyzing emerging topics. trustworthy, and untrustworthy attribute sets are selected from
Similarly, tweets specifically tied to health-related information prior work in social psychology [10], [12], [13]. We follow
operations have been analyzed for spreading disinformation, the conventional stimulus selection criteria when target words
are not available in prior work [10]. We systematically select
and all require manual annotation [20], [21], [22], [23].
Twitter analysis has shown significant signals indicating that neutral words that represent the target and their corresponding
Russia’s Internet Research Agency (IRA) and other Russian hashtags (e.g., russia and #russia).
When word embeddings are trained on a small corpus, or the
coordinated information operations aim to spread societal
division in the U.S. [7], [4], [15]. These studies point to topics, word sets are considerably small (fewer than 8 words), the bias
such as the #BlackLivesMatter campaign and the 2016 U.S. score may be insignificant. Adding more stimuli increases the
presidential campaign, where IRA and Russian information significance of the WEAT’s results. Both calm and panic were
operations inject discourse to disrupt the information exchanges represented with 4 words in prior work [13]. Since some of
for a targeted group [7], [15], [24], [11], [14]. We use this those words were not present in our embeddings trained on a
research as the basis for selecting target concepts in our small Twitter corpus, we added synonyms from a similar study
represent each attribute set with 5 words [26]. T WITTER -G’s
experiments.
Caliskan et al. show that word embeddings capture the dictionary did not contain many of the hashtags we used in the
human-like biases and veridical information which embedded COVID-19 domain. Consequently, while representing Russia
in the statistical regularities of language [1]. They present the and China in the WEAT for T WITTER -G, we replaced the
WEAT, which is a non-parametric method to quantify biases hashtags with four major city names.
We run a validation set of experiments testing our adaptations
present in a language corpus using word embeddings. The
WEAT provides eight bias tests from the Implicit Association of the WEAT on known information operations, and we
Test (IAT), which is a validated bias measurement method in investigate our method on an emerging domain in a case
social psychology [1], [10]. Kurdi et al. investigate intergroup study experiment using COVID-19 data. Since we do not
attitudes and beliefs using the IAT, and find that implicit have validation data for COVID-19, we run several counterexperiments that test Russia against countries other than China.
associations correlate to intergroup attitudes [12].
IV. DATASETS
III. A PPROACH
We choose two Twitter datasets with ground truth informaWe use the original design of WEAT to measures biases in tion on bias associations: i) RU-D IS I NFO, a corpus released in
word embeddings trained on language corpora and adapt it to June 2019 that contains Russian information operation tweets
study information operations [1]. The WEAT takes two sets released by Twitter in January 2019, and ii) IRA-D IS I NFO, a
of target words (e.g., words representing African Americans corpus released in October 2018 that contains tweets traced to
and words representing European Americans) and two sets the IRA [16]. These tweets were flagged by Twitter as “stateof polar attributes (e.g., words representing pleasantness and backed information operations”. We generate lowercase, 300words representing unpleasantness) and computes an effect dimensional, Global Vectors for Word Representation (GloVe)
size (Cohen’s d) to measure the bias associations between word embeddings for each dataset [27]. We refer to the Russia
the target sets and polar attribute sets. By definition, a |d| ≥ word embeddings set as RU-D IS I NFO and Russia’s IRA word
0.80 indicates a high effect size [25]. Formally, let X and Y embeddings set as IRA-D IS I NFO.
be two target word sets of equal size and A and B be two
To evaluate results on an emerging topic (COVID-19), we
polar attribute sets of equal size. The effect size quantifies the use three sets of word embeddings: i) T WITTER -G, a general
standardized differential association between the targets and large-scale Twitter control corpus that reflects baseline biases
the polar attributes with the following formula:
[28], ii) COVID-G, a general coronavirus related public dataset

Embeddings

Topic

Word Set

IRA-D IS I NFO

#BlackLivesMatter/Police

#blacklivesmatter, #blm, #ferguson, #handsupdontshoot, #icantbreathe / #alllivesmatter, #backtheblue, #bluelivesmatter, #police-

RU-D IS I NFO

Trump/Clinton

trump, #trump, #maga, #trump2016, @realdonaldtrump / clinton, #clinton, #hillaryclinton, #clinton2016, @hillaryclinton

RU-D IS I NFO

Trump/Obama

trump, #trump, #maga, #trump2016, @realdonaldtrump / obama, #obama, #barackobama, #yeswecan, @barackobama

RU-D IS I NFO

Trump/Sanders

trump, #trump, #maga, #trump2016, @realdonaldtrump / sanders, #sanders, @berniesanders, #feelthebern, #berniesanders

COVID-G&AC

Russia/China

livesmatter, #thinblueline

moscow, russia, russian, russians, #moscow, #russia, #russian, #russians / beijing, china, chinese, wuhan, #beijing, #china, #chinese,
#wuhan

COVID-G&AC

Russia/Germany

moscow, russia, russian, russians, #moscow, #russia, #russian, #russians / berlin, german, germans, germany, #berlin, #german,
#germany

COVID-G&AC

Russia/Iran

moscow, russia, russian, russians, #moscow, #russia, #russian, #russians / iran, iranian, iranians, tehran, #iran, #iranian, #iranians,
#tehran

COVID-G&AC

Russia/USA

moscow, russia, russian, russians, #moscow, #russia, #russian, #russians / america, american, usa, washington, #america, #american,
#usa, #washington

T WITTER -G

Russia/China

moscow, novosibirsk, petersburg, russia, russian, russians, volgograd, yekaterinburg / beijing, chengdu, china, chinese, shanghai,
shenzhen, tianjin, wuhan

All Embeddings

Pleasant/Unpleasant

glorious, happy, joy, laughter, love, pleasure, peace, wonderful / agony, awful, evil, failure, horrible, hurt, nasty, terrible

All Embeddings

Calm/Panic

calm, peaceful, quiet, relaxed, tranquil∗ / anxious, fear, frightened∗ , panicked, scared

All Embeddings

Trustworthy/Untrustworthy

friendly, trustworthy, warm, sincere, nice, kind, supportive / selfish, mean, dishonest, cold, disloyal, untrustworthy, deceitful

∗

The word ‘tranquil’ was not present in IRA-D IS I NFO’s dictionary. As a result, the word ‘tranquil’ from calm attributes, and the word ‘frightened’ (chosen at random) from panic
attributes were deleted while running WEAT on IRA-D IS I NFO.

TABLE I: Target and attribute words sets for our WEAT implementations

Anti-Chinese Hashtags

A. Method Validation

#chinavirus, #wuhan, #wuhanvirus, #chinavirusoutbreak,
#wuhancoronavirus, #wuhaninfluenza, #wuhansars,
#chinacoronavirus, #wuhan2020, #chinaflu,
#wuhanquarantine, #chinesepneumonia, #coronachina,
#wohan

RU-D IS I NFO word embeddings: We implement the Trustworthy/Untrustworthy bias test to measure the association of
the winning presidential candidate, Donald Trump, who U.S.
government sources determined was characterized by Russian
information operations as more trustworthy than the opposing
presidential candidate Hillary Clinton [7], [30].

TABLE II: List of Anti-Chinese hashtags for Twitter
Embeddings

of tweets [29] collected during 12–22 March 2020, and iii)
COVID-AC, a set of tweets we collected during 11–18 March
2020 that contain 14 hashtags (see Table II) that are related to
the COVID-19 pandemic and targeting China and Wuhan.
For T WITTER -G, we use the lowercase, pre-trained GloVe
Twitter word embeddings1 , which are widely used word
embeddings trained on 2 billion random tweets [27]. We
use T WITTER -G to obtain control results that capture known
human-like biases [28]. Consistent with our experimental
datasets, we generate 300-dimensional GloVe word embeddings
for the COVID-G (general COVID-19 tweets) and COVIDAC (COVID-19 tweets with China related hashtags) corpora.
Our generated embeddings will be publicly available online2 .
V. R ESULTS
We first validated our method using the RU-D IS I NFO and
IRA-D IS I NFO word embeddings, and then we apply our
method to the COVID-AC, COVID-G and T WITTER -G
embeddings to analyze its results on an emerging domain.
1 200-dimensional
2 Git

Repo

embeddings trained on 27 billion tokens

RU-D IS I NFO

Targets
Trump vs. Clinton
Trump vs. Sanders
Trump vs. Obama

Attributes
Trustworthy

-

Untrustworthy

d∗

p∗

1.27
1.03
−0.39

0.023
0.051
0.737

TABLE III: RU-D IS I NFO WEAT experiments
The Trustworthy/Untrustworthy bias test produces an effect
size of d = 1.27 (P = 0.023) using the RU-D IS I NFO word embeddings, consistent with prior research showing that Russian
information operations characterized Clinton as deceitful and
untrustworthy [7], [30]. Since the presidential candidate WEAT
measures a pro-target versus an anti-target (pro-Trump/antiClinton), we run counter experiments to validate our results. We
substitute Bernie Sanders and Barack Obama for Hillary Clinton
to measure the bias against another presidential candidate and
the current president of the U.S. from the opposite political
party. The results in Table III validate that our method identifies
the targeted discourse from the 2016 presidential election
Russian information operation, as Bernie Sanders has a lower
effect size and Barack Obama has an insignificant effect size.
IRA-D IS I NFO word embeddings: We implement the
Calm/Panic bias test to measure the association of #BlackLivesMatter to calm and #BlueLivesMatter to panic, since

Fig. 1. Calm/Panic WEAT measuring Russia’s association to calm and China’s
association to panic
Embeddings

Targets

Attributes

d∗

p∗

COVID-AC
COVID-G
T WITTER -G

Russia
vs.
China

Pleasant
Unpleasant

1.04
1.17
-0.92

0.016
<10−2
0.031

∗ We

report the effect sizes (d, rounded down), p values (p, rounded up).

TABLE IV: WEAT measuring Russia’s association to pleasantness and China’s association to unpleasantness
prior work analyzing IRA information operations on Twitter
indicate the IRA promoted the #BlackLivesMatter campaign
[15], [4]. The Calm/Panic bias test produces an effect size of
d = 1.14 (P = 0.036), indicating that the tweets flagged as
IRA information operations associate the #BlackLivesMatter
campaign to calm and #BlueLivesMatter counter-campaign to
panic, consistent with manual analysis [15], such as “group
identities are at the core of the IRA’s attack strategy . . . Black
users were confronted with an endless cavalcade of racism,
often perpetrated by white police officers”[4].
B. Emerging Domain Case Study: COVID-19

a small set of vocabulary words. Second, words with low
frequency might not be well represented in the embedding
space. Overall, our COVID-19 related results might reflect the
state of these countries during the COVID-19 pandemic. For
example, the more widespread COVID-19 in a country, the
more negative its associations might become. Nevertheless,
observing consistent pro-Russian biases in a COVID-19 corpus
with anti-Chinese hashtags is an unexpected finding that
suggests further investigation into information operations might
provide useful insights.
VI. F UTURE W ORK AND D ISCUSSION
Our adaptations of the WEAT to include Calm/Panic and
Trustworthy/Untrustworthy identify biases that are common
tactics in information operations on social media. We were able
to test our method using known information operations (e.g.,
the 2016 presidential election) and tweets that were manually
annotated as information operations and categorized by source
organization (e.g., Russian government). While we were able to
also test our method on an emerging domain, with COVID-19
as a case study, we manually selected our target word sets.
A direction for future work would be to automatically select
target word sets from the subset of tweets that are of interest in
detecting a potential information operation. Another challenge
in working with Twitter data for an emerging domain is being
able to detect the bias shift, if it exists, by having multiple
subsets of tweets to test on. Different sets of tweets have do not
necessarily share the same vocabulary, which can be a signal
itself, but does not guarantee consistent experiments in terms
of word sets. As noted, not all of the target words and attribute
words were present across all Twitter word embedding sets in
our experiments, and we had to adapt accordingly.
While we did not have validation data when we began our
case study experiments, numerous reports have been released
since then confirming Russia’s involvement in spreading
disinformation about COVID-19 on social media platforms,
namely Twitter [31], [32]. This confirmation of our results
suggests that our method provides an effective method to
identify information operations on emerging domains

We implement the Calm/Panic bias test and Pleasant/Unpleasant bias test (see Table IV) across the T WITTER -G,
COVID-G, and COVID-AC, word embeddings to compare
results and identify bias shifts (see Figure 1). We find a strong
pro-Russian and anti-Chinese bias in the Calm/Panic bias test
with an effect size of d = 1.31 (P < 10−2 ) using the COVIDAC word embeddings. The COVID-G word embeddings also
contain a significant, but smaller effect d = 0.85 (P = 0.045).
Finally, in the T WITTER -G word embeddings, bias drastically
moves to the opposite direction to d = −0.86 (P = 0.047). In
this control dataset, Russia is associated with panic whereas
VII. C ONCLUSION
China is associated with calm.
To investigate the scope of anti-Chinese biases, we ran
Using a non-parametric AI method to quantify biases
Calm/Panic and Pleasant/Unpleasant bias tests for numerous expressed on Twitter, our novel approach allows for realcountries (country-x) on COVID-AC. Consistent with our time bias analysis of a given text corpus, without requiring
main experiments, we select neutral, representative words expert annotated data. We adapt WEAT to measure bias
for each country (see Table I). China vs. country-x bias associations for concepts central to information operations such
tests indicate significant anti-Chinese biases. On the other as Calm/Panic and Trustworthy/Untrustworthy. Measuring these
hand, Russia vs. country-x strongly associates Russia with biases can help track how information operations spread chaos
calm and associates countries such as Germany (d = 1.00), and distrust in targeted groups. We validate our method on
Iran (d = 1.10), and USA (d = 0.81) with panic. All the Twitter data linked to known Russian and IRA information opWEAT tests with significant results indicate pro-Russian biases. erations, selecting word sets that represent targeted information
Nevertheless, some of the Russia vs. country-x results are not campaigns (#BlackLivesMatter and the 2016 U.S. presidential
statistically significant potentially due to two reasons. First, we election). We identify pro-Russian and anti-Chinese biases in
are not able to identify 8 words to represent some countries recent COVID-19 related Twitter data. Various domains can
accurately for the WEAT test. COVID-AC embeddings are apply this practical method by selecting the desired opposing
trained on a relatively small corpus and accordingly contain targets (e.g., Russia vs. China) to discover and measure the

present biases. This method could be used to characterize
attitudes on social media platforms prior to major world events,
such as the upcoming U.S. presidential election, or the quickly
evolving COVID-19 outbreak, by automatically identifying
emerging biases. If unexpected biases are detected, researchers
might then examine whether these could be artificially and
deliberately introduced to the public sphere.
R EFERENCES
[1] A. Caliskan, J. J. Bryson, and A. Narayanan, “Semantics derived
automatically from language corpora contain human-like biases,” Science,
vol. 356, no. 6334, pp. 183–186, 2017.
[2] J. Weedon, W. Nuland, and A. Stamos, “Information operations and facebook,” Retrieved from Facebook: https://fbnewsroomus. files. wordpress.
com/2017/04/facebook-and-information-operations-v1. pdf, 2017.
[3] L. Howell et al., “Digital wildfires in a hyperconnected world,” WEF
report, vol. 3, pp. 15–94, 2013.
[4] D. Freelon and T. Lokot, “Russian twitter disinformation campaigns
reach across the american political spectrum,” Harvard Kennedy School
Misinformation Review, vol. 1, no. 1, 2020.
[5] K. E. Matsa and E. Shearer, “News use across social media platforms
2018,” Pew Research Center, 2018.
[6] M. Del Vicario, A. Bessi, F. Zollo, F. Petroni, A. Scala, G. Caldarelli,
H. E. Stanley, and W. Quattrociocchi, “The spreading of misinformation
online,” Proceedings of the National Academy of Sciences, vol. 113,
no. 3, pp. 554–559, 2016.
[7] S. C. Woolley and P. N. Howard, Computational propaganda: political
parties, politicians, and political manipulation on social media. Oxford
University Press, 2018.
[8] A. Gupta, P. Kumaraguru, C. Castillo, and P. Meier, “Tweetcred: Realtime credibility assessment of content on twitter,” in International
Conference on Social Informatics. Springer, 2014, pp. 228–243.
[9] R. Gorwa, R. Binns, and C. Katzenbach, “Algorithmic content moderation: Technical and political challenges in the automation of platform
governance,” Big Data & Society, vol. 7, no. 1, p. 2053951719897945,
2020.
[10] A. G. Greenwald, D. E. McGhee, and J. L. Schwartz, “Measuring
individual differences in implicit cognition: the implicit association test.”
Journal of Personality and Social Psychology, vol. 74, no. 6, p. 1464,
1998.
[11] M. Hindman and V. Barash, “Disinformation, and Influence Campaigns
on Twitter,” Knight Foundation, 2018.
[12] B. Kurdi, T. C. Mann, T. E. Charlesworth, and M. R. Banaji, “The relationship between implicit intergroup attitudes and beliefs,” Proceedings
of the National Academy of Sciences, vol. 116, no. 13, pp. 5862–5871,
2019.
[13] A. J. Werntz, S. A. Steinman, J. J. Glenn, M. K. Nock, and B. A. Teachman, “Characterizing implicit mental health associations across clinical
domains,” Journal of behavior therapy and experimental psychiatry,
vol. 52, pp. 17–28, 2016.
[14] A. Arif, L. G. Stewart, and K. Starbird, “Acting the part: Examining
information operations within# blacklivesmatter discourse,” Proceedings
of the ACM on Human-Computer Interaction, vol. 2, no. CSCW, pp.
1–27, 2018.
[15] M. Anderson, S. Toor, L. Rainie, and A. Smith, “Activism in the social
media age,” Pew Research Center, vol. 11, 2018.
[16] “Twitter Transparency Report,” https://transparency.twitter.com/, 2018,
accessed: 2020-02-25.
[17] X. Liu, A. Nourbakhsh, Q. Li, R. Fang, and S. Shah, “Real-time rumor
debunking on twitter,” in Proceedings of the 24th ACM International
on Conference on Information and Knowledge Management, 2015, pp.
1867–1870.
[18] C. Buntain and J. Golbeck, “Automatically identifying fake news in
popular twitter threads,” in 2017 IEEE International Conference on
Smart Cloud (SmartCloud). IEEE, 2017, pp. 208–215.
[19] J. Im, E. Chandrasekharan, J. Sargent, P. Lighthammer, T. Denby,
A. Bhargava, L. Hemphill, D. Jurgens, and E. Gilbert, “Still out there:
Modeling and identifying russian troll accounts on twitter,” arXiv preprint
arXiv:1901.11162, 2019.
[20] S. O. Oyeyemi, E. Gabarron, and R. Wynn, “Ebola, twitter, and
misinformation: a dangerous combination?” Bmj, vol. 349, p. g6178,
2014.

[21] A. Ghenai and Y. Mejova, “Catching zika fever: Application of crowdsourcing and machine learning for tracking health misinformation on
twitter,” arXiv preprint arXiv:1707.03778, 2017.
[22] D. A. Broniatowski, A. M. Jamison, S. Qi, L. AlKulaib, T. Chen, A. Benton, S. C. Quinn, and M. Dredze, “Weaponized health communication:
Twitter bots and Russian trolls amplify the vaccine debate,” American
Journal of Public Health, vol. 108, no. 10, pp. 1378–1384, 2018.
[23] Y. Ortiz-Martínez and L. F. Jiménez-Arcia, “Yellow fever outbreaks
and twitter: Rumors and misinformation,” American journal of infection
control, vol. 45, no. 7, pp. 816–817, 2017.
[24] A. Entous, C. Timberg, and E. Dwoskin, “Russian operatives used facebook ads to exploit america’s racial and religious divisions,” Washington
Post, vol. 25, 2017.
[25] J. Cohen, Statistical power analysis for the behavioral sciences. Academic press, 2013.
[26] J. L. Tsai, B. Knutson, and H. H. Fung, “Cultural variation in affect
valuation.” Journal of personality and social psychology, vol. 90, no. 2,
p. 288, 2006.
[27] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors
for word representation,” in Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), 2014, pp.
1532–1543.
[28] S. Dev and J. Phillips, “Attenuating bias in word vectors,” arXiv preprint
arXiv:1901.07656, 2019.
[29] S. Smith, “Coronavirus (covid19) Tweets. Tweets using hashtags associated with Coronavirus,” https://www.kaggle.com/smid80/
coronavirus-covid19-tweets, 2020, accessed: 2020-05-31.
[30] A. Bovet and H. A. Makse, “Influence of fake news in Twitter during the
2016 US presidential election,” Nature Communications, vol. 10, no. 1,
pp. 1–14, 2019.
[31] “Gec special report: Pillars of russia’s disinformation and propaganda
ecosystem,” The Department of State, Washington, DC, Tech. Rep.,
august 2020.
Atlantic
Treaty
Organization,
“Nato’s
approach
[32] North
to
countering
disinformation:
a
focus
on
covid-19,”
https://www.nato.int/cps/en/natohq/177273.htm, Brussels, Belgium,
2020.

