Distributed Computing in a Pandemic: A Review of
Technologies available for Tackling COVID-19

arXiv:2010.04700v3 [cs.DC] 18 May 2021

A Preprint
Jamie J. Alnasir
Department of Computing
Imperial College
London, UK
j.alnasir@imperial.ac.uk

May 15, 2021

Abstract
The current COVID-19 global pandemic caused by the SARS-CoV-2 betacoronavirus has resulted
in over a million deaths and is having a grave socio-economic impact, hence there is an urgency to
ﬁnd solutions to key research challenges. Some important areas of focus are: vaccine development,
designing or repurposing existing pharmacological agents for treatment by identifying druggable
targets, predicting and diagnosing the disease, and tracking and reducing the spread. Much of this
COVID-19 research depends on distributed computing.
In this article, I review distributed architectures — various types of clusters, grids and clouds — that
can be leveraged to perform these tasks at scale, at high-throughput, with a high degree of parallelism,
and which can also be used to work collaboratively.
High-performance computing (HPC) clusters, which aggregate their compute nodes using highbandwidth networking and support a high-degree of inter-process communication, are ubiquitous
across scientiﬁc research — they will be used to carry out much of this work. Several bigdata
processing tasks used in reducing the spread of SARS-CoV-2 require high-throughput approaches,
and a variety of tools, which Hadoop and Spark oﬀer, even using commodity hardware.
Extremely large-scale COVID-19 research has also utilised some of the world’s fastest supercomputers, such as IBM’s SUMMIT — for ensemble docking high-throughput screening against SARSCoV-2 targets for drug-repurposing, and high-throughput gene analysis — and Sentinel, an XPE-Cray
based system used to explore natural products. Grid computing has facilitated the formation of the
world’s ﬁrst Exascale grid computer. This has accelerated COVID-19 research in molecular dynamics
simulations of SARS-CoV-2 spike protein interactions through massively-parallel computation and
was performed with over 1 million volunteer computing devices using the Folding@home platform.
Grids and clouds both can also be used for international collaboration by enabling access to important
datasets and providing services that allow researchers to focus on research rather than on timeconsuming data-management tasks.
Keywords SARS-CoV-2 · COVID-19 · distributed · HPC · supercomputing · grid · cloud · cluster

A preprint - May 15, 2021

1 Introduction
A novel betacoronavirus named SARS-CoV-2 (Severe Acute Respiratory Syndrome coronavirus 2) is the cause of the
clinical disease COVID-19 — its spread is responsible for the current coronavirus pandemic and the resulting global
catastrophe [1]. The initial outbreak of the disease was ﬁrst detected in December 2019 in Wuhan (Hubei province,
China) manifesting as cases of pneumonia, initially of unknown aetiology. On the 10th of January 2020, Zhang et al.
released the initial genome of the virus [2]. Shortly after, it was identiﬁed — by deep sequencing analysis of lower
respiratory tract samples — as a novel betacoronavirus and provisionally named 2019 novel coronavirus (2019-nCoV)
[3, 4]. By the 30th of January 2020, the WHO (World Health Organisation) declared the outbreak a Public Health
Emergency of International Concern [5], and a global pandemic on the 11th of March [6]. At the time of writing
(May 2021) there now are over 160 million reported cases of COVID-19 globally and more than 3,334,000 deaths have
occurred as a result of the disease [7]. In addition to the casualties, the pandemic is also having a grave socio-economic
impact; it is a global crisis to which researchers will typically apply a variety computational techniques and technologies
to several key areas of focus [8, 9]. These include, but are not limited to, vaccine development, designing or repurposing
existing pharmacological agents for treatment by identifying druggable targets, predicting and diagnosing the disease,
e.g. clinical decision support, and tracking and reducing the spread [10, 11, 12]. Many of the tasks involved can
leverage a variety of distributed computing approaches which can be applied at scale, at high-throughput, and with a
high degree of parallelism — they often also need to be performed collaboratively.
The classiﬁcation of SARS-CoV-2 as a betacoronavirus, and the release of its genome earlier on January 2020, has
enabled research to focus on speciﬁc strategies. It is known from the previous 2003 SARS outbreak that ACE2
(Angiotensin Converting Enzyme) is the main entry point the virus targets to infect its host [13, 14]. To this end, for
drug repurposing or development, COVID-19 research is focused on modelling the interaction between the coronavirus
spike protein (S-protein) and ACE2, and in understanding the structure of the S-protein as an epitope for vaccine
development. Other important targets are the virus’s proteome and the Papain-like and Main proteases – PL-pro and
ML-pro, respectively [15]. Given the urgency to reduce mortality, signiﬁcant eﬀorts are being made to re-purpose
medicines that are appropriate and already approved. Whilst a WHO scientiﬁc brieﬁng refers to this practice —
off-label prescribing — in the clinical setting, much of the initial work to predict potential drug candidates will be
carried out computationally via in-silico screening [16, 17]. Furthermore, the scale of the pandemic and the global
production of bigdata, particularly whilst vaccines are still being developed, will rely on bigdata analytics to model the
spread of the disease, and inform government policy and reduce the death rate.
This review paper explores the variety of distributed and parallel computing technologies which are suitable for the
research eﬀort to tackle COVID-19, and where examples exist, work carried out using them. This review will not
cover machine-learning or deep-learning methods, which although they employ highly-parallel GPU computing, are
not necessarily distributed - whilst they are highly relevant to several COVID-19 research areas, it is separate area in
its own right.
The distributed computing architectures that are suitable for application to COVID-19 research exist in several diﬀerent
topologies — they can be fundamentally categorised as clusters, grids and clouds [18], and will be covered in the next
sections.

2 Cluster Computing
Cluster computing, unlike client-server or n-tier architectures — where the focus is on delineating resources — group
together compute nodes to improve performance through concurrency [19]. The increasing amount of COVID-19
research to be completed on such systems, coupled with its urgency, will necessitate further performance increases for
large-scale projects. They can be achieved by vertical scaling — increasing the number of CPU cores in individual
compute nodes of the system — or horizontal scaling — increasing the number of compute nodes in the system, hence
2

A preprint - May 15, 2021

some of the distributed systems employed in the research we review here exhibit both of these characteristics, often at
very large-scales.
2.1 High-performance Computing with MPI
High-performance computing (HPC) is a key enabling technology for scientiﬁc and industrial research [20]; HPC
systems are ubiquitous across scientiﬁc and academic research institutions. Most of the computational research
projects investigating the structure, function and genome of SARS-CoV-2 will be performed on HPC. This will be
predominantly in-house, but in some cases will be via access to external HPC resources, e.g. in collaboration between
institutions.
By employing high-bandwidth networking interconnects, HPC clusters facilitate a high degree of inter-process communication and extremely large scalability, for instance the Message Passing Interface (MPI) framework. Software
implemented using MPI can exploit for many of the computationally complex problems in COVID-19 research, such as
ensemble docking and mathematical modeling [21]. The use of MPI for distributing complex scientiﬁc computation is
well-established and many HPC systems are dependent on MPI libraries such OpenMPI and MVAPICH. Consequently,
there have been further developments and reﬁnement of these libraries over the last two decades — mainly in reducing
latency and memory requirements [22]. OpenMPI and MPICH have had their most recent releases in 2020. Recently,
new MPI implementations are coming to the fore, such as in LinaLC, a docking program employing strategies such as
mixed multi-threading schemes to achieve further performance gains at an extremely large scale, that can be applied to
COVID-19 research which we will discuss in the next section.
2.1.1 Ensemble Docking
A key task in identifying potential pharmacological agents to target SARS-CoV-2 is molecular docking — in-silico
simulation of the electrostatic interactions between a ligand and its target — is used to score ligands according to
their aﬃnity to the target [23, 24]. The complex computational process is extensively used in drug development and
repurposing and is often time-consuming and expensive [25, 26]. The protein and enzyme targets that are docked
against are not static, but are constantly moving in ways which are dependent on several factors such as temperature,
electrostatic attractions and repulsions with nearby molecules, solvation (interaction with the solvent environment)
etc. These factors cause atoms in the molecules, within the constraints of the types of bonds the bind them, to
adopt spatial arrangements — termed conformations — that correspond to local energy minima on the energy surface.
Molecular Dynamics (MD) uses in-silico computation to simulate this process, the outcome of which is typically
clusters ("ensembles") of the most probable conformations for docking, i.e. ensemble docking [27].
In the past, popular tools such as AutoDock Vina — widely-used for performing both molecular docking and virtual
screening — were being used primarily on single high-end workstations. Consequently, their parallelism was optimised
for multithreading on multicore systems. However, further gains in such tools have been made by developing or reimplementing existing code for the ﬁne-grained parallelism oﬀered by MPI, and at the same time, leveraging the
scale at which HPC systems can operate. In previous work, Zhang et al. have further modiﬁed the AutoDock Vina
source to implement a mixed MPI and multi-threaded parallel version called VinaLC. They have demonstrated this
works eﬃciently at a very large-scale — 15K CPUs — with an overhead of only 3.94%. Using the DUD dataset
(Database of Useful Docking Decoys), they performed 17 million ﬂexible compound docking calculations which were
completed on 15,408 CPUs within 24 h. with 70% of the targets in the DUD data set recovered using VinaLC. Projects
such as this can be repurposed and applied to identifying potential leads for binding to the SARS-CoV-2 S-protein
or the S-protein:Human ACE2 interface, either through the repurposing or the identiﬁcation of putative ligands [28].
Furthermore, given the urgency in ﬁnding solutions to the current COVID-19 pandemic — where high-throughput
performance gains and extreme scalability are required — these features can be achieved by re-implementing tools in
similar ways to which VinaLC has been optimised from the AutoDock codebase.
3

A preprint - May 15, 2021

2.2 Supercomputers and COVID-19
2.2.1 Drug Repurposing
In recent COVID-19 focused research, Smith et al. have utilised IBM’s SUMMIT supercomputer — the world’s fastest
between November 2018 and June 2020 — to perform ensemble docking virtual high-throughput screening against
both the SARS-CoV-2 S-protein and the S-protein:Human ACE2 interface [29, 30].
SUMMIT, launched by ORNL (Oak Ridge National Laboratory) and based at it’s Oak Ridge Leadership Computing
Facility, comprises 4,608 compute nodes, each with two IBM POWER9 CPUs (containing nine cores each), and six
Nvidia Tesla Volta GPUs for a total of 9,216 CPUs and 27,648 GPUs [31]. Nodes each have 600 GB of memory,
addressable by all CPUs and GPUs, with an additional 800 GB of non-volatile RAM that can be used as a burst buﬀer
or as extended memory. SUMMIT implements a heterogeneous computing model — in each node the two POWER9
CPUs and Nvidia Volta GPUs are connected using Nvidia’s high-speed NVLink. The interconnect between the nodes
consist of 200 Gb/s Mellanox EDR Inﬁniband for both storage and inter-process messaging and supports embedded
in-network acceleration for MPI and SHMEM/PGAS.
For a source of ligands, they used the SWEETLEAD dataset which is a highly-curated in-silico database of 9,127
chemical structures representing approved drugs, chemical isolates from traditional medicinal herbs, and regulated
chemicals, including their stereoisomers[32]. The work involved three phases of computation: structural modelling,
molecular dynamics simulations (ensemble building), and in-silico docking. Since the 3D structure of the SARS-CoV2 S-protein was not yet available during the initial phase of this research, the ﬁrst phase (structural modelling) was
required and a 3D model was built with SWISSMODEL [33] using the sequences for the COVID-19 S-protein (NCBI
Ref. Seq: YP_009724390.1) and the crystal structure of SARS-CoV S-protein as a template to generate the model
of the SARS-CoV-2 S-protein:ACE2 complex. In the second phase, molecular dynamics simulations were carried
out using GROMACS (compiled on ORNL SUMMIT and run with CHARMM36 force-ﬁeld [34, 35]) to generate an
ensemble of highest probability, lowest energy conformations of the complex which were selected via clustering of the
conformations. In the ﬁnal in-silico docking phase, AutoDock Vina was run in parallel using an MPI wrapper.
This work has identiﬁed 47 hits for the S-protein:ACE2 interface, with 21 of these having US FDA regulatory approval
and 30 hits for the S-protein alone, with 3 of the top hits having regulatory approval.
2.2.2 High-throughput and Gene Analysis
Another research project by Garvin et al., that has also been undertaken using SUMMIT, focused on the role of
bradykinin and the RAAS (Renin Angiotensin Aldosterone System) in severe, life-threatining COVID-19 symptoms
by analysing 40,000 genes using sequencing data from 17,000 bronchoalveolar lavage (BAL) ﬂuid samples [36, 37].
RAAS regulates blood pressure and ﬂuid volume through the hormones renin, angiotensin and aldosterone. Key
enzymes in this system are ACE (Angiotensin Converting Enzyme), and ACE2 which work in antagonistic ways to
maintain the levels of bradykinin, a nine-amino acid peptide that regulates the permeability of the veins and arterioles
in the vascular system. Bradykinin induces hypotension (lowering of blood pressure) by stimulating the dilation of
aerterioles and the constriction of veins, resulting in leakage of ﬂuid into capillary beds. It has been hypothesised
that dysregulated of bradykinin signaling is responsible for the respiratory complications seen in COVID-19 — the
bradykinin storm [38].
This work involved massive-scale, gene-by-gene RNA-Seq analysis of SARS-CoV2 patient samples with those of the
control samples, using a modiﬁed transcriptome. The modiﬁed transcriptome was created to allow the researchers to
quantify the expression of SARS-CoV2 genes and compare them with the expression of human genes. To create the
modiﬁed transcriptome, reads from the SARS-CoV2 reference genome were appended to transcripts from the latest
human transcriptome, thereby allowing the mapping of reads to the SARS-CoV2 genes. The SUMMIT supercomputer
enabled the exhaustive gene-wise tests (all the permutations of all the genes) to be performed at a massive scale in order
4

A preprint - May 15, 2021

to test for diﬀerential expression, with the Benjamini-Hochberg method applied to the resulting p-values to correct for
multiple comparisons.
Their analysis appears to conﬁrm dysregulation of RAAS, as they found decreased expression of ACE together
with increased expression of ACE2, renin, angeiotensin, key RAAS receptors, and both bradykinin receptors. They
also observed increased expression of kininogen and a number of kallikrein enzymes that are kininogen activating
— the activated form, kinins, are polypeptides that are involved in vasodilation, inﬂammatory regulation, and blood
coagulation. As they point out, atypical expression levels for genes encoding these enzymes and hormones are predicted
to elevate bradykinin levels in multiple tissues and organ systems, and explain many of the symptoms observed in
COVID-19.
2.2.3 Exploring Natural Products for Treatment
So far, we have discussed some examples of the use of in-silico docking and screening that have utilised HPC to
identify existing medicines that could potentially be re-purposed for treating COVID-19. However, another strategy —
one that is used to develop new therapeutics — explores the chemistry of natural products, i.e. chemical compounds
produced by living organisms. To this end, in another research project that also performs in-silico docking and
screening using a supercomputer, Sentinel, Baudry et al. focus on natural products [39]. They point out that, natural
products, owing to the long periods of natural selection they are subjected to, perform highly selective functions. Their
work, therefore, aims to identify pharmacophores (spatial arrangement of chemical functional groups that interact with
a speciﬁc receptor or target molecular structure) that can be used to develop rationally designed novel medicines to
treat COVID-19. In addition to simulating the interaction with the S-protein RBD, they also included those with the
SARS-2 proteome (the sum of the protein products transcribed from its genome), speciﬁcally the main protease and
the papain-like protease enzymes. These are also important targets as they are highly conserved in viruses and are part
of the replication apparatus.
Sentinel, the cluster used for this research, is a Cray XC50, a 48-node, single-cabinet supercomputer featuring a
massively parallel multiprocesser architecture and is based in the Microsoft Azure public cloud data centre. It has
1,920 physical Intel Skylake cores operating at 2.4GHz with Hyperthreading (HT) / Simultaneous Multi-Tasking (SMT)
enabled, therefore providing 3,840 hyperthreaded CPU cores. Each node has 192 GB RAM and they are connected by
an Aries interconnect28 in a Dragonﬂy topology. A Cray HPE ClusterStor-based parallel ﬁle system is used, providing
612 TB of shared storage that is mounted on every node.
The development of medicines from natural products is challenging for several reasons: supply of the plant and marine
organisms, seasonal variation in the organism, extinction of organism sources, and natural products often occur as
mixtures of structurally related compounds, even after fractionation, only some of which are active. Contamination,
stability, solubility of the compounds, culturing source microorganisms, and cases where synergistic activities require
two constituents to be present to display full activity can also present diﬃculties [40]. Baudry et al., therefore,
performed their screening using a curated database of 423,706 natural products, COCONUT (COlleCtion of Open
NatUral producTs) [41]. COCONUT has been compiled from 117 existing natural product databases for which citations
in literature since 2000 exist.
Using molecular dynamics simulation coordinate ﬁles for the structures of the S-protein, main protease and the papainlike protease enzymes — generated with GROMACS [35] and made available by Oak Ridge National Laboratory —
they generated an ensemble using the ten most populated conﬁrmation for each, for in-silico docking. As AutoDock
was used, its codebase was compiled optimised for Sentinel, with some optimisations for Skylake CPU and memory
set in the Makeﬁle compiler options.
They performed pharmacophore analysis of the top 500 unique natural product conformations for each target (S-protein,
PL-pro, M-pro). Filtering was applied to the list of putative ligands such that there were no duplicate instances of the
same compound, they only occurred in the set for a single target, and were deemed to be drug-like using the MOE
5

A preprint - May 15, 2021

(Molecular Operating Environment) descriptor [42] from the COCONUT dataset. This resulted in 232, 204, and 164
compounds for the S-protein, PL-pro, M-pro, respectively. Of these, the top 100 natural products were superimposed
onto their respective predicted binding locations on their binding proteins and those that correctly bind to the correct
region (i.e. active site) were subjected to for pharmacophoric analysis. For the S-protein, two clusters of 24 and 73
compounds were found to bind to either side of a loop that interacts with ACE2. For PL-pro, the papain-like protease,
again two clusters of 40 and 60 compounds were found to bind to either side of a beta-sheet. Finally, for ML-pro,
the main protease, ﬁve clusters of binding compounds were found, one cluster in in close proximity to the proteases
catalytic site.
The common pharmacophores partaking in these interactions were assessed from the relevant clusters, resulting in a
greater understanding of the structure-activity relationship of compounds likely to be inhibitory to the SARS-CoV-2
S-protein, PL-pro, and ML-pro proteases. As a result, several natural product leads have been suggested which could
undergo further testing and development, and the pharmacophore knowledge could be used to reﬁne existing leads and
guide rational drug design for medicines to treat COVID-19.

2.3 Hadoop and Spark
Apache Hadoop is an open-source software “ecosystem” comprising a collection of interrelated, interacting projects,
distributed platform and software framework that is typically installed on a Linux compute cluster, notwithstanding
that it can be installed on a single standalone machine (usually only for the purposes of study or prototyping). Hadoop
is increasingly used for bigdata processing [43, 44]. Bigdata — which we will discuss in more detail with respect to
the COVID-19 pandemic — is characterised as data possessing large volume, velocity, variety, value and veracity —
known as the v’s of bigdata [45, 46]. A signiﬁcant portion of bigdata generated during the COVID-19 pandemic will
be semi-structured data from a variety of sources. MapReduce is a formalism for programatically accessing distributed
data across Hadoop clusters which store and process data as sets of key-value pairs (i.e. tuples) on which Map and
Reduce operations are carried out [47]. This makes MapReduce particularly useful for processing this semi-structured
data and building workﬂows.
Apache Spark, often viewed as as the successor to Hadoop is a distributed computing framework in its own right, which
can be used standalone or can utilise the Hadoop platform’s distributed ﬁle system (HDFS), and a resource scheduler
— typically Apache YARN. Spark, therefore, can also run MapReduce programs (written in Python, Java, or Scala)
[48]. It has been designed to overcome the constraints of Hadoop’s acyclic data ﬂow model, through the introduction
of a distributed data structure — the Resilient Distributed Dataset (RDD) — which facilitates the re-usability of
intermediate data between operations, in-memory caching, and execution optimisation (known as lazy evaluation) for
signiﬁcant performance gains over Hadoop [49].

2.4 COVID-19 Bigdata Analytics
As brieﬂy mentioned earlier, bigdata refers to data sets that, by virtue of their massive size or complexity, cannot
be processed or analysed by traditional data-processing methods, and, therefore, usually require the application of
distributed, high-throughput computing. Bigdata analytics — the collection of computational methods that are applied
for gaining valuable insight from bigdata — employs highly specialised platforms and software frameworks, such as
Hadoop or Spark. In a paper that focused on AI for bigdata analytics in infectious diseases, which was written over
a year before the current COVID-19 pandemic, Wong et al. point out that, in our current technological age, a variety
of sources of epidemiological transmission data exist, such as sentinel reporting systems, disease centres, genome
databases, transport systems, social media data, outbreak reports, and vaccinology related data [50]. In the early stages
of global vaccine roll out, compounded by the diﬃculty of scaling national testing eﬀorts, this data is crucial for contact
tracing, and for building models to understand and predict the spread of the disease [51].
6

A preprint - May 15, 2021

Furthermore, given the current COVID-19 pandemic has rapidly reached a global scale, the amount of data produced
and the variety of sources is even greater than before. Such data is, in most cases, semi-structured or unstructured and,
therefore, requires pre-processing [52]. The size and rate in which this data is being produced during this pandemic,
particularly in light of the urgency, necessitates bigdata analytics to realise the potential it has to aid in ﬁnding solutions
to arrest the spread of the disease by, for example, breaking the chain of transmission (i.e. via track-and-trace systems),
and informing government policy [53].
The Apache Hadoop ecosystem has a several projects ideally suited to processing COVID-19 big data, and by virtue
of them all utilising Hadoop’s cluster infrastructure and distributed ﬁle system, they gain from the scalability and faulttolerance inherent in the framework. For example, for pre-processing bigdata — often referred to as cleaning dirty data
— Pig is a high-level data-ﬂow language that can compile scripts into sequences of MapReduce steps for execution on
Hadoop [54]. Apache Spark, owing to its in-memory caching and execution optimisations discussed earlier, oﬀers at
least two orders of magnitude faster execution than Hadoop alone and, though centred around MapReduce programming,
is less constrained to it. Hive [55] is a data-warehousing framework which has an SQL type query language, HBase
[56] a distributed scalable database, and Mahout [57] can be used for machine-learning and clustering of data.
An example of how Hadoop can be applied to analytics of COVID-19 big data is shown in recent work by Huang et al
who have analysed 583,748,902 geotagged tweets for the purposes of reporting on human mobility — a causal factor
in the spread of the disease [58]. In doing so they have demonstrated that bigdata captured from social media can be
used for epidemiological purposes and can do so with less invasion of privacy that such data oﬀers. They do point out,
however, that a limitation to this approach is that only a small portion of the total twitter corpus is available via the API.
That said, an important outcome of this work is their proposed metric for capturing overall mobility during phases of
pandemics — the MRI (Mobility-Responsiveness) Indicator which can be used as a proxy for human mobility.
Whilst Hadoop and Spark are frequently applied to data analytics, they have also been employed in bioinformatics
— such as in processing Next-generation sequencing data, e.g. SNP genotyping, de novo assembly, read alignment,
reviewed in [59] and structural biology, e.g. in-silico molecular docking, structural alignment / clustering of proteinligand complexes, and protein analysis reviewed in [60].

3 Grid Computing
Grids provide a medium for pooling resources and are constructed from a heterogeneous collection of geographically
dispersed compute nodes connected in a mesh across the internet or corporate networks. With no centralised point
of control, grids broker resources by using standard, open, discoverable protocols and interfaces to facilitate dynamic
resource-sharing with interested parties [61]. Particularly applicable to COVID-19 research are the extremely large
scalability grid computing oﬀers and the infrastructure for international collaboration they facilitate, which we will
discuss in the following sections.
3.1 Large-scale Parallel-processing Using Grids
The grid architecture allows for massive parallel computing capacity by the horizontal scaling of heterogeneous
compute nodes, and the exploitation of underutilised resources through methods such as idle CPU-cycle scavenging
[62]. Distributed, parallel processing using grids is ideally suited for batch tasks that can be executed remotely without
any signiﬁcant overhead.
An interesting paradigm of grid computing, that has now been applied to Molecular Dynamics research for COVID-19,
leverages this scalability, particularly for applications in scientiﬁc computing, is known as volunteering distributed
computing having evolved during the growth of the internet from the 2000s onwards. This involves allocating work to
volunteering users on the internet (commonly referred to as the @home projects) with tasks typically executed while
the user’s machine is idle [63].
7

A preprint - May 15, 2021

3.2 The World’s First Exascale Computer Assembled Using Grid Volunteer Computing
A recent project, that focused on simulating the conformations adopted by the SARS-CoV-2 S-protein, culminated in
the creation of the ﬁrst Exascale grid computer. This was achieved by enabling over a million citizen scientists to
volunteer their computers to the Folding@home grid computing platform, which was ﬁrst founded in 2000 to understand
protein dynamics in function and dysfunction [64, 65]. The accomplishment of surmounting the Exascale barrier by
this work is based on a conservative estimate that the peak performance of 1.01 exaFLOPS on the Folding@home
platform was achieved at a point when 280,000 GPUs and 4.8 million CPU cores were performing simulations. The
estimate counts the number of GPUs and CPUs that participated during a three-day window, and makes the conservative
assumption about the computational performance of each device. Namely, that each GPU/CPU participating has worse
performance than a card released before 2015.
In addition to understanding how the structure of the SARS-CoV-2 S-protein dictates its function, simulating the
ensemble of conformations that it adopts allows characterisation of its interactions. These interactions with the ACE2
target, the host system antibodies, as well as glycans on the virus surface, are key to understanding the behaviour of
the virus. However, as pointed out in this work, datasets generated by MD simulations typically consist of only a
few microseconds of simulation — at most millisecond timescales — for a single protein. An unprecedented scale of
resources are therefore required to perform MD simulations for all of the SARS-CoV-2 proteins. The Folding@home
grid platform has enabled this, generating a run of 0.1 s of simulation data that illuminates the movement and
conformations adopted by these proteins over a biologically relevant time-scale.
3.3 International Collaboration Through Grids
Grids are an ideal infrastructure for hosting large-scale international collaboration. This was demonstrated by the
Globus Toolkit produced by the Global Alliance, which became a de facto standard software for grids deployed in
scientiﬁc and industrial applications. It was designed to facilitate global sharing of computational resources, databases
and software tools securely across corporate and institutions [66]. However, development of the toolkit ended in 2018
due to a lack of funding and the service remains under a freemium mode. Globus’s work in enabling worldwide
collaboration continue through their current platform which now employs cloud computing to provide services — this
is discussed further in section 4.
Some notable large-scale grids participating in COVID-19 research are: the World Community Grid launched by
IBM [67], the WLCG (Worldwide LHC Computing Grid) at CERN [68], Berkeley Open Infrastructure for Network
Computing (BOINC) [69], the European Grid Infrastructure (EGI) [70], the Open Science Grid (OSG) [71] and
previously Globus. Interestingly, grids can be constructed from other grids — for example, BOINC is part of IBM
WCG, and CERN’s WLCG is based on two main grids, the EGI and OSG, which is based in the US.
3.4 COVID-19 Research on Genomics England Grid
Genomics England, founded in 2014 by the UK government and owned by the Department of Health & Social Care,
has been tasked with delivering the 100,000 genomes project which aims to study the genomes of patients with cancer
or rare diseases [72, 73]. It was conceived at a time when several government and research institutions worldwide
announced large-scale sequencing projects — akin to an arms race of sequencing for patient-centric precision medicine
research. In establishing the project, the UK government and Illumina decided to secure sequencing services for the
project from Illumina [74]. Sequencing of the 100,000 genomes has resulted in 21 PB of data and involved 70,000 UK
patients and family members, 13 genomic medicines centres across 85 recruiting NHS trusts, 1,500 NHS staﬀ, and
2,500 researchers and trainees globally [75].
In 2018, after sequencing of the 100,000 genomes was completed, the UK government announced the signiﬁcant
expansion of the project — to sequence up to ﬁve million genomes over ﬁve years [76]. At the time, the Network
8

A preprint - May 15, 2021

Attached Storage (NAS) held 21 PB of data and had reached its node-scaling limit and so a solution that could scale
to hundreds of Petabytes was needed — after consultation with Nephos Technologies, a more scalable storage system
comprising a high-performance parallel ﬁle system from WekaIO, Mellanox® high-speed networking, and Quantum
ActiveScale object storage was implemented [77]. Genomics England’s Helix cluster, recently commissioned in 2020,
has 60 compute nodes each with 36 cores (providing 2,160 cores) and approximately 768 GB RAM. It has a dedicated
GPU node with 2x nVidia Tesla V100 GPUs installed [78].
GenOMICC (Genetics Of Mortality In Critical Care) is a collaborative project, ﬁrst established in 2016, to understand
and treat critical illness such as sepsis and emerging infections (e.g. SARS/MERS/Flu) is now also focusing on
the COVID-19 pandemic. The collaboration involves Genomics England, ISARIC (The International Severe Acute
Respiratory and Emerging Infection Consortium), InFACT (The International Federation of Acute Care Triallists),
Asia-Paciﬁc Extra-Corporeal Life Support Organisation (AP ELSO) and the Intensive Care Society. The aim is to
recruit 15,000 participants for genome sequencing, who have experienced only mild symptoms, i.e. who have tested
positive for COVID-19, but have not been hospitalised. The rationale is that in addition to co-morbidities, there are
genetic factors that determine whether a patient will suﬀer mild or severe, potentially life-threatenting illness — this
would also explain why some young people, who are ﬁt and healthy have suﬀered severely and others who are old
and frail did not. Furthermore, since many people who have suﬀered severe illness from COVID-19 were elderly
or from ethnic minorities, the aim is to recruit participants that are from these backgrounds who suﬀered from mild
symptoms of COVID-19. To this end, the project will carry out GWAS (Genome Wide Association Studies) to identify
associations between genetic regions (loci) and increased susceptibility to COVID-19 [79].
3.5 Other COVID-19 Research on Grids
During the previous 2002-4 SARS-CoV-1 outbreak, DisoveryNet — a pilot designed and developed at Imperial College
and funded by the UK e-Science Programme — enabled a collaboration between its team and researchers from SCBIT
(Shanghai Centre for Bioinformation Technology) to analyse the evolution of virus strains from individuals of diﬀerent
countries [80]. This was made possible through its provision of computational workﬂow services, such as an XML
based workﬂow language and the ability to couple workﬂow process to datasources, as part of an e-Science platform to
facilitate the extraction of knowledge from data (KDD) [81]. It is coincidental that this grid technology in its infancy,
and in its pilot phase, was used in a prior pandemic, especially since many of the services will be employed during the
current one, in particular the support for computational workﬂows and the use of large datasets made available through
grids and clouds.
In work that utilises various grid resources, including EGI and OSG, together with the European Open Science Cloud
(EOSC) [82], Hassan et al. have performed an in-silico docking comparison between human COVID-19 patient antibody
(B38) and RTA-PAP fusion protein (ricin a chain-pokeweed antiviral protein) against targets (S-protein RBD, Spike
trimer, and membrane-protein) in SARS-CoV-2 [83]. RTA-PAP, plant-derived N-glycosidase ribosomal-inactivating
proteins (RIPs), is a fusion of ricin a chain — isolated from Ricinus communis — and pokeweed antiviral protein
— isolated from Phytolacca Americana, which the same researchers had demonstrated to be anti-infective against
Hepatitis B in prior work [84]. They also utilised a grid based service called WeNMR, which provides computational
workﬂows for NMR (Nucleic Magnetic Resonance)/SAX (Small-angle X-ray scattering) via easy-to-use web interfaces
[85], and the CoDockPP protein-protein software to perform the docking [86]. They found favourable binding aﬃnities
(low binding energies) for the putative fusion protein RTA-PAP binding with both the SARS-CoV-2 S-protein trimer
and membrane protein, which can be further explored for development as antivirals for use against COVID-19.

4 Cloud Computing
A consequence of the data-driven, integrative nature of bioinformatics and computational biology [87], as well as
advancements in high-throughput next-generation sequencing [88], is that cloud-services such as for instance Amazon
9

A preprint - May 15, 2021

AWS, Microsoft Azure, and Google Cloud, are increasingly being used in research [89, 90]. These areas of research
underpin the COVID-19 research eﬀort and hence the use of cloud services will no doubt contribute signiﬁcantly to
the challenges faced.
Clouds provide pay-as-you-go access to computing resources via the internet through a service provider and with
minimal human interaction between the user and service provider. Resources are accessed on-demand, generically
as a service, without regard for physical location or speciﬁc low-level hardware and in some cases without software
conﬁguration [91]. This has been made possible by the developments in virtualisation technologies such as Xen, and
Windows Azure Hypervisor (WAH) [92, 93]. Services are purchased on-demand in a metered fashion, often to augment
local resources and aid in completion of large or time-critical computing tasks. This oﬀers small research labs access
to infrastructure that they would not be able to aﬀord to invest in for use on-premises, as well as services that would be
time consuming and costly to develop [94]. Furthermore, there is variety in the types of resources provided in the form
of diﬀerent service models oﬀered by cloud providers, such as Software as a Service (SaaS). Platform as a Service
(PaaS) and Infrastructure as a Service (IaaS) [95].
To a great extent, scientiﬁc and bioinformatics research projects utilise cloud services through IaaS (Infrastructure as
a Service) and PaaS (Platform as a Service). In the IaaS approach, processing, storage and networking resources are
acquired and leased from the service provider and conﬁgured by the end user to be utilised through the use of virtual
disk images. These virtual disks are provided in proprietary formats, for instance the AMI (Amazon Machine Image)
on AWS or VHD (Virtual Hard Disk) on Azure, serve as a bit-for-bit copy of the state of a particular VM [90]. They are
typically provisioned by the service provider with an installation of commonly used Operating Systems conﬁgured to
run on the cloud service’s Infrastructure, and service providers usually oﬀer a selection of such images. This allows the
end user to then install and precisely conﬁgure their own or third party software, save the state of the virtual machine,
and deploy the images elsewhere.
In contrast, in the PaaS approach, the end user is not tasked with low level conﬁguration of software and libraries
which are instead provided to the user readily conﬁgured to enable rapid development and deployment to the cloud. For
example AWS provides a PaaS for MapReduce called Elastic MapReduce which it describes as a “Managed framework
for Hadoop that makes it easy, fast, and cost-eﬀective to process vast amounts of data across dynamically scalable
Amazon EC2 instances” [96]. In fact MapReduce is oﬀered as a PaaS by all of the major cloud-service providers
(Amazon AWS, Google Cloud and Microsoft Azure) [97].
Cloud computing is market-driven and has emerged thanks to improvements in capabilities the internet which have
enabled the transition of computational research from mainstay workstations and HPC clusters into the cloud. Clouds
oﬀer readily provisionable resources which, unlike grids —- where investment can be lost when scaled down —
projects utilising cloud infrastructure do not suﬀer the same penalty. However, there is no up-front cost to establishing
infrastructure in the case of clouds. One potential drawback is that whilst the ingress of data into clouds is often free,
there is invariably a high cost associated with data egress which, depending on the size of the computational results,
can make it more costly than other infrastructures in terms of extricating computational results [94]. These are salient
factors with respect to the likely short-term duration of some of the pandemic research tasks that are being carried out.
4.1 International Collaboration through Clouds
As discussed in earlier (section 3.3), the Globus service has evolved from the Globus Alliance grid consortium’s work
on the standardisation and provision of grid services. Currently, Globus is a cloud-enabled platform that facilitates
collaborative research through the provision of services which focus primarily on data management. Globus is used
extensively by the global research community, and at the time of writing, there are over 120,000 users across more than
1,500 institutions registered and connected by more than 30,000 global endpoints. Endpoints are logical ﬁle transfer
locations (source or destination) that are registered with the service and represent a resource (e.g. a server, cluster,
storage system, laptop, etc.) between which ﬁles can be securely transferred by authorised users. Globus has enabled
10

A preprint - May 15, 2021

the transfer of more than 800 PB of data to-date — presently more than 500 TB are transferred daily. Some of the
services it provides are given in Table 1 below.
Feature
Identity management

File transfers
File sharing
Workflow automation
Dataset assembly
Publication repository
Collaboration
Dataset discovery

Description
Authentication and authorization interactions are brokered between end-users, identity providers, resource
servers (services), and clients
Can be performed securely, either by request or automated via script
Allows sharing between users, groups, and setting access permissions
Automate workﬂow steps into pipelines
Researchers can develop and deposit datasets, and describe their attributes using domain-speciﬁc metadata.
Curators review, approve and publish data
Collaborators can access shared ﬁles via Globus login
— no local account is required — and then download
Peers and collaborators can search and discover
datasets

Table 1: Some important services Globus provides
Another cloud project to enable research collaboration, currently still in development, is the European Open Science
Cloud (EOSC), which was proposed in 2016 by the European Commission with a vision to enable Open Science [82].
It aims to provide seamless cloud-services for storage, data management and analysis and facilitate re-use of research
data by federating existing scientiﬁc infrastructures dispersed across EU member states. After an extensive consultation
period with scientiﬁc and institutional stakeholders, the outcome is a road-map of project milestones, published in
2018, these are: i) Architecture, ii) Data, iii) Services, iv) Access and Interface, and v) Rules and Governance — and
are anticipated to be completed by 2021.

5 Distributed Computing Resources provided freely to COVID-19 Researchers
In order to facilitate and accelerate COVID-19 research,a number of organisations are oﬀering distributed computational
resources freely to researchers [98]. For instance, a number of research institutions that extensively use HPC — many of
which host the world’s most powerful supercomputers — have joined together to form the COVID-19 High Performance
Computing Consortium. Cloud providers such as Amazon AWS, Google, Microsoft and Rescale are also making their
platforms available, generally through the use of computational credits, and are largely being oﬀered to researchers
working on COVID-19 diagnostic testing and vaccine research. Table 2 lists some of the computational resources on
oﬀer, and the speciﬁc eligibility requirements for accessing them.

1Although the size of the infrastructure in this project is small, the dataset represents a large-scale study.

11

A preprint - May 15, 2021

Provider / Initiative

Offering

Eligibility

[99] COVID-19 High Performance
Computing Consortium

Access to global supercomputers at
the institutions taking part in the
consortium, such as:

Requests need to demonstrate:
• Potential near-term beneﬁts for COVID19 response
• Feasibility of the technical approach
• Need for HPC
• HPC knowledge and experience of the
proposing team
• Estimated computing resource requirements

• Oak Ridge Summit
• Argonne Theta
• Lawrence Berkeley National Laboratory Cori
• and many more
Also, other resources contributed by
members, such as:
• IBM, HP, Dell, Intel,
nVidia
• Amazon, Google
• National infrastructures
(UK, Sweden,
Japan,
Korea, etc)
• and many others
[100] Tech against COVID: Rescale
partnership with Google Cloud and
Microsoft Azure

HPC resources through the Rescale
platform

Any researcher, engineer, or scientist can apply
who is targeting their work to combat COVID-19
in developing test kits and vaccines.

[101] AWS Diagnostic Development Initiative1

AWS in-kind credits and technical
support.

Accredited research institutions or private entities:
• a using AWS to support research-oriented
workloads for the development of pointof-care diagnostics
• other COVID-19 infectious disease diagnostic projects considered

[102] Lifebit

Premium license for Lifebit CloudOS

Exact eligibility criteria not published, but is advertised for researchers developing diagnostics, treatments, and vaccines for COVID-19. Contact lifebit
with details of project.

Table 2: Free HPC and Cloud-computing resources for COVID-19 researchers

12

A preprint - May 15, 2021

Ref. / Name

Platform

Scale

[29] Smith et al.

IBM Summit
supercomputer

up to 4,608 nodes,
9,216 CPUs,
27,648 GPUs

Research task

Tools

Outcome

in-silico ensemble docking
& screening of existing
medicines for repurposing

GROMACS,
CHARMM32,
AutoDock Vina

Identiﬁed 47 hits for the Sprotein:ACE2 interface, with 21
of these having US FDA regulatory
approval. 30 hits for the S-protein
alone, with 3 of the top hits having
regulatory approval.

[36] Garvin et al.

IBM Summit
supercomputer

up to 4,608 nodes,
9,216 CPUs,
27,648 GPUs

large-scale gene analysis

AutoDock

Observed atypical expression levels for
genes in RAAS pointing to bradykinin
dysregulation and storm hypothesis.

[39] Baudry et al.

Cray Sentinel
supercomputer

up to 48 nodes,
1,920 physical cores
3,840 HT/SMT cores

in-silico docking

AutoDock

Pharmacophore analysis of natural
product compounds likely to be inhibitory to the SARS-CoV-2 S-protein,
PL-pro, and ML-pro proteases.

Folding@home grid

4.8 million CPU cores
∼280,000 GPUs

MD simulations

GROMACS,
CHARMM36,
AMBER03

Generated an unprecedented 0.1 s of
MD simulation data.

[83] Hassan et al.

EGI, OSD grids
& EOSC cloud

unspecified

in-silico docking

weNMR,
CoDockPP

Demonstrated high in-silico binding
aﬃnities of fusion protein RTA-PAP putative ligand with both the SARS-CoV-2
S-protein trimer and membrane protein.

[79] Pairo et al.

Genomics England
grid, Helix cluster

up to 60 nodes
(2,160 cores),
2x V100 GPUs

GWAS

Not yet speciﬁed

Recruitment of 15,000 participants is
ongoing.

[58] Huang et al.

On-premises
Hadoop cluster

13 Hadoop nodes 2

Twitter analytics

Hive, Impala

Analysis of over 580 million global geotagged tweets demonstrated that twitter
data is amenable to quantitatively assess user mobility for epidemiological
study, particularly in response to periods of the pandemic and government
announcements on mitigating measures.
Metric proposed: MRI (Mobility-based
Response Index) to act as proxy for human movement.

[64] Zimmerman
et al.

Table 3: Comparison of COVID-19 research exploiting large-scale distributed computing

6 Conclusions
There are a variety of distributed architectures that can be employed to perform eﬃcient, large-scale, and highly-parallel
computation requisite for several important areas of COVID-19 research. Some of the large-scale COVID-19 research
projects we have discussed that utilise these technologies are summarised in Table 3 — these have focused on in-silico
docking, MD simulation and gene-analysis.
High-performance computing (HPC) clusters are ubiquitous across scientiﬁc research institute and aggregate their
compute nodes using high-bandwidth networking interconnects. Employing communications protocols, such as
Message Passing Interface (MPI), they enable software to achieve a high degree of inter-process communication.
Hadoop and Spark facilitate high-throughput processing suited for the bigdata tasks in COVID-19 research. Even when
Hadoop/Spark clusters are built using commodity hardware, their ecosystem of related software projects can make use
of the fault-tolerant, scalable Hadoop framework i.e. HDFS distributed ﬁle system — features that are usually found
in more expensive HPC systems. Although not widely adopted, nor a common use, Hadoop and Spark have also been
employed for applications in bioinformatics (e.g. processing sequencing data) and structural biology (e.g. performing
docking, clustering of protein-ligand conformations).
Key points

2Although the size of the infrastructure in this project is small, the dataset represents a large-scale study.

13

A preprint - May 15, 2021

• HPC is commonly used in research institutions. However, access to the world’s supercomputers allows for
the largest scale projects to be completed quicker, which is particularly important given the time urgency of
COVID-19 research.
• Bigdata generated during the pandemic - which can be used for epidemiological modeling and critical track
and trace systems - can be processed using platforms such as Spark and Hadoop.
• Grid computing platforms oﬀer unprecedented computing power through volunteer computing, enabling
large-scale analysis during the pandemic that hitherto has not been achieved at this scale.
• Both grids and clouds can also be used for international research collaboration by providing services, frameworks and APIs, but diﬀer in their geographical distribution and funding models.
COVID-19 research has utilised some of the world’s fastest supercomputers, such as IBM’s SUMMIT — to perform
ensemble docking virtual high-throughput screening against SARS-CoV-2 targets for drug-repurposing, and highthroughput gene analysis — and Sentinel, an XPE-Cray based system used to explore natural products. During the
present COVID-19 pandemic, researchers working on important COVID-19 problems, who have relevant experience,
now have expedited and unprecedented access to supercomputers and other powerful resources through the COVID-19
High Performance Computing Consortium. Grid computing has also come to the fore during the pandemic by enabling
the formation of an Exascale grid computer allowing massively-parallel computation to be performed through volunteer
computing using the Folding@home platform.
Grids and clouds provide services such as Globus provide a variety of services, for example, reliable ﬁle transfer, workﬂow automation, identity management, publication repositories, and dataset discovery, thereby allowing researchers to
focus on research rather than on time-consuming data-management tasks. Furthermore, cloud providers such as AWS,
Google, Microsoft and Rescale are oﬀering free credits for COVID-19 researchers.
In the near future, we will be able to assess the ways in which distributed computing technologies have been deployed
to solve important problems during the COVID-19 pandemic and we will no doubt learn important lessons that are
applicable to a variety of scenarios.

7 Acknowledgements
The author wishes to thank Eszter Ábrahám for proofreading the manuscript.

8 Conflicts of Interest Statement
The author declares no conﬂicts of interest.

9 Authors’ information
Jamie J. Alnasir is currently a post-doctoral research associate at the SCALE Lab, Department of Computing, Imperial
College London. Having gained his Ph.D. from the University of London, he worked in the Scientiﬁc Computing department at the Institute of Cancer Research in London providing consulting on HPC and Research Software Engineering
before moving to Imperial College. His research interests are distributed computing, high-performance computing,
DNA-storage, computational biology, next-generation sequencing and bioinformatics. He is also a Genomics England
Clinical Interpretation Partnership (GeCIP) member.
14

A preprint - May 15, 2021

References
[1] Mary A Lake. What we know so far: Covid-19 current clinical knowledge and research. Clinical Medicine,
20(2):124, 2020.
[2] YZ Zhang. Initial genome release of novel coronavirus, 2020.
[3] Hongzhou Lu, Charles W Stratton, and Yi-Wei Tang. Outbreak of pneumonia of unknown etiology in wuhan,
china: The mystery and the miracle. Journal of medical virology, 92(4):401–402, 2020.
[4] Chaolin Huang, Yeming Wang, Xingwang Li, Lili Ren, Jianping Zhao, Yi Hu, Li Zhang, Guohui Fan, Jiuyang
Xu, Xiaoying Gu, et al. Clinical features of patients infected with 2019 novel coronavirus in wuhan, china. The
lancet, 395(10223):497–506, 2020.
[5] WHO. Statement on the second meeting of the international health regulations (2005) emergency committee
regarding the outbreak of novel coronavirus (2019-ncov). 2020.
[6] World Health Organization et al. Who director-general’s opening remarks at the media brieﬁng on covid-19-11
march 2020, 2020.
[7] Ensheng Dong, Hongru Du, and Lauren Gardner. An interactive web-based dashboard to track covid-19 in real
time. The Lancet infectious diseases, 20(5):533–534, 2020.
[8] Dayong Zhang, Min Hu, and Qiang Ji. Financial markets under the global pandemic of covid-19. Finance
Research Letters, page 101528, 2020.
[9] Maria Nicola, Zaid Alsaﬁ, Catrin Sohrabi, Ahmed Kerwan, Ahmed Al-Jabir, Christos Iosiﬁdis, Maliha Agha, and
Riaz Agha. The socio-economic implications of the coronavirus pandemic (covid-19): A review. International
journal of surgery (London, England), 78:185, 2020.
[10] Luca Ferretti, Chris Wymant, Michelle Kendall, Lele Zhao, Anel Nurtay, Lucie Abeler-Dörner, Michael Parker,
David Bonsall, and Christophe Fraser. Quantifying sars-cov-2 transmission suggests epidemic control with
digital contact tracing. Science, 368(6491), 2020.
[11] Stephen M Kissler, Christine Tedĳanto, Edward Goldstein, Yonatan H Grad, and Marc Lipsitch. Projecting the
transmission dynamics of sars-cov-2 through the postpandemic period. Science, 368(6493):860–868, 2020.
[12] Guillermo I Perez Perez and Amin Talebi Bezmin Abadi. Ongoing challenges faced in the global control of
covid-19 pandemic. Archives of Medical Research, 2020.
[13] Wenhui Li, Michael J Moore, Natalya Vasilieva, Jianhua Sui, Swee Kee Wong, Michael A Berne, Mohan
Somasundaran, John L Sullivan, Katherine Luzuriaga, Thomas C Greenough, et al. Angiotensin-converting
enzyme 2 is a functional receptor for the sars coronavirus. Nature, 426(6965):450–454, 2003.
[14] Keĳi Kuba, Yumiko Imai, Shuan Rao, Hong Gao, Feng Guo, Bin Guan, Yi Huan, Peng Yang, Yanli Zhang, Wei
Deng, et al. A crucial role of angiotensin converting enzyme 2 (ace2) in sars coronavirus–induced lung injury.
Nature medicine, 11(8):875–879, 2005.
[15] Rolf Hilgenfeld. From sars to mers: crystallographic studies on coronaviral proteases enable antiviral drug
design. The FEBS journal, 281(18):4085–4096, 2014.
[16] Andre C Kalil. Treating covid-19—oﬀ-label drug use, compassionate use, and randomized clinical trials during
pandemics. Jama, 323(19):1897–1898, 2020.

[17] Oﬀ-label use of medicines for COVID-19. https://www.who.int/publications/i/item/off-label-use-of-medicines2020. [Online; accessed 11-September-2020].
[18] Hameed Hussain, Saif Ur Rehman Malik, Abdul Hameed, Samee Ullah Khan, Gage Bickler, Nasro Min-Allah,
Muhammad Bilal Qureshi, Limin Zhang, Wang Yongji, Nasir Ghani, et al. A survey on resource allocation in
high performance distributed computing systems. Parallel Computing, 39(11):709–736, 2013.
15

A preprint - May 15, 2021

[19] George F Coulouris, Jean Dollimore, and Tim Kindberg. Distributed systems: concepts and design. pearson
education, 2005.
[20] EPSRC.
An analysis of the impacts and outputs of investment
https://epsrc.ukri.org/newsevents/pubs/impactofnationalhpc/, 2016.
07-September-2020].

in national HPC.
[Online; accessed

[21] Mark Donald Hill, Norman Paul Jouppi, and Gurindar Sohi. Readings in computer architecture. Gulf Professional
Publishing, 2000.
[22] Galen M Shipman, Timothy S Woodall, Richard L Graham, Arthur B Maccabe, and Patrick G Bridges. Inﬁniband scalability in open mpi. In Parallel and Distributed Processing Symposium, 2006. IPDPS 2006. 20th
International, pages 10–pp. IEEE, 2006.
[23] Garrett M Morris and Marguerita Lim-Wilby. Molecular docking. Molecular modeling of proteins, pages
365–382, 2008.
[24] Xuan-Yu Meng, Hong-Xing Zhang, Mihaly Mezei, and Meng Cui. Molecular docking: a powerful approach for
structure-based drug discovery. Current computer-aided drug design, 7(2):146–157, 2011.
[25] Hamilton Moses, E Ray Dorsey, David HM Matheson, and Samuel O Thier. Financial anatomy of biomedical
research. Jama, 294(11):1333–1342, 2005.
[26] Michael D Rawlins. Cutting the cost of drug development? Nature reviews Drug discovery, 3(4):360–364,
2004.
[27] Rommie E Amaro, Jerome Baudry, John Chodera, Özlem Demir, J Andrew McCammon, Yinglong Miao, and
Jeremy C Smith. Ensemble docking in drug discovery. Biophysical journal, 114(10):2271–2278, 2018.
[28] Xiaohua Zhang, Sergio E Wong, and Felice C Lightstone. Message passing interface and multithreading hybrid
for parallel molecular docking of large databases on petascale high performance computing machines. Journal
of computational chemistry, 34(11):915–927, 2013.
[29] Micholas Smith and Jeremy C Smith. Repurposing therapeutics for covid-19: supercomputer-based docking to
the sars-cov-2 viral spike protein and viral spike protein-human ace2 interface. 2020.

[30] Kerner, S.M.
IBM Unveils Summit, the World’s Fastest Supercomputer (For Now).
https://www.serverwatch.com/server-news/ibm-unveils-summit-the-worlds-faster-supercomputer-for-now
2018. [Online; accessed 07-September-2020].
[31] Sudharshan S Vazhkudai,Bronis R de Supinski, Arthur S Bland, Al Geist, James Sexton, Jim Kahle, Christopher J
Zimmer, Scott Atchley, Sarp Oral, Don E Maxwell, et al. The design, deployment, and evaluation of the coral preexascale systems. In SC18: International Conference for High Performance Computing, Networking, Storage
and Analysis, pages 661–672. IEEE, 2018.
[32] Paul A Novick, Oscar F Ortiz, Jared Poelman, Amir Y Abdulhay, and Vĳay S Pande. Sweetlead: an in silico
database of approved drugs, regulated chemicals, and herbal isolates for computer-aided drug discovery. PLoS
One, 8(11):e79568, 2013.
[33] Torsten Schwede, Jurgen Kopp, Nicolas Guex, and Manuel C Peitsch. Swiss-model: an automated protein
homology-modeling server. Nucleic acids research, 31(13):3381–3385, 2003.
[34] John Ossyra, Ada Sedova, Arnold Tharrington, Frank Noé, Cecilia Clementi, and Jeremy C Smith. Porting
adaptive ensemble molecular dynamics workﬂows to the summit supercomputer. In International Conference
on High Performance Computing, pages 397–417. Springer, 2019.
[35] Mark James Abraham, Teemu Murtola, Roland Schulz, Szilárd Páll, Jeremy C Smith, Berk Hess, and Erik
Lindahl. Gromacs: High performance molecular simulations through multi-level parallelism from laptops to
supercomputers. SoftwareX, 1:19–25, 2015.
16

A preprint - May 15, 2021

[36] Michael R Garvin, Christiane Alvarez, J Izaak Miller, Erica T Prates, Angelica M Walker, B Kirtley Amos, Alan E
Mast, Amy Justice, Bruce Aronow, and Daniel Jacobson. A mechanistic model and therapeutic interventions
for covid-19 involving a ras-mediated bradykinin storm. Elife, 9:e59177, 2020.
[37] Smith, T. IA Supercomputer Analyzed Covid-19 — and an Interesting New Theory Has Emerged.
https://elemental.medium.com/a-supercomputer-analyzed-covid-19-and-an-interesting-new-theory-has2020. [Online; accessed 09-September-2020].
[38] Joseph A Roche and Renuka Roche. A hypothesized role for dysregulated bradykinin signaling in covid-19
respiratory complications. The FASEB Journal, 2020.
[39] Kendall Byler, Joseph Landman, and Jerome Baudry. High performance computing prediction of potential
natural product inhibitors of sars-cov-2 key targets. 2020.
[40] Jesse W-H Li and John C Vederas. Drug discovery and natural products: end of an era or an endless frontier?
Science, 325(5937):161–165, 2009.
[41] Sorokina,
M.;
Steinbeck,
C.
COlleCtion
of
Open
NatUral
http://doi.org/10.5281/zenodo.3778405, 2020. [Online; accessed 11-September-2020].

producTs.

[42] Santiago Vilar, Giorgio Cozza, and Stefano Moro. Medicinal chemistry and the molecular operating environment
(moe): application of qsar and molecular docking to drug discovery. Current topics in medicinal chemistry,
8(18):1555–1572, 2008.
[43] David G Messerschmitt, Clemens Szyperski, et al. Software ecosystem: understanding an indispensable
technology and industry. MIT Press Books, 1, 2005.
[44] JV Joshua, DO Alao, SO Okolie, and O Awodele. Software ecosystem: Features, beneﬁts and challenges. 2013.
[45] Douglas Laney. 3D data management: Controlling data volume, velocity, and variety. Technical report, META
Group, February 2001.
[46] Christine L Borgman. Big Data, little data, no data: Scholarship in the networked world. Mit Press, 2015.
[47] Benjamin Fish, Jeremy Kun, Adám D Lelkes, Lev Reyzin, and György Turán. On the computational complexity
of mapreduce. In International Symposium on Distributed Computing, pages 1–15. Springer, 2015.
[48] James G Shanahan and Laing Dai. Large scale distributed data science using apache spark. In Proceedings of
the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 2323–2324.
ACM, 2015.
[49] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J
Franklin, Scott Shenker, and Ion Stoica. Resilient distributed datasets: A fault-tolerant abstraction for inmemory cluster computing. In Proceedings of the 9th USENIX conference on Networked Systems Design and
Implementation, pages 2–2. USENIX Association, 2012.
[50] Zoie SY Wong, Jiaqi Zhou, and Qingpeng Zhang. Artiﬁcial intelligence for infectious disease big data analytics.
Infection, disease & health, 24(1):44–48, 2019.
[51] Kaiyuan Sun, Jenny Chen, and Cécile Viboud. Early epidemiological analysis of the coronavirus disease 2019
outbreak based on crowdsourced data: a population-level observational study. The Lancet Digital Health, 2020.
[52] Israel Edem Agbehadji, Bankole Osita Awuzie, Alfred Beati Ngowi, and Richard C Millham. Review of big
data analytics, artiﬁcial intelligence and nature-inspired computing models towards accurate detection of covid19 pandemic cases and contact tracing. International journal of environmental research and public health,
17(15):5330, 2020.
[53] Nicola Luigi Bragazzi, Haĳiang Dai, Giovanni Damiani, Masoud Behzadifar, Mariano Martini, and Jianhong
Wu. How big data and artiﬁcial intelligence can help better manage the covid-19 pandemic. International
Journal of Environmental Research and Public Health, 17(9):3176, 2020.
17

A preprint - May 15, 2021

[54] Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. Pig latin: a notso-foreign language for data processing. In Proceedings of the 2008 ACM SIGMOD international conference
on Management of data, pages 1099–1110. ACM, 2008.
[55] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, Zheng Shao, Prasad Chakka, Suresh Anthony, Hao Liu, Pete
Wyckoﬀ, and Raghotham Murthy. Hive: a warehousing solution over a map-reduce framework. Proceedings of
the VLDB Endowment, 2(2):1626–1629, 2009.
[56] Lars George. HBase: The Definitive Guide: Random Access to Your Planet-Size Data. " O’Reilly Media, Inc.",
2011.
[57] Dmitriy Lyubimov and Andrew Palumbo. Apache Mahout: Beyond MapReduce. CreateSpace Independent
Publishing Platform, 2016.
[58] Xiao Huang, Zhenlong Li, Yuqin Jiang, Xiaoming Li, and Dwayne Porter. Twitter reveals human mobility
dynamics during the covid-19 pandemic. PloS one, 15(11):e0241957, 2020.
[59] Ronald C Taylor. An overview of the hadoop/mapreduce/hbase framework and its current applications in
bioinformatics. BMC bioinformatics, 11(Suppl 12):S1, 2010.
[60] Jamie J Alnasir and Hugh P Shanahan. The application of hadoop in structural bioinformatics. Briefings in
bioinformatics, 21(1):96–105, 2020.
[61] Ian Foster, Yong Zhao, Ioan Raicu, and Shiyong Lu. Cloud computing and grid computing 360-degree compared.
In Grid Computing Environments Workshop, 2008. GCE’08, pages 1–10. Ieee, 2008.
[62] Madhuri D Bhavsar and Shrikant N Pradhan. Scavenging idle cpu cycles for creation of inexpensive supercomputing power. International Journal of Computer Theory and Engineering, 1(5):602, 2009.
[63] Elmar Krieger and Gert Vriend. Models@ home: distributed computing in bioinformatics using a screensaver
based approach. Bioinformatics, 18(2):315–318, 2002.
[64] Maxwell I Zimmerman, Justin R Porter, Michael D Ward, Sukrit Singh, Neha Vithani, Artur Meller, Upasana L
Mallimadugula, Catherine E Kuhn, Jonathan H Borowsky, Rafal P Wiewiora, et al. Citizen scientists create an
exascale computer to combat covid-19. BioRxiv.
[65] Adam L Beberg, Daniel L Ensign, Guha Jayachandran, Siraj Khaliq, and Vĳay S Pande. Folding@ home:
Lessons from eight years of volunteer distributed computing. In Parallel & Distributed Processing, 2009.
IPDPS 2009. IEEE International Symposium on, pages 1–8. IEEE, 2009.
[66] Luis Ferreira, Viktors Berstis, Jonathan Armstrong, Mike Kendzierski, Andreas Neukoetter, Masanobu Takagi,
Richard Bing-Wo, Adeeb Amir, Ryo Murakawa, Olegario Hernandez, et al. Introduction to grid computing with
globus. IBM redbooks, 9, 2003.
[67] IBM World Community Grid - about page. https://www.worldcommunitygrid.org/about_us/viewAboutUs.do,
2020. [Online; accessed 16-September-2020].
[68] A Sciaba, S Campana, M Litmaath, F Donno, JT Moscicki, N Magini, H Renshall, and J Andreeva. Computing
at the petabyte scale with the wlcg. Technical report, 2010.
[69] David P Anderson. Boinc: A system for public-resource computing and storage. In Fifth IEEE/ACM international
workshop on grid computing, pages 4–10. IEEE, 2004.
[70] Fabrizio Gagliardi. The egee european grid infrastructure project. In International Conference on High
Performance Computing for Computational Science, pages 194–203. Springer, 2004.
[71] Ruth Pordes, Don Petravick, Bill Kramer, Doug Olson, Miron Livny, Alain Roy, Paul Avery, Kent Blackburn,
Torre Wenaus, Frank Würthwein, et al. The open science grid. In Journal of Physics: Conference Series,
volume 78, page 012057. IOP Publishing, 2007.
18

A preprint - May 15, 2021

[72] Nayanah Siva. Uk gears up to decode 100 000 genomes from nhs patients. The Lancet, 385(9963):103–104,
2015.
[73] James Gallagher, BBC.
DNA project ’to make UK world genetic research
http://www.bbc.co.uk/news/health-28488313, 2014. [Online; accessed 21-January-2019].

leader’.

[74] Vivien Marx. The dna of a nation. Nature, 524(7566):503–505, 2015.

[75] Genomics England. 100,000 Genomes project by numbers. https://www.genomicsengland.co.uk/the-100000-genomes-p
2014. [Online; accessed 24-November-2019].
[76] Genomics
England.
Secretary
of
State
for
Health
and
Social
Care
announces
ambition
to
sequence
5
million
genomes
within
ﬁve
years.
https://www.genomicsengland.co.uk/matt-hancock-announces-5-million-genomes-within-five-years/,
2018. [Online; accessed 30-October-2020].

[77] HPC wire. Genomics England Scales Up Genomic Sequencing with Quantum ActiveScale Object Storage.
https://www.hpcwire.com/off-the-wire/genomics-england-scales-up-genomic-sequencing-with-quantum-ac
2020. [Online; accessed 22-September-2020].

[78] Genomics England.
Genomics England Research Environment - HPC (Helix) Migration 2020.
https://cnfl.extge.co.uk/display/GERE/HPC+%28Helix%29+Migration+2020#HPC(Helix)Migration2020-Chang
2020. [Online; accessed 22-September-2020].
[79] Erola Pairo-Castineira, Sara Clohisey, Lucĳa Klaric, Andrew Bretherick, Konrad Rawlik, Nicholas Parkinson,
Dorota Pasko, Susan Walker, Anne Richmond, Max Head Fourman, et al. Genetic mechanisms of critical illness
in covid-19. medRxiv, 2020.
[80] AKTP Au, V Curcin, M Ghanem, N Giannadakis, Y Guo, MA Jafri, M Osmond, A Oleynikov, AS Rowe, J Syed,
et al. Why grid-based data mining matters? ﬁghting natural disasters on the grid: from sars to land slides. In
UK e-science all-hands meeting (AHM 2004), Nottingham, UK, pages 121–126, 2004.
[81] Anthony Rowe, Dimitrios Kalaitzopoulos, Michelle Osmond, Moustafa Ghanem, and Yike Guo. The discovery
net system for high throughput bioinformatics. Bioinformatics, 19(suppl_1):i225–i231, 2003.
[82] Paul Ayris, Jean-Yves Berthou, Rachel Bruce, Stefanie Lindstaedt, Anna Monreale, Barend Mons, Yasuhiro
Murayama, Caj Södergård, Klaus Tochtermann, and Ross Wilkinson. Realising the european open science
cloud. 2016.
[83] Yasser Hassan, Sherry Ogg, and Hui Ge. Novel anti-sars-cov-2 mechanisms of fusion broad range anti-infective
protein ricin a chain mutant-pokeweed antiviral protein 1 (rtam-pap1) in silico. 2020.
[84] Yasser Hassan, Sherry Ogg, and Hui Ge. Expression of novel fusion antiviral proteins ricin a chain-pokeweed
antiviral proteins (rta-paps) in escherichia coli and their inhibition of protein synthesis and of hepatitis b virus
in vitro. BMC biotechnology, 18(1):47, 2018.
[85] Tsjerk A Wassenaar, Marc Van Dĳk, Nuno Loureiro-Ferreira, Gĳs Van Der Schot, Sjoerd J De Vries, Christophe
Schmitz, Johan Van Der Zwan, Rolf Boelens, Andrea Giachetti, Lucio Ferella, et al. Wenmr: structural biology
on the grid. Journal of Grid Computing, 10(4):743–767, 2012.
[86] Ren Kong, Feng Wang, Jian Zhang, Fengfei Wang, and Shan Chang. Codockpp: A multistage approach for global
and site-speciﬁc protein–protein docking. Journal of chemical information and modeling, 59(8):3556–3564,
2019.
[87] Joel T Dudley, Yannick Pouliot, Rong Chen, Alexander A Morgan, and Atul J Butte. Translational bioinformatics
in the cloud: an aﬀordable alternative. Genome medicine, 2(8):51, 2010.
[88] Lin Dai, Xin Gao, Yan Guo, Jingfa Xiao, and Zhang Zhang. Bioinformatics clouds for big data manipulation.
Biology direct, 7(1):43, 2012.
19

A preprint - May 15, 2021

[89] Michael C Schatz, Ben Langmead, and Steven L Salzberg. Cloud computing and the dna data race. Nature
biotechnology, 28(7):691, 2010.
[90] Hugh P Shanahan, Anne M Owen, and Andrew P Harrison. Bioinformatics on the cloud computing platform
azure. PloS one, 9(7):e102642, 2014.
[91] James E Smith and Ravi Nair. The architecture of virtual machines. Computer, 38(5):32–38, 2005.
[92] Andrew J Younge, Robert Henschel, James T Brown, Gregor Von Laszewski, Judy Qiu, and Geoﬀrey C Fox.
Analysis of virtualization technologies for high performance computing environments. In Cloud Computing
(CLOUD), 2011 IEEE International Conference on, pages 9–16. IEEE, 2011.
[93] Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, and
Andrew Warﬁeld. Xen and the art of virtualization. In ACM SIGOPS operating systems review, volume 37,
pages 164–177. ACM, 2003.
[94] Vivek Navale and Philip E Bourne. Cloud computing applications for biomedical science: A perspective. PLoS
computational biology, 14(6):e1006144, 2018.
[95] Peter Mell, Tim Grance, et al. The nist deﬁnition of cloud computing. 2011.
[96] Amazon. Amazon EMR (Elastic MapReduce). https://aws.amazon.com/emr/, 2016. [Online; accessed
14-April-2019].
[97] Thilina Gunarathne, Tak-Lon Wu, Judy Qiu, and Geoﬀrey Fox. Mapreduce in the clouds for science. In
Cloud Computing Technology and Science (CloudCom), 2010 IEEE Second International Conference on, pages
565–572. IEEE, 2010.
[98] Miller,
S.
COVID
researchers
can
apply
for
free
cloud
https://gcn.com/articles/2020/03/24/cloud-vendors-covid-research.aspx, 2020.
accessed 09-September-2020].

services.
[Online;

[99] COVID-19 HPC Consortium.
COVID researchers can apply for free cloud services.
https://www.xsede.org/covid19-hpc-consortium, 2020. [Online; accessed 01-April-2020].
[100] Amazon AWS. Tech Against COVID: Rescale and Microsoft Azure donate supercomputing resources to help researchers combat global pandemic. https://partner.microsoft.com/ru-ru/case-studies/rescale,
2020. [Online; accessed 01-April-2020].

[101] Amazon
AWS.
COVID
researchers
can
apply
for
free
cloud
services.
https://aws.amazon.com/government-education/nonprofits/disaster-response/diagnostic-dev-initiative
2020. [Online; accessed 01-April-2020].

[102] Lifebit. Lifebit Provides Free Cloud Operating System, Data Hosting & Analysis Tools to COVID-19 Researchers.
https://blog.lifebit.ai/2020/03/30/lifebit-provides-free-cloud-operating-system-data-hosting-analy
2020. [Online; accessed 01-April-2020].

20

