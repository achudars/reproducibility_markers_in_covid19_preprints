arXiv:2006.14707v1 [cs.LG] 25 Jun 2020

Machine-Learning Driven Drug Repurposing for
COVID-19
Semih Cantürk ∗
Zetane Systems
1870 Boulevard des Sources, Suite 307
Pointe-Claire, Canada H9R 5N4
semih@zetane.com

Aman Singh ∗
Zetane Systems
1870 Boulevard des Sources, Suite 307
Pointe-Claire, Canada H9R 5N4
aman@zetane.com

Patrick St-Amant ∗
Zetane Systems
1870 Boulevard des Sources, Suite 307
Pointe-Claire, Canada H9R 5N4
patrick@zetane.com

Jason Behrmann
Zetane Systems
1870 Boulevard des Sources, Suite 307
Pointe-Claire, Canada H9R 5N4
jason@zetane.com

Abstract
The integration of machine learning methods into bioinformatics provides particular benefits in identifying how therapeutics effective in one context might have
utility in an unknown clinical context or against a novel pathology. We aim to
discover the underlying associations between viral proteins and antiviral therapeutics that are effective against them by employing neural network models. Using
the National Center for Biotechnology Information virus protein database and the
DrugVirus database, which provides a comprehensive report of broad-spectrum
antiviral agents (BSAAs) and viruses they inhibit, we trained ANN models with
virus protein sequences as inputs and antiviral agents deemed safe-in-humans as
outputs. Model training excluded SARS-CoV-2 proteins and included only Phases
II, III, IV and Approved level drugs. Using sequences for SARS-CoV-2 (the coronavirus that causes COVID-19) as inputs to the trained models produces outputs of
tentative safe-in-human antiviral candidates for treating COVID-19. Our results
suggest multiple drug candidates, some of which complement recent findings from
noteworthy clinical studies. Our in-silico approach to drug repurposing has promise
in identifying new drug candidates and treatments for other viruses.

∗

Equal contributions

Preprint. Under review.

1

Introduction

Artificial intelligence (AI) technology is a recent addition to bioinformatics that shows much promise
in streamlining the discovery of pharmacologically active compounds [1]. Machine learning (ML)
provides particular benefits in identifying how drugs effective in one context might have utility in
an unknown clinical context or against a novel pathology [2]. The application of ML in biomedical
research provides new means to conduct exploratory studies and high-throughput analyses using
information already available. In addition to deriving more value from past research, researchers can
develop ML tools in relatively short periods of time.
Past research now provides a sizable bank of information concerning drug-biomolecule interactions.
Using drug repurposing as an example, we can now train predictive algorithms to identify patterns in
how antiviral compounds bind to proteins from diverse virus species. We aim to train an ML model
so that when presented with the proteome of a novel virus, it will suggest antivirals based on the
protein segments present in the proteome. The final output from the model is a best-fit prediction as
to which known antivirals are likely to associate with those familiar protein segments.
These benefits are of particular interest for the current COVID-19 health crisis. The novelty of
SARS-CoV-2 requires that we execute health interventions based on past observations. Grappling
with an unforeseen pandemic with no known treatments or vaccines, the potential for rapid innovation
from ML is of utmost significance. The ability to conduct complex analyses with ML enables us
to research insights quickly that can help steer us in the right direction for future studies likely
to produce fruitful results. We present here multiple models that produced a number of antiviral
candidates for treating COVID-19. Out of our 12 top predicted drugs, 6 of them have shown positive
results in recent findings based on cell culture results and clinical trials. These promising antivirals
are lopinavir, ritonavir, ribavirin [3], cyclosporine, [4], rapamycin [5], and nitazoxanide [6]. For the 7
other predicted drugs, further research is needed to evaluate their effectiveness against SARS-CoV-2.

2

Method

2.1
2.1.1

Data
Sourcing and Preparation

We used two main data sources for this study. The first database was the DrugVirus database [7];
DrugVirus provides a database of broad-spectrum antiviral agents (BSAAs) and the associated viruses
they inhibit. The database covers 83 viruses and 126 compounds, and provides information on the
status as antiviral of each compound-virus pair. These statuses fall into eight categories representing
the progressive drug trial phases: Cell cultures/co-cultures, Primary cells/ organoids, Animal model,
Phases I-IV and Approved. See Appendix A for a more intuitive pivot table view of the database.
The second database is the National Center for Biotechnology Information (NCBI) Virus Portal
[8]; as of April 2020, this database provides approximately 2.8 million amino-acid and 2 million
nucleotide sequences from viruses with humans as hosts. Each row of this database contains an
amino acid sequence specimen from a study, as well as metadata that includes the associated virus
species. In our work, we considered sequences only from the 83 virus species in the DrugVirus
database or their subspecies in order to be able to merge the two data sources successfully. We also
constrained ourselves to amino-acid sequences only in the current iteration. The main reasons for this
are two-fold:
1. Amino-acid sequences are essentially derived from the DNA sequences, which may encode
overlapping information on different levels. In somewhat simplified terms, amino-acid
sequences are the outputs of a layer of preprocessing on genetic material (in the form of
DNA/RNA).
2. Nucleotide triplets (codons) map to amino-acids, making amino-acid sequences much shorter
and easier to extract features both in preprocessing and in the machine learning methods
themselves. Shorter sequences also mean the ML pipeline will be more resource-efficient,
i.e. easier to train.
The amino-acids were downloaded as three datasets: HIV types 1 & 2 (1,192,754 sequences),
Influenza types A, B & C (644,483 sequences), and the “Main” dataset for all other types including
2

SARS-CoV-2 (785,624 sequences). Each dataset came with two components. The “sequence”
component is composed on Accession IDs and the amino-acid sequence itself, while the “metadata”
component includes all other data (e.g. virus species, date specimen was taken, an identifier of the
related study) as well as the Accession ID to enable merging the two components.
The amount of research with a focus on Influenza and HIV naturally lead to these viruses comprising
most of the samples. In our experiments, we have excluded these viruses, and have worked only with
dataset #3, though the other datasets can be integrated into the main one during the class balancing
process, an idea we will discuss in Section 4, Future Work.
2.1.2

Preprocessing

The first step of the preparation phase was to merge the “sequence” and “metadata” components into
a single NCBI dataset based on sequence IDs. Afterwards, we mapped the "Species" column in this
main dataset to the Virus Name column in the DrugVirus database. This step was required as these
two columns that denote the virus species in the respective datasets did not match due to subspecies
present in the sequence dataset and alternative naming of some viruses.
Afterwards, we processed the DrugVirus dataset to a format suitable for merging with the NCBI
data frame. Every row of the DrugVirus dataset consists of a single drug-virus pairing and their
respective interaction/drug trial phase, meaning any given drug and virus appeared in multiple rows
of the dataset. We derived a new DrugVirus dataset that functioned as a dictionary where each unique
virus was a key, and the interactions with antivirals encoded as a multi-label binary vector (1 if viable
antiviral according to the original dataset, 0 if not) of length 126 (the number of antivirals) which
corresponded to the value. We came up with three “versions” depending on how we decided an
antiviral was a viable candidate to inhibit a virus. The criteria depended on drug trial phases:
1. In the first version, any interaction between a drug-virus pair is designated by a 1. This
means drugs that did not go past cell cultures/co-cultures or primary cells/organoids testing
are still considered viable candidates.
2. This second version expands upon the first stemming from our discovery that an attained
trial phase in the database does not necessarily mean previous phases were also listed in
the database. For example, we found that for a given virus, a given drug had undergone
Phase III testing, designated by a 1, but Phase I & II were listed as 0s. This undermined our
assumption that drug trials are hierarchical; though, in reality this is usually the case. This
can be caused by missing data reporting or possibly skipped phases. We proceeded with the
hierarchy assumption, and extended the database in (1) to account for the previous phases.
This meant that in this second version, an Approved drug will have all phases designated
with 1s, for example. Keeping track of the 8 phases meant that the size of the database also
grew by 8.
3. In the third version, we considered a drug-virus pair as viable only if it has attained Phase II
or further drug trials, signifying some success with human trials have been observed. In the
results presented in Section 3, our training database was based on this third version of the
DrugVirus database.
The full dataset was then generated by merging this “new” version of the DrugVirus dataset with
the NCBI dataset. We then generated two versions of this full dataset: one that consists of all
SARS-CoV-2 sequences and one that consists of all other viruses available. This enabled us to
compare how successful our models are in a case when they have not been trained on the virus species
at all and have to detect peptide substructures in the sequences to suggest antivirals. A sample of this
final database (with some columns excluded for brevity) is available in Appendix B.
Upon inspection of the data, we found that there were replete of duplicate or extremely similar virus
sequences. To reduce this exploitability and pose a more challenging problem, we removed the
duplicate sequences that belonged to the same species and had the exact same length. This reduced
the size of the dataset by approximately 98%. The counts for each virus species before and after
dropping duplicate viruses is available in Appendix C1 and C2.
3

2.1.3

Balancing

Our main database also contained a class imbalance in the number of times certain virus species
appeared in the database. We oversampled rare viruses (e.g., West Nile virus: 175 sequences) and
excluded the very rare species which compose less than 0.5% of the available unique samples in the
dataset (e.g., Andes virus: 4 sequences), and undersampled the common viruses (e.g., Hepatitis C:
16,040 sequences). This produced a more modest database of 30,479 amino acid sequences, with each
virus having samples in the 400–900 range (see Appendix C3). We kept the size of the dataset small
both to enable easier model training and validation in early iterations and to handle data imbalance
more smoothly.
The class imbalance problem also presented itself in the antiviral compounds. Even with balanced
virus classes, the number of times each drug occurred within the dataset varied, simply because some
drugs apply to more viruses than others. To alleviate this, we computed class weights for each drug,
which we then provided to the models in training. This enabled a fairer assessment and a more varied
distribution of antivirals in predicted outputs.
2.1.4

Train/Test Splitting

The final step of data processing involved generating the training and validation sets. We split the data
in two different ways, resulting in two different experiments (see Section 2.3, Experiment Setup for
the full experiment pipeline). Experiment I is based on a standard, randomized an 80% training/20%
validation split on the main dataset.
For Experiment II, we split the data on virus species, meaning the models were forced to predict drugs
for a species that it was not trained on, and have to detect peptide substructures in the amino-acid
sequences to suggest drugs. In this setup we also guaranteed that the SARS-CoV-2 sequences were
always in the test set, in addition to three other viruses randomly picked from the dataset. We used
a variant of this setup that trains on all virus sequences except SARS-CoV-2 and is validated on
SARS-CoV-2 only to generate the results presented in Section 3.

2.2

Models

A growing number of studies demonstrate the success of using artificial neural networks (ANN)
in evaluating biological sequences in drug repositioning and repurposing [9][10]. Previous work
on training neural networks on nucleotide or amino-acid sequences have been successful with
recurrent models such as gated recurrent units (GRU), long short-term memory networks (LSTM) and
bidirectional LSTMs (biLSTM), as well as 1D convolutions and 2D convolutional neural networks
(CNN) [11][12]. We have therefore focused on these network architectures, and conducted our
experiments with an LSTM with 1D convolutions and bidirectional layers as well as a CNN. The
network architectures are explained briefly below.
LSTM and 1D Convolutions For the LSTM, A character-level tokenizer was used to encode the
FASTA sequences into vectors consumable by the network. The sequences were then padded with
zeros or cut off to a fixed length 500 to maintain a fixed input size. The network architecture consisted
of an embedding layer, followed by 1D convolution and bidirectional LSTM layers (each followed by
maxpooling), and two fully connected layers. A more detailed architecture diagram is available in
Appendix D.
Convolutional Neural Network (CNN) For the CNN, the input features were one-hot encoded
based on the FASTA alphabet/charset, which assisted in interpretability when examining the 2D
input arrays as images. The inputs are also fixed at a length of 500, resulting in 500 x 28 images,
where 28 is the number of elements in the FASTA charset. The network architecture consists of four
2D convolutions with filter sizes of 1x28, 2x28, 3x28 and 5x28 respectively, which are maxpooled,
concatenated and passed through a fully connected layer. A more detailed architecture diagram is
available in Appendix E.
4

2.3

Experiment Setup

The experiments were run on a computer with an 2.7 GHz Intel Broadwell CPU (61 GB RAM) and
NVIDIA K80 GPU (12 GB). Both models completed a 20-epoch experiment in 60-90 minutes. One
to three training and evaluation runs were made for each setup during model and hyperparameter
selections, and ten training and evaluation runs were done to produce the average metrics in Section 3.
The experiments start by determining the model to use and apply the appropriate preprocessing steps
mentioned in Section 2.2. We then proceed with determining the dataset to train and validate on.
This part of the experiment setup is more extensively covered in Section 2.1.4, Train/Test Splitting.
We used binary cross entropy (BCE) loss, Adam optimizer, precision, recall and F1-score as metrics
since accuracy tends to be an unreliable metric given the class imbalance and the sparse nature of
our outputs. After training and validation, predictions were done on the validation set and the results
were post-processed for interpretability.
In post-processing, we applied a threshold to the sigmoid function outputs of the neural network,
where we assigned each drug a probability of being a potential antiviral for a given amino acid
sequence. After experimenting with different values, we settled on a threshold value of 0.2. Postprocessing outputs a list of drugs that were selected along with the respective probabilities for the drugs
being “effective” against the virus with the given amino acid sequence. For other hyperparameters
involved as well as information on hyperparameter tuning, see Appendix F.

3

Results

Here we present the results for the two experiments described in Section 2.1.4, Train/Test Splitting.
The figures and tables presented in this section are based on the LSTM and CNN architectures
described in Section 2.2, which were trained on 128 batch size and 0.01 and 0.001 learning rates
respectively for 20 epochs with an Adam optimizer.
3.1

Experiment I: Train/Test Split

In the regular setup, we performed an 80%/20% train-test split on our data of 30,479 sequences. The
metrics for the best set of hyperparameters (based on validation set F1-score) for both the CNN and
LSTM architectures respectively are presented in Table 1. Similarly, plots for the same set of models
and hyperparameters over 20 epochs are presented in Figures 1 and 2.
Our models handled the task successfully, achieving 0.958 F1-score in a multi-label multi-class
problem setting. This means that the models were able to match the virus species with the sequence
substructures and appropriately assign the inhibiting antivirals with accuracy. These satisfactory
results led to us implementing Experiment II.
3.2

Experiment II: Predictions on Unseen Virus Species

In Experiment II, the models predicted antiviral drugs for virus species they haven’t been trained
on. This meant the models were not able to recommend drugs by “recognizing” the virus from the

Table 1: Experiment I, metrics with 95% confidence intervals for optimal hyperparameters tested.
Training set
Model

Accuracy

Precision

Recall

F1-score

Loss

LSTM
CNN

0.998 ± 2.3e-4
0.999 ± 1e-4

0.951 ± 0.0059
0.992 ± 0.0023

0.914 ± 0.01
0.989 ± 0.0047

0.932 ± 0.0081
0.990 ± 0.0035

0.0126
0.00265

Validation set
Model

Accuracy

Precision

Recall

F1-score

Loss

LSTM
CNN

0.998 ± 2.1e-4
0.999 ± 6e-5

0.956 ± 0.011
0.986 ± 0.0019

0.892 ± 0.012
0.948 ± 0.0039

0.923 ± 0.0074
0.967 ± 0.0022

0.0134
0.00767

5

1.0

1.0

0.8

0.8

0.6

0.6
0.4

0.4
accuracy
f1
precision
recall

0.2
0.0

val_accuracy
val_f1
val_precision
val_recall

0.2
0.0

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

(a) Training set

(b) Validation set

Figure 1: Metrics for Experiment I over 20 epochs for the LSTM with 95% confidence intervals,
batch size = 128, LR = 0.001

1.000

1.000

0.975

0.975

0.950

0.950

0.925

0.925

0.900

0.900
0.875

0.875
accuracy
f1
precision
recall

0.850
0.825
0.800

val_accuracy
val_f1
val_precision
val_recall

0.850
0.825
0.800

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

(a) Training set

(b) Validation set

Figure 2: Metrics for Experiment I over 20 epochs for the CNN with 95% confidence intervals, batch
size = 128, LR = 0.01

sequence and therefore had to rely only on peptide substructures in the sequences to assign drugs. In
the results presented below, the test set consists of SARS-CoV-2, Herpes simplex virus 1, Human
Astrovirus and Ebola virus, whose sequences were removed from the training set.
We see here that the CNN (and the LSTM) had issues with convergence, and the accuracies are clearly
below their counterparts in the regular setup, though this is certainly expected. We now turn to the
actual predictions on the sequences and attempt to interpret them.
Upon examination of drug predictions for Herpes simplex virus 1 (HSV-1), however, we see that our
CNN was in fact quite successful. In Table 3 and Table 4, Count represents how many times each
drug was flagged as potentially effective for HSV-1 sequences, and Mean Probability denotes the
average confidence predicted over all instances of the drug. A sample of the outputs where these
metrics are derived from is available in Appendix G. Antivirals used for Phase II and further trials for
HSV-1 are highlighted in bold, meaning all six drugs in the database that are used for Phase II and
further trials are predicted by our model. Three of the top five predictions are approved antivirals
for HSV-1 and the only remaining one is predicted 11th among 126 antivirals. This high level of
accuracy is remarkable given that our model has not been trained on HSV-1 sequences.
Predictions for SARS-CoV-2 With some variation between the two, both the LSTM (Table 4a)
and the CNN (Table 4b) seem to converge on a number of drugs: ritonavir, lopinavir (both Phase III
6

Table 2: Experiment II, metrics with 95% confidence intervals for optimal hyperparameters tested.
Training set
Model

Accuracy

Precision

Recall

F1-score

Loss

LSTM
CNN

0.998 ± 5e-4
0.999 ± 1.6e-4

0.942 ± 0.011
0.990 ± 0.0038

0.877 ± 0.029
0.983 ± 0.0085

0.908 ± 0.021
0.986 ± 0.0063

0.00369
0.00325

Validation set
Model

Accuracy

Precision

Recall

F1-score

Loss

LSTM
CNN

0.958 ± 0.0019
0.960 ± 7.9e-4

0.530 ± 0.048
0.588 ± 0.026

0.227 ± 0.014
0.239 ± 0.0068

0.318 ± 0.0099
0.340 ± 0.0033

3.764
7.562

1.0

1.000

0.9

0.975

0.7

0.925

0.6

0.900

0.5

0.875

0.4

accuracy
f1
precision
recall

0.850
0.825
0.800

val_accuracy
val_f1
val_precision
val_recall

0.8

0.950

0.3
0.2

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

(a) Training set

(b) Validation set

Figure 3: Metrics for Experiment II over 20 epochs for the CNN with 95% confidence intervals, batch
size = 128, LR = 0.01

Table 3: Experiment II with CNN, Count/Mean Probability table for 588 HSV-1 sequences
Antiviral

Count

Mean Probability

Valacyclovir
Cidofovir
Foscarnet
Brincidofovir
Aciclovir
Rapamycin
Ganciclovir
Artesunate
Cyclosporine
Letermovir
Tilorone
Tenofovir

578
567
566
559
525
419
410
333
328
317
243
173

0.899 ± 0.0047
0.737 ± 0.017
0.904 ± 0.0066
0.646 ± 0.048
0.759 ± 0.0081
0.540 ± 0.017
0.552 ± 0.064
0.453 ± 0.026
0.449 ± 0.094
0.430 ± 0.02
0.433 ± 0.071
0.573 ± 0.017

Drugs with more than 100 counts are listed.
Antivirals used for Phase II and further trials
for HSV-1 are highlighted in bold. Mean
probabilities are provided with 95% confidence intervals.

7

Table 4: Experiment II, Count/Mean Probability table for 97 SARS-CoV-2 sequences, compounded
results of 10 trials. Top 20 antivirals shown.
Antiviral

Count

Mean Probability

Antiviral

Count

Mean Probability

Tilorone
Brincidofovir
Ganciclovir
Ritonavir
Lopinavir
Valacyclovir
Cidofovir
Foscarnet
Artesunate
Nitazoxanide
Rapamycin
Pleconaril
Cyclosporine
Chloroquine
Letermovir
Aciclovir
Ribavirin
CYT107
Tenofovir
Alisporivir

321
197
113
106
106
93
83
58
55
48
32
30
24
21
19
19
17
5
5
3

0.627 ± 0.029
0.457 ± 0.023
0.469 ± 0.040
0.589 ± 0.049
0.588 ± 0.049
0.522 ± 0.046
0.400 ± 0.039
0.473 ± 0.047
0.385 ± 0.048
0.359 ± 0.048
0.399 ± 0.065
0.626 ± 0.059
0.429 ± 0.084
0.499 ± 0.11
0.395 ± 0.089
0.347 ± 0.077
0.415 ± 0.10
0.369 ± 0.39
0.340 ± 0.20
0.550 ± 0.80

Tilorone
Brincidofovir
Ritonavir
Lopinavir
Valacyclovir
Ganciclovir
Cidofovir
Nitazoxanide
Artesunate
Rapamycin
Foscarnet
Cyclosporine
Ribavirin
Aciclovir
Chloroquine
Thymalfasin
Azithromycin
CYT107
Tenofovir
Letermovir

177
116
114
93
35
31
26
24
23
12
18
10
9
8
6
5
5
3
2
2

0.885 ± 0.031
0.807 ± 0.044
0.808 ± 0.046
0.881 ± 0.030
0.748 ± 0.085
0.793 ± 0.089
0.694 ± 0.10
0.767 ± 0.13
0.740 ± 0.10
0.769 ± 0.14
0.809 ± 0.11
0.780 ± 0.22
0.731 ± 0.23
0.671 ± 0.20
0.938 ± 0.064
0.826 ± 0.38
0.648 ± 0.49
0.540 ± 0.94
0.990 ± 0.11
0.550 ± 0.88

(a) LSTM

(b) CNN

for MERS-CoV), tilorone (Approved for MERS-CoV) and brincidofovir are in the top five candidates
in both, while valacyclovir, ganciclovir, rapamycin and cidofovir rank high up in both lists. Most
of the remaining drugs are present in both lists as well. The LSTM is more conservative in its
predictions than the CNN, and the overall counts for SARS-CoV-2 are significantly lower than for
Herpes simplex virus 1 for both, pointing a comparable lack of confidence on the models’ part in
predicting SARS-CoV-2 sequences.
A further step we took for the SARS-CoV-2 sequences was visualizing the layer activations in the
Zetane Engine to validate that the model was processing the data at a fine-grained level. This was
done in similar fashion to a study where integrated gradients were used to generate attributions on a
neural network performing molecule classification [13]. The layer activations in both models showed
that different antivirals activated different subsequences of a given sequence at the amino acid level,
thus validating our approach. The filter activations are available in Appendix H.

4

Discussion and Future Work

The preliminary results of our experiments show promise and merit further investigation. We note that
our ML models predict that some antivirals that show promise as treatments against MERS-CoV may
also be effective against SARS-CoV-2. These include the broad-spectrum antiviral tilorone [14] and
the drug lopinavir [15], the latter of which is now in Phase IV clinical trials to determine its efficacy
against COVID-19 [16]. Such observations suggest with confidence that our models can recognize
reliable patterns between particular antivirals and species of viruses containing homologous amino
acid sequences in their proteome.
Additional observations that support our findings have come to light from a study in The Lancet
published shortly before this article [3]. This open-label, randomized, Phase II trial observed that the
combined administration of the drugs interferon beta-1b, lopinavir, ritonavir and ribavirin provides
an effective treatment of COVID-19 in patients with mild to moderate symptoms. Both of our models
flagged three of the drugs in that trial (note that interferon was not part of our datasets). In terms of
8

number of occurrences aka Count, ritonavir, lopinavir and ribavirin were ranked 4th, 5th and 11th by
the LSTM, while the CNN model ranked them 3rd, 4th and 10th, respectively.
Other studies have also focused on the treatment of SARS-CoV-2 by drugs predicted in our experiments. Wang et al. discovered that nitazoxanide (LSTM rank 10th, CNN rank 8th) inhibited
SARS-CoV-2 at a low-micromolar concentration [6]. Gordon et al. suggest promising results in viral
growth and cytotoxicity assays with rapamycin as an inhibitor (11th & 10th) [5], while de Wilde et al.
point out that cyclosporine (13th & 12th) is known to be effective against diverse coronaviruses [4].
Such observations are encouraging. They demonstrate that predictive models may have value in
identifying potential therapeutics that merit priority for advanced clinical trials. They also add to
growing observations that support using ML to streamline drug discovery. From that perspective,
our models suggest that the broad spectrum antiviral tilorone, for instance, may be a top candidate
for COVID-19 clinical trials in the near future. Other candidates highlighted by our results and may
merit further studies are brincidofovir, foscarnet, artesunate, cidofovir, valacyclovir and ganciclovir.
The antivirals identified here have some discrepancies with emerging research findings as well. For
instance, our models did not highlight the widely available anti-parasitic ivermectin. One research
study observed that ivermectin could inhibit the replication of SARS-CoV-2 in vitro [17]. Another
large-scale drug repositioning survey screened a library of nearly 12,000 drugs and identified six
candidate antivirals for SARS-CoV-2: PIKfyve kinase inhibitor Apilimod, cysteine protease inhibitors
MDL-28170, Z LVG CHN2, VBY-825, and ONO 5334, and the CCR1 antagonist MLN-3897 [18]. It
comes as no surprise that our models did not identify these compounds as our data sources did not
contain them. Future efforts to strengthen our ML models will thus require us to integrate a growing
bank of novel data from emerging research findings into our ML pipeline.
In terms of our machine learning models, better feature extraction can improve predictions drastically.
This step involves improvements through better data engineering and working with domain experts
who are familiar with applied bioinformatics to better understand the nature of our data and find ways
to improve our data processing pipeline. Some proposals for future work that could strengthen the
performance of our machine learning process are as follows:
1. Deeper interaction with domain experts and further lab testing would lead to a better
understanding of the antivirals and the amino-acid sequences they target, leading to building
better ML pipelines for drug repurposing.
2. Better handling of duplicates can improve the quality of data available. The current approach
(which is based on species and sequence length) can be improved through using string
similarity measures such as Dice coefficient, cosine similarity, Levenshtein distance etc.
3. Influenza and HIV datasets should be integrated into the data generation and processing
pipeline to enhance available data.
4. Vectorizers can be used to extract features as n-grams (small sequences of chars), which
has attained success in similar problems [19]. Other unsupervised learning methods such as
singular value decomposition also may be applicable to our study [20].

Broader Impact
We hope that the machine learning approaches and pipelines developed here may provide longterm benefit to public health. The fact that our results show much promise in streamlining drug
discovery for SARS-CoV-2 motivates us to adapt our current models so we can conduct identical
drug repurposing assessments for other known viruses. Moreover, experimental data suggests that
our approaches are generalizable to other viruses (see the HSV-1 example in Section 3.2, Experiment
II) - we are therefore confident that we could adapt our models to conduct equivalent studies during
the next outbreak of a novel virus. This also means our methods can be used to repurpose existing
drugs in order to find more potent treatments for known viruses.
The direct beneficiaries of our findings are members of the clinical research community. Using
relatively few resources, ML-guided drug repurposing technology can help prioritize clinical investigations and streamline drug discovery. In addition to reducing costs and expediting clinical
innovation, such efficiency gains may reduce the number of clinical trials – and thus human subjects
used in risky research – needed to find effective treatments (this pertains to the ethical imperative
9

to avoid harm when possible). Also of importance is that in-silico analyses using machine learning
provide yet another means to employ past research findings in new investigations. ML-guided drug
repurposing thus provides means to obtain further value from knowledge on-hand; maximizing value
in this case is laudable on many fronts, especially in terms of providing maximum benefit from
publicly-funded research.
The negative consequences that could arise should our models fail appear limited but are noteworthy
nonetheless. Note that our models aim to only indicate possible therapeutics that merit further clinical
investigation in order to prove any antiviral activity against SARS-CoV-2. Should our models fail by
recommending spurious treatments, these incorrect predictions may divert limited time and resources
towards frivolous investigations. It should also be noted that our methods aim to primarily work as
guidance for medical experts, and not as a be-all-end-all solution. And any incorrect inferences made
by our models are bound to be detected early by medical experts.
Communicating any machine-learning predictions of tentative antiviral drugs from this study requires
much caution. The current pandemic continues to demonstrate how fear, misinformation and a lack
of knowledge about a novel communicable disease can encourage counterproductive health-seeking
behaviour amongst the public. Soon after the coronavirus became a widely understood threat, the
internet was awash in false – sometimes downright harmful – information about preventing and
treating COVID-19. Included within this misleading health information were premature claims by
some prominent government officials that therapeutics like chloroquine and hydroxychloroquine
might hold promise as a repurposed drug for COVID-19. Such unfounded advice caused avoidable poisonings from people self-medicating with chloroquine. Subsequent clinical investigations
demonstrated no notable benefit and potential adverse reactions to chloroquine when used to treat
COVID-19. Such unfortunate events remind us that preliminary findings may be misinterpreted as
conclusive treatments or as evidence to support inconclusive health claims.

Acknowledgments and Disclosure of Funding
We would like to thank the administrators of the DrugVirus and the NCBI Virus Portal for providing
the datasets that are central to this study. We appreciate comments on preliminary drafts of this
manuscript from Dr Tariq Daouda from the Massachusetts General Hospital, Broad Institute, Harvard
Medical school.
The authors declare they will not obtain any direct financial benefit from investigating and reporting on
any given pharmaceutical compound. The following study is funded by the authors’ employer, Zetane
Systems, which produces software for AI technologies implemented in industrial and enterprise
contexts.

References
[1]

[2]

[3]

[4]

[5]

N. Stephenson, E. Shane, J. Chase, et al., “Survey of Machine Learning Techniques in Drug
Discovery,” en, Current Drug Metabolism, vol. 20, no. 3, pp. 185–193, May 2019, ISSN:
13892002. DOI: 10.2174/1389200219666180820112457.
F. Napolitano, Y. Zhao, V. M. Moreira, et al., “Drug repositioning: A machine-learning
approach through data integration,” en, Journal of Cheminformatics, vol. 5, no. 1, p. 30, Dec.
2013, ISSN: 1758-2946. DOI: 10.1186/1758-2946-5-30.
I. F.-N. Hung, K.-C. Lung, E. Y.-K. Tso, et al., “Triple combination of interferon beta-1b,
lopinavir–ritonavir, and ribavirin in the treatment of patients admitted to hospital with COVID19: An open-label, randomised, phase 2 trial,” en, The Lancet, S0140673620310424, May
2020, ISSN: 01406736. DOI: 10.1016/S0140-6736(20)31042-4.
A. H. de Wilde, J. C. Zevenhoven-Dobbe, Y. van der Meer, et al., “Cyclosporin A inhibits
the replication of diverse coronaviruses,” en, Journal of General Virology, vol. 92, no. 11,
pp. 2542–2548, Nov. 2011, ISSN: 0022-1317, 1465-2099. DOI: 10.1099/vir.0.034983-0.
D. E. Gordon, G. M. Jang, M. Bouhaddou, et al., “A SARS-CoV-2 protein interaction map
reveals targets for drug repurposing,” en, Nature, Apr. 2020, ISSN: 0028-0836, 1476-4687.
DOI : 10.1038/s41586-020-2286-9.

10

[6]

[7]

[8]

[9]

[10]

[11]
[12]

[13]

[14]

[15]

[16]
[17]

[18]
[19]
[20]

M. Wang, R. Cao, L. Zhang, et al., “Remdesivir and chloroquine effectively inhibit the recently
emerged novel coronavirus (2019-nCoV) in vitro,” en, Cell Research, vol. 30, no. 3, pp. 269–
271, Mar. 2020, ISSN: 1001-0602, 1748-7838. DOI: 10.1038/s41422-020-0282-0.
P. I. Andersen, A. Ianevski, H. Lysvand, et al., “Discovery and development of safe-in-man
broad-spectrum antiviral agents,” en, International Journal of Infectious Diseases, vol. 93,
pp. 268–276, Apr. 2020, ISSN: 12019712. DOI: 10.1016/j.ijid.2020.02.018.
J. R. Brister, D. Ako-adjei, Y. Bao, and O. Blinkova, “NCBI Viral Genomes Resource,” en,
Nucleic Acids Research, vol. 43, no. D1, pp. D571–D577, Jan. 2015, ISSN: 1362-4962, 03051048. DOI: 10.1093/nar/gku1207.
Y. Donner, S. Kazmierczak, and K. Fortney, “Drug Repurposing Using Deep Embeddings of
Gene Expression Profiles,” en, Molecular Pharmaceutics, vol. 15, no. 10, pp. 4314–4325, Oct.
2018, ISSN: 1543-8384, 1543-8392. DOI: 10.1021/acs.molpharmaceut.8b00284.
X. Zeng, S. Zhu, X. Liu, et al., “deepDR: A network-based deep learning approach to in silico
drug repositioning,” en, Bioinformatics, vol. 35, no. 24, L. Cowen, Ed., pp. 5191–5198, Dec.
2019, ISSN: 1367-4803, 1460-2059. DOI: 10.1093/bioinformatics/btz418.
T. K. Lee and T. Nguyen, “Protein Family Classification with Neural Networks,” en, 2016.
J. Hou, B. Adhikari, and J. Cheng, “DeepSF: Deep convolutional neural network for mapping
protein sequences to folds,” en, Bioinformatics, vol. 34, no. 8, A. Valencia, Ed., pp. 1295–1303,
Apr. 2018, ISSN: 1367-4803, 1460-2059. DOI: 10.1093/bioinformatics/btx780.
K. McCloskey, A. Taly, F. Monti, et al., “Using attribution to decode binding mechanism in
neural network models for chemistry,” en, Proceedings of the National Academy of Sciences,
p. 201 820 657, May 2019, ISSN: 0027-8424, 1091-6490. DOI: 10.1073/pnas.1820657116.
S. Ekins, T. R. Lane, and P. B. Madrid, “Tilorone: A Broad-Spectrum Antiviral Invented in the
USA and Commercialized in Russia and beyond,” en, Pharmaceutical Research, vol. 37, no. 4,
p. 71, Apr. 2020, ISSN: 0724-8741, 1573-904X. DOI: 10.1007/s11095-020-02799-8.
T. Yao, J. Qian, W. Zhu, et al., “A systematic review of lopinavir therapy for SARS coronavirus
and MERS coronavirus—A possible reference for coronavirus disease-19 treatment option,”
en, Journal of Medical Virology, vol. 92, no. 6, pp. 556–563, Jun. 2020, ISSN: 0146-6615,
1096-9071. DOI: 10.1002/jmv.25729.
S. H. Basha, “Corona virus drugs – a brief overview of past, present and future,” en, vol. 2, no.
2, p. 16, 2020.
L. Caly, J. D. Druce, M. G. Catton, et al., “The FDA-approved drug ivermectin inhibits the
replication of SARS-CoV-2 in vitro,” en, Antiviral Research, vol. 178, p. 104 787, Jun. 2020,
ISSN : 01663542. DOI : 10.1016/j.antiviral.2020.104787.
L. Riva, S. Yuan, X. Yin, et al., “A Large-scale Drug Repositioning Survey for SARS-CoV-2
Antivirals,” en, Microbiology, preprint, Apr. 2020. DOI: 10.1101/2020.04.16.044016.
B. Szalkai and V. Grolmusz, “Near Perfect Protein Multi-Label Classification with Deep
Neural Networks,” en, ArXiv:1703.10663 [cs, q-bio, stat], Mar. 2017, arXiv: 1703.10663.
C. Wu, M. Berry, S. Shivakumar, and J. McLarty, “Neural networks for full-scale protein
sequence classification: Sequence encoding with singular value decomposition,” en, Machine
Learning, vol. 21, no. 1-2, pp. 177–193, 1995, ISSN: 0885-6125, 1573-0565. DOI: 10.1007/
BF00993384.

11

Appendices

Table of Contents
Appendix A The DrugVirus database displayed in a pivot table

13

Appendix B Merged database sample

14

Appendix C Database profile
C.1 Virus counts before dropping duplicate sequences . . . . . . . . . . . . . . . .
C.2 Virus counts after dropping duplicate sequences . . . . . . . . . . . . . . . . .
C.3 Virus counts after dropping duplicate sequences and balancing . . . . . . . . . .

15
15
16
17

Appendix D LSTM Architecture

18

Appendix E CNN Architecture

19

Appendix F Hyperparameter and Architecture Selection

20

Appendix G Output table sample

21

Appendix H Filter Activation Visualizations

22

12

A

The DrugVirus database displayed in a pivot table

13

14

Varicella zoster virus
Varicella zoster virus
Varicella zoster virus
Hepatitis A virus
Herpes simplex virus 1
Enterovirus A
Enterovirus B
Norovirus
Hepatitis C virus
Respiratory syncytial virus

YIDPVVVLDF
YIDPVVVLDF
TGFYIDPVVV
ANYNHSDEYL
STTQPQLQTT
NDPITNAVES
MGAQVSTQKT
IFEKHSRYKY
GCSFSIFLLA
KKDLKPQTT

DNA polymerase catalytic subunit, partial
DNA polymerase catalytic subunit, partial
DNA polymerase catalytic subunit, partial
VP1, partial
glycoprotein G, partial
VP1, partial
polyprotein, partial
RNA replicase, partial
polyprotein, partial
glycoprotein, partial

GenBank_Title
0
0
0
0
0
0
0
0
0
0

4-HPR
0
0
0
0
0
0
0
0
0
0

ABT-263

1
1
1
0
1
0
0
0
0
0

Aciclovir

0
0
0
0
0
0
0
0
1
0

Alisporivir

Table 5: A section of the merged database used in model training. Each sequence is associated with a virus species and paired with a length-126
binary vector of antiviral drugs (only 4 elements of the vector are visible), where a 1 denotes effectiveness linked with a given amino acid
sequence. Only the first 10 elements of the amino acid sequence are displayed for brevity.

Virus Name

FASTA Sequence

B
Merged database sample

C
C.1

Database profile
Virus counts before dropping duplicate sequences

Table 6: Virus counts before dropping duplicate sequences
Virus Name

Count

Virus Name

Count

Hepatitis C virus
Epstein-Barr virus
Cytomegalovirus
Norovirus
Dengue virus
Hepatitis B virus
Herpes simplex virus 1
Ebola virus
Respiratory syncytial virus
Varicella zoster virus
Herpes simplex virus 2
Human papillomavirus
Adenovirus
Human herpesvirus 6
Human parainfluenza virus 2
Enterovirus A
Chikungunya virus
Measles virus
Lassa virus
MERS coronavirus
Enterovirus C
Enterovirus D
SARS coronavirus
Zika virus
HHV-8
SFTS virus
Enterovirus B
Human coronavirus OC43
Human metapneumovirus
Hepatitis E virus
Human parainfluenza virus 1
Molluscum contagiosum virus

29620
16040
13089
7924
7720
7642
7039
6250
6067
6061
5890
4613
4394
3622
2518
2511
2105
1672
1222
1171
1095
1063
867
845
838
835
831
795
761
694
591
491

Parvovirus B19
Human coronavirus strain NL63
Human rhinovirus A
Parechovirus A3
Crimean-Congo hemorrhagic fever virus
Rabies virus
Yellow fever virus
Ross River virus
West Nile virus
Marburg virus
Human coronavirus strain 229E
Japanese encephalitis virus
Hepatitis A virus
Rubella virus
Human rhinovirusÂ B
Vaccinia virus
Human herpesvirus 7
Rift Valley fever virus
Nipah virus
Hantavirus
Tick-borne encephalitis virus
John Cunningham virus
Saffold virus
Human Astrovirus
Junin virus
LCM virus
Sindbis virus
Bunyamwera virus
Powassan virus
Andes virus
Sin Nombre virus

435
317
295
289
238
220
198
189
175
168
156
153
149
131
110
107
107
100
81
76
76
50
49
39
28
21
12
8
5
4
3

15

C.2

Virus counts after dropping duplicate sequences

Table 7: Virus counts after dropping duplicate sequences
Virus Name

Count

Virus Name

Count

Hepatitis C virus
Dengue virus
Adenovirus
Cytomegalovirus
Hepatitis B virus
Herpes simplex virus 2
Herpes simplex virus 1
Norovirus
Epstein-Barr virus
Respiratory syncytial virus
Enterovirus B
Enterovirus A
Human papillomavirus
Chikungunya virus
Vaccinia virus
Human herpesvirus 6
Enterovirus D
Hepatitis E virus
Human rhinovirus A
Human metapneumovirus
Molluscum contagiosum virus
Enterovirus C
HHV-8
Parechovirus A3
Zika virus
Varicella zoster virus
MERS coronavirus
Hepatitis A virus
Parvovirus B19
Lassa virus
Ebola virus
Hantavirus
Human parainfluenza virus 2

870
691
657
654
644
635
588
579
545
474
468
417
386
358
348
322
301
293
289
284
284
271
264
247
223
213
211
207
200
188
164
162
149

Human rhinovirus B
Human Astrovirus
Measles virus
Crimean-Congo hemorrhagic fever virus
Saffold virus
Human coronavirus OC43
Human coronavirus strain NL63
Human herpesvirus 7
John Cunningham virus
Rabies virus
SFTS virus
West Nile virus
Rubella virus
Human parainfluenza virus 1
Hepatitis D virus
Human coronavirus strain 229E
Rift Valley fever virus
Yellow fever virus
Tick-borne encephalitis virus
Japanese encephalitis virus
Nipah virus
SARS coronavirus
Andes virus
Junin virus
LCM virus
Ross River virus
Marburg virus
Sin Nombre virus
Sindbis virus
Bunyamwera virus
Powassan virus
BK virus

134
132
120
118
112
104
87
87
83
81
79
75
73
73
65
57
47
40
36
32
29
22
20
15
15
12
12
11
4
4
4
1

16

C.3

Virus counts after dropping duplicate sequences and balancing

Table 8: Virus counts after dropping duplicate sequences and balancing
Virus Name

Count

Enterovirus B
Hepatitis C virus
Human metapneumovirus
Enterovirus A
Human parainfluenza virus 1
HHV-8
Human papillomavirus
Parechovirus A3
Chikungunya virus
Human coronavirus strain NL63
Vaccinia virus
Human herpesvirus 7
Dengue virus
Zika virus
John Cunningham virus
Adenovirus
Ebola virus
Cytomegalovirus
Hantavirus
Rabies virus
Hepatitis B virus
Human herpesvirus 6
Varicella zoster virus
Herpes simplex virus 2
MERS coronavirus
SFTS virus
Hepatitis A virus
Enterovirus D
Measles virus
West Nile virus
Parvovirus B19
Human parainfluenza virus 2
Crimean-Congo hemorrhagic fever virus
Herpes simplex virus 1
Hepatitis E virus
Rubella virus
Norovirus
Human rhinovirus A
Molluscum contagiosum virus
Lassa virus
Saffold virus
Epstein-Barr virus
Enterovirus C
Human rhinovirus B
Human Astrovirus
Human coronavirus OC43
Respiratory syncytial virus

936
870
852
834
803
792
772
741
716
696
696
696
691
669
664
657
656
654
648
648
644
644
639
635
633
632
621
602
600
600
600
596
590
588
586
584
579
578
568
564
560
545
542
536
528
520
474

17

D

LSTM Architecture

Figure 4: LSTM architecture used. Number of trainable parameters: 1,740,266. "?" denotes the
batch dimension. For more information on architecture selection, see Section F, Hyperparameter and
Architecture Selection.

18

pt

19
Figure 5: CNN architecture used. Number of trainable parameters: 209,022. "?" denotes the batch dimension. For
more information on architecture selection, see Section F,
Hyperparameter and Architecture Selection.

E
CNN Architecture

F

Hyperparameter and Architecture Selection

Table 9: List of hyperparameters experimented with. The hyperparameters used for generating the
results in the article are highlighted in bold.
Hyperparameter
Learning rate
Batch size
Threshold
Epochs
Sequence length cutoff

LSTM

CNN

1e-1, 1e-2, 1e-3, 1e-4
32, 64, 128, 256, 512
0.9, 0.7, 0.5, 0.2, 0.1
5 to 30 (20)
500, 1000

1e-1, 1e-2, 1e-3, 1e-4
32, 64, 128, 256, 512
0.9, 0.7, 0.5, 0.2
5 to 30 (20)
500, 1000

The hyperparameters tested in our experiments are presented in Section F It is certainly possible to
improve the accuracies of our experiments by conducting a vaster coverage of the loss landscape
through more extensive training (e.g. running longer experiments with smaller learning rates on more
complex network architectures), especially for results in Experiment II. However, due to performance
constraints, the scope of hyperparameter tuning as well as the ANN architectures experimented on
are relatively constrained as we focused on the methodology as opposed to optimal performance
in this study. It should be noted that much improvement is possible in this front, as pointed out in
Discussion and Future Work. Additional notes regarding our observations during hyperparameter
tuning are presented below.
• For the threshold, we wanted to predict eagerly, i.e. we considered false negatives more costly
errors than false positives. A high threshold would mean the outputs would be composed
only of the antivirals our models are very confident about per amino acid sequence. This
we deem undesirable, as while we do hope these outputs narrow the scope of antivirals
to focus on, over-restricting could prevent antivirals that are predicted frequently yet with
low probability be detected. A low threshold such as 0.2 filtered the number of antivirals
sufficiently, but also left enough breathing room for the domain experts to draw their own
conclusions on a per-drug basis.
• While a larger sequence length cutoff was possible and not detrimental to the results, we
deemed 500 a suitable trade-off in terms of performance versus accuracy, as many sequences
do not reach lengths in the thousands to begin with.
• As mentioned, the number of epochs trained could be increased, as we did not see dramatic
signs of overfitting at 20 epochs or further. However, a flattening of the metrics were evident
around 20 epochs with the hyperparameters listed, which therefore was selected a suitable
stopping point.

20

21

SARS-CoV-2
SARS-CoV-2
SARS-CoV-2

MKFLVFLGII
MGYINVFAFPFT
MESLVPGFNE

ORF8 protein
ORF10 protein, partial
orf1ab polyprotein

GenBank_Title

Rapamycin, Mitoxantrone
Brincidofovir, Nitazoxanide, Tilorone
Lopinavir, Ritonavir, Tilorone,

Antivirals

0.994, 0.351
0.959, 0.865, 0.452
0.761, 0.653, 0.280

Probabilities

Table 10: A section of sample outputs for amino acid sequences and their associated antivirals. Post-processing outputs a list of
drugs that were selected along with the respective probabilities of the drugs being “effective” against the virus with the given
amino acid sequence.

Virus Name

FASTA Sequence

G
Output table sample

H

Filter Activation Visualizations

Figure 6: Activation visualizations of the LSTM layers for a given sequence

(a) MaxPool

(b) Conv1D

(c) ReLU

Figure 7: Filter activations for three different LSTM layers for a given amino acid sequence. Warmer
colors indicate higher activation regions.

22

(a) Conv2D

(b) Elu

(c) Concat

(d) Transpose

Figure 8: Filter activations for four different CNN layers for a given amino acid sequence. Warmer
colors indicate higher activation regions.

23

