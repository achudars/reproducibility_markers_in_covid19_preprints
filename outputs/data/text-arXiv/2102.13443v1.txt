1

Reverse-Bayes methods:
a review of recent technical advances

arXiv:2102.13443v1 [stat.ME] 26 Feb 2021

Leonhard Held

∗,§

, Robert Matthews

†,§

, Manuela Ott

∗,‡

, and Samuel Pawel

∗,§

Abstract. It is now widely accepted that the standard inferential toolkit used by
the scientific research community – null-hypothesis significance testing (NHST)
– is not fit for purpose. Yet despite the threat posed to the scientific enterprise,
there is no agreement concerning alternative approaches. This lack of consensus
reflects long-standing issues concerning Bayesian methods, the principal alternative to NHST. We report on recent work that builds on an approach to inference
put forward over 70 years ago to address the well-known “Problem of Priors” in
Bayesian analysis, by reversing the conventional prior-likelihood-posterior (“forward”) use of Bayes’s Theorem. Such Reverse-Bayes analysis allows priors to be
deduced from the likelihood by requiring that the posterior achieve a specified
level of credibility. We summarise the technical underpinning of this approach,
and show how it opens up new approaches to common inferential challenges, such
as assessing the credibility of scientific findings, setting them in appropriate context, estimating the probability of successful replications, and extracting more
insight from NHST while reducing the risk of misinterpretation. We argue that
Reverse-Bayes methods have a key role to play in making Bayesian methods more
accessible and attractive to the scientific community. As a running example we
consider a recently published meta-analysis from several randomized controlled
clinical trials investigating the association between corticosteroids and mortality
in hospitalized patients with COVID-19.
Keywords: Reverse-Bayes, Analysis of Credibility, Bayes factor, false positive
risk, prior-data conflict.

1

Introduction: the origin of Reverse-Bayes methods
“We can make judgments of initial probabilities and infer final
ones, or we can equally make judgments of final ones and infer
initial ones by Bayes’s theorem in reverse.”
Good (1983, p. 29)

There is now a common consensus that the most widely-used methods of statistical
inference have led to a crisis in both the interpretation of research findings and their
replication (e. g. Gelman and Loken, 2014; Wasserstein and Lazar, 2016). At the same

∗ Department

of Biostatistics, University of Zurich, leonhard.held@uzh.ch, samuel.pawel@uzh.ch
of Mathematics, Aston University, rajm@physics.org
‡ Data Team, Swiss National Science Foundation, manuela.ott@snf.ch
§ Supported by the Swiss National Science Foundation (http://p3.snf.ch/Project-189295)
† Department

2
time, there is a lack of consensus on how to address the challenge, as highlighted by the
plethora of alternative techniques to null-hypothesis significance testing now being put
forward (see e. g. Wasserstein et al., 2019, and references therein). Especially striking
is the relative dearth of alternatives based on Bayesian concepts. Given their intuitive
inferential basis and output (see e. g. Wagenmakers et al., 2008; McElreath, 2018, or
some other textbook), these would seem obvious candidates to supplant the prevailing frequentist methodology. However, it is well-known that the adoption of Bayesian
methods continues to be hampered by several factors, such as the belief that advanced
computational tools are required to make Bayesian statistics practical (e. g. Green
et al., 2015). The most persistent of these is that the full benefit of Bayesian methods
demands specification of a prior level of belief, even in the absence of any appropriate
insight. This “Problem of Priors” has cast a shadow over Bayesian methods since their
emergence over 250 years ago (see e. g. McGrayne, 2011), and has led to a variety of
approaches, such as prior elicitation, prior sensitivity analysis, and objective Bayesian
methodology; all have their supporters and critics.
One of the least well-known was suggested over 70 years ago (Good, 1950) by one
of the best-known proponents of Bayesian methods during the 20th century, I.J. Good.
It involves reversing the conventional direction of Bayes’s Theorem and determining
the level of prior belief required to reach a specified level of posterior belief, given the
evidence observed. This reversal of Bayes’s Theorem allows the assessment of new findings on the basis of whether the resulting prior is reasonable in the light of existing
knowledge. Whether a prior is plausible in the light of existing knowledge can be assessed informally or more formally using techniques for comparing priors with existing
data as suggested by Box (1980) and further refined by Evans and Moshonov (2006).
Good stressed that despite the routine use of the adjectives “prior” and “posterior” in
applications of Bayes’s Theorem, the validity of any resulting inference does not require
a specific temporal ordering, as the theorem is simply a constraint ensuring consistency
with the axioms of probability. While reversing Bayes’s Theorem is still regarded as
unacceptable by some on the grounds it allows “cheating” in the sense of choosing priors to achieve a desired posterior inference (e. g. O’Hagan and Forster, 2004, p. 143),
others point out this is not an ineluctable consequence of the reversal (e. g. Cox, 2006,
p. 78–79). As we shall show, recent technical advances further weaken this criticism.
Good’s belief in the value of Reverse-Bayes methods won support from E.T. Jaynes
in his well-known treatise on probability. Explaining a specific manifestation of the approach (to be discussed shortly) Jaynes remarked: “We shall find it helpful in many cases
where our prior information seems at first too vague to lead to any definite prior probabilities; it stimulates our thinking and tells us how to assign them after all” (Jaynes,
2003, p. 126). Yet despite the advocacy of two leading figures in the foundations of
Bayesian methodology, the potential of Reverse-Bayes methods has remained largely
unexplored. Most published work has focused on their use in putting new research
claims in context, with Reverse-Bayes methods being used to assess whether the prior
evidence needed to make a claim credible is consistent with existing insight (Carlin and
Louis, 1996; Matthews, 2001a,b; Spiegelhalter, 2004; Greenland, 2006, 2011; Held, 2013;
Colquhoun, 2017, 2019; Held, 2019a, 2020; Pawel and Held, 2020).

3
The purpose of this paper is to highlight recent technical developments of Good’s
basic idea which lead to inferential tools of practical value in data analysis. Specifically,
we show how Reverse-Bayes methods address the current concerns about the interpretation of new findings and their replication. We begin by illustrating the basics of the
Reverse-Bayes approach for both hypothesis testing and parameter estimation. This
is followed by a discussion of Reverse-Bayes methods for assessing effect estimates in
Section 2. These allow the credibility of both new and existing research findings reported in terms of NHST to be evaluated in the context of existing knowledge. This
enables researchers to go beyond the standard dichotomy of statistical significance/nonsignificance, extracting further insight from their findings. We then discuss the use of
the Reverse-Bayes approach in the most recalcitrant form of the Problem of Priors,
involving the assessment of research findings which are unprecedented and thus lacking any clear source of prior support. We show how the concept of intrinsic credibility
resolves this challenge, and puts recent calls to tighten p-value thresholds on a principled basis (Benjamin et al., 2017). In Section 3 we describe Reverse-Bayes methods
with Bayes factors, the principled solution for Bayesian hypothesis testing. Finally, we
describe in Section 4. Reverse-Bayes approaches to interpretational issues that arise in
conventional statistical analysis based on p-values, and how they can be used to flag the
risk of inferential fallacies. We close with some extensions and final conclusions.

1.1

Reverse-Bayes for hypothesis testing

The subjectivity involved in the specification of prior distributions is often seen as a
weak point of Bayesian inference. The Reverse-Bayes approach can help to resolve this
issue both in hypothesis testing and parameter estimation, we will start with the former.
Consider a null hypothesis H0 with prior probability π = Pr(H0 ), so Pr(H1 ) = 1 − π
is the prior probability of the alternative hypothesis H1 . Computation of the posterior
probability of H0 is routine with Bayes’ theorem:
Pr(H1 | data) =

Pr(data | H1 ) Pr(H1 )
.
Pr(data | H0 ) Pr(H0 ) + Pr(data | H1 ) Pr(H1 )

Bayes’ theorem can be written in more compact form as
Pr(H1 | data)
Pr(data | H1 ) Pr(H1 )
=
,
Pr(H0 | data)
Pr(data | H0 ) Pr(H0 )

(1)

i. e. the posterior odds are the likelihood ratio times the prior odds. The standard
’forward-Bayes’ approach thus fixes the prior odds (or one of the underlying probabilities), determines the likelihood ratio for the available data, and takes the product to
compute the posterior odds. Of course, the latter can be easily back-transformed to the
posterior probability Pr(H1 | data), if required. The Problem of Priors is now apparent:
in order for us to update the odds in favour of H1 , we must first specify the prior odds.
This can be problematic in situations where, for example, the evidence on which to base
the prior odds is controversial or even non-existent.

4
However, as Good emphasised it is entirely justifiable to “flip” Bayes’s theorem
around, allowing us to ask the question: Which prior, when combined with the data,
leads to our specified posterior?
Pr(H1 )
Pr(H1 | data) . Pr(data | H1 )
=
.
Pr(H0 )
Pr(H0 | data) Pr(data | H0 )

(2)

For illustration we re-visit an example put forward by Good (1950, p. 35), perhaps
the first published Reverse-Bayes calculation. It centres on a question for which the setting of an initial prior is especially problematic: does an experiment provide convincing
evidence for the existence of extra-sensory perception (ESP)? The substantive hypothesis H1 is that ESP exists, so that H0 asserts it does not exist. Imagine an experiment
in which a person has to make n consecutive guesses of random digits (between 0 and
9) and all are correct. The likelihood ratio is therefore
1
Pr(data | H1 )
=
= 10n .
Pr(data | H0 )
(1/10)n
It is unlikely that sceptics and advocates of the existence of ESP would ever agree on
what constitutes reasonable priors from which to start a standard Bayesian analysis of
the evidence. However, Good argued that Reverse-Bayes offers a way forward by using it
to set bounds on the prior probabilities for H1 and H0 . This is achieved via the outcome
of an imaginary (Gedanken) experiment capable of demonstrating H1 is more likely than
H0 , that is, of leading to posterior probabilities such that Pr(H1 | data) > Pr(H0 | data).
Using this approach, which Good termed the Device of Imaginary Results, we see that if
the ESP experiment produced 20 correct consecutive guesses, (2) implies that ESP may
be deemed more likely than not to exist by anyone whose priors satisfy Pr(H1 )/Pr(H0 ) >
10−20 . In contrast, if only n = 3 correct guesses emerged, then the existence of ESP
could be rejected by anyone whose priors satisfy Pr(H1 )/Pr(H0 ) < 10−3 . Using Bayes’s
Theorem in reverse has thus led to a quantitative statement of the prior beliefs that
either advocates or sceptics of ESP must be able to justify in the face of results from
a real experiment. The practical value of Good’s approach was noted by Jaynes in
his treatise: “[I]n the present state of development of probability theory, the device of
imaginary results is usable and useful in a very wide variety of situations, where we
might not at first think it applicable” (Jaynes, 2003, p. 125–126).
It is straightforward to extend (1) and (2) to hypotheses that involve unknown
parameters θ. The likelihood ratio Pr(data | H1 )/ Pr(data | H0 ) is then called a Bayes
factor (Jeffreys, 1961; Kass and Raftery, 1995) where
Z
Pr(data | Hi ) = Pr(data | θ, Hi )f (θ | Hi )dθ
is the marginal likelihood under hypothesis Hi , i = 0, 1, obtained be integration of the
ordinary likelihood with respect to the prior distribution f (θ | Hi ). We will apply the
Reverse-Bayes approach to Bayes factors in Section 3 and 4.

5

1.2

Reverse-Bayes for parameter estimation

We can also apply the Reverse-Bayes idea to continuous prior and posterior distributions
of a parameter of interest θ. Reversing Bayes’ theorem
f (θ | data) =

f (data | θ)f (θ)
f (data)

then leads to
f (θ) = f (data)

f (θ | data)
.
f (data | θ)

(3)

So the prior is proportional to the posterior divided by the likelihood with proportionality constant f (data).
Consider Bayesian inference for the mean θ of a univariate normal distribution,
assuming the variance σ 2 is known. Let x denote the observed value from that N(θ, σ 2 )
distribution and suppose the prior for θ (and hence also the posterior) is normal. Each
of them is determined by two parameters, usually the mean and the variance, but two
distinct quantiles would also work. If we fix both parameters of the posterior, then the
prior in (3) is – under a certain regularity condition – uniquely determined. For ease of
presentation we work with the observational precision κ = 1/σ 2 and denote the prior
and posterior precision by δ and δ 0 , respectively. Finally let µ and µ0 denote the prior
and posterior mean, respectively.
Forward-Bayesian updating tells us how to compute the posterior precision and
mean:
δ0

=

µ0

=

δ + κ,
µδ + xκ
.
δ0

Reverse-Bayes simply inverts these equations, which leads to the following:
= δ 0 − κ,
µ0 δ 0 − xκ
µ =
,
δ
δ

(4)
(5)

provided δ 0 > κ, i. e. the posterior precision must be larger than the observational
precision.
We will illustrate the application of (4) and (5) as well as the methodology in the
rest of this review using a recent meta-analysis combining information from n = 7 randomized controlled clinical trials investigating the association between corticosteroids
and mortality in hospitalized patients with COVID-19 (WHO REACT Working Group,
2020); its results are reproduced in Figure 1 (here and henceforth, odds ratios (ORs)
are expressed as log odds ratios to transform the range from (0, ∞) to (−∞, +∞), consistent with the assumption of normality). Let xi = θ̂i denote the maximum likelihood
estimate (MLE) of the log odds ratio θ in the i-th study with standard error σi . The
d = 0.66 [95% CI,
meta-analytic odds ratio estimate under the fixed-effects model is OR

6
0.53, 0.82], respectively θ̂ = −0.42 [95% CI, -0.63, -0.20] for the log odds ratio θ, indicating evidence for lower mortality of patients treated with corticosteroids compared
to patients receiving usual care or placebo. The pooled effect estimate θ̂ represents a
posterior mean µ0 with posterior precision δ 0 = 83.8.

Deaths/Patients
p

p Box

0.69 [−1.54, 2.93]

0.54

0.33

−0.22 [−0.72, 0.27]

0.38

0.40

0.0002

0.22

−0.79 [−1.61, 0.03]

0.06

0.36

2/14

1.39 [−0.43, 3.21]

0.14

0.05

26/105

29/92

−0.34 [−0.96, 0.29]

0.29

0.79

13/24

13/23

−0.10 [−1.25, 1.06]

0.87

0.58

Trial

Steroids

No Steroids

logOR [95%CI]

DEXA−COVID 19

2/7

2/12

CoDEX

69/128

76/128

RECOVERY

95/324

283/683

−0.53 [−0.82, −0.25]

CAPE COVID

11/75

20/73

COVID STEROID

6/15

REMAP−CAP
Steroids−SARI
FE Model

−0.42 [−0.63, −0.20]

−2

−1

0

1

2

3

4

Log Odds Ratio

Figure 1: Forest plot of fixed effects meta-analysis of randomized clinical trials investigating association between corticosteroids and mortality in hospitalized patients with
COVID-19 (WHO REACT Working Group, 2020). Shown are number of deaths among
total number of patients for treatment/control group, log odds ratio effect estimates
with 95% confidence interval, two-sided p-values p, and prior-predictive tail probabilities pBox with a meta-analytic estimate based on the remaining studies serving as the
prior.
With a meta-analysis such as this, it is of interest to quantify potential conflict
among the effect estimates from the different studies. To do this, we follow Presanis
et al. (2013) and compute a prior-predictive tail probability (Box, 1980; Evans and
Moshonov, 2006) for each study-specific estimate θ̂i , with a meta-analytic estimate
based on the remaining studies serving as the prior. Fixed effects (FE) meta-analysis
is standard (forward-)Bayesian updating for normally distributed effect estimates with
an initial flat prior considered here. Hence, instead of fitting a reduced meta-analysis
for each study, we can simply use the the Reverse-Bayes equations (4) and (5) together
with the overall estimate to compute the parameters of the prior in the absence of the
i-th study (denoted by the index −i):
δ−i

= δ 0 − 1/σi2 ,

µ−i

=

µ0 δ 0 − θ̂i /σi2
.
δ−i

7
For example, through omitting the RECOVERY Collaborative Group (2020) trial result
θ̂i = −0.53 with standard error σi = 0.145 we obtain δ−i = 36.1 and µ−i = −0.26. A
prior predictive tail probability using the approach from Box (1980) is then obtained
by computing pBox = Pr(χ21 ≥ t2Box ) with
θ̂i − µ−i
= −1.24.
tBox = p 2
σi + 1/δ−i
This leads to pBox = 0.22 for the RECOVERY trial, indicating very little prior-data
conflict, see Figure 1 for the tail probabilities pBox for the other studies.
Instead of determining the prior completely based on the posterior, one may also
want to fix one parameter of the posterior and one parameter of the prior. This is
of particular interest in order to challenge “significant” or “non-significant” findings
through the Analysis of Credibility, as we will see in the following section.

2

Reverse-Bayes methods for the assessment of effect
estimates

A more general question amenable to Reverse-Bayes methods is the assessment of effect
estimates and their statistical significance or non-significance. This issue has recently
attracted intense interest following the public statement of the American Statistical
Association about the misuse and misinterpretation of the NHST concepts of statistical significance and non-significance (Wasserstein and Lazar, 2016). First investigated
20 years ago in Matthews (2001a) with subsequent discussion in Matthews (2001b),
Reverse-Bayes methods for assessing both statistically significant and non-significant
findings has been termed the Analysis of Credibility (or AnCred, Matthews, 2018),
whose principles and practice we now briefly review.

2.1

The Analysis of Credibility

Suppose the study gives rise to a conventional confidence interval for the unknown
effect size θ at level 1 − α with lower limit L and upper limit U . Assume that L and U
are symmetric around the point estimate θ̂ (assumed to be normally distributed with
standard error σ). AnCred then takes this likelihood and uses a Reverse-Bayes approach
to deduce the prior required in order to generate evidence for the existence of an effect,
in the form of a posterior that excludes no effect. As such, AnCred allows evidence
deemed statistically significant/non-significant in the NHST framework to be assessed
for its credibility in the Bayesian framework. As the latter represents Pr(H0 | data)
and thus a conditioning on the data rather than the null hypothesis, it is inferentially
directly relevant to researchers. After a suitable transformation AnCred can be applied
to a large number of commonly used effect measures such as differences in means, odds
ratios, relative risks and correlations (see the literature of meta-analysis for details
about conversion among effect size scales, e. g. Cooper et al., 2019, Chapter 11.6). The
inversion of Bayes’s Theorem needed to assess credibility requires the form and location

8
of the prior distribution to be specified. This in turn depends on whether the claim
being assessed is statistically significant or non-significant; we consider each below.
Challenging statistically significant findings
A statistically significant finding at level α is characterized by both L and U being
2
either positive or negative. Equivalently z 2 > zα/2
is required where z = θ̂/σ denotes
the corresponding test statistic and zα/2 the (1 − α/2)-quantile of the standard normal
distribution.
For significant findings, the idea is to ask how sceptical we would have to be not
to find the apparent effect estimate convincing. To this end, a “critical prior interval”
(Matthews, 2001b) with limits −S and S is derived such that the corresponding posterior
credible interval just includes zero, the value of no effect. This critical prior interval can
then be compared with internal or external evidence to assess if the finding is credible
or not, despite being “statistically significant”.
More specifically, a reverse Bayes approach is applied to significant confidence intervals (at level α) based on a normally distributed effect estimate. The prior is a
“sceptical” mean-zero normal distribution with variance τ 2 = g · σ 2 , so the only free
parameter is the relative prior variance g = τ 2 /σ 2 . The posterior is hence also normal
and either its lower α/2-quantile (for positive θ̂) or upper 1 − α/2-quantile (for negative
θ̂) is fixed to zero, so just represents “non-credible”. The sufficiently sceptical prior then
has relative variance

1
2

if z 2 > zα/2
2 /z 2
z
g=
(6)
α/2 − 1

undefined
else
see Held (2019a, Appendix) for a derivation. The corresponding scepticism limit is
S=

(U − L)2
√
,
4 UL

(7)

which holds for any value of α provided the effect is significant at that level.
The left plot in Figure 2 illustrates the AnCred procedure for the finding from
the RECOVERY trial (RECOVERY Collaborative Group, 2020). The trial found a
decrease in COVID-19 mortality for patients treated with corticosteroids compared to
usual care or placebo (θ̂ = −0.53 [95% CI, -0.82, -0.25]). The sufficiently sceptical prior
has relative variance g = 0.39, so the sufficiently sceptical prior variance needs to be
roughly 2.5 times smaller than the variance of the estimate to make the result noncredible. The scepticism limit on the log odds ratio scale turns out to be -0.18, which is
0.84 on the odds ratio scale. Thus sceptics may still reject the RECOVERY trial finding
as lacking credibility despite its statistical significance if external evidence suggests
mortality reductions (in terms of odds) are unlikely to exceed around 1 − 0.84 ≈ 16%.

9
Challenging statistically non-significant findings
It is also possible to challenge “non-significant” findings (i. e. those for which the CI
2
now includes zero, so z 2 < zα/2
) using a prior that pushes the posterior towards being
credible in the Bayesian sense, with posterior credible interval no longer including zero,
corresponding to no effect.
Matthews (2018) proposed the “advocacy prior” for this purpose, a normal prior
with positive mean µ and variance τ 2 chosen such that the α/2-quantile is fixed to
zero (for positive effect estimates θ̂ > 0). He showed that the “advocacy limit” AL, the
(1 − α/2)-quantile of the advocacy prior is
AL = −

U +L
(U − L)2
2 UL

(8)

to reach credibility of the corresponding posterior at level α. We show in Appendix A
that the corresponding relative prior mean m = µ/θ̂ is

2
2

if z 2 < zα/2
2
2
(9)
m = 1 − z /zα/2

undefined
else.
There are two important properties of the advocacy prior. First, the coefficient of
variation CV is
−1
CV = τ /µ = zα/2
.
The advocacy prior θ ∼ N(µ, τ 2 = µ2 CV2 ) is hence characterized by a fixed coefficient
of variation, so this prior has equal evidential weight (quantified in terms of µ/τ = zα/2 )
as data which are “just significant” at level α. Second, the advocacy limit AL defines
the family of normal priors capable of rendering a “non-significant” finding credible
at the same level. Such priors are summarized by the credible interval (Lo , Uo ) where
Lo ≥ 0, Uo ≤ AL. Thus when confronted with a “non-significant” result – often, and
wrongly, interpreted as indicating no effect – advocates of the existence of an effect
may still claim the existence of the effect is credible to the same level if there exists
prior evidence or insight compatible with the credible interval (Lo , Uo ) where Lo ≥ 0,
Uo ≤ AL. If the evidence for an effect is weak (strong), the resulting advocacy prior will
be broad (narrow), giving advocates of an effect more (less) latitude to make their case
under terms of AnCred. Note that (8) and (9) also hold for negative effect estimates,
where we fix the (1−α/2)-quantile of the advocacy prior to zero and define the advocacy
limit AL as the α/2-quantile of the advocacy prior.
For illustration we consider the data from the REMAP-CAP trial (REMAP-CAP
Investigators, 2020) that supported the RECOVERY trial finding of decreased COVID19 mortality from corticosteroid use. However, this trial involved far fewer patients, and
despite the point estimate showing efficacy, the relatively large uncertainty rendered
the overall finding non-significant at the 5% level (θ̂ = −0.34 [95% CI, −0.96, 0.29]).
Such an outcome is frequently (and wrongly) taken to imply no effect. The use of
AnCred leads to a more nuanced conclusion. The advocacy limit AL on the log odds

10

Significant effect estimate
1.4

Non−significant effect estimate
2

Reverse−Bayes

Reverse−Bayes

S
1

1
OR

OR

−S
1/1.4

1/2

1/4
1/2
AL
1/8
Sceptical
Prior

Data

Posterior

Advocacy
Prior

Data

Posterior

Figure 2: Two examples of the Analysis of Credibility. Shown are point estimates within
95% confidence/credible intervals. The left plot illustrates how a sceptical prior is used to
challenge the significant finding from the RECOVERY trial (RECOVERY Collaborative
Group, 2020). The right plot illustrates how an advocacy prior is used to challenge a
non-significant finding from the REMAP-CAP trial (REMAP-CAP Investigators, 2020).
In both scenarios the posterior is fixed to be just credible/non-credible.

ratio scale for REMAP-CAP is −1.89, i. e. 0.15 on the odds ratio scale, see also the
right plot in Figure 2. Thus advocates of the effectiveness of corticosteroids can regard
the trial as providing credible evidence of effectiveness despite its non-significance if
external evidence supports mortality reductions (in terms of odds) in the range 0%
to 85%. So broad an advocacy range reflects the fact that this relatively small trial
provides only modest evidential weight, and thus little constraint on prior beliefs about
the effectiveness of corticosteroids.
Relationship between Analysis of Credibility and the fail-safe N method
There is an interesting connection between AnCred and the well-known “fail-safe N ”
method, sometimes also called “file-drawer analysis”. This method, first introduced
by Rosenthal (1979) and later refined by Rosenberg (2005), is commonly applied to
the results from a meta-analysis and answers the question: “How many unpublished
negative studies do we need to make the meta-analytic effect estimate non-significant?”
A relatively large N of such unpublished studies suggests that the estimate is robust
to potential null-findings, for example due to publication bias. Calculations are made
under the assumption that the unpublished studies have an average effect of zero and
a precision equal to the average precision of the published ones.

11
While the method does not identify nor adjust for publication bias, it provides a
quick way to assess how robust the meta-analytic effect estimate is. The method is
available in common software packages such as metafor (Viechtbauer, 2010) and its
simplicity and intuitive appeal have made it very popular among researchers.
AnCred and the fail-safe N are both based on the idea to challenge effect estimates
such that they become “non-significant/not credible”, and it is easy to show that the
methods are under some circumstances also technically equivalent. To illustrate this, we
consider again the meta-analysis on the association between corticosteroids and COVID19 mortality (WHO REACT Working Group, 2020) which gave the pooled log odds ratio
estimate θ̂ = −0.42 with standard error σ = 0.11, posterior precision δ 0 = 83.8 and test
statistic z = θ̂/σ = −3.81.
Using the Rosenberg (2005) approach (as implemented in the fsn() function from
the metafor package) we find that at least N = 20 additional but unpublished nonsignificant findings are needed to make the published meta-analysis effect non-significant.
If instead, we challenge the overall estimate with AnCred, we obtain the relative prior
variance g = 0.36 using equation (6), so τ 2 = 0.0043. Taking into account the average
precision δ 0 /n = 11.98 of the different effect estimates estimates in the meta-analysis
leads to N = n/(δ 0 ·τ 2 ) = 19.5 which is equivalent to the fail-safe N result after rounding
to the next larger integer.

2.2

Intrinsic credibility

The Problem of Priors is at its most challenging in the context of entirely novel “out
of the blue” effects for which no obviously relevant external evidence exist. By their
nature, such findings often attract considerable interest both within and beyond the research community, making their reliability of particular importance. Given the absence
of external sources of evidence, Matthews (2018) proposed the concept of intrinsic credibility. This requires that the evidential weight of an unprecedented finding is sufficient
to put it in conflict with the sceptical prior rendering it non-credible. In the AnCred
framework, this implies a finding possesses intrinsic credibility at level α if the estimate
θ̂ is outside the corresponding sceptical prior interval [−S, S] extracted using ReverseBayes from the finding itself, i. e. θ̂2 > S 2 with S given in (7). Matthews showed this
implies an unprecedented finding is intrinsically credible at level α = 0.05 if its p-value
does not exceed 0.013.
Held (2019a) refined the concept by suggesting the use of a prior-predictive check
(Box, 1980; Evans and Moshonov, 2006) to assess potential prior-data conflict. With
this approach the uncertainty of the estimate θ̂ is also taken into account since it is
based on the prior-predictive distribution, in this case θ̂ ∼ N(0, σ 2 + τ 2 = σ 2 (1 + g))
with g as given in (6). Intrinsic credibility is declared if the (two-sided) tail-probability



pBox = Pr χ21 ≥ θ̂2 /(σ 2 + τ 2 ) = Pr χ21 ≥ z 2 /(1 + g)
of θ̂ under the prior-predictive distribution is smaller than α. It turns out that the p-value
associated with θ needs to be at least as small as 0.0056 to obtain intrinsic credibility

12
at level α = 0.05, providing another principled argument for the recent proposition to
lower the p-value threshold for the claims of new discoveries to 0.005 (Benjamin et al.,
2017). A simple check for intrinsic credibility is based on the credibility ratio, the ratio
of the upper to the lower limit (or vice versa) of a confidence interval for a credible
effect size. If the credibility ratio is smaller than 5.8 then the result is intrinsically
credible (Held, 2019a). This holds for confidence intervals at all possible values of α, not
just for the 0.05 standard. For example, in the RECOVERY study the 95% confidence
interval for the log-odds ratio ranges from −0.82 to −0.25, so the credibility ratio is
−0.82/ − 0.25 = 3.27 < 5.8 and the result is intrinsically credible at the standard 5%
level.
Replication of effect direction
Whether intrinsic credibility is assessed based on the prior or the prior-predictive distribution, it depends on the level α in both cases. To remove this dependence, Held (2019a)
proposed to consider the smallest level at which intrinsic credibility can be established,
defining the p-value for intrinsic credibility



|z|
,
pIC = 2 1 − Φ √
2
see Held (2019a, section 4) for the derivation. Now z = θ̂/σ, so compared to the standard
p-value p = 2 {1 − Φ (|z|)}, the p-value for intrinsic credibility is based on twice the
variance σ 2 of the estimate θ̂. Although motivated from a different perspective, inference
based on intrinsic credibility thus mimics the doubling the variance rule advocated by
Copas and Eguchi (2005) as a simple means of adjusting for model uncertainty.
Moreover, Held (2019a) showed that pIC is connected to prep (Killeen, 2005), the
probability that a replication will result in an effect estimate θ̂r in the same direction
as the observed effect estimate θ̂, by prep = 1 − pIC /2. Hence, an intrinsically credible
estimate at a small level α will have high chance of replicating since prep ≥ 1 − α/2.
Note that prep lies between 0.5 and 1 with the extreme case prep = 0.5 if θ̂ = 0.
As an example, the p-value for intrinsic credibility for the RECOVERY trial finding
(with p-value p = 0.0002) cited earlier is pIC = 0.01 and thus the probability of the
replication effect going in the same direction (i. e. reduced mortality in this case) is
0.995. In contrast, the finding from the smaller REMAP-CAP trial (with p = 0.29)
leads to pIC = 0.46, and the probability of effect direction replication is hence only
0.77.

3

Reverse-Bayes methods with Bayes factors

The AnCred procedure as described above uses posterior credible intervals as a means of
quantifying evidence. However, quantification of evidence with Bayes factors is a more
principled solution for hypothesis testing in the Bayesian framework (Jeffreys, 1961;
Kass and Raftery, 1995). Bayes factors enable direct probability statements about null

13
and alternative hypothesis and they can also quantify evidence for the null hypothesis,
both are impossible with indirect measures of evidence such as p-values (Held and
Ott, 2018). Reverse-Bayes approaches combined with Bayes factor methodology was
pioneered in Carlin and Louis (1996) but then remained unexplored until Pawel and
Held (2020) proposed an extension of AnCred where Bayes factors are used as a means
of quantifying evidence. Rather than determining a prior such that a finding becomes
“non-credible” in terms of a posterior credible interval, this approach determines a
prior such that the finding becomes “non-compelling” in terms of a Bayes factor. In the
second step of the procedure, the plausibility of this prior is quantified using external
data from a replication study. Here, we will illustrate the methodology using only an
original study; we mention extensions for replications in Section 5.1.
Sceptical priors
A standard hypothesis test compares the null hypothesis H0 : θ = 0 to the alternative
H1 : θ 6= 0. Bayesian hypothesis testing requires specification of a prior distribution
of θ under H1 . A typical choice is a local alternative, a unimodal symmetric prior
distribution centred around the null value (Johnson and Rossell, 2010). We consider
again the sceptical prior θ | H1 ∼ N(0, τ 2 = g · σ 2 ) with relative prior variance g for this
purpose. This leads to the Bayes factor comparing H0 to H1 being
BF01 =

p



z2
g
·
1 + g · exp −
.
1+g 2

Yet again, the amount of evidence which the data provide against the null hypothesis
depends on the prior parameter g; As g becomes smaller (g ↓ 0), the null and the
alternative will become indistinguishable, so the data are equally likely under both
(BF01 → 1). On the other hand, for increasingly diffuse priors (g → ∞), the null
hypothesis will always prevail (BF01 → ∞) due to the Jeffreys-Lindley
paradox (Robert,

2014). In between, the BF01 reaches a minimum at g = max z 2 − 1, 0 leading to
minBF01

(

√
|z| · exp −z 2 /2 · e
=
1

if |z| > 1
else

(10)

which is an instance of a minimum Bayes factor, the smallest possible Bayes factor
within a class of alternative hypotheses, in this case zero-mean normal alternatives
(Edwards et al., 1963; Berger and Sellke, 1987; Sellke et al., 2001; Held and Ott, 2018).
Reporting of minimum Bayes factors is one attempt of solving the problem of priors
in Bayesian inference. However, this bound may be rather small and the corresponding
prior unrealistic. In contrast, the Reverse-Bayes approach makes the choice of the prior
explicit by determining the relative prior variance parameter g such that the finding is
no longer compelling, followed by assessing the plausibility of this prior. To do so, one
first fixes BF01 = γ, where γ is a cut-off above which the result is no longer convincing,
for example γ = 1/10, the level for strong evidence according to Jeffreys (1961). The

14
sufficiently sceptical relative prior variance is then given by
 2
z2
− z − 1
if −
≥1
g=
q
q

undefined
else

 2
 2
z
where q = W − 2 · exp −z
γ

(11)

where W(·) is the Lambert W function (Corless et al., 1996), see Pawel and Held (2020,
Appendix B) for a proof.
The sufficiently sceptical relative prior variance g exists only for a cut-off γ if
minBF01 ≤ γ, similar to standard AnCred where it exists only at level α if the original
finding was significant at the same level. In contrast to standard AnCred, however, if
the sufficiently sceptical relative prior variance g exists, there are always two solutions,
a consequence of the Jeffreys-Lindley paradox: If BF01 decreases in g below the chosen
cut-off γ, after attaining its minimum it will monotonically increase and intersect a
second time with γ, admitting a second solution for the sufficiently sceptical prior.
We revisit the meta-analysis example considered earlier: The left plot in Figure 3
shows the Bayes factor BF01 as a function of the relative prior variance g for each
finding included in the meta-analysis. Most of them did not include a great number of
participants and thus provide little evidence against the null for any value of the relative
prior variance g. In contrast, the finding from the RECOVERY trial (RECOVERY
Collaborative Group, 2020) provides more compelling evidence and can be challenged
up to minBF01 = 1/148.9. For example, we see in Figure 3 that the sceptical prior
variance needs to be g = 0.59, so 1.69 times smaller than the variance of the effect
estimate, such that the finding is no longer compelling at level γ = 1/10. This translates
to a 95% prior credible interval from 0.8 to 1.24 for the OR. Hence, a sceptic might
still consider the RECOVERY finding to be unconvincing, despite its minimum BF
being very compelling, if external evidence supports ORs in that range. Note that also
g 0 = 8190 gives a Bayes factor of BF01 = 1/10, however, such a large relative prior
variance represents ignorance rather than scepticism and is less useful for Reverse-Bayes
inference.
The plausibility of the sufficiently sceptical prior can be evaluated in light of external evidence, but what should we do in the absence of such? We could again use the
Box (1980) prior-predictive check, however, the resulting tail probability is difficult to
compare to the Bayes-factor cut-off γ. When a specific alternative model to the null
is in mind, Box also suggested to use a Bayes factor contrasting the two models. Following this approach, Pawel and Held (2020) proposed to define a second Bayes factor
contrasting the sufficiently sceptical prior to an optimistic prior, which they defined as
θ | H2 ∼ N(θ̂, σ 2 ) the posterior of θ based on the data and the reference prior f (θ) ∝ 1.
We can then conclude that the effect estimate is intrinsically credible at level γ if the
data favour the optimistic prior over the sufficiently sceptical prior at a higher level
than 1/γ (i. e. if BF12 ≤ γ), analogously to intrinsic credibility based on significance.
For example, we obtain BF12 = 1/64 for the finding from the RECOVERY trial, so it

15

Sceptical prior

Advocacy prior

3

3

1

1
1/3
BF01

BF01

1/3
1/10

1/10
1/30
1/30
1/100
1/100

1/300
0

0.6

0.1

1

4

9

16

25

−2

Relative prior variance g

Trial

DEXA−COVID 19

CoDEX

−1

0

0.4

1

2

Relative prior mean m
RECOVERY

CAPE COVID

COVID STEROID

REMAP−CAP

Steroids−SARI

Figure 3: Illustration of the AnCred with Bayes factors procedure using the findings from
the meta-analysis on the association of COVID-19 mortality and corticosteroids. The left
plot shows the Bayes factor BF01 as a function of the relative variance g of the sceptical
prior. The result from the RECOVERY trial is challenged with a sceptical prior such
that BF01 = 1/10, for the other trials such a prior does not exist. The right plot shows
the Bayes factor BF01 as a function of the relative mean m = µ/θ̂ of the advocacy prior
where the coefficient of variation from the prior is fixed to CV = τ /µ = z(1/3)−1 = 0.67.
The RECOVERY and the CAPE COVID findings are challenged such that BF01 = 1/3,
for the other trials such a prior does not exist.

is intrinsically credible at γ = 1/10. To remove the dependence on the choice of γ, one
can then determine the smallest cut-off γ where intrinsic credibility can be established,
defining a Bayes factor for intrinsic credibility similar to the definition of the p-value for
intrinsic credibility. For the RECOVERY finding, this turns out to be BFIC = 1/25.
Advocacy priors
A natural question is whether we can also define an advocacy prior, a prior which renders
an uncompelling finding compelling, in the AnCred framework with Bayes factors. In
traditional AnCred, advocacy priors always exist since one can always find a prior that,
when combined with the data, can overrule them. This is fundamentally different to
inference based on Bayes factors, where the prior is not synthesized with the data, but
rather used to predict them. A classical result due to Edwards et al. (1963) states that
if we consider the class of all possible priors under H1 , the minimum Bayes factor is

16
given by

minBF01 = exp −z 2 /2

(12)

which is obtained for H1 : θ = θ̂. This implies that a non-compelling finding can not be
“rescued” further than to this bound. For example, for the finding from the REMAPCAP trial (REMAP-CAP Investigators, 2020) the bound is unsatisfactorily minBF01 =
1/1.7, so at most “worth a bare mention” according to Jeffreys (1961).
Putting these considerations aside, we may still consider the class of N(µ, τ 2 ) priors
under the alternative H1 . The Bayes factor contrasting H0 to H1 is then given by
(
"
#)
p
1 θ̂2
(θ̂ − µ)2
2
2
BF01 = 1 + τ /σ · exp −
− 2
.
2 σ2
σ + τ2
The reverse-Bayes approach now determines the prior mean µ and variance τ 2 which lead
to the Bayes factor BF01 being just at some cut-off γ. However, if both parameters are
free, there are infinitely many solutions to BF01 = γ, if any exist at all. The traditional
AnCred framework resolves this by restricting the class of possible priors to advocacy
−1
priors with fixed coefficient of variation of CV = τ /µ = zα/2
. We can translate this
idea to the Bayes factor AnCred framework and fix the prior’s coefficient of variation
to CV = z(γ)−1 , where z(γ) is a z-value corresponding to minBF01 = γ. Inverting
equation (12) leads to
p
z(γ) = −2 log γ.
Under this constraint, the prior carries the same evidential weight as data with minBF01 =
γ. Moreover, the determination of the prior parameters becomes more feasible since there
is only one free parameter left (either µ or τ 2 ).
The right plot in Figure 3 illustrates application of the procedure on data from
the meta-analysis on association between COVID-19 mortality and corticosteroids. The
coefficient of variation of the advocacy prior is fixed to CV = z(1/3)−1 = 0.67 and thus
the Bayes factor BF01 only depends on the relative mean parameter m. While under
the sceptical prior, only the RECOVERY finding could be challenged at γ = 1/3, with
this advocacy prior it is now also possible for the CAPE COVID finding (Dequin et al.,
2020). We see that, a prior with mean µ = m · θ̂ = 0.37 · −0.79 = −0.29 and standard
deviation τ = CV · µ = 0.2 is able to make the finding compelling at γ = 1/3. This
corresponds to a 95% prior credible interval from 0.5 to 1.1 for the OR. Advocates
may thus still consider the “non-compelling” finding as providing moderate evidence
in favour of a benefit, if external evidence supports mortality reductions in that range.
Note that the advocacy prior may not be unique, e. g. for the CAPE COVID finding
the prior with relative mean m0 = 1.26 and standard deviation τ 0 = 0.67 renders the
data also just compelling at γ = 1/3. We recommend to choose the prior with m closer
to zero, as it is the more conservative choice.

4

Reverse-Bayes Analysis of the False Positive Risk

Application of the Analysis of Credibility with Bayes factors as described in Section 3
assumes some familiarity with Bayes factors as measures of evidence. Colquhoun (2019)

17
argued that very few nonprofessional users of statistics are familiar with the notion
of Bayes factors or likelihood ratios. He proposes to quantify evidence with the false
positive risk, “if only because that is what most users still think, mistakenly, that is what
the p-value tells them”. More specifically, Colquhoun (2019) defines the false positive
risk (FPR) as the posterior probability that the point null hypothesis H0 of no effect
is true given the observed p-value p, i. e. FPR = Pr(H0 | p). As before, H0 corresponds
to the point null hypothesis H0 : θ = 0. Note also that we take the exact (two-sided)
p-value p as the observed “data”, regardless of whether or not it is significant at some
pre-specified level, the so-called “p-equals” interpretation of NHST (Colquhoun, 2017).
FPR can be calculated based on the Bayes factor associated with p. For ease of
presentation we invert Bayes’ theorem (1) and obtain
FPR
Pr(H0 | p)
Pr(H0 )
=
= BF01
,
1 − FPR
Pr(H1 | p)
Pr(H1 )

(13)

where BF01 = 1/BF10 is the Bayes factor for H0 against H1 , computed directly from
the observed p-value p.
The common ’forward-Bayes’ approach is to compute the FPR from the prior probability Pr(H0 ) and the Bayes factor with (13). However, the prior probability Pr(H0 )
is usually unknown in practice and often hard to assess. This can be resolved via the
Reverse-Bayes approach (Colquhoun, 2017, 2019): Given a p-value and a false positive
risk value, calculate the corresponding prior probability Pr(H0 ) that is needed to achieve
that false positive risk. Of specific interest is the value FPR = 5%, because many scientists believe that a Type-I error of 5% is equivalent to a FPR of 5% (Greenland et al.,
2016). This is of course not true and we follow Berger and Sellke (1987, Example 1) and
use the reverse-Bayes approach to derive the necessary prior assumptions on Pr(H0 ) to
achieve FPR = 5% with Equation (13):

−1
1 − FPR
· BF01
Pr(H0 ) = 1 +
.
(14)
FPR
Colquhoun (2017, appendix A.2) uses a Bayes factor based on the t-test, but for
compatibility with the previous sections we assume normality of the underlying test
statistic. We consider Bayes factors under all simple alternatives, but also Bayes factors
under local normal priors, see Held and Ott (2018) for a detailed comparison.
Instead of working with a Bayes factor for a specific prior distribution, we prefer
to work with the minimum Bayes factor minBF01 as introduced in Section 3. In what
follows we will use the minimum Bayes factor based on the z-test (Held and Ott, 2018,
Section 2.1 and 2.2). The minimum Bayes factor based on the z-test among all possible
priors can be computed using the function zCalibrate in the package pCalibrate. The
option alternative = "local" gives the minBF (10) under local normal priors.
Let minBF01 denote the minimum Bayes factor over a specific class of alternatives.
From equation (14) we obtain the inequality

−1
1 − FPR
Pr(H0 ) ≤ 1 +
· minBF01
.
(15)
FPR

18
The right-hand side is thus an upper bound on the prior probability Pr(H0 ) for a given
p-value to achieve a pre-specified FPR value.
There are also minBFs not based on the z-test statistic, but directly on the (twosided) p-value p, the so-called “−e p log p” (Sellke et al., 2001) calibration

−e p log p for p < 1/e
minBF =
(16)
1
otherwise,
and the “−e q log q” calibration, where q = 1 − p (Held and Ott, 2018, Section 2.3):

−e (1 − p) log(1 − p) for p < 1 − 1/e
minBF =
(17)
1
otherwise.
For small p, equation (17) can be simplified to minBF ≈ e p, which mimics the Good
(1958) transformation of p-values to Bayes factors (Held, 2019b).
The two p-based calibrations are also available in the package pCalibrate. They
carry less assumptions than the minimum Bayes factors based on the z-test under normality. The “−e p log p” provides a general bound under all unimodal and symmetrical
local priors for p-values from z-tests (Sellke et al., 2001, Section 3.2). The “−e q log q”
calibration is more conservative and gives a smaller bound on the Bayes factor than
the “−e p log p” calibration. It can be viewed as a general lower bound under simple
alternatives where the direction of the effect is taken into account, see Held and Ott
(2018, Section 2.1 and 2.3).
The left plot in Figure 4 shows the resulting upper bound on the prior probability
Pr(H0 ) as a function of the two-sided p-value if the FPR is fixed at 5%. For p = 0.05,
the “−e p log p” bound is around 11% and 28% for the “−e q log q” calibration. The corresponding values based on the z-test are slightly smaller (10% and 15%, respectively).
All the probabilities are below the 50% value of equipoise, illustrating that borderline
significant result with p ≈ 0.05 do not provide sufficient evidence to justify an FPR
value of 5%. For p = 0.005, the upper bounds are closer to 50% (37% for local and 57%
for simple alternatives).
Turning again to the example from the RECOVERY trial (RECOVERY Collaborative Group, 2020), the p-value associated with the estimated treatment effect is
p = 0.0002. The left plot in Figure 4 shows that the false positive risk can safely be
assumed to be around 5% (or lower), since the upper bound on Pr(H0 ) are all very large
for such a small p-value.
Fixing FPR at the 5% level may be considered as arbitrary. Another widespread
misconception is the belief that that the FPR is equal to the p-value. Held (2013) used
a reverse-Bayes approach to investigate which prior assumptions are required such that
FPR = p holds. Combining (14) with the “−e p log p” calibration (16) gives the explicit
condition
Pr(H0 ) ≤ 1/ {1 − e (1 − p) log(p)}
whereas the “−e q log q” calibration (17) leads to


(1 − p)2
log(1 − p) ≈ 1/ {1 + e (1 − p)} ,
Pr(H0 ) ≤ 1/ 1 − e
p

19
FPR = p−value

FPR = 5%
30%

57.5%

50%
36.9%

20%

10%

calibration type
z (simple)
z (local)
− e q log q
− e p log p
0.0001

0.0005

15.2%

20%
upper bound on Pr(H 0)

upper bound on Pr(H 0)

100%

15.2%
11.4%

10%

10%

5.3%

5%

10%

3%

0.005

0.05

0.0001

p−value (two−sided z−test)

0.0005

0.005

0.05

p−value (two−sided z−test)

Figure 4: The left plot shows the upper bound on the prior probability Pr(H0 ) to
achieve a false positive risk of 5% as a function of the p-value calibrated with either a
z-test calibration (simple and local alternatives) or with the “−e p log p” or “−e q log q”
calibrations, respectively. The right plot shows the upper bound on Pr(H0 ) as a function
of the p-value using the same calibrations but assuming the p-value equals the FPR.

which is approximately 1/(1 + e) = 26.9% for small p.
The right plot in Figure 4 compares the bounds based on these two calibrations with
the ones obtained from simple respectively local alternatives. We can see that strong
assumptions on Pr(H0 ) are needed to justify the claim FPR = p: Pr(H0 ) cannot be
larger than 15.2% if the p-value is conventionally significant (p < 0.05). For p < 0.005,
the bound drops further to 11.4%. Even under the conservative “−e q log q” calibration,
the upper bound on Pr(H0 ) is 26.9% for small p and increases only slightly for larger
values of p. This illustrates that the misinterpretation FPR = p only holds if the prior
probability of H0 is substantially smaller than 50%, an assumption which is questionable
in the absence of strong prior knowledge.

5
5.1

Discussion
Extensions, work in progress and outlook

The Reverse-Bayes methods described above have focused on the comparison of the
prior needed for credibility with findings from other studies and/or more general in-

20
sights. However, replication studies make an obvious additional source of external evidence, as these are typically conducted to confirm original findings by repeating their
experiments as closely as possible. The question is then whether the original findings
have been successfully “replicated”, currently of considerable concern to the research
community. To date, there remains no consensus on the precise meaning of replication
in a statistical sense. The proposal of Held (2020) (see also Held et al., 2020) was to
challenge the original finding using AnCred, as described in Section 2.1, and then evaluate the plausibility of the resulting prior using a prior-predictive check on the data
from a replication study. A similar procedure but using AnCred based on Bayes factors
as in Section 3 was proposed in Pawel and Held (2020). Reverse-Bayes inference seems
to fit naturally into this setting as it provides a formal framework to challenge and
substantiate scientific findings.
Apart from using data from a replication study, there are also other possible extensions of AnCred: We proposed either prior-predictive checks (Box, 1980; Evans and
Moshonov, 2006) or Bayes-factors (Jeffreys, 1961; Kass and Raftery, 1995) for the formal evaluation of the plausibility of the priors derived through Reverse-Bayes. Other
methods could be used for this purpose, for example, Bayesian measures of surprise (Bayarri and Morales, 2003). Furthermore, AnCred in its current state is derived assuming
a normal likelihood for the effect estimate θ̂. This is the same framework as in standard
meta-analysis, and it provides a good approximation for studies with reasonable sample
size (Carlin, 1992). Nevertheless, the normality assumption could be relaxed and more
robust distributions could be considered, for example a t-distribution, which could lead
to more accurate inferences for studies with small sample size.

5.2

Conclusions

The inferential advantages of Bayesian methods are increasingly recognised within the
statistical community. However, among the majority of working researchers they have
failed to make any serious headway, and retain a reputation for complex and “controversial”.
In this review, we have outlined how an idea that began with Jack Good’s proposal
for resolving the “Problem of priors” over 70 years ago (Good, 1950) has experienced
a renaissance over recent years. The basic idea is to invert Bayes’ theorem: a specified posterior is combined with the data to obtain the Reverse-Bayes prior, which is
then used for further inference. This approach is useful in situations where it is difficult
to decide what constitutes a reasonable prior, but easy to specify the posterior which
would lead to a particular decision. Starting with the work of Matthews (2001a,b), the
Reverse-Bayes methodology has been shown capable of addressing many common inferential challenges, including assessing the credibility of scientific findings (Spiegelhalter,
2004; Greenland, 2006, 2011), making sense of “out of the blue” discoveries with no
prior support (Matthews, 2018; Held, 2019a), estimating the probability of successful
replications (Held, 2019a, 2020), and extracting more insight from standard p-values
while reducing the risk of misinterpretation (Held, 2013; Colquhoun, 2017, 2019). The
appeal of Reverse-Bayes techniques has recently been widened by the development of

21
inferential methods using both posterior probabilities and Bayes Factors (Carlin and
Louis, 1996; Pawel and Held, 2020).
These developments come at a crucial time for the role of statistical methods in
research. Despite the many serious – and now well-publicised – inadequacies of NHST
(Wasserstein and Lazar, 2016), the research community has shown itself to be remarkably reluctant to abandon NHST. Techniques based on the Reverse-Bayes methodology
of the kind described in this review could encourage the wider use of Bayesian inference
by researchers. As such, we believe they can play a key role in the scientific enterprise
of the 21th century.

Software
All analyses were performed in the R programming language version 4.0.3 (R Core
Team, 2017). The code to reproduce all analyses is available at https://gitlab.uzh.
ch/samuel.pawel/Reverse-Bayes-Code.
Acknowledgments Support by the Swiss National Science Foundation (Project #
189295) is gratefully acknowledged.

References
Bayarri, M. and Morales, J. (2003). “Bayesian measures of surprise for outlier detection.” Journal of Statistical Planning and Inference, 111(1-2): 3–22.
URL https://doi.org/10.1016/s0378-3758(02)00282-3 20
Benjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J.,
Berk, R., Bollen, K. A., Brembs, B., Brown, L., Camerer, C., Cesarini, D., Chambers,
C. D., Clyde, M., Cook, T. D., Boeck, P. D., Dienes, Z., Dreber, A., Easwaran, K.,
Efferson, C., Fehr, E., Fidler, F., Field, A. P., Forster, M., George, E. I., Gonzalez,
R., Goodman, S., Green, E., Green, D. P., Greenwald, A. G., Hadfield, J. D., Hedges,
L. V., Held, L., Ho, T. H., Hoijtink, H., Hruschka, D. J., Imai, K., Imbens, G.,
Ioannidis, J. P. A., Jeon, M., Jones, J. H., Kirchler, M., Laibson, D., List, J., Little,
R., Lupia, A., Machery, E., Maxwell, S. E., McCarthy, M., Moore, D. A., Morgan,
S. L., Munafó, M., Nakagawa, S., Nyhan, B., Parker, T. H., Pericchi, L., Perugini,
M., Rouder, J., Rousseau, J., Savalei, V., Schönbrodt, F. D., Sellke, T., Sinclair, B.,
Tingley, D., Zandt, T. V., Vazire, S., Watts, D. J., Winship, C., Wolpert, R. L.,
Xie, Y., Young, C., Zinman, J., and Johnson, V. E. (2017). “Redefine statistical
significance.” Nature Human Behaviour , 2(1): 6–10.
URL https://doi.org/10.1038/s41562-017-0189-z 3, 12
Berger, J. O. and Sellke, T. (1987). “Testing a point null hypothesis: Irreconcilability
of P values and evidence (with discussion).” Journal of the American Statistical
Association, 82: 112–139.
URL https://doi.org/10.1080/01621459.1987.10478397 13, 17

22
Box, G. E. P. (1980). “Sampling and Bayes’ Inference in Scientific Modelling and
Robustness (with discussion).” Journal of the Royal Statistical Society, Series A,
143: 383–430.
URL https://doi.org/10.2307/2982063 2, 6, 7, 11, 14, 20
Carlin, B. P. and Louis, T. A. (1996). “Identifying Prior Distributions That Produce
Specific Decisions, With Application to Monitoring Clinical Trials.” In Berry, D.,
Chaloner, K., and Geweke, J. (eds.), Bayesian Analysis in Statistics and Econometrics: Essays in Honor of Arnold Zellner , 493–503. New York: Wiley. 2, 13, 21
Carlin, J. B. (1992). “Meta-analysis for 2 × 2 tables: A Bayesian approach.” Statistics
in Medicine, 11(2): 141–158.
URL https://doi.org/10.1002/sim.4780110202 20
Colquhoun, D. (2017). “The reproducibility of research and the misinterpretation of
p-values.” Royal Society Open Science, 4(12).
URL https://dx.doi.org/10.1098/rsos.171085 2, 17, 20
— (2019). “The False Positive Risk: A Proposal Concerning What to Do About pValues.” The American Statistician, 73(sup1): 192–201.
URL https://doi.org/10.1080/00031305.2018.1529622 2, 16, 17, 20
Cooper, H., Hedges, L. V., and Valentine, J. C. (eds.) (2019). The Handbook of Research
Synthesis and Meta-Analysis. Russell Sage Foundation.
URL https://doi.org/10.7758/9781610448864 7
Copas, J. and Eguchi, S. (2005). “Local model uncertainty and incomplete-data bias
(with discussion).” Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 67(4): 459–513.
URL https://doi.org/10.1111/j.1467-9868.2005.00512.x 12
Corless, R. M., Gonnet, G. H., Hare, D. E. G., Jeffrey, D. J., and Knuth, D. E. (1996).
“On the Lambert W function.” Advances in Computational Mathematics, 5(1): 329–
359.
URL https://doi.org/10.1007/bf02124750 14
Cox, D. R. (2006). Principles of Statistical Inference. Cambridge: Cambridge University
Press. 2
Dequin, P.-F., Heming, N., Meziani, F., Plantefève, G., Voiriot, G., Badié, J., François,
B., Aubron, C., Ricard, J.-D., Ehrmann, S., Jouan, Y., Guillon, A., Leclerc, M.,
Coffre, C., Bourgoin, H., Lengellé, C., Caille-Fénérol, C., Tavernier, E., Zohar, S.,
Giraudeau, B., Annane, D., and and, A. L. G. (2020). “Effect of Hydrocortisone on
21-Day Mortality or Respiratory Support Among Critically Ill Patients With COVID19.” JAMA, 324(13): 1298.
URL https://doi.org/10.1001/jama.2020.16761 16
Edwards, W., Lindman, H., and Savage, L. J. (1963). “Bayesian Statistical Inference in
Psychological Research.” Psychological Review , 70: 193–242.
URL https://doi.org/10.1037/h0044139 13, 15

23
Evans, M. and Moshonov, H. (2006). “Checking for prior-data conflict.” Bayesian
Analysis, 1(4): 893–914.
URL https://doi.org/10.1214/06-ba129 2, 6, 11, 20
Gelman, A. and Loken, E. (2014). “The statistical crisis in science.” American Scientist,
102(6): 460–465.
URL https://doi.org/10.1511/2014.111.460 1
Good, I. J. (1950). Probability and the Weighing of Evidence. London, UK: Griffin. 2,
4, 20
— (1958). “Significance Tests in Parallel and in Series.” Journal of the American
Statistical Association, 53(284): 799–813.
URL https://doi.org/10.1080/01621459.1958.10501480 18
— (1983). Good Thinking: The Foundations of Probability and Its Applications. Minneapolis: University of Minnesota Press. 1
Green, P., Latuszyński, K., Pereyra, M., and Robert, C. (2015). “Bayesian computation:
a summary of the current state, and samples backwards and forwards.” Statistics and
Computing, 25(6): 835–862.
URL https://doi.org/10.1007/s11222-015-9574-5 2
Greenland, S. (2006). “Bayesian perspectives for epidemiological research: I. Foundations and basic methods.” International Journal of Epidemiology, 35: 765–775.
URL https://doi.org/10.1093/ije/dyi312 2, 20
— (2011). “Null misinterpretation in statistical testing and its impact on health risk
assessment.” Preventive Medicine, 53: 225–228.
URL https://doi.org/10.1016/j.ypmed.2011.08.010 2, 20
Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., and
Altman, D. G. (2016). “Statistical tests, P values, confidence intervals, and power: a
guide to misinterpretations.” European Journal of Epidemiology, 31(4): 337–350.
URL https://doi.org/10.1007/s10654-016-0149-3 17
Held, L. (2013). “Reverse-Bayes analysis of two common misinterpretations of significance tests.” Clinical Trials, 10: 236–242.
URL https://doi.org/10.1177/1740774512468807 2, 18, 20
— (2019a). “The assessment of intrinsic credibility and a new argument for p < 0.005.”
Royal Society Open Science.
URL https://doi.org/10.1098/rsos.181534 2, 8, 11, 12, 20
— (2019b). “On the Bayesian interpretation of the harmonic mean p-value.” Proceedings
of the National Academy of Sciences, 116(13): 5855–5856.
URL https://doi.org/10.1073/pnas.1900671116 18
— (2020). “A new standard for the analysis and design of replication studies (with
discussion).” Journal of the Royal Statistical Society: Series A (Statistics in Society),
183(2): 431–448.
URL https://doi.org/10.1111/rssa.12493 2, 20

24
Held, L., Micheloud, C., and Pawel, S. (2020). “The assessment of replication success
based on relative effect size.” Technical report.
URL http://arxiv.org/abs/2009.07782 20
Held, L. and Ott, M. (2018). “On p-Values and Bayes Factors.” Annual Review of
Statistics and Its Application, 5(1).
URL https://doi.org/10.1146/annurev-statistics-031017-100307 13, 17, 18
Jaynes, E. T. (2003). Probability Theory: The Logic of Science. Cambridge, UK New
York, NY: Cambridge University Press.
URL https://doi.org/10.1017/cbo9780511790423 2, 4
Jeffreys, H. (1961). Theory of Probability. Oxford: Oxford University Press. 3rd edition.
4, 12, 13, 16, 20
Johnson, V. E. and Rossell, D. (2010). “On the use of non-local prior densities in
Bayesian hypothesis tests.” Journal of the Royal Statistical Society, Series B , 72(2):
143–170.
URL https://doi.org/10.1111/j.1467-9868.2009.00730.x 13
Kass, R. E. and Raftery, A. E. (1995). “Bayes factors.” Journal of the american
statistical association, 90(430): 773–795.
URL 10.1080/01621459.1995.10476572 4, 12, 20
Killeen, P. R. (2005). “An Alternative to Null-Hypothesis Significance Tests.” Psychological Science, 16(5): 345–353.
URL https://doi.org/10.1111/j.0956-7976.2005.01538.x 12
Matthews, R. A. J. (2001a). “Methods for assessing the credibility of clinical trial
outcomes.” Drug Information Journal , 35: 1469–1478.
URL https://doi.org/10.1177/009286150103500442 2, 7, 20
— (2001b). “Why should clinicians care about Bayesian methods? (with discussion).”
Journal of Statistical Planning and Inference, 94: 43–71.
URL https://doi.org/10.1016/S0378-3758(00)00232-9 2, 7, 8, 20
— (2018). “Beyond ’significance’: principles and practice of the Analysis of Credibility.”
Royal Society Open Science, 5(1): 171047.
URL https://doi.org/10.1098/rsos.171047 7, 9, 11, 20
McElreath, R. (2018). Statistical Rethinking. Chapman and Hall/CRC.
URL https://doi.org/10.1201/9781315372495 2
McGrayne, S. B. (2011). The Theory That Would Not Die. New Haven, CT: Yale
University Press. 2
O’Hagan, A. and Forster, J. (2004). Kendall’s Advanced Theory of Statistic 2B . Wiley,
second edition. 2
Pawel, S. and Held, L. (2020). “The sceptical Bayes factor for the assessment of replication success.”
URL https://arxiv.org/abs/2009.01520 2, 13, 14, 20, 21

25
Presanis, A. M., Ohlssen, D., Spiegelhalter, D. J., and Angelis, D. D. (2013). “Conflict Diagnostics in Directed Acyclic Graphs, with Applications in Bayesian Evidence
Synthesis.” Statistical Science, 28(3): 376–397.
URL https://doi.org/10.1214/13-sts426 6
R Core Team (2017). R: A Language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria.
URL https://www.R-project.org/ 21
RECOVERY Collaborative Group (2020). “Dexamethasone in Hospitalized Patients
with Covid-19 – Preliminary Report.” New England Journal of Medicine.
URL https://doi.org/10.1056/nejmoa2021436 7, 8, 10, 14, 18
REMAP-CAP Investigators (2020). “Effect of Hydrocortisone on Mortality and Organ
Support in Patients With Severe COVID-19.” JAMA, 324(13): 1317.
URL https://doi.org/10.1001/jama.2020.17022 9, 10, 16
Robert, C. P. (2014). “On the Jeffreys-Lindley Paradox.” Philosophy of Science, 81(2):
216–232. https://doi.org/10.1086/675729. 13
Rosenberg, M. S. (2005). “The file-drawer problem revisited: A general weighted method
for calculating fails-safe numbers in meta-analysis.” Evolution, 59(2): 464–468.
URL https://doi.org/10.1111/j.0014-3820.2005.tb01004.x 10, 11
Rosenthal, R. (1979). “The file drawer problem and tolerance for null results.” Psychological Bulletin, 86(3): 638–641.
URL https://doi.org/10.1037/0033-2909.86.3.638 10
Sellke, T., Bayarri, M. J., and Berger, J. O. (2001). “Calibration of p Values for Testing
Precise Null Hypotheses.” The American Statistician, 55: 62–71.
URL https://doi.org/10.1198/000313001300339950 13, 18
Spiegelhalter, D. J. (2004). “Incorporating Bayesian Ideas into Health-Care Evaluation.”
Statistical Science, 19(1): 156–174.
URL https://doi.org/10.1214/088342304000000080 2, 20
Viechtbauer, W. (2010). “Conducting Meta-Analyses in R with the metafor Package.”
Journal of Statistical Software, 36(3).
URL https://doi.org/10.18637/jss.v036.i03 11
Wagenmakers, E.-J., Lee, M., Lodewyckx, T., and Iverson, G. J. (2008). Bayesian
Versus Frequentist Inference, 181–207. New York, NY: Springer New York.
URL https://doi.org/10.1007/978-0-387-09612-4_9 2
Wasserstein, R. L. and Lazar, N. A. (2016). “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician, 70(2): 129–133.
URL https://doi.org/10.1080/00031305.2016.1154108 1, 7, 21
Wasserstein, R. L., Schirm, A. L., and Lazar, N. A. (2019). “Moving to a World Beyond
“p < 0.05”.” The American Statistician, 73(sup1): 1–19.
URL https://doi.org/10.1080/00031305.2019.1583913 2

26
WHO REACT Working Group (2020). “Association Between Administration of Systemic Corticosteroids and Mortality Among Critically Ill Patients With COVID-19:
A Meta-analysis.” JAMA, 324(13): 1330–1341.
URL https://doi.org/10.1001/jama.2020.17023 5, 6, 11

Appendices
A

Proof of equation (9)

2
Suppose that the estimate θ̂ is not significant at level α, so z 2 /zα/2
< 1. With U, L =
2
σ 2 and U − L = 2 zα/2 σ.
θ̂ ± zα/2 σ we have U + L = 2 θ̂, U L = θ̂2 − zα/2

We therefore obtain with (8):
µ=

2
2 θ̂ zα/2
σ2
(2 zα/2 σ)2
2 θ̂
=
.
=
2
2
2
2
2
2
1 − z 2 /zα/2
zα/2 σ − θ̂
α/2 σ )

2 θ̂
AL
=−
2
2
2 (θ̂ − z 2

The advocacy standard deviation is τ = AL/(2 zα/2 ) = µ/zα/2 and the coefficient of
−1
variation is therefore CV = τ /µ = zα/2
.

