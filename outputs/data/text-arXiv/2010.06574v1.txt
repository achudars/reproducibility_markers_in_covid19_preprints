arXiv:2010.06574v1 [cs.DC] 13 Oct 2020

IMPECCABLE: Integrated Modeling PipelinE for COVID Cure
by Assessing Better LEads
Aymen Al Saadi1 , Dario Alfe2,10 , Yadu Babuji3 , Agastya Bhati2 , Ben Blaiszik3,4 , Thomas Brettin4 ,
Kyle Chard3,4 , Ryan Chard3 , Peter Coveney∗2,9 , Anda Trifan4 , Alex Brace4 , Austin Clyde2 , Ian
Foster3,4 , Tom Gibbs8 , Shantenu Jha∗1,5 , Kristopher Keipert8 , Thorsten Kurth8 , Dieter Kranzlmüller7 ,
Hyungro Lee1 , Zhuozhao Li3 , Heng Ma4 , Andre Merzky1 , Gerald Mathias7 , Alexander Partin4 ,
Junqi Yin6 , Arvind Ramanathan∗4 , Ashka Shah4 , Abraham Stern8 , Rick Stevens∗3,4 , Li Tan5 , Mikhail
Titov1 , Aristeidis Tsaris6 , Matteo Turilli1 , Huub Van Dam5 , Shunzhou Wan2 , David Wifling7
1

5

Rutgers University, 2 University College London, 3 University of Chicago, 4 Argonne National Laboratory,
Brookhaven National Laboratory, 6 Oak Ridge Leadership Computing Facility, 7 Leibniz Supercomputing Centre,
8 NVIDIA Corporation, 9 University of Amsterdam, 10 University of Naples Federico II, ∗ Contact Authors

Abstract
The drug discovery process currently employed in the pharmaceutical industry typically requires about 10 years and $2-3 billion
to deliver one new drug. This is both too expensive and too slow,
especially in emergencies like the COVID-19 pandemic. In silico
methodologies need to be improved to better select lead compounds
that can proceed to later stages of the drug discovery protocol accelerating the entire process. No single methodological approach can
achieve the necessary accuracy with required efficiency. Here we
describe multiple algorithmic innovations to overcome this fundamental limitation, development and deployment of computational
infrastructure at scale integrates multiple artificial intelligence and
simulation-based approaches. Three measures of performance are:
(i) throughput, the number of ligands per unit time; (ii) scientific
performance, the number of effective ligands sampled per unit time;
and (iii) peak performance, in flop/s. The capabilities outlined here
have been used in production for several months as the workhorse of
the the computational infrastructure to support the capabilities of the
US-DOE National Virtual Biotechnology Laboratory in combination
with resources from the EU Centre of Excellence in Computational
Biomedicine.

Matteo Turilli1 , Huub Van Dam5 , Shunzhou Wan2 , David Wifling7 . 2020.
IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads. In Supercomputing ’20: International Conference for High
Performance Computing, Networking, Storage, and Analysis. ACM, New
York, NY, USA, 13 pages. https://doi.org/10.1145/1122445.1122456

1

2

Table 1: Performance Attributes
Performance Attribute
Category of achievement
Type of method used
Results reported on basis of
Precision reported
System scale
Measurement mechanism

Datasets, neural networks, gaze detection, text tagging, docking
molecular dynamics, free energy estimation

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Supercomputing ’20, November 16–19, 2020, Virtual
© 2018 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/1122445.1122456

Performance Attributes

Performance attributes are listed in Table 1.

Keywords

ACM Reference Format:
Aymen Al Saadi1 , Dario Alfe2,10 , Yadu Babuji3 , Agastya Bhati2 , Ben Blaiszik3,4 ,
Thomas Brettin4 , Kyle Chard3,4 , Ryan Chard3 , Peter Coveney∗2,9 , Anda
Trifan4 , Alex Brace4 , Austin Clyde2 , Ian Foster3,4 , Tom Gibbs8 , Shantenu
Jha∗1,5 , Kristopher Keipert8 , Thorsten Kurth8 , Dieter Kranzlmüller7 , Hyungro Lee1 , Zhuozhao Li3 , Heng Ma4 , Andre Merzky1 , Gerald Mathias7 ,
Alexander Partin4 , Junqi Yin6 , Arvind Ramanathan∗4 , Ashka Shah4 , Abraham Stern8 , Rick Stevens∗3,4 , Li Tan5 , Mikhail Titov1 , Aristeidis Tsaris6 ,

Justification

COVID-19 has claimed a million lives and resulted in over 35 million infections; there is an urgent need to identify drugs that can
inhibit SARS-CoV-2. IMPECCABLE innovatively couples multiple algorithms to overcome fundamental limitations of classical in
silico drug design. We discuss how algorithmic and computational
innovations are advancing scientific discovery.

3

Our Submission
Scalability, time-to-solution,
peak performance
Explicit
Whole app including I/O
Mixed
Measured on full system
Timers, FLOP count,
performance modeling

Problem Overview

Drug discovery is an astonishingly resource intensive process; the average time to search, design, and effectively bring a clinically tested
drug can range between 10 to 15 years, and can cost over 1 billion
dollars [25, 39]. Considering the universe of about 1068 possible
compounds to traverse for effective drugs, there is an immediate
need for more efficient, higher throughput, and more meaningful
frameworks for early stage drug discovery [15].
In the context of COVID-19, a grand challenge within the drugdiscovery community is the need for capabilities that can screen tens
of billions of small molecules against the SARS-CoV-2 proteome
and identify high quality lead molecules that have the potential
to inhibit the virus life cycle [34]. To achieve this goal, in silico

Supercomputing ’20, November 16–19, 2020, Virtual

methodologies need to be significantly improved to better design
lead compounds that have the potential to become drugs that can be
pushed to the later stages of drug discovery [9, 40, 61]. However, no
single algorithm or method can achieve the necessary accuracy with
required efficiency. IMPECCABLE innovatively brings together
multiple algorithms into a single unified pipeline with an interactive
and iterative methodology allowing both upstream and downstream
feedback to overcome fundamental limitations of classical in silico
drug design. We demonstrate the impact of IMPECCABLE by measuring both raw throughput, defined as ligands per unit time, as well
as its scientific performance, defined as ligands sampled per unit
time.

3.1

Targeting the SARS-CoV-2 proteome

The SARS-CoV-2 genome consists of 29 proteins of which 16 nonstructural proteins (NSPs) represent various enzymes that play critical roles in the virus life cycle [34]. In spite of our understanding of
the roles of these NSPs in the virus life cycle as well as the fact that
closely related protein structures from the middle eastern respiratory
syndrome (MERS) and the SARS coronaviruses are known, there
are no known antivirals currently available for SARS-CoV-2 [51].
The structural biology community has embarked on a massive effort
to provide access to 3D crystallographic structures of the NSPs and
therefore, this represents a tremendous opportunity for designing
therapeutics targeting the disease.
Early stages of drug discovery rely on (experimental) high throughput screening (HTS) protocols to hone in on a suitable chemical
compound library that can be particularly useful against known protein targets [16]. Although HTS approaches are widely available
and used, the sheer combinatorics of drug-like molecules poses a
tremendous challenge in exhaustively sampling the compound space
to find viable molecules. Instead, a number of open source initiatives
are currently building virtual HTS platforms targeting the entire
viral proteome [1, 2]. Indeed, the goals of our efforts include high
throughput structure-based protein-ligand docking simulations, followed by iterative refinements to these virtual screening results to
filter out compounds that “show promise” in biochemical or wholecell assays, safety and toxicology tests, finally leading to a set of
compounds that can proceed towards clinical trials. This process
represents a filtering process, with successive steps leading to better
drug candidates targeting the virus. Although covering such a vast
chemical space would be nearly impossible, even with access to the
entire world’s computing resources, there are open data resources
which provide access to a fairly representative, yet diverse chemical
space (∼1012 compounds).

3.2

The IMPECCABLE Solution

Artificial intelligence (AI) and machine learning (ML) have played
a pivotal role in COVID-19 drug discovery [61]. However, most
ML/AI efforts have largely focused on building effective means
to analyze large volumes of data generated through either ligand
docking simulations—for the purposes of filtering favorable vs. unfavorable ligand binding poses in a given protein—or molecular
dynamics (MD) simulations of selected protein-ligand complexes.
While docking programs are generally good at pose prediction,
they are less effective in predicting binding free-energy of proteinligand complexes. Similarly, while MD simulations are effective at

Al Saadi, Alfe, et al.

predicting binding-free energies, their intrinsic limitations in sampling protein-ligand complex formation processes imply that the
approach may be computationally infeasible to translate on large
compound libraries. If ML/AI methods can glue information across
docking and MD simulation techniques, while leveraging their individual strengths to provide meaningful feedback in terms of identifying parts of the compound libraries that may result in better lead
molecules, higher effective throughput (sampling larger compound
libraries), and meaningful enrichment of lead molecules for targeting
SARS-CoV-2 proteins would be possible.
IMPECCABLE addresses these scientific and methodological
challenges by providing a scalable, flexible, and extensible infrastructure for campaigns designed to discover improved leads targeted
at SARS-CoV-2. As shown in Fig. 1, the IMPECCABLE campaign
consists of an iterative loop initiated with ML predictions (ML1),
followed by three stages of data processing (S1, S2, S3). IMPECCABLE centers on the use of ML/AI techniques (ML1 and S2)
interfaced with physics-based computational methods to estimate
docking poses of compounds that are promising leads for a given
protein target (S1) and binding free-energy computations (S3).

High Throughput
Docking (S1)

Coarse Grained
Coarse
Grained
Binding
Aﬃnity
Binding
Aﬃnity
(S3-CG)
(S3-CG)

Latent Space Representation
Latent Space Representation
and
and
Steered Advanced
Sampling
Steered Advanced
(S2) Sampling
(S2)

Enhanced Sampling of
protein target states
ML-predict Docking
Scores (ML1)

Oﬄine Ensemble
Docking
Fine-Grained
Fine-Grained
Binding Aﬃnity
Binding
Aﬃnity
(S3-FG)
(S3-FG)

Stability
Measures/Features for
Protein-Ligand
Interactions

Improved Binding
Free-Energy Estimates

Figure 1: The IMPECCABLE Solution: represents an entire
virtual drug discovery pipeline, from hit to lead through to lead
optimization. The constituent components are deep-learning
based surrogate model for docking (ML1), Autodock-GPU (S1),
coarse and fine-grained binding free energies (S3-CG and S3FG) and S2 (DeepDriveMD).
ML techniques overcome the limitations of S1 and S3 by predicting the likelihood of binding between small molecules and a
protein target (ML1), and accelerating the sampling of conformational landscapes to bound the binding free-energy values for a
given protein-ligand complex (S2). Interfacing ML approaches with
physics-based models (docking and MD simulations), we achieve
at least three orders of magnitude improvement in the size of compound libraries that can be screened with traditional approaches,
while simultaneously providing access to binding free-energy calculations that can impose better confidence intervals in the ligands
selected for further (experimental or computational) optimization.
ML1: Machine Learning Models for docking score prediction
Scoring functions are used to score poses in order to determine the
most likely pose of the molecule, the magnitude of which is used

IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads

to provide an indication of active versus inactive ligands, and lastly
to rank order sets of libraries. We create a ML surrogate model to
replace the use of docking as a means of locating regions of chemical
space likely to include strong binding drug leads. The only free
variable for the surrogate ML ranking function is the basic molecular
information, which typically presents as a SMILES string. We use a
simple featurization method, namely 2D image depictions, as they
do not require complicated architectures such as graph convolution
networks, while demonstrating good prediction. We obtain these
image depictions from the nCov-Group Data Repository [10], which
contains various descriptors for 4.2B molecules generated on HPC
systems with the assistance of Parsl [11].
S1: High-throughput Docking Protein-ligand docking encompasses a computational pipeline consisting of ligand 3D structure
(conformer) enumeration, exhaustive docking and scoring, and final
pose scoring. The input to the docking protocol requires a protein structure with a designed binding region, or a crystallized ligand from which a region can be inferred, as well as a database of
molecules to dock in SMILES format. SMILES format is a compact
representation of a 2D molecule.
S2: Machine Learning Driven Molecular Dynamics Machine
learning tools are able to quantify statistical insights into the timedependent structural changes a biomolecule undergoes in simulations, identify events that characterize large-scale conformational
changes at multiple timescales, build low-dimensional representations of simulation data capturing biophysical/biochemical/biological
information, use these low-dimensional representations to infer kinetically and energetically coherent conformational substates, and
obtain quantitative comparisons with experiments.
Deep structured learning approaches automatically learn lowerlevel representations (or features) from the input data and successively aggregating them such that they can be used in a variety
of supervised, semi-supervised and unsupervised machine learning tasks. We developed variational autoencoders to automatically
reduce the high dimensionality of MD trajectories and cluster conformations into a small number of conformational states that share
similar structural, and energetic characteristics [14]. We use S2 to
drive adaptive sampling simulations, and use the acceleration of
“rare” events, to investigate protein-ligand interactions [28, 37].
S3: Binding Free Energy Calculations Hit-to-Lead (H2L), sometimes also called lead generation, is a step in the drug discovery
process where promising lead compounds are identified from initial
hits generated at preceding stages. It involves evaluation of initial
hits followed by some optimization of potentially good compounds
to achieve nanomolar affinities. The change in free energy between
free and bound states of protein and ligand, also known as binding
affinity, is a promising measure of the binding potency of a molecule,
and hence it is used as a parameter for evaluating and optimizing hits
at H2L stage. We employ the ESMACS protocol [47–49, 55, 56],
for estimating binding affinities of protein-ligand complexes. We
differentiate between coarse-grained (CG) and fine-grained (FG)
ESMACS variants, which differ in the number of replicas (6 vs. 24),
equilibration duration (1 vs. 2ns), simulation duration (4 vs. 10ns)
etc. The computational cost of ESMACS-CG is about an order of
magnitude less than that of ESMACS-FG.
IMPECCABLE integrates multiple methods and dynamically
selects active ligands for progressively expensive methods. In fact,

Supercomputing ’20, November 16–19, 2020, Virtual

at any stage, only the most promising candidates are advanced to
the next stage, yielding a N-deep pipeline, where each downstream
stage is computationally more expensive, but also more accurate
than previous stages. The methods chosen vary in computational
cost per ligand by more than six orders of magnitude; in the docking
stage of IMPECCABLE each dock costs about 10−4 node-hours per
ligand; fine-grained binding free energy costs about 102 node-hours
per ligand. This provides an important dynamic range of accuracy,
and thus potential for scientific performance enhancement. Tuning
the cost of each method by extending or contracting the number of
iterations of each method allows for enhanced scientific performance
and throughput.
The integration of methods with varying computational characteristics into a cohesive whole to support sustained computational
campaign requires innovative computational infrastructure. There
are no turnkey or shrink-wrap solutions to support campaigns. We
employ RADICAL-Cybertools (RCT) [36], a set of software systems, to manage the execution of heterogeneous workflows and
workloads on leadership computing facilities. RCT have the required
design and necessary abstractions to manage the heterogeneity of
workloads and platforms, and the integrated campaign.

4

State of the Art

We outline briefly the current state-of-the-art with respect to our
computational campaign, summarizing some of the challenges and
limitations in the context of IMPECCABLE .
The idea of building large-scale virtual screening pipelines is not
entirely new. However, many of these toolkits are either proprietary
(e.g., Schrodinger, OpenEye) or even if they are openly available
(e.g., AMPL [32]), they end up being very customized. Some of
the early approaches [26, 41, 60] proposed the use of statistical
techniques to be incorporated – either for downstream refinement or
for selection of viable leads – however, sheer size of ligand docking
simulations as well as MD simulations could easily overwhelm these
approaches, leading to very little gain from the overall process. With
the advent of deep learning [27] and recent advances in computing
technology for running MD simulations, the burden of handling large
volumes of virtual screening data and simulations has been largely
reduced. This in turn has spurred momentum in developing several
automated pipelines whose goal is to improve not just the virtual
screening process itself, but also to enable downstream processes
such as lead refinement and optimization.
Central to most high throughput virtual screening pipelines are
scoring functions that are used to determine which pose is selected
via exhaustive search and how the resulting molecule and pose rank
within a dataset. While some docking protocols expand the typical
exhaustive mapping of a scoring function over possible positions,
the distinction between protocols boils down to the scoring function.
The scoring function is used for three purposes: (i) to detect when
the ligand is properly positioned in the pocket (pose prediction);
(ii) to provide a general assessment of activity (virtual screening),
and (iii) to rank ordering compound libraries from their selected
poses (binding affinity ranking) [18, 23]. Given the central role of
scoring functions, the enterprise of molecular docking pragmatically
and theoretically rests on the speed and accuracy of these scoring
functions. Some scoring functions can be slow, and particularly well
suited for pose prediction, while others may be fast and only capable

Supercomputing ’20, November 16–19, 2020, Virtual

of decoy detection. However, scoring functions by themselves are not
sufficient; they need to be augmented with physically accessible (and
experimentally verifiable) quantities such as binding free-energies
(or binding affinities).
There are a large number of in silico methods available to calculate binding affinities. The major ones, in the order of increasing accuracy as well as computational expense, are as follows: (1)
molecular docking [13, 33], (2) linear interaction energy [63], (3)
MMPBSA/MMGBSA and (4) alchemical methods (Thermodynamic
Integration/Free Energy Perturbation) [43, 62]. Alchemical methods
are theoretically the most exact whereas the other three involve approximations to some extent, decreasing in the above order. This
is why we need to choose an appropriate method for each step in
the entire drug discovery process, keeping in mind the level of accuracy desired and the number of compounds to be considered. In
our workflow, we employ the cheapest and not greatly accurate
docking methods at the initial stages, MMPBSA-based ESMACS
at the hit-to-lead stage and TI based TIES at the lead optimization
stage. The throughput reduces by orders of magnitude as we move to
subsequent stages with higher levels of accuracy desired and hence
the computational cost remains under control.
As demonstrated in Tab. 2, IMPECCABLE integrates methods
with 6–7 orders of magnitude computational cost, and although
hitherto not part of our demonstrated scientific impact, can support
TIES which is a further two orders of magnitude more expensive
than ESMACS. Thus, IMPECCABLE represents a unique solution
that integrates multiple methods, with a collective and integrated
performance significantly greater than any single algorithm and
method alone.

5

Innovations Realized

IMPECCABLE embodies innovation within the individual methods
it employs, as well as in the way it couples these methods. Underpinning this coupling is a flexible and scalable infrastructure. The ability
to potentially screen much larger libraries with higher throughputs
so as to identify greater number of of viable SARS-CoV-2 protein
target specific leads is the ultimate measure of scientific productivity.
To that end, we characterize the contribution of each component,
as well as the improvement in performance of the IMPECCABLE
pipeline by integrating individual components. Actual results are
presented in Section 7.

5.1

Algorithmic & Methodological

5.1.1 S1: AutoDock-GPU The CUDA-accelerated AutoDock4.2.6
(AutoDockGPU) leverages a highly parallel implementation of the
Lamarckian genetic algorithm (LGA) by processing ligand-receptor
poses in parallel over multiple compute units. AutodockGPU was
developed in collaboration with NVIDIA and Aaron Scheinberg
(Jubilee Development) with a target of the Summit system at Oak
Ridge Leadership Computing Facility (OLCF). AutoDockGPU applies the legacy Solis-Wets local search method along with a new
local-search method based on gradients of the scoring function. One
of these methods, ADADELTA, has proven to increase significantly
the docking quality in terms of RMSDs and scores with observed
speedups of 56x over the original serial AutoDock 4.2 (Solis-Wets)
on CPU. A drug screen takes the best scoring pose from these independent outputs. Autodock-GPU uses OpenMP threading-based

Al Saadi, Alfe, et al.

pipeline for hiding ligand input and staging, and the receptor-reuse
functionality for docking many ligands to a single receptor. From
the computational performance perspective, we measure the total
number of docking calculations performed per GPU, which provides
a measure of the overall docking capability.
5.1.2 ML1: ML-based docking score predictor We developed
a ML surrogate model to replace docking as means of locating regions of chemical space likely to include strong binding drug leads.
Docking does not rely on proteins, instead proteins are coded into
the scoring function. Thus the only free variable for the surrogate
ML ranking function is the basic molecular information. This usually
presents as SMILES string, and there is an entire field of deep learning for molecular property prediction based on this approach [21].
A simple featurization method has been widely ignored—2D
image depictions. From the 2D depiction of a molecule, chemists
are generally able to identify major properties such as H-acceptors,
estimate the molecule’s weight, and even determine if a molecule
might bind to a protein [22, 35, 57]. This featurization method,
unlike graph structure, is able to utilize off-the-shelf convolutional
neural networks. By using 2D images, we are able to initialize our
models with pretrained weights that are typically scale and rotation
invariant for image classification tasks, which is exactly we require
in order to infer if a small molecule will bind well to a given SARSCoV-2 target.
We model the gains and losses using a cost function based on the
regression enrichment surface (RES)[18]. The RES measures how
well a surrogate model can detect the true top ranking molecules
given a certain allocation of predicted hits.
While this analysis brings forth the failure of the model to exactly
replicate the rank ordering of the compounds at scale, it provides
the operational benefit of these models—the predictive ML model
will indeed be able to filter with near 100% accuracy two orders of
magnitude from the data library. Thus, if all else is equal, with only
the additional cost of docking additional compounds, we are able to
expand the set of viable leads detected by two orders of magnitude,
without loss of performance in the top regions of detection (Sec. 7).
5.1.3 S3: Adaptive ESMACS Binding affinity is a small number
(a few tens of kcal/mol) that is derived from absolute free energies
which are large (a few hundreds to thousands of kcal/mol). Thus, the
usual practice of performing MMPBSA calculations on conformations generated using a single MD simulation does not give reliable
binding affinities. ESMACS, on the other hand, performs ensemble MD simulation, where each independent simulation is termed a
replica. Parameters such as the size of ensemble simulation (or the
number of replicas) and the length of individual replica are chosen
such that our results become reliable quantities [38, 54]. Another
factor that plays a role in determination of these parameters is the
level of precision desired and the cost-benefit ratio. The number of
replicas performed is adjusted to a find a sweet spot between computational cost and the level of precision acceptable at a particular
stage of the pipeline.
ESMACS is costlier than the standard approach of performing a
single simulation of similar duration. This increased cost however,
is more than compensated by the enhanced precision of ESMACS
results which makes the resultant ranking of compounds much more
reliable compared to standard approaches with similar accuracy.

IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads

MMPBSA based free energies have huge variability in results rendering them non-reproducible [38, 47, 49, 50, 54]. In fact, fewer
iterations are required to achieve the same level of convergence in
chemical space on using ensemble simulation based methods, which
leads to comparable (or even reduced) computational cost overall,
than on using standard single simulation approaches. This apparent
increased cost has advantages, such as an increased level of confidence in predicted ranking of compounds, and thus much more
reliable training data for an ML model.
We used ESMACS-CG to perform the initial screening of thousands of hits in order to reduce computational cost while compromising on the level of precision and ranking of compounds, and
used ESMACS-FG for the latter stages when we have better binding
poses, and/or LPC conformations. Selectively using ESMACS-FG
on a refined set of complexes decreases the computational cost substantially without affecting the quality of results.
5.1.4 S2: AI-Driven MD We leverage DeepDriveMD [29] to
simulate large ensembles of protein-ligand complexes. We have
shown that DeepDriveMD can potentially accelerate protein folding
simulations by at least 2 orders of magnitude. Here DeepDriveMD
(S2) builds an adaptive sampling framework to support the exploration of protein-ligand bound states that are not often accessible to
approaches such as ESMACS (S3).
A key innovation is support for extremely large numbers of ligandprotein complexes (LPC). This stems from the fact that a ESMACSCG (S3-CG) simulations may generate on average, six ensembles
which are analyzed by our novel MD-driven AI approaches to identify 5–10 novel states. Hence, we also implemented a novel approach
for analyzing large MD ensemble simulation datasets using a 3D
adversarial autoencoder (3D-AAE), a significant improvement over
approaches such as variational autoencoders in that it is more robust and generalizable to protein coordinate datasets than contact
maps (or other raw inputs) extracted from MD simulations. Similarly to autoencoders, 3D-AAE builds a latent embedding space
for MD simulations to characterize conformational changes within
protein-ligand complexes from ESMACS-CG/FG simulation trajectories. The 3D-AAE includes the PointNet encoder, Chamfer
distance-based reconstruction loss, and a Wasserstein adversarial
loss with gradient penalty to build a latent manifold on which all
simulations are projected. From this latent manifold, we use local
outlier factor (LOF) detection to identify ‘interesting’ protein-ligand
complexes that are then selected for S3-FG simulations. The iterative feedback between two stages of S3-CG/FG and S2 enables
accurate estimates for the binding free-energy, and allows us to filter
compounds based on their affinity to the protein, while accounting
for the intrinsic conformational flexibility of the LPC.
Measuring DeepDriveMD performance for LPCs presents additional challenges. For example, input from the S3-CG pipeline stage
are relatively short time-scale, whereas LPC association processes
tend to vary significantly in time scales. Thus, we chose a pragmatic
measure of LPC stability that takes into account the number of heavy
atom contacts between the protein and the ligand of interest. From
the top scoring LPCs that are selected from S3-CG, we use a novel
3D-AAE to filter those conformations that show increased stability
profiles in the LPCs. We posit that these LPCs are of the most interest, since the increased stability potentially contributes to favorable
interactions between protein and ligand. We also measure 3D-AAE

Supercomputing ’20, November 16–19, 2020, Virtual

performance in terms of its ability to learn effective latent space
representations from S3-CG stage (through standard measures such
as training and validation loss metrics).

Table 2: Normalized computational costs on Summit.
Method

Docking (S1)
BFE-CG (S3-CG)
Ad. Sampling (S2)
BFE-FG (S3-FG)
BFE-TI (not integrated)

Nodes per
ligand
1/6
1
2
4
64

Hours per
ligand
(approx)
0.0001
0.5
2
1.25
10

Node-hours
per ligand
∼0.0001
0.5
4
5
640

Putting it together and collective performance. Each stage of
IMPECCABLE when augmented with relatively simple ML/AI approaches provides a significant boost to the coverage of the compound diversity as well as conformational landscapes of proteinligand complexes. Using training data generated on small O(106 )
compound libraries, ML1 enables a significant improvement in filtering large compound O(109 ) libraries, increasing the coverage by
4–6 orders of magnitude.
The second step, which results in filtering the top 1% of these
compounds (which can also set by the end-user) through AutoDockGPU, identifies high confidence lead molecules that can bind to a
given SARS-CoV-2 target. The purpose of the ML1 is to predict if
the given molecule will dock the protein well, and not to predict the
docking pose. We exploit the intrinsic strengths of most docking
programs in predicting the binding pose for a given LPC, such that
the intial poses selected follow physical principles (i.e., optimizing
electrostatic and hydrophobic complementarity).
The next stage, namely S3-CG refines the filtered compounds to
obtain an estimate of the binding-free energy. This step is crucial in
the sense that it seeds the further pipeline with higher confidence
leads that may have favorable interactions with the protein target.
This set of diverse LPC are input to S2, which leverages the 3DAAE to learn a latent manifold that consists of a description of which
LPCs are most stable. In addition, the latent manifold also captures
intrinsic dimensions of the protein’s conformational landscape that
are perturbed by the ligand’s interactions. Using outlier detection
methods, we then filter these landscapes further to include only a
small number of LPCs on which SG-FG are implemented to ultimately suggest strong confidence intervals for binding free-energy
of the LPCs selected.
The final stage of the pipeline provides additional features that
identify key complementarity features (e.g., electrostatic interactions
through hydrogen bonds or hydrophobic interactions) that can be
input to the docking program for further refinement. Each successive
iteration of IMPECCABLE thus provides successive yields of LPCs
that could be modeled as an active learning pipeline for obtaining
highly specific small-molecules that can inhibit a SARS-CoV-2
protein of interest.

Supercomputing ’20, November 16–19, 2020, Virtual

5.2

Computational Infrastructure

The integration of aforementioned diverse methods with varying
computational characteristics, performance and scalability challenges, into a dynamic and adaptive computational campaign requires innovative computational infrastructure. The campaign workload is a diverse mix of task types, e.g., MPI, single GPU, multinode
GPU and regular CPU; this mix of tasks changes over the course of
the campaign There are multiple stages which couple and concurrently execute deep learning and traditional simulations. Coupling
and concurrently executing these diverse tasks is challenging, but
is made more difficult by virtue of having different models and
coupling with simulations across multiple stages.
The dynamic variation of workload arises due to many reasons,
for example: (i) adaptive methods, e.g., each LPC has a different rate
of convergence for structural and energetic properties, and thus the
duration varies; (ii) cost of docking per ligand varies across different
drug compound libraries and the ligands they contain; and (iii) for
methods that involve learning, (re-) training times are dependent on
specific ligands and the number of simulations.
IMPECCABLE employs the Ensemble Toolkit (EnTK)[12], which
uses RADICAL-Pilot (RP) [31] for flexible and scalable execution
of workflows with heterogeneous tasks. Together they conform to the
middleware building blocks architectural pattern — which constitute
recent advances in the science of HPC workflows [44] — to permit
a decoupling of the programming system from underlying execution
capabilities.
5.2.1 Programming System EnTK is a Python implementation
of a workflow engine, designed to support the programming and execution of applications with ensembles of tasks. EnTK executes tasks
concurrently or sequentially, depending on their arbitrary priority
relation. We use the term “task” to indicate a stand-alone process that
has well-defined input, output, termination criteria, and dedicated
resources. For example, a task can indicate an executable which
performs a simulation or a data processing analysis, executing on
one or more nodes on Summit. Tasks are grouped into stages and
stages into pipelines depending on the priority relation among tasks.
Tasks without a reciprocal priority relation can be grouped into the
same stage, whereas tasks that need to be executed before other tasks
have to be grouped into different stages. Stages are then grouped
into pipelines and, in turn, multiple pipelines can be executed either
concurrently or sequentially. Specifically, EnTK:
• permits asynchronous execution of concurrent pipelines (each
pipeline can progress at its own pace);
• allows arbitrary sizing of stages (variable concurrency);
• supports heterogeneous tasks of arbitrary types, and combinations, as well as their inter-mixing;
• promotes “ensembles” as first-class code abstraction;
• selects parameters at runtime so as to provide near-optimal
selection of cost versus accuracy [19, 20].
These are necessary capabilities to explore LPCs of varying complexity and cost, without constraining the number of concurrent
investigations, and different methods run in arbitrary order.
5.2.2 Execution Framework for Dynamic Resource Management Given the extreme workload heterogeneity and workload
variation between and across stages, dynamic resource management
is critical. Dynamic resource management capability is provided by

Al Saadi, Alfe, et al.

RADICAL-Pilot (RP) [31, 45], a Python implementation of the pilot
paradigm and architectural pattern [46]. Pilot systems enable users
to submit pilot jobs to computing infrastructures and then use the
resources acquired by the pilot to execute one or more workloads,
i.e., set of tasks. Tasks are executed concurrently and sequentially,
depending on the available resources. For example, given 10,000
single-node tasks and 1000 nodes, a pilot system will execute 1000
tasks concurrently and each one on the remaining 9000 tasks sequentially, whenever a node becomes available. RP enables the execution
of heterogeneous workloads comprised of one or more scalar, MPI,
OpenMP, multi-process, and multi-threaded tasks. RP directly schedules and executes on the resources of one or more pilots without
having to use the infrastructure’s batch system.
RP offers unique features when compared to other pilot systems
or tools that enable the execution of multi-task workloads on HPC
systems: (1) concurrent execution of heterogeneous tasks on the
same pilot; (2) support of all the major HPC batch systems; (3)
support of more than twelve methods to launch tasks; and (4) a
general purpose architecture. RP can execute single or multi core
tasks within a single compute node, or across multiple nodes. RP
isolates the execution of each tasks into a dedicated process, enabling
concurrent execution of heterogeneous tasks by design.

6

Performance Measurement

To understand the performance of IMPECCABLE it is imperative to
understand the nature of the computational campaign, its composite
workflows, their constituent workloads, desired performance and
factors determining scalability.

6.1

Computational Characteristics
ML1

Docking Surrogate

S1

AutoDock-GPU

S3-CG

S2

S3-FG

Figure 2: Programming and execution view: Each stage of
the (S3-CG)-(S2)-(S3-FG) pipeline comprises multiple heterogeneous tasks; each stage executes for varying durations.
Fig. 2 provides an overview of how the IMPECCABLE campaign
is constructed and executed. It comprises four distinct computational
workflows: a machine learning surrogate (ML1), docking (S1), binding free energy calculations (S3), and latent space representation
and steered advanced sampling via MD simulations (S2). Each is
a distinct workflow with well-defined inputs and outputs, multiple
executables with defined dependencies, and termination criteria,
able to produce stand-alone scientifically meaningful end-results.
Each workflow represents the expertise and unique scientific and
methodological contribution from a different team.

IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads

We codify IMPECCABLE workflows as a five-stage EnTK pipeline
using a general-purpose language (Python) and application-specific
constructs from the PST (Pipeline, Stage, Task) programming model.
These abstraction simplify creating and executing ensemble applications with complex coordination and communication requirements.
Pipelines can contain different workloads, e.g., distinct instances of
S* for a given LPC, but also possibly multiple instances of a given
S* for many LPC concurrently. Autodock-GPU is executed as a
single task running on several thousand nodes, as is the docking surrogate, which is a relatively short duration task. The remaining three
stages are workflows which are expressed as pipelines, comprised
of differing stages and varying duration and number of tasks concurrently executing. The horizontal length of a box is proportional
to the number of nodes utilized by a stage / computation, and the
vertical length of boxes represent the temporal duration; boxes are
not drawn to scale (see Table).
6.1.1 ML1: Deep Learning Docking Emulator This step is a
docking emulator which serves as a pre-selection tool for docking
calculations performed in step S1. The goal is to reduce the search
space from about 126M ligands down to a manageable amount for
the docking calculations. The emulator is based on a resnet-50 [24]
deep neural network: it transforms image representations of ligand
molecules into a docking score. To convert the ligand SMILES
strings into images we employed the mol2D drawing submodule
from rdKit [7]. The target scores are binding energies which are
mapped into the interval [0, 1], with higher scores representing lower
binding energies and thus higher docking probabilities. The main
computational motifs are dense linear algebra, convolutions and elementwise operations on 4D tensors. The network is implemented in
PyTorch and pre-trained on 500,000 randomly selected samples from
the OZD ligand dataset across each receptor (for our purposes, each
PDB entry corresponds to a separate receptor, providing access to
an ensemble of docking simulations). For deployment, we compiled
the model using NVIDIA TensorRT v7.2 [6] with cuDNN v8.0 [17]
employing the torch2trt helper tool [8]. As base precision we
chose half precision (FP16), so that we can use the Tensor Cores on
Summit’s V100 GPUs.
Inference workloads are notoriously IO bound, and thus we employ various optimizations to improve throughput. We start with the
ULT911 dataset [5], which is supplied as a collection of 12,648 files
with 10,000 ligands, each in Python pickle format. We first used gzip
to compress each file, achieving an average compression factor of
14.2. We use MPI to distribute the individual files evenly across a
large number of GPUs and bind one rank to each GPU. While we
perform the model scaffolding phase, i.e. creating the computational
graph and loading the weights from the pre-trained model file, each
rank stages its assigned shard of the data from GPFS into node-local
NVME. During the inference process, each rank utilizes multiple
data loader processes where each is employing 2 prefetching threads:
the first one loads compressed files from NVME into DRAM and
decompresses them on the fly while the second iterates through the
uncompressed data in memory, extracts the image and metadata
information and feeds them to the neural network. The whole logic
is implemented using the thread-safe python queue module. We
further use careful exception handling to make the setup resilient
against sporadic IO errors. After inference is done, the resulting lists
of docking scores and metadata information such as ligand id and

Supercomputing ’20, November 16–19, 2020, Virtual

Summit Launch node
1
RP Agent Bootstrapper
2
RP Scheduler

2
RP Executor

3

4

5

PRTE/JSRUN

5

5

Compute Node 1

Compute Node n

PRTE/JSRUN

PRTE/JSRUN

CPUs

GPUs

CPUs

Master

Worker

Worker

1 CPU

36 CPU

18 CPU

6 GPU

GPUs
6 GPU

6
MPI Task

32 CPU

Master
Worker

Task
Function

6
RP component
System Component

Figure 3: RAPTOR Execution Framework: One of the two execution frameworks used to support heterogeneous tasks and
dynamic workloads on Summit.

SMILES string are gathered and concatenated on rank 0 and written
into a CSV file which is forwarded to step S1.
6.1.2 S1: Physics-based Ensemble Docking To support the
scaling requirements of S1, we implemented a Master/Worker overlay on top of the pilot-job abstraction. Fig. 3 illustrates the RAdicalPilot Task OveRlay (RAPTOR) master/worker implementation on
Summit. Once RAPTOR has acquired its resources by submitting
a job to Summit’s batch system, it bootstraps its Agent (Fig. 3-1)
and then launches a task scheduler and a task executor (Fig. 3-2).
Scheduler and Executor launch one or more masters on one or more
compute nodes (Fig. 3-3). Once running, a master schedules one or
more workers on RP Scheduler (Fig. 3-4). Those workers are then
launched on one or more compute nodes by RP Executor (Fig. 3-5).
Finally, the master schedules function calls on the available workers
for execution (Fig. 3-6), load-balancing across workers so to obtain
maximal resource utilization.
The duration of the docking computation varies significantly between individual receptors. The long tail poses a challenge to load
balancing; the relatively short docking times pose a challenge to
scalability. Load balancing is addressed by iterating through the list
of compounds in a round-robin fashion, and by dynamic load distribution which depends on the load of the individual workers. Further,
balancing is achieved by: (i) tasks are communicated in bulks as to
limit the communication load and frequency; (ii) multiple master processes are used to limit the number of workers served by each master,
avoiding respective bottlenecks; (iii) multiple concurrent pilots are
used to isolate the docking computation of individual compounds
within each pilot allocation. The combination of these approaches
results in a near linear scaling up to several thousand nodes, while
maintaining high utilization for large numbers of concurrently used
nodes.

Supercomputing ’20, November 16–19, 2020, Virtual

6.1.3 S2 and S3: Advanced Sampling and Binding Free Energy We implement S2 and S3 as iterative pipelines that comprise
heterogeneous stages, with each stage supporting the parallel execution of tasks. In S2, the pipeline starts with MD simulations that are
run concurrently; it completes a single iteration by passing through
deep learning stages for AAE model training and the outlier detection. In a single iteration, tasks are scheduled across single GPU,
multiple GPUs, and CPU-GPU tasks. For instance, the MD stage
uses a single GPU per simulation (OpenMM), the data aggregation
stage uses CPUs only, the ML training stage uses six GPUs per
model, and the outlier detection stage uses a mixture of CPUs and
GPUs. We also employ data parallelism for model training using
PyTorch distributed data parallel module.
Similarly, S3 involves two stages of equilibration and one stage
of simulation; each stage runs an ensemble of from six (S3-CG) to
24 (S3-FG) OpenMM tasks. We also employ NAMD-based TIES in
conjunction with ESMACS; this requires placing distinct simulations
to GPU (OpenMM) and CPU (NAMD) concurrently for the optimal
resource utilization on Summit.
The architecture of RAPTOR (Fig. 3) differs from that of the
classic RADICAL-Pilot used for S2 and S3 on Summit [45]. The
need for two task execution frameworks arises primarily from the
dynamism and heterogeneity of workloads. For example, six orders
of magnitude difference in the temporal duration (Tab. 2) of tasks
requires that the Master/Worker overlay sustain a throughput of up
to 50M docking hits per hour on ∼1000 nodes on Summit.

7

Performance Results

We report on both scientific and computational performance.

7.1

Scientific Performance

Available compound libraries are large, with ZINC providing over
230 million purchasable compounds [42] in ready-to-dock, 3D formats, and MCULE having 100 million purchasable compounds in
similar formats [5]. Hence there is a need to obtain an appropriate
sampling of the compound libraries based on the diversity of the
compounds as well as their availability for both docking calculations (to generate the training data) and inference runs (ML1 results).
We selected a subset of 6.5 million compounds from the ZINC library along with the Enamine diversity set [3] and DrugBank [52]
compounds to develop our training library (OZD library, hereafter).
We also chose a similar subset of 6.5 million compounds from the
MCULE library (ORD library, hereafter) for the purposes of testing if ML1 can indeed be used for transferring knowledge learned
from one library to another. These libraries pay attention only to
the diversity of the compounds selected, and are independently selected, although between the two libraries we observed an overlap
of approximately 1.5 million compounds.
7.1.1 ML1 results We trained our ML1 models on docking runs
(generated offline) for the four main target SARS-CoV-2 proteins,
namely 3C like protease (3CLPro), papain-like protease (PLPro),
ADP-Ribose-1"-Monophosphatase (ADRP), and non-structural protein 15 (NSP15). These proteins all represent important drug targets
against SARS-CoV-2 virus. Here we present only a vignette of results from the PLPro target and specifically from the receptor derived
from the PDB-ID 6W9C. The RES plot from Fig. 4 indicates that the
extraction of the top scoring molecules from the OZD library can be
thought of as a selection process of 𝛿 molecules, to the subsequent

Al Saadi, Alfe, et al.

Figure 4: RES profile for PLPro docking runs. As explained in
the main text, RES provides a summary estimate of how many
top scoring compounds can be covered given some target number (𝛿) of molecules to be ranked.

stages. Given a specific budget of 𝛿 molecules to pass along, we
can imagine a vertical line along the 𝑥-axis of Fig. 4 at the point
𝛿/𝑢 representing the budget, where any point to the right of that
line represents an unattainable number of compounds. One can also
imagine a constraint through 𝑦 = 𝑥, as points above this line represent situations where a wider range of the top distribution may prove
too expensive, although reasonable for some tasks, uHTS screening
is in pursuit of ultra high ranking compounds.
Given these two constraints, one can see that as 𝛿 increases, so
to does the accuracy of capturing some desired threshold of the top
distribution. If downstream tasks allot 𝛿 = 𝑢10−3 compounds, then
the plot indicates that we will capture 50% of the top ranking 𝑢10−4
compounds, or around 40% of the top ranking 𝑢10−3 compounds.
In concrete terms, for this library, the ML model here correctly
identifies 500 of the top 1000 scoring compounds from the docking
study, or about 4000 of the top 10,000 compounds. However, not all
top-ranking compounds are correlated with obtaining high binding
affinity to PLPro. The RES plot also provides a quantitative estimate
of the number of compounds we have to sample from lower ranking
one so that we do not inadvertently miss out other high affinity
compounds. Hence we also select about 15–20% of compounds
from the RES to the subsequent stages.
7.1.2 S3: CG-ESMACS For each target of the four target proteins mentioned above, multiple crystal structures were used to
perform docking and a separate list of top 10,000 compounds based
on their docking scores was generated at the ML1 stage. Therefore,
depending on the number of crystal structures used for each target,
there were collectively 20,000-40,000 compounds available for performing binding affinity predictions using CG-ESMACS. At this
stage of our pipeline, we chose 10,000 compounds for each target
by picking out the structurally most diverse compounds from all
compounds available. This was done for two reasons: (i) based on
the docking scores, all the available compounds were stable poses,
and (ii) allowing for maximum possible coverage of the chemical
space allowing for better and quicker identification of its relevant
regions to focus on in next iterations.

IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads

A

D
RMSD (Å)

C

E

Binding Free energy (𝚫G)

B

L-78- FG-S3

RMSD (Å)

L-2105

L-4044

L-6939

L-78- CG-S3

L-6967

Figure 5: Preliminary results from IMPECCABLE on PLPro receptor (PDB ID: 6W9C). (A) Summary histogram of
the distribution of binding free energies estimated using CGESMACS. (B) Summary of RMSD (Å) determined from CGESMACS LPC ensembles show a rather tight distribution with
a few LPCs that exhibit greater fluctuations. (C) Latent space
representation from the 3D-AAE model depicting the outliers
from RMSD distributions (>1.9 ) and the rest as gray dots. The
latent space also summarizes the extent of sampling from these
simulations. (D) Structure of PLPro bound to one of the highly
specific molecule (L78) in its active site. (E) A zoomed in version of the same compound (L78) showing close interactions
with key residues in PLPro (green highlight). The panel on the
left depicts how upon running FG-ESMACS we obtain tighter
binding through the compound moving further into the binding site, forming strong hydrophobic interactions and hydrogen
bonds. Other compounds that did not perform as well in the
FG-ESMACS (see 6) are shown for comparison; although the
bound structures exhibit similar interaction patterns, none of
them stabilize further in the binding site, leading to a reduction
in their binding free energy estimates.

We performed CG-ESMACS to get binding affinities for all these
compounds chosen, amounting to a total of 40,000 S3-CG calculations thus far. Fig. 5A shows a probability distribution of the 10,000
binding affinities for PLPro. The values typically lie between -60 to
+20 kcal/mol for all proteins. The resultant trajectories and binding
affinity values from this stage were used as input for S2 to identify
potentially useful conformations that were fed into S3-FG.
7.1.3 Using S2 to seed S3-FG For PLPro, about 5000 compounds were chosen based on the structural diversity criterion for
PDBID 6w9c. The trajectories corresponding to these 5000 compounds generated by S3-CG were used to build a combined dataset
of 100,978 examples. The point cloud data, representing the coordinates of the 309 backbone C𝛼 atoms of the protein, was randomly
split into training (80%) and validation input (20%) and was used to
train the 3D-AAE model for 100 epochs using a batch size of 64. The
data was projected onto a latent space of 64 dimensions constrained
by a Gaussian prior distribution with a standard deviation of 0.2.
The loss optimization was performed with the Root Mean Square
Propagation (RMSprop) optimizer, a gradient descent algorithm for
mini-batch learning, using a learning rate of 0.00001.

Supercomputing ’20, November 16–19, 2020, Virtual

We also added hyperparameters to scale individual components of
the loss. The reconstruction loss was scaled by 0.5 and the gradient
penalty was scaled by a factor of 10. We trained the model using
several combinations of hyperparameters, mainly varying learning
rate, batch size and latent dimension. The embedding learned from
the 3D-AAE model summarizes a latent space that is similar to variational autoencoders, except that 3D-AAEs tend to be more robust to
outliers within the simulation data. The embeddings learned from the
simulations allow us to cluster the conformations (in an unsupervised
manner) based on their similarity in overall structure, which can be
typically measured using quantities such as root-mean squared deviations (RMSD). The 5,000 ligands were further analyzed and 5
structures with the lowest free energy (L6967, L2105, L78, L6939,
L4044) were selected for generating embeddings for 1200 examples,
using the hyperparameters learned from 3D-AAE performed on the
full set of 5,000 ligands. For visualizing and assessing the quality of
the model in terms latent space structure, we computed t-stochatstic
neighborhood embedding (t-SNE) [30] on the embeddings from the
validation set. The validation data was painted with grey while the
test data was painted with the root mean squared deviation (RMSD)
of each structure to the starting conformation (Fig. 5B-C).
7.1.4 S3: FG-ESMACS The large amount of data generated by
S3-CG was analysed at S2 and based on that potentially good conformations were identified for the compounds with large negative
binding affinities from CG-ESMACS. This process led us to filter
out five outlier conformations each for the top five compounds based
on S3-CG results. We used these 25 conformations to perform the
costlier FG-ESMACS calculations to demonstrate the capability of
our pipeline to confidently identify favourable interactions between
protein and ligands. This helps us mark favourable regions in the
chemical space deserving more attention, which in turn trains our
ML model to generate and/or predict better compounds in the next
iteration. Fig. 6 displays a comparison of the provisional results
from FG-ESMACS with those from CG-ESMACS. It is manifest
that FG-ESMACS predicts much lower binding affinities than those
predicted by CG-ESMACS (Fig. 5D-E). The force-field used in both
cases was the same; only the starting structures varied between them.
This implies that the outliers filtered out by S2 indeed captured
some favourable interactions and successfully identified good conformations out of the large number of conformations generated by
S3-CG. This is an excellent demonstration of the novel capability
of IMPECCABLE to quickly sample the relevant chemical space
and hence accelerate the process of drug discovery. With every iteration, such refinements improve the compounds generated by our
ML model manifold: we expect to design potential inhibitors for the
target protein in much less time than the standard drug discovery
approaches.

7.2

Computational Performance

Fig. 7 shows an example of how independent pipelines can be integrated into a single workflow. Each pipeline is comprised of stages,
each with an arbitrary number of tasks. Tasks have heterogeneous
execution time and computational requirements. Stages can execute
concurrently or sequentially, depending on available resources and
task, stage and pipeline interdependencies. In the depicted integration, single-GPU tasks execute alongside MPI GPU and few CPU
tasks, in distinct and customized execution environments. Note that

Supercomputing ’20, November 16–19, 2020, Virtual

Al Saadi, Alfe, et al.

Results from S3-CG and S3-FG

-40

∆GESMACS (kcal/mol)

FG
CG

-60

-80
l2105

l4044

l6939
Ligand

l6967

l78

Figure 6: Comparison of S3-CG and S3-FG results for the five
best binders for PLPro (PDBID: 6w9c) based on CG-ESMACS
results. S2 selected five outlier conformations for each binder
and performed FG-ESMACS on them. The provisional results
confirm improved binding for the selected conformations in all
five compounds, as FG energies are lower than CG.
the overheads (light-colored vertical areas of the plots) are invariant
to scale, i.e., they do not depend on the number of concurrent tasks
executed or on the length of those tasks.

We always normalize the measurements to a single Summit node
for the same reason. As all of our applications are perfectly load
balanced with respect to a Summit node (mostly even with respect
to individual GPUs within that node), this procedure yields a representative flop count. We use the methodology of Yang et al. [59] and
the NVIDIA NSight Compute 2020 GPU profiling tool to measure
flops for all precisions and sum them to obtain a mixed precision
flop count. When possible, we use start/stop profiler hooks to filter
out the representative work units. In order to obtain the flop rate, we
divide the aggregated flops for each EnTK task by the time it takes
to complete that respective task, including pre- and post-processing
overhead. Note that we do not account for any CPU flops invested
in this calculation as we expect that number to be small. We discuss
the specifics for each component:
ML1: We count flops as described above for 10 steps at batch size
256. From that, we derive a flop count per batch per GPU.
S1: We count flops for a five-ligand AutoDock-GPU run on one GPU
to derive flops for a single ligand. We chose this ligand complexity
to represent the majority of the ligands processed in the run.
S2: This stage has multiple steps, but we only account for the autoencoder training and the MD performed in this stage. For the former,
we measure the flops per batch for a batch size of 32 for training
and validation separately and weight them proportionally by their
relative number of batches. After each training epoch, a validation
is performed and the train/validation dataset split is 80%/20%. This
can be translated into a overall flop count for the full autoencoder
stage. For the MD part, we profile 20 steps of OpenMM and compute
a complexity per step.
S3-CG/FG: These two stages both have two steps, a minimization
and an MD step. We count the flops for 10 iterations of the minimization algorithm and for 20 steps of the MD run to derive a flop count
per minimization and MD step. Since the algorithmic complexity
differs between CG and FG, we profile those separately.

8
Figure 7: A time-series of node utilization. The Fig. depicts
the integrated execution of three GPU-intensive workflows (S3CG)-(S2)-(S3-FG). S3-CG, S2 and S3-FG are heterogeneous
and multi-stage workflows themselves.
We measure flops (floating point operations, not rates) per work
unit for the most relevant components of each stage. We define
a work unit to be a representative code section such as an MD
time integration step for MD-based or a data sample for DL-based
applications. Thus we can compute the aggregate invested flops by
scaling the measured flop counts to the respective work set sizes
used in the actual runs.
Table 3: Throughput and performance measured as peak flop
per second (mixed precision, measured over short but time interval) per Summit node (6 NVIDIA V100 GPU).
Comp.
ML1
S1
S3–CG
S3–FG

#GPUs
1536
6000
6000
6000

Tflop/s
753.9
112.5
277.9
732.4

Throughput
319674 ligands/s
14252 ligands/s
2000 ligand/s
200 ligand/s

Implications

Multiscale biophysics-based computational lead discovery is an
important strategy for drug development and while it can be considerably faster than experimental screening it has been until now too
slow to explore libraries of the scale of billions of molecules even
on the fastest machines.
The work reported here not only addresses this performance issue,
by demonstrating a path towards an overall improvement of throughput of computational drug discovery of order 1000x by integrating
machine learning components with the physics based components,
but it also addresses other important aspects of improved workflows
for computational lead discovery, namely the generalization and
integration of feedback between the physics models and the ML
models in a tightly coupled workflow.
By introducing ML modules paired with and trained from the
physics modules output, over time the ML component models improve such that the overall workflow becomes tuned to the specific
target problem. It is also likely that it will be possible to transfer
these learnings from one problem to the next. The generalizability
of this approach is under active investigation.
The feedback is not only local to each stage of the workflow
but end-to-end, so this work also represents the beginning of an

IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads

autonomous drug development system when coupled to automated
experimental screening and eventually to drug synthesis capability.
The work reported here has played a central role in the DOE effort
to use computational molecular design approaches to develop medical therapeutics for COVID-19. DOE established in April 2020 the
National Virtual Biotechnology Laboratory (NVBL) [4], to organize
the DOE national laboratories into a series of projects—including
therapeutics development—aimed at addressing key challenges in
responding to COVID-19. Methods and infrastructure reported in
this paper are being used to screen over 4.2 billion molecules [10]
against over a dozen drug targets in SARS-CoV-2. This work has
already lead to the identification and experimental validation of over
1000 compounds, resulting in over 40 hits that are progressing to
advanced testing.
To get a sense of the scale of operations using methods and infrastructure reported here: in the past three months, we have used
more than 2.5M node-hours across diverse HPC platforms, including TACC’s Frontera, Livermore Computing’s Lassen, ANL’s Theta
(and associated A100 nodes), LRZ’s SuperMUC-NG, and ORNL
Summit to obtain scientific results. We have executed individual
parts of the campaign on suitable platforms, e.g., we sustained 40M
docking hits per hour over 24 hours on 4000 nodes on Frontera.
We performed thousands of AI-driven MD simulations across GPUbased systems: Lassen, Summit and Theta’s A100 nodes, as well as
a few tens of thousands of CG-ESMACS runs on Summit, ARCHER
(EPCC, UK), Monsoon2 (Metoffice, UK) and SuperMUC-NG.
In doing so, IMPECCABLE has screened ∼1011 ligands. We have
performed up to 5 × 107 docking-hits per hour using OpenEye and
Autodock-GPU, and sustained this throughput on ∼4000 nodes. Individual workflow components deliver 100× to 1000× improvement
over traditional methods. IMPECCABLE has computed binding free
energies on 104 LPCs concurrently.
While much work remains to be done, we have demonstrated
some important milestones towards the ultimate goal. These include:
orders of magnitude end-to-end performance improvement of computational methods for drug discovery, making it feasible and routine
to search giga-scale libraries of compounds across collections of
drug targets; integration of physics based modeling with AI methods
into multiscale workflow at the largest-scale possible providing a
pathway for exascale drug discovery, and developed a prototype infrastructure that can be adapted to a broad range of near autonomous
drug development scenarios by means of additional modules and
models that fill out the drug discovery pipeline.
We have developed this campaign with multiple target problems
in mind. A major influence is our experience in working on the
DOE/NCI JDAS4C Pilot1 effort to advance Cancer drug development through AI [58] and the related ECP CANDLE project [53]. In
the Pilot1 activity we are building Cancer drug response models that
predict the response of tumors to drugs or drug combinations. Such
models could be coupled to the the workflow described in this paper
to add additional feedback on predicted efficacy of a target molecule.
Work in this area is ongoing to build a bridge between tissue level
efficacy models such as the ones developed in Pilot1 and CANDLE
and drug target oriented models reported here.
We envision ultimately a series of models to be part of this extended workflow that predict not only efficacy, but also other important properties of candidate compounds such as drug metabolism,

Supercomputing ’20, November 16–19, 2020, Virtual

pharmacokinetics, absorption, toxicity, distribution, and excretion
that traditionally would be assessed via experimental methods. Building a comprehensive suite of ML-based predictors is precisely the
goal of the ATOM consortium, to which this project is a contributor. ATOM aims to dramatically accelerate drug development by
building and training ML models that systematically replace routine
experimental screening in early stages of drug development, with
the goal of reducing the time from drug target to clinical trial from
5–6 years to one year.
Acknowledgements: Research was supported by the DOE Office of Science
through the National Virtual Biotechnology Laboratory, a consortium of
DOE national laboratories focused on response to COVID-19, with funding
provided by the Coronavirus CARES Act. This research was supported as
part of the CANDLE project by the Exascale Computing Project (17-SC20-SC), a collaborative effort of the U.S. Department of Energy Office of
Science and the National Nuclear Security Administration. This work has
been supported in part by the Joint Design of Advanced Computing Solutions
for Cancer (JDACS4C) program established by the U.S. Department of
Energy (DOE) and the National Cancer Institute (NCI) of the National
Institutes of Health. We are grateful for funding for the UK MRC Medical
Bioinformatics project (grant no. MR/L016311/1), the UK Consortium on
Mesoscale Engineering Sciences (UKCOMES grant no. EP/L00030X/1)
and the European Commission for the EU H2020 CompBioMed2 Centre
of Excellence (grant no. 823712), as well as financial support from the
UCL Provost. Access to SuperMUC-NG, at the Leibniz Supercomputing
Centre in Garching, was made possible by a special COVID-19 allocation
award from the Gauss Centre for Supercomputing in Germany. Anda Trifan
acknowledges support from the United States Department of Energy through
the Computational Sciences Graduate Fellowship (DOE CSGF) under grant
number: DE-SC0019323. We acknowledge amazing support from OLCF—
Don Maxwell, Bronson Messier and Sean Wilkinson. We also wish to thank
Dan Stanzione and Jon Cazes at Texas Advanced Computing Center.

Supercomputing ’20, November 16–19, 2020, Virtual

References
[1] [n. d.]. COVID-19 Molecular Structure and Therapeutics Hub. https://covid.
molssi.org.
[2] [n. d.]. COVID Moonshot. https://covid.postera.ai/covid.
[3] [n. d.]. Enamine diversity library. https://enamine.net/hit-finding/diversitylibraries.
[4] [n. d.]. National Virtual Biotechnology Laboratory (NVBL). https://science.osti.
gov/nvbl.
[5] [n. d.]. Ultimate 100 Million Compounds. https://ultimate.mcule.com.
[6] 2020. NVIDIA TensorRT developer blog. https://developer.nvidia.com/blog/tag/
tensorrt/
[7] 2020. rdKit Mol2D drawing documentation. http://rdkit.org/docs/source/rdkit.
Chem.Draw.rdMolDraw2D.html
[8] 2020. torch2trt github repository. https://github.com/NVIDIA-AI-IOT/torch2trt
[9] Dinler A Antunes, Didier Devaurs, and Lydia E Kavraki. 2015. Understanding the
challenges of protein flexibility in drug design. Expert opinion on drug discovery
10, 12 (2015), 1301–1313.
[10] Yadu Babuji, Ben Blaiszik, Tom Brettin, Kyle Chard, Ryan Chard, Austin Clyde,
Ian Foster, Zhi Hong, Shantenu Jha, Zhuozhao Li, et al. 2020. Targeting SARSCoV-2 with AI-and HPC-enabled lead generation: A First Data release. arXiv
preprint arXiv:2006.02431 (2020).
[11] Yadu Babuji, Anna Woodard, Zhuozhao Li, Daniel S Katz, Ben Clifford, Rohan
Kumar, Lukasz Lacinski, Ryan Chard, Justin M Wozniak, Ian Foster, and Kyle
Chard. 2019. Parsl: Pervasive parallel programming in Python. In 28th International Symposium on High-Performance Parallel and Distributed Computing.
25–36.
[12] Vivek Balasubramanian, Matteo Turilli, Weiming Hu, Matthieu Lefebvre, Wenjie Lei, Ryan Modrak, Guido Cervone, Jeroen Tromp, and Shantenu Jha. 2018.
Harnessing the power of many: Extensible toolkit for scalable ensemble applications. In 2018 IEEE International Parallel and Distributed Processing Symposium
(IPDPS). IEEE, 536–545.
[13] Pedro J. Ballester, Adrian Schreyer, and Tom L. Blundell. 2014. Does a
More Precise Chemical Description of Protein–Ligand Complexes Lead to
More Accurate Prediction of Binding Affinity? Journal of Chemical Information and Modeling 54, 3 (2014), 944–955. https://doi.org/10.1021/ci500091r
arXiv:https://doi.org/10.1021/ci500091r PMID: 24528282.
[14] Debsindhu Bhowmik, Shang Gao, Michael T. Young, and Arvind Ramanathan.
2018. Deep clustering of protein folding simulations. BMC Bioinformatics 19, 18
(2018), 484. https://doi.org/10.1186/s12859-018-2507-5
[15] Regine S Bohacek, Colin McMartin, and Wayne C Guida. 1996. The art and practice of structure-based drug design: A molecular modeling perspective. Medicinal
research reviews 16, 1 (1996), 3–50.
[16] JR Broach and J Thorner. 1996. High-throughput screening for drug discovery.
Nature 384, 6604 Suppl (November 1996), 14—16. https://doi.org/10.1038/
384014a0
[17] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John
Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: Efficient Primitives
for Deep Learning. arXiv:cs.NE/1410.0759
[18] Austin Clyde, Xiaotian Duan, and Rick Stevens. 2020. Regression Enrichment
Surfaces: a Simple Analysis Technique for Virtual Drug Screening Models. arXiv
preprint arXiv:2006.01171 (2020).
[19] Jumana Dakka, Kristof Pallas Vivek Balasubramanian, Matteo Turilli, David W
Wright, Stefan J Zasada, Shunzhou Wan, Peter V Coveney, and Shantenu Jha.
2018. Concurrent and Adaptive Extreme Scale Binding Free Energy Calculations.
14th IEEE International Conference on e-Science, e-Science 2018, Amsterdam
(2018), 189–200. https://doi.org/10.1109/eScience.2018.00034
[20] Jumana Dakka, Kristof Farkas-Pall, Vivek Balasubramanian, Matteo Turilli, Shunzhou Wan, David W. Wright, Stefan J. Zasada, Peter V. Coveney, and Shantenu Jha.
2018. Enabling Trade-offs Between Accuracy and Computational Cost: Adaptive
Algorithms to Reduce Time to Clinical Insight. In 18th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing, CCGRID 2018, Washington,
DC, USA, May 1-4, 2018. 572–577. https://doi.org/10.1109/CCGRID.2018.00005
[21] Daniel C. Elton, Zois Boukouvalas, Mark D. Fuge, and Peter W. Chung. 2019.
Deep learning for molecular design—a review of the state of the art. Molecular
Systems Design & Engineering 4, 4 (2019), 828–849. https://doi.org/10.1039/
c9me00039a
[22] Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas, and Nathan
Baker. 2017. Chemception: A Deep Neural Network with Minimal Chemistry
Knowledge Matches the Performance of Expert-developed QSAR/QSPR Models.
arXiv:stat.ML/1706.06689
[23] Isabella A Guedes, Felipe SS Pereira, and Laurent E Dardenne. 2018. Empirical scoring functions for structure-based virtual screening: applications, critical
aspects, and challenges. Frontiers in Pharmacology 9 (2018), 1089.
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual
Learning for Image Recognition. arXiv:cs.CV/1512.03385
[25] James P Hughes, Stephen Rees, S Barrett Kalindjian, and Karen L Philpott. 2011.
Principles of early drug discovery. British journal of pharmacology 162, 6 (2011),

Al Saadi, Alfe, et al.

1239–1249.
[26] Ashutosh Kumar and Kam Y.J. Zhang. 2015. Hierarchical virtual screening
approaches in small molecule drug discovery. Methods 71 (2015), 26 – 37.
https://doi.org/10.1016/j.ymeth.2014.07.007 Virtual Screening.
[27] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature
521, 7553 (2015), 436–444. https://doi.org/10.1038/nature14539
[28] H. Lee, M. Turilli, S. Jha, D. Bhowmik, H. Ma, and A. Ramanathan. 2019. DeepDriveMD: Deep-Learning Driven Adaptive Molecular Simulations for Protein
Folding. In 2019 IEEE/ACM Third Workshop on Deep Learning on Supercomputers (DLS). 12–19.
[29] Hyungro Lee, Matteo Turilli, Shantenu Jha, Debsindhu Bhowmik, Heng Ma,
and Arvind Ramanathan. 2019. DeepDriveMD: Deep-Learning Driven Adaptive
Molecular Simulations for Protein Folding. In 2019 IEEE/ACM Third Workshop
on Deep Learning on Supercomputers (DLS). IEEE, 12–19. https://doi.org/10.
1109/DLS49591.2019.00007 arXiv:1909.07817
[30] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research 9, Nov (2008), 2579–2605.
[31] Andre Merzky, Matteo Turilli, Manuel Maldonado, Mark Santcroos, and Shantenu
Jha. 2018. Using pilot systems to execute many task workloads on supercomputers.
In Workshop on Job Scheduling Strategies for Parallel Processing. Springer, 61–
82.
[32] Amanda J Minnich, Kevin McLoughlin, Margaret Tse, Jason Deng, Andrew
Weber, Neha Murad, Benjamin D Madej, Bharath Ramsundar, Tom Rush, Stacie
Calad-Thomson, et al. 2020. AMPL: A Data-Driven Modeling Pipeline for Drug
Discovery. Journal of Chemical Information and Modeling 60, 4 (2020), 1955–
1968.
[33] N Moitessier, P Englebienne, D Lee, J Lawandi, and C R Corbeil. 2008.
Towards the development of universal, fast and highly accurate docking/scoring methods: a long way to go.
British Journal of Pharmacology 153, S1 (2008), S7–S26.
https://doi.org/10.1038/sj.bjp.0707515
arXiv:https://bpspubs.onlinelibrary.wiley.com/doi/pdf/10.1038/sj.bjp.0707515
[34] Jerry M. Parks and Jeremy C. Smith. 2020.
How to Discover Antiviral Drugs Quickly.
New England Journal of Medicine 382,
23 (2020), 2261–2264.
https://doi.org/10.1056/NEJMcibr2007042
arXiv:https://doi.org/10.1056/NEJMcibr2007042 PMID: 32433861.
[35] Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay Pande. 2015. Massively Multitask Networks for Drug Discovery.
arXiv:stat.ML/1502.02072
[36] Research in Advanced DIstributed Cyberinfrastructure and Applications Laboratory (RADICAL). 2019. RADICAL-Cybertools (RCT). https://github.com/
radical-cybertools, Last accessed on 2019-03-31.
[37] Raquel Romero, Arvind Ramanathan, Tony Yuen, Debsindhu Bhowmik, Mehr
Mathew, Lubna Bashir Munshi, Seher Javaid, Madison Bloch, Daria Lizneva,
Alina Rahimova, Ayesha Khan, Charit Taneja, Se-Min Kim, Li Sun, Maria I.
New, Shozeb Haider, and Mone Zaidi. 2019. Mechanism of glucocerebrosidase activation and dysfunction in Gaucher disease unraveled by molecular
dynamics and deep learning. Proceedings of the National Academy of Sciences 116, 11 (2019), 5086–5095. https://doi.org/10.1073/pnas.1818411116
arXiv:https://www.pnas.org/content/116/11/5086.full.pdf
[38] S. Kashif Sadiq, David Wright, Simon J. Watson, Stefan J. Zasada, Ileana Stoica,
and Peter V. Coveney. 2008. Automated Molecular Simulation Based Binding
Affinity Calculator for Ligand-Bound HIV-1 Proteases. Journal of Chemical
Information and Modeling 48, 9 (2008), 1909–1919. https://doi.org/10.1021/
ci8000937 arXiv:https://doi.org/10.1021/ci8000937 PMID: 18710212.
[39] Atanu Saha and Heather Roberts. 2020. Pharmaceutical industry’s changing
market dynamics. International Journal of the Economics of Business (2020),
1–17.
[40] Justin S Smith, Adrian E Roitberg, and Olexandr Isayev. 2018. Transforming
computational drug discovery with machine learning and AI.
[41] Francesca Spyrakis, Paolo Benedetti, Sergio Decherchi, Walter Rocchia, Andrea Cavalli, Stefano Alcaro, Francesco Ortuso, Massimo Baroni, and Gabriele
Cruciani. 2015. A Pipeline To Enhance Ligand Virtual Screening: Integrating
Molecular Dynamics and Fingerprints for Ligand and Proteins. Journal of Chemical Information and Modeling 55, 10 (2015), 2256–2274. https://doi.org/10.
1021/acs.jcim.5b00169 arXiv:https://doi.org/10.1021/acs.jcim.5b00169 PMID:
26355717.
ZINC 15 – Ligand Dis[42] Teague Sterling and John J. Irwin. 2015.
covery for Everyone.
Journal of Chemical Information and Modeling 55, 11 (2015), 2324–2337.
https://doi.org/10.1021/acs.jcim.5b00559
arXiv:https://doi.org/10.1021/acs.jcim.5b00559 PMID: 26479676.
[43] T. P. Straatsma, H. J. C. Berendsen, and J. P. M. Postma. 1986. Free energy of
hydrophobic hydration: A molecular dynamics study of noble gases in water. The
Journal of Chemical Physics 85, 11 (1986), 6720–6727. https://doi.org/10.1063/
1.451846 arXiv:https://doi.org/10.1063/1.451846
[44] Matteo Turilli, Vivek Balasubramanian, Andre Merzky, Ioannis Paraskevakos,
and Shantenu Jha. 2019. Middleware building blocks for workflow systems.
Computing in Science & Engineering 21, 4 (2019), 62–75.

IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads

[45] Matteo Turilli, Andre Merzky, Thomas Naughton, Wael Elwasif, and Shantenu
Jha. 2019. Characterizing the Performance of Executing Many-tasks on Summit.
IPDRM Workshop, SC19 (2019). https://arxiv.org/abs/1909.03057.
[46] Matteo Turilli, Mark Santcroos, and Shantenu Jha. 2018. A Comprehensive
Perspective on Pilot-Job Systems. ACM Comput. Surv. 51, 2, Article 43 (April
2018), 32 pages. https://doi.org/10.1145/3177851
[47] Shunzhou Wan, Agastya P. Bhati, Sarah Skerratt, Kiyoyuki Omoto, Veerabahu
Shanmugasundaram, Sharan K. Bagal, and Peter V. Coveney. 2017. Evaluation
and Characterization of Trk Kinase Inhibitors for the Treatment of Pain: Reliable
Binding Affinity Predictions from Theory and Computation. Journal of Chemical
Information and Modeling 57, 4 (2017), 897–909. https://doi.org/10.1021/acs.
jcim.6b00780 arXiv:https://doi.org/10.1021/acs.jcim.6b00780 PMID: 28319380.
[48] Shunzhou Wan, Agastya P Bhati, Stefan J Zasada, and Peter V Coveney. 2020.
Rapid, accurate, precise and reproducible ligand-protein binding free energy
prediction. Journal of Royal Society Interface Focus 10 (2020). https://doi.org/
10.1098/rsfs.2020.0007
[49] Shunzhou Wan, Agastya P. Bhati, Stefan J. Zasada, Ian Wall, Darren Green,
Paul Bamborough, and Peter V. Coveney. 2017. Rapid and Reliable Binding
Affinity Prediction of Bromodomain Inhibitors: A Computational Study. Journal
of Chemical Theory and Computation 13, 2 (2017), 784–795. https://doi.org/
10.1021/acs.jctc.6b00794 arXiv:https://doi.org/10.1021/acs.jctc.6b00794 PMID:
28005370.
[50] Shunzhou Wan, Bernhard Knapp, David W. Wright, Charlotte M. Deane,
and Peter V. Coveney. 2015. Rapid, Precise, and Reproducible Prediction
of Peptide–MHC Binding Affinities from Molecular Dynamics That Correlate Well with Experiment. Journal of Chemical Theory and Computation 11, 7 (2015), 3346–3356.
https://doi.org/10.1021/acs.jctc.5b00179
arXiv:https://doi.org/10.1021/acs.jctc.5b00179 PMID: 26575768.
[51] Xueqing Wang and Yuanfang Guan. [n. d.]. COVID-19 drug repurposing: A review
of computational screening methods, clinical trials, and protein interaction assays.
Medicinal Research Reviews n/a, n/a ([n. d.]). https://doi.org/10.1002/med.21728
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/med.21728
[52] David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, Nazanin
Assempour, Ithayavani Iynkkaran, Yifeng Liu, Adam Maciejewski, Nicola Gale,
Alex Wilson, Lucy Chin, Ryan Cummings, Diana Le, Allison Pon, Craig Knox,
and Michael Wilson. 2017. DrugBank 5.0: a major update to the DrugBank
database for 2018. Nucleic Acids Research 46, D1 (11 2017), D1074–D1082.
https://doi.org/10.1093/nar/gkx1037 arXiv:https://academic.oup.com/nar/articlepdf/46/D1/D1074/23162116/gkx1037.pdf
[53] Justin M. Wozniak, Rajeev Jain, Prasanna Balaprakash, Jonathan Ozik, Nicholson T. Collier, John Bauer, Fangfang Xia, Thomas Brettin, Rick Stevens, Jamaludin
Mohd-Yusof, Cristina Garcia Cardona, Brian Van Essen, and Matthew Baughman.
2018. CANDLE/Supervisor: A workflow framework for machine learning applied
to cancer research. BMC Bioinformatics 19, 491 (2018).
[54] David W. Wright, Benjamin A. Hall, Owain A. Kenway, Shantenu Jha,
and Peter V. Coveney. 2014. Computing Clinically Relevant Binding Free
Energies of HIV-1 Protease Inhibitors. Journal of Chemical Theory and
Computation 10, 3 (2014), 1228–1241.
https://doi.org/10.1021/ct4007037
arXiv:https://doi.org/10.1021/ct4007037 PMID: 24683369.
[55] David W. Wright, Fouad Husseini, Shunzhou Wan, Christophe Meyer, Herman van Vlijmen, Gary Tresadern, and Peter V. Coveney. 2020. Application of the ESMACS Binding Free Energy Protocol to a Multi-Binding
Site Lactate Dehydogenase A Ligand Dataset. Advanced Theory and Simulations 3, 1 (2020), 1900194.
https://doi.org/10.1002/adts.201900194
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/adts.201900194
[56] David W Wright, Shunzhou Wan, Christophe Meyer, Herman Van Vlijmen, Gary
Tresadern, and Peter V Coveney. 2019. Application of ESMACS binding free
energy protocols to diverse datasets: Bromodomain-containing protein 4. Scientific
Reports 9, 1 (2019), 1–15.
[57] Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet: a
benchmark for molecular machine learning. Chem. Sci. 9 (2018), 513–530. Issue
2. https://doi.org/10.1039/C7SC02664A
[58] Fangfang Xia, Maulik Shukla, Thomas Brettin, Cristina Garcia-Cardona, Judith
Cohn, Jonathan E. Allen, Sergei Maslov, Susan L. Holbeck, James H. Doroshow,
Yvonne A. Evrard, Eric A. Stahlberg, and Rick L. Stevens. 2018. Predicting tumor
cell line response to drug pairs with deep learning 19 (2018).
[59] Charlene Yang. 2020. Hierarchical Roofline Analysis: How to Collect Data using
Performance Tools on Intel CPUs and NVIDIA GPUs. arXiv:cs.DC/2009.02449
[60] Xiaohua Zhang, Sergio E. Wong, and Felice C. Lightstone. 2014. Toward Fully
Automated High Performance Computing Drug Discovery: A Massively Parallel
Virtual Screening Pipeline for Docking and Molecular Mechanics/Generalized
Born Surface Area Rescoring to Improve Enrichment. Journal of Chemical Information and Modeling 54, 1 (2014), 324–337. https://doi.org/10.1021/ci4005145
arXiv:https://doi.org/10.1021/ci4005145 PMID: 24358939.
[61] Yadi Zhou, Fei Wang, Jian Tang, Ruth Nussinov, and Feixiong Cheng. 2020.
Artificial intelligence in COVID-19 drug repurposing. The Lancet Digital Health

Supercomputing ’20, November 16–19, 2020, Virtual

(2020).
[62] Robert W. Zwanzig. 1954. High-Temperature Equation of State by a Perturbation
Method. I. Nonpolar Gases. The Journal of Chemical Physics 22, 8 (1954), 1420–
1426. https://doi.org/10.1063/1.1740409 arXiv:https://doi.org/10.1063/1.1740409
[63] Johan Åqvist, Victor B. Luzhkov, and Bjørn O. Brandsdal. 2002. Ligand Binding
Affinities from MD Simulations. Accounts of Chemical Research 35, 6 (2002), 358–
365. https://doi.org/10.1021/ar010014p arXiv:https://doi.org/10.1021/ar010014p
PMID: 12069620.

