GOAT: GPU Outsourcing of Deep Learning Training With Asynchronous
Probabilistic Integrity Verification Inside Trusted Execution Environment

arXiv:2010.08855v1 [cs.CR] 17 Oct 2020

A P REPRINT
Aref Asvadishirehjini
Department of Computer Science
University of Texas at Dallas
aref@utdallas.edu

Murat Kantarcioglu
Department of Computer Science
University of Texas at Dallas
muratk@utdallas.edu

Bradley Malin
Department of Biomedical Informatics
Vanderbilt University Medical Center
b.malin@vanderbilt.edu

A BSTRACT
Machine learning models based on Deep Neural Networks (DNNs) are increasingly deployed in a
wide range of applications, ranging from self-driving cars to COVID-19 treatment discovery. To
support the computational power necessary to learn a DNN, cloud environments with dedicated
hardware support have emerged as critical infrastructure. However, there are many integrity challenges
associated with outsourcing computation. Various approaches have been developed to address these
challenges, building on trusted execution environments (TEE). Yet, no existing approach scales up to
support realistic integrity-preserving DNN model training for heavy workloads (deep architectures
and millions of training examples) without sustaining a significant performance hit. To mitigate
the time gap between pure TEE (full integrity) and pure GPU (no integrity), we combine random
verification of selected computation steps with systematic adjustments of DNN hyperparameters
(e.g., a narrow gradient clipping range), hence limiting the attacker’s ability to shift the model
parameters significantly provided that the step is not selected for verification during its training phase.
Experimental results show the new approach achieves 2X to 20X performance improvement over
pure TEE based solution while guaranteeing a very high probability of integrity (e.g., 0.999) with
respect to state-of-the-art DNN backdoor attacks.

1

Introduction

Every day, Artificial Intelligence (AI) and Deep Learning (DL) are incorporated into some new aspects of the society.
As a result, numerous industries increasingly rely on DL models to make decisions, ranging from computer vision to
natural language processing [1, 2, 3, 4, 5, 6, 7, 8]. The training process for these DL models requires a substantial
quantity of computational resources (often in a distributed fashion) for training, which traditional CPUs are unable
to fulfill. Hence, special hardware, with massive parallel computing capabilities such as GPUs, is often utilized [9].
At the same time, the DL model building process is increasingly outsourced to the cloud. This is natural, as applying
cloud services (e.g., Amazon EC2, Microsoft Azure or Google Cloud) for DL training can be more fiscally palatable
for companies by enabling them to focus on the software aspect of their products. Nevertheless, such outsourcing
raises numerous concerns with respect to the privacy and integrity of the learned models. In recognition of the privacy
and integrity concerns around DL (and Machine Learning (ML) in general), a considerable amount of research has
been dedicated to applied cryptography, in three general areas: 1) Multi-Party Computation (MPC) (e.g., [10]), 2)
Homomorphic Encryption (HE) (e.g., [11]), and 3) Trusted Execution Environment (TEE) (e.g., [12, 13]). However,
the majority of these investigations are limited in that: 1) they are only applicable to simple shallow network models,
2) they are evaluated with datasets that have a small number of records (such as MNIST [14] and CIFAR10 [15]),
and 3) they incur a substantial amount of overhead that is unacceptable for real-life DL training workloads. In their
effort to mitigate some of these problems, and securely move from CPUs to GPUs, Slalom [16] mainly focus on the
computational integrity at the test phase while depending on the application context. It can also support enhanced data
privacy, however, at a much greater performance cost.
To address these limitations, we introduce GOAT (See Figure 1); a framework for integrity-preserving learning as a
service that provides integrity guarantees in outsourced DL model training in TEEs. We assume that only the TEE
running in the cloud is trusted, and all the other resources such as GPUs can be controlled by an attacker to launch
an attack (e.g., insert a trojan). In this context, our goal is to support the realistic deep learning training workloads

A PREPRINT

Clip

SGD(t)

SGD(0)

W0

Wt+1

Wt

W2

W1

Update

Batch
Randomness

∇t
Forward

Backward

Forward

Backward

Forward

Forward

Backward

∇t

Backward

Figure 1: The main architecture of GOAT. The TEE handles mini-batch selection, layer specific randomness, and
parameter initialization. The GPU performs forward and backward passes over the mini-batch and reports the computed
gradients to TEE for gradient clipping, weight updates, and preserving intermediate reports in case this step is selected
for verification inside the TEE.

while ensuring data and model integrity. To achieve this goal, we focus on the settings where maintaining the learning
process’s integrity is critical, while the training data may not contain privacy sensitive information. For example, we
may want to build a traffic sign detection model on public traffic sign images and may still like to prevent attacks that
can insert trojan during the training phase. Furthermore, we want to provide assurances that the model is trained on
the specified dataset, with known parameters so that the performance of the model can be replicated and audited for
accountability and integrity.
The trivial approach of executing the entire learning process inside a TEE is not scalable since TEEs are much
slower compared to GPUs. Furthermore, even the existing performance improvement techniques (e.g., random matrix
verification provided in [16]) are not enough to scale up to large DL model learning settings.
To alleviate the TEE bottleneck, we propose incorporating random verification of the computation steps. This strategy
is based on the observation that it is unnecessary to verify all of the GPU’s computation steps. Rather, we only need
to verify occasionally to catch any deviation with a very high likelihood. Given that random verification may itself
be insufficient (theoretically, an attacker can launch a successful attack by modifying only a single unconstrained
gradient update), we further show how parts of the DL hyperparameter setting process, such as clipping rate should be
modified to prevent single step attacks, and require a larger number of malicious updates by an attacker that controls
the GPU. Simply, GOAT limits the amount of change an adversary can inflict on a model through a single SGD
step. As a consequence, the adversary is forced to keep attacking while randomly being verified by the TEE. Using
the state-of-the-art backdoor attacks, we illustrate that random verification technique can detect attacks with a high
probability (e.g., 0.99) while enabling 2x-20x performance gains compared to pure TEE based solutions.
The specific contributions of this paper are as follows:
• We introduce the first approach to support integrity-preserving DL training by random verification of stochastic
gradient (SGD) steps inside TEE to ensure the integrity of training pipeline data, parameters, computation
function, etc. with a high probability.
• We illustrate how gradient clipping can be used as a defensive measure against single (or infrequent) step
attack in combination with random verification.
• We show the effectiveness of our TEE random verification and gradient clipping through extensive experimentation on DNN backdoor attacks.

2

Background

Our system combines deep learning training on specialized fast hardware such as Graphical Processing Units (GPU)
with Intel Software Guard Extensions (SGX) based TEE to ensure the produced model’s integrity. Details on SGD
training and gradient clipping are provided in Appendices B and C.
2

A PREPRINT

2.1

Attacks on DNN Models in Training Phase

Attacks on DNN models can be realized during both training or test phases. However, GOAT is concerned with
integrity/accountability issues during the training phase of DNN models, such that attacks related to testing are out of
the scope of this paper since test time attacks ( [17, 18]) have been addressed before (e.g., Slalom [16]). In the literature,
particularly in the computer vision domain, targeted trojan attacks on DNN classification models have become a real
concern as deep learning has grown in its adoption. These attacks tend to alter the prediction of models if a specific
condition in the input is met. These conditions may be feature-based [19, 20, 21] or instance-based [22, 23]. Recently,
trojan attacks have been extended to Reinforcement Learning (RL) and text classification models [24, 25].
In practice, these attacks are implemented by manipulating samples during training through data poisoning. For
instance, stamping images with a pattern and modifying its label. Interestingly, these models provide similar competitive
classification test accuracy compared to clean models (i.e., models have not been attacked). As a consequence, it is
non-trivial to distinguish these trojaned models from non-trojaned ones based on model accuracy alone. To make
matters worse, even if the model owner was aware of examples of the trojan trigger pattern, the owner would need to
patch the model through re-training to dampen the efficacy of the trojan trigger pattern. Retraining does not always
guarantee complete removal of the trojan behavior from the model. To date, various techniques have been proposed
to diagnose and mitigate of trojaned models. However, all approaches are either based on unrealistic assumptions or
are excessively costly. For instance, the Neural Cleanse [26] requires access to a sizable sample of clean inputs to
reverse-engineer the backdoor and has shown to be successful only for trigger patterns with a relatively small size.
ABS [27] improves upon Neural Cleanse in that requires a significantly smaller number of samples; however, it assumes
that the responsible trojan neurons can activate trojan behavior independently from each other, which is unlikely to be
true in practice.
Attacking the training pipeline to inject a trojan(s) in the final model is the cheapest and, thus, is likely the most
desirable form of attack for real-world adversaries to launch. As such, throughout this work, we mainly focus on
showing our methods’ effectiveness in preventing this type of attack from happening. It should be noted that our method
is orthogonal to attack type and is sufficiently generic to catch any continuous attack during the training of a DNN
model. GOAT relies upon proactive training as opposed to post-training or deployment-time methods to assess the
health of a DNN model. As we explain later in section 3, we assume that the initial training dataset is provided by an
honest user and is free of manipulation. With this as a basis, GOAT limits the amount of change an adversary can inflict
on a model through a single SGD step. As a consequence, the adversary is forced to keep attacking while randomly
being verified by the TEE.
2.2

Integrity for DNN Training

GOAT’s main goal is to enable high-integrity training pipeline so that end users are assured that the model is built on
the specified dataset, using specified parameters without modification. Thus, the final model users know who built the
model, what dataset was used for training, and what algorithms were put in place for building the model. If, at any point
during training, GOAT detects a deviation from the specified execution, it will not sign the final model to ascertain its
validity. [16] took a first step towards achieving both fast and reliable execution in the test phase but neglected the
training phase. The training phase is far more computationally demanding than the test phase, such that verification
of all steps in training requires a substantially longer time. Since parameters keep changing, we cannot benefit from
pre-computation. Second, backward pass involves computing gradients for both the inputs and the parameters and takes
longer than forward pass. Despite the mentioned hurdles, as our investigation shows, it may not be necessary to verify
every step to achieve integrity guarantees with high probability.
2.3

Intel SGX

SGX [28] is an example of a common TEE that is available in many modern-day computers. As outlined in Table 2, it
provides a secluded hardware reserved area, namely, processor reserved memory (PRM), that is kept private (i.e., it is
not readable in plaintext) from the host, or any privileged processes, and is free from direct undetected tampering. It
also supports remote attestation, such that users can attest the platform and the running code within enclave before
provisioning their secrets to a remote server. Calls from routines that should transition to/from enclave are handled
through predefined entry points that are called Ecall/Ocall that must be defined in advance, before building the enclave
image. While it provides security and privacy for numerous applications (e.g., [29, 30, 31]), due to its limited memory
and computational capacity, directly running unmodified applications inside SGX can induce a significant hit on
performance. This is especially the case for applications that require large amounts of memory, such as training DNNs.
3

A PREPRINT

3

Threat Model

Attacks on the integrity of DNNs can be orchestrated at different stages of the model learning pipeline (e.g., data
collection or training). We assume the TEE node in GOAT is trusted, and the bytes stored on the PRM are always
encrypted and authenticated before they are fetched inside the CPU. We assume that the data sent to GOAT comes
from honest users via a secure/authenticated channel and is devoid of malicious samples.1 For the training phase,
we assume that the adversary has complete knowledge about the network structure, learning algorithm, and inputs
(after TEE performs an initial pre-processing) to the model. In our threat model, the adversary is in complete control
of the host system’s software stack, and hardware (unprotected RAM, GPU), except for the CPU package and its
internals. Therefore, the code that runs inside the enclave is free from tampering, and the data that is accessed inside
the cache-lines or registers are not accessible to the adversary. For the inputs supplied to DNN tasks, the adversary is
capable of performing insertion, modification, and deletion to influence the final model towards her advantage. As a
result, an attacker may report wrong gradients as opposed to correctly computed ones.

4

System Design

GOAT offers integrity and accountability for the training phase of a DNN model while inducing limited computational
overhead. An overview of GOAT is illustrated in figure 1, and we refer the reader to table 2 for symbol descriptions and
abbreviations.
Before the training phase initiates, the training dataset is decrypted and validated inside the TEE. Besides, for each SGD
step, the randomness regarding the mini-batch selection and the network layers [32] are derived within the TEE and
supplied to the GPU. As a result, the adversary will face a more constrained environment. Moreover, in our design, the
GPU always performs a forward and a backward pass and reports the computed gradients to the TEE. At this point, the
TEE clips the gradients and updates the parameters (low overhead operation). Finally, GOAT randomly decides to verify
the SGD steps within the TEE or not. GOAT is optimized to guarantee a high level of integrity and the correctness
of the model while providing an infrastructure that does not suffer from the substantial computational requirements
of pure TEE-based solutions. We assume an honest and authenticated user will send her data encryption key Kclient
(after remote-attestation) to the TEE. Next, the TEE decrypts/verifies the initial encrypted dataset using the Kclient and
supplies the trainer (GPU) the plain-text of the training set. If the TEE fails to detect any violations of the protocol
during training, it will sign a message that certifies the final model.(Please see Appendix A for more details of the
signed message). Then, during testing and deployment, the user can verify the digital signature of the model.
Training with GOAT At the beginning of mini-batch iteration i, TEE supplies the untrusted GPU with the randomness
for that iteration. After completion of the forward and backward passes over the mini-batch, the computed gradients
are sent to the TEE for clipping and updating the parameters. Next, the TEE integrates the clipped gradients with the
parameters of the previous step. GOAT always clips the reported gradients and ensures that they are within a narrow
range so that evolving the model towards the attacker’s intended model requires a prolonged malicious intervention by
the attacker. GOAT accepts the reported gradients and only applies the clipped version to the snapshot taken at the
specific iteration. If the computation at that step is selected for random verification, then the faulty behavior can be
detected. If not, the chance that the model evolved towards the attacker’s desired optima will likely require multiple
rounds providing ample opportunity for detection. The verification is done randomly to prevent an attacker from
guessing which step is verified.
Probabilistic Verification with GOAT The TEE randomly decides whether or not to verify the computation over each
mini-batch. If the mini-batch is selected for verification, then the intermediate results are saved, and the verification
task is pushed into a verification queue. Verification by the TEE can take place asynchronously and it does not halt
the computation for future iterations on the GPU. The authenticity of snapshots is always verified with a key that is
session
derived from a combination of the TEE’s session key, SKSGX
and the corresponding iteration. When the TEE
verifies step i, it populates the network parameters with the snapshot it created for the step i − 1. It then regenerates
the randomness from step i to obtain the batch indices and correctly sets up the per layer randomness. Given that the
TEE’s goal is to verify that the reported gradients for step i are correctly computed, GOAT does not keep track of the
activation results. Rather it only requires the computed gradients, batch mean/std (for BatchNorm layer), and matrix
multiplication outcomes (in case random matrix multiplication verification is chosen). These required parameters are
saved for verification.
Randomized Matrix Multiplication Verification with GOAT Matrix Multiplications (MM) take up the bulk of the
resource-heavy computations in DNNs. In modern DNN frameworks, convolutional and connected layers computation
is implemented in the form of a matrix multiplication in both of the forward and backward passes. Table 4 in Appendix G
1

Detecting malicious samples is beyond the scope of this work.

4

A PREPRINT

VGG16
Forward

ResNet152

Backward

4.41

SGX

1.79

SGXRMM

3.01

SGX

2.24
1.43

3.93

VGG19

1.15
0.93
0.51

SGXRMM

SGX

1.05
0.73
0.43

SGXRMM

1.63
1.05
0.7

SGX

1.1
0.47
0.33

1.6
1.04
0.63

4
3.5
3
2.5
2
1.5
1
0.5

0.93
0.42
0.29

Throughput (Images/Sec)

Throughput Performance (ImageNet)

SGXRMM

ResNet34

Overall

Figure 2: Throughput of the SGD training step for VGG19,VGG16, ResNet152, and Resnet34 on ImageNet dataset
with repect to forward and backward passes. RMM can lead to verification that is twice as fast as full MM verification
in case of a VGG architecture.

Table 1: Trained models with restricted clipping and learning rate
Dataset
GTSRB
MNIST
CIFAR10

clip
10−4
10−4
5 × 10−4

lr
10−4
5 × 10−5
10−1

%clean
≥ %95
≥ %95
≥ %90

%attack
≥ %85
≥ %85
≥ %85

total
124
49
94

depicts the computations in the forward pass and backward gradient with respect to the weights and previous layers’
outputs in the form of a rank 2 tensor multiplication. Fortunately, there exists an efficient verification algorithm for
matrix multiplication( [33]) when the elements of matrices belong to a field. In this work, we leverage these random
matrix multiplication verification algorithms as well.

5

Integrity Analysis

To achieve the integrity goal, we need to derive the probability pv (i.e., the verification probability of each step) to
achieve our integrity goal pi (i.e., the probability that attacker can modify the result without being detected is less than
1 − pi ).
Definition 1. Assuming the DNN training requires a total of B steps, for each step report (Rb ∀b ∈ [1, B] ∧ Rb ∈
{0, 1}) has a probability pc for being corrupted ( i.e., Rb = 1), and the overall integrity requirement probability goal pi
(for example pi = 0.999).
Theorem 1. Given a total of B steps during SGD training, the required probability of choosing a step to verify (pv )
log(1−pi )
− 1).
should be greater than B −1 ( log(1−p
c)
Remark. Given each step contains m independent matrix multiplication (MM) operations that is to be repeated k times
(independently), and each random entry is chosen from a field of size |S|, the probability of error (accepting a wrong
MM equality) is less than α = |S|1mk [33].
Theorem 2. If random matrix multiplication verification is used, given the configuration of Theorem 1, the required
log(1−pi )
probability of choosing a step to verify (pv ) should be greater than B −1 ( log((1+(α−1)p
− 1).
c)
We refer the reader to Appendix D.1 and D.2 for complete proofs of the theorems. The threshold probability in
Theorem 2 yields approximately the same values as Theorem 1 when α → 0. However, the randomized matrix
multiplication verification requires a O(N 2 ) operations (assuming two N × N matrices) compared to regular matrix
multiplication that requires O(N 3 ) operations.

6

Experimental Evaluation

We executed our experiments on a server with Linux OS, Intel Xeon CPU E3-1275 v6@3.80GHz, 64GB of RAM
and an NVIDIA Quadro P5000 GPU with 16GB of memory. Our attack code is implemented in python 3.6 using
5

0.02
0.00
1 2 3 4 5 6

0.06
0.04
0.02
0.00
1 2 3 4 5 6

backdoor trigger

0.02
0.00
0.9

0.06
0.04
0.02
0.00
0.2

1 2 3 4 5 6

0.02
0.00
0.9

Verfication Rate (0.001 Failure)

0.04

Poisoning Rate

0.9

0.20
0.10
0.00
0.2

0.06
0.04
0.02
0.00
0.5
Poisoning Rate

(h) GTSRB

0.9

(f) CIFAR10

GTSRB based on poisoning rate

0.1

0.5
Attack Start Epoch

(e) GTSRB

MNIST based on poisoning rate

0.5

0.5

CIFAR10 based on start epoch of attack

Attack Start Epoch

(d) MNIST
Verfication Rate (0.001 Failure)

0.00

(c) CIFAR10

GTSRB based on start epoch of attack

Attack Start Epoch

(g) MNIST

0.10

backdoor trigger

Verfication Rate (0.001 Failure)

0.04

0.1

0.20

(b) GTSRB
Verfication Rate (0.001 Failure)

Verfication Rate (0.001 Failure)

(a) MNIST

0.5

CIFAR10 based on backdoor trigger

backdoor trigger

MNIST based on start epoch of attack

0.2

Verfication Rate (0.001 Failure)

0.04

GTSRB based on backdoor trigger

0.9

Verfication Rate (0.001 Failure)

Verfication Rate (0.001 Failure)

MNIST based on backdoor trigger

Verfication Rate (0.001 Failure)

A PREPRINT

CIFAR10 based on poisoning rate

0.20
0.10
0.00
0.1

0.5

0.9

Poisoning Rate

(i) CIFAR10

Figure 3: A boxplot representation of the verification rates required by TEE for detection failure ≤ 10−3 . Based on the
attack hyper-parameters: 1) backdoor trigger type (first row). 2) epoch that attack starts (second row, 20%×,50%×,
and 90%×|epochs|). 3) backdoor poisoning ratio

the pytorch [34] library. We use Intel SGX as our platform for TEE. For SGX proof-of-concept implementation,
DarkNet [35] library is significantly modified to run the experiments. Our SGX code has been tested with SDK 2.9 and
the code runs inside a docker container in hardware mode. Our experiments are designed to investigate two aspects.
First, we evaluate the impact of integrating randomized matrix verification and show that it can potentially increase
computational efficiency. Second, we analyze the effectiveness of gradient clipping in forcing the attacker to deviate
from the honest protocol with significant numbers of mini-batch steps. Various attack hyper-parameters (e.g. poisoning
rate) were evaluated in determining their importance towards a successful attack with minimal deviation. This is
important because, if the attacker needs to deviate over more mini-batch steps, then the TEE can detect such deviations
with a smaller number of random verification steps (i.e., pc is higher in equation 2 ).
TEE Performance We tested TEE performance on VGG16, VGG19 ([36]), ResNet152, and ResNet34 ([37])
architectures with the ImageNet ([38]) dataset (see Appendix F.2 for results with CIFAR10 dataset( [15])). Figure 2
illustrates the throughput of deep networks. Usually, most of the computation takes place in the convolution layers
of the network. However, the backward pass on average yields a smaller throughput since it involves one more
MM (weight gradients, and input gradients) than the forward pass which only invokes MM once (i.e., output of
convolution or fully-connected layers). The implementation is quite efficient in terms of MM operations which uses
both vectorized instructions along with multi-threading. Both VGG networks gives better improvements (≈ 2.2X)
than ResNet (≈ 1.2X). Considering the TEE randomly decides to verify a SGD step with probability pv , the overall
improvement is approximately multiplied by p1v X. For instance, if we assume the attacker only requires to deviate with
probability 5.10−5 (i.e., deviating in five steps out of a hundred thousand of steps), we can detect it with pv ≈ 0.1 (See
6

A PREPRINT

Figure 4d in Appendix) while maintaining integrity violation detection probability (pi > 0.999). For VGG networks it
means approximately 22X improvements compared to a pure TEE-based solution that verifies every step. Nonetheless,
as our experiments suggest, we believe that attacking these deep models that are trained on massive datasets should
require a significantly higher number of deviations. This in return may result in a much higher performance gain.
Combined Impact of Gradient Clipping and Learning Rate on Attack Success As shown in Table 1, we selected
models that achieved high performance on both clean and poisoned test samples. We conducted the attack in the
following manner. 2 First, the trainer follows the correct protocol (e.g. learning with mini-batch SGD) until epochattack .
At this epoch, the attacker starts by injecting a certain number of poisoned samples (poisrate × batch_size) from every
class into the training batch and labels it as the target label. The attacker continues to attack until they achieve a desired
threshold in terms of success rate (correctly classifying backdoored samples as the attacker’s target label). Once it
passes the threshold, the attacker halts the attack and returns to the honest protocol, while observing the decay in attack
success rate. If the success rate falls below the desired low-threshold, the attacker transitions back to attack mode and
repeats the aforementioned strategy. CIFAR10 ([15]), GTSRB ([39]) and MNIST ([14]), were used to analyze the
impact of multiple factors imposed by the attacker that also aims to maintain an acceptable accuracy over clean test set.
For MNIST, and GTSRB, the Adam [40] optimizer (which requires a smaller initial learning rate), and for the CIFAR10
dataset, SGD with momentum are used. Additionally, for MNIST, and GTSRB 10% of the training set was chosen
for the validation set to help adjust the learning rate based on the validation set loss. Save for CIFAR10 dataset, the
learning rate was set to decay (by tenfold) at fixed epochs (40, 70, 100).
Backdoor Trigger Pattern We applied 6 different backdoor triggers ( all of them are shown in Appendix, Figure 8).
The MNIST dataset only has single channel images, we converted it to a three channel image to apply the triggers 3
to 6. As shown in Figures 3 (a–c) the trigger pattern can significantly influence the effectiveness of the attack. The
red lines show the median, while the green dots correspond to the mean. In all of the datasets, the first trigger pattern
(Figure 8a) was the most effective one. It covers a wider range of pixels compared to the second trigger type (Figure 8b).
As a consequence, it is more likely for the model to remember the trigger pattern across longer periods of SGD steps.
Moreover, because photo filters (e.g. Instagram) are popular these days, we investigate the possibility of conducting
attacks using some of the filters(or transformations) as the trigger pattern. However, covering a very wide range of
pixels does not lead to a stealthy attack, as illustrated for the last four patterns. These patterns cover the whole input
space and transform it to a new one that they share a lot of spacial similarities while only different in tone or scale (e.g.
Figure 8d). Learning to distinguish inputs that are similar, but only different in their tone is demanding in terms of
continuity of the attack. In this case, both of the images (w. and w.o. the trigger) are influencing most of the parameters
and filters in a contradictory manner (different classification label). Thus it takes a significant number of steps for the
network to learn to distinguish them when gradient clipping is applied.
Attack Start Epoch Another major factor influencing the evasiveness of the attack is when attacker starts the attack.
For instance, early in the training phase the learning rate will be high, such that a savvy attacker might believe they
can avoid low clipping values by initiating their attack. However, if the attack begins too early, then it is unlikely that
the model has yet converged. Therefore, beginning the attack too early may require an unnecessarily high number of
(unnecessarily) poisoned batches, which, in turn, would raise the probability of detection. Yet even if the attacker was
successful, once they halt the attack, the model will likely evolve the parameters back to a clean state relatively quickly,
and may require the attacker to re-initiate their attack. At the same time, given that the system uses a low clipping value,
if the attacker waits toward the end of training, the attack is again unlikely to be effective. It would be unlikely that the
attack succeeds before the end of the training; particularly due to having a considerably smaller learning rate. As shown
in Figures 3 (d–f), the best time for the attack is when the model has a relatively low loss on clean training inputs, and
the combination of learning rate and clipping value (effective attainable update) could yield the model to move toward
attacker’s desired optima. However, for MNIST, which is an easier learning task, attacking early gives the attacker a
better chance to launch a stealthier attack. We speculate that this is due to the fast convergence of the model. After a
few epochs, it quickly reaches a stable low training loss value for clean images. As a result, when the attacker concludes
the attack (after reaching to the desired threshold), it is generally preserved far longer than the other two datasets.
Mini-Batch Poisoning Ratio As we stated earlier, the poisrate parameter is the ratio of the number of poisoned
samples in the batch to the batch size. It is one of the critical factors amongst those we investigated. Especially when
gradient clipping is used, setting poisrate appropriately can help the attacker by moving more parameters toward the
desired optima. However going beyond the ratio 0.9 (i.e., poisrate > 0.9 can impact the training negatively for both
clean inputs and poisoned inputs. As depicted in Figures 3 (g–i), our experiments suggest that filling more than half
2

We also experimented with the scenario where an attacker attack at steps chosen randomly. This type of attack was not successful.
Hence we do not report those results here.

7

A PREPRINT

the batch with poisoned samples seems to be effective across all datasets. Although for the MNIST dataset, it shows
that higher values can slightly perform better, but this is not confirmed by experiments on more complex datasets. For
example, for the GTSRB dataset, we could not find a successful attack model with acceptable clean input accuracy.

7

Related Works

TEEs [28, 41] are becoming more popular to provide privacy and integrity guarantee for applications across domains. [30,
29, 42, 43, 44, 45, 46, 47, 48, 49]. For deep learning inference, Slalom [16] introduced a notion of mixed computation
between SGX and GPU. It operates in two modes, one is pure-integrity and the other is privacy-preserving mode for
inputs to the model (no privacy for the model parameters). It showed the great potential for outsourcing computation to a
special hardware and verifying the computation inside enclave using Freivalds’ scheme [33]. In contrast, GOAT aims at
solving the integrity problem for the training phase. Authors in [50] proposed Origami; a privacy-preserving inference
framework which partially computes some number of early layers in both enclave and GPU (similar to Slalom), while
outsourcing the rest of the layers (input reconstruction as they claimed becomes infeasible) in plaintext solely onto the
GPU.
For deep learning training using TEEs, Chiron [12] was the first framework that proposed a distributed privacypreserving solution using SGX. Yet in reality, private training within enclave and no other special hardware can take
months for large architectures with potentially millions of records in the dataset. Especially, due to architectural
limitations of SGX, special care must be taken to avoid allocating large enclaves (stay within PRM), and use partitioning
scheme to enhance performance. TensorSCONE [31] is a porting of TensorFlow for SGX that supports both training
and inference inside the enclave. It can support tensorflow graph definitions for models and it is integrable with current
DNN ecosystem for applications that privacy is crucial. Myelin [13] is a training framework on multi-source data
where the parties do not trust each other. Myelin uses SGX to train the data in plaintext within enclave while applying
differential privacy [51] measures to encounter potential privacy loss of the private inputs to other parties.

8

Conclusion

This paper introduced the GOAT system, which provides integrity in outsourced DNN training using TEEs. As
our experimental investigation illustrates, GOAT scales up to realistic workloads by randomizing both mini-batch
verification and matrix multiplication to achieve integrity guarantees with a high probability. We have further shown
that random verification in combination with hyperparameter adjustment (e.g., setting low clipping rates), can achieve
2X-20X performance improvements in comparison to pure TEE-based solutions while catching potential integrity
violations with a very a high probability.
Acknowledgments
This work was supported, in part by grant 2R01HG006844 from the National Human Genome Research Institute, NSF
Awards CNS-1837627, OAC-1828467, IIS-1939728 and ARO award W911NF-17-1-0356. Finally, the authors would
like to thank Dr. Yan Zhou for constructive criticism of the manuscript.

References
[1] Rov Csongor. Tesla raises the bar for self-driving carmakers.
[2] Fei Wang, Lawrence Peter Casalino, and Dhruv Khullar. Deep Learning in Medicine—Promise, Progress, and
Challenges. JAMA Internal Medicine, 179(3):293–294, 03 2019.
[3] M. E. Morocho Cayamcela and W. Lim. Artificial intelligence in 5g technology: A survey. In 2018 International
Conference on Information and Communication Technology Convergence (ICTC), pages 860–865, 2018.
[4] Jian-hua Li. Cyber security meets artificial intelligence: A survey. Frontiers of Information Technology &
Electronic Engineering, 19(12):1462–1474, 2018.
[5] N. C. Abay, C. G. Akcora, Y. R. Gel, M. Kantarcioglu, U. D. Islambekov, Y. Tian, and B. Thuraisingham.
Chainnet: Learning on blockchain graphs with topological features. In 2019 IEEE International Conference on
Data Mining (ICDM), pages 946–951, 2019.
[6] Sara Rouhani, Tahrima Rahman, and Vibhav Gogate. Algorithms for the nearest assignment problem.
[7] Ahmet Murat Ozbayoglu, Mehmet Ugur Gudelek, and Omer Berat Sezer. Deep learning for financial applications:
A survey. Applied Soft Computing, page 106384, 2020.
8

A PREPRINT

[8] Xiaofei Wang, Yiwen Han, Victor CM Leung, Dusit Niyato, Xueqiang Yan, and Xu Chen. Convergence of
edge computing and deep learning: A comprehensive survey. IEEE Communications Surveys & Tutorials,
22(2):869–904, 2020.
[9] Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. Benchmarking state-of-the-art deep learning software
tools. CoRR, abs/1608.07249, 2016.
[10] Payman Mohassel and Yupeng Zhang. Secureml: A system for scalable privacy-preserving machine learning.
IACR Cryptology ePrint Archive, 2017:396, 2017.
[11] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John Wernsing. Cryptonets:
Applying neural networks to encrypted data with high throughput and accuracy. In International Conference on
Machine Learning, pages 201–210, 2016.
[12] Tyler Hunt, Congzheng Song, Reza Shokri, Vitaly Shmatikov, and Emmett Witchel. Chiron: Privacy-preserving
machine learning as a service. CoRR, abs/1803.05961, 2018.
[13] Nick Hynes, Raymond Cheng, and Dawn Song. Efficient deep learning on multi-source private data. CoRR,
abs/1807.06689, 2018.
[14] Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
[15] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).
[16] Florian Tramer and Dan Boneh. Slalom: Fast, verifiable and private execution of neural networks in trusted
hardware. In International Conference on Learning Representations, 2019.
[17] Yan Zhou, Murat Kantarcioglu, and Bowei Xi. A survey of game theoretic approach for adversarial machine
learning. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 9(3):e1259, 2019.
[18] Xianmin Wang, Jing Li, Xiaohui Kuang, Yu-an Tan, and Jin Li. The security of machine learning in an adversarial
setting: A survey. Journal of Parallel and Distributed Computing, 130:12–23, 2019.
[19] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine
learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.
[20] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning
attack on neural networks. 2017.
[21] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.
[22] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning
systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.
[23] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom
Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing
Systems 31, pages 6103–6113. Curran Associates, Inc., 2018.
[24] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl: Trojan attacks on deep reinforcement
learning agents. arXiv preprint arXiv:1903.06638, 2019.
[25] Lichao Sun. Natural backdoor attack on text data, 2020.
[26] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao. Neural cleanse: Identifying and
mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages
707–723, 2019.
[27] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs: Scanning
neural networks for back-doors by artificial brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference
on Computer and Communications Security, CCS ’19, page 1265–1282, New York, NY, USA, 2019. Association
for Computing Machinery.
[28] Victor Costan and Srinivas Devadas. Intel sgx explained. IACR Cryptology ePrint Archive, 2016(086):1–118,
2016.
[29] Christian Priebe, Kapil Vaswani, and Manuel Costa. Enclavedb: A secure database using sgx. In 2018 IEEE
Symposium on Security and Privacy (SP), pages 264–278. IEEE, 2018.
[30] Fahad Shaon, Murat Kantarcioglu, Zhiqiang Lin, and Latifur Khan. Sgx-bigmatrix: A practical encrypted data
analytic framework with trusted processors. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pages 1211–1228, 2017.
9

A PREPRINT

[31] Roland Kunkel, Do Le Quoc, Franz Gregor, Sergei Arnautov, Pramod Bhatotia, and Christof Fetzer. Tensorscone:
A secure tensorflow framework using intel sgx. arXiv preprint arXiv:1902.04413, 2019.
[32] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958,
2014.
[33] Rusins Freivalds. Probabilistic machines can use less running time. In IFIP congress, volume 839, page 842,
1977.
[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito,
Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019.
[35] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/, 2013–2016.
[36] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
[37] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
[38] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image
Database. In CVPR09, 2009.
[39] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine learning algorithms
for traffic sign recognition. Neural Networks, (0):–, 2012.
[40] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[41] Architecure ARM. Security technology building a secure system using trustzone technology (white paper). ARM
Limited, 2009.
[42] Stefan Brenner, Colin Wulf, David Goltzsche, Nico Weichbrodt, Matthias Lorenz, Christof Fetzer, Peter Pietzuch, and Rüdiger Kapitza. Securekeeper: confidential zookeeper using intel sgx. In Proceedings of the 17th
International Middleware Conference, pages 1–13, 2016.
[43] Ben Fisch, Dhinakaran Vinayagamurthy, Dan Boneh, and Sergey Gorbunov. Iron: functional encryption using
intel sgx. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,
pages 765–782, 2017.
[44] Stefan Brenner, Tobias Hundt, Giovanni Mazzeo, and Rüdiger Kapitza. Secure cloud micro services using intel
sgx. In IFIP International Conference on Distributed Applications and Interoperable Systems, pages 177–191.
Springer, 2017.
[45] Raymond Cheng, Fan Zhang, Jernej Kos, Warren He, Nicholas Hynes, Noah Johnson, Ari Juels, Andrew Miller,
and Dawn Song. Ekiden: A platform for confidentiality-preserving, trustworthy, and performant smart contract
execution. arXiv preprint arXiv:1804.05141, 2018.
[46] Huibo Wang, Pei Wang, Yu Ding, Mingshen Sun, Yiming Jing, Ran Duan, Long Li, Yulong Zhang, Tao Wei,
and Zhiqiang Lin. Towards memory safe enclave programming with rust-sgx. In Proceedings of the 2019 ACM
SIGSAC Conference on Computer and Communications Security, pages 2333–2350, 2019.
[47] Huibo Wang, Erick Bauman, Vishal Karande, Zhiqiang Lin, Yueqiang Cheng, and Yinqian Zhang. Running
language interpreters inside sgx: A lightweight,legacy-compatible script code hardening approach. Asia CCS ’19,
page 114–121, New York, NY, USA, 2019. Association for Computing Machinery.
[48] Erick Bauman, Huibo Wang, Mingwei Zhang, and Zhiqiang Lin. Sgxelide: Enabling enclave code secrecy via
self-modification. In Proceedings of the 2018 International Symposium on Code Generation and Optimization,
CGO 2018, page 75–86, New York, NY, USA, 2018. Association for Computing Machinery.
[49] Md Shihabul Islam, Mustafa Safa Ozdayi, Latifur Khan, and Murat Kantarcioglu. Secure iot data analytics in
cloud via intel sgx, 2020.
[50] Krishna Giri Narra, Zhifeng Lin, Yongqin Wang, Keshav Balasubramaniam, and Murali Annavaram. Privacypreserving inference in machine learning services using trusted execution environments. arXiv preprint
arXiv:1912.03485, 2019.
10

A PREPRINT

[51] Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In 2010 IEEE 51st Annual
Symposium on Foundations of Computer Science, pages 51–60. IEEE, 2010.
[52] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097–1105, 2012.
[53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 1–9, 2015.
[54] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-resnet and
the impact of residual connections on learning. In Thirty-first AAAI conference on artificial intelligence, 2017.
[55] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics,
pages 400–407, 1951.
[56] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 6.5 back-propagation and other differentiation algorithms.
Deep Learning, pages 200–220, 2016.
[57] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 10.11 optimization for long-term dependencies. Deep
Learning, pages 408–411, 2016.
[58] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A
theoretical justification for adaptivity, 2019.
[59] Xiangyi Chen, Zhiwei Steven Wu, and Mingyi Hong. Understanding gradient clipping in private sgd: A geometric
perspective, 2020.

A

Model Signing By TEE

We assume, an honest and authenticated user will send her data encryption key Kclient (after remote-attestation) to the
TEE. Next, the TEE decrypts and verifies the initial encrypted dataset using the Kclient and supplies the trainer (GPU)
the plain-text of the training set. If the TEE fails to detect any violations of the protocol during training, it will sign the
following message that certifies the final model where W is the model parameters, ds is training dataset, v_num is the
model version number, SHA256 is Sha 256 bit cryptographic hash function,
v
SHA256(SHA256(W)||v_num||SHA256(ds)||Sigclient
)
s
with signature key SigSGX
of the enclave

Table 2: Symbols and Acronyms Description
Category

TEE

Neural Network
General

B

Symbol
session
KSGX
s
SigSGX
Kclient
v
Sigclient
PRM
EPC
RMM
FV
DNN
W
ds
v_num

Description
TEE’s session key for learning task
SGX signature signing key
client’s encryption key
clients public key
Processor Reserved Memory
Enclave Page Cache
Randomized Matrix Multiplication
Full Verification (No RMM)
Deep Neural Network
model parameters
training dataset
model version number

Deep Learning Training

In the recent decade, Deep Neural Networks (DNN) gained an enormous attraction in solving problems related to
computer vision and natural language processing [52, 36, 37, 53, 54]. In practice, these networks are stacks of layers that
l
l
each perform a transformation FW
(·) ∀l ∈ |L| where X l+1 = FW
(X l ) and |L| is the number of layers. The training
∗
task is to learn the correct parameters (point-estimates) W that optimizes (commonly minimizes) a task-specific (e.g.
classification) loss function L.
The most common way of training the DNN’s parameters is through mini-batch Stochastic Gradient Descent (SGD) [55].
A randomly selected mini-batch of dataset is fed to the DNN and the value of objective function L is calculated. This
11

A PREPRINT

is usually called the forward pass. Next, to derive the partial gradients of L w.r.t W (∇L
W ), a backward pass is
performed [56]. Finally, parameters are then updated according to the equation 1 where 0 < α < 1 is called the learning
rate. Depending on the complexity of the dataset and the task, it might require hundreds of passes (called epoch) over
the input dataset for convergence.
t

W t+1 = W t − α∇L
Wt

C

(1)

Gradient Clipping

Gradient Clipping (GC) is a method that is mainly known to help mitigate the problem of exploding gradients during
training [57]. GC forces the gradients to fall within a narrow interval. There have been some efforts to analyze GC with
respect to convergence. Authors in [58] prove (assuming a fixed step size) that training with GC can be arbitrarily faster
than training without it. Moreover, the theoretical analysis suggested that small clipping values can damage the training
performance. However, in practice that is rarely the case. [59] has an interesting theoretical analysis coupled with
empirical evidence (symmetry of gradients distribution w.r.t SGD trajectory) that answers the gap between previous
theoretical and practical observations.

D
D.1

Integrity Proofs
Random Mini-Batch Verification

PdB×p e
Definition 2. Define random variable X = b=1 v Vb to be the total number of random verification done. Here Vb
is 1 if the batch is chosen for verification but failed the verification (due to malicious computation). Please note that we
need to catch at least one deviation with probability greater than pi , and invalidate the overall model learning.

Proof. Proof of theorem 1
P (X ≥ 1) = 1 − P (X = 0) ≥ pi
1 − pi ≥ P (X = 0)


dB × pv e 0
1 − pi ≥
pc (1 − pc )dB×pv e
0
1 − pi ≥ (1 − pc )dB×pv e
log(1 − pi ) ≥ dB × pv e log(1 − pc )
log(1 − pi )
pv > B −1 (
− 1)
log(1 − pc )

(2)

As shown in Figure. 4 it is only required to verify a much smaller subset of the batch computations inside the TEE to
ensure a high probability of correct computation. For example, for large datasets such as Imagenet [38], it is needed to
verify less than %1 of computation to have 0.9999 guarantee (when corruption probability is %0.5) on the correctness
of computation outsourced to the GPU.
D.2

Random Mini-Batch Verification with Randomized Matrix Multiplication

PdB×p e
Definition 3. Define random variable Vb0 = 1 if Rb = 1 ∧ M M _verif y(b) = 0 otherwise 0 and X = b=1 v Vb0 .
We need to detect at least one deviation with probability greater than pi while conducting random matrix multiplication
verification.

12

A PREPRINT

epochs= 200,dataset size = 60K, pi > 0.99

Verification Probability (pv )

0.45

b = 64
b = 128
b = 256
b = 512

0.4
0.35

epochs= 200,dataset size = 60K, pi > 0.999
0.45

0.35

0.3

0.3

0.25

0.25

0.2

0.2

0.15

0.15

0.1

0.1

5 · 10−2

5 · 10−2

0

b = 64
b = 128
b = 256
b = 512

0.4

0
1

1.5

2

2.5

3

3.5

4

Corruption Probability (pc )

4.5

5

1

1.5

2

(a)

3

3.5

4

4.5

5

·10−4

(b)

epochs= 200,dataset size = 1M , pi > 0.99
0.6

b = 64
b = 128
b = 256
b = 512

0.55

Verification Probability (pv )

2.5

Corruption Probability (pc )

·10−4

0.5
0.45

epochs= 200,dataset size = 1M , pi > 0.999
0.6

0.5
0.45

0.4

0.4

0.35

0.35

0.3

0.3

0.25

0.25

0.2

0.2

0.15

0.15

0.1

0.1

5 · 10−2

5 · 10−2

0

b = 64
b = 128
b = 256
b = 512

0.55

0
0.5

1

1.5

2

2.5

3

3.5

4

Corruption Probability (pc )

4.5

5

0.5

1

·10−5

(c)

1.5

2

2.5

3

3.5

4

Corruption Probability (pc )

4.5

5

·10−5

(d)

Figure 4: Required verification probability with respect to batch corruption probability and the desired integrity
probability for a fixed 200 epochs and different SGD batch size.
Proof. Proof of theorem 2
P (X ≥ 1) ≥
1 − P (X = 0) ≥
1 − pi

≥

pi
pi


dB × pv e
(pc (1 − α))0 ((1 − pc ) + pc α)dB×pv e
0

1 − pi ≥ ((1 − pc ) + pc α)dB×pv e
log(1 − pi ) ≥ dB × pv e log((1 − pc ) + pc α)
log(1 − pi )
pv > B −1 (
− 1)
log((1 + (α − 1)pc )

E

(3)

Verification Probability Growth with Respect to Detection Probability

Fig. 4 shows how verification probability changes with respect to the probability that a batch step is maliciously
manipulated by the attacker. First row shows the verification probability for a dataset with 60K samples. Second row
depicts the required for much bigger dataset (1M samples) over different mini-batch sizes. The smaller the mini-batch
size is, there is a higher chance for detecting malicious behavior.
13

A PREPRINT

200

1.5
1.25

VGG19
SGX

VGG16

ResNet152
SGX RMM

ResNet34

Time (Sec)

Throughput (Images/Sec)

1.75

1.0
0.75

100

0.5
0.25
0.1

0
(100,32)

(150,48)

(180,64)

(200,80)

(220,96)

(100,32)

(SGX Max Heap Size, Blocking Size) MB

(150,48)

(180,64)

(200,80)

(220,96)

(SGX Max Heap Size, Blocking Size) MB

(a) Available heap with respect to throughput

(b) Available heap with respect to time spent on matrixmatrix(vector) multiplication

Figure 5: The impact of increasing TEE heap size on (a) overall throughput and (b) the time spent in matrix multiplication
routine . VGG shows significant reduction in performance as opposed to ResNet.

Arch
VGG11
VGG13
VGG16

F
F.1

Table 3: TEE Architectures Used
FC1
FC2
FC3
(128,10) (128,64,10) (256,128,10)
(128,10) (128,64,10) (256,128,10)
(128,10) (128,64,10) (256,128,10)

Experimental Evaluation
Enclave Heap Size Impact on TEE Performance

As shown in Figure 5, which depicts the impact of the heap size on the performance of DNNs for a single SGD step, it
can be seen that increasing the heap size way beyond the available hardware limit (around 92MB) can have a negative
impact on performance especially in the case of the VGG architecture. This result is mainly due to 1) driver level
paging, which needs to evict enclave pages that require an extra level of encryption/decryption and 2) extra bookkeeping
for the evicted pages.
F.2

TEE Performance on CIFAR10

Figure 6 shows throughput performance for CIFAR10 dataset and 9 different VGG architectures (Table 3). We chose
three VGG(11,13,16) architectures adapted for CIFAR10 image inputs with custom fully connected layers attached
to its end. For CIFAR10, we generally do not benefit from randomized matrix multiplication scheme as well as
ImageNet. Mainly it is because most of the operations and network layers fit well within the hardware memory limit.
Therefore, since dimensions of MM operations are not too big, it does not improve significantly to use randomized
matrix multiplications.
F.3

Impact of Gradient Clipping for Honest Trainers

One important question is whether the gradient clipping used to prevent attacker to change parameters in a given
mini-batch update would have performance impact during training session where there is no attack. Six experiment
configurations were repeated 5 times each with different randomness (initialization, batch order, etc.). Initial learning
rates are set ∈ (0.1, 0.01) and clipping thresholds are set ∈ (nil, 0.001, 0.0005). In total, there are 30 ResNet56
(complex architecture with state-of-the-art performance) models trained on the CIFAR10 dataset (with no attack) for
200 epochs. It is shown that the clipping rate have very little impact on the test set accuracy, given an appropriate initial
learning rate is chosen. Usually, for SGD [55] optimizer with momentum, a value of 0.1 is chosen, (and for Adam
optimizer a value less than 0.001). For these experiments, we used the configuration with unbounded gradient updates
as the main reference point. For learning decay schedule, we used a fixed decay by tenfold at epochs (50, 90, 130,160).
In figure 7a describes the mean and standard deviation (dashed lines) of test accuracy taken for 5 repetitions at each
epoch for the two learning rate configurations. As it can be seen both models start to take giant leaps toward convergence
at the first two learning decays enforced by the scheduler. Please note that these are reference runs that no gradient
14

A PREPRINT

SGXRMM

VGG11-FC1

SGX

SGXRMM

VGG11-FC2

SGX

SGXRMM

VGG11-FC3

SGX

SGXRMM

VGG13-FC1

Forward

SGX

SGXRMM

VGG13-FC2

Backward

SGX

SGXRMM

VGG13-FC3

SGXRMM

VGG16-FC1

34.39

38.1

SGX

SGX

Overall

clipping is applied during the update step. Toward the end of training, the setting with the higher initial learning rate
slightly performs better in terms of accuracy (the y-axis is not in % scale).
In figure 7b, for each composition of learning rate, and clipping value, the highest difference (accuracy rate) with
respect to reference run is plotted. The plot shows test accuracy is not influenced so much by the clipping value, rather,
it is highly dependent on the learning rate value. when lr = 0.1, both clipping values can achieve values that are close
to the reference runs that has no gradient clipping, however, slightly smaller (most epochs it is negative, except jumps
in the start). In figure 7c, the opposite of the previous measure is plotted. Again, by the end of the training, the gaps are
significantly tightened for the case where a better learning is chosen. Therefore, having a smaller clipping value is not
really impacting the performance in any considerable way.
Overall, figure 7, shows that clipping does not really impact the learning task negatively, once a good learning rate is
chosen. One can observe that if the trainer choose an acceptable learning rate for the task, small clipping values such as
0.001 or 0.0005) does not impede the learning task. Once the model passes the first learning rate decay schedule, all
the configuration behave the same in terms of their test performance compared to their reference model (no gradient
clipping limit).
All Backdoor Trigger Examples

Figure 8 shows six different backdoor trigger patterns that are used to conduct the attacks.

G

Matrix Multiplication Ops of Common DNN Layers

Table. 4 shows common MM operations in DNNs. Connected and convolutional layers use MM routines to compute
feed forward output, parameter gradients, and input gradients.

H

GOAT Blocking of Big Matrices

By default, GOAT allocates/releases resources on a per layer basis. In the event that even for a single sample, it is not
possible to satisfy the memory requirements of network (either large network or large inputs), GOAT breaks each layer
even further.
15

SGXRMM

VVG16-FC3

Figure 6: Throughput of SGD training step for VGG19,VGG16, ResNet152, and Resnet34 on ImageNet dataset.
Randomized Matrix Multiplication can make verification twice faster in case of VGG architecture.

F.4

9.01

17.43

SGXRMM

VGG16-FC2

10.72
8.36

9.96

10.78
8.46

10.75
8.48
SGX

9.82

19.25

19.87

30.67

34.02

40.18

40.75
16.99

16.04
12.04

16.76

16.11
12.06

10

SGX

39.3

48.31
31.62

32.66

41.36
24.89
12.75

16.26
12.32

17.3

17.1
13.18

15.73

16.99
13.1

17.07

17.02
13.25

30

20

42.32

47.99

50.91

53.27
32.76

40

31.44

34.59

50

45.3

45.75

60

Throughput (Images/Sec)

57.42

57.11

59.84

Throughput Performance (CIFAR10)

A PREPRINT

Table 4: Matrix Multiplication Operations
Layer Type

Pass

Computation

Forward

O[B][O] = I[B][I] × (W[O][I] )|

Backward Parameters
Gradient

O
|
∇W
[O][I] = (∇[B][O] ) × I[B][I]

Fully Connected

Backward Inputs
Gradient

Forward

Backward Inputs
Gradient

Verification
= (W[O][I] )| × R[O][1]
= I[B][I] × Υ[I][1]
= O[B][O] × R[O][1]
= I[B][I] × R[I][1]

|
Z[O][1] = (∇O
[B][O] ) × Υ[B][1]

Z 0 [O][1] = ∇W
[O][I] × R[I][1]
Υ[O][1] = W[O][I] × R[I][1]
∇I[B][I] = ∇O
[B][O] × W[O][I]

O[f ][wo .ho ] = W[f ][k2 .Ci ] × I[k2 .Ci ][wo .ho ]

Convolutional
Backward Parameters
Gradient

Υ[I][1]
Z[B][1]
Z 0 [B][1]
Υ[B][1]

O
|
∇W
[f ][k2 .Ci ] = ∇[f ][wo .ho ] × (I[k2 .Ci ][wo .ho ] )

Z[B][1] = ∇O
[B][O] × Υ[O][1]
Z 0 [B][1] = ∇I[B][I] × R[I][1]
Υ[1][k2 .Ci ] = R[1][f ] × W[f ][k2 .Ci ]
Z[1][wo .ho ] = Υ[1][k2 .Ci ] × I[k2 .Ci ][wo .ho ]
Z 0 [1][wo .ho ] = R[1][f ] × O[f ][wo .ho ]
Υ[wo .ho ][1] = (I[k2 .Ci ][wo .ho ] )| × R[k2 .Ci ][1]
Z[f ][1] = ∇O
[f ][wo .ho ] × Υ[wo .ho ][1]
Z 0 [f ][1] = ∇W
[f ][k2 .Ci ] × R[k2 .Ci ][1]
Υ[1][f ] = R[1][k2 .Ci ] × (W[f ][k2 .Ci ] )|

∇I[k2 .Ci ][wo .ho ] = (W[f ][k2 .Ci ] )| × ∇O
[f ][wo .ho ]

Z[1][wo .ho ] = Υ[1][f ] × ∇O
[f ][wo .ho ]
Z 0 [1][wo .ho ] = R[1][k2 .Ci ] × ∇I[k2 .Ci ][wo .ho ]

For convolutional layers, the main memory bottleneck is im2col3 which converts the layer’s input (for each sample) of
size ci · wi · hi to [k 2 · ci ] × [wo · ho ] (k is kernel window size) matrix for a more efficient matrix multiplication. GOAT
divides the inputs across the channel dimension and processes the im2col on maximum possible channels that can be
processed at once.
For fully-connected layers, the main memory bottleneck is the parameters matrix W[O]·[I] that does not depend on the
batch size. GOAT divides the matrix across the first dimension (rows), and processes the outputs on the maximum
possible size of rows that fits inside the TEE for the corresponding layer.

3

extracts redundant patches from the input image and lays in columnar format

16

A PREPRINT

Mean and STD of reference accuracy across 5 repeats for each epoch
0.90
Mean Accuracy

0.70
lr=10−1
mean

0.60

0.04

lr=10−2
std

0.50

0.02

STD Accuracy

0.06

0.80

0.40
0.30

0.00
0

20

40

60

80

100
Epoch

120

140

160

180

200

(a)

Highest test accuracy gap across 5 repeats

Best Difference

0.05

lr=10−1 lr=10−2

clip=5.10−4

clip=10−3

0.00

−0.05

−0.10

60

80

100

120
Epoch

140

160

180

200

180

200

(b)

Lowest test accuracy gap across 5 repeats

Worst Difference

0.05

0.00

−0.05

−0.10

−0.15
60

80

100

120
Epoch

140

160

(c)

Figure 7: 7a Reference Models (no gradient clipping) mean/std on test accuracy of 5 repeats for two different learning
rates. Each configuration had 5 repeats and a reference model (no attack and unbounded updates).
7b For each

rep
rep
17
run configuration the test accuracy difference dif flr,clip is defined as max acclr,clip − accref
∀rep ∈ [1, 5]. 7c


rep
rep
min acclr,clip − accref
∀rep ∈ [1, 5]

A PREPRINT

(a) Large with multiple color variations

(b) Small with low color variations in two separate corners

(c) Gray scale

(d) Instagram Kelvin Filter

(e) Color Rotation Filter

(f) Mix of Rotation and Instagram Nashville Filters

Figure 8: All examples of triggers on CIFAR10 images

18

