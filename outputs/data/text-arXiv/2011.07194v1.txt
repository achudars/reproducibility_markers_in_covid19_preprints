arXiv:2011.07194v1 [stat.AP] 14 Nov 2020

Leveraging Administrative Data for Bias Audits: Assessing
Disparate Coverage with Mobility Data for COVID-19 Policy
Amanda Coston

Neel Guha

Derek Ouyang

acoston@cs.cmu.edu
Carnegie Mellon University

nguha@stanford.edu
Stanford University

douyang1@stanford.edu
Stanford University

Lisa Lu

Alexandra Chouldechova

Daniel E. Ho

lcl@law.stanford.edu
Stanford University

achould@cmu.edu
Carnegie Mellon University

dho@law.stanford.edu
Stanford University

ABSTRACT

Grantz et al. describe “a critical need to understand where and to
what extent these biases may exist” in their discussion on the use
of mobility data for COVID-19 response.
Of particular interest is potential sampling bias with respect to
important demographic variables in the context of the pandemic:
age and race. Older age has been established as an increased risk factor for COVID-19-related mortality [52]. African-American, NativeAmerican and LatinX communities have seen disproportionately
high case and death counts from COVID-19 [45] and the pandemic
has reinforced existing health inequities that affect vulnerable communities [24]. If certain races or age groups are not well-represented
in data used to inform policy-making, we risk enacting policies that
fail to help those at greatest risk and serve to further exacerbate
disparities.
In this paper we assess SafeGraph, a widely-used point-of-interest
(POI)-based mobility dataset1 for disparate coverage by age and
race. We define coverage with respect to a POI: coverage is the
proportion of traffic at a POI that is recorded in the mobility data.
For privacy reasons, many mobility datasets are aggregated up
from the individual level to the physical point-of-interest (POI)
level. Due to this aggregation, we lack the resolution to assess
individual-level coverage quantities like the fraction of members of
a demographic subgroup of interest who are represented in the data.
Nonetheless, our POI-based notion of coverage is relevant for many
COVID-19 policies that are made based on traffic to POIs, such as
deciding to close certain business sectors or determining where to
locate resources like pop-up testing sites. We use differences in the
distributions of age and race across POIs to assess demographic
disparities in coverage.
While we focus here on a specific dataset and implications for
COVID-19 policy, the question of how one can assess disparate
coverage is a more general one in algorithmic governance. Ground
truth is often lacking, which is precisely why policymakers and
academics have flocked toward big data, on the implicit assumption
that scale can overcome more conventional questions of data reliability, sampling bias, and the like [2, 35]. Government agencies may
not always have access to protected attributes, making fairness and
bias assessments challenging [32].
The main contributions of our paper are as follows:

Anonymized smartphone-based mobility data has been widely
adopted in devising and evaluating COVID-19 response strategies
such as the targeting of public health resources. Yet little attention
has been paid to measurement validity and demographic bias, due
in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on
unique visits and demographics. We illustrate how linking largescale administrative data can enable auditing mobility data for bias
in the absence of demographic information and ground truth labels.
More precisely, we show that linking voter roll data—containing
individual-level voter turnout for specific voting locations along
with race and age—can facilitate the construction of rigorous bias
and reliability tests. These tests illuminate a sampling bias that is
particularly noteworthy in the pandemic context: older and nonwhite voters are less likely to be captured by mobility data. We
show that allocating public health resources based on such mobility
data could disproportionately harm high-risk elderly and minority
groups.

1

INTRODUCTION

Mobility data has played a central role in the response to COVID19. Describing the movement of millions of people, smartphonebased mobility data has been used to analyze the effectiveness
of social distancing polices (non-pharmaceutical interventions),
illustrate how movement impacts the transmission of COVID-19,
and probe how different sectors of the economy have been affected
by social distancing policies [1, 4, 7, 10, 20, 22, 33, 46]. Despite the
high-stakes settings in which this data is deployed, there has been
no independent assessment of the reliability of this data. In this
paper we show how administrative data (i.e., data from government
agencies kept for administrative purposes) can be used to perform
such an assessment.
Data reliability should be a foremost concern in all policy-making
and policy evaluation settings, and is especially important for
mobility data due to the lack of transparency surrounding data
provenance. Mobility data providers obtain their data from opt-in
location-sharing mobile apps, such as navigation, weather, or social
media apps, but do not disclose which specific apps feed into their
data [31]. This opacity prevents data consumers such as policymakers and researchers from understanding who is represented in the
mobility data, a key question for enabling effective and equitable
policies in high-stakes settings such as the COVID-19 pandemic.

(1) We show how administrative data can enable audits for bias
and reliability (§ 5)
1 POIs

refer to anywhere people spend money or time, including schools, brick-andmortar stores, parks, etc. https://www.safegraph.com/

1

Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra Chouldechova, and Daniel E. Ho

(2) We characterize the measurement validity of a smartphonebased mobility dataset that is widely used for COVID-19
research, SafeGraph (§ 4.2, 6.1)
(3) We illuminate significant demographic disparities in the
coverage of SafeGraph (§ 6.2)
(4) We illustrate how this disparate coverage may distort policy
decisions to the detriment of vulnerable populations (§ 6.3)
(5) We perform robustness tests that evaluate our results against
a possible type of confounding (§ 5.3, 6.2.1)

challenging, though, when there is no clear gold standard against
which to validate mobile phone data.” We provide the first rigorous
test for demographic bias using auxiliary estimates of ground truth.
Third, our work bears similarity to the literature on demographic
bias in medical data and decision-making. A long line of research
has demonstrated that medical research is disproportionately conducted on white males [17, 38, 41]. This literature has cataloged
the harmful effects of making treatment decisions for subgroups
that were underrepresented in the data [5, 47, 48]. In much the
same vein, our work calls into question research conclusions based
on SafeGraph data that may not be relevant for older or minority
subgroups.
Last, our work relates more broadly to the sustained efforts
within machine learning to understand sources of demographic bias
in algorithmic decision making [12, 13, 21, 25, 34]. Important work
has audited demographic bias of facial recognition technology [8],
child welfare screening tools [11], criminal risk assessment scores
[43], and health care allocation tools [2, 39]. Often the underlying
data is identified as a major source of bias that propagates through
the algorithm and leads to disparate impacts in the decision-making
stage. Similarly, our study illustrates how disparate coverage in
smartphone-based data can misallocate COVID-19 resources.

Our paper proceeds as follows. Section 2 and Section 3 discuss
related work and background on the uses of mobility data in the
pandemic. Section 4 provides an overview of our auditing framework and formalizes the assumptions to construct bias and reliability tests. Section 5 discusses the estimation approach using voter
roll data. Section 6 presents results that while SafeGraph can be
used to estimate voter turnout, the mobility data systematically
undersamples older individuals and minorities. Section 7 discusses
interpretation and limitations.

2

RELATED WORK

Our assessment of disparate coverage is related to several strands
in the literature. First, the most closely related work to ours is SafeGraph’s own analysis of sampling bias discussed below. However,
that analysis examines demographic bias only at the national aggregated level and does not address the question of demographic bias
for POI-specific inferences. Ours is the first independent assessment
of demographic bias to the extent we are aware.
Second, our work relates to existing work on demographic bias
in smartphone-based estimates [51]. A notable line of survey research has examined the distinct demographics of smartphone
users [18, 36]. [49] and [50] document significant concerns about
mobility-based estimates from mobile phone data, including particularly low coverage for elderly. The literature further finds that
smartphone ownership in the United States varies significantly with
demographic attributes [6]. In 2019 an estimated 81% of Americans owned smartphones with ownership rates of 96% for those
aged 18-29 and ownership rates of 53% for those aged over 65 [42].
Racial disparities in smartphone ownership are less pronounced,
with an ownership rate of 82%, 80%, and 79% for White, Latinx,
and African-American individuals, respectively. Even conditional
on mobile phone ownership, however, demographic disparities may
still exist. App usage may differ by demographic group. According
to one report, 69% of U.S. teenagers, for instance, use Snapchat,
compared to 24% of U.S. adults. Of particular relevance to mobility
datasets, the rate at which users opt in to location sharing may vary
by demographic subgroup. Hoy and Milne, for instance, reported
that college-aged women exhibit greater concerns with third party
data usage. And even among users who who opt in to a specific
app, usage behavior may differ according to demographics. Older
users, for instance, may be more likely to use a smartphone as a
“classic phone” [3].
Our work is in many ways a response to a recent call to characterize the biases in mobility data used for COVID-19 policies [23].
Grantz et al. highlight the potential for demographic bias, citing
“clear sociodemographic and age biases of mobile phone ownership.”
They note, “Identifying and quantifying these biases is particularly

3

BACKGROUND ON SAFEGRAPH MOBILITY
DATA

We now discuss the SafeGraph mobility dataset, illustrate how
this data has been widely deployed to study and provide policy
recommendations for the public health response to COVID-19, and
discuss SafeGraph’s own assessment of sampling bias.

3.1

SafeGraph Mobility Data

SafeGraph contains mobility data from roughly 47M mobile devices
in the United States. The company sources this data from mobile
applications, such as navigation, weather, or social media apps,
where users have opted in to location tracking. It aggregates this information by points-of-interest (POIs) such as schools, restaurants,
parks, airports, and brick-and-mortar stores. Hourly visit counts
are available for each of over 6M POIs in their database.2 Individual
device pattern data is not distributed for researchers due to privacy
concerns. Our analysis relies on SafeGraph’s ‘research release’ data
which aggregates visits at the POI level.

3.2

Use of SafeGraph Data in COVID-19
Response

When the pandemic hit, SafeGraph released much of its data for free
as part of the “COVID-19 Data Consortium” to enable researchers,
non-profits, and governments to leverage insights from mobility
data. As a result, SafeGraph’s mobility data has become the dataset
de rigueur in pandemic research. The Centers for Disease Control
and Prevention (CDC) employs SafeGraph data to examine the effectiveness of social distancing measures [37]. According to SafeGraph,
the CDC also uses SafeGraph to identify healthcare sites that are
reaching capacity limits and to tailor health communications. The
California Governor’s Office, and the cities of Los Angeles [19],
2 https://docs.safegraph.com/docs/places-summary-statistics

2

Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy

San Francisco, San Jose, San Antonio, Memphis, and Louisville,
have each relied on SafeGraph data to formulate COVID-19 policy,
including risk measurements of specific areas and facilities and
enforcement of social distancing measures. Academics, too, have
employed the data widely to understand the pandemic: [10] used
SafeGraph data to examine how social distancing compliance varied
by demographic group; [15, 16] used SafeGraph to infer the effect
of “superspreader” events such as the Sturgis Motorcycle Rally and
campaign events; [40] examined whether social distancing was
more prevalent in in areas with higher xenophobia; and [1] examined whether social distancing compliance was driven by political
partisanship, to name a few. What is common across all of these
works is that they assume that SafeGraph data is representative of
the target population.

3.3

demographic bias only at census aggregated levels and does not address the question of demographic bias for POI-specific inferences,
an independent coverage audit remains critical.

4

AUDITING FRAMEWORK

In this section we outline our proposed auditing methodology and
state the conditions under which the proposed method allows us
to detect demographic disparities in coverage. We motivate our
approach by first describing the idealized audit we would perform if
we had access to ground truth data. We then modify this framework
to account for the limitations of available administrative data.

4.1

Notation

Let I = {1, ..., 𝑛} denote a set of SafeGraph POIs. Let 𝑆 𝑗 ∈ R𝑛
denote a vector of the SafeGraph traffic count (i.e. number of visits)
𝑗
for day 𝑗 ∈ J where each element 𝑆𝑖 indicates the traffic to POI 𝑖 on
𝑗
day 𝑗. Similarly let 𝑇𝑖 denote the ground truth traffic (visits) to POI
𝑖 during day 𝑗. When the context is clear, we omit the superscript 𝑗
when referring to vectors 𝑆 ∈ R𝑛 and 𝑇 ∈ R𝑛 . We use ⊘ to denote
Hadamard division (the element-wise division of two matrices).
With this, we define our coverage function 𝐶 (𝑆,𝑇 ).

SafeGraph Analysis of Sampling Bias

SafeGraph has issued a public report about the representativeness
of its data [44]. While SafeGraph does not have individual user
attributes (e.g., race, education, income), it merged census data
based on census block group (CBG), the smallest geographic unit
for which the census publishes data, to assess bias along demographic characteristics. The racial breakdown of device holders, for
instance, was allocated proportionally based on the racial breakdown of a CBG. SafeGraph then compared the total SafeGraph
imputed demographics against census population demographics
at the national, state, county, and CBG levels. According to SafeGraph, the results looked “quantitatively very close to the expected”
at the state and county levels, but the sampling rates at the CBG
level looked highly unrepresentative. SafeGraph warned that “local
analyses examining only a few CBGs” should proceed with caution.
SafeGraph’s examination for sampling bias should be applauded.
Companies may not always have the incentive to address these
questions directly, and SafeGraph’s analysis is transparent, with
data and replication code provided. As far as we are aware, it remains the only analysis of SafeGraph sampling bias.
Nevertheless, their analysis suffers from several key limitations.
First, because SafeGraph lacks demographic information about the
users, the imputation of the CBG attributes imposes a strong homogeneity assumption. The mere fact that 52% of Atlanta’s population
is African American does not mean that five out of ten SafeGraph
devices in Atlanta belong to African-Americans. Second, by aggregating demographic analyses nationally for a single attribute at a
time, the results may miss significant differences in the joint distribution of features. For instance, if coverage is better for younger
populations and for whiter populations, but whiter populations
are on average older than non-white populations, then evaluating
coverage marginally against either race or age will underestimate
disparities. Indeed we present evidence for such an effect in § 6.
Third, the dramatic variation in SafeGraph coverage across CBGs is
serious cause for concern because many of the COVID-19 analyses
referenced above leverage SafeGraph data at finer geographic units
than CBGs (e.g. POIs). This risks drawing conclusions from data at
a level of resolution that SafeGraph has not established to be free
from coverage disparities. Because SafeGraph’s analysis examines

Definition 1 (Coverage function). Let 𝐶 (𝑆,𝑇 ) : R𝑛 × R𝑛 ↦→
R𝑛 denote the following coverage function:
𝐶 (𝑆,𝑇 ) = 𝑆 ⊘ 𝑇
The coverage function yields a vector where the ith element equals 𝑇𝑆𝑖𝑖
and describes the coverage of POI i.
𝑗

Let 𝐷𝑖 denote a numeric measure of the demographics of vis𝑗
itors to POI 𝑖 on day 𝑗; for instance 𝐷𝑖 may be the percentage
of visitors to a location on a specific day that are over the age of
cov (𝑋 ,𝑌 )
65. Let cor(𝑋, 𝑌 ) = √
denote the Pearson correlation
var (𝑋 ) var (𝑦)

between vectors 𝑋 and 𝑌 and let 𝑟 (𝑋 ) be the rank function that returns the rank of vector 𝑋 .3 Our audit will consider the (Spearman)
rank correlation cor(𝑟 (𝑋 ), 𝑟 (𝑌 )), which provides a more flexible
measure of association since it assumes only monotonicity (versus
the linearity assumption in the Pearson correlation).

4.2

Idealized Audit

Our audit assesses how well SafeGraph measures ground truth
visits and whether this coverage varies with demographics. We
operationalize these two targets as follows:
Definition 2 (Measurement signal and validity). Define the
strength of measurement signal as
cor(𝑟 (𝑆), 𝑟 (𝑇 )).
A positive signal indicates facial measurement validity, and a
signal close to one indicates high measurement validity.
Definition 3 (Disparate coverage). We will say that disparate coverage exists when the rank correlation between coverage
3 The

rank assigns each element of the vector the value of its rank in an increasing
ranking of all elements in the vector. For example the rank of vector "(5, 1, 3)" would
be "(3, 1, 2)".
3

Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra Chouldechova, and Daniel E. Ho

𝑎𝑔𝑒

𝑠𝑚𝑎𝑟𝑡𝑝ℎ𝑜𝑛𝑒 𝑢𝑠𝑒

𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒

𝑎𝑔𝑒

𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒

𝑐𝑜𝑛𝑓 𝑜𝑢𝑛𝑑𝑒𝑟

(a) Causal association

X
𝑎𝑔𝑒

𝑟𝑢𝑟𝑎𝑙 𝑟𝑒𝑔𝑖𝑜𝑛

ˆ
𝑎𝑔𝑒

(b) Non-causal association

Figure 1: Possible mechanisms under which disparate coverage arises. Disparate coverage may be a result of a causal
associations such as (a) whereby older people are less likely
to own or use smartphones and therefore places frequented
by older people have lower coverage. Disparate coverage
may also arise due to a non-causal associations such as (b)
whereby rural regions have higher percentages of older residents and worse cell reception which reduces coverage. Both
types of associations are policy-relevant because in both
cases, certain age groups are underrepresented.

demographics based on the event (e.g., international soccer game
between two countries) in order to assess disparate coverage? Two
major impediments are lack of access to precise demographic estimates as well as confounding factors such as tailgating that may
vary with demographics, thereby violating Assumption 1.

5.1

Administrative data on voter turnout

We propose a solution using large-scale administrative data that
records individual-level visits along with demographic information:
voter turnout data. Such data has several unique advantages. First,
because these stem from official certified records by election authorities, voter turnout information is of uniquely high fidelity. In
an analysis of five voter file vendors, Pew Research, for instance,
found that the vendors had 85% agreement about turnout in the
2018 election [30]. Second, numerous states break out voter turnout
by in person, election day voting. This is critical, given the rise of
absentee, mail, and early voting, enabling us to infer the exact count
of individuals visiting a specific voting location on a specific day.
Third, voter registration forms typically include fields for date of
birth, gender, and often race.4 When race is not provided, data
vendors estimate race. The Pew study found race to be 79% accurate across the five vendors, with accuracies varying from 67% for
African-Americans to 72% for Hispanics to 93% for non-Hispanics.5
Fourth, voting (poll) locations provide many data samples across a
wide geography with demographic variation, which is necessary
given that SafeGraph POI visit information is necessarily aggregated. In short, this administrative data enables us to cleanly infer
the demographics and number of visitors to polling locations on
election day.
We use individual voter records provided by L26 , a private voter
file vendor which aggregates publicly available voter records from
jurisdictions nationwide. Our analysis relies primarily on four fields
from the voter files: age, race, precinct, and turnout. While this data
is near ideal, it is missing one key piece of information: the poll
location. We hence obtained a crosswalk of voting precinct to poll
location from the North Carolina Secretary of State. This crosswalk
enables us to map each voter via their voting precinct to a SafeGraph
POI. We note that poll locations are often schools, community
centers, religious institutions, and fire stations. These POIs may

We are interested in identifying an association of any kind; we
are not concerned with identifying a causal effect. Age might have
a causal effect on smartphone usage, setting aside the question of
manipulability [28], as depicted in the top panel (a) of Fig. 1. But as
the bottom panel (b) depicts, age may not directly affect SafeGraph
coverage but be directly correlated with a factor like urban/rural
residence, which in turn does affect SafeGraph coverage. For either
mechanism, the policy-relevant conclusion remains that SafeGraph
is underrepresenting certain age groups.

Real-world audit

In reality, there is no ground truth source of information about
foot traffic and the corresponding demographics for all 6 million
POIs. Instead, we must make do with estimates of 𝑇 and 𝐷 based
on auxiliary data sources about some subset of visits to a subset
of POIs. In order to identify the relationship of interest (Def. 3)
between coverage and demographics, we need the following to
hold:
Assumption 1 (No induced confounding). The estimation
procedure does not induce a confounding factor that affects both the
estimate of demographics and the estimate of coverage (see Figure 2).
Assumption 2 (No selection bias). The selection is not based on
an interaction between factors that affect coverage and demographics.
After introducing our auxiliary data and the estimation procedure, § 5.3 revisits the plausibility and testability of these assumptions. Appendix C discusses the analogous assumptions required
to identify the target for measurement validity (Def. 2).

5

𝑐𝑜𝑣 𝑒𝑟𝑎𝑔𝑒
b

Figure 2: Our results assume the estimation of coverage and
demographics such as age does not induce confounding. We
describe a test for time-invariant confounding in § 5.3 and
report results in § 6.2.1

and the demographic measure is statistically different from zero:

cor 𝑟 (𝐶 (𝑆,𝑇 ), 𝑟 (𝐷) ≠ 0.

4.3

X

𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒

ESTIMATION VIA ADMINISTRATIVE DATA

It is quite challenging to identify data sources for ground truth
visits to POIs with corresponding demographic information [23].
Consider for instance large sporting events where stadium attendance is closely tracked. Can we leverage differences in audience

4 North

Carolina, for instance requests both race and ethnicity (https://s3.amazonaws.
com/dl.ncsbe.gov/Voter_Registration/NCVoterRegForm_06W.pdf).
5 The study did not name which voter file vendors were analyzed.
6 https://l2political.com/

4

Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy

and age.7 We control for this confounding by estimating non-voter
traffic using mean imputation. In Appendix D, we provide similar
results using a local linear regression type imputation procedure.

hence also have non-voter traffic on election day. We address this
possible source of confounding in § 5.2. Overall, our data includes
595K voters who turned out to vote at 549 voting locations that
could be matched. Table 1 presents summary statistics on voters
associated with polling locations that could be matched, showing
that our data is highly representative of all voting locations. (Details
on the data and preprocessing are provided in Appendices A and B.)

5.2.1 Additional notation. Letting 𝑗 ∗ denote election day, we esti𝑗∗
mate the non-voter traffic at poll location 𝑖 on election day, 𝑍𝑖 , by
averaging SafeGraph traffic to 𝑖 on adjacent days:
𝑗 ∗ −1

𝑆
𝑗∗
𝑍𝑖 = 𝑖

2
This adjustment enables us to compute the marginal traffic over
the estimated baseline, which we term SafeGraph marginal traffic.8

Matched Voters All Voters
Voters
595,159 1,581,937
Mean Age
52.84
52.78
Std Age
16.65
16.59
Proportion over 65
0.26
0.26
Proportion Hispanic
0.04
0.04
Proportion Black
0.19
0.19
Proportion White
0.71
0.71
Table 1: Demographics of all voters in North Carolina’s 2018
general election compared to voters included in our analysis
("matched voters"). The matched voters are representative
of the full voting population. Details of the matching procedure are given in Appendix B.

Definition 4 (Marginal traffic). SafeGraph marginal traffic
𝑗∗
𝑗∗
denotes device counts above estimated baseline: 𝑆𝑖 − 𝑍𝑖 .
𝑗∗

Let 𝑉𝑖 denote the number of voters at poll location 𝑖 as recorded
by L2. With this, we refine our definition of coverage using the
coverage function from Def. 1:
Definition 5 (SafeGraph coverage). SafeGraph coverage is
∗
∗
∗
𝐶 (𝑆 𝑗 − 𝑍 𝑗 , 𝑉 𝑗 ). Each element 𝑖 of this vector refers to the ratio of
marginal traffic at POI 𝑖 to voter turnout at 𝑖.
We focus on two measures of demographics: 𝐴, the proportion
of voters over the age of 65, and 𝑅, the proportion of voters who are
an ethnic group besides white.9 Let I denote the indicator function.

The disparate impact (Def. 3) question in this setting is does
SafeGraph coverage of voters at different poll locations vary with
voter demographics? We focus on two key demographic risk factors
for COVID-19: age and race. We summarize the age distribution at
a polling location 𝑖 by computing the proportion of voters over age
65, which we denote by 𝐴𝑖 . Let 𝑅𝑖 denote the proportion of voters
whose ethnicity in L2 data is listed as Hispanic and Portuguese,
Likely African-American, East and South Asian, or Other (all voters
whose ethnicity was not listed as European). We will refer to 𝑅𝑖 as
the proportion of non-white voters.
We briefly discuss issues of representativeness. Because the voting population is not a random sample of the population, the magnitude of an association between coverage and age/race among
voters is likely different than the magnitude among the whole population. Since the voting population is older and more white than
the general population [30], the association among voters could
very well underestimate the magnitude of the population association. However, our target measure of bias (Def. 3) does not depend
on the magnitude of an association. Assuming Conditions 1 and 2
hold, evidence for any association on the voting population is indicative of an association (of perhaps different magnitude) on the
full population.

5.3

Testing disparate coverage among voters

Following our proposed audit in Def. 3, we test whether there is
a rank correlation between 𝐶 (𝑆 𝑗∗ − 𝑍 𝑗∗, 𝑉 𝑗∗ ) and demographic
measure 𝐷. If so, and if we believe Assumptions 1- 2 hold, then we
may conclude SafeGraph has disparate coverage. We next discuss
how we can partially test Assumption 1 and how we can reason
about the remaining assumptions.
We can relax Assumption 1 that the estimation procedure does
not induce confounding by decomposing confounding into timeinvariant and time-varying confounding. We can test for timeinvariant confounding, enabling us to make the weaker and more
reasonable assumption of no time-varying confounding. A timeinvariant confounder is a confounder that affects our demographic estimate as well as traffic on election day and on nonelection days. This contrasts to a time-varying confounder that
affects our demographic estimate and traffic on election day but
does not affect traffic on non-election days. Examples of timeinvariant and time-varying confounding are given in Figure 3. The
assumption of no time-varying confounding is untestable but it is
reasonable to believe this holds in our setting. Most voting places,
7 Non-voter

5.2

𝑗 ∗ +1

+ 𝑆𝑖

traffic may affected by device attribution errors, in which device GPS
locations are incorrectly assigned to one of two adjacent POIs. SafeGraph reports
in its user documentation that "[it] is more difficult to measure visits to a midtown
Manhattan Starbucks than a visit to a suburban standalone Starbucks." If younger
voting populations are more likely to vote in dense urban polling locations, then even if
there isn’t large non-voter traffic in the same facility, large traffic in an adjacent facility
could still be incorrectly attributed to the polling location with greater likelihood than
to a suburban polling location. However, this source of confounding can be controlled
for using the same technique described.
8 The adjustment resulted in negative estimates of voter traffic for poll locations at
schools. Because of the difficulties in estimating the baseline for schools, we have
removed schools from our analysis.
9 In what follows we use the generic variable 𝐷 to indicate either measure of
demographics.

Adjustment for non-voter traffic

Non-voter traffic may be incorporated into SafeGraph measures
and may confound our analysis if the magnitude of that non-voter
traffic varies with the demographic attributes of the voters. For
instance, if younger voting populations are more likely to vote at
community centers which have large non-voter traffic and older
voting populations are more likely to vote at churches which have
small non-voter traffic, then even if SafeGraph has no disparate coverage, we would observe a negative relationship between coverage
5

Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra Chouldechova, and Daniel E. Ho

Time-invariant confounding

Time-varying confounding

Example: Younger voting populations vote at places like
community centers with large non-voter traffic whereas
older populations vote at places like fire stations with little
non-voter traffic.

Example: Younger voting populations vote at places that
are open to non-voter traffic on election day whereas older
populations vote at places that are closed to non-voter
traffic on election day

Testable (see § 5.3)

Untestable assumption

Figure 3: We distinguish between two types of confounding: time-invariant versus time-varying confounding. We test for
time-invariant confounding (§ 5.3) but we cannot test for time-varying confounding. Our results assume no time-varying
confounding.
for instance, are public places making it unlikely that the non-voter
traffic is affected differentially on election and non-election days.
Another possible time-varying confounder would be if voting locations with older (or largely non-white) voters are more likely to
be placed outside of the SafeGraph geometry for device attribution
(e.g., parking lot). We do not believe this is likely because voting
locations are typically indoors for security and climate reasons
during a November election.

at home when they go vote but always carry their smartphones
otherwise whereas younger (or white) voters bring their smartphones to the polls and elsewhere. We believe such mechanisms
are unlikely. Testing this assumption would require the use of an
additional auxiliary dataset which is outside the scope of this paper.

5.3.1 Placebo test for time-invariant confounding. To test for any
time-invariant confounding induced by the estimation procedure,
we conduct a placebo test that repeats the disparate coverage audit
(Def. 3) comparing marginal traffic on placebo (non-election) days
to the election-day demographics.10 Using 48 weekdays in October
and November of 2018, we generate a placebo distribution of the
estimated correlation coefficients for all placebo days against which
we compare the election-day estimate. Algorithm 1 provides details.
If the election-day correlation is unlikely under placebo distribution (i.e. small p-value), then we say the placebo test passes. If we
additionally believe there is no time-varying confounding, then we
can conclude that SafeGraph has disparate coverage of voters on
election day.

Election day brings a dramatic increase in traffic to polling locations relative to non-election days, and any valid measure of visits
should detect this outlier. Figure 4 shows the daily aggregate traffic
across poll locations for October and November of 2018, and as expected, we see a significant increase in both total traffic (top panel)
and marginal traffic (bottom panel) on election day. To assess the
strength of this signal using the framework described above (Def. 2),
we present the correlation between marginal SafeGraph traffic on
election day and actual voter turnout. The rank correlation
test

yields a positive correlation: cor 𝑟 (𝑆 𝑗∗ −𝑍 𝑗∗ ), 𝑟 (𝑉 𝑗∗ ) = 0.445 with
p-value < 0.001.11 Figure 5 displays this relationship by comparing
𝑗∗
𝑗∗
the marginal election traffic 𝑆𝑖 − 𝑍𝑖 on the 𝑥-axis against actual

6 RESULTS
6.1 Measurement Validity

𝑗∗

voter counts 𝑇𝑖 on the 𝑦-axis for each polling location.
This corroborates that SafeGraph data is able to detect broad
patterns in movement and visits. That said, the estimates at the
individual polling place location level are quite noisy: root meansquared error is 899 voters with a standard error of 45. For instance,
amongst polling places that registered 20 marginal devices, roughly
250 to 1600 actual voters turned out. This significant noise is likely
due to a combination of factors. First, SafeGraph may incorrectly
attribute voters to nearby POIs because of incorrect building geometries. Second, we are not able to perfectly adjust for non-election
traffic. Third, SafeGraph may have disparate coverage of voters by
demographic attributes. This last factor is the focus of our analysis.

Algorithm 1: Placebo test of time-invariant confounding
Input: Voter data (𝑉 𝑗∗, 𝐷 𝑗∗ ) SafeGraph data {(𝑆 𝑗 , 𝑍 𝑗 )}𝑛𝑗=1
Result: 𝑝-value for the election-day correlation under the
placebo distribution
for 𝑗 = 1, 2, . . . 𝑛 do
Compute 𝜌 𝑗 = cor(𝑟 (𝐶 (𝑆 𝑗 − 𝑍 𝑗 , 𝑉 𝑗∗ )), 𝑟 (𝐷 𝑗∗ )).
end
∑︁
return 𝑝 = 𝑛1

I{(𝜌 𝑗 ≤ 𝜌 𝑗 ∗ )}

𝑗≠𝑗 ∗

In order to generalize these findings to the broader population
on non-election day, Assumption 2 (no selection bias) must hold.
Examples of violations of this assumption might include: (i) The
older (or non-white) population who doesn’t vote is more likely to
use smartphones than the older (or non-white) population who does
vote; and (ii) Older (or non-white) voters leave their smartphones

6.2

Demographic Bias

We assess whether the demographic composition of voters who
actually turned out to vote in person is correlated with coverage.
First, we find a statistically significant negative correlation between
11 The

10 This

correlation is similar but slightly lower for unadjusted SafeGraph traffic:
cor 𝑟 (𝑆 𝑗 ∗ ), 𝑟 (𝑉 𝑗 ∗ ) = 0.409 with p-value < 0.001.

placebo test is similar to methods of randomization inference in the literature
on treatment effects [27].
6

Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy

10000

0.04

●

20000
●● ●●●●
●●●
●

●●●●● ●●●

●
●●●● ● ● ●
●

●
●●

●

0

0.02

Coverage

SafeGraph traffic

0.03

All

●
●

●●●●●

●●●● ●●

20000

Marginal

10000

●
●
●
●
●
●●●● ●●
●●●● ●●
● ● ● ●
●
●

0

●
●

●

●

●

0.00

●●●

●●● ●
●
●
●

●●

0.01

−0.01

●

Oct 01

Oct 15

Nov 01

Nov 15

0.0

Dec 01

0.2

0.4

0.6

Proportion over age 65

Date
# Voters (1K)

10

15

20

● Monday ● Tuesday ● Wednesday ● Thursday ● Friday

Figure 4: SafeGraph traffic by weekday over October and November 2018 for all polling locations in North Carolina. The
top panel shows all SafeGraph traffic and the bottom panel
the marginal traffic computed using the method in § 5.2. In
both total and marginal traffic, the election day (dotted) line
shows a significant boost in traffic.

6000

0.030

Coverage

0.025

0.020

●

0.015

0.010

Voters

4000
●

0.00

0.25

0.50

0.75

1.00

Proportion Non−White

●
●
●

●
●
● ●

2000

●

●

●
●

●
●

0
−25

●

●
●

●
●
●
● ●
● ●●

25

12

15

18

21

●

●

●
●● ●
●
●
●●
●●
●
●
●
●
●●
● ● ●
●
● ●
● ●
●● ● ● ● ● ●●
●
● ●
●
●
● ● ●
● ● ● ●● ●
● ●
● ●●
●
● ●● ●
●
●
●
●
●
●
●
● ●●
● ● ● ●● ●
● ● ● ●● ● ●
●
●
● ●
●
●
●
●
●
● ● ●
●● ●
●
●
●
●
● ●
● ●●
● ● ● ●●
●● ● ●
●
● ●●
●
●
●●
●
● ●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●●
●
●●●
●●●●
●
●
●●●●
●
●
●
●
●
●●
●
●
● ●
●
●
●
●●
●●
●
●
●
●
●● ●●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●●●
●
●
●●
●
●
●
●
●● ● ●●
●●
●●
●
●●
●
●●
●
●
●● ●
●
●
●●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●
● ● ● ●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
● ●●
●
●
●
●
●●●
●
●
●
●
●●
● ●
●
●
●
●
● ●
●
●
●
●
● ●●
●
●
●
●
●
●● ●●
●
● ●● ●
●
●
●

0

# Voters (1K)

●
●

●

●

50

●
●●

Figure 6: Estimated SafeGraph coverage rates against age
and race for North Carolina 2018 general election for ventiles of poll location by age (top) and race (bottom).

●

75

100

SafeGraph marginal election traffic

correlation test yields cor 𝑟 (𝐶 (𝑆 − 𝑍, 𝑉 ), 𝑟 (𝐴) = −0.18 with pvalue < 0.001. We also find that coverage weakly decreases as
the proportion of non-white voters increases (bottom panel). The

rank correlation of race and coverage is cor 𝑟 (𝐶 (𝑆 − 𝑍, 𝑉 ), 𝑟 (𝑅) =
−0.076 with p-value = 0.073.
We now examine the interaction between age and race, which is
particularly important for two reasons. First, there are widespread
concerns that disparate impact can be more pronounced at the
“intersection” of protected groups [8, 9, 14]. Second, as we show
in Appendix A, age and race are highly correlated in our sample.
Polling locations with younger voters are also more likely to have

Figure 5: Election day traffic as observed by SafeGraph (𝑥axis) and actual voter turnout across polling locations (𝑦axis). Each dot represents a polling location in North Carolina in the 2018 general election.

SafeGraph coverage and the primary risk factor of COVID-19: age.
The top panel of Figure 6 shows how SafeGraph coverage 𝐶 (𝑆 −
𝑍, 𝑉 ) varies with 𝐴, the proportion of voters over age 65. The rank
7

Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra Chouldechova, and Daniel E. Ho

Race quartile (4 = largest percent non−white)

higher proportions of minority voters. We hence fit simple linear regressions to model coverage as a function of the percentage over 65,
percentage white, and the interaction between these demographic
attributes. Appendix E presents fuller regression results, which
demonstrate that once we control for age, the effect for race is more
robust. Because the demographics have to be interpreted jointly,
the top panel of Figure 7 presents a heat map of coverage with age
bins (quartiles) on the 𝑥-axis and race bins (quartiles) on the 𝑦-axis.
This bottom left cell, for instance, shows that precincts that are the
most white and young have highest coverage rates. Conditional on
a young precinct, a greater minority population decreases coverage. The lowest coverage is for older minority precincts. The lower
panel of Figure 7 similarly plots race on the 𝑥-axis against coverage on the 𝑦-axis, separating older precincts (yellow) and younger
precincts (blue). As can be seen from the regression lines, there is
both an average difference between older and younger precincts
and coverage declines as the minority population increases.
In short, we have provided evidence that there is disparate SafeGraph coverage by two protected attributes that are risk factors for
COVID-19: age and race. We bolster this claim of disparate coverage
by next showing that we pass the placebo test for time-invariant
confounding.

2.3

1.9

0.7

1.1

3

1.8

2.4

1.7

1.8

2

2.9

1.8

1.9

1.4

1

3.9

2.3

2

1.5

2

3

4

1

Age quartile (4 = oldest)
% Coverage

1

2

3

0.04

6.2.1 Placebo tests of Assumption 1. In this section we use the
placebo test framework described in § 1 to support the assumption of no time-invariant confounding. We consider 𝜌 𝑟 (𝐶 (𝑆 𝑗 −

𝑍 𝑗 , 𝑉 𝑗∗ )), 𝑟 (𝐷 𝑗∗ ) , the rank correlation between voter demographics and the ratio of SafeGraph marginal traffic on non-election
days to voter turnout. Evidence of a non-zero correlation may
suggest time-invariant confounding induced by our estimation procedure. Figure 8 shows that the election-day rank correlation is
significantly outside the placebo distribution (empirical one-sided
𝑝-values are 0 and 0.03 for age and race, respectively). Our second
robustness check computes the coefficients for the linear regression
of 𝐶 (𝑆 𝑗 − 𝑍 𝑗 , 𝑉 𝑗∗ ) ∼ 𝐴 𝑗∗, 𝑅 𝑗∗ on all weekdays 𝑗 in October and
November 2018. The coefficients for age and race are statistically
outside the placebo distribution (empirical one-sided 𝑝-values are 0
and 0.02 for age and race respectively). In Appendix C, we present
results that show that placebo tests for measurement validity also
pass.

6.3

4

Coverage

0.03

0.02

0.01

0.00
0.00

0.25

0.50

0.75

1.00

Proportion Non−White
# Voters (1K)
4

Policy implications

8

12

Poll age
elder

young

Figure 7: Intersectional coverage effects by race and age. The
top panel presents the coverage rate by quartiles of age on
the 𝑥-axis and race on the 𝑦-axis. The bottom panel plots the
coverage rate on the 𝑦-axis against percentage of non-white
voters at the polling location on the 𝑥-axis for older polling
locations (yellow) versus younger polling locations (blue)
for ventiles of poll location. Coverage is lowest among older
minority populations and highest among younger whiter
populations.

We now examine the policy implications of disparate coverage in
light of the widespread adoption of SafeGraph data in COVID-19
response. In particular, we show how disparate coverage may lead
to under-allocation of important health resources to vulnerable
populations. For instance, suppose the policy decision at hand is
where to locate mobile pop-up COVID-19 testing sites, and suppose
the aim is to place these sites in the most trafficked areas to encourage asymptomatic individuals to get tested. One approach would
use SafeGraph traffic estimates to rank order POIs. How would
this ordering compare to the optimal ordering by ground truth
traffic? Using voter turnout as an approximation to ground truth
traffic, we perform linear regression of the rank of voter turnout
against rank according to SafeGraph traffic as well as age and race:
𝑟 (𝑉 ) ∼ 𝑟 (𝑆 − 𝑍 ) + 𝐴 + 𝑅. Table 2 presents results of this rank
regression (where rank is in descending order), confirming that

the SafeGraph rank is significantly correlated with ground truth
rank. But the large coefficient on age indicates that each percentage
point increase in voters over 65 is associated with a 4 point drop
in rank relative to the optimal ranking. Similarly, the coefficient
on race indicates that each point increase in percent non-white is
8

Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy

associated with a one point drop in rank relative to the optimal
ranking. This demonstrates that ranking by SafeGraph traffic may
disproportionately harm older and minority populations by, for
instance, failing to locate pop-up testing sites where needed the
most.

7.5

age

5.0

count

2.5

0.0

Table 2: To evaluate a potential rank-based policy allocation,
we compare the rank of voter turnout against rank by SafeGraph traffic, controlling for age and race in a linear regression. Although SafeGraph rank is correlated with the optimal rank by voter turnout, the coefficients on age and race
indicate that each demographic percentage point increase is
associated with a 4-point and 1-point drop in rank for age
and race, respectively. This indicates that significant adjustments based on demographic composition should be made
to a SafeGraph ranking. Failure to do so may direct resources
away from older and more minority populations.

7.5

race

5.0

2.5

0.0
−0.2

−0.1

0.0

Rank correlation ρ(r(D),r(C(S−Z,V)))
Election

0.1

Regular

Figure 8: Distribution of placebo rank correlations between
election-day demographics and marginal SafeGraph traffic
on non-election days. Under the empirical placebo distribution, the election-day coverage’s negative correlations with
age (top panel) and race (bottom panel) are very unlikely (pvalue < 0.05). This placebo analysis supports the assumption of no time-invariant confounding. Placebo correlations
computed for 48 weekdays in October and November 2018.

Dependent variable:
Voter turnout rank
SafeGraph rank

0.325∗∗∗
(0.041)

% over 65

4.226∗∗∗
(0.734)

% non-white

0.930∗∗∗
(0.283)

Constant

40.864∗
(24.379)

20

15
age

10

count

5

0

Observations
R2
Adjusted R2
Residual Std. Error
F Statistic

20

15
race

10

5

Note:

549
0.204
0.199
141.903 (df = 545)
46.468∗∗∗ (df = 3; 545)
∗ p<0.1; ∗∗ p<0.05; ∗∗∗ p<0.01

0
−0.04

−0.02

0.00

Coefficient of linear regression of rate on age and race
Election

We also consider the implications of using SafeGraph to inform
resource allocation decisions, such as provision of health care resources such as masks, decisions to open or close categories of
businesses in public health orders, and whether to allocate investigations in failures to comply with social distancing. We compare
two approaches to such resource allocation decisions as follows:
We bin polling locations into terciles based on their age and race
and calculate what the allocation would be under ground truth
(from voter turnout data) and under the SafeGraph data. Table 3
presents results. Each cell presents the proportion of resources that
would be allocated to that age-race tercile, demonstrating that strict
reliance on SafeGraph would under-allocate resources by 35% to the
oldest/most non-white category (𝑝-value < 0.05) and over-allocate
resources by 30% to the youngest/whitest category (𝑝-value < 0.05).

Regular

Figure 9: Placebo distribution of coefficients of the linear
regression of marginal SafeGraph coverage on election-day
age and race demographics. Under the empirical placebo
distribution, the election-day’s negative coefficients for age
(top panel) and race (bottom panel) are very unlikely (pvalue < 0.05). This placebo analysis supports our conclusion
that SafeGraph data has disparate coverage by age and race.
Placebo regressions computed for 48 weekdays of October
and November 2018.

9

Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra Chouldechova, and Daniel E. Ho

SafeGraph allocation Optimal voter allocation
1
0.35 ± 0.03
0.27 ± 0.01
2
0.32 ± 0.03
0.28 ± 0.02
3
0.18 ± 0.02
0.21 ± 0.01
4
0.15 ± 0.02
0.23 ± 0.02
Table 3: Allocation of resources for age-race tertiles by SafeGraph versus by true voter counts (with standard errors).
The SafeGraph allocation redirects over 30% of the optimal
allocation from the oldest/most non-white tertile (3) to the
youngest/whitest tertile (1) (p-value < 0.05).

more tractable. If for instance researchers could identify that a
data point emanates from Snapchat, then they could use what is
known about the Snapchat user base to make adjustments. Given
its increasing importance for policy, SafeGraph should consider
disclosing more details about which apps feed into their ecosystem.

8

Mobility data based on smartphones has been rapidly adopted in
the COVID-19 response. As [23] note, one of the most profound
challenges arising with such rapid adoption has been the need
to assess the potential for demographic bias “when there is no
gold standard against which to validate mobile phone data.” Our
paper illustrates one potential path forward, by linking smartphonebased data to high-fidelity ground truth administrative data. Voter
turnout records, which record at the individual level whether a
registered voter traveled to a polling location on a specific day
and describe the voter’s demographic information, enable us to
develop a straightforward audit test for disparate coverage. We find
that coverage is notably skewed along race and age demographics,
both of which are significant risk factors for COVID-19 related
mortality. Without paying attention to such blind spots, we risk
exacerbating serious existing inequities in the health care response
to the pandemic.

The clear policy implication here is that while SafeGraph information may aid in a policy decision, auxiliary information (including prior knowledge) should likely be combined to make final
resource allocation decisions.

7

CONCLUSION

DISCUSSION

We have provided the first independent audit of demographic bias
of a smartphone-based mobility dataset that has been widely used
in the policy response to COVID-19. Our audit indicates that the
data underrepresents two high risk groups: older and more nonwhite populations. Our results suggest that policies made without
adjustment for this sampling bias may disproportionately harm
these high risk groups. However, we note a limitation to our analysis.
Because SafeGraph information is aggregated for privacy reasons,
we are not able to test coverage at the individual level. To avoid
a potential ecological fallacy, our results should be interpreted
as a statement about POIs rather than individuals. That is, POIs
frequented by older (or minority) visitors have lower coverage than
POIs frequented by younger (or whiter) populations. Of course,
policy decisions are typically made at some level of aggregation, so
the demographic bias we document at this level remains relevant
for those decisions.
A key future research question is how to use the results of this
audit to improve policy decisions. We suggest a few possible future directions. Our results can be used to inform a bias correction
approach that would for instance construct weights to adjust estimates based on race and age. Such an approach crucially requires
knowledge about the likely demographic composition, which may
be difficult for many policy settings. Another further avenue to explore is the methodology for “normalization factors.” SafeGraph, for
instance, suggests using census block group (CBG)-based normalization factors, essentially using known devices in a CBG against the
CBG census population [44]. While this bias correction might help
to estimate population parameters (e.g., percentage of CBG population not abiding by social distancing), it is unlikely to capture the
kind of demographic interaction effects we document here. Much
more work should be done to study disparate coverage and ideally
provide e.g. a weighing correction to the normalization factors that
properly accounts for the demographic disparities documented in
this audit.
Another possible solution is increased transparency. Researchers
do not know details about the source of SafeGraph’s mobility data,
namely which mobile apps feed into the SafeGraph ecosystem.
Access to such information may make the bias correction approach

ACKNOWLEDGMENTS
We thank SafeGraph for making their data available, answering our
many questions, and providing helpful feedback. We are grateful
to Stanford’s Institute for Human-Centered Artificial Intelligence,
the Stanford RISE initiative, the K&L Gates Presidential Fellowship,
and the National Science Foundation for supporting this research.
This material is based upon work supported by the the National
Science Foundation Graduate Research Fellowship Program under
Grant No. DGE1745016. Any opinions, findings, and conclusions
or recommendations expressed in this material are those of the
author(s) and do not necessarily reflect the views of the National
Science Foundation. We thank Mark Krass for first suggesting voter
turnout data and thank Angie Peng for providing helpful feedback.

10

Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy

REFERENCES
[1] Hunt Allcott, Levi Boxell, Jacob Conway, Matthew Gentzkow, Michael Thaler, and
David Y Yang. 2020. Polarization and public health: Partisan differences in social
distancing during the Coronavirus pandemic. Working Paper w26946. National
Bureau of Economic Research (NBER).
[2] Kristen M Altenburger, Daniel E Ho, et al. 2018. When Algorithms Import
Private Bias into Public Enforcement: The Promise and Limitations of Statistical
Debiasing Solutions. Journal of Institutional and Theoretical Economics 174, 1
(2018), 98–122.
[3] Ionut Andone, Konrad Błaszkiewicz, Mark Eibes, Boris Trendafilov, Christian
Montag, and Alexander Markowetz. 2016. How Age and Gender Affect Smartphone Usage. In UbiComp ’16. Association for Computing Machinery, New York,
NY, USA, 9–12. https://doi.org/10.1145/2968219.2971451
[4] Seth G. Benzell, Avinash Collis, and Christos Nicolaides. 2020. Rationing
social contact during the COVID-19 pandemic: Transmission risk and social benefits of US locations. Proceedings of the National Academy of Sciences 117, 26 (2020), 14642–14644. https://doi.org/10.1073/pnas.2008025117
arXiv:https://www.pnas.org/content/117/26/14642.full.pdf
[5] Guillermo Bernal and María R Scharró-del Río. 2001. Are empirically supported
treatments valid for ethnic minorities? Toward an alternative approach for treatment research. Cultural Diversity and Ethnic Minority Psychology 7, 4 (2001),
328.
[6] Krishna K Bommakanti, Laramie L Smith, Lin Liu, Diana Do, Jazmine CuevasMota, Kelly Collins, Fatima Munoz, Timothy C Rodwell, and Richard S Garfein.
2020. Requiring smartphone ownership for mHealth interventions: who could
be left out? BMC public health 20, 1 (2020), 81.
[7] Adam Brzezinski, Valentin Kecht, and David Van Dijcke. 2020. The Cost of Staying
Open: Voluntary Social Distancing and Lockdowns in the US. Technical Report.
Oxford University.
[8] Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness,
Accountability, and Transparency (Proceedings of Machine Learning Research),
Sorelle A. Friedler and Christo Wilson (Eds.), Vol. 81. PMLR, New York, NY, USA,
77–91. http://proceedings.mlr.press/v81/buolamwini18a.html
[9] Ángel Alexander Cabrera, Will Epperson, Fred Hohman, Minsuk Kahng, Jamie
Morgenstern, and Duen Horng Chau. 2019. FairVis: Visual analytics for discovering intersectional bias in machine learning. In 2019 IEEE Conference on Visual
Analytics Science and Technology (VAST). IEEE, Virtual, 46–56.
[10] Serina Y Chang, Emma Pierson, Pang Wei Koh, Jaline Gerardin, Beth Redbird,
David Grusky, and Jure Leskovec. 2020. Mobility network modeling explains
higher SARS-CoV-2 infection rates among disadvantaged groups and informs
reopening strategies.
[11] Alexandra Chouldechova, Diana Benavides-Prado, Oleksandr Fialko, and Rhema
Vaithianathan. 2018. A case study of algorithm-assisted decision making in child
maltreatment hotline screening decisions. In Conference on Fairness, Accountability and Transparency (Proceedings of Machine Learning Research), Sorelle A.
Friedler and Christo Wilson (Eds.), Vol. 81. PMLR, New York, NY, USA, 134–148.
http://proceedings.mlr.press/v81/chouldechova18a.html
[12] Alexandra Chouldechova and Aaron Roth. 2018. The Frontiers of Fairness in
Machine Learning. arXiv:arXiv:1810.08810
[13] Sam Corbett-Davies and Sharad Goel. 2018. The Measure and Mismeasure of
Fairness: A Critical Review of Fair Machine Learning. arXiv:arXiv:1808.00023
[14] Kimberlé Crenshaw. 1989. Demarginalizing the intersection of race and sex:
A black feminist critique of antidiscrimination doctrine, feminist theory and
antiracist politics. University of Chicago Legal Forum 1, 8 (1989), 139. Issue 1.
[15] Dhaval M Dave, Andrew I Friedson, Kyutaro Matsuzawa, Drew McNichols, Connor Redpath, and Joseph J Sabia. 2020. Did President Trump’s Tulsa Rally Reignite
COVID-19? Indoor Events and Offsetting Community Effects. Technical Report.
National Bureau of Economic Research.
[16] Dhaval M Dave, Andrew I Friedson, Drew McNichols, and Joseph J Sabia. 2020.
The Contagion Externality of a Superspreading Event: The Sturgis Motorcycle Rally
and COVID-19. Technical Report. National Bureau of Economic Research.
[17] Rebecca Dresser. 1992. Wanted single, white male for medical research. The
Hastings Center Report 22, 1 (1992), 24–29.
[18] David Dutwin, Scott Keeter, and Courtney Kennedy. 2010. Bias from wireless
substitution in surveys of Hispanics. Hispanic journal of behavioral sciences 32, 2
(2010), 309–328.
[19] Philip Mielke Eva Pereira, Bryan Bonack and Chelsea Lawson. 2020. Using Data
to Govern Through a Crisis. https://www.safegraph.com/webinar-governthrough-a-crisis
[20] Maryam Farboodi, Gregor Jarosch, and Robert Shimer. 2020. Internal and external
effects of social distancing in a pandemic. Technical Report. National Bureau of
Economic Research.
[21] Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam
Choudhary, Evan P. Hamilton, and Derek Roth. 2019. A Comparative Study
of Fairness-Enhancing Interventions in Machine Learning. In Proceedings of
the Conference on Fairness, Accountability, and Transparency. Association for

[22]

[23]

[24]

[25]
[26]
[27]

[28]
[29]

[30]

[31]

[32]

[33]

[34]
[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]
[43]
[44]

[45]

11

Computing Machinery, New York, NY, USA, 329–338. https://doi.org/10.1145/
3287560.3287589
Song Gao, Jinmeng Rao, Yuhao Kang, Yunlei Liang, and Jake Kruse. 2020. Mapping
county-level mobility pattern changes in the United States in response to COVID19. SIGSPATIAL Special 12, 1 (2020), 16–26.
Kyra H Grantz, Hannah R Meredith, Derek AT Cummings, C Jessica E Metcalf,
Bryan T Grenfell, John R Giles, Shruti Mehta, Sunil Solomon, Alain Labrique,
Nishant Kishore, et al. 2020. The use of mobile phone data to inform analysis of
COVID-19 pandemic epidemiology. Nature Communications 11, 1 (2020), 1–8.
Darrell M Gray, Adjoa Anyane-Yeboa, Sophie Balzora, Rachel B Issaka, and
Folasade P May. 2020. COVID-19 and the other pandemic: populations made
vulnerable by systemic inequity. Nature Reviews Gastroenterology & Hepatology
17, 9 (2020), 520–522.
Moritz Hardt and Solon Barocas. 2017. Fairness in machine learning.
M Hlavac. 2018. Stargazer: Well-formatted regression and summary statistics
tables (R Package version 5.2)[Computer software].
Daniel E Ho and Kosuke Imai. 2006. Randomization inference with natural
experiments: An analysis of ballot effects in the 2003 California recall election.
Journal of the American statistical association 101, 475 (2006), 888–900.
Paul W Holland. 1986. Statistics and Causal Inference. J. Amer. Statist. Assoc. 81,
396 (1986), 945–960.
Mariea Grubbs Hoy and George Milne. 2010. Gender differences in privacy-related
measures for young adult Facebook users. Journal of Interactive Advertising 10, 2
(2010), 28–45.
Ruth Igielnik, Scott Keeter, Courtney Kennedy, and Bradley Spahn. 2018. Commercial voter files and the study of US politics. Technical Report. Pew Research Center. www.pewresearch.org/2018/02/15/commercial-voter-files-and-the-studyof-us-politics
Michael H. Keller Jennifer Valentino-DeVries, Natasha Singer and Aaron Krolik.
2018. Your Apps Know Where You Were Last Night, and They’re Not Keeping
It Secret. https://www.washingtonpost.com/nation/2020/06/01/americans-aredelaying-medical-care-its-devastating-health-care-providers/?arc404=true
Nathan Kallus, Xiaojie Mao, and Angela Zhou. 2020. Assessing Algorithmic
Fairness with Unobserved Protected Class Using Data Combination. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency
(FAT* ’20). Association for Computing Machinery, New York, NY, USA, 110.
https://doi.org/10.1145/3351095.3373154
Benjamin D. Killeen, Jie Ying Wu, Kinjal Shah, Anna Zapaishchykova, Philipp
Nikutta, Aniruddha Tamhane, Shreya Chakraborty, Jinchi Wei, Tiger Gao,
Mareike Thies, and Mathias Unberath. 2020. A County-level Dataset for Informing the United States’ Response to COVID-19. arXiv:arXiv:2004.00756
Pauline T Kim. 2017. Auditing algorithms for discrimination. University of
Pennsylvania Law Review Online 166 (2017), 189.
David Lazer, Ryan Kennedy, Gary King, and Alessandro Vespignani. 2014. The
parable of Google Flu: traps in big data analysis. Science 343, 6176 (2014), 1203–
1205.
Sunghee Lee, J Michael Brick, E Richard Brown, and David Grant. 2010. Growing
cell-phone population and noncoverage bias in traditional random digit dial
telephone health surveys. Health services research 45, 4 (2010), 1121–1139.
Amanda Moreland. 2020. Timing of State and Territorial COVID-19 Stay-at-Home
Orders and Changes in Population Movement—United States, March 1–May 31,
2020. MMWR. Morbidity and Mortality Weekly Report 69 (2020), 1198–1203.
Gina Moreno-John, Anthony Gachie, Candace M Fleming, Anna NapolesSpringer, Elizabeth Mutran, Spero M Manson, and Eliseo J Pérez-Stable. 2004.
Ethnic minority older adults participating in clinical research. Journal of Aging
and Health 16, 5_suppl (2004), 93S–123S.
Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019.
Dissecting racial bias in an algorithm used to manage the health of populations.
Science 366, 6464 (2019), 447–453.
Maria Petrova, Ruben Enikolopov, Georgy Egorov, and Alexey Makarin. 2020.
Divided We Stay Home: Social Distancing and Ethnic Diversity. Technical Report.
National Bureau of Economic Research.
Vickie L Shavers-Hornaday, Charles F Lynch, Leon F Burmeister, and James C
Torner. 1997. Why are African Americans under-represented in medical research
studies? Impediments to participation. Ethnicity & health 2, 1-2 (1997), 31–45.
Mobile Fact Sheet. 2019. Pew Research Center, Internet and Technology. June 12,
2019.
Jennifer L Skeem and Christopher T Lowenkamp. 2016. Risk, race, and recidivism:
Predictive bias and disparate impact. Criminology 54, 4 (2016), 680–712.
RF Squire. 2019.
Measuring and Correcting Sampling Bias in Safegraph Patterns for More Accurate Demographic Analysis.
https:
//www.safegraph.com/blog/measuring-and-correcting-sampling-bias-foraccurate-demographic-analysis/?utm_source=content&utm_medium=
referral&utm_campaign=colabnotebook&utm_content=panel_bias
Don Bambino Geno Tai, Aditya Shah, Chyke A Doubeni, Irene G Sia, and Mark L
Wieland. 2020. The Disproportionate Impact of COVID-19 on Racial and Ethnic
Minorities in the United States. Clinical Infectious Diseases 2020 (06 2020), 1–4.
https://doi.org/10.1093/cid/ciaa815

Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra Chouldechova, and Daniel E. Ho

[46] Laris Karklis Ted Mellnik and Andrew Ba Tran. 2020.
Americans
are delaying medical care, and it’s devastating health-care providers.
https://www.washingtonpost.com/nation/2020/06/01/americans-are-delayingmedical-care-its-devastating-health-care-providers/?arc404=true
[47] Sandra Millon Underwood. 2000. Minorities, women, and clinical cancer research:
the charge, promise, and challenge. Annals of Epidemiology 10, 8 (2000), S3–S12.
[48] Darshali A Vyas, Leo G Eisenstein, and David S Jones. 2020. Hidden in plain
sight—reconsidering the use of race correction in clinical algorithms.
[49] Amy Wesolowski, Caroline O Buckee, Kenth Engø-Monsen, and Charlotte Jessica Eland Metcalf. 2016. Connecting mobility to infectious diseases: the promise
and limits of mobile phone data. The Journal of infectious diseases 214, suppl_4
(2016), S414–S420.
[50] Amy Wesolowski, Nathan Eagle, Abdisalan M Noor, Robert W Snow, and Caroline O Buckee. 2012. Heterogeneous mobile phone ownership and usage patterns
in Kenya. PloS one 7, 4 (2012), e35319.
[51] Nathalie E Williams, Timothy A Thomas, Matthew Dunbar, Nathan Eagle, and
Adrian Dobra. 2015. Measures of human mobility using mobile phone records
enhanced with GIS data. PloS one 10, 7 (2015), e0133630.
[52] Fei Zhou, Ting Yu, Ronghui Du, Guohui Fan, Ying Liu, Zhibo Liu, Jie Xiang,
Yeming Wang, Bin Song, Xiaoying Gu, et al. 2020. Clinical course and risk factors
for mortality of adult inpatients with COVID-19 in Wuhan, China: a retrospective
cohort study. The lancet 395, 10229 (2020), 1054–1062.

12

Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy

APPENDIX
A DATA
A.1 Mobility Data

A.2

Race quartile (4 = largest % non−white)

Our mobility data comes from SafeGraph via its COVID-19 Data
Consortium. Specifically, we rely on the SafeGraph Patterns data,
which provides daily foot traffic estimates to individual POIs, and
the Core Places data, which contains basic location information for
POIs.

Election Data

Our election data comes from certified turnout results of the 2018
North Carolina general election, as collected by L2. For each registered voter, L2 provides demographic data, such as name, age,
ethnicity, and voting district/precinct, as well as their voter history.
We provide some additional descriptive information about the
data here. First, Figure 10 shows the correlation between age and
race across polling locations. This illustrates the importance of
jointly interpreting how coverage varies by age and race.

4

43

26

26

18

3

35

30

24

24

2

24

33

33

23

1

11

24

30

48

2

3

4

1

Age quartile (4 = oldest)
# Polls

20 30 40

4

38.5

15.8

17.3

9

3

38.1

24.2

18.9

15.6

2

22.5

27.1

24.5

17.5

1

7.2

16.8

22

26.6

2

3

4

Race quartile (4 = largest % non−white)

Proportion non−white

0.6

0.4

0.2

0.0
1

Age quartile (4 = oldest)

0.0

0.2

0.4

0.6

Proportion over age 65
# Voters (1K)

10

15

# Voters (1K)

20

30

Figure 11: Joint distribution polling locations and voters by
age quartiles (𝑥-axis) and race quartiles (𝑦-axis).

Figure 10: Non-white voters are more likely to be young
Second, the top panel of Figure 11 illustrates the density of locations by age quartile on the 𝑥-axis and race quartile on the 𝑦-axis.
The two modal polling locations are for locations with white elderly populations and non-white young populations. The bottom
panel displays average number of voters in these cells, showing that
young, high-minority cells represent a particularly large number
of voters.

A.3

10

20

city, state, and zip code, as well as the precinct associated with the
polling location.

B

DETAILS ON DATA CLEANING AND
MERGING

This study required that we merge the points-of-interest (POIs) as
defined by SafeGraph with the polling locations in North Carolina
in 2018. To do so, we used SafeGraph’s Match Service12 , which
takes in a POI dataset and using an undisclosed algorithm, matches
it with its list of all POIs, appending the unique SafeGraph ID for all

Poll Location Data

Our polling location and precinct data for North Carolina for Election Day 2018 was acquired from the North Carolina Secretary
of State. This dataset contains the street address for each polling
place, including location name, county, house number, street name,

12 https://docs.safegraph.com/docs/matching-service-overview

13

Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra Chouldechova, and Daniel E. Ho
12.5

matched POIs. The service utilizes a variety of basic information13
to determine matches; of these, we provided the location name (or
polling place name), street address, city, state, and postal code for
all polling locations in North Carolina in 2018. The match rate, i.e.
the percentage of input polling locations SafeGraph could match
with one of its POIs, was 77.6%.
The polling location dataset, now having SafeGraph IDs for
each matched location, was then joined with the SafeGraph Places
dataset, which contains basic information like location name and
address for the POI, for comparison between the matched POI and
the polling location. The SafeGraph matching algorithm was at
times too lenient, matching locations near each other but with different names or matching locations with different street addresses.
To remedy this, we ran the dataset through a postprocessing script
which removed matches where the addresses differed by three or
more words to account for false positives. This resulted in a match
rate of 47.7%. We then filtered out POIs where SafeGraph returned
multiple candidate matches since we could not be confident the
first match was the correct match. This resulted in a match rate of
42.2%.
Finally, we mapped voters from the L2 voter file to the appropriate polling location with SafeGraph ID. The L2 voter file contains the
precinct for each voter, and the polling location data associates each
precinct with a polling location, so by mapping voter to precinct
and precinct to polling location (and SafeGraph ID) we could fetch
the polling location for each voter for which there was a match
with a SafeGraph POI. We observed differences in how the polling
data and L2 named the same precinct. For instance, one source may
using preceding zeros "0003" whereas another may not "3" or one
source may use "WASHINGTON WARD 1" whereas the second uses
"WASHINGTON 1".

B.1

10.0

count

7.5

5.0

2.5

0.0
0.0

Election

0.4

Regular

Figure 12: Placebo distribution of rank correlation between
voters and marginal SafeGraph traffic. As expected, voter
turnout is positively correlated with SafeGraph marginal
traffic only on election day (empirical p-value = 0).
factor that affects both the estimates of ground truth visits 𝑇 and the
estimated marginal SafeGraph traffic 𝑆 − 𝑍 .
Assumption 4 (No selection bias (measurement validity)).
The selection is not based on an interaction between factors that affect
ground truth visits 𝑇 and the estimated marginal SafeGraph traffic
𝑆 − 𝑍.
As we do above for disparate coverage, we can partially test
Assumption 3 using placebo inference (see next section). While
we can test for time-invariant confounding, we cannot test for
time-varying confounding. Nonetheless it is difficult to postulate a
reasonable mechanism for time-varying confounding in our measurement analysis. Assumption 4 would be violated if SafeGraph
coverage is better for polling location POIs versus non-polling location POIs.

Challenges for scalability

While a key virtue of our approach is bringing in auxiliary ground
truth data, the drawback is that this approach is not conducive
to iterative audits over time (or geography) because of scalability
challenges. Voter locations change with every election and there is
no national database that collects voter location information over
time. Creating the crosswalks between (a) SafeGraph POIs and voter
locations and (b) voter locations and precincts in voter turnout files
is a heavily manual process that differs for each jurisdiction, given
the decentralized nature of election administration.

C

0.2

Rank correlation ρ(r(V),r(S−Z))

C.1

Robustness test for time-invariant
confounding

To test for time-invariant confounding in our estimation of the
correlation between ground truth visits and SafeGraph visits, we
consider the rank correlation between voter turnout and SafeGraph
marginal traffic on non-election days. We would not expect to find
a non-zero such correlation, and indeed Figure 12 shows that the
positive correlation on election day is significantly outside the
distribution for placebo days (empirical one-sided 𝑝-value = 0).

ASSUMPTIONS REQUIRED FOR
MEASUREMENT VALIDITY

In this section, we provide the analogue of § 4.3 and 6.2.1 for measurement validity. The assumptions required for our measurement
validity analysis are much weaker than those discussed and evaluated in the main paper, but we provide the results here for completeness. To identify the relationship between ground truth visits
and SafeGraph traffic, we need the following to hold:

D

IMPUTATION OF NON-VOTER TRAFFIC

To estimate voter traffic on election day, we estimate the amount
of traffic on non-election day Tuesdays using SafeGraph Monthly
Places Patterns data from January 2018 to April 2020 and subtract
that estimate from the total recorded traffic on election days. In
particular, to estimate the number of visits on a given non-election
day Tuesday, we use the number of visits on adjacent weekdays
to calculate that estimate. We took two approaches to making that

Assumption 3 (No induced confounding (measurement validity)). The estimation procedure does not induce a confounding
13 https://docs.safegraph.com/v4.0/docs/places-schema

14

Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy
0.03

calculation and selecting the optimal number of adjacent weekdays
to base the estimate off.
In the first approach, we look at the 𝑋 adjacent weekdays before
and after a given Tuesday and use the average of the traffic on
all those weekdays to estimate the traffic on Tuesday. That is, for
𝑋 = 2, we calculate the estimate as the average of the traffic on the
Friday and Monday before and Wednesday and Thursday after a
given Tuesday. We performed this calculation for all North Carolina
polling locations and all Tuesdays, excluding Election Days and
the first and last Tuesdays from January 2018 to April 2020, with
traffic data available from the SafeGraph Patterns data, which gave
us 147,613 data points. We tested 𝑋 ∈ [1, 4] as this considers all
weekdays up to the next or previous Tuesday. The following are
the evaluation metrics for this approach:
𝑋
1
2
3
4

RMSE
10.85
10.37
10.61
10.75

R2
0.870
0.881
0.875
0.874

Coverage

0.02

0.00

−0.01

MAE
4.05
3.91
3.95
4.00

0.0

# Voters (1K)

R2
0.872
0.888
0.890
0.893

MAE
4.16
3.89
3.83
3.76

0.6

10

15

20

25

0.020

0.015

The linear model using traffic from the four adjacent weekdays
before and after the given Tuesday performed the best across both
approaches, so we used this model to estimate non-voter traffic on
Election Day.
We used the model to predict the number of non-voter visits to
each of 806 polling location POIs on Election Day (November 6𝑡ℎ ,
2018). 10 of the POIs did not have Patterns visit count data for 4
weekdays before Election Day (October 31𝑠𝑡 , 2018), so we imputed
the traffic to be the traffic 3 weekdays before (November 1𝑠𝑡 , 2018)
for those POIs.
The model was also used to impute the traffic at poll location POIs
on the 48 weekdays between October 1𝑠𝑡 , 2018 and November 30𝑡ℎ .
These predictions were then used to repeat the data analyses on
SafeGraph coverage as an additional robustness check, producing
similar results to original analysis that relied on mean imputation
to adjust for non-voter traffic. Figures 13 illustrates comparable
results as before.

E

0.4

0.025

Coverage

RMSE
10.65
9.98
9.88
9.76

0.2

Proportion over age 65

Averaging the traffic on the two weekdays before and after a given
Tuesday performs best by all three evaluation metrics.
In the second approach, we used the traffic on adjacent weekdays as features for a linear regression model, to account for the
possibility that traffic on certain weekdays may be more impactful
in calculating an accurate estimate. With the same dataset as the
one described for the first approach we used 10-fold cross validation
with 3 repeats, with the following results:
𝑋
1
2
3
4

0.01

0.010
0.00

0.25

0.50

0.75

1.00

Proportion Non−White
# Voters (1K)

15

20

25

Figure 13: Estimated SafeGraph coverage rates against age
and race for North Carolina 2018 general election for ventiles of poll location by age (top) and race (bottom).
that is white is associated with an increase in the coverage rate.
The third column fits interaction terms, for which we present the
substantive interpretation in Section 6.2.

ADDITIONAL RESULTS

Table 4 presents the full regression results for Section 6.2. The first
column shows that the percent of the voting population over 65 is
negatively associated with coverage. The second column shows that
controlling for age, an increase in the percentage of the population
15

Amanda Coston, Neel Guha, Derek Ouyang, Lisa Lu, Alexandra Chouldechova, and Daniel E. Ho

Table 4

% over 65

(1)

(2)

(3)

−0.037∗∗∗

−0.044∗∗∗

(0.009)

(0.010)

−0.075∗∗
(0.033)

0.010∗∗∗
(0.004)

−0.0003
(0.012)

% white

% white × % over 65

Constant

Observations
R2
Adjusted R2
Residual Std. Error
F Statistic

0.042
(0.043)
0.031∗∗∗
(0.003)

0.025∗∗∗
(0.003)

0.033∗∗∗
(0.009)

452
0.034
0.031
0.018 (df = 450)
15.668∗∗∗ (df = 1; 450)

452
0.049
0.045
0.018 (df = 449)
11.639∗∗∗ (df = 2; 449)

452
0.051
0.045
0.018 (df = 448)
8.073∗∗∗ (df = 3; 448)

∗ p<0.1; ∗∗ p<0.05; ∗∗∗ p<0.01
Note:
Table 5: Linear regression models of coverage rate by demographic attributes of polling locations.

16

