CMT in TREC-COVID Round 2: Mitigating the Generalization
Gaps from Web to Special Domain Search
♠

Chenyan Xiong ∗ , Zhenghao Liu♥∗ , Si Sun♥∗ , Zhuyun Dai♣∗ , Kaitao Zhang♥∗ , Shi Yu♥ *
Zhiyuan Liu♥ , Hoifung Poon♠ , Jianfeng Gao♠ , Paul Bennett♠

arXiv:2011.01580v1 [cs.IR] 3 Nov 2020

Tsinghua University♥ , Microsoft Research♠ , Carnegie Mellon University♣
{liu-zh16, s-sun17, zkt18, yus17}@mails.tsinghua.edu.cn;
liuzy@tsinghua.edu.cn; zhuyund@cs.cmu.edu;
{chenyan.xiong, hoifung, jfgao, pauben}@microsoft.com

ABSTRACT
Neural rankers based on deep pretrained language models (LMs)
have been shown to improve many information retrieval benchmarks.
However, these methods are affected by their the correlation between pretraining domain and target domain and rely on massive
fine-tuning relevance labels. Directly applying pretraining methods to specific domains may result in suboptimal search quality
because specific domains may have domain adaption problems, such
as the COVID domain. This paper presents a search system to alleviate the special domain adaption problem. The system utilizes
the domain-adaptive pretraining and few-shot learning technologies
to help neural rankers mitigate the domain discrepancy and label
scarcity problems. Besides, we also integrate dense retrieval to alleviate traditional sparse retrieval’s vocabulary mismatch obstacle. Our
system performs the best among the non-manual runs in Round 2 of
the TREC-COVID task, which aims to retrieve useful information
from scientific literature related to COVID-19. Our code is publicly
available at https://github.com/thunlp/OpenMatch.

KEYWORDS
TREC-COVID, Domain Discrepancy, Label Scarcity, Vocabulary
Mismatch, Dense Retrieval

1

INTRODUCTION

Recent years have witnessed continuous successes of neural ranking
models in information retrieval [6, 17, 19, 25]. Most notably, deep
pretrained language models (LMs) achieve state-of-the-art performance on several web search benchmarks [4, 18, 27]. Their success
relies on the learned semantic information from general domain
corpus with the language model pretraining [4, 28].
However, ranking models in specific domains usually face the
domain adaption problem, which comes from two generalization
gaps between the general and the specific domain. The first gap
derives from the discrepancy of vocabulary distributions in different
domains. Taking the COVID domain as an example [23, 24], the earliest related publication appeared at the end of 2019. Even pretrained
LMs targeting the biomedical domain [2, 13] are unfamiliar with
new medical terms like COVID-19 because their pretraining corpora
have not contained such new terminologies. The other gap is the label scarcity. For the specific searching scenario, large-scale relevance
labels are luxury, such as biomedical and scientific domains.
*

indicates equal contribution.

In addition, most information retrieval (IR) systems usually use
sparse ranking methods in the first-stage retrieval, such as BM25,
which are based on term-matching signals to calculate the relevance
between query and document. Nevertheless, these systems may fail
when queries and documents use different terms to describe the same
meaning, which is known as the vocabulary mismatch problem [5, 8].
The vocabulary mismatch problem of sparse retrieval has become
an obstacle to existing IR systems, especially for specific domains
that have lots of in-domain terminologies.
This paper presents a solution to alleviate the specific domain
adaption problem with three core technics. The first one conducts
domain-adaptive pretraining (DAPT) [10] to help pretrained language models learn semantics of special domain terminologies to
keep the language knowledge is the latest. The second one uses Contrast Query Generation (ContrastQG) and ReInfoSelect [29] to mitigate the label scarcity problem in the specific domain. ContrastQG
and ReInfoSelect focus on generating and filtering pseudo relevance
labels to further improve ranking performance, respectively. Finally,
our system integrates dense retrieval to alleviate the sparse retrieval’s
vocabulary mismatch bottleneck. Dense retrieval can encode query
and document to dense vectors to measure the relevance between
query and document in the latent semantic space [3, 9, 12, 14, 25].
Using above technologies, our system achieves the best performance among non-manual groups in Round 2 of TREC-COVID [23],
which is a COVID-domain TREC task to evaluate information retrieval systems for searching COVID-19 related literature.
The next section will analyze the generalization gaps and vocabulary mismatch faced by the COVID domain search. Sec.3 and Sec.4
describe in detail how our system alleviates these problems. Sec.5
shows the evaluation results and hyperparameter study. In the Sec.6
and Sec.7, we discuss the negative attempts and our concerns of the
residual collection evaluation [21] used in TREC-COVID.

2

DATA STUDY

This section studies the generalization gaps from web to COVID
domain, and the vocabulary mismatch problem of sparse retrieval.
Domain Discrepancy. Most existing pretrained language models
divide uncommon words into subwords, which aims to alleviate
the out-of-vocabulary problem [22]. As shown in Figure 1, the subword ratio of TREC-COVID queries is dramatically higher than
that of the web domain dataset, MS MARCO [1]. The results show
that existing pretrained language models treat most COVID-domain

Subword Ratio

80%
70%
TREC-COVID
MS MARCO
60%
50%
40%
30%
20%
10%
0% Biomedical + General General
Biomedical + CS
(BioBERT)
(BERT)
(SciBERT)
Pretraining Corpus Domain

the specificity between two documents to generate more meaningful
queries instead of keyword-style queries.
The entire synthesis process uses two query generators named
𝑄𝐺 and 𝐶𝑜𝑛𝑡𝑟𝑎𝑠𝑡𝑄𝐺, which aim to generate pseudo queries according to documents. Both 𝑄𝐺 and 𝐶𝑜𝑛𝑡𝑟𝑎𝑠𝑡𝑄𝐺 are implemented with
standard GPT-2 [20]. 𝑄𝐺 is trained on medical MS MARCO’s positive passage-query pairs (𝑑 + , 𝑞) following the previous method [15].
𝐶𝑜𝑛𝑡𝑟𝑎𝑠𝑡𝑄𝐺 is directly trained on medical MS MARCO’s triples
by encoding the concatenated text of positive and negative passages
(𝑑 + , 𝑑 − ) to generate query 𝑞.
At inference time, we first leverage 𝑄𝐺 to generate queries 𝑞
based on a single COVID domain document 𝑑:

Figure 1: The proportion of query words that are decomposed
into subwords by the pretrained language model’s vocabulary.

𝑞 = 𝑄𝐺 (𝑑).
Then we utilize BM25 to retrieve two related documents (𝑑 +′ , 𝑑 −′ )
that show different correlation according to the generated query 𝑞.
Finally, 𝐶𝑜𝑛𝑡𝑟𝑎𝑠𝑡𝑄𝐺 is used to generate another query 𝑞 ′ based on
the two contrastive documents (𝑑 +′ , 𝑑 −′ ):

terminologies as unfamiliar words, indicating a considerable discrepancy between the existing pretraining and the COVID domain.
Label Scarcity. The label scarcity in the COVID domain search
is very prominent. Only 30 queries were judged in the second round
of TREC-COVID. In contrast, medical MS MARCO contains more
than 78,800 annotated queries, which is the medical subset of MS
MARCO filtered by the previous work [16].
Vocabulary Mismatch. We observed that BM25 only covered
35% of relevant documents in the top 100 retrieved documents. The
result reveals that retrieving relevant documents only according to
term-matching signals will hinder the search system’s effectiveness.

3

𝑞 ′ = 𝐶𝑜𝑛𝑡𝑟𝑎𝑠𝑡𝑄𝐺 (𝑑 +′ , 𝑑 −′ ).
The synthetic triple (𝑞 ′, 𝑑 +′ , 𝑑 −′ ) is used as weakly supervised data to
train the neural ranker.
ReInfoSelect [29] uses reinforcement learning to select weak
supervision data. ReInfoSelect evaluates the neural ranker’s performance on the target data and regards the NDCG difference as the
reward. Then the reward signal from target data is propagated to
guide data selector via the policy gradient.
In our system, we use ContrastQG and medical MARCO to construct the weakly supervised data. The annotated data of TRECCOVID Round 1 is used as the target data. The trial-and-error learning mechanism of ReInfoSelect can select proper weakly supervised
data according to neural ranker’s performance in the target domain,
which helps to further mitigate the domain discrepancy.

SYSTEM DESCRIPTION

Our system employs a two-stage retrieval architecture, which utilizes
BM25 for base retrieval and SciBERT [2] for reranking. The domainadaptive pretraining and two few-shot learning techniques are used
to mitigate the generalization gaps faced by SciBERT in the COVID
domain. Dense retrieval is also incorporated into our system to
alleviate BM25’s vocabulary mismatch problem.

3.1

3.3

Domain-Adaptive Pretraining

SciBERT has been used in our system since it is pretrained with
scientific texts and biomedical publications. However, COVID is a
new concept that has not appeared in previous pretraining corpora.
Therefore, we conduct domain-adaptive pretraining (DAPT) [10]
for SciBERT. Our approach is straightforward to continuously train
SciBERT with CORD-19 corpus [24], which is a growing collection
of scientific papers about COVID-19 and coronavirus.

3.2

Dense Retrieval

Dense retrieval maps queries and documents to the same distributed
representation space and retrieves related documents based on the
similarities between document vectors and query vectors [12, 25].
Let each training instance contain a query 𝑞, relevant (positive)
𝑗
document 𝑑 + and 𝑚 irrelevant (negative) documents 𝐷 − = {𝑑 − }𝑚
𝑗=1 .
Dense retrieval first encodes the query 𝑞 and all documents 𝑑 to
dense vectors 𝒒 and 𝒅. Then the similarity of 𝒒 and 𝒅 is calculated
as 𝑠𝑖𝑚(𝒒, 𝒅). The training objective can be formulated as learning
a distributed representation space that the positive document has a
higher similarity to the query than all negative documents:

Few-Shot Learning

𝑙𝑜𝑠𝑠 (𝑞, 𝑑 +, 𝐷 − ) = −log

We introduce two few/zero-shot learning methods named ContrastQG
and ReInfoSelect [29] to alleviate the label scarcity challenge when
fine-tuning the neural ranking model. Specifically, we first use ContrastQG to generate weakly supervised data in a zero-shot manner
and then utilize a weak supervision data selection method, ReInfoSelect, to recognize high quality training data.
ContrastQG is a zero-shot data synthetic method aiming to generate queries for synthesizing weakly supervised relevance signals.
Unlike the prior work [15], ContrastQG synthesizes a query given a
relevant text pair rather than a single related text, which can capture

𝑒 𝑠𝑖𝑚 (𝒒,𝒅+ )
,
𝒋
Í
𝑠𝑖𝑚 (𝒒,𝒅 − )
𝑒 𝑠𝑖𝑚 (𝒒,𝒅+ ) + 𝑚
𝑗=1 𝑒

where the similarity 𝑠𝑖𝑚(·, ·) is the dot product between vectors.

4

IMPLEMENTATION DETAILS.

In this section, we describe the system’s implementation details.
Dataset. The testing data of TREC-COVID Round 2 contains the
May 1, 2020 version of the CORD-19 document set [24] (59,851
COVID-related papers) and 35 queries written by biomedical professionals. Among these queries, the first 30 queries have been judged
2

Table 1: Overall accuracy in Round 2 of TREC-COVID. The testing results of baselines and our three submitted runs (marked with
asterisk∗ ) are from official evaluations. Compared baselines are BM25 Fusion (base retrieval), T5 Fusion as well as SciBERT Fusion.
Run ID

Method

r2.fusion2
covidex.t5
GUIR S2 run1
SparseDenseSciBert
ReInfoSelect
n.a.
ContrastNLGSciBert
n.a.
n.a.

BM25 Fusion
T5 Fusion
SciBERT Fusion
SciBERT + DAPT + DenseRetrieval∗
SciBERT + DAPT + ContrastQG + ReInfoSelect∗
SciBERT + DAPT + ReInfoSelect
SciBERT + DAPT + ContrastQG∗
SciBERT + DAPT
SciBERT

Table 2: Dev results of SciBERT with different reranking depth
in Round 2 of TREC-COVID. The top 10 hole rate denotes the
unlabeled proportion of the top 10 reranked results.
Rerank Depth
20
50
100
500
1000

NDCG@10
0.6545
0.6853
0.6838
0.6044
0.5826

P@5
0.7429
0.7714
0.7543
0.6971
0.6686

Top 10 Hole Rate
0.03
0.07
0.12
0.23
0.26

Hyperparameter Study

Among all hyperparameters, we found the reranking depth significantly impacts the neural ranking model’s effectiveness. As shown
in Table 2, SciBERT’s performance is significantly limited at the
shallow reranking depth (≤20), mainly caused by the low ranking
accuracy of BM25. With the increase the reranking depth to 50 and
100, the neural ranker shows stable performance and achieves the
best. Nevertheless, the reranking accuracy begins to drop as the
depth continues to increase. The possible reason is that the neural
ranker is not good enough to distinguish truly relevant documents
when more noisy documents are included.

in the Round 1. In the experiment, we use TREC-COVID Round 1’s
annotated data as the development set (30 queries) and the medical
MS MARCO [16] as the training data (78,895 queries).
System Setup. For data preprocessing, we concatenated title and
abstract to represent each document and deleted stop words for all
queries. Our system utilized the BM25 constructed by Anserini [26]
as the base retrieval and adopted the dense retrieval implementation
provided by Gao, et al. [9]. The neural ranker based on SciBERT [2]
was used in dense retrieval and reranking stages [16] with the learning rate of 2e-5 and the batch size of 32. We set the warm-up proportion as 0.1 and limited the maximum sequence length to 256.
The NDCG@10 score on the development set is used to measure
the convergence and is calculated every three training steps. Our
system is based on PyTorch, and the training process it involves can
be implemented on a GeForce RTX 2080 Ti.

5.3

Query Analysis

Figure 2 shows the testing results of each query. The first 30 queries
have been judged in Round 1, and others are newly added in Round
2 (query 31-35). Our system outperforms baselines on most queries
with previous annotations. Besides, our system is also comparable
to the T5 Fusion system on new queries and avoids the sharp drop
of the SciBERT Fusion system (such as 34th query), which shows
our system’s robustness.

EVALUATION RESULTS

This section presents evaluation results and hyperparameter studies.

5.1

R2 (test)
NDCG@10 P@5
0.5553
0.6800
0.6250
0.7314
0.6251
0.7486
0.6772
0.7600
0.6259
0.6971
0.6210
0.6914
0.6138
0.7314
0.5880
0.6800
0.5828
0.6629

terminologies is crucial for language models. Then the system’s performance has been further improved with about 6.5% NDCG@10
gains by ContrastQG and ReInfoSelect. ContrastQG generates lots of
pseudo relevance labels, which provides more training guidance for
neural rankers in the specific domain. ReInfoSelect further boosts
models with more fine-grained selected supervisions. The most
significant improvement comes from the fusion of dense retrieval,
where the P@5 score is increased by 11.8%. This result shows that
dense retrieval can significantly improve retrieval effectiveness by
alleviating sparse retrieval’s vocabulary mismatch problem.

5.2

5

R1 (dev)
NDCG@10 P@5
0.6056
0.7200
0.5124
0.6333
0.6032
0.6867
0.7424
0.8933
0.7134
0.8333
0.7061
0.8000
0.6830
0.8467
0.6775
0.7400
0.6598
0.7733

Overall Results
6

Table 1 shows the overall performance of different models in the
TREC-COVID task. Three top systems during Round 2 evaluation
and several variants of our systems are compared.
Our system achieved the best performance in Round 2 of TRECCOVID. From our detailed experimental results, our method significant improves the ranking performance of SciBERT in the COVID
domain. The domain-adaptive pretraining (DAPT) helps to improve
SciBERT, which illustrates that learning the semantics of these new

FAILED ATTEMPTS

This section discusses some of our failed attempts and experience.
Manual Labeling. A straightforward approach to mitigate the label scarcity is to annotate more data within this domain manually. We
recruited three medical students who compiled 50 COVID-related
queries and assigned the relevance label to the top 20 documents
retrieved by BM25 for each query. However, our annotations were
not able to get good agreement with TREC-COVID’s annotations.
3

NDCG@10

T5 Fusion

SciBERT Fusion

Our System

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35
Query ID

NDCG@10 Gain

Figure 2: Round 2 testing result on each query of baselines and our system’s best version (SciBERT + DAPT + Dense Retrieval). The
X-axis denotes the Query ID in Round 2 of TREC-COVID, and Y-axis represents the NDCG@10 score. Noted that Queries 1-30 have
been annotated in Round 1, and Queries 31-35 are newly added in Round 2.

+0.4
All
Old
New
+0.3
+0.2
+0.1
0.0
-0.1
-0.2
-0.3 Ours @2 @3 @4 @5 @6 @7 @8 @9 @10
Top@10 Feedback System

that BERT-Large has no obvious advantage over SciBERT-Base and
Conv-KNRM performs the worst. The main reason for the poor
performance of Conv-KNRM is that we did not use its subword
version [11], which led to a severe out-of-vocabulary problem.
Fusion Attempts. Two fusion methods have been tried to integrate dense retrieval into our system. One approach is to combine
dense retrieval with BM25 in the base retrieval stage. The other is to
fuse dense retrieval into SciBERT’s reranking processing directly.
The second method works better in our limited attempts.

7

CONCERNS ON RESIDUAL EVALUATION

This section discusses our observations about the residual collection
evaluation used in the TREC-COVID task. In residual collection
evaluation, test queries can be divided into old queries and new
queries. The old queries have been annotated in previous rounds,
but their annotated documents will be removed from the collection
before scoring. TREC-COVID allows IR systems to use old queries’
relevance judgments and classify such systems as feedback types.
Figure 3 shows the evaluation results of the top 10 feedback
systems in Round 2 of TREC-COVID. Although these systems performed closely in overall scores, they showed significant differences
in the old and new queries. E.g., the 2nd system’s performance in
the new query is greatly better than that in the old query. In contrast,
some systems’ ranking accuracy for the new query is considerably
lower than in the old query, even worse than the base retrieval BM25
Fusion system, such as the 3rd-5th and 9th systems.
A powerful search system is desirable to achieve balanced performance on known and unknown queries. However, this result shows
that the residual collection evaluation may bias towards seen queries,
which are much easier in real production scenarios.

Figure 3: The NDCG@10 gain of the top 10 feedback systems
relative to BM25 Fusion system. ‘All’ represents the average
gain of all queries in TREC-COVID Round 2, ‘Old’ and ‘New’
mean the annotated queries in Round 1 and the newly added
queries in Round 2, respectively.

Corpus Filtering. MacAvaney et al. [16] proposed to narrow the
retrieval scale by filtering out the document published before 2020.
Nevertheless, our analysis found that this method excluded more
than 80% of documents from the second round of corpus, dropping
a large amount of useful COVID-related literature, such as SARS
and MERS. Thus, we did not adopt this method in our system.
Neural Reranker. We also attempted two other neural ranking models besides SciBERT for document reranking, including
BERT [7] and Conv-KNRM [6]. Our experimental results show
4

Acknowledgments. We thank Luyu Gao for sharing the implementation of Dense Retrieval, the Track organizers for hosting this
track, Sean Macavaney for releasing the medical MS MARCO filter, and Jimmy Lin & the Anserini project for open sourcing the
well-rounded BM25 first stage retrieval.

(2020), 1234–1240.
[14] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2020.
Sparse, Dense, and Attentional Representations for Text Retrieval. arXiv preprint
arXiv:2005.00181 (2020).
[15] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2020. Zeroshot Neural Retrieval via Domain-targeted Synthetic Query Generation. arXiv
preprint arXiv:2004.14503 (2020).
[16] Sean MacAvaney, Arman Cohan, and Nazli Goharian. 2020. SLEDGE: A Simple Yet Effective Baseline for Coronavirus Scientific Knowledge Search. arXiv
preprint arXiv:2005.02365 (2020).
[17] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR:
Contextualized embeddings for document ranking. In Proceedings of SIGIR. 1101–
1104.
[18] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.
arXiv preprint arXiv:1901.04085 (2019).
[19] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, and Xueqi Cheng.
2017. Deeprank: A new deep architecture for relevance ranking in information
retrieval. In Proceedings of CIKM. 257–266.
[20] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language models are unsupervised multitask learners. (2019).
[21] Gerard Salton and Chris Buckley. 1990. Improving retrieval performance by
relevance feedback. Journal of the American society for information science 41, 4
(1990), 288–297.
[22] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine
translation of rare words with subword units. arXiv preprint arXiv:1508.07909
(2015).
[23] Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman,
William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2020.
TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection.
arXiv preprint arXiv:2005.04474 (2020).
[24] Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang
Yang, Darrin Eide, Kathryn Funk, Rodney Kinney, Ziyang Liu, William Merrill,
et al. 2020. CORD-19: The Covid-19 Open Research Dataset. arXiv preprint
arXiv:2004.10706 (2020).
[25] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. arXiv preprint
arXiv:2007.00808 (2020).
[26] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the use of
Lucene for information retrieval research. In Proceedings of SIGIR. 1253–1256.
[27] Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Simple applications of BERT
for ad hoc document retrieval. arXiv preprint arXiv:1903.10972 (2019).
[28] Hongfei Zhang, Xia Song, Chenyan Xiong, Corby Rosset, Paul N Bennett, Nick
Craswell, and Saurabh Tiwary. 2019. Generic intent representation in web search.
In Proceedings of SIGIR. 65–74.
[29] Kaitao Zhang, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2020. Selective
Weak Supervision for Neural Information Retrieval. In Proceedings of WWW.
474–485.

REFERENCES
[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong
Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.
2016. Ms marco: A human generated machine reading comprehension dataset.
arXiv preprint arXiv:1611.09268 (2016).
[2] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language
Model for Scientific Text. In Proceedings of EMNLP-IJCNLP. 3606–3611.
[3] Wei-Cheng Chang, Felix X Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar.
2020. Pre-training tasks for embedding-based large-scale retrieval. arXiv preprint
arXiv:2002.03932 (2020).
[4] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M
Voorhees. 2020. Overview of the trec 2019 deep learning track. arXiv preprint
arXiv:2003.07820 (2020).
[5] W Bruce Croft, Donald Metzler, and Trevor Strohman. 2010. Search engines:
Information retrieval in practice. Vol. 520. Addison-Wesley Reading.
[6] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional
neural networks for soft-matching n-grams in ad-hoc search. In Proceedings of
WSDM. 126–134.
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of NAACL. 4171–4186.
[8] George W. Furnas, Thomas K. Landauer, Louis M. Gomez, and Susan T. Dumais.
1987. The vocabulary problem in human-system communication. Commun. ACM
30, 11 (1987), 964–971.
[9] Luyu Gao, Zhuyun Dai, Zhen Fan, and Jamie Callan. 2020. Complementing Lexical Retrieval with Semantic Residual Embedding. arXiv preprint
arXiv:2004.13969 (2020).
[10] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,
Doug Downey, and Noah A Smith. 2020. Don’t Stop Pretraining: Adapt Language
Models to Domains and Tasks. arXiv preprint arXiv:2004.10964 (2020).
[11] Sebastian Hofstätter, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury. 2019.
On the effect of low-frequency terms on neural-IR models. In Proceedings of
SIGIR. 1137–1140.
[12] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question
Answering. arXiv preprint arXiv:2004.04906 (2020).
[13] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim,
Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, 4

5

