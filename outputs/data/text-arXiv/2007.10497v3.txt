1

CovidDeep: SARS-CoV-2/COVID-19 Test Based
on Wearable Medical Sensors and Efficient
Neural Networks

arXiv:2007.10497v3 [cs.HC] 28 Oct 2020

Shayan Hassantabar, Novati Stefano, Vishweshwar Ghanakota, Alessandra Ferrari, Gregory N. Nicola,
Raffaele Bruno, Ignazio R. Marino, Kenza Hamidouche, and Niraj K. Jha, Fellow, IEEE
Abstract—The novel coronavirus (SARS-CoV-2) has led to a pandemic. Because of its highly contagious nature, it has spread rapidly,
resulting in major disruption to public health and a huge loss of human life. In addition, due to governmental orders for isolation and social
distancing, it has also had a severe negative impact on the world economy. As a result, it is widely recognized that widespread testing is
key to containing the spread of the disease and opening up the economy. However, the current testing regime based on Reverse
Transcription-Polymerase Chain Reaction for SARS-CoV-2 has been unable to keep up with testing demands and also suffers from a
relatively low positive detection rate in the early stages of the resultant disease, called COVID-19. Hence, there is a need for an alternative
approach for repeated large-scale testing of SARS-CoV-2/COVID-19. The emergence of wearable medical sensors (WMSs) and deep
neural networks (DNNs) points to a promising approach to address this challenge. WMSs enable continuous and user-transparent
monitoring of physiological signals. However, disease detection based on WMSs/DNNs and their deployment on resource-constrained
edge devices remain challenging problems. To address these problems, we propose a framework called CovidDeep that combines
efficient DNNs with commercially available WMSs for pervasive testing of the virus in both the asymptomatic and symptomatic cases.
CovidDeep does not depend on manual feature extraction. It directly operates on WMS data and some easy-to-answer questions in a
questionnaire whose answers can be obtained through a smartphone application. We collected data from 87 individuals, spanning three
cohorts that include healthy, asymptomatic (but SARS-CoV-2-positive) as well as symptomatic COVID-19 patients. We trained DNNs on
various subsets of the features automatically extracted from six WMS and questionnaire categories to perform ablation studies to
determine which subsets are most efficacious in terms of test accuracy for a three-way classification. The highest test accuracy we
obtained was 98.1%. Since data collection was limited to only 87 individuals (because of the intensive nature of data collection), we also
experimented with augmenting the real training dataset with a synthetic training dataset drawn from the same probability distribution. We
used the synthetic dataset to impose a prior on the DNN weights. Furthermore, we leveraged a grow-and-prune DNN synthesis paradigm
to simultaneously learn both the weights and the network architecture. Addition of synthetic data and use of grow-and-prune synthesis
boosted the accuracy of the various DNNs further and simultaneously reduced their size and floating-point operations. This makes the
CovidDeep DNNs both accurate and efficient, in terms of memory requirements and computations. The resultant DNNs can be easily
deployed on edge devices, e.g., smartwatch or smartphone, which has the added benefit of preserving patient privacy.
Index Terms—COVID-19 test; grow-and-prune synthesis; neural networks; SARS-CoV-2; synthetic data generation; wearable medical
sensors.

F

1

I NTRODUCTION

S

ARS-C O V-2, also known as novel coronavirus, emerged
in China and soon after spread across the globe. The
World Health Organization (WHO) named the resultant
disease COVID-19. COVID-19 was declared a pandemic
on March 11, 2020 [1]. In its early stages, the symptoms
of COVID-19 include fever, cough, fatigue, and myalgia.
However, in more serious cases, it can lead to shortness of
This work was supported by NSF under Grant No. CNS-1907381. Shayan
Hassantabar and Niraj K. Jha are with the Department of Electrical
Engineering, and Kenza Hamidouche is with the Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ, 08544 USA, e-mail:{seyedh,jha,kenzah}@princeton.edu. Novati
Stefano and Alessandra Ferrari are with the Division of Infectious Diseases Unit, Fondazione IRCCS Policlinico San Matteo, Pavia, Italy, email: {S.Novati,alessandra.ferrari}@smatteo.pv.it. Raffaele Bruno is with the
Division of Infectious Diseases Unit, Fondazione IRCCS Policlinico San
Matteo, Pavia, Italy, and the Department of Medical, Surgical, Diagnostic and
Pediatric Science, University of Pavia, Italy, e-mail: {raffaele.bruno@unipv.it}.
Vishweshwar Vinnakota and Gregory N. Nicola are with NeuTigers, Inc.,
Brooklyn, NY 11201, USA, e-mail: {vishu,greg}@neutigers.com. Ignazio R.
Marino is with Thomas Jefferson University and Jefferson Health, Philadelphia,
PA 19107, USA, e-mail: {Ignazio.Marino@jefferson.edu}.

breath, pneumonia, severe acute respiratory disorder, and
heart problems, and may lead to death [2]. It is of paramount
importance to detect which individuals are infected at as
early a stage as possible in order to limit the spread of
disease through quarantine and contact tracing. In response
to COVID-19, governments around the world have issued
social distancing and self-isolation orders. This has led
to a significant increase in unemployment across diverse
economic sectors. As a result, COVID-19 has triggered an
economic recession in a large number of countries [3].
Reverse Transcription-Polymerase Chain Reaction (RTPCR) is currently the gold standard for SARS-CoV-2 detection
[4]. This test is based on viral nucleic acid detection in sputum
or nasopharyngeal swab. Although it has high specificity,
it has several drawbacks. The RT-PCR test is invasive and
uncomfortable, and non-reusable testing kits have led to
significant supply chain deficiencies. SARS-CoV-2 infection
can also be assessed with an antibody test [5]. However,
antibody titers are only detectable from the second week of
illness onwards and persist for an uncertain length of time.

2

The antibody test is also invasive, requiring venipuncture
which, in combination with a several-day processing time,
makes it less ideal for rapid mass screening. In the current
economic and social situation, there is a great need for an
alternative SARS-CoV-2/COVID-19 detection method that is
easily accessible to the public for repeated testing with high
accuracy.
To address the above issues, researchers have begun
to explore the use of artificial intelligence (AI) algorithms
to detect COVID-19 [6]. Initial work concentrated on CT
scans and X-ray images [4], [7]–[21]. A survey of such
datasets can be found in [22], [23]. These methods often
rely on transfer learning of a convolutional neural network
(CNN) architecture, pre-trained on large image datasets, on
a smaller COVID-19 image dataset. However, such an imagebased AI approach faces several challenges that include lack
of large datasets and inapplicability outside the clinic or
hospital. In addition, other work [24] shows that it is difficult
to distinguish COVID-19 pneumonia from influenza virus
pneumonia in a clinical setting using CT scans. Thus, the
work in this area is not mature yet.
CORD-19 [25] is an assembly of 59000 scholarly articles on COVID-19. It can be used with natural language
processing methods to distill useful information on COVID19-related topics.
AI4COVID-19 [26] performs a preliminary diagnosis of
COVID-19 through cough sample recordings with a smartphone application. However, since coughing is a common
symptom of two dozen non-COVID-19 medical conditions,
this is an extremely difficult task. Nonetheless, AI4COVID-19
shows promising results and opens the door for COVID-19
diagnosis through a smartphone.
The emergence of wearable medical sensors (WMSs)
offers a promising way to tackle these challenges. WMSs can
continuously sense physiological signals throughout the day
[27]. Hence, they enable constant monitoring of the user’s
health status. Training AI algorithms with data produced
by WMSs can enable pervasive health condition tracking
and disease onset detection [28]. This approach exploits
the knowledge distillation capability of machine learning
algorithms to directly extract information from physiological
signals. Thus, it is not limited to disease detection in the
clinical scenarios.
We propose a framework called CovidDeep for daily
detection of SARS-CoV-2/COVID-19 based on off-the-shelf
WMSs and compact deep neural networks (DNNs). It
bypasses manual feature engineering and directly distills
information from the raw signals captured by available
WMSs. It addresses the problem posed by small COVID-19
datasets by relying on intelligent synthetic data generation
from the same probability distribution as the training data
[29]. These synthetic data are used to pre-train the DNN
architecture in order to impose a prior on the network
weights. To cut down on the computation and storage costs of
the model without any loss in accuracy, CovidDeep leverages
the grow-and-prune DNN synthesis paradigm [30], [31]. This
not only improves accuracy, but also shrinks model size and
reduces the computation costs of the inference process.
The major contributions of this article are as follows:
•
We propose CovidDeep, an easy-to-use, accurate, and
pervasive SARS-CoV-2/COVID-19 detection frame-

•

•

•

work. It combines features extracted from physiological signals using WMSs and simple-to-answer questions in a smartphone application-based questionnaire
with efficient DNNs.
It uses an intelligent synthetic data generation module
to obtain a synthetic dataset [29], labeled by decision
rules. The synthetic dataset is used to pre-train the
weights of the DNN architecture.
It uses a grow-and-prune DNN synthesis paradigm
that learns both an efficient architecture and weights
of the DNN at the same time [30], [31].
It provides a solution to the daily SARS-CoV2/COVID-19 detection problem. It captures all the
required physiological signals non-invasively through
comfortably-worn WMSs that are commercially available.

The rest of the article is organized as follows. Section
2 reviews background material. Section 3 describes the
CovidDeep framework. Section 4 provides implementation
details. Section 5 presents experimental results. Section 6
provides a short discussion on CovidDeep and possible
directions for future research. Finally, Section 7 concludes the
article.

2

BACKGROUND

In this section, we discuss background material related to
the CovidDeep framework. It involves recent methods for
synthesizing and training efficient DNN architectures.
One approach is based on the use of efficient building
blocks. Using such blocks results in compact networks
and significantly reduces the computational costs and storage needs. For example, inverted residual blocks used in
MobileNetV2 [32] reduce the number of parameters and the
floating-point operations (FLOPs) greatly. In addition, spatial
convolution is one of the most computationally expensive
operations in CNN architectures. To address this issue,
ShuffleNet-v2 [33] uses the depth-wise separable convolutions and channel-shuffling operations. Furthermore, Shift
[34] addresses this problem by using shift-based modules
that combine shifts and point-wise convolutions. Neural
architecture search (NAS) is also used in the literature to
automatically generate compact architectures. For example,
FBNetV2 [35] uses differentiable NAS approach to synthesize
compact CNN architectures. Efficient performance predictors,
e.g., for accuracy, latency, and energy, are also used to
accelerate the DNN search process [36], [37]. FBNetV3
[38] takes into account the training recipe (i.e., training
hyperparameters) in the NAS as well, leading to finding
higher accuracy-recipe combinations.
In addition, DNN compression methods can remove
redundancy in the DNN models. Network pruning [39]
uses a pruning methodology to remove redundancy from
both CNN and multilayer-perceptron architectures. ESE [40]
shows the pruning methods are also helpful in removing
redundancy in recurrent neural networks. Dai et al. [30],
[41] combine network growth with pruning to generate
efficient CNNs and long short-term memories. SCANN [31]
combines feature dimensionality reduction with grow-andprune synthesis to generate very compact models that can

3

be easily deployed on edge devices and Internet-of-Things
sensors.
Orthogonal to the above works, low-bit quantization of
DNN weights can also be used to reduce computations in a
network with little to no accuracy drop [42].

3

•

M ETHODOLOGY

In this section, we present the CovidDeep framework. First,
we give an overview of the entire framework. Then, we
describe the DNN architecture that is used in CovidDeep for
inference. We also describe how synthetic data generation
can be used to impose a prior on the DNN weights and then
use the DNN grow-and-prune synthesis paradigm to boost
the test accuracy further and ensure computational efficiency
of the model.
3.1

•

Framework overview

The CovidDeep framework is shown in Fig. 1. CovidDeep
obtains data from two different sources: physiological signals
and questionnaire. It has two flows: one that does not use synthetic data and another one that does. When synthetic data
are not used, the framework just uses the real dataset divided
into three categories: training, validation, and test. It trains
the DNNs with the training dataset and picks the best one
for the given set of features based on the validation dataset,
and finally tests this DNN on the test dataset to obtain the
test accuracy. However, when the real training dataset size is
small, it is often advantageous to draw a synthetic dataset
from the same probability distribution. CovidDeep uses
synthetic data generation methods to increase the dataset
size and use such data to pre-train the DNN architecture.
Then, it uses grow-and-prune synthesis to generate inference
models that are both accurate and computationally-efficient.
The models generated by CovidDeep are efficient enough to
be deployed on the edge, e.g., the smartphone or smartwatch,
for SARS-CoV-2/COVID-19 inference.
Next, we discuss the data input, model training, and
model inference details.
•

Data input: As mentioned above, physiological signals and a questionnaire are the two sources of data
input to the model. The physiological signals are
derived from WMSs embedded in a smartwatch as
well as a discrete pulse oximeter and blood pressure
monitor. These signals can be easily obtained in a
non-invasive, passive, and user-transparent manner.
The list of these signals includes Galvanic skin response (GSR), inter-beat interval (IBI) that indicates
the heart rate, skin temperature, oxygen saturation,
and blood pressure (systolic and diastolic). In the
questionnaire, we asked the following yes/no questions: immune-compromised, chronic lung disease,
cough, shortness of breath, chills, fever, muscle pain,
headache, sore throat, smell-taste loss, and diarrhea.
We collected data on age, gender, weight, height,
and smoking/drinking (yes/no), but did not find
them to be useful either because of overfitting or
being unrepresentative. All the relevant data sources
are aggregated into a comprehensive data input for
further processing.

Model training: CovidDeep uses different types of
DNN models: (i) those trained on the raw data only,
(ii) those trained on raw data augmented with synthetic data to boost accuracy, and (iii) those subjected
to grow-and-prune synthesis for both boosting accuracy further and reducing model size. The first type
of DNN model uses a few hidden layers. The second
type of DNN model is trained based on a system
called TUTOR [29] and is suitable for settings where
data availability is limited. It provides the DNN with a
suitable inductive bias. The third type of DNN model
is based on the grow-and-prune DNN synthesis
paradigm and employs three architecture-changing
operations: neuron growth, connection growth, and
connection pruning. These operations have been
shown to yield DNNs that are both accurate and
efficient [31].
Model inference: CovidDeep enables the users to
have SARS-CoV-2/COVID-19 detection decision on
their edge device on demand.

Next, we discuss the CovidDeep DNN architecture.
3.2

Model architecture

Fig. 2 shows the processing pipeline of the CovidDeep
framework. The architecture takes the data inputs (shown
at the bottom) and generates a prediction, i.e., the detection
decision, (shown at the top). The pipeline consists of four
steps: data pre-processing, synthetic data generation and
architecture pre-training, grow-and-prune synthesis, and
output generation through softmax.
In the data pre-processing stage, data normalization and
data alignment/aggregation are done.
•

Data normalization: This step is aimed at changing
feature values to a common scale. While data normalization is not always required, it is highly beneficial
in the case of datasets that have features with very
different ranges. It leads to better noise tolerance and
improvement in model accuracy [43]. Data normalization can be done in several ways, such as min-max
scaling and standardization. In this work, we use
min-max scaling to map each data input to the [0, 1]
interval. Scaling can be done as follows:

xscaled =
•

x − min(x)
max(x) − min(x)

Data alignment/aggregation: The data from different
WMSs may have different start times and frequencies.
In order to merge them into a dataset, we need to synchronize the data streams based on their timestamps.
The answers to the questions in the questionnaire are
also added to the final dataset.

Synthetic data generation: The training dataset generated in
the above manner is next used to generate a synthetic dataset
that is used to pre-train the DNN. These synthetic data and
pre-training steps are based on the TUTOR framework [29].
The schematic diagram of the training scheme based on
synthetic data is shown in Fig. 3. The synthetic dataset is
generated in three different ways in TUTOR:

4

Data input

DNN training

Deployment

Physiological
BP
Pre-training on synthetic data

GSR
…

Grow-and-prune training

DT/RF
KB

Ox.
Training

Pre-trained NNs

Initial
architecture

Comprehensive
Dataset

Connection
pruning

Syn data

Questionnaire

Neuron
growth

Connection
growth
Final
architecture

Edge-side
Inference

MND

Cough
Fever
…

GMM
KDE

Fig. 1. Schematic diagram of the CovidDeep framework (GSR: Galvanic skin response, IBI: inter-beat interval, Ox.: oxygen saturation, BP: blood
pressure, DT/RF: decision tree/random forest, NN: neural network, KB: knowledge-base, MND: multi-variate Normal distribution, GMM: Gaussian
mixture model, KDE: kernel density estimation).

DT/RF

Softmax

KB

Sparsely-connected
neurons

Training

Pre-trained NNs

Comprehensive
Dataset

…

Fully-connected
layers

Syn data
MND
GMM

Syn. data gen.
Data pre-processing
WMS physiological signals

KDE

Fig. 3. The schematic diagram for pre-training of the DNN model with
the synthetic dataset (DT/RF: decision tree/random forest, NN: neural
network, KB: knowledge-base).

Questionnaire

Fig. 2. An illustration of the CovidDeep processing pipeline to generate
predictions from data inputs.

•

•

Using multi-variate Normal distribution (MND): In
this approach, the real training dataset, i.e., the one
obtained as a fraction of the data obtained from the
WMSs and questionnaire, is modeled as a normal
distribution to generate the synthetic data.
Using Gaussian mixture model (GMM): This approach uses a multi-dimensional GMM to model
the data distribution. The optimal number of GMM

•

components is obtained with the help of a validation dataset. Subsequently, the synthetic dataset is
generated from this GMM.
Using kernel density estimation (KDE): This approach
uses non-parametric density estimation to estimate
the probability distribution as a sum of many kernels.
In our implementation, KDE is based on the Gaussian
kernel function. The synthetic data are generated
based on samples generated from this model.

Building a knowledge base (KB): After generation of the
synthetic data, we need to label the data points. To this
end, we build a KB from the real training dataset. Decision
tree (DT) and random forest (RF) are two classical machine
learning methods that are inherently rule-based. In fact, each

5

decision path in a decision tree, from the root to a leaf, can
be thought of as a rule. Therefore, we aim to identify the set
of rules that best describes the data. We use such a model as
a KB to label the generated synthetic dataset.
Training with synthetic data: We use the labeled synthetic
data to impose a prior on the DNN weights. To accomplish
this, we pre-train the DNN model by using the generated synthetic dataset. This provides the network with an appropriate
inductive bias and helps the network to “get underway.” This
helps improve accuracy when data availability is limited.
3.3

Grow-and-prune synthesis of the DNN

In this section, we discuss the grow-and-prune synthesis
paradigm [30], [31]. The approach presented in [31] allows
the depth of the DNN to grow during synthesis. Thus, a
hidden neuron can receive inputs from any neuron activated
before it (including input neurons) and can feed its output to
any neuron activated after it (including output neurons). As
a result, the depth of the model is determined based on how
the hidden neurons are connected, enabling the depth to
be changed during training. We use three basic architecturechanging operations in the grow-and-prune synthesis process
that are discussed next.
Connection growth: This activates the dormant connections
in the network. The weights of the added connections are
set to 0 and trained later. We use two different methods for
connection growth:
•

•

Gradient-based growth: This approach was first
introduced by Dai et al. [30]. Algorithm 1 shows the
process of gradient-based growth. Each weight matrix
has a corresponding binary mask of the same size.
This mask is used to disregard the inactive connections. The algorithm adds connections to reduce the
loss function L significantly. To this end, the gradients
of all the dormant connections are evaluated and their
effectiveness ranked based on this metric. During
a training epoch, the gradients of all the weight
matrices for all the data mini-batches are captured in
the back-propagation step. An inactive connection is
activated if its gradient magnitude is large relative to
the gradients in its associated layer.
Full growth: This connection growth restores all the
dormant connections in the network to make the
DNN fully-connected.

Connection pruning: Connection pruning deactivates the
connections that are smaller than a specified threshold.
Algorithm 2 shows this process.
Neuron growth: This step adds neurons to the network and
thus increases network size. This is done by duplicating
existing neurons in the architecture. To break the symmetry,
random noise is added to the weights of all the connections
related to the newly added neurons. The neurons to be
duplicated are either selected randomly or based on higher
activation values. The process is explained in Algorithm 3.
We apply connection pruning after neuron growth and
connection growth in each iteration. Grow-and-prune synthesis starts from a fully connected architecture (mask values set
to 1) and runs for a pre-defined number of iterations. Finally,
the architecture that performs the best on the validation
dataset is chosen.

Algorithm 1 Connection growth algorithm
Input: W ∈ RM ×N : weight matrix of dimension M × N
(connecting layer with M neurons to layer with N
neurons); M ask ∈ RM ×N : weight mask of the same
dimension as the weight matrix; Network P ; W.grad:
gradient of the weight matrix (of dimension M × N );
data D; α: growth ratio
if full growth then
M ask[1:M,1:N ] = 1
else if gradient-based growth then
Forward propagation of data D through network P
and then back propagation
Accumulation of W.grad for one training epoch
t = (α×M N )th largest element in the |W.grad| matrix
for all w.gradij do
if |w.gradij | > t then
M askij = 1
end if
end for
end if
W = W ⊗ M ask
Output: Modified weight matrix W and mask matrix M ask
Algorithm 2 Connection pruning algorithm
Input: Weight matrix W ∈ RM ×N ; mask matrix M ask of
the same dimension as the weight matrix; α: pruning
ratio
t = (α × M N )th largest element in |W |
for all wij do
if |wij | < t then
M askij = 0
end if
end for
W = W ⊗ M ask
Output: Modified weight matrix W and mask matrix M ask

4

I MPLEMENTATION D ETAILS

In this section, we first explain how the data were obtained
from 87 individuals and how various datasets were prepared
from the data. We also provide implementation details of the
CovidDeep DNN model.
4.1

Data collection and preparation

We collected physiological signals and questionnaire data
with Institutional Research Board (IRB) approval at San
Matteo Hospital in Pavia, Italy. 30 individuals were healthy
(referred to as Cohort 1) and the remaining were SARSCoV-2-positive with varying levels of disease severity. The
SARS-CoV-2-positive cases were categorized into two other
cohorts: asymptomatic (Cohort 2 with 27 individuals) and
symptomatic (Cohort 3 with 30 individuals). Distinguishing
among these cohorts is important to ascertain who may be
spreading the virus unknowingly and to determine whether
medical support is needed for symptomatic individuals.
Hence, we train DNN models that can perform three-way
classification.
To collect the physiological signals, we used commercially
available devices: Empatica E4 smartwatch (sensors we

6

Algorithm 3 Neuron growth algorithm

TABLE 1
Data types collected in the CovidDeep framework

M ×N

Input: Network P ; weight matrix W ∈ R
; mask matrix
M ask of the same dimension as the weight matrix; data
D; candidate neuron nj to be added; array A of activation
values for all hidden neurons
if activation-based selection then
forward propagation through P using data D
i = argmax (A)
else if random selection then
randomly pick an active neuron ni
end if
M askj· = M aski· , M ask·j = M ask·i
wj· = wi· + noise, w·j = w·i + noise
Output: Modified weight matrix W and mask matrix M ask

found useful: GSR, IBI, skin temperature), a pulse oximeter,
and a blood pressure monitor. Alongside the physiological
signals, we employed a questionnaire to collect information
about possible COVID-19-related symptoms from all the
individuals. We also collected data about age, gender, weight,
height, and smoking/drinking (yes/no), but did not rely on
these features as they were not necessarily representative
of the larger population. Table 1 shows all the data types
that we found to be useful. The smartwatch data capture
the physiological state of the user. GSR measures continuous
variations in the electrical characteristics of the skin, such
as conductance, which can be caused by variations in body
sweat. IBI correlates with cardiac health. Furthermore, skin
acts as a medium for insulation, sweat, and control of blood
flow. Although it is not a clear indicator of internal body
temperature, skin temperature helps assess skin health. The
pulse oximeter indirectly measures blood oxygen saturation.
It is a comfortable and painless way of measuring how well
oxygen is being sent to parts of the body furthest from the
heart, such as the arms and legs. Blood pressure exposes
various underlying health problems. Last, but not the least,
the questionnaire elicits information that may help improve
COVID-19 detection accuracy. From all these sources of
data, we derive various subsets as datasets for use in the
CovidDeep framework to see which data features are the
most beneficial to obtaining a high detection accuracy. In
addition, the various sensor subsets have different costs.
Hence, our results also let one take test accuracy vs. cost into
consideration.
Before data collection commences, we inform the participants about the procedure. We then collect some relevant
information and COVID-19-related symptoms in response
to a questionnaire. We place the pulse oximeter on the index
finger of the user for blood oxygen measurement. We also
obtain the systolic/diastolic blood pressure measurements.
We place the smartwatch on the participant’s wrist. Data collection lasts for at most one hour for each participant, during
which time we collect sensor data from the smartwatch. We
stream the data from the smartwatch to the smartphone over
Bluetooth in real-time using a smartphone application. This
application collects the data and performs basic validation to
ensure data integrity.
Next, we pre-process the raw data to generate a comprehensive dataset. To this end, we first synchronize the WMS

Data type

Data source

Immune-compromised
Chronic lung disease
Shortness of breath
Cough
Fever
Muscle pain
Chills
Headache
Sore throat
Smell/taste loss
Diarrhea

Questionnaire
Questionnaire
Questionnaire
Questionnaire
Questionnaire
Questionnaire
Questionnaire
Questionnaire
Questionnaire
Questionnaire
Questionnaire

Galvanic skin response (µS)
Skin temperature (◦ C )
Inter-beat interval (ms)

Smartwatch
Smartwatch
Smartwatch

Oxygen saturation (%)
Systolic blood pressure (mmHg)
Diastolic blood pressure (mmHg)

Pulse oximeter
Blood pressure monitor
Blood pressure monitor

data streams. We then divide the data streams into 15-second
data windows. We then split the participants into three
different sets of training, validation, and test. The training
set contains data from 52 individuals, approximately 60% of
all the participants. Among the 52 individuals represented
in the training set, 18 are healthy, 16 are asymptomatic (but
virus-positive), and 18 are symptomatic (and virus-positive).
The validation set consists of data from 17 individuals,
approximately 20% of all the participants, with 6, 5, and
6 individuals from Cohorts 1, 2, and 3, respectively. The test
set contains data from 18 individuals, approximately 20%
of all the participants, with 6 individuals from each of the
three cohorts. This data partitioning ensures that all the data
collected from any individual are limited to just one of the
three sets. Furthermore, the data instances extracted from
each individual have no time overlap. In addition, in order to
conduct ablation studies to gauge the impact of different data
streams, we create different datasets, with various subsets of
all the features.
4.2

Model implementation

We have implemented the CovidDeep framework in PyTorch.
We perform DNN training on the Nvidia Tesla P100 data
center accelerator, with 16GB of memory. We use cuDNN
library to accelerate GPU processing. Next, we give the
details of the implemented DNN architectures trained on the
different datasets.
We train various DNNs (with different numbers of layers
and different numbers of neurons per layer) and verify
their performance on the validation dataset. In general, a
four-layer architecture with 256, 128, 128, and 3 neurons,
respectively, performs the best. The number of neurons in the
input layer depends on which subset of features is selected
for training the DNN. In the case of the full dataset, the
input layer has 194 neurons, which indicates the dataset
dimension. We obtain the features of the dataset from the
15s data window as follows. Sensor data collected from
the smartwatch in the data window consist of 180 signal
readings, hence 180 features, from the three data streams

7

running at 4Hz. We derive 11 features from the 11 questionnaire questions. Finally, we append the pulse oximeter
oxygen saturation measurement and systolic/diastolic blood
pressure measurements to obtain a feature vector of length
194.
We use leaky ReLU as the nonlinear activation function
in all the DNN layers. As explained in Section 3, we generate
three DNNs for each dataset: (i) DNN trained on the real
training dataset, (ii) DNN pre-trained on the synthetic dataset
and then trained on the real training dataset, and (iii) DNN
synthesized and trained with the grow-and-prune synthesis
paradigm.
4.3

Network training

We use the stochastic gradient descent optimizer for DNN
training, with a learning rate of 5e-3 and batch size of 256. We
use 100000 synthetic data instances to pre-train the network
architecture. Moreover, in the grow-and-prune synthesis
phase, we train the network for 20 epochs each time the
architecture changes. We apply network-changing operations
over five iterations. In this step, we use pruning to achieve a
pre-defined number of connections in the network, chosen
based on performance on the validation set.

5

E XPERIMENTAL R ESULTS

In this section, we analyze the performance of CovidDeep
DNN models. We target three-way classification among the
three cohorts described earlier. In addition, we perform an
ablation study to analyze the impact of different subsets
of features as well as different steps of CovidDeep DNN
synthesis.
The CovidDeep DNN models are evaluated with four
different metrics: test accuracy, false positive rate (FPR), false
negative rate (FNR), and F1 score. These terms are based on
the following:
•

•

True positive (negative): SARS-CoV-2/COVID-19
(healthy) data instances classified as SARS-CoV2/COVID-19 (healthy).
False positive (negative): healthy (SARS-CoV2/COVID-19) data instances classified as SARS-CoV2/COVID-19 (healthy).

These metrics evaluate the model performance from different
perspectives. Test accuracy evaluates its overall prediction
power. It is simply the ratio of all the correct predictions on
the test data instances and the total number of such instances.
The FPR is defined as the ratio of the number of negative,
i.e., healthy, instances wrongly categorized as positive (false
positives) and the total number of actual negative instances.
The FNR is the ratio of positives that yield different test
outcomes. Thus, there is an FNR for both Cohorts 2 and
3. Because of the three-way classification, the F1 score we
report is the Macro F1 score.
5.1

Model performance evaluation

We obtained the highest test accuracy with a DNN model
trained with the grow-and-prune synthesis paradigm on the
dataset that contained features from four categories: GSR,
pulse oximeter (Ox), blood pressure (BP), and questionnaire

TABLE 2
Confusion matrix for the most accurate three-way classification model
Label↓\Prediction→

C1

C2

C3

Total

C1
C2
C3
Total

1066
54
0
1120

9
1152
0
1161

0
0
975
975

1075
1206
975
3256

(Q). Table 2 shows the confusion matrix for three-way
classification among the three cohorts: Cohort 1 (healthy),
Cohort 2 (asymptomatic-positive), Cohort 3 (symptomaticpositive), denoted as C1, C2, and C3, respectively. CovidDeep
DNN achieves a test accuracy of 98.1%. The model achieves
an FPR of only 0.8%. The low FPR means that the model
does not raise many false alarms. It results in a 4.5% FNR for
Cohort 2 and a 0.0% FNR for Cohort 3, denoted as FNR(2)
and FNR(3), respectively (each FNR refers to the ratio of
the number of false predictions for that cohort divided by
the total number of data instances of that type). The low
FNRs demonstrate the ability of the DNN model to not miss
virus-positive cases. Moreover, the Macro F1 score of the
DNN model is also high: 98.2%.
Next, we compare the three DNN models, trained on the
real training dataset, with the aid of synthetic data, and with
the aid of grow-and-prune synthesis, for the most accurate
case in Table 3. From this comparison, we see that the use of
synthetic data and then grow-and-prune synthesis is able to
boost the test accuracy compared to the DNN model trained
on just the real dataset. In addition, we see improvements in
the FPR and FNR values. The F1 score also follows the same
trend, increasing with the use of synthetic data, and even
more with the use of grow-and-prune synthesis.
5.2

Ablation studies

In this section, we report results on various ablation studies.
We begin by considering DNN models trained on features
obtained from subsets of the six data categories (five sensors
and the questionnaire). This helps us understand the impact
of each of these categories and their various combinations.
Then, we analyze the impact of different parts of the
CovidDeep training process, pre-training with synthetic data,
and grow-and-prune synthesis.
Since there are six data categories from which the
corresponding features are obtained, there are 64 subsets.
However, one of these subsets is the null subset. Thus, we
evaluate the remaining 63 subsets. For these evaluations, we
only consider the first two types of DNN models, referred
to as DNN Models 1 and 2. We consider grow-and-prune
synthesis-based models later. The results shown in Table 4
correspond to the case when features from only one, two
or three data categories are chosen, and in Table 5 when
features from four, five or six data categories are chosen.
We first notice that DNN Model 2 generally performs
better than DNN Model 1 across the various performance
metrics. This underscores the importance of using synthetic
data when the available dataset size is not large. Second,
we observe that since this is a three-way classification, only
33.3% accuracy is possible by randomly predicting one of the
three Cohorts. Thus, even single data categories (GSR, Temp,

8

TABLE 3
Test accuracy, FPR, FNRs, and F1 score (all in %) for the three DNN models obtained for the most accurate case
DNN model trained on

Acc.

FPR

FNR(2)

FNR(3)

F1 Score

Real training dataset
Real+synthetic training dataset
Real+synthetic training dataset + grow-prune

79.9
84.8
98.1

22.5
14.1
0.8

34.2
28.4
4.5

0.0
0.0
0.0

80.9
85.5
98.2

IBI, Ox, BP, Q) enable much better prediction than by chance.
These single data categories are still only weak learners
of the correct label, when used in isolation. Third, DNN
models, in general, tend to perform better on the various
performance metrics when more data categories are used.
However, this is not always true. For example, we obtain the
highest accuracy of 93.6% with DNN Model 2 when only
features from four (GSR, Temp, Ox, BP) of the six categories
are used. Adding features based on IBI or Q or both to these
four categories actually reduces the test accuracy. This may
be due to the curse of dimensionality. When the number
of features increases, in general, the dataset size needs to
be increased to obtain a good accuracy. For a fixed dataset
size, this curse indicates that the number of features should
be reduced. However, throwing out informative features
would also reduce accuracy. In addition, some features are
interactive, i.e., work synergistically to increase accuracy.
Hence, a balance has to be found between accuracy and
the number of features. Finally, when not all sensors are
available (perhaps due to cost reasons), a suitable set that
still provides reasonable accuracy can be chosen based on
the given cost budget. This may help a broader cross-section
of the population access the technology.
To illustrate the effect of the different parts of the
CovidDeep training process, we compare 11 CovidDeep
DNN models, trained based on the different DNN synthesis
and training steps. We chose these models from different
accuracy ranges. Table 6 shows comparison results for the
three-way classification task. We have already compared
various performance metrics for DNN Models 1 and 2
earlier. Hence, here, we just report their accuracy, FLOPs,
and number of model parameters (#Param). The best DNN
Model 3 was obtained with the help of the validation dataset.
This enabled us to find the best #Param. value. Only this
model was tested on the test dataset. Acc.(1) and Acc.(2),
respectively, refer to the accuracy of DNN Models 1 and 2.
The FLOPs and #Param. for these two models are identical.
We report all the performance metrics for DNN Model 3 that
is generated by grow-and-prune synthesis using both real
and synthetic data. Thus, the starting point for DNN Model
3 synthesis is DNN Model 2. Next, we compare DNN Model
3 with the other two models based on various measures and
show why it is suitable for deployment on the edge devices.

•

•

•

Smaller model size: It contains 3.4× fewer parameters on an average (geometric mean) than DNN
Models 1 and 2, thus significantly reducing the
memory requirements.
Less computation: It reduces FLOPs per inference
by 3.5× on an average (geometric mean) relative to
DNN Models 1 and 2, thus facilitating more efficient
inference on the edge devices.

6

Better performance: It improves accuracy on an
average by 7.8% (1.9%) relative to DNN Model 1
(2), while also lowering FPR and FNRs, in general.

D ISCUSSION AND F UTURE W ORK

In this section, we discuss the inspirations we took from the
human brain in the synthesis process of CovidDeep DNNs. We
also discuss future directions in medical research enabled by the
CovidDeep framework.
An interesting ability of the human brain is to efficiently
solve novel problems in a new domain despite limited prior
experience. Inspired by this human capability, CovidDeep
uses the TUTOR [29] approach for synthetic data generation
and labeling to help the neural network start from a better
initialization point. Use of gradient descent from a learned
initialization point provides the DNN with an appropriate
inductive bias. Hence, it reduces the need for large datasets
that are not readily available for SARS-CoV-2/COVID-19 AI
research.
The CovidDeep DNN training process takes another inspiration from the human brain development process in the growand-prune synthesis step. The human brain undergoes dynamic
changes in its synaptic connections every second of its lifetime.
Acquisition of knowledge depends on these synaptic rewirings
[44]. Inspired by this phenomenon, CovidDeep utilizes the
grow-and-prune synthesis paradigm to enable DNN architecture
adaptation throughout training. CovidDeep DNNs synthesized
with grow-and-prune synthesis do not suffer from the situation
faced by most current DNNs: fixed connections during training.
This enables CovidDeep to generate very compact, yet accurate,
models for SARS-CoV-2/COVID-19 detection.
CovidDeep uses physiological signals extracted using commercially available devices and achieves high test accuracy. As
a result, it provides a testing mechanism that is accurate, easily
accessible to the general public, and easy for individuals to use.
Furthermore, this mechanism only requires a few minutes of
data collection from an individual to perform an inference. Note
that at most one hour of data collection from each individual
was only required for training of the DNN models. It does not
require the presence of a nurse or physician during testing. In
fact, besides the data collected by the smartwatch and discrete
sensors (for obtaining blood oxygen and blood pressure), the
additional information required by the electronic questionnaire
is small, related to the general health of the subject, and can be
easily filled out with a yes/no answer. Thus, CovidDeep has
the potential to significantly decrease the spread of SARS-CoV-2,
save hundreds of thousands of lives, and drastically reduce the
need for hospitalization, while also helping the world economy
recover.
CovidDeep demonstrates that WMS-based SARS-CoV2/COVID-19 detection is feasible. Previously, diabetes diagnosis
was shown to be possible with the help of such sensors [28]. We
believe that WMS-based disease detection is feasible for a large
number of diseases [27].
Since data were collected from only 87 individuals, despite
being augmented with synthetic training data drawn from the
real training data probability distribution, more work is needed
for validating the various DNN models in the field, especially

9

TABLE 4
Test accuracy, FPR, FNRs, and F1 score (all in %) for two DNN models obtained for feature subsets from one, two or three data categories

Data category

Acc.

FPR

GSR
Temp
IBI
Ox
BP
Q
GSR+Temp
GSR+IBI
GSR+Ox
GSR+BP
GSR+Q
Temp+IBI
Temp+Ox
Temp+BP
Temp+Q
IBI+Ox
IBI+BP
IBI+Q
Ox+BP
Ox+Q
BP+Q
GSR+Temp+IBI
GSR+Temp+Ox
GSR+Temp+BP
GSR+Temp+Q
GSR+IBI+Ox
GSR+IBI+BP
GSR+IBI+Q
GSR+Ox+BP
GSR+Ox+Q
GSR+BP+Q
Temp+IBI+Ox
Temp+IBI+BP
Temp+IBI+Q
Temp+Ox+BP
Temp+Ox+Q
Temp+BP+Q
IBI+Ox+BP
IBI+Ox+Q
IBI+BP+Q
Ox+BP+Q

54.2
57.2
66.6
45.4
44.3
61.4
57.2
74.9
52.7
55.2
89.1
68.1
48.3
50.3
68.9
48.1
47.8
80.9
59.6
50.2
51.8
70.5
69.1
57.0
83.6
64.8
60.2
87.7
71.3
69.9
63.9
57.4
55.8
73.6
70.6
53.3
47.9
65.0
54.8
55.9
66.9

22.1
31.5
55.1
56.2
96.3
0.0
33.4
3.2
29.0
70.7
6.8
19.3
26.3
84.5
26.5
60.4
92.8
19.5
56.2
56.2
56.2
11.5
22.1
64.0
0.2
14.0
34.4
11.2
40.7
22.9
26.5
38.9
71.6
17.2
34.5
56.2
46.6
59.1
56.2
56.2
56.2

DNN Model 1
FNR(2) FNR(3)
23.3
60.3
24.0
59.6
60.3
100.0
60.3
34.6
44.2
53.8
23.3
53.9
78.4
54.7
60.4
68.0
54.0
34.2
54.8
80.2
80.1
54.7
33.5
54.8
44.2
45.4
52.8
23.3
37.1
56.7
73.8
62.4
51.2
51.8
44.2
71.8
94.9
37.5
67.8
65.2
35.0

F1 Score

Acc.

FPR

44.6
57.5
65.6
45.5
36.4
53.5
57.3
74.3
51.3
52.7
89.6
68.4
46.5
45.9
69.8
49.8
44.8
81.8
59.1
52.5
49.9
70.8
70.0
55.4
83.9
64.8
61.5
88.3
71.2
71.0
62.3
57.5
53.9
74.5
72.1
55.8
43.5
66.1
57.2
55.0
68.2

54.2
58.6
66.8
45.4
44.3
63.0
76.9
76.1
47.5
64.1
89.2
68.2
49.3
53.7
69.0
49.0
48.5
80.9
66.9
50.2
57.6
76.6
69.7
67.0
91.3
70.8
64.3
88.8
81.9
75.5
64.1
61.8
55.3
77.1
72.3
53.4
49.9
64.1
55.0
53.4
66.9

22.1
32.2
53.1
56.2
96.3
0.0
6.4
3.6
44.3
46.4
6.7
19.9
24.2
74.0
26.3
58.3
89.8
17.8
56.2
56.2
56.2
3.5
23.1
34.2
0.2
19.1
32.2
7.7
23.1
22.7
25.9
30.7
70.0
9.0
33.9
56.2
40.8
60.8
56.2
56.2
56.2

99.6
33.4
21.1
46.7
5.2
5.2
31.4
37.4
71.3
5.2
0.0
18.8
46.7
5.2
0.0
22.7
5.2
0.0
5.2
5.2
0.0
17.9
37.2
5.2
0.0
45.8
29.5
0.0
5.2
5.2
0.0
22.2
5.2
5.0
5.4
5.2
5.2
5.2
5.2
4.6
5.2

DNN Model 2
FNR(2) FNR(3)
23.4
60.2
25.1
59.6
60.3
100.0
44.1
31.9
44.7
51.2
23.3
52.9
77.7
54.7
60.3
68.0
54.9
35.8
35.0
80.2
60.3
46.0
27.1
54.4
23.3
43.2
43.7
23.3
4.1
41.8
73.8
57.8
54.0
53.6
40.4
71.4
94.7
38.4
67.2
71.6
35.0

F1 Score

99.5
28.2
21.1
46.7
5.2
0.0
15.4
36.3
71.3
5.2
0.0
18.9
46.7
5.2
0.0
22.1
5.2
0.0
5.2
5.2
5.2
17.2
42.4
5.2
0.0
23.0
29.5
0.0
29.8
5.2
0.0
22.2
5.2
0.0
5.2
5.2
5.2
5.2
5.2
5.2
5.2

44.7
58.7
66.0
45.5
36.4
54.7
76.5
75.5
46.1
63.7
89.7
68.6
47.3
50.9
69.9
50.7
46.3
81.7
66.8
52.5
56.8
76.7
70.2
66.4
91.7
71.7
64.8
89.4
82.1
76.7
62.4
61.8
53.0
77.5
73.7
55.9
45.1
65.0
57.4
52.3
68.2

TABLE 5
Test accuracy, FPR, FNRs, and F1 score (all in %) for two DNN models obtained for feature subsets from four, five or six data categories

Data category

Acc.

FPR

GSR+Temp+IBI+Ox
GSR+Temp+IBI+BP
GSR+Temp+IBI+Q
GSR+Temp+Ox+BP
GSR+Temp+Ox+Q
GSR+Temp+BP+Q
GSR+IBI+Ox+BP
GSR+IBI+Ox+Q
GSR+IBI+BP+Q
GSR+Ox+BP+Q
Temp+IBI+Ox+BP
Temp+IBI+Ox+Q
Temp+IBI+BP+Q
Temp+Ox+BP+Q
IBI+Ox+BP+Q
GSR+Temp+IBI+Ox+BP
GSR+Temp+IBI+Ox+Q
GSR+Temp+IBI+BP+Q
GSR+Temp+Ox+BP+Q
GSR+IBI+Ox+BP+Q
Temp+IBI+Ox+BP+Q
GSR+Temp+IBI+Ox+BP+Q

76.6
62.5
87.1
77.6
80.7
60.0
75.0
69.8
59.3
79.9
59.2
63.1
54.5
67.1
66.9
77.1
67.2
64.3
83.8
71.8
62.5
77.8

23.3
27.1
0.2
24.2
22.5
11.5
23.3
32.2
32.6
22.5
52.9
48.5
31.9
56.2
56.2
29.1
5.8
4.7
0.4
37.5
44.8
18.3

DNN Model 1
FNR(2) FNR(3)
27.0
53.4
34.7
34.7
27.8
93.4
42.6
48.5
80.3
34.2
58.9
52.2
90.3
34.5
35.0
31.8
79.1
88.2
39.1
38.5
57.0
39.4

19.2
29.2
0.0
5.2
5.2
5.2
5.2
5.2
0.8
0.0
5.2
5.2
5.2
5.2
5.2
5.2
5.2
5.1
5.2
5.2
5.2
5.2

F1 Score

Acc.

FPR

77.3
62.4
87.5
77.8
81.7
53.2
76.1
71.4
57.1
80.9
61.1
65.1
49.8
68.3
68.2
78.2
65.3
57.8
84.2
73.3
64.5
78.8

74.5
73.3
89.1
93.6
81.2
61.8
76.8
76.1
66.2
84.8
66.9
62.1
54.7
66.8
66.9
83.3
83.1
69.0
83.8
75.3
66.6
83.7

28.5
13.6
1.6
1.7
22.5
11.5
24.2
40.4
3.4
14.1
53.8
56.2
30.7
56.2
56.2
34.2
20.1
15.7
0.4
23.8
48.8
26.9

DNN Model 2
FNR(2) FNR(3)
28.3
44.0
27.9
11.4
26.4
93.0
37.0
24.5
84.5
28.4
37.2
48.0
90.7
35.3
35.0
10.3
23.5
65.8
39.1
41.1
42.4
15.9

18.8
19.8
0.0
5.2
5.2
0.0
5.2
4.9
4.6
0.0
5.2
5.2
5.1
5.2
5.2
5.2
5.2
4.7
5.2
5.2
5.2
5.2

F1 Score
75.2
73.4
89.6
93.7
82.2
54.5
77.8
77.1
60.7
85.5
67.9
64.0
49.8
68.1
68.2
83.7
83.9
67.0
84.2
76.6
68.3
84.1

10

TABLE 6
Comparison of the three DNN models (all performance metrics in %) for various feature sets

Data category
GSR+Ox+BP+Q
GSR+IBI+Q
GSR+Q
GSR+Temp+Q
GSR+Temp+IBI+Q
GSR+Temp+Ox+Q
GSR-Temp-IBI-Ox-Q
GSR+Temp+IBI+Ox+BP
GSR+Ox+BP
GSR+Temp+Ox+BP
IBI+Q

Acc.(1)
79.9
87.7
89.1
83.6
87.1
80.7
67.2
77.1
71.3
77.6
80.9

DNN Models 1 and 2
Acc.(2) FLOPs #Param.
84.8
88.8
89.2
91.3
89.1
81.2
83.1
83.3
81.9
93.6
80.9

136.4k
165.6k
134.9k
165.6k
196.3k
166.1k
196.8k
192.2k
130.8k
161.5k
134.9k

68.5k
83.1k
67.7k
83.1k
98.4k
83.3k
98.7k
96.4k
65.7k
81.0k
67.7k

since the data were obtained from a single location in Italy. This
process has begun across various continents.

7

C ONCLUSION

In this article, we proposed a framework called CovidDeep to
facilitate daily and pervasive detection of SARS-CoV-2/COVID19. The framework combines off-the-shelf WMSs with efficient
DNNs to achieve this goal. CovidDeep DNNs can be easily
deployed on edge devices (e.g., smartphones and smartwatches)
as well as servers. CovidDeep uses synthetic data generation
to alleviate the need for large datasets. In addition, training
of CovidDeep DNNs based on the grow-and-prune synthesis
paradigm enables them to learn both the weights and the
architecture during training. CovidDeep was evaluated based
on data collected from 87 individuals. The highest accuracy
it achieves is 98.1%. However, several subsets of features that
correspond to easily accessible sensors in the market also achieve
high enough accuracy to be practically useful. With more data
collected from larger deployment scenarios, the accuracy of
CovidDeep DNNs can be improved further through incremental
learning.
Contributions: The SARS-CoV-2/COVID-19 detection project
was conceived by Niraj K. Jha. He also supervised the dataset
preparation and DNN model generation efforts. Shayan Hassantabar performed DNN synthesis and evaluation. Vishweshwar Ghanakota developed the smartphone application for
data collection, authenticated the credentials of the application
sending data, ensured data integrity, and ran pre-processing
scripts. Gregory N. Nicola MD and Ignazio R. Marino MD
defined the patient cohorts, and helped with the IRB approval
process. Gregory N. Nicola MD, Ignazio R. Marino MD, and
Bruno Raffaele decided on the questions to be placed in the
questionnaire. Novati Stefano, Alessandra Ferrari, and Bruno
Raffaele collected data from patients and healthy individuals and
labeled the data. Kenza Hamidouche helped with the synthesis
and evaluation of the DNN models. All co-authors helped with
the revision and editing of the manuscript.
Acknowledgments: The project was facilitated by the tireless
efforts of Bob Schena (CEO, Rajant Corp.) and Adel Laoui (CEO,
NeuTigers, Inc.). Giana Schena and Maria Schena helped with
buying and transporting the instruments as well as Englishto-Italian translations of various documents. Joe Zhang helped
initially with feature extraction from the raw dataset. Claudia
Cirillo coordinated the administrative work and helped with
translation of documents to Italian for the IRB application. Ravi
Jha helped with proofreading of the manuscript.The Chief of
the Italian Police, Franco Gabrielli, helped ensure safe and fast
entrance and transfer of US researchers on Italian soil during
the COVID-19 lockdown.
Competing interests: Five of the co-authors of this article,
Niraj K. Jha, Shayan Hassantabar, Vishweshwar Ghanakota,

Acc.

FLOPs

98.1
91.5
91.3
91.3
90.7
87.7
86.4
84.6
82.4
82.3
81.7

19.5k
39.5k
9.5k
151.5k
19.5k
119.5k
59.5k
59.5k
89.5k
129.5k
19.5k

DNN Model 3
#Param FPR FNR(2)
10.0k
20.0k
5.0k
76.0k
10.0k
60.0k
30.0k
30.0k
45.0k
65.0k
10.0k

0.8
1.3
0.2
0.2
0.2
0.3
11.3
29.5
23.8
25.2
29.3

4.5
21.9
23.2
23.3
20.7
28.7
22.6
11.2
2.1
21.0
23.3

FNR(3)

F1 Score

0.0
0.0
0.0
0.0
5.2
5.2
5.2
5.2
29.8
5.2
0.0

98.2
91.9
91.7
91.7
91.0
88.1
87.0
85.1
82.5
82.8
82.5

Gregory N. Nicola MD, and Kenza Hamidouche have equity in
NeuTigers, Inc. Neutigers, along with Rajant Corporation and
Thomas Jefferson University and Jefferson Health, enabled data
collection from San Matteo Hospital, Pavia, Italy.

R EFERENCES
[1]

[2]

[3]

[4]
[5]

[6]
[7]
[8]
[9]
[10]
[11]
[12]

[13]
[14]
[15]

World Health Organization and others, “Coronavirus disease
2019 (COVID-19): Situation report, 72,” 2020. [Online].
Available: https://apps.who.int/iris/bitstream/handle/10665/
331685/nCoVsitrep01Apr2020-eng.pdf
E. Mahase, “Coronavirus: COVID-19 has killed more people
than SARS and MERS combined, despite lower case fatality
rate,” British Medical Journal, vol. 368, 2020. [Online]. Available:
https://www.bmj.com/content/368/bmj.m641
M. Nicola, Z. Alsafi, C. Sohrabi, A. Kerwan, A. Al-Jabir, C. Iosifidis,
M. Agha, and R. Agha, “The socio-economic implications of the
coronavirus and COVID-19 pandemic: A review,” Int. J. Surgery,
2020.
C. Butt, J. Gill, D. Chun, and B. A. Babu, “Deep learning system to
screen coronavirus disease 2019 pneumonia,” Applied Intelligence,
p. 1, 2020.
K. Dheda, S. Jaumdally, M. Davids, J.-W. Chang, P. Gina, A. Pooran,
E. Makambwa, A. Esmail, E. Vardas, and W. Preiser, “Diagnosis of
COVID-19: Considerations, controversies and challenges in South
Africa,” Wits Journal of Clinical Medicine, vol. 2, no. SI, p. 3, 2020.
J. Bullock, K. H. Pham, C. S. N. Lam, M. Luengo-Oroz et al.,
“Mapping the landscape of artificial intelligence applications against
COVID-19,” arXiv preprint arXiv:2003.11336, 2020.
M. Farooq and A. Hafeez, “COVID-ResNet: A deep learning
framework for screening of COVID-19 from radiographs,” arXiv
preprint arXiv:2003.14395, 2020.
L. Wang and A. Wong, “COVID-Net: A tailored deep convolutional
neural network design for detection of COVID-19 cases from chest
radiography images,” arXiv preprint arXiv:2003.09871, 2020.
“SIRM COVID-19 Database,” https://www:sirm:org/category/
senza-categoria/covid-19/.
“Diagnosi radiologica e prevenzione della diffusione di COVID-19
nei dipartimenti di radiologia.” [Online]. Available: https://www.
sirm.org/wp-content/uploads/2020/03/SIRM-Covid-19.pdf
J. Zhang, Y. Xie, Y. Li, C. Shen, and Y. Xia, “COVID-19 screening on
chest X-ray images using deep learning based anomaly detection,”
arXiv preprint arXiv:2003.12338, 2020.
A. Narin, C. Kaya, and Z. Pamuk, “Automatic detection of
coronavirus disease (COVID-19) using X-ray images and deep
convolutional neural networks,” arXiv preprint arXiv:2003.10849,
2020.
A. Abbas, M. M. Abdelsamea, and M. M. Gaber, “Classification of
COVID-19 in chest X-ray images using DeTraC deep convolutional
neural network,” arXiv preprint arXiv:2003.13815, 2020.
L. O. Hall, R. Paul, D. B. Goldgof, and G. M. Goldgof, “Finding
COVID-19 from chest X-rays using deep learning on a small
dataset,” arXiv preprint arXiv:2004.02060, 2020.
P. K. Sethy and S. K. Behera, “Detection of coronavirus disease
(COVID-19) based on deep features,” Preprints, vol. 2020030300, p.
2020, 2020.

11

[16] L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, J. Bai, Y. Lu, Z. Fang,
Q. Song et al., “Artificial intelligence distinguishes COVID-19 from
community acquired pneumonia on chest CT,” Radiology, p. 200905,
2020.
[17] O. Gozes, M. Frid-Adar, H. Greenspan, P. D. Browning, H. Zhang,
W. Ji, A. Bernheim, and E. Siegel, “Rapid AI development cycle for
the coronavirus (COVID-19) pandemic: Initial results for automated
detection & patient monitoring using deep learning CT image
analysis,” arXiv preprint arXiv:2003.05037, 2020.
[18] I. D. Apostolopoulos and T. A. Mpesiana, “COVID-19: Automatic
detection from X-ray images utilizing transfer learning with
convolutional neural networks,” Physical and Engineering Sciences
in Medicine, p. 1, 2020.
[19] S. Wang, Y. Zha, W. Li, Q. Wu, X. Li, M. Niu, M. Wang, X. Qiu,
H. Li, H. Yu et al., “A fully automatic deep learning system for
COVID-19 diagnostic and prognostic analysis,” medRxiv, 2020.
[20] P. Afshar, S. Heidarian, F. Naderkhani, A. Oikonomou, K. N. Plataniotis, and A. Mohammadi, “COVID-CAPS: A capsule networkbased framework for identification of COVID-19 cases from X-ray
images,” arXiv preprint arXiv:2004.02696, 2020.
[21] S. Hassantabar, M. Ahmadi, and A. Sharifi, “Diagnosis and
detection of infected tissue of COVID-19 patients based on lung Xray image using convolutional neural network approaches,” Chaos,
Solitons & Fractals, vol. 140, p. 110170, 2020.
[22] R. Kalkreuth and P. Kaufmann, “COVID-19: A survey on public
medical imaging data resources,” arXiv preprint arXiv:2004.04569,
2020.
[23] J. P. Cohen, P. Morrison, and L. Dao, “COVID-19 image data
collection,” arXiv preprint arXiv:2003.11597, 2020.
[24] L. Lin, G. Fu, S. Chen, and J. Tao, “CT manifestation of coronavirus
disease (COVID-19) pneumonia and influenza virus pneumonia: A
comparative study,” American J. Roentgenology, pp. 1–9, 2020.
[25] “COVID-19 Open Research Dataset (CORD-19),” https://pages:
semanticscholar:org/Coronavirus-research.
[26] A. Imran, I. Posokhova, H. N. Qureshi, U. Masood, S. Riaz, K. Ali,
C. N. John, and M. Nabeel, “AI4COVID-19: AI enabled preliminary
diagnosis for COVID-19 from cough samples via an app,” arXiv
preprint arXiv:2004.01275, 2020.
[27] H. Yin and N. K. Jha, “A health decision support system for disease
diagnosis based on wearable medical sensors and machine learning
ensembles,” IEEE Trans. Multi-Scale Computing Systems, vol. 3, no. 4,
pp. 228–241, 2017.
[28] H. Yin, B. Mukadam, X. Dai, and N. Jha, “DiabDeep: Pervasive
diabetes diagnosis based on wearable medical sensors and efficient
neural networks,” IEEE Trans. Emerging Topics in Computing, 2019.
[29] S. Hassantabar, P. Terway, and N. K. Jha, “TUTOR: Training neural
networks using decision rules as model priors,” arXiv preprint
arXiv:2010.05429, 2020.
[30] X. Dai, H. Yin, and N. K. Jha, “NeST: A neural network synthesis
tool based on a grow-and-prune paradigm,” IEEE Trans. Computers,
vol. 68, no. 10, pp. 1487–1497, Oct. 2019.
[31] S. Hassantabar, Z. Wang, and N. K. Jha, “SCANN: Synthesis of compact and accurate neural networks,” arXiv preprint arXiv:1904.09090,
2019.
[32] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“MobileNet v2: Inverted residuals and linear bottlenecks,” in Proc.
IEEE/CVF Conf. Computer Vision and Pattern Recognition, 2018, pp.
4510–4520.
[33] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, “ShuffleNet v2: Practical
guidelines for efficient CNN architecture design,” arXiv preprint
arXiv:1807.11164, vol. 1, 2018.
[34] B. Wu, A. Wan, X. Yue, P. Jin, S. Zhao, N. Golmant, A. Gholaminejad,
J. Gonzalez, and K. Keutzer, “Shift: A zero flop, zero parameter
alternative to spatial convolutions,” in Proc. IEEE Conf. Computer
Vision and Pattern Recognition, 2018, pp. 9127–9135.
[35] A. Wan, X. Dai, P. Zhang, Z. He, Y. Tian, S. Xie, B. Wu, M. Yu, T. Xu,
K. Chen et al., “FBNetV2: Differentiable neural architecture search
for spatial and channel dimensions,” in Proc. IEEE Conf. Computer
Vision and Pattern Recognition, 2020, pp. 12 965–12 974.
[36] X. Dai, P. Zhang, B. Wu, H. Yin, F. Sun, Y. Wang, M. Dukhan,
Y. Hu, Y. Wu, Y. Jia, P. Vajda, M. Uyttendaele, and N. K. Jha,
“ChamNet: Towards efficient network design through platformaware model adaptation,” in Proc. IEEE Conf. Computer Vision and
Pattern Recognition, 2019.
[37] S. Hassantabar, X. Dai, and N. K. Jha, “STEERAGE: Synthesis of
neural networks using architecture search and grow-and-prune
methods,” arXiv preprint arXiv:1912.05831, 2019.

[38] X. Dai, A. Wan, P. Zhang, B. Wu, Z. He, Z. Wei, K. Chen, Y. Tian,
M. Yu, P. Vajda et al., “FBNetV3: Joint architecture-recipe search
using neural acquisition function,” arXiv preprint arXiv:2006.02049,
2020.
[39] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and
Huffman coding,” arXiv preprint arXiv:1510.00149, 2015.
[40] S. Han, J. Kang, H. Mao, Y. Hu, X. Li, Y. Li, D. Xie, H. Luo, S. Yao,
Y. Wang et al., “ESE: Efficient speech recognition engine with
sparse LSTM on FPGA,” in Proc. ACM/SIGDA Int. Symp. FieldProgrammable Gate Arrays, 2017, pp. 75–84.
[41] X. Dai, H. Yin, and N. K. Jha, “Grow and prune compact, fast, and
accurate LSTMs,” IEEE Trans. Computers, vol. 69, no. 3, pp. 441–452,
Mar. 2020.
[42] C. Zhu, S. Han, H. Mao, and W. J. Dally, “Trained ternary
quantization,” arXiv preprint arXiv:1612.01064, 2016.
[43] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Proc. Advances
in Neural Information Processing Systems, 2012, pp. 1097–1105.
[44] S. Grossberg, “Nonlinear neural networks: Principles, mechanisms,
and architectures,” Neural Networks, vol. 1, no. 1, pp. 17–61, 1988.

