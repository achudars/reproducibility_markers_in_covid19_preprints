Auto-FedAvg: Learnable Federated Averaging for Multi-Institutional Medical
Image Segmentation

1

Johns Hopkins University 2 NVIDIA
3
Japan Self-Defense Forces Central Hospital 4 Xiangyang NO.1 People Hospital
5
National Cancer Institute 6 National Institutes of Health 7 ASST Santi Paolo e Carlo
8
University of Milan 9 Foundation IRCCS Ospedale Maggiore Policlinico Hospital

Abstract
Federated learning (FL) enables collaborative model
training while preserving each participant’s privacy, which
is particularly beneficial to the medical field. FedAvg is
a standard algorithm that uses fixed weights, often originating from the dataset sizes at each client, to aggregate
the distributed learned models on a server during the FL
process. However, non-identical data distribution across
clients, known as the non-i.i.d problem in FL, could make
this assumption for setting fixed aggregation weights suboptimal. In this work, we design a new data-driven approach, namely Auto-FedAvg, where aggregation weights
are dynamically adjusted, depending on data distributions
across data silos and the current training progress of the
models. We disentangle the parameter set into two parts, local model parameters and global aggregation parameters,
and update them iteratively with a communication-efficient
algorithm. We first show the validity of our approach by
outperforming state-of-the-art FL methods for image recognition on a heterogeneous data split of CIFAR-10. Furthermore, we demonstrate our algorithm’s effectiveness on
two multi-institutional medical image analysis tasks, i.e.,
COVID-19 lesion segmentation in chest CT and pancreas
segmentation in abdominal CT.

1. Introduction
Federated Learning (FL) [1, 2, 3] is a machine learning paradigm where clients collaboratively train a model
without exchanging the underlying raw data. Compared
to traditional centralized training, FL aims to benefit each
participant while mitigating the potential for violating data
* Work

done during an internship at Nvidia

Federated Server
Local
Data

𝑤 ← ෍ 𝛼𝑘 𝑤𝑘
𝑘

𝛼1
𝛼2

Local
Data

𝛼3
Fixed
aggregation
weights

Local
Data

Global
model
𝑤

FedAvg
𝛼1

Local
Data

𝛼2
𝛼3
𝛼1

Local
Data

𝛼2
𝛼3
𝛼1

Local
Data

𝛼2
𝛼3

Federated Server
𝑤 ← ෍ 𝛼𝑘 𝑤𝑘
𝑘

𝛼1 𝛼2 𝛼3

𝛼1
𝛼1 𝛼2 𝛼3

𝛼2
𝛼3
Learnable
aggregation
weights

Global
model
𝑤

Auto-FedAvg

Time

arXiv:2104.10195v1 [eess.IV] 20 Apr 2021

Yingda Xia1 *, Dong Yang2 , Wenqi Li2 , Andriy Myronenko2 , Daguang Xu2 , Hirofumi Obinata3 ,
Hitoshi Mori3 , Peng An4 , Stephanie Harmon5 , Evrim Turkbey6 , Baris Turkbey5 , Bradford Wood6 ,
Francesca Patella7 , Elvira Stellato8 , Gianpaolo Carrafiello8 , Anna Ierardi9 , Alan Yuille1 , Holger Roth2

𝛼1 𝛼2 𝛼3

𝛼1 𝛼2 𝛼3

Figure 1: An illustration of FedAvg (top) and Auto-FedAvg
(bottom). In FedAvg, the server collects locally trained
models from each client and obtains a global model by
weighted averaging with fixed aggregation weights. In contrast, in Auto-FedAvg, the aggregation weights are learned
on the clients and dynamically adjusted throughout the
training process when communicating with the server.
privacy. FL was initially designed for mobile and edge
devices [1] involving thousands of clients with often interrupted connectivity and only relatively small data each.
However, recent studies involving only a small number of
relatively reliable clients, e.g., medical institutions, have
raised interest in utilizing FL for healthcare applications
[4]. The latter scenario is referred to as “cross-silo” FL in
Kairouz et al. [5] and is the focus of this paper.
Federated averaging (FedAvg) [1] is a simple yet effective algorithm for federated learning, following a serverclient setup with two repeated stages: (i) the clients train
their models locally on their data, and (ii) the server col-

lects and aggregates the models to obtain a global model
by weighted averaging. The aggregation weight of FedAvg
is usually determined by the number of data samples on
each client. This design choice assumes that data is uniformly distributed on the clients, and a stochastic gradient
descent (SGD) optimizer is enforced. However, this setting can hardly be optimal and even detrimental because the
clients’ underlying data distributions remain unknown and
are most likely non-independent and identically distributed
(non-i.i.d). Domain shifts in the data are expected among
different clients in real-world scenarios.
In this paper, we aim to improve FedAvg by automatically learning how to aggregate different client models
more optimally. Our approach, namely Auto-FedAvg, is
data-driven and differentiable while keeping the privacypreserving aspects of FL. Recall that FedAvg involves two
iterative steps. Our approach introduces a third step. After the clients finish training their local models, we learn
a set of global aggregation weights in a data driven fashion, which the server later uses in the weighted average for
computing the global model. Learning the global aggregation weights is beneficial in two aspects: (i) Since the convergence rate is likely to be different across the clients, dynamically adjusting aggregation weights can accelerate the
training process. (ii) Better performance and generalizability can be achieved because the global model is more robust
when applied to all the client’s test data since we directly
optimize the local loss to update the aggregation weights
by modelling them as a stochastic process utilizing the
Dirichlet distribution. We also designed a communicationefficient algorithm to achieve this goal without violating the
data privacy constraint of FL.
We first validate the effectiveness of our approach on the
CIFAR-10 dataset, where we outperform the state-of-theart method, FedMA [6] by 1.45% using the same heterogeneous data partitioning. Moreover, we outperform the FedAvg algorithm on two medical image segmentation tasks,
i.e., multi-institutional and multi-national COVID-19 lesion
segmentation and pancreas segmentation, showing its realworld potential.
Our contributions are summarized as follows:
• We propose to directly learn the model aggregation
weights in FL from data with gradient descent using
a Dirichlet distribution, which is adaptive to the underlying data and learning progress.
• We design a new communication algorithm to fulfill
the proposed goal with limited extra communication
cost in cross-silo FL and without violating the data privacy constraints of FL.
• We outperform state-of-the-art approaches on a heterogeneous data split of CIFAR-10. Furthermore, we ex-

tensively analyze the proposed algorithm on two multiinstitutional medical imaging studies with real-world
datasets.

2. Related Work
Federated Learning. Here, we introduce some common
algorithms for FL. Federated Averaging (FedAvg) [1] is a
standard algorithm, where parameters of local models are
averaged with fixed weights to obtain a global model. The
aggregation weight of each client is usually set to be proportional to the size of client’s dataset. FedMA [6] refined
the aggregation process by matching and averaging hidden
elements with similar feature signatures. The idea of integrating knowledge distillation into FL has also been explored [7, 8].
Recently, the issue of FL on non-i.i.d data draws emerging attentions. Several works have been proposed to address
data heterogeneity in FL settings [9, 10, 11, 12, 13], among
which one direction is to optimize the process of model aggregation that we also consider in this paper. For example,
Wang et al. [14] proposed a normalized averaging method
that eliminates objective inconsistency while preserving fast
convergence for heterogeneous data clients. Chen et al. [13]
analyzed median-based FL algorithms. Agnostic Federated
Learning [15] proposed to optimize a centralized model for
any target distribution formed by a mixture of the client distributions. FedBE [16] learns a Bayesian ensemble from the
distribution of the models. These works explore statistics
or underlying distribution of the models to adjust aggregation strategies. In contrast, we propose to directly learn the
aggregation weights by gradient-based optimization on the
clients’ data. Other recent works also discuss the possibility
for model personalization [17, 18, 19]. Most recent works
demonstrate good theoretical analysis but are only evaluated on manually created toy examples. It is not clear if
the approaches would generalize well to real-world medical
imaging datasets such as those studied in this work.
Multi-institutional Medical Image Analysis. Due to its
privacy-preserving attributes, FL is particularly attractive
for the medical domain. Rieke et al. [4] discussed the potential of FL in digital health. Meanwhile, multiple realworld investigations of FL have been applied to medical
image analysis, which is itself a well-explored field with
deep learning [20, 21, 22]. Examples of FL in medical
imaging include multi-institutional brain tumor segmentation [23, 24], breast density classification [25] and fMRI
analysis [26]. In addition to FL settings, Chang et al. [27]
synthesized medical images with a GAN [28] without sharing data between institutions. On top of privacy concerns,
Liu et al. [29], Dou et al. [30] and Xia et al. [31] emphasized
the challenge of domain shift for multi-institutional medical
data and developed algorithms to solve domain adaptation

and generalization problems in prostate segmentation, brain
tissue segmentation and liver segmentation from multi-site
medical images, respectively. However, these non-i.i.d.
challenges have not been resolved in FL for medical imaging [4].
Automated Machine Learning. This paper introduces an
automated approach to find the best aggregation weights for
federated learning. Our approach is inspired by recent advances of automated machine learning (AutoML), including hyper-parameter search [32, 33, 34], neural architecture
search (NAS) with reinforcement learning [35, 36], evolution algorithm [37, 38] and differentiable approaches [39,
40]. A recent approach [41] improves NAS by modeling the
architecture mixing weight using a Dirichlet distribution, a
mathematical formulation that we also utilize in this work.
In the broad sense of AutoML, our approach can also be
categorized as a differentiable hyper-parameter search algorithm in the continuous search space of FedAvg aggregation
weights.

samples on each client (αk = nnk ) as mentioned before. We
pick C = K for simplicity and the update of the
P global
model w in each communication round as w ← k nnk wk
where wk is the current model of client k.
The aggregation weights chosen by vanilla FedAvg is
based on the assumption that data follows a uniform distribution across clients and are computed based on the number
of SGD steps performed on each client. However, since the
data distribution at each client is unknown and could possibly be non-i.i.d or involve domain shifts, this assumption is
not guaranteed and can result in sub-optimal or even detrimental effects.

3.2. Optimization Objectives
To counteract the limitations of FedAvg, we propose
our differentiable approach to directly learn the aggregation
weights α from data at the clients. Denote by L the loss
function. We propose a constrained objective function in:
min

3. Auto-FedAvg

α

In this section, we first describe the general notations of
federated learning and revisit FedAvg [1] in Sec 3.1. We
then introduce our optimization objective in Sec 3.2, where
we will also introduce how we parameterize the aggregation weights to follow certain constraints, as well as variants
of the aggregation strategies, i.e., network-wise and layerwise. Finally, in Sec 3.3, we describe our full algorithm in
detail and analyze the communication cost of the proposed
Auto-FedAvg approach.

3.1. Revisiting FedAvg
Suppose K clients collaboratively train a global model
with parameter w in a standard FL setting. In particular, the
aim is to minimize:
K
X
min
αk Lk (w),
(1)
w

k=1

where
P Lk (w) is the local loss function of client k, αk ≥
0 and k αk = 1. Suppose there are nk data samples
P on
client k, then we usually set αk = nnk , where n = k nk
is the total number of data samples used in the FL setting.
To relieve the communication burden, FedAvg [1] allows
the clients to update their local models for a certain period
of time with the stochastic gradient descent (SGD) optimizer. We denote the local loss function given a data sample x and the current model weight w as l(w, x). The server
then collects C models (C ≤ K), aggregates them with
weighted averaging to update the global model, and sends
the new global model back to the clients for re-initialization
of next round of FL training. The aggregation weights
α ∈ RK are set to be proportional to the number of data

K
X
k=1

s. t.

K
X

Lk (

K
X

αk wk )

k=1

αk = 1 and αk > 0,

(2)

k=1

where wk = arg minLk (w) is the local model updated on
w

the training set of client k. The motivation of the proposed
objective is that we directly learn the aggregation weight by
gradient descent from data in a differentiable way, while
keeping the local models fixed after completing their local training. Since there is no data sharing between clients,
we will introduce a communication algorithm to achieve the
learning objective later in Sec 3.3. We first discuss the variants of the constraints of Eq. 2 as follows.
Constraints of the aggregation weights. Here, we provide two assumptions for the optimization constraints in
Eq. 2. To achieve these constraints, we introduce a new
set of variable β = [β1 , .., βK ], which is a vector with the
same dimension as α = [α1 , .., αK ]. We define a function
γ to transform β to α :
β)
α = γ(β

(3)

Softmax function. One obvious choice to satisfy the constraint of α is to apply a softmax function to β
exp (βk )
αk = PK
i=1 exp (βi )

(4)

PK
Thus, the loss function becomes l( k=1 αk wk , x) =
β , x), which only depends on β and x, since we keep the
L(β
model weights wk fixed in the aggregation weight learning
process (Eq. 2). In practice, we can compute the gradient of

each βk and directly update them based on a client’s local
data with gradient descent.
Dirichlet distribution. A better choice is to treat the aggregation weight α as random variables, modeled by the
Dirichlet distribution parameterized by the concentration β :
β ). This formulation induces stochasticity that
α ∼ Dir(β
naturally encourages exploration in the search space during
the sampling process in training. The probability density
function is formed as:
K
1 Y βk −1
α|β
β) =
αk
,
Dir(α
β)
B(β

(5)

k=1

QK
R∞
Γ(βk )
PK
β ) = Γ(k=1
where B(β
and Γ(z) = 0 xz−1 e−x dx
k=1 βk )
is the gamma function. The Dirichlet distribution is the
conjugate prior of a multinomial distribution with a simplex. Each sample will already satisfy our constraint of the
aggregation weights in Eq. 2. Thus we find the Dirichlet
distribution to be a natural formulation to model the aggregation weights during FL while utilizing its properties for
gradient-based optimization [41, 42]. It is also worth mentioning that the uniform distribution is a special case of the
Dirichlet distribution when α1 = α2 = ... = αK = 1.
In the training phase, given a data sample x, we sample
α from the Dirichlet distribution with concentration β , apβ , x)
proximate the gradient of β given the loss function L(β
using implicit reparameterization [43] and update the concentration β . During inference, we compute the mode of
the distribution, which represents the values with maximum
probability.

βk − 1
αk = PK
i=1 βi − K

(6)

Aggregation strategies. In the process of model aggregation, our approach introduces more flexibility in terms of
the design of the aggregation weights than FedAvg, because
we are able to learn the parameterized aggregation weights
in a differentiable way from data. Here, we describe two
natural variants.
Network-wise aggregation weights. In this scenario, each
aggregation weight αk in α is a scalar. The aggregation
proP
cess is the same as described previously: w ← k αk wk .
Layer-wise aggregation weights. Our approach allows an
easy extension to network-wise aggregation, namely layerwise aggregation. Suppose the deep network model we are
training has P layers. We denote wk,p as the p-th layer parameter of the model of client k. Then αk = [αk,1 , .., αk,P ]
is a P -dimensional vector. Thus
we are able to obtain the
PK
p-th layer weight wp by wp ← k=1 αk,p wk,p .
As for the constraints discussed previously, βk =
[βk,1 , .., βk,P ] is now a P -dimensional vector as well. Then,

Algorithm 1 Auto-FedAvg. We denote the total number
of rounds as T , the interval to learn aggregation weights
as t0 , local training iterations for client k as Mk , and the
aggregation weight learning iterations as S.
Server executes:
t
t
Define α t = [α1t , .., αK
], β t = [β1t , .., βK
].
0
0
0
0
β )
Initialize w and β . α = γ(β
for t ← 1, ..., T do
for k ← 1, ..., K in parallel do
wkt ← LocalTrain(k, wt−1 )
if t mod t0 = 0 then
t
β t ← LearnAggWeight(w1t , .., wK
, β t−1 )
t
t
β )
α ← γ(β
else
t−1
αt ←
PαK
t
w ← k=1 αkt wkt
return wT
LocalTrain(k, w):
for t ← 1, .., Mk do
Sample batch x from client k’s training data
Compute loss l(w; x)
Compute gradient of w and update w
return w
LearnAggWeight(w1 , .., wK , β 0 ):
for k ← 1, ..., K do
Server send w1 , .., wk−1 , wk+1 , .., wK to client k
for s ← 1, .., S do
for k ← 1, ..., K in parallel do
Sever send β s−1 to client k
Sample batch x from client k’s local data
β s−1 ; x)
Compute loss L(β
Compute/estimate gradient and update β s−1 as β s,k
s,k
Send βP
back to the server
K
1
s,k
βs ← K
k=1 β
S
return β

exp (β

)

αk,p = PK expk,p
is the equation when using softmax,
(βk,p )
k=1
β p ) when applying the Dirichlet distribution.
and α p ∼ Dir(β

3.3. Algorithm
Optimizing the objective function in Eq. 2 is not trivial
under the FL setting, since (i) we can only rely on the local data on each client which is inaccessible to the server,
and (ii) we would like to maintain a relatively low communication cost. We describe the algorithm of Auto-FedAvg
in Algorithm 1. In each communication round t, the server
first sends out the global model to all the clients. When
the clients finish updating the local models in parallel, the
server gathers them and aggregates the models with a set
t
of learnable weights α t = [α1t , .., αK
] by weighted averag-

ing to obtain an updated global model wt . α t is parameterized by β t using function γ and the actual instantiation of γ
in Eq. 3 is determined by whether we use softmax (Eq. 4)
or the Dirichlet distribution (Eq. 6) as the method to parameterize α. The learning process of β t is described in
LearnAggWeight of Algorithm 1.
In LearnAggWeight, each client receives a copy of
all the model weights w1 , .., wK and keeps them fixed during this process. In each local iteration s, each client
samples a mini-batch x from their own local data, and
computes the current α from β s−1 depending on the softmax or Dirichlet assumption we apply to the aggregation
weights,Pbefore forwarding x into the local model with
K
weight k=1 αk wk . Then the client will compute the loss
β s−1 , x) and update β s,k based on the compufunction L(β
tation (softmax) or estimation (Dirichlet distribution) of the
gradient [43], as mentioned in Sec 3.2. The server will
gather β s,k from every client k in every iteration s and average them to obtain a new global β s .
Communication efficiency analysis. The communication of β is very efficient because β is merely a set of K
scalars or K low dimensional vectors (of size P ) in either
“network-wise aggregation weight” or “layer-wise aggregation weight” strategy, which is negligible compared to
communicating the full network parameters as in a standard
FedAvg round. The major extra communication burden of
aggregation weight learning is introduced when the server
sends all local models to each client in the very first step. As
a result, we only do the aggregation weight learning process
every t0 rounds to further relieve the additional communication burden compared to FedAvg. The extra communication cost ratio (extra cost divided by FedAvg communication cost) is K−1
2t0 . A detailed derivation of which can be
found in the supplementary material. This is more acceptable in cross-silo federated learning setting, which typically
contains only a small number of clients with relatively reliable internet connectivity [5]. For example, in our COVID19 lesion segmentation experiments, K = 3 and t0 = 10,
results in an extra 10% communication cost compared to
FedAvg.

4. Experiments
4.1. CIFAR-10
We first validate our approach on the CIFAR-10 dataset.
To compare our approach with the state-of-the-art FL methods such as FedProx [44] and FedMA [6] on the benchmark
dataset, we use the same heterogeneous data partition of
FedMA [6] on the CIFAR-10 dataset that simulates an environment where the number of data points and class proportions are unbalanced using their publicly available code1 .
In this way, we can directly compare with the results in the
1 https://github.com/IBM/FedMA

Table 1: CIFAR-10 classification with heterogeneous partition. Baseline numbers are from [6] on the same data split.
Method
FedAvg
FedProx [44]
Ensemble
FedMA [6]
FedMA [6] (our impl.)

∗

final accuracy(%)
86.29
85.32
75.29
87.53
87.47

Auto-FedAvg-L-Softmax*
88.64
Auto-FedAvg-L-Dirichlet*
88.37
88.60
Auto-FedAvg-N-Softmax*
Auto-FedAvg-N-Drichlet*
88.98
With the interval of aggregation weight learning t0 = 10.

paper, which are shown in Table 1. We train the baseline
and our experiments for 99 rounds with 16 clients before
we test on the test set, where the same network architecture
of VGG-9 is adopted. The re-implementation of FedMA
achieves 87.47% accuracy, which is very close to the reported performance 87.53% [6], indicating the correctness
of our experimental setup. For our Auto-FedAvg algorithm,
we experiment with different design choices described in
Sec 3.2, i.e. layer-wise (“L”) or network-wise (“N”) aggregation strategy and softmax (“Softmax”) or Dirichlet assumption (“Dirichlet”) over the constraints of the aggregation weights. Based on the metric of final accuracy, all
our experimental variants outperform the baselines and our
“Auto-FedAvg-N-Dirichlet” achieved the best final accuracy of 88.98%, outperforming the published FedMA result
by 1.45%.

4.2. Multi-national COVID-19 lesion segmentation
4.2.1

Experimental results

The study with first real-world data of our federated learning algorithm is COVID-19 diagnosis, which has caused a
world-wide pandemic in the year of 2020 and 2021. Machine learning based algorithms have been developed to
quickly diagnose the disease and study the imaging characteristics [45, 46, 47]. In this study, we focus on the critical
task of COVID-19 lesion segmentation on multi-national
COVID-19 datasets.
Dataset description. This study contains CT scans of
SARS-CoV-2 infected patients collected from three international medical centers, including (i) 671 scans from
[anonymized hospitals] in China (denoted as Dataset I), (ii)
88 scans from [anonymized hospitals] in Japan (denoted as
Dataset II), and (iii) 186 scans from [anonymized hospitals]
in Italy (denoted as Dataset III). Two expert radiologists annotated these CT scans assigning a foreground (COVID19 lesion) and background label for each voxel. For each
dataset, we randomly split the annotated cases into training/validation/testing, resulting in splits of 447/112/112 for
Dataset I, 30/29/29 for Dataset II, and 124/31/31 for Dataset
III. We visualize examples in Fig 2 and show the intrinsic

Table 2: Multi-national COVID-19 lesion segmentation. “Global test avg” is the major metric to measure the generalizability
of the FL global model. n specifies the total dataset size at the client.
Method

I (n=671)

II (n=88)

III (n=186)

global test avg

local avg

local gen

Local only - I
Local only - II
Local only - III

59.82
41.92
34.50

61.82
59.95
52.54

51.80
50.18
65.85

57.81
50.68
50.96

61.87

48.79

FedAvg
FedAvg - even
FedProx

59.93
56.73
60.33

63.79
64.31
64.98

60.52
64.98
60.45

61.41 ±0.19
62.01 ±0.30
61.92 ±0.53

62.47
62.24
61.99

58.80
59.28
58.33

Auto-FedAvg-L-Softmax
Auto-FedAvg-L-Dirichlet
Auto-FedAvg-N-Softmax
Auto-FedAvg-N-Dirichlet
Auto-FedAvg-N-Dirichlet*

59.03
58.59
59.58
60.37
60.42

64.96†
61.66†
61.89 ±0.54
64.95†
64.96†
62.83 ±0.14
64.50†
63.35†
62.48 ±0.24
65.28†
64.76†
63.47† ±0.22
64.86†
64.07†
63.11† ±0.33
With the interval of aggregation weight learning t0 = 10.
† Significance of the global model over FedAvg.

63.17
63.08
63.42
64.04
63.74

58.96
59.51
59.62
60.79
60.23

∗

aimed to handle the problem of foreground-background imbalance. The architecture of the segmentation network is
3D U-Net [20, 22]. In the training process, we resample each CT volume to a fixed voxel spacing of (0.8mm,
0.8mm, 5mm) and randomly crop region of interests (ROIs)
of 256 × 256 × 32. In the testing phase, we adopt a sliding
window scheme with a stride of (64, 64, 8) and resample to
the original voxel spacing for final evaluation.
image

label

FedAvg

Auto-FedAvg

Figure 2: Examples of COVID-19 lesion segmentation of
patients from China (top) and Italy (bottom). From left to
right: original CT scan, human label (in green), FedAvg
segmentation results, and our segmentation results. Our
Auto-FedAvg mitigates the issue of under-segmentation
(top) and reduces false-positive prediction (bottom) in these
two examples, respectively.
domain shift between datasets (e.g., caused by resolution
and contrast).
Implementation details. We simulate a federated learning
scenario, in which each dataset represents one FL client,
and ensure no data is transferred among the clients. In all
FL experiments, we fix the number of total communication
rounds T to 300 and validate on the local validation sets
in each round to select the best local models (with highest
validation score on each single client) and the global model
(with highest average validation accuracy over three validation sets). In our experiments, we initialize the concentration β of the Dirichlet distribution as (6.0, 6.0, 6.0). In
the local training process of each client, we adopt the Adam
optimizer [48] with a learning rate of 0.0001, (β1 , β2 ) as
(0.5, 0.99), and no decay. Each training round performs
300 iterations with a batch size of 4. These hyperparameters are tuned to achieve the best local performances. Dice
loss [21] is used as the training objective, which is a widelyapplied loss function in medical image segmentation and

Evaluation metrics. We measure the performance of the
segmentation models by Dice similarity coefficient (DSC),
a standard evaluation metric used for medical image segmentation. For all the FL experiments, we test the performance of the best global model, selected by highest average validation accuracy of all three clients, on the test
data of each client, corresponding to the first three columns
(I/II/III) of Table 2. We compute the average of the three
test accuracies to measure the average performance of the
model on three datasets, corresponding to the forth column
“global test avg”. This metric represents a measure for the
generalizability of the global model, and serves as the major metric for performance evaluation. Moreover, we test
the best local models on all clients, selected by the highest
local validation score, resulting in a 3x3 matrix of scores.
The on-diagonal scores represent the local performances of
each model, which is averaged as “local avg” in column
five. The off-diagonal scores represent the generalization
performance of each model, which is averaged as “local
gen” in column six. We run three repeats of each configuration of the FL experiments and report the standard deviation
on “global test avg” to measure the stability of our results.
Results. We display the quantitative results in Table 2 and
two examples for qualitative analysis in Fig 2. We first
train the models locally without communication to obtain
the baselines of the local models, shown in the first three
rows in the table. Unsurprisingly, all three local models have relatively low generalization performance when
tested on other clients, indicating domain shifts across the

1
2
3

0

100

round

200

300

(a) The learning curve of α.

global val acc (%)

0.6
0.5
0.4
0.3
0.2

60
55
50
45
40

III

probability density

low

0

100

round

FedAvg
Auto-FedAvg
200
300

(b) Validation accuracy growth.

high

round

III

III

I

II

II I

I

0

30

III

II
150

I

II
300

𝛽

(6.0, 6.0, 6.0)

(6.4, 8.0, 6.4)

(13.4,6.0,6.4)

(18.3, 5.3, 6.9)

𝛼

(0.33,0.33,0.33)

(0.30,0.39,0.31)

(0.54,0.22,0.24)

(0.63,0.16,0.21)

(c) Visualizations of Dirichlet distribution.

Figure 3: Analysis of the learning process during “Auto-FedAvg-N-Dichlet”.

4.2.2 Analysis studies
Learning process. Here, we aim to analyze the learning
process of Auto-FedAvg. The learning curve of the aggregation weights α, validation accuracy growth, and the
visualization of the Dirichlet distribution are displayed in
Fig. 3. The sub-figures correspond to our best performing

model “Auto-FedAvg-N-Dichlet” in Table 2. As shown in
Fig. 3a, in the first 30 rounds, α2 and α3 rise moderately,
indicating the global model could benefit from increasing
the weight of the models from client II and client III in the
early stage. This matches our expectation that client II and
client III converge faster than client I because client II and
client III own significantly less data than client I. Giving
them more weight in the aggregation process accelerates
the training process. As shown in Fig 3b, our approach has
a faster growth in validation score than FedAvg. After approximately 40 rounds, we observe a rise of α1 and drops of
α2 and α3 , indicating that assigning higher weights to client
I benefits the global model eventually, making it more generalizable across different clients.
In terms of the latent Dirichlet distribution of α (shown
in Fig 3c), we plot the different states of α as well as the
latent variable β in round 0, 30, 50, and 300. Interestingly,
the distribution becomes more concentrated with a smaller
variance in round 300 compared to that of round 0. We
interpret it as a higher certainty of the aggregation weights
in the end of the training process than that in the beginning
(starting from an initialization with β = (6.0, 6.0, 6.0)).
The effect of interval of the aggregation weight learning.
We plot the global test accuracy under different aggregation weight learning intervals t0 in Fig. 4.
The overall ten64
dency is that the
performance will de63
grade if we increase
62
the interval t0 . It is
worth
mentioning
61 1 2 5 10 20 50
that an interval of
interval of aggregation weight learning t0
t0 = 10 will only
add to 10% extra
communication cost Figure 4: The impact of the aggrewith merely a 0.36% gation weight learning interval t0 .
decline in performance compared to t0 = 1. In terms
of future applications of our approach, we expect no
concern with such additional communication burden as
medical institutions usually have relatively reliable network
connectivity.
Re-initialize aggregation weights before learning each

global test acc (%)

three datasets. For the FedAvg baseline, we experiment
with two different sets of aggregation weights, i.e., normalized dataset size and uniform weights, denoted by “FedAvg” and “FedAvg-even”, respectively. We also implement FedProx [44] with the empirically best µ = 0.001.
For our Auto-FedAvg algorithm, we experiment with different design choices described in Sec 3.2, i.e. layer-wise
(“L”) or network-wise (“N”) aggregation strategy and softmax (“Softmax”) or Dirichlet assumption (“Dirichlet”) over
the constraints of the aggregation weights. We find that
“Auto-FedAvg-N-Dirichlet” gives the best results, outperforming “FedAvg” by 2.06% on general global model performance (column ”global test avg”), by 1.57% on average local model performance (column “local avg”), and by
1.99% on local model generalization (column “local gen”).
We furthermore performed a Wilcoxon signed rank test on
the test set (first four columns), where the significant improvements (p  0.05) over FedAvg are marked with superscript † .
Generally speaking, the Dirichlet distribution performs
better at modeling the aggregation weights than softmax.
Interestingly, the performance of the layer-wise aggregation strategy is worse than the network-wise aggregation
strategy. The gradient of network-wise aggregation weights
can be viewed as a summation of all gradients of layerwise aggregation weights. In this sense, we suspect that
network-wise aggregation acts as a regularization of layerwise weights. We also conduct diagnosis experiments and
provided them in supplementary materials, where we display the patterns of the learned layer-wise weights and suggest a layer-wise smoothing loss can improve the results of
the layer-wise aggregation strategy. The improvement of
the layer-wise smoothing loss for the layer-wise aggregation
strategy further serves as evidence that the network-wise aggregation may act as regularization over the layer-wise one.

Table 3: Multi-institutional pancreas segmentation. “Global test avg” is the major metric to measure the generalizability of
the FL global model. n specifies the total dataset size at the client.
Method

I (n=281)

II (n=82)

III (n=30)

global test avg

local avg

local gen

Local only - I
Local only - II
Local only - III

69.43
49.69
42.35

71.38
75.47
61.18

63.79
53.02
51.08

68.20
59.39
51.34

65.32

56.9

FedAvg
FedAvg - even
FedProx

71.85
69.18
71.96

78.36
78.82
78.35

69.12
70.91
69.57

73.11 ±0.17
72.97 ±0.14
73.29 ±0.25

72.84
73.49
73.66

70.75
71.33
70.79

Auto-FedAvg-L-Softmax
Auto-FedAvg-L-Dirichlet
Auto-FedAvg-N-Softmax
Auto-FedAvg-N-Dirichlet
Auto-FedAvg-N-Dirichlet*

71.22
71.06
70.40
71.20
71.26

78.38
71.04†
73.54 ±0.28
79.60†
70.56†
73.74 ±0.34
78.80
70.48†
73.22 ±0.21
79.30†
71.19†
73.90 ±0.25
79.90†
71.49†
74.21 ±0.28
With the interval of aggregation weight learning t0 = 5.
† Significance of the global model over FedAvg.

73.92
74.17
74.02
74.25
74.33

71.49
71.68
71.50
71.83
72.30

∗

round. Before each round of learning aggregation weights,
we reuse the previously learned α from the last round as initialization. Another variant could be re-initializing α to be
(0.33,0.33,0.33) before each round of aggregation weights
learning (see LearnAggWeight, Alg. 1). We conduct
an experiment under this setting named “Auto-FedAvg-NDirichlet*” in Table 2 and find the performance of test accuracy drops to 61.84% compared to the original 63.11%.
We speculate that this is because reusing the previous α not
only offers a good starting point of the current round but
also accelerates the learning process.

4.3. Multi-institutional Pancreas Segmentation
Dataset description. In this experiment, we study pancreas
segmentation from CT scans, which is an important prerequisite of pancreatic tumor detection and surgical planning [49]. We use the provided annotations from three
public datasets, i.e., (i) pancreas segmentation subset of
Medical Segmentation Decathlon [50] which contains 281
cases (denoted as Dataset I), (ii) the Cancer Image Archive
(TCIA) Pancreas-CT dataset [51] which contains 82 cases
(denoted as Dataset II), and (iii) Beyond the Cranial Vault
(BTCV) Abdomen data set [52] which contains 30 cases
(denoted as Dataset III). All the data include manual per
voxel annotations of the pancreas from radiologists. For
each dataset, we randomly split the annotated cases into
training/validation/test sets, which are 95/93/93 for Dataset
I, 28/27/27 for Dataset II, and 10/10/10 for Dataset III.
Implementation details. In all FL experiments, we fix the
number of total communication rounds to 50. In the local
training process of each client, we adopt an initial learning
rate of 0.001 with a cosine learning rate decay and with a
batch size of 16. Same as the Covid-19 study, these hyperparameters are tuned to achieve the best local performances.
Results. We keep the same notation of our experiments as
in Sec. 4.2. We found the conclusions are the same: our
Auto-FedAvg outperforms FedAvg in all metrics and “Auto-

FedAvg-N-Dirichlet” is the best in both local performance
and generalizability, indicating that the network-wise aggregation and using the Dirichlet distribution to model aggregation weights produce the best results. The conclusion
is the same as of COVID dataset that network-wise formulation is better than layer-wise formulation and Dirichlet models aggregation weights better than the softmax. As
for the significance test (Wilcoxon signed rank test) of the
global model, we achieve significant improvements for two
of three clients (datasets II and III), while the performance
stays comparable to FedAvg for I (with no significant difference). Interestingly, we find that with interval t0 = 5, as
denoted as “Auto-FedAvg-N-Dirichlet*”, the performance
is even better than its t0 = 1 counterpart (Auto-FedAvg-NDirichlet). This could result from the benefit of stabilization
when the server keeps the aggregation weights fixed during
the interval.

5. Conclusions, Limitations, and Future Work
In this paper, we introduced Auto-FedAvg, which improves the standard federated learning (FL) algorithm, FedAvg, by automatically and dynamically learning the aggregation weights instead of keeping them fixed. We also proposed a communication-efficient algorithm that alternates
updates between the local model weights and the global aggregation weights. We further explored different constraints
over the aggregation weights and variants of aggregation
strategies. Experiments on the Cifar-10 and two extensive
studies on real-world medical image analysis datasets illustrate the effectiveness of our approach.
One limitation of our algorithm is that relatively stable connections between the server and each of the clients
are necessary. This is feasible in our “cross-silo” situation but could be problematic in “cross-device” scenarios
where new edge devices regularly drop in or out [5]. As
a result, decreasing the communication frequency and integrating mechanisms for tolerating regular disconnections

are two directions to improve the scalability of the current
design. Our algorithm also introduced a general and flexible
means to boost the performance of FL by updating a small
number of global parameters and could be combined with
differential privacy techniques for added protection against
potential inversion attacks [23, 53]. Here, we explored only
the network-wise and layer-wise learning of aggregation
weights. Future research can include more complex aggregation operations and additional parameters to allow further
personalization for accounting for non-i.i.d data and domain
shift cases in FL.

References
[1] Brendan McMahan, Eider Moore, Daniel Ramage, Seth
Hampson, and Blaise Aguera y Arcas. Communicationefficient learning of deep networks from decentralized data.
In Artificial Intelligence and Statistics, pages 1273–1282.
PMLR, 2017. 1, 2, 3
[2] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.
Federated machine learning: Concept and applications. ACM
Transactions on Intelligent Systems and Technology (TIST),
10(2):1–19, 2019. 1
[3] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia
Smith. Federated learning: Challenges, methods, and future
directions. IEEE Signal Processing Magazine, 37(3):50–60,
2020. 1
[4] Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari,
Holger Roth, Shadi Albarqouni, Spyridon Bakas, Mathieu N
Galtier, Bennett Landman, Klaus Maier-Hein, et al. The future of digital health with federated learning. npj Digit. Med.,
3:119, 2020. 1, 2, 3
[5] Peter Kairouz, H Brendan McMahan, Brendan Avent,
Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Keith
Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019. 1, 5, 8
[6] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federated learning
with matched averaging. ICLR, 2020. 2, 5
[7] Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park,
Mehdi Bennis, and Seong-Lyun Kim. Communicationefficient on-device machine learning: Federated distillation
and augmentation under non-iid private data. arXiv preprint
arXiv:1811.11479, 2018. 2
[8] Daliang Li and Junpu Wang.
Fedmd: Heterogenous
federated learning via model distillation. arXiv preprint
arXiv:1910.03581, 2019. 2
[9] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon
Civin, and Vikas Chandra. Federated learning with non-iid
data. arXiv preprint arXiv:1806.00582, 2018. 2
[10] Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and
Wojciech Samek. Robust and communication-efficient federated learning from non-iid data. IEEE transactions on neural networks and learning systems, 2019. 2

[11] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and
Zhihua Zhang. On the convergence of fedavg on non-iid
data. arXiv preprint arXiv:1907.02189, 2019. 2
[12] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri,
Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha
Suresh. Scaffold: Stochastic controlled averaging for federated learning. ICML, 2020. 2
[13] Xiangyi Chen, Tiancong Chen, Haoran Sun, Zhiwei Steven
Wu, and Mingyi Hong. Distributed training with heterogeneous data: Bridging median- and mean-based algorithms.
NeurIPS, 2020. 2
[14] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and
H. Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. NeurIPS,
2020. 2
[15] Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.
Agnostic federated learning. ICML, 2019. 2
[16] Hong-You Chen and Wei-Lun Chao. Fedbe: Making
bayesian model ensemble applicable to federated learning.
arXiv preprint arXiv:2009.01974, 2020. 2
[17] Filip Hanzely, Slavomı́r Hanzely, Samuel Horváth, and Peter
Richtárik. Lower bounds and optimal algorithms for personalized federated learning. NeurIPS, 2020. 2
[18] Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen.
Personalized federated learning with moreau envelopes.
NeurIPS, 2020. 2
[19] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad
Mahdavi. Adaptive personalized federated learning. arXiv
preprint arXiv:2003.13461, 2020. 2
[20] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234–241.
Springer, 2015. 2, 6
[21] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 fourth international
conference on 3D vision (3DV), pages 565–571. IEEE, 2016.
2, 6
[22] Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp,
Thomas Brox, and Olaf Ronneberger. 3d u-net: learning
dense volumetric segmentation from sparse annotation. In
International conference on medical image computing and
computer-assisted intervention, pages 424–432. Springer,
2016. 2, 6
[23] Wenqi Li, Fausto Milletarı̀, Daguang Xu, Nicola Rieke,
Jonny Hancox, Wentao Zhu, Maximilian Baust, Yan Cheng,
Sébastien Ourselin, M Jorge Cardoso, et al. Privacypreserving federated brain tumour segmentation. In International Workshop on Machine Learning in Medical Imaging,
pages 133–141. Springer, 2019. 2, 9
[24] Micah J Sheller, Brandon Edwards, G Anthony Reina, Jason Martin, Sarthak Pati, Aikaterini Kotrotsou, Mikhail
Milchenko, Weilin Xu, Daniel Marcus, Rivka R Colen, and

Spyridon Bakas. Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient
data. Scientific reports, 10(1):1–12, 2020. 2
[25] Holger R Roth, Ken Chang, Praveer Singh, Nir Neumark,
Wenqi Li, Vikash Gupta, Sharut Gupta, Liangqiong Qu,
Alvin Ihsani, Bernardo C Bizzo, et al. Federated learning
for breast density classification: A real-world implementation. In Domain Adaptation and Representation Transfer,
and Distributed and Collaborative Learning, pages 181–
191. Springer, 2020. 2
[26] Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence Staib,
Pamela Ventola, and James S Duncan. Multi-site fmri analysis using privacy-preserving federated learning and domain
adaptation: Abide results. arXiv preprint arXiv:2001.05647,
2020. 2
[27] Qi Chang, Hui Qu, Yikai Zhang, Mert Sabuncu, Chao Chen,
Tong Zhang, and Dimitris N Metaxas. Synthetic learning: Learn from distributed asynchronized discriminator gan
without sharing medical image data. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 13856–13866, 2020. 2

[35] Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh
Raskar. Designing neural network architectures using reinforcement learning. arXiv preprint arXiv:1611.02167, 2016.
3
[36] Barret Zoph and Quoc V Le. Neural architecture search with
reinforcement learning. ICLR, 2017. 3
[37] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V
Le. Regularized evolution for image classifier architecture
search. In Proceedings of the aaai conference on artificial
intelligence, volume 33, pages 4780–4789, 2019. 3
[38] Lingxi Xie and Alan Yuille. Genetic cnn. In Proceedings of
the IEEE international conference on computer vision, pages
1379–1388, 2017. 3
[39] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts:
Differentiable architecture search. ICLR, 2019. 3
[40] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1294–
1303, 2019. 3

[28] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
Neural Information Processing Systems, 2014. 2

[41] Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, and Cho-Jui Hsieh. Drnas: Dirichlet neural
architecture search. arXiv preprint arXiv:2006.10355, 2020.
3, 4

[29] Quande Liu, Qi Dou, Lequan Yu, and Pheng Ann Heng. Msnet: Multi-site network for improving prostate segmentation
with heterogeneous mri data. IEEE Transactions on Medical
Imaging, 2020. 2

[42] Martin Jankowiak and Fritz Obermeyer. Pathwise derivatives
beyond the reparameterization trick. In International conference on machine learning, pages 2235–2244. PMLR, 2018.
4

[30] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas,
and Ben Glocker. Domain generalization via model-agnostic
learning of semantic features. In Advances in Neural Information Processing Systems, pages 6450–6461, 2019. 2

[43] Mikhail Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. In Advances in Neural
Information Processing Systems, pages 441–452, 2018. 4, 5

[31] Yingda Xia, Dong Yang, Zhiding Yu, Fengze Liu, Jinzheng
Cai, Lequan Yu, Zhuotun Zhu, Daguang Xu, Alan Yuille,
and Holger Roth. Uncertainty-aware multi-view co-training
for semi-supervised medical image segmentation and domain adaptation. Medical Image Analysis, 65:101766, 2020.
2
[32] Marcin Andrychowicz, Misha Denil, Sergio Gomez,
Matthew W Hoffman, David Pfau, Tom Schaul, Brendan
Shillingford, and Nando De Freitas. Learning to learn by
gradient descent by gradient descent. In Advances in neural
information processing systems, pages 3981–3989, 2016. 3
[33] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation
policies from data. arXiv preprint arXiv:1805.09501, 2018.
3
[34] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pages 702–703, 2020. 3

[44] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated
optimization in heterogeneous networks. arXiv preprint
arXiv:1812.06127, 2018. 5, 7
[45] Stephanie A Harmon, Thomas H Sanford, Sheng Xu,
Evrim B Turkbey, Holger Roth, Ziyue Xu, Dong Yang, Andriy Myronenko, Victoria Anderson, Amel Amalou, et al.
Artificial intelligence for the detection of covid-19 pneumonia on chest ct using multinational datasets. Nature communications, 11(1):1–7, 2020. 5
[46] Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin
Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Qi Song, et al. Artificial intelligence distinguishes covid-19 from community
acquired pneumonia on chest ct. Radiology, 2020. 5
[47] Feng Shi, Jun Wang, Jun Shi, Ziyan Wu, Qian Wang, Zhenyu
Tang, Kelei He, Yinghuan Shi, and Dinggang Shen. Review
of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for covid-19. IEEE reviews
in biomedical engineering, 2020. 5
[48] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. ICLR, 2015. 6

[49] Mark E Lowe, Dana K Andersen, Richard M Caprioli, Jyoti Choudhary, Zobeida Cruz-Monserrate, Anil K Dasyam,
Christopher E Forsmark, Fred S Gorelick, Joe W Gray,
Mark Haupt, et al. Precision medicine in pancreatic disease—knowledge gaps and research opportunities: Summary of a national institute of diabetes and digestive and kidney diseases workshop. Pancreas, 48(10):1250, 2019. 8
[50] Amber L Simpson, Michela Antonelli, Spyridon Bakas,
Michel Bilello, Keyvan Farahani, Bram Van Ginneken, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens,
Bjoern Menze, et al. A large annotated medical image dataset
for the development and evaluation of segmentation algorithms. arXiv preprint arXiv:1902.09063, 2019. 8
[51] Holger R Roth, Le Lu, Amal Farag, Hoo-Chang Shin, Jiamin
Liu, Evrim B Turkbey, and Ronald M Summers. Deeporgan:
Multi-level deep convolutional networks for automated pancreas segmentation. In International conference on medical
image computing and computer-assisted intervention, pages
556–564. Springer, 2015. 8
[52] B Landman, Z Xu, J Eugenio Igelsias, M Styner,
TR Langerak, and A Klein. Miccai multi-atlas labeling beyond the cranial vault–workshop and challenge, 2015. 8
[53] Georgios A Kaissis, Marcus R Makowski, Daniel Rückert,
and Rickmer F Braren. Secure, privacy-preserving and federated machine learning in medical imaging. Nature Machine
Intelligence, pages 1–7, 2020. 9

