Fighting the COVID-19 Infodemic:
Modeling the Perspective of Journalists, Fact-Checkers,
Social Media Platforms, Policy Makers, and the Society
Firoj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov∗ , Hamdy Mubarak,
Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem Darwish, Preslav Nakov
Qatar Computing Research Institute, HBKU, Qatar
∗
Sofia University, Sofia, Bulgaria

arXiv:2005.00033v2 [cs.CL] 9 Jun 2020

Abstract
With the emergence of the COVID-19 pandemic, the political and the medical aspects
of disinformation merged as the problem got
elevated to a whole new level to become the
first global infodemic. Fighting this infodemic
is ranked second in the list of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number
of challenging problems such as identifying
messages containing claims, determining their
check-worthiness and factuality, and their potential to do harm as well as the nature of that
harm, to mention just a few. Thus, here we design, annotate, and release to the research community a new dataset for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests
of journalists, fact-checkers, social media platforms, policy makers, and society as a whole,
and (iii) covers both English and Arabic. Finally, we show strong evaluation results using
state-of-the-art Transformers, thus confirming
the practical utility of the annotation schema
and of the dataset.

1

Introduction

The rise of social media has made them one of
the main communication channels for information
dissemination and consumption. In fact, many people rely on social media as their primary source of
news instead of traditional news (Perrin, 2015). Unfortunately, the democratic nature of social media,
where anybody can easily become a news producer,
has raised questions about the quality and the factuality of the information that is shared on these
platforms. Eventually, social media have become
the main channel to spread disinformation (Kumar
and Shah, 2018; Alzanin and Azmi, 2018).

Figure 1 shows some example tweets that demonstrate how online users discuss topics related to
COVID-19 in social media. As we can see, the
problem is much broader than simply looking at
factuality, and the tweets we show could be of interest to journalists, fact-checkers, social media platforms, policymakers, and society in general. The
examples include tweets spreading rumors (Figure 1a), promoting conspiracy theories (Figure 1h),
making jokes (Figure 1c), instilling panic (Figure
1b), promoting fake cures (Figure 1d), or spreading xenophobia, racism, and prejudices (Figure 1e).
Other examples contain information that could be
potentially useful and might deserve the attention
and some action/reaction by government entities.
For example, the tweet in Figure 1f blames the
authorities for their inaction regarding COVID-19
testing. The tweets in the Figures 1f and 1g are
also useful both for authorities and for the general
public as they discuss actions a government has
taken to fight the pandemic and suggests actions
that probably should be taken.
For the tweets in Figure 1, it is necessary to understand whether the information is correct, harmful, calling for action to be taken by relevant authorities, etc. Rapidly sorting these questions is crucial
to help organizations channel their efforts, and to
counter the spread of disinformation, which may
cause panic, mistrust, and other problems.
There has been a lot of fact-checking effort by
about 200 organizations worldwide,1 such as PolitiFact, FactCheck, Snopes, and Full Fact, but it is all
manual and very time-consuming, and thus it often
comes too late and it does not scale. Moreover,
the focus has been exclusively on factuality and
to some extent on check-worthiness; yet, when it
comes to COVID-19, there is a broader range of
relevant issues, as the above examples show.
1

http://tiny.cc/zd1fnz

Figure 1: Example tweets, which would be of potential interest to journalists, fact-checkers, social media platforms,
policy makers, government entities, and the society as a whole.

Addressing any of the above issues requires significant efforts in terms of (i) defining comprehensive annotation guidelines, (ii) collecting tweets
about COVID-19 and sampling from them for annotation, (iii) manually annotating the tweets, and
(iv) training and evaluating models. Given the interconnected nature of the issues, it is more efficient
to address them simultaneously. With this consideration in mind, we adopt a multifaceted approach,
focusing on three key aspects: (i) Does the tweet
contain a claim that is worth fact-checking? (ii) Is
the tweet harmful to the society? and (iii) Should
a government entity take notice of it? We define
seven questions to cover these aspects.
Our contributions are as follows:
• We develop comprehensive guidelines to annotate social media messages that combine the
perspectives and the interests of journalists,
fact-checkers, social media platforms, policymakers, and the society as a whole.
• We develop an annotated dataset of tweets
covering two languages: English and Arabic.
• We present experimental results showing very
sizable improvements over the baselines when
using coarse-grained yes/no labels. For most
questions, we further demonstrate sizable improvements in a fine-grained multiclass setup
(5-10 classes per question). Finally, we show
the potential for cross-language learning.

2

Related Work

Journalists, online users, and researchers are wellaware of the proliferation of false information, and
thus topics such as credibility and fact-checking
have become important research topics. The interested reader can learn more about “fake news” from
the overview by Shu et al. (2017), which adopted
a data mining perspective and focused on social
media. Another survey by Thorne and Vlachos
(2018) took a fact-checking perspective on “fake
news” and related problems. Yet another survey
was performed by Li et al. (2016), covering truth
discovery in general. Finally, two articles in Science offer a general overview and discussion on the
science of “fake news” (Lazer et al., 2018) and of
the process of proliferation of true and false news
online (Vosoughi et al., 2018).
2.1

Fact-Checking

Research on fact-checking claims was largely
based on datasets mined from major fact-checking
organizations. Some of the larger datasets include
the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims
from 8 fact-checking organizations, the MultiFC
dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K
claims Truth of Various Shades (Rashkin et al.,
2017) dataset, among other smaller-size ones.

There have been also datasets for other languages, created in a similar fashion, e.g., Baly
et al. (2018) created a dataset of 402 Arabic claims
extracted from Verify-SY. A number of datasets
were created as part of shared tasks. In most cases,
they did not rely on fact-checking websites, but
performed their own annotation, either (a) manually, e.g., the SemEval-2017 task 8 (Derczynski
et al., 2017) and the SemEval-2019 task 7 (Gorrell et al., 2019) on Determining Rumour Veracity and Support for Rumours (RumourEval), the
SemEval-2019 task 8 on Fact-Checking in Community Question Answering Forums (Mihaylova et al.,
2019), the CLEF-2019 on Automatic Identification
and Verification of Claims (Elsayed et al., 2019)
and the CLEF-2020 Lab on Enabling Automatic
Identification and Verification of Claims in Social
Media (Barrón-Cedeño et al., 2020), which featured both English and Arabic, or (b) using crowdsourcing, e.g., the FEVER 2018 and 2019 tasks on
Fact Extraction and VERification, which focused
on fact-checking made-up claims about content
present in Wikipedia (Thorne et al., 2018, 2019).
Unlike our work, the above datasets did not focus
specifically on tweets (they included claims originating in the news, speeches, political debates, community question answering forums, or just made
up by human annotators; RumourEval is a notable
exception), targeted factuality only (while we cover
a number of other issues), were limited to a single
language (typically English; except for the CLEF2019 and CLEF-2020 labs, which support English
and Arabic), and did not focus on COVID-19.
2.2

Check-Worthiness Estimation

Another relevant line of research is on checkworthiness estimation, i.e., detecting claims that
should be prioritized for fact-checking. While early
work used manual annotations (Hassan et al., 2015),
most datasets were once again derived from factchecking websites, e.g., by observing which claims
in a political debate or speech were selected for
manual fact-checking (Gencheva et al., 2017; Patwari et al., 2017). Such datasets often originated or
were extended as part of shared tasks such as the
2018-2020 editions of the above-mentioned CLEF
CheckThat! lab (Nakov et al., 2018; Elsayed et al.,
2019; Barrón-Cedeño et al., 2020). These datasets
are subject to similar limitations as for the factchecking datasets above, the most important one
being that they focus on a single task.

2.3

COVID-19 Research

In this study, we mainly focus on social media content. We specifically target disinformation posted
on Twitter that is related to the COVID-19 pandemic. A recent effort related to the pandemic used
the Sina Weibo microblogging platform to study
different situational information types, e.g., “caution and advice” (Li et al., 2020). In another study,
the authors reported media bias and rumor amplification patterns for COVID-19 (Cinelli et al.,
2020) using five different social media platforms.
Medford et al. (2020) analyzed COVID-19 related
tweets to understand different content types such
as emotional, racially prejudiced, xenophobic or
content that causes fear.
Other recent work includes identifying lowcredibility information using data from social media (Yang et al., 2020), detecting prejudice (Vidgen
et al., 2020), finding challenges related to data,
tools, and ethical issues (Ding et al., 2020), analyzing the spread of COVID-19 misinformation
in relation to culture, society, and politics (Leng
et al., 2020), detecting the spread of misleading
information and the credibility of users who propagate it (Mourad et al., 2020), identifying positive influencers to propagate information to (PastorEscuredo and Tarazona, 2020), analyzing the users
who spread misinformation and the propagation of
misinformation (Shahi et al., 2020), analyzing psychometric aspects in relation to the COVID-19 infodemic (Jolly et al., 2020), developing a multilingual
COVID-19 Instagram dataset (Zarei et al., 2020),
and detecting disinformation campaigns (Vargas
et al., 2020). All this work focused on a limited set
of issues, while here we model the perspectives of
journalists, fact-checkers, social media platforms,
policy makers, and the society.

3

Dataset

3.1

Data Collection

For this task, we collected COVID-19 related
tweets using twarc,2 a Python wrapper for the
Twitter Streaming API. Specifically, we collected
tweets that matched a set of COVID-19 related
keywords (See Appendix A). We ran queries in
two time frames, namely: March 9–10, 2020 and
March 20–25, 2020. We filtered out all non-Arabic
or non-English tweets, and we annotated the most
frequent tweets in English and Arabic.
2

https://github.com/DocNow/twarc

Tweet 1: So, the last week I have been battling COVID-19 &amp; Pneumonia. Never in my life have I been this ill. “Young people
arent at risk, theyll only have mild symptoms” Wrong. I want to open up about the difficulties Ive gone through these past days, what
it was like in the ICU...
Expl: This has a factual claim, in which user posted his personal testimony, mentioning his
Q1 YES
experience as a COVID-19 patient.
Expl: As the twitter user himself is providing his testimony, therefore, it might be correct informaNO, probably contains
Q2
tion. In addition, the user is a verified user, which makes us to believe that it has a less chance of
no false info
misinformation.
Q3 YES, probably of interest
Expl: General population might get interest in this how it is like to be a COVID-19 patient.
Q4 NO, probably not harmful
Expl: As it would not harm to anyone, therefore it is not harmful.
Expl: It is a factual claim and worthwhile to fact-check, however, it is less important for the
Q5 YES, not urgent
fact-checker.
Expl: It is not harmful for the society as it does not express anything that can affect society.
Q6 NO, not harmful
Expl: Upon reading the whole threads it seems that user explicitly blames authorities by mentioning
“. . . The government has failed us. Im lucky, others wont be. Its far past the time to take action. Not
Q7 YES, blame authorities
words, ACTION. Step the fuck up, and protect the people of this country. If they wont, we need to.
Stay inside, be smart. No death is worth you being ignorant. We can do this.”.
Tweet 2: This is unbelievable. It reportedly took Macron’s threat to close the UK border for Boris Johnson to finally shutdown bars
and restaurants. The Elysee refers to UK policy as ’benign neglect’. This failure of leadership is costing lives.
Expl: This tweet contains factual claim. This is correlation and causation. The claim is “UK closed
Q1 YES
the borders because of the Macron’s threat”.
Q2 NO, probably contains no false info Expl: It may not contains false info as it came from an authentic person.
Expl: Many people might be interested for the information in this tweet as the Prime minister took
Q3 YES, probably of interest
some action to prevent COVID-19.
Q4 YES, definitely harmful
Expl: It is harmful as it blames government officials.
Q5 YES, very urgent
Expl: Professional fact-cheker should verify this immediately as it is attacking government officials.
Q6 YES, rumor, or conspiracy
Expl: The content of the tweet cannot be easily verified as it could be a political move to attack Boris.
Q7 YES, blame authorities
Expl: The content of the tweet clearly blemes authority.

Table 1: Examples of annotated tweets, their labels, and some explanations.

3.2

Annotation Task

The annotation task aims to determine whether a
tweet contains a factual claim, as well as its veracity, its potential to cause harm (to the society,
to a person, to an organization, or to a product),
whether it requires verification, and how interesting it is for a government entity to pay attention to.
To address them, we defined three goals, namely:
(i) if there is a claim in the tweet, and is it worth
fact-checking? (Q1-5) (ii) is the tweet harmful to
the society? (Q6) and (iii) should a government
body or policy makers take notice of it (Q7)? These
are then formulated into seven questions presented
in Table 2. A complete listing of annotation instructions, with examples, is available in Appendix B.
Although some of the questions are correlated,
the annotation instructions are designed so that
the dataset can be used independently for different
tasks. Questions 2-4 (see Table 2) are designed
as both categorical and numerical (i.e., using a
Likert scale) in order to enable their use in either
classification or regression tasks.
In Table 1, we present a sample tweet, annotated
for all questions. Tweet 1 negates the claim that
“Young people aren’t at risk” through personal testimony of the experience of being a COVID-19
patient. Thus, Question 1 is marked as Yes. The
tweet probably contains no false information as a
verified user is providing information about himself.
The tweet is of interest to the general population

as it clears the misconception about “young people
not at risk”. The tweet is not harmful to society
but it blames the authorities. Tweet 2 contains a
potentially harmful claim with a causal argument
possibly requiring urgent fact-checking. Further, it
attacks government officials, which may warrant
a response, clarification, or attention from policy
makers.
Our comprehensive guidelines can serve as an
annotation standard to encourage community efforts towards sorting information based on authenticity and relevance to journalists, fact-checkers,
and policymakers. The diversity of annotations enables interesting modeling solutions. Developing
such guidelines and the dataset have been challenging due to the concise and noisy nature of messages
in social media, some temporal aspects, and the
high degree of subjectivity. The temporal aspect
refers to the time the tweet was posted, e.g., at
time t a claim may be true, but not at time t + 1.
This time t can be a day, a week, or a month. The
guidelines were developed over multiple labeling
iterations of examples, and the annotations and
guidelines were refined in consolidation meetings.
From a modeling perspective, though each question
serves as an independent task, some of questions
are directly connected and can be considered in
relation to each other. For example, the fifth question can be analyzed in relation to the first four
questions.

Similarly, all tasks can be combined in a multitask setting for building one model that serves all
the above-described purposes. Another interesting
research frontier to explore is how to integrate in
the modeling process additional information, such
as images, videos, emoticons, or links to external
websites that users post as part of their tweets to
support their claims. Note that the annotations were
carried out while taking into account this supplementary information, even when the tweets were
posted as a reply.
3.3

Exp.

Class labels

EN

AR

504

218

199
305

78
140

Q2: To what extent does the tweet appear
to contain false information?

305

140

Multi

No, definitely contains no false info
No, probably contains no false info
not sure
Yes, probably contains false info
Yes, definitely contains false info

46
177
45
25
12

31
62
5
40
2

Bin

No
Yes

223
37

93
42

305

140

10
46
8
180
61

1
5
9
76
49

56
241

6
125

305

140

Multi

No, definitely not harmful
No, probably not harmful
not, sure
Yes, probably harmful
Yes, definitely harmful

111
67
2
67
58

68
21
3
46
2

Bin

No
Yes

178
125

89
48

305

140

Q1: Does the tweet contain a verifiable
factual claim?
Bin

No
Yes

Q3: Will the tweet’s claim have an effect on or
be of interest to the general public?

Annotation Challenges

As social media data is noisy and the annotation
tasks are highly subjective, disagreement is a typical scenario. The disputed labels were resolved
in a consensus meeting. Such an approach has
also been used in similar work (Zubiaga et al.,
2015). In the cases where disagreements were
not resolved among the annotators’ group working on the tweets, another consensus meeting was
carried out among all the annotators who worked
on defining the labels and on improving the annotation guidelines. The annotation task was also
time-consuming. For example, on average, an hour
was needed for two annotators to resolve the disagreement for 20 tweets.

Multi

No, definitely not of interest
No, probably not of interest
not sure
Yes, probably of interest
Yes, definitely of interest

Bin

No
Yes

Multi

No, no need to check
No, too trivial to check
Yes, not urgent
Yes, very urgent

81
64
117
43

22
55
48
15

3.4

Bin

No
Yes

145
160

77
63

Q6: Is the tweet harmful for society and why?

504

218

No, joke or sarcasm
No, not harmful
not sure
Yes, bad cure
Yes, other
Yes, panic
Yes, rumor conspiracy
Yes, xenophobic racist prejudices
or hate speech

62
333
2
3
25
23
42

2
159
0
1
5
12
33

14

6

No
Yes

395
107

57
218

Q7: Do you think that this tweet should get
the attention of any government entity?

504

218

Multi

No, not interesting
not sure
Yes, asks question
Yes, blame authorities
Yes, calls for action
Yes, classified as in question 6
Yes, contains advice
Yes, discusses action taken
Yes, discusses cure
Yes, other

319
6
2
81
8
34
9
12
5
28

163
0
0
13
1
30
1
6
4
0

Bin

No
Yes

319
179

163
55

Labels for Classification Tasks

The annotation has been designed in a way that
fine-grained (i.e., multiclass) labels can be easily
transformed into coarse-grained (i.e., binary, Yes vs
No) labels. Therefore, we transformed multiclass
labels for Q2-7 into binary labels considering all
Yes* into Yes, and all No* into No while dropping
not sure tweets, as reported in Table 2. We used
these two sets of labels for two types of experiments: binary and multiclass.
3.5

Q5: Do you think that a professional fact-checker
should verify the claim in the tweet?

Multi

Bin

Data Statistics

Based on the annotation instructions, in the first
phase, we annotated 504 tweets in English and 218
in Arabic.3 Each tweet have been judged by at least
three annotators. In total, seven annotators were
involved in the process. In Table 2 we report the
distribution of class labels of the annotated English
and Arabic tweets. Though we focus the following analysis on English tweets, the distribution of
3

Q4: To what extent does the tweet appear to be
harmful to society, person(s), company(s) or product(s)?

Note that our annotation task is currently ongoing and we
expect to annotate more tweets in the near future. We will
make them available on ANONYMOUS

Table 2: Class distribution for both datasets. EN - English, AR - Arabic. In rows with question the number
refers to the total number of tweets for the respective
language. Bin - binary, Multi - multiclass. For the binary task, we re-label all Yes* labels to Yes and No* to
No, and drop not sure labels.

the Arabic ones is similar, and therefore similar
conclusions can be drawn.
We found that the class distribution for Q1 for
the English tweets is quite balanced, (YES:61%
and NO:39%). Only the tweets that are labeled
as factual claims were annotated for Q2-5. For
the question Q2, the label “NO, probably contains
no false info” shows a higher distribution comparatively, which entails that in the majority of cases the
identified claims are deemed to be most likely true.
Out of 305 tweets labeled for Q2, in about 73% of
the cases, it contains no false information, whereas
12% were categorized as “not sure” and 15% as
“contains false information”. While computing the
statistics, we combined “probably” and “definitely”
into one set for both positive and negative answers,
respectively.
For Q3, which asks if the tweet is of interest
to general public, we found the distribution to be
skewed towards Yes with 79% of the distribution.
This can be attributed to the fact that the tweets
were selected based on frequency of retweets and
likes. For Q4, which judges if the tweet is harmful
to the society, the claims in the tweets vary from
not harmful to harmful. For Q5 that asks if a professional fact-checkers should verify the claim, the
majority of the cases were either “YES, not urgent”
(38%) or “NO, no need to check” (27%). It appears that a professional fact-checker should verify
the claims mentioned in the tweets immediately in
only a small number of cases (14%). For Questions
Q3-5, the “not sure” cases are generally very few.
However, “not sure” cases are substantially more
prevalent in the case of Q2. False information identification (Q2) is a challenging, because it requires
further probing into external information. When annotating Q2, annotators were asked to examine the
content of the tweets (i.e., user identifier, threads,
videos, and images), by examining the whole tweet
from its original URL. For example, in the following tweet Epidemiologist Marc Lipsitch, director of
Harvard’s Center for Communicable Disease Dynamics: “In the US it is the opposite of contained.’
https://t.co/IPAPagz4Vs” it was difficult to determine whether it contains false information without
looking at the tweet in its entirety. The original
tweets often contain images and videos, which can
help to identify the veracity of the claim and it is
necessary to look at them.
For Q6, most of the tweets are classified as
“not harmful” for society and as “joke or sarcasm”.

From the critical classes, 3% of the tweets are classified as containing “xenophobic, racist, prejudices
or hate speech” and 5% for “spreading panic”. For
Q7, it is clear that in the majority of cases (64%)
the tweets are not of interest to government entities,
however, 16% of the cases blame authorities.

4

Evaluation

We experiment with both binary and multiclass
settings for our English and Arabic datasets (see
Table 2 for data statistics).
4.1

Experimental Setup

Data Preprocessing The preprocessing includes
removal of hash-symbol, non-ASCII characters;
case folding; URLs are replaced with a URL tag;
and usernames are also replaced with a user tag.
We split the data into 10-folds while maintaining
the class distribution as close as possible to the
overall distribution. For all tasks, we perform 10fold cross-validation where for each run, we use
train, dev, and test sets with with 80%, 10%, and
10% splits respectively.
Models Pre-trained models have achieved stateof-the-art performance for several NLP tasks. We
experiment with several pre-trained models to evaluate their efficacy under various training scenarios
such as, binary versus multiclass classification, lowresource task scenarios, presence of multilingual
dataset, etc. More specifically, we use BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) for English language experiments and multilingual BERT (mBERT), XLMr (Conneau et al., 2019) and AraBERT (Baly et al.,
2020) for Arabic language experiments. In addition to pre-trained models, we also evaluate the
performance of static-embedding based classification using FastText (Joulin et al., 2017).
For transformer-based models, we use the Transformer Toolkit (Wolf et al., 2019). We fine-tune
each model using the default settings for three
epochs as described in (Devlin et al., 2018). Due to
the instability of the pre-trained models as reported
by Devlin et al. (2018), we do 10 runs of each experiment using different random seeds and pick the
model that performs the best on the development
set. For FastText, we use pre-trained embeddings
trained on Common Crawl (released by FastText
for both English and Arabic). We also use the
built-in hyperparameter tuning from the FastText

library to get the best set of hyperparameters on
our development set.
Evaluation Metrics We compute the accuracy,
precision (P), recall (R), and F1-measure (F1) (in
terms of macro, and weighted average) from the
overall confusion matrix of the 10-fold cross validation runs. Such a confusion matrix has been
computed by summing up the individual confusion
matrices. The reason to choose the weighted metric is that it takes into account the class imbalance
problem. Due to the limited space, we report the
weighted F1 scores in the paper. The results of
other metrics are provided in the appendix H.4.
4.2
4.2.1

Results
Baseline

respectable performance on many tasks, with the
best results on Q5. We hypothesize that due to the
small size of Arabic data, the pre-trained models
are likely to overfit and they may be less effective.
Further, most pre-trained models are not trained on
Arabic tweets.
4.2.3

Multiclass Classification

The second part of Table 3 presents the multiclass
results. The Cls column shows the number of
classes per task. Note that the size (i.e., the proportion of class labels) of the multiclass data is
substantially smaller than the binary class. In addition, the multiclass is a much harder task compared
to binary classification. This is reflected in the
substantially lower classification results.

As a baseline, we use a majority class baseline i.e.,
class with the highest frequency. For questions with
highly imbalanced class distributions, the majority
class baseline is very high. For example, for Q3,
we have an imbalanced distribution in the Arabic
dataset, with tweets with a ‘Yes’ category in the
binary setting comprised 93% of all tweets.

Results on English dataset All models performed better than the majority class. The most
successful model is mBERT, which performed the
best in four out of six tasks. It is interesting to see
that mBERT outperformed BERT in several cases
also. The size of data or the complexity of the task
may have led to this result.

4.2.2

Results on Arabic dataset Interestingly, FastText outperformed all pre-trained models. None
of the pre-trained models show consistent results
on all tasks. This could be due to the scarcity of
data, which may not be sufficient to optimize the
large number of parameters of the pre-trained models. This is a useful finding that shows the value of
static-embeddings in sparse data scenarios.
There are several possible interesting directions
to explore in order to make pre-trained models effective in low-resource scenarios. For example, to
avoid over-fitting instead of fine-tuning the whole
model, one may fine-tune only a part of the network
or fine-tune only the classification layer. Smaller
BERT based models such as DistilBERT has significantly less number of parameters with a small
drop in performance. Such might be effective in
our case. We plan to explore this in future work.

Binary Classification

The first part of Table 3 presents the results of
binary classification using various models.
Results on English dataset All models performed better than the majority class and FastText,
confirming the efficacy of using contextualized embeddings in performing our proposed classification tasks. Comparing various pre-trained models,
BERT outperformed all others in six out of seven
tasks while ALBERT performed the worst in most
of the cases. For Q1, RoBERTa and mBERT performed better than BERT with RoBERTa performing the best.
Results on Arabic dataset In all of the cases except Q3 Arabic, all models performed better than
the majority class. For Q3, we have highly imbalanced distribution in the Arabic dataset with 125
instances of label ‘Yes’ and only 6 instances of
label ‘No’. Such imbalance complicates the task of
machine learning models.
Comparing models, we did not see a clear domination of any of them as in the case of English.
However, XLM-r had worse performance. mBERT
outperformed all the other models in four out of
seven tasks. AraBERT performed better than other
models for Q4. Interestingly FastText achieved

4.2.4

Multilingual

Currently, the labeled datasets are comparatively
small for both Arabic and English. In order to
alleviate sparsity, we experiment with combining
both Arabic and English data for training using a
multilingual model. For training the models, we
used mBERT and fine-tune the network for each
question. The results are reported in Table 4.

English
Q.

Cls

Maj.

FastText

BERT

Arabic

mBERT

RoBERTa

ALBERT

Maj.

FastText

mBERT

AraBERT

XLM-r

86.5
83.9
79.6
78.5
72.7
79.2
79.0

50.2
56.2
93.2
51.2
39.0
62.7
64.0

75.8
68.2
93.2
79.2
78.6
79.4
74.1

88.1
79.1
89.2
78.5
76.4
80.4
78.5

82.6
71.1
77.8
80.4
76.1
77.3
77.9

76.9
60.2
89.2
69.0
66.5
64.6
64.0

44.8
45.4
39.5
48.0
56.5
53.5

27.2
38.2
31.8
22.2
61.5
64.0

47.4
83.1
54.4
77.2
79.3
75.7

42.8
27.0
43.7
59.0
40.9
66.3

42.1
21.4
44.9
57.7
38.9
63.9

37.4
20.0
34.2
46.1
44.5
64.0

Binary (Coarse-grained)
Q1
Q2
Q3
Q4
Q5
Q6
Q7

2
2
2
2
2
2
2

45.6
79.2
72.7
43.5
36.1
69.3
50.0

72.8
82.6
77.2
69.6
63.1
71.6
69.9

87.6
86.9
84.3
84.0
81.3
86.1
89.3

Q2
Q3
Q4
Q5
Q6
Q7

5
5
5
5
8∗
10∗

42.6
43.8
19.4
21.3
52.6
49.1

44.0
48.3
35.5
37.6
53.9
57.8

48.5
57.6
41.6
50.4
57.2
54.6

88.3
83.1
81.6
82.7
80.0
76.8
81.9

90.6
82.9
80.8
83.8
73.7
81.0
84.7

Multiclass (Fine-grained)
52.2
45.1
42.9
52.3
62.7
58.7

46.6
50.9
44.1
50.3
58.4
55.2

Table 3: Monolingual experiments using different models. Shown are binary and multiclass results (weighted
F1), for English and Arabic, using various Transformers and FastText. The results that improve over the majority
class baseline (Maj.) are in bold, and the best system is underlined. Legend: Q. – question, Cls – number of
classes, the * in Q6 and Q7 is a reminder that for Arabic they have 7 classes (not 8 and 10 as for English).

The reported results suggest that overall we obtain better performance with multilingual training.
For the binary task, Q2 monolingual results are
higher, while multiclass results for Q2 and Q3 are
higher.
Since simply concatenating the data of two languages is helpful, this encourages us to try other
methods of data augmentation such as automatically translating the data of one language and using
it with the target language. We left such exploration
for future work.

English
Q.

Cls

Q1
Q2
Q3
Q4
Q5
Q6
Q7

2
2
2
2
2
2
2

En

En+Ar

Arabic
Ar

Ar+En

Binary (Coarse-grained)
88.3
83.1
81.6
82.7
80.0
76.8
81.9

89.7
81.9
83.0
83.2
83.9
83.0
84.0

88.1
79.1
89.2
78.5
76.4
80.4
78.5

89.9
82.5
83.3
86.0
78.6
82.8
84.6

Multiclass (Fine-grained)
Q2
Q3
Q4
Q5
Q6
Q7

5
5
5
4
8*
10*

52.2
45.1
42.9
52.3
62.7
58.7

47.0
50.1
43.8
59.8
61.8
58.4

42.8
27.0
43.7
59.0
40.9
66.3

46.6
28.0
47.9
70.5
42.7
68.0

Table 4: Multilingual experiments using mBERT.
Shown are results for using Arabic data to help English,
and English data to help Arabic (weighted F1).

5

Conclusion and Future Work

We presented an annotation scheme and a corresponding manually annotated dataset of COVID19 tweets, aiming to help in the fight against the
global infodemic, which emerged as a result of the
COVID-19 pandemic. The dataset combines the
perspectives and the interests of journalists, factcheckers, social media platforms, policymakers,
and society as a whole. It includes annotations in
English and Arabic and is made freely available to
the research community. We provided evaluation
results for both English and Arabic using different
Transformer model architectures.
We will be expanding the annotations, and we
will make them available at (ANONYMOUS). We
plan to recruit professional annotators to expand
the size of the dataset significantly. We would also
allow people to contribute to these annotations using a crowd-sourcing platform (ANONYMOUS).
There are a number of other interesting research
directions that can be pursued using our dataset,
such as multi-task learning (e.g., Q2, Q3, and Q4
can be used to improve the performance of Q5 as
they are correlated; see Appendix D for more details), use of meta-data from Twitter (e.g., verified
user account, reference to authentic URL, link to
articles), of multi-modal information (e.g., image
or video to verify the authenticity of the claim,
retweets) for better classification and for data augmentation. Another research direction is to model
the problem as ordinal regression, as the labels for
questions Q2, Q3 and Q4 are defined on an ordinal
scale.

References
Gerald Albaum. 1997. The likert scale revisited. Market Research Society. Journal., 39(2):1–21.
Gordon W Allport and Leo Postman. 1947. The psychology of rumor.
Samah M Alzanin and Aqil M Azmi. 2018. Detecting
rumors in social media: A survey. Procedia computer science, 142:294–300.
Isabelle Augenstein, Christina Lioma, Dongsheng
Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, and Jakob Grue Simonsen. 2019.
MultiFC: A real-world multi-domain dataset for
evidence-based fact checking of claims. In Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4685–4697, Hong
Kong, China. Association for Computational Linguistics.

Zubiaga. 2017. SemEval-2017 Task 8: RumourEval:
Determining rumour veracity and support for rumours. In Proceedings of the 11th International
Workshop on Semantic Evaluation, SemEval ’17,
pages 60–67, Vancouver, Canada.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Kaize Ding, Kai Shu, Yichuan Li, Amrita Bhattacharjee, and Huan Liu. 2020. Challenges in combating
covid-19 infodemic – data, tools, and ethics.
Tamer Elsayed, Preslav Nakov, Alberto BarrónCedeño, Maram Hasanain, Reem Suwaileh, Giovanni Da San Martino, and Pepa Atanasova. 2019.
CheckThat! at CLEF 2019: Automatic identification
and verification of claims. In Advances in Information Retrieval, ECIR ’19, pages 309–315. Springer
International Publishing.

Fady Baly, Hazem Hajj, et al. 2020.
Arabert:
Transformer-based model for arabic language understanding. In Proceedings of the 4th Workshop on
Open-Source Arabic Corpora and Processing Tools,
with a Shared Task on Offensive Language Detection, pages 9–15.

Pepa Gencheva, Preslav Nakov, Lluı́s Màrquez, Alberto Barrón-Cedeño, and Ivan Koychev. 2017.
A context-aware approach for detecting worthchecking claims in political debates. In Proceedings
of the International Conference Recent Advances in
Natural Language Processing, RANLP ’17, pages
267–276, Varna, Bulgaria.

Ramy Baly, Mitra Mohtarami, James Glass, Lluı́s
Màrquez, Alessandro Moschitti, and Preslav Nakov.
2018. Integrating stance detection and fact checking
in a unified corpus. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT ’18, pages 21–
27, New Orleans, LA, USA.

Genevieve Gorrell, Ahmet Aker, Kalina Bontcheva,
Leon Derczynski, Elena Kochkina, Maria Liakata,
and Arkaitz Zubiaga. 2019. SemEval-2019 task 7:
RumourEval, determining rumour veracity and support for rumours. In Proceedings of the 13th International Workshop on Semantic Evaluation, pages
845–854, Minneapolis, Minnesota, USA.

Alberto Barrón-Cedeño, Tamer Elsayed, Preslav
Nakov, Giovanni Da San Martino, Maram Hasanain,
Reem Suwaileh, and Fatima Haouari. 2020. Checkthat! at CLEF 2020: Enabling the automatic identification and verification of claims in social media. In
Proceedings of the 42nd European Conference on Information Retrieval, ECIR ’19, pages 499–507, Lisbon, Portugal.
David A Broniatowski, Amelia M Jamison, SiHua Qi,
Lulwah AlKulaib, Tao Chen, Adrian Benton, Sandra C Quinn, and Mark Dredze. 2018. Weaponized
health communication: Twitter bots and russian
trolls amplify the vaccine debate. American journal
of public health, 108(10):1378–1384.
Matteo Cinelli, Walter Quattrociocchi, Alessandro
Galeazzi, Carlo Michele Valensise, Emanuele Brugnoli, Ana Lucia Schmidt, Paola Zola, Fabiana Zollo,
and Antonio Scala. 2020. The covid-19 social media
infodemic.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmn, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised
cross-lingual representation learning at scale.
Leon Derczynski, Kalina Bontcheva, Maria Liakata,
Rob Procter, Geraldine Wong Sak Hoi, and Arkaitz

Naeemul Hassan, Chengkai Li, and Mark Tremayne.
2015. Detecting check-worthy factual claims in
presidential debates. In Proceedings of the 24th
ACM International on Conference on Information
and Knowledge Management, CIKM ’15, pages
1835–1838.
Baani Leen Kaur Jolly, Palash Aggrawal, Amogh
Gulati, Amarjit Singh Sethi, Ponnurangam Kumaraguru, and Tavpritesh Sethi. 2020. Psychometric analysis and coupling of emotions between state
bulletins and twitter in india during covid-19 infodemic.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and
Tomas Mikolov. 2017. Bag of tricks for efficient
text classification. In Proceedings of the 15th Conference of the European Chapter of the Association
for Computational Linguistics: Volume 2, Short Papers, pages 427–431. Association for Computational
Linguistics.
Lev Konstantinovskiy, Oliver Price, Mevan Babakar,
and Arkaitz Zubiaga. 2018. Towards automated
factchecking: Developing an annotation schema and
benchmark for consistent automated claim detection.
CoRR, abs/1809.08193.
Srijan Kumar and Neil Shah. 2018. False information
on web and social media: A survey. arXiv preprint
arXiv:1804.08559.

Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2019. Albert: A lite bert for self-supervised learning of language representations.
David M.J. Lazer, Matthew A. Baum, Yochai Benkler, Adam J. Berinsky, Kelly M. Greenhill, Filippo
Menczer, Miriam J. Metzger, Brendan Nyhan, Gordon Pennycook, David Rothschild, Michael Schudson, Steven A. Sloman, Cass R. Sunstein, Emily A.
Thorson, Duncan J. Watts, and Jonathan L. Zittrain. 2018. The science of fake news. Science,
359(6380):1094–1096.
Yan Leng, Yujia Zhai, Shaojing Sun, Yifei Wu, Jordan
Selzer, Sharon Strover, Julia Fensel, Alex Pentland,
and Ying Ding. 2020. Analysis of misinformation
during the covid-19 outbreak in china: cultural, social and political entanglements.
Lifang Li, Qingpeng Zhang, Xiao Wang, Jun Zhang,
Tao Wang, Tian-Lu Gao, Wei Duan, Kelvin Kamfai Tsoi, and Fei-Yue Wang. 2020. Characterizing
the propagation of situational information in social
media during covid-19 epidemic: A case study on
weibo. IEEE Transactions on Computational Social
Systems.
Yaliang Li, Jing Gao, Chuishi Meng, Qi Li, Lu Su,
Bo Zhao, Wei Fan, and Jiawei Han. 2016. A survey on truth discovery. SIGKDD Explor. Newsl.,
17(2):1–16.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining approach.
Richard J. Medford, Sameh N. Saleh, Andrew Sumarsono, Trish M. Perl, and Christoph U. Lehmann.
2020. An ”infodemic”: Leveraging high-volume
twitter data to understand public sentiment for the
covid-19 outbreak. medRxiv.
Tsvetomila Mihaylova, Georgi Karadzhov, Pepa
Atanasova, Ramy Baly, Mitra Mohtarami, and
Preslav Nakov. 2019. SemEval-2019 task 8: Fact
checking in community question answering forums.
In Proceedings of the 13th International Workshop
on Semantic Evaluation, SemEval ’19, pages 860–
869, Minneapolis, MN, USA.
Azzam Mourad, Ali Srour, Haidar Harmanani, Cathia
Jenainatiy, and Mohamad Arafeh. 2020. Critical impact of social networks infodemic on defeating coronavirus covid-19 pandemic: Twitter-based study and
research directions.
Preslav Nakov, Alberto Barrón-Cedeño, Tamer Elsayed, Reem Suwaileh, Lluı́s Màrquez, Wajdi Zaghouani, Pepa Atanasova, Spas Kyuchukov, and
Giovanni Da San Martino. 2018. Overview of the
CLEF-2018 CheckThat! lab on automatic identification and verification of political claims. In Proceedings of the Ninth International Conference of
the CLEF Association: Experimental IR Meets Multilinguality, Multimodality, and Interaction, Lecture
Notes in Computer Science, pages 372–387, Avignon, France. Springer.

David Pastor-Escuredo and Carlota Tarazona. 2020.
Characterizing information leaders in twitter during
covid-19 crisis.
Ayush Patwari, Dan Goldwasser, and Saurabh Bagchi.
2017. TATHYA: a multi-classifier system for detecting check-worthy statements in political debates. In
Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM ’17,
pages 2259–2262, Singapore.
Andrew Perrin. 2015. Social media usage. Pew research center, pages 52–68.
Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana
Volkova, and Yejin Choi. 2017. Truth of varying
shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, EMNLP ’17, pages 2931–2937, Copenhagen, Denmark.
Gautam Kishore Shahi, Anne Dirkson, and Tim A. Majchrzak. 2020. An exploratory study of covid-19
misinformation on twitter.
Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, and
Huan Liu. 2017. Fake news detection on social media: A data mining perspective. SIGKDD Explor.
Newsl., 19(1):22–36.
Andon Tchechmedjiev, Pavlos Fafalios, Katarina
Boland, Malo Gasquet, Matthäus Zloch, Benjamin
Zapilko, Stefan Dietze, and Konstantin Todorov.
2019. ClaimsKG: A knowledge graph of factchecked claims. In Proceedings of the 18th International Semantic Web Conference, ISWC ’19, pages
309–324, Auckland, New Zealand.
James Thorne and Andreas Vlachos. 2018. Automated
fact checking: Task formulations, methods and future directions. In Proceedings of the 27th International Conference on Computational Linguistics,
COLING ’18, pages 3346–3359, Santa Fe, NM,
USA.
James Thorne,
Andreas Vlachos,
Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction and
verification. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, NAACL-HLT ’18, pages 809–819,
New Orleans, Louisiana, USA.
James Thorne, Andreas Vlachos, Oana Cocarascu,
Christos Christodoulopoulos, and Arpit Mittal. 2019.
The FEVER2.0 shared task. In Proceedings of the
Second Workshop on Fact Extraction and VERification (FEVER), pages 1–6, Hong Kong, China. Association for Computational Linguistics.
Luis Vargas, Patrick Emami, and Patrick Traynor. 2020.
On the detection of disinformation campaign activity
with network analysis.
Bertie Vidgen, Austin Botelho, David Broniatowski,
Ella Guest, Matthew Hall, Helen Margetts, Rebekah
Tromble, Zeerak Waseem, and Scott Hale. 2020. Detecting east asian prejudice on social media.

Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018.
The spread of true and false news online. Science,
359(6380):1146–1151.
William Yang Wang. 2017. “Liar, liar pants on fire”:
A new benchmark dataset for fake news detection.
In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics, ACL ’17,
pages 422–426, Vancouver, Canada.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.
Kai-Cheng Yang, Christopher Torres-Lugo, and Filippo Menczer. 2020. Prevalence of low-credibility
information on twitter during the covid-19 outbreak.
Koosha Zarei, Reza Farahbakhsh, Noel Crespi, and
Gareth Tyson. 2020. A first instagram dataset on
covid-19.
Arkaitz Zubiaga, Maria Liakata, Rob Procter, Kalina
Bontcheva, and Peter Tolmie. 2015. Towards detecting rumours in social media. In Workshops at the
Twenty-Ninth AAAI Conference on Artificial Intelligence.

Appendix
A

Data Collection

The following keywords are used for collecting
English and Arabic tweets.
• English: #covid19, #CoronavirusOutbreak,
#Coronavirus, #Corona, #CoronaAlert,
#CoronaOutbreak, Corona, covid-19
• Arabic:

AKðPñ» ,AKðPñ»#
(Corona),
YK Ym.Ì '@_ AKðPñ»_ ð Q¯# ,
Yj.JÖÏ @_ AKðPñ»_ ð Q¯# , (novel Coronavirus), AKðPñ»_ ð Q¯# ,AKPñ»_ ð Q¯#
(Coronavirus), and YK Ym.Ì '@_ AKðPñ»# (new
Corona)

B

Detail Annotation Instructions

General Instructions:
1. For each tweet, the annotator needs to read
the text including the hashtags and also look
at the tweet itself when necessary by going to
the link (i.e., for Q2-7 it might be required to
open the tweet link).4
3. The annotators may look at the images and
the videos, to the Web pages that the tweet
links to, as well as to the tweets in the same
thread when making a judgment, if required.
4. The annotators are not required to complete
questions Q2-Q5 if the answer to question Q1
is NO.
B.1

Verifiable Factual Claim

Question 1: Does the tweet contain a verifiable
factual claim?
A verifiable factual claim is a sentence claiming
that something is true, and this can be verified using factual verifiable information such as statistics,
4
The reason for not going to the tweet link for Q1 is that we
wanted to reduce the complexity of the annotation task and to
focus on the content of the tweet only. As for Q2, it might be
important to check if the tweet was posted by an authoritative
source, and thus it might be useful for the annotator to open
the tweet to get more context. After all, this is how real
users perceive the tweet. Since the annotators would open the
tweet’s link for Q2, they can use that information for the rest
of the questions as well (even though this is not required).
2. The annotators should assume the time when the tweet was
posted as a reference when making judgments, e.g., “Trump
thinks, that for the vast majority of Americans, the risk is very,
very low.” would be true when he made the statement but false
by the time annotations were carried out for this tweet. The
annotator should consider the time when the tweet was posted.

specific examples, or personal testimony. Factual
claims include the following:5
• Stating a definition;
• Mentioning quantity in the present or the past;
• Making a verifiable prediction about the future;
• Statistics or specific examples;
• Personal experience or statement (e.g., “I
spent much of the last decade working to develop an #Ebola treatment.”)
• Reference to laws, procedures, and rules of
operation;
• References (e.g., URL) to images or videos
(e.g., “This is a video showing a hospital in
Spain.”);
• Statements which can be technically classified
as questions, but in fact contain a verifiable
claim based on the criteria above (e.g., “Hold
on - #China Communist Party now denying
#CoronavirusOutbreak originated in China?
This after Beijing’s catastrophic mishandling
of the virus has caused a global health crisis?”)
• Statements about correlation or causation.
Such a correlation or causation needs to
be explicit, i.e., sentences like “This is
why the beaches haven’t closed in Florida.
https://t.co/8x2tcQeg21” is not a claim because it does not explicitly say why, thus it is
not verifiable.
Tweets containing personal opinions and preferences are not factual claims. Note that if a tweet is
composed of multiple sentences or clauses, at least
one full sentence or clause needs to be a claim in
order for the tweet to contain a factual claim. If a
claim exist in a sub-sentence or sub-clause, then the
tweet is not considered to have a factual claim. For
example, “My new favorite thing is Italian mayors
and regional presidents LOSING IT at people violating quarantine” is not a claim – it is in fact an
opinion. Moreover, if we consider “Italian mayors
and regional presidents LOSING IT at people violating quarantine” it would be a claim. In addition,
when answering this question, annotators should
not open the tweet URL. Since this is a binary decision task, the answer of this question consists of
two labels as defined below.
Labels:
5

Inspired by (Konstantinovskiy et al., 2018).

• YES: if it contains a verifiable factual claim;
• NO: if it does not contain a verifiable factual
claim;
• Don’t know or can’t judge: the
content of the tweet does not have enough
information to make a judgment. It is recommended to categorize the tweet using this
label when the content of the tweet is not understandable at all. For example, it uses a
language (i.e., non-English) or references that
it is difficult to understand;
Examples:
1. Please don’t take hydroxychloroquine
(Plaquenil) plus Azithromycin for #COVID19
UNLESS your doctor prescribes it. Both
drugs affect the QT interval of your heart and
can lead to arrhythmias and sudden death,
especially if you are taking other meds or
have a heart condition.
Label: YES
Explanation: There is a claim in the text.

To answer this question, it is recommended to
open the link of the tweet and to look for additional information for the veracity of the claim
identified in question 1. For example, if the tweet
contains a link to an article from a reputable information source (e.g., Reuters, Associated Press,
France Press, Aljazeera English, BBC), then the
answer could be “. . . contains no false info”.
Examples:
1. “Dominican Republic found the cure for
Covid-19 https://t.co/1CfA162Lq3”
Label: 5. YES, definitely
contains false information
Explanation: This is not correct information
at the time of this tweet is posted.
2. This is Dr. Usama Riaz. He spent past weeks
screening and treating patients with Corona
Virus in Pakistan. He knew there was no PPE.
He persisted anyways. Today he lost his own
battle with coronavirus but he gave life and
hope to so many more. KNOW HIS NAME
https://t.co/flSwhLCPmx
Label: 2. NO, probably contains
no false info
Explanation: The content of the tweet states
correct information.

2. Saw this on Facebook today and its a must
read for all those idiots clearing the shelves
#coronavirus #toiletpapercrisis #auspol
Label: NO
Explanation: There is no claim in the text.
B.3
B.2

False Information

Question 2: To what extent does the tweet appear
to contain false information?
The stated claim may contain false information.
This question labels the tweets with the categories
mentioned below. False Information appears on
social media platforms, blogs, and news-articles to
deliberately misinform or deceive the readers (Kumar and Shah, 2018).
Labels: The labels for this question are defined
with a five point Likert scale (Albaum, 1997). A
higher value means that it is more likely to be false:
1. NO, definitely contains no
false information
2. NO, probably contains no false
information
3. Not sure
4. YES, probably contains false
information
5. YES, definitely contains false
information

Interest to General Public

Question 3: Will the tweet’s claim have an effect
on or be of interest to the general public?
Most often people do not make interesting
claims, which can be verified by our general knowledge. For example, though “The sky is blue” is a
claim, it is not interesting to the general public. In
general, topics such as healthcare, political news,
and current events are of higher interest to the general public. Using the five point Likert scale the
labels are defined below.
Labels:
1.
2.
3.
4.
5.

NO, definitely not of interest
NO, probably not of interest
Not sure
YES, probably of interest
YES, definitely of interest

Examples:
1. Germany is conducting 160k Covid-19 tests
a week. It has a total 35k ventilators, 10k
ordered to be made by the govt. It has
converted a new 1k bed hospital in Berlin.

Its death rate is tiny bcos its mass testing
allows quarantine and bcos it has fewer non
reported cases.
Label: 4. YES: probably of
interest
Explanation: This information is relevant
and of high interest for the general population
as it reports how a country deals with
COVID-19.
2. Fake news peddler Dhruv Rathee had said:
“Corona virus won’t spread outside China, we
need not worry” Has this guy ever spoke
something sensible? https://t.co/siBAwIR8Pn
Label: 2. NO, probably not of
interest
Explanation: The information is not interesting for the general public as it is an opinion
and providing statement of someone else.
B.4

Harmfulness

Question 4: To what extent does the tweet appear
to be harmful to society, person(s), company(s) or
product(s)?
The purpose of this question is to determine if
the content of the tweet aims to and can negatively
affect society as a whole, specific person(s), company(s), product(s), or spread rumors about them.
The content intends to harm or weaponize the information6 (Broniatowski et al., 2018). A rumor
involves a form of a statement whose veracity is
not quickly verifiable or ever confirmed7 .
Labels: To categorize the tweets we defined the
following labels based on the Likert scale. A higher
value means a higher degree of harm.
1.
2.
3.
4.
5.

NO, definitely not harmful
NO, probably not harmful
Not sure
YES, probably harmful
YES, definitely harmful

Examples:
1. How convenient but not the least bit surprising from Democrats! As usual they put
politics over American citizens. @SpeakerPelosi withheld #coronavirus bill so DCCC
could run ads AGAINST GOP candidates!
#tcot
Label: 5. YES, definitely
6

The use of information as a weapon to spread misinformation and mislead people.
7
https://en.wikipedia.org/wiki/Rumor

harmful
Explanation: This tweet is weaponized to
target Nancy Pelosi and the Democrats in
general.
2. As we saw over the wkend, disinfo is being
spread online about a supposed national
lockdown and grounding flights. Be skeptical
of rumors. Make sure youre getting info
from legitimate sources. The @WhiteHouse
is holding daily briefings and @cdcgov is
providing the latest.
Label: 1. NO, definitely not
harmful
Explanation: This tweet is informative and
gives advice. It does not attack anyone and is
not harmful.
B.5

Need of Verification

Question 5: Do you think that a professional
fact-checker should verify the claim in the tweet?
It is important to verify a factual claim by a
professional fact-checker, as the claim may cause
harm to society, specific person(s), company(s),
product(s), or some government entities. However,
not all factual claims are important or worthwhile
to be fact-checked by a professional fact-checker,
because it is a time-consuming procedure. Therefore, the purpose is to categorize the tweet using the
labels defined below. While doing so, the annotator
can rely on the answers to the previous questions.
For this question, we defined the following labels
to categorize the tweets.
Labels:
1. NO, no need to check: the tweet
does not need to be fact-checked, e.g., because it is not interesting, a joke, or does not
contain any claim.
2. NO, too trivial to check:
the
tweet is worth fact-checking, however, this
does not require a professional fact-checker,
i.e., a non-expert might be able to fact-check
the claim. For example, one can verify the
information using reliable sources such as the
official website of the WHO, etc. An example
of a claim is as follows: “The GDP of the
USA grew by 50% last year.”
3. YES, not urgent: the tweet should be
fact-checked by a professional fact-checker,
however, it is not urgent or critical;
4. YES, very urgent: the tweet can cause
immediate harm to a large number of people,

therefore, it should be verified as soon as possible by a professional fact-checker;
5. Not sure: the content of the tweet does not
have enough information to make a judgment.
Examples:
1. Things the GOP has done during the Covid19 outbreak: - Illegally traded stocks - Called
it a hoax - Blamed it on China - Tried to
bailout big business without conditions What
they havent done: - Help workers - Help small
businesses - Produced enough tests or ventilators
Label: 2. YES, very urgent
Explanation: Clearly, the content of the
tweet blames authority, hence, it is important
to verify this claim immediately by a professional fact-checker. In addition, the attention
of government entities might be required in
order to take necessary actions.
2. ALERT
The corona virus can be
spread through internationally printed albums.
If you have any albums at home, put on some
gloves, put all the albums in a box and put it
outside the front door tonight. I’m collecting
all the boxes tonight for safety. Think of your
health.
Label: 5. NO, no need to check
Explanation: This is a joke and does not need
to be checked by a professional fact checker.
B.6

Harmful to Society

Question 6: Is the tweet harmful for society and
why?
The purpose of this question is to categorize if
the content of the tweet is intended to harm or is
weaponized to mislead the society. To identify that
we defined the following labels for the categorization.
Labels:
A. NO, not harmful: the content of the
tweet would not harm the society (e.g., “I
like corona beer”).
B. NO, joke or sarcasm: the tweet contains a joke (e.g., “If Corona enters Spain, itll
enter from the side of Barcelona defense”) or
sarcasm (e.g., “‘The corona virus is a real
thing.’ – Wow, I had no idea!”).
C. Not sure: if the content of the tweet is not
understandable enough to judge.

D. YES, panic: the tweet spreads panic. The
content of the tweet can cause sudden fear
and anxiety for a large part of the society
(e.g., “there are 50,000 cases ov COVID-19
in Qatar”).
E. YES, xenophobic, racist,
prejudices, or hate-speech:
the tweet reports xenophobia, racism or
prejudiced expression(s). According to the
dictionary8 Xenophobic refers to fear or
hatred of foreigners, people from different
cultures, or strangers. Racism is the belief that
groups of humans possess different behavioral
traits corresponding to physical appearance
and can be divided based on the superiority
of one race over another.9 It may also refer
to prejudice, discrimination, or antagonism
directed against other people because they are
of a different race or ethnicity. Prejudice is an
unjustified or incorrect attitude (i.e., typically
negative) towards an individual based solely
on the individual’s membership of a social
group.10 An example of a xenophobic
statement is “do not buy cucumbers from
Iran”.
F. YES, bad cure: the tweet reports a questionable cure, medicine, vaccine or prevention
procedures (e.g., “. . . drinking bleach can help
cure coronavirus”).
G. YES, rumor, or conspiracy: the
tweet reports or spreads a rumor. It is defined as a “specific (or topical) proposition
for belief passed along from person to person
usually by word of mouth without secure standards of evidence being present” (Allport and
Postman, 1947). For example, “BREAKING:
Trump could still own stock in a company that,
according to the CDC, will play a major role
in providing coronavirus test kits to the federal
government, which means that Trump could
profit from coronavirus testing. #COVID-19
#coronavirus https://t.co/Kwl3ylMZRk”
H. YES, other: if the content of the tweet
does not belong to any of the above categories,
then this category can be chosen to label the
tweet.
8

https://www.dictionary.com/
https://en.wikipedia.org/wiki/Racism
10
https://www.simplypsychology.org/
prejudice.html
9

B.7

Requires attention

Question 7: Do you think that this tweet should
get the attention of any government entity?
Most often people tweet by blaming authorities,
providing advice, and/or call for action. Sometimes
that information might be useful for some government entities to make a plan, respond or react on it.
The purpose of this question is to categorize such
information. It is important to note that not all information requires attention from a government entity.
Therefore, even if the tweet’s content belongs to
any of the positive categories, it is important to
understand whether that requires government attention. For the annotation, it is mandatory to first
decide on whether attention is necessary from government entities (i.e., YES/NO). If the answer is
YES, it is obligatory to select a category from the
YES sub-categories mentioned below.
Labels:
A. NO, not interesting: if the content
of the tweet is not important or interesting for
any government entity to pay attention to.
B. Not sure: if the content of the tweet is not
understandable enough to judge.
C. YES, categorized as in
if some government
question 6:
entities need to pay attention to this tweet as
it is harmful for society, i.e., it is labeled as
any of the YES sub-categories in question 6.
D. YES, other: if the tweet cannot be labeled as any of the above categories, then this
label should be selected.
E. YES, blame authorities: the tweet
contains information that blames some government entities or top politician(s), e.g.,
“Dear @VP Pence: Is the below true? Do you
have a plan? Also, when are local jurisdictions going to get the #Coronavirus test kits
you promised?”.
F. YES, contains advice: the tweet
contains advice about social, political, national, or international issues that requires attention from some government entities (e.g.,
The elderly & people with pre-existing health
conditions are more susceptible to #COVID19.
To stay safe, they should: XKeep distance
from people who are sick XFrequently wash
hands with soap & water XProtect their mental health).
G. YES, calls for action: the tweet

contains information that states that some government entities should take action for a particular issue (e.g., I think the Government should
close all the Barber Shops and Salons , let people buy shaving machines and other beauty
gardgets keep in their houses. Salons and
Barbershops might prove to be another Virus
spreading channels @citizentvkenya @SenMutula @CSMutahi Kagwe).
H. YES, discusses action taken: if
the tweet discusses actions taken by governments, companies, individuals for any particular issue, for example, closure of bars, conferences, churches due to the corona virus
(e.g., Due to the current circumstances with
the Corona virus, The 4th Mediterranean Heat
Treatment and Surface Engineering Conference in Istanbul postponed to 26-28 Mays
2021.).
I. YES, discusses cure: if attention is
needed from some government entities as the
tweet discusses a possible cure, vaccine, or
treatment for a disease (e.g., Pls share this
valuable information. Garlic boiled water
can be cure corona virus).
J. YES, asks question: if the content of
the tweet contains a question over a particular
issue and it requires attention from government entities (e.g., Special thanks to all doctors and nurses, new found respect for youll.
Is the virus going to totally disappear in the
summer? I live in USA and praying that when
the temperature warms up the virus will go
away...is my thinking accurate?)

C

Class Label Distribution

In Figure 2 and 3, we report detailed class label
distribution of each question. In general the class
distributions are similar in both English and Arabic.

D
D.1

Correlation Between Questions
English Tweets Dataset

In Figure 4, we report the contingency and correlation tables in a form of a heatmap for different question pairs obtained from the English tweet
dataset. For questions Q2-3, it appears that there
is a high association11 between “. . . no false info”
11
Note that, a Chi-Square test could have been a viable
solution to prove such an association, however, our data size
is still small (in many cases cell values are less than 5) to do
such a test.

(a) Questions (Q1-5).

(b) Questions (Q6-7).

Figure 2: Distribution of class labels for English tweets

(a) Questions (Q1-5).

(b) Questions (Q6-7).

Figure 3: Distribution of class labels for Arabic tweets

(a) Heatmap for Q2 and Q3.

(b) Heatmap for Q2 and Q4.

(c) Heatmap for Q2 and Q5.

(d) Heatmap for Q3 and Q4.

(e) Heatmap for Q3 and Q5.

(f) Heatmap for Q4 and Q5.

(g) Heatmap for Q6 and Q7. YES, X/R/P/HS – YES, xenophobic,
racist, prejudices or hate speech

(h) Correlation between Q2 to Q4.

Figure 4: Contingency and correlation heatmaps of English tweets for different question pairs

(a) Heatmap for Q2 and Q3.

(b) Heatmap for Q2 and Q4.

(c) Heatmap for Q2 and Q5.

(d) Heatmap for Q3 and Q4.

(e) Heatmap for Q3 and Q5.

(f) Heatmap for Q4 and Q5.

(g) Heatmap for Q6 and Q7. YES, X/R/P/HS – YES, xenophobic,
racist, prejudices or hate speech

(h) Correlation between Q2 to Q4.

Figure 5: Contingency and correlation heatmaps of Arabic tweets for different question pairs

and the general public interest as shown in Figure
4a. For questions Q2 and Q4 (Figure 4b), a high
association can be observed between “. . . no false
info” and “. . . not harmful” (65%) compared to
“harmful” (34%) for either an individual, products
or government entities. By analyzing questions
Q2 and Q5 (Figure 4c), we conclude that “. . . no
false info” is associated with either “no need to
check” or “too trivial to check”, highlighting the
fact that a professional fact-checker does not need
to spend time on them. From questions Q3 and
Q4 (Figure 4d), it appears that when the content
of the tweets is “not harmful” the general public
interest is higher (61%) than when it is “harmful”
(39%). From question Q3 and Q5 (Figure 4e), we
see an interesting phenomenon, namely tweets with
a high general public interest have a greater association with a professional fact-checker having to
verify them (61%) compared to either “too trivial
to check” or “no need to check” (39%). The questions Q4 and Q5 (Figure 4f) show that “harmful”
tweets require more attention (53%) from a professional fact-checkers than “not harmful” tweets
(45%). Our findings for Q6 and Q7 (Figure 4g) suggest that the majority of the tweets are not harmful
for society, which also requires less attention from
government entities. The second most common
tweet label for Q7 blames authorities, though they
are mostly not harmful for society.
In Figure 4h, we report the correlation between
questions Q2-4 for the English tweets in order to
understand their association. We computed the correlation using the Likert scale values (i.e., 1-5) that
we defined for these questions. We observed that
overall Q2 and Q3 are negatively correlated, which
infers that if the claim contains no false information, it is of high interest to the general public. This
can be also observed in Figure 4a. Questions Q2
and Q4 show a positive correlation, which might
be due to their high association with “. . . no false
info” and “. . . not harmful”.
D.2

Arabic Tweets Dataset

In Figure 5, we report heatmaps to illustrate the association across questions using the Arabic tweets.
From Q2 and Q3 (Figure 5a), we can observe that
the association between “. . . contains no false info”
and general public interest is higher (67%) than
“. . . contains false info” (29%). From questions Q2
and Q4 (Figure 5b), we conclude that “. . . contains
no false info” is associated with “. . . not harm-

ful” and “. . . contains false info” is associated with
“. . . harmful”, which can also be established from
its high correlation of 0.74 in Figure 5h. From the
relation between Q2 and Q5 (Figure 5c), it can be
seen that in the majority of the cases “. . . contains
no false info” is associated with either “no need to
check” or “too trivial to check”, which means that
a professional fact-checker does not need to verify
them. The analysis between questions Q3 and Q4
suggests that general public interest is higher when
the content of the tweets is not harmful (68%) than
harmful (30%) (Figure 5d). From questions Q3
and Q5, we can observe that the general public interest is higher when the claim(s) in the tweets are
either “no need to check” or “too trivial to check”
(Figure 5e). The analysis between question Q4
and Q5 shows that “not harmful” tweets are either
“no need to check” or “too trivial to check” by a
professional fact-checker (Figure 5f). From the
questions Q6 and Q7, we notice that in the majority
of the cases the tweets are not harmful for society
and hence they are not interesting for government
entities (Figure 5g).

E

Multimedia in Tweets

In this subsection, we study the correlation between
whether a tweet has multimedia (video, image,
or none) and our annotation. Generally, people
trust videos more than images or plain texts which
suggests that tweets with video potentially have a
higher impact.
In the Arabic dataset, we didn’t find any clear
preference for Q1-Q4, i.e., a tweet with video, image, or only text can contain a factual claim with almost similar ratios. For Q5 (i.e., need fact-checking
by a professional fact-checker), when a tweet has
a video, in 44% of the cases, annotators selected
“Yes” compared to 17% and 25% for image and
text only respectively. Fact-checking of videos is
not always trivial. For Q6 (i.e., harmful to society)
and Q7 (i.e., should attract government attention),
when a tweet has a video, it has the potential to
be harmful and get government attention is higher
than tweets with only images or text. Annotators
selected “Yes” in almost 33% of the tweets having videos for Q6 and Q7 and this ratio decreased
to almost half for tweets having images or text
only. This shows the importance of having videos
in tweets as it gives more trust.

Others
35.3%

US
40.0%

Nigeria
2.1%
Malaysia
4.2%
UK
7.4%

India
11.1%

(a) English dataset
Others
8.3%
Yemen
1.8%
US
2.3%
UK
2.8%
UAE
3.2%
Kuwait
5.0%
Egypt
5.0%

unverified accounts (Q7, English). These are general observations from the currently small number
of annotated tweets, and there are some differences
between the English and Arabic annotations. The
quantitative study can be held at a later stage using
a larger dataset.
This correlation could be one of the features that
a classifier can use to predict labels for unseen
tweets, can also help in speeding up the annotation
process by providing initial default values before
manual revision. In addition, in some cases, verified accounts can be used to check annotation quality, for example, tweets from @WHO should not
be labeled as weaponized or harmful to society.

H
KSA
61.0%

Qatar
10.6%

(b) Arabic dataset

Figure 6: Country distribution for English and Arabic
tweets

Experimental Parameters and Results

H.1

Transformers Parameters

Below we list the hyperparameters that we used for
training across all Transformers based models. All
experimental scripts will be publicly available.
• Batch size: 8
• Learning rate (Adam): 2e-5
• Number of epochs: 3

F

Geographical distribution

Figure 6 shows the geographical distribution of annotated tweets for English and Arabic. We consider
the country of the tweet author or the original author in case of retweeting. It is observed that most
English tweets came from the US, India, and the
UK (∼60%), while most Arabic tweets came from
KSA and Qatar (∼70%). For both languages, there
are tweets from a large number of countries, which
indicates a good diversity of interests, topics, styles,
etc. that strengthens our study.

G

Verified and Unverified Accounts

We study the correlation between tweet labels and
whether or not the original author of a tweet has a
verified account. Verified accounts include government entities, public figures, celebrities, etc., which
have a large number of followers, so their tweets
typically have a high impact on society.
Figure 7 shows that verified accounts tend to post
more tweets that contain factual claims than unverified accounts (Q1), and their tweets are more likely
to not contain false information (Q2), be of higher
interest to the general public (Q3), be less harmful
to society (Q6, Arabic), and attract greater attention from a government entity than tweets from

• Max seq length: 128
Number of parameters:
• BERT (bert-base-uncased): L=12, H=768,
A=12, total parameters = 110M; where L is
number of layers (i.e., Transformer blocks),
H is the hidden size, and A is the number of
self-attention heads.
• RoBERTa (roberta-base): similar to BERTbase with a higher number of parameters
(125M).
• ALBERT (albert-base-v1): similar to BERTbase with a reduced parameters size of 12M.
• AraBERT (bert-base-arabert): same number
as BERT (110M).
• BERT Multilingual (bert-base-multilingualuncased) (mBERT): similar to BERT-base
with a higher number of parameters (172M).
• XML-RoBERTa (xlm-roberta-base): L=12,
H=768, A=12; total parameters = 270M.
H.2

FastText Parameters

We plan to release all the FastText parameters with
our released packages. We have not listed them
here due to their exhaustive list.

Figure 7: Distribution of datasets for all the questions associated with user accounts. NA refers to tweets that have
not been labeled for those questions, they are identical to the tweets categorized with the label NO in Q1.

H.3

Computing Infrastructure and Runtime

We used the NVIDIA Tesla V100-SXM2-32 GB
GPU machine consists of 56 cores and 256GB CPU
memory. To perform an experiment for a question
on average the computing time took 40 minutes
using a BERT base model, which results in around
4 hours for seven questions using one transformer
architecture.

Binary
Q.

Acc.

M-F1

Multiclass
W-F1

Acc.

M-F1

W-F1

51.1
54.1
36.4
38.7
63.9
64.9

18.9
22.5
27.5
33.2
12.60
14.00

44.0
48.3
35.5
37.6
53.9
57.8

63.5
58.1
51.9
61.9
69.4
62.9

24.3
28.1
33.4
51.8
15.5
10.70

52.5
54.1
44.5
58.0
59.2
57.5

36.0
28.7
35.1
56.9
20.1
11.40

55.5
52.4
47.6
64.1
64.2
57.6

60.0
52.6
47.1
68.1
71.6
63.1

20.2
24.8
31.0
57.6
17.6
9.70

47.4
49.3
41.8
64.6
62.1
54.7

60.6
55.5
43.5
45.2
68.0
68.4

26.6
23.2
32.4
41.7
15.9
16.5

51.8
48.9
42.3
45.2
58.8
61.8

Majority
Q1
Q2
Q3
Q4
Q5
Q6
Q7

60.8
85.2
80.0
58.1
51.6
78.4
64.0

37.8
46.0
44.4
36.7
34.0
44.0
39.0

Q1
Q2
Q3
Q4
Q5
Q6
Q7

92.0
88.1
90.3
90.0
89.0
92.5
93.4

91.5
66.0
81.2
89.7
89.0
88.0
92.8

Q1
Q2
Q3
Q4
Q5
Q6
Q7

93.7
87.0
87.3
91.3
85.5
89.2
90.8

93.4
60.7
76.0
91.0
85.4
82.8
89.9

Q1
Q2
Q3
Q4
Q5
Q6
Q7

91.8
87.8
89.0
85.8
82.9
86.3
89.2

91.2
66.6
77.8
85.4
82.9
74.6
88.4

Q1
Q2
Q3
Q4
Q5
Q6
Q7

74.3
85.9
80.0
75.5
67.1
77.3
75.6

72.2
56.6
55.8
74.7
66.6
55.6
71.6

46.0
78.4
71.1
42.7
35.1
69.0
50.0
BERT

H.4

Results

The detail classification results on dev and test sets
in terms of accuracy (Acc), macro-F1 (M-F1) and
weighted-F1 (W-F1) for English data are reported
in Table 5 and 6, respectively. For Arabic data the
detail of dev and test sets results are reported in
Table 7 and 8, respectively.
In Table 9 and 10, we report on dev and test
set for multilingual setting where training is performed by combining English and Arabic data and
evaluated on English data. With the same multilingual setting the results on Arabic evaluation is
reported in Table 11 and 12 for dev and test sets,
respectively.

91.9
85.3
89.1
90.0
89.0
92.2
93.4

RoBERTa
93.7
83.3
85.9
91.2
85.4
88.8
90.7

64.5
54.2
51.9
67.7
72.2
61.6

ALBERT
91.7
85.3
87.3
85.8
82.9
84.4
89.2
FastText
73.8
81.7
75.4
75.4
66.7
73.2
74.6

Table 5: Classification results on dev set (English
data) using different models including majority baseline for different questions. Acc. - Accuracy, M-F1 macro F1, W-F1 - weighted average F1. For Q1, binary
and multiclass setting’s results are same.

Binary
Q.

Acc.

M-F1

Multiclass
W-F1

Acc.

M-F1

Binary

W-F1

Q.

Acc.

M-F1

Majority
Q1
Q2
Q3
Q4
Q5
Q6
Q7

60.5
85.8
81.1
58.7
52.5
78.7
64.1

37.7
46.2
44.8
37.0
34.4
44.0
39.0

45.6
79.2
72.7
43.5
36.1
69.3
50.0

58.0
59.0
36.4
38.4
66.1
63.3

87.7
89.2
86.5
84.2
81.3
87.1
89.4

87.0
69.0
71.1
83.3
81.2
77.9
88.4

Q1
Q2
Q3
Q4
Q5
Q6
Q7

90.7
86.9
83.5
83.8
73.8
82.3
84.9

90.1
57.8
65.0
83.2
73.6
70.0
83.2

Q1
Q2
Q3
Q4
Q5
Q6
Q7

86.5
87.7
83.8
78.5
72.8
82.5
79.3

85.8
60.3
61.1
77.7
72.6
65.1
76.8

Q1
Q2
Q3
Q4
Q5
Q6
Q7

73.2
86.5
80.5
70.3
63.3
75.7
70.5

71.1
57.4
58.1
68.1
62.9
52.7
66.8

87.6
86.9
84.3
84.0
81.3
86.1
89.3

14.7
14.8
10.7
13.9
9.90
7.80

42.6
43.8
19.4
21.3
52.6
49.1

Q1
Q2
Q3
Q4
Q5
Q6
Q7

63.6
71.4
92.9
64.3
53.3
72.7
75.0

38.9
41.7
48.1
39.1
34.8
42.1
42.9

59.7
60.3
49.2
54.8
68.3
62.7

21.9
30.6
29.5
44.6
14.0
9.50

48.5
57.6
41.6
50.4
57.2
54.6

Q1
Q2
Q3
Q4
Q5
Q6
Q7

88.2
86.4
84.3
85.0
83.3
84.1
85.9

87.5
82.1
49.9
82.5
83.3
78.7
76.1

58.4
52.5
49.2
54.4
67.5
59.3

21.3
25.9
31.8
43.2
15.6
9.70

46.6
50.9
44.1
50.3
58.4
55.2

Q1
Q2
Q3
Q4
Q5
Q6
Q7

84.5
84.3
67.1
86.4
82.7
86.4
86.4

82.5
78.6
45.7
84.2
82.2
80.0
77.1

58.7
50.5
45.9
52.1
66.9
61.5

17.3
19.6
31.0
41.4
13.9
9.80

44.8
45.4
39.5
48.0
56.5
53.5

Q1
Q2
Q3
Q4
Q5
Q6
Q7

79.1
75.7
84.3
76.4
68.7
74.1
75.0

74.9
57.3
49.9
67.6
65.6
48.6
42.9

49.5
59.5
89.4
50.3
37.1
61.2
64.3

44.3
54.3
48.6
39.3
72.9
74.8

12.3
14.1
13.1
14.1
12.1
12.2

27.2
38.2
31.8
22.2
61.5
64.0

51.1
54.1
36.4
38.7
63.9
64.9

18.9
22.5
27.5
33.2
12.6
14.0

44.0
48.3
35.5
37.6
53.9
57.8

Q1
Q2
Q3
Q4
Q5
Q6
Q7

73.6
85.0
92.9
82.1
80.0
82.3
85.5

69.1
81.2
48.1
78.3
79.9
74.6
76.5

88.3
85.9
85.5
84.4
83.4
83.6
83.7

54.7
24.0
52.0
74.0
31.4
82.3

30.1
15.2
23.7
48.5
11.0
18.4

49.8
30.0
44.4
65.2
42.0
75.5

84.1
83.3
74.9
85.9
82.4
85.1
84.4

54.0
19.3
56.0
72.7
30.0
81.4

24.1
12.8
24.2
51.5
10.9
16.5

45.7
23.5
51.1
65.5
40.7
74.2

47.3
22.0
46.0
63.3
37.7
80.5

17.0
14.4
15.5
36.1
8.8
12.7

34.8
25.5
34.1
53.3
43.9
71.7

63.3
84.7
68.0
77.3
77.3
81.4

38.1
58.8
34.2
66.9
26.0
20.4

61.2
84.0
64.5
75.2
72.4
76.1

XML-r

FastText
72.8
82.6
77.2
69.6
63.1
71.6
69.9

W-F1

AraBERT

ALBERT
86.5
83.9
79.6
78.5
72.7
79.2
79.0

M-F1

mBERT

RoBERTa
90.6
82.9
80.8
83.8
73.7
81.0
84.7

Acc.

Majority

BERT
Q1
Q2
Q3
Q4
Q5
Q6
Q7

Multiclass
W-F1

77.7
69.3
85.5
72.4
66.3
65.0
64.3
FastText

Table 6: Classification results on test set (English
data) using different models including majority baseline for different questions.

72.4
84.8
89.4
80.9
80.0
80.9
83.7

Table 7: Classification results on dev set (Arabic data)
using different models including majority baseline for
different questions.

Q.

Acc.

M-F1

W-F1

Binary: En

Binary
Q.

Acc

M-F1

64.2
68.9
95.4
65.0
55.0
73.9
74.8

39.1
40.8
48.8
39.4
35.5
42.5
42.8

88.1
80.0
87.0
78.8
76.4
81.7
80.7

87.0
74.7
51.8
76.1
76.2
73.3
69.0

Q1
Q2
Q3
Q4
Q5
Q6
Q7

83.0
74.1
68.7
81.0
76.4
79.8
80.7

80.6
63.8
45.0
78.0
75.7
68.1
67.9

Q1
Q2
Q3
Q4
Q5
Q6
Q7

78.9
69.6
87.0
73.0
68.6
74.3
74.8

73.6
47.2
51.8
63.5
65.5
45.9
42.8

W-F1

Acc

M-F1

W-F1

50.2
56.2
93.2
51.2
39.0
62.7
64.0

44.3
54.3
48.6
39.3
72.9
74.8

12.3
14.1
13.1
14.1
12.1
12.2

27.2
38.2
31.8
22.2
61.5
64.0

88.1
79.1
89.2
78.5
76.4
80.4
78.5

25.1
14.5
20.1
44.5
10.5
15.4

42.8
27.0
43.7
59.0
40.9
66.3

50.0
17.9
53.6
65.0
28.9
74.3

23.2
11.1
20.6
42.6
9.8
12.2

42.1
21.4
44.9
57.7
38.9
63.9

48.6
20.0
45.7
55.0
38.5
74.8

19.3
14.3
14.7
31.2
9.0
12.2

37.4
20.0
34.2
46.1
44.5
64.0

48.6
22.9
52.9
65.7
30.7
75.7

AraBERT
82.6
71.1
77.8
80.4
76.1
77.3
77.9
XML-r
76.9
60.2
89.2
69.0
66.5
64.6
64.0

76.6
69.6
95.4
79.6
78.6
81.2
77.1

73.1
61.5
48.8
76.8
78.5
71.5
62.5

63.5
58.1
51.9
61.9
69.4
62.9

75.8
68.2
93.2
79.2
78.6
79.4
74.1

49.3
83.6
57.1
78.6
82.6
80.3

29.2
64.5
28.9
70.6
35.5
33.6

91.1
87.4
86.6
90.6
85.2
82.8
91.6

47.4
83.1
54.4
77.2
79.3
75.7

24.3
28.1
33.4
51.8
15.5
10.7

52.5
54.1
44.5
58.0
59.2
57.5

Q.

Acc.

M-F1

W-F1

Binary: En

92.9
87.8
88.0
89.4
85.8
90.2
91.6

92.5
63.0
78.4
89.2
85.6
83.8
91.0

92.9
84.3
87.0
89.4
85.7
89.6
91.6

Multiclass: En+Ar
61.9
57.4
48.1
73.9
72.9
64.7

27.7
29.1
35.2
70.0
22.8
11.8

50.9
53.3
47.8
72.8
66.6
59.8

Q1
Q2
Q3
Q4
Q5
Q6
Q7

87.7
89.2
86.5
84.2
81.3
87.1
89.4

Q1
Q2
Q3
Q4
Q5
Q6
Q7

87.7
59.7
60.3
49.2
54.8
68.3
62.7

87.0
69.0
71.1
83.3
81.2
77.9
88.4

87.6
86.9
84.3
84.0
81.3
86.1
89.3

Multiclass: En
87.0
21.9
30.6
29.5
44.6
14.0
9.5

Acc.

M-F1

W-F1

Binary: En+Ar

87.6
48.5
57.6
41.6
50.4
57.2
54.6

89.9
86.2
84.8
83.2
83.9
84.3
83.9

89.1
55.3
69.7
82.7
83.8
72.9
82.8

89.7
81.9
83.0
83.2
83.9
83.0
84.0

Multiclass: En+Ar
89.9
59.3
54.8
44.9
61.3
68.5
62.3

89.1
20.4
25.0
34.5
56.5
19.8
11.6

89.7
47.0
50.1
43.8
59.8
61.8
58.4

Table 10: Multilingual experiments: classification results on English test set using mBERT for both binary
and multiclass settings.
Acc.

M-F1

W-F1

Binary: Ar
Q1
Q2
Q3
Q4
Q5
Q6
Q7

88.2
86.4
84.3
85.0
83.3
84.1
85.9

Q2
Q3
Q4
Q5
Q6
Q7

54.7
24.0
52.0
74.0
31.4
82.3

87.5
82.1
49.9
82.5
83.3
78.7
76.1

30.1
15.2
23.7
48.5
11.0
18.4

Acc.

M-F1

W-F1

Binary: En+Ar
88.3
85.9
85.5
84.4
83.4
83.6
83.7

Multiclass: Ar

Table 8: Classification results on test set (Arabic data)
using different models including majority baseline for
different questions.

W-F1

Table 9: Multilingual experiments: classification results on dev set using mBERT for both binary and multiclass settings. For En+Ar setting, the training is performed by combining English and Arabic data and evaluated on English data.

Q.

FastText
Q1
Q2
Q3
Q4
Q5
Q6
Q7

Q2
Q3
Q4
Q5
Q6
Q7

90.6
71.2
77.3
90.3
85.2
71.9
90.9

M-F1

Binary: En+Ar

Multiclass: En

mBERT
Q1
Q2
Q3
Q4
Q5
Q6
Q7

91.2
89.6
88.0
90.6
85.2
84.9
91.6

Multiclass

Majority
Q1
Q2
Q3
Q4
Q5
Q6
Q7

Q1
Q2
Q3
Q4
Q5
Q6
Q7

Acc

49.8
30.0
44.4
65.2
42.0
75.5

86.8
88.6
75.7
90.0
82.0
91.4
88.6

85.6
86.9
48.2
88.5
81.9
88.9
83.5

86.7
88.9
80.6
89.7
82.0
91.3
88.1

Multiclass: En+Ar
58.7
24.7
50.7
78.7
32.3
81.4

32.2
12.3
29.1
65.1
10.3
12.8

55.5
25.7
53.3
75.1
43.3
77.1

Table 11: Multilingual experiments: classification results on dev set using mBERT for both binary and multiclass settings. For En+Ar setting, the training is performed by combining English and Arabic data and evaluated on Arabic data

Q.

Acc

M-F1

W-F1

Binary: Ar
Q1
Q2
Q3
Q4
Q5
Q6
Q7

88.1
80.0
87.0
78.8
76.4
81.7
80.7

87.0
74.7
51.8
76.1
76.2
73.3
69.0

48.6
22.9
52.9
65.7
30.7
75.7

25.1
14.5
20.1
44.5
10.5
15.4

M-F1

W-F1

Binary: En+Ar
88.1
79.1
89.2
78.5
76.4
80.4
78.5

89.9
82.2
77.1
86.1
78.6
83.0
85.3

42.8
27.0
43.7
59.0
40.9
66.3

51.4
27.9
51.4
73.6
32.1
73.9

Multiclass: Ar
Q2
Q3
Q4
Q5
Q6
Q7

Acc

89.0
80.0
46.6
84.5
78.4
77.4
78.6

89.9
82.5
83.3
86.0
78.6
82.8
84.6

Multiclass: En+Ar
26.5
14.8
23.0
59.7
10.1
10.9

46.6
28.0
47.9
70.5
42.7
68.0

Table 12: Multilingual experiments: classification results on Arabic test set using mBERT for both binary
and multiclass settings.

