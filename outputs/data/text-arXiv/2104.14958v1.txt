Assessment of the influence of features on a classification
problem: an application to COVID-19 patients
Laura Davila-Penaa,∗, Ignacio Garcı́a-Juradob , Balbina Casas-Méndeza

arXiv:2104.14958v1 [stat.ML] 9 Apr 2021

a

MODESTYA Research Group, Department of Statistics, Mathemtical Analysis and
Optimisation and IMAT, Faculty of Mathematics, University of Santiago de Compostela,
Campus Vida, 15782, Santiago de Compostela, Spain.
b
MODES Research Group, Department of Mathematics and CITIC, Faculty of Computer
Science, University of A Coruña, Campus de Elviña, 15071, A Coruña, Spain.

∗

Corresponding author
Email addresses: lauradavila.pena@usc.es (Laura Davila-Pena),
ignacio.garcia.jurado@udc.es (Ignacio Garcı́a-Jurado),
balbina.casas.mendez@usc.es (Balbina Casas-Méndez)
Preprint submitted to European Journal of Operations Research

May 3, 2021

Abstract
This paper deals with an important subject in classification problems addressed by machine learning techniques: the evaluation of the influence of
each of the features on the classification of individuals. Specifically, a measure of that influence is introduced using the Shapley value of cooperative
games. In addition, an axiomatic characterisation of the proposed measure
is provided based on properties of efficiency and balanced contributions.
Furthermore, some experiments have been designed in order to validate the
appropriate performance of such measure. Finally, the methodology introduced is applied to a sample of COVID-19 patients to study the influence
of certain demographic or risk factors on various events of interest related
to the evolution of the disease.
Keywords: Machine learning; Classification; Influence of features; Shapley
value; COVID-19
2010 MSC: 97R40, 91A80, 62H30

1. Introduction
A classification problem consists of predicting the value of a qualitative
response variable for one or more individuals, making use of the values we
know of certain variables (features) of such individuals. Those predictions
are based on the knowledge obtained through a training sample of individuals whose values of the features and of the response variable are known.
Classification problems can be addressed by using machine learning techniques. Numerous classifiers have been proposed and analysed in the machine learning literature (see, for example, Fernández-Delgado et al., 2014).

2

Many of them, in addition to classifying, allow the assessment of the importance that the various features have had in the classification of a particular
individual. In Strumbelj & Kononenko (2010) a general assessment procedure is introduced; it is general in the sense that it is model-agnostic, i.e.,
valid for any classifier. The Strumbelj and Kononenko’s procedure is based
on the Shapley value for cooperative games (Shapley, 1953). The Shapley
value is a rule for distributing the profits generated by a collection of cooperating agents. Such value has multiple applications in very diverse fields
(see, for example, Moretti & Patrone, 2008), including data science; an application of the Shapley value in data science can be seen in Garcı́a-Jurado
et al. (2004).
In machine learning, making explanations of a model and understanding
its behaviour is one of the most important tasks. Works such as Strumbelj &
Kononenko (2010) provide a local interpretability of the classifiers involved,
that is, explanations are made only for a specific instance. Nevertheless,
in recent years, globally-interpretability machine learning models have been
developed, allowing the users to understand the process for getting a particular prediction (see, for example, Wang & Rudin, 2015).
In the machine learning literature there are other papers that study the
influence of features on classification. Recently, several works make use of
other approaches or are applied to specific classification techniques, and applications to health problems are often of great interest. Ghaddar & NaoumSawaya (2018) introduce a new iterative approach to address the problem of
the selection of features within the classification of support vector machines
and apply it to a case of medical diagnosis of tumors. Agor & Özaltın (2019)
propose a two-level programming approach to present a selection problem
for classification and develop a solution based on a new genetic algorithm.
3

They implement the framework proposed in a case study where they distinguish between good and poor quality colposcopy images. Anaraki & Usefi
(2019) is the first work to report on using perturbation theory in feature
selection. The authors have proved that perturbation theory can detect
correlations between features. Benı́tez-Peña et al. (2019) propose a feature
selection procedure based on mathematical optimisation integrated into the
support vector machines classification procedure, which accommodates the
costs of asymmetric misclassification. Cura (2020) proposes a method that
jointly optimises the classification and selection of features. The study uses
multiple local search algorithms that run in parallel and share information.
This collection of related works shows how the problem of the selection of
features in classification is often approached by means of techniques taken
from operational research, which is the discipline where cooperative game
theory can also be placed.
However, probably the closest study to the subject of our research is
Datta et al. (2015). In that paper, they study how influential are the various features in a classification problem when the response variable is binary
and the classifier returns for each individual the value of the response variable. In this paper we consider the more general problem in which the
response variable may not be binary and, in addition, the predictor returns
for each individual the probability that the response variable takes each of
its possible values. Our approach is significantly more generic and therefore
more applicable. In any case, the methodology that is being developed here
is completely different from that of Datta et al. (2015), even though it also
makes use of cooperative game theory.
The purpose of this paper is also related to the Strumbelj and Kononenko’s
procedure, but it is essentially different because it is not locally oriented. We
4

do not attempt to evaluate the influence of each feature on the classification
of a particular individual, but rather to evaluate the influence of each feature on the value taken by the response variable. Specifically, we introduce
an influence measure that assess the influence of each feature in the classification of all individuals whose prediction of the response variable turns out
to be a pre-set one. Moreover, an axiomatic characterisation of the introduced measure is provided, by using properties of efficiency and balanced
contributions. This last property is based on the principle of reciprocity, as
introduced by Myerson (1980), which is often used in the literature on the
Shapley value. Myerson’s property of balanced contributions asserts that,
for any two players, the gain or loss to each player when the other “leaves”
the game should be equal.
We illustrate our purpose with an example taken from the analysis of
the evolution of COVID-19 patients. In the fight against a pandemic it is
very important to be able to detect groups at risk in the face of various
eventualities. For instance, knowing the characteristics of individuals that
significantly increase their probability of decease in the event of infection
can be useful for health authorities to make appropriate decisions. Therefore, once various features that can influence the mortality of a patient are
selected, it would be valuable to know how to evaluate the influence of each
of them on such mortality. In addition to mortality there are other eventualities that may be relevant, such as the need for hospitalisation, the need
for admission to intensive care units, the need for access to certain health
infrastructures, etc.
The organization of this paper is as follows. Section 2 presents the measure of influence and provides its axiomatic characterisation. In Section 3,
various experiments are carried out in order to check the performance of the
5

methodology introduced. Finally, in Section 4, based on the data from a
sample of COVID-19 patients, we detect features which affect to the mortality or hospitalisation of a patient and evaluate their influence.
2. Assessing Influence in Classification
In a classification problem we have a vector of features X = (X1 , . . . , Xk )
and a qualitative response variable Y . K = {1, . . . , k} denotes the set of
indices of the features. Each feature Xj takes values in a finite set Aj and
Y takes values in a finite set B. We also have a training sample M =
{(X i , Y i )}ni=1 , where X i = (X1i , . . . , Xki ) and Y i are the observed values
of the features and the response variable corresponding to individual i. A
classification problem is thus characterised by a triplet (X, Y, M).
A classifier trained with sample M is a map f M that assigns to every a ∈
A = A1 ×· · ·×Ak (an observation of X) a probability distribution over B, i.e.,
P
f M (a) = (fbM (a))b∈B with fbM (a) ≥ 0, for all b ∈ B, and b∈B fbM (a) = 1.
Each fbM (a) is the estimated probability that an individual whose observed
values of the features are given by a belongs to group b of the response
variable Y . Now we provide the main definition in this section.
Definition 1. An influence measure for (X, Y, M) is a map I that assigns
to every aj ∈ Aj , b ∈ B, and T ⊆ K (T 6= ∅) a vector I(aj , b, T ) =
(Il (aj , b, T ))l∈T ∈ RT . The vector I(aj , b, T ) provides an evaluation of the
influence that each feature Xl , with l ∈ T , has on whether the response is
worth b when Xj is worth aj and we only take into account the features
{Xl }l∈T .
Section 4 illustrates the interest of having such a measure. For instance,
we use it to assess various risk features on the decease of COVID-19 patients.
6

We aim to introduce a sensible influence measure. Let us see some
properties that are considered desirable. The first property is related to
rationality. It states that, if there is only one feature, then the measure of
the influence that the feature has on whether the response variable takes
a certain value is the probability that the classifier we use assigns to that
value. Formally, the following rationality property is established.
f M -Rationality. An influence measure satisfies f M -Rationality if, for every (X, Y, M) with k = 1, every a ∈ A, and b ∈ B, it holds that
I(a, b, K) = fbM (a).
Notice that we define the influence measure for (X, Y, M) but we allow
the possibility of ignoring some features, i.e., the measure evaluates each
(aj , b, T ) with T possibly being different from K. A natural extension of the
f M -Rationality is the following property that states that, when we ignore
all features except one, then the measure of the influence that this feature
has on whether the response variable takes a certain value is the expected
probability that the classifier we use assigns to that value. Formally, an
influence measure should satisfy the following property.
General f M -Rationality. An influence measure satisfies the property
General f M -Rationality if, for every (X, Y, M) and every aj ∈ Aj , b ∈ B
and l ∈ K, it holds that
Il (aj , b, {l}) =

1
nbaj

X
(X i ,Y i )∈Mbaj

1
| AK\{l} |

X

fbM (Xli , a0K\{l} ) (1)

a0K\{l} ∈AK\{l}

where Mbaj denotes the subsample of M formed by the observations (X i , Y i )
with Xji = aj and Y i = b, and nbaj denotes the size of the subsample Mbaj .
7

Moreover, AV , aV , and XVi denote the restrictions of A, a, and X i to the
variables of V , respectively (for all V ⊆ K).
Again, a natural extension of the General f M -Rationality results when
we substitute {l} ⊆ K by T ⊆ K in expression (1). This gives rise to the
following efficiency property, that establishes that the total influence of a set
T ⊆ K of features is the expected prediction provided by the classifier when,
for every (X i , Y i ) ∈ Mbaj , we fix the values of the features in T according
to (X i , Y i ).
f M -Efficiency. An influence measure satisfies f M -Efficiency if, for every
(X, Y, M), every aj ∈ Aj , b ∈ B, and T ⊆ K (T 6= ∅), it holds that
X

Il (aj , b, T ) =

l∈T

1
nbaj

1

X
(X i ,Y i )∈Mbaj

| AK\T |

X

fbM (XTi , a0K\T ).

a0K\T ∈AK\T

The last property considered is a fairness property that treats all features
in a balanced way. Informally, it states that given two of these features, the
effect of ignoring one to the measure of the influence of the other is identical
for both features.
Balanced Contributions. An influence measure satisfies Balanced Contributions if, for every (X, Y, M), every aj ∈ Aj , b ∈ B, T ⊆ K (T 6= ∅),
and l, m ∈ T with l 6= m,
Il (aj , b, T ) − Il (aj , b, T \{m}) = Im (aj , b, T ) − Im (aj , b, T \{l}).
Now we state and prove the main result from this section. It provides
a characterisation and a formal expression of an influence measure that
satisfies all the properties introduced above.
8

Theorem 2. There is only one influence measure for (X, Y, M) which satisfies the properties of f M -Efficiency and Balanced Contributions. For all
aj ∈ Aj , b ∈ B, T ⊆ K (T 6= ∅), and l ∈ T , it is given by
IlΦ (aj , b, T ) =

1
nbaj

X

b
Φl (vX
i |T ),

(2)

(X i ,Y i )∈Mbaj

b denotes the game with set of players
where Φ denotes the Shapley value, vX
i

K given by
b
vX
i (S) =

1
| AK\S |

X

fbM (XSi , a0K\S ),

(3)

a0K\S ∈AK\S

b | denotes the restriction of the game v b to the
for all S ⊆ K, and vX
i T
Xi

subsets of T .
Remark 3. For each instance (X i , Y i ) and each coalition S ⊆ K, the characteristic function in (3) displays the prediction provided by the classifier
when the values of the features in S are set to those in (X i , Y i ). In Strumbelj & Kononenko (2010), a cooperative game of difference of predictions is
defined, whose characteristic function assigns to each coalition the value of
the characteristic function in (3) minus the expected prediction if no features values are set. The mathematical properties of our game and that of
Strumbelj and Kononenko are essentially the same.

Proof of Theorem 2.
Existence. To show that I Φ satisfies f M -Efficiency, let aj ∈ Aj , b ∈ B, and
T ⊆ K (T 6= ∅). Shapley (1953) proves that the Shapley value of cooperative
games satisfies an efficiency property. In our case, this property implies that
X

b
b
Φl (vX
i |T ) = vX i (T ).

l∈T

9

Applying this result we obtain that:
X

IlΦ (aj , b, T )

l∈T

=

X 1
nbaj
l∈T

=

1
nbaj

=

1
nbaj

=

1
nbaj

X

b
Φl (vX
i |T )

(X i ,Y i )∈Mbaj

X

X

b
Φl (vX
i |T )

(X i ,Y i )∈Mbaj l∈T

X

b
vX
i (T )

(X i ,Y i )∈Mbaj

X
(X i ,Y i )∈Mbaj

1
| AK\T |

X

fbM (XTi , a0K\T ).

a0K\T ∈AK\T

To show that I Φ satisfies Balanced Contributions, let aj ∈ Aj , b ∈
B, T ⊆ K (T 6= ∅), and l, m ∈ T with l 6= m. Myerson (1980) proves
that the Shapley value of cooperative games satisfies a property of balanced
contributions. In our case, this property implies that
b
b
b
b
Φl (vX
i |T ) − Φl (vX i |T \{m} ) = Φm (vX i |T ) − Φm (vX i |T \{l} ).

Applying this result we obtain that:

=

IlΦ (aj , b, T ) − IlΦ (aj , b, T \{m})
X
1
1
b
Φl (vX
i |T ) −
b
naj i i
nbaj
b
(X ,Y )∈Maj

=
=
=
=

1
nbaj
1
nbaj
1
nbaj

X

X

b
Φl (vX
i |T \{m} )

(X i ,Y i )∈Mbaj

b
b
[Φl (vX
i |T ) − Φl (vX i |T \{m} )]

(X i ,Y i )∈Mbaj

X

b
b
[Φm (vX
i |T ) − Φm (vX i |T \{l} )]

(X i ,Y i )∈Mbaj

X

b
Φm (vX
i |T ) −

(X i ,Y i )∈Mbaj

1
nbaj

Φ (a , b, T ) − I Φ (a , b, T \{l}).
Im
j
m j

10

X
(X i ,Y i )∈Mbaj

b
Φm (vX
i |T \{l} )

Uniqueness. We show uniqueness by induction on the size of T . Suppose
that I 1 and I 2 are two influence measures satisfying f M -Efficiency and
Balanced Contributions. If | T |= 1, by f M -Efficiency,
I 1 (aj , b, T ) =

1
nbaj

X

b
2
vX
i (T ) = I (aj , b, T ).

(X i ,Y i )∈Mbaj

Assume now that I 1 (aj , b, S) = I 2 (aj , b, S) for all S ⊆ T with 1 ≤ |S| < |T |.
Then by Balanced Contributions, for all l, m ∈ T , l 6= m,
2
1
(aj , b, T ).
(aj , b, T ) = Il2 (aj , b, T ) − Im
Il1 (aj , b, T ) − Im

(4)

Using f M -Efficiency,

X

Il1 (aj , b, T ) =

X

Il2 (aj , b, T ).

(5)

l∈T

l∈T

By (4) and (5) it is obtained that:
Il1 (aj , b, T ) = Il2 (aj , b, T ) for all l ∈ T.
This last expression gives the uniqueness. 2

Remark 4. From the property of f M -Efficiency of I Φ it immediately folX
lows that the amount T Φ (aj , b, K) :=
IlΦ (aj , b, K) is
l∈K

T Φ (aj , b, K) =

1
nbaj

X

b
vX
i (K).

(X i ,Y i )∈Mbaj

Notice that T Φ (aj , b, K) belongs to [0, 1] and it can be interpreted as an
estimate of the probability that the corresponding response of an individual
with characteristic aj falls in group b. Also IjΦ (aj , b, K) is the part that
11

corresponds to the feature Xj of the distribution between the k features of
the quantity given by the number T Φ (aj , b, K). In this way, the evolutions
of the numbers {IjΦ (aj , b, K) | aj ∈ Aj } and {T Φ (aj , b, K) | aj ∈ Aj } are
very illustrative of the influence of the various values of Xj on Y taking the
value b. For example, if for aj and b we observe that IjΦ (aj , b, K) is close to
T Φ (aj , b, K) and that the latter is close to 1, we can conclude that individuals
with feature Xj equal to aj have a high probability of being classified in b and
that this is mainly due to feature Xj .

3. Empirical results
In this section we show the performance of the proposed influence measure (2) by means of a computational study. Two different experiments have
been carried out using the software R. The objective of such simulations is
to corroborate that the results obtained by the methodology introduced in
the current work are in accordance with the expected ones. The classifier
used in this paper is Breiman’s random forest classifier (Breiman, 2001),
implemented in Weka1 and used through RWeka2 . This choice is motivated
by the excellent result of the random forest type classifiers (see, for example, Fernández-Delgado et al., 2014). The code was run on a quad-core
Intel i7-8665U CPU with 16GB of RAM.
The procedure adopted in both experiments is as follows. We start from
a sample of individuals from which their attributes and response are known,
M = {(X i , Y i )}ni=1 . Right after, such sample is used to train a classifier
previously chosen, obtaining f M . The purpose of this work is not to classify
1
2

http://www.cs.waikato.ac.nz/ml/weka.
https://cran.r-project.org/web/packages/RWeka/index.html.

12

new instances, but to study the influence of features on classification. In
accordance with Remark 4, to evaluate the influence of feature Xj on the
response Y taking the value b, the quantities IjΦ (aj , b, K) and T Φ (aj , b, K)
are computed and analysed for all aj ∈ Aj .
For the first experiment, a sample of 1000 instances with four binary features {X1 = A, X2 = B, X3 = C, X4 = D} was generated. Such attributes
take the values 0 and 1 with probability 0.5 (hence, aj ∈ Aj = {0, 1}, j ∈ K).
In half of the instances, the class of the response variable is determined by
the value of the attribute A (i.e., Y = A); while in the remaining instances,
it is feature B the one which defines the class (i.e., Y = B); note that
b ∈ B = {0, 1}. The following step is to select those observations whose assigned class was b = 1. Afterwards, for each attribute Xj , j ∈ K, and each
of its possible values, we study the influence that such feature had on the
response when it took such value. Since the procedure by which the class
has been generated is known, it is evident that the influence of attributes C
and D should be independent of their values. Furthermore, the value 1 for
features A and B should have a stronger influence in the classification than
the value 0. Table 3.1 and Figures 3.1, 3.2, 3.3 and 3.4 present the results
obtained for this simulation, which took a runtime of 32.8 minutes.
Indeed, it can be seen that for attributes A and B the value IjΦ (aj , b, K)
is closer to T Φ (aj , b, K) when aj = 1, and that the former is near 1. However,
such a difference can not be observed for features C and D, implying that
new instances have the same probability of being classified as 0 or 1 whatever
their values for C and D are.

13

Xj , j ∈ K

aj

T Φ (aj , b, K)

IjΦ (aj , b, K)

0

0.489

-0.127

1

0.834

0.369

0

0.471

-0.137

1

0.851

0.380

0

0.759

0.122

1

0.758

0.123

0

0.749

0.113

1

0.768

0.132

A

B

C

D
Table 3.1: Results for simulation 1.

●

●

●

●

●

●

●

●
●

●

●

●

Figure 3.1: Classification and influence

Figure 3.2: Classification and influence

for feature A (Simulation 1).

for feature B (Simulation 1).

14

●

●

●

●

●

●

●

●

●

●

●

●

Figure 3.3: Classification and influence

Figure 3.4: Classification and influence

for feature C (Simulation 1).

for feature D (Simulation 1).

The second experiment differs from the previous one in the procedure to
assign the class to the instances. The response is now generated as a binary
vector which takes the values 0 and 1 with probability 0.5, independently of
the attributes. The goal of this simulation is to show that the influence of
the features in the classification of the instances with response b = 1 does
not depend on the features’ values. Table 3.2 and Figures 3.5, 3.6, 3.7 and
3.8 present the results obtained for this simulation. The computational time
was 33.4 minutes.

15

Xj , j ∈ K

aj

T Φ (aj , b, K)

IjΦ (aj , b, K)

0

0.512

0.122

1

0.519

0.133

0

0.506

0.124

1

0.525

0.134

0

0.549

0.165

1

0.480

0.094

0

0.524

0.139

1

0.506

0.116

A

B

C

D
Table 3.2: Results for simulation 2.

Again, the outcomes are as expected: for each feature, there are barely
differences in the values IjΦ (aj , b, K) and T Φ (aj , b, K) when aj changes.

●

●

●

●

●

●

●

●

●

●

●

●

Figure 3.5: Classification and influence

Figure 3.6: Classification and influence

for feature A (Simulation 2).

for feature B (Simulation 2).

16

●

●

●

●

●
●

●

●

●
●
●
●

Figure 3.7: Classification and influence

Figure 3.8: Classification and influence

for feature C (Simulation 2).

for feature D (Simulation 2).

In view of the previous results, our methodology seems to be appropriate
to study the influence that the different features’ values have on the classification of individuals. Since the experiments are satisfactory, this analytic
tool is ready to be applied to problems taken from real life. Consequently,
this procedure has been employed in a real dataset concerning COVID-19
patients, whose results are presented in the next section.
4. Application to the classification of COVID-19 patients
This section analyses a database of 10,454 patients from Galicia (a region
in the northwest of Spain) infected with COVID-19 from March 6, 2020 to
May 7, 2020. The objective is to study the influence of various patients’
characteristics in three binary response variables of special interest: the need
for hospitalisation, the need for ICU admission, and the eventual decease.
The emphasis is not on the predictive classification of new patients, but on
the analysis of the characteristics that influenced that the patients whose
17

complete history is known had a positive response in the binary variables
indicated.
The features or attributes (categorical variables) which have been considered in this study are the following:
• Age: 0 (0-49 y/o), 1 (50-64 y/o), 2 (65-79 y/o), 3 (80 y/o and over).
• Sex: 0 (woman), 1 (man).
• Cardiovascular Diseases: 0 (without diseases), 1 (mild diseases), 2
(severe diseases: ischemia with angina, infarction, stroke).
• Respiratory Diseases: 0 (no diseases), 1 (mild diseases), 2 (severe
diseases: malignancy, COPD, pneumonia).
• Metabolic Diseases: 0 (no diseases), 1 (mild diseases), 2 (severe
diseases: malignancy, insulin-dependent diabetes).
• Urinary Diseases: 0 (none or mild diseases), 1 (severe diseases: malignancy, kidney failure).
The binary response variables considered in this application are:
• Decease (exitus): 0 (no), 1 (yes).
• ICU admission: 0 (no), 1 (yes).
• Need for Hospitalisation: 0 (no), 1 (yes).
It is interesting to note that the methodology developed in Section 2
can be used to evaluate the influence not only of the features chosen, but
also of other characteristics present in the database, on the aforementioned
response variables or on others that may be considered of interest. Some of
18

these specific attributes could be asthma, COPD, hypertension, obesity or
diabetes.
Having seen the suitable results obtained in Section 3 for the experiments described, we will make use of the previous procedure to estimate
the influence of the features in the classification with respect to a binary
response variable Y (which takes values in B = {0, 1}). For instance, the
interest would reside in selecting those individuals who resulted in decease
(that is, b = 1) when our purpose is to know what are the most influential
attributes for the exitus. Note that to estimate the influence of feature Xj
on Y , we use the influence that Xj has in the classification of the elements
of the sample M using an excellent classifier, since it is precisely trained
with the sample M. As in the previous section, we use the random forest
classifier introduced by Breiman (2001) and implemented in R through the
RWeka library.
In order to facilitate the reading of this document, a detailed study of the
influence of the feature age on the three classification problems is presented.
The results obtained for the remaining attributes are left to Appendix A.
Let {X1 = age, X2 = sex, X3 = cardi, X4 = resp, X5 = meta, X6 = uri}
be the set of the features. Table 4.1 shows, for each category of the feature
age, the frequency of such category in the sample (in parentheses), the estimated probability that an individual with age in the considered category
will decease (in the Classification row) and the evaluation of the influence
that age really has on that prediction (in the Influence row). Formally,
the Classification row contains the values {T Φ (aj , decease = 1, K) | Xj =
age, aj ∈ {0, 1, 2, 3}} (see Remark 4) and the Influence row contains the
values {IjΦ (aj , decease = 1, K) | Xj = age, aj ∈ {0, 1, 2, 3}} (see Definition 1 and Theorem 2). Note that for each age category (that is, aj when
19

Xj = age), the influence IjΦ (aj , decease, K) is the share-out which corresponds to the age after distributing the classification T Φ (aj , decease, K)
among the features. Thus, the closer the influence is to the classification on
a certain value of the feature considered, the higher the relative influence of
such feature on the classification. On the other hand, the higher the classification in a certain value of the considered feature, the higher the probability
of a decease taking place if an individual presents that feature’s value.
age - decease
0 (6)

1 (35)

2 (132)

3 (371)

Classification

0.010

0.180

0.134

0.287

Influence

-0.083

0.010

0.015

0.198

Table 4.1: Values of Classification and Influence for the different levels of feature age when
studying decease=1.

In Figure 4.1 the numbers from Table 4.1 are represented for a better
visualisation of their joint evolutions. In this case, it can be observed that the
influence of age in a patient’s decease grows as we move forward in the age
ranges but, in any case, such influence only seems relevant for the third age
bracket (from 80 y/o onwards); the influence is negative for those individuals
under 50 years old. It is observed that the estimated probability of decease
is slightly lower for age range 2 than for age group 1, contrary to what might
be expected; however, taking into consideration that the influence of age on
decease in these two age groups is very low, the difference in such estimated
probability will be due to a greater presence of other risk features in this
specific sample for individuals in range 1 than for individuals in range 2. In
any case, what seems to be drawn from this study is that the clear influence
20

in a greater probability of decease occurs in individuals over 80 y/o and not
in other age groups.

●

●

●

●
●
●

●

●

●

●

Figure 4.1: Evolution of Classification and Influence for the different levels of feature age
when studying decease=1.

For the classification problem of ICU admission, Table 4.2 and Figure 4.2
show an evident drop in the Classification and Influence of age between the
individuals of ranges 2 and 3. In fact, the estimated probability of being
admited in the ICU is the lowest for patients older than 80 years old. This
could be related to the fact that those individuals are the ones who have
a higher probability of passing away, as we have previously studied, even
before their admission in the ICU.

21

age - ICU
0 (29)

1 (75)

2 (170)

3 (10)

Classification

0.042

0.084

0.135

0.043

Influence

-0.005

0.035

0.064

-0.025

Table 4.2: Values of Classification and Influence for the different levels of feature age when
studying ICU=1.

●

●

●

●
●
●

●
●

●
●

Figure 4.2: Evolution of Classification and Influence for the different levels of feature age
when studying ICU=1.

In the case of need for hospitalisation, the Influence and Classification
increase as we advance in the first three age groups. However, there is barely
differences between the last two age ranges, as presented in Table 4.3 and
Figure 4.3. The influence of age in being hospitalised is lightly lower for
those over 80, even though the estimated probability is somewhat higher for
these patients.

22

age - hospitalisation
0 (292)

1 (485)

2 (923)

3 (793)

Classification

0.124

0.269

0.484

0.505

Influence

-0.112

0.021

0.209

0.193

Table 4.3: Values of Classification and Influence for the different levels of feature age when
studying hospitalisation=1.

●

●

●

●

●
●

●

●

●

●

Figure 4.3: Evolution of Classification and Influence for the different levels of feature age
when studying hospitalisation=1.

Results obtained for the feature age have been analysed in detail, in
order to suggest guidelines of interpretation to readers. The outcomes for
the rest of the attributes are presented in Appendix A, as a means to avoid
overloading this document.

23

Acknowledgements
The authors are grateful to Ricardo Cao Abad and to the Dirección Xeral
de Saúde Pública of the Xunta de Galicia in Spain. This work has been
supported by the ERDF, the Government of Spain/AEI [grants MTM201787197-C3-1-P and MTM2017-87197-C3-3-P]; the Xunta de Galicia [Grupos de Referencia Competitiva ED431C-2016-015 and ED431C-2017/38, and
Centro Singular de Investigación de Galicia ED431G/01]; and by the collaborative research project of the IMAT “Mathematical, statistical and dynamic study of the epidemic COVID-19”, subsidized by the Vice-Rector’s
Office for Research and Innovation at the University of Santiago de Compostela, Spain. The research of Laura Davila-Pena has been funded by the
Government of Spain [grant FPU17/02126].

References
Agor, J., & Özaltın, O. Y. (2019). Feature selection for classification models
via bilevel optimization. Computers & Operations Research, 106 , 156–
168. doi:10.1016/j.cor.2018.05.005.
Anaraki, J. R., & Usefi, H. (2019). A feature selection based on perturbation
theory. Expert Systems with Applications, 127 , 1–8. doi:10.1016/j.eswa.
2019.02.028.
Benı́tez-Peña, S., Blanquero, R., Carrizosa, E., & Ramı́rez-Cobo, P. (2019).
Cost-sensitive feature selection for support vector machines. Computers
& Operations Research, 106 , 169–178. doi:10.1016/j.cor.2018.03.005.
Breiman, L. (2001). Random forests. Machine Learning, 45 , 5–32. doi:10.
1023/A:1010933404324.
24

Cura, T. (2020). Use of support vector machines with a parallel local search
algorithm for data classification and feature selection. Expert Systems
with Applications, 145 , 113133. doi:10.1016/j.eswa.2019.113133.
Datta, A., Datta, A., Procaccia, A. D., & Zick, Y. (2015). Influence in
classification via cooperative game theory. In Twenty-Fourth International
Joint Conference on Artificial Intelligence (pp. 511–517).
Fernández-Delgado, M., Cernadas, E., Barro, S., & Amorim, D. (2014). Do
we need hundreds of classifiers to solve real world classification problems?
Journal of Machine Learning Research, 15 , 3133–3181.
Garcı́a-Jurado, I., Méndez-Naya, L., & Sánchez-Sellero, C. (2004). Density estimation using game theory. Mathematical Methods of Operations
Research, 59 , 349–357. doi:10.1007/s001860400349.
Ghaddar, B., & Naoum-Sawaya, J. (2018). High dimensional data classification and feature selection using support vector machines. European
Journal of Operational Research, 265 , 993–1004. doi:10.1016/j.ejor.
2017.08.040.
Moretti, S., & Patrone, F. (2008). Transversality of the Shapley value. TOP ,
16 , 1–41. doi:10.1007/s11750-008-0044-5.
Myerson, R. B. (1980). Conference structures and fair allocation rules. International Journal of Game Theory, 9 , 169–182. doi:10.1007/BF01781371.
Shapley, L. S. (1953). A value for n-person games. In H. W. Kuhn, &
A. W. Tucker (Eds.), Contributions to the Theory of Games (AM-28),
Volume II (pp. 307–318).

Princeton University Press.

9781400881970-018.
25

doi:10.1515/

Strumbelj, E., & Kononenko, I. (2010). An efficient explanation of individual
classifications using game theory. Journal of Machine Learning Research,
11 , 1–18.
Wang, F., & Rudin, C. (2015). Falling rule lists. In Artificial Intelligence
and Statistics (pp. 1013–1022).

Appendix A. Results for COVID-19 patients

●

●

●

●

●

●

●

●

●

●
●
●

●

●

Figure A.1: Evolution of Classification and Influence for the different levels of feature sex
(left) and cardi (right) when studying decease=1.

26

●

●

●

●

●

●

●
●

●
●

●

●
●
●

●

●

Figure A.2: Evolution of Classification and Influence for the different levels of feature resp
(left) and meta (right) when studying decease=1.

●

●

●

●

●
●

Figure A.3: Evolution of Classification and Influence for the different levels of feature uri
when studying decease=1.

27

●

●

●

●

●

●

●

●

●
●

●

●

●

●

Figure A.4: Evolution of Classification and Influence for the different levels of feature sex
(left) and cardi (right) when studying ICU=1.

●

●

●

●

●

●

●

●
●
●

●

●

●

●

●

●

Figure A.5: Evolution of Classification and Influence for the different levels of feature resp
(left) and meta (right) when studying ICU=1.

28

●

●

●

●

●
●

Figure A.6: Evolution of Classification and Influence for the different levels of feature uri
when studying ICU=1.

●

●

●

●

●

●

●
●

●

●

●
●

●

●

Figure A.7: Evolution of Classification and Influence for the different levels of feature sex
(left) and cardi (right) when studying hospitalisation=1.

29

●

●

●

●

●

●

●

●
●
●

●
●

●

●

●

●

Figure A.8: Evolution of Classification and Influence for the different levels of feature resp
(left) and meta (right) when studying hospitalisation=1.

●

●

●

●

●
●

Figure A.9: Evolution of Classification and Influence for the different levels of feature uri
when studying hospitalisation=1.

30

