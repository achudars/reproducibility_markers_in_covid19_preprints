An Evaluation of Two Commercial Deep Learning-Based Information
Retrieval Systems for COVID-19 Literature
Sarvesh Soni, Kirk Roberts
School of Biomedical Informatics
University of Texas Health Science Center at Houston
Houston TX, USA
{sarvesh.soni, kirk.roberts}@uth.tmc.edu

Abstract

arXiv:2007.03106v2 [cs.IR] 27 Jul 2020

The COVID-19 pandemic has resulted in a
tremendous need for access to the latest scientific information, primarily through the use
of text mining and search tools. This has led
to both corpora for biomedical articles related
to COVID-19 (such as the CORD-19 corpus
(Wang et al., 2020)) as well as search engines to query such data. While most research
in search engines is performed in the academic field of information retrieval (IR), most
academic search engines–though rigorously
evaluated–are sparsely utilized, while major
commercial web search engines (e.g., Google,
Bing) dominate. This relates to COVID-19
because it can be expected that commercial
search engines deployed for the pandemic will
gain much higher traction than those produced
in academic labs, and thus leads to questions about the empirical performance of these
search tools. This paper seeks to empirically
evaluate two such commercial search engines
for COVID-19, produced by Google and Amazon, in comparison to the more academic prototypes evaluated in the context of the TRECCOVID track (Roberts et al., 2020). We performed several steps to reduce bias in the available manual judgments in order to ensure a
fair comparison of the two systems with those
submitted to TREC-COVID. We find that the
top-performing system from TREC-COVID
on bpref metric performed the best among the
different systems evaluated in this study on all
the metrics. This has implications for developing biomedical retrieval systems for future
health crises as well as trust in popular health
search engines.

1

Background and Significance

There has been a surge of scientific studies related
to COVID-19 due to the availability of archival
sources as well as the expedited review policies
of publishing venues. A systematic effort to consolidate the flood of such information content, in

the form of scientific articles, along with studies
from the past that may be relevant to COVID-19 is
being carried out as requested by the White House
(Wang et al., 2020). This effort led to the creation
of CORD-19, a dataset of scientific articles related
to COVID-19 and the other viruses from the coronavirus family. One of the main aims for building such a dataset is to bridge the gap between
machine learning and biomedical expertise to surface insightful information from the abundance of
relevant published content. The TREC-COVID
challenge was introduced to target the exploration
of the CORD-19 dataset by gathering the information needs of biomedical researchers (Roberts
et al., 2020; Voorhees et al., 2020). The challenge involved an information retrieval (IR) task
to retrieve a set of ranked relevant documents for a
given query. Similar to the task of TREC-COVID,
major technology companies Amazon and Google
also developed their own systems for exploring the
CORD-19 dataset.
Both Amazon and Google have made recent
forays into biomedical natural language processing (NLP). Amazon launched Amazon Comprehend Medical (ACM) for the developers to process unstructured medical data effectively (KassHout and Wood, 2018). This motivated several
researchers to explore the tool’s capability in information extraction (Bhatia et al., 2019; Guzman
et al., 2020; Heider et al., 2020). Interestingly,
the same technology is also incorporated to their
search engine for the CORD-19 dataset. It will be
useful to assess the overall performance of their
search engine that utilizes the company’s NLP
technology. Similarly, BERT from Google (Devlin et al., 2019) is enormously popular. BERT is
a powerful language model that is trained on large
raw text datasets to learn the nuances of natural
language in an efficient manner. The methodology of training BERT helps it transfer the knowl-

edge from vast raw data sources to other specific domains such as biomedicine. Several works
have explored the efficacy of BERT models in the
biomedical domain for tasks such as information
extraction (Wu et al., 2020) and question answering (Soni and Roberts, 2020). Many biomedical
and scientific variants of the model have also been
built, such as BioBERT (Lee et al., 2019), Clinical BERT (Alsentzer et al., 2019), and SciBERT
(Beltagy et al., 2019). Google has even incorporated BERT into their web search engine (Nayak,
2019). Since this is the same technology that powers Google’s CORD-19 search explorer, it will be
interesting to assess the performance of this search
tool.
However, despite the popularity of these companies’ products, no formal evaluation of these
systems is made available by the companies. Also,
neither of these companies participated in the
TREC-COVID challenge. In this paper, we aim
to evaluate these two IR systems and compare
against the runs submitted to TREC-COVID challenge to gauge the efficacy of what are likely highutilized search engines.

2
2.1

Methods
Information Retrieval Systems

We evaluate two publicly available IR systems targeted toward exploring the COVID-19 Open Research Dataset (CORD-19)1 (Wang et al., 2020).
These systems are launched by Amazon (CORD19 Search2 ) and Google (COVID-19 Research Explorer3 ). We hereafter refer to these systems by
the names of their corporations, i.e., Amazon and
Google. Both the systems take as input a query
in the form of natural language and return a list of
documents from the CORD-19 dataset ranked by
their relevance to the given query.
Amazons system uses an enriched version of
the CORD-19 dataset constructed by passing
it through a language processing service called
Amazon Comprehend Medical (ACM) (KassHout and Snively, 2020). ACM is a machine
learning-based natural language processing (NLP)
pipeline to extract clinical concepts such as signs,
symptoms, diseases, and treatments from unstructured text (Kass-Hout and Wood, 2018). The
1
https://www.semanticscholar.org/
cord19
2
https://cord19.aws
3
https://covid19-research-explorer.
appspot.com

data is further mapped to clinical topics related
to COVID-19 such as immunology, clinical trials,
and virology using multi-label classification and
inference models. After the enrichment process,
the data is indexed using Amazon Kendra that also
uses machine learning to provide natural language
querying capabilities for extracting relevant documents.
Googles system is based on a semantic search
mechanism powered by BERT (Devlin et al.,
2019), a deep learning-based approach to pretraining and fine-tuning for downstream NLP tasks
(document retrieval in this case) (Hall, 2020). Semantic search, unlike lexical term-based search
that aims at phrasal matching, focuses on understanding the meaning of user queries for searching. However, deep learning models such as BERT
require a substantial amount of annotated data to
be tuned for some specific task/domain. Biomedical articles have very different linguistic features
than the general domain, upon which the BERT
model is built. Thus, the model needs to be tuned
for the target domain, i.e., biomedical domain, using annotated data. For this purpose, they use
biomedical IR datasets from the BioASQ challenges4 . Due to the smaller size of these biomedical datasets, and the large data requirement of the
neural models, they use a synthetic query generation technique to augment the existing biomedical IR datasets (Ma et al., 2020). Finally, these
expanded datasets are used to fine-tune the neural model. They further enhance their system by
combining term- and neural-based retrieval models by balancing the memorization and generalization dynamics (Jiang et al., 2020).
2.2

Evaluation

We use a topic set collected as part of the TRECCOVID challenge for our evaluations (Roberts
et al., 2020; Voorhees et al., 2020). These topics
are a set of information need statements motivated
by searches submitted to the National Library of
Medicine and suggestions from researchers on
Twitter. Each topic consists of three fields with
varying levels of granularity in terms of expressing
the information need, namely, (a keyword-based)
query, (a natural language) question, and (a longer
descriptive) narrative. A few example topics from
Round 1 of the challenge are presented in Table
1. The challenge participants are required to re4

http://bioasq.org

Topic 30

Topic 10

Topic 7

Table 1: Three example topics from Round 1 of the TREC-COVID challenge.

Query : serological tests for coronavirus
Question : are there serological tests that detect antibodies to coronavirus?
Narrative : looking for assays that measure immune response to coronavirus that will help
determine past infection and subsequent possible immunity.
Query : coronavirus social distancing impact
Question : has social distancing had an impact on slowing the spread of COVID-19?
Narrative : seeking specific information on studies that have measured COVID-19’s transmission in one or more social distancing (or non-social distancing) approaches.
Query : coronavirus remdesivir
Question : is remdesivir an effective treatment for COVID-19?
Narrative : seeking specific information on clinical outcomes in COVID-19 patients treated
with remdesivir.

turn a ranked list of documents for each topic (also
known as runs). The first round of TREC-COVID
used a set of 30 topics and exploited the April 10,
2020 release of CORD-19. Round 1 of the challenge was initiated on April 15, 2020 with the runs
from participants due April 23. Relevance judgments were released May 3.
We use the question and narrative fields from
the topics to query the systems developed by Amazon and Google. These fields are chosen following the recommendations set forward by the organizations, i.e., to use fully formed queries with
questions and context. We use two variations for
querying the systems. In the first variation, we
query the systems using only the question. In the
second variation, we also append the narrative to
provide more context.
As we accessed these systems in the first week
of May 2020, the systems could be using the latest version of CORD-19 at that time (i.e., May 1
release). Thus, we filter the list of returned documents and only include the ones from the April 10
release to ensure a fair comparison with the submissions to the Round 1 of TREC-COVID challenge. We compare the performance of these systems (by Amazon and Google) with the 5 top submissions to the TREC-COVID challenge Round
1 (on the basis of bpref scores). It is valid to
compare Amazon and Google systems with the
submissions from Round 1 because all these systems are similarly built without using any relevance judgments from TREC-COVID.
Relevance judgments (or assessments) for
TREC-COVID are carried out by individuals with
biomedical expertise. The assessments are performed using a pooling mechanism where only

the top-ranked results from different submissions
are assessed. A document is assigned one of the
three possible judgments, namely, relevant, partially relevant, or not relevant. We use relevance
judgments from Rounds 1 and 2. However, even
the combined judgments from both the rounds
may not ensure that the relevance judgments for
top-n documents for both the evaluated systems
exist. It has recently been shown that pooling effects can negatively impact post-hoc evaluation of
systems that did not participate in the pooling (Yilmaz et al., 2020). So, to create a level ground for
comparison, we perform additional relevance assessments for the documents from evaluated systems that may not have been covered by the combined set of judgments from TREC-COVID. In total, 141 documents were assessed by 2 individuals
who are also involved in performing the relevance
judgments for TREC-COVID.
The runs submitted to TREC-COVID could
contain up to 1000 documents per topic. Due to
the restrictions posed by the evaluated systems, we
could only fetch up to 100 documents per query.
This number further decreases when we remove
the documents that are not covered as part of the
April 10 release of CORD-19. Thus, to ensure a
fair comparison of the evaluated systems with the
runs submitted to TREC-COVID, we calculate the
minimum number of documents per topic (we call
it topic-minimum) across the different variations
of querying the evaluated systems (i.e., question
or question+narrative). We then use this topicminimum as a threshold for the maximum number of documents per topic for all evaluated systems. This ensures that each system returns the
same number of documents for a particular topic.

Table 2: Evaluation results after setting a threshold at the number of documents per topic using a minimum number
of documents present for each individual topic. The relevance judgments used are a combination of Rounds 1 and
2 of TREC-COVID and our additional relevance assessments. The highest scores for the evaluated and TRECCOVID systems are underlined.

TREC-COVID

System
question
Amazon
question + narrative
question
Google
question + narrative
1. sab20.1.meta.docs
2. sab20.1.merged
3. UIowaS Run3
4. smith.rm3
5. udel fang run3

P@5
0.6733
0.72
0.5733
0.6067
0.78
0.6733
0.6467
0.6467
0.6333

P@10
0.6333
0.64
0.57
0.56
0.7133
0.6433
0.6367
0.6133
0.6133

3

Figure 1: A box plot of the number of documents for
each topic as used in our evaluations (after filtering
the documents based on the April 10th release of the
CORD-19 dataset and setting a threshold at the minimum number of documents for any given topic).

We use the standard measures in our evaluation as employed for TREC-COVID, namely,
bpref (binary preference), NDCG@10 (normalized discounted cumulative gain with top 10 documents), and P@5 (precision at 5 documents).
Here, bpref only uses judged documents in calculation while the other two measures assume the
non-judged documents to be not relevant. Additionally, we also calculate MAP (mean average
precision), NDCG, and P@10. Note that we can
precisely calculate some of the measures that cut
the number of documents at up to 10 since we
have ensured that both the evaluated systems (for
both the query variations) have their top 10 documents manually judged (through TREC-COVID
judgments and our additional assessments as part
of this study). We use the trec eval tool5 for our
evaluations, which is a standard system employed
for the TREC challenges.

MAP
0.0722
0.0766
0.0693
0.0687
0.0999
0.0787
0.0952
0.0914
0.0857

NDCG
0.1838
0.1862
0.1831
0.1821
0.2266
0.1971
0.2091
0.2095
0.1977

bpref
0.1049
0.1063
0.1069
0.1054
0.1352
0.1154
0.1279
0.1303
0.1187

Results

The total number of documents used for each topic
based on the topic-minimums are shown in the
form of a box plot in Figure 1. Approximately, an
average of 43 documents are evaluated per topic
with a median number of documents as 40.5. This
is another reason for using a topic-wise minimum
rather than cutting off all the systems to the same
level as the lowest return count (that would be 25
documents). Having a topic-wise cut-off allowed
us to evaluate the runs with the maximum possible
documents while keeping the evaluation fair.
The evaluation results of our study are presented
in Table 2. Among the commercial systems that
we evaluated as part of this study, the question
plus narrative variant of the system by Amazon
performed consistently better than any other variant in terms of all the included measures other
than bpref. In terms of bpref, the question-only
variant of the system from Google performed the
best among the evaluated systems. Note that the
best run from the TREC-COVID challenge, after
cutting off using topic-minimums, still performed
better than the other four submitted runs included
in our evaluation. Interestingly, this best run also
performed substantially better than all the variants
of both commercial systems evaluated as part of
the study on all the calculated metrics. We discuss
more about this system below.

4
5
https://github.com/usnistgov/trec_
eval

NDCG@10
0.539
0.5583
0.4972
0.5112
0.6109
0.5555
0.5466
0.5225
0.5398

Discussion

We evaluate two commercial IR systems targeted
toward extracting relevant documents from the

CORD-19 dataset. For comparison, we also include the 5 best runs from TREC-COVID in our
evaluation. We additionally annotate a total of
141 documents from the runs by the commercial systems to ensure a fair comparison between
these runs and the runs from TREC-COVID challenge. We find that the best system from TRECCOVID in terms of bpref metric outperformed all
the commercial system variants on all the evaluated measures including P@5, NDCG@10, and
bpref, which are the standard measures used in
TREC-COVID.
The commercial systems often employ cutting
edge technologies, such as ACM and BERT used
by Amazon and Google, while developing their
systems. Also, the availability of technological resources such as CPUs and GPUs may be better in
industry settings than in academic settings. This
follows a common concern in academia, namely
that the resource requirements for advanced machine learning methods (e.g., GPT-3 (Brown et al.,
2020)) are well beyond the capabilities available
to the vast majority of researchers. However, instead these results demonstrate the potential pitfalls of deploying a deep learning-based system
without proper tuning. The sabir (sab20.*) system
does not use machine learning at all: it is based
on the very old SMART system (Buckley, 1985)
and does not utilize any biomedical resources. It
is instead carefully deployed based on an analysis
of the data fields available in CORD-19. Subsequent rounds of TREC-COVID have since overtaken sabir (based indeed on machine learning
with relevant training data). The lesson, then, for
future emerging health events is that deploying
“state-of-the-art” methods without event-specific
data may be dangerous, and in the face of uncertainty simple may still be best.
As evident from Figure 1, many of the documents retrieved by the commercial systems were
not part of the April 10 release of CORD-19. We
queried these systems after another version of the
CORD-19 dataset was released. New sources of
papers were constantly being added to the dataset
alongside updating the content of existing papers and adding newly published research related
to COVID-19. This may have led to the retrieval of more articles from the new release of
the dataset. However, for a fair comparison between the commercial and the TREC-COVID systems, we pruned the list of documents and per-

formed additional relevance judgments. We have
included the evaluation results that would have
resulted without our modifications in the supplemental material. The performance of these two
systems drops precipitously. Yet, as addressed,
this would not have been a “fair” comparison and
thus the corrective measures described above were
necessary to ensure the scientific validity of our
comparison.

5

Conclusion

We assessed the performance of two commercial
IR systems using similar evaluation methods and
measures as the TREC-COVID challenge. To
facilitate a fair comparison between these systems and the top 5 runs submitted to the TRECCOVID, we cut all the runs at different thresholds and performed more relevance judgments beyond the assessments provided by TREC-COVID.
We found that the top performing system from
TREC-COVID on bpref metric remained the best
performing system among the commercial and
the TREC-COVID submissions on all the evaluation metrics. Interestingly, this best performing
run comes from a simple system that is purely
based on the data elements present in the CORD19 dataset and does not apply machine learning.
Thus, applying cutting edge technologies without
enough target data-specific modifications may not
be sufficient for achieving optimal results.

Acknowledgments
The authors thank Meghana Gudala and Jordan
Godfrey-Stovall for conducting the additional retrieval assessments. This work was supported in
part by the National Science Foundation (NSF)
under award OIA-1937136.

References
Emily Alsentzer, John Murphy, William Boag, WeiHung Weng, Di Jindi, Tristan Naumann, and
Matthew McDermott. 2019. Publicly Available
Clinical BERT Embeddings. In Proceedings of the
2nd Clinical Natural Language Processing Workshop, pages 72–78.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific
Text. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
3615–3620.

Parminder Bhatia, Busra Celikkaya, Mohammed
Khalilia, and Selvan Senthivel. 2019. Comprehend
Medical: A Named Entity Recognition and Relationship Extraction Web Service. In 2019 18th IEEE
International Conference On Machine Learning And
Applications (ICMLA), pages 1844–1851.

Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and
Ryan McDonald. 2020. Zero-shot Neural Retrieval
via Domain-targeted Synthetic Query Generation.
arXiv:2004.14503 [cs].

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language Models are Few-Shot
Learners. arXiv:2005.14165 [cs].

Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina
Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen
Voorhees, Lucy Lu Wang, and William R. Hersh.
2020. TREC-COVID: Rationale and Structure of an
Information Retrieval Shared Task for COVID-19.
Journal of the American Medical Informatics Association.

Chris Buckley. 1985. Implementation of the SMART
information retrieval system. Technical Report 85686, Cornell University.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
4171–4186.
Benedict Guzman, Isabel Metzger, Yindalon
Aphinyanaphongs, and Himanshu Grover. 2020.
Assessment of Amazon Comprehend Medical:
Medication Information Extraction.
Keith Hall. 2020. An NLU-Powered Tool to Explore
COVID-19 Scientific Literature.
Paul M. Heider, Jihad S. Obeid, and Stéphane M.
Meystre. 2020.
A Comparative Analysis of
Speed and Accuracy for Three Off-the-Shelf DeIdentification Tools. AMIA Summits on Translational Science Proceedings, 2020:241–250.
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and
Michael C. Mozer. 2020. Characterizing Structural
Regularities of Labeled Data in Overparameterized
Models. arXiv:2002.03206 [cs, stat].
Taha A. Kass-Hout and Ben Snively. 2020. AWS
launches machine learning enabled search capabilities for COVID-19 dataset.
Taha A. Kass-Hout and Matt Wood. 2018. Introducing
medical language processing with Amazon Comprehend Medical.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2019. BioBERT: A pre-trained
biomedical language representation model for
biomedical text mining. Bioinformatics, pages 1–7.

Pandu Nayak. 2019. Understanding searches better
than ever before.

Sarvesh Soni and Kirk Roberts. 2020. Evaluation of
Dataset Selection for Pre-Training and Fine-Tuning
Transformer Language Models for Clinical Question Answering. In Proceedings of the LREC, pages
5534–5540.
Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina
Demner-Fushman, William R. Hersh, Kyle Lo, Kirk
Roberts, Ian Soboroff, and Lucy Lu Wang. 2020.
TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection. ACM SIGIR Forum,
54:1–12.
Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,
Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn
Funk, Rodney Kinney, Ziyang Liu, William Merrill, Paul Mooney, Dewey Murdick, Devvret Rishi,
Jerry Sheehan, Zhihong Shen, Brandon Stilson,
Alex D. Wade, Kuansan Wang, Chris Wilhelm,
Boya Xie, Douglas Raymond, Daniel S. Weld,
Oren Etzioni, and Sebastian Kohlmeier. 2020.
CORD-19: The Covid-19 Open Research Dataset.
arXiv:2004.10706v2.
Stephen Wu, Kirk Roberts, Surabhi Datta, Jingcheng
Du, Zongcheng Ji, Yuqi Si, Sarvesh Soni, Qiong
Wang, Qiang Wei, Yang Xiang, Bo Zhao, and Hua
Xu. 2020. Deep learning in clinical natural language
processing: A methodical review. Journal of the
American Medical Informatics Association, 27:457–
470.
Emine Yilmaz, Nick Craswell, Bhaskar Mitra, and
Daniel Campos. 2020. On the Reliability of Test
Collections for Evaluating Systems of Different
Types. In Proceedings of the 43rd International
ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2101–2104.

A

Supplementary Material

The results without taking into account our additional annotations, i.e., only using the relevance
judgments from TREC-COVID rounds 1 and 2,
are presented in Table 3. Similarly, the results
without setting an explicit threshold on the number
of returned documents by the systems are shown in
Table 4. The results without any of the two modifications made by us are provided in Table 5.

Table 3: Evaluation results after setting a threshold at the number of documents per topic using a minimum number
of documents present for each individual topic. The relevance judgments used are a combination of Rounds 1 and
2 of TREC-COVID (WITHOUT our additional relevance assessments). The highest scores for the evaluated and
TREC-COVID systems are underlined.

TREC-COVID

System
question
Amazon
question + narrative
question
Google
question + narrative
1. sab20.1.meta.docs
2. sab20.1.merged
3. UIowaS Run3
4. smith.rm3
5. udel fang run3

P@5
0.6467
0.6933
0.5667
0.56
0.78
0.6667
0.6467
0.6467
0.6333

P@10
0.5933
0.5933
0.5133
0.5133
0.7133
0.64
0.6367
0.6133
0.6133

NDCG@10
0.5095
0.5307
0.4688
0.4795
0.6109
0.5539
0.5466
0.5225
0.5398

MAP
0.069
0.0722
0.0655
0.0656
0.1007
0.0789
0.096
0.0922
0.0866

NDCG
0.1794
0.1804
0.1785
0.1763
0.2278
0.1968
0.2099
0.2107
0.1989

bpref
0.1035
0.1031
0.1048
0.1031
0.1361
0.1155
0.1287
0.1315
0.1196

Table 4: Evaluation results WITHOUT setting a threshold at the number of documents per topic using a minimum
number of documents present for each individual topic. The relevance judgments used are a combination of
Rounds 1 and 2 of TREC-COVID and our additional relevance assessments. The highest scores for the evaluated
and TREC-COVID systems are underlined.

TREC-COVID

System
question
Amazon
question + narrative
question
Google
question + narrative
1. sab20.1.meta.docs
2. sab20.1.merged
3. UIowaS Run3
4. smith.rm3
5. udel fang run3

P@5
0.6733
0.72
0.5733
0.6067
0.78
0.6733
0.6467
0.6467
0.6333

P@10
0.6333
0.64
0.57
0.56
0.7133
0.6433
0.6367
0.6133
0.6133

NDCG@10
0.539
0.5583
0.4972
0.5112
0.6109
0.5555
0.5466
0.5225
0.5398

MAP
0.0765
0.0788
0.0775
0.0763
0.2037
0.1598
0.174
0.1947
0.1911

NDCG
0.1931
0.1903
0.2001
0.1979
0.4702
0.4415
0.4145
0.4461
0.4495

bpref
0.1134
0.1105
0.1227
0.121
0.3404
0.3433
0.3229
0.3406
0.3246

Table 5: Evaluation results WITHOUT setting a threshold at the number of documents per topic using a minimum
number of documents present for each individual topic. The relevance judgments used are a combination of Rounds
1 and 2 of TREC-COVID (WITHOUT our additional relevance assessments). The highest scores for the evaluated
and TREC-COVID systems are underlined.

TREC-COVID

System
question
Amazon
question + narrative
question
Google
question + narrative
1. sab20.1.meta.docs
2. sab20.1.merged
3. UIowaS Run3
4. smith.rm3
5. udel fang run3

P@5
0.6467
0.6933
0.5667
0.56
0.78
0.6667
0.6467
0.6467
0.6333

P@10
0.5933
0.5933
0.5133
0.5133
0.7133
0.64
0.6367
0.6133
0.6133

NDCG@10
0.5095
0.5307
0.4688
0.4795
0.6109
0.5539
0.5466
0.5225
0.5398

MAP
0.0732
0.0744
0.0734
0.0728
0.2038
0.1589
0.1742
0.1956
0.1914

NDCG
0.1888
0.1846
0.1954
0.1919
0.4693
0.4393
0.4139
0.4469
0.4497

bpref
0.1121
0.1074
0.1208
0.1188
0.3406
0.3426
0.3225
0.3413
0.3248

