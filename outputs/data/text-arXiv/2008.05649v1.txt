Sensitivity Analysis of Error-Contaminated
Time Series Data under Autoregressive
Models with Application of COVID-19 Data

arXiv:2008.05649v1 [stat.ME] 13 Aug 2020

Qihuang Zhang and Grace Y. Yi1

Abstract
Autoregressive (AR) models are useful tools in time series analysis. Inferences under such
models are distorted in the presence of measurement error, which is very common in practice. In this article, we establish analytical results for quantifying the biases of the parameter
estimation in AR models if the measurement error effects are neglected. We propose two
measurement error models to describe different processes of data contamination. An estimating equation approach is proposed for the estimation of the model parameters with
measurement error effects accounted for. We further discuss forecasting using the proposed
method. Our work is inspired by COVID-19 data, which are error-contaminated due to
multiple reasons including the asymptomatic cases and varying incubation periods. We
implement our proposed method by conducting sensitivity analyses and forecasting of the
mortality rate of COVID-19 over time for the four most populated provinces in Canada.
The results suggest that incorporating or not incorporating measurement error effects yields
rather different results for parameter estimation and forecasting.

Keywords: Autoregressive Model, COVID-19, Forecasting, Measurement Error, Sensitivity
Analysis, Time Series.
Short title: Time Series with Measurement Error COVID-19
1
Corresponding Author: Department of Statistical and Actuarial Sciences, Department of Computer
Science, University of Western Ontario, London, Ontario, Canada, N6A 5B7. Department of Statistics and
Actuarial Science, University of Waterloo. Email: gyi5@uwo.ca

1

Introduction

Time series data are common in the fields of epidemiology, economics, and engineering.
Various models and methods have been developed for analyzing such data. The validity of
these methods, however, hinges on the condition that time series data are precisely collected.
This condition is restrictive in applications. Measurement error is often inevitable. In the
study of air pollution, for example, it is difficult or even impossible to precisely obtain the
true measurement of the daily air population level.
Some work on time series subject to measurement error is available in the literature.
Tanaka (2002) proposed a Lagrange multiplier test to assess the presence of measurement
error in time series data. Staudenmayer and Buonaccorsi (2005) explored the classical measurement error model for the autoregressive process. Tripodis and Buonaccorsi (2009) studied
measurement error in forecasting using the Kalman filter. Dedecker et al. (2014) considered
a non-linear AR(1) model with measurement error. Despite available discussions of measurement error in time series, several limitations restrict the application scope of the existing
work. Most available methods consider only autoregressive models without the drift and
assume the simplest additive measurement error model. Furthermore, most work involves a
complex formulation to adjust for the measurement error effects, which is not straightforward to implement for practitioners. In addition, to our knowledge, there is no available
work addresses measurement error effects on prediction under autoregressive models.
In this article, we systematically explore analysis of error-prone time series data under
autoregressive models. We propose two types of models to delineate measurement error
processes: additive regression models and multiplicative models. These modeling schemes
offer us great flexibility in facilitating different applications. We investigate the impact of the
naive analysis which ignores the feature of measurement error in the inferential procedures,
and we obtain analytical results for characterizing the biases incurred in the naive analysis.
We develop an estimating equation approach to adjust for the measurement error effects on
time series analysis. We establish asymptotic results for the proposed estimators, and develop
the theoretical results for the forecasting of times series in the presence of measurement
error. Finally, we describe a block bootstrap algorithm for computing standard errors of the
1

proposed estimators.
Our work is partially motivated by the data of COVID-19, a wide-spread disease that has
become a global health challenge and has caused over ten million infections and half million
deaths as of August, 2020. Because of the special features of the disease, the COVID-19 data
introduce a number of new challenges: 1) due to the asymptomatic infected cases and the
patients with light symptoms who do not go to hospitals, the number of reported cases with
COVID-19 is typically smaller than the true number of infected cases; 2) due to the limited
test resources, many infected cases are not able to be identified instantly; and 3) the varying
incubation periods lead to the delay of the identification of the infections. Consequently, the
discrepancy between the reported case number and the true case number can be substantial,
and ignoring these features and applying the traditional time series analysis method would
no longer produce valid results.
In this paper, we apply the developed methods to analyze the COVID-19 data. We are
interested in studying how the mortality rate in a region may change over time and describing
the trajectory of the death rate. While the mortality rate of a disease is defined as the death
number divided by the case number, the determination of the mortality rate of COVID-19
is challenging. In contrast to the standard definition, Baud et al. (2020) estimated mortality
rates by dividing the number of deaths on a given day by the number of patients with
confirmed COVID-19 infections 14 days earlier, driven by the consideration of the maximum
incubation time to be 14 days. Due to the unique features of COVID-19, there does not seem
to be a precise way to define the mortality rate of COVID-19. In this paper, we conduct
sensitivity analyses to assess the severity of the pandemic by using different definitions of the
mortality rate and considering different ways of modeling measurement error in the data.
The remainder of the article is organized as follows. The notation and the setup for
autoregressive time series models and the proposed measurement error models are introduced
in Section 2. In Section 3, we present the theoretical results for characterizing the impact of
measurement error on the analysis of time series data. In Section 4, we develop an estimating
equation approach to adjust for the biases due to measurement error. In Section 5, we
implement the proposed method to analyze the COVID-19 data in four Canadian provinces.
The article is concluded with a discussion presented in Section 6.
2

2

Model Setup and Framework

2.1

Time Series Model

Consider a T × 1 vector of time series, X (T ) = (X1 , X2 , . . . , XT )T . We are interested in
modeling the dependence of Xt on it previous observations X (t−1) and we consider it to be
postulated by an autoregressive model with lag p
Xt = φ0 +

p
X

φj Xt−j + t ,

(1)

j=1

where p is an integer smaller than T , (t) = (1 , . . . , t )T is independent of X (t) = (X1 , . . . , Xt )T
with each t having zero mean and variance σ2 , φ0 is a constant drift, and φ = (φ1 , . . . , φp )T
is the regression coefficient.
The additive form in (1) and the zero mean assumption of t show that φ0 and φ are
constrained by
et−1 )}T φ,
φ0 = E(Xt ) − {E(X

(2)

et−1 = (Xt−1 , . . . , Xt−p )T . To make the process of Xt stationary, φ1 , . . . , φp are further
where X
constrained such that all the roots of the equation in z
z p − φ1 z p−1 − · · · − φp = 0
have absolute values smaller than 1 (Brockwell and Davis 2002, Sec.3.1.). For example,
a stationary AR(1) process requires that |φ1 | < 1, and a stationary AR(2) process needs
that (φ1 + φ2 ) < 1, (φ2 − φ1 ) < 1 and |φ2 | < 1. Here we are interested in the estimation
of parameters, φ and φ0 . Let µ denote the mean E(Xt ) of the time series, which equals
φ0
1−φ1 −...−φp

if Xt is (weakly) stationary. When p = 1, the stationarity of a time series implies

Var(Xt ) =

σ2
1−φ21

2.2

for t = 1, . . . , T .

Estimation of Model Parameters

The estimation of the parameters in the AR(p) time series model (1) can be carried out by
the least squares method. To see this, we first focus on estimation of φ = (φ1 , . . . , φp )T . Let
P
P
S(φ) = Tt=p+1 {Xt − (φ0 + pj=1 φj Xt−j )}2 be the sum of the squared difference between
3

Xt and its linearly combined history with lag p. Then applying the constraint (2) gives
h
i2
P
et−1 − E(X
et−1 )}T φ .
S(φ) = Tt=p+1 {Xt − E(Xt )} − {X
To minimize S(φ) with respect to φ, we solve
(LS)

φb

=

∂S(φ)
∂φ

T
n
on
oT
X
e
e
e
e
Xt−1 − E(Xt−1 ) Xt−1 − E(Xt−1 )

= 0 for φ and obtain the solution

!−1

t=p+1

T
n
o
X
e
e
Xt−1 − E(Xt−1 ) {Xt − E(Xt )} ,
t=p+1

(3)
where for t = 1, . . . , T , E(Xt ) can be estimated by

1
T

PT

t=1

Xt , which is denoted as µ
b.

Next, by the constraint (2), replacing E(Xt ) by µ
b gives an estimator of φ0 :
φb(LS)
b−µ
b
0 = µ

p
X

φbj .

(4)

j=1

Re-expressing (1) as t = Xt − (φ0 +

Pp

j=1

φj Xt−j ) and by the definition of S(φ), we may

estimate Var(t ) = σ2 by
σ
b2(LS) =
=

1
b
S(φ)
T −p
T
X
1

{Xt − E(Xt )}2 −

T − p t=p+1

T
X
2
et−1 − E(X
et−1 )}T φb
{Xt − E(Xt )}{X
T − p t=p+1

T
X
1
et−1 − E(X
et−1 )}{X
et−1 − E(X
et−1 )}T φb
φbT {X
+
T − p t=p+1

(5)

with E(Xt ) estimated by µ
b.
Estimators (3)–(5) can be derived in an alternative way. First, by the stationarity of the
Xt , for k = 0, . . . , p and p ≤ t, Cov(Xt , Xt−k ) is time-independent and let γk denote it; it is
clear that γ0 represents Var(Xt ) for any t. Let Γ be the autocovariance matrix


γ0 · · · γp−1


.. 
 .
..
Γ =  ..
.
. .


γp−1 · · · γ0
bk =
Let γ
b = (γb1 , · · · , γbp )T with γ

1
T −k

PT

t=k+1 (Xt

−µ
b)(Xt−k − µ
b) being an estimator of γk for

b be the estimator of Γ with γk replaced by γbk for k = 0, . . . , p − 1.
k = 0, . . . , p, and let Γ
Next, we examine the summation terms in (3) and (5) by using the fact that as T → ∞,
PT
PT
1
1
2 p
T p
e
e
→ γ0 , T −p
→ γ, and
t=p+1 {Xt − E(Xt )} −
t=p+1 {Xt − E(Xt )}{Xt−1 − E(Xt−1 )} −
T −p
4

1
T −p

PT

t=p+1 {Xt−1 −E(Xt−1 )}{Xt−1 −E(Xt−1 )}

e

e

e

e

T

p

−→ Γ. Then, (3)–(5) motivate an alternative

method of finding estimators for φ, φ0 , and σ2 , by solving the estimating equations:
b−1 γ
φ=Γ
b;
φ0 =

1−

p
X

!
φi

µ
b;

(6)

i=1

b
σ2 = γb0 − 2φT γ
b + φT Γφ,
b φb0 and σ
for φ, φ0 , and σ2 . Let φ,
b2 denote the resultant estimators of φ, φ0 , and σ2 ,
respectively. These estimators are asymptotically equivalent to the least squares estimators
p
p
p
b2(LS) −→ 0, as
→ 0 and σ
b2 − σ
φb(LS) , φb(LS)
b2(LS) in a sense that φb − φb(LS) −→ 0, φb0 − φb(LS)
0 , and σ
0 −

T → ∞, and hence, they are consistent (Box et al. 2015, Ch.7, A.7.4).
Estimating equations (6) offer a unified estimation framework in its connections with
not only the least squares estimation but also the maximum likelihood method under the
assumption of Gaussian error as well as the Yule-Walker method. Similar to the least squares
method, finding estimators using one of those approaches is asymptotically equivalent to
solving (6) for φ, φ0 and σ2 (Box et al. 2015, Ch.7, A.7.4).

3

Measurement Error and Impact

3.1

Measurement Error Models

Suppose that for t = 1, . . . , T , the observation of Xt is subject to measurement error and
the precise measurement of Xt may not be observed, but its surrogate measurement Xt∗ is
available. We consider two measurement error models.
The first measurement error model takes an additive form
Xt∗ = α0 + α1 Xt + et

(7)

for t = 1, . . . , T , where the error term et is independent of Xt with mean 0 and timeindependent variance σe2 and is assumed to be independent for t = 1, . . . , T , and α = (α0 , α1 )T
is the parameter vector. Here, α0 represents the systematic error and α1 represents the
5

constant inflation (or shrinkage) due to the measurement error. For instance, if α0 = 0, then
setting α1 < 1 (or α1 > 1) features the scenario where Xt∗ tends to be smaller (or larger)
than Xt if the noise term is ignored. This model generalizes the classical additive model
considered by Staudenmayer and Buonaccorsi (2005) who considered the case with α0 = 0
and α1 = 1.
By the stationarity of the Xt , we note that model (7) yields E(Xt∗ ) = α0 + α1 µ and
Var(Xt∗ ) = α12 γ0 + σe2 ;

(8)

the variability of the Xt∗ can be greater or smaller than that of the Xt , depending on the
value of α1 .
The second measurement error model assumes a multiplicative form:
Xt∗ = β0 ut Xt ,

(9)

for t = 1, . . . , T , where β0 is a positive scaling parameter, and the ut are the error terms
which are independent of each other as well as of the Xt , and have mean one and timeindependent variance σu2 . Depending on the distribution of the error term ut , (9) can feature
different types of discrepancy between Xt and Xt∗ .
The stationarity of the Xt together with model (9) implies E(Xt∗ ) = β0 µ, and

Var(Xt∗ ) = β02 (σu2 + 1)γ0 + σu2 µ2 ,

(10)

where we use the independence of Xt and ut .
Since E(Xt∗ ) is time-independent for both (7) and (9), in the following discussion, we
let µ∗ denote E(Xt∗ ) for t = 1, . . . , T . The modeling of the measurement error process by
(7) or (9) introduces extra parameters {α0 , α1 , σe2 } or {β0 , σu2 }, where the variance of the
error term is bounded by the variability of Xt∗ together with others. Clearly, (8) shows that
σe2 < Var(Xt∗ ) and (10) implies that σu2 <

3.2

Var(Xt∗ )
.
β02 µ2

Naive Estimation and Bias for AR(1) Model

Estimating equations (6) are useful when measruements of Xt are available. However, due
to the measurement error, Xt is not observed so (6) cannot be directly used for estimation of
6

the parameters for model (1). As the surrogate Xt∗ for Xt is available, one may attempt to
employ the naive analysis to model (1) with Xt replaced by Xt∗ . Here we study the impact
of measurement error on the naive analysis disregarding the difference between Xt and Xt∗ .
We start with the AR(1) model, i.e., model (1) with p = 1.
If we naively replace Xt in (1) by Xt∗ , then the time series model (1) becomes
∗
+ ∗t ,
Xt∗ = φ∗0 + φ∗1 Xt−1

(11)

where (φ∗0 , φ∗1 )T and ∗t show possible differences from the corresponding quantity in the
model (1). To estimate φ∗0 and φ∗1 , we may employ the ordinary least squares (OLS) method.
P
∗
)2 with respective to φ∗0 and
Specifically, we minimize S(φ∗0 , φ∗1 ) = Tt=2 (Xt∗ − φ∗0 − φ∗1 Xt−1
φ∗1 , yielding the OLS estimators of φ∗1 and φ∗0 :
PT
φb∗1

∗
∗
∗
t=2 (Xt−1 − X̄(−1) )(Xt −
PT
∗
∗
2
t=2 (Xt−1 − X̄(−1) )

X̄ ∗ )

,

φb∗0 = X̄t∗ − φb∗1 X̄ ∗ ,

and
∗
=
where X̄(−1)

=

1
T −1

PT

t=2

Theorem 1 Let ω1 =

∗
and X̄ ∗ =
Xt−1

α21 σ2
,
α21 σ2 +σe2 (1−φ21 )

1
T −1

(12)

PT

t=2

Xt∗ .


φ∗1 = φ1 ω1 , and φ∗0 = α0 +

α 1 φ0
1−φ1



(1 − φ1 ω1 ). Assume

the stationarity of the times series. If the measurement error process satisfies (7), then
p
p
(1) φb∗1 −−→ φ∗1 and φb∗0 −−→ φ∗0 as T → ∞,

(2) ∗t = α0 (1 − φ∗1 ) + α1 φ0 − φ∗0 + α1 (φ1 − φ∗1 )Xt−1 + (1 − φ∗1 )et + α1 t for t = 1, . . . , T ,
 2 
σ
+ (1 − ω1 φ1 )2 σe2 + α12 σ2 .
and hence Var(∗t ) = φ21 α12 (1 − ω1 )2 1−φ
2
1

The proof of the theorem is included in Supplementary Appendix A.2. This theorem
essentially implies that the naive estimator under the additive form in (7) is inconsistent
because φ∗1 6= φ1 and φ∗0 6= φ0 . The naive estimator φb∗1 attenuates and the attenuation factor
ω1 depends on the parameters α1 and σe2 of the measurement error model (7) as well as φ1
and σ2 in the time series model (1). The coefficient α1 in the measurement error model
(7) affects the estimation of the both naive estimators φb∗1 and φb∗0 , while the intercept α0
influences the estimation of φ∗0 only, but not φ∗1 or Var(∗ ).
7

Theorem 2 Let ω2 = {1 + σu2 +

2 φ2
(1+φ1 )σu
0 −1
} ,
(1−φ1 )σ2

φ∗1 = φ1 ω2 , and φ∗0 =

β 0 φ0
1−φ1

(1 − ω2 φ1 ). If the

times series is stationary and the measurement error process satisfies (9), then
p
p
(1) φb∗1 −−→ φ∗1 and φb∗0 −−→ φ∗0 as T → ∞,

(2) ∗t = β0 φ0 ut − φ∗0 + β0 Xt−1 (φ1 ut − ω2 φ1 ut−1 ) + β0 ut t for t = 1, . . . , T ,
and hence Var(∗t ) = β02 {σu2 φ20 + (1 + σu2 )σ2 } + β02 φ21

(1+ω22 ) σ2
.
ω2 (1−φ21 )

The proof of the theorem is included in Supplementary Appendix A.3. This theorem
says the attenuation effect resulting from the measurement error on estimation of φ1 . The
constant scaling parameter β0 in the measurement error model (9) does not influence the
estimation of φ1 but affects the estimation of φ0 and σ2 . The attenuation factor ω2 is
determined by the magnitude σu2 of measurement error as well as the values of φ0 , φ1 , and
σ2 of the time series model (1).

3.3

Naive Estimation and Bias for AR(p) Model with p ≥ 2

We now extend the discussion in Section 3.2 to the AR(p) model with p ≥ 2. Replacing Xt
with Xt∗ in (1) gives the working model
Xt∗

=

φ∗0

+

p
X

∗
φ∗j Xt−j
+ ∗t ,

(13)

j=1

where φ∗ = (φ∗1 , . . . , φ∗p )T and ∗t may differ from the corresponding symbol in (1). If mimicking the procedure of using (6) with Xt replaced by Xt∗ to estimate φ∗ , φ∗0 and σ2∗ in (13),
then we let φb∗ = (φb∗1 , . . . , φb∗p )T , φb∗0 and σ
b∗2 denote the resultant estimators. Similar to γ
bk and
P
P
T −k
1
∗
∗
µ
b, we define µ
b∗ = T1 Tt=1 Xt∗ and γ
bk∗ = T −k
b∗ )(Xt+k
−µ
b∗ ) for k = 1, . . . , p. Let
t=1 (Xt − µ
P
γ
b∗ = (b
γ1∗ , . . . , γ
bp∗ )T and γ
b0∗ = T1 Tt=1 (Xt∗ − µ
b∗ )(Xt∗ − µ
b∗ ).
We now discuss the asymptotic results of the naive estimators under different measurement error models.
Theorem 3 Let 1p be the p×1 unit and let Ip be the p×p identity matrix. Define γ ∗ = α12 γ,
γ0∗ = α12 γ0 + σe2 , φ∗ = α12 (α12 Γ + σe2 Ip )−1 γ, φ∗0 = (1 − φ∗ · 1p ) (α0 + α1 µ) and σ2∗ = α12 γ0 +
σe2 − α14 γ T (α12 Γ + σe2 Ip )

−1

γ. Under regularity conditions, if the time series is stationary and

the measurement error process satisfies (7), then
8

p

p

(1) γ
b∗ −−→ γ ∗ and γ
b0∗ −−→ γ0∗

as T → ∞.

p
p
p
b2∗ −−→ σ2∗
(2) φb∗ −−→ φ∗ , φb∗0 −−→ φ∗0 , and σ

as T → ∞.

(3) Let Q1 denote the (p+1)×(p+1) asymptotic covariance matrix of

√  ∗ ∗T T
T (b
γ0 , γ
b ) − (γ0∗ , γ ∗T )T

as T → ∞. Then the elements of Q1 are given by
∗
q100
= α14 q00 + 4α12 γ0 σe2 + E(e4t ) − σe4 ;
∗
q10p
= α14 q0p + 4α12 γp σe2 ;
∗
q1pr
= α14 qpr + 2α12 σe2 (γ|p−r| + γp+r ) for r 6= 0, r 6= p;
∗
q1pp
= α14 qpp + 2α12 σe2 (γ0 + γ2p ) + σe4 ;

for p ≥ 1, where qjk is the (j, k) element of the asymptotic covariance matrix of
(b
γ0 , γ
bT )T , given by (Brockwell et al. 1991, Sec. 7.3)
∞
X

qjk = (η − 3)γj γk +

(γi γi−j+k + γi+k γi−j )

(14)

i=−∞

for (j, k) = (0, 0), (0, p), (p, p) and (p, r) with r 6= 0 and r 6= p, with η = E(4t )/σ4 .
The proof of Theorem 3 is presented in Supplementary Appendix A.4. Similar to the
results in Theorem 1, the intercept α0 only influence φ0 and does not influence φ.
Theorem 4 Let γ ∗ = β02 γ, γ0∗ = β02 {(σu2 + 1)γ0 + σu2 µ2 }, φ∗ = {Γ + σu2 (γ0 + µ2 )Ip }

−1

φ∗0 = β0 (1 − φ∗T · 1p ) µ, and σ2∗ = β02 (σu2 + 1)γ0 + β02 σu2 µ2 − β02 γ T {Γ + σu2 (γ0 + µ2 )Ip }

−1

γ,
γ.

Under regularity conditions, if the time series are stationary and the measurement error
process satisfy (9), then
p

p

(1) γ
b∗ −−→ γ ∗ and γ
b0∗ −−→ γ0∗

as T → ∞.

p
p
p
(2) φb∗ −−→ φ∗ , φb∗0 −−→ φ∗0 , and σ
b2∗ −−→ σ2∗

as T → ∞.

(3) Let Q2 denote the (p+1)×(p+1) asymptotic covariance matrix of

9

√  ∗ ∗T T
T (b
γ0 , γ
b ) − (γ0∗ , γ ∗T )T

as T → ∞. Then the elements of Q2 are given by
∗
q200
= β04 (σu2 + 1)2 q00 + β04 {E(u4t ) − (σu2 + 1)2 }E(Xt − µ)4

+ 4µβ04 σu2 (σu2 + 1)v0 + 4µβ04 {E(u4t ) − E(u3t ) − σu2 (σu2 + 1)}E(Xt − µ)3

+ 2µ2 β04 E(u4t ) − 2E(u3t ) + 1 − σu4 γ0
"
#
∞
X



2 4
4
4
3
2
4
+ 4µ β0 σu
γh + E(ut ) − 2E(ut ) + σu + 1 − σu γ0 + µ4 β04 E{(ut − 1)4 } − σu4 ;
h=−∞




∗
q20p
= β04 qp (σu2 + 1) + β04 E(u3t ) − (σu2 + 1) E{(Xt − µ)3 (Xt+p − µ)} + E{(Xt − µ)3 (Xt−p − µ)}


+ 2µβ04 σu2 v0p + µβ04 E{3u3t − 3u2t − 2σu2 } E{(Xt − µ)2 (Xt−p − µ)} + E{(Xt − µ)2 (Xt+p − µ)}

∗
q2pr

+ 6µ2 β04 E(ut − 1)3 γp + 4µ2 β04 σu2 γp ;

= β04 qpr + β04 σu2 E{(Xt − µ)2 (Xt+p − µ)(Xt+r − µ)} + E{(Xt − µ)(Xt+p − µ)2 (Xt+p+r − µ)}

+E{(Xt−r − µ)(Xt − µ)2 (Xt+p − µ)} + E{(Xt − µ)(Xt+p−r − µ)(Xt+p − µ)2 }
+ µβ04 σu2 [E{(Xt − µ)(Xt+p − µ)(Xt+r − µ)} + E{(Xt − µ)(Xt+p − µ)(Xt+p+r − µ)}
+E{(Xt−r − µ)(Xt − µ)(Xt+p − µ)} + E{(Xt − µ)(Xt+p−r − µ)(Xt+p − µ)}]
+ 2µ2 β04 σu2 (γ|p−r| + γp+r ) for r 6= p, r 6= 0;

∗
q2pp
= β04 qpp + β04 (σu4 + 2σu2 )Var{(Xt − µ)(Xt+p − µ)} + 2β04 E{(Xt − µ)(Xt+p − µ)2 (Xt+2p − µ)}

+ µβ04 σu2 E{(Xt − µ)(Xt+p − µ)2 } + 2E{(Xt − µ)(Xt+p − µ)(Xt+2p − µ)}

+ E{(Xt − µ)2 (Xt+p − µ)} + 2µ2 β04 σu4 γp + 2µ2 β04 σu2 (γ0 + γ2p ) + µ4 β04 σu4 ;

where the qjk are given by (14), for (j, k) = (0, 0), (0, p), (p, p) and (p, r) with r 6= 0
P P
and r 6= p, and vp = limT →∞ T1 Tt=1 Ts=1 E{(Xt − µ)(Xt+p − µ)(Xs − µ)}.
The proof of the theorem is presented in Supplementary Appendix A.5. The multiplicative measurement error ut contributes to the biasedness of the parameter estimation for φ,
while the scaling parameter β0 has no effects on the naive estimator φb∗ .

4

Methodology of Correcting Measurement Error Effects

4.1

Estimation of Model Parameters

In the presence of measurement error, measurements of the Xt are not always available but
surrogate measurements Xt∗ are available. It may be tempting to conduct a naive analysis
10

by implementing (6) with the Xt replaced by the Xt∗ , or equivalently with µ
b and γ
bk replaced
by µ
b∗ and the γ
bk∗ , respectively, to find estimators of φ, φ0 and σ2 . However, by Theorems 3–
4, such a procedure typically yields biased estimators. In this section, we develop new
estimators accounting for the measurement error effects described by either the additive
model (7) or the multiplicative model (9).
Our idea is still to employ (6) to find consistent estimators of φ, φ0 and σ2 , but instead
of replacing µ
b and the γ
bk with µ
b∗ and the γ
bk∗ as in the naive analysis, we replace µ
b and
the γ
bk in (6) with new functions of the Xt∗ , denoted as µ
e and the γ
ek , which adjust for the
measurement error effects. Specifically, if we can find µ
e and the γ
ek such that they resemble
µ
b and the γ
bk in the sense that as T → ∞,
µ
e and µ
b have the same limit in probability,
and

γ
ek and γ
bk have the same limit in probability for k = 0, . . . , p,

(15)

then substituting µ
b and the γ
bk with µ
e and the γ
ek in (6) yields consistent estimators of φ, φ0
and σ2 .
e denote Γ with the γk replaced by
With the availability of the γ
ek satisfying (15), let Γ
the γ
ek . Then provided regularity conditions, consistent estimators of φ, φ0 and σ2 can be
obtained by solving the estimating equations for φ, φ0 , and σ2 :
e−1 γ
φ=Γ
e,
φ0 =

1−

p
X

!
φi

µ
e,

(16)

i=1

e
σ2 = γ
e0 − 2φT γ
e + φT Γφ.
It is immediate to obtain the following result.
Theorem 5 Assume regularity conditions hold and the time series are stationary. If µ
e and
e φe0 , and
the γ
ek are functions of the Xt∗ with t = 1, . . . , T and they satisfy (15), and let φ,
σe 2 denote the estimators for φ, φ0 and σ2 , respectively, obtained by solving (16). Then, as
T →∞
p
p
p
(1) φe −→ φ, φe0 −→ φ0 , and σe 2 −→ σ2 ;

11

(2)

√

d

n(φe − φ) −−→ N (0, GQGT ),

b∗T )T .
where G is the matrix of derivatives of φe with respect to the components of (b
γ0∗ , γ
Here Q = Q1 , the matrix in Theorem 3, if measurement error follows the model (7);
and Q = Q2 , the matrix in Theorem 4, if measurement error follows the model (9).
Now we discuss explicitly how to determine µ
e and the γ
ek under the measurement error
model (7) or (9). With (7), take µ
e=
With (9), take µ
e=

µ
b∗
β0

,γ
e0 =

γ0∗
2 )β 2
(1+σu
0

µ
b∗
α1

− α0 , γ
e0 =

−

2 µ2
σu
2 +1
σu

, and

1
(b
γ0∗ − σe2 ),
α21
γ
b∗
γ
ek = βk2 for
0

and γ
ek =

γ
bk∗
α21

for k = 1, . . . , p.

k = 1, . . . , p. By the results

in Theorem 3(1) and Theorem 4(1), it can be easily verified that these µ
e and the γ
ek satisfy
(15).
We conclude this section with a procedure of estimating the asymptotic covariance mae While Theorem 5 presents the sandwich form of the asymptotic
trix for the estimator φ.
e its evaluation involves lengthy calculations. We may alternatively
covariance matrix of φ,
employ the block bootstrap algorithm (Lahiri 1999) to obtain variance estimates for φe using
the following steps. Firstly, we set a positive integer, say N , as the number for the bootstrap sampling; N can be set as a large number such as 1000. Next, we repeat through the
following five steps:
Step 1: At iteration n ∈ {1, . . . , N }, we initialize a null time series X (n,0) of dimension 0
and specify a block length, say b, which is an integer between 0 and T . Initialize
m=1.
(m−1)

Step 2: Sample an index, say i, from {0, . . . , T − b}, and then define Xadd

=

{Xi+1 , . . . , Xi+b }.
(m−1)

Step 3: Update the previous time series X (n,m−1) by appending Xadd

to it, and let

X (n,m) denote the new time series.
Step 4: If the dimension X (n,m) is smaller than T then return to Steps 2 and 3; otherwise drop the elements in the time series with the index greater than T to
ensure the dimension of X (n,m) is identical to T and then go to Step 5.

12

Step 5: Obtain an estimate φe(n) of parameter φ by applying the times series X (n,m)
to (16). If n < N , then set n to be n + 1 and go back to Step 1 to repeat;
otherwise stop.
¯
Let φe(n) =

1
N

PN

n=1

¯
φe(n) be the sample mean. The bootstrap variance of φe is then given

by,
N
1 X e(n) ¯e(n) 2
e
Varboot (φ) =
(φ − φ ) .
N n=1

4.2

Forecasting and Prediction Error

Forecasting is an important application of the autoregressive models. Specifically, in forecasting based on the observed time series X(T ) = {x1 , . . . , xT }, we are interested in the predictions
of {XT +1 , . . . , XT +H } for a positive integer H, which is done one by one starting from the
nearest time point T + 1 to the farthest time point T + H. To this end, let h = 1, . . . , H,
the h-step forecasting of XT +h is based on its history of lag-p, {XT +h−1 , . . . , XT +h−p }, by
bT +h , where for
using the conditional expectation E(XT +h |xT +h−1 , . . . , xT +h−p ), denoted X
j = T + h − 1, . . . , T + h − p, xj is the observe value of Xj if j ≤ T ; and xj is the prebj , if j > T . This prediction minimizes the squared prediction error
dicted value of Xj , X
bT +h − XT +h )2 (e.g., Box et al. 2015, p.131).
E(X
If no measurement error is involved, due to the zero mean of the random error term t in
the AR(p) model (1), for h = 1, . . . , H, the conditional expectation can be calculated by
bT +h = φ0 + φ1 xT +h−1 + . . . + φp xT +h−p .
X

(17)

When measurement error appears, the observe values xj for j = T, . . . , T − p + 1 in (17)
are no longer available but their surrogates Xj∗ are available. We now provide a sensible
estimate of Xj by using the measurement error model for characterizing the relationship of
Xj and Xj∗ . If measurement error follows (7), we “estimate” Xj by
bj = 1 (Xj∗ − α0 )
X
α1

for j = t, . . . , t − p + 1;

(18)

bj is “estimated” by
if the measurement error follows (9), then X
bj =
X

Xj∗
β0

for j = t, . . . , t − p + 1.
13

(19)

bj ) = Xj for j = t, . . . , t − p + 1.
These “estimates” are unbiased in the sense that E(X
Consequently, for h = 1, . . . , H, XT +h is predicted as
bT +h = φ0 + φ1 X
bT +h−1 + · · · + φp X
bT +h−p .
X

(20)

In contrast to the observed values {xT , . . . , xT −p+1 }, also referred to as the initial values
of the forecasting of XT +1 , . . . , XT +H , the estimates determined by (18) or (19) introduce
additional prediction error which should be characterized. Without the loss of generality, we
consider p = 1 to illustrate the recursive calculation of the prediction error; the prediction
error with higher orders of the autoregressive process can be derived recursively in a similar
way but with more complex expressions.
If the measurement error follows (7), the mean squared prediction error of the 1-step
prediction is given by
2
b
P(1)
e = E(XT +1 − XT +1 )

bT ) − (φ0 + φ1 XT + T +1 )}2
= E{(φ0 + φ1 X

2
 
eT
− φ1 XT − T +1
= E φ1 Xt +
α1
φ2 σ 2
= 1 2 e + σ2 ,
α1
where the last step is due to the independence between et and t+1 , as well as E(e2t ) = σe2
and E(2t ) = σ2 .
Then, the h-step prediction error is given by
2
b
P(h)
e = E(XT +h − XT +h )
n 

o2
bT +h−1 − XT +h−1 − T +1
= E φ1 X

= φ21 P(h−1)
+ σ2
e
h−1

φ2h σ 2 X 2i 2
φ1 σ ,
= 1 2e +
α1
i=0

(21)
(h−1)

where the last step comes from the recursive evaluation of Pe

.

Similarly, if the measurement error follows (9), the mean squared prediction error is given

14

by
2
b
P(1)
e = E(XT +1 − XT +1 )


σ2
2
2
= φ1
+ µ σu2 + σ2 ,
1 − φ21

where we use the independence of t+1 , ut and Xt , E(ut ) = 1, and Var(Xt ) =

σ2
1−φ21

due to

the stationary AR(1) process. Hence,
2
b
P(h)
e = E(XT +h − XT +h )
n 

o2
bT +h−1 − XT +h−1 − T +1
= E φ1 X

+ σ2
= φ21 P(h−1)
e
=

φ2h−2
P(1)
1
e

+

h−2
X

2
φ2i
1 σ

i=0

=

φ2h
1




h−1
X
σ2
2
2
2
+ µ σu +
φ2i
1 σ .
1 − φ21
i=0
(h)

The evaluation of the mean squared prediction error Pe

(22)

is carried out by replacing the

parameters with their estimators. We comment that the common second term in (21) and
P
2i 2
(22), h−1
i=0 φ1 σ , is the mean squared prediction error for the AR(1) model for error-free
settings (e.g. Box et al. 2015, p.152), which equals

1−φ2h
1
σ2.
1−φ21 

For an α with 0 < α < 1, then h-step (1 − α)-prediction interval is constructed as
h
i
bT +h − q α P(h) , X
bT +h + q α P(h) ,
X
e
e
2
2
bT +h − XT +h . In practice, under normal
where q α2 the α-level quantile of the distribution of X
assumption of t and et , one can take q α2 to be the α-level quantile of the standard normal
distribution (Brockwell and Davis 2002, p.108).

5
5.1

Analysis of COVID-19 Death Rates
Study Objective

Using Canadian provincial COVID-19 data containing the daily confirmed cases and deaths
from April 3, 2020 to May 4, 2020, we compare the times series of death rates for British
15

Columbia, Ontario, Quebec, and Alberta, the four provinces in Canada which experience severe situations. The daily confirmed cases and fatalities are taken from “1Point3Acres.com”
(https://coronavirus.1point3acres.com/).
In epidemiology, the mortality rate, defined as the proportion of cumulative deaths of
the disease in the total number of people diagnosed with the disease (Kanchan et al. 2015),
is often used to measure the severeness of an infectious disease. For COVID-19, determining
the mortality rate is not trivial due to the difficulty in precisely determining the number of
infected cases. Due to the limited test capacity, individuals with light symptoms are not
being tested. Asymptomatic infections and the incubation period make it difficult to acquire
an accurate number of infections. To circumvent this, we explore different definitions of
death rates. Definition 1 is from Baud et al. (2020) who estimated mortality rates by
dividing the number of deaths on a given day by the number of patients with confirmed
COVID-19 infection 14 days before, with the consideration of the maximum incubation time
to be 14 days. On the other hand, the median time from symptom onset to intensive care
unit admission is about 10 days ([3] in Baud et al. 2020), so we consider Definition 2 which
is the number of deaths of COVID-19 on day t divided by the number of confirmed cases at
day (t − 10). In comparison, we also consider Definition 3 by calculating the death rate on
day t as the ratio of the number of deaths on day t to the number of confirmed cases on day
t.
While the first two ways may help more reasonably estimate mortality rates than the third
definition, these calculated rates still differ from the true mortality rates because of underreported cases which are primarily due to limited test capacity and undetected asymptomatic
infections. To reflect the discrepancy between the reported and the true mortality rates for
each province, for each definition of the mortality rate, we let X1,t , X2,t , X3,t , and X4,t ,
represent the true mortality rate on day t for British Columbia, Ontario, Quebec and Alberta,
∗
∗
∗
∗
respectively; and let X1,t
, X2,t
, X3,t
and X4,t
denote the reported mortality rate on day t in

British Columbia, Ontario, Quebec and Alberta, respectively. The objective is to use the
reported mortality rates {Xit∗ : t = 1, . . . , 31} to infer the true mortality rates Xi,t which
are modeled by (1) separately for i = 1, . . . , 4. In addition, we want to forecast the true
mortality rate of COVID-19 for a future time period. Due to the undetected asymptomatic
16

∗
cases and untested cases for light symptoms, the reported mortality rates Xi,t
are typically
∗
overestimated (i.e., Xi,t
≥ Xi,t ) for i = 1, . . . , 4. As there is no exact information to guide

us how to characterize the relationship between Xit∗ and Xit , here we conduct sensitivity
∗
studies by considering measurement error model (7) or (9). We use the observed data Xi,t
∗
: t = 1, ..., Ti } with T1 = T2 = 31, to estimate
from April 3, 2020 to May 4, 2020, i.e., {Xi,t

the model parameters in (1) with measurement error effects accounted for, and then forecast
the mortality rate of COVID-19, from May 5, 2020 to May 9, 2020, in British Columbia,
Ontario, Quebec and Alberta, Canada.

5.2

Models Building

Figure 1 displays the trajectory of the mortality rates of COVID-19 in the four provinces
that are obtained from the three definitions. To assess the stationarity of the Xit∗ , we conduct
∗
the augmented DickeyFuller (ADF) tests (Cheung and Lai 1995) to times series {Xi,t
:t=
∗
∗
: t = 1, . . . , T } for i = 1, . . . , 4 in
− Xi,t
1, . . . , T }, or its differencing transformation {Xi,(t+1)

each definition. Supplementary Table 4 presents the test statistics and p-value of the ADF
test for each time series, where “TSV” represents a test statistics value.
[ Place Figure 1 About Here ]
To determine the lag value p for the autoregression model (1) used for the time series
{Xi,t : t = 1, ..., Ti } with T1 = T2 = 31 for i = 1, . . . , 4, we fit the naive model (13) with ∗t
assumed to follow a normal distribution N (0, σ∗2 ), and use the AIC criterion by minimizing
−2

T
X

logf (x∗t |x∗t−1 , . . . , x∗t−p ) + 2p,

(23)

t=p
∗
∗
where f (x∗t |x∗t−1 , . . . , x∗t−p ) is the conditional probability of Xt∗ given Xt−1
, . . . , Xt−p
. The

results are summarized in Supplementary Table 5, where no-differencing or 1-differencing is
applied, the entries with “-” indicate that the corresponding model is not applicable due to
the ADF test results.
We take those lag values for an AR(p) model to feature the true mortality rate Xi,t for each
definition and i = 1, . . . , 4. To be specific, for the British Columbia data, with Definition 1
17

we consider two models: AR(1) model for the time series with 1-order differencing and
AR(2) model for the time series with no-differencing; with Definitions 2 and 3, we consider
AR(2) and AR(1) models, respectively, for the time series with 1-order differencing. For the
Ontario data, we consider AR(1) and AR(4) for the time series with 1-order differencing in
Definitions 1 and 3, respectively, and AR(2) for Definition 2 with no transformation. For
the Quebec data, we consider AR(1) and AR(2) models for the times series with 1-order
differencing in Definitions 1 and 2, respectively. For Alberta data, we consider an AR(1)
model for the times series with 1-order differencing for both Definitions 1 and 2.

5.3

Sensitivity Analyses

As there are no additional data available for estimating the parameters for the model (7)
or (9), we conduct sensitivity analyses using the findings in the literature. Different studies
showed different estimates of the asymptomatic infection rates, changing from 17.9% to
78.3% (Kimball 2020; Day 2020). To accommodate the heterogeneity of different studies,
He et al. (2020) carried out a meta-analysis and obtained an estimate of the asymptomatic
infection rate to be 46%. If under-reported confirmed cases are only caused from undetected
asymptomatic cases, then Xt = (1 − τA )Xt∗ , or equivalently,
Xt∗ =

1
Xt ,
1 − τA

(24)

where τA represents the rate of asymptomatic infections.
Now we use (24) as a starting point to conduct sensitivity analyses. In the multiplicative
model (9), we take β0 ut =

1
.
1−τA

With E(ut ) = 1, we set β0 =

1
1−τA

by setting τA = 46%,

the value from the meta-analysis of He et al. (2020). To see different degrees of error, we
2
2
consider σu2 to take a small value, say σu1
, and a large value, say, σu2
, which is alternatively

reflected by the change of the coefficient of variation, CV =

σu
,
E(ut )

of the error term ut from

σu1 × 100% to σu2 × 100%.
When using the additive model (7) to characterize the measurement error process, motivated by (24), we set α0 = 0 and α1 =

1
,
1−46%

2
and let σe2 take a small value, say σe1
, and

2
a large value, say σe2
, to feature an increasing degree of measurement error. Due to the

18

constraints for the parameters discussed for (8) and (10), we set the values for σu1 , σu2 , σe1 ,
and σe2 case by case for each definition and for each province, which are recorded in Table 6.
The model fitting results are reported in Tables 1–2 and Supplementary Table 7 for the
three definitions of mortality rates, where the point estimates (EST), the associated standard
errors (SE), and the p-values for the model parameters are included. Table 1 shows that with
Definition 1, the estimates of φ0 in the absolute value from the proposed method are smaller
than those of naive method, while the estimates of φ1 produced from the proposed and naive
methods exhibit an opposite direction. As expected, the standard errors for the proposed
method are generally larger than those of the naive method. However, both methods find no
evidence to support that φ0 and φ1 are different from zero for the data of British Columbia
and Ontario, suggesting that the mortality rates of these two provinces remain statistically
unchanged. At the significance level 0.1, the naive method and the proposed method show
different evidence for the data of Quebec and Alberta. The naive method suggests a likely
downward trend with p-value 0.071 and 0.061 for testing of φ0 for Quebec and Alberta,
respectively. The proposed method, on the other hand, show that φ0 is insignificant for
these two provinces.
Table 2 displays the results for Definition 2. For the British Columbia data, the estimates
of the three parameters φ1 , φ2 and φ3 produced from the proposed method are smaller than
those yielded from the naive method, whereas the standard errors output from the proposed
method are larger than those from the naive method. However, at the significance level 0.05,
both methods find no evidence to show the significance of φ0 , φ1 and φ2 , suggesting that
the mortality rate of British Columbia remain unchanged with time. Similar findings are
revealed for the Alberta data except that the parameter estimates output from the proposed
method are larger than those produced from the naive method. For the Ontario and Quebec
data, the revealings from the two methods are quite different. For Ontario, both methods
show that φ0 is insignificant and φ1 is significant. The evidence of φ2 , however, depends on
the nature of measurement error. On the contrary, the findings for Quebec do not tend to
show a definite direction, and they vary with the model form or degree of the measurement
error process.
Table 7 shows the results for Definition 3. For the British Columbia data, the estimates
19

produced by the proposed method are smaller than those yielded from the naive method. The
standard errors output from the proposed methods inflate as the degree of measurement error
increases. The naive and proposed methods reveal different evidence for the significance of φ0
and φ1 , and the degree of measurement error affects the findings too. For the Ontario data,
both methods uncover the same type of evidence for all the parameters at the significance
level 0.05, except for the case with the large error under the multiplicative model.
[ Place Tables 1–2 About Here ]

5.4

Forecasting

With the fitted model for each time series in Section 5.3, we forecast the true mortality rate
for the subsequent five days (May 5 – May 9) using the method described in Section 4.2.
Specifically, since the true mortality rates are not observable, we “estimate” them using (18)
and (19), respectively, for the measurement error models (7) and (9), and then we forecast
the values of Xi,32 , Xi,33 , Xi,34 , Xi,35 , and Xi,36 using (20).
(h)

To quantify the forecasting performance, we calculate Pe

for h = 1, . . . , H for each

specified model of the mortality rates Xi,t , and we report the results, together with the total
PH
(h)
in Tables 8–10, where H is set as 5. For h = 1, . . . , H, we report the observed
h=1 Pe
bT +h )2 , and the expected prediction error defined in (21) and (22).
prediction error (XT +h − X
Forecasting results based on the three definitions of mortality rates are reported in Figures 4–3 for the four provinces, where the prediction results after May 4 are marked in blue
and red for the measurement error models (7) and (9), respectively, together with prediction areas marked in shaded parts, as well as the prediction results obtained from the naive
method by using (20) with naive estimates of φ (marked in dark yellow). In comparison,
we display the reported mortality rate (in black) from Apr 3, 2020 to May 9, 2020 as well
as the adjusted mortality rates obtained from (24) (in green); in addition, we report the
fitted values using (17) in blue points. To compare the forecasting results in the presence
of different degrees of measurement error. We report the results derived from a mild degree of measurement error in top subfigures and place those obtained from a large degree of
measurement error in bottom subfigures.
20

[ Place Figure 2 About Here ]
The results for British Columbia are presented in Figure 2 and Web Figures 4–6. With
Definition 1, the methods with measurement error effects accommodated suggest that the
mortality rate in the past and its forecasting values are around 4%, whereas the results
obtained from the method without accounting for measurement error effects indicate that
the mortality rates over time are higher than 6%. With Definition 2, the methods with
or without accounting for measurement error effects reveal that the mortality rates over
time are, respectively, below 3.5% and above 5%. With Definition 3, the methods with or
without accounting for measurement error effects indicate that the mortality rates over time
are, around 3% and above 4%, respectively.
[ Place Figure 3 About Here ]
The results for Ontario are presented in Figure 3 and Supplementary Figures 7–8. With
Definition 1, the methods with measurement error effects accommodated suggest that the
mortality rate over time is around 7% over time, while the reported mortality rate over time
is about 12.5%. With Definition 2, the methods with and without incorporating the feature
of measurement error indicate the mortality rate in the past and its forecasting values are,
respectively, below 6% and around 10%. With Definition 3, the mortality rate increases over
time substantially. The methods with measurement error effects accommodated suggest that
the mortality rate increases from 2% to above 4% whereas the reported mortality rate shows
that rate increases from below 4% to above 8%.
The results for Quebec are presented in Supplementary Figures 9–10. With Definition 1
the methods with measurement error effects accommodated show that the mortality rate is
around 6.5% over time, whereas the method without considering measurement error indicates
the mortality rate is over 10%. With Definition 2, the methods with or without addressing
the measurement error effects show that the mortality rates over time are, respectively, below
6% and above 7.5%.
The results for Alberta are presented in Supplementary Figures 11–12. With Definition 1
the methods with and without measurement error accommodated suggest that the mortality
21

rates are, respectively, around 2% and 4% over time. With Definition 2, the methods with
or without addressing the measurement error effects show that the historical mortality rate
and its predictions are, respectively, below 2% and above 2%.

5.5

Model Assessment

The specification of lag p for model (1) of the true mortality rates {Xi,t : t = 1, . . . , T } is
∗
: t = 1, . . . , T }, but
based on (23) which is derived from the reported mortality rates {Xi,t

not from {Xi,t : t = 1, . . . , T } itself. This discrepancy introduces the possibility of model
misspecification when featuring the series Xi,t using (1). To investigate this, we conduct a
sensitivity analysis by considering the AR(p) with a different value of p for the Xi,t from
Definition 1. As Table 5 indicates the feasibility of using AR(1) for all four provinces, here
we further employ the AR(2) model to do forecasting for the period from May 5 to May 9.
In Table 3, we report the observed and expected prediction errors of the forecasting
using AR(2) models in comparison with AR(1) models. Comparing different lag orders of
the autoregressive models, we find that in terms of the observed prediction error, the selected
AR(1) models have better performance than the AR(2) models for the data of Ontario and
Alberta, and the results for British Columbia and Quebec are fairly similar. It is noticed
that both the observed prediction error and the expected prediction error associated with
the proposed method tend to become small when the degree of measurement error increases
for British Columbia, Ontario, and Quebec.
[ Place Table 3 About Here ]

6

Discussion

In this article, we investigate the impact of measurement error on time series analysis under
autoregressive models and establish analytic results under the additive and multiplicative
measurement error models. We propose an estimating equation method to correct for the
biases induced from the naive analysis which disregards the differences between the true
measurements and their surrogate measurements. We rigorously establish the theoretical re22

sults for the proposed method. As a genuine application, we apply to the proposed method
to analyze the mortality rates of COVID-19 data in four provinces, British Columbia, Ontario, Quebec, and Alberta, which have the most severe virus outbreaks in Canada. The real
data analysis clearly demonstrates that incorporating measurement error in the analysis can
uncover various different results.
Our method has the flexibility or robustness in that distribution assumptions are required
to describe the measurement error process as well as the time series autoregressive process.
While our research is motivated by the faulty nature of COVID-19 data, the proposed method
can be applied to handle other problems related to error-contaminated time series. Our
development here is directed to using autoregressive models to delineate time series data.
The same principles can be applied to other model forms such as moving average models or
autoregressive moving average models which may be used to handle error-prone time series
data, where technical details can be more notationally involved.
When checking the stationarity of time series, we apply the ADF test to the observed
time series Xt∗ , which is mainly driven by the unavailability of the true values of Xt , as well as
the fact that the weakly stationarity of observed time series implies the weakly stationarity
of the true time series if measurement error is featured with (7) or (9). It is interesting to
rigorously develop a formal test similar to the ADF test to handle time series subject to
measurement error.

Acknowledgements
This research is partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) as well as the Rapid Response Program COVID-19 of the Canadian
Statistical Sciences Institute (CANSSI). Yi is Canada Research Chair in Data Science (Tier
1). Her research was undertaken, in part, thanks to funding from the Canada Research
Chairs Program.

23

References
Baud, D., Qi, X., Nielsen-Saines, K., Musso, D., Pomar, L., and Favre, G. (2020). Real
estimates of mortality following COVID-19 infection. The Lancet Infectious Diseases.
Box, G. E., Jenkins, G. M., Reinsel, G. C., and Ljung, G. M. (2015). Time Series Analysis:
Forecasting and Control. New Jersey, NJ: John Wiley & Sons.
Brockwell, P. J. and Davis, R. A. (2002). Introduction to Time Series and Forecasting. New
York, NY: Springer-Verlag.
Brockwell, P. J., Davis, R. A., and Fienberg, S. E. (1991). Time Series: Theory and Methods.
New York, NY: Springer Science & Business Media.
Cheung, Y.-W. and Lai, K. S. (1995). Lag order and critical values of the augmented dickeyfuller test. Journal of Business & Economic Statistics, 13(3):277–280.
Day, M. (2020). COVID-19: four fifths of cases are asymptomatic, China figures indicate.
The BMJ, 369.
Dedecker, J., Samson, A., and Taupin, M.-L. (2014). Estimation in autoregressive model
with measurement error. ESAIM: Probability and Statistics, 18:277–307.
He, W., Yi, G. Y., and Zhu, Y. (2020). Estimation of the basic reproduction number, average
incubation time, asymptomatic infection rate, and case fatality rate for COVID-19: Metaanalysis and sensitivity analysis. Journal of Medical Virology.
Kanchan, T., Kumar, N., and Unnikrishnan, B. (2015). Mortality: Statistics. In PayneJames, J. and Byard, R. W., editors, Encyclopedia of Forensic and Legal Medicine: Second
Edition, pages 572–577. Oxford, OX:Elsevier.
Kimball, A. (2020). Asymptomatic and presymptomatic SARS-CoV-2 infections in residents of a long-term care skilled nursing facilityKing County, Washington, March 2020.
Morbidity and Mortality Weekly Report, 69:377–381.

24

Lahiri, S. N. (1999). Theoretical comparisons of block bootstrap methods. The Annals of
Statistics, 27(1):386–404.
Staudenmayer, J. and Buonaccorsi, J. P. (2005). Measurement error in linear autoregressive
models. Journal of the American Statistical Association, 100(471):841–852.
Tanaka, K. (2002). A unified approach to the measurement error problem in time series
models. Econometric Theory, 18(2):278–296.
Tripodis, Y. and Buonaccorsi, J. P. (2009). Prediction and forecasting in linear models with
measurement error. Journal of statistical planning and inference, 139(12):4039–4050.

25

26
φ1
φ0
φ1

Large
2 )
(σu2

The Proposed Method

with Multiplicative Error

φ1

2 )
(σe2

φ0

φ0

Large

with Additive Error

2 )
(σu1

φ1

Small

φ0

2 )
(σe1

φ1

0.192

-0.025

0.151

-0.027

0.146

-0.027

0.146

-0.027

0.138

-0.050

φ0

Small

-

Naive

EST

Parameter

The Proposed Method

Error Degree

Method

0.300

0.024

0.236

0.024

0.468

0.025

0.532

0.025

0.214

0.043

SE

0.535

0.308

0.535

0.286

0.760

0.298

0.788

0.313

0.533

0.272

p-value

British Columbia

used to fit the data of British Columbia, Ontario, Quebec and Alberta

0.476

-0.078

0.275

-0.107

0.345

-0.097

0.237

-0.113

0.215

-0.215

EST

3.955

1.690

0.238

0.152

0.939

0.263

0.280

0.134

0.157

0.243

SE

Ontario

0.905

0.964

0.260

0.488

0.717

0.715

0.406

0.406

0.183

0.384

p-value

0.031

-0.180

0.016

-0.183

0.027

-0.181

0.014

-0.183

0.012

-0.340

EST

1.327

0.127

0.166

0.099

0.323

0.100

1.566

0.111

0.124

0.180

SE

Quebec

0.981

0.170

0.923

0.078

0.934

0.083

0.993

0.112

0.923

0.071

p-value

0.087

-0.016

0.060

-0.017

0.183

-0.014

0.056

-0.017

0.052

-0.031

EST

0.360

0.015

0.180

0.009

1.596

0.073

0.185

0.009

0.144

0.016

SE

Alberta

0.812

0.299

0.740

0.080

0.909

0.845

0.764

0.088

0.721

0.061

p-value

Table 1: Definition 1: The parameter estimation under different measurement error models: the AR(1) model with “order-1 differencing” is

27

0.034

φ0

-0.393

φ2

0.039
-0.584

φ1

φ0
2 )
Large (σu2

with Multiplicative Error

-0.273

-0.439

0.034

φ0
φ1

-0.320

φ2

Small

-0.497

φ1

0.036

-0.268

φ2

The Proposed Method

2 )
(σu1

Large

φ0

with Additive Error
2 )
(σe2

φ2

-0.432

-0.254

φ1

-0.415

φ2

0.062

φ0
φ1

EST

Parameter

The Proposed Method

Small

-

Naive

2 )
(σe1

Error Degree

Method

0.322

0.339

0.032

0.204

0.205

0.020

0.354

0.265

0.024

0.205

0.201

0.020

0.185

0.186

0.034

SE

0.245

0.111

0.236

0.205

0.053

0.115

0.384

0.085

0.164

0.215

0.053

0.114

0.195

0.046

0.097

p-value

British Columbia

differencing” is used to fit the data of British Columbia and Quebec.

-0.451

1.255

1.112

-0.389

1.188

1.139

-0.390

1.189

1.138

-0.375

1.173

1.146

-0.370

1.167

2.126

EST

0.510

0.503

0.747

0.162

0.231

0.748

0.172

0.239

0.747

0.141

0.216

0.759

0.140

0.209

1.388

SE

Ontario

0.384

0.020

0.149

0.024

<0.001

0.141

0.032

<0.001

0.140

0.014

<0.001

0.144

0.014

<0.001

0.138

p-value

-0.353

-0.143

0.127

0.128

-0.394

-0.164

0.132

0.162

-0.327

-0.130

0.124

0.174

-0.309

-0.122

0.225

EST

0.111

0.194

0.036

0.042

0.199

0.229

0.041

0.044

0.096

0.165

0.032

0.042

0.092

0.136

0.058

SE

Quebec

0.004

0.467

0.002

0.006

0.059

0.480

0.004

0.001

0.002

0.435

0.001

0.000

0.003

0.380

0.001

p-value

-

-0.205

-0.008

-

-0.144

-0.007

-

-0.158

-0.007

-

-0.131

-0.007

-

-0.124

-0.013

EST

-

0.317

0.012

-

0.205

0.012

-

0.247

0.012

-

0.185

0.012

-

0.172

0.022

SE

Alberta

-

0.524

0.546

-

0.487

0.564

-

0.529

0.554

-

0.486

0.567

-

0.477

0.561

p-value

to fit the data of Ontario, the AR(1) model with “order-1 differencing” is used to fit the data of Alberta, and the AR(2) model with “order-1

Table 2: Definition 2: The parameter estimation under different measurement error models: the AR(2) model with “no differencing” is used

28

a

AR(2)

AR(2)

AR(2)

0.005
0.006

Mild

Moderate

0.006

0.005

Mild

Moderate

0.003

0.005

Moderate

-

0.004

0.006

Mild

AR(1)a

0.004

Moderate

0.002

-

Mild

0.038

Moderate
Alberta

0.051

Mild

0.032

0.052

Mild

Moderate

0.129

0.060

Moderate

-

0.061

0.060

Mild

AR(1)a

0.061

Moderate

0.163

-

Mild

0.085

Moderate
Quebec

0.034

Mild

0.045

0.029
AR(2)

Mild

Moderate

0.073

0.004

Moderate

-

0.000

0.000

Mild

AR(1)a

0.001

Moderate

0.020

-

Mild

0.010

Moderate
Ontario

0.010

Mild

0.010

0.010

Mild

Moderate

0.016

0.010

-

Moderate

The selected model

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

0.010
0.010

AR(1)a

Mild

Moderate

0.010

Additive

0.015

-

Mild

Naive

Day 1

British Columbia

Model

2
2
σe
(or σu
)

Method

0.019

0.016

0.018

0.016

0.010

0.013

0.012

0.017

0.012

0.007

0.141

0.190

0.109

0.195

0.524

0.215

0.216

0.215

0.216

0.607

0.001

0.012

0.008

0.014

0.107

0.010

0.002

0.000

0.004

0.087

0.005

0.005

0.005

0.005

0.014

0.005

0.005

0.005

0.005

0.015

Day 2

0.059

0.052

0.056

0.051

0.033

0.045

0.044

0.052

0.044

0.027

0.333

0.438

0.247

0.446

1.226

0.477

0.479

0.478

0.479

1.357

0.071

0.027

0.031

0.026

0.240

0.014

0.003

0.000

0.007

0.196

0.010

0.010

0.010

0.010

0.031

0.011

0.011

0.011

0.011

0.032

Day 3

0.109

0.099

0.104

0.097

0.064

0.089

0.087

0.098

0.087

0.055

0.560

0.723

0.396

0.734

2.115

0.776

0.778

0.776

0.778

2.289

0.024

0.076

0.063

0.083

0.550

0.000

0.044

0.023

0.056

0.521

0.010

0.010

0.010

0.010

0.042

0.011

0.011

0.011

0.011

0.043

Day 4

0.141

0.129

0.136

0.127

0.081

0.118

0.115

0.129

0.115

0.070

0.774

0.988

0.519

1.002

3.085

1.050

1.053

1.051

1.053

3.294

0.310

0.222

0.221

0.227

1.111

0.035

0.152

0.110

0.175

1.059

0.000

0.000

0.000

0.000

0.019

0.000

0.000

0.000

0.000

0.020

Day 5

Observed Prediction Error

0.334

0.301

0.320

0.296

0.191

0.270

0.263

0.302

0.262

0.160

1.847

2.390

1.303

2.429

7.079

2.578

2.586

2.580

2.587

7.709

0.491

0.370

0.368

0.379

2.081

0.063

0.201

0.134

0.243

1.884

0.034

0.035

0.035

0.035

0.122

0.037

0.037

0.037

0.037

0.126

h=1 OPE(h)

PH

0.022

0.030

0.081

0.112

0.122

0.022

0.031

0.035

0.115

0.125

0.332

0.345

0.413

1.375

1.746

0.205

0.399

0.811

1.561

1.811

0.454

0.571

1.470

2.256

2.517

0.270

0.558

1.453

2.264

2.527

0.034

0.043

0.151

0.151

0.161

0.034

0.044

0.154

0.154

0.164

Day 1

0.022

0.031

0.081

0.112

0.122

0.022

0.031

0.035

0.115

0.125

0.332

0.345

0.413

1.375

1.746

0.205

0.399

0.811

1.561

1.811

0.469

0.606

1.658

2.398

2.648

0.331

0.599

1.626

2.391

2.643

0.035

0.044

0.155

0.155

0.165

0.035

0.044

0.157

0.157

0.167

Day 2

0.022

0.031

0.085

0.115

0.125

0.022

0.031

0.035

0.115

0.125

0.234

0.356

0.407

1.447

1.809

0.205

0.399

0.811

1.561

1.811

0.415

0.603

1.646

2.398

2.648

0.345

0.603

1.646

2.399

2.649

0.035

0.044

0.157

0.157

0.167

0.035

0.044

0.157

0.157

0.167

Day 3

0.022

0.031

0.085

0.115

0.125

0.022

0.031

0.035

0.115

0.125

0.234

0.356

0.407

1.447

1.809

0.205

0.399

0.811

1.561

1.811

0.390

0.603

1.649

2.399

2.649

0.348

0.603

1.649

2.399

2.649

0.035

0.044

0.157

0.157

0.167

0.035

0.044

0.157

0.157

0.167

Day 4

0.022

0.031

0.085

0.115

0.125

0.022

0.031

0.035

0.115

0.125

0.187

0.357

0.402

1.451

1.811

0.205

0.399

0.811

1.561

1.811

0.375

0.603

1.649

2.399

2.649

0.348

0.603

1.649

2.399

2.649

0.035

0.044

0.157

0.157

0.167

0.035

0.044

0.157

0.157

0.167

Day 5

Expected Prediction Error
EPE(h)

0.109

0.155

0.419

0.570

0.621

0.109

0.157

0.177

0.577

0.627

1.319

1.760

2.043

7.096

8.921

1.025

1.995

4.057

7.807

9.057

2.103

2.986

8.072

11.851

13.111

1.642

2.965

8.023

11.853

13.117

0.173

0.220

0.778

0.777

0.828

0.174

0.222

0.784

0.783

0.834

h=1

PH

Table 3: The observed prediction error and expected prediction error for different lag order of autoregressive models

British Columbia

Ontario

Quebec

Alberta

Fatality Rate (%)

30

20

Definition of
Death Rate
Definition 1
Definition 2
Definition 3
10

0
Apr 06

Apr 13

Apr 20

Apr 27

May 04

Apr 06

Apr 13

Apr 20

Apr 27

May 04

Apr 06

Apr 13

Apr 20

Apr 27

May 04

Apr 06

Apr 13

Apr 20

Apr 27

May 04

Date

Figure 1: The time series plots of the death rate with different definitions

29

30

2

3

4

5

2

3

4

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Figure 2: British Columbia by Definition 3 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate (May

Fatality Rate (%)

5

Additive

Mild

31

2

4

6

8

2

4

6

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

(in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported mortality rates

Figure 3: Ontario by Definition 3 (AR(4), order-1 differencing): A 5-day forecasting of the true mortality rate (May 5 - May 9)

Fatality Rate (%)

8

Additive

Mild

Supplementary Materials for “Sensitivity
Analysis of Error-Contaminated Time Series
Data under Autoregressive Models with
Application of COVID-19 Data”
A

Appendix

A.1

Regularity Conditions

(R1) The time series {Xt : t = 1 . . . , T } is stationary.
(R2) The observed error-prone time series {Xt∗ : t = 1 . . . , T } is stationary.
(R3) For any t ∈ {1, . . . , T },
(R4) For any p,

1
T

PT PT
t=1

1
T

s=1

PT

s=1

γ|s−t| → 0 as T → ∞.

E{(Xt − µ)(Xt+p − µ)(Xs − µ)} < ∞.

While the two process {Xt : t = 1, . . . , T } and {Xt∗ : t = 1, . . . , T } are constrained
by the measurement error model (7) or (9), they can both be assumed to be stationary
without inducing conflicting requirements on the associated processes. Obviously, the weak
stationarity of {Xt : t = 1, . . . , T } implies the weak stationarity of {Xt∗ : t = 1, . . . , T } if
they are linked by (7) or (9). Condition (R3) says that as the time series goes long enough,
the average of the covariances between any paired variables is is negligible. Condition (R4)
requires the summation of the third moment of Xt is O(T ), which is needed in Theorem 4
when φ0 6= 0; this condition can be satisfied if E(3t ) = 0, for example.

A.2

The proof of Theorem 1

Applying the weak law of large numbers to φb∗1 given by (12), we obtain that the estimator
φb∗1 converges in probability to

∗ )
Cov(Xt∗ ,Xt−1
,
∗
Var(Xt−1 )

which is denoted as φ∗1 . Now we further examine

φ∗1 by using the AR(1) model (1) and the measurement error model (7):
∗
Cov(Xt∗ , Xt−1
)
∗
Var(Xt−1 )
Cov(α0 + α1 Xt + et , α0 + α1 Xt−1 + et−1 )
=
Var(α0 + α1 Xt + et )
2
α Cov(Xt , Xt−1 )
= 2 1
α1 Var(Xt ) + Var(et )
α2 Cov(φ0 + φ1 Xt−1 + t , Xt−1 )
= 1
α12 Var(Xt ) + Var(et )
α12 Var(Xt−1 )
= φ1 · 2
,
α1 Var(Xt ) + V ar(et )

φ∗1 =

where the second step is due to (7), the third step is because of the independence among the
Xt and the et , and the fourth step is because of (1). Since the time series {Xt } is stationary,
it follows that Var(Xt ) = Var(Xt−1 ) =
φ∗1

σ2
,
1−φ21

and hence

α12 σ2
= φ1 ω1 .
= φ1 · 2 2
α1 σ + σe2 (1 − φ21 )

(S.1)

Next, applying the Slutsky’s theorem to (12), we have that as T → ∞,
p
φb∗0 →
− E(Xt∗ ) − φ∗1 E(Xt∗ ),


α 1 φ0
α 1 φ0
where the limit equals α0 + 1−φ1 (1 − φ1 ω1 ) by (S.1) and the fact that E(Xt∗ ) = α0 + 1−φ
.
1

Finally, plugging the AR(1) model (1) into the measurement error model (11), we obtain
that
Xt∗ = α0 + α1 (φ0 + φ1 Xt−1 + t ) + et .

(S.2)

On the other hand, plugging the measurement error model (7) into the working model (11),
we obtain that
Xt∗ = φ∗0 + φ∗1 (α0 + α1 Xt−1 + et ) + ∗t .
Then equating (S.2) and (S.3) that
∗ = α0 (1 − φ∗1 ) + α1 φ0 − φ∗0 + α1 (φ1 − φ∗1 )Xt−1 + (1 − φ∗1 )et + α1 t .
Consequently, by the independence assumption for Xt−1 , et and t , we obtain that
V ar(∗t ) = φ21 α12 (1 − ω1 )2 Var(Xt−1 ) + (1 − ω1 φ1 )2 Var(et ) + α12 Var(t )


σ2
2 2
2
= φ1 α1 (1 − ω1 )
+ (1 − ω1 φ1 )2 σe2 + α12 σ2 .
1 − φ21
2

(S.3)

A.3

The proof of Theorem 2

p
As noted in the beginning of A.2, as T → ∞, φb∗1 −→ φ∗1 where
∗
Cov(Xt∗ , Xt−1
)
∗
b
φ1 =
.
∗
Var(Xt−1 )

Now we further examine φ∗1 by using the AR(1) model (1) and the measurement error
model (9):
∗
)
Cov(Xt∗ , Xt−1
∗
Var(Xt−1 )
Cov(β0 ut Xt , β0 ut−1 Xt−1 )
=
Var(β0 ut−1 Xt−1 )
2
β Cov(ut Xt , ut−1 Xt−1 )
= 0 2
β0 Var(ut−1 Xt−1 )
Cov{ut (φ0 + φ1 Xt−1 + t ), ut−1 Xt−1 }
=
Var(Xt−1 ut−1 )
Cov(ut Xt−1 , ut−1 Xt−1 )
= φ1
Var(ut−1 Xt−1 )
2
E(ut ut−1 Xt−1
) − E(ut Xt−1 )E(ut−1 Xt−1 )
= φ1
2
2
E(ut−1 Xt−1
) − E 2 (ut−1 Xt−1 )
2
) − E(ut )E(ut−1 )E 2 (Xt−1 )
E(ut )E(ut−1 )E(Xt−1
= φ1
2
E(u2t−1 )E(Xt−1
) − E 2 (ut−1 Xt−1 )
E(ut )E(ut−1 )Var(Xt−1 )
= φ1
2
{Var(ut−1 ) + E (ut−1 )}{Var(Xt−1 ) + E 2 (Xt−1 )} − E 2 (ut−1 )E 2 (Xt−1 )
Var(Xt−1 )
= φ1
{Var(ut−1 ) + 1}{Var(Xt−1 ) + E 2 (Xt−1 )} − E 2 (Xt−1 )
Var(Xt−1 )
= φ1
,
Var(ut−1 )Var(Xt−1 ) + Var(ut−1 )E 2 (Xt−1 ) + Var(Xt−1 )

φ∗1 =

(S.4)

where the second step is due to measurement error model (9), the seventh step is because
ut , ut−1 and Xt−1 are mutually independent, and the second last step is due to E(ut ) = 1.
Since the time series {Xt } is stationary, it follows that E(Xt ) = E(Xt−1 ) =

3

φ0
1−φ1

and

Var(Xt ) = Var(Xt−1 ) =
φ∗1 = φ1
= φ1

σ2
.
1−φ21

Hence (S.4) becomes

Var(Xt−1 )
Var(ut−1 )Var(Xt−1 ) + Var(ut−1 )E 2 (Xt−1 ) + Var(Xt−1 )
σ2
1−φ21
2

σ
2
σu2 1−φ
2 + σu
1

= φ1



φ0
1−φ1

2

+

σ2
1−φ21

σ2
= φ1 ω2 .
1
σ2 σu2 + σ2 + σu2 φ20 1+φ
1−φ1

(S.5)

Next, applying the Slustky’s Theorem to (12) gives that as T → ∞,


β
φ
p
0
0
∗
(1 − φ1 ω2 )
φb0 −−→
1 − φ1
by (S.5) as well as E(Xt∗ ) =

β0 φ0
.
1−φ1

Finally plugging the AR(1) model (1) into the measurement error model (9), we obtain
that
Xt∗ = β0 (φ0 + φ1 Xt−1 + t )ut .

(S.6)

On the other hand, plugging the measurement error model (9) into the working model (11),
we obtain that
Xt∗ = φ∗0 + φ∗1 (β0 Xt−1 ut−1 ) + ∗t .
Then equating (S.6) and (S.7) gives that
∗ = β0 φ0 ut − φ∗0 + β0 Xt−1 (φ1 ut − ω2 φ1 ut−1 ) + β0 ut t .

4

(S.7)

yielding that
V ar(∗t ) = φ20 β02 Var(ut ) + β02 φ21 Var(Xt−1 ut ) + β02 ω22 φ21 Var(Xt−1 ut−1 ) + β02 V ar(ut t )
2
= φ20 β02 σu2 + (β02 φ21 + β02 ω22 φ21 ){E(Xt−1
u2t−1 ) − E 2 (Xt )E 2 (ut−1 )} + β02 {E(u2t )E(2t ) − E 2 (ut )E 2 (t )}
2
= φ20 β02 σu2 + (β02 φ21 + β02 ω22 φ21 ){E(Xt−1
)E(u2t−1 ) − E 2 (Xt )E 2 (ut−1 )} + β02 (σu2 + 1)σ2

= β02 {σu2 φ20 + (1 + σu2 )σ2 }


+ β02 φ21 (1 + ω22 ) {Var(ut−1 ) + E 2 (ut−1 )}{Var(Xt−1 ) + E 2 (Xt−1 )} − E 2 (Xt−1 )


= β02 {σu2 φ20 + (1 + σu2 )σ2 } + β02 φ21 (1 + ω22 ) {Var(ut−1 ) + 1}{Var(Xt−1 ) + E 2 (Xt−1 )} − E 2 (Xt−1 )

= β02 {σu2 φ20 + (1 + σu2 )σ2 } + β02 φ21 (1 + ω22 ) Var(ut−1 )Var(Xt−1 ) + Var(ut−1 )E 2 (Xt−1 ) + Var(Xt−1 )
V ar(Xt−1 )
ω2
2
2
1 + ω2 σ
,
= β02 {σu2 φ20 + (1 + σu2 )σ2 } + β02 φ21
ω2 1 − φ21
= β02 {σu2 φ20 + (1 + σu2 )σ2 } + β02 φ21 (1 + ω22 )

where the second step is because of the independence assumption as well as E(u2t−1 ) = E(u2t )
and E(ut−1 ) = E(ut ) such that Var(Xt−1 ut ) = Var(Xt−1 ut−1 ), and the second last step is
due to ω2 =

A.4

Var(Xt−1 )
Var(ut−1 )Var(Xt−1 )+Var(ut−1 )E 2 (Xt−1 )+Var(Xt−1 )

in (S.5).

The proof of Theorem 3

Proof of Theorem 3(1):

For k = 1, . . . , p, applying the weak law of large numbers to γ
bk∗ , we obtain that as T → ∞,
∗
the estimator γ
bk∗ converges in probability to Cov(Xt∗ , Xt−k
), denoted γk∗ .

Next, we examine γk . By the form of measurement error model (7), we have that for
0 < k < t,
∗
Cov(Xt∗ , Xt−k
) = Cov(α0 + α1 Xt + et , α0 + α1 Xt−k + et−k )

= α12 Cov(Xt , Xt−k ) = α12 γk ,
and by (8), Var(Xt∗ ) = α12 γ0 + σe2 , which is denoted as γ0∗ .
Thus, Theorem 3(1) follows.

5

Proof of Theorem 3(2):
First, by Theorem 3(1), we write
γ
b∗ = α12 γ + op (1)

(S.8)

and
b∗ = α12 Γ + σe2 Ip + op (1),
Γ

∗
γ
b0∗ · · · γ
bp−1


..
.. 
..
b∗ = 
bk in
where Γ
.
 .
. . Then the naive estimator φb∗ is obtained by replacing γ


∗
··· γ
b0∗
γ
bp−1
(6) with γ
bk∗ ,


−1


φb∗ = α12 Γ + σe2 Ip + op (1)

−1

and hence φ∗ = α12 (α12 Γ + σe2 Ip )



α12 γ + op (1) = α12 α12 Γ + σe2 Ip

−1

γ + op (1),

(S.9)

p
γ such that φb∗ −→ φ∗ as T → ∞.

Again, replacing γ
bk in (6) with γ
bk∗ gives the naive estimator φb∗0
!
!
p
T
T
X
X
X
1
1
∗
φb∗0 =
φb∗k
X∗ −
Xt−k
T − p t=p t
T
−
p
t=p
k=1
= E(Xt∗ ) − E(Xt∗ )

p
X

φb∗k + op (1)

k=1

= α0 + α1 E(Xt ) − {α0 + α1 E(Xt )}

p
X

{φ∗k + op (1)} + op (1)

k=1

= (1 − φ∗T · 1p ) (α0 + α1 µ) + op (1),
where φbk and φk are respectively the kth element of φb and φ, the third step is because
φbk = φk + op (1) by (S.9) as well as the model form (7), and the last step is due to the
stationarity of the time series {Xt } such that E(Xt ) = µ.
b∗ φb∗ by
b∗ + φb∗T Γ
Finally, noting that the native estimator σ
b2∗ is given by σ
b2∗ = γ
b0∗ − 2φb∗T γ
applying a version similar to (6), we obtain that
b∗ φb∗
σ
b2∗ = γ
b0∗ − 2φb∗T γ
b∗ + φb∗T Γ
= (α12 γ02 + σe2 ) − 2α14 γ T (α12 Γ + σe2 Ip )−1 γ + α14 γ T (α12 Γ + σe2 Ip )−1 (α12 Γ + σe2 Ip )(α12 Γ + σe2 Ip )−1 γ + op (1)
= α12 γ0 + σe2 − α14 γ T (α12 Γ + σe2 Ip )−1 γ + op (1),
6

where the second step is due to (8), (S.8) and (S.9).
Proof of Theorem 3(3):
Step 1: We show certain identities before proving Theorem 3(3).
1. By model (7), we have that
T

Xt∗

1X
−µ
b = α0 + α1 Xt + et −
(α0 + α1 Xt + et )
T t=1
!
!
T
T
1X
1X
= α 1 Xt −
Xt + et −
et
T t=1
T t=1
∗

= α1 (Xt − µ
b) + (et − ē),
where the first step is because µ
b∗ =

1
T

PT

t=1

Xt∗ and in the last step ē =

(S.10)
1
T

PT

t=1 et .

2. For any t and s, we have that

Cov (Xt − µ
b)2 , (Xs − µ
b)(es − ē)
=E{(Xt − µ
b)2 (Xs − µ
b)(es − ē)} − {E(Xt − µ
b)2 }E{(Xs − µ
b)(es − ē)}
=E{(Xt − µ
b)2 (Xs − µ
b)}E(es − ē) − {E(Xt − µ
b)2 }E(Xs − µ
b)E(es − ē)
=0,

(S.11)

where the second step is due to the independence of et and Xt , and the last step is by
E(es − ē) = 0.
3. By the independence of et and es for t 6= s, we have that
Cov {(Xt − µ
b)(et − ē), (Xs − µ
b)(es − ē)}
=E{(Xt − µ
b)(et − ē)(Xs − µ
b)(es − ē)} − E{(Xt − µ
b)(et − ē)}E{(Xs − µ
b)(es − ē)}
=E{(Xt − µ
b)(Xs − µ
b)}E{(et − ē)}E{(es − ē)} − E{(Xt − µ
b)}E{(et − ē)}E{(Xs − µ
b)}E{(es − ē)}
=0,

(S.12)

where the second step is due to the independence of the et and the Xt , and the last step is
by E(es − ē) = 0.

7

4. For any t, we have that
Var {(Xt − µ
b)(et − ē)}
=E{(Xt − µ
b)2 (et − ē)2 } − E 2 {(Xt − µ
b)(et − ē)}
=E{(Xt − µ
b)2 }E{(et − ē)2 } − E 2 {(Xt − µ
b)}E 2 {(et − ē)}
=E{(Xt − µ
b)2 }E{(et − ē)2 }.

(S.13)

5. For any t, we have
lim E{(Xt − µ
b)2 }

T →∞

= lim E{(Xt − µ)2 + (µ − µ
b)2 + 2(Xt − µ)(µ − µ
b)}
T →∞

=γ0 + lim E{(b
µ − µ)2 } + 2 lim E{(Xt − µ)(µ − µ
b)}
T →∞
T →∞
#
"
T
X
1
(Xs − µ)}
=γ0 + lim E{(b
µ − µ)2 } − 2 lim E (Xt − µ){
T →∞
T →∞
T s=1
T
1X
=γ0 + lim V ar(b
µ) − 2 lim
E{(Xt − µ)(Xs − µ)}
T →∞
T →∞ T
s=1
T
1X
=γ0 + 0 − 2 lim
γ|s−t|
T →∞ T
s=1

=γ0 ,
where the third step is due to µ
b−µ =

(S.14)
1
T

P

s=1 (Xs

− µ), and the fourth step is because

E(b
µ −µ) = 0 by stationarity of the time series, the second last step is due to lim V ar(b
µ) = 0
T →∞

(Brockwell et al. 1991, Theorem 7.1.1.), and the last step due to Condition (R3).
6. Similar to (S.14), we have that
lim E{(Xt − µ
b)(Xt−p − µ
b)}

T →∞

= lim E{(Xt − µ + µ − µ
b)(Xt−p − µ + µ − µ
b)}
T →∞

= lim [E{(Xt − µ)(Xt−p − µ)} + E{(µ − µ
b)(Xt−p − µ)} + E{(µ − µ
b)(Xt − µ)} + E{(µ − µ
b)(µ − µ
b)}]
T →∞

T
1X
(γ|t−s| + γ|t−s−p| ) + lim Var(b
µ)
=γp + lim
T →∞ T
T →∞
s=1

=γp ,

(S.15)
8

where the last step is due to Condition (R3) and lim V ar(b
µ) = 0 (Brockwell et al. 1991,
T →∞

Theorem 7.1.1).
7. For any t, we have
E{(et − ē)2 }
=E{e2t − 2et ē + ē2 }
)
(
T
T X
T
X
X
1
2
E(et es ) + 2
E(et es )
= E(e2t ) −
T s=1
T t=1 s=1
(
)
T
X
2
1
=E(e2t ) + − E(et et ) + 2
E(e2t )
T
T t=1
=

T −1 2
T −1
E(e2t ) =
σe ,
T
T

(S.16)

so lim E{(et − ē)2 } = σe2 .
T →∞

8. By the independence of et and Xt , for any s and t, we have that
Cov{(Xt − µ
b)(et − ē), (es − ē)2 }
=E{(Xt − µ
b)(et − ē)(es − ē)2 } − E{(Xt − µ
b)(et − ē)}E{(es − ē)2 }
=E(Xt − µ
b)E{(et − ē)(es − ē)2 } − E(Xt − µ
b)E(et − ē)E(es − ē)2
=0,

(S.17)

where the last step is due to E(Xt − µ
b) = 0 and E(et − ē) = 0.
9. For any t 6= s, Cov {(et − ē)2 , (es − ē)2 } = 0; and for t = s,
Var{(et − ē)2 }
=E{(et − ē)4 } − E 2 {(et − ē)2 }
=E(e4t ) − 4E(e3t ē) + 6E(e2t ē2 ) − 4E(et ē3 ) + E(ē4t ) − {E(e2t ) − 2E(et ē) + E(ē2 )}2




4
6(T − 1)
6
4
1
3(T − 1)
4
4
2 2
4
4
4
2 2
=E(et ) − E(et ) +
{E(et )} + 2 E(et ) − 3 E(et ) +
E(et ) +
{E(et )}
T
T2
T
T
T3
T3

2
2
1
2
2
2
− E(et ) − E(et ) + E(et ) ,
(S.18)
T
T
so lim Var{(et − ē)2 } = E(e4t ) − {E(e2t )}2 = E(e4t ) − σe4 .
T →∞

9

10. Similar to the derivation in (S.18), we can show Cov{(et − ē)2 , (es − ē)(es+p − ē)} = 0
for s 6= t and s 6= t − p. For a given t,
Cov{(et − ē)2 , (et − ē)(et+p − ē)}
=E{(et − ē)3 (et+p − ē)} − E{(et − ē)2 }E{(et − ē)(et+p − ē)},

(S.19)

which can be derived analogously to the (S.18) that limT →∞ E{(et − ē)3 (et+p − ē)} − E{(et −
ē)2 }E{(et − ē)(et+p − ē)} = E{e3t et+p } − E{e2t }E{et et+p } = 0 and similarly lim Cov{(et −
T →∞

2

ē) , (et−p − ē)(et − ē)} = 0.
11. For any t,
Cov {(Xt − µ
b)(et+p − ē), (Xt+p−r − µ
b)(et+p − ē)}
 

= E (Xt − µ
b)(Xt+p−r − µ
b)(et+p − ē)2 − E(Xt − µ
b)E(Xt+p−r − µ
b)E 2 (et+p − ē)

= E {(Xt − µ
b)(Xt+p−r − µ
b)} E (et+p − ē)2


T −1
σe2 ,
(S.20)
= γ|p−r|
T
where the second step is because of E(Xt − µ
b) = 0 and the independence of Xt and et , the
third step is due to (S.16) and (S.15). Hence,
lim Cov {(Xt − µ
b)(et+p − ē), (Xt+p−r − µ
b)(et+p − ē)} = γ|p−r| σe2 .

T →∞

Similarly,
lim Cov {(Xt+p − µ
b)(et − ē), (Xt−r − µ
b)(et − ē)} = γ|p−r| σe2 .

T →∞

Then, similarly,
Cov {(Xt − µ
b)(et+p − ē), (Xt+p+r − µ
b)(et+p − ē)}
 

= E (Xt − µ
b)(Xt+p+r − µ
b)(et+p − ē)2 − E(Xt − µ
b)E(Xt+p−r − µ
b)E 2 (et+p − ē)

= E {(Xt − µ
b)(Xt+p+r − µ
b)} E (et − ē)2


T −1
(S.21)
= γp+r
σe2 ,
T
and hence limT →∞ Cov {(Xt − µ
b)(et+p − ē), (Xt+p+r − µ
b)(et+p − ē)} = γp+r σe2 . Similarly,
limT →∞ Cov {(Xt+p − µ
b)(et − ē), (Xt+r − µ
b)(et − ē)} = γp+r σe2 .
10

12. By independence assumption between {et }, if t 6= s or p 6= r, we have that
Cov {(et − ē)(et+p − ē), (es − ē)(es+r − ē)} = 0.

(S.22)

In addition, by (S.16), we have that
Var {(et − ē)(et+p − ē)}

= E (et − ē)2 (et+p − ē)2


= E (et − ē)2 E (et+p − ē)2 ,
2

T −1
σe4 ,
=
T
so limT →∞ Var {(et − ē)(et+p − ē)} = σe4 .

Step 2: Now we prove the results in (3).

11

(S.23)

∗
1◦ . We first show the derivation of q100
as follows:
(
)
T
T
X
X
1
1
∗
q100
= lim T Cov
(X ∗ − µ
b∗ )2 ,
(X ∗ − µ
b∗ )2
T →∞
T t=1 t
T s=1 s
"
T
1 X 2
= lim T Cov
α1 (Xt − µ
b)2 + 2α1 (Xt − µ
b)(et − ē) + (et − ē)2 ,
T →∞
T t=1
#
T
1X 2
α (Xs − µ
b)2 + 2α1 (Xs − µ
b)(es − ē) + (es − ē)2
T s=1 1
(
)
T
T
X
X
1
1
= α14 lim T Cov
(Xt − µ
b)2 ,
(Xs − µ
b)2
T →∞
T t=1
T s=1
(
)
T
T
1X
1X
+ lim T Cov
2α1 (Xt − µ
b)(et − ē),
2α1 (Xs − µ
b)(es − ē)
T →∞
T t=1
T s=1
(
)
T
T
X
1
1X
(et − ē)2 ,
(es − ē)2
+ lim T Cov
T →∞
T t=1
T s=1

=

α14 q00

T
T
4α12 X X
Cov {(Xt − µ
b)(et − ē), (Xs − µ
b)(es − ē)}
+ lim
T →∞ T
t=1 s=1
T
T

1 XX
+ lim
Cov (et − ē)2 , (es − ē)2
T →∞ T
t=1 s=1

=

α14 q00

T
4α12 X
Cov {(Xt − µ
b)(et − ē), (Xt − µ
b)(et − ē)}
+ lim
T →∞ T
t=1

T

1X
Cov (et − ē)2 , (et − ē)2
T →∞ T
t=1


= α14 q00 + 4α12 E (Xt − µ
b)2 (et − ē)2 + E(e4t ) − E(e2t )

+ lim

2

= α14 q00 + 4α12 γ0 σe2 + E(e4t ) − σe4 ,
where the second step is due to (S.10), the third step is because of (S.11), (S.17), and the
o
n P
P
b)2 , T1 Ts=1 (Xs − µ
b)2 , the fifth step is due
definition q00 = limT →∞ T Cov T1 Tt=1 (Xt − µ
to (S.12) and (S.18), and the sixth step is because (S.13) and (S.18), and the last step is
because (S.16) and (S.17).

12

∗
2◦ . We derive the value of q10p
:
(
)
T −p
T
X
X
1
1
∗
∗
q10p
= lim T Cov
(Xt∗ − µ
b∗ )2 ,
(Xs∗ − µ
b∗ )(Xs+p
−µ
b∗ )
T →∞
T t=1
T − p s=1
"
T
1 X 2
= lim T Cov
α1 (Xt − µ
b)2 + 2α1 (Xt − µ
b)(et − ē) + (et − ē)2 ,
T →∞
T t=1
T −p

1 X 2
α (Xs − µ
b)(Xs+p − µ
b) + α1 (Xs − µ
b)(es+p − ē)
T − p s=1 1
#
+ α1 (Xs+p − µ
b)(es − ē) + (es − ē)(es+p − ē)
)
T −p
T
X
X
1
1
(Xt − µ
b)2 ,
(Xs − µ
b)(Xs+p − µ
b)
= α14 lim T Cov
T →∞
T t=1
T − p s=1
(
)
T −p
T
1X
1 X
+ lim T Cov
2α1 (Xt − µ
b)(et − ē),
α1 (Xs − µ
b)(es+p − ē)
T →∞
T t=1
T − p s=1
(
)
T −p
T
1X
1 X
+ lim T Cov
2α1 (Xt − µ
b)(et − ē),
α1 (Xs+p − µ
b)(es − ē)
T →∞
T t=1
T − p s=1
)
(
T −p
T
X
1
1X
(et − ē)2 ,
(es − ē)(es+p − ē)
+ lim T Cov
T →∞
T t=1
T − p s=1
(

T

=

α14 q0p

T −p

2α12 X X
+ lim
Cov {(Xt − µ
b)(et − ē), (Xs − µ
b)(es+p − e¯s )}
T →∞ T − p
t=1 s=1
T

T −p

2α12 X X
Cov {(Xt − µ
b)(et − ē), (Xs+p − µ
b)(es − ē)}
T →∞ T − p
t=1 s=1

+ lim

=

α14 q0p

T
2α12 X
+ lim
Cov {(Xt − µ
b)(et − ē), (Xt−p − µ
b)(et − ē)}
T →∞ T − p
t=p
(s=t−p)
T −p
2α12 X
Cov {(Xt − µ
b)(et − ē), (Xt+p − µ
b)(et − ē)}
T →∞ T − p
t=1

+ lim

(s=t)


b)(Xt+p − µ
b)(et − ē)2
= α14 q0p + 2α12 E (Xt − µ
b)(Xt−p − µ
b)(et − ē)2 + 2α12 E (Xt − µ


= α14 q0p + 4α12 γp σe2 ,
where the second step is due to (S.10), the third step is because of (S.11) and (S.17), the
n P
o
PT −p
1
b)2 , T −p
(X
−
µ
b
)(X
−
µ
b
)
fourth step is by definition that q0p = limT →∞ T Cov T1 Tt=1 (Xt − µ
s
s+p
s=1
and (S.19), the fifth step is due to (S.12), and the last step is result from (S.16) and (S.15).
13

∗
3◦ . We derive q1pr
for p > 0, r > 0 and p 6= r:

(
∗
q1pr

= lim T Cov
T →∞

)
T −p
T −r
1 X ∗
1 X ∗
∗
∗
∗
∗
∗
∗
(X − µ
b )(Xt+p − µ
b ),
(X − µ
b )(Xs+r − µ
b)
T − p t=1 t
T − r s=1 s
T −p

"

1 X 2
= lim T Cov
α1 (Xt − µ
b)(Xt+p − µ
b) + α1 (Xt − µ
b)(et+p − ē)
T →∞
T − p t=1
+α1 (Xt+p − µ
b)(et − ē) + (et − ē)(et+p − ē)} ,
T −r

1 X 2
b)(Xs+r − µ
b) + α1 (Xs − µ
b)(es+r − ē)
α (Xs − µ
T − r s=1 1
#
+ α1 (Xs+r − µ
b)(es − ē) + (es − ē)(es+r − ē)
)
T −p
T −r
1 X
1 X
lim T Cov
(Xt − µ
b)(Xt+p − µ
b),
(Xs − µ
b)(Xs+r − µ
b)
T →∞
T − p t=1
T − r s=1
(
)
T −p
T −r
1 X
1 X
α1 (Xt − µ
b)(et+p − ē),
+ lim T Cov
α1 (Xs − µ
b)(es+r − ē)
T →∞
T − p t=1
T − r s=1
(
)
T −p
T −r
1 X
1 X
+ lim T Cov
α1 (Xt − µ
b)(et+p − ē),
α1 (Xs+r − µ
b)(es − ē)
T →∞
T − p t=1
T − r s=1
)
(
T −p
T −r
1 X
1 X
α1 (Xt+p − µ
b)(et − ē),
α1 (Xs − µ
b)(es+r − ē)
+ lim T Cov
T →∞
T − p t=1
T − r s=1
(
)
T −p
T −r
1 X
1 X
α1 (Xt+p − µ
b)(et − ē),
+ lim T Cov
α1 (Xs+r − µ
b)(es − ē)
T →∞
T − p t=1
T − r s=1
(

=

α14

=

α14 qpr

+

+

α12

α12

T
lim
T →∞ (T − p)(T − r)

T −p
X

Cov {(Xt − µ
b)(et+p − ē), (Xt+p−r − µ
b)(et+p − ē)}

t=max(1,r−p+1)
(s=t+p−r)

T −p−r
X
T
lim
Cov {(Xt − µ
b)(et+p − ē), (Xt+p+r − µ
b)(et+p − ē)}
T →∞ (T − p)(T − r)
t=1
(s=t+p)

+

α12

T −p
X
T
lim
Cov {(Xt+p − µ
b)(et − ē), (Xt−r − µ
b)(et − ē)}
T →∞ (T − p)(T − r)
t=r+1
(s=t−r)

+

α12

T
lim
T →∞ (T − p)(T − r)

T −max(p,r)

X

Cov {(Xt+p − µ
b)(et − ē), (Xt+r − µ
b)(et − ē)}

t=1
(s=t)

= α14 qpr + 2α12 σe2 (γ|p−r| + γp+r ),

(S.24)
14

where the second step is due to (S.10), the third step is because of (S.11) and a similar
version to (S.17), the fourth step is because (S.22) and by the definition that
o
n
PT −p
PT −r
1
1
b)(Xt+p − µ
b), T −r s=1 (Xs − µ
b)(Xs+r − µ
b) , and the
qpr = limT →∞ T Cov T −p t=1 (Xt − µ
last step is from (S.20) and (S.21).

15

∗
4◦ . Finally, we present the derivation of q1pp
for p 6= 0,
)
(
T −p
T −p
X
X
1
1
∗
∗
∗
(X ∗ − µ
b∗ )(Xt+p
−µ
b∗ ),
(X ∗ − µ
b∗ )(Xs+p
−µ
b∗ )
q1pp
= lim T Cov
T →∞
T − p t=1 t
T − p s=1 s
"
T −p
1 X 2
α1 (Xt − µ
b)(Xt+p − µ
b) + α1 (Xt − µ
b)(et+p − ē)
= lim T Cov
T →∞
T − p t=1

+α1 (Xt+p − µ
b)(et − ē) + (et − ē)(et+p − ē)} ,
T −p

1 X 2
α (Xs − µ
b)(Xs+p − µ
b) + α1 (Xs − µ
b)(es+p − ē)
T − p s=1 1
#
+ α1 (Xs+p − µ
b)(es − ē) + (es − ē)(es+p − ē)
)
T −p
T −p
X
X
1
1
(Xt − µ
b)(Xt+p − µ
b),
(Xs − µ
b)(Xs+p − µ
b)
= α14 lim T Cov
T →∞
T − p t=1
T − p s=1
(
)
T −p
T −p
1 X
1 X
+ lim T Cov
α1 (Xt − µ
b)(et+p − ē),
α1 (Xs − µ
b)(es+p − ē)
T →∞
T − p t=1
T − p s=1
(
)
T −p
T −p
1 X
1 X
+ lim T Cov
α1 (Xt − µ
b)(et+p − ē),
α1 (Xs+p − µ
b)(es − ē)
T →∞
T − p t=1
T − p s=1
(
)
T −p
T −p
1 X
1 X
+ lim T Cov
α1 (Xt+p − µ
b)(et − ē),
α1 (Xs − µ
b)(es+p − ē)
T →∞
T − p t=1
T − p s=1
(
)
T −p
T −p
1 X
1 X
+ lim T Cov
α1 (Xt+p − µ
b)(et − ē),
α1 (Xs+p − µ
b)(es − ē)
T →∞
T − p t=1
T − p s=1
(
)
T −p
T −p
1 X
1 X
+ lim T Cov
(et − ē)(et+p − ē),
(es − ē)(es+p − ē)
T →∞
T − p t=1
T − p s=1
(

=

α14 qpp

+

α12

T −p
X
T
lim
Cov {(Xt − µ
b)(et+p − ē), (Xt − µ
b)(et+p − ē)}
T →∞ (T − p)2
t=1
s=t
T −2p

X
T
Cov {(Xt − µ
b)(et+p − ē), (Xt+2p − µ
b)(et+p − ē)}
T →∞ (T − p)2
t=1

+ α12 lim

s=t+p

+

α12

T −p
X
T
lim
Cov {(Xt+p − µ
b)(et − ē), (Xt−p − µ
b)(et − ē)}
T →∞ (T − p)2
t=1+p
s=t−p

+

α12

T
X
T
lim
Cov {(Xt+p − µ
b)(et − ē), (Xt+p − µ
b)(et − ē)}
T →∞ (T − p)2
t=1
s=t

+ α12 lim

T →∞

T
Var {(et − ē)(et+p − ē)} = α14 qpp + 2α12 σe2 (γ0 + γ2p ) + σe4 , (S.25)
2
(T − p)
16

where the second step is due to (S.10), the third step is because of (S.11) and a similar
version to (S.17), the fourth step is because (S.22) and by the definition that
o
n
PT −p
PT −p
1
1
b)(Xt+p − µ
b), T −p s=1 (Xs − µ
b)(Xs+p − µ
b) , the last
qpp = limT →∞ T Cov T −p t=1 (Xt − µ
step is because of (S.23), and (S.20) and (S.21) with q = p.

A.5

The proof of Theorem 4

Proof of Theorem 4(1):
For k = 1, . . . , p, applying the weak law of large numbers to γ
bk∗ , we obtain that as T → ∞,
∗
), which is denoted as γk∗ .
the estimator γ
bk∗ converges in probability to Cov(Xt∗ , Xt−k

Next, we examine γk . By the form of measurement error model (9), we have that for
0 < k < t,
∗
Cov(Xt∗ , Xt−k
)

= Cov(β0 Xt ut , β0 Xt−k ut−k )
= β02 {E(Xt ut Xt−k ut−k ) − E(Xt ut )E(Xt−k ut−k )}
= β02 {E(ut )E(ut−k )Cov(Xt , Xt−k )}
= β02 {Cov(Xt , Xt−k )} = β02 γk ,
and by (10), Var(Xt∗ ) = β02 {(σu2 + 1)γ0 + σu2 µ2 }, which is denoted as γ0∗ . Thus, Theorem 4(1)
follows.

Proof of Theorem 4(2):
First, by Theorem 4(1), we write
γ
b∗ = β02 γ + op (1)
and


2
2
2 2
2
2
β (σ + 1)γ0 + β0 σu µ
β0 γ1 · · ·
β0 γp−1
 0 u

.
..


..
b∗ = 
..
Γ
.
 + op (1)
.


2
2
2
2
2 2
β0 γp−1
β0 γp−2 · · · β0 (σu + 1)γ0 + β0 σu µ

= β02 Γ + σu2 (γ0 + µ2 )Ip + op (1).
17



γ
b0∗

···

.

..
b∗ =  ..
where Γ
.

∗
···
γ
bp−1
∗
(6) with γ
bk ,



∗
γ
bp−1


.. 
bk in
. . Then the naive estimator φb∗ is obtained by replacing γ

γ
b0∗



φb∗ = [β02 Γ + σu2 (γ0 + µ2 )Ip + op (1)]−1 {β02 γ + op (1)} = Γ + σu2 (γ0 + µ2 )Ip

−1

γ + op (1),
(S.26)

and hence φ∗ = {Γ + σu2 (γ0 + µ2 )Ip }

−1

p

γ such that φb∗ −→ φ∗ as T → ∞.

Again, by replacing γ
bk in (6) with γ
bk∗ gives the naive estimator φb∗0
!
!
p
T
T
X
X
X
1
1
∗
φb∗0 =
φb∗k
X∗ −
Xt−k
T − p t=p t
T
−
p
t=p
k=1
= E(Xt∗ ) − E(Xt∗ )

p
X

φb∗k + op (1)

k=1

= β0 E(Xt ) − β0 E(Xt )

p
X

{φ∗k + op (1)} + op (1)

k=1

= β0 (1 − φ∗T 1p )µ + op (1),
where φbk and φk are respectively the kth element of φb and φ, the third step is because
φbk = φk + op (1) by (S.26) as well as the model form (9), and the last step is due to the
stationarity of the time series {Xt } such that E(Xt ) = µ.
b∗ φb∗ by
b0∗ − 2φb∗T γ
b∗2 = γ
Finally, noting that the native estimator σb ∗2 is given by σ
b∗ + φb∗T Γ
applying a version similar to (6), we obtain that
b∗ φb∗
σ
b∗2 =b
γ0∗ − 2φb∗T γ
b∗ + φb∗T Γ

=β02 (σu2 + 1)γ0 + σu2 µ2 − 2β02 γ T {Γ + σu2 (γ0 + µ2 )I}−1 γ
+ β02 γ T {Γ + σu2 (γ0 + µ2 )I}−1 {Γ + σu2 (γ0 + µ2 )I}{Γ + σu2 (γ0 + µ2 )I}−1 γ + op (1)

=β02 (σu2 + 1)γ0 + σu2 µ2 − β02 γ T {Γ + σu2 (γ0 + µ2 )I}−1 γ + op (1).
Proof of Theorem 4(3):
Step 1: We show that as T → ∞,
√
T

!
T −p
T
X
1X ∗
1
∗
∗
(Xt − µ∗ )(Xt+p
− µ∗ ) −
(Xt∗ − µ
b∗ )(Xt+p
−µ
b∗ ) = op (1).
T t=1
T − p t=1
18

(S.27)

With some simple algebra,
(
)
T −p
T
X
√
1
1X ∗
∗
∗
T
(X − µ∗ )(Xt+p
− µ∗ ) −
(X ∗ − µ
b∗ )(Xt+p
−µ
b∗ )
T t=1 t
T − p t=1 t
(
)
T −p
T
X
√
1X ∗
1
∗
∗
= T
(X − µ∗ )(Xt+p
− µ∗ ) −
(X ∗ − µ∗ + µ∗ − µ
b∗ )(Xt+p
− µ∗ + µ∗ − µ
b∗ )
T t=1 t
T − p t=1 t
(
T −p
T
√
1X ∗
1 X ∗
∗
∗
∗
∗
(Xt − µ )(Xt+p − µ ) −
(Xt − µ∗ )(Xt+p
− µ∗ )
= T
T t=1
T − p t=1
)
T −p
T −p
T −p
X
X
1 X ∗
1
1
−
(X − µ∗ )(µ∗ − µ
b∗ ) −
(X ∗ − µ∗ )(µ∗ − µ
b∗ ) −
(µ∗ − µ
b∗ )2
T − p t=1 t
T − p t=1 t+p
T − p t=1


T −p
T
X
√
T −p
1 X ∗
1
∗
∗
∗
∗
= T
(X − µ )(Xt+p − µ ) + √
−1
(Xt∗ − µ∗ )(Xt+p
− µ∗ )
T
T − p t=1 t
T t=T −p+1
!
T −p
T −p
X
X
√
1
1
X∗ +
X∗ − µ
µ∗ − µ∗ )
b∗ − µ∗
+ T (b
(S.28)
T − p t=1 t
T − p t=1 t+p
, I1 + I2 + I3 .
Now we examine each term in (S.28) as T → ∞ separately. First,
T −p
p
1 X ∗
∗
I1 = − √
(Xt − µ∗ )(Xt+p
− µ∗ )
T
−
p
T
t=1
p
= − √ {γp∗ + op (1)} = op (1)
as T → ∞.
(S.29)
T
P
1
∗
−
Next, we examine the second term I2 in (S.28). Since T − 2 E[ Tt=T −p+1 (Xt∗ − µ∗ )(Xt+p
1

1

µ∗ )] ≤ T − 2 pVar(Xt ) (Brockwell et al. 1991, p.230) and T − 2 pVar(Xt ) → 0 as T → ∞, we
have that
1
I2 = √
T

T
X

∗
(Xt∗ − µ∗ )(Xt+p
− µ∗ ) = op (1).

t=T −p+1

19

(S.30)

Finally, we examine I3 in (S.28).
T −p

1 X ∗
X −µ
b∗
T − p t=1 t+p
=

T −p
p
T
1 X ∗
1X ∗ 1 X ∗
Xt+p −
Xt −
X
T − p t=1
T t=1
T t=p+1 t

T −p
p
T −p
1X ∗ 1X ∗
1 X ∗
X −
X −
X
=
T − p t=1 t+p T t=1 t
T t=1 t+p

=(

T −p
p
1X ∗
1 X ∗
1
Xt+p −
− )
X
T − p T t=1
T t=1 t

=op (1)
where µ
b∗ =

1
T

PT

t=1

Xt∗ , and

1
T

as

Pp

t=1

T → ∞,

(S.31)

Xt∗ = op (1) because E( T1

Pp

t=1

Xt∗ ) =

1
pE(Xt )
T

→ 0 as

T → ∞. In addition, by the weak law of large numbers,
T −p

1 X ∗
p
Xt − µ∗ −→ 0
T − p t=1

as

T → ∞.

(S.32)

By condition (R2) and the central limit theorem for strictly stationary p-dependent sequences
(Brockwell et al. 1991, Theorem 6.4.2), we have
√

T (b
µ∗ − µ∗ ) = Op (1).

(S.33)

Therefore, applying (S.29), (S.30), (S.31), (S.32) and (S.33) yields (S.27).

Step 2: We show that as T → ∞, the asymptotic covariance matrix of

√  ∗ ∗T T
T (b
γ0 , γ
b ) − (γ0∗ , γ ∗T )T

equals
(
lim Cov

T →∞

)
T
T
X
1
1 X ∗
∗
∗
√
− µ∗ ), √
− µ∗ ) .
(Xt − µ∗ )(Xt+r
(Xs∗ − µ∗ )(Xs+q
T t=1
T s=1

20

For k ≤ p
√

T (b
γk − γk )
(
)
T −k
√
1 X ∗
∗
= T
(X − µ
b∗ )(Xt+k
−µ
b∗ ) − γk
T − k t=1 t
(
)
T
√
1X ∗
∗
= T
(X − µ∗ )(Xt+k
− µ∗ ) − γk
T t=1 t
)
(
T
−k
T
X
X
√
1
1
∗
∗
(Xt∗ − µ
b∗ )(Xt+k
−µ
b∗ ) −
(Xt∗ − µ∗ )(Xt+k
− µ∗ )
+ T
T − k t=1
T t=1
)
(
T
1 X ∗
∗
− µ∗ ) − γk + op (1),
= √
(Xt − µ∗ )(Xt+k
T t=1
where the last step is due to (S.27).
Hence, the (r, q) element of matrix lim Var
T →∞

(
lim Cov

T →∞

√ 

T (b
γ0∗ , γ
b∗T )T − (γ0∗ , γ ∗T )T
is given by

)
T
T
X
1
1 X ∗
∗
∗
√
(Xt − µ∗ )(Xt+r
− µ∗ ), √
(Xs∗ − µ∗ )(Xs+q
− µ∗ ) .
T t=1
T s=1

Step 3: We show certain identities to be used for proving Theorem 4(3):
1. By model (9), we have that
Xt∗ − µ∗ = β0 Xt ut − β0 µ
= β0 Xt ut − β0 ut µ + β0 ut µ − β0 µ
= β0 {ut (Xt − µ) + µ(ut − 1)}
where the first step is because µ∗ = E(β0 Xt ut ) = β0 E(Xt )E(ut ) = β0 µ.

21

(S.34)

2. We have that
T
T

1 XX
Cov u2t (Xt − µ)2 , u2s (Xs − µ)2
lim
T →∞ T
t=1 s=1
T
T

1 XX
E{u2t u2s (Xt − µ)2 (Xs − µ)2 } − E(u2t )E(u2s )E{(Xt − µ)2 }E{(Xs − µ)2 } ,
T →∞ T
t=1 s=1

= lim

T
T

1 XX
E(u2t u2s )E{(Xt − µ)2 (Xs − µ)2 } − E(u2t )E(u2s )E{(Xt − µ)2 }E{(Xs − µ)2 }
T →∞ T
t=1 s=1

= lim

s6=t

T

1 X
E(u4t )E{(Xt − µ)4 } − E 2 (u2t )E 2 {(Xt − µ)2 } ,
+ lim
T →∞ T
t=1
s=t

1
T →∞ T

= lim

T X
T
X
t=1 s=1
s6=t

1
T →∞ T

+ lim



T

1X 2 2
E(u2t )E(u2s )Cov{(Xt − µ)2 , (Xs − µ)2 } + lim
E (ut )Var{(Xt − µ)2 }
T →∞ T
t=1
s=t

T
X



E(u4t ) − E 2 (u2t ) E{(Xt − µ)4 }

t=1
s=t

T
T
T

1 XX
1 X
E(u2t )E(u2s )Cov{(Xt − µ)2 , (Xs − µ)2 } + lim
E(u4t ) − E 2 (u2t ) E{(Xt − µ)4 }
T →∞ T
T →∞ T
t=1 s=1
t=1

= lim

=(σu2 + 1)2 q00 + {E(u4t ) − (σu2 + 1)2 }E{(Xt − µ)4 },

(S.35)

where the second and third step is due to the independence between ut and Xt . In the last
T P
T
P
step, we use the definition q00 = limT →∞ T1
Cov {(Xt − µ)2 , (Xs − µ)2 }, E(u2t ) = σu2 +1,
t=1 s=1

and the fact that E(u4t ) and E{(Xt − µ)4 } are time-independent which are derived from
Conditions (R1) and (R2) together with independence between ut and Xt .
3. Similar to the derivation in (S.35), now we derive the summation of Cov{β02 u2t (Xt −

22

µ)2 , β02 us us+p (Xs − µ)(Xs+p − µ)} for p > 0,
T
T
1 XX
lim
Cov{β02 u2t (Xt − µ)2 , β02 us us+p (Xs − µ)(Xs+p − µ)}
T →∞ T
t=1 s=1
T
T
β04 X X 
E(u2t us us+p )E{(Xt − µ)2 (Xs − µ)(Xs+p − µ)}
T →∞ T
t=1 s=1

−E(u2t )E(us )E(us+p )E(Xt − µ)2 E{(Xs − µ)(Xs+p − µ)}

= lim

T
T
β04 X X
E(u2t )E(us )E(us+p )Cov{(Xt − µ)2 , (Xs − µ)(Xs+p − µ)}
= lim
T →∞ T
t=1 s=1
T
β04 X 
+ lim
E(u3t )E(ut+p ) − E(u2t )E(ut )E(ut+p ) E{(Xt − µ)3 (Xt+p − µ)}
T →∞ T
t=1

+ lim

T →∞

s=t
T
β04 X

T
T



E(u3t )E(ut−p ) − E(u2t )E(ut )E(ut−p ) E{(Xt − µ)3 (Xt−p − µ)}

t=1
s=t−p
T

β4 X X 2
= lim 0
(σu + 1)Cov{(Xt − µ)2 , (Xs − µ)(Xs+p − µ)}
T →∞ T
t=1 s=1

+ β04 E(u3t ) − E(u2t ) E{(Xt − µ)3 (Xt+p − µ)}

+ β04 E(u3t ) − E(u2t ) E{(Xt − µ)3 (Xt−p − µ)},



= β04 q0p (σu2 + 1) + β04 E(u3t ) − (σu2 + 1) E{(Xt − µ)3 (Xt+p − µ)} + E{(Xt − µ)3 (Xt−p − µ)} ,
(S.36)
where the first step is because Xt and ut are independent, and the second last step is due
to E(u2t ) = V ar(ut ) + E(u2t ) = σu2 + 1 and is derived similar to the second and third step in
T P
T
P
Cov{(Xt −
(S.35), and the last step is because of the definition that q0p = limT →∞ T1
t=1 s=1

µ)2 , (Xs − µ)(Xs+p − µ)} and the fact that E{(Xt − µ)3 (Xt+p − µ)}, E{(Xt − µ)3 (Xt−p − µ)}
and E(u3t ) are time-independent, derived from Conditions (R1) and (R2) together with the
independence between ut and Xt .
4.

Analogous to the derivation in (S.35) and (S.36), we derive the summation of

23

Cov{ut ut+p (Xt − µ)(Xt+p − µ), us us+r (Xs − µ)(Xs+r − µ)} for p > 0, r > 0 and p 6= r,
β04

T
T
1 XX
lim
Cov{ut ut+p (Xt − µ)(Xt+p − µ), us us+r (Xs − µ)(Xs+r − µ)}
T →∞ T
t=1 s=1
T
T
1 XX
E(ut ut+p us us+r )Cov{(Xt − µ)(Xt+p − µ), (Xs − µ)(Xs+r − µ)}
T →∞ T
t=1 s=1

=β04 lim

T
1 X
E(u2t )E(ut+p )E(ut+r ) − 1 E{(Xt − µ)2 (Xt+p − µ)(Xt+r − µ)}
T →∞ T
t=1

+ β04 lim

1
T →∞ T

+ β04 lim

s=t
T
X



E(u2t+p )E(ut )E(ut+p+r ) − 1 E{(Xt − µ)(Xt+p − µ)2 (Xt+p+r − µ)}

t=1
s=t+p

T
1 X 
E(u2t )E(ut+p )E(ut−r ) − 1 E{(Xt−r − µ)(Xt − µ)2 (Xt+p − µ)}
T →∞ T
t=1

+ β04 lim

s=t−r

1
T →∞ T

+ β04 lim

T
X


E(u2t+p )E(ut )E(ut+p−r ) − 1 E{(Xt − µ)(Xt+p−r − µ)Xt+p − µ)2 }

t=1
s=t+p−r

=β04 qpr + β04 σu2 E{(Xt − µ)2 (Xt+p − µ)(Xt+r − µ)} + β04 σu2 E{(Xt − µ)(Xt+p − µ)2 (Xt+p+r − µ)}
+ β04 σu2 E{(Xt−r − µ)(Xt − µ)2 (Xt+p − µ)} + β04 σu2 E{(Xt − µ)(Xt+p−r − µ)(Xt+p − µ)2 },
(S.37)
where the third step is derived analogously to the second step of (S.36), and E(ut ut+p us us+r ) =
T P
T
P
Cov{(Xt − µ)(Xt+p −
1, and the last step is due to the definition qpr = limT →∞ T1
t=1 s=1

µ), (Xs −µ)(Xs+r −µ)} and the fact that E{(Xt −µ)2 (Xt+p −µ)(Xt+r −µ)}, E{(Xt −µ)(Xt+p −
µ)2 (Xt+p+r − µ)}, E{(Xt−r − µ)(Xt − µ)2 (Xt+p − µ)}, and E{(Xt − µ)(Xt+p − µ)2 (Xt+2p − µ)}
are time-independent derived from Conditions (R1) and (R2).
5. Similar to the derivation in (S.35), (S.36), and (S.37), we derive the summation of

24

Cov{ut ut+p (Xt − µ)(Xt+p − µ), us us+p (Xs − µ)(Xs+p − µ)} for p > 0,
β04

T
T
1 XX
lim
Cov{ut ut+p (Xt − µ)(Xt+p − µ), us us+p (Xs − µ)(Xs+p − µ)}
T →∞ T
t=1 s=1
T
T
1 XX
E(ut )E(ut+p )E(us )E(us+p )Cov{(Xt − µ)(Xt+p − µ), (Xs − µ)(Xs+p − µ)
T →∞ T
t=1 s=1

= β04 lim

T
1 X
E(u2t )E(u2t+p ) − 1 Var{(Xt − µ)(Xt+p − µ)}
T →∞ T
t=1

+ β04 lim

1
T →∞ T

+ β04 lim

s=t
T
X



E(u2t+p )E(ut )E(ut+2p ) − 1 E{(Xt − µ)(Xt+p − µ)2 (Xt+2p − µ)}

t=1
s=t+p

T
1 X 
E(u2t )E(ut−p )E(ut+p ) − 1 E{(Xt−p − µ)(Xt − µ)2 (Xt+p − µ)}
T →∞ T
t=1

+ β04 lim

s=t−p

= β04 qpp + β04 (σu4 + 2σu2 )Var{(Xt − µ)(Xt+p − µ)} + 2β04 E{(Xt − µ)(Xt+p − µ)2 (Xt+2p − µ)},
(S.38)
where the last step is by the definition qpp = limT →∞

1
T

T P
T
P

Cov{(Xt − µ)(Xt+p − µ), (Xs −

t=1 s=1

µ)(Xs+p − µ)} and E{(Xt − µ)(Xt+p − µ)2 (Xt+2p − µ)} = E{(Xt−p − µ)(Xt − µ)2 (Xt+p − µ)}
due to the stationarity of the time series and the fact that Var{(Xt − µ)(Xt+p − µ)} and
E{(Xt −µ)(Xt+p −µ)2 (Xt+2p −µ)} are time-independent, resulting from the Conditions (R1)
and (R2).
6. For any t, s and p, we have that
Cov{(Xt − µ)(Xt−p − µ), (Xs − µ)}
=E{(Xt − µ)(Xt−p − µ)(Xs − µ)} − E{(Xt − µ)(Xt−p − µ)}E(Xs − µ)
=E{(Xt − µ)(Xt−p − µ)(Xs − µ)},
where the last step is because E(Xs − µ) = 0.

25

(S.39)

7. For any t and s, we have that
Cov{ut (ut − 1)(Xt − µ), us (us − 1)(Xs − µ)}
=E{ut (ut − 1)(Xt − µ)us (us − 1)(Xs − µ)} − E{ut (ut − 1)(Xt − µ)}E{us (us − 1)(Xs − µ)}
=E{ut (ut − 1)(Xt − µ)us (us − 1)(Xs − µ)}
=E{ut (ut − 1)us (us − 1)}E{(Xt − µ)(Xs − µ)},

(S.40)

where the second step is because of the independence between ut and Xt and that E(Xt −µ) =
0. Then, E{ut (ut −1)us (us −1)} = σu4 for t 6= s and E{u2t (ut −1)2 } = E(u4t )−2E(u3t )+σu2 +1
for any t.
By (S.40), we have that
T
T
1 XX
Cov {ut (ut − 1)(Xt − µ), us (us − 1)(Xs − µ)}
T →∞ T
t=1 s=1

lim

T
T
1 XX
E{ut (ut − 1)us (us − 1)}E{(Xt − µ)(Xs − µ)}
T →∞ T
t=1 s=1

= lim

T
T
T
1 XX 4
1 X
= lim
σu E{(Xt − µ)(Xs − µ)} + lim
E(u4t ) − 2E(u3t ) + σu2 + 1 − σu4 E{(Xt − µ)2 }
T →∞ T
T →∞ T
t=1
t=1 s=1
s=t

=σu4

∞
X


γh + E(u4t ) − 2E(u3t ) + σu2 + 1 − σu4 γ0 ,

(S.41)

h=−∞

where the last is because limT →∞

1
T

PT PT
t=1

s=1

E{(Xt −µ)(Xs −µ)} =

P∞

h=−∞

γh (Brockwell

et al. 1991, Theorem 7.1.1).
8. For any t, s and p > 0, we have that
Cov{ut (ut − 1)(Xt − µ), us+p (us − 1)(Xs+p − µ)}
=E{ut (ut − 1)(Xt − µ)us+p (us − 1)(Xs+p − µ)} − E{ut (ut − 1)(Xt − µ)}E{us+p (us − 1)(Xs+p − µ)}
=E{ut (ut − 1)us+p (us − 1)}E{(Xt − µ)(Xs+p − µ)}
=E{ut (ut − 1)us+p (us − 1)}γ|s+p−t| ,

(S.42)

where the second step is because of the independence between ut and Xt and that E(Xt −µ) =
0. Then, E{ut (ut − 1)us+p (us − 1)} = 0 for t 6= s and E{ut (ut − 1)2 ut+p } = E{ut (ut − 1)2 } =
E{(ut − 1)3 } + σu2 for any s = t.
26

9. By independence of ut and us , for t 6= s, we have that
Cov{u2t (Xt − µ)2 , (us − 1)2 } = 0,

(S.43)

and for any t,
Cov{u2t (Xt − µ)2 , (ut − 1)2 }
=E{u2t (ut − 1)2 (Xt − µ)2 } − E{u2t (Xt − µ)2 }E{(ut − 1)2 }


= E{u2t (ut − 1)2 } − E(u2t )E(ut − 1)2 E{(Xt − µ)2 }

= E(u4t ) − 2E(u3t ) + σu2 + 1 − σu4 − σu2 γ0

= E(u4t ) − 2E(u3t ) + 1 − σu4 γ0 .

(S.44)

10. By independence of ut and us , for s 6= t, s 6= t + p and any p, we have that
Cov{ut ut+p (Xt − µ)(Xt+p − µ), (us − 1)2 } = 0.

(S.45)

For any t and p > 0,
Cov{ut ut+p (Xt − µ)(Xt+p − µ), (ut − 1)2 }
=E{ut ut+p (ut − 1)2 (Xt − µ)(Xt+p − µ)} − E{ut ut+p (Xt − µ)(Xt+p − µ)}E{(ut − 1)2 }


= E{ut ut+p (ut − 1)2 } − E(ut ut+p )E{(ut − 1)2 } E{(Xt − µ)(Xt+p − µ)}

=E (ut − 1)3 γp ,
(S.46)
and

Cov{ut ut−p (Xt − µ)(Xt−p − µ), (ut − 1)2 } = E (ut − 1)3 γp .
11. For any t and s, and r 6= p and r > 0, we have that
Cov{ut ut+p (Xt − µ)(Xt+p − µ), (us − 1)(us+r − 1)} = 0.

(S.47)

By independence of ut and us , for t 6= s and any p, we have that
Cov{ut ut+p (Xt − µ)(Xt+p − µ), (us − 1)(us+p − 1)} = 0,

27

(S.48)

and for any t and p > 0,
Cov{ut ut+p (Xt − µ)(Xt+p − µ), (ut − 1)(ut+p − 1)}
=E{ut ut+p (ut − 1)(tt+p − 1)(Xt − µ)(Xt+p − µ)}
− E{ut ut+p (Xt − µ)(Xt+p − µ)}E{(ut − 1)(ut+p − 1)}
=E{ut (ut − 1)}E{ut+p (ut+p − 1)}E{(Xt − µ)(Xt+p − µ)}
=σu4 γp .

(S.49)

12. For any t, we have that
Cov{ut (ut − 1)(Xt − µ), (us − 1)2 }
= E{ut (ut − 1)(Xt − µ)(us − 1)2 } − E{ut (ut − 1)(Xt − µ)}E{(us − 1)2 }


= E{ut (ut − 1)(us − 1)2 } − E{ut (ut − 1)}E{(us − 1)2 } E(Xt − µ) = 0,

(S.50)

where the last step is because E(Xt − µ) = 0.
13. By independence assumption between {ut }, if t 6= s or p 6= r, we have that
Cov {(ut − 1)(ut+p − 1), (us − 1)(us+r − 1)} = 0.

(S.51)

In addition, for any t and p we have that
Var {(ut − 1)(ut+p − 1)}

= E (ut − 1)2 (ut+p − 1)2


= E (ut − 1)2 E (ut+p − 1)2
= σu4 ,

(S.52)

and for any t, we have that
Var(ut − 1)2
=E{(ut − 1)4 } − E 2 {(ut − 1)2 }
=E{(ut − 1)4 } − σu4 .

28

(S.53)

Step 4: Now we prove the results in (3).
∗
1◦ . We first show the derivation of q200
as follows:
(
)
T
T
X
X
1
1
∗
= lim T Cov
q200
(Xt∗ − µ∗ )2 ,
(Xs∗ − µ∗ )2
T →∞
T t=1
T s=1
T
T

β04 X X
= lim
Cov u2t (Xt − µ)2 + 2µut (ut − 1)(Xt − µ) + µ2 (ut − 1)2 ,
T →∞ T
t=1 s=1

u2s (Xs − µ)2 + 2µus (us − 1)(Xs − µ) + µ2 (us − 1)2
T
T

β04 X X
Cov u2t (Xt − µ)2 , u2s (Xs − µ)2
T →∞ T
t=1 s=1

= lim

T
T

4µβ04 X X
Cov u2t (Xt − µ)2 , us (us − 1)(Xs − µ)
T →∞ T
t=1 s=1

+ lim

T
T

2µ2 β04 X X
+ lim
Cov u2t (Xt − µ)2 , (us − 1)2
T →∞
T t=1 s=1
T
T
4µ2 β04 X X
Cov {ut (ut − 1)(Xt − µ), us (us − 1)(Xs − µ)}
+ lim
T →∞
T t=1 s=1
T
T

µ4 β04 X X
+ lim
Cov (ut − 1)2 , (us − 1)2
T →∞ T
t=1 s=1

= β04 (σu2 + 1)2 q0 + β04 {E(u4t ) − (σu2 + 1)2 }E{(Xt − µ)4 }
+ 4µβ04 σu2 (σu2 + 1)v00 + 4µβ04 {E(u4t ) − E(u3t ) − σu2 (σu2 + 1)}E{(Xt − µ)3 }

+ 2µ2 β04 E(u4t ) − 2E(u3t ) + 1 − σu4 γ0
"
#
∞
X

+ 4µ2 β04 σu4
γh + E(u4t ) − 2E(u3t ) + σu2 + 1 − σu4 γ0
h=−∞



+ µ4 β04 E{(ut − 1)4 } − σu4 ,
where the second step is due to (S.34), the third step is because of (S.50), the last step is by
(S.35), (S.39), (S.41), (S.43), (S.44), and (S.53).

29

∗
2◦ . Then we derive the value of q20p
:

(
∗
q20p
= lim T Cov
T →∞

= lim

T
T
X
1X ∗
∗ 2 1
∗
(Xt − µ ) ,
(Xs∗ − µ∗ )(Xs+p
− µ∗ )
T
T
t=1

T
T
β04 X X

T →∞

T

)

s=1


Cov u2t (Xt − µ)2 + 2µut (ut − 1)(Xt − µ) + µ2 (ut − 1)2 ,

t=1 s=1

us us+p (Xs − µ)(Xs+p − µ) + µus (us+p − 1)(Xs − µ) + µus+p (us − 1)(Xs+p − µ) + µ2 (us − 1)(us+p − 1)
T
T

β04 X X
Cov u2t (Xt − µ)2 , us us+p (Xs − µ)(Xs+p − µ)
T →∞ T

= lim

+

+

+

t=1 s=1
T
T

µβ04 X X
lim
Cov u2t (Xt − µ)2 , us (us+p − 1)(Xs − µ) + us+p (us − 1)(Xs+p
T →∞ T
t=1 s=1
T
T
4
2µβ0 X X
lim
Cov {us us+p (Xs − µ)(Xs+p − µ), ut (ut − 1)(Xt − µ)}
T →∞ T
t=1 s=1
T
T

µ2 β04 X X 
Cov u2t (Xt − µ)2 , (us − 1)(us+p − 1)
lim
T →∞ T
t=1 s=1

− µ)



+ Cov (ut − 1)2 , us us+p (Xs − µ)(Xs+p − µ)
T
T
2µ2 β04 X X
Cov {ut (ut − 1)(Xt − µ), us (us+p − 1)(Xs − µ)}
T →∞
T

+ lim

t=1 s=1

+ lim

T →∞

+ lim

T →∞

2µ2 β04

T X
T
X

T

Cov {ut (ut − 1)(Xt − µ), us+p (us − 1)(Xs+p − µ)}

t=1 s=1
T
T
µ4 β04 X X

T


Cov (ut − 1)2 , (us − 1)(us+p − 1)

t=1 s=1




+ 1) + β04 E(u3t ) − (σu2 + 1) E{(Xt − µ)3 (Xt+p − µ)} + E{(Xt − µ)3 (Xt−p − µ)}


+ µβ04 E{u3t − u2t } E{(Xt − µ)2 (Xt−p − µ)} + E{(Xt − µ)2 (Xt+p − µ)}


+ 2µβ04 σu2 v0p + 2µβ04 E{u3t − u2t − σu2 } E{(Xt − µ)2 (Xt−p − µ)} + E{(Xt − µ)2 (Xt+p − µ)}

+ 2µ2 β04 E(ut − 1)3 γp + 4µ2 β04 E(ut − 1)3 + σu2 γp + µ4 β04 σu4



= β04 qp (σu2 + 1) + β04 E(u3t ) − (σu2 + 1) E{(Xt − µ)3 (Xt+p − µ)} + E{(Xt − µ)3 (Xt−p − µ)}


+ 2µβ04 σu2 vp + µβ04 E{3u3t − 3u2t − 2σu2 } E{(Xt − µ)2 (Xt−p − µ)} + E{(Xt − µ)2 (Xt+p − µ)}
=

β04 q0p (σu2

+ 6µ2 β04 E(ut − 1)3 γp + 4µ2 β04 σu2 γp ,

where the second step is by (S.34), the third step is because (S.39) and (S.50), and the
second last step is because (S.36), (S.47), (S.46), (S.42), and (S.51).

30

∗
3◦ . Then we derive the value of q2pr
for r 6= p

(
∗
q2pr

= lim T Cov
T →∞

T
T
1X ∗
1X ∗
∗
∗
(Xt − µ∗ )(Xt+p
− µ∗ ),
(X − µ∗ )(Xs+r
− µ∗ )
T t=1
T s=1 s

)

T
T
β04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ)
T →∞ T
t=1 s=1

= lim

+ µut (ut+p − 1)(Xt − µ) + µut+p (ut − 1)(Xt+p − µ) + µ2 (ut − 1)(ut+p − 1),
us us+r (Xs − µ)(Xs+r − µ) + µus (us+r − 1)(Xs − µ) + µus+r (us − 1)(Xs+r − µ) + µ2 (us − 1)(us+r − 1)
T
T
β04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ), us us+r (Xs − µ)(Xs+r − µ)}
T →∞ T
t=1 s=1

= lim

T
T
µβ04 X X
+ lim
Cov {us us+r (Xs − µ)(Xs+r − µ), ut (ut+p − 1)(Xt − µ)}
T →∞ T
t=1 s=1
T
T
µβ04 X X
Cov {us us+r (Xs − µ)(Xs+r − µ), ut+p (ut − 1)(Xt+p − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µβ04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ), us (us+r − 1)(Xs − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µβ04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ), us+r (us − 1)(Xs+r − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
2µ2 β04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ), (us − 1)(us+r − 1)}
T →∞
T t=1 s=1

+ lim

T
T
µ2 β04 X X
Cov {ut (ut+p − 1)(Xt − µ), us (us+r − 1)(Xs − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µ2 β04 X X
Cov {ut (ut+p − 1)(Xt − µ), us+r (us − 1)(Xs+r − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µ2 β04 X X
+ lim
Cov {ut+p (ut − 1)(Xt+p − µ), us (us+r − 1)(Xs − µ)}
T →∞ T
t=1 s=1
T
T
µ2 β04 X X
Cov {ut+p (ut − 1)(Xt+p − µ), us+r (us − 1)(Xs+r − µ)}
T →∞ T
t=1 s=1

+ lim

T

T

µ4 β04 X X
Cov {(ut − 1)(ut+p − 1), (us − 1)(us+q − 1)} ,
T →∞ T
t=1 s=1

= β04 qpr + β04 σu2 E{(Xt − µ)2 (Xt+p − µ)(Xt+r − µ)} + E{(Xt − µ)(Xt+p − µ)2 (Xt+p+r − µ)}

+E{(Xt−r − µ)(Xt − µ)2 (Xt+p − µ)} + E{(Xt − µ)(Xt+p−r − µ)(Xt+p − µ)2 }
+ lim

+ µβ04 σu2 [E{(Xt − µ)(Xt+p − µ)(Xt+r − µ)} + E{(Xt − µ)(Xt+p − µ)(Xt+p+r − µ)}
+E{(Xt−r − µ)(Xt − µ)(Xt+p − µ)} + E{(Xt − µ)(Xt+p−r − µ)(Xt+p − µ)}]
+ 2µ2 β04 σu2 (γ|p−r| + γp+r ),

(S.54)

where the second step is by (S.34), the third step is because (S.39) and (S.50), and the
31

second last step is because (S.36), (S.47), (S.42), and (S.51).
∗
∗
4◦ . Finally, similar to the derivation of q2pq
, now we derive the value of q2pp

(
∗
q2pp

= lim T Cov
T →∞

)
T
T
1X ∗
1X ∗
∗
∗
∗
∗
∗
∗
(X − µ )(Xt+p − µ ),
(X − µ )(Xs+p − µ )
T t=1 t
T s=1 s

T
T
β04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ) + µut (ut+p − 1)(Xt − µ)
T →∞ T
t=1 s=1

= lim

+ µut+p (ut − 1)(Xt+p − µ) + µ2 (ut − 1)(ut+p − 1),
us us+r (Xs − µ)(Xs+p − µ) + µus (us+p − 1)(Xs − µ) + µus+p (us − 1)(Xs+p − µ) + µ2 (us − 1)(us+p − 1)
T
T
β04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ), us us+p (Xs − µ)(Xs+p − µ)}
T →∞ T
t=1 s=1

= lim

T
T
2µ2 β04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ), (us − 1)(us+p − 1)}
T →∞
T t=1 s=1

+ lim

T
T
µβ04 X X
Cov {us us+p (Xs − µ)(Xs+p − µ), ut (ut+p − 1)(Xt − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µβ04 X X
Cov {us us+p (Xs − µ)(Xs+p − µ), ut+p (ut − 1)(Xt+p − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µβ04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ), us (us+p − 1)(Xs − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µβ04 X X
Cov {ut ut+p (Xt − µ)(Xt+p − µ), us+p (us − 1)(Xs+p − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µ2 β04 X X
Cov {ut (ut+p − 1)(Xt − µ), us (us+p − 1)(Xs − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µ2 β04 X X
Cov {ut (ut+p − 1)(Xt − µ), us+p (us − 1)(Xs+p − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µ2 β04 X X
Cov {ut+p (ut − 1)(Xt+p − µ), us (us+p − 1)(Xs − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µ2 β04 X X
Cov {ut+p (ut − 1)(Xt+p − µ), us+p (us − 1)(Xs+p − µ)}
T →∞ T
t=1 s=1

+ lim

T
T
µ4 β04 X X
Cov {(ut − 1)(ut+p − 1), (us − 1)(us+p − 1)} ,
T →∞ T
t=1 s=1

+ lim

= β04 qpp + β04 (σu4 + 2σu2 )Var{(Xt − µ)(Xt+p − µ)} + 2β04 E{(Xt − µ)(Xt+p − µ)2 (Xt+2p − µ)} (S.55)


+ µβ04 σu2 E{(Xt − µ)(Xt+p − µ)2 } + 2E{(Xt − µ)(Xt+p − µ)(Xt+2p − µ)} + E{(Xt − µ)2 (Xt+p − µ)}
+ 2µ2 β04 σu4 γp + 2µ2 β04 σu2 (γ0 + γ2p ) + µ4 β04 σu4 ,

where the second step is by (S.34), the third step is because (S.39) and (S.50), and the last
step is because (S.38), (S.48), (S.49) and (S.52).
32

B

Tables
Supplementary Table 4: The results of the augmented Dickey-Fuller test
British Columbia

Definition
Definition 1

Definition 2

Definition 3

Transformation

TSV

Ontario

p-value

TSV

Quebec

p-value

TSV

Alberta

p-value

TSV

p-value

Xt

-8.346

<0.01

-1.527

0.755

-1.813

0.645

-2.850

0.245

Xt+1 − Xt

-6.974

<0.01

-5.522

<0.01

-3.880

0.027

-3.516

0.059

Xt

-1.208

0.878

-4.294

<0.01

-2.018

0.566

-1.768

0.662

Xt+1 − Xt

-3.336

0.084

-2.599

0.342

-3.340

0.084

-3.296

0.090

Xt

-1.325

0.833

-2.264

0.471

0.098

0.999

-2.688

0.307

Xt+1 − Xt

-3.590

0.048

-4.584

<0.01

-2.209

0.492

-2.008

0.569

Supplementary Table 5: The results of the augmented Dickey-Fuller test
British Columbia
Definition

Ontario

Quebec

Alberta

Differencing

lag p

Differencing

lag p

Differencing

lag p

Differencing

lag p

1 degree

1

1 degree

1

1 degree

1

1 degree

1

no differencing

2

-

-

-

-

-

-

Definition 2

1 degree

2

no differencing

2

1 degree

2

1 degree

1

Definition 3

1 degree

1

1 degree

4

-

-

-

-

Definition 1

33

34

0.1
0.01

0.05
0.2

0.03
0.3

Additive (σe2 )
Multiplicative (σu2 )

Additive (σe2 )
Multiplicative (σu2 )

Additive (σe2 )
Multiplicative (σu2 )

0.3

Multiplicative (σu2 )

0.6

0.06

AR(2)

0.5

0.1

AR(2)

0.02

0.2

AR(2)*

0.6

0.2

-

-

-

1

1

0.01

0.1

0.1

0.02

0.2

0.05

AR(4)

0.005

0.05

AR(2)*

-

-

0.5

0.5

AR(1)

AR(1)
0.1

Ontario

British Columbia

Additive (σe2 )

Error Model

* The time series with no differencing

Definition 3

Definition 2

Definition 1

Definition

that are used for sensitivity analyses.

-

-

-

1

1

-

-

0.3

0.1

-

-

-

0.6

0.2

AR(2)

-

-

0.5

0.5

AR(1)

Quebec

-

-

-

0.8

0.3

-

-

0.4

0.05

-

-

-

0.8

0.1

AR(1)

-

-

0.4

0.1

AR(1)

Alberta

Supplementary Table 6: The parameter values of σe2 or σu2 for the measurement error model (7) or (9)

Supplementary Table 7: Definition 3: The parameter estimation under different measurement error models: the AR(1) model with “order-1 differencing” is used to fit the data of
British Columbia and the AR(4) model with “order-1 differencing” is used to fit the data of
Ontario.
British Columbia
Method

Ontario

Parameter

EST

SE

p-value

EST

SE

p-value

φ0

0.105

0.038

0.018

0.379

0.057

<0.001

φ1

-0.207

0.077

0.020

-0.086

0.099

0.391

φ2

-

-

-

-0.287

0.106

0.012

φ3

-

-

-

-0.301

0.094

0.004

φ4

-

-

-

-0.284

0.078

0.001

φ0

0.057

0.021

0.021

0.206

0.031

<0.001

φ1

-0.213

0.086

0.029

-0.088

0.100

0.383

φ2

-

-

-

-0.290

0.109

0.014

φ3

-

-

-

-0.303

0.094

0.003

The Proposed Method

φ4

-

-

-

-0.287

0.081

0.002

with Additive Error

φ0

0.058

0.021

0.017

0.212

0.036

<0.001

φ1

-0.234

0.147

0.137

-0.102

0.123

0.417

φ2

-

-

-

-0.306

0.139

0.037

φ3

-

-

-

-0.318

0.107

0.006

φ4

-

-

-

-0.308

0.093

0.003

φ0

0.058

0.023

0.027

0.210

0.033

<0.001

φ1

-0.244

0.090

0.019

-0.097

0.107

0.375

φ2

-

-

-

-0.300

0.117

0.016

φ3

-

-

-

-0.312

0.098

0.004

The Proposed Method

φ4

-

-

-

-0.300

0.087

0.002

with Multiplicative Error

φ0

0.066

0.035

0.087

0.230

0.058

0.001

Naive

Error Degree

-

2
Small (σe1
)

2
Large (σe2
)

2
Small (σu1
)

2
Large (σu2
)

φ1

-0.401

0.219

0.092

-0.139

0.183

0.454

φ2

-

-

-

-0.347

0.213

0.116

φ3

-

-

-

-0.354

0.159

0.035

φ4

-

-

-

-0.361

0.149

0.023

35

36

0

2

4

6

8

0

2

4

6

Apr 13

Apr 20

Apr 27

Additive

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

reported mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

rate (May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the

Supplementary Figure 4: British Columbia by Definition 1 (AR(2), no differencing): A 5-day forecasting of the true mortality

Fatality Rate (%)

8

Mild

37

0

2

4

6

0

2

4

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

the reported mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

mortality rate (May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow);

Supplementary Figure 5: British Columbia by Definition 1 (AR(1), order-1 differencing): A 5-day forecasting of the true

Fatality Rate (%)

6

Additive

Mild

38

1

2

3

4

5

6

1

2

3

4

5

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

the reported mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

mortality rate (May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow);

Supplementary Figure 6: British Columbia by Definition 2 (AR(3), order-1 differencing): A 5-day forecasting of the true

Fatality Rate (%)

6

Additive

Mild

39

0

10

20

−10

0

10

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 7: Ontario by Definition 1 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

20

Additive

Mild

40

2

4

6

8

10

2

4

6

8

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 8: Ontario by Definition 2 (AR(1), no differencing): A 5-day forecasting of the true mortality rate (May

Fatality Rate (%)

10

Additive

Mild

41

0

5

10

15

0

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 9: Quebec by Definition 1 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

10

Additive

Mild

42

2

4

6

8

10

2.5

5.0

7.5

Apr 13

Apr 20

Apr 27

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 10: Quebec by Definition 2 (AR(2), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

10.0

Additive

Mild

43

0

2

4

6

0

2

4

Apr 13

Apr 20

Apr 27

Additive

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 11: Alberta by Definition 1 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

6

Mild

44

0

1

2

3

4

0

1

2

3

Apr 13

Apr 20

Apr 27

Additive

May 04

Day

Apr 13

Apr 20

Apr 27

Multiplicative

May 04

Moderate

Multiplicative

Additive

95% Prediction Interval

Naive

Multiplicative

Additive

Measurement Error Type

Reported Fatality

Fitted Fatality

Adjusted Fatality

Reference Time Series

mortality rates (in black) and the adjusted true mortality rate accounting for the asymptomatic cases (in green).

(May 5 - May 9) based on the additive (in blue) or multiplicative (in red) versus the naive model (in dark yellow); the reported

Supplementary Figure 12: Alberta by Definition 2 (AR(1), order-1 differencing): A 5-day forecasting of the true mortality rate

Fatality Rate (%)

4

Mild

45

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive
0.012
0.011
0.013

Moderate
Mild
Moderate

0.004
0.004
0.003

Moderate
Mild
Moderate

0.060
0.061
0.060

Moderate
Mild
Moderate

0.002
0.004
0.006
0.004
0.005

Mild
Moderate
Mild
Moderate

Alberta

0.163
0.061

Mild

Quebec

0.004

Mild

0.830

Ontario

0.017
0.011

-

Day 2

0.013

0.012

0.017

0.012

0.007

0.215

0.216

0.215

0.216

0.607

0.132

0.119

0.119

0.116

0.077

0.001

0.001

0.001

0.001

0.006

British Columbia

Mild

Naive

Day 1

-

2
2
σe
(or σu
)

Method

definition of death rates.

0.045

0.044

0.052

0.044

0.027

0.477

0.479

0.478

0.479

1.357

0.000

0.001

0.001

0.002

3.409

0.004

0.005

0.005

0.005

0.017

Day 3

0.089

0.087

0.098

0.087

0.055

0.776

0.778

0.776

0.778

2.289

0.225

0.171

0.172

0.161

0.360

0.029

0.027

0.027

0.027

0.058

Day 4

0.118

0.115

0.129

0.115

0.070

1.050

1.053

1.051

1.053

3.294

0.029

0.007

0.007

0.004

8.264

0.037

0.035

0.036

0.035

0.081

Day 5

Observed Prediction Error
OPE(h)

0.270

0.263

0.302

0.262

0.160

2.578

2.586

2.580

2.587

7.709

0.389

0.302

0.304

0.288

12.940

0.083

0.078

0.080

0.078

0.178

Definition 1

h=1

PH

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.169

0.176

0.591

0.607

0.612

0.015

0.020

0.057

0.066

0.069

Day 1

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.418

0.420

1.422

1.440

1.446

0.019

0.023

0.070

0.078

0.081

Day 2

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.630

0.602

2.040

2.046

2.048

0.018

0.023

0.070

0.078

0.081

Day 3

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.775

0.704

2.378

2.373

2.372

0.019

0.023

0.072

0.080

0.082

Day 4

0.022

0.031

0.035

0.115

0.125

0.205

0.399

0.811

1.561

1.811

0.888

0.754

2.531

2.517

2.514

0.019

0.023

0.073

0.080

0.083

Day 5

Expected Prediction Error
EPE(h)

0.109

0.157

0.177

0.577

0.627

1.025

1.995

4.057

7.807

9.057

2.879

2.655

8.963

8.983

8.991

0.090

0.111

0.342

0.382

0.396

h=1

PH

Supplementary Table 8: Definition 1: The observed prediction error and expected prediction error for different

46

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive

Naive

Multiplicative

Additive
0.010
0.010
0.010

Moderate
Mild
Moderate

0.004

Mild
Moderate

0.000
0.000
0.002

Moderate
Mild
Moderate

0.001
0.001
0.001
0.001
0.001

Mild
Moderate
Mild
Moderate

Alberta

0.013
0.000

Mild

Quebec

0.000
0.000

Moderate

0.001

Mild

0.020

Ontario

0.015
0.010

-

Day 2

0.001

0.001

0.001

0.001

0.000

0.003

0.002

0.002

0.001

0.044

0.010

0.002

0.000

0.004

0.087

0.005

0.005

0.005

0.005

0.015

British Columbia

Mild

Naive

Day 1

-

2
2
σe
(or σu
)

Method

definition of death rates.

0.003

0.003

0.003

0.003

0.002

0.004

0.003

0.003

0.003

0.086

0.014

0.003

0.000

0.007

0.196

0.011

0.011

0.011

0.011

0.032

Day 3

0.006

0.006

0.006

0.007

0.003

0.017

0.013

0.014

0.013

0.183

0.000

0.044

0.023

0.056

0.521

0.011

0.011

0.011

0.011

0.043

Day 4

0.019

0.019

0.019

0.019

0.012

0.073

0.066

0.068

0.065

0.413

0.035

0.152

0.110

0.175

1.059

0.000

0.000

0.000

0.000

0.020

Day 5

Observed Prediction Error
OPE(h)

0.030

0.031

0.031

0.031

0.019

0.098

0.084

0.087

0.081

0.739

0.063

0.201

0.134

0.243

1.884

0.037

0.037

0.037

0.037

0.126

Definition 2

h=1

PH

0.008

0.012

0.036

0.044

0.047

0.030

0.044

0.130

0.163

0.174

0.270

0.558

1.453

2.264

2.527

0.034

0.044

0.154

0.154

0.164

Day 1

0.008

0.012

0.037

0.045

0.047

0.030

0.045

0.133

0.165

0.176

0.331

0.599

1.626

2.391

2.643

0.035

0.044

0.157

0.157

0.167

Day 2

0.008

0.012

0.037

0.045

0.047

0.033

0.049

0.149

0.181

0.191

0.345

0.603

1.646

2.399

2.649

0.035

0.044

0.157

0.157

0.167

Day 3

0.008

0.012

0.037

0.045

0.047

0.034

0.049

0.151

0.182

0.192

0.348

0.603

1.649

2.399

2.649

0.035

0.044

0.157

0.157

0.167

Day 4

0.008

0.012

0.037

0.045

0.047

0.034

0.050

0.153

0.183

0.193

0.348

0.603

1.649

2.399

2.649

0.035

0.044

0.157

0.157

0.167

Day 5

Expected Prediction Error
EPE(h)

0.042

0.059

0.185

0.223

0.236

0.162

0.236

0.716

0.874

0.926

1.642

2.965

8.023

11.853

13.117

0.174

0.222

0.784

0.783

0.834

h=1

PH

Supplementary Table 9: Definition 2: The observed prediction error and expected prediction error for different

47

Multiplicative

Additive

Naive

Multiplicative

Additive

Day 2

0.001
0.001

Mild
Moderate

0.048
0.002
0.002
0.002
0.002

Mild
Moderate
Mild
Moderate

Ontario

0.001

Moderate

0.000

0.004

0.004

0.004

0.004

0.132

0.001

0.001

0.001

0.001

0.003

British Columbia

0.001

-

Naive

Day 1

Mild

2
2
σe
(or σu
)

Method

definition of death rates.

0.011

0.011

0.011

0.011

0.243

0.000

0.000

0.000

0.000

0.020

Day 3

0.015

0.016

0.016

0.017

0.333

0.006

0.005

0.005

0.005

0.057

Day 4

0.023

0.023

0.023

0.024

0.464

0.010

0.009

0.009

0.009

0.090

Day 5

Observed Prediction Error
OPE(h)

0.055

0.057

0.057

0.058

1.219

0.017

0.016

0.016

0.016

0.170

Definition 3

h=1

PH

0.009

0.011

0.036

0.039

0.039

0.005

0.007

0.026

0.029

0.030

Day 1

0.009

0.011

0.036

0.039

0.039

0.005

0.008

0.028

0.030

0.031

Day 2

0.009

0.011

0.036

0.039

0.039

0.005

0.008

0.028

0.030

0.031

Day 3

0.010

0.012

0.039

0.042

0.042

0.005

0.008

0.028

0.030

0.031

Day 4

0.010

0.012

0.039

0.042

0.042

0.005

0.008

0.028

0.030

0.031

Day 5

Expected Prediction Error
EPE(h)

0.048

0.056

0.187

0.200

0.202

0.023

0.038

0.137

0.151

0.155

h=1

PH

Supplementary Table 10: Definition 3: The observed prediction error and expected prediction error for different

