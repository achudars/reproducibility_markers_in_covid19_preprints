Noname manuscript No.
(will be inserted by the editor)

Disinformation in the Online Information Ecosystem:
Detection, Mitigation and Challenges
Amrita Bhattacharjee · Kai Shu · Min
Gao · Huan Liu

arXiv:2010.09113v1 [cs.SI] 18 Oct 2020

the date of receipt and acceptance should be inserted later

Abstract With the rapid increase in access to internet and the subsequent
growth in the population of online social media users, the quality of information posted, disseminated and consumed via these platforms is an issue of
growing concern. A large fraction of the common public turn to social media
platforms and in general the internet for news and even information regarding
highly concerning issues such as COVID-19 symptoms. Given that the online
information ecosystem is extremely noisy, fraught with misinformation and
disinformation, and often contaminated by malicious agents spreading propaganda, identifying genuine and good quality information from disinformation
is a challenging task for humans. In this regard, there is a significant amount of
ongoing research in the directions of disinformation detection and mitigation.
In this survey, we discuss the online disinformation problem, focusing on the
recent ‘infodemic’ in the wake of the coronavirus pandemic. We then proceed
to discuss the inherent challenges in disinformation research, and then elaborate on the computational and interdisciplinary approaches towards mitigation
of disinformation, after a short overview of the various directions explored in
detection efforts.
A. Bhattacharjee · H. Liu
Department of Computer Science and Engineering
Arizona State University
Tempe, Arizona, USA
E-mail: {abhatt43,huanliu}@asu.edu
K. Shu
Department of Computer Science
Illinois Institute of Technology
Chicago, Illinois, USA
E-mail: kshu@iit.edu
M. Gao
School of Big Data and Software Engineering
Chongqing University
Chongqing, China
E-mail: gaomin@cqu.edu.cn

2

Amrita Bhattacharjee et al.

Keywords disinformation · mitigation · detection · social media

1 Introduction
Over the last few decades Artificial Intelligence research has progressed by
leaps and bounds, and given the parallel advancements in the development of
computational hardware components, better, faster and more advanced methods of using AI are being adopted in different fields including medical science,
environmental technology, agriculture, etc. Realizing the vast potential for
applying AI to solve socio-economic, political and even cultural issues, experts and practitioners from academia and industry have come together to
form organizations and launch projects for this very purpose. A few examples
include efforts by Google AI1 , Microsoft2 , International Telecommunication
Union(ITU) which is an agency of the United nations3 among others.
Among the several diverse domains in which AI is being used, one gaining
increasing popularity is that of information literacy, detection and mitigation
of disinformation, especially on social media [88]. According to a report by
the International Telecommunication Union, there were 4.13 billion internet
users in 2019, which is more than 50% of the total 7.7 billion global population. According to the same report by the International Telecommunication
Union, among social media platforms, Facebook is one most widely used by 2.2 billion people in 2018. Along with Facebook, other social media platforms like Twitter, Reddit etc., and personal blogs, opinion based websites are
a perfect ground for the spread of false information - disinformation. Large
scale online disinformation has the potential to shift public opinion, polarize people, and can even threaten public health, democracy and international
relations [58] [62].
False information and fabricated content on social media exist in different
forms and go by different names. Misinformation [102] is usually referred to
erroneous or false information that is disseminated unintentionally, and is often
considered as an “honest mistake”. On the other hand, disinformation is false
information, propagated intentionally with some ulterior motive in mind, be it
political, economic, or simply to instigate people. Disinformation can usually
be verified to be false and may also consist of true facts expressed in the
wrong context in order to mislead readers. Fake news [43] is usually seen as a
subset of disinformation and refers to false content or propaganda published in
such a way that it appears to be genuine news. Conspiracy theories [49] have
been defined as implausible beliefs and stories that speculate that malicious
actors have secretly come together to deceive the public and achieve some goal.
Another word used in this context, hoax means something that appears to be
true but is meant to trick or deceive people. Troll or internet troll [31] is a
1
2
3

https://ai.google/social-good/
https://www.microsoft.com/en-us/ai/ai-for-good
https://aiforgood.itu.int/

Disinformation Mitigation - An Overview

3

bot or a human on the internet that tries to instigate and polarize people by
spreading inflammatory and highly offensive content and comments.

Misinformation

Disinformation

False or deceptive
online content

Fake news

Conspiracy
theory

Hoax

Troll

Fig. 1 Types of false or deceptive online content

In this paper, we focus on the domain of disinformation and discuss past,
ongoing and potential future applications of artificial intelligence and interdisciplinary ideas to solve specific problems in detection and mitigation of online
disinformation. The rest of the paper is structured as follows: Section 2 presents
a case study of disinformation in the context of the coronavirus pandemic. In
Section 3 we describe the challenges in disinformation research, particularly
in detection and mitigation tasks and discuss some approaches to tackle such
challenges. In Section 4 we provide a brief overview on approaches in disinformation detection before discussing computational methods for disinformation
mitigation in Section 5. In Section 6 we elaborate on some interdisciplinary approaches towards disinformation detection and mitigation, before concluding
the survey and talking about future work in Section 7.

2 Disinformation: A Case Study during the COVID-19 Crisis
Although disinformation and fake news have been in existence for the last few
decades, the proliferation of social media along with the low cost of generating
and spreading content online, have made it increasingly easier for false information to be widely disseminated. Hence, disinformation and fake news now
stand as a threat to public opinion and democracy. Recently, the COVID-19
pandemic has brought on a new wave of misinformation and disinformation
on social media. The information overload, consisting of both true and false
information, on social media has been so overwhelming that the World Health
Organization has identified this phenomenon as an ‘infodemic’ [46]. According
to [12], the pandemic has resulted in the spread of 2,000 myths and conspiracy
theories regarding the virus. Given the large fraction of people who seek out
information on the internet, this is alarming. On 4th May, 2020, a COVID-19

4

Amrita Bhattacharjee et al.

related conspiracy film named ‘Plandemic’ went viral on social media platforms including Facebook and Youtube [27], spreading dangerous claims that
the spread of the virus and the subsequent development of the vaccine is
pre-planned by global elites. The video even included apparent frontline doctors claiming that wearing a mask “activates” the coronavirus. The spread
of such misinformation and disinformation in the context of the coronavirus
is especially threatening because it can make people lose trust in scientists,
governments, and can gravely undermine a coordinated response to the virus,
thereby making it impossible to wipe out or even control the virus. Social media platforms like Twitter and Facebook have been working hard to moderate
and remove content that violates their community standards, including coronavirus related misleading posts from politicians [17]. Disinformation related
to the coronavirus mostly fall into the following few categories.
2.1 Health Related Mis/Disinformation
The highly infectious nature of the novel coronavirus and the detrimental effects it has on human beings has caused a great amount of panic all across the
world, and it is natural for people to be desperate to find cures or promising
preventative measures. Although till date there does not exist a cure or vaccine for the virus, individuals and companies have tried to take advantage of
the panic and gullibility of consumers and claimed to create/discover products
that ‘cure’ or ‘boost immunity’ against the novel coronavirus. Some examples
of such false cures include colloidal silver, herbal teas, essential oils, dietary
supplements etc. Several groups and companies have been identified and warnings have been issued against these fraudulent activities and claims that have
the potential to cause great harm to gullible consumers [5]. Dealing with
such health-related misinformation or disinformation is quite challenging for
a number of reasons. Even official, otherwise credible sources may mistakenly
publish wrong misinformation. For example, the US Food and Drug Administration(FDA) had to revoke their authorization of the emergency use of chloroquine and hydroxychloroquine for treating hospitalized COVID-19 patients, as
evidence suggested that the risks outweighed the benefits [9]. Furthermore, the
President of the United States of America had falsely claimed that ingesting
disinfectants including bleach could be effective against COVID-19 [11], and
that the drug hydroxychloroquine is a global cure for the coronavirus [8] [13].
Both these claims including several other drug related claims making rounds
on the internet were fact-checked and debunked. Such dangerous claims have
already had detrimental effects and people lacking health-related knowledge
have taken harmful steps following such myths, even resulting in death [20].
2.2 Conspiracy Theories
There have also been an increasing number of conspiracy theories cropping up
regarding the origin and spread of the novel coronavirus. According to a recent

Disinformation Mitigation - An Overview

5

study by the Pew Research Center [4], 71% of Americans have been exposed
to some conspiracy theory regarding the coronavirus and around a quarter
of Americans believe in one of the conspiracies. As shown in the ‘Plandemic’
video, there are conspiracy theories suggesting the virus is man-made and
was created by global elites for profit and power. Some conspiracy theories
claim that wearing face masks can cause carbon dioxide intoxication, while
some view it as an infringement of civil liberties [1] [33]. According to some
conspiracy theorists, 5G radiation helps the spread of the virus, or that the
virus is a man-made bioweapon created in a lab. Some others claim that Bill
Gates funded the creation of the virus and has intentions of depopulating
the world and implanting microchips in people [25]. A large group of people
believe that the virus is simply a hoax and is made up or exaggerated by the
media, often for political reasons [10]. In the midst of the rapidly evolving
coronavirus situation, such conspiracy theories do nothing but fuel the panic
and desperation among people who are already desperately seeking answers
and explanations. In extreme cases, these conspiracy theories may even damage
public trust in the coordinated response to the virus, healthcare regulations,
etc. and in the process worsen the situation.

2.3 Scams and Frauds
The coronavirus pandemic has had devastating effects on global economy, and
pushed millions of people into unemployment. Governments, including the US
government, had arranged for relief/stimulus checks, unemployment benefits,
etc. to relieve citizens of some stress during such a difficult time. Unfortunately,
scammers have been trying to utilize this situation to their advantage, by tapping into people’s despair and desperation and ultimately robbing them in
the process. Over the duration of the coronavirus pandemic, malicious groups
and individuals have been targeting vulnerable people including older, retired
people, leveraging their virus-related fear to push them to make hasty decisions. Many such scams were in the form of scam calls, emails related to
stimulus checks, charity scams, and even dubious work-from-home opportunities in the wake of the massive unemployment as a result of the pandemic4 .
According to [7], 68% of the over 204,000 complaint calls made to US Federal Trade Commission(FTC) regarding COVID-19 related payments, were to
report fraud or identity theft. Fraud websites claimed to offer coronavirus vaccinations, although one does not exist yet, and even offered free coronavirus
and antibody tests [19]. Fraudsters even pretended to be contact tracers and
contacted gullible people to get their personal information which were then
used in health insurance scams [24]. According to [7], there have also been
instances of scam calls offering help with loan payments and bills, websites
selling ‘miracle’ cures for the coronavirus, and even emails with malware links
in them. Several government and non-government agencies have put out warn4

https://www.fcc.gov/covid-scams

6

Amrita Bhattacharjee et al.

ings to make consumers aware of the increased number of scams and frauds
during this time, especially COVID-19 related ones.

3 Challenges in Disinformation Research
Research in the disinformation domain is often challenging and thus, even
though there has been a significant amount of work in certain aspects of disinformation, much of the landscape is unexplored and under-explored. There are
several issues and nuances that hinder the process, some of which we discuss
here:
– Broad field - Disinformation and fabricated content have existed in almost
all domains, be it political propaganda or fabricated scientific research
papers.
– Dynamic field - Disinformation is an ever-changing field of research. New
problems, challenges and threats keep cropping up every day, with the
latest ones being the threat of AI generated news articles and extreme
human-like language generation capabilities. So it is important to model
future threats and take action accordingly.
– Interdisciplinary issue - The problem of fake news and disinformation is
heavily an interdisciplinary issue involving computer scientists, political
scientists and strategists, policy makers, ethics experts and even psychology
and cognitive scientists.
Disinformation and false/fabricated content exist in almost all domains and
information dissemination platforms, be it social media sites like Facebook and
Twitter, or crowd-sourced knowledge bases such as Wikipedia. Since disinformation manifests in such a wide variety of forms, including different modalities
such as text, fake videos, fabricated images, claims and images put out of context etc., it is significantly difficult, even as a human being, to understand and
comprehend what classifies as disinformation and what does not. Furthermore,
pre-existing beliefs, cognitive biases and being confined in echo chambers [93]
may also come in the way of making an informed decision regarding the veracity of a claim or post on social media. Hence, as it is quite challenging even for
humans to understand and identify disinformation, it is expected that it would
be harder for automated detection algorithms to correctly, effectively and efficiently identify disinformation. Furthermore, mitigation is an even harder
task because the first step to slowing the spread of a piece of disinformation
is detecting it, which itself is very challenging. Apart from that, mitigation
also comes with its own set of challenges. We discuss a few of the challenges
in detection and mitigation of disinformation and some approaches to tackle
them:

Disinformation Mitigation - An Overview

7

3.1 Data Challenge
Disinformation and fake news exist in almost all kinds of domains, be it political, health-related, rumors about celebrities, etc. News from different domains
have different styles of writing and even a quite different set of vocabulary.
This makes the problem of automatic disinformation detection very challenging, because automatic detection models trained with data from one domain
would not perform well when used to detect disinformation from another domain. Furthermore, it is also infeasible to develop separate classifiers for each
domain, since it is time and resource consuming, and also because annotated
datasets may not be available for all kinds of domains. Annotating datasets
is an expensive process and may also require domain knowledge experts. To
add to this problem, annotated datasets may have some event or topic specific
keywords and phrases, and again, a classifier trained on this dataset would not
perform well in other domains where that event or topic is not significant. Furthermore, some news articles may be pure satire without any mal-intention.
Disinformation detectors may fail to correctly differentiate a fake article from
a satire if it was not trained with that kind of data. Another challenge in the
generation of datasets for training, as identified by authors in [48], is that extracting ‘true’ news articles from reputed news websites and ‘fake’ ones from
suspicious websites may not result in a proper dataset for training purposes.
This is because even otherwise reputable news sources may post misinformation, and not all news articles on suspicious news websites may be false. Hence,
a better way of annotating is required, and instead of relying on the reputation of the news source, the authors have leveraged fact-checking websites to
harvest the veracity labels for news articles.
The problem of domain specific labelled data in disinformation datasets
can be partially addressed by using transfer-learning based approaches to train
automatic disinformation detectors. Although this idea has not been explored
extensively, a few recent works leveraging transfer learning to solve the problem of lack of labelled data for some domain are [55] [60]. To tackle the issue of
domain or event specific features, adversarial techniques could be used to train
the disinformation classifier such that it generalizes well to other kinds of domain/event. The objective is to prevent the classifier from learning the domain
or event specific features. Following this idea, Wang et al. [101] have proposed
an Event Adversarial Neural Network that tries to learn event-independent,
shared features instead of event specific ones so that the model performs well
on newly emerged disinformation.

3.2 Early Detection and Effective Mitigation
Disinformation and deceptive content on social media platforms are designed
and created in such a way so as to affect the reader emotionally and trigger
a quick response. Sensationalized headlines, ‘clickbaity’ images and language
having extreme sentiment are often used to get the attention of the user and

8

Amrita Bhattacharjee et al.

get the user to engage with the post. Such false posts may spread like wildfire
via readers sharing and re-sharing the post. For example, as we mentioned in
Section 2, a conspiracy video named ‘Plandemic’ went viral and had 8 million
viewers before being taken down [28]. The video had numerous false claims
regarding the coronavirus pandemic and even claimed that the outbreak was
pre-planned by a group of global elites for gaining profit and power. Ever since
the video surfaced online, people including influential politicians and athletes
shared it and endorsed it. On Facebook the video had around 2.5 million user
engagements in the form of likes, comments and shares before it was removed
from the platform [16]. Disinformation like these can have disastrous effects
such as decreasing public trust in healthcare systems, co-coordinated response
to the virus and in turn public health. In high-risk cases like this and otherwise, early detection of disinformation is extremely important in order to
minimize the harm caused. However this is an extremely challenging task, in
case of both manual and automated detection. First, manually fact-checking
claims for early detection might be challenging for newly emerging incidents as
these would require subject matter experts to verify the claims, due to the lack
of a pre-existing knowledge base. Furthermore, the process of fact-checking by
third-party fact-checkers takes time. Second, automatic detection would not
perform well since the model has not been trained on the newly emerged idea.
The performance would be even worse if there are domain inconsistencies between the domain in which the model was trained and the one in which it
is being used. Also, in the early stages of disinformation propagation, there
might be a lack of user comments and reactions which may have served as
rich sources of auxiliary information for disinformation detection. Early detection of disinformation is also essential for effective mitigation. The sooner
a disinformation post can be identified, the quicker a counter cascade can be
launched to fact-check the claim and curb the virality of the spread.

Although this is a relatively under-explored domain, some researchers have
proposed frameworks and models to tackle this issue. Liu and Wu [70] proposed a method that detect if a news article is fake based on the characteristics
of users on its propagation path. This method requires only the first five minutes of propagation data in order to get a good detection accuracy. To detect
fake news articles that have been published on a news website but not disseminated on social media yet, Zhou et al. [105] proposed a framework that relies
on features extracted from the news content - the extracted features have different granularities - syntax-level, lexicon-level, semantic-level etc. To tackle
the problem of the lack of user comments as a source of auxiliary information
in case of early detection, Qian et al. [80] developed a model that learns to
generate user responses conditioned on the article, that were then used in the
classification process along with the word and sentence level information from
the actual article. An interesting work by Vicario et al. [98] identifies topics
that might arise in misinformation posts in the future, thus providing some
kind of early warning.

Disinformation Mitigation - An Overview

9

3.3 Challenges in Fact-checking based Approaches
Researchers and practitioners in journalism domain have been using factchecking based methods to verify claims, be it written or verbal. Manual
fact-checking is an extremely slow process and requires domain experts to
inspect and investigate the news article and determine its veracity. Given the
huge amount of information generated and propagated online everyday, it is
impossible for traditional fact-checking to scale up to such a level. Furthermore, in this context early detection becomes a challenge as by the time the
veracity of the article has been determined, readers may have already propagated it to several others. Furthermore, it has also been seen that rebuttals
and exposure to a fact-checked piece of news after the initial exposure to the
false article, does not work as well as we would expect. Ecker [56] explains
this phenomenon from a psychology point of view, in the context of retraction
of faulty claims and how humans react to that. We go into details regarding
this topic in Section 6.1.
Automatic or computational fact-checking [51] is a better alternative to
manual fact-checking and may potentially alleviate the problem of latency in
verifying the truthfulness of claims and articles. However, this approach has
its own set of challenges and obstacles. Most automatic fact-checking methods
require a pre-existing knowledge base, usually in the form of a knowledge
graph, which needs to be regularly updated to ensure the knowledge base
is up to date. This is especially difficult in case of newly emerging topics and
knowledge, such as the symptoms and severity of COVID-19 in the early stages
of the pandemic. Furthermore, natural language processing models trained
on a certain corpus of claims may have linguistic biases. Although this can
be alleviated by using structured data (subject, predicate, object), getting
structured data is difficult. Furthermore, pre-existing knowledge bases may
themselves be noisy or biased, based on how these were created. Ensuring
the quality of the knowledge-base being used as ground truth is an additional
challenge.

3.4 Multi-modality Approaches
News articles and posts on social media are usually accompanied by an image
or a video related to or supporting the text. Presence of a related image or
video alongside the text increases the perceived credibility of the news article or
claim, even if the image is a fake or manipulated image. Highly sensationalized
posts designed to shift public opinion often leverage carefully manipulated
images, videos and even Deepfakes, which are almost indistinguishable to the
human eye. Genuine images and videos might even be taken out of context
to convey a meaning not originally intended. Hence, this poses an additional
challenge in disinformation detection on social media. Given that fabrication
of content and hence deceit can occur in so many different forms, it is difficult
to design generalizable models to detect disinformation and fabricated content.

10

Amrita Bhattacharjee et al.

Although individual aspects of the problem, such as fake image detection, have been addressed by many researchers, research along the direction of
multi-modal detection is limited and hence has great scope for improvement.
Significant efforts in this area include works by Wang et al. [101] which uses
text and visual feature extractors and then uses the concatenated feature representations to train the fake news detector. However, the correlations between
image and text are not captured in this method. In cases where the image and
the text in a post are individually both true, but viewing them together may
mislead the reader or represent something totally different, it is important to
learn the joint representation of text and image. Khattar et al. [66] use a variational autoencoder to learn a shared representation of the text and image, and
is able to reconstruct the text and image separately from this shared latent
representation, thereby capturing the correlations and dependencies across the
two modalities. A similar approach by Zhou et al. [106] learns the text and image representations separately and then computes similarity between text and
visual representations. Identifying a ‘mismatch’ between the modalities helps
in detect fake news. Cui et al. [53] have incorporated three types of modalities in their model - text, image and the user profile. The proposed fake news
detector also incorporates sentiments from user comments to determine if the
article/post is fake or not. An interesting approach utilizing knowledge from
external real-world knowledge graph, alongside textual and visual features, is
that of Zhang et al. [104]. Video and audio features along with text have been
utilized in [68]. Other efforts in multi-modal fake news detection include [64]
[92].

3.5 Policy Issues and Fairness
This challenge is mostly unique to the task of disinformation mitigation. After
successful detection of a false post on a social media platform, the next step is
to decide what corrective action is to be taken. This is especially challenging
from the point of view of social media platforms because of debates over the
right balance between free speech and protection against false information.
Censorship and in particular, social media censorship have been an issue of
national and international debate and taking concrete action against online
disinformation is difficult. Recently, the US White House declared an executive order [15] against online censorship by social media platforms, on the
ground that it violates free speech and the foundations of open debate and
even democracy. On the other hand, social media platforms also face immense
criticism from a large section of the general public as well as large organization
regarding the lousy approach to curb misinformation and disinformation on
the platforms [42]. Categorizing a controversial post as disinformation or an
opposing viewpoint often requires nuanced debate which makes it difficult for
social media platforms to take quick action. Given how rapidly fake news and
disinformation can spread on online social media platforms, countries and governments are also concerned about disinformation and propaganda influencing

Disinformation Mitigation - An Overview

11

elections. For example, tech giants have been under the radar and the European Union have been putting pressure on these companies to be proactive in
regulating disinformation and fake news [39] [14]. Furthermore, to complicate
things, different countries and regions of the world have different cultures when
it comes to media, journalism and consumption of news and information.
In the United States, for example, social media platforms, including their
owners and regulators, have often been accused of holding left liberal ideologies and ‘unfairly’ silencing opposing narratives when these had false or
deceptive content. Hence, the topic of fairness and political bias comes up and
it is an issue of debate. There exists a blurred fine line between free speech,
self-expression and potentially causing harm via deceptive or partially false
information, especially during important events such as the US Presidential
elections and the coronavirus response. Some groups may feel they are being
targeted by social media censorship regulations more than others, and hence
developing disinformation mitigation policies keeping everyone’s interests in
mind raises significant fairness concerns.

4 Detecting Disinformation as a Preliminary Step Towards
Mitigation
In order to mitigate and slow down the spread of disinformation, the first step
is to accurately and efficiently identify disinformation. Effective mitigation is
only possible if the piece of disinformation is detected early on, i.e. the spread
of the post or news article through the network is not yet significant. Given
that early and effective detection of disinformation is an indispensable step
leading up to its potential mitigation, in this section, we discuss some efforts
in different directions of disinformation detection. Ever since the role of fake
news and disinformation in the 2016 US Presidential elections became evident [47] [86], there has been an increasing amount of efforts in developing
manual and automated ways of detecting disinformation on social media. Several challenges and caveats in the process of effective and efficient detection of
disinformation, including the challenges we described in the previous section,
have been identified, along with preliminary solutions. In general, the disinformation detection task is to output a ‘true’ or ‘false’ label, given a news article
as input. Often there are other auxiliary information along with the news content text. Here we provide a brief summary on some prominent disinformation
detection approaches.
News Content and Social Context: Most earlier disinformation or deception detection approaches used hand-crafted features, often focusing on the
textual and linguistic features of the news content. Linguistic features usually
included lexical and syntactic features among others [77] [57]. Other features
were platform-dependent, for example, number of re-tweets and likes on Twitter. Gradually researchers realized that social networks hold a lot of useful
information that can be leveraged to improve detection performance. Thus,
researchers started using social context [90], user profiles [89], user engage-

12

Amrita Bhattacharjee et al.

ment [87], and relationships among news articles, readers and publishers to
detect fake news and disinformation [54].
User Comments: User comments on social media are rich sources of
information when it comes to identifying the truthfulness of an article. For
newly emerging topics and events, the ground truth is usually not available. In
such cases, user comments can provide weak social signal towards the veracity
of the news article, thus helping in early detection [91]. Such a technique
can also tackle the challenge of limited labelled data. In the case of early
detection, it may so happen that user responses or comments are unavailable
since the news article was just posted online. To deal with this situation,
authors in [80] develop a model which learns how users respond to news
articles and then generates user responses to aid the detection process. Some
recent works in disinformation or deception detection using signals from the
crowd are [67] [97] [94].
Fact-checking based methods: Fact-checking [99] based approaches of
detecting disinformation may be of two types - manual or automatic. Manual
fact-checking either relies on domain experts to fact-check and verify claims,
or crowdsource knowledge from users. Automatic fact-checking based detection approaches use structured knowledge bases such as knowledge graphs to
detect the veracity of claims and news articles. This is done by first extracting
knowledge (in the form of subject, predicate, object triples) from news articles
and then comparing against a pre-existing knowledge graph. Ciampaglia et
al. [51] used transitive closure in weighted knowledge graphs to derive a value
for semantic proximity. A simpler web search based automated fact-checking
approach have been proposed by Karadzhov et al. [65]. As explained in [59],
automated fact-checking has several challenges - one of the significant ones
being the lack of a large-scale supporting knowledge base or knowledge graph.
Explainable and Cross-Domain detection: Some new and interesting areas in fake news detection research are explainable detection and crossdomain detection. With interpretability becoming an important topic of conversation in AI research, explanations behind why a news article was classified
as ‘fake’ is an interesting aspect to look into. Efforts in this direction include
[103] [85] [82]. Understanding why an article is labelled as fake can help design
better detection algorithms. As we mentioned in the previous section, effective cross-domain detection is a potential solution to the labelled data scarcity
problem. As shown in [63], naive fake news detectors do not perform well
across domains and hence, there is a lot of scope for research in this area.

5 Computational Methods for Mitigating Disinformation
Given the unpredictable and noisy online information ecosystem, and the ever
increasing ease of creating and disseminating content, mitigating the spread
of false content and claims has emerged to be another important task. Some
studies have shown that fake news and disinformation spread faster and wider
than true news [100] [18]. This is mostly because disinformation generates

Disinformation Mitigation - An Overview

13

more extreme reactions in readers, thus making then engage with the content
more. Since social media ranking algorithms are designed in such a way so as
to promote posts and content that get more user engagement, disinformation
posts gain more visibility due to high user activity. Algorithms that govern the
visibility of articles and posts on social media, including recommendation and
ranking based algorithms need to be designed with this in mind. Furthermore,
effective mitigation of disinformation campaigns on social media also requires
early detection of such disinformation. In this section, we discuss a few of the
computational approaches in which mitigation of disinformation is addressed.

5.1 Network Intervention
A social media platform may be represented as a network where nodes are
the users (i.e. accounts on that social media platform) and edges are the connections between them (such as ‘friendship’). Disinformation on social media
spreads in the form of a cascade, where the ‘infection’ is initiated by a few
nodes that created or posted the false content, and then it is propagated by
other nodes that get influenced by this piece of disinformation. If the disinformation is capable of influencing the exposed users effectively, such a spread
can grow exponentially and hence, early detection and mitigation of the disinformation campaign is of paramount importance. One way of slowing down the
spread of the disinformation cascade is to launch a counter cascade, for example, one that contains the fact-checked version of that disinformation article.
Such a problem is often referred to as the influence limitation or influence minimization problem. In the influence limitation problem, the objective is to find
a (or the smallest) set of nodes in the network where a counter-cascade could
be initiated such that the effect of the original cascade could be minimized.
Although finding the optimal set of seed nodes for counter campaign propagation has been proven to be NP-hard, however, approximation algorithms
have been developed for some variants of the problem [50] [75] [71]. In case
of disinformation mitigation, the objective of this problem is to ‘immunize’ or
inoculate as many nodes as quickly as possible, in order to minimize the harm
caused by the spread of the piece of disinformation. Budak et al. [50] have
described a greedy approach for a variant of the influence limitation problem,
using carefully chosen heuristics, to arrive to an approximate solution to the
problem. This proposed algorithm works on a discrete time model as opposed
to a continuous time model used in [71]. While most previous works considered only two cascades, [96] tackled the problem of multiple cascades and also
introduced the concept of ‘cascade priority’ whereby a node in the network
has varying probabilities of getting influenced by each of the cascades. In a
recent work [61], the authors have considered the problem of cross-platform
misinformation propagation and proposed a greedy approach based on survival
theory to solve this issue.

14

Amrita Bhattacharjee et al.

5.2 Content Flagging
Social media platforms that host user-generated content often give users the
option to ‘flag’ or ‘report’ some content, if it is offensive, harmful and/or is
false information etc. In platforms where there is significant freedom in generating and disseminating content, a centralized system of moderating such
user-generated content is extremely challenging, if not impossible, due to the
sheer volume of the content and also the domain-knowledge that might be necessary to debunk false claims. Hence, most social media platforms leverage the
readers and the audience to maintain the quality of content online, whereby a
user can raise ‘flag’ the content if it violates the community guidelines of that
platform. In the specific case of flagging content as ‘false information’, these
platforms often use 3rd party fact-checkers to judge the veracity of the claim
and take required action. If the claim is proven to be false or harmful, the
social media platform may remove the post entirely or reduce its reach. Some
platforms also notify users of the fact-check performed and provides context
as to why sharing/interacting with the post is not ideal [74] [72]. While some
political and non-political organizations have been putting pressure on social
media platforms to take proactive steps in controlling the spread of illegal content online [40], several other organizations have reprimanded the same social
platforms when disinformation and false claims were flagged and removed from
the platform, on the grounds that such false claims and opinions posted by
people are not ‘illegal’, and this kind of censorship by social media platforms
threatens free speech [41]. Hence, content flagging and removal for mitigating disinformation is quite challenging and there are a lot of opportunities for
developing better solutions.

5.3 User Susceptibility
Another direction explored in mitigation of disinformation is analyzing user behavior and identifying susceptible or gullible users, i.e. users who are most vulnerable to consumption and propagation of fake news or false claims [81] [84].
Authors in [81] attempted to identify groups of vulnerable users on Twitter
when it comes to fake news consumption, using an unsupervised approach
to cluster users into 3 categories - good, malicious, and vulnerable/naive based on user, content and network features. Recent studies have shown that
certain groups of users, for example, older people [34], might be more susceptible to fake news and disinformation. Such identification of vulnerable groups
may help in performing targeted fact-checking, or using specialized but ethical ways of delivering the true information or facts to these groups, after a
previous exposure to fake news.

Disinformation Mitigation - An Overview

15

6 An Interdisciplinary Look at Disinformation
As we mentioned in Section 3, disinformation research requires a significant
amount of interdisciplinary collaborations. Methods and models in computer
science that deploy artificial intelligence and machine learning to detect disinformation and slow the spread of disinformation are only a small part of
the solution. Such techniques need to be augmented by knowledge from other
domains. A lot can be learned from domains such as social science, psychology and cognitive science regarding how and why rational human beings fall
prey to fake news and disinformation, what psychological processes trigger
responses to and engagement with disinformation, etc. Although researchers
have started exploring such ideas, there is a lot more left to understand and
learn from, and such auxiliary knowledge from other disciplines can help in
the design and development of better detection and mitigation solutions.
Furthermore, the problem of disinformation is not just a technical one.
Public outreach programs with convincing messages and campaigns need to
be organized to address the issue of disinformation, so that the lay person can
make better decisions with respect to the quality of news and information they
consume.

6.1 Understanding and Identifying Disinformation Interdisciplinarily
In order to tackle the problem of online disinformation and deception at the
grass roots level, several studies have been performed to understand the cognitive and psychological processes at play when an individual interacts with
and believes in a piece of fake news, and in general, what makes humans
vulnerable to disinformation. As described in [29], when presented with too
much information and resources, the human mind naturally tends to use cognitive shortcuts or ‘heuristics’ in order to make decisions, instead of critically
analyzing each and every piece of information. Such is the case in today’s online information ecosystem where people are inundated with news, opinions,
viewpoints, that are often extremely contradictory, thus making the task of
assessing the veracity of such articles increasingly difficult. The idea of cognitive shortcuts and heuristics in the context of credibility evaluation has been
mentioned in [73], and the authors explain how the use of such heuristics
may often lead humans to make incorrect judgements and decisions. Among
the heuristics described by authors in [73], some are reputation heuristic users judge the credibility of a new article by how reputed or recognized the
news source is, endorsement heuristic - users tend to believe a news article to
be credible if a known and trusted acquaintance shared and hence endorsed
the piece of news, and self-confirmation heuristic or confirmation bias - users
tend to gravitate towards news and information sources that align with their
pre-existing beliefs.
Another interesting idea behind vulnerability to disinformation is that of
Dual Process Theory [78], according to which, the human mind has two ways of

16

Amrita Bhattacharjee et al.

thinking and processing information - the first is an automatic (unconscious)
process that requires little effort, and the second is an analytic (conscious)
process that requires significant amount of effort. Quick, automatic processing, similar to using heuristics for information processing may also mislead
humans and make them misinterpret the credibility of news. Motivated reasoning, that is reasoning and rationalizing in such a way so as to align with
prior beliefs, is also another reason behind the gullibility of humans to disinformation [36] [45]. Nowadays social media platforms often use 3rd party factcheckers to verify the truthfulness of news articles and claims being shared on
the platforms. If a disinformation post is fact-checked and found to be false,
several social media platforms, such as Twitter and Facebook, usually display
a tag or a disclaimer, mentioning the result of the fact-check [74]. However,
studies and research have shown that such rebuttals are ineffective [56] and
may even trigger readers of the disinformation piece to strengthen their beliefs
in the original false narrative. Some researchers [52] [76] explain how rebuttals
and retractions of misinformation can even reinforce people’s belief in the misinformation by the ‘backfire’ effect. In a less extreme scenario, such corrective
retractions may not have any effect on the reader due to the lag between the
exposure to the original piece of misinformation and the retraction, since by
that time the readers already have established mental models to make sense
of that piece of misinformation [95]. To complicate the matter even further,
experiments have shown that exposure to ‘disputed’ flags or fact-checks on
disinformation headlines seems to increase the perceived truthfulness of news
headlines that have not been fact-checked and hence do not have a visible
flag [79]. Hence, disinformation mitigation is an extremely challenging task
and requires knowledge and concepts from various disciplines and domains.
Knowledge from cognitive science has also been borrowed and used in disinformation detection. An example is [69], where the authors have tried to
detect disinformation by using cues such as consistency and coherency of the
message, source credibility and general acceptability.

6.2 Interdisciplinary Efforts in Disinformation Mitigation
Apart from the technical approaches towards disinformation mitigation described above, there have also been efforts in other domains, such as, psychology and cognitive sciences. Fake news and disinformation is a global phenomenon and with today’s ease of online content generation and propagation,
it is becoming more and more important to spread awareness regarding the
prevalence of fake news and how to best avoid it. With around 4.57 billion
active internet users worldwide(as of July 2020) [23], there is a growing need
to accept that false information and deception is here to stay, and to spread
awareness among the public about the existence of fake news and how to spot
fake news. Furthermore, several studies and surveys have shown that humans
are extremely susceptible to fake news; even individuals who claim to be immune to fake news [2] [44]. A 2019 study conducted by the Stanford History

Disinformation Mitigation - An Overview

17

Education Group [38] showed that students were unable to assess the credibility of online sources and were susceptible to misinformation. A shocking
90% of the students failed on 4 out of the 6 tasks they were asked to perform, thus demonstrating the need for media literacy. Many organizations,
academics and non-profits have contributed to efforts in this aspect and here
we mention some of the notable ones. Some organizations dedicated to promoting media literacy among the public are Media Education Lab5 , Media
Literacy Now6 , The National Association for Media Literacy Education7 and
Center for Media Literacy8 . Several journalistic standards also exist to prevent misleading or false information from being created, and also to maintain
ethics. The Trust Project9 is a consortium of around 120 news organizations
and they have developed 8 ‘Trust Indicators’ in an effort to establish trustworthiness, transparency and accountability of news sources. Their objective
is to re-establish public trust in quality news sources through these metrics
or indicators of trust and accountability. Campaigns to increase public awareness regarding prevalence of fake news and pointers on how to spot fake news
are conducted in many countries [6] [22]. Similar campaigns are also being included in the curriculum of schools to educate children from a young age about
how to interact with online media [32] [37] [26] [30]. Another media literacy
initiative is Drog [21]: apart from educational programs and campaigns, they
have also designed a game that helps the player build psychological resistance
to disinformation by learning how disinformation is created and disseminated
[83].

7 Conclusion and Future Work
In this paper we gave a brief overview of the current situation of disinformation research, especially disinformation on online social media platforms. We
discussed the various challenges in disinformation detection and mentioned
some of the ways researchers have tackled such challenges. We then focused
on disinformation mitigation efforts, both from a computer science and an
interdisciplinary point of view.
As we mentioned previously, much of the disinformation research landscape is under-explored and there is immense scope for future work. With
recent advancements in large, highly expressive language models and automatic text generation capabilities, large-scale disinformation attacks seem to
be a potential threat that researchers should prepare for. OpenAI’s GPT-2 and
GPT-3, that came out very recently, are extremely powerful. Malicious agents
can fine-tune such models to automate the generation of false or misleading
news articles [35] [3]. Given how human-like the generation usually is, readers
5
6
7
8
9

https://mediaeducationlab.com/
https://medialiteracynow.org/
https://namle.net/
http://www.medialit.org/
https://thetrustproject.org/

18

Amrita Bhattacharjee et al.

may get easily convinced that the machine-generated article is a legitimate
news article. When combined with fabricated images, videos and Deepfakes,
this presents an added dimension of difficulty to designing defense methods.
Defending against such attacks is a promising new area of research that needs
focus at this moment.
Furthermore, more interdisciplinary research in the mitigation of disinformation is necessary. Although there are independent works on human gullibility and susceptibility to disinformation, these concepts can be utilized in
computer science to design better recommendation or ranking algorithms to
rank news articles on social media platforms. Current recommendation and
ranking algorithms on social media platforms tend to selectively display news
stories to users that align with their pre-existing beliefs, thus exacerbating the
problem of echo chambers, increasing polarization among the population and
ultimately making the problem of disinformation mitigation even more challenging. Hence, there is a need to design better algorithms to ensure the users
are exposed to a fine balance between desirable news articles and articles with
opposing viewpoints.

References
1. 14 coronavirus myths busted by science.
https://www.livescience.com/
coronavirus-myths.html. Accessed: 2020-09-20
2. 4 in 5 Singaporeans confident in spotting fake news but 90 per cent wrong
when put to the test: Survey.
https://www.statista.com/statistics/617136/
digital-population-worldwide/. Accessed: 2020-09-17
3. A fake blog written by AI shot to the top of Hacker News after people thought it
was real — here’s how a college student made it. https://www.businessinsider.com/
fake-ai-generated-gpt3-blog-hacker-news-2020-8. Accessed: 2020-09-23
4. A look at the Americans who believe there is some truth to the conspiracy theory
that COVID-19 was planned. https://www.pewresearch.org/fact-tank/2020/07/24/
a-look-at-the-americans-who-believe-there-is-some-truth-to-the-conspiracy-theory-that-covid-19-was-planned/.
Accessed: 2020-07-29
5. Attorney General James Orders Alex Jones to Stop Selling Fake
Coronavirus
Treatments.
https://ag.ny.gov/press-release/2020/
attorney-general-james-orders-alex-jones-stop-selling-fake-coronavirus-treatments.
Accessed: 2020-07-25
6. Be media smart: New public awareness campaign launched. https://www.webwise.ie/
trending/be-media-smart-new-public-awareness-campaign-launched/. Accessed:
2020-09-17
7. Beware of Robocalls, Texts and Emails Promising COVID-19 Cures or Stimulus Payments. https://www.aarp.org/money/scams-fraud/info-2020/coronavirus.
html. Accessed: 2020-09-21
8. Bleach touted as ‘miracle cure’ for COVID being sold on Amazon . https://
www.theguardian.com/world/2020/sep/19/bleach-miracle-cure-amazon-covid. Accessed: 2020-09-20
9. Coronavirus
(COVID-19)
update:
FDA
revokes
emergency
use
authorization
for
chloroquine
and
hydroxychloroquine.
https://www.fda.gov/news-events/press-announcements/
coronavirus-covid-19-update-fda-revokes-emergency-use-authorization-chloroquine-and.
Accessed: 2020-09-22
10. Coronavirus
deniers
and
hoaxers
persist
despite
dire
warnings,
claiming
‘it’s
mass
hysteria’.
https://www.washingtonpost.

Disinformation Mitigation - An Overview

11.

12.

13.

14.

15.

16.

17.
18.

19.

20.

21.

22.

23.
24.
25.
26.

27.

28.
29.

19

com/national/coronavirus-deniers-outbreak-hoax/2020/03/19/
46bc5e46-6872-11ea-b313-df458622c2cc_story.html. Accessed: 2020-09-20
Coronavirus:
medical
experts
denounce
trump’s
theory
of
‘disinfectant
injection’.
https://www.theguardian.com/world/2020/apr/23/
trump-coronavirus-treatment-disinfectant. Accessed: 2020-09-20
COVID-19 has fueled more than 2,000 rumors and conspiracy theories. https://
www.livescience.com/covid-19-rumors-conspiracy-theories-infodemic.html. Accessed: 2020-09-20
Drinking
bleach,
hydroxychloroquine
and
other
COVID-19
myths.
https://www.ems1.com/coronavirus-covid-19/articles/
drinking-bleach-hydroxychloroquine-and-other-covid-19-myths-4cpZ1a0uImeCph70/.
Accessed: 2020-09-20
Emmanuel
Macron
promises
ban
on
fake
news
during
elections.
https://www.theguardian.com/world/2018/jan/03/
emmanuel-macron-ban-fake-news-french-president. Accessed: 2020-09-23
Executive Order on Preventing Online Censorship.
https://www.whitehouse.
gov/presidential-actions/executive-order-preventing-online-censorship/. Accessed: 2020-09-23
Facebook and other companies are removing viral ‘plandemic’ conspiracy
video.
https://www.washingtonpost.com/technology/2020/05/07/
plandemic-youtube-facebook-vimeo-remove/. Accessed: 2020-09-15
Facebook deletes Brazil president’s coronavirus misinfo post. https://techcrunch.
com/2020/03/30/facebook-removes-bolsonaro-video/. Accessed: 2020-08-31
Fake
news:
Lies
spread
faster
on
social
media
than
truth
does.
https://www.nbcnews.com/health/health-news/
fake-news-lies-spread-faster-social-media-truth-does-n854896.
Accessed:
2020-09-15
FBI
warns
of
potential
fraud
in
antibody
testing
for
COVID19.
https://www.fbi.gov/news/pressrel/press-releases/
fbi-warns-of-potential-fraud-in-antibody-testing-for-covid-19.
Accessed:
2020-09-21
Fearing coronavirus, arizona man dies after taking a form of chloroquine used to treat aquariums.
https://www.cnn.com/2020/03/23/health/
arizona-coronavirus-chloroquine-death/index.html. Accessed: 2020-09-20
Fight disinformation through gaming and education: the drog media literacy initiative.
https://www.disinfo.eu/outreach/our-webinars/
fight-disinformation-through-gaming-and-education-the-drog-media-literacy-initiative/.
Accessed: 2020-09-23
Finland is winning the war on fake news. what it’s learned may be crucial
to western democracy.
https://edition.cnn.com/interactive/2019/05/europe/
finland-fake-news-intl/. Accessed: 2020-10-01
Global digital population as of july 2020. https://www.statista.com/statistics/
617136/digital-population-worldwide/. Accessed: 2020-09-17
Health insurance scams.
https://www.aarp.org/money/scams-fraud/info-2019/
health-insurance.html. Accessed: 2020-09-21
How Bill Gates became the voodoo doll of Covid conspiracies. https://www.bbc.com/
news/technology-52833706. Accessed: 2020-09-20
How finland is fighting fake news - in the classroom. https://www.weforum.org/
agenda/2019/05/how-finland-is-fighting-fake-news-in-the-classroom/.
Accessed: 2020-09-17
How
the
‘Plandemic’
Movie
and
Its
Falsehoods
Spread
Widely
Online.
https://www.nytimes.com/2020/05/20/technology/
plandemic-movie-youtube-facebook-coronavirus.html. Accessed: 2020-08-31
How the ‘plandemic’ video hoax went viral. https://www.theverge.com/2020/5/12/
21254184/how-plandemic-went-viral-facebook-youtube. Accessed: 2020-09-15
How your brain tricks you into believing fake news. https://time.com/5362183/
the-real-fake-news-crisis/. Accessed: 2020-09-17

20

Amrita Bhattacharjee et al.

30. In
the
age
of
fake
news,
here’s
how
schools
are
teaching
kids
to
think
like
fact-checkers.
https://qz.com/1533747/
in-the-age-of-fake-news-heres-how-schools-are-teaching-kids-to-think-like-fact-checkers/.
Accessed: 2020-09-17
31. Internet troll. https://en.wikipedia.org/wiki/Internet_troll. Accessed: 2020-0922
32. ‘media literacy is literacy’: Here’s how educators and lawmakers are working
to set students up for success online. https://www.the74million.org/article/
media-literacy-is-literacy/. Accessed: 2020-09-17
33. No, Face Masks Can’t Cause CO2 Poisoning.
https://www.healthline.com/
health-news/no-face-masks-cant-cause-co2-poisoning. Accessed: 2020-09-20
34. Older People More Susceptible to Fake News, More Likely to Share
It.
https://www.usnews.com/news/politics/articles/2019-01-09/
study-older-people-are-more-susceptible-to-fake-news-more-likely-to-share-it.
Accessed: 2020-08-31
35. OpenAI’s
‘dangerous’
AI
text
generator
is
out:
People
find
GPT-2’s
words
‘convincing’.
https://www.zdnet.com/article/
openais-dangerous-ai-text-generator-is-out-people-find-gpt-2s-words-convincing/.
Accessed: 2020-09-23
36. The psychology of misinformation: Why we’re vulnerable. https://firstdraftnews.
org/latest/the-psychology-of-misinformation-why-were-vulnerable/. Accessed:
2020-09-20
37. Spread
of
fake
news
prompts
literacy
efforts
in
schools.
https://www.pbs.org/newshour/education/
spread-of-fake-news-prompts-literacy-efforts-in-schools.
Accessed: 202009-17
38. Students’
Civic
Online
Reasoning.
https://sheg.stanford.edu/
students-civic-online-reasoning. Accessed: 2020-09-23
39. Tech
firms
could
face
new
EU
regulations
over
fake
news.
https://www.theguardian.com/media/2018/apr/24/
eu-to-warn-social-media-firms-over-fake-news-and-data-mining.
Accessed:
2020-09-23
40. Tech
giants
pressured
to
auto-flag
“illegal”
content
in
europe.
https://techcrunch.com/2017/09/28/
tech-giants-pressured-to-auto-flag-illegal-content-in-europe/.
Accessed:
2020-08-31
41. Trump responds to twitter’s fact-check by targeting social-media protections.
https://www.technologyreview.com/2020/05/28/1002357/
trump-signs-social-media-order-after-twitter-fact-check/. Accessed: 2020-0831
42. US
legislator,
David
Cicilline,
joins
international
push
to
interrogate
platform
power.
https://techcrunch.com/2019/08/15/
us-legislator-david-cicilline-joins-international-push-to-interrogate-platform-power/.
Accessed: 2020-09-23
43. Webopedia: Fake news definition. https://www.webopedia.com/TERM/F/fake-news.
html. Accessed: 2020-09-21
44. Why are we still falling for fake news? https://www.psychologytoday.com/us/blog/
raising-humans-in-digital-world/201905/why-are-we-still-falling-fake-news.
Accessed: 2020-09-17
45. Why Do People Fall for Fake News? https://www.nytimes.com/2019/01/19/opinion/
sunday/fake-news.html. Accessed: 2020-09-20
46. World health organization. ”novel coronavirus (2019-nCoV): Situation report
- 13.” (2020).
https://www.who.int/docs/default-source/coronaviruse/
situation-reports/20200202-sitrep-13-ncov-v3.pdf. Accessed: 2020-08-31
47. Allcott, H., Gentzkow, M.: Social media and fake news in the 2016 election. Journal
of economic perspectives 31(2), 211–36 (2017)
48. Asr, F.T., Taboada, M.: The data challenge in misinformation detection: Source reputation vs. content veracity. In: Proceedings of the First Workshop on Fact Extraction
and VERification (FEVER), pp. 10–15 (2018)

Disinformation Mitigation - An Overview

21

49. Bale, J.M.: Political paranoia v. political realism: On distinguishing between bogus
conspiracy theories and genuine conspiratorial politics. Patterns of Prejudice 41(1),
45–60 (2007)
50. Budak, C., Agrawal, D., El Abbadi, A.: Limiting the spread of misinformation in social
networks. In: Proceedings of the 20th international conference on World wide web, pp.
665–674 (2011)
51. Ciampaglia, G.L., Shiralkar, P., Rocha, L.M., Bollen, J., Menczer, F., Flammini, A.:
Computational fact checking from knowledge networks. PloS one 10(6), e0128193
(2015)
52. Cook, J., Ecker, U., Lewandowsky, S.: Misinformation and how to correct it. Emerging trends in the social and behavioral sciences: An interdisciplinary, searchable, and
linkable resource pp. 1–17 (2015)
53. Cui, L., Wang, S., Lee, D.: Same: sentiment-aware multi-modal embedding for detecting fake news. In: Proceedings of the 2019 IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining, pp. 41–48 (2019)
54. Della Vedova, M.L., Tacchini, E., Moret, S., Ballarin, G., DiPierro, M., de Alfaro,
L.: Automatic online fake news detection combining content and social signals. In:
2018 22nd Conference of Open Innovations Association (FRUCT), pp. 272–279. IEEE
(2018)
55. Dhamani, N., Azunre, P., Gleason, J.L., Corcoran, C., Honke, G., Kramer, S., Morgan,
J.: Using Deep Networks and Transfer Learning to Address Disinformation. arXiv
preprint arXiv:1905.10412 (2019)
56. Ecker, U.K.: Why rebuttals may not work: the psychology of misinformation. Media
Asia 44(2), 79–87 (2017)
57. Feng, S., Banerjee, R., Choi, Y.: Syntactic stylometry for deception detection. In: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics
(Volume 2: Short Papers), pp. 171–175 (2012)
58. Gangware, C., Nemr, W.: Weapons of mass distraction: Foreign state-sponsored disinformation in the digital age (2019)
59. Graves, D.: Understanding the promise and limits of automated fact-checking (2018)
60. Guo, M., Xu, Z., Liu, L., Guo, M., Zhang, Y.: An Adaptive Deep Transfer Learning Model for Rumor Detection without Sufficient Identified Rumors. Mathematical
Problems in Engineering 2020 (2020)
61. Hosni, A.I.E., Li, K., Ahmad, S.: Minimizing rumor influence in multiplex online social
networks based on human individual and social behaviors. Information Sciences 512,
1458–1480 (2020)
62. Jackson, D.: Issue brief: How disinformation impacts politics and publics. National
Endowment for Democracy (2018)
63. Janicka, M., Pszona, M., Wawer, A.: Cross-Domain Failures of Fake News Detection.
Computación y Sistemas 23(3) (2019)
64. Kang, S., Hwang, J., Yu, H.: Multi-modal component embedding for fake news detection. In: 2020 14th International Conference on Ubiquitous Information Management
and Communication (IMCOM), pp. 1–6. IEEE (2020)
65. Karadzhov, G., Nakov, P., Màrquez, L., Barrón-Cedeño, A., Koychev, I.: Fully automated fact checking using external sources. arXiv preprint arXiv:1710.00341 (2017)
66. Khattar, D., Goud, J.S., Gupta, M., Varma, V.: Mvae: Multimodal variational autoencoder for fake news detection. In: The World Wide Web Conference, pp. 2915–2921
(2019)
67. Kim, J., Tabibian, B., Oh, A., Schölkopf, B., Gomez-Rodriguez, M.: Leveraging the
crowd to detect and reduce the spread of fake news and misinformation. In: Proceedings
of the Eleventh ACM International Conference on Web Search and Data Mining, pp.
324–332 (2018)
68. Krishnamurthy, G., Majumder, N., Poria, S., Cambria, E.: A deep learning approach
for multimodal deception detection. arXiv preprint arXiv:1803.00344 (2018)
69. Kumar, K.K., Geethakumari, G.: Detecting misinformation in online social networks
using cognitive psychology. Human-centric Computing and Information Sciences 4(1),
1–22 (2014)

22

Amrita Bhattacharjee et al.

70. Liu, Y., Wu, Y.F.B.: Early detection of fake news on social media through propagation
path classification with recurrent and convolutional networks. In: Thirty-Second AAAI
Conference on Artificial Intelligence (2018)
71. Luo, C., Cui, K., Zheng, X., Zeng, D.: Time critical disinformation influence minimization in online social networks. In: 2014 IEEE Joint Intelligence and Security
Informatics Conference, pp. 68–74. IEEE (2014)
72. Lyons, T.: News feed fyi: Replacing disputed flags with related articles. Facebook
Newsroom (2017)
73. Metzger, M.J., Flanagin, A.J.: Credibility and trust of information in online environments: The use of cognitive heuristics. Journal of pragmatics 59, 210–220 (2013)
74. Mosseri, A.: News feed fyi: Addressing hoaxes and fake news. Facebook newsroom 15,
12 (2016)
75. Nguyen, N.P., Yan, G., Thai, M.T., Eidenbenz, S.: Containment of misinformation
spread in online social networks. In: Proceedings of the 4th Annual ACM Web Science
Conference, pp. 213–222 (2012)
76. Nyhan, B., Reifler, J.: When corrections fail: The persistence of political misperceptions. Political Behavior 32(2), 303–330 (2010)
77. Ott, M., Choi, Y., Cardie, C., Hancock, J.T.: Finding deceptive opinion spam by any
stretch of the imagination. arXiv preprint arXiv:1107.4557 (2011)
78. Pennycook, G.: A perspective on the theoretical foundation of dual process models.
Dual process theory 2, 34 (2017)
79. Pennycook, G., Bear, A., Collins, E.T., Rand, D.G.: The implied truth effect: Attaching
warnings to a subset of fake news headlines increases perceived accuracy of headlines
without warnings. Management Science (2020)
80. Qian, F., Gong, C., Sharma, K., Liu, Y.: Neural user response generator: Fake news
detection with collective user intelligence. In: IJCAI, vol. 18, pp. 3834–3840 (2018)
81. Rajabi, Z., Shehu, A., Purohit, H.: User behavior modelling for fake information mitigation on social web. In: International Conference on Social Computing, BehavioralCultural Modeling and Prediction and Behavior Representation in Modeling and Simulation, pp. 234–244. Springer (2019)
82. Reis, J.C., Correia, A., Murai, F., Veloso, A., Benevenuto, F.: Explainable machine
learning for fake news detection. In: Proceedings of the 10th ACM Conference on Web
Science, pp. 17–26 (2019)
83. Roozenbeek, J., van der Linden, S.: Fake news game confers psychological resistance
against online misinformation. Palgrave Communications 5(1), 1–10 (2019)
84. Sameki, M., Zhang, T., Ding, L., Betke, M., Gurari, D.: Crowd-o-meter: Predicting if
a person is vulnerable to believe political claims. In: HCOMP, pp. 157–166 (2017)
85. Shu, K., Cui, L., Wang, S., Lee, D., Liu, H.: defend: Explainable fake news detection.
In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 395–405 (2019)
86. Shu, K., Liu, H.: Detecting fake news on social media. Synthesis Lectures on Data
Mining and Knowledge Discovery 11(3), 1–129 (2019)
87. Shu, K., Mahudeswaran, D., Wang, S., Lee, D., Liu, H.: FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying
Fake News on Social Media. Big Data 8(3), 171–188 (2020)
88. Shu, K., Wang, S., Lee, D., Liu, H.: Disinformation, Misinformation, and Fake News
in Social Media
89. Shu, K., Wang, S., Liu, H.: Understanding user profiles on social media for fake news
detection. In: 2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), pp. 430–435. IEEE (2018)
90. Shu, K., Wang, S., Liu, H.: Beyond news contents: The role of social context for fake
news detection. In: Proceedings of the Twelfth ACM International Conference on Web
Search and Data Mining, pp. 312–320 (2019)
91. Shu, K., Zheng, G., Li, Y., Mukherjee, S., Awadallah, A.H., Ruston, S., Liu, H.: Leveraging Multi-Source Weak Social Supervision for Early Detection of Fake News. arXiv
preprint arXiv:2004.01732 (2020)
92. Singhal, S., Shah, R.R., Chakraborty, T., Kumaraguru, P., Satoh, S.: Spotfake: A
multi-modal framework for fake news detection. In: 2019 IEEE Fifth International
Conference on Multimedia Big Data (BigMM), pp. 39–47. IEEE (2019)

Disinformation Mitigation - An Overview

23

93. Sunstein, C.R.: # Republic: Divided democracy in the age of social media. Princeton
University Press (2018)
94. Tchakounté, F., Faissal, A., Atemkeng, M., Ntyam, A.: A Reliable Weighting Scheme
for the Aggregation of Crowd Intelligence to Detect Fake News. Information 11(6),
319 (2020)
95. Thorson, E.: Belief echoes: The persistent effects of corrected misinformation. Political
Communication 33(3), 460–480 (2016)
96. Tong, A., Du, D.Z., Wu, W.: On misinformation containment in online social networks.
In: Advances in neural information processing systems, pp. 341–351 (2018)
97. Tschiatschek, S., Singla, A., Gomez Rodriguez, M., Merchant, A., Krause, A.: Fake
news detection in social networks via crowd signals. In: Companion Proceedings of the
The Web Conference 2018, pp. 517–524 (2018)
98. Vicario, M.D., Quattrociocchi, W., Scala, A., Zollo, F.: Polarization and fake news:
Early warning of potential misinformation targets. ACM Transactions on the Web
(TWEB) 13(2), 1–22 (2019)
99. Vlachos, A., Riedel, S.: Fact checking: Task definition and dataset construction. In:
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational
Social Science, pp. 18–22 (2014)
100. Vosoughi, S., Roy, D., Aral, S.: The spread of true and false news online. Science
359(6380), 1146–1151 (2018)
101. Wang, Y., Ma, F., Jin, Z., Yuan, Y., Xun, G., Jha, K., Su, L., Gao, J.: Eann: Event
adversarial neural networks for multi-modal fake news detection. In: Proceedings of
the 24th acm sigkdd international conference on knowledge discovery & data mining,
pp. 849–857 (2018)
102. Wu, L., Morstatter, F., Carley, K.M., Liu, H.: Misinformation in social media: definition, manipulation, and detection. ACM SIGKDD Explorations Newsletter 21(2),
80–90 (2019)
103. Yang, F., Pentyala, S.K., Mohseni, S., Du, M., Yuan, H., Linder, R., Ragan, E.D., Ji,
S., Hu, X.: XFake: explainable fake news detector with visualizations. In: The World
Wide Web Conference, pp. 3600–3604 (2019)
104. Zhang, H., Fang, Q., Qian, S., Xu, C.: Multi-modal knowledge-aware event memory
network for social media rumor detection. In: Proceedings of the 27th ACM International Conference on Multimedia, pp. 1942–1951 (2019)
105. Zhou, X., Jain, A., Phoha, V.V., Zafarani, R.: Fake news early detection: A theorydriven model. Digital Threats: Research and Practice 1(2), 1–25 (2020)
106. Zhou, X., Wu, J., Zafarani, R.: Safe: Similarity-aware multi-modal fake news detection.
arXiv preprint arXiv:2003.04981 (2020)

