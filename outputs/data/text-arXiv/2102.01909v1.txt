HeBERT & HebEMO: a Hebrew BERT Model and a Tool for Polarity
Analysis and Emotion Recognition
Avihay Chriqui, Inbal Yahav

arXiv:2102.01909v1 [cs.CL] 3 Feb 2021

Abstract
The use of Bidirectional Encoder Representations from Transformers (BERT) models for different natural language processing (NLP) tasks, and for sentiment analysis in particular, has become
very popular in recent years and not in vain. The use of social media is being constantly on the
rise. Its impact on all areas of our lives is almost inconceivable. Researches show that social media
nowadays serves as one of the main tools where people freely express their ideas, opinions, and
emotions. During the current Covid-19 pandemic, the role of social media as a tool to resonate
opinions and emotions, became even more prominent.
This paper introduces HeBERT and HebEMO. HeBERT is a transformer-based model for modern Hebrew text. Hebrew is considered a Morphological Rich Language (MRL), with unique characteristics that pose a great challenge in developing appropriate Hebrew NLP models. Analyzing
multiple specifications of the BERT architecture, we come up with a language model that outperforms all existing Hebrew alternatives on multiple language tasks.
HebEMO is a tool that uses HeBERT to detect polarity and extract emotions from Hebrew
user-generated content (UGC), which was trained on a unique Covid-19 related dataset that we
collected and annotated for this study. Data collection and annotation followed an innovative
iterative semi-supervised process that aimed to maximize predictability. HebEMO yielded a high
performance of weighted average F1-score = 0.96 for polarity classification. Emotion detection
reached an F1-score of 0.78-0.97, with the exception of surprise, which the model failed to capture
(F1 = 0.41). These results are better than the best-reported performance, even when compared to
the English language.

Preprint submitted to XX

December 2020

1. Introduction
Sentiment analysis, also referred to as opinion mining, is probably one of the most common
tasks in natural language processing (NLP) (Liu 2012, Zhang et al. 2018). The goal of sentiment
analysis is to systematically identify what people think or feel toward entities such as products,
services, individuals, events, news articles, and topics. A typical measure of sentiment analysis,
called polarity, is the binning of the overall sentiment into the three categories of positive, neutral,
or negative.
Another prominent sentiment analysis task is emotion detection - a process for extracting finergrained emotions such as happiness, anger, and fear from human language. Plutchik (1980) defines
emotions on a wheel of four bipolar emotion pairs of joy versus sadness, anger versus fear, trust
versus disgust, and surprise versus anticipation. This wheel serves as the theoretical basis of
common automated emotion detection algorithms (Medhat et al. 2014).
Sentiments, and emotions in particular, play a vital roles in understanding individuals as they
potentially reflect beliefs, behaviours, and even mental state of a person. In accordance, many realworld applications across different disciplines deploy emotion detection tools. In marketing, for
example, emotions that are expressed on online products reviews were shown to affect to products’
virality and profitability (Chitturi et al. 2007, Ullah et al. 2016, Adamopoulos et al. 2018). In
finance, Bellstam et al. (2020) extracted sentiments from textual descriptions of firm activities by
financial analysts to measure corporate innovation. In psychology, emotions were used to detect
distress (Shapira et al. 2020) or even early warning of suicide-related emotions (Desmet and Hoste
2013). In contemporaneous research sentiments and emotions detection were used to examine public
wealth during the Covid-19 pandemic (Ahorsu et al. 2020, Pfefferbaum and North 2020).
The literature offers a considerable number of methods and models for sentiment analysis, with
a strong bias towards polarity detection. Models for emotion detection, although less common,
are also accessible to the research community in multiple languages. Emotion detection methods,
however, do not currently support the Hebrew language, and are not easily adjustable due to unique
linguistic and cultural features of this language.
Hebrew is considered a Morphological Rich Language (MRL) (Tsarfaty et al. 2010). Similar to
other MRLs, in Hebrew grammatical relations are expressed via the addition of suffixed, prefixes,

2

of affixes to the words, instead of the addition of particles. The word-order in Hebrew sentences
are rather flexible. Words often have double, triple, or more meanings, which change depending
on its context. Further, written Hebrew contains vocalization diacritics, known as niqqud “dots”,
which are missing in non-formal scripts; other Hebrew characters represent some, but not all of the
vowels. Words that are pronounced differently can by thus written in the same way.
These unique characteristics of Hebrew pose a challenge in developing appropriate Hebrew NLP
models. Architectural choices should be done with care, to ensure that the features of the model
are well represented.
This paper offers several important contributions. First, we pre-trained a language model
for modern Hebrew, called HeBERT. HeBERT is a BERT-based (Devlin et al. 2018) model that
was trained for the unsupervised fill-in-the-blank task (known as Masked Language Modeling MLM (Fedus et al. 2018)). HeBERT was trained on two large-scale Hebrew corpus - Wikipedia
and OSCAR, and was evaluated on five key NLP tasks, namely fill-in-the-blank, out-of-vocabulary
(OOV), Name Entity Recognition (NER), Part of Speech (POS), and sentiment (polarity) analysis.
In this paper we examine several architectural choices for the BERT model, which we hypothesize
to yield best performance, and compare between them.
Second, we developed HebEMO - a tool to detect sentiments, specifically - polarity and emotions,
from user-generated content (UGC). Our sentiment detectors is based on HeBERT and operates
on a document level. HebEMO achieved a high performance of weighted average F1-score = 0.96
for polarity classification. Emotion detection reached an F1-score of 0.8-0.97, with the exception
of surprise, which the model failed to capture (F1 = 0.41). This results are better then the best
reported performance, even when compared to the English language.
The data collected for this study includes UGC from three major news sites in Israel, which
were posted in response to Covid-19 related articles during 2020. The psychological literature
showed that the Covid-19 pandemic intensified the emotions among the public (Pedrosa et al.
2020), allowing us to collect a unique online discourse that is highly emotional. Comments were
selected to annotation following an innovative semi-supervised iterative labeling approach that
aimed to maximize predictability.

3

2. Background
Psychologists and psychoanalysts has long known that, despite the importance of non-verbal
behavior, words are the most natural way to externally express an inner emotional world (Ortony
et al. 1987). Respectively, theorists of emotions stress that emotional experience and its intensity
can be inferred from spoken or written language (Argaman 2010). However, theorists show that
emotions are cultural artifacts (Rosaldo et al. 1984). Consecutively, languages differ in the degree
of emotionally and the way emotions are expressed words (Wierzbicka 1994, Kövecses 2003). In
line with these cultural differences, Kövecses (2003) explained that verbalizing emotions is often
dominated the use of metaphorical and metonymic expressions, which may differ across languages.
Likewise, Kim-Prieto and Diener (2009) related expression of emotions to religion. In the middleeast emotions were shown to be closely tied to moral system of a culture, and thus to play a decisive
role in communicating between middle-east and non-middle-east cultures (Fattah and Fierke 2009).
The above discussion implies that emotion detection tools that are implemented in one language
are not easy transferable to another language, especially if these are culturally distant. The need
in language-specific model is thus apparent. In the reminder of this section we present recent
advances in sentiment analysis approaches, and emotion detection in particular. We then briefly
discuss advances in language models, with a focus on the Hebrew language and its challenges.
2.1. Sentiment Analysis
Sentiment analysis, also called opinion mining, or subjectivity analysis (Liu and Zhang 2012)
refer to the task of extracting emotional and subjective information from written text. For various
reasons, in most studies sentiment is estimated on the discrete scale of positive, neutral, or negative
sentiments. This division is often denoted polarity analysis. Many studies offered an overview of
common sentiment methods (e.g., Liu et al. 2019, Hemmatian and Sohrabi 2019, Yue et al. 2019,
Yadav and Vishwakarma 2020). We present here the main points, with an emphasis on models
that form the basis of this study.
Sentiment analysis has been studied at three granularity of text (Liu et al. 2019): Document
level, that is, classifying whether an entire document expresses a positive or negative sentiment;
Sentence level - assigning sentiment to each sentence in the document separately; and aspect level,
that is assigning sentiment to each aspects discussed in the text. The later requires a pre-processing

4

step to extract aspects from a written text. In this paper we follow a document-level approach, as
our observations mostly contain 1-3 sentences in a document.
Methods for sentiment classification can be largely categories into three approaches. The first,
and perhaps the most popular, is the lexicon-based approach. Based on the theory of emotions,
this approach uses sentiment terms to score emotions in the text. Linguistic inquiry and word count
(LIWC) for example, is a popular software that was developed to assess (among others) emotions
in text, using a psychometrically validated internal dictionary (Pennebaker et al. 2001). The main
advantage of lexicon-based approach is that it is unsupervised, meaning that it can be applied
without any training or labeled data (Yue et al. 2019). The main limitations of this approach is
that is does not account for the context of terms in the lexicon, thus overlooks complex linguistic
features such as sarcasm, ambiguity, and idioms (Liu 2012). Accordingly, its accuracy is fairly low
compared to the alternative approaches.
The second sentiment classification approach is Deep Learning (DL)-based. DL approaches
are supervised methods that are based on multiple-layer neural networks. DL-based sentiment
classification models differ by their network architecture. Among the common architectures are the
(1) Convolutional neural networks (CNNs), which transform a structured input layer (e.g., sentences
or documents represented as bag-of-words (BOW) or word embedding vectors), via convolutional
layers, to a sentiment class (Kim 2014); (2) Recursive or Recurrent neural networks (RNNs), which
handle unstructured sequential data, such as textual sentences data, and learn the relations between
them (Dong et al. 2014); and (3) Long short-term memory (LSTM), a popular variant of RNN,
which can catch long-term dependencies between date segments, in one direction (e.g., lefe to right)
or in both (denoted, biLSTM architecture) (Hochreiter and Schmidhuber 1997).
In a recent paper, Amram et al. (2018) raised the question of the relationship between the characteristics of a language and the DL architectural choices of a sentiment classifier. They analysed
this question for the morphological-rich Hebrew language (MRL). Specifically, they compared the
performance of CNN and BiLSTM architectures on a sentiment task. They assumed that the later
will implicitly capture main morphological signatures, and thus outperform the former. Interestingly, and unlike finding in English sentiment analysis (Yin et al. 2017, Acheampong et al. 2020),
they found that CNN yielded overall better performance (accuracy = 0.89) than biLSTM, even
when the latter was trained on a pre-morphological segmented inputs. As far as we know, this is
5

the only paper that developed and evaluated sentiment analysis for the Hebrew language.
The last sentiments classification method, which we adopt in this paper, is transfer learningbased approach. Transfer learning is the act of carrying knowledge gained from one problem and
apply it to another similar problem (Pan and Yang 2009). This idea in NLP is implemented via
Transformers (Tay et al. 2020). Similar to RNN, transformer is a DL learning approach to process
sequential data. Its primary advantage is its unique attention mechanism, which eliminates the need
to process data in order, and allows for parallelization Vaswani et al. (2017). With transformers, a
language is first algorithmically learnt irrespectively of the language task (e.g., sentiment analysis
task), then the language model is transferred to this tasks. This process is called fine-tuning.
One of the most common transformer models for NLP tasks is the renown Bidirectional Encoder
Representations from Transformers (BERT) model Devlin et al. (2018). Transformers model, and
BERT specifically, is widely used and gives the best results in down-the-stream tasks such as
sentiment analysis (Zampieri et al. 2019, Patwa et al. 2020).
2.2. Emotion Recognition
Emotion recognition is a sub-task in sentiment analysis that offers a finer granularity sentiment
level, compared to polarity analysis. Different psychologies offer similar, yet different, models to
distinguish between humans’ basic emotions.
Two definitions of emotions dominate the NLP literature, with no clear preference between
them (Kratzwald et al. 2018); by Ekman (1999) emotions are distinct categories, meaning that
each emotion differ from the others in important ways rather than simply their intensity. Ekman (1999) identified six cross-culture basic emotions that fit facial expressions: anger, disgust,
fear, happiness, sadness and surprise. In contrast, Plutchik (1980) stressed that emotion can be
treated as dimensional models. Specifically, he claimed that there are relations between occurrences
and intensities of basic emotions, which together bind all human emotions scale. Plutchik (1980)
defines four polar-pairs of basic emotions: joy–sadness, anger–fear, trust–disgust, and surprise–
anticipation. Combination of dyads or triads of emotions define another set of 56 emotions. For
example, envy is a combination of sadness and anger. Either way, the two approaches agree on the
set of emotions defined as ”basic” emotions.
Though common, emotion recognition is not as wide spread as polarity analysis and are considered more challenging (Acheampong et al. 2020). Among the main challenges are that emotions
6

are not implicit in a text, thus it is harder to infer them via lexicon-based approach. Yet, labeled
data is commonly not available. Further, existing datasets are rather imbalanced. Naturally, the
lack of data availability is more significant in non-English languages (Ahmad et al. 2020).
In General, models for emotion recognition are similar in architecture to polarity detection and
treated as a multi-label classification task. Recent research showed an advantage to pre-trained
bidirectional LSTMs architectures on CNN and unidirectional RNN (Acheampong et al. 2020),
and to transforms (Chatterjee et al. 2019, Zhong et al. 2019) on other DL approaches. In a recent
SemEval competition (Chatterjee et al. 2019) that included an emotion detection task for three
emotions (angry, happy, sad), transformer-based model were shown to give the best performance,
with performance ranges between F1-Score = 0.75 - 0.8, precision = 0.78 - 0.85, and recall = 0.78
- 0.85. Likewise, for the Arabic language, which is the closed MRL language to Hebrew,Antoun
et al. (2020) showed better performances of polarity analysis (improvement of 1% to 6% in accuracy)
using pre-trained Arabic BERT, then using any other architectures.
2.3. Language models
Transfer learning for polarity analysis and/ or emotion recognition assumes the existence of
a pre-trained language model, such as fastText (Joulin et al. 2016), ELMo (Embeddings from
Language Models, based on forward and backward LSTMs) (Peters et al. 2018), GPT (Generative
Pre-trained Transformer) (Radford et al. 2018), or BERT Devlin et al. (2018).
To train a language model one need to make three decision regarding the model:
1. Input representation (tokenization): what is the granularity of the tokens that are fed to the
model. Common granularity levels include characters, n-gram based sub-words, morphemebased sub-words, and full words (see Figure 1 for the differences between the approaches),
2. Architectural choices: what is the exact architecture and specifications of the neural network,
and
3. Output: what is the (unsupervised) task that the model is trained on.
The difference between the input representations manifests in the features that the language
model is able to capture, and the training complexity. Character-based representation is better
at learning word-morphology, especially for low-frequency words and MRLs (Belinkov et al. 2017,
Vania et al. 2018), but it comes with longer training time and dipper architecture, compared to
7

Figure 1: Input representation alternatives

other methods (Bojanowski et al. 2015). Word-based representation, on the other hand, treats each
word as a separate token, and thus is considered better at understanding semantics (Pota et al.
2019). With that, words that differ by prefix or suffix are considered different, therefore enforcing
the storing of a very large vocabulary. Out-of-vocabulary (OOV) tokens are not represented. The
mid-way, using sub-word, is sought to balance the two approaches, as it overcomes the OOV problem
and requires a manageable vocabulary size (Wu et al. 2016). With sub-words, words are broken into
either n-grams characters, or according to morphemes that have lingual meaning (but also higher
computational costs). Previous literature showed inconsistent improvement using morpheme-based
approach compared to n-gram based approach (Bareket and Tsarfaty 2020). Recently, Klein and
Tsarfaty (2020) showed that sub-word splitting in the multilingual BERT model (mBERT, Devlin
et al. (2018)) is sub-optimal for capturing morphological information.
For the question of architecture selection, Devlin et al. (2018) and Radford et al. (2019) showed
that for similar model size, BERT outperforms other architectures such as GPT and ELMo on
sentiment tasks.
With respect to the model output, there are two tasks on which a model can be trained. The first

8

is predict-the-future, meaning that the model is trained to predict the last token of a sentence. This
task accounts for left-to-right context only. The second is fill-in-the-blank task, where the model
is trained to fill in a missing token within a sentence. Fill-in-the-blank task therefore accounts
for full (bi-directional) sentence context, and is able to better capture the meaning of tokens,
both syntactically and semantically (Devlin et al. 2018). Recently, Levine et al. (2020) offered a
method to optimize these tasks, called Pointwise Mutual Information (PMI) masking. The authors
suggested that instead of filling a single random token, the model will be trained to fill a set of
tokens that carry mutual information.
3. HeBERT: language model
In this section we develop an unsupervised Hebrew BERT model, which we will later fine-tune
for the tasks of polarity analysis and emotion recognition. For that, we start by addressing the
three decisions related to the model: tokenization, architecture, and output, in the context of the
Hebrew language.
To recall, Hebrew is an MRL that has several important characteristics. Grammatical relations
in Hebrew are expressed via the addition of suffixed, prefixes, and affixes; Hebrew sentences are
nearly order-free; many Hebrew words have multiple meanings, which change depending on its
context; Hebrew contains vocalization diacritics that are missing in non-formal scripts, implying
that Words that are pronounced differently can by thus written in the same way.
With these features in context, we first address the last two questions of architectural choice
and model output. As discussed earlier, the literature offers evidence that BERT networks have the
ability to best capture linguistic information and phrase-level information (Jawahar et al. 2019),
a necessary requirement for MRLs. No similar literature is available for the alternative models.
Following that, we decided to train a base BERT, using the default architecture. As output task, we
use the default fill-in-the-blank BERT’s task. Fill-in-the-blank has the advantage of understanding
bi-directional context, which corresponds to the order-free property of Hebrew sentences.
With respect to the input - the granularity of the tokens - the literature on MRLs, and Hebrew
in specific, is inconclusive. Belinkov et al. (2017) and Vania et al. (2018) showed that characterbased representation, which recently became increasingly popular, is better at learning Hebrew
morphology, especially for low-frequency words. For sentiment tasks, however, Amram et al. (2018)
9

and Tsarfaty et al. (2020) showed that a word-based representation yields better predictions than
a char-based representation. With regard to sub-words representation, Klein and Tsarfaty (2020)
suggested (but have not examined) that BERT for Hebrew will benefit from using morpheme-based
sub-words, compared n-gram based sub-words. Similar argument was made for Arabic, which is
the closest language to Hebrew (Antoun et al. 2020). Antoun et al. (2020) also showed that Arabicspecific BERT model performs better than the mBERT.
To understand what causes differences in findings between different researchers, consider the
following three examples:
1. First, is the word NA’AL. NA’AL can be translated as either locked (e.g., he locked the door),
a shoe, or the the past, singular, tense of the verb wearing (a shoe). It is also often used as
a slang for stupid. The actual semantic of NA’AL in a sentence is derived from the context.
In that respect, a high text granularity (such as word-based) might be sought to better
represent Hebrew, as it is better in capturing semantic in a context. (Pota et al. 2019).
2. Next, is the word NA’ALO, which is an inflection of the word NA’AL with the suffix ”O”.
NA’ALO can refer to either ”his shoe” or ”locked it”. In that respect, a finer text granularity, such as char-based, which is better at learning morphology, might be preferred.
3. Finally, consider the splitting of the word NA’ALO. Here, a meaningful splitting would be
NA’AL-O. However, that can be only achieved with morpheme-based sub-words. In Hebrew
that can be achieved with YAP (Yet Another Parser, by More et al. (2019)). The alternative,
n-gram based sub-words, will result in additional splitting, that might have lower semantic
meaning than morpheme-based sub-words, yet higher robustness to OOV.
Given the above discussion, we hypothesize that sub-word representation, which balances semantic meaning with morphology, will best capture the features of the Hebrew language, and
will yield better performance for various language tasks. Comparing ngram-based sub-words with
morpheme-based sub-words, we expect the latter to have an advantage on unsupervised tasks that
require good ”understanding” of the language features, yet might not have advantage in downstream tasks. We further expect that architecture of BERT will compensate on the overlooked
morphology that results from higher granularity compared with the char-based approach.
To examine our hypothesis we first train and evaluate multiple small-size BERT models that
10

differ by the granularity of the input. We then choose the best performing architecture, and re-train
the model on a much larger corpus.
3.1. Comparison analysis of tokenization approaches
We examine four alternative text representations: char-based; two n-gram-based sub-word,
differ by the total vocabulary size (30K tokens Vs. 50K tokens); morpheme-based sub-word; and
word-based, which considers all words in the corpus, after trimming terms in the lowest 5th quantile
according to their term frequency (vocabulary size of over 53K tokens).
To compare between the input alternatives, we first train small-sized base-BERTs on Hebrew
Wikipedia dump1 . Our working assumption is that the performance of the small-sized BERTs
trained is monotonic with their performance when trained on a larger corpus with the same parameters, yet require significantly less resources.
We evaluate the models’ performances in 1 on two common unsupervised language tasks, and
three down stream tasks (the best results for each task are in bold):
1. Unsupervised language tasks:
(a) Fill-in-the-blank - the ability to fill in a missing token; tested on newspaper article 2 and a
fairy-tail dataset3 . The performance was measured with sequence’s perplexity (P P (W ))
- a common measure to examine the ability of the language model to evaluate the
correctness of sentences in a sample set. Perplexity of a sequence W with N tokens (W =
{w1 , w2 , ..., wn }) calculates as the exponential average log-likelihood of the sequence
P
t
(P P (W ) = exp{− N1 N
i logpθ (wi |w<i )}, where logpθ (wi |w<i ) log-likelihood of the i h
token conditioned on the preceding tokens, according to the language model).
(b) Generalizability to OOV - the ability of the language model to generalize beyond the
trained corpus (Wikipedia vocabulary), as measured by the percentage of tokens in the
testing set, for which the language model could not predict the term embedding. tested
on the corpus reported in Amram et al. (2018).
2. Down-stream classification tasks:
1
As of September 2013; Retrieved from https://u.cs.biu.ac.il/ yogo/hebwiki/. The data includes over 63 millions
words and 3.8 millions sentences.
2
Retrived from: https://www.haaretz.co.il
3
Retrieved from https://benyehuda.org/ - a volunteer-based free digital library expanding access to Hebrew literature.

11

(a) Named-entity recognition (NER) - the ability of the model to classify named entities in
text, such as persons’ names, organizations, and locations; tested on a labelled dataset
form Mordecai and Elhadad, and evaluated with F1-score (2 ×

precision×recall
precision+recall ).

(b) Part of speech (POS) - the ability of the model to classify the grammatical role that a
word or phrase plays in a sentence (e.g noun, pronoun, and verb); ested on a labelled
Israeli newspaper dataset form Sima’an et al. (2001), and evaluated with F1-score.
(c) Polarity analysis - tested on the polarity data that was collected and labelled by Amram
et al. (2018) and evaluated with F1-score.

metric
chars (1k)
sub-words (30k)
sub-words (50k)
morpheme-based
word-based

Fill-the-blank

OOVS

NER

POS

Polarity analysis

(Perplexity)

(%)

(F-1 score)

(F-1 score)

(F-1 score)

1.17
4.4
5.7
8.9
209830

∼0
∼0
∼0
0.75
50

0.74
0.79
0.79
0.92
0.86

0.93
0.95
0.95
0.88
N/A∗

0.69
0.79
0.71
0.65
0.43

Table 1: Comparison of tasks’ performance for different input alternatives
∗

POS cannot be comuted due to the high OOV percentage.

For the unsupervised tasks, we observed similar performance between the sub-words variants
(char-based and morpheme-based). Specifically, all methods were able to capture OOV tokens,
with the exception of special emojis. The performance for the fill-in-the-blank task is monotonic
with respect to the dictionary size. This is not surprising, as smaller size dictionaries leaves less
room to mistakes.
For downstream tasks, char-based (slightly) outperformed morpheme-based tokenization in the
POS task, while morpheme-based tokenization was best for NER. For the task of polarity analysis,
small-size dictionary char-based was significantly better than all other sub-word approaches. As
expected, the char-based and word-based approaches were sub-optimal in all of the supervised
tasks.
Theses results suggest that (1) there is no single representation that is optimal for the entire
set of tasks, (2) sub-words representation outperform char- and word- based, and (3) for sentiment
tasks, which is the focus of this work, a smaller-size char-based sub-words representation yields the
highest performance.
12

task
metric
HeBERT
Current SOTA
mBERT

Fill-the-blank

OOVS

NER

POS

Polarity analysis

(Perplexity)

(%)

(F-1 score)

(F-1 score)

(F-1 score)

3.24
(Not reported)
1

∼0
8%
0

0.85
0.84
0.74

0.97
(Not reported)
0.94

0.93
0.89
0.88

Table 2: HeBERT performance, compared to alternative models.

3.2. Final model
Provided the performance reported above, we next train a large-size BERT on both the Wikipedia
corpus and an OSCAR (Open Super-large Crawled ALMAnaCH coRpus) corpus - a huge multilingual corpus (Ortiz Suárez et al. 2020), on a small-size char-based sub-words dictionary. For the
Hebrew language, OCSAR contains a corpus of size 9.8 GB, including 1 billion words and over 20.8
million sentences (after de-duplicating the original data).
We used Pytorch implementation of transformers in Python (Wolf et al. 2020) to train a baseBERT network for 4 epochs, with learning rate = 5e-5, using Adam optimizer in batches of 128
sentences each. The performance of the final model is reported in Table 2, and compared to the
current state-of-the-art (SOTA) performance in Hebrew, as reported in Amram et al. (2018) and
Bareket and Tsarfaty (2020) (non BERT), and to mBERT. The best results for each task are in
bold.
The results suggests that while mBERT outperform HeBERT in unsupervised tasks, HeBERT
is best for supervised tasks, even when compared to the current SOTA. Of note, mBERT contains
only 2000 tokens in Hebrew (compared to 30K in HeBERT). The higher performance in supervised
tasks is thus not surprising, as discussed earlier.
4. HebEMO: a model for polarity analysis and emotion recognition
In this section we develop HebEMO - a model for sentiment analysis, including polarity analysis and emotion recognition. HebEMO predicts sentiments at a comment level, and is based on
HeBERT. The development of the model is based on three main efforts, namely data collection,
data annotation, and fine-tuning of HeBERT, which we describe in details in the reminder of this
section.

13

4.1. Data collection and annotation
The data collected for this study compiled from user comments that were posted during the
Covid-19 pandemic (Jan-Dec, 2020) - a highly emotional period (Pedrosa et al. 2020), in response
to Covid-19 related articles.
In 2016, the President of the State of Israel, Reuven (Rubi) Rivlin, stated that Israel’s society is composed of four equal-sized ”tribes”, namely secular, national-religious, ultra-orthodox
(”Haredi”), and Arab Steiner (2016). Each group is represented in both politics and the media.
Accordingly, we collected data from three popular Israeli newsites that represent the three Hebrewspeaking ”tribes”. Specifically, our data contains all related articles from Ynet4 , which is identified
with the secular ”tribe” (with a slight left-wing political leaning); Israel Hayom5 (translate: Israel
Today), which is identified with the national-religious (with a slight write-wing political leaning),
and Be-Hadre Haredin6 (translate: in Haredis’ Rooms) for the ultra-orthodox group.
For each article, we collected the articles text, the publish date, the section were is was published
(e.g., news, health, sport), the author, and the comments section. We excluded from the dataset
comments that did not contain Hebrew words, and comments with less than 3 words. We further
merged duplicate letters in sequence (three or more identical punctuation symbols) and removed
links and double spaces. The compiled corpus, summarized in Table 3, contained over half a million
comments on 10,794 titles in various sections.
4.2. Data annotation
We annotated a total of 4000 comments. Comments were selected to annotation following an
innovative iterative labeling approach that aimed to minimize the renown imbalance problem in
the emotion recognition literature (Acheampong et al. 2020). The process is described bellow and
illustrated in Figure 2.
Our iterative process was initialized in step 1 with a naive unsupervised lexicon-based approach.
For that, we Google-translated EmoLex: a freely-available polarity and emotions dictionaries (Mohammad and Turney 2013). EmoLex contains a list of manually collected (via crowdsourcing)
English words classified to one or more eight basic emotions and the two polarity segments. We
4

https://www.ynet.co.il/
https://www.israelhayom.co.il/
6
https://www.bhol.co.il/
5

14

source

section

Ynet

activism
article
articles
dating
digital
economy
entertainment
food
health
judaism
news
sport
vacation
wheels
article
opinion
news

Israel Hayom
Be-Hadre Haredin

#titles

#comments

3
500
4506
34
124
1486
153
41
271
63
138
71
181
32
2651
36
191

80
30053
156174
793
2067
55521
7000
4064
8112
4321
8214
363
10467
964
71372
392
805

Table 3: Description of the collected data

then used the translated dictionaries to score the entire set of lemmatized comments in our dataset.
Lemmatization was achieved with UDPipe (Straka et al. 2016).
In step 2, provided the initial sentiment score, we selected a set of 150 comments that contained
75 comments that received the highest positive polarity score, and 75 that received the highest
negative polarity score. Similarly, for each of the eight emotions we selected a set of 75 comments
in which the emotion is highly expressed, and another 75 comments in which the emotion is not
expressed. The resulted set, after removing duplicate comments comprised a total of 1500 initially
labeled comments.
We then turned to Prolific7 , a trusted online labor and research platform, to manually reannotate the 1500 comments. Each comment was annotated by at least three distinct Prolific native
Hebrew-speaking participants. Specifically, annotators were asked to rate comment’s polarity on a
symmetric 5-points scale of {strongly negative, negative, neutral, positive, strongly positive}, and
the expression of each emotion in the comment on a polar 3-point scale of {not expressed (in the
comment), expressed, strongly expressed}. The participants were given the context of the comment
(i.e., the title of the news article, on which the comment was posted). Each participant annotated
20 randomly selected comments.
The reliability of the participants’ annotations were then computed with Krippendorff’s alpha
7

https://www.prolific.co/

15

Figure 2: Iterative annotation process

(Krippendorff 1970), a measure of inter-raters’ agreement. We measured reliability independently
for each sentiment in a comment. Agreement was examined on coarser sentiment scale of polarity
= {positive, neutral, negative}, and emotion = {expressed, not expressed}. For example, if two
raters, i and j, rated the emotion ”anger” in a comment c as Lic,anger = ”expressed” and Ljc,anger
=”strongly expressed”, we computed they mutual response as ”agreement” (formally, the observed
agreement between the raters was δ(Lic,anger , Ljc,anger ) = 0); if the rates were Lic,anger = ”expressed”
(or ”strongly expressed”) and Ljc,anger = ”not expressed”, we computed their mutual response as
”disagreement” (δ(Lic,anger , Ljc,anger ) = 1).
In step 3, we trained an initial HeBERT-based sentiment (supervised) classifier (see details in
Section 4.3) on the crowd-annotated data, and predicted the polarity and emotions propensity on
the reminder of the corpus.
We then repeated steps 2 and 3 until the performance of our classifier converged. Convergence
occurred after three iterations, and a total of 4000 partially labeled comments (partially means that
the raters agreed on at least one sentiment). Tables 4 and 5 summarize the number of comments
for each sentiment (polarity and emotion, respectively), for which there was high agreement among
raters, and the percent of the comments that express this sentiment. For example, the expression/
non expression of the emotion ”Anger” was labelled in 1979 distinct comments, in which in 78% of
the comments ”Anger” is expressed, and in 22% it is not expressed.
Interestingly, though we attempted to balance the expression and non-expression of each sen16

Polarity

# labeled comments

% comments

253
55
1525

13.8%
3%
83.2%

Positive
Natural
Negative

Table 4: Summary of the polarity data

emotion

# labeled comments

% comments

1979
2115
681
1041
2342
998
698
1956

78%
83%
58%
45%
12%
59%
17%
11%

Anger
Disgust
Anticipation
Fear
Joy
Sadness
Surprise
Trust

Table 5: Summary of the emotions data

timent in our labelled data, our raters had significantly lower agreement on positive sentiments,
specifically, positive polarity, expression of happiness, surprise, and trust, and non-expression of
anger and disgust.
As theoretically suggested by Plutchik (1980), we observed high negative correlation between
emotions, and positive correlation between closely related emotions according to Plutchik’s wheel
of emotion (see Table 6).
Anger

Disgust

Anticipation

Fear

Joy

Sadness

Surprise

Trust

Anger
Disgust
Anticipation
Fear
Joy
Sadness
Surprise
Trust

1.00
0.46
0.10
0.15
0.25
0.21
0.06
0.27

1.00
0.09
0.11
0.27
0.16
0.04
0.31

1.00
0.14
0.12
0.13
0.10
0.11

1.00
0.11
0.28
0.15
0.07

1.00
0.12
0.05
0.41

1.00
0.12
0.08

1.00
0.07

1.00

Polarity

0.47

0.44

0.11

0.09

0.36

0.14

0.05

0.40

Table 6: Pearson score for correlation among emotions by online taggers

The final classification model was denoted HebEMO.

17

Polarity

1.00

4.3. Fine-tuning of HeBERT: the classification model
Our classification algorithm was modelled via fine-tuning HeBERT for document level classification task. Prediction probabilities were computed with a softmax activation function. We
treated the polarity task as a multinomial problem with three classes (positive, neutral, negative);
Emotions are modelled as independent dichotomous classification tasks (expressed, not expressed),
as they can co-exist in each comment. Attempts to merge emotion pairs (e.g., joy-sad) into a single
classification pairs yielded lower performance.
To train and evaluate our model, We randomly partitioned the corpus into training (70%),
validation (15%), and test (15%) sets. In order to avoid data leakage, the tokenization process (in
HeBERT) was not trained on the UGC dataset. We repeated this process following a bootstrap
approach with 50 samples (each generated a different data partition) and examined the stability of
our results.
5. Results
We applied HebEMO to our annotated dataset and examined its performance, as measured
by precision, recall, F1 score and overall accuracy of the expressed sentiment. Table 7 presents
the performance of our model on the polarity task, and Table 8 presents the performance for
emotion recognition. The weighted average performance across all sentiments is F1-score = 0.931,
and overall accuracy = 0.91. With the exception of the emotion ”surprise”, the performance of
the model ranges between F1-score and accuracy of 0.78-0.97. These performances, as far as we
know, exceed the state-of-the-art works for user-generated content emotion recognition in English
(Ghanbari-Adivi and Mosleh 2019, Mohammad et al. 2018).
The emotion ”surprise” is known to be hard to detect. As mentioned in Zhou et al. (2020),
the best reported F1-Score for this emotion in English was found to be as low as 0.19 (Mohammad
et al. 2018). In our dataset, the amount of labelled data for the pair anticipation-surprise was also
the lowest among all emotions pairs (see Table 5), implying the this pair is a challenging labeling
task even for human annotators.
Next, we re-trained HebEMO on the polarity data reported by Amram et al. (2018). Amram
et al. (2018) collected comments that were written in response to official tweets posted by the Israeli
president, Mr. Reuven Rivlin between June and August, 2014 (a total of 12,804 Hebrew comments).
18

positive
natural
negative
accuracy

precision

recall

f1-score

0.96
0.83
0.97

0.92
0.56
0.99

0.94
0.67
0.98
0.97

Table 7: HebEMO performance on polarity task in the UGC data

Anger
Disgust
Anticipation
Fear
Joy
Sadness
Surprise
Trust

F1

Precision

Recall

Accuracy

0.97
0.96
0.85
0.80
0.88
0.84
0.41
0.78

0.97
0.97
0.83
0.84
0.89
0.83
0.47
0.88

0.97
0.95
0.87
0.77
0.87
0.84
0.37
0.70

0.95
0.93
0.84
0.80
0.97
0.79
0.78
0.95

Table 8: HebEMO performance on expressed emotion in the UGC data

The comments were manually annotated as: supportive (positive), criticizing (negative), or off-topic
(neutral) comments. The authors published a partitioned dataset (training and validation) for the
benefit of comparisons between language models. The performance of our model is presented
in Table 9, along with the improvement/ deterioration from the authors’ reported performance.
The results show an overall improved performance, with the exception of of-topic precision. The
improvement is significant at the 95% confidence level.

possitive
negetive
off-topic

precision

recall

f1-score

0.95

0.96

0.95

(+.03)

(+.01)

(+.01)

0.89

0.89

0.89

(+.05)

(+.02)

(+.04)

0.70

0.56

0.62

(-.3)

(+.55)

(+.03)

0.93

accuracy

(+.03)

Table 9: The performance of HebEMO when trained on the polarity corpus reported by Amram et al. (2018))

19

6. Summary and discussion
This paper presented HeBERT - the first Hebrew BERT model and the new state-of-theart model for multiple Hebrew tasks, and HebEMO - a tool for polarity analysis and emotion
recognition from Hebrew UGC.
Although HeBERT was trained to optimize sentiment analysis, we showed that it outperforms
mBERT in a variety of supervised language tasks. This finding is consistent with the literature that
proposes that language-specific model is better than a multilingual model. HeBERT also showed
better performance than the current non-BERT SOTA for Hebrew.
For the task of extracting sentiments from UGC, we showed that a morpheme-based fed model
that aims to ”understands” features of the language was sub-optimal to a model that did not
address the language features (ngram-based sub-words). Furthermore, smaller-size dictionary was
better than the larger-size dictionary. A plausible explanation to these results is that UGC contains
unofficial language, which includes non-lexical words such as slang words and typos. Over-fitting a
model to a language in this case may overlook the unique characteristics of the unofficial language.
In future work we plan to examine the performance of HebEMO when HeBERT is trained on
a PMI masking task, rather than fill-in-the-blank.

20

References
Acheampong FA, Wenyu C, Nunoo-Mensah H (2020) Text-based emotion detection: Advances, challenges,
and opportunities. Engineering Reports e12189.
Adamopoulos P, Ghose A, Todri V (2018) The impact of user personality traits on word of mouth: Textmining social media platforms. Information Systems Research 29(3):612–640.
Ahmad Z, Jindal R, Ekbal A, Bhattachharyya P (2020) Borrow from rich cousin: transfer learning for
emotion detection using cross lingual embedding. Expert Systems with Applications 139:112851.
Ahorsu DK, Lin CY, Imani V, Saffari M, Griffiths MD, Pakpour AH (2020) The fear of covid-19 scale:
development and initial validation. International journal of mental health and addiction .
Amram A, David AB, Tsarfaty R (2018) Representations and architectures in neural sentiment analysis for
morphologically rich languages: A case study from modern hebrew. Proceedings of the 27th International Conference on Computational Linguistics, 2242–2252.
Antoun W, Baly F, Hajj H (2020) Arabert: Transformer-based model for arabic language understanding.
arXiv preprint arXiv:2003.00104 .
Argaman O (2010) Linguistic markers and emotional intensity. Journal of psycholinguistic research 39(2):89–
99.
Bareket D, Tsarfaty R (2020) Neural modeling for named entities and morphology (nemoˆ 2). arXiv preprint
arXiv:2007.15620 .
Belinkov Y, Durrani N, Dalvi F, Sajjad H, Glass J (2017) What do neural machine translation models learn
about morphology? arXiv preprint arXiv:1704.03471 .
Bellstam G, Bhagat S, Cookson JA (2020) A text-based analysis of corporate innovation. Management
Science .
Bojanowski P, Joulin A, Mikolov T (2015) Alternative structures for character-level rnns. arXiv preprint
arXiv:1511.06303 .
Chatterjee A, Narahari KN, Joshi M, Agrawal P (2019) Semeval-2019 task 3: Emocontext contextual emotion
detection in text. Proceedings of the 13th International Workshop on Semantic Evaluation, 39–48.
Chitturi R, Raghunathan R, Mahajan V (2007) Form versus function: How the intensities of specific emotions evoked in functional versus hedonic trade-offs mediate product preferences. Journal of marketing
research 44(4):702–714.
Desmet B, Hoste V (2013) Emotion detection in suicide notes. Expert Systems with Applications 40(16):6351–
6358.
Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional transformers for
language understanding. arXiv preprint arXiv:1810.04805 .
Dong L, Wei F, Tan C, Tang D, Zhou M, Xu K (2014) Adaptive recursive neural network for targetdependent twitter sentiment classification. Proceedings of the 52nd annual meeting of the association
for computational linguistics (volume 2: Short papers), 49–54.
Ekman P (1999) Basic emotions. Handbook of cognition and emotion 98(45-60):16.
Fattah K, Fierke KM (2009) A clash of emotions: The politics of humiliation and political violence in the
middle east. European journal of international relations 15(1):67–93.
Fedus W, Goodfellow I, Dai AM (2018) Maskgan: Better text generation via filling in the . arXiv preprint
arXiv:1801.07736 .
Ghanbari-Adivi F, Mosleh M (2019) Text emotion detection in social networks using a novel ensemble
classifier based on parzen tree estimator (tpe). Neural Computing and Applications 31(12):8971–8983.
Hemmatian F, Sohrabi MK (2019) A survey on classification techniques for opinion mining and sentiment
analysis. Artificial Intelligence Review 1–51.
Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural computation 9(8):1735–1780.
Jawahar G, Sagot B, Seddah D (2019) What does bert learn about the structure of language?

21

Joulin A, Grave E, Bojanowski P, Douze M, Jégou H, Mikolov T (2016) Fasttext.zip: Compressing text
classification models. arXiv preprint arXiv:1612.03651 .
Kim Y (2014) Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882 .
Kim-Prieto C, Diener E (2009) Religion as a source of variation in the experience of positive and negative
emotions. The Journal of Positive Psychology 4(6):447–460.
Klein S, Tsarfaty R (2020) Getting the## life out of living: How adequate are word-pieces for modelling
complex morphology? Proceedings of the 17th SIGMORPHON Workshop on Computational Research
in Phonetics, Phonology, and Morphology, 204–209.
Kövecses Z (2003) Metaphor and emotion: Language, culture, and body in human feeling (Cambridge University Press).
Kratzwald B, Ilić S, Kraus M, Feuerriegel S, Prendinger H (2018) Deep learning for affective computing:
Text-based emotion recognition in decision support. Decision Support Systems 115:24–35.
Krippendorff K (1970) Estimating the reliability, systematic error and random error of interval data. Educational and Psychological Measurement 30(1):61–70.
Levine Y, Lenz B, Lieber O, Abend O, Leyton-Brown K, Tennenholtz M, Shoham Y (2020) Pmi-masking:
Principled masking of correlated spans. arXiv preprint arXiv:2010.01825 .
Liu B (2012) Sentiment analysis and opinion mining. Synthesis lectures on human language technologies
5(1):1–167.
Liu B, Zhang L (2012) A survey of opinion mining and sentiment analysis. Mining text data, 415–463
(Springer).
Liu R, Shi Y, Ji C, Jia M (2019) A survey of sentiment analysis based on transfer learning. IEEE Access
7:85401–85412.
Medhat W, Hassan A, Korashy H (2014) Sentiment analysis algorithms and applications: A survey. Ain
Shams engineering journal 5(4):1093–1113.
Mohammad S, Bravo-Marquez F, Salameh M, Kiritchenko S (2018) Semeval-2018 task 1: Affect in tweets.
Proceedings of the 12th international workshop on semantic evaluation, 1–17.
Mohammad SM, Turney PD (2013) Crowdsourcing a word-emotion association lexicon 29(3):436–465.
Mordecai NB, Elhadad M (????) Hebrew named entity recognition. MONEY 81(83.93):82–49.
More A, Seker A, Basmova V, Tsarfaty R (2019) Joint transition-based models for morpho-syntactic parsing:
Parsing strategies for mrls and a case study from modern hebrew. Transactions of the Association for
Computational Linguistics 7:33–48.
Ortiz Suárez PJ, Romary L, Sagot B (2020) A monolingual approach to contextualized word embeddings for mid-resource languages. Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, 1703–1714 (Online: Association for Computational Linguistics), URL
https://www.aclweb.org/anthology/2020.acl-main.156.
Ortony A, Clore GL, Foss MA (1987) The referential structure of the affective lexicon. Cognitive science
11(3):341–364.
Pan SJ, Yang Q (2009) A survey on transfer learning. IEEE Transactions on knowledge and data engineering
22(10):1345–1359.
Patwa P, Aguilar G, Kar S, Pandey S, PYKL S, Gambäck B, Chakraborty T, Solorio T, Das A (2020)
Semeval-2020 task 9: Overview of sentiment analysis of code-mixed tweets. arXiv e-prints arXiv–2008.
Pedrosa AL, Bitencourt L, Fróes ACF, Cazumbá MLB, Campos RGB, de Brito SBCS, e Silva ACS (2020)
Emotional, behavioral, and psychological impact of the covid-19 pandemic. Frontiers in psychology 11.
Pennebaker JW, Francis ME, Booth RJ (2001) Linguistic inquiry and word count: Liwc 2001. Mahway:
Lawrence Erlbaum Associates 71(2001):2001.
Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L (2018) Deep contextualized
word representations. arXiv preprint arXiv:1802.05365 .
Pfefferbaum B, North CS (2020) Mental health and the covid-19 pandemic. New England Journal of Medicine
.

22

Plutchik R (1980) A general psychoevolutionary theory of emotion. Theories of emotion, 3–33 (Elsevier).
Pota M, Marulli F, Esposito M, De Pietro G, Fujita H (2019) Multilingual pos tagging by a composite deep
architecture based on character-level features and on-the-fly enriched word embeddings. KnowledgeBased Systems 164:309–323.
Radford A, Narasimhan K, Salimans T, Sutskever I (2018) Improving language understanding by generative
pre-training.
Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I (2019) Language models are unsupervised
multitask learners. OpenAI blog 1(8):9.
Rosaldo MZ, Shweder RA, LeVine RA (1984) Culture theory: essays on mind, self, and emotion.
Shapira N, Lazarus G, Goldberg Y, Gilboa-Schechtman E, Tuval-Mashiach R, Juravski D, Atzil-Slonim D
(2020) Using computerized text analysis to examine associations between linguistic features and clients’
distress during psychotherapy. Journal of counseling psychology .
Sima’an K, Itai A, Winter Y, Altman A, Nativ N (2001) Building a tree-bank of modern hebrew text.
Traitement Automatique des Langues 42(2):247–380.
Steiner T (2016) President rivlin’s” four tribes” initiative: The foreign policy implications of a democratic
& inclusive process to address israel’s socio-demographic transformation .
Straka M, Hajic J, Straková J (2016) Udpipe: trainable pipeline for processing conll-u files performing
tokenization, morphological analysis, pos tagging and parsing. Proceedings of the Tenth International
Conference on Language Resources and Evaluation (LREC’16), 4290–4297.
Tay Y, Dehghani M, Bahri D, Metzler D (2020) Efficient transformers: A survey. arXiv preprint
arXiv:2009.06732 .
Tsarfaty R, Bareket D, Klein S, Seker A (2020) From spmrl to nmrl: What did we learn (and unlearn) in a
decade of parsing morphologically-rich languages (mrls)? arXiv preprint arXiv:2005.01330 .
Tsarfaty R, Seddah D, Goldberg Y, Kübler S, Versley Y, Candito M, Foster J, Rehbein I, Tounsi L (2010)
Statistical parsing of morphologically rich languages (spmrl) what, how and whither. Proceedings of the
NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, 1–12.
Ullah R, Amblee N, Kim W, Lee H (2016) From valence to emotions: Exploring the distribution of emotions
in online product reviews. Decision Support Systems 81:41–53.
Vania C, Grivas A, Lopez A (2018) What do character-level models learn about morphology? the case of
dependency parsing. arXiv preprint arXiv:1808.09180 .
Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention
is all you need. Advances in neural information processing systems, 5998–6008.
Wierzbicka A (1994) Emotion, language, and cultural scripts. .
Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, Cistac P, Rault T, Louf R, Funtowicz M, Davison
J, Shleifer S, von Platen P, Ma C, Jernite Y, Plu J, Xu C, Scao TL, Gugger S, Drame M, Lhoest
Q, Rush AM (2020) Transformers: State-of-the-art natural language processing. Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 38–
45 (Online: Association for Computational Linguistics), URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6.
Wu Y, Schuster M, Chen Z, Le QV, Norouzi M, Macherey W, Krikun M, Cao Y, Gao Q, Macherey K, et al.
(2016) Google’s neural machine translation system: Bridging the gap between human and machine
translation. arXiv preprint arXiv:1609.08144 .
Yadav A, Vishwakarma DK (2020) Sentiment analysis using deep learning architectures: a review. Artificial
Intelligence Review 53(6):4335–4385.
Yin W, Kann K, Yu M, Schütze H (2017) Comparative study of cnn and rnn for natural language processing.
arXiv preprint arXiv:1702.01923 .
Yue L, Chen W, Li X, Zuo W, Yin M (2019) A survey of sentiment analysis in social media. Knowledge and
Information Systems 1–47.
Zampieri M, Malmasi S, Nakov P, Rosenthal S, Farra N, Kumar R (2019) Semeval-2019 task 6: Identifying
and categorizing offensive language in social media (offenseval). arXiv preprint arXiv:1903.08983 .

23

Zhang L, Wang S, Liu B (2018) Deep learning for sentiment analysis: A survey. Wiley Interdisciplinary
Reviews: Data Mining and Knowledge Discovery 8(4):e1253.
Zhong P, Wang D, Miao C (2019) Knowledge-enriched transformer for emotion detection in textual conversations. arXiv preprint arXiv:1909.10681 .
Zhou D, Wu S, Wang Q, Xie J, Tu Z, Li M (2020) Emotion classification by jointly learning to lexiconize and
classify. Proceedings of the 28th International Conference on Computational Linguistics, 3235–3245.

24

