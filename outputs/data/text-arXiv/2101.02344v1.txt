DICE: D EEP S IGNIFICANCE C LUSTERING
FOR O UTCOME -AWARE S TRATIFICATION

arXiv:2101.02344v1 [cs.LG] 7 Jan 2021

Yufang Huang
Cornell University
yfhuang1992new@gmail.com

Kelly M. Axsom
Columbia University Irving Medical Center
kma2161@cumc.columbia.edu

Lakshminarayanan Subramanian
New York University
lakshmi@nyu.edu

John Lee
Weill Cornell Medicine
jrl2002@med.cornell.edu

Yiye Zhang
Cornell University
yiz2014@med.cornell.edu

January 8, 2021

A BSTRACT
We present deep significance clustering (DICE), a framework for jointly performing representation
learning and clustering for “outcome-aware” stratification. DICE is intended to generate cluster
membership that may be used to categorize a population by individual risk level for a targeted
outcome. Following the representation learning and clustering steps, we embed the objective function
in DICE with a constraint which requires a statistically significant association between the outcome
and cluster membership of learned representations. DICE further includes a neural architecture
search step to maximize both the likelihood of representation learning and outcome classification
accuracy with cluster membership as the predictor. To demonstrate its utility in medicine for patient
risk-stratification, the performance of DICE was evaluated using two datasets with different outcome
ratios extracted from real-world electronic health records. Outcomes are defined as acute kidney
injury (30.4%) among a cohort of COVID-19 patients, and discharge disposition (36.8%) among a
cohort of heart failure patients, respectively. Extensive results demonstrate that DICE has superior
performance as measured by the difference in outcome distribution across clusters, Silhouette score,
Calinski-Harabasz index, and Davies-Bouldin index for clustering, and Area under the ROC Curve
(AUC) for outcome classification compared to several baseline approaches.

1

Introduction

Representation learning (Bengio et al., 2013; Baldi and Hornik, 1989) and clustering (Xu and Wunsch, 2005) are
unsupervised algorithms whose results are driven by input features and priors generally. They are often exploratory
in nature, but in certain use cases users have a priori expectations for the outputs from representation learning and
clustering. In the latter case, having targeted self-supervision in the learning process so as to meet the expectation
of the users brings practical value for representation learning and clustering algorithms. This paper proposes deep
significance clustering (DICE), an algorithm for self-supervised, interpretable representation learning and clustering
targeting features that best stratify a population concerning specific outcomes of interest. Here outcome is a specific
result or effect that can be measured.
DICE is motivated by practical needs in healthcare to develop treatment protocols for subgroups of similar patients
with differing risk levels. The complexity and often the lack of clear clinical practice guidelines warrant the discovery
of underlying strata in the data to assist with clinical decision making. An motivating example is creating a safe triage
protocol for patients in the emergency departments (ED), where patients present with a wide array of conditions. Two
groups of patients may have similar likelihood of a safe outcome but presenting with differing clinical profiles thus
needing different interventions in the protocol. Another example, heart failure (HF) is a syndrome that impacts nearly
6 million Americans and is associated with a 50% 5-year mortality (Ziaeian and Fonarow, 2016). More than 80%

of individuals suffer from three or more comorbidities (van Deursen et al., 2014). The complexity due to frequent
comorbidity or the lack of clear guidelines warrant the discovery of patient subtypes to assist with clinical decision
making. For machine learning to assist in this context, it is insufficient to use a classification model to simply classify
each patient’s outcome. At the same time, using clustering algorithms to identify strata does not guarantee that the
stratification is meaningful with respect to the outcome of interest. Existing representation learning, clustering, and
classification algorithms serve to cluster patients or classify patients, but few is optimized to jointly achieve these goals.

Figure 1: The framework of the proposed deep significance clustering (DICE). Clustering is applied to the representation
zp . A statistical significance constraint is explicitly added to ensure the association of the clustering membership c and
outcome y, which facilitates the learning of discriminative representations zp .
DICE, a framework to learn a deep representation and cluster memberships from heterogeneous data was developed
in an effort to bridge representation learning, clustering, and classification. Its architecture is illustrated in Fig. 1.
Representation learning allows us to discover a concise representation from the heterogeneous and sparse health
data, which we use to discover latent clusters within a patient population using clustering algorithms. As a way to
provide more interpretability of the representation learning and clustering, DICE uses a combined objective function
and a constraint that requires statistically different outcome distribution across clusters. The statistical significance
is determined using models that are well-understood by clinicians such as regression while adjusting for patient
demographics. The combined objective function and constraint serve to force DICE to learn representations that lead to
clusters discriminative to the outcome of interest. Furthermore, a neural architecture search (NAS) is designed with an
alternative grid search over the number of clusters and hyper-parameters in the representation learning network. The
finalized representation and cluster memberships, which represent significantly different outcome levels, are then used
as the class labels for a multi-class classification. This is intended to allow new patients to be categorized according to
risk-level specific subgroups learned from historic data.
An important distinction between DICE and purely unsupervised, or supervised, algorithms is that DICE learns
outcome-aware clusters in an unlabeled population where the outcome-aware clusters are later used to assign risk-levels
for future unseen cohort. Previous studies (Zhang et al., 2019b) that incorporated statistical significance analyzed it
separately after the representation learning process. Our paper considers the statistical significance while performing
deep clustering as a constraint in an elaborately designed unified framework. To summarize, our approach makes the
following key contributions:
• We propose a unified objective function to achieve the joint optimization for outcome-driven representation
and clustering membership from heterogeneous health data.
• We propose an explicit constraint that forces statistical significance of the association between the cluster
membership and the outcome to drive the learning.
• We utilize a neural architecture search with an alternative grid search for hyper-parameters in the deep
significant clustering network.
We evaluated DICE on two real-world datasets collected from electronic health records (EHR) data at an academic
medical center. Extensive experiments and analyses demonstrate that the DICE obtains better performance than
2

several baseline approaches in outcome discrimination, Area under ROC Curve (AUC) for prediction, and clustering
performance metrics including Silhouette score, Calinski-Harabasz index and Davies-Bouldin index.

2

Related Work

Clustering is a fundamental topic in the exploratory data mining which can be applied to many fields, including
bioinformatics (Lopez et al., 2018), marketing (Jagabathula et al., 2018), computer vision (Yang et al., 2019) and
natural language processing (Blei et al., 2003). Due to the inefficiency of similarity measures with high-dimensional big
data, traditional clustering approaches, e.g., k-means (MacQueen et al., 1967), finite mixture model (McLachlan and
Peel, 2004; Wedel and DeSarbo, 1994) and Gaussian Mixture Models (GMM) (Bishop, 2006), generally suffer from
high computational complexity on large-scale datasets (Min et al., 2018). Also, while mixture models share similar
intention as DICE, they further have distribution assumptions on observations (Zhong and Ghosh, 2003). Jagabathula
et al. (2020) proposed a conditional gradient approach for nonparametric estimation of mixing distributions. Data
transformation approaches which map the raw data into a new feature space have been studied, including principal
component analysis (PCA) (Wold et al., 1987), kernel methods (Hofmann et al., 2008), model-based clustering (Fraley
and Raftery, 2002; Zhong and Ghosh, 2003) and spectral methods (Ng et al., 2002; Von Luxburg, 2007). However,
clustering of high-dimensional heterogeneous data is still challenging for these approaches because of inefficient data
representation.
Deep representation learning can be used to transform the data into clustering-friendly representation (Hershey et al.,
2016; Xie et al., 2016; Li et al., 2018; Yang et al., 2017; Zhang et al., 2019a). Parametric t-SNE (Van Der Maaten,
2009) uses deep neural network to parametrize the embedding of t-SNE (Maaten and Hinton, 2008) with the same
time complexity of O(n2 ), where n is the number of data points. DEC (Xie et al., 2016) further relaxes parametric
t-SNE with a centroid-based probability distribution which reduces complexity to O(nK) from tree-based t-SNE of
O(nlog(n)), where K is the number of centroids. Some approaches learn self-supervised representation (Jing and Tian,
2020; Chu and Cai, 2017; Caron et al., 2018).
Recent deep clustering approaches are learning-based and conduct inference in one-shot, consisting of two stages, i.e.,
deep representation learning followed by various clustering models. Caron et al. (2018) jointly learned the parameters
of a deep network and the cluster assignments of the resulting representation. DGG (Yang et al., 2019) further uses
gaussian mixture variational autoencoders and graph embedding to improve the clustering and data representation
abilities. Yang et al. (2017) use alternating stochastic optimization to update clustering centroids and representation
learning parameters iteratively. Different from Yang et al. (2017), DICE constructs a clustering prediction network
and updates representation learning parameters through self-supervised learning by considering cluster memberships
as pseudo-labels of the clustering prediction network. Different from Zhang et al. (2019a) adding a constraint on a
centroid-based probability distribution, DICE considers statistical significance and proposes a novel constraint added to
the cluster membership to obtain statistical significant clustering memberships.
NAS is a technique to find the network architecture with the highest performance on the validation set. Early NAS
conducted architecture optimization and network learning in a nested manner (Baker et al., 2016; Zoph and Le, 2016;
Zoph et al., 2018). These works typically used reinforcement learning or evolution algorithms to explore the architecture
search space A. A recent work decoupled architecture search and weight optimization in a one-shot NAS framework
and uses evolutionary architecture search to find candidate architectures after training (Guo et al., 2019). EfficientNet
and EfficientDet (Tan and Le, 2019; Tan et al., 2019) further used grid search to balance network depth, width, and
resolution and achieve state-of-the-art results on the ImageNet and COCO datasets respectively (Deng et al., 2009; Lin
et al., 2014). We propose an alternative grid search to optimize the number of clusters and other hyper-parameters in the
DICE framework.

3

Method

Given a dataset X = {X1 , ..., XP } with P subjects, we denote each subject as a sequence of events Xp =
n
[x1p , x2p , ..., xp p ] of length np . A multivariate feature vector xtp = [xtp,1 , xtp,2 , ..., xtp,F ] ∈ RF is the t-th instance
of subject p in sequence Xp , where F is the number of features at each timestamp. We have an outcome yp for each
subject p. Our goal is to stratify X of P subjects into K clusters while enforcing statistical significance in the association
of the cluster membership and the outcome while adjusting for relevant covariates.
3

3.1

Learning representation

The first step is to transform discrete sequences into latent continuous representations, followed by clustering and
outcome classification. The latent representation learning for each subject is performed by an LSTM autoencoder
(AE) (Sutskever et al., 2014). The AE consists of two parts, the encoder and the decoder, denoted as E and F,
n
respectively. Given the p-th input sequence Xp = (x1p , x2p , · · · , xp p ), the encoder can be formulated as zp = E(Xp ; θE )
, where zp ∈ Rd is the representation, d is the dimension of representation, and E is a LSTM network with parameter
θE (Hochreiter and Schmidhuber, 1997). We choose the last hidden state zp of LSTM to be the representation of the
input Xp . The decoder can be formulated as X̃p = F(zp ; θF ) , and F is the other LSTM network with parameter θF .
The representation learning is achieved by minimizing the reconstruction error
P
1 X
kF(E(Xp ; θE ); θF ) − Xp k2L2 ,
P p=1

min LAE =

θE ,θF

(1)

where we use L2 norm in the loss.
We employ LSTM networks as encoder and decoder for sequential data, as illustrated in Figure 1. Our framework can
also be used for one-time features (only one timestamp). Multi-layer perceptrons can used as the encoder and decoder
for one-time features.
3.2

Self-supervised learning by clustering

The obtained representations Z = {zp }P
p=1 can be employed for clustering with K clusters,
min

M,{cp }P
p=1

s.t.

Lclustering =

P
X

kzp − Mcp k22

p=1

T

1 cp = 1,

ckp

(2)

∈ {0, 1},

∀ p ∈ {1, 2, ..., P }, k ∈ {1, 2, ..., K},
k
where K is a hyper-parameter of total number of clusters to tune, cp = [c1p , ..., cK
p ], cp is the cluster membership of
cluster k, M ∈ Rd×K and the k-th columns of M is the centroid of the k-th cluster.

To enable fast inference and learn representation with the driven of outcome, we build a cluster classification network
for deep clustering based on self-supervision from cp in equation (2). We employ the clustering results {cp }P
p=1 from a
priori, such as k-means (MacQueen et al., 1967) or Gaussian Mixture Models (GMM) (Bishop, 2006), in equation (2)
as pseudo-labels, and update the parameters of the encoder E and F. The cluster membership assignment can be
formulated as a classification network,
ĉp = g(zp ; θ1 ),

min L1 = −
θ1

P X
K
X

ckp log(ĉkp ),

(3)

p=1 k=1

where ĉp = [ĉ1p , ..., ĉK
p ] is the predicted cluster membership from the cluster classification network g(·; θ1 ), θ1 is the
parameter in the cluster classification network, L1 is the negative log-likelihood loss for multi-class cluster classification.
We will show that deep clustering bridges the representation learning with the following statistical significance constraint
related to the outcome.
3.3

Outcome classification

After obtaining cluster memberships {cp }P
p=1 for K clusters, we use the cluster memberships and other confounders
such as demographics to predict the outcome, formulated as:
ŷp = g([cp , vp ]; θ2 ),
min L2 = −
θ2

P
X


yp log(ŷp ) + (1 − yp )log(1 − ŷp ) ,

(4)

p=1

where vp represents confounders to adjust in testing the significance, [·, ·] denotes the concatenation of cluster
membership feature and confounders. g(·; θ2 ) is the logistic regression for the outcome classification, and L2 is the
negative log-likelihood loss for the outcome classification.
4

Interpretability is a crucial issue that has not been resolved for the application of deep learning methods in medicine. It’s
hard to explain why the final outcome prediction is positive or negative for a test case. Using the cluster membership
from the learned representation as the input to predict the outcome allows us to infer a broad theme with a set of learned
representations, thus providing more interpretability to the deep representation learning results. Interpretability is further
enhanced by enforcing the following statistical significance constraint to the cluster membership w.r.t. the outcome.
3.4

Statistical significance constraint

The main novelty of DICE is the introduction of a statistical significance constraint to the cluster membership w.r.t. the
outcome distribution to drive the deep clustering process. After obtaining cluster memberships {cp }P
p=1 for K clusters,
we require that the association between the cluster membership and outcome be statistically significant while adjusting
for relevant confounders.
To quantify the significant difference of cluster k1 and cluster k2 (k1 6= k2 ), we use likelihood-ratio test (Hosmer and
Lemeshow, 2000) to calculate the p-value of variable ck2 when considering cluster ck1 as the reference, where ck refers
to the cluster membership belonging to cluster k, formulated as,


L2 (g([c/{ck1 , ck2 }, v]; θ2 ), y)
Gk1 ,k2 = −2 log
(5)
L2 (g([c/{ck1 }, v]; θ2 ), y)
Then we obtain the p-value from Chi-square distribution, denoted as Sk1 ,k2 . Finally, we have a matrix S ∈ RK×K
with 0 as diagonal elements, and Sk1,k2 (k1 6= k2 ) is the p-value represent the significance difference of cluster k2
corresponding to reference cluster k1 . If all the elements in S are below a predefined threshold of significance α
(equivalently, Gk1 ,k2 > αG ), we conclude that all the clusters are significantly different with each other related to
outcome y. In this paper we use α = 0.05.
In the implementation, we design a mask technique to remove variables of input c, corresponding to cluster k1 and cluster
k2 , in equation (5), then calculate the likelihood ratio Gk1 ,k2 and add significance constraint to the likelihood-ratio
Gk1 ,k2 , that is Gk1 ,k2 > αG , ∀k1 6= k2 .
3.5

Objective function

We utilize NAS to optimize the network hyper-parameters in the DICE. There are mainly two groups of network
hyper-parameters, the hyper-parameter in the clustering and the network hyper-parameters in the representation learning,
in the DICE. Basically, NAS conducts two processes iteratively. The first is the neural weights optimization of a given
network architecture, which is the network architecture with the fixed number of clusters K and hidden state dimension
d in DICE. The second is the neural architecture search process. NAS is conducted in the search phase to select a good
combination of hyper-parameters and has no direct association with the cost function of neural weights optimization.
3.5.1

Optimization of a given network architecture

We denote our network architecture as N (K, d, θ), where θ = {θE , θF , M, θ1 , θ2 } are the weights of network. The
neural weights optimization is
minL(N (K, d, θ))
θ

= minλ1 LAE + Lclustering + λ2 L1 + λ3 L2 + λ4 (αG − Gk1 ,k2 )
θ

s.t.

1T cp = 1 cp,j ∈ {0, 1},
∀p ∈ {1, ..., P }, j ∈ {1, ..., K},
k1 6= k2 , ∀k1 , k2 ∈ 1, · · · , K

(6)

where λ1 , λ2 , λ3 , and λ4 are trade-offs for LAE , L1 , L2 , and the statistical significance constraint.
We iteratively optimize deep clustering and the other components with the statistical significance constraint. We
firstly employ a priori, such as k-means (MacQueen et al., 1967), to obtain pseudo-labels for the cluster classification
network. Then we can optimize LAE for the representation learning network, L1 for cluster classification network, L2
for outcome classification network, and the statistical significance constraint jointly. The algorithm is elaborated in
Algorithm 1.
5

Algorithm 1: DICE: Deep significance clustering
Input: X, {v}, K, d
P
Output: {zp }P
p=1 , {cp }p=1
Initialize the autoencoder of representation learning through LAE ;
Extract representations {z};
for i = 1 : niter do
Optimize Lclustering by k-means;
Calculate the cluster membership;
Use the cluster memberships as pseudo-labels for cluster classification network in L1 ;
for j = 1 : nepoch do
Jointly optimize LAE , L1 , L2 , and Gk1 ,k2 ;
end
Extract representations {z};
end
P
return {zp }P
p=1 , {cp }p=1

3.5.2

Architecture search

We choose the network architecture which is trained on the training set and has the best evaluation performance on
validation set, that is
(K ? , d? ) = argmax AU Cval (N (K, d, θ)),
(7)
K,d

where AU Cval (·) is the AUC score on the validation set.

4

Experiments

We conducted experiments on two datasets and compared against three baseline methods. We also carried out ablation
experiments to study the impact of statistical significance constraint of DICE.
4.1

Experimental setting

Data We used datasets on two patient populations: heart failure (HF) and COVID-19, extracted from electronic health
records (EHRs) at an urban academic medical center. The datasets were split into training, validation, and test sets in a
4 : 1 : 1 ratio.
• HF: We included HF patients (n = 1, 585) aged 18 to 89 from years 2014 to 2018 who were treated on the
Medicine service. HF was defined by ICD-9/10-CM. The outcome is defined as discharged to home (36.8%).
Demographics, medical events (diagnoses, medications and procedures) were included in the data. Events
were timestamped by day and concatenated as features. We added normalized days by subtracting initial
presentation time into input features.
• COVID-19 (AKI): We included patients aged 18 to 101 who presented to the ED and admitted for COVID-19
disease (n = 1, 002) in 2020. COVID-19 was defined by a positive polymerase chain reaction test. The
outcome is acute kidney injury (AKI) (30.4%). Age, gender, and laboratory values within 24 hours of ED
arrival were included in the data. One-time features for each patient were used.
Baselines We compared our method with baseline methods including (1) principal component analysis (PCA) (kmeans), (2) autoencoder (AE) (k-means), and (3) AE w/ classification (k-means). For PCA (k-means), we merged
sequential data into one-time features in HF dataset to learn PCA representations, followed by k-means clustering. In
AE (k-means), k-means clustering was applied directly to representations learned from AE (Sutskever et al., 2014). In
AE w/ class. (k-means), we firstly jointly trained AE and outcome classification with representation learned from AE as
the input for outcome classification, then applied k-means clustering to the final learned representation. We report the
results of these baseline methods of the same hyper-parameters with DICE.
Training We conducted experiments in PyTorch1 on NVIDIA GeForce RTX 2070. We initialized the autoencoder
with one epoch training. We set p-value α = 0.05 which leads to αG = 3.841, niter = 60, nepoch = 1. The λ1 ,
1

https://pytorch.org

6

Figure 2: The model selection on HF dataset. “yes” represents that the architecture network met the significance
constraint, and “no” otherwise.
λ2 , λ3 , λ4 were set as 0.1, 10, 1.0, 1.0, respectively, based on the accuracy on the validation set. It took about 7
minutes to optimize each network architecture. For COVID-19 dataset, the encoder and decoder are set as two-layer
fully-connected neural networks with ReLU (Nair and Hinton, 2010) activation functions in the intermediate layers.
4.2

Results

We used NAS to choose the best model, then qualitatively compared our method with baselines using clustering and
classification metrics. Ablation studies were also conducted to compare performance absent the statistical significance
constraint.
Neural network architecture search Our search spaces were {(K, d)|K ∈ {2, 3, 4, 5}, d ∈ {20, 25, ..., 100}} for
the HF dataset and {(K, d)|K ∈ {2, 3, 4, 5}, d ∈ {10, 11, ..., 20}} for the COVID-19 dataset, which are set according
to the number of features and size of datasets. Figure 2 demonstrates the NAS process, with AUC values from the
validation set of different neural network architecture on the Y-axis and d on the X-axis. The translucent markers
represent that the architectures cannot meet the significance constraint. From Figure 2, we can see that the statistical
significance constraint can drive the model towards higher AUC, as also demonstrated in the ablation study described
below. Maximizing the AUC, the network hyper-parameters K = 4, d = 35 for the HF dataset, and K = 3, d = 16 for
the COVID-19 dataset, were chosen as the optimal parameters.

a: DICE.

b: PCA (k-means).

c: AE (k-means).

d: AE w/ class. (k-means).

Figure 3: Visualization of patient subtyping results by various methods on HF dataset.
Visualization of representation For the HF dataset, we demonstrate the clustering results through the visualization
of representation in Figure 3. Compared with Figure 3b, Figure 3c and Figure 3d, the 4 clusters in Figure 3a discovered
by DICE displayed tighter separation, with the highest outcome ratio 79.93% in cluster 1 to the lowest outcome ratio
8.61% in cluster 4. The baseline AE w/ class. (k-means) also discovered 4 clusters with the outcome ratio in each cluster
ranging from 72.22% to 5.85%, but the clusters are not well separated. PCA (k-means) and AE (k-means) did not
7

a: DICE.

b: PCA (k-means).

c: AE (k-means).

d: AE w/ class. (k-means).

Figure 4: Visualization of patient subtyping results by various methods on COVID-19 dataset.

a: DICE.

b: PCA (k-means).

c: AE (k-means).

d: AE w/ class. (k-means).

Figure 5: Outcome stratification results by various methods on COVID-19 dataset.
discover clusters with outcomes as clearly separated as DICE, likely because the two baselines are not outcome-driven.
Our DICE learns representation through outcome-driven and conducts self-supervised learning with pseudo-labels,
therefore we can obtain clear outcome risk stratification and well separated clusters at the same time. Visualizations
of patient subtyping results for the COVID-19 dataset are shown in Figure 4. DICE again obtained clearer separation
between clusters. The outcome stratification results are given in Figure 5. From Figure 5, we can see that DICE obtained
better outcome stratification as measured by the difference in outcome ratio between clusters.
Clustering performance on unseen data The learned cluster membership from historic data can serve as a pseudolabel for unseen data, such that new patients may be classified into one of the risk levels. The clustering performance
on the test set is shown in Table 1. Since the ground truth labels of stratification are unknown, we used Silhouette
score (Rousseeuw, 1987), Calinski-Harabasz index (Caliński and Harabasz, 1974), and Davies-Bouldin index (Davies
and Bouldin, 1979) to evaluate the clustering performance. DICE achieved the best separation across all the three
metrics in both HF dataset and COVID-19 dataset.
Outcome classification via learned representation We used the learned representation from DICE for outcome
classification using logistic regression, as shown in Table 2. DICE outperformed the baselines in AUC, accuracy (ACC),
true positive rate (TPR), false negative rate (FPR), positive predictive value (PPV) and negative predictive value (NPV).

Table 1: Clustering performance evaluation on the test set. Upper: HF dataset. Lower: COVID-19 dataset.
Silhouette score↑ Calinski-Harabasz index ↑ Davies-Bouldin index ↓
PCA (k-means)
0.0973
16.0928
2.6093
AE (k-means)
0.2811
68.0664
1.7438
AE w/ class. (k-means)
0.3458
200.0490
1.3043
DICE
0.4838
212.1706
0.8637
PCA (k-means)
0.1877
29.9614
1.8403
AE (k-means)
0.4622
162.79197
0.8413
AE w/ class. (k-means)
0.2660
92.3932
1.1244
DICE
0.5141
253.5772
0.6641
8

Table 2: Outcome prediction comparison on the test set.
AUC↑ ACC↑ FPR↓
PCA (k-means)
0.773
0.712 0.222
AE (k-means)
0.712
0.697 0.150
AE w/ class. (k-means) 0.818
0.765 0.251
DICE
0.834
0.780 0.257
PCA (k-means)
0.738
0.701 0.276
AE (k-means)
0.686
0.695 0.285
AE w/ class (k-means)
0.734
0.689 0.302
DICE
0.777
0.734 0.263

Upper: HF dataset. Lower:
TPR↑ FNR↓ TNR↑
0.598 0.402
0.778
0.433 0.567
0.850
0.794 0.206
0.746
0.845 0.155
0.743
0.647 0.353
0.724
0.647 0.353
0.716
0.667 0.333
0.698
0.726 0.275
0.737

COVID-19 dataset.
PPV↑ NPV ↑
0.611
0.769
0.627
0.721
0.647
0.862
0.656
0.892
0.508
0.824
0.5
0.822
0.493
0.827
0.544
0.861

The reason DICE had high FPR and low TNR in HF dataset compared to baselines may be explained by the high
positive case ratio in the HF dataset.
Fairness on race To ensure fairness of the algorithm, we tested DICE within each demographic patient subgroups in
the HF dataset. The AUCs for Unknown, Asian, Other, Black, and White are 0.9053, 0.8824, 0.8563, 0.8321, 0.8470,
respectively, when cluster membership is used as the predictor. The AUCs for Unknown, Asian, Other, Black, and
White are 0.8632, 0.8289, 0.7816, 0.8535, 0.8525, respectively, when learned representation is used as the predictor.
Ablation study We conducted an ablation experiment on the HF dataset to gauge the effect of the statistical significance constraint. When we disabled the statistical significance constraint, 2 clusters, with outcome distributions of
80.1% and 9.01% were chosen by NAS, compared to the 4-level separation in Figure 3a. The maximum AUC score
with cluster membership as the predictor was 0.8427 in the ablation study compared to the maximum AUC score 0.8539.
In addition, the percentage of eligible neural network decreased from 82.4% to 64.7% for K = 5 in the ablation study.
These three phenomenons indicate that statistical significance constraint contributes to clearer outcome stratification
especially for bigger K.

5

Conclusion

We demonstrated DICE using AE for representation learning, followed by a cluster classification network. In the
training, we employ k-means to generate pseudo-labels to train the cluster classification network, and an alternative grid
search in NAS for the optimal network hyper-parameters. In the experiments to discover subgroups of patients in two
disease populations: HF and COVID-19, we found that, compared to baseline, DICE better separated the population as
measured by clustering indices. The cluster membership from DICE also leads to higher AUC in classifying outcomes,
and was further used to assign unseen data into risk-levels.
Future studies will evaluate extension of DICE on multi-class outcomes. In this paper, we conducted experiments on 2
datasets with outcome ratio of roughly 30%. Future studies will also evaluate DICE on more imbalanced datasets. In
addition, the flexibility of the DICE framework will allow alternate methods for representation learning and clustering
to be evaluated depending on the needs of the application area.
DICE joins concepts of deep learning and statistics in medicine to explore clearer presentation of deep learning results.
In application, DICE differs from a pure prediction method in that, in addition to predicting individual patients’ risk
levels, it simultaneously assigns them into clusters of patients with similar clinical profiles. Thus, DICE also differs
from a pure clustering algorithm for its outcome-aware nature in assigning clusters. Outputs from DICE may be more
actionable in alerting healthcare providers of not only high-risk patients, but also providing interpretable insights for
subgroup-specific strategies. Beyond HF and COVID-19, DICE may have the potential to be used in other clinical areas
to facilitate subtype-specific care and clinical pathways for clinical decision support.

References
Baker, B., Gupta, O., Naik, N., and Raskar, R. (2016). Designing neural network architectures using reinforcement
learning. arXiv preprint arXiv:1611.02167.
Baldi, P. and Hornik, K. (1989). Neural networks and principal component analysis: Learning from examples without
local minima. Neural networks, 2(1):53–58.
Bengio, Y., Courville, A., and Vincent, P. (2013). Representation learning: A review and new perspectives. IEEE
transactions on pattern analysis and machine intelligence, 35(8):1798–1828.
9

Bishop, C. M. (2006). Pattern recognition and machine learning. springer.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research,
3(Jan):993–1022.
Caliński, T. and Harabasz, J. (1974). A dendrite method for cluster analysis. Communications in Statistics-theory and
Methods, 3(1):1–27.
Caron, M., Bojanowski, P., Joulin, A., and Douze, M. (2018). Deep clustering for unsupervised learning of visual
features. In Proceedings of the European Conference on Computer Vision (ECCV), pages 132–149.
Chu, W. and Cai, D. (2017). Stacked similarity-aware autoencoders. In IJCAI, pages 1561–1567.
Davies, D. L. and Bouldin, D. W. (1979). A cluster separation measure. IEEE transactions on pattern analysis and
machine intelligence, PAMI-1(2):224–227.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee.
Fraley, C. and Raftery, A. E. (2002). Model-based clustering, discriminant analysis, and density estimation. Journal of
the American statistical Association, 97(458):611–631.
Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., and Sun, J. (2019). Single path one-shot neural architecture
search with uniform sampling. arXiv preprint arXiv:1904.00420.
Hershey, J. R., Chen, Z., Le Roux, J., and Watanabe, S. (2016). Deep clustering: Discriminative embeddings for
segmentation and separation. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 31–35. IEEE.
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735–1780.
Hofmann, T., Schölkopf, B., and Smola, A. J. (2008). Kernel methods in machine learning. The annals of statistics,
pages 1171–1220.
Hosmer, D. W. and Lemeshow, S. (2000). Applied logistic regression. Wiley New York.
Jagabathula, S., Subramanian, L., and Venkataraman, A. (2018). A model-based embedding technique for segmenting
customers. Operations Research, 66(5):1247–1267.
Jagabathula, S., Subramanian, L., and Venkataraman, A. (2020). A conditional gradient approach for nonparametric
estimation of mixing distributions. Management Science.
Jing, L. and Tian, Y. (2020). Self-supervised visual feature learning with deep neural networks: A survey. IEEE
Transactions on Pattern Analysis and Machine Intelligence.
Li, F., Qiao, H., and Zhang, B. (2018). Discriminatively boosted image clustering with fully convolutional auto-encoders.
Pattern Recognition, 83:161–173.
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C. L. (2014). Microsoft
coco: Common objects in context. In European conference on computer vision, pages 740–755. Springer.
Lopez, R., Regier, J., Cole, M. B., Jordan, M. I., and Yosef, N. (2018). Deep generative modeling for single-cell
transcriptomics. Nature methods, 15(12):1053–1058.
Maaten, L. v. d. and Hinton, G. (2008). Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579–
2605.
MacQueen, J. et al. (1967). Some methods for classification and analysis of multivariate observations. In Proceedings
of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281–297. Oakland, CA,
USA.
McLachlan, G. J. and Peel, D. (2004). Finite mixture models. John Wiley & Sons.
Min, E., Guo, X., Liu, Q., Zhang, G., Cui, J., and Long, J. (2018). A survey of clustering with deep learning: From the
perspective of network architecture. IEEE Access, 6:39501–39514.
Nair, V. and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. In ICML.
Ng, A. Y., Jordan, M. I., and Weiss, Y. (2002). On spectral clustering: Analysis and an algorithm. In Advances in neural
information processing systems, pages 849–856.
Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of
computational and applied mathematics, 20:53–65.
Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in
neural information processing systems, pages 3104–3112.
10

Tan, M. and Le, Q. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. In International
Conference on Machine Learning, pages 6105–6114.
Tan, M., Pang, R., and Le, Q. V. (2019). Efficientdet: Scalable and efficient object detection. arXiv preprint
arXiv:1911.09070.
Van Der Maaten, L. (2009). Learning a parametric embedding by preserving local structure. In Artificial Intelligence
and Statistics, pages 384–391.
van Deursen, V. M., Urso, R., Laroche, C., Damman, K., Dahlström, U., Tavazzi, L., Maggioni, A. P., and Voors, A. A.
(2014). Co-morbidities in patients with heart failure: an analysis of the european heart failure pilot survey. European
journal of heart failure, 16(1):103–111.
Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and computing, 17(4):395–416.
Wedel, M. and DeSarbo, W. S. (1994). A review of recent developments in latent class regression models. Advanced
Methods of Marketing Research, R. Bagozzi (Ed.), Blackwell Pub, pages 352–388.
Wold, S., Esbensen, K., and Geladi, P. (1987). Principal component analysis. Chemometrics and intelligent laboratory
systems, 2(1-3):37–52.
Xie, J., Girshick, R., and Farhadi, A. (2016). Unsupervised deep embedding for clustering analysis. In International
conference on machine learning, pages 478–487.
Xu, R. and Wunsch, D. (2005). Survey of clustering algorithms. IEEE Transactions on neural networks, 16(3):645–678.
Yang, B., Fu, X., Sidiropoulos, N. D., and Hong, M. (2017). Towards k-means-friendly spaces: Simultaneous deep
learning and clustering. In international conference on machine learning, pages 3861–3870. PMLR.
Yang, L., Cheung, N.-M., Li, J., and Fang, J. (2019). Deep clustering by gaussian mixture variational autoencoders with
graph embedding. In Proceedings of the IEEE International Conference on Computer Vision, pages 6440–6449.
Zhang, H., Basu, S., and Davidson, I. (2019a). A framework for deep constrained clustering-algorithms and advances.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 57–72. Springer.
Zhang, X., Chou, J., Liang, J., Xiao, C., Zhao, Y., Sarva, H., Henchcliffe, C., and Wang, F. (2019b). Data-driven
subtyping of parkinson’s disease using longitudinal clinical records: a cohort study. Scientific reports, 9(1):1–12.
Zhong, S. and Ghosh, J. (2003). A unified framework for model-based clustering. Journal of machine learning research,
4(Nov):1001–1037.
Ziaeian, B. and Fonarow, G. C. (2016). Epidemiology and aetiology of heart failure. Nature Reviews Cardiology,
13(6):368–378.
Zoph, B. and Le, Q. V. (2016). Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.
Zoph, B., Vasudevan, V., Shlens, J., and Le, Q. V. (2018). Learning transferable architectures for scalable image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697–8710.

11

