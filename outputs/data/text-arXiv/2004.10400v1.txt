1

COVID-19 publications: Database coverage, citations, readers, tweets,
news, Facebook walls, Reddit posts
Kayvan Kousha, Statistical Cybermetrics Research Group, University of Wolverhampton,
Wulfruna Street, Wolverhampton WV1 1LY, UK. ORCID: 0000-0003-4827-971X
Mike Thelwall, Statistical Cybermetrics Research Group, University of Wolverhampton,
Wulfruna Street, Wolverhampton WV1 1LY, UK. ORCID:0000-0001-6065-205X
The COVID-19 pandemic requires a fast response from researchers to help address biological,
medical and public health issues to minimize its impact. In this rapidly evolving context,
scholars, professionals and the public may need to quickly identify important new studies. In
response, this paper assesses the coverage of scholarly databases and impact indicators
during 21 March to 18 April 2020. The results confirm a rapid increase in the volume of
research, which particularly accessible through Google Scholar and Dimensions, and less
through Scopus, the Web of Science, PubMed. A few COVID-19 papers from the 21,395 in
Dimensions were already highly cited, with substantial news and social media attention. For
this topic, in contrast to previous studies, there seems to be a high degree of convergence
between articles shared in the social web and citation counts, at least in the short term. In
particular, articles that are extensively tweeted on the day first indexed are likely to be highly
read and relatively highly cited three weeks later. Researchers needing wide scope literature
searches (rather than health focused PubMed or medRxiv searches) should start with Google
Scholar or Dimensions and can use tweet and Mendeley reader counts as indicators of likely
importance.
Keywords: COVID-19; Dimensions; Google Scholar; Altmetrics; Mendeley; Citation impact;
Scopus; Web of Science; PubMed;

Introduction
The international scientific effort to mitigate COVID-19 is unprecedented in scale and rapidity.
For instance, PubMed added related publications daily between 1st January to 18th of April
2020 1, with a peak of over 300 in a single day. This effort is in response to the lethality and
rapid spread of the disease, as well as the major economic and social consequences of COVID19 lockdowns. As part of the response, researchers, professionals and the public will need to
consult the scientific literature for the latest findings. Whilst this is normal for science,
standard literature search methods may be ineffective in a rapid publishing environment.
Traditional citation indexes may not be fast enough, especially given that they do not index
most preprints, and citation counts may not help point to important studies. The more
inclusive online citation indexes of sites like Google Scholar and Dimensions.ai seem like
suitable alternatives since they index both the traditional scholarly literature and documents
not published in journals, including preprints (Herzog, Hook, & Konkiel, 2020; Kousha, &
Thelwall, 2019a). Whilst there are initiatives to help various communities with curated
collections of COVID-19 documents, such as published biomedical documents from PubMed
Central (PMC, 2020), preprints from medRxiv and bioRxiv (medRxiv, 2020), and a data mining
collection (Allen Institute, 2020), none are complete. It is therefore important to assess the

1

https://www.nlm.nih.gov/pubs/techbull/nd08/nd08_pm_new_date_field.html

2
COVID-19 coverage and growth of scholarly publication indexes, as well as the value of
citation counts for new COVID-19 research.
350

Number of COVID-19 publications

300
250
200
150
100

18/04/2020

16/04/2020

14/04/2020

12/04/2020

10/04/2020

08/04/2020

06/04/2020

04/04/2020

02/04/2020

31/03/2020

29/03/2020

27/03/2020

25/03/2020

23/03/2020

21/03/2020

19/03/2020

17/03/2020

15/03/2020

13/03/2020

11/03/2020

09/03/2020

07/03/2020

05/03/2020

03/03/2020

01/03/2020

28/02/2020

26/02/2020

24/02/2020

22/02/2020

20/02/2020

18/02/2020

16/02/2020

14/02/2020

12/02/2020

10/02/2020

08/02/2020

06/02/2020

04/02/2020

02/02/2020

31/01/2020

29/01/2020

27/01/2020

25/01/2020

23/01/2020

21/01/2020

19/01/2020

0

17/01/2020

50

Figure 1. Daily additions of COIVD-19 publications to PubMed (17 Jan-18 April). Query used
((((((("COVID-19") OR "Novel coronavirus") OR "2019-nCoV") OR "SARS-CoV-2") OR
"coronavirus 2") OR "Coronavirus disease 2019") OR Corona virus disease 2019) AND
("2019/12/01"[Date - Publication] : "3000"[Date - Publication]).
In parallel with scholarly needs for literature, the public, professionals and policy
makers also need to access current COVID-19 research to inform their decision-making, such
as whether to recommend wearing protective masks. This may be in addition to, or to clarify,
World Health Organisation guidelines (WHO, 2020). They may therefore share relevant
academic research in the social web (e.g., Merchant & Lurie, 2020), generating interest that
may picked up by alternative indicators (altmetrics). Thus, altmetrics, may be useful in helping
the public to identify the most relevant research or may help point researchers to topics
considered important by the public. It would therefore be helpful to assess whether
altmetrics can perform this role. In particular, since altmetrics can reflect both academic and
non-academic interests (Mohammadi, Barahmand, & Thelwall, 2019; Mohammadi, Thelwall,
Kwasny, & Holmes, 2018), it is not clear whether they will essentially be early indicators of
citation impact or whether they reflect societal or other impacts for COVID-19. Altmetrics
have already been shown useful to identify the spread of a misleading COVID-19 paper that
was subsequently withdrawn (Ioannidis, 2020).
This paper addresses the above issues through a primarily descriptive analysis of the
evolution of four online scholarly databases, and associated altmetrics, over four weeks in
March-April 2020, when many countries were experiencing a lockdown. A previous study of
20 January to 12 April 2020 has shown continually increasing growth in the COVID-19
coverage of scholarly databases, with substantial variations between fields (Torres-Salinas,
2020). Individual highly cited or shared papers are also important to examine for qualitative
insights into the types of research that are attracting substantial attention. The following
research questions drive this paper.
• Which scholarly databases index the most COVID-19 publications (extending: TorresSalinas, 2020)?
• Which COVID-19 documents have become highly cited or highly discussed?
• Do altmetrics and early citation counts reflect similar types of COVID-19 impact?

3
•

Can any altmetrics serve as early indicators of future citation impact for COVID-19
documents?

Background
The novel coronavirus 2019 (COVID-19) was first reported in Wuhan City, China in December.
Quickly disseminating scientific results about COVID-19 is vital to allow quick action from
successful clinical results (Song & Karako, 2020). The importance of scientific publishing to
respond to infectious disease outbreaks has been emphasised by many bibliometric studies
of previous cases (Rethlefsen & Livinski, 2013), such as SARS (Kostoff & Morse, 2011; Tian &
Zheng, 2015), H7N9 influenza (Tian & Zheng, 2015), HIV/AIDS (Pouris & Pouris, 2011), Ebola
(Pouris & Ho, 2016) and Zika (Delwiche, 2018).
One recent study using Dimensions, Scopus, WoS and the LitCovid (Chen, Allot, & Lu,
2020) curated list has investigated the daily growth of Covid-19 related publications in citation
databases and digital libraries during 1st January to 7th of April, finding that Dimensions had
best coverage (9,435 publications) compared to WoS (718) and Scopus (1,568). The weekly
growth of PubMed was about 1,000 publications and the PubMed Central (1,398), medRxiv
(989) and SSRN (608) repositories had best coverage of open access COVID-19 publications
(Torres-Salinas, 2020). Google Scholar was not assessed and all evidence was extracted from
Dimensions, so the counts for other repositories may not be complete.

Dimensions citations
Dimensions.ai (Herzog, Hook, & Konkiel, 2020) is an online scholarly database that operates
similarly to Google Scholar, in the sense of indexing documents using public information from
the Web but has an Applications Programming Interface (API) that supports automatic
downloading for all query matches. It indexes most documents in Scopus (Thelwall, 2018b),
although not for all fields (Orduña-Malea & Delgado-López-Cózar, 2018). It seems to have
substantial coverage of preprint servers, such as arXiv, and so probably has much larger
coverage overall, especially for recently published papers. Its coverage seems to be higher
than Scopus and the Web of Science (WoS), comparable to CrossRef but lower than Google
Scholar and Microsoft Academic (Harzing, 2019). In line with this, citation counts for papers
in Dimensions can be expected to be slightly higher than for Scopus and WoS but substantially
higher for newer documents.

Altmetrics: Mendeley Readers
Counts of readers from the social reference sharing site Mendeley form the most extensively
researched and understood altmetric. A non-trivial minority of researchers (about 5%) used
Mendeley by 2014 according to one survey, with disciplinary differences (Van Noorden,
2014). People typically register documents in Mendeley when they have read them or intend
to read them (Mohammadi, Thelwall, & Kousha, 2016), so it is reasonable to regard Mendeley
counts as an indicator of readership. According to self-reports in the site, users are
predominantly academics and postgraduate students, with a few undergraduates, librarians
and people in non-academic occupations (Mohammadi, Thelwall, Haustein, & Larivière,
2015). Thus, Mendeley is an indicator of predominantly academic readership, with an
element of student readership.
A range of studies have investigated the relationship between Mendeley reader
counts and citation counts, finding moderate or strong positive correlations (Costas, Zahedi,

4
& Wouters, 2015). Correlations between mature citation counts and Mendeley reader counts
are strong and positive in almost all narrow fields in Scopus (Thelwall, 2017a), supporting their
use as a citation impact type of indicator. Whilst the two types of data seem to be close to
interchangeable for sets of mature articles (although they can differ sharply for individual
education-oriented papers: Thelwall, 2017c), the advantage of Mendeley reader counts is
that they appear and are useful a year before citation counts (Thelwall, 2017b). They may
even be common enough to be used for scientometric purposes by the publication month of
the publishing journal. Moreover, since early Mendeley reader counts correlate positively
with later citation counts (Thelwall, 2018a), Mendeley reader counts are early academic
impact indicators. They should therefore be a better academic impact indicator than citation
counts for fast moving issues, such as COVID-19.

Altmetrics: Tweeters, Facebook Walls
Twitter is a widely recognized source of altmetrics. More articles have non-zero tweet counts
than non-zero scores on any other altmetric, other than Mendeley (Thelwall, Haustein,
Larivière, & Sugimoto, 2013). As a news-oriented social media platform, articles can expect
to get a substantial proportion of their tweets in the week of publication, so Twitter is visible
long before citations. Nevertheless, tweeter counts (counting the number of tweeters rather
than the number of tweets) are problematic to interpret. Whilst about half of people that
tweet academic research are not academics (Mohammadi, Thelwall, Kwasny, & Holmes,
2018), tweets typically contain just article titles or brief summaries (Thelwall, Tsou, Weingart,
Holmberg, & Haustein, 2013), serving as publicity rather than evidence of impact. Together
with often close to zero correlations with citation counts (Costas, Zahedi, & Wouters, 2015;
Haustein, Larivière, Thelwall, Amyot, & Peters, 2014; Thelwall, Haustein, Larivière, &
Sugimoto, 2013), there is insufficient evidence to claim that tweeter counts are indicators of
either academic or societal impact. Nevertheless, they may have some value for healthrelated research, where there is more public interest in academic research (Haustein,
Larivière, Thelwall, Amyot, & Peters, 2014; Mohammadi, Gregory, Thelwall, Barahmand,
2020).
Facebook wall posts function like tweeter counts except that they are rarer (Costas,
Zahedi, & Wouters, 2015; Thelwall, Haustein, Larivière, & Sugimoto, 2013). Since most of
Facebook is private and Altmetric.com obtains its Facebook Wall counts only from public
pages, this altmetric probably reflects a tiny fraction of all Facebook posts and may be
oriented to organizational uses of Facebook (including journals) rather than typical users, and
few posts are directly from academics (Mohammadi, Barahmand, & Thelwall, 2019).

Altmetrics: News and Reddit
Altmetric.com harvests citations from online free news websites and the news-oriented site
Reddit. Altmetrics from both are relatively rare and have very low correlations with citation
counts (Costas, Zahedi, & Wouters, 2015; Thelwall, Haustein, Larivière, & Sugimoto, 2013).
Nevertheless, health-related topics are newsworthy (Clark & Illman, 2006; Kousha, &
Thelwall, 2019b), including for infectious diseases (e.g., SARS: Lewison, 2008) and so they may
be useful for COVID-19.

5

Methods
The research design is in three parts. First, to assess the relative coverage of scholarly
databases, a range were queried daily from 21 March 2020 to record the number of COVID19 documents indexed. Second, lists of documents matching a set of COVID-19 queries were
downloaded from Dimensions.ai and altmetrics for these were gathered from Mendeley
(Gunn, 2014) and Altmetric.com (Adie & Roe, 2013; Robinson-García, Torres-Salinas, Zahedi,
& Costas, 2014) daily and the individual scores and documents compared. Third, a March-24
dataset was created to track a set of documents indexed on the same day.

Scholarly database indexing of COVID-19 publications
To assess the indexing of COVID-19-related publications, the two mainstream scholarly
databases, Scopus and Web of Science (WoS) were queried as well as a range of other
academic sources that may index relevant documents. After testing, the core queries used to
identify relevant documents were as follows. These are not comprehensive but are high
precision, unless stated, and should include the most recent research focusing on the issue,
assuming that it includes the current official disease description.

6
Table 1. COVID-19 queries for a range of scholarly sources.
Source
Query
Scope/Year
Google
"COVID-19"
All/2019-2020
Scholar
"COVID-19" OR "Novel coronavirus" OR All fields and
"2019-nCoV" OR "SARS-CoV-2" OR publication
"coronavirus 2" OR "Coronavirus disease types/2019Dimensions 2019" OR "Corona virus disease 2019"
2020
((((((("COVID-19")
OR
"Novel All fields and
coronavirus") OR "2019-nCoV") OR publication
"SARS-CoV-2") OR "coronavirus 2") OR types from Dec
"Coronavirus disease 2019") OR Corona 2019
virus
disease
2019)
AND
("2019/12/01"[Date - Publication] :
PubMed
"3000"[Date - Publication])
Mendeley
"COVID-19"
All/not assigned
Self-reported repository statistics for Full text papers
self-curated collection.
medRxiv
and
bioRxiv

Scopus
WoS
Core
Collection

PMC
Clinical
Trials.gov

2
3

(ALL ( "COVID-19" ) OR ALL ("Novel
coronavirus") OR ALL ("2019-nCoV") OR
ALL ("SARS-CoV-2") OR ALL ("coronavirus
2") OR ALL ("Coronavirus disease 2019")
OR ALL ("Corona virus disease 2019" ) )
AND PUBYEAR = 2020 OR PUBDATETXT
(december 2019)
TOPIC=("COVID-19"
OR
"Novel
coronavirus" OR "2019-nCoV" OR "SARSCoV-2" OR "coronavirus 2" OR
"Coronavirus disease 2019" OR "Corona
virus disease 2019")
(((((((("COVID-19")
OR
"Novel
coronavirus") OR "Novel coronavirus")
OR "2019-nCoV") OR "SARS-CoV-2") OR
"coronavirus 2") OR "Coronavirus disease
2019") OR "Corona virus disease 2019")
AND ("2019/12/01"[Publication Date] :
"3000"[Publication Date])
COVID OR "SARS-CoV-2" OR "2019-nCoV"

https://connect.biorxiv.org/relate/content/181
https://clinicaltrials.gov/ct2/results?cond=COVID-19

All fields and
publication
types/20192020

All fields and
publication
types
/20192020

Comments
OR does not work.
May be inaccurate.

OR does not work
Repository
statistics
for
COVID-19
SARSCoV-2
preprints
from medRxiv and
bioRxiv2.

Including
Conference
Proceedings
Citation Index.

Full
text
publications
from Dec 2019

Query predefined
statistics3.

7

Document and altmetric comparison datasets
Initial testing suggested that Dimensions and Google Scholar had the largest coverage of
COVID-19 documents. Since Google Scholar does not have an API and the number of matches
exceeds its 1000 limit per query, it was not possible to extract Google Scholar’s set of
matching documents. In contrast, Dimensions.ai has an API allowing complete sets of
matching document records to be downloaded and so it was chosen as the base source of
COVID-19 documents. It was checked daily with the following set of queries in the Dimensions
API (Applications Programming Interface), designed to match publications about COVID-19
using various related names. These queries are all designed to be precise but there were still
a few false matches. All queries ended in, “return publications [basics + extras]”
• search publications for "COVID-19" where year >= 2019
• search publications for "Novel coronavirus" where year >= 2019
• search publications for "2019-nCoV" where year >= 2019
• search publications for "SARS-CoV-2" where year >= 2019
• search publications for "coronavirus 2" where year >= 2019
• search publications for "Coronavirus disease 2019" where year >= 2019
• search publications for "Corona virus disease 2019" where year >= 2019
The resulting 21,395 publications were mainly open access (53%; 75% for the March 24 set –
see later) and predominantly from health-related specialties (Table 2).
Table 2. The top 10 Dimensions subject codes for the complete and March 24 datasets.
FOR code
1117 Public Health and Health Services
1108 Medical Microbiology
1103 Clinical Sciences
0601 Biochemistry and Cell Biology
1107 Immunology
0604 Genetics
1102 Cardiorespiratory Medicine &
Haematology
0801 Artificial Intelligence and Image Processing
1109 Neurosciences
0605 Microbiology

All
3072
2773
2159
1192
1096
803
459
383
316
364

All
%
2762.9 13%
2240.8 10%
1860.9 9%
946.3 4%
873.4 4%
642.7 3%
384.4
336.0
257.9
224.3

2%
2%
1%
1%

Mar24
78
32
32
14
5
4
7
6
2
0

Mar24 %
73.3 21%
27.2 8%
28.8 8%
11.3 3%
2.8 1%
2.8 1%
6.5
6.0
1.3
0.0

2%
2%
0%
0%

The datasets analysed include substantial numbers of papers from preprint planforms,
including medRxiv, SSRN, arXiv, bioRxiv, ChemRxiv and Research Square (Table 3, as in: TorresSalinas, 2020) as well as books and more traditional journals (Table 3).

8
Table 3. The top 10 journals, as recorded in Dimensions, for the complete and March 24
datasets.
Journal
[None]
medRxiv
SSRN Electronic Journal
arXiv
bioRxiv
Research Square
The BMJ
ChemRxiv
Viruses
Journal of Medical Virology

All
%
2932 14%
1234 6%
855 4%
389 2%
358 2%
341 2%
262 1%
210 1%
196 1%
176 1%

Mar-24
13
30
0
16
1
13
9
8
1
4

%
4%
9%
0%
5%
0%
4%
3%
2%
0%
1%

Comment
Books, book chapters, theses
Health sciences preprints
Social science preprints
Physics/computing preprints
Biological sciences preprints
Preprint platform
Core medical journal
Chemistry preprints
MDPI open access journal
Wiley journal

Although most documents were classified as Articles by Dimensions, this type includes
medRxiv preprints and diverse types of document published in journals, such as notes, short
communications, editorials and commentaries (Table 4). Since many editorials seemed to
discuss the impact of COVID-19 on the journal or field, this added less citable documents to
the Article class. The surprising number of books and book chapters (13% overall) seems to
be primarily due to pre-COVID-19 discussions about Coronaviruses, matching the query
“Coronavirus 2”. The low number of conference proceedings may be due to conference
cancellations, or the inability of most conferences to respond to the COVID-19 timescale.
Since the Dimensions type Article includes documents that would not be classed as
standard journal articles in scientometric analyses, the 295 Dimensions “Articles” from March
24 were visited to classify them by type. Only 106 of these seemed to be standard journal
articles. The rest were mainly editorials, letters (called letters, letters to the editor, or
correspondence; one Letter was classed as an article) or news stories. In some cases,
documents were called “article” by the publishing journal but were clearly news stories
published in a news-focused magazine/journal. The reduced set of 106 journal articles from
March 24, 2020 was used for follow-up correlation tests.
Table 4. The top 10 document types, as recorded in Dimensions, for the complete and March
24 datasets.
Type

All

Article
Book
Chapter

16330
832
1645

%

76%
4%
8%

Preprint
2236
10%
Monograph
166
1%
Proceeding
186
1%
Total
21392 100%

Mar-24 %
295
4
5

85%
1%
1%

43 12%
2
1%
0
0%
349 100%

Comments
Includes preprints from medRxiv, editorials,
commentaries
Matches more general “Coronavirus 2” research
Matches more general “Coronavirus 2” research
Includes arRxiv, Research Square, chemRxiv, JMIR
Preprints, SSRN
Matches more general “Coronavirus 2” research
Conference proceedings

After Webometric Analyst had downloaded a complete set of records each day, the Mendeley
API was used to identify the number of Mendeley readers for each document, again using
Webometric Analyst. It queries by DOI and by title/author/year and combines non-

9
overlapping results for the most complete reader count. This follows best practice (Zahedi,
Haustein, & Bowman, 2014). Webometric Analyst was also used to identify counts of citations
in Twitter, Facebook, Reddit and online news outlets to these documents, as identified by DOI
queries to Altmetric.com. This data provider seems to have the most comprehensive coverage
of Twitter, the largest of the sources (Ortega, 2018). Twitter and Facebook are logical choices
to investigate because they seem to be the social media sources that most cite academic
research (Costas, Zahedi, & Wouters, 2015; Thelwall, Haustein, Larivière, & Sugimoto, 2013).
Reddit and news may give a news perspective, although Reddit is a multipurpose site (Ovadia,
2015; Stoddard, 2015) and the news sources harvested by Altmetric.com presumably exclude
some major paywalled press sources.
There were some gaps in the data collection due to documents not being returned by
a query on one day when they had been returned on a previous day. This produced missing
citation and altmetric scores, affecting the analysis. To avoid this issue, these missing values
were replaced by approximate values by linear interpolation (when scores were available for
previous and subsequent dates), linear extrapolation (when at least two previous but no
subsequent scores were available), or constant values (when only one previous value was
available).

Analysis
The coverage of the different sources was evaluated by comparing (on a graph) the number
of query matches over time. This is not a fair comparison because the queries are not
equivalent, a researcher may use other queries, and the sources index with different levels of
comprehensiveness. For example, a source that indexed full text of documents would get
more and probably less relevant hits than a source indexing the title and abstract, even if they
had the same coverage.
To assess the types of document generating the most impact for each source, the top
5 for each indicator was extracted to give a manageable set. A comparison of the relative
ranks of these documents for the different indicators was used to guide the evaluation of the
relative importance of the document characteristics, along with the document age (younger
documents would tend to have lower scores in less rapidly evolving indicators). This focus on
the highest scoring documents seems reasonable since they are likely to be the most
influential or important, even though different trends may apply to more average documents.
To compare the average accumulation speed and scores of COVID-19 documents, a
base set was chosen, consisting of documents first indexed in Dimensions on 24 March 2020.
This was the date from the first week with the most new documents (excluding the first day).
These documents form a set that are likely to have been published on or shortly before 24
March 2020. The altmetric and citation scores for this set were compared over time to assess
their evolution and relative magnitude. Averages were calculated with geometric means (with
a +1 offset: Fairclough, & Thelwall, 2015) rather than arithmetic means due to the highly
skewed nature of citations (de Solla Price, 1976; Wallace, Larivière, & Gingras, 2009) and
altmetrics (Thelwall & Wilson, 2016; Yu, Xu, Xiao, Hemminger, & Yang, 2017). The scores of
this set were then compared using Spearman correlations to assess the extent to which they
may reflect similar types of impact (Sud & Thelwall, 2014). Since altmetrics other than
Mendeley tend to have very weak correlations with citation counts (Costas, Zahedi, &
Wouters, 2015; Haustein, Larivière, Thelwall, Amyot, & Peters, 2014; Thelwall, Haustein,
Larivière, & Sugimoto, 2013), high correlations are not expected. Field normalization was not
used for either analysis because (a) the papers cover a relatively narrow topic (COVID-19)

10
even though they span many subject areas and (b) it is impractical to field normalize the
values because this would require daily updates of the whole of Dimensions, Altmetric.com
and Mendeley for the calculations.

Results
Coverage of scholarly databases
Based on the estimated number of manual search results returned by the sources queried, it
seems that Google Scholar has substantially wider coverage of COVID-19 publications than all
other sources (Figure 2). The results for Google Scholar may well be substantially inflated by
its web search component indexing advertisements or warnings in webpages alongside
articles irrelevant to the disease, however, so these results are not robust. To illustrate the
existence of these false matches, a search for “COVID-19” in Google Scholar with a date range
specified as 1990-2000 (i.e., 20 years before the name was coined) on April 21, 2020 returned
an estimated 5,010 matches 4. Each incorrect Google Scholar match reported snippets not
from the paper, such as, “PEDIATRICS COVID-19 COLLECTION We are fast-tracking and
publishing the latest research and articles related to COVID-19 for free.” Nevertheless, it
seems likely that Google Scholar indexes at least as many documents as Dimensions.
Google Scholar and Dimensions index both publisher records and other online
publications (preprint archives for Dimensions, wider web sources for Google Scholar) seem
able to identify COVID-19 publications more quickly or more widely than the Web of Science
and Scopus. This suggests that academics studying the area should consider them if more
specialist databases, such as PubMed, are not adequate. This argument does not take into
account the importance of the documents, however, and it is possible that the key
publications are quickly peer reviewed, published and indexed by Scopus and WoS. The exact
COVID-19 coverage of Google Scholar is difficult to assess because it is not possible to
download and check all matches in the absence of a Google Scholar API to download large
sets of publication records.

Number of COVID publication

30000
25000
20000
15000
10000
5000
0

Google Scholar

Dimensions

PubMed

Mendeley

medRxiv or bioRxiv

Scopus

WoS

PMC

ClinicalTrials.gov

Figure 2. The daily number of hits for COVID-19 queries (see Table 1) from a range of scholarly
sources (22 March – 18 April). The Google Scholar estimates include many false matches.
4

https://scholar.google.co.uk/scholar?q=covid-19&hl=en&as_sdt=0%2C5&as_ylo=1990&as_yhi=2000

11

Overlaps between Dimension, Scopus and Web of Science

The extent of overlaps between the COVID-19 query results for Dimensions, Scopus and WoS
were estimated on 19 April, 2020 to assess whether they were indexing the same
publications. To obtain a relevant set of COVID-19 publications, only publications from 20192020 with the term "COVID" OR "coronavirus" OR "2019-nCoV" OR "SARS-CoV-2" OR
"Corona" in their titles were selected. Publications with DOIs were matched between the
three databases to assess the percentage overlap between them (Table 5). Few of the
Dimensions publications were also in Scopus (23.3%) or WoS (11.8%). Two fifths (40.4%) of
the Scopus publications were in WoS and four fifths (81.9%) of WoS publications were in
Scopus. Google Scholar could not be compared without a comprehensive list of search
matches.
Table 5. Overlaps for COVID-19 publications between Dimension, Scopus and Web of Science
(18 April, 2020).
Total
Overlap % (No.)
Non-overlapping % (No.)
publications Scopus
WoS
Scopus
WoS
Dimensions 8,642
23.3% (2,010)
11.8% (1,017) 76.7% (6,632) 88.2% (7,624)
Scopus
2,166
40.4% (874)
59.6% (1,292)
WoS
1,067
81.9% (874)
18.1% (193)
In terms of citations found by the three databases for the matching publications, Dimensions
citation counts for all its matching COVID-19 publications were 4.9 and 2.8 times as numerous
as WoS and Scopus, suggesting for the recently published or in-press articles, Dimensions had
faster citation indexing than WoS and Scopus or from faster sources, such as preprint archives.
This could be important when scholars want to consider early citation impact evidence for
identifying relevant COVID-19 publications or for the impact assessment of published articles.

Most cited papers.
The documents with the most Mendeley readers and Dimensions citations tended to be
similar and to provide primary clinical and epidemiological evidence about COVID-19 (Table
6). Shorter publication formats and analyses are more evident in the social web and news
sources, representing a partially different type of document. The social web and news articles
also seemed to give information that might be particularly useful as public health information
for the vast majority of the planet’s population that had not yet caught COVID-19 by 18 April
2020. These include studies on facemasks, the stability of the virus on surfaces, and pregnancy
risks.

12
Table 6. Characteristics and ranks of COVID-19 papers in the top five for Dimensions,
Mendeley, Twitter, Facebook, and News, and their ranks in these sites on 18 April 2020.
Citation and altmetric counts are in the figures below.
Title
Journal* Date
Type
D M T F
Clinical features of patients infected with
24/1/20 Article
1
1
2019 novel coronavirus in Wuhan, China
A novel coronavirus from patients with
pneumonia in China, 2019
Early transmission dynamics in Wuhan,
China, of Novel coronavirus-Infected
pneumonia
Epidemiological and clinical characteristics of
99 cases of 2019 novel coronavirus
pneumonia in Wuhan, China: a descriptive
study

Lancet

NEJM
NEJM

Lancet

Clinical characteristics of 138 hospitalized
patients with 2019 novel CoronavirusInfected pneumonia in Wuhan, China
JAMA
Clinical characteristics of coronavirus disease
2019 in China
NEJM
Clinical course and risk factors for mortality
of adult inpatients with COVID-19 in Wuhan,
China: a retrospective cohort study
Lancet
Nature
The proximal origin of SARS-CoV-2
Medicine
Treatment of 5 critically ill patients with
COVID-19 with convalescent plasma
JAMA
Respiratory virus shedding in exhaled breath Nature
and efficacy of face masks
Medicine
Aerosol and surface stability of SARS-CoV-2
as compared with SARS-CoV-1
NEJM
Coronavirus latest: CERN scientists join the
COVID-19 fight.
Nature
Clinical characteristics and intrauterine
vertical transmission potential of COVID-19
infection in nine pregnant women: a
retrospective review of medical records
Lancet
Characteristics of and important lessons
from the coronavirus disease 2019 (COVID19) outbreak in China
JAMA
The incubation period of coronavirus disease Annals of
2019 (COVID-19) from publicly reported Internal
confirmed cases: estimation and application. Medicine
Morbidity
Severe outcomes among patients with Mortality
coronavirus disease 2019 (COVID-19) - Weekly
United States, February 12-March 16, 2020. Report

*NEJM: New England Journal of Medicine

20/2/20 Brief
report
26/3/20 Article

2

4

3

3

30/1/20 Article

4

Original
5
Investigat
ion
28/2/20 Article

2

11/3/20 Article

5

N

3

7/2/20

17/3/20 Correspo
ndence
27/3/20 Prelimina
ry Comm.
3/4/20 Brief
Comm.
17/3/20 Correspo
ndence
8/4/20 News

4
3
1

4

2
3
5

1
1

12/2/20 Article

2

24/2/20 Viewpoint

5

4

10/3/20 Original
research

2

18/3/20 Report

5

13
None of the five documents most cited on Reddit were also in the top five for the other
sources, although they seem to cover similar topics (Table 7). The paper about Malayan
pangolins is the exception for not covering the primary characteristics of the disease or public
health issues. This may be an artefact of the relatively low numbers of Reddit citations.
Table 7. Characteristics and ranks of COVID-19 papers in the top five for Reddit, and their
ranks in these sites on 18 April 2020. There is no overlap with Table 1. Citation and altmetric
counts are in the figures below.
Title
Journal
Date
Type
R
The neuroinvasive potential of SARS-CoV2 may play a J of Medical 27/2/20
Review
1
role in the respiratory failure of COVID-19 patients
Persistence of coronaviruses on inanimate surfaces
and its inactivation with biocidal agents
High temperature and high humidity reduce the
transmission of COVID-19
Identifying SARS-CoV-2 related coronaviruses in
Malayan pangolins
Early release - high contagiousness and rapid spread
of severe acute respiratory syndrome coronavirus 2 Volume 26, Number 7-July 2020

Virology
J of Hospital 6/2/20
Infection
SSRN
Nature
Emerging
Infectious
Diseases

Review

2

10/3/20

Preprint

3

26/3/20

Article

4

7/4/20

Research

5

Although the top five articles for Dimensions were published in 2020, by 21 March they had
all been cited at least 200 times in Dimensions (Figure 3), perhaps mainly by preprints, letters
and short form fast publishing formats, such as brief communications, (academic) news, and
case reports. All five documents exhibit a reasonably steady rate of increase. The
simultaneous jumps in the lines presumably reflect weekly large scale database refreshing for
Dimensions, although there were also smaller daily changes.

Figure 3. The cumulative number of Dimensions citations for the five most cited COVID-19
documents.

14
The top five Mendeley documents also started 21 March with a high number of readers, but
almost five times more than the number of Dimensions citations (Figure 4). There was a
similar pattern of steadily increasing numbers of Mendeley readers with periodic
interruptions. In this case the interruptions resulted in temporary decreases in the numbers
of Mendeley readers. This could be due to two factors. Either the database consolidates
weekly, such as by merging duplicates, or its search is somehow weakened periodically so
that the free text search (which is submitted in parallel with the DOI search) matches less
documents. It is not possible to check which is correct from the data since Mendeley reports
reader counts but not the identities of these readers.

Figure 4. The cumulative number of Mendeley readers for the five most read COVID-19
documents.
Twitter shows a very different pattern to Dimensions and Mendeley. First, some of the
documents are much younger, published during the date range analysed. Second, the number
of tweeters achieves close to its maximum when first found by Dimensions, although this is
not necessarily the original publication date.

15

Figure 5. The cumulative number of Tweeters for the five most tweeted COVID-19 documents.
Facebook has a similar growth pattern to Twitter, except that there is a period of increasing
interest for the proximal origin paper (Figure 6), which has a more moderate growth on
Twitter. The (apparently speculative) news story about CERN scientists that was popular on
Facebook did not get traction on Twitter and seems unlikely to be much cited or read.

Figure 6. The cumulative number of Facebook wall posts for the five most walled COVID-19
documents.
The top news-cited articles were all covered by at least 400 news sources by the end of the
period (Figure 7). Perhaps surprisingly, given that news is very time-dependant, all the sources

16
experienced significant increases in the number of citing sources. Either Altmetric.com is
constantly expanding its coverage of news sources (which is possible, but seems unlikely) or
news stories about COVID-19 are prepared to cite old articles, perhaps for a more in-depth
commentary or as background context for new articles.

Figure 7. The number of news citations for the five most News-cited COVID-19 documents.
There were relatively few citations from Reddit, despite its use as a news source and many
academic themes (subreddits) within the site (Figure 8). Perhaps reflecting its news status,
older articles do not seem to increase their Reddit citation counts.

Figure 8. The cumulative number of Reddit posts for the five most posted COVID-19
documents.

17

A comparison between average scores for different sources
The 24th March was selected for a time series analysis because this date in the first week had
the most new articles (349) found by Dimensions. For documents first found by Dimensions
on 24 March 2020 and matching the COVID-19 queries, the average score was highest for
Twitter and already above 1 on the start day (Figure 9). Average tweeter counts then
increased slowly after the first few days. In contrast, average Mendeley reader counts for
these 349 articles started close to zero and increased rapidly, except for weekly Mendeley
indexing adjustments. Mendeley overtook Twitter after a week.

Figure 9. Daily average (geometric mean) citations by source for documents first found by
Dimensions on 24 March 2020 (n=349).
The average citation counts for the remaining three sources were all much lower than for
Mendeley and Twitter (Figure 10). Whilst Facebook and Reddit both displayed a similar
growth pattern to Twitter (rapid initially, then slow), both News citations and Dimensions
citations increased steadily. The average number of citations after three and half weeks is
surprising for Dimensions, as is the constant growth for News sources.

18

Figure 10. Daily average (geometric mean) citations by source for documents first found by
Dimensions on 24 March 2020, excluding Mendeley and Twitter (n=349).

Overlaps in citation counts between sources
Spearman correlation tests reveal the extent to which the same documents that are cited by
one source are also cited by another source, together with the extent that they are cited. By
April 18, 2020, correlations between Dimensions citations and altmetrics for documents first
found by Dimensions in March 24 were strong, except for Reddit (Table 7). Since most (229;
66%) documents were uncited by 18 April, the correlation mainly confirms that, except for
Reddit, news stories, publishing authors, and users of the different platforms tended to select
the same documents for attention. The altmetrics also correlated moderately or strongly with
each other, except for Reddit, in agreement with this conclusion. Thus, for the narrow topic
of COVID-19, there seems to be a researcher-news-social media consensus about the most
important topics, at least in the (very) short term.
The correlations (Table 8) do not take into account field differences or document type
differences. The relatively high correlations could be at least partially due to ignoring
contributions of low relevance to COVID-19, such as book chapters mentioning the possibility
of a coronavirus 2, editorials, letters, and subject areas making relatively peripheral
contributions to immediate needs.
Table 8. Spearman correlations between citation counts and altmetrics from 18 April 2020 for
COVID-19 documents first found by Dimensions on 24 March 2020. All are statistically
significant at p=0.001 (n=349).
Mendeley Twitter
Dimensions .653***
.659***
Mendeley
1 .689***
Twitter
1
Facebook
News

Facebook
.453***
.375***
.411***
1

***Statistically significant at p=0.001

News
.529***
.473***
.626***
.376***
1

Reddit
.249***
.354***
.363***
.251***
.335***

19

The influence of non-article document types on the correlations were tested by filtering out
all non-articles. After manually removing documents that were not journal articles (mainly
editorials, news, and letters), there were 106 standard journal articles (including reviews).
Nevertheless, the correlations did not substantially change (Table 9). Some of the editorials
were cited, explaining the lack of change. The positive correlations seemed to be due to
articles with a stronger focus on COVID-19 being more noticed, whereas articles with a weaker
focus on COVID-19 or giving weaker evidence were less noticed. Thus, both altmetrics and
citations seem to focus on contributions that are more core to COVID-19 as a medical and
public health issue.
The two most common Dimensions subject codes for the March 24 set were 1117
Public Health and Health Services (n=78) and 1103 Clinical Sciences (n=32). Except for Reddit
(correlations close to 0), the pairwise correlations change little if the set is restricted to only
subject categories 1117 or only 1103, with or without excluding non-article types. For
example, the lowest correlation between Twitter and Dimensions for any of these four
restricted sets is 0.638 (category 1103 with all document types, n=32). Thus, except for Reddit,
the strong positive correlations between indicators do not seem to be due to field differences
in the dataset.
Table 9. Spearman correlations between citation counts and altmetrics from 18 April 2020 for
COVID-19 journal articles first found by Dimensions on 24 March 2020. (n=106).
Mendeley Twitter
Dimensions 0.693*** 0.734***
Mendeley
1
0.687***
Twitter
1
Facebook
News

Facebook
0.589***
0.401***
0.562***
1

News
0.585***
0.473***
0.719***
0.440***
1

Reddit
0.250**
0.316***
0.382***
0.215*
0.334***

*Statistically significant at p=0.05; **Statistically significant at p=0.01; ***Statistically
significant at p=0.001

Early altmetrics and later citation counts
Ideally, an indicator would help researchers and policy makers to identify important articles
when they are first published, without having to wait for enough citations. To check for early
evidence of later citation impact, the indicators were correlated with Dimensions citation
counts on April 18, representing longer term citation counts (this is a weak proxy since
decades are sometimes used for long term citations in other contexts, e.g., Stegehuis, Litvak,
& Waltman, 2015).
On the day that a document is first findable in Dimensions, its tweeter count is the
best indicator of likely long-term citation impact (Figure 11). Twitter users seem to be able to
notice documents approximately on the date of first publication for their potential
importance to COVID-19. After this date, the tweeter count does not increase much and its
correlation with longer term Dimensions citations is stable. After about three weeks,
Mendeley reader counts take over as a marginally better indicator of longer-term citation
impact. It is not clear whether the same would be true for more mature citation counts,
however, such as after a year. It is possible that early Dimensions citations (and Mendeley
readers) reflect more temporary interest and are themselves highly influenced by the news
or social sharing on Twitter, for example. The most cited sets of five papers analysed above

20
suggest that highly recognised papers are particularly important for the disease, however. As
above, this correlation ignores field differences and document type differences, although
document differences seem to have little effect (Tables 8, 9).

Figure 11. Spearman correlations between altmetrics and Dimensions citation counts from 18
April for COVID-19 documents first found by Dimensions on 24 March 2020 (n=349).
Correlations with Dimensions citation counts on the same dates are also reported for context.

Discussion
The results are limited by the range of factors mentioned in the Methods section. In particular,
the coverage figures for the sources are not directly comparable due to the different scopes
of the queries. In addition, the count data has not been field-normalized so the coverage
comparisons do not reveal disciplinary differences. The correlations may also be exaggerated
by not taking into account disciplinary differences. The results may show different patterns
for earlier or later time periods. The properties of the scholarly databases and Altmetric.com’s
strategies may evolve over time, rendering the results obsolete. They may also not be
applicable for later stages of COVID-19 research or for future epidemics or pandemics.
The COVID-19 query results comparison confirm the previous finding that COVIDrelated academic databases are appearing rapidly many different databases (Torres-Salinas,
2020). In addition, they confirm that Dimensions finds many publications not in Scopus and
WoS but that Scopus indexes nearly all relevant publications found in the WoS core collection
with the Conference Proceedings Citation Index. Presumably the difference would be smaller
if other parts of WoS were included, such as the Book Citation Index, although the core
collection includes the Emerging Sources Citation Index (Clarivate, 2020).
The results are not directly comparable to studies from before COVID-19 due to the
unprecedented speed and volume of publishing on the topic. For example, Dimensions
citation counts accrue more rapidly than previously reported for any topic. For comparison,
the Scopus citations of 12 subject categories (full journal articles only) were a maximum of
0.12 in the month of publication, whereas the COVID-19 mixed set averaged almost double
this after three weeks. The results are also qualitatively different in some respects. Whilst

21
correlation tests have previously found tweeter counts to have little value as a scholarly
impact indicator due to very low correlation with citation counts (Costas, Zahedi, & Wouters,
2015; Haustein, Larivière, Thelwall, Amyot, & Peters, 2014; Thelwall, Haustein, Larivière, &
Sugimoto, 2013), the current study has found Tweet counts to be reasonable academic impact
indicators and the best early impact indicator for the first three weeks. This may be partly due
to the set of articles here covering multiple disciplines, but the results for the top-cited
documents suggest that altmetrics are effective at pointing to the documents that are most
central to COVID-19 as a medical and public health issue. Thus, the unprecedented threat of
COVID-19 seems to have led to an unprecedentedly high and focused level of societal and
academic attention being given to the most relevant research.

Conclusions
The confirmed rapid increase in COVID-19 academic publications is encouraging in terms of
the academic community rapidly reacting to the need for relevant research and
commentaries. The importance of short form and quick contributions (viewpoints,
correspondence, brief reports) is also evident in the highly cited papers, as is the importance
of academic research for practical public health issues.
Despite the apparent high medical and public health value of some academic papers,
the huge number of publications returned by a relevant search will presumably make the
most important publications more difficult to find. This should not be a problem for medical
researchers trained to use MeSH queries effectively, but might be problematic for other
researchers, end users and the public, who may find bewilderingly many matches for their
queries. The altmetric results suggest that altmetrics may be helpful for researchers needed
to quickly identify the most useful new documents from the large number published daily.
Altmetric counts may help to distinguish between core primary research and other
contributions, such as editorial commentaries with narrower disciplinary or professional
relevance (e.g., radiographers). Perhaps ironically, given that a core original goal for altmetrics
was to develop indicators of societal impact that were different from scholarly impact
indicators (Priem, Taraborelli, Groth, & Neylon, 2010), their greatest value (as early impact
indicators) seems be occurring when the two concepts are most closely converging.

References
Adie, E., & Roe, W. (2013). Altmetric: Enriching scholarly content with article‐level discussion
and metrics. Learned Publishing, 26(1), 11-17.
Allen
Institute
(2020).
COVID-19
Open
Research
Dataset
(CORD-19).
https://pages.semanticscholar.org/coronavirus-research
Chen, Q., Allot, A., & Lu, Z. (2020). Keep up with the latest coronavirus research. Nature,
579(7798), 193-193.
Clarivate
(2020).
Web
of
Science:
Emerging
Sources
Citation
Index.
https://clarivate.com/webofsciencegroup/solutions/webofscience-esci/
Clark, F., & Illman, D. L. (2006). A longitudinal study of the New York Times Science Times
section. Science Communication, 27(4), 496-513.
Costas, R., Zahedi, Z., & Wouters, P. (2015). Do “altmetrics” correlate with citations? Extensive
comparison of altmetric indicators with citations from a multidisciplinary perspective.
Journal of the Association for Information Science and Technology, 66(10), 2003-2019.

22
de Solla Price, D. (1976). A general theory of bibliometric and other cumulative advantage
processes. Journal of the American society for Information science, 27(5), 292-306.
Delwiche, F. A. (2018). Bibliometric analysis of scholarly publications on the Zika virus, 1952–
2016. Science & Technology Libraries, 37(2), 113-129.
Fairclough, R., & Thelwall, M. (2015). More precise methods for national research citation
impact comparisons. Journal of Informetrics, 9(4), 895-906.
Gunn, W. (2014). Mendeley: Enabling and understanding scientific collaboration. Information
Services & Use, 34(1-2), 99-102.
Harzing, A. W. (2019). Two new kids on the block: How do Crossref and Dimensions compare
with Google Scholar, Microsoft Academic, Scopus and the Web of Science?
Scientometrics, 120(1), 341-349.
Haustein, S., Larivière, V., Thelwall, M., Amyot, D., & Peters, I. (2014). Tweets vs. Mendeley
readers: How do these two social media metrics differ? IT-Information Technology,
56(5), 207-215.
Herzog, C., Hook, D., & Konkiel, S. (2020). Dimensions: Bringing down barriers between
scientometricians and data. Quantitative Science Studies, 1(1), 387-395.
Ioannidis, J. P. (2020). Coronavirus disease 2019: the harms of exaggerated information and
non‐evidence‐based measures. European Journal of Clinical Investigation.
https://doi.org/10.1111/eci.13222
Kostoff, R. N., & Morse, S. A. (2011). Structure and infrastructure of infectious agent research
literature: SARS. Scientometrics, 86(1), 195-209.
Kousha, K. & Thelwall, M. (2019b). An automatic method to identify citations to journals in
news stories: A case study of the UK newspapers citing Web of Science journals, Journal
of Data and Information Science, 4(3), 73-95.
Kousha, K., & Thelwall, M. (2019a). Can Google Scholar and Mendeley help to assess the
scholarly impacts of dissertations? Journal of Informetrics, 13(2), 467-484.
Lewison, G. (2008). The reporting of the risks from severe acute respiratory syndrome (SARS)
in the news media, 2003–2004. Health, Risk and Society, 10(3), 241-262.
medRxiv (2020). COVID-19 SARS-CoV-2 preprints from medRxiv and bioRxiv
https://connect.medrxiv.org/relate/content/181
Merchant, R. M., & Lurie, N. (2020). Social media and emergency preparedness in response
to novel coronavirus. JAMA. doi:10.1001/jama.2020.4469
Mohammadi, E., Barahmand, N., & Thelwall, M. (2019). Who shares health and medical
scholarly articles on Facebook? Learned Publishing, 33(1), 111–118.
Mohammadi, E., Gregory, K., Thelwall, M., Barahmand, N. (2020). Which health and
biomedical topics generate the most Facebook interest and the strongest citation
relationships? Information Processing and Management, 57(3).
Mohammadi, E., Thelwall, M. & Kousha, K. (2016). Can Mendeley bookmarks reflect
readership? A survey of user motivations. Journal of the Association for Information
Science and Technology, 67(5), 1198-1209.
Mohammadi, E., Thelwall, M., Haustein, S., & Larivière, V. (2015). Who reads research
articles? An altmetrics analysis of Mendeley user categories. Journal of the Association
for Information Science and Technology, 66(9), 1832-1846.
Mohammadi, E., Thelwall, M., Kwasny, M., & Holmes, K. (2018). Academic information on
Twitter: A user survey. PLOS ONE, 13(5): e0197265.
Orduña-Malea, E. & Delgado-López-Cózar, E. (2018). Dimensions: re-discovering the
ecosystem of scientific information. El Profesional de la Información, 27(2), 420-431.

23
Ortega, J. L. (2018). Reliability and accuracy of altmetric providers: a comparison among
Altmetric. com, PlumX and Crossref Event Data. Scientometrics, 116(3), 2123-2138.
Ovadia, S. (2015). More than just cat pictures: Reddit as a curated news source. Behavioral &
Social Sciences Librarian, 34(1), 37-40.
PMC
(2020).
Public
Health
Emergency
COVID-19
Initiative.
https://www.ncbi.nlm.nih.gov/pmc/about/covid-19/
Pouris, A., & Ho, Y. -. (2016). A bibliometric analysis of research on Ebola in Science Citation
Index
expanded.
South
African
Journal
of
Science,
112(3-4).
doi:10.17159/sajs.2016/20150326
Pouris, A., & Pouris, A. (2011). Scientometrics of a pandemic: HIV/AIDS research in South
Africa and the world. Scientometrics, 86(2), 541-552.
Priem, J., Taraborelli, D., Groth, P., & Neylon, C. (2010). Altmetrics: A manifesto.
http://altmetrics.org/manifesto/
Rethlefsen, M. L., & Livinski, A. A. (2013). Infectious diseases citation patterns: mapping the
literature 2008–2010. Journal of the Medical Library Association: JMLA, 101(1), 55.
Robinson-García, N., Torres-Salinas, D., Zahedi, Z., & Costas, R. (2014). New data, new
possibilities: exploring the insides of Altmetric.com. El Profesional de la Información,
23(4), 359-366.
Song, P., & Karako, T. (2020). COVID-19: Real-time dissemination of scientific information to
fight a public health emergency of international concern. BioScience Trends, 14(1).
doi:10.5582/BST.2020.01056
Stegehuis, C., Litvak, N., & Waltman, L. (2015). Predicting the long-term citation impact of
recent publications. Journal of informetrics, 9(3), 642-657.
Stoddard, G. (2015). Popularity and quality in social news aggregators: A study of reddit and
hacker news. In Proceedings of the 24th international conference on world wide web
(pp. 815-818).
Sud, P., & Thelwall, M. (2014). Evaluating altmetrics. Scientometrics, 98(2), 1131-1143.
Thelwall, M. & Wilson, P. (2016). Mendeley readership altmetrics for medical articles: An
analysis of 45 fields, Journal of the Association for Information Science and Technology,
67(8), 1962-1972.
Thelwall, M. (2017a). Are Mendeley reader counts useful impact indicators in all fields?
Scientometrics, 113(3), 1721–1731.
Thelwall, M. (2017b). Are Mendeley reader counts high enough for research evaluations when
articles are published? Aslib Journal of Information Management, 69(2), 174-183.
doi:10.1108/AJIM-01-2017-0028
Thelwall, M. (2017c). Why do papers have many Mendeley readers but few Scopus-indexed
citations and vice versa? Journal of Librarianship & Information Science, 49(2), 144-151.
Thelwall, M. (2018a). Early Mendeley readers correlate with later citation counts..
Scientometrics, 115(3), 1231–1240.
Thelwall, M. (2018b). Dimensions: A competitor to Scopus and the Web of Science? Journal
of Informetrics, 12(2), 430–435.
Thelwall, M. Tsou, A., Weingart, S., Holmberg, K., & Haustein, S. (2013). Tweeting links to
academic articles, Cybermetrics, 17(1). https://digital.csic.es/handle/10261/174572
Thelwall, M., Haustein, S., Larivière, V., & Sugimoto, C. R. (2013). Do altmetrics work? Twitter
and
ten
other
social
web
services.
PloS
One,
8(5),
e64841.
doi:10.1371/journal.pone.0064841

24
Tian, D., & Zheng, T. (2015). Emerging infectious disease: Trends in the literature on SARS and
H7N9 influenza. Scientometrics, 105(1), 485-495.
Torres-Salinas, D. (2020). Ritmo de crecimiento diario de la producción científica sobre Covid19. Análisis en bases de datos y repositorios en acceso abierto. El Profesional de la
Información, 29(2), e290215. https://doi.org/10.3145/epi.2020.mar.15
Van Noorden, R. (2014). Online collaboration: Scientists and the social network. Nature news,
512(7513), 126-130.
Wallace, M. L., Larivière, V., & Gingras, Y. (2009). Modeling a century of citation distributions.
Journal of Informetrics, 3(4), 296-303.
WHO
(2020).
Rolling
updates
on
coronavirus
2019.
https://www.who.int/emergencies/diseases/novel-coronavirus-2019/events-as-theyhappen
Yu, H., Xu, S., Xiao, T., Hemminger, B. M., & Yang, S. (2017). Global science discussed in local
altmetrics: Weibo and its comparison with Twitter. Journal of Informetrics, 11(2), 466482.
Zahedi, Z., Haustein, S., & Bowman, T. (2014). Exploring data quality and retrieval strategies
for Mendeley reader counts. In SIG/MET Workshop, ASIS&T 2014 Annual Meeting,
Seattle. http://www. asis.org/SIG/SIGMET/data/uploads/sigmet2014/zahedi.pdf

