1

A Compressed Sensing Approach to Pooled RT-PCR Testing for
COVID-19 Detection

arXiv:2005.07895v2 [q-bio.QM] 29 Apr 2021

Sabyasachi Ghosh†, Rishi Agarwal †, Mohammad Ali Rehan †, Shreya Pathak †,
Pratyush Agarwal †, Yash Gupta †, Sarthak Consul ∗, Nimay Gupta †, Ritika †, Ritesh Goenka †,
Ajit Rajwade †, Manoj Gopalkrishnan ∗

We propose ‘Tapestry’, a novel approach to pooled testing with application to COVID-19 testing with quantitative Reverse
Transcription Polymerase Chain Reaction (RT-PCR) that can result in shorter testing time and conservation of reagents and testing
kits. Tapestry combines ideas from compressed sensing and combinatorial group testing with a novel noise model for RT-PCR used
for generation of synthetic data. Unlike Boolean group testing algorithms, the input is a quantitative readout from each test and the
output is a list of viral loads for each sample relative to the pool with the highest viral load. While other pooling techniques require a
second confirmatory assay, Tapestry obtains individual sample-level results in a single round of testing, at clinically acceptable false
positive or false negative rates. We also propose designs for pooling matrices that facilitate good prediction of the infected samples
while remaining practically viable. When testing n samples out of which k ≪ n are infected, our method needs only O(k log n)
tests when using random binary pooling matrices, with high probability. However, we also use deterministic binary pooling matrices
based on combinatorial design ideas of Kirkman Triple Systems to balance between good reconstruction properties and matrix
sparsity for ease of pooling.
√ A lower bound on the number of tests with these matrices for satisfying a sufficient condition for
guaranteed recovery is k n. In practice, we have observed the need for fewer tests with such matrices than with random pooling
matrices. This makes Tapestry capable of very large savings at low prevalence rates, while simultaneously remaining viable even
at prevalence rates as high as 9.5%. Empirically we find that single-round Tapestry pooling improves over two-round Dorfman
pooling by almost a factor of 2 in the number of tests required. We describe how to combine combinatorial group testing and
compressed sensing algorithmic ideas together to create a new kind of algorithm that is very effective in deconvoluting pooled tests.
We validate Tapestry in simulations and wet lab experiments with oligomers in quantitative RT-PCR assays. An accompanying
Android application Byom Smart Testing makes the Tapestry protocol straightforward to implement in testing centres, and is made
available for free download. Lastly, we describe use-case scenarios for deployment.
Index Terms—Compressed sensing, coronavirus, COVID-19, group testing, Kirkman/Steiner triples, mutual coherence, pooled
testing, sensing matrix design.

I. I NTRODUCTION
The coronavirus disease of 2019 (COVID-19) crisis has led
to widespread lockdowns in several countries, and has had a
major negative impact on the economy. Early identification of
infected individuals can enable quarantining of the individuals
and thus control the spread of the disease. Such individuals
may often be asymptomatic for many days. Widespread testing
with the RT-PCR (reverse transcription polymerase chain
reaction) method can help identify the infected individuals.
However, widespread testing is not an available option in many
countries due to constraints on resources such as testing time
(∼ 3 − 4 hours per round), basic equipment, skilled manpower
and reagents.
The current low rate of COVID-19 infection in the world
population [1] means that most samples tested are not infected,
so that most tests are wasted on uninfected samples. Group
testing is a process of pooling together samples of n different
people into multiple pools, and testing the pools instead of
each individual sample. A negative result on a pool implies
that all samples participating in it were negative. This saves a
huge amount of testing resources, especially with low infection
AR acknowledges support from SERB Matrics grant MTR/2019/000691.
AR and MG acknowledge support from IITB WRCB grant #10013976, and
DST-Rakshak grant #10013980.
†Dept. of Computer Science & Engg., IIT Bombay, India; {sghosh,
ajitvr}@cse.iitb.ac.in
∗ Dept. of Electrical Engineering, IIT Bombay; manojg@ee.iitb.ac.in
This work has been accepted for publication at the IEEE Open Journal of
Signal Processing.

rates. Group testing for medical applications has a long history
dating back to the 1940s when it was proposed for testing
of blood samples for syphilis [2]. Simple two-round group
testing schemes have already been applied in the field by
several research labs [3], [4] for COVID-19 testing. Such tworound group testing schemes require pooling of samples and
a second round of sample handling for all samples in positive
pools. This second round of sample handling can increase the
time to result and be laborious to perform since it requires the
technician to wear PPE one more time, do another round of
RNA extraction, and PCR. In situations where the result needs
to be delivered fast, a second round of sample handling and
testing must be avoided. In such situations, these schemes are
less attractive.
We present Tapestry, a novel combination of ideas from
combinatorial group testing and compressed sensing (CS) [5]
which uses the quantitative output of PCR tests to reconstruct
the viral load of each sample in a single round. Tapestry has
been validated with wet lab experiments with oligomers [6].
In this work, we elaborate on the results from the algorithmic
perspective for the computer science and signal processing
communities. Tapestry has a number of salient features which
we enumerate below.
1) Tapestry delivers results in a single round of testing, without
the need for a second confirmatory round, at clinically acceptable false negative and false positive rates. The number
m of required tests is only O(k log n) for random binary

2

2)

3)

4)

5)

6)
7)

pooling matrix constructions, as per compressed sensing
theory for random binary matrices [7]. In the targeted
use cases where the number of infected samples k ≪ n,
we see that m ≪ n. However, our deterministic pooling
matrix constructions based on Kirkman Triple Systems [8],
[9] require fewer tests in practice (see Sec. III-F8 for a
discussion on why this may be the case). Consequently we
obtain significant savings in testing time and resources such
as number of tests, quantity of reagents, and manpower.
Tapestry reconstructs relative viral loads i.e., ratio of viral
amounts in each sample to the highest viral amount across
pools. It is believed that super-spreaders and people with
severe symptoms have higher viral load [10], [11], so
this quantitative information might have epidemiological
relevance.
Tapestry takes advantage of quantitative information in
PCR tests. Hence it returns far fewer false positives than
traditional binary group testing algorithms such as C OMP
(Combinatorial Orthogonal Matching Pursuit)[12], while
maintaining clincally acceptable false negative rates. Furthermore, it takes advantage of the fact that a negative pool
has viral load exactly zero. Traditional CS algorithms do not
take advantage of this information. Hence, Tapestry demonstrates better sensitivity and specificity than CS algorithms.
Because each sample is tested in three pools, Tapestry can
detect some degree of noise in terms of cross-contamination
of samples and pipetting errors.
Tapestry allows PCR test measurements to be noisy. We
develop a novel noise model to describe noise in PCR
experiments. Our algorithms are tested on this noise model
in simulation.
All tuning parameters for execution of the algorithms are
inferred on the fly in a data driven fashion.
Each sample contributes to exactly three pools, and each
pool has the same number of samples. This simplifies the
experimental design, conserves samples, keeps pipetting
overhead to a minimum, and makes sure that dilution due
to pool size is in a manageable regime.

The organization of the paper is as follows. We first present a
brief overview of the RT-PCR method in Sec. II. The precise
mathematical definition of the computational problem being
solved in this paper is then put forth in Sec. III-A. We describe
traditional and CS-based group-testing algorithms for this
problem in Sec. III-B, III-C and III-D. The Tapestry method is
described in Sec. III-D. The sensing matrix design problem, as
well as theoretical guarantees using Kirkman Triple Systems
or random binary matrices, are described in Sec. III-F. Results
on synthetic data are presented in Sec. IV. This is followed by
results on data from lab experiments performed with oligomers
to mimic the clinical situation as closely as possible. In Sec.
V, we compare our work to recent related approaches. We
conclude in Sec. VI with a glance through different scenarios
where our work could be deployed. The supplemental material
contains several additional experimental details as well as
proofs of some theoretical results.

II. RT-PCR M ETHOD
We present here a brief summary of the RT-PCR process,
referring to [13] for more details. In the RT-PCR method for
COVID-19 testing, a sample in the form of naso- or oropharyngeal swabs is collected from a patient. The sample is
then dispersed into a liquid medium. The RNA molecules of
the virus present in this liquid medium are converted into
complementary DNA (cDNA) via a process called reverse
transcription. DNA fragments called primers complementary
to cDNA from the viral genome are then added. They attach
themselves to specific sections of the cDNA from the viral
genome if the virus is present in the sample. The cDNA
of these specific viral genes then undergoes a process of
exponential amplification in an RT-PCR machine. Here, cDNA
is put through several cycles of alternate heating and cooling
in the presence of Taq polymerase and appropriate reagents.
This triggers the creation of many new identical copies of
specific portions of the target DNA, roughly doubling in
number with every cycle of heating and cooling. The reaction volume contains sequence-specific fluorescent markers
which report on the total amount of amplified DNA of the
appropriate sequence. The resulting fluorescence is measured,
and the increase can be observed on a computer screen in
real time. The time when the amount of fluorescence exceeds
the threshold level is known as the threshold cycle Ct , and
is a quantitative readout from the experiment. A smaller
Ct indicates greater number of copies of the virus. Usually
Ct takes values anywhere between 16 to 32 cycles in real
experiments. PCR can detect even single molecules. A single
molecule typically would have Ct value of around 40 cycles.
A typical RT-PCR setup can test 96 samples in parallel. The
test takes about 3-4 hours to execute.
III. T ESTING M ETHODS
A. Statement of the Computational Problem
Let x denote a vector of n elements where xi is the viral
load (i.e. viral amount) of the ith person. Throughout this paper
we assume that only one sample per person is extracted. Hence
x contains the viral loads corresponding to n different people.
Note that xi = 0 implies that the ith person is not infected.
Due to the low infection rate for COVID-19 as yet even in
severely affected countries [1], x is considered to be a sparse
vector with at the most k ≪ n positive-valued elements. In
group testing, small and equal volumes of the samples of a
subset of these n people are pooled together according to a
sensing or pooling matrix A = (Aji )m×n whose entries are
either 0 or 1. The viral loads of the pools will be given by:
zj =

n
X
i=1

Aji xi = Aj x, 1 ≤ j ≤ m, 1 ≤ i ≤ n,

(1)

where Aji = 1 if a portion of the sample of the ith person is
included in the j th pool, and Aj is the j th row of A. In all,
some m < n pools are created and individually tested using
RT-PCR. We now have the relationship z = Ax, where z is
the m-element vector of viral loads in the mixtures, and A
denotes a m × n binary ‘pooling matrix’ (also referred to as

3

a ‘sensing matrix’ in CS literature). Note that each positive
RT-PCR test will yield a noisy version of zj , which we refer
to as yj . The relation between the ‘clean’ and noisy versions
is given as follows (also see Eqn. 7):
yj = zj (1 + q)ej = (1 + q)ej Aj x,

(2)

where ej ∼ N (0, σ 2 ) and q ∈ (0, 1) is the fraction of viral
cDNA that replicates in each cycle. The factor (1 + q)ej
reflects the stochasticity in the growth of the numbers of
DNA molecules during PCR. Here σ is known and constant.
Equivalently for positive tests, we have:
j

log yj = log(A x) + log(1 + q)ej .

(3)

In case of negative tests, yj as well as zj are 0-valued, and no
logarithms need be computed. In non-adaptive group testing,
the core computational problem is to estimate x given y and A
without requiring any further pooled measurements. It should
be noted that though we have treated each element of x to be
a fixed quantity, it is in reality a random variable of the form
xi ∼ Poisson(λi ) where λi ≥ 0. If matrix A contains only
ones and zeros, this implies that zj ∼ Poisson(Aj x) because
the sum of Poisson random variables is also a Poisson random
variable.
1) Derivation of Noise Model
For a positive pool j, the quantitative readout from RT-PCR
is not its viral load but the observed cycle time tj when its
fluorescence reaches a given threshold F (see Sec. II). In order
to be able to apply CS techniques (see Sec. III-C), we derive
a relationship between the cycle time of a sample and its viral
load. Because of exponential growth (see [14]), the number of
molecules of viral cDNA in pool j at cycle time t, denoted
by vj (t) is given by:
vj (t) = zj (1 + q)t .

(4)

Also, t is a real number, with ⌊t⌋ indicating the number of PCR
cycles that have passed, and t − ⌊t⌋ indicating the fraction of
wall-clock time within the current cycle. The fluorescence of
the pool, fj (t), is directly proportional to the number of virus
molecules vj (t). That is,
fj (t) = Kvj (t) = Kzj (1 + q)t ,

(5)

where K is a constant of proportionality. Suppose the fluorescence of pool j should reach the threshold value F at cycle
time τj , according to Eqn. 5. Due to the stochastic nature of the
reaction, as well as measurement error in the PCR machine,
the threshold cycle output by the machine will not reflect this
true cycle time. We model this discrepancy as Gaussian noise.
Hence, the true cycle time τj and the observed cycle time tj
are related as τj = tj + ej , where ej ∼ N (0, σ 2 ) as before.
Now, since fj (τj ) = F , using Eqn. 5, we have
F = Kzj (1 + q)τj = Kyj (1 + q)tj .

(6)

The latter equality is since we use the noisy cycle threshold
tj to compute viral load, where yj is defined to be the noisy
viral load of pool j. Hence we find
yj = zj (1 + q)τj −tj = zj (1 + q)ej = (1 + q)ej Aj x,

(7)

obtaining the relationship from Eqn. 2.
Constants F and K are unknown. Hence it is not possible to
directly obtain yj from tj without additional machine-specific
calibration. However, we can find the ratio between the noisy
viral loads of two pools using Eqn. 6. Let ymin be the noisy
viral load of the pool with the minimum observed threshold
cycle (tmin ) among all pools. Then we define relative viral
loads as:
yj
zj
x
e=
yej =
= (1 + q)tmin −tj , zej =
,x
(8)
ymin
ymin
ymin
where zej is the relative viral load of a pool, yej is its noisy
e is the vector of relative viral loads of each
version, and x
sample. We note that due to Eqn. 7, the following relation
holds:
e,
(9)
yej = zej (1 + q)ej = (1 + q)ej Aj x

Hence we can apply CS techniques from Sec. III-C to determine the relative magnitudes of viral loads without knowing
F and K. We provide more comments about the settings of
various noise model parameters for our experiments, in Sec.
IV, particularly in Sec. IV-A6.
B. Combinatorial Group-Testing

Combinatorial Orthogonal Matching Pursuit (C OMP) is a
Boolean nonadaptive group testing method [15, Sec. 2.3]. Here
one uses the simple idea that if a mixture yej tests negative
then any sample x
ei for which Aji = 1 must be negative.
Note that pools which test negative are regarded as noiseless
observations, as argued in Sec. III-A1. The other samples are
all considered to be positive. This algorithm guarantees that
there are no ‘false negatives’. However it can produce a very
large number of ‘false positives’. For example, a sample x
ek
will be falsely reported to be positive if every mixture yej it
is part of, also contains at least one other genuinely positive
sample. The C OMP algorithm is largely insensitive to noise.
Moreover a small variant of it can also produce a list of ‘high
confidence positives’, after identifying the (sure) negatives.
This happens when a positive mixture yej contains only one
sample x
ei , not counting the other samples which were declared
sure negatives in the earlier step. Such a step of identifying
‘high confidence positives’ is included in the so-called Definite
Defectives (D D) Algorithm [15, Sec. 2.4]. However D D labels
all remaining items to be negative, potentially leading to a
large number of false-negatives. The performance guarantees
for C OMP have been analyzed in [12] and show that C OMP
requires ek(1 + δ) log n tests for an error probability less than
n−δ (see Sec. III-F8). This analysis has been extended to
include the case of noisy test results as well [12]. However
C OMP can result in a large number of false positives if not
enough tests are used, and it also does not predict viral loads.
C. Compressed Sensing for Pooled Testing
Group testing is intimately related to the field of compressed
sensing (CS) [16], which has emerged as a significant sub-area
of signal and image processing [5], with many applications in
biomedical engineering [17], [18], [19]. In CS, an image or a
signal x with n elements, is directly acquired in compressed

4

format via m linear measurements of the form y = Ax + η.
Here, the measurement vector y has m elements, and A is
a matrix of size m × n, and η is a vector of noise values.
If x is a sparse vector with k ≪ n non-zero entries, and A
obeys the so-called restricted isometry property (RIP), then
exact recovery of x from y, A is possible [20] if η = 0. In
the case of measurement noise, the recovery of x produces
a solution that is provably close to the original x. A typical
recovery problem P0 consists of optimizing the following cost
function:
minkxk0 s.t. ky − Axk2 ≤ ε,
(10)
where ε is an upper bound (possibly a high probability upper
bound) on kηk2 , and kxk0 is the number of non-zero elements
in x. In the absence of noise, a unique and exact solution to
this problem is possible with as few as 2k measurements in
y if x has k non-zero elements [20]. Unfortunately, this optimization problem P0 is NP-Hard and the algorithm requires
brute-force subset enumeration. Instead, the following problem
P1 (often termed ‘Basis Pursuit Denoising’ or B PDN) is solved
in practice:
minkxk1 s.t. ky − Axk2 ≤ ε.

(11)

P1 is a convex optimization problem which yields the same
solution as the earlier problem (with similar conditions on
x, A) at significantly lower computational cost, albeit with
O(k log n) measurements (i.e. typically greater than 2k) [5],
[20].
The order k restricted isometry constant (RIC) of a matrix A
is defined as the smallest constant δk , for which the following
relationship holds for all k-sparse vectors x (i.e. all vectors
with at the most k non-zero entries): (1−δk )kxk22 ≤ kAxk22 ≤
(1 + δk )kxk22 . The matrix A is said to obey the order k
restricted isometry property (RIP) if δk is close to 0. This
property essentially implies that no k-sparse vector (other
than the zero vector) can lie in the null-space of A. Unique
recovery of k-sparse signals requires that no 2k-sparse vector
lies in the nullspace of A [20]. A matrix A which obeys
RIP of order 2k satisfies this property. It has been proved
that matrices with entries randomly and independently drawn
from distributions such as Rademacher or Gaussian, obey the
RIP of order k with high probability [21], provided they
have at least O(k log n) rows. There also exist deterministic
binary sensing
matrix designs (e.g. [22]) which require
√
O(max(k 2 , n)) measurements. However it has been shown
recently [23] that the constant factors in the deterministic
case are significantly smaller than those in the former random
case when n < 105 , making the deterministic designs more
practical for typically encountered problem sizes. The solution
to the optimization problems P0 and P1 in Eqns. 10 and 11
respectively, are provably robust to noise [5], and the recovery
error decreases with decrease in noise magnitude. The error
bounds for P0 in Eqn. 10 are of the form, for solution x̂ [24]:
ε
ε
√
≤ kx − x̂k2 ≤ √
,
(12)
1 + δ2k
1 − δ2k
whereas those for P1 in Eqn. 11 have the form [24]:
kx − x̂k2 ≤ εζ(δ2k ).

(13)

Here ζ(δ2k ) is a monotonically increasing function of δ2k ∈
(0, 1) and has a small value in practice.
The Restricted Isometry Property as defined above is also
known as RIP-2, because it uses the ℓ2 -norm. Many other
sufficient conditions for recovery of k-sparse vectors exist.
We define the following which we use later in Sec. III-F and
supplemental Sec. S.V to prove theoretical guarantees of our
method.
Definition 1. RIP-1: [25, Defn. 8] A m × n matrix A is said
to obey RIP-1 of order k if ∃ δk ∈ (0, 1) such that for all
k-sparse vectors x ∈ Rn ,
kxk1 ≤ kAxk1 ≤ (1 + δk )kxk1
.
Definition 2. RNSP: [23, Eqn. 12] A m × n matrix A is
said to obey the Robust Nullspace Property (RNSP) of order
k if ∃ ρ < 1 and τ > 0 such that for all x ∈ Rn it holds that
||xS ||2 ≤ ρ||xS̄ ||1 + τ ||Ax||2
for all S ⊂ {1 . . . n} with |S| ≤ k.
Definition 3. ℓ2 -RNSP: [7, Defn. 1] A m × n matrix A is
said to obey the ℓ2 -robust Nullspace Property (ℓ2 -RNSP) of
order k if ∃ ρ ∈ (0, 1) and τ > 0 such that for all x ∈ Rn it
holds that
ρ
||xS ||2 ≤ √ ||xS̄ ||1 + τ ||Ax||2
k
for all S ⊂ {1 . . . n} with |S| ≤ k.
Over the years, a variety of different techniques for compressive recovery have been proposed. We use some of these
for our experiments in Sec. III-D. These algorithms use
different forms of sparsity and incorporate different types of
constraints on the solution.
D. CS and Traditional GT Combined
Algorithm 1 Tapestry Method
Input: n samples, m × n pooling matrix A
1: Perform pooling according to pooling matrix A and create
m pooled samples
2: Run RT-PCR test on these m pooled samples and receive
m × 1 vector of cycle threshold values t
e from t
3: Compute m × 1 vector of relative viral loads y
4: Use C OMP to filter out negative tests and sure negative
samples. Compute submatrix AX̄ ,Ȳ , yeȲ and list HCP
of ‘high-confidence positives’ along with their viral loads
(see Sec. III-B).
eX̄ from
5: Use a CS decoder to recover relative viral loads x
yeȲ , AX̄ ,Ȳ
e by setting its
6: Compute n × 1 relative viral load vector x
e X̄ , and setting remaining entries to 0.
entries from x
e, HCP.
7: return x
The complete pipeline of the Tapestry method is presented
in Algorithm 1. First, a wet lab technician performs pooling

5

of n samples into m pools according to a m × n pooling
matrix A. Then they run the RT-PCR test on these m pools
(in parallel). The output of the RT-PCR tests – the threshold
cycle (Ct ) values of each pool – is processed to find the relative
viral load vector ye of the m pools (as shown in Eqn. 8). This
is given as input to the Tapestry decoding algorithm, which
e.
outputs a sparse relative viral load vector x
The Tapestry decoding algorithm, our approach toward
group-testing for COVID-19, involves a two-stage procedure1.
In the first stage, we apply the C OMP algorithm described in
e to form a
Sec. III-B, to identify the sure negatives (if any) in x
set X . Let Y be the set of zero-valued measurements in ye (i.e.
negative tests). Please refer to Sec. III-A1 for the definition
e, ye. Moreover, we define X¯, Ȳ as the complement-sets
of x
of X , Y respectively. Also, let yȲ be the vector of m − |Y|
measurements which yielded a positive result. Let xX̄ be
the vector of n − |X | samples, which does not include the
|X | surely negative samples. Let AX̄ ,Ȳ be the submatrix
of A, having size (m − |Y|) × (n − |X |), which excludes
rows corresponding to zero-valued measurements in y and
columns corresponding to negative elements in x. In the
eX̄ from
second stage, we apply a CS algorithm to recover x
yeȲ , AX̄ ,Ȳ . To avoid symbol clutter, we henceforth just stick to
the notation y, x, A, m, n, even though they respectively refer
eX̄ , AX̄ ,Ȳ , m − |Y|, n − |X |.
to yeȲ , x
Note that the CS stage following C OMP is very important
for the following reasons:
1) C OMP typically produces a large number of false positives.
The CS algorithms help reduce the number of false positives
as we shall see in later sections.
2) C OMP does not estimate viral loads, unlike CS algorithms.
3) In fact, unlike CS algorithms, C OMP treats the measurements in y as also being binary, thus discarding a lot of
useful information.
4) C OMP preserves the RIP-1, RIP-2, RNSP, and ℓ2 -RNSP
of the pooling matrix, i.e. if A obeys any of RIP-1, RIP2, RNSP or ℓ2 -RNSP of order k, then AX̄ ,Ȳ also obeys
the same property of the same order k with the same
parameters. We formalize and prove these claims in the
supplemental section S.V.
However, the C OMP algorithm prior to applying the CS
algorithm is also very important for the following reasons:
1) Viral load in negative pools is exactly 0. C OMP identifies
the sure negatives in x from the negative measurements
in y. Traditional CS algorithms do not take advantage of
this information, since they assume all tests to be noisy
(Eqns. 10 and 11). It is instead easier to discard the obvious
negatives before applying the CS step.
2) Since C OMP identifies the sure negatives, therefore, it
effectively reduces the size of the problem to be solved
by the CS step from (m, n) to (m − |Y|, n − |X |).
3) In a few cases, a (positive) pool in Ȳ may contain only one
contributing sample in X̄ , after negatives have been eliminated by C OMP. Such a sample is called a ‘high-confidence
positive’, and we denote the list of high-confidence posi1 The two-stage procedure is purely algorithmic. It does not require two
consecutive rounds of testing in a lab.

tives as HCP. In rare cases, the CS decoding algorithms
we employed (see further in this section) did not recognize
such a positive. However, such samples will still be returned
by our algorithm as positives, in the set HCP (see last step
of Alg. 1, and ‘definite defectives’ in Sec. III-B).
For CS recovery, we employ one of the following algorithms after C OMP: the non-negative LASSO (N NLASSO),
non-negative orthogonal matching pursuit (N NOMP), Sparse
Bayesian Learning (S BL), and non-negative absolute deviation
regression (N NLAD). For problems of small size, we also apply
a brute force (B F) search algorithm to solve a problem similar
to P0 from Eqn. 10 combinatorially.
1) The Non-negative LASSO (N NLASSO)
The LASSO (least absolute shrinkage and selection operator) is a penalized version of the constrained problem P1 in
Eqn. 11, and seeks to minimize the following cost function:
Jlasso (x; y, A) := ky − Axk22 + λkxk1 .

(14)

Here λ is a regularization parameter which imposes sparsity
in x. The LASSO has rigorous theoretical guarantees [26]
(chapter 11) for recovery of x as well as recovery of the
support of x (i.e. recovery of the set of non-zero indices
of x). Given the non-negative nature of x, we implement a
variant of LASSO with a non-negativity constraint, leading to
the following optimization problem:
Jnnlasso (x; y, A) := ky − Axk22 + λkxk1 s.t. x ≥ 0. (15)
Selection of λ: There are criteria defined in [26] for
selection of λ under iid Gaussian noise, so as to guarantee
statistical consistency. However, in practice, cross-validation
(CV) can be used for optimal choice of λ in a purely datadriven fashion from the available measurements. The details
of this are provided in the supplemental section S.III.
2) Non-negative Orthogonal Matching Pursuit (N NOMP)
Orthogonal Matching Pursuit (OMP) [27] is a greedy approximation algorithm to solve the optimization problem in
Eqn. 10. Rigorous theoretical guarantees for OMP have been
established in [28]. OMP proceeds by maintaining a set H of
‘selected coefficients’ in x corresponding to columns of A.
In each round a column of A is picked greedily, based on
the criterion of maximum
absolute correlation with a residual
P
vector r := y − k∈H Ak x̂k . Each time a column is picked,
all the coefficients extracted so far (i.e. in set H) are updated.
This is done by computing the orthogonal projection of y
onto the subspace spanned by the columns in H. The OMP
algorithm can be quite expensive computationally. Moreover,
in order to maintain non-negativity of x, the orthogonal
projection step would require the solution of a non-negative
least squares problem, further adding to computational costs.
However, a fast implementation of a non-negative version of
OMP (N NOMP) has been developed in [29], which is the
implementation we adopt here. For the choice of ε in Eqn.
10, we can use CV as described in Sec. III-D1.
3) Sparse Bayesian Learning (S BL)
Sparse Bayesian Learning (S BL) [30], [31] is a non-convex
optimization algorithm based on Expectation-Maximization

6

(EM) that has empirically shown superior reconstruction performance to most other CS algorithms with manageable computation cost [32]. In S BL, we consider the case of Gaussian
noise in y and a Gaussian prior on elements of x, leading to:
exp(−ky − Axk22 /(2σ 2 ))
(2πσ 2 )n/2
exp(−x2i /(2ϕi ))
√
p(xi ; ϕi ) =
; ϕi ≥ 0.
2πϕi
p(y|x) =

(16)
(17)

Since both x and ϕ (the vector of the {ϕi }ni=1 values)
are unknown, the optimization for these quantities can be
performed using an EM algorithm. In the following, we
shall denote Φ := diag(ϕ). Moreover, we shall use the
notation Φ(l) for the estimate of Φ in the lth iteration.
The E-step of the EM algorithm here involves computing
Q(Φ|Φ(l) ) := Ex|y;Φ(l) log p(y, x; Φ). It is to be noted that
the posterior distribution p(x|y; Φ(l) ) has the form N (µ, Σ)
where µ := ΣAT y/σ 2 and Σ := (AT A/σ 2 + (Φ(l) )−1 )−1 .
The M-step involves maximization of Q(Φ|Φ(l) ), leading to
the update Φ(l+1) = diag(µ2i +Σii ). The E-step and M-step are
executed alternately until convergence. Convergence to a fixedpoint is guaranteed, though the fixed point may or may not be
a local minimum. However, all local minima are guaranteed
to produce sparse solutions for x (even in the presence of
noise) because most of the ϕi values shrink towards 0. The
S BL procedure can also be modified to dynamically update the
noise variance σ 2 (as followed in this paper), if it is unknown.
All these results can be found in [31]. Unlike N NLASSO
or N NOMP, the S BL algorithm from [31] expressly requires
Gaussian noise. However we use it as is in this paper for
the simplicity it affords. Unlike N NOMP or N NLASSO, there
is no explicit non-negativity constraint imposed in the basic
S BL algorithm. In our implementation, the non-negativity is
simply imposed at the end of the optimization by setting to
0 any negative-valued elements in µ, though more principled,
albeit more computationally heavy, approaches such as [33]
can be adopted.
4) Non-negative Absolute Deviation Regression (N NLAD)
The Non-Negative Absolute Deviation Regression (N NLAD)
[34] and Non-negative Least squares (N NLS) [7] seek to
respectively minimize
Jnnlad (x; y, A) := ky − Axk1 s.t. x ≥ 0,
Jnnls (x; y, A) := ky − Axk2 s.t. x ≥ 0.

(18)
(19)

It has been shown in [34] that N NLAD is sparsity promoting
for certain conditions on the sensing matrix A, and that its
minimizer x∗ obeys bounds of the form ||x−x∗ ||1 ≤ C||η||1 ,
where C is a constant independent of x, x∗ , η, y. A salient
feature of N NLAD/N NLS is that they do not require any parameter tuning. This property makes them useful for matrices
of smaller size where cross-validation may be unreliable.
E. Generalized Binary Search Techniques
There exist adaptive group testing techniques which can
determine k infected samples in O(k log n) tests via repeated
binary search. These techniques are impractical in our setting

due to their sequential nature and large pool sizes. We provide
details of these techniques in the supplemental section S.II.
We also compare with a two-stage approach called Dorfman’s
method [2] in Sec. IV-A7.
F. Sensing Matrix Design
1) Physical Requirements of the Sensing Matrix
The sensing matrix A must obey some properties specific to
this application such as being non-negative. For ease and speed
of pipetting, it is desirable that the entries of A be (1) binary
(where Aji = 0 indicates that sample i did not contribute to
pool j, and Aji = 1 indicates that a fixed volume of sample i
was pipetted into pool j), and (2) sparse. Sparsity ensures that
not too many samples contribute to a pool, and that a single
sample does not contribute to too many pools. The former
is important because typically the volume of sample that is
added in a PCR reaction is fixed. Increasing pool size means
each sample contributes a smaller fraction of that volume. This
leads to dilution which manifests as a shift of the Ct value
towards larger numbers. If care is not taken in this regard,
this can affect the power of PCR to discriminate between
positive and negative samples. The latter is important because
contribution of one sample to a large number of pools could
lead to depletion of sample.
2) RIP-1 of Expander Graph Adjacency Matrices
The Restricted Isometry Property (RIP-2) of sensing matrices is a sufficient condition for good CS recovery as
described in Sec. III-C. However the matrices which obey
the aforementioned physical constraints are not guaranteed
to obey RIP-2. Instead, we consider sensing matrices which
are adjacency matrices of expander graphs. A left-regular
bipartite graph G((VI , VO ), E ⊆ VI × VO ) with degree of
each vertex in VI being d, is said to be a (k, ǫ)-unbalanced
expander graph for some integer k > 0 and some real-valued
ǫ ∈ (0, 1), if for every subset S ⊆ VI with |S| ≤ k, we
have |N (S)| ≥ (1 − ǫ)d|S|. Here N (S) denotes the union set
of neighbors of all nodes in S. Intuitively a bipartite graph
is an expander if every ‘not too large’ subset has a ‘large’
boundary. It can be proved that a randomly generated leftregular bipartite graph with |VO | ≥ O(k log n), n = |VI |
is an expander, with high probability [35], [36]. Moreover,
it has been shown in [25, Thm. 1] that the scaled adjacency
matrix A/d of a (k, ǫ)-unbalanced expander graph obeys RIP1 (Defn. 1) of order k. Here columns of A correspond to
vertices in VI , and rows correspond to vertices in VO . That
is, for any k-sparse vector x, the following relationship holds:
kxk1 ≤ kAxk1 /d ≤ (1+Cǫ)kxk1 for some absolute constant
C > 1. This property again implies that the null-space of A
cannot contain vectors that are ‘too sparse’ (apart from the
zero-vector). This summarizes the motivation behind the use
of expanders in compressive recovery of sparse vectors, and
also in group testing [25].
3) Matrices derived from Kirkman Triple Systems
Although randomly generated left-regular bipartite graphs
are expanders, we would need to verify whether a particular
such graph is a good expander, which may take prohibitively
long in practice [35]. In the application at hand, this can

7

5

15

35

Fig. 1. A full Kirkman matrix with m = 15 rows and n = m
/3 = 35
2
columns. Each cell denotes an entry of the matrix, with white cells denoting
the location of a 0 entry and the greyed out cells indicating the location of a 1
entry. Each column has exactly 3 entries with value 1. Each row has 7 entries
with value 1. There are (m − 1)/2 = 7 groups of columns, each consisting
of m/3 = 5 columns. Each row in a column group has exactly one 1 entry.
Matrices of size 15 × 20, 15 × 25, 15 × 30 or 15 × 35 may be served by
choosing the first 4, 5, 6, or 7 column groups, while keeping the number of
1 entries in each row equal.

prove to be a critical limitation since matrices of various sizes
may have to be served, depending on the number of samples
arriving in that batch at the testing centre, and the number
of tests available to be performed. Hence, we have chosen
to employ deterministic procedures to design such matrices,
based on objects from combinatorial design theory known as
Kirkman triples (see [8], [9]).
We first recall Kirkman Triple Systems (an example of
which is illustrated in Fig. 1) which are Steiner Triple Systems
with an extra property. Steiner Triple Systems consist of
n = m
2 /3 column vectors with m elements each, with
each entry being either 0 or 1 such that each column has
exactly three 1s, every pair of rows has dot product equal
to 1 and every pair of columns has dot product at most 1
[37]. This means that each column of a Steiner Triple System
corresponds to a triplet of rows (i.e. contains exactly three 1s),
and every pair of rows occurs together in exactly one such
triplet (i.e. for every pair of rows indexed by i, j, there exists
exactly one column index k for which Aik = Ajk = 1). If the
columns of a Steiner Triple System can be arranged such that
the sum of columns from i to i + m/3 − 1 equals 1 ∈ Rm
for every i ≡ 1 modulo m/3 then the Steiner Triple System
is said to be resolvable, and is known as a Kirkman Triple
System [8]. That is, the set of columns of a Kirkman Triple
System can be partitioned into (m−1)/2 disjoint groups, each
consisting of m/3 columns, such that each row has exactly
one 1 entry in a given such group of columns. Because of this
property, we may choose any l such groups of columns of a
Kirkman Triple System to form a m × n matrix, n > m, with
n = lm/3, and 3 < l ≤ (m − 1)/2, while keeping the number
of 1 entries in each row the same. From here on, we refer to
such matrices as Kirkman matrices. If l = (m − 1)/2, then
we refer to it as a full Kirkman matrix, else it is referred to
as a partial Kirkman matrix. Note that in a partial Kirkman
matrix, the dot product of any two rows may be at most 1,
whereas in a full Kirkman matrix, it must be equal to 1.
Notice that m = 6t + 3 for some t ∈ Z≥0 for a Kirkman
Triple System to exist, since m−1 must be divisible by 2, and

m must be divisible by 3. This, and the existence of Kirkman
Triple Systems for all t ∈ Z≥0 have been proven in [9].
Explicit constructions of Kirkman Triple Systems for m ≤ 99
exist [8]. Generalizations of Kirkman Triple Systems under the
name of the Social Golfer Problem is an active area of research
(see [38], [39]). The Social Golfer Problem asks if it is possible
for g × p golfers to play in g groups of p players each for w
weeks, such that no two golfers play in the same group more
than once
 [40, Sec. 1.1]. Kirkman Triple Systems with m rows
and m
2 /3 columns are a solution to the Social Golfer Problem
for the case when p = 3, g = m/3 and w = (m − 1)/2. Full
or partial Kirkman matrices may be constructed via greedy
search techniques used for solving the Social Golfer Problem
(such as in [41]). Previously, Kirkman matrices have been
proposed for use as Low-Density Parity Check codes in [42],
due to high girth2 of Kirkman matrix bipartite graphs and the
ability to serve only part of the matrix while keeping the row
weights3 equal. Matrices derived from Steiner Triple Systems
have previously been used for pooled testing for transcription
regulatory network mapping in [43]. Further, matrices derived
from Steiner Systems [44], a generalization of Steiner Triple
Systems, have been proposed for optimizing 2-stage binary
group testing in [45].
4) RIP-1 and Expansion Properties of Kirkman Matrices
We show that Kirkman matrix bipartite graphs are (k, ǫ)unbalanced expanders, with ǫ = (k − 1)/2d, where d is the
left-degree of the graph and is 3 for Kirkman matrices. Given
a set S of column vertices such that |S| ≤ k, we note that the
size of the union set of neighbours
of S, |N (S)|, is at least

|S|d − pr, where p = |S|
is
the
number
of (unordered) pairs
2
of columns in S, and r is the maximum number of row vertices
in common between any two column vertices. For a Kirkman
matrix, since any two columns have dot product at most 1,
hence r = 1. Therefore, |N (S)| ≥ d|S|(1 − (|S| − 1)/2d).
Since |S| ≤ k, therefore |N (S)| ≥ d|S|(1 − (k − 1)/2d).
This implies that Kirkman matrix biparite graphs are (k, ǫ)unbalanced expanders, with ǫ = (k − 1)/2d. If we put in
the requirement that d = 3 for Kirkman matrices and ǫ < 1,
we find that k < 7. Hence it follows from [25, Thm. 1] that
the scaled Kirkman matrix has RIP-1 of order k for k < 7
and ǫ = (k − 1)/6. This suggests exact recovery for upto 3
infected samples using CS. However, in practice, we observe
that using our method we are able to recover much higher
number of positives, at the cost of an acceptable number of
false positives and rare false negatives (Sec. IV).
5) Optimality of Girth 6 Matrices
A Steiner Triple System bipartite graph does not have a
cycle of length 4. If it did, then there would exist two rows a
and b, and two columns u and v of the Steiner Triple System
matrix A such that Aau = Abu = 1 and Aav = Abv = 1. This
would violate the property that dot product of any two rows of
the Steiner Triple System must be equal to 1. Furthermore, [42,
Lemma 1] show that Steiner Triple System bipartite graphs
have girth equal to 6. Since Kirkman Triple Systems are
resolvable Steiner Triple Systems (see definitions earlier in this
2 The

girth of a graph is equal to the length of the shortest cycle in it.
as the number of 1 entries in a row

3 defined

8

section), their bipartite graphs also have girth equal to 6. For
a bipartite graph constructed from a partial Kirkman matrix,
the girth is at least 6, since dropping some column vertices
will not introduce new cycles in the graph. Furthermore, it
is shown in [23, Thm. 10] that adjacency matrices of leftregular graphs with girth at least 6 satisfy RNSP (Defn. 2) of
order k (for suitable k). Consequently, they may be used for
CS decoding [23, Thm. 5]. They also give lower bounds on
the number of rows m of left-regular bipartite graph matrices
whose column weight4 is more than 2, for them to have high
girth and consequently satisfy RNSP of order k, given k and
n [23, Eqn. 32, 33]. Given k and n, these lower bounds are
minimized for graphs√of girth 6 and 8, √
and the bounds are,
respectively, m ≥ k n and m ≥ k 3/2 n ([23, Eqn. 37]).
However, with the additional requirement that m <
√ n for CS,
it is found that girth 6 matrices can recover k < √ n defects,
while girth 8 matrices can only recover k < 3 n defects.
Hence, matrices whose bipartite graphs have girth equal to 6
are optimal in this sense. Full Kirkman matrix bipartite graphs
are left-regular and have girth 6, as argued earlier, and hence
they satisfy RNSP, may be used for compressive sensing, and
are optimal in the sense of being able to handle most number
of defects while minimizing the number of measurements. We
note that since we employ Kirkman triples, each column has
only three 1s. The theoretical guarantees for such matrices
hold for signals with ℓ0 norm less than or equal to 2. However,
we have obtained acceptable false positive and false negative
rates in practice for much larger sparsity levels, as will be seen
in Sec. IV.
6) Disjunctness Property of Kirkman Matrices
In order for a matrix to be suitable for our method, it should
not only be good for CS decoding algorithms, but also for
C OMP. Kirkman matrices are 2-disjunct, and can recover up
to 2 defects exactly using C OMP. In a k-disjunct matrix, there
does not exist any column such that its support is a subset
of the union of the support of k other columns [15]. Matrices
which are k-disjunct have exact support recovery guarantee for
k-sparse vectors, using C OMP (see [15]). Disjunctness follows
from the following properties of Kirkman matrices – that two
columns in a Kirkman matrix have at most one row in common
with an entry of 1, and that each column has exactly three 1
entries. Consider Ra , Rb , and Rc , the sets of rows for which
the three columns a, b and c respectively have a 1 entry. Note
that |Ra | = |Rb | = |Rc | = 3, and |Rp ∩ Rq | ≤ 1 for p, q ∈
{a, b, c}, p 6= q. If Rc ⊆ Ra ∪ Rb , then either |Rc ∩ Ra | > 1
or |Rc ∩ Rb | > 1, which presents a contradiction.
Empirically we find that even for k > 2, C OMP reports
only a small fraction of the total number of samples as
positives when using Kirkman matrices (Table I). In Sec.
S.XIV (Proposition 6) of the supplemental material, we prove
that if a fraction f ∈ (0, 1) of the tests come out to be positive,
then C OMP reports strictly less than fraction f 2 of the samples
as positive for a full Kirkman matrix. This provides intuition
behind why Kirkman matrices may be well-suited for our
combined C OMP + CS method, since most samples are already
eliminated by C OMP. On the other hand, CS decoding (without
4 defined

as the number of 1 entries in a column

the earlier C OMP step) on the full Kirkman matrix does not
perform as well, as shown in the supplemental section S.IX.
7) Advantages of using Kirkman Matrices
As we have seen in earlier sections, Kirkman matrices are
suitable for use in compressed sensing due to their expansion,
RIP-1 and high girth properties, as well as for binary group
testing due to disjunctness. Furthermore, the dot product
between two columns of a Kirkman matrix being at most
1 ensures that no two samples participate in more than one
test together. This has favourable consequences in terms of
placing an upper bound on the mutual coherence of the
matrix, defined as:
µ(A) := maxi6=j

|Ai t Aj |
,
kAi k2 kAj k2

(20)

where Ai refers to the ith column of A. Matrices with lower
µ(A) values have lower values of worst case upper bounds
on the reconstruction error [46]. These bounds are looser than
those based on the RIC that we saw in previous sections.
However, unlike the RIC, the mutual coherence is efficiently
computable.
A practical benefit of Kirkman triples that is not shared by
Steiner triples is that the former can
 be served for number
of samples far less than n = m
2 /3 while keeping pools
balanced (i.e. ensuring that each pool is created from the same
number of samples). In fact, we can choose n to be any integer
multiple of m/3, and ensure that every pool gets the same
number of samples, as discussed in section III-F3. Notice that
the expansion, RIP-1, high girth and disjunctness properties
hold for full as well as partial Kirkman matrices, as proven in
previous sections. This allows us to characterize the properties
of the full Kirkman matrix, and use that analysis to predict
how it will behave in the clinical situation where the pooling
matrix to be served may require very specific values of m, n
depending on the prevalence rate.
Column weight: Kirkman matrices have column weight
equal to 3 - that is, each sample goes to 3 pools. It is possible
to construct matrices with higher number of pools per sample
(such as those derived from the Social Golfer Problem [38],
which will retain several benefits of the Kirman matrices: (1)
They would have the ability to serve only part of the matrix;
(2) They would retain the the expander and RIP-1 properties,
following a proof similar to the one in Sec. III-F4; (3) They
would not have any 4-cycles in the corresponding bipartite
graph, following a similar argument as in Sec. III-F5; and
(4) They would possess the disjunctness property following a
proof similar to the one in Sec. III-F6). Nevertheless, the time
and effort needed for pooling increases with more pools per
sample. Further, higher pools per sample will come at the cost
of a larger number of tests (if pool size is kept constant), or
larger pool size (if number of tests is kept constant). Higher
number of tests is undesirable for obvious reasons, while larger
pool size may lead to dilution of the sample within a pool,
leading to individual RT-PCR tests failing.
8) Optimal Binary Sensing Matrices with Random Construction
While Kirkman matrices
which satisfy RNSP of order k
√
must have at least k n measurements, we can get much

9

better bounds in theory if we use random constructions.
From [7, Prop. 10] we see that with high probability, 0/1
Bernoulli(p) matrices need only O(k log n) measurements in
order to satisfy ℓ2 -RNSP (Defn. 3) of order k, with p ∈ (0, 1)
being the probability with which each entry of the matrix is
independently 1.
In the supplemental section S.V, we prove that ℓ2 -RNSP
is preserved by C OMP. That is, the reduced matrix AX̄ ,Ȳ
obeys ℓ2 -RNSP of order k with the same parameters as the
original matrix A. Hence our method only needs O(k log n)
measurements for robust recovery of k-sparse vectors with
such random matrix constructions. Bernoulli(p) matrices are
also good for C OMP – [12, Thm. 4] shows that Bernoulli(p)
matrices with p = 1/k need only O(k log n) measurements for
exact support recovery of k-sparse vectors with C OMP with
vanishingly small probability of error.
In practice, we observe that Kirkman matrices perform
better than Bernoulli(p) matrices using our method in the
regime of our problem size. This gap between theory and
practice
may be arising due to the following reasons: (1) The
√
k n lower bound for Kirkman triples is for a sufficient but not
necessary condition for sparse recovery; (2) The O(k log n)
may be ignoring a very large constant factor which affects
the performance of moderately-sized problems such as the
ones reported in this paper; and (3) The theoretical bounds
are for exact recovery with vanishingly small error, whereas
we allow some false positives and rare false negatives in
our experiments. Similar comparisons between binary and
Gaussian random matrices have been recently put forth in
[23]. Moreover, the average column weight of Bernoulli(p)
matrices is pm, where m is the number of measurements.
This is typically much higher than column weight 3 of
Kirkman matrices and hence undesirable (see Sec. III-F7). In
the supplemental section S.VI, we compare the performance
of Kirkman matrices with Bernoulli(0.1) and Bernoulli(0.5)
matrices.
9) Mutual Coherence optimized Sensing Matrices
As mentioned earlier, the mutual coherence from Eqn.
20 is efficient to compute and optimize over. Hence, there
is a large body of literature on designing CS matrices by
minimizing µ(A) w.r.t. A, for example [47]. We followed
such a procedure for designing sensing matrices for some of
our experimental results in Sec. IV-B. For this, we follow
simulated annealing to update the entries of A, starting with
an initial condition where A is a random binary matrix.
For synthetic experiments, we compared such matrices with
Bernoulli(p) random matrices, adjacency matrices of biregular
random sparse graphs (i.e. matrices in which each column
has the same weight, and each row has the same weight which may be different than the column weight), and Kirkman
matrices. We found that matrices of Kirkman triples perform
very well empirically in the regime of sizes we are interested
in, besides facilitating easy pipetting, and hence the results are
reported using only Kirkman matrices.
IV. E XPERIMENTAL R ESULTS
In this section, we show a suite of experimental results on
synthetic data as well as on real data.

A. Results on Synthetic Data
1) Choice of Sensing Matrix
Recall from section II that a typical RT-PCR setup can test
96 samples in parallel. Three of these tests are used as control
by the RT-PCR technician in order to have confidence that
the RT-PCR process has worked. Hence, in order to optimize
the available test bandwidth of the RT-PCR setup, the number
of tests we perform in parallel should be ≤ 93, and as close
to 93 as possible. Since in Kirkman matrices, the number of
rows must be 6t + 3 for some t ∈ Z≥0 , hence we choose 93.
With this choice, the number of samples tested n has to be a
multiple of 93/3 = 31, hence we chose n = 961. This matrix
is not a full Kirkman matrix – a full matrix with 93 rows will
have 1426 columns. However, we keep the number of columns
of the matrix under 1000 due to challenges in pooling large
number of samples. Furthermore, n = 961, m = 93 satisfies
more than 10x factor improvement in testing while detecting
1% infected samples with reasonable sensitivity and specificity
and is in a regime of interest for widespread screening or
repeated testing.
We also present results with a 45 × 105 partial Kirkman
matrix in the supplemental Sec. S.VIII. This matrix gives 2.3x
improvement in testing while detecting 9.5% infected samples
with reasonable sensitivity and specificity. Further, two such
batches of 105 tests in 45 pools may be run in parallel in a
single RT-PCR setup.
2) Signal/Measurement Generation
For the case of synthetic data, we generated k-sparse
signal vectors x of dimension n = 961, for each k in
{5, 8, 10, 12, 15, 17, 20}. We choose a wide range of k in
order to demonstrate that not only do our algorithms have
high sensitivity and specificity for large values of k, they also
keep performing reasonably, well beyond the typical operating
regime. The support of each signal vector x – given k –
was chosen by sampling a k-sparse binary vector uniformly at
random from the set of all k-sparse binary vectors. The magnitudes of the non-zero elements of x were picked uniformly
at random from the range [1, 32768]. This high dynamic range
in the value of x was chosen to reflect a variance in the typical
threshold cycle values (Ct ) of real PCR experiments, which
can be between 16 and 32. From Eqn. 6, we can infer that
viral loads vary roughly as 2−Ct (setting q = 1), up to constant
multiplicative terms. In all cases, m = 93 noisy measurements
in y were simulated following the noise model in Eqn. 3 with
σ = 0.1 and q = 0.95. A 93×961 Kirkman sensing matrix was
used for generating the measurements. The Poisson nature of
the elements of x in Eqn. 3 was ignored. This approximation
was based on the principle
∼ Poisson(λ), then
√ that if X √
λ/λ = 1/ λ which becomes
Std. Dev.(X)/E(X) =
smaller and smaller as λ increases. The recovery algorithms
were tested on Q = 1000 randomly generated signals for each
value of k.
3) Algorithms tested
The following algorithms were compared:
1) C OMP (see Table I)
2) C OMP followed by N NLASSO (see Table II)
3) C OMP followed by S BL (see Table III)

10

4) C OMP followed by N NOMP (see Table IV)
5) C OMP followed by N NLAD (see Table V)
6) C OMP followed by N NLS (see Table S.VI in the Supplementary)
For each algorithm any positives missed during the CS stage
but caught by D D were declared as positives, as mentioned
in Sec. III-D. For small sample sizes we also tested C OMPB F, i.e. C OMP followed by brute-force search for samples
in x with non-zero values. Details of this algorithm and
experimental results with it are presented in the supplemental
section S.IV.
4) Comparison Criteria
In the following, x̂ denotes the estimate of x. Most numerical algorithms do not produce vectors that are exactly
sparse and have many entries with very tiny magnitude, due
to issues such as choice of convergence criterion. Since in
this application, support recovery is of paramount importance
to identify which samples in x were infected, we employed
the following post-processing step: All entries in x̂ whose
magnitude fell below a threshold τ := 0.2 × xmin were set
to zero, yielding a vector x̄. Here xmin refers to the least
possible value of the viral load, and this can be obtained
offline from practical experiments on individual samples. In
these synthetic experiments, we simply set xmin := 1. We
observed that varying the value of τ over a fairly wide range
had negligible impact on the results, as can be observed from
Sec. S.XII of the supplemental material. For S BL, we set τ
to 0 and also set also negative entries in the estimate to 0.
For N NOMP, such thresholding was inherently not needed.
The various algorithms were compared with respect to the
following criteria:
1) RMSE := kx − x̄k2 /kxk2
2) Number of false positives (FP) := |{i : xi = 0, x̂i > 0}|
3) Number of false negatives (FN) := |{i : xi > 0, x̂i = 0}|
4) Sensitivity (also called Recall or True Positive rate) :=
#correctly detected positives/#actual positives
5) Specificity (also called True Negative Rate) := #correctly
detected negatives/#actual negatives.
5) Main Results
It should be noted that all algorithms were evaluated on
1000 randomly generated sparse signals, given the same sensing matrix. The average value as well as standard deviation
of all quality measures (over the 1000 signals) are reported in
the Tables I, II, III, IV, V, S.VI. A comparison of Table I to
Tables II, III, IV, V, S.VI indicates that C OMP followed by
N NLASSO/S BL/N NOMP/N NLAD/N NLS significantly reduces
the false positives at the cost of a rare false negative. The
RMSE is also significantly improved, since C OMP does not
estimate viral loads. At the same time, C OMP significantly
reduces the size of the problem for the CS stage. For example,
for the 93 × 961 Kirkman matrix, when number of infected
samples k is 12, the average size of the matrix after C OMP
filtering is ∼ 30 × 37. From Table I we see that Definite Defectives classifies many positives as high-confidence positives,
for k upto 8. We note that the experimental results reported
in these tables are quite encouraging, since these experiments
are challenging due to small m and fairly large k, n, albeit

with testing on synthetic data. We noticed that running the CS
algorithms without the C OMP step did not perform as well,
results for which are presented in the supplemental section
S.IX. We observed that the advantages of our combined group
testing and compressed sensing approach holds regardless of
the sensing matrix size. For comparison, results of running
our algorithms using a 45 × 105 Kirkman matrix instead of
the 93 × 961 Kirkman matrix are presented in supplemental
section S.VIII.
6) Parameter Selection
As mention earlier, the regularization parameters in various
estimators such as C OMP-N NLASSO, C OMP-N NLAD, C OMPN NOMP, etc. are estimated via cross-validation. For these
estimators, we therefore do not require knowledge of the σ
parameter in the noise model from Eqn. 3. The q parameter
in the noise model is set to 0.95 in all our experiments. It is a
reasonable choice as the molecule count is known to roughly
double in each cycle of RT-PCR [14]. Moreover, variation of
q in the range from 0.7 to 1 showed negligible variation in
the results of our wet-lab experiments as can be seen in Sec.
S.XI and Table S.VIII of the supplemental material. Also note
that we only report viral loads relative to ymin (see Eqn. 8) we do not attempt to estimate ymin . These relative viral loads
are interpretable by the RT-PCR technicians since they know
tmin , the minimum Ct (threshold cycle) value observed in that
experiment. Note as well that since ymin is the viral load of
the pool with the minimum Ct value – it corresponds to the
pool with the maximum viral load in that experiment.
7) Comparison with Dorfman Pooling
We also performed a comparison of our algorithms with
the popular two-stage Dorfman pooling method (an adaptive
method), with regard to the number of tests required. In the
first stage of the Dorfman pooling technique, the n samples
are divided into n/g pools, each of size g. Each of these
n/g pools are tested, and a negative result leads to all
members of that pool being considered negative (i.e. noninfected). However, the pools that are tested positive are passed
onto a second stage, where all members of those pools are
individually tested. The optimal pool size g ∗ will minimize
the expected number of tests taken by this process (given that
the membership in each pool is decided randomly). A formula
for the expected number of tests taken by Dorfman testing is
derived in [2]. The derivation in [2] assumes the following:
(1) Any given sample may be positive with probability p,
independently of the other samples; (2) The number of samples
n is divisible by the pool size g. We modify the formula from
[2] for the case that n is not divisible by g (supplemental
section S.XIII), and find g ∗ by choosing the value of g which
minimizes this number. We set p = k/n, so that out of n
samples, the number of infected samples is k in expectation.
Table VI shows the expected number of tests computed from
the formula in supplemental section S.XIII, assuming that the
expected number of infected samples k (and thus the optimal
pool size g ∗ ) is known in advance. We also empirically verified
the expected number of tests by performing 1000 Monte Carlo
simulations of Dorfman testing with the optimal pool size g ∗
for each case, and did not observe much deviation from the
numbers reported in Table VI. Comparisons of Tables I, II,

11

TABLE I
P ERFORMANCE OF C OMP AND D D ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
AND STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000

#FN
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0

#FP
1.6 ± 1.2
7.9 ± 3.0
15.3 ± 4.5
25.3 ± 6.7
46.1 ± 10.3
62.3 ± 13.7
91.5 ± 18.1

Sens.
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000

Spec.
0.9983 ± 0.0013
0.9918 ± 0.0031
0.9839 ± 0.0048
0.9733 ± 0.0070
0.9512 ± 0.0109
0.9340 ± 0.0146
0.9028 ± 0.0192

VALUE , MEAN

#HCP
4.7
4.3
2.5
1.1
0.2
0.1
0.0

TABLE II
P ERFORMANCE OF C OMP FOLLOWED BY N NLASSO ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.047 ± 0.020
0.069 ± 0.028
0.100 ± 0.049
0.149 ± 0.092
0.295 ± 0.166
0.404 ± 0.184
0.563 ± 0.172

#FN
0.0 ± 0.1
0.1 ± 0.2
0.2 ± 0.5
0.6 ± 0.7
1.0 ± 1.1
1.1 ± 1.4
1.1 ± 1.8

#FP
0.8 ± 0.9
4.0 ± 2.1
7.9 ± 3.4
12.9 ± 5.0
28.5 ± 16.5
46.3 ± 23.5
78.4 ± 26.7

Sens.
0.9990 ± 0.0141
0.9925 ± 0.0307
0.9780 ± 0.0454
0.9538 ± 0.0591
0.9316 ± 0.0722
0.9355 ± 0.0817
0.9452 ± 0.0923

Spec.
0.9991 ± 0.0009
0.9958 ± 0.0022
0.9917 ± 0.0035
0.9864 ± 0.0052
0.9699 ± 0.0175
0.9509 ± 0.0249
0.9167 ± 0.0284

TABLE III
P ERFORMANCE OF C OMP FOLLOWED BY S BL ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.043 ± 0.017
0.058 ± 0.021
0.071 ± 0.025
0.094 ± 0.035
0.123 ± 0.108
0.165 ± 0.179
0.318 ± 0.305

#FN
0.0 ± 0.0
0.0 ± 0.2
0.1 ± 0.2
0.1 ± 0.4
0.3 ± 0.6
0.5 ± 0.8
1.3 ± 1.6

#FP
0.9 ± 0.9
4.3 ± 2.1
8.2 ± 3.1
13.6 ± 4.4
25.1 ± 6.9
35.1 ± 9.9
54.5 ± 13.2

Sens.
0.9998 ± 0.0063
0.9958 ± 0.0227
0.9937 ± 0.0247
0.9886 ± 0.0310
0.9804 ± 0.0396
0.9713 ± 0.0491
0.9349 ± 0.0803

Spec.
0.9991 ± 0.0010
0.9955 ± 0.0023
0.9913 ± 0.0033
0.9856 ± 0.0046
0.9735 ± 0.0073
0.9628 ± 0.0105
0.9420 ± 0.0140

TABLE IV
P ERFORMANCE OF C OMP FOLLOWED BY N NOMP ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.043 ± 0.019
0.060 ± 0.025
0.077 ± 0.035
0.115 ± 0.067
0.242 ± 0.190
0.361 ± 0.243
0.589 ± 0.282

#FN
0.0 ± 0.1
0.1 ± 0.4
0.3 ± 0.5
0.5 ± 0.7
1.5 ± 1.4
2.8 ± 2.2
6.1 ± 3.0

#FP
0.3 ± 0.6
1.8 ± 2.0
3.7 ± 3.2
7.8 ± 4.9
15.6 ± 6.0
20.8 ± 5.6
27.0 ± 5.2

Sens.
0.9982 ± 0.0209
0.9831 ± 0.0472
0.9739 ± 0.0541
0.9565 ± 0.0560
0.9013 ± 0.0951
0.8329 ± 0.1268
0.6941 ± 0.1520

Spec.
0.9997 ± 0.0006
0.9981 ± 0.0021
0.9961 ± 0.0034
0.9918 ± 0.0051
0.9835 ± 0.0064
0.9780 ± 0.0059
0.9713 ± 0.0055

TABLE V
P ERFORMANCE OF C OMP FOLLOWED BY N NLAD ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.050 ± 0.021
0.077 ± 0.034
0.107 ± 0.050
0.167 ± 0.095
0.296 ± 0.160
0.401 ± 0.171
0.530 ± 0.166

#FN
0.0 ± 0.0
0.0 ± 0.2
0.2 ± 0.4
0.5 ± 0.7
0.9 ± 1.0
0.8 ± 1.2
0.2 ± 0.9

#FP
1.0 ± 1.0
4.9 ± 2.3
9.3 ± 3.0
14.4 ± 5.1
29.7 ± 16.7
50.2 ± 23.9
87.9 ± 25.1

Sens.
0.9996 ± 0.0089
0.9939 ± 0.0270
0.9809 ± 0.0441
0.9574 ± 0.0586
0.9393 ± 0.0687
0.9549 ± 0.0717
0.9884 ± 0.0427

Spec.
0.9990 ± 0.0010
0.9949 ± 0.0024
0.9903 ± 0.0032
0.9848 ± 0.0054
0.9686 ± 0.0176
0.9468 ± 0.0253
0.9066 ± 0.0267

12

TABLE VI
E XPECTED NUMBER OF TESTS NEEDED BY OPTIMAL D ORFMAN T ESTING
FOR NUMBER OF SAMPLES (n) 105 AND 961 FOR VARIOUS k. N OTE THAT
OUR PROPOSED METHODS BASED ON CS REQUIRE MUCH FEWER TESTS
(45 AND 93) TYPICALLY, AND DO NOT REQUIRE TWO ROUNDS OF
TESTING .
k
5
8
10
12
15
17
20

N = 105
# Tests
Pool Size
43.7
5
55.3
4
61.3
4
67.0
4
73.9
3
78.2
3
84.3
3

k
5
8
10
12
15
17
20

N = 961
# Tests
Pool Size
136.5
14
172.2
11
192.2
11
209.6
9
233.7
9
248.7
8
269.4
7

TABLE VII
T RUE SPARSITY k VERSUS ESTIMATED SPARSITY kest ( ON SYNTHETIC
DATA ) FOR 93 × 961 K IRKMAN MATRIX . M EAN AND STANDARD
DEVIATION OF ESTIMATED SPARSITY IS COMPUTED OVER 1000 SIGNALS
FOR EACH k.
k
5
10
15
20
25
30

kest
5.01 ± 0.33
10.07 ± 0.69
15.13 ± 1.17
20.26 ± 1.63
25.54 ± 2.03
30.53 ± 2.61

III, IV with the two-stage Dorfman pooling method in VI
show that our methods require much fewer tests, albeit with a
slight increase in number of false negatives. Moreover, all our
methods are single-stage methods and therefore require less
time for testing, unlike the Dorfman method which requires
two stages of testing.
8) Estimation of number of infected samples
The number of CS measurements for successful recovery
depends on the number of non-zero elements (ℓ0 norm) of
the underlying signal. For example, this varies as O(k
√log n)
for
for randomized sensing matrices [5] or as O(max(k 2 , n)√
deterministic designs [22]. There is a lower bound of k n
measurements for certain types of expander matrices to satisfy
a sufficient (but not necessary) condition for recovery [23].
However, in practice k is always unknown, which leads to
the question as to how many measurements are needed as a
minimum for a particular problem instance. To address this,
we adopt the technique from [48] to estimate k on the fly
from the compressive measurements. This technique does not
require signal recovery for estimating k. The relative error in

TABLE VIII
C OMPARISON OF MEAN NUMBER OF FALSE NEGATIVE AND FALSE
POSITIVES FOR COMP, COMP-SBL AND COMP-SBL WITH GRACEFUL
FAILURE MODE FOR HIGH VALUES OF k FOR THE 93 × 961 K IRKMAN
MATRIX . T HE ALGORITHM GOES INTO GRACEFUL FAILURE MODE WHEN
ESTIMATED SPARSITY IS GREATER THAN OR EQUAL TO 20
k
15
20
25
30

COMP
#FN
#FP
0
45.3
0
92.7
0
151.2
0
212.1

COMP-SBL
#FN
#FP
0.3
24.9
1.3
55.4
4
97.5
6.9
140.6

COMP-SBL-graceful
#FN
#FP
0.3
24.8
0.4
80.2
0
151.2
0
212.1

p
the estimate of k is shown to be O( log m/m) [49], which
diminishes as m increases (irrespective of the true k). Table
VII shows the accuracy of our sparsity estimate on synthetic
data.
The advantage of this estimate of k is that it can drive the
C OMP-B F algorithm, as well as act as an indicator of whether
there exist any false negatives. We can use this knowledge to
enable a graceful failure mode. In this mode, if our estimate
of k is larger than what the CS algorithms can handle, we
return only the output of the C OMP stage. Hence in such
rare cases, it minimizes the number of false negatives, at the
cost of many false positives. In these cases a second stage
of individual testing must be done on the samples which were
declared positive. Table VIII shows the effect of using graceful
failure mode with C OMP followed by S BL for large values of
k. In these experiments, output of C OMP is returned if the
estimated sparsity, kest , is greater than or equal to 20. We
see that C OMP-S BL with graceful failure mode matches the
behaviour of C OMP-S BL at sparsity value lower than 20, and
that of C OMP at sparsity value greater than 20. At sparsity
equal to 20, it compromises between the high false positives
of C OMP, and the high false negatives of C OMP-S BL. This is
because of the variability in kest , which can occasionally be
less than 20 even if k is equal to 20.
B. Results on Real Data
We acquired real data in the form of test results on pooled
samples from two labs: one at the National Center of Biological Sciences (NCBS) in India, and the other at the Wyss
Institute at the Harvard Medical School, USA. In both cases,
viral RNA was artificially injected into k of the n samples
where k ≪ n. From these n samples, a total of m mixtures
were created. For the datasets obtained from NCBS that we
experimented with, we had m = 16, n = 40, k ∈ {1, 2, 3, 4}.
For the data from the Wyss Institute, we had m = 24, n = 60,
k = 2 and m = 30, n = 120, k = 2. The results for all these
datasets are presented in Table IX. The 16 × 40 and 24 × 60
pooling matrices were obtained by performing a simulated
annealing procedure to minimize the mutual coherence (see
Sec. III-F9), starting with a random sparse binary matrix as
initial condition. The 30 × 120 pooling matrix was a Kirkman
matrix. We used q = 0.95 in all cases to obtain relative
viral loads from Ct values, using Eqn. 8. While q may be
estimated from raw RT-PCR data (Sec. S.XI, supplemental
material), we found q = 0.95 to be a reasonable choice,
and did not observe any variation in the number of reported
positives when this parameter was changed between 0.7 to 1.
For N NLASSO, N NLS and N NLAD, we use τ = 0.2 × yemax as
the threshold below which an estimated relative viral load is
set to 0, since value of xmin may not always be available
for real experiments. Here yemax is the relative viral load
of the pool with the largest Ct value, and consequently the
smallest viral amount. We see that the CS algorithms reduce
the false positives, albeit with an introduction of occasional
false negatives for higher values of k. We also refer the reader
to our work in [6] for a more in-depth description of results
on real experimental data.

13

TABLE IX
R ESULTS OF LAB EXPERIMENTS WITH EACH ALGORITHM
Dataset
Harvard 24 × 60, k = 2

Harvard 30 × 120, k = 2

NCBS-0 16 × 40, k = 0

NCBS-1 16 × 40, k = 1

NCBS-2 16 × 40, k = 2

NCBS-3 16 × 40, k = 3

NCBS-4 16 × 40, k = 4

Algorithm
C OMP
C OMP -S BL
C OMP -N NOMP
C OMP -N NLASSO
C OMP -N NLAD
C OMP -N NLS
C OMP
C OMP -S BL
C OMP -N NOMP
C OMP -N NLASSO
C OMP -N NLAD
C OMP -N NLS
C OMP
C OMP -S BL
C OMP -N NOMP
C OMP -N NLASSO
C OMP -N NLAD
C OMP -N NLS
C OMP
C OMP -S BL
C OMP -N NOMP
C OMP -N NLASSO
C OMP -N NLAD
C OMP -N NLS
C OMP
C OMP -S BL
C OMP -N NOMP
C OMP -N NLASSO
C OMP -N NLAD
C OMP -N NLS
C OMP
C OMP -S BL
C OMP -N NOMP
C OMP -N NLASSO
C OMP -N NLAD
C OMP -N NLS
C OMP -B F
C OMP
C OMP -S BL
C OMP -N NOMP
C OMP -N NLASSO
C OMP -N NLAD
C OMP -N NLS
C OMP -B F

C. Discussion
Each algorithm we ran presented a different set of tradeoffs
between sensitivity and specificity. While C OMP provides us
with sensitivity equal to 1, it suffers many false positives,
especially for higher k. For other algorithms, in general both
the sensitivity and the specificity decrease as k is increased.
C OMP-N NOMP (Table IV) has the highest specificity, but it
comes at the cost of sensitivity. C OMP-S BL (Table III) has
the best sensitivity for most values of k amongst the CS
algorithms. C OMP-N NLASSO (Table II) has better specificity
than C OMP-S BL for small values of k, but loses out for
k ≥ 15. C OMP-N NLAD and C OMP-N NLS (Tables V and
S.VI) start behaving like C OMP for higher values of k,
effectively bounding the number of false negatives. However,
their number of false positives is almost as much as those with
C OMP.
Ideally, we want both high sensitivity and high specificity
while catching a large number of infected samples. Hence,

# true pos
2
2
2
2
2
2
2
2
2
2
2
2
0
0
0
0
0
0
1
1
1
1
1
1
2
2
2
2
2
2
3
2
2
2
3
2
2
4
3
2
3
2
3
2

# false neg
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
0
1
1
0
1
2
1
2
1
2

#false pos
1
1
0
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
0
1
1
1
1
3
2
2
2
2
2
2

we look at k ∗ , which is the maximum number of infected
samples k for which the sensitivity and specificity of the
algorithm are greater than or equal to some threshold values.
For the 45 × 105 Kirkman matrix, we chose the sensitivity
threshold as 0.99 and the specificity threshold as 0.95. For
the 93 × 961 Kirkman matrix, we chose both thresholds to be
0.99, since a specificity threshold of 0.95 gives too many false
positives for 961 samples. We observed that C OMP-S BL has
k ∗ = 10 for both matrices, which is the highest amongst all
algorithms tested. Typically we do not know the number of
infections, but a prevalence rate of infection. The number of
infected samples out of a given set of n samples may be treated
as a Binomial random variable with probability of success
equal to the prevalence rate. Under this assumption, using
C OMP-S BL with the 93 × 961 Kirkman matrix, we observed
that the maximum prevalence rate for which sensitivity and
specificity are both above 0.99 is 1%. Similarly, using C OMPS BL with the 45 × 105 Kirkman matrix, we observed that the

14

maximum prevalence rate for which sensitivity is above 0.99
and specificity is above 0.95 is 9.5%. Thus, Tapestry is viable
at prevalence rates as high as 9.5%, while reducing testing
cost by a factor of 2.3. On the other hand, if the prevalence
rate is only 1% or less, it can reduce testing cost by a factor
of 10.3.
Comments about sensitivity and specificity: We observe
that the sensitivity and specificity of our method on synthetic
data is within the recommendations of the U.S. Food and Drugs
Administration (FDA), as provided in this document [50].
The document provides recommendations for percent positive
agreement (PPA) and percent negative agreement (PNA) of
a COVID-19 test with a gold standard test (such as RT-PCR
done on individual samples). PPA and PNA are used instead
of sensitivity and specificity when ground-truth positives are
not known. Since for synthetic data we know the ground truth
positives, we compare their PPA and PNA recommendations
with the sensitivity and specificity observed by us. We use
C OMP-S BL for comparison, since we consider it to be our
best method.
For ‘Testing patients suspected of COVID-19 by their
healthcare provider’ (point G.4.a, page 7 of [50]), the document considers positive and negative agreement of ≥ 95%
as acceptable clinical performance (page 9, row 2 of table
in [50]). The sensitivity and specificity of our method on the
93 × 961 Kirkman matrix is within this range for k ≤ 17
infected samples (Table III). For the 45 × 105 matrix, it is
within this range for k ≤ 10 infected samples (Table S.XII).
For ‘Screening individuals without symptoms or other reasons to suspect COVID-19 with a previously unauthorized test’
(point G.4.c, page 10 of [50]), the document considers positive
agreement of ≥ 95% and negative agreement of ≥ 98% as
acceptable (along with the lower bounds of two-sided 95%
confidence interval to be > 76% and > 95% respectively).
Similarly, for ‘Adding population screening of individuals
without symptoms or other reasons to suspect COVID-19 to
an authorized test’ (point G.4.d, page 12 of [50]) the document
has the same criterion as for point G.4.c. Our sensitivity and
specificity are within the ranges specified for the 93 × 961
Kirkman matrix for k ≤ 12 (Table III). While we do not report
confidence intervals (as suggested for point G.4.c and G.4.d
of [50]), the standard deviation of sensitivity and specificity
reported by us are fairly low, and we believe the performance
of our method is within the recommendations of [50]. Since
our numbers are on synthetic data - these numbers may vary
upon full clinical validation, especially considering that there
may be more sources of error in a real test. Nonetheless, we
find these numbers to be encouraging.
Further, we note that while our method incurs an occasional
false negative, the viral loads of these false negative values are
fairly small. This means that super-spreaders (who are believed
to have high viral load [10]) will almost always be caught by
our method. In the supplemental material, we discuss this in
more detail in Sec. S.X, and provide a table of mean and
standard deviations of viral loads of false negatives (Table
S.VII) for all our methods on synthetic data.
Tapestry can detect certain errors caused by incorrect pipetting, pool contamination, or failed RT-PCR amplification of

some pools. This is done by performing a consistency check
after the C OMP stage. If there is a pool which is positive, but
all of the samples involved in it have been declared negative
by C OMP, this is indicative of error. In case of error, we list
all samples categorized by the number of tests that they are
positive in. However, the C OMP consistency check will not
catch all errors. Alternately, the noisy C OMP [12] algorithm
may be used to correct for errors in the C OMP stage. A full
exposition on detection and correction of errors is left as future
work.
Although Tapestry can work with a variety of sensing matrix
designs, we found Kirkman matrices to be most suitable for
our purposes. This is due to lower sparsity and smaller pool
sizes presented by Kirkman matrices. Our algorithms also
exhibit a more stable behaviour over a wide range of the
number of infected samples k when using Kirkman matrices.
We compare some alternative matrix designs in section S.VI.
V. R ELATION TO P REVIOUS W ORK
We review some recent work which apply CS or combinatorial group testing for COVID-19 testing. The works in [51],
[52], [53] adopt a nonadaptive CS based approach. The works
in [54], [55], [56] use combinatorial group testing. Compared
to these methods, our work is different in the following ways
(also see [6]):
1) Real/Synthetic data: Our work as well as that in [52] have
tested results on real data, while the rest present only
numerical or theoretical results.
2) Quantitative Noise model: Our work uses the physicallyderived noise model in Eqn. 3 (as opposed to only Gaussian
noise). This noise model is not considered in [51]. The
work in [53] considers unknown noise. Combinatorial group
testing methods [54], [55], [56] do not make use of quantitative information. The work in [52] uses only binary test
information, even though the decoding algorithm is based
on CS.
3) Algorithms: The work in [51] adopts the B PDN technique
(i.e P1 from Eqn. 11) as well as the brute-force search
method for reconstruction. The work in [52], [57] uses the
L ASSO, albeit with a ternary representation for the viral
loads. The work in [53] uses N NLAD. We use the L ASSO
with a non-negative constraint, the brute-force method,
N NLAD, as well as other techniques such as S BL and
N NOMP, all in combination with C OMP. The work in [51]
assumes knowledge of the (Gaussian) noise variance for
selection of ε in the estimator in Eqn. 11, whereas we
use cross-validation for all our estimators. The technique
in [52] uses a slightly different form of cross-validation
for selection of the regularization parameter in LASSO.
Amongst combinatorial algorithms, [56] uses C OMP, while
[54] and [55] use message passing.
4) Sensing matrix design: The work in [51] uses randomly
generated expander graphs, whereas we use Kirkman matrices. The work in [52] uses randomly generated sparse
Bernoulli matrices or Reed-Solomon codes, while [55] uses
Low-Density Parity Check (LDPC) codes [58]. The work
in [53] uses Euler square matrices [59], and the work in

15

[56] uses the Shifted Transversal Design [60]. Both are
airport security personnel, delivery personnel, or hospital
deterministic disjunct matrices like Kirkman matrices. Each
staff.
sample in our matrix participates in 3 pools as opposed to 3) Testing of 399 individuals in 63 tests. This can be used
5 pools as used in [55], 6 pools as used in [52] and [56],
to test students coming back to campuses, or police force,
and 8 pools as used in [53], which is advantageous from
or asymptomatic people in housing blocks and localities
the point of view of pipetting time.
currently under quarantine.
5) Sparsity estimation: Our work uses an explicit sparsity 4) Testing of 961 people in 93 tests, assuming low infection
estimator and does not rely on any assumption regarding
rate. This might be suitable for airports and other places
the prevalence rate.
where samples can be collected and tested immediately,
6) Numerical comparisons: We found that C OMP-N NLAD
and it might be possible to obtain liquid handling robots.
works better than the N NLAD method used in [53] on our Outputs: We have designed an Android app named Byom
matrices (see Tables V and S.XIX) . We also found that Smart Testing to make our Tapestry protocol easy to deploy in
C OMP-N NLASSO and C OMP-S BL have better sensitivity the future. The app can be accessed at [62]. We are also sharing
and specificity than C OMP-N NLAD (see Tables II, III, and our code and some amount of data at [63]. More information
V). The method in [52] can correctly identify up to 5/384 is also available at our website [64].
(1.3%) of samples with 48 tests, with an average number Future work: Future work will involve extensive testing on
of false positives that was less than 2.75, and an average real COVID-19 data, and extensive implementation of a variety
number of false negatives that was less than 0.33. On of algorithms for sensing matrix design as well as signal
synthetic simulations with their 48 × 384 Reed-Solomon recovery, keeping in mind the accurate statistical noise model
code based matrix (released by the authors) for a total of and accounting for occasional pipetting errors.
100 x vectors with ℓ0 norm of 5 using C OMP-N NLASSO,
we obtained 1.51 false positives and 0.02 false negatives
ACKNOWLEDGEMENT
on an average with a standard deviation of 1.439 and 0.14
AR
acknowledges
support from SERB Matrics grant
respectively. Using C OMP-S BL instead of C OMP-N NLASSO
MTR/2019/000691.
AR
and MG acknowledge support from
with all other settings remaining the same, we obtained
IITB
WRCB
grant
#10013976,
and DST-Rakshak grant
1.4 false positives and 0.0 false negatives on an average
#10013980.
The
authors
thank
the
two
anonymous reviewers
with a standard deviation of 1.6 and 0.1 respectively. As
as
well
as
the
Associate
Editor
for
careful
review of the
such, a direct numerical comparison between our work
previous
version
of
this
paper
and
helpful
suggestions
which
and that in [52] is not possible, due to lack of available
have
greatly
improved
this
paper.
real data, however these numbers yield some indicator of
performance.
R EFERENCES
7) Number of Tests: We use 93 tests for 961 samples while
achieving more than 0.99 sensitivity and specificity for k =
[1] D. Benatia, R. Godefroy, and J. Lewis, “Estimating COVID-19 prevalence in the united states: A sample selection model approach,” https://
10 infections using C OMP-S BL. In a similar setting, [55]
www.medrxiv.org/content/10.1101/2020.04.20.20072942v1.
use 108 tests for Q = 1000 samples under prevalence rate
[2] R. Dorfman, “The detection of defective members of large populations,”
0.01 for exact 2-stage recovery. The work in [56] uses 186
The Annals of Mathematical Statistics, vol. 14, no. 4, p. 436–440, 1943.
[3] “Israelis introduce method for accelerated covid-19 testing,” https://
tests for 961 samples under the same prevalence rate, albeit
www.israel21c.org/israelis- introduce- method-for-accelerated- covid-19for sensitivity equal to 1 and very high specificity. Matrix
testing/, [Online. Retrieved 08-Apr-21].
sizes studied in other work are very different than ours.
[4] “Corona ’pool testing’ increases worldwide capacities many times
over,”
https://healthcare- in-europe.com/en/news/corona-pool-testingThe work in [61] builds on top of our Tapestry scheme to
increases- worldwide-capacities- many-times-over.html,
[Online.
reduce the number of tests, but it is a two-stage adaptive
Retrieved 08-Apr-21].
technique and hence will require much more testing time.
[5] E. Candes and M. Wakin, “An introduction to compressive sampling,”
VI. C ONCLUSION
We have presented a non-adaptive, single-round technique
for prediction of infected samples as well as the viral loads,
from an array of n samples, using a compressed sensing
approach. We have empirically shown on synthetic data as
well as on some real lab acquisitions that our technique can
correctly predict the positive samples with a very small number
of false positives and false negatives. Moreover, we have
presented techniques for appropriate design of the mixing
matrix. Our single-round testing technique can be deployed
in many different scenarios such as the following:
1) Testing of 105 symptomatic individuals in 45 tests.
2) Testing of 195 asymptomatic individuals in 45 tests assuming a low rate of infection. A good use case for this is

IEEE Signal Processing Magazine, 2008.
[6] S. Ghosh et al., “Tapestry: A single-round smart pooling technique for
COVID-19 testing,” medRxiv, 2020. [Online]. Available: https://www.
medrxiv.org/content/early/2020/05/02/2020.04.23.20077727
[7] R. Kueng and P. Jung, “Robust nonnegative sparse recovery and the
nullspace property of 0/1 measurements,” IEEE Transactions on Information Theory, vol. 64, no. 2, pp. 689–703, 2018.
[8] E. W. Weisstein, “Kirkman’s schoolgirl problem,” https://mathworld.
wolfram.com/KirkmansSchoolgirlProblem.html, from MathWorld–A
Wolfram Web Resource. [Online. Retrieved 08-Apr-21].
[9] D. K. Ray-Chaudhuri and R. M. Wilson, “Solution of kirkman’s schoolgirl problem,” in Proc. symp. pure Math, vol. 19, 1971, pp. 187–203.
[10] P. M. Beldomenico, “Do superspreaders generate new superspreaders?
a hypothesis to explain the propagation pattern of COVID-19,” International Journal of Infectious Diseases, vol. 96, pp. 461–463, 2020.
[11] Y. Liu et al., “Viral dynamics in mild and severe cases of COVID-19,”
The Lancet Infectious Diseases, 2020.
[12] C. L. Chan et al., “Non-adaptive probabilistic group testing with
noisy measurements: Near-optimal bounds with efficient algorithms,”
in 49th Annual Allerton Conference on Communication, Control, and
Computing (Allerton), 2011, pp. 1832–1839.

16

[13] N. Jawerth, “How is the COVID-19 virus detected using real time
RT-PCR?” https://www.iaea.org/newscenter/news/how-is-the-covid-19virus-detected- using-real-time-rt-pcr.
[14] “Efficiency of real-time pcr,” https://www.thermofisher.com/in/en/
home/life-science/pcr/real-time-pcr/real-time-pcr-learning- center/realtime-pcr-basics/efficiency- real-time-pcr-qpcr.html, [Online; accessed
5-April-2021].
[15] M. Aldridge, L. Baldassini, and O. Johnson, “Group testing algorithms:
Bounds and simulations,” IEEE Transactions on Information Theory,
vol. 60, no. 6, pp. 3671–3687, 2014.
[16] A. Gilbert, M. Iwen, and M. Strauss, “Group testing and sparse signal
recovery,” in Asilomar Conference on Signals, Systems and Computers,
2008, p. 1059–1063.
[17] N. Zhao et al., “Motion compensated dynamic mri reconstruction with
local affine optical flow estimation,” IEEE Trans. on Biomedical Engg.,
vol. 66, no. 11, 2019.
[18] Z. Zhang et al., “Compressed sensing for energy-efficient wireless
telemonitoring of noninvasive fetal ECG via block sparse bayesian
learning,” IEEE Trans. Biomedical Engg., vol. 60, no. 2, 2013.
[19] Y. Liu, M. D. Vos, and S. V. Huffel, “Compressed sensing of multichannel EEG signals: The simultaneous cosparsity and low-rank optimization,” IEEE Trans. Biomedical Engg., vol. 62, no. 8, 2015.
[20] E. Candes, “The restricted isometry property and its implications for
compressive sensing,” Comptes Rendus Mathematiques, 2008.
[21] R. Baraniuk et al., “A simple proof of the restricted isometry property
for random matrices,” Constr Approx, vol. 28, p. 253–263, 2008.
[22] R. DeVore, “Deterministic construction of compressed sensing matrices,” J. Complexity, vol. 23, p. 918–925, 2007.
[23] M. Lotfi and M. Vidyasagar, “Compressed sensing using binary matrices
of nearly optimal dimensions,” IEEE Transactions on Signal Processing,
vol. 68, pp. 3008–3021, 2020.
[24] M. Davenport et al., “Introduction to compressed sensing,” in Compressed Sensing: Theory and Applications, Y. Eldar and G. Kutyniok,
Eds. Cambridge University Press, 2012, p. 1–64.
[25] R. Berinde et al., “Combining geometry and combinatorics: A unified
approach to sparse signal recovery,” in 46th Annual Allerton Conference
on Communication, Control, and Computing, 2008, pp. 798–805.
[26] T. Hastie, R. Tibshirani, and M. Wainwright, Statistical Learning with
Sparsity: The LASSO and Generalizations. CRC Press, 2015.
[27] Y. Pati, R. Rezaiifar, and P. Krishnaprasad, “Orthogonal matching
pursuit: recursive function approximation with application to wavelet
decomposition,” in Asilomar Conf. On Signals, Systems and Computing,
1993, pp. 40–44.
[28] T. T. Cai and L. Wang, “Orthogonal matching pursuit for sparse signal
recovery with noise,” IEEE Transactions on Information Theory, vol. 57,
no. 7, pp. 4680–4688, 2011.
[29] M. Yaghoobi, D. Wu, and M. Davies, “Fast non-negative orthogonal
matching pursuit,” IEEE Signal Processing Letters, vol. 22, no. 9, pp.
1229–1233, 2015.
[30] M. Tipping, “Sparse Bayesian learning and the relevance vector machine,” Journal of Machine Learning Research, 2001.
[31] D. Wipf and B. D. Rao, “Sparse Bayesian learning for basis selection,”
IEEE Trans. Signal Processing, vol. 52, no. 8, 2004.
[32] E. Crespo Marques et al., “A review of sparse recovery algorithms,”
IEEE Access, vol. 7, pp. 1300–1322, 2019.
[33] A. Nalci et al., “Rectified gaussian scale mixtures and the sparse nonnegative least squares problem,” IEEE Transactions on Signal Processing, vol. 66, no. 12, pp. 3124–3139, 2018.
[34] H. Petersen, B. Bah, and P. Jung, “Efficient noise-blind ℓ1 -regression of
nonnegative compressible signals,” 2020.
[35] M. Lotfi and M. Vidyasagar, “A fast noniterative algorithm for compressive sensing using binary measurement matrices,” IEEE Transactions on
Signal Processing, vol. 66, no. 15, 2018.
[36] M. Raginsky et al., “Performance bounds for expander-based compressed sensing in poisson noise,” IEEE Trans. Sig. Processing, vol. 59,
no. 9, 2011.
[37] Wikipedia contributors, “Steiner triple systems,” https://en.wikipedia.
org/wiki/Steiner system#Steiner triple systems, 2021, [Online; accessed 8-April-2021].
[38] E. J. Pegg, “Social golfer problem,” https://mathworld.wolfram.com/
SocialGolferProblem.html, from MathWorld–A Wolfram Web Resource,
created by Eric W. Weisstein. [Online. Retrieved 08-Apr-21].
[39] “Math games: Social golfer problem,” http://www.mathpuzzle.com/
MAA/54-Golf%20Tournaments/mathgames 08 14 07.html.
[40] M. Triska, Solution methods for the social golfer problem. Citeseer,
2008.

[41] I. Dotú and P. Van Hentenryck, “Scheduling social golfers locally,” in
International Conference on Integration of Artificial Intelligence (AI)
and Operations Research (OR) Techniques in Constraint Programming.
Springer, 2005, pp. 155–167.
[42] S. J. Johnson and S. R. Weller, “Construction of low-density paritycheck codes from kirkman triple systems,” in GLOBECOM’01. IEEE
Global Telecommunications Conference (Cat. No. 01CH37270), vol. 2.
IEEE, 2001, pp. 970–974.
[43] V. Vermeirssen et al., “Matrix and steiner-triple-system smart pooling
assays for high-performance transcription regulatory network mapping,”
Nature methods, vol. 4, no. 8, pp. 659–664, 2007.
[44] Wikipedia contributors, “Steiner systems,” https://en.wikipedia.org/wiki/
Steiner system, 2021, [Online; accessed 8-April-2021].
[45] V. D. Tonchev, “Steiner systems for two-stage disjunctive testing,”
Journal of combinatorial optimization, vol. 15, no. 1, pp. 1–6, 2008.
[46] C. Studer and R. Baraniuk, “Stable restoration and separation of approximately sparse signals,” Applied and Computational Harmonic Analysis,
vol. 37, no. 1, pp. 12 – 35, 2014.
[47] V. Abdoghasemi et al., “On optimization of the measurement matrix for
compresive sensing,” in EUSIPCO, 2010.
[48] V. Bioglio, T. Bianchi, and E. Magli, “On the fly estimation of the
sparsity degree in compressed sensing using sparse sensing matrices,”
in ICASSP, 2015, p. 3801–3805.
[49] C. Ravazzi et al., “Sparsity estimation from compressive projections via
sparse random matrices,” EURASIP J. Adv. Signal Process., vol. 56,
2018.
[50] “In vitro diagnostics euas, section: Templates for eua
submissions/diagnostic templates (molecular and antigen), bullet:
Molecular diagnostic template for laboratories,” retrieved 27-Mar-21.
[Online]. Available: https://www.fda.gov/medical-devices/coronavirusdisease-2019-covid-19-emergency-use-authorizations- medical-devices/
in-vitro-diagnostics- euas#covid19ivdTemplates
[51] J. Yi, R. Mudumbai, and W. Xu, “Low-cost and high-throughput
testing of COVID-19 viruses and antibodies via compressed sensing:
System concepts and computational experiments,” https://arxiv.org/abs/
2004.05759, 2020.
[52] N. Shental et al., “Efficient high-throughput SARS-CoV-2
testing to detect asymptomatic carriers,” Science Advances, 2020.
[Online]. Available: https://advances.sciencemag.org/content/early/2020/
08/20/sciadv.abc5961
[53] H. Petersen, B. Bah, and P. Jung, “Practical high-throughput, nonadaptive and noise-robust SARS-CoV-2 testing,” https://arxiv.org/abs/
2007.09171, 2020.
[54] J. Zhu, K. Rivera, and D. Baron, “Noisy pooled PCR for virus testing,”
https://arxiv.org/abs/2004.02689, 2020.
[55] J.-T. Seong, “Group testing-based robust algorithm for diagnosis of
COVID-19,” Diagnostics, vol. 10, no. 6, p. 396, 2020.
[56] M. Täufer, “Rapid, large-scale, and effective detection of COVID-19 via
non-adaptive testing,” Journal of Theoretical Biology, vol. 506, 2020.
[57] H. Nida et al., “Highly efficient de novo mutant identification in a
sorghum bicolor tilling population using the comseq approach,” The
Plant Journal, vol. 86, no. 4, pp. 349–359, 2016.
[58] D. J. MacKay, “Good error-correcting codes based on very sparse
matrices,” IEEE transactions on Information Theory, vol. 45, no. 2, pp.
399–431, 1999.
[59] R. R. Naidu, P. Jampana, and C. S. Sastry, “Deterministic compressed
sensing matrices: Construction via euler squares and applications,” IEEE
Transactions on Signal Processing, vol. 64, no. 14, pp. 3566–3575, 2016.
[60] N. Thierry-Mieg, “A new pooling strategy for high-throughput screening:
the shifted transversal design,” BMC bioinformatics, vol. 7, no. 1, p. 28,
2006.
[61] A. Heidarzadeh and K. R. Narayanan, “Two-stage adaptive pooling
with rt-qpcr for COVID-19 screening,” arXiv preprint arXiv:2007.02695,
2020.
[62] “Byom app,” https://rebrand.ly/byom-app.
[63] “Tapestry code,” https://github.com/atoms-to-intelligence/tapestry.
[64] “Tapestry website,” https://www.tapestry- pooling.com/.
[65] D. Du, F. K. Hwang, and F. Hwang, Combinatorial group testing and
its applications. World Scientific, 2000.
[66] J. Zhang et al., “On the theoretical analysis of cross validation in
compressive sensing,” in ICASSP, 2014, pp. 3370–3374.
[67] Y. Li and G. Raskutti, “Minimax optimal convex methods for Poisson
inverse problems under lq-ball sparsity,” IEEE Trans. Information Theory, vol. 64, no. 8, 2018.

1

Supplementary Materials: A Compressed Sensing Approach to
Pooled RT-PCR Testing for COVID-19 Detection

arXiv:2005.07895v2 [q-bio.QM] 29 Apr 2021

Sabyasachi Ghosh†, Rishi Agarwal †, Mohammad Ali Rehan †, Shreya Pathak †,
Pratyush Agarwal †, Yash Gupta †, Sarthak Consul ∗, Nimay Gupta †, Ritika †, Ritesh Goenka †,
Ajit Rajwade †, Manoj Gopalkrishnan ∗

S.I. N OTATIONS U SED
We refer the reader to the main paper for the meaning of
various symbols and notations employed in this document.
S.II. G ENERALIZED B INARY S EARCH T ECHNIQUES
In the class of ‘adaptive group testing’ techniques, the n
samples are distributed into two or more groups, each of
smaller size, and the smaller groups are then individually
tested. In one particular adaptive method called generalized
binary splitting (GBS) [65], this procedure is repeated (in
a binary search fashion) until a single infected sample is
identified. This requires O(log n) sequential tests, where each
test requires mixing up to n/2 samples. This sample is
then discarded, and the entire procedure is performed on
the remaining n − 1 samples. Such a procedure does not
introduce any false negatives, and does not require prior
knowledge of the number of infected samples k. It requires a
total of only O(k log n) tests, if k is the number of infected
samples. However such a multi-stage method is impractical to
be deployed due to its sequential nature, since each RT-PCR
stage requires nearly 3-4 hours. Moreover, each mixture that is
tested contains contributions from as many as O(n) samples,
which can lead to significant dilution or may be difficult to
implement in the lab. Hence in this work, we do not pursue
this particular approach. Such an approach may be very useful
if each individual test had a quick turn-around time.
S.III. D ETAILS OF C ROSS -VALIDATION
S ENSING

FOR

C OMPRESSED

For this, the measurements in y are divided into two
randomly chosen disjoint sets: one for reconstruction (R)
and the other for validation (V). A decoding algorithm such
as N NLASSO is executed independently on multiple values
of the regularization parameter λ from a candidate set Λ.
(Other decoding algorithms will have their own parameters.
For example, N NOMP or BPDN will use the parameter ε,
i.e. the bound on the noise magnitude.) For each λ value, an
estimate x̂λ is produced using
P measurements only from R,
and the CV error ve (λ) := i∈V (yi − Ai x̂λ )2 is computed.
The value of λ which yields the least value of ve (λ) is chosen,
and a final estimate of x is obtained by executing the algorithm
again, but now using all measurements from R ∪ V. If V is
large enough, then ve (λ) is shown to be a good estimate of the
†Dept. of Computer Science & Engg., IIT Bombay, India; {sghosh,
ajitvr}@cse.iitb.ac.in
∗ Dept. of Electrical Engineering, IIT Bombay; manojg@ee.iitb.ac.in

actual error kx − x̂λ k2 , as has been shown for Gaussian noise
[66]. Nonetheless, it should be noted that CV is a method of
choice for parameter selection in CS even under a variety of
other noise models such as Poisson [67], etc, and we have
experimentally observed that it works well even in the case of
our noise model in Eqn. 3 of the main paper.
S.IV. B RUTE - FORCE

SEARCH METHOD

We refer to C OMP-B F as a method where we apply C OMP
followed by a brute-force search to minimize the cost function
in Eqn. S1 below. The brute-force search is computationally
feasible only when C(n, k) is ‘reasonable’ in value (note that
the effective n is often reduced after application of C OMP),
and so we employ it only for small-sized matrices. The method
essentially enumerates all possible supports of x which have
size k. For each such candidate support set Z, the following
cost function is minimized using the fmincon routine of
MATLAB which implements an interior-point optimizer 1 :
J(xZ ) := k log y − log AZ xZ k2 such that xZ ≥ 0. (S1)
Results with the C OMP-B F method are shown in Table S.I.
The special advantage of the brute-force method is that it
requires only m = 2k pools, which is less than O(k log n).
However, such a method requires prior knowledge of k, or an
estimate thereof. We employ a method to estimate k directly
from y, A. This is described in Sec. IV-A8 of the main paper.
The results in Table S.I assume that the exact k was known,
or that the estimator predicted the exact k. However, we
observed that the estimator from Sec. IV-A8 can sometimes
over-estimate k. Hence, we also present results with C OMPB F where the brute-force search assumed that the sparsity was
(over-estimated to be) k + 1 instead of k. These are shown
in Table S.II. A comparison of Tables S.I and S.II shows that
RMSE deteriorates if k is incorrectly estimated. However there
is no adverse effect on the number of false negatives, and only
a small adverse effect on the number of false positives.
S.V. RIP AND RNSP P RESERVATION

AFTER

C OMP

We prove that the reduced matrix AX̄ ,Ȳ obtained after
C OMP (Sec. III-D of the main paper) preserves RIP-1, RIP-2
and RNSP of the full matrix A, of same order and with the
same parameters. Recall definition of X , Y, X̄ and Ȳ from
section III-D of the main paper.
1 https://in.mathworks.com/help/optim/ug/choosing-the-algorithm.html#
bsbwxm7

2

Again, since xj = 0 ∀j ∈ X , the second term in eq. (S4) is
0. Hence,
X X
p 1/p
kAxkp =
.
(S5)
Aij xj

TABLE S.I
P ERFORMANCE OF C OMP FOLLOWED BY BRUTE - FORCE SEARCH
(C OMP -B F ) TO MINIMIZE THE FUNCTION IN E QN . S1 FOR A 16 × 40
MATRIX OPTIMIZED ON MUTUAL COHERENCE , WITH DIFFERENT VALUES
OF k, ASSUMING THAT THE TRUE VALUE OF k WAS KNOWN . R ESULTS FOR
BOTH METHODS ARE REPORTED ON SYNTHETIC DATA .
Method
C OMP
C OMP -B F
C OMP
C OMP -B F
C OMP
C OMP -B F

k
2
2
3
3
4
4

RMSE
1.00
0.03
1.00
0.05
1.00
0.05

#false neg.
0.00
0.00
0.00
0.00
0.00
0.00

#false pos.
0.50
0.00
1.85
0.00
3.35
0.00

sens.
1.00
1.00
1.00
1.00
1.00
1.00

i∈Ȳ

spec.
0.99
1.00
0.95
1.00
0.91
1.00

Notice that the RHS in eq. (S5) is simply kAX̄ ,Ȳ x̄kp by
definition of AX̄ ,Ȳ and x̄. Hence, kAxkp = kAX̄ ,Ȳ x̄kp .
This lemma is critical to proving the following RIPpreservation theorems:
Theorem 2. Let d be a constant. If the scaled matrix A/d
satisfies RIP-1 of order k with k-order RIC δk ∈ (0, 1), and
k ≤ |X̄ |, then the scaled C OMP-reduced matrix AX̄ ,Ȳ /d also
satisfies RIP-1 of order k with RIC δk . That is, if

TABLE S.II
P ERFORMANCE OF C OMP FOLLOWED BY BRUTE - FORCE SEARCH
(C OMP -B F ) TO MINIMIZE THE COST FUNCTION IN E QN . S1 FOR 16 × 40
MATRIX OPTIMIZED ON MUTUAL COHERENCE , WITH DIFFERENT VALUES
OF k, ASSUMING THAT THE SPARSITY VALUE WAS ESTIMATED TO BE
k + 1. R ESULTS FOR BOTH METHODS ARE REPORTED ON SYNTHETIC
DATA .
Method
C OMP
C OMP -B F
C OMP
C OMP -B F
C OMP
C OMP -B F

k
2
2
3
3
4
4

RMSE
1.00
0.47
1.00
0.24
1.00
0.15

#false neg.
0.00
0.00
0.00
0.00
0.00
0.00

#false pos.
0.75
0.55
1.70
0.80
3.00
0.90

sens.
1.00
1.00
1.00
1.00
1.00
1.00

1
kAxk1 ≤ (1 + δk )kxk1 ,
d
for all n-dimensional k-sparse vectors x then,
kxk1 ≤

spec.
0.98
0.99
0.95
0.98
0.92
0.97

kx̄k1 ≤

Proof. Let x̄ be a |X̄ | dimensional k-sparse vector. Consider
a n-dimensional vector x such that xX̄ = x̄ and xX = 0. x
is also a k-sparse vector. By definition of RIP-1,
1
kAxk1 ≤ (1 + δk )kxk1 .
d
Using Lemma 1, kAxk1 = kAX̄ ,Ȳ x̄k1 . Hence,
kxk1 ≤

(S2)

1
kA
x̄k1 ≤ (1 + δk )kxk1 .
d X̄ ,Ȳ
Also note that kxk1 = kx̄k1 . Hence,
kxk1 ≤

This is because C OMP eliminates all samples for which
Aij > 0 and yi = 0 – hence if for any sample j, Aij > 0 and
yi = 0, then j ∈ X . Second, we observe the following:

kx̄k1 ≤

Lemma 1. Let x̄ denote a |X̄ | dimensional vector. Consider
a n-dimensional vector x such that xX̄ = x̄ and xX = 0.
Then, kAxkp = kAX̄ ,Ȳ x̄kp for any p ∈ Z+ .

Theorem 3. Let d be a constant. If the scaled matrix A/d
satisfies RIP-2 of order k with RIC δk ∈ (0, 1) and k ≤ |X̄ |,
then the scaled C OMP-reduced matrix AX̄ ,Ȳ /d also satisfies
RIP-2 of order k with RIC δk . That is, if
1
kAxk22 ≤ (1 + δk )kxk22 .
d2
for all n-dimensional k-sparse vectors x then,
(1 − δk )kxk22 ≤

i

=

|Ai x|p +

=

X

|Ai x|p +

i∈Ȳ

i∈Ȳ

X

i∈Y

|Ai x|p

X X

i∈Y

1/p

Aij xj +

X

Aij xj

j∈X

j∈X̄

p 1/p

(S3)
Using eq. (S2), the second term in eq. (S3) is 0. The third
term is 0 as well, since xj = 0 ∀j ∈ X . Hence,
X
1/p
kAxkp =
|Ai x|p
i∈Ȳ

=

X X
i∈Ȳ

j∈X̄

Aij xj +

X

j∈X

Aij xj

p 1/p

.

(S4)

1
kA
x̄k1 ≤ (1 + δk )kx̄k1
d X̄ ,Ȳ

.

Proof. Let Ai denote the 1 × n ith row vector of the matrix
A.
1/p
X
|Ai x|p
kAxkp =
X

1
kA
x̄k1 ≤ (1 + δk )kx̄k1
d X̄ ,Ȳ

for all |X̄ |-dimensional k-sparse vectors x̄.

First, notice that in all the rows of the matrix A for
which the measurement vector y was 0, the entries in A
corresponding to indices from X̄ contained only 0. That is,
Aij = 0 ∀i ∈ Y, j ∈ X̄ .

j∈X̄

.

(1 − δk )kx̄k22 ≤

1
kAX̄ ,Ȳ x̄k22 ≤ (1 + δk )kx̄k22
d2

for all |X̄ |-dimensional k-sparse vectors x̄.
Proof. Let x̄ be a |X̄ | dimensional k-sparse vector. Consider
a n-dimensional vector x such that xX̄ = x̄ and xX = 0.
Hence x is also a k-sparse vector. By definition of RIP-2,
1
kAxk22 ≤ (1 + δk )kxk22 .
d2
Using Lemma 1, kAxk2 = kAX̄ ,Ȳ x̄k2 . Hence,
(1 − δk )kxk22 ≤

(1 − δk )kxk22 ≤

1
kAX̄ ,Ȳ x̄k22 ≤ (1 + δk )kxk22 .
d2

3

Also note that kxk2 = kx̄k2 . Hence,

1
(1 − δk )kx̄k22 ≤ 2 kAX̄ ,Ȳ x̄k22 ≤ (1 + δk )kx̄k22
d

.
Theorem 4. If the matrix A satisfies ℓ2 -RNSP of order k with
parameters ρ and τ , then the C OMP-reduced matrix AX̄ ,Ȳ
also satisfies ℓ2 -RNSP of order k with the same parameters ρ
and τ . That is, if
ρ
kxS k2 ≤ √ kxS̄ k1 + τ kAxk2 ∀x ∈ Rn
k
holds for all S ⊂ {1 . . . n} with |S| ≤ k then,
ρ
kx̄B k2 ≤ √ kx̄B̄ k1 + τ kAX̄ ,Ȳ x̄k2 ∀x̄ ∈ R|X̄ |
k
holds for all B ⊂ {1 . . . |X̄ |} with |B| ≤ k.
Proof. Let x̄ be a |X̄ |-dimensional vector. Consider a ndimensional vector x such that xX̄ = x̄ and xX = 0. For
any set B ⊂ {1, . . . , |X̄ |}, consider the set S ⊂ X̄ , such that
xS = x̄B , and the set S̄ = X ∪ (X̄ − S). By definition of
ℓ2 -RNSP,
ρ
kxS k2 ≤ √ kxS̄ k1 + τ kAxk2
k
Using Lemma 1, kAxk2 = kAX̄ ,Ȳ x̄k2 . Also note that
kxS k2 = kx̄B k2 , and kxS̄ k1 = kx̄B̄ k1 . Hence,
ρ
kx̄B k2 ≤ √ kx̄B̄ k1 + τ kAX̄ ,Ȳ x̄k2 .
k
Theorem 5. If the matrix A satisfies RNSP of order k with
parameters ρ and τ , then the C OMP-reduced matrix AX̄ ,Ȳ
also satisfies RNSP of order k with the same parameters ρ
and τ . That is, if
kxS k2 ≤ ρkxS̄ k1 + τ kAxk2 ∀x ∈ Rn
holds for all S ⊂ {1 . . . n} with |S| ≤ k then,
kx̄B k2 ≤ ρkx̄B̄ k1 + τ kAX̄ ,Ȳ x̄k2 ∀x̄ ∈ R|X̄ |
holds for all B ⊂ {1 . . . |X̄ |} with |B| ≤ k.
Proof. The proof follows the same argument as for theorem 4,
except the coefficient √ρk is replaced by ρ.
S.VI. S ENSING M ATRIX C OMPARISON
We generated sensing matrices optimized to have low
mutual coherence, as described in Sec. III-F. We have observed that these matrices, besides originating from randomly
generated matrices, have a much higher number of non-zero
elements and higher pool sizes compared to Kirkman or STS
matrices. This leads to difficulty in pooling, increased pooling
time, wastage of sample, and sample dilution. A well-tuned
93 × 961 matrix that we generated using this method had 6517
non-zero elements, with its smallest pool being of size 66. In
comparison, the Kirkman triple matrix of the same dimensions
had 2883 non-zero elements and a pool size equal to 31. Table
S.III shows the performance of C OMP followed by S BL on

this matrix designed to minimize coherence, using synthetic
data. For smaller values of k (upto 12), it has lower false
negatives and false positives than the same algorithm on the
93 × 961 Kirkman matrix (Table III). However, the number
of false positives increases significantly for higher number of
infections.
We also considered Bernoulli(p) random matrices, whose
entries are 1 with probability p, and 0 otherwise. Here p is the
Bernoulli parameter and lies in the range (0, 1). It is shown
in [7] that Bernoulli(p) matrices are good for compressed
sensing. That is, they satisfy a robust nullspace property of
order k if the number of rows m = O(k log n/k), with very
high probability. On the other hand, Bernoulli(1/k) sensing
matrices with O(k log n) rows incur low probability of error
when C OMP is used to determine the support of k-sparse
vectors [12]. Table S.IV shows the performance of C OMP
followed by S BL on a 93 × 961 Bernoulli(0.5) matrix. The
C OMP step does not help that much in this case, leading to
a large number of false positives in the subsequent S BL step.
Table S.V shows the performance of C OMP followed by S BL
on a 93 × 961 Bernoulli(0.1) matrix. Results on this matrix
are better than the Bernoulli(0.5) matrix. However, neither of
these matrices come close to the performance exhibited by
Kirkman triple matrices (e.g. Table III).
S.VII. N ON - NEGATIVE L EAST S QUARES (N NLS )
We present the results of running C OMP followed by Nonnegative least squares (N NLS, section III-D4) on synthetic data
for the 93 × 961 Kirkman matrix in Table S.VI.
S.VIII. S YNTHETIC DATA RESULTS ON 45 × 105 K IRKMAN
TRIPLE MATRIX

We present performance of C OMP followed by CS algorithms on synthetic data for the Kirkman triple matrix of
size 45 × 105 in Tables S.X, S.XI, S.XII, S.XIII, S.XIV,
and S.XV. We followed the same methodology as in Sec.
IV-A1 of the main paper for these experiments. Using C OMP
without the CS step gives acceptable performance up to k = 8.
For larger values of k, significant improvements are seen by
performing the additional CS step after C OMP. C OMP-S BL is
the best algorithm, achieving high sensitivity and specificity
for a wide range of k. It is followed closely by C OMP-N NLAD,
C OMP-N NLASSO, and C OMP-N NLS, all of which give higher
false negatives than C OMP-S BL for high values of k. C OMPN NOMP provides the highest specificity, albeit at the cost of
lower sensitivity than the other algorithms, especially for high
values of k. We note that C OMP-S BL and C OMP-N NLAD
achieve greater than 0.99 sensitivity and 0.95 specificity for
k upto 10. C OMP-S BL’s performance degrades gracefully for
higher values of k.
S.IX. S YNTHETIC DATA RESULTS USING CS

ALGORITHMS

ONLY

We provide results of running CS algorithms on synthetic
data for the 93 × 961 and 45 × 105 Kirkman triple matrices,
without doing the C OMP preprocessing step, in Tables S.XVI,
S.XVII, S.XVIII, S.XIX, S.XX, S.XXI, S.XXII, S.XXIII,

4

TABLE S.III
P ERFORMANCE OF COMP FOLLOWED BY SBL ( ON SYNTHETIC DATA ) FOR 93 × 961 MATRIX OPTIMIZED FOR LOW MUTUAL COHERENCE . F OR EACH
CRITERION , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.031 ± 0.012
0.041 ± 0.013
0.049 ± 0.013
0.062 ± 0.018
0.098 ± 0.030
0.120 ± 0.029
0.126 ± 0.029

#FN
0.0 ± 0.0
0.0 ± 0.1
0.0 ± 0.2
0.1 ± 0.3
0.2 ± 0.5
0.4 ± 0.6
0.6 ± 0.7

#FP
0.1 ± 0.3
1.8 ± 1.5
5.7 ± 3.3
13.0 ± 6.3
32.0 ± 12.8
49.8 ± 18.4
86.2 ± 28.6

Sens.
1.0000 ± 0.0000
0.9996 ± 0.0068
0.9973 ± 0.0168
0.9920 ± 0.0262
0.9843 ± 0.0320
0.9777 ± 0.0356
0.9718 ± 0.0368

Spec.
0.9999 ± 0.0003
0.9982 ± 0.0015
0.9940 ± 0.0034
0.9863 ± 0.0066
0.9662 ± 0.0135
0.9473 ± 0.0195
0.9084 ± 0.0304

TABLE S.IV
P ERFORMANCE OF COMP FOLLOWED BY SBL ( ON SYNTHETIC DATA ) FOR 93 × 961 B ERNOULLI (0.5) MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 100 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.123 ± 0.051
0.147 ± 0.027
0.167 ± 0.026
0.190 ± 0.033
0.254 ± 0.050
0.288 ± 0.064
0.365 ± 0.096

#FN
0.1 ± 0.3
0.3 ± 0.5
0.4 ± 0.6
0.5 ± 0.6
1.0 ± 0.9
1.3 ± 0.9
1.9 ± 1.3

#FP
127.9 ± 127.8
405.9 ± 132.3
466.4 ± 65.7
475.1 ± 49.5
479.5 ± 37.0
484.6 ± 37.1
488.7 ± 44.9

Sens.
0.9840 ± 0.0543
0.9675 ± 0.0652
0.9580 ± 0.0603
0.9558 ± 0.0533
0.9353 ± 0.0607
0.9241 ± 0.0554
0.9040 ± 0.0651

Spec.
0.8662 ± 0.1337
0.5741 ± 0.1388
0.5096 ± 0.0690
0.4994 ± 0.0521
0.4932 ± 0.0391
0.4867 ± 0.0393
0.4806 ± 0.0477

TABLE S.V
P ERFORMANCE OF COMP FOLLOWED BY SBL ( ON SYNTHETIC DATA ) FOR 93 × 961 B ERNOULLI (0.1) MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.035 ± 0.015
0.052 ± 0.018
0.073 ± 0.028
0.106 ± 0.039
0.137 ± 0.034
0.138 ± 0.031
0.141 ± 0.031

#FN
0.0 ± 0.0
0.0 ± 0.1
0.1 ± 0.2
0.2 ± 0.4
0.4 ± 0.6
0.5 ± 0.6
0.6 ± 0.8

#FP
1.5 ± 1.6
8.0 ± 5.6
17.9 ± 9.7
33.7 ± 18.3
68.8 ± 30.9
98.6 ± 38.5
155.8 ± 51.7

Sens.
0.9998 ± 0.0063
0.9974 ± 0.0179
0.9940 ± 0.0242
0.9863 ± 0.0334
0.9757 ± 0.0386
0.9720 ± 0.0382
0.9679 ± 0.0394

Spec.
0.9985 ± 0.0017
0.9916 ± 0.0058
0.9811 ± 0.0102
0.9645 ± 0.0193
0.9273 ± 0.0327
0.8955 ± 0.0408
0.8345 ± 0.0549

TABLE S.VI
P ERFORMANCE OF COMP FOLLOWED BY NNLS ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED , ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.046 ± 0.018
0.070 ± 0.031
0.097 ± 0.044
0.154 ± 0.093
0.288 ± 0.164
0.397 ± 0.174
0.528 ± 0.167

#FN
0.0 ± 0.1
0.1 ± 0.3
0.2 ± 0.5
0.5 ± 0.7
0.9 ± 1.0
0.8 ± 1.2
0.2 ± 0.9

#FP
0.8 ± 0.9
4.0 ± 2.2
7.9 ± 3.1
13.1 ± 5.5
29.1 ± 17.0
50.0 ± 24.1
87.8 ± 25.3

S.XXIV and S.XXV. We find that running the CS algorithms
without the C OMP step increases false negatives or false
positives by a large factor for each algorithm. We observed
that this method was computationally more expensive due to
the algorithms running on the full matrix. We used the same
methodology as in Sec. IV-A1. Results for all algorithms are
over 1000 randomly generated signals for each k, except for
N NLASSO on 93 × 961 Kirkman triple matrix, for which only
100 signals were used.
S.X. V IRAL L OADS OF FALSE N EGATIVES
As seen in Tables I through V in the main paper, our method
gives far fewer false positives when compared to C OMP, at the
cost of a rare false negative. Furthermore, due to the numerical

Sens.
0.9992 ± 0.0126
0.9912 ± 0.0324
0.9777 ± 0.0468
0.9566 ± 0.0586
0.9397 ± 0.0685
0.9547 ± 0.0715
0.9883 ± 0.0428

Spec.
0.9992 ± 0.0009
0.9958 ± 0.0023
0.9917 ± 0.0032
0.9862 ± 0.0058
0.9692 ± 0.0180
0.9470 ± 0.0256
0.9067 ± 0.0269

nature of our method, there is asymmetry in our mode of
failure, and we fail less on samples with high viral load. That
is, even the rare false negatives that we fail to detect have very
small viral loads. Table S.VII shows the mean and standard
deviation of viral loads of false negative samples for obtained
by our method on synthetic data for a range of values of k. We
see that for k upto 10, these false negative viral load values
are very small. For example, if we use C OMP followed by
S BL, the average viral load value of false negative samples is
0.023 ± 0.026, around 40 times lower than the maximum viral
load (1.0). This means that our method almost never misses a
sample with high viral load. This matters particularly because
COVID-19 super-spreaders are believed to have a high viral
load [10].

5

TABLE S.VII
V IRAL LOAD OF FALSE NEGATIVE SAMPLES FOR THE COMP FOLLOWED BY CS ALGORITHMS USING A 93 × 961 K IRKMAN MATRIX ON SYNTHETIC
DATA , NORMALIZED BY THE MAXIMUM POSSIBLE VIRAL LOAD . M EAN AND STANDARD DEVIATION REPORTED OVER 1000 RANDOMLY GENERATED
SIGNALS .
k
5
8
10
12
15
17
20

COMP-SBL
0.013 ± 0.007
0.015 ± 0.017
0.023 ± 0.026
0.047 ± 0.080
0.093 ± 0.134
0.192 ± 0.227
0.306 ± 0.250

COMP-NNOMP
0.028 ± 0.031
0.023 ± 0.024
0.032 ± 0.052
0.066 ± 0.099
0.151 ± 0.181
0.224 ± 0.217
0.299 ± 0.240

COMP-NNLASSO
0.021 ± 0.011
0.024 ± 0.022
0.035 ± 0.040
0.057 ± 0.066
0.098 ± 0.107
0.110 ± 0.113
0.192 ± 0.164

S.XI. D EPENDENCE OF

RELATIVE VIRAL LOADS ON
PARAMETER q

Typical RT-PCR output will not give us viral loads of the
pools, rather, only the threshold cycle (Ct ) values (see Sec. II
of the main paper). In order to convert from the Ct values to
the relative viral load vector, we use Eqn. (8), as explained in
Sec. III-A1 of the main paper. For this, we need the value of
the parameter q, which we set to 0.95, since we expect the
viral amount in each pool to roughly double in each RT-PCR
cycle.
The parameter q may be estimated from raw RT-PCR data,
which contains the fluorescence values found at the end of
each RT-PCR thermal cycle for each pool. If we take the
natural logarithm on both Eqn. (5) (page 3 of the main paper),
we can see that the logarithm of the fluorescence of a pool has
a linear dependence on cycle time. Hence, q may be obtained
by performing linear regression on log fluorescence values for
any (positive) pool [47].
The value of q thus obtained will have some dependence
on the pool so chosen. However, the relative viral loads of
declared positives obtained by our algorithm show negligible
variance in most cases, especially for samples with high viral
load, over a large range of q (see Table S.VIII). In some cases,
especially for samples with low viral loads, we do observe
some variance. However, it never happens that a sample with
high relative viral load is reported to have low relative viral
load, or vice-versa. Moreover, the declared positives do not
change for any experiment.

S.XII. S ENSITIVITY

OF RESULTS TO CHOICE OF
THRESHOLD τ

COMP-NNLAD
0.012 ± 0.014
0.029 ± 0.032
0.034 ± 0.036
0.067 ± 0.075
0.084 ± 0.083
0.085 ± 0.083
0.104 ± 0.120

COMP-NNLS
0.011 ± 0.008
0.029 ± 0.030
0.030 ± 0.031
0.059 ± 0.067
0.081 ± 0.082
0.086 ± 0.088
0.105 ± 0.122

S.XIII. O PTIMAL E XPECTED N UMBER
T ESTS

OF

D ORFMAN

A formula for the expected number of tests using Dorfman
Testing is presented in [2]. However, it is valid only for the
case when the number of samples is a multiple of the pool size.
We slightly modify their derivation to handle the case when
it is not so. Let there be n samples {1 . . . n}, with disease
prevalence rate p = k/n. That is, any given sample is positive
with probability p = k/n, independently of other samples.
Thus, out of n samples, the expected number of samples which
are infected is k. Let the samples be divided into ⌊n/g⌋ pools
of size g ≥ 2, and one pool of size r = n − ⌊n/g⌋ g. Since
p is the probability that a given sample is infected, hence
1 − p is the probability that a given sample is not infected.
Then p′ (l) = 1 − (1 − p)l is the probability of a pool of
size l being infected. Let Tg be the total number of first stage
and second stage tests taken by the Dorfman testing method,
given the pool size g. Let the pools thus formed be numbered
{1 . . . ⌊n/g⌋ + 1}, with a 0-sized last pool if r = 0 for the
purposes of our computation. Also, let tj = 1 if the j th pool
tested positive, 0 otherwise. The 0-sized pool is considered to
be tested as negative. Also note that if r = 1, the last pool
need not be retested even if it tested as positive. Let r̄ denote
the number of tests done for the last pool if it tested positive.
Hence r̄ = 0 if r ≤ 1, else r̄ = r. Then,
Tg =

n
⌊X
g⌋

j=1

tj g + r̄t⌊ n ⌋+1 +
g

 
n
.
g

By linearity of expectations, we get
 
 
n ′
n
′
p (g) + r̄p (r) +
.
E[Tg ] = g
g
g

(S6)

(S7)

The optimal expected number of tests is min E[Tg ], and the
g

As discussed in section IV-A4, we use a threshold of τ =
0.2 × xmin , below which entries of the estimated viral load
vector x̂ are set to 0. Here xmin = 1.0 for our synthetic data.
This is needed for the algorithms C OMP-N NLASSO, C OMPN NLAD and C OMP-N NLS. Table S.IX shows the variance in
performance of these algorithms for different choices of τ , for
the case when the number of infected samples k = 10. We see
that τ = 0 gives many false positives as compared to the other
choices. For other values of τ , only C OMP-N NLASSO shows
some variance. We chose τ = 0.2 to provide good tradeoff
between sensitivity and specificity.

optimal pool size is g ∗ = arg min E[Tg ]. Since there is no
g

closed form for this, we implemented this numerically in
practice - see Table VI from the main paper.
S.XIV. F RACTION

C OMP
K IRKMAN MATRICES
Proposition 6. Let A be an m × n full Kirkman matrix. Let
y = Ax be a measurement vector for some x ∈ Rn such that
kxk0 = k. Let f := kyk0 /m ∈ (0, 1) be the fraction of pools
that are tested positive. Then the fraction of samples declared
positive by C OMP is strictly less than f 2 .
OF SAMPLES REMAINING AFTER

USING

6

TABLE S.VIII
E FFECT OF PARAMETER q

ON THE REPORTED RELATIVE RELATIVE VIRAL LOADS OF REAL DATA , USING COMP-SBL. V IRAL LOADS REPORTED ARE
RELATIVE TO THE VIRAL LOAD CONTENT OF THE POOL WITH THE SMALLEST THRESHOLD CYCLE (Ct ) VALUE IN THAT PARTICULAR EXPERIMENT.
e WHICH HAVE VALUE MORE THAN 0 IN THAT EXPERIMENT, FOR ANY VALUE
S AMPLE ID S ARE THE INDICES OF THE ESTIMATED VIRAL LOAD VECTOR x
OF q. I N SOME CASES , THERE MAY BE MORE SAMPLE ID ENTRIES FOR AN EXPERIMENT THAN THE NUMBER OF GROUND TRUTH POSITIVE SAMPLES k IN
THAT EXPERIMENT, DUE TO FALSE POSITIVES . R EPORTED VIRAL LOAD OF REMAINING SAMPLES IS 0.

q→
Harvard 24 × 60, k = 2

Sample IDs

10
28
54

0.7
0.13
0.86
0.08

0.75
0.11
0.85
0.09

0.8
0.09
0.84
0.09

0.85
0.07
0.84
0.10

0.9
0.06
0.83
0.10

0.95
0.04
0.82
0.11

1
0.03
0.82
0.11

20
114

0.7
0.86
0.90

0.75
0.85
0.89

0.8
0.85
0.89

0.85
0.84
0.88

0.9
0.83
0.88

0.95
0.83
0.87

1
0.82
0.87

14

0.7
0.78

0.75
0.77

0.8
0.77

0.85
0.76

0.9
0.75

0.95
0.74

1
0.73

9
22

0.7
0.59
0.60

0.75
0.58
0.59

0.8
0.58
0.59

0.85
0.57
0.59

0.9
0.57
0.58

0.95
0.56
0.58

1
0.56
0.57

4
6
23

0.7
0.011
0.07
0.77

0.75
0.008
0.06
0.76

0.8
0.005
0.05
0.76

0.85
0.004
0.05
0.75

0.9
0.003
0.04
0.75

0.95
0.003
0.04
0.74

1
0.002
0.03
0.74

11
17
18
33
36

0.7
0.014
0.04
0.001
0.94
0.11

0.75
0.009
0.04
0.002
0.94
0.10

0.8
0.006
0.04
0.002
0.94
0.09

0.85
0.004
0.04
0.003
0.94
0.08

0.9
0.003
0.03
0.003
0.94
0.07

0.95
0.003
0.03
0.003
0.93
0.06

1
0.003
0.03
0.004
0.93
0.05

q→
Harvard 30 × 120, k = 2

NCBS-1 16 × 40, k = 1

Sample IDs
q→
Sample ID
q→

NCBS-2 16 × 40, k = 2

Sample IDs
q→

NCBS-3 16 × 40, k = 3

Sample IDs

q→
NCBS-4 16 × 40, k = 4

Sample IDs

Proof. We will prove the proposition for the case when A is a
Steiner Triple System matrix. Since every full Kirkman matrix
is also a Steiner Triple System matrix, the same proof holds.
Let Ȳ be the set of pools that tested positive and X̄ be
the set of samples declared positive by C OMP. Then, we have
|Ȳ| = f m. Recall from Sec. III-F3 of the main paper that since
A is a Steiner Triple System matrix each column consists of
3 entries with value 1. Notice that any column which does
not have all three of its 1 entries in Ȳ would have gotten
eliminated by C OMP. Hence the reduced matrix AX̄ ,Ȳ also
has the property that each column consists of 3 entries with
value 1.
Recall from Sec. III-F3 of the main paper that in a Steiner
Triple System matrix, each column corresponds to a triplet of
rows, and each (unordered) pair of rows occurs together in
exactly
 1 such triplet. Hence each column of A corresponds
to 32 = 3 unique pairs of rows of A. Consequently, in the
reduced matrix AX̄ ,Ȳ , each column corresponds to 3 unique
pairs of rows of AX̄ ,Ȳ , since for each column in AX̄ ,Ȳ , all
the rows with 1 entries are in Ȳ.
Hence, for AX̄ ,Ȳ , total number of such unique pairs of rows
as enumerated by its columns is 3|X̄ |. This number must be
less than or equal to the maximum
number of unique pairs of

rows of AX̄ ,Ȳ , which is |Ȳ|
=
f
m(f
m − 1)/2.
2
Therefore, we have
f m(f m − 1)
f m(f m − f )
m(m − 1)
<
= f2
.
6
6
6
(S8)
Recall from Sec. III-F3 of the main paper that the number
of columns n of a Steiner Triple System matrix is equal to
|X̄ | ≤

m
2


/3 = m(m − 1)/6. Hence,

|X̄ |
< f 2.
n

(S9)

This gives some intuition as to why C OMP eliminates so
many samples when using Kirkman matrices, as observed from
Table I of the main paper, especially in the regime k ≪ m.
Since each sample can only make 3 tests positive, number of
positive tests is at most 3k. Hence f ≤ 3k/m for Kirkman
matrices. For example, when using a full Kirkman matrix, if
the fractions of positive tests f is 0.1, fraction of samples that
remain after C OMP is at most f 2 = 0.01. Similarly, if f = 0.3,
then f 2 = 0.09, and so on.

7

TABLE S.IX
S ENSITIVITY OF CS

ALGORITHMS TO THE CHOICE OF THRESHOLD τ . R EPORTED NUMBERS ARE FOR 1000 SIGNALS WITH
K IRKMAN MATRIX .

τ
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1

RMSE
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527
0.1060 ± 0.0527

τ
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1

RMSE
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520
0.0981 ± 0.0520

τ
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1

RMSE
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538
0.0989 ± 0.0538

COMP-NNLAD
#FP
15.4640 ± 4.6299
9.3480 ± 3.1325
9.3450 ± 3.1308
9.3440 ± 3.1298
9.3420 ± 3.1309
9.3420 ± 3.1309
9.3400 ± 3.1315
9.3380 ± 3.1314
9.3360 ± 3.1319
9.3340 ± 3.1315
9.3310 ± 3.1317
COMP-NNLS
#FN
#FP
0.0000 ± 0.0000
15.4250 ± 4.4020
0.2540 ± 0.4896
7.9430 ± 3.2712
0.2540 ± 0.4896
7.9430 ± 3.2712
0.2540 ± 0.4896
7.9420 ± 3.2701
0.2540 ± 0.4896
7.9420 ± 3.2701
0.2540 ± 0.4896
7.9420 ± 3.2701
0.2540 ± 0.4896
7.9420 ± 3.2701
0.2540 ± 0.4896
7.9420 ± 3.2701
0.2540 ± 0.4896
7.9420 ± 3.2701
0.2540 ± 0.4896
7.9420 ± 3.2701
0.2540 ± 0.4896
7.9420 ± 3.2701
COMP-NNLASSO
#FN
#FP
0.0000 ± 0.0000
15.1620 ± 4.4452
0.2570 ± 0.4870
7.7850 ± 3.3660
0.2570 ± 0.4870
7.7760 ± 3.3540
0.2590 ± 0.4880
7.7730 ± 3.3530
0.2590 ± 0.4880
7.7670 ± 3.3520
0.2600 ± 0.4885
7.7660 ± 3.3527
0.2600 ± 0.4885
7.7640 ± 3.3516
0.2600 ± 0.4885
7.7640 ± 3.3516
0.2600 ± 0.4885
7.7620 ± 3.3497
0.2610 ± 0.4890
7.7620 ± 3.3497
0.2610 ± 0.4890
7.7610 ± 3.3513
#FN
0.0000 ± 0.0000
0.2030 ± 0.4242
0.2030 ± 0.4242
0.2030 ± 0.4242
0.2030 ± 0.4242
0.2030 ± 0.4242
0.2030 ± 0.4242
0.2030 ± 0.4242
0.2030 ± 0.4242
0.2030 ± 0.4242
0.2030 ± 0.4242

k = 10 ON THE 93 × 961

Sens.
1.0000 ± 0.0000
0.9797 ± 0.0424
0.9797 ± 0.0424
0.9797 ± 0.0424
0.9797 ± 0.0424
0.9797 ± 0.0424
0.9797 ± 0.0424
0.9797 ± 0.0424
0.9797 ± 0.0424
0.9797 ± 0.0424
0.9797 ± 0.0424

Spec.
0.9837 ± 0.0049
0.9902 ± 0.0033
0.9902 ± 0.0033
0.9902 ± 0.0033
0.9902 ± 0.0033
0.9902 ± 0.0033
0.9902 ± 0.0033
0.9902 ± 0.0033
0.9902 ± 0.0033
0.9902 ± 0.0033
0.9902 ± 0.0033

Sens.
1.0000 ± 0.0000
0.9746 ± 0.0490
0.9746 ± 0.0490
0.9746 ± 0.0490
0.9746 ± 0.0490
0.9746 ± 0.0490
0.9746 ± 0.0490
0.9746 ± 0.0490
0.9746 ± 0.0490
0.9746 ± 0.0490
0.9746 ± 0.0490

Spec.
0.9838 ± 0.0046
0.9916 ± 0.0034
0.9916 ± 0.0034
0.9916 ± 0.0034
0.9916 ± 0.0034
0.9916 ± 0.0034
0.9916 ± 0.0034
0.9916 ± 0.0034
0.9916 ± 0.0034
0.9916 ± 0.0034
0.9916 ± 0.0034

Sens.
1.0000 ± 0.0000
0.9743 ± 0.0487
0.9743 ± 0.0487
0.9741 ± 0.0488
0.9741 ± 0.0488
0.9740 ± 0.0489
0.9740 ± 0.0489
0.9740 ± 0.0489
0.9740 ± 0.0489
0.9739 ± 0.0489
0.9739 ± 0.0489

Spec.
0.9841 ± 0.0047
0.9918 ± 0.0035
0.9918 ± 0.0035
0.9918 ± 0.0035
0.9918 ± 0.0035
0.9918 ± 0.0035
0.9918 ± 0.0035
0.9918 ± 0.0035
0.9918 ± 0.0035
0.9918 ± 0.0035
0.9918 ± 0.0035

TABLE S.X
P ERFORMANCE OF C OMP AND D D ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
AND STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000

#FN
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0

#FP
1.0 ± 1.0
4.4 ± 2.2
8.0 ± 3.2
12.2 ± 4.1
19.9 ± 5.8
24.9 ± 6.6
32.0 ± 8.1

Sens.
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000
1.0000 ± 0.0000

Spec.
0.9899 ± 0.0099
0.9541 ± 0.0223
0.9163 ± 0.0338
0.8689 ± 0.0446
0.7791 ± 0.0647
0.7174 ± 0.0747
0.6233 ± 0.0955

VALUE , MEAN

#HCP
4.8
5.2
4.0
2.5
0.9
0.5
0.1

TABLE S.XI
P ERFORMANCE OF COMP FOLLOWED BY NNLASSO ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND
EACH k VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.047 ± 0.020
0.064 ± 0.024
0.079 ± 0.031
0.100 ± 0.041
0.143 ± 0.076
0.178 ± 0.097
0.256 ± 0.131

#FN
0.0 ± 0.1
0.0 ± 0.2
0.1 ± 0.3
0.3 ± 0.5
0.5 ± 0.7
0.9 ± 0.9
1.2 ± 1.1

#FP
0.5 ± 0.7
2.3 ± 1.6
3.9 ± 2.2
6.1 ± 2.9
9.4 ± 3.9
12.0 ± 5.3
17.1 ± 9.5

Sens.
0.9994 ± 0.0109
0.9941 ± 0.0265
0.9892 ± 0.0317
0.9783 ± 0.0409
0.9654 ± 0.0465
0.9493 ± 0.0521
0.9380 ± 0.0557

Spec.
0.9949 ± 0.0072
0.9761 ± 0.0165
0.9585 ± 0.0230
0.9340 ± 0.0313
0.8956 ± 0.0432
0.8639 ± 0.0605
0.7993 ± 0.1123

8

TABLE S.XII
P ERFORMANCE OF COMP FOLLOWED BY SBL ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.046 ± 0.019
0.058 ± 0.020
0.070 ± 0.023
0.085 ± 0.035
0.112 ± 0.041
0.142 ± 0.081
0.195 ± 0.145

#FN
0.0 ± 0.1
0.0 ± 0.1
0.1 ± 0.3
0.1 ± 0.3
0.2 ± 0.5
0.4 ± 0.6
0.7 ± 0.9

#FP
0.5 ± 0.7
2.4 ± 1.6
4.3 ± 2.3
6.7 ± 2.9
10.8 ± 4.0
13.7 ± 4.5
18.0 ± 5.8

Sens.
0.9994 ± 0.0109
0.9974 ± 0.0179
0.9930 ± 0.0255
0.9911 ± 0.0271
0.9846 ± 0.0319
0.9754 ± 0.0376
0.9628 ± 0.0443

Spec.
0.9945 ± 0.0075
0.9749 ± 0.0169
0.9550 ± 0.0241
0.9281 ± 0.0312
0.8799 ± 0.0445
0.8445 ± 0.0517
0.7885 ± 0.0685

TABLE S.XIII
P ERFORMANCE OF COMP FOLLOWED BY NNOMP ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.046 ± 0.020
0.060 ± 0.024
0.073 ± 0.029
0.090 ± 0.036
0.135 ± 0.069
0.175 ± 0.103
0.266 ± 0.169

#FN
0.0 ± 0.1
0.1 ± 0.3
0.2 ± 0.5
0.3 ± 0.6
0.6 ± 0.8
1.0 ± 1.0
2.0 ± 1.6

#FP
0.2 ± 0.5
1.0 ± 1.3
2.2 ± 2.1
3.9 ± 2.8
7.7 ± 3.7
10.2 ± 3.8
12.7 ± 3.8

Sens.
0.9984 ± 0.0178
0.9882 ± 0.0377
0.9774 ± 0.0478
0.9713 ± 0.0503
0.9573 ± 0.0528
0.9406 ± 0.0590
0.8998 ± 0.0789

Spec.
0.9980 ± 0.0047
0.9895 ± 0.0134
0.9769 ± 0.0217
0.9582 ± 0.0300
0.9139 ± 0.0414
0.8842 ± 0.0426
0.8502 ± 0.0448

TABLE S.XIV
P ERFORMANCE OF COMP FOLLOWED BY NNLAD ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.050 ± 0.022
0.068 ± 0.028
0.087 ± 0.037
0.105 ± 0.041
0.150 ± 0.070
0.195 ± 0.100
0.261 ± 0.126

#FN
0.0 ± 0.0
0.0 ± 0.2
0.1 ± 0.3
0.2 ± 0.4
0.5 ± 0.7
0.8 ± 0.8
1.1 ± 1.1

#FP
0.6 ± 0.8
2.7 ± 1.7
5.0 ± 2.3
7.3 ± 3.0
10.5 ± 3.9
13.0 ± 5.7
18.1 ± 10.2

Sens.
0.9998 ± 0.0063
0.9968 ± 0.0199
0.9903 ± 0.0309
0.9841 ± 0.0356
0.9693 ± 0.0441
0.9554 ± 0.0491
0.9438 ± 0.0548

Spec.
0.9936 ± 0.0079
0.9721 ± 0.0174
0.9479 ± 0.0246
0.9215 ± 0.0318
0.8837 ± 0.0431
0.8527 ± 0.0644
0.7870 ± 0.1203

TABLE S.XV
P ERFORMANCE OF COMP FOLLOWED BY NNLS ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION AND EACH k
VALUE , MEAN AND STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.046 ± 0.019
0.063 ± 0.023
0.079 ± 0.033
0.097 ± 0.036
0.138 ± 0.068
0.183 ± 0.099
0.251 ± 0.129

#FN
0.0 ± 0.0
0.0 ± 0.2
0.1 ± 0.4
0.2 ± 0.5
0.5 ± 0.7
0.8 ± 0.8
1.2 ± 1.1

#FP
0.5 ± 0.7
2.2 ± 1.5
4.1 ± 2.2
6.2 ± 2.9
9.3 ± 4.0
12.1 ± 5.9
17.6 ± 10.5

Sens.
0.9998 ± 0.0063
0.9944 ± 0.0265
0.9863 ± 0.0358
0.9810 ± 0.0382
0.9660 ± 0.0469
0.9525 ± 0.0500
0.9425 ± 0.0553

Spec.
0.9947 ± 0.0071
0.9772 ± 0.0159
0.9565 ± 0.0234
0.9337 ± 0.0313
0.8966 ± 0.0444
0.8629 ± 0.0667
0.7935 ± 0.1231

TABLE S.XVI
P ERFORMANCE OF NNLASSO WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION ARE REPORTED OVER 100 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.254 ± 0.368
0.326 ± 0.360
0.418 ± 0.346
0.558 ± 0.295
0.656 ± 0.196
0.723 ± 0.161
0.787 ± 0.099

#FN
0.1 ± 0.3
0.2 ± 0.5
0.5 ± 0.7
0.9 ± 0.9
1.9 ± 1.4
2.3 ± 1.4
2.8 ± 1.8

#FP
78.2 ± 28.2
82.2 ± 33.3
85.4 ± 36.3
83.5 ± 30.9
95.1 ± 34.2
113.2 ± 43.2
141.4 ± 43.2

Sens.
0.9780 ± 0.0690
0.9738 ± 0.0672
0.9470 ± 0.0717
0.9283 ± 0.0759
0.8713 ± 0.0915
0.8635 ± 0.0823
0.8580 ± 0.0887

Spec.
0.9182 ± 0.0295
0.9137 ± 0.0349
0.9102 ± 0.0382
0.9120 ± 0.0326
0.8995 ± 0.0362
0.8801 ± 0.0457
0.8497 ± 0.0459

9

TABLE S.XVII
P ERFORMANCE OF SBL WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION ARE REPORTED OVER 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.105 ± 0.213
0.113 ± 0.211
0.065 ± 0.024
0.070 ± 0.024
0.108 ± 0.139
0.179 ± 0.236
0.317 ± 0.351

#FN
0.0 ± 0.0
0.1 ± 0.2
0.0 ± 0.1
0.1 ± 0.4
0.1 ± 0.4
0.6 ± 1.2
1.4 ± 2.2

#FP
388.4 ± 63.2
420.2 ± 55.5
443.9 ± 42.9
449.9 ± 38.1
450.0 ± 29.1
456.5 ± 30.0
467.4 ± 30.5

Sens.
1.0000 ± 0.0000
0.9925 ± 0.0297
0.9990 ± 0.0099
0.9942 ± 0.0295
0.9913 ± 0.0277
0.9659 ± 0.0702
0.9290 ± 0.1098

Spec.
0.5937 ± 0.0661
0.5591 ± 0.0582
0.5332 ± 0.0452
0.5259 ± 0.0401
0.5243 ± 0.0308
0.5165 ± 0.0318
0.5033 ± 0.0324

TABLE S.XVIII
P ERFORMANCE OF NNOMP WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION ARE REPORTED OVER 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.048 ± 0.022
0.062 ± 0.025
0.168 ± 0.280
0.242 ± 0.331
0.526 ± 0.416
0.734 ± 0.410
0.902 ± 0.306

#FN
0.0 ± 0.2
0.2 ± 0.4
0.8 ± 1.7
2.0 ± 3.3
6.4 ± 5.3
10.3 ± 6.2
15.4 ± 5.4

#FP
2.6 ± 8.5
9.5 ± 17.7
16.7 ± 22.1
19.4 ± 21.7
11.9 ± 16.9
9.8 ± 15.5
6.2 ± 10.1

Sens.
0.9940 ± 0.0341
0.9750 ± 0.0559
0.9180 ± 0.1740
0.8292 ± 0.2745
0.5747 ± 0.3565
0.3918 ± 0.3632
0.2290 ± 0.2697

Spec.
0.9973 ± 0.0089
0.9900 ± 0.0186
0.9825 ± 0.0232
0.9795 ± 0.0229
0.9874 ± 0.0178
0.9897 ± 0.0164
0.9934 ± 0.0107

TABLE S.XIX
P ERFORMANCE OF NNLAD WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION ARE REPORTED OVER 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.080 ± 0.028
0.103 ± 0.036
0.130 ± 0.050
0.178 ± 0.086
0.293 ± 0.150
0.404 ± 0.177
0.542 ± 0.163

#FN
0.0 ± 0.1
0.1 ± 0.3
0.2 ± 0.5
0.6 ± 0.7
0.9 ± 1.0
0.8 ± 1.2
0.3 ± 1.0

#FP
19.9 ± 6.3
43.8 ± 16.2
45.9 ± 23.6
38.3 ± 24.1
37.7 ± 18.3
53.9 ± 22.9
88.1 ± 24.8

Sens.
0.9972 ± 0.0235
0.9914 ± 0.0322
0.9750 ± 0.0479
0.9539 ± 0.0596
0.9390 ± 0.0672
0.9558 ± 0.0721
0.9855 ± 0.0489

Spec.
0.9791 ± 0.0066
0.9541 ± 0.0170
0.9517 ± 0.0249
0.9597 ± 0.0254
0.9602 ± 0.0193
0.9429 ± 0.0243
0.9063 ± 0.0263

TABLE S.XX
P ERFORMANCE OF NNLS WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 93 × 961 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION ARE REPORTED OVER 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.072 ± 0.024
0.091 ± 0.031
0.120 ± 0.052
0.164 ± 0.087
0.301 ± 0.160
0.412 ± 0.170
0.538 ± 0.155

#FN
0.1 ± 0.2
0.2 ± 0.5
0.4 ± 0.6
0.6 ± 0.8
1.0 ± 1.1
0.7 ± 1.2
0.3 ± 1.0

#FP
64.0 ± 5.6
66.7 ± 4.2
69.0 ± 4.1
70.1 ± 7.8
67.2 ± 12.0
69.3 ± 10.9
92.1 ± 18.0

Sens.
0.9880 ± 0.0492
0.9739 ± 0.0564
0.9634 ± 0.0564
0.9472 ± 0.0634
0.9359 ± 0.0706
0.9561 ± 0.0713
0.9855 ± 0.0499

Spec.
0.9330 ± 0.0058
0.9300 ± 0.0044
0.9275 ± 0.0043
0.9261 ± 0.0082
0.9289 ± 0.0127
0.9266 ± 0.0115
0.9021 ± 0.0192

TABLE S.XXI
P ERFORMANCE OF NNLASSO WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.066 ± 0.022
0.082 ± 0.024
0.096 ± 0.030
0.111 ± 0.040
0.150 ± 0.075
0.182 ± 0.089
0.258 ± 0.136

#FN
0.0 ± 0.2
0.1 ± 0.3
0.2 ± 0.4
0.4 ± 0.6
0.7 ± 0.8
0.9 ± 0.9
1.2 ± 1.1

#FP
19.6 ± 2.8
21.5 ± 2.7
22.2 ± 2.7
22.6 ± 2.7
23.3 ± 5.1
24.3 ± 7.4
29.7 ± 16.9

Sens.
0.9914 ± 0.0416
0.9840 ± 0.0433
0.9785 ± 0.0442
0.9699 ± 0.0488
0.9561 ± 0.0526
0.9467 ± 0.0537
0.9379 ± 0.0567

Spec.
0.8037 ± 0.0283
0.7788 ± 0.0277
0.7666 ± 0.0283
0.7573 ± 0.0289
0.7414 ± 0.0562
0.7239 ± 0.0845
0.6504 ± 0.1983

10

TABLE S.XXII
P ERFORMANCE OF SBL WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.078 ± 0.026
0.088 ± 0.029
0.095 ± 0.029
0.105 ± 0.034
0.122 ± 0.051
0.138 ± 0.081
0.202 ± 0.154

#FN
0.0 ± 0.0
0.0 ± 0.2
0.1 ± 0.3
0.2 ± 0.4
0.3 ± 0.6
0.4 ± 0.7
0.8 ± 0.9

#FP
49.9 ± 3.6
48.9 ± 3.8
47.8 ± 4.0
47.2 ± 3.9
46.3 ± 4.1
45.6 ± 4.2
45.4 ± 4.6

Sens.
0.9996 ± 0.0089
0.9959 ± 0.0230
0.9898 ± 0.0316
0.9865 ± 0.0331
0.9789 ± 0.0377
0.9736 ± 0.0383
0.9595 ± 0.0470

Spec.
0.5012 ± 0.0364
0.4962 ± 0.0395
0.4968 ± 0.0418
0.4924 ± 0.0420
0.4855 ± 0.0457
0.4815 ± 0.0474
0.4663 ± 0.0542

TABLE S.XXIII
P ERFORMANCE OF NNOMP WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.050 ± 0.023
0.070 ± 0.027
0.085 ± 0.032
0.103 ± 0.036
0.141 ± 0.066
0.182 ± 0.108
0.278 ± 0.177

#FN
0.0 ± 0.1
0.1 ± 0.4
0.2 ± 0.5
0.4 ± 0.6
0.6 ± 0.8
1.1 ± 1.1
2.1 ± 1.7

#FP
1.6 ± 3.7
5.7 ± 6.3
9.9 ± 6.8
14.4 ± 6.5
18.8 ± 5.0
20.0 ± 3.7
20.1 ± 2.8

Sens.
0.9986 ± 0.0167
0.9819 ± 0.0515
0.9759 ± 0.0520
0.9667 ± 0.0509
0.9575 ± 0.0509
0.9368 ± 0.0639
0.8944 ± 0.0828

Spec.
0.9836 ± 0.0365
0.9408 ± 0.0646
0.8960 ± 0.0720
0.8451 ± 0.0700
0.7912 ± 0.0555
0.7724 ± 0.0421
0.7634 ± 0.0332

TABLE S.XXIV
P ERFORMANCE OF NNLAD WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.067 ± 0.026
0.089 ± 0.031
0.106 ± 0.040
0.128 ± 0.047
0.166 ± 0.076
0.201 ± 0.094
0.272 ± 0.125

#FN
0.0 ± 0.1
0.1 ± 0.2
0.1 ± 0.4
0.3 ± 0.5
0.6 ± 0.7
0.9 ± 0.9
1.2 ± 1.2

#FP
5.6 ± 2.4
11.4 ± 4.1
14.5 ± 5.0
15.8 ± 5.3
16.7 ± 5.0
17.1 ± 5.5
21.3 ± 9.9

Sens.
0.9986 ± 0.0167
0.9926 ± 0.0305
0.9865 ± 0.0362
0.9778 ± 0.0404
0.9615 ± 0.0478
0.9494 ± 0.0513
0.9404 ± 0.0578

Spec.
0.9441 ± 0.0242
0.8822 ± 0.0419
0.8475 ± 0.0529
0.8301 ± 0.0565
0.8141 ± 0.0560
0.8052 ± 0.0629
0.7500 ± 0.1164

TABLE S.XXV
P ERFORMANCE OF NNLS WITHOUT COMP ( ON SYNTHETIC DATA ) FOR 45 × 105 K IRKMAN TRIPLE MATRIX . F OR EACH CRITERION , MEAN AND
STANDARD DEVIATION VALUES ARE REPORTED ACROSS 1000 SIGNALS .
k
5
8
10
12
15
17
20

RMSE
0.066 ± 0.021
0.082 ± 0.025
0.094 ± 0.029
0.110 ± 0.037
0.147 ± 0.068
0.183 ± 0.089
0.262 ± 0.131

#FN
0.1 ± 0.2
0.1 ± 0.4
0.3 ± 0.5
0.3 ± 0.5
0.6 ± 0.7
0.9 ± 0.9
1.3 ± 1.2

#FP
19.3 ± 2.8
21.4 ± 2.5
22.0 ± 2.7
22.4 ± 2.8
23.1 ± 3.0
23.6 ± 3.8
25.3 ± 7.2

Sens.
0.9882 ± 0.0471
0.9831 ± 0.0452
0.9745 ± 0.0492
0.9722 ± 0.0446
0.9571 ± 0.0493
0.9474 ± 0.0537
0.9368 ± 0.0599

Spec.
0.8067 ± 0.0285
0.7793 ± 0.0261
0.7684 ± 0.0284
0.7596 ± 0.0304
0.7435 ± 0.0332
0.7324 ± 0.0429
0.7026 ± 0.0843

