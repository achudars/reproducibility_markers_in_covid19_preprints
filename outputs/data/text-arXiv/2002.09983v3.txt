arXiv:2002.09983v3 [stat.ME] 19 Apr 2020

Joint spatio-temporal analysis of multiple response
types using the hierarchical generalized
transformation model with application to
coronavirus disease 2019 and social distancing
Jonathan R. Bradley1

Abstract
Social distancing can be described as an effort to maintain a physical distance between individuals
and has become a necessary public health measure to combat cornoavirus disease 2019 (COVID19). Social distancing is known to weaken incidences and deaths due to COVID-19, however, there
are detrimental economic and psychological effects. This motivates us to analyze incidences (and
deaths) of COVID-19 along with a measure of the health of the US economy (i.e., the adjusted closing price of the Dow Jones Industrial), and a measure of the public interest in COVID-19 through
Google Trends data. The model we implement is developed to be easily adapted to a data scientist’s
preferred method for continuous data, which is done to aid future analyses of this important dataset.
This dataset consists of multiple response types (e.g., continuous-valued, count-valued, binomial
counts). Thus, we introduce a reasonable easy-to-implement all-purpose method that “converts” a
statistical model for continuous responses (the preferred model) into a Bayesian model for multiresponse data sets. To do this, we transform the data such that the continuous-valued transformed
data can be reasonably modeled using the preferred model and the transformation itself is treated
as unknown. The implementation of our approach involves two steps. The first step produces
posterior replicates of the transformed data using a latent conjugate multivariate (LCM) model.
The second step involves generating values from the posterior distribution implied by the preferred model. We refer to our model as the hierarchical generalized transformation (HGT) model.
In a simulation, we demonstrate the flexibility of the HGT model by incorporating two different
preferred models: Bayesian additive regression trees (BART) and the spatial mixed effects (spatiotemporal mixed effects) models. We provide a thorough joint multiple-response spatio-temporal
analysis of COVID-19 cases, the adjust closing price of the Dow Jones Industrial, and Google
Trends data.

Keywords: Bayesian hierarchical model; Big data; Multiple Response Types; Markov chain
Monte Carlo; Non-Gaussian; Nonlinear; Gibbs sampler; Log-Linear Models.
1 (to

whom correspondence should be addressed) Department of Statistics, Florida State University, 117 N. Woodward Ave., Tallahassee, FL 32306-4330, jrbradley@fsu.edu

1 Introduction
COVID-19 was first detected in a live animal market in Wuhan City within the Hubei Province of
China. This virus spreads easily from person to person, and there are cases of this virus where an
individual is unsure of how they became infected (i.e., community spread). To date, there is no
vaccine to prevent COVID-19, which has become a pandemic. As such, many governmental organizations, including the Centers for Disease Control and Prevention (CDC), have advised placing
distance between yourself and other individuals (i.e., social distancing). Social distancing is an
important public health measure that reduces close contact with people that may be infected by
maintaining physical distance between all individuals (Wilder-Smith and Freedman, 2020; Zhang
et al., 2020). However, social distancing comes as a cost, and can be detrimental to economies
and cause psychological distress (Long, 2020). With the negative effects of COVID-19 and social
distancing in mind, we are interested in performing a joint spatio-temporal analysis of reported
deaths and cases of COVID-19, the daily adjusted closing price of the Dow Jones Industrial (DJI),
and a Google Trends data on searches of “coronavirus.”
The data on reported deaths and cases of COVID-19 were obtained from the Johns Hopkins
University Center for Systems Science and Engineering (JHU CCSE) Coronavirus repository (publicly available at https://github.com/CSSEGISandData/COVID-19), a subset of which, is made
available in the R package coronavirus (R. Krispin, 2020). Cases, recoveries and mortality
counts are available over regions (i.e., country or province) and discrete time (daily). In this article, we model these counts using a Poisson distribution, and our main interest lies in estimating
the mean number of reported deaths and cases of COVID-19, and estimating its dependence with
interest in COVID-19 and DJI data.
The number of Google searches of “coronavirus” is indicative of the high interest on COVID-19
and can act as a loose proxy for the public interest in COVID-19. This search information is made
available through Google Trends data (Google, 2020). Google Trends provide daily time series of

1

an “interest” measure of searches on Google. This interest measure is defined on a scale from zero
to one hundred with 100 indicating high interest and zero indicating low interest. In this article,
we model the Google Trends interest score for the search “coronavirus” as binomial with sample
size 100, since this response is a non-negative, integer-valued response that is bounded above by
100. We are interested in estimating the mean interest measure and estimating its dependence on
the reported deaths and cases of COVID-19 and DJI data.
The DJI follows 30 publicly owned blue chip (i.e., nationally recognized and financially secure)
companies that trade on the New York Stock Exchange (NYSE) and the National Association of
Securities Dealers Automated Quotations (NASDAQ). It is a benchmark for blue-chip stocks and is
often treated as a measure of the economic health of the US. This data was obtained through Yahoo
Finance (Yahoo, 2020). We model the adjusted daily closing price with a Gaussian distribution,
since it is continuous valued. Our main interest in DJI is in determining and summarizing the
relationship between the adjusted closing price with both interest in COVID-19 and reported cases
and deaths due to COVID-19.
A major difficulty in jointly analyzing these data is that the response types are different (i.e.,
Poisson, binomial, and Gaussian). There are several methods for jointly modeling data consisting
of multiple response types, however these approaches often require substantial methodological development, or creates clear computational difficulties. For example, Markov models Yang et al.
(2014), copulas (Liu et al., 2009; Xue and Zou, 2012; Dobra and Lenkoski, 2011; Liu et al., 2012),
multi-task learning models (Argyriou et al., 2007; Kim and Xing, 2009; Yang et al., 2009), regression trees, and random forests (Hastie et al., 2009; Fellinghauer et al., 2013) have been adapted
to this multiple response setting. However, these methods do not immediately incorporate a data
scientist’s preferred model. An important goal of this article is to allow our model to be flexible
enough that it can be adapted to other data scientist’s preferred model. There has been a call to
action for researchers to analyze COVID-19 (Office of Science and Technology Policy, 2020), and
because of this, it is desirable to have tool that makes it easy for data scientists to jointly analyze
2

Google Trends, DJI, and incidences of COVID-19 using their preferred model. While our proposed model allows for this flexibility, it can interpreted as a simple combination of two existing
methods: generalized linear mixed effects models (e.g., see McCulloch et al., 2008, for a standard
reference) and LCMs (Bradley et al., 2019a).
The GLMM is a standard approach to model non-Gaussian data. For example, Bernoulli data
is modeled hierarchically, where the logit of the probability of success can be analyzed using a
data scientist’s preferred model. GLMMs lack conjugacy, which creates noticeable difficulty when
implementing a GLMM on a modern high-dimensional data set. A more recent alternative is the
LCM. Basic theoretical results and empirical analyses in Bradley et al. (2018), Hu and Bradley
(2018), H.-C.Yang et al. (2019), Bradley et al. (2019c), and Bradley et al. (2019a) suggest that
one can outperform a standard GLMM (specifically Latent Gaussian Process (LGP) models) in
terms of prediction error. However, both the GLMM and LCM requires the preferred model to be
a mixed effects model, and the LCM requires one to modify the distribution of random effects to
follow the appropriate distribution based on conjugacy.
A classical approach is to transform the data, so that the transformed data can be reasonably
modeled using the distribution assumed by the preferred model. In the non-Bayesian settings this
literature is extremely well-developed and includes the Box-Cox transformations (Box and Cox,
1964), the alternating conditional expectations (ACE; Breiman and Friedman, 1985) algorithm,
graphical techniques (McCulloch, 1993), and the Yeo-Johnson power transformation (Yeo and
Johnson, 2000), among other techniques. More recently developments in rank based algorithms
(Servin and Stephens, 2007; McCaw et al., 2019; Beasley et al., 2009) and quantile-matching (McCullagh and Tresoldi, 2020) have also been proposed in the non-Bayesian setting. It is important
to note that Bayesian models for transformations have been proposed as well, but focus on the case
where continuous non-normal data are observed and the preferred model assumes normality. In
particular, these Bayesian models put a prior on the free parameter within the Box-Cox transformation or the Yeo-Johnson power transformation (Kim et al., 2013; Charitidou et al., 2015, 2018).
3

No such Bayesian model has been developed to analyze multi-response response data using any
preferred model for a continuous response.
There are three distributions that define our hierarchical generalized transformation (HGT)
model: (a) the distribution of the data given a transformation, (b) the prior distribution of the
transformation, and (c) the distribution of the process of interest (i.e., the aforementioned preferred model). In this article, we model the data given a transformation (a) using members from
the exponential family. Specifically, given a transformation, continuous data follows the normal
distribution, categorical data follows the binomial distribution, and count-data follow the Poisson
distribution. These distributions are conjugate with the normal, the logit-beta (Gao and Bradley,
2019; Bradley et al., 2019c) and the log-gamma distributions (Bradley et al., 2018; Hu and Bradley,
2018; Bradley et al., 2019a; H.-C.Yang et al., 2019), which are special cases of the DiaconisYlvishaker (DY) distribution (e.g., see Diaconis and Ylvisaker, 1979; Chen and Ibrahim, 2003, for
key references). Consequently, the prior distribution of the transformation (b) is modeled with a
DY distribution, which defines an LCM model for the transformations.
While we are motivated by COVID-19 and the detrimental impacts of social distances, the
methodology developed in this manuscript is of independent interest, since this is a new way
in Bayesian statistics to model non-Gaussian processes using models for continuous data. Furthermore, our methodology also allows one to analyze a single non-Gaussian response type in a
straightforward manner. That is, the implementation of our approach can be done using composite
sampling. In particular, the first step is to sample from the posterior distribution of the transformation. Then the second step is to sample from the conditional distribution of the latent process of
interest given the transformation. This conditional distribution is derived from the preferred model.
The first step of the composite sampler is computationally straightforward because the DY
distribution is conjugate (and easy to sample from) with the exponential family. Additionally,
the first step of this algorithm is important for the purpose of analyzing multiple response types.
Specifically, at the end of the first step we obtain a replicate from the posterior distribution of
4

the transformation (which is continuous valued). Thus, the first step of the composite sampling
algorithm “transforms” the multi-response data into a continuous-valued quantity appropriate for
the preferred model.
Implementation of the preferred model is unchanged in the second step of our composite sampling algorithm. This is particularly noteworthy, as many of the Bayesian statistical models derived
for Gaussian data are not immediately computationally efficient in the non-Gaussian data setting
(e.g., see Bradley et al., 2019b; Kang and Cressie, 2011; Katzfuss and Cressie, 2012, for examples
in the spatial setting). This is because GLMMs in the non-Gaussian setting have full-conditional
distributions that are not Gaussian, and can not be sampled from immediately. Bayesian methods that do not have easy to sample from full-conditional distributions require difficult to tune
Metropolis-Hastings algorithms (e.g., see Bradley et al., 2019a, for an example), inefficient rejection samplers (e.g., see Damien et al., 1999), or significant reparameterization to make approximate
Bayesian methods (that are only appropriate for small parameter spaces) practical (Rue et al., 2009;
Neal, 2011). The second step of our composite sampling algorithm allows one to circumvent this
issue entirely, and simply use the computational strategies that were developed for the preferred
model.
The two steps of our composite sampler can be seen as sequential smoothing. By “smoothing”
we mean a function of the data that attempts to discover important features in the data (e.g., see
Simonoff, 2012, for a standard reference). Multiple layers of smoothing may lead to estimates
that are “oversmooth,” in the sense that many features of the data are not captured. To avoid
oversmoothing we specify the model so that the posterior distribution of the transformation is
“saturated.” Recall a saturated model is one in which there exists at least as many parameters as
there are data points, and fitting this model allows you to exactly recover the original data set.
Hence, saturated models are often an extreme example of overfitting. Thus, in the first step of
our composite sampler we choose to overfit the data, and in the second step we smooth overfitted
values (again this is done to avoid oversmoothing).
5

In the classical log-linear model literature, saturated models are useful for selecting more parsimonious models (e.g., see Agresti, 2007, for a standard reference). Specifically, the most parsimonious reduced model that is not significantly different (in terms of the deviance or chi-square
statistic) from the saturated model is used for statistical inference. Consequently, specifying the
transformation model to be saturated allows us to assess the goodness-of-fit of the preferred model
in a fully Bayesian manner that is similar to what is done in classical residual analysis.
It has recently been shown that forecasts regarding COVID-19 requires sophisticated models.
Following the results of Donnat and Holmes (2020), we include spatio-temporal random effects
through the use of basis function expansions (e.g., see Cressie and Wikle, 2011, for a standard reference). Additionally, to improve the performance of forecasting we adopt the training, validation,
and testing data framework that has become standard among the machine learning literature (e.g.,
see Hastie et al., 2009, for a standard reference).
The remainder of this article is organized as follows. In Section 2, we introduce our motivating
dataset and describe how standard modeling procedures are not appropriate for this dataset. Then,
we introduce the HGT model to analyze multi-response data with unknown transformations in
Section 3. Additionally, we provide a specific class of transformation models and an example
model specification. Then in Section 4, we provide details on using training, validation, and testing
data for statistical inference. A summary of all the Bayesian models used in our analysis is also
provided. In Section 5, we give simulation studies to illustrate that our approach has been been
developed in a manner that one can incorporate their preferred statistical model. In particular, we
apply our approach to BART models and a spatio-temporal mixed effects (SME) model. Section 6
contains our joint analysis of COVID-19 mortality, incidences and recoveries, along with Google
Trends data, and DJI data. Section 7 contains a discussion and derivations are provided in the
appendices.

6

Reported Deaths due to COVID−19

250
50

150

Frequency

100
50

Frequency

500
300
0

20

40

60

80

0

0

100

0

20

Time

40

60

0

20

40

60

Time

Google Trends Interest Score in COVID−19

60
0

20

40

24000

Interest Score

80

28000

100

Adjusted Closing Price of DJI

Adjusted Closing Price

80

Time

20000

Frequency

Recovered from COVID−19

150

Reported Cases of COVID−19

0

10

20

30

40

50

60

70

0

Time

10

20

30

40

50

60

70

Time

Figure 1: We plot the number of reported COVID-19 infections (top left), reported COVID-19
deaths (top middle), the reported recoveries from COVID-19 (top right), the DJI adjusted closing
price (bottom left), and the Google Trends interest score for searches of “coronavirus” (bottom
right). Note that the DJI price data is not available on Saturday and Sundays. The black circles are
the observed data, and blue lines connecting these points are added as a reference. The top row
represents only a summary of available data, since we also observe these counts over 184 countries
and 82 provinces.

7

80

2 Motivating Dataset
Denote the data with Zi j , where i indexes replicates and j indexes response type such that i =
1, . . ., I j and j = 1, 2, 3. We consider the setting where for each i, Zi1 is continuous-valued, Zi2 is
integer-valued ranging from 0, . . . , bi , and Zi3 is count valued. Specifically, Zi1 represents a measure
of the adjusted closing price of DJI, Zi2 is the integer-valued interest score for COVID-19 searches
as computed by Google Trends (with bi ≡ 100), and i indexes the days ranging from January 22,
2020 to April 8, 2020. The data Zi3 represents the i-th replicate of the number of COVID-19 cases,
where for each i there is an associated region (e.g., China) Ai ⊂∈ [−180, 180] × [−90, 90], day
ti (between January 22, 2020 to April 8, 2020), and an indicator di of whether or not the count
consists of reported deaths. Let di = 1 if Zi3 represents reported deaths and di = 0 otherwise.
Likewise, let ui represent an indicator of whether or not the count consists of reported recoveries.
Also let ti = 1, . . . , T = 78 represent each day between January 22, 2020 to April 8, 2020. In
Figure 1, we plot the number of reported COVID-19 infections, reported COVID-19 deaths, the
DJI adjusted closing price, and the Google Trends interest score for searches of “coronavirus.”
There are many “off-the-shelf” approaches that one might consider to analyze this data. For
example, one might define the following linear model,
T

T

j=1

j=1

Y1 = x′i1 β 1 + βY 2 ∑ Y j2 I(ti = j) + βY 3 ∑ Y j3 I(ti = j) + ξi1 ; i = 1, . . ., I1 ,
where ξi1 is normally distributed with mean zero and variance σξ2 , I(·) is an indicator function,

βY k ∈ R, β 1 is an unknown p-dimensional vector, and xi1 is a p-dimensional covariate vector.
However, this conditionally specified model enforces a strong assumption of linearity between the
different response types. Furthermore, the variability (and dependence) of Yi2 and Yi3 is ignored.
To incorporate the variability across response types (i.e., across j) and allow for non-linear

8

relationships, one might also consider the following hierarchical model:

Zi1 ∼ Normal(Yi1 , v)


exp (Yi2 )
Zi2 ∼ Binomial bi ,
1 + exp (Yi2 )


Zi3 ∼ Poisson exp Yi j ; i = 1, . . . , I j , j = 1, 2, 3,

(1)

where Yi j is an unobserved latent process, Normal(Yi1 , v) is a shorthand for the normal distribution
with mean Yi j ∈ R and variance v > 0, Binomial(bi , p) is a shorthand for the binomial distribution
with bi > 1 number of trials and probability of success p ∈ (0, 1), and Poisson(µi j ) is a shorthand
for the Poisson distribution with mean µi j . The covariance between observations is determined by
the model for Yi j :


cov(Zi j , Zmk ) = E cov(Zi j , Zmk )|Yi j ,Ymk + cov E(Zi j |Yi j ), E(Zmk |Ymk ) ,
n
o

−1
= cov E(Zi j |Yi j ), E(Zmk |Ymk ) = cov ci j g−1
(Y
),
c
g
(Y
)
,
ij
mk j
mk
j

(2)

for i 6= m and j 6= k, where the functions g1 (xi ) = xi , g2 (xi ) = log(xi /1 − xi ), and g3 (xi ) = log(xi )
are referred to as “link functions,” and ci1 = ci3 = 1 and ci2 = bi . Similarly, predicted values are
determined by the model for Yi j :
n
o

E(Zi j ) = E E(Zi j |Yi j ) = E ci j g−1
(Y
)
.
i
j
j

(3)

Thus, cross-dependence and predictions are modeled through the statistical model assumed for the
process Yi j , and a standard choice in this context is the GLMM:
Yi j = x′i j β j + S ′i j η + ξi j ,

(4)

where xi j is a known p-dimensional vector of covariates and Si j is a pre-specified r-dimensional

9

ind

ind

vector of basis functions, β j = (β1 j , . . ., β p j )′ , η = (η1 , . . . , ηr )′ , βk j ∼ Normal(0, σβ2 ), ηk ∼
ind

Normal(0, ση2 ), ξi j ∼ Normal(0, σξ2 ), σβ2 > 0, ση2 > 0, and σξ2 > 0. Then, the cross-response

spatio-temporal covariance implied by this model is cov Yi j ,Ymk |ση2 = ση2 S′i j Smk , which propa-

gates through and enforces dependence in the data through Equation (2). The relationship between

the different response types can be found by estimating the unknown function S′i j η (e.g., using
posterior means and credible intervals).
Computationally, the GLMM is difficult to implement in a Bayesian context. For example, a
Gibbs sampler requires one to simulate from the following full-conditional distributions (Gelfand,
2000), and in this setting these distributions do not have a known form that is straightforward to
simulate from. There are several approximate Bayesian computational tools available, however,
for moderate sizes of p and r these approaches are not feasible. In particular, Hamiltonian Monte
Carlo (HMC; Neal et al., 2011) and the integrated nested Laplace approximation (INLA; Rue
et al., 2009) are only appropriate for small parameter spaces (e.g, Martino and Riebler, 2019,
suggests no more than 15 parameters when implementing INLA). Additionally, INLA only allows
for marginal inference (Kristensen et al., 2015). The computational issues of the hierarchical model
in (1) and ( 4) may become even more cumbersome when considering a different model for Yi j .
This is especially pertinent for our dataset, since the US government has put out a call to action
(Office of Science and Technology Policy, 2020) for data scientists to analyze COVID-19 datasets,
and it would be preferable to have approach that is flexible enough for others to specify their own
model for Yi j without major changes to implementation.

10

3 The Hierarchical Generalized Transformation Model
3.1 Unknown Transformations of Multiple Response Types
One classical strategy to model non-Gaussian data is to impose a transformation such that,

h j (Zi j )|Yi j , θ ∼ Dist Yi j , θ ,

i = 1, . . ., I j , j = 1, 2, 3,

(5)

where h(·) is a transformation of the datum Zi j , “Dist” is a short-hand used for a probability

density function (pdf), g j E(Zi j ) = Yi j ∈ R and θ ∈ Θ ⊂ R p . Additionally, Yi j is defined for

i = 1, . . ., I and j = 1, 2, 3, where I ≥ max(I1 , I2 , I3 ). Here, “Dist Yi j , θ ” represents the aforementioned preferred model. In what remains, inference on {Yi j } and θ is the primary goal. To aid in

our exposition we drop the functional notation for h(·) and write hi j = h j (Zi j ). As an example of
“Dist,” suppose we assume
hi j = Yi j + εi j ,

(6)

ind

where εi j ∼ Normal(0, σε2) and σε2 > 0, and the model on Yi j in (4) is assumed.
Transformations convert a multiple response type data set (e.g., {Zi j }) to a single response
type data set (e.g., {hi j }), since hi j follows a single distribution with a continuous support. Consequently, transformations have become a standard tool in analyzing multiple response types. Recall, transformations such as these have a long history including the box-cox transformations (Box
and Cox, 1964), graphical techniques (McCulloch, 1993), the alternating conditional expectations
(ACE; Breiman and Friedman, 1985) algorithm, and the Yeo-Johnson power transformation (Yeo
and Johnson, 2000, among others).
In this paper, we introduce a Bayesian solution to the problem of an unknown transformation.
In particular, we define pdf and probability mass functions (pmf), f (Zi j |hi j ). We refer to these distributions as “transformation models.” In Section 3.2, we describe Bayesian implementation using
a general transformation model and any well defined preferred model. Then, in Section 3.3 the
11

specification of the transformation model is given. Finally we provide an example in Section 3.4.

3.2 General Bayesian Implementation
In this section, we describe Bayesian implementation of the model introduced in Section 3.1.
Here, let n = ∑3j=1 I j , the n-dimensional data vector ztrn = (Z11 , . . . , ZI3 3 )′ , the n-dimensional
transformed data vector h = (h11 , . . . , hI3 3 )′ , N = 3I ≥ n, and the N-dimensional latent process
y = (Y11 , . . .,YI1 ,Y12 , . . . ,YI2 ,Y13 , . . . ,YI3 )′ . Notice, that I j ≤ I, which allows for missing values of
Yi j .
From (5), the preferred model “Dist” is represented in terms of a hierarchical model:
f (hi j |Yi j , θ )m(h|γ );

i = 1, . . ., I j , j = 1, 2, 3,

f (y|θ )
f (θ ),

(7)

where m(·) is a real-valued function of h, which we will define below. Following the terminology used in Cressie and Wikle (2011), we call f (hi j |Yi j , θ )m(h|γ ) the “transformed data model,”
f (Yi j |θ ) the “process model,” and f (θ ) the “parameter model” (or prior). Bayes rule can be used
to produce the following conditional distribution (e.g., see Gelman et al., 2013, for a standard
reference),
f (y, θ |h) = R R

f (h|y, θ ) f (y|θ ) f (θ )
,
f (h|y, θ ) f (y|θ ) f (θ )dyd θ

(8)

where we have assumed hi j is conditionally independent of hkm given Yi j and θ for k 6= i and m 6= j
so that f (h|y, θ ) = ∏i ∏ j f (hi j |Yi j , θ ). Similarly, one can use Bayes rule to produce the posterior
distribution of the transformed data. That is,
R

f (h|ztrn ) = R R

f (ztrn |h) f (h|γ ) f (γ ) d γ
,
f (ztrn |h) f (h|γ ) f (γ ) dh d γ

12

(9)

where the distribution f (h|γ ) is referred to as a “transformation prior,” the q-dimensional realvalued vector γ is referred to as a transformation hyperparameter, and the distribution f (γ )
is referred to as a “transformation hyperprior.”

To guarantee that our choice of the trans-

formation prior and transformed data model are consistent with each other we set m(h|γ ) =
f (h|γ )/

RR

f (h|y, θ ) f (y|θ ) f (θ )dyd θ .

Equations (8) and (9) can be used to produce a posterior distribution for y and θ . That is, suppose f (h|y, θ ), f (y|θ ), f (θ ), f (ztrn |h), f (h|γ ), and f (γ ) are proper. Suppose ztrn is conditionally
independent of γ given h, and ztrn and (y′ , θ ′ )′ are conditionally independent given h. Then:
f (y, θ |z) =

Z

f (y, θ |h) f (h|z)dh.

(10)

The derivation of (10) can be found in Appendix A.
The model in (10) can easily be simulated from using a composite sampling scheme, provided
that it is easy to simulate from f (y, θ |h). Algorithm 1 gives the step-by-step implementation of
how to simulate from the posterior distribution in (10). Here, we see that the implementation of the
HGT model is similar to the bootstrap implementation, where we have replaced a resampling step
with sampling from f (h|z) and the full-conditional distributions associated with γ . This similarity
emphasizes the flexibility of allowing for unknown transformations in a Bayesian context, since
the bootstrap algorithm is an established flexible approach in the literature (e.g., see Efron, 1992,
for an early reference). Of course, the bootstrap algorithm produces replicates from a different
distribution than that of Algorithm 1. Specifically, the bootstrap method results in an approximate
sample from the sampling distribution of a statistic. Whereas, the composite sampling approach in
Algorithm 1 can be seen as a means to sample from (10). This is also different from the Bayesian
bootstrap (Rubin, 1981), which does not restrict the samples to be from a posterior distribution of
the form in (10).

13

Algorithm 1: Implementation of the HGT Model.
Set b = 1 and initialize h, γ , y, and θ with h[0] , γ [0] , y[0] , and θ [0] .
Sample h[b] from f (h|z, γ [b−1] ).
Sample γ [b] from their full-conditional distributions. We use the slice sampler (Neal et al.,
2003) if the full-conditional does not have a closed form.
[b]
from f (y, θ |h[b] ), which is the posterior distribution associated with the
4: Sample y[b] and θ
preferred model described in (8).
5: Set b = b + 1.
6: Repeat Steps 2−5 until b = B for a prespecified value of B.
1:
2:
3:

3.3 Modeling the Data Given Transformations
Consider the following specifications of the data models:

Zi1 |hi1 ∼ Normal(hi1 , v)


exp (hi2 )
Zi2 |hi2 ∼ Binomial bi ,
1 + exp (hi2 )


Zi3 |hi3 ∼ Poisson exp hi j ; i = 1, . . . , I j , j = 1, 2, 3,

(11)

which is different from the GLMM in (1). Specifically, instead of conditioning on the latent process
of interest Yi j , we condition on the transformation hi j .
With the transformation model f (ztrn |h) defined, we are left to specify a transformation prior
and transformation hyperprior. We define the transformation prior to be the conjugate distributions
associated with (11). It follows from Diaconis and Ylvisaker (1979) that the conjugate distribution
for hi j is given by,

fDY (hi j |α j , κ j , a, b) = K(α j , κ j )exp α j hi j − κ j ψ j (hi j ) ; i = 1, . . ., I j , j = 1, . . . , J,

(12)

where K(α j , κ j ) is a normalizing constant, hi j ∈ R, α1 ∈ R, κ2 > α2 , αm > 0, and κk > 0; for
m = 2, 3, and k = 1, 3. Let ψ1 (Z) = Z 2 , ψ2 (Z) = log(1+eZ ), and ψ3 (Z) = exp(Z). Also, we use the
shorthand DY(α j , κ j ; ψ j ) to represent the pdf in (12). Finally, let γ = (α1 , α2 , α3 , κ1 , κ2 , κ3 , v)′ be

14

the transformation hyperparameter. The DY distribution is a special case of the recently introduced
conjugate multivariate distribution (Bradley et al., 2019a), where the matrix-valued covariance
parameter is set equal to the identity matrix.
Equations (11) and (12) can be used to produce a full-conditional distribution for the elements
of h:
(
 
 
 )
1 −1 Zi1
1 −1
hi1 |Zi1 , γ ∼ Normal
2κ 1 +
; i = 1, . . ., I1
+ α1 , 2 κ 1 +
v
v
v
hi2 |Zi2 , γ ∼ DY (α2 + Zi2 , κ2 + bi ; ψ2 ) ; i = 1, . . ., I2
hi3 |Zi3 , γ ∼ DY (α3 + Zi3 , κ3 + 1; ψ3 ) ; i = 1, . . . , I3.

(13)

The derivations of the full conditional distributions are fairly straightforward, and are given in
Appendix A. One can simulate directly from the posterior distribution in (13). Replicates of hi j
from (13) can be computed using the following transformation (Bradley et al., 2019a):

 

1 −1 Zi1
+ α1 + w1 ; i = 1, . . . , I1
hi1 = 2κ1 +
v
v


w2
d
hi2 = log
; i = 1, . . ., I2
1 − w2
d

d

hi3 = log (w3 ) ; i = 1, . . . , I3 ,

(14)

d

where “=” stands for equal in distribution, w1 |Zi1 , α1 , κ1 , v is distributed normally with mean zero
−1
and variance 2κ1 + 1v
, w2 |Zi2 , α2 , κ2 is distributed according to a beta distribution with shape
parameters (α2 + Zi2 ) and (κ2 − α2 + bi − Zi2 ), and w3 |Zi3 , α3 , κ3 is distributed according to a

gamma distribution with shape parameter (α3 + Zi3 ) and rate parameter (κ3 + 1). Step 2 of Algorithm 1 involves simulating according to (14), which is straightforward.
The specification of a transformation hyperprior for γ is crucial to guarantee that f (hi j |Zi j , γ ) is
proper in the event that Zi3 = 0, Zi2 = 0, or Zi3 = bi . Thus, we assume α1 = κ1 = 0, α2 and α3 are

15

distributed according to a gamma distribution, κ2 |α2 is distributed according to a shifted (by α2 )
gamma distribution, κ3 follows a gamma distribution, and v is distributed according to an inverse
gamma distribution (e.g., see Gelman, 2006, among others). These transformation hyperpriors are
explicitly stated, and the full-conditional distributions for γ are derived in Appendix B.1.
Section 3.2 is flexible enough to allow for a transformation prior that implies cross-dependence
among the elements of h, but we do not consider this case in this article. The main reason for
this choice is that transformations are used in place of the original data set when implementing the
preferred model (Step 4 of Algorithm 1). That is, the transformed values are used as a proxy for (or
in place of) the data in the preferred model. Consequently, we would like to specify h to “overfit”
the data so that h can reasonably be thought of as a proxy for the data.
Our choice of the prior in (12) leads to posterior replicates that overfit the data. In particular, it
is straightforward to verify that
lim lim E {hi1 |Zi1 , γ } = Zi1

κ1 →0 α1 →0


lim lim E b j g−1
2 (h j2 )|Z j2 , γ = Z j2

κ2 →0 α2 →0

(15)


lim lim E g−1
3 (hk3 )|Zk3 , γ = Zi3 ; i = 1, . . . , I1 , j = 1, . . ., I2 , k = 1, . . . , I3 .

κ3 →0 α3 →0

See Appendix A for the derivation of (15). Thus, the posterior mean of h (on the original scale
of the data) are exactly the observed data {Zi j } as the hyperparameters go to zero. This suggests
that estimates from f (h|ztrn ) overfits the data, however, it is not necessarily true that f (y, θ |ztrn )
overfits the data.

16

3.4 Example of Bayesian Implementation
Consider the following mixed effects model for the transformed data (e.g., see Cressie and Johannesson, 2008, among others):
ind

Transformed Data Model : hi j |β , η , ξi j , λ ∼ Normal x′i j β + S′i j η + ξi j , σ 2

Process Model 1 : η |ση2 ∼ Normal 0 r , ση2 Ir ;


2 ind
2
Process Model 2 : ξi j |σξ ∼ Normal 0, σξ ;
Prior 1 : σ 2 ∼ IG (αv , βv ) ;


Prior 2 : β ∼ Normal 0 p , σβ2 I p ;

Prior 3 : σξ2 ∼ IG αξ , βξ ;

Prior 4 : ση2 ∼ IG (αη , βη ) ; i = 1, . . .I j , j = 1, 2, 3,



m(h|λ );

(16)

where xi j is a p-dimensional vector of known covariates, Ir is a r × r identity matrix, 0 r is
an r-dimensional vector of zeros, αv = αη = αξ = 1, βv = βη = βξ = 1, σβ2 = 100, and

ξ = (ξ11 , . . ., ξI3 3 )′ . The hyperparameters are chosen so that the prior is relatively “flat” and we
find that our results are robust to these specifications. In Algorithm 1, we set Yi j = x′i j β +S′i j η + ξi j
and θ = (β ′ , σ 2 , σξ2 , ση2 )′ . The choice of basis functions and specification of r are important. In
Appendix B.2, we give these details.
The full conditional distributions for y and θ are well-known (e.g.,see Cressie and Wikle, 2011,
for a standard reference reference) and are listed in Appendix B.3. Thus, Step 2 of Algorithm 1
involves simulating according to (14) and Step 4 of Algorithm 1 involves sequentially simulating
simulating from these standard full-conditional distributions. Details are given in Appendix B.1
and B.3.

17

4 Statistical Inference
Estimation and prediction over the training set can be done by computing summary statistics using
the quantities generated in Step 4 of Algorithm 1. However, to forecast values (e.g., future cases
or deaths due to COVID-19) we make use of validation and testing datasets, which is a common
strategy in machine learning (Hastie et al., 2009).

4.1 Estimation and Goodness-of-Fit using Training Data
Estimation and prediction of Yi j at i = 1, . . ., I j is rather natural using the output from Algorithm
[b]

1. In particular, let θ [b] and Yi j be the b-th replicate from Step 4 in Algorithm 1. Then one can
estimate θ and Yi j using summary statistics such as:
b i j |ztrn ) =
E(Y
b θ |ztrn ) =
E(

B
B
1
[b]
∑ ∑ Yi j ; i = 1, . . . ; I j , j = 1, 2, 3
B − b0 b=b
0 +1 b=b0
B
B
1
∑ ∑ θ [b],
B − b0 b=b
0 +1 b=b0

among several other summary statistics are also computed in our analyses (in our analyses we also
compute percentiles to asses uncertainty). Here, b0 is a “burn-in” value. In the context of the linear
model in Section 3.4, we would be interested in summary statistics of ∑i:ti =t ∑ j S′i j η , where recall

η is the random effect that is shared across response types. Estimates of this random effect can be
used to summarize the relationship between response types.
Assessment of the goodness of fit can be done similar to residual analyses of transformed data
′
in traditional regression analyses. We compute the residuals δ = δi j : i = 1, . . ., I j , j = 1, 2, 3 ,

δi j = hi j −Yi j , and compute a credible region associated with δ (e.g., see Gelman et al., 2013, for
a standard reference). For example, for each i and j, find the values qL,i j and qU,i j , where
Z qU,i j
qL,i j

f (δi j |z)d δi j = 1 − α ; i = 1, . . ., I j , j = 1, . . . , J,
18

(17)

and where α is prespecified. A default choice is α = 0.05. In practice, it is rather straightforward
[b]

[b]

to approximate qL,i j and qU,i j . Let hi j and Yi j be the b-th posterior replicate of hi j and Yi j so that
[b]

[b]

[b]

δi j = hi j −Yi j is the b-th posterior replicate of δi j . Then qL,i j and qU,i j can be approximated with
[b]

the α /2 and 1 − α /2 percentiles of the set {δi j : b = 1, . . ., B}, respectively. If the value of zero
lies within this interval (e.g., qL,i j < 0 < qU,i j ) for many values of i and j, then this suggests that
the model for y provides a reasonable fit to this data set.
Equation (15) shows that the posterior mean of the transformation models overfits the data,
which we motivated as a way to avoid oversmoothing estimates of y and θ in Algorithm 1. However, the fact that the transformation model overfits is also important from the point-of-view of
diagnostics. In particular, in the goodness-of-fit literature, overfitted values are often used as a
proxy for the data. For example, in log-linear models the most parsimonious reduced model that
is not significantly different (in terms of the deviance or chi-square statistic) from the saturated
model (an overfitted model) is used for statistical inference (e.g., see Agresti, 2007, for a standard
reference). This is exciting because it provides a new way to conduct classical residual analysis in
a Bayesian multi-response context. In particular, in Sections 5 we give an example of plotting the
(posterior median) residuals versus a useful covariate not included in the analysis to assess whether
or not it should be included in a model.

4.2 Estimating Hyperparamers using a Validation Dataset
In machine learning, one often adjusts the model for being biased towards the training data by holding aside a dataset to estimate hyperparameters. This hold-out dataset is referred to as a validation
′
dataset (Hastie et al., 2009). The validation dataset zval = (Zi j : i = I val
j + 1, . . . , I, j = 1, 2, 3) is
val
observed over the indices i ∈ {I j + 1, . . ., I val
j } and j = 1, 2, 3, where I j < I j ≤ I. Additionally, let

Yi∗j be different from, but independent and identically distributed as Yi j . We can not replace Yi∗j with
Yi j in our analysis of the validation data, otherwise, the validation data would be included with the

19

Algorithm 2: Steps Needed for Fitting the Validation Data.
1:

∗[0]

Set b = 1 and initialize Yi∗j and κ with Yi j

and κ [0] .

∗[b]

Sample Yi j using Algorithm 1
3: Sample κ [b] from it’s full-conditional distribution. We use the slice sampler (Neal et al., 2003)
since the full-conditional distribution does not have a closed form.
4: Set b = b + 1.
5: Repeat Steps 2−5 until b = B for a prespecified value of B.

2:

training data when updating Yi j . Then, we assume
Zi1 |Yi1∗ ∼ Normal(k1 (Yi1∗ , κ ), v)
Zi2 |Yi2∗ ∼ Binomial {bi , k2 (Yi2∗ , κ )}
Zi3 |Yi3∗ ∼ Poisson {k3 (Yi3∗ , κ )} ; i = I j + 1, . . ., I val
j
f (κ ),

(18)
(19)

where κ is a generic d-dimensional vector of real-valued parameters and f (κ ) is the prior distribution of this parameter. The functions k j (Yi j , κ ) are not necessarily equal to g j (Yi j ), and we
parameterize the unknown function k j with κ . In this article, we allow for either k j = g j so that

κ ≡ 0, or g j to be adjusted linearly so that,
k j (Y ) = κ j0 + κ j1 g j (Y ); j = 1, 2, 3, Y ∈ R,

(20)

and κ = (κ10 , κ20 , κ30 , κ11 , κ21 , κ31 )′ . In this setting, we choose the improper flat prior f (κ ) = 1.
When k j = g j so that κ ≡ 0 there is no need to consider a validation dataset, since there is no
hyperparameter κ to estimate.

20

Algorithm 3: Steps Needed for Forecasting.
∗∗[0]

1:

∗
Set b = 1 and initialize Yi∗∗
j and κ with Yi j

2:

Sample Yi j using Algorithm 1.
Sample κ ∗[b] using Algorithm 2.
[b]
Sample Zi j from (21).
Set b = b + 1.
Repeat Steps 2−5 until b = B for a prespecified value of B.
[b]
Compute the sample mean and variance (across the index b) of Zi j .

3:
4:
5:
6:
7:

and κ ∗[0] .

∗∗[b]

4.3 Forecasting
We produce next day forecasts for the variables in our study. In particular, the testing observations
∗
∗∗
are defined over the indices i = I val
j + 1, . . . , I for j = 1, 2, 3. We let κ and Yi j be distributed

according to f (κ |ztrn , zval ) and f (Yi j |ztrn ), respectively. Again, we can not let Yi∗∗
j equal Yi j , since
otherwise, the testing data would be included when updating Yi j based on the training data. Then,
we assume that
Zi1 |Yi1∗∗ , κ ∗ ∼ Normal(k1 (Yi1∗∗ , κ ∗ ), v)
Zi2 |Yi2∗∗ , κ ∗ ∼ Binomial {bi , k2 (Yi2∗∗ , κ ∗ )}
Zi3 |Yi3∗∗ , κ ∗ ∼ Poisson {k1 (Yi3∗∗ , κ ∗ )} ; i = I val
j , . . . , I, j = 1, 2, 3.

(21)

Predictions of the data process and estimation of cross-covariances can be found using in a similar
manner as (3) and (2). That is, the posterior mean and covariance of Zi j and Zkm is, E(Zi j |ztrn )
and cov(Zi j , Zkm |ztrn ), where recall, under the mixed effects assumption cov(Zi j , Zkm |ztrn ) =
S′i j cov(η |ztrn )Si j , which is not necessarily zero. Implementation is be summarized in Algorithm
3. When g j ≡ k j and κ ≡ 0, the predictions and covariances are simply
n
o
E(Zi j |ztrn ) = E ci j g−1
(Y
)|z
ij
trn
j

−1
cov(Zi j , Zmk |ztrn ) = cov(ci j g−1
j (Yi j ), cmk g j (Ymk )|ztrn ),

21

(22)

which can be directly computed from Step 4 of Algorithm 1. Once the next day data is observed,
it is treated as “testing data,” which is then used to assess the performance of our forecasts (e.g.,
through the root mean squared error, etc.).

4.4 Summaries of the Models used for Inference
There are three models used to do statistical inference, one that uses the training data, another
based on validation data, and a third based on testing data. The joint distribution of the training
data, processes, and parameters is written as the product of the following conditional distributions:

Training Data Model 1 : Zi1 |hi1 ∼ Normal(hi1 , v)


exp (hi2 )
Training Data Model 2 : Zi2 |hi2 ∼ Binomial bi ,
1 + exp (hi2 )


Training Data Model 3 : Zi3 |hi3 ∼ Poisson exp hi j ; i = 1, . . ., I j , j = 1, 2, 3
Transformed Data Model : f (hi j |Yi j , θ )m(h|γ ); i = 1, . . ., I j , j = 1, 2, 3

(23)

Process Model : f (y|θ )
Prior : f (θ )
Transformation Hyperprior : f (γ ).

The model in (23) is the aforementioned HGT model. This is a well defined proper model (see
Appendix A for these details), provided that f (hi j |θ ), f (y|θ ), and f (θ ) are proper.
Recall that one motivation for the model in (23) is that one can incorporate their preferred
model for continuous data directly into our framework. This is especially important to aid researchers in analyzing COVID-19 using their preferred approach (cite) in a computationally efficient manner, since Algorithm 1 does not require one to change the implementation of their
preferred model. This flexibility arises in the data scientist’s specification of f (hi j |θ ), f (y|θ ),
and f (θ ). In Section 3.3 we specify f (hi j |θ ), f (y|θ ), and f (θ ) using a mixed effects model,
22

and in Section 5 we also consider using BART to illustrate this flexibility. Although we only consider Bayesian specifications of the preferred model, Step 4 can easily be substituted with replicates/estimates of y and θ (computed using h[b] ) from empirical Bayesian models, approximate
Bayesian models, or frequentist models.
The LCM is explicitly used in the HGT model in (23) through the term m(h|y), where recall

m(h|y) = R R

∏i, j fDY (hi j |α j , κ j , a, b)
,
( f (h|y, θ ) f (y|θ ) f (θ )dyd θ

γ = (α1 , α2 , α3 , κ1 , κ2 , κ3 , a, b)′ , and the prior for γ is defined in Appendix B.1. Recall that Algorithm 1 is a collapsed Gibbs sampler, where we update h and γ using the marginal distribution
of (23) found by integrating our y and θ . Specifically, when integrating our y and θ in (23), we
obtain

Training Data Model 1 : Zi1 |hi1 ∼ Normal(hi1 , v)


exp (hi2 )
Training Data Model 2 : Zi2 |hi2 ∼ Binomial bi ,
1 + exp (hi2 )


Training Data Model 3 : Zi3 |hi3 ∼ Poisson exp hi j ; i = 1, . . ., I j , j = 1, 2, 3
Transformation Prior :

∏ fDY (hi j |α j , κ j , a, b)
i, j

Transformation Hyperprior : f (γ ).
which leads to the computationally simple updates of h and θ developed in Section 3.3 to be used
in Step 2 of Algorithm 1.
The joint distribution of the validation data, processes, and parameters is written as the product

23

of the following conditional distributions:
Validation Data Model 1 : Zi1 |Yi1∗ , κ ∼ Normal(k1 (Yi1∗ , κ ), v)

(24a)

Validation Data Model 2 : Zi2 |Yi2∗ , κ ∼ Binomial {bi , k2 (Yi2∗ , κ )}

(24b)

Validation Data Model 3 : Zi3 |Yi3∗ , κ ∼ Poisson {k3 (Yi3∗ , κ )}

(24c)

Posterior Process Model : f (Yi∗j |ztrn )
Prior : f (κ ); i = I j + 1, . . ., I val
j , j = 1, 2, 3,
where recall that the goal of this model is to estimate κ from its posterior f (κ |zval , ztrn ), which is
a parameter that allows one to avoid overfitting the training data. The distribution f (Yi∗j |ztrn ) is the
posterior distribution implied by the model in (23). Model (24) can be implemented through Algorithm 2. When f (y|θ ) is specified according to a linear model (i.e., Equation (4)) then Equations
(24a) through (24c) can be thought of as a GLMM (McCulloch et al., 2008). GLMMs also arise in
our model for testing data. The joint distribution of the testing data, processes, and parameters is
written as the product of the following conditional distributions:
Testing Data Model 1 : Zi1 |Yi1∗∗ , κ ∗ ∼ Normal(k1 (Yi1∗∗ , κ ∗ ), v)
Testing Data Model 2 : Zi2 |Yi2∗∗ , κ ∗ ∼ Binomial {bi , k2 (Yi2∗∗ , κ ∗ )}
Testing Data Model 3 : Zi3 |Yi3∗∗ , κ ∗ ∼ Poisson {k3 (Yi3∗∗ , κ ∗ )}

(25)

Posterior Process Model : f (Yi∗∗
j |ztrn )
Posterior Parameter Model : f (κ ∗ |zval , ztrn ); i = I val
j + 1, . . . , I, j = 1, 2, 3,
∗∗
where the goal is to predict Zi j at i = I val
j +1, . . . , I and j = 1, 2, 3. The distribution f (Yi j |ztrn ) is the

posterior distribution implied by the model in (23) and f (κ ∗ |zval , ztrn ) is the posterior distribution
from (24). Model (25) can be implemented through Algorithm 3. For example, Zi j in Section 6
is the number of observed cases, deaths, and recoveries from COVID-19 in April 8, 2020, and the
24

posterior predictions from the model in (25) represent the next day forecasts.

5 Simulations
The goals of this simulation study is to provide a standard demonstration that the HGT model
produces reasonable predictions. Another goal is to illustrate the flexibility of the HGT model
to specify a data scientist’s preferred model for continuous data. To do this we apply (23) to the
spatio-temporal mixed effects model in Section 3.4 and BART (details in Appendix B.4).

5.1 Simulation Setup
Friedman (1991) introduced a simulation design, which has become a useful benchmark study
(e.g., see Chipman et al., 2010, among others). Let
h(x1,i j , . . ., x10,i j ) = 10sin(π x1,i j x2,i j ) + 20(x3,i − 0.5)2 + 10x4,i j + 5x5,i ; i = 1, . . ., I, j = 1, 2, 3,
(26)
which includes two non-linear terms, two linear terms, and a non-linear interaction. We consider
the following specifications of the distributional assumptions associated with the data:

Zi1 ∼ Normal(h(x1,i1 , . . ., x10,i1 ), 1)


exp (h(x1,i2 , . . . , x10,i2 ))
Zi2 ∼ Binomial 300,
1 + exp (h(x1,i2 , . . . , x10,i2 ))

Zi3 ∼ Poisson exp (h(x1,i3 , . . . , x10,i3 )) ,
for i = 1, . . . , I j . Methods are compared using the root mean squared error (RMSE),


I
3
 ∑i=1 ∑ j=1



h
i2 1/2


−1
−1
gbj h(x1,i j , . . . , x10,i j ) − g j h(x1,i j , . . ., x10,i j )

 ,
3I
25

(27)

where gb−1
j (h) is estimated using Monte-Carlo integration using 2,000 iterations with a burn-in of

−1
1,000. For each Bayesian method, we let gb−1
j (h) be the pointwise posterior mean of g j (h). We

fit the preferred model using covariates x1,i j , x3,i j , x4,i j , . . . , x10,i j , and hence, we consider the case

were an important covariate is not observed (i.e., {x2,i j }) and several unneeded covariates are included (i.e., {x6,i j , . . . , x10,i j } are not present in (26)). The omissions of {x2,i j } when implementing
our method is a slight departure from the original setup in Friedman (1991). However, we feel that
it is more realistic to assume that not all covariates are observed in practice, and will be a helpful choice for illustration. We specify xk,i j ∼ Uniform(0, 1), where Uniform(0, 1) is a shorthand
for the uniform distribution over the interval [0, 1] and k = 1, . . ., 10. The preferred models are
spatio-temporal mixed effects and BART (and an extension), whose implementation are described
in Appendix B.3 and Appendix B.4, respectively. Additionally, the choice of basis functions are
described in Appendix B.1. In the implementation of each preferred method, we allow each response type to have different regression coefficients.

5.2 Simulations: Joint Analysis of Multiple Response Types
In this section, we evaluate the predictive performance of our Bayesian model with unknown transformations in the multi-response setting. In particular, we set the preferred model equal to BART
(Chipman et al., 2010) and a Bayesian version of the spatio-temporal mixed effects model (Cressie
and Johannesson, 2008) using basis functions introduced by (Hughes and Haran, 2013). The posterior mean of hi j (referred to as the saturated model) are included as a default poor estimator, since
it is known to overfit the data (see Proposition 3).
The data are simulated according to (27), with I = 1000, I1 = 350, I2 = 350, and I3 = 200. We
do not include a validation dataset so that k j ≡ g j . We repeat this simulation study 20 times, and
we provide violin plots of the RMSE over the 20 replicates by method in Figure 2. In Figure 3 we
also plot the true function versus the estimated function for a single replicate data set. Figures 1

26

Figure 2: A violin plot of the RMSE (y-axis) by method (x-axis) over 20 independent replicates of
the data. The data are simulated as described in Section 5.1. Each method is implemented using
Algorithm 1, except the method “Saturated.”

and 2 suggest that the transformation-based spatio-temporal mixed effects (BART) performs well
in terms of predictive performance. For the replicate in Figure 3 the transformation-based spatiotemporal mixed effects (and BART) model had 97% (94%) of the point-wise credible intervals of
the elements of δ containing zero. The patterns observed in Figure 2 mimic the goodness-of-fit
diagnostics, which is notable because the goodness-of-fit diagnostics are data driven (and hence
can be used in practice) while Figure 2 is based on the unknown truth. These results suggests that
the Bayesian transformations can be used to obtain predictions in the non-Gaussian setting using
two standard models, and also has a useful built-in goodness-of-fit diagnostic.
Now, suppose we have observed the values of {x2,i j }, and recall these covariates are not included in the analysis. In Figure 4, we plot the posterior median of the residuals versus the covariate {x2,i j } across the indexes i and j for a single replicate of the data set. The plot clearly indicates
a sinusoidal or possibly quadratic pattern, which suggests that this behavior is not captured in our

27

Figure 3: Estimates versus the truth for a single replicate data set. The data are simulated as
described in Section 5.1. The estimate is labeled on the y-axis. The red line indicates the line
y = x.

model for y. We know this to be true because {x2,i j } is not included in our implementation, but the
data was generated using {x2,i j }. This is an illustration of how our approach provides a Bayesian
analog to a graphical technique from classical regression analysis (i.e., systematic patterns in residuals from a multiple regression versus a covariate suggest that the covariate should be included in
the analysis).

5.3 Simulations: Robustness to Departures from Model Assumptions
In this simulation study we compare the predictive performance our Bayesian transformation approach to predictions from the preferred model itself. A straightforward way to do this is to restrict
ourselves to the continuous data-only setting, in which both modeling paradigms can be implemented. The data are simulated according to (27), with I1 = 800, I = 1000, and I2 = I3 = 0. We
do not include a validation dataset so that k j ≡ g j .

28

Figure 4: We simulate a single replicate of {Yi j } according to Section 5.1. Then a spatio-temporal
mixed effects model is implemented using the specifications in Section 3.4. This plot displays the
posterior median of {δi j } (see Section 4.1) versus x2,i j , which is not included in our implementation of the spatio-temporal mixed effects model. A systematic pattern in this plot suggests that
including x2,i j would improve our analysis of y.

We repeat this simulation study 20 times, and we provide violin plots of the RMSE over the
20 replicates by method in Figure 5. In this section, we include an additional predictor: soft
BART (SBART; Linero and Yang, 2018, see Appendix B.4 for more details). We again see that
the Bayesian transformation versions of BART and spatio-temporal mixed effects outperform the
saturated model, with the spatio-temporal mixed effects model clearly outperforming BART. Additionally, the Bayesian transformation version of BART and spatio-temporal mixed effects perform
only slightly better than or the same as their non-transformed counterparts. Here we see that
SBART performs worse than the saturated model in terms of RMSE. The Bayesian transformation
version of SBART does not perform noticeably different than SBART in terms of RMSE. Thus,
in the continuous only setting, if the preferred model performs well (or poorly) one should expect
the Bayesian transformation approach to perform well (or poorly). Recall that we can use the

29

Figure 5: A violin plot of the RMSE (y-axis) by method (x-axis) over 20 independent replicates of
the data. The data are simulated as described in Section 5.1. Each method is implemented using
Algorithm 1, except the method “Saturated.” The observed data set are used as the predicted values
for the method “Saturated.” The left panel displays the results of the Bayesian transformation
methods, and the right panel presents the results of the original methods.

goodness-of-fit approach in Section 4.1 to assess when a method performs poorly in practice. For
example, for a single replicate data set, we found that the percent of credible intervals of the elements of δ that contain zero (by method) are as follows: 99.8% (spatio-temporal mixed effects),
77.4% (BART), and 58.1% (SBART). This produces the same rankings of the method in terms of
RMSE.

6 Joint analysis of COVID-19 occurrences, the adjusted closing
price of the Dow Jones Industrial, and Google Trends data
We now present our joint analysis of deaths due by and occurrences of COVID-19, the adjusted
closing price of the DJI, and the Google Trends interest score in searches of “coronavirus” (see time
30

20

40

60

200
100
0

0
0

0

20

40

60

0

20

40

60

Time

Time

Time

Adjusted Closing Price

Sum of Logit Interest

Residual Plot

0

20

40
Time

60

80

0

20

40
Time

60

2
0
−2

5
−5

0

Logit Interest

2400
2000

Price

2800

10

Posterior Median Residual

4

0

300

400

Sum of Log Recovered

Log Frequency

Log Frequency

600
400
200

Log Frequency

50 100 150 200 250

Sum of Log Mortality

800

Sum of Log Confirmed Cases

0

20

40

60

Time

Figure 6: Goodness of Fit: We plot the sum (over regions) of log number of reported COVID-19
infections (top left), sum (over regions) log number of reported COVID-19 deaths (top middle),
sum (over regions) log number of reported COVID-19 recoveries (top right), the DJI adjusted
closing price (bottom left), and the logit (log(Yi2 /100 − Yi2 )) Google Trends interest score for
searches of “coronavirus” (bottom middle). Note that the DJI price data is not available on Saturday
and Sundays. The red lines represent the predicted values from our model, and the black circle
represent the observed values. The black lines are pointwise 95% credible intervals. The credible
intervals are left out in the bottom panels for visualization purposes (credible intervals are large),
and in this panel each datum falls within their respective credible interval. The posterior median
residuals versus time is given in the bottom right panel.

31

80

series displays of this data in Figure 1). We implement the HGT model, and assume the process
and priors in (16). In our model Zi1 represents the negative adjusted closing price per $10,000.
This transformation is made so that we see increasing trend over time among all three response
types. Our specifications of the basis functions are defined in Appendix B.2, and covariates for
the region and response-type are included. The data from January 22, 2020 to April 6, 2020 are
the training data (n = 10, 600), the data on April 7, 2020 is held-out as a validation dataset (373
observations), and the data on April 8 is held-out as a testing dataset (374 observations).
The MCMC is implemented according to Algorithms 1 through 3 with 10,000 replicates and
a burn-in of 1,000. Convergence was assessed visually through the use of trace plots and through
Gelman-Rubin diagnostics (Gelman et al., 2013) with no indications of a lack of convergence.
All of our analyses were implemented on Windows 10 with the following specifications: Intel(R)
CORE(TM) i5-8250U CPU with 1.60Gh.

6.1 Goodness of Fit
In Figure 6 we plot the posterior mean death, confirmed cases, recovered cases, adjusted closing
price, and Google Trends interest score. Here, we see that the predicted values are reasonably close
to their observed values with the observed data close contained within a pointwise 95% credible
interval. These results suggest that the in-sample error is small, and that the predicted values
reflect the general patterns of the data. Goodness of fit can be formally investigated according to
Section 4.1. Roughly 99.4% percent of the credible intervals, as defined in (17), contain zero. This
provides additional evidence the model provides a reasonable fit to the data. In the bottom right
panel of Figure 6 we plot the posterior median residual (i.e., δ ) versus the time the observation
was recorded. Here we see roughly no pattern over time, which suggests that our specification of
the basis functions were reasonable.

32

Variance v. Mean

6
2

4

Posterior Mean

8
6
4
2
0

Log Testing Data

8

10

Predictions v. Testing Data

2

4

6

8

2.0

Predictions

2.5

3.0

3.5

Posterior Variance

Figure 7: Forecasting: In the left panel we plot the forecasted testing data using Algorithm 3. Here
the testing data represents all confirmed cases, recoveries, and deaths on April 8, 2020. The right
panel plots the posterior variance of the predicted testing data versus the posterior mean.

6.2 Estimation and Prediction
We did not include the data on April 8-th, 2020, which was the most current value available at
the time of the analysis. We use the model to predict the number of deaths, number of confirmed
recoveries, and number of confirmed cases according to Algorithm 3. In Figure 7 we provide the
posterior means associated with these values versus the testing data. In general, the posterior means
trends the testing data, except for smaller testing values, where there is a tendency to overestimate
the log count. However, the percentage (over the testing data) of pointwise credible intervals that
contain the the testing data is 98.4%, which suggest that the uncertainty of these estimates are
captured in the model. This property of the model is also seen in the plot of the posterior variance
versus the posterior mean, also displayed in Figure 7. Here, smaller predicted values tend to be
over-dispersed, and larger predicted values appear to be equi-dispersed. Thus, we appear to have
accurate predictions of the areas with the largest confirmed cases, recoveries, and deaths. Being
33

15
10
5
0
−10 −5

Cross−Response Effects

Posterior Mean Effects

0

10

20

30

40

50

Time

Figure 8: We plot the posterior mean of ∑Ti =t S′i j η . The red line indicates pointwise 95% credible
intervals.

able to accurately estimate large values of (log) occurrences is particularly important. That is, if we
know where there are large occurrences of confirmed cases, then additional testing of individuals
in these regions allows one to isolate all those who test positive in this region, which ultimately
reduces the spread of COVID-19 from this region to others (Ai et al., 2020). Consequently, models
such as ours can be useful at stopping the spread of COVID-19. However, finer-scale regional
data would be necessary for this model to be helpful in narrowing in on potential “hot-spots” in
practice.
In Figure 8, we plot the posterior mean of the random effects that is shared across response-type
along with pointwise 95% credible intervals (see Section 4.1). The time period between January
22, 2020 and February 23, 2020 was particularly crucial, since this time range saw the strongest
direct effects between between COVID-19 cases, the negative adjusted closing price, and Google
34

Trends interest-score in the Google search “coronavirus.” Furthermore, the fact that zero does
not tend to fall within the credible intervals suggests that our incorporation of dependence across
response-types, spatial regions, and days was reasonable. February 23, 2020 (ti = 33) marks the
time in which the adjusted closing price initially started to decrease (see Figures 1 and 6), and the
Google Trends interest score increases. After February 23, 2020 the random effect appears to be
negative-valued, which suggests an indirect relationship among these responses.

7 Discussion
COVID-19 is a global epochal health disaster, and social distancing has become a necessary public
health measure to protect the health of individuals. In this article, we investigate the relationship
between COVID-19 cases, the US economy (specifically the adjusted closing price of DJI), and
interest on Google (specifically Google Trends interest score for the search “coronavirus”). The
data and model suggests that the relationship among these three values had the strongest positive
relationship during a majority of February 2020, which suggests that this was an important time
period. Additionally, there are clear cross-dependencies among response types, regions, and days.
It is important to comment that correlation does not imply causation, and to make explicit causal
conclusions one needs to adopt methods among the causal inference literature (Rubin, 2005). Finally, our model produces reasonable forecasts of the log frequency of cases, deaths, and recoveries
from COVID-19. This suggests that with finer-scale regional data, this model could potentially be
useful for targeting future hot-spots of COVID-19.
We introduce the HGT model in order to analyze COVID-19 and social distancing related
variables, which is derived from a straightforward combination of the LCM and the GLMM. This
combination is motivated as a means to aid other researchers to analyze multi-response datasets
such as the one considered in this article. In particular, our approach provides several contributions
to Bayesian statistics. First, we have developed a general all-purpose Bayesian model to analyze
35

multiple responses (e.g., continuous, Binomial counts, and Poisson counts). Our approach allows
one to directly incorporate their preferred Bayesian model to analyze multi-response data without
completely abandoning their approach to the implementation of their preferred model. Second, we
developed a general Bayesian analog to the classical comparison between a saturated model and a
reduced model. This results in the use of classical residual analysis for assessing goodness-of-fit
in Bayesian models for multi-response data. Code and tutorials on how to adapt the HGT to your
preferred model can be found at https://github.com/JonathanBradley28/CM.
In our simulations, an illustration was given of non-linear functional analysis of multiple response types using BART as the preferred model. Additionally, an illustration was given of a joint
spatial analysis of multiple response types using a spatio-temporal mixed effects model as the preferred model. These results suggest that the prediction error of our approach is small (in terms of
RMSE), and we can develop multi-response versions of two different preferred models seamlessly.
Additionally, data driven goodness-of-fit diagnostics were able to lead to the same conclusion as
the RMSE criterion (based on the latent process) that is unobserved in practice.

Acknowledgments
This research was partially supported by the U.S. National Science Foundation (NSF) under NSF
grant SES-1853099. I also would like to thank Drs. Christopher Wikle and Scott Holan at the
University of Missouri on their feedback on an earlier version of this article.

References
Agresti, A. (2007). Categorical data analysis, 2nd Ed.. Springer.
Ai, T., Yang, Z., Hou, H., Zhan, C., Chen, C., Lv, W., Tao, Q., Sun, Z., and Xia, L. (2020).
“Correlation of chest CT and RT-PCR testing in coronavirus disease 2019 (COVID-19) in China:
a report of 1014 cases.” Radiology, 200–642.

36

Argyriou, A., Evgeniou, T., and Pontil, M. (2007). “Multi-task feature learning.” Advances in
neural information processing systems, 19.
Beasley, T. M., Erickson, S., and Allison, D. B. (2009). “Rank-based inverse normal transformations are increasingly used, but are they merited?” Behavior genetics, 39, 5, 580.
Box, G. E. P. and Cox, D. R. (1964). “An analysis of transformations.” Journal of the Royal
Statistical Society: Series B (Methodological), 26, 2, 211–243.
Bradley, J., Holan, S., and Wikle, C. (2018). “Computationally Efficient Distribution Theory for
Bayesian Inference of High-Dimensional Dependent Count-Valued Data.” Bayesian Analysis,
13, 253–302.
Bradley, J. R., Holan, S. H., and Wikle, C. K. (2019a). “Bayesian Hierarchical Models with Conjugate Full-Conditional Distributions for Dependent Data from the Natural Exponential Family.”
Journal of the American Statistical Association.
Bradley, J. R., Wikle, C. K., and Holan, S. H. (2019b). “Hierarchical Models for Spatial Data with
Errors that are Correlated with the Latent Process.” Statistica Sinica.
— (2019c). “Spatio-temporal models for big multinomial data using the conditional multivariate
logit-beta distribution.” Journal of Time Series Analysis, 40, 3, 363–382.
Breiman, L. and Friedman, J. H. (1985). “Estimating optimal transformations for multiple regression and correlation.” Journal of the American statistical Association, 80, 391, 580–598.
Casella, G. and Berger, R. (2002). Statistical Inference. Pacific Grove, CA: Duxbury.
Charitidou, E., Fouskakis, D., and I. Ntzoufras, I. (2018). “Objective Bayesian transformation and
variable selection using default Bayes factors.” Statistics and Computing, 28, 3, 579–594.
Charitidou, E., Fouskakis, D., and Ntzoufras, I. (2015). “Bayesian transformation family selection:
Moving toward a transformed Gaussian universe.” Canadian Journal of Statistics, 43, 4, 600–
623.
Chen, M. H. and Ibrahim, J. G. (2003). “Conjugate priors for generalized linear models.” Statistica
Sinica, 13, 2, 461–476.
Chipman, H. and McCulloch, R. (2016). “BayesTree: Bayesian additive regression trees.” R
package version 0.3-1.4.
Chipman, H. A., George, E. I., , and McCulloch, R. E. (2010). “BART: Bayesian additive regression trees.” The Annals of Applied Statistics, 4, 1, 266–298.
Cressie, N. and Johannesson, G. (2008). “Fixed rank kriging for very large spatial data sets.”
Journal of the Royal Statistical Society, Series B, 70, 209–226.

37

Cressie, N. and Wikle, C. K. (2011). Statistics for Spatio-Temporal Data. Hoboken, NJ: Wiley.
Damien, P., Wakefield, J., and Walker, S. (1999). “Gibbs Sampling for Bayesian Non-Conjugate
and Hierarchical Models by Using Auxiliary Variables.” Journal of the Royal Statistical Society.
Series B (Statistical Methodology), 61.
Diaconis, P. and Ylvisaker, D. (1979). “Conjugate priors for exponential families.” The Annals of
Statistics, 17, 269–281.
Dobra, A. and Lenkoski, A. (2011). “Copula Gaussian graphical models and their application to
modeling functional disability data.” The Annals of Statistics, 5, 969–993.
Donnat, C. and Holmes, S. (2020). “Modeling the Heterogeneity in COVID-19’s Reproductive
Number and its Impact on Predictive Scenarios.” arXiv preprint arXiv:2004.05272.
Efron, B. (1992). “Bootstrap methods: another look at the jackknife.” In Breakthroughs in statistics, 569–593. Springer.
Fellinghauer, B., Buhlmann, P., Ryffel, M., Rhein, M. V., and Reinhardt, J. D. (2013). “Stable
graphical model estimation with random forests for discrete, continuous, and mixed variables.”
Computational Statistics amd Data Analysis, 64, 132152.
Friedman, J. H. (1991). “Multivariate adaptive regression splines.” The Annals of Statistics, 19, 1,
1–67.
Gao, H. and Bradley, J. R. (2019). “Bayesian analysis of areal data with unknown adjacencies
using the stochastic edge mixed effects model.” Spatial Statistics.
Gelfand, A. E. (2000). “Gibbs sampling.” Journal of the American statistical Association, 95, 452,
1300–1304.
Gelman, A. (2006). “Prior distributions for variance parameters in hierarchical models.” Bayesian
Analysis, 1, 515–533.
Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013).
Bayesian Data Analysis, 3rd edn.. Boca Raton, FL: Chapman and Hall/CRC.
Google (2020). “Google Trends.” https://trends.google.com/trends/.
Griffith, D. (2000). “A linear regression solution to the spatial autocorrelation problem.” Journal
of Geographical Systems, 2, 141–156.
— (2002). “A spatial filtering specification for the auto-Poisson model.” Statistics and Probability
Letters, 58, 245–251.
— (2004). “A spatial filtering specification for the auto-logistic model.” Environment and Planning
A, 36, 1791–1811.
38

H.-C.Yang, Hu, G., and Chen, M.-H. (2019). “Bayesian Variable Selection for Pareto Regression
Models with Latent Multivariate Log Gamma Process with Applications to Earthquake Magnitudes.” Geosciences, 9, 4, 169.
Hastie, T., Tibshirani, R., and Friedman, J. (2009). The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. New York, NY: Springer.
Hu, G. and Bradley, J. R. (2018). “A Bayesian spatial–temporal model with latent multivariate
log-gamma random effects with application to earthquake magnitudes.” Stat, 7, 1, e179.
Hughes, J. and Haran, M. (2013). “Dimension reduction and alleviation of confounding for spatial
generalized linear mixed model.” Journal of the Royal Statistical Society, Series B, 75, 139–159.
Kang, E. L. and Cressie, N. (2011). “Bayesian inference for the spatial random effects model.”
Journal of the American Statistical Association, 106, 972 – 983.
Katzfuss, M. and Cressie, N. (2012). “Bayesian hierarchical spatio-temporal smoothing for very
large datasets.” Environmetrics, 23, 94–107.
Kim, S., Chen, M. H., Ibrahim, J. G., Shah, A. K., and Lin, J. (2013). “Bayesian inference
for multivariate meta-analysis Box–Cox transformation models for individual patient data with
applications to evaluation of cholesterol-lowering drugs.” Statistics in Medicine, 32, 23, 3972–
3990.
Kim, S. and Xing, E. P. (2009). “Statistical estimation of correlated genome associations to a
quantitative trait network.” PLos Genetics, 5.
Kristensen, K., Nielsen, A., Berg, C. W., Skaug, H., and Bell, B. (2015). “TMB: automatic differentiation and Laplace approximation.” arXiv preprint arXiv:1509.00660.
Lehmann, E. and Casella, G. (1998). Theory of Point Estimation. 2nd ed. New York, NY: Springer.
Linero, A. R. and Yang, Y. (2018). “Bayesian regression tree ensembles that adapt to smoothness
and sparsity.” Journal of the Royal Statistical Society: Series B (Statistical Methodology), 80, 5,
1087–1110.
Liu, H., Han, F., Yuan, M., Lafferty, J., and Wasserman, L. (2012). “High-dimensional semiparametric gaussian copula graphical models.” The Annals of Statistics, 40, 2293–2326.
Liu, H., Lafferty, J., and Wasserman, L. (2009). “The nonparanormal: Semiparametric estimation
of high dimensional undirected graphs.” The Journal of Machine Learning Research, 10, 2295–
2328.
Long, N. J. (2020). “From social distancing to social containment: reimagining sociality for the
coronavirus pandemic.” Medicine Anthropology Theory.

39

Martino, S. and Riebler, A. (2019). “Integrated nested Laplace approximations (inla).” arXiv
preprint arXiv:1907.01248.
McCaw, Z. R., Lane, J. M., Saxena, R., Redline, S., and Lin, X. (2019). “Omnibus Inverse Normal
Transformation Based Association Test Improves Power in Genome-Wide Association Studies
of Quantitative Traits.” bioRxiv, 635706.
McCullagh, P. and Tresoldi, M. F. (2020). “A likelihood analysis of quantile-matching transformations.” arXiv preprint arXiv:2001.03709.
McCulloch, C. E., Searle, S. R., and Neuhaus, J. M. (2008). Generalized, Linear, and Mixed
Models. NJ: Wiley.
McCulloch, R. E. (1993). “Fitting regression models with unknown transformations using dynamic
graphics.” Journal of the Royal Statistical Society: Series D (The Statistician), 42, 2, 153–160.
Moran, P. A. P. (1950). “Notes on Continuous Stochastic Phenomena.” Biometrika, 37, 17–23.
Neal, R. M. (2011). “MCMC Using Hamiltonian Dynamics.” In Handbook of Markov Chain
Monte Carlo, eds. S. Brooks, A. Gelman, G. L. Jones, and X. Meng, 113–160. Chapman and
Hall.
Neal, R. M. et al. (2003). “Slice sampling.” The annals of statistics, 31, 3, 705–767.
— (2011). “MCMC using Hamiltonian dynamics.” Handbook of markov chain monte carlo, 2, 11,
2.
Office of Science and Technology Policy (2020). “Call to Action to the Tech Community on New
Machine Readable COVID-19 Dataset.” https://www.whitehouse.gov/briefings-statements/callaction-tech-community-new-machine-readable-covid-19-dataset/.
R. Krispin (2020). “Package ‘coronavirus’.” Retrieved March, 2020.
Rubin, D. B. (1981). “The bayesian bootstrap.” The annals of statistics, 130–134.
— (2005). “Causal inference using potential outcomes: Design, modeling, decisions.” Journal of
the American Statistical Association, 100, 469, 322–331.
Rue, H., Martino, S., and Chopin, N. (2009). “Approximate Bayesian inference for latent Gaussian models using integrated nested Laplace approximations.” Journal of the Royal Statistical
Society, Series B, 71, 319–392.
Servin, B. and Stephens, M. (2007). “Imputation-based analysis of association studies: candidate
regions and quantitative traits.” PLoS genetics, 3, 7.
Simonoff, J. S. (2012). Smoothing methods in statistics. Springer Science & Business Media.

40

Wahba, G. (1990). Spline Models for Observational Data. Philadelphia, PA: Society for Industrial
and Applied Mathematics.
Wilder-Smith, A. and Freedman, D. O. (2020). “Isolation, quarantine, social distancing and community containment: pivotal role for old-style public health measures in the novel coronavirus
(2019-nCoV) outbreak.” Journal of travel medicine, 27, 2, taaa020.
Xue, L. and Zou, H. (2012). “Regularized rank-based estimation of high-dimensional nonparanormal graphical models.” The Annals of Statistics, 40, 2541–2571.
Yahoo (2020). “Yahoo Finance.” https://finance.yahoo.com/.
Yang, E., Ravikumar, P., Allen, G. I., Baker, Y., Wan, Y. W., and Liu, Z. (2014). “A general
framework for mixed graphical models.” arXiv:1411.0288.
Yang, X., Kim, S., and Xing, E. P. (2009). “Heterogeneous multitask learning with joint sparsity
constraints.” NIPS, 21512159.
Yeo, I.-K. and Johnson, R. A. (2000). “A new family of power transformations to improve normality or symmetry.” Biometrika, 87, 4, 954–959.
Zhang, J., Litvinova, M., Liang, Y., Wang, Y., Wang, W., Zhao, S., Wu, Q., Merler, S., Viboud, C.,
and Vespignani, A. (2020). “Age profile of susceptibility, mixing, and social distancing shape
the dynamics of the novel coronavirus disease 2019 outbreak in China.” medRxiv.

Appendix A: Derivations
Derivation of (10): The distributions in (8) and (9) can be used to produce the following expression
of the joint distribution of the data, process, and parameters

f (ztrn , y, θ ) =

where f (ztrn , h) =

R

Z Z

f (ztrn |h) f (y, θ |h) f (h|γ ) f (γ ) dh d γ =

Z

f (y, θ |h) f (ztrn , h) dh ,

f (ztrn |h) f (h|γ ) f (γ ) d γ and we have used the assumption of conditional inde-

pendence between z and (y, θ ) given h. Then dividing by f (ztrn ) =
yields,
f (y, θ |z) =

Z

f (y, θ |h) f (h|z)dh,

41

RR

f (ztrn |h) f (h|γ ) f (γ ) dh d γ

which is the desired result.

Derivation of (13): Versions of this proof can be found in Diaconis and Ylvisaker (1979) and
Bradley et al. (2019a). The two distributions in (11) associated with j = 2 and j = 3 are members
of the natural exponential family (Lehmann and Casella, 1998), which are of the form,

f (Zi j |hi j , α j , κ j ) ∝ exp Zi j hi j − ci j ψ j (hi j ) ; i = 1, . . . , I j , j = 2, 3,
where ci2 = bi and ci3 = 1. Upon multiplying by (12) we have:

f (hi j |Zi j , α j , κ j ) ∝ exp (Zi j + α j )hi j − (κ j + ci j )ψ j (hi j ) ∝ DY(α j + Zi j , κ j + ci j ; ψ j ),
which proves the result for j = 2 and j = 3. For j = 1,
 


Zi1
1
h2
+ α1 hi1 − κ1 +
f (hi1 |Zi1 , α1 , κ1 ) ∝ exp
v
2v i j
( 


 
 2)

1
1 −1 Zi1
1 hi j
hi1
= exp 2 2κ1 +
2κ 1 +
+ α1
− 2κ 1 +
v
v
v
2
v 2
( 


 
 2

1
1 −1 Zi1
1 hi j
hi1
∝ exp 2 2κ1 +
2κ 1 +
+ α1
− 2κ 1 +
v
v
v
2
v 2
)


 
 
2
1
1 −2
1 −1 Zi1
1
2κ 1 +
2κ 1 +
2κ 1 +
+ α1
−
2
v
v
v
v
n
o2 
 
1 −1 Zi1
v + α1
 hi1 − 2κ1 + v

= exp 


1 −1
2 2κ 1 + v
(
 
 
 )
1 −1 Zi1
1 −1
∝ Normal
2κ 1 +
,
+ α1 , 2 κ 1 +
v
v
v


which completes the results.

42

Derivation of (15): In Equation (14) we see that


1
E(hi1|Zi1 , γ ) = 2κ1 +
v

−1 



 

1 −1 Zi1
Zi1
+ α1 + E(w1 |Zi1 , γ ) = 2κ1 +
+ α1 ,
v
v
v

which converges to Zi1 as α1 and κ1 approach zero. The expectation of a beta distribution is well
known (Casella and Berger, 2002), which from (14) gives us

E {g(hi2)|Zi2 , γ } = E(w2 |Zi1 , γ ) =

α2 + Zi2
,
κ 2 + bi

which converges to Zi2 /bi as α2 and κ2 approach zero. Similarly, the expectation of a gamma
distribution is well known (Casella and Berger, 2002), which from (14) gives us

E {g(hi3)|Zi3 , γ } = E(w3 |Zi1 , γ ) =

α3 + Zi3
,
κ3 + 1

which converges to Zi3 as α3 and κ3 approach zero.

Proof that (23) is proper: The joint distribution of the training data, transformed data, process,
parameters, and transformation hyperprior is given by:
(

I1

∏ f (Zi1 |hi1)
i=1

)(

I2

∏ f (Zi2 |hi2)
i=1

)(

I3

∏ f (Zi3|hi3)
i=1

)

f (h|y, θ )m(h|γ ) f (y|θ ) f (θ ) f (γ ).

Then integrate out y and θ to obtain,
(

I1

∏ f (Zi1 |hi1)
i=1

)(

I2

∏ f (Zi2|hi2)
i=1

)(

I3

∏ f (Zi3 |hi3)
i=1

43

)(

∏ fDY (hi j |α j , κ j , a, b)
i, j

)

f (γ ),

which follows from,
Z Z

=

f (h|y, θ )m(h|γ ) f (y|θ ) f (θ )dyd θ

Z Z

f (h|y, θ ) f (y|θ ) f (θ )dyd θ R R

∏i, j fDY (hi j |α j , κ j , a, b)
=
fDY (hi j |α j , κ j , a, b).
( f (h|y, θ ) f (y|θ ) f (θ )dyd θ ∏
i, j

Finally, we have the result, since the normal, binomial, Poisson, and DY distributions (Diaconis
and Ylvisaker, 1979) are proper and the prior on γ is proper.

Appendix B: Additional Model Details
Appendix B.1: Full-Conditional Distributions for the Transformation Hyperparameters
The full-conditional distributions for the transformation hyperparameters are found by multiplying
f (h|γ ) and f (γ ) as follows:
I

I1
∑ 2 (Zi1 − hi1 )
+ a1 , i=1
+ b1
v|· ∼ IG
2
2
f (α2 |·) ∝ α2a2 −1 exp(−b2 α2 )
Γ(α
f (α3 |·) ∝ α3a3 −1 exp(−b3 α3 )
f (κ2 |·) ∝ (κ2 − α2 )

ζ2 −1

!

I2
1
exp(α2 ∑ hi2 )
I
I
2 ) 2 Γ(κ2 − α2 ) 2
i=1

I3
κ3I3 α3
exp(
α
3 ∑ hi3 )
Γ(α3)I3
i=1

I2
Γ(κ2 )I2
exp(−κ2 ∑ log(1 + exp(hi2 )))I (κ3 ≥ α3 )
exp(−η2 κ2 )
Γ(κ2 − α2 )I2
i=1
I3

f (κ3 |·) ∝ (κ3 − α3 )ζ3−1 exp(−η3 κ3 )κ3I3 α3 exp(−κ3 ∑ exp(hi3))I (κ3 ≥ α3 ),

(B.1.1)

i=1

where Γ(t) =

R ∞ t−1
exp(−x)dx, I (·) is the indicator function, and IG(a, b) is an inverse gamma
0 x

distribution with shape a > 0 and rate b > 0. In our implementation we set the parameters a1 =
44

a2 = a3 = ζ2 = ζ3 1 and b1 = b2 = b3 = η2 = η3 = 1. We have found that our results are robust to
this specification. Step 3 of Algorithm 1 involves simulating from the full conditional distributions
in (B.1.1).

Appendix B.2: Choices of Basis Functions
In Section 5, the r-dimensional real-valued vector Si j is defined to be the Moran’s I basis function
(Hughes and Haran, 2013). The Moran’s I basis functions (Griffith, 2000, 2002, 2004) are motivated as a way to remove confounding between β and η , and allow for dimension reduction. The
basis functions are derived from the Moran’s I operator used in spatial statistics (Moran, 1950).
Specifically, basis functions are specified to be in the orthogonal column space associated with

the hat matrix X(X′ X)−1 X′ , where the N × p matrix X = xi j : i = 1, . . . , I, j = 1, 2, 3 . Define the
Moran’s I operator



G(X, At ) ≡ IN − X(X′ X)−1 X′ W IN − X(X′ X)−1 X′ ,
where W is a generic real-valued N × N matrix, which is often specified to be an adjacency matrix
that characterizes a network. The spectral representation G(X, W) = Φ Λ Φ ′ , is computed using a
N × N orthogonal matrix Φ and a N × N diagonal matrix with positive elements Λ. Let the N × r
real matrix consisting of the first r columns of Φ be denoted by S. The row of S corresponding to
the (i, j)-th data is set to equal to Si j . In Section 5, we set r = 500.
In Section 5, the r-dimensional real-valued vector Si j is defined to be thin-plate splines (Wahba,
(k)

1990). Specifically, let the m-th element of the 10-dimensional vector Si1 be defined as,
(ti /78 − cm )2 log {abs(ti /78 − cm )} ,

(B.2.1)

where cm = {0, 0.11, 0.22, 0.33, 0.44, 0.56, 0.67, 0.78, 0.89, 1} are 10 equally spaced values over
45

{t1 , . . .,t78 }. Then, let the m-th element of the 25-dimensional vector S∗i j be
(ti /78 − c∗m )2 log {abs(ti /78 − c∗m )} ,

(B.2.2)

where {c∗m } is a set of 25 equally spaced time-points between zero and one. Let the |Ak |×10 matrix
(k)

(k)

(1)

(266)

S1 = (Si1 : Ai = Ak ) and the I1 × 2660 matrix S1 = blkdiag(S1 , . . ., S1

), where blkdiag is

the block-diagonal operator and |Ak | is the number of observations recorded in region Ak so that
I1 = ∑k |Ak |. Here, the I1 ×2660 matrix S1 defines a set of basis matrices for each of the 266 regions
in the study, and hence, we allow for different time series within each region. Note that some
regions contain others (e.g., provinces are contained with countries). As such, shared time series
within a country imply within-country spatial dependence. Define the I j × 25 matrix S j = (S∗i j : i =
1, . . ., I j ) for j = 2, 3, which defines basis matrices for each individual response types. Then collect
all individual-level basis matrices into the matrix n × 2710 matrix S∗∗ = blkdiag(S1 , S2 , S3 ). Let
the n × 25 matrix S∗ = (S∗i j : i = 1, . . . , I j , j = 1, 2, 3), which represents the set of basis functions
that are shared among all response types. Finally, the n × 2735 matrix S = (S∗ , S∗∗ ) represents the
basis matrix used in our analysis, and the 2735-dimensional (i, j)-th row is denoted with Si j .

46

Appendix B.3: Full-Conditional Distributions for the Spatio-Temporal Mixed
Effects Model
The full conditional distributions for this spatio-temporal mixed effects model are well-known
(e.g.,see Cressie and Wikle, 2011, for a standard reference reference) and are as follows:




β |· ∼ Normal µ ∗β , Σ ∗β ;


η |· ∼ Normal µ ∗η , Σ∗η ;


ξ |· ∼ Normal µ ∗ξ , Σ ∗ξ ;

!−1
1
1
Σ ∗β ≡
,
X′ X + 2 I p
σ2
σβ
!−1
1
1
1
µ ∗η ≡ 2 Σ∗η (h − Xβ − ξ ), Ση∗ ≡
Ir + 2 Ir
2
σ
σ
ση
!−1
1
1
1
µ ∗ξ ≡ 2 Σ ξ∗ (h − Xβ − Sη ), Σ ∗ξ ≡
In + 2 In
. (B.3.1)
σ
σ2
σξ
1
µ ∗β ≡ 2 Σ ∗β (h − ξ − Sη ),
σ

The full conditional distributions for variance parameters are well-known (e.g.,see Gelman et al.,
2013, for a standard reference) and are as follows:
I

3

′

′

2
∑i=1
∑ j=1 (hi1 − xi j β − Si j η − ξi j )
n
2
σ |· ∼ IG
+ αv ,
+ βv
2
2


η ′η
r
2
+ αη ,
+ βη
ση |· ∼ IG
2
2
!
′
ξ
ξ
n
+ αξ ,
+ βξ .
σξ2 |· ∼ IG
2
2

!

(B.3.2)

Step 4 of Algorithm 1 for this model involves simulating from the full-conditional distributions in
(B.3.1) and (B.3.2).

47

Appendix B.4: Bayesian Additive Regression Trees
Consider the following expression for the BART model (e.g., see Chipman et al., 2010, among
others):
2

ind

Data Model : hi j |Mk , Tk , σ , λ ∼ Normal

Prior 1 : µgh |Tk ∼ Normal 0,


1
;
4ε 2 m

(

m

∑ w(xi j ; Mk , Tk ), σ

k=1

2

)

m(h|λ );

Prior 2 : σ 2 ∼ IG (αv , βv ) ;
uk

Prior 3 : f (Tk ) ∝ ∏ α (1 + dg )−β ; i = 1, . . .I j , j = 1, 2, 3,

(B.4.1)

g=1

where xi j is a p-dimensional vector of known covariates, w(·) is a decision tree (see definition
in Chipman et al., 2010), set Mk = (µ11 µb′ k k) , bk is the k-th terminal node, and dk is the depth of
internal node k. The hyperparameters ε ∈ [1, 3], αν > 0, βv > 0, α > 0, and β > 0 are chosen based
on the default specifications of the R package BayesTree (Chipman and McCulloch, 2016). Implementation is achieved through a Metropolis-within-Gibbs sampler and a backfitting algorithm
as described in Chipman et al. (2010). This Markov chain Monte Carlo (MCMC) algorithm is
computed using the R package BayesTree. That is, Step 4 of Algorithm 1 for this model involves
simulating from posterior distribution of {Mk }, {Tk }, and σ 2 using BayesTree. The SBART
method is an extension of the BART algorithm, which involves a different specification of w(·).
Public use code described in Linero and Yang (2018) is used.

48

