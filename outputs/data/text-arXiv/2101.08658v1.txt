Fidelity and Privacy of
Synthetic Medical Data
Review of Methods and Experimental Results
January 2021

Ofer Mendelevitch, Michael D. Lesh SM MD FACC,
Keywords: synthetic data; statistical fidelity; privacy; data access; data sharing; open data; metrics;
EMR; EHR; clinical trials; review of methods; de-identification; re-identification

1

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Table of Contents
Table of Contents

2

Abstract

3

1. Introduction

3

1.1 De-Identification and Re-Identification

4

1.2 Synthetic Data

4

2. Synthetic Data in Medicine
2.1 The Syntegra Synthetic Data Engine and Medical Mind
3. Statistical Fidelity Validation

5
6
6

3.1 Visualize and Compare Datasets

6

3.2 Population Statistics

7

3.3 Single Variable (Marginal) Distributions

8

3.4 Pairwise Correlation

9

3.5 Multivariate Metrics

10

3.5.1 Predictive Model Performance

10

3.5.2 Survival Analysis

11

3.5.3 Discriminator AUC

11

3.6 Clinical Consistency Assessment

12

4. Privacy Validation

12

4.1 Disclosure Metrics

12

4.1.1 Record Distance Metric

13

4.1.2 Membership Inference Test

13

4.1.3 File Membership Hypothesis Test

15

4.1.4 Attribute Inference Test

15

4.1.5 TCAP

15

4.2 Copy Protection Metrics

16

4.2.1 Distance to Closest Record - DCR

16

4.2.2 Exposure

17

5. Experimental Results

17

5.1 Datasets

17

5.2 Results

18

5.2.1 Statistical Fidelity

18

5.2.2 Privacy

28

6. Discussion and Analysis

31

6.1 DIG dataset results analysis

32

6.2 NIS dataset results analysis

32

6.3 TEXAS dataset results analysis

33

6.4 BREAST results analysis

33

7. Conclusions

34

References

36

2

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Abstract
The digitization of medical records ushered in a new era of big data to clinical science. With this
digitization came the possibility that data could be shared, to multiply insights beyond what investigators
could abstract from paper records logged into simple databases. The need to share individual-level
medical data to accelerate innovation in precision medicine continues to grow, and has never been more
urgent, as scientists grapple with the COVID-19 pandemic. One promise was that sharing large patientlevel medical datasets would support innovation in precision medicine. However, enthusiasm for the use
of big data has been tempered by a fully appropriate concern for patient autonomy and privacy. That is,
the ability to extract private or confidential information about an individual, in practice, renders it difficult
to share data, since significant infrastructure and data governance must be established before data can
be shared. Although HIPAA provided de-identification as an approved mechanism for data sharing,
linkage attacks were identified in the early 2000’s as a major vulnerability. A variety of mechanisms have
been established to avoid leaking private information. Data owners anonymize the data in a number of
ways, such as removing identifiable features (eg, names, date-of-birth, zip code, etc), strictly limiting the
amount of information that can be shared, or employing mathematical techniques such as differential
privacy. Another approach, which we focus on here, is creating synthetic data that mimics the underlying
data. For synthetic data to be a useful mechanism in support of medical innovation and a proxy for realworld evidence (RWE), one must demonstrate two properties of the synthetic dataset: (1) any analysis
that one would perform on the real data must be matched by analysis of the synthetic data (statistical
fidelity); and (2) the synthetic data must preserve privacy, with minimal risk of re-identification (privacy
guarantee). In this paper we propose a framework for quantifying the statistical fidelity and privacy
preservation properties of synthetic datasets and demonstrate these metrics for synthetic data
generated by Syntegra’s technology.
Keywords: synthetic data; statistical fidelity; safety; privacy; metrics; EMR; EHR; clinical trials; review of
methods; generative models; deep learning;

1. Introduction
A prerequisite to healthcare innovation is the availability of high-quality, unbiased, and diverse patient-level medical
datasets. Increasingly, patient data from patient care and clinical trials or an increasing number of commercial data
sources (for example, there is now a wealth of general consumer data as well as specific health-related data from
wearable devices and fitness apps) are being generated by providers, governments, industry and individuals
themselves. While such datasets can be a rich resource for investigators in those organizations, they are generally
not accessible to the broader research community due to patient privacy concerns. Even when it is possible for a
researcher to gain access to such data, ensuring proper governance and complying with strict legal requirements
is a lengthy and expensive process. This can severely hamper the timeliness of research and, consequently, its
translational benefits to patient care. This delay is particularly devastating now, during the rapidly advancing COVID19 pandemic.
As noted in [1], “sharing data produced from clinical trials...has the potential to advance scientific discovery, improve
clinical care, and increase knowledge gained from data collected in these trials. As such, data sharing has become
an ethical and scientific imperative.” Unfortunately, this ethical mandate may conflict with the equally important
ethical and legal mandate to protect patient privacy, and the reality is that data owners are reluctant to share patient
level medical datasets. And even with these restrictions, “data sharing” as described in the academic literature
generally applies to credentialed academic researchers collaborating with other academic investigators, or lifesciences industries who can provide large research grants to those academics .
As discussed in Section 2, methods of de-identification that are based on [2] (HIPAA’s safe-harbor provisions), first
established by law in 1996, are much less effective now, considering the vulnerability to linkage attacks - combining
“de-identified” data with information available elsewhere, such as social media or public records, to extract highly
sensitive personal information ([3], [4]). At the same time, they may also significantly degrade the utility of the data
[25].

3

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

1.1 De-Identification and Re-Identification
As detailed in [5], the simplest method of privacy protection is to remove all fields (HIPAA prescribed 18 such fields)
that could directly and uniquely identify an individual, such as name, social security number, and phone number.
Until the mid 2000s, this was considered adequate to de-identify data, and pursuant to the HIPAA Privacy Rule 45
CFR 164 such data no longer constitutes protected health information (PHI). The assumption was that once these
18 types of information were removed or masked, and the disclosing entity had no actual knowledge that the
information in the de-identified dataset could be used to identify an individual, the disclosure did not constitute a
significant risk to privacy. However, in recent years, it became clear that many other data fields can be used to
identify individuals [4].
GDPR expands the scope of protected information beyond PHI, instead using the term “personal data,” where
“personal data” means any information relating to an identified or identifiable natural person (“data subject”),
whereas HIPAA is limited to information generated by healthcare providers, insurers and clearinghouses and
pertaining to the medical treatment of patients. An identifiable natural person is one who can be identified, directly
or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an
online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural
or social identity of that natural person.
Moreover, GDPR distinguishes two types of de-identification: pseudonymization and anonymization.
Pseudonymized data may in many cases be similar to de-identified data under HIPAA, but contrary to HIPAA which
explicitly does not apply to de-identified data, GDPR still imposes legal restrictions on pseudonymized data, albeit
less stringent ones than for fully identifiable data. For example, data containing an encrypted patient key could
potentially still be rendered de-identified under HIPAA, provided only the data source, but not the data recipient can
decrypt this key or otherwise use it to recover the patient’s identity, whereas under GDPR such data would only be
pseudonymized, and GDPR would continue to apply. Full anonymization pursuant to GDPR is defined such that
even if the identified source data or any other auxiliary data (whether or not available to the data recipient), could
be used to recover patient identity; it is, however, difficult to achieve and retains very little analytic utility. Thus, in
order to be able to use GDPR protected data, significant legal burdens will typically have to be met, and this can be
especially challenging for entities outside the EU. Importantly, defining “identifiable” data is complicated by the
possibility of re-identification [6]. Re-identification is the matching of anonymized data back to an individual. In recent
years, faith that de-identification prevents re-identification can no longer be supported. For example, a 2009 Social
Security Number study [7] showed that data about an individual's place and date of birth, voter registration, and
other publicly available information can be used to predict their Social Security number.
Traditional techniques for disclosure limitation can be classified in a number of ways, grouped by information limiting
methods and data perturbation methods. Information limiting methods are those that delete, mask, suppress, or
obscure data fields or values in order to prevent re-identification. Data perturbation methods are those that use
statistical means to alter the underlying data itself, by adding noise and/or limit the query results that can be drawn
from the data.

1.2 Synthetic Data
In this paper, we will focus on the use of synthetic medical data, a novel method for data sharing that does not
explicitly limit or perturb data, and instead learns a high granularity statistical representation from the data in order
to generate completely synthetic medical records using sampling and randomization. This in principle can provide
high statistical fidelity and low risk of disclosure.
Given the stakes in healthcare, when claims are made that any particular method renders a dataset non-reidentifiable, there must be a set of metrics to ensure that such a claim is warranted. In general, there is a tradeoff
between the risk of disclosure and the utility of a disclosed dataset - the higher the utility, the lower the level of
privacy guarantee, and vice-versa, as shown in figure 1:

4

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 1: trade-off between utility and privacy
Different disclosure techniques represent different trade-off curves; for example, information-limiting techniques are
often more detrimental to utility than perturbation techniques, whereas synthetic data (if done properly) can provide
the best trade-off.
Therefore, it is essential to develop a set of metrics and associated acceptance criteria that can be used to
determine when synthetic data can be trusted to provide real world evidence. Of course, any real-world dataset
from which an investigator hopes to draw conclusions based on a statistical analysis, is itself a sample of a much
larger universe. In other words, our expectation that the synthetic dataset matches the real dataset is really a proxy
for the expectation that the synthetic dataset and real dataset are both samples from the same underlying
distribution.
As an example, suppose we have a dataset of 1000 patients with coronary heart disease, 500 of whom have had
treatment X and 500 have been treated with a placebo. The outcome is that treatment X is significantly better than
placebo. A clinician or a regulatory body has enough assurance that they are willing to recommend treatment X for
all qualifying patients with heart disease, acknowledging that those 1000 patients are a representative sample of
say 19,000,000 Americans with coronary heart disease. Taken together, the fidelity metrics suggested in this paper
provide a high level of assurance that the real dataset and synthetic dataset are drawn from the same distribution.
In fact, we may find that synthetic data is actually a better representation of the universe ground truth. That is
because creation of the synthetic dataset may in some cases remove out of distribution anomalies. The synthetic
data engine could have an error filtering function that is not present when the real dataset is used directly in
statistical analyses.
We present here a collection of metrics and visualizations to assess the privacy and fidelity claims of any method
for synthetic data generation, and the method of synthetic medical data generation in particular. In section 2 we
provide a brief overview of synthetic data generation in healthcare. In section 3 and 4 we describe our proposed
framework for evaluating statistical fidelity of synthetic data against the real data and measuring privacy and
disclosure risk. In section 5 we present experimental results with four datasets, using Syntegra’s generative engine,
in section 6 we analyze our results, and in section 7 we present our conclusions.

2. Synthetic Data in Medicine
Synthetic data has been used in a number of applications, such as computer vision and robotics [8], and creating
synthetic controls for economics and social sciences [9], where randomized trials are not possible, but a comparison
to an intervention is required. The privacy preserving properties of synthetic data has been initially discussed in
[10], and much later in [11] and [21].
For reducing the risk of patient data re-identification and accelerating the process by which such data is made more
widely available, synthetically generated data is a promising alternative or addition to standard anonymization
procedures. Some mechanisms of creating synthetic data do so based on simulating disease processes, known
patterns of care, or disease-specific guidelines (notably [12] and [13]), which means starting with heuristics about
medical conditions, such as practice guidelines or literature review, and letting rules drive new record production.
These methods typically fail to anticipate unusual edge cases, such as surgical complications, other adverse events
or rare conditions, which means that they fail to satisfy the needs of modern precision medicine. For regulatory
approval or post-approval vigilance of therapeutics that require real world data, and where synthetic data will
5

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

substitute for the actual data because of privacy concerns, the synthetic data should be created from the data itself
[14], i.e., data-driven synthesis as opposed to process- or rule-based simulation. Indeed, outside of pedagogical or
population-based modeling, synthetic data has seen limited use in mission-critical applications such as synthetic
control groups in clinical trials to support regulatory approval or post-approval surveillance.

2.1 The Syntegra Synthetic Data Engine and Medical Mind
At Syntegra, we have developed a novel machine-learning-based synthetic data generator, the Syntegra Medical
Mind, that can convert any type of clinical data into an equivalent synthetic version. The patient-level data can be
from any real-world database (RWD), or from observational or prospective clinical trials. The synthetic dataset is
intended to match the statistical properties of the original, while providing strong privacy protection, as validated by
the metrics defined below. Note that rather than providing only aggregate statistics, the synthetic data is created at
the level of individual participants. The goal is a dataset that mimics the statistical properties of the real data and
can be used for any analysis, including training-state-of-the-art, non-linear predictive models, with performance that
matches that of models trained on the original data.
Unlike many previous synthetic data techniques that add noise or attempt to simulate results based on hand-coded
rules or guidelines, our method is data-centric: it uses a very large neural network to learn, in an unsupervised
fashion, the underlying probability distribution in the real data. A synthetic dataset is then generated that accurately
maintains the statistical properties of the real data, while preserving privacy. The engine works by viewing all the
data points for a given participant as a "patient sentence", with events in time or tabular data, and learning the
underlying latent probability distribution by training a language models1 on those “sentences''; subsequently, the
trained model can be used to generate synthetic sentences by sampling from the learned distribution (which is
encoded by the neural network). By repeating that process any number of times, we generate a set of patient-level
clinical records. Utilizing transfer learning, our generative model leverages a pre-trained corpus that is fine-tuned,
in an unsupervised fashion, by the real dataset that one wants converted into a synthetic equivalent. The pre-trained
corpus contains a representation of medical patterns extracted from general and health-care related data,
augmented by new datasets as they are encountered. That is, the Syntegra Medical Mind improves over time as
each new set of real data enriches the pre-trained corpus.

3. Statistical Fidelity Validation
Given a real medical dataset A, and a synthetic dataset B generated based on A, our goal is to measure the
statistical fidelity between A and B. But what do we mean by the term “statistical fidelity”?
Here, we explore six methods to compare the degree to which the synthetic dataset is an accurate replica of the
original dataset:
1) Visualize the real and synthetic datasets using dimensionality reduction
2) Compare summary statistics
3) Compare single variable distributions
4) Compare pairwise correlations
5) Multivariate and non-linear metrics
6) Clinical consistency check on the synthetic data
As demonstrated by the famous Anscombe’s quartet [15] example - statistical metrics have known limitations, which
proper visualization can sometimes help address. Thus, in our framework for statistical fidelity we use a combination
of visualization and metrics, recognizing that good metrics provide an easy numerical value by which to judge fidelity
comparing real to synthetic data, whereas visualization provides a more nuanced view of differences.

3.1 Visualize and Compare Datasets
Before diving into numerical analysis, it is accepted data science methodology first to visualize datasets, especially
where one wishes to compare two datasets, and where the number of records is large. Graphical visualization
makes use of human’s innate ability to recognize patterns, concordance, and deviation. However, given that a
medical dataset can be of very high dimension, with heterogeneous data types, it can be a challenge to create a
visualization that is coherent to a human observer. Recently, robust dimensionality reduction methods such as
tSNE and UMAP [22] were introduced. UMAP seeks to learn the manifold structure of a dataset and find a low
dimensional embedding that preserves the essential topological structure of that manifold.

1

https://openai.com/blog/better-language-models/

6

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 2a: UMAP visualization of a clinical trial dataset. On left, the real dataset; on right, the synthetic dataset.
In Figure 2a, each point represents a dimensionally reduced single patient record from a high-dimensional medical
dataset. The axes themselves have no convenient interpretation, but the clustering present in the data is clear in
the visualization. Figure 2a shows the full UMAP scatter plot of the whole datasets, whereas figure 2b zooms in on
a specific small cluster of points in more detail.

Figure 2b: UMAP visualization of a clinical trial dataset - small cluster of patient records (zoomed in) - real data on
the left, synthetic data on the right
Undoubtedly, some records are close to others, hence clustering on multiple size scales is noted. Some records
are “edge cases” and represent small groups far from the centroid (red circles). Note that the coverage of the
synthetic dataset (figure 2a - right) is quite an accurate representation of the distribution of the real data (figure 2a
- left), including covering the edge cases. This is important, as simple numerical analysis may show that the
synthetic and real data have similar central tendency, which might miss a failure of the synthetic dataset to replicate
the small cohorts or edge cases present in the real data. The distribution of real and synthetic points is ideally close
but should not be identical. That is, the synthetic data should not be just a copy of the real. Larger magnification
reveals that they are not identical, and our privacy metrics (see section 4) prove that copying has not taken place.
By using UMAP, we can clearly see that even small cohorts are picked up in the synthetic data. Another reason
small cohort coverage is important is that standard de-identification methods, such as for HIPAA compliant deidentification, often require removal of small cohorts out of concern for re-identification via membership inference.
So, a truly robust synthetic data engine will maintain these small cohorts, while still maintaining privacy.

3.2 Population Statistics
The next step in validation that should be performed when presented with any new dataset is summary statistics or
population statistics [16]. The difference in our case is that these summary statistics will be computed in a paired
fashion on the real and then on the synthetic data, and then compared. Table 1a provides an example for numeric
variables, where we compare the mean and standard deviation for real vs synthetic, whereas table 1b provides an
example for categorical variables, where we compare the count and percentage in each category:

7

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Numeric variables
variable

Real
mean

Categorical variables
Synthetic

SD

mean

SD

AGE

55.4

8.8

55.3

8.9

BMI

24.3

4.9

24.5

5.0

HEIGHT

170.1

22.1

171.2

21.9

WEIGHT

82.9

23.5

83.2

23.5

Table 1a

variable

Value

SEX

Female

1,519

22.3

1634

24.0

Male

5,281

77.7

5166

76.0

991

14.6

1073

15.8

5809

85.4

5727

84.2

Real
n

RACE

NonWhite
White

Synthetic
%

n

%

Table 1b

The goal of this exercise is to understand whether population-level statistics on the real data match those of the
synthetic data.

3.3 Single Variable (Marginal) Distributions
Moving from variable-level statistics to distributions we compare the distribution of a given variable in the real data
to its distribution in the synthetic data. For numeric variables, the distributions can be easily compared using a
histogram:

Figures 3a and 3b: Histogram comparing similar (left) and differing (right) distributions of numeric variables
The (non-parametric) 2-sided KS-statistic2 can be used as a statistical test to determine whether the two variables
(real vs. synthetic) are drawn from the same distribution. The KS-statistic is a value between 0 and 1; when this
value is small (or the associated p-value is high), then we cannot reject the hypothesis that the distribution of the
real variable is the same as that of the synthetic variable, which means there is high statistical fidelity of synthetic
data when compared to real data.
For the example in figure 3a above, the KS-statistic value is 0.0126 and the p-value is 0.405, clearly consistent with
a good fit. Figure 3b demonstrates the opposite case where the two distributions don’t match -In this case the KSstatistic value is 0.392 and the p-value<0.0001, again consistent with our expectations of a poor fit.
For categorical variables, the category-based histogram is a useful visualization tool to understand differences in
distribution:

Figures 4a and 4b: Histogram comparing similar (left) and differing (right) distributions of categorical variables
The Kullback–Leibler (KL) divergence is a measure of how one probability distribution diverges from a second
expected probability distribution, and it is often used to compare two categorical distributions. The closer the KL
divergence is to 0, the more similar the distributions. In figure 4a, the KL divergence is 0.0062, suggesting a close

2

Note that other statistical metrics, such as chi-squared test for independence, are also acceptable

8

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

match between the distributions. In the distribution shown in Figure 4b, the KL divergence is 0.2645, consistent
with a mismatch between the distributions.
Another interesting metric for comparing individual categorical variables is support coverage, as defined in [17],
which measures how much of the variable’s support (number of unique categorical values or levels) in the real data
is covered in the synthetic data. Concretely, support coverage is defined as:
$

|𝑆 # |
1
𝑆(𝑋! , 𝑋" ) = ) $
|𝑅 |
𝑉
#%&

Where 𝑆 # represents the cardinality of synthetic variable v, and 𝑅# represents the cardinality of real variable v, and
V is the set of all categorical variables. A high support coverage value reflects better coverage of categorical levels
in the synthetic data and thus higher fidelity.
Yet another option here is wasserstein distance3 (also known as “earth-mover’s” distance) which is often used
instead of KL-divergence.

3.4 Pairwise Correlation
Univariate metrics do not describe interactions between variables. It is useful therefore to measure the pairwise
correlation4 between some or all pairs of variables in the dataset; if the pairwise correlation of the synthetic data is
similar to that in the real data, then statistical fidelity is maintained.

Figure 5a: Pairwise correlation heatmap - high fidelity
Figure 5a compares the correlation heatmap generated with the real data (left) against the same heatmap generated
with the synthetic data (right) and is helpful in understanding the degree to which pairwise correlations between
each pair of variables are maintained. The variables included in the heatmap are a choice of the evaluator. If the
number of variables is small, all variables can be included. If the number of variables exceeds what can be
understandably displayed, one can include the N most common variables in the dataset, variables of clinical
importance, or some other criteria. It can be of interest to compare relatively uncommon variables in the heatmap
to see how well the synthetic engine handles rare conditions or edge cases present in the real dataset.

Figure 5b: Pairwise correlation heatmap - low fidelity
3
4

https://en.wikipedia.org/wiki/Wasserstein_metric
Herein we use Pearson correlation, but other forms of correlation (e.g. Spearman or Kendall) are equally valuable

9

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 5b demonstrates low fidelity between the real and synthetic data. Pairwise correlation can be measured
quantitatively with the pairwise correlation difference (PCD), computed as the L1 or L2 norm of the difference
between the correlation matrices:
𝑃𝐶𝐷'& = /|𝐶𝑜𝑟𝑟(𝑟𝑒𝑎𝑙) − 𝐶𝑜𝑟𝑟(𝑠𝑦𝑛)|/&
𝑃𝐶𝐷'( = /|𝐶𝑜𝑟𝑟(𝑟𝑒𝑎𝑙) − 𝐶𝑜𝑟𝑟(𝑠𝑦𝑛)|/(
Where Corr(real) represents the correlation matrix for the real data, and Corr(syn) is the same for synthetic data.
PCD of 0 or close to 0 coincide with the synthetic data being closer to the real data, and higher values mean less
statistical fidelity. The highest value of PCD is 1. For example, in figure 5a, the PCD-L1 is 0.007 whereas for figure
5b, the PCD-L1 is 0.17, consistent with our expectations.

3.5 Multivariate Metrics
Although relatively simple to visualize and understand, both single variable and pairwise fidelity metrics lack the
ability to evaluate the statistical fidelity from a perspective that takes into account all the variables and their granular
linear and non-linear interactions. To address this, we propose two multivariate metrics: (1) predictive model
performance (2) discriminator AUC / pMSE

3.5.1 Predictive Model Performance
An effective form of fidelity validation works as follows: train two instances of a machine learning model (using a
commonly accepted algorithm such as a linear regression, random forest, gradient boosted trees, or a deep neural
network) - one trained with real data, and the other trained with synthetic data; then compare the predictive
performance of the models on a real-data validation set. Using a modern predictive modeling algorithm, we can
gain insight into data fidelity at the multivariate level since the models exploit non-linearities and multivariate
correlations in the predictive variables.
If the target variable for the predictive model is a binary variable (classification), a common and widely used metric
for measuring the performance of the predictive models is the area-under-the-curve of the receiver-operatorcharacteristic (ROC-AUC)5. Additional metrics such as accuracy, precision, recall and F1 are also informative, but
to avoid too many metrics we recommend choosing only the relevant ones for the use case at hand. The ratio of
the synthetic ROC-AUC and the real ROC-AUC becomes the quality metric for statistical fidelity, with higher values
representing better statistical fidelity. The higher this ratio, the better the fidelity of synthetic data with the real.
If the target variable for the predictive model is a continuous variable, common and widely used metrics of
performance are RMSE (root-mean-squared-error) and MAE (mean-absolute-error).
For predictive models with binary targets, we can visualize the models’ performance through the ROC, and calculate
the ratio in ROC-AUC between the model based on real data vs. the model based on synthetic data. With high
fidelity synthetic data, we expect the ROC and corresponding AUC of the synthetic data to be close to each other.

Figures 6a and 6b: ROC AUC metrics comparing two predictive models with high (left) and low (right) fidelity

5
https://en.wikipedia.org/wiki/Receiver_operating_characteristic
Note that ROC-AUC is equivalent to another statistics commonly used to summarize the ROC curve - the C-statistic.

10

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

In figure 6a we see an example of two predictive models (generated using gradient boosted trees) where the ROC
curves of the two models are nearly identical, and the AUC metrics are within a margin of error. This reflects very
good fidelity of the synthetic data with the real data. In figure 6b we see the opposite case where the fidelity between
the two datasets is low, resulting in the ROC curve of synthetic data closer to the red line (representing random
decision).
When using predictive model performance to compare synthetic to real datasets, it is important to ensure proper
data science hygiene principles:
● Use models that provide the best predictive performance. For example, in many real-world scenarios linear
models underperform more advanced modeling techniques such as gradient boosted trees or random
forest.
● Always set aside a separate validation set from the real dataset and evaluate performance of the predictive
models on that validation set for both real and synthetic cases.
● Use hyperparameter tuning to ensure the compared models are fully optimized.
● Run the predictive model multiple times with different randomizations to ensure the outcome is not due to
chance.
In practical use, the predicted variable would be one of clinical significance to the dataset being considered. For
example, for general hospitalized patients, length-of-stay and readmission risk might be relevant, whereas for
patients with severe COVID-19, the outcomes might be risk of admission to the ICU, risk of intubation, length of
time on a ventilator, and risk of death. It is also important that there are no variables used in the prediction that leak,
or anticipate, the predicted variable. For example, the lab test “type and cross for blood transfusion” would be
inappropriate to use as a predictor of “likelihood of surgery in the next week.”
Model performance analysis pertains to the synthetic data engine and says nothing about how useful or accurate
the predictions. For example, there may be independent variables missing from the original dataset that materially
impact the outcome. In predicting death due to COVID-19, unless every patient's resuscitation status is included in
the input data, a predictor of death will severely underperform. Patients with “do not resuscitate" orders are much
more likely to die then a patient with all other features matched, but without such an order. If a “do not resuscitate”
order is absent from the features of the predictive model, the model will fail. The synthetic engine will generate a
dataset equivalent to the real dataset, and the output (ROC curve) using the real and synthetic data will match, but
the generative model has no way to know that a critical variable was not included.
When comparing predictive models trained on real vs synthetic data, it is useful to evaluate the “feature importance”
from these models and compare the most important features between the real and synthetic datasets. Predictive
models are increasingly coming under scrutiny, requiring interpretation of the model output to open up the blackbox risk in such models. It follows that if a synthetic data set is an accurate representation of a real data set, the
interpretation of a predictive model trained using real and using synthetic data should be very close. A leading
method is Shapley values (SHAP6) [24], which we use in our experiments and evaluation. Aside from the
visualization of SHAP values, we use nDCG7 (normalized discounted cumulative gain) to compare the ranking of
feature importances and summarize the difference in a single metric; we don’t expect feature rankings between real
and synthetic to match exactly as small changes in training algorithms can result in minor changes in feature
importance, but we do expect them to be very similar, with an nDCG value close to 1.0.

3.5.2 Survival Analysis
Survival analysis provides a statistical framework to analyze time-to-event outcomes. To understand whether the
characteristics of time-to-event are maintained in the synthetic data, we perform Kaplan-Meier8 analysis on the real
data and then compare analysis on the synthetic data when there is a temporal aspect of the real data. In addition
to the visualization of Kaplan-Meier curves, we use the p-value from the analysis as a metric to compare real data
to synthetic data.

3.5.3 Discriminator AUC
Inspired by generative adversarial networks9, another metric for multivariate fidelity is the “discriminator AUC”.
Specifically, we build a classification model trained to discriminate between the real data records and the synthetic
ones. Using ROC-AUC as a measure of performance for this discriminator model, a synthetic dataset with high
6

An alternative is to use LIME - https://homes.cs.washington.edu/~marcotcr/blog/lime/
https://en.wikipedia.org/wiki/Discounted_cumulative_gain
8
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3059453/
9
https://arxiv.org/abs/1406.2661
7

11

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

statistical fidelity to the real dataset will result in a ROC-AUC value for this discriminator model that is close to 0.5
(representing random decision classifier), whereas low fidelity synthetic data are reflected with ROC-AUC values
closer to 1. As described above with predictive models, it’s important to use a strong modeling technique and run
the discriminative model multiple times with different randomizations to ensure the outcome is not due to chance.
The propensity-score mean squared error (pMSE) is a variant of ROC-AUC described in [18]. Similar to
discriminator AUC, we build a discriminator model, and estimate the propensity of each record to be real or
synthetic. The metric is then defined as the mean-squared-error between the propensity scores and the actual:
*

𝑝𝑀𝑆𝐸 = )(𝑝) − 𝑐)
)%&

Where pi represents the propensity score for record i, and c is the proportion of synthetic records in the training set
for the discriminator. We expect low pMSE (close to 0) for a synthetic dataset with high statistical fidelity to the real.

3.6 Clinical Consistency Assessment
In synthetic medical data, a final metric of fidelity for synthetic data is based on domain expertise that validate the
data quality. These rules are generated by clinicians or scientists to identify inconsistencies in the data that
represent failed synthetic data generation, for example:
1.
2.
3.
4.

Patient is male and pregnant
Patient is female and has been diagnosed with prostate cancer
Patient is age 0-3, with adult demographics such as weight > 100 lbs, height > 5 feet, etc.
Patient is listed as dead, at time T but has clinical events at time T* > T

Given a library of rules, we calculate the number of records with inconsistencies as a percentage of the overall
number of records.

4. Privacy Validation
One of the primary goals of using synthetic data is to prevent disclosure of private patient information.
We evaluate synthetic data privacy as follows:
● Disclosure metrics: understanding how much disclosure risk may result from access to the synthetic
dataset.
● Copy protection metrics: demonstrating that records from the real dataset are not “copied over” to the
synthetic dataset.
We assume that direct identifiers are always removed from the real data before it is being used to generate synthetic
data. This is often already the case for the source data, with commonly used de-identification software and/or
service can be applied to achieve this initial state.

4.1 Disclosure Metrics
There are two forms of disclosure used in classical statistical disclosure practice (not related specifically to synthetic
data): identity disclosure and information disclosure. Identity disclosure is the discovery of the identity of the subject
of a disclosed record, whereas information disclosure is the discovery of (additional) information about a known
subject. Identity disclosure usually leads to information disclosure (once the subject of a record is identified, the
record provides additional, potentially sensitive information about that individual, such as medical or psychiatric
conditions, HIV status, etc.), so classical disclosure limitation aims to prevent identity disclosure, and the HIPAA
Privacy Rule is explicitly framed in this way.
We do note that information disclosure can happen without identity disclosure, in real datasets. For example, if a
de-identified dataset contains a 3-digit ZIP code and the age and gender of patients (something generally permitted
under HIPAA) and it is known from general census data that there are five individuals of a given age and gender
in a certain 3-digit ZIP code, and the de-identified data contains five distinct individuals all of whom have diabetes,
then the user of the de-identified data can deduce that the five individuals in that age/gender/ZIP code bucket whose
identities may be found in voter registries, consumer data etc., are all diabetics, even if it is still not known how to
match the de-identified records to the known identities. For handling real data, whether or not such disclosures are
acceptable needs some careful consideration. In this instance it may be undesirable, but in other cases such

12

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

disclosure may only include general statistical information that can be disclosed. It is often challenging to carry out
a systematic analysis of such disclosure, and this is another advantage of synthetic data.
For synthetic data, the concept of identity disclosure is somewhat ill-defined. In any meaningful application of data
synthesis, where records are generated from a distribution, there is by construction no connection between a given
synthetic record and any unique individual.
What is true for synthetic datasets as well as for classically de-identified datasets is that there can be no prohibited
disclosure of identity or information about an individual, if no data associated with this individual is used in the
construction of the data. Thus, a de-identified dataset will not violate the privacy of a patient not represented in the
data, and a synthetic dataset will not do so if no information associated with the individual was used to train the
synthetic algorithm. The only information about an individual that could be leaked from a synthetic dataset created
without the use of that patient’s true data is general statistical information that we do not aim to conceal. For
example, if a synthetic dataset reveals that all patients who have an appendectomy performed have been diagnosed
with appendicitis, and the user of that synthetic dataset concludes that their work colleague who mentioned having
had their appendix removed must have had such a diagnosis, this would not be an illegitimate information
disclosure.
Thus, to understand disclosure risk for synthetic data we focus on membership inference and attribute inference
attacks. In a membership inference attack the adversary aims to identify the participation of a known patient record
in the training of the synthetic generation algorithm, and in an attribute inference attack, the adversary attempts to
infer values of certain attributes from the synthetic dataset.

4.1.1 Record Distance Metric
For many of the privacy tests we need a reasonable measure of distance between each pair of records. Some
common distance functions are Euclidean distance, cosine distance, or Hamming distance; many others exist - see
e.g. [17]). With healthcare data it is important to provide a true distance metric10 that appropriately deals with both
numeric and categorical values, as well as missing values.
For example, a possible metric can be defined as follows:
1. For categorical features, we define the distance as:
3,,-. 0 % 2
𝑑𝑖𝑠𝑡(𝑥, 𝑦) = { &,,-.
012
2.

For numerical features, we bucket the feature values into percentile bins, and calculate the distance the
same way as for categoricals.

We can then calculate the distance between a synthetic row 𝑠 and a real row 𝑟 as the Hamming distance between
the two.

4.1.2 Membership Inference Test
Membership inference attacks seek to infer membership of a patient record in the real dataset from which the
synthetic data was generated. For example, if the training set used to train a generative model consists of HIV
positive patient records (e.g., the HIV status is not included as a field, but it is a clinical study of HIV patients), then
inferring whether a patient record was included in that training will reveal that that patient is HIV positive.
We now describe a validation test for membership inference on synthetic data. Following [21] let A be a (large)
training set of real patient data, and 𝑟 ∈ 𝐴 a patient record. For simplicity of presentation, we assume that A contains
one unique record per patient, though this limitation can be easily removed. We split the training data randomly
into two disjoint subsets of equal size, 𝐴 = 𝐴& ∪ 𝐴( . We train the generative model on 𝐴& and generate a synthetic
data set 𝐵& . We utilize a reasonable measure of distance between the source patient record r and any synthetic
record s, such as the one described in 4.1.1.
Even though there is no well-defined concept of the “identity of a synthetic record”, we can design a well-defined
test for “membership inference”. Concretely, we consider a hypothetical adversary which has access to a subset of
records in A which we denote 𝐴4 (note that records in 𝐴4 may belong to either 𝐴& and 𝐴( ), and that attempts a
membership inference “attack” as follows:
10

A true distance metric is symmetric and satisfies the triangle inequality, with each item having distance 0 to itself.

13

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

●
●

Given a patient record 𝑟 ∈ 𝐴4 , and the disclosed synthetic dataset 𝐵& , the adversary identifies the closest
record 𝑠 ∈ 𝐵& with distance 𝑑(𝑟, 𝑠 ).
The adversary determines that 𝑟 is part of the training set of 𝐵& if 𝑑(𝑟, 𝑠) is lower than some threshold (for
example, if we use Hamming distance, a reasonable threshold value might be 2, 3 or 5)

We want to evaluate the success rate of such an adversarial strategy. For each record 𝑟 ∈ 𝐴4 we know if it actually
belongs to 𝐴& or 𝐴( , and can then determine whether the adversary’s decision constitutes a true positive, true
negative, false positive or false negative, and measure the confusion matrix. We then compute precision11 and
recall and plot them as a function of the % of records in A that are present in 𝐴4 .
Figure 7a describes in a visual form the approach for the membership inference attack:

Figure 7a: membership inference diagram
Precision represents the number of correct decisions the adversary has made; since we randomly split A into 𝐴&
and 𝐴( with equal sizes, the baseline precision is 0.5 (corresponding to random choice), and any value above that
reflects increasing levels of disclosure risk (or increased risk of success for the membership inference attack).
Recall (aka sensitivity) represents the percent of records known to an attacker that can be found in the training set.
Clearly as the Hamming distance threshold increases (and recall increases), the attacker identifies more and more
such records as belonging to the training set (although that doesn’t mean such identification is correct as reflected
in the precision).
This is demonstrated in figure 7b below:

Figure 7b: membership inference metrics
11

https://en.wikipedia.org/wiki/Precision_and_recall

14

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

As can be seen here, the success rate of this inference membership attack is very close to the baseline of 0.5 and
thus represents minimal risk of disclosure, regardless of the recall /Hamming distance value chosen. From the
example in Figure 11b, it is clear that no matter the threshold chosen by the adversary (Hamming distance threshold
chosen, reflected in the recall), the likelihood of making a correct membership inference metric is very close to
“random guess”.
As a summary metric, we calculate a membership inference score as the maximum difference between the actual
precision to the baseline precision (0.5) over all Hamming distance values included in the test. Good privacy is
achieved when this score is low and close to 0.

4.1.3 File Membership Hypothesis Test
In a more complex variant of file membership inference, we perform a random membership inference test N times,
and use hypothesis testing to gain insight into possible success of a membership inference attack. In a similar
manner to 4.1.2, we split the training data randomly into two disjoint subsets of equal size, 𝐴 = 𝐴& ∪ 𝐴( . We then
train the generative model on 𝐴& to generate a synthetic data set 𝐵& , and train the generative model on 𝐴( to
generate a synthetic data set 𝐵( . For any patient record 𝑟 ∈ 𝐴, we find the closest records 𝑠& ∈ 𝐵& and 𝑠( ∈ 𝐵( . If
𝑑(𝑟, 𝑠& ) < 𝑑(𝑟, 𝑠( ) we say r is in B1, and otherwise r is in B2. We deem this determination to be correct if 𝑟 ∈ 𝐴& and
false otherwise.
Running this process N times, each time performing the randomization using a different random seed, we can
perform hypothesis testing. Let P be the probability that our test is “correct”, or experimentally the rate of “correct”
answers over a large sample. We then make the NULL-hypothesis H0: 𝑃 ≠ 1/2. Using standard Central Limit
Theorem methods we can then aim to reject H0. If we succeed in doing so, this is a powerful indication of privacy.
It states that we are unable to guess with any greater accuracy than a coin toss whether a patient’s records were
even present in the data used to train a model. This precludes any identity or information disclosure.

4.1.4 Attribute Inference Test
Following [21] an attacker is attempting to infer the value of an unknown attribute about a patient from the synthetic
data. Assuming the attacker possesses a subset P of attributes for some real records A’ (also called quasi-identifiers
such as age, gender, race), she attempts to infer the value of the missing attribute by identifying a set of rows in
the synthetic dataset that match those quasi-identifiers and predicting the value of the attribute using the synthetic
matched rows (this could be using an ML model or simpler techniques like majority voting or mean/median).
We measure the success of this attack methodology using F1 score12 for categorical variables and using root-meansquared-error for numeric variables. Table 2 demonstrates an example where the attacker uses age, sex and race
as quasi-identifier, with “HIV status” as the attribute value under attack:
Age
0-25
50-75
50-75

Sex
male
female
female

Race
asian
white
black

F1
0.375
0.1143
0.4432

Table 2: Example attribute inference results for HIV status
A high F1 score (close to 1.0) reflects successful attribute inference, whereas a lower F1 score reflects the failure
of such attack. For RMSE a low value reflects successful attribute inference and high values reflect failure.

4.1.5 TCAP
The concept of targeted correct attribution probability (TCAP) has been defined in [19] as an appropriate measure
of disclosure risk for synthetic datasets, and is calculated as follows:
Consider a real dataset A and its synthetic equivalent B, and an attacker with access to an additional real dataset
C. Let 𝐾5 and 𝑇5 be the vectors for key variables and target variables in the original dataset A, and 𝐾6 /𝑇6 , 𝐾7 /𝑇7 be
those vectors for the synthetic dataset B and auxiliary dataset C used in the attack.

12

https://en.wikipedia.org/wiki/F1_score

15

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

The TCAP score for a record j in the original dataset A is thus:
∑9)%&N𝑇5,) = 𝑇6,8 , 𝐾7,) = 𝐾6,8 O
𝑇𝐶𝐴𝑃5,8 =
∑9)%&N𝐾7,) = 𝐾6,8 O
Where the square brackets are Iverson brackets13. If the TCAP score is 0, then the synthetic dataset carries no risk
of disclosure, whereas a score closer to 1 disclosure is possible.
TCAP of course depends on the size of the key and the types of variables included in the key. In most datasets a
reasonable attack key can be determined based on assessment of which variables might be available in C. In the
case where no specific key is available, we propose calculating TCAP for a reasonable subset of possible key
combinations of a given key size (e.g., M = 2, 3, 4, 5), and reporting the minimum, average and maximum of the
TCAP.
We find membership inference and attribute inference tests to be superior to TCAP but describe it above for
completeness.

4.2 Copy Protection Metrics
An important question when looking at high fidelity synthetic data that is derived from real data is “how do I know
that the synthetic data is not a simple copy or minor perturbation of the original real data”, resulting in high risk of
disclosure?
To address this issue, we propose two methods:
● Distance-to-closest-record (or DCR)
● Exposure to unintended memorization

4.2.1 Distance to Closest Record - DCR
For any given synthetic record 𝑠 ∈ 𝐵 we define DCR(s) as the normalized distance between s and that record in the
real dataset 𝑟 ∈ 𝐴 that is closest to it. Assuming an appropriate distance metric d(x,y) as discussed in 4.1.1, to
compute the DCR for a given row 𝑠 in the synthetic dataset we take the minimal distance to all candidate rows in
the real dataset 𝑟8 and obtain that row’s DCR value.
𝐷𝐶𝑅(𝑠) = 𝑚𝑖𝑛8 𝑑𝑖𝑠𝑡(𝑠, 𝑟8 )
The intuition here is that DCR provides us with an understanding of the distance between records in the synthetic
and real datasets, and any simple perturbation of the real data will be easily exposed by the DCR metric.
Figure 8a demonstrates a privacy-preserving synthetic dataset where DCR values are “far” from 0, whereas figure
8b demonstrates a situation where some synthetic records exactly match the real records, while some other do not,
which represents a high risk of unintended copying of at least some of the records (those with DCR=0).

Figures 8a and 8b: DCR distribution example: privacy preserving synthetic dataset(left) and synthetic data
containing many copies of the original (right).
For privacy-preserving synthetic data we expect DCR between the synthetic dataset and the real dataset (syntheticreal) to be roughly the same distribution as the DCR between records in the real dataset and itself (real-real), and
generally to see the lowest DCR values to be far from 0, as depicted in figure 8a.
13

https://en.wikipedia.org/wiki/Iverson_bracket

16

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

4.2.2 Exposure
Our second copy protection methodology is specific to synthetic data engines that can associate a probability to
any given sample or record 𝑃(𝑠𝑎𝑚𝑝𝑙𝑒), representing how likely that sample is to occur in the synthetic dataset; as
has been recently demonstrated in [20] and [26], this is useful in order to assess whether or not a synthetic data
generator might be unintentionally memorizing the input data.
Following [20] we propose to measure the extent of unintended memorization by injecting “canary samples” (values
far outside the distribution of real data) into the training set and measuring log-perplexity (defined as
𝑙𝑜𝑔(𝑃(𝑠𝑎𝑚𝑝𝑙𝑒))) and exposure in the synthetic dataset. Specifically, for a given synthetic dataset generated using
a synthetic engine, we define for a given canary r injected into the training data the exposure as:
𝑒𝑥𝑝𝑜𝑠𝑢𝑟𝑒(𝑟) = 𝑙𝑜𝑔( (|𝑅|) − 𝑙𝑜𝑔( (𝑟𝑎𝑛𝑘(𝑟))
Where rank(r) is the rank of the chosen canary amongst all possible canaries and R is the space of all possible
canaries. Exposure is a real value ranging between 0 and 𝑙𝑜𝑔( (|𝑅|); its maximum can be achieved only by the
most-likely, top-ranked canary; conversely, its minimum of 0 is the least likely; across possibly inserted canaries,
the median exposure is 1.
Thus, our proposed metric works as follows:
● Insert N randomly chosen canaries into the training set A
● Generate a synthetic dataset B from A
● Calculate the average exposure over the N canaries
By estimating the size of R, we can calculate the “extraction threshold” as 𝑙𝑜𝑔( (|𝑅|) and measure the exposure
level against that threshold. If the actual exposure is lower than the threshold, we determine that the risk of
unintended memorization is low, whereas if the exposure is higher than the threshold then the risk is high.
Note that this method requires access to the synthetic data generation engine, as the canaries need to be included
in the training dataset.

5. Experimental Results
We now evaluate our fidelity and safety metrics with four datasets, which we abbreviate as: DIG, NIS, TEXAS, and
BREAST. For each dataset, we generate synthetic data using Syntegra’s engine, and compute the fidelity and
privacy validation metrics. Note that for some metrics, like ML modeling ROC-AUC or discriminator AUC we actually
ran this evaluation 5 times, each time generating synthetic data using a different randomization seed, and measured
the metrics in each case, using mean and std-deviation to report the average and standard-deviation of each metric.

5.1 Datasets
The Digitalis Investigation Group (DIG) study was a randomized, double-blind, multicenter trial with more than 300
centers in the United States and Canada. The purpose of the trial was to examine the safety and efficacy of digoxin
in treating patients with congestive heart failure (CHF), in sinus rhythm, with an ejection fraction ≤ 0.45. Endpoints
were mortality from any cause (the primary endpoint) and on hospitalization for heart failure (the secondary end
point) over a three-to-five-year period. We downloaded openly available, participant-level data from the NIH/NHLBI
BioLINCC data repository. This dataset includes 6,800 patient records and 71 variables, and we generated a
dataset of 20,000 synthetic patient records.
The National Inpatient Sample database14 (NIS) is a patient-level administrative claims database that represents
approximately 20% of discharges from US community hospitals. The NIS includes data on patient demographics,
primary and secondary diagnoses, medical comorbidities, surgical procedures, length of stay, and discharge
disposition. We utilized a version of this dataset with 116,009 patient records and 31 variables and generated a
dataset with 200,000 synthetic patient records.
The TEXAS Hospital Discharge dataset is a large public use data file provided by the Texas Department of State
Health Services. The dataset we use consists of 50,000 records uniformly sampled from a pre-processed data file
14

https://www.hcup-us.ahrq.gov/nisoverview.jsp

17

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

that contains records from 2013, similar to how it’s used in [23], retaining the same 18 data attributes (of which 11
are categorical and 7 continuous attributes). We generated a dataset with 100,000 synthetic patient records.
The National Cancer Institute Surveillance, Epidemiology, and End Results Program (SEER) collects cancer
incidence data from population-based cancer registries covering approximately 35 percent of the U.S. population.
The SEER registries collect data on patient demographics, primary tumor site, tumor morphology, stage at
diagnosis, and first course of treatment, and they follow up with patients for vital status. We used the portion of the
SEER dataset comprised of breast cancer patient records, taken from the SEER Incidence database15, and which
consists of 1,072,173 patient records with 117 variables including demographics, cancer diagnosis and
classification, and other related fields. We generated a dataset with 1,500,000 synthetic patient records.
Table 3 below summarizes the characteristics of all the datasets included in our study:
Dataset
DIG
NIS
TEXAS
BREAST

Rows in original data
Columns
6,800
71
116,009
31
50,000
18
1,072,173
117

Rows generated
20,000
200,000
100,000
1,500,000

Table 3: dataset characteristics summary

5.2 Results
In this section we describe our experimental results. In most cases we compare the original dataset to the full
synthetic dataset (which is often generated with a larger number of records), except when specifically mentioned.

5.2.1 Statistical Fidelity
First, we look at population statistics for each dataset, comparing real to synthetic for selected variables in each
dataset. We randomly sample the synthetic dataset to the same size as the original. The results are demonstrated
in tables 4a-4d below:
Numeric variables
variable

Real
mean
SD
63.5
10.9
27.1
5.2
78.8
12.7
28.5
8.8

AGE
BMI
HEART_RATE
EJF_PER

Categorical variables

Synthetic
mean
SD
63.5
10.7
27.2
5.1
78.8
12.1
29.1
9.0

variable

Val

SEX

Real

Female
Male
NonWhite
White
Class I
Class II
Class III
Class IV

RACE
FUNCTCLS

n
1,519
5,281
991
5,809
907
3,664
2,081
142

%
22.3
77.7
14.6
85.4
13.4
53.9
30.6
2.1

Synthetic
n
%
1,580
23.2
5,220
76.8
1,034
15.2
5,766
84.8
888
13.3
3,621
53.3
2,142
31.5
147
2.2

Table 4a: DIG - population statistics - example numeric and categorical variables
Numeric variables
variable

age
Length of stay

Real
mean
66.6
7.5

Categorical variables
Synthetic

SD
13.7
9.2

mean
66.4
7.4

SD
13.8
9.8

variable

Val

Major
amputation

No

n
105,251

%
90.7

n
105,053

%
90.6

Yes
male
female

10,757
67,114
48,892

9.3
57.9
42.1

10,955
66,131
49,876

9.4
57.0
43.0

Sex

Real

Synthetic

Table 4b: NIS - population statistics - example numeric and categorical variables
Numeric variables
variable
Total charges ($)
Total charges
accomod ($)
Length of stay

15

Real

Categorical variables
Synthetic

mean
43,473
9,572

SD
79,960
23,859.

mean
45,155
9,653

SD
83575
23814

5.2

10.4

5.1

9.7

variable
Admit weekday

Val
1
2
3
4

Real
n
8,659
8,604
8,053
7,963

Synthetic
%
17.3
17.2
16.1
15.9

n
9,189
7,918
7,749
8,317

%
18.4
15.8
15.5
16.6

https://seer.cancer.gov/data/

18

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

RACE

5
6
7
American
indian/eskimo
Asian or pacific
islander
Black
White
Other

7,400
4,717
4,604
129

14.8
9.4
9.2
0.3

7,353
4,874
4,600
108

14.7
9.7
9.2
0.2

1,026

2.1

980

2.0

6,585
31,389
10,865

13.2
62.8
21.7

6,321
31,118
11,472

12.6
62.2
22.9

Table 4c: Texas - population statistics - example numeric and categorical variables
Numeric variables
variable

Real

Categorical variables
Synthetic

mean

SD

mean

SD

Tumor size

96.6

258.3

88.7

246.3

# in-situ
malignant

1.4

0.6

1.3

0.6

variable
sex
grade

Val
Female
Male
Grade I
Grade II
Grade III
Grade IV

Real
n
99,286
714
20,060
39,519
30.462
901

Synthetic
%
99.3
0.7
22.1
43.5
33.5
1.0

n
99,361
639
20,723
40,378
30,428
801

%
99.4
0.6
22.4
43.7
33.0
0.9

Table 4d: BREAST - population statistics - example numeric and categorical variables (sample size reduced from
1,500,000 to 100,000).
We ran our univariate analysis across all variables in each dataset, comparing marginal distributions between real
and synthetic. Given limited space we will display here a few selected variables for each dataset. Note that some
variables have non-standard distributions causing some visualizations with histograms to have strange artifacts,
however the match between real and synthetic is still maintained.

Figure 9a: DIG - real vs synthetic - marginal distributions for numeric variables: AGE and BMI

Figure 9b: DIG - real vs synthetic - marginal distributions for categorical variables: no. of symptoms and sex
Due to limited space, we are not showing visualizations for all variables; however, Table 5 lists most of the relevant
numerical metrics:

19

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Variable

KL-Div

Variable

Wass. Dist.

KS-Val

KS-p

Ace inhibitor
Current angina
Death or hosp from WHF
Death?
Digoxin within past week
Dose of digoxin or placebo
Dyspnea on exertion
Dyspnea at rest
EF method
Elevated jugular venus pressure
Etiology of CHF
History of diabetes
History of hypertension
Hosp: any hospitalization
Hosp: cardiovascular disease
Hosp: coronary revascularization
Hosp: digoxin toxicity
Hosp: MI
Hosp: other cardiovascular event
Hosp: respiratory infection
Hosp: stroke
Hosp: supraventricular arrhythmia
Hosp: unstable angina
Hosp: ventricular arrhythmia
Hosp: worsening heart failure
Number of hospitalizations
Number of symptoms
NYHA Functional class
Peripheral edema
Previous MI
Race
Reason for death
Recommended digoxin dose
S3 gallop
Sex
Treatment

0.00030
0.00324
0.00109
0.00266
0.00086
0.00165
0.00547
0.00522
0.00026
0.00243
0.00335
0.00443
0.00139
0.00508
0.00377
0.00041
0.00000
0.00218
0.00158
0.00143
0.00086
0.00166
0.00185
0.00039
0.00052
0.01114
0.00326
0.00012
0.00326
0.00054
0.00041
0.00433
0.00167
0.00302
0.00023
0.00001

Age
Body mass index
Chest x-ray (CT ratio)
Days until 1st hosp
Days until CREV
Days until CVD
Days until death
Days until DIG
Days until DWHF
Days until MI
Days until OCVD
Days until OTH
Days until RINF
Days until STRK
Days until SVA
Days until UANG
Days until VENA
Days until WHF
DBP (mmHg)
Duration of CHF (months)
EF (percent)
Heart Rate (beats/min)
SBP (mmHg)
Serum Cr (mg/dL)
Serum K level

0.0224
0.0220
0.1014
0.1453
0.1071
0.1223
0.0988
0.1016
0.0815
0.1051
0.1184
0.1382
0.1091
0.1065
0.1149
0.1193
0.1024
0.0814
0.0369
0.0181
0.0456
0.0445
0.0693
0.0957
0.0193

0.0127
0.0136
0.0511
0.0648
0.0620
0.0623
0.0585
0.0602
0.0540
0.0612
0.0666
0.0729
0.0637
0.0613
0.0650
0.0650
0.0601
0.0540
0.0158
0.0115
0.0287
0.0318
0.0389
0.0531
0.0178

0.6889
0.5962
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
<0.0001
0.4063
0.7925
0.0109
0.0033
0.0001
<0.0001
0.3346

Table 5: full univariate metrics for DIG dataset

Figure 9c: NIS - real vs synthetic - marginal distributions for numeric variables: Age and length of stay

20

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 9d: NIS - real vs synthetic - marginal distributions for categorical variables: depression and amputation

Figure 9e: TEXAS - real vs synthetic - marginal distributions for numeric variables

Figure 9f: TEXAS - real vs synthetic - marginal distributions for categorical variables: Race and Severity of Illness

21

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 9g: BREAST - real vs synthetic - marginal distributions for numerical: cs tumor size and survival in months

Figure 9h: BREAST - real vs synthetic - marginal distributions for categorical variables: summary stage and
laterality
In table 6 we show the marginal distribution match metrics; recall that for numeric variables we use the KS-Statistic
and for categorical variables we use the KL-divergence, and in both cases a value close to 0.0 reflects a good
match in distributions:
Dataset
DIG
DIG
DIG
DIG
NIS
NIS
NIS
NIS
TEXAS
TEXAS
TEXAS
TEXAS
BREAST
BREAST
BREAST
BREAST

Variable
AGE
BMI
Num of Symptoms
SEX
Age
Length of stay
Depression
Major Amputation
Total charges
Length of stay
Race
Severity of illness
CS tumor size
Survival months
Summary stage
laterality

Metric
KS-Stat
KS-Stat
KL-Div
KL-Div
KS-Stat
KS-Stat
KL-Div
KL-Div
KS-Stat
KS-Stat
KL-Div
KL-Div
KS-Stat
KS-Stat
KL-Div
KL-Div

Value
0.0082
0.0156
0.0051
0.0004
0.0095
0.0174
0.0001
<0.0001
0.0160
0.0209
0.0008
0.0003
0.0128
0.0082
0.0006
0.0002

Table 6: univariate distribution metrics
Next, we compute pairwise correlations between variables and plot that as a heatmap, where the strength of the
correlation is color-coded with dark blue representing low correlation and red representing strong correlation:

22

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 10a: real vs synthetic - DIG - pairwise correlations; PCD-L1 = 0.0153

Figure 10b: real vs synthetic - NIS - pairwise correlations; PCD-L1 = 0.0071

Figure 10c: real vs synthetic - TEXAS - pairwise correlations; PCD-L1 = 0.0056

23

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 10d: real vs synthetic - BREAST - pairwise correlations; PCD-L1 = 0.0044
Note how the heatmaps in this case are quite similar to each other, reflecting the fact that the synthetic data
maintains the pairwise correlations that exist in the original DIG dataset.
The next set of statistical fidelity metrics examine the multivariate distribution match between real and synthetic
data. Concretely, in our experiments we performed this analysis for each dataset using the following: UMAP
visualization, predictive performance of ML models, discriminator AUC and pMSE.
We computed the UMAP dimensionality reduction as a visual aid, comparing real to synthetic data, as shown in
figures 11a-11d:

Figure 11a: DIG - UMAP dimensionality reduction - real vs synthetic

Figure 11b: NIS - UMAP dimensionality reduction - real vs synthetic

24

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 11c: TEXAS - UMAP dimensionality reduction - real vs synthetic

Figure 11d: BREAST - UMAP dimensionality reduction - real vs synthetic
This UMAP visualization demonstrates the match in coverage between real and synthetic, across all clusters of
patient records, even for those who represent rare cohorts.
To further evaluate nonlinear multivariate statistical fidelity, for each dataset we trained a machine learning model
(using gradient boosted trees) to predict an outcome of choice from the dataset, using selected predictors from the
data. We trained this model on 80% of the real data and left 20% of the data for validation. We then trained a model
on the synthetic data and validated its performance against the same validation set (20% of the real data). We
repeated this 5 times, each time generating a different synthetic dataset (using random seeds) and measured the
resulting metrics for each, reporting the mean and standard deviation. We also computed the feature importance of
each model (using SHAP values) and compared the top-ranking features for the model trained on real data vs the
model trained on synthetic data. This is demonstrated in figures 12a-12j:

Figures 12a and 12b: DIG - predicting “hosp. for worsening heart failure” (left) and “all-cause mortality” (right)

25

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 12c: DIG - feature importance (SHAP) for “hosp. for worsening heart failure.” nDCG = 0.973

Figure 12d: DIG - feature importance (SHAP) for “all-cause mortality” - real vs synthetic. nDCG = 0.947

Figures 12e and 12f: NIS - predicting “major amputation” and feature importance (nDCG = 0.9868)

26

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figures 12g and 12h: TEXAS - “illness severity” and feature importance (nDCG=0.9819) real vs synthetic

Figures 12i and 12j: BREAST - predicting vital status & feature importance real vs synthetic. (nDCG = 0.9969)
Two of our datasets include time-to-event variables, and for those we perform the Kaplan-Meier analysis as shown
in figures 13a-b

Figure 13a: DIG - Kaplan Meier analysis for “hospitalization for worsening heart failure” - real vs synthetic

27

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 13b: BREAST - Kaplan Meier analysis for vital status - real vs synthetic
As described in 3.5.3, two useful metrics for a match in multivariate statistical properties between real and synthetic
are the discriminator AUC and pMSE. We computed these metrics for all datasets, each time comparing real to 5
different generated synthetic datasets - the results are shown in table 7:
Dataset
DIG
NIS
TEXAS
BREAST

DISC-AUC
0.6405 ± 0.0046
0.5554 ± 0.0092
0.6577 ± 0.0071
0.5406 ± 0.0025

pMSE
0.0706 ± 0.0004
0.0193 ± 0.0002
0.0369 ± 0.0012
0.1715 ± 0.0000

Table 7: discriminator AUC and pMSE for all datasets

5.2.2 Privacy
As described in section 4 above, we demonstrate privacy preservation by simulating membership inference and
attribute inference and use distance-to-closest-record to demonstrate visually the distance between records in the
synthetic dataset to the closest record in the real dataset, as a copy-protection measure.
In our experiments we implemented a measure of distance between records as described in 4.1.1. It is important
to look at DCR between synthetic and real data, as well as compare this to the distribution of distances between
any real record and other real records. Figures 14a-d show the resulting DCR distributions for our 4 datasets, both
synthetic-to-real (green) and real-to-real (blue):

Figure 14a: DIG - distance to closest record synthetic-to-real (green) vs real-to-real (blue).

28

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 14b: NIS - distance to closest record synthetic vs real

Figure 14c: TEXAS - distance to closest record synthetic vs real

Figure 14d: BREAST - distance to closest record synthetic vs real
In addition to DCR, we measure in our experiments the likelihood of an adversary to successfully complete an
attribute inference or membership inference attacks.
Figures 15a-d show our results for membership inference

29

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 15a: DIG - membership inference

Figure 15b: NIS - membership inference

Figure 15c: TEXAS - membership inference

30

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Figure 15d: BREAST - membership inference
Tables 8a-d show our results from the attribute inference attack simulation results. We show for each dataset the
top-5 quasi-identifier combinations that represent the highest F1 scores:
Age
25-50
75-100
50-75
25-50
75-100

Sex
Female
Male
Male
Male
Female

Race
White
White
White
White
NonWhite

F1
0.3819
0.3559
0.3539
0.3523
0.3461

Table 8a: DIG - top 5 attribute inference attacks for “hospitalization for worsening heart failure” using age, sex and
race as quasi-identifiers
Age
0-25
25-50
25-50
25-50
25-50

Sex
Male
Male
Male
Female
Female

Race
White
White
Hispanic
white
UNK

IncQ
$35000 - $45000
>$45000
>$45000
>$45000
$35000 - $45000

F1
0.4922
0.4920
0.4915
0.4900
0.4895

Table 8b: NIS - top 5 attribute inference attacks for “mortality” using age, gender, race and income quartile
Age
0-25
0-25
0-25
0-25
0-25

Race
Other
Asian or Pacific Islander
American Indian/Eskimo
American Indian/Eskimo
Asian or Pacific Islander

Ethnicity
INVALID
Hispanic
Hispanic
INVALID
Non-hispanic

F1
0.3947
0.1357
0.1034
0.1000
0.0997

Table 8c: TEXAS - top 5 attribute inference attacks for “illness severity” using age, race and ethnicity
Age
48
24
37
65
16

Race
Black
Asian or Pacific Islander
White
Asian or Pacific Islander
White

Sex
Male
Female
Male
Male
Female

F1
0.4687
0.4006
0.4583
0.4545
0.4545

Table 8d: BREAST - top 5 attribute inference attacks for “vital status” using age, race and sex

6. Discussion and Analysis
We now discuss our analysis and observations from the experimental results for each dataset.

31

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

6.1 DIG dataset results analysis
The DIG clinical trial dataset includes 6,800 participant records, each with 71 variables. We trained Syntegra’s
engine on this dataset and generated 25,000 synthetic patient records, then compared the real to synthetic using
our fidelity and privacy evaluation framework.
Analysis of fidelity for individual variable distributions is described in table 4a and 5, and figures 9a and 9b. For
categorical variables the KL-divergence values range from 1e-5 to 0.011, and for numeric variables the KS-statistic
values range from 0.0115 to 0.0729 consistent with good correlation between the real and synthetic distributions
across the board. The pairwise heatmap visualization (figure 10a) demonstrates how the pairwise correlation in the
real dataset is maintained in the synthetic, consistent with the low PCD-L1 value of 0.0153.
In figure 11a we show the 2D UMAP dimensionality reduction for both real and synthetic datasets, demonstrating
broad coverage (fidelity) and diversity (privacy); concretely, we observe that real data is split into various “clusters”
of participants that are similar to each other (points are close together) and the synthetic points cover the same
cluster areas; nevertheless, the synthetic points are not superimposed directly on the real points, but rather are
“nearby” in the cluster - this points to strong preservation of privacy.
For multivariate fidelity, we trained two machine learning model, based on gradient boosted trees; one to predict
“all-cause mortality” (the primary outcome in the study) and another to predict “worsening heart failure”, a secondary
outcome that was identified as significant in the study. The ROC curves and the associated AUC-ROC metric
(figures 12a and 12b) demonstrate the ability to train a ML model exclusively from the synthetic data and achieve
comparable results to the model trained on the real data. Interestingly, non-linear predictive models such as we
have built here, were not reported in the original study results, as that technique was not possible with the
technology of the time. Synthetic data can allow novel re-use of earlier clinical trial data in support of precision
medicine. Figures 12c and 12d show the feature importance graphs, based on SHAP values, demonstrating that
not only the predictive models arrive at similar performance, they also utilize the same features for the model in
close to the same order. The feature ranking comparison metric of nDCG for DIG is 0.973 for “worsening heart
failure” and 0.947 for “all-cause mortality”.
The DIG study includes several time-to-event variables, which allowed us to conduct survival analysis and compare
real to synthetic as shown in figure 13a - the resulting Kaplan Meier estimator curves demonstrate similar likelihood
of being endpoint-free for those treated with digoxin vs the placebo and a similar (low) p-value consistent with the
statistical significance of treatment-related hospitalization for worsening heart failure.
As discussed in 3.5.3 we also measured discriminator AUC and pMSE as additional measures of multivariate
fidelity; for the DIG dataset, the discriminator AUC is at 0.6405 and pMSE is at 0.0706, both reasonably low and
consistent with our expectations for a relatively small dataset. As we’ll see below, larger datasets tend to result in
better values for both discriminator AUC and pMSE.
To demonstrate the privacy of synthetic DIG data, we calculated DCR and created a plot of the distribution of DCR
for every synthetic record against the real data, as shown in figure 14a. We observe that the lowest DCR value is
about 12 (out of 71 variables) which demonstrates good separation between real and synthetic; furthermore, the
real-to-real (blue) distribution (removing a comparison of each real record to itself which is, by definition, zero) is
very similar to synthetic-to-real, with small deviation to the right of about 1 DCR.
We simulated a membership inference attack, the results of which are shown in figure 15a. We observe that for
Hamming distance of 18, the success of membership inference attack has the highest variation, with precision
values ranging from negative (below 0.5, worse than random) to 0.65 with 60% of the population known to the
adversary. With a Hamming distance of 18, the recall is quite low, as expected due to the low Hamming distance
chosen, at about 10%, which means the adversary can only apply her membership inference strategy to 10% of
the records available to her. With Hamming distances of 20, 21 and 23, recall increases as we expect but the
precision of this inference strategy continues to be low and close to 0.5 (random chance).
Table 8a shows our results of the attribute inference attack simulation, using age (grouped into 0-25, 25-50, 50-75
and 75-100), sex and race to predict “hospitalization for worsening heart failure”. The best F1 value achieved in
such an attack is 0.3819 consistent with very low success of this type of attack, or low risk of disclosure.

6.2 NIS dataset results analysis
The NIS dataset includes 116,009 patient records, each with 31 variables. We trained Syntegra’s engine on this
dataset and generated 200,000 synthetic patient records, then compared the real to synthetic using our fidelity and
privacy evaluation framework.
Analysis of fidelity for individual variable distributions is described in table 4b, and figures 9c and 9d. The metrics
for univariate distributions are low, as expect, both for categorical variables (with KL-divergence values of 0.0001
for “depression” and 2.6e-5 for “Major amputation”), and for numeric variables (with KS-statistic values of 0.0095
for “age” and 0.0174 for “length of stay”); the pairwise heatmap visualization (figure 10b) demonstrates very good

32

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

fidelity in pairwise correlations (although correlations are general low in this dataset), consistent with a PCD-L1
value of 0.0044.
In figure 11b we show the 2D UMAP dimensionality reduction for both real and synthetic datasets, demonstrating
broad coverage (fidelity) and diversity (privacy); as with the DIG dataset, we observe that real data is split into a
large number of “clusters” of participants that are similar to each other and the synthetic points cover the same
areas, while not being completely superimposed on the real data but rather being nearby and similar to the real
points.
For multivariate fidelity, we trained a machine learning model, based on gradient boosted trees, to predict “major
amputation” - one of the primary outcomes of interest in the study. The ROC curves are shown in figure 12e
demonstrating AUC of 0.9128 for real and 0.9124 for synthetic - almost identical. Figure 12f shows the feature
importance graphs, based on SHAP values, with good match in the ranking, and nDCG of 0.947.
Discriminator AUC is at 0.5554 and pMSE is at 0.0193, both very low demonstrating difficulty for a discriminator
model to differentiate between real and synthetic records.
The DCR plot for NIS is shown in figure 14b. We observe that the lowest DCR value is zero, which could be
interpreted as potentially risky. However, observing a very similar distribution for the real-to-real DCR plot provides
the context here - in this dataset, it is quite common to find many records, in the real dataset, that have duplicate
values (DCR=0), and thus it’s not surprising that synthetic records also contain a similar proportion of such records.
In this case, membership inference and attribute inference provide a more concrete view of disclosure risk.
From our membership inference attack simulation, shown in figure 15b, we observe that this type of attack is quite
unsuccessful, as for various levels of Hamming distance threshold, the precision is quite close to 0.5 - the random
chance. Table 8b shows our results of the attribute inference attack simulation, using age (grouped into 0-25, 2550, 50-75 and 75-100), sex, race and income quartile to predict “mortality”. The best F1 value achieved in such an
attack is 0.4922 which points to low success of inference attack for this dataset.

6.3 TEXAS dataset results analysis
The TEXAS dataset includes 50,000 patient records, each with 18 variables. We trained Syntegra’s engine on this
dataset and generated 100,000 synthetic patient records, then compared the real to synthetic using our fidelity and
privacy evaluation framework.
Analysis of fidelity for individual variable distributions is described in table 4c, and figures 19e and 9f. The metrics
for univariate distributions are low, as expect, both for categorical variables (with KL-divergence values of 0.0003
for “severity of illness” and 0.0008 for “race”), and for numeric variables (with KS-statistic values of 0.0160 for “total
charges” and 0.0209 for “length of stay”); the pairwise heatmap visualization (figure 10c) demonstrates very good
fidelity in pairwise correlations (although correlations are general low in this dataset), consistent with a PCD-L1
value of 0.0044.
In figure 11c we show the 2D UMAP dimensionality reduction for both real and synthetic datasets, demonstrating
broad coverage (fidelity) and diversity (privacy); even more than the NIS or DIG datasets, we observe here that
many small clusters and good coverage by synthetic data points.
For multivariate fidelity, we trained a machine learning model, based on gradient boosted trees, to predict “severity
of illness”. The ROC curves are shown in figure 12g demonstrating AUC of 0.6196 for real and 0.6192 for synthetic
- almost identical. We note that in this case the predictive model is not performing very well (at ROC-AUC of 0.62)
but it performs similarly for real and synthetic data, which is the main exploration of our study. Figure 12h shows
the feature importance graphs, based on SHAP values, with good match in the ranking, and nDCG of 1.0.
Discriminator AUC is at 0.6577 and pMSE is at 0.0369; as with DIG, we see a discriminator can have some but not
great success at discriminating between real and synthetic samples.
The DCR plot for TEXAS is shown in figure 14c. Similar to the NIS dataset, we observe that the lowest DCR value
is zero, but the real-to-real DCR plot shows a similar distribution, leading us to the same conclusion that privacy is
more difficult to ascertain just by looking at DCR, and we focus on membership and attribute inference.
From our membership inference attack simulation, shown in figure 15c, we observe that this type of attack is quite
unsuccessful, as for various levels of Hamming distance threshold, the precision is quite close to 0.5 - the random
chance. Table 8c shows our results of the attribute inference attack simulation, using age (grouped into 0-25, 2550, 50-75 and 75-100), race and ethnicity to predict “illness severity”. The best F1 value achieved in such an attack
is 0.3947 which points to low success of inference attack for this dataset.

6.4 BREAST results analysis
The BREAST dataset includes 1,072,173 patient records, each with 117 variables. We trained Syntegra’s engine
on this dataset and generated 1,500,000 synthetic patient records, then compared the real to synthetic using our
fidelity and privacy evaluation framework.

33

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

Analysis of fidelity for individual variable distributions is described in table 4d, and figures 9g and 9h. The metrics
for univariate distributions are low, as expect, both for categorical variables (with KL-divergence values of 0.0006
for “summary stage” and 0.0002 for “laterality”), and for numeric variables (with KS-statistic values of 0.0082 for
“survival months” and 0.0128 for “CS tumor size”); the pairwise heatmap visualization (figure 10d) demonstrates
very good fidelity in pairwise correlations, consistent with a PCD-L1 value of 0.0044.
In figure 11d we show the 2D UMAP dimensionality reduction for both real and synthetic datasets, demonstrating
broad coverage (fidelity) and diversity (privacy) across all point clusters, similar to what we observed with other
datasets.
For multivariate fidelity, we trained a machine learning model, based on gradient boosted trees, to predict “severity
of illness”. The ROC curves are shown in figure 12i demonstrating AUC of 0.8899 for real and 0.8902 for synthetic
- almost identical. Figure 12j shows the feature importance graphs, based on SHAP values, with good match in the
ranking, and nDCG of 0.9969.
The BREAST dataset includes several time-to-event variables, which allowed us to conduct survival analysis and
compare real to synthetic as shown in figure 13b - the resulting Kaplan Meier estimator curves demonstrate similar
survival likelihoods for males/females between real and synthetic.
Discriminator AUC is at 0.5406 and pMSE is at 0.1715; as with NIS, we observe that large datasets tend to result
in good discriminator AUC values (close to 0.5) reflecting difficulty of the discriminator to distinguish between real
and synthetic data.
The DCR plot for BREAST is shown in figure 14d, and results for membership inference attack simulation are shown
in figure 15d and table 8d, respectively. We observe that membership inference is quite unsuccessful, as for various
levels of Hamming distance threshold, the precision is quite close to 0.5 - the random chance. With attribute
inference, we used age, race and sex as the quasi-identifiers and “vital status” as the target, and here again results
show the highest F1 value at 0.4687, reflecting a low success rate for inference attack with this dataset.

7. Conclusions
For medical data to drive innovation, PHI must be removed to maintain patient privacy, at a minimum. Unfortunately,
this is no longer adequate for widespread data sharing, beyond limited access to certified faculty investigators within
academic institutions. Data sharing, especially the most sensitive variables or at scale, between academic
institutions and commercial third parties where therapies are most likely to be developed. Despite best practices
for limiting access, terms of use agreements, and other ways to assure compliance, it is widely recognized that deidentified data can be re-identified using techniques such as membership or inference attacks. Outside the US,
where GDPR applies, even a de-identified or pseudo-anonymized dataset cannot be shared. A purely synthetic
dataset, however, since it contains no actual patient data, is not subject to GDPR regulations.
Synthetic data has been proposed as a method which allows low-burden access to medical data, such that
disclosure risk is reduced or eliminated. There is a range of methods that have been proposed by which to create
synthetic data starting from a real dataset (as opposed to simulated or artificially constructed datasets based on
rules). Regardless of the method, for widespread adoption, an acceptable level of fidelity and privacy must be
obtained. In this paper, we have described a suite of metrics to assess a) the statistical fidelity of synthetic data as
compared to the original data, and b) the disclosure risk or risk of re-identification. We further demonstrated our
experimental results on four different datasets using Syntegra’s Medical Mind synthetic data engine.
Other novel methods of testing privacy can be considered, such as making a synthetic dataset openly accessible
to Kaggle or other crowd-sourced competitions, with a cash prize for anyone who can “hack” their way into exposing
information about the real data that was used to generate the synthetic data.
Ultimately, our metrics, taken as a whole in relation to either a specific dataset or the overall process of synthetic
data generation, needs to drive a decision as to whether synthetic data may substitute for real world data. That
decision could be anchored in analysis of whether the synthetic data is as good a representation of the real data,
as the real data is representative of the entire population from which the real-world data is sampled. In other words,
in order for insights generated by statistical analysis on the real data to be generalized to a wider group of patients,
it is assumed that the real data is a representative sample of the larger universe of patients. In technical terms, we
know the real data is a sample from an unknown probability distribution, and we expect the synthetic data to be
sampled as near as possible from that same distribution.

Similarly, we believe release of high fidelity synthetic data can dramatically reduce the risk of disclosure, especially
relative to traditional de-identification, but is not required to be 100% perfect privacy in order to be practically usable.

34

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

To date, we have been unable to re-identify any real data based on the synthetic version, and the privacy metrics
applied to synthetic data produced by the Syntegra Medical Mind suggest a very low risk of disclosure.

Once synthetic data is considered an acceptable surrogate for real data, and assuming basic access controls, a far
larger group of researchers can have much quicker and less expensive access to all varieties of healthcare data.
Wider access will accelerate a diverse set of insights and therapies than our current burdensome, and largely failed,
system of data exchange.
Acknowledgments
We want to thank Marlene Grenon, Greg Zahner, Patrick Baier and Arthur Copstein for their input and support in
for their insights and assistance in constructing this manuscript, and the whole Syntegra team for their dedicated
work implementing the Syntegra Medical Mind and all these validation metrics.

35

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

References
[1] Bauchner, Howard, et al. “Data Sharing: An Ethical and Scientific Imperative.” JAMA, vol. 315, no. 12, Mar.
2016, p. 1238. DOI.org (Crossref), doi:10.1001/jama.2016.2420.
[2] Rights (OCR), Office for Civil. “Methods for De-Identification of PHI.” HHS.Gov, 7 Sept. 2012,
https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html.
[3] Narayanan, A., and V. Shmatikov. “Robust De-Anonymization of Large Sparse Datasets.” 2008 IEEE
Symposium on Security and Privacy (Sp 2008), 2008, pp. 111–25. IEEE Xplore, doi:10.1109/SP.2008.33.
[4] Sweeney, Latanya. “K-ANONYMITY: A MODEL FOR PROTECTING PRIVACY.” International Journal of
Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 10, no. 05, Oct. 2002, pp. 557–70. DOI.org
(Crossref), doi:10.1142/S0218488502001648.
[5] Salder, Chris (2020) “Protecting Privacy in Data Releases”, https://www.newamerica.org/oti/reports/primerdisclosure-limitation/
[6] Henriksen-Bulmer, Jane, and Sheridan Jeary. “Re-Identification Attacks—A Systematic Literature Review.”
International Journal of Information Management, vol. 36, no. 6, Dec. 2016, pp. 1184–92. DOI.org (Crossref),
doi:10.1016/j.ijinfomgt.2016.08.002.
[7] Acquisti, A., and R. Gross. “Predicting Social Security Numbers from Public Data.” Proceedings of the National
Academy of Sciences, vol. 106, no. 27, July 2009, pp. 10975–80. DOI.org (Crossref),
doi:10.1073/pnas.0904891106.
[8] Nisselson, Evan (2018) “Deep learning with synthetic data will democratize the tech industry”,
https://techcrunch.com/2018/05/11/deep-learning-with-synthetic-data-will-democratize-the-tech-industry/
[9] Abadie, alberto (2020) “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological
Aspects”, https://www.aeaweb.org/content/file?id=12409
[10] Rubin Donald, (1993) “Statistical disclosure limitation”,
https://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/discussion-statistical-disclosurelimitation2.pdf
[11] Howe, Bill, et al. “Synthetic Data for Social Good.” ArXiv:1710.08874 [Cs], Oct. 2017. arXiv.org,
http://arxiv.org/abs/1710.08874.
[12] Walonoski, Jason, et al. “Synthea: An Approach, Method, and Software Mechanism for Generating Synthetic
Patients and the Synthetic Electronic Health Care Record.” Journal of the American Medical Informatics
Association, vol. 25, no. 3, Mar. 2018, pp. 230–38. DOI.org (Crossref), doi:10.1093/jamia/ocx079.
[13] Dube, Kudakwashe, and Thomas Gallagher. “Approach and Method for Generating Realistic Synthetic
Electronic Healthcare Records for Secondary Use.” Foundations of Health Information Engineering and Systems,
edited by Jeremy Gibbons and Wendy MacCaull, Springer, 2014, pp. 69–86. Springer Link, doi:10.1007/978-3642-53956-5_6.
[14] Stout, D (2018) “The New Synthetic: The Promise of Using Simulated Patients from Real World Data in Life
Sciences Research and Development”, https://www.virtusa.com/perspectives/whitepaper/the-new-synthetic
[15] Jon Starkweather, PhD (2015) “A quick demonstration of the importance of graphing your data: A polite nod
of thanks to F. J. Anscombe.”, https://it.unt.edu/sites/default/files/ansquart_l_jds_jul2015.pdf
[16] Daniel, Wayne W., and Chad Lee Cross. Biostatistics: A Foundation for Analysis in the Health Sciences.
Eleventh edition, Wiley, 2019.
[17] Goncalves, Andre, et al. “Generation and Evaluation of Synthetic Patient Data.” BMC Medical Research
Methodology, vol. 20, no. 1, May 2020, p. 108. BioMed Central, doi:10.1186/s12874-020-00977-1.
[18] Snoke, Joshua, et al. “General and Specific Utility Measures for Synthetic Data.” Journal of the Royal
Statistical Society: Series A (Statistics in Society), vol. 181, no. 3, June 2018, pp. 663–88. DOI.org (Crossref),
doi:10.1111/rssa.12358.
[19] Taub, Jennifer and Elliot, Mark (2019) “The Synthetic Data Challenge.”,
https://unece.org/fileadmin/DAM/stats/documents/ece/ces/ge.46/2019/mtg1/SDC2019_S3_UK_Synthethic_Data_
Challenge_Elliot_AD.pdf
[20] Carlini, Nicholas, et al. “The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural
Networks.” ArXiv:1802.08232 [Cs], July 2019. arXiv.org, http://arxiv.org/abs/1802.08232.
[21] Yan, Chao, et al. “Generating Electronic Health Records with Multiple Data Types and Constraints.”
ArXiv:2003.07904 [Cs, Stat], Mar. 2020. arXiv.org, http://arxiv.org/abs/2003.07904.
[22] McInnes, Leland, et al. “UMAP: Uniform Manifold Approximation and Projection.” Journal of Open Source
Software, vol. 3, no. 29, Sept. 2018, p. 861. DOI.org (Crossref), doi:10.21105/joss.00861.
[23] Stadler, Theresa, et al. “Synthetic Data -- A Privacy Mirage.” ArXiv:2011.07018 [Cs], Dec. 2020. arXiv.org,
http://arxiv.org/abs/2011.07018.

36

Syntegra © - Fidelity and Privacy of Synthetic Medical Data

[24] Lundberg, Scott, and Su-In Lee. “A Unified Approach to Interpreting Model Predictions.” ArXiv:1705.07874
[Cs, Stat], Nov. 2017. arXiv.org, http://arxiv.org/abs/1705.07874.
[25] Tomashchuk, Oleksandr, et al. “A Data Utility-Driven Benchmark for De-Identification Methods.” Trust,
Privacy and Security in Digital Business, edited by Stefanos Gritzalis et al., Springer International Publishing,
2019, pp. 63–77. Springer Link, doi:10.1007/978-3-030-27813-7_5.
[26] Carlini, Nicolas, et al. “Extracting Training Data from Large Language Models”,
https://arxiv.org/pdf/2012.07805.pdf

37

