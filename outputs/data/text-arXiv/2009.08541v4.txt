Variational Disentanglement for Rare Event Modeling
Zidi Xiu, Chenyang Tao, Michael Gao, Connor Davis, Benjamin Goldstein, Ricardo Henao

arXiv:2009.08541v4 [stat.ML] 16 Dec 2020

Duke University
{zidi.xiu, chenyang.tao}@duke.edu

Abstract
Combining the increasing availability and abundance of
healthcare data and the current advances in machine learning methods have created renewed opportunities to improve
clinical decision support systems. However, in healthcare risk
prediction applications, the proportion of cases with the condition (label) of interest is often very low relative to the available sample size. Though very prevalent in healthcare, such
imbalanced classification settings are also common and challenging in many other scenarios. So motivated, we propose a
variational disentanglement approach to semi-parametrically
learn from rare events in heavily imbalanced classification
problems. Specifically, we leverage the imposed extremedistribution behavior on a latent space to extract information
from low-prevalence events, and develop a robust prediction
arm that joins the merits of the generalized additive model
and isotonic neural nets. Results on synthetic studies and diverse real-world datasets, including mortality prediction on a
COVID-19 cohort, demonstrate that the proposed approach
outperforms existing alternatives.

INTRODUCTION
Early identification of in-hospital patients who are at imminent risk of life-threatening events, e.g., death, ventilation or
intensive care unit (ICU) transfer, is a critical subject in clinical care (Bedoya et al. 2019). Especially during a pandemic
like COVID-19, the needs for healthcare change dramatically. With the ability to accurately predict the risk, an automated triage system will be well-positioned to help clinicians better allocate resources and attention to those patients
whose adverse outcomes can be averted if early intervention
efforts were in place.
Despite the great promise it holds, with the richness of
modern Electronic Health Record (EHR) repositories, the
construction of such a system faces practical challenges.
A major obstacle is the scarcity of patients experiencing
adverse outcomes of interest. In the COVID-19 scenario,
which we consider in our experiments, the mortality of patients tested positive at the Duke University Health System
(DUHS) is slightly lower than 3%. Further, in another typical EHR dataset we consider, less than 5% of patients are
reported to suffer adverse outcomes (ICU transfer or death).
In these low-prevalence scenarios, commonly seen in clinical practice, standard classification models such as logistic regression suffer from majority domination, in which

models tend to favor the prediction accuracy of majority
groups. This is clearly undesirable for critical-care applications, given the high false negative rates (Type-II error), in
which patients in urgent need of care could be falsely categorized.
Situations where the distribution of labels is highly
skewed and the accuracy of the minority class bears particular significance (Dal Pozzolo et al. 2017; Lu, Guo, and Li
2020; Machado and Lopes 2020) have been associated with
the name imbalanced dataset (He and Garcia 2009), whereas
the methods dealing with such cases are coined extreme classification (Zong, Huang, and Chen 2013). Under such a setting, the lack of representation of minority cases severely
undermines the ability of a standard learner to discriminate,
relative to balanced datasets (Mitchell 1999). Consequently,
these solutions do not generalize well on minority classes,
where the primary interest is usually focused.
To address such a dilemma, several remedies have been
proposed to account for the imbalance between class representations. One of the most popular strategies is the
sampling-based adjustment, where during training, a model
oversamples the minority classes (or undersamples the majority classes) to create balance artificially (Drummond,
Holte et al. 2003). To overcome the biases and the lack
of information that naive sampling adjustments might induce, variants have been proposed to maximally preserve
the clustering structure of the original dataset (Mani and
Zhang 2003; Yen and Lee 2009) and to promote diversity
of oversampling schemes (Han, Wang, and Mao 2005). Alternatively, cost-sensitive weighting where minority losses
are assigned larger weights provides another popular option
via tuning the relative importance of minority classes (Elkan
2001; Munro et al. 1996; Zhou and Liu 2005).
While the above two strategies introduce heuristics to alleviate the issues caused by class imbalance, importance
sampling (IS) offers a principled treatment that flexibly combines the merits of the two (Hahn and Jeruchim 1987; Heidelberger 1995). Each example is sampled with the probability of a pre-specified importance weight, and with the
weight’s inverse when accounting for the relative contribution in the overall loss. This helps to flexibly tune the
representation of rare events during training, without biasing the data distribution (Heidelberger 1995; Shimodaira
2000; Gretton et al. 2009). It is important to note that, poor

choice of importance weights may result in uncontrolled
variance that destabilizes training (Robert and Casella 2013;
Botev and Kroese 2008), calling for adaptive (Rubinstein
and Kroese 2013) or variance reduction schemes (Rubinstein and Kroese 2016) to protect against such degeneracy.
Apart from the above strategies that fall within the standard empirical risk minimization framework, recent developments explicitly seek better generalization for the minority classes. One such example is the one-class classification that aims to capture one target class from a general
population (Tax 2002). Meta-learning and few-shot learning
strategies instead trying to transfer the knowledge learned
from data-rich classes to facilitate the learning of data-scarce
classes (Böhning, Mylona, and Kimber 2015; Finn, Abbeel,
and Levine 2017). Additionally, non-cross-entropy based
losses or penalties have been proved useful to imbalanced
classification tasks (Weinberger and Saul 2009; Huang et al.
2016). For instance, the Focal loss (Lin et al. 2017) upweights the harder examples, and Cao et al. (2019) introduced a label-distribution-aware margin loss encouraging
minority classes having larger margins.
In this work, we present a novel solution called variational inference for extremals (VIE), capitalizing on the
learning of more generalizable representations for the minority classes. Our proposal is motivated by the observation that the statistical features of “rarity” have been largely
overlooked in the current literature of rare-event modeling.
And the uncertainties of rare-events are often not considered. Framed under the Variational Inference framework,
we formulate our model with the assumption that the extreme presentation of (unobserved) latent variables can lead
to the occurrence (or the inhibition) of rare events. This encourages the accurate characterization of the tail distribution of the data representation, which has been missed by
prior work to the best of our knowledge. Building upon
the state-of-the-art machine learning techniques, our solution features the following contributions: (i) the model accounts for representation uncertainty based on variational inference; (ii) the adoption of mixed Generalized Pareto priors
to promote the learning of heavy-tailed feature representations; and (iii) integration of additive isotonic regression to
disentangle representation and facilitate generalization. We
demonstrate how our framework facilitates both model generalization and interpretation, with strong empirical performance reported across a wide-range of benchmarks.

BACKGROUND
To simplify our presentation, we focus on the problem of
rare event classification for binary outcomes. The generalization to the multiple-class scenario is simple and presented
in the Supplementary Material (SM) 1 . Let D = {xi , yi }N
i=1
be a dataset of interest, where xi and yi denote predictors
and outcomes, respectively, and N is the sample size. Without loss of generality, we denote y = 1 as the minority event
label (indicating the occurrence of an event of interest), and
y = 0 as the majority label.
1

SM can be found at https://arxiv.org/abs/2009.08541

In the following, we will briefly review the three main
techniques we used in this work, namely, variational inference (VI), extreme value theory (EVT), and additive isotonic regression. VI allows for approximate maximum likelihood inference while accounting for data uncertainty. EVT
provides a principled and efficient way to model extreme,
heavy-tailed representations. Additive isotonic regression
further introduces monotonic constraints to disentangle the
contribution of each latent dimension to the outcome.

Variational inference
Consider a latent variable model pθ (v, z) = pθ (v|z)p(z),
where v ∈ Rm is the observable data, z ∈ Rp is the unobservable latent variable, and θ represents the parameters
of the likelihood
model, pθ (v|z). The marginal likelihood
R
pθ (v) = pθ (v, z)dz requires integrating out the latent
z, which typically, for complex distributions, does not enjoy a closed-form expression. This intractability prevents direct maximum likelihood estimation for θ in the latent variable setup. To overcome this difficulty, Variational Inference
(VI) optimizes computationally tractable variational bounds
to the marginal log-likelihood (Kingma and Welling 2014;
Chen et al. 2018). Concretely, the most popular choice of VI
optimizes the following Evidence Lower Bound (ELBO):


pθ (v, Z)
ELBO(v; pθ (v, z), qφ (z|v)) , EZ∼qφ (z|v) log
qφ (Z|v)
≤ log pθ (v),
(1)
where qφ (z|v) is an approximation to the true (unknown) posterior pθ (z|v), and the inequality is a direct result of Jensen’s inequality. The variational gap
between the ELBO and true marginal log-likelihood,
i.e., log pθ (v) − ELBO(v; pθ (v, z), qφ (z|v)), is given by
the Kullback–Leibler (KL) divergence between posteriors,
i.e., KL(qφ (z|v)||pθ (z|v)) = EZ∼qφ (z|v) [log qφ (Z|v)] −
EZ∼qφ (z|v) [log pθ (Z|v)], which implies that the ELBO
tightens as qφ (z|v) approaching the true posterior pθ (z|v).
For estimation, we seek parameters θ and φ that maximize
the ELBO in (1).
Given a set of observations {vi }N
i=1 sampled from
data distribution v ∼ pd (v), maximizing the expected
ELBO is also equivalent to minimizing the KL divergence
KL(pd (v) k pθ (v)) between the empirical and model distributions. When pθ (v|z) and qφ (z|v) are specified as neural
networks, the resulting architecture is commonly known as
the variational auto-encoder (VAE) (Kingma and Welling
2014), where qφ (z|v) and pθ (v|z) and are known as encoder
and decoder, respectively. Note that qφ (z|v) is often used for
subsequent inference tasks on new data.

Extreme Value Theory
Extreme Value Theory (EVT) provides a principled probabilistic framework for describing events with extremely low
probabilities, which we seek to exploit for better rare event
modeling. In particular, we focus on the exceedance models, where we aim to capture the asymptotic statistical behavior of values surpassing an extreme threshold (Davison
and Smith 1990; Tao et al. 2017), which we briefly review

below following the notation of Coles et al. (2001). Without
loss of generality, we consider exceedance to the right, i.e.,
values greater than a threshold u. For a random variable X,
the conditional cumulative distribution of exceedance level
x beyond u is given by Fu (x) = P (X − u ≤ x|X > u) =
F (x+u)−F (u)
, where x > 0 and F (x) denotes the cumula1−F (u)
tive density function for X.
A major result from EVT is that under some mild regularity conditions, e.g., continuity at the right end of F (x)
and others, Fu (x) will converge to the family of Generalized Pareto Distributions (GPD) regardless of F (x),
as u approaches the right support boundary of F (x)
(Balkema and De Haan 1974; Pickands III et al. 1975),
L∞
Gξ,σ,u (x) (Falk, Hüsler, and Reiss
i.e., limu→∞ Fu (x) −→
2010), where GPDξ,σ,u (x) is of the form
(
1
1 − [1 + ξ(x − u)/σ]− ξ , if ξ 6= 0
Gξ,σ,u (x) =
(2)
1 − exp[−(x − u)/σ],
if ξ = 0
where σ is a positive scale parameter. When ξ < 0 the exceedance x has bounded support 0 ≤ x ≤ u − σ/ξ, otherwise when ξ ≥ 0, x is unbounded. A major implication
of this asymptotic behavior is that, for modeling extreme
values, one only needs to fit extreme samples to the loglikelihood function of the GPD.

Additive Isotonic Regression
Also known as monotonic regression, isotonic regression is
a non-parametric regression model that constrains the relation between predictor and outcome to be monotonic, (e.g.,
non-decreasing f (a) ≤ f (b) for a ≤ b) (Barlow et al.
1972). Such monotonic constraint is a natural and flexible
extension to the standard linear relation assumed by many
statistical models. To accommodate multi-covariate predictors, additive isotonic regression combines isotonic models for each individual one-dimensional predictor (Bacchetti
1989). Standard implementations often involve specialized
algorithms, such as local scoring algorithms (Hastie 2017)
and the alternating conditional expectation (ACE) method
of Breiman and Friedman (1985). All these approaches typically require costly iterative computations and are not scalable to large datasets. Here we consider recent advances in
unconstrained monotonic neural networks, which allow for
efficient and flexible end-to-end learning of monotonic relations with robust neural nets based on standard training
schemes such as stochastic gradient descent (Sill 1998; Wehenkel and Louppe 2019).

VARIATIONAL INFERENCE OF
EXTREMALS
The proposed model is based on the hypothesis that extreme events are driven by the extreme values of some latent factors. Specifically, we propose to recast the learning
of low-prevalence events into the learning of extreme latent
representations, thus amortizing the difficulties associated
with directly modeling rare events as outcomes. To allow
for more efficient learning from the rare events, we make

some further assumptions to regularize the latent representation: (i) effect disentanglement: the contribution from each
dimension of the latent representation to the event occurrence is additive; (ii) effect monotonicity: there is a monotonic relation between the outcome likelihood and the values
of each dimension of the latent representation. The key to the
proposed approach is using an additive isotonic neural network to model the one-dimensional disentangled monotonic
relations from a latent representation, which is obtained via
variational inference. Specifically, we impose an EVT prior
to explicitly capture the information from the few minority
group samples into the tail behavior of the extreme representation. Below we provide the rationale for our choices
followed by a description of all model components.
Disentanglement & additive isotonic regression. Consistent with assumptions (i) and (ii), we posit a scenario
in which the underlying representation of extreme events is
more frequent at the far end of the representation spectrum,
for which additive isotonic regression is ideal. The disentanglement consists of modeling each latent dimension individually, thus avoiding the curse of dimensionality when modeling combinatorial effects with few examples. Further, the
monotonicity constraint imposed by the isotonic regression
model restricts possible effect relations, thereby improving
generalization error by learning with a smaller, yet still sufficiently expressive, class of models (Bacchetti 1989).
EVT & VI. Note that the spread of representation of extreme events is expected to be more uncertain relative to
those of the normal, more abundant events, due to a few
plausible causes: (i) extreme events represent the breakdown
of system normality and are expected to behave in uncertain
ways; (ii) there is only a small number of examples available
for the extreme events, so the learned feature encoder will
tend to be unreliable. As a result, it is safely expected that the
encoded features associated with the extremes events will
lie outside the effective support of the Gaussian distribution
assumed by the standard VI model. In other words, the representation of the events can manifest as a heavy-tailed distribution. This will compromise the validity and generalizability of a prediction model if not dealt with appropriately.
So motivated, we explicitly model the distribution of the extreme underlying representations via EVT. Using EVT, we
decouple the learning of the tail end of the representation
distribution. Since EVT-based estimation only requires very
few parameters, it allows for accurate modeling with a small
set of tail-end samples. Further, in combination with the
variational inference framework, it accounts for representation uncertainty via the use of a stochastic encoder, which
further strengthens model robustness.
Benefits of heavy-tailed modeling. A few other considerations further justify modeling with a heavy-tailed distribution for the extreme event representation. One obvious benefit is that it allows better model resolution along the representation axis, i.e., better risk stratification. For light-tail
representations, extreme examples are clustered in a narrow region where the tail vanishes, thus a standard (lighttailed) learning model will report the average risk in that
region. However, if the representations are more spread out,
then there is a more gradual change in risk, which can be

event
non-event

z2

grounG truth
GauVVian
GMM
EVT

z1

z2

Figure 1: Left: Distribution of a two-dimensional latent
space z where the long tail associates with higher risk. Right:
Tail estimations with different schemes for the long-tailed
data in one-dimensional space. EVT provides more accurate
characterization comparing to other mechanisms.

where αj serves as a weight which controlling the effect directions. In other words, when αj > 0, it can be interpreted
as an event stimulator; otherwise it is an event blocker. To
ensure h(s; θ) is non-negative, we apply exponential activation function to the network’s output. The integration of
z is conducted with numerical integration by the RiemannStieltjes method (Davis and Rabinowitz 2007).
From (5) we obtain log pθ (y|z) = `CLL (y, H(z; θ)),
where `CLL (y, a) = log{1y=1 (y)(1 − exp(− exp(a))) +
1y=0 (y) exp(− exp(a))} is the complementary log-log
(CLL) link, where 1(·) is the indicator function. We prefer
CLL over the standard logistic link since the CLL link is
more sensitive at the tail end (Aranda-Ordaz 1981).

Latent Prior: Gaussian GPD Mixture
better captured, as shown in Figure 1. Another argument
for favoring heavy-tailed representations is that heavy-tailed
phenomena are very common in nature (Bryson 1974), and
these tail samples are often encoded less robustly due to
the lack of training examples. Allowing long-tail representations relieves the burden of an encoder.
Model structure. We consider latent variable model
pθ (y, x, z) = pθ (y|z)pθ (x|z)p(z), where v = {x, y} are
the observed variables. Under the VI framework, similar to
(1) we write the ELBO(v; pθ (v, z), qφ (z|v)) as
EZ∼qφ (z|v) [log pθ (y|Z)] + EZ∼qφ (z|v) [log pθ (x|Z)]
− KL(qφ (z|v) k p(z))

(3)

where pθ (y|z) is specified as an additive isotonic regression model, p(z) is modeled with EVT, and the approximate
posterior, qφ (z|v), is specified as an inverse auto-regressive
flow. Note that unlike in the standard ELBO in (3), we have
dropped the term EZ∼qφ (z|v) [log pθ (x|z)] because we are
not interested in modeling the covariates. Note this coincides
with the variational information bottleneck (VIB) formulation (Alemi et al. 2016). Additionally, the posterior qφ (z|v)
will not be conditioned on y, but only on x, because in practice, the labels y are not available at inference time. Specifically, we rewrite the objective in (3) as
Ψβ (x, y; pθ (y|z), qφ (z|x)) =
(4)
EZ∼qφ (z|x) [log pθ (y|Z)] − βKL(qφ (z|x) k p(z)),
where β is a hyperparameter controlling the relative contribution of the KL term to the objective. Below we provide
details for each component of the proposed approach.

Decoder: Additive Monotonic Neural Network
First,
let us consider the following monotone mapping
Rz
h(s; θ)ds + γ, consisting on integrating a non-negative
l
function h(s; θ) specified as a neural network with onedimensional input, s, and parameterized by θ. The choice
of the lower end l is arbitrary, and γ is a bias term. For
multi-dimensional latent representation z ∈ Rp , we write
the additive monotonic neural network (AMNN) as
Z zj
p
X
H(z; θ) =
[αj
hj (s; θ)ds] + γ,
(5)
j

l

To better capture the tail behavior of the latent representation, we assume random variable Z ∼ p(z) is a mixture of a
standard Gaussian distribution truncated at u and a GPD for
modeling the tail end thresholded at u, i.e., F (z) = Φ(z)
when z ≤ u and F (z) = Φ(u) + (1 − Φ(u))Gξ,σ (z − u)
when z > u, where Φ(z) denotes the CDF of a standard
Gaussian distribution. Note that for z > u, F (z) can be ex˜ σ̃, ũ) (McNeil 1997),
pressed as a GPD with parameters (ξ,
˜
where ξ = ξ and if ξ 6= 0, σ̃ = σ(1 − Φ(u))ξ and
ũ = u − σ̃((1 − Φ(u))−ξ − 1)/ξ. Otherwise, when ξ = 0,
σ̃ = σ and ũ = u+ σ̃ log(1−Φ(u)). Consequently, the CDF
for the mixed GPD is given by
F (z) = 1(−∞,u] (z)Φ(z) + 1(u,∞) (z)Gũ,ξ,σ̃ (z).

(6)

For simplicity, we denote the set of parameters in GPD as
ψ={ξGPD ,σGPD } and the threshold u is a user-defined parameter. In the experiments we set u to Φ−1 (0.99).

Latent Posterior: Inverse Autoregressive Flow
Considering we have adopted a long-tailed GPD prior, we
seek a posterior approximation qφ (z|x) that is: (i) a flexible parameterization to approximate arbitrary distributions;
and (ii) with a tractable likelihood to be able to evaluate
the KL(qφ (z|x) k p(z)) exactly. We need (i) because the
true posterior is likely to exhibit heavy-tailed behavior due
to the extended coverage of the GPD prior, and (ii) is to
ensure accurate and low-variance Monte Carlo estimation
of the KL-divergence at the tail end of the prior. These requirements invalidate some popular choices, e.g., a standard
Gaussian posterior is light-tailed, and the implicit neuralsampler-based posterior typical in the work of adversarial
variational Bayes (Mescheder, Nowozin, and Geiger 2017),
does not have a tractable likelihood.
One model family satisfying the above two requirements is known as the generative flows (Rezende and Mohamed 2015), where simple invertible transformations with
tractable log Jacobian determinants are stacked together,
transforming a simple base distribution into a complex one,
while still having closed-form expressions for the likelihood.
In this work, we consider the inverse autoregressive flow
(IAF) model (Kingma et al. 2016). The flow chain is built
as:
zt = µt + σt zt−1 , for 1 ≤ t ≤ T,
(7)

where µt ∈ Rp and σt ∈ Rp are learnable parameters,
denotes the element-wise product, z0 is typically
drawn from a p-dimensional Gaussian distribution, z0 ∼
N (µ0 , Diag(σ02 )) where µ0 and σ0 are obtained from an initial encoder defined by a neural network given input x with
parameter φ. A sample from the posterior qφ (z|x) is given
by zT , obtained by “flowing” z0 through (7). Provided the
t
t
Jacobians dzdµt−1
and dzdσt−1
are strictly upper triangular (Papamakarios, Pavlakou, and Murray 2017), we obtain the following closed-form expression for the log posterior
log q(z|x) = log q(z0 |x) −

T
X

log det

t=1

=−

p
X
j=1

dzt
dzt−1

T
X
1 2 1
ej + log(2π) +
log σt,j
2
2
t=0

(8)
!
,

where ej = (xj − µ0,j )/σ0,j for the jth dimension.

Posterior Match with Fenchel Mini-Max Learning
We consider an additional modification that explicitly encourages
the match of the aggregated posterior qφ (z) =
R
qφ (z|x)pd (x)dx to the prior p(z), which has been reported to be vastly successful at improving VAE learning
(Mescheder, Nowozin, and Geiger 2017). In our case, qφ (z)
does not have a closed-form expression for the likelihood
ratio of the KL formulation, which motivates us to use a
sample-based estimator. We consider the mini-max KL estimator based on the Fenchel duality (Tao et al. 2019; Dai
et al. 2018). Concretely, recall the KL can be expressed in
its Fenchel dual form 2

Algorithm 1: Variational Inference with Extremals.
Data: D = (x, y). x: inputs, y: labels
Networks and parameters:
Init-Encoder(x, ; φ): Initial encoder network;
IAF(z; φ): recursive autoregressive neural network;
ν(z; ω): critic neural network;
AMNN(z; θ): additive monotonic neural net;
prior: pψ (z) =MixedGPD(z;ψ,u), ψ={ξGPD ,σGPD }
Initialize: Init-Encoder, IAF, ν, AMNN, ψ
for iteration k ∈ {1, . . . , K} do
m
Sample {(xi , yi )}m
i=1 from D, {i }i=1 from p()
[µ0 , σ0 ] =Init-Encoder(x, ; φ)
Sample zpr from pψ (z), z0 from N (µ0 , Σ0 )
Compute lpost := log qφ (z0 |x)
for step t ∈ {1, . . . , T } do
[µt , σt ] =IAF(zt−1
P ; φ), zt = µt + σt zt−1
lpost = lpost − (log σt )
end
log pθ (y|zT ) = `CLL (y,
PAMNN(zT ; θ))
1
Descend ω by ∇ω m
[νω (zpr ) − log νω (zT )]
Ascend
Ω
=
{φ,
ψ,
θ}
by
P
1
[log pθ (y|zT ) − log νω (zT ) − KL],
∇Ω m
where KL = lpost − log pψ (zT )
end

To avoid collapsing to suboptimal local minima, we train
the encoder arm more frequently to compensate for the detrimental posterior lagging phenomenon (He et al. 2019). The
pseudo-code for the proposed VIE is summarized in Algorithm 1 and detailed architecture can be found in the SM.

Γ(p, qφ , ν) = EZ∼qφ (z) [ν(Z)] − EZ 0 ∼p(z) [exp(ν(Z 0 ))]
KL(qφ (z) k p(z)) = max Γ(p, q, ν),
ν∈F

(9)

where ν(z) is commonly known as the critic function in the
adversarial learning literature, and we maximize wrt ν(z) in
the space of all functions F, modeled with a deep neural network. We use (4) and (9) to derive an augmented ELBO that
further penalizes the discrepancy between the aggregated
posterior and the prior, i.e., Ψβ (x, y; pθ (y|z), qφ (z|x)) −
λKL(qφ (z) k p(z)), where λ is a regularization hyperparameter (Chen, Feng, and Lu 2018). Solving for this objective results in the following mini-max game
max min Ψβ (x, y; pθ (y|z), qφ (z|x)) − λΓ(pθ , qφ , ν), (10)
θ,φ

ν

where β and λ are regularization hyperparameters. In a similar vein to β-VAE and adversarial variational Bayes (AVB),
our objective leverages β, λ > 0 to balance the prediction
accuracy and the complexity of the latent representation via
KL regularization. Further, from Ψβ (x, y; pθ (y|z), qφ (z, x))
in (4), note that the decoder pθ (y|z) is obtained from the
additive neural network in (5), pψ (z) is the Gaussian GPD
mixture with CDF in (6), qφ (z|x) is the autoregressive flow
implied by (7) and ν(z; ω) is the critic function specified as
a neural network and parameterized by ω.
2

We have removed the constant term for notational clarity.

RELATED WORK
Rare-event modeling with regression. Initiated by King
and Zeng (2001), the discussion on how to handle the unique
challenges presented by rare-event data for regression models has attracted extensive research attention. The statistical literature has mainly focused on bias correction for sampling (Fithian and Hastie 2014) and estimation (Firth 1993),
driven by theoretical considerations in maximum likelihood
estimation. However, their assumptions are often violated in
the face of modern datasets (Sur and Candès 2019), characterized by high-dimensionality and complex interactions.
Our proposal approaches a solution from a representation
learning perspective (Bengio, Courville, and Vincent 2013),
by explicitly exploiting the statistical regularities of extreme
values to better capture extreme representations associated
with rare events.
Re-sampling and loss correction. Applying statistical
adjustments during model training is a straightforward solution to re-establish balance, but often associated with obvious caveats. For example, the popular down-sampling and
up-sampling (He and Garcia 2009) discard useful information or introduce artificial bias, exacerbating the chances
of capturing spurious features that may harm generalization (Drummond, Holte et al. 2003; Cao et al. 2019), and
their performance gains may be limited (Byrd and Lipton

Table 1: Ablation study of VIE with different combinations of architectures on realistic synthetic datasets with 1% event rate.
The oracle model has used the ground-truth model parameters to predict.
Average AUC (standard deviation)
VAE
VAE-GPD
IAF-GPD
Fenchel-GPD
VIE

Average AUPRC (standard deviation)

Prior

Encoder

Decoder

Prior Match

n=5k

n=10k

n=20k

n=5k

n=10k

n=20k

Gaussian
mixed GPD
mixed GPD
mixed GPD
mixed GPD

Gaussian
Gaussian
IAF
Implicit
IAF

MLP
AMNN
AMNN
AMNN
AMNN

True
False
False
True
True

0.552 (0.092)
0.569 (0.062)
0.511(0.021)
0.623 (0.036)
0.684 (0.031)

0.682 (0.030)
0.599 (0.010)
0.551 (0.018)
0.668 (0.044)
0.697 (0.036)

0.674 (0.020)
0.653 (0.027)
0.665 (0.029)
0.694 (0.021)
0.701 (0.017)

0.026 (0.010)
0.021 (0.003)
0.017(0.002)
0.037 (0.010)
0.050 (0.009)

0.053 (0.010)
0.027 (0.005)
0.019 (0.002)
0.048 (0.013)
0.061 (0.025)

0.061 (0.017)
0.035 (0.013)
0.025 (0.008)
0.062 (0.026)
0.079 (0.025)

Oracle (with 90% confidence interval)

2019). While traditionally tuned by trial and error, recent
works have explored automated weight adjustments (Lin
et al. 2017; Zhang et al. 2020) , and principled loss correction that factored in class-size differences (Cui et al. 2019).
Our contribution is orthogonal to these developments and
promises additional gains when used in synergy.
Transferring knowledge from the majority classes.
Adapting the knowledge learned from data-rich classes to
their under-represented counterparts has shown success in
few-shot learning, especially in the visual recognition field
(Wang, Ramanan, and Hebert 2017; Chen et al. 2020), and
also in the clinical setting (Böhning, Mylona, and Kimber
2015). However, their success often critically depends on
strong assumptions, the violation of which typically severely
undermines performance (Wang et al. 2020). Related are the
one-class classification (OCC) models (Tax 2002), assuming stable patterns for the majority over the minority classes.
Our assumptions are weaker than those made in these model
categories, and empirical results also suggest the proposed
VIE works more favorably in practice (see experiments).

EXPERIMENTS
We carefully evaluate the proposed VIE on a diverse set of
realistic synthetic data and real-world datasets with different degrees of imbalance. Our implementation is based on
PyTorch, and code to replicate our experiments are available from https://github.com/ZidiXiu/VIE/. We provide additional experiments and analyses in the SM.
Baseline Models We consider the following set of competing baselines to compare the proposed solution: LASSO
regression (Tibshirani 1996), MLP with re-sampling and
re-weighting (MLP), Importance-Weighting model (IW)
(Byrd and Lipton 2019), FOCAL loss (Lin et al. 2017),
Label-Distribution-Aware Margin loss (LDAM) (Cao et al.
2019), and SVD based one-class classification model (DeepSVDD) (Ruff et al. 2018). We tune the hyper-parameters of
baseline models on the validation dataset, and pick best performing hyper-parameters to evaluate test set performance.
For detailed settings please refer to the SM.
Evaluation Metrics To quantify model performance, we
consider AUC and AUPRC. AUC is the area under the Receiver Operating Characteristic (ROC) curve, which provides a threshold-free evaluation metric for classification
model performance. AUC summarizes the trade-off between
True Positive Rate (TPR) and False Positive Rate (FPR).
AUPRC summarizes the trade-off between TPR and True

0.704 [0.662, 0.751]

0.092 [0.058, 0.141]

Predictive Rate. Specifically, it evaluates the area under
Precision-Recall (PR) curve. We discuss other metrics in the
SM. In simulation studies, we repeat simulation ten times to
obtain empirical AUC and AUPRC confidence intervals. For
real world datasets, we applied bootstrapping to estimate the
confidence intervals.

Ablation study for VIE
VIE applies a few state-of-art techniques in variational inference in order to achieve optimal performance. In this section, we decouple their contributions via an ablation study,
to justify the necessity of including those techniques in our
final model. To this end, we synthesize a semi-synthetic
dataset based on the Framingham study (Mitchell et al.
2010), a long-term cardiovascular survival cohort study.
We use a realistic model to synthesize data from the realworld covariates under varying conditions, i.e., different
event rates, sample size, non-linearity, etc. More specifically, we use the CoxPH-Weibull model (Bender, Augustin,
and Blettner 2005) to simulate the survival times of patients
− log U
T = { λ exp(g(x))
}1/ν , where g(x) is either a linear function
or a randomly initialized neural net. Our goal is to predict
whether the subject will decease within a pre-specified time
frame, i.e., T < t0 . Via adjusting the cut-off threshold t0 ,
we can simulate different event rates. A detailed description
of the simulation strategy is in the SM.
We experiment with different combinations of advanced
VI techniques, as summarized in Table 1. Limited by space,
we report results at 1% event rate with g(·) set to a randomly initialized neural network under various sample sizes.
Additional results on linear models and other synthetic
datasets are consistent and can be found in the SM. IAF
and GPD only variants perform poorly, even compared to
the vanilla VAE solution. This is possibly due to the fact
that priors are mismatched. Explicitly matching to the prior
via Fenchel mini-max learning technique improves performance. However, without using an encoder with a tractable
likelihood, the model cannot directly leverage knowledge
from the GPD prior likelihood. Stacked together (mixed
GPD+IAF+Fenchel), our full proposal of VIE consistently
outperforms its variants, approaching oracle performance in
the large sample regime.

Real-World Datasets
To extensively evaluate real-world performance, we consider a wide range of real-world datasets, briefly summa-

Table 2: Average AUC and AUPRC from real-world datasets.
average AUC
COVID

average AUPRC

InP

SEER

SLEEP

COVID

InP

SEER

SLEEP

Event category

Mortality

Combined

12h

24h

48h

168h

3mo

11mo

600d

Mortality

Combined

12h

24h

48h

168h

3mo

11mo

600d

LASSO
MLP
DeepSVDD
IW
Focal
LDAM

0.856
0.862
NA
0.856
0.829
0.857

0.853
0.854
NA
0.860
0.854
0.843

0.822
0.824
0.633
0.776
0.750
0.819

0.789
0.806
0.608
0.748
0.779
0.805

0.767
0.762
0.605
0.726
0.741
0.785

0.760
0.768
0.551
0.728
0.705
0.774

0.888
0.885
0.592
0.798
0.868
0.893

0.845
0.856
0.572
0.832
0.835
0.861

0.720
0.730
0.644
0.642
0.633
0.755

0.235
0.225
NA
0.193
0.238
0.202

0.542
0.531
NA
0.511
0.484
0.535

0.092
0.093
0.020
0.073
0.044
0.086

0.131
0.141
0.030
0.086
0.112
0.130

0.159
0.159
0.044
0.105
0.120
0.148

0.216
0.221
0.063
0.165
0.149
0.197

0.140
0.169
0.026
0.123
0.141
0.177

0.309
0.322
0.068
0.274
0.263
0.332

0.164
0.182
0.118
0.120
0.101
0.179

VIE

0.883

0.867

0.840

0.818

0.793

0.780

0.895

0.862

0.778

0.268

0.535

0.100

0.150

0.179

0.240

0.189

0.345

0.196

INP

SEER

SLEEP

25,315
1268(668)
2.6%, 8%

67,655
73(39)
1 ∼ 5%

68,082
789(771)
1 ∼ 5%

5026
206(162)
5%

Density

sample size
dimension
event rate (%)

COVID

0.4

posterior
prior

0.3

4

2

risk
0.4

0.2

2
0
0

0.2 −2

0.1

−2

0.0 −4

0.0

rized below: (i) COVID: A dataset of patients admitted to
the DUHS with positive COVID-19 testing, to predict death
or use of a ventilator. (ii) InP (O’Brien et al. 2020): An
in-patient data from DUHS, to predict the risk of death or
ICR transfers. (iii) SEER (Ries et al. 2007): A public dataset
studying cancer survival among adults curated by the U.S.
Surveillance, Epidemiology, and End Results (SEER) Program, here we use a 10-year follow-up breast cancer subcohort. (iv) SLEEP (Quan et al. 1997): The Sleep Heart Health
Study (SHHS) is a prospective cohort study about sleeping
disorder and cardiovascular diseases. Summary statistics of
these four real-world datasets are given in Table 3. Note that
InP, SEER and SLEEP are all survival datasets, among
which SEER and SLEEP include censored subjects. We follow the data pre-processing steps in (Xiu et al. 2020). To create outcome labels, we set a cut-off time to define an event of
interest the same as in the ablation study, and exclude subjects censored before the cut-off time. The excluded samples
only account for less than 0.2% of the whole population, and
therefore it is expected to have a very limited impact on our
results. Datasets have been randomly split into training, validation, and testing datasets with ratio 6:2:2. See the SM for
details on data pre-processing.
Table 2 compares VIE to its counterparts, where the numbers are averaged over the bootstrap samples. We see the
proposed VIE yields the best performance in almost all
cases, and the lead is more significant with low event rates.
Note that the one-class classification based DeepSVDD performs poorly, which implies treating rare events as outliers are inappropriate in the scenarios considered here. Reweighting and resampling based methods (IW, Focal) are
less stable compared to those simple baselines (LASSO,
MLP). The theoretically optimal LDAM works well in general, second only to VIE in most settings. To further demonstrate the stability of our method, we visualize the bootstrapped evaluation scores for the COVID dataset in Figure 3, and defer the additional cross-validation results to the
SM. We see that VIE leads consistently.
We also verify empirically that the estimated GPD shape

6

4

0.6

risk

Table 3: Summary statistics for real-world datasets.

−4

−2

0

2

4

event
event

z1

non-event
non-event

Figure 2: First latent dimension from the InP dataset (1%
event rate). Left: Learned prior and posterior distribution,
and monotonic predicted risks (right axis). Right: The latent
representation values distribution grouped by event type.

parameters ξGPD are mostly positive (see the SM), indicating
heavier than Gaussian tails as we have hypothesized. In Figure 2, we visualize one such latent dimension from the InP
dataset, along with the associated risk learned by AMNN. In
this example, the tail part is heavier than Gaussian and is associated with elevated risk. See our SM for examples where
the extended tail contributes to prohibit the event.
IW
Focal
LASSO
LDAM
VIE
0.75

0.80

0.85

AUC Score

0.90

0.1

0.2

0.3

AUPRC Score

0.4

Figure 3: Bootstrapped AUC (left) and AUPRC (right) distributions for the COVID mortality data (2.6% event rate).

CONCLUSIONS
Motivated by the challenges of rare-event prediction in
clinical settings, we presented Variational Inference with
Extremals (VIE), a novel extreme representation learningbased variational solution to the problem. In this model we
leveraged GPD to learn the extreme distributions with few
samples and applied additive monotonic neural networks to
disentangle the latent dimensions’ effects on the outcome.
VIE featured better generalization and interpretability, as
evidenced by a strong performance on real and synthetic
datasets. In future work, we will extend this framework to
the context of causal inference to quantify treatment effects
in the label imbalanced setting (Lu et al. 2020).

−4

event

Acknowledgements The authors would like to thank the
Duke Institute for Health Innovation (DIHI) for providing
access to curated COVID-19 data and outcomes. This research was supported in part by NIDDK R01-DK123062
and NIH/NIBIB R01-EB025020.

Coles, S.; Bawa, J.; Trenner, L.; and Dorazio, P. 2001. An introduction to statistical modeling of extreme values, volume 208.
Springer.

References

Dai, B.; Dai, H.; He, N.; Liu, W.; Liu, Z.; Chen, J.; Xiao, L.; and
Song, L. 2018. Coupled variational bayes via optimization embedding. In NIPS.

Alemi, A. A.; Fischer, I.; Dillon, J. V.; and Murphy, K. 2016. Deep
variational information bottleneck. In ICLR.
Aranda-Ordaz, F. J. 1981. On two families of transformations to
additivity for binary response data. Biometrika 68(2): 357–363.
Bacchetti, P. 1989. Additive isotonic models. Journal of the American Statistical Association 84(405): 289–294.
Balkema, A. A.; and De Haan, L. 1974. Residual life time at great
age. The Annals of probability 792–804.
Barlow, R. E.; Bartholomew, D. J.; Bremner, J. M.; and Brunk,
H. D. 1972. Statistical inference under order restrictions: The theory and application of isotonic regression. Technical report, Wiley
New York.
Bedoya, A. D.; Clement, M. E.; Phelan, M.; Steorts, R. C.; O’Brien,
C.; and Goldstein, B. A. 2019. Minimal impact of implemented
early warning score and best practice alert for patient deterioration.
Critical care medicine 47(1): 49–55.
Bender, R.; Augustin, T.; and Blettner, M. 2005. Generating survival times to simulate Cox proportional hazards models. Statistics
in medicine 24(11): 1713–1723.
Bengio, Y.; Courville, A.; and Vincent, P. 2013. Representation
learning: A review and new perspectives. IEEE transactions on
pattern analysis and machine intelligence 35(8): 1798–1828.

Cui, Y.; Jia, M.; Lin, T.-Y.; Song, Y.; and Belongie, S. 2019. Classbalanced loss based on effective number of samples. In CVPR.

Dal Pozzolo, A.; Boracchi, G.; Caelen, O.; Alippi, C.; and Bontempi, G. 2017. Credit card fraud detection: a realistic modeling
and a novel learning strategy. IEEE transactions on neural networks and learning systems 29(8): 3784–3797.
Davis, P. J.; and Rabinowitz, P. 2007. Methods of numerical integration. Courier Corporation.
Davison, A. C.; and Smith, R. L. 1990. Models for exceedances
over high thresholds. Journal of the Royal Statistical Society: Series B (Methodological) 52(3): 393–425.
Drummond, C.; Holte, R. C.; et al. 2003. C4. 5, class imbalance,
and cost sensitivity: why under-sampling beats over-sampling. In
ICML Workshop.
Elkan, C. 2001. The foundations of cost-sensitive learning. In
IJCAI.
Falk, M.; Hüsler, J.; and Reiss, R.-D. 2010. Laws of small numbers:
extremes and rare events. Springer Science & Business Media.
Finn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic metalearning for fast adaptation of deep networks. In ICML.
Firth, D. 1993. Bias reduction of maximum likelihood estimates.
Biometrika 80(1): 27–38.

Böhning, D.; Mylona, K.; and Kimber, A. 2015. Meta-analysis of
clinical trials with rare events. Biometrical Journal 57(4): 633–
648.

Fithian, W.; and Hastie, T. 2014. Local case-control sampling: Efficient subsampling in imbalanced data sets. Annals of statistics
42(5): 1693.

Botev, Z. I.; and Kroese, D. P. 2008. An efficient algorithm for
rare-event probability estimation, combinatorial optimization, and
counting. Methodology and Computing in Applied Probability
10(4): 471–505.

Gretton, A.; Smola, A.; Huang, J.; Schmittfull, M.; Borgwardt, K.;
and Schölkopf, B. 2009. Covariate shift by kernel mean matching.
Dataset shift in machine learning 3(4): 5.

Boyd, K.; Costa, V. S.; Davis, J.; and Page, C. D. 2012. Unachievable region in precision-recall space and its effect on empirical
evaluation. In ICML.
Breiman, L.; and Friedman, J. H. 1985. Estimating optimal transformations for multiple regression and correlation. Journal of the
American statistical Association 80(391): 580–598.

Hahn, P.; and Jeruchim, M. 1987. Developments in the theory and
application of importance sampling. IEEE transactions on Communications 35(7): 706–714.
Han, H.; Wang, W.-Y.; and Mao, B.-H. 2005. Borderline-SMOTE:
a new over-sampling method in imbalanced data sets learning. In
ICIC.

Bryson, M. C. 1974. Heavy-tailed distributions: properties and
tests. Technometrics 16(1): 61–68.

Hastie, T. J. 2017. Generalized additive models. In Statistical models in S, 249–307. Routledge.

Byrd, J.; and Lipton, Z. 2019. What is the effect of importance
weighting in deep learning? In ICML.

He, H.; and Garcia, E. A. 2009. Learning from imbalanced data.
IEEE Transactions on knowledge and data engineering 21(9):
1263–1284.

Cao, K.; Wei, C.; Gaidon, A.; Arechiga, N.; and Ma, T. 2019.
Learning imbalanced datasets with label-distribution-aware margin
loss. In NeurIPS.
Chen, J.; Feng, J.; and Lu, W. 2018. A Wiener causality defined by
relative entropy. In ICONIP.
Chen, J.; Xiu, Z.; Goldstein, B. A.; Henao, R.; Carin, L.; and Tao,
C. 2020. Supercharging Imbalanced Data Learning With Causal
Representation Transfer. arXiv preprint arXiv:2011.12454 .
Chen, L.; Tao, C.; Zhang, R.; Henao, R.; and Carin, L. 2018. Variational inference and model selection with generalized evidence
bounds. In ICML.

He, J.; Spokoyny, D.; Neubig, G.; and Berg-Kirkpatrick, T. 2019.
Lagging inference networks and posterior collapse in variational
autoencoders. In ICLR.
Heidelberger, P. 1995. Fast simulation of rare events in queueing
and reliability models. ACM Transactions on Modeling and Computer Simulation (TOMACS) 5(1): 43–85.
Huang, C.; Li, Y.; Change Loy, C.; and Tang, X. 2016. Learning
deep representation for imbalanced classification. In CVPR.
King, G.; and Zeng, L. 2001. Logistic regression in rare events
data. Political Analysis 9(2): 137–163.

Kingma, D. P.; Salimans, T.; Jozefowicz, R.; Chen, X.; Sutskever,
I.; and Welling, M. 2016. Improved variational inference with inverse autoregressive flow. In NIPS.
Kingma, D. P.; and Welling, M. 2014. Auto-encoding variational
Bayes. In ICLR.
Lin, T.-Y.; Goyal, P.; Girshick, R.; He, K.; and Dollár, P. 2017.
Focal loss for dense object detection. In ICCV.
Lu, D.; Guo, F.; and Li, F. 2020. Evaluating the causal effects of
cellphone distraction on crash risk using propensity score methods.
Accident Analysis & Prevention 143: 105579.
Lu, D.; Tao, C.; Chen, J.; Li, F.; Guo, F.; and Carin, L. 2020. Reconsidering Generative Objectives For Counterfactual Reasoning.
In NeurIPS.

Rubinstein, R. Y.; and Kroese, D. P. 2013. The cross-entropy
method: a unified approach to combinatorial optimization, MonteCarlo simulation and machine learning. Springer Science & Business Media.
Rubinstein, R. Y.; and Kroese, D. P. 2016. Simulation and the
Monte Carlo method, volume 10. John Wiley & Sons.
Ruff, L.; Vandermeulen, R.; Goernitz, N.; Deecke, L.; Siddiqui,
S. A.; Binder, A.; Müller, E.; and Kloft, M. 2018. Deep one-class
classification. In ICML.
Shimodaira, H. 2000. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of
Statistical Planning and Inference 90(2): 227–244.
Sill, J. 1998. Monotonic networks. In NIPS.

Machado, J. T.; and Lopes, A. M. 2020. Rare and extreme events:
the case of COVID-19 pandemic. Nonlinear Dynamics 1.

Sur, P.; and Candès, E. J. 2019. A modern maximum-likelihood
theory for high-dimensional logistic regression. Proceedings of the
National Academy of Sciences 116(29): 14516–14525.

Mani, I.; and Zhang, I. 2003. kNN approach to unbalanced data distributions: a case study involving information extraction. In ICML
Workshop.

Tao, C.; Chen, L.; Dai, S.; Chen, J.; Bai, K.; Wang, D.; Feng, J.;
Lu, W.; Bobashev, G.; and Carin, L. 2019. On Fenchel Mini-Max
Learning. In NeurIPS.

McNeil, A. J. 1997. Estimating the tails of loss severity distributions using extreme value theory. ASTIN Bulletin: The Journal of
the IAA 27(1): 117–137.

Tao, C.; Nichols, T. E.; Hua, X.; Ching, C. R.; Rolls, E. T.; Thompson, P. M.; Feng, J.; Initiative, A. D. N.; et al. 2017. Generalized
reduced rank latent factor regression for high dimensional tensor
fields, and neuroimaging-genetic applications. NeuroImage 144:
35–57.

Mescheder, L.; Nowozin, S.; and Geiger, A. 2017. Adversarial
variational bayes: Unifying variational autoencoders and generative adversarial networks. In ICML.
Mitchell, G. F.; Hwang, S.-J.; Vasan, R. S.; Larson, M. G.; Pencina,
M. J.; Hamburg, N. M.; Vita, J. A.; Levy, D.; and Benjamin, E. J.
2010. Arterial stiffness and cardiovascular events: the Framingham
Heart Study. Circulation 121(4): 505.
Mitchell, T. M. 1999. Machine learning and data mining. Communications of the ACM 42(11): 30–36.
Munro, D.; Ersoy, O.; Bell, M.; and Sadowsky, J. 1996. Neural
network learning of low-probability events. IEEE Transactions on
Aerospace and Electronic Systems 32(3): 898–910.

Tax, D. M. J. 2002. One-class classification: Concept learning in
the absence of counter-examples. Ph.D. thesis, Technische Universiteit Delft (The Netherlands).
Tibshirani, R. 1996. Regression shrinkage and selection via the
lasso. Journal of the Royal Statistical Society: Series B (Methodological) 58(1): 267–288.
Wang, Y.; Yao, Q.; Kwok, J. T.; and Ni, L. M. 2020. Generalizing from a few examples: A survey on few-shot learning. ACM
Computing Surveys (CSUR) 53(3): 1–34.
Wang, Y.-X.; Ramanan, D.; and Hebert, M. 2017. Learning to
model the tail. In NIPS.

O’Brien, C.; Goldstein, B. A.; Shen, Y.; Phelan, M.; Lambert, C.;
Bedoya, A. D.; and Steorts, R. C. 2020. Development, Implementation, and Evaluation of an In-Hospital Optimized Early Warning Score for Patient Deterioration. MDM Policy & Practice 5(1):
2381468319899663.

Wehenkel, A.; and Louppe, G. 2019. Unconstrained monotonic
neural networks. In NeurIPS.

Papamakarios, G.; Pavlakou, T.; and Murray, I. 2017. Masked autoregressive flow for density estimation. In NIPS.

Xiu, Z.; Tao, C.; Goldstein, B. A.; and Henao, R. 2020. Variational
learning of individual survival distributions. In ACM CHIL.

Pickands III, J.; et al. 1975. Statistical inference using extreme
order statistics. the Annals of Statistics 3(1): 119–131.

Yen, S.-J.; and Lee, Y.-S. 2009. Cluster-based under-sampling approaches for imbalanced data distributions. Expert Systems with
Applications 36(3): 5718–5727.

Quan, S. F.; Howard, B. V.; Iber, C.; Kiley, J. P.; Nieto, F. J.;
O’Connor, G. T.; Rapoport, D. M.; Redline, S.; Robbins, J.; Samet,
J. M.; et al. 1997. The sleep heart health study: design, rationale,
and methods. Sleep 20(12): 1077–1085.
Rezende, D. J.; and Mohamed, S. 2015. Variational inference with
normalizing flows. In ICML.
Ries, L. G.; Young, J.; Keel, G.; Eisner, M.; Lin, Y.; Horner, M.;
et al. 2007. SEER survival monograph: cancer survival among
adults: US SEER program, 1988-2001, patient and tumor characteristics. National Cancer Institute, SEER Program, NIH Pub (076215): 193–202.
Robert, C.; and Casella, G. 2013. Monte Carlo statistical methods.
Springer Science & Business Media.

Weinberger, K. Q.; and Saul, L. K. 2009. Distance metric learning
for large margin nearest neighbor classification. Journal of Machine Learning Research 10(2).

Zhang, L.; Zhang, C.; Quan, S.; Xiao, H.; Kuang, G.; and Liu, L.
2020. A Class Imbalance Loss for Imbalanced Object Recognition.
IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing 13: 2778–2792.
Zhou, Z.-H.; and Liu, X.-Y. 2005. Training cost-sensitive neural
networks with methods addressing the class imbalance problem.
IEEE Transactions on knowledge and data engineering 18(1): 63–
77.
Zong, W.; Huang, G.-B.; and Chen, Y. 2013. Weighted extreme
learning machine for imbalance learning. Neurocomputing 101:
229–242.

Supplementary Material to “Variational Disentanglement for Rare Event Modeling”
Contents
When the prevalence of an event is extremely low, but the event itself has substantial importance, the methods to identify
such targets are called rare event modeling. Accurate and robust modeling of rare events is significant in many fields, such as
identifying patients in high-risk and hopefully to prevent adverse outcomes from happening based on early intervention.
The scarcity of rare cases can cause extreme imbalanced among the dataset. Therefore, rare event modeling is challenging for
most standard statistical approaches. As we discussed in the main text, careful statistical adjustments and new methodologies are
required to approach such imbalance. Otherwise, the classifiers would be driven to the majority side and give misleading results.
Also, the lack of representation in the minority class may cause unadjusted models to wrongly capturing spurious features that
cannot generalize well to other observations. The apex of the risk curve or the mass of risk density usually overlays with the
tail of the feature representation distribution, as illustrated in Figure S1, traditional statistical methods (such as Gaussian based
approaches) often ill perform at the tail end, which can lead to lack-of-fit and poor generalization ability.
Gaussian

Gaussian
HHavy Tail

risk

Figure S1: Feature representation mismatch at the tail parts. The heavy-tailed distribution can exploit extreme behavior in the
latent space.
We approach a solution to such challenges with a variational representation learning scheme that models disentangled extreme
representations. Further, we design a robust, powerful prediction arm that combines the merits of a generalized additive model
and isotonic neural net.

A. Derivation of Mixed GPD tail distribution
An important theory in Extreme value theory (EVT) shows that under some mild conditions, the conditional cumulative distribution of exceedance over a threshold u follows Generalized Pareto Distribution, GPD(u, ξ, σ) (McFadden 1978), which has
the cumulative distribution function (CDF) as:
(
1
1 − [1 + ξ(x − u)/σ]− ξ , if ξ 6= 0
Gξ,σ,u (x) =
1 − exp[−(x − u)/σ],
if ξ = 0
where σ is a positive scale parameter. According the shape parameter ξ, x could have different support. When ξ < 0, the
exceedance x has bounded support 0 ≤ x ≤ u − σ/ξ, otherwise x is bounded by 0 on the left. u is the location parameter. The
corresponding PDF is:
(
1
σ −1 [1 + ξ(x − u)/σ]− ξ −1 , if ξ 6= 0
gξ,σ,u (x) =
σ −1 exp[−(x − u)/σ],
if ξ = 0
Thus the log-likelihood function is:
(
6 0
− log σ − ( 1ξ + 1) log[1 + ξ(x − u)/σ], if ξ =
log likelihood(x; ξ, σ, u) =
− log σ − (x − u)/σ,
if ξ = 0
To enable modeling of the extreme representations, we adopt the Generalized Pareto Distribution as the tail part of our new
variational prior, and the regular bulk representations z ≤ u with a standard Gaussian distribution. Then mixed extreme tail
distribution has the form (McNeil 1997),
F (z) = P (Z ≤ z) = P (Z ≤ u) + (1 − P (Z ≤ u))Fu (z − u)
When z > u, the tail estimator is,
F̂ (z) = (1 − Fn (u))Gu,ξ,σ,u (z) + Fn (u)

to approximate F (z). Now we show that F̂ (z) also has a GPD distribution with same ξ and the following scale and location
parameters,

σ̃ = σ(1 − Fn (u))ξ , ũ = u − σ̃((1 − Fn (u))−ξ − 1)/ξ, if ξ 6= 0
σ̃ = σ, ũ = u + σ̃ log(1 − Fn (u)),
if ξ = 0
When ξ = 0,
F̂ (z) = (1 − Fn (u))(1 − exp(−(x − u)/σ)) + Fn (u)
= 1 − (1 − Fn (u)) exp(−(x − u)/σ)
= 1 − exp(log(1 − Fn (u))) exp(−(x − u)/σ)
1
= 1 − exp(− (x − u − σ log(1 − Fn (u))))
σ
1
= 1 − exp(− (x − ũ))
σ̃
When ξ 6= 0,
1

F̂ (z) = (1 − Fn (u))(1 − (1 + ξ(x − u)/σ)− ξ ) + Fn (u)
1

= 1 − (1 − Fn (u))(1 + ξ(x − u)/σ)− ξ

1

= 1 − [(1 − Fn (u))−ξ (1 + ξ(x − u)/σ)]− ξ

= (1 − Fn (u))−ξ + (1 − Fn (u))−ξ · ξ(x − u)/σ
1
ξ(x − u)
=
+
(1 − Fn (u))ξ
σ(1 − Fn (u))ξ
σ ξ(x − u)
= +
σ̃
σ̃
σ − σ̃ + ξ(x − ũ)
=1+
σ̃
ξ(x − ũ + ξ −1 σ − ξ −1 σ̃)
=1+
σ̃
Therefore, ũ = ũ − ξ −1 σ − ξ −1 σ̃.

B. Implementation Details
Our main algorithm was written in PyTorch (version 1.3.1) (Paszke et al. 2017). The experiments were conducted on an Intel(R)
Xeon(R) and Tesla P100-PCIE-16GB GPU (except for the COVID dataset). The COVID dataset were stored and analyzed on a
protected virtual network space with Inter(R) Xeon(R) Gold 6152 CPU 2.10GHz 2 Core(s).
Model Structure. In VIE, we end up optimizing the following objective,
max min{Ex,y∼D [Ψβ (x, y; pθ , qφ ) − λΓ(pθ , qφ , ν)]},
θ,φ

ν

(11)

where
Ψβ (x, y; pθ , qφ ) = EZ∼qφ (z|x) [log pθ (y|Z)]
−

βKL[qφ (z|x)||p(z)],

Note that the GPD parameters (ξ, σ) are absorbed in φ, and hyperparameter u is used in the GPD prior p(z). u is set to be
Fz−1 (0.99) in all experiments. When the event rate is ≥ 1%, we set λ, β = (1 × 10−3 , 1 × 10−5 ), otherwise we shrink the
parameters to λ, β = (1 × 10−4 , 1 × 10−6 ).
More concretely, the constituting parts pθ (y|z), pθ (z|x), qφ (z|x) and ν(z) are specified as follows
pθ (y|z) ← Φ(H(z; θ)) Log-Log link (13),
H(z; θ) ← Additive Monotone Neural Net (14) with
p(z) ← Mixed GPD (u, ξp , σp ) (6), p = 4
qφ (z|x) ← Inverse Autoregressive Flow (8), nstep = 5
ν(z) ← Standard neural network.

(12)

Pseudo-code for VIE is presented in Algorithm 2. In all experiments, AMNN, IAF, ν(z) are specified in terms of two-layer
MLPs of 32 hidden units with Rectified Linear Unit (ReLU) activation functions. The initial encoder Init-Encoder is

Algorithm 2: Variational Inference with Extremals.
Data: D = (x, y). x: inputs, y: labels
Networks and parameters: Init-Encoder(x, ; φ): Initial encoder network; IAF(z; φ): recursive autoregressive
neural network; ν(z; ω): critic neural network;
AMNN(z; θ): additive monotonic neural net;
prior: pψ (z) =MixedGPD(z;ψ,u), ψ={ξGPD ,σGPD }
Initialize: Init-Encoder, IAF, ν, AMNN, ψ
for iteration k ∈ {1, . . . , K} do
m
Sample {(xi , yi )}m
i=1 from D, {i }i=1 from p()
[µ0 , σ0 ] =Init-Encoder(x, ; φ)
Sample zpr from pψ (z), z0 from N (µ0 , Σ0 )
Compute lpost := log qφ (z0 |x)
for step t ∈ {1, . . . , T } do
[µt , σt ] =IAF(zt−1
P ; φ), zt = µt + σt zt−1
lpost = lpost − (log σt )
end
log pθ (y|zT ) = `CLL (y,
PAMNN(zT ; θ))
1
Descend ω by ∇ω m
[νω (zpr ) − log νω (zT )]
Ascend
Ω = {φ, ψ, θ} by
P
1
∇Ω m
[log pθ (y|zT ) − log νω (zT ) − KL], where KL = lpost − log pψ (zT )
end

specified as a three-layers MLPs of 32 hidden units. We set the minibatch size to m = 200. The critic network ν(x) uses the
RMSprop optimizer with learning rate 1 × 10−3 , other parts of the algorithm used Adam optimizer with learning rate 1 × 10−4 .
To avoid over-fitting, we set the dimension of latent space as 4 in all experiments.
Note we have used the Complementary Log-Log (CLL) link function for pθ (y|z) in (13),
Φ(a) = 1 − exp(− exp(a)),

(13)

for the outcome model as opposed to the standard Logistic link 1/(1 + exp(−a)). The CLL link is more sensitive at the tail
end, so it is more frequently used in statistical models dealing with vanishing probabilities (Aranda-Ordaz 1981).
To avoid collapsing to suboptimal local minimums, we train the encoder arm more frequently to compensate for the detrimental posterior lagging phenomenon (He et al. 2019). Our pseudo-code for VIE is summarized in Algorithm 1.
z −l

Numerical Integration. Following (5), we divide region [l, zj ] evenly into M bins of width dj = jM , with zj,M = zj . For
r
in each bin. The integral approximation on support [zj,k , zj,k+1 ] is the rectangular
the M bins, we select a random point zj,k
R zj
PM −1
(r)
(r)
area hj (zj,k ) ∗ dj . As a result, the integral 0 h(s; θ)ds is approximated with k
hj (zj,k )dj . With this approximation (5)
can be written as:
p
M
X
X
(r)
H(z; θ) =
αj dj
hj (zj,k ) + γ.
(14)
j

k

We set M = 100 and l = −5 in all the experiments.
Discussions on Evaluation Metrics. In the main text, we reported AUC and AUPRC instead of single evaluation metrics,
e.g., overall accuracy or error rate. Standard statistical metrics like Brier Scores (BS) and Binary classification entropy (BCE)
could be deceptive when the event rate is low, e.g., ≤ 10% (Schmid and Griffith 2014). We will add BCE loss and the positive
case BCE loss in the following sections in the simulation study for reference. Some poorly performed models can have relatively
low BCE scores. In this case, the ground truth (Oracle) is the best reference we have.

C. Ablation Study
We examine model performance on two simulation strategies. The first one is the semi-synthetic dataset, which exploits the
real-world covariates structures. The second one is a synthetic dataset based on our extreme representation assumptions.
Semi-synthetic Datasets We synthesize a semi-synthetic dataset based on the Framingham study (Mitchell et al. 2010), a
long-term cardiovascular survival cohort study. After quality control, 40, 046 subjects with nine covariates (four continuous
and five categorical) are included.
We use a realistic model to synthesize data from the real-world covariates under varying conditions, i.e., different event rates,
sample size, nonlinearity, etc. More specifically, we use the coxPH-Weibull model (Bender, Augustin, and Blettner 2005) to

− log U
simulate the survival time of patients T = { λ exp(g(x))
}1/ν , where g(x) is either a linear function or a randomly initialized
neural net. Our goal is to predict whether the subject will decease within a pre-specified time frame, i.e., T < t0 . Via adjusting
the cut-off threshold t0 , we can simulate different event rates. The details are shown in Algorithm 3.

Algorithm 3: Semi-synthetic Data
Extract covariates from Framingham Dataset ;
Set ν, λ (the parameters of cox-Weibull distribution);
Set time cut-bound t0 ;
Decide g(x) : Rq → R form;
for i ∈ {1, . . . , n} do
Sample ui from Unif(0, 1);
− log ui
}1/ν ;
Compute ti = { λ exp(g(x
i ))
Compute yi = 1[ti < t0 ];
di = (yi , xi );
end
return D = {di ; i = 1 . . . n}
In our experiments, the performances when g(·) set as a randomized neural network or a linear function do not differ very
much. For simplicity, we will present the results under the neural network settings. Apart from the results at 1% event rate given
in the main text, we will show the results at 0.5% and 5% event rates here. The oracle results are calculated with plugging in
the true g(xi ) in Algorithm 3, and the randomness is from the generating scheme of the survival time t.
Additional Results for semi-synthetic datasets In 1% event rate case presented in the main text, the AUC and AUPRC
distributions are summarized in Figure S2, which corresponds to the average and standard deviation values presented in Table 1.
We further examine the cases with 0.5% and 5% event rates to evaluate our method’s robustness. Results are summarized in
AUC distribution at 1% with training size 5000

AUPRC distribution at 1% with training size 5000

VAE

model

VAE-GPD
IAF-GPD
Fenchel
VIE
Oracle
0.45

0.50

0.55

0.60

0.65

0.70

0.75

0.02

0.04

0.06

0.08

0.10

0.12

0.14

0.16

AUC

AUPRC

AUC distribution at 1% with training size 10000

AUPRC distribution at 1% with training size 10000

VAE

model

VAE-GPD
IAF-GPD
Fenchel
VIE
Oracle
0.55

0.60

0.65

0.70

0.75

0.02

0.04

0.06

0.08

0.10

AUC

AUPRC

AUC distribution at 1% with training size 20000

AUPRC distribution at 1% with training size 20000

VAE

model

VAE-GPD
IAF-GPD
Fenchel
VIE
Oracle
0.60

0.62

0.64

0.66

0.68

AUC

0.70

0.72

0.74

0.02

0.04

0.06

0.08

0.10

0.12

0.14

AUPRC

Figure S2: Box plot of 10 independent 1% event rate semi-synthetic analysis.
Table S1 and Table S2 respectively. Apart from the threshold-free metrics AUC and AUPRC, we also presented Binary CrossEntropy loss (BCE) and the BCE loss associated with true events (positive case losses). Note that for an imbalanced dataset,
BCE loss can be misleading. In the model, VAE-GPD, which is poorly-behaved in AUC and AUPRC, can have relatively low
BCE loss since the majority group overwhelms the minority (more important) group. We can refer to the BCE loss and positive

case loss in the oracle results for reference. VIE performs consistently close to the oracle results, especially with low event rate
and small training sample size, and Fenchel-GPD is in the second place.
Table S1: Ablation study of VIE with 0.5% event rate in semi-synthetic settings.

VAE
VAE-GPD
IAF-GPD
Fenchel-GPD
VIE
Oracle

Average AUC (std) ↑
n=5k
n=10k
n=20k
0.494 (0.111) 0.623 (0.102) 0.697 (0.061)
0.560 (0.045) 0.602 (0.044) 0.635 (0.045)
0.631 (0.039) 0.555 (0.042) 0.533 (0.061)
0.615 (0.059) 0.652 (0.055) 0.667 (0.024)
0.654 (0.074) 0.692 (0.076) 0.693 (0.036)
0.688 (0.618, 0.769)

Average AUPRC (std)↑
n=5k
n=10k
n=20k
0.007 (0.005) 0.017 (0.010) 0.020 (0.007)
0.008 (0.002) 0.013 (0.010) 0.016 (0.003)
0.011 (0.007) 0.008 (0.002) 0.017 (0.013)
0.022 (0.016) 0.021 (0.012) 0.025 (0.008)
0.022 (0.010) 0.027 (0.015) 0.024 (0.009)
0.043 (0.023, 0.071)

Average BCE Loss (std) ↓
n=5k
n=10k
n=20k
0.498 (0.309) 0.035 (0.005) 0.031 (0.004)
5.250 (2.096) 0.768 (0.258) 0.152 (0.222)
0.032 (0.003) 0.038 (0.008) 0.043 (0.014)
0.034 (0.004) 0.037 (0.006) 0.033 (0.002)
0.041 (0.018) 0.036 (0.003) 0.032 (0.003)
0.034 (0.028, 0.040)

Average Positive Case Loss (std) ↓
n=5k
n=10k
n=20k
0.010 (0.010) 0.027 (0.005) 0.026 (0.005)
0.000 (0.000) 0.004 (0.001) 0.018 (0.008)
0.027 (0.003) 0.024 (0.001) 0.027 (0.007)
0.028 (0.005) 0.032 (0.006) 0.027 (0.002)
0.026 (0.005) 0.030 (0.005) 0.026 (0.003)
0.029 (0.023, 0.035)

Table S2: Ablation study of VIE with 5% event rate in semi-synthetic settings.

VAE
VAE-GPD
IAF-GPD
Fenchel-GPD
VIE
Oracle

Average AUC (std) ↑
n=5k
n=10k
n=20k
0.594 (0.118) 0.666 (0.021) 0.693 (0.011)
0.583 (0.027) 0.607 (0.014) 0.663 (0.009)
0.664 (0.017) 0.554 (0.033) 0.503 (0.020)
0.666 (0.014) 0.687 (0.016) 0.681 (0.008)
0.679 (0.018) 0.693 (0.027) 0.693 (0.015)
0.694 (0.670, 0.717)

Average AUPRC (std)↑
n=5k
n=10k
n=20k
0.113 (0.049) 0.144 (0.017) 0.179 (0.018)
0.075 (0.008) 0.087 (0.013) 0.137 (0.024)
0.113 (0.022) 0.063 (0.007) 0.057 (0.003)
0.145 (0.011) 0.184 (0.022) 0.166 (0.013)
0.142 (0.018) 0.172 (0.032) 0.193(0.013)
0.197 (0.179, 0.218)

Average BCE Loss (std) ↓
n=5k
n=10k
n=20k
0.308 (0.166) 0.198 (0.004) 0.198 (0.009)
2.106 (1.553) 0.581 (0.056) 0.195 (0.011)
0.194 (0.008) 0.208 (0.004) 0.214 (0.009)
0.196 (0.008) 0.189 (0.007) 0.190 (0.011)
0.188 (0.011) 0.193 (0.005) 0.190 (0.006)
0.185 (0.179, 0.198)

Average Positive Case Loss (std) ↓
n=5k
n=10k
n=20k
0.111 (0.046) 0.147 (0.003) 0.149 (0.008)
0.017 (0.015) 0.043 (0.004) 0.143 (0.013)
0.144 (0.009) 0.157 (0.004) 0.160 (0.008)
0.148 (0.008) 0.141 (0.007) 0.141 (0.012)
0.139 (0.013) 0.145 (0.006) 0.139 (0.008)
0.137 (0.130, 0.149)

Long-tailed Synthetic Datasets We design the long-tailed synthetic datasets based on our proposed method, where the latent
variable z enjoys a long-tailed distribution. The pseudo-code for this simulation strategy is shown in Algorithm 4, where t0 is
a pre-specified time-cut, and H(·) is a randomized monotone neural network to create a monotone mapping from z to the risk.
Algorithm 4: Generation of long-tailed data.
Set sample size n, latent space dimension p, number of covariates q ;
Set µp , Σp , ξp , σp (the parameters of a long-tailed distribution);
Set ν, λ (the parameters of cox-Weibull distribution);
Set time cut-bound t0 ;
Initialize ψ (for MLP g(·; ψ) : Rp → Rq ), θ (for AMNN H(·; θ) : Rp → R);
for i ∈ {1, . . . , n} do
Sample zi from mixed GPD (µp , Σp , ξp , σp );
Compute xi = g(zi ; ψ);
Sample ui from Unif(0, 1);
− log ui
Compute ti = { λ exp(H(z
}1/ν ;
i ;θ))
Compute yi = I(ti < t0 );
di = (yi , xi )
end
;
return D = {di ; i = 1 . . . n}
Table S3 summarizes the findings with the long-tailed distributed latent space datasets, with 1% event rate. VIE can achieve
relatively high AUC and AUPRC even with a small training sample size, which suggests that the proposed method can recover
the long-tailed behavior in the feature representation. Among the combinations of different VI techniques, the Fenchel duality
mechanism facilitates the distribution matching the best among other inference techniques.
Table S3: Ablation study of VIE with 1% event rate in longtailed-synthetic settings
VAE
VAE-GPD
IAF-GPD
Fenchel-GPD
VIE
Oracle

Average AUC (std) ↑
n=5k
n=10k
n=20k
0.722 (0.140) 0.741 (0.099) 0.798 (0.034)
0.498 (0.039) 0.450 (0.021) 0.441 (0.055)
0.688 (0.021) 0.632 (0.037) 0.555 (0.039)
0.804 (0.028) 0.807 (0.026) 0.818 (0.020)
0.823 (0.024) 0.810 (0.023) 0.836 (0.026)
0.829 (0.802, 0.868)

Average AUPRC (std)↑
n=5k
n=10k
n=20k
0.128 (0.076) 0.119 (0.064) 0.177 (0.034)
0.013 (0.002) 0.009 (0.000) 0.009 (0.002)
0.097 (0.019) 0.078 (0.027) 0.046 (0.014)
0.174 (0.030) 0.155 (0.052) 0.166 (0.044)
0.175 (0.036) 0.163 (0.044) 0.202 (0.024)
0.188 (0.153, 0.243)

Average BCE Loss (std) ↓
n=5k
n=10k
n=20k
0.138 (0.213) 0.121 (0.200)
0.055 (0.004)
12.097 (5.582) 22.445 (10.247) 13.438 (7.159)
0.051 (0.005) 0.055 (0.003)
0.062 (0.007)
0.054 (0.007) 0.051 (0.003)
0.047 (0.004)
0.047 (0.005) 0.050 (0.003)
0.049 (0.005)
0.049 (0.042, 0.055)

Average Positive Case Loss (std) ↓
n=5k
n=10k
n=20k
0.029 (0.010) 0.034 (0.009) 0.039 (0.005)
0.000 (0.000) 0.000 (0.000) 0.001 (0.002)
0.040 (0.005) 0.044 (0.004) 0.051 (0.007)
0.041 (0.007) 0.040 (0.004) 0.037 (0.005)
0.037 (0.004) 0.040 (0.003) 0.037 (0.005)
0.039 (0.033, 0.045)

In summary, we have tested the performance on various simulation settings (model assumptions, event rates, sample sizes,
non-linearity, etc.) where VIE takes the lead in all cases. IAF- and GPD-only variants perform poorly, even not comparable to
the vanilla VAE model. This is possibly due to the prior is not matched. Explicitly matching the prior via the Fenchel mini-max

scheme improves the performance, especially in the long-tailed representation datasets. Stacked together, our full proposal of
VIE consistently outperforms its variants and always approaching the oracle performance in the large sample regime.

D. Real-world datasets
We consider 5 real-world datasets, including 3 survival datasets in the study. Among those dataset, COVID and InP are from
Duke University Health System (DUHS), which are not public at this time. SEER(Ries et al. 2007) and SLEEP(Quan et al.
1997) are two public survival datasets. Besides above clinical-based datasets, we further evaluate the model performance on
Fraud dataset (Dal Pozzolo et al. 2017) in this supplementary material.
Baseline Models. In all experiments, LDAM, FOCAL, IW, DeepSVDD and MLP are specified in terms of three-layer MLPs
of 32 hidden units with ReLU activation. When tuning parameters for LASSO, based on the notation of Pedregosa et al. (2011)
function sklearn.linear model.Lasso, we choose α from [10−5 , 10−4 , 10−3 , 10−2 , 0.1, 0.2, 0.5, 0.8] referred to the
best performance on the validation datasets. In Focal Loss, the parameter γ is selected from the list [0.1, 0.5, 1.0, 1.5, 2.0] based
on the best performance on the validation datasets.
COVID Dataset The dataset includes inpatient encounters to DUHS as of January 1, 2020. Vitals, administered medications,
lab results, comorbidities, etc. are used as predictors to identify the risk of inpatient death, ventilation, and ICU transfer as
adverse outcomes. The raw data’s detailed description for each group of covariates can be found in Table S4. The mortality rate
in this dataset is 2.8%, ventilation 7.8% and ICU transfer 18%. From rare event modeling purposes, apart from the mortality
prediction, we set the group of patients who experienced either death or ventilation as the combined adverse outcome group,
which has 8% event rate.
Table S4: Raw COVID dataset covariates before pre-processing.
Data Name

Data Type

Number Covariates

Demographics
Previous Encounters
Prior Procedures w/n year
Problem List w/n year
Comorbidities w/n year
Chief Complaint
Lab Analytes Collected
Lab Analytes Results
Orders Placed
Medications Administered
Vitals Recorded

Numerical
Numerical
Categorical
Categorical
Categorical
Categorical
Categorical
Numerical
Categorical
Categorical
Numerical

1 (age)
2
186
273
545
100
44
44
32
74
37

In Figure S3, we presented the comparison of VIE versus other baseline models with the two outcomes (combined and
mortality). VIE shows strong performance under these metrics.
0.925

model

0.6

Focal
LASSO
LDAM
VIE

0.900
0.5

AUPRC

AUC

0.875
0.850
0.825

0.4
0.3

0.800
0.2

0.775

0.1
mortality&ventilation/8%

mortality/2.6%

Outcomes/ Event Rate (%)

mortality&ventilation/8%

mortality/2.6%

Outcomes/ Event Rate (%)

Figure S3: Bootstrapped AUC (left) and AUPRC (right) Distribution of COVID datset with different outcomes. Note that
comparisons of AUPRC among event-rates groups are meaningless.
Cross-validation results To qualitatively show the superior performance of VIE, we examine the performances on a 5-fold
cross validation of the COVID dataset for mortality prediction. VIE outperforms other baseline consistentlly on each fold.

Comparing to the performance of the second-best model LDAM with a paired t test, the p-value yields 0.093 < 0.1, with effect
size 0.98, which shows the performance gap is statistically significant at α = 0.1.
Table S5: 5-Fold cross validation results for COVID-19 dataset

AUC

AUPRC

k-Fold

1

2

3

4

5

1

2

3

4

5

LASSO
VAE
MLP
Focal
LDAM
VIE

0.845
0.831
0.852
0.847
0.842
0.860

0.834
0.842
0.836
0.837
0.848
0.849

0.819
0.848
0.845
0.836
0.843
0.851

0.818
0.800
0.852
0.851
0.839
0.865

0.817
0.764
0.821
0.836
0.839
0.840

0.234
0.198
0.248
0.229
0.240
0.263

0.183
0.181
0.174
0.158
0.197
0.210

0.213
0.191
0.236
0.216
0.234
0.238

0.185
0.196
0.235
0.241
0.209
0.256

0.181
0.147
0.182
0.186
0.182
0.201

InP Dataset The dataset is another inpatient data of 82,450 Duke University Health System (DUHS) collected between 20142016. We abstracted time-varying clinical data (i.e., vital signs, laboratory tests, medications) and followed patients until the
Intensive Care Unit (ICU) transfer or Death. We extracted their first encounter in the system (the admission) to generate this
classification study to predict the risk of the occurrence of adverse outcomes (death or ICU transfer). The descriptions of raw
data can be found in Table S6. With different sizes of the time windows, we generate four classification datasets with different
Table S6: Raw InP dataset covariates before pre-processing
Data Name

Data Type

Number Covariates

Demographics
Admission Information
Vitals Recorded
Lab Analytes Collected
Lab Analytes Results
Lab Orders Placed

Numerical
Categorical
Numerical
Categorical
Numerical
Categorical

3 (age, sex, race)
2 (source and department)
10 (Diastolic, Resp, SpO2, etc.)
30
30
30

Time Window (hours)/ Event Rate (%)

event rates. As summarized in Figure S4, VIE takes a consistent lead in both AUC and AUPRC. The advantage enlarges when
the event rates drop. Note that the trend of AUPRC when event rate shrinking is not meaningful(Boyd et al. 2012).

168h/5%

48h/3%

24h/2%

Focal
LASSO
LDAM
VIE

12h/1%

0.70

0.75

0.80

AUC

0.85

0.90

0.05

0.10

0.15

0.20

0.25

0.30

AUPRC

Figure S4: Bootstrapped AUC (left) and AUPRC (right) Distribution of InP datset with different event rate. Note that comparisons of AUPRC among event-rates groups are meaningless.
SEER and SLEEP Datasets SEER and SLEEP are two public survival datasets that contain censoring (i.e., an event that
is not reported during the follow-up period of a subject). To create a classification dataset from a survival dataset, we deleted
patients censored before the time-cut. The proportion of subjects excluded for SEER is less than 0.1%, for SLEEP dataset is
less than 0.2%, which should not affect the overall credibility of the analysis. We follow the pre-processing steps provided in
Chapfuwa et al. (2020).

Bootstrapping of AUC by time of SEER dataset

Bootstrapping of AUPRC by time of SEER dataset
Focal
LASSO
LDAM
VIE

0.40
0.92
0.35
0.90
0.30
0.88
0.25
0.86
0.20
0.84
0.15
0.82
0.10
0.80
11m/5%

3m/1%

11m/5%

Time Window (months)/ Event Rate (%)

3m/1%

Time Window (months)/ Event Rate (%)

Figure S5: Bootstrapped AUC (left) and AUPRC (right) Distribution of SEER dataset with different event rate.
DeepSVDD

Methods

IW
Focal
LASSO
MLP
LDAM
VIE
0.5

0.6

0.7

0.8

0.9

0.05

0.10

0.15

AUC Score

0.20

0.25

0.30

0.35

0.40

AUPRC Score

Figure S6: Bootstrapped AUC (left) and AUPRC (right) Distribution of SLEEP dataset with 5% event rate.
Credit Card Fraud Detection To evaluate the performances on non-clinical data, we examined the VIE model on fraud
detection benchmark dataset (Dal Pozzolo et al. 2017), where fraudulent credit card transactions are coined as rare events
(∼ 0.2%). The dataset includes 284k records with 29 covariates. We split the original dataset into training, validation, and
testing datasets with a 6:2:2 ratio to ensure fair and stable comparison. The hyperparameters are selected based on the best
performance on the validation dataset. The average and standard deviation of the metrics are presented in Table S7. VIE
outperforms other baselines and achieved an average of over 0.99 AUC in the bootstrapped samples.
Table S7: Fraud transaction classification.

AUC
AUPRC

Lasso

MLP

DeepSVDD

IW

Focal

LDAM

VIE

0.981 (0.006)
0.79 (0.032)

0.984 (0.007)
0.80 (0.030)

0.796 (0.019)
0.01 (0.002)

0.777 (0.026)
0.57 (0.039)

0.916 (0.020)
0.79 (0.032)

0.987 (0.005)
0.78 (0.037)

0.991 (0.003)
0.80 (0.032)

Exploration of the feature representation We visualize the marginal relationship between latent space dimensions and risk
in the real-world dataset InP dataset (1% event rate), which are shown in Figure S7. The first dimension (top-left), the extremal
behavior contributes significantly and positively to the event risk prediction. The other three dimensions serve as inhibitors to
the event risk. Empirically, all the latent dimensions have a long-tailed distribution, with learned scale parameter ξ > 0.
We also embed the posterior space z on a 2D plot with t-SNE, with probability contour lines, as shown in Figure S8. The
events are concentrated to one end of the latent space.

E. Generalized to Multiple-class classification
Our binary classification framework can be generalized to multiple-class problems easily. We will stick to the mixed-GPD
distribution of the posterior z with p dimensions, and increase the number of monotone networks for each dimension of z. In
the binary case, each dimension of z corresponds to one monotone network, here we can set it to k networks per-dimension. In
total, we now have p × k monotone functions. Then we can apply an FC layer to the final output, with m categories, as shown
in Figure S9. In the learning object, we would replace the binary cross-entropy loss (BCE) with regular cross-entropy loss (CE)
in the reconstruction term.

(a) Learned prior and posterior distribution and monotonic(b) The latent representation values distribution grouped by
event type
predicted risks

Figure S7: Four latent dimensions from the InP dataset (1%) event rate, where the extreme distribution in the first dimension
is the simulator to the events, the other three dimensions serve as inhibitors

non-event
event

20
10
0
−10
−20
−30
−40

−20

0

20

40

Figure S8: t-SNE plots with latent representation z.

pθ (y|z) ← Φ(H(z; θ)) Soft-max ,
H(z; θ) ← k Additive Monotone Neural Nets (14)
p(z) ← Mixed GPD (u, ξp , σp ) (6),
qφ (z|x) ← Inverse Autoregressive Flow (8),
ν(z) ← Standard neural network.

(15)

We generate a toy dataset to illustrate VIE’s performance on multiclassification problems. Based on Algorithm 3,
instead of setting a binary time-cut, now we split the generated time t with a sequence of time-cuts based on the
percentiles [5%, 15%, 30%, 60%] of t. In this way, we have a dataset with 5 categorical outcomes with event rates
[5%, 10%, 15%, 30%, 40%], respectively. To evaluate the performance, except for the per-class accuracy, we use F 1 score,
2
which is a the harmonic mean of precision (True Positives) and recall (sensitivity), recall−1 +precision
−1 , ranges from 0 to 1, where
1 indicating better performance. We use micro-averaged F1-score (micro-F1) to calculate the overall F1 scores for all classes,
Comparing to related methods: FOCAL and LDAM, the model VIE results on this toy example are comparable per class and
better in terms of F1 score. FOCAL and LDAM are specified as 3-layer MLPs with 32 hidden units, and VIE uses the previous
setting, except for k = 3.

𝑝(𝑦|𝑥)

𝑥!
𝑥"
𝑥…
𝑥$

𝑧#

𝑧"

𝑧#

𝑧"

𝑞! (𝑧|𝑥, ϵ)

𝑝& (𝑦|𝑧)
𝑧$

𝑦=2

𝑧%

𝑧$

𝑧%

𝑦=1

𝑦=⋯

𝑦=𝑐

Figure S9: Illustration of multi-classification framework.
Table S8: Performance on few-shots learning dataset
class 1

class 2

class 3

class 4

class 5

event rates

5%

10%

15%

30%

40%

Focal
LDAM
VIE

0.503
0.012
0.054

0.202
0.321
0.054

0.095
0.228
0.046

0.139
0.165
0.372

0.089
0.594
0.823

micro-F1
0.1368
0.3522
0.4521

SM References
Chapfuwa, Paidamoyo, Chunyuan Li, Nikhil Mehta, Lawrence Carin and Ricardo Henao. 2020. Survival cluster analysis. In
Proceedings of the ACM Conference on Health, Inference, and Learning. pp. 60–68.
Dal Pozzolo, Andrea, Giacomo Boracchi, Olivier Caelen, Cesare Alippi and Gianluca Bontempi. 2017. “Credit card fraud
detection: a realistic modeling and a novel learning strategy.” IEEE transactions on neural networks and learning systems
29(8):3784–3797.
McFadden, Daniel. 1978. “Modeling the choice of residential location.” Transportation Research Record (673).
McNeil, Alexander J. 1997. “Estimating the tails of loss severity distributions using extreme value theory.” ASTIN Bulletin:
The Journal of the IAA 27(1):117–137.
Paszke, Adam, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison,
Luca Antiga and Adam Lerer. 2017. “Automatic differentiation in PyTorch.”.
Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot and E. Duchesnay. 2011. “Scikit-learn: Machine Learning in
Python.” Journal of Machine Learning Research 12:2825–2830.
Quan, Stuart F, Barbara V Howard, Conrad Iber, James P Kiley, F Javier Nieto, George T O’Connor, David M Rapoport, Susan
Redline, John Robbins, Jonathan M Samet et al. 1997. “The sleep heart health study: design, rationale, and methods.” Sleep
20(12):1077–1085.
Ries, LA Gloeckler, JL Young, GE Keel, MP Eisner, YD Lin, MJ Horner et al. 2007. “SEER survival monograph: cancer
survival among adults: US SEER program, 1988-2001, patient and tumor characteristics.” National Cancer Institute, SEER
Program, NIH Pub (07-6215):193–202.
Schmid, Christopher H and John L Griffith. 2014. “Multivariate classification rules: calibration and discrimination.” Wiley
StatsRef: Statistics Reference Online .

