VacSIM: L EARNING E FFECTIVE S TRATEGIES FOR COVID-19
VACCINE D ISTRIBUTION USING R EINFORCEMENT L EARNING
A P REPRINT

arXiv:2009.06602v2 [cs.AI] 18 Jan 2021

Raghav Awasthi1 , Keerat Kaur Guliani2 , Saif Ahmad Khan1 , Aniket Vashishtha5 , Mehrab Singh Gill1 , Arshita
Bhatt3 , Aditya Nagori4 , Aniket Gupta1 , Ponnurangam Kumaraguru1 , and Tavpritesh Sethi1
1

Indraprastha Institute of Information Technology Delhi
2
Indian Institute of Technology Roorkee
3
Bhagwan Parshuram Institute of Technology, New Delhi
4
CSIR-Institute of genomics and integrative biology, New Delhi
5
Maharaja Surajmal Institute of Technology, New Delhi

January 19, 2021

1

Abstract

A COVID-19 vaccine is our best bet for mitigating the ongoing onslaught of the pandemic. However, vaccine is also
expected to be a limited resource. An optimal allocation strategy, especially in countries with access inequities and
temporal separation of hot-spots, might be an effective way of halting the disease spread. We approach this problem
by proposing a novel pipeline VacSIM that dovetails Sequential Decision based RL models into a Contextual Bandits
approach for optimizing the distribution of COVID-19 vaccine. Whereas the Reinforcement Learning models suggest
better actions and rewards, Contextual Bandits allow online modifications that may need to be implemented on a dayto-day basis in the real world scenario. We evaluate this framework against a naive allocation approach of distributing
vaccine proportional to the incidence of COVID-19 cases in five different States across India and demonstrate up to
9039 additional lives potentially saved and a significant increase in the efficacy of limiting the spread over a period of
45 days through the VacSIM approach. We also propose novel evaluation strategies including standard compartmental
model-based projections and a causality preserving evaluation of our model. Finally, we contribute a new Open-AI
environment meant for the vaccine distribution scenario and open-source VacSIM for wide testing and applications
across the globe(http://vacsim.tavlab.iiitd.edu.in:8000/).
Content Areas: COVID-19, Vaccine Distribution, Policy Modeling, Reinforcement Learning, Contextual Bandits
Problem

2

Introduction

Vaccines have played a crucial role in combating infectious diseases for hundreds of years[1]. The successful eradication of smallpox, a transmittable disease where a majority of cases ended in deaths, happened due to the gradual and
effective widespread use of vaccines. [2] Avoidance of millions of deaths and side effects caused by diseases like polio,
diptheria and tetanus because of vaccination efforts prove their impact and significance for global health. Despite this
impact, if such a vital resource is not optimally distributed and allocated to the regions and communities where its
need is the most, this can lead to huge repercussions that could aggravate the situation. All countries across the globe
are eagerly waiting for the launch of an effective vaccine against SARS-CoV-2. The Operation Warp Speed[3] aims
to deliver 300 million doses of a safe, effective vaccine for COVID-19 by January 2021; however, the pace of development continues to be punctuated by the safety concerns [4]. As potential candidates start getting ready to enter the
market, there will be an urgent need for optimal distribution strategies that would mitigate the pandemic at the fastest
rate possible [5] [6]. Center for American Progress estimated that 462 million doses of COVID-19 vaccine along with
accompanying syringes, would be needed for the US alone to reach herd immunity [7]. Here we summarize the key
factors that will need to be considered for effective mitigation:

A PREPRINT - JANUARY 19, 2021

• Scarcity of supply: Despite large scale production efforts, it is expected that the vaccine will still be a scarce
resource as compared to the number of people who would need it. In addition to the vaccine itself, there may
also be scarcity in the components leading to its delivery, e.g. syringes. The White House director of trade
and manufacturing policy stated that the US would need 850 million syringes to deliver the vaccine en-masse.
This highlights the next challenge of the optimal distribution of scarce resources related to the vaccine.
• Equitable distribution: A truly equitable distribution will not just be defined by the population or incidence
of new cases alone, although these will be strong factors. Other factors ensuring equity of distribution include
quantum of exposure, e.g. to the healthcare workforce that needs to be protected. In this paper, we assume that
the exposure is proportional to the number of cases itself, although the proposed methodology allows more
nuanced models to be constructed. There may also be unseen factors such as vaccine hoarding and political
influence, which are not accounted for in this work.
• Transparent, measurable and effective policy: The design of policy would need to be guided by data,
before, during and after the vaccine administration to a section of the population. Since the viral dynamics
are rapidly evolving, the policy should allow changes to be transparent and effects to be measurable in order
to ensure maximum efficacy of the scarce vaccine resource. On the larger scale of states and nations, this
would imply continuous monitoring of incidence rates vis-a-vis a policy action undertaken.
Although the aforementioned factors seem straightforward, the resulting dynamics that may emerge during the actual
roll-out of the vaccine may be far too complex for human decision making. The daunting nature of such decisionmaking can be easily imagined for large and diverse countries such as India and the United States, especially where
Health is a State subject. Artificial intelligence for learning data-driven policies is expected to aid such decision
making as there would be limited means to identify optimal actions in real-world. A ‘near real-time’ evaluation as per
the demographic layout of states and consequent initiation of a rapid response to contain the spread of COVID-19 [8]
will be required. Furthermore, these policies will need to be contextualized to the many variables governing demand
or ‘need’ for the vaccine distribution to be fair and equitable[9]. Therefore, ground testing of these scenarios is not an
option, and countries will have to face this challenge. In this paper, we introduce VacSIM, a novel feed-forward
reinforcement learning approach for learning effective policy combined with near real-time optimization of
vaccine distribution and demonstrate its potential benefit if applied to five States across India. Since real-world
experimentation was out of the question, the change in projected cases obtained via a standard epidemiological model
was used to compare the VacSIM policy with a naive approach of incidence-based allocation. Finally, our novel model
is open-sourced and can be easily deployed by policymakers and researchers, thus can be used in any part of the world,
by anyone, to make the process of distribution more transparent.

3

Methods

To get an optimal distribution of vaccine, we proposed a novel pipeline where we adjoined ACKTR/DQL model with
Contextual Bandits model in a feed-forward way which makes the optimization process more robust, context-specific
and easily deployable in an online way.
3.1

Deep Q Learning

DQL[10] [11] is a Reinforcement Learning which depends on the Q function or the state-action value function. Given
that an agent follows a policy π, then Qπ (s,a) denotes the expected return or reward obtained by the agent if it chooses
action a at state s and then keeps on following the policy π. Q(s,a) estimates how favourable it is to choose action
a at state s according to the current policy. The optimal Q function, Q*(s, a) denotes the maximum return or reward
that can be achieved by first choosing action a at state s and then following the optimal policy. Q-Learning uses the
Bellman Equation for iterative updating.
Q∗ (s, a) = E[r + γmaxa0 Q∗ (s0 , a0 )]

(1)

where
r is the immediate reward
γ is the discount factor
As evident from the equation, Q*(s, a) is the sum of the immediate reward and the maximum reward possible thereafter
from state s’ discounted by γ which is same as γ discounted maximum Q value for state s’.
2

A PREPRINT - JANUARY 19, 2021

Q-Learning becomes impractical for large environments. However, Deep Q Learning uses neural networks for estimating the Q-values and minimizes the loss at each iteration i as given below.
Li (θi ) = Es,a,r,s0 ∼ ρ(.) [((yi − Q(s, a; θi ))2 ]

(2)

yi = r + γmaxa0 Q(s0 , a0 ; θi−1 )

(3)

where
yi is the Temporal Difference (TD) target
yi - Q is the TD error
ρ represents behaviour distribution
3.2

Actor-Critic using Kronecker-Factored Trust Region (ACKTR)

ACKTR is a scalable and sample efficient algorithm to apply the natural gradient method to policy gradient for actorcritic methods. Proposed in [12], it is used to calculate and apply the natural gradient update to both actor and critic.
It uses a critic to estimate the advantage function. Training the model amounts to solving the least-squares problem
of minimizing the mean squared error (MSE) between the model predictions and the target values, i.e., (r(x) =
estimated(x) − target(x)).
3.3

Contextual Bandits problem

The Contextual Bandits algorithm is an extension of the multi-armed bandits approach [13] which contextualizes the
choice of the bandit to its current environment. This serves to circumvent the problem where a multi-armed bandit
may simply end up playing the same action multiple times even though the environment (context) may have changed,
thus getting stuck at a sub-optimal condition. Contextual Bandits play an action based on its current context, given a
corresponding reward, hence are more relevant to real-world environments such as the vaccine distribution problem
attacked in this work. Given, for time t = 1...n, a set of contexts C and a set of possible actions X and reward/payoffs
P are defined. At a particular instant, based on a context ct ∈ C, an action xt ∈ X is chosen and a reward or a payoff
pt = f (ct , xt ) is obtained. Regret[14] is a conceptual function to understand and optimize the performance of the
Bandits problem. Since we don’t know if an action played was the most ‘reward-fetching’, rewards against all actions
that can be played are sampled, and the difference between the action chosen and the action against the maximum
reward is defined as ‘regret’. Therefore, minimizing regret achieves the goal of maximizing reward. For an optimal
action x∗ ∈ X such that the expectation of reward against this action is maximum, P
p∗ = maxxt ∈X (E(p | xt )), the
n
∗
∗
regret and cumulative regret can be expressed as Z = [p − E(p | xt )] and Z = t=1 Z respectively. Contextual
Bandits model was implemented using the Python Package of Vowpal Wabbit[15].
3.4

VacSIM: A Feed forward Approach

In this paper, we concatenate the Reinforcement Learning sub-models (i.e. ACKTR and DQN) and Contextual Bandits
sub-model in a feedforward manner. This was incorporated to select the optimal policy through the Contextual Bandits
approach from the ones generated by the Reinforcement Learning-based models, as shown in Figure 1. This was done
to address the following challenges that need to be tackled in real-world problems such as optimal vaccine distribution:
• Solving in real-time: A vaccine distribution problem is expected to be fast-paced. Thus an overwhelming
amount of brainstorming with constrained resources and time would be required to develop an effective policy
for the near future. This calls for the development of a prompt and an easily reproducible setup.
• Lack of ground truth: This is one of the key challenges addressed in this paper. Since the roll-out of the
vaccine will not give us the liberty of testing various hypotheses, a lack of ground truth data is generally
expected. This is analogous to zero-shot learning problems, thus precluding a simple supervised learningbased approach.
• Absence of evaluation with certainty: Lack of ground testing naturally implies nil on-ground evaluation.
In that case, it often becomes challenging to employ evaluation metrics that offer a significant amount of
confidence in results. In order to solve this problem, we rely upon the simulated decrease in the number of
susceptible people as the vaccine is administered.
• Model scaling: We ensured that the learning process of the models simulates the relationship between different objects in the real world environment accurately, and at the same time can be scaled down in response
3

A PREPRINT - JANUARY 19, 2021

to computational efficiency and resource utilization challenges. This is done by choosing the right set of
assumptions that reflect the real world.

Figure 1: VacSIM architecture: A novel feed forwarded pipeline for policy learning of COVID-19 vaccine
distribution. To make decisions online through Contextual Bandit Problem, reward and action obtained from the
DQN model were fed forward into the training of Contextual Bandit learning. Optimal online decisions can be
calculated by trained Contextual Bandit problems.
While reinforcement learning approaches replicate human decision-making processes, the absence of evaluation makes
them less trustworthy, especially when real lives may be at stake. Therefore, we pipelined the Reinforcement Learning
models with a Contextual Bandits approach where recommendations for vaccine distribution policy were used as
training data for the latter.

4

Results

Figure 2: (Left) Statewise confirmed cases for the 5 States (Right) Heatmap showing the distribution of the
Context in each State.

4

A PREPRINT - JANUARY 19, 2021

Figure 3: Flexible model setup for optimization of vaccine distribution policy in India using the VacSIM approach
where components can be easily replaced with alternatives and adopted for different diverse use cases.

4.1

Model Setup

We extracted the State-wise time series data of COVID-19 characteristics from the website https://mohfw.gov.in/ and
used them in the experiments as described below.
The five States chosen for this study, i.e., Assam, Delhi, Jharkhand, Maharashtra and Nagaland are representative
of possible scenarios for the vaccine distribution problem, i.e., high incidence(Maharashtra and Delhi), moderate
incidence (Assam) and low incidence (Jharkhand and Nagaland). In choosing the five different States, we hope to
generalize our predictions to other States across the spectrum while minimizing the bias introduced into the learning
by a widely variant COVID-19 incidence across the country.
In order to avail vaccine most volunerbale population and to combat covid -19 infection spread we selected following
contexts for our models:
Death Rate: The percentage ratio of the predicted deaths in the State to the total predicted cases in that State
calculated using projections obtained from a fitted standard Compartmental model, i.e. a Susceptible, Exposed,
Infected and Recovered(SEIR) model.
Recovery Rate: The percentage ratio of the predicted recoveries in the State to the total predicted cases in that State
using projections obtained from the SEIR model.
Population of a State: We extracted population of a state from the 2010 census data conducted by the government of
India. [16]
Susceptible Cases of a State: We estimated susceptible population as the difference between the population of a
particular state and total number of predicted infected cases.
Hospital Facilities in a State: We used overall Hospital Beds, overall ICU Beds and Ventilators data in our
models[17].

5

A PREPRINT - JANUARY 19, 2021

S.No.

Hyperparameter Name

Hyperparameter Value

1
2
3
4
5
6

Batch Size: number of vials in one round of distribution. 1000000
Exploration Rate of ACKTR
40%
Exploration Rate of DQN
10%
Vaccine efficacy
100%
Bucket size
1000
Number of recipients per day
5
Table 1: Hyper-parameters used during Policy learning

Age Distribution of a State: In order to prioritize vulnerable population we considered people with age greater than
50.
The implementation of VacSIM pipeline is detailed henceforth.
4.2

Policy Gradient Models

Open-AI[18] stable-baselines framework was used to construct a novel and relevant environment suited to our
problem statement for ACKTR and DQN to learn in this environment.
a. Input: With the observation space being declared as continuous and action space being declared as discrete. A
context vector describing the situation of the state in terms of Total Predicted Cases, Predicted Death Rate, Predicted
Recovery Rate, Susceptible cases, Population, Overall ICU Beds, Overall Hospital Beds, Ventilators and Age
distribution at a given time, was fed as input. Predictions were obtained from SEIR projections[19].
The action space is discrete with the number of actions equal to bucket size, which is considered a hyper-parameter.
We set the bucket size equal to 1000.
b. Training: Following are the assumptions used while building the environment:
• The nature of the vaccine is purely preventative and NOT curative, i.e., it introduces immunity when administered to a susceptible person against future COVID infection but plays no role in fighting against existing
infections (if any).
• The vaccine has 100% efficacy, i.e. all people who are vaccinated will necessarily develop immunity against
future SARS-CoV-2 infection. This assumption is easily modifiable and is expected to have little consequence
in deciding the optimal allocation unless data reveal differential immunity response of certain populations
within India. However, we leave scope for it to be modified as per the situation by taking this as a hyperparameter for VacSIM.
Reward Function: The reward function was designed to maximize the decrease in the susceptible population count
with the minimum amount of vaccine, considering it to be a limited resource.
Ri = exp(−(Ai − Si )2 /10−4 )

(4)

where
Ri is the reward given by the environment
Si is the fraction of susceptible population in a particular state
Ai is the fraction of vaccine distributed to the state by the model
The Explore-Exploit Tradeoff: We tap into the explore-vs-exploit dilemma, allowing the model to reassess its
approach with ample opportunities and accordingly, redirect the learning process. We set the exploration rate at 10%
for DQN and 40% for ACKTR. However, this too is flexible and can be treated as a hyper-parameter.
In Figure 4, increasing trend of reward shows that the models are learning well.
Hyper-parameters: A complete list of the hyper-parameters is given in Table 1.

6

A PREPRINT - JANUARY 19, 2021

c. Output: The output of the first sub-model was a distribution set dictating the share of each recipient in the batch of
vaccines dispatched at the time of delivery.

4.3

Contextual Bandits Model

The output of the first sub-model is spanned over 26 days (1 December-26 December 2020). The distribution sets so
obtained were scaled to buckets ranging from 100 to 1000 with an increment step of 100. The actions of the five States
associated with each bucket size were normalized to get the percentage distribution ratio for all States. Normalized
here refers to the percentage ratio of a given distribution set of a State for a given bucket size and the sum of the
distribution sets of the 5 States over that bucket size for a given date.
a. Training: We considered 1 December-26 December data along with distribution sets and the corresponding set of
rewards obtained from the RL models as training data for Contextual Bandits.
b. Number of actions and features: The action space in this model is taken as Discrete with the number of possible
actions equal to bucket size. The total number of vials is divided among each of these buckets. Each state is assigned
a number of buckets in the range [0, bucket size-1] during the distribution process. The percentage of vaccine allotted
to a particular state is then calculated from the number of buckets assigned to that state. The features in the Context
were the same as those in sub-model 1.
c. Testing: Using the context, actions and the corresponding set of rewards as the training dataset, we tested the model
day-wise for a period of six days (26 December-31 December) with each day having 10 possible bucket sizes, i.e. 100
to 1000 for each State as output.
d. Output: The unadjusted actions (which were not normalized) obtained after testing the model were first adjusted
bucket-wise for each State for a particular date by taking the percentage ratio of the unadjusted action of that State
and the sum of unadjusted actions of all five States.
We consider that the vaccine is distributed on 31 December 2020. The consequent distribution of vaccines among the
five States for each bucket size was then evaluated. Our models learn well for the bucket range 200-500. The vaccine
distribution for the same in case of the VacSIM policies is shown in Table 2. Results indicate a huge correlation
between the distributions of both ACKTR+CB and DQN+CB since both aim at reducing the overall susceptible
population as optimally as possible.

:

Figure 4 (Top):Increasing Reward Curve of DQN. (Bottom) Smoothed Reward Curve of ACKTR

7

A PREPRINT - JANUARY 19, 2021

Model
ACKTR + CB

DQN + CB

Bucket Size
200
300
400
500
200
300
400
500

Assam
15.5
15.5116
15.5172
15.5206
15.8974
15.9864
16.1616
16.129

Delhi
8
7.9208
8.1281
8.055
8.2051
8.1633
8.3333
8.2661

Jharkhand
17.5
17.4917
17.734
17.6817
16.9231
17.0068
17.1717
17.1371

Maharashtra
58
57.7558
57.3892
57.3674
58.4615
58.1633
57.5758
57.6613

Nagaland
1
1.3201
1.2315
1.3752
0.5128
0.6803
0.7576
0.8065

Table 2: VacSIM model output on different bucket size

4.4

Model Evaluation through Projection Scenario

Since there is no way that the evaluation of distribution policy can be done in the absence of vaccine and real-world
distribution, we defined the Naive baseline distribution policy as % of vaccine given to a state = % of Infected People
in that state and compared it with our model’s learned distribution. With 1000000 doses and 5 States, we simulated
the distribution of the available vaccine on 31 December for the Naive and VacSIM policies. The number of resulting
(projected) infections for 45 days after the vaccine distribution was calculated using the SEIR Model. For each bucket
in the range [200,500], day-wise total cases of all 5 States were summed up for both policy models and then their
differences were measured with total cases for all 5 states arising after Naive distribution as shown in Figure 5. Our
results indicate that the ACKTR based policy additionally reduces a total of 8845, 7787, 4703 and 5103 infected cases,
with bucket sizes 200, 300, 400 and 500 respectively, in the next 45 days. Likewise DQN based policy additionally
reduces a total of 9039, 6686, 5355 and 4698 infected cases, with bucket sizes 200, 300, 400 and 500 respectively.

Figure 5: Difference of total COVID-19 projected cases in next 45 days after vaccine distribution by Naive Approach
and through Model driven way

8

A PREPRINT - JANUARY 19, 2021

4.5

Model evaluation through learning the causal structure of simulated data obtained

The ultimate goal of vaccine distribution is to reduce mortality and morbidity. Since our model relies entirely upon
simulations, in the absence of a vaccine, we checked if the data generated by such an approach follow the cause-andeffect relationships as expected in the real-world data. Structure-learning was carried out using a data-driven Bayesian
network[20] approach using Hill Climbing algorithm[21] with Akaike Information Criterion and Bayesian Information
Criterion as a scoring function, and ensemble averaging over 501 bootstrapped networks. These models were learned
using wiseR package[22].

Figure 6: Ensemble averaged causal structure of the Bayesian network obtained from 501 bootstraps, using Hill
Climbing optimizer for AIC (left) and BIC (right) as a scoring function. Vaccine Percentage obtained from the
model was observed as a parent node of Susceptible cases, thus indicating the causality preserving nature of
VacSIM simulations.
State-wise time series data of death, recovery, infected people, susceptible people and the amount of vaccine obtained
from our model were used to learn the structure. Blacklisting among nodes was done such that vaccine percentage
cannot be the child node of COVID-19 trajectory indicators (Susceptible, Recovery, Infected People, Death). The
resulting structure shows a causal relationship between the vaccine amount(parent node) and susceptible count (child
node), thus confirming the technical correctness of the VacSIM model through an external evaluation approach (refer
Figure 6).

5

Conclusion

Researchers worldwide are working around the clock to find a vaccine against SARS-CoV-2, the virus responsible for
the COVID-19 pandemic. Once available, distribution of the vaccine will have numerous logistical challenges (supply
chain, legal agreements, quality control, application to name a few) which might slow down the distribution process.
In this paper, we have developed a novel distribution policy model, VacSIM using Reinforcement Learning. We have
pipelined an Actor-Critic using Kronecker-Factored Trust Region (ACKTR) model/ Deep Q Networks (DQN) model
and Contextual Bandit model in a feed-forward way such that the output (Action and Rewards) of ACKTR/DQN
model are fed into the Contextual Bandit model in order to provide a sensible context comprising of actions and rewards. Contextual Bandits then optimize the policy considering demographic metrics such as the population of the
state, Overall Hospital Beds, Overall ICU Beds, Ventilators and time series-based characteristics of the COVID-19
spread (susceptible population, recovery rate, death rate, total infected cases) as Context. While distributing the vaccine, identifying the part of the population who needs it the most is a challenging task, and in our case, we addressed
it by the usage of the aforementioned Context. Rather than using the present-day count of infected and susceptible
people, we have used SEIR-based projections, which makes our predicted policy more robust and trustworthy. Evaluation of model-driven policy is a tough assignment due to unavailability of ground truth, and we proposed a novel
causality-preserving approach to evaluate such models. The open-source code will enable testing of our claims by
other researchers. VacSIM may have some limitations shared by all RL models, i.e. the transparency of their learning
9

A PREPRINT - JANUARY 19, 2021

process and the explainability of their decisions. Secondly, the development of VacSIM has been carried out while
observing the pandemic in the past few months. However, the dynamic nature of pandemic may require a change in
actions, thus calling for common-sense working alongside artificial intelligence. In conclusion, we believe that artificial intelligence has a role to play in an optimal distribution of the scarce resources such as vaccines, syringes, drugs,
personal protective equipment (PPEs) etc. that the world will see in the coming months. We provide a novel, opensource and extensible solution to this problem that policymakers and researchers may refer to while making decisions.
This feedforward network can be adapted and used for optimal allocation of various essential resources in varying
scenarios and context.

References
[1] B. Greenwood, “The contribution of vaccination to global health: past, present and future,” Philosophical
Transactions of the Royal Society B: Biological Sciences, vol. 369, no. 1645, Jun. 2014. [Online]. Available:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4024226/
[2] “Smallpox
vaccines.”
smallpox-vaccines

[Online].

Available:

https://www.who.int/news-room/feature-stories/detail/

[3] N. Division, “Fact Sheet: Explaining Operation Warp Speed,” Jun. 2020, last Modified: 2020-08-12T16:32-04:00.
[Online]. Available: https://www.hhs.gov/about/news/2020/06/16/fact-sheet-explaining-operation-warp-speed.
html
[4] “AstraZeneca
Covid-19
vaccine
study
is
put
on
hold,”
Sep.
2020.
[Online].
Available:
https://www.statnews.com/2020/09/08/
astrazeneca-covid-19-vaccine-study-put-on-hold-due-to-suspected-adverse-reaction-in-participant-in-the-u-k/
[5] D. B. White and D. C. Angus, “A Proposed Lottery System to Allocate Scarce COVID-19 Medications:
Promoting Fairness and Generating Knowledge,” JAMA, vol. 324, no. 4, pp. 329–330, Jul. 2020, publisher:
American Medical Association. [Online]. Available: https://jamanetwork.com/journals/jama/fullarticle/2767751
[6] R. Khamsi, “If a coronavirus vaccine arrives, can the world make enough?” Nature, vol. 580, no.
7805, pp. 578–580, Apr. 2020, number: 7805 Publisher: Nature Publishing Group. [Online]. Available:
https://www.nature.com/articles/d41586-020-01063-8
[7] T. Spiro and Z. Emanuel, “A Comprehensive COVID-19 Vaccine Plan.”
[8] D. Foster, C. Mcgregor, and S. El-masri, “A Survey of Agent-Based Intelligent Decision Support Systems to
Support Clinical,” in Management and Research”, 1st Intl. Workshop on Multi-Agent Systems for Medicine,
Computational Biology, and Bioinformatics.
[9] S. Deo, S. Manurkar, S. Krishnan, and C. Franz, “COVID-19 Vaccine: Development, Access and Distribution in
the Indian Context,” no. 378, p. 16, 2020.
[10] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan, J. Quan, A. Sendonaris, I. Osband,
G. Dulac-Arnold, J. Agapiou, J. Leibo, and A. Gruslys, “Deep Q-learning From Demonstrations,” Proceedings
of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, Apr. 2018, number: 1. [Online]. Available:
https://ojs.aaai.org/index.php/AAAI/article/view/11757
[11] H. van Hasselt, A. Guez, and D. Silver, “Deep Reinforcement Learning with Double Q-learning,”
arXiv:1509.06461 [cs], Dec. 2015, arXiv: 1509.06461. [Online]. Available: http://arxiv.org/abs/1509.06461
[12] Y. Wu, E. Mansimov, S. Liao, R. Grosse, and J. Ba, “Scalable trust-region method for deep reinforcement
learning using Kronecker-factored approximation,” arXiv:1708.05144 [cs], Aug. 2017, arXiv: 1708.05144.
[Online]. Available: http://arxiv.org/abs/1708.05144
[13] M. Collier and H. U. Llorens, “Deep Contextual Multi-armed Bandits,” arXiv:1807.09809 [cs, stat], Jul. 2018,
arXiv: 1807.09809. [Online]. Available: http://arxiv.org/abs/1807.09809
[14] C. Riquelme, G. Tucker, and J. Snoek, “Deep Bayesian Bandits Showdown: An Empirical Comparison of
Bayesian Deep Networks for Thompson Sampling,” arXiv:1802.09127 [cs, stat], Feb. 2018, arXiv: 1802.09127.
[Online]. Available: http://arxiv.org/abs/1802.09127
[15] “Vowpal Wabbit.” [Online]. Available: https://vowpalwabbit.org/
[16] “Census of India Website : Office of the Registrar General & Census Commissioner, India.” [Online]. Available:
https://censusindia.gov.in/2011-common/introductionToNpr.html
[17] “Development Data Lab.” [Online]. Available: http://www.devdatalab.org/shrug
10

A PREPRINT - JANUARY 19, 2021

[18] OpenAI, “Gym: A toolkit for developing and comparing reinforcement learning algorithms.” [Online].
Available: https://gym.openai.com
[19] M. Y. Li and J. S. Muldowney, “Global stability for the SEIR model in epidemiology,” Mathematical
Biosciences, vol. 125, no. 2, pp. 155–164, Feb. 1995. [Online]. Available: http://www.sciencedirect.com/science/
article/pii/0025556495927565
[20] D. Heckerman, “A Tutorial on Learning With Bayesian Networks,” arXiv:2002.00269 [cs, stat], Feb. 2020,
arXiv: 2002.00269. [Online]. Available: http://arxiv.org/abs/2002.00269
[21] J. A. Gámez, J. L. Mateo, and J. M. Puerta, “Learning Bayesian networks by hill climbing: efficient methods
based on progressive restriction of the neighborhood,” Data Mining and Knowledge Discovery, vol. 22, no. 1,
pp. 106–148, Jan. 2011. [Online]. Available: https://doi.org/10.1007/s10618-010-0178-6
[22] T. Sethi and S. Maheshwari, “wiseR: A Shiny Application for End-to-End Bayesian Decision Network Analysis
and Web-Deployment,” Nov. 2018. [Online]. Available: https://CRAN.R-project.org/package=wiseR

11

