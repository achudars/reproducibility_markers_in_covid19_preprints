Fact or Fiction: Verifying Scientific Claims
David Wadden†∗ Shanchuan Lin† Kyle Lo‡ Lucy Lu Wang‡
Madeleine van Zuylen‡ Arman Cohan‡ Hannaneh Hajishirzi†‡
†
University of Washington, Seattle, WA, USA
‡
Allen Institute for Artificial Intelligence, Seattle, WA, USA
{dwadden,linsh,hannaneh}@cs.washington.edu
{kylel,lucyw,madeleinev,armanc}@allenai.org

Claim

arXiv:2004.14974v6 [cs.CL] 3 Oct 2020

Abstract
We introduce scientific claim verification, a
new task to select abstracts from the research literature containing evidence that S UP PORTS or R EFUTES a given scientific claim,
and to identify rationales justifying each decision. To study this task, we construct S CI FACT, a dataset of 1.4K expert-written scientific claims paired with evidence-containing
abstracts annotated with labels and rationales.
We develop baseline models for S CI FACT, and
demonstrate that simple domain adaptation
techniques substantially improve performance
compared to models trained on Wikipedia or
political news. We show that our system is
able to verify claims related to COVID-19 by
identifying evidence from the CORD-19 corpus. Our experiments indicate that S CI FACT
will provide a challenging testbed for the development of new systems designed to retrieve
and reason over corpora containing specialized
domain knowledge. Data and code for this
new task are publicly available at https://
github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo
are available at https://scifact.apps.
allenai.org.

1

Introduction

Due to rapid growth in the scientific literature, it
is difficult for researchers – and the general public even more so – to stay up to date on the latest
findings. This challenge is especially acute during
public health crises like the current COVID-19 pandemic, due to the extremely fast rate at which new
findings are reported and the risks associated with
making decisions based on outdated or incomplete
information. As a result, there is a need for automated tools to assist researchers and the public in
evaluating the veracity of scientific claims.
∗
Work performed during internship with the Allen Institute for Artificial Intelligence.

Cardiac injury is common in
critical cases of COVID-19.

Corpus

Fact-checker
Decision: SUPPORTS
More severe COVID-19 infection
is associated with higher mean
troponin (SMD 0.53, 95% CI 0.30
to 0.75, p < 0.001)
Rationale

Figure 1: A scientific claim, supported by evidence
identified by our system. To correctly verify this claim,
the system must possess background knowledge that
troponin is a protein found in cardiac muscle and that
elevated levels of troponin are a marker of cardiac
injury. In addition, it must be able to reason about directional relationships between scientific processes: replacing higher with lower would cause the rationale
to R EFUTE the claim rather than S UPPORT it. Finally,
the system should interpret p < 0.001 as an indication
that the reported finding is statistically significant.

Fact-checking – a task in which the veracity
of an input claim is verified against a corpus of
documents that support or refute the claim – has
been studied to combat the proliferation of misinformation in political news, social media, and on
the web (Thorne et al., 2018; Hanselowski et al.,
2019). However, verifying scientific claims poses
new challenges to both dataset construction and
effective modeling. While political claims are readily available on fact-checking websites and can be
verified by crowd workers, annotators with extensive domain knowledge are required to generate
and verify scientific claims.
In addition, NLP systems for scientific claim
verification must possess additional capabilities beyond those required to verify factoid claims. For
instance, to verify the claim shown in Figure 1, a

Claim 1: Lopinavir / ritonavir have exhibited favorable clinical responses when used as a treatment for coronavirus.
Supports: . . . Interestingly, after lopinavir/ritonavir (Kaletra, AbbVie) was administered, β-coronavirus viral loads significantly
decreased and no or little coronavirus titers were observed.
Refutes: The focused drug repurposing of known approved drugs (such as lopinavir/ritonavir) has been reported failed for
curing SARS-CoV-2 infected patients. It is urgent to generate new chemical entities against this virus . . .
Claim 2: The coronavirus cannot thrive in warmer climates.
Supports: ...most outbreaks display a pattern of clustering in relatively cool and dry areas...This is because the environment
can mediate human-to-human transmission of SARS-CoV-2, and unsuitable climates can cause the virus to destabilize quickly...
Refutes: ...significant cases in the coming months are likely to occur in more humid (warmer) climates, irrespective of the
climate-dependence of transmission and that summer temperatures will not substrantially limit pandemic growth.

Table 1: Evidence identified by our system as supporting and refuting two claims concerning COVID-19.

system must have the ability to access scientific
background knowledge, reason over increases and
decreases in quantities or measurements, and make
sense of specialized statistical language.
In this paper, we introduce the task of scientific claim verification to evaluate the veracity of
scientific claims against a scientific corpus. Table 1 presents some examples. To facilitate research on this task, we construct S CI FACT, an
expert-annotated dataset of 1,409 scientific claims
accompanied by abstracts that support or refute
each claim, and annotated with rationales (Lei et al.,
2016) justifying each S UPPORTS / R EFUTES decision. To create the dataset, we develop a novel annotation protocol in which annotators re-formulate
naturally occurring claims in the scientific literature
– citation sentences – into atomic scientific claims.
Using citation sentences as a source of claims both
speeds the claim generation process and guarantees
that the topics discussed in S CI FACT are representative of the research literature. In addition, citation
links indicate the exact documents likely to contain
evidence necessary to verify a given claim.
We establish performance baselines on S CI FACT
with an approach similar to DeYoung et al. (2020a),
which achieves strong performance on the F EVER
claim verification dataset (Thorne et al., 2018). Our
baseline is a pipeline system which retrieves abstracts related to an input claim, uses a BERTbased (Devlin et al., 2019) sentence selector to identify rationale sentences, and labels each abstract
as S UPPORTS, R EFUTES, or N O I NFO with respect
to the claim. We demonstrate that our baseline
can benefit from training on claims from domains
including Wikipedia articles and politics.
We showcase the ability of our model to verify expert-written claims concerning the novel
coronavirus COVID-19 against the newly-released

CORD-19 corpus (Wang et al., 2020). Expert annotators judge retrieved evidence to be plausible for
23 of 36 claims.1 Our results and analyses demonstrate the importance of the new task and dataset to
support significant future research in this domain.
In summary, our contributions include: (1) We
introduce and formalize the scientific claim verification task. (2) We develop a novel annotation
protocol to generate and verify 1.4K naturallyoccurring claims about scientific findings. (3) We
establish strong baselines on this task, and identify substantial opportunities for improvement at
all stages of the modeling pipeline. (4) We demonstrate the efficacy of our system in a real-world case
study verifying claims about COVID-19 against the
research literature.

2

Background and task definition

As illustrated in Figure 1, scientific claim verification is the task of identifying evidence from the
research literature that S UPPORTS or R EFUTES a
given scientific claim. Table 1 shows the results
of our system applied to claims about the novel
coronavirus COVID-19. For each claim, the system identifies relevant scientific abstracts, and labels the relation of each abstract to the claim as
either S UPPORTS or R EFUTES. Verifying scientific
claims is challenging and requires domain-specific
background knowledge – for instance, in order to
identify the evidence supporting Claim 1 in Table 1, the system must determine that a reduction in
coronavirus viral load indicates a favorable clinical
response, even though this fact is never mentioned.
Scientific claims In S CI FACT, a scientific claim is
an atomic verifiable statement expressing a finding
1
We emphasize that our model is a research prototype and
should not be used to make any medical decisions whatsoever.

about one aspect of a scientific entity or process,
which can be verified from a single source.2 For
instance, “The R0 of the novel coronavirus is 2.5”
is valid, but opinion-based statements like “The
government should require people to stand six feet
apart to stop coronavirus” are not. Compound
claims like “Aerosolized coronavirus droplets can
travel at least 6 feet and can remain in the air for 3
hours” should be split into two atomic claims.
Claims in S CI FACT are natural – they are derived from citation sentences, or citances (Nakov
et al., 2004), that occur naturally in scientific articles. This is similar to political fact-checking
datasets such as UKP Snopes (Hanselowski et al.,
2019), which use political fact-checking websites
as a source of natural claims. On the other hand,
claims in the popular F EVER dataset (Thorne et al.,
2018) are synthetic, since they are created by annotators by mutating sentences from the Wikipedia
articles that will serve as evidence.
Supporting and refuting evidence In most factchecking work, claims are assigned a global truth
label based on the entirety of the available evidence.
For example in F EVER, the claim “Barack Obama
was the 44th President of the United States” can be
verified using Wikipedia as an evidence source.
While S CI FACT claims are indeed verifiable assertions about scientific findings, accurately assigning a global truth label to a scientific claim (given a
fixed scientific corpus) requires a systematic review
by a team of experts. In this work we focus on the
simpler task of assigning S UPPORTS or R EFUTES
relations to individual claim-abstract pairs.
Each S UPPORTS or R EFUTES relation between
claim and abstract must be justified by at least one
rationale. A rationale is a minimal collection of
sentences which, taken together as premises in the
context of the abstract, can reasonably be judged by
a domain expert as implying the claim. Rationales
facilitate the development of interpretable models
which not only have the ability to make label predictions, but can also identify the exact sentences
that are necessary for their decisions.

3

The S CI FACT dataset

The S CI FACT dataset consists of 1,409 scientific
claims3 verified against a corpus of 5,183 abstracts.
2
Requiring annotators to search multiple sources increases
cognitive burden and decreases annotation quality.
3
S CI FACT is comparable in size to recent scientific datasets
for tasks such as QA (e.g. PubMedQA (Jin et al., 2019)

N=601

Corpus
Citing

Seed
N=140

Cocited
N=4,259

Distractor

Claim
Cardiac injury is
common in critical
cases of COVID-19.

Figure 2: Corpus construction. Citing abstracts are
identified for each seed document. A claim is written
based on the source citance in the citing abstract.

Abstracts that support or refute each claim are annotated with rationales. We describe our corpus
creation and annotation process.
3.1

Data source and corpus construction

To construct S CI FACT, we use S2ORC (Lo et al.,
2020), a publicly-available corpus of millions of
scientific articles. To ensure that documents in our
dataset are of high quality, we randomly sample
articles from a manually curated collection of wellregarded journals spanning domains from basic science (e.g., Cell, Nature) to clinical medicine (e.g.,
JAMA, BMJ). The full list of journals is included in
Appendix C.1. We restrict to articles with at least
10 citations. The resulting collection is referred
to as our seed set. We use the S2ORC citation
graph to sample source citances from citing articles which cite these seed articles. If a citance cites
other articles not in the seed set, we refer to these
as co-cited articles and add them to the corpus, as
depicted in Figure 2. The content of the cited abstracts encompasses a diverse array of topics within
biomedicine, as shown in Figure 3. The majority
of citances used for S CI FACT cite only the seed
article (no co-cited articles), as we found in initial
annotation experiments that these citances tended
to yield specific, easy-to-verify claims.
To expand the corpus, we identify five papers
cited in the same paper as each source citance but
in a different paragraph, and add these to the corpus as distractor abstracts. These abstracts often
has 1,000 questions), and information extraction (e.g. SciERC (Luan et al., 2018) has 500 annotated abstracts).

Humans
Mice
Animals
Female
Male
Middle Aged
Aged
Adult
Cell Line
Signal Transduction
RNA
Receptors
Gene Expression Regulation
Models
Risk Factors
Cells
Cell Differentiation
Mutation
Microscopy
Infant
0.0

cation model performs poorly, suggesting that the
negation process did not introduce severe artifacts.
3.3

0.2 0.4 0.6 0.8 1.0
Fraction of evidence abstracts

Figure 3: Most frequently occurring Medical Subject
Headings (MeSH) terms (y-axis) among cited abstracts.
MeSH is a controlled vocabulary used for indexing articles in PubMed. Topics range from clinical trial reports (“Humans”, “Risk Factors”) to molecular biology
(“Cell Line”, “RNA”).

discuss similar topics to the evidence documents,
increasing the difficulty of abstract retrieval and
making our metrics more accurately reflect the system’s performance on a large research corpus.
3.2

Claim writing

Annotation Annotators are shown a source citance
in the context of an article, and are asked to write up
to three claims based on the content of the citance;
see Appendix C.2 for an example. This results in
natural claims because the annotator does not see
the cited article’s abstract – the cited abstract – at
the time of claim writing. Annotators are asked
to skip citances that do not make statements about
specific scientific findings.
The claim writers included four experts with
background in scientific NLP, fifteen undergraduates studying the life sciences, and four graduate
students (doctoral or medical) in the life sciences.
Detailed information on the annotator training process can be found in Appendix C.3. The claimwriting interface is shown in Appendix D.
Claim negation Unless the authors of the source
citance were mistaken, cited articles should provide supporting evidence for the claims made in
a citance. To obtain examples where an abstract
R EFUTES a claim, an NLP expert wrote negations
of existing claims, taking precautions not to bias
the negations by using obvious keywords like “not”
(Schuster et al., 2019; Gururangan et al., 2018). In
§6.1, we demonstrate that a “claim-only” verifi-

Claim verification

Annotation For each claim, all of the claim’s cited
abstracts are annotated for evidence. Annotators
are shown a single claim - cited abstract pair, and
asked to label the pair as S UPPORTS, R EFUTES, or
N O I NFO. Although our task definition allows for a
single claim to be both supported and refuted (by
different abstracts) – an occurrence we observe on
real-world COVID-19 claims (§6.3) – this never
occurs in our dataset. Each claim has a single label.
Counts for each label are shown in Table 2a. Overall, the annotators found evidence in 63% of cited
abstracts. If the annotator assigns a S UPPORTS or
R EFUTES label, they must also identify all rationales as defined in §2. Table 2b provides statistics
on the number of sentences per rationale, the number of rationales per claim / abstract pair, and the
number of evidence abstracts per claim. No abstract has more than 3 rationales for a given claim,
and all rationales consist of at most three sentences.
Rationales in S CI FACT are mutually exclusive. 28
rationales contain non-contiguous sentences.
The verifiers included three NLP experts, five
life science undergraduates, and five graduate students studying life sciences. Annotators verified
claims that they did not write themselves. Annotation guidelines are provided in Appendix D.
S CI FACT claims are verified against abstracts
rather than full articles since (1) abstracts can be
annotated more scalably, (2) evidence is found in
the abstract in more than 60% of cases, and (3) previous attempts at full-document annotation suffered
from low annotator agreement (§7).
Quality We assign 232 claim-abstract pairs for independent re-annotation. The label agreement is
0.75 Cohen’s κ, comparable with the 0.68 Fleiss’
κ reported in Thorne et al. (2018), and 0.70 Cohen’s κ reported in Hanselowski et al. (2019). To
measure rationale agreement, we treat each sentence as either classified as “part of a rationale” or
“not part of a rationale” and compute sentence-level
agreement. The resulting Cohen’s κ is 0.71.

4

The S CI FACT task

Task Formulation The inputs to our task are a scientific claim c and a corpus of abstracts A. All abstracts a ∈ A are labeled as y(c, a) ∈ {S UPPORTS,
R EFUTES, N O I NFO } with respect to a claim c.

Fold

S UPPORTS

N O I NFO

R EFUTES

All

Train
Dev
Test

332
124
100

304
112
100

173
64
100

809
300
300

All

556

516

337

1409

(a) Distribution of claim labels in S CI FACT.

Cited abstracts per claim
Evidence abstracts per claim
Rationales per abstract
Sentences per rationale

0

1

2

3+

516
-

1278
830
552
1542

86
37
290
92

45
26
153
11

(b) Evidence counts at various levels of granularity. For example, Column 2 of the row “Rationales / abstract” indicates that
290 claim / abstract pairs are supported by 2 distinct rationales.

Table 2: Statistics on claim labels, and the number of
evidence abstracts and rationales per claim.

The abstracts that either S UPPORT or R EFUTE c
are referred to as evidence abstracts for c, denoted
as E(c). Each evidence abstract a ∈ E(c) is annotated with rationales. A single rationale Ri is
a collection of sentences {r1 (c, a), . . . , rm (c, a)},
where m is the number of sentences in rationale Ri .
We denote the set of all rationales as R(c, a) =
{R1 (c, a), . . . , Rn (c, a)}.
Given a claim c and a corpus A, the system
b
must predict a set of evidence abstracts E(c).
For
b
each abstract a ∈ E(c),
it must predict a label
yb(c, a), and a collection of rationale sentences
b a) = {b
S(c,
s1 (c, a), . . . , sb` (c, a)}. Note that although the gold annotations may contain multiple
separate rationales, to simplify the prediction task
we only require the model to predict a single collection of rationale sentences; these sentences may
encompass multiple gold rationales.
Task Evaluation We evaluate the task at two levels
of granularity. For abstract-level evaluation, we
assess the model’s ability to identify the abstracts
that support or refute the claim. For sentence-level
evaluation, we evaluate the model’s performance
at identifying the sentences sufficient to justify the
abstract-level predictions. We conduct evaluations
in both the “Open” F EVER-style (Thorne et al.,
2018) setting where the evidence abstracts must
be retrieved, and the “Oracle abstract” ERASERstyle (DeYoung et al., 2020a) setting where the
gold evidence abstracts E(c) are provided.
Abstract-level evaluation is inspired by the
F EVER score. Given a claim c, a predicted evidence
b is correctly labeled if (1) a is a
abstract a ∈ E(c)

gold evidence abstract for c, and (2) The predicted
label is correct: yb(c, a) = y(c, a). It is correctly
rationalized if, in addition, the predicted rationale
sentences contain a gold rationale, i.e., there exists
b a).
some gold rationale Ri (c, a) ⊆ S(c,
Like F EVER, which limits the maximum number
of predicted rationale sentences to five, S CI FACT
limits to three predicted rationale sentences. Overall performance is measured by the micro-F1 of
the precision and recall over the correctly-labeled
and correctly-rationalized evidence abstracts. We
refer to these evaluations as AbstractLabel-Only and
AbstractLabel+Rationale , respectively.
Sentence-level evaluation measures performance
in identifying individual rationale sentences. Unlike the abstract-level metrics, this evaluation penalizes the prediction of extra rationale sentences.
A predicted rationale sentence sb(c, a) is correctly selected if (1) It is a member of some gold
rationale Ri (c, a), (2) all other sentences from the
same gold rationale Ri (c, a) are among the preb a), and (3) yb(c, a) 6= N O I NFO4 . It is
dicted S(c,
correctly labeled if, in addition, the abstract a is
correctly labeled: yb(c, a) = y(c, a).
Overall performance is measured by the microF1 of the precision and recall of correctly-selected
and correctly-labeled rationale sentences, denoted
SentenceSelection-Only and SentenceSelection+Label .
For sentence-level evaluation, we do not limit the
number of predicted rationale sentences, since the
evaluation penalizes models that over-predict.

5

V ERI S CI: Baseline model

We develop a baseline (referred to as V ERI S CI) that
takes a claim c and corpus A as input, identifies
b
evidence abstracts E(c),
and predicts a label yb(c, a)
b a) for each a ∈ E(c).
b
and rationale sentences S(c,
Following the “BERT-to-BERT” model presented
in DeYoung et al. (2020a); Soleimani et al. (2019),
V ERI S CI is a pipeline of three components:
1. A BSTRACT R ETRIEVAL retrieves k abstracts
with highest TF-IDF similarity to the claim.
2. R ATIONALE S ELECTION identifies rationale
b a) for each abstract.
sentences S(c,
3. L ABEL P REDICTION makes the final label prediction yb(c, a).
Rationale selection Given a claim c and abstract a, we train a model to predict zi ,
4

Condition (3) eliminates rationale sentences which were
identified by the rationale selector, but proved insufficient to
justify a final S UPPORTS / R EFUTES decision

1[ai is a rationale sentence] for each sentence ai
in a. For each sentence, we encode the concatenated sequence wi = [ai , SEP, c] using a BERTstyle language model and predict a score z̃i =
σ[f (CLS(wi ))], where σ is the sigmoid function,
f is a linear layer and CLS(wi ) is the CLS token
from the encoding of wi . We train the model on
pairs of claims and their cited abstracts and minimize cross-entropy loss between zi and z̃i . For
each claim, we use cited abstracts labeled N O I NFO,
as well as non-rationale sentences from abstracts
labeled S UPPORTS and R EFUTES as negative examples. To make predictions, we select all sentences ai with z˜i > t as rationale sentences, where
t ∈ [0, 1] is tuned on the dev set (Appendix A.1).
Label prediction Sentences identified by the rationale selector are passed to a separate BERTbased model to make the final labeling decision.
Given a claim c and abstract a, we concatenate
the claim and the predicted rationale sentences
u = [b
s1 (c, a), . . . sb` (c, a), SEP, c]5 , and predict
ỹ(c, a) = φ[f (CLS(u))], where φ is the softmax
function, and f is a linear layer with three outputs
representing the {S UPPORTS, R EFUTES, N O I NFO
} labels. We minimize the cross-entropy loss between ỹ(c, a) and the true label y(c, a).
We train the model on pairs of claims and their
cited abstracts using gold rationales as input. For
cited abstracts labeled N O I NFO, we choose the
k sentences from the cited abstract with highest TF-IDF similarity to the claim as input rationales. For prediction, we use the predicted
b a) as input and predict
rationale sentences S(c,
ŷ(c, a) = argmax ỹ(c, a). N O I NFO is predicted
for abstracts with no rationale sentences.
We experimented with a label prediction model
which encodes entire abstracts via the Longformer
(Beltagy et al., 2020), and makes predictions using the document-level CLS token. Performance
was not competitive with our pipeline setup, likely
because the label predictor struggles to identify
relevant information when given full abstracts.

6

Experiments

In our experiments, we (1) analyze the performance
of each individual component of V ERI S CI, (2) evaluate full task performance in both the “Oracle abstract” and “Open” settings, (3) present promising
results verifying claims about COVID-19 using
5

We truncate the rationale input if it exceeds the BERT
token limit. c is never truncated.

R ATIONAL -S ELECT.
Training data
F EVER
UKP Snopes
S CI FACT
F EVER + S CI FACT
Sentence encoder
S CI BERT
BioMedRoBERTa
RoBERTa-base
RoBERTa-large

P

R

41.5 57.9
42.5 62.3
73.7 70.5
72.4 67.2
P

R

74.5 74.3
75.3 69.9
76.1 66.1
73.7 70.5

L ABEL -P RED .

F1

ACC .

48.4
50.5
72.1
69.7

67.6
71.3
75.7
81.9

F1

ACC .

74.4
72.5
70.8
72.1

69.2
71.7
62.9
75.7

Model inputs

P

R

F1

ACC .

Claim-only
Abstract-only

60.1

60.9

60.5

44.5
53.3

Table 3: Comparison of different training datasets, encoders, and model inputs for R ATIONALE S ELECTION
and L ABEL P REDICTION, evaluated on the S CI FACT
dev set. The claim-only model cannot select rationales.

V ERI S CI, and (4) discuss some modeling challenges presented by the dataset.
6.1

Pipeline components

We examine the effects of different training
datasets, sentence encoders, and model inputs on
the performance of the R ATIONALE S ELECTION
and L ABEL P REDICTION modules. The R ATIO NALE S ELECTION module is evaluated on its ability
to select rationale sentences given gold abstracts6 .
The L ABEL P REDICTION module is evaluated on its
3-way label classification accuracy given gold rationales from cited abstracts. Cited abstracts labeled
N O I NFO are included in the evaluation. These abstracts have no gold rationale sentences; as in §5,
we provide the k most similar sentences from the
abstract as input (more details in Appendix A).
Training Data We train on (1) F EVER, (2) UKP
Snopes, (3) S CI FACT, and (4) F EVER pretraining
followed by S CI FACT fine-tuning. RoBERTa-large
(Liu et al., 2019) is used as the sentence encoder.
Sentence encoder We fine-tune S CI BERT (Beltagy et al., 2019), BioMedRoBERTa (Gururangan
et al., 2020), RoBERTa-base, and RoBERTa-large.
S CI FACT is used as training data.
Model Inputs We examine the performance of
“claim-only” and “abstract-only” models trained
on S CI FACT, using RoBERTa-large as the sentence
encoder. The claim-only model makes label predic6

Our F EVER-trained R ATIONALE S ELECTION module
achieves 79.9 sentence-level F1 on the F EVER test set, virtually identical to 79.6 reported in DeYoung et al. (2020a).

Sentence-level
Selection-Only
Selection+Label
Retrieval Model

P

R

F1

P

R

Oracle rationale 1 100.0 80.5 89.22.1 89.6 72.2
Oracle
abstract

Zero-shot
V ERI S CI

2
3

42.5
76.1

Zero-shot
V ERI S CI

5
6

28.7
45.0

79.93.0

45.1 43.82.0 36.1 38.4 37.22.3
63.8 69.42.6 66.5 55.7 60.63.1

Oracle rationale 4 100.0 56.5 72.23.3 87.6 49.5
Open

F1

63.23.7

37.6 32.52.3 23.7 31.1 26.92.3
47.3 46.13.0 38.6 40.5 39.53.0

Abstract-level
Label-Only
Label+Rationale
P

R

F1

P

R

90.1 77.5 83.32.4 90.1 77.5

F1
83.32.4

86.9 53.6 66.33.1 67.9 41.9 51.83.4
87.3 65.3 74.72.8 84.9 63.5 72.72.9
88.9 54.1 67.23.2 88.9 54.1

67.23.2

56.0 42.3 48.23.3 42.3 32.0 36.43.3
47.5 47.3 47.43.1 46.6 46.4 46.53.1

Table 4: Test set performance on S CI FACT, according to the metrics from §4. For the “Oracle abstract” rows,
the system is provided with gold evidence abstracts. “Oracle rationale” rows indicate that the gold rationales are
provided as input. “Zero-shot” indicates zero-shot performance of a verification system trained on F EVER. Additionally, standard deviations are reported as subscripts for all F1 scores. See Appendix B for standard deviations
on all reported metrics.

tions based on the claim text alone, without access
to evidence abstracts. The abstract-only model
selects rationale sentences and makes label predictions without access to the claim.
Results The results are shown in Table 3. For L A BEL P REDICTION, the best performance is achieved
by training first on the large F EVER dataset and
then fine-tuning on the smaller in-domain S CI FACT
training set. To understand the benefits of F EVER
pretraining, we examined the claim / evidence pairs
where the F EVER + S CI FACT- trained model made
correct predictions but the S CI FACT- trained model
did not. In 36 / 44 of these cases, the S CI FACTtrained model predicts N O I NFO. Thus pretraining
on F EVER appears to improve the model’s ability to recognize textual entailment relationships
between evidence and claim – particularly relationships indicated by non-domain-specific cues like
“is associated with” or “has an important role in”.

in order to identify relevant evidence.
6.2

Full task

Experimental setup Based on the results from
§6.1, we use the R ATIONALE S ELECTION module
trained on S CI FACT only, and the L ABEL P REDIC TION module trained on F EVER + S CI FACT for our
final end-to-end system V ERI S CI. Although S CI BERT performs slightly better on rationale selection,
using RoBERTa-large for both R ATIONALE S ELEC TION and L ABEL P REDICTION gave the best fullpipeline performance on the dev set, so we use
RoBERTa-large for both components. For the A B STRACT R ETRIEVAL module, the best dev set fullpipeline performance was achieved by retrieving
the top k = 3 documents.

For R ATIONALE S ELECTION, training on S CI FACT alone produces the best results. We examined the rationales that the S CI FACT- trained model
identified but the F EVER- trained model missed,
and found that they generally contain sciencespecific vocabulary. Thus, training on additional
out-of-domain data provides little benefit.

Model comparisons We report performance of
three model variants. For the “Oracle rationale”
setting, the R ATIONALE S ELECTION module is replaced by an oracle which outputs gold rationales
for correctly retrieved documents, and no rationales
for incorrect retrievals. The “Zero-shot” setting reports the zero-shot generalization performance of
a model trained on F EVER (the results on UKP
Snopes were slightly worse). V ERI S CI reports the
performance of our best system.

RoBERTa-large exhibits the strongest performance on label prediction, while S CI BERT has
a slight edge on rationale selection. The “claimonly” model exhibits very poor performance, which
provides some reassurance that the claim negation
procedure described in §3.2 does not introduce obvious statistical artifacts. Similarly, the poor performance of the “abstract-only” model indicates that
the model needs access to the claim being verified

Results The results are shown in Table 4. In the
oracle abstract setting, the abstract-level F1 scores
are roughly comparable to label classification accuracies, and the AbstractLabel+Rationale score in Row
3 implies an end-to-end classification accuracy of
roughly 70%, given gold abstracts.
Access to in-domain data during training
clearly improves performance.
Despite the
small size of S CI FACT, training on these data

Reasoning type

Example

Science
background

Claim:
Evidence:
Gold Verdict:
Reasoning:

Rapamycin slows aging in fruit flies.
. . . feeding rapamycin to adult Drosophila produces life span extension . . .
S UPPORTS
Drosophila is a type of fruit fly.

Directionality

Claim:
Evidence:
Gold Verdict:
Reasoning:

Inhibiting glucose-6-phospate dehydrogenase impairs lipogenesis
. . . suppression of 6PGD increased lipogenesis
R EFUTES
A decrease (not increase) in lipogenesis would indicate lipogenesis impairment.

Numerical
reasoning

Cause and
effect

Claim:
Evidence:

Bariatric surgery improves resolution of diabetes.
Strong associations were found between bariatric surgery and the resolution of T2DM,
with a HR of 9.29 (95% CI 6.84-12.62)...
Gold Verdict: S UPPORTS
Reasoning:
A HR (hazard ratio) that is greater than 1 with 95% confidence indicates improvement.

Claim:
Evidence:
Gold Verdict:
Reasoning:

Major vault protein (MVP) functions to decrease tumor aggression.
Knockout of MVP leads to miR-193a accumulation...inhibiting tumor progression
R EFUTES
Knocking out (removing) MVP inhibits tumor progression → MVP increases tumor
aggression.

Claim:
Evidence:
Coreference

Low saturated fat diets have adverse effects on the development of infants
Neurological development of children in the intervention group was at least as good as ...
the control group
Gold Verdict: R EFUTES
Reasoning:
The intervention group in this study was placed on a low saturated fat diet.

Table 5: Reasoning types required to verify S CI FACT claims which are classified incorrectly by our modeling
baseline. Words crucial for correct verification are highlighted.

leads to relative improvements of 47% on
open SentenceSelection+Label , and 28% on open
AbstractLabel+Rationale over F EVER alone (Row 6 vs.
Row 5). The three pipeline components make similar contributions to the overall model error. Replacing R ATIONALE S ELECTION with an oracle leads
to a roughly 20-point rise in SentenceSelection+Label
F1 (Row 6 vs. Row 4). Replacing A BSTRACT R E TRIEVAL with an oracle as well leads to a gain of
roughly 20 more points (Row 4 vs. Row 1).
Nearly all correctly-labeled abstracts are supported by at least one rationale. There is only a twopoint difference in F1 between AbstractLabel-Only
and AbstractLabel+Rationale in the oracle setting
(Row 3), and a one-point difference in the
open setting (Row 6). The differences between
SentenceSelection-Only and SentenceSelection+Label are
larger, caused by examples where the model finds
the evidence but fails to predict its relationship to
the claim. We examine these in §6.4.
We evaluate the statistical robustness of our results by generating 10,000 bootstrap-resampled versions of the test set (Dror et al., 2018) and computing the standard deviation of all performance
metrics. Table 4 shows the standard deviations in
F1 score. Uncertainties on all metrics for both the
dev and test set can be found in Appendix B. The re-

sults indicate that the observed differences in model
performance are statistically robust and cannot be
attributed to random variation in the dataset.
6.3

Verifying claims about COVID-19

We conduct exploratory experiments using our system to verify claims concerning COVID-19. We
tasked a medical student to write 36 COVID-related
claims. For each claim c, we used V ERI S CI to
b
predict evidence abstracts E(c).
The annotator exb
amined each (c, E(c)) pair. A pair was labeled
b was nonempty, and at least half of
plausible if E(c)
b were judged to have
the evidence abstracts in E(c)
reasonable rationales and labels. For 23 / 36 claims,
the response of V ERI S CI was deemed plausible by
our annotator, demonstrating that V ERI S CI is able
to successfully retrieve and classify evidence in
many cases. Two examples are shown in Table 1.
In both cases, our system identifies both supporting
and refuting evidence.
6.4

Error analysis

To better understand the errors made by V ERI S CI,
we conduct a manual analysis of test set predictions
where an evidence abstract was correctly retrieved,
but where the model failed to identify any relevant
rationales or predicted an incorrect label. We iden-

tify five modeling capabilities required to correct
these mistakes (Table 5 provides examples):
Science background includes knowledge of
domain-specific lexical relationships.
Directionality requires understanding increases or
decreases in scientific quantities.
Numerical reasoning involves interpreting numerical or statistical findings.
Cause and effect requires reasoning about counterfactuals.
Coreference involves drawing conclusions using
context stated outside of a rationale sentence.

7

knowledge intensive tasks which require an understanding of the relationship between an input query
and relevant supporting text.
Automated evidence synthesis (Marshall and
Wallace, 2019; Beller et al., 2018; Tsafnat et al.,
2014; Marshall et al., 2017) seeks to automate the
process of creating systematic reviews of the medical literature7 – for instance, by extracting PICO
snippets (Nye et al., 2018) and inferring the outcomes of clinical trials (Lehman et al., 2019; DeYoung et al., 2020b). We hope that systems for claim
verification will serve as components in future evidence synthesis frameworks.

Related work
8

Fact checking and rationalized NLP models
Fact-checking datasets include PolitiFact (Vlachos and Riedel, 2014), Emergent (Ferreira and
Vlachos, 2016), LIAR (Wang, 2017), SemEval
2017 Task 8 RumorEval (Derczynski et al., 2017),
Snopes (Popat et al., 2017), CLEF-2018 CheckThat! (Barrón-Cedeño et al., 2018), Verify (Baly
et al., 2018), Perspectrum (Chen et al., 2019),
F EVER (Thorne et al., 2018), and UKP Snopes
(Hanselowski et al., 2019). Hanselowski et al.
(2019) provides a thorough review. To our knowledge, there are no existing data sets for scientific
claim verification. We refer to our task as “claim
verification” rather than “fact-checking” to emphasize that our focus is to help researchers make sense
of scientific findings, not to counter disinformation.
Fact-checking is one of a number of tasks where
a model is required to justify a prediction via rationales from the source document. The ERASER
dataset (DeYoung et al., 2020a) provides a suite
of benchmark datasets (including S CI FACT) for
evaluating rationalized NLP models.
Related scientific NLP tasks The citation contextualization task (Cohan et al., 2015; Jaidka et al.,
2017) is to identify spans in a cited document that
are relevant to a particular citation in a citing document. Unlike S CI FACT, these citations are not
re-written into atomic claims and are therefore
more difficult to verify. Expert annotators achieved
very low (21.7%) inter-annotator agreement on the
BioMedSumm dataset (Cohen et al., 2014), which
contains 314 citations referencing 20 papers.
Biomedical question answering datasets include
BioASQ (Tsatsaronis et al., 2015) and PubMedQA
(Jin et al., 2019), which contain 855 and 1,000
“yes / no” questions respectively (Gu et al., 2020).
Claim verification and question answering are both-

Conclusion and future work

Claim verification allows us to trace the sources
and measure the veracity of scientific claims. These
abilities have emerged as particularly important
in the context of the current pandemic, and the
broader reproducibility crisis in science. In this
article, we formalize the task of scientific claim
verification, and release a dataset (S CI FACT) and
models (V ERI S CI) to support work on this task.
Our results indicate that it is possible to train models for scientific fact-checking and deploy them
with reasonable efficacy on real-world claims related to COVID-19.
Scientific claim verification presents a number
of promising avenues for research on models capable of incorporating background information, reasoning about scientific processes, and assessing
the strength and provenance of various evidence
sources. This last challenge will be especially crucial for future work that seeks to verify scientific
claims against sources other than the research literature – for instance, social media and the news.
We hope that the resources presented in this paper encourage future research on these important
challenges, and help facilitate progress toward the
broader goal of scientific document understanding.

Acknowledgments
This research was supported by the ONR MURI
N00014-18-1-2670, ONR N00014-18-1-2826,
DARPA N66001-19-2-4031, NSF (IIS 1616112),
Allen Distinguished Investigator Award, and the
Sloan fellowship. We thank the Semantic Scholar
team at AI2, UW-NLP, and H2lab at UW for helpful comments and feedback.
7
https://www.cochranelibrary.com/
about/about-cochrane-reviews

References
Ramy Baly, Mitra Mohtarami, James Glass, Lluı́s
Màrquez, Alessandro Moschitti, and Preslav Nakov.
2018. Integrating stance detection and fact checking
in a unified corpus. In NAACL.
Alberto Barrón-Cedeño, Tamer Elsayed, Reem
Suwaileh, Lluı́s Màrquez i Villodre, Pepa
Atanasova, Wajdi Zaghouani, Spas Kyuchukov,
Giovanni Da San Martino, and Preslav Nakov.
2018. Overview of the clef-2018 checkthat! lab on
automatic identification and verification of political
claims. task 2: Factuality. In CLEF.
Elaine Beller, Justin Clark, Guy Tsafnat, Clive Elliott
Adams, Heinz Diehl, Hans Lund, Mourad Ouzzani,
Kristina Thayer, James Thomas, Tari Turner, J. S.
Xia, Karen A. Robinson, and Paul P Glasziou. 2018.
Making progress with the automation of systematic
reviews: principles of the international collaboration for the automation of systematic reviews (icasr).
Systematic Reviews, 7.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:
A pretrained language model for scientific text. In
EMNLP.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
ArXiv, abs/2004.05150.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statistical significance in nlp. In EMNLP.
Sihao Chen, Daniel Khashabi, Wenpeng Yin, Chris
Callison-Burch, and Dan Roth. 2019. Seeing things
from a different angle: Discovering diverse perspectives about claims. In NAACL.
Arman Cohan, Luca Soldaini, and Nazli Goharian.
2015. Matching citation text and cited spans in
biomedical literature: a search-oriented approach.
In NAACL.
Kevin Bretonnel Cohen, Hoa Trang Dang, Anita
de Waard, Prabha Yadav, and Lucy Vanderwende.
2014. Tac 2014 biomedical summarization track.
https://tac.nist.gov/2014/BiomedSumm/.
Leon Derczynski, Kalina Bontcheva, Maria Liakata,
Rob Procter, Geraldine Wong Sak Hoi, and Arkaitz
Zubiaga. 2017. SemEval-2017 task 8: RumourEval:
Determining rumour veracity and support for rumours. In SemEval.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL.
Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,
Eric Lehman, Caiming Xiong, Richard Socher, and
Byron C. Wallace. 2020a. Eraser: A benchmark to
evaluate rationalized nlp models. In ACL.

Jay DeYoung, Eric Lehman, Ben Nye, Iain James
Marshall, and Byron C. Wallace. 2020b. Evidence inference 2.0: More data, better models. In
BioNLP@ACL.
Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The hitchhiker’s guide to testing statistical significance in natural language processing. In
ACL.
Bradley Efron and Robert Tibshirani. 1993. An introduction to the bootstrap.
William Ferreira and Andreas Vlachos. 2016. Emergent: a novel data-set for stance classification. In
NAACL.
Yu Gu, Robert Tinn, Hao Cheng, M. Lucas,
Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020.
Domain-specific language model pretraining for
biomedical natural language processing. ArXiv,
abs/2007.15779.
Suchin Gururangan, Ana Marasovic, Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don’t stop pretraining:
Adapt language models to domains and tasks. In
ACL.
Suchin Gururangan, Swabha Swayamdipta, Omer
Levy, Roy Schwartz, Samuel R. Bowman, and
Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In NAACL.
Andreas Hanselowski, Christian Stab, Claudia Schulz,
Zile Li, and Iryna Gurevych. 2019. A richly annotated corpus for different tasks in automated factchecking. In CoNLL.
Kokil Jaidka, Muthu Kumar Chandrasekaran, Devanshu Jain, and Min-Yen Kan. 2017. The cl-scisumm
shared task 2017: Results and key insights. In
BIRNDL@JCDL.
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W.
Cohen, and Xinghua Lu. 2019. Pubmedqa: A
dataset for biomedical research question answering.
In EMNLP.
Eric Lehman, Jay DeYoung, Regina Barzilay, and Byron C. Wallace. 2019. Inferring which medical
treatments work from reports of clinical trials. In
NAACL.
Tao Lei, Regina Barzilay, and Tommi S. Jaakkola.
2016. Rationalizing neural predictions. In ACL.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S. Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In ACL.

Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh
Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In EMNLP.

Andreas Vlachos and Sebastian Riedel. 2014. Fact
checking: Task definition and dataset construction.
In ACL Workshop on Language Technologies and
Computational Social Science.

Iain James Marshall, Joël Kuiper, Edward Banner, and
Byron C. Wallace. 2017. Automating biomedical evidence synthesis: Robotreviewer. ACL.

Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,
Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn
Funk, Rodney Kinney, Ziyang Liu, William. Merrill, Paul Mooney, Dewey A. Murdick, Devvret
Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Christopher Wilhelm, Boya Xie, Douglas M. Raymond, Daniel S.
Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020.
Cord-19: The covid-19 open research dataset.
ArXiv, abs/2004.10706.

Iain James Marshall and Byron C. Wallace. 2019.
Toward systematic review automation: a practical
guide to using machine learning tools in research
synthesis. Systematic Reviews, 8.
Preslav I Nakov, Ariel S Schwartz, and Marti Hearst.
2004. Citances: Citation sentences for semantic
analysis of bioscience text. In SIGIR workshop on
Search and Discovery in Bioinformatics.
Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei
Yang, Iain James Marshall, Ani Nenkova, and Byron C. Wallace. 2018. A corpus with multi-level annotations of patients, interventions and outcomes to
support language processing for medical literature.
In ACL.
Kashyap Popat, Subhabrata Mukherjee, Jannik
Strötgen, and Gerhard Weikum. 2017. Where the
truth lies: Explaining the credibility of emerging
claims on the web and social media. In WWW.
Tal Schuster, Darsh J. Shah, Yun Jie Serene Yeo,
Daniel Filizzola, Enrico Santus, and Regina Barzilay. 2019. Towards debiasing fact verification models. In EMNLP.
Amir Soleimani, Christof Monz, and Marcel Worring.
2019. Bert for evidence retrieval and claim verification. In European Conference on Information Retrieval.
James Thorne,
Andreas Vlachos,
Christos
Christodoulopoulos, and Arpit Mittal. 2018.
Fever: a large-scale dataset for fact extraction and
verification. In NAACL.
Guy Tsafnat, Paul P Glasziou, Miew Keen Choong,
Adam G. Dunn, Filippo Galgani, and Enrico W.
Coiera. 2014. Systematic review automation technologies. Systematic Reviews, 3:74 – 74.
George Tsatsaronis, Georgios Balikas, Prodromos
Malakasiotis, Ioannis Partalas, Matthias Zschunke,
Michael R. Alvers, Dirk Weissenborn, Anastasia
Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artières,
Axel-Cyrille Ngonga Ngomo, Norman Heino, Éric
Gaussier, Liliana Barrio-Alvers, Michael Schroeder,
Ion Androutsopoulos, and Georgios Paliouras. 2015.
An overview of the bioasq large-scale biomedical
semantic indexing and question answering competition. In BMC Bioinformatics.

William Yang Wang. 2017. “liar, liar pants on fire”: A
new benchmark dataset for fake news detection. In
ACL.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.

A

Model implementation details

All models are implemented using the Huggingface
Transformers package (Wolf et al., 2019).
A.1

Parameters for the final V ERI S CI system

For the A BSTRACT R ETRIEVAL module, V ERI S CI
retrieves the top k = 3 documents ranked by TFIDF similarity using unigram + bigram features.
These parameters are tuned on the S CI FACT development set.
When making predictions using the R ATIO module described in §5, we find
that the usual decision rule of predicting ẑi = 1
when z̃i ≥ 0.5 works well for models trained on
S CI FACT. However, for models trained on F EVER
and UKP Snopes, we achieve better performance
by tuning the classification threshold t, such that
ẑi = 1 when z̃i ≥ t, on the S CI FACT dev set.
The best threshold was t = 0.025 when training
on F EVER, and t = 0.75 when training on UKP
Snopes.
NALE S ELECTION

A.3

We adopt similar settings as we used for the R A TIONALE S ELECTION module and only change the
learning rate to 1e-5 for the transformer base and
1e-4 for the linear layer for models trained on S CI FACT, F EVER, and UKP Snopes. When training on
claim / cited abstract pairs labeled N O I NFO, we use
the k sentences in the abstract with greatest similarity to the claim as rationales (§5). k is sampled
from {0, 1} with uniform probability.
A.4

Training the R ATIONALE S ELECTION
module

We experiment with various learning rates when
training S CI BERT, BioMedRoBERTa, RoBERTabase, and RoBERTa-large. Below we describe the
setting for training RoBERTa-large.
For models trained on S CI FACT, we use an initial learning rate of 1e-5 on the transformer base
and 1e-3 on the linear layer. For F EVER + S CI FACT, the learning rate is set to 1e-5 for the entire
model for pre-training on F EVER and fine-tuning
on S CI FACT. We use a batch size of 256 through
gradient accumulation and apply cosine learning
rate decay over 20 epochs to find the best performing model on the dev set.
For models trained on F EVER, we set the learning rate to 5e-6 for the transformer base and 5e-5
for the linear layer. For models trained on UKP
Snopes, we set the learning rate 1e-5 for the transformer base and 1e-4 for the linear layer. We find
that these learning rates help the models converge.
We only train the model for 3 epochs on F EVER
and 5 epochs on UKP Snopes because they are
larger datasets and the models converged within
early epochs.

Additional training details

All models are trained using a single Nvidia P100
GPU on Google Colabortoary Pro platform.8 For
the R ATIONALE S ELECTION module, it takes about
150 minutes to train on S CI FACT for 20 epochs.
120 minutes on UKP Snopes for 5 epochs, and 700
minutes on F EVER for 3 epochs. For the L ABEL P REDICTION module, it takes about 130 minutes
to train on S CI FACT for 20 epochs, 160 minutes
on UKP Snopes for 5 epochs, and 640 minutes on
F EVER for 3 epochs.
A.5

A.2

Training the L ABEL P REDICTION
module

Hyperparameter search

The learning rate, batch size, and number of epochs
are the most important hyperparameters. We perform manual tuning and select the hyperparameters
that produce the highest F1 on the development
set. For the learning rate, we experiment with 1e-3,
1e-4, 5e-5, 1e-5, and 5e-6. For batch size, we experiment with 64 and 256. The number of epochs
are cutoff after the model converges.

B

Statistical analysis

We assess the uncertainty in the results reported
in the main results (Table 4) using a simple bootstrap approach (Dror et al., 2018; Berg-Kirkpatrick
et al., 2012; Efron and Tibshirani, 1993). Given
our test set with ntest = 300 claims, we generate nboot = 10, 000 bootstrap-resampled test sets
by resampling (uniformly, with replacement) ntest
claims from the test set. For each resampled test set,
we compute the metrics in Table 4. Table 6 reports
the mean and standard deviation of these metrics,
computed over the bootstrap samples. Table 7 reports dev set metrics. Our conclusion that training
on S CI FACT improves performance is robust to the
uncertainties presented in these tables.
8

https://colab.research.google.com/

Sentence-level
Selection-Only
Selection+Label
Retrieval
Oracle
abstract

Open

Model

Row

P

R

F1

P

R

F1

Oracle rationale

1

100.00.0

80.53.3

89.22.1

89.62.7

72.23.7

79.93.0

Zero-shot
V ERI S CI

2
3

42.62.2
76.22.9

45.23.2
63.93.6

43.82.0
69.42.6

36.22.5
66.53.4

38.43.0
55.73.7

37.22.3
60.63.1

Oracle rationale

4

100.00.0

56.64.0

72.23.3

87.63.5

49.53.9

63.23.7

Zero-shot
V ERI S CI

5
6

28.72.3
45.03.0

37.63.4
47.43.8

32.52.3
46.13.0

23.82.3
38.53.0

31.13.1
40.63.6

26.92.3
39.53.0

(a) Sentence-level results.
Label-Only
Retrieval
Oracle
abstract

Open

Model

Row

Oracle rationale

Abstract-level
Label+Rationale

P

R

F1

P

R

F1

1

90.12.2

77.52.8

83.32.4

90.12.2

77.52.8

83.32.4

Zero-shot
V ERI S CI

2
3

86.92.9
87.32.6

53.63.4
65.33.2

66.33.1
74.72.8

67.93.9
84.92.8

41.93.2
63.53.2

51.83.4
72.62.9

Oracle rationale

4

88.92.7

54.13.5

67.23.2

88.92.7

54.13.5

67.23.2

Zero-shot
V ERI S CI

5
6

56.03.9
47.53.3

42.33.4
47.33.5

48.23.3
47.43.1

42.34.0
46.63.3

32.03.2
46.43.5

36.43.3
46.43.1

(b) Abstract-level results

Table 6: Test set results as in Table 4, reporting mean and standard deviation over 10,000 bootstrap samples.
Standard deviations are reported as subscripts. Some means reported here are slightly different from Table 4 due
to sampling variability.

Sentence-level
Selection-Only
Selection+Label
Retrieval
Oracle
abstract

Open

Model

Row

P

R

F1

P

R

F1

Oracle rationale

1

100.00.0

81.93.2

90.01.9

91.42.5

74.93.6

82.32.9

Zero-shot
V ERI S CI

2
3

40.72.1
79.42.7

48.13.4
59.03.6

44.02.1
67.72.8

36.12.5
71.43.5

42.63.4
53.03.6

39.02.5
60.83.3

Oracle rationale

4

100.00.0

58.44.3

73.73.4

90.23.3

52.74.3

66.43.9

Zero-shot
V ERI S CI

5
6

28.62.0
52.53.5

38.53.6
43.83.7

32.82.3
47.73.2

24.82.2
46.93.7

33.43.4
39.23.6

28.42.4
42.63.2

(a) Sentence-level results.
Label-Only
Retrieval
Oracle
abstract

Open

Abstract-level
Label+Rationale

Model

Row

P

R

F1

P

R

F1

Oracle rationale

1

91.42.2

76.13.0

83.02.5

91.42.2

76.13.0

83.02.5

Zero-shot
V ERI S CI

2
3

88.92.8
91.02.3

58.33.7
67.43.3

70.43.2
77.42.7

69.23.9
85.22.9

45.43.5
63.23.5

54.83.5
72.53.1

Oracle rationale

4

91.02.6

53.13.8

67.03.4

91.02.6

53.13.8

67.03.4

Zero-shot
V ERI S CI

5
6

52.73.7
55.43.7

41.63.7
47.53.6

46.53.4
51.03.3

43.63.7
52.63.7

34.43.5
45.13.6

38.43.3
48.53.3

(b) Abstract-level results

Table 7: Dev set results as in Table 4, reporting mean and standard deviation over 10,000 bootstrap samples.

Journal
BMJ
Blood
Cancer Cell
Cell
Cell Metabolism
Cell Stem Cell
Circulation
Immunity
JAMA
Molecular Cell
Molecular Systems Biology
Nature
Nature Cell Biology
Nature Communications
Nature Genetics
Nature Medicine
Nature Methods
Nucleic Acids Research
Plos Biology
Plos Medicine
Science
Science Translational Medicine
The Lancet

Count
60
8
8
51
10
41
12
33
79
27
5
29
26
19
8
89
1
10
36
38
7
2
22

Other

120

Total

741

Table 8: Number of cited documents by journal. Some
co-cited articles (§3.1) come from journals outside our
curated set; these are indicated by “Other”.

C
C.1

Dataset collection and corpus statistics
Corpus

Source journals Table 8 shows the number of
cited abstracts from each of our selected journals.
The “Other” category includes “co-cited” (§3.1)
abstracts that came from journals not among our
pre-defined set.
Distractor abstracts In §3.1, we mention how
we increase the size of the corpus by adding distractor abstracts. The reason why we do not use
the entirety of a large research corpus like S2ORC
as our fact-checking corpus is that doing so would
introduce many false negative retrievals: abstracts
containing evidence relevant to a given claim, but
not mentioned in the claim’s source citance. This
can occur either because the citance authors simply
were not aware of these abstracts, or because the
abstracts were published after the citance was writ-

Source citance
"Future studies are also warranted to evaluate
the potential association between WNT5A/PCP
signaling in adipose tissue and
atherosclerotic CVD, given the major role that
IL-6 signaling plays in this condition as
revealed by large Mendelian randomization
studies 44, 45 ."

Claim
IL-6 signaling plays a major role in
atherosclerotic cardiovascular disease.

Figure 4: A claim written based on a citance. Material unrelated to the citation is removed. The acronym
“CVD” is expanded to “cardiovascular disease”.

ten. These retrievals would be incorrectly marked
wrong by our evaluation metrics.
Distractor abstracts as defined in §3.1 have two
qualities that make them a good addition to the
S CI FACT corpus: (1) They are cited in the same
articles as our evidence abstracts, meaning that
they often discuss similar topics and increase the
difficulty of abstract retrieval methods based on
lexical similarity. (2) The authors of our citances
were aware of the distractor abstracts, and chose
not to mention them in the citances used to generate
claims. This makes them unlikely to be a source of
false negative retrievals.
C.2

Annotation examples

Converting citances to claims Figure 4 shows
an example of a citance re-written as a claim.
The citance discusses the relationship between
“atherosclerotic CVD” and “IL-6”, and cites two papers (44 and 45) as evidence. To convert to a claim,
the acronym “CVD” is expanded to “cardiovascular disease”, irrelevant information is removed, and
the claim is written as an atomic factual statement.
Multiple rationales Figure 5 shows a claim supported by two rationales from the same abstract.
The text of each rationale on its own is sufficient to
entail the claim.
C.3

Annotators and quality control

Claim writing Student claim writers attended an
in-person training session where they were introduced to the task and received in-person feedback
from the four experts. Following training, student
annotators continued writing claims remotely. The
expert annotators monitored claims for quality during the remote annotation process, and provided

Claim
Antibiotic induced alterations in the gut
microbiome reduce resistance against
Clostridium difficile

Decision: SUPPORTS
Antibiotics can have significant and longlasting effects on the gastrointestinal tract
microbiota, reducing colonization resistance
against pathogens including Clostridium
difficile.
Rationale 1
Our results indicate that antibiotic-mediated
alteration of the gut microbiome converts the
global metabolic profile to one that favours
C. difficile germination and growth.
Rationale 2

Figure 5: A claim supported by two rationales from the
same abstract. The text of each rationale on its own
provides sufficient evidence to verify the claim.

feedback when necessary; low-quality claims were
returned to the annotators for re-writing. As a final
check, all submitted claims were proofread (and
edited if necessary) by an undergraduate whose
claims were deemed especially high-quality by the
expert annotators.
Claim negations As mentioned in §3.2, an expert annotator wrote claim negations to introduce
cases where an abstract R EFUTES a claim. The annotator skipped claims that could only be negated
by adding obvious triggers like “not”. The majority of claim negations involved a reversal of
effect direction; for instance “A high microerythrocyte count protects against severe anemia” can be
negated as “A high microerythrocyte count raises
vulnerability to severe anemia”.
Claim verification Annotations were performed
remotely through a web interface. Annotators were
required to pass a 10-question “quiz” before annotating their own claims. After passing the quiz,
subsequent submissions were reviewed by an NLP
expert until that expert deemed the annotator reliable. Approved annotators were then assigned to
review each others’ submissions. In general, graduate students were assigned to review annotations
from undergraduates.

D

Annotation interfaces and guidelines

We show a screenshot of the claim writing interface
in Figure 6, and the claim verification interface in
Figure 7. The complete annotation guide for claim
verification is available at the following URL:

https://scifact.s3-us-west-2.amazonaws.
com/doc/evidence-annotation-instructions.
pdf.

Figure 6: The claim-writing interface. The citation sentence is highlighted in blue on the top left. Additional
context is provided on bottom left. The right side shows two claims that could be written based on this citation
sentence.

Figure 7: The evidence collection interface.

