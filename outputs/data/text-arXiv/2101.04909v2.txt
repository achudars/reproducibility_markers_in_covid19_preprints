COVID-19 P ROGNOSIS VIA S ELF -S UPERVISED
R EPRESENTATION L EARNING AND M ULTI -I MAGE P REDICTION
A P REPRINT

arXiv:2101.04909v2 [cs.CV] 25 Jan 2021

Anuroop Sriram∗
Facebook AI Research
Joelle Pineau
Facebook AI Research

Matthew Muckley∗
Facebook AI Research

Krzysztof J. Geras
NYU School of Medicine

Koustuv Sinha
Facebook AI Research

Lea Azour
NYU School of Medicine

Nafissa Yakubova
Facebook AI Research

Farah Shamout
NYU Abu Dhabi

Yindalon Aphinyanaphongs
NYU School of Medicine

William Moore
NYU School of Medicine

January 26, 2021

A BSTRACT
The rapid spread of COVID-19 cases in recent months has strained hospital resources, making rapid
and accurate triage of patients presenting to emergency departments a necessity. Machine learning
techniques using clinical data such as chest X-rays have been used to predict which patients are most
at risk of deterioration. We consider the task of predicting two types of patient deterioration based
on chest X-rays: adverse event deterioration (i.e., transfer to the intensive care unit, intubation, or
mortality) and increased oxygen requirements beyond 6 L per day. Due to the relative scarcity of
COVID-19 patient data, existing solutions leverage supervised pretraining on related non-COVID
images, but this is limited by the differences between the pretraining data and the target COVID-19
patient data. In this paper, we use self-supervised learning based on the momentum contrast (MoCo)
method in the pretraining phase to learn more general image representations to use for downstream
tasks. We present three results. The first is deterioration prediction from a single image, where our
model achieves an area under receiver operating characteristic curve (AUC) of 0.742 for predicting
an adverse event within 96 hours (compared to 0.703 with supervised pretraining) and an AUC of
0.765 for predicting oxygen requirements greater than 6 L a day at 24 hours (compared to 0.749
with supervised pretraining). We then propose a new transformer-based architecture that can process
sequences of multiple images for prediction and show that this model can achieve an improved AUC
of 0.786 for predicting an adverse event at 96 hours and an AUC of 0.848 for predicting mortalities
at 96 hours. A small pilot clinical study suggested that the prediction accuracy of our model is
comparable to that of experienced radiologists analyzing the same information.

1

Introduction

The SARS COV-2 infection (COVID-19) has caused a surge of patients with respiratory illness presenting to emergency
departments worldwide throughout 2020. When patients arrive at the emergency department, hospitals need to evaluate
their risk of deterioration for effective clinical decision making and resource allocation. This is especially important
during a pandemic when hospital resources are strained. Initial patient assessment often includes imaging studies, and
these initial assessments can be used for resource planning within the hospital. Chest X-ray radiography (CXR) is
a well-established imaging examination for patients with symptoms of shortness of breath and fever [1]. Although
traditionally used for diagnosis, the use of CXR has expanded to COVID deterioration prediction [2, 3, 4] and is a
standard component of COVID patient health assessments.
* Equal

contribution.

A PREPRINT - JANUARY 26, 2021

Deep learning methods for image-based diagnosis are a standard tool used in radiology research [5, 6, 7, 8, 9, 10]
and have been applied to COVID-19 [2, 3, 11, 12, 13, 14, 15]. Most deep learning methods for radiology rely on
the collection of large numbers of labeled radiology images for supervised training, which introduces a number of
constraints. First, the collection of large, labeled, training sets is expensive [7], and can only be accomplished by
well-funded research organizations. Second, it can be difficult to assign labels for many radiology tasks. Radiologists
are often tasked to give impressions of images and radiology reports are expected to convey nuance and uncertainty with
regards to the imaging findings. This can create downstream uncertainty in label assignment [16]. Third and related,
the requirement for large datasets often restricts the use of deep learning methods to established and well-understood
pathologies. Thus far, large public X-ray datasets have been collected at major research centers over time spans as
long as a decade [17, 16, 18]. The availability of such data is essential to facilitate new research into machine learning
methods for improving image-based diagnosis. However these datasets do not capture data from emerging diseases,
such as COVID-19.
At this time, most hospital centers have not collected enough COVID-19 patient data at the scale required to train deep
learning representations. Technological and privacy considerations prevent data pooling between many research centers.
One common approach to dealing with small amounts of labeled data is to apply transfer learning [19], where a model
is pretrained in a supervised fashion on a large, labeled dataset (e.g., ImageNet), then finetuned on the task of interest.
However, transfer learning can lead to poor performance if the tasks are too different, and the model is not able to learn
the features necessary for the transfer task during the pretraining step.
Recently, new self-supervised methods which rely on contrastive losses have been shown to generate representations
that are as good for classification as those generated using purely supervised methods [20, 21, 22, 23]. The advantage
of contrastive loss functions is that they are able to achieve feature extraction independent of labels or tasks associated
with the pretraining dataset. These features are then used for training a classifier in the fine-tuning stage with the
target data. In this paper, we study the applicability of self-supervised learning to the task of COVID-19 deterioration
prediction. We pretrain a model using momentum contrast (MoCo) [22, 23] on two large, public chest X-ray datasets,
MIMIC-CXR-JPG [18, 24, 25] and CheXpert [16], and then use the pretrained model as a feature extractor for the
downstream task of prediction COVID-19 patient outcomes. Concurrent to our work, Sowrirajan et al. [26] have
shown that MoCo pre-training can be used to learn representations from chest X-rays that are useful for identifying the
presence of pleural effusions.
We study the effectiveness of this approach on three downstream tasks: 1) adverse event prediction from single images
(SIP), oxygen requirements prediction from single images (ORP), and adverse event prediction from multiple images
(MIP). We find that, when using the DenseNet architecture [27, 6, 10], self-supervised training achieved higher areas
under receiver operating characteristic curve (AUC) on a hold-out test set than supervised training for predicting
an adverse event at all time points. Self-supervised pretrained models achieved higher AUC values for the oxygen
requirements prediction task at early time points. We then show that performance for predicting adverse events can be
further improved by making predictions based on image sequences with a Transformer model. Finally, we characterize
the performance of our single-image models vs. the previous COVID-GMIC model [2] and our multi-image models vs.
human radiologists in a pilot clinical study. In combination with this paper, we also open source our code and models
pretrained on public data at https://github.com/facebookresearch/CovidPrognosis for other researchers
interested in using them for fine-tuning on their own X-ray datasets.

2
2.1

Datasets
Public Chest X-ray Datasets (Pretraining Data)

We used two datasets for pretraining our models [18, 24, 25] and CheXpert [16]. The MIMIC-CXR dataset consists of
377,110 chest X-ray images corresponding to 227,835 radiographic studies performed at the Beth Israel Deaconess
Medical Center in Boston, MA. The CheXpert dataset consists of 224,316 chest radiographs of 65,240 patients compiled
from chest radiographic studies performed at the Stanford Hospital. The datasets did not contain any COVID-19
patients during the time of our study as they were collected before the COVID-19 pandemic. Imaging findings included
a variety of chest conditions such as atelectasis, cardiomegaly, consolidation, edema, and pleural effusion, based on a
labeler designed for the CheXpert dataset [18, 16]. We did not use these findings in our study beyond initial validation
of the training pipeline.
2.2

COVID-19 Dataset and Deterioration Labels (Fine-tuning Data)

For fine-tuning, we used an extended version of the NYU COVID dataset previously described [2], containing 26,838
X-ray radiographs from 4,914 patients. COVID-19 prognosis can be defined by a wide array of factors. Our goal
2

A PREPRINT - JANUARY 26, 2021

in this study is to predict patient deterioration that will require physical transfers throughout the hospital or altered
equipment usage. As an example, one type of deterioration would be a transfer to the ICU. During the transfer process,
a COVID-positive patient is removed from isolation, heightening the risk of intra-hospital infection. By locating at-risk
patients closer to the ICU from the beginning, the the transfer time can be reduced, correspondingly reducing the risk of
intra-hospital infection. Additionally, there and limited number of intensive care beds available per hospital. Predicting
the need for these beds is critical to hospital resource management. Another form of deterioration is increased oxygen
utilization. Normally, patients are administered oxygen via the nasal cannula. When a patient’s oxygen requirements
exceeds 6 L per day, oxygen administration through the nasal cannula is no longer possible and delivery must be done
via a mask.
Formally, our study considers two label classes: 1) adverse events [2, 3] and 2) increased oxygen requirements. An
“adverse event” consists of any of the three events: transfer to the intensive care unit (ICU), intubation, or mortality. We
labeled each image with whether the patient developed any adverse event within 24, 48, 72 or 96 hours of the scan. We
obtained these labels by occurrences of adverse events recorded in the patient’s anonymized electronic health records
data shared by NYU, following previous practice [2]. We define “increased oxygen utilization” as an event where a
patient requires more than 6 L of oxygen in a day. Similarly to the case of adverse events, we labeled each image in the
dataset with whether the patient required increased oxygen on within 24, 48, 72, or 96 hours of the scan.
2.3

Definition of Prediction Tasks

The goal of our machine learning models is to predict deterioration labels from either single chest radiograph or
sequences of chest radiographs. The first task is to predict adverse events from a single chest radiograph. This
constitutes a multi-class classification setting for ICU transfer, intubation, mortality, and “Any” adverse event at 24, 48,
72, 96, and “Any” time windows (20 total labels). We call this task the Single Image Prediction (SIP) task. The second
task is to predict increased oxygen requirements from a single chest radiograph. This also constitutes a multi-class
classification setting with increased oxygen at 24, 48, 72, 96, and “Any” time windows (5 total labels). We call this the
Oxygen Requirement Prediction (ORP) task.
Our final task is to predict adverse events from a sequence of radiographs. This task is of particular interest, as it more
closely aligns with how radiologists interpret images. Radiologists use relative changes in images to identify a patient’s
trajectory. For this task, we considered the same multi-class classification problem as in the SIP setting. We call this
task the Multiple Image Prediction (MIP) task.
2.4

Data Splits

Table 1 shows the counts of scans and patients from the NYU COVID data in each split for each task. The tasks vary
in counts due to varying selection criteria used for each task. For the SIP task, we followed the previous standard [2]
in including chest radiographs of patients collected in the emergency department and excluding chest radiographs of
patients who had already experienced any of the three adverse events, as they had already deteriorated. This enables
us to directly compare our results to those in [2]. For the ORP task, we only included chest radiographs of patients
collected in the emergency department, and did not apply any other exclusion criteria because we considered the oxygen
requirement prediction to be independent of the occurrence of any adverse event. In the MIP task, the goal was to
predict deterioration of patients in the emergency department and during any subsequent hospital admissions. Therefore,
for this task we only excluded chest radiographs collected after a patient had experienced any adverse event.

Task
SIP
MIP
ORP

Table 1: Subsplits for prediction tasks.
# Scans
# Patients
{train, val}

test

total

{train, val}

test

total

5,617
17,915
15,370

770
2,205
5,711

6,387
20,120
21,081

2,943
2,774
2,887

718
438
1,096

3,661
3,212
3,983

Figure 1 shows the number of adverse events in the SIP and MIP splits as a function of time. We note the labels are
necessarily monotonically increasing due to the fact that adverse events at 48 hours include all events at 24 hours, etc.
We did not make any special adjustments to our models to take advantage of this aspect, but this could be a topic for
future investigation.
3

A PREPRINT - JANUARY 26, 2021

Adverse Event Counts

Adverse Events Over Time
1200

ICU (MIP)
Intubated (MIP)
Mortality (MIP)
ICU (SIP)
Intubated (SIP)
Mortality (SIP)

1000
800
600
400
200
24

48
72
Adverse Event Time (hours)

96

Figure 1: Distribution of adverse events with respect to the time from chest X-ray acquisition for the single-image (SIP)
and multiple image (MIP) prediction tasks.

3

Methods

Here we describe the models and training process. All of our proposed models were based on the DenseNet-121
architecture [27], which has been previously applied to chest radiographs [6, 10, 3]. We begin by describing our
self-supervised pretraining procedure, then follow with the fine-tuning procedure for the SIP and ORP tasks. Lastly, we
describe the development of a new transformer based architecture that we applied to the MIP task.
3.1

Self-Supervised Pretraining using Momentum Contrast Learning

The contrastive loss framework is as follows: a deep neural network is constructed for mapping an image to a latent
space [20]. The neural network is trained to minimize a contrastive loss. The contrastive loss is constructed such that
similar images are mapped to vectors that are closer to each other (as measured by a contrastive loss function) while
dissimilar images are mapped to vectors that are further apart. Figure 2 shows an schematic of the contrastive loss
training procedure.

encoder

rq

Aug. 1
xq

L(rq , rk )

Aug. 2
base image

momentum
encoder

rk

xk
Figure 2: Diagram for momentum contrast training. A base image is transformed via two random augmentations
(Aug. 1 and Aug. 2) into images xq and xk . xq is passed through an encoder network, while xk is passed through a
momentum encoder network. The representations generated by each network are then passed into a contrastive loss that
promotes similarity between the representations rq and rk .
Each training step begins by selecting a base image, x, from a training dataset of unlabeled images. Then, two different
augmentations are chosen at random and applied to the base image separately to generate two augmented images, the
query image xq and the key image xk . These images are passed through two different neural networks, called the
encoder and the momentum encoder respectively, to generate representation rq for xq and rk for xk .
4

A PREPRINT - JANUARY 26, 2021

The goal of the contrastive loss is to identify that rq and rk come from the same underlying image in the presence of
divergent augmentations. In practice contrastive losses usually require very large batch sizes due to the fact that many
negative examples (i.e., values for rq,j and rk,i where rk,i is not from the same image) are necessary to achieve strong
performance [21].
MoCo [22] is a recent contrastive loss method that avoids the need for large batch sizes by maintaining a queue of
representations. Within the queue, the model stores K examples of rk,i for i ∈ [1, ..., K]. The model is then asked to
identify which of the K examples is the matching one for rq . This can be modeled mathematically via the InfoNCE
contrastive loss function [28]:
exp (rq · rk+ /τ )
L (rq , rk ) = −log PK
,
(1)
i=1 exp (rq · rk,i /τ )
where τ is a temperature hyperparameter and K is the number of currently stored representations. The gradient from the
contrastive loss is backpropagated to the encoder network, and then the momentum encoder is updated via a momentum
update [22]. The momentum update forces the momentum encoder to change more slowly than the encoder network,
which helps stabilize training.
3.2

Single Image Prediction (SIP) and Oxygen Requirement Prediction (ORP) Models

The SIP and ORP tasks were described in Section 2.3. We extend the MoCo encoder model to an image classifier
model by appending a linear classifier to the end of the encoder pipeline. We then fine-tune the model on the COVID
image dataset, reshaping input images to a 224 × 224 grid and applying random horizontal and vertical flipping during
training. We used the Adam optimizer [29] with a cosine annealing learning rate decay [30].
3.3

Multiple Image Prediction (MIP) Model

For the MIP task, we propose a new model that takes a sequence of X-ray images (x0 , ..., xn ) along with their scan
times (t0 , ..., tn ) relative to the final scan as inputs and predicts the likelihood of adverse events occurring after the final
scan. The overall model structure is shown in Figure 3. The scan times are represented as the number of hours from the
final scan time. Thus, if a patient had two previous scans done, e.g., 50 hours and 20 hours before the final scan, then
the scan times will be represented as (t0 = −50, t1 = −20, t2 = 0). The final scan time tn is always 0.
During the forward pass, each image xi is first passed to the MoCo encoder model in parallel to obtain an image
representation hi . Separately, each relative scan time ti is passed to a Continuous Position Embedding (CPE) module
to learn a time embedding ei . The CPE module, described in more detail below, maps each time point to a different
embedding. The two representations hi and ei for each i are then concatenated together and then projected to a lower
dimension using a fully connected layer. The full sequence of images is then input to a transformer network [31]. The
transformer network maps an input sequence to an output sequence of the same length using a self-attention mechanism.
The output of the transformer is the sum-pooled to obtain a hidden representation f that contains aggregated information
from all images. This hidden representation f is input to a linear classifier.
Similar to the SIP task, the weights of the MoCo encoder are initialized to the pretrained weight values while the
weights of the transformer and the linear classifier are randomly initialized. The entire network is fine-tuned jointly on
a labeled dataset to mimize the binary cross entropy loss.
Continuous Positional Embedding (CPE) The CPE, inspired by the positional embedding (PE) from [31], is
designed to map each time point to a d-dimensional vector representation. The CPE has the same functional form as the
PE, but it can take continuous values within a certain range as input. Formally, the CPE maps a relative scan time s to a
vector e as:


e(t,2i) = sin t/100002i/d ,


e(t,2i+1) = cos t/100002i/d ,

(2)

where t ∈ [0, 360). We only include X-ray scans taken within the last 360 hours (15 days) of the end exam since the
COVID-19 infection is unlikely to last longer than 15 days.
DropImage Regularizer To reduce overfitting and to improve the robustness of the MIP model, we apply a masking
based regularizer that we call the DropImage regularizer. The DropImage regularizer, when applied to a an image
5

A PREPRINT - JANUARY 26, 2021

Figure 3: Schematic of the Multiple Image Prediction (MIP) model. The MIP model takes a sequence of images along
with their relative scan times as input and outputs a binary prediction for each adverse event. The MoCo encoder
model is applied to each image to learn image representations. These image representations are concatenated with a
time embedding obtained from the relative scan times and then projected to a lower dimension and then input to a
transformer network to aggregate information from all images. The output of the transformer is pooled together and
input to a linear classifier.

sequence, drops a subset of images chosen independently at random. The final image is never removed. DropImage
encourages learning more robust models that can make good predictions even if some of the past images were missing.
Further, DropImage can also be viewed as a data augmentation scheme.

4
4.1

Experiments
Pretraining

We applied both supervised and self-supervised pretraining procedures. Our supervised pretraining was similar to
that for CheXpert [16]. For supervised pretraining, we trained the DenseNet-121 model with the Adam optimizer
[29] with a learning rate of 10−3 , weight decay of 10−5 , and batch size of 64 on the MIMIC-JPG dataset [24]. Data
augmentation included interpolation to a 224 × 224 grid and random vertical/horizontal flipping. We pretrained for 10
epochs, decaying the learning rate by a factor of 10 each epoch.
We pretrained our self-supervised model using the momentum contrast training described in Section 3.1. For data
augmentation, we used random cropping and interpolation to a 224 × 224 grid, random horizontal/vertical flipping, and
random Gaussian noise addition. We investigated a number of augmentation strategies for pretraining the MoCo models,
including noise additions, affine transformations, color transformations, and X-ray acquisition simulation, but in the
end we found that these provided little benefit beyond those in the original MoCo paper [22]. After the augmentations,
we applied histogram normalization. The histogram normalization procedure preserves the relative values of the pixels
by interpolating along a shifted version of the image pixel value histogram while constraining the resulting pixel values
to be in the specified target range. The augmentations are described in further detail in Appendix A.
6

A PREPRINT - JANUARY 26, 2021

We tuned the following hyperparameters during the pretraining phase: learning rate, MoCo latent feature dimension
size, and the queue size. We searched over a logarithmic scale of values, varying the learning rate within 10{−2,−1,0} ,
and MoCo feature dimensions within {64, 128, 256}. The queue size was fixed at 65,536. We used a batch size
of 128 for each of 8 GPUs, the largest we could achieve in initial testing, accumulating gradients using PyTorch’s
DistributedDataParallel framework [32, 22]. We selected hyperparameters based a cross-validation analysis on the
downstream tasks. We optimized models using stochastic gradient descent with momentum [33], using 0.9 as the
momentum term and a weight decay parameter of 10−4 . Pretraining took approximately four days using eight 16 GB
Nvidia V100 GPUs.
4.2

Example Downstream Task Outputs

Figure 4 illustrates the frameworks for each of the prediction tasks over the succeeding 24 hours. Figure 4a shows an
example of SIP predictions from a patient with increased lung opacity. In this case the patient did not suffer any adverse
event in the next 24 hours, but ultimately suffered all three adverse events within 72 hours. Figure 4b shows an example
of ORP predictions from a patient that required increased oxygeen within 24 hours. In Figure 4c, a sequence of chest
X-rays with increasing lung opacity is used by the MIP model to predict COVID deterioration for a patient. The images
were taken 49 hours apart. For the case of Figure 4c, the patient was transferred to the ICU, intubated, and suffered a
mortality within 24 hours.

SIP Predictions
ICU24: 0.844
Int24: 0.997
Mor24: 0.559

ORP Predictions
>6L24: 0.768

(a)

(b)

MIP Predictions
ICU24: 0.784
Int24: 0.782
Mor24: 0.965
(c)

Figure 4: Example model outputs for ICU (ICU24), intubation (Int24) mortality (Mor24), and oxygen greater than
6 L per day (>6L24) prediction tasks, all at 24 hours. (a) Example SIP outputs based on a single image with evident
increased lung opacity. In this case the patient did not suffer any adverse event in the next 24 hours, but would ultimately
suffer all three adverse events within 72 hours. (b) Example ORP output based on a single image. This patient required
greater than 6 L per day of oxygen within 24 hours. (c) Example MIP outputs. Both images were taken from the
same patient with 49 hours of separation. Increased lung opacity is observable in the second (later) image. The patient
suffered all three adverse events within 24 hours.

4.3

Single-Image Task Ablations

We considered both self-supervised and supervised pretraining for each of our single-image tasks (SIP and ORP). Data
splits for these tasks are shown in Table 1. Following previous work [16, 13, 3], we compare models via their areas
under receiver operator characteristic curve (AUC). In order to facilitate a straightforward comparisons, our pretraining
in this section for both supervised and MoCo only used the MIMIC dataset.
Before fine-tuning for the SIP and ORP tasks, the weights of the encoder are initialized to the pretrained weight values
while the weights of the linear classifier are randomly reinitialized with a uniform distribution [34]. We considered
three different fine-tuning ablations. The first ablation (CL) fixed the MoCo encoder (including BatchNorm statistics)
and only trained the new linear classifier [22]. The second ablation (FT) allowed the entire model to train, not just
the classifier. The final ablation (FT RA) allowed the entire model to train and also incorporated further random
augmentations (rotation, X-shear, Y-shear, and translations). The CL model was fine-tuned for 5 epochs. The FT models
were fine-tuned for 20 epochs, and the FT RA models were fine-tuned for 40 epochs. We also show a “Scratch” ablation,
which is a DenseNet model with random initialization [34] trained only on the COVID data (i.e., no pretraining).
7

A PREPRINT - JANUARY 26, 2021

We tuned each ablation using a stratified 5-fold cross validation of the {train,val} split of the data. We tuned learning
rate in the range 10{−4,−3,−2,−1} and optimized over all MoCo/supervised-pretrained models for model selection. We
also tuned whether to use the Adam [29] or SGD [33] optimizers. All fine-tunings used the cosine annealing learning
rate decay [30]. After the cross-validation, we selected the best model for each ablation and applied a bootstrap analysis
to estimate performance characteristics on the test set.
To test for significance between methods we applied 1,000 bootstrap iterations to the test set and computed the difference
in AUC between the best MoCo method and the best supervised pretraining method. Methods where the best MoCo
AUC is significantly higher than the best supervised AUC are indicated with *. Our tables show results in two rows
for each method. The top row shows the holdout test AUC, while the bottom row shows percentile bootstrap 95%
confidence intervals. In some cases we have statistical significance with overlapping confidence intervals–this is because
the two methods are correlated, an aspect captured in the bootstrapped AUC differences. We chose this non-parametric
test over the standard method of DeLong [35, 36] to maintain a consistent presentation, but in a separate analysis found
the DeLong AUC difference test generally matched our non-parametric test.
Table 2: SIP AUCs Across Ablations (95% CI: Bootstrap)
AUC of Any Adverse Event Prediction

Scratch FT (No PT)
Supervised PT CL
Supervised PT FT
Supervised PT FT RA
Moco PT CL
Moco PT FT
Moco PT FT RA

24 hours

48 hours

72 hours

96 hours

0.616
(0.550, 0.679)

0.645
(0.588, 0.701)

0.651
(0.598, 0.705)

0.667
(0.620, 0.715)

0.668
(0.599, 0.738)
0.672
(0.610, 0.739)
0.658
(0.591, 0.722)

0.690
(0.625, 0.753)
0.697
(0.639, 0.754)
0.694
(0.638, 0.747)

0.681
(0.628, 0.730)
0.687
(0.631, 0.737)
0.686
(0.635, 0.736)

0.692
(0.645, 0.743)
0.702
(0.653, 0.750)
0.703
(0.658, 0.748)

0.691
(0.627, 0.748)
0.686
(0.619, 0.751)
0.658
(0.590, 0.724)

0.716
(0.662, 0.766)
0.719
(0.659, 0.769)
0.703
(0.646, 0.761)

0.711
(0.660, 0.758)
0.726*
(0.677, 0.774)
0.708
(0.656, 0.758)

0.730
(0.686, 0.775)
0.742*
(0.696, 0.785)
0.729
(0.683, 0.772)

A comparison via AUC values for supervised vs. self-supervised pretraining for the SIP task is shown in Table 2. AUC
values generally increased over longer time intervals. MoCo models had the highest AUC scores for predicting adverse
events at all time windows, achieving significance at 72 and 96 hours. The MoCo PT CL model (pretrained with MoCo,
only fine-tuning classification layer) had the highest AUC at 24 hours, whereas the PT FT model (which allowed the
entire model to fine-tune) achieved highest AUC at 48, 72 and 96 hours. The Scratch method performed worst across
all architectures and pretraining methods. Performance improved between Scratch and supervised pretraining, and then
again between supervised pretraining and MoCo pretraining.
A comparison via AUC values for Supervised vs. Self-Supervised pretraining for the ORP task is shown in Table 3.
The time course trend is inverted relative to Table 2, with O2 models becoming less accurate at larger time windows. In
this case, Supervised pretraining models performed more similarly to MoCo-pretrained models. The MoCo PT FT
model performing had the best AUC at 24 and 48 hours, while the Supervised PT CL model had the best AUC at 72 and
96 hours. Only 24 hour prediction using MoCo was significant based on the bootstrap tests.
4.4

SIP Comparison to COVID-GMIC

COVID-GMIC is a previous neural network image analysis model developed for the purpose of COVID prognosis
prediction [2]. COVID-GMIC utilizes supervised pretraining with an architecture explicitly designed for synthesizing
information at coarse and fine image scales [37] with supervised pretraining on the NIH dataset [17]. Our SIP
experiments used the same test set that was used for the COVID-GMIC, so we compare our models directly in Table 4.
For this comparison we included a different MoCo pretraining procedure where we used a combination of both the
MIMIC-CXR [18] and CheXpert [16] data sets for pretraining. Despite having more data, in our 5-fold cross-validation
of the train,val split the same the same model was selected as in Section 4.3.
8

A PREPRINT - JANUARY 26, 2021

Table 3: ORP AUCs Across Ablations (95% CI Method: Bootstrap)
AUC of O2 > 6L requirement Prediction

Scratch FT (No PT)
Supervised PT CL
Supervised PT FT
Supervised PT FT RA
Moco PT CL
Moco PT FT
Moco PT FT RA

24 hours

48 hours

72 hours

96 hours

0.696
(0.636, 0.754)

0.670
(0.619, 0.720)

0.654
(0.611, 0.697)

0.632
(0.591, 0.674)

0.749
(0.698, 0.793)
0.727
(0.676, 0.777)
0.715
(0.655, 0.769)

0.706
(0.659, 0.750)
0.701
(0.655, 0.746)
0.685
(0.641, 0.731)

0.712
(0.672, 0.751)
0.707
(0.668, 0.746)
0.695
(0.657, 0.731)

0.706
(0.671, 0.741)
0.694
(0.658, 0.729)
0.679
(0.644, 0.716)

0.699
(0.644, 0.750)
0.765*
(0.719, 0.807)
0.722
(0.671, 0.767)

0.683
(0.635, 0.724)
0.713
(0.671, 0.753)
0.696
(0.651, 0.739)

0.690
(0.654, 0.728)
0.702
(0.666, 0.737)
0.698
(0.661, 0.734)

0.686
(0.652, 0.722)
0.688
(0.654, 0.722)
0.694
(0.658, 0.725)

Table 4: SIP AUCs Across Methods (95% CI Method: Bootstrap)
AUC of Any Adverse Event Prediction

DenseNet MoCo (best)
COVID-GMIC

24 hours

48 hours

72 hours

96 hours

0.686
(0.619, 0.751)
0.695
(0.627, 0.754)

0.719
(0.659, 0.769)
0.716
(0.661, 0.766)

0.726
(0.677, 0.774)
0.717
(0.661, 0.766)

0.742
(0.696, 0.785)
0.738
(0.691, 0.781)

In this case we did not apply significance testing since we did not have the raw predictions from this paper. Both the
MoCo and COVID-GMIC methods are comparable across time points, with wide overlaps in confidence intervals. The
widest gap in performance was for predicting adverse events in the next 24 hours, where COVID-GMIC had a 0.02
higher AUC. At longer time intervals the performance of MoCo methods began to improve, showing higher AUC scores
than COVID-GMIC on the test set at 48, 72 and 96 hours.
4.5

Multi-Image Task Ablations

Table 5 shows ablations for the MIP task. In this case, we compared and tested the difference between the MoCo
PT CL ablation and the Transformer model built on top of MoCo representations. We trained each model with the
Adam optimizer [29] for 50 epochs with a batch size of 32. We tuned the following hyperparameters using grid
search: learning rate, drop image probability (pdrop ), projection dimension and the pooling method. We searched over
learning rate values of 10{−3,−2,−1} , pdrop values in {0, 0.1, 0.2, 0.5}, projection dimension in {16, 32, 64, 128}. For
the pooling method, we used either sum pooling or simply taking the final time step. We set the dropout value for the
transformer and the CPE to be 0.5 and weight decay to 10−5 .
Table 5: MIP AUCs Across Ablations (95% CI Method: Bootstrap)
AUC of Any Adverse Event Prediction

MoCo PT CL
MoCo PT + Transformer

24 hours

48 hours

72 hours

96 hours

0.747
(0.719, 0.776)
0.767*
(0.740, 0.796)

0.755
(0.730, 0.780)
0.778*
(0.752, 0.802)

0.758
(0.734, 0.782)
0.778*
(0.755, 0.800)

0.768
(0.746, 0.790)
0.786
(0.764, 0.807)

9

A PREPRINT - JANUARY 26, 2021

The Transformer method performed better than the single-image MoCo method for predicting adverse events at every
time window with statistical significance at 24, 48, and 72 hours. A more detailed version of Table 5 comparing
Single-image and multi-image prediction for other adverse events is shown in TableA1 of Appendix B. The MIP
Transformer model was also better at predicting ICU transfers, intubations, and mortalities. The improvements for ICU
transfers were significant at all time windows, while mortalities were significant at 72 and 96 hours. The performance
differences for intubation were not statistically significant.
4.6

Reader Study

We compared the performance of our model with two chest radiologists from NYU Langone Health in a pilot reader
study with a sample of 200 image sequences from the test set (note: this was a new, distinct reader study from that used
previously [2]). The results of the study for “any” adverse events are shown in Table 6, while Table A2 in Appendix C
shows an extended version of the data with breakdowns by adverse event category. We also present the results from
averaging the prediction of both radiologists (shown as “Radiologist A + B”).
Table 6: MIP AUCs Compared to Radiologists (95% CI Method: Bootstrap)
AUC of Any Adverse Event Prediction

MoCo PT + Transformer
Radiologist A
Radiologist B
Radiologist A + B

24 hours

48 hours

72 hours

96 hours

0.785
(0.717, 0.852)
0.786
(0.694, 0.840)
0.789
(0.709, 0.846)
0.784
(0.703, 0.774)

0.801
(0.739, 0.861)
0.769
(0.709, 0.840)
0.787
(0.716, 0.846)
0.787
(0.722, 0.786)

0.790
(0.726, 0.855)
0.733
(0.664, 0.804)
0.762
(0.693, 0.826)
0.761
(0.687, 0.754)

0.790
(0.727, 0.853)
0.739
(0.658, 0.797)
0.751
(0.684, 0.816)
0.754
(0.678, 0.745)

For this task the radiologists indicated the presence of intubation support devices in the X-ray. We performed an audit
of our selection criteria and found that in some cases these patients did not have an intubation event in the anonymized
electronic health records despite having the support devices visible on the chest X-ray. In other cases, the adverse event
was recorded later. Given that images with support devices were likely in the training set, we allowed radiologists to
simply predict positive for any cases where they saw a support device, as this information was available to the model as
well.
Both radiologists exhibited wide confidence intervals as indicated by bootstrap methods. Radiologist AUC values
trended better than our model for the task of intubation prediction (Appendix C), while our model trended better for
ICU transfer and mortality prediction. On the task of predicting any adverse event, our model performed better for
longer term prediction. The largest differences for AUC scores between radiologists and our model occurred when
predicting mortalities, where our model had better AUC scores. Radiologists were aware of the primary timeframe of
the dataset (i.e., April-June 2020).

5
5.1

Discussion
Performance Among Proposed Models

Our study examined whether performance for COVID prognosis prediction could be improved via the use of two
techniques: 1) self-supervised pretraining and 2) prediction based on image sequences. Self-supervised pretraining
achieved the highest AUC scores on our hold-out test set for all SIP tasks of interest. Our bootstrap statistical tests
found significance at 72 and 96 hours. For ORP tasks, supervised pretraining performed similarly to MoCo pretraining,
except for 24 hours. This may indicate that features associated with pathological lung findings are more predictive for
oxygen requirements than for adverse events, but this topic needs to be studied further.
Interestingly, the predictions for the SIP tasks became more accurate over longer time intervals, whereas the predictions
for the ORP tasks became more accurate over shorter time intervals. The SIP results could have improved due to the
presence of more positive labels at the later time windows. Label scarcity hindered model performance in all test sets,
as indicated by the bootstrap confidence intervals in Section 4.4 and Appendix B. However, the inversion of the trend
10

A PREPRINT - JANUARY 26, 2021

for ORP cannot be explained by label scarcity. It could be that there are image features that become readily apparent for
the ORP task close to the actual moment of increased oxygen needs.
Our greatest performance gain occurred when when predicting based on image sequences rather than single images.
These improvements were statistically significant via our non-parametric tests. Radiologist sentiment indicated that it is
difficult to identify patient conditions from single images, as temporal trajectories are critical for performing analyses.
The results of our transformer models support this intuition, with AUC scores improving for all tasks by at least 0.01
and as much as 0.05 when using image sequences rather than individual images.
5.2

Test Set Uncertainty

A recent meta-study noted that many prediction models targeting COVID are at a high risk of overfitting and bias [38].
We addressed this via two methods: 1) pretraining on non-COVID data and 2) explicit sample selection investigation
via bootstrap experiments. We observed that the final bootstrap confidence intervals in Section 4.4 were wide enough
to include broad classes of ablations, corroborating the uncertainties associated with small datasets. The greatest
sensitivity was with respect to positive labels, which were infrequent among the test splits for the various tasks. It
would be reasonable to expect forward performance anywhere within the range of the bootstrapped confidence intervals.
This uncertainty is on top of other caveats already well-known in the medical field, namely, uncertainty arising due to
changes in the patient population and treatment effects.
5.3

Pilot Clinical Study Limitations

Our pilot clinical study found that our multi-image prediction model had comparable performance to human radiologists
for most tasks and surpassed human radiologists in predicting mortalities. However, this study has some limitations.
We asked radiologists to rate the probability of adverse events over the upcoming time windows. Depending on the
hospital and local practices, this may not be a part of the standard radiologist workflow. Radiologists are typically asked
to summarize imaging findings and to convey nuance with how the information in image features may impact clinical
care. It is possible that with further calibration, radiologist performance could be improved for the task of adverse event
prediction. We present these results to help readers put model performance in the context of currently possible clinical
predictions.
5.4

Alternative Approaches

Supervised DenseNet pretraining was previously used [2] to generate higher AUC scores than we report here. The
previous work included further optimizations, including a larger hyperparameter sweep and averaging predictions of
several models. Some of these optimizations - in particular model ensembling - were not investigated in our study, but
could lead to a closing of the gap in performance between supervised and self-supervised models that we observe in
this paper.
Since the start of our work, new approaches for self-supervision have been published in the literature. Clustering can
be used to relax the approach in this paper that only looks at a single image for matching [15]. Based on our ORP
results, another approach that might be beneficial would be including labels as a form of weak supervision along side a
constrastive loss. Investigating these alternative approaches may provide further performance improvements for the
self-supervised methods.

6

Conclusion

The COVID-19 pandemic has created a major need for risk stratification of patients in clinical settings. Developing AI
methods for this disease is difficult, as acquisition of large datasets is difficult or impossible for most medical centers.
In this paper we attempted to address this problem via self-supervised contrastive loss pretraining. We found that
our single-image prediction model was able to match a previous model [2] using separate advances, opening up new
possibilities for research on synergies between contrastive pretraining and model architecture design. Our multi-image
model performance surpassed that of all single-image models. In comparison to radiologists, our multi-image prediction
model was comparable in its ability to predict patient deterioration and stronger in its ability to predict mortality. We
hope these contributions will assist the community going forward on the task of hospital resource planning.
11

A PREPRINT - JANUARY 26, 2021

7

Acknowledgments

We would like to thank the IT team at the NYU hospital for their help in acquiring and curating COVID-19 X-ray data.
We thank the members of the NYU medical vision team for helpful discussions and insights on X-ray data. We thank
Jean-Remi King, C. Lawrence Zitnick, Mark Tygert, Marcus Rohrbach, Tullie Murrell, Michal Drozdzal, Adriana
Romero and Lucas Caccia from Facebook AI Research for useful conversations on technical components of the work.
We also thank Michael Chasse and Louis Mullie from University of Montreal and CHUM for sharing their knowledge
on interpretation of COVID patient data. We would like to thank the Stanford CheXpert team for providing their dataset
for this research. Additionally, this work at NYU was supported in part by a grant from the National Institutes of Health
(P41EB017183).

A

Pretraining Augmentation Parameters

For self-supervised momentum contrast pretraining, we used the following augmentations (in order): random resizing/cropping, random horizontal flipping, random vertical flipping, random Gaussian blur, Gaussian noise addition,
and histogram normalization. For each sample, each random augmentation was applied with probability p = 0.5. The
cropping from the random resizing/cropping augmentation was done at an image scale uniformly distributed between
20% and 100% the size of the original image.
For the blur augmentation, we applied the following normalized Gaussian kernel:


1 x2 + y 2
1
√ exp −
,
g(x, y) =
2
2 σkernel
σkernel 2π

(3)

where σ was selected for each sample uniformly at random between 0.1 and 2.0 pixels.
We selected the standard deviation for the noise addition randomly according to the following formula:
µimage
σnoise =
,
(4)
SNR
where SNR was selected uniformly between 4 and 8 for each sample and µimage was the average pixel value of the input
sample image.

B

Bootstrap Results

The extended results for comparing single-image prediction to the MIP method are shown in Table A1 (* indicates
bootstrap significance at α = 0.05). In this case, the Transformer-based MIP model gave larger advantages over the SIP
MoCo pretrained models, with some labels being significant. This indicates the benefits when incorporating image
sequences for COVID prognosis prediction.

C

Reader Study (Extended)

The extended results for the reader study are shown in Table A2. In this case we bold the best-performing method
when the test AUC of all other methods are outside its 95% Bootstrap confidence interval. The DenseNet MoCo PT +
Transformer models had the highest test set AUC across all tasks except intubation and “Any” adverse event at 24 hours.
Generally, these scores occurred with significant overlap among the bootstrap confidence intervals. The one setting
where we observed significance was in the case of mortality prediction, where MIP model AUCs were substantially
higher than that of radiologists across all time points.

12

A PREPRINT - JANUARY 26, 2021

Table A1: MIP AUCs Across Methods (95% CI Method: Bootstrap)
AUC of Any Adverse Event Prediction

DenseNet MoCo PT
MoCo PT + Transformer

24 hours

48 hours

72 hours

96 hours

0.747
(0.719, 0.776)
0.767*
(0.740, 0.796)

0.755
(0.730, 0.780)
0.778*
(0.752, 0.802)

0.758
(0.734, 0.782)
0.778*
(0.755, 0.800)

0.768
(0.746, 0.790)
0.786
(0.764, 0.807)

AUC of ICU Prediction

DenseNet MoCo PT
DenseNet MoCo PT + Transformer

24 hours

48 hours

72 hours

96 hours

0.705
(0.673, 0.747)
0.725*
(0.672, 0.788)

0.706
(0.674, 0.740)
0.740*
(0.686, 0.798)

0.706
(0.672, 0.734)
0.738*
(0.689, 0.794)

0.712
(0.679, 0.738)
0.746*
(0.697, 0.789)

AUC of Intubation Prediction

DenseNet MoCo PT
DenseNet MoCo PT + Transformer

24 hours

48 hours

72 hours

96 hours

0.719
(0.681, 0.757)
0.794
(0.721, 0.863)

0.710
(0.674, 0.745)
0.754
(0.685, 0.822)

0.705
(0.672, 0.737)
0.725
(0.646, 0.791)

0.700
(0.668, 0.732)
0.690
(0.635, 0.751)

AUC of Mortality Prediction

DenseNet MoCo PT
DenseNet MoCo PT + Transformer

24 hours

48 hours

72 hours

96 hours

0.812
(0.769, 0.855)
0.825
(0.777, 0.870)

0.822
(0.790, 0.854)
0.827
(0.796, 0.860)

0.822
(0.792, 0.852)
0.825*
(0.795, 0.855)

0.817
(0.792, 0.843)
0.848*
(0.819, 0.865)

13

A PREPRINT - JANUARY 26, 2021

Table A2: MIP AUCs Compared to Radiologists (95% CI Method: Bootstrap)
AUC of Any Adverse Event Prediction

DenseNet MoCo PT + Transformer
Radiologist A
Radiologist B
Radiologist A + B

24 hours

48 hours

72 hours

96 hours

0.785
(0.717, 0.852)
0.786
(0.694, 0.840)
0.789
(0.709, 0.846)
0.784
(0.703, 0.774)

0.801
(0.739, 0.861)
0.769
(0.709, 0.840)
0.787
(0.716, 0.846)
0.787
(0.722, 0.786)

0.790
(0.726, 0.855)
0.733
(0.664, 0.804)
0.762
(0.693, 0.826)
0.761
(0.687, 0.754)

0.790
(0.727, 0.853)
0.739
(0.658, 0.797)
0.751
(0.684, 0.816)
0.754
(0.678, 0.745)

AUC of ICU Prediction

DenseNet MoCo PT + Transformer
Radiologist A
Radiologist B
Radiologist A + B

24 hours

48 hours

72 hours

96 hours

0.773
(0.690, 0.856)
0.733
(0.648, 0.821)
0.751
(0.650, 0.827)
0.733
(0.651, 0.737)

0.774
(0.700, 0.849)
0.742
(0.648, 0.806)
0.706
(0.639, 0.802)
0.733
(0.648, 0.728)

0.751
(0.671, 0.830)
0.678
(0.602, 0.762)
0.684
(0.609, 0.770)
0.694
(0.610, 0.690)

0.747
(0.668, 0.823)
0.676
(0.604, 0.764)
0.692
(0.613, 0.768)
0.697
(0.613, 0.691)

AUC of Intubation Prediction

DenseNet MoCo PT + Transformer
Radiologist A
Radiologist B
Radiologist A + B

24 hours

48 hours

72 hours

96 hours

0.768
(0.661, 0.874)
0.828
(0.747, 0.883)
0.844
(0.755, 0.887)
0.832
(0.757, 0.822)

0.717
(0.620, 0.813)
0.763
(0.673, 0.831)
0.752
(0.658, 0.824)
0.776
(0.668, 0.750)

0.704
(0.606, 0.802)
0.726
(0.648, 0.815)
0.735
(0.633, 0.809)
0.748
(0.642, 0.729)

0.690
(0.595, 0.784)
0.704
(0.614, 0.776)
0.704
(0.609, 0.771)
0.709
(0.612, 0.693)

AUC of Mortality Prediction

DenseNet MoCo PT + Transformer
Radiologist A
Radiologist B
Radiologist A + B

24 hours

48 hours

72 hours

96 hours

0.785
(0.683, 0.888)
0.673
(0.530, 0.811)
0.590
(0.429, 0.722)
0.626
(0.489, 0.630)

0.776
(0.679, 0.872)
0.658
(0.549, 0.772)
0.576
(0.443, 0.697)
0.625
(0.507, 0.625)

0.817
(0.746, 0.887)
0.642
(0.554, 0.750)
0.589
(0.488, 0.695)
0.627
(0.530, 0.630)

0.814
(0.750, 0.879)
0.643
(0.547, 0.729)
0.631
(0.546, 0.728)
0.640
(0.552, 0.642)

14

A PREPRINT - JANUARY 26, 2021

References
[1] Mythreyi Bhargavan and Jonathan H Sunshine. Utilization of radiology services in the United States: Levels and
trends in modalities, regions, and populations. Radiology, 234(3):824–832, 2005.
[2] Farah E. Shamout, Yiqiu Shen, Nan Wu, Aakash Kaku, Jungkyu Park, Taro Makino, Stanisław Jastrz˛ebski,
Jan Witowski, Duo Wang, Ben Zhang, Siddhant Dogra, Meng Cao, Narges Razavian, David Kudlowitz, Lea
Azour, William Moore, Yvonne W. Lui, Yindalon Aphinyanaphongs, Carlos Fernandez-Granda, and Krzysztof J.
Geras. An artificial intelligence system for predicting the deterioration of COVID-19 patients in the emergency
department. arXiv preprint arXiv:2008.01774, 2020.
[3] Young Joon Kwon, Danielle Toussie, Mark Finkelstein, Mario A. Cedillo, Samuel Z. Maron, Sayan Manna,
Nicholas Voutsinas, Corey Eber, Adam Jacobi, Adam Bernheim, Yogesh Sean Gupta, Michael S. Chung, Zahi A.
Fayad, Benjamin Glicksberg, Eric K. Oermann, and Anthony B. Costa. Combining initial radiographs and clinical
variables improves deep learning prognostication of patients with COVID-19 from the emergency department.
Radiology: Artificial Intelligence, 2020.
[4] Kang Zhang, Xiaohong Liu, Jun Shen, Zhihuan Li, Ye Sang, Xingwang Wu, Yunfei Zha, Wenhua Liang, Chengdi
Wang, Ke Wang, Linsen Ye, Ming Gao, Zhongguo Zhou, Liang Li, Jin Wang, Zehong Yang, Huimin Cai, Jie Xu,
Lei Yang, Wenjia Cai, Wenqin Xu, Shaoxu Wu, Wei Zhang, Shanping Jiang, Lianghong Zheng, Xuan Zhang,
Li Wang, Liu Lu, Jiaming Li, Haiping Yin, Winston Wang, Oulan Li, Charlotte Zhang, Liang Liang, Tao Wu,
Ruiyun Deng, Kang Wei, Yong Zhou, Ting Chen, Johnson Yiu-Nam Lau, Manson Fok, Jianxing He, Tianxin Lin,
Weimin Li, and Guangyu Wang. Clinically applicable AI system for accurate diagnosis, quantitative measurements,
and prognosis of COVID-19 pneumonia using computed tomography. Cell, 181(6), 2020.
[5] Jie-Zhi Cheng, Dong Ni, Yi-Hong Chou, Jing Qin, Chui-Mei Tiu, Yeun-Chung Chang, Chiun-Sheng Huang,
Dinggang Shen, and Chung-Ming Chen. Computer-aided diagnosis with deep learning architecture: Applications
to breast lesions in US images and pulmonary nodules in CT scans. Scientific Reports, 6(1):1–13, 2016.
[6] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul,
Curtis Langlotz, Katie Shpanskaya, et al. CheXNet: Radiologist-level pneumonia detection on chest x-rays with
deep learning. arXiv preprint arXiv:1711.05225, 2017.
[7] Gabriel Chartrand, Phillip M Cheng, Eugene Vorontsov, Michal Drozdzal, Simon Turcotte, Christopher J Pal,
Samuel Kadoury, and An Tang. Deep learning: A primer for radiologists. Radiographics, 37(7):2113–2131, 2017.
[8] Paras Lakhani and Baskaran Sundaram. Deep learning at chest radiography: automated classification of pulmonary
tuberculosis by using convolutional neural networks. Radiology, 284(2):574–582, 2017.
[9] Rikiya Yamashita, Mizuho Nishio, Richard Kinh Gian Do, and Kaori Togashi. Convolutional neural networks: an
overview and application in radiology. Insights Into Imaging, 9(4):611–629, 2018.
[10] Pranav Rajpurkar, Jeremy Irvin, Robyn L. Ball, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy
Ding, Aarti Bagul, Curtis P. Langlotz, Bhavik N. Patel, Kristen W. Yeom, Katie Shpanskaya, Francis G. Blankenberg, Jayne Seekins, Timothy J. Amrhein, David A. Mong, Safwan S. Halabi, Evan J. Zucker, Andrew Y. Ng, and
Matthew P. Lungren. Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt
algorithm to practicing radiologists. PLoS Medicine, 15(11):1–17, 2018.
[11] Xiaocong Chen, Lina Yao, Tao Zhou, Jinming Dong, and Yu Zhang. Momentum contrastive learning for few-shot
COVID-19 diagnosis from chest CT images. arXiv preprint arXiv:2006.13276, 2020.
[12] Amine Amyar, Romain Modzelewski, Hua Li, and Su Ruan. Multi-task deep learning based CT imaging analysis
for COVID-19 pneumonia: Classification and segmentation. Computers in Biology and Medicine, 126:104037,
2020.
[13] Ran Zhang, Xin Tie, Zhihua Qi, Nicholas B. Bevins, Chengzhu Zhang, Dalton Griner, Thomas K. Song, Jeffrey D.
Nadig, Mark L. Schiebler, John W. Garrett, Ke Li, Scott B. Reeder, and Guang-Hong Chen. Diagnosis of
COVID-19 pneumonia using chest radiography: Value of artificial intelligence. Radiology, page 202944, 2020.
[14] Yujin Oh, Sangjoon Park, and Jong Chul Ye. Deep learning COVID-19 features on CXR using limited training
data sets. IEEE Transactions on Medical Imaging, 39(8):2688–2700, 2020.
[15] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised
learning of visual features by contrasting cluster assignments. arXiv preprint arXiv:2006.09882, 2020.
[16] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,
Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, Jayne Seekins, David A. Mong, Safwan S. Halabi, Jesse K.
Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz, Bhavik N. Patel, Matthew P. Lungren, and Andrew Y.
Ng. CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 33, pages 590–597, 2019.
15

A PREPRINT - JANUARY 26, 2021

[17] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. ChestX-ray8:
Hospital-scale chest X-Ray database and benchmarks on weakly-supervised classification and localization of
common thorax diseases. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 2097–2106, 2017.
[18] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying
Deng, Roger G Mark, and Steven Horng. MIMIC-CXR, a de-identified publicly available database of chest
radiographs with free-text reports. Scientific Data, 6:317, 2019.
[19] Hoo-Chang Shin, Holger R Roth, Mingchen Gao, Le Lu, Ziyue Xu, Isabella Nogues, Jianhua Yao, Daniel Mollura,
and Ronald M Summers. Deep convolutional neural networks for computer-aided detection: CNN architectures,
dataset characteristics and transfer learning. IEEE Transactions on Medical Imaging, 35(5):1285–1298, 2016.
[20] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping.
In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 2, pages
1735–1742, 2006.
[21] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.
[22] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 9729–9738, 2020.
[23] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive
learning. arXiv preprint arXiv:2003.04297, 2020.
[24] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng,
Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. MIMIC-CXR-JPG, a large publicly available
database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019.
[25] Ary L. Goldberger, Luis A. N. Amaral, Leon Glass, Jeffrey M. Hausdorff, Plamen Ch. Ivanov, Roger G. Mark,
Joseph E. Mietus, George B. Moody, Chung-Kang Peng, and H. Eugene Stanley. PhysioBank, PhysioToolkit, and
PhysioNet: Components of a new research resource for complex physiologic signals. Circulation, 101(23):e215–
e220, 2000.
[26] Hari Sowrirajan, Jingbo Yang, Andrew Y. Ng, and Pranav Rajpurkar. Moco pretraining improves representation
and transferability of chest x-ray models, 2020.
[27] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4700–4708,
2017.
[28] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.
arXiv preprint arXiv:1807.03748, 2018.
[29] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on
Learning Representations, 2015.
[30] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International
Conference on Learning Representations, 2017.
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages
5998–6008, 2017.
[32] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian
Vaughan, Pritam Damania, and Soumith Chintala. PyTorch distributed: Experiences on accelerating data parallel
training. arXiv preprint arXiv:2006.15704, 2020.
[33] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and
momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning, pages
1139–1147, 2013.
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level
performance on ImageNet classification. In Proceedings of the IEEE International Conference on Computer
Vision, pages 1026–1034, 2015.
[35] Elizabeth R. DeLong, David M. DeLong, and Daniel L. Clarke-Pearson. Comparing the areas under two or more
correlated receiver operating characteristic curves: A nonparametric approach. Biometrics, 44(3):837–845, 1988.
16

A PREPRINT - JANUARY 26, 2021

[36] X. Sun and W. Xu. Fast implementation of DeLong’s algorithm for comparing the areas under correlated receiver
operating characteristic curves. IEEE Signal Processing Letters, 21(11):1389–1393, 2014.
[37] Yiqiu Shen, Nan Wu, Jason Phang, Jungkyu Park, Kangning Liu, Sudarshini Tyagi, Laura Heacock, S Gene Kim,
Linda Moy, Kyunghyun Cho, et al. An interpretable classifier for high-resolution breast cancer screening images
utilizing weakly supervised localization. Medical Image Analysis, 68:101908, 2021.
[38] Laure Wynants, Ben Van Calster, Gary S Collins, Richard D Riley, Georg Heinze, Ewoud Schuit, Marc M J
Bonten, Darren L Dahly, Johanna A A Damen, Thomas P A Debray, Valentijn M T de Jong, Maarten De Vos,
Paula Dhiman, Maria C Haller, Michael O Harhay, Liesbet Henckaerts, Pauline Heus, Nina Kreuzberger, Anna
Lohmann, Kim Luijken, Jie Ma, Glen P Martin, Constanza L Andaur Navarro, Johannes B Reitsma, Jamie C
Sergeant, Chunhu Shi, Nicole Skoetz, Luc J M Smits, Kym I E Snell, Matthew Sperrin, René Spijker, Ewout W
Steyerberg, Toshihiko Takada, Ioanna Tzoulaki, Sander M J van Kuijk, Florien S van Royen, Jan Y Verbakel,
Christine Wallisch, Jack Wilkinson, Robert Wolff, Lotty Hooft, Karel G M Moons, and Maarten van Smeden.
Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal. British
Medical Journal, 369, 2020.

17

