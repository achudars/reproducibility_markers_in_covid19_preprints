Improving Baseline Subtraction for Increased Sensitivity of Quantitative
PCR Measurements
Paul N. Patrone,‚àó Anthony J. Kearsley, Erica L. Romsos, and Peter M. Vallone

arXiv:2004.05466v1 [q-bio.QM] 11 Apr 2020

National Institute of Standards and Technology
(Dated: April 14, 2020)
Motivated by the current COVID-19 health-crisis, we examine the task of baseline subtraction
for quantitative polymerase chain-reaction (qPCR) measurements. In particular, we present an
algorithm that leverages information obtained from non-template and/or DNA extraction-control
experiments to remove systematic bias from amplification curves. We recast this problem in terms of
mathematical optimization, i.e. by finding the amount of control signal that, when subtracted from
an amplification curve, minimizes background noise. We demonstrate that this approach can yield
a decade improvement in sensitivity relative to standard approaches, especially for data exhibiting
late-cycle amplification. Critically, this increased sensitivity and accuracy promises more effective
screening of viral DNA and a reduction in the rate of false-negatives in diagnostic settings.
Keywords: qPCR, DNA Detection, Background Subtraction, Measurement Sensitivity

Quantitative
polymerase
chain-reaction
(qPCR) measurements have had a long and
successful history as a diagnostic tool in the
medical community [1]. However, preliminary
evidence suggests that false-negatives are on the
order of 30% for detection of the SARS-CoV-2
virus [2]. Moreover, a recent study suggests
that these incorrect results are associated with
asymptomatic carriers and/or early stages of
infection, when viral loads may be low [3].
Thus, methods that can better characterize
and increase sensitivity of qPCR measurements
will be critical for both understanding and
controlling the current outbreak.
In this greater context, the task of baseline correction remains an overlooked problem
that limits the sensitivity of qPCR. Typical approaches rely on local, empirical fits of the amplification curves at low cycle numbers n ‚â§ 15,
i.e. before the onset of exponential growth. One
assumes that this local behavior can be extrapolated to high cycle numbers to model the global
background structure. However, it is well known
that empirically motivated extrapolation can introduce significant errors, especially when data
are fit with polynomials [4]. Moreover, it is unreasonable to assume that baseline corrections
so extracted will be valid for high-cycle numbers
(e.g. n ‚â• 35). In this region, exquisite accuracy
is needed to detect low initial template concentrations, and poor baseline subtraction will confound interpretation of measurements. Clearly,
more robust approaches based on global properties of the data are needed to realize the full
sensitivity of qPCR.
To address this problem, we present a new
baseline-subtraction strategy that avoids the use

‚àó

paul.patrone@nist.gov

of empirical models by directly leveraging the behavior of appropriate control experiments. The
main idea of our approach is to postulate that
the measurement signal is a linear combination
of fluorescence due to DNA reporters and an unknown amount of extraction-blank (EB) signal.
We directly estimate the contribution of the latter by determining the amount that, when subtracted from the measured signal, minimizes the
background noise. We show that this approach
yields up to a decade improvement in sensitivity
relative to approaches based on linear fits, especially for amplification curves that only show
growth at late cycles. Because such curves correspond to low initial DNA template numbers, we
speculate that our approach can be used to decrease the rate of false-negatives in qPCR-based
diagnostic tools.
A notable aspect of our approach is that it
makes few assumptions about the measurement
process, thereby increasing robustness. In particular, denote: (i) the measured fluorescence at
the nth cycle by fn ; (ii) an appropriately measured ‚Äúbaseline‚Äù signal (see below for more details) by bn ; and the ‚Äútrue‚Äù or corrected signal
corresponding to the number of DNA strands by
œÑn . Our main assumption can be stated mathematically as
fn = œÑn + abn + c

(1)

where a is an unknown coefficient expressing the
amount of baseline signal polluting the true one,
and c is an unknown constant offset. Physically,
we interpret the latter as arising from dark currents and/or electronics offsets in the photodetector setup [5]. We hypothesize that the contribution from bn corresponds to slight fluorescence
of buffer components or impurities carried over
from the DNA extraction process. Thus, the coefficient a corresponds to the unknown variabil-

2
ity with which these components are unintentionally added to each well.
To determine a and c, we first formulate an
objective
L(a, c) = (a ‚àí 1)2 +
"
+

1
‚àÜN

Nh
X
(fn ‚àí abn ‚àí c)2
‚àÜN ‚àí 1
n=N0
#2
Nh
X
fn ‚àí abn ‚àí c
(2)

n=N0

where  is a regularization parameter satisfying 0 <   1, the term (a ‚àí 1)2 penalizes
large deviations of the baseline correction from
magnitude of the measured control signal, and
‚àÜN = Nh ‚àí N0 . Here N0 and Nh are lower
and upper cycles for which œÑn is expected to be
zero; below we discuss an adaptive routine by
which these constants can be estimated. Next,
we minimize L(a, c) with respect to both variables, which determines them uniquely [6].1 This
optimization can be performed using a variety of
numerical solvers in commercial software packages, e.g. the fminunc function in Matlab [7].2
Finally, the corrected signal is estimated by rewriting Eq. (1) in terms of œÑn using the optimal
values of a and c.
Several comments are in order. The second
and third terms appearing on the right-hand side
(RHS) of Eq. (2) are the variance and squaredmean of œÑn across all cycles for which the fluorescence signal should be below the noise threshold. Thus, in our approach, optimizing Eq. (2)
ensures that a and c correspond to baseline subtractions that simultaneously suppress low-cycle
noise and the corresponding mean. While this
is analogous to standard approaches that use
a least-squares linear fit, the bn in Eq. (1) is
measured empirically as opposed to postulated.
Moreover, as bn can be determined independently for each realization of a qPCR measurement (i.e. by running controls), variations arising from different machines, operators, etc. are
inherently captured in the baseline subtraction
process.
We also emphasize that the measurements
generating bn should mimic as closely as possible

1
2

More specifically, the objective is a strictly convex
quadratic and thus possesses a unique minimizer.
Certain commercial equipment, instruments, or materials are identified in this paper in order to specify the
experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and
Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best
available for the purpose.

the conditions of a sample containing DNA. In
our examples below, we find that baseline subtraction using EBs only provides marginal improvement relative to using non-template controls (NTCs). However, in setups where there is
risk of contamination, EBs will provide a more
accurate representation of the baseline because
they are a product that represents the treatment
of each unknown sample more effectively than
NTCs, which are comprised of water or sterile
buffer. Thus, when possible, it is useful to run
both to better characterize sources of the baseline.
To illustrate the usefulness of Eqs. (1) and (2)
we performed a series of qPCR experiments using the Quantifiler Trio (Thermo Fisher) commercial qPCR chemistry.
Extraction blanks were created by extracting
six individual sterile cotton swabs (Puritain) using the Qiagen EZ1 Advanced XL and DNA Investigator kit (Qiagen). 290 ¬µL of G2 buffer and
10 ¬µL of Proteinase K were added to the tube
and incubated in a thermal mixer (Eppendorf) at
56 ‚ó¶ C for 15 minutes prior to being loaded onto
the purification robot. The ‚ÄúTrace Tip Dance‚Äù
protocol was run on the EZ1 Advanced XL with
elution of the DNA into 50 ¬µL of TE (Qiagen).
After elution, all EBs were pooled into one tube
for downstream analysis.
Human DNA Quantitation Standard (Standard Reference Material 2372a) [8] Component
A and Component B were each diluted 10fold. Component A was diluted by adding 10
¬µL of DNA to 90 ¬µL 10 mmol/L 2 amino 2
(hydroxymethyl) 1,3 propanediol hydrochloride
(Tris HCl) and 0.1 mmol/L ethylenediaminetetraacetic acid disodium salt (disodium EDTA)
using deionized water adjusted to pH 8.0 (TE‚àí4 ,
pH 8.0 buffer) from its certified concentration.
Component B was diluted by adding 8.65 ¬µL of
DNA to 91.35 ¬µL of TE‚àí4 . From the initial 10fold dilution, additional serial dilutions were performed down to 0.0024 pg into a regime to produce samples with high Cq values (> 35). The
concentration values targeted were determined
empirically though the analysis of previous dilution data (not shown) and ranged from approximately 100 pg to 0.0024 pg, with an expectation
that the 100 pg datapoint would fall between 28
and 29 Cq . These two components were chosen
due to Component A being male and Component
B being female. The qPCR chemistry used for
these experiments is a chemistry which employs
a Y-chromosomal target, which should amplify
for Component A (male), but not for Component B (female), thereby permitting additional
verification that the EBs are a reasonable proxy
for baseline behavior.

3

FIG. 1. Baseline subtraction according to four different approaches for the ABY fluorescence channel on Plate
2; see the supplemental information for corresponding plots for the other data. All plots show absolute value
of the normalized fluorescence, so that no data is omitted on the log scale. In each plot, the two amplification
curves exhibiting exponential growth around cycle 20 correspond to the Neat A and B components, which
were used to set a scale for subsequent dilutions. Top left: Raw data minus the mean values of cycles 3 to 15
computed individually for each curve. Note that the systematic effects, which resemble square-root behavior,
dominate the baseline at almost all cycles. Bottom left: Raw data minus a linear fit computed individually
for each curve from cycles 3 to 15. Top Right: Raw data corrected by optimizing of Eq. (2) using the mean
NTC for bn . Note that the systematic effects present on the left plots have been eliminated. Bottom Right:
Raw data corrected by using the EB in place of the NTCs for bn . Note that the systematic effects have been
eliminated.

For all qPCR reactions, Quantifiler Trio was
used. Each reaction consisted of 10 ¬µL qPCR
Reaction mix, 8 ¬µL Primer mix, and 2 ¬µL of
sample (i.e. DNA, NTC, or EB) setup in a 96well optical qPCR plate (Phoenix) and sealed
with optical adhesive film (VWR). After sealing
the plate, it was briefly centrifuged to eliminate
bubbles in the wells. qPCR was performed on an
Applied Biosystems 7500 HID instrument with
the following 2-step thermal cycling protocol: 95
‚ó¶
C for 2 min followed by 40 cycle of 95 ‚ó¶ C for 9
sec and 60 ‚ó¶ C for 30 sec. Data collection takes
place at the 60 ‚ó¶ C stage for 30 sec for each of the
cycles across all wells. Upon completion of every
run, data was analyzed in the HID Real Time
qPCR Analysis Software v1.2 (Thermo Fisher)
with a fluorescence threshold of 0.2. Data was
exported into an Excel file for further analysis.
Two qPCR plates of 96 samples were run.

Plate one consisted of dilutions of Component A
and B from 100 pg to 0.16 pg in triplicate, Neat
Component A and B solutions in triplicate, 21
EBs and 21 NTCs, where NTC is TE-4 buffer
without going through the extraction process.
Plate two consisted of dilutions of Component
A and B from 0.31 pg to 0.002 pg in replicates
of five, Neat Component A and B solutions in
singlet, seven EBs, and seven NTCs. From this
data, we were able to generate curves with Cq
values ranging from approximately 27 to 40.
For all custom data analyses using Eq. (2),
we set N0 = 3 (to avoid transient effects associated with the first few cycles) and determined Nh
adaptively. Specifically, we first performed an
initial optimization of Eq. (2) by setting Nh = 15
to obtain a rough estimate of the amplification
curves. We used these curves to estimate the cycle number Cq at which any given fluorescence

4

FIG. 2. Close-up of the bottom subplots of Fig. 1. In both sub-plots, the yellow dotted line is the mean value
of all amplifications curves whose final value is less than 0.1. The red dotted line is 2 standard deviations
away from the mean. Note that the extraction-blank baseline subtraction decreases the characteristic scales
of the background by up to a decade.

curve was closest to 0.1. For Cq ‚â§ 35, we set
Nh = Cq ‚àí 5, which should put any given amplification curve in the noise regime. For any
value of Cq > 35, we set Nh = 32. Then we
reran the analysis to generate our final, baselinesubtracted amplification curves. We used a value
of  = 0.001 for all calculations.3
Figure 1 shows a representative plot comparing four baseline subtraction strategies for the
ABY fluorescence channel on plate 2; see the
supplemental information (SI) for other datasets.
For the top-left plot we subtracted the average
of each curve from cycles 3 to 15; for the bottomleft, we subtracted a line fit to the same points.
The right plots use our new method, taking bn to
be the NTC (top) or the EB (bottom). Importantly, the two strategies that leverage average
behavior of the data (left) introduce significant
systematic effects with a characteristic scale of
between 0.05 and 0.1 in terms of normalized fluorescence. By contrast, baseline subtraction according to optimization of Eq. (2) (right) using
either the NTCs or EBs eliminates all of the systematic effects and decreases the noise threshold
by roughly a decade; see also Fig. 2.
Figure 3 shows the corresponding NTC and
EB measurements for this system. From the
figure, it is clear why the linear fit fails: neither the NTCs nor EBs can be modeled with
lines past about cycle 20. Moreover, a fit to the
low-cycle data clearly underestimates the level of
background. Thus, subtracting a linear fit will
underestimate the level of background, which ex-

3

Data and analysis scripts are available upon request.

FIG. 3. NTC (solid) and EB (solid with dots) curves
measured with the data shown in Fig. 1. Note that
controls exhibit the same general behavior, which
suggests their similar efficacy in removing baseline
effects in the right plots of Figs. 1.

plains the rise in data seen on the left plots of
Fig. 1. Moreover, the scale of the rise in the
NTCs and EBs is consistent with the level of
systematic effects seen in those approaches.
As a consistency check, we also applied our
method to baseline subtraction of the EB curves.
Were this to generate false amplification, it
would suggest lack of robustness in the method.
However, as Fig. 4 shows, background subtraction of EB (using the mean EB for bn ) does not
generate any significant systematic effects, with
all corrected datasets remaining within the noise.
While more detailed studies are needed to
characterize the improvements in absolute performance, the following conclusion is evident:
the sensitivity of qPCR measurements can be

5

FIG. 4. Baseline subtraction of the EBs using the
mean EB according to optimization of Eq. (2). Note
that the corrected curves remain below 0.01 and do
not exhibit any significant systematic effects.

dramatically improved, essentially today, through

[1] M. J. Espy, J. R. Uhl, L. M. Sloan, S. P. Buckwalter, M. F. Jones, E. A. Vetter, J. D. C. Yao,
N. L. Wengenack, J. E. Rosenblatt, F. R. Cockerill, and T. F. Smith, Clinical Microbiology Reviews 19, 165 (2006).
[2] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv,
Q. Tao, Z. Sun, and L. Xia, Radiology 0, 200642
(2020).
[3] Y. Liu, L.-M. Yan, L. Wan, T.-X. Xiang,
A. Le, J.-M. Liu, M. Peiris, L. L. M. Poon,
and W. Zhang, The Lancet Infectious Diseases
(2020), 10.1016/S1473-3099(20)30232-2.
[4] M. Powell, Approximation Theory and Methods
(Cambridge University Press, 1981).

our baseline-subtraction strategy leveraging NTC
/ EB signals; moreover, this can only improve
the quality of any qPCR-based testing. In the
long term, more formalized studies based on
rigorous uncertainty quantification and consistency checks for fidelity of corrected data will
be needed to realize the full potential of our approach; such work is currently on-going. This
will be important in developing a more complete
epidemiological picture of viral spread. In the
short term, however, we anticipate that this improved sensitivity will allow for lower detection
thresholds, thereby decreasing false-negatives in
testing.
Acknowledgements: The authors thank Dr.
Charles Romine for catalyzing a series of discussions that led to this work.
This work is a contribution of the National Institute of Standards and Technology and is not
subject to copyright in the United States.

[5] P. Patrone, A. Kearsley, J. Majikes, and J. A.
Liddle, Submitted.
[6] J. E. Dennis and R. B. Schnabel, Numerical Methods for Unconstrained Optimization and
Nonlinear Equations (Society for Industrial and
Applied Mathematics, 1996).
[7] MATLAB, version 8.1.0 (R2013a) (The MathWorks Inc., Natick, Massachusetts, 2013).
[8] E. Romsos, M. Kline, D. Duewer, B. Toman,
and N. Farkas, Certification of Standard Reference Material 2372a Human DNA Quantitation
Standard (Natl. Inst. Stand. Technol. Spec. Publ.
260-189, 2018).

