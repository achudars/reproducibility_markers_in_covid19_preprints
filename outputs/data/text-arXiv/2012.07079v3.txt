CHS-Net: A Deep learning approach for hierarchical
segmentation of COVID-19 infected CT images
Narinder Singh Punna,∗, Sonali Agarwala

arXiv:2012.07079v3 [eess.IV] 17 Jan 2021

a

Indian Institute of Information Technology Allahabad, Prayagraj, Uttar Pradesh, India

Abstract
The pandemic of novel severe acute respiratory syndrome coronavirus 2
(SARS-CoV-2) also known as COVID-19 has been spreading worldwide, causing rampant loss of lives. Medical imaging such as computed tomography
(CT), X-ray, etc., plays a significant role in diagnosing the patients by presenting the excellent details about the structure of the organs. However,
for any radiologist analyzing such scans is a tedious and time-consuming
task. The emerging deep learning technologies have displayed its strength
in analyzing such scans to aid in the faster diagnosis of the diseases and
viruses such as COVID-19. In the present article, an automated deep learning based model, COVID-19 hierarchical segmentation network (CHS-Net)
is proposed that functions as a semantic hierarchical segmenter to identify
the COVID-19 infected regions from lungs contour via CT medical imaging.
The CHS-Net is developed with the two cascaded residual attention inception U-Net (RAIU-Net) models where first generates lungs contour maps
and second generates COVID-19 infected regions. RAIU-Net comprises of
a residual inception U-Net model with spectral spatial and depth attention
network (SSD), consisting of contraction and expansion phases of depthwise
separable convolutions and hybrid pooling (max and spectral pooling) to
efficiently encode and decode the semantic and varying resolution information. The CHS-Net is trained with the segmentation loss function that is
the weighted average of binary cross entropy loss and dice loss to penalize false negative and false positive predictions. The approach is compared
with the recently proposed research works on the basis of standard metrics
∗

Corresponding author
Email addresses: pse2017002@iiita.ac.in (Narinder Singh Punn ),
sonali@iiita.ac.in (Sonali Agarwal)
Preprint submitted to arXiv

January 19, 2021

like accuracy, precision, sensitivity, recall, F1 score, dice coefficient and Jaccard index. With extensive trials, it is observed that the proposed approach
outperformed the recently proposed approaches and effectively segments the
COVID-19 infected regions in the lungs.
Keywords: COVID-19, Coronavirus, CT images, Deep learning,
Hierarchical segmentation.
1. Introduction
The novel coronavirus, also known as COVID-19, is an on-going worldwide pandemic that initiated from Wuhan, the People’s Republic of China
in December 2019 and till December 10, 2020, have caused 68,894,596 infections and 1,569,374 deaths worldwide [1]. The exponential growing trend
of COVID-19 is highlighted in Fig. 1 that shows the number of confirmed
cases out of which 2% died and 64% recovered worldwide since the time it
is recorded [2]. With the exponential spread of the virus, the World Health
Organization (WHO) declared the coronavirus outbreak as a public health
emergency of international concern (PHEIC) in January 2020 and later as
pandemic in March 2020 [3]. This has raised the concern in every sector of
the international community such as public health, transportation, marketing, tourism, manufacturing, lifestyle, etc. Even with various advancements
in technology, unfortunately till now there is no concrete solution or medicine
to cure COVID-19 and hence the international community is adopting the
avoidance and preventive measures that involve self hygiene, no social contact, avoiding finger touch on the public doors, elevators, etc. [4]. Since
COVID-19 is highly contagious, the infected ones are kept in isolation and
closely monitored by the doctors and experts for treatment to minimize its
spread. Moreover, the availability of resources of COVID-19 detection and
diagnosis is quite limited as compared to its requirement, hence researchers
are exploring all possible ways to detect and analyse the impact of infection on human body. With this motivation, biomedical image analysis have
become prominent area of research to aid in the diagnosis of COVID-19.

2

Figure 1: Exponential growth trend of COVID-19 worldwide.

In biomedical image analysis, the problems can be interpreted as classification and segmentation to identify and detect any abnormality in the
radiography [5] via deep learning techniques, where the convolution neural network (CNN) based architectures are the most promising and popular
choice in the research community. CNNs have displayed remarkable performance over the years and are being deployed for endoscopic videos [6], CT
images [7], diagnosis of pediatric pneumonia using chest X-ray images [8, 9],
etc. In the context of COVID-19, the classification task involves a prediction for the patient being infected with the virus in the presence of binary
or multi-class samples [8, 10, 11] (involving other viruses or diseases than
COVID-19), whereas in segmentation the coronavirus infected regions are
localized and in-painted [12, 13] via lungs CT or X-ray imaging.
1.1. Why lungs segmentation?
Currently, reverse transcription-polymerase chain reaction (RT-PCR) test
act as the standard to diagnose and confirm the symptoms of COVID-19 in
any patient [14]. However, the RT-PCR assay is deficit to fulfil the demand
in every area. The test takes around 4 to 6 hours and is less sensitive to
confirm the coronavirus at the initial stages. The current findings indicate
that COVID-19 affects various organs of a human being, such as blood vessels,
heart, stomach, intestines, brain and kidneys [15]. The virus enters into the
cells surface receptors angiotensin-converting enzyme 2 or ACE2 which is
present on alveoli of the lungs. Therefore, lungs become the primary target
for the virus affection which later spread to other body organs. Following
from this context, computed tomography (CT) imaging of the human lungs
3

Figure 2: Chest CT imaging of normal and COVID-19 infected lungs.

is considered to diagnose and test the COVID-19 infections. It has been
observed that bilateral, multifocal and peripheral ground glass opacification
(GGO) that follows typical patterns, are predominant CT findings in patients
suffering from COVID-19 [16] as highlighted in Fig. 2. However, for any
radiologist analyzing CT scan is a time consuming and tedious task. Thus,
biomedical image analysis techniques involving deep learning and machine
learning algorithms are developed for faster cures and treatment.

Figure 3: Schematic representation of base U-Net model.

1.2. U-Net
U-Net [17] model, as shown in Fig. 3, is the most extensively utilized
CNN based deep learning architecture for medical image segmentation. Due
to its symmetrical encoder-decoder framework divided into contraction and
4

Figure 4: Standard convolution operation vs depthwise separable convolution operation.

expansion path, the model can extract low and high-level features at the
varying hierarchy of resolutions, and reconstruct the output segmentation
map in the desired dimensions.
Following this, many variations of U-Net have been proposed [18, 19, 20,
21]. In most of the extended versions of U-Net, the feature maps produced
in contraction phase are preprocessed via attention gates (AG) [22], squeeze
and excitation block (SE) [23], spatial and channel SE blocks [24], etc., before concatenating with the corresponding expansion layer. It has also been
observed that to segment regions of varying shapes and size require different
sizes of receptive field [25]. Since, the COVID-19 infected regions may vary
in shape, size and location, the present article incorporates the proposed
inception block into the standard U-Net. Furthermore, to improve the computational power, the proposed approach also integrates depthwise separable
convolution (DSC) layers [26], divided into two stages: depthwise and pointwise convolutions. Fig. 4 draws the contrast between standard convolution
(SC) and DSC for some feature map, F ∈ Rw×h×d , where w is width, h is
height and d is depth of the input. It is observed that DSC reduces the

5

number of multiplications (M) and parameters (P) than the SC as given by
the Eq. 1. This significantly reduces the training time and computational
cost without affecting the performance of the model.
NDSC
1
1
= + 2
NSC
r f

(1)

where NDSC and NSC indicates number of parameters or multiplications in
depthwise separable convolution and standard convolution respectively, r is
the depth of the output volume and f indicates dimension of the kernel as
shown in Fig. 4.
1.3. Why hierarchical segmentation?
The present article addresses the challenging problem of efficiently identifying the COVID-19 infected regions in the CT images. Since these infected
regions are present inside the lungs, the information present outside the lungs
area becomes irrelevant. Therefore, in the proposed approach instead of direct segmentation, a hierarchical segmentation approach is introduced. In
hierarchical segmentation, two residual attention inception U-Net (RAIUNet) models are cascaded where the first model extracts lungs region from
the CT images to generate the lungs contour feature maps and the second
model utilizes these maps to segment the COVID-19 infected areas. Later,
with the extensive trials, it was observed that the hierarchical segmentation
approach significantly outperformed the direct segmentation scheme and also
advanced the recently proposed approaches for COVID-19 segmentation.
1.4. Challenges addressed
COVID-19 infection segmentation in CT images is a challenging task due
to the following concerns:
1. The presence of high variation in pattern, area and locale of infections
in CT slices makes it difficult to segment. For instance, small infected
regions can easily get neglected by the model which increases the falsenegative predictions. This challenge is addressed by proposing inception convolution blocks that follow depthwise separable convolutions
(DSC) of varying filter sizes (2 × 2, 3 × 3, 5 × 5) and a hybrid pooling
layer accompanied with batch normalization and rectified linear unit
activation.

6

2. Limited data availability of the COVID-19 infected patients, resulting
due to privacy and security concerns. This affects the training of deep
learning models. This challenge is addressed by the segmentation loss
function and fusion of publicly accessible multiple datasets of CT volumes consisting of coronavirus and non-coronavirus slices to generate
a large volume of data.
3. The intensity variance between the infected regions and background
(regions outside the lungs area) is small, this restricts the deep learning
models to identify the infected regions efficiently. This challenge is
tackled by proposing a hierarchical segmentation approach where the
irrelevant background is discarded before the identification of COVID19 infection by generating lungs contour maps; later these contours are
utilized to efficiently detect the infected regions.
4. The proposed CHS-Net model is a deep network, where the deep networks suffer from performance degradation due to the problem of vanishing and exploding gradient. To address this problem, each block
of RAIU-Net model is equipped with residual (skip) connections to
improve the flow of information in the network.
1.5. Our contribution
This article presents the following contribution in the COVID-19 infectious image segmentation research:
1. A novel deep learning hierarchical approach, CHS-Net, built using
RAIU-Net, is proposed for segregating the coronavirus infected areas
using CT scans by exploiting the potential strategies of the state-ofthe-art deep learning models.
2. A residual inception module is incorporated with a U-Net model to
efficiently decode the semantic and varying resolution information considering the features extracted at each layer of the contraction phase
to generate infection segmentation.
3. A hybrid of max pooling and spectral pooling is proposed for the efficient reduction in the spatial dimension of the feature maps with
minimal loss of information.
4. A skip connection based on spectral spatial and depth attention (SSD)
mechanism is proposed that uses global spectral pooling to infer the
inter-spatial and channel features correlations for the effective flow of
feature maps between the contraction and expansion phases.
7

5. A fusion dataset is introduced with 3560 CT slices, developed using
COVID-19 CT segmentation nr.2 dataset [27] and COVID-19 CT lung
and infection segmentation dataset [28]. Each CT slice has the corresponding lungs mask and COVID-19 infection mask. The code and
dataset will be released at the github repository1 .
1.6. Article organization
The rest of the paper is presented in various sections involving a literature survey in the related work section which highlights the recent findings
and approaches for COVID-19 detection via CT medical imaging. The later
section discusses the proposed approach to effectively identify and segregate
the COVID-19 infected regions in the lungs. Furthermore, the experimental
and results section describes the obtained results along with the exhaustive
experimental trials and comparative analysis, dataset description and ablation study. The final section highlights the concluding remarks and further
possible extensions of the work.
2. Related work
With rapid advancements in technology, many artificial intelligence driven
solutions are being developed to fight against COVID-19 pandemic [29]. In
recent studies [30, 31, 32], CT abnormalities corresponding to COVID-19
are being utilized by practitioners and doctors. It is observed that CT scan
highlights discrete patterns to identify the infected patients even at the initial
stages, making automatic CT medical imaging analysis a promising area of
research among the research community [32]. It is also observed that CT
diagnosis for COVID-19 abnormality detection can be carried before the
appearance of clinical symptoms [16]. Hence, many research works have been
proposed for automatic early detection with classification and segmentation
of the COVID-19 infection from CT scans [33, 34].
Li et al. [35] proposed a fully automatic CNN based COVID-19 detection
neural network (COVNet), to classify COVID-19 abnormalities from community acquired pneumonia (CAP) and normal cases using chest CT imaging.
The authors achieved 96% area under the receiver operating characteristic
curve (AUC-ROC) to identify COVID-19 cases and performed better than
1

https://git.io/Jtec9

8

RT-PCR testing. Even after achieving good results, the authors claimed
that the proposed approach is not sufficient to segregate and classify different
types of pneumonia due to the limited data availability. Butt et al. [36] proposed deep learning based COVID-19 screening system to distinguish corona
infected samples from non-corona samples. The proposed system yielded
faster detection rate than RT-PCR testing with an overall accuracy score of
86.7%. Shan et al. [13] proposed V-Net [37] based deep learning model to
segment and quantify the COVID-19 infected regions. The authors achieved
a dice similarity index of 91.6% ± 10.0% between manual and deep learning
enabled automatic delineation. However, these approaches do not provide
localized information about the infected regions in the CT scan of the lungs.
A multistage deep learning framework is proposed by Gozes et al. [38]
that follows segmentation to remove the irrelevant regions and classification
of segmented regions into coronavirus infected and other viral pneumonia.
For segmentation, the U-Net [17] model is utilized to acquire the relevant
regions and then a pretrained ResNet-50 [39] model is fine-tuned to classify
COVID-19 infected samples. Yan et al. [40] proposed COVID-SegNet accompanied with feature variation block and progressive atrous convolutions
to highlight the diverse infected regions along with the boundaries. The proposed approach achieved a dice score of 0.726 for COVID-19 segmentation.
Furthermore, Hu et al. [41] developed an object detection based approach
to highlight the infected region with the help of the bounding boxes. The
authors followed a weakly supervised approach to improve model performance with a limited number of labelled COVID-19 samples. The authors
employed VGG model variants to classify COVID-19 from CAP and nonpneumonia cases. In similar work, Fan et al. [12] proposed lung infection
segmentation deep network (Inf-Net) to segment COVID-19 infected regions
with ground glass opacities (GGO) and consolidation while also addressing
the challenges of high variation characteristics and low intensities of the abnormalities, and limited availability of the infected samples. With extensive
trials, Inf-Net outperformed the recently proposed approaches. Following
from these notions the present article contributes towards further improvement in the COVID-19 segmentation performance by introducing a hierarchical segmentation approach that generates lungs contour maps to efficiently
segment the coronavirus infected regions.

9

Figure 5: Schematic representation of CHS-Net (a) generates lungs contour maps, (b)
generates infected regions.

3. Proposed network
The proposed COVID-19 hierarchical segmentation network (CHS-Net),
as shown in Fig. 5, is inspired from the state-of-the-art deep learning architectures: U-Net [17], Google’s inception model [42], residual network [39] and
attention strategy [22]. For hierarchical segmentation, two RAIU-Net models are connected in a series where the first model generates lungs contour
maps Fig. 5(a) and then the second model utilizes these maps to identify
the infected regions Fig. 5(b). The depth of the model is divided into four
stages where each stage extract feature maps at different spatial dimensions.
In contraction phase each stage reduces the width and height by 50%, and
increase depth by 50% and vice-versa in the expansion phase.
Fig. 6 presents the schematic representation of the building block of
RAIU-Net for some input feature map, F ∈ Rw×h×d . RAIU-Net model is
developed in a U-Net fashion where each 2D convolution is replaced with
inception blocks (concatenated 1×1, 3×3 and 5×5 DSCs, and hybrid pooling followed by batch normalization to reduce the internal covariance shift
and rectified linear unit as activation) while following the residual learning
approach. The residual function reformulates the layer as learning in correspondence to the layer input. The extracted features from each residual
inception block (RIB) are concatenated with corresponding transposed con10

Figure 6: Building block of RAIU-Net.

volutions [43] representing similar spatial dimensions in the expansion phase,
using skip connections [44]. Instead of direct concatenation, these skip connections are equipped with spectral spatial and depth attention (SSD) network to process the extracted feature maps and preserve the most relevant
high or low-level feature maps. The dimension inconsistency of the concatenating layers is removed using strided convolution to reduce spatial dimensions and 1 × 1 convolution to reduce the depth, as shown in Fig. 6. Finally,
the sigmoid activated 1 × 1 convolution outputs the lungs contour map along
with segmented regions of COVID-19 abnormalities.
3.1. Residual inception block
Fig. 7 presents the schematic representation of the RIB for some input
feature map, Fi ∈ Rw×h×d . It is developed using double inception convolution
with a shortcut connection from the input to the output layer. The shortcut
or residual connection follows 3 × 3 DSC whose batch normalized output is
merged with the output of the double inception convolution (IC1 and IC2 )
to extract feature maps using d0 number of filters. The consecutive RIBs are
connected with the help of valid hybrid pooling which reduces the dimensions
of feature maps while preserving the prominent features to produce feature
0
map, Fo ∈ Rw/2×h/2×d .
11

Figure 7: Schematic representation of the residual inception block.

3.1.1. Hybrid pooling
Many pooling variants have been proposed based on the value, rank,
probability and domain transformation [45], among which spectral pooling
is found to preserve more spatial information while also reducing the dimensions. Max pooling is featured in every deep learning architecture, however,
it only preserves the sharpest features of an image. Whereas the spectral
pooling is one of the domain transformation based variants that tend to preserve more information and also reduces the dimensions. Fig. 8 presents the
comparison of max pooling and spectral pooling for varying filter sizes as
2 × 2, 8 × 8, 16 × 16 and 64 × 64 on randomly selected CT slice.

Figure 8: Downsampling using max and spectral pooling with the factors (a) 2×2, (b)
8×8, (c) 16×16 and (d) 64×64.

12

In spectral pooling 2D discrete Fourier transform (DFT) of the input
image, Iw,h,d , is computed as shown in Eq. 2 (concatenated over depth d),
providing the frequency maps shifted to the center DC component to truncate
the high frequency [46]. Finally, the inverse DFT is computed to map the
filtered frequency back into the spatial domain, where inverse DFT can be
computed as a conjugate of the DFT as DF T −1 (·) = DF T (·)∗ .
w−1 h−1

DF T (Iw,h,d )mn =

[
d

jm
kn
1 XX
√
Ijkd e−2πi( w + h )
wh j=0 k=0

(2)

∀m ∈ {0, . . . , w − 1}, ∀n ∈ {0, . . . , h − 1}
S

where d indicates concatenation of DF T of each feature map along the
channel axis.
In the present article, a hybrid pooling (Ph ) approach is proposed to leverage the features of both spectral and max pooling. In this, an input image
undergoes spectral and max pooling in parallel followed by 1 × 1 convolution.
This can be performed to generate output with either valid or same padding.
3.1.2. Inception convolution
It is observed that each feature extraction and reconstruction phase requires different filter sizes to recognize regions or objects of varying dimensions, locale and area. The inception convolution comprises parallel DSCs
with filter sizes 1×1, 3×3 and 5×5, and hybrid pooling accompanied with
batch normalization (BN) (faster convergence and reduce covariance shift)
and rectified linear unit (ReLU) activation function (adds non-linearity) to
extract multi-level features for the same instance. The extracted feature
maps are then concatenated using 1×1 convolution to optimize the cross
channel correlation without considering or modifying the spatial dimensions,
followed by BN and ReLU. Fig. 9 describes the schematic representation of
the inception convolution that acts as a basis of the RIB.
For an input feature map, Fi ∈ Rw×h×d , the inception convolution operation can be represented as in Eq. 3.




IC (Fi ) = K1×r ∗ 


[

(Kf ×r ∗ Fi )p,q,r 

f ∈(1,3,5)


[

Ph (Fi )
p,q,r

13

(3)

Figure 9: Design of inception convolution (IC).





(Kf ×r ∗ Fi )p,q,r = max BN DSC (K, Fi )p,q,r
DSC(K, F)p,q,r =

r
X

Ki .F(p,q,i)

i

w,h
[X
d




,0

(4)
!

K(j,k,d)

F(j+p,k+q,d)

(5)

j,k

S
where
indicates concatenation of feature maps along channel axis and
indicates element-wise product. Kf ×r represents a kernel or a filter with
dimensions (f × f × d × r) (r indicates number of filters and d indicates
same depth of the kernel as input), (Kf ×r ∗ I)p,q,r (shown in Eq. 6) indicates
a transformed image with features mapping from dimension as w×h×d 7→ p×
q × r by utilizing the DSC that follows from pointwise convolution operation
of the depthwise convolved feature maps. The output dimension, Do (p, q, r)
can be computed as shown in Eq. 7.
(Kf ×r ∗ I)p,q,r =

w X
h
[X
r

14

i=1 j=1

Ki,j Ip+i,q+i

(6)


Do (p, q, r) =

 
 
h + 2p − f
w + 2p − f
+1 ,
+ 1 ,r ;s > 0
s
s

(7)

where s and p denotes the amount of strides and padding respectively.
3.2. Spectral spatial and depth attention
The attention map aids the network to selectively process the information
instead of complete volume by utilizing the inter spatial and channel features
correlations. To the best of our knowledge, so far the attention map is
obtained by applying either global average pooling or global max pooling, or
both [47, 48]. However, it is evident that these pooling operations tend to
be biased towards extreme features (pixels with high intensity). Therefore,
a global spectral pooling layer is employed into the attention mechanism
to extract most prominent features and generate the attention descriptors,
(spatial) and A1×1×d
Aw×h×1
(depth), that infers distinctive object features
s
d
for an input volume Fi . Finally, these attention descriptors undergo elementwise multiplication with the Fi followed by ReLU activated 1 × 1 convolution
and BN, to produce refined volumes.
3.2.1. Spectral depth attention
The operation of spectral depth attention approach is described in Fig. 10.
The input feature maps, Fi ∈ Rm×n×p undergoes global spectral pooling
(Pgsp ) to generate Fgsp ∈ R1×1×p . The resulting flattened features pass
through the shallow convolution neural network (SCNN). The network comprises of two blocks of 1 × 1 ReLU activated convolutions to generate depth
attention descriptor Ad ∈ R1×1×p as shown in Eq. 8. The attention descriptor, Ad then undergoes element-wise multiplication with Fi , followed by
batch normalization and ReLU activation to produce spectral depth attention volume Fd ∈ Rm×n×p as shown in Eq. 9.

Ad = K1×p ∗ K1×p/2 ∗ Pgsp (Fi ) 1,1,p
Fd = (K1×p ∗ (Ad

15

Fi ))m,n,p

(8)
(9)

Figure 10: Representaiton of spectral depth attention network architecture.

3.2.2. Spectral spatial attention
The overall approach is illustrated in Fig. 11. For some feature map at
layer l, Fil ∈ Rm×n×p , the spectral spatial attention takes the input as Fil and
spectral depth attention of the previous layer feature map, Fdl−1 (Fil−1 ), where
Fil−1 ∈ R2m×2n×p/2 . The operation starts with the two-strided convolution of
Fil−1 such that it downsamples to F 0 ∈ Rm×n×p which is then merged with
the 1 × 1 convolution of Fil following the BN and ReLU activation function
indicated as γ in Eq. 10. Later, spatial attention descriptor, As ∈ R2m×2n×1
is generated by upsampling the ReLU activated 1 × 1 convolution of the
γ as shown in Eq. 11. Finally, similar to the spectral depth attention, As
is element-wise multiplied with Fdl−1 accompanied with 1 × 1 convolution
and batch normalization to form spectral spatial attention volume, Fs ∈
R2m×2n×p/2 as shown in Eq. 12.

Figure 11: Representaiton of spectral spatial attention network architecture.

16



γ = K2×p ∗ Fdl−1 + K1×p ∗ Fil

(10)

As = (U p (σ (K1×p ∗ γ)))2m,2n,1

(11)

Fs = K1×p/2 ∗ As

Fdl−1


2m,2n,p/2

(12)

3.3. Objective function
The CHS-Net is trained with the segmentation loss function L defined
as the weighted average of binary cross-entropy loss (BCL) and dice loss
(DL). The segmentation task can be treated as a binary classification task
to classify each pixel either belonging to the background (negative) or the
region of interest (positive).
The binary cross entropy as defined in Eq. 13 is the most widely used
loss function for binary classification and works effectively if there are equal
distributions of positive and negative samples.

BCL (y, p (y)) = −

N
X

(yi .log (p (yi )) + (1 − yi ) .log (1 − p (yi )))

(13)

i

where N indicates total number of pixels in an image I, yi and p(yi ) presents
the ground truth value and predicted value of ith pixel respectively.
However, due to fewer infectious pixels in CT images, standalone binary
cross entropy is not sufficient. Therefore to better penalize the false positive
and negative predictions, dice loss is also utilized, defined in Eq. 14. The
DL tends to equally penalize the false negative (FN) and false positive (FP)
predictions.
P
2 N
i yi .p(yi )
(14)
DL (y, p (y)) = 1 − PN
2 PN
2
i |yi | +
i |p(y i )|
The overall loss function L is represented by Eq. 15.
1
1
L (I, p (I)) = BCL (y, p (y)) + DL (y, p (y))
2
2

17

(15)

Table 1: CT slices distribution details in the synthesized fused dataset.
Fused dataset
Dataset

No. of CT
Volumes

Dimension

No. of
CT slices

COVID-19 CT
9
630x630xd*
segmentation nr. 2 [27]
3560
COVID-19 CT lung and
630x630xd*
20
infection segmentation [28]
512x512xd*
*d indicates that the number of slices varies for each volume

No. of
Lungs mask

No. of
COVID-19
infection mask

3560

2200

4. Experimentation and results
4.1. Dataset description
The CHS-Net is trained and evaluated on the synthesized dataset generated using publicly available COVID-19 CT segmentation datasets [27, 28].
This dataset source [27] is the only open-access data repository that has
COVID-19 CT segmentation datasets because of government norms and personal privacy. These two datasets are merged to form an aggregated dataset
that addresses the problem of limited availability of the COVID-19 data. The
fused dataset consists of 3560 CT slices with dimensions as 256 × 256 × 1,
each having associated lungs mask and COVID-19 infection mask. Table 1
highlights the class summary details of the fused dataset. The CT slices
with just black or dark pixels were irrelevant and hence filtered out. In the
fused dataset, for each CT slice, there are corresponding lungs mask, but not
COVID-19 mask due to the absence of infection and hence are represented
with set of dark pixels of dimension 256 × 256 × 1.

Figure 12: CT slices and corresponding ground truth masks from the fused dataset.

18

These 2D CT slices are extracted from the 29 3D volumes of the CT
imaging having non-uniform or varying dimensions, resized to 256 × 256 × 1.
Each of these slices is annotated carefully by expert radiologists to generate
the segmentation mask. Fig. 12 shows the sample slices along with the ground
truth segmentation mask corresponding to the lungs and COVID-19 infected
region. Each pixel of the slices is marked with class labels as 1 or 0 where 1
means the pixel belongs to the region of interest that is associated with lungs
in lungs annotation and COVID-19 (GGO and consolidations) in infection
annotation, and 0 means the background.

Figure 13: Confusion matrix and evaluation metrices.

4.2. Training and testing
The training and testing sets are acquired from the fused dataset to train
and evaluate the proposed CHS-Net model which comprises 70% and 30% of
the total images respectively. Furthermore, in each set the distribution of the
samples is kept 1:1 from both the datasets [27, 28]. Besides, other test sets
are also generated as 30% of random samples from individual datasets. The
training phase of the CHS-Net is assisted with stochastic gradient descent
(SGD) as training weights optimizer to minimize the segmentation loss function (objective function), Adam as a learning rate optimizer [49], 5-fold cross
validation for robustness, and early stopping to avoid the overfitting problem [50] techniques on the high performance computing environment with
Nvidia RTX Titan GPUs. The trained model is evaluated with the help of
the test sets in terms of accuracy, precision, recall, specificity, F1 score (dice
coefficient) and Jaccard index (intersection over union). Fig. 13 describes the
confusion matrix that can be generated for the predicted mask corresponding
to the CT slice to compute the above discussed metrics based on the correct

19

Figure 14: Qualitative results of COVID-19 infection segmentation on test set obtained
from fused dataset using CHS-Net. The quantities indicate the dice score and Jaccard
index values respectively, for each generated mask.

prediction: true positive (TP), true negative (TN) and incorrect prediction:
false positive (FP), false negative (FN).
4.3. Results and discussion
The proposed model generates the semantic segmentation mask of the
COVID-19 infected regions in the lungs based on the generated lungs contour maps. Fig. 14 presents the qualitative results of CHS-Net over randomly
chosen CT axial slices. This visual representation and the obtained evaluation metrics scores indicate that the results match the ground truth masks
and hence effectively detects and localize the coronavirus infected regions.
Table 2 discusses the results of the proposed model (CHS-Net) in terms of
accuracy (Acc.), precision (Pre.), specificity (Spe.), recall (Rec.), dice coefficient (DSC), and Jaccard index (JI) for hierarchical semantic segmentation
of COVID-19 infectious images following from the lungs segmentation.
Furthermore, the obtained results are compared with other state-of-theart segmentation models, as shown in Table 3. It is observed that CHS-Net
approach outperformed the other approaches with significant improvement
in the evaluation metrics values, especially in the dice coefficient along with
the least number of training parameters. However, among these evaluation
20

Table 2: Quantitative results of the CHS-Net on the test set generated from fused dataset.

Segmentation
Lungs
COVID-19

Acc.
0.99
0.96

Pre.
0.97
0.75

Spe. Rec.
0.99 0.95
0.97 0.88

DC
0.96
0.81

JI
0.98
0.89

Table 3: Comparative analysis of the CHS-Net with recently proposed a pproaches on the
fused dataset.

Model

Param.
2

SegNet [52]+VGG16
29.4M
U-Net [17]+VGG162
21.7M
U-Net++ [21]+ResNet503
37.6M
4
AU-Net [22]+VGG16
22M
Inf-Net [12]+Res2Net5
33M
1
RAIU-Net (Ours)
4.2M
CHS-Net+RAIU-Net1 (Ours) 8.4M
*bold quantities indicate highest scores

Infection segmentation
Acc. Pre. Spe. Rec. DC
0.94 0.35 0.79 0.46 0.37
0.94 0.38 0.84 0.52 0.44
0.95 0.70 0.90 0.75 0.72
0.95 0.67 0.98 0.76 0.71
0.95 0.74 0.98 0.76 0.75
0.95 0.71 0.96 0.80 0.75
0.96 0.75 0.97 0.88 0.81

JI
0.51
0.59
0.77
0.78
0.81
0.80
0.89

metrics, precision is obtained with the least value, indicating that model
generates approximately 25% of false positive predictions from the pool of
test set, which is comparatively less than other approaches but leaves the
void for further improvements. Furthermore, the models such as attention
U-Net [22] and Inf-Net [12] achieved the highest specificity score of 98% which
however is similar to the CHS-Net model.
In contrast, the success of the CHS-Net follows from its hierarchical segmentation strategy executed via proposed cascaded RAIU-Net model, where
instead of directly segmenting the COVID-19 infected regions, lungs contour maps are generated from the predicted lungs mask which then serve
as the input to another RAIU-Net model for localizing the infected regions.
Moreover, in CHS-Net, spectral representations (hybrid pooling and global
spectral pooling) are employed that aids in efficiently downsampling the feature maps with least loss of information. Furthermore residual inception
2

https://github.com/lsh1994/keras-segmentation
https://github.com/MrGiovanni/UNetPlusPlus/tree/master/keras
4
https://github.com/ozan-oktay/Attention-Gated-Networks
5
https://github.com/DengPingFan/Inf-Net
3

21

Table 4: Effects of the proposed components on the model performance for direct segmentation of COVID-19 infectious regions.

Mode
BU
BU+RIB
BU+RIB+HP
BU+RIB+HP+SSD (RAIU-Net)

Acc.
0.68
0.75
0.88
0.95

Infection segmentation
Pre. Spe. Rec. DC
0.04 0.80 0.28 0.07
0.55 0.90 0.35 0.43
0.63 0.95 0.68 0.65
0.71 0.96 0.80 0.75

JI
0.25
0.50
0.68
0.80

Table 5: Effects of the proposed components on the model performance for hierarchical
segmentation of COVID-19 infectious regions.

Mode
BU
BU+RIB
BU+RIB+HP
BU+RIB+HP+SSD
(CHS-Net)

Segmentation
Lungs
Infection
Lungs
Infection
Lungs
Infection
Lungs
Infection

Acc.
0.92
0.75
0.98
0.88
0.98
0.95
0.99
0.96

Pre.
0.73
0.21
0.85
0.68
0.91
0.73
0.97
0.75

Spe.
0.95
0.80
0.98
0.94
0.99
0.95
0.99
0.97

Rec.
0.78
0.32
0.88
0.53
0.92
0.82
0.95
0.88

DC
0.75
0.25
0.86
0.60
0.91
0.78
0.96
0.81

JI
0.77
0.28
0.90
0.64
0.94
0.85
0.98
0.89

blocks tends to efficiently encode and decode the semantic and varying resolution information. In addition, the adopted SSD mechanism refines the high
and low extracted feature maps that are later merged in the reconstruction
phase.
Furthermore, the significance of each component (RIB, HP and SSD) utilized in the proposed framework is presented in Table 4 and Table 5 with the
help of the evaluation metrics to highlight their impact on the segmentation performance using the test set. Table 4 illustrates the ablation study
for direct COVID-19 infected region segmentation whereas Table 5 shows it
for hierarchical segmentation (CHS-Net). This ablation study is carried under the same environment and comprises of baseline U-Net model (BU) [17]
that follows standard convolutions and max pooling operations. Later, this
model is extended with the proposed components as shown in Table 4 and
Table 5 to highlight the significance of each component for achieveing the
concerned results. As observed in both the tables, even for baseline model
the specificity metric value is significantly higher as compared to other met22

rics, indicating that model is predicting all pixels as background (dark) and
hence not able to detect any infection, whereas with continuous incorporation
of the proposed components the model tends to perform better in localizing
the complex patterns associated with the infection. In addition, it is also
observed that hierarchical segmentation approach works significantly better
than direct segmentation, where each component certainly contributes for
improvement in the model performance.
5. Conclusion
This article proposes a COVID-19 hierarchical segmentation network,
CHS-Net, to identify the COVID-19 infected regions from the generated lungs
contour maps with computed tomography (CT) images via cascaded residual attention inception U-Net (RAIU-Net) models. The RAIU-Net model
improves upon the base U-Net model by exploiting various state-of-the-art
components to improve the feature extraction and reconstruction process,
where a residual inception block (RIB) and spectral spatial and depth attention (SSD) network tends to effectively encode and decode the feature maps
at varying resolutions, while also addressing the major challenges involved in
the segmentation. With extensive trials it was observed that the results of the
CHS-Net model outperformed the other recently proposed approaches which
were evaluated using standard benchmark performance metrics i.e. accuracy
(96%), precision (75%), recall (88%), specificity (97%), F1 score (dice similarity coefficient) (81%) and Jaccard index (intersection over union) (89%).
The ablation study of the CHS-Net highlighted the contribution of each component towards the improvement in the segmentation results. Besides, trials
can be made to tweak and tune the architecture with other deep learning
components to further improve the results. It is believed that the potential
of CHS-Net design can also be extended to other applications concerning
biomedical image segmentation.
Acknowledgment
We thank our institute, Indian Institute of Information Technology Allahabad (IIITA), India and Big Data Analytics (BDA) lab for allocating the
centralised computing facility and other necessary resources to perform this
research. We extend our thanks to our colleagues for their valuable guidance
and suggestions.
23

References
[1] WHO, Who timeline - covid-19, https://www.who.int/news-room/
detail/08-04-2020-who-timeline---covid-19, [Online; accessed
December 10, 2020] (2020).
[2] G. repository, 2019 novel coronavirus covid-19 (2019-ncov) data repository by johns hopkins csse, https://github.com/CSSEGISandData/
COVID-19, [Online; accessed December 10, 2020] (2020).
[3] WHO, Coronavirus disease 2019 (covid-19) situation report –
81,
https://www.who.int/docs/default-source/coronaviruse/
situation-reports/, [Online; accessed December 10, 2020] (2020).
[4] N. S. Punn, S. K. Sonbhadra, S. Agarwal, Monitoring covid-19 social
distancing with person detection and tracking via fine-tuned yolo v3 and
deepsort techniques, arXiv preprint arXiv:2005.01385 (2020).
[5] V. Rajinikanth, N. Dey, A. N. J. Raj, A. E. Hassanien, K. Santosh,
N. Raja, Harmony-search and otsu based system for coronavirus disease (covid-19) detection using lung ct scan images, arXiv preprint
arXiv:2004.03431 (2020).
[6] P. Gómez, M. Semmler, A. Schützenberger, C. Bohr, M. Döllinger, Lowlight image enhancement of high-speed endoscopic videos using a convolutional neural network, Medical & biological engineering & computing
57 (7) (2019) 1451–1463.
[7] J. Choe, S. M. Lee, K.-H. Do, G. Lee, J.-G. Lee, S. M. Lee, J. B.
Seo, Deep learning–based image conversion of ct reconstruction kernels
improves radiomics reproducibility for pulmonary nodules or masses,
Radiology 292 (2) (2019) 365–373.
[8] N. S. Punn, S. Agarwal, Automated diagnosis of covid-19 with limited posteroanterior chest x-ray images using fine-tuned deep neural
networks, arXiv preprint arXiv:2004.11676 (2020).
[9] D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim, H. Liang, S. L.
Baxter, A. McKeown, G. Yang, X. Wu, F. Yan, et al., Identifying medical diagnoses and treatable diseases by image-based deep learning, Cell
172 (5) (2018) 1122–1131.
24

[10] K. Li, Y. Fang, W. Li, C. Pan, P. Qin, Y. Zhong, X. Liu, M. Huang,
Y. Liao, S. Li, Ct image visual quantitative evaluation and clinical classification of coronavirus disease (covid-19), European radiology (2020)
1–10.
[11] D. Singh, V. Kumar, M. Kaur, Classification of covid-19 patients from
chest ct images using multi-objective differential evolution–based convolutional neural networks, European Journal of Clinical Microbiology
& Infectious Diseases (2020) 1–11.
[12] D.-P. Fan, T. Zhou, G.-P. Ji, Y. Zhou, G. Chen, H. Fu, J. Shen, L. Shao,
Inf-net: Automatic covid-19 lung infection segmentation from ct images,
IEEE Transactions on Medical Imaging (2020).
[13] F. Shan, Y. Gao, J. Wang, W. Shi, N. Shi, M. Han, Z. Xue, Y. Shi,
Lung infection quantification of covid-19 in ct images with deep learning,
arXiv preprint arXiv:2003.04655 (2020).
[14] T. Ai, Z. Yang, H. Hou, C. Zhan, C. Chen, W. Lv, Q. Tao, Z. Sun, L. Xia,
Correlation of chest ct and rt-pcr testing in coronavirus disease 2019
(covid-19) in china: a report of 1014 cases, Radiology (2020) 200642.
[15] M. Wadman, J. Couzin-Frankel, J. Kaiser, C. Matacic, How does coronavirus kill, Clinicians trace a ferocious rampage through the body, from
brain to toes (2020) 1502–1503.
[16] S. K. Jon-Emile,
An illustrated guide to the chest
ct
in
covid-19,
https://pulmccm.org/uncategorized/
an-illustrated-guide-to-the-chest-ct-in-covid-19/, [Online;
accessed June 12, 2020] (2020).
[17] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for
biomedical image segmentation, in: International Conference on Medical
image computing and computer-assisted intervention, Springer, 2015,
pp. 234–241.
[18] N. S. Punn, S. Agarwal, Inception u-net architecture for semantic segmentation to identify nuclei in microscopy cell images, ACM Transactions on Multimedia Computing, Communications, and Applications
(TOMM) 16 (1) (2020) 1–15.
25

[19] M. Z. Alom, M. Hasan, C. Yakopcic, T. M. Taha, V. K. Asari, Recurrent residual convolutional neural network based on u-net (r2u-net) for
medical image segmentation, arXiv preprint arXiv:1802.06955 (2018).
[20] N. S. Punn, S. Agarwal, Multi-modality encoded fusion with 3d inception u-net and decoder model for brain tumor segmentation, Multimedia
Tools and Applications (2020) 1–16.
[21] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, J. Liang, Unet++: A nested
u-net architecture for medical image segmentation, in: Deep Learning in
Medical Image Analysis and Multimodal Learning for Clinical Decision
Support, Springer, 2018, pp. 3–11.
[22] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, et al., Attention u-net: Learning where to look for the pancreas, arXiv preprint
arXiv:1804.03999 (2018).
[23] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, in: Proceedings of the IEEE conference on computer vision and pattern recognition,
2018, pp. 7132–7141.
[24] A. G. Roy, N. Navab, C. Wachinger, Concurrent spatial and channel
‘squeeze & excitation’in fully convolutional networks, in: International
conference on medical image computing and computer-assisted intervention, Springer, 2018, pp. 421–429.
[25] W. Luo, Y. Li, R. Urtasun, R. Zemel, Understanding the effective receptive field in deep convolutional neural networks, in: Advances in neural
information processing systems, 2016, pp. 4898–4906.
[26] F. Chollet, Xception: Deep learning with depthwise separable convolutions, in: Proceedings of the IEEE conference on computer vision and
pattern recognition, 2017, pp. 1251–1258.
[27] Covid-19
ct
segmentation
dataset
nr.
medicalsegmentation.com/covid19/, [Online;
12, 2020] (2020).

26

2,
https://
accessed August

[28] Covid-19 ct lung and infection segmentation dataset, https://zenodo.
org/record/3757476#.X1nqY4vhWUn, [Online; accessed August 12,
2020] (2020).
[29] S. Agarwal, N. S. Punn, S. K. Sonbhadra, P. Nagabhushan, K. Pandian,
P. Saxena, Unleashing the power of disruptive and emerging technologies
amid covid 2019: A detailed review, arXiv preprint arXiv:2005.11507
(2020).
[30] Y. Li, L. Xia, Coronavirus disease 2019 (covid-19): role of chest ct in
diagnosis and management, American Journal of Roentgenology 214 (6)
(2020) 1280–1286.
[31] X. Ding, J. Xu, J. Zhou, Q. Long, Chest ct findings of covid-19 pneumonia by duration of symptoms, European Journal of Radiology (2020)
109009.
[32] H. Meng, R. Xiong, R. He, W. Lin, B. Hao, L. Zhang, Z. Lu, X. Shen,
T. Fan, W. Jiang, et al., Ct imaging and clinical course of asymptomatic
cases with covid-19 pneumonia at admission in wuhan, china, Journal
of Infection (2020).
[33] F. Shi, J. Wang, J. Shi, Z. Wu, Q. Wang, Z. Tang, K. He, Y. Shi, D. Shen,
Review of artificial intelligence techniques in imaging data acquisition,
segmentation and diagnosis for covid-19, IEEE reviews in biomedical
engineering (2020).
[34] A. Shoeibi, M. Khodatars, R. Alizadehsani, N. Ghassemi, M. Jafari,
P. Moridian, A. Khadem, D. Sadeghi, S. Hussain, A. Zare, et al., Automated detection and forecasting of covid-19 using deep learning techniques: A review, arXiv preprint arXiv:2007.10785 (2020).
[35] L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, J. Bai, Y. Lu, Z. Fang,
Q. Song, et al., Artificial intelligence distinguishes covid-19 from community acquired pneumonia on chest ct, Radiology (2020).
[36] C. Butt, J. Gill, D. Chun, B. A. Babu, Deep learning system to screen
coronavirus disease 2019 pneumonia, Applied Intelligence (2020) 1.

27

[37] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural
networks for volumetric medical image segmentation, in: 2016 fourth
international conference on 3D vision (3DV), IEEE, 2016, pp. 565–571.
[38] O. Gozes, M. Frid-Adar, H. Greenspan, P. D. Browning, H. Zhang,
W. Ji, A. Bernheim, E. Siegel, Rapid ai development cycle for the coronavirus (covid-19) pandemic: Initial results for automated detection &
patient monitoring using deep learning ct image analysis, arXiv preprint
arXiv:2003.05037 (2020).
[39] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image
recognition, in: Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[40] Q. Yan, B. Wang, D. Gong, C. Luo, W. Zhao, J. Shen, Q. Shi, S. Jin,
L. Zhang, Z. You, Covid-19 chest ct image segmentation–a deep convolutional neural network solution, arXiv preprint arXiv:2004.10987 (2020).
[41] S. Hu, Y. Gao, Z. Niu, Y. Jiang, L. Li, X. Xiao, M. Wang, E. F. Fang,
W. Menpes-Smith, J. Xia, et al., Weakly supervised deep learning for
covid-19 infection detection and classification from ct images, IEEE Access (2020).
[42] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in:
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2015, pp. 1–9.
[43] V. Dumoulin, F. Visin, A guide to convolution arithmetic for deep learning, arXiv preprint arXiv:1603.07285 (2016).
[44] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, C. Pal, The
importance of skip connections in biomedical image segmentation, in:
Deep Learning and Data Labeling for Medical Applications, Springer,
2016, pp. 179–187.
[45] N. Akhtar, U. Ragavendran, Interpretation of intelligence in cnn-pooling
processes: A methodological survey, Neural Computing and Applications (2020) 1–20.

28

[46] O. Rippel, J. Snoek, R. P. Adams, Spectral representations for convolutional neural networks, in: Advances in neural information processing
systems, 2015, pp. 2449–2457.
[47] S. Zagoruyko, N. Komodakis, Paying more attention to attention: Improving the performance of convolutional neural networks via attention
transfer, arXiv preprint arXiv:1612.03928 (2016).
[48] S. Woo, J. Park, J.-Y. Lee, I. So Kweon, Cbam: Convolutional block
attention module, in: Proceedings of the European conference on computer vision (ECCV), 2018, pp. 3–19.
[49] S. Ruder, An overview of gradient descent optimization algorithms,
arXiv preprint arXiv:1609.04747 (2016).
[50] R. Caruana, S. Lawrence, C. L. Giles, Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping, in: Advances in
neural information processing systems, 2001, pp. 402–408.
[51] Y. Qiu, Y. Liu, J. Xu, Miniseg: An extremely minimum network for
efficient covid-19 segmentation (2020). arXiv:2004.09750.
[52] V. Badrinarayanan, A. Kendall, R. Cipolla, Segnet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE transactions on pattern analysis and machine intelligence 39 (12) (2017) 2481–
2495.

29

