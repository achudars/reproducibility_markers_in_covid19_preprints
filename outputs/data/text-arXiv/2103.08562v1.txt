Is Medical Chest X-ray Data Anonymous?
Kai Packhäuser1,*,+ , Sebastian Gündel1,*,+ , Nicolas Münster1 , Christopher Syben1 ,
Vincent Christlein1 , and Andreas Maier1
1 Pattern

arXiv:2103.08562v1 [cs.CV] 15 Mar 2021

Recognition Lab, Department of Computer Science, Friedrich-Alexander University Erlangen-Nürnberg,
91058 Erlangen, Germany
* {kai.packhaeuser, sebastian.guendel}@fau.de
+ these authors contributed equally to this work

ABSTRACT
With the rise and ever-increasing potential of deep learning techniques in recent years, publicly available medical
data sets became a key factor to enable reproducible development of diagnostic algorithms in the medical domain1–3 .
Medical data contains sensitive patient-related information and is therefore usually anonymized by removing patient
identifiers, e. g., patient names before publication4, 5 . To the best of our knowledge, we are the first to show that a
well-trained deep learning system is able to recover the patient identity from chest X-ray data. We demonstrate this
using the publicly available large-scale ChestX-ray14 dataset, a collection of 112,120 frontal-view chest X-ray images
from 30,805 unique patients. Our verification system is able to identify whether two frontal chest X-ray images are
from the same person with an AUC of 0.9940 and a classification accuracy of 95.55 %. We further highlight that the
proposed system is able to reveal the same person even ten and more years after the initial scan. When pursuing a
retrieval approach, we observe an mAP@R of 0.9748 and a precision@1 of 0.9963. Based on this high identification
rate, a potential attacker may leak patient-related information and additionally cross-reference images to obtain more
information. Thus, there is a great risk of sensitive content falling into unauthorized hands or being disseminated
against the will of the concerned patients. Especially during the COVID-19 pandemic, numerous chest X-ray datasets
have been published to advance research6–9 . Therefore, such data may be vulnerable to potential attacks by deep
learning-based re-identification algorithms.

Chest radiography (X-ray) is a modality that is routinely used for diagnostic procedures but also interventional purposes around
the world10 . It became the most common medical imaging examination for pulmonary diseases and allows a clear investigation
of the thorax11 . Chest X-ray imaging is therefore well-suited for diagnosing several pathologies including pulmonary nodules,
masses, pleural effusions, pneumonia, COPD, and cardiac abnormalities12 . It is also used for COVID-1913 screening, as
abnormalities characteristic of those infected with the coronavirus can be detected in radiographs14 . While chest radiography
plays a crucial role in clinical care, discovering certain diseases and abnormalities in chest radiographs can be a challenging
task for radiologists, which potentially results in undesirable misdiagnoses15 . Therefore, computer-aided detection (CAD)
systems based on deep learning (DL)16 techniques have been developed in recent years to facilitate radiology workflows. These
systems, characterized by their enormous benefits, can be utilized for a wide range of applications, e. g., for the automatic
recognition of abnormalities in chest radiographs12, 17 and the detection of tumors in breast mammography18 . Some techniques
even show the potential to exceed human performance19 . However, the CAD systems are only treated as an additional source to
support the radiologists and to increase certainty in their reading decisions.
On the one hand, the large variety of medical applications allows DL to grow and tackle real-life problems that were
previously not solvable or improving solutions offered by traditional machine learning methods16 . On the other hand, DL is
a data-driven approach and well-known for its need for big data to train the neural networks20, 21 . For these reasons, a vast
amount of medical datasets have been published in recent years that enable researchers to develop diagnostic algorithms in
the medical field in a reproducible way1 . These include several large-scale chest radiography datasets, e. g., the CheXpert22 ,
the PLCO23 and the ChestX-ray1424 datasets. But especially during the COVID-19 pandemic25, 26 , the number of publicly
available chest radiography datasets increased rapidly. A few selected examples are the COVID-19 Image Data Collection6 ,
the Figure 1 COVID-19 Chest X-ray Dataset Initiative7 , the ActualMed COVID-19 Chest X-ray Dataset Initiative8 , and the
COVID-19 Radiography Database9 .
Chest radiography datasets typically consist of two parts: First, the image data itself, which provides clinical information
about the anatomical structure of the thorax. Second, the associated metadata, which contains sensitive patient-related information that is either stored in a separate file or embedded directly in the images4 . Proper data anonymization constitutes an
important step when preparing medical data for public usage to ensure that a patient’s identity cannot be revealed in publicly
available datasets4 . In practice, any personally identifiable information is removed from the data before it is shared. These
1

Figure 1. General problem scenario: Comparing a given chest radiograph to publicly available dataset images by means of
DL techniques would either result in discrete labels indicating whether or not the dataset images belong to the same patient as
the given radiograph (verification scenario) or yield a ranked list of the most similar radiographs related to the given scan
(retrieval scenario). Images belonging to the same patient are highlighted with the same color. The given radiograph is marked
with an asterisk. The shown cases would enable a potential attacker to link sensitive patient-related information contained in
the dataset to the image of interest.
objectives and requirements are specified, e. g., by the Health Insurance Portability and Accountability Act (HIPAA)27 in the
United States or the General Data Protection Regulation (GDPR)28 in Europe.
In 2017, Google entered into a project with the National Institutes of Health (NIH) to publish a dataset containing 100,000
chest radiographs. However, the release was canceled two days before publication after Google was informed by the NIH that
the radiographs still contained personal information which indicates that the data was incorrectly anonymized29, 30 . This major
incident highlights that many potential pitfalls can arise when clinical and technological institutions collect and share large
medical datasets to revolutionize health-care.
In the past, various data de-identification techniques have been proposed, including commonly-used methods like
pseudonymization31 and k-anonymity32 . Pseudonymization describes a technique that replaces a true identifier, e. g., the
name or the patient identification number by a pseudonym that is unique to the patient but has no relation to the person31 .
However, pseudonymization is a rather weak anonymization technique as the patient’s identity may still be revealed, e. g.,
by cross-referencing with other publicly available datasets. In contrast, k-anonymity modifies the data before sharing in
such a way that every sample in the published dataset can be associated with at least k different subjects. In this way, the
probability of performing identity disclosure is limited to at most 1/k32, 33 . Nevertheless, when background knowledge is
available, k-anonymity is susceptible to many attacks.
To date, little attention has been paid to the possibility of re-identifying patients in large medical datasets by means of DL
techniques. However in theory, medical data disclosure, as illustrated in Figure 1, could be facilitated for potential attackers
by using suitable DL approaches. Consider a publicly available dataset that is supposedly anonymized but contains further
sensitive patient-related information, e. g., diagnosis, treatment history, and clinical institution. If a radiograph of known
identity is accessible to a potential attacker and a properly working verification or re-identification model exists, then the model
could be used to compare the given radiograph to each image in the dataset which would essentially result in a set of images
belonging to the same patient (patient verification) or yield a ranked list of the most similar images to the given radiograph
(patient re-identification). In this way, the patient’s identity may be linked to sensitive data contained in the dataset. As a
result, more patient-related information may have been leaked, highlighting the enormous data security and data privacy issues
involved.

2/12

Table 1. Overview of the obtained verification results for our experiments using varying training set sizes Ns . Moreover,
different data handling techniques were used (FTS: Fixed training set; RNP: Randomized negative pairs). For each experiment,
the training sets were balanced with respect to the amount of positive and negative image pairs. In this table, we present the
area under the curve (AUC) (together with the lower and upper bounds of the 95 % confidence intervals from 10,000 bootstrap
runs), the accuracy, and the F1-score. Bold text emphasizes the overall highest AUC value.
Data
handling

Ns

100,000

200,000
FTS

400,000

RNP

800,000

FTS

η

AUC + 95 % CI

10−3

0.8610

10−4

0.8509

10−5

0.7429

10−6

0.7257

10−7

0.7440

10−3

0.9448

10−4

0.8886

10−5

0.8158

10−6

0.7615

10−7

0.7526

10−3

0.9541

10−4

0.9587

10−5

0.8518

10−6

0.7417

10−7

0.7306

10−3

0.9826

10−4

0.9896

10−4

0.9940

10−5

0.9278

10−5

0.9200

10−6

0.8669

10−7

0.8126

0.8632
0.8588
0.8532
0.8485
0.7459
0.7399
0.7289
0.7225
0.7470
0.7409
0.9461
0.9435
0.8906
0.8866
0.8185
0.8132
0.7645
0.7586
0.7556
0.7495
0.9552
0.9529
0.9599
0.9575
0.8542
0.8494
0.7449
0.7386
0.7337
0.7275
0.9833
0.9820
0.9901
0.9891
0.9944
0.9937
0.9294
0.9262
0.9217
0.9182
0.8692
0.8646
0.8152
0.8099

Accuracy

Specificity

Recall

Precision

F1-score

0.7782

0.7710

0.7853

0.7742

0.7797

0.7461

0.8414

0.6507

0.8040

0.7193

0.6322

0.7924

0.4721

0.6945

0.5621

0.6702

0.6719

0.6685

0.6708

0.6696

0.6741

0.7002

0.6480

0.6837

0.6654

0.8743

0.8685

0.8800

0.8700

0.8750

0.7920

0.8547

0.7293

0.8339

0.7781

0.6758

0.8574

0.4943

0.7761

0.6039

0.6755

0.7448

0.6062

0.7038

0.6514

0.6819

0.6791

0.6847

0.6809

0.6828

0.8852

0.8655

0.9049

0.8706

0.8874

0.8755

0.9290

0.8219

0.9205

0.8684

0.7450

0.8410

0.6490

0.8032

0.7179

0.6572

0.7152

0.5991

0.6778

0.6360

0.6567

0.6801

0.6333

0.6644

0.6485

0.9324

0.9393

0.9254

0.9385

0.9319

0.9537

0.9541

0.9532

0.9541

0.9536

0.9555

0.9822

0.9287

0.9812

0.9542

0.8339

0.8946

0.7732

0.8801

0.8232

0.8215

0.8888

0.7543

0.8715

0.8087

0.7752

0.8165

0.7339

0.7999

0.7655

0.7247

0.7502

0.6992

0.7368

0.7175

In our work, we investigated whether conventional anonymization techniques are secure enough and whether it is possible
for re-identify and de-anonymize individuals from their medical data using DL-based methods. Therefore, we considered the
public ChestX-ray14 dataset24 , which is one of the most widely used research datasets in the field of radiographic problems.
Our algorithms are trained to determine whether two arbitrary chest radiographs can be recognized to belong to the same patient
or not. Moreover, we investigated whether patients can be re-identified when metadata is not available. Furthermore, this work
aims to draw attention to the massive problem of releasing medical data without considering that DL systems can easily be
used to reveal a patient’s identity. Therefore, we call for reconsidering conventional anonymization techniques and developing
more secure methods that resist potential attacks by DL algorithms.

Patient Verification
First, we trained a siamese neural network (SNN) architecture on the ChestX-ray14 dataset to determine whether two individual
chest radiographs correspond to the same patient or not. Our model was designed to process the two input images in two
identical network branches, which are then combined by a merging layer. The fused information is fed through further network
layers resulting in a single output score indicating the identity similarity.
Table 1 summarizes the outcomes of our evaluation. We analyzed a multitude of different experimental setups with
varying learning rates η and differing balanced training set sizes Ns . Moreover, we investigated the effect of using epoch-wise
3/12

1

0.8

0.8
True Positive Rate

True Positive Rate

1

0.6
0.4
N = 800, 000
N = 400, 000
N = 200, 000
N = 100, 000

0.2
0

0

0.2

0.4
0.8
0.6
False Positive Rate

True Positive Rate

0.97

0.97

0.94

0.91

0.91

0.93

0

1

0.93

η
η
η
η
η

0.4
0.2

Figure 2. ROC curves for different training set sizes Ns
and a fixed LR of η = 10−4 . The fixed data handling
technique was employed.

1.0

0.6

0

0.2

= 10−3
= 10−4
= 10−5
= 10−6
= 10−7

0.4
0.8
0.6
False Positive Rate

1

Figure 3. ROC curves for varying LRs η and a training
set size of Ns = 800, 000 image pairs. The fixed data
handling technique was employed.

0.92
0.83

0.87

0.85

0.82

0.86

9

10

11

12

0.8

0.96

0.95

No

Yes

0.6
0.4
0.2
<1

1

2

3

7
5
6
8
4
(a) Age difference in years

(b) Abnormality change

Figure 4. True positive rates (TPRs) for image pairs with (a) age differences and (b) with changes in the disease pattern.
randomized negative pairs (RNP) versus fixed training sets (FTS) for the entire learning procedure. For all experiments, we
used the same balanced validation and testing set with 50,000 and 100,000 image pairs, respectively, without patient overlap
between any split. To assess the performance of the trained models, we performed a receiver operating characteristic (ROC)
analysis by computing the AUC value together with the 95 % confidence intervals from 10,000 bootstrap runs. Moreover, we
calculated the accuracy, specificity, recall, precision, and F1-score.
The results indicate that the amount of training data plays a crucial role in the patient verification task. We observe a
significant performance increase as the training set size grows. For instance, when using 100,000 image pairs for training, we
obtain AUC values in the range of 0.7440 to 0.8610. In contrast, by enlarging the training set size to 800,000 image pairs, we
receive AUC scores in the range of 0.8126 to 0.9940. These findings have been visualized in the ROC curves shown in Figure 2
which illustrates the effect of the training set size on the verification performance when using fixed training sets. When training
our SNN with 800,000 image pairs, we obtained an AUC score of 0.9896. The influence of the learning rate (LR) is shown in
Figure 3.
We also observed that randomly constructing the negative image pairs in each epoch led to further improvements in the
final model performance. By using this data handling technique, we achieved our overall best results. The respective outcomes
are reported in Table 1. When training our network architecture with a LR of 10−4 and a total of 800,000 training samples with
epoch-wise randomly constructed negative pairs, the AUC score improved from 0.9896 to 0.9940. Besides, the other reported
evaluation metrics apart from the recall also increased compared to the results achieved by the model trained with the fixed set.

4/12

True Positives

(a)

(b)

(c)

(d)

False Positive

False Negative

(e)

(f)

Figure 5. Exemplary image pairs are classified by our best performing verification model. Each column represents one image
pair. The first four columns (a)–(d) show true positive classifications. The last two columns (e) and (f) depict a false positive
and a false negative classification, respectively.
We also analyzed how our best model behaves when comparing images of the same patient where the acquisition dates
are several years apart. The results are illustrated in Figure 4a. We received a TPR of 0.97 for image pairs that had small age
differences of one year or less. As the age variation between the follow-up images and the initial scan increases, we observe
a slight decrease in the TPR values. Nevertheless, our model still shows competitive results even if the patient’s age in two
images differs by several years. Even for an age difference of twelve years, we can verify that two images are belonging to the
same patient by 86 %.
Additionally, we investigated the model’s verification capability in the case of new abnormality patterns appearing in
follow-up scans that did not occur in previously acquired chest radiographs. Figure 4b shows that regardless of the abnormality,
we nearly observe no decline in the TPR values, proving the robustness of our trained SNN architecture.
Finally, we perform a qualitative evaluation where we visually inspect some exemplary image pairs evaluated using our
best-performing verification model. In Figure 5, we show four true positive (TP) classifications (a)–(d), one pair that has been
classified as a false positive (FP) (e), and one example for a false negative (FN) image pair (f). The shown images clearly
illustrate the high variance present in the ChestX-ray14 dataset.The first image pair (a) shows two images belonging to the
same patient with a difference of seven years. Clear differences in pixel intensities and lung shape are observed. However,
both images belong to the same person, cf. the small visible stitches in the area of the upper right lung. Also, image pairs with
large difference in scaling (b) or rotation (c) are verified correctly. Our model is also robust to the patients’ pathology: While
the upper image of (b) shows characteristics of pneumothorax, the patient suffered from cardiomegaly, effusion, and masses
in the lower image, according to the provided annotations. Similarly in (c), where the upper image indicates the presence of
infiltration and pneumothorax, whereas the lower scan shows signs of infiltration and nodules Figure 5 (e) shows an exemplary
image pair that has falsely been classified as positive. Conversely, (f) depicts a positive image pair that has been incorrectly
classified as negative.

Patient Re-identification
For our patient re-identification experiments, we trained another SNN architecture on the ChestX-ray14 dataset. In contrast to
the verification model, we omitted all the layers from the merging layer onwards. The main objective was to learn appropriate
feature representations instead of directly determining whether the inputs belong to the same patient or not. After training
the network, we used the ResNet-50 backbone as a feature extractor for the actual image retrieval task. By computing the
Euclidean distance between the embeddings of the query image and each other image, we obtained for each query image a
ranked list of its most similar images in terms of identity.
The results of the corresponding image retrieval experiments are summarized in Table 2. When using the original image
size of 1024×1024 pixels for evaluation, we obtain a precision@1 of more than 99 % showing that the closest match nearly
always is the same patient. The high mean average precision at R (mAP@R) of about 97 % further depicts that most of the
most similar images are correctly identified. We observe a slight decrease in performance as the image size was reduced.
Nevertheless, when the images were downsampled to a resolution of 512×512 pixels, we still obtain high performance values.
5/12

Dataset

ChestX-ray14

COVID-19 dataset

Input dimensions

mAP@R

R-Precision

Precision@1

1024×1024

0.9748

0.9763

0.9963

800×800

0.9709

0.9726

0.9958

512×512

0.9572

0.9601

0.9945

224×224

0.7730

0.7979

0.9756

original

0.8569

0.8707

0.8821

Table 2. Overview of the obtained results for our image retrieval experiments. In this table, we report the mAP@R, the
R-Precision, and the Precision@1. The first 4 rows show the results on the ChestX-ray14 dataset for different image resolutions
used for evaluation. The last row indicates the results on the COVID-19 Image Data Collection. Bold text represents the overall
highest performance metrics.
When the image size was reduced too aggressively, e. g., to 224×224 pixels, the mAP@R and the R-Precision rates drop. Yet,
we still observed a high Precision@1 of more than 97 %.
Furthermore, we analyzed the re-identification performance on the COVID-19 Image Data Collection6 , a dataset that was
created and published as an initiative to provide COVID-19 related chest radiographs and CT scans for machine learning tasks.
As can be seen in Table 2 (last row), we also obtain high retrieval values although we haven’t seen any image during training,
which demonstrates the feasibility of the trained re-identification network on other datasets than the ChestX-ray14 dataset.
Note that only those images in the COVID-19 Image Data Collection were used for evaluation that were acquired using the
anterior-posterior or the posterior-anterior view, while images taken in the lateral position and CT scans were discarded.

Discussion and Conclusion
In this paper, we investigated the patient verification and re-identification capabilities of DL techniques on chest radiographs.
We have shown that well-trained SNN architectures are able to compare two individual frontal chest radiographs and reliably
predict whether these images belong to the same patient or not. Moreover, we have shown that DL models have the potential
to accurately retrieve relevant images in a ranked list. Our models have been evaluated on the publicly available ChestXray14 dataset and showed competitive results with an AUC of up to 0.9940 and classification accuracy of more than 95 %
in the verification scenario and an mAP@R of 97 % and a precision@1 of about 99 percent in the image retrieval scenario.
Especially the fact that basic SNNs have the capability to re-identify patients despite potential age differences or disease
changes demonstrated the effectiveness of DL techniques for this task.
As shown in Figure 5, the used dataset suffers from a high variance because of different normalization schemes across
the images. Moreover, we hypothesize that special noise patterns characteristic for unique patients appear in the images
which might unintentionally improve the re-identification performance. For example, the initial anonymization strategy may
be biased towards the clinical institution and, therefore, also towards follow-up images. To get a better impression of the
re-identification capability of our SNN architecture, we also intend to investigate other datasets which show less or ideally no
correlation between potential noise patterns and the patient identity. Therefore, further research on multiple datasets should
ideally be considered. For the re-identification experiments, we already evaluated our models on a completely different dataset,
the COVID-19 Image Data Collection. While the retrieval rates are lower, we still obtain precision@1 values of more than
88 % without fine-tuning on this dataset. This indicates that re-identification is also applicable for data that was acquired in
various hospitals around the world where other pre-processing steps may be taken before data publication compared to the
ChestX-ray14 dataset.
Moreover, we want to accentuate that our trained network architectures are able to handle non-rigid transformations that
may appear between two images of the same person in the ChestX-ray14 dataset. Such deformations can occur due to various
breath states in follow-up scans or due to different positioning during X-ray acquisition. Hence, the shape of the heart and lungs,
or the contours of the ribs may appear deformed compared to an initial scan. The obtained results lead to the assumption that
our trained SNN architectures can withstand such deformations and can therefore be used for reliable patient re-identification
on chest radiographs.
We conclude that publicly available medical chest X-ray data is not entirely anonymous. Using a DL-based re-identification
network enables an attacker to compare a given radiograph with public datasets and to associate accessible metadata with the
image of interest. Thus, sensitive patient data is exposed to a high risk of falling into the unauthorized hands of an attacker who
may disseminate the gained information against the will of the concerned patient. At this point, we want to emphasize that data
leakage of this kind requires that the attacker has previously gained access to an image of a known person. This could happen,
for example, through a stolen CD containing raw medical data of a specific patient. However, even if the attacker owns an
6/12

image of an unknown identity, a re-identification model can be used to find the same patient across various datasets. Assuming
multiple datasets contain the same patient but different metadata, an attacker would be able to obtain a more complete picture
of the respective patient. We hypothesize that collecting patient information by this means could significantly help an attacker
infer the true identity of the patient. We therefore urge that conventional anonymization techniques be reconsidered and that
more secure methods be developed to resist the potential attacks by DL-based algorithms.

Methods
Siamese neural networks
To re-identify patients from their chest radiographs, we employ SNN architectures for both the classification and the retrieval
tasks. A SNN receives two input images which are processed by two identical feature extraction blocks sharing the same set of
network parameters. The resulting feature representations can then be used to compare the inputs. The concept of a SNN was
initially introduced by Bromley et al.34 for handwritten signature verification. Taigman et al.35 applied this idea in the field of
face verification and proposed the DeepFace system. Moreover, Koch et al.36 presented an approach for one-shot learning on
the Omniglot37 and MNIST38 datasets.
NIH ChestX-ray14 dataset
With a total of 112,120 frontal-view chest radiographs from 30,805 unique patients, the NIH ChestX-ray1424 dataset counts
to one of the largest publicly available chest radiography datasets in the scientific community. Due to follow-up scans, the
image collection provides an average of 3-4 images per patient. The radiographs are 8-bit gray-scale images with a size of
1024 × 1024 pixels. Associated metadata is available for all images in the dataset. The additional data comprises information
about the underlying disease patterns (either no finding or a combination of up to 14 common thoracic pathologies), the number
of follow-up images taken, the patients’ age and gender, and the projection view (anterior-posterior or posterior-anterior) used
for radiography acquisition. According to the publisher, the dataset was carefully screened to remove all personally identifiable
information before release39 . Therefore, the patient names were replaced by integer IDs. Moreover, personal data in the image
domain itself has been made unrecognizable by placing black boxes over the corresponding image areas.
Data preparation
Since SNN architectures require pairs of images for training and evaluation, we construct both positive and negative image pairs
from the images contained in the ChestX-ray14 dataset. In this context, a positive pair consists of two images belonging to the
same patient, whereas a negative pair comprises two images that belong to different patients. Mathematically, the constructed
dataset can be described according to
S = {(xx11 , x 12 , y1 ), ..., (xxm1 , x m2 , ym ), ..., (xxM1 , x M2 , yM )} , with ym ∈ {0, 1} ,

(1)

where the triplet (xxm1 , x m2 , ym ) represents one sample consisting of two images x m1 and x m2 , and the corresponding label ym . M
refers to the total number of samples and m denotes an iterator variable in the range of m ∈ [1, M]. The class label ym symbolizes
a binary variable that takes the value 0 for negative image pairs and 1 for positive image pairs.
To ensure that images from one patient only appear either in the training, validation, or testing set, we use the patient-wise
splitting strategy. According to the official split, the data is roughly divided into 70 % training, 10 % validation, and 20 %
testing. Based on this split, we construct the actual image pairs for each subset.
Offline mining

For patient verification, we follow an offline mining approach, meaning that the positive and negative image pairs are
generated once before conducting the experiments. First, the positive pairs are generated by only considering the patients
for whom multiple images exist in the respective subset. For each patient with follow-up images, we produce all possible
tuple combinations assuming the images to be unique. Then, the negative pairs in each subset are randomly generated and
concatenated with the positive pairs afterwards.
Online mining

For the patient re-identification experiments, we choose an online mining approach, meaning that image pairs are formed in
each batch during the training procedure. This means that the embeddings of all batch images are first computed and then
subsequently used in all possible combinations as input for the loss function. Moreover, all patients with only one available
image were discarded from the training set.
7/12

Figure 6. SNN architecture used for patient verification on the ChestX-ray1424 dataset. The feature extraction blocks (light
blue) share the same set of network parameters and produce the feature representations z 1 and z 2 (yellow). After merging
(orange) and an additional FC and sigmoid layer σ (light blue), the network yields the final output score ŷ (green). For our
patient re-identification experiments, we used the same architecture but ejected all the layers from the merging layer onwards.
Patient verification
Deep learning architecture

For patient verification, the used SNN architecture (see Figure 6) receives two images x 1 and x 2 of size 3×256×256. Both
inputs are processed by a pre-trained ResNet-50 incorporated in each network branch. In its original version, the ResNet-50
was designed to classify images into 1,000 object categories trained on the ImageNet40 dataset.To adapt the ResNet-50 to
our specific needs, we replace its classification layer with a layer consisting of 128 output neurons producing the feature
representations z 1 and z 2 , respectively. To merge both network branches, the absolute difference of the sigmoid activations of
the two feature vectors is computed. We add a fully-connected (FC) layer to reduce the dimensionality to one neuron, followed
by another sigmoid activation function σ which yields the final output score ŷ ∈ [0, 1].
Training strategy

The verification model is trained using the binary cross-entropy (BCE) loss. Characterizing a simplification of the crossentropy (CE) loss for two-class problems, the BCE is given by
2

LBCE = − ∑ yi,c log(ŷi,c ) = −yi,1 log(ŷi,1 ) − (1 − yi,1 ) log(1 − ŷi,1 ) .

(2)

c=1

In this equation, c represents the class iterator, y denotes the true class indication of c for an observation i, and ŷ constitutes the
predicted output score of the model. Due to the fact that our verification model produces a single output neuron, we discard the
class iterator c in following notations. The BCE loss evaluates the performance of a binary classification model, where the
output of the classifier is a probability between 0 and 1. The loss value increases as the predicted output ŷi diverges from the
true class label yi . For instance, predicting a probability value of ŷi = 0.01 for an observation of class yi = 1 would correlate to
a high BCE loss. A perfect model would yield a loss of 0, which means that the prediction probability matches the actual class
label.
While training a learning algorithm, we aim to minimize the loss function of the used model. Neural networks are typically
optimized by using the mini-batch stochastic gradient descent (SGD) algorithm41, 42 . As an iterative algorithm, it performs a
parameter update step for every mini-batch of Nb samples according to
θ t+1 = θ t − η

1 Nb
∑ ∇θ t L(ŷi , yi ; θ t ) ,
Nb i=1

(3)

where L(ŷi , yi ; θ t ) represents the per-example loss between the predicted output and the actual class label, θ t are the network
parameters at iteration t, and η denotes the LR defining the influence of the gradient to the current parameters. In this work,
we combine mini-batch SGD and the adaptive moment estimation (Adam)43 method. The Adam optimizer holds the useful
property that it computes individual adaptive LRs for different parameters by using first and second order moments of past
gradients43 . This leads to parameters reaching an optimum being updated using smaller effective steps, whereas parameters far
away from the optimum get updated using larger step sizes. Thus, Adam often results in a faster convergence compared to plain
8/12

mini-batch SGD43 . The batch size Nb is set to 32 in all our experiments. We use different LRs to investigate their effect on the
model’s performance. Furthermore, we include an early stopping criterion with a patience p = 5, which means that the training
procedure stops as soon as the network does not improve for 5 epochs. We train the architecture using input dimensions of
3×256×256.
Evaluation techniques

We utilize ROC curves to visualize the trained verification models based on their performance. An ROC curve represents a
two-dimensional graph in which the TPR is plotted against the false positive rate (FPR) at various threshold settings44 . The
TPR and FPR arise from
T PR =

TP
, and
T P + FN

FPR =

FP
,
FP + T N

(4)

where TP and TN symbolize the number of true positives and true negatives, and FP and FN denote the amount of false positives
and false negatives, respectively. The resulting ROC curve indicates how many true positive classifications can be gained as an
increasing number of false positive classifications is allowed.
In order to compare several classification models, it might be beneficial to reduce the ROC curve to a single scalar value
that represents the expected performance. This is achieved by calculating the AUC. Since the AUC reflects a proportion of the
area of the unit square, its value will always range from 0 to 144 . The higher the AUC score, the better the model’s average
performance. Nevertheless, it has to be mentioned that a classifier with a high AUC might perform worse in a specific region of
ROC space than a classifier with a low AUC value.
Additionally, we evaluate the performance by computing the accuracy, specificity, recall, precision and F1-score. Therefore,
the threshold at the output neuron is set to t = 0.5. Note that recall equates with the term of the TPR. The following formulas
mathematically define the selected performance metrics:
Accuracy =
Recall =
F1 =

TP+TN
T P + T N + FP + FN

TP
T P + FN

2 · Precision · Recall
Precision + Recall

(5)

Speci f icity =

(7)

Precision =

TN
T N + FP

TP
T P + FP

(6)
(8)

(9)

Patient re-identification
Deep learning architecture

For patient re-identification, we train a SNN architecture which receives two images x1 and x2 of size 3×1024×1024. Both
inputs are processed by a pre-trained ResNet-50 incorporated in each network branch. However, the network head of the used
ResNet-50 is slightly modified. The average pooling layer is replaced by an adaptive average pooling layer producing feature
maps of size 5×5. In addition to the adaptive average pooling layer, an adaptive max-pooling layer is applied which also yields
feature maps of size 5×5. The outputs of the pooling layers are concatenated and processed by a 1×1 convolutional layer
reducing the number of feature maps from 2048 to 100. The feature maps are then flattened, followed by two successive FC
layers resulting in 128-dimensional feature representations z1 and z2 for the first and the second network branch.
Training strategy

The re-identification model is trained using the contrastive loss function which is typically utilized to achieve a meaningful
mapping F from high to low dimensional space. By using the contrastive loss, the network learns to map similar inputs to
nearby points on the output manifold while dissimilar inputs are mapped to distant points. Mathematically, it is given by
(
1
kF(xxi,1 ) − F(xxi,2 )k22
if yi = 1
Lcontrastive = 21
(10)
2
x
x
if yi = 0 ,
2 max(0, m − kF(x i,1 ) − F(x i,2 )k2 )
where x i,1 and x i,2 represent the two input images and yi denotes the class label of the image pair. Note that positive pairs are
labeled with yi = 1, whereas negative pairs are labeled with yi = 0. Negative pairs contribute to the loss only if their distance is
smaller than a certain margin m. In this work, the margin is set to m = 1.
For our image retrieval experiments, the SNN architecture is optimized using the SGD algorithm (already explained above)
in combination with the 1cycle learning policy45, 46 . When using the 1cycle LR schedule, the LR η steadily increases until it
reaches a chosen maximum value and gradually decreases again thereafter. This schedule changes the LR after every single
9/12

batch and is pursued a pre-defined number of epochs. The upper bound is chosen at 0.1584 with the help of a LR finder. The
lower bound is set to 0.0063. The L2 regularization technique is used with a decay factor of 10−5 . Moreover, the batch size is
adjusted to 32. We optimize the SNN architecture by first training the adapted network head of the incorporated ResNet-50
for 30 epochs with all other parameters being frozen. Then, the complete architecture is trained for another cycle, this time
consisting of 50 epochs.
Since the batch size limits the task of constructing informative positive and negative pairs in the online mining strategy, the
concept of cross-batch memory47 is utilized to generate sufficient pairs across multiple mini-batches. This concept is based
upon the observation that the embedding features generally tend to change slowly over time. This “slow drift” phenomenon
allows the use of embeddings of previous iterations that would normally be considered out-dated and discarded. For our
experiments, a memory size of 128 is chosen, meaning that the last 4 batches are considered for mining.
Evaluation techniques

To evaluate the re-identification performance of our trained model, several metrics are computed. R-Precision represents the
precision at R, where R denotes the number of relevant images for a given query image. In other words, if the top-R retrieved
images show r relevant images, then R-Precision can be calculated from equation 11. Note that this value is then averaged over
all query samples. Precision@1 constitutes a special case and evaluates how many times the top-1 images in the retrieved lists
are relevant.
R-Precision =

r
R

(11)

To further consider the order of the relevant images within the retrieved list, the mean average precision at R (mAP@R)
is computed according to equation 12. The mAP@R denotes the mean of the average precision scores at R (AP@R) over all
Q query images. The AP@R (see equation 13) is the average of the precision values over all R relevant samples, where P@i
refers to the precision at rank i and rel@i is an indicator function which equals 1 if the sample is relevant at rank i and 0 if it is
not relevant.
mAP@R =

1 Q
∑ APi @R
Q i=1

(12)

AP@R =

1 R
∑ P@i × rel@i
R i=1

(13)

Data availability
The NIH ChestX-ray14 dataset used throughout the current study is available via Box at https://nihcc.app.box.
com/v/ChestXray-NIHCC. The COVID-19 Image Data Collection is available on GitHub at https://github.com/
ieee8023/covid-chestxray-dataset.
Code availability
The code used to train and evaluate both the patient verification and the patient re-identification models will be made available
after acceptance. Correspondence and requests for materials should be addressed to K.P. or S.G.
Acknowledgements
The research leading to these results has received funding from the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation program (ERC grant no. 810316).
Author contributions
A.M., V.C., S.G., and C.S. conceived the main idea. K.P. and N.M. performed the experiments and the evaluation. K.P. wrote
the main part of the manuscript. S.G. offered continuous support during the experiments and the writing process. S.G., C.S.,
V.C., and A.M. provided expertise through intense discussions. All authors reviewed the manuscript.
Competing interests
The authors declare no competing interests.

References
1. Oakden-Rayner, L. Exploring Large-scale Public Medical Image Datasets. Acad. Radiol. 27, 106–112 (2020).
2. Dash, S., Shakyawar, S. K., Sharma, M. & Kaushik, S. Big data in healthcare: management, analysis and future prospects.
J. Big Data 6, 1–25 (2019).
3. Kostkova, P. et al. Who Owns the Data? Open Data for Healthcare. Front. public health 4, 7 (2016).
10/12

4. Willemink, M. J. et al. Preparing Medical Imaging Data for Machine Learning. Radiology 295, 4–15 (2020).
5. Kaissis, G. A., Makowski, M. R., Rückert, D. & Braren, R. F. Secure, privacy-preserving and federated machine learning
in medical imaging. Nat. Mach. Intell. 2, 305–311 (2020).
6. Cohen, J. P. et al. COVID-19 Image Data Collection: Prospective Predictions are the Future. arXiv: 2006.11988 (2020).
7. Chung, A.
Figure 1 COVID-19
Figure1-COVID-chestxray-dataset (2020).

Chest

X-ray

Dataset

Initiative.

https://github.com/agchung/

ActualMed COVID-19 Chest
8. Chung, A.
Actualmed-COVID-chestxray-dataset (2020).

X-ray

Dataset

Initiative.

https://github.com/agchung/

9. Rahman, T., Chowdhury, M. & Khandakar, A. COVID-19 Radiography Database. https://www.kaggle.com/tawsifurrahman/
covid19-radiography-database (2020).
10. Maier, A., Steidl, S., Christlein, V. & Hornegger, J. Medical Imaging Systems: An Introductory Guide, vol. 11111 (Springer,
2018).
11. Raoof, S. et al. Interpretation of Plain Chest Roentgenogram. Chest 141, 545–558 (2012).
12. Gündel, S. et al. Learning to recognize Abnormalities in Chest X-Rays with Location-Aware Dense Networks. In
Iberoamerican Congress on Pattern Recognition, 757–765 (Springer, 2018).
13. World Health Organization (WHO). Coronavirus. https://www.who.int/health-topics/coronavirus (2020). [Online; accessed:
21.12.2020].
14. Wang, L., Lin, Z. Q. & Wong, A. COVID-Net: a tailored deep convolutional neural network design for detection of
COVID-19 cases from chest X-ray images. Sci. Reports 10, 1–12 (2020).
15. Lee, C. S., Nagy, P. G., Weaver, S. J. & Newman-Toker, D. E. Cognitive and System Factors Contributing to Diagnostic
Errors in Radiology. Am. J. Roentgenol. 201, 611–617 (2013).
16. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015).
17. Gündel, S. et al. Multi-task Learning for Chest X-ray Abnormality Classification on Noisy Labels. arXiv: 1905.06362
(2019).
18. Akselrod-Ballin, A. et al. A region based convolutional network for tumor detection and classification in breast mammography. In Deep learning and data labeling for medical applications, 197–205 (Springer, 2016).
19. Rajpurkar, P. et al. CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning. arXiv:
1711.05225 (2017).
20. Roh, Y., Heo, G. & Whang, S. E. A Survey on Data Collection for Machine Learning: A Big Data - AI Integration
Perspective. IEEE Transactions on Knowl. Data Eng. (2019).
21. Maier, A., Syben, C., Lasser, T. & Riess, C. A gentle introduction to deep learning in medical image processing. Zeitschrift
für Medizinische Physik 29, 86–101 (2019).
22. Irvin, J. et al. CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison. In
Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, 590–597 (2019).
23. Gohagan, J. K., Prorok, P. C., Hayes, R. B. & Kramer, B.-S. The Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer
Screening Trial of the National Cancer Institute: History, organization, and status. In Controlled Clinical Trials, vol. 21,
251S–272S (2000).
24. Wang, X. et al. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification
and Localization of Common Thorax Diseases. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2097–2106 (2017).
25. Bandyopadhyay, D. et al. Covid-19 pandemic: cardiovascular complications and future implications. Am. J. Cardiovasc.
Drugs 1–14 (2020).
26. Spinelli, A. & Pellino, G. Covid-19 pandemic: perspectives on an unfolding crisis. The Br. journal surgery (2020).
27. Centers for Disease Control and Prevention. Health Insurance Portability and Accountability Act of 1996 (HIPAA).
https://www.cdc.gov/phlp/publications/topic/hipaa.html (2018). [Online; accessed: 23.12.2020].
28. European Union. Complete guide to GDPR compliance. https://gdpr.eu/ (2020). [Online; accessed: 23.12.2020].
29. O’Connor, M. Google axed release of vast x-ray dataset following NIH privacy concerns. https://www.healthimaging.com/
topics/imaging-informatics/google-axed-release-x-ray-dataset-nih-concerns (2019). [Online; accessed: 17.12.2020].
11/12

30. Vincent, J. Google scrapped the publication of 100,000 chest x-rays due to last-minute privacy problems. https:
//www.theverge.com/2019/11/15/20966460/google-scrapped-publication-100000-chest-x-rays-nih-project-2017 (2019).
[Online; accessed: 17.12.2020].
31. Noumeir, R., Lemay, A. & Lina, J.-M. Pseudonymization of Radiology Data for Research Purposes. J. Digit. Imaging 20,
284–295 (2007).
32. Sweeney, L. k-anonymity: A model for protecting privacy. Int. J. Uncertainty, Fuzziness Knowledge-Based Syst. 10,
557–570 (2002).
33. Gkoulalas-Divanis, A. & Loukides, G. Medical Data Privacy Handbook (Springer, 2015).
34. Bromley, J. et al. Signature Verification using a “Siamese” Time Delay Neural Network. Int. J. Pattern Recognit. Artif.
Intell. 7, 669–688 (1993).
35. Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. DeepFace: Closing the Gap to Human-Level Performance in Face
Verification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1701–1708 (2014).
36. Koch, G., Zemel, R. & Salakhutdinov, R. Siamese Neural Networks for One-shot Image Recognition. In ICML Deep
Learning Workshop (2015).
37. Lake, B. M., Salakhutdinov, R. & Tenenbaum, J. B. Human-level concept learning through probabilistic program induction.
Science 350, 1332–1338 (2015).
38. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-Based Learning Applied to Document Recognition. Proc. IEEE
86, 2278–2324 (1998).
39. National Institutes of Health (NIH).
NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community.
https://www.nih.gov/news-events/news-releases/
nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community (2017). [Online;
accessed: 05.01.2021].
40. Deng, J. et al. ImageNet: A Large-Scale Hierarchical Image Database. In 2009 IEEE Conference on Computer Vision and
Pattern Recognition, 248–255 (2009).
41. LeCun, Y. A., Bottou, L., Orr, G. B. & Müller, K.-R. Efficient BackProp. In Neural Networks: Tricks of the Trade, 9–48
(Springer, 2012).
42. Goodfellow, I., Bengio, Y. & Courville, A. Deep Learning (MIT Press, 2016). URL: http://www.deeplearningbook.org.
43. Kingma, D. P. & Ba, J. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980 (2014).
44. Fawcett, T. An introduction to ROC analysis. Pattern Recognit. Lett. 27, 861–874 (2006).
45. Smith, L. N. & Topin, N. Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. In
Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications, vol. 11006, 1100612 (International
Society for Optics and Photonics, 2019).
46. Smith, L. N. Cyclical Learning Rates for Training Neural Networks. In 2017 IEEE Winter Conference on Applications of
Computer Vision (WACV), 464–472 (IEEE, 2017).
47. Wang, X., Zhang, H., Huang, W. & Scott, M. R. Cross-Batch Memory for Embedding Learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6388–6397 (2020).

12/12

