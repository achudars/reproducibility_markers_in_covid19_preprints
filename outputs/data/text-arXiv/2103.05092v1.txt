Forest Guided Smoothing
Isabella Verdinelli and Larry Wasserman
Department of Statistics and Data Science
Carnegie Mellon University
March 8 2021

arXiv:2103.05092v1 [stat.ML] 8 Mar 2021

Abstract
We use the output of a random forest to define a family of local smoothers with
spatially adaptive bandwidth matrices. The smoother inherits the flexibility of the
original forest but, since it is a simple, linear smoother, it is very interpretable and it
can be used for tasks that would be intractable for the original forest. This includes
bias correction, confidence intervals, assessing variable importance and methods for
exploring the structure of the forest. We illustrate the method on some synthetic
examples and on data related to Covid-19.

Keywords: Random Forest, Nonparametric regression, generalized Jackknife

1

1

Introduction

Random forests are often an accurate method for nonparametric regression but they are
notoriously difficult to interpret. Also, it is difficult to construct standard errors, confidence
intervals and meaningful measures of variable importance. In this paper, we construct a
spatially adaptive local linear smoother that approximates the forest. Our approach builds
on the ideas in Bloniarz et al. (2016) and Friedberg et al. (2020). The main difference is that
we define a one parameter family of bandwidth matrices which help with the construction
of confidence intervals, and measures of variable importance.
Our starting point is the well-known fact that a random forest can be regarded as a
type of kernel smoother (Breiman (2000); Scornet (2016); Lin and Jeon (2006); Geurts
et al. (2006); Hothorn et al. (2004); Meinshausen (2006)). We take it as a given that the
forest is an accurate predictor and we do not make any attempt to improve the method.
Instead, we want to find a family of linear smoothers that approximate the forest. Then
we show how to use this family for interpretation, bias correction, confidence intervals,
variable importance and for exploring the structure of the forest.
Related Work. Our work builds on Bloniarz et al. (2016) and Friedberg et al. (2020).
Bloniarz et al. (2016) fit a local linear regression using weights from a random forest. They
show that this often leads to improved prediction. Friedberg et al. (2020) go further and
modify the forest algorithm to account for the fact that a local linear fit will be used and
to reduce the bias of the fit. This further improves the performance and yields confidence
intervals.
We use the forest weights to fit a local linear regression but we do so by first building
a family of bandwidth matrices {hHx : h > 0, x ∈ Rd } depending on one free parameter
2

h > 0. We use the bandwidth matrices to define a kernel from which we get the local linear
fit. Creating the bandwidth matrices has several advantages. First, it allows us to use the
generalized jackknife to correct the bias and construct confidence intervals. In contrast to
Friedberg et al. (2020), this allows us to use any off-the-shelf random forest; no adjustments
to the forest algorithm are required. Second, the collection of bandwidth matrices will be
used to create several summaries of the forest. For example, we can examine how much
smoothing is done with respect to different covariates and in different parts of the covariate
space. We also define the notion of a typical bandwidth matrix using the Wasserstein
barycenter. Third, we can explore variable importance based on local slopes at different
resolutions by varying the parameter h thus giving a multiresolution measure of variable
importance.
Paper Outline. In Section 2 we define the forest guided smoother. In Section 3
we discuss the construction of confidence intervals. In Section 4 we present methods for
exploring the structure of the forest. Examples are presented in Section 5. Section 6
contains concluding remarks.

2

Forest-Guided Smoothers

Let
(X1 , Y1 ), . . . , (Xn , Yn ) ∼ P
where Yi ∈ R and Xi ∈ Rd . We assume that d < n and is fixed. Let µ(x) = E[Y |X = x]
denote the regression function. Recall that the random forest estimator µ̂RF (x) is
B
1 X
µ̂RF (x) =
µ̂j (x)
B j=1

3

where each µ̂j is a tree estimator built from a random subsample of the data, a random
subsample of features and B is the number of subsamples.
We take, as a starting point, the assumption that µ̂RF is a good estimator. Our goal
is not to improve the random forest or provide explanations for its success. Rather, we
construct an estimator that provides a tractable approximation to the forest which can
then be used for other tasks.
As noted by Hothorn et al. (2004); Meinshausen (2006) the random forest estimator
µ̂(x)RF can be re-written as
µ̂RF (x) =

n
X

wi (x)Yi

i=1

for some weights wi (x) where wi (x) ≥ 0 and

P

i

wi (x) = 1. As these authors note, these

weights behave like a spatially adaptive kernel.
We proceed as follows. As in Friedberg et al. (2020) we split the data into two groups
D1 and D2 . For simplicity, assume each has size n. We construct a random forest µ̂RF
from D1 . Now we define the bandwidth matrix
Hx =

1X
wi (x)(Xi − x)(Xi − x)T
n i

!1/2
(1)

where the sum is over D1 . Let K be a spherically symmetric kernel and define
K(x; Hx ) = |Hx |−1 K(Hx−1 x).
This yields a kernel centered at x whose scale matches the scale of the forest weights. We
then define the one parameter family of bandwidth matrices Ξ = {hHx : h > 0, x ∈ Rd }.
We define the forest guided local linear smoother, or FGS, to be the local linear smoother
µ̂h (x) with kernel K(x; hHx ), that is, µ̂h (x) = β̂0 (x) obtained by minimizing
2
X
Yi − β0 (x) − β(x)T (Xi − x) K(Xi − x; hHx ).
i

4

Then
µ̂h (x) = eT1 (XxT Wx Xx )−1 Xx Wx Y =

X

`i (x; hHx )Yi

i

where



T

1 (X1 − x)

..
 ..
Xx =  .
.

1 (Xn − x)T




,


Wx is a diagonal matrix with Wx (i, i) = K(Xi − x; hHx ), e1 = (1, 0, . . . , 0)T and
`(x; hHx ) = eT1 (XxT Wx Xx )−1 Xx Wx .

(2)

When h = 1, which can be regarded as a default value, we write µ̂h (x) simply as µ̂(x).
Although we focus on local linear regression, one can also use this for kernel regression or
higher order local polynomial regression. We shall see that µ̂(x) is often a good approximation to µ̂RF (x).
Remark: Other approaches for choosing Hx are possible. For example, one could
minimize the difference between K(x − Xi ; Hx ) and wi (x) over all positive definite matrices
Hx . However, (1) is simple and in our experience works quite well. In high dimensional
cases, Hx would require regularization but we do not pursue the high dimensional case in
this paper.
Figure 1 shows a one-dimensional example. The top left shows the data, the random
forest estimator µ̂RF in lack, and the true function in red. The forest guided smoother µ̂(x)
is the black line in the top right plot. The bottom left shows the weights w1 (x), . . . , wn (x)
at x = 0 and the the bottom right shows our kernel approximation to the weights. We see
that the FGS approximates the forest and the kernel approximates the weights very well.
5

3
−3

−1

0

Y

1

2

3
2
1
0

Y

−1
−3
−3

−2

−1

0

1

2

3

−3

−2

−1

X

−3

−2

−1

0

0

1

2

3

1

2

3

X

1

2

3

−3

−2

−1

0

Figure 1: Top left: The data points, the random forest estimator µ̂RF (x) in black, and the
true function in red. Top right: forest guided smoother µ̂(x) in black, and the true function
in red. Bottom left: forest weights w1 (x), . . . , wn (x) evaluated at x = 0. Bottom right: kernel
approximation to the weights.

Figure 2 shows a two-dimensional example. Here we show the forest weights as gray circles
and the ellipse represents the approximating kernel. The target point is (0,0). Again, the
kernel approximates the weights.
For getting standard errors and confidence intervals, we will also need to estimate the
variance
σ 2 (x) = Var(Y |X = x).
We will proceed as follows. Let ri = Yi − µ̂RF (Xi ) be the residuals from the forest. We
regress the ri2 ’s on Xi ’s to estimate σ 2 (x) using another random forest. We find that this
approach tends to under-estimate σ 2 (x) in some cases and we replace σ̂(x) with c σ̂(x)
6

0.3
0.2
0.1
0.0
−0.1
−0.2
−0.3
−0.3

−0.2

−0.1

0.0

0.1

0.2

0.3

Figure 2: The dots represent data point. The target point x is indicated by the red square. The
gray circles show the forest weights and the blue ellipse represents the kernel approximation to
the weights.

where we use c = 1.5 as a default to compensate for this in our examples.

3

Confidence Intervals

In this section we construct estimators of the bias of µ̂(x) and then obtain confidence intervals for µ(x). This is difficult to do directly from the forest without delicate modifications
of the forest algorithm to undersmooth, as in Friedberg et al. (2020). But bias estimation using standard methods is possible with the FGS. We start by recalling some basic
properties of local linear smoothers.

3.1

Properties of Smoothers

Let µ̂ be the local linear smoother based on bandwidth matrices Hx ≡ Hn,x . Let f (x) be
R
R
the density of X, define µ2 (K) by uuT K(u)du = µ2 (K)I, and R(K) = K 2 (u)du. Let
7

Hess be the Hessian of µ. Ruppert and Wand (1994) consider the following assumptions.
(A1) K is compactly supported and bounded. All odd moments of K vanish.
(A2) σ 2 (x) is continuous at x and f is continuously differentiable. Also, the second
order derivatives of µ are continuous. Further, f (x) > 0 and σ 2 (x) > 0.
(A3) Hn,x is symmetric and positive definite. As n → ∞ we have n−1 |Hn,x | → 0 and
Hn,x (i, j) → 0 for every i and j.
(A4) There exists cλ such that
λmax (Hn,x )
≤ cλ
λmin (Hn,x )
for all n where λmax and λmin denote the maximum and minimum eigenvalues.
Under these conditions, Ruppert and Wand (1994) showed that the bias B(x, Hx ) and
variance V (x, Hx ) of µ̂(x), conditional on X1 , . . . , Xn are
1
B(x, Hx ) = µ2 (K)tr(Hx2 Hess(x)) + oP (tr(Hx2 ))
2

(3)

and
V (x, Hx ) =

σ 2 (x)R(K)
(1 + oP (1)).
n|Hx |f (x)

(4)

It follows that the bias using bandwidth hHx satisfies
B(x, hHx ) = h2 cn (x) + oP (h2 tr(Hx2 ))
for some cn (x).
Assumptions (A3) and (A4) capture the idea that the bandwidth matrix needs to shrink
towards 0 in some sense. Assumption (A4) essentially says that Hn,x behaves like a scalar
8

tending to 0 times a fixed positive definite matrix. For our results, we will make this more
explicit and slightly strengthen (A4) to:
(A4) There exists a sequence φn → 0 and a positive definite symmetric matrix Cx such
that Hn,x ∼ φn C(x) where φn  (1/n)a for some 0 < a < 1.
With (A4) we have B(x, hHx ) = h2 c(x)/n2 + oP (h2 ). To construct the bias correction
we need to add the following stronger smoothness condition.
(A5) For some t, the tth order derivatives of µ are continuous and there exist functions
c1 (x), . . . , ct (x) such that, for any h > 0,
B(x, hHx ) =

t
X
cj (x)hj
j=2

naj


+ oP

1
nat


.

Ruppert (1997) showed how to estimate the bias of µ̂(x) by fitting the estimator for
several values of the bandwidth. This type of bias estimation has been used in other
contexts and is sometimes referred to as generalized jackknife; see for example Cattaneo
et al. (2013).
In more detail, Ruppert’s method (i.e. the generalized jackknife) works as follows.
Choose a set of b bandwidths h1 , h2 , . . . , hb and let m̂ = (µ̂h1 (x), . . . , µ̂hb (x)). Let κn =
(µ(x), κ2,n (x), . . . , κt,n (x))T where κj,n (x) = cj (x)/nja . Let


2
3
t
1 h1 h1 . . . h1






2
3
t
1 h2 h2 . . . h2 
H=
.
 ..
..
..
.. 
.
.
.
. 


2
4
t
1 hb hb . . . hb
We estimate κn by least squares, namely,
κ̂n = argminc ||m̂ − H c||2 = (HT H)−1 HT m̂.
9

Now m̂ = LY where


`1 (x; h1 Hx ) `2 (x; h1 Hx ) . . . `n (x; h1 Hx )








L = `1 (x; h2 Hx ) `2 (x; h2 Hx ) . . . `n (x; h2 Hx )


..
..
..


.
.
.


`1 (x; hb Hx ) `2 (x; hb Hx ) . . . `n (x; hb Hx )
where `i (x; hj Hx ) are the elements of the vector `(x; hj Hx ) defined in (2). Therefore
κ̂n = (HT H)−1 HT L Y.
We estimate the bias of µ̂h (x) by
B̂(x, h) =

t
X

κ̂j,n (x)hj = g T (HT H)−1 HT L Y

j=2

where g = (0, h2 , . . . , ht )T . The de-biased estimator is the first element of κ̂n , that is,
µ† (x) = eT1 (HT H)−1 HT LY =

X

`˜i (x)

i

˜ = eT (HT H)−1 HT L.
where `(x)
1
The variance of µ† (x) (conditional on the Xi ’s) is
Var[µ† (x)] =

X

`˜2i (x)σ 2 (Xi )

i

and the estimated variance is
s2 (x) =

X

`˜2i (x)σ̂ 2 (Xi ).

i

Ruppert used the bias estimation method as part of a bandwidth selection method. We
are interested, instead, to get a centered central limit theorem. We now confirm that this
10

indeed works. For the theory, we need to be more specific about the choice of bandwidths
in the bias correction procedure. Specifically, let hj = αj n−γ , for j = 1, 2, . . . b, with
0 < α1 < . . . < αb being constants not depending on n.
Theorem 1 Assume that, conditional on D1 , assumptions (A1)-(A5) hold and:
P

(i)

supx |σ̂ 2 (x) − σ 2 (x)| → 0,

(ii)

−a < γ <

1−ad
.
d

Further, if t < d/2 we require a < 1/(d − 2t). Also, assume that Y is bounded and that
b > t + 1. Then
µ† (x) − µ(x)
s(x)

N (0, 1).

Hence,
P(µ(x) ∈ Cn (x)) → 1 − α
where Cn (x) = µ† (x) ± zα/2 s(x).
The proof is in the appendix.
It is important to note that γ can be 0 or even negative. Ruppert (1997) requires γ > 0.
The difference is that our bandwidth is of the form hHx and Hx is already tending to 0
and we only need the product to go to 0. This significantly simplifies the choice of grid of
bandwidths because the bandwidths can be constant order and don’t need to change with
n. For example, one could use a grid like (1/8, 1/4, 1/2, 1, 2, 4, 8). We recommend including
h = 1 in the grid as this corresponds to the original FGS.

11

3.2

Variability Intervals

A commonly used alternative to confidence intervals for nonparametric regression is to
form some sort of interval around the estimate that informally represents uncertainty but
without the coverage claim of a confidence interval. We will refer to these as variability
intervals. The simplest approach is to use µ̂(x) ± cα s(x) where s2 (x) is the estimated
variance of µ̂(x). If µ̂(x) satisfies a central limit theorem and cα = zα/2 then this is a
confidence interval for E[µ̂(x)].
In our case, such a variability interval is simply Cn (x) = µ̂(x) ± zα/2 s(x). The extra
parameter h is not needed since we do not use the generalized jackknife to reduce the bias.
However, it might be useful to construct multiresolution variability intervals at various
resolutions h. This is the approach to inference recommended by Chaudhuri and Marron
(2000) who refer to this as scale-space inference.
In Section 5 we illustrate this multiresolution approach for estimating the gradient as
a measure of variable importance.

3.3

Discussion of Other Methods

Variability intervals for forests have been obtained in Mentch and Hooker (2016); Peng
et al. (2019) by deriving a U-statistic based central limit theorem. Wager et al. (2014)
estimate the variance of the forest using the jackknife. The advantage of these approaches
is that they do not need to use sample splitting as we do.
Confidence intervals were obtained by Athey et al. (2019) and Friedberg et al. (2020).
They also use data splitting. The main difference is that we leave the forest algorithm
untouched and we use the generalized jackknife to reduce the bias. Instead, they modify

12

the construction of the forest and require that the forest is constructed to satisfy certain
assumptions; specifically they require that the forest is built from subsamples of size s  nβ
where
βmin



d log(1/α)
< β < 1,
=1− 1+
π log 1/(1 − α)

where π/d is a lower bound on the probability of splitting on a feature and each tree leaves
a fraction of points α on each size of every split.
The advantage of this approach is that it only requires the regression function to be
Lipschitz whereas the generalized jackknife assumes that µ(x) has at least t + 1 derivatives.
The disadvantage is that the conditions on the construction of the forest are rather complicated and non-standard and one cannot use any off-the-shelf forest. As noted in Friedberg
et al. (2020), the tuning of forest parameters in practice can be quite different than what
is assumed in the theory. Our main assumption is simply that the local smoother has
standard bias and variance properties.
Both approaches require assumptions and it is difficult to say that one set of assumptions
is better than the other as they are quite incomparable. One is an assumption about the
algorithm and the other is an assumption about the function and the bandwidth.
Remark: It may be the case that there are irrelevant variables. That is, we have have
that µ(x) = µ(xS ) for some subset of variables xS . If the forest is able to discover the
relevant variables, then the bandwidth matrix Hx might not shrink in the direction of the
irrelevant variables. This is a good thing but, technically, the conditions (A3-A5) may be
violated. However, the gradient and Hessian of µ(x) vanish in the irrelevant directions and
Theorem 1 still holds.

13

3.4

Examples of Confidence Intervals

Now we consider some examples. In each case, we use t = 2. The results using t = 3 and
t = 4 are similar.
Figure 3 shows three, one dimensional examples. The plots on the left show the true
functions in black and the average over 100 simulations of the pointwise 95 percent confidence bands in red. The plots on the right show the coverage (estimated by simulation) as a
function of x. The sample size in each case is n = 1, 000. The functions are µ(x) = sin(4x),
p
µ(x) = I(x > 1/2) − 1/2 and µ(x) = x(1 − x) sin(2.1π/(x + .35)). The data were generated as Yi = µ(Xi ) + σi where i ∼ N (0, 1), σ = .1, .03, .03 in the three examples and
Xi ∼ Unif(0, 1). We take the grid of bandwidths h1 , . . . , hb to be an equally spaced grid of
size 20 from h = .1 to h = 2. In each case the coverage reaches its nominal value.
Next we consider some multivariate examples. The first is from Friedman and Roosen
(1995) and is Yi = µ(Xi ) + σ where
µ(x) = 10 sin(πx1 x2 ) + 20(x3 − 0.5)2 + 10x4 + 5x5

(5)

with n = 500, σ = 1 and Xi is uniform on [0, 1]5 . We take h to be in an equally spaced grid
of size 20 from h = 1 to h = 5. We construct 90 percent confidence intervals at 10 randomly
selected points. The second example is from Friedberg et al. (2020) and is Yi = µ(Xi ) + σ
where now
µ(x) =

10
5
+
1 + exp(−10(x1 − .5)) 1 + exp(−10(x2 − .5))

(6)

with n = 500, σ = 5 and Xi is uniform on [0, 1]5 . We take h to be in an equally spaced
grid of size 20 from h = 1 to h = 30. We construct 90 percent confidence intervals at 10
randomly selected points.
Table 1 and 2 show coverage and average length of confidence intervals at 10 randomly
14

0.95

0.8
0.0

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.95

0.8
0.0

0.4

0.0

Coverage

0.4

0.0

Coverage

0.0

0.95

0.8

0.8

0.6

0.4

0.4

0.0 0.2 0.4 0.6

0.2

−0.4

µ(x)

0.4

Coverage

1.0
0.0
−1.0

µ(x)
µ(x)

0.0

h

h

Figure 3: The figure shows three one-dimensional examples. The black lines on the left plots show
the true function. The red lines on the left plots show the confidence bands from one simulation.
The right plots show the estimated coverage at each x based on 100 simulations.

15

Table 1: Coverage and average length of confidence intervals for the function in (5)
1

2

3

4

5

6

7

8

9

10

Coverage

0.87

0.84

0.84

0.92

0.88

0.88

0.85

0.90

0.85

0.86

Length

4.66

4.47

4.53

4.72

4.53

4.84

4.72

4.86

4.49

4.59

Table 2: Coverage and average length of confidence intervals for the function in (6)
1

2

3

4

5

6

7

8

9

10

Coverage

0.93

0.84

0.91

0.91

0.92

0.93

0.85

0.92

0.87

0.94

Length

9.63

9.62

9.61

10.14

9.39

9.98

10.75

9.59

10.68

8.95

chosen points for the functions in (5) and (6). The coverage is close to the nominal value
and the lengths are close to those in Friedman and Roosen (1995) and Friedberg et al.
(2020).
Remark. Our grids were chosen to achieve good coverage and length for the examples.
In practice we suggest a grid ranging from h = .1 to h = 10. While this choice cannot be
claimed to be optimal, and may not eliminate the bias, it should result in some amount
of bias reduction. As pointed out in the discussion of Cattaneo et al. (2013), finding an
optimal grid for the generalized jackknife is an unsolved problem.

16

4

Exploring the Forest

In this section we show how the forest guided smoother can be used to examine properties
of the forest.
A random forest is a complex object and is difficult to interpret. In contrast, the FGS
is completely determined by the set of bandwidth matrices Ξ = {Hx : x ∈ Rd } which is a
subset of the manifold of symmetric positive-definite matrices. We now consider a variety of
methods for summarizing and exploring the set Ξ. In this section we describe the methods.
Examples are given in Section 5.

4.1

Summarizing the Spatial Adaptivity of the Kernels

Here we show how to quantify the degree to which Hx varies with x. We take K to be a
multivariate Gaussian. The kernel at x is K(x, Hx ). First we define what the kernel looks
like on average over x. To do this we find the Wasserstein barycenter of the distributions
{K(0, Hx )}.
The Wasserstein barycenter comes from the theory of optimal transport; a good reference on this area is Peyré and Cuturi (2019). Recall first that the (second order) Wasserstein
distance between two distributions P1 and P2
W22 (P1 , P2 ) = inf EJ [||X − Y ||2 ]
J

where X ∼ P1 , Y ∼ P2 and the infimum is over all joint distributions J with marginals P1
and P2 . In the special case of Normals, where P1 = N (µ1 , Σ1 ) and P2 = N (µ2 , Σ2 ) we have
(
)
1/2

1/2

W 2 (P1 , P2 ) = ||µ1 − µ2 ||2 + tr(Σ1 ) + tr(Σ2 ) − 2tr (Σ1 Σ2 Σ1 )1/2 .

17

The Wasserstein barycenter of a set of distributions Qx indexed by x is the distribution Q
that minimizes
Z

W 2 (Qx , Q)dPX (x).

This barycenter is useful because it preserves the shape of the distributions. For example,
the barycenter of a N (µ1 , 1) and N (µ2 , 1) is N ((µ1 + µ2 )/2, 1). The Euclidean average is
the mixture (1/2)N (µ1 , 1)+(1/2)N (µ2 , 1) which does not preserve the shape of the original
densities.
In our case, we summarize the set of bandwidth matrices by finding the barycenter
of the set of distributions {K(0, HXi )}. The barycenter in this case can be shown to be
K(0, H) were H is the unique positive definite matrix such that
Z
1/2
1/2
H = (H Hx H )1/2 dPX (x).

(7)

In our examples, we will compute H to see what a typical bandwidth matrix looks like.
We also compute the Frechet variance
Z
V = W 2 (H, Hx )dPX (x)
which gives a sense of how much the bandwidth matrices vary over x. If Hx does not vary
with x then then V = 0.
Next we consider another way to summarize the FGS. For each Xi , we find the effective
bandwidth with respect to each covariate by finding the length of the ellipse {x : (u −
−1
Xi )T HX
(u − Xi ) ≤ c2 } in the direction of each coordinate axis, for any c > 0. In other
i
q
−1
words, we compute ∆j (Xi ) = c2 /HX
(j, j). In the example section we’ll see that plots
i

of these quantities can be very informative.

18

4.2

Comparing the Forest and the Smoother

How much prediction accuracy is lost by using the smoother instead of the forest? To
answer this question we define
Γ = E[(Y − µ̂(X))2 − (Y − µ̂RF (X))2 ].
We can get an estimate of Γ using the approach in Williamson et al. (2020).
Split the data into four groups D1 , D2 , D3 , D4 each of size m ≈ n/4. From D1 get µ̂RF
and from D2 get µ̂. Let
Γ̂ =

1 X
1 X
ri −
si
m i∈D
m i∈D
3

4

where
ri = (Yi − µ̂RF (Xi ))2 ,

si = (Yi − µ̂(Xi ))2 .

Then, Williamson et al. (2020) show that
√

m(Γ̂ − Γ)

N (0, τ 2 )

P
P
and a consistent estimate of τ 2 is m−1 ( i (ri − r)2 + i (si − s)2 ). Hence, a 1 − α confidence
√
interval for Γ is Γ̂±zα/2 τ̂ / m. (One can repeat this by permuting the blocks and averaging
if desired.)

4.3

Multiresolution Local Variable Importance

One popular method of assessing local variable importance is to estimate the gradient of
µ̂ or, equivalently, to use local linear approximations Ribeiro et al. (2016); Plumb et al.
(2018). Using the forest guided local linear smoother we get an estimate of the gradient and

19

its standard error for free. Furthermore, we can do this at various resolutions by varying
h.
Let β̂h (x) = (β̂h,1 (x), . . . , β̂h,d (x)). Now
β̂h,j (x) =

X

Yi `ij (x; hHx )

i

where `ij (x; hHx ) is the ith element of the vector
eTj+1 (XxT Wx Xx )−1 Xx Wx ,
where Wx is a diagonal matrix with Wx (i, i) = K(Xi − x; hHx ) and ej+1 is the vector that
is all 0 except it is 1 in the j + 1 position. The standard error of β̂j,h (x) is sej,h (x) =
qP
2
2
i σ̂ (Xi )`ij (x; hHx ). A 1 − α variability interval is β̂j,h (x) ± zα/2 sej,h (x).
A plot of the values β̂h,j (Xi ) gives a global sense of the local importance of the j th
covariate. A plot of β̂h,j (x) as a function of h for a fixed x summarizes local variable
importance at various resolutions.

5

Examples

In this section, we illustrate the methods from the previous section on two examples. The
first is a synthetic example and the second is a data example.

5.1

Synthetic Example

We return to the example given in (6). Figure 4 shows the Wasserstein barycenter of the
bandwidth matrices. The barycenter shows that the typical bandwidth for the first two
variables is small. This makes sense as the function only depends on x1 and x2 . Also, the
small off-diagonals suggest the bandwidth matrix is typically not far from diagonal.
20

1

+0.100
+0.085

2

+0.070

+0.040

3

Row

+0.055

+0.025
4

+0.010
−0.005

5

−0.020

1

2

3

4

5

Column

Figure 4: Barycenter of the bandwidth matrices for the example in equation (6).
The Frechet variance is 0.019, suggesting that the bandwidth matrix does not vary
greatly across the sample space.
Figure 5 shows violin plots of effective bandwidths. The effective bandwidth ∆j is
smaller for x1 and x2 than for the other variables. This is what we would expect since
µ(x) does not depend on x3 , x4 or x5 . The forest attempts to smooth over these irrelevant
variables and hence the approximating bandwidth matrices tend to be large in the directions
of the irrelevant variables. This confirms what we found with the barycenter.
The four plots in Figure 6 show the local slopes β̂1 (Xi ), . . . , β̂5 (Xi ) for each of the five
covariates (over all Xi ) at four different resolutions. and variable importance (bottom) at
several resolutions h = 0.1, 0.5, 1 and 2. The two smallest resolutions (h = 0.1, h = 0.5)
are uninformative. The two larger resolutions (h = 1, h = 2) provide clear evidence of the

21

Covariates

1

2

3

4

5

0.5

Bandwidth

0.4

0.3

0.2

0.1
1

2

3

4

5

Covariates

Figure 5: Example (6). Effective bandwidths for each covariate.
importance of x1 and x2 . Note that importance variables correspond to small bandwidths
but large slopes.
Figure 7 shows variability intervals for β̂1,h (x), · · · , β̂4,h (x) at
x = (1/2, 1/2, 1/2, 1/2, 1/2), the center of the support of X. (The fifth variable is not
shown.) These intervals are plotted versus increasing values of h resulting in (pointwise)
variability bands for βj,h (x).
Again, we see that x1 and x2 are the important variables as the bands exclude 0 for
larger values of h while the bands for x3 and x4 include 0 for all h.
Next we compare the FGS to the forest. The top left plot of Figure 8 shows histograms
of the squared residuals for the forest and of the FGS. The two histograms are very similar.
It also shows two scatterplots of µ̂RF (Xi ) and eµ̂(Xi ). and of their residuals.

22

Covariates

1

2

3

4

Covariates

5

1

2

3

4

5

100

25

50

β

β

0

0
−25
−50
−50

−100
1

2

3

Covariates −−
Covariates

1

2

4

5

1

2

h=0.1
3

3

4

5

Covariates −− h=0.5
4

Covariates

5

1

2

3

4

5

15

20

β

β

10
10

5
0
0
1

2

3

4

5

1

Covariates −− h=1

2

3

Covariates −−

4

5

h=2

0

−5

10

0

β

20

β

5

30

10

40

Figure 6: {β̂j (X1 ), . . . , β̂j (Xn )} for each covariate at four resolutions, h = 0.1, 0.5, 1, 2.

0.5

1.0

1.5

2.0

0.5

1.0

1.5

2.0

1.5

2.0

h

0

β
−12

−5

−8

β

−4

5

0 2

h

0.5

1.0

1.5

2.0

0.5

h

1.0
h

Figure 7: Variability intervals for β̂1,h (x), · · · , β̂4,h (x) at x = (1/2, 1/2, 1/2, 1/2, 1/2).

23

150

150

100

100

0

50

50
0
0

50

100

150

200

250

0

100

150

200

250

Squared Residuals From FGS

0

0

1

5

2

FGS

FGS

3

10

4

5

15

Squared Residuals From RF

50

0

5

10

15

0

Forest

1

2

3

4

5

Forest

Figure 8: Top left: squared residuals from the random forest. Top right: squared residuals from
the FGS. Bottom left: Plot of µ̂RF (Xi ) versus µ̂(Xi ). Bottom right: Plot of |Yi − µ̂RF (Xi )| versus
|Yi − µ̂(Xi )|.

We do see a very slight loss in accuracy for the FGS but the difference is small. It
appears that the two fits are very similar. To formalize this, we estimate Γ as described
in Section 4.2 and we find that the 95 per cent confidence interval Γ̂ = −0.134 ± 4.9 again
suggesting little difference between the two methods. Thus we conclude that the FGS
appears to be a good approximation to the forest.

24

5.2

Covid-19

In this section we consider data on Covid-19 obtained from the API of the CMU Delphi
group at covidcast.cmu.edu.
Our goal is to construct a random forest to predict Y = average daily deaths from these
variables:
cli
Percentage of people with Covid-like symptoms (surveys of Facebook users)
dr
Percentage of daily doctor visits that are due to Covid-like symptoms
cases
Newly reported Covid-19 cases per 100,000 people
home
Proportion of people staying home
masks
Percentage of people who say they wear a mask in public
hospital Percentage of daily hospital admissions with Covid-19
prevdeaths Previous number of deaths due to Covid-19
The variable Y is averaged over December 1 2020 to December 12 2020. The covariates
are averaged from October 1 2020 to December 1 2020. We took the logarithms of all
variables and then scaled each covariate to have mean 0 and variance 1.
The problem of predicting the epidemic is an intensely studied issue and our goal is not
to develop a cutting edge prediction method. Rather, we use these data as a vehicle for
illustrating our methods.
After fitting the FGS we can summarize the local fit for various counties by reporting
the local slopes β̂1 (x), . . . , β̂d (x) and their standard errors. Table 1 and Table 2, below
show this for four counties. The nice thing about the FGS is that we can describe the
model for any county in the familiar form of a (local) linear model. This makes the model
very interpretable for users such as public health officials.

25

New York County, NY

Elkhart County, IN

Coefficients

β̂

Standard Error

Coefficients

β̂

Standard Error

cli
dr
cases
home
masks
hospital
prevdeaths

-0.026
-0.042
-0.012
0.148
-0.016
0.003
0.128

0.057
0.034
0.044
0.056
0.042
0.058
0.082

cli
dr
cases
home
masks
hospital
prevdeaths

0.015
0.155
-0.150
0.204
0.124
-0.158
0.141

0.119
0.148
0.106
0.159
0.118
0.098
0.087

Table 3
DuPage County, IL

Lubbock County, TX

Coefficients

β̂

Standard Error

Coefficients

β̂

Standard Error

cli
dr
cases
home
masks
hospital
prevdeaths

0.62
0.032
-0.029
-0.006
0.013
-0.061
0.211

0.077
0.049
0.063
0.089
0.083
0.100
0.069

cli
dr
cases
home
masks
hospital
prevdeaths

0.043
0.110
-0.066
0.105
0.063
-0.065
0.107

0.094
0.097
0.080
0.108
0.072
0.061
0.074

Table 4
Figure 9 shows the effective bandwidths and local slopes at resolution h = 2. The
two most important variables (small bandwidths and large slopes) are x4 (home) and x7
(previous deaths). The importance of previous deaths is obvious. The fact that social
mobility (home) is important is notable but we should emphasize that this is a predictive
analysis not a causal analysis.

26

Covariates

1

3

5

2

4

6

7

Bandwidth

3

2

1

1

2

3

4

5

6

7

Covariates

Covariates

1

3

5

2

4

6

7

0.2

Beta

0.1

0.0

−0.1

1

2

3

4

5

6

7

Covariates

Figure 9: Bandwidth plot (top) and β plot (bottom) for Covid example

27

1

+1.00

2

+0.85

3

+0.70

+0.40

4

Row

+0.55

5

+0.25

6

+0.10
−0.05
7

−0.20

1

2

3

4

5

6

7

Column

Figure 10: Barycenter of the bandwidth matrices for Covid example.
Figure 10 shows the barycenter of the bandwidth matrices. Note that the fourth and
seventh elements on the diagonal are the smallest confirming the importance of those
variables. We also see some correlation between the bandwidths for x5 and x6 . The Frechet
variance is 0.817 suggesting that Hx varies quite a bit with x (recall that all the variables
are scaled to have variance 1).

6

Discussion

Throughout this paper we have assumed that the number of covariates d is fixed. If d
increases with n then local linear fitting will not work. Instead one will need to include
28

some sort of ridge or `1 penalty. Furthermore, when d is large, Hx will not be invertible
and so regularization on Hx is required.
We have focused on random forests but similar ideas can be used for other black box
methods such as neural nets. Koh and Liang (2017) show how to compute the influence
function for deep nets and other predictors. The influence function can be used to define
a spatially adaptive kernel as we have done using the weights from a forest.
In our examples we have not found much difference between the forest and the FGS.
But this may be due to the fact that we have not considered complex high dimensional
problems. Understanding when a complex predictor can be approximated by a spatially
varying local smoother is a interesting but challenging problem.

References
Susan Athey, Julie Tibshirani, Stefan Wager, et al. Generalized random forests. The Annals
of Statistics, 47(2):1148–1178, 2019.
Adam Bloniarz, Ameet Talwalkar, Bin Yu, and Christopher Wu. Supervised neighborhoods
for distributed nonparametric regression. In Artificial Intelligence and Statistics, pages
1450–1459, 2016.
Leo Breiman. Randomizing outputs to increase prediction accuracy. Machine Learning, 40
(3):229–242, 2000.
Matias D Cattaneo, Richard K Crump, and Michael Jansson. Generalized jackknife estimators of weighted average derivatives. Journal of the American Statistical Association,
108(504):1243–1256, 2013.
29

Probal Chaudhuri and James Steven Marron. Scale space view of curve estimation. Annals
of Statistics, pages 408–428, 2000.
Rina Friedberg, Julie Tibshirani, Susan Athey, and Stefan Wager. Local linear forests.
Journal of Computational and Graphical Statistics, pages 1–15, 2020.
Jerome H Friedman and Charles B Roosen. An introduction to multivariate adaptive
regression splines, 1995.
Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine
learning, 63(1):3–42, 2006.
Torsten Hothorn, Berthold Lausen, Axel Benner, and Martin Radespiel-Tröger. Bagging
survival trees. Statistics in medicine, 23(1):77–91, 2004.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. arXiv preprint arXiv:1703.04730, 2017.
Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors. Journal of the
American Statistical Association, 101(474):578–590, 2006.
Nicolai Meinshausen. Quantile regression forests. Journal of Machine Learning Research,
7(Jun):983–999, 2006.
Lucas Mentch and Giles Hooker. Quantifying uncertainty in random forests via confidence
intervals and hypothesis tests. The Journal of Machine Learning Research, 17(1):841–
881, 2016.

30

Wei Peng, Tim Coleman, and Lucas Mentch. Asymptotic distributions and rates of convergence for random forests via generalized u-statistics. arXiv preprint arXiv:1905.10651,
2019.
Gregory Plumb, Denali Molitor, and Ameet S Talwalkar. Model agnostic supervised local
explanations. In Advances in Neural Information Processing Systems, pages 2515–2524,
2018.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?”
explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD
international conference on knowledge discovery and data mining, pages 1135–1144, 2016.
David Ruppert. Empirical-bias bandwidths for local polynomial nonparametric regression
and density estimation. Journal of the American Statistical Association, 92(439):1049–
1062, 1997.
David Ruppert and Matthew P Wand. Multivariate locally weighted least squares regression. The annals of statistics, pages 1346–1370, 1994.
Erwan Scornet. Random forests and kernel methods. IEEE Transactions on Information
Theory, 62(3):1485–1500, 2016.
Stefan Wager, Trevor Hastie, and Bradley Efron. Confidence intervals for random forests:
The jackknife and the infinitesimal jackknife. The Journal of Machine Learning Research,
15(1):1625–1651, 2014.
Brian D Williamson, Peter B Gilbert, Noah R Simon, and Marco Carone.

A uni-

fied approach for inference on algorithm-agnostic variable importance. arXiv preprint
arXiv:2004.03683, 2020.
31

Appendix
Here we recall Theorem 1 and give an outline of the proof.
P

Theorem 3.1 Assume that (i) supx |σ̂ 2 (x) − σ 2 (x)| → 0, (ii) σ 2 (x) > 0,
−a<γ <

(iii)

1 − ad
d

and further, if t < d/2 we require a < 1/(d − 2t). Also, assume that Y is bounded and that
b > t + 1. Then
µ† (x) − µ(x)
q
ˆ † (x)]
Var[µ

N (0, 1).

Proof Outline. First note that the condition γ > −a ensures that n|hHx | → ∞ and
this implies Var[µ† (x)] → 0. We write
µ† (x) − E[µ† (x)] E[µ† (x)] − µ(x)
µ† (x) − µ(x)
p
= p
+ p
.
Var(µ† (x))
Var(µ† (x))
Var(µ† (x))
Recall that µ† (x) = eT1 (HT H)−1 HT m̂ where m̂ = (µ̂(x; h1 Hx ), . . . , µ̂(x; hb Hx ))T . Now
E[m̂] = Hκn (x) + o(n−at ) where we recall that


κn (x) = µ(x),

t
X

cj (x)hj1 /naj , . . . ,

j=2

t
X

cj (x)hjb /nja

T

.

j=2

Hence,
E[µ† (x) − µ(x)] = eT1 (HT H)−1 HT [Hκn (x) + o(n−at )] = o(n−at ).
Let V be the covariance matrix of m̂. Then, arguing as in the proof of Theorem 2.1 of
Ruppert and Wand (1994), there exists a b × b positive definite matrix A depending on K,
α1 , . . . , αb , x, f (x) and σ 2 (x) but not on n, such that
V=

A
(1 + oP (1)).
n|h1 Hx |
32

Hence,
Var[µ† (x)] = eT1 (HT H)−1 HT VH(HT H)−1 e1
1
eT1 (HT H)−1 e1 (1 + oP (1))
n|h1 Hx |
1
(1 + oP (1))
=
n|h1 Hx |

=

= OP (n1−d(a+γ) )
since eT1 (HT H)−1 e1 = O(1). Since γ < (1 − ad)/d ≤ min{(1 − ad)/d, (2at − da + 1)/d} it
follows that
E[µ† (x)] − µ(x)
p
= oP (1).
Var(µ† (x))
Now E[|µ† (x) − E[µ† (x)]|3 = O(n2d(a+γ)−2 ). Hence
E[|µ† (x) − E[µ† (x)]|3 ]
= O(n2d(a+γ)−2 n(3/2)(1−d(a+γ)) ) = oP (1)
Var[µ† (x)]3/2
since γ < (1 − ad)/d. Hence, by Lyapunov’s central limit theorem,
µ† (x) − E[µ† (x)]
p
Var(µ† (x))

N (0, 1).

P
P
ˆ † (x)]/Var[µ† (x)] →
Finally, since supx |σ̂ 2 (x)−σ 2 (x)| → 0 and σ 2 (x) > 0, it follows that Var[µ

1 and the result follows.

33

