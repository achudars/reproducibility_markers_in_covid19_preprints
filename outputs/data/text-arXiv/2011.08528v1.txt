D ECISION AND F EATURE L EVEL F USION OF D EEP F EATURES
E XTRACTED FROM P UBLIC COVID-19 DATA - SETS

arXiv:2011.08528v1 [eess.IV] 17 Nov 2020

A P REPRINT
Hamza Osman Ilhan
Department of Computer Science
Yildiz Technical University
Istanbul, Turkey
hoilhan@yildiz.edu.tr

Gorkem Serbes
Department of Biomedical Engineering
Yildiz Technical University
Istanbul, Turkey
gserbes@yildiz.edu.tr

Nizamettin Aydin
Department of Computer Science
Yildiz Technical University
Istanbul, Turkey
naydin@yildiz.edu.tr

November 18, 2020

A BSTRACT
The Coronavirus disease (COVID-19), which is an infectious pulmonary disorder, has affected
millions of people and has been declared as a global pandemic by the WHO. Due to highly contagious nature of COVID-19 and its high possibility of causing severe conditions in the patients, the
development of rapid and accurate diagnostic tools have gained importance. The real-time reverse
transcription-polymerize chain reaction (RT-PCR) is used to detect the presence of Coronavirus RNA
by using the mucus and saliva mixture samples taken by the nasopharyngeal swab technique. But,
RT-PCR suffers from having low-sensitivity especially in the early stage. Therefore, the usage of
chest radiography has been increasing in the early diagnosis of COVID-19 due to its fast imaging
speed, significantly low cost and low dosage exposure of radiation. In our study, a computer-aided
diagnosis system for X-ray images based on convolutional neural networks (CNNs) and ensemble
learning idea, which can be used by radiologists as a supporting tool in COVID-19 detection, has
been proposed. Deep feature sets extracted by using seven CNN architectures were concatenated for
feature level fusion and fed to multiple classifiers in terms of decision level fusion idea with the aim
of discriminating COVID-19, pneumonia and no-finding classes. In the decision level fusion idea, a
majority voting scheme was applied to the resultant decisions of classifiers. The obtained accuracy
values and confusion matrix based evaluation criteria were presented for three progressively created
data-sets. The aspects of the proposed method that are superior to existing COVID-19 detection
studies have been discussed and the fusion performance of proposed approach was validated visually
by using Class Activation Mapping technique. The experimental results show that the proposed
approach has attained high COVID-19 detection performance that was proven by its comparable
accuracy and superior precision/recall values with the existing studies.
Keywords COVID-19 · Convolutional Neural Networks · Support Vector Machines · Feature Level Fusion · Decision
Level Fusion · Ensemble Learning · Class Activation Mapping · Transfer Learning · Multistage Learning

1

Introduction

The coronavirus disease 2019 (COVID-19) is a respiratory disorder, which may have varying severity respiratory
symptoms from the common cold to fatal pneumonia. COVID-19 is caused by a novel coronavirus known as the severe
acute respiratory syndrome coronavirus 2 (SARS-CoV2). The first cases of the COVID-19, which were reported as
pneumonia patients with unknown cause, were originated from Wuhan, Hubei province of China on December, 2019.
The COVID-19 has spread to most of the provinces of China in 30 days [1] and the condition has been declared as a
global pandemic by the World Health Organization (WHO) in a very short time [2, 3]. It was thought that the epicenter
of the outbreak was a wholesale market selling seafood and other exotic animals, including bats, marmots, and snakes
[2]. SARS-CoV2 has very high contagious nature with a 1-14 days long incubation period. Some of the carriers may
not show any symptoms while a significant amount of the patients may have minor symptoms such as dry-cough,

A PREPRINT - N OVEMBER 18, 2020

sore throat, headache, fatigue, and sputum production. However, the virus can be fatal if the immune system of the
patient is weak [4]. The conditions seen in the severe and critical patients may be the pneumonia, acute respiratory
distress syndrome, pulmonary edema or multiple organ failure [5, 6]. In [7], it was stated that approximately 14% of the
COVID-19 patients have experienced severe conditions such as the dyspnea, while 5% of the patients were in critical
condition including respiratory failure, septic shock, or multiple organ dysfunction. Early diagnosis of the COVID-19
and the application of successful treatment is the key factor to reduce the complication and mortality in patients having
underlying medical conditions such as hypertension, diabetes, cardiovascular disease and asthma [8, 9, 7, 10].
Another important factor related with the COVID-19 is the transmission mechanism of the SARS-CoV2. The primary
propagation mechanism of the SARS-CoV2 has been identified as the spread of respiratory droplets through sneezing
and coughing, which have the potential to cover a distance up to 1.8 meters [11]. This highly contagious nature of
the SARS-CoV2 puts any person, who has a close contact history with the patient, in a very high risk. Although, the
primary source of the SARS-CoV2 transmission has been identified as the symptomatic people, asymptomatic people
can also have a possibility to be a risk factor [11]. The higher risk of getting severe COVID-19 disease for the patients
having existing medical conditions and being over age 60 years, and the high potential of fast propagation risk of
COVID-19 results in a significant need for the fast and accurate diagnosis tools.
As the most common test technique to diagnose COVID-19, the real-time reverse transcription-polymerase chain
reaction (RT-PCR) is used to detect the presence of viral RNA. In this method, a sample including a mixture of mucus
and saliva is taken by using the nasopharyngeal swab technique for being assessed for virus existence. Besides, the
specimen is recommended to be taken form the lower respiratory tract of the patient when severe respiratory ailments
are seen [12]. However, the RT-PCR suffers from having low-sensitivity especially in the early stage [13, 14] and it was
mentioned in [15] that the chest radiography has performed very well in the early diagnosis of COVID-19. Therefore, it
is believed that complementing the nucleic acid testing with chest radiography based diagnosis has promising potential
in the early detection of COVID-19 [16].
Regarding the chest radiography techniques, X-rays and Computer tomography (CT) scans are the most commonly
used imaging methods to diagnose the thoracic abnormalities. Although the CT scan can provide finer details of the
3D anatomy of human body, X-rays are more convenient to differentiate between viral and non-viral pneumonia due
to its fast imaging speed, significantly low cost and low dosage exposing of radiation [17]. Furthermore, in [18], the
most common manifestations and patterns of lung abnormality on portable chest radiography (CXR) in COVID-19
were described and it was mentioned that the CXR will likely be the most commonly utilized method for diagnosis and
follow up of COVID-19 because of the infection control issues related to patient transport to CT suites, the problems
experienced in CT room decontamination, and lack of CT availability in parts of the world. In [19], an experimental
CXR scoring system, which was tested on hospitalized patients with COVID-19 pneumonia, was presented to quantify
and monitor the severity and progression of disease. The authors found that the inter-observer agreement of the
developed system was very good and the CXR based scoring is a promising tool for predicting mortality in hospitalized
patients with SARS-CoV2 infection. In the light of the advantages of X-ray imaging over CT scan in the diagnosis and
monitoring of COVID-19, we focus on developing a X-ray imaging based automated system which has the ability of
differentiating viral pneumonia (COVID-19) from non-viral pneumonia and normal controls (No findings).
Computer-aided diagnosis (CAD) has been successfully used as a supporting tool for the diagnosis process of radiologists
since 1980s [20]. The CAD systems are mostly developed as a complementary decision making approach to the diagnosis
of physicians due to their advantages such as being reproducible and having the ability of detecting subtle changes
that cannot be observed by the visual inspection. With respect to the usage of X-ray imaging based CAD systems
in the diagnosis of thoracic diseases, the recent advances in deep learning have led to breakthrough improvements
in the discrimination of viral and non-viral pneumonia. In [21], a diagnostic tool, which is based on a deep-learning
framework for diagnosis of pediatric pneumonia using chest X-ray images, was proposed. In [22], the performance
of customized convolutional neural networks (CNNs) to differentiate between bacterial and viral pneumonia types in
pediatric CXRs was presented. Additionally, various deep learning approaches were successfully employed to diagnose
pneumonia and other pathologies in [23, 24, 25]. In order to detect COVID-19 samples by using X-rays, a deep learning
architecture, which employs depthwise convolutions with varying dilation rates to incorporate local and global features
extracted from diversified receptive fields, was presented in [26]. In [27], various deep learning models were utilized
for feature extraction and the obtained feature sets were processed using the Social Mimic optimization method. Later,
the modified deep features were given to support vector machines (SVMs) with the aim of COVID-19 detection. In
[28], a concatenated neural network, which is based on Xception and ResNet50V2 networks for classifying the chest
X-ray images into three categories of normal, pneumonia, and COVID-19, was presented in an unbalanced data-set
configuration. In [29], a patch-based CNN approach with a relatively small number of trainable parameters was given for
COVID-19 diagnosis. In this method, random patches were cropped from the X-ray images and the final classification
result was obtained by majority voting from inference results at multiple patch locations. In [30], a comparative
individual analysis of the recent deep learning models including VGG16, VGG19, DenseNet201, InceptionResNetV2,

2

A PREPRINT - N OVEMBER 18, 2020

InceptionV3, Resnet50, and MobileNetV2 was presented in the detection and classification of COVID-19. An Auxiliary
Classifier Generative Adversarial Network based model was employed in [31] for generating synthetic chest X-ray
CXR images to avoid overfitting and increase the generalization capability of employed CNNs. In [32], an end-to-end
deep learning architecture, which was an enhanced version of the Darknet-19 model, was employed for the multi-class
classification (COVID vs. No-Findings vs. Pneumonia).
Although previous studies have shed some lights on the deep learning-based diagnosis by using X-ray images and
significant improvement has been obtained, none of the previous works have been able to propose a complete solution
to the COVID-19 detection problem. Additionally, the COVID-19 outbreak is recent and the content of the public X-ray
imaging databases is still progressing. Due to this gradual increase in the number of COVID-19 images in the public
databases, a need of developing new algorithms, which have generalization capability for new COVID-19 samples,
have been raised. In this study, we propose a deep features based ensemble learning model, which uses feature and
decision level fusion, in order to satisfy the aforementioned needs in COVID-19 diagnosis. The main contributions
of this study are summarized as follows: i) The proposed learning model was applied to progressively created three
public COVID-19 databases in order to measure its generalization capability and reduce the biasing effect that can
occur in unbalanced databases. ii) The individual performance of seven powerful deep learning architectures including
the Mobilenet, VGG16, ResNet50, ResNet101, NasNet, InceptionV3 and Xception were presented. iii) The same
seven deep learning models were employed as feature extractors and the obtained individual deep features were fed to
non-linear kernel SVMs with the aim of COVID-19 detection. iv) The extracted deep features by using individual CNNs
were concatenated to form a single feature vector (feature level fusion) which was subsequently given to classifiers. v)
The decisions of the individual classifiers were combined by employing the majority voting schema (decision level
fusion). vi) The experimental results have demonstrated the effectiveness and robustness of the proposed ensemble
approach in epidemic screening by reaching high general accuracy values accompanied with high COVID-19 F1-scores,
precision and recall values. The rest of the study is organized as follows; Section 2 introduces materials and methods.
Section 3 presents the experimental results and finally, Section 4 presents the discussion and conclusion.

2

Materials and Methods

In this study, an ensemble of CNNs with the aid of decision and feature level fusion idea was proposed to solve the
classification problem in X-ray images for COVID-19, No-Findings and Pneumonia classes. For doing that three public
X-ray datasets were employed in the experiments and the generalization capability of the proposed approach has been
proven. In the ensemble of CNNs, transfer learning layout of seven deep convolutional neural network (CNN) models,
which were initially pre-trained by a huge image collection repository, the ImageNet, were utilized. The employed deep
networks, whose individual classification performance were also given, were the MobilenetV2, VGG16, ResNet50,
ResNet101, NasNet, InceptionV3 and Xception. In addition, the same seven deep networks were also employed as deep
feature extractors and the obtained deep features were fused and the resultant concatenated feature vector was fed to
non-linear kernel based SVMs to increase the discrimination performance.
2.1

Dataset Information

In our study, three databases were constructed in a progressive way to measure the classification performance and
generalization ability of the proposed approach by using the combinations of three publicly available data-sets. Firstly,
the data-set that has been already used in [32] was employed as the baseline reference database and it is named as DB1.
DB1 consists of 126 COVID-19 images, 500 pneumonia images and 500 normal (no-finding) images. The COVID-19
images of DB1 were taken from a public data-set, which is constantly updated by researchers [33]. The remaining 1000
non-COVID X-ray images were taken from the public ChestX-ray8 dataset [34] and the DB1 was finalized with 1126
X-ray images. Secondly, at the date of this study, 80 new COVID-19 samples, which have been appended to DB1 by
researchers after the publication of [32], were added to DB1 to be able to compare our study with other state-of-art
findings. This new database, which contains 1206 X-ray images in total, is named as DB2. Lastly, 113 new COVID-19
samples obtained from different domain were added to DB2 to be able to create a more balanced data-set that would be
more convenient to measure performance of the proposed method. The new 113 COVID-19 samples were taken from
[35] resulting in the DB3, which contains 1319 X-ray images in total. A short summary of the constructed data-sets for
the proposed study is given in Table 1.
2.2

Employed Deep Learning Architectures

The traditional machine learning approaches, which consist of sequential sub-steps such as pre-processing, feature
extraction, feature reduction/selection and classification, require domain specific expertise in order to obtain satisfactory
performance in medical image analysis. The spatial and frequency domain features are the most popular approaches to
3

A PREPRINT - N OVEMBER 18, 2020

Table 1: The Image Distributions over Classes in Tested Datasets
Labels
DB1 [32] DB2 [32]+[33] DB3 [32]+[33]+[35]
COVID-19
126
206
319
No Findings
500
500
500
Pneumonia
500
500
500
Total
1126
1206
1319

obtain discriminating information from the raw images. For example, the Scale-Invariant Feature Transform (SIFT) and
Maximally Stable Extreme Regions (MSER) methods are used in literature [36, 37] as the spatial domain interest point
extraction techniques and the interest points based features are employed in traditional learning models subsequently.
Regarding the frequency domain feature extractors like short time Fourier Transform (STFT) and wavelet transform
(WT), the parameter selection procedure makes them hard to implement and dependent to user experience. For example,
the window-type, window-length and overlapping ratio must be chosen in an optimum way for STFT, while the mother
wavelet type and decomposition level must be correctly tuned in WT to obtain a representative distribution of time-scale
atoms [38, 39]. On the other hand, even if the training processing times of deep learners are relatively long, they are
implemented in end-to-end architectures which have no need or having minimum need for extra pre-processing steps and
optimum tuning of feature extractor parameters. Additionally, deep neural networks (DNNs), which gained importance
in machine learning and pattern recognition in recent years, have the ability to learn high order robust features from raw
data [40, 41]. In contrast, traditional machine learning methods are still highly error prone and inaccurate to be used
in a sensitive decision making process. Therefore, in order to benefit from the aforementioned superiorities of deep
learners, seven CNN models including the MobileNetV2, VGG16, ResNet50, ResNet101, NasNet, InceptionV3 and
Xception, have been applied to three public databases with the aim of three-class (COVID, No-Findings, Pneumonia)
discrimination of X-ray images in the proposed study.
2.2.1

MobileNetV2

Although higher accuracy values can be achieved by using deeper and larger networks, these networks do not ensure
efficiency in terms of size and speed, making them inconvenient for mobile applications. However, the fast and accurate
diagnosis of COVID-19 is critical in the current pandemic condition causing the small mobile deep learning solutions
more preferable. The MobileNetV2 [42], as an improvement of MobileNetV1, can be a powerful and versatile solution
for mobile dignosis of COVID-19 due to its high performance proven in various application areas including medieval
writer identification [43], detecting underwater live crabs [44], real-time crowd counting [45] and remote wave gauging
[46]. The main characteristic of MobileNetV2 relies on the usage of depthwise separable convolutions in which two
1D convolutions with two kernels are used instead of employing 2D convolution with a single kernel. As a result, the
training phase can be carried out by using fewer parameters and less memory that results in a small and efficient model.
2.2.2

VGG16

The VGG16 [47] is a pre-trained very large CNN that was invented by VGG (Visual Geometry Group) from University
of Oxford. The VGG16 was the 1st runner-up of the ILSVR (ImageNet Large Scale Visual Recognition Competition)
2014 in the classification task. The VGG16 architecture uses simple 3×3 size kernels in convolutional layers and
combine them in a sequence to emulate the effect of larger receptive fields. The implemented VGG16 architecture
is composed of 13 convolutional layers followed by 3 fully connected layers. Despite the simplicity of the VGG16
architecture, its memory usage and computational cost is dramatically high due to the exponentially increasing kernels.
2.2.3

ResNet50 and ResNet101

The ResNet deep learning models [48], which have introduced the “skip connections” concept, are the sub-classes
of CNNs. In ResNets, some of the convolutional layers are bypassed (the concept of “skip connections”) at a time
and the batch normalization is applied along with non-linearities (ReLU) [49]. In ResNet architectures, the "skip
connections" enables to train much deeper networks and they give the network the option to simply copy the activations
from ResNet block to ResNet block, preserving information as data goes through the layers [50]. The two versions of
ResNet family, the ResNet50 and ResNet101 having 49 and 100 convolutional layers respectively, were employed in
the current proposed COVID-19 diagnosis approach as a classifier and deep feature extractor.
4

A PREPRINT - N OVEMBER 18, 2020

2.2.4

NasNet

As a relatively recent network, the NASNet [51], whose CNN architecture was designed by another neural network,
outperformed the previous state-of-the-art on the ILSVRC 2012 dataset. The NASNet architecture was created by
use of the Neural Architecture Search (NAS) framework providing an algorithm for finding optimal neural network
architectures [52]. In this algorithm, a controller recurrent neural network creates architectures aimed to perform at a
specific level for a particular task, and by trial and error learns to propose better and better models [50].
2.2.5

InceptionV3

In the InceptionV3 [53], the inception modules, which are repeatedly stacked together to form a large network, are
employed as an alternative to sequentially ordered convolution layers. In the inception modules, an asymmetric
convolution structure is obtained by using multiple filters of various sizes resulting in more and more abundant spatial
features with increased diversity. The usage of inception modules not only cause significant decrements in the number
of parameters, it also increases the recognition ability of the network by using multiple scale features [54].
2.2.6

Xception

As an improved version of inception architecture, the Xception [55] algorithm uses depthwise separable convolutions
which enables more efficient use of model parameters. In the Xception, the standard inception modules are replaced
with the depthwise separable convolutions (enhanced inception modules) that use the depth dimension (the number of
channels) as well as the spatial information. The enhanced inception modules result in stronger features including the
depth information.

2.3

Transfer Learning

In the context of deep learning based classification, transfer learning involves training a deep-net on a labelled training
dataset (consisting of high number of samples) in order to circumvent the obstacle of insufficient number of training
samples. For the analysis of medical images, the weights of the deep-net learned during the training of a CNN on a
main dataset (for example ImageNet [56]) are transferred to a second CNN, which is then re-trained on labelled samples
of desired medical data set using pre-learned weights. The final training phase is named as "fine tuning"; in which the
certain layers of pre-trained net can be frozen (the weights of these layers stay fixed) while the remaining layers can be
fine-tuned to suit the classification problem, except the last fully connected layer.
In our study, the employed CNNs were applied to COVID-19 data-sets by using the Transfer Learning strategy in
the light of literature findings. In [57], it was mentioned that the performance of knowledge transfer depends on
the dissimilarity between the database on which a CNN is trained and the database to which the knowledge is to be
transferred. The distance between the natural image databases, that are employed for knowledge transfer, and COVID-19
data-sets is considerable. However, recent studies show that there is a potential for having benefit from knowledge
transfer in medical data sets. For instance, in [58], a pre-trained CNN was employed as a feature extractor with the aim
of chest pathology identification. In [59], pre-trained CNN based features have shown improved performance as they
were fused with traditional handcrafted features in a nodule detection system. In addition to their feature extractor usage,
the knowledge transferred CNNs can also be employed as the main classification framework with fine-tuning. For
example, in [60], it was shown that the fine-tuned CNNs have performed similarly or better than the CNNs trained from
scratch. In this study, pre-trained weights from [61] were transferred in either a shallow tuning or deep tuning strategy
in which the weights of few layers for the former and many layers for the latter were trained. The results highlighted
that medical image analysis requires deep tuning of more layers in contrast to many other computer vision tasks. In
another study, it was demonstrated that fine-tuning of pre-trained networks worked better compared to nets trained from
scratch in the analysis of skin lesions [62]. Additionally, in [63] knowledge transfer from natural images was applied in
thoraco-abdominal lymph node detection and interstitial lung disease classification resulting in higher performance
than training the CNNs from scratch. Similarly, in [64], transfer learning strategy was employed to identify the fetal
abdominal standard plane and the approach revealed improved capability of the algorithm to encode the complicated
appearance of the abdominal plane. In our study, due to the aforementioned superiority of fine-tuning strategy, seven
CNNs, which have already been trained by natural image database (ImageNet), were fine-tuned to extract deep features
by using the X-ray samples. Later, these deep features were employed in the classification of chest X-ray images with
individual and ensemble learning models.
5

A PREPRINT - N OVEMBER 18, 2020

Figure 1: The deep feature extraction module

2.4

Decision and Feature Level Fusion

In a pattern recognition system, the ultimate goal is the design of best possible classification model for a specific
problem such as the COVID-19 detection by using X-ray images. Traditionally, various classification models that
have different theories and methodologies are applied to a specific pattern recognition problem, and the best model in
terms of performance metrics is chosen. However, it was observed that the sets of patterns misclassified by the various
classifiers would not necessarily overlap, even if one of the models has yielded the best accuracy. Hence, different
classifiers may be harnessed to improve the overall performance by using their possible complementary information
about the patterns to be classified, when they are used in an ensemble scheme [65]. This type of ensemble learning
scheme is called decision level fusion based learning, in which the individual decisions of different models are combined
to derive a consensus decision instead of relying on a single decision-making model. The hard-level combination
uses the individual outputs of each classifier after they are binarized by applying a threshold to the classifier output
probabilities (estimates of a posteriori probability of the class) to map them into class labels. As a member of hard-level
combination, the majority voting strategy simply counts the votes received from each classifier and the class that has
the largest number of votes is selected as the consensus decision.
As an additional fusion strategy, the feature level fusion, in which various sets of features obtained by different feature
extractors are combined, has high potential to achieve better classification performance [66, 67, 68, 69]. Feature level
fusion generally consists of the concatenation of various normalized feature subsets resulting in a single feature vector
forming a complete representation of different views (deep features obtained by using various CNNs). Regarding the
CNNs based feature level fusion studies, even if the various CNN models are based on different configurations (or
architectures), the fine-tuning of these CNN models by using the same target database (COVID-19 database in our
study) consisting of concatenated feature vectors, can provide complementary information [70, 71].
2.5

Proposed Deep Features Based Ensemble Model

In this study, seven CNN models (the MobilenetV2, VGG16, ResNet50, ResNet101, NasNet, InceptionV3, and
Xception) have been used as the main structure of proposed framework. During the development of proposed method,
firstly, these seven CNN models have been employed as deep feature extractors as depicted in Figure 1. As seen in
Figure 1, the three databases were fed to the individual CNNs, which have already been pre-trained by using ImageNet
[56], with the aim of network specific deep feature extraction by using a 5-fold cross-validation scheme. The optimum
hyperparameters were chosen by employing a batch-size, epoch, and learning rate analysis that was based on trial and
error strategy. Accordingly, the number of training epochs was chosen as 50, while a batch-size of 16 was employed.
The learning rate that controls the speed of convergence was set to 0.0001, when Stochastic Gradient Descent with
momentum was used as the optimization technique. Subsequent to the deep feature extraction phase, the obtained
deep features were fed to a softmax classifier satisfying the end-to-end learning scheme of classical deep learning. The
classical softmax layer of CNNs, which is the generalization of logistic sigmoid function with the ability of mapping
deep-features onto probability values used as outputs in discrimination problems having three or more classes, is named
as "softmax classifier" in our study. The softmax classifier [72, 73, 74] is employed to measure the discriminating power
of deep features obtained from the individual CNNs and also for the concatenated deep feature vector. The predictions
of the individual CNNs were obtained as shown in the second-column/upper-row of Figure 2.
In [75] and [76], it was mentioned that the CNNs, which are very good at learning invariant features, may show lower
performance than the SVMs in classification. On the other hand, the SVMs are very successful at producing optimal
6

A PREPRINT - N OVEMBER 18, 2020

Figure 2: The multistage learning approach and decision level fusion of individual classifiers

Figure 3: The Flowchart of the proposed method employing feature and decision level fusion.

decision surfaces from well behaved feature vectors, while having difficulties to represent the variances occurred in
image features. Regarding the chest X-ray images used in our study, the areas that characterize the lung consolidation
pattern may be located in various parts of the lung with changing size resulting in significant variances. Therefore,
in addition to individual CNN based learning, a multistage model, in which the CNNs are employed to extract deep
features that have potential to detect and recognize lung consolidation patterns, and non-linear SVMs that are trained
by feeding the deep features learned by the CNNs, was presented and its performance was validated by using three
databases. This multistage learning approach that uses CNNs and SVMs in a cascade connection has been successfully
employed in various areas with the aim of classification performance improvement [77, 78, 79]. In this configuration,
fully-connected activations of each CNN have been employed as feature extractors (given in Figure 1) and the obtained
deep feature vectors were fed to classifiers in a 5-fold validation scheme. Additionally, with the aim of performance
improvement, the predictions obtained from classical end-to-end CNN learning (deep features that are fed to sotmax
classifier) and kernel based SVMs (deep features that are fed to SVMs) were fused by using the voting approach in
accordance with the combinations given in Table 2. The SVM based learning configuration that uses the deep features
and the applied voting strategy was presented in Figure 2.
Regarding the feature level fusion phase; the deep features extracted by each employed CNNs were concatenated into
a single fused feature vector. Subsequently, the fused feature vector was fed to the softmax classifier and also to the
non-linear SVMs. After this, the individual predictions of the softmax classifier and SVMs were obtained as depicted in
Figure 3. Furthermore, to benefit from the possible complementary behaviour of the learning models, the obtained
individual decisions were fused by using the majority voting and this final approach, which gives the best performance,
was chosen as our proposed method. The detailed flowchart of proposed method including the deep feature extraction
module, feature level fusion, multistage learning and decision level fusion can be seen in Figure 3.
7

A PREPRINT - N OVEMBER 18, 2020

3

Experimental Results

The individual performance of the employed seven CNN models plus the results of concatenated feature vector can be
seen in the first column of Table 2 in terms of the accuracy metric (each presented accuracy value was calculated as the
mean of 5-folds and the standard deviation of these 5-folds were also represented for clarification). The highest accuracy
values were obtained as 87.6%, 85.7% and 85.7% for the DB1, DB2 and DB3 by employing the InceptionV3, ResNet50
and VGG16 respectively. In contrast, when the poorest individual performances are investigated, it is seen that the
MobileNetV2 had the worst accuracy value as 84.2% for DB1, while the NasNet has ended up with the accuracy values
as 83.3% and 84.1% for DB2 and DB3 respectively. The second and third columns of Table 2 show the accuracy values
obtained by multistage learning scheme, which uses non-linear SVM kernels, for the individual deep feature sets and
also for the concatenated feature vector as given in bottom row group. As seen in column 2, the highest accuracy value
was obtained by using the RBF kernel as 87.6% for the DB1 with no increment compared to softmax classifier. On the
other hand, the RBF kernel based SVM learning, which were fed by VGG16 deep features, has slightly increased best
accuracy value to 85.9% for DB2, while the InceptionV3 has reached to 86.2% for DB3 by using RBF kernel based
multistage approach. In addition, the columns 4, 5, 6 and 7 indicate the accuracy values obtained by using the decision
level fusion strategy composed of the combinations of softmax classifier, radial basis function (RBF) and polynomial
kernel based SVMs as highlighted in the Table 2.
Table 2: The detailed presentation of accuracy values obtained from applied individual and ensemble learning scenarios
for three data-sets. The standard deviations of accuracy values obtained from 5-folds are presented in parentheses.
Accuracy (Standard Deviation)
Inception V3

Individual Performances of
Deep Neural Networks

Xception

MobileNet V2

ResNet50

ResNet101

NasNet

VGG16
Feature Level
Fusion

Concatenated
Vector

DB1
DB2
DB3
DB1
DB2
DB3
DB1
DB2
DB3
DB1
DB2
DB3
DB1
DB2
DB3
DB1
DB2
DB3
DB1
DB2
DB3
DB1
DB2
DB3

Individual Classifiers
Softmax
SVM
SVM
Classifier
(RBF)
(Poly)
87.6 (2.2)
87.6 (2.3)
87.3 (2.2)
83.6 (2.4)
84.0 (4.4)
83.6 (4.8)
84.5 (2.6)
86.2 (2)
85.9 (2.6)
86.7 (3.8)
85.7 (4.2)
85.6 (3.4)
84.7 (2.1)
83.7 (2.8)
83.0 (2.6)
85.3 (1.6)
83.3 (1.8)
82.1 (3.6)
84.2 (2.8)
86.8 (2.7)
86.0 (1.2)
84.5 (2)
84.7 (2.5)
84.4 (2.4)
85.5 (3.3)
85.8 (2.8)
85.6 (2.1)
86.3 (2.3)
87.4 (2.5)
87.0 (2.7)
85.7 (1.2)
84.9 (5.2)
82.6 (3.8)
85.4 (2.6)
85.6 (3.1)
85.2 (3.1)
85.5 (1.5)
85.7 (2.1)
85.5 (1.6)
84.1 (4.4)
84.8 (3.9)
83.4 (4.1)
85.1 (2.8)
85.3 (3.1)
84.9 (3)
85.2 (2.5)
84.8 (2.3)
84.3 (2.6)
83.3 (2.1)
81.4 (3.1)
79.8 (6.4)
84.1 (2.6)
83.6 (2.6)
82.3 (3)
85.8 (3.1)
86.3 (2.9)
85.9 (3.1)
85.1 (1)
85.9 (1.1)
85.2 (1.4)
85.7 (1.8)
85.9 (2.2)
85.8 (2.2)
90.2 (2.3)
90.7 (1.7)
90.3 (2.2)
87.3 (2.4)
86.9 (2)
87.2 (2.2)
88.7 (3)
89.2 (2.9)
89.3 (2.9)

Fusion #1
RBF + Poly
87.5 (2)
83.6 (3.3)
86.4 (2.6)
85.3 (3.8)
82.8 (1.5)
82.3 (1.6)
86.1 (2.1)
84.8 (2.6)
85.9 (2.5)
87.3 (2.5)
83.5 (1.8)
85.4 (2.9)
85.7 (1.5)
84.7 (4.3)
85.2 (1.8)
84.4 (2.3)
79.7 (3.1)
82.9 (2.3)
86.3 (2.3)
86.0 (0.8)
85.8 (2.2)
90.8 (1.6)
87.1 (2.1)
89.2 (2.8)

Decision Level Fusion over Predictions
Fusion #2
Fusion #3
Softmax + RBF
Softmax + Poly
87.3 (3.5)
87.4 (2.3)
83.0 (3.4))
82.8 (3.3)
86.0 (2)
86.0 (2.2)
85.7 (3.8)
86.4 (3.5)
83.9 (1.7)
83.4 (1.8)
83.9 (1.6)
82.9 (0.9)
84.7 (1.3)
84.7 (1.6)
84.6 (2.5)
84.7 (2.6)
85.9 (3.2)
85.9 (2.4)
86.2 (2.6)
86.4 (2.5)
85.0 (1.9)
83.9 (1.4)
85.4 (2.6)
85.4 (2.8)
85.4 (1.8)
85.4 (2.3)
84.8 (3.9)
84.3 (5)
85.2 (3.1)
85.1 (2.9)
84.8 (2.2)
84.4 (2.2)
81.8 (2.7)
80.2 (3.2)
83.9 (2.6)
83.4 (2.4)
86.0 (3)
85.9 (2.9)
85.6 (0.6)
85.4 (1)
86.0 (2.3)
86.0 (2.1)
90.4 (2.1)
90.4 (2)
87.3 (2)
87.3 (2.1)
89.0 (2.8)
89.1 (2.9)

Fusion #4
All
87.7 (2.3)
83.7 (3.3)
86.3 (2.2)
86.8 (3.4)
84.2 (1.2)
84.0 (0.7)
86.1 (1.1)
85.0 (2.5)
86.1 (1.8)
87.5 (2.3)
85.2 (0.8)
85.6 (2.8)
85.8 (1.9)
84.7 (4)
85.1 (2.9)
84.6 (2.1)
81.8 (3.2)
83.5 (3.3)
86.1 (3.4)
85.9 (1.1)
86.1 (2.1)
90.8 (1.7)
87.6 (2)
89.5 (2.7)

Regarding the effect of feature level fusion, the bottom row group of Table 2 and the Figure 4, in which the error
values obtained from the three COVID-19 databases for the individual softmax classifier based learning models plus
concatenated feature vector can be investigated. As seen in Figure 4, the error values, which were obtained from the
deep feature vector formed by using feature level fusion, are significantly lower than individual softmax classifier
performance by reaching 9.8%, 12.7% and 11.3% errors for the DB1, DB2 and DB3 respectively. As understood from
Table 2 and the Figure 4, not a specific individual deep feature set (extracted by using a specific CNN) has outperformed
the others for all three databases. This situation indicates that there is a significant need for ensemble learning which
may pave the way for the complementary information achievement. It should also be noted that the error values for DB1
and DB3 were even further reduced by 0.5% and 0.6% respectively when RBF and polynomial kernel based multistage
learning algorithms were applied.
The contribution of decision level fusion can be investigated by using the right-side of Table 2 and the Figure 5. In
Figure 5, the conventional classification performance of the softmax classifier (as it is used in traditional CNN based
learning) was chosen as the reference baseline performance for seven CNN based deep feature extraction schemes. For
comparison, the increments or decrements seen in the accuracy values obtained by the multistage SVM based learning
and the decision level fusion were represented for each deep feature set plus the concatenated feature vector (obtained
by the feature level fusion). When the Table 2 is investigated, it is seen that the highest accuracy values within the entire
8

A PREPRINT - N OVEMBER 18, 2020

Figure 4: Classification errors of the individual learning and the feature level fusion schemes when the softmax classifier
is employed.

Figure 5: The accuracy variations compared to CNNs when multistage learning and/or majority voting is applied are
presented.

test set combinations were obtained as 90.8%, 87.6% and 89.5% for the DB1, DB2 and DB3 respectively, when the
fourth decision level fusion approach, including the majority voting of hard labels obtained by softmax classifier, RBF
9

A PREPRINT - N OVEMBER 18, 2020

Figure 6: Confusion Matrices obtained from Fusion #4 strategy (Left DB1, Middle DB2, Right DB3)

and polynomial SVMs, was employed. As illustrated in Figure 5, almost for all multistage SVM based learning and
decision level fusion cases applied to MobileNetV2 and VGG16 based deep features, up to 2.5% increase in accuracy
rate was achieved. On the contrary, approximately all the accuracy values obtained by decision level fusion, when they
were applied to Xception and NasNet based deep features, were lower than the baseline softmax classifier performance.
In accordance with the remaining ResNet50, ResNet101 and InceptionV3 based deep features, neither the positive nor
the negative effect of multistage learning and decision level fusion was clearly seen. For instance, up to 2% increase
in the accuracy values was seen for the InceptionV3 based scenarios in DB3, while slight improvements have been
achieved by using ResNet101 based scenarios for DB2.
As alternative objective evaluation criteria, the confusion matrix based metrics were calculated to be able to show the
performance of proposed approach. For doing this, the true positive (TP), true negative (TN), false positive (FP), and
false negative (FN) values were obtained for each database. The confusion matrices obtained by the Fusion #4 strategy
applied to 3 databases were given as Figure 6 for further understanding. After the confusion matrices were obtained, 3
objective evaluation metrics were calculated as follows:
P recision =

Recall =

F 1 score = 2

TP
TP + FP

TP
TP + FN

P recision × Recall
P recision + Recall

(1)

(2)

(3)

Among these, the precision emphasizes how precise the learning model is out of those predicted positive samples, how
much of the predicted positives are actual positive. The precision is an important parameter to determine when the costs
of FP predictions is high. Moreover, the recall measures how much of the actual positive samples are captured by the
model by labeling it as positive (TP). The recall is an essential parameter when there is a high cost associated with FN
samples. The behaviour of precision vs recall of the COVID-19, pneumonia and no-finding classes obtained by using
majority voted decisions, described as Fusion #4, of individual deep feature sets (obtained by a specific CNN) plus the
concatenated feature vector (obtained by the feature level fusion) is given in Figure 7. It is seen that the precision and
recall values obtained by the concatenated feature vector were higher than the individual deep feature sets in almost all
cases. As presented in Table 2, the highest accuracy values were obtained when the Fusion #4 strategy was applied
for all three databases. The obtained precision and recall values for Fusion #4 strategy is also depicted in Table 3
to go in deeper investigation. As seen in this table, in almost all classes and databases, the highest precision, recall
and F1-scores were obtained for the COVID-19 class which has the highest priority in our classification problem. In
addition, an important evaluation metric named as Kappa, which is a statistical measure of inter-annotator agreement
for categorical items by comparing an observed accuracy with an expected accuracy [80], was given for all databases in
Table 3. As mentioned in [81], the Kappa values greater than 0.80 are called almost perfect classification. Hence, the
obtained Kappa values (0.845, 0.798 and 0.835 for DB1, DB2 and DB3 respectively) shows the success of proposed
approach following Fusion #4 strategy in COVID-19 diagnosis problem. As a final point to remark, all the F1-measure
values indicating how precise the classifier is (what percentage of the samples assigned to a certain class is classified
correctly), as well as how robust it is (what percentage of the samples belonging to a certain class is classified correctly),
were quite high for the COVID-19 class, showing the success of proposed Fusion #4 strategy.
10

A PREPRINT - N OVEMBER 18, 2020

(a)

(b)

(c)

Figure 7: Obtained precision and recall values of Fusion #4 strategy for each individual CNN and concatenated feature
vector.

11

A PREPRINT - N OVEMBER 18, 2020

Table 3: The detailed presentation of evaluation metrics for the Fusion #4 strategy.

Feature Level
Fusion

DB1 ([32])

4

Precision
Recall
F-Score
Accuracy
Kappa

DB2 ([32] + [33])

DB3 ([32] + [33] + [35])

COVID-19

No Findings

Pneumonia

COVID-19

No Findings

Pneumonia

COVID-19

No Findings

Pneumonia

100
97.6
98.78

87.26
93.2
90.13

92.53
86.8
89.57

96.82
88.83
92.65

82.88
93
87.65

89.47
81.6
85.35

97.72
94.35
96.01

85.52
91
88.17

88.51
84.8
86.62

90.84
0.845

87.56
0.801

89.46
0.839

Discussion and Conclusion

Although the RT-PCR is the most common technique to diagnose COVID-19, chest radiography based approaches have
been extensively used as complementary diagnosis tools due to the low-sensitivity drawback of RT-PCR especially seen
in the early stage of COVID-19. The X-ray scanning has been preferred as the primary radiography based imaging
approach in COVID-19 detection due to its fast imaging speed, low cost and low dosage exposing of radiation compared
to CT. However, the interpretation success of X-ray images strongly depends on the radiologist’s experience and visual
inspection of the X-ray images belonging to several patients takes significant time and effort. In order to increase the
objectivity of the X-ray imaging interpretation and decrease the required time and effort, CAD systems have been used
as supporting decision mechanisms in the detection of COVID-19 cases. In this respect, several studies employing
deep networks as the decision tool were published lately as depicted in Table 4. The performance of our proposed
study is compared with the previous studies in terms of the; i) the number of COVID-19 image samples existing in the
employed database, ii) the number of classes that are tried to be separated, iii) the accuracy value plus the precision
and recall metrics. As seen in Table 4, the number of employed X-ray samples for model generation is very low in
[84], [85] and [86], which have used only 50, 100 and 100 X-ray images respectively. In such systems, which have few
number of samples for training and testing, the learning model usually memorizes the data resulting in over-fitting. On
the other hand, although nearly sufficient number of X-ray samples exist in [26], [29], [82], [83], [87] and [88], the
ratio of COVID-19 samples are very low compared to the distribution of the remaining classes. However, most of the
learning models tend to work on balanced class distributions or equal misclassification costs, and the performance of
these learning methods can be significantly compromised when imbalanced data sets, like the employed COVID-19
vs non COVID-19 distributions seen in [26, 29, 82, 83, 87, 88], are used. Therefore, in our study, the employed data
bases were progressively created, starting from the usage of samples given in [32] as DB1, till minimum imbalance
between employed classes was achieved in DB3. As seen in Table 4, our method has outperformed [32], [26] and [88]
in terms of accuracy, precision and recall metrics, while our algorithm provides competitive performance compared to
[29], [82], [83] and [87] in terms of accuracy. It should be noted that our approach applied to DB1 is having the same
number of image samples and same cross-validation strategy compared to [32], while a similar 5-fold cross-validation
with different number of X-ray samples was carried out in [26] and [88]. Furthermore, the precision values obtained by
using our method were significantly higher than [32], [26], [29] and [88], while higher performance was achieved in
terms of recall compared to [82], [83] and [87].
As seen in Table 2 and the Figure 4, none of the individual learning models has been significantly outperformed the
others (The statistical significance analysis results of the tested approaches are given in the Supplementary Materials as
Table S1 and S2). However, accuracy improvements up to 3% were achieved when feature level fusion has been applied
to obtained deep features. When the multistage learning and decision level fusion approaches are investigated, it is seen
that the accuracy rises up to 2.5% and 1% have been obtained for the deep features extracted by using MobileNetV2
and VGG16 respectively. The supportive effect of SVM usage and majority voting for these two CNNs can be related
to their sizes, which are the cause of possible underfitting and overfitting. As mentioned in [89], small networks such
as the MobileNetV2 usually suffer from underfitting, while very large models such as the VGG16 may have trouble
with overfitting [90]. However, a learner such as SVM, which is good at producing optimal decision surfaces even
there is noise on the data, can have positive effect on the classification accuracy similar to our case. On the contrary,
same multistage learning and majority voting strategy did not work well, resulting accuracy reductions for the deep
featured obtained by Xception and NasNet. When the architecture of NasNet is investigated, it is seen that the NasNet
was constructed by a neural architecture search based optimization carried out by using reinforcement learning. As a
result of this process, the well-designed scalable and convolutional cells are defined in the optimum way, resulting in
an architecture that is prone to produce robust features as in our case. In a similar way, in the Xception, the usage of
depthwise separable convolutions paves the way of efficiently usage of model parameters producing stronger features.
Hereby, the cascade connection of the SVMs to the last FC layer of Xception and NasNet plus the usage of majority
voting has no supportive effect in classification. So, the network based discrimination is more than enough for these
two CNNs.
12

A PREPRINT - N OVEMBER 18, 2020

Table 4: Performance comparison of related works on COVID-19 detection problem with the proposed method.
Paper

Ozturk et al.
2020 [32]

Architecture

Total Number
of Images
1125

DarkCovidNet
625

Mahmud et al.
2020 [26]
Oh et al.
2020 [29]
Waheed et al.
2020 [31]
Wang et al.
2020 [82]
Ioannis and Mpeslana
2020 [83]
Sethy and Behera
2020 [84]
Narin et al.
2020 [85]
Hemdan et al.
2020 [86]

CovXNet

5856

A patch-based ResNet18

15043

Augmentation by CovidGAN
Classification by VGG16

1124
No Augmentation
3260
With Augmentation

COVID-Net

13975

VGG-19

1428

ResNet-50 + SVM

50

ResNet-50

100

COVIDX-Net

50

Brunese et al.
2020 [87]

Cascade Analyse
by Multiple CNN models
(Only Second Model Results)

3003

Zhang et al.
2020 [88]

Confidence-aware
Anomaly Detection (CAAD)

43583

COVID-19 (250)
Other(2753)
COVID-19 (106)
Normal(107)
Viral Pneumonia (5977)
Healthy + Non-Viral (37393)
COVID-19 (125)
No Findings (500)
Pneumonia (500)
COVID-19 (206)
No Findings (500)
Pneumonia (500)
COVID-19 (319)
No Findings (500)
Pneumonia (500)

1125
Proposed
Study

Deep Feature and
Decision Level
Fusion ()

Class Names and
number of Images
COVID-19 (125)
No Findings (500)
Pneumonia (500)
COVID-19 (125)
No Findings (500)
COVID-19 (305)
No Findings (1583)
Viral Pneumonia (1493)
Bacterial Pneumonia (2780)
COVID-19 (180)
Pneumonia (6012)
Normal (8851)
COVID-19 (403)
Normal (721)
COVID-19 (1741)
Normal (1519)
COVID-19 (358)
Pneumonia (5551)
Normal (8066)
COVID-19 (224)
Normal (504)
Pneumonia (700)
COVID-19 (25)
Non-Covid (25)
COVID-19 (50)
Non-Covid (50)
COVID-19 (25)
Non-Covid (25)

1206
1319

Accuracy (%)

Precision (%)
(COVID-19)

Recall (%)
(COVID-19)

87.02

80.7

97.87

98.08

97.97

90.65

90.2

90.8

89.9

91.9

76.9

100

85

89

69

95

96

90

93.3

98.91

91

93.48

98.75

92.85

95.38

93.47

97.24

98

100

96

90

83

100

97

94

87

72.77

71.7

73.83

90.84

100

97.6

87.56

96.82

88.83

89.46

97.72

94.35

Another important fact that needs to be discussed about our proposed system, in which the Fusion #4 strategy was
applied, is the obtained high precision and recall values. The precision value is directly related with the number of FP
samples and low precision in COVID-19 means high number of healthy subjects that are misdiagnosed as COVID-19.
An early quarantine measure applied to COVID-19 patients is employed as the fundamental disease control strategy
across the countries [91]. Apart from the physical damages, the quarantine may cause dramatic psychological effects
on the mental health. In previous studies, it was reported that the psychological impact of quarantine can vary from
immediate effects such as irritability, fear of spreading infection to family members, confusion, anger, loneliness, anxiety,
frustration, denial, insomnia, despair, depression, to extremes of consequences including suicide [92, 93, 94]. Therefore,
the FP samples frequently seen in a COVID-19 detection system may cause significant undesired psychological and
social consequences. However, as seen in Table 3, the proposed system has precision values, belonging to COVID-19
class, as 100%, 96.82% and 97.72% for the DB1, DB2 and DB3 respectively showing its almost perfect FP sample
reduction performance. The recall metric, which is directly connected to FN samples, is also essential in COVID-19
detection because of the high cost associated with FN samples. Misdiagnosing a COVID-19 patient may cause dramatic
consequences due to the very easy and fast transmission mechanism of the SARS-CoV2. The subject misdiagnosed as
normal can spread the disease to his/her close environment in a very short time resulting in new patients who are ready
to spread the disease further. However, thanks to our proposed approach, high recall values, reaching up to 97.6% and
94.35% in DB1 and DB3 respectively, were obtained by using the Fusion #4 strategy.
Although the deep learning approaches have enabled unprecedented breakthroughs in medical image analysis, the
interpretable modules are sacrificed for uninterpretable ones that achieve higher performance through greater abstraction
(more layers) and tighter integration (end-to-end training) in CNNs [95]. However, in [96], the Class Activation
Mapping (CAM) technique, which is a way of producing visual explanations of the predictions of deep learning
13

A PREPRINT - N OVEMBER 18, 2020

Figure 8: CAM visualizations of two patients obtained by six CNNs (top and bottom rows) and the flow-chart of
employed feature level fusion (middle row).

models [97], was proposed to make the CNNs more transparent and explainable. By using the CAM technique, useful
knowledge about the employed prediction regions in the COVID-19 detection problem can be investigated. For example,
the failure regions can be visually identified for the wrongly classified samples and necessary modifications in the
learning models can be applied towards the most fruitful research directions. Besides, for a deep model, which is very
strong in diagnosis, the CAM technique can visually identify the lung consolidation patterns as a supportive diagnostic
tool for doctors. In Figure 8, two CAMs obtained from COVID-19 samples are given with the aim of visual validation
of employed CNNs. In the CAMs, the red color highlights the lung regions where the employed CNN model focuses on
(activating around those patterns) most during the discrimination. In Figure 8, upper row, the CAMs obtained with six
CNNs, excluding VGG16 due to inability of representing its CAM by using employed approach, for a 83 year old male
having mitral insufficiency, pulmonary hypertension and atrial fibrillation with COVID-19 infection, can be seen. In
this patient, Ground-glass opacification (GGO) and consolidation in the right upper lobe and left lower lobe is seen
as the indicators of COVID-19. The InceptionV3 and ResNet50 have correctly localized the right upper lobe pattern,
while missing the left lower lobe. However, the Xception has successfully detected both two pathological regions with
high spatial resolution. In the bottom row, the CAMs of a 53-year-old female, whose X-ray contains multifocal patchy
opacities in both lungs, was depicted. This case is a good example to see the effect of feature level fusion of different
CNNs because of the existing three separate opacity patterns. While the MobileNet has strong focus on right side
single pattern, the InceptionV3, ResNet50 and Xception has low activation on right side. However, the ResNet101 and
InceptionV3 have highly focused on left side upper pattern, while the Xception and MobileNet has significant activity
near the left side lower pattern. When the complementary effect of these CAMs is considered, it is obvious that the
fusion of features obtained by these CNNs would have higher discriminating power. In the middle part of Figure 8, a
flowchart explaining how the features obtained from various CNNs are concatenated is given for further understanding.
Additionally, in Figure 9, X-ray images belonging to the same patient with bilateral GGO are shown. The image in
upper row is taken on the second day of diagnosis while the bottom row X-ray image is taken on the fourth day. As
it can be seen, the active regions belonging to a specific CNN are consistent and not dramatically changing towards
second and fourth day images.
In future research, we aim to focus on following research paths related with COVID-19 for further improvement; i) a
different version of the feature level fusion, in which the features obtained from the various layers of the same CNN
are concatenated, can be employed instead of the fusion of features obtained from the last FC layer of different type
CNNs. By doing that diverse features, which contain more semantic information in the top layers and more low-level
information in bottom layers, can be combined to provide more discriminative information. ii) since the outbreak is
recent, the number of COVID-19 X-ray images, which can be used in CAD system design studies, is very limited. Even
though there exists a recent study [31] that uses Generative Adversarial Network (GAN) for increasing the number of
training samples, the performance can be improved by using Progressive Growing GAN [98] for augmentation. Besides,
14

A PREPRINT - N OVEMBER 18, 2020

Figure 9: CAM visualizations of the same patient on the second and fourth day of diagnosis.

the quality of artificial COVID-19 samples can be improved by integrating more labeled data into the learning process
by using GANs. iii) the Canonical Correlation Analysis (CCA) [99, 100], which aims at measuring linear relationships
between two sets of variables by using the within-set and between-set sample covariance matrices, can be employed as
a feature fusion approach instead of simple concatenation of deep features. By utilizing the multi-view features (the
deep features extracted from different CNNs and/or from the different layers of the same CNN), more discriminating
features having maximized correlation between various sets can be attained with the hope of performance increase in
COVID-19 detection. iv) the hyperparameters, which are adjusted prior to the learning process and affect how the
learning algorithm fits the model to data, can be tuned by using automatic tuning algorithms such as the Bayesian
optimization [101]. In this way, the optimum hyperparameters for the COVID-19 detection problem can be tuned for
both CNNs and SVMs to obtain higher performance.

References
[1]

Wu, Z.; McGoogan, J.M. Characteristics of and important lessons from the coronavirus disease 2019 (COVID19) outbreak in China: summary of a report of 72 314 cases from the Chinese Center for Disease Control and
Prevention. Jama 2020, 323, 1239–1242.

[2]

Sohrabi, C.; Alsafi, Z.; O’Neill, N.; Khan, M.; Kerwan, A.; Al-Jabir, A.; Iosifidis, C.; Agha, R. World Health
Organization declares global emergency: A review of the 2019 novel coronavirus (COVID-19). International
Journal of Surgery 2020.

[3]

Lai, C.C.; Shih, T.P.; Ko, W.C.; Tang, H.J.; Hsueh, P.R. Severe acute respiratory syndrome coronavirus 2
(SARS-CoV-2) and corona virus disease-2019 (COVID-19): the epidemic and the challenges. International
journal of antimicrobial agents 2020, p. 105924.

[4]

Razai, M.S.; Doerholt, K.; Ladhani, S.; Oakeshott, P. Coronavirus disease 2019 (covid-19): a guide for UK
GPs. BMJ 2020, 368.

[5]

Chen, N.; Zhou, M.; Dong, X.; Qu, J.; Gong, F.; Han, Y.; Qiu, Y.; Wang, J.; Liu, Y.; Wei, Y.; others.
Epidemiological and clinical characteristics of 99 cases of 2019 novel coronavirus pneumonia in Wuhan,
China: a descriptive study. The Lancet 2020, 395, 507–513.

[6]

Chung, M.; Bernheim, A.; Mei, X.; Zhang, N.; Huang, M.; Zeng, X.; Cui, J.; Xu, W.; Yang, Y.; Fayad, Z.A.;
others. CT imaging features of 2019 novel coronavirus (2019-nCoV). Radiology 2020, 295, 202–207.

[7]

Clerkin, K.J.; Fried, J.A.; Raikhelkar, J.; Sayer, G.; Griffin, J.M.; Masoumi, A.; Jain, S.S.; Burkhoff, D.;
Kumaraiah, D.; Rabbani, L.; others. COVID-19 and cardiovascular disease. Circulation 2020, 141, 1648–1655.

[8]

Fang, L.; Karakiulakis, G.; Roth, M. Are patients with hypertension and diabetes mellitus at increased risk for
COVID-19 infection? The Lancet. Respiratory Medicine 2020, 8, e21.

[9]

Li, B.; Yang, J.; Zhao, F.; Zhi, L.; Wang, X.; Liu, L.; Bi, Z.; Zhao, Y. Prevalence and impact of cardiovascular
metabolic diseases on COVID-19 in China. Clinical Research in Cardiology 2020, 109, 531–538.

[10]

Hegde, S. Does asthma make COVID-19 worse?, 2020.

[11]

Chamola, V.; Hassija, V.; Gupta, V.; Guizani, M. A Comprehensive Review of the COVID-19 Pandemic and
the Role of IoT, Drones, AI, Blockchain, and 5G in Managing its Impact. IEEE Access 2020, 8, 90225–90265.
15

A PREPRINT - N OVEMBER 18, 2020

[12]

[13]
[14]
[15]
[16]
[17]

[18]
[19]
[20]
[21]

[22]

[23]
[24]
[25]
[26]

[27]

[28]

[29]
[30]
[31]
[32]

[33]

Pfefferle, S.; Reucher, S.; Nörz, D.; Lütgehetmann, M. Evaluation of a quantitative RT-PCR assay for the
detection of the emerging coronavirus SARS-CoV-2 using a high throughput system. Eurosurveillance 2020,
25, 2000152.
Li, Y.; Xia, L. Coronavirus disease 2019 (COVID-19): role of chest CT in diagnosis and management.
American Journal of Roentgenology 2020, 214, 1280–1286.
Fang, Y.; Zhang, H.; Xie, J.; Lin, M.; Ying, L.; Pang, P.; Ji, W. Sensitivity of chest CT for COVID-19:
comparison to RT-PCR. Radiology 2020, p. 200432.
Wang, Y.; Dong, C.; Hu, Y.; Li, C.; Ren, Q.; Zhang, X.; Shi, H.; Zhou, M. Temporal changes of CT findings in
90 patients with COVID-19 pneumonia: a longitudinal study. Radiology 2020, p. 200843.
Kanne, J.P.; Little, B.P.; Chung, J.H.; Elicker, B.M.; Ketai, L.H. Essentials for radiologists on COVID-19: an
update—radiology scientific expert panel, 2020.
Self, W.H.; Courtney, D.M.; McNaughton, C.D.; Wunderink, R.G.; Kline, J.A. High discordance of chest x-ray
and computed tomography for detection of pulmonary opacities in ED patients: implications for diagnosing
pneumonia. The American journal of emergency medicine 2013, 31, 401–405.
Jacobi, A.; Chung, M.; Bernheim, A.; Eber, C. Portable chest X-ray in coronavirus disease-19 (COVID-19): A
pictorial review. Clinical Imaging 2020.
Borghesi, A.; Maroldi, R. COVID-19 outbreak in Italy: experimental chest X-ray scoring system for quantifying
and monitoring disease progression. La radiologia medica 2020, p. 1.
Doi, K. Computer-aided diagnosis in medical imaging: historical review, current status and future potential.
Computerized medical imaging and graphics 2007, 31, 198–211.
Kermany, D.S.; Goldbaum, M.; Cai, W.; Valentim, C.C.; Liang, H.; Baxter, S.L.; McKeown, A.; Yang, G.; Wu,
X.; Yan, F.; others. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell
2018, 172, 1122–1131.
Rajaraman, S.; Candemir, S.; Kim, I.; Thoma, G.; Antani, S. Visualization and interpretation of convolutional
neural network predictions in detecting pneumonia in pediatric chest radiographs. Applied Sciences 2018,
8, 1715.
Baltruschat, I.M.; Nickisch, H.; Grass, M.; Knopp, T.; Saalbach, A. Comparison of deep learning approaches
for multi-label chest X-ray classification. Scientific reports 2019, 9, 1–10.
Yadav, S.S.; Jadhav, S.M. Deep convolutional neural network based medical image classification for disease
diagnosis. Journal of Big Data 2019, 6, 113.
Jaiswal, A.K.; Tiwari, P.; Kumar, S.; Gupta, D.; Khanna, A.; Rodrigues, J.J. Identifying pneumonia in chest
X-rays: A deep learning approach. Measurement 2019, 145, 511–518.
Mahmud, T.; Rahman, M.A.; Fattah, S.A. CovXNet: A multi-dilation convolutional neural network for
automatic COVID-19 and other pneumonia detection from chest X-ray images with transferable multi-receptive
feature optimization. Computers in Biology and Medicine 2020, p. 103869.
Toğaçar, M.; Ergen, B.; Cömert, Z. COVID-19 detection using deep learning models to exploit Social Mimic
Optimization and structured chest X-ray images using fuzzy color and stacking approaches. Computers in
Biology and Medicine 2020, p. 103805.
Rahimzadeh, M.; Attar, A. A modified deep convolutional neural network for detecting COVID-19 and
pneumonia from chest X-ray images based on the concatenation of Xception and ResNet50V2. Informatics in
Medicine Unlocked 2020, p. 100360.
Oh, Y.; Park, S.; Ye, J.C. Deep learning covid-19 features on cxr using limited training data sets. IEEE
Transactions on Medical Imaging 2020.
Elasnaoui, K.; Chawki, Y. Using X-ray images and deep learning for automated detection of coronavirus
disease. Journal of Biomolecular Structure and Dynamics 2020, pp. 1–22.
Waheed, A.; Goyal, M.; Gupta, D.; Khanna, A.; Al-Turjman, F.; Pinheiro, P.R. Covidgan: Data augmentation
using auxiliary classifier gan for improved covid-19 detection. IEEE Access 2020, 8, 91916–91923.
Ozturk, T.; Talo, M.; Yildirim, E.A.; Baloglu, U.B.; Yildirim, O.; Acharya, U.R. Automated detection of
COVID-19 cases using deep neural networks with X-ray images. Computers in Biology and Medicine 2020, p.
103792.
Cohen, J.P.; Morrison, P.; Dao, L.; Roth, K.; Duong, T.Q.; Ghassemi, M. COVID-19 Image Data Collection:
Prospective Predictions Are the Future. arXiv preprint arXiv:2006.11988 2020.
16

A PREPRINT - N OVEMBER 18, 2020

[34]

[35]
[36]

[37]
[38]

[39]

[40]
[41]
[42]

[43]
[44]
[45]

[46]
[47]
[48]
[49]
[50]
[51]
[52]
[53]
[54]
[55]
[56]

Wang, X.; Peng, Y.; Lu, L.; Lu, Z.; Bagheri, M.; Summers, R.M. Chestx-ray8: Hospital-scale chest x-ray
database and benchmarks on weakly-supervised classification and localization of common thorax diseases.
Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2097–2106.
Agchung. Actualmed COVID-19 Chest X-ray Dataset Initiative, 2020.
Ilhan, H.O.; Sigirci, I.O.; Serbes, G.; Aydin, N. A fully automated hybrid human sperm detection and
classification system based on mobile-net and the performance comparison with conventional methods.
Medical & Biological Engineering & Computing 2020, pp. 1–22.
Ilhan, H.O.; Serbes, G.; Aydin, N. Automated sperm morphology analysis approach using a directional
masking technique. Computers in Biology and Medicine 2020, p. 103845.
Serbes, G.; Sakar, C.O.; Kahya, Y.P.; Aydin, N. Effect of different window and wavelet types on the performance
of a novel crackle detection algorithm. International Conference on Hybrid Information Technology. Springer,
2011, pp. 575–581.
Ilhan, H.O.; Serbes, G.; Aydin, N. Dual Tree Complex Wavelet Transform Based Sperm Abnormality
Classification. 2018 41st International Conference on Telecommunications and Signal Processing (TSP). IEEE,
2018, pp. 1–5.
Ravì, D.; Wong, C.; Deligianni, F.; Berthelot, M.; Andreu-Perez, J.; Lo, B.; Yang, G.Z. Deep learning for
health informatics. IEEE journal of biomedical and health informatics 2016, 21, 4–21.
Afshar, P.; Mohammadi, A.; Plataniotis, K.N.; Oikonomou, A.; Benali, H. From handcrafted to deep-learningbased cancer radiomics: challenges and opportunities. IEEE Signal Processing Magazine 2019, 36, 132–160.
Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; Chen, L.C. Mobilenetv2: Inverted residuals and linear
bottlenecks. Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp.
4510–4520.
Cilia, N.D.; De Stefano, C.; Fontanella, F.; Marrocco, C.; Molinara, M.; Di Freca, A.S. An end-to-end deep
learning system for medieval writer identification. Pattern Recognition Letters 2020, 129, 137–143.
Cao, S.; Zhao, D.; Liu, X.; Sun, Y. Real-time robust detector for underwater live crabs based on deep learning.
Computers and Electronics in Agriculture 2020, 172, 105339.
Gao, C.; Wang, P.; Gao, Y. MobileCount: An Efficient Encoder-Decoder Framework for Real-Time Crowd
Counting. Chinese Conference on Pattern Recognition and Computer Vision (PRCV). Springer, 2019, pp.
582–595.
Buscombe, D.; Carini, R.J.; Harrison, S.R.; Chickadel, C.C.; Warrick, J.A. Optical wave gauging using deep
neural networks. Coastal Engineering 2020, 155, 103593.
Simonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 2014.
He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 770–778.
Pacheco, A.G.; Krohling, R.A. The impact of patient clinical information on automated skin cancer detection.
Computers in biology and medicine 2020, 116, 103545.
Lundervold, A.S.; Lundervold, A. An overview of deep learning in medical imaging focusing on MRI.
Zeitschrift für Medizinische Physik 2019, 29, 102–127.
Zoph, B.; Vasudevan, V.; Shlens, J.; Le, Q.V. Learning transferable architectures for scalable image recognition.
Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8697–8710.
Cogan, T.; Cogan, M.; Tamil, L. MAPGI: Accurate identification of anatomical landmarks and diseased tissue
in gastrointestinal tract using deep learning. Computers in biology and medicine 2019, 111, 103351.
Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; Wojna, Z. Rethinking the inception architecture for computer
vision. Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826.
Zhuang, X.; Zhang, T. Detection of sick broilers by digital image processing and deep learning. Biosystems
Engineering 2019, 179, 106–116.
Chollet, F. Xception: Deep learning with depthwise separable convolutions. Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017, pp. 1251–1258.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.;
Bernstein, M.; others. Imagenet large scale visual recognition challenge. International journal of computer
vision 2015, 115, 211–252.
17

A PREPRINT - N OVEMBER 18, 2020

[57]

Azizpour, H.; Razavian, A.S.; Sullivan, J.; Maki, A.; Carlsson, S. Factors of transferability for a generic
convnet representation. IEEE transactions on pattern analysis and machine intelligence 2015, 38, 1790–1802.

[58]

Bar, Y.; Diamant, I.; Wolf, L.; Greenspan, H. Deep learning with non-medical training used for chest pathology
identification. Medical Imaging 2015: Computer-Aided Diagnosis. International Society for Optics and
Photonics, 2015, Vol. 9414, p. 94140V.

[59]

Van Ginneken, B.; Setio, A.A.; Jacobs, C.; Ciompi, F. Off-the-shelf convolutional neural network features for
pulmonary nodule detection in computed tomography scans. 2015 IEEE 12th International symposium on
biomedical imaging (ISBI). IEEE, 2015, pp. 286–289.

[60]

Tajbakhsh, N.; Shin, J.Y.; Gurudu, S.R.; Hurst, R.T.; Kendall, C.B.; Gotway, M.B.; Liang, J. Convolutional
neural networks for medical image analysis: Full training or fine tuning? IEEE transactions on medical
imaging 2016, 35, 1299–1312.

[61]

Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classification with deep convolutional neural networks.
Advances in neural information processing systems, 2012, pp. 1097–1105.

[62]

Menegola, A.; Fornaciali, M.; Pires, R.; Avila, S.; Valle, E. Towards automated melanoma screening: Exploring
transfer learning schemes. arXiv preprint arXiv:1609.01228 2016.

[63]

Shin, H.C.; Roth, H.R.; Gao, M.; Lu, L.; Xu, Z.; Nogues, I.; Yao, J.; Mollura, D.; Summers, R.M. Deep
convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and
transfer learning. IEEE transactions on medical imaging 2016, 35, 1285–1298.

[64]

Chen, H.; Ni, D.; Qin, J.; Li, S.; Yang, X.; Wang, T.; Heng, P.A. Standard plane localization in fetal ultrasound
via domain transferred deep neural networks. IEEE journal of biomedical and health informatics 2015,
19, 1627–1636.

[65]

Kittler, J.; Hatef, M.; Duin, R.P.; Matas, J. On combining classifiers. IEEE transactions on pattern analysis
and machine intelligence 1998, 20, 226–239.

[66]

Li, S.; Kwok, J.Y.; Tsang, I.H.; Wang, Y. Fusing images with different focuses using support vector machines.
IEEE Transactions on neural networks 2004, 15, 1555–1561.

[67]

Ulukaya, S.; Serbes, G.; Kahya, Y.P. Overcomplete discrete wavelet transform based respiratory sound
discrimination with feature and decision level fusion. Biomedical Signal Processing and Control 2017,
38, 322–336.

[68]

Sakar, C.O.; Serbes, G.; Gunduz, A.; Tunc, H.C.; Nizam, H.; Sakar, B.E.; Tutuncu, M.; Aydin, T.; Isenkul,
M.E.; Apaydin, H. A comparative analysis of speech signal processing algorithms for Parkinson’s disease
classification and the use of the tunable Q-factor wavelet transform. Applied Soft Computing 2019, 74, 255–263.

[69]

Gunatilaka, A.H.; Baertlein, B.A. Feature-level and decision-level fusion of noncoincidently sampled sensors
for land mine detection. IEEE transactions on pattern analysis and machine intelligence 2001, 23, 577–589.

[70]

Raja, K.; Venkatesh, S.; Christoph Busch, R.; others. Transferable deep-cnn features for detecting digital and
print-scanned morphed face images. Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, 2017, pp. 10–18.

[71]

Du, P.; Li, E.; Xia, J.; Samat, A.; Bai, X. Feature and model level fusion of pretrained CNN for remote sensing
scene classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 2018,
12, 2600–2611.

[72]

Khamparia, A.; Saini, G.; Pandey, B.; Tiwari, S.; Gupta, D.; Khanna, A. KDSAE: Chronic kidney disease
classification with multimedia data learning using deep stacked autoencoder network. Multimedia Tools and
Applications 2019, pp. 1–16.

[73]

Sharif, M.I.; Li, J.P.; Khan, M.A.; Saleem, M.A. Active deep neural network features selection for segmentation
and recognition of brain tumors using MRI images. Pattern Recognition Letters 2020, 129, 181–189.

[74]

Liao, B.; Xu, J.; Lv, J.; Zhou, S. An image retrieval method for binary images based on DBN and softmax
classifier. IETE Technical Review 2015, 32, 294–303.

[75]

Huang, F.; LeCun, Y. Large-scale learning with svm and convolutional netw for generic object recognition.
2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2006, Vol. 10.

[76]

Tang, Y. Deep learning using support vector machines. CoRR, abs/1306.0239 2013, 2.

[77]

Liang, W.j.; Zhang, H.; Zhang, G.f.; Cao, H.x. Rice blast disease recognition using a deep convolutional neural
network. Scientific reports 2019, 9, 1–10.
18

A PREPRINT - N OVEMBER 18, 2020

[78]

Su, R.; Liu, T.; Sun, C.; Jin, Q.; Jennane, R.; Wei, L. Fusing convolutional neural network features with
hand-crafted features for osteoporosis diagnoses. Neurocomputing 2020, 385, 300–309.

[79]

Fuhad, K.; Tuba, J.F.; Sarker, M.; Ali, R.; Momen, S.; Mohammed, N.; Rahman, T. Deep Learning Based
Automatic Malaria Parasite Detection from Blood Smear and Its Smartphone Based Application. Diagnostics
2020, 10, 329.

[80]

McHugh, M.L. Interrater reliability: the kappa statistic. Biochemia medica: Biochemia medica 2012,
22, 276–282.

[81]

Landis, J.R.; Koch, G.G. An application of hierarchical kappa-type statistics in the assessment of majority
agreement among multiple observers. Biometrics 1977, pp. 363–374.

[82]

Wang, L.; Wong, A. COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of
COVID-19 Cases from Chest X-Ray Images. arXiv preprint arXiv:2003.09871 2020.

[83]

Apostolopoulos, I.D.; Mpesiana, T.A. Covid-19: automatic detection from x-ray images utilizing transfer
learning with convolutional neural networks. Physical and Engineering Sciences in Medicine 2020, p. 1.

[84]

Sethy, P.K.; Behera, S.K. Detection of coronavirus disease (covid-19) based on deep features. Preprints 2020,
2020030300, 2020.

[85]

Narin, A.; Kaya, C.; Pamuk, Z. Automatic detection of coronavirus disease (covid-19) using x-ray images and
deep convolutional neural networks. arXiv preprint arXiv:2003.10849 2020.

[86]

Hemdan, E.E.D.; Shouman, M.A.; Karar, M.E. Covidx-net: A framework of deep learning classifiers to
diagnose covid-19 in x-ray images. arXiv preprint arXiv:2003.11055 2020.

[87]

Brunese, L.; Mercaldo, F.; Reginelli, A.; Santone, A. Explainable Deep Learning for Pulmonary Disease and
Coronavirus COVID-19 Detection from X-rays. Computer Methods and Programs in Biomedicine 2020, p.
105608.

[88]

Zhang, J.; Xie, Y.; Li, Y.; Shen, C.; Xia, Y. Covid-19 screening on chest x-ray images using deep learning
based anomaly detection. arXiv preprint arXiv:2003.12338 2020.

[89]

Zhang, X.; Zhou, X.; Lin, M.; Sun, J. Shufflenet: An extremely efficient convolutional neural network for
mobile devices. Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp.
6848–6856.

[90]

Howard, A.G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang, W.; Weyand, T.; Andreetto, M.; Adam,
H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861 2017.

[91]

Rubin, G.J.; Wessely, S. The psychological effects of quarantining a city. Bmj 2020, 368.

[92]

Dubey, S.; Biswas, P.; Ghosh, R.; Chatterjee, S.; Dubey, M.J.; Chatterjee, S.; Lahiri, D.; Lavie, C.J. Psychosocial impact of COVID-19. Diabetes & Metabolic Syndrome: Clinical Research & Reviews 2020.

[93]

Brooks, S.K.; Webster, R.K.; Smith, L.E.; Woodland, L.; Wessely, S.; Greenberg, N.; Rubin, G.J. The
psychological impact of quarantine and how to reduce it: rapid review of the evidence. The Lancet 2020.

[94]

Barbisch, D.; Koenig, K.L.; Shih, F.Y. Is there a case for quarantine? Perspectives from SARS to Ebola.
Disaster medicine and public health preparedness 2015, 9, 547–553.

[95]

Selvaraju, R.R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; Batra, D. Grad-cam: Visual explanations
from deep networks via gradient-based localization. Proceedings of the IEEE international conference on
computer vision, 2017, pp. 618–626.

[96]

Zhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; Torralba, A. Learning deep features for discriminative
localization. Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp.
2921–2929.

[97]

Arun, N.; Gaw, N.; Singh, P.; Chang, K.; Aggarwal, M.; Chen, B.; Hoebel, K.; Gupta, S.; Patel, J.; Gidwani,
M.; others. Assessing the (Un) Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical
Imaging. arXiv preprint arXiv:2008.02766 2020.

[98]

Karras, T.; Aila, T.; Laine, S.; Lehtinen, J. Progressive growing of gans for improved quality, stability, and
variation. arXiv preprint arXiv:1710.10196 2017.

[99]

Hotelling, H. Relations Between Two Sets of Variates. Biometrika 1936, 28, 321–377.

[100]

Kettenring, J.R. Canonical analysis of several sets of variables. Biometrika 1971, 58, 433–451.
19

A PREPRINT - N OVEMBER 18, 2020

[101]

Wu, J.; Chen, X.Y.; Zhang, H.; Xiong, L.D.; Lei, H.; Deng, S.H. Hyperparameter optimization for machine
learning models based on Bayesian optimization. Journal of Electronic Science and Technology 2019, 17, 26–
40.

20

