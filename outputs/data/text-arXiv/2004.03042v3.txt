arXiv:2004.03042v3 [eess.IV] 7 Sep 2020

COVID-MobileXpert: On-Device COVID-19 Patient
Triage and Follow-up using Chest X-rays
Xin Li

Chengyin Li

Dongxiao Zhu

Department of Computer Science
Wayne State University
Detroit, MI 48202
Email: xinlee@wayne.edu

Department of Computer Science
Wayne State University
Detroit, MI 48202
Email: cyli@wayne.edu

Department of Computer Science
Wayne State University
Detroit, MI 48202
Email: dzhu@wayne.edu

Abstract—During the COVID-19 pandemic, there has been an
emerging need for rapid, dedicated, and point-of-care COVID19 patient disposition techniques to optimize resource utilization
and clinical workflow. In view of this need, we present COVIDMobileXpert: a lightweight deep neural network (DNN) based
mobile app that can use chest X-ray (CXR) for COVID-19
case screening and radiological trajectory prediction. We design
and implement a novel three-player knowledge transfer and
distillation (KTD) framework including a pre-trained attending
physician (AP) network that extracts CXR imaging features
from a large scale of lung disease CXR images, a fine-tuned
resident fellow (RF) network that learns the essential CXR
imaging features to discriminate COVID-19 from pneumonia
and/or normal cases with a small amount of COVID-19 cases, and
a trained lightweight medical student (MS) network to perform
on-device COVID-19 patient triage and follow-up. To tackle the
challenge of vastly similar and dominant fore- and background
in medical images, we employ novel loss functions and training
schemes for the MS network to learn the robust features. We
demonstrate the significant potential of COVID-MobileXpert for
rapid deployment via extensive experiments with diverse MS
architecture and tuning parameter settings. The source codes for
cloud and mobile based models are available from the following
url:https://github.com/xinli0928/COVID-Xray.
Index Terms—COVID-19, SARS-CoV-2, On-device Machine
Learning, Trajectory Prediction, Chest X-Ray (CXR)

I. I NTRODUCTION
Due to its flu-like symptoms and potentially serious outcomes, a dramatic increase of suspected COVID-19 cases
are expected to overwhelm the healthcare system during the
flu season. Health systems still largely allocate facilities and
resources such as Emergency Department (ED) and Intensive
Care Unit (ICU) on a reactive manner facing significant labor
and economic restrictions. To optimize resource utilization
and clinical workflow, a rapid, automated, and point-of-care
COVID-19 patient management technology that can triage
(COVID-19 case screening) and follow up (radiological trajectory prediction) patients is urgently needed.
Chest X-ray (CXR), though less accurate than a PCR
diagnostic, chest Computed Tomography (CT) or serological
test, became an attractive option for patient management due
to its impressive portability, availability and scalability [1]. At
present, the bottleneck lies in the shortage of board certified
radiologists who are capable of identifying massive COVID19 positive cases to reduce wait time at ED and determining

the radiological trajectory of the COVID-19 patients after
admission. The intensive development of deep neural network
(DNN) powered CXR image analysis has seen the unprecedented success in automatic classification and segmentation of
lung diseases [2]. Using the cloud solutions such as Google
Cloud Platform or on-premise computing clusters to train a
sophisticated DNN (e.g., DenseNet-121 [3]) with dozens of
millions of parameters and hundreds of layers via billions of
operations for both training and inference, these large scale
Artificial Intelligence (AI) models achieve amazing performance that even outperforms board certified radiologists in
some well-defined tasks [4].
With the increasing number of smart devices and improved
hardware, there is a growing interest to deploy machine
learning models on the device to minimize latency and maximize the protection of privacy. However, up to date ondevice medical imaging applications are very limited to basic
functions, such as the DICOM image view. In the COVID-19
environment, a mobile AI approach is expected not only to
protect patient privacy, but also to provide a rapid, effective
and efficient assessment of COVID-19 patients without the
immediate need for an on-site radiologist. However, a major
challenge that prevents wide adoption of the mobile AI approach is lack of lightweight yet accurate and robust neural
networks.
Adequate knowledge has been accumulated from training
the large scale DNN systems to accurately discern the subtle
difference among the different lung diseases by learning the
discriminative CXR imaging features [4]. Leveraging these
results, we design and implement a novel three-player knowledge transfer and distillation (KTD) framework composed of
an Attending Physician (AP) network, a Resident Fellow (RF)
network, and a Medical Student (MS) network for on-device
COVID-19 patient triage and follow-up. In a nutshell, we pretrain a full AP network using a large scale of lung disease
CXR images [2], [4], followed by fine-tuning a RF network
via knowledge transfer using labeled COVID-19, pneumonia
and normal CXR images, then we train a lightweight MS
network for on-device COVID-19 patient triage and followup via knowledge distillation. After the KTD framework, the
lightweight MS network is able to produce expressive features
to identify COVID-19 cases as well as predict the radiological

Fig. 1. The analytical workflow of COVID-19 CXR interpretation

systems.

trajectory. The unique features of the KTD framework are
knowledge transfer from large-scale existing lung disease
images to enhance expressiveness of learned representation
and novel loss functions to increase robustness of knowledge
distillation to the MS network.
To the best of our knowledge, currently, there is no mobile
AI system for on-device COVID-19 patient triage and followup using CXR images. In this work, we present an AIpowered system, COVID-MobileXpert, to triage and follow
up COVID-19 patients using portable X-rays at the patients
location. At the ED, COVID-MobileXpert calculates COVID19 probabilistic risk to assist automated triage of COVID-19
patients. At the ICU or general ward (GW), it uses a series
of longitudinal CXR images to determine whether there is an
impending deterioration in the health condition of COVID-19
patients. Therefore, COVID-MobileXpert is essential to fully
realize the potential of CXR to exert both immediate and longterm positive impacts on US healthcare systems. It enjoys the
following advantages: 1) accurately detecting positive COVID19 cases particularly from closely related pneumonia cases; 2)
continuously following up admitted patients via radiological
trajectory prediction.
II. R ELATED W ORK

prepossessing, discriminative patterns for COVID-19 such as
ground-glass opacification/opacity (GGO) are then extracted
by CNNs. Most current studies have directly borrowed or
adopted well-known architectures such as ResNet [5], [10],
InceptionV3 [5], DenseNet [7], [8], [11], and VGG [10], [12].
After feature extraction, three major tasks have been performed: diagnosis, severity evaluation, and trajectory prediction. COVID-19 diagnosis is usually considered to be a classification problem. The most straightforward way of detecting
COVID-19 is to train a classifier with cross-entropy loss which
is applied within most of these approaches. Other than these
straightforward approaches, Zhang et al. [13] employed an
unsupervised anomaly detection approach that detects COVID19 cases as outliers. Moreover, Hassanien et al. [14] developed
a classifier based on a support vector machine.
As for severity evaluation, Cohen et al. [11] and Zhu et
al. [12] directly predicted lung disease severity scores using a
linear regression model based on extracted features. In order
to associate each score with a confidence value, Signoroni
et al. [9] treated this task as a joint multi-class classification
and regression problem using a compound loss function.
Based on the severity assessment, the trajectory prediction
can be achieved by calculating the difference in severity score
between two adjacent CXR images. Other than basic score
level interpretation, Duchesne et al. [15] built their trajectory
prediction model based on feature level. When the feature
from a single CXR was extracted by DenseNet-121, they used
logistic regression to classify the trajectory into one of three
categories: “Worse”, “Stable”, or “Improved”. However, the
feature from a single CXR may not be sufficient to predict
radiological trajectory. In order to tackle these challenges, our
model forecasts radiological trajectory using feature extracted
from a series of longitudinal CXR images of a single patient.
By incorporating longitudinal CXR images into our model,
novel imaging features of progressive disease, including subtle
changes of radiological features that are invisible to the human
eye, can be detected.

A. COVID-19 CXR Interpretation

B. On-device AI Model

During the COVID-19 pandemic in the past few months,
convolutional neural networks (CNNs) have been successfully
employed to assist with COVID-19 CXR interpretation. Since
mid-February 2020, we collected over 40 works (most of them
in a pre-print format) focusing on this subject. Although they
may focus on different specific tasks, as shown in Fig. 1,
they follow a similar pipeline: image prepossessing, feature
extraction, and interpretation.
Data augmentation and segmentation are widely used as part
of these approaches to avoid overfitting. In addition to the basic
transformation-based method which includes rotating, flipping,
scaling used in [5], Khalifa et al. [6] applied Generative
Adversarial Network to generate virtual samples for data
augmentation. To reduce the interference caused by unrelated
area, Yeh et al. [7], Lv et al. [8] and Signoroni et al. [9] applied
a U-net based method to perform a fast lung segmentation
and preserved the region of interest (ROI) only. After image

Currently, most AI models trained for COVID-19 interpretation are full DNNs that are not suitable to deploy on resourceconstrained mobile devices. As there is no existing on-device
medical image interpretation research, the vast majority of
the existing work focuses on comparing the performance of
different lightweight neural networks such as MobileNetV2
[16], SquezzeNet [17], Condense-Net [18], ShuffleNetV2 [19],
MnasNet [20] and MobileNetV3 [21] using small benchmark
natural image datasets such as CIFAR 10/100. MnasNet and
MobileNetV3 are representative models generated via automatic neural architecture search (NAS) whereas all other
networks are manually designed [22]. Due to the practical
hardware resource constraint of mobile devices, natural image
classification and segmentation performance have been compared based on accuracy, energy consumption, runtime, and
memory complexity that no single network has demonstrated
superior performance in all tasks [23]. Besides tailor-made

Fig. 2. Overview of the three-player KTD training architecture demonstrating the knowledge transfer from AP to RF and the knowledge

distillation from RF to MS. The blue and purple arrows demonstrate the training for two tasks: patient triage and follow-up respectively.

network architectures for mobile devices, compression of the
full DNN at the different stages of training also stands as
a promising alternative. For in-training model compression,
for example, Chen et al. [24] designed a novel convolution
operation via factorizing the mixed feature maps by their
frequencies to store and process feature maps that vary spatially slower at a lower spatial resolution to reduce both
memory and computation cost of the image classification.
Post-training or fine-tuning model compression techniques
such as quantization [25] and/or pruning techniques [26] are
often used to reduce the model size at the expense of reduced
prediction accuracy. Wang et al. [27] demonstrated using 8bit floating-point numbers for representing weight parameters
without compromising the model’s accuracy. Lou et al. [28]
automatically searched a suitable precision for each weight
kernel and chose another precision for each activation layer
and demonstrated a reduced inference latency and energy
consumption while achieving the same inference accuracy.
Tung and Mori [29] combined network pruning and weight
quantization in a single learning framework to compress
several DNNs without sacrificing accuracy.
In order to improve the performance of the lightweight ondevice models, knowledge distillation [30] is also used where
a full teacher model is trained on the cloud or an on-premise
GPU cluster, and a student model is trained at the mobile
device with the ‘knowledge’ distilled via the soft labels from
the teacher model. Thus the student model is trained to mimic
the outputs of the teacher model as well as to minimize the
cross-entropy loss between the true labels and predictive probabilities (soft labels). Knowledge distillation yields compact
student models that outperform the compact models trained
from scratch without a teacher model [31]. Goldblum et al.
[32] attempted to encourage the student network to output
correct labels using the training cases crafted with a moderate
adversarial attack budget to demonstrate the robustness of
knowledge distillation methods. Unlike the natural images,
on-device classification of medical images remain largely an
uncharted territory due to the following unique challenges:
1) label scarcity in medical images significantly limits the

Metric
# of CONV layers
Total weights
Total MACs

DenseNet-121
120
7.9M
2900M

MobileNetV2
20
3.47M
300M

SqueezeNet
22
0.72M
282M

TABLE I
C OMPARISON OF FULL AND COMPACT DNN MODEL COMPLEXITY.

generalizability of the machine learning system; 2) vastly
similar and dominant fore- and background in medical images
make it hard samples for learning the discriminating features
between different disease classes. To tackle these unique
challenges we propose a novel three-player framework for
training a lightweight network towards accurate and hardware
friendly on-device COVID-19 patient triage and follow-up.
III. M ETHOD
A. Model Architecture
We employ DenseNet-121 architecture as the template to
pre-train and fine-tune the AP and RF networks. In addition, among well-studied lightweight CNNs [23], we select
the most well-applied network MobileNetV2, and the most
lightweighted network SqueezeNet as the candidate MS networks for on-device COVID-19 case screening and radiological trajectory prediction. Table I summarizes the key model
complexity parameters [23]. Fig. 2 illustrates the three-player
KTD training framework where the knowledge of abnormal
CXR images is transferred from AP network to RF network
and knowledge of discriminating COVID-19, non-COVID-19,
and pneumonia is distilled from the RF network to the MS
network.
B. The KTD Training Scheme
We pre-train the AP network as the source task, i.e., lung
disease classification, and fine-tune, validate and test the RF
network as the destination task. Different from recent studies
[33] that pre-train the models with natural image datasets
such as ImageNet, we pre-train the DenseNet-121 based
AP network using the more related ChestX-ray8 dataset [2]
of 108,948 lung disease cases to extract the CXR imaging
features of lung diseases instead of generic natural imaging
features. Specifically, beyond the dense block, we employ a

shared fully connected layer for extracting the general CXR
imaging feature and 8 fully connected disease-specific layers
(including pneumonia as one disease layer) to extract diseasespecific features (Fig. 2). Following the pre-training using
large ChestX-ray8 dataset, the weights defining the general
CXR imaging feature and the pneumonia disease feature are
transferred to fine-tune the DenseNet-121 based RF network
using a smaller compiled dataset of 3 classes of CXR images,
i.e., COVID-19, normal and pneumonia. The latter is randomly
initialized using two sets of weight parameters corresponding
to normal and COVID-19 classes with the initial values
of other weight parameters transferred from the pre-trained
source model. The RF network is then used to train the
lightweight MS network, e.g., MobileNetV2, or SqueezeNet,
via knowledge distillation.
As shown in the MS section in Fig. 2, after knowledge
distillation, the trained MS network can triage patients by
screening COVID-19 cases following the blue arrow. Then a
radiological trajectory prediction model is further developed
based on the trained MS network. Following the purple
arrow, given a series of longitudinal CXR images from one
patient, all images are fed into the pre-trained MS network
for extracting disease-specific features. These features are then
aggregated using different schemes before prediction. Here we
investigate two different schemes: 1) calculating the difference
between the last two CXR images’ features; 2) chronologically
concatenating all features. After feature aggregation, two fully
connected layers are randomly initialized and trained with
softmax loss function for the trajectory prediction.
C. Loss Functions
As stated above, a unique challenge in medical imaging
classification is the so-called “hard sample problem” [34],
i.e., a subtle difference on the ROI across the images with
a large amount of shared fore- and backgrounds. Motivated
by this, we use an in-house developed loss function, i.e.,
Probabilistically Compact (PC) loss, for training the MS model
and compared with ArcFace [35], the additive angular margin
loss for deep face recognition, using the classical softmax loss
as the baseline. Both PC and ArcFace losses are designed for
improving classification performance on hard samples. PC loss
is to encourage the maximized margin between the most probable label (predictive probability) and the first several most
probable labels whereas ArcFac loss is to encourage widening
the geodesic distance gap between the closest labels. In terms
of predicted probabilities, DNN robustness is beneficial from
the large gap between fy (x) and fk (x) (k 6= y), where fy (x)
represents the true class and fk (x) (k 6= y) represents the
most probable class. Indeed, the theoretical study [36] in deep
learning shows that the gap fy (x) − maxk fk (x) can be used
to measure the generalizability of DNNs.
The PC loss to improve CNN’s robustness is as follows:
Lpc (θ) =

K
1 X X
N

K
X

max{0, fj (xik )+ξ−fk (xik )},

k=1 ik ∈Sk j=1,j6=k

(1)

Fig. 3. An example of data preparation for a series of longitudinal

CXR images with radiological trajectory labels. The patient is in
critical condition on t3 then recovered afterward.

where N is the number of training samples, ξ > 0 is the probability margin treated as a hyperparameter. Here, we include all
non-target classes in the formulation and penalize any classes
for each training sample that violate the margin requirement
for two reasons: (1) by maintaining the margin requirement
for all classes, it provides us convenience in implementation
as the first several most probable classes can change during the
training process; and (2) if one of the most probable classes
satisfies the margin requirement, all less probable classes will
automatically satisfy this requirement and hence have no effect
on the PC loss. Compared with previous works that explicitly
learn features with large inter-class separability and intra-class
compactness, the PC loss avoids assumptions on the feature
space, instead, it only encourages the feature learning that
leads to probabilistic intra-class compactness by imposing a
probability margin ξ.
IV. E XPERIMENT AND R ESULTS
In Section IV, we design and conduct extensive experiments
to evaluate the performance of the compact MS network
in patient triage and follow-up. In order to gain a holistic
view of the model behavior, we investigate the performance
concerning multiple choices of loss functions and values of
tuning parameters for COVID-19 case screening as well as
various choices of feature aggregation schemes and classifiers
for radiological trajectory prediction.
A. Datasets
The CXR image dataset for COVID-19 patient triage is
composed of 179 CXR images from normal class [37], 179
from pneumonia class [37] and 179 from COVID-19 class containing both PA (posterior anterior) and AP (anterior posterior)
positions [38] and we split it into training/validation/testing
sets with 125/18/36 cases (7:1:2) in each class. Since some
patients have multiple CXR images in COVID-19 class, we
sample images per patient for each split to avoid images from
the same patient being included in both training and test sets.
For the radiological trajectory dataset, we assign a opacity
score S for each COVID-19 positive CXR image in [38]
using the scoring system provided by [11]. Fig. 3 shows
an example of how we generate CXR image sequences

and assign corresponding radiological trajectory labels (i.e.,
“Worse”, “Stable”, “Improved”). Given a COVID-19 patient’s
CXR images over four time points (the maximum length is
set to four time points), we can create three CXR image
sequences with zero-padding. For each sequence, we calculate
the difference in the score of the last two CXR images. If the
difference is larger than 0.3 the sequence is categorized as
“Worse”, if the difference is less than −0.3, it is labeled as
“Improved”, otherwise, the category is “Stable”. We collect
a total of 159 CXR image sequences from 100 patients in
[38] and the dataset contains 80 “Worse” samples, 28 “Stable”
samples, and 51 “Improved” samples. Similarly, we split it into
training/validation/testing sets with 111/16/32 samples (7:1:2).
B. Implementation Details
We implement our model on a GeForce GTX 1080ti GPU
platform using PyTorch. The network is trained with the Adam
optimizer for 50 epochs with a mini-batch size of 32 (triage
task) and 10 (follow-up task). The parameter values that give
rise to the best performance on the validation dataset are used
for testing. Similar to [15], when training the radiological
trajectory prediction model, we employ the pre-trained MS
network as a feature extractor (fixed weights). To overcome
the overfitting problem, we also apply a dropout regularization
with a rate of 0.5.
C. Tunning Parameters
ξ: in the PC loss formula (Eq. 1), a large value encourages
the probabilistic intra-class compactness.
α: in knowledge distillation framework [30], [32] (Eq. 2),


min E(X,y)∼D αt2 KL Sθt (X) , T t (X)
θ
(2)

+(1 − α)` Sθt (X), y ],
it regularizes the ‘strength’ of knowledge distillation by
specifying the relative contributions of the distillation loss,
i.e., KL (Sθt (X) , T t (X)), measuring how well the MS model
mimics the RF model’s behavior using KL divergence and the
classification loss of the MS model, i.e., ` (Sθt (X), y). Sθ (.)
and T (.) represent the RF model and MS model, respectively.
The larger value, the stronger knowledge distillation is enforced from the RF model to the MS model.
T : in Eq. 2, it represents temperature where T = 1 corresponds
to the standard softmax loss. As the value of T increases, the
probability distribution generated by the softmax loss becomes
softer, providing more information regarding which classes the
RF model found more similar to the predicted class.

α
0.2
0.4
0.6
0.8
T
1
5
10

MobileNetV2/SqueezeNet (T=5)
PC(ξ = 0.8)
PC(ξ = 0.995)
ArcFace
0.870/0.798
0.833/0.777
0.870/0.750
0.880/0.777
0.870/0.815
0.861/0.796
0.851/0.796
0.851/0.787
0.851/0.805
0.880/0.824
0.870/0.796
0.851/0.796
MobileNetV2/SqueezeNet (α = 0.8)
PC(ξ = 0.8)
PC(ξ = 0.995)
ArcFace
0.851/0.750
0.880/0.814
0.870/0.796
0.880/0.824
0.870/0.796
0.851/0.796
0.880/0.796
0.842/0.750
0.861/0.787

SM
0.861/0.777
0.833/0.759
0.861/0.796
0.833/0.787
SM
0.870/0.796
0.833/0.787
0.870/0.824

TABLE II
C LASSIFICATION PERFORMANCE OF MS NETWORKS , T HE VALUES IN ./.
INDICATE M OBILE N ET V2 VS . S QUEEZE N ET.

Distilling knowledge from the RF network to the lightweight
MS network, we observe an impressive performance that a
vast majority of accuracy values are well above 0.850 for CXR
image classification. Table II shows the classification accuracy
results of both MobileNetV2 and SqueezneNet architecture
with different loss functions and values of tuning parameters.
It is clear that the knowledge distillation is essential to train
the lightweight MS network without compromising much
accuracy since the MS network alone, without knowledge
distillation, achieves a baseline classification accuracy of 0.843
(MobileNetV2) and 0.732 (SqueezeNet), which are lower than
those with knowledge distillation shown in Table II.
Looking at Table II in more detail, we note that the performance of MobileNetV2 and SqueezeNet are not sensitive to
the choice of temperatures (T) and strengths of distillation (α),
however, it is very sensitive to the choice of loss functions.
Overall, the PC loss developed in-house that flattens other
probable class predictions perform the best across diverse
settings of the tuning parameters, indicating the quality of
knowledge distilled from the RF network to the MS network
plays a pivotal role in training the lightweight MS network to
ensure an accurate on-device COVID-19 patient triage.
In order to systematically evaluate the performance of the
MS networks under the different decision thresholds, we
use the AUROC value to assess how well the model is
capable of discriminating COVID-19 cases from normal cases,
pneumonia cases as well as normal plus pneumonia cases. In
Fig. 4, both compact MS networks, i.e., MobileNetV2 and
SqueezeNet, demonstrate a remarkable performance on all discrimination tasks that are comparable to that of the large scale
cloud-based RF network, i.e., DenseNet-121. Importantly, both
MobileNetV2 and SqueezeNet achieve high AUROC values
of 0.970 and 0.964 when discriminating COVID-19 cases
against mixed pneumonia and normal cases demonstrating
strong potential for on-device triage using CXR images.

D. Evaluation of COVID-19 Patient Triage Performance
We first report the classification accuracy to select the best
MS model under different values of hyperparameters, followed
by systematic evaluation of the model’s discriminating power
of COVID-19 from non-COVID pneumonia and normal cases
using AUROC values. With the knowledge transfer from the
AP network pre-trained with a large set of abnormal lung
disease cases, the RF network demonstrates a remarkably high
accuracy of 0.935 in the classification of CXR images.

E. Evaluation of COVID-19 Patient Follow-up Performance
Similar to [15], we first report the classification accuracy
of discriminating “Worse” versus “Improved” cases to select
the best combination of classifiers and feature aggregation
schemes for on-device radiological trajectory prediction, followed by systematic evaluation of the model’s discriminating
power of “Worse” cases from “Improved” and “Stable cases
using AUROC values. Based on the features extracted from

Fig. 4. The upper panel shows the performance of the large-scale RF network and two compact MS networks of discriminating (a) COVID-19
vs. Normal cases; (b) COVID-19 vs. Pneumonia cases and (c) COVID-19 vs. Normal + Pneumonia cases, while the lower panel shows the
performance of discriminating (d) “Worse” vs. “Improved” cases; (e) “Worse” vs. “Stable” cases and (f) “Worse” vs. “Improved” + “Stable”
cases.
MobileNetV2/SqueezeNet (DenseNet-121)
Classifiers
Difference
Concatenation
Logistic Regression 0.560/0.640 (0.720)
0.760/0.720 (0.800)
Gradient Boosting
0.680/0.640 (0.680)
0.680/0.680 (0.680)
0.680/0.600 (0.680)
0.720/0.680 (0.720)
Random Forest
Our FC-classifier
0.720/0.680 (0.720)
0.800/0.760 (0.800)
TABLE III
P ERFORMANCE COMPARISON OF TWO FEATURE AGGREGATION SCHEMES
(D IFFERENCE VS . C ONCATENATION ) WITH FOUR DIFFERENT CLASSIFIERS
USING TWO MS N ETWORKS (M OBILE N ET V2 AND S QUEEZE N ET ) AS THE
FEATURE EXTRACTOR . VALUES IN PARENTHESES INDICATE THE UPPER
BOUND OF ACCURACY YIELDED BY RF N ETWORK (D ENSE N ET-121).

the MS networks, four classifiers are trained for radiological
trajectory prediction: 1) logistic regression; 2) gradient boosting; 3) random forest and 4) MS networks followed by fully
connected layers (our FC-classifier).
As shown in Table III, we observe the classifiers trained
based on the feature extracted from both compact MS networks, i.e., MobileNetV2 and SqueezeNet, achieve a very
similar level of performance to those trained with the feature
from large scale RF network i.e., DenseNet-121. This again
demonstrates that KTD training architecture with PC loss performs a high-quality knowledge distillation from RF network
to lightweight MS networks.
When doing comparison between the feature aggregation
schemes, we can see a significant improvement from using a
series of longitudinal features over using only the difference

between the last two sets of features. As for classifier selection, compared with the conventional classifiers i.e., logistic
regression, gradient boosting and random forest, our FCclassifier is able to learn a series of subtle changes related to
radiological features from CXR images, thus achieving a better
performance. As a result, the best on-device performance is
obtained by our FC-classifier with feature concatenation using
MobileNetV2, which attains the upper bound of accuracy
(0.800) yielded by DenseNet-121.
Duchesne et al. [15] also report a high accuracy (0.827)
of predicting the “Worse” category based on the feature
extracted from a single CXR with their highly imbalanced
testing dataset, which contains over 84.6% samples labeled
as “Worse”. However, the reported accuracy is lower than a
simple baseline: a dummy classifier that always predicts the
most frequent label “Worse” would yield a higher accuracy of
0.846. To make a comparison, we reimplement their model
[15] on our more balanced dataset and record a result of
0.600, which implies that using the feature from a single CXR
may not be sufficient to predict radiological trajectory. On the
other hand, by using feature concatenation from a series of
longitudinal CXR images, our model demonstrates better and
more reliable performance (0.800).
In order to systematically evaluate the performance of the
MS networks under the different decision thresholds, we again

Fig. 5. A schematic overview of on-device deployment of the COVID-MobileXpert.
Mobile Systems
The MS Network
MobileNetV2
SqueezeNet

CPU (%)
69.3
37.7

MobileNetV2
SqueezeNet

67.7
32.7

Nexus One
Memory (MB)
69.4
67.5
Nexus S
88.8
64.4

Energy
Heavy
Medium

CPU (%)
67.2
29.0

Heavy
Medium

66.2
28.8

Pixel
Memory (MB)
70.5
29.0
Pixel 2
69.4
70.1

Energy
Heavy
Medium

CPU (%)
68.7
26.7

Heavy
Medium

63.6
25.8

Pixel 2 XL
Memory (MB)
72.8
68.6
Pixel 3 XL
76.5
66.1

Energy
Heavy
Medium
Heavy
Medium

TABLE IV
C OMPARISON OF RESOURCE CONSUMPTION OF THE TWO ON - DEVICE MS NETWORKS DEPLOYED TO THE SIX A NDROID BASED MOBILE DEVICES .

use the AUROC value to assess how capable the model is
in discriminating “Worse” cases from “Improved” cases and
“Stable” cases. As shown in Fig. 4, MobileNetV2 shows a
close performance compared to the RF network (DenseNet121). It is important to note that MobileNetV2 networks can
achieve a high AUROC value of 0.883 enabling it to identify
“Worse” cases from “Improved” cases and show a significant
potential of on-device follow-up using CXR images.
V. P ERFORMANCE E VALUATION ON M OBILE D EVICES
For on-device COVID-19 patient triage and follow-up with
resource constraints, resource consumption is also an important consideration for performance evaluation in addition to
accuracy. In order to systematically assess the performance of
our COVID-19 on-device app, we select six mobile systems
released following a chronic order, i.e., Nexus One / Nexus S
(low-end); Pixel/ Pixel 2 (mid-range) and Pixel 2 XL/ Pixel
3 XL (high-end). Using the Pytorch Mobile framework, we
deploy the three MS networks to the six Android based mobile
systems and compare the resource consumption with regard to
CPU, memory and energy usages. Fig. 5 describes a workflow
to build an Android app based on the MS networks for ondevice patient triage and follow-up.
In Table IV, it is clear that the MobileNetV2 based COVID19 app is resource-hungry, demonstrated by much higher resource consumption than SqueezeNet. Thus, the high accuracy
achieved by MobileNetV2 is at the cost of high resource
consumption. Within each app, we observe a downward trend
in resource consumption following the chronic order, reflecting
a continuous improvement of mobile device hardware. Overall,
MobileNetV2 based COVID-19 apps are more suitable for

high-performing mobile devices due to the high accuracy
achieved with a higher resource consumption. On the other
hand, SqueezeNet is more suitable for low-end mobile devices
with both lower accuracy and resource consumption.
VI. C ONCLUSIONS
The classical two-player knowledge distillation framework
[30] has been widely used to train a compact network that is
hardware-friendly with ample applications [23]. In the related
task of on-device natural image classification, the teacher
network is pre-trained with ImageNet and distill the knowledge
to a lightweight student network (e.g., MobileNetV2). This
two-player framework, although seemingly successful, can be
problematic for on-device medical imaging based COVID-19
case screening and radiological trajectory prediction described
herein. The large gap between natural images and the medical
images of a specific disease such as COVID-19 makes the
knowledge distillation less effective as it is supposed to be.
The small number of labeled COVID-19 images for training
further aggravates the situation.
In our three-player KTD framework, knowledge transfer
from the AP network to the RF network can be viewed
as a more effective regularization as they are built on the
same network architecture, which in turn, make the knowledge
distillation more effective since the RF network and MS
network share the same training set. Different from what has
extensively investigated focusing on the impact of distillation
strength and temperature, we uncover a pivotal role of employing novel loss functions in refining the quality of knowledge
to be distilled. Hence our three-player framework provides a

more effective way to train the compact on-device model using
a smaller dataset while preserving performance.
From a more broad perspective, the three-player KTD
framework is generally applicable to train other on-device
medical imaging classification and segmentation apps for
point-of-care screening of other human diseases such as lung
[2] and musculoskeletal [39] abnormalities.
R EFERENCES
[1] H. Y. F. Wong, H. Y. S. Lam, A. H.-T. Fong, S. T. Leung, T. W.-Y. Chin,
C. S. Y. Lo, M. M.-S. Lui, J. C. Y. Lee, K. W.-H. Chiu, T. Chung et al.,
“Frequency and distribution of chest radiographic findings in covid-19
positive patients,” Radiology, p. 201160, 2020.
[2] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers,
“Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common thorax
diseases,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 2097–2106.
[3] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,
“Densely connected convolutional networks,” 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), Jul 2017.
[Online]. Available: http://dx.doi.org/10.1109/CVPR.2017.243
[4] P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Ding,
A. Bagul, C. Langlotz, K. Shpanskaya et al., “Chexnet: Radiologistlevel pneumonia detection on chest x-rays with deep learning,” arXiv
preprint arXiv:1711.05225, 2017.
[5] M. E. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M. A. Kadir,
Z. B. Mahbub, K. R. Islam, M. S. Khan, A. Iqbal, N. Al-Emadi et al.,
“Can ai help in screening viral and covid-19 pneumonia?” arXiv preprint
arXiv:2003.13145, 2020.
[6] N. E. M. Khalifa, M. H. N. Taha, A. E. Hassanien, and S. Elghamrawy,
“Detection of coronavirus (covid-19) associated pneumonia based on
generative adversarial networks and a fine-tuned deep transfer learning
model using chest x-ray dataset,” arXiv preprint arXiv:2004.01184,
2020.
[7] C.-F. Yeh, H.-T. Cheng, A. Wei, H.-M. Chen, P.-C. Kuo, K.-C. Liu, M.C. Ko, R.-J. Chen, P.-C. Lee, J.-H. Chuang, C.-M. Chen, Y.-C. Chen,
W.-J. Lee, N. Chien, J.-Y. Chen, Y.-S. Huang, Y.-C. Chang, Y.-C. Huang,
N.-K. Chou, K.-H. Chao, Y.-C. Tu, Y.-C. Chang, and T.-L. Liu, “A
cascaded learning strategy for robust covid-19 pneumonia chest x-ray
screening,” 2020.
[8] D. Lv, W. Qi, Y. Li, L. Sun, and Y. Wang, “A cascade network for
detecting covid-19 using chest x-rays,” 2020.
[9] A. Signoroni, M. Savardi, S. Benini, N. Adami, R. Leonardi, P. Gibellini,
F. Vaccher, M. Ravanelli, A. Borghesi, R. Maroldi et al., “End-to-end
learning for semiquantitative rating of covid-19 severity on chest x-rays,”
arXiv preprint arXiv:2006.04603, 2020.
[10] L. O. Hall, R. Paul, D. B. Goldgof, and G. M. Goldgof, “Finding covid19 from chest x-rays using deep learning on a small dataset,” 2020.
[11] J. P. Cohen, L. Dao, P. Morrison, K. Roth, Y. Bengio, B. Shen,
A. Abbasi, M. Hoshmand-Kochi, M. Ghassemi, H. Li et al., “Predicting
covid-19 pneumonia severity on chest x-ray with deep learning,” arXiv
preprint arXiv:2005.11856, 2020.
[12] J. Zhu, B. Shen, A. Abbasi, M. Hoshmand-Kochi, H. Li, and T. Q.
Duong, “Deep transfer learning artificial intelligence accurately stages
covid-19 lung disease severity on portable chest radiographs,” PloS one,
vol. 15, no. 7, p. e0236621, 2020.
[13] J. Zhang, Y. Xie, Z. Liao, G. Pang, J. Verjans, W. Li, Z. Sun, J. He,
Y. Li, C. Shen, and Y. Xia, “Viral pneumonia screening on chest x-ray
images using confidence-aware anomaly detection,” 2020.
[14] A. E. Hassanien, L. N. Mahdy, K. A. Ezzat, H. H. Elmousalami, and
H. A. Ella, “Automatic x-ray covid-19 lung image classification system
based on multi-level thresholding and support vector machine,” medRxiv,
2020.
[15] S. Duchesne, D. Gourdeau, P. Archambault, C. Chartrand-Lefebvre,
L. Dieumegarde, R. Forghani, C. Gagne, A. Hains, D. Hornstein, H. Le
et al., “Tracking and predicting covid-19 radiological trajectory using
deep learning on chest x-rays: Initial accuracy testing,” medRxiv, 2020.
[16] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2018, pp. 4510–4520.

[17] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,
and K. Keutzer, “Squeezenet: Alexnet-level accuracy with 50x fewer
parameters and¡ 0.5 mb model size,” arXiv preprint arXiv:1602.07360,
2016.
[18] G. Huang, S. Liu, L. Van der Maaten, and K. Q. Weinberger, “Condensenet: An efficient densenet using learned group convolutions,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2018, pp. 2752–2761.
[19] X. Zhang, X. Zhou, M. Lin, and J. Sun, “Shufflenet: An extremely efficient convolutional neural network for mobile devices,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2018, pp. 6848–6856.
[20] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard,
and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for
mobile,” in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2019, pp. 2820–2828.
[21] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang,
Y. Zhu, R. Pang, V. Vasudevan et al., “Searching for mobilenetv3,” in
Proceedings of the IEEE International Conference on Computer Vision,
2019, pp. 1314–1324.
[22] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A
survey,” arXiv preprint arXiv:1808.05377, 2018.
[23] S. Dhar, J. Guo, J. Liu, S. Tripathi, U. Kurup, and M. Shah, “On-device
machine learning: An algorithms and learning theory perspective,” arXiv
preprint arXiv:1911.00623, 2019.
[24] Y. Chen, H. Fan, B. Xu, Z. Yan, Y. Kalantidis, M. Rohrbach, S. Yan, and
J. Feng, “Drop an octave: Reducing spatial redundancy in convolutional
neural networks with octave convolution,” in Proceedings of the IEEE
International Conference on Computer Vision, 2019, pp. 3435–3444.
[25] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, “Xnor-net:
Imagenet classification using binary convolutional neural networks,” in
European conference on computer vision. Springer, 2016, pp. 525–542.
[26] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman
coding,” arXiv preprint arXiv:1510.00149, 2015.
[27] N. Wang, J. Choi, D. Brand, C.-Y. Chen, and K. Gopalakrishnan,
“Training deep neural networks with 8-bit floating point numbers,” in
Advances in neural information processing systems, 2018, pp. 7675–
7684.
[28] Q. Lou, F. Guo, M. Kim, L. Liu, and L. Jiang., “Autoq:
Automated kernel-wise neural network quantization,” in International
Conference on Learning Representations, 2020. [Online]. Available:
https://openreview.net/forum?id=rygfnn4twS
[29] F. Tung and G. Mori, “Clip-q: Deep network compression learning by inparallel pruning-quantization,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2018, pp. 7873–7882.
[30] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” arXiv preprint arXiv:1503.02531, 2015.
[31] M. Phuong and C. Lampert, “Towards understanding knowledge distillation,” in International Conference on Machine Learning, 2019, pp.
5142–5151.
[32] M. Goldblum, L. Fowl, S. Feizi, and T. Goldstein, “Adversarially robust
distillation,” arXiv preprint arXiv:1905.09747, 2019.
[33] L. Wang and A. Wong, “Covid-net: A tailored deep convolutional neural
network design for detection of covid-19 cases from chest radiography
images,” arXiv preprint arXiv:2003.09871, 2020.
[34] X. Li, X. Li, D. Pan, and D. Zhu, “On the learning property of logistic
and softmax losses for deep neural networks,” 2020.
[35] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular
margin loss for deep face recognition,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2019, pp.
4690–4699.
[36] B. Neyshabur, S. Bhojanapalli, D. McAllester, and N. Srebro, “Exploring
generalization in deep learning,” in Advances in Neural Information
Processing Systems, 2017, pp. 5947–5956.
[37] R. S. of North America, “RSNA pneumonia detection challenge,” https:
//www.kaggle.com/c/rsna-pneumonia-detection-challenge, 2018.
[38] J. P. Cohen, P. Morrison, and L. Dao, “Covid-19 image data collection,”
arXiv preprint arXiv:2003.11597, 2020.
[39] P. Rajpurkar, J. Irvin, A. Bagul, D. Ding, T. Duan, H. Mehta, B. Yang,
K. Zhu, D. Laird, R. L. Ball et al., “Mura: Large dataset for abnormality detection in musculoskeletal radiographs,” arXiv preprint
arXiv:1712.06957, 2017.

