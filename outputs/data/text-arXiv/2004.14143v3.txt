Zero-Shot Learning and its Applications from
Autonomous Vehicles to COVID-19 Diagnosis: A Review
Mahdi Rezaei
1
2

arXiv:2004.14143v3 [cs.CV] 29 Nov 2020

1

1,

, Mahsa Shahidi

2,★

Institute for Transport Studies, The University of Leeds, Leeds, LS2 9JT, UK
Department of Computer Engineering, Qazvin Azad University, Qazvin, IR
m.rezaei@leeds.ac.uk 2 m.shahidi@qiau.ac.ir

ABSTRACT
The challenge of learning a new concept, object, or a new medical disease recognition without receiving any examples beforehand is called
Zero-Shot Learning (ZSL). One of the major issues in deep learning based methodologies such as in Medical Imaging and other real-world
applications is the requirement of large annotated datasets prepared by clinicians or experts to train the model. ZSL is known for having
minimal human intervention by relying only on previously known or trained concepts plus currently existing auxiliary information. This is
ever-growing research for the cases where we have very limited or no annotated datasets available and the detection/recognition system has
human-like characteristics in learning new concepts. This makes the ZSL applicable in many real-world scenarios, from unknown object
detection in autonomous vehicles to medical imaging and unforeseen diseases such as COVID-19 Chest X-Ray (CXR) based diagnosis. In
this review paper, we introduce a novel and broaden solution called Few/one-shot learning, and present the definition of the ZSL problem
as an extreme case of the few-shot learning. We review over fundamentals and the challenging steps of Zero-Shot Learning, including
state-of-the-art categories of solutions, as well as our recommended solution, motivations behind each approach, their advantages over each
category to guide both clinicians and AI researchers to proceed with the best techniques and practices based on their applications. Inspired
from different settings and extensions, we then review through different datasets inducing medical and non-medical images, the variety of
splits, and the evaluation protocols proposed so far. Finally, we discuss the recent applications and future directions of ZSL. We aim to
convey a useful intuition through this paper towards the goal of handling complex learning tasks more similar to the way humans learn. We
mainly focus on two applications in the current modern yet challenging era: coping with an early and fast diagnosis of COVID-19 cases, and
also encouraging the readers to develop other similar AI-based automated detection/recognition systems using ZSL.
Keywords – COVID-19 Pandemic; SARS-CoV-2; Chest X-Ray (CXR); Zero-Shot Learning; Deep Learning; Semantic Embedding; Machine
Learning; Autonomous Vehicles; Supervised Annotation.

1 Introduction
bject recognition is one of the highly researched
areas of computer vision. Recent recognition models
have led to great performance through established
techniques and large annotated datasets. After several years of
research, the attention over this topic has not only dimmed but
it has been proven that there are still ways and rooms to refine
models to eliminate existing issues in this area. The number of
newly emerging unknown objects are growing. Some examples
of these unseen or rarely-seen objects are futuristic object
designs like the next generation of concept cars, other existing
concepts but with restricted access to them (such as licensed or
private medical imaging datasets), or rarely seen objects (such
a traffic signs with graffiti on them), or fine-grained categories
of objects (such as detection of COVID-19 in comparison
with the easier task of detecting a common pneumonia). This
brings the necessity of developing a fresh way of solving object
recognition problems that concern lesser human supervision
and lesser annotated datasets. Several approaches have tried to
gather web images to train the developed deep learning models,

O

Corresponding Author: m.rezaei@leeds.ac.uk (M. Rezaei)

but aside from the problem of the noisy images, the searched
keywords are still a form of human supervision. One-Shot
learning (OSL) and Few-shot learning (FSL) are two solutions
that are able to learn new categories via one or a few images,
respectively [1], [2], [3].
Natural language processing (NLP) is another major area of
research in AI and the application of Few-shot learning in the
integration of NLP and object recognition has become a hot
topic recently. [4] was the first FSL-based model to improve the
performance of an NLP system. Zero-shot learning (ZSL) [5],
[6], [7], [8], [9] is an emerging research which is completely
free of any laborious task of data collection and annotation by
experts. Zero-shot learning is a novel concept and learning
technique without accessing any exemplars of the unseen
categories during training, yet it is able to build recognition
models with the help of transferring knowledge from previously
seen categories and auxiliary information. The auxiliary
information may include textual description, attributes, or
vectors of word labels. This means the ZSL is interdisciplinary
by nature with two inseparable components of visual and
textual data.
One of the interesting facts about ZSL is its similarity with

(a) Positive COVID-19

(b) Axial chest CTs with visible GGO patches, consolidation, and parenchymal thickening

Figure 1. Posterior-Anterior (PA)/ Anterior-Posterior (AP) Chest X-Rays and the corresponding CT images of COVID-19

patients
the way human learns and recognise a new concept without
seeing them beforehand. For example, a ZSL-based model
would be able to automatically learn and diagnose COVID-19
patients, based on the existing chest X-ray images of patients
with asthma and lungs inflammatory diseases which are already recognised and labelled by clinicians, plus some new
auxiliary information about the COVID-19 attributes. Here,
the auxiliary data can be the description of physicians and
clinicians about the unique type of visual patterns, features,
damages, or differences they have noticed on the Chest X-ray
of patients with positive COVID-19 comparing to asthma
X-ray images. A similar concept or approach is applicable in
autonomous vehicles, [10], where a self-driving car is responsible for automatic detection of surrounding cars including
e.g. an unseen Tesla concept car based on the subgroup of
labelled classic sedan cars plus auxiliary information about the
common differences of concept cars than the classic cars; or
recognising a Persian deer, based on the auxiliary information
available for it and its appearance similarities or differences
with other previously known deer. For instance, it belongs to
a subgroup of the fallow deer, but with a larger body, bigger
antlers, white spots around the neck, and also flat antlers for
the male type.
Figure 1(a) shows three examples of Posterior-Anterior (PA)
and AP projection of chest X-rays of positive cases of COVID19, and Figure 1(b) represents their corresponding axial CT
scans, taken from the COVID-ChestXRay dataset [11]. As it
can be seen in the images, common evident anomalies may
include unilateral or bilateral patchy ground-glass opacities
(GGOs), patchy consolidations and parenchymal thickening.
The goal of this research is to build an artificial intelligence
based-model that can diagnoses COVID-19 without providing
any visual exemplars in the training phase. In that case, the side
(auxiliary) information should be provided to assist diagnosis

in the test phase. In Figure 2, the auxiliary information is
provided in the form of textual descriptions for two examples
of concept cars and COVID-19 X-rays. In Figure 2(a) we aim
at distinguishing new unseen concept cars (bottom row), using
description on the exterior of the target and how it differs an
already learned car from existing classic vehicle classification
system such as in [12]. Similarly, visual differences and
similarities between healthy Chest X-rays, Asthma cases, and
COVID-19 positive cases are described in Figure 2(b) as the
auxiliary information.
Let’s assume our pre-trained AI-based medical imaging
system is capable of detecting Asthma cases, based on
common deep learning techniques using a previously large
dataset of labelled Asthma Chest X-ray images. However,
these days we are facing an unknown COVID-19 pandemic
with very limited annotated Chest X-rays. Obviously, we
can not proceed on the same way of training traditional
deep-learning methods, due to very sparse labelled images for
COVID-19. The good point is that our medical experts and
clinicians can provide some auxiliary information (textual
descriptions) about common features and similarities among
the COVID-19 positive chest X-rays to infer their findings. In
Figure 3, the side information is provided in form of what
“attributes”: such as foggy effects, white spot features, blurred
edges, and white/low-intensity pixel dominance in various
areas of the chest X-ray images of COVID-19 patients.
Our idea behind the utilisation of ZSL models is to detect,
understand, and recognise new concepts using an existing
similar deep-learning based classifier, plus the integration of
auxiliary information. This turns it to a completely new and
efficient detector/recogniser or diagnosing system without the
requirement of collecting a new dataset and a vast amount
of costly and time-consuming labelling, especially when a
2/26

(a) Concept cars auxiliary information: “The body of the car has a singular and

(b) COVID-19 X-ray auxiliary information: “Bilateral multifocal

unified shape with smoother curves. The wheels’ colour, curves, and design
match the body as a singular integrated piece. LED lights are omnipresent all
around the car.”

patchy GGOs and consolidation can be seen. Edges are blurred
and the intensity sharpness of both lungs have decreased.”

Figure 2. Similarities and differences between seen and unseen examples derived from textual descriptions and train and test

images. The test images are concept cars (a) and COVID-19 symptoms (b).

speedy solution is crucial and life-saving; such as the recent
global pandemic.
In this research we will have four main contributions, as
follows:
• We propose to categorise the reviewed approaches based
on the embedding spaces that each model uses to
learn/infer unseen objects/concepts as well as describing the variations to the data embedding inside those
embedding spaces (Figure 3 and Table 1).
• We evaluate the performance of the state-of-the-art models on famous benchmark datasets (Table 3–5, Fig. 4). To
the best of our knowledge, we are the first to include the
evaluation of data-synthesising methods in the research
field of applied Zero-shot learning.
• We study the motivation behind leveraging each space as
a way to solve the ZSL challenge by reviewing current
issues and solutions to them.
• We provide sufficient technical justifications to support
the ideas of using the proposed ZSL model as one of the
best practices for COVID-19 diagnosis and other similar
applications.
The rest of the materials in the article is organised as
follows. In Section 2, we introduce the problem of Few-shot,
One-shot and Zero-shot learning. In Section 3, we discuss
about the test and train phases of the Zero-shot learning
and generalised Zero-shot learning systems. Section 4
provides with embedding approaches followed by evaluation
protocols in Section 5. In Section 6, we analyse the outcome

of the experiments performed on different state-of-the-art
methodologies. Further discussion about the applications of
ZSL is investigated in Section 7. In Section 8, we discuss the
outcome of this research, and finally, the concluding remarks
in Section 9.

2 Few-Shot / One-Shot and Zero-Shot Learning
Few-shot learning (FSL) is the challenge of learning novel
classes with a tiny training dataset of one or a few images per
category. FSL is closely related to knowledge transfer where a
model, previously trained on large data, is used for a similar task
with fewer training data. The more the transferred knowledge
is accurate, the better FSL will generalise. Moreover, many
approaches employ meta-learning to learn the challenge of fewshot or few-example learning [13], [14]. The main challenge
is to improve the generalisation ability as it often faces the
overfitting problem.
In this type of learning, there is an auxiliary dataset that
contains 𝑁 classes each having 𝐾 annotated samples of the
new examples in the training phase. This makes the problem
into a N-way-K-shot classification:
𝑁𝑡
𝐷 𝑠 = {(𝑥𝑖 , 𝑦 𝑖 )}𝑖=1

(1)

where 𝑥𝑖 is the 𝑖 𝑡 ℎ training example and 𝑦 𝑖 is its corresponding
label. 𝑁𝑡 = 𝐾 × 𝑁 denotes the number of N categories and 𝐾
defines the number of examples. Few-shot learning has 𝐾 > 1
samples.
Among the relevant research works, [15] use the shared features among classes to compensate for the requirement for the
3/26

large data, and follows a learning procedure based on boosted
decision stumps. HDP-DBM [16] develops a compound of a
deep Boltzmann machine and a hierarchical Dirichlet process
to learn the abstract knowledge at different hierarchies of the
concept categories. [13] Proposes prototypical networks that
computes Euclidean distance between prototype representations of each class. It was not until recently that Few-shot
learning was introduced in computer-aided diagnosis. For
the first time, the idea of using additional information (attributes) in FSL, was introduced in [4]. [17] proposes a model
to classify skin lesions. [18] use FSL for Glaucoma Diagnosis
from fundus images. [19] study the problem of chest X-ray
classification of five symptoms including Consolidation.
In the case of one-shot learning, there is only 𝐾 = 1 example
per class in the supporting set, thus it faces more challenge
in comparison to the FSL. Bayesian Program Learning (BPL)
framework [20] present each concept of the handwritten characters as a simple probabilistic program. [21] proposes crossgeneralisation algorithm. It replaces the features from the
previously learned classes with similar features of the novel
classes to adapt to the target task. In Bayesian learning, [22]
depicts prior knowledge in the form of probability density
function on the parameters of the model, and updates them
to compute the posterior model. Matching Nets (MN) [23]
uses non-parametric attentional memory mechanisms, and
an “episode” during the training time. [24] capture salient
features of general lung datasets using an encoder and augment
multiple views for images, then use the prototypical network
for a 2-way, 1-shot classification.
Zero-shot learning is the extreme case of the FSL where
𝐾 = 0. In other words, the difference between the two is the
devoid of any visual examples of the target classes in the
training phase of ZSL, while in few-shot learning, the support
set contains few labelled samples of the novel categories. Also,
auxiliary information in the form of class embeddings is one of
the main components of Zero-shot learning. ZSL approaches
might extend their solutions to one-shot or few-shot learning
by either updating the training data with one or few generated
samples from augmentation techniques, or by having access to
a few of the unseen images during the training time [25], [26],
[27], [28], [29], [30], [31] [32], [33], [34], [25]. [34] and [25]
both use auxiliary text-based information.

3 ZSL Test and Training Phases
ZSL models can be seen from two points of views in terms
of training and test phase: Classic ZSL and Generalised ZSL
(GZSL) settings. In the classic ZSL settings, the model only
detects the presence of new classes at the test phase, while
in GZSL settings, the model predicts both unseen and seen
classes at the test time; hence, GZSL is more applicable for
real-world scenarios [35], [36], [37], [38], [25]. The same
idea can be applied to FSL to train in the generalised model,
called generalised few-shot learning (GFSL) that detects both
known and novel classes at the test time.
In the next paragraphs, we discuss two types of training

approaches: Inductive vs. Transductive training.
Inductive Training: This training setting only uses the seen
class of information to learn a new concept. The training data
for the inductive setting is:
𝐷 𝑡 = {(𝑥, 𝑦, 𝑐(𝑦))|𝑥 ∈ 𝑋 𝑆 , 𝑦 ∈ 𝑌 𝑆 , 𝑐(𝑦) ∈ 𝐶 𝑆 }

(2)

where 𝑥 represents image features, 𝑦 is the class labels, and
𝑐(𝑦) denotes the class embeddings. Moreover, 𝑋 𝑆 and 𝑌 𝑆
indicate seen class images and seen class labels, respectively.
Inductive learning accounts for the majority of the settings
used in ZSL and Generalised Zero-Shot Learning (GZSL). e.g.
in [6], [39], [40], [41], [42], [43], [44], [45], [46], [31], [34].
Transductive Training: Although the original idea of zeroshot learning is more related to the inductive setting, in
many scenarios, the transductive setting is used where either
unlabelled visual or textual information, or both for unseen
classes are used together with the seen class data e.g. in
in [47], [48], [49], [50], [51], [52], [42], [45], [46], [31], [53],
[54], [34], [55]. The training data for transductive learning is:
𝐷 𝑡 = {(𝑥, 𝑦, 𝑐(𝑦))|𝑥 ∈ 𝑋 𝑆∪𝑈 , 𝑦 ∈ 𝑌 𝑆∪𝑈 , 𝑐(𝑦) ∈ 𝐶 𝑆∪𝑈 }(3)
where 𝑋 𝑆∪𝑈 denotes that images come from the union of seen
and unseen classes. Similarly, 𝑌 𝑆∪𝑈 and 𝐶 𝑆∪𝑈 indicate the
train labels and class embeddings belong to both seen and
novel categories.
According to [56], any approach that relies on label propagation will fall into the category of transductive learning. Feature
generating network with labelled source data and unlabelled
target data [34] are also considered as transductive methods.
The transductive setting is seen as one of the solutions to
the domain shift problem, since the provided unseen labelled
information during training reduces the discrepancy between
the two domains.
There is a slight nuance between the transductive learning
and semi-supervised learning; in the transductive setting, the
unlabelled data solely belong to the unseen test classes, while
in semi-supervised setting, unseen test classes might not be
present in the unlabelled data. Furthermore, the difference
between FSL and the transductive ZSL learning is the existence
of a few labelled examples of the unseen classes alongside
annotated seen class examples in the few-shot learning. While
in the transductive ZSL setting, the examples for the unseen
classes are all unlabelled.
ZSL models are developed based on two high-level
major strategies to be taken into account: a) defining the
“Embedding Space” to combine visual and non-visual auxiliary
data, and b) choosing an appropriate “Auxiliary Data
Collection” technique.
a) Embedding Spaces. Figure 3 demonstrates the overall
structure of a ZSL system in terms of embedding spaces
and auxiliary data types collection techniques. Such systems
either map the visual data to the semantic space (Figure 3.a)
4/26

CNN

Feature space 𝑥˜

Latent space
Semantic space 𝑐(𝑦)
(a)

(b)

Foggy effects;
White spots;
Dense inflammations;
Bright-pixels dominance

(c)

Figure 3. Overview of ZSL models. Typical approaches use one of the three embedding types or a combination of them. (a)

Semantic embedding models that map visual features to the semantic space. (b) Models that map visual and semantic features
to an intermediate latent space. (c) Visual embedding models that map semantic features to the visual space.

or embed both visual and semantic data to a common latent
space (Figure 3.b), or see the task as a missing data problem,
and then map the semantic information to the visual space
(Figure 3.c). Two or all of these approaches can also be
combined and embedded together to boost up the benefits of
each individual categories.
From a different point of view, semantic spaces can
also be sub-categorised into euclidean and non-euclidean
spaces. The intrinsic relationship between data points is
better preserved when the geometrical relation between
them is considered. These spaces are commonly based
on clusters or graph networks. Some researchers may
prefer manifold learning for the ZSL challenge. e.g.
in [47], [57], [45], [52], [58], [59], [60], [61], [62], [37].
The Euclidean spaces are more conventional and simpler as
the data has a flat representation in such spaces. However,
the loss of information is a common issue of these spaces,
as well. Examples of methods using Euclidean spaces
are [5], [39], [41], [63], [64], and [25].
b) Auxiliary Data Collection. As mentioned before, Zero-shot
learning is the challenge of learning novel classes without
seeing their exemplars during the training. Instead, the freely
available auxiliary information is used to compensate for
the lack of visually labelled data. Such information can be
categorised into two groups:
Human annotated attributes. The supervised way of annotating each image with its related attributes is an arduous
process and requires time and expertise, but since they are manual, they yield noiseless and important attributes needed for
learning and inference. There are several datasets in which side
information in the form of attributes can be attained for each
image. i.e. aPY [65], AWA1 [5], AWA2 [7], CUB [66], and
SUN [67]. Several ZSL methods leverage the attributes as the

side information [6], [41], [68], or visual attributes [69], [65].
Unsupervised auxiliary information. There are several
forms of auxiliary information that have minimum supervision
and are widely used in the ZSL setting, such as human gazes
[70], WordNet which is a large-scale lexical database of
117,000 English words [71], [72], [28], [6], [73], [74], [75],
[76], [77], [60], [61], or Textual descriptions such as Web
search [72], Wikipedia articles [39], [40], [78], [6], [79],
[75], [76], [8], [80], [81], and sentence descriptions [82].
Textual side information needs to be transformed into class
embeddings in order to be used at the training stage and testing
stages. Word embedding and language embedding are the two
representation techniques used for textual side information. As
we gradually proceed, later we review on different embedding
classes as well.

4 ZSL Data Embedding Techniques
In this section, we first provide the task definition of ZSL
and GZSL. Then we review the four recent approaches on the
problem.
In the standard inductive setting as mentioned earlier in
Section 3, the training set is
𝐷 𝑡 = {(𝑥, 𝑦, 𝑐(𝑦))|𝑥 ∈ 𝑋 𝑆 , 𝑦 ∈ 𝑌 𝑆 , 𝑐(𝑦) ∈ 𝐶 𝑆 }

(4)

and the objective function to be minimised is as follows:
L=

𝑁
1 ∑︁
𝐿(𝑦 𝑛 , 𝑓 (𝑥 𝑛 ;𝑊)) + Ω(𝑊)
𝑁 𝑛=1

(5)

where, 𝑓 (𝑥, 𝑦;𝑊) = argmax𝐹 (𝑥, 𝑦) is the mapping function.
𝑦 ∈𝑌

Through the training phase, the classifier 𝑓 : 𝑋 → 𝑌 𝑈 is
learned for ZSL to predict only the novel classes at the test
5/26

time, or 𝑓 : 𝑋 → 𝑌 𝑈 ∪𝑌 𝑆 for the GZSL challenge to estimate
both novel classes and the previously learned seen classes.
For instance, the classifier 𝑓 can be a COVID-19 diagnoser.
We categorise the embedding methodologies into four categories based on the space they learn/infer target classes (like
COVID-19 detection in Figure 3):
1. Semantic Embedding: A semantic space with textual
nature in which features are in the form of class embeddings.
2. Intermediate-Space Embedding: A space where both
class embeddings and visual feature embedding present
in conjunction.

label embedding vectors. [29] use a random forest approach
for learning more discriminative attributes. Hierarchy and
Exclusion (HEX) [86] considers relations between objects
and attributes and maps the visual features [87], [88] of
the images to a set of scores to estimate labels for unseen
categories. [89] take on an unsupervised approach where they
capture the relations between the classes and attributes with
a three-dimensional tensor while using a DAP-based scoring
function to infer the labels. LAGO by [90] also follow the
DAP model. It learns soft and-or logical relations between
attributes. Using soft-OR, the attributes are divided into
groups, and the label class from unseen samples is predicted
via a soft-AND within these groups. If each attribute comes
from a singleton group, the all-AND will be used.
4.1.2 Label Embedding

3. Visual Embedding: A space where training and inferring
is done with visual feature representations similar to the
traditional recognition problems.
4. Hybrid Embedding Models: A combination of spaces
are used in some models to bring together the advantages
the different spaces have.
The majority of methods focus on the general tasks; however,
they are scalable to disease classification.
4.1 Semantic Embedding

Semantic embedding itself can be sub categorised into two
tasks of Attribute Classification and Label Embedding which
will be discussed here:
4.1.1 Attribute Classifiers

Primitive approaches of Zero-Shot learning leverage manually
annotated attributes in a two-stage learning schema. Attributes
in an image are predicted in the first stage and labels of unseen
classes would be chosen using similarity measures in the
second stage. [69] use a probabilistic classifier to learn the
attributes and then estimates posteriors for test classes. [71]
propose a method to avoid manual supervision with mining
the attributes in an unsupervised manner. [72] adopt DAP
together with a hierarchy-based knowledge transfer for largescale settings. [83]’s method is based on IAP, and uses SelfOrganising and Incremental Neural Networks (SOINN) to
learn and update attributes online. Later in IAP-SS by [83],
an online incremental learning approach is used for faster
learning of the new attributes. The Direct Attribute Prediction
(DAP) [5] first learns the posteriors of the attributes, then
estimates the posteriors of seen classes. On the other hand,
Indirect Attribute Prediction (IAP) [5] first learns the posteriors
for seen classes then uses them to compute the posteriors for
the attributes. [84] use a unified probabilistic model based on
the Bayesian Network (BN) [85] that discovers and captures
both object-dependent and object-independent relationships
to overcome the problem of relating the attributes. ConSE
[40] learns the probability of the training samples. It then
predicts an unseen class by the convex combination of the class

Instead of using an intermediate step, more recent approaches
learn to map images to the structured euclidean semantic space
automatically which would be the implicit way of representing
knowledge. The compatibility function for linear mapping is:
𝐹 (𝑥, 𝑦; 𝑤) = 𝜃 (𝑥)𝑇 𝑤𝑐(𝑦)

(6)

where 𝜃 (𝑥)𝑇 is the image embedding for training classes and
𝑤 is the parameters in vector form to be learned. In the case
of bilinear projection where it is more common, 𝑤 takes the
form of matrix:
𝐹 (𝑥, 𝑦;𝑊) = 𝜃 (𝑥)𝑇 𝑊 𝑐(𝑦)

(7)

SOC [91] first maps the image features to the semantic embedding space, it then estimates the correct class using nearest
neighbour. DeViSE by [39] uses a linear corresponding function with a combination of dot-product similarity and hinge
rank loss used in [92]. ALE [50] optimises the ranking loss
in [93] alongside the bi-linear mapping compatibility function. SJE [6] learns a bi-linear compatibility function using
the structural SVM objective function [94]. ESZSL [41]
introduces a better regulariser and optimises a close form
solution objective function in a linear manner. ZSLNS [76]
proposes a 𝑙 1,2 -norm based loss function. [30] take on a metric
learning approach and linearly embed the visual features to
the attribute space. LAGO [90] is a probabilistic model that
depicts soft and-or relations between groups of attributes. In
a case where all attributes form all-OR group, It becomes
similar to ESZSL [41] and learns a bilinear compatibility
function. AREN [95] uses attentive region embedding while
learning the bilinear mapping to the semantic space in order
to enhance the semantic transfer. ZSLPP [8] combines two
networks VPDE-net for detecting bird parts from images and
PZSC-net that trains a part-based Zero-Shot classifier from
the noisy text of the Wikipedia. DSRL [56] uses non-negative
sparse matrix factorisation to align vector representations with
the attribute-based label representation vectors so that more
relevant visual features are passed to the semantic space.
Some approaches to ZSL use non-linear compatibility functions. CMT [96] use a two-layer neural network, similar to
6/26

common MLP networks by [97] alongside the compatibility
function. In UDA [48] a non-linear projection from feature
space to semantic space (word vector and attribute) is proposed in an unsupervised domain adaptation problem based on
regularised sparse coding. [79] use a deep neural network [87]
regression which generates pseudo attributes for each visual
category via Wikipedia. LATEM [73] constructs a piecewise non-linear compatibility function alongside a ranking
loss. [32] regularise the model using structural relations of the
cluster by which cluster centres characterise visual features.
QFSL by [53] solves the problem in a transductive setting, and
projects both sources and target images into several specified
points to fight bias problem.
GFZSL [31] uses both linear and non-linear regression models and generates a probability distribution for each class. For
transductive setting, it uses Expectation-Maximisation (EM)
to estimate a Gaussian Mixture Model (GMM) of unlabelled
data in an iterative manner.
Leveraging the non-euclidean spaces to capture the manifold
structure of the data is another approach to the problem.
Together with the knowledge graphs, the explicit relations
between the labels will be demonstrated. In this setting, the
side information mainly comes from a hierarchy ontology like
WordNet. The mapping function will have the following form:
𝐹 (𝑥, 𝑦;𝑊) = 𝜃 (𝑋, 𝐴)𝑇 𝑊 𝑐(𝑦)

(8)

where 𝑋 is the 𝑛 × 𝑘 feature matrix and 𝐴 is the adjacency
matrix of the graph.
Propagated Semantic Transfer (PST) [47] first uses DAP model
to transfer knowledge to novel categories, following the graphbased learning schema, it improves local neighbourhood in
them. DMaP [59] jointly optimises the projecting of the visual
features and the semantic space to improve the transferability of
the visual features to the semantic space manifold. MFMR [58]
decomposes the visual feature matrix into three matrices to
further facilitate the mapping of visual features to the semantic
spaces. To improve the representation of the geometrical
manifold structure of the visual and semantic features, manifold
regularisation is used. In [61] a Graph Search Neural Network
(GSNN) [77] is used in the semantic space based on the
WordNet knowledge graph to predict multiple labels per image
using the relations between them. [60] distils both auxiliary
information in forms of word embedding and knowledge
graph to learn novel categories. DGP [62] proposes dense
graph propagation to propagate knowledge directly through
dense connections. In [37] a graphical model with a low
dimensional visually semantic space is utilised which has
a chain-like structure to close the gap between the highdimensional features and the semantic domain.
4.2 Intermediate-Space Embedding

One of the methods of embedding is to measure the similarity
between the visual and semantic features in a joint space.

4.2.1 Fusion-based Models

Considering unseen classes as a fusion of previously learned
seen concepts is called hybrid learning. Standard scoring
function for hybrid models is defined as:
∑︁
𝑓 (𝑥, 𝑦;𝑊) =
(𝑊, 𝜃 𝑆 (𝑥))𝑐(𝑦)
(9)
𝑠 ∈𝑆

SSE [43] considers the histogram similarity between the
seen class auxiliary information and seen visual data. In
SYNC [44] uses two spaces of semantic and model space, and
the alignment is conducted with phantom classes. With the
sparse linear combination of the classifiers for the phantom
classes, the final classifier is learned. TVSE [52] learns a
latent space using collective matrix factorisation with graph
regularisation to incorporate the manifold structure between
source and target instances, moreover, it represents each sample
as a mixture of seen class scores. LDF [98] combines the
prototypes of seen classes and jointly learns embeddings for
both user-defined attributes and latent attributes.
4.2.2 Joint Representation Space Models

Inferring unseen labels via measuring similarity between crossmodal data in a shared latent space is another workaround to
the ZSL challenge. The first term in the objective function for
standard cross-modal alignment approaches is:
L = min k𝑥 𝑆 − 𝑐(𝑦) 𝑆 𝑦 𝑆 k 2𝐹
𝑐 ( 𝑦) 𝑆

(10)

with 𝑌 being a One-hot vector of corresponding class labels and
k.k 2𝐹 is the Frobenius norm. Approaches to joint space learning
are grouped into two categories, Parametric which follow a
slow learning via optimising a problem and Non-parametric
that leverage data points extracted from neural networks in a
shared space. In parametric methods including [49] a multiview alignment space is proposed for embedding low-level
visual features. The learning procedure is based on the multiview Canonical Correlation Analysis (CCA) [99]. [74] applies
PCA and ICA embeddings to reveal the visual similarity
across the classes and obtains the semantic similarity with
the WordNet graph, followed by embedding the two outputs
into a common space. MCZSL [75] uses visual part and
multi-cue language embedding in a joint space. In [100] both
images and words are represented by Gaussian distribution
embeddings. JLSE [51] decides on a dictionary learning
approach to learn the parameters of source and target domains
across two separate latent spaces where the similarity is
computed by the likelihood of similarity independent to the
class label. CDL [101] uses a coupled dictionary to align
the structure of visual-semantic space using discriminative
information of the visual space. In [102] and [103] a coupled
sparse dictionary is leveraged to relate visual and attribute
features together. It uses entropy regularisation to alleviate
the domain shift problem.
There are several non-parametric methods. ReViSE [33]
that combines auto-encoders with Maximum Mean Discrepancy (MMD) loss [104] in order to align the visual and textual
7/26

features. DMAE [105] introduces a latent alignment matrix
with representations from auto-encoders optimised by kernel target alignment (KTA) [106] and squared-loss mutual
information (SMI) [107]. DCN [35] proposes a novel Deep
Calibration Network in which an entropy minimisation principle is used to calibrate the uncertainty of unseen classes as
well as seen classes.
To narrow the semantic gap, BiDiLEL [108] introduces a
sequential bidirectional learning strategy and creates a latent
space using the visual data, then the semantic representations
of unseen classes are embedded in the previously created
latent space. This method comprises both parametric and
non-parametric models.
4.3 Visual Embedding

Visual embedding is the other type of ZSL methods that
performs classification in the original feature space and is
orthogonal to semantic space projection. This is done by
learning a linear or non-linear projection function. For linear
corresponding functions, WAC-Linear [78] uses textual description for seen and unseen categories and projects them to
the visual feature space with a linear classifier. [45] follows a
transductive setting in which it refines unseen data distributions using unseen image data. To approximate the manifold
structure of data, they used a global linear mapping for synthesising virtual cluster centres. [42] assigns pseudo labels to
samples using reliability (with robust SVM) and diversity (via
diversity regularisation). For learning Ia Non-linear corresponding function, In WAC-Kernel [109] in order to leverage
any kind of side information, a kernel method is proposed
to predict a kernel-based on the representer theorem [110].
DEM [111] uses the least square embedding loss to minimise
the discrepancy between the visual features and their class
representation embedding vector in the visual feature space.
OSVE [112] reversely maps from attribute space to visual
space then trains the classifier using SVM [113]. In [114] the
authors introduce a stacked attention network that corporates
both global and local visual features weighted by relevance
along with the semantic features. In [54] visual constraint is
used in class centres in the visual space to avoid the domain
shift problem.
4.3.1 Visual Data Augmentation

There are a variety of generative networks that augment
unseen data, taking GAN [115] as an example, the first term
in objective function would be:
L = max E[log 𝐷 (𝑥, 𝑐(𝑦))] + min E[log(1 − 𝐷 ( 𝑥,
˜ 𝑐(𝑦))]
(11)
𝑥˜ = 𝐺 (𝑧, 𝑐(𝑦)) is the synthesised data of the generator and
𝑧 ∈ 𝑅 𝑑𝑧 is random Gaussian noise. The role of the discriminator 𝐷 and generator 𝐺 contradicts in loss function as the
first one attempts to maximise the loss while the latter tries to
minimise it.
Another widely used generative neural network is the Varia-

tional AutoEncoder (VAE) [116]:
L = E𝑞 𝜙 (𝑧 | 𝑥) [log 𝑝 𝜃 (𝑥|𝑧)] − 𝐷 𝐾 𝐿 (𝑞 𝜙 (𝑧|𝑥)|| 𝑝 𝜃 (𝑧))
(12)
The first term is the reconstruction loss, and the latter is the
Kullback-Leibler divergence that works as a regulariser.
RKT [57] leverages relational knowledge of the manifold
structure in the semantic space, and generates virtually labelled
data for unseen classes from Gaussian distributions generated
by sparse coding. Then it projects them alongside the seen data
to the semantic space via linear mapping. GLaP [46] generates
virtual instances of an unseen class with the assumption that
each representation obeys a prior distribution where one can
draw samples from. To ease the embedding to the semantic
space, GANZrl [117] proposes to increase the visual diversity
by generating samples with specified semantics using GAN
models. SE-GZSL [36] uses a feedback-driven mechanism for
its discriminator that learns to map the produced images to the
corresponding class attribute vectors. To enforce the similarity
of the distribution of the sample and generated sample, a loss
component was added to the VAE objective [116] function.
Synthesised images often suffer from looking unrealistic
since they lack intricate details. A way around this issue is to
generate features instead. [118] uses a GMMN model [119] to
generate visual features for unseen classes. In [120] a multimodal cycle consistency loss is used in training the generator
for better reconstruction of the original semantic features.
CVAE-ZSL [64] takes attributes and generates features for the
unseen categories via a Conditional Variations AutoEncoder
(CVAE) [121]. 𝐿 2 norm is used as the reconstruction loss.
GAZSL [81] utilises noisy textual descriptions from Wikipedia
to generate visual features. A visual pivot regulariser is
introduced to help generate features with better qualities. fCLSWGAN [63] combines three conditional GAN variants
for a better data generation. f-VAEGAN-D2 [34] combines
the architectures of conditional VAE [121], GAN [115] and
a non-conditional discriminator for the transductive setting.
LisGAN [122] generates unseen features from random noises
using conditional Wasserstein GANs [123]. For regularisation,
they introduced semantically meaningful soul samples for each
class and forced the generated features to be close to at least one
of the soul samples. Gradient Matching Network (GMN) [55]
trains an improved version of the conditional WGAN [124] to
produce image features for the novel classes. It also introduces
Gradient Matching (GM) loss to improve the quality of the
synthesised features. In order to synthesise unseen features,
SPF-GZSL [38] selects similar instances and combines them
to form pseudo features using a centre loss function [125]. In
Don’t Even Look Once (DELO) by [126] a detection algorithm
is conducted to synthesise unseen visual features to gain high
confidence predictions for unseen concepts while maintaining
low confidence for backgrounds with vanilla detectors.
Instead of augmenting data using synthesising methods,
data can be acquired by gathering web images. [80] jointly use
web data which are considered weakly-supervised categories
8/26

alongside the fully-supervised auxiliary labelled categories.
It then learns a dictionary for the two categories.
4.4 Hybrid Embedding Models

Several works make use of both visual and semantic projections to reconstruct better semantics to confront domain
shift issue by alleviating the contradiction between the two
domains. Semantic AutoEncoder (SAE) [127] adds a visual
feature reconstruction constraint. It combines linear visual-tosemantic (encoder) and linear semantic-to-visual (decoder).
SP-AEN [128] is a supervised Adversarial AutoEncoder [132]
which improves preserving the semantics by reconstructing
the images from the raw 256 x 256 x 3 RGB colour space.
BSR [129] uses two different semantic reconstructing regressors to reconstruct the generated samples into semantic
descriptions. CANZSL [130] combines feature-synthesis with
semantic embedding by using a GAN for generating visual
features and an inverse GAN to project them into semantic
space. In this way, the produced features are consistent with
their corresponding semantics.
Some of the synthesising approaches utilise a common
latent space to align the generated features space with the
semantic space to facilitate capturing the relations between the
two spaces. [68] introduce a latent-structure-preserving space
where synthesised features from given attributes would suffer
less from bias and variance decay with the help of Diffusion
Regularisation. CADA-VAE [25] generates a visual feature
latent space where both of visual and semantic features are
embedded in this space by a VAE [116]. It uses Distribution
Alignment (DA) loss and Cross-Alignment (CA) loss to align
the cross-modal latent distributions.
GDAN [131] combines all three approaches and designs a
dual adversarial loss. In this way, regressor and discriminator
learn from each other.
A summary of the different approaches is reported in Table 1.
The number of methods are growing with time and we can
interpret that some areas like direct learning, common space
learning and visual data synthesising are more popular in
solving the task, while models combining different approach
are fairly newer techniques thus have fewer works that are
reported here.

5 Evaluation Protocols
In this section, we review some of the standard evaluation
techniques to analyse the performance of the ZSL techniques
based on the common benchmark datasets in the field, also in
terms of dataset splits, class embeddings, image embeddings,
and various evaluation metrics. First, the benchmark datasets.

Images are categorised based on their visual attributes. A new
version of this dataset is proposed by [8] in which the identical
leaf nodes are merged to their parent nodes where their only
differences were genders and resulted in final 404 classes.
Attribute datasets. SUN Attribute [67] is a medium-scale
and fine-grained attribute dataset consisting of 102 attributes,
717 categories and a total of 14,340 images of different
scenes. CUB-200-2011 Birds (CUB) [66] is a 200 category
fine-grained attribute dataset with 11,788 images of bird
species that includes 312 attributes. Animals with Attributes
(AWA1) [5] is another attribute dataset of 30,475 images with
50 categories and 85 attributes, the image features in this
dataset are licensed and not available publicly. Later, Animals
with Attributes 2 (AWA2) was presented by [7] which is a
free version of AWA1 with more images than the previous
one (37,322 images), with the same number of classes and
attributes, but different images. aPascal and Yahoo (aPY) [65]
is a dataset with a combination of 32 classes, including 20
pascal and 12 yahoo attribute classes with 15,339 images and
64 attributes in total.
A summary of the statics for the attribute datasets are
gathered in Table 2.
ImageNet. ImageNet [134] is a large-scale dataset that
contains 14 million images, shared between 21k categories
with each image having one label that makes it a popular
benchmark to evaluate models in real-world scenarios. Its
organisation is based on WordNet hierarchy [135]. ImageNet
is imbalanced between classes as the number of samples in
each class vary greatly and is partially fine-grained. A more
balanced version has 1k classes with 1000 images in each
category.
There are several approaches in FSL setting for COVID-19
diagnosis, however ZSL is still new in the field of disease
recognition, we introduce a dataset suited for the task of
ZSL/GZSL that contains the required image and textual
descriptions in one place.
COVID-ChestXRay. COVID-ChestXRay [11] is a small
and public dataset of CXR and CT scans suitable for ZSL and
Few-shot learning experiences. At the time of this research,
it had 444 unique clinical notes for a total of 16 categories,
from no finding (normal cases) to other pneumonic cases like
COVID-19, MERS, and SARS.
5.2 Dataset Splits

Here we discuss the original splits of the datasets as well as
the other splits proposed for the Zero-shot problem.
5.1 Benchmark Datasets

There are several well-known benchmark datasets for
Zero-shot learning that are frequently used.
North America Birds (NAB) [133] is a fine-grained dataset
of birds consisting of 1,011 classes and 48,562 images.

Standard Splits (SS). In ZSL problems, unseen classes
should be disjoint to seen classes and test time samples limited
to unseen classes, thus the original splits aim to follow this
setting. SUN [67] proposed to use 645 classes for training
9/26

Table 1. Common ZSL and GZSL methods categorised based on their embedding space model, with further divisions in a

top-down manner.
Models

Categories

Main Features

Two-Step
Learning

Attributes classifiers

Semantic
Embedding

Description
DAP-Based [69], [71], [72], [5], [89], [90] IAP-Based [69], [83], [5], [40]
Bayesian network (BN) [84], Random Forest Model [29], HEX Graph [86]

Implicit knowledge
representation

Linear [91], [39], [50], [6], [41], [76], [30], [90], [95], [8], [56], [31] or
Non-Linear [48], [79], [73], [32], [53], [31]Compatibility Functions

Explicit knowledge
representation

Graph Conv. Networks (GCN) [60], Knowledge Graphs [61], [47], [59], [62],
3-Node Chains [37], Matrix Tri-Factorisation with Manifold Regularisation [58]

Fusion-based
Models

Fusion of seen class data

Combination of seen classes properties [43], [44], [98], Combination of seen
class scores [52]

Common
Representation
Space Models

Mapping of the visual and
semantic spaces in a joint
intermediate space

Parametric [49], [74], [75], [100], [51], [101], [102], [103],
Non-parametric [33], [105], [35], or Both [108]

Visual Space
Embedding

Learning of the semantic to
visual projection

Linear [78], [45], [42] or Non-linear [109], [111], [112], [114], [54] Projection
functions

Direct Learning

Cross-Modal
Latent
Embedding

Image generation
Visual
Embedding

Hybrid

Data
Augmentation

Visual feature generation

Gaussian distribution [57], [46], GAN [117], VAE [36]
GAN [120], [81], [122], WGAN [63], [55], CVAE [64], [126], VAE+GAN [34],
GMMN [118], Similar feature combination [38]

Leveraging Web
Data

Web images crawling

Visual+Semantic
Embedding

Reconstruction of the semantic
features

AutoEncoder [127], Adversarial AutoEncoder [128], GAN with two
reconstructing regressors [129], GAN an inverse GAN [130]

Visual+Cross
Modal
Embedding

Feature generation with aligned
semantic features

Semantic to visual mapping [68], VAE [25]

All

Utilisation of generator and
discriminator together with the
regressor

GAN + Dual Learning [131]

among which 580 of the classes are used for training, 65
classes are for validation and the remaining 72 classes will be
used for testing. For CUB, [50] introduces the split of 150
training classes (including 50 validation classes) and 50 test
classes. As for AWA1, [5] introduced the standard split of
40 classes for training (13 validation classes) and 10 classes
for testing. The same splits are used for AWA2. In aPY, 20
classes of Pascal are used for training (15 classes for training
and 5 for validation), while the 12 classes of Yahoo are used
for testing.
Proposed Splits (PS). The standard split images from
SUN, CUB, AWA1 and aPY overlap with some images
of pre-trained ResNet-101 ImageNet model. To solve
the problem, proposed splits (PS) is introduced by [136]
where no test images are contained in the ImageNet 1K dataset.
ImageNet. [136] proposes 9 ZSL splits for the ImageNet
dataset; two of which evaluate the semantic hierarchy in
distance-wise scales of 2-hops (1509 classes) and 3-hops

Dictionary learning [80]

(7678 classes) from the 1k training classes. The remaining
six splits consider the imbalanced size of classes with
increasing granularity splits starting from 500, 1K and 5K
least-populated classes to 500, 1K and 5K most-populated
classes, or All which denotes a subset of 20k other classes for
testing.
Seen-Unseen relatedness. To measure the relatedness of
seen samples to unseen classes, [8] introduces two splits
Super-Category-Shared (SCS) and Super-Category-Exclusive
(SCE). SCS is the easy split since it considers the relatedness
to the parent category while SCE is harder and measures the
closeness of an unseen sample to that particular child node.
5.3 Class Embeddings

There exist several class embeddings, each suitable for a
specific scenario. Class embeddings are in forms of vectors of
real numbers which can further be used to make predictions
based on the similarity between them and can be obtained
through three categories: attributes, word embeddings, and
10/26

Table 2. Statics of the attribute datasets accounting for the number of attributes, classes plus their splits and their total number

of images.
Attribute Datasets
SUN [67]
CUB [66]
AWA1 [5]
AWA2 [7]
aPY [65]

#attributes

𝑦

𝑦𝑈

𝑦𝑆

#images

102
312
85
85
64

717
200
50
50
32

580+65
100+50
27+13
27+13
15+5

72
50
10
10
12

14,340
11,788
30,475
37,322
15,339

hierarchical ontology. The last two are done in an unsupervised
manner thus do not require human labour.
5.3.1 Supervised Attribute-Embeddings

Human annotated attributes are done under the supervision
of experts with a great amount of effort. Binary, relative and
real-valued attributes are three types of attributes embeddings.
Binary attributes depict the presence of an attribute in an
image thus value is either 0 or 1. They are the easiest type and
are provided in benchmark attribute datasets AWA1, AWA2,
CUB, SUN, aPY. Relative attributes [137] on the other hand,
show the strength of an attribute in a given image comparing to
the other images. The real-valued attributes are in continuous
form thus they have the best quality [6]. In the SUN attribute
dataset [67], they have achieved confidence through averaging
the binary labels from multiple annotators.
5.3.2 Unsupervised Word-Embeddings

Also known as Textual corpora embedding. Bag of Words
(BOW) [138] is a one-hot encoding approach. It simply shows
the number of occurrences of the words in a representation
called bag and is negligent of word orders and grammar.
One-hot encoding approaches had a drawback of giving the
stop words (like "a", "the" and "of") high relevancy counts.
Later, Term Frequency-Inverse Document Frequency (TFIDF): [139] used term weighting to alleviate this problem
by filtering the stop words and to keep meaningful words.
Word2Vec [140], a widely used two-layered neural embedding
model and has two variants, CBOW and skip-gram. CBOW
predicts a target word in the centre of a context using its
surroundings words while the skip-gram model predicts surrounding words using a target word. CBOW is faster in train
and usually results in better accuracy for frequent words while
Skip-gram is preferred for rare words and it works well with
sparse training data. Global Vectors (GloVe) [141] is trained
on Wikipedia. It combines local context window methods and
global matrix factorization. Glove learns to consider global
word-word co-occurrence matrix statistics to build the word
embeddings.
5.3.3 Hierarchy Embedding

WordNet [135] is a large-scale public lexical database of
117,000 synsets. Synsents are a group of words that are
semantically related to each other. i.e. synonyms, homonyms
and meronymies of English words that are organised using the

hierarchy distances with a graph structure, thus Approaches
based on knowledge graphs often follow the WordNet to
measure the similarity between the word meanings [71], [72],
[28], [6], [73], [74], [75], [76], [77], [60], [61].
5.3.4 Language Modelling

In the general ZSL scenarios, word by word representations
considered; however, with the advent of transfer learning in
the natural language processing (NLP), and the introduction of
contextual word embeddings, the boundaries of the capabilities
of the embeddings has been pushed further. Unlike the
traditional word embeddings, language models can capture
the meaning of the words based on the context in which they
appear. Several contextual representations that have been
introduced recently and showed great results. These existing
pre-trained models can be fined tuned on various ZSL tasks.
ELMo [142] is a contextual embedding model. Following morphological clues together with a deep bidirectional
language model (biLM), ELMo learns the representations.
Bidirectional Encoder Representations from Transformer
(BERT) [143] is a multi-layer bidirectional Transformer encoder [144] trained upon BooksCorpus [145] dataset and
English Wikipedia. It outperforms ELMo with having more
parameters and layers.The pre-trained BERT model can be
fine-tuned with just one additional output layer. However,
BERT suffers from fine-tuning discrepancy due to ignoring
the relation the masked positions have. XLNet [146] uses
an autoregressive model to introduce a method that overcomes the shortcoming of BERT. In addition to the datasets
used by BERT, XLNet pre-trains the model on Giga5 [147],
ClueWeb 2012-B extended by [148] and Common Crawl∗ ∗.
ALBERT [149] increases the model size. It lowers the memory
usage with two parameter reduction techniques. The first one
is a factorized embedding parameterization. The second one
is cross-layer parameter sharing. These two techniques result
in lower memory usage and higher training speed than BERT.
The data used for pre-training is the same as XLNeT.
In this article, we report the results of ZSL and GZSL using
the same class embeddings as [136] that is Word2Vec trained
on Wikipedia for ImageNet and per-class attributes for the
attribute datasets, and for the seen-unseen relatedness task
we follow [8] and consider TF-IDF for the CUB and NAB
datasets.
∗∗ http://commoncrawl.org

11/26

5.4 Image Embeddings

6 Experimental Results

Existing models use either shallow or deep feature representation. Examples of shallow features are SIFT [150],
PHOG [151], SURF [152] and local self-similarity histograms [153]. Among the mentioned features, SIFT is
the commonly used features in ZSL models like [50], [44]
and [49].
Deep features are obtained from deep CNN architectures [87] and contain higher-level features. Extracted features
are one of the followings:

As the main contributions of this research, and for the first
time, we provide a comprehensive experiments of 21 stateof-the-art models in ZSL/GZSL domain that include the
evaluations and comparisons of data-synthesising methods.
In this section, first we provide the results for ZSL, GZSL and
seen-unseen relatedness on attribute datasets, then we present
the experimental results on the ImageNet dataset. A minor part
of the results is reported from [7] for a more comprehensive
comparison.

4,096-dim top-layer hidden unit activations (fc7) of the
AlexNet [154], 1000-dim last fully connected layer (fc8) of
VGG-16 [155], 4,096-dim of the 6th layer (fc6) and 4,096-dim
of the last layer (fc7) features of the VGG-19 [155]. 1,024-dim
top-layer pooling units of the GoogleNet [156]. and 2048-dim
last layer pooling units of the ResNet-101 [157].
In this paper, we consider the ResNet-101 network which is
pre-trained on ImageNet-1K without any fine-tuning. That
is the same image embedding used in [136]. Features are
extracted from whole images of SUN, CUB, AWA1, AWA2,
and ImageNet and the cropped bounding boxes of aPY. For
the seen-unseen relatedness task, VGG-16 is used for CUB
and NAB as proposed in [8].

5.5 Evaluation Metrics

Common evaluation criteria used for ZSL challenge are:
Classification accuracy. One of the simplest metrics is
classification accuracy in which the ratio of the number of
the correct predictions to samples in class 𝑐 is measured.
However, it results in a bias towards the populated classes.
Average per-class accuracy. To reduce the bias problem
for the populated classes, average per-class accuracies are
computed by multiplying the division of the classification
accuracy to division of their cumulative sum.
k𝑦 k

𝑎𝑐𝑐 𝑦 =

1 ∑︁ #correct predictions in class 𝑦
k𝑦k 𝑦=1
#samples in class 𝑦

[7]

Harmonic mean. For performance evaluation on both seen
and unseen classes (i.e. the GZSL setting), the Top-1 accuracies for the seen and unseen classes are used to compute the
harmonic mean:
𝐻=

2 ∗ 𝑎𝑐𝑐 𝑦 𝑆 ∗ 𝑎𝑐𝑐 𝑦𝑈
𝑎𝑐𝑐 𝑦 𝑆 + 𝑎𝑐𝑐 𝑦𝑈

[7]

In this paper, we designate the Top-1 accuracies and the
harmonic mean as the evaluation protocols.

6.1 Zero-Shot Learning Results

For the original ZSL task where only unseen classes are being
estimated during the test time, we compare 21 state-of-the-art
models in Table 3, among which, DAP [5], IAP [5] and ConSE
[40] belong to attribute classifiers. CMT [96], LATEM [73],
ALE [50], DeViSE [39], SJE [6], ESZSL [41], GFZSL [31] and
DSRL [56] are from compatibility learning approaches, SSE
[43] and SYNC [44] are representative models of cross-modal
embedding, DEM [111], GAZSL [81], f-CLSWGAN [63],
CVAE-ZSL [64], SE-ZSL [36] are visual embedding models.
From the hybrid or combination category, we compare the
results of SAE [127]. Three transductive approaches ALEtran [50], GFZSL-tran [31] and DSRL [56] are also presented
among the selected models. Due to the intrinsic nature of
the transductive setting, the results are competitive and in
some cases better than the inductive methods, i.e. for GFZSLtran [31] the accuracy is 9.9% higher than CVAE-ZSL [64]
for PS split of AWA1 dataset. However, in comparison with
the inductive form of the same model, there are cases where
the inductive model has better accuracies. i.e. in PS split
of the aPY dataset, the performance is 38.4% vs 37.1% or
for ALE-tran [50] model in PS split of SUN it’s 58.1% vs
55.7%, also for PS split of CUB it is 54.9% vs 54.5% with its
inductive type. GFZSL [31], a compatibility-based approach,
has the best scores compared to other models of the same
category in every dataset except for the CUB where SJE [6]
tops the results in both splits. This superiority could be due
to the generative nature of the model. GFZSL [31] performs
the best on AWA1 both in inductive and transductive settings.
Out of cross-modal methods, SYNC [44] performs better than
SSE [43] in SUN and CUB datasets, while for AWA1, AWA2
and aPY in SS split it has lower performance than SSE [43] in
the proposed split. Visual generative methods have proved to
perform better as they make the problem into the traditional
supervised form, among which, SE-ZSL [36] has the most
outstanding performance. For the proposed split in one case on
CUB dataset, SE-ZSL [36] performs better than ALE-tran [50]
which is its transductive counterpart where the accuracies are
59.6% vs 54.5%. In PS split of AWA1, CVAE-ZSL [64] stays
at the top, with 1.9% higher accuracy than the second-best
performing model. The accuracies for SS splits are higher
than PS in most cases and the reason could be the test images
included in training samples, especially for AWA1 and AWA2,
as reported in [136].
12/26

Table 3. Zero-shot learning results for the Standard Split (SS) and Proposed Split (PS) on SUN, CUB, AWA1, AWA2, and aPY

datasets. We measure Top-1 accuracy in % for the results. †and ‡denote inductive and transductive settings respectively
Methods

†

‡

SUN

CUB

AWA1
SS
PS

AWA2
SS
PS

SS

PS

SS

PS

DAP [5]
IAP [5]
ConSE [40]
CMT [96]
SSE [43]
LATEM [73]
ALE [50]
DeViSE [39]
SJE [6]
ESZSL [41]
SYNC [44]
SAE [127]
GFZSL [31]
DEM [111]
GAZSL [81]
f-CLSWGAN [63]
CVAE-ZSL [64]
SE-ZSL [36]

38.9
17.4
44.2
41.9
54.5
56.9
59.1
57.5
57.1
57.3
59.1
42.4
62.9
64.5

39.9
19.4
38.8
39.9
51.5
55.3
58.1
56.5
53.7
54.5
56.3
40.3
60.6
61.9
61.3
60.8
61.7
63.4

37.5
27.1
36.7
37.3
43.7
49.4
53.2
53.2
55.3
55.1
54.1
33.4
53.0
60.3

40.0
24.0
34.3
34.6
43.9
49.3
54.9
52.0
53.9
53.9
55.6
33.3
49.3
51.7
55.8
57.3
52.1
59.6

57.1
48.1
63.6
58.9
68.8
74.8
78.6
72.9
76.7
74.7
72.2
80.6
80.5
83.8

44.1
35.9
45.6
39.5
60.1
55.1
59.9
54.2
65.6
58.2
54.0
53.0
68.3
68.4
68.2
68.8
71.4
69.5

58.7
46.9
67.9
66.3
67.5
68.7
80.3
68.6
69.5
75.6
71.2
80.7
79.3
80.8

ALE-tran [50]
GFZSL-tran [31]
DSRL [56]

-

55.7
64.0
56.8

-

54.5
49.3
48.7

-

65.6
81.3
74.7

-

6.2 Generalised Zero-Shot Learning Results

A more real-world scenario where previously learned concepts
are estimated alongside new ones is necessary to experiment.
21 state-of-the-art models, same as ZSL challenge, include:
DAP [5], IAP [5], ConSE [40], CMT [96], SSE [43], LATEM
[73], ALE [50], DeViSE [39], SJE [6] ,ESZSL [41], SYNC
[44], SAE [127], GFZSL [31], DEM [111], GAZSL [81],
f-CLSWGAN [63], CVAE-ZSL [64], SE-GZSL [36], ALEtrain [50], GFZSL-tran [31], DSRL [56]. CADA-VAE [25]
is added to the comparison as a model combining the visual
feature augmentation approach with the cross-modal alignment.
CMT* [96] has a novelty detection and is included in the report
as an alternative version to CMT [96]. The reports in Table 4
are in PS splits. As shown in the table, the results on 𝑦 𝑆 are
dramatically higher than 𝑦𝑈 since in GZSL, the test search
space includes seen classes as well as unseen classes, this gap
is the most conspicuous in attribute classifiers like DAP [5]
that performs poorly on AWA1 and AWA2, hybrid approaches
and in GFZSL [31] where it results in 0% accuracy on SUN
and CUB when training classes are estimated at test time.
However for three models f-CLSWGAN [63], SE-GZSL [36]
and CADA-VAE [25] in SUN dataset, the accuracy for 𝑦𝑈
is higher than 𝑦 𝑆 , i.e. for SE-GZSL [36] it is 10.4% higher.
For a fair comparison, the weighted average of training and
test classes is also reported. According to harmonic means,
the best model on all evaluated datasets is SE-ZSL [36],
although the results haven’t been reported for aPY. In some
cases, the attribute classifier achieves the best results on 𝑦 𝑆 .
Transductive models have fluctuating results in comparison
with their inductive types. CADA-VAE [25] achieves the best
performance in all of the harmonic means cases (results for

aPY
SS

PS

46.1
35.9
44.5
37.9
61.0
55.8
62.5
59.7
61.9
58.6
46.6
54.1
63.8
67.1
68.4
68.2
65.8
69.2

35.2
22.4
25.9
26.9
31.1
34.5
30.9
35.4
32.0
34.4
39.7
8.3
51.3
-

33.8
36.6
26.9
28.0
34.0
35.2
39.7
39.8
32.9
38.3
23.9
8.3
38.4
35.0
41.1
40.5
-

70.7
78.6
72.8

-

46.7
37.1
45.5

aPY are not reported) and shows the best results, higher than
all of the transductive methods.
6.3 Seen-Unseen Relatedness Results

For fine-grained problems, sometimes it is important to measure the closeness of previously known concepts to novel
unknown ones. For this purpose, a total of eleven models
are compared in Table 5. MCZS [75], WAC-Linear [78],
WAC-Kernel [109], ESZSL [41], SJE [6], ZSLNS [76],
SynC 𝑓 𝑎𝑠𝑡 [44], SynC𝑂𝑉 𝑂 [44], ZSLPP [8], GAZSL [81]
and CANZSL [130]. SCE is the hard split thus has lower results compared to the SCS splits. The two variations reported
for SYNC [44] model, SynC 𝑓 𝑎𝑠𝑡 denotes the setting in which
the standard Crammer-Singer loss is used, and SynC 𝑓 𝑎𝑠𝑡 [44]
depicts setting with one-versus-other classifiers. The first
setting has better accuracies on CUB. CANZSL [130] outperforms all other models in both datasets and splits and improves
the accuracy by 4% from 10.3% to 14.3% on SCE split of
the CUB dataset and 35.6% vs 38.1% in SCS splits of NAB
compared to the next best performing model is GAZSL [81].
Similar to previous experiments, in the seen-unseen relatedness challenge, models that contain feature generating steps
have the highest results.
6.4 Zero-Shot Learning Results on ImageNet

ImageNet is a large-scale single-labelled dataset with an
imbalanced number of data that possesses WordNet hierarchy
instead of human-annotated attributes, thus is useful mean to
measure the performance of various methods in recognitionin-the-wild scenarios. The performances of 12 state-of-the-art
models are reported here. They are ConSE [40], CMT [96],
LATEM [73], ALE [50], DeViSE [39], SJE [6] ,ESZSL [41],
13/26

Table 4. Generalised Zero-Shot Learning results for the Proposed Split (PS) on SUN, CUB, AWA1, AWA2, and aPY datasets.
We measure the Top-1 accuracy in % for seen (𝑦 𝑆 ), unseen (𝑦𝑈 ) and their harmonic mean (H). †and ‡denote inductive and
transductive settings, respectively.

†

‡

Methods

𝑦𝑈

SUN
𝑦𝑆

H

𝑦𝑈

CUB
𝑦𝑆

H

𝑦𝑈

AWA1
𝑦𝑆

H

𝑦𝑈

AWA2
𝑦𝑆

H

𝑦𝑈

aPY
𝑦𝑆

H

DAP [5]
IAP [5]
ConSE [40]
CMT [96]
CMT* [96]
SSE [43]
LATEM [73]
ALE [50]
DeViSE [39]
SJE [6]
ESZSL [41]
SYNC [44]
SAE [127]
GFZSL [31]
DEM [111]
GAZSL [81]
f-CLSWGAN [63]
CVAE-ZSL [64]
SE-GZSL [36]
CADA-VAE [25]

4.2
1.0
6.8
8.1
8.7
2.1
14.7
21.8
16.9
14.7
11.0
7.9
8.8
0.0
20.5
21.7
42.6
40.9
47.2

25.1
37.8
39.9
21.8
28.0
36.4
28.8
33.1
27.4
30.5
27.9
43.3
18.0
39.6
34.3
34.5
36.6
30.5
35.7

7.2
1.8
11.6
11.8
13.3
4.0
19.5
26.3
20.9
19.8
15.8
13.4
11.8
0.0
25.6
26.7
39.4
26.7
34.9
40.6

1.7
0.2
1.6
7.2
4.7
8.5
15.2
23.7
23.8
23.5
12.6
11.5
7.8
0.0
19.6
23.9
43.7
41.5
51.6

67.9
72.8
72.2
49.8
60.1
46.9
57.3
62.8
53.0
59.2
63.8
70.9
54.0
45.7
57.9
60.6
57.7
53.3
53.5

3.3
0.4
3.1
12.6
8.7
14.4
24.0
34.4
32.8
33.6
21.0
19.8
13.6
0.0
29.2
34.3
49.7
34.5
46.7
52.4

0.0
2.1
0.4
0.9
8.4
7.0
7.3
16.8
13.4
11.3
6.6
8.9
1.8
1.8
32.8
25.7
57.9
56.3
57.3

88.7
78.2
88.6
87.6
86.9
80.5
71.7
76.1
68.7
74.6
75.6
87.3
77.1
80.3
84.7
82.0
61.4
67.8
72.8

0.0
4.1
0.8
1.8
15.3
12.9
13.3
27.5
22.4
19.6
12.1
16.2
3.5
3.5
47.3
39.2
59.6
47.2
61.5
64.1

0.0
0.9
0.5
0.5
8.7
8.1
11.5
14.0
17.1
8.0
5.9
10.0
1.1
2.5
30.5
19.2
52.1
58.3
55.8

84.7
87.6
90.6
90.0
89.0
82.5
77.3
81.8
74.7
73.9
77.8
90.5
82.2
80.1
86.4
86.5
68.9
68.1
75.0

0.0
1.8
1.0
1.0
15.9
14.8
20.0
23.9
27.8
14.4
11.0
18.0
2.2
4.8
45.1
31.4
59.4
51.2
62.8
63.9

4.8
5.7
0.0
1.4
10.9
0.2
0.1
4.6
4.9
3.7
2.4
7.4
0.4
0.0
11.1
14.2
32.9
-

78.3
65.6
91.2
85.2
74.2
78.9
73.0
73.7
76.9
55.7
70.1
66.3
80.9
83.3
75.1
78.6
61.7
-

9.0
10.4
0.0
2.8
19.0
0.4
0.2
8.7
9.2
6.9
4.6
13.3
0.9
0.0
19.4
24.1
42.9
-

ALE-tran [50]
GFZSL-tran [31]
DSRL [56]

19.9
0
17.7

22.6
41.6
25.0

21.2
0
20.7

23.5
24.9
17.3

45.1
45.8
39.0

30.9
32.2
24.0

25.9
48.1
22.3

-

-

12.6
31.7
20.8

73.0
67.2
74.7

21.5
43.1
32.6

8.1
0.0
11.9

-

-

Table 5. Seen-Unseen relatedness results on CUB and NAB

datasets with easy (SCS) and hard (SCE) splits. Top-1
accuracy is reported in %
Methods
MCZSL [75]
WAC-Linear [78]
WAC-Kernel [109]
ESZSL [41]
SJE [6]
ZSLNS [76]
SynC 𝑓 𝑎𝑠𝑡 [44]
SynC𝑂𝑉 𝑂 [44]
ZSLPP [8]
GAZSL [81]
CANZSL [130]

CUB
SCS
SCE

NAB
SCS
SCE

34.7
27.0
33.5
28.5
29.9
29.1
28.0
12.5
37.2
43.7
45.8

11.4
24.3
24.5
18.4
30.3
35.6
38.1

5.0
7.7
7.4
7.3
8.6
5.9
9.7
10.3
14.3

6.0
6.3
6.8
3.8
8.1
8.6
8.9

SYNC [44], SAE [127], f-CLSWGAN [63], CADA-VAE
[25] and f-VAEGAN-D2 [34]. All of the Top-1 accuracies,
except for the data generating models are reported from [136]
experiments. As it can be understood from Figure 4a, Feature
generating methods have outstanding performance compared
to other approaches. Although the results of f-VAEGAND2 [34] are available only for 2H, 3H and all splits, it still
has the highest accuracies among other models. SYNC [44]
and f-CLSWGAN [63] are the next best performing models
with approximately the same accuracies. ConSE [40] is a
representative model from attribute-classifier based models, as

it is also superior to direct compatibility approaches. ESZSL
[41], a model with linear compatibility function outperforms
the other model within its category. However, in one case,
SJE [6] has slightly better accuracy in L500 split setting. It can
be interpreted from the figures that on coarse-grained classes,
the results are conspicuously better, while fine-grained classes
with few images per class have more challenges. However, if
the test search space is too big then the accuracies decrease.
i.e. M5K has lower accuracies compared to L500 splits, and
on 20K split, it is the lowest.
The GZSL results are important in the way that they depict
the models’ ability to recognise both seen and unseen classes
at the test time. The results for the SYNC [44] model is only
reported in the L5K setting. As shown in Figure 4a, the trend
is Similar to ZSL where populated classes have better results
than the least populated classes, yet have poor results if the
search spaces become too big like the decreasing trends in
most and least populated classes. Moreover, data-generating
approaches dominate other strategies. CADA-VAE [25] that
has the advantages of both cross-modal alignment and data
feature synthesising methods, evidently outperforms other
models. In one case, i.e M500, it nearly has double the
accuracy of f-CLSWGAN [63]. For the semantic embedding
category, although ESZSL [41] had better results on ZSL,
it falls behind approaches like ALE [50], DeViSE [39] and
SJE [6].

14/26

ConSE

ConSE

10

CMT

15

CMT

LATEM

LATEM

8

DeViSE
SJE

10
ESZSL
SYNC
SAE
f-CLSWGAN

5

Top-1 Acc. (in %)

Top-1 Acc. (in %)

ALE

ALE
DeViSE
SJE

6

ESZSL
SYNC
SAE

4

f-CLSWGAN

CADA-VAE
f-VAEGAN-D2

0

CADA-VAE
f-VAEGAN-D2

2

0
2H

3H

M500

M1K

M5K

L500

L1K

L5K

All

(a) Zero-Shot Learning

2H

3H

M500

M1K

M5K

L500

L1K

L5K

All

(b) Generalised Zero-Shot Learning

Figure 4. ImageNet results measured with Top-1 accuracy in % for the 9 splits including 2 and 3 hops away from ImageNet-1K
training classes (2H and 3H) and 500, 1K and 5K most (M) and least (L) populated classes, and All the remaining
ImageNet-20K classes.

7 Applications
During the very recent years, zero-shot learning has proved to
be a necessary challenge to-be-solved for different scenarios
and applications. The number of demands for learning
without accessing to the unseen target concepts is also
increasing each year.
Zero-shot learning is widely discussed in the computer
vision field, such as object recognition in general, as in [12] and
[158] where they aim to locate the objects beside recognising
them. Several other variations of ZSL models are proposed for
the same task purpose such as [159], [160] and [161]. Zeroshot emotion recognition [162] has the task of recognising
unseen emotions while zero-shot semantic segmentation aims
to segment the unseen object categories [163] and [164].
Moreover, on the task of retrieving images from a large scale
set of data, Zero-shot has a growing number of research [165]
[166] along with sketch-based image retrieval systems [167],
[168] and [169]. Zero-shot learning has an application on
visual imitation learning to reduce human supervision by
automating the exploration of the agent [170], [171]. Action
recognition is the task of recognising the sequence of actions
from the frames of a video. However, if the new actions
are not available when training, Zero-shot learning can be a
solution, such as in [172], [173], [174] and [175]. Zero-shot
Style Transfer in an image is the problem of transferring the
texture of source image to target image while the style is not
pre-determined and it is arbitrary [176]. Zero-shot resolution
enhancement problem aims at enhancing the resolution of an
image without pre-defined high-resolution images for training
examples [177]. Zero-shot scene classification for HSR images
[178] and scene-sketch classification has been studied in [179]
as other applications of ZSL in computer vision.

Zero-shot learning has also left its footprint in the area of
NLP. Zero-Shot Entity Linking, links entity mentions in the
text using a knowledge base [180]. Many research works
focus on the task of translating languages to another without
pre-determined translation between pairs of samples [181],
[182], [183], [184]. In sentence embedding [185] and in Style
transfer of text, a common technique is to convert the source
to another style via arbitrary styles like the artistic technique
discussed in [186].
In the audio processing field, zero-shot based voice conversion
to another speaker’s voice [187] is an applicable scenario of
ZSL.
In the era of the COVID-19 pandemic, many researchers
have tried to work on Artificial Intelligence and Machine
learning based methodologies to recognise the positive cases
of the COVID-19 patients based on the CT scan images or
Chest X-rays. Two prominent features in chest CT used for
diagnosis are ground glass opacities (GGO) and consolidation
which has been considered by some of the researchers such
as [188], [189], [190], and [191]. [192] uses three CNN model
to detect COVID-19, in which the ResNet50 shows a very
high rate of classification performance. [191] introduces a
deep-learning based system that segments the infected regions
and the entire lung in an automatic manner. [193] shows
that the increase in unilateral or bilateral Procalcitonin and
consolidation with surrounding halo is prominent in chest CT
of paediatric patients. [194] introduces the COVNet to extract
the 2D local and 3D global features in 3D chest CT slices.
The method claims the ability of classifying COVID-19 from
community acquired pneumonia (CAP). [195] shows different
imaging patterns of the COVID-19 cases depending on the
time of infection. [196] classifies four stages to respiratory
CT scan changes and shows the most dramatic changes to be
15/26

in the first 10 days from the onset of initial symptoms. [197]
introduce a deep learning based anomaly detection model
which extracts the high-level features from the input chest Xray image. [198] introduce COVIDX-Net to classify the positive
cases for the COVID-19 in X-ray images. It uses 7 different
architectures, which VGG19 outperforms the others. [199]
propose a COVID-CAPS that is based on the Capsule Networks
[200] to avoid the drawbacks of CNN-based architectures as
it captures better spatial information. It performs on a small
dataset of X-ray images. [201] employ a class decomposition
mechanism in DeTraC [202] which is a deep convolutional
network that can handle image dataset irregularities of the
X-ray images. Zhang et al. [203] propose a method for Xray medical image segmentation using task driven generative
adversarial networks. [204] proposes a 21-layer CNN called
CheXNet, trained on the ChestX-ray14 dataset [205] to detect
pneumonia with the localisation of the most infected areas
from the X-ray images. [206] shows a possible diagnostic
criteria could be the existence of bilateral pulmonary areas
of consolidation found in the chest X-rays, and [207] use
DenseNet-169 for the purpose of feature extraction followed
by an SVM classifier to detect Pneumonia from chest X-ray
images.
A common weakness among the majority of the abovementioned research works is that they either conduct their
evaluations on a very limited number of cases due to the lack of
comprehensive datasets (which puts the validity of the reported
results under a question), or they suffer from underlying
uncertainties due to unknown nature and characteristics of the
novel COVID-19, not only for the medical community, but
also for the machine learning and data analytic experts. In
such an uncertain atmosphere with limited training dataset,
we strongly recommend the adaptation of Zero-shot learning
and its variances (as discussed in Figure 4) as an efficient deep
learning based solution towards COVID-19 diagnosis.
Diagnosis and recognition of the very recent and global
challenge of COVID-19 disease caused by the severe acute
respiratory syndrome Coronavirus 2 (SARS-CoV-2) is a perfect real-world application of Zero-shot learning, where we
do not have millions of annotated datasets available; and the
symptoms of the disease and the chest X-ray of infected people
may significantly vary from person to person. Such a scenario
can truly be considered as a novel unseen target or classification challenge. We only know some of the symptoms of
the infected people with COVID-19 in forms of advices, text
notes, chest X-ray interpenetration, all as the auxiliary data
which have partial similarities with other lung inflammatory
diseases, such as Asthma or SARS. So, we have to seek for
a semantic relationship between training and the new unseen
classes. Therefore, ZSL can help us significantly to cope with
this new challenge like the induction of the SARS-CoV-2,
from previously learned diagnosis of the Asthma, and the
Pneumonia using written medical documents of the respiratory tracts and chest X-ray images. In the case of the few-shot
learning, a handful of the chest CT scans or X-ray of the

positive cases of the COVID-19 can also be beneficial as
further support-set alongside the chest X-ray images of SARS,
Asthma and Pneumonia to infer the novel COVID-19 cases.
As a general rule and based on the recent successful applications, we can infer that in any scenarios that the goal is set
to reduce supervision, and the target of the problem can be
learned through side information and its relation to the seen
data, the Zero-shot learning method can be conducted as one
of the best learning techniques and practices.

8 Discussion
A typical zero-shot learning problem is usually faced with
three popular issues that need to be solved in order to enhance
the performance of the model. These issues are Bias, Hubness
and domain shift; and every model revolves around solving
one or more of the issues mentioned. In this section, we
discuss efforts done by different approaches to alleviate bias,
hubness and domain-shift and infer the logic each approach
owns to learn its model.
Bias. The problem with ZSL and GZSL tasks is that the
imbalanced data between training and test classes cause a bias
towards seen classes at prediction time. Other reasons for
bias could be high-dimensionality and the devoid of manifold
structure of features. Several data generating approaches have
worked on alleviating bias by synthesising visual data for
unseen classes. [63] generate semantically rich CNN features
of the unseen classes to make unseen embedding space more
known. [64] generates pseudo seen and unseen class features,
and then it trains an SVM classifier to mitigate bias. [55]
improve the quality of the synthesised examples by using
gradient matching loss. Models combining data generation or
reconstruction along with other techniques have proved to be
effective in alleviating bias. [68] use an intermediate space
to help discover the geometric structure of the features that
previously didn’t with the regression-based projections. [128]
used calibrated stacking rule. [25] generated latent feature
sizes of 64 with the idea that low-dimensional representations
tend to mitigate bias. [129] uses two regressors to calculate
reconstruction to diminish the bias. Transductive-based
approaches like [55] are also used to solve the bias issue.
In [53], it forces the unseen classes to be projected into fixed
pre-defined points to avoid results with bias.
Hubness [208]. In large-dimensional mapping spaces,
samples (hubs) might end up falsely as the nearest neighbours
of several other points in the semantic space and result in an
incorrect prediction. To avoid the hubness, [108] propose a
stage-wise bidirectional latent embedding framework. When
a mapping is done from high-dimensional feature space
to a low-dimensional semantic space using regressors, the
distinctive features will partially fade while in the visual
feature space, the structures are better preserved. Hence, the
visual embedding space is well-known for mitigating the
hubness problem. [54] and [111] use the output of the visual
16/26

space of the CNN as the embedding space.
Domain-shift. Zero-shot learning challenge can be considered as a domain adaptation problem. This is because
the source labelled data is disjoint with the target unlabelled
domain data. This is called project domain-shift. Domain
adaptation techniques are used to learn the intrinsic relationships among these domains and transfer knowledge between
the two. A considerable amount of works has been done
through a transductive setting which has been successful to
overcome the domain-shift issue. [49] a multi-view embedding
framework, performs label propagation on graph a heuristic
one-stage self-learning approach to assign points to their nearest data points. [48] introduces a regularised sparse coding
based unsupervised domain adaptation framework that solves
the domain shift problem. [209] use a structured prediction
method to solve the problem by visually clustering the unseen
data. [54] use a visual constraint on the centre of each class
when the mapping is being learned. Since the pure definition
of the ZSL challenge is the inaccessibility of unseen data
during training, several inductive approaches tried to solve the
problem as well. [127] propose to reconstruct the visual features to alleviate this issue. [56] perform sparse non-negative
matrix factorisation for both domains in a common semantic
dictionary. MFMR [58] exploits the manifold structure of test
data with a joint prediction scheme to avoid domain shift. [103]
use entropy minimisation in optimisation. [38] preserve the
semantic similarity structure in seen and unseen classes to
avoid the domain-shift occurrence. [122] mitigates projection
domain-shift by generating soul samples that are related to the
semantic descriptions.
These three common issues together with inferiorities of
each methods will be a motivation to decide on a particular
approach when solving the ZSL problem. Attribute classifiers
are considered customised since human-annotations are used;
however, this makes the problem a laborious task that has
strong supervision. Compatibility learning approaches have
the ability to learn directly by eliminating the intermediate
step but often face with the bias and hubness problem. Manifold learning solves this weakness of the semantic learning
approaches by preserving the geometrical structure of the
features. Cross-modal latent embedding approaches take on a
different point of view and leverage both visual and semantic
features and the similarity and differences between them. They
often propose methods for aligning the structures between the
two modes of features. This category of methods also suffers
from the hubness problem for the problems dealing with highdimensional data. Visual space embedding approaches have
the advantage of turning the problem into a supervised one
by generating or aggregating visual instances for the unseen
classes. Plus are a favourable approach for solving hubness
problem due to the high-dimensionality of the visual space that
can preserve information structure better and also bias problem
by alleviating the imbalanced data by generating unseen class
samples. Here a challenge would be generating more realistic

looking data. Another different setting is transductive learning
that present solutions to bias problem, by creating balance in
data by gathering unseen data, yet not applicable to many of
the real-world problems since the original definition of ZSL
limits the use of unseen data during the training phase.
Depending on the real-world scenarios, each way of solving
the problem might be the most appropriate choice. Some
approaches improve the solution by combining two or more
methods to benefit from each one’s strengths.

9 Conclusion
In this article, we performed a comprehensive and multifaceted review on the Zero-Shot/Generalised Zero-shot Learning challenge, its fundamentals, and variants for different
scenarios and applications such as COVID-19 diagnosis, Autonomous Vehicles, and similar complex real-world applications which involve fully/partially new concepts that have
never/rarely seen before, besides the barrier of limited annotated dataset. We divided the recent state-of-the-art methods
into four space-wise embedding categories. We also reviewed
different types of side and auxiliary information. We went
through the popular datasets and their corresponding splits for
the problem of ZSL. The paper also contributed in performing
the experiment results for some of the common baselines and
elaborated on assessing the advantages and disadvantages of
each group, as well as the ideas behind different areas of
solutions to improve each group. Our evaluation reveals that
data synthesis methods and combinational approaches yield
the best performance, as by synthesising data, the problem
shifts to the classic recognition/diagnosis problem, and by
combining other methods, the model utilises the advantage
of each embedding techniques. The models even outperform
compatibility learning models in transductive setting. This
means, the models consisting a visual data generation step,
lead to better results than other approaches and settings. Furthermore, the accuracies improve when the unseen classes
have closer semantic hierarchy and relatedness distance to
the seen classes. Finally, we reviewed the current and potential real-world applications of ZSL and GZSL in the near
future. To the best of our knowledge, such a comprehensive
and detailed technical review and categorisation of the ZSL
methodologies, alongside with an efficient solution for the
recent challenge of COVID-19 pandemic is not done before;
hence, we expect it to be helpful in developing new research
directions among AI and health-related research community.

References
1. Miller, E. G., Matsakis, N. E. & Viola, P. A. Learning
from one example through shared densities on transforms. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000
(Cat. No. PR00662), vol. 1, 464–471, DOI: https:
//doi.org/10.1109/CVPR.2000.855856 (IEEE, 2000).
17/26

2. Lake, B., Salakhutdinov, R., Gross, J. & Tenenbaum,
J. One shot learning of simple visual concepts. In
Proceedings of the annual meeting of the cognitive
science society, vol. 33:33, DOI: https://doi.org/10.1.1.
207.8634 (2011).
3. Koch, G., Zemel, R. & Salakhutdinov, R. Siamese neural
networks for one-shot image recognition. In ICML deep
learning workshop, vol. 2 (Lille, 2015).
4. Tsai, Y.-H. H. & Salakhutdinov, R. Improving one-shot
learning through fusing side information. arXiv preprint
arXiv:1710.08347 (2017).
5. Lampert, C. H., Nickisch, H. & Harmeling, S. Attributebased classification for zero-shot visual object categorization. IEEE Transactions on Pattern Analysis Mach. Intell.
36, 453–465, DOI: https://doi.org/10.1109/TPAMI.2013.
140 (2013).

processing systems, 4077–4087, DOI: https://dl.acm.org/
doi/10.5555/3294996.3295163 (2017).
14. Kang, B. et al. Few-shot object detection via feature
reweighting. In Proceedings of the IEEE International
Conference on Computer Vision, 8420–8429, DOI: https:
//doi.org/10.1109/ICCV.2019.00851 (2019).
15. Torralba, A., Murphy, K. P. & Freeman, W. T. Shared
features for multiclass object detection (Springer, 2006).
16. Salakhutdinov, R., Tenenbaum, J. B. & Torralba, A.
Learning with hierarchical-deep models. IEEE transactions on pattern analysis machine intelligence 35, 1958–
1971, DOI: https://doi.org/10.1109/TPAMI.2012.269
(2012).
17. Prabhu, V. U. Few-shot learning for dermatological
disease diagnosis. Ph.D. thesis, Georgia Institute of
Technology (2019).

6. Akata, Z., Reed, S., Walter, D., Lee, H. & Schiele,
B. Evaluation of output embeddings for fine-grained
image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2927–2936, DOI: https://doi.org/10.1109/CVPR.2015.
7298911 (2015).

18. Kim, M., Zuallaert, J. & De Neve, W. Few-shot learning
using a small-sized dataset of high-resolution fundus
images for glaucoma diagnosis. In Proceedings of the
2nd International Workshop on Multimedia for Personal
Health and Health Care, 89–92, DOI: https://doi.org/10.
1145/3132635.3132650 (2017).

7. Xian, Y., Schiele, B. & Akata, Z. Zero-shot learning-the
good, the bad and the ugly. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
4582–4591, DOI: https://doi.org/10.1109/CVPR.2017.
328 (2017).

19. Rajan, D., Thiagarajan, J. J., Karargyris, A. & Kashyap,
S. Self-training with improved regularization for
few-shot chest x-ray classification. arXiv preprint
arXiv:2005.02231 (2020).

8. Elhoseiny, M., Zhu, Y., Zhang, H. & Elgammal, A. Link
the head to the" beak": Zero shot learning from noisy text
description at part precision. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
6288–6297, DOI: https://doi.org/10.1109/CVPR.2017.
666 (IEEE, 2017).
9. Wang, W., Zheng, V. W., Yu, H. & Miao, C. A survey of
zero-shot learning: Settings, methods, and applications.
ACM Transactions on Intell. Syst. Technol. (TIST) 10,
1–37, DOI: https://doi.org/10.1145/3293318 (2019).
10. Rezaei, M. & Klette, R. Look at the driver, look at the
road: No distraction! no accident! In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, 129–136, DOI: https://doi.org/10.1109/
CVPR.2014.24 (2014).
11. Cohen, J. P., Morrison, P. & Dao, L. Covid-19 image
data collection. arXiv 2003.11597 (2020).
12. Rezaei, M., M., T. & R., K. Robust vehicle detection
and distance estimation under challenging lighting conditions. IEEE transactions on intelligent transportation
systems 2723–2743, DOI: https://doi.org/10.1109/TITS.
2015.2421482 (2015).
13. Snell, J., Swersky, K. & Zemel, R. Prototypical networks
for few-shot learning. In Advances in neural information

20. Lake, B. M., Salakhutdinov, R. & Tenenbaum, J. B.
Human-level concept learning through probabilistic
program induction. Science 350, 1332–1338, DOI:
https://doi.org/10.1126/science.aab3050 (2015).
21. Bart, E. & Ullman, S. Cross-generalization: Learning
novel classes from a single example by feature replacement. In 2005 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’05),
vol. 1, 672–679, DOI: https://doi.org/10.1109/CVPR.
2005.117 (IEEE, 2005).
22. Fei-Fei, L., Fergus, R. & Perona, P. One-shot learning
of object categories. IEEE transactions on pattern
analysis machine intelligence 28, 594–611, DOI: https:
//doi.org/10.1109/TPAMI.2006.79 (2006).
23. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D.
& Others. Matching networks for one shot learning.
In Advances in neural information processing systems,
3630–3638, DOI: https://doi.org/10.7551/mitpress/7503.
001.0001 (2016).
24. Chen, X., Yao, L., Zhou, T., Dong, J. & Zhang, Y.
Momentum contrastive learning for few-shot covid19 diagnosis from chest ct images. arXiv preprint
arXiv:2006.13276 (2020).
25. Schonfeld, E., Ebrahimi, S., Sinha, S., Darrell, T. &
Akata, Z. Generalized zero-and few-shot learning via
aligned variational autoencoders. In Proceedings of
18/26

the IEEE Conference on Computer Vision and Pattern
Recognition, 8247–8255, DOI: https://doi.org/10.1109/
CVPR.2019.00844 (2019).
26. Yu, X. & Aloimonos, Y. Attribute-based transfer learning
for object categorization with zero/one training example.
In European conference on computer vision, 127–140,
DOI: https://doi.org/10.1007/978-3-642-15555-0-10
(Springer, 2010).
27. Sharmanska, V., Quadrianto, N. & Lampert, C. H. Augmented attribute representations. In European Conference on Computer Vision, 242–255, DOI: https://doi.
org/10.1007/978-3-642-33715-4_18 (Springer, 2012).
28. Akata, Z., Perronnin, F., Harchaoui, Z. & Schmid, C.
Label-embedding for attribute-based classification. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 819–826, DOI: https://doi.org/
10.1109/CVPR.2013.111 (2013).
29. Jayaraman, D. & Grauman, K. Zero-shot recognition
with unreliable attributes. In Advances in neural information processing systems, 3464–3472, DOI: https:
//dl.acm.org/doi/10.5555/2969033.2969213 (2014).
30. Bucher, M., Herbin, S. & Jurie, F. Improving semantic
embedding consistency by metric learning for zeroshot classiffication. In European Conference on Computer Vision, 730–746, DOI: https://doi.org/10.1007/
978-3-319-46454-1_44 (Springer, 2016).
31. Verma, V. K. & Rai, P. A simple exponential family
framework for zero-shot learning. In Joint European conference on machine learning and knowledge discovery
in databases, 792–808, DOI: https://doi.org/10.1007/
978-3-319-71246-8_48 (Springer, 2017).
32. Changpinyo, S., Chao, W.-L. & Sha, F. Predicting visual
exemplars of unseen classes for zero-shot learning. In
Proceedings of the IEEE International Conference on
Computer Vision, 3476–3485, DOI: https://doi.org/10.
1109/ICCV.2017.376 (2017).
33. Tsai, Y.-H. H., Huang, L.-K. & Salakhutdinov, R. Learning robust visual-semantic embeddings. In 2017 IEEE
International Conference on Computer Vision (ICCV),
3591–3600, DOI: https://doi.org/10.1109/ICCV.2017.
386 (IEEE, 2017).
34. Xian, Y., Sharma, S., Schiele, B. & Akata, Z. f-vaegand2: A feature generating framework for any-shot learning.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 10275–10284, DOI:
https://doi.org/10.1109/CVPR.2019.01052 (2019).
35. Liu, S., Long, M., Wang, J. & Jordan, M. I. Generalized
zero-shot learning with deep calibration network. In Advances in Neural Information Processing Systems, 2005–
2015, DOI: https://doi.org/10.1109/LSP.2020.2977498
(2018).

36. Kumar Verma, V., Arora, G., Mishra, A. & Rai, P.
Generalized zero-shot learning via synthesized examples.
In Proceedings of the IEEE conference on computer
vision and pattern recognition, 4281–4289, DOI: https:
//doi.org/10.1109/CVPR.2018.00450 (2018).
37. Zhu, P., Wang, H. & Saligrama, V. Generalized zeroshot recognition based on visually semantic embedding.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2995–3003, DOI: https:
//doi.org/10.1109/CVPR.2019.00311 (2019).
38. Li, C. et al. Generalized zero shot learning via synthesis
pseudo features. IEEE Access 7, 87827–87836, DOI:
https://doi.org/10.1109/ACCESS.2019.2925093 (2019).
39. Frome, A. et al. Devise: A deep visual-semantic embedding model. In Advances in neural information
processing systems, 2121–2129, DOI: https://doi.org/10.
1.1.466.176 (2013).
40. Norouzi, M. et al. Zero-shot learning by convex combination of semantic embeddings. ArXiv Mach. Learn.
DOI: arXiv:1312.5650 (2013).
41. Romera-Paredes, B. & Torr, P. An embarrassingly
simple approach to zero-shot learning. In International
Conference on Machine Learning, 2152–2161, DOI:
https://doi.org/10.1007/978-3-319-50077-5_2 (2015).
42. Guo, Y., Ding, G., Han, J. & Gao, Y. Zero-shot
recognition via direct classifier learning with transferred samples and pseudo labels. In Thirty-First
AAAI Conference on Artificial Intelligence, DOI: https:
//dl.acm.org/doi/10.5555/3298023.3298158 (2017).
43. Zhang, Z. & Saligrama, V. Zero-shot learning via semantic similarity embedding. In Proceedings of the IEEE
international conference on computer vision, 4166–4174,
DOI: https://doi.org/10.1109/ICCV.2015.474 (2015).
44. Changpinyo, S., Chao, W.-L., Gong, B. & Sha, F.
Synthesized classifiers for zero-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 5327–5336, DOI: https:
//doi.org/10.1109/CVPR.2016.575 (2016).
45. Zhao, B., Wu, B., Wu, T. & Wang, Y. Zero-shot learning
posed as a missing data problem. In Proceedings of
the IEEE International Conference on Computer Vision,
2616–2622, DOI: https://doi.org/10.1109/ICCVW.2017.
310 (2017).
46. Li, Y. & Wang, D. Zero-shot learning with generative
latent prototype model. arXiv preprint arXiv:1705.09474
(2017).
47. Rohrbach, M., Ebert, S. & Schiele, B. Transfer learning in a transductive setting. In Advances in neural
information processing systems, 46–54, DOI: https:
//dl.acm.org/doi/10.5555/2999611.2999617 (2013).
48. Kodirov, E., Xiang, T., Fu, Z. & Gong, S. Unsupervised
domain adaptation for zero-shot learning. In Proceedings
19/26

of the IEEE international conference on computer vision,
2452–2460, DOI: https://doi.org/10.1109/ICCV.2015.
282 (2015).
49. Fu, Y., Hospedales, T. M., Xiang, T. & Gong, S. Transductive multi-view zero-shot learning. IEEE transactions on pattern analysis machine intelligence 37,
2332–2345, DOI: https://doi.org/10.1109/TPAMI.2015.
2408354 (2015).
50. Akata, Z., Perronnin, F., Harchaoui, Z. & Schmid, C.
Label-embedding for image classification. IEEE transactions on pattern analysis machine intelligence 38,
1425–1438, DOI: https://doi.org/10.1109/TPAMI.2015.
2487986 (2015).
51. Zhang, Z. & Saligrama, V. Zero-shot learning via joint
latent similarity embedding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
6034–6042, DOI: https://doi.org/10.1109/CVPR.2016.
649 (2016).
52. Xu, X., Shen, F., Yang, Y., Shao, J. & Huang, Z.
Transductive visual-semantic embedding for zero-shot
learning. In Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval, 41–49,
DOI: https://doi.org/10.1145/3078971.3078977 (ACM,
2017).
53. Song, J., Shen, C., Yang, Y., Liu, Y. & Song, M.
Transductive unbiased embedding for zero-shot learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 1024–1033, DOI:
https://doi.org/10.1109/CVPR.2018.00113 (2018).
54. Wan, Z. et al. Transductive zero-shot learning with visual
structure constraint. arXiv preprint arXiv:1901.01570
(2019).
55. Sariyildiz, M. B. & Cinbis, R. G. Gradient matching generative networks for zero-shot learning. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2168–2178, DOI:
https://doi.org/10.1109/CVPR.2019.00227 (2019).
56. Ye, M. & Guo, Y. Zero-shot classification with
discriminative semantic representation learning. In
Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 7140–7148, DOI:
10.1109/CVPR.2017.542 (2017).
57. Wang, D., Li, Y., Lin, Y. & Zhuang, Y. Relational
knowledge transfer for zero-shot learning. In Thirtieth
AAAI Conference on Artificial Intelligence, DOI: https:
//dl.acm.org/doi/10.5555/3016100.3016198 (2016).
58. Xu, X. et al. Matrix tri-factorization with manifold
regularizations for zero-shot learning. In Proceedings
of the IEEE conference on computer vision and pattern
recognition, 3798–3807, DOI: https://doi.org/10.1109/
CVPR.2017.217 (2017).

59. Li, Y., Wang, D., Hu, H., Lin, Y. & Zhuang, Y. Zero-shot
recognition using dual visual-semantic mapping paths.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 3279–3287, DOI: https:
//doi.org/10.1109/CVPR.2017.553 (2017).
60. Wang, X., Ye, Y. & Gupta, A. Zero-shot recognition
via semantic embeddings and knowledge graphs. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 6857–6866, DOI: DOI:10.
1109/CVPR.2018.00717 (2018).
61. Lee, C.-W., Fang, W., Yeh, C.-K. & Frank Wang, Y.-C.
Multi-label zero-shot learning with structured knowledge
graphs. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 1576–1585,
DOI: https://doi.org/10.1109/CVPR.2018.00170 (2018).
62. Kampffmeyer, M. et al. Rethinking knowledge graph
propagation for zero-shot learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, 11487–11496, DOI: https://doi.org/10.
1109/CVPR.2019.01175 (2019).
63. Xian, Y., Lorenz, T., Schiele, B. & Akata, Z. Feature generating networks for zero-shot learning. In
Proceedings of the IEEE conference on computer vision and pattern recognition, 5542–5551, DOI: https:
//doi.org/10.1109/CVPR.2018.00581 (2018).
64. Mishra, A., Krishna Reddy, S., Mittal, A. & Murthy,
H. A. A generative model for zero shot learning using
conditional variational autoencoders. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, 2188–2196, DOI: https://doi.
org/10.1109/CVPRW.2018.00294 (2018).
65. Farhadi, A., Endres, I., Hoiem, D. & Forsyth, D. Describing objects by their attributes. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
1778–1785, DOI: https://doi.org/10.1109/CVPR.2009.
5206772 (IEEE, 2009).
66. Wah, C., Branson, S., Welinder, P., Perona, P. & Belongie,
S. The caltech-ucsd birds-200-2011 dataset. California
Inst. Technol. (2011).
67. Patterson, G. & Hays, J. Sun attribute database:
Discovering, annotating, and recognizing scene attributes. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, 2751–2758, DOI: https:
//doi.org/10.1109/CVPR.2012.6247998 (IEEE, 2012).
68. Long, Y. et al. From zero-shot learning to conventional
supervised classification: Unseen visual data synthesis.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 1627–1636, DOI: 10.
1109/CVPR.2017.653 (2017).
69. Lampert, C. H., Nickisch, H. & Harmeling, S. Learning
to detect unseen object classes by between-class attribute
transfer. In 2009 IEEE Conference on Computer Vision
20/26

and Pattern Recognition, 951–958, DOI: https://doi.org/
10.1109/CVPR.2009.5206594 (IEEE, 2009).
70. Karessli, N., Akata, Z., Schiele, B. & Bulling, A.
Gaze embeddings for zero-shot image classification.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 4525–4534, DOI:
https://doi.org/10.1109/CVPR.2017.679 (2017).
71. Rohrbach, M., Stark, M., Szarvas, G., Gurevych, I.
& Schiele, B. What helps where–and why? semantic relatedness for knowledge transfer. In 2010
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 910–917, DOI: https:
//doi.org/10.1109/CVPR.2010.5540121 (IEEE, 2010).
72. Rohrbach, M., Stark, M. & Schiele, B. Evaluating
knowledge transfer and zero-shot learning in a largescale setting. In CVPR 2011, 1641–1648, DOI: https:
//doi.org/10.1109/CVPR.2011.5995627 (IEEE, 2011).
73. Xian, Y. et al. Latent embeddings for zero-shot classification. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 69–77, DOI:
https://doi.org/10.1109/CVPR.2016.15 (2016).
74. Lu, Y. Unsupervised learning on neural network outputs:
with application in zero-shot learning. arXiv preprint
arXiv:1506.00990 (2015).
75. Akata, Z., Malinowski, M., Fritz, M. & Schiele, B.
Multi-cue zero-shot learning with strong supervision.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 59–68, DOI: https:
//doi.org/10.1109/CVPR.2016.14 (2016).
76. Qiao, R., Liu, L., Shen, C. & Van Den Hengel, A.
Less is more: zero-shot learning from online textual
documents with noise suppression. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, 2249–2257, DOI: https://doi.org/10.1109/
CVPR.2016.247 (2016).
77. Marino, K., Salakhutdinov, R. & Gupta, A. The more you
know: Using knowledge graphs for image classification.
arXiv preprint arXiv:1612.04844 DOI: https://doi.org/
10.1109/CVPR.2017.10 (2016).
78. Elhoseiny, M., Saleh, B. & Elgammal, A. Write a
classifier: Zero-shot learning using purely textual descriptions. In Proceedings of the IEEE International
Conference on Computer Vision, 2584–2591, DOI:
https://doi.org/10.1109/ICCV.2013.321 (2013).
79. Lei Ba, J., Swersky, K., Fidler, S. & Others. Predicting deep zero-shot convolutional neural networks using
textual descriptions. In Proceedings of the IEEE International Conference on Computer Vision, 4247–4255,
DOI: https://doi.org/10.1109/ICCV.2015.483 (2015).
80. Niu, L., Veeraraghavan, A. & Sabharwal, A. Webly
supervised learning meets zero-shot learning: A hybrid
approach for fine-grained classification. In Proceedings

of the IEEE Conference on Computer Vision and Pattern
Recognition, 7171–7180, DOI: https://doi.org/10.1109/
CVPR.2018.00749 (2018).
81. Zhu, Y., Elhoseiny, M., Liu, B., Peng, X. & Elgammal, A.
A generative adversarial approach for zero-shot learning
from noisy texts. In Proceedings of the IEEE conference
on computer vision and pattern recognition, 1004–1013,
DOI: https://doi.org/10.1109/CVPR.2018.00111 (2018).
82. Reed, S., Akata, Z., Lee, H. & Schiele, B. Learning
deep representations of fine-grained visual descriptions.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 49–58, DOI: doi:10.
1109/CVPR.2016.13 (2016).
83. Kankuekul, P., Kawewong, A., Tangruamsub, S. &
Hasegawa, O. Online incremental attribute-based zeroshot learning. In 2012 IEEE Conference on Computer
Vision and Pattern Recognition, 3657–3664, DOI: https:
//doi.org/10.1109/CVPR.2012.6248112 (IEEE, 2012).
84. Wang, X. & Ji, Q. A unified probabilistic approach
modeling relationships between attributes and objects.
In Proceedings of the IEEE International Conference on
Computer Vision, 2120–2127, DOI: https://doi.org/10.
1109/ICCV.2013.264 (2013).
85. Murphy, K. P. Machine learning: a probabilistic perspective (MIT press, 2012).
86. Deng, J. et al. Large-scale object classification using label relation graphs. In European conference on
computer vision, 48–64, DOI: https://doi.org/10.1007/
978-3-319-10590-1_4 (Springer, 2014).
87. Teimouri, M., Delavaran, M. H. & Rezaei, M. A
real-time ball detection approach using convolutional
neural networks. In The 23rd Annual RoboCup International Symposium, DOI: https://doi.org/10.1007/
978-3-030-35699-6_25 (2019).
88. Rezaei, M. Creating a cascade of haar-like classifiers:
Step by step (2014).
89. Al-Halah, Z., Tapaswi, M. & Stiefelhagen, R. Recovering
the missing link: Predicting class-attribute associations
for unsupervised zero-shot learning. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, 5975–5984, DOI: https://doi.org/10.1109/
CVPR.2016.643 (2016).
90. Atzmon, Y. & Chechik, G. Probabilistic and-or attribute grouping for zero-shot learning. arXiv preprint
arXiv:1806.02664 (2018).
91. Palatucci, M., Pomerleau, D., Hinton, G. E. & Mitchell,
T. M. Zero-shot learning with semantic output codes.
In Advances in neural information processing systems, 1410–1418, DOI: https://dl.acm.org/doi/10.5555/
2984093.2984252 (2009).
21/26

92. Weston, J., Bengio, S. & Usunier, N. Large scale
image annotation: learning to rank with joint wordimage embeddings. Mach. learning 81, 21–35, DOI:
https://doi.org/10.1007/s10994-010-5198-3 (2010).
93. Usunier, N., Buffoni, D. & Gallinari, P. Ranking
with ordered weighted pairwise classification. In
Proceedings of the 26th annual international conference on machine learning, 1057–1064, DOI: https:
//doi.org/10.1145/1553374.1553509 (ACM, 2009).
94. Tsochantaridis, I., Joachims, T., Hofmann, T. & Altun, Y.
Large margin methods for structured and interdependent
output variables. J. machine learning research 6, 1453–
1484, DOI: doi=10.1.1.92.6373 (2005).
95. Xie, G.-S. et al. Attentive region embedding network
for zero-shot learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
9384–9393, DOI: https://doi.org/10.1109/CVPR.2019.
00961 (2019).
96. Socher, R., Ganjoo, M., Manning, C. D. & Ng, A. Zeroshot learning through cross-modal transfer. In Advances
in neural information processing systems, 935–943,
DOI: https://dl.acm.org/doi/10.5555/2999611.2999716
(2013).
97. Rezaei, M. & Isehaghi, M. An efficient method for
license plate localization using multiple statistical features in a multilayer perceptron neural network. In 2018
9th Conference on Artificial Intelligence and Robotics
and 2nd Asia-Pacific International Symposium, 7–13,
DOI: https://doi.org/10.1109/aiar.2018.8769804 (IEEE,
2019).
98. Li, Y., Zhang, J., Zhang, J. & Huang, K. Discriminative
learning of latent features for zero-shot recognition.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 7463–7471, DOI: https:
//doi.org/10.1109/CVPR.2018.00779 (2018).
99. Gong, Y., Ke, Q., Isard, M. & Lazebnik, S. A multi-view
embedding space for modeling internet images, tags, and
their semantics. Int. journal computer vision 106, 210–
233, DOI: https://doi.org/10.1007/s11263-013-0658-4
(2014).

103. Rostami, M. et al. Zero-shot image classification using coupled dictionary embedding. arXiv preprint
arXiv:1906.10509 (2019).
104. Gretton, A., Borgwardt, K., Rasch, M., Schölkopf, B.
& Smola, A. J. A kernel method for the two-sampleproblem. In Advances in neural information processing systems, 513–520, DOI: https://doi.org/10.5555/
2976456.2976521 (2007).
105. Mukherjee, T., Yamada, M. & Hospedales, T. M.
Deep matching autoencoders.
arXiv preprint
arXiv:1711.06047 (2017).
106. Cristianini, N., Shawe-Taylor, J., Elisseeff, A. & Kandola,
J. S. On kernel-target alignment. In Advances in neural
information processing systems, 367–373, DOI: https:
//doi.org/10.1007/3-540-33486-6_8 (2002).
107. Yamada, M. et al. Cross-domain matching with squaredloss mutual information. IEEE transactions on pattern
analysis machine intelligence 37, 1764–1776, DOI: https:
//doi.org/10.1109/TPAMI.2014.2388235 (2015).
108. Wang, Q. & Chen, K. Zero-shot visual recognition via bidirectional latent embedding. Int. J. Comput. Vis. 124, 356–383, DOI: https://doi.org/10.1007/
s11263-017-1027-5 (2017).
109. Elhoseiny, M., Elgammal, A. & Saleh, B. Write
a classifier: Predicting visual classifiers from unstructured text. IEEE transactions on pattern analysis machine intelligence 39, 2539–2553, DOI: https:
//doi.org/10.1109/TPAMI.2016.2643667 (2016).
110. Schölkopf, B., Herbrich, R. & Smola, A. J. A generalized
representer theorem. In International conference on
computational learning theory, 416–426, DOI: https:
//doi.org/10.1007/3-540-44581-1_27 (Springer, 2001).
111. Zhang, L., Xiang, T. & Gong, S. Learning a deep
embedding model for zero-shot learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2021–2030, DOI: https://doi.org/10.1109/
CVPR.2017.321 (2017).

100. Mukherjee, T. & Hospedales, T. Gaussian visuallinguistic embedding for zero-shot recognition. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 912–918, DOI:
https://doi.org/10.18653/v1/D16-1089 (2016).

112. Long, Y., Liu, L. & Shao, L. Towards fine-grained open
zero-shot learning: Inferring unseen visual features
from attributes. In 2017 IEEE Winter Conference on
Applications of Computer Vision (WACV), 944–952,
DOI: https://doi.org/10.1109/WACV.2017.110 (IEEE,
2017).

101. Jiang, H., Wang, R., Shan, S. & Chen, X. Learning
class prototypes via structure alignment for zero-shot
recognition. In Proceedings of the European conference
on computer vision (ECCV), 118–134, DOI: https://doi.
org/10.1007/978-3-030-01249-6_8 (2018).

113. Arvanaghi, M. & Rezaei, M. Facial age estimation using
hybrid haar wavelet and color features with support
vector regression. In 2017 Artificial Intelligence and
Robotics (IRANOPEN), 6–12, DOI: https://doi.org/10.
1109/RIOS.2017.7956436 (2017).

102. Kolouri, S., Rostami, M., Owechko, Y. & Kim, K. Joint
dictionaries for zero-shot learning. In Thirty-Second
AAAI Conference on Artificial Intelligence (2018).

114. Ji, Z. et al. Stacked semantics-guided attention model
for fine-grained zero-shot learning. In Advances in
Neural Information Processing Systems, 5995–6004,
22/26

DOI: https://dl.acm.org/doi/10.5555/3327345.3327499
(2018).
115. Goodfellow, I. et al. Generative adversarial nets.
In Advances in neural information processing systems, 2672–2680, DOI: https://dl.acm.org/doi/10.5555/
2969033.2969125 (2014).
116. Kingma, D. P. & Welling, M. Auto-encoding variational
bayes. arXiv preprint arXiv:1312.6114 DOI: https://doi.
org/10.1016/0370-2693 (2013).
117. Tong, B. et al. Adversarial zero-shot learning with semantic augmentation. In Thirty-Second AAAI Conference on
Artificial Intelligence (2018).
118. Bucher, M., Herbin, S. & Jurie, F. Generating visual representations for zero-shot classification. In Proceedings
of the IEEE International Conference on Computer Vision, 2666–2673, DOI: https://doi.org/10.1109/ICCVW.
2017.308 (2017).
119. Li, Y., Swersky, K. & Zemel, R. Generative moment
matching networks. In International Conference on
Machine Learning, 1718–1727, DOI: https://dl.acm.org/
doi/10.5555/3045118.3045301 (2015).
120. Felix, R., Kumar, V. B. G., Reid, I. & Carneiro, G. Multimodal cycle-consistent generalized zero-shot learning. In
Proceedings of the European Conference on Computer
Vision (ECCV), 21–37, DOI: https://doi.org/10.1007/
978-3-030-01231-1_2 (2018).
121. Sohn, K., Lee, H. & Yan, X. Learning structured output representation using deep conditional generative
models. In Advances in neural information processing systems, 3483–3491, DOI: https://dl.acm.org/doi/10.
5555/2969442.2969628 (2015).
122. Li, J. et al. Leveraging the invariant side of generative
zero-shot learning. arXiv preprint arXiv:1904.04092
DOI: https://doi.org/10.1109/CVPR.2019.00758 (2019).
123. Arjovsky, M., Chintala, S. & Bottou, L. Wasserstein
generative adversarial networks. In International conference on machine learning, 214–223, DOI: https:
//dl.acm.org/doi/10.5555/3305381.3305404 (2017).
124. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V.
& Courville, A. C. Improved training of wasserstein
gans. In Advances in neural information processing systems, 5767–5777, DOI: https://dl.acm.org/doi/10.5555/
3295222.3295327 (2017).
125. Wen, Y., Zhang, K., Li, Z. & Qiao, Y. A discriminative feature learning approach for deep face recognition.
In European conference on computer vision, 499–515,
DOI: https://doi.org/10.1007/978-3-319-46478-7_31
(Springer, 2016).
126. Zhu, P., Wang, H. & Saligrama, V. Dont even look once:
Synthesizing features for zero-shot detection. arXiv
preprint arXiv:1911.07933 (2019).

127. Kodirov, E., Xiang, T. & Gong, S. Semantic autoencoder
for zero-shot learning. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
3174–3183, DOI: https://doi.org/10.1109/CVPR.2017.
473 (2017).
128. Chen, L., Zhang, H., Xiao, J., Liu, W. & Chang, S.-F.
Zero-shot visual recognition using semantics-preserving
adversarial embedding networks. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition, 1043–1052, DOI: https://doi.org/10.1109/
CVPR.2018.00115 (2018).
129. Shibing, X. & Zishu, G. Bi-semantic reconstructing
generative network for zero-shot learning. arXiv preprint
arXiv:1912.03877 (2019).
130. Chen, Z., Li, J., Luo, Y., Huang, Z. & Yang,
Y. Canzsl: Cycle-consistent adversarial networks
for zero-shot learning from natural language. arXiv
preprint arXiv:1909.09822 DOI: https://doi.org/10.1109/
WACV45572.2020.9093610 (2019).
131. Huang, H., Wang, C., Yu, P. S. & Wang, C.-D. Generative dual adversarial network for generalized zero-shot
learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 801–810, DOI:
https://doi.org/10.1109/CVPR.2019.00089 (2019).
132. Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I. &
Frey, B. Adversarial autoencoders. arXiv preprint
arXiv:1511.05644 (2015).
133. Van Horn, G. et al. Building a bird recognition app
and large scale dataset with citizen scientists: The fine
print in fine-grained dataset collection. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 595–604, DOI: https://doi.org/10.1109/
CVPR.2015.7298658 (2015).
134. Deng, J. et al. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer
vision and pattern recognition, 248–255, DOI: https:
//doi.org/10.1109/CVPR.2009.5206848 (Ieee, 2009).
135. Miller, G. A. Wordnet: a lexical database for english.
Commun. ACM 38, 39–41, DOI: https://doi.org/10.1145/
219717.219748 (1995).
136. Xian, Y., Lampert, C. H., Schiele, B. & Akata, Z. Zeroshot learning-a comprehensive evaluation of the good,
the bad and the ugly. IEEE transactions on pattern
analysis machine intelligence DOI: https://doi.org/10.
1109/TPAMI.2018.2857768 (2018).
137. Parikh, D. & Grauman, K. Relative attributes. In
2011 International Conference on Computer Vision, 503–
510, DOI: https://doi.org/10.1109/ICCV.2011.6126281
(IEEE, 2011).
138. Harris, Z. S. Distributional structure, DOI: https://doi.
org/10.1080/00437956.1954.11659520 (1954).
23/26

139. Salton, G. & Buckley, C. Term-weighting approaches
in automatic text retrieval. Inf. processing & management 24, 513–523, DOI: doi.org/10.1016/0306-4573(88)
90021-0 (1988).

152. Bay, H., Ess, A., Tuytelaars, T. & Van Gool, L. Speededup robust features (surf). Comput. vision image understanding 110, 346–359, DOI: https://doi.org/10.1016/j.
cviu.2007.09.014 (2008).

140. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S.
& Dean, J. Distributed representations of words and
phrases and their compositionality. In Advances in neural
information processing systems, 3111–3119, DOI: https:
//dl.acm.org/doi/10.5555/2999792.2999959 (2013).

153. Shechtman, E. & Irani, M. Matching local selfsimilarities across images and videos. In 2007 IEEE
Conference on Computer Vision and Pattern Recognition,
1–8, DOI: https://doi.org/10.1109/CVPR.2007.383198
(IEEE, 2007).

141. Pennington, J., Socher, R. & Manning, C. Glove:
Global vectors for word representation. In Proceedings
of the 2014 conference on empirical methods in natural language processing (EMNLP), 1532–1543, DOI:
https://doi.org/10.3115/v1/D14-1162 (2014).

154. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097–1105, DOI: https://doi.org/10.1145/3065386
(2012).

142. Peters, M. E. et al. Deep contextualized word representations. arXiv preprint arXiv:1802.05365 DOI:
https://doi.org/10.18653/v1/N18-1202 (2018).

155. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 (2014).

143. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805
DOI: https://doi.org/10.18653/v1/N19-1423 (2018).

156. Szegedy, C. et al. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, 1–9, DOI: https://doi.org/10.
1109/CVPR.2015.7298594 (2015).

144. Vaswani, A. et al. Attention is all you need. In Advances
in neural information processing systems, 5998–6008,
DOI: https://dl.acm.org/doi/10.5555/3295222.3295349
(2017).

157. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
770–778, DOI: https://doi.org/10.1109/CVPR.2016.90
(2016).

145. Zhu, Y. et al. Aligning books and movies: Towards
story-like visual explanations by watching movies and
reading books. In Proceedings of the IEEE international
conference on computer vision, 19–27, DOI: https://doi.
org/10.1109/ICCV.2015.11 (2015).
146. Yang, Z. et al. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, 5753–5763 (2019).
147. Parker, R., Graff, D., Kong, J., Chen, K. & Maeda, K.
English gigaword fifth edition, linguistic data consortium.
Google Scholar (2011).
148. Callan, J., Hoy, M., Yoo, C. & Zhao, L. Clueweb09 data
set (2009).

158. Sabzevari, R., Shahri, A., Fasih, A. R., Masoumzadeh,
S. & Rezaei Ghahroudi, M. Object detection and localization system based on neural networks for robo-pong.
In Proceeding of the 5th International Symposium on
Mechatronics and its Applications, ISMA 2008, DOI:
https://doi.org/10.1109/ISMA.2008.4648837 (2008).
159. Bansal, A., Sikka, K., Sharma, G., Chellappa, R. &
Divakaran, A. Zero-shot object detection. In Proceedings of the European Conference on Computer
Vision (ECCV), 384–400, DOI: https://doi.org/10.1007/
978-3-030-01246-5_24 (2018).

149. Lan, Z. et al. Albert: A lite bert for self-supervised
learning of language representations. arXiv preprint
arXiv:1909.11942 (2019).

160. Rahman, S., Khan, S. & Porikli, F. Zero-shot object
detection: Learning to simultaneously recognize and
localize novel concepts. In Asian Conference on Computer Vision, 547–563, DOI: https://doi.org/10.1007/
978-3-030-20887-5-34 (Springer, 2018).

150. Lowe, D. G. Distinctive image features from scaleinvariant keypoints. Int. journal computer vision 60, 91–
110, DOI: https://doi.org/10.1023/B:VISI.0000029664.
99615.94 (2004).

161. Demirel, B., Cinbis, R. G. & Ikizler-Cinbis, N. Zero-shot
object detection by hybrid region embedding. ArXiv
Comput. Vis. Pattern Recognit. DOI: arXiv:1805.06157
(2018).

151. Bosch, A., Zisserman, A. & Munoz, X. Representing
shape with a spatial pyramid kernel. In Proceedings
of the 6th ACM international conference on Image and
video retrieval, 401–408, DOI: https://doi.org/10.1145/
1282280.1282340 (2007).

162. Zhan, C., She, D., Zhao, S., Cheng, M.-M. & Yang, J.
Zero-shot emotion recognition via affective structural
embedding. In Proceedings of the IEEE International
Conference on Computer Vision, 1151–1160, DOI: https:
//doi.org/10.1109/ICCV.2019.00124 (2019).
24/26

163. Bucher, M., Vu, T.-H., Cord, M. & Pérez, P. Zero-shot semantic segmentation. arXiv preprint arXiv:1906.00817
(2019).
164. Wang, W., Lu, X., Shen, J., Crandall, D. J. & Shao,
L. Zero-shot video object segmentation via attentive
graph neural networks. In Proceedings of the IEEE
International Conference on Computer Vision, 9236–
9245, DOI: https://doi.org/10.1109/ICCV.2019.00933
(2019).
165. Long, Y., Liu, L., Shen, Y. & Shao, L. Towards affordable semantic searching: Zero-shot retrieval via
dominant attributes. In Thirty-Second AAAI Conference
on Artificial Intelligence (2018).
166. Xu, Y. et al. Attribute hashing for zero-shot image
retrieval. In 2017 IEEE International Conference on
Multimedia and Expo (ICME), 133–138, DOI: https:
//doi.org/10.1109/ICME.2017.8019425 (IEEE, 2017).
167. Dutta, A. & Akata, Z. Semantically tied paired cycle
consistency for zero-shot sketch-based image retrieval.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 5089–5098, DOI: https:
//doi.org/10.1109/CVPR.2019.00523 (2019).
168. Dey, S., Riba, P., Dutta, A., Llados, J. & Song, Y.-Z.
Doodle to search: Practical zero-shot sketch-based image
retrieval. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2179–2188,
DOI: https://doi.org/10.1109/CVPR.2019.00228 (2019).
169. Shen, Y., Liu, L., Shen, F. & Shao, L. Zero-shot sketchimage hashing. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 3598–
3607, DOI: https://doi.org/10.1109/CVPR.2018.00379
(2018).
170. Pathak, D. et al. Zero-shot visual imitation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, 2050–2053, DOI:
https://doi.org/10.1109/CVPRW.2018.00278 (2018).
171. Lázaro-Gredilla, M., Lin, D., Guntupalli, J. S. & George,
D. Beyond imitation: Zero-shot task transfer on robots by
learning concepts as cognitive programs. Sci. Robotics
4, eaav3150, DOI: https://doi.org/10.1126/scirobotics.
aav3150 (2019).
172. Gao, J., Zhang, T. & Xu, C. I know the relationships:
Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs. In Proceedings
of the AAAI Conference on Artificial Intelligence, vol. 33,
8303–8311, DOI: https://doi.org/10.1609/aaai.v33i01.
33018303 (2019).
173. Qin, J. et al. Zero-shot action recognition with errorcorrecting output codes. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
2833–2842, DOI: https://doi.org/10.1109/CVPR.2017.
117 (2017).

174. Mishra, A. et al. A generative approach to zero-shot and
few-shot action recognition. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 372–
380, DOI: https://doi.org/10.1109/WACV.2018.00047
(IEEE, 2018).
175. Shen, L., Yeung, S., Hoffman, J., Mori, G. & Fei-Fei, L.
Scaling human-object interaction recognition through
zero-shot learning. In 2018 IEEE Winter Conference on
Applications of Computer Vision (WACV), 1568–1576,
DOI: https://doi.org/10.1109/WACV.2018.00181 (IEEE,
2018).
176. Sheng, L., Lin, Z., Shao, J. & Wang, X. Avatar-net:
Multi-scale zero-shot style transfer by feature decoration.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 8242–8250, DOI: https:
//doi.org/10.1109/CVPR.2018.00860 (2018).
177. Shocher, A., Cohen, N. & Irani, M. “zero-shot” superresolution using deep internal learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 3118–3126, DOI: https://doi.org/10.1109/
CVPR.2018.00329 (2018).
178. Li, A., Lu, Z., Wang, L., Xiang, T. & Wen, J.R. Zero-shot scene classification for high spatial
resolution remote sensing images. IEEE Transactions on Geosci. Remote. Sens. 55, 4157–4167, DOI:
https://doi.org/10.1109/TGRS.2017.2689071 (2017).
179. Xie, Y., Xu, P. & Ma, Z. Deep zero-shot learning for
scene sketch. In 2019 IEEE International Conference
on Image Processing (ICIP), 3661–3665, DOI: https:
//doi.org/10.1109/ICIP.2019.8803426 (IEEE, 2019).
180. Logeswaran, L. et al. Zero-shot entity linking by reading
entity descriptions. arXiv preprint arXiv:1906.07348
DOI: https://doi.org/10.18653/v1/P19-1335 (2019).
181. Gu, J., Wang, Y., Cho, K. & Li, V. O. K. Improved zeroshot neural machine translation via ignoring spurious
correlations. arXiv preprint arXiv:1906.01181 DOI:
https://doi.org/10.18653/v1/P19-1121 (2019).
182. Johnson, M. et al. Google’s multilingual neural machine translation system: Enabling zero-shot translation.
Transactions Assoc. for Comput. Linguist. 5, 339–351,
DOI: https://doi.org/10.1162/tacl_a_00065 (2017).
183. Ha, T.-L., Niehues, J. & Waibel, A. Effective strategies
in zero-shot neural machine translation. arXiv preprint
arXiv:1711.07893 DOI: http://dx.doi.org/10.1162/neco.
1997.9.8.1735 (2017).
184. Lakew, S. M., Lotito, Q. F., Negri, M., Turchi, M. &
Federico, M. Improving zero-shot translation of lowresource languages. arXiv preprint arXiv:1811.01389
(2018).
185. Artetxe, M. & Schwenk, H. Massively multilingual
sentence embeddings for zero-shot cross-lingual transfer
and beyond. Transactions Assoc. for Comput. Linguist.
25/26

7, 597–610, DOI: https://doi.org/10.1162/tacl_a_00288
(2019).
186. Carlson, K., Riddell, A. & Rockmore, D. Zero-shot
style transfer in text using recurrent neural networks.
ArXiv Mach. Learn. DOI: https://doi.org/abs/1711.04731
(2017).
187. Qian, K., Zhang, Y., Chang, S., Yang, X. & HasegawaJohnson, M. Autovc: Zero-shot voice style transfer with
only autoencoder loss. In International Conference on
Machine Learning, 5210–5219 (2019).
188. Fang, Y. et al. Sensitivity of chest ct for covid-19:
comparison to rt-pcr. Radiology 200432, DOI: https:
//doi.org/10.1148/radiol.2020200432 (2020).
189. Ye, Z., Zhang, Y., Wang, Y., Huang, Z. & Song, B.
Chest ct manifestations of new coronavirus disease 2019
(covid-19): a pictorial review. Eur. Radiol. 1–9, DOI:
https://doi.org/10.1148/radiol.2020200343. (2020).
190. Li, Y. & Xia, L. Coronavirus disease 2019 (covid-19):
role of chest ct in diagnosis and management. Am. J.
Roentgenol. 1–7, DOI: https://doi.org/10.2214/AJR.20.
22954 (2020).
191. Shan, F. et al. Lung infection quantification of covid19 in ct images with deep learning. arXiv preprint
arXiv:2003.04655 (2020).
192. Narin, A., Kaya, C. & Pamuk, Z. Automatic detection
of coronavirus disease (covid-19) using x-ray images
and deep convolutional neural networks. arXiv preprint
arXiv:2003.10849 (2020).
193. Xia, W. et al. Clinical and ct features in pediatric
patients with covid-19 infection: Different points from
adults. Pediatr. pulmonology 55, 1169–1174, DOI:
https://doi.org/10.1002/ppul.24718 (2020).
194. Li, L. et al. Artificial intelligence distinguishes covid19 from community acquired pneumonia on chest ct.
Radiology 200905, DOI: https://doi.org/10.1148/radiol.
2020200905 (2020).
195. Shi, H. et al. Radiological findings from 81 patients
with covid-19 pneumonia in wuhan, china: a descriptive
study. The Lancet Infect. Dis. (2020).
196. Zheng, C. Time course of lung changes at chest ct
during recovery from coronavirus disease 2019 (covid19). Radiology 295, 715–721, DOI: https://doi.org/10.
1148/radiol.2020200370 (2020).
197. Zhang, J., Xie, Y., Li, Y., Shen, C. & Xia, Y.
Covid-19 screening on chest x-ray images using deep
learning based anomaly detection. arXiv preprint
arXiv:2003.12338 (2020).

199. Afshar, P. et al. Covid-caps: A capsule network-based
framework for identification of covid-19 cases from x-ray
images. arXiv preprint arXiv:2004.02696 (2020).
200. Hinton, G. E., Sabour, S. & Frosst, N. Matrix capsules
with em routing. OpenReview (2018).
201. Abbas, A., Abdelsamea, M. M. & Gaber, M. M. Classification of covid-19 in chest x-ray images using detrac deep convolutional neural network. arXiv preprint
arXiv:2003.13815 DOI: https://doi.org/10.1101/2020.
03.30.20047456 (2020).
202. Abbas, A., Abdelsamea, M. M. & Gaber, M. M. Detrac: Transfer learning of class decomposed medical
images in convolutional neural networks. IEEE Access 8,
74901–74913, DOI: https://doi.org/10.1109/ACCESS.
2020.2989273 (2020).
203. Zhang, Y., Miao, S., Mansi, T. & Liao, R. Unsupervised x-ray image segmentation with task driven
generative adversarial networks. Med. Image Analysis
62, 101664, DOI: https://doi.org/10.1016/j.media.2020.
101664 (2020).
204. Rajpurkar, P. et al. Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning. arXiv
preprint arXiv:1711.05225 (2017).
205. Wang, X. et al. Chestx-ray8: Hospital-scale chest
x-ray database and benchmarks on weakly-supervised
classification and localization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2097–2106, DOI:
https://doi.org/10.1109/CVPR.2017.369 (2017).
206. Rutigliano, I., Gorgoglione, S., Pacilio, A., De Meco,
C. & Sacco, M. C. Chronic eosinophilic pneumonia: A
pediatric case. Clin Med Rev Case Rep 6, 264, DOI:
https://doi.org/10.1089/ped.2014.0330.95 (2019).
207. Varshni, D., Thakral, K., Agarwal, L., Nĳhawan, R. &
Mittal, A. Pneumonia detection using cnn based feature extraction. In 2019 IEEE International Conference
on Electrical, Computer and Communication Technologies (ICECCT), 1–7, DOI: https://doi.org/10.1155/2019/
4180949 (IEEE, 2019).
208. Radovanović, M., Nanopoulos, A. & Ivanović, M. Hubs
in space: Popular nearest neighbors in high-dimensional
data. J. Mach. Learn. Res. 11, 2487–2531 (2010).
209. Zhang, Z. & Saligrama, V. Zero-shot recognition via
structured prediction. In European conference on computer vision, 533–548, DOI: https://doi.org/10.1007/
978-3-319-46478-7-33 (Springer, 2016).

198. Hemdan, E. E.-D., Shouman, M. A. & Karar, M. E.
Covidx-net: A framework of deep learning classifiers
to diagnose covid-19 in x-ray images. arXiv preprint
arXiv:2003.11055 DOI: https://doi.org/10.1016/j.cmpb.
2020.105581 (2020).
26/26

