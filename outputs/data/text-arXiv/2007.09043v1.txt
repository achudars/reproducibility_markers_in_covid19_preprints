arXiv:2007.09043v1 [q-fin.ST] 17 Jul 2020

Estimation of time-varying kernel densities
and chronology of the impact
of COVID-19 on financial markets
Matthieu Garcina,∗, Jules Kleinb , Sana Laaribib
July 20, 2020

Abstract
The time-varying kernel density estimation relies on two free parameters: the bandwidth
and the discount factor. We propose to select these parameters so as to minimize a criterion
consistent with the traditional requirements of the validation of a probability density forecast.
These requirements are both the uniformity and the independence of the so-called probability
integral transforms, which are the forecast time-varying cumulated distributions applied to the
observations. We thus build a new numerical criterion incorporating both the uniformity and
independence properties by the mean of an adapted Kolmogorov-Smirnov statistic. We apply
this method to financial markets during the COVID-19 crisis. We determine the time-varying
density of daily price returns of several stock indices and, using various divergence statistics,
we are able to describe the chronology of the crisis as well as regional disparities. For instance,
we observe a more limited impact of COVID-19 on financial markets in China, a strong impact
in the US, and a slow recovery in Europe.

Keywords – bandwidth selection, divergence statistics, financial crisis, kernel density, probability
integral transform

1

Introduction

The knowledge of the distribution of price returns is overriding in finance. Indeed, forecasts and
risk measures, such as the Value-at-Risk (VaR), the expected shortfall, or even the volatility, can be
seen as scalars calculated from a probability density function (pdf). Practitioners appreciate these
scalars for their simplicity but the pdf contains relevant and more comprehensive information. An
accurate description of the pdf is thus worthwhile in a financial perspective.
For this reason, pdf in finance should not be limited to the popular Gaussian distribution. More
realistic parametric distributions have thus been put forward [26], like the NIG distribution [12]
or the alpha-stable one [37], among many others. Besides, the non-parametric alternative makes
it possible to depict more accurately the real pdf but it may be subject to overfitting if it does not
∗ Corresponding author: matthieu.garcin@m4x.org.
Léonard de Vinci Pôle Universitaire, Research center, 92916 Paris La Défense, France.
b ESILV, 92916 Paris La Défense, France.
The authors would like to thank Brieuc-Marie Le Brigand for his valuable help in the implementation of some of
the methods described in this paper.
a

1

include any regularization. For this purpose, Beran has proposed the minimum Hellinger distance
approach, in which a non-parametric pdf has to be estimated first, before being approximated by
a parametric distribution [4]. This approach finds some applications in finance [24]. Other semiparametric approaches include the distortion of a parametric density in order to take into account
higher-order empirical moments, using for example an Edgeworth expansion [22, 15], which also
has some applications in finance [29]. Finally, non-parametric approaches, like the kernel density,
include a smoothing parameter, called bandwidth, which is supposed to balance accuracy and
statistical robustness [41, 38]. Selecting an appropriate bandwidth is a hard task often left aside
by practitioners in finance, but some statistical methods propose criteria that the bandwidth should
minimize, like the asymptotic mean integrated square error (AMISE) [19].
In finance, non-parametric methods are not limited to the estimation of a pdf. We can cite for
example their use to estimate the impact of market events, as with the non-parametric news impact
curve in econometric volatility models [30, 11, 7, 14]. The rationale of such an approach is that
linear impact models misestimate the reaction of markets to extreme events. Similarly, parametric
models do often not describe accurately enough the tails of the pdf of price returns. Extreme events
may also lead to other methodological choices in addition to the non-parametric approach. Indeed,
like all the previous financial crisis, the recent crisis provoked by the COVID-19 pandemic has
highlighted that the occurrence of an extreme event may have a sustainable effect on the market,
namely on the pdf of price returns. Bad economic news do not only result in one extreme daily
price variation but it can initiate a longer turmoil period. This alternation of market regimes
incites to introduce time-varying densities. Once again, several approaches are possible, depending
on the fact that we consider a parametric pdf [1] or a non-parametric one [17]. We are particularly
interested in the non-parametric approach, which offers the possibility to reach a higher accuracy.
We stress the fact that applications of time-varying kernels in finance are not limited to estimating
a pdf of price returns. They indeed include interest rate models [44] or the study of the dynamic
pdf of correlation coefficients with a particular shape during tumble periods [23].
Time-varying kernel densities rely on the choice of two important free parameters: the bandwidth
smooths the pdf at a given date, exactly as in the static approach, whereas the discount factor
smooths the variations in time. Harvey and Oryshchenko propose a maximum likelihood approach
to select these free parameters [17]. However, the literature about density validation requires
stronger properties, namely the fact that the cumulative distribution function (cdf) of price returns
must form a set of iid uniform variables, known as the probability integral transforms (PITs) [9].
In the present paper, we thus propose a new selection rule of the bandwidth and of the discount
parameter, so that it is consistent with the validation rule of the pdf. The main challenge is then
to build the criterion, that is to define a function of the bandwidth and of the discount factor
that we intend to minimize. Indeed, the traditional approach for validating an estimated pdf
consists in a series of statistical tests and graphical analysis, not in a sole numerical criterion. The
criterion we propose relies on a Kolmogorov-Smirnov statistic, that we can replace by any statistic
of distribution divergence. We also adapt this statistic so as to minimize the discrepancy of the
series of the PITs for which we need the independence. Our work is not the first one to stress
the limitations of the maximum likelihood approach in the selection of the two free parameters.
We can indeed cite an article following a least-square approach [35] and another one maximizing
a uniformity criterion for the PITs by the mean of an artificial neural network [42]. Our method
differs from the latter as it also takes into account the independence of the PITs and it requires
only standard statistical tools compared to artificial neural networks.
We apply our new method to several stock indices before and during the financial crisis induced
by the COVID-19 in the US, in Europe, and in Asia. The question of the impact of the pandemic
on stock markets is a hot topic. Several papers deal with this subject and stress the exceptional
amplitude of the crisis [2, 3, 32]. We propose here a new outlook on this financial crisis, using the

2

time-varying kernel densities to describe its chronology. We also study the significance of the daily
kernel density with respect to the pdf in a steady market. This makes it possible to determine the
interval of dates for which the distribution of price returns significantly indicates a financial crisis.
In particular, we observe that the speed at which markets recover varies a lot among the regions
considered.
The paper is organized as follows. In Section 2, we introduce the method for estimating a dynamic
kernel density along with the selection rule for the bandwidth and the discount factor. In Section 3,
we apply this method to stock markets during the COVID-19 crisis. Section 4 concludes.

2

Statistical methods

In this section, we introduce a method to estimate a time-varying density. For this purpose, we
recall how we can estimate a static non-parametric density as well as its dynamic adaptation. This
method relies on the choice of two free parameters. The main innovation of this paper consists in
basing this choice on a quantitative version of criteria usually devoted to the evaluation of forecast
densities. The last subsection is about some divergence statistics between two densities. We will
use these divergences in the empirical part of this paper to quantify the amplitude of the variations
of the densities through time and to determine the significance of these variations.

2.1

Kernel density

A widespread non-parametric method to estimate a pdf uses kernels. The kernel density is defined
by:


t
x − Xi
1 X
K
,
fˆ(x) =
th i=1
h
where h > 0 is the bandwidth and K a function following the same rules as a pdf, namely it
is positive, integrable and its integral is one [41, 38]. With these two properties, fˆ also has the
features of a density. In particular, when integrating fˆ, the substitution y = (x − Xi )/h in each of
the
R t integrals in the sum clearly shows that we need to normalize the sum by th in order to have
fˆ(x)dx = 1. The symmetry and the continuity of the kernel is also often desirable.
R
The rationale of the kernel density is to make a continuous generalization of a histogram. Indeed,
in the histogram, we count the number of occurrences in given intervals. The thinner the intervals,
the more accurate the density estimation. But very thin intervals lead to overfitting, with a very
erratic estimated density. To avoid this, we prefer to smooth the histogram. A simple manner
to do this consists in replacing the number of occurrences of observations in each thin interval
by a criterion of proximity of each observation to the middle of this interval. This is how the
kernel density works. The proximity function K must thus reach its maximum in zero and it must
decrease progressively when its argument gets away from zero. Thus, the impact of Xt on the
estimated pdf in x is maximal for x = Xt and it decreases progressively when |x − Xt | becomes
higher until reaching zero, at least asymptotically. It means that the observation of Xt will have
no impact on the density fˆ(x) if Xt is by far greater or lower than x.
There exists a large literature on the choice of the kernel K [34, 6]. Epanechnikov and Gaussian
kernels are widespread, due to their simplicity.1 But it seems, according to the related literature,
that the choice of the kernel if often less overriding than the choice of the bandwidth h. Indeed,
this parameter plays the role of a regularization parameter. In practice, we tune h in order to
1 In

the empirical application, we use Epanechnikov kernel.

3

balance accuracy and robustness. The larger h, the wider each kernel and the larger the interval
on which each observation has an impact. We review in Section 2.4 some methods to select this
bandwidth.

2.2

Dynamic kernel density

We can change the formulation of the kernel density in order to take into account its progressive
evolution through time. We get this dynamic version of the kernel density by the mean of weights
wt,i :


t
x − Xi
1X
ˆ
wt,i K
,
(1)
ft (x) =
h i=1
h
Pt
such that
i=1 wt,i = 1 [17]. For a fixed t, if the weights wt,i increase with i, more recent
observations will be overweighted and the update of the kernel density is consistent with the
economic intuition. The exponential weighting is widespread in the statistical literature as it can
reduce the computation of a density update to a simple recursive formula instead of the linear cost
induced by the whole estimation of the density from scratch. We then express the weights by
wt,i =

1 − ω t−i
ω ,
1 − ωt

where 0 < ω ≤ 1. With this setting for the weights, we note fˆth,ω the density introduced in
equation (1). Then, when the duration of the estimation sample is large enough with respect to
the speed of decay of the weights, a good approximation of ωt,i is (1 − ω)ω t−i . The recursive
formula that the dynamic kernel density follows is then:


x − Xt+1
1−ω
h,ω
fˆt+1
(x) = ω fˆth,ω (x) +
K
.
(2)
h
h
The two free parameters of this dynamic non-parametric density are the bandwidth h, and the
discount factor ω. Starting at a given time t0 from a density with an exponential weighting, such
as in equation (1), we obtain the density at subsequent times iteratively by applying equation (2).
Along with the time-varying density, we can build the corresponding cdf. Integrating equation (1),
the first estimated cdf, at time t0 , is:
F̂th,ω
(x) =
0



t0
1−ω X
x − Xi
t0 −i
,
ω
K
1 − ω t0 i=1
h

where K is the primitive of K such that lim K(x) = 1. Subsequently, we get the cdf at a time
x→∞
t + 1 by the mean of the following iteration:


x − Xt+1
h,ω
(x) = ω F̂th,ω (x) + (1 − ω)K
F̂t+1
,
h
which is the primitive of equation (2).
Other approaches are possible for estimating a time-varying density. For example, we could have
estimated static densities on successive intervals and have then smoothed the transition between the
resulting densities. For parametric densities, this amounts to smoothing time-varying parameters,
which is a well-known subject in statistics [13]. However, this approach is not very natural for
non-parametric densities, as we need a big amount of data to estimate one static density.
4

2.3

Evaluation of the quality of the dynamic density

The purpose of density forecast may vary a lot. In a financial perspective, one may need it to build
risk measures, or to forecast an average price return or a most likely price return. In practice, an
investment decision is to be made relying on this density. One must then evaluate the quality of
the forecast with respect to a loss function corresponding to this decision. Unfortunately, there
cannot exist any absolute ranking of density forecasts valid for all the possible loss functions [9].
We therefore have to make a choice which is necessarily subject to discussion. In the econometric
literature, we can find an evaluation of the density forecast by the mean of the likelihood of
observations [17]. We think that this choice, as focusing on the body of the distribution, neglects
the behaviour of the density in its tails. A more general perspective would incite to choose a
density forecast consistent with the real density, even with its tails. Such a forecast would be more
relevant in finance for calculating a VaR or an expected shortfall. However, the real density is never
observed. If we had a static density forecast, the evaluation of this forecast could simply consist in
comparing it with the empirical density of all the observed price returns. But the forecast density is
supposed to change at each time, so that we have to base our analysis on another invariant density.
This is the purpose of the analysis of the PITs, introduced by Diebold, Gunther, and Tay [9] and
widespread in the literature of evaluation of density forecasts [16, 17, 20]. We now expose this
method, that we will adapt in the next subsection from the evaluation of density forecasts to the
selection of the bandwidth and of the discount factor.
We observe T successive price returns: X1 , ..., XT . We use the t0 first to build a density estimation
fˆth,ω
, using equation (1). This density includes a discount in order to depict more closely recent
0
observations. We thus conceive fˆth,ω
as a forecast of the true density ft0 +1 of Xt0 +1 , as well as
0
h,ω
ˆ
we conceive ft , for any t ≥ t0 , as a forecast of the true density ft+1 of Xt+1 . Of course, ft
varies with t, and we only observe one random variable in this density, namely Xt . However, we
are able to build a density which does not depend on t and which will therefore be very useful for
evaluating the quality of the density forecast. This invariant distribution is the one of the PIT
variables, which are defined by:
h,ω
(Xt ).
Zth,ω = F̂t−1
h,ω
Indeed, if our forecast is good, that is if ∀t ≥ t0 + 1, F̂t−1
= Ft , where Ft is the true cdf, then,
whatever t, Zth,ω follows a uniform random variable in the interval [0, 1]: Zth,ω ∼ U(0, 1) [9]. In
fact, this idea is quite old [33] and is even something with which any person simulating random
variables following a given cdf is familiar. In addition to being uniform, the variables Zth,ω
, ..., ZTh,ω
0 +1
must also be independent.

Thanks to the PITs, we have T − t0 observations in the same uniform density. This makes it
, ..., ZTh,ω are indeed iid and
possible to evaluate the density forecast: we have to check that Zth,ω
0 +1
uniform in [0, 1]. We expose in the next subsection the difference of approach regarding this point
between the evaluation of density forecast and our framework, which is about the selection of
optimal parameters.

2.4

Selection of the bandwidth and of the discount factor

The literature about bandwidth selection is very rich [41, 19, 38]. Beyond the rule-of-thumb which
often leads the choice of the bandwidth h among practitioners, we can cite more relevant methods
of selection, such as the minimization of AMISE [36], evaluated for instance with cross validation or
a plug-in technique. As exposed in the previous subsection, we will try to select h in order to make
the distribution of the PITs close to a uniform distribution, and the uniform case is trivial in the

5

AMISE approach and makes this method ineffective. We can also cite the possibility to estimate
a time-varying bandwidth, like in the literature about online estimation of kernel density [43, 21].
Our approach will be different from the online framework: our time-varying aspect is not about h
but about the density.
Our problem, in addition, is not only about selecting h. We have to select it jointly with the
discount factor ω. As already mentioned, we can base this selection on the maximization of a
likelihood [17]. But we want to have an accurate description of the true density, not to make the
best point forecast. This thus incites us to use PITs and to adapt the method of evaluation of
density forecasts. We have two objectives regarding the PITs: the uniformity and the independence.
We first focus on the uniformity. According to Diebold, Gunther, and Tay, methods based on
statistical tests, such as the Kolmogorov-Smirnov test, are not relevant because nonconstructive,
insofar as they do not indicate why PITs are not uniform [9]. They thus prefer a qualitative
analysis using graphical tools such as a simple histogram. Besides this mainstream approach, some
papers propose a statistical test assessing the uniformity of the PITs [5]. Our framework is in fact
different as we do not want to determine whether our density forecast is good or not. Instead, given
a density model, we only want to select its best parameters, here h and ω. Maybe our forecast
will be poor, even though the non-parametric approach makes this case unlikely, but we will have
done the best with respect to the model used. We thus do not want to test the consistence of
our PITs with a uniform distribution, but we select the parameters h and ω minimizing some test
statistic. We choose to minimize the Kolmogorov-Smirnov statistic, k, because it is widespread
and easy to understand. This statistic is simply the maximum gap between the empirical cdf and
the theoretical one, which, in our case, is uniform:
k(Zth,ω
, ..., ZTh,ω ) =
0 +1

max

t0 +1≤s≤T

s − t0
h,ω
− Zπ(s;t
,
0 +1,T )
T − t0 + 1

h,ω
where s ∈ Ju, vK 7→ π(s; u, v) is a permutation of Ju, vK defining the new order: Zπ(u;u,v)
≤

h,ω
h,ω
h,ω
Zπ(u+1;u,v)
≤ ... ≤ Zπ(v−1;u,v)
≤ Zπ(v;u,v)
.

But the Kolmogorov-Smirnov statistic says nothing about the independence of the variables and
the fact that the sampling is random [10]. This property is however crucial and its absence could
lead to nonsense estimations [18, 8]. In the standard approach regarding the evaluation of density
forecast, the independence is assessed by graphical tools, such as a correlogram [9]. We would like
again a more systematic approach. We thus use an additional criterion coming from the literature
of simulation of quasi-random variables. We indeed want our series of PITs to be a low-discrepancy
sequence [27, 39, 28]. This is even more important that we want to estimate a time-varying density
of price returns in a regime-switching market. The rationale behind the discrepancy is that the
uniformity must be a feature not only of the PITs in the interval [t0 + 1, T ] but also of the PITs in
any of its subintervals: the sequence must be equidistributed. This method will avoid almost static
densities in which price returns are globally well distributed, but with mainly high PITs in a bullish
regime and then mainly low PITs during a crisis period. The discrepancy criterion we propose to
minimize is then the worst uniformity statistic over all the subintervals of [t0 + 1, T ]. However,
the Kolmogorov-Smirnov statistic depends on the size of the sample. We thus consider √
instead
a size-adapted version of this statistic. Indeed, for a sample of n → +∞ observations, n × k
has a limit distribution which does not depend on n and that is used for the Kolmogorov-Smirnov
statistical test [25]. We also choose a minimal size ν above which we consider that the asymptotic
Kolmogorov distribution may be applied.2 Thanks to this size-adapted statistic, our discrepancy
2 We may consider, for example, ν = 22, so that we verify the uniformity for every one-month interval of daily
price returns. It is the choice made in the empirical part of this paper.

6

in fact focuses on the subinterval of size higher than ν with the least uniform PITs:
√

h,ω
h,ω
h,ω
,
...,
Z
)
=
max
t
−
s
+
1
×
k(Z
dν (Zth,ω
,
...,
Z
)
.
t
s
T
0 +1
t0 +1≤s<s+ν−1≤t≤T

Finally, the optimal bandwidth and discount factor are defined as the parameters minimizing this
uniformity and discrepancy statistic:
(h? , ω ? ) = argmin dν (Zth,ω
, ..., ZTh,ω ).
0 +1

(3)

h>0,0<ω≤1

We also propose a constrained version of this optimisation problem. Indeed, the above unconstrained problem may lead to a dynamic of densities far from the economic intuition, for example
with very rough densities. In practice, the time-varying densities we have built with this method
seem empirically robust, at first sight. But we are interested in defining some reasonable bounds for
h or ω. In order to have a robust time-varying density, we want that an isolated observation does
not change too much the density between two consecutive dates. To state things quantitatively,
we want to limit the Kolmogorov-Smirnov statistic between two densities at consecutive dates.
We propose ν −1 as an upper bound. This choice has the advantage not to introduce a new free
parameter but to link it to the validation rule exposed above. The rationale of this bound is the
following. With the daily ν −1 bound, the maximal change of the Kolomogorov-Smirnov statistic,
which is 1, cannot be reached before the minimal validation horizon, ν. In a new market regime,
the density may be strongly transformed. The time needed to update the density and to depict this
new regime will thus be roughly ν days if the bound for the daily variation of the KolomogorovSmirnov statistic is ν −1 . Minimizing the discrepancy of the PITs for intervals smaller than ν days
would then be irrelevant. This is the reason why there should be a consistence of the minimal
validation horizon with the bound of the variation of the Kolmogorov-Smirnov statistic between
two consecutive dates. The link between the ν −1 bound and the parameters is straightforward, as
the update of the cdf leads to an increase of the cdf at one point of at most 1 − ω. Therefore, we
introduce a new bound for ω and the constrained problem is as follows:
(h?c , ωc? ) =

argmin
h>0,1−ν −1 <ω≤1

dν (Zth,ω
, ..., ZTh,ω ).
0 +1

(4)

We could also want to set bounds to h in order to secure the robustness of the density at one date
instead of the robustness across time. Nevertheless, we consider that the robustness across time
is enough. Indeed, as the density will not change very rapidly, each density will be a fairly good
forecast for observations close in time.

2.5

Amplitude of the variations of the series of densities

In the method exposed above to select h and ω, we minimize the divergence between an empirical
distribution and a uniform one. In particular, we use the Kolmogorov-Smirnov statistic to depict
this divergence because of both its simplicity and its asymptotic behaviour. But other divergence
metrics could replace the Kolmogorov-Smirnov statistic in this method.
We can also use various divergence statistics to quantify to which extent the estimated pdf is
different from what it was at a reference date and thus track the evolution of the pdf through time.
This is what these various statistics are devoted to in the empirical part of this paper. In addition,
thanks to simulations, we will determine confidence intervals for each of these divergences at each
date, so that we will be able to assess whether the evolution of the pdf through time is significant
or not. We now review three of these divergence statistics in addition to the Kolmogorov-Smirnov
7

statistic. Some are based on a comparison of densities, and others on a comparison of cdfs or even
of quantiles.
First we recall the definition of the Kolmogorov-Smirnov statistic between the cdfs F̂t and F̂t0 :


k F̂t , F̂t0 = sup F̂t (x) − F̂t0 (x) .
x∈R

Whereas the Kolmogorov-Smirnov statistic considers the maximal difference between two cdfs, the
Hellinger distance is the cumulated difference between densities:
s
2
Z q
q


1
ˆ
ˆ
ˆ
ˆ
H ft , ft0 =
ft (x) − ft0 (x) dx.
2 R
The p-Wasserstein distance, given p ≥ 1, is related to the optimal transportation theory [40]. It is
the minimal cost to reconfigure one pdf in another one. The Kolmogorov-Smirnov statistic is the
L∞ distance between the cdfs, whereas the Wasserstein metric is the Lp distance between their
quantiles. The p-Wasserstein distance is indeed defined by [31]:

 Z
Wp F̂t , F̂t0 =
0

1

p

F̂t−1 (α) − F̂t−1
(α) dα
0

1/p
.

In this paper, we focus on the case p = 1, for which the Wasserstein distance is also equal to
the L1 distance between cdfs [31]. It thus clearly generalizes the Kolmogorov-Smirnov statistic.
We will see in the empirical part of this paper that this generalization may be more appropriate
than the Kolmogorov-Smirnov statistic to assess the significance of the variations of the distribution. Indeed, the occurrence of several extreme observations may not impact significantly the
Kolmogorov-Smirnov statistic, which mainly focuses on the body of the distribution, whereas the
Wasserstein distance takes into account the whole distribution. On the other hand, since a uniform distribution is not subject to a dichotomy between body and tails, the Kolmogorov-Smirnov
statistic seems appropriate for assessing the uniformity of the PITs.
As opposed to the other divergences, the Kullback-Leibler divergence is not strictly speaking a
distance function as it is not symmetric in the two densities. It is related to Shannon’s entropy. It
is defined by:
!

 Z
ˆt (x)
f
DKL fˆt fˆt0 =
fˆt (x) log
dx.
fˆt0 (x)
R
All these divergences can be generalized easily if we work with a discrete grid instead of R. They
are also always positive and with a value of zero if fˆt = fˆt0 .

3

Empirical results

We have applied the above method to the estimation of time-varying densities of several stock
indices. We consider American indices (NASDAQ Composite, S&P 500, S&P 100), European
indices (EURO STOXX 50, Euronext 100, DAX, CAC 40), and Asian indices (Nikkei 225, KOSPI,
SSE 50), with a particular focus on S&P 500, EURO STOXX 50, and the South-Korean KOSPI
indices. We have used data from Yahoo finance in the time interval from 04/17/2015 to 05/28/2020.
The study period includes the economic crisis related to the COVID-19. In particular, we study the
impact of the COVID-19 on three stock markets corresponding to economic areas with different
8

crisis management regarding the pandemic. The questions we want to answer are about the
significance of this impact and the characterization of a recovery after the peak of the crisis.
We have estimated daily a pdf of daily price returns from the date t0 corresponding to November
1st, 2019. These densities include observations from 2015, exponentially weighted with an optimal
discount factor depending on the index. We provide these optimal discount factors in Table 1,
along with the optimal bandwidth, determined by equation (3), as well as the constrained version
defined by equation (4).

NASDAQ Composite
S&P 500
S&P 100
EURO STOXX 50
Euronext 100
DAX
CAC 40
Nikkei 225
KOSPI
SSE 50
Mean value
Median value

h?
0.0110
0.0122
0.0124
0.0124
0.0110
0.0038
0.0117
0.0124
0.0124
0.0107
0.0110
0.0122

ω?
0.827
0.840
0.877
0.831
0.883
0.864
0.864
0.790
0.813
0.778
0.838
0.840

h?c
0.0101
0.0121
0.0124
0.0124
0.0123
0.0014
0.0119
0.0165
0.0286
0.0002
0.0112
0.0123

ωc?
0.955
0.955
0.955
0.960
0.963
0.955
0.960
0.955
0.955
0.974
0.959
0.955

Table 1: Optimal bandwidth h? and discount factor ω ? for several
stock indices for densities between November 2019 and May 2020.
The constrained version is h?c and ωc? .
The optimal bandwidth is close to 0.012, except for the DAX, for which it is much lower. We also
observe a low optimal bandwidth on the Chinese market (SSE 50), but only in the constrained case
and it is mitigated by a particularly high discount factor in this case which ensures the stability
of the density. For the rest of the empirical study, we consider a common pair of parameters for
all the indices, so as to make fair comparisons. We thus focus on the parameters of the S&P 500
index in the constrained case reported in Table 1, because they are close to the median parameters:
hm = 0.0121 and ω m = 0.955.
For the S&P 500, EURO STOXX 50, and KOSPI indices, we display in Figure 1 the estimated
dynamic pdf of price returns at four dates which illustrate the chronology of the impact of the
pandemic on financial markets:
. before the crisis: on the 16th December 2019, in a period where the markets were steady,
. at the first turmoil in the markets, the 7th February 2020,
. at the peak of the pandemic, which occurs at a different date for each market,
. at the end of our sample, the 28th May 2020.
We determine themdate
of the peak of the pandemic as the date t maximizingmthem Hellinger distance
m
of the density fˆth ,ω with respect to the estimated pdf in t0 , that is fˆth0 ,ω . This peak does
not follow an epidemiological definition, since we only observe financial data. It corresponds to a
maximal divergence with the steady state of the market.
For the KOSPI and the EURO STOXX 50, the pdf before the crisis looks like a Gaussian distribution, with thin tails. Then the pdf slightly widens on the losses side. At the peak of the
9

Figure 1: Estimated dynamic pdf of daily price returns for S&P 500
(top left), EURO STOXX 50 (top right), and KOSPI (bottom) indices.

10

crisis, the pdf crushes, with very fat tails. After the peak, it tends to an asymmetric distribution
with a negative skewness and slowly decreasing tails. The chronology is similar for the S&P500,
except that the pdf the 7th February is similar to the one before the crisis. It may indicate a low
responsiveness of the US market in front of the outbreak. Or it may denote a temporary lag in the
impact of the COVID-19 on the US market, reflecting the lag in the spread of the outbreak in the
region.
Displaying pdfs at several dates as in Figure 1 makes it possible to depict the chronology of the
crisis. But it is limited since displaying this density every day of our sample would make the
figure unreadable. Therefore, instead of displaying each pdf, we display one statistic per day. This
statistic must reflect the divergence of the pdf with respect to a steady state of markets. We thus
determine the Kolmogorov-Smirnov statistic, the Hellinger distance, the 1-Wasserstein distance, as
well as the Kullback-Leibler divergence of the pdf each day with respect to the pdf in t0 . Results
are displayed in Figure 2. Whatever the divergence statistic, we observe first a slight increase
from 0 toward a low positive value until the beginning of the crisis, where the divergence sharply
increases till the peak where it begins to slowly decrease. This last phase corresponds to the slow
recovery of the markets after the crisis.

Figure 2: Daily evolution through time of four divergence statistics:
the Kolmogorov-Smirnov statistic (top left), the Hellinger distance
(top right), the Wasserstein distance (bottom left), and the KullbackLeibler divergence (bottom right). The curves correspond to S&P
500 (black), EURO STOXX 50 (dark grey), and KOSPI (light grey)
indices. The dotted lines are simulated confidence intervals, with
confidence levels, from the bottom to the top: 95%, 99% and 99.9%.
In addition to the evolution through time of the four divergence statistics, Figure 2 shows confidence
intervals for each statistic. These confidence intervals come from the simulation of 10,000 Brownian
motions on which we apply our method of density estimation and implementation of divergence
statistics. The null hypothesis H0 is thus that all the price returns are iid Gaussian variables. For
11

each statistic and each date, we represent the quantile estimated on simulations and corresponding
to three confidence levels: 95%, 99% and 99.9%. At a given date t, for a given stock index, if
the divergence of the current pdf with respect to the pdf in t0 is above a particular curve of the
confidence interval, we reject H0 with the corresponding confidence level p. In other words, we
consider the pdf in t to be significantly different from the pdf in t0 with a confidence level p.
Depending on the divergence considered, we are able to determine the peak of the impact as the
date maximizing the statistic. We display in Table 2 the date of the peak as well as the value of
the divergence at the peak, before the crisis, and late May in the Hellinger approach. According
to this table, the strongest impact is in the US but the recovery seems faster there than in Europe.
The smallest impact and the fastest recovery is by far in China. The peak occurs between the 25th
March (China and South-Korea) and the 6th April (US and Japan), whatever the index, except
for the DAX, whose peak is in May. We use the other divergences as a robustness check of these
results. The conclusions are in fact similar: small impact and almost total recovery late May for
the Chinese SSE 50 index, strongest impact on the US market, slowest recovery on the European
market. We also observe some variations in the estimation of the peak date. The most surprising
one is provided by the Kolmogorov-Smirnov statistic, according to which the peak is reached first
in Europe the 18th March, before continental Asia and US the 23rd March, and finally Japan the
2nd April.

NASDAQ Composite
S&P 500
S&P 100
EURO STOXX 50
Euronext 100
DAX
CAC 40
Nikkei 225
KOSPI
SSE 50

H on 7th Feb.
0.111
0.084
0.089
0.051
0.050
0.061
0.052
0.095
0.083
0.070

H at the peak
0.531
0.562
0.562
0.466
0.484
0.458
0.479
0.477
0.518
0.381

Date of the peak
2020-04-01
2020-04-06
2020-04-06
2020-03-27
2020-03-27
2020-05-05
2020-03-27
2020-04-06
2020-03-25
2020-03-25

H on 28th May
0.295
0.363
0.324
0.398
0.385
0.377
0.389
0.350
0.294
0.122

Table 2: Hellinger distance H with respect to t0 . The peak corresponds to when the maximal Hellinger distance is reached. Dates are
in 2020.
We stress the fact that the alternative chronology of the peaks is not the only particularity of
the Kolmogorov-Smirnov statistic with respect to the three other divergence statistics we have
implemented. For instance, the significance of the financial crisis in some regions is questionable
according to this divergence statistic, as one can see in Figure 2. We can nevertheless explain
this striking, and certainly dubious, conclusion. Indeed, when simulating two sets of iid random variables, we get two kernel densities but the Kolmogorov-Smirnov statistic focuses on only
one quantile, generally corresponding to where the cdf is the steepest, that is in the body of
the distribution. If we disrupt one of these two densities with a limited number of outliers, the
Kolmogorov-Smirnov statistic may not change a lot as this modification mainly impacts the tails
and not the body of the distribution. On the contrary, the three other divergence statistics are less
robust to outliers as they are defined by integrals over all the distribution. Their responsiveness
to a crisis is thus higher. For this reason, we prefer them to the Kolmogorov-Smirnov statistic for
assessing the significance of the variations of a dynamic pdf.

12

4

Conclusion

In this paper, we have introduced a new method to select the two free parameters of a dynamic
kernel density estimation, namely the discount factor and the bandwidth. This method relies on
the maximisation of the accuracy of the daily pdf. This accuracy is to be understood in the sense of
the literature about density forecast evaluation: the PIT of each new observation, expressed using
the time-varying distribution, forms a set of variables which must be iid uniform variables. We use
the Kolmogorov-Smirnov statistic and a discrepancy statistic to build a quantitative criterion of
accuracy of the pdf. It is this criterion that we try to maximize when selecting the bandwidth and
the discount factor of our time-varying pdf.
We have applied this method to financial data. In particular, we represent the evolution of the pdf
of daily price returns for several stock indices during the COVID-19 pandemic. We are thus able
to expose an accurate chronology of the financial crisis. Though the impact of the pandemic on
the Chinese market seems limited, we observe that the strongest impact occurred in the US. The
slowest recovery is in Europe, for which the pdf of daily returns is still significantly different from
a steady market late May 2020. On the contrary, the recovery of the Chinese and South-Korean
markets is very rapid. According to several divergence statistics late May 2020, they are even not
significantly different from what they were before the crisis.

References
[1] Ammy-Driss, A., and Garcin, M. (2020), Efficiency of the financial markets during the
COVID-19 crisis: time-varying parameters of fractional stable dynamics, Working paper
[2] Arias-Calluari, K., Alonso-Marroquin, F., Najafi, M.N., and Harré, M. (2020),
Forecasting the effect of COVID-19 on the S&P500, arXiv preprint
[3] Baker, S.R., Bloom, N., Davis, S.J., Kost, K.J., Sammon, M.C., and Viratyosin, T.
(2020), The unprecedented stock market impact of COVID-19, NBER working paper w26945
[4] Beran, R. (1977), Minimum Hellinger distance estimates for parametric models, Annals of
statistics, 5, 3: 445-463
[5] Berkowitz, J. (2001), Testing density forecasts, with applications to risk management, Journal of business & economic statistics, 19, 4: 465-474
[6] Bouezmarni, T., and Rombouts, J.V. (2010), Nonparametric density estimation for multivariate bounded data, Journal of statistical planning and inference, 140, 1: 139-152
[7] Bühlmann, P., and McNeil, A.J. (2002), An algorithm for nonparametric GARCH modelling, Computational statistics & data analysis, 40, 4: 665-683
[8] Davis, M.H. (2016), Verification of internal risk measure estimates, Statistics & risk modeling, 33, 3-4: 67-93
[9] Diebold, F.X., Gunther, T.A., and Tay, A.S. (1998), Evaluating density forecasts, with
applications to financial risk management, International economic review, 39: 863-883
[10] Diebold, F.X., Tay, A.S., and Wallis, K.F. (1999), Evaluating density forecasts of inflation: the survey of professional forecasters, in Engle, R.F., and White, H. (Eds.), Cointegration, causality, and forecasting: a Festschrift in honour of Clive W.J. Granger, Oxford
university press, pp. 76-90
13

[11] Fan, J., and Yao, Q. (1998), Efficient estimation of conditional variance functions in
stochastic regression, Biometrika, 85, 3: 645-660
[12] Forsberg, L., and Bollerslev, T. (2002), Bridging the gap between the distribution of
realized (ECU) volatility and ARCH modelling (of the Euro): the GARCH-NIG model, Journal
of applied econometrics, 17, 5: 535-548
[13] Garcin, M. (2017), Estimation of time-dependent Hurst exponents with variational smoothing
and application to forecasting foreign exchange rates, Physica A: statistical mechanics and its
applications, 483: 462-479
[14] Garcin, M., and Goulet, C. (2019), Non-parametric news impact curve: a variational
approach, to appear in Soft computing
[15] Garcin, M., and Guégan, D. (2014), Probability density of the empirical wavelet coefficients
of a noisy chaos, Physica D: nonlinear phenomena, 276: 28-47
[16] Gneiting, T., Balabdaoui, F., and Raftery, A.E. (2007), Probabilistic forecasts, calibration and sharpness, Journal of the royal statistical society: series B (statistical methodology),
69, 2: 243-268
[17] Harvey, A., and Oryshchenko, V. (2012), Kernel density estimation for time series data,
International journal of forecasting, 28, 1: 3-14
[18] Holzmann, H., and Eulert, M. (2014), The role of the information set for forecasting –
with applications to risk management, Annals of applied statistics, 8, 1: 595-621
[19] Jones, M.C., Marron, J.S., and Sheather, S.J. (1996), A brief survey of bandwidth
selection for density estimation, Journal of the American statistical association, 91, 433: 401407
[20] Ko, S.I., and Park, S.Y. (2013), Multivariate density forecast evaluation: a modified approach, International journal of forecasting, 29, 3: 431-441
[21] Kristan, M., Leonardis, A., and Skočaj, D. (2011), Multivariate online kernel density
estimation with Gaussian kernels, Pattern recognition, 44, 10-11: 2630-2642
[22] Lacoume, J.-L., Amblard, P.-O., and Comon, P. (1997), Statistiques d’ordre supérieur
pour le traitement du signal, Masson, Paris
[23] Li, Z., Liu, S., and Tian, M. (2014), Collective behavior of equity returns and market
volatility, Journal of data science, 12, 3: 545-561
[24] Luong, A., and Bilodeau, C. (2017), Simulated minimum Hellinger distance estimation
for some continuous financial and actuarial models, Open journal of statistics, 7, 4: 743-759
[25] Marsaglia, G., Tsang, W.W., and Wang, J. (2003), Evaluating Kolmogorov’s distribution, Journal of statistical software, 8, 18: 1-4
[26] Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T. (2012),
Parametric return density estimation for reinforcement learning, arXiv preprint
[27] Niederreiter, H. (1988), Low-discrepancy and low-dispersion sequences, Journal of number
theory, 30, 1: 51-70
[28] Niederreiter, H. (2017), Recent constructions of low-discrepancy sequences, Mathematics
and computers in simulation, 135: 18-27
14

[29] Ñı́guez, T.M., and Perote, J. (2017), Moments expansion densities for quantifying financial risk, North American journal of economics and finance, 42: 53-69
[30] Pagan, A.R., and Schwert, G.W. (1990), Alternative models for conditional stock volatility, Journal of econometrics, 45, 1-2: 267-290
[31] Panaretos, V.M., and Zemel, Y. (2019), Statistical aspects of Wasserstein distances,
Annual review of statistics and its application, 6: 405-431
[32] Pavlyshenko, B.M. (2020), Regression approach for modeling COVID-19 spread and its
impact on stock market, arXiv preprint
[33] Rosenblatt, M. (1952), Remarks on a multivariate transformation, Annals of mathematical
statistics, 23, 3: 470-472
[34] Scaillet, O. (2004), Density estimation using inverse and reciprocal inverse Gaussian kernels, Nonparametric statistics, 16, 1-2: 217-226
[35] Semeyutin, A., and O’Neill, R. (2019), A brief survey on the choice of parameters for:
”Kernel density estimation for time series data”, North American journal of economics and
finance, 50: 101038
[36] Silverman, B.W. (1986), Density estimation for statistics and data analysis, CRC press
[37] Tokat, Y., Rachev, S.T., and Schwartz, E.S. (2003), The stable non-Gaussian asset
allocation: a comparison with the classical Gaussian approach, Journal of economic dynamics
and control, 27, 6: 937-969
[38] Tsybakov, A.B. (2008), Introduction to nonparametric estimation, Springer science & business media
[39] Tuffin, B. (1996), On the use of low discrepancy sequences in Monte Carlo methods, Monte
Carlo methods and applications, 2, 4: 295-320
[40] Villani, C. (2003), Topics in optimal transportation, American mathematical society, 58
[41] Wand, M.P., and Jones, M.C. (1994), Kernel smoothing, CRC press
[42] Wang, X., Tsokos, C.P., and Saghafi, A. (2018), Improved parameter estimation of
time dependent kernel density by using artificial neural networks, Journal of finance and data
science, 4, 3: 172-182
[43] Wegman, E.J., and Davies, H.I. (1979), Remarks on some recursive estimators of a probability density, Annals of statistics, 7, 2: 316-327
[44] Zhang, T., and Wu, W.B. (2015), Time-varying nonlinear regression models: nonparametric estimation and model selection, Annals of statistics, 43, 2: 741-768

15

