Tilted Nonparametric Regression Function Estimation
Farzaneh Boroumand1,2 , Mohammad T. Shakeri2 , Nino Kordzakhia1,3 ,
Mahdi Salehi4 , Hassan Doosti1∗

arXiv:2102.02381v1 [stat.ME] 4 Feb 2021

1

Department of Mathematics and Statistics, Faculty of Science and Engineering, Macquarie University, NSW, Australia.
(Farzaneh.Boroumand@mq.edu.au)
2

Department of Biostatistics, Health School, Mashhad University of Medical Sciences, Mashhad, Iran.
(shakerimt@mums.ac.ir)
3
4

(Nino.Kordzakhia@mq.edu.au)

Department of Mathematics and Statistics, University of Neyshabur, Neyshabur, Iran.
(salehi.sms@neyshabur.ac.ir)
∗

Correspond author: Hassan.Doosti@mq.edu.au

Abstract
This paper provides the theory about the convergence rate of the tilted version of linear
smoother. We study tilted linear smoother, a nonparametric regression function estimator,
which is obtained by minimizing the distance to an infinite order flat-top trapezoidal kernel
estimator. We prove that the proposed estimator achieves a high level of accuracy. Moreover,
it preserves the attractive properties of the infinite order flat-top kernel estimator. We also
present an extensive numerical study for analysing the performance of two members of the
tilted linear smoother class named tilted Nadaraya-Watson and tilted local linear in the finite
sample. The simulation study shows that tilted Nadaraya-Watson and tilted local linear
perform better than their classical analogs in some conditions in terms of Mean Integrated
Squared Error (MISE). Finally, the performance of these estimators as well as the conventional
estimators were illustrated by curve fitting to COVID-19 data for 12 countries and a doseresponse data set.
Keywords— Tilted estimators; Nonparametric regression function estimation; Rate of
convergence; Infinite order flat top kernels

1

Introduction

Let the regression model be
Yi = r(Xi ) + i , 1 ≤ i ≤ n,

(1.1)

where (Y1 , X1 ), (Y2 , X2 ), . . . , (Yn , Xn ), are the data pairs, the design variable X ∼ fX , X and  are
independent, i ’s are independent and identically distributed (iid) errors with zero mean E() = 0
and variance E(2 ) = σ 2 . The regression function r and fX are unknown. In this paper, we
will focus on a nonparametric approach to estimate r. The main subject of this study is a class
of nonparametric estimators called linear smoother. Nadaraya-Watson estimator and local linear
estimator are two prevailing members of this class of estimators. An estimator r̆ of r, is said to be

1

a linear smoother if it can be written in a form of linear function of weighted Y sample. Let the
weight-vector be
l(x) = (l1 (x), ..., ln (x))T .
Then the linear smoother r̆ can be written as
r̆n (x) = l(x)T Y =

Xn
i=1

li (x)Yi ,

(1.2)

Pn
where i=1 li (x) = 1, see Buja et al. [1]. Nadaraya-Watson estimator and local linear estimator
can be written as a form of linear smoother with the following weight functions. The weight
functions for Nadaraya-Watson smoother, see Nadaraya [2], Watson [3], are
K( Xih−x )
, i = 1, . . . , n.
li,N W (x) = Pn
Xj −x
j=1 K( h )

(1.3)

For the standard local linear smoother the weight functions are defined as follows,
bi (x)
li,ll (x) = Pn
, i = 1, . . . , n,
j=1 bj (x)
bi (x) = K(

(1.4)

Xi − x
)(Sn ,2 (x) − (Xi − x)Sn ,1 (x)), i = 1, . . . , n,
h

Xi − x
)(Xi − x)j , j = 1, 2.
i=1
h
where K is a kernel function. The kernel function depends on the bandwidth, or smoothing,
parameter h and assigns weights to the observations according to the distance to the target point
x, see McMurry and Politis, [4]. The small values of h cause the neighboring points of x to have
the larger influence on the estimate leading to curvature changes in the estimated curve. The
larger values of h imply that the distanced data points will have the same effect as the neighboring
points on the local fit, resulting in a smoother estimate. Thus finding an optimal h is the essential
task in the estimation procedure, see Wasserman [5]. One of the ways finding the optimal h is by
minimising the leave-one-out cross validation score function, [5]. The leave-one-out cross validation
score is defined by
n
1X
CV = R̂(h) =
(Yi − r̆(−i) (Xi ))2 ,
(1.5)
n i=1
Sn ,j (x) =

Xn

K(

where r̆(−i) (Xi ) is obtained from (1.2) by omitting the ith pair (Xi , Yi ). In this work, we will
present the tilted versions of linear smoother. A tilting technique applied to an empirical distribution, leads to replacing 1/n data weights from uniform distribution by pi , 1 ≤ i ≤ n, from
general multinomial distribution over data. Hall and Yao [6] studied asymptotic properties of
the tilted regression estimator with autoregressive errors using generalized empirical likelihood
method, which typically involves solving a non-linear and high dimensional optimization problem.
Grenander [7] introduced a tilted method to impose restrictions on the density estimates. There
are two approaches to estimating of the tilting parameters: Empirical likelihood and Distance
Measure based approaches. The empirical likelihood-based method is a semi-parametric method
which provides a convenience of adding a parametric model through estimating equations. Owen
2

[8] proposed an empirical likelihood to be used as an alternative to the likelihood ratio tests, and
derived its asymptotic distribution. Chen [9], Zhang [10], Schick et al. [11], Müller et al. [12] further developed the empirical likelihood-based method for estimating the tilting parameters. Chen
[9] applied the empirical likelihood method to estimate the tilting parameters pi , 1 ≤ i ≤ n, under
the constraints on the shape of distribution. In his kernel-based estimator, n−1 was replaced by
the weights obtained from the empirical likelihood method. In [9] it was proved that the proposed
estimator has a smaller variance than the conventional kernel estimators. Schick et al.[11], also
used the similar approach obtaining the consistent tilted estimator with higher efficiency than that
of conventional estimators in the autoregression framework. In contrast in the Distance Measure
approach, the tilted estimators are defined by minimizing distances, conditional to various types of
constraints. Hall and Presnell [13], Hall and Huang [14], Carroll et al. [15], Doosti and Hall [16],
Doosti et al. [17] used the setup-specific Distance Measure approaches for estimating the tilting
parameters. Carroll [15], proposed a new approach for density function estimation, and regression
function estimation as well as hypothesis testing under shape constraints in the model with measurement errors. A tilting method used in [15] led to curve estimators under some constraints.
Doosti and Hall [16] introduced a new higher order nonparametric density estimator, using tilting
method, where they used L2 -metric between the proposed estimator and a consistent ’Sinc’ kernel
based estimator. Doosti et al. [17], have introduced a new way of choosing the bandwidth and
estimating the tilted parameters based on the cross-validation function. In [17], it was shown that
the proposed density function estimator had improved efficiency and was more cost-effective than
the conventional kernel-based estimators studied in this paper.
In this work, we propose a new tilted version of a linear smoother which is obtained by minimising the distance to a comparator estimator. The comparator estimator is selected to be an infinite
order flat-top kernel estimator. This class of estimators is characterized by a Fourier transform,
which is flat near the origin and infinitely differentiable elsewhere, see [18]. We prove that the
tilted estimators achieve a high level of accuracy, yet preserving the attractive properties of an
infinite-order flat-top kernel estimator.
The rest of this paper contains the additional four sections and Appendix. In the Section
2, we provide the notation, definitions and preliminary results. The Section 2 also includes the
definition of an infinite-order estimator, as a comparator estimator. Section 3 contains the main
results formulated in Theorems 1-3. We present a simulation study in the Section 4. The real data
applications are provided in the Section 5. The proof of the main theorem is accommodated in the
Appendix.

2

Notation and preliminary results

Definition 2.1. A general infinite order flat-top kernel K is defined
Z
∞
1
λ(s)e−isx ds,
K(x) =
2π −∞
where λ(s) is the Fourier transform of kernel K, and c > 0 is a fixed constant.
(
1,
| s |≤ c
λ(s) =
,
g(| s |), | s |> c
and g is not unique and it should be chosen to make λ(s), λ2 (s), and sλ(s) integrable [4].
3

(2.1)

2.1

Infinite order flat-top kernel regression estimator

Let ř be a linear smoother
ř =
where

Xn
i=1

ˇli (xi )Yi ,

(2.2)

Xi −x
ˇli (x) = P K( h )
Xj −x
n
j=1 K( h )

and K is an infinite order flat top kernel from (2.1), also see McMurry and Politis [4]. The
trapezoidal kernel
2(cos(x/2) − cos(x))
K(x) =
,
πx2
is an infinite order flat top kernel satisfying Definition 2.1 since the Fourier transform of K(x) is


| s |≤ 1/2,
1
λ(s) = 2(1− | s |) 1/2 <| s |≤ 1,


0
| s |> 1.

2.2

Tilted linear smoother

We define tilted linear smoother as follows
r̂n (x|h, p) =

Xn
i=1

pi li (xi )Yi ,

(2.3)

Pn
where pi ’s are tilting parameters, pi ≥ 0 and i pi = 1. The bandwidth parameter h and the
vector of tilting parameters p = (p1 , · · · , pn ), are to be estimated. In Section 4, we evaluate the
performance of tilted versions of Nadaraya-Watson (1.2) and standard local linear estimators (1.3)
in finite samples.

3

Main results

Let r̂n (.|θ) be the tilted linear smoother from (2.3) for the regression function r, where θ = (h, p)
is a vector of unknown parameters. Further ř from (2.2) will be used as a comparator estimator of
r, ř can be any estimator with an optimal convergence rate [18]. We will estimate θ by minimising
the L2 − distance between r̂n (.|θ) and ř preserving the convergence rate of ř, provided the following
assumptions hold
(a) kř − rk = Op (δn )
(b) There exists θ̃ such that r̂n (.|θ̃) and ř possess the same convergence rates, i.e. kr̂n (.|θ̃) − rk =
Op (δn ),
where δn ≥ 0 converges to 0 as n tends to ∞, e.g. δn = n−c for some c ∈ (0, 1/2). A further
discussion on the assumptions (a)-(b) can be found in Doosti and Hall, [16].
We define θ̂ as the solution to the optimisation problem as
θ̂ = arg min kr̂n (.|θ) − řk
θ

4

(3.1)

subject to the constraints for the bandwidth parameter h > 0 and vector p introduced in Section
2.2.
In Theorem 1, we show that the convergence rate of r̂n (.|θ̂) and ř is Op (δn ).
Theorem 1. If the assumptions (a)-(b) hold then for any θ̂ which fulfills (3.1) we have
kr̂n (.|θ̂) − rk = Op (δn ).
Proof. Due to Assumption (a), there exists θ̃ such that
kr̂n (.|θ̃) − řk ≤ kr̂n (.|θ̃) − rk + kr − řk = Op (δn ),
in which the first equation is a result of the triangle inequality, and specifically from the fact that
kr − řk = Op (δn );

(3.2)

see assumption, part(a). If θ̃ is as in assumption 1, part (2) then
kr̂n (.|θ̂) − řk ≤ kr̂n (.|θ̃) − řk = Op (δn ).

(3.3)

Together, results (3.2) and (3.3) imply Theorem 1.
Theorem 1 implies that the convergence rate of r̂n (.|θ̂) estimator coincides with that of ř with
the bandwidth parameter h replaced by its ‘plug-in’ type estimate similar to that from [4] and [18].
The regression function r ∈ C, where C is a class of regression functions, if
lim lim sup sup[P {kr̂n (.|θ̃) − rk ≥ Cδn } + P {kř − rk ≥ Cδn }] = 0,

C→∞ n→∞

(3.4)

r∈C

subject to existence of θ̃.
Theorem 2. If (3.4) holds for regression functions from C then
lim lim sup sup P {kr̂n (.|θ̂) − rk ≥ Cδn } = 0.

C→∞ n→∞

(3.5)

r∈C

Theorem 2 states that r̂n (.|θ̂) and ř converge to r uniformly in C.
Let X1 , X2 , ..., Xn be iid random variables with probability density function (pdf) f (x) and
ĝ(x) be its kernel based density function estimator
n

ĝ(x) =

1 X
x − Xi
K(
)
nh i=1
h

and g(x) = Ef ĝ(x).
Suppose that (c) - (d) hold for φK and φq , Fourier transforms for K and q = r · g, respectively,
Pk
(c) φK (t)−1 = 1 + j=1 cj t2j , c1 , ..., ck are real numbers;
R
(d) |φq (t)||t|2k dt < ∞;
(e) RFor constants C1 , ..., C5 > 0, and j = 1, ..., k the derivatives q (2j) exist, |q (2j) (x)| ≤ C1 and
|q (2j) (x)| ≤ C1 , and either
5

(1) |q (2j) (x)/r · f (x)| ≤ C1 for all x, or
(2) |q (2j) (x)/r · f (x)| ≤ C1 (1 + |x|)C2 for all x and P (|X| ≥ x) ≤ C3 exp(−C4 xC5 ), x > 0.
(f) Under assumption (e)-(1), δn → 0 as n → ∞, so that n1/2 δn → ∞ , and under assumption
(e)-(2), n1/2 log(n)−C2 /4C5 δn → ∞, where C2 and C5 are defined in (e)-(2).
Assumption (c)-(f) are reasonable and considered in tilted density function estimation in [16]. It
is anticipated that kr̂n (.|θ̂) − rk = Op (δn ), where δn converges to 0 slower than n−1/2 as shown in
Theorem 3. Next we formulate the assumption using the first term of the expression in the left
hand side of (3.4)
lim lim sup sup Pr (kř − rk > Cδn ) = 0.
(3.6)
C→∞ n→∞

r∈C

Theorem 3. Let (a)-(f) be valid and θ̂ be defined in (3.1).
I. If in addition kř − rk = Op (δn ) then kr̂n (.|θ̂) − rk = Op (δn ).
II. If the assumptions in (e) hold uniformly for q ∈ C and obtain (3.6) then (3.5) is valid.
The proof of Theorem 3 is given in the Appendix.

4

Simulation study

We present the results of simulation study of the performance of tilted estimators in various
settings. Data were generated using exponential function and sin regression functions with normal
and uniform design distributions. Four samples of sizes n = (60, 100, 200, 1000) were sampled from
the population with regression errors which had standard deviations σ = (0.3, 0.5, 0.7, 1, 1.5, 2). In
the setting for each set of σ and n we generated 500 data sets. The Median Integrated Squared
Error (MISE) was estimated using the Monte Carlo method. The leave-one-out cross validation
score from (1.5) was employed to choose the optimal bandwidths for Nadaraya-Watson and local
linear estimators, [5]. For an infinite order flat-top kernel estimator, bandwidth was selected using
the rule of thumb introduced by McMurry and Politis [4] as part of ’iosmooth’. The bandwidth
parameters for tilted estimators were estimated
√ using our suggested procedure. The exponential
regression function r1 (x) = x + 4exp(−2x2 )/ 2π. The design densities were taken to be uniform
on [−2, 2] and N (0, 1). The Integrated Squared Error (ISE) was calculated over the interval [−2, 2].
The sin regression function r2 (x) = sin(4πx) was paired with the uniform design density on [0, 1].
The ISE has been calculated over [0, 1] and [0.15, 0.85], the latter is chosen for addressing boundary
effect.

6

In Table 1 we provide the MISEs for the proposed estimators, the comparator estimator, and the
conventional estimators. Data were generated using r1 (x) regression function along with normal
design density and normal distribution for the error term. It is evident that for moderate sample
size (n=200) and a large sample size (n=1000) and for the medium standard deviation (0.5 and
0.7), the tilted estimators outperformed other estimators. Moreover, for larger sample size, as
standard deviation of error terms increases, the MISE of tilted estimators decreases.
Table 1: MISE for Infinite Order (IO) estimator with the trapezoidal kernel, Nadaraya-Watson
(NW) estimator, standard local linear (LL) estimator, tilted NW estimator with 4 (NW p4) and 10
(NW p10) weighting nodes, tilted LL estimator with 4 (LL p4) and 10 (LL p10) weighting nodes,
Exponential regression function and normal design density function. In each row the minimum
MISE is highlighted in bold.
n

60

100

200

1000

σ
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2

IO
0.2070
0.2771
0.3755
0.5888
1.1451
1.9295
0.0814
0.1382
0.2233
0.3955
0.8122
1.4227
0.04982
0.0740
0.1071
0.1743
0.3395
0.5732
0.02481
0.0293
0.0353
0.0502
0.0825
0.1279

NW
0.0861
0.1630
0.2710
0.4626
0.9115
1.5667
0.0571
0.1044
0.1649
0.2842
0.5757
0.9750
0.02886
0.0632
0.1092
0.2046
0.43460
0.7590
0.0078
0.0173
0.0287
0.0494
0.08977
0.1442

LL
0.0742
0.1486
0.2432
0.4110
0.8514
1.4383
0.0462
0.0893
0.1454
0.2534
0.5228
0.8954
0.0267
0.0553
0.0857
0.1407
0.2751
0.4676
0.0121
0.0197
0.0288
0.0466
0.0812
0.1254

7

NW p4
0.1422
0.2152
0.3039
0.5055
0.9576
1.6168
0.0609
0.1131
0.1879
0.3343
0.6764
1.1569
0.03518
0.0589
0.0907
0.1526
0.3022
0.5068
0.0127
0.0194
0.02748
0.0443
0.0799
0.1292

NW p10
0.1676
0.2294
0.3275
0.5597
1.0860
1.7774
0.06478
0.1168
0.1903
0.3616
0.7340
1.3134
0.0435
0.0640
0.0936
0.1568
0.3232
0.5470
0.0222
0.0251
0.0304
0.0435
0.0776
0.1243

LL p4
0.1194
0.1930
0.3009
0.5042
0.9590
1.6119
0.05156
0.1055
0.1744
0.3166
0.6802
1.1310
0.0293
0.0552
0.08777
0.1528
0.3007
0.4991
0.0101
0.0166
0.0247
0.0420
0.0814
0.1299

LL p10
0.1566
0.2203
0.3223
0.5261
1.0620
1.7292
0.0595
0.1107
0.1846
0.3480
0.7217
1.2404
0.0377
0.05780
0.0867
0.1527
0.3165
0.5310
0.0187
0.0225
0.0277
0.04245
0.0752
0.1246

In Table 2, we provide the MISEs for simulated data using the exponential regression function
r1 (x) With the uniform design density and the random normal error term. For fixed sample size,
as the standard deviation increases, the tilted estimators otperform others. Although, for large
sample sizes, the conventional estimators tend to perform better than tilted estimators. For smaller
sample sizes and the moderate standard deviation levels, the tilted N-W estimator remains superior
to the conventional estimators at some extent.
Table 2: MISE for Infinite Order (IO) estimator with the trapezoidal kernel, Nadaraya-Watson
(NW) estimator, standard local linear (LL) estimator, tilted NW estimator with 4 (NW p4) and 10
(NW p10) weighting nodes, tilted LL estimator with 4 (LL p4) and 10 (LL p10) weighting nodes,
exponential regression function and uniform design density. In each row the minimum MISE is
highlighted in bold.
n

60

100

200

1000

σ
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2

IO
0.1559
0.1980
0.2515
0.3588
0.6530
1.0524
0.1195
0.1432
0.1781
0.2490
0.4165
0.6487
0.0991
0.1089
0.1256
0.1577
0.2401
0.3568
0.0801
0.0823
0.0853
0.0922
0.1080
0.1294

NW
0.0663
0.1398
0.2152
0.3697
0.6281
0.9892
0.0442
0.0914
0.1443
0.2324
0.4366
0.6780
0.0232
0.0470
0.0822
0.1299
0.2542
0.3878
0.0058
0.0125
0.0207
0.0356
0.0716
0.1286

LL
0.0566
0.1362
0.2098
0.3599
0.6821
2.2171
0.0360
0.0809
0.1376
0.2438
0.5221
0.8402
0.0209
0.0441
0.0757
0.1351
0.3349
0.4218
0.0056
0.01286
0.0206
0.0359
0.0670
0.1056

NW p4
0.1308
0.1724
0.2316
0.3418
0.6197
0.9871
0.1034
0.1253
0.1607
0.2305
0.4041
0.6107
0.0891
0.0993
0.1172
0.1533
0.2386
0.3464
0.0776
0.0790
0.0801
0.0830
0.0972
0.1209

8

NW p10
0.1529
0.1953
0.2492
0.3650
0.6520
1.0597
0.1191
0.1426
0.1766
0.2469
0.4144
0.6371
0.0997
0.1086
0.1253
0.1589
0.2416
0.3534
0.0800
0.0825
0.0845
0.0917
0.1074
0.1308

LL p4
0.1237
0.1690
0.2406
0.3691
0.6676
1.0287
0.0982
0.1218
0.1581
0.2497
0.4373
0.6619
0.0833
0.0944
0.1107
0.1554
0.2587
0.3938
0.0724
0.0718
0.07170
0.0728
0.0911
0.1235

LL p10
0.1470
0.1901
0.2433
0.3608
0.6692
1.0287
0.1156
0.1372
0.1695
0.2460
0.4198
0.6401
0.0972
0.1063
0.1228
0.1590
0.2426
0.3573
0.0797
0.0819
0.0843
0.0895
0.1041
0.1271

Table 3 presents the MISEs for the simulated data using sin function with uniform design
density and normal random errors. For fixed sample size and moderate standard deviations 0.5
and 0.7, the tilted estimators perform better than conventional estimators. For sample size n=1000
with and increasing standard deviation, the tilted estimators demonstrate better performance over
others.
Table 3: MISE for Infinite Order (IO) estimator with the trapezoidal kernel, Nadaraya-Watson
(NW) estimator, standard local linear (LL) estimator, tilted NW estimator with 4 (NW p4) and 10
(NW p10) weighting nodes, tilted LL estimator with 4 (LL p4) and 10 (LL p10) weighting nodes,
sin regression function, uniform design density, edges included. In each row the minimum MISE is
highlighted in bold.
n

60

100

200

1000

σ
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2

IO
0.04034
0.0663
0.1085
0.1958
0.4036
0.7003
0.0222
0.0371
0.0595
0.1054
0.2183
0.3784
0.0123
0.0191
0.0299
0.0522
0.1073
0.1830
0.0056
0.0070
0.0089
0.0132
0.0234
0.0376

NW
0.0286
0.0638
0.0840
0.1703
0.2410
0.3739
0.0166
0.0326
0.0533
0.0936
0.1541
0.2869
0.0093
0.0190
0.0306
0.0492
0.0885
0.1334
0.0023
0.0049
0.0082
0.0145
0.0254
0.0424

LL
0.0234
0.0570
0.1329
0.1424
0.2652
0.3958
0.0129
0.0286
0.0498
0.0868
0.1728
0.2985
0.0070
0.0161
0.0273
0.0494
0.1043
0.1432
0.0019
0.0042
0.0071
0.0127
0.0241
0.0386

9

NW p4
0.0315
0.0585
0.0971
0.1731
0.3616
0.6198
0.0193
0.0348
0.0560
0.1008
0.2078
0.3520
0.0112
0.0195
0.0308
0.0533
0.1046
0.1770
0.0046
0.0065
0.0091
0.01402
0.0250
0.0404

NW p10
0.0331
0.0597
0.0969
0.1749
0.3718
0.6463
0.0197
0.0349
0.0554
0.1004
0.2072
0.3580
0.0114
0.0194
0.0296
0.0517
0.1048
0.1789
0.0051
0.0066
0.0091
0.01346
0.0242
0.0388

LL p4
0.0228
0.0507
0.0897
0.1714
0.3600
0.6203
0.0130
0.0282
0.0506
0.0989
0.2067
0.3573
0.0067
0.0143
0.0251
0.0484
0.1044
0.1812
0.0018
0.0034
0.0056
0.0101
0.0214
0.0369

LL p10
0.0264
0.0534
0.090
0.1692
0.3604
0.6307
0.0152
0.0308
0.0517
0.09720
0.2032
0.3549
0.0080
0.0150
0.0260
0.0471
0.1011
0.1752
0.0027
0.0043
0.0064
0.0108
0.0212
0.0358

For studying boundary effect the results provided in Table 3 and 4 were evaluated under the
identical experimental specifications except the MISEs in Table 4 were evaluated over [0.15,0.85].
According to the results when the sample size and standard deviation increased, the tilted estimators demonstrated improved performance.
Table 4: MISE for Infinite Order (IO) estimator with the trapezoidal kernel, Nadaraya-Watson
(NW) estimator, standard local linear (LL) estimator, tilted NW estimator with 4 (NW p4) and 10
(NW p10) weighting nodes, tilted LL estimator with 4 (LL p4) and 10 (LL p10) weighting nodes,
sin regression function, uniform design density, edges excluded. In each row the minimum MISE
is highlighted in bold.
n

60

100

200

1000

σ
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2
0.3
0.5
0.7
1
1.5
2

IO
0.0261
0.0461
0.0740
0.1315
0.2736
0.4747
0.0132
0.0233
0.0380
0.0688
0.1459
0.2536
0.0063
0.0112
0.0183
0.0330
0.0679
0.1160
0.0013
0.0022
0.0035
0.0064
0.0136
0.0236

NW
0.0182
0.0421
0.0498
0.1130
0.1461
0.2646
0.0101
0.0194
0.0307
0.0562
0.0912
0.1750
0.0054
0.0121
0.01733
0.0296
0.0531
0.0816
0.0014
0.0028
0.0045
0.0089
0.0152
0.0247

LL
0.0143
0.0327
0.0791
0.0796
0.1468
0.2208
0.0078
0.0168
0.0285
0.0493
0.0943
0.1584
0.0044
0.0095
0.0164
0.0300
0.0627
0.0834
0.0012
0.0026
0.0044
0.0077
0.0144
0.0229

10

NW p4
0.0201
0.03747
0.0628
0.1189
0.2519
0.4271
0.0108
0.0201
0.0340
0.0626
0.1319
0.2304
0.0054
0.0100
0.0162
0.0298
0.0636
0.1116
0.0011
0.0019
0.0032
0.0060
0.0126
0.0219

NW p10
0.0213
0.0394
0.0638
0.1209
0.2610
0.4508
0.0109
0.0207
0.0346
0.0641
0.1365
0.2391
0.0054
0.0102
0.0166
0.0307
0.0653
0.1125
0.0011
0.0018
0.0033
0.0062
0.0131
0.0229

LL p4
0.0142
0.0319
0.05783
0.1095
0.2420
0.4215
0.0082
0.0173
0.0308
0.0587
0.1270
0.2228
0.0040
0.0084
0.0149
0.0288
0.0610
0.1094
0.0009
0.0017
0.0029
0.0057
0.0123
0.0218

LL p10
0.0184
0.0365
0.0618
0.1184
0.2513
0.4406
0.0096
0.0197
0.0331
0.0626
0.1349
0.2362
0.0049
0.0097
0.0164
0.0302
0.0645
0.1132
0.0010
0.0018
0.0031
0.0060
0.0130
0.0225

It is a known fact that the performance of Nadaraya-Watson estimator deteriorates near edges,
[19]. This effect is often referred to as a boundary problem. The results presented in Table 3 and
4, illustrate that the for scenarios (n = 60, σ = 0.5), (n = 200, σ = 0.7) and (n = 1000, σ =
{1, 1.5, 2}) the tilted Nadaraya-Watson estimator outperformed its classical counterpart. From the
boxplots in Figure 1 it is evident that the tilted estimators have smaller median ISEs. The extreme
values of the ISEs for the tilted estimators are smaller than these of the conventional estimators.
Similarity between the ISE distributions and their spreads of the IO and tilted estimators can also
be seen in Figure 1.

Figure 1: Boxplots of Integrated Square Errors (ISE) for Infinite Order (IO) estimator with the
trapezoidal kernel, Nadaraya-Watson (NW) estimator, standard local linear (LL) estimator, tilted
NW estimator with 4 (NW p4) and 10 (NW p10) weighting nodes, tilted LL estimator with 4
(LL p4) and 10 (LL p10) weighting nodes, sin regression function, edges excluded, n = 1000 and
σ = 0.7

11

In this simulation study for carrying out the MISE analysis, we had 500 replications done using
Monte Carlo method. For MISE evaluation at each combination of an estimator, a function, a
standard deviation and for a fixed sample size we had to solve the optimization problem. For
the numerical implementation, we used the parallel computing technique in R facilitated through
’snow’, ’doparallel’, and ’foreach’ packages.

5

Real data

In this section, we study the performance of tilted estimators in the real data environment.

5.1

COVID-19 Data

The tilted N-W estimator along with two other kernel-based estimators are being used for a curve
fitting to the COVID-19 data. We shall apply the tilted N-W estimator approach to daily confirmed
new cases and number of daily death for 12 countries including Iran, Australia, Italy, Belgium, Germany, Spain, Brazil, United Kingdom, Canada, Chile, South Africa and United States of America,
from 23 February 2020 to 28 October 2020, downloaded from https://www.ecdc.europa.eu. The
logarithmic transformation has been applied, and when the number of deaths or new confirmed
cases were zero we altered these observations by a positive value eliminating the associated singularity issue. The optimal bandwidth for each Nadaraya-Watson estimator was found through
minimization of relevant cross-validation function, [5], at the same time we kept the bandwidth
fixed for an infinite order flat-top kernel (IO) estimator which was found using Mcmurry and
Politis’ rule of thumb, [4]. Along with the tilted N-W estimator, we applied the NW, and IO estimators. The tilted NW estimator performed the best in terms of the Mean Square Errors (MSE).
Table 5 and 6 provide the MSE for each estimator for the confirmed new cases and number of
deaths. In terms of minimising the MSE, the tilted NW estimator ranked first, followed by IO
and N-W estimators. Slightly, improved performance of the tilted NW estimator is attributed to
the lower MSE components at the edges versus other kernel-based regression function estimators
which are generally known for so-called "edge effect", [20]. Figures 2 and 3 shows curve fit to the
daily confirmed cases and daily deaths respectively.

12

13

Figure 2: Daily confirmed cases for 12 countries including Iran, Australia, Italy, Belgium, Germany,
Spain, Brazil, United Kingdom, Canada, Chile, South Africa and United States of America

Table 5: COVID-19 Daily Confirmed Cases: MSE for Nadaraya-Watson (NW), Infinite Order
(IO), and tilted (NW p4) estimators. In each row the minimum MSE is highlighted in bold.
Country
Iran
Australia
Italy
Belgium
Germany
Spain
Brazil
United Kingdom
Canada
Chile
South Africa
United States of America

IO
324
5.48
445
2357
641
41621
108590
1789
175
6040
1158
43910

NW
326
5.41
1590
3071
1029
41754
108246
2083
183
6685
1403
46512

NW p4
305
5.01
372
2232
594
41241
107781
1737
173
5991
1133
41694

Table 6: COVID-19 Death: MSE for Nadaraya-Watson (NW), Infinite Order (IO), and tilted
(NW p4) estimators. In each row the minimum MSE is highlighted in bold.
Country
Iran
Australia
Italy
Belgium
Germany
Spain
Brazil
United Kingdom
Canada
Chile
South Africa
United States of America

14

IO
1.36
0.0225
1.97
0.0699
0.71
37.74
63.99
13.57
0.40
9.10
3.32
201.39

NW
1.39
0.0223
3.66
0.3077
0.82
37.92
64.21
14.50
0.47
9.73
3.50
205.98

NW p4
1.32
0.0222
1.89
0.0690
0.68
36.77
63.86
13.01
0.39
8.95
3.22
197.76

15

Figure 3: Daily deaths for 12 countries including Iran, Australia, Italy, Belgium, Germany, Spain,
Brazil, United Kingdom, Canada, Chile, South Africa and United States of America

5.2

Dose-Response data

The dose-response data refers to a study of phenylephrine effects on rat corpus cavernosum strips.
This data first appeared in Boroumand et al. [21] where the dose-response curves to phenylephrine
(0.1 µM to 300 µM ) were obtained by applying the robust four-parameter logistic (4PL) regression.
Here we have used a tilted smoother approach to dose-response curve fitting. In terms of Mean
Square Errors (MSEs) the tilted local linear estimators performed better than local linear, infinite
order flat-top kernel estimators including the robust 4PL model. The fitted dose-response curves
using the tilted local linear, local linear, infinite order flat-top kernel estimator, and 4PL model
are plotted in Figure 4. The corresponding MSEs are listed in the caption of Figure 4.

Figure 4: Dose-response curves: MSE for tilted local linear, local linear, infinite order flat-top
kernel estimator and 4PL model are 95.1023, 95.3267, 95.53077 and 110.7539, respectively.
The original dose-response data contained the outliers and the standard 4PL model had a poor
fit. Due to this we compared the performance of the tilted estimator with the robust 4PL model.
The tilted local linear estimator outperformed the robust 4PL model in terms of MSE.

Acknowledgement
This research was undertaken with the assistance of resources and services from the National
Computational Infrastructure (NCI), which is supported by the Australian Government. This
research forms part of the first author’s PhD thesis approved by the ethics committee of Mashhad
University of Medical Sciences with the project code 971017.

16

References
[1]

Andreas Buja, Trevor Hastie, and Robert Tibshirani. “Linear smoothers and additive models”. In: The Annals of Statistics (1989), pp. 453–510.

[2]

Elizbar A Nadaraya. “On estimating regression”. In: Theory of Probability & Its Applications
9.1 (1964), pp. 141–142.

[3]

Geoffrey S Watson. “Smooth regression analysis”. In: Sankhyā: The Indian Journal of Statistics, Series A (1964), pp. 359–372.

[4]

Timothy L McMurry and Dimitris N Politis. “Nonparametric regression with infinite order
flat-top kernels”. In: Journal of Nonparametric Statistics 16.3-4 (2004), pp. 549–562.

[5]

Larry Wasserman. All of nonparametric statistics. Springer Science & Business Media, 2006.

[6]

Peter Hall and Qiwei Yao. “Data tilting for time series”. In: Journal of the Royal Statistical
Society: Series B (Statistical Methodology) 65.2 (2003), pp. 425–442.

[7]

Ulf Grenander. “On the theory of mortality measurement: part ii”. In: Scandinavian Actuarial
Journal 1956.2 (1956), pp. 125–153.

[8]

Art B Owen. “Empirical likelihood ratio confidence intervals for a single functional”. In:
Biometrika 75.2 (1988), pp. 237–249.

[9]

Song Xi Chen. “Empirical likelihood-based kernel density estimation”. In: Australian Journal
of Statistics 39.1 (1997), pp. 47–56.

[10]

Biao Zhang. “A note on kernel density estimation with auxiliary information”. In: Communications in Statistics-Theory and Methods 27.1 (1998), pp. 1–11.

[11]

Anton Schick and Wolfgang Wefelmeyer. “Improved density estimators for invertible linear processes”. In: Communications in Statistics—Theory and Methods 38.16-17 (2009),
pp. 3123–3147.

[12]

Ursula U Müller, Anton Schick, and Wolfgang Wefelmeyer. “Weighted residual-based density
estimators for nonlinear autoregressive models”. In: Statistica Sinica (2005), pp. 177–195.

[13]

Peter Hall and Brett Presnell. “Intentionally biased bootstrap methods”. In: Journal of the
Royal Statistical Society: Series B (Statistical Methodology) 61.1 (1999), pp. 143–158.

[14]

Peter Hall and Li-Shan Huang. “Nonparametric kernel regression subject to monotonicity
constraints”. In: Annals of Statistics (2001), pp. 624–647.

[15]

Raymond J Carroll, Aurore Delaigle, and Peter Hall. “Testing and estimating shape-constrained
nonparametric density and regression in the presence of measurement error”. In: Journal of
the American Statistical Association 106.493 (2011), pp. 191–202.

[16]

Hassan Doosti and Peter Hall. “Making a non-parametric density estimator more attractive,
and more accurate, by data perturbation”. In: Journal of the Royal Statistical Society: Series
B (Statistical Methodology) 78.2 (2016), pp. 445–462.

[17]

Hassan Doosti, Peter Hall, and Jorge Mateu. “Nonparametric tilted density function estimation: A cross-validation criterion”. In: Journal of Statistical Planning and Inference 197
(2018), pp. 51–68.

[18]

Timothy L McMurry and Dimitris N Politis. “Minimally biased nonparametric regression
and autoregression”. In: REVSTAT–Statistical Journal 6.2 (2008), pp. 123–150.
17

[19]

Wendy L Martinez and Angel R Martinez. Computational statistics handbook with MATLAB.
Vol. 22. 2007.

[20]

Peter Hall and Thomas E Wehrly. “A geometrical method for removing edge effects from
kernel-type nonparametric regression estimators”. In: Journal of the American Statistical
Association 86.415 (1991), pp. 665–672.

[21]

Farzaneh Boroumand et al. “Application of Outlier Robust Nonlinear Mixed Effect Estimation in Examining the Effect of Phenylephrine in Rat Corpus Cavernosum”. In: Iranian
Journal of Pharmaceutical Sciences 12.3 (2016), pp. 47–54.

18

Appendices
A

Proof of Theorem 3

In this section we provide proof of Theorem 3 for tilted Nadaraya-Watson estimator as a form
of tilted linear smoother from (2.3). The result for tilted local linear smoother can be proved
analogously.
Pn
Proof.
Let pi = n1 π(Xi ) where π ≥ 0 and is a smooth function, i=1 pi = 1 which is equivalent
R
π(X)fX (x)dx = 1 for continuous X. For simplicity, we replace fX (x) by f then
n
X

n

pi =

i=1

1X
π(Xi )
n i=1

= Eπ(X) + Op (n−1/2 )
Z
= π(X)f dx + Op (n−1/2 )
= 1 + Op (n−1/2 ),
P
thus for normalising pi s so that i pi = 1 we need to multiply the estimator (2.3) by 1+Op (n−1/2 ).
The factor Op (n−1/2 ) is negligibly small. We choose π such that r̂n (x|h, p) in (2.3) is unbiased
estimator for r, i.e
E r̂(x|h, p) = r.
(A.1)
From (A.1) we have
n

E r̂(x)ĝ(x) =

x − Xi
1X
Epi Yi K(
)
h i=1
h
n

1 X
x − Xi
EE{Yi π(Xi )K(
)|Xi }
nh i=1
h
Z
1 ∞
x−t
)fX (t)dt,
=
r(t)π(t)K(
h −∞
h

=

where

(A.2)

n

1 X
x − Xi
ĝ(x) =
K(
)
nh i=1
h
and g(x) = Eĝ(x). It can be shown that the left-hand side of (A.2) is converging to r(x)g(x).
We have
Z
1 ∞
x − t0
r(x)g(x) =
r(t0 )π(t0 )K(
)fX (t0 )dt0 ,
h −∞
h
multiplying both sides by e−itx and integrating over x, we deduce
Z
Z
1 ∞
x − t0
e−itx rπf (t0 )K(
)dx0 dt0 ,
Φrg (t) =
h −∞
h
19

by changing variable

x−t0
h

= u, we have
Z

∞

e−itx rπf (t0 )Φk (t)dt0

Φrg (t) =
−∞

= Φk (t)Φrπf (t).
Φrg (t)
,
Φk (t)
Z ∞
Φrg (t)
1
e−itx
πrf =
,
2π −∞
Φk (t)
Z ∞
Φrg (t)
1
e−itx
dt,
π(X) =
2πrf −∞
Φk (t)
Φrπf =

(A.3)

if kernel K holds the assumption (c) and q = r · g meet the assumption (d), then
π =1+

n
X

Cj (−h2 )j

j=1

rg (2j)
,
rf

(A.4)

with π from (A.3), then r̂n in unbiased. Next we show that π satisfies 0 < π(X) < 1.
If the assumption (e) relaxed then there exist C6 and h0 ≥ 0, for all h, 0 ≤ h ≤ h0 , π > 0 and
sup π ≤ C6 < ∞ then for unbiased r̂n
Z
Z
x−X
1
E{π 2 (X)K 2 (
)}dx,
var{r̂n (x|h, p)}dx ≤
2
nh
h
Z
1
(sup π)2 K 2 dx,
≤
nh
= O{nh−1 }.
(A.5)
So M SE can be written as
Z
M SE{r̂n (x|h, p)} =

E{r̂n (x|h, p) − r}2 dx,

= O{nh−1 }.
We recall that
(f) Under the assumption (e)-(1), δn → 0 as n → ∞, so that n1/2 δn → ∞ , and under assumption
(e)-(2), n1/2 log(n)−C2 /4C5 δn → ∞, where C2 and C5 are defined in (e)-(2).
Then under the assumption (e)-(1) and (f), we have that n1/2 δn → ∞ thus n−1 = o(δn2 ). Consequently, there exists h(n) ↓ 0 as n → ∞ such that (nh)−1 = O(δn2 ), for some large n, h < h0
since 0 ≤ h ≤ h0 . Next, by replacing O(δn2 ) in the right-hand side of (A.5), we have a new form of
(A.5) which is true for specific choice of π defined at (A.3), and considering θ̃ = (h, p) in the case
of (2.3):
lim lim sup P {kr̂n (.|θ̃) − rk ≥ Cδn } = 0.

C→∞ n→∞

20

(A.6)

P
For this version of pi = n−1 π(Xi ), i pi = 1 does not satisfy. However, this issue can be fixed by
normalisation similar to that done in the first paragraph of the proof.
Property (A.6) implies part I of Theorem 3 and part II can be concluded under uniformity of
(A.6) over C.
Under assumption (e)-(2) and (A.4), |rg (2j) (x)/rf | ≤ C1 (1 + |x|)C2 for 1 ≤ j ≤ k, defining
C6 = max(|C1 |, ..., |Ck |), we have for 0 ≤ h ≤ 1
|π(X) − 1| = |

n
X

Cj (−h2 )j

j=1

rg (2j)
|,
rf

≤ C1 C6 K(1 + |x|)C2 h2 ,
2 2
so, if λ1n → ∞ and λC
1n h → 0, then

sup |π(X) − 1| → 0

(A.7)

it means that whenever |X| ≤ λ1n , 0 < π(X) < 1.
In the first paragraph of the proof, we showed that π(X) ≥ 0. Then we found an upper bound
for π(X) in (A.7) when X ∈ [−λ1n , λ1n ]. Now, we want to show that the probability of X being
out of this interval is almost zero which means for all X, 0 < π(X) < 1.
Assumption (e)-(2) implies that
5
P (|X| ≥ λ1n ) ≤ C3 exp(−C4 λC
1n ).

(A.8)

Using (f), n−1/2 (log n)C2 /2C5 δn → ∞, or equivalently, δn2 = λ2n n−1 (log n)C2 /2C5 → ∞, where
λ2n exists. We choose h so that (nh)−1 = O(δn2 ); or for simplicity, (nh−1 ) = δn2 , then h =
−C2 /2C5
C5 /C2 1/C2
; let λ1n = {λ2−η
}
, where η ∈ (0, 2), so
λ−1
2n (log n)
2n (log n)
(2−η)C5 /C2

5
exp(−C4 λC
1n ) = exp(−C4 λ2n

= O(n

−C

),

for all C > 0. Therefore by (A.8),
P (|X| ≥ λ1n ) = O(n−C ),
for all C > 0.

21

log n),

