COVID-19 Screening Using Residual Attention Network an
Artificial Intelligence Approach
Vishal Sharma

Curtis Dyreson

Department of Computer Science
Utah State University
Logan, Utah
vishal.sharma@usu.edu

Department of Computer Science
Utah State University
Logan, Utah
curtis.dyreson@usu.edu

arXiv:2006.16106v3 [eess.IV] 20 Oct 2020

ABSTRACT
Coronavirus Disease 2019 (COVID-19) is caused by severe acute
respiratory syndrome coronavirus 2 virus (SARS-CoV-2). The virus
transmits rapidly; it has a basic reproductive number (ùëÖ0 ) of 2.2‚àí2.7.
In March 2020, the World Health Organization declared the COVID19 outbreak a pandemic. COVID-19 is currently affecting more than
200 countries with 6M active cases. An effective testing strategy for
COVID-19 is crucial to controlling the outbreak but the demand for
testing surpasses the availability of test kits that use Reverse Transcription Polymerase Chain Reaction (RT-PCR). In this paper, we
present a technique to screen for COVID-19 using artificial intelligence. Our technique takes only seconds to screen for the presence
of the virus in a patient. We collected a dataset of chest X-ray images
and trained several popular deep convolution neural network-based
models (VGG, MobileNet, Xception, DenseNet, InceptionResNet) to
classify the chest X-rays. Unsatisfied with these models, we then
designed and built a Residual Attention Network that was able to
screen COVID-19 with a testing accuracy of 98% and a validation
accuracy of 100%. A feature maps visual of our model show areas in
a chest X-ray which are important for classification. Our work can
help to increase the adaptation of AI-assisted applications in clinical
practice. The code and dataset used in this project are available
at https://github.com/vishalshar/covid-19-screening-using-RAN-on-Xray-images.

KEYWORDS
COVID-19 screening, Residual Attention Network, Deep Learning,
Machine Learning, Chest X-Ray

1

INTRODUCTION

In February 2013, some people in Guangdong province in China
became infected with a severe acute respiratory syndrome virus
(SARS-CoV) [32]. Eventually, SARS was detected in about 8000
patients across 26 countries, the World Health Organization (WHO)
reported 774 deaths due to SARS [27]. In September 2012, a similar
incident happened with the Middle East respiratory syndrome virus
(MERS-CoV). There were 2494 confirmed cases of infection with
858 deaths due to MERS-CoV [28].
Both SARS and MERS pale in significance to the latest CoV outbreak concerning human health. In November 2019 pneumonia-like
cases due to unknown causes started to appear in Wuhan, China
killing hundreds of people in the initial weeks. In early 2020, the
International Committee on the Taxonomy of Viruses (ICTV) declared the virus as Coronavirus Disease 2019 (COVID-19) caused
by the SARS-CoV-2 virus [32]. The reproductive number (ùëÖ0 ) of
COVID-19 is 2.2 ‚àí 2.7 [33] higher than SARS coronavirus due to a S

protein in the RBD region of SARS-CoV-2 [30]. The highly transmissible virus quickly spread globally. By August 10, 2020 COVID-19
had been detected in 213 countries with six and a half million active cases and seven hundred and thirty thousand deaths. Due to a
lack of medical supplies and staff, COVID-19 has overpowered the
medical system of over 200 countries, and was declared a pandemic
by World Health Organization (WHO) on March 11, 2020.
Diagnosing who has COVID-19 can help curb its spread by quarantining those infected. Currently, the most widely used technique
for detecting COVID-19 is with viral nucleic acid detection using
Reverse Transcription Polymerase Chain Reaction (RT-PCR), which
works by detecting viral RNA from sputum or a nasopharyngeal
swab [40]. Unfortunately, there is a shortage of RT-PCR test kits [24].
RT-PCR tests are also relatively slow, a test takes, at best, about
four hours to complete, even in a highly controlled environment.
RT-PCR tests also have a high false positive rate (about 30%) [41].
A common notable symptom of COVID-19 patients is difficulty
breathing [46].
Recent advances in the field of computer vision suggest the possibility of a faster, more widely available alternative for detecting
COVID-19. A CT scan of a patient‚Äôs chest has shown higher accuracy and sensitivity than a RT-PCR test for COVID-19 detection [1].
Another study [17] further validates at least 20% higher detection
sensitivity from CT scans versus RT-PCR tests. Identifying who has
COVID-19 using an imaging technique is relatively non-intrusive,
uses widely-available X-ray or CT scanners, and can be used at a
very early stage to diagnose COVID-19 [43].
Deep neural networks have been successful in processing medical images [44] and image processing in general, for instance in
object detection [35], image segmentation [4], and image classification [19]. Deep learning techniques such as, convolutional neural
networks (CNN) are popular for processing medical images, e.g., for
classification [21] and segmentation [18]. Advances in CNN models
over the past few years have led to robust implementations such
as VGGNet [23], Inception [34], DenseNet [16], Xception [9], and
MobileNet [15]. Recently, Attention Mechanism [37] which generates attention-aware features, based on spatial features has become
popular in the fields of computer vision and image processing.
In this paper, we report on experiments with various deep learning models to detect and classify COVID-19 from chest X-ray images of patients. The models we use are VGG, ResNet, MobileNet,
DenseNet, Xception, Attention, and Residual based CNN (Residual
Attention Network). We first collected X-ray images of COVID-19
patients and people without the disease, which we call the ‚Äúnormal" class. It is important to note that the normal class can have
other illnesses, such as pneumonia. This image dataset is from a

10

Age Category

70-80

9

60-70

5
14

50-60

11

11

40-50

11

12
6

30-40

4
2

20-30 2 2

UMAP feature space two

Italy
Spain
China
Taiwan
Vietnam
USA
Australia
Canada
UK
Iran
Sweden
Israel

80+ 1

NORMAL Patients
COVID Patients

10

Male
Female

10-20 1

8

0

5
(a)

10
15
20
# of COVID-19 Patients

Distribution w.r.t Age

25

34
17
13
12
6
4
2
1
1
1
1
1
0

(b)

10
20
30
# of COVID-19 Patients

40

Distribution w.r.t Location

Figure 2: Distribution of our dataset with positive COVID-19 based on

7

location, gender and age

6

configurations and reports results, and finally Section 6 presents
conclusions and future work.

5

2

There are over 24,000 research papers on COVID-19 from well
known sources like ùëèùëñùëúùëÖùë•ùëñùë£, ùëéùëüùëãùëñùë£ and ùëöùëíùëëùëÖùë•ùëñùë£. More than 1,500
of these papers are peer reviewed [29]. There are two recent review
papers about using AI techniques in COVID-19 detection [31, 42].
We focus on research that uses deep learning for COVID-19 detection. Wang et al. [39] used 1,065 chest CT scan images of COVID-19
patients to build a classifier using InceptionNet. They report an accuracy of 89.5%, a specificity of 0.88, and a sensitivity of 0.87. Xu et
al. [7] used 3D Convolution Neural Networks (CNNs) and reported
an accuracy of 86.7%. Chen et al. [8] segment the infected areas in
CT scans using UNet++ [47]. Using transfer learning and predefined
models to classify COVID-19 in CT scans has also been researched,
for instance using DenseNet e.g., [20, 22], ResNet e.g., [13, 26], and
CNN e.g., [3, 13, 20]. Traditional machine learning (ML) methods
of feature extraction and conventional ML algorithms for classification have also been used. Mucahid et al. [6] used feature extraction
techniques GLCM (grey level co-occurrence matrices), LDP (local
directional pattern), GLRLM (grey-level run length matrix), and
DWT (discrete wavelet transform), and using extracted features in
a Support Vector Machine (SVM) for classification. They report an
accuracy of 99.68% in the best configuration settings. Alqudah et
al. [2] applied various ML techniques, such as SVM and Random
Forest, and reported an accuracy of 95%.
To the best of our knowledge, we are the first to design and build
a Residual Attention Mechanism to extract spatial-aware features
and perform the classification of a COVID-19 dataset. We obtained
higher test and validation accuracy, precision, recall, sensitivity, and
specificity than previous work. Additionally, most of the previous
research used more sophisticated CT scan images which usually
take 20 to 30 minutes to perform, but we use X-ray images that are
faster to extract, about 10 minutes in most cases. X-ray machines
are more widely available than CT scanners, and there are portable
X-ray units that can be deployed anywhere, not just in medical
facilities.

4

1

2

3

4

5

6

UMAP feature space one

7

RELATED WORK

8

Figure 1: UMAP features of chest X-ray images of patients in our dataset
diverse population in terms of location, age, and gender. To better
understand the dataset, we analyzed it with a popular non-linear
dimensionality reduction technique, Uniform Manifold Approximation and Projection (UMAP). The reduction produces a clear
distinction between those with COVID-19 and those without (as
shown in Figure 1). Next, we split the dataset into training, testing,
and validation sets using stratified folds. We configured several
deep learning models for optimal results and trained the models
on the dataset. To improve the modeling, we designed and built
our own Residual Attention Network with better feature extraction
using custom designs of residual and attention block. Our model
outperforms other models with 98% accuracy on the test set and
100% accuracy on the validation set. We extracted feature maps
from our model and observed the Residual Attention Network detecting potential COVID-19 infected areas. The major challenge
in this research is developing an effective model using only a very
small dataset. We address this challenge by designing our Residual
and Attention block to avoid overfitting. This paper makes the
following contributions:
‚Ä¢ a novel dataset of curated images for use in COVID-19 research,
‚Ä¢ a problem-specific, highly accurate classification model using Residual Connection and Attention Mechanism,
‚Ä¢ an explainable diagnosis using feature maps, and
‚Ä¢ a reproducible dataset and classifier; all of our code and data
are in the public domain.1

3

DATASET

This paper is organized as follows. Section 2 outlines related
work and potential limitations. Section 3 describes our dataset
collection and analysis. Section 4 describes our approach and design
of our Residual Attention Network. Section 5 shows experimental

We collected images only from public sources, which provide the
data while maintaining patients‚Äô privacy. The dataset of COVID19 X-ray images comes from radiopaedia.org2 , the website of the

1 github.com/vishalshar/covid-19-screening-using-RAN-on-X-ray-images

2 radiopaedia.org

2

Italian Society of Medical and Intervention Radiology3 . Cohen et
al. [11] scraped the images from the website using PDF processing
tools. We selected 120 images of patients with COVID-19, specifically, we selected all Posterioranterior (PA) images. The PA view
has an anterior aspect in which the ribs are much clearer than the
Anteriorposterior or Lateral view X-ray. To collect images of nonCOVID-19 (which we call normal) chests we randomly selected
images from a repository collected by Mooney et al. [25]. In this
repository, there are chest X-ray images of patients with pneumonia as well as of healthy patients. We extracted 119 PA view X-ray
images of normal patients. In total, our dataset has 239 images with
120 from COVID-19 patients and 119 from normal patients.

Input
Residual
Relu Activation
Batch Norm

Weight Layer 1

Conv2D
Relu Activation
Batch Norm

Weight Layer 2

Conv2D
Relu Activation
Batch Norm
Conv2D

3.1

Dataset Statistics

Output

A wide distribution in a dataset is an important factor for training
a deep learning model, since training on narrowly distributed data
may lead to a biased model due to a failure to generalize the classification features. Figure 2 depicts our dataset distribution, with
respect to location, gender, and age of patients. The figure shows
that our image collection comes from 62 male and 39 female patients with a normal age distribution that is shifted towards elderly
patients. These patients are from twelve countries, however, patient
ethnicity was not made public.
Age: The dataset has patients from age 12 to 84, with an average
age of 57.33 years. Figure 2a shows the amount of samples in the
age category as well as the gender count in each category. Our
dataset has a wide range of patients in terms of their ages.
Location: Figure 2b shows the location of patients. The location
is an important attribute, since a model trained on data from only
one country may become biased. Greater variation in the training
data can help generalize a deep learning model. Our dataset has
images of patients from twelve countries.

3.2

Figure 3: Image on the left shows a Residual block and on the right shows
full pre-activation used as our Residual block

4.1

Deep convolution networks have revolutionized the field of image
classification. Advancements in algorithms and hardware networks
have increased the ability to add layers to a deep convolution network. With the increase in the depth of the network, it becomes
harder to train a neural network because of vanishing gradients.
Networks with too many layers become highly unstable as the
value of gradients approaches zero in early layers. Every additional
layer gradient value becomes smaller and eventually insignificant.
Vanishing gradients degrade the performance of the network and
adding more layers only exacerbates the problem. To solve the vanishing gradient problem, Kaiming He et al. [14] proposed residual
connections. A residual connection merges the output of a layer
with the input of a previous layer, which ensures that gradient
values do not suddenly vanish. As shown in Figure 3, on the left is
a residual block with a residual connection.
A deep learning model in general tries to learn a mapping function ùêª (ùë•) from an input ùë• to output ùë¶,

UMAP Exploration

We applied the Uniform Manifold Approximation and Projection
(UMAP) technique, which is a non-linear dimensionality reduction
technique, to the images. The feature space of UMAP is found by
searching for a low-dimensional projection of data which is the
closest equivalent to the real data using a fuzzy topological structure.
We prepared the dataset for UMAP by performing a standard image
pre-processing, as described further in Section 5.1. Figure 1 shows
the result of the UMAP. In the figure, the X-ray of a normal patient
has a green bounding box while that of a COVID-19 patient has a
red bounding box. The figure shows two clusters, one dominated
by COVID-19 images, the other by normal images.

4

Residual Block

ùêª (ùë•) = ùë¶

(1)

In a residual block, instead of learning a direct mapping, it uses the
difference between the mapping of ùë• and the original input ùë•,
ùêπ (ùë•) = ùêª (ùë•) ‚àí ùë•

(2)

re-arranging gives,
ùêª (ùë•) = ùêπ (ùë•) + ùë•
(3)
residual block learns the residual ùêπ (ùë•) with given ùë• as an input and
ùêª (ùë•) as the true output. This technique helps when increasing the
depth of a neural network.
Our experiments with arranging residual block for optimal gradient flow showed full pre-activation with batch normalization
gives the best results, which was also suggested by Kaiming He et
al. [14]. Our full pre-activation block is shown in the right of Figure 3, where we use Relu activation, batch normalization and a 2D
convolution layer stacked three times. The sequence and stacking

APPROACH

In this section, we explain our custom Residual block, Attention
block, and how we used them to design and build a Residual Attention Network, which extends the original Residual Attention
Network [38].

3 https://www.sirm.org/category/senza-categoria/covid-19/

3

4.3

Residual
Sigmoid

Upsample
Dot product

Addition

Downsample

Encoder

Conv Layer

Decoder

Figure 4: Design of Attention Block
of our block are different than originally proposed [38], which was
batch normalization, Relu activation and convolution layer stacked
twice. From our experiments, we observed that batch normalization
after Relu activation performs better. The reason for better performance happened when input features for a layer are negative, in the
network they would have been truncated using non-linearity activation function, Relu, before batch normalization. Performing batch
normalization prior to Relu activation will include these negative
values in feature space.

4.2

Attention Block

The attention mechanism has become a very popular technique
in natural language processing, image processing, and computer
vision. The attention mechanism can generate attention-aware
features and features that can be extracted based on spatial, context, or channel aware-features. It also learns the importance and
correlations among features. Using visual attention in an image
classification task helps determine important image regions and
their correlations. The presence or absence of image regions is
critical to classification. In our case, these regions are evidence of
COVID-19 infection in chest X-ray images. Our attention module
consists of two branches: a ‚Äútrunk‚Äù branch ùëá (ùë•) with two stacked
residual blocks and an encoder-decoder ‚Äúmask‚Äù branch ùëÄ (ùë•).
The encoder in the mask branch consists of downsampling using
max-pooling followed by residual connection and downsampling
again. The encoder acts as an input reducer. The decoder consists
of upsampling using bilinear interpolation. In the original Residual
Attention Network [38] there is only one upsample but, in a literature survey we found that the performance of a Residual Attention
Network can be increased by increasing the number of up-sampling
layers. We extended the Attention block by adding two upsample
layers in our model. The encoder and decoder are followed by two
convolution layers and a sigmoid activation as displayed in Figure 4. The trunk branch consists of two stacked residual blocks that
perform feature processing.
The final output of the module is
ùê∫ (ùë•) = (1 + ùëÄ (ùë•)) ‚àó ùëá (ùë•)

Residual Attention Network

Our Residual Attention Network is built by multiple stacks of our
basic unit module. The stacking of blocks is designed for optimal
performance and to prevent overfitting. The Attention block is
designed to explore fine-grained feature maps since COVID-19
infections could be a fine detail in an X-ray. There are two major
attention categories: Soft and Hard, we use Soft attention to learn
alignment for several patches.
The Residual block captures high-level features and provides
input to Attention block. The Attention module generates specialized low-level features on Residual input. It divides an image into
a few high-level features and from those features extracts several
low-level features. We stack Residual and Attention layers alternatively three times. The Residual layer extracts high-level features
from the input image which are then passed to the Attention block,
which extracts low-level features. These low-level features become
an input to the next Residual block. It works as both a top-down
and bottom-up approach, the top-down network produces dense
features and the bottom-up one produces low-resolution feature
maps. Our architecture is shown in Table 1. This technique has
proved successful in image segmentation. Our scenario is very similar to segmentation where we try to identify low-level patches of
COVID-19 infections in a chest X-ray.

(4)

Layers
Convolution 2D
MaxPool 2D

Output Size
112√ó112
56√ó56

Residual Block

56√ó56

Attention Block

56√ó56

Residual Block

28√ó28

Attention Block

28√ó28

Residual Block

14√ó14

Attention Block

14√ó14

Residual Block

7√ó7

Residual Block

7√ó7

Residual Block

7√ó7

AvgPooling 2D
FC, Softmax
Depth

1√ó1

Kernel Size
(5√ó5), ùëù=same
(2√ó2), 2
1 √ó 1 32
¬©
¬™
¬≠3 √ó 3 32 ¬Æ
¬´1 √ó 1 128¬¨
Attention√ó1
1 √ó 1 128
¬©
¬™
¬≠3 √ó 3 128¬Æ
¬´1 √ó 1 256¬¨
Attention√ó1
1 √ó 1 256
¬©
¬™
¬≠3 √ó 3 256¬Æ
¬´1 √ó 1 512¬¨
Attention√ó1
1 √ó 1 512
¬©
¬™
¬≠3 √ó 3 512 ¬Æ
¬´1 √ó 1 1024¬¨
1 √ó 1 1024
¬©
¬™
¬≠3 √ó 3 1024¬Æ
¬´1 √ó 1 1024¬¨
1 √ó 1 1024
¬©
¬™
¬≠3 √ó 3 1024¬Æ
¬´1 √ó 1 1024¬¨
(7√ó7)
2
115

Table 1: Residual Attention Network architecture
adding 1 to the equation ensures that in case of mask branch with
zero output the trunk branch computation passes through, which
dampens the susceptibility to noisy data.

4

5

EXPERIMENTS

DenseNet: DenseNet extends the idea of residual from ResNet,
but instead of learning residual (the difference between previous
and current layer) it proposes to merge the output of the previous
and current layer. We use DenseNet121 and DenseNet201 in our
experiments.
NASNet: NASNet is Neural Architecture Search Network. They
are a family of models designed to learn model architecture automatically on the dataset of interest. In our case imagenet was used
as dataset to design NASNet.
Vanilla Residual Attention Network (RAN): To compare our
work with existing state-of-the-art Residual Attention Network, we
also implemented Attention-56 which was proposed in [38].

This section presents an experimental evaluation of our model. We
start with data preprocessing and stratified data split for training,
testing, and validation. We describe the experimental setup, our
models, and their configurations, and we show the results.

5.1

Data Preprocessing

Data preprocessing plays an important role in training a deep learning model. Previous research has highlighted the impact of preprocessing on model performance. Images in our dataset have different
sizes, so we first start with standardizing the image size to 224 √ó 224
pixels. The collected images also have different color patterns, so
we normalize the color pattern to RGB. Lastly, we normalize the
maximum intensity of a pixel to 255 (lowest is 0). Our dataset has
two different classes, labeled COVID, and Normal; during training,
we use a label binarizer to convert them to one-hot encoding.

5.2.2 Evaluation Metric. To evaluate the performance of our
models, we use the most commonly used performance metrics for
deep learning, namely, sensitivity, specificity, precision, recall, and
accuracy. Their value range is [0, 1]: higher is better. The metrics
are given below, where TP is true positive, FP is false positive, TN
is true negative, and FN is false negative.

5.1.1 Data Split. The dataset is split into training (70%), testing (20%), and validation (10%) sets. We use stratified splits which
ensures that each split has an equal number of samples from each
class as shown in Table 2. We also add a random rotation of 15‚ó¶ to
images, which adds more stability to our model during training.

Sensitivity (Sens) = TPTP
+ FN
Specificity (Spec) = FP TP
+ TN
Precision (Prec) = TPTP
+ FP

Training
Testing
Validation

COVID
84
25
11

Normal
83
25
11

Total
167
50
22

120

119

239

Recall (Rec) = TPTP
+ FN
TP+TN
Accuracy (Acc)= TP+FP+TN+FN

5.2.3 Models configurations. Our selection of model size ranges
from 15 layers to 270 layers deep. We trained each model with an
initial learning rate of 1e‚àí4 and a mini-batch size of 8 with 100
epochs. We used Adam as our optimizer. Adam optimizer uses two
popular optimization techniques, Root Mean Square Propagation
(RMSprop) and Stochastic Gradient Descent (SGD), with momentum. To provide stability during the training of our models, we
used a learning rate decay with the decay rate shown in Equation 5.
During the training, we used Binary Cross entropy as the loss
function for all of the models. For all benchmark models, we use
them with pre-trained weights using imagenet, which contains 14
million images with over 1000 classes. Training on a large dataset
requires massive computational power, for example, NASNetLarge
was trained using 500 GPUS for four days on the imagenet dataset.
We used the weights of models after the training and retrained
them on our dataset. This methodology is commonly known as
transfer learning.
Initial Learning Rate
Decay Rate =
(5)
Epochs

Table 2: COVID dataset splits using stratified sampling

5.2

Experimental setup

All experiments were carried out on a computer with Intel i7 5820k,
Nvidia 1080ti, 16GB of RAM running Ubuntu 18.04 OS. We use
Python 3.7 and its libraries (sklearn, tensorflow, keras) to write and
train the deep learning models. All of the networks were trained
on Nvidia 1080ti (3584 CUDA cores and 11GB DDR5 RAM) with
CUDA and cuDNN configured for performance enhancement.
5.2.1 Baseline. We use the following models as baseline for our
Residual Attention Network:
VGGNet: VGGNets were introduced in 2014 with the intention
to perform state of the art image classification with the least depth of
CNN possible. We use both VGG16 and VGG19 in our experiments.
ResNet: ResNet introduced the concept of residual connection
to solve the vanishing gradient problem. Deciding a kernel size for
ResNet is hard and that‚Äôs why we use InceptionResNet a variant of
ResNet, which uses multiple size kernels within the same layer.
Xception: Xception introduced the concept of depthwise separable convolution to reduce the number of the parameter without
loss of performance.
MobileNet: In addition to depthwise convolution, MobileNet
introduced pointwise convolution to reduce the number of parameters in order of 100 to 1000. We specifically use MobileNet and its
variant MobileNetV2.

5.2.4 Output head modification. The predefined models have
been designed and trained for the very large imagenet dataset. This
dataset consists of of 1000 classes and millions of images whereas
our dataset has two classes and 239 images. To prevent overfitting
of the predefined models on our small dataset, we modified the
output layers of all of the models. We removed the output layer
and added a custom output head. For example, VGG16 has three
fully connected layers as the output head, the first two layers have
4096 neurons while the third has 1000 (number of classes) neurons.
We modified this output from three layers to two layers with 64

5

1.0

1.0

0.8

0.6

Residual Attention
VGG_19
MobileNet
VGG_16
NASNetLarge
DenseNet201
0

10

20

30

40

50

Accuracy

Accuracy

0.8

Xception
DenseNet121
InceptionResNetV2
0.4
MobileNetV2
NASNetMobile
Vanilla RAN

Epochs

70

60

80

90

100

0.2

0.6

Residual Attention
VGG_19
Xception
VGG_16
NASNetLarge
DenseNet201
0

Accuracy over epochs of models on training set

10

20

30

40

50

Epochs

MobileNet
DenseNet121
0.4
InceptionResNetV2
MobileNetV2
NASNetMobile
0.2
Vanilla RAN
60

70

80

90

100

Accuracy over epochs of models on testing set

Figure 5: Training and testing accuracy for all models. The shaded region represents 95% confidence interval.
Testing Set
Network
NASNetMobile
MobileNetV2
InceptionResNetV2
DenseNet201
DenseNet121
Xception
NASNetLarge
MobileNet
Vanilla Residual Att Net
VGG16
VGG19
Residual Att Net (Our)

Validation Set

Layers

Sens

Spec

Prec

Rec

Acc

Sens

Spec

Prec

Rec

Acc

AUC

198
54
246
202
122
42
270
29
145
15
18
115

0.72
0.72
0.40
0.52
0.92
0.84
0.72
0.56
0.88
1.00
0.96
1.00

0.68
1.00
1.00
1.00
0.72
1.00
0.92
1.00
0.92
0.88
0.92
0.96

0.69
1.00
1.00
1.00
0.77
1.00
0.90
1.00
0.92
0.89
0.92
0.96

0.70
0.78
0.63
0.68
0.90
0.86
0.77
0.69
0.88
1.00
0.96
1.00

0.7000
0.8600
0.7000
0.7600
0.8200
0.9200
0.8200
0.7800
0.9000
0.9400
0.9400
0.9800

0.82
0.55
0.55
0.82
0.82
0.91
0.82
0.82
0.91
1.00
0.92
1.00

0.55
0.91
1.00
0.91
0.82
0.82
0.91
1.00
0.91
0.82
1.00
1.00

0.64
0.86
1.00
0.90
0.82
0.83
0.90
1.00
0.91
0.85
1.00
1.00

0.75
0.67
0.69
0.83
0.82
0.90
0.83
0.85
0.91
1.00
0.92
1.00

0.6818
0.7273
0.7727
0.8182
0.8636
0.8636
0.8636
0.9091
0.9091
0.9091
0.9545
1.0000

0.86
0.72
0.72
0.82
0.90
0.88
0.90
0.94
0.94
0.96
0.98
1.00

Table 3: Sensitivity, Specificity and Accuracy from all models on Testing and Validation set (sorted based on validation accuracy)

neurons in the first layer and 2 (number of classes) neurons in the
second layer. We did this on all of the models to standardize the
output head.

5.3

since the top two performing networks are shallow networks (1/4
of the size of the largest network). Figure 5 shows the model training and testing accuracy for every epoch. We observe large/deeper
models (DenseNet201, NASNetLarge, InceptionResNetV2) tend to
overfit and shallower models (VGG16, VGG19, Residual Attention
Network) are able to generalize our dataset better. We also observe
our residual attention model during testing took 15 epochs to start
improving accuracy. This can be explained by our layers weight
initializer using Xavier where it initializes the weights of the layer
with zero mean and unit variance.
Overall, our experiments show that the Residual Attention Network performs best: with 98% accuracy on testing and 100% on
validation to screen COVID-19 in X-ray images.
COVID-19 prediction explanation: Figure 6 visualizes the feature
maps of the first two layers of our Residual Attention Network
model trained on our COVID-19 dataset. There are 64 images in
each picture with an 8 √ó 8 grid for each image. The feature maps
were generated by passing an image to our trained model and
collecting data of activated neurons. We select only the first two
layers and 64 neurons of each layer, since the number of neurons

Results

Table 3 displays the accuracy of the models on the testing and validation sets of our COVID-19 dataset. We observe that our custom
Residual Attention Network outperforms all other deep learning
models, while NASNetMobile turns out to be the worst performer.
Our experiments show that VGG19 performs better than VGG16.
Even though both models are similar, VGG19 has more layers and
it is commonly thought that deeper pre-trained models perform
better than shallow models. But that is not the case for our dataset,
for instance, the network with a shallower depth, DenseNet121,
performed better than a very deep network, DenseNet201 (both
networks are at least six times deeper than VGG19). From this, we
observe that it boils down to feature maps of layers from a model,
which is dependent on the convolution layers. We also observe that
very deep networks do not perform better than shallower ones,

6

(a) Feature Map at Convolution Layer 1

(b) Feature Map at pooling Layer

Figure 6: Feature maps of Residual Attention Network at first 2 Layers
and parameters grows exponentially at later layers. We observe
from the feature maps of both layers that our model can extract
relevant details from an image, e.g., lungs, spine, veins, potential
COVID-19 affected areas.

6

about patient symptoms or time with the disease, only that the
X-rays are of COVID-19 positive patients. Since early detection is
important to effective quarantining, it would be interesting to test
our model on X-rays from patients who have just been infected or
who are asymptomatic.

CONCLUSION

ACKNOWLEDGMENTS

This paper proposes a novel method to detect COVID-19 using
chest X-ray images. The method uses a Residual Attention Network
and data augmentation. We collected and curated a dataset of 239
images: 120 images of patients infected with COVID-19, and 119
images of non-COVID-19 patients, which we label ‚Äúnormal.‚Äù The
dataset is diverse in terms of patient age, gender, and location. We
applied a non-linear dimensionality reduction technique, UMAP,
and observed a clear distinction between X-rays of COVID-19 and
normal patients. We designed and implemented a Residual Attention Network to classify COVID-19 patients and compared our
model with many popular deep learning models: VGG, DenseNet,
NASNet, Xception, and Inception. Our experiments show that our
custom Residual Attention Network performs best among all of the
models with 98% testing and 100% validation accuracy. We generated feature maps of the Residual Attention Network and they show
that the low-level features extracted from a given image include
areas of potential COVID-19 infection.
This research shows that chest X-ray images can potentially be
used to detect COVID-19, or can be combined with other testing
methods to corroborate a diagnostic outcome. The immediate future
work is to add more images to our dataset. The second avenue of
future work is to work with domain experts to study the utility of
deploying the technique in practice. Due to the recent nature of
the COVID-19 pandemic, the deployment has lagged behind the
research. Another aspect of future work is to study whether chest
X-rays can be used to detect COVID-19 early in the illness or in
asymptomatic cases. Our X-ray dataset does not have information

This work was supported in part by the National Science Foundation under Award No. 1759965, Collaborative Research: ABI Development: Symbiota2: Enabling greater collaboration and flexibility for
mobilizing biodiversity data. Opinions, findings and conclusions or
recommendations expressed in this material are those of the author(s) and do not necessarily reflect those of the National Science
Foundation.

REFERENCES
[1] Tao Ai, Zhenlu Yang, Hongyan Hou, Chenao Zhan, Chong Chen, Wenzhi Lv, Qian
Tao, Ziyong Sun, and Liming Xia. 2020. Correlation of Chest CT and RT-PCR
Testing in Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014
Cases. Radiology (2020), 200642. https://doi.org/10.1148/radiol.2020200642 PMID:
32101510.
[2] Ali Mohammad Alqudah, Shoroq Qazan, Hiam Alquran, Isam Abu Qasmieh, and
Amin Alqudah. 2020. Covid-2019 Detection Using XRay Images And Artificial
Intelligence Hybrid Systems. (2020).
[3] Khalid El Asnaoui, Youness Chawki, and Ali Idri. 2020. Automated methods
for detection and classification pneumonia based on x-ray images using deep
learning. arXiv preprint arXiv:2003.14363 (2020).
[4] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. 2017. Segnet: A deep
convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence 39, 12 (2017), 2481‚Äì2495.
[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural
Machine Translation by Jointly Learning to Align and Translate. (2014).
arXiv:cs.CL/1409.0473
[6] Mucahid Barstugan, Umut Ozkaya, and Saban Ozturk. 2020. Coronavirus (covid19) classification using ct images by machine learning methods. arXiv preprint
arXiv:2003.09424 (2020).
[7] Charmaine Butt, Jagpal Gill, David Chun, and Benson A Babu. 2020. Deep learning
system to screen coronavirus disease 2019 pneumonia. Applied Intelligence (2020),
1.

7

[8] Jun Chen, Lianlian Wu, Jun Zhang, Liang Zhang, Dexin Gong, Yilin Zhao, Shan
Hu, Yonggui Wang, Xiao Hu, Biqing Zheng, et al. 2020. Deep learning-based model
for detecting 2019 novel coronavirus pneumonia on high-resolution computed
tomography: a prospective study. medRxiv (2020).
[9] Fran√ßois Chollet. 2017. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE conference on computer vision and pattern
recognition. 1251‚Äì1258.
[10] Joseph Paul Cohen, Paul Morrison, and Lan Dao. 2020. COVID-19 image data collection. arXiv 2003.11597 (2020). https://github.com/ieee8023/
covid-chestxray-dataset
[11] Joseph Paul Cohen, Paul Morrison, and Lan Dao. 2020. COVID-19 image data collection. arXiv 2003.11597 (2020). https://github.com/ieee8023/
covid-chestxray-dataset
[12] George E Dahl, Dong Yu, Li Deng, and Alex Acero. 2011. Context-dependent
pre-trained deep neural networks for large-vocabulary speech recognition. IEEE
Transactions on audio, speech, and language processing 20, 1 (2011), 30‚Äì42.
[13] Ophir Gozes, Maayan Frid-Adar, Nimrod Sagie, Huangqi Zhang, Wenbin Ji, and
Hayit Greenspan. 2020. Coronavirus detection and analysis on chest ct with deep
learning. arXiv preprint arXiv:2004.02640 (2020).
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual
Learning for Image Recognition. CoRR abs/1512.03385 (2015). arXiv:1512.03385
http://arxiv.org/abs/1512.03385
[15] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 (2017).
[16] Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. 2016. Densely Connected
Convolutional Networks. CoRR abs/1608.06993 (2016). arXiv:1608.06993 http:
//arxiv.org/abs/1608.06993
[17] Zixing Huang, Shuang Zhao, Zhenlin Li, Weixia Chen, Lihong Zhao, Lipeng
Deng, and Bin Song. 2020. The Battle Against Coronavirus Disease 2019 (COVID19): Emergency Management and Infection Control in a Radiology Department.
Journal of the American College of Radiology (2020). https://doi.org/10.1016/j.jacr.
2020.03.011
[18] Baris Kayalibay, Grady Jensen, and Patrick van der Smagt. 2017. CNN-based
segmentation of medical imaging data. arXiv preprint arXiv:1701.03056 (2017).
[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information
processing systems. 1097‚Äì1105.
[20] Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi
Lu, Zhenghan Fang, Qi Song, et al. 2020. Artificial intelligence distinguishes
COVID-19 from community acquired pneumonia on chest CT. Radiology (2020),
200905.
[21] Qing Li, Weidong Cai, Xiaogang Wang, Yun Zhou, David Dagan Feng, and Mei
Chen. 2014. Medical image classification with convolutional neural network.
(2014), 844‚Äì848.
[22] Xin Li and Dongxiao Zhu. 2020. Covid-xpert: An ai powered population screening
of covid-19 cases using chest radiography images. arXiv preprint arXiv:2004.03042
(2020).
[23] Shuying Liu and Weihong Deng. 2015. Very deep convolutional neural network
based image classification using small training sample size. In 2015 3rd IAPR
Asian conference on pattern recognition (ACPR). IEEE, 730‚Äì734.
[24] Kelly Geraldine Malone. March 25, 2020. Testing backlog linked to shortage of
chemicals needed for covid-19 test. https://bit.ly/2SEXzVY
[25] Paul Mooney. June 2019. Chest X-Ray Images (Pneumonia). https://www.kaggle.
com/paultimothymooney/chest-xray-pneumonia
[26] Ali Narin, Ceren Kaya, and Ziynet Pamuk. 2020. Automatic detection of coronavirus disease (covid-19) using x-ray images and deep convolutional neural
networks. arXiv preprint arXiv:2003.10849 (2020).
[27] World Health Organization. 2004. Summary of probable SARS cases with onset
of illness from 1 November 2002 to 31 July 2003. https://www.who.int/csr/sars/
country/table2004_04_21/en/
[28] World Health Organization. 2013. Middle East respiratory syndrome coronavirus
(MERSCoV). https://www.who.int/emergencies/mers-cov/en/
[29] MIT Technology Review. 2020. Over 24,000 coronavirus research papers are now
available in one place. (2020). https://www.technologyreview.com/2020/03/16/
905290/coronavirus-24000-research-papers-available-open-data/
[30] Muhammad Shereen, Suliman Khan, Abeer Kazmi, Nadia Bashir, and Rabeea
Siddique. 2020. COVID-19 infection: Origin, transmission, and characteristics
of human coronaviruses. Journal of Advanced Research 24 (03 2020). https:
//doi.org/10.1016/j.jare.2020.03.005
[31] Feng Shi, Jun Wang, Jun Shi, Ziyan Wu, Qian Wang, Zhenyu Tang, Kelei He,
Yinghuan Shi, and Dinggang Shen. 2020. Review of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for covid-19.
IEEE Reviews in Biomedical Engineering (2020).
[32] Tanu Singhal. 2020. A Review of Coronavirus Disease-2019 (COVID-19). The Indian Journal of Pediatrics 87 (03 2020). https://doi.org/10.1007/s12098-020-03263-6

[33] Sanche Steven, Yen Ting Lin, Chonggang Xu, Ethan Romero-Severson, Nick
Hengartner, and Ruian Ke. 2020. High Contagiousness and Rapid Spread of
Severe Acute Respiratory Syndrome Coronavirus 2. In Center for Disease Control
and Prevention (CDC). https://doi.org/10.3201/eid2607.200282
[34] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed,
Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2014. Going Deeper with Convolutions. CoRR abs/1409.4842 (2014).
arXiv:1409.4842 http://arxiv.org/abs/1409.4842
[35] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. 2013. Deep neural networks for object detection. In Advances in neural information processing systems.
2553‚Äì2561.
[36] John Hopkins University and Medicine. 2020. Coronavirus resource center. https:
//coronavirus.jhu.edu/us-map
[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998‚Äì6008.
[38] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang,
Xiaogang Wang, and Xiaoou Tang. 2017. Residual attention network for image
classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 3156‚Äì3164.
[39] Shuai Wang, Bo Kang, Jinlu Ma, Xianjun Zeng, Mingming Xiao, Jia Guo, Mengjiao
Cai, Jingyi Yang, Yaodong Li, Xiangfei Meng, et al. 2020. A deep learning algorithm
using CT images to screen for Corona Virus Disease (COVID-19). MedRxiv (2020).
[40] Shuai Wang, Bo Kang, Jinlu Ma, Xianjun Zeng, Mingming Xiao, Jia Guo, Mengjiao
Cai, Jingyi Yang, Yaodong Li, Xiangfei Meng, and Bo Xu. 2020. A deep learning
algorithm using CT images to screen for Corona Virus Disease (COVID-19).
medRxiv (2020). https://doi.org/10.1101/2020.02.14.20023028
[41] Yu-Huan Wu, Shang-Hua Gao, Jie Mei, Jun Xu, Deng-Ping Fan, Chao-Wei Zhao,
and Ming-Ming Cheng. 2020. JCS: An Explainable COVID-19 Diagnosis System
by Joint Classification and Segmentation. (2020). arXiv:eess.IV/2004.07054
[42] Laure Wynants, Ben Van Calster, Marc MJ Bonten, Gary S Collins, Thomas PA Debray, Maarten De Vos, Maria C Haller, Georg Heinze, Karel GM Moons, Richard D
Riley, et al. 2020. Prediction models for diagnosis and prognosis of covid-19
infection: systematic review and critical appraisal. bmj 369 (2020).
[43] Xiaowei Xu, Xiangao Jiang, Chunlian Ma, Peng Du, Xukun Li, Shuangzhi Lv, Liang
Yu, Yanfei Chen, Junwei Su, Guanjing Lang, Yongtao Li, Hong Zhao, Kaijin Xu,
Lingxiang Ruan, and Wei Wu. 2020. Deep Learning System to Screen Coronavirus
Disease 2019 Pneumonia. (2020). arXiv:physics.med-ph/2002.09334
[44] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks?. In Advances in neural information
processing systems. 3320‚Äì3328.
[45] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. 2018. SelfAttention Generative Adversarial Networks. (2018). arXiv:stat.ML/1805.08318
[46] Jianpeng Zhang, Yutong Xie, Yi Li, Chunhua Shen, and Yong Xia. 2020. Covid-19
screening on chest x-ray images using deep learning based anomaly detection.
arXiv preprint arXiv:2003.12338 (2020).
[47] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming
Liang. 2018. UNet++: A Nested U-Net Architecture for Medical Image Segmentation. CoRR abs/1807.10165 (2018). arXiv:1807.10165 http://arxiv.org/abs/1807.
10165

8

