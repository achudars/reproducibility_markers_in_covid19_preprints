arXiv:2010.06906v1 [cs.CL] 14 Oct 2020

No Rumours Please! A Multi-Indic-Lingual
Approach for COVID Fake-Tweet Detection
Debanjana Kar*

Mohit Bhardwaj*

Suranjana Samanta

Amar Prakash Azad

Dept. of CSE
IIT Kharagpur, India
debanjana.kar@iitkgp.ac.in

Dept. of CSE
IIIT Delhi, India
mohit19014@iiitd.ac.in

IBM Research
Bangalore, India
suransam@in.ibm.com

IBM Research
Bangalore, India
amarazad@in.ibm.com

Abstract—The sudden widespread menace created by the
present global pandemic COVID-19 has had an unprecedented
effect on our lives. Man-kind is going through humongous fear
and dependence on social media like never before. Fear inevitably
leads to panic, speculations, and spread of misinformation. Many
governments have taken measures to curb the spread of such
misinformation for public well being. Besides global measures,
to have effective outreach, systems for demographically local
languages have an important role to play in this effort. Towards
this, we propose an approach to detect fake news about COVID19 early on from social media, such as tweets, for multiple
Indic-Languages besides English. In addition, we also create an
annotated dataset of Hindi and Bengali tweet for fake news
detection. We propose a BERT based model augmented with
additional relevant features extracted from Twitter to identify
fake tweets. To expand our approach to multiple Indic languages,
we resort to mBERT based model which is fine tuned over created
dataset in Hindi and Bengali. We also propose a zero shot learning
approach to alleviate the data scarcity issue for such low resource
languages. Through rigorous experiments, we show that our
approach reaches around 89% F-Score in fake tweet detection
which supercedes the state-of-the-art (SOTA) results. Moreover,
we establish the first benchmark for two Indic-Languages, Hindi
and Bengali. Using our annotated data, our model achieves about
79% F-Score in Hindi and 81% F-Score for Bengali Tweets. Our
zero shot model achieves about 81% F-Score in Hindi and 78%
F-Score for Bengali Tweets without any annotated data, which
clearly indicates the efficacy of our approach.
Index Terms—Fake tweet, multilingual BERT, Random Forest
Classifier, social impact, COVID19

I. I NTRODUCTION
With the insurgence of the most devastating pandemic of
the century, COVID-19, the entire planet is going through
an unprecedented set of challenges and fear. Every now
and then new revelations surface either for COVID solutions
e.g. medicines, vaccines, mask usage, or regarding COVID
dangers. Along with factual information, it has been observed
that large amounts of misinformation are circulating on social
media platforms such as Twitter. The COVID-19 outbreak has
affected our lives in a significant way. Not only does it pose a
threat to the physical health of an individual, but rumors and
fake facts can have an adverse effect on one’s mental wellbeing. Such misinformation can bring another set of challenges
to governance if not detected in time especially due to it’s viral
nature.
* Work done during internship at IBM Research, India.

Fig. 1. Two examples of COVID-19 related tweets; Left one shows an
example of fake tweet, which has high number of retweet and like counts;
Right one shows an example of tweet with real facts, which is has low number
of retweets. Number of retweets and likes is proportional to the popularity of
a tweet.

Veracity of rumours over social media makes the detection
challenging, and has been studied widely in recent past [1],
[2]. Though more popular or highly retweeted messages may
seem to be factual, it need not be the case especially in fast
dissemination periods such as the COVID era; Fig. 1 depicts
an example. In addition, the proliferation of Twitter to diverse demography induces challenges due to usage of locality
specific languages. For example, various Indic languages, e.g.
Hindi, Bengali, Telugu, Kannada are widely used in Twitter.
Though there are some datasets released in English [3] for
COVID fake-news, there are hardly any datasets released for
Indic-languages.
In this paper, we propose an approach where besides network and user related features, the text content of message is
given high importance. In particular, we train our model to
understand the textual content using mBERT [4] on COVID
dataset to classify fake or genuine tweet. Our proposed work
describes a method to detect fake and offensive tweets about
COVID-19 using a rich feature set and a deep neural network
classifier. We leverage Twitter dataset for fake news detection
in English language released in [3]. Since today’s society
is cosmopolitan and multi-lingual, we consider a language
agnostic model to detect fake tweets. For fake detection in a
multi-lingual environment we fine-tune mBERT (Multilingual
BERT)1 to obtain textual features from tweets. We created
fake news dataset in two Indic-Languages (Hindi and Bengali)
by extracting tweets and annotated them for misinformation.
Our multilingual model achieves reasonably high accuracy
1 https://github.com/google-research/bert/blob/master/multilingual.md

to detect fake news in Indic-Languages when trained with
combined dataset (English and Indic-Languages). Towards
scalability and generalization to other Indic-Languages, we
also propose a zero-shot approach where the model is trained
on two languages, e.g. English with Hindi, and tested on a
third language, e.g. Bengali. We experimented rigorously with
various languages and dataset settings to understand the model
performance. Our experimental results indicate comparable
accuracy in zero shot setting as well. Belonging to the IndoAryan family of languages, the Indic languages share similar
syntactic constructs which seems to aid the cross lingual
transfer learning and help attain good accuracy.
Our key contributions can be enumerated as follows:
i) We have created COVID-19 fake multilingual tweet dataset Indic-covidemic fake tweet dataset, for Indic Languages (Hindi
and Bengali), which is being released. As per our knowledge,
this is the first multilingual COVID-19 tweet related dataset
in Indic language.
ii) We propose an mBERT based model for Indic languages,
namely Hindi and Bengali, for fake tweet detection. We
show that the model, fine-tuned on our proposed multilingual
dataset, outperforms single language models. Moreover, we
establish bench mark results for Indic languages for COVID
fake tweet detection.
iii) Our zero shot setting, suitable for low resource Indic
languages, performs comparable to models trained with combined dataset on Indic languages. Our experimental evaluation
indicates that feature representations are highly transferable
across Indic Languages enabling extension to other Indic
languages.
II. R ELATED LITERATURE
Fake News Detection: Fake news or false information
involves various research streams such as fact-checking [5],
topic credibility [6]. An overview of fake news detection
approaches through data mining perspective on social media
has been discussed in [1] and [7]. Various studies has also
been carried out related to COVID-19 related disinformation.
Identifying low-credibility information using data from social
media is studied in [8], detecting prejudice in [9], related
to ethical issues in [2], misinformation spread of sociocultural issues in [10] and detecting misleading information
and credibility of users who spread it in [11]. Some recent
work have focussed on detecting fake news from Tweets. In
GCAN [12], credibility of user is studied based on sequence of
retweets using graph networks. On the other hand, SpotFake
[13] uses multimodal information from tweet text, using BERT
embeddings, and corresponding image to detect fake tweets.
In HawksEye [14], textual and temporal information about
retweets are leveraged. Our work differs from these as we
train our model to identify fake news leveraging mainly text
features supported by other user informations related to their
credibility on Twitter.
BERT [4] has become a state-of-the-art as a contextual
representation of various NLP tasks. mBERT, a multilingual
variant, is pretrained on 104 languages which gained large

popularity due to its exceedingly well performance in various cross lingual NLP tasks [15] lately. In our models, we
finetuned both BERT and mBERT on exiting and our created
dataset for fakenews detection tasks.
Covid-19 Datasets: Recent surge in fake news detection research is also evident from various rumour detection datasets,
such as, SemEval2019 task7 [16] to determine rumour veracity, SemEval 2019 task 8 [17] for fact-checking. Infodemic
Covid-19 dataset [3] is one of the first tweet dataset to
distinguish fake and negative tweets. Though most studies and
dataset focusses only English language, it has become more
important to study in other languages as well, in multilingual, especially with regard to COVID. In [18], COVID-19
Instagram dataset is developed for multilingual usage. In [3].
besides English tweets, Arabic tweets have been also used to
explore misinformation impact. However, there is no dataset
available for fake tweet detection in Indic languages to the
best of our knowledge. We are the first to create dataset for
fake news in Twitter in two common Indic Languages, Hindi
and Bengali, called as Indic-covidemic.
III. DATA SET
Indic-covidemic tweet dataset is one of the first multilingual
Indic language tweet dataset designed for the task of detecting
fake tweets. We describe the details of the dataset in the
following sub-sections.
1) English Annotations: In our work, the definition of fake
tweets holds as follows : ”Any tweet that doesn’t contain a
verifiable claim is a malicious or fake tweet.” For our task, we
use the Infodemic Covid19 dataset [3] as one of the training
dataset for our classifier. This dataset has 504 tweets in English
and 218 tweets in Arabic, annotated with fine-grained labels
related to disinformation about COVID-19. The labels answer
seven different questions, which are related to negative effect
and factual truthfulness of the tweet and we only consider the
annotations for the first question, which is: ”Does the tweet
contain a verifiable factual claim?”. For the interest of this
task, we only utilize the English tweets of Infodemic Covid19
dataset.
2) Indic Annotations: In most of the existing tasks, detection of fake tweets have been predominantly done only in
English. But in recent times, Twitter has seen a significant
rise in regional language tweets2 . This formed our motivation
to extend this task to Indic languages as well. For this task,
the Indic languages of our choice are Hindi and Bengali.
This choice is guided by the fact that these languages are
the two most widely spoken languages in India3 and also the
two most widely used Indic languages in the world. We now
enlist the methods in which the Indic tweets were obtained
and processed below.
2 Source:
https://tech.economictimes.indiatimes.com/news/internet/
india-is-clocking-fastest-revenue-growth-for-twitter-india-md-manish-maheshwari/
71999148
3 Source: https://en.wikipedia.org/wiki/List of languages by number of
native speakers in India

We obtain the Bengali tweets from [19] which is a database
of over 36,000 COVID related Bengali tweets, without any
task-specific annotated labels. We randomly select 100 tweets
from this database and annotate it with the same labeling
schema followed by [3] for the first annotation question as
mentioned above. We further augment the dataset with Bengali
translations of the English tweets of the Infodemic dataset. We
perform the translations using Google Translate API. Out of
all the translations obtained we only keep those translations
whose language is detected as Bengali by the same translation
API.
For Hindi, we make use of the tweepy API [20] to scrape
COVID related tweets from the Twitter interface. This was
achieved by firing an API search with COVID related keyterms like ”Covid”, ”Corona”, etc. We follow the translation
data augmentation process and the same labeling schema for
Hindi tweets as well. Augmenting the dataset with machine
translated texts adds noise to the dataset and helps in training
a more robust model. The tweets in this dataset have been
collected over a time period of March to May 2020 and records
a balanced distribution of spam to non spam tweets which can
be observed in the Tab. I.
Language
English
Bengali
Hindi
Total

Fake
199
183
192
574

Non-Fake
305
297
262
864

TABLE I
I NDIC M ISINFORMATION DATASET S TATISTICS

We also consider the features extracted from the Twitter user
information, as a part of the dataset. For English, we make use
of the user features provided by [3]. For Bengali and Hindi we
have scraped the user information using the tweepy API and
have provided it along the dataset. We have made our dataset
and codes publicly available to further the cause of research
in this domain.4
IV. P ROPOSED A PPROACH
Our proposed approach is built on top of the method used
in [3]. The main essence of the proposed approach lies in the
features used for classification task and the different classifiers
and their corresponding adaptation done for identifying the
fake tweets. The details are described below.
A. Extracted Features
We extract various textual and statistical information from
the tweet text messages and user information separately, and
analyse their role in the classification process. The different
features are enlisted as follows :
1) Text Features (tweettext): We extract some of the twitter
and textual features such as:
i retweet count - The number of times a tweet has been
retweeted
4 https://github.com/DebanjanaKar/Covid19

FakeNews Detection

ii favourite count - The number of likes received by a
tweet.
iii number of upper characters in a tweet
iv number of question mark (s) in a tweet
v number of exclamation mark (s) in a tweet
2) Twitter User Features (tweetuser): ”A man is known
by the company he keeps.” A considerable amount of
literature in this task has already cited the immense
importance of analysing the user’s persona by extracting
features from the user’s Twitter profile. In our work, we
extract the following 19 features from a user’s profile, out
of which 7 features are new additions with respect to the
work in [3]:
i Chars in desc : The number of characters in user’s
description.
ii Chars in real name : The number of characters in
user’s real name.
iii Chars in user handle : The number of characters in
user’s handle on twitter.
iv Num Matches (new): Number of character matches in
real name and username.
v Total URLs in desc (new): Total number of URLs in
user’s description.
vi Official URL exists (new): Whether the user has an
official URL or not.
vii Followers count: Number of people that are following
the user.
viii Friends count: Number of people the user is following.
ix Listed count: Number of lists to which the user has
been added.
x Favourites count: Total number of likes the user has
received throughout the account life.
xi Geo enabled: Whether the user has allowed location
access.
xii Acc life (new): Number of days since the account was
created.
xiii Verified: Whether the user account is verified officially
by twitter or not.
xiv Num tweet: Total number of tweets tweeted by the
user.
xv Protected: Whether the user account is protected or not.
xvi Posting frequency (new): Number of tweets tweeted
per day by the user.
xvii Activity (new): Number of days since the user’s latest
tweet.
xviii Avg likes per tweet (new): Total likes received by the
user divided by total tweets tweeted by the user.
xix Follower friends ratio: Followers’ to friends’ ratio.
3) Fact verification score (FactVer) - We use the tweet text
as a search query in Google search engine and consider
the search results from reliable sources only. For this, we
have manually selected 50 websites as reliable, which
consists of popular news websites. We get the FactVer
score by taking the average of the Levenshtein distance
between the tweet text and the titles obtained from the

search results from these reliable links only.
4) Bias score (Bias) - An observed trait of fake tweets is that
it often contains disturbing content which is malicious
and in violation of Twitter’s terms of use [21]. A 2018
Amnesty International report5 shows women are abused
on Twitter every 30 seconds, with racism, sexism and
homophobia in the social media platform and usually
such tweets contain untrue facts presented in a vindictive
fashion. To explicitly make use of this knowledge, we use
a Linear Support Vector Classifier (SVM) pre-trained on
a very large annotated corpus. We use the pre-trained
profanity checker to obtain the probability of a tweet
containing offensive language and use it as the ’bias
score’ in our model. As can be observed in the feature
map above Fig. 2, the ’bias score’ obtained shows a strong
negative correlation with the fakeness of the tweets.
5) Source Tweet Embedding (TextEmbd): The final and the
most obvious feature in this task is the tweet itself. We
consider the BERT/mBERT embeddings of the tweet text
as the feature.

Fig. 3. Model (non Transformer) architecture used to detect fake tweets.

different feature combinations and classifiers for our purpose.
We use Linear Support Vector Machine (SVM), Random
Forest Classifier (RFC), Multi Layer Perceptron (MLP) and
in-built multilingual BERT (mBERT, base cased) classifier
(FC layer + softmax) for the classification task. We fine-tune
the mBERT classifier by appending a layer (768 times 2) at
the end followed by softmax and tune it for the classification
task. The parameter details of the classifiers can be found in
the experimental section. We have experimented with various
combinations of classifiers and features and the noteworthy
ones have reported in the next section.
V. E XPERIMENTAL D ETAILS
We perform out experimentation on detecting fake tweets
on our proposed Indic-covidemic fake tweet dataset. The
tweets are pre-processed, where the hashtags, URL, user-tags
have been removed and the text is converted to lower case
for uniformity. We first perform experimentation on English
Tweets and extend it to suit the needs of the Indic tweets. The
details are explained in the following sub-sections.

Fig. 2. Correlation of hand-crafted features with the class-labels show the
effectiveness of the extracted features for the classification task.

The correlation plots of the hand-crafted features with the
class-label is shown in Fig. 2. It can be observed that not all
features are helpful for the classification task. Nevertheless for
now, we have considered all the features together, and keep
automatic feature selection as a future scope of work.
B. Classifier
Given a tweet, the above-mentioned features are extracted
from it, various combinations of which are passed to the
choice of classifier for the prediction task. The architecture
of the proposed framework is shown in Fig. 3. We use
5 Source:
(https://www.amnesty.org.uk/press-releases/
women-abused-twitter-every-30-seconds-new-study#.XBjMprLxot0.twitter)

A. English Tweet Classification - mono-Lingual classifier
We started our experimentation by classifying the English
tweets to either Fake or Non-Fake class. We use the BERT
model with pre-trained weights to get the sentence embedding
of the tweet texts. We used 80% of the dataset for training and
20% for testing. Pretrained BERT embeddings are being used
with (or without) different feature combinations, to classify
using Support Vector Machine (BERT SVM) with linear kernel (C=1, Gamma=1), Random Forest Classifier (BERT RFC)
with 400 estimators, Multi-Layer Perceptron Neural Network
(BERT MLP) classifier. MLP uses 30 neurons in the first
hidden layer and 10 neurons in second hidden layer, with
ReLU activation function and trained for 1000 epochs during
training. We also use mBERT embedding with top classifier
layer (mBERT NN) using FC layer (768x2) followed by softmax for classification purpose. Table II shows the performance

analysis with various combinations of features and classifiers.
We keep the notable feature combinations, while discarding
the non-useful ones. The first row denotes the results obtained
by Alam et. al. [3] using mBERT model, which is being
considered for comparative study.
Model
Infodemic [3]
BERT SVM

BERT RFC

BERT MLP

mBERT NN
(fine-tuned)

Features
TextEmbd
TextEmbd
TextEmbd + tweettext
TextEmbd + tweetuser
TextEmbd
TextEmbd + tweettext
TextEmbd + tweetuser
TextEmbd
TextEmbd + tweettext
TextEmbd + tweetuser
TextEmbd
TextEmbd + tweettext
TextEmbd + tweetuser
TextEmbd + FactVer
TextEmbd + Bias
TextEmbd + tweetuser
+ FactVer

Prec.
74
50
50
73
72
88
73
64
58
87.17
84.38
90.32
90.33
82.35
89.65

Metric Scores
Recall F-score
88.3
76
75
31
38
30
37.5
75
74
74
73
89
89
71
72
62
63
57
57.5
91.89
89.47
84.38
84.38
87.5
88.88
87.5
88.88
87.5
84.84
81.25
85.25

TABLE II
MONO-LINGUAL SETTING (ENGLISH TWEETS):P RECISION ,
R ECALL AND F- SCORES OF OUR PROPOSED METHOD USING DIFFERENT
COMBINATIONS OF FEATURES AND CLASSIFIERS FOR E NGLISH TWEETS .

Analysis - We observe that adding of features always does
not improve the performance of the classifier. This is not
strange while dealing with hand-crafted features. Most of the
user features are correlated to the class-label. Hence it is
expected that addition of user features will boost the classifier
performance. However, due to the shortcomings of SVM and
MLP in dealing with small sample size problem, we rely
on the metric numbers obtained by RFC (offers a piecewise linear decision boundary in the feature space) and Finetuned in-build mBERT classifier (mBERT NN). We observe
that TextEmbd, tweetuser and FactVer are strong features
and consider these for further analysis of Indic tweets. We
emphasise on a high recall value as we want a low false
positive value so that fake tweets gets filtered out to the best
possible manner.
B. Indic Tweet Classification - multi-Lingual classification
We fine-tune mBERT and use text embedding (with and
without FactVer) as features. We use fine-tuned mBERT for
the classification task (mBERT NN). Tab. III shows the performance of different experiments in multi-lingual settings. We
also test each of the language tweets separately in a zero-shot
settings, where one of the language tweets are being used only
as the test set. As mentioned by Alam et. al. [3], transformers
models are sensitive to the seed value specially when the
training data is not very large. Following [3]. We experiment
with 20 different random seed and fix one particular seed for
the entire experimentation.
Analysis - TextEmbd is a strong feature which gives consistent performance in all the settings. This particular feature is
tuned as per the FCLayer (in-build classifier of mBERT) which

further favours it. Addition of FactVer helps for Hindi, but not
for Bengali tweets. However, we would like to conclude that it
is a strong feature as well. It is to be noted, that the adaptation
from English+Hindi to Bengali and English+Bengali to Hindi
is encouraging. But when the classifier is tested on English
tweets, while using Hindi+Bengali as train set, the performance deteriorates, This is due to the fact that the similarities
between Hindi and Bengali is much more than Hindi-English
or Bengali-English.
C. Ablation Study
We study the cases where our proposed method acts dubiously, which are as follows:
• Fake information from verified Twitter users - We have
observed that the model gets confused in detecting false
information when tweeted by a verified user. This can
be overcome by considering similar examples during
training, which will give less importance to the particular
feature (verified) in tweetuser feature set.
• Adapting model for English tweets in zero-shot learning
setting - As mentioned above, the model does not generalize when English is the test set and the model has been
trained on Hindi and Bengali tweets. This is reflected
in the last row of Tab. III. This is due to the immense
linguistic difference of English with that of Hindi and
Bengali. Since both Hindi and Bengali are Indo-Aryan
languages, they are compatible with each other. This
problem can be overcome by adding language specific
layers in the architecture, which will take care of the
linguistic features, specially in zero shot learning settings.
• Correlated features - Statistical classifiers do not always
scale performance as the number of features increase,
unlike deep neural networks. The hand-crafted features
may seem intuitive at times, but may not always have the
distinctive property to help in the classification task. We
see this nature in our experiments as well.
We would like to emphasis that these gaps are mostly due to
less number of training samples and inherent drawbacks in the
classifiers that we have adapted for the task.
D. API and Demo
As a part of the project, we expose the model as a service
through a flask API and static HTML page, where given the
tweet id/text, it determines if the tweet is fake or not. Currently,
this has been deployed in IBM intranet. It allows anybody to
check if a given tweet is a fake one or not. Some of the API
is shown in Fig. 4.
VI. C ONCLUDING R EMARKS
In this work, we propose a multilingual approach to detect
fake news about COVID-19 from Twitter posts for multiple
Indic-Languages. In addition, we also created an annotated
dataset, Indic-covidemic tweet dataset, of Hindi and Bengali
tweets for fake news detection. The proposed model is build on
mBERT based embedding augmented with additional relevant
features from Twitter to classify fake or genuine tweets. We

Evaluation type
Cross-Domain
Data Augmentation

Cross-Domain
Zero-shot

Language
Train
Test
Eng+Hin+Ben
Hindi
Eng+Hin+Ben
Hindi
Eng+Hin+Ben Bengali
Eng+Hin+Ben Bengali
Eng+Ben
Hindi
Eng+Hin
Bengali
Hin+Ben
English

Features
TextEmbd
TextEmbd+FactVer
TextEmbd
TextEmbd+FactVer
TextEmbd
TextEmbd
TextEmbd

Prec.
72.72
75.00
76.47
73.5
70.30
90.66
92.75

Metric Scores
Recall
F-score
84.21
78.04
84.0
79.24
86.66
81.25
83.33
78.12
95.80
81.09
68.68
77.79
62.95
75.00

TABLE III
MULTILINGUAL SETTING (ENGLISH, HINDI, BENGALI): P RECISION , R ECALL AND F- SCORES OF OUR PROPOSED METHOD USING DIFFERENT
COMBINATIONS OF FEATURES AND CLASSIFIERS ( FIRST THREE ROWS ) AND USING DIFFERENT COMBINATIONS OF TRAIN AND TEST SETS FOR THE BEST
PERFORMING CLASSIFIER FOR I NDIC LANGUAGES ( MULTI - LINGUAL SETTINGS ).

Fig. 4. Snapshots of the HTML page integrated with flask API which is used to predict tweet text using our proposed classifier model.

show that our model reaches around 89% F-score in fake
news detection which supercedes SOTA scores for English
dataset. We show that model trained with multiple IndicLanguages (our Indic-covidemic tweet dataset) fake news
dataset tweets shows improved performance which can be
attributed to cross-lingual transfer learning as many IndicLanguages share similar syntactic constructs. Moreover, we
establish first benchmark for two Indic languages, Hindi and
Bengali. Our model achieves about 79% F-Score in Hindi
and 81% F-score for Bengali Tweets. Further, with the focus
to scale our model for other low resource Indic-Languages,
we have proposed zero-shot learning approach for fake tweet
detection. Our zero shot model achieves about 81% accuracy
in Hindi and 78% accuracy for Bengali Tweets without any
annotated data, which clearly indicates the efficacy of our
approach. In future, we would like to add an automatic feature
selection module which can take the most informative subset
of handcrafted features for the classification task.
R EFERENCES
[1] K. Shu, A. S. Suhang, Wang, J. Tang, and H. Liu, “Fake news detection
on social media: A data mining perspective,” in SIGKDD Explor. Newsl,
vol. 19(1), 2017, pp. 22–36.
[2] K. Ding, K. Shu, Y. Li, A. Bhattacharjee, and H. Liu., “Challenges in
combating covid-19 infodemic – data, tools, and ethics,” 2020.
[3] F. Alam, S. Shaar, F. Dalvi, H. Sajjad, A. Nikolov, H. Mubarak, G. D. S.
Martino, A. Abdelali, N. Durrani, K. Darwish, and P. Nakov, “Fighting
the covid-19 Infodemic: Modeling the perspective of journalists, factcheckers, social media platforms, policy makers, and the society,” 2020.
[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
of deep bidirectional transformers for language understanding,” 2018.
[5] J. Thorne, A. Vlacho, C. Christodoulopoulos, and A. Mittal, “Fever: a
large-scale dataset for fact extraction and verification,” in Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT’18,New Orleans,
Louisiana, USA., 2018, pp. 809–819.
[6] J. Thorne, A. Vlachos, O. Cocarasc, C. Christodoulopoulos, and A. Mittal, “The fever2.0 shared task,” in Second Workshop on Fact Extraction
and VERification (FEVER), Association for Computational Linguistics,
HongKong, China, 2019, pp. 1–6.

[7] Y. Li, J. Gao, C. Meng, Q. Li, L. Su, B. Zhao, W. Fan, and J. Han, “A
survey on truth discovery,” in SIGKDD Explor. Newsl., vol. 17(2), 2016,
pp. 1–16.
[8] K.-C. Yang, C. Torres-Lugo, and F. Menczer, “Prevalence of lowcredibility information on twitter during the covid-19 outbreak,” 2020.
[9] B. Vidgen, A. Botelho, D. Broniatowski, E. Guest, M. Hall, H. Margetts,
R. Tromble, Z. Waseem, and S. Hale, “Detecting east asian prejudice
on social media,” 2020.
[10] Y. Leng, Y. Zhai, S. Sun, Y. Wu, J. Selzer, S. Strover, J. Fensel,
A. Pentland, and Y. Ding, “Analysis of misinformation during the covid19 outbreak in china: cultural, social and political entanglements,” 2020.
[11] A. Mourad, A. Srour, H. Harmanan, C. Jenainatiy, and M. Arafeh,
“Critical impact of social networks infodemic on defeating coronavirus
covid-19 pandemic: Twitter-based study and research directions,” 2020.
[12] Y.-J. Lu and C.-T. Li, “Gcan: Graph-aware co-attention networks for
explainable fake news detection on social media,” 2020.
[13] S. Singhal, R. R. Shah, T. Chakraborty, P. Kumaraguru, and S. Satoh,
“Spotfake: A multi-modal framework for fake news detection,” in 2019
IEEE Fifth International Conference on Multimedia Big Data (BigMM),
2019, pp. 39–47.
[14] H. S. Dutta, V. R. Dutta, A. Adhikary, and T. Chakraborty, “Hawkeseye:
Detecting fake retweeters using hawkes process and topic modeling,”
IEEE Transactions on Information Forensics and Security, vol. 15, pp.
2667–2678, 2020.
[15] T. Pires, E. Schlinger, and D. Garrette, “How multilingual is multilingual
BERT?” in 57th Annual Meeting of the Association for Computational
Linguistics, Florence, Italy. Association for Computational Linguistics.,
2019, pp. 4996–5001.
[16] G. Gorrell, A. Aker, K. Bontcheva, L. Derczynski, E. Kochkina, M. Liakata, and A. Zubiaga, “Semeval-2019 task 7: Rumoureval, determining
rumour veracity and support for rumours,” in 13th International Workshop on Semantic Evaluation, Minneapolis, Minnesota, USA, 2019, pp.
845–854.
[17] T. Mihaylova, G. Karadzhov, P. Atanasova, R. Baly, M. Mohtarami,
and P. Nakov, “Semeval-2019 task 8: Rumoureval, determining rumour
veracity and support for rumours,” in 13th Inter- national Workshop on
Semantic Evaluation, Minneapolis, Minnesota, USA, 2019, pp. 860–869.
[18] K. Zarei, R. Farahbakhsh, N. Crespi, and G. Tyson, “A first instagram
dataset on covid-19,” 2020.
[19] A. Garain, “Covid-19 tweets dataset for bengali language,” 2020.
[Online]. Available: http://dx.doi.org/10.21227/wdt0-ya78
[20] “Tweepy.” [Online]. Available: https://www.tweepy.org/
[21] I. Inuwa-Dutse, M. Liptrott, and I. Korkontzelos, “Detection of spamposting accounts on twitter,” Neurocomputing, vol. 315, pp. 496–511,
2018.

