1

COVID-19 Cough Classification using Machine
Learning and Global Smartphone Recordings

arXiv:2012.01926v1 [cs.SD] 2 Dec 2020

Madhurananda Pahar, Marisa Klopper, Robin Warren, and Thomas Niesler
Abstract—We present a machine learning based COVID-19 cough classifier which is able to discriminate COVID-19 positive coughs
from both COVID-19 negative and healthy coughs recorded on a smartphone. This type of screening is non-contact and easily applied,
and could help reduce workload in testing centers as well as limit transmission by recommending early self-isolation to those who have
a cough suggestive of COVID-19. The two dataset used in this study include subjects from all six continents and contain both forced
and natural coughs. The publicly available Coswara dataset contains 92 COVID-19 positive and 1079 healthy subjects, while the
second smaller dataset was collected mostly in South Africa and contains 8 COVID-19 positive and 13 COVID-19 negative subjects
who have undergone a SARS-CoV laboratory test. Dataset skew was addressed by applying synthetic minority oversampling (SMOTE)
and leave-p-out cross validation was used to train and evaluate classifiers. Logistic regression (LR), support vector machines (SVM),
multilayer perceptrons (MLP), convolutional neural networks (CNN), long-short term memory (LSTM) and a residual-based neural
network architecture (Resnet50) were considered as classifiers. Our results show that the Resnet50 classifier was best able to
discriminate between the COVID-19 positive and the healthy coughs with an area under the ROC curve (AUC) of 0.98 while a LSTM
classifier was best able to discriminate between the COVID-19 positive and COVID-19 negative coughs with an AUC of 0.94. The
LSTM classifier achieved these results using 13 features selected by sequential forward search (SFS). Since it can be implemented on
a smartphone, cough audio classification is cost-effective and easy to apply and deploy, and therefore is potentially a useful and viable
means of non-contact COVID-19 screening.
Index Terms—Cough, classification, machine learning, COVID-19, logistic regression (LR), support vector machine (SVM),
convolutional neural network (CNN), long short term memory (LSTM), Resnet50

F

1

I NTRODUCTION

C

OVID19 (COronaVIrus Disease of 2019), caused by
the Severe Acute Respiratory Syndrome (SARS-CoV2)
virus, was announced as a global pandemic on February 11,
2020 by the World Health Organisation (WHO). It is a new
coronavirus but similar to other coronaviruses, including
SARS-CoV (severe acute respiratory syndrome coronavirus)
and MERS-CoV (Middle East respiratory syndrome coronavirus) which caused disease outbreaks in 2002 and 2012,
respectively [1], [2].
The most common symptoms of COVID-19 are fever,
fatigue and a dry cough [3]. Other symptoms include
shortness of breath, joint pain, muscle pain, gastrointestinal symptoms and loss of smell or taste [4]. At the time
of writing, there are 63 million active cases of COVID-19
globally, and there have been 1.5 million deaths, with the
USA reporting the highest number of cases (13.4 million)
and deaths (267,306) [5]. The scale of the pandemic has
caused some health systems to be overrun by the need for
testing and the management of cases.
Several attempts have been made to identify early symptoms of COVID-19 through the use of artificial intelligence
applied to images. The residual neural network (Resnet50)
•

•

Madhurananda Pahar and Thomas Niesler works at Department of Electrical and Electronics Engineering, University of Stellenbosch, Stellenbosch, South Africa - 7600.
E-mail: mpahar@sun.ac.za, trn@sun.ac.za
Marisa Klopper and Robin Warren works at SAMRC Centre for Tuberculosis Research, University of Stellenbosch, Cape Town, South Africa 7505.
E-mail: marisat@sun.ac.za, rw1@sun.ac.za

architecture has been shown to perform better than other
pre-trained models such as AlexNet, GoogLeNet, VGG16
in these tasks, For example, COVID-19 was detected from
computed tomography (CT) images by using a Resnet50 architecture with a 96.23% accuracy [6]. The same architecture
was shown to detect pneumonia due to COVID-19 with an
accuracy of 96.7% [7] and to detect COVID-19 from x-ray
images with an accuracy of 96.30% [8].
Coughing is one of the predominant symptoms of
COVID-19 [9]. However, coughing is also a symptom of
more than 100 other diseases, and their effects on the
respiratory system vary [10]. For example, lung diseases
can cause the airway to be either restricted or obstructed
and this can influence the acoustics of the cough [11]. It
has also been postulated that the glottis behaves differently
under different pathological conditions [12], [13] and that
this makes it possible to distinguish between coughs due
to TB [14], asthma [15], bronchitis and pertussis (whooping
cough) [16], [17], [18], [19].
Respiratory data such as breathing, sneezing, speech,
eating behaviour and coughs can be processed by machine
learning algorithms to diagnose respiratory illness such as
COVID-19 [20], [21], [22]. Simple machine learning tools,
like a binary classifier, are able to distinguish COVID-19 respiratory sounds from healthy counterparts with an AUC exceeding 0.80 [23]. Detecting COVID-19 by analysing only the
cough sounds is also possible. AI4COVID-19 is a mobile app
which records 3 seconds of cough audio which is analysed
automatically to provide an indication of COVID-19 status
within 2 minutes [24]. A medical dataset containing 328
cough sounds have been recorded from 150 patients of four

2

COSWARA DATASET

PREPROCESSING
& FEATURE
EXTRACTION

COVID-19
COUGH
CLASSIFIER

SARCOS DATASET

TRAINED AND EVALUATED
ON COSWARA DATASET

TRAINED ON COSWARA
DATASET AND EVALUATED
ON SARCOS DATASET

BEST CLASSIFIER
(RESNET 50)
ACHIEVES AUC: 0.98
BEST CLASSIFIER
(LSTM) ACHIEVES AUC:
0.94 FROM BEST 13
FEATURES

Fig. 1. Origin of participants in the Coswara and the Sarcos dataset: Participants in the Coswara dataset come from five different continents,
excluding Africa. The majority (91%) of participants in Coswara dataset are from Asia, as explained in Figure 2. Sarcos participants who supplied
geographical information are mostly (62%) from South Africa, as shown in Figure 3.

different types: COVID-19, Asthma, Bronchitis and Healthy.
A deep neural network (DNN) was shown to distinguish
between COVID-19 and other coughs with an accuracy of
96.83% [25]. There appear to be unique patterns in COVID19 coughs that allow a pre-trained Resnet18 classifier to
identify COVID-19 coughs with an AUC of 0.72. In this
case cough samples were collected over the phone from
3621 individuals with confirmed COVID-19 [26]. COVID-19
coughs were classified with a higher AUC of 0.97 (sensitivity
= 98.5% and specificity = 94.2%) by a Resnet50 architecture
trained on coughs from 4256 subjects and evaluated on 1064
subjects that included both COVID-19 positive and COVID19 negative subjects [27].
Data collection from COVID-19 patients is challenging
and often not publicly available. A database consisting of
coughing sounds recorded during or after the acute phase
of COVID-19 from patients via public media interviews has
been developed in [28]. The Coswara dataset is publicly
available and collected in a more controlled and targeted
manner [29]. At the time of writing, this dataset included
usable ‘deep cough’ recordings from 92 COVID-19 positive
and from 1079 healthy subjects. We have also begun to compile our own dataset by collecting recordings from subjects
who have undergone a SARS-CoV laboratory test in South
Africa. This Sarcos (SARS COVID-19 South Africa) dataset
is currently still small and includes 21 subjects (8 COVID-19
positive and 13 COVID-19 negative).
Both the Coswara and Sarcos dataset are imbalanced
since COVID-19 positive subjects are outnumbered by nonCOVID-19 subjects. Nevertheless, collectively these two
dataset contain recordings from all six continents, as shown
in Figure 1. To improve our machine learning classifier’s
performance, we have applied the Synthetic Minority Oversampling Technique (SMOTE) to balance our dataset. Subsequently, classifier hyperparameters were optimised by
using a leave-p-out cross validation, followed by training
and evaluation of artificial neural networks (ANN), such
as LR, SVM, MLP and deep neural networks (DNN) such
as CNN, LSTM, Resnet50 classifiers. Resnet50 produced the
highest area under the ROC curve value of 0.9759 ≈ 0.98
while trained and evaluated on the Coswara dataset. No
classifier has been trained on the Sarcos dataset as it is
small, but has been used to evaluate the performance of
the best-performed DNN classifiers on the Coswara dataset.
It has also been found that highest AUC of 0.9375 ≈ 0.94
has been achieved from best 13 features extracted from
the Sarcos dataset after running a greedy search algorithm
such as a sequential forward search (SFS). We conclude that

diagnosis of COVID-19 is possible from only cough audio
recorded via smartphone, as our AI based cough classifier can discriminate COVID-19 positive coughs from both
COVID-19 negative and healthy coughs anywhere on the
planet. However, additional validation is required to obtain
approval from regulatory bodies for use as a diagnostic tool.

2

DATA C OLLECTION

2.1

Collected Dataset

2.1.1

Dataset 1: Coswara Dataset

The Coswara project is aimed at developing a diagnostic tool for COVID-19 based on respiratory, cough and
speech sounds [29]. The public can contribute to this
web-based data collection effort using their smartphones
(https://coswara.iisc.ac.in). The collected audio data includes fast and slow breathing, deep and shallow coughing,
phonation of sustained vowels and spoken digits. Age,
gender, geographical location, current health status and preexisting medical conditions are also recorded. Health status
includes ‘healthy’, ‘exposed’, ‘cured’ or ‘infected’. Audio
recordings were sampled at 44.1 KHz and subjects were
from all continents except Africa, as shown in Figure 2.
The collected data is currently being annotated and will be
released in due course. In this study we have made use
of the raw audio recordings and applied preprocessing as
described in Section 2.2.
2.1.2

Datset 2: Sarcos Dataset

Like Coswara, this dataset was collected using an online
platform: https://coughtest.online. Subjects were prompted
to record their cough using their smartphone. Only coughs
were collected as audio samples, and only subjects who had
recently undergone a SARS-CoV laboratory test were asked
to participate. The sampling rate for the audio recordings
was 44.1 KHz. In addition to the cough audio recordings,
subjects were presented with a voluntary and anonymous
questionnaire, providing informed consent. The questionnaire prompted for the following information.
•
•
•
•
•
•
•
•

Age and gender.
If tested by an authorised COVID-19 testing centre.
Days since the test was performed.
Lab result (COVID-19 positive or negative).
Country of residence.
Known contact with COVID-19 positive patient.
Known lung disease.
Symptoms and temperature.

3
COVID Positive and
Healthy Subjects

Age Distribution

Male and Female
Subjects

100

COVID Positive and
Negative Subjects

Male and Female
Subjects

Days since the Lab Test

80

92
Healthy

889

60

1079

COVID
Positive

6

13

6

15

4

8

40

3

6

20

282

0
0

20

40

60

80

FEMALE

COVID Negative

MALE

COVID Positive

Subjects with
COVID-19 contacts

Female

Male

1 to 3

4 to 6

2

7 to 9 10 to 12

>15

Days since Coughing started

Do they have a
Normal Cough?
12

12

Asia (91%)

12
9

9

1

Australia (0.14%)
Europe (2.75%)
North America (5.5%)

0
No

Yes

No

Yes

5%

•
•

If they are a regular smoker.
If they have a current cough and for how many days.

There were 13 (62%) subjects who asserted that they are
South African residents representing the African continent,
as shown in Figure 3. There were no subject from Africa
in the Coswara dataset. Thus, together, the Coswara and
Sarcos dataset include subjects from all six continents.
2.2

1

1

2

13 to
15

>15

Brazil (1)
33%

62%

Prefer not to say (7)
South Africa (13)

Fig. 3. Sarcos dataset at the time of experimentation: There are
13 COVID-negative and 8 COVID-positive subjects in the processed
dataset. Unlike Coswara dataset, there are more female than male
subjects. Most of the subjects had their lab test performed less than
two weeks ago. Of the 21 subjects, 12 had been in contact with another
COVID-19 positive person. Only 9 of the subjects reported coughing as
a symptom, and for these the reported duration of coughing symptoms
was variable. There were 13 subjects from Africa (South Africa), 1
from South America (Brazil), and the rest declined to specify their
geographic location.

Data Preprocessing

The amplitudes of the raw audio data in the Coswara and
the Sarcos dataset were normalised, after which periods of
silence were removed from the signal to within a 50 ms
margin using a simple energy detector. Figure 4 shows an
example of the original raw audio, as well as the preprocessed audio.
The coughs in both Coswara and Sarcos dataset after
preprocessing are shown in Table 1. The Coswara dataset
contains 92 COVID-19 positive and 1079 healthy subjects
and the Sarcos dataset contains 8 COVID-19 positive and 13
COVID-19 negative subjects.
2.3

2

Country of Origin

South America (0.14%)

Fig. 2. Coswara dataset at the time of experimentation: There are
1079 healthy and 92 COVID-19 positive subjects in the processed
dataset, used for feature extraction and classifier training. Most of the
subjects are middle aged, between 20 to 50. There are 282 female
and 889 male subjects and most of them are from Asia. Subjects are
from these five continents: Asia (Bahrain, Bangladesh, China, India,
Indonesia, Iran, Japan, Malaysia, Oman, Philippines, Qatar, Saudi Arabia, Singapore, Sri Lanka, United Arab Emirates), Australia, Europe
(Belgium, Finland, France, Germany, Ireland, Netherlands, Norway, Romania, Spain, Sweden, Switzerland, Ukraine, United Kingdom), North
America (Canada, United States), South America (Argentina, Mexico)

2

1 to 3 4 to 6 7 to 9 10 to
12

Dataset Balancing

Table 1 shows that COVID-19 positive subjects are underrepresented in both dataset. To compensate for this imbalance, which can detrimentally affect machine learning [30],
[31], we have applied SMOTE data balancing during training [32], [33]. This technique has previously been successfully applied to cough detection and classification based on
audio recording [17]. SMOTE oversamples the minor class
by generating synthetic examples, instead of for example
random oversampling.
In our dataset, for each COVID-19 positive cough, 5
other COVID-19 positive coughs were randomly chosen
and the one with the smallest Euclidean distance from the

Fig. 4. A processed COVID-19 cough audio which is shorter than the
original cough but keeps all spectrum resolution.

original cough xN N is selected. We note the COVID-19
positive class as x. Then, the synthetic samples are created
according to Equation 1.

xSM OT E = x + u · (xN N − x)

(1)

The multiplicative factor u is uniformly distributed between 0 and 1 [34].
We have also implemented other extensions of SMOTE
such as borderline-SMOTE [35], [36] and adaptive synthetic
sampling [37]. However, the best results were obtained by
using SMOTE without any extension.
The balanced processed coughs from all the subjects are
used in the feature extraction process and then used for

4

TABLE 1
Coughs in the both Coswara and Sarcos Dataset: Of 1171 subjects
with usable ‘deep cough’ recordings, 92 were COVID-19 positive while
1079 subjects were healthy. The Coswara dataset has total 1.05 hours
of cough audio recording used in the data balancing, feature extraction
and classifier training and evaluation process. Sarcos dataset has 1.28
minutes of cough audio recordings used for data balancing, feature
extraction and classifier evaluation.
No. of
Subjects

Total
Lengths

Average
Length

STD
Length

92

4.24 mins

2.77 sec

1.62 sec

1079
1171

0.98 hours
1.05 hours

3.26 sec
3.22 sec

1.66 sec
1.67 sec

8

0.5 mins

3.75 sec

2.61 sec

13

0.78 mins

3.59 sec

3.04 sec

21

1.28 mins

3.65 sec

2.82 sec

Coswara
COVID Positive
Coswara Healthy
Coswara Total
Sarcos
COVID Positive
Sarcos
COVID Negative
Sarcos Total

FEATURE
EXTRACTION

MFCC ∆, ∆2

FEATURE DATASET USED IN
CLASSIFICATION PROCESS

training and evaluating our classifiers.

F EATURE E XTRACTION

The feature extraction process is illustrated in Figure 5.
We have considered mel frequency cepstral coefficients
(MFCCs), log energies, zero-crossing rate (ZCR) and kurtosis as features.
Mel frequency cepstral coefficients (MFCCs)

Mel-frequency cepstral coefficients (MFCCs) have been used
very successfully as features in audio analysis and especially
in automatic speech recognition [38]. They have also been
found to be useful for differentiating dry coughs from wet
coughs [39].
We have used the traditional MFCC extraction method
considering higher resolution MFCCs, while mel-scaled filters are calculated by following Equation 2, along with
velocity and acceleration.

fmel (f ) = 2595 × (1 +
3.2

ZCR =

T −1
1 X
λ(st st−1 < 0)
T − 1 t=1

(4)

where λ = 1 when the sign of st and st−1 differ and λ = 0
when the sign of st and st−1 is the same.
3.4

Kurtosis

The kurtosis [42] indicates the tailedness of a probability
density. For the samples of an audio signal, it indicates
the prevalence of higher amplitudes. Kurtosis has been
calculated according to Equation 5.

KURTOSIS

SPLIT THE PROCESSED COUGH
INTO MULTIPLE SEGMENTS

Fig. 5. Feature Extraction: Processed cough recordings, shown in
Figure 4, are split into individual sections after which features including
MFCCs (including velocity and acceleration), log energies, ZCR and
kurtosis are extracted.

3.1

The zero-crossing rate (ZCR) [41] is the number of times
the signal changes sign within a frame, as indicated in
Equation 4. ZCR indicates the variability present in the
signal.

E[(xi [k] − µ)4 ]
(5)
σ4
These features have been extracted by using the hyperparameters explained in Table 2 for all cough recordings.

ZCR

3

Zero-crossing rate (ZCR)

Λx =

MFCC

LOG ENERGIES

A PROCESSED COUGH AUDIO

3.3

f
)
700

(2)

Log Energies

This feature [40] is well used in improving performance of
neural networks. If the input signal is s(t) and N is the
total number of samples in the signal, then log energy is L,
defined by Equation 3.
P
|s(t)|2
L = log10 (0.001 +
)
(3)
N

4

C LASSIFIER ARCHITECTURES

In the following we will briefly describe the classifiers which
were evaluated in our experimental evaluation.
4.1

Logistic Regression (LR)

Logistic regression (LR) models have been found to outperform other state-of-the-art classifiers such as classification
trees, random forests, artificial neural networks such as SVM
in some clinical prediction tasks [14], [43], [44]. The output
P of a LR model is given by Equation 6, where a and b are
the model parameters.

1
(6)
1 + e−(a+bx)
Since P varies between 0 and 1, it can be interpreted as
a probability and is very useful in binary classification.
We have used gradient descent weight regularisation as
well as lasso (l1 penalty) and ridge (l2 penalty) estimators
during training [45], [46]. These regularisation hyperparameters are optimised during cross validation, explained in
Section 5.2.
This LR classifier has been intended primarily as a baseline against which any improvements offered by the more
complex architectures can be measured.
P =

4.2

Support Vector Machine (SVM)

Support vector machine (SVM) classifiers have performed
well in both detecting [47], [48] and classifying [49] cough
events.
We have used both linear and non-linear SVM classifiers
φ(w) which is computed in the Equation 7.

1 T
w w − J(w, b, a)
(7)
2
and where, J(w, b, a) is the term to minimise by the hyperparameter optimization for the parameters mentioned in
Table 3.
φ(w) =

5

1
α2

α1

1

0

α4

CONVOLUTION
2-D LAYERS

APPLY MAXPOOLING
WITH
DROPOUT

APPLY
FLATTENING
WITH
DROPOUT

RATE = α3

RATE = α3

α4

β1

α2
INPUT FEATURE
MATRIX

REDUCE NUMBER
OF DENSE LAYER
TO 8 AND FINALLY
TO 2

INPUT FEATURE
MATRIX

β1 NUMBER OF
LSTM UNITS

APPLY
FLATTENING
WITH DROPOUT
RATE = α3

Fig. 6. CNN Classifier: Our CNN classifier uses α1 two-dimansional
convolutional layers with kernel size α2 , rectified linear units as activation functions and a dropout rate of α3 . After max-pooling, two dense
layers with α4 and 8 units respectively and rectified linear activation
functions follow. The network is terminated by a two-dimensional softmax where one output represents the COVID-19 positive class and the
other Healthy or COVID-19 negative class. During training, features are
presented to the neural network in batches of size ξ1 for ξ2 epochs.

4.3

Multilayer Perceptron (MLP)

n
X
y = φ(
wi xi + b) = φ(wT x + b)

4.4

(8)

Convolutional Neural Network (CNN)

A convolutional neural network (CNN) is a popular deep
neural network architecture which is primarily used in image classification [54]. For example, in the past two decades
CNNs have been applied successfully to complex tasks
such as face recognition [55]. The core of a CNN can be
expressed by Equation 9, where net(t, f ) is the output of
the convolutional layer [56].

d~s(t) ~
~
= h(~s(t), ~x(t)) + φ
dt

XX
m

x[m, n]w[t−m, f −n] (9)

n

where ∗ is the convolution operation, w is the filter or
kernel matrix and x is the input image. In the final layer,
the softmax activation function is applied [57].
The hyperparameters optimised for the CNN classifier
used in this study is mentioned in Table 3 and visually
explained in Figure 6.

(10)

where, ~h(~s(t), ~
x(t)) is a vector-valued function of
vector-valued arguments [62].
The hyperparameters optimised for the LSTM classifier
used in this study is mentioned in Table 3 and illustrated in
Figure 7.
4.6

Resnet50 Classifier

The deep residual learning (Resnet) neural network [63] is
a very deep architecture that contains skip layers, and has
been found to outperform other very deep architectures. It
performs particularly well on image classification tasks on
the dataset such as ILSVRC, the CIFAR10 dataset and the
COCO object detection dataset [64]. Resnet50 has already
been used in successfully detecting COVID-19 from CT
images [6], coughs [27] and also other detection tasks such
as Alzheimer’s [65]. We have used the default Resnet50
structure mentioned in Table 1 of [63].

5
net(t, f ) = (x∗w)[t, f ] =

Long Short Term Memory (LSTM) Neural Network

A long short term memory (LSTM) model is a type of
recurrent neural network whose architecture allows it to
remember previously-seen inputs when making its classification decision [58]. It has been successfully used in
automatic cough detection [59], and also in other types of
acoustic event detection [60], [61].
~ is a constant d-dimensional vector, t ∈ R+ and ~s(t)
If φ
is the value of the d-dimensional state signal vector, then
the LSTM can be described by Equation 10.

i=1

where x is the input-vector, w is the weight-vector, b is the
bias and φ is the non-linear activation function. The weights
and the bias are optimised during supervised training.
During training, we have applied stochastic gradient descent with the inclusion of an l2 penalty. This penalty, along
with the number of hidden layers have been considered as
the hyperparameters which were tuned using the leave-pout cross validation process (Figure 8 and Section 5.2).

REDUCE NUMBER
OF DENSE LAYER
TO 8 AND
FINALLY TO 2

Fig. 7. LSTM classifier: Our LSTM classifier has β1 LSTM units, each
with rectified linear activation functions and a dropout rate of α3 . This
is followed by two dense layers with α4 and 8 units respectively and
rectified linear activation functions. The network is terminated by a
two-dimensional softmax where one output represents the COVID-19
positive class and the other Healthy or COVID-19 negative class. During
training, features are presented to the neural network in batches of size
ξ1 for ξ2 epochs.

4.5

A multilayer perceptron (MLP) is a neural network with
multiple layers of neurons separating input and output [50].
These models are capable of learning non-linear relationships and have for example been shown to be effective when
discriminating Influenza coughs from other coughs [51].
MLP have also been applied to tuberculosis coughs [48] and
to cough detection in general [52], [53]. The MLP classifier
is based on the computation in Equation 8.

0

5.1

C LASSIFICATION P ROCESS
Hyperparameter Optimisation

Both feature extraction and classifier architectures have a
number of hyperparameters that must be optimised. These
hyperparameters are listed in Tables 2 and 3.
As the sampling rate is 44.1 KHz for all audio; by
varying the frame lengths from 28 to 212 i.e. 256 to 4096,
features are extracted from frames whose duration varies
from approximately 5 to 100 ms. Different phases in a cough

6

TABLE 2
Feature extraction hyperparameters used in feature extraction
process, explained in Section 3

FULL DATASET OF N PATIENTS
NEXT TEST SET

Hyperparameter
No. of MFCCs
(MFCC=)
Frame length
(Frame=)
No. of Segments
(Seg=)

Description
Number of lower order
MFCCs to keep
Frame-size in which
audio is segmented
No. of segments in which
frames were grouped

OUTER LOOP
N - J PATIENTS (classifier performance)

J PATIENTS

Range
13 × k, where
k = 1, 2, 3, 4, 5
2k where
k = 8, · · · , 12
10 × k, where
k = 5, 7, 10, 12, 15

NEXT DEV SET

INNER LOOP
(hyperparameters)

K PATIENTS
TEST

N – J – K PATIENTS

TRAIN

DEV

carry important features [39] and thus has been segmented
into parts, as shown in Figure 5, which varies from 50 to 150
with steps of 20 to 30. By varying the number of lower order
MFCCs to keep (from 13 to 65, with steps of 13), the spectral
resolution of the features was varied.

EVALUATE

CHOOSE OPTIMUM
HYPERPARAMETERS

TABLE 3
Classifier hyperparameters, optimised using leave-p-out cross
validation shown in Figure 8 and explained in Section 5.2. For
regularisation strength (ν1 ) and l2 penalty (ζ1 ), i has the range −7 to 7
with steps of 1.

EVALUATE

Fig. 8. Leave p-out cross validation has been used to train and
evaluate the classifiers. The train and test split ratio has been 4 : 1.
Hyperparameters

Classifier

Range
10−7 to 107
in steps of 10i
0 to 1 in steps of 0.05
0 to 1 in steps of 0.05
10 to 100 in steps of 10
10−7 to 107
in steps of 10i

Regularisation
strength (ν1 )
l1 penalty (ν2 )
l2 penalty (ν3 )
No. of hidden layers (η )

LR
LR
MLP

l2 penalty (ζ1 )

MLP

Stochastic gradient
decent (ζ2 )
Batch Size (ξ1 )
No. of epochs (ξ2 )
No. of Conv filters (α1 )
Kernel size (α2 )
Dropout rate (α3 )
Dense layer size (α4 )
LSTM units (β1 )

MLP

0 to 1 in steps of 0.05

CNN, LSTM
CNN, LSTM
CNN
CNN
CNN, LSTM
CNN, LSTM
LSTM

Learning rate (β2 )

LSTM

2k where k = 6, 7, 8
10 to 20 in steps of 20
3 × 2k where k = 3, 4, 5
2 and 3
0.1 to 0.5 in steps of 0.2
2k where k = 4, 5
2k where k = 6, 7, 8
10k where
k = −2, −3, −4

5.2

LR

Cross Validation

All our classifiers have been trained and evaluated by using
a nested leave-p-out cross validation scheme, as shown in
Figure 8 [66]. Since only the Coswara dataset was used for
training and parameter optimisation, N = 1171. As the train
and test split is 4 : 1; J = 234 and K = 187.
The figure shows that, in an outer loop, J patients are
removed from the complete set N to be used for later
independent testing. Then, a further K patients are removed
from the remaining N − J to serve as a development set to
optimise the hyperparameters listed in Table 3. The inner
loop considers all such sets of K patients, and the optimal
hyperparameters are chosen on the basis of all these partitions. The resulting optimal hyperparameters are used to
train a final system on all N − J patients which is evaluated
on the test set consisting of J patients. This entire procedure
is repeated for all possible non-overlapping test sets in the

outer loop. Final performance is calculated by averaging
over these outer loops.
This cross-validation procedure makes best use of our
small dataset by allowing all patients to be used for both
training and testing purposes while ensuring unbiased hyperparameter optimisation and a strict per-patient separation between cross-validation folds.
5.3

Classifier Evaluation

Receiver operating characteristic (ROC) curves were calculated within the inner and outer loops in Figure 8. The area
under the ROC curve (AUC) indicates how well the classifier has performed over a range of decision thresholds [67].
From these ROC curves, the decision that achieves an equal
error rate (γEE ) was computed. This is the threshold for
which the difference between the classifier’s true positive
rate (TPR) and false positive rate (FPR) is minimised.
Denote the mean per-frame probability that a cough is
from a COVID-19 positive patient by P̂ :
K
P

P̂ =

P (Y = 1|X, θ)

i=1

(11)
κ
where κ indicates the number of frames in the cough and
P (Y = 1|X, θ) is the output of the classifier for input X
and parameters θ. Now define the indicator variable C as:
(
1 if P̂ ≥ γEE
C=
(12)
0 otherwise
We now define two COVID-19 index scores (COV ID I1
and COV ID I2 ) in Equations 13 and 14 respectively.
N
P1

COV ID I1 =

C

i=1

N1

(13)

7

N
P2

COV ID I2 =

P (Y = 1|X)

i=1

(14)
N2
In Equation 13, N1 is the number of coughs from the
patient in question while in Equation 14, N2 indicates the
total number of frames of cough audio gathered from the
patient. Hence Equation 11 computes a per-cough average
probability while and Equation 14 computes a per-frame
average probability.
The COVID-19 index scored given by Equations 13
and 14 can both be used to make classification decisions.
We have found that, for some classifier architectures one
will lead to better performance than the other. Therefore, we
have made the choice of the scoring function an additional
hyperparameter to be optimised during cross validation.
We have calculated the specificity and sensitivity from
these predicted values and then comparing them with the
actual values and finally AUC has been calculated and used
as a method of evaluation. These results are shown in Table
4 and 5.

6

R ESULTS

Classification performance for the Coswara datset is shown
in Table 4 and for the Sarcos dataset in Table 5. The Coswara
results are averages calculated over the outer loop test-sets
during cross validation. The Sarcos results, are classifiers
trained on the Coswara data and evaluated on the 21
patients in the Sarcos dataset. These tables also show the
optimal values of the hyperparameters determined during
cross-validation.

which achieved an AUC of 0.897. The optimised LR and
SVM classifiers showed substantially weaker performance,
with AUCs of 0.736 and 0.815 respectively.
We also see from Table 4 that using a larger number of
MFCCs consistently leads to improved performance. Since
the spectral resolution used to compute the 39-dimensional
MFCCs surpasses that of the human auditory system, we
conclude that the classifiers are using information not generally perceivable to the human listener in their decisions.
We have come to similar conclusions in previous work considering the coughing sounds of tuberculosis patients [14].
The mean ROC curves for the optimised classifier of each
architecture are shown in Figure 9. We see that LSTM, CNN
and Resnet50 classifiers achieve better performance that the
remaining architectures at most operating points. Furthermore, the figure confirms that the Resnet50 architecture also
in most cases achieved better classification performance that
the CNN and LSTM. There appears to be a small region
of the curve where the CNN outperforms the Resnet50
classifier, but this will need to be verified by future further
experimentation with larger dataset.
When the CNN, LSTM and Resnet50 classifiers trained
on the Coswara dataset (as shown in Table 4) were applied
to the Sarcos dataset, the performance shown in Table 5 is
achieved. We see that performance has in all cases deteriorated relative to the better-matched Coswara dataset. Best
performance was achieved by the LSTM classifier, which
achieved an AUC of 0.7786. Next, we improve this classifier
by applying feature selection.
6.1

Feature Selection

Sequential Forward Search (SFS) is a greedy search for
the individual features dimensions that contribute the most
towards the classifier performance [68]. The application of
SFS to the LSTM classifier allowed performance on the
Sarcos dataset to improve from an AUC of 0.779 to 0.938,
as shown in Figure 10.

Fig. 9. Mean ROC curve for the classifiers trained and evaluated on
the Coswara dataset: The highest AUC of 0.98 was obtained from the
Resnet50. LR classifier has the lowest AUC of 0.74.

Table 4 shows that the Resnet50 classifier exhibits best
performance, with an AUC of 0.976 when using a 117dimensional feature vector consisting of 39 MFCCs with
appended velocity and acceleration extracted from frames
that are 1024 samples long and when grouping the coughs
into 50 segments. The corresponding accuracy is 95.3%
with sensitivity 93% and specificity 98%. This exceeds the
minimum requirements for a community-based triage test
as determined by the WHO. The CNN and LSTM classifiers
also exhibited good performance, with AUCs of 0.953 and
0.942 respectively, thus comfortably outperformed the MLP,

Fig. 10. Sequential Forward Search when applied to a feature vector
composed of 13 MFCCs with appended velocity and acceleration, log
energies, ZCR and kurtosis. Peak performance is observed when using
the first 13 features.

The feature selection hyperparameters in these experiments were 13 MFCCs, 2048 samples (i.e. 0.46 sec) long
frames and coughs grouped the into 70 segments. Thus,
SFS could select from a total of 42 features: MFCCs along
with their velocity and accelerations, log energy, ZCR and

8

TABLE 4
Classifier performance while trained and evaluated on the Coswara dataset: The best-two performing neural network classifiers along with
their feature extraction hyperparameters after optimising classifier hyperparameters. Resnet50 has performed the best.
Classifiers

Features

LR
LR
SVM
SVM
MLP
MLP
CNN
CNN
LSTM
LSTM
Resnet50
Resnet50

MFCC=13, Frame=1024, Seg=120
MFCC=26, Frame=1024, Seg=70
MFCC=39, Frame=2048, Seg=100
MFCC=26, Frame=1024, Seg=50
MFCC=26, Frame=2048, Seg=100
MFCC=13, Frame=1024, Seg=100
MFCC=26, Frame=1024, Seg=70
MFCC=39, Frame=1024, Seg=50
MFCC=13, Frame=2048, Seg=70
MFCC=26, Frame=2048, Seg=100
MFCC=39, Frame=1024, Seg=50
MFCC=26, Frame=1024, Seg=70

Specificity
57%
59%
74%
74%
87%
84%
99%
98%
97%
97%
98%
98%

Performance
Sensitivity
Accuracy
94%
75.7%
74%
66.3%
71%
72.28%
74%
73.91%
88%
87.5%
68%
76.02%
90%
94.57%
90%
94.35%
91%
94.02%
90%
93.65%
93%
95.3%
93%
95.01%

AUC
0.7362
0.7288
0.8154
0.8044
0.8969
0.8329
0.9530
0.9499
0.9419
0.9319
0.9759
0.9632

TABLE 5
Best Classifier performance while trained on the Coswara dataset and evaluated on the Sarcos dataset: along with their feature extraction
hyperparameters after optimising classifier hyperparameters. The LSTM classifier has outperformed the other classifiers and after applying SFS, it
has achieved the AUC 0.9375. Only performance from deep architectures are shown here, as they are significantly better than other classifiers.
Classifiers

Features

CNN
LSTM
Resnet50
LSTM + SFS

MFCC=26, Frame=1024, Seg=70
MFCC=13, Frame=2048, Seg=70
MFCC=39, Frame=1024, Seg=50
MFCC=13, Frame=2048, Seg=70

Kurtosis. After performing SFS, a peak AUC of 0.9375 was
observed on the Sarcos dataset when using the best 13
features among the 42, as shown in Figure 11.

Fig. 11. Mean ROC curve for the best performing classifier in
Figure 9 when evaluated on the Sarcos dataset when using all 42
features and when using the best 13 features.

7

C ONCLUSION AND F UTURE W ORK

We have developed COVID-19 cough classifiers using
smartphone audio recordings and a number of machine
learning architectures. To train and evaluate these classifiers,
we have used two dataset. The first, larger, dataset is publicly available contains data from 1171 subjects (92 COVID19 positive and 1079 healthy) coming from all six continents
except Africa. The second, smaller, dataset contains 62%
of subjects from South Africa and data from 8 COVID-19
positive and 13 COVID-19 negative subjects. Thus, together
the two dataset include data from subjects residing on all six

Specificity
61%
73%
57%
96%

Performance
Sensitivity
Accuracy
85%
73.02%
75%
73.78%
93%
74.58%
91%
92.91%

AUC
0.755
0.7786
0.74
0.9375

continents. After preprocessing and extracting MFCC, frame
energy, ZCR and kurtosis features from the cough audio
recordings, we have trained and evaluated six classifiers
using a leave-p-out cross validation. Our best performing
classifier is based on the Resnet50 architecture and is able to
discriminate between COVID-19 coughs and healthy coughs
with an AUC of 0.98. The LSTM model performed the best
in discriminating COVID-19 positive coughs from COVID19 negative coughs with AUC 0.94 after determining the 13
best features using sequential forward search (SFS).
Although these systems require more stringent validation on larger dataset, the results we have presented are
very promising and indicate that COVID-19 screening based
on automatic classification of coughing sounds is viable.
Since the data has been captured on smartphones, and since
the classifier can in principle also be implemented on such
device, such cough classification is cost-efficient, easy to
apply and easy to deploy. It therefore has the potential of
being particularly useful in a practical developing-world
scenario.
In ongoing work, we are continuing to enlarge our
dataset, and to update our best systems as this happens.
We are also beginning to consider the best means of implementing the classifier on a readily-available consumer
smartphone platform.

ACKNOWLEDGEMENTS
We would like to thank South African Medical Research
Council (SAMRC) for providing funds to support this research and South African Centre for High Performance
Computing (CHPC) for providing computational resources
on their Lengau cluster for this research.

9

R EFERENCES
[1]
[2]

[3]

[4]
[5]
[6]
[7]

[8]
[9]

[10]
[11]
[12]
[13]

[14]

[15]

[16]
[17]

[18]

[19]

[20]

WHO et al., “Summary of probable sars cases with onset of
illness from 1 November 2002 to 31 July 2003,” http://www. who.
int/csr/sars/country/table2004 04 21/en/index. html, 2003.
R. Miyata, N. Tanuma, M. Hayashi, T. Imamura, J.-i. Takanashi,
R. Nagata, A. Okumura, H. Kashii, S. Tomita, S. Kumada et al.,
“Oxidative stress in patients with clinically mild encephalitis/encephalopathy with a reversible splenial lesion (mers),” Brain
and Development, vol. 34, no. 2, pp. 124–127, 2012.
D. Wang, B. Hu, C. Hu, F. Zhu, X. Liu, J. Zhang, B. Wang,
H. Xiang, Z. Cheng, Y. Xiong et al., “Clinical characteristics of
138 hospitalized patients with 2019 novel coronavirus–infected
pneumonia in Wuhan, China,” JAMA, vol. 323, no. 11, pp. 1061–
1069, 2020.
A. Carfı̀, R. Bernabei, F. Landi et al., “Persistent symptoms in
patients after acute COVID-19,” JAMA, vol. 324, no. 6, pp. 603–
605, 2020.
(2020, Nov.) COVID-19 dashboard by the center for systems
science and engineering (csse). John Hopkins University. [Online].
Available: https://coronavirus.jhu.edu/map.html
S. Walvekar, D. Shinde et al., “Detection of COVID-19 from CT
images using resnet50,” Detection of COVID-19 from CT Images
Using resnet50 (May 30, 2020), 2020.
H. Sotoudeh, M. Tabatabaei, B. Tasorian, K. Tavakol, E. Sotoudeh,
and A. L. Moini, “Artificial intelligence empowers radiologists to
differentiate pneumonia induced by COVID-19 versus influenza
viruses,” Acta Informatica Medica, vol. 28, no. 3, p. 190, 2020.
M. Yildirim and A. Cinar, “A deep learning based hybrid approach
for COVID-19 disease detections,” Traitement du Signal, vol. 37,
no. 3, pp. 461–468, 2020.
A. Chang, G. Redding, and M. Everard, “Chronic wet cough:
protracted bronchitis, chronic suppurative lung disease and
bronchiectasis,” Pediatric Pulmonology, vol. 43, no. 6, pp. 519–531,
2008.
T. Higenbottam, “Chronic cough and the cough reflex in common
lung diseases,” Pulmonary pharmacology & therapeutics, vol. 15,
no. 3, pp. 241–247, 2002.
K. F. Chung and I. D. Pavord, “Prevalence, pathogenesis, and
causes of chronic cough,” The Lancet, vol. 371, no. 9621, pp. 1364–
1374, 2008.
J. Korpáš, J. Sadloňová, and M. Vrabec, “Analysis of the cough
sound: an overview,” Pulmonary Pharmacology, vol. 9, no. 5-6, pp.
261–268, 1996.
J. Knocikova, J. Korpas, M. Vrabec, and M. Javorka, “Wavelet
analysis of voluntary cough sound in patients with respiratory
diseases,” Journal of Physiology and Pharmacology, vol. 59, no. Suppl
6, pp. 331–40, 2008.
G. Botha, G. Theron, R. Warren, M. Klopper, K. Dheda,
P. Van Helden, and T. Niesler, “Detection of tuberculosis by automatic cough sound analysis,” Physiological Measurement, vol. 39,
no. 4, p. 045005, 2018.
M. Al-khassaweneh and R. Bani Abdelrahman, “A signal processing approach for the diagnosis of asthma from cough sounds,”
Journal of Medical Engineering & Technology, vol. 37, no. 3, pp. 165–
171, 2013.
R. X. A. Pramono, S. A. Imtiaz, and E. Rodriguez-Villegas, “A
cough-based algorithm for automatic diagnosis of pertussis,” PloS
one, vol. 11, no. 9, p. e0162128, 2016.
A. Windmon, M. Minakshi, P. Bharti, S. Chellappan, M. Johansson,
B. A. Jenkins, and P. R. Athilingam, “Tussiswatch: A smart-phone
system to identify cough episodes as early symptoms of chronic
obstructive pulmonary disease and congestive heart failure,” IEEE
Journal of Biomedical and Health Informatics, vol. 23, no. 4, pp. 1566–
1573, 2018.
R. V. Sharan, U. R. Abeyratne, V. R. Swarnkar, and P. Porter, “Automatic croup diagnosis using cough sound recognition,” IEEE
Transactions on Biomedical Engineering, vol. 66, no. 2, pp. 485–495,
2018.
G. Rudraraju, S. Palreddy, B. Mamidgi, N. R. Sripada, Y. P. Sai,
N. K. Vodnala, and S. P. Haranath, “Cough sound analysis and
objective correlation with spirometry and clinical diagnosis,” Informatics in Medicine Unlocked, p. 100319, 2020.
G. Deshpande and B. Schuller, “An overview on audio, signal,
speech, & language processing for COVID-19,” arXiv preprint
arXiv:2005.08579, 2020.

[21] A. N. Belkacem, S. Ouhbi, A. Lakas, E. Benkhelifa, and C. Chen,
“End-to-end ai-based point-of-care diagnosis system for classifying respiratory illnesses and early detection of COVID-19,” arXiv
preprint arXiv:2006.15469, 2020.
[22] B. W. Schuller, D. M. Schuller, K. Qian, J. Liu, H. Zheng, and X. Li,
“COVID-19 and computer audition: An overview on what speech
& sound analysis could contribute in the sars-cov-2 corona crisis,”
arXiv preprint arXiv:2003.11117, 2020.
[23] C. Brown, J. Chauhan, A. Grammenos, J. Han, A. Hasthanasombat,
D. Spathis, T. Xia, P. Cicuta, and C. Mascolo, “Exploring automatic
diagnosis of COVID-19 from crowdsourced respiratory sound
data,” arXiv preprint arXiv:2006.05919, 2020.
[24] A. Imran, I. Posokhova, H. N. Qureshi, U. Masood, S. Riaz,
K. Ali, C. N. John, and M. Nabeel, “AI4COVID-19: AI enabled
preliminary diagnosis for COVID-19 from cough samples via an
app,” arXiv preprint arXiv:2004.01275, 2020.
[25] A. Pal and M. Sankarasubbu, “Pay attention to the cough:
Early diagnosis of COVID-19 using interpretable symptoms embeddings with cough sound signal processing,” arXiv preprint
arXiv:2010.02417, 2020.
[26] P. Bagad, A. Dalmia, J. Doshi, A. Nagrani, P. Bhamare, A. Mahale,
S. Rane, N. Agarwal, and R. Panicker, “Cough against COVID:
Evidence of COVID-19 signature in cough sounds,” arXiv preprint
arXiv:2009.08790, 2020.
[27] J. Laguarta, F. Hueto, and B. Subirana, “COVID-19 artificial intelligence diagnosis using only cough recordings,” IEEE Open Journal
of Engineering in Medicine and Biology, 2020.
[28] M. Cohen-McFarlane, R. Goubran, and F. Knoefel, “Novel coronavirus cough database: Nococoda,” IEEE Access, vol. 8, pp.
154 087–154 094, 2020.
[29] N. Sharma, P. Krishnan, R. Kumar, S. Ramoji, S. R. Chetupalli, P. K.
Ghosh, S. Ganapathy et al., “Coswara–a database of breathing,
cough, and voice sounds for COVID-19 diagnosis,” arXiv preprint
arXiv:2005.10548, 2020.
[30] J. Van Hulse, T. M. Khoshgoftaar, and A. Napolitano, “Experimental perspectives on learning from imbalanced data,” in Proceedings
of the 24th international conference on Machine learning, 2007, pp.
935–942.
[31] B. Krawczyk, “Learning from imbalanced data: open challenges
and future directions,” Progress in Artificial Intelligence, vol. 5, no. 4,
pp. 221–232, 2016.
[32] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,
“Smote: synthetic minority over-sampling technique,” Journal of
artificial intelligence research, vol. 16, pp. 321–357, 2002.
[33] G. Lemaı̂tre, F. Nogueira, and C. K. Aridas, “Imbalanced-learn:
A python toolbox to tackle the curse of imbalanced datasets in
machine learning,” The Journal of Machine Learning Research, vol. 18,
no. 1, pp. 559–563, 2017.
[34] L. L. Blagus, R., “Smote for high-dimensional class-imbalanced
data,” BMC Bioinformatics, vol. 14, p. 106, 2013.
[35] H. Han, W.-Y. Wang, and B.-H. Mao, “Borderline-smote: a new
over-sampling method in imbalanced data sets learning,” in International conference on intelligent computing. Springer, 2005, pp.
878–887.
[36] H. M. Nguyen, E. W. Cooper, and K. Kamei, “Borderline oversampling for imbalanced data classification,” International Journal
of Knowledge Engineering and Soft Data Paradigms, vol. 3, no. 1, pp.
4–21, 2011.
[37] H. He, Y. Bai, E. A. Garcia, and S. Li, “Adasyn: Adaptive synthetic
sampling approach for imbalanced learning,” in 2008 IEEE international joint conference on neural networks (IEEE world congress on
computational intelligence). IEEE, 2008, pp. 1322–1328.
[38] Wei Han, Cheong-Fat Chan, Chiu-Sing Choy, and Kong-Pang Pun,
“An efficient MFCC extraction method in speech recognition,” in
IEEE International Symposium on Circuits and Systems, 2006.
[39] H. Chatrzarrin, A. Arcelus, R. Goubran, and F. Knoefel, “Feature
extraction for the differentiation of dry and wet cough sounds,” in
IEEE International Symposium on Medical Measurements and Applications. IEEE, 2011.
[40] S. Aydın, H. M. Saraoğlu, and S. Kara, “Log energy entropy-based
eeg classification with multilayer neural networks in seizure,”
Annals of biomedical engineering, vol. 37, no. 12, p. 2626, 2009.
[41] R. Bachu, S. Kopparthi, B. Adapa, and B. D. Barkana,
“Voiced/unvoiced decision for speech signals based on zerocrossing rate and energy,” in Advanced techniques in computing
sciences and software engineering. Springer, 2010, pp. 279–282.

10

[42] L. T. DeCarlo, “On the meaning and use of kurtosis.” Psychological
methods, vol. 2, no. 3, p. 292, 1997.
[43] E. Christodoulou, J. Ma, G. S. Collins, E. W. Steyerberg, J. Y.
Verbakel, and B. Van Calster, “A systematic review shows no
performance benefit of machine learning over logistic regression
for clinical prediction models,” Journal of Clinical Epidemiology, vol.
110, pp. 12–22, 2019.
[44] S. Le Cessie and J. C. Van Houwelingen, “Ridge estimators in
logistic regression,” Journal of the Royal Statistical Society: Series C
(Applied Statistics), vol. 41, no. 1, pp. 191–201, 1992.
[45] Y. Tsuruoka, J. Tsujii, and S. Ananiadou, “Stochastic gradient
descent training for l1-regularized log-linear models with cumulative penalty,” in Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, 2009, pp. 477–485.
[46] H. Yamashita and H. Yabe, “An interior point method with a
primal-dual quadratic barrier penalty function for nonlinear optimization,” SIAM Journal on Optimization, vol. 14, no. 2, pp. 479–
499, 2003.
[47] V. Bhateja, A. Taquee, and D. K. Sharma, “Pre-processing and
classification of cough sounds in noisy environment using svm,”
in 2019 4th International Conference on Information Systems and
Computer Networks (ISCON). IEEE, 2019, pp. 822–826.
[48] B. H. Tracey, G. Comina, S. Larson, M. Bravard, J. W. López,
and R. H. Gilman, “Cough detection algorithm for monitoring
patient recovery from pulmonary tuberculosis,” in 2011 Annual
international conference of the IEEE engineering in medicine and biology
society. IEEE, 2011, pp. 6017–6020.
[49] R. V. Sharan, U. R. Abeyratne, V. R. Swarnkar, and P. Porter,
“Cough sound analysis for diagnosing croup in pediatric patients
using biologically inspired features,” in 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology
Society (EMBC). IEEE, 2017, pp. 4578–4581.
[50] H. Taud and J. Mas, “Multilayer perceptron (mlp),” in Geomatic
Approaches for Modeling Land Change Scenarios. Springer, 2018, pp.
451–455.
[51] L. Sarangi, M. N. Mohanty, and S. Pattanayak, “Design of mlp
based model for analysis of patient suffering from influenza,”
Procedia Computer Science, vol. 92, pp. 396–403, 2016.
[52] J.-M. Liu, M. You, Z. Wang, G.-Z. Li, X. Xu, and Z. Qiu, “Cough
detection using deep neural networks,” in 2014 IEEE International
Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 2014,
pp. 560–563.
[53] J. Amoh and K. Odame, “Deepcough: A deep convolutional neural
network in a wearable cough detection system,” in 2015 IEEE
Biomedical Circuits and Systems Conference (BioCAS). IEEE, 2015,
pp. 1–4.
[54] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” Communications
of the ACM, vol. 60, no. 6, pp. 84–90, 2017.
[55] S. Lawrence, C. L. Giles, A. C. Tsoi, and A. D. Back, “Face recognition: A convolutional neural-network approach,” IEEE transactions
on neural networks, vol. 8, no. 1, pp. 98–113, 1997.
[56] S. Albawi, T. A. Mohammed, and S. Al-Zawi, “Understanding of a
convolutional neural network,” in 2017 International Conference on
Engineering and Technology (ICET). IEEE, 2017, pp. 1–6.
[57] X. Qi, T. Wang, and J. Liu, “Comparison of support vector machine
and softmax classifiers in computer vision,” in 2017 Second International Conference on Mechanical, Control and Computer Engineering
(ICMCCE). IEEE, 2017, pp. 151–155.
[58] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[59] I. D. Miranda, A. H. Diacon, and T. R. Niesler, “A comparative
study of features for acoustic cough detection using deep architectures,” in 2019 41st Annual International Conference of the IEEE
Engineering in Medicine and Biology Society (EMBC). IEEE, 2019,
pp. 2601–2605.
[60] E. Marchi, F. Vesperini, F. Weninger, F. Eyben, S. Squartini, and
B. Schuller, “Non-linear prediction with lstm recurrent neural
networks for acoustic novelty detection,” in 2015 International Joint
Conference on Neural Networks (IJCNN). IEEE, 2015, pp. 1–7.
[61] J. Amoh and K. Odame, “Deep neural networks for identifying
cough sounds,” IEEE transactions on biomedical circuits and systems,
vol. 10, no. 5, pp. 1003–1011, 2016.
[62] A. Sherstinsky, “Fundamentals of recurrent neural network (rnn)
and long short-term memory (lstm) network,” Physica D: Nonlinear
Phenomena, vol. 404, p. 132306, 2020.

[63] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2016, pp. 770–778.
[64] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision. Springer, 2014,
pp. 740–755.
[65] J. Laguarta, F. Hueto, P. Rajasekaran, S. Sarma, and B. Subirana,
“Longitudinal speech biomarkers for automated alzheimer’s detection,” 2020.
[66] S. Liu, “Leave-p-out cross-validation test for uncertain verhulstpearl model with imprecise observations,” IEEE Access, vol. 7, pp.
131 705–131 709, 2019.
[67] T. Fawcett, “An introduction to ROC analysis,” Pattern Recognition
Letters, vol. 27, no. 8, pp. 861–874, 2006.
[68] P. A. Devijver and J. Kittler, Pattern recognition: A statistical approach. Prentice Hall, 1982.

Madhurananda Pahar received his BSc in
Mathematics from University of Calcutta, India;
MSc in Computing for Financial Markets & PhD
in Computational Neuroscience from University
of Stirling, Scotland. Currently he is working as
a post-doctoral fellow in the University of Stellenbosch, South Africa. His research interests are
in machine learning and signal processing for
audio signals and smart sensors in bio-medicine
such as detection and classification of TB and
COVID-19 coughs in real-world environment.

Marisa Klopper is a researcher at the Division
of Molecular Biology and Human Genetics of
Stellenbosch University, South Africa. She holds
a PhD in Molecular Biology from Stellenbosch
University and her research interest is in TB
and drug-resistant TB diagnosis, epidemiology
and physiology. She has been involved in cough
classification for the last 6 years, with application
to TB and more recently COVID-19.

Robin Warren is the Unit Director of the South
African Medical Research Council’s Centre for
Tuberculosis Research and Distinguished Professor at Stellenbosch University. He has a B2
rating by the National Research Council (NRF)
and is a core member of the DSI-NRF Centre
of Excellence for Biomedical Tuberculosis Research and head the TB Genomics research
thrust. He has published over 320 papers in the
field of TB and have an average H-index of 65.

Thomas Niesler obtained the B.Eng (1991) and
M.Eng (1993) degrees in Electronic Engineering
from the University of Stellenbosch, South Africa
and a Ph.D. from the University of Cambridge,
England, in 1998. He joined the Department
of Engineering, University of Cambridge, as a
lecturer in 1998 and subsequently the Department of Electrical and Electronic Engineering,
University of Stellenbosch, in 2000, where he
has been Professor since 2012. His research
interests lie in the areas of signal processing,
pattern recognition and machine learning.

