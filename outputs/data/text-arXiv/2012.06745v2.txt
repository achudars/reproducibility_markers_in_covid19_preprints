Proceedings of Machine Learning Research vol 107:1–25, 2021

Optimal Policies for a Pandemic: A Stochastic Game Approach and a
Deep Learning Algorithm
Yao Xuan
Robert Balkin

YXUAN @ MATH . UCSB . EDU
RBALKIN @ UCSB . EDU

arXiv:2012.06745v2 [math.OC] 8 Mar 2021

Department of Mathematics, University of California, Santa Barbara, CA 93106-3080, USA

Jiequn Han

JIEQUNH @ PRINCETON . EDU

Department of Mathematics, Princeton University, Princeton, NJ 08544-1000, USA

Ruimeng Hu

RHU @ UCSB . EDU
Department of Mathematics and Department of Statistics and Applied Probability, University of California,
Santa Barbara, CA 93106-3080, USA

Hector D. Ceniceros

CENICEROS @ UCSB . EDU

Department of Mathematics, University of California, Santa Barbara, CA 93106-3080, USA

Abstract
Game theory has been an effective tool in the control of disease spread and in suggesting optimal
policies at both individual and area levels. In this paper, we propose a multi-region SEIR model
based on stochastic differential game theory, aiming to formulate optimal regional policies for
infectious diseases. Specifically, we enhance the standard epidemic SEIR model by taking into
account the social and health policies issued by multiple region planners. This enhancement makes
the model more realistic and powerful. However, it also introduces a formidable computational
challenge due to the high dimensionality of the solution space brought by the presence of multiple
regions. This significant numerical difficulty of the model structure motivates us to generalize the
deep fictitious algorithm introduced in [Han and Hu, MSML2020, pp.221–245, PMLR, 2020] and
develop an improved algorithm to overcome the curse of dimensionality. We apply the proposed
model and algorithm to study the COVID-19 pandemic in three states: New York, New Jersey and
Pennsylvania. The model parameters are estimated from real data posted by the Centers for Disease
Control and Prevention (CDC). We are able to show the effects of the lockdown/travel ban policy
on the spread of COVID-19 for each state and how their policies affect each other.
Keywords: Stochastic differential game, pandemic, optimal policy, enhanced deep fictitious play

1. Introduction
The pandemic of coronavirus disease 2019 (COVID-19) has brought a huge impact on our lives.
Based on the CDC Data Tracker, as of early December 2020, there have been more than 15 million confirmed cases of infection and more than 290 thousand cases of death in the United States.
Needless to say, the economic impact has also been catastrophic, resulting in unprecedented unemployment and the bankruptcy of many restaurants, recreation centers, shopping malls, etc.
In a classic, compartmental epidemiological model each individual is assigned a label, e.g.,
Susceptible, Exposed, Infectious, Removed, Vaccinated. The labels’ order shows the flow patterns between the compartments (SIR, SEIR, SIRV models). Other approaches include network
models, which explicitly include the interaction of individuals, in addition to the modeling of each

© 2021 Y. Xuan, R. Balkin, J. Han, R. Hu & H.D. Ceniceros.

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

individual’s dynamics, and agent-based models that are useful in informing decision making when
accurately calibrated. Moreover, the consideration of pharmaceutical and/or non-pharmaceutical
intervention policies naturally couples game theory to epidemiological models by controlling when
and how the game is played in such models. For example, in some early studies Bauch et al. (2003);
Bauch and Earn (2004), one can use non-repeated games to incorporate game theory into modeling
at the individual level, where individuals (known as “players” in the game theory) maximize their
gain by weighing the costs and benefits of different strategies. We refer to the review paper Chang
et al. (2020) and the references therein for more details.
Differential games, initiated by Isaacs (1965), as an offspring of game theory and optimal control, provide modeling and analysis of conflict in the context of a dynamical system. They have
been intensively employed across many disciplines, including management science, economics, social science, biology, military, etc. One of the core objectives in differential games is to compute
Nash equilibria that refer to strategies by which no player has an incentive to deviate. However, a
major bottleneck comes from the notorious intractability of N -player games, and the direct computation of Nash equilibria is extremely time-consuming and memory demanding. In a series of
recent works by Hu (2020); Han and Hu (2020); Han et al. (2020a,b), the deep fictitious play (DFP)
theory and algorithms were developed for stochastic differential games (SDG) with a large number of heterogeneous players. The DFP framework embeds the fictitious play idea, introduced by
Brown (1949, 1951), into designed architectures of deep neural networks to produce accurate and
parallelizable algorithms with convergence analysis, and resolve the intractability issue (curse of
dimensionality) caused by the complex modeling and underlying high-dimensional space in SDG.
Building from the DFP theory and algorithms for computing Nash equilibria in SDG, we propose here to strengthen the classical SEIR model by taking into account the social and health policies
issued by multiple region planners. We call this new model a stochastic multi-region SEIR model
because it couples the stochastic differential game theory with the SEIR model, making it more
realistic and powerful. The computational challenge introduced by the high-dimensionality of the
multi-region solution space is addressed by generalizing the deep fictitious algorithm proposed by
Han and Hu (2020). This new approach leads to an enhanced deep fictitious play algorithm to overcome the curse of dimensionality and further reduce the computational complexity. To showcase the
performance of the proposed model and algorithm, we apply them to a case study of the COVID-19
pandemic in three states: New York (NY), New Jersey (NJ), and Pennsylvania (PA). We present the
optimal lockdown policy corresponding to the Nash equilibrium of the multi-region SEIR model.
We remark that our work is not to predict a pandemic, but to provide a game-themed framework,
a deep learning algorithm and possible outcomes for competitive region planners. We hope that
information can provide some qualitative guidance for policymakers on the impact of certain policies. The parameters used in the numerical experiments are based on the current knowledge of the
Coronavirus, which is still under development. In practice, at the beginning of the Coronavirus, a
governor might not be able to trace the infections fully, and infected cases may not be fully identified. All these may lead to the inaccuracy of parameter estimations despite our using of the best
available data. Therefore, it may be hard to match the predicted results with practical observation.
The rest of the paper is organized as follows. In Section 2, we propose a novel multi-region
epidemiological model and explain how social and health policies issued by region planners are
integrated into dynamics, leading to a game feature of the problem. We explain the numerical
challenge in Section 3 and propose an enhanced deep fictitious play algorithm for memory and
computational efficiency. Section 4 focuses on a NY-NJ-PA case study with detailed discussions
2

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

on parameter choices and the resulted optimal policies. We present some concluding remarks in
Section 5 and provide technical details in the appendices.

2. Mathematical modeling: A multi-region SEIR model
We consider a pandemic spreading in N geographical regions, and each planner controls the loss of
her region by implementing some policies. We aim to study how the region planners’ policies affect
each other, and the equilibrium policies.
Let us start with a modified version of the very-known epidemic SEIR model (cf. Liu et al.
(1987)), where each region’s population is assigned to compartments with four labels: Susceptible,
Exposed, Infectious, and Removed. Individuals with different labels denote S: those who are not
yet infected; E: who have been infected but are not yet infectious themselves; I: who have been
infected and are capable of spreading the disease to those in the susceptible category, and R: who
have been infected and then removed from the disease due to recovery or death. The region planners
can issue certain policies to mitigate the pandemic, for instance, policies that can help reduce the
transmission rates and death rates. Mathematically, denote by Stn , Etn , Itn , Rtn the proportion of
population in the four compartments of region n at time t. We consider the following stochastic
multi-region SEIR model:
dStn = −

N
X

β nk Stn Itk (1 − θ`nt )(1 − θ`kt ) dt − v(hnt )Stn dt − σsn Stn dWtsn ,

(1)

k=1

dEtn =

N
X

β nk Stn Itk (1 − θ`nt )(1 − θ`kt ) dt − γEtn dt + σsn Stn dWtsn − σen Etn dWten ,

k=1
n
dIt = (γEtn − λ(hnt )Itn ) dt + σen Etn dWten ,
dRtn = λ(hnt )Itn dt + v(hnt )Stn dt, n ∈ N :=

(2)
{1, 2, . . . , N },

N
1
where `t ≡ (`1t , . . . , `N
t ) and ht ≡ (ht , . . . , ht ) are policies chosen by the region planners at time
t. Each planner n seeks to minimize its region’s cost within a period [0, T ]:
Z T

 n

n
−rt n
n
n n
n
n
−rt
n 2
J (`, h) := E
e P (St + Et + It )`t w + a(κIt χ + pIt c) + e η(ht ) dt . (3)
0

We now give detailed description of this model (1)–(3):
S: β nk denotes the average number of contacts per person per time. The transition rate between
S n and E n due to contacting infectious people in the region k is proportional to the fraction of those contacts between an infectious and a susceptible individual, which result in the
susceptible one becoming infected, i.e., βStn Itk . Although some regions may not be geographically connected, the transmission between the two is still possible due to air travels but
is less intensive than the transmission within the region, i.e., β nk > 0 and β nn  β nk for all
k 6= n.
`nt ∈ [0, 1] denotes the decision of the planner n on the fraction of population being locked
down at time t. We assume that those in lockdown cannot be infected. However, the policy
may only be partially effective as essential activities (food production and distribution, health,
and basic services) have to continue. Here we use θ ∈ [0, 1] to measure this effectiveness,
3

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

and the transition rate under the policy ` thus become β nk Stn Itk (1 − θ`nt )(1 − θ`kt ). The case
θ = 1 means the policy is fully effective.
hnt ∈ [0, 1] denotes the effort the planner n decide to put into the health system, which we
refer as health policy. It will influence the vaccination availability v(·) and the recovery rate
λ(·) of this model.
v(hnt ) denotes the vaccination availability of region n at time t. Once vaccinated, the susceptible individuals v(hnt )Stn become immune to the disease, and join the removed category Rtn .
We model it as an increasing function of hnt , and if the vaccine has not been developed yet,
we can define v(x) = 0 for x ≤ h.
E: γ describes the latent period when the person has been infected but not infectious yet. It is
the inverse of the average latent time, and we assume γ to be identical across all regions. The
transition between E n and I n is proportional to the fraction of exposed, i.e., γEtn .
I: λ(·) represents the recovery rate. For the infected individuals, a fraction λ(hn )I n (including
both death and recovery from the infection) joins the removed category Rn per time unit. The
rate is determined by the average duration of infection D. We model the duration (so does the
recovery rate) related to the health policy hnt decided by its planner. The more effort put into
the region (i.e., expanding hospital capacity, creating more drive-thru testing sites), the more
clinical resources the region will have and the more resources will be accessible by patients,
which could accelerate the recovery and slow down death. The death rate, denoted by κ(·), is
crucial for computing the cost of the region n; see the next item.
Cost: Each region planner faces four types of cost. One is the economic activity loss due to the
lockdown policy, where w is the productivity rate per individual, and P n is the population of
the region n. The second one is due to the death of infected individuals. Here κ is the death
rate which we assume for simplicity to be constant, and χ denotes the cost of each death. The
hyperparameter a describes how planners weigh deaths and infections comparing to other
costs. The third one is the inpatient cost, where p is the hospitalization rate, and c is the cost
per inpatient day. The last term η(hnt )2 is the grants putting into the health system. We choose
a quadratic form to account for diminishing marginal utility (view it from η(hnt )2 to hnt ). All
costs are discounted by an exponential function e−rt , where r is the risk-free interest rate,
to take into account the time preference. Note that region n’s cost depends on all regions’
policies (`, h), as {I k , k 6= n} appearing in the dynamics of S n . Thus we write is J n (`, h).
The choices of epidemiological parameters will be discussed in Section 4.1. Next, we summarize
the key assumptions in the above model:
1. The dynamics of an epidemic are much faster than the vital (birth and death) dynamics. So
vital dynamics are omitted in the above model.
2. The planning is of a short horizon and will be adjusted frequently as the epidemic develop.
For simplicity, we assume this is no migration between regions over the time [0, T ].
3. Individuals who once recovered from the disease, are immune and free of lockdown policy.
4. The dynamics obeys the conservation law: Stn + Etn + Itn + Rtn = P n . This means that the
process Rn is redundant.
4

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

5. The dynamics of S, E and I are subjected to random noise, to account for the noise introduced
during data recording, false-positive/negative test results, exceptional cases when recovered
individuals become susceptible again, minor individual differences in the latent period, etc.
6. Individuals who are not under lockdown have the same productivity, no matter their categories. We assume this for simplicity remark that this can be improved by assigning different
productivity to individuals with or without symptoms.
The above modeling and objectives can be viewed as a stochastic differential game between
N players1 . Here, we view the whole problem as a non-cooperative game, as many regions make
decisions individually and indeed even compete for scarce resources (frontline workers, personal
protective equipment, etc.) during the outbreak. Each player n controls her states (S n , E n , I n , Rn )
through her strategy (`n , hn ) in order to minimize the associated cost J n . The optimizers then are
interpreted as the optimal lockdown policy and optimal effort putting into the health system.
For a non-cooperative game, one usually refers to Nash equilibrium as a notion of optimality.
For completeness, we review the definition here.
Definition 2.1 A Nash equilibrium is a tuple (`∗ , h∗ ) = (`1,∗ , h1,∗ , . . . , `N,∗ , hN,∗ ) ∈ AN such that
∀n ∈ N , and (`n , hn ) ∈ A,

J n (`∗ , h∗ ) ≤ J n ((`−n,∗ , `n ), (h−n,∗ , hn )),

where `−n,∗ represents strategies of players other than the n-th one:
`−n,∗ := [`1,∗ , . . . , `n−1,∗ , `n+1,∗ , . . . , `N,∗ ] ∈ AN −1 ,
A denotes the set of admissible strategies for each player and AN is the produce of N copies of A.
For simplicity, we have assumed all players taking actions in the same space.
In the sequel, to fix the notations, we shall use
• a regular character with a superscript n for an object from player n;
• a boldface character for a collection of objects from all players, i.e., St ≡ [St1 , . . . , StN ]T ;
• a boldface character with a superscript −n for a collection of objects from all players except
n, i.e., St−n ≡ [St1 , . . . , Stn−1 , Stn+1 , . . . , StN ]T .
A Markovian Nash equilibrium is a Nash equilibrium defined above with A being the set of
Borel measurable functions: (`, h) : [0, T ] × R3N → [0, 1]2 . In other words, the policies (`nt , hnt ) at
time t are functions of the time t and the current values of all players’ state processes (St , Et , It ).
We omit the dependence on Rt as it is redundant as a consequence of the conservation law.
We derive below the Hamilton-Jacobi-Bellman (HJB) equations characterizing the Markovian
Nash equilibrium. To simplify the notation, we first rewrite the dynamics of (St , Et , It ) defined in
(1)–(2) into a vector form Xt ≡ [St , Et , It ]T ≡ [St1 , · · · , StN , Et1 , · · · , EtN , It1 , · · · , ItN ]T ∈ R3N .
Again, we shall drop the redundant process Rt . The dynamics of Xt reads:
dXt = b(t, Xt , `(t, Xt ), h(t, Xt )) dt + Σ(Xt ) dWt ,
1. Henceforth, we shall use planner and player interchangeably

5

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

where b, Σ are deterministic functions in R3N and R3N ×2N , and {Wt }0≤t≤T is a 2N -dimensional
standard Brownian motion. Each player n aims to minimize the expected running cost
Z T

n
n
n
E
f (t, Xt , ` (t, Xt ), h (t, Xt )) dt .
(4)
0

We defer the specific definitions of b, Σ, W , and f n to Appendix A.1 to facilitate the exposition.
We now define the value function of player n by
Z T

n
n
n
n
V (t, x) = inf E
f (s, Xs , ` (s, Xs ), h (s, Xs )) ds|Xt = x .
(`n ,hn )∈A

t

By dynamic programming, it solves the following HJB system

1
∂t V n +
inf
H n (t, x, (`, h)(t, x), ∇x V n ) + Tr(Σ(x)T Hessx V n Σ(x)) = 0,
n
n
2
2
(` ,h )∈[0,1]
 n
V (T, x) = 0, n ∈ N ,

(5)

where H n is the usual Hamiltonian defined by
H n (t, x, `, h, p) = b(t, x, `, h) · p + f n (t, x, `n , hn ),

(6)

∂t denotes the time derivative, ∇x V and Hessx V denote the gradient and the Hessian of the function
V with respect to x, respectively, and Tr stands for the trace of a matrix.
Finding the optimal policies for N regions is equivalent to solving N -coupled 3N + 1 dimensional nonlinear equations (5). For example, when N = 3, each PDE is 10-dimensional and
conventional methods start to lose their efficiency. The recently proposed deep learning algorithm
in Han and Hu (2020), known as deep fictitious play (DFP), has shown excellent numerical performance in solving high-dimensional, coupled HJB equations with convergence analysis (Han et al.,
2020a). In the next section, we will first review and then propose an enhanced version of DFP to
tackle some new issues.

3. Numerical methodology: Enhanced deep fictitious algorithm
We first briefly review the deep fictitious play (DFP) algorithm for solving equation (5) and we refer
readers to Han and Hu (2020) for full details. With the idea of fictitious play, DFP recasts the N player game into N decoupled optimization problems, which are solved repeatedly stage by stage.
Each individual problem is solved by the deep BSDE method (E et al., 2017; Han et al., 2018). The
algorithm starts with some initial guess (`0 , h0 ), where the superscript 0 stands for stage 0. At the
(m + 1)th stage, given the optimal policies (`m , hm ) at the previous stage, the algorithm solves the
following PDEs


∂t V n,m+1 +
inf
H n (t, x, (`n , `−n,m , hn , h−n,m )(t, x), ∇x V n,m+1 )

n ,hn )∈[0,1]2

(`


1
(7)
+ Tr(Σ(x)T Hessx V n,m+1 Σ(x)) = 0,


2


V n,m+1 (T, x) = 0, n ∈ N ,

6

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

and obtains the (m + 1)th stage’s optimal strategy by:
(`n,m+1 , hn,m+1 )(t, x) =

arg min H n (t, x, (`n , `−n,m , hn , h−n,m )(t, x), ∇x V n,m+1 (t, x)).
(`n ,hn )∈[0,1]2

(8)
Here, (`−n,m , h−n,m ) stands for others’ optimal policies from the mth stage and are considered to
be fixed functions when solving the PDE at the current stage. In the sequel, to simplify notations
we omit the stage label m in the superscript when there is no risk of confusion. To solve (7) at each
stage, it is first rewritten in the DFP as
1
∂t V n + Tr(Σ(x)T Hessx V n Σ(x)) + µn (t, x; `−n , h−n ) · ∇x V n
2
+ g n (t, x, Σ(x)T ∇x V n ; `−n , h−n ) = 0,

(9)

with some functions µn and g n . The solution is then approximated by solving the equivalent BSDE
(Xtn , Ytn , Ztn ) ∈ R3N × R × R2N :

Z t
Z t

n
n
n
−n
−n
n


(10)
X
=
x
+
µ
(s,
X
;
(`
,
h
)(s,
X
))
ds
+
Σ(Xsn ) dWs ,
0
 t
s
s
0
0
Z T
Z T


n
n
n
−n
−n
n
n

(Zsn )T dWs ,
(11)
g (s, Xs , Zs ; (` , h )(s, Xs )) ds −
Yt =
t

t

in the sense of (cf. Pardoux and Peng (1992); El Karoui et al. (1997); Pardoux and Tang (1999))
Ytn = V n (t, Xtn )

and

Ztn = Σ(Xtn )T ∇x V n (t, Xtn ).

The high-dimensional BSDE (10)–(11) is tackled by the deep BSDE method proposed in E et al.
(2017); Han et al. (2018).
In Han and Hu (2020), the algorithm solves the BSDE by parametrizing V n (t, x) using neural
networks (NN) and then obtains the approximate optimal policy by plugging the NN outputs into
(8). For memory efficiency, the algorithm only stores the NNs’ parameters at the current and the
one-step previous stages. This strategy works well for games like the linear-quadratic game, but it
would be ineffective if `−n or h−n explicitly appears in the minimizer in (8). In this case, when
evaluating others’ strategy `−n or h−n at stage m, it does not only need NNs at stage m but also
at stages m − 1, m − 2, ..., 0. This means one needs to store NNs’ parameters for all the previous stages from 1, . . . , m, and evaluate the associated output. Therefore, the time complexity of
evaluating (`−n , h−n )(s, Xsn ) up to stage m is O(m2 ) and the memory complexity is O(m). This
is infeasible in practice, as for real problems it hundreds of stages are needed. To overcome this
significant problem, we propose an enhanced version of the original algorithm which reduces the
time complexity to O(m) and the memory complexity to O(1). We present this new, enhanced
algorithm (with pseudocode).
3.1. Algorithm
In order to reduce the computational complexity of evaluating (`−n , h−n )(s, Xsn ) in the situation when `−n or h−n explicitly appears in the minimizer in (8), we propose the Enhanced Deep
Fictitious Play which parametrizes both V n (t, x) and policy (`n , hn )(t, x) by NNs. For simplicity, we state the algorithm based on a generic stochastic differential game, where (possibly highdimensional) controls are denoted by αn (t, x) for player n.
7

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

In each stage of the Enhanced Deep Fictitious Play, for each planner n, the loss that our algorithm aims to minimize consists of two parts: (1) the loss related to solving (10)–(11) and (2)
the error of approximating the optimal strategy αn within some hypothesis spaces. The resulted
approximation α̃n will be used in the next stage of fictitious play:
Z T
n 2
kαn (s, Xsn ) − α̃n (s, Xsn )k22 ds)
infn
E(|YT | + τ
n
Y0 ,α̃n ,{Zt }0≤t≤T

s.t.

Xtn
Ytn

0

t

Z

(s, Xsn ; α̃−n (s, Xsn )) ds

Z

t

Σ(Xsn ) dWs ,
0
0
Z t
Z t
n
n
n
n
−n
n
= Y0 −
g (s, Xs , Zs ; α̃ (s, Xs )) ds +
(Zsn )T dWs ,
µ

= x0 +

n

+

0

(12)

0

αn (s, Xsn ) = arg min H n (s, Xsn , (β n , α̃−n )(s, Xsn ), Zsn ), 2
βn

where k · k2 denotes the 2-norm, α̃−n denotes the collection of approximated optimal controls from
the previous stage except player n, and τ is a hyperparameter denoting the weight between two
terms in the loss function. As detailed in Section 3.2, the hypothesis space for which we search α̃n
is characterized by another NN, in addition to the one to approximate Y0 and {Ztn }0≤t≤T . Although
representing α̃n with a neural network introduces approximation errors, it allows us to efficiently
access the proxy of the optimal strategy α−n in the last stage by calling corresponding networks,
instead of storing and calling all the previous strategies α−n,m−1 , . . . , α−n,1 due to the recursive
dependence.
Numerically we solve a discretized version of (12). Given a partition π of size NT on the time
interval [0, T ], 0 <= t0 < t1 < .... < tNT = T , the algorithm reads (to ease the notation, we
replace the subscript tk by k):
X n,π
2
(13)
αk − α̃kn,π (Xkn,π ) 2 ∆tk }
inf
E{|YTn,π |2 + τ
N
−1
0
00
T
ψ0 ∈N0n ,{φk ∈Nkn ,ξk ∈Nkn }k=0
k

s.t. X0n,π = X0 , Y0n,π = ψ0 (X0n,π ) , Zkn,π = φk Xkn,π , α̃kn,π (Xkn,π ) = ξk (Xkn,π ),
αkn,π = arg min H n (tk , Xkn,π , (β n , α̃−n,π
)(Xkn,π ), Zkn,π ),
k
βn

k = 0, . . . , NT − 1




n,π
n,π
)
∆tk + Σ tk , Xkn,π ∆Wk ,
(X
Xk+1
= Xkn,π + µn tk , Xkn,π ; α̃−n,π
k
k
n,π
n,π 
n,π T
Yk+1
= Ykn,π − g n tk , Xkn,π , Zkn,π ; α̃n,π
∆Wk ,
k (Xk ) ∆tk + Zk
0

00

(14)
(15)

n NT −1
T −1
where ∆tk = tk+1 − tk , ∆Wk = Wtk+1 − Wtk , and N0n , {Nkn }N
k=0 , {Nk }k=0 are hypothesis
spaces for player n, which will be specified later through neural network structures.
The expectation in (13) is further approximated by Monte Carlo samples of (14)-(15). The parameters in the hypothesis spaces are determined by stochastic gradient descent (SGD) algorithms
such that the approximated expectation is minimized, which in turn gives the optimal deterministic
functions (ψ0∗ , φ∗k , ξk∗ ). We expect that (ψ0∗ , φ∗k , ξk∗ ) will approximate (V n , ∇x V n , αn ) well when
T −1
this proxy of (13) is small. Particularly, {ξk∗ }N
k=0 serves as an efficient tool to evaluate the optimal
policy at the current stage for finding Nash equilibrium. Implementation details and the full algorithm are presented in Section 3.2. Note that when τ = 0 and α̃ are replaced by α in the above

2. Here we have assumed that the Hamiltonian H n depends on ∇x V through ΣT ∇x V .

8

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

algorithm, the Enhanced Deep Fictitious Play degenerates to the Deep Fictitious Play proposed in
Han and Hu (2020).
3.2. Implementation
Here we provide some detail to implement the methodology in Section 3.1. First, we specify the hy0
n00 NT −1
n
n
n
T −1
pothesis spaces for neural networks N0n ,{Nkn }N
k=0 , {Nk }k=0 , corresponding to V , ∇x V , α
n
(the superscript m is dropped again for simplicity). V (t, x) is parametrized directly by a neural
network NN(t, x) . Corresponding map Σ(X)T ∇x V n (t, X) that defines Ztn in the optimization
problem (12) could be parametrized by Σ(x)∇x NN(t, x). Naturally, Σ(x)∇x NN(tk , x) is a hy0
0
pothesis function in Nkn . Under this parametrization rule, the hypothesis functions in N0n and
n
T −1
{Nkn }N
k=0 share the same set of parameters. The policy function α (t, x) is parametrized by ang x) and then NN(t
g k , x) plays the role of a hypothesis function in N n00 .
other neural network NN(t,
k
00 N −1
T
In other words, {Nkn }k=0
share the same set of neural networks. In a stochastic game of N
players, there are 2N neural networks in total, with N neural networks corresponding to V n (t, x)
and N neural networks corresponding to αn (t, x). At stage m, the N V -networks are trained to
approximate the solution of PDE (9) and the N α-networks are trained to approximate the current
optimal policy computed by (8) using the optimal strategies in the last stage. The updated neural
T −1
and optimal
networks at stage m would be used at stage m + 1 to simulate paths {Xkn,π }N
k=0
strategies by (8). In this work, fully connected neural networks with three hidden layers are used.
Second, at each stage, the 2N neural networks could be decoupled to N pairs of V -network
and α-network based on players. Then, the N pairs of neural networks could be trained in parallel,
which dramatically reduces computational time. As Han and Hu (2020) and Seale and Burnett
(2006) pointed out, it is not necessary to solve the individual control problem accurately in each
stage; the parameters at each stage are updated starting from the optimal parameters in the last stage
without re-initialization. This requires only a moderate number of epochs for the stochastic gradient
descent at each stage.
The full implementation of Enhanced Deep Fictitious Play is shown in Algorithm 1. For simplicity, we state the algorithm based on a generic stochastic differential game.
Due to page limits, the exact choice of NN architectures will be detailed in Appendix B.1. To
determine the total stages of fictitious play M , we monitor the relative changes of αn and V n , and
stop the process when the relative change from stage to stage is below a threshold. Regarding the
total number of SGD per stage, as shown in (Han and Hu, 2020, Figure 1), the original DFP is insensitive to the choice of NSGD per stage . We find the enhanced version sharing the same behavior when
apply to the COVID-19 case study. We give more details in Section 4.2, and further experiments
regarding different choices of M and NSGD per stage in Appendix B.2.
For problems without analytical solutions, one natural concern is the reliability of numerical
solutions. Theoretically, the quantity (13) serves as the indicator of the numerical accuracy. In
the original DFP where the second term in (13) does not exist, Theorem 3 in Han et al. (2020b)
ensures the convergence to the true Nash equilibrium under technical assumptions when (13) is
small enough for each fictitious play stage and with sufficiently large M and small ∆tk . In practice,
the quantity in (13) is approximated by its Monte Carlo counterpart, which we define as the loss
function of our algorithms. Therefore, having small training losses during all stages will ensure
convergence. Extending Theorem 3 in Han et al. (2020b) to the current setting is beyond the scope
of this paper and is left for further work.

9

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

Algorithm 1 Enhanced Deep Fictitious Play for Finding Markovian Nash Equilibrium
Require: N = # of players, NT = # of subintervals on [0, T ], M = # of total stages in fictitious
play, Nsample = # of sample paths generated for each player at each stage of fictitious play,
NSGD per stage = # of SGD steps for each player at each stage, Nbatch = batch size per SGD
update, α0 : the initial policies that are smooth enough
1: Initialize N deep neural networks to represent V n,0 and N deep neural networks to represent
αn,0 , n ∈ N
2: for m ← 1 to M do
3:
for all n ∈ N do in parallel
T
4:
Generate Nsample sample paths {Xkn,π }N
k=0 according to (14) and the realized approximate
n,π
−n,m−1
optimal policies α̃
(tk , Xk ) (Remark: α̃ represents social and health policies in
the multi-region SEIR model)
5:
for e ← 1 to NSGD per stage do
6:
Update the parameters of the nth V -neural network and α-neural network one step with
Nbatch paths using the SGD algorithm (or its variant), based on the loss function (13)
7:
end for
8:
Obtain the approximate optimal policy α̃n,m represented by the latest policy neural network
9:
end for
10:
Collect the approximate optimal policies at stage m: α̃m ← (α̃1,m , . . . , α̃N,m )
11: end for
12: return The approximate optimal policy α̃M

4. Application on COVID-19
Our case study is based on COVID-19. We focus mainly on the lockdown/travel ban policy between
different regions. Therefore, to simplify the presentation, we omit the health policy h in the following discussion and make v(·) = v, λ(·) = λ, and η = 0. Moreover, as vaccines are not available to
the population yet, we let v(·) = v = 0.
4.1. Parameter choices
In single-region SEIR models, the transmission rate, β, is the basic reproductive number divided
by the length of time an individual is infectious. In our model, we assume that there is a regionindependent constant β that underlies the rate of infections for each population. The transmission
rates β nk between regions are related to the underlying transmission rate β, and the amount of travel
between regions n and k.
To quantify the size of travel between regions, we assume there is a constant fraction of people
from region n that travel to region k, f nk , at any given moment in time. We note that realistically
one may expect f nk to depend on time and also on the epidemic status of regions n and k. However,
for simplicity, we will not consider these scenarios in our numerical experiments. We assume that
f nn  f nk , f nn  f kn , ∀k 6= n, meaning that most of the population n resides in region n at any
given time, and also that most of the people in region n at a given time are from n and not travelers
from another region. We will see later that this implies β nn  β nk ∀k 6= n.

10

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

To further clarify the transmission of infection from one region due to another, we need to
describe the set of parameters {β nk : n, k ∈ N }. Here, β nk represents the rate of transmission
from region k to region n. Specifically, β nk is the number of infected people in population n per
a contactable, infectious individual in population k per day, assuming that 100% of population n
is susceptible. This definition of β nk comes from the derivation of the SDE system itself (1)–(2).
Accounting for infection in region n by individuals in region k from travel to both region n and k,
we have that

k

nk kk
kn nn P
β(f
f
+
f
f
)
, if k 6= n
β nk =
(16)
Pn

β(f nn )2 ,
if k = n.
We defer the detailed derivation of (16) to Appendix A.3.
Therefore, to specify β nk , we need to provide β and f nk . We will specify f nk for New York
(NY), New Jersey (NJ), and Pennsylvania (PA) in the next section. To estimate β, we choose a
basic reproductive number R0 = 2.2, which is consistent with Fauci et al. (2020), and assume
that the length of each individual being infectious is 13 days. More precisely, we assume infected
individuals either recover or die in 13 days. Under these assumptions, we obtain β = 2.2
13 ≈ 0.17,
consistent with Linton et al. (2020) as a 13 day median time until death from illness onset is used.
The infection fatality rate, or IFR, is the fraction of those infected who died from the infection. We choose the IFR to be 0.65% according to the CDC estimate. This is also consistent with
Meyerowitz-Katz and Merone (2020), which suggests a point estimate of 0.68%. The assumptions of an IFR of 0.65% and an infectious period of 13 days determines that the recovery rate
1
(including both recovery and death due to infection) is λ = 13
≈ 0.0769, and the death rate is
(0.65%)
κ = 13 = 0.0005. We choose the latent period to be 5 days according to Lauer et al. (2020).
This means that the we will have γ = 15 . Note that this choice has also been used in other models
such as Alvarez et al. (2020) and Peng et al. (2020). We assume that the parameters for noise-level
σsn , σen , n ∈ N are all 0.0002, and the extent to which one adheres to the social distancing policy,
θ, is either θ = 0.9 or θ = 0.99.
With most of the parameters for the SDE model (1)-(2) discussed, we now address those specific
to defining the cost. Regarding the risk-free-rate r, note that U.S. Treasury yields are historically
low and the uncertainty in the current level of inflation. We choose for simplicity that r = 0.
Also, considering that we are interested in simulations with time periods of less than a year, the
discounting is negligible. The parameter w represents the dollar output per individual per day. To
estimate w, we use GDP per capita per day, yielding the estimate w = 172.6 dollars per person per
day. Following Alvarez et al. (2020) and Hall et al. (2020), we use the value of a statistical life, χ, to
be 20 times GDP per capita. This results in χ = 1.95·106 dollars per person. According to the CDC
summary of U.S. COVID-19 activity, the hospitalization rate was 228.7 per 100,000 population by
11/14/2020. Thus, we set p = 228.7 × 10−5 . The cost per inpatient day is c = 73300/13 dollars,
estimated according to Health (2020). The attention hyperparameter a takes various values in the
case study, and will be specified in Section 4.2.
4.2. NY-NJ-PA COVID-19 case study
In this section, we apply our model (1)–(3) to analyze COVID-19 related policy in three adjacent
states: New York, New Jersey, and Pennsylvania. This case study is done over 180 days starting
11

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

from 03/15/2020, using the Enhanced Deep Fictitious Play algorithm introduced in Section 3 (cf.
Algorithm 1) and the parameters discussed in 4.1. The exact formulas of µn and hn in equation (9)
are derived in Appendix A.2.
We refer to New York State as region 1, New Jersey State as region 2, and Pennsylvania State as
region 3. Their respective populations are P 1 = 19.54 million, P 2 = 8.91 million, and P 3 = 12.81
million. Regarding β nk , ∀n, k = 1, 2, 3, we assume that: (a) 90% of any state’s population is
residing in their state at a given time; (b) the remaining population (travelers) visit the other regions
in an equal proportion; and (c) there is no travel outside of the considered regions, i.e., the NYNJ-PA is a closed system. The reasoning for (c) is that, under our model assuming that infection
only occurs in the regions considered, (c) is equivalent to allowing people traveling outside the
considered regions, but the travelers cannot be affected. For simplicity, we assume this is the case.
Under these assumptions, we will have f nn = 90% for n = 1, 2, 3 and f nk = 5% for n 6= k, and
obtain the values of β nk through (16).
Figure 1 presents the equilibrium policy issued by the governors of NY, NJ, and PA when the
policy effectiveness is θ = 0.99, i.e., 99% of the population follow the lockdown order. The hyperparameter is a = 100, i.e., each governor values people’s death 100 times the lockdown cost. In this
scenario, the governors take action at an early stage and soon reach the strictest policy. Once the
disease is under control, they may relax the policy later. The percentage of Susceptible, Exposed,
Infectious, and Removed stays almost constant in the end. As a comparison, Figure 2 illustrates
how the pandemic gets out of control if governors show inaction or issue mild lockdown policies.

Figure 1: Plots of optimal policies (top-left), Susceptibles (top-right), Exposed (bottom-left) and
Infectious (bottom-right) for three states: New York (blue), New Jersey (orange) and
Pennsylvania (green). The shaded areas depict the mean and 95% confidence interval
over 256 sample paths. Choices of parameters are in Section 4.1, a = 100 and θ = 0.99.

12

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

Figure 2: An illustration that governors’ inaction or mild control leads to disease spreading.
Experiment 1: dependence on a. We further analyze how the planners’ view on the death of
human beings changes their policies. In reality, economic loss is not the only factor the planners
concern about. It is also important to mitigate the infections and deaths within the budget and
available resources. Different views and values from the planners will lead to different policies. In
this experiment, we consider different attitudes towards the infection, especially death caused by
COVID-19. This is reflected by the attention hyperparameter a. Large a implies that planners care
more about human beings and are willing to spend more effort or endure more economic loss on
lockdown to avoid further infection and death. In comparison, smaller a implies that planners care
less about infection and death and instead pay more attention to minimizing the total cost.
The numerical results in Figures 3 and 4 are consistent with intuition. With a large a (top-left
panels), meaning the planners give more consideration to infection and death, they tend to issue
a restrict lockdown policy, which helps slow down the disease spreads and reduce the percentage
of infected people. As a becomes smaller (top-right panels), planners weigh more the economic
loss and spend fewer efforts on lockdown. When the attention a is small enough, some states even
give up controlling the disease spread due to economic concern (bottom panels). As a result, the
pandemic would get out of control by the end of the simulation period. This mild lockdown policy
leads to a natural spread of disease (also shown in Figure 2).
Experiment 2: dependence on θ. We next analyze how the residents’ willingness to comply with
the lockdown policy changes the optimal policies and the development of a pandemic. The larger the
θ is, the more likely the residents will follow the lockdown policy, and the larger the difference the
control makes on the pandemic situation. Conversely, small θ weakens the effect of the lockdown
policy. In the extreme case of θ = 0, no matter how strict the lockdown policy is, the pandemic
will become a natural spread because the control term in (1) disappears. In short, this willingness
to policy compliance should be an essential factor in decision-making.

13

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

Figure 3: Plots of optimal policies with different choice of a for three states: New York (blue), New
Jersey (orange) and Pennsylvania (green), when the lockdown efficiency is θ = 0.9.

To this end, we compare the optimal policy when θ = 0.9 and θ = 0.99 in Figure 5. Panels (ad) show the difference of optimal policies `(t) and the Susceptible S(t) in the tri-state game under
different θ when a = 50. In both situations, the pandemic is well-controlled, with the percentage
of susceptible people staying stable in the end. Moreover, in the case of θ = 0.99, people are
more willing to comply with the policies. Consequently, the planners are allowed to use a less strict
lockdown policy as shown in Figure 5(b) compared to 5(a), which saves the lockdown cost. Figure
5 (e-h) shows an interesting case in the comparison of θ = 0.9 and θ = 0.99. In this scenario, with
the same attention parameter (a = 25), θ = 0.9 leads to a mild lockdown policy, see Figure 5(e),
while θ = 0.99 provides a possibility to stop the spread of virus, see Figure 5(f). We believe that the
decision when θ = 0.9 is a compromise as the lockdown is not efficient enough to reduce largely
the infection and death loss by paying lockdown cost, and also due to the limited simulation period,
i.e., the policies could have been different if we had the simulation until the disease dies out. We
also believe that the early give-up by NJ drives NY and PA to lift lockdown policies at a later stage,
because even NY and PA issue strict policies, they are still facing severe infections from NJ due to its
high infected percentages and the existence of travel between states whatever the policy is. So their
interventions are not worth the candle. Figure 5(f)(h) further elucidate the importance of residents’
support in slowing down the pandemic. Further experiments based on different sets of (a, θ) reveal
the possibility of having multiple Nash equilibrium, with more elaboration in Appendix B.3.
To summarize, the numerical experiments illustrate that both the balance of economy and infection/death from the view of plan-makers and the willingness of residents to follow the lockdown
policy play an important role in decision-making. In reality, all three states issued stay-at-home

14

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

Figure 4: Plots of optimal policies with different choice of a for three states: New York (blue), New
Jersey (orange) and Pennsylvania (green), when the lockdown efficiency is θ = 0.99.

orders in March, and attempted to reopen in June. By comparing real world policies and our simulations of `(x), we may infer α and θ for NY, NJ, and PA in our model, i.e., θ = 0.99 and a = 25.

5. Conclusion
In this paper, we propose a novel multi-region SEIR model to study optimal policies under a pandemic. Our new model, built on game theory, takes into account how the social and health policies
issued by multiple region planners affect the progress of infectious diseases. This feature makes the
model more realistic and powerful but also introduces a formidable computational challenge due
to the high-dimensionality of the solution space and the strong coupling of planners’ policies. We
propose the enhanced deep fictitious play algorithm to overcome the curse of dimensionality and
use the model and algorithm in a case study of the COVID-19 pandemic in three states, New York,
New Jersey, and Pennsylvania. The model parameters are estimated from real data posted by the
CDC. We are able to show the effect of lockdown/travel ban policy on the spread of COVID-19 for
each state and how people’s willingness to comply and planners’ attitude towards deaths influence
the equilibrium strategies as a consequence of the competition between regions. We hope our model
can draw more attention to studying optimal interventions in infectious diseases using game theory.
Our numerical simulations can shed light on public policies.
In reality, during a pandemic, the planning is usually for short periods and adjusted frequently.
This can be modeled using repeated games, and planners may infer other regions’ cost functional
from past game outcomes. The assumptions that some parameters are identical across different
regions can be relaxed and the health policy can also be added for more accurate simulations. These
will be left for future work.
15

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

Figure 5: Comparison of optimal policies for three states (NY = blue, NJ = orange, PA = green)
and their susceptibles between different policy effectivenss θ and hyperparameter a.

Acknowledgments
R.H. was partially supported by NSF grant DMS-1953035. H.D.C. acknowledges partial support
from NSF grant DMS-1818821. This project was jointly supervised by R.H. and H.D.C.. Y.X.
and R.B. have equal contribution as the first authors. J.H. contributes on the algorithms and early
discussions of the model setup.

16

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

References
F. E. Alvarez, D. Argente, and F. Lippi. A simple planning problem for COVID-19 lockdown.
Working Paper 26981, National Bureau of Economic Research, 2020.
C. T. Bauch and D. J. D. Earn. Vaccination and the theory of games. Proceedings of the National
Academy of Sciences, 101(36):13391–13394, 2004.
C. T. Bauch, A. P. Galvani, and D. J. D. Earn. Group interest versus self-interest in smallpox
vaccination policy. Proceedings of the National Academy of Sciences, 100(18):10564–10567,
2003.
G. W. Brown. Some notes on computation of games solutions. Technical report, Rand Corp Santa
Monica CA, 1949.
G. W. Brown. Iterative solution of games by fictitious play. Activity Analysis of Production and
Allocation, 13(1):374–376, 1951.
S. L. Chang, M. Piraveenan, P. Pattison, and M. Prokopenko. Game theoretic modelling of infectious
disease dynamics and intervention methods: a review. Journal of Biological Dynamics, 14(1):
1–33, 2020.
W. E, J. Han, and A. Jentzen. Deep learning-based numerical methods for high-dimensional
parabolic partial differential equations and backward stochastic differential equations. Communications in Mathematics and Statistics, 5(4):349–380, 2017.
N. El Karoui, S. Peng, and M. C. Quenez. Backward stochastic differential equations in finance.
Mathematical Finance, 7(1):1–71, 1997.
A. S. Fauci, H. C. Lane, and R. R. Redfield. COVID-19 —navigating the uncharted. New England
Journal of Medicine, 382(13):1268–1269, 2020.
R. E. Hall, C. I Jones, and P. J. Klenow. Trading off consumption and COVID-19 deaths. Working
Paper 27340, National Bureau of Economic Research, 2020.
J. Han and R. Hu. Deep fictitious play for finding Markovian Nash equilibrium in multi-agent
games. In Proceedings of The First Mathematical and Scientific Machine Learning Conference
(MSML), volume 107, pages 221–245, 2020.
J. Han, A. Jentzen, and W. E. Solving high-dimensional partial differential equations using deep
learning. Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.
J. Han, R. Hu, and J. Long. Convergence of deep fictitious play for stochastic differential games.
arXiv preprint arXiv:2008.05519, 2020a.
J. Han, R. Hu, and J. Long. Barron metric for the convergence of empirical distribution. in preparation, 2020b.
Fair Health.
Costs for a Hospital Stay for COVID-19, 2020.
URL https://www.
fairhealth.org/article/costs-for-a-hospital-stay-for-covid-19.

17

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

R. Hu. Deep fictitious play for stochastic differential games. Communications in Mathematical
Sciences, 2020.
R. Isaacs. Differential Games: A Mathematical Theory with Applications to Warfare and Pursuit,
Control and Optimization. London: John Wiley and Sons, 1965.
S. A. Lauer, K. H. Grantz, Q. Bi, F. K. Jones, Q. Zheng, H. R. Meredith, A. S. Azman, N. G. Reich,
and J. Lessler. The incubation period of coronavirus disease 2019 (COVID-19) from publicly
reported confirmed cases: Estimation and application. Annals of Internal Medicine, 172(9):577–
582, 2020.
N.M. Linton, T. Kobayashi, Y. Yang, K. Hayashi, A.R. Akhmetzhanov, S.-M. Jung, B. Yuan, R. Kinoshita, and H. Nishiura. Incubation period and other epidemiological characteristics of 2019
novel coronavirus infections with right truncation: A statistical analysis of publicly available
case data. Journal of Clinical Medicine, 538(9), 2020.
W.-M. Liu, H. W. Hethcote, and S. A. Levin. Dynamical behavior of epidemiological models with
nonlinear incidence rates. Journal of Mathematical Biology, 25(4):359–380, 1987.
G. Meyerowitz-Katz and L. Merone. A systematic review and meta-analysis of published research
data on COVID-19 infection-fatality rates. medRxiv, 2020. doi: 10.1101/2020.05.03.20089854.
E. Pardoux and S. Peng. Backward stochastic differential equations and quasilinear parabolic partial
differential equations. In Stochastic Partial Differential Equations and Their Applications, pages
200–217. Springer, 1992.
E. Pardoux and S. Tang. Forward-backward stochastic differential equations and quasilinear
parabolic PDEs. Probability Theory and Related Fields, 114(2):123–150, 1999.
L. Peng, W. Yang, D. Zhang, C. Zhuge, and L. Hong. Epidemic analysis of covid-19 in china by
dynamical modeling, 2020.
D. Seale and J. Burnett. Solving large games with simulated fictitious play. International Game
Theory Review, 8(03):437–467, 2006.

Appendix A. Technical Details to the Stochastic Multi-region SEIR Model
A.1. The dynamics of Xt in Section 2
In Section 2, for the ease of notations and clarity of the presentation, we rewrite the dynamics of
(Stn , Etn , Itn ) defined in (1)–(2) in the vector form
dXt = b(t, Xt , `(t, Xt ), h(t, Xt )) dt + Σ(Xt ) dWt ,

(17)

where Xt ≡ [St , Et , It ]T ≡ [St1 , · · · , StN , Et1 , · · · , EtN , It1 , · · · , ItN ]T ∈ R3N , the Markovian
controls (`, h) are given by `(t, x) = [`1 , . . . , `N ]T (t, x) and h(t, x) = [h1 , . . . , hN ]T (t, x). In
the sequel, for a vector x ≡ (s, e, i) ∈ R3N , we shall index them in two ways,
(s1 , · · · , sN , e1 , · · · , eN , i1 , · · · , iN ) or (x1 , . . . , x3N ).
18

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

and use them interchangeably. The former one emphasizes the dependence on each category, while
the later notation is more condensed. Similarly, for partial derivatives, we will have two set of
notations
(∂s1 , · · · , ∂sN , ∂e1 , · · · , ∂eN , ∂i1 , · · · , ∂iN ) or (∂x1 , · · · , ∂x3N ).
We give precise definitions of (17) in this appendix. b(t, x, `, h) = [b1 , . . . , b3N ]T (t, x, `, h) is
a deterministic vector-valued function:

N
X




−
β jk sj ik (1 − θ`j (t, x))(1 − θ`k (t, x)) − v(hj (t, x))sj ,
j ∈ N,




 k=1
N
bj (t, x, `, h) = X
0
0

β j k sj 0 ik (1 − θ`j (t, x))(1 − θ`k (t, x)) − γej 0 ,
j ∈ N + N,




k=1



0
 0
γej − λ(hj (t, x))ij 0 ,
j ∈ N + 2N, and j 0 = j mod N.
Σ(x) = (Σj,k (x)) is a matrix-valued deterministic function in R3N ×2N with non-zero entries given
below:
Σj,j (x) = −σsj sj ,

Σj+N,j (x) = σsj sj ,

Σj+N,j+N (x) = −σej ej ,

Σj+2N,j+N (x) = σej ej ,

j ∈ N.

and {Wt }0≤t≤T is a 2N -dimensional standard Brownian motion:
Wt = [Wts1 , · · · , WtsN , Wte1 , · · · , WteN ]T .
Each region’s running cost f n defined in (4) is
f n (t, x, `, h) = e−rt P n [(sn + en + in )`n (t, x)w + a(κin χ + pin c)] + e−rt η(hn (t, x))2 .
A.2. The decoupled HJB equations in the form of (9)
Recall that we aim to solve (7) using the BSDE approach (nonlinear Feynman Kac relation). To this
end, in this appendix, we will rewrite it in the form of (9) and identify µn and g n . The first step is to
identify the minimizer in the Hamiltonian (6). Keeping in mind that our testing case is COVID-19
where vaccines are not fully developed yet, the term v(hnt ) = 0 is essentially zero in the past. Also,
to focus on the lockdown/travel ban policy between different regions (as we did in the case study),
we shall exclude the health policy h(t, x) from planners’ decision problem, i.e., v(·) = 0, λ(·) = λ,
and η = 0 in the following derivations, and remove the dependence of h in all relevant functions.
We remark that including the health policy h(t, x) is a straightforward generalization.
Recall that the Hamiltonian in (7) reads:
H n (t, x, (`n , `−n,m ), ∇x V n,m+1 )
= b(t, x, (`n , `−n,m )) · ∇x V n,m+1 + f n (t, x, `n )
=

3N
X
j=1

bj (t, x, (`n , `−n,m ))

∂V n,m+1
+ e−rt P n [(sn + en + in )`n (t, x)w + a(κin χ + pin c)],
∂xj

19

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

and recall that `−n,m = (`1,m , . . . , `n−1,m , `n+1,m , . . . , `N,m ) represents the mth stage strategies
of all players other than n, which are given functions in this derivation. The first order condition
requires for `n :
0=

N
X
j=1
j6=n






∂V n,m+1 ∂V n,m+1
∂V n,m+1 ∂V n,m+1
−
+ β nj sn ij
−
(1 − θ`j,m ) β jn sj in
∂ej
∂sj
∂en
∂sn

n

+2(1 − θ` )β

nn


sn in

∂V n,m+1 ∂V n,m+1
−
∂en
∂sn



1
− e−rt P n (sn + en + in )w.
θ

The critical point given by the above equation
indeed gives a minimizer
of the Hamiltonian, as long
 n,m+1

∂V
∂V n,m+1
as it is in [0, 1]. Because we can show
− ∂sn
> 0 by comparing V n,m+1 (t, x +
∂en
n+N ) and V n,m+1 (t, x+n ) using their definitions, where j is a 3N -vector with only one nonzero
entry   1 at j th position. Intuitively, with all others players’ initial condition the same, if player n
starts with a higher exposed proportion en + , it will produce more cost, comparing with the same
increase proportion still being susceptible sn + . To summarize, we deduce the optimal policy for
player n at stage m + 1 is given by:
(


∂V n,m+1 ∂V n,m+1
1
n,m+1
nn
`
(t, x) = 2β sn in
−
(18)
− e−rt P n (sn + en + in )w
∂en
∂sn
θ


 )


X
∂V n,m+1 ∂V n,m+1
∂V n,m+1 ∂V n,m+1
nj
j,m
jn
−
+ β sn ij
−
+
(1−θ` ) β sj in
∂ej
∂sj
∂en
∂sn
j=1
j6=n


× 2θβ

nn


sn in

∂V n,m+1 ∂V n,m+1
−
∂en
∂sn

−1
∧ 1 ∨ 0,

where we use the conventional notations a ∧ b = min{a, b} and a ∨ b = max{a, b}. Plugging
(18) into equation (7) and by straightforward computation, one obtains for the (m + 1)th stage,
µn,m+1 (t, x; `−n,m ) = [µn,m+1
, . . . , µn,m+1
](t, x; `−n,m )T is
1
3N
µn,m+1
= −β jn sj in (1 − θ`j,m (t, x)) −
j

N
X

β jk sj ik (1 − θ`j,m (t, x))(1 − θ`k,m (t, x)),

k=1
k6=n

j ∈N \n
µn,m+1
= −β nn sn in −
n

X

β nk sn ik (1 − θ`k,m (t, x))

k=1
k6=n
n,m+1
µn,m+1
− γej ,
N +j = −µj

µn,m+1
2N +j = γej − λij ,

20

j ∈ N.

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

To write g n,m+1 as a function of (t, x, z), we first compute
Σ(x)T ∇x V n (t, x)
T

∂V n 
∂V n
∂V n 
∂V n
∂V n 
∂V n
∂V n 
∂V n
, · · · , σsN sN
, σe1 e1
, · · · , σeN eN
.
−
−
−
−
= σs1 s1
∂e1
∂s1
∂eN
∂sN
∂i1
∂e1
∂iN
∂eN

and then g n,m+1 is given by:
θ2 nn
g n,m+1 (t, x, z; `−n,m ) =
β zn in [`n,m+1 (t, x)]2
σsn






N


X
β jn
θ nn
β nj
−rt n
j,m
+ e P (sn + en + in )w − 2
β zn in −
zn ij +
zj in ) `n,m+1 (t, x)
θ(1 − θ` (t, x))(


σsn
σsn
σsj


j=1


j6=n

+ e−rt P n a(κin χ + pin c).

A.3. The derivation of the transmission rate β nk in (16)
We denote by β the underlying transmission rate of the virus, which is assumed to be region independent. This transmission rate is the average number of people infected by an infectious person per
day (assuming that the susceptible population is 100%). Thus, if a proportion S of the population
is susceptible, then βS represents the average number of people infected per infectious person per
day. If there are a total of P people in a population with a fraction I being infectious, then βS(P I)
is the number of newly infected per day.
Thus, in the context of a single-region SEIR model, the number of newly infected (or the influx
to the exposed population) that occurs within (t, t + dt) is given by βS(t)(I(t)P ) dt. Dividing by
P , the influx to the scaled exposed population in (t, t + dt) is βS(t)I(t) dt.
Now let us consider the multi-region case and temporarily ignore the effect of lockdown. The
term from (1)–(2) that gives the influx to E n due to infection from I k is β nk S n (t)I k (t) dt. To
determine β nk , we build this exact influx from core assumptions.
First, we quantify the number of people from region n that are infected by those from region
k. Specifically, the influx in the interval (t, t + dt) to the unscaled exposed population n due to
transmission from population k is given by
X
(βf n` S n (t))(f k` I k (t)P k ) dt,
(19)
`

where f ij is the (assumed constant) fraction of people from i currently in region j at any moment
in time.
Equation (19) is obtained by summing the number of infections in population n due to population k across each region. The summand represents these infections occurring in region `. This can
be seen as the term f n` S n (t) is the proportion of population n that are susceptible and within region
`. Of this population, there will be βf n` S n (t) infections per infectious individual per day. Since the
21

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

number of infectious from k that are in ` is f k` I k (t)P k , we have that (βf n` S n (t))(f k` I k (t)P k ) dt
is the number of new infections in population n due to population k occurring in the region ` within
the time interval (t, t + dt).
Let us assume for now that n 6= k. Since we assume that 1 > f nn  f nk , the terms in (19)
besides the cases where ` = n or ` = k are negligible. Removing the negligible terms and dividing
by the population P n , we see that the influx to the scaled exposed population E n due to transmission
from population k over the interval (t, t + dt) is
Pk
β(f nn f kn + f nk f kk )S n (t)I k (t) dt,
Pn
which is exactly the influx represented by the model of β nk S n (t)I k (t) dt. This verifies the form
of β nk for n 6= k in (16). Similarly if we take n = k in (19) and ignore all terms other than
` = n(= k), then we derive the formula for β nn in (16).

Appendix B. Detailed discussions on the enhanced DFP algorithm
B.1. Implementation details
In the NY-NJ-PA case study, we choose feedforward architectures for both V -networks and αnetworks. Both have three hidden layers with a width of 40 neurons. The activation function
in each hidden layer is tanh(x). We do not apply activation function to the output layer of V networks, and choose sigmoid function ρs (x) = 1+e1−x for the α-networks. Other hyperparameters
are summarized in the table below.
hyperparameter
value

lr
5e-4

M
250

NSGD per stage
100

Nbatch
256

NT
40

τ
1e−3 /180

Table 1: Hyperparameters in the case study: lr denotes the learning rate in stochastic gradient
descent method, M is the total stages of fictitious play, NSGD per stage is the number of
stochastic gradient descent done in each minimization problem (13), Nbatch is batch size
in each stochastic gradient descent, NT is the discretization steps on [0, T ], and τ is the
weight of the control part in the loss function (13).

B.2. Discussion on the choice of M and NSGD per stage
We provide further experiments here on various choices of M and NSGD per stage . In Figure 6, we
plot both validation loss and log loss against M for three states, which are produced by evaluating
the NNs using unseen data after each fictitious play stage. In each panel, loss curves associated with
different number of SGDs per stage are presented in different colors (blue = 50, orange = 75, green
= 100, red = 125, purple = 150).
The numerical results show that the validation losses for all states decrease as the number of
DFP stages M increases. Moreover, it shows that different NSGD per stage generates loss curves with
similar patterns. Smaller NSGD per stage is more stable on the validation loss of PA. This result is consistent with Han and Hu (2020) and Seale and Burnett (2006), which convey that it is unnecessary
to solve the problem extremely accurate in each stage and that a moderate number of NSGD per stage
is sufficient. As a result, we choose NSGD per stage = 100 in our case study.
22

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

Figure 6: Loss curves of each state. Left: validation losses versus rounds M of the enhanced
deep fictitious play; Right: log10 validation loss versus rounds M of the enhanced deep
fictitious play. The loss curves with respect to NSGD per stage = 50, 75, 100, 125, 150 are
depicted in blue, orange, green, purple and red. A smoothed moving average with window
size 3 is applied in the final plots.

B.3. Stability over different experiments
Here we present experiments to investigate the Nash equilibrium of the model with different combinations of parameters. For each combination of parameters, we use the same hyper-parameters and
repeat the experiments several times. We run the algorithm for a certain computational budget, and
then filter out the results with a fluctuating loss near the stopping and check the converged equilibrium. In the first combination of parameters, we take θ = 0.99 and a = 100, corresponding to the
case that a governor weighs the deaths much more than the economic loss and tries to avoid it, and
the residents have a strong willingness to follow the governor’s policies. Intuitively, the pandemic
is possible to get well-controlled. Our numerical experiments confirm this intuition: all converging trails lead to the same Nash equilibrium. A representative plot of X(t) = (S(t), E(t), I(t)) is
shown in Figure 7.

23

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

Figure 7: With the parameter combination θ = 0.99, a = 100, the algorithm identifies one Nash
equilibrium for the NY-NJ-PA case study.

In the second batch of experiments, we take θ = 0.9 and a = 50, corresponding to the case
that a governor pays less attention to the number of death and the residents are less willing to
follow the policies compared to the first batch of experiments. The change leads to the possibility
of multiple Nash equilibrium and the pandemic being out of control. In this case, with different
NNs’ initialization, the algorithm identifies two Nash equilibria: 75% of the experiments converge
to the Nash equilibrium that the pandemic gets controlled and 25% of the experiments converge to
the other Nash equilibrium where the pandemic gets out of control.

Figure 8: With the parameter combination θ = 0.9, a = 50, the algorithm identifies two possible
Nash equilibria: an under control one (topic panels, with 75% of the experiments) and on
out-of-control one (bottom panels, with 25% of the experiments).

24

O PTIMAL P OLICIES FOR A PANDEMIC : M ODELING AND A LGORITHM

In conclusion, it is possible to have multiple Nash equilibria depending on the parameter chosen
in our stochastic multi-region SEIR model. There is usually a single Nash equilibrium for parameters chosen at extreme values, while for the parameters selected in the middle range, there could
exist multiple Nash equilibria. When multiple equilibria exist, we conjecture that the possibility to
reach a particular one depends on where we start the fictitious play (the initialization of the NNs’
parameters).

25

