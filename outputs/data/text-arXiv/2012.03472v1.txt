Quantum-Enhanced Machine Learning for Covid-19 and
Anderson Insulator Predictions

arXiv:2012.03472v1 [quant-ph] 7 Dec 2020

Paul-Aymeric McRaea,b , Michael Hilkea,c
a

Department of Physics, McGill University, Montréal, Canada H3A 2T8
b
Mila - Quebec AI Institute, Montréal, Québec, Canada
c
INTRIQ - Quebec Institute for Quantum Information

Abstract
Quantum Machine Learning (QML) algorithms to solve classifications problems have been made available thanks to recent advancements in quantum
computation. While the number of qubits are still relatively small, they have
been used for “quantum enhancement” of machine learning. An important
question is related to the efficacy of such protocols. We evaluate this efficacy
using common baseline data sets, in addition to recent coronavirus spread
data as well as the quantum metal-insulator transition in three dimensions.
For the computation, we used the 16 qubit IBM quantum computer. We find
that the “quantum enhancement” is not generic and fails for more complex
machine learning tasks.
Keywords: Quantum Machine Learning, COVID-19, Metal-Insulator,
Quantum Computing
1. Introduction
In the last decade or so, quantum computing has seen huge leaps in innovation and public awareness [1]. It has now become possible to run programs
on real quantum computers [2] or to realistically simulate them on a classical
machine [3]. Theoretically a quantum computer can offer great speed-up [4]
or better results [5] as compared to algorithms performed on a classical binary
system. Given these advances in the field it became interesting to combine it
with another rapidly-expanding topic in computer science: machine learning,
[6, 7, 8] leading to the aptly-named domain of Quantum Machine Learning
(QML). Thanks to IBM’s QISKit (Quantum Information Software Kit) [2],
QML has become tangible and accessible [9], with interesting results.
Preprint submitted to Elsevier

December 8, 2020

It is useful to define and classify different forms of QML. An effective
representation is to consider the usual machine learning (ML) framework,
where we have an input, I, a computation, C and an output O. In conventional
ML all three (I, C and O) are performed classically. In QML, any of I, C,
or O or any combination thereof can be quantum. Arguably, the simplest
approach is to take a quantum system, for example an electron in a random
potential (Anderson localization) [10], which we label quI (quantum input)
and use classical machine learning techniques to improve the classification
boundary of localized and extended states [11]. Here we push the scheme one
step further, by using a partial quantum C (or quC). In this classification
scheme, both the input and the computation is partially quantum as we
describe below.
In contrast, it is possible to use a purely classical I, for example the Covid19 infection rate in the US, but use a hybrid quantum C. In some cases, this
approach was shown to lead to an improvement in the classification outcome,
which was dubbed, “quantum enhancement” [12]. We use this scheme to
explore the possible “quantum enhancement” of different data samples, such
as the more complex Covid-19 data mentioned above. In all cases, we consider
a classical O, so that we only consider the cases I-quC-O and quI-quC-O as
our QML framework.
1.1. Support Vector Machines
The flavour of machine learning used here is a Support Vector Machine
(SVM). SVMs are used to classify data into different categories (usually two)
[13, 14, 15]. They are a type of supervised learning [14, 15]; that is, the branch
of machine learning in which a already-classified data set is used as an input
on which the machine trains [15]. Upon successful training, the SVM can
then predict the category in which a new data point belongs [15, 16, 17].
Support Vector Machines work by taking the training data set and virtually plotting the data in d-dimensional space where d is the number of
parameters (or features) of each data point. It then attempts to find the
(d − 1)-dimensional hyperplane which separates the data into two classes (in
the case of data with two parameters this is a line) [18]. Specifically, it aims
to find the hyperplane with the largest margin between points of either class
[14, 18]. Points lying on this margin are known as support vectors [18].
In many cases it is impossible to draw a separating hyperplane between
classes of data, especially when the boundary is highly nonlinear [18]. In such
a case where the SVM is unable to find a suitable separating hyperplane, a
2

Figure 1: Two-parameter (x1 , x2 ) data set with two categories (clubs and diamonds) along
with possible separating hyperplanes. The green line does not separate the data into two
sets. The orange and purple lines do separate the data but do not do so maximally. The
blue line is the maximally separating hyperplane with support vectors in blue circles.

technique known as a “kernel trick” is used, which involves using a non-linear
function (often called a Feature Map [19]) to project the data into a higher
dimension where a separating hyperplane may be found [20].
In this work, all classical SVM operations have been performed using the
SVM Python module developed by scikit-learn, whose documentation can be
found in [14].
1.2. Quantum Support Vector Machines
The quantum version of the SVM is best described as “quantum-assisted”
or “quantum-enhanced” [19] in the sense that the algorithm is largely classical with certain operations performed by a quantum processor (real or simulated). The Quantum SVM (or QSVM) made available by IBM QISKit
functions the same way as a classical SVM but the feature mapping [19]
and some operations of the hyperplane calculation [15] are handled by the
quantum processor.

3

1.3. Variational Quantum Classifier
An important quantum alternative to the QSVM is the Variational Quantum Classifier (VQC). This belongs to a class known as Variational Quantum
Algorithms, which are hybrid classical-quantum algorithms which iteratively
use the measurement of a parametrized quantum circuit as the input of a
classical process. The classical algorithm’s output is then passed as the parameter of the quantum circuit, and the loop then repeats [21, 22]. The VQC
encodes a data input as a quantum state, which is then processed and the
quantum probability is then measured, which serves as the base inference
for the binary classification. By refining this inference, the VQC can make
predictions based on data.

4

(a)

(b)
Figure 2: Sample circuits from (a) QSVM and (b) VQC runs on a 5-qubit system. Wraparound for visualization.

5

1.4. IBM Quantum Experience and QISKit
The term “quantum computing” can refer to several different types of
machines [1]. Here we refer specifically to those publicly available via the
IBM Quantum Experience. These operate on the model of quantum circuits
[23], a system which can be defined as follows: a classical computer stores
information in bits which can hold a value of either 0 or 1, and can perform
operations on these bits using a combination of logic gates (specifically AND,
OR, NOT) [24]. By analogy a quantum circuit uses qubits which can store
superpositions of |0i and |1i states and operates on them using quantum gates
[25], which in quantum mechanical terms can be described by operators on
two-level quantum systems and product states of such systems [26].
IBM has developed cloud-based access to real quantum circuit-based computers as part of the IBM Quantum Experience [27] for which software can
be written using QISKit [1]. Several backends are available, with 5 [28] or
16 qubits [29] or a simulator of over 30 qubits [30]. Their real computers are
built using transmons [31], a type of superconducting charge qubit [32, 33],
with algorithms designed to reduce errors that arise due to the fragility of
quantum information [34, 35].
1.5. Sources of Data
For our work, we use publicly available data sets provided by UC Irvine
as well as computed data. The first UCI data set is the “Breast Cancer
Wisconsin (Diagnostic) Data Set”, which contains 569 instances of tumor
data each containing 32 attributes which are used to determine whether the
tumor is benign or malignant. The second UCI set is known as “Wine Data
Set” and contains 178 instances of data sourced from wine, each containing
13 attributes which are used to classify which of three cultivars the wine was
sourced from [36]. These two data sets are often used as baselines for QML
trials, including in [37] as well as in IBM’s own QISKit tutorials [38].
Our generated data falls into three classes: ad-hoc data, Anderson data
and Coronavirus data. The ad-hoc data describes all data generated on a 20
by 20 grid, some of which use Python’s built-in pseudorandom functions. The
coronavirus data used was compiled by The New York Times and publicly
released on GitHub [39]. It contains the cumulative number of cases in the
United states on a county and state level. For this, the county-level data is
used. Finally the Anderson data is based on Anderson’s seminal result, where
he showed that certain materials can undergo a quantum phase transition
between conductor and an insulator based on the strength of disorder in the
6

system [10]. This can be numerically simulated [40] by varying energy and
disorder to get a two-dimensional data picture. The data is generated using
an iterative Green’s function technique to obtain the transmission coefficient
as a function of disorder strength [41].
2. Methods
2.1. Classification
As SVMs are designed to classify data into one of two sets (in most
cases) based on a training input, all data used needed some parameter that
delineates the set to which a data point would belong. The training input
is defined by taking the entirety of the classified data set and using 67% to
fill the training set. The remaining 33% is used as a “testing set” to verify
the accuracy of the SVM. The classifier used per data type is described as
follows.
For both the “Wine” and “Breast Cancer” data sets, QISKit provides
functions to procure the data such that it is already classified, and each
point can be reduced in dimensionality using the PCA algorithm. In our
experiments, we primarily reduced the feature dimension to two, though
certain experiments use three- or four-dimensional feature data.
In the case of the ad-hoc data the classifier is defined along with the
data, with the goal to create a particular “shape”. As mentioned, all data
was generated on a 20 × 20 grid with increment size 1. The four different
generated data sets are presented in Figure 3.

7

(a)

(b)

(c)

(d)

Figure 3: Ad-Hoc data. (a) Perfectly linearly separable data; (b) Alternating category
between adjacent points; (c) Randomly assigned points; (d) “Circle” of one category surrounded by the other

The coronavirus example uses data points consisting of a US county’s
population and its total number of cases, per day since the first recorded
case in the country. The classifier was whether a county is deemed “safe” or
“unsafe” based on whether the famous “curve” (i.e. the number of new cases
per day) was sufficiently flat or in fact decreasing. This was done by observing
the curve locally by taking the change in the number of cases over three days
preceding May 8th (an arbitrary choice). The derivative of cases per day
over the three days was then averaged. If this average was below a certain
threshold, i.e. the curve was flat or decreasing, the county was deemed safe.
Otherwise, it is classified as unsafe. The ML experiments were run using
thresholds of 1, 3, and 5 new cases per day on average. Only counties which
had a nonzero number of cases over the May 5th to 8th interval were included
in the data set. Likewise counties which changed the geographical area of
reported data over this time period were excluded.

8

Figure 4: Coronavirus data. Yellow counties are unsafe (curve not flattened), teal counties
are safe (curve flattened) and purple countries have no cases or insufficient data. We define
“Safe” as ≤ 5 new cases per day on average.

The Anderson data was generated numerically by considering an L × L ×
L cube between two semi-infinite leads. The conductivity of the material
is computed by the transmission coefficient via the Landauer formula as
a function of Energy (E), disorder (Va ), cube side length (L) for different
disorder configurations. The transmission coefficient is obtained from the
Green’s function by tracing over all channels [41]. The full Green’s function is
computed using a layer by layer iterative computation of the partial Green’s
function [42]. The conductance (which we denote G) was calculated for
varying values of the parameters (E, L, Va and disorder configurations). We
used 100 different disorder configurations for the averaging and a maximum
of L = 14. The dimensionality of this matrix was reduced by log-averaging
the value of G for all disorder configurations for fixed E, Va , and L, leading to
Ḡ. The following finite-size scaling method was used to differentiate metallic
and insulating states [43]. A least-squares linear fit to approximate Ḡ as a
Ḡ
) was recorded for each
function of L, whose slope (or partial derivative ∂∂L
E and Va . The classification was then made based on this value: a positive
∂ Ḡ
means the system is conductive, while a negative number indicates the
∂L
system is an insulator. Values within 1 × 10−6 of 0 were ignored.

9

(a)

(b)

Figure 5: Anderson data: Plot (a) shows the raw generated data where the color scale
corresponds to sgn(∂L G)| log |∂L G|| and plot (b) is the classified data: sgn(∂L G) (red
corresponds to a conductor, blue to an insulator, and white was excluded with values
|∂L G)| < 10−6 ).

2.2. Randomization
To achieve an apparently random state, two principles were kept in mind:
reproducibility and invertibility. As such, “randomization” was achieved
using custom made Pseudo-Random Number Generators (PRNG), which
generate numbers that appear random but whose behaviour can actually
be predicted [44]. Specifically given the same input and the same defining
parameter (dubbed the “seed”) a PRNG will always return the same value
[44]. Here, each of the two parameters for each data point was run through
the same PNRG, though different problems used different generators.
We can define a PRNG as follows:
rand(x) = 1 + (a · x + b)

mod m

(1)

Where b is an arbitrary integer, and a and m are prime numbers, with
a < m and m chosen such that it is greater than all the values of the data,
in order to guarantee invertibility. The inverse can be found as:
rand−1 (y) = a−1 · (y − 1 − b)

mod m

(2)

Where a−1 is the modular inverse of a (that is, the value such that a ·
a
mod m ≡ 1. This value can be found by iterating through all integers
between 1 and m until a number satisfying this condition is found. If a and
−1

10

m are coprime (which they must be since they are both prime) then this
inverse must exist and is unique [45].
2.3. Data Reduction
During the experimentation, we developed a technique to reduce the size
of the data set. Notably this had the benefit of removing outliers, which we
observed increased the accuracy. As well, with a smaller data set the training
time would be reduced. To achieve this reduction it was important to keep
data point more relevant to the training (that is, points closer to the data
boundary where classification becomes more difficult). This was achieved
by using a classical linear SVM to draw the hyperplane that best separates
the data into two classes, then selecting data points that fall within some
distance of this hyperplane. The chosen distance was tuned manually for
best results.
2.4. Machine Learning Procedures
Classical machine learning experiments were done using the scikit-learn
Python package [14] and specifically the SVM module. Most experiments
were done “out of the box” by using a linear kernel, but occasionally the machine was tweaked by tuning the hyperparameters, specifically by changing
the kernel to a polynomial or RBF. In some trials, the data was preprocessed
using a standard scaler, which rescales the data by subtracting the mean of
the data from the sample and dividing the result by the standard deviation.
This drastically decreases the training time, especially on large data sets. As
a rule, on data which was randomized, unscaled data would not be tested as
the algorithm would not converge in reasonable time.
All quantum machine learning was done using IBM QISKit’s QSVM module [46]. The feature map used was Second-order Pauli-Z evolution circuit
[47] using two repeated circuits and linear entanglement. QML experiments
were performed on two backends: the QASM simulator [3] and the 16-qubit
computer in Melbourne [29].
3. Results and Analysis
3.1. Volatility of Results
Havenstein et. al. [37] show that a VQC can outperform a classical SVM
on both the Wine and Breast Cancer data sets, a result we were unable to
reproduce. This can be accounted by the volatility of running similar, but
11

not identical trials. For instance we notice that accuracy differences can arise
from minor changes in the way the data is partitioned into training/test data,
including the seed (called state in the code) used to pseudo-randomly shuffle
the data as well as the size of the sets. We report these difference in Table 1.
Table 1: Accuracy of different runs using slightly different train/test partitions for the
QASM simulator (first table) and the IBMQ 16-Qubit Melbourne Backend (second table).

Data
Using
Using
Using
Using

Partitioning
33-37 test/train
30-70 test/train
33-67 test/train
30-70 test/train

Data
Using
Using
Using
Using

Partitioning
33-37 test/train
30-70 test/train
33-67 test/train
30-70 test/train

split,
split,
split,
split,

split,
split,
split,
split,

random
random
random
random

random
random
random
random

state
state
state
state

state
state
state
state

42
42
12
12

Accuracy (%)
75.0
72.22
75.0
66.67

42
42
12
12

Accuracy (%)
75.0
85.0
66.67
72.22

We also note that on successive trials run on either a real quantum backend or the statevector simuluator, accuracy would fluctuate, which makes
the outputs of these machines stochastic and non-reproducible.
3.2. Time Complexity and Runtime
While experimenting with QML we noted very long training and prediction times when using large data sets. Naturally this raised the question of
how runtimes scale with the size of the input. We observed that the duration
of a QSVM run on the QASM Simulator scaled as O(n2 ) where n is the size
of the training data (Fig. 6). We similarly observed linear O(n) time using
a VQC with a QASM simulator (Fig. 6) as well as an exponential O(2d )
time dependence on the feature dimension d (see Figure 7). The simulator
was used rather than real backend since the time to run experiments on them
depends on the number of jobs in the queue which is a factor beyond our control. We demonstrate the time dependence in the figures using least-squares
fits.
As presented in an overview by [48], the theorized time complexity of
the QSVM algorithm is Õ(log(nd)κ2 /3 ) where κ is a quantity dependent
12

Figure 6: Computation time for quantum classification algorithms versus number of training data points, along with least-squares fits.

on the kernel (specifically, it is the condition number of a matrix which
must be inverted),  is the accuracy, and we use the notation Õ(h(n)) =
O(h(n)log(h(n)) (i.e. suppressing slowly varying multiplicative terms). This
notably differs from our observation, which is likely to arise from two factors:
(1) the fact that a simulator was used and not a pure quantum circuit and
(2) the difference between implementation of an algorithm and theoretical
performance. Expanding on this second point, consider the analogous example of binary search, which can be completed in O(log(n)) time, but only if
the list is sorted. If not, the list must be sorted first, and this overhead will
cost at least O(nlog(n)) time.
By exploring the implementation of the QSVM we can see the O(n2 )
comes from computation of the kernel matrix; that is, the matrix of the kernel
function computed between all pairs of data points. Since for n datapoints
there are n2 such pairs, the origin of the quadratic computational complexity
is obvious. The VQC’s linear dependence on n is harder to explain, though
a heuristic explanation is the absence of any quadratic-time functions and
the fact that the algorithm likely makes at least one pass over all the data.
We were, however, unable to explain the exponential dependence on feature
dimension.

13

Figure 7: Computation time for VQC versus feature dimension. We have both the exponential evolution (which is captured by big-O, which drops slowly increasing terms) and
the closer fit: an exponential plus a linear function.

3.3. Baselines Revisited
A quick search through literature or online documentation will reveal
that the Wine and Breast Cancer data sets are commonly used to test QML
algorithms [49, 37, 50]. However, these are often run without comparisons
to a classical equivalent, such as the case of the QISKit tutorials [38] or the
thesis by Mendozza [50]. Others, such as Havenstein et. al. [37] make explicit
comparisons, though we were not able to reproduce their exact results for
reasons stated in Subsection 3.1.
Here we present our findings for our experimentation, with varying parameters such as feature dimension, number of data points, and backend
used. In no experiments using either the Breast Cancer or Wine data sets
were we able to observe a better accuracy of a quantum method as compared to classical counterparts. This would seem to indicate that claims
of a quantum advantage using these baselines may be overstated, and that
a new baseline should be explored which more clearly presents a quantum
advantage.

14

Figure 8: Accuracy of various classification methods versus number of training samples
from the Breast Cancer data set. The classical methods (Linear and RBF kernel SVMs)
consistently outperform the QML.

3.4. Results on Coronavirus and Anderson
While using a benchmark data set to reveal details on the performance of
a machine learning algorithm, the most important question when exploring
a novel technique is how well does it work in practice. For this we used two
data sets from “real” sources (Coronavirus and Anderson data) as well as
several manufactured data distributions (which we refer to as Ad-Hoc). In
this subsection we present our conclusions from this data. Our raw results
can be found in Appendix B.
Here again we note that in nearly every case, a form of classical classification outperforms a quantum counterpart. In some rare instances, quantum
narrowly displays better accuracy than classical algorithms, generally when
the setup has a degree of randomness involved. On these data sets we observe the larger prevalence of a potentially catastrophic behaviour: some of
the quantum methods achieve an artificially high accuracy by solely predicting one class. This generally occurs when the data disparity is skewed
towards one class. However, unlike the classical methods implemented in
scikit-learn, QISKit classification algorithms do not allow the same degree of
hyperparameter tuning, which makes this behaviour potentially unavoidable.

15

4. Conclusion
While quantum computation is a field of rapidly evolving interest, the
practicality of quantum classification algorithms lags behind that of simple
classical methods. Lower accuracy and longer times, caused by the high
complexity of simulation and the relatively limited real backends, have caused
QML to be a novelty more than a great leap in innovation. Nevertheless, with
refinement of existing algorithms, including more variability in the tuning of
hyperparameters, QML may in the future emerge as a strong contender in
the field of machine learning.
References
[1] National Academies of Sciences, Engineering, and Medicine . Quantum Computing: Progress and Prospects.
Washington, DC:
The National Academies Press; 2019.
ISBN 978-0-309-479691. URL: https://www.nap.edu/catalog/25196/quantum-computingprogress-and-prospects. doi:10.17226/25196.
[2] IBM Research Editorial Staff . QISKit – Quantum Information Software
Kit for Quantum Computation. IBM Research Blog; 2019. URL: https:
//www.ibm.com/blogs/research/2018/02/qiskit-index/.
[3] Qiskit Development Team . Qasmsimulator. Qiskit 0.19.6 documentation; 2020. URL: https://qiskit.org/documentation/stubs/
qiskit.providers.aer.QasmSimulator.html.
[4] Ronnow TF, Wang Z, Job J, Boixo S, Isakov SV, Wecker D, et al.
Defining and detecting quantum speedup. Science 2014;345(6195):420–
4. URL: https://doi.org/10.1126/science.1252319. doi:10.1126/
science.1252319.
[5] Montanaro A. Quantum algorithms: an overview. npj Quantum Information 2016;2(1). URL: https://doi.org/10.1038/npjqi.2015.23.
doi:10.1038/npjqi.2015.23.
[6] Dunjko V, Briegel HJ. Machine learning & artificial intelligence in the
quantum domain: a review of recent progress. Reports on Progress in
Physics 2018;81(7):074001.

16

[7] Arunachalam S, de Wolf R. Guest column. ACM SIGACT News
2017;48(2):41–67. URL: https://doi.org/10.1145/3106700.3106710.
doi:10.1145/3106700.3106710.
[8] Ciliberto C, Herbster M, Ialongo AD, Pontil M, Rocchetto A, Severini
S, et al. Quantum machine learning: a classical perspective. Proceedings of the Royal Society A: Mathematical, Physical and Engineering
Sciences 2018;474(2209):20170551. URL: https://doi.org/10.1098/
rspa.2017.0551. doi:10.1098/rspa.2017.0551.
[9] Havlicek V, Temme K, Córcoles A, Liu P, Chen R, Pistoia M, et al.
Qiskit aqua: Experiment with classification problem with quantumenhanced support vector machines. 2017.
[10] Anderson PW. Absence of diffusion in certain random lattices. Phys
Rev 1958;109:1492–505. URL: https://link.aps.org/doi/10.1103/
PhysRev.109.1492. doi:10.1103/PhysRev.109.1492.
[11] Ohtsuki T, Ohtsuki T. Deep learning the quantum phase transitions in
random electron systems: Applications to three dimensions. Journal of
the Physical Society of Japan 2017;86(4):044708.
[12] Dunjko V, Taylor JM, Briegel HJ. Quantum-enhanced machine learning.
Physical review letters 2016;117(13):130501.
[13] Cortes C, Vapnik V. Support-vector networks. Machine Learning 1995;20(3):273–97. URL: https://doi.org/10.1007/bf00994018.
doi:10.1007/bf00994018.
[14] Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel
O, et al. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research 2011;12:2825–30.
[15] Rebentrost P, Mohseni M, Lloyd S. Quantum support vector machine
for big data classification. Physical Review Letters 2014;113(13).
URL:
https://doi.org/10.1103/physrevlett.113.130503.
doi:10.1103/physrevlett.113.130503.
[16] Mohri M. Foundations of Machine Learning (Adaptive Computation and
Machine Learning). The MIT Press; 2012. ISBN 026201825X. URL:
https://www.xarg.org/ref/a/026201825X/.
17

[17] Russell S. Artificial Intelligence: A Modern Approach (3rd Edition).
Pearson; 2009. ISBN 0136042597. URL: https://www.xarg.org/ref/
a/0136042597/.
[18] Berwick R. An idiot’s guide to support vector machines (SVMs). 2003.
[19] Havlı́ček V, Córcoles AD, Temme K, Harrow AW, Kandala A, Chow JM,
et al. Supervised learning with quantum-enhanced feature spaces. Nature 2019;567(7747):209–12. URL: https://doi.org/10.1038/s41586019-0980-2. doi:10.1038/s41586-019-0980-2.
[20] Boser BE, Guyon IM, Vapnik VN. A training algorithm for optimal
margin classifiers. In: Proceedings of the fifth annual workshop on Computational learning theory - COLT 92. ACM Press; 1992,URL: https:
//doi.org/10.1145/130385.130401. doi:10.1145/130385.130401.
[21] LaRose R, Tikku A, O’Neel-Judy É, Cincio L, Coles PJ. Variational quantum state diagonalization.
npj Quantum Information 2019;5(1). URL: https://doi.org/10.1038/s41534-019-0167-6.
doi:10.1038/s41534-019-0167-6.
[22] Saldanha D, Agastya A, G P, and MRP. A review of supervised
variational quantum classifiers. International Journal of Engineering Research and 2020;V9(04). URL: https://doi.org/10.17577/
ijertv9is040485. doi:10.17577/ijertv9is040485.
[23] Qiskit Development Team . Quantum circuits. Qiskit 0.19.6 documentation; 2020. URL: https://qiskit.org/documentation/apidoc/
circuit.html.
[24] Patterson DA, Hennessy JL. Computer Organization and Design: The
Hardware/Software Interface. Fifth ed.; Morgan Kaufmann; 2013. ISBN
0124077269.
[25] Asfaw A, Bello L, Ben-Haim Y, Bravyi S, Capelluto L, Vazquez AC,
et al. Learn Quantum Computation Using Qiskit. 2020. URL: http:
//community.qiskit.org/textbook.
[26] Nielsen MA, Chuang IL. Quantum computation and quantum information. 10th anniversary ed.; Cambridge University Press; 2011. ISBN
9781107002173.
18

[27] IBM Makes Quantum Computing Available on IBM Cloud to Accelerate Innovation. 2016. URL: https://www-03.ibm.com/press/us/en/
pressrelease/49661.wss.
[28] IBM Q Team .
IBM Q 5 Yorktown backend specification
V1.3.0. 2019. URL: https://github.com/Qiskit/ibmq-deviceinformation/tree/master/backends/yorktown/V1.
[29] IBM Q Team .
IBM Q 16 Melbourne backend specification
V1.3.0. 2019. URL: https://github.com/Qiskit/ibmq-deviceinformation/blob/master/backends/melbourne/V1/.
[30] IBM Research Editorial Staff . An Open High-Performance Simulator for Quantum Circuits. IBM Research Blog; 2018. URL: https:
//www.ibm.com/blogs/research/2018/05/quantum-circuits/.
[31] Gambetta JM, Chow JM, Steffen M. Building logical qubits in a superconducting quantum computing system. npj Quantum Information 2017;3(1):2. URL: https://doi.org/10.1038/s41534-016-00040. doi:10.1038/s41534-016-0004-0.
[32] Kjaergaard M, Schwartz ME, Braumüller J, Krantz P, Wang JIJ,
Gustavsson S, et al. Superconducting qubits: Current state of play.
Annual Review of Condensed Matter Physics 2020;11(1):369–395. URL:
http://dx.doi.org/10.1146/annurev-conmatphys-031119-050605.
doi:10.1146/annurev-conmatphys-031119-050605.
[33] Koch J, Yu TM, Gambetta J, Houck AA, Schuster DI, Majer J,
et al. Charge-insensitive qubit design derived from the cooper pair
box. Physical Review A 2007;76(4). URL: http://dx.doi.org/10.1103/
PhysRevA.76.042319. doi:10.1103/physreva.76.042319.
[34] Jerry Chow . Dealing with errors in quantum computing. IBM Research Blog; 2014. URL: https://www.ibm.com/blogs/research/2014/
06/dealing-with-errors-in-quantum-computing/.
[35] Chow JM, Gambetta JM, Magesan E, Abraham DW, Cross AW, Johnson BR, et al. Implementing a strand of a scalable fault-tolerant quantum computing fabric. Nature Communications 2014;5(1):4015. URL:
https://doi.org/10.1038/ncomms5015. doi:10.1038/ncomms5015.
19

[36] Dua D, Graff C. UCI machine learning repository. 2017. URL: http:
//archive.ics.uci.edu/ml.
[37] Havenstein C, Thomas D, Chandrasekaran S. Comparisons of performance between quantum and classical machine learning. SMU
Data Science Review 2018;1(4). URL: https://scholar.smu.edu/cgi/
viewcontent.cgi?article=1047&context=datasciencereview.
[38] Abraham H, AduOffei , Agarwal R, Akhalwaya IY, Aleksandrowicz G,
Alexander T, et al. Qiskit: An open-source framework for quantum
computing. 2019. doi:10.5281/zenodo.2562110.
[39] Smith M, Yourish K, Almukhtar S, Collins K, Ivory D, Harmon A.
Coronavirus (Covid-19) Data in the United States. GitHub; 2020.
[40] Croy A, Römer RA, Schreiber M. Localization of electronic states in
amorphous materials: Recursive green’s function method and the metalinsulator transition at e 6= 0. In: Hoffmann KH, Meyer A, editors. Parallel Algorithms and Cluster Computing. Berlin, Heidelberg: Springer
Berlin Heidelberg. ISBN 978-3-540-33541-2; 2006, p. 203–26.
[41] Datta S. Nanoscale device modeling: the green’s function method. Superlattices and microstructures 2000;28(4):253–78.
[42] MacKinnon A. The calculation of transport properties and density of
states of disordered solids. Zeitschrift für Physik B Condensed Matter
1985;59(4):385–90.
[43] MacKinnon A, Kramer B. One-parameter scaling of localization length
and conductance in disordered systems. Physical Review Letters
1981;47(21):1546.
[44] Vadhan SP. Pseudorandomness. Foundations and Trends in Theoretical Computer Science 2012;7(1–3):1–336. URL: http://dx.doi.org/
10.1561/0400000010. doi:10.1561/0400000010.
[45] Shoup V. A computational introduction to number theory and algebra.
Cambridge: Cambridge University Press; 2005. ISBN 978-0521851541.
[46] Qiskit Development Team .
Qsvm.
Qiskit 0.19.6 documentation; 2020.
URL: https://qiskit.org/documentation/stubs/
qiskit.aqua.algorithms.QSVM.html.
20

[47] Qiskit Development Team . Zzfeaturemap. Qiskit 0.19.6 documentation; 2020. URL: https://qiskit.org/documentation/stubs/
qiskit.circuit.library.ZZFeatureMap.html.
[48] Duan B, Yuan J, Yu CH, Huang J, Hsieh CY.
A survey on
hhl algorithm: From theory to application in quantum machine
learning. Physics Letters A 2020;384(24):126595. URL: http://
www.sciencedirect.com/science/article/pii/S037596012030462X.
doi:https://doi.org/10.1016/j.physleta.2020.126595.
[49] Sharma S. Qeml (quantum enhanced machine learning): Using quantum computing to enhance ml classifiers and feature spaces. 2020.
arXiv:2002.10453.
[50] Mendozza R, Acampora G, Vitiello A. A quantum implementation
of support vector machine algorithm on ibm qx processors. Ph.D.
thesis; University of Naples Federico II Department of Physics;
2019.
URL: http://www.fisica.unina.it/documents/12375590/
13725484/2862 MendozzaR 16-10-2019.pdf/2fe12929-a8e7-46b7bae8-b3f06d4c1f8f.
Appendix A. RBF kernel classical SVM
An RBF kernel classical SVM without data preprocessing can be used to
locate the data boundary between the insulator/conductor phase transition in
the Anderson problem to 96.67% accuracy. Even when the data was reduced
and the boundary became less visibly obvious, this method of classification
was able to obtain 79.7% accuracy. We can conclude that this is a good
metric for classifying the properties of a a disordered solid.
We also observed that classically, using an RBF kernel, we were able to
predict whether a county in the USA was deemed safe to 90.99% accuracy. A
linear kernel yielded a slightly smaller accuracy at 90.56%. As a verification
of the quality of this process, we used all data collected from the 8th of May
as a training set and tested the machine on several other dates, with both
RBF and linear kernels, as reported in Table A.2. We observe that although
RBF is incrementally better on present-day data, the predictive capacity
of the linear kernel surpasses it. We also note the linear kernel’s decaying
prediction accuracy, which we can likely attribute to the data drifting further
from its initial state.
21

Table A.2: Prediction accuracy of a machine trained using May 8th COVID data.

Linear
RBF

May 9 May 18 May 28
91.55% 91.07% 90.20%
81.01% 82.61% 81.84%

June 8
88.34%
82.72%

Appendix B. Data Tables
Algorithm

Backend

SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC
QSVM
QSVM
VQC

Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator
ibmq 16 melbourne
statevector simulator
statevector simulator

Time (Seconds,
rounded)
0
0
0
0
11
48
1966
0
31

Accuracy

Only one Class
Predicted?
False
False
False
False
False
False
False
False
False

85.0
85.0
85.0
85.0
75.0
70.0
75.0
75.0
75.0

Table B.3: Results of trials on breast cancer data set with 60 total samples. Trials run on
ibmq 16 melbourne and statevector simulator are the results from one trial and were not
necessarily reproducible.

Feature Dimension

2

3

4

Algorithm

Backend

Time
(Seconds,
rounded)

Accuracy
(%)

SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
VQC

Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator

0
0
6
32
0
0
14
58
0
0
0
0
128

100.0
100.0
100.0
71.43
100.0
92.86
64.29
42.86
92.86
92.86
100.0
100.0
50.0

Only one
Class
Predicted?
False
False
False
False
False
False
False
False
False
False
False
False
False

Table B.4: Results of trials of the Wine data set, with increasing feature dimension.
Feature dimensions higher than 3 are not supported for QSVMs.

22

Data

All Data

All Data,
Randomized

Algorithm

Backend

SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC

Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator

Time
(Seconds,
rounded)
0
0
0
0
445
315
16
0
0
0
444
314

Accuracy
(%)
100.0
100.0
100.0
100.0
43.18
54.55
50.76
59.85
50.0
46.21
50.0
54.55

Only
one
Class
Predicted?
False
False
False
False
False
False
False
False
True
False
False
False

Table B.5: Results for trials using perfectly linearly separable Ad-Hoc data.
Data

All Data

All Data,
Randomized

Algorithm

Backend

SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC

Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator

Time
(Seconds,
rounded)
0
0
0
0
433
301
17
0
0
0
423
300

Accuracy
(%)
50.0
50.0
20.45
53.03
48.48
52.27
47.73
45.45
50.0
42.42
48.48
62.12

Only
one
Class
Predicted?
False
False
False
False
False
False
False
False
True
False
False
False

Table B.6: Results for trials using alternating grid Ad-Hoc data.
Data

All Data

All Data,
Randomized

Algorithm

Backend

SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC

Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator

Time
(Seconds,
rounded)
0
0
0
0
424
299
33
0
0
0
422
299

Accuracy
(%)
52.63
52.63
52.63
46.62
49.62
51.13
54.14
52.63
52.63
50.38
50.38
49.62

Only
one
Class
Predicted?
True
True
False
False
False
False
False
True
True
False
False
False

Table B.7: Results for trials using random grid Ad-Hoc data.
Data

All Data

All Data,
Randomized

Algorithm

Backend

SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC

Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator

Time
(Seconds,
rounded)
0
0
0
0
423
299
42
0
0
0
422
300

Accuracy
(%)
72.93
72.93
97.74
90.98
72.93
57.89
66.17
72.93
72.93
72.93
72.93
54.89

Table B.8: Results for trials using “circle” Ad-Hoc data.

23

Only
one
Class
Predicted?
True
True
False
False
True
False
False
True
True
True
True
False

Data

All Data

All Data,
Randomized

Reduced Data

Reduced Data,
Randomized

Algorithm

Backend

SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel
SVM, Linear Kernel, scaled
SVM, RBF Kernel
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
QSVM
QSVM

Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator
ibmq 16 melbourne
ibmq 16 melbourne

Time
(Seconds,
rounded)
0
0
0
0
14009
1722
0
0
14042
1707
0
0
0
0
493
322
0
0
485
323
117525
(X)

Accuracy
(%)
95.07
95.51
96.67
96.0
56.52
49.42
49.28
93.0
54.64
54.2
76.69
78.2
79.7
77.0
54.14
60.9
50.38
60.0
60.15
59.4
57.14
69.17

Only
one
Class Predicted?
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False
False

Table B.9: Results of the trials using the Anderson data set. For the reduced data, we
picked points within δ = 2 of the hyperplane learned using a linear SVM. Included are
two separate runs on the reduced, randomized data with a real 16-qubit computer. The
second run did not have time measured.

Data

All Data

All Data,
Randomized

Reduced Data

Reduced Data,
Randomized

Algorithm

Backend

SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
QSVM
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC

Local Processor
Local Processor
qasm simulator
qasm simulator
ibmq 16 melbourne
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator

Time
(Seconds,
rounded)
0
0
28505
2825
(X)
0
0
28562
2845
0
0
27485
2709
0
0
28533
2826

Accuracy
(%)
90.56
90.99
81.02
61.29
81.02
81.02
90.35
81.02
65.64
92.14
91.92
84.61
61.13
84.61
90.92
54.04
64.78

Only
one
Class Predicted?
False
False
True
False
True
True
False
True
False
False
False
True
False
True
False
False
False

Table B.10: Results of Coronavirus trials where the cutoff for safety is 5 new cases per
day. For the reduced data, we picked points within δ = 2000 of the hyperplane learned
using a linear SVM. Time was not measured for the 16-qubit run.

24

Data

All Data

All Data,
Randomized

Reduced Data

Reduced Data,
Randomized

Algorithm

Backend

SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC

Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator

Time
(Seconds,
rounded)
0
0
27450
3003
0
0
28466
3000
0
0
26579
2869
0
0
27890
2849

Accuracy
(%)
87.92
87.92
74.89
58.9
74.89
86.33
74.89
61.12
89.54
90.66
78.53
59.96
78.53
86.21
78.53
64.07

Only
one
Class Predicted?
False
False
True
False
True
False
True
False
False
False
True
False
True
False
True
False

Table B.11: Results of Coronavirus trials where the cutoff for safety is 3 new cases per
day. For the reduced data, we picked points within δ = 2000 of the hyperplane learned
using a linear SVM.

Data

All Data

All Data,
Randomized

Reduced Data

Reduced Data,
Randomized

Algorithm

Backend

SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC
SVM, Linear Kernel, scaled
SVM, RBF Kernel, scaled
QSVM
VQC

Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator
Local Processor
Local Processor
qasm simulator
qasm simulator

Time
(Seconds,
rounded)
0
0
29125
3005
0
0
30751
3014
0
0
26656
2855
0
0
29351
2775

Accuracy
(%)
80.51
79.77
60.06
52.86
60.06
78.5
60.06
54.77
83.2
84.66
63.49
52.86
63.49
76.48
63.49
57.11

Only
one
Class Predicted?
False
False
True
False
True
False
True
False
False
False
True
False
True
False
True
False

Table B.12: Results of Coronavirus trials where the cutoff for safety is 1 new case per day.
For the reduced data, we picked points within δ = 2000 of the hyperplane learned using a
linear SVM.

25

