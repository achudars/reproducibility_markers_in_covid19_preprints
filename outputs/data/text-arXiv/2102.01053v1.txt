arXiv:2102.01053v1 [stat.ME] 1 Feb 2021

New estimation approaches for graphical models with
elastic net penalty
Davide Bernardini,1 Sandra Paterlini,1 Emanuele Taufer1
1

Department of Economics and Management, University of Trento,

Abstract
In the context of undirected Gaussian graphical models, we introduce three estimators
based on elastic net penalty for the underlying dependence graph. Our goal is to estimate
the sparse precision matrix, from which to retrieve both the underlying conditional dependence graph and the partial correlation graph. The first estimator is derived from the direct
penalization of the precision matrix in the likelihood function, while the second from using
conditional penalized regressions to estimate the precision matrix. Finally, the third estimator relies on a 2-stages procedure that estimates the edge set first and then the precision
matrix elements. Through simulations we investigate the performances of the proposed
methods on a large set of well-known network structures. Empirical results on simulated
data show that the 2-stages procedure outperforms all other estimators both w.r.t. estimating the sparsity pattern in the graph and the edges’ weights. Finally, using real-world
data on US economic sectors, we estimate dependencies and show the impact of Covid-19
pandemic on the network strength.

Keywords — Gaussian graphical models, Elastic net penalty, Network estimation

1

1

Introduction

Let X = [X1 , X2 , ..., Xp ]> be a p-dimensional multivariate Gaussian random vector, denoted
with Np (µ, Σ), where µ is the mean vector and Σ is the covariance matrix. The matrix Θ =
Σ−1 is often called precision (or concentration) matrix. In this paper, we consider the problem
of estimating a sparse Θ in a multivariate Gaussian framework by introducing three estimation
techniques based on the elastic net penalty [1].
Graphical models are useful for representing a set of random variables and their conditional
dependence structure. A graphical model is made by two elements: a graph G(N, E) and a joint
distribution f . The set of nodes N = {1, 2, ..., p} of the graph represents random variables over
which the joint distribution is defined. Instead, the edge set E represents the pairs of variables
which are conditionally dependent given the remaining ones. Thus, a pair (i, j) ∈ E, with
i, j = 1, ..., p ∧ i 6= j, if and only if Xi 6⊥⊥ Xj |XC where C = {k ∈ N|k 6= i, j} and⊥⊥ indicates
independence between two random variables. For a detailed description of graphical models,
see for example Lauritzen [2] or Koller and Friedman [3]. Here, we consider undirected Gaussian graphical models where the joint distribution of random variables is multivariate normal.
In this special case, the dependence structure among each component in the multivariate distribution is obtainable through Θ, see [2] for example; if θij is the (i, j) element of Θ, then θij = 0
if and only if Xi ⊥⊥ Xj |XC .
Estimation of Θ in the Gaussian setting may serve other purposes. A sparse estimate Θ̂ of a
sparse Θ allows us not only to reconstruct the conditional dependence graph, but also the partial
correlation graph. The latter can be useful to model how shocks in the variables propagate in
the whole system, as discussed for example by Anufriev and Panchenko [4].
In this article, we introduce and test through simulations three estimators for the precision matrix Θ. These methods rely on penalization techniques to produce a sparse estimate. Penal-

2

ized estimation methods are used nowadays to estimate sparse and typically more interpretable
models. One could see them as noise filter techniques able to retrieve the relevant information
from data. In the context of linear regression, Tibshirani [5] proposed the lasso regression to
achieve sparsity in the estimated vector of regressors’ coefficients. By adding a penalty equal
to the `1 -norm of the coefficients’ vector to the standard OLS problem, lasso allows to automatically perform model selection and estimation in a single step. Lasso can then deal with
high-dimensional situations, where the number of parameters to estimate is larger than the sample size of data available. Beside regression, the lasso or a similar approach could then also
be considered to estimate the sparse dependence graph of a Gaussian graphical model. For example, Meinshausen and Bühlmann [6] used lasso regressions to estimate the edge set of the
graph. Banerjee et al. [7] and Friedman et al. [8] used an element-wise `1 -norm to penalize the
precision matrix in the log-likelihood function of the multivariate Gaussian distribution. Differently from the approach of Meinshausen and Bühlmann [6], Banerjee et al. [7] and Friedman et
al. [8] are able not only to perform edge set selection, but also to produce a sparse estimate of
Θ.
Starting from the original article about lasso in linear regression [5], several modifications have
been proposed in the literature. Among them, one of the most successful and widely used is
the elastic net penalty, proposed by Zou and Hastie [1]. In its original formulation, elastic
net penalty consists of adding an additional `2 -norm to the `1 -norm penalty in the objective
function of the OLS regression problem. According to [1], elastic net is then capable to address
some of the limitations of the lasso approach, such as the seemingly random selection among
highly correlated variables or, in the low-dimensional case, the worse prediction performance
with respect to ridge regression in presence of high correlation. Contrary to the wide usage of
lasso type penalty in the estimation of conditional dependence graphs, the elastic net penalty
has so far received limited attention. To the best of our knowledge only few recent works use
3

an elastic net type penalty for graphical models, see Cucuringu et al. [9], Ryali et al. [10] and
Liu et al. [11]. Cucuringu et al. [9] relies on a neighbors selection approach similar to [6],
which is related to the second and third estimator we propose in this paper (see section 2.2 and
2.3). Still, our goal is to produce a sparse estimate of Θ, not only to estimate the dependence
graph. Ryali et al. [10] rely on a maximum likelihood approach, which is connected to our
first estimation method (see section 2.1). However, compared to Ryali et al.[10], we derive an
optimization procedure inspired by [8].
In this paper, after introducing the three proposed estimators of Θ, we propose an extensive
comparison among them and glasso [8] on well-known network structures. In fact, we test them
on a large spectrum of graph’s topologies, such as scale-free, small-world and core-periphery
networks. Such topologies can be observed in the real-world, such as the scale-free property
being observed in the network of Web pages [12], while small-world properties are detected in
social networks and neural networks [13]. Empirical results on simulated data allow to compare
the validity of the proposed approaches. Then, we focus on a real-world application directed to
estimate dependencies across US sectors and evaluate the impact of Covid-19 pandemic on the
network strength.
This paper is structured as follows. Section 2 describes the three estimation approaches we
propose. In section 3 we report the simulations’ settings, while in section 4 we discuss the
empirical results. In section 5, we present an application using real-world data. To conclude we
discuss briefly our main findings in section 6.

4

2

Elastic Net Estimators

We propose here three different estimation methods that rely on the elastic net penalty for the
estimation of sparse precision matrices.
The first method results in an estimate of matrix Θ through a penalized log-likelihood optimization similar to the procedure used in the graphical lasso (glasso) [8]. The penalty used is
a combination of element-wise `1 -norm and `2 -norm of a matrix. Here, we call this approach
graphical elastic net, with acronym gelnet.
The second method is inspired by the approach of Yuan [14] for high-dimensional inverse covariance matrix estimation. It uses conditional regressions with elastic net penalty to produce
an asymmetric estimate of Θ. We then use a procedure to derive a symmetric estimate starting
from the first raw and asymmetric one. We call this second approach conditional regressions
graphical elastic net, with acronym CR-gelnet.
The third method relies on a combination of the first two. At first, it uses conditional regressions
with elastic net penalty to estimate the sparsity structure, that is the positions of zeroes, of the
matrix Θ. This is similar to the neighbourhood selection of Meinshausen and Bühlmann [6].
Then, the method proceeds with a constrained maximum likelihood estimation of Θ, where
the constraints are the zero off-diagonal elements found in the previous step. Because of this
two stages procedure, we call this final approach two stages graphical elastic net, with acronym
2S-gelnet.

2.1

Graphical elastic net - [gelnet]

The first method follows closely the approach proposed by Friedman et al. [8], thus it is a
penalized log-likelihood optimization problem. We suggest a direct modification of the original
algorithm by [8], taking into account an additional part in the penalty term. The new augmented
5

penalty is a convex linear combination of two penalties defined respectively as element-wise `1
and `2 norms of precision matrix Θ. This augmented penalty leads to the following convex
optimization problem:
n
o
2
Θ̂ = argmax log(det(Θ)) − trace(SΘ) − αλ||Θ||1 − (1 − α)λ||Θ||2

(1)

Θ

where:
• ||Θ||1 =

Pp Pp

• ||Θ||2 =

qP P
p
p

i=1

j=1;j6=i

i=1

|θij | is the element-wise `1 -norm of matrix Θ

2
j=1;j6=i (θij )

is the element-wise `2 -norm of matrix Θ

• S is the sample covariance matrix of data
• α ∈ [0, 1] is the first penalty parameter. This is used to control the balance between
`1 -norm and `2 -norm penalties. Note that when α = 1 we have the graphical lasso optimization problem.
• λ > 0 is the second penalty parameter and it is used to control the strength of the penalty.
This is the penalized log-likelihood of the Gaussian multivariate distribution Np (µ, Σ) already
partially maximized 1 with respect to mean vector µ and ignoring the constant term. We exclude
diagonal elements from the element-wise matrix norms as we want to shrink and induce sparsity
in the off-diagonal elements only.
We derive the penalized log-likelihood with respect to Θ, similarly to [8] using rules from [15].
So we obtained the following optimal conditions:
Θ−1 − S − αλΓ − 2(1 − α)λΘ = 0

(2)

where Γ is the subgradient matrix whose elements are γij = 1 if θij > 0, γij = [−1, 1] if θij = 0
and γij = −1 if θij < 0.
1

Partially maximized because we use the estimate of µ to compute S

6

The block coordinate descent algorithm proposed for graphical lasso [8] can be adapted to our
new optimization problem with elastic net penalty. This algorithm is similar to a coordinate
descent algorithm with the difference that it updates a block of variables (here the elements of
j-th row/column) at a time leaving the rest of them fixed.
Let W the estimate of Σ and consider the following notation for the partitioning of a generic
matrix A. Let A11 represents matrix A with row and column k-th removed, a12 the k-th column
of A with k-th row removed, a21 the k-th row with k-th column removed (equal to a12 if A is
symmetric) and a22 is the element of A in the k-th row and k-th column.


A11 a12
A=
a21 a22

(3)

The block coordinate method cycles through each k-th row/column solving for each k the following subproblem, while keeping the other values fixed:
w12 − s12 − αλγ12 − 2(1 − α)λθ12 = 0

(4)

and we set wkk = skk , not penalizing the diagonal.
Then, using the formula for block-partitioned matrix [8], we have that:


W11 w12
w21 w22




−1

Θ11 − θ12θ22θ21
T
= 
12
− W11 θθ22

12
−W11 θθ22

1
θ22

−

θ21 W11 θ12
2
θ22




(5)

12
12
Substituting w12 with −W11 θθ22
and setting b = − θθ22
we can rewrite the optimal condition as

follows:
W11 b − s12 + αλγ12 + 2(1 − α)λθ22 b = 0

(6)

These optimal conditions resemble closely the normal equations of the linear regression with
elastic net penalty term. In fact, it is possible to use the coordinate descent approach [16] to

7

find the optimal solution of b. The optimal updates for each block of variables can be derived
as follows:
p−1
X

W11,jt bt + W11,jj bj − s12,j

t6=j

Setting cj =

Pp−1
t6=j




−αλ  if bj < 0
− αλ, αλ if bj = 0
+ 2(1 − α)λθ22 bj +

αλ
if bj > 0

(7)

W11,jt bt − s12,j and hj = W11,jj , we have for each variable in the current

block:


 cj + hj bj + 2(1 − α)λθ22bj − αλ
cj − αλ, cj + αλ

cj + hj bj + 2(1 − α)λθ22 bj + αλ

if bj < 0
if bj = 0
if bj > 0

(8)

Therefore the optimal updates are:
• b∗j =
•

b∗j

−cj +αλ
hj +2(1−α)λθ22

if bj < 0 thus −cj + αλ < 0 ⇒ cj > αλ


= 0 with [cj − αλ, cj + αλ] containing 0, thus

• b∗j =

−cj −αλ
hj +2(1−α)λθ22

cj − αλ ≤ 0
⇒ −αλ ≤ cj ≤ αλ
cj + αλ ≥ 0

if bj > 0 thus −cj − αλ > 0 ⇒ cj < −αλ

or using soft-thresholding operator:




Soft cj , αλ := sign(cj ) |cj | − αλ
b∗j

−Soft(cj , αλ)
=
hj + 2(1 − α)λθ22

+

(9)

Once the coordinate descent algorithm has converged to the optimal values b∗ , we can update
w12 :
w12 = W11 b∗

(10)

then θ22 from:
θ22 =

1
w22 − b∗ w12

(11)

and finally θ12 from:
θ12 = −b∗ θ22
8

(12)

The difference here between the original graphical lasso algorithm is that at each block-update
θ22 and θ12 are also updated.
The block coordinate algorithm then proceeds with the updates of the next block, which is the
next row/column of the matrix W. After all p rows/columns are updated, numerical convergence
is checked. This is performed by checking if the biggest difference, in absolute value, among
the elements of two full subsequent updates of W is less than a specific threshold δ. After the
algorithm converges, the final updated version of Θ is the estimate Θ̂ of the precision matrix
and the final updated version of W is the estimate of Σ.
Pseudo-code is reported in Algorithm 1:
Algorithm 1: gelnet
Set W = S and Θ = S−1
Set a threshold value δ
Set Convergence = FALSE
while Convergence==FALSE do
Wold = W
for k=1,..,p do
1) Subdivide W, S and Θ into blocks as in formula 3
2) Evaluate using a coordinate descent algorithm the optimal values b∗ of b. In the first
update of W set randomly their initial values, while in the following updates of W use
their previous estimates.
3) Use b∗ to update the values of w12 , θ22 and θ12
4) Store b∗ , these will be the next starting point for the coordinate descent for this block
of variables in the next update of the matrices
end for
if maxi,j (abs(Wold,ij - Wij )) < δ then
Convergence=TRUE
end if
end while

9

2.2

Conditional regressions graphical elastic net - [CR-gelnet]

The conditional regression method we propose follows the approach of Yuan [14] and Bogdan
et al. [17]. The core idea of this approach is to fit an elastic net penalized regression for each
component of a multivariate Gaussian random vector. That is, the i-th element of the random
vector becomes the dependent variable while the other elements are the independent variables.
These coefficients of linear regressions are proportional to the elements of Θ. Thus, given a
p-dimensional random variable, the estimated coefficients of each penalized regression fitted
are used to produce an estimate of Θ. Since this estimate is not symmetric, an additional procedure must be employed to produce a symmetric estimate of Θ. In the following part of this
subsection, we describe in more details the whole procedure.
Let X the p-dimensional random vector with Gaussian distribution. The conditional distribution of i-th component Xi , given the remaining components X−i can be expressed as linear
relation [14]:
>
Xi |X−i = ai + X−i
bi + i

(13)

−1
with ai = µi − Σi,−i Σ−1
−i,−i µ−i and bi = Σ−i,−i Σ−i,i If X ∼ N (µ, Σ), then [14] [18]:

• random error term i ∼ N (0, Σii − Σi,−i Σ−1
−i,−i Σ−i,i )
−1
• the conditional distribution of Xi |X−i ∼ N (µi +Σi,−i Σ−1
−i,−i (X−i −µ−i ), Σii −Σi,−i Σ−i,−i Σ−i,i )

where Σ−i,−i refers to the covariance matrix Σ without the i-th row and column, Σi,i to the
element of Σ in the i-th column and row, Σi,−i to the i-th row of Σ but without the i-th element,
Σ−i,i to the i-th column of Σ but without the i-th element.
−1
Setting zi = (Σi,i −Σi,−i Σ−1
and using the rule for the inverse of a matrix subdivided
−i,−i Σ−i,i )

in blocks, as in [14] for example, we have that:

−1
Σi,i Σi,−i
=
Σ−i,i Σ−i,−i
10

(14)


zi
−zi Σi,−i Σ−1
−i,i
=
=
−1
−1
−1
−Σ−1
−i,i Σ−i,i zi Σ−i,−i + Σ−i,−i Σ−i,i zi Σi,−i Σ−i,−i




Θi,i Θi,−i
=
Θ−i,i Θ−i,−i
It is clear from the formula above that Θi,i =

1
Var(i )

(15)


(16)
bi
and Θ−i,i = − Var(
. Therefore the
i)

sparsity pattern in the regression coefficients bi corresponds to the sparsity pattern in the offdiagonal elements of the i-th column of Θ. Thus ideally the matrix Θ can be reconstructed by
properly rescaling the regression coefficients bi . This is the justification behind this conditional
regression approach for the estimation of a sparse Θ.
The estimation of the full matrix Θ is done by the estimation of p penalized regressions, one for
each component of multivariate random variable on the remaining p − 1 components. Yuan [14]
used the Dantzig selector for the estimation of bi , but here we use p elastic net regressions all
having the same parameters α and λ. Both estimation techniques can produce sparse estimates
b̂i of bi , thus we can obtain a sparse estimate Θ̂ of Θ.
Since this approach doesn’t guarantee that the estimated matrix Θ̂ is symmetric, an additional
procedure is needed to make this estimate symmetric. Ideally a good procedure here should be
able to retain sparsity. We propose and use two simple and computationally cheap procedures:
• L2 method (L2): we replace the elements θ̂ij and θ̂ji of the initial asymmetric estimated matrix Θ̂ with their average. This is analogous to minimize the Frobenius distance
(element-wise `2 -norm) between the original asymmetric matrix and a symmetric matrix
as in [17]. Note that this rule retains sparsity when both elements (i, j) and (j, i) are zero.
• Minimum element method (MinEl): we substitute the elements θ̂ij and θ̂ji with the
minimum element, in absolute value, between elements (i, j) and (j, i) of the asymmetric

11

estimate Θ̂, as proposed by Cai et al. [19]. Note that this rule retains sparsity when at
least one element between (i, j) and (j, i) is zero.
The conditional regression approach for estimating the matrix Θ can be summarized as in Algorithm 2:
Algorithm 2: CR-gelnet
Set Θ̂ as Ipxp (p by p identity matrix)
for i=1,..,p do
1) Fit an elastic net penalized regression using coordinate descent procedure in order to
T
estimate the coefficients’ vector bi of ai + X−i
bi + i
2) Use residuals of the regression to estimate Var(i )
ˆ i ), update the values of Θ̂i,i and Θ̂−i,i
3) Given the estimated values b̂i and Var(
end for
Use MinEl or L2 method to make the estimated Θ̂ symmetric.

2.3

Two stages graphical elastic net - [2S-gelnet]

As pointed out at the beginning of the section, this last approach can be seen as a mix of the
two methods already discussed. It consists of 2 stages. Firstly we use elastic net penalized
conditional regressions to estimate the sparsity pattern in the matrix Θ. Secondly we use the
estimated positions of zeros to do a constrained log-likelihood estimation of Θ, given that the
distribution of data is a multivariate Gaussian.
Meinshausen and Bühlmann [6] suggested that the edge set E of the conditional dependence
graph of an undirected Gaussian graphical model can be estimated by using penalized regressions able to induce sparsity in the estimated coefficients. Their approach is usually called
neighbourhood selection and its aim is only to estimate the edge set E of a Gaussian graphical
model, but not the estimation of the precision matrix. They used conditional lasso regressions
to reconstruct a sparse graph. A zero regression coefficient implies zero partial correlation and
thus the absence of the related edge in the graph, at least in the multivariate Gaussian situa12

tion. For our estimation problem, it means a zero in the precision matrix, thus a zero coefficient
constraint in the log-likelihood optimization problem. We can easily extend the approach of
Meinshausen and Bühlmann [6] using conditional elastic net regressions instead of lasso regressions to estimate the edge set of the graph of a Gaussian graphical model.
Let X be the n by p matrix of observations, we fit p conditional elastic net penalized regressions
of each component over the remaining ones:
n
o
2
2
b̂i = argminbi ||Xi − ai − X−i bi ||2 + λ[α||bi ||1 + (1 − α)||bi ||2 ]

(17)

where Xi is the i-th column of X and X−i is the matrix X without i-th column. This approach
could lead to different conclusions about the inclusion of the edge (j, i) (undirected edge between j and i) because the estimated regression coefficient b̂ji (j-th element of b̂i ) is not necessary zero when b̂ij (i-th element of b̂j ) is zero, or vice versa. Meinshausen and Bühlmann [6]
suggested two possible rules to deal with this situation:
• AND rule: edge (i, j) ∈ Ê if b̂ij 6= 0 ∧ b̂ji 6= 0
• OR rule: edge (i, j) ∈ Ê if b̂ij 6= 0 ∨ b̂ji 6= 0
where Ê represent the estimated edge set of the underlying undirected Gaussian graphical
model. We follow these two rules to set the zero positions in the precision matrix Θ we want to
estimate.
Once the edge set E is estimated and we have the set of constraints for the elements in the
precision matrix, we have to solve the following constrained optimization problem:
n
o
max log(det(Θ)) − trace(SΘ)
Θ

(18)

s.t.
θij = θji = 0 if edge (i, j) ∈
/ Ê

13

This problem can be rewritten in Lagrangian form as:
n

argmax log(det(Θ)) − trace(SΘ) −
Θ

X

γij θij

o

(19)

(i,j)∈E
/

To solve this optimization problem, we use the algorithm proposed by Hastie et al. [20] to
maximize the constrained log-likelihood and produce a (sparse) estimate Θ̂, given the network
structure already estimated in the previous step.
Pseudo-code for the whole procedure is reported in Algorithm 3
Algorithm 3: 2S-gelnet
for i=1,..,p do
Fit an elastic net penalized regression using coordinate descent procedure in order to
estimate the vector of coefficients bi
end for
Use each b̂i estimated coefficients’ vector in combination with an AND or OR rule to
estimate the edge set E
Estimate the precision matrix Θ with the constraints given by the estimated edge set. That
is, the elements of the estimated precision are constrained to zero if the corresponding edge
is missing

14

3
3.1

Methodological Set-Up
Simulation Set-Up

Here, we briefly describe the simulation set-up to test the performances of the three estimators
described in section 2. For the CR-gelnet we test both the L2 method and MinEl method. For
the 2S-gelnet approach we test the AND and OR rules. In addition, as a benchmark, we also
consider the graphical lasso [8]. We test the estimators on 7 network structures/graphs with 30
nodes each (see appendix A for adjacency matrices).
Net1: Scale-Free
Net2: Random
Net3: Hub
Net4: Cluster
Net5: Band
Net6: Small-World
Net7: Core-Periphery
For each network structure, 30 datasets are randomly generated from a multivariate normal distribution. We consider both 1000 and 200 sample sizes in order to check the performances in
low and high-dimensional case, respectively. We rely on the R package huge to generate the
sparse precision matrices that represent the given network structures (for topologies 1-5, parameters: v=0.3; u=0.1). Moreover, we also consider small-world and core-periphery topologies,
as they are well-known structures, and to generate simulated data we rely on the algorithms
proposed by Watts and Strogatz [13] and by Torri et al. [21]. After specifying the precision
matrix, we invert it and thus use the covariance matrix as input to generate data from a multivariate Gaussian distribution. For each dataset, we estimate the best model using BIC or 5-fold

15

cross-validation (see for example [8], [22], [21] and [23]) with the following grid of tuning
parameters:
• α : 41 equally spaced values between 0 and 1
• λ : 101 equally spaced values between 0 and 0.4
We notice that it might happen that multiple equivalent optima are found for different combinations of (α, λ). In such situations, we consider the one with the minimum α.

3.2

Performance measures

We consider a correctly identified edge as a true positive, while a false positive is a missing
edge that is incorrectly included in the estimated edge set. Thus, a true negative is a missing
edge that is correctly excluded from the estimated edge set, while false negative is an existing
edge that is not identified. We use Receiver Operator Characteristic (ROC) curves, accuracy
and F1 -score to evaluate the binary classification performances.
ROC curves report the performance of a binary classifier in terms of false positive rate (FPR)
and true positive rate (TPR). FPR is the ratio between false positives and all real negatives,
while TPR is defined as the ratio of true positives over total real positives. Different estimates
are produced for each combination of the parameters α and λ. Then, each estimate is plotted
on a plane accordingly to its FPR and TPR, reported on x-axis and y-axis, respectively. In each
ROC curve, a single point is an estimate of the underlying graph for a given value of λ. For the
estimators based on elastic net penalty, we plot only the estimated models, given the optimally
selected value of α, as plotting all points for each α would result in lack of clarity. So here the
varying parameters is λ and not a probability threshold as with common binary classifiers.
Symbols ”o” and ”x” are used to plot ROC curves for BIC and cross-validation, respectively.
The optimally selected models are then reported in black instead of red.
16

Accuracy and F1 -score are defined as follows:
• Accuracy =

True positives+True Negatives
Positives+Negatives

Precision·Recall
• F1 -score = 2 Precision+Recall
where:

– Precision =
– Recall =

True positives
True Positives+False Positives

True positives
True Positives+False Negatives

Both measures are bounded between 0 and 1. The higher the value, the better the classification
of the estimator. While accuracy is generally easier to interpret, F1 -score is more suitable when
there is imbalance among classes, that is when the ratio of the number of actual edges over the
total number of edges in the hypothetical complete graph is far from 0.5. Finally, we also evaluate the Frobenius distance among estimated and true partial correlation matrices as a measure
of numerical accuracy of the estimates. One can retrieve a partial correlation matrix P or P̂ from
−θ
the true and estimated precision matrix, respectively, using pij = √ ij

θii θjj

for i, j = 1, 2, .., k.

We follow a common convention and set pii = 0 for i = 1, 2, .., k. The Frobenius distance is
then defined as:

v
uX
p
u p X
t
||P − P̂||F =
|pij − p̂ij |2
i=1 j=1

The closer the Frobenius distance is to 0, the better the estimate.

17

(20)

4

Simulation Analysis

In this section, we analyse the performance of the three elastic net algorithms and the graphical
lasso (glasso) on the seven network configurations (see Section 3 and appendix A), using both
BIC and 5-fold cross validation. Figure 1 displays the ROC curves for the simulations with
sample size n =1000, while Tables 1, 2 and 3, 4 report the mean values and standard deviations
of accuracy and F1 -score for the BIC and 5-fold cross validation, respectively. Tables 5 and 6
report the mean values of Frobenius distance in the low-dimensional case. Tables 9, 10, 11 and
12 in the appendix B.1 and B.2 display the average value of the optimally selected α. Tables 13,
14, 15, 16, 17 and 18 report the mean values of accuracy, F1 -score and Frobenius distance in the
high-dimensional case (n=200). Figure 5 then displays the ROC curves in the high-dimensional
case.

18

ROC Curves − Scale−Free (n = 1000)
gelnet
1.00

TPR

0.75

0.50

0.25

0.00

●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00

0.25

0.50

0.75

●

2S−gelnet (AND)

2S−gelnet (OR)

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

CR−gelnet (L2)

●

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

CR−gelnet (MinEl)

●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

0.75

●

glasso

●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

0.75

●

●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

0.75

●

Opt
●

No

●

Yes

Selection
●

BIC
CV

1.00

FPR

ROC Curves − Random (n = 1000)
gelnet
1.00

TPR

0.75

0.50

0.25

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

2S−gelnet (AND)

2S−gelnet (OR)

CR−gelnet (L2)

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

CR−gelnet (MinEl)

glasso

●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

Opt
●

No

●

Yes

Selection
●

BIC
CV

0.00
0.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.00

FPR

ROC Cu ves − Hub n = 1000

TPR

S
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

AND

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●

S

OR

CR

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

FPR

19

●

CR

M E

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●●
●●
●●●
●
●
●●
●●
●
● ●●
●
●
● ●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●● ●
●
●
●●

●
●●●

●

Op
N
Y

Se ec on
BC
CV

ROC Curves − Cluster (n = 1000)
gelnet
1.00

TPR

0.75

0.50

0.25

2S−gelnet (AND)

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●● ●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

2S−gelnet (OR)

CR−gelnet (L2)

●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

CR−gelnet (MinEl)

glasso

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●●
●●● ●
●
●
●
●
●
●
●
●
●
●●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

Opt
●

No

●

Yes

Selection
●

BIC
CV

0.00
0.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.00

FPR

ROC Curves − Band (n = 1000)
gelnet
1.00

TPR

0.75

0.50

0.25

0.00

2S−gelnet (AND)

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00

0.25

0.50

0.75

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●● ●●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

0.75

●

2S−gelnet (OR)

CR−gelnet (L2)

●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

CR−gelnet (MinEl)
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
● ●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

glasso

●
●●
●
●
●
●
●
●
●
●
●

0.75

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

0.75

●

Opt
●

No

●

Yes

Selection
●

BIC
CV

1.00

FPR

ROC Cu ves − Sma −Wo d n = 1000

TPR

S
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

AND

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

S

OR

CR

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

CR

M E

●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●●
●
●●●
●●●
●
●●●●
●●
●●●
●
● ●
●
● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●●●●

●●
●
●●

●

Op
N
Y

Se ec on
BC
CV

FPR

ROC Cu ves − Co e−Pe phe y n = 1000
S

AND

S

OR

CR

CR

M E

Op
N

TPR

Y

Se ec on
BC
CV

FPR

Figure 1: ROC curves for seven network structures (in rows) for gelnet (column 1) 2S-gelnet
(columns 2 & 3) CR-gelnet (columns 4 & 5) and glasso (column 6) - (n=1000)
20

From the ROC curves in Figure 1, we see that the 2S-gelnet in columns 2 and 3 performs always
better than the other methods. Both using BIC criterion and cross-validation, the optimally
selected models are the closest to the top-left corner of [0, 1] × [0, 1] square (high TPR, low
FPR). Also notice that the optimal estimates of the 2-stages approach 2S-gelnet are very close
both using BIC and cross-validation. When considering the other methods (i.e. gelnet - column
1, CR-gelnet - columns 4 and 5 and glasso - column 6) the BIC criterion leads to better models
than the cross-validation in terms of classification performance, as the optimal models (i.e.
black circle) are closer to the top-left corner of the ROC curve than the models selected by
cross-validation (i.e. black cross).

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.9394
(0.0179)
0.8818
(0.0207)
0.9072
(0.0185)
0.8153
(0.0309)
0.7099
(0.0362)
0.8740
(0.0201)
0.7606
(0.0409)

2S-gelnet 2S-gelnet
(AND)
(OR)
0.9955
0.9962
(0.0048) (0.0039)
0.9958
0.9960
(0.0039) (0.0036)
0.9972
0.9962
(0.0033) (0.0041)
0.9847
0.9742
(0.0065) (0.0078)
0.9795
0.9700
(0.0051) (0.0056)
0.9963
0.9959
(0.0031) (0.0035)
0.9155
0.8629
(0.0185) (0.0214)

CR-gelnet
(L2)
0.9495
(0.0175)
0.9084
(0.0208)
0.9088
(0.0235)
0.8474
(0.0359)
0.8241
(0.0296)
0.9057
(0.0241)
0.7588
(0.0416)

CR-gelnet
(MinEl)
0.9456
(0.0206)
0.9186
(0.0262)
0.9004
(0.0267)
0.8665
(0.0212)
0.8657
(0.0271)
0.9150
(0.0256)
0.7710
(0.0266)

glasso
0.9399
(0.0175)
0.8818
(0.0200)
0.9072
(0.0185)
0.8140
(0.0293)
0.7100
(0.0362)
0.8748
(0.0202)
0.7582
(0.0416)

Table 1: Average Accuracy (Std. Dev. in brackets) - BIC calibration (n = 1000)

21

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.8013
(0.0373)
0.7038
(0.0338)
0.7545
(0.0338)
0.5946
(0.0475)
0.5094
(0.0251)
0.7133
(0.0336)
0.4226
(0.0473)

2S-gelnet
(AND)
0.9994
(0.0013)
0.9968
(0.0049)
0.9996
(0.0011)
0.9803
(0.0093)
0.9734
(0.0092)
0.9980
(0.0035)
0.8861
(0.0224)

2S-gelnet
(OR)
0.9991
(0.0021)
0.9949
(0.0066)
0.9996
(0.0012)
0.9693
(0.0104)
0.9659
(0.0072)
0.9962
(0.0048)
0.8258
(0.0284)

CR-gelnet
(L2)
0.7118
(0.0340)
0.6270
(0.0433)
0.6517
(0.0422)
0.5523
(0.0520)
0.5315
(0.0282)
0.6341
(0.0421)
0.4654
(0.0499)

CR-gelnet
(MinEl)
0.7239
(0.0524)
0.6655
(0.0506)
0.5707
(0.0598)
0.6017
(0.0444)
0.6347
(0.0335)
0.6628
(0.0533)
0.4772
(0.0520)

glasso
0.8034
(0.0360)
0.7145
(0.0339)
0.7595
(0.0313)
0.6038
(0.0505)
0.5113
(0.0302)
0.7182
(0.0319)
0.4305
(0.0441)

Table 2: Average Accuracy (Std. Dev. in brackets) - 5-CV calibration (n = 1000)

Tables 1 and 2 report the average accuracy (standard deviation in brackets) for the BIC and
5-fold cross validation, respectively. Bold values indicate best result. Notice that the 2 stages
procedure 2S-gelnet consistently outperforms all other methods in terms of accuracy. When we
use BIC calibration, 2S-gelnet (OR) is better than AND rule for scale-free and random networks,
while 2S-gelnet (AND) is always better or equal to 2S-gelnet (OR) when 5-fold cross-validation
is considered. CR-gelnet tends to outperform glasso and gelnet for BIC calibration (see Table
1), while reporting mixed results for 5-fold cross-validation (see Table 2).

22

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.6932
(0.0647)
0.6712
(0.0374)
0.5853
(0.0519)
0.6731
(0.0356)
0.5726
(0.0303)
0.6809
(0.0345)
0.5982
(0.0318)

2S-gelnet 2S-gelnet
(AND)
(OR)
0.9683
0.9734
(0.0328) (0.0270)
0.9829
0.9838
(0.0155) (0.0145)
0.9790
0.9720
(0.0238) (0.0294)
0.9612
0.9362
(0.0157) (0.0180)
0.9497
0.9280
(0.0118) (0.0123)
0.9865
0.9852
(0.0111) (0.0126)
0.8160
0.7311
(0.0330) (0.0298)

CR-gelnet
(L2)
0.7315
(0.0675)
0.7256
(0.0445)
0.5922
(0.0669)
0.7151
(0.0486)
0.6889
(0.0360)
0.7419
(0.0499)
0.6095
(0.0396)

CR-gelnet
(MinEl)
0.7186
(0.0794)
0.7507
(0.0590)
0.5715
(0.0693)
0.7397
(0.0304)
0.7440
(0.0387)
0.7622
(0.0547)
0.6203
(0.0272)

glasso
0.6949
(0.0635)
0.6711
(0.0362)
0.5853
(0.0519)
0.6713
(0.0332)
0.5727
(0.0303)
0.6822
(0.0348)
0.5959
(0.0318)

Table 3: Average F1 -score (Std. Dev. in brackets) - BIC calibration (n = 1000)

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.4065
(0.0460)
0.4483
(0.0271)
0.3467
(0.0314)
0.4836
(0.0305)
0.4408
(0.0131)
0.4835
(0.0286)
0.3930
(0.0194)

2S-gelnet
(AND)
0.9955
(0.0098)
0.9871
(0.0190)
0.9971
(0.0080)
0.9508
(0.0220)
0.9360
(0.0208)
0.9927
(0.0126)
0.7671
(0.0355)

2S-gelnet
(OR)
0.9934
(0.0146)
0.9796
(0.0246)
0.9971
(0.0092)
0.9253
(0.0231)
0.9191
(0.0155)
0.9861
(0.0172)
0.6830
(0.0362)

CR-gelnet
(L2)
0.3183
(0.0261)
0.3925
(0.0271)
0.2719
(0.0237)
0.4589
(0.0300)
0.4523
(0.0147)
0.4233
(0.0263)
0.4118
(0.0230)

CR-gelnet
(MinEl)
0.3311
(0.0442)
0.4200
(0.0370)
0.2333
(0.0256)
0.4879
(0.0301)
0.5149
(0.0236)
0.4449
(0.0387)
0.4175
(0.0255 )

glasso
0.4088
(0.0445)
0.4575
(0.0283)
0.3510
(0.0298)
0.4896
(0.0322)
0.4420
(0.0163)
0.4878
(0.0276)
0.3962
(0.0181)

Table 4: Average F1 -score (Std. Dev. in brackets) - 5-CV calibration (n = 1000)

23

When considering the mean F1 -scores, reported in Tables 3 and 4 for BIC calibration and 5-fold
cross-validation, respectively, results are qualitatively similar, with 2S-gelnet clearly outperforming all other methods, reaching values very close to 1. The other methods instead report
quite unsatisfactory performances in comparison, especially CR-gelnet for 5-fold cross validation.
Finally, Tables 5 and 6 report the average Frobenius distance between the theoretical partial
correlation matrix and the estimated one. The lower the value, the closer is the estimate to the
true model. Similarly to the classification performances, the values in these tables show that
the 2S-gelnet performs better then other methods, using both BIC criterion approach and 5-fold
cross-validation.

gelnet
Scale-Free

0.4169
(0.0472)
Random
0.5965
(0.0418)
Hub
0.3084
(0.0312)
Cluster
0.9486
(0.0740)
Band
1.1918
(0.1045)
Small-World
0.5693
(0.0436)
Core-Periphery
2.0539
(0.1492)

2S-gelnet 2S-gelnet
(AND)
(OR)
0.2369
0.2276
(0.0628) (0.0582)
0.2758
0.2712
(0.0397) (0.0368)
0.1884
0.1999
(0.0572) (0.0651)
0.3794
0.3987
(0.0431) (0.0395)
0.3612
0.3727
(0.0405) (0.0432)
0.2815
0.2780
(0.0347) (0.0334)
0.5885
0.7003
(0.1001) (0.1718)

CR-gelnet
(L2)
0.4372
(0.0419)
0.5764
(0.0462)
0.3446
(0.0261)
0.8028
(0.0654)
0.9540
(0.0879)
0.5681
(0.0467)
1.4341
(0.1394)

CR-gelnet
(MinEl)
0.4352
(0.0483)
0.5532
(0.0464)
0.3870
(0.0339)
0.7262
(0.0550)
0.8147
(0.0752)
0.5426
(0.0480)
1.1791
(0.0943)

glasso
0.4174
(0.0469)
0.5958
(0.0427)
0.3084
(0.0312)
0.9454
(0.0740)
1.1919
(0.1045)
0.5699
(0.0409)
2.0477
(0.1510)

Table 5: Average Frobenius distance (Std. Dev. in brackets) - BIC calibration (n = 1000)

24

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.3937
(0.0397)
0.5160
(0.0288)
0.4087
(0.0308)
0.7179
(0.0360)
0.8113
(0.0493)
0.5045
(0.0362)
1.1909
(0.1027)

2S 2S-gelnet
(AND)
0.1828
(0.0405)
0.2608
(0.0432)
0.1374
(0.0382)
0.3944
(0.0588)
0.3688
(0.0416)
0.2606
(0.0293)
0.6252
(0.0978)

2S-gelnet
(OR)
0.1870
(0.0466)
0.2708
(0.0455)
0.1364
(0.0393)
0.4260
(0.0808)
0.3792
(0.0443)
0.2757
(0.0463)
0.6640
(0.0857)

CR-gelnett
(L2)
0.4148
(0.0395)
0.5020
(0.0321)
0.4427
(0.0343)
0.6405
(0.0371)
0.6550
(0.0478)
0.5010
(0.0405)
0.8655
(0.0652)

CR-gelnett
(MinEl)
0.4419
(0.0473)
0.5224
(0.0334)
0.5340
(0.0451)
0.6433
(0.0364)
0.6282
(0.0479)
0.5254
(0.0468)
0.8096
(0.0596)

glasso
0.3928
(0.0391)
0.5154
(0.0291)
0.4062
(0.0325)
0.7219
(0.0354)
0.8143
(0.0509)
0.5042
(0.0360)
1.2073
(0.0994)

Table 6: Average Frobenius distance (Std. Dev. in brackets) - 5-CV calibration (n = 1000)

Appendix B.2 reports the results with sample size of 200. We see that the results in the highdimensional case are similar to the ones discussed in this section when considering the classification performances. Looking at accuracy and F1 -scores in Tables 13, 14 and Tables 15, 16,
they qualitatively hold (with only one exception when CR-gelnet (L2) slightly outperforms 2Sgelnet (AND) in term of F1 -score using BIC calibration, see Table 15). Still, if we look at the
optimally selected models reported in ROC curves, we don’t always have an estimator whose
optimal estimates are clearly the highest and leftmost in the [0, 1] × [0, 1] square. One difference is that in the high-dimensional situation 2S-gelnet (AND) is better 2S-gelnet (OR) only
when considering cluster, band and small-world networks looking both at average accuracy and
F1 -score. Looking at the Frobenius distance in Tables 17 and 18, most of the times there is at
least one version (between AND/OR rules) of 2S-gelnet that has better performance than other
estimators. There are 2 only two exceptions, where CR-gelnet outperforms 2S-gelnet (see Table
25

18). There are also 4 situations in which only one of version of 2S-gelnet is outperformed by
other algorithms, see Tables 17 and 18.
Summing up, empirical results suggest that the 2-stages estimator 2S-gelnet tends to have a
remarkable performance no matter what the network structure is, the parameter selection procedure used (i.e. BIC or 5-fold cross-validation) and the high/low-dimensional set-up. Empirical
results show only 2 cases where both versions of 2S-gelnet are outperformed by CR-gelnet. CRgelnet tends to outperform the glasso and gelnet when relying on BIC calibration, while it has
often unsatisfactory performance for 5-fold cross-validation. Gelnet and glasso performances
are quite close to each other. This is not surprising as the optimal choice for α is often equal to
1, which then results in avoiding the penalty based on the 2-norm and just relying on the 1-norm
(see Tables 9 and 10 in appendix B.1 for more details). This pattern in the optimal selection of
α is replicated also in the high-dimensional case (see Tables 11 and 12 in appendix B.2), but
with a notable exception with the core-periphery structure (i.e. Net 7). In the low-dimensional
case, 2S-gelnet (AND) performs the best in terms of F1 -score and it is quite closely followed by
2S-gelnet (OR), while if we look at the average accuracy it depends on the network structure.
In the high-dimensional case there isn’t a clear winner between the two versions of 2S-gelnet,
but they still outperform all other methods.

26

5

The Unites States network of economic sectors

In this section, we focus on the use of the 2S-gelnet to estimate the network across different US
economic sectors. Such a network could play an important role in the dynamics of aggregate
macroeconomic fluctuations (see for example [24]). The network is estimated by using market
data log-returns of S&P 500 sectors’ indices, being then the so-called perceived network, as
pointed out for example by [4]. This estimated network could differ from the underlying realworld one, even if we had a perfect estimator. Nonetheless the perceived network could be
estimated from more readily available market data and it can provide useful information about
market expectations relative to economic inter-linkages among sectors.
We use daily time series of log-returns of the 10 economic sector indices constituents of S&P
500 in the period 01/01/2018-25/11/2020. To rule out possible serial correlation and volatility
clustering, we don’t estimate the network directly on the daily log-returns. First, we fit autoregressive models (AR(1)) with generalized auto-regressive conditional heteroskedasticity (i.e.
GARCH(1,1)). Then, we use the residuals as inputs for the graphical models to estimate the
perceived network using 2S-gelnet (AND) with the BIC criterion. We consider both intervals for
each year as well the entire 3 years period. Figure 2 displays the 4 optimal estimated networks.
In red, we point out edges corresponding to negative partial correlations, while blue edges
represent positive partial correlations. The width of the edge is proportional to the magnitude
of the partial correlation estimates.

27

Estimated network 2018

Estimated network 2019

energy consumer non−cyclicals

p corr>0

energy consumer non−cyclicals

p corr>0

p corr<0

p corr<0

consumer cyclicals

financials

healthcare

basic materials

industrials

healthcare

utilities

real estate

basic materials

industrials

technology

utilities

real estate

Estimated network 2020

technology

Estimated network 2018−2020

energy consumer non−cyclicals

p corr>0

consumer cyclicals

financials

energy consumer non−cyclicals

p corr>0

p corr<0

p corr<0

consumer cyclicals

financials

healthcare

basic materials

industrials

healthcare

utilities

real estate

consumer cyclicals

financials

basic materials

industrials

technology

utilities

real estate

Figure 2: Estimated networks using 2S-gelnet (AND)

28

technology

Always included−excluded relations
2018−2020 (2S−gelnet)

Stable relations 2018−2020 (2S−gelnet)
energy consumer non−cyclicals

Stable

energy consumer non−cyclicals

Included

Unstable

Excluded

consumer cyclicals

financials

healthcare

basic materials

industrials

healthcare

utilities

real estate

consumer cyclicals

financials

basic materials

industrials

technology

utilities

real estate

technology

Figure 3: Stable relations (left) and Always included-excluded relations (right)

Figure 3 reports on the left the stable relations (in green) among all the dependencies estimated
in each of the 3 years and on the right the always included, or stable, relations (in green) and
the always excluded relations (in red). So, for example, the connection between the industrial
and basic materials sectors is a stable one across 2018, 2019 and 2020, while the connection
between technology and basic materials is not stable because it is only detected in 2018. The estimated networks can then help to better point out hidden relationships to better capture potential
spillover effects among sectors. Furthermore, we notice that some connections have been never
detected, such as the one between real estate and technology. The numbers of estimated links
in the entire period has been equal to 26, which is consistent with the 27 links estimated during
2018 and 2020, while only in 2019, we had a lower number of estimated links (i.e. 23). Therefore, the estimated structure suggests that, at least in the perceived network, only a bit more than
half of the possible connections are relevant, as we know that the complete graph would have
45 edges in total. The increase in the number of connections detected between 2019 and 2020,
could also be possibly attributed to the Covid-19 pandemic. It is well-known that correlations in
29

financial markets tend to increases during crisis and therefore new connections among sectors
could emerge, suggesting other channels of contagions across sectors, which financial actors
might want to monitor to cope with risk aversion and a more uncertain scenario. Some new
links emerge in the network estimated using 2020 data. In fact, we detect, for example, positive
correlations between industrials and real estate and between financial and real estate. The first
relation might be associated to a general slowdown of construction industry, at least in the first
months since the spread of the pandemic. The second connection may remind us about a link
that played a crucial role in the past financial crisis of 2008.
Table 7 displays mean values of some common network measures to analyze the characteristics
of the estimated networks. In fact, network analysis can be a useful tool to study relations
among economic entities, see for example Jackson [25] and Borgatti et al. [26]. The degree
of a node is the number of edges connected to the node. This gives us an indication about
how many sectors are connected to one sector. The distance between two nodes is the length
of a shortest path joining the two nodes. Ideally the shorter the distance, the faster a shock
propagates. Eccentricity of a node is the maximum distance between that node and all other
nodes in the network. Eccentricity could give us also a measure of diffusion timing. The (local)
clustering coefficient measures the fraction of connections, over the total possible number of
connections, among the neighbors of a node. This can suggest us how much the economic
sectors tend to cluster together. Lastly, we report the mean value of strength. The strength of
a node is the sum of the weights of all edges connected to that node [27]. Here, the weights
are the partial correlations. This measure suggests then how intense, or strong, are overall the
connections among sectors and could be used to detect potential crisis period. Looking at these
characteristics in Table 7, we observe a decrease of the mean values of degree and clustering
coefficient between 2018 and 2019 followed by an increase between 2019 and 2020. This is in
line with the sparser network detected in 2019 (with 23 edges). The average strength follows
30

a similar pattern, with a more noticeable increase between 2019 and 2020 probably fostered
by Covid-19 pandemic. The higher mean value of distance in 2019 is also in line with the
sparser graph detected and it could also point out a slight increase in time of shock’s diffusion.
Nonetheless the mean values of distance and eccentricity are small, suggesting that a shock
will propagate quickly in the network. Looking at the average eccentricity we can see that,
on average, a shock in a sector reach other sectors in a bit more than 2 steps. With only 10
nodes it is difficult to classify these networks in a specific category, nonetheless they show
small average distance and noticeable clustering, even if it is not extremely high. These are two
core characteristics of small-world graphs [13].

Measure (mean values)
Degree
Distance
Eccentricity
Clustering
Strength

2018
5.400
1.422
2.200
0.591
0.891

2019
4.600
1.511
2.200
0.450
0.870

2020
5.400
1.422
2.200
0.555
0.944

2018-2020
5.200
1.422
2.000
0.547
0.939

Table 7: Network measures for estimated graphs using 2S-gelnet (AND)
Finally, in Table 8, we report results from the simulation of diffusion of a shock. In fact, after
estimating the network structure and potential channels of contagion, it is interesting to try
to evaluate the potential impact of a shock and the network resilience. Following the method
suggested by Anufriev and Panchenko [4], we use the partial correlation matrix P to simulate a
positive shock of magnitude 1 in the financial sector. Then, we rely on equation (21) to evaluate
not only the direct effects, but also the second and higher order effects:
s∞ =

∞
X

Pt e = (I − P)−1 e

(21)

t=0

where s∞ is the final steady-state of the effects of the initial vector of shocks e, here e =
[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]> , where the fifth element of e corresponds to the financial sector.
31

Period
2018 2019
2020
Basic materials
3.207 2.643 10.745
Consumer cyclicals
3.502 3.039 8.229
Consumer non-cyclicals 1.593 1.350 6.885
Energy
1.769 1.849 6.507
Financials
3.545 3.656 12.254
Healthcare
3.376 1.675 7.126
Industrials
4.070 3.804 12.877
Real estate
1.093 0.679 8.674
Technology
3.502 2.981 8.062
Utilities
0.289 0.473 7.016

18-20
6.356
5.582
3.935
4.092
7.308
4.575
8.079
4.412
5.653
3.520

Table 8: Final steady-states after a unitary shock in financial sector

From Table 8 the estimated effect of a shock differs from year to year. So, for example, even if
most of the characteristics reported in Table 7 are similar for 2018 and 2020, the effect of a shock
in the financial sector is different in this two years, with a much larger impact in 2020. This
could depend not only on a ”rewiring” of edges, but also on the strength of the relations among
sectors given by partial correlations in P, which can be detected by looking at the weights of the
estimated sectoral network (see the mean values of strength in Table 7). Figure 4 also plots the
evolution of the average strength of networks, estimated using a rolling window approach with
window size equal to one year. Notice how the Covid-19 pandemic could have played a role in
the increase in average strength, as there is a sharp increase from January 2020 to March 2020.
Also the average strength stays at very high level for all 2020, suggesting a prolonged effect on
the degree and level of interconnections across sectors, with potential systemic consequences.

32

0.92

0.90

20

20
11
/

10
/

20

20
09
/

08
/

20

20

20
07
/

06
/

05
/

20

20

20

19

20
04
/

03
/

02
/

01
/

12
/

19

19

19

19

19
11
/

10
/

09
/

08
/

07
/

19

19
06
/

19

05
/

04
/

19

19

19
03
/

02
/

01
/

18

0.88

12
/

Strength (mean value)

0.94

Figure 4: Mean value of strength estimated on a rolling window of 12 months (with shifts of 1
month)

6

Conclusions

In this article, we propose three methods to compute a sparse estimate of the precision matrix Θ
in multivariate Gaussian settings. The estimated precision matrix can then be used to reconstruct
the conditional dependence graph of the related undirected graphical model. Also, with a proper
rescale of Θ, we can derive a symmetric matrix of partial correlations, which can be employed in
real-world applications to detect potential dependencies across economic sectors and spillover
effects. All the methods proposed rely on the elastic net penalty. Our first method, gelnet,
exploits penalized log-likelihood estimation, while the second method, CR-gelnet, consists of
p penalized regressions with a subsequent procedure to make the estimate symmetric. The last
and third method, 2S-gelnet, is a two steps procedure that at first estimate the sparsity pattern
in the precision matrix and then estimate the precision matrix elements using a log-likelihood
33

procedure with constraints given by the sparsity pattern. We perform extensive simulations
to test the proposed methods on a large set of well-known network structures, representing
different types of conditional dependence graph in Gaussian graphical models.
The main result of our analysis through simulations is that the two stages graphical elastic net
(2S-gelnet) performs better than the other two, not only accordingly to F1 -score and accuracy,
but also to Frobenius distance. Using BIC calibration instead of cross-validation is not relevant
when we are using 2S-gelnet, while it leads to different estimated networks when we are using
the remaining two methods, gelnet and CR-gelnet. Moreover, we notice that the degree of
correctness in identification of the network is dependent on the underlying structure. According
to our simulations, the core-periphery topology is in fact the most problematic structure to
estimate.
Finally, we present an empirical study of the network among 10 US economic sectors. We
use the daily prices of sector indices from S&P500 and the 2S-gelnet to estimate the relations
among these sectors and thereby to estimate the perceived network. We point out the network
connections as well as the properties, evaluating the potential impact of a shock through the
contagion channels. Moreover, we study the evolution of the network strength in time, which
increases significantly in correspondence to the emergence of the Covid-19 pandemic in Europe
and USA, suggesting that such an event has potential systemic impact on all the economic
sectors.

Acknowledgments
The authors would like to thank participants to the 14th International Conference on Computational and Financial Econometrics (December 19-21, 2020) conference for the helpful
comments

34

References
[1] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of
the Royal Statistical Society (Series B), 67:301–320, 2005.
[2] S. Lauritzen. Graphical Models. Oxford University Press, 1996.
[3] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques.
MIT Press, 2009.
[4] M. Anufriev and V. Panchenko. Connecting the dots: Econometric methods for uncovering
networks with an application to the australian financial institutions. European Journal of
Banking and Finance, 61:241–255, 2015.
[5] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society. Series B (Methodological), 58(1):267–288, 1996.
[6] N. Meinshausen and P. Bühlmann. High-dimensional graphs and variable selection with
the lasso. The Annals of Statistics, 34(3):1436–1462, 2006.
[7] O. Banerjee, L. El Ghaoui, and A. d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. Journal of Machine
Learning Research, 9:485–516, 2008.
[8] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the
graphical lasso. Biostatistics, 9(3):432–441, 2008.
[9] M. Cucuringu, J. Puente, and D. Shue. Model selection in undirected graphical models
with the elastic-net. arXiv:1111.0559, 2011.

35

[10] S. Ryali, K. Supekar T. Chen, and V. Menon. Estimation of functional connectivity in
fmri data using stability selection-based sparse partial correlation with elastic net penalty.
NeuroImage, 59(4):3852–3861, 2012.
[11] B. Liu, L. Jing, J. Yu, and J. Li. Robust graph learning via constrained elastic-net regularization. Neurocomputing, 171:299–312, 2016.
[12] A-L. Barabasi and R. Albert. Emergence of scaling in random networks. Science,
286:509–512, 1999.
[13] D. J. Watts and S. H. Strogatz. Collective dynamics of ’small-world’ networks. Nature,
393:440–442, 1998.
[14] M. Yuan. High dimensional inverse covariance matrix estimation via linear programming.
Journal of Machine Learning Research, 11:2261–2286, 2010.
[15] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
[16] J. Friedman, T. Hastie, and R. Tibshirani. Reguralization paths for generalized linear
models via coordinate descent. Journal of Statistical Software, 33(1):1–22, 2010.
[17] M. Bogdan, S. Lee, and P. Sobczyk. Sparse inverse covariance matrix estimation with
graphical slope. Technical report, 2018.
[18] K.V. Mardia, J.T. Kent, and J.M. Bibby. Multivariate Analysis. Academic Press, 1979.
[19] T. Cai, W. Liu, and Xi Luo. A constrained l1 minimization approach to sparse precision
matrix estimation. Journal of the American Statistical Association, 106(494):594–607,
2011.

36

[20] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer, 2009.
[21] G. Torri, R. Giacometti, and S. Paterlini. Robust and sparse banking network estimation.
European Journal of Operational Research, 270(1):51–65, 2018.
[22] P. Bühlmann S. van de Geer. Statistics for High-Dimensional Data: Methods, Theory and
Applications. Springer, 2011.
[23] R. Foygel and M. Drton. Extended bayesian information criteria for gaussian graphical
models. Advances in Neural Information Processing Systems (NIPS 2010), 23:604–612,
2010.
[24] D. Acemoglu, V. M. Carvalho, A. Ozdaglar, and A. Tahbaz-Salehi. The network origins
of aggregate fluctuations. Econometrica, 80:1977–2016, 2012.
[25] M. O. Jackson. Social and Economic Networks. Princeton University Press, 2008.
[26] S. P. Borgatti, A. Mehra, D. J. Brass, and G. Labianca. Network analysis in the social
sciences. Science, 323:892–895, 2009.
[27] A. Barrat, M. Barthelemy, R. Pastor-Satorras, and A. Vespignani. The architecture of
complex weighted networks. PNAS, 101:3747–3752, 2004.

37

Appendices
A

Adjacency Matrices

Adjacency matrix − Scale−Free

Adjacency matrix − Random

1

Adjacency matrix − Hub

1

1

value

value

value

NON ZERO

NON ZERO

NON ZERO

ZERO

ZERO

ZERO

30

30
1

30

30
1

Adjacency matrix − Cluster

30

1

Adjacency matrix − Band

1

Adjacency matrix − Small−World

1

1

value

30

value

value

NON ZERO

NON ZERO

NON ZERO

ZERO

ZERO

ZERO

30
1

30

30

30
1

30

1

Adjacency matrix − Core−Periphery
1

value
NON ZERO
ZERO

30
1

30

38

30

B
B.1

Additional simulation results
Additional results with sample size = 1000

Tables 9, 10, 11 and 12 show the average value of optimal α (standard deviation in brackets,
networks’ numbers as listed in Section 3, sample size 1000 and 200):
Net 1
gelnet
0.9992
(0.0046)
2S-gelnet (AND)
0.5458
(0.2267)
2S-gelnet (OR)
0.5717
(0.2444)
CR-gelnet (L2)
0.9883
(0.0170)
CR-gelnet (MinEl)
0.9817
(0.0207)

Net 2
0.9950
(0.0121)
0.5383
(0.1497)
0.7008
(0.2101)
0.9825
(0.0199)
0.9783
(0.0260)

Net 3
1.0000
(0.0000)
0.4367
(0.1685)
0.5133
(0.2390)
0.9917
(0.0152)
0.9867
(0.0194)

Net 4
0.9967
(0.0086)
0.8500
(0.0983)
0.9025
(0.0925)
0.9742
(0.0275)
0.9750
(0.0254)

Net 5
0.9983
(0.0063)
0.9150
(0.0860)
0.9050
(0.1018)
0.9883
(0.0205)
0.9792
(0.0294)

Net 6
0.9950
(0.0102)
0.6192
(0.2418)
0.7133
(0.1893)
0.9817
(0.0207)
0.9842
(0.0232)

Net 7
0.9917
(0.0137)
0.8925
(0.0791)
0.9575
(0.0451)
0.9767
(0.0227)
0.9700
(0.0368)

Net 6
1.0000
(0.0000)
0.7517
(0.1964)
0.8008
(0.2024)
1.0000
(0.0000)
0.9992
(0.0046)

Net 7
1.0000
(0.0000)
0.8642
(0.1298)
0.8892
(0.0921)
0.9975
(0.0076)
0.9833
(0.0379)

Table 9: α optimal values using BIC - n = 1000

Net 1
gelnet
1.0000
(0.0000)
2S-gelnet (AND)
0.4358
(0.1395)
2S-gelnet (OR)
0.4583
(0.1473)
CR-gelnet (L2)
1.0000
(0.0000)
CR-gelnet (MinEl)
0.9983
(0.0063)

Net 2
1.0000
(0.0000)
0.7592
(0.1902)
0.8158
(0.1713)
1.0000
(0.0000)
0.9942
(0.0142)

Net 3
1.0000
(0.0000)
0.4725
(0.0658)
0.4992
(0.0860)
0.9992
(0.0046)
1.0000
(0.0000)

Net 4
1.0000
(0.0000)
0.8508
(0.1450)
0.8675
(0.1594)
0.9992
(0.0046)
0.9933
(0.0245)

Net 5
1.0000
(0.0000)
0.8200
(0.1833)
0.9258
(0.0880)
0.9983
(0.0063)
0.9942
(0.0204)

Table 10: α optimal values using 5-CV - n = 1000

39

B.2

Results with sample size = 200

Net 1
0.9992
(0.0046)
2S-gelnet (AND)
0.7008
(0.2121)
2S-gelnet (OR)
0.7325
(0.1786)
CR-gelnet (L2)
0.9925
(0.0238)
CR-gelnet (MinEl)
0.9900
(0.0155)
gelnet

Net 2
0.9992
(0.0046)
0.8108
(0.1469)
0.8267
(0.1129)
0.9858
(0.0224)
0.9767
(0.0270)

Net 3
1.0000
(0.0000)
0.7042
(0.1934)
0.7508
(0.1613)
0.9950
(0.0121)
0.9842
(0.0180)

Net 4
0.9958
(0.0095)
0.9225
(0.0752)
0.9208
(0.0838)
0.9858
(0.0276)
0.9833
(0.0240)

Net 5
0.9933
(0.0130)
0.9208
(0.0763)
0.9617
(0.0439)
0.9908
(0.0154)
0.9800
(0.0240)

Net 6
0.9983
(0.0063)
0.7808
(0.1700)
0.8558
(0.1471)
0.9867
(0.0170)
0.9817
(0.0245)

Net 7
0.9975
(0.0076)
0.7867
(0.1750)
0.8158
(0.1743)
0.9908
(0.0167)
0.9808
(0.0252)

Net 6
1.0000
(0.0000)
0.6400
(0.2165)
0.7642
(0.1954)
1.0000
(0.0000)
0.9975
(0.0076)

Net 7
0.9358
(0.1324)
0.5025
(0.1413)
0.6892
(0.1946)
0.9983
(0.0063)
0.5392
(0.3234)

Table 11: α optimal values using BIC - n = 200

Net 1
1.0000
(0.0000)
2S-gelnet (AND)
0.5958
(0.1662)
2S-gelnet (OR)
0.8150
(0.1511)
CR-gelnet (L2)
1.0000
(0.0000)
CR-gelnet (MinEl)
0.9900
(0.0548)
gelnet

Net 2
1.0000
(0.0000)
0.6242
(0.2238)
0.7517
(0.2065)
1.0000
(0.0000)
0.9992
(0.0046)

Net 3
1.0000
(0.0000)
0.5483
(0.1393)
0.8292
(0.1292)
1.0000
(0.0000)
1.0000
(0.0000)

Net 4
1.0000
(0.0000)
0.7825
(0.1930)
0.7267
(0.2093)
0.9992
(0.0046)
0.9958
(0.0115)

Net 5
1.0000
(0.0000)
0.9067
(0.1006)
0.9008
(0.1146)
1.0000
(0.0000)
1.0000
(0.0000)

Table 12: α optimal values using 5-CV - n = 200

40

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.9472
(0.0195)
0.9005
(0.0262)
0.9217
(0.0267)
0.8574
(0.0319)
0.8031
(0.0390)
0.8828
(0.0254)
0.8397
(0.0223)

2S-gelnet 2S-gelnet
(AND)
(OR)
0.9754
0.9823
(0.0094) (0.0065)
0.9677
0.9695
(0.0134) (0.0081)
0.9762
0.9892
(0.0100) (0.0076)
0.9312
0.9187
(0.0114) (0.0133)
0.9178
0.8979
(0.0148) (0.0158)
0.9691
0.9638
(0.0089) (0.0103)
0.8734
0.8787
(0.0098) (0.0098)

CR-gelnet
(L2)
0.9550
(0.0171)
0.9172
(0.0285)
0.9436
(0.0204)
0.8669
(0.0285)
0.8360
(0.0470)
0.9100
(0.0238)
0.8574
(0.0173)

CR-gelnet
(MinEl)
0.9530
(0.0139)
0.9231
(0.0231)
0.9349
(0.0274)
0.8773
(0.0314)
0.8480
(0.0370)
0.9112
(0.0284)
0.8658
(0.0154)

glasso
0.9470
(0.0195)
0.9004
(0.0266)
0.9217
(0.0267)
0.8569
(0.0325)
0.8031
(0.0398)
0.8828
(0.0256)
0.8405
(0.0214)

Table 13: Average Accuracy (Std. Dev. in brackets) - BIC calibration (n = 200)

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.8218
(0.0342)
0.7398
(0.0366)
0.7926
(0.0266)
0.6702
(0.0321)
0.5763
(0.0357)
0.7520
(0.0306)
0.6588
(0.0413)

2S-gelnet 2S-gelnet
(AND)
(OR)
0.9785
0.9841
(0.0081) (0.0066)
0.9641
0.9664
(0.0162) (0.0135)
0.9711
0.9942
(0.0193) (0.0072)
0.9149
0.9018
(0.0184) (0.0206)
0.8966
0.8617
(0.0224) (0.0296)
0.9589
0.9581
(0.0152) (0.0141)
0.8759
0.8771
(0.0127) (0.0102)

CR-gelnet
(L2)
0.8311
(0.0331)
0.7513
(0.0309)
0.7804
(0.0335)
0.6898
(0.0263)
0.6381
(0.0419)
0.7480
(0.0360)
0.7137
(0.0325)

CR-gelnet
(MinEl)
0.8631
(0.0315)
0.7990
(0.0374)
0.8215
(0.0402)
0.7289
(0.0405)
0.7078
(0.0563)
0.7918
(0.0432)
0.6870
(0.0665)

glasso
0.8283
(0.0337)
0.7484
(0.0332)
0.7958
(0.0275)
0.6789
(0.0342)
0.5895
(0.0392)
0.7566
(0.0281)
0.6710
(0.0312)

Table 14: Average Accuracy (Std. Dev. in brackets) - 5-CV calibration (n = 200)

41

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.7162
(0.0726)
0.7041
(0.0551)
0.6316
(0.0825)
0.7006
(0.0374)
0.6267
(0.0330)
0.6915
(0.0437)
0.5878
(0.0267)

2S-gelnet 2S-gelnet
(AND)
(OR)
0.8390
0.8783
(0.0520) (0.0416)
0.8774
0.8822
(0.0430) (0.0275)
0.8417
0.9247
(0.0590) (0.0484)
0.8301
0.8035
(0.0268) (0.0272)
0.8178
0.7759
(0.0272) (0.0286)
0.8900
0.8730
(0.0293) (0.0331)
0.6118
0.6239
(0.0253) (0.0320)

CR-gelnet
(L2)
0.7500
(0.0676)
0.7443
(0.0660)
0.7022
(0.0701)
0.7301
(0.0371)
0.7016
(0.0541)
0.7468
(0.0490)
0.6122
(0.0265)

CR-gelnet
(MinEl)
0.7358
(0.0587)
0.7560
(0.0555)
0.6702
(0.0937)
0.7505
(0.0444)
0.7197
(0.0482)
0.7516
(0.0600)
0.6066
(0.0281)

glasso
0.7153
(0.0728)
0.7039
(0.0556)
0.6316
(0.0825)
0.7003
(0.0382)
0.6268
(0.0339)
0.6914
(0.0443)
0.5888
(0.0257)

Table 15: Average F1 -score (Std. Dev. in brackets) - BIC calibration (n = 200)

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.4319
(0.0454)
0.4800
(0.0341)
0.3854
(0.0314)
0.5326
(0.0234)
0.4776
(0.0212)
0.5191
(0.0307)
0.4691
(0.0281)

2S-gelnet 2S-gelnet
(AND)
(OR)
0.8492
0.8834
(0.0468) (0.0449)
0.8623
0.8691
(0.0500) (0.0392)
0.8184
0.9587
(0.0890) (0.0466)
0.7992
0.7727
(0.0330) (0.0319)
0.7846
0.7301
(0.0341) (0.0379)
0.8599
0.8566
(0.0422) (0.0402)
0.5963
0.6140
(0.0304) (0.0290)

CR-gelnet
(L2)
0.4451
(0.0483)
0.4915
(0.0303)
0.3725
(0.0349)
0.5479
(0.0204)
0.5177
(0.0286)
0.5157
(0.0366)
0.5168
(0.0247)

CR-gelnet
(MinEl)
0.4968
(0.0569)
0.5456
(0.0455)
0.4241
(0.0603)
0.5824
(0.0347)
0.5731
(0.0471)
0.5649
(0.0510)
0.5102
(0.0412)

glasso
0.4410
(0.0454)
0.4881
(0.0323)
0.3891
(0.0315)
0.5391
(0.0261)
0.4856
(0.0238)
0.5234
(0.0292)
0.4752
(0.0234)

Table 16: Average F1 -score (Std. Dev. in brackets) - 5-CV calibration (n = 200)

42

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.9606
(0.0855)
1.3067
(0.0819)
0.6730
(0.0694)
2.0306
(0.1764)
2.7983
(0.2547)
1.2784
(0.1155)
2.6523
(0.0521)

2S-gelnet 2S-gelnet
(AND)
(OR)
0.8646
0.7754
(0.1244) (0.1220)
0.9879
0.9766
(0.1040) (0.0972)
0.8162
0.5948
(0.1600) (0.1534)
1.4731
1.5541
(0.1645) (0.1714)
1.4456
1.7339
(0.2668) (0.3060)
1.0332
1.0564
(0.1243) (0.1312)
2.5540
2.5386
(0.0658) (0.0766)

CR-gelnet
(L2)
1.0110
(0.0903)
1.3110
(0.1025)
0.7923
(0.0636)
1.8228
(0.1710)
2.2333
(0.2748)
1.3079
(0.1160)
2.6623
(0.0718)

CR-gelnet
(MinEl)
1.0137
(0.0847)
1.2653
(0.1011)
0.8792
(0.0662)
1.6742
(0.1420)
1.8659
(0.1753)
1.2450
(0.1000)
2.6952
(0.0586)

glasso
0.9600
(0.0856)
1.3064
(0.0825)
0.6730
(0.0694)
2.0291
(0.1768)
2.7995
(0.2553)
1.2789
(0.1153)
2.6535
(0.0509)

Table 17: Average Frobenius distance (Std. Dev. in brackets) - BIC calibration (n = 200)

gelnet
Scale-Free
Random
Hub
Cluster
Band
Small-World
Core-Periphery

0.8710
(0.0527)
1.1257
(0.0605)
0.8164
(0.0800)
1.5540
(0.0841)
1.8879
(0.1100)
1.1340
(0.0780)
2.4513
(0.0539)

2S-gelnet 2S-gelnet CR-gelnet CR-gelnet
glasso
(AND)
(OR)
(L2)
(MinEl)
0.8818
0.7911
0.8881
0.9215
0.8684
(0.0987) (0.1309)
(0.0554)
(0.0587) (0.0522)
1.0743
1.0670
1.1049
1.1260
1.1262
(0.1381) (0.1175)
(0.0661)
(0.0695) (0.0596)
0.8538
0.4724
0.8432
0.8944
0.8139
(0.1698) (0.1922)
(0.0841)
(0.0848) (0.0804)
1.5160
1.5786
1.4468
1.4244
1.5650
(0.2032) (0.2450)
(0.0997)
(0.0893) (0.0870)
1.4020
1.5212
1.6073
1.5173
1.9210
(0.2447) (0.2680)
(0.1080)
(0.1061) (0.1265)
1.0784
1.0780
1.1288
1.1505
1.1338
(0.1346) (0.1328)
(0.0809)
(0.0854) (0.0783)
2.5927
2.5544
2.3939
2.3299
2.4544
(0.0683) (0.0671)
(0.0623)
(0.1323) (0.0463)

Table 18: Average Frobenius distance (Std. Dev. in brackets) - 5-CV calibration (n = 200)

43

ROC Curves − Scale−Free (n = 200)
1.00

TPR

0.75

0.50

0.25

gelnet

2S−gelnet (AND)

2S−gelnet (OR)

CR−gelnet (L2)

CR−gelnet (MinEl)

glasso

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●●
●●
●●●●●●●
●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●
●●
●
●●
●●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●●
●●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●
●
●●
●●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●●
●●
●
●● ●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●●
●●
●●●●●●●
●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00
0.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

Opt
●

No

●

Yes

Selection
●

BIC
CV

1.00

FPR

ROC Curves − Random (n = 200)
1.00

TPR

0.75

0.50

0.25

gelnet

2S−gelnet (AND)

2S−gelnet (OR)

CR−gelnet (L2)

CR−gelnet (MinEl)

glasso

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

Opt
●

No

●

Yes

Selection
●

BIC
CV

0.00
0.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.00

FPR

ROC Cu ves − Hub n = 200

TPR

S
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

AND

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●●●
●●
●●
●
●●●
●●
●
●●●●
●
●●●●●●
●●●●
●●●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

S

OR

CR

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
● ●
●
●
●●
●●●
●
●●
●●●●
●●●●●●●●●●●●●●●● ●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●●●●●●●●●●●● ● ●● ● ●● ● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

CR

M E

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●●
●
●●
●●
●●
●●●●
●●
●●
●
●●●
●●
●●●●●●
●
●●●●
●●●
●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●●●●●
●
●●●●●● ●● ● ● ● ● ● ● ● ●
●
●
●
●
●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

Op
N
Y

Se ec on
BC
CV

FPR

ROC Cu ves − C us e n = 200
S

AND

S

OR

CR

CR

M E

Op
N

TPR

Y

Se ec on
BC
CV

FPR

44

ROC Curves − Band (n = 200)
1.00

TPR

0.75

0.50

0.25

0.00

gelnet

2S−gelnet (AND)

2S−gelnet (OR)

CR−gelnet (L2)

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●● ●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

CR−gelnet (MinEl)

glasso

●
●
●
●
●
●
●
●
●
●
●
●●●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

0.75

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●●●
●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

1.000.00

0.25

0.50

0.75

Opt
●

No

●

Yes

Selection
●

BIC
CV

1.00

FPR

ROC Curves − Small−World (n = 200)
1.00

TPR

0.75

0.50

0.25

gelnet

2S−gelnet (AND)

2S−gelnet (OR)

CR−gelnet (L2)

CR−gelnet (MinEl)

glasso

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
● ● ● ●● ●●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●●●
●
●
●●
●●
●●●●●●
●●●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●● ● ●
●●●
●●
●●●
●●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●●
● ● ●●
●●●●●● ●
● ●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●●●
●●●
●
●●
●
●●●
●
●●
●●● ●
●● ● ●
●● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
● ● ● ●● ●●●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

0.00
0.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

1.000.00

0.25

0.50

0.75

Opt
●

No

●

Yes

Selection
●

BIC
CV

1.00

FPR

ROC Cu ves − Co e−Pe phe y n = 200

TPR

S
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

AND

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●● ●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●● ●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

S

OR

CR

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●●
●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

CR

M E

●
●● ●
●
●●
●●
●
●●
●●●●
●●
●
●●
●●
● ●
●
●●●●●
●●
●●●● ●● ● ●
●
●
●●
● ●● ●
●●
●●
●●
●
● ●
●
●● ●
●●●
●
●
●
●
●
●
●
●
●
●● ●●
●
●
●
●● ●●●
●
●
●
●●●●
●●
●
●
●
●
●
●●●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

FPR

Figure 5: ROC curves for seven network structures (in rows) for gelnet (column 1) 2S-gelnet
(columns 2 & 3) CR-gelnet (columns 4 & 5) and glasso (column 6) - (n=200)

45

Op
N
Y

Se ec on
BC
CV

