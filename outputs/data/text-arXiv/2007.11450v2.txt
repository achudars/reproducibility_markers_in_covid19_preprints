JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

4S-DT: Self Supervised Super Sample
Decomposition for Transfer learning with
application to COVID-19 detection

arXiv:2007.11450v2 [eess.IV] 15 Sep 2020

Asmaa Abbas, Mohammed M. Abdelsamea, and Mohamed Medhat Gaber

Abstract—Due to the high availability of large-scale annotated
image datasets, knowledge transfer from pre-trained models
showed outstanding performance in medical image classification.
However, building a robust image classification model for datasets
with data irregularity or imbalanced classes can be a very
challenging task, especially in the medical imaging domain.
In this paper, we propose a novel deep convolutional neural
network, we called Self Supervised Super Sample Decomposition
for Transfer learning (4S-DT) model. 4S-DT encourages a coarseto-fine transfer learning from large-scale image recognition
tasks to a specific chest X-ray image classification task using
a generic self-supervised sample decomposition approach. Our
main contribution is a novel self-supervised learning mechanism
guided by a super sample decomposition of unlabelled chest X-ray
images. 4S-DT helps in improving the robustness of knowledge
transformation via a downstream learning strategy with a classdecomposition layer to simplify the local structure of the data.
4S-DT can deal with any irregularities in the image dataset
by investigating its class boundaries using a downstream classdecomposition mechanism. We used 50,000 unlabelled chest Xray images to achieve our coarse-to-fine transfer learning with an
application to COVID-19 detection, as an exemplar. 4S-DT has
achieved a high accuracy of 99.8% (95% CI: 99.44 %, 99.98%)
in the detection of COVID-19 cases on a large dataset and an
accuracy of 97.54% (95% CI: 96.22%, 98.91%) on an extended
test set enriched by augmented images of a small dataset, out
of which all real COVID-19 cases were detected, which was the
highest accuracy obtained when compared to other methods.
Index Terms—self-supervision; chest X-ray image classification; transfer learning; data irregularities; convolutional neural
network.

I. I NTRODUCTION
Diagnosis of COVID-19 is associated with the symptoms
of pneumonia and chest X-ray tests [1]. Chest X-ray is the
essential imaging technique that plays an important role in the
diagnosis of COVID-19 disease. Fig. 1 shows examples of a) a
normal chest X-ray, a positive one with COVID-19, a positive
image with the severe acute respiratory syndrome (SARS), and
b) some examples of other unlabelled chest X-ray images used
in this work.
Several statistical machine learning methods have been
previously used for automatic classification of digitised lung
A. Abbas is with Mathematics Department, University of Assiut, Assiut,
Egypt
M. Abdelsamea is with School of Computing and Digital Technology,
Birmingham City University, Birmingham, UK and Mathematics Department,
University of Assiut, Assiut, Egypt
M. Gaber is with School of Computing and Digital Technology, Birmingham
City University, Birmingham, UK

Fig. 1: Examples of a) labelled chest X-ray images (from
left to right: normal, COVID-19, and SARS images), and
b) unlabelled chest X-ray images used in this work for selfsupervision learning.

images [2], [3]. For instance, in [4], a small set of three statistical features were calculated from lung texture to distinguish
between malignant and benign lung nodules using a Support
Vector Machine SVM classifier. A statistical co-occurrence
matrix method was used with Backpropagation Network [5]
to classify samples from being normal or cancerous. With the
high availability of enough annotated image data, deep learning
approaches [6]–[8] usually provide a superiority performance
over the statistical machine learning approaches. Convolutional
Neural Networks (CNN) is one of the most commonly used
deep learning approaches with superior achievements in the
medical imaging domain [9]. The primary success of CNN
is due to its capability to learn local features automatically
from domain-specific images, unlike the statistical machine
learning methods. One of the popular strategies for training
a CNN model is to transfer learned knowledge from a pretrained network that fulfilled one generic task into a new
specific task [10]. Transfer learning is faster and easy to apply
without the need for a huge annotated dataset for training;
therefore many scientists tend to adopt this strategy especially
with medical imaging. Transfer learning can be accomplished
with three main scenarios [11]: a) “shallow tuning”, which
adapts only the classification layer in a way to cope with
the new task, and freezes the weights of the remaining layers
without updating; b) “deep tuning” which aims to retrain all the
weights of the adopted pre-trained network from end-to-end;

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

and (c) “fine-tuning” that aims to gradually train layers by
tuning the learning parameters until a significant performance
boost is achieved. Transfer knowledge via fine-tuning scenario
demonstrated outstanding performance in chest X-ray and
computed tomography image classification [12], [13].
The emergence of COVID-19 as a pandemic disease dictated
the need for faster detection methods to contain the spread of
the virus. As aforementioned, chest X-ray imaging comes in
as a promising solution, particularly when combined with
an effective machine learning model. In addition to data
irregularities that can be dealt with through class decomposition,
scarcity of data, especially in the early months of the pandemic,
made it hard to realise the adoption of chest X-ray images as a
means for detection. On the other hand, self-supervised learning
is being popularised recently to address the expensive labelling
of data acquired at an unprecedented rate. In self-supervised
learning, unlabelled data is used for feature learning by assigning each example a pseudo label. In the case of convolutional
neural networks (CNN) applied on image data, each image is
assigned a pseudo label, and CNN is trained to extract visual
features of the data. The training of a CNN by pseudo labelled
images as input is called pretext task learning. While the
training of the produced CNN from the pretext training using
labelled data is called downstream task training. Inherently such
a pipeline allows for effective utilisation of large unlabelled
data sets. The success of the pretext task learning relies on
pseudo labelling methods. In [14], four categories of methods
were identified. Context-based image feature learning by means
of context similarity has demonstrated a particularly effective
pseudo labelling mechanism. DeepCluster [15] is the stateof-the-art method under this category. DeepCluster is a super
sample decomposition method that generates pseudo labels
through the clustering of CNN features. Sample decomposition
is the process of applying clustering on the whole training
set as a step for improving supervised learning performance
[16]. When the clustering is performed on a larger data sample,
we refer to this process as a super sample decomposition.
However, we argue that the coupling of the pretext task and
the pseudo labelling can limit the effectiveness of the pretext
task in the self-supervised learning process. In our proposed
super sample decomposition, the pretext task training uses
cluster assignments as pseudo labels, where the clustering
process is decoupled from the pretext training. We propose the
clustering of encoded images through an auto-encoder neural
network, allowing flexibility of utilising different features and
clustering methods, as appropriate. We argue that this can be
most effective in medical image classification, evident by the
experimentally validated use of class decomposition for transfer
learning in a method coined as DeTraC [17].
In this paper, we propose a novel deep convolutional neural
network, we term Self Supervised Super Sample Decomposition
for Transfer learning (4S-DT) model for the detection of
COVID-19 cases 1 . 4S-DT has been designed in a way
to encourage a coarse-to-fine transfer learning based on a
self-supervised sample decomposition approach. 4S-DT can
deal with any irregularities in the data distribution and the
1 The

developed code is available at https://github.com/asmaa4may/4S-DT.

2

limited availability of training samples in some classes. The
contributions of this paper can be summarised as follows. We
provide
• a novel mechanism for self-supervised sample decomposition using a large set of unlabelled chest X-ray images
for a pretext training task;
• a generic coarse-to-fine transfer learning strategy to gradually improve the robustness of knowledge transformation
from large-scale image recognition tasks to a specific
chest X-ray image classification task;
• a downstream class-decomposition layer in the downstream training phase to cope with any irregularities in
the data distribution and simplify its local structure; and
• a thorough experimental study on COVID-19 detection,
pushing the boundaries of state-of-the-art techniques in
terms of accuracy, and robustness of the proposed model.
The paper is organised as follow. In Section II, we review
the state-of-the-art methods for COVID-19 detection. Section
III discusses the main components of our proposed 4S-DT
model. Section IV describes our experiments on several chest
X-ray images collected from different hospitals. In Section V,
we discuss our findings and conclude the work.
II. P REVIOUS WORK ON COVID-19 DETECTION FROM
CHEST X- RAY
In February 2020, the World Health Organisation (WHO)
has declared that a new virus called COVID-19 has started
to spread aggressively in several countries [18]. Diagnosis
of COVID-19 is typically associated with pneumonia-like
symptoms, which can be revealed by both genetic and imaging
tests. Fast detection of the virus will directly contribute to
managing and controlling its spread. Imaging tests, especially
chest X-ray, can provide fast detection of COVID-19 cases. The
historical conception of medical image diagnostic systems has
been comprehensively explored through an enormous number
of approaches ranging from statistical machine learning to
deep learning. A convolutional neural network is one of the
most effective approaches in the diagnosis of lung diseases
including COVID-19 directly from chest X-ray images. Several
recent reviews have been carried out to highlight significant
contributions to the detection of COVID-19 [19]–[21]. For
instance, in [22], a modified version of ResNet-50 pre-trained
CNN model has been used to classify CT images into three
classes: healthy, COVID-19 and bacterial pneumonia. In [23],
a CNN model, called COVID-Net, based on transfer learning
was used to classify chest X-ray images into four classes:
normal, bacterial infection, non-COVID, and COVID-19 viral
infection. In [24], a weakly-supervised approach has been
proposed using 3D chest CT volumes for COVID-19 detection
and lesion localisation relying on ground truth masks obtained
by an unsupervised lung segmentation method and a 3D ResNet
pre-trained model. In [25], a dataset of chest X-ray images
from patients with pneumonia, confirmed COVID-19 disease,
and normal incidents, was used to evaluate the performance of
the state-of-the-art CNN models based on transfer learning. The
study suggested that transfer learning can provide important
biomarkers for the detection of COVID-19 cases. It has been

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

experimentally demonstrated that transfer learning can provide representation vector hd and the reconstructed image x̂ can be
a robust solution to cope with the limited availability of training defined as
samples from confirmed COVID-19 cases [26].
hd = f (W (1) x + b(1) )
(1)
In [27], self-supervised learning using context distortion
(2)
d
(2)
is applied for classification, segmentation, and localisation in
x̂ = f (W h + b )
(2)
different medical imaging problems. When used in classification, the method was applied for scan plane detection in fetal
where W (1) and W (2) are the weight matrices, b(1) and
(2)
2D ultrasound images, showing classification improvement in b
are the bias vectors, and f is the active function. The
some settings. However, we argue that our proposed method reconstruction error L(x, x̂) between x̂ and x is defined as
is more effective in image segmentation and localisation,
because context distortion is able to generate localised features,
1
2
(3)
L(x, x̂) = kx − x̂k
instead of global image features that can be more effective for
2
classification tasks.
The overall cost function of the n0 unlabelled
Having reviewed the related work, it is evident that despite
the great success of deep learning in the detection of COVID-19 images,EAE (W, b), can be defined as
cases from chest X-ray images, data scarcity and irregularities


have not been explored. It is common in medical imaging in
nl −1 X
sl sX
n0
l+1
X
1
λ X
(l)
particular that datasets exhibit different types of irregularities
i
i


EAE (W, b) =
L(x , x̂ ) +
(Wji )2
n0 i=1
2
(e.g. overlapping classes with imbalance problems) that affect
l=1 i=1 j=1
the resulting accuracy of deep learning models. With the
(4)
unfolding of COVID-19, chest X-ray images are rather scarce.
Thus, this work focuses on coping with data irregularities
where the first term denotes the reconstruction error of the
through class decomposition, and data scarcity through super whole datasets, and the second term is the regularisation weight
sample decomposition, as detailed in the following section.
penalty term, which aims to prevent over-fitting by restraining
the magnitude of the weights. λ is the weight decay parameter,
III. 4S-DT MODEL
nl is the layer number of the network, sl denotes the neuron
(l)
This section describes, in sufficient details, our proposed deep number in layer l , and Wji is the connecting weight between
convolutional neural network, Self Supervised Super Sample neuron i in layer l + 1 and neuron j in layer l.
Decomposition for Transfer learning (4S-DT) model for detectOnce the training of the AE has been accomplished, Densitying COVID-19 cases from chest X-ray images. Starting with an Based Spatial Clustering of Applications with Noise (DBSCAN)
overview of the architecture through to the different components is used to cluster the image data distribution X into a number
of the model, the section discusses the workflow and formalises of classes c based on the extracted features hd . DBSCAN is
the method. 4S-DT model consists of three training phases, an unsupervised clustering algorithm, which is a considerably
see Fig. 2. In the first phase, we train an autoencoder model to representative density-based clustering algorithm that defines
extract deep local features from each sample in a super large clusters as the largest set of points connected by density.
set of unlabelled generic chest X-ray images. Then we adapted
Let the image dataset X be mapped into a low-dimensional
0
a sample decomposition mechanism to create pseudo labels for feature space denoted by H ∈ Rn ×d , where H =
the generic chest X-ray images. In the second phase, we use (h1 , h2 , ..., hn0 ). An image xi (represented by hi ) is densitythe pseudo labels to achieve a coarse transfer learning using connected to image xj (represented by hj ) with respect to Eps
an ImageNet pre-trained CNN model for the classification of (i.e. neighbourhood radius) and M inP ts (i.e. the minimum
pseudo-labelled chest X-ray images (as a pretext training task), number of objects within the neighbourhood radius of core
resulting in a chest X-ray-related convolutional features. In the object) if there exists a core object xk such that both xi and xj
last phase, we use trained convolutional features to achieve are directly density-reachable from xk with respect to Eps and
downstream training. The downstream training task is more M inP ts. An image xi is directly density-reachable from an
task-specific by adapting a fine transfer learning from chest X- image xj if xi is within the Eps-neighbourhood of NEps (xj ),
ray recognition to COVID-19 detection. In this stage, we also and xj is a core object, where Eps-neighbourhood can be
adapt a class-decomposition layer to simplify the local structure defined as
of the image data distribution, where a sophisticated gradient
descent optimisation method is used. Finally, we apply a classNEps (xj ) = {xi ∈ X|dis(xi , xj ) ≤ Eps}.
(5)
composition to refine the final classification of the images.
A. Super sample decomposition
0

Given a set of unlabelled images X = {x1 , x2 , ..., xn }, our
super sample decomposition component aims to find and use
pseudo labels during the pretext training task of 4S-DT. To
this end, an autoencoder (AE) is first used to extract deep
features associated to each image. For each input image x, the

DBSCAN results in C clusters, where each cluster is
constructed by maximising the density reachability relationship
among images of the same cluster. The C cluster labels will
be assigned to the n0 unlabelled images and will be presented
as pseudo labels for the pretext training task and hence the
downstream training task. The pseudo-labelled image dataset
can then be defined as X 0 = {(xi , y c )|c ∈ C}.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Fig. 2: Graphical representation of 4S-DT model.

B. Pretext training

minority and majority classes. Large-scale annotated image
datasets (such as ImageNet) provide effective solutions to such
a challenge via transfer learning where tens of millions of
parameters (of CNN architectures) are required to be trained.

With the high availability of large-scale annotated image
datasets, the chance for the different classes to be wellrepresented is high. Therefore, the learned in-between classboundaries are most likely to be generic enough to new
A shallow-tuning mode was used during the adaptation and
samples. On the other hand, with the limited availability of training of an ImageNet pre-trained CNN model using the
annotated medical image data, especially when some classes collected chest X-ray image dataset. We used the off-the-shelf
are suffering more compared to others in terms of the size CNN features of pre-trained models on ImageNet (where the
and representation, the generalisation error might increase. training is accomplished only on the final classification layer)
This is because there might be a miscalibration between the to construct the image feature space.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

Mini-batch of stochastic gradient descent (mSGD) was
used to minimise the categorical cross entropy loss function,
Ecoarse (·)

Ecoarse y c , z 0 (xj , W 0 )

= −

C
X


y c ln z 0 xj , W 0 , (6)

c=1
c
where x is the set of self-labelled images in
 the training, y
0
j
0
is their associated self labels while z x , W is the predicted
output from a softmax function, where W 0 is the converged
weight matrix associated to the ImageNet pre-trained model
(i.e. we used W 0 of ImageNet pre-trained CNN model for
weight initialisation to achieve a coarse transfer learning).

training data, stochastic gradient descent (SGD) can heavily
be fluctuating the objective/loss function and hence overfitting
can occur. To improve convergence and overcome overfitting,
the mini-batch of stochastic gradient descent (mSGD) was used
to minimise the objective function, Ef ine (·), with categorical
cross-entropy loss

j

C. Downstream training
A fine-tuning mode was used during the adaptation of
ResNet model using feature maps from the coarse transfer
learning stage. However, due to the high dimensionality
associated with the images, we applied PCA to project the highdimension feature space into a lower-dimension, where highly
correlated features were ignored. This step is important for the
downstream class-decomposition process in the downstream
training phase to produce more homogeneous classes, reduce
the memory requirements, and improve the efficiency of the
framework.
Now assume that our feature space (PCA’s output) is
represented by a 2-D matrix (denoted as dataset A), and L is
a class category. A and L can be rewritten as


a11 a11 . . .
a1m
 a21 a22 . . .
a2m 


A= .
.
.
..  , L = {l1 , l2 , . . . , lc0 } , (7)
..
..
 ..
.
an1

an2

...

anm

where n is the number of images, m is the number of
features, and c0 is the number of classes. For downstream classdecomposition, we used k-means clustering [28] to further
divide each class into homogeneous sub-classes (or clusters),
where each pattern in the original class L is assigned to a class
label associated with the nearest centroid based on the squared
euclidean distance (SED):
SED =

k X
n
X

(j)

k ai − cj k,

(8)

j=1 i=1

where centroids are denoted as cj . Once the clustering is
accomplished, each class in L will further be divided into k
subclasses, resulting in a new dataset (denoted as dataset B).
Accordingly, the relationship between dataset A and B can be
mathematically described as:
0

A = (A|L) 7→ B = (B|C )

Ef ine


gl , z(o , Ŵ )
i

j

= −

ck
X



gli ln z oj , Ŵ , (10)

i=1

where oj is the set of input labelled
 images
 in the training,
gli is the ground truth labels, while z oj , Ŵ is the predicted
output from a softmax function, where Ŵ is the converged
weight matrix associated to the coarse transfer learning model.
Performance evaluation
In the downstream class-decomposition layer of 4S-DT, we
divide each class within the image dataset into several subclasses, where each subclass is treated as a new independent
class. In the composition phase, those sub-classes are assembled
back to produce the final prediction based on the original image
dataset.For performance evaluation, we adopted Accuracy
(ACC), Specificity (SP) and Sensitivity (SN) metrics for multiclasses confusion matrix, the input image can be classified
into one of (c0 ) non-overlapping classes. As a consequence,
the confusion matrix would be a (Nc0 × Nc0 ) matrix and the
matrices are defined as:
0

Accuracy(ACC)

=

c
1X
T Pi + T Ni
(11)
,
0
c i=1 T Pi + T Ni + F Pi + F N i

=

c
T Pi
1X
,
0
c i=1 T Pi + F Ni

=

c
T Ni
1X
,
c0 i=1 T Ni + F Pi

0

Sensitivity(SN )

(12)

0

Specificity(SP )

(13)

where c0 is the original number of classes in the dataset, T P
is the true positive in case of COVID-19 case and T N is the
true negative in case of normal or other disease, while F P and
F N are the incorrect model predictions for COVID-19 and
other cases. Also, the T P , T N , F P and F N for a specific
class i are defined as:

T Pi =

n
X

xii

(14)

xjk , j 6= i, k 6= i

(15)

i=1

(9)

where the number of instances in A is equal to B while
C encodes the new labels of the subclasses (e.g. C0 =
{l11 , l12 , . . . , l1k , l21 , l22 , . . . , l2k , . . . lc0 k }).
For transfer learning, we used ResNet [29] model, which
showed excellent performance with only 18 layers. Here we
consider freezing the weights of low-level layers and update
weighs of high-level layers. With the limited availability of
0

0



T Ni =

c X
c
X
j=1 k=1

F Pi =
F Ni =

c
X

xji , j 6= i

(16)

xij , j 6= i,

(17)

j=1
c
X
j=1

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

TABLE I: The distribution of classes in COVID-19 dataset-A.
Type
Training set
Augmented training set
Testing set 1
Testing set 2

COVID-19
74
662
31
283

SARS
8
69
3
30

Normal
56
504
24
216

Total
138
1235
58
529

TABLE II: The distribution of classes in COVID-19 dataset-B.
Class Name
COVID-19
Normal
Pneumonia

Train
460
1266
3418

Test
116
317
855

Total
576
1583
4273

All the experiments in our work have been carried out in
where xii is an element in the diagonal of the matrix. Having MATLAB 2019a on a PC with the following configuration: 3.70
discussed and formalised the 4S-DT model in this section in GHz Intel(R) Core(TM) i3-6100 Duo, NVIDIA Corporation
detail, the following section validates the model experimentally. with the donation of the Quadra P5000GPU, and 8.00 GB
The model establishes the effectiveness of self-supervised super RAM.
sample decomposition in detecting COVID-19 from chest X-ray
images.
B. Self supervised training of 4S-DT
We trained our autoencoder with 80 neurons in the first
hidden layer and 50 neurons in the second hidden layer for the
This section presents the datasets used in training and reconstruction of input unlabelled images, see Fig 3. The trained
evaluating our 4S-DT model, and discusses the experimental autoencoder is then used to extract a set of deep features from
results.
the unlabelled chest X-ray images. The extracted features were
fed into the DBSCAN clustering algorithm for constructing
the clusters (and hence the pseudo-labels). Since DBSCAN is
A. Datasets
sensitive to the neighbourhood radius, we employed a k-nearestIn this work, we used three datasets of labelled and unlabelled
neighbour (k-NN) [37] search to determine the optimal (Eps)
chest X-ray images, defined respectively as:
value. As demonstrated in Fig 4, the optimal value for Eps was
• Unlabelled chest X-ray dataset, a large set of chest
1.861. M inP ts parameter has been derived from the number
X-ray images used as an unlabelled dataset: A set of of features (d) such that M inP ts ≥ d + 1. Consequently, we
50,000 unlabelled chest X-ray images collected from three used and tested different values for M inP ts parameter such
different datasets: 1) 336 cases with a manifestation of as 51, 54, and 56 resulting in 13, 6, and 4 clusters respectively.
tuberculosis, and 326 normal cases from [30], [31]; 2) For the coarse transfer learning, we used ResNet18 pre-trained
5,863 chest X-Ray images with 2 categories: pneumonia CNN model. The classification performance, on the pseudoand normal from [32]; and 3) a set of 43,475 chest X-ray labelled samples, associated with the 13, 6, and 4 clusters were
images randomly selected from a total of 112,120 chest 48.1%, 53.26%, and 64.37%, respectively. Therefore, we fix
X-ray images, including 14 diseases, available from [33]. the number of clusters (and hence the number of pseudo labels)
• COVID-19 dataset-A, an imbalanced set of labelled chest
to be 4 in all experiments in this work.
X-ray with COVID-19 cases: 80 normal cases from [34],
[35], and chest X-ray dataset from [36], which contains
105 and 11 cases of COVID-19 and SARS, respectively.
We divided the dataset into two groups: 70% for training
and 30% for testing. Due to the limited availability of
training images, we applied different data augmentation
techniques (such as: flipping up/down and right/left,
translation and rotation using random five different angles)
to generate more samples, see Table I.
• COVID-19 dataset-B, we used a public chest X-ray
dataset that already divided into two sets (training and
testing), each set consists of three classes (e.g. COVID19, Normal, and Pneumonia), see Table II. The dataset
is available for download at: (https://www.kaggle.com/ Fig. 3: An example of a reconstructed chest X-ray image by
prashant268/chest-xray-covid19-pneumonia).
our Autoencoder.
Note that chest X-ray images of dataset-A and datset-B are
progressively updated and the distributions of images in these
1) Downstream class-decomposition of 4S-DT: We used
datasets (e.g. Tables I and II) can be considered as a snapshot AlexNet [38] pre-trained network based on a shallow learning
at the time of submitting this paper. Therefore, any attempt mode to extract discriminative features of the labelled dataset.
to compare the performance of methods on such datasets at We set a value of 0.0001 for the learning rate, except the last
different points in time would be misleading. Moreover, the fully connected layer (was 0.01), the min-batch size was 128
performance of the methods reported in this paper is expected with the minimum of 256 epochs, 0.001 was set for the weight
to improve in the future with the growing availability of labelled decay to prevent the overfitting through training the model,
images.
and 0.9 for the momentum speed. At this stage, 4096 attributes
IV. E XPERIMENTAL R ESULTS

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

TABLE III: The adopted ResNet architecture used in the finetuning study in our experiments.
Layer Name
Conv1
Conv2
Conv3
Conv4
Conv5

ResNet_18
7 × 7 , 64, stride (2)
Layer-Res2a
Layer-Res2b
Layer-Res3a
Layer-Res3b
Layer-Res4a
Layer-Res4b
Layer-Res5a
Layer-Res5b

Output Size
112 × 112 × 64
56 × 56 × 64
28 × 28 × 128
14 × 14 × 256
7 × 7 × 512

TABLE IV: Classification measurements obtained by 4S-DT
without both 4S-D and class-decomposition (w/o 4S-D+CD)
and without 4S-D (w/o 4S-D) only on the 58 labelled chest
X-ray images (i.e. testing set 1).
Fig. 4: Estimation of the optimal Eps.

were obtained, therefore we used PCA to reduce the dimension
of feature space. For the class decomposition step, we used
k-means clustering [28], where k has been selected to be 2
and hence each class in L has been further divided into two
subclasses, resulting in a new dataset with six classes. The
adoption of k-means for class decomposition with k = 2 is
based on the results achieved by the DeTraC model in [17].
C. Classification performance on COVID-19 dataset-A

(w/o 4S-D+CD)
ACC (%) SN (%) SP (%)
89.66%
96.77%
81.48%

ACC (%)
93.10%

(w/o 4S-D)
SN (%)
96.77 %

SP (%)
88.89 %

and downstream class-decomposition components. Also, we
applied 4S-DT based on ResNet18 pre-trained network on
the original classes of COVID-19 dataset with an imbalance
classes after eliminating the samples from the training set. As
we see in Fig. 5, 4S-DT has achieved 96.43% accuracy (95%
CI: 92.5%, 98.6%) in the detection of COVID-19 cases with
sensitivity 97.1% (95% CI: 92.24%, 97.76%) and 95.60 %
(95% CI: 93.41%, 96.5%) for specificity.

We first validate the performance of 4S-DT with ResNet18
(as the backbone network) on the 58 test images (i.e. testing set
1), where augmented training set is used for training, see Table
I. Our ResNet architecture consists of residual blocks and each
block has two 3 × 3 Conv layers, where each layer is followed
by batch normalisation and a ReLU activation function. Our
ResNet architecture consists of residual blocks and each block
has two 3 × 3 Conv layers, where each layer is followed by
batch normalisation and a ReLU activation function. Table III
illustrates the adopted architecture used in our experiment.
During the training of the backbone network, the learning Fig. 5: Confusion matrix obtained by a) 4S-DT on the 58 test
rate for all the CNN layers was fixed to 0.0001 except for the set, and b) 4S-DT when trained on augmented images only
last fully connected layer (was 0.01) to accelerate the learning. and tested on the 196 cases.
The mini-batch size was 256 with a minimum of 200 epochs,
0.0001 was set for the weight decay to prevent the overfitting
To allow for further investigation and make testing of
through training the model, and the momentum value was 0.95. COVID-19 detection more challenging, we applied the same
The schedule of drop learning rate was set to 0.95 every 5 data augmentation techniques (used for the training samples)
epochs. The results were summarised in Table V. Moreover, to the small testing set to increase the number of testing
we also compare the performance of the proposed model samples. The new test sample distribution, we called testing set
without the self supervised sample decomposition component 2, contains 283 COVID-19 images, 30 SARS images, and 216
(i.e. w/o 4S-D or DeTraC-ResNet18 [17]) and without both normal images, see Table I. Consequently, we used testing set 2
4S-D and class-decomposition (w/o 4S-D+CD or ResNet18 for testing and augmented training set for training (see Table I),
[29] pre-trained network on) the 58 testing set. 4S-DT has unless otherwise mentioned, for the performance evaluation of
achieved 100% accuracy in the detection of COVID-19 cases all methods in the experiments described below. We validated
with 100% (95% confidence interval (CI): 96.4%, 98.7%) for the performance of a) the full version of 4S-DT with 4Ssensitivity and specificity (95% CI: 94.5%, 100%), see Fig. 5. D component and b) without 4S-D. For a fair comparison,
As illustrated by Fig. 5 and Table IV, 4S-DT shows a superiority we used the same backbone network (i.e. ResNet18) with
and a significant contribution in improving the transfer learning the downstream class-decomposition component, where both
process with both the self supervised sample decomposition versions have been trained in a shallow and fine-tuning

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

mode. As illustrated by Table V, 4S-D component shows
significant improvement in shallow- and fine-tuning modes
in all cases. More importantly, our full version model with
4S-D demonstrates better performance, in each case, with less
number of epochs, confirming its efficiency and robustness at
the same time.
Moreover, we compared the classification performance of 4SDT with other models used for COVID-19 detection, including
GoogleNet [39], DeTraC. 4S-DT has achieved a high accuracy
of 97.54% (95% CI: 96.22%, 98.91%) with a specificity of
97.15% (95% CI: 94.23%, 98.85%) and sensitivity of 97.88%
(95% CI: 95.46%, 99.22%) on the 529 test chest X-ray images
of test set 2, see Table VI. Moreover, as shown by Table VI, 4SDT has demonstrated superiority in performance, confirming
its effectiveness in improving the classification accuracy of
transfer learning models. Finally, Fig. 6 shows the Area Under
the receiver curve (AUC) between the true positive rate and
false positive rate obtained by 4S-DT, with AUC value of
99.58% (95% CI: 99.01%, 99.95%), to confirm its robustness
behaviours during the training process.

8

of 99.3%(95%CI : 93.91%, 99.79%), andspecif icityof 100%
(95% CI: 99.69%, 100%) in the detection of COVID-19 cases.
V. D ISCUSSION AND CONCLUSION

The diagnosis of COVID-19 is associated with the
pneumonia-like symptoms that can be revealed by genetic and
imaging tests. Chest X-ray imaging test provides a promising
fast detection of COVID-19 cases and consequently can
contribute to controlling the spread of the virus. In medical
image classification, paramount progress has been made using
ImageNet pre-trained convolutional neural networks (CNNs),
exploiting the high availability of large-scale annotated image
datasets. The historical conception of such approaches has been
comprehensively explored through several transfer learning
strategies, including fine-tuning and deep-tuning mechanisms.
They usually require an enormous number of balanced annotated images distributed over several classes/diseases (which
is impractical in the medical imaging domain). In medical
image analysis, data irregularities still remain a challenging
problem, especially with the limited availability of confirmed
samples with some diseases such as COVID-19 and SARS,
which usually results in miscalibration between the different
classes in the dataset. Consequently, COVID-19 detection from
chest X-ray images presents a challenging problem due to the
irregularities and the limited availability of annotated cases.
Here, we propose a new CNN model, we called Self
Supervised Super Sample Decomposition for Transfer learning
(4S-DT) model. 4S-DT has been designed to cope with
such challenging problems by adapting a self-supervised
sample decomposition approach to generate pseudo-labels
for the classification of unlabelled chest X-ray images as
a pretext learning task. 4S-DT has also the ability to deal
with data irregularities by a class-decomposition adapted in
its downstream learning component. 4S-DT has demonstrated
its effectiveness and efficiency in coping with the detection
of COVID-19 cases in a dataset with irregularities in its
distribution. In this work, we used 50,000 unlabelled chest Xray images for the development of our self-supervised sample
decomposition approach to perform transfer learning with an
application to COVID-19 detection. We achieved an accuracy of
Fig. 6: ROC curve obtained during the training of 4S-DT with 97.54% with a specificity of 97.15% and sensitivity of 97.88%
ResNet pre-trained model.
on 529 test chest X-ray images (of COVID-19 dataset-A), i.e.
testing set 2, with 283 COVID-19 samples. We also achieved
a high accuracy of 99.8% in the detection of COVID-19 cases
D. Classification performance on COVID-19 dataset-B
of COVID-19 dataset-B.
With the continuous collection of data, we aim in the future
To evaluate the performance of 4S-DT on COVID-19 datasetto
extend the development and validation of 4S-DT with
B, we applied different ImageNet pre-trained CNN networks
multi-modality
datasets, including clinical records. As a future
such as: VGG19 [41], ResNet [42], GoogleNet [39], and
development,
we
also aim to add an explainability component
Mobilenetv2 [43]. Parameter settings for each pre-trained
to
increase
the
trustworthiness
and usability of 4S-DT. Finally,
model during the training process are reported in Table
one
can
use
model
pruning
and
quantisation to improve the
VII.Transfer learning has been accomplished via deep tuning
efficiency
of
4S-DT,
allowing
deployment
on handheld devices.
scenario (with 15 epochs and SGD was the optimiser). The
classification performance on COVID19 cases was reported
in Table VIII. Fig 7 illustrates the confusion matrix obtained
R EFERENCES
by each pre-trained Networks for each class in the dataset.
[1] H. Shi, X. Han, N. Jiang, Y. Cao, O. Alwalid, J. Gu, Y. Fan, and C. Zheng,
As demonstrated by Table VIII, 4S-DT has achieved a high
“Radiological findings from 81 patients with covid-19 pneumonia in
accuracy of 99.8% (95% CI: 99.44 %, 99.98%), with sensitivity
wuhan, china: a descriptive study,” The Lancet Infectious Diseases, 2020.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

TABLE V: Overall classification performance of 4S-DT model with and without self-supervised sample decomposition (4S-D)
component, on testing set 2.
Layer

Shallow
Res5b
Res5a
Res4b
Res4a
Res3b

ACC
(%)
92.12
93.84
93.84
93.96
94.04
94.34

without 4S-D
SN
SP
(%)
(%)
64.13
94.2
64.18
94.06
64.18 94.06
64.33
94.20
64.52 94.37
64.16
94.25

Epochs
#
61
75
83
112
82
73

ACC
(%)
97.48
97.66
97.23
96.8
97.99
97.99

with
SN
(%)
88.64
93.08
87.33
86.3
87.15
87.71

4S-D
SP
(%)
98.01
98.41
97.73
97.91
98.10
97.42

Epochs
#
29
42
33
32
25
32

Fig. 7: The confusion matrix results of COVID-19 dataset-B obtained by 4S-DT, based on different pre-trained networks: a)
VGG19, b) GoogleNet, c) ResNet and d) Mobilenetv2.
TABLE VI: Classification performance of 4S-DT and other
models on testing set 2 of the COVID-19 dataset.
Model
4S-DT (ResNet)
4S-DT (GoogleNet)
4S-DT (Vgg19)
DeTraC- ResNet18 [40]
ResNet18 [29]
DeTraC-GoogleNet [40]
DeTraC-Vgg19 [40]

ACC (%)
97.54
94.15
95.28
95.12
92.5
91.01
93.42

SN (%)
97.88
97.07
93.66
97.91
65.01
76.03
89.71

SP (%)
97.15
93.08
97.15
91.87
94.3
82.6
95.7

[5]

[6]

[7]

[8]
[2] E. Dandil, M. Cakirolu, Z. Eki, M. Ozkan, O. K. Kurt, and A. Canan,
“Artificial neural network-based classification system for lung nodules
on computed tomography scans,” in 2014 6th International conference
of soft computing and pattern recognition (SoCPaR). IEEE, 2014, pp.
382–386.
[3] J. Kuruvilla and K. Gunavathi, “Lung cancer classification using neural
networks for ct images,” Computer methods and programs in biomedicine,
vol. 113, no. 1, pp. 202–209, 2014.
[4] T. Manikandan and N. Bharathi, “Lung cancer detection using fuzzy

[9]
[10]

[11]

auto-seed cluster means morphological segmentation and svm classifier,”
Journal of medical systems, vol. 40, no. 7, p. 181, 2016.
P. Sangamithraa and S. Govindaraju, “Lung tumour detection and classification using ek-mean clustering,” in 2016 International Conference on
Wireless Communications, Signal Processing and Networking (WiSPNET).
IEEE, 2016, pp. 2201–2206.
E. Pesce, S. J. Withey, P.-P. Ypsilantis, R. Bakewell, V. Goh, and
G. Montana, “Learning to detect chest radiographs containing pulmonary
lesions using visual attention networks,” Medical Image Analysis, vol. 53,
pp. 26 – 38, 2019.
Y. Xie, J. Zhang, and Y. Xia, “Semi-supervised adversarial model for
benign–malignant lung nodule classification on chest ct,” Medical Image
Analysis, vol. 57, pp. 237 – 248, 2019.
A. Abbas and M. M. Abdelsamea, “Learning transformations for automated classification of manifestation of tuberculosis using convolutional
neural network,” in 2018 13th IEEE International Conference on
Computer Engineering and Systems (ICCES), Dec 2018, pp. 122–126.
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,
no. 7553, p. 436, 2015.
S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transactions
on knowledge and data engineering, vol. 22, no. 10, pp. 1345–1359,
2009.
Q. Li, W. Cai, X. Wang, Y. Zhou, D. D. Feng, and M. Chen,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

TABLE VII: Parameters settings for each pre-trained model used for training 4S-DT model on COVID-19 dataset-B.
Pre-trained model
VGG19
GoogleNet
ResNet
Mobilenetv2

Learning rate
0.01
0.0001
0.001
0.001

MB-Size
32
128
256
64

TABLE VIII: The classification performance of COVID-19
dataset-B obtained by 4S-DT based on different pre-trained
models.
pre-trained model
VGG19
GoogleNet
ResNet
Mobilenetv2

Acc (%)
99.8
99.2
99.6
99.6

SN(%)
99.7
93.9
96.5
97.4

SP(%)
100
99.7
99.9
99.8

[27]
[28]

[29]
[30]

[12]

[13]

[14]

[15]

[16]

[17]

[18]
[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

“Medical image classification with convolutional neural network,” in
2014 13th International Conference on Control Automation Robotics
Vision (ICARCV). IEEE, 2014, pp. 844–848.
R. Joyseeree, S. Otálora, H. Müller, and A. Depeursinge, “Fusing
learned representations from riesz filters and deep cnn for lung tissue
classification,” Medical Image Analysis, vol. 56, pp. 172 – 183, 2019.
M. Gao, U. Bagci, L. Lu, A. Wu, M. Buty, H.-C. Shin, H. Roth,
G. Z. Papadakis, A. Depeursinge, R. M. Summers et al., “Holistic
classification of ct attenuation patterns for interstitial lung diseases via
deep convolutional neural networks,” Computer Methods in Biomechanics
and Biomedical Engineering: Imaging Visualization, vol. 6, no. 1, pp.
1–6, 2018.
L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
neural networks: A survey,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2020.
M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
unsupervised learning of visual features,” in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 132–149.
L. Rokach, O. Maimon, and O. Arad, “Improving supervised learning
by sample decomposition,” International Journal of Computational
Intelligence and Applications, vol. 5, no. 01, pp. 37–53, 2005.
A. Abbas, M. M. Abdelsamea, and M. M. Gaber, “Detrac: Transfer
learning of class decomposed medical images in convolutional neural
networks,” IEEE Access, vol. 8, pp. 74 901–74 913, 2020.
W. H. Organization, W. health organization et al., “Coronavirus disease
(covid-2019) situation reports,” 2020.
F. Shi, J. Wang, J. Shi, Z. Wu, Q. Wang, Z. Tang, K. He, Y. Shi, and
D. Shen, “Review of artificial intelligence techniques in imaging data
acquisition, segmentation and diagnosis for covid-19,” IEEE Reviews in
Biomedical Engineering, 2020.
D. Dong, Z. Tang, S. Wang, H. Hui, L. Gong, Y. Lu, Z. Xue, H. Liao,
F. Chen, F. Yang, R. Jin, K. Wang, Z. Liu, J. Wei, W. Mu, H. Zhang,
J. Jiang, J. Tian, and H. Li, “The role of imaging in the detection
and management of covid-19: a review,” IEEE Reviews in Biomedical
Engineering, pp. 1–1, 2020.
L. Li, L. Qin, Z. Xu, Y. Yin, X. Wang, B. Kong, J. Bai, Y. Lu, Z. Fang,
Q. Song et al., “Artificial intelligence distinguishes covid-19 from
community acquired pneumonia on chest ct,” Radiology, p. 200905,
2020.
Y. Song, S. Zheng, L. Li, X. Zhang, X. Zhang, Z. Huang, J. Chen, H. Zhao,
Y. Jie, R. Wang et al., “Deep learning enables accurate diagnosis of
novel coronavirus (covid-19) with ct images,” medRxiv, 2020.
L. Wang and A. Wong, “Covid-net: A tailored deep convolutional neural
network design for detection of covid-19 cases from chest radiography
images,” 2020.
X. Wang, X. Deng, Q. Fu, Q. Zhou, J. Feng, H. Ma, W. Liu, and
C. Zheng, “A weakly-supervised framework for covid-19 classification
and lesion localization from chest ct,” IEEE Transactions on Medical
Imaging, pp. 1–1, 2020.
I. D. Apostolopoulos and T. A. Mpesiana, “Covid-19: automatic detection
from x-ray images utilizing transfer learning with convolutional neural
networks,” Physical and Engineering Sciences in Medicine, p. 1, 2020.
Y. Oh, S. Park, and J. C. Ye, “Deep learning covid-19 features on cxr

[31]

[32]

[33]

[34]

[35]

[36]
[37]
[38]
[39]

[40]
[41]
[42]
[43]

Weight decay
0.0001
0.001
0.0001
0.001

Learning rate-decay
0.9 every 2 epochs
0.95 every 2 epochs
0.9 every 3 epochs
0.9 every 2 epochs

using limited training data sets,” IEEE Transactions on Medical Imaging,
pp. 1–1, 2020.
L. Chen, P. Bentley, K. Mori, K. Misawa, M. Fujiwara, and D. Rueckert,
“Self-supervised learning for medical image analysis using image context
restoration,” Medical image analysis, vol. 58, p. 101539, 2019.
X. Wu, V. Kumar, J. R. Quinlan, J. Ghosh, Q. Yang, H. Motoda, G. J.
McLachlan, A. Ng, B. Liu, S. Y. Philip et al., “Top 10 algorithms in
data mining,” Knowledge and information systems, vol. 14, no. 1, pp.
1–37, 2008.
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
S. Jaeger, A. Karargyris, S. Candemir, L. Folio, J. Siegelman, F. Callaghan,
Z. Xue, K. Palaniappan, R. K. Singh, S. Antani et al., “Automatic
tuberculosis screening using chest radiographs,” IEEE transactions on
medical imaging, vol. 33, no. 2, pp. 233–245, 2013.
S. Candemir, S. Jaeger, K. Palaniappan, J. P. Musco, R. K. Singh,
Z. Xue, A. Karargyris, S. Antani, G. Thoma, and C. J. McDonald, “Lung
segmentation in chest radiographs using anatomical atlases with nonrigid
registration,” IEEE transactions on medical imaging, vol. 33, no. 2, pp.
577–590, 2013.
D. S. Kermany, M. Goldbaum, W. Cai, C. C. Valentim, H. Liang, S. L.
Baxter, A. McKeown, G. Yang, X. Wu, F. Yan et al., “Identifying medical
diagnoses and treatable diseases by image-based deep learning,” Cell,
vol. 172, no. 5, pp. 1122–1131, 2018.
X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers,
“Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common thorax
diseases,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017, pp. 2097–2106.
S. Candemir, S. Jaeger, K. Palaniappan, J. P. Musco, R. K. Singh,
Z. Xue, A. Karargyris, S. Antani, G. Thoma, and C. J. McDonald, “Lung
segmentation in chest radiographs using anatomical atlases with nonrigid
registration,” IEEE Transactions on Medical Imaging, vol. 33, no. 2, pp.
577–590, Feb 2014.
S. Jaeger, A. Karargyris, S. Candemir, L. Folio, J. Siegelman,
F. Callaghan, Z. Xue, K. Palaniappan, R. K. Singh, S. Antani, G. Thoma,
Y. Wang, P. Lu, and C. J. McDonald, “Automatic tuberculosis screening
using chest radiographs,” IEEE Transactions on Medical Imaging, vol. 33,
no. 2, pp. 233–245, Feb 2014.
J. P. Cohen, P. Morrison, and L. Dao, “Covid-19 image data collection,”
arXiv preprint arXiv:2003.11597, 2020.
S. A. Dudani, “The distance-weighted k-nearest-neighbor rule,” IEEE
Transactions on Systems, Man, and Cybernetics, no. 4, pp. 325–327,
1976.
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” in Advances in neural
information processing systems, 2012, pp. 1097–1105.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2015, pp. 1–9.
A. Abbas, M. M. Abdelsamea, and M. M. Gaber, “Classification of
covid-19 in chest x-ray images using detrac deep convolutional neural
network,” Appl Intell., pp. 1–11, 2020.
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi, “Inception-v4
inception-resnet and the impact of residual connections on learning,” in
Thirty-first AAAI conference on artificial intelligence, 2017.
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,
“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings
of the IEEE conference on computer vision and pattern recognition,
2018, pp. 4510–4520.

