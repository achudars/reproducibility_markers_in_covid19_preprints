JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020

1

Interpretable Machine Learning for COVID-19: An
Empirical Study on Severity Prediction Task

arXiv:2010.02006v6 [cs.LG] 30 Apr 2021

Han Wu, Wenjie Ruan, Jiangtao Wang, Dingchang Zheng, Bei Liu, Yayuan Geng, Xiangfei Chai,
Jian Chen, Kunwei Li, Shaolin Li and Sumi Helal, Fellow, IEEE

Abstract—The black-box nature of machine learning models
hinders the deployment of some high-accuracy medical diagnosis
algorithms. It is risky to put one’s life in the hands of models that
medical researchers do not fully understand or trust. However,
through model interpretation, black-box models can promptly
reveal significant biomarkers that medical practitioners may have
overlooked due to the surge of infected patients in the COVID-19
pandemic.
This research leverages a database of 92 patients with confirmed SARS-CoV-2 laboratory tests between 18th January 2020
and 5th March 2020, in Zhuhai, China, to identify biomarkers
indicative of infection severity prediction. Through the interpretation of four machine learning models, decision tree, random
forests, gradient boosted trees, and neural networks using permutation feature importance, Partial Dependence Plot (PDP),
Individual Conditional Expectation (ICE), Accumulated Local
Effects (ALE), Local Interpretable Model-agnostic Explanations
(LIME), and Shapley Additive Explanation (SHAP), we identify
an increase in N-Terminal pro-Brain Natriuretic Peptide (NTproBNP), C-Reaction Protein (CRP), and lactic dehydrogenase
(LDH), a decrease in lymphocyte (LYM) is associated with severe
infection and an increased risk of death, which is consistent with
recent medical research on COVID-19 and other research using
dedicated models. We further validate our methods on a large
open dataset with 5644 confirmed patients from the Hospital
Israelita Albert Einstein, at São Paulo, Brazil from Kaggle, and
unveil leukocytes, eosinophils, and platelets as three indicative
biomarkers for COVID-19.
Impact Statement—The pandemic is a race against time. We
seek to answer the question, how can medical practitioners
employ machine learning to win the race in the pandemic?
This work was supported in part by HY Medical Technology, Scientific
Research Department Beijing, CN
Han Wu is with the Coventry University, Priory St, Coventry CV1 5FB
UK. (e-mail: hw@exeter.ac.uk).
Wenjie Ruan is with the University of Exeter, Stocker Rd, Exeter EX4 4PY
UK (e-mail: W.Ruan@exeter.ac.uk).
Jiangtao Wang is with Coventry University, Priory St, Coventry CV1 5FB
UK (e-mail: jiangtao.wang@coventry.ac.uk).
Dingchang Zheng is with Coventry University, Priory St, Coventry CV1
5FB UK (e-mail: ad4291@coventry.ac.uk).
Bei Liu is with the 910 Hospital of PLA, Department of Gastroenterology,
CN (e-mail: liubei0927@outlook.com).
Yayuan Geng is with HY Medical Technology, Scientific Research Department Beijing, CN (e-mail: gengyayuan@huiyihuiying.com).
Xiangfei Chai is with HY Medical Technology, Scientific Research Department Beijing, CN (e-mail: chaixiangfei@huiyihuiying.com).
Jian Chen is with Fifth Affiliated Hospital of Sun Yat-sen University,
Department of Radiology Zhuhai, CN (e-mail: drchenj@126.com).
Kunwei Li is with Fifth Affiliated Hospital of Sun Yat-sen University,
Department of Radiology Zhuhai, CN (e-mail: likunwei@mail.sysu.edu.cn).
Shaolin Li is with Fifth Affiliated Hospital of Sun Yat-sen University,
Department of Radiology, Zhuhai, CN, and Guangdong Provincial Key
Laboratory of Biomedical Imaging Zhuhai, Guangdong, CN (e-mail: lishlin5@mail.sysu.edu.cn).
Sumi Helal is with University of Florida, Gainesville, Florida, USA (e-mail:
helal@acm.org).

Instead of targeting at a high-accuracy black-box model that
is difficult to trust and deploy, we use model interpretation that
incorporates medical practitioners’ prior knowledge to promptly
reveal the most important indicators in early diagnosis, and thus
win the race in the pandemic.
Index Terms—Artificial intelligence in medicine, Artificial
intelligence in health, Interpretable Machine Learning

I. I NTRODUCTION

T

He sudden outbreak of COVID-19 has caused an unprecedented disruption and impact worldwide. With more
than 100 million confirmed cases as of February 2021, the
pandemic is still accelerating globally. The disease is transmitted by inhalation or contact with infected droplets with an
incubation period ranging from 2 to 14 days [1], making it
highly infectious and difficult to contain and mitigate.
With the rapid transmission of COVID-19, the demand for
medical supplies goes beyond hospitals’ capacity in many
countries. Various diagnostic and predictive models are employed to release the pressure on healthcare workers. For
instance, a deep learning model that detects abnormalities and
extract key features of the altered lung parenchyma using chest
CT images is proposed [2]. On the other hand, Rich Caruana
et al. exploit intelligible models that use generalized additive
models with pairwise interactions to predict the probability
of readmission [3]. To maintain both interpretability and
complexity, DeepCOVIDNet is present to achieve predictive
surveillance that identifies the most influential features for
the prediction of the growth of the pandemic[4] through the
combination of two modules. The embedding module takes
various heterogeneous feature groups as input and outputs
an equidimensional embedding corresponding to each feature
group. The DeepFM [5] module computes second and higherorder interactions between them.
Models that achieves high accuracy provide fewer interpretations due to the trade-off between accuracy and interpretability [6]. To be adopted in healthcare systems that require both
interpretability and robustness[7], the Multi-tree XGBoost
algorithm is employed to identify the most significant indicators in COVID-19 diagnosis [8]. This method exploits the
recursive tree-based decision system of the model to achieve
high interpretability. On the other hand, a more complex
convolutional neural network (CNN) model can discriminates
COVID-19 from Non-COVID-19 using chest CT image [9].
It achieves interpretability through gradient-weighted class
activation mapping to produce a heat map that visually verifies
where the CNN model is focusing.

2

JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020

Fig. 1: The difference between the usual workflow of machine learning, and our approach.

Besides, several model-agnostic methods have been proposed to peek into black-box models, such as Partial Dependence Plot (PDP) [10], Individual Conditional Expectation
(ICE) [11], Accumulated Local Effects (ALE) [12], Permutation Feature Importance [13], Local Interpretable Modelagnostic Explanations (LIME) [14], Shapley Additive Explanation (SHAP) [15], and Anchors [16]. Most of these modelagnostic methods are reasoned qualitatively through illustrative
figures and human experiences. To quantitatively measure
their interpretability, metrics such as faithfulness [17] and
monotonicity [18] are proposed.
In this paper, instead of targeting a high-accuracy model, we
interpret several models to help medical practitioners promptly
discover the most significant biomarkers in the pandemic.
Overall, this paper makes the following contributions:
1) Evaluation: A systematic evaluation of the interpretability of machine learning models that predict the severity
level of COVID-19 patients. We experiment with six
interpretation methods and two evaluation metrics on our
dataset and receive the same result as research that uses
a dedicated model. We further validate our approach on
a dataset from Kaggle.
2) Implication: Through the interpretation of models
trained on our dataset, we reveal N-Terminal pro-Brain
Natriuretic Peptide (NTproBNP), C-Reaction Protein
(CRP), lactic dehydrogenase (LDH), and lymphocyte
(LYM) as the most indicative biomarkers in identifying
patients’ severity level. Applying the same approach on
the Kaggle dataset, we further unveil three significant
features, leukocytes, eosinophils, and platelets.
3) Implementation: We design a system that healthcare
professionals can interact with its AI Models to incorporate model insights with medical knowledge. We
release our implementation, models for future research
and validation. 1
1 Our source code and models are available at https://github.com/
wuhanstudio/interpretable-ml-covid-19.

II. P RELIMINARY OF AI I NTERPRETABILITY
In this section, six interpretation methods, Partial Dependence Plot, Individual Conditional Expectation, Accumulated
Local Effects, Local Interpretable Model-agnostic Explanations, and Shapley Additive Explanation are summarized.
We also summarize two evaluation metrics, faithfulness and
monotonicity.
A. Model-Agnostic Methods
In healthcare, restrictions to using only interpretable models
bring many limitations in adoption while separating explanations from the model can afford several beneficial flexibilities
[19]. As a result, model-agnostic methods have been devised
to provide interpretations without knowing model details.
Partial Dependence Plot: Partial Dependence Plots (PDP)
reveal the dependence between the target function and several
target features. The partial function fˆxs (xs ) is estimated by
calculating averages in the training data, also known as the
Monte Carlo method. After setting up a grid for the features
we are interested in (target features), we set all target features
in our training set to be the value of grid points, then make
predictions and average them all at each grid. The drawback
of PDP is that one target feature produces 2D plots and two
produce 3D plots while it can be pretty hard for a human to
understand plots in higher dimensions.
n

1Xˆ
f (xs , xic )
fˆxs (xs ) =
n 1

(1)

Individual Conditional Expectation: Individual Conditional Expectation (ICE) is similar to PDP. The difference is
that PDP calculates the average over the marginal distribution
while ICE keeps them all. Each line in the ICE plot represents
predictions for each individual. Without averaging on all instances, ICE unveils heterogeneous relationships but is limited
to only one target feature since two features result in overlay
surfaces that cannot be identified by human eyes [20].

HAN. WU et al.: INTERPRETABLE MACHINE LEARNING FOR COVID-19: AN EMPIRICAL STUDY ON SEVERITY PREDICTION TASK

Accumulated Local Effects: Accumulated Local Effects
(ALE) averages the changes in the predictions and accumulate
them over the local grid. The difference with PDP is that the
value at each point of the ALE curve is the difference to the
mean prediction calculated in a small window rather than all of
the grid. Thus ALE eliminates the effect of correlated features
[20] which makes it more suitable in healthcare because
it’s usually irrational to assume young people having similar
physical conditions with the elderly.
Permutation Feature Importance: The idea behind Permutation Feature Importance is intuitive. A feature is significant for the model if there is a noticeable increase in the
model’s prediction error after permutation. On the other hand,
the feature is less important if the prediction error remains
nearly unchanged after shuffling.
Local Interpretable Model-agnostic Explanations: Local
Interpretable Model-agnostic Explanations (LIME) uses interpretable models to approximate the predictions of the original
black-box model in specific regions. LIME works for tabular
data, text, and images, but the explanations may not be stable
enough for medical applications.
Shapley Additive Explanation: Shapley Additive exPlanation (SHAP) borrows the idea of Shapley value from Game
Theory [21], which represents contributions of each player
in a game. Calculating Shapley values is computationally
expensive when there are hundreds of features, thus Lundberg,
Scott M., and Su-In Lee proposed a fast implementation for
tree-based models to boost the calculation process [15]. SHAP
has a solid theoretical foundation but is still computationally
slow for a lot of instances.
To summarize, PDP, ICE, and ALE only use graphs to
visualize the impact of different features while Permutation
Feature Importance, LIME, and SHAP provide numerical
feature importance that quantitatively ranks the importance of
each feature.
B. Metrics for Interpretability Evaluation
Different interpretation methods try to find out the most
important features to provide explanations for the output.
But as Doshi-Velez and Kim questioned, ”Are all models
in all defined-to-be-interpretable model classes equally interpretable?” [6] And how can we measure the quality of different
interpretation methods?
Faithfulness: Faithfulness incrementally removes each of
the attributes deemed important by the interpretability metric, and evaluate the effect on the performance. Then it
calculates the correlation between the weights (importance)
of the attributes and corresponding model performance and
returns correlation between attribute importance weights and
the corresponding effect on classifier [17].
Monotonicity: Monotonicity incrementally adds each attribute in order of increasing importance. As each feature is
added, the performance of the model should correspondingly
increase, thereby resulting in monotonically increasing model
performance, and it returns True of False [18].
In our experiment, both faithfulness and monotonicity are
employed to evaluate the interpretation of different machine
learning models.

3

III. E MPIRICAL S TUDY ON COVID
In this section, features in our raw dataset and procedures
of data preprocessing are introduced. After preprocessing,
four different models: decision tree, random forest, gradient
boosted trees, and neural networks are trained on the dataset.
Model interpretation is then employed to understand how
different models make predictions, and patients that models
make false diagnoses are investigated respectively.
A. Dataset and Perprocessing
The raw dataset consists of patients with confirmed SARSCoV-2 laboratory tests between 18th Jan. 2020 and 5th Mar.
2020, in Zhuhai, China. Our Research Ethics Committee
waived written informed consent for this retrospective study
that evaluated de-identified data and involved no potential risk
to patients. All the data of patients have been anonymized
before analysis.
Tables in the Appendix list all 74 features in the raw
dataset consisting of Body Mass Index (BMI), Complete Blood
Count (CBC), Blood Biochemical Examination, inflammatory
markers, symptoms, anamneses, among others. Whether or
not health care professionals will order a test for patients
is based on various factors such as medical history, physical
examination, and etc. Thus, there is no standard set of tests
that are compulsory for every individual which introduces
data sparsity. For instance, Left Ventricular Ejection Fraction
(LVEF) are mostly empty because most patients are not
required to take the color doppler ultrasound test .
After pruning out irrelevant features, such as patients’ medical numbers that provide no medical information, and features
that have no patients’ records (no patient took this test), 86
patients’ records with 55 features are selected for further
investigation. Among those, 77 records are used for training,
cross-validation, and 9 reserved for testing. The feature for
classification is Severity01 which indicates normal with 0, and
severe with 1. More detailed descriptions about features in our
dataset are listed in the Appendix.
Feature engineering is applied before training and interpreting our models, as some features may not provide valuable
information or provide redundant information.
First, constant and quasi-constant features were removed.
For instance, the two features, PCT2 and Stomachache, have
the same value for all patients providing no valuable information in distinguishing normal and severe patients.
Second, correlated features were removed because they
provide redundant information. Table I lists all correlated
features using Pearson’s correlation coefficient.
TABLE I: Feature Correlation
Feature 1

Feature 2

Correlation

cTnICKMBOrdinal1
LDH
NEU2
LYM2
NTproBNP
BMI
NEU1

cTnICKMBOrdinal2
HBDH
WBC2
LYM1
N2L2
Weight
WBC1

0.853741
0.911419
0.911419
0.842688
0.808767
0.842409
0.90352

4

JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020

1) There is strong correlation between cTnICKMBOrdinal1
and cTnICKMBOrdinal2 because they are the same test
among a short range of time which is the same for LYM1
and LYM2.
2) LDH and HBDH levels are significantly correlated with
heart diseases, and the HBDH/LDH ratio can be calculated to differentiate between liver and heart diseases.
3) Neutrophils (NEU1/NEU2) are all correlated to the
immune system. In fact, most of the white blood cells
that lead the immune system’s response are neutrophils.
Thus, there is a strong correlation between NEU1 and
WBC1, NEU2 and WBC2.
4) In the original dataset, there is no much information
about N2L2 which is correlated with NTproBNP, thus
NTproBNP remains.
5) the correlation between BMI and weight is straight
forward because Body Mass Index (BMI) is a person’s
weight in kilograms divided by the square of height in
meters.
Third, statistical methods that calculate mutual information
is employed to remove features with redundant information.
TABLE II: Features with Mutual Information
Statistical Methods

Removed Features

Mutual Information
Univariate

Height, CK, HiCKMB, Cr, WBC1, Hemoptysis
Weight, AST, CKMB, PCT1, WBC2

Mutual information is calculated using equation 2 that
determines how similar the joint distribution p(X, Y) is to
the products of individual distributions p(X)p(Y). Univariate
Test measures the dependence of two variables, and a high
p-value indicates a less similar distribution between X and Y.
I(X; Y ) =

X
x,y

p(x, y)log

p(x, y)
p(x)p(y)

Random Forest: Random Forests (RF) is a kind of ensemble learning method [23] that employs bagging strategy.
Multiple decision trees are trained using the same learning
algorithm, and then predictions are aggregated from the individual decision tree. Random forests produce great results
most of the time even without much hyper-parameter tuning.
As a result, it has been widely accepted for its simplicity and
good performance. However, it is rather difficult for humans
to interpret hundreds of decision trees, so the model itself is
less interpretable than a single decision tree.
Gradient Boosted Trees: Gradient Boosted Trees is another
ensemble learning method that employs boosting strategy [24].
Through sequentially adding one decision tree at one time,
gradient boosted trees combine results along the way. With
fine-tuned parameters, gradient boosting can result in better
performance than random forests. Still, it is tough for humans
to interpret a sequence of decision trees and thus considered
as black-box models.
Neural Networks: Neural Networks could be the most
promising model in achieving a high accuracy and even
outperforms humans in medical imaging [25]. Though the
whole network is difficult to understand, deep neural networks
are stacks of simple layers, thus can be partially understood
through visualizing outputs of intermediate layers [26].
As for the implementation, there is no hyperparameter for
the decision tree. For random forests, 100 trees are used during
the initialization. The hyperparameters for gradient boosted
trees are selected according to prior experience. The structure
for neural networks is listed in table III. All these methods are
implemented using scikit-learn [27], Keras and python3.6.
TABLE III: The structure of Neural Networks

(2)

After feature engineering, there are 37 features left for
training and testing.
B. Training Models
Machine learning models outperform humans in many different areas in terms of accuracy. Interpretable models such
as the decision tree are easy to understand, but not suitable
for large scale applications. Complex models achieve high
accuracy while giving less explanation.
For healthcare applications, both accuracy and interpretability are significant. Four different models are selected to extract
information from our dataset: Decision Tree, Random Forests,
Gradient Boosted Trees, and Neural Networks.
Decision Tree: Decision Tree (DT) is a widely adopted
method for both classification and regression. It’s a nonparametric supervised learning method that infers decision
rules from data features. The decision tree try to find decision
rules that make the best split measured by Gini impurity or
entropy. More importantly, the generated decision tree can be
visualized, thus easy to understand and interpret [22].

Layer Type

Output Shape

Param

Dense
Dropout
Dense
Dropout
Dense
Dropout
Dense

(None, 10)
(None, 10)
(None, 15)
(None, 15)
(None, 5)
(None, 5)
(None, 1)

370
0
165
0
80
0
6

After training, gradient boosted trees and neural networks
achieve the highest precision on the test set. Among 9 patients
in our test set, four of them are severe. Both the decision tree
and random forests fail to identify two severe patients, while
Gradient Boosted Trees and Neural Networks find all of the
severe patients.
TABLE IV: Classification Results on our dataset
Classifier

Decision Tree
Random Forest
Gradient Boosted
Trees
Neural Networks

CV

Test Set

95%
confidence
interval

F1
0.55
0.62
0.67

Precision
0.67
0.67
0.78

Recall
0.50
0.50
1.00

F1
0.57
0.57
0.80

0.31
0.31
0.27

0.58

0.78

1.00

0.80

0.27

HAN. WU et al.: INTERPRETABLE MACHINE LEARNING FOR COVID-19: AN EMPIRICAL STUDY ON SEVERITY PREDICTION TASK

(a) Decision Tree

(b) Random Forest

(c) Gradient Boosted Trees

(d) Neural Networks

5

Fig. 2: Partial Dependence Plot: There is a positive correlation between the level of NTproBNP/CRP and the probability of
turning severe because as NTproBNP/CRP increases, the average possibility (y-axis) of turning severe increases.

C. Interpretation (Permutation Feature Importance)

D. Interpretation (PDP, ICE, ALE)

First, we use Permutation Feature Importance to find the
most important features in different models. In table V, CRP2
and NTproBNP are recognized as most important features by
most models.

After recognizing the most important features, PDP, ICE,
and ALE are employed to further visualize the relationship
between CRP and NTproBNP.
In the PDPs, all of the four models indicate a higher risk
of turning severe with the increase of NTproBNP and CRP
which is consistent with the retrospective study on COVID19. The difference is that different models have different
tolerances and dependence on NTproBNP and CRP. Averagely, the decision tree has less tolerance on a high level
of NTproBNP (>2000ng/ml), and gradient boosted trees give
a much higher probability of death as CRP increases. Since
PDPs only calculate an average of all instances, we use ICEs
to identify heterogeneity.
ICE reveals individual differences. Though all of the models
give a prediction of a higher risk of severe as NTproBNP
and CRP increase, some patients have a much higher initial
probability which indicates other features have an impact on
overall predictions. For example, elderly people have higher
NTproBNP than young people and have a higher risk of
turning severe.
In the ALEs, as NTproBNP and CRP get higher, all of the
four models give a more positive prediction of turning severe,
which coincides with medical knowledge.

TABLE V: Five most important features
Model

Most Important Features

Decision Tree
Random Forest
Gradient Boosted Trees
Neural Networks

NTproBNP, CRP2, ALB2, Temp, Symptom
CRP2, NTproBNP, cTnI, LYM1, ALB2
CRP2, cTnITimes, LYM1, NTproBNP, Phlegm
NTproBNP, CRP2, CRP1, LDH, Age

According to medical knowledge, CRP refers to C-Reactive
Protein, which increases when there’s inflammation or viral
infection in the body. C-reactive protein (CRP) levels are
positively correlated with lung lesions and could reflect disease
severity[28]. NTproBNP refers to N-Terminal prohormone of
Brain Natriuretic Peptide, which will be released in response
to changes in pressure inside the heart. The CRP level in severe
patients rises due to viral infection, and patients with higher
NT-proBNP (above 88.64 pg/mL) level had more risks of inhospital death [29].

6

JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020

(a) Decision Tree

(b) Random Forest

(c) Gradient Boosted Trees

(d) Neural Networks

Fig. 3: Individual Conditional Expectation: Each line in different colors represents a patient. As we increase NTproBNP/CRP
while keeping other features the same, the probability of turning severe increases for each individual, but each patient has a
different starting level because their other physical conditions differ.

(a) Decision Tree

(b) Random Forest

(c) Gradient Boosted Trees

(d) Neural Networks

Fig. 4: Accumulated Local Effects: As the level of NTproBNP/CRP increases, the possibility of turning severe (yellow) goes
above the average.

HAN. WU et al.: INTERPRETABLE MACHINE LEARNING FOR COVID-19: AN EMPIRICAL STUDY ON SEVERITY PREDICTION TASK

7

E. Misclassified Patients
Even though the most important features revealed by our
models exhibit medical meaning, some severe patients fail
to be recognized. Both Gradient Boosted Trees and Neural
Networks recognize all severe patients and yield a recall of
1.00, while the decision tree and random forests fail to reveal
two of them.
Patient No. 2 (normal) is predicted with a probability of
0.53 of turning severe which is around the boundary (0.5).
While for patient No. 5 (severe), the model gives a relatively
low probability of turning severe (0.24).

Besides, neural networks notice that the patient is elderly
(Age = 63). If we calculate the average age in different severity
levels, it is noticeable that elderly people are more likely to
deteriorate.

TABLE VI: Misclassified Patients

Gradient boosted trees and neural networks make correct
predictions because they trust more in test results, while the
decision tree relies more on whether or not a patient has
symptoms. As a result, gradient boosted trees and neural
networks are capable of recognizing patients that are likely
to turn severe in the future while the decision tree makes
predictions relying more on patients’ current situation.
Medical research is a case-by-case study. Every patient is
unique. It’s strenuous to find a single criterion that suits every
patient, thus it’s important to focus on each patient and make
a diagnosis accordingly. This is one of the benefits of using
interpretable machine learning. It unveils the most significant
features for most patients and provides the interpretation for
each patient as well.

No

Class

Probability of Severe

Prediction

Type

2
5

Normal
Severe

0.53
0.24

Severe
Normal

False Positive
False Negative

F. Interpretation (False Negative)
Suppose different models represent different doctors, then
the decision tree and random forests make the wrong diagnosis
for patient no. 5. The reason human doctors classified the
patient as severe is that he actually needed a respirator to
survive. To further investigate why the decision tree and
random forests make wrong predictions, Local Interpretable
Model-agnostic Explanations (LIME) and (Shapley Additive
Explanation) SHAP are employed.
LIME: Features in green have a positive contribution to
the prediction (increasing the probability of turning severe),
and features in red have a negative effect on the prediction
(decreasing the probability of turning severe).
SHAP: Features pushing the prediction to be higher (severe)
are shown in red, and those pushing the prediction to be lower
(normal) are in blue.
1) Wrong Diagnoses: Take the decision tree as an example,
in the figure 5a, the explanation by LIME illustrates that
NTproBNP and CRP are two features (in green) that have
a positive impact on the probability of turning severe. Even
though patient No.5 is indeed severe, the decision tree gives an
overall prediction of normal (false negative). Thus, we would
like to investigate features that have a negative impact on the
probability of turning severe.
In the figure 6c, the explanation by SHAP reveals that the
patient is diagnosed as normal by the decision tree because
the patient has no symptom. Even though the patient has a
high NTproBNP and CRP, having no symptom makes it less
likely to classify him as severe. The record was taken when
the patient came to the hospital for the first time. It is likely
that the patient developed symptoms later and turned severe.
However, both gradient boosted trees and neural networks
are not deceived by the fact the patient has no symptom. Their
predictions indicate that the patient is likely to turn severe in
the future.
2) Correct Diagnoses: In the figure 6c and figure 6d,
gradient boosted trees and neural networks do not prioritise
the feature symptom. They put more weight on test results
(NTproBNP and CRP). Thus they make correct predictions
based on the fact that the patient’s test results are serious.

TABLE VII: Average Age in different severity levels
Severity Level

Average Age

0
1
2
3

36.83
47.45
54.31
69.40

G. Interpretation (False Positive)
With limited medical resources at the initial outbreak of the
pandemic, it’s equally important to investigate false positive,
so that valuable resources can be distributed to patients in
need.
In table VI, patient 2 is normal, but all of our models diagnose the patient as severe. To further explain the false positive
prediction, table VIII lists anonymized medical records for
patient 2 (normal) and patient 5 (severe) for comparison.
TABLE VIII: Record of the false positive Patient 2
Feature
Sex
Age
AgeG1
Temp
cTnITimes
cTnI
cTnICKMBOrdinal1
LDH
NTproBNP
LYM1
N2L1
CRP1
ALB1
CRP2
ALB2
Symptoms
NDisease

Patient 5 (Severe)

Patient 2 (Normal)

1.00
63.00
1.00
36.40
7.00
0.01
0.00
220.00
433.00
1.53
3.13
22.69
39.20
22.69
36.50
None
Hypertention

1.00
42.00
0.00
37.50
8.00
0.01
0.00
263.00
475.00
1.08
2.16
36.49
37.60
78.76
37.60
Fever
Hypertention,
DM, Hyperlipedia

8

JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020

1) Doctors’ Diagnoses: We present the test results of both
patients to doctors without indicating which patient is severe.
All doctors mark patient No. 2 as more severe which is the
same as our models. Doctors’ decisions are based on the
COVID-19 Diagnosis and Treatment Guide in China. The
increased level in CRP, LDH, decreased level in LYM are
associated with severe COVID-19 infection in the guideline,
and patient 2 has a higher level of CRP and LDH, a lower
level of LYM than patient 5. As a result, doctors’ diagnoses
are consistent with models’ predictions
2) Models’ Diagnoses: Even though all of the four models
make the same predictions as human doctors, it’s important to
confirm models’ predictions are in accordance with medical
knowledge. Table IX lists the three most important features
in the interpretation of LIME and SHAP. More detailed
interpretations are illustrated in the figure 7 and figure 8.

H. Evaluating Interpretation
Though we do find some indicative symptoms of COVID19 through model interpretation, they are confirmed credible
because these interpretations are corroborated by medical
research. If we use the interpretation to understand a new virus
at the early stage of an outbreak, there will be less evidence
to support our interpretation. Thus we use Monoitinicity and
Faithfulness to evaluate different interpretations using IBM
AIX 360 toolbox [31]. The decision tree only provides a binary
prediction (0 or 1) rather than a probability between 0 and 1,
so it cannot be evaluated using Monotonicity and Faithfulness.
TABLE X: Failthfulness Evaluation

TABLE IX: Most important features from LIME, SHAP
Model

LIME

Decision Tree
Random Forest
Gradient Boosted Trees
Neural Networks

NTproBNP, CRP2, NauseaNVomit
NTproBNP, CRP2, CRP1
NTproBNP, CRP2, LYM1
NTproBNP, CRP2, PoorAppetite

Model

SHAP

Decision Tree
Random Forests
Gradient Boosted Trees
Neural Networks

CRP2, NTproBNP, ALB2
CRP2, CRP1, LDH
CRP2, NTproBNP, LDH
CRP2, NTproBNP, CRP1

In table IX, NTproBNP, CRP, LYM, LDH are the most
common features that are deemed crucial by all different
models. The three features, CRP, LYM, LDH, are listed as
the most indicative biomarkers in the COVID-19 guideline.
While the correlation between NTproBNP and COVID-19
are investigated in a paper from World Health Organization
(WHO) global literature on coronavirus disease, that reveals
elevated NTproBNP is associated with increased mortality in
patients with COVID-19 [30].
As a result, the prediction of false-positive is consistent with
doctors’ diagnoses. Patient 2 who is normal is diagnosed as
severe by both doctors and models. One possibility is that
even though the patients’ test results are not optimistic, he
did not require a respirator to survive when he came to the
hospital for the first time, so he was classified as normal.
In this way, models’ predictions can act as a warning. If a
patient is diagnosed as severe by models, and the prediction
is in accordance with medical knowledge, but the patient feels
normal, we can suggest to the patient to put more attention on
his health condition.
In conclusion, as illustrated previously in the explanation
for patient 5 (false negative), every patient is unique. Some
patients are more resistant to viral infection, while some
are more vulnerable. Pursuing a perfect model is tough in
healthcare, but we can try to understand how different models
make predictions using interpretable machine learning to be
more responsible with our diagnoses.

Models

LIME

SHAP

Random Forests
Gradient Boosted Trees
Neural Networks

0.37
0.46
0.45

0.59
0.49
0.33

Faithfulness (ranging from -1 to 1) reveals the correlation
between the importance assigned by the interpretability algorithm and the effect of each attribute on the performance of
the model. All of our interpretations receive good faithfulness
scores, and SHAP receives a higher faithfulness score than
LIME on average. The interpretation by SHAP receives better
results because the Shapley value is calculated by removing the
effect of specific features which is similar to how faithfulness
is computed, so SHAP is more akin to faithfulness.
TABLE XI: Monotonicity Evaluation
Models

LIME

SHAP

Random Forests
Gradient Boosted Trees
Neural Networks

False
22% True
False

False
22% True
False

As for monotonicity, most interpretation methods receive a
False though we do find valuable conclusions from interpretations. The difference between faithfulness and monotonicity is
that faithfulness incrementally removes each of the attributes,
while monotonicity incrementally adds each of the attributes.
By incrementally adding each attribute, initially, the model
may not be able to make correct predictions with only one
or two features, but this does not mean these features are
not important. Evaluation metrics for different interpretation
methods is still an active research direction, and our results
may hopefully stimulate further research on the development
of better evaluation metrics for interpreters.
I. Summary
In this section, the interpretation of four different machine
learning models reveals that N-Terminal pro-Brain Natriuretic
Peptide (NTproBNP), C-Reaction Protein (CRP), and lactic dehydrogenase (LDH), lymphocyte (LYM) are the four
most important biomarkers that indicate the severity level of
COVID-19 patients. In the next section, we further validate
our methods on two datasets to corroborate our proposal.

HAN. WU et al.: INTERPRETABLE MACHINE LEARNING FOR COVID-19: AN EMPIRICAL STUDY ON SEVERITY PREDICTION TASK

(a) Decision Tree

(b) Random Forests

(c) Gradient Boosted Trees

(d) Neural Networks

9

Fig. 5: LIME Explanation (False-Negative Patient No.5): Features in green have a positive contribution to the prediction
(increasing the probability of turning severe), and features in red have a negative effect on the prediction (decreasing the
probability of turning severe)

(a) Decision Tree

(b) Random Forests

(c) Gradient Boosted Trees

(d) Neural Networks

Fig. 6: SHAP Explanation (False-Negative Patient No.5): Features pushing the prediction to be higher (severe) are shown in
red, and those pushing the prediction to be lower (normal) are in blue.

10

JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020

(a) Decision Tree

(b) Random Forests

(c) Gradient Boosted Trees

(d) Neural Networks

Fig. 7: LIME Explanation (False-Positive Patient No.2): Features in green have a positive contribution to the prediction
(increasing the probability of turning severe), and features in red have a negative effect on the prediction (decreasing the
probability of turning severe)

(a) Decision Tree

(b) Random Forests

(c) Gradient Boosted Trees

(d) Neural Networks

Fig. 8: SHAP Explanation (False-Positive Patient No.2): Features pushing the prediction to be higher (severe) are shown in
red, and those pushing the prediction to be lower (normal) are in blue.

HAN. WU et al.: INTERPRETABLE MACHINE LEARNING FOR COVID-19: AN EMPIRICAL STUDY ON SEVERITY PREDICTION TASK

IV. VALIDATION ON OTHER DATASETS
At the initial outbreak of the pandemic, our research
leverages a database consisting of patients with confirmed
SARS-CoV-2 laboratory tests between 18th January 2020,
and 5th March 2020, in Zhuhai, China, and reveals that an
increase in NTproBNP, CRP, and LDH, and a decrease in
lymphocyte count indicates a higher risk of death. However,
the dataset has a limited record of 92 patients which may not
be enough to support our proposal. Luckily, and thanks to
global cooperation, we do have access to larger datasets. In
this section, we further validate our methods on two datasets,
one with 485 infected patients in Wuhan, China[8], and the
other with 5644 confirmed cases from the Hospital Israelita
Albert Einstein, at São Paulo, Brazil from Kaggle.
A. Validation on 485 infected patients in China
The medical record of all patients in this dataset was
collected between 10th January and 18th February 2020,
within a similar date range as our dataset. Yan et al. construct
a dedicated simplified and clinically operable decision model
to rank 75 features in this dataset, and the model demonstrates
that three key features, lactic dehydrogenase (LDH), lymphocyte (LYM), and high-sensitivity C-reactive protein (hs-CRP)
can help to quickly prioritize patients during the pandemic,
which is consistent with our interpretation in Table V.
Findings from the dedicated model are consistent with
current medical knowledge. The increase of hs-CRP reflects
a persistent state of inflammation [32]. The increase of LDH
reflects tissue/cell destruction and is regarded as a common
sign of tissue/cell damage, and the decrease of lymphocyte is
supported by the results of clinical studies [33].

11

B. Validation on 5644 infected patients in Brazil
Our approach obtains the same result on the dataset with 92
patients from Zhuhai, China, and a medium-size dataset with
485 patients from Wuhan, China. Besides, we further validate
our approach on a larger dataset with 5644 patients in Brazil,
from Kaggle.
This dataset consists of 111 features including anonymized
personal information, laboratory virus tests, urine tests, venous
blood gas analysis, arterial blood gases, blood routine test,
among other features. All data were anonymized following
the best international practices and recommendations. The
difference between this dataset and ours is that all data are
standardized to have a mean of zero and a unit standard
deviation, thus the original data range that contains clinical
meaning is lost. Still, the most important medical indicators
can be extracted using interpretation methods.
TABLE XII: Patient No.0 in the Kaggle Dataset
Feature

Value

SARS-Cov-2 test result
Patient Age Quantile
Hematocrit
Platelets
Mean platelet volume
Mean corpuscular hemoglobin concentration (MCHC)
Leukocytes
Basophils
Eosinophils
Monocytes
Proteina C reativa mg/dL

1
14.00
0.92
-1.26
0.79
-0.65
-1.47
-1.14
-0.83
0.96
0.236

Following the same approach, a preprocessing is applied on
the dataset that removes irrelevant features such as patients’
intention to the ward level, and features that have less than
100 patient’s record, for instance, urine tests and aerial blood
gas tests. On the other hand, patients that have less than 10
records are dropped, because these records do not provide
enough information. After preprocessing, we have a full record
of 420 patients with 10 features.
TABLE XIII: Classification Results (Kaggle)
Classifier

Fig. 9: A decision rule using three key features and their
thresholds in absolute value. Num, the number of patients in
a class; T, the number of correctly classified; F, the number
of misclassified patients. [8]
Our methods reveal the same results without taking efforts
to design a dedicated interpretable model but can be more
prompt to react to the pandemic. During pandemic outbreak,
a prompt reaction that provides insights on the new virus could
save lives and time.

Decision Tree
Random Forests
Gradient Boosted
Trees
Neural Networks

CV

Test Set

95%
confidence
interval

F1
0.37
0.37
0.56

Precision
0.88
0.90
0.90

Recall
0.75
0.50
0.75

F1
0.71
0.67
0.59

0.098
0.089
0.089

0.38

0.90

0.50

0.67

0.089

After training and interpreting four different models, decision tree, random forests, gradient boosted trees, and neural
networks, the most important features are identified and listed
in table XIV. The three most common indicative features are
leukocytes, eosinophils, and platelets.

12

JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020

(a) Decision Tree

(c) Gradient Boosted Trees

(b) Random Forests

(d) Neural Networks

Fig. 10: LIME Explanation (Kaggle Patient 0): Features in green have a positive contribution to the prediction (increasing
the probability of turning severe), and features in red have a negative effect on the prediction (decreasing the probability of
turning severe)

According to medical research, patients with increased
leukocyte count are more likely to develop critically illness,
more likely to admit to an ICU, and have a higher rate of
death [34]. Du et al. noted that at the time of admission,
81% of the patients had absolute eosinophil counts below
the normal range in the medical records of 85 fatal cases
of COVID-19[35]. Wool G.D. and Miller J.L. discovered that
COVID-19 is associated with increased numbers of immature
platelets which could be another mechanism for increased
clotting events in COVID-19[36].
TABLE XIV: Five most important features (Kaggle)
Model

Most Important Features

Decision Tree
Random Forest
Gradient Boosted Trees
Neural Networks

Leukocytes, Eosinophils, Patient age quantile
Leukocytes, Eosinophils, Platelets
Patient age quantile, Hematocrit, Platelets
Leukocytes, Platelets, Monocytes

In addition, the two datasets collectively reveal that elderly
people are more susceptible to the virus. The significant
feature NTproBNP in the Chinese dataset is often used to
diagnose or rule out heart failure which is more likely to occur
in elderly people. And patients that have abnormally low levels
of platelets are more likely to be older, male as well [36].
To further validate our interpretation, faithfulness and monotonicity are calculated and listed in tables XV and XVI. Similarly, our interpretations are consistent with medical knowledge and receive a good faithfulness score, but receive a worse
score on monotonicity because the calculation procedure of
monotonicity is contrary to faithfulness.

TABLE XV: Failthfulness Evaluation (Kaggle)
Models

LIME

SHAP

Random Forests
Gradient Boosted Trees
Neural Networks

0.71
0.61
0.25

0.82
0.72
0.42

TABLE XVI: Monotonicity Evaluation (Kaggle)
Models

LIME

SHAP

Random Forests
Gradient Boosted Trees
Neural Networks

False
True
False

False
False
False

V. C ONCLUSION
In this paper, through the interpretation of four different
machine learning models, we reveal that N-Terminal pro-Brain
Natriuretic Peptide (NTproBNP), C-Reaction Protein (CRP),
and lactic dehydrogenase (LDH), lymphocyte (LYM) are the
four most important biomarkers that indicate the severity
level of COVID-19 patients. Our findings are consistent with
medical knowledge and recent research that exploits dedicated
models. We further validate our methods on a large open
dataset from Kaggle and unveil leukocytes, eosinophils, and
platelets as three indicative biomarkers for COVID-19.
The pandemic is a race against time. Using interpretable machine learning, medical practitioners can incorporate insights
from models with their prior medical knowledge to promptly
reveal the most significant indicators in early diagnosis and
hopefully win the race in the fight against the pandemic.

HAN. WU et al.: INTERPRETABLE MACHINE LEARNING FOR COVID-19: AN EMPIRICAL STUDY ON SEVERITY PREDICTION TASK

A PPENDIX
TABLE XVII: Diagnoses
Feature

Comments

Severity03
Severity01

Severe (3) - Normal (0)
Severe (1), Normal (0)

TABLE XVIII: Personal Info
Feature

Comments

MedNum
No
Sex
Age
AgeG1
Height
Weight
BMI

Medical Number
Patient No.
Man (1), Woman(0)
Age > 50(1), Age ≤ 50(0)
Body Mass Index

TABLE XXII: Symptoms and Anamneses
Feature

Comments

Symptom
Fever
Cough
Phlegm
Hemoptysis
SoreThroat
Catarrh
Headache
ChestPain
Fatigue
SoreMuscle
Stomachache
Diarrhea
PoorAppetie
NauseaNVomit
Hypertention
Hyperlipedia
DM
Lung
CAD
Arrythmia
Cancer

Diabetic Mellitus
Lunge Disease
Coronary Heart Disease
-

TABLE XIX: Complete Blood Count
Feature

Comments

WBC1
NEU1
LYM1
N2L1
WBC2
NEU2
LYM2
N2L2

White Blood Cell (first time)
Neutrophil Count (first time)
Lymphocyte Count (first time)
White Blood Cell (second time)
Neutrophil Count (second time)
Lymphocyte Count (second time)
-

TABLE XX: Inflammatory Markers
Feature

Comments

PCT1
CRP1
PCT2
CRP2

Procalcitonin (first time)
C-Reactive Protein (first time)
Procalcitonin (second time)
C-Reactive Protein (second time)

TABLE XXI: Biochemical Examination
Feature

Comments

AST
LDH
CK
CKMB
HBDH
HiCKMB
Cr
ALB1
ALB2

Aspartate aminotransferase
Lactate Dehydrogenase
Creatine Kinase
The amount of an isoenzyme of creatine kinase (CK)
Alpha-Hydroxybutyrate Dehydrogenase
Highest CKMB
Serum Creatinine
Albumin Count (first time)
Albumin Count (second time)

TABLE XXIII: Other test results
Feature

Comments

Temp
LVEF
Onset2Admi
Onset2CT1
Onset2CTPositive1
Onset2CTPeak
cTnITimes
cTnI
cTnlCKMBOrdinal1
cTnlCKMBOrdinal2
CTScore
AIVolumneP
SO2
PO2
YHZS
RUL
RML
RLL
LUL
LLL

Temperature
Left Ventricular Ejection Fraction
Time from onset to admission
Time from onset to CT test
Time from onset to CT test positive
Time from onset to CT peak
When was cTnI tested
Cardiac Troponin I
The value when hospitalized
The maximum value when hospitalized
Peak CT Score
Peak Volume
Empty
Empty
Empty
Empty
Empty
Empty
Empty
Empty

13

14

JOURNAL OF IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE, VOL. 00, NO. 0, MONTH 2020

R EFERENCES
[1] T. Singhal, “A review of coronavirus disease-2019 (covid-19),” The
Indian Journal of Pediatrics, vol. 87, no. 4, pp. 1–6, 2020.
[2] S. Basu, S. Mitra, and N. Saha, “Deep learning for screening covid19 using chest x-ray images,” in 2020 IEEE Symposium Series on
Computational Intelligence (SSCI). IEEE, 2020, pp. 2521–2527.
[3] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad,
“Intelligible models for healthcare: Predicting pneumonia risk and
hospital 30-day readmission,” in KDD ’15, 2015.
[4] A. Ramchandani, C. Fan, and A. Mostafavi, “Deepcovidnet: An interpretable deep learning model for predictive surveillance of covid-19
using heterogeneous features and their interactions,” IEEE Access, vol. 8,
pp. 159 915–159 930, 2020.
[5] H. Guo, R. Tang, Y. Ye, Z. Li, and X. He, “Deepfm: A factorizationmachine based neural network for ctr prediction,” in International Joint
Conference on Artificial Intelligence, 08 2017, pp. 1725–1731.
[6] F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable
machine learning,” arXiv preprint arXiv:1702.08608, 2017.
[7] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[8] L. Yan, H.-T. Zhang, J. Goncalves, Y. Xiao, M. Wang, Y. Guo, C. Sun,
X. Tang, L. Jing, M. Zhang, X. Huang, Y. Xiao, H. Cao, Y. Chen,
T. Ren, F. Wang, Y. Xiao, S. Huang, X. Tan, N. Huang, B. Jiao,
C. Cheng, Y. Zhang, A. Luo, L. Mombaerts, J. Jin, Z. Cao, S. Li, H. Xu,
and Y. Yuan, “An interpretable mortality prediction model for covid-19
patients,” Nature Machine Intelligence, vol. 2, no. 5, pp. 283–288, May
2020.
[9] E. Matsuyama et al., “A deep learning interpretable model for novel
coronavirus disease (covid-19) screening with chest ct images,” Journal
of Biomedical Science and Engineering, vol. 13, no. 07, p. 140, 2020.
[10] J. H. Friedman, “Greedy function approximation: A gradient boosting
machine.” Ann. Statist., vol. 29, no. 5, pp. 1189–1232, 10 2001.
[11] A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin, “Peeking inside the
black box: Visualizing statistical learning with plots of individual conditional expectation,” Journal of Computational and Graphical Statistics,
vol. 24, 09 2013.
[12] D. W. Apley and J. Zhu, “Visualizing the effects of predictor variables in
black box supervised learning models,” Journal of the Royal Statistical
Society Series B, vol. 82, no. 4, pp. 1059–1086, September 2020.
[13] A. Fisher, C. Rudin, and F. Dominici, “All models are wrong, but many
are useful: Learning a variable’s importance by studying an entire class
of prediction models simultaneously,” Journal of Machine Learning
Research, vol. 20, no. 177, pp. 1–81, 2019.
[14] M. T. Ribeiro, S. Singh, and C. Guestrin, “”why should i trust you?”:
Explaining the predictions of any classifier,” in Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, ser. KDD ’16. New York, NY, USA: Association for
Computing Machinery, 2016, p. 1135–1144.
[15] S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model
predictions,” in Advances in Neural Information Processing Systems
30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017,
pp. 4765–4774.
[16] M. T. Ribeiro, S. Singh, and C. Guestrin, “Anchors: High-precision
model-agnostic explanations,” in AAAI, 2018.
[17] D. Alvarez-Melis and T. S. Jaakkola, “Towards robust interpretability
with self-explaining neural networks,” arXiv preprint arXiv:1806.07538,
2018.
[18] R. Luss, P.-Y. Chen, A. Dhurandhar, P. Sattigeri, Y. Zhang, K. Shanmugam, and C.-C. Tu, “Generating contrastive explanations with monotonic attribute functions,” arXiv preprint arXiv:1905.12698, 2019.
[19] M. T. Ribeiro, S. Singh, and C. Guestrin, “Model-agnostic interpretability of machine learning,” arXiv preprint arXiv:1606.05386, 2016.
[20] C. Molnar, Interpretable machine learning. Lulu. com, 2020.
[21] L. S. Shapley, “17. a value for n-person games,” Contributions to the
Theory of Games (AM-28), Volume II, p. 307–318, 1953.
[22] L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classification
and regression trees. belmont, ca: Wadsworth international group.”
Encyclopedia of Ecology, vol. 57, no. 1, pp. 582–588, 2015.
[23] L. Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp.
5–32, 2001.
[24] S. Schaal and C. C. Atkeson, “From isolation to cooperation: An alternative view of a system of experts,” in Advances in Neural Information
Processing Systems 8. MIT Press, 1996, pp. 605–611.

[25] A. Maier, C. Syben, T. Lasser, and C. Riess, “A gentle introduction to
deep learning in medical image processing,” Zeitschrift für Medizinische
Physik, vol. 29, no. 2, pp. 86 – 101, 2019, special Issue: Deep Learning
in Medical Physics.
[26] G. Montavon, W. Samek, and K.-R. Müller, “Methods for interpreting
and understanding deep neural networks,” Digital Signal Processing,
vol. 73, p. 1–15, Feb 2018.
[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine
Learning Research, vol. 12, pp. 2825–2830, 2011.
[28] L. Wang, “C-reactive protein levels in the early stage of covid-19,”
Medecine et maladies infectieuses, vol. 50, no. 4, pp. 332–334, 2020.
[29] L. Gao, D. Jiang, X.-s. Wen, X.-c. Cheng, M. Sun, B. He, L.-n. You,
P. Lei, X.-w. Tan, S. Qin et al., “Prognostic value of nt-probnp in patients
with severe covid-19,” Respiratory research, vol. 21, pp. 1–7, 2020.
[30] R. Pranata, I. Huang, A. A. Lukito, and S. B. Raharjo, “Elevated nterminal pro-brain natriuretic peptide is associated with increased mortality in patients with covid-19: systematic review and meta-analysis,”
Postgraduate Medical Journal, vol. 96, no. 1137, pp. 387–391, 2020.
[31] V. Arya, R. K. Bellamy, P.-Y. Chen, A. Dhurandhar, M. Hind, S. C.
Hoffman, S. Houde, Q. V. Liao, R. Luss, A. Mojsilović et al., “One
explanation does not fit all: A toolkit and taxonomy of ai explainability
techniques,” arXiv preprint arXiv:1909.03012, 2019.
[32] E. K. Bajwa, U. A. Khan, J. L. Januzzi, M. N. Gong, B. T. Thompson,
and D. C. Christiani, “Plasma C-reactive protein levels are associated
with improved outcome in ARDS,” Chest, vol. 136, no. 2, pp. 471–480,
Aug 2009.
[33] N. Chen, M. Zhou, X. Dong, J. Qu, F. Gong, Y. Han, Y. Qiu, J. Wang,
Y. Liu, Y. Wei et al., “Epidemiological and clinical characteristics of
99 cases of 2019 novel coronavirus pneumonia in wuhan, china: a
descriptive study,” The lancet, vol. 395, no. 10223, pp. 507–513, 2020.
[34] K. Zhao, R. Li, X. Wu, Y. Zhao, T. Wang, Z. Zheng, S. Zeng, X. Ding,
and H. Nie, “Clinical features in 52 patients with covid-19 who have
increased leukocyte count: a retrospective analysis,” European Journal
of Clinical Microbiology & Infectious Diseases, vol. 39, no. 12, pp.
2279–2287, 2020.
[35] Y. Du, L. Tu, P. Zhu, M. Mu, R. Wang, P. Yang, X. Wang, C. Hu,
R. Ping, P. Hu et al., “Clinical features of 85 fatal cases of covid-19
from wuhan. a retrospective observational study,” American journal of
respiratory and critical care medicine, vol. 201, no. 11, pp. 1372–1379,
2020.
[36] G. D. Wool and J. L. Miller, “The impact of covid-19 disease on platelets
and coagulation,” Pathobiology, vol. 88, no. 1, pp. 14–26, 2021.

