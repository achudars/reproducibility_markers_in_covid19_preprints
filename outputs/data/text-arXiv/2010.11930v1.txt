Scientific Claim Verification with V ERT5 ERINI
Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, and Jimmy Lin
David R. Cheriton School of Computer Science
University of Waterloo

arXiv:2010.11930v1 [cs.CL] 22 Oct 2020

Abstract
This work describes the adaptation of a pretrained sequence-to-sequence model to the
task of scientific claim verification in the
biomedical domain. We propose V ERT5 ERINI
that exploits T5 for abstract retrieval, sentence
selection and label prediction, which are three
critical sub-tasks of claim verification. We
evaluate our pipeline on S CI FACT, a newly curated dataset that requires models to not just
predict the veracity of claims but also provide
relevant sentences from a corpus of scientific
literature that support this decision. Empirically, our pipeline outperforms a strong baseline in each of the three steps. Finally, we
show V ERT5 ERINI’s ability to generalize to
two new datasets of COVID-19 claims using
evidence from the ever-expanding CORD-19
corpus.

1

Introduction

The popularity of social media and other means of
disseminating content, combined with automated
algorithms that amplify signals, has increased the
proliferation of misinformation. This has caused
increased attention in the community towards building better fact verification systems. Until recently,
most fact verification datasets were constrained to
domains such as Wikipedia, discussion blogs, and
social media (Thorne et al., 2018; Hanselowski
et al., 2019).
In the current environment, amidst the COVID19 pandemic and the unease that comes perhaps
with insufficient insight about the virus, there has
been a sharp increase in scientific curiosity among
the general public. While such curiosity is always
appreciated, this has inadvertently resulted in a
large spike of scientific facts being misrepresented,
often to push personal or political agenda, inducing
ineffective and often even harmful policies and
behaviours.

To mitigate this issue, Wadden et al. (2020) introduced the task of scientific claim verification
where systems need to evaluate the veracity of a
claim against a scientific corpus. To facilitate this,
they introduced the S CI FACT dataset that consists
of scientific claims accompanied with abstracts that
either support or refute the claim. The dataset also
provide a set of rationale sentences for each claim
that is often both necessary and sufficient to conclude its veracity.
In addition, they provide V ERI S CI, a baseline for
this task that takes inspiration from previous state
of the art systems (DeYoung et al., 2020) for the
FEVER claim verification dataset (Thorne et al.,
2018). This pipeline retrieves relevant abstracts
by TF-IDF similarity, uses a BERT-based model
(Devlin et al., 2019) to select rationale sentences,
and finally label each abstract as either S UPPORTS,
N O I NFO or R EFUTES with respect to the claim.
Despite the success of BERT for a tasks like
passage-level (Nogueira et al., 2019), documentlevel (Dai and Callan, 2019; MacAvaney et al.,
2019; Yilmaz et al., 2019) and sentence-level
(Soleimani et al., 2019) retrieval, there is evidence
that ranking with sequence-to-sequence models can
achieve even better effectiveness, particularly in
zero-shot scenarios or with limited training data
(Nogueira et al., 2020). This was further demonstrated in the TREC-COVID challenge (Roberts
et al., 2020) where one of the best performing
systems used sequence-to-sequence models for retrieval (Zhang et al., 2020). Similar trends are
noted in CovidQA (Tang et al., 2020), a question
answering dataset for COVID-19, where zero-shot
sequence-to-sequence models outperformed other
baselines.
Hence, we propose V ERT5 ERINI, where all
three steps—abstract retrieval, sentence selection
and label prediction exploit T5 (Raffel et al.,
2019), a powerful sequence-to-sequence language

H0,0
q

BM25

R0,0
a1

R0,1

H0,1
Abstract T5

a2
Corpus

a5

a4

s4,2

a1

s1,1

R1

H1

s4,2

Sentence T5

s1,2
s1,4

s1,2

a3
a4

s4,1

q
T5

pi

s1,3
s1,4

ai

q
T5

e4

R2

H2

Supports

Label T5

s1,2

e1

s1,4

si,j

Rejects

q

pi,j

T5

li

s4,2

ei

Figure 1: Illustration of the V ERT5 ERINI pipeline.

model. V ERT5 ERINI significantly outperforms the
V ERI S CI baseline on the S CI FACT tasks and hence
qualifies as a strong pipeline for the task of Scientific Claim Verification. We also demonstrate
the efficacy of our system in verifying two different sets of COVID-19 claims with no additional
training or hyperparameter tuning.

2

Task

In the S CI FACT task, systems are provided with a
scientific claim q and a corpus of abstracts C and
tasked to return:
• A set of evidence abstracts Ê(q).
• A label ŷ(q, a) that maps claim q and abstract a
to one of {S UPPORTS, R EFUTES, N O I NFO}.
• A set of rationale sentences Ŝ(q, a) when
ŷ(q, a) ∈ {S UPPORTS, R EFUTES}.

• Sentence-level evaluation, where systems are
evaluated on whether they can identify sentences
sufficient to justify the abstract-level predictions. First, ŝ ∈ Ŝ(q, a) is correctly selected
if ∃R ∈ R(q, a) such that both ŝ ∈ R and
R ⊆ Ŝ(q, a). Second, it is correctly labelled,
if in addition, ŷ(q, a) = y(q, a). These evaluations are referred to as SentenceSelection-Only and
SentenceSelection+Label , respectively.

3

Method

Our proposed framework, V ERT5 ERINI (see Figure 1), has three major components:
1. H0 : Abstract Retrieval — which given claim
q retrieves the top-k abstracts from corpus C.
2. H1 : Sentence Selection — which given claim
q and one of the top-k abstracts a, selects sentences from a that form Ŝ(q, a).

Given the ground truth label y(q, a), the set of
gold abstracts E(q), and the set of gold rationales
R(q, a) (each gold rationale is a set of sentences),
the predictions are evaluated in two ways:

3. H2 : Label Prediction — which given claim q
and the rationale sentences Ŝ(q, a), predicts the
final label ŷ(q, a).

• Abstract-level evaluation, where systems are
judged on whether they can identify abstracts
that support or refute the claim. First, a ∈ Ê(q)
is correctly labelled if both a ∈ E(q) and
ŷ(q, a) = y(q, a). Second, it is correctly rationalized, if in addition, ∃R ∈ R(q, a) such that
R ⊆ Ŝ(q, a).1 These evaluations are referred
to as AbstractLabel-Only and AbstractLabel+Rationale ,
respectively.

Given a scientific claim q and a corpus C of scientific abstracts, H0 is tasked with retrieving the
top-k abstracts from C. We propose both a singlestage and a two-stage abstract retrieval pipeline.
In both cases, the first stage H0,0 involves treating the query as a ”bag of words” for ranking abstracts from the corpus using a BM25 scoring function (Robertson et al., 1994). Our implementation uses the Anserini IR toolkit (Yang et al., 2017,
2018),2 which is built on the popular open-source

1

In S CI FACT’s abstract-level evaluation, it is required that
|Ŝ(q, a)| ≤ 3.

3.1 H0 : Abstract Retrieval

2

http://anserini.io/

Claim

Label

Evidence

ALDH1 expression
is associated with
poorer prognosis in
breast cancer.

S UPPORTS

Application of stem cell biology to breast cancer research has been limited
by the lack of simple methods for identification and isolation of normal and
malignant stem cells. . . . In a series of 577 breast carcinomas, expression of
ALDH1 detected by immunostaining correlated with poor prognosis. . . .

CX3CR1 on the
Th2 cells impairs T
cell survival

R EFUTES

Allergic asthma is a T helper type 2 (T(H)2)-dominated disease of the lung.
. . . We found that CX3CR1 signaling promoted T(H)2 survival in the inflamed lungs, and injection of B cell leukemia/lymphoma-2 protein (BCl2)-transduced CX3CR1-deficient T(H)2 cells into CX3CR1-deficient mice
restored asthma. . . .

Arterioles have a N O I NFO
larger lumen diameter than venules.

N/A

Table 1: Three S CI FACT claims, their labels and their corresponding evidence (rationale highlighted in bold) if
available. V ERT5 ERINI correctly predicts these labels and retrieves the matching evidence.

Lucene search engine. The output of this stage is a
list of k0 candidate abstracts.
The second abstract reranking stage, H0,1 , is
tasked to estimate a score p quantifying how relevant a candidate abstract a is to a query q. In this
stage, the abstracts retrieved in H0,0 are reranked
by a pointwise reranker, which we call monoT5.
Our reranker is based on Nogueira et al. (2020),
which uses T5 (Raffel et al., 2019), a sequence-tosequence model pretrained with a similar masked
language modeling objective as BERT. In this
model, all target tasks are cast as sequence-tosequence tasks. We adapt the approach to abstract
reranking by using the following input sequence:
Query: q Document: a Relevant:
The model is fine-tuned to produce the words “true”
or “false” depending on whether the abstract is relevant or not to the query. That is, “true” and “false”
are the “target words” (i.e., ground truth predictions
in the sequence-to-sequence transformation). Since
S CI FACT abstracts tend to be longer than context
limit of T5, we first segment each abstract into
spans by applying a sliding window of 6 sentences
with a stride of 3.
In order to fine-tune monoT5 on abstract reranking in S CI FACT, we use all cited abstracts in the
train set as positive examples. For each claim, we
select negative examples by randomly selecting a
non-ground truth abstract among the top-10 BM25
ranked candidates. We train on this set with a batch
size of 128 for 200 steps which corresponds to
approximately 5 epochs.
At inference time, we first to compute probabilities for each query–segment pair (in a reranking

setting), we apply a softmax only on the logits
of the “true” and “false” tokens. We then obtain
the relevance score of the document as the highest
probability assigned to the “true” token among all
segments. The top-k0 abstracts, R0 , with respect
to these scores are then selected.
We run inference with three different monoT53
settings for abstract reranking: (1) fine-tuned on
the MS MARCO passage (Bajaj et al., 2016)
dataset; (2) fine-tuned on MS MARCO then
fine-tuned again on the medical subset of MS
MARCO (MacAvaney et al., 2020); and (3) finetuned on MS MARCO then fine-tuned again on
S CI FACT.
We choose to pretrain relevance classifiers on
MS MARCO passage as it has been shown to help
in various other tasks (Nogueira et al., 2020; Zhang
et al., 2020; Yilmaz et al., 2019). Similarly, MacAvaney et al. (2020) demonstrate that fine-tuning
the classifiers on MS MARCO MED helps with
biomedical-domain relevance ranking.
3.2 H1 : Sentence Selection
In this stage, the goal is to select rationale sentences
Ŝ(q, a) from each abstract a for each of the top-k
abstracts retrieved Ê(q). We use T5 for this task
too. The following input sequence is used:
Query: q Document: s Relevant:
where s is a sentence in the abstract a.
We fine-tune a monoT5 (trained on MS MARCO
passage) on S CI FACT’s gold rationales as positive
examples and sentences randomly sampled from
3

All models are T5-3B.

Set

S UPPORTS

Train
Dev
Test

332
124
100

N O I NFO
304
112
100

R EFUTES
173
64
100

Total

Claim Set

S UPPORTS

R EFUTES

Total

809
300
300

COVID-19 S CI FACT

-

-

36

COVID-19 Scientific

41

101

142

Table 3: COVID-19 claims.

Table 2: S CI FACT label distribution.

E(q) as negatives. We train on this set of sentences
with a batch size of 128 for 2500 steps.
During inference, similar to abstract ranking,
we compute a probability of the sentence being
relevant based on the logits of “true” and “false”
tokens. Finally, we filter out all sentences whose
“true” probability is below the threshold of 0.999
to obtain Ŝ(q, a).
3.3 H2 : Label Prediction
Given the claim q, an abstract a and their corresponding set of rationale sentences Ŝ(q, a),
H2 is tasked to predict a label ŷ(q, a) ∈
{S UPPORTS, N O I NFO, R EFUTES}. Yet again, we
use T5 for this task with input sequence:
hypothesis: q sentence1: s1 · · · sentencez: sz
where s1 , · · · , sz are the rationale sentences in
Ŝ(q, a). The target sequence is one of “true”,
“weak” or “false” tokens corresponding to the labels
S UPPORTS, N O I NFO or R EFUTES, respectively.
S UPPORTS and R EFUTES training examples are
selected from evidence sets of cited abstracts for
each claim. The sentences in each evidence set
will be concatenated with the claim in the above
input sequence format as a single example for the
corresponding label. The N O I NFO examples are
selected by concatenating 1 or 2 randomly-selected
non-rationale sentences from each of the cited abstracts across all labels. Here, we fine-tuned a fresh
T5-3B (that was just pretrained on the mixture task)
and not a fine-tuned monoT5 since there is no natural transfer from the relevance ranking task. We use
a batch size of 128 and pick out the best checkpoint
after [200, 400, 600, 800, 1000] steps based on the
development set scores.
During inference, the token with the highest
probability will be the label ŷ(q, a) for abstract
a ∈ E(q).

4
4.1

refute each claim are annotated with rationale sentences (see Table 1 for examples). The label distribution is provided in Table 2. There are 1,409
claims, 809 of which are part of the training set
and the rest are split equally across the development and test set. The test set is balanced with 100
claims for each class (S UPPORTS, N O I NFO and
R EFUTES). Yet from Table 2, it is clear that the
training and development sets have significant class
imbalance. This coupled with the small set sizes
highlight the importance of zero-shot or few-shot
systems for this task.
To show that our system is able to verify claims
related to COVID-19 by identifying evidence from
the much larger CORD-19 corpus,4 we evaluate
V ERT5 ERINI in a zero-shot setting on two other
datasets:
COVID-19 S CI FACT (Wadden et al., 2020) is a set
of 36 COVID-related claims curated by a medical
student. In this set, the same claim can sometimes
be both supported and refuted by different abstracts,
a scenario not observed in the main S CI FACT task.
Two examples in this set are shown in Table 4.
COVID-19 Scientific (Lee et al., 2020) contains
142 claims (label distribution in Table 3) gathered by collecting COVID-related scientific truths
and myths from sources like the Centers for Disease Control and Prevention (CDC), MedicalNewsToday, and the World Health Organization
(WHO). Unlike the other two datasets, COVID19 Scientific only provides a single label y(q) ∈
{S UPPORTS, R EFUTES} for a claim. They mention that in the construction of the dataset, claims
that were unverifiable according to the CDC or the
WHO were mapped to R EFUTES. Hence, we make
the following modifications to V ERT5 ERINI:
• First if ŷ(q, a) = N O I NFO, then ŷ(q, a) is modified to R EFUTES.
• Second, ŷ(q) = maxa∈Ê(q) ŷ(q, a).

Experimental Setup
Datasets

• Third, if |
4

S CI FACT (Wadden et al., 2020) consists of a corpus of 5,183 abstracts. Abstracts that support or

S

a∈Ê(q) Ŝ(q, a)|

= 0 i.e. the set of all

We use the 2020-06-17 dump of CORD-19, which contains 192,459 abstracts, about 40 times as many abstracts as
those in S CI FACT.

Claim

Label

Evidence

Hypertension and Dia- S UPPORTS
betes are the most common comorbidities for
COVID-19.

Investigations reported that hypertension, diabetes, and cardiovascular
diseases were the most prevalent comorbidities among the patients with
coronavirus disease 2019 (COVID-19). . . . The aim of this review was to
summarize the current knowledge about the relationship between hypertension
and COVID-19 and the role of hypertension on outcome in these patients.

The Secondary Attack R EFUTES
rate of COVID-19
is 10.5% for household members/close
contacts.

Background: As of April 2, 2020, the global reported number of COVID-19
cases has crossed over 1 million with more than 55,000 deaths. . . . We estimated the household SAR to be 13.8% (95% CI: 11.1–17.0%) if household
contacts are defined as all close relatives and 19.3% (95% CI: 15.5–23.9%)
if household contacts only include those at the same residential address as
the cases, assuming a mean incubation period of 4 days and a maximum
infectious period of 13 days.

Table 4: Two COVID-19 claims from S CI FACT, their predicted labels and their corresponding predicted evidence
(rationale highlighted in bold).
Claim

Label

Evidence

Some people become
infected by COVID-19
but don’t develop any
symptoms and don’t
feel unwell.

S UPPORTS

COVID-19 is an emerging infectious disease with widespread transmission of
the coronavirus SARS-CoV-2 in the Netherlands. . . . Others do not show any
symptoms, but can still contribute to the transmission of the virus. . . .

Young people will not
get COVID-19.

R EFUTES

Objective: To explore the epidemiological characteristics of COVID-19 associated with SARS-Cov-2 in Guizhou province, and to compare the differences
in epidemiology with other provinces. . . . Most of COVID-19 patients were
18-45 years old (52.27%). . . . CONCLUSION: Among the cases, most
patients were young adults.

Bill Gates caused the in- R EFUTES
fection of COVID-19.

N/A

Table 5: Three COVID-19 Scientific claims from Lee et al. (2020), their predicted labels and their corresponding
predicted evidence (rationale highlighted in bold). Note that if the system can not find any supporting evidence
for a claim, it refutes the claim.

evidence sentences across the abstracts is empty,
then ŷ(q) = R EFUTES.
Three examples from this set are shown in Table 5. As once can imagine, it would be impossible to find any discussion for outlandish claims
like “Bill Gates caused the infection of COVID19” in a corpus of biomedical literature and hence
V ERT5 ERINI maps it to R EFUTES.
4.2

Baselines

For the S CI FACT and COVID-19 S CI FACT
end-to-end tasks, the baseline system used is
V ERI S CI (Wadden et al., 2020). It has an abstract
retrieval module that uses TF-IDF, a sentence selection module trained on S CI FACT and a label
prediction module trained on FEVER + S CI FACT.
For the abstract retrieval module, they note the best
full-pipeline development set scores by retrieving
the top three documents.

For the COVID-19 Scientific task, we compare
with the following two baselines established by Lee
et al. (2020):
• LiarMisinfo (Lee et al., 2020) uses a BERTlarge (Devlin et al., 2019) label prediction model
fine-tuned on LIAR-PolitiFact (Wang, 2017), a
set of 12.8k claims collected from PolitiFact. It
is worth noting that LIAR-PolitiFact does not
contain any claims related to COVID-19.
• LM Debunker (Lee et al., 2020) uses GPT-2 (Radford et al., 2019) to determine the perplexity of
the claim given evidence sentences. Claims with
a perplexity score higher than a threshold are
labeled R EFUTES while the others are labeled
S UPPORTS.
The sentence selection module in both baselines
employ TF-IDF followed by some rule-based evidence filtering to select the top three sentences

Method

R@3

R@5

Oracle
TF-IDF

97.61
69.38

100.00
75.60

BM25
T5 (MS MARCO)
T5 (MS MARCO MED)
T5 (S CI FACT)

79.90
86.12
85.65
86.60

84.69
89.95
89.00
89.40

Table 6: Comparison of abstract retrieval methods on
the development set of S CI FACT.
Method

P

R

F1

RoBERTa-large

73.71

70.49

72.07

T5

79.29

73.22

76.14

Table 7: Comparison of different sentence selection
methods on S CI FACT’s development set.

for each claim. LiarMisinfo represents a zero-shot
model where no fine-tuning is performed on the
COVID-19 Scientific set. LM Debunker, on the
other hand, first partitions the set into a validation
and a test set. Then, the validation set is used to
tune the perplexity threshold for the model following which evaluation is performed on the test set.

5
5.1

Results
Abstract Retrieval

Table 6 reports R@3 and R@5 for abstract retrieval.
The oracle (first row) shows that most claims from
the development set have fewer than 3 relevant
abstracts and all have fewer than 5. For comparison, we show the effectiveness of TF-IDF used by
Wadden et al. (2020).
We find that using BM25 results in a effectiveness improvement of around 10 points in comparison to the TF-IDF baseline. Using T5 to rerank the
top-20 abstracts retrieved from BM25 results in a
17-point improvement over baseline.
However, there is almost no difference in efficacy whether T5 was fine-tuned on S CI FACT or on
MS MARCO MED. This is potentially due to the
relatively small size of the S CI FACT dataset and
that MS MARCO MED data is not entirely relevant to the target task. Hence, we use T5 trained
on MS MARCO in the full pipeline experiments
(Section 5.4).
5.2

Sentence Selection

Table 7 reports the precision, recall and F1 scores
for the sentence selection task.
We find that T5 (MS MARCO) fine-tuned on
S CI FACT outperforms the RoBERTa-large base-

Method

Label

P

R

F1

RoBERTa-large

S UPPORTS
N O I NFO
R EFUTES

92.56
74.82
77.05

81.16
92.86
66.20

86.49
82.87
71.21

T5

S UPPORTS
N O I NFO
R EFUTES

93.13
85.25
86.76

88.41
92.86
83.10

90.71
88.89
84.89

Table 8: Comparison of different label prediction models, on S CI FACT’s development set.

line fine-tuned on S CI FACT used by Wadden et al.
(2020). This result together with those from Table 6
demonstrates the effectiveness of the T5 model at
selecting evidence in various levels of granularity.
5.3

Label Prediction

In Table 8, we present label-wise precision, recall,
and F1 scores for the label prediction task. In the
case of S UPPORTS and R EFUTES labels, the input
to the model are gold rationales from cited abstracts.
The exception is for N O I NFO labels, whose cited
abstracts are available but no gold rationales exist.
In this case, we pick the two most similar sentences
according to TF-IDF from each of these abstracts.
The results across all labels demonstrate that
T5 fine-tuned on S CI FACT’s label prediction task
shows significant improvements over the baseline
RoBERTa-large that was fine-tuned on FEVER followed by fine-tuning on S CI FACT’s label prediction task. We believe some of this can be credited
to T5’s pretraining on a mixture of multiple tasks.
Although this mixture does not include FEVER,
it contains various other NLI datasets including
MNLI (Williams et al., 2018) and QNLI (Rajpurkar
et al., 2016).
5.4

Full Pipeline

In Table 9 and 10, we report the precision, recall and F1 scores of abstract-level evaluation
and sentence-level evaluation respectively for full
pipeline systems.
Rows 1, 2, 6, 7 present the scores in the oracle abstract retrieval setting, where gold evidence
abstracts are provided to systems. We see that
our pipeline outperforms V ERI S CI by around 10
F1 points at both the abstract and sentence level.
The improvements are even more significant in
the AbstractLabel+Rationale and SentenceSelection+Label
evaluation settings (rows 6, 7 in Tables 9 and 10,
respectively) which require more from systems in
terms of sentence selection and label prediction.

Label Only

Label Only

Method

P

R

F1

Method

P

R

F1

(1) Oracle (V ERI S CI)
(2) Oracle (ours)

90.97
92.70

67.46
78.95

77.47
85.27

(3) V ERI S CI

55.31

47.37

51.03

(1) V ERI S CI
(2) V ERT5 ERINI (BM25)
(3) V ERT5 ERINI (T5)

47.5
63.1
63.6

47.3
60.8
66.2

47.4
61.9
64.9

(4) V ERT5 ERINI (BM25)
(5) V ERT5 ERINI (T5)

70.88
65.07

61.72
65.07

65.98
65.07

Method

P

R

F1

(4) V ERI S CI
(5) V ERT5 ERINI (BM25)
(6) V ERT5 ERINI (T5)

46.6
60.3
61.5

46.4
58.1
64.0

46.5
59.2
62.7

Label+Rationale

Label+Rationale
Method

P

R

F1

(6) Oracle (V ERI S CI)
(7) Oracle (ours)

85.16
88.76

63.16
75.60

72.53
81.65

(8) V ERI S CI

52.51

44.98

48.45

Method

P

R

F1

62.40
61.72

(7) V ERI S CI
(8) V ERT5 ERINI (BM25)
(9) V ERT5 ERINI (T5)

45.0
64.9
66.2

47.3
58.9
63.5

46.1
61.8
64.8

P

R

F1

38.6
58.3
60.0

40.5
53.0
57.6

39.5
55.5
58.8

(9) V ERT5 ERINI (BM25)
(10) V ERT5 ERINI (T5)

67.03
61.72

58.37
61.72

Selection Only

Table 9: Full pipeline abstract-level effectiveness on
S CI FACT’s development set.

Selection+Label
Method
(10) V ERI S CI
(11) V ERT5 ERINI (BM25)
(12) V ERT5 ERINI (T5)

Selection Only
Method

P

R

F1

(1) Oracle (V ERI S CI)
(2) Oracle (ours)

79.41
83.54

59.02
72.13

67.71
77.42

(3) V ERI S CI

52.46

43.72

47.69

(4) V ERT5 ERINI (BM25)
(5) V ERT5 ERINI (T5)

67.70
64.81

53.83
57.37

59.97
60.87

Selection+Label
Method

P

R

F1

(6) Oracle (V ERI S CI)
(7) Oracle (ours)

71.32
78.16

53.01
67.49

60.82
72.43

(8) V ERI S CI

46.89

39.07

42.62

63.92
60.80

50.82
53.83

56.62
57.10

(9) V ERT5 ERINI (BM25)
(10) V ERT5 ERINI (T5)

Table 10: Full pipeline sentence-level effectiveness on
S CI FACT’s development set.

In rows 3-5 and 8-10, we report scores in the full
pipeline setting where systems are also required
to retrieve relevant abstracts. We evaluate two full
pipeline systems, one that used BM25 alone and
another that used BM25 followed by T5 (MARCO)
for abstract retrieval. Both these systems outperform the baseline system V ERI S CI by about 14 F1
points. This comes as no surprise seeing that our
models displayed significant improvements along
each of the three steps.
Notice that in Table 6, using T5 (MARCO)
brings large gains in terms of R@3 of the BM25
baseline. Yet in the case of the full-pipeline with
these two abstract retrieval methods, we only notice
comparable efficacy on the development set. We
believe this might be linked to the relatively small

Table 11: Full pipeline effectiveness on S CI FACT’s test
set.

size of the development set and choose to probe the
S CI FACT hidden test set with both these systems.
From Table 11, it is clear that in the hidden
test set, both our systems outperform the baseline V ERI S CI, with evaluation aspects like Sentence+Label (rows 10-12) showing relative improvements of around 50%. Comparing with respective scores in Table 9, 10, we also see no indication of overfitting. We also note that abstract
retrieval using the two-stage approach brings significant gains here (rows 5, 11 vs 6, 12) unlike in
the development set. This shows that neural reranking, even though used in a zero-shot formulation, is
critical to getting higher quality abstracts from the
corpus C thereby improving effectiveness in later
stages too.
5.5

Verification of COVID-19 claims

Finally we evaluate our most effective pipeline,
V ERT5 ERINI (T5), on the two sets of COVIDrelated claims. We do this in a zero-shot paradigm
in that we do not fine-tune our model on either of
these sets.
In the COVID-19 S CI FACT set, for each claim
q, we use V ERT5 ERINI (T5) to predict evidence
abstracts, Ê(q). A (q, Ê(q)) pair is considered plausible if at least half of the evidence abstracts in Ê(q)
are found to have reasonable rationales and labels.

Method

Accuracy

F1-Macro

F1-Binary

LiarMisinfo
LM Debunker

61.5
75.4

59.2
69.8

82.8
83.1

V ERT5 ERINI (T5)

78.2

73.2

83.8

Table 12: Label prediction effectiveness on COVID-19
Scientific claims

For 30/36 claims, we find that V ERT5 ERINI (T5)
provides plausible evidence abstracts. These claims
have reasonable labels and evidence rationales selected successfully from evidence abstracts. This
is in comparison to the 23/36 claims that V ERI S CI
provides plausible evidence, demonstrating the effectiveness of our system in the zero-shot case.
In the COVID-19 Scientific set, we compare
the effectiveness of V ERT5 ERINI with that of two
baselines considered by Lee et al. (2020). Table 12
reports the accuracy, the F1-Macro and the F1Binary scores on the test set. The F1-Binary score
corresponds to the F1 score of the R EFUTES label,
since debunking misinformation is critical. Note
that the LM Debunker baseline uses the average
scores across a four-fold cross-validation on the
test set, unlike V ERT5 ERINI and LiarMisinfo. We
observe that V ERT5 ERINI outperforms both the
baselines in a zero-shot setting, without any in-task
tuning like the LM Debunker. The adaptability of
V ERT5 ERINI to both these new tasks with no additional training makes a strong case for the strength
of our pipeline.

6

Conclusion

We introduced V ERT5 ERINI, a novel pipeline
for scientific claim verification that exploits a
generation-based approach to abstract ranking, sentence selection and claim verification. Such systems are of significance in this age of misinformation amplified by the COVID-19 pandemic. We
find that our system outperforms the state of art in
the end-to-end task. We note improvements in each
of the three steps, demonstrating the importance
of this generative approach as well as zero-shot
and few-shot transfer capabilities. Finally, we find
that V ERT5 ERINI generalizes to two new COVID19 related sets with no tuning of parameters while
maintaining high efficacy.
Yet, there is still a large gap between our system
and an oracle. Ideally, a system that performs scientific claim verification should possess additional
attributes like:
• Numerical reasoning — the ability to interpret

statistical and numerical findings and ranges.
• Biomedical background — the ability to leverage
knowledge about domain-specific lexical relationships.
Future work that incorporate such attributes might
be critical towards building higher quality scientific
fact verification systems.

Acknowledgements
This research was supported in part by the Canada
First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC)
of Canada. Additionally, we would like to thank
Google for computational resources in the form of
Google Cloud credits.

References
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,
Jianfeng Gao, Xiaodong Liu, Rangan Majumder,
Andrew McNamara, Bhaskar Mitra, Tri Nguyen,
Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2016. MS MARCO: A human generated MAchine Reading COmprehension
dataset. arXiv:1611.09268.
Zhuyun Dai and Jamie Callan. 2019. Context-aware
sentence/passage term importance estimation for
first stage retrieval. arXiv:1910.10687.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota.
Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,
Eric Lehman, Caiming Xiong, Richard Socher, and
Byron C. Wallace. 2020. ERASER: A benchmark to
evaluate rationalized NLP models. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 4443–4458, Online. Association for Computational Linguistics.
Andreas Hanselowski, Christian Stab, Claudia Schulz,
Zile Li, and Iryna Gurevych. 2019. A richly annotated corpus for different tasks in automated factchecking. In Proceedings of the 23rd Conference on Computational Natural Language Learning
(CoNLL), pages 493–503, Hong Kong, China. Association for Computational Linguistics.
Nayeon Lee, Yejin Bang, Andrea Madotto, and Pascale Fung. 2020. Misinformation has high perplexity. arXiv:2006.04666.

Sean MacAvaney, Arman Cohan, and Nazli Goharian. 2020. SLEDGE: A Simple Yet Effective Baseline for COVID-19 Scientific Knowledge Search.
arXiv:2005.02365.
Sean MacAvaney, Andrew Yates, Arman Cohan, and
Nazli Goharian. 2019. CEDR: Contextualized embeddings for document ranking. In Proceedings of
the 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 1101–1104.
Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and
Jimmy Lin. 2020. Document ranking with a pretrained sequence-to-sequence model. In Findings of
EMNLP.
Rodrigo Nogueira, Wei Yang, Jimmy Lin, and
Kyunghyun Cho. 2019. Document expansion by
query prediction. arXiv:1904.08375.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
Models are Unsupervised Multitask Learners. OpenAI Blog.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text transformer. arXiv:1910.10683.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, Austin,
Texas.
Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina
Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen
Voorhees, Lucy Lu Wang, and William R. Hersh.
2020. TREC-COVID: Rationale and structure of
an information retrieval shared task for COVID-19.
Journal of the American Medical Informatics Association.
Stephen E. Robertson, Steve Walker, Susan Jones,
Micheline Hancock-Beaulieu, and Mike Gatford.
1994. Okapi at TREC-3. In Proceedings of the
3rd Text REtrieval Conference (TREC-3), pages 109–
126.
Amir Soleimani, Christof Monz, and Marcel Worring.
2019. BERT for Evidence Retrieval and Claim Verification. arXiv:1910.02655.
Raphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil
Gupta, Phuong Cam, Kyunghyun Cho, and Jimmy
Lin. 2020. Rapidly bootstrapping a question answering dataset for COVID-19. arXiv:2004.11339.
James Thorne,
Andreas Vlachos,
Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for Fact Extraction
and VERification. In Proceedings of the 2018

Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers), pages 809–819, New Orleans, Louisiana.
Association for Computational Linguistics.
David Wadden, Kyle Lo, Lucy Lu Wang, Shanchuan
Lin, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying
scientific claims. In EMNLP.
William Yang Wang. 2017. “Liar, Liar Pants on Fire”:
A new benchmark dataset for Fake News Detection.
arXiv:1705.00648.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguistics.
Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:
Enabling the use of Lucene for information retrieval
research. In Proceedings of the 40th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2017),
pages 1253–1256, Tokyo, Japan.
Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini:
Reproducible ranking baselines using Lucene. Journal of Data and Information Quality, 10(4):Article
16.
Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian
Zhang, and Jimmy Lin. 2019. Cross-domain modeling of sentence-level evidence for document retrieval. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
3481–3487.
Edwin Zhang, Nikhil Gupta, Raphael Tang, Xiao Han,
Ronak Pradeep, Kuang Lu, Yue Zhang, Rodrigo
Nogueira, Kyunghyun Cho, Hui Fang, and Jimmy
Lin. 2020. Covidex: Neural Ranking Models and
Keyword Search Infrastructure for the COVID-19
Open Research Dataset. arXiv:2007.07846.

