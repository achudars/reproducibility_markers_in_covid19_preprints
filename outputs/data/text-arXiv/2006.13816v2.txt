Document Classification for COVID-19 Literature
Bernal Jiménez Gutiérrez, Juncheng Zeng, Dongdong Zhang, Ping Zhang, Yu Su
The Ohio State University
{jimenezgutierrez.1,zeng.671,zhang.11069,
zhang.10631,su.809}@osu.edu

arXiv:2006.13816v2 [cs.IR] 9 Sep 2020

Abstract
The global pandemic has made it more important than ever to quickly and accurately retrieve relevant scientific literature for effective
consumption by researchers in a wide range
of fields. We provide an analysis of several
multi-label document classification models on
the LitCovid dataset, a growing collection of
23,000 research papers regarding the novel
2019 coronavirus. We find that pre-trained language models fine-tuned on this dataset outperform all other baselines and that BioBERT surpasses the others by a small margin with microF1 and accuracy scores of around 86% and
75% respectively on the test set. We evaluate
the data efficiency and generalizability of these
models as essential features of any system prepared to deal with an urgent situation like the
current health crisis. Finally, we explore 50 errors made by the best performing models on
LitCovid documents and find that they often
(1) correlate certain labels too closely together
and (2) fail to focus on discriminative sections
of the articles; both of which are important issues to address in future work. Both data and
code are available on GitHub 1 .

1

Introduction

The COVID-19 pandemic has made it a global priority for research on the subject to be developed
at unprecedented rates. Researchers in a wide variety of fields, from clinicians to epidemiologists
to policy makers, must all have effective access to
the most up to date publications in their respective
areas. Automated document classification can play
an important role in organizing the stream of articles by fields and topics to facilitate the search
process and speed up research efforts.
To explore how document classification models
can help organize COVID-19 research papers, we
use the LitCovid dataset (Chen et al., 2020), a collection of 23,000 newly released scientific papers
1
https://github.com/dki-lab/
covid19-classification

compiled by the NIH to facilitate access to the literature on all aspects of the virus. This dataset
is updated daily and every new article is manually assigned one or more of the following 8 categories: General, Transmission Dynamics (Transmission), Treatment, Case Report, Epidemic Forecasting (Forecasting), Prevention, Mechanism and
Diagnosis. We leverage these annotations and the
articles made available by LitCovid to compile a
timely new dataset for multi-label document classification.
Apart from addressing the pressing needs of the
pandemic, this dataset also offers an interesting
document classification dataset which spans different biomedical specialities while sharing one overarching topic. This setting is distinct from other
biomedical document classification datasets which
tend to exclusively distinguish between biomedical
topics such as hallmarks of cancer (Baker et al.,
2016), chemical exposure methods (Baker, 2017)
or diagnosis codes (Du et al., 2019). The dataset’s
shared focus on the COVID-19 pandemic also sets
it apart from open-domain datasets and academic
paper classification datasets such as IMDB or the
arXiv Academic Paper Dataset (AAPD) (Yang
et al., 2018) in which no shared topic can be found
in most of the documents, and it poses unique challenges for document classification models.
We evaluate a number of models on the LitCovid
dataset and find that fine-tuning pre-trained language models yields higher performance than traditional machine learning approaches and neural
models such as LSTMs (Adhikari et al., 2019b;
Kim, 2014; Liu et al., 2017). We also notice that
BioBERT (Lee et al., 2019), a BERT model pretrained on the original corpus for BERT plus a large
set of PubMed articles, performed slightly better
than the original BERT base model. We also observe that the novel Longformer (Beltagy et al.,
2020) model, which allows for processing longer
sequences, matches BioBERT’s performance when

# of Classes
# of Articles
Avg. sentences
Avg. tokens
Total # of tokens

LitCovid
8
23,038
74
1,399
32,239,601

CORD-19 Test
8
100
109
2,861
286,065

Class
Prevention
Treatment
Diagnosis
Mechanism
Case Report
Transmission
Forecasting
General

Table 1: Dataset statistics for the LitCovid and Test CORD-19
Datasets.

1024 subwords are used instead of 512, the maximum for BERT models.
We then explore the data efficiency and generalizability of these models as crucial aspects to
address for document classification to become a
useful tool against outbreaks like this one. Finally,
we discuss some issues found in our error analysis such as current models often (1) correlating
certain categories too closely with each other and
(2) failing to focus on discriminative sections of a
document and get distracted by introductory text
about COVID-19, which suggest venues for future
improvement.

2

Datasets

In this section, we describe the LitCovid dataset
in more detail and briefly introduce the CORD-19
dataset which we sampled to create a small test set
to evaluate model generalizability.
2.1

LitCovid

The LitCovid dataset is a collection of recently published PubMed articles which are directly related
to the 2019 novel Coronavirus. The dataset contains upwards of 23,000 articles and approximately
2,000 new articles are added every week, making it
a comprehensive resource for keeping researchers
up to date with the current COVID-19 crisis.
For a large portion of the articles in LitCovid,
either the full article or at least the abstract can be
downloaded directly from their website. For our
document classification dataset, we select 23,038
articles which contain full texts or abstracts from
the 35,000+ articles available on August 1st , 2020.
As seen in table 1, these selected articles contain
on average approximately 74 sentences and 1,399
tokens, reflecting the roughly even split between
abstracts and full articles we observe from inspection.
Each article in LitCovid is assigned one or more
of the following 8 topic labels: Prevention, Treatment, Diagnosis, Mechanism, Case Report, Transmission, Forecasting and General. Even though
every article in the corpus can be labelled with mul-

LitCovid
11,042
6,897
4,754
3,549
1,914
1,065
461
368

CORD-19 Set
12
20
25
70
2
6
2
7

Table 2: Number of documents in each category for the LitCovid and CORD-19 Test Datasets.

tiple tags, most articles, around 76%, contain only
one label. Table 2 shows the label distribution for
the subset of LitCovid which is used in the present
work. We note that there is a large class imbalance,
with the most frequently occurring label appearing
almost 20 times as much as the least frequent one.
We split the LitCovid dataset into train, dev, test
with the ratio 7:1:2.
2.2

CORD-19

The COVID-19 Open Research Dataset (CORD19) (Wang et al., 2020) was one of the earliest
datasets released to facilitate cooperation between
the computing community and the many relevant
actors of the COVID-19 pandemic. It consists of
approximately 60,000 papers related to COVID19 and similar coronaviruses such as SARS and
MERS since the SARS epidemic of 2002. Due to
their differences in scope, this dataset shares only
around 1,200 articles with the LitCovid dataset.
In order to test how our models generalize to
a different setting, we asked biomedical experts
to label a small set of 100 articles found only in
CORD-19. Each article was labelled independently
by two annotators. For articles which received
two different annotations (around 15%), a third
annotator broke ties. Table 1 shows the statistics
of this small set and Table 2 shows its category
distribution.

3

Models

In the following section we provide a brief description of each model and the implementations
used. We use micro-F1 (F1) and accuracy (Acc.)
as our evaluation metrics, as done in (Adhikari
et al., 2019a). All reproducibility information can
be found in Appendix A.
3.1

Traditional Machine Learning Models

To compare with simpler but competitive traditional baselines we use the default scikit-learn (Pedregosa et al., 2011) implementation of logistic re-

Model
LR
SVM
LSTM
LSTMreg
KimCNN
XML-CNN
BERTbase
BERTlarge
Longformer
BioBERT

Dev Set
Acc.
F1
68.5
81.4
71.2
83.4
69.0 ±0.9 83.9 ±0.1
71.2 ±0.5 83.9 ±0.3
69.9 ±0.2 83.3 ±0.3
72.9 ±0.4 84.1 ±0.2
74.3 ±0.6 85.5 ±0.4
75.1 ±3.9 85.9 ±1.9
74.4 ±0.8 85.6 ±0.5
75.0 ±0.5 86.3 ±0.2

Test Set
Acc.
F1
68.6
81.4
70.7
83.3
68.9 ±0.3 83.2 ±0.2
70.8 ±0.7 83.6 ±0.5
68.8 ±0.1 82.7 ±0.1
71.7 ±0.7 83.5 ±0.3
73.6 ±1.0 85.1 ±0.5
74.4 ±2.7 85.3 ±1.4
73.9 ±0.8 85.5 ±0.5
75.2 ±0.7 86.2 ±0.6

Table 3: Performance for each model expressed as mean ±
standard deviation across three training runs.

gression and linear support vector machine (SVM)
for multi-label classification which trains one classifier per class using a one-vs-rest scheme. Both
models use TF-IDF weighted bag-of-words as input.
3.2

Conventional Neural Models

Figure 1: Data efficiency analysis. Pre-trained language
models achieve their maximum performance on only 20% of
the training data.

ods by a sizable margin in both accuracy and microF1 score. The best performing models is BioBERT
which achieves a micro-F1 score of 86.2% and an
accuracy of 75.2% on the test set.

Using Hedwig2 , a document classification toolkit,
we evaluate the following models: KimCNN (Kim,
2014), XML-CNN (Liu et al., 2017) as well as an
unregularized and a regularized LSTM (Adhikari
et al., 2019b). We notice that they all perform
similarly and slightly better traditional methods.

4

3.3

During a sudden healthcare crisis like this pandemic it is essential for models to obtain useful results as soon as possible. Since labelling
biomedical articles is a very time-consuming process, achieving peak performance using less data
becomes highly desirable. We thus evaluate the
data efficiency of these models by training each of
the ones shown in Figure 1 using 1%, 5%, 10%,
20% and 50% of our training data and report the
micro-F1 score on the dev set. When selecting
the data subsets, we sample each category independently to make sure they are all represented.
We observe that pre-trained models are much
more data-efficient than other models and that
BioBERT is the most efficient, demonstrating the
importance of domain-specific pre-training.

Pre-Trained Language Models

Using the same Hedwig document classification
toolkit, we evaluate the performance of DocBERT
(Adhikari et al., 2019a) on this task with a few different pre-trained language models. We fine-tune
BERT base, BERT large (Devlin et al., 2019) and
BioBERT (Lee et al., 2019), a version of BERT
base which was further pre-trained on a collection
of PubMed articles. We find all BERT models
achieve best performance with their highest possible sequence length of 512 subwords. Additionally,
we fine-tune the pre-trained Longformer (Beltagy
et al., 2020) in the same way and find that it performs best when a maximum sequence length of
1024 is used. As in the original Longformer paper, we use global attention on the [CLS] token for
document classification but find that performance
improves by around 1% if we use the average of
all tokens as input instead of only the [CLS] representation. We hypothesize that this effect can
be observed because the LitCovid dataset contains
longer documents on average that the Hyperpartisan dataset used in the original Longformer paper.
We find that all pre-trained language models outperform the previous traditional and neural meth2

https://github.com/castorini/hedwig

Results & Discussion

In this section, we explore data efficiency, model
generalizability and discuss potential ways to improve performance on this task in future work.
4.1

4.2

Data Efficiency

CORD-19 Generalizability

To effectively respond to this pandemic, experts
must not only learn as much as possible about the
current virus but also thoroughly understand past
epidemics and similar viruses. Thus, it is crucial for
models trained on the LitCovid dataset to successfully categorize articles about related epidemics.
We therefore evaluate some of our baselines on
such articles using our labelled CORD-19 subset.
We find that the accuracy and micro-F1 metrics

Article
Analysis on epidemic situation and spatiotemporal changes of COVID-19 in Anhui.
... We mapped the spatiotemporal changes of confirmed cases, fitted the epidemic situation by the population growth
curve at different stages and took statistical description and analysis of the epidemic situation in Anhui province.
2019 Novel coronavirus: where we are and what we know.
There is a current worldwide outbreak of a new type of coronavirus (2019-nCoV), which originated from Wuhan in
China and has now spread to 17 other countries.
... This paper aggregates and consolidates the virology, epidemiology, clinical management strategies ...
In addition, by fitting the number of infections with a single-term exponential model ...
Managing Cancer Care During the COVID-19 Pandemic: Agility and Collaboration Toward a Common Goal.
The first confirmed case of coronavirus disease 2019 (COVID-19) in the United States was reported on
January 20, 2020, in Snohomish County, Washington. ...

Label

Prediction

Forecasting

Prevention
Forecasting

Treatment
Mechanism
Transmission
Forecasting

Prevention
Forecasting

Treatment

Prevention

Table 4: LitCovid Error Samples. Sentences relevant to the article’s category are highlighted with blue and general ones with
red.

drop by approximately 35 and 15 points respectively. This massive drop in performance from a
minor change in domain indicates that the models
have trouble ignoring the overarching COVID-19
topic and isolating relevant signals from each category.

It is interesting to note that Mechanism is the
only category for which BioBERT performs better
in CORD-19 than in LitCovid. This could be due
to Mechanism articles using technical language and
there being enough samples for the models to learn;
in contrast with Forecasting which also uses specific language but has far fewer training examples.
BioBERT’s binary F1 scores for each category on
both datasets can be found in Appendix B.

Forecasting tags are predicted in conjunction with
the Prevention tag much more frequently than what
is observed in the labels as can be seen from the
table in Appendix C. Future work should attempt
to explicitly model correlation between categories
to help the model recognize the particular cases
in which labels should occur together. The first
row in Table 4 shows a document labelled as Forecasting which is also incorrectly predicted with a
Prevention label, exemplifying this issue.
Finally, we observe that models have trouble
identifying discriminative sections of the document
due to how much introductory content on the pandemic can be found in most articles. Future work
should explicitly model the gap in relevance between introductory sections and crucial sentences
such as thesis statements and article titles. In Table
4, the second and third examples would be more
easily classified correctly if specific sentences were
ignored while others attended to more thoroughly.
This could also increase interpretability, facilitating
analysis and further improvement.

4.3

5

SVM
LSTMreg
Longformer
BioBERT

Acc.
29.0
32.7 ±1.5
41.3 ±6.4
36.0 ±7.8

F1
62.8
67.7 ±0.7
70.0 ±2.9
69.7 ±2.8

Table 5: Performance on the CORD-19 Test Set expressed as
mean ± standard deviation across three training runs.

Error Analysis

We analyze 50 errors made by both highest scoring BioBERT and the Longformer models on LitCovid documents to better understand their performance. We find that 34% of these were annotation errors which our best performing model predicted correctly. We also find that 10% of the errors
were nearly impossible to classify using only the
text available on LitCovid, and the full articles are
needed to make better-informed prediction. From
the rest of the errors we identify some aspects of
this task which should be addressed in future work.
We first note these models often correlate certain categories, namely Prevention, Transmission
and Forecasting, much more closely than necessary.
Even though these categories are semantically related and some overlap exists, the Transmission and

Conclusion

We provide an analysis of document classification
models on the LitCovid dataset for the COVID19 literature. We determine that fine-tuning pretrained language models yields the best performance on this task. We study the generalizability
and data efficiency of these models and discuss
some important issues to address in future work.

Acknowledgments
This research was sponsored in part by the Ohio
Supercomputer Center (Center, 1987). The authors
would also like to thank Lang Li and Tanya BergerWolf for helpful discussions.

References
Ashutosh Adhikari, Achyudh Ram, Raphael Tang, and
Jimmy Lin. 2019a. Docbert: Bert for document classification. ArXiv, abs/1904.08398.
Ashutosh Adhikari, Achyudh Ram, Raphael Tang, and
Jimmy Lin. 2019b. Rethinking complex neural network architectures for document classification. In
NAACL-HLT.
Simon Baker. 2017. Corpus and Software.
Simon Baker, Ilona Silins, Yufan Guo, Imran Ali, Johan Högberg, Ulla Stenius, and Anna Korhonen.
2016. Automatic semantic classification of scientific literature according to the hallmarks of cancer.
Bioinformatics, 32 3:432–40.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv:2004.05150.
Ohio Supercomputer Center. 1987. Ohio supercomputer center.
Q. Chen, A. Allot, and Z. Lu. 2020. Keep up with the
latest coronavirus research. Nature, 579(7798):193.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of deep
bidirectional transformers for language understanding. ArXiv, abs/1810.04805.
Jingcheng Du, Qingyu Chen, Yifan Peng, Yang Xiang,
Cui Tao, and Zhiyong Lu. 2019. Ml-net: multi-label
classification of biomedical texts with deep neural
networks. Journal of the American Medical Informatics Association : JAMIA.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim,
Donghyeon Kim, Sunkyu Kim, Chan Ho So,
and Jaewoo Kang. 2019. BioBERT: a pre-trained
biomedical language representation model for
biomedical text mining. Bioinformatics.
Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang. 2017. Deep learning for extreme multilabel text classification. Proceedings of the 40th
International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Gilles Louppe, Peter
Prettenhofer, Ron Weiss, Vincent Dubourg, Jacob
VanderPlas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and Edouard
Duchesnay. 2011. Scikit-learn: Machine learning
in python. J. Mach. Learn. Res., 12:2825–2830.

Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,
Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn
Funk, Rodney Michael Kinney, Ziyang Liu, William.
Merrill, Paul Mooney, Dewey A. Murdick, Devvret
Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Christopher Wilhelm, Boya Xie, Douglas M. Raymond, Daniel S.
Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020.
Cord-19: The covid-19 open research dataset.
ArXiv, abs/2004.10706.
Pengcheng Yang, Xu Sun, Wei Li, Shuming Ma, Wei
Wu, and Houfeng Wang. 2018. Sgm: Sequence generation model for multi-label classification. In COLING.

A

Experimental Set-up

We split the LitCovid dataset into train, dev, test
with the ratio 7:1:2.
We adopt micro-F1 and accuracy as our evaluation metrics, same as (Adhikari et al., 2019a).
We use scikit-learn (Pedregosa et al., 2011) and
Hedwig evaluation scripts to evaluate all the models. For preprocessing, tokenization and sentence
segmentation, we use the NLTK library.
All the document classification models used in
the paper, logistic regression 1 SVM 2 DocBERT
3 , Reg-LSTM 4 , Reg-LSTM 5 , XML-CNN 6 , Kim
CNN 7 are run based on the implementations listed
here and strictly followed their instructions. We
used the following pre-trained language models,
BioBERT 8 , BERT base 9 , BERT large 10 and the
Longformer 11 .
For reproducibility, we list all the key hyperparameters, the tuning bounds and the # of parameters
for each model in Table A1. For the logistic regression and the SVM all hyperparameters used were
default to scikit-learn and therefore are excluded
from this table. For all models we train for a maximum of 30 epochs with a patience of 5. We used
micro-F1 score for all hyperparameter tuning. All
models were run on NVIDIA GeForce GTX 1080
GPUs.

1
https://scikit-learn.org/stable/
modules/generated/\sklearn.linear_model.
LogisticRegression.html
2
https://scikit-learn.org/stable/
modules/generated/sklearn.svm.SVC.html
3
https://github.com/castorini/hedwig/
blob/master/models/bert
4
https://github.com/castorini/hedwig/
blob/master/models/reg_lstm
5
https://github.com/castorini/hedwig/
blob/master/models/reg_lstm
6
https://github.com/castorini/hedwig/
blob/master/models/xml_cnn
7
https://github.com/castorini/hedwig/
blob/master/models/kim_cnn
8
https://huggingface.co/monologg/
biobert_v1.1_pubmed
9
https://huggingface.co/
bert-base-uncased
10
https://huggingface.co/
bert-large-uncased
11
https://github.com/allenai/longformer

Model

Kim CNN

XML-CNN

LSTM

LSTMReg

BERTbase

BERTlarge

BioBERT

Longformer

Hyperparameters
batch size: 32
learning rate: 0.001
dropout: 0.1
mode: static
output channel: 100
word dimension: 300
embedding dimension: 300
epoch decay: 15
weight decay: 0
batch size: 32
learning rate: 0.001
dropout: 0.7
dynamic pool length: 8
mode: static
output channel: 100
word dimension: 300
embedding dimension: 300
epoch decay: 15
weight decay: 0
hidden bottleneck dimension: 512
batch size: 8
learning rate: 0.001
dropout: 0.1
hidden dimension: 512
mode: static
output channel: 100
word dimension: 300
embedding dimension: 300
number of layers: 1
epoch decay: 15
weight decay: 0
bidirectional: true
bottleneck layer: true
weight drop: 0.1
embedding dropout: 0.2
temporal averaging: 0.0
temporal activation regularization: 0.0
activation regularization: 0.0
batch size: 8
learning rate: 0.001
dropout: 0.5
hidden dimension: 300
temporal averaging: 0.99
mode: static
output channel: 100
word dimension: 300
embedding dimension: 300
number of layers: 1
epoch decay: 15
weight decay: 0
bidirectional: true
bottleneck layer: true
weight drop: 0.1
embedding dropout: 0.2
temporal activation regularization: 0.0
activation regularization: 0.0
learning rate: 2 × 10−5
max sequence length: 512
batch size: 6
model: bert-base-uncased
warmup proportion: 0.1
gradient accumulation steps: 1
learning rate: 2 × 10−5
max sequence length: 512
batch size: 2
model: bert-large-uncased
warmup proportion: 0.1
gradient accumulation steps: 1
learning rate: 2 × 10−5
max sequence length: 512
batch size: 6
model: monologg/biobert v1.1 pubmed
warmup proportion: 0.1
gradient accumulation steps: 1
learning rate: 2 × 10−5
max sequence length: 1024
batch size: 3
model: longformer-base-4096
warmup proportion: 0.1
gradient accumulation steps: 1

Hyperparameter bounds

Number of Parameters

batch size: (16, 32, 64)
learning rate: (0.01, 0.001, 0.0001)
dropout: (0.1, 0.5, 0.7)

362,708

batch size: (32, 64)
learning rate: (0.001, 0.0001, 1 × 10−5 )
dropout: (0.1, 0.5, 0.7)
dynamic pool length: (8, 16, 32)

1,653,716

learning rate: (0.01, 0.001, 0.0001)
hidden dimension: (300, 512)

3,342,344

batch size: (8,16)
learning rate: (0.01, 0.001, 0.0001)
hidden dimension: (300, 512)
dropout: (0.5, 0.6)

1,449,608

learning rate: (0.001, 0.0001,
2 × 10−5 , 1 × 10−6 )
maximum sequence length: (256, 512)

110M

learning rate: (0.001, 0.0001,
2 × 10−5 , 1 × 10−6 )
maximum sequence length: (256, 512)

336M

learning rate: (0.001, 0.0001,
2 × 10−5 , 1 × 10−6 ))
maximum sequence length: (256, 512)

108M

learning rate: (0.001, 0.0001,
2 × 10−5 , 1 × 10−6 ))
maximum sequence length: (1024, 3584)

148M

Table A1: Hyperparameters, tuning bounds and number of parameters for each method.

B

Performance by Category
Category

Prevention
Case Report
Treatment
Diagnosis
Mechanism
Forecasting
General
Transmission

Binary F1 Score
LitCovid
CORD-19 Set
Dev
92.7 ±0.7
66.7 ±2.5
87.9 ±1.5
66.7 ±0.0
85.6 ±0.9
53.9 ±9.0
82.1 ±0.5
63.0 ±3.3
81.5 ±2.1
86.8 ±0.8
70.3 ±2.3
0.0 ±0.0
35.5 ±33.9
0.0 ±0.0
60.9 ±1.6
56.4 ±3.2

Table A2: BioBERT Binary F1 scores per category on the
LitCovid dev set and the CORD-19 test set. Scores are given
as mean ± standard deviation across three BioBERT training
runs.

C

Category Correlation

Category

Forecasting
Transmission

Full Label
Single Label
+ Prevention
Single Label
+ Prevention

Percentage of Docs
with Category
Label Prediction
34.9
8.5
62.7
88.0
16.4
16.2
45.4
49.0

Table A3: This table shows how the Longformer model predicts (Forecasting & Prevention) and (Transmission & Prevention) much more frequently than can be found in the labels.
The numbers are percentages of total number of documents
with that category label.

