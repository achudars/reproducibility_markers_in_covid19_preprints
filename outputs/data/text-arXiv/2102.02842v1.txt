The EpiBench Platform to Propel AI/ML-based Epidemic Forecasting:
A Prototype Demonstration Reaching Human Expert-level Performance
Ajitesh Srivastava, Tianjian Xu, Viktor K. Prasanna

arXiv:2102.02842v1 [cs.LG] 4 Feb 2021

University of Southern California
Los Angeles, USA {ajiteshs, frostxu, prasanna}@usc.edu

Abstract

As of writing this paper, a Google Scholar search for “covid
forecasting” yields 14000 results. AI/ML conferences are
seeing an increasing number of papers on epidemic forecasting, all of which lack a well-defined benchmark. While a
researcher may compare their approach against a traditional
modeling strategy such as SEIR (Hethcote 2000), our experience suggests that the forecasts are highly sensitive to the
implementation which requires deciding data pre-processing

and learning strategy. Consequently, “SEIR modeling” is
only a part of the forecasting process, but itself cannot be
considered a forecasting method to compare against; it only
describes the epidemic process, but does not dictate how to
pre-process the data and learn the model parameters, which
are critical and non-trivial decisions. This is analogous to
how one cannot simply compare against “deep learning” and
must declare the structure of the network and the learning algorithm. Therefore, in the current state, performing an evaluation and claiming whether an AI/ML new approach is better or when one approach performs better than the other is
not possible. There is a need to define the boundary by recognizing the state-of-the-art approaches so that new research
can be conducted and verified to push the boundary.
The existence of such benchmarks has propelled other
fields in AI/ML. For instance, datasets like ImageNet (Deng
et al. 2009) and CIFAR-100 (Krizhevsky, Hinton et al. 2009)
along with methods like ResNet (He et al. 2016) and VGG16 (Simonyan and Zisserman 2014) have created a standard
against which new approaches need to be compared for Image Classification. Since the release of ImageNet in 2013,
the accuracy has been improved from 51% to almost 90% as
seen in Figure 1. The Figure also shows an increasing interest in the task on that dataset over the years as an increasing
number of “other models”. Standard benchmarks have also
propelled research in other fields such as Speech (Panayotov
et al. 2015) and Graph Neural Networks (Hu et al. 2020).
We propose EpiBench a platform to bring the same benefits to AI/ML applied to epidemic forecasting. We are creating a platform for the community to engage in discussions
regarding identification of critical modeling and learning
decisions, a community-driven ensemble approach development, and identification of proper evaluation techniques.
The platform will be easily extensible by the community
in terms of adding new forecasting tasks and datasets. Finally, while EpiBench focuses on retrospective forecasting,
i.e., forecasting when ground truth is already available, the
approaches identified to be the state-of-the-art will benefit
real-time forecasting efforts like Epidemic Prediction Initiative (CDC 2016) by the CDC, and other agencies around the
world which drive the Government’s response. In this paper,
we make the following contributions:

Copyright © 2021, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.

• We introduce a prototype of EpiBench which is currently
running and accepting submissions for the task of fore-

During the COVID-19 pandemic, a significant effort has
gone into developing ML-driven epidemic forecasting techniques. However, benchmarks do not exist to claim if a new
AI/ML technique is better than the existing ones. The “covidforecast-hub” is a collection of more than 30 teams, including us, that submit their forecasts weekly to the CDC. It is
not possible to declare whether one method is better than
the other using those forecasts because each team’s submission may correspond to different techniques over the period
and involve human interventions as the teams are continuously changing/tuning their approach. Such forecasts may
be considered “human-expert” forecasts and do not qualify
as AI/ML approaches, although they can be used as an indicator of human expert performance. We are interested in
supporting AI/ML research in epidemic forecasting which
can lead to scalable forecasting without human intervention. Which modeling technique, learning strategy, and data
pre-processing technique work well for epidemic forecasting
is still an open problem. To help advance the state-of-theart in AI/ML applied to epidemiology, a benchmark with a
collection of performance points is needed and the current
“state-of-the-art” techniques need to be identified. We propose EpiBench a platform consisting of community-driven
benchmarks for AI/ML applied to epidemic forecasting to
standardize the challenge with uniform evaluation protocol.
In this paper, we introduce a prototype of EpiBench which is
currently running and accepting submissions for the task of
forecasting COVID-19 cases and deaths in the US states and
We demonstrate that we can utilize the prototype to develop
an ensemble relying on fully automated epidemic forecasts
(no human intervention) that reaches human-expert level ensemble currently being used by the CDC.

Introduction

Figure 1: Increasing interest in image classification over time with best performances over the years on ImageNet (Stojnic and
Taylor 2020). Benchmarks like ImageNet helped propel Computer Vision research.
casting COVID-19 cases and deaths in the US states and
counties.
• We demonstrate that we can utilize the prototype to develop an ensemble relying on fully automated epidemic
forecasts (no human intervention) that reaches humanexpert level ensemble currently being used by the CDC.
• We demonstrate how EpiBench can identify critical decisions in AI/ML-based epidemic forecasting.
We emphasize that this paper is not intended to demonstrate
a fully-fledged benchmarking platform, nor to show that a
new forecasting method is superior. Instead, the intention is
to demonstrate the need for such a platform and results from
a prototype as stated above in the contributions, that indicate
the potential of the envisioned platform.

Background
Existing Infrastructure
We first describe some recent platforms that have emerged
for submissions and evaluations of COVID-19 forecasts and
draw a contrast from our goal.
COVID-19 Forecast Hub The COVID-19 forecasting
hub (Ray et al. 2020) keeps a “live” record for forecasts of
COVID-19 in the US, created by more than 30 leading infectious disease modeling teams from around the globe, including ours, in coordination with CDC. Every week the teams
submit their latest forecasts for the number of cumulative
and/or incident cases and deaths for US states, counties, and
at the national level. The forecasts are reported at a weekly
granularity for the upcoming Sundays. This ensures that the
periodicity in reporting does not affect the evaluation. Furthermore, daily forecasts are not being used or being communicated (Ray et al. 2020).
The submitted forecasts may be considered real-time expert forecasts, although, they may not qualify as an AI/ML
methodology, although parts of forecast generation may use
AI/ML. First, the forecasts represent the submissions by a

team and not necessarily by a particular method – the teams
are allowed to change their methods. While this platform is
useful to provide the best guess of the experts in real-time
(as an epidemic is unfolding), due to varying methodologies over different periods of time, it does not provide any
insight into which methods work and when. A comparison
of a new methodology against the existing submissions will
lead to misleading conclusions. For instance, suppose a new
methodology computes its errors at different points in time
and compares against the errors of a particular team’s submission from the forecast hub (see Figure 2). From this comparison, one may falsely conclude that the new methodology is better. However, the submitted forecasts may be produced by three different methodologies over time, one of
which when used for retrospective forecasts may produce
better results. Further, many submissions are tuned by the
experts for every submission, through trial-and-error, visual
inspection, and may derive knowledge from other sources
that can evolve over time. Such methods may not classify as
an AI/ML method as they require significant human involvement and will not scale to a large number of time-series. Yet,
the submitted forecasts provide a collection of baselines that
may be used as performance points representing the realtime expert forecasts, without the claim of one method being
better than the other.
Epidemic Prediction Initiative While the COVID-19
forecasting hub has received a lot of attention recently, it
is a part of the Epidemic Prediction Initiative (EPI) by the
CDC (CDC 2016). EPI aims to improve the science and usability of epidemic forecasting by facilitating open forecasting projects and challenges. EPI provides access to some
data and defines a metric with which the forecasts are going to be evaluated. EPI has several forecasting challenges
including COVID-19, Aedes, West Nile Virus, Dengue, and
Influenza. When deciding on the state-of-the-art methodology and comparing a new approach against an existing
one, EPI suffers from the same issues as the COVID-19

Figure 2: Comparison of a new methodology against the forecast hub may be misleading as the submitted forecasts may be a
combination of various methods.
forecast hub. EpiBench will complement the efforts of EPI
and COVID-19 forecast hub-like initiatives by benchmarking methodologies instead of the comparison of teams while
utilizing similar data and expertise from the community.
The existing initiatives will continue to assist ongoing epidemics through the collective wisdom of the experts, while
EpiBench will provide a platform for “retrospective” forecasting to compare and learn from and encourage the development of new AI/ML methods.
Existing Evaluations for COVID-19 Particularly, for
COVID-19, some teams who regularly submit their forecasts to the CDC have been performing periodic evaluations
of all the submissions. For example, Youyang Gu evaluates
week-ahead predictions for cases and death for county, state,
and national level forecasts for the US1 using mean absolute error (Willmott and Matsuura 2005). The team CaltechCS1562 evaluate daily death forecasts over 7-day period
and weekly forecasts. Steve McConnell3 performs a detailed
evaluation of death forecasts of all the submitted models using a number of metrics that address the drawbacks of the
traditional evaluation metrics. A forecast hub for Germany
and Poland also evaluates submitted forecasts4 for countrylevel cases and deaths, although they seek submissions of
Admin1-level (state) as well. A detailed comparison of publicly available death forecasts for countries around the world
is tracked by Covidcompare5 . Our own evaluations are available on our webpage 6 , where we present mean absolute
errors over time for all the methods submitted to the CDC
for US state-level case and death forecasts, and for those
submitted to Poland+Germany forecast hub for country and
admin1-level case and death forecasts.
The fundamental difference between these evaluations
1
https://github.com/youyanggu/covid19-forecast-hubevaluation
2
http://cs156.caltech.edu/model/compare.html
3
https://github.com/stevemcconnell/covid19-forecastevaluations
4
https://jobrac.shinyapps.io/app evaluation/
5
http://covidcompare.io
6
https://scc-usc.github.io/ReCOVER-COVID-19/#/
leaderboard

and our goals is that they compare the performance of forecasters rather than AI/ML methods. This is because (i) not
all forecasts are generated using AI/ML approaches, (ii)
forecasts may be manually tweaked before submission by
a human-expert, and (iii) over time forecasters may keep
changing their methodology. While such evaluations are crucial for understanding where each team stands every week as
the epidemic unfolds, it provides us with no insight on what
methods work well and thus does not add to our understanding of how epidemic forecasting can be improved without
human intervention.

Acceptable Methods
EpiBench aims to enable AI/ML research in epidemic forecasting that currently has a big hurdle of lack of proper
benchmarks. It will create a platform for sustained research
in improving AI/ML methods as well as our understanding of which technique works when. Before we proceed, we
clarify what qualifies as AI/ML in this context.
We define an AI/ML approach or methodology as a
technique that is a self-contained algorithm/code that does
not rely on human intervention for generating different forecasts within the same task and has no foresight, i.e., does
not use information/data that was available after the date at
which the forecast is being generated. For instance, suppose,
the task is to forecast the number of deaths in the US states
due to COVID-19 in the next 5 weeks of every Sunday in the
period of April to October. Let M be a fixed function determined by the AI/ML methodology. Then for any date ti in
the period, with the version of historical data Di as it was
seen on ti , the generated forecasts should be M(Di ), without any additional inputs. Any change in the methodology,
including simple data pre-processing change, is considered
to lead to a different methodology. Further, M should not
have any direct dependence on the ti . This excludes explicit
programming of using one methodology for an interval and
a different one for another. Any approach that does not qualify based on the above but does not use any foresight will be
considered human expert forecasts. While the state-of-theart will be defined by AI/ML methods in EpiBench, human
expert forecasts will also be included to indicate human expert performance. Submissions in COVID-19 forecast-hub

and EPI will be considered under this category.
We focus on advancing AI/ML approaches as the immediate target for EpiBench because AI/ML approaches are more
scalable. There are more than 3000 counties in the US for
which the data is readily available. Google COVID-19 Open
Data Initiative7 provides epidemiological data for around
20,000 locations. Due to the global crisis experienced worldwide during COVID-19, we envision that more data will be
available for the future epidemics, resulting in a large number of time-series infeasible to be manually analyzed. Therefore, there is a need for AI/ML-driven fully automated approaches and a platform for proper evaluation and to set up
the community for the following innovations, which will be
enabled by EpiBench.

Results from the Prototype
We have developed an early prototype of EpiBench called
COVID-19 forecast-bench (Srivastava and Xu 2020) for
the task of COVID-19 case and death predictions in the
US states and counties. It keeps a daily record of the versions of case and death time-series as reported by JHU
CSSE COVID-19 Data (Dong, Du, and Gardner 2020). The
submitting teams are requested to provide details of their
methodologies, particularly regarding data pre-processing,
modeling technique, and learning strategy. It is clarified that
even changing a simple decision like the smoothing window qualifies as a different methodology. We have already
received 3 AI/ML methods excluding several of our own.
We have also included more than 30 methodologies (human
expert-driven) pulled from the COVID19 forecast-hub (Lab
2020). Currently, we are providing evaluations for 1-, 2, 3-, and 4-week ahead forecasts using mean absolute error (Willmott and Matsuura 2005): For each prediction ŷi
corresponding to the ground truth yi , i ∈ {1, . . . , n}, mean
absolute error (MAE) is defined as:
X |yi − ŷi |
M AE =
.
(1)
n
i
For the prototype, we define the ground truth as the positive cases and deaths timeseries reported in the latest version of JHU CSSE COVID-19 Data. It is known that this
data changes over time, due to back correction (Dong, Du,
and Gardner 2020). Therefore, the latest version of the data
is the best known approximation of the true targets (positive
cases and dates) for a prior date.

Constituent Methodologies as “Predictors” To demonstrate the utility of retrospective forecasts in ensemble development, we use variations of our SIkJα approach (Srivastava, Xu, and Prasanna 2020). This approach models the
epidemic as a discrete-time process with temporally varying
infection and death rates. The model considers many complexities such as unreported cases due to any reason (asymptomatic, mild symptoms, willingness to get tested), immunity (if any) or complete isolation, and reporting delay, and
yet, it can be reduced to a system of two linear equations
which can be fitted one after the other resulting in fast yet
reliable forecasts. The forecasts are performed by smoothing the data, learning the parameters by fitting the reduced
model, and then simulating the model into the future using
these learned parameters.
We use three variations of our SIkJα approach for the task
of predicting deaths in the US states over the period of July
to October:
• SIkJα smooth14 un10 hyper7: This indicates that the input data was set to smooth over 14 days, the underreporting factor was set to 10 (this input has little effect on short-term forecasts (Srivastava, Xu, and Prasanna
2020)), and the hyper-parameters were learned on a heldout validation data of the last 7 days.
• SIkJα smooth7 un10 hyper7: This indicates that the input data was set to smooth over 7 days.
• SIkJα window noval: This indicates that the input data
was set to smooth over 14 days and the hyper-parameters
were learned by fitting on a window of the past few days
(itself treated as a hyper-parameters) without a separate
validation set.
Each of the above has advantages and disadvantages.
SIkJα smooth14 un10 hyper7 is able to avoid noise by
smoothing over a large window, while it may over-smooth
a newly emerging pattern which is better detected by
SIkJα smooth7 un10 hyper7. Having a small validation set
as in these two approaches helps identify hyper-parameters
during changing trends. However, this may also overfit, and
during stable trends picking the hyper-parameters based on
fitting over a large window could be better. Note that our
SIkJα is fast (≈ 4s for all US states per run), and so we are
able to quickly generate different sensible variations. Next,
we will use the results of these methodologies as the constituent predictors in the ensemble.

Ensemble Development
One advantage of having retrospective forecasts with various methodologies is to be able to design intelligent ensemble techniques. On the other hand, the ensemble approach
of the forecast-hub is limited to taking the average of individual submissions (Ray et al. 2020). This is due to the
fact that the submissions by individual teams do not stick to
the same methodology and involve human decision making.
Therefore, learning a model treating the past submissions as
predictors is not useful as the nature of the predictors keeps
changing.
7

https://github.com/GoogleCloudPlatform/covid-19-open-data

Ensemble using Random Forests We wish to utilize the
individual predictors to improve the forecasts. We generate
the forecasts using each of the above on each Sunday using
the version of the data that was available on that day, for
a period of July-Oct and use them as input predictors in a
Random Forest (Meinshausen 2006). Additionally, it takes
the cumulative and incident deaths that week and the incident deaths on the previous week as inputs. The objective is
to find a regression model that can consider the outputs of
the above three approaches and the recent trend, and “readjust” them to reduce error in the future. The Random Forest was implemented using the Matlab TreeBagger function

Figure 3: Comparison of the ensemble approach built on EpiBench AI/ML-based approaches against human expert forecasts
and their ensemble.
with 100 trees8 . For each Sunday, the Random Forest was
trained using the outcomes in the last two weeks for which
ground truth is available. For 4-week ahead forecast, this
would mean training on the forecasts generated 4-5 weeks
ago by the three approaches listed above. The code to reproduce our results is available in the forecast-bench repository 9 .
Figure 3 shows the errors obtained on 4-week ahead predictions as a function of the date of releasing the forecast.
The ground truth is the latest version of the death time-series
available from JHU CSSE (Dong, Du, and Gardner 2020).
We compared our ensemble approach to the following:
• The individual constituent AI/ML approaches:
These
are
shown
in
dark
dashed
lines
and
include
SIkJα smooth14 un10 hyper7,
SIkJα smooth7 un10 hyper7, and SIkJα window noval.
• Top human-expert forecasts submitted to the CDC: These
are shown in blurred dashed lines and include submissions
from YYG Paramsearch, LNQ ensemble, Oliver Wyman
Pandemic Navigator, Steve McConnell CovidComplete,
UCLA SuEIR, and our own submission USC SIkJalpha,
all of which are available on the forecast-hub. We have
also included a version of our model that appears on our
Github repository10 and may differ slightly from our submissions to the forecast-hub.
• COVID Hub Ensemble: This is the ensemble (average) of
all the submitted forecasts shown in solid dark line. This
has been shown to perform better or on par with individual
submissions (Ray et al. 2020).
We observe that our ensemble (dark solid purple line) results in a lower error compared to the constituent AI/ML
8

https://www.mathworks.com/help/stats/treebagger.html
https://github.com/scc-usc/covid19-forecast-bench
10
https://github.com/scc-usc/ReCOVER-COVID-19
9

approaches (dark dashed lines). It has a competitive performance with the forecast-hub ensemble and outperforms it
consistently over the last few weeks. We emphasize that our
ensemble has no human intervention and the whole process
from loading the data to producing the forecasts is fully automated. This demonstrates that our ensemble can utilize
AI/ML approaches to achieve human-expert level performance. We envision that the ensemble error will further reduce through the inclusion of more AI/ML-driven forecasts.
It is to be noted that we do not claim that our ensemble approach is the best AI/ML-based epidemic forecasting approach because such a claim will require more
AI/ML methodologies to be included. This is precisely
where EpiBench will make an impact by providing a platform for new methodologies to compare against existing
ones under a uniform evaluation protocol.

Assessing Forecasting Decisions
Going from data to forecasts requires many decisions that
can influence the accuracy. These include, but not limited
to, the following:
• Data pre-processing: Whether to smooth the data over 7
days or 14 days, picking a rule to declare a point anomalous and a rule to replace anomalous data points.
• Choice of modeling: Whether to use an SEIR model, a
deep learning model, or an ARIMA model.
• Learning strategy: Whether to use Bayesian learning,
least square regression, or gradient descent.
• Hyper-parameter setting: Specific structure of the neural
network, size of the window to consider, or a forgetting
factor/weighted least square to only account for the latest
trends.
Each of the above decisions may lead to different outcomes. Despite the plethora of work in forecasting this year,

Figure 4: The choice of smoothing window can significantly
affect the error.

Figure 5: The choice of the day of forecast can significantly
affect the error.

Discussions
it is not known which of the above decisions are more critical. As an example, consider the two dark lines (green and
purple) in Figure 4 plotted against the blurred lines corresponding to the performance of submissions at the COVID19 forecast-hub. The two dark lines (green and purple) show
a significant difference in their performance, yet the two
methodologies are identical with the exception that one of
them smooths the data over 14 days and the other over 7
days. Many other approaches perform in the middle of the
two while being very different methodology-wise as they
use deep learning or Bayesian learning as opposed to our
regression approach. This may indicate that for this task, the
outcome is more dependent on data pre-processing rather
than the choice of the learning strategy. Such an observation can accelerate the computation as regression can be
performed much faster than Bayesian learning, and also, it
can direct the research in improving data pre-processing and
noise filtering which may be more critical for this task and
similar tasks of the future.
Figure 5 shows the distribution of errors obtained by a
method over different days of the week. The plot indicates
that there can be a significant difference in performance
based on when the forecasts are performed. Again, this suggests that there may be some simple decisions, often overlooked, that affect the outcome. While we have performed
these analyses only on variations of our own model SIkJα,
it is likely to be the case for other models as well. This is
supported by the fact that many of these submitted forecasts
claim to use SEIR model11 , yet they produce drastically different outcomes. This reinforces our claim that decisions beyond the choice of a model are critical in producing accurate
forecasts. EpiBench will enable us along with the research
community in identifying which decisions are critical by
presenting the evaluation of many different methodologies
and identifying them based on specific decisions they make
to arrive at their forecasts.
11

https://www.cdc.gov/coronavirus/2019-ncov/coviddata/forecasting-us.html

Evaluation Protocol
We wish to leverage EpiBench to start a discussion that will
expose the advantages and drawbacks of various metrics and
identify a set of metrics for point, interval, and probabilistic
forecasts to provide a uniform evaluation protocol. We are
planning to initially use the following well-known error metrics, categorized with respect to the type of forecast output.
Point forecasts refer to the forecasting of one number for
each ground truth of the future, such as predicted number of
new cases on a future date and declaring a future date when
the peak is expected to happen. For such forecasts we will
use mean absolute error (MAE) defined for each prediction
ŷi corresponding to the ground truth yi , i ∈ {1, . . . , n} and a
variation of mean percentage absolute error called symmetric MAPE or SMAPE (O’Connor and Lawrence 1998):
X |yi − ŷi |
M AE =
.
(2)
n
i
SP AM E =

1 X |yi − ŷi |
.
n i 0.5|yi + ŷi |

(3)

Variations of both MAE and MAPE are widely used in
time-series forecasting (Chatfield 2000). Particularly, MAE
is the evaluation preferred by the CDC for point forecasts (CDC 2016).
Probabilistic forecasts refer to the forecasting of a probability distribution for ground truth of the future, i.e., assigning a probability to each discrete possibility. For such forecasts, we will initially use a log score (Gneiting and Raftery
2007) which is used by the CDC to evaluate real-time submissions of Flu forecasts (CDC 2016).
1X
max{ln P (Ei ), −10} ,
(4)
LS =
n i
where P (Ei ) is the probability assigned to the event Ei that
is observed in the ground truth. If the assigned probability
is so low that ln P (Ei ) is less than −10 or undefined, it is

Figure 6: Drawback of Mean Absolute Error (MAE): May assign a low error to a result that intuitively is worse. (These plots
are © Copyright Steve McConnell. Additional information is available at (McConnell 2020))
replaced by −10. This ensures that one significantly poor
score does not affect the average. A higher score is preferred.
Interval forecasts refer to the reporting of a range with
a confidence interval suggesting the likelihood of the true
value falling in the range. One way to evaluate prediction intervals is “coverage” (Ray et al. 2020) that measures the percentage of time the observed value falls in the provided interval for given confidence (such as 95% confidence interval).
Other ways of evaluating interval forecasts and more generally quantile forecasts while penalizing long ranges also
exist in the literature (Gneiting and Raftery 2007).
While the above-mentioned metrics have been widely
used in epidemic forecasting and time-series forecasting,
none of them are unanimously accepted to be the best metric,
and each one suffers from drawbacks. For instance, aggregating errors using MAE may lead to assigning a low error
to a result that intuitively is worse (see Figure 6). We will
perform a detailed study of drawbacks of the initial metrics,
and engage the community in the design and selection of
future evaluation metrics.

EpiBench: Planned Functionalities
Expanding from our prototype which is in the form a GitHub
repository, We plan to create an infrastructure consisting of a
website, a GitHub organization and a Slack workspace. The
user will be able to perform the following actions using the
EpiBench platform:

• View current evaluations for submitted models and a list
of state-of-the-art based on various metrics for different
epidemic forecasting tasks on the EpiBench website.
• Upload a set of forecasts for evaluation through a Git
pull request to the appropriate repository in the collection. A link to the proper repository will be available on
the EpiBench website.
• Download code for evaluation and top among the previously submitted forecasts available on our GitHub. This
top subset determines the state-of-the-art based on a selected evaluation metric and helps the download and
repository maintenance easier by limiting the size.
• Access all the submitted forecasts through a Google Drive
link on the website.
• Participate in discussions on specific code/repository by
opening a “GitHub Issue”. Also, discussion on evaluation
metrics, new forecasting tasks will be facilitated through
Slack (sla 2020).
• Upload code for execution to reproduce results and to run
the code on more datasets.

Conclusions
We have demonstrated a prototype of EpiBench, a platform planned to provide a benchmark to compare AI/ML
methodologies for epidemic forecasting against each other
and against human-expert forecasts. We aim to define the

state-of-the-art using EpiBench which will help the AI/ML
community to push the boundary. Having various AI/ML
methods can also lead to robust and accurate ensemble forecasting. We show that using our own AI/ML approaches
(fully automated, no human intervention) in the prototype,
we are able to develop an ensemble that matches an ensemble of human expert-level performance on COVID-19 death
forecasting in the US states. Moreover, EpiBench can help
us identify which decisions (data pre-processing, modeling
choice, learning strategy, hyper-parameter tuning, etc.) in
the forecasting approach are critical in epidemic forecasting
and help direct the research accordingly. In the prototype,
we have shown that the forecasts may be highly sensitive to
data pre-processing. This relates back to the ensemble development as we were able to utilize the forecasts from different data pre-processing (smoothing) and hyper-parameter
tuning methods to obtain significant improvements in the ensemble without altering the choice of modeling or learning
strategy.
In the future, we wish to expand EpiBench to various
epidemic forecasting tasks. The platform will be available
to the AI/ML researchers and epidemiologist through (i)
the EpiBench website for viewing current evaluations of
AI/ML approaches and a list of state-of-the-art based on various metrics for different epidemic forecasting tasks; (ii) a
GitHub organization with repositories for uploading forecasts for evaluation and downloading evaluation code and
previously submitted forecasts; (iii) a Slack workspace for
community discussion on evaluation, new forecasting tasks,
and dissemination of updates.

Acknowledgments
This work was supported by National Science Foundation
Award No. 2027007. We would like to thank Youyang Gu,
Dean Karlen and UMich-RidgeTfReg team from the University of Michigan for their initial submissions to the forecastbench. We also thank Steve McConnell for useful discussions on evaluation metrics.

References
2020. Slack. https://slack.com/.
CDC. 2016. Epidemic Prediction Initiative. https://predict.
cdc.gov/.
Chatfield, C. 2000. Time-series forecasting. CRC press.
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and FeiFei, L. 2009. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, 248–255. Ieee.
Dong, E.; Du, H.; and Gardner, L. 2020. An interactive webbased dashboard to track COVID-19 in real time. The Lancet
infectious diseases 20(5): 533–534.
Gneiting, T.; and Raftery, A. E. 2007. Strictly proper scoring
rules, prediction, and estimation. Journal of the American
statistical Association 102(477): 359–378.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of the

IEEE conference on computer vision and pattern recognition, 770–778.
Hethcote, H. W. 2000. The mathematics of infectious diseases. SIAM review 42(4): 599–653.
Hu, W.; Fey, M.; Zitnik, M.; Dong, Y.; Ren, H.; Liu, B.;
Catasta, M.; and Leskovec, J. 2020. Open graph benchmark:
Datasets for machine learning on graphs. arXiv preprint
arXiv:2005.00687 .
Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple
layers of features from tiny images .
Lab, R. 2020. The COVID-19 Forecast Hub.
covid19forecasthub.org/.

https://

McConnell, S. 2020. CovidComplete Data Center. https:
//stevemcconnell.com/covidcomplete/. Accessed: 2020-1025.
Meinshausen, N. 2006. Quantile regression forests. Journal
of Machine Learning Research 7(Jun): 983–999.
O’Connor, M.; and Lawrence, M. 1998. Judgmental forecasting and the use of available information. Wright, G. and
Goodwin, P. Editors .
Panayotov, V.; Chen, G.; Povey, D.; and Khudanpur, S. 2015.
Librispeech: an asr corpus based on public domain audio
books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 5206–5210.
IEEE.
Ray, E. L.; Wattanachit, N.; Niemi, J.; Kanji, A. H.; House,
K.; Cramer, E. Y.; Bracher, J.; Zheng, A.; Yamana, T. K.;
Xiong, X.; et al. 2020. Ensemble Forecasts of Coronavirus
Disease 2019 (COVID-19) in the US. medRxiv .
Simonyan, K.; and Zisserman, A. 2014. Very deep convolutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556 .
Srivastava, A.; and Xu, T. 2020. COVID19 Forecast Bench.
https://github.com/scc-usc/covid19-forecast-bench.
Srivastava, A.; Xu, T.; and Prasanna, V. K. 2020. Fast and
Accurate Forecasting of COVID-19 Deaths Using the SIkJ
alpha Model. arXiv preprint arXiv:2007.05180 .
Stojnic, R.; and Taylor, R. 2020. Image Classification on Imagenet, Papers with code. https://paperswithcode.com/sota/
image-classification-on-imagenet. Accessed: 2020-10-25.
Willmott, C. J.; and Matsuura, K. 2005. Advantages of the
mean absolute error (MAE) over the root mean square error
(RMSE) in assessing average model performance. Climate
research 30(1): 79–82.

