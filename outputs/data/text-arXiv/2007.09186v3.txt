AWS CORD-19 Search: A Neural Search Engine for COVID-19
Literature
Parminder Bhatia, Lan Liu, Kristjan Arumae, Nima Pourdamghani, Suyog Deshpande,
Ben Snively, Mona Mona, Colby Wise, George Price, Shyam Ramaswamy, Xiaofei Ma,
Ramesh Nallapati, Zhiheng Huang, Bing Xiang, Taha Kass-Hout
Amazon Web Services AI
parmib, liuall, arumae, nimpourd, suyogd, snivelyb,
monamo, colbywi, gwprice, shyar, xiaofeim, rnallapa,
zhiheng, bxiang, tahak @amazon.com

arXiv:2007.09186v3 [cs.IR] 7 Oct 2020

Abstract
Coronavirus disease (COVID-19) has been declared as a pandemic by WHO with thousands
of cases being reported each day. Numerous
scientific articles are being published on the
disease raising the need for a service which
can organize, and query them in a reliable fashion. To support this cause we present AWS
CORD-19 Search (ACS), a public, COVID-19
specific, neural search engine that is powered
by several machine learning systems to support natural language based searches. ACS
with capabilities such as document ranking,
passage ranking, question answering and topic
classification provides a scalable solution to
COVID-19 researchers and policy makers in
their search and discovery for answers to high
priority scientific questions. We present a
quantitative evaluation and qualitative analysis
of the system against other leading COVID19 search platforms. ACS is top performing
across these systems yielding quality results
which we detail with relevant examples in this
work.

1

Introduction

With the global outbreak of Coronavirus disease
(COVID-19) (Guan et al., 2020), the world is in
turmoil. Medical researchers are required to work
quickly to fully understand and to provide a form of
intervention for the virus. Due to a large research
focus on the disease, knowledge is published at a
rapid rate throughout the world. One such repository of information is curated through the COVID19 Open Research Dataset Challenge (CORD-19)
(Wang et al., 2020). CORD-19 is a joint challenge
put forth by Allen Institute (AI2), National Institutes of Health (NIH), and the United States federal
government via the White House. The objective of
the challenge is to make sense of and extract useful
knowledge across thousands of scholarly articles
related to COVID-19. CORD-19 aims to connect

the machine learning community with biomedical
domain experts and policy makers in a race to identify effective treatments and management policies
for COVID-19. In accordance with this initiative
our goal is to provide a scalable solution aimed at
aiding COVID-19 researchers and policy makers in
their search and discovery for answers to high priority scientific questions. These questions should be
understood in their natural language form; examples include: “What do we know about COVID-19
risk factors?”, “Which medications were most beneficial in the 2002 SARS outbreak?”, and “Which is
the most referenced paper doing study for Hydroxychloroquine?” To appropriately answer these questions we require a system with a strong biomedical
understanding of the natural queries and structured
knowledge (Rotmensch et al., 2017).
AWS CORD-19 Search (ACS) provides an easy
to use search interface where researchers query using natural language 1 . ACS goes beyond keyword
matching by understanding question semantics to
efficiently find relevant answers. As illustrated in
Figure 1, we provide the system with a natural language query inquiring about IL-6 inhibitors and
showcase the system response with relevant query
components highlighted. This query can confuse a
traditional search engine that relies solely on text
matching as it can only capture term overlap between a query and a document and may not necessarily be relevant to the researcher’s true intent
(i.e. learning implicit relations). Our system, however, establishes the relationship between IL-6 and
severity of SARS-CoV-2 showing evidence where
elevated IL-6 occur in a large number of patients
with severe COVID-19 and higher mortality rates.
In the following example, if tasked with understanding the epidemiology and transmission of
COVID-19 a researcher may ask about salivary
1

https://cord19.aws/

Query

Article

Response

Topic(s)

“Are IL-6 inhibitors key to
COVID-19?”

SARS-CoV-2 and COVID19: is interleukin-6 (IL-6)
the ’culprit lesion’ of ARDS
onset?

“...monoclonal antibody against IL-6, is being tested in a
clinical trial against COVID-19 (Sarilimumab COVID19). Another drug that showed potential inhibition of IL6 related JAK/STAT pathway is glatiramer acetate which
showed potential to downregulate both IL-17 and IL-6”

Clinical Treatment

“When is the salivary viral load highest for COVID19?”

Elective, Non-urgent Procedures
and
Aesthetic
Surgery in the Wake of
SARS–COVID-19 [...]

“Patients with COVID-19 have demonstrated high viral
loads in the upper respiratory tract soon after their infection, with the highest load assumed to be the day before
symptoms appear.”

Clinical Treatment

“Is convalescent plasma therapy a precursor to vaccine?”

COVID-19
convalescent
plasma transfusion

“...a passive immunotherapy, has been used as a possible therapeutic option when no proven specific vaccine
or drug is available for emerging infections. ”

Clinical Treatment,
Immunology, Lab
Trials

Figure 1: Sample natural language queries and responses using AWS CORD-19 Search. The response field is
taken directly from the top result of this service. Also provided are the article titles where the answer is taken from
as well as selected topics for the response.

viral load. We provide a further advantage from
an organizational perspective using topics with domain specific relations. The user can select clinicaltreatments to refine search articles to understand
that “...the highest load [is] assumed to be the day
before symptoms appear.” Similarly if commissioned to understand convalescent plasma therapy
and selecting the three topics as shown in the last
example the system identifies the most relevant article and answers the question in a highlight. There
are a few search systems have been developed to
support COVID-19 search leveraging CORD-19
corpus. We evaluate the ACS against two other
search engines, and ACS is proved to be a topperforming system.
In the following sections we will discuss the
various individual Amazon Web Services (AWS)
products that allow ACS to provide this functionality, highlighting the value to scientists who can
quickly query, validate their research, and advance
their investigations.

2

System Overview

AWS CORD-19 Search (ACS) relies on a deep
semantic search model to return a ranked list of
relevant documents. This system performs document ranking, passage ranking, question answering and FAQ matching. Additionally, it leverages
knowledge graphs and topic modeling to enrich the
biomedical data and boost searching performance.
This provides a scalable solution to COVID-19 researchers and policy makers. In this section we
present the overall architecture of the system and a
closer look at several individual components. Figure 2 provides an overview of this architecture.

2.1

Amazon Kendra

Amazon Kendra2 is a semantic search and question
answering service provided by AWS for enterprise
customers. Kendra allows its customers to power
natural language based searching across their own
data. As response to the worldwide COVID-19
pandemic, Kendra has also been tooled to support
COVID-19 related searching and question answering using the document corpus from CORD-19.
The end-to-end Kendra system consists of several
components.
To complement the extracted answers, Kendra
uses a deep learning based semantic search model
to return a ranked list of relevant documents. Amazon Kendra’s ability to understand natural language
questions is at the core of its search engine returning the most relevant passage and related documents
• Document ranking (DR) - Like any traditional search engine, Kendra returns a ranked
list of relevant documents based on the user’s
query to fulfill their information needs. A
deep semantic search model is used to understand natural language questions in addition
to the keyword search.
• Passage ranking (PR) & Question Answering (QA) - Kendra ranks the passages and
tries to extract the answer from the top relevant passages with a deep reading comprehension model.
• FAQ Matching (FAQM) - If there exists Frequently Answered Questions and their corresponding answers, Kendra will automatically
2

https://aws.amazon.com/kendra/

Figure 2: System Architecture.

match a new coming query with FAQs and
extract the answer if a strong match is found.
In order to improve Kendra CORD-19 search
and make it clinically more relevant for medical
researchers, we leverage knowledge extracted using the Amazon Comprehend Medical (CM) core
NERe service as well as topics created from Amazon Comprehend Custom Classification 3 . More
explicitly, a knowledge graph is built using the
medical entities extracted with CM NERe API, and
the topics of the article are produced using a semisupervised prior based LDA approach. Both are
used to enrich the data when indexing Kendra.
2.2

Comprehend Medical

Amazon Comprehend Medical 4 (CM) (Bhatia
et al., 2019), is a HIPAA eligible AWS service for
medical domain entity recognition (Bhatia et al.,
2018), relationship extraction (Singh and Bhatia,
2019) and normalization. Comprehend Medical
supports entity types divided into five different categories (Anatomy, Medical Condition, Medication,
Protected Health Information, and Test, Treatment,
& Procedure) and four traits (Negation, Diagnosis,
Sign and Symptom). These entities are directly
used to enrich the Kendra search.
3

https://docs.aws.amazon.
com/comprehend/latest/dg/
how-document-classification.html
4
https://aws.amazon.com/comprehend/
medical/

2.3

COVID-19 Knowledge Graph

Knowledge graphs (KGs) are structural representations of relations between real-world entities in the
form of triplets containing a head entity, a tail entity,
and the relation type connecting them. KG based
information retrieval has shown great success in the
past decades (Dalton et al., 2014). The COVID-19
Knowledge Graph (CKG) (Fig 3) is a directed property graph constructed from the CORD-19 Open
Research Dataset of scholarly articles. Entities
including scholarly articles, authors, author institutions, citations, extracted topics and comprehend
medical entities are used to form relations in the
CKG. The resulting KG continues to grow as the
CORD-19 dataset increases and currently contains
over 335k entities and 3.3M relations. The CKG
powers a number of features on ACS including: article recommendations, citation-based navigation,
and search result ranking by author or institution
publication count. Scientific article recommendations are made possible by a document similarity engine that quantifies similarity between documents by combining semantic embeddings obtained from a pre-trained language model (Beltagy
et al., 2019) with document knowledge graph embeddings (Wang et al., 2017; Zheng et al., 2020)
capturing topological information from the CKG.
2.4

Topic Models

Topic modeling is a statistical discovery paradigm
for generating topics that occur in a collection of
documents. Perhaps the most widely used model

Figure 3: Visualization of COVID-19 Knowledge graph.

for topic modeling is Latent Dirichlet Allocation
(LDA) (Blei et al., 2003), a generative model which
groups documents together by observed content,
often giving each document a mixture of topics it
belongs to. An extension of this work termed Zlabel LDA (Andrzejewski and Zhu, 2009) utilizes
priors to allow the model to force certain topics
which the users have manually curated, or wish to
see clustered together.
2.4.1 Generating Topics
For the purposes of this work we experimented
with 5, 10, and 20 topic models 5 . The outputs
of each clustering size were manually inspected
and topic labels were provided by us when inspecting the top ten terms for each cluster. The final
granularity of the topic models was chosen by manually deleting and merging topics from the 20 topic
model. In general we were able to clearly extract
groups which centered around important topics including virology, proteomics, epidemiology, and
cellular biology to name a few. However when
faced with 20 topics the less populated ones tended
to be noisy, and captured peripheral information
5
These models were trained using CORD-19 data available
as of April 6th, 2020.

present in the input, such as language (e.g. Spanish
and French) or provide redundancies with existing
topics (e.g. two topics for Influenza). As a control we ran a publicly available implementation of
Z-label LDA 6 with no priors which yields topics
close to those extracted using Comprehend Topic
Modeling. Although similar we observed better
definition in certain groups (such as pulmonary
diseases, and policy/industry), and decided to use
this as the curation entry-point. Our goal was to
limit these topics to ten, and compile them in advance as much as possible. With the help of medical professionals we eliminated and combined topics to form the following: Vaccines/immunology,
Genomics, Public health Policies, Epidemiology,
Clinical Treatment, Virology, Influenza, Healthcare
Industry, Pulmonary Infections, and Lab Trials (human).
2.4.2 Multi-Label Classification
Having to manually feed a topic model and re-train
on the entire corpus once new data becomes available is largely inefficient. We therefore used the
topic model labels to train a multi-label classifier
6
http://pages.cs.wisc.edu/˜andrzeje/
research/zl_lda.html

Search Engine

P@1

P@5

P@10

P@20

R@10

R@20

ndcg@20

0.0260
0.0109
0.0236

0.0459
0.0173
0.0429

0.4380
0.1633
0.4022

0.0582
0.0356
0.0474

0.5357
0.3229
0.4133

Keyword Queries
ACS
Covidex
COVID-19 RE

0.5250
0.3421
0.5750

0.5650
0.2316
0.5650

0.5325
0.2079
0.4775

0.4775
0.1658
0.4412

Natural Language Questions
ACS
Covidex
COVID-19 RE

0.8750
0.4750
0.6000

0.7000
0.4800
0.5300

0.6400
0.4225
0.4925

0.5550
0.3625
0.4600

0.0345
0.0204
0.0267

Table 1: Evaluation results on TREC-COVID dataset

(Read et al., 2011). To evaluate the performance
of this model we calculate the average F1 across
test samples by calculating the set overlap between
our gold standard (topic model) and system labels
(multi-label classification). This held-out test set
contains 20% of the CORD-19 data available at the
time.
Using this metric our trained model achieved an
average F1 of 91.92, with on average 2.37 labels
per document. Fewer than 1% of the documents
in the test set received no label, using 0.5 as the
confidence threshold.

3

Evaluation

ACS supports document ranking (DR), passage
ranking (PR) and question answering (QA) on the
top suggested results. In this section we evaluate
the overall performance of ACS with publicly available datasets. The TREC-COVID track (Voorhees
et al., 2020) built DR test collections that contain
document relevance ground truth and are used to
assess DR only. For PR and QA, we manually annotate questions collected from CovidQA (Tang et al.,
2020), the first open sourced Covid-19 QA dataset.
The ACS results are compared against Covidex7
and Google Covid-19 Research Explorer8 (COVID19 RE), which are also COVID-19 public search
engines that facilitate DR and answer highlighting
in passage.
The TREC-COVID challenge track contains 40
topic sets along with their document relevance
judgement. The topic sets are written by its organizers with biomedical training, and motivated
by search submitted to the National Library of
Medicine and social media. Each topic consists
7

https://covidex.ai/
https://covid19-research-explorer.
appspot.com/
8

of three fields with different levels of granularity, a
keyword-based query (KQ), a more precise natural
language question (NQ), and a longer descriptive
narrative. The DR assessments are performed following the TREC pooling mechanism. The participants submitted ranked lists of documents for
each topic set, based on which a depth of 10 to 20
documents are pooled and combined as a collection
of (q, D) pairs. The pairs are then assessed by annotators with in-domain expertise. There are three
rounds of judgement available that corresponding
to three different versions of CORD-19 corpus. To
ensure sufficient coverage of annotation, we aggregate all rounds results into 33,064 (q, D) relevance
judgements. Since the document id may change
across versions, we map ids from each round to the
May 19 release of CORD-19 corpus.
To collect DR results from the three systems, we
crawled the top 50 articles by querying the engines
with KQ and its NQ variation from the topic sets on
June 15, 2020. The crawled data, which are characterized by the article title and link, are mapped
to May 19 CORD-19 corpus. Note that articles
that cannot be found in the corpus are removed to
ensure fair comparison.
We use the standard DR metrics in our evaluation, namely, the precision and recall at k (P@k,
R@k) and normalized discounted cumulative gain
in the top k documents (NDCG@k). Note that we
evaluate with k up to 20 since TREC-COVID has
a pooling depth with 20 at most. Table 1 presents
the DR performance of the engines over KQ and
NQ, respectively. ACS performs consistently better
than the other engines on NQ and mostly on KQ,
and all engines perform better on NQ comparing
with KQ.
In addition, we are able to evaluate how robust
each system is against query variation from KQ

Search Engine

Top3
EM
F1

Top10
EM
F1

Top30
EM
F1

ACS
Covidex
COVID-19 RE

11.7
0.90
10.0

16.0
2.10
13.5

26.0
3.60
18.2

35.6
18.9
31.8

42.2
24.2
36.9

50.4
27.6
43.8

Table 2: Top results variation from KQ to NQ on TREC-COVID dataset

Search Engine
ACS
Covidex
COVID-19 RE

P@1

PR
P@2

P@3

0.4074
0.4074
0.4074

0.5370
0.4444
0.3704

0.4938
0.4074
0.3580

P@1

QA
P@2

P@3

0.3333
0.1481
0.2963

0.2593
0.1852
0.2407

0.2099
0.2099
0.2593

Table 3: Passage ranking (PR) and question answering (QA) performance on CovidQA dataset

to NQ. Ideally, the results shall remain unchanged
with query variation that requests the same information. We define exact match (EM) and F1 score
among top k results to evaluate the robustness. Let
Q be the topic sets, and N Qkq , KQkq denote the
top k searching results of natural language question and keyword query corresponding to the same
topic q ∈ Q, respectively. As shown in Eq.(1), we
take the top k results KQkq as ground truth, and
compute the average exact match and F1 score of
the top k results N Qkq , and then average over all
queries. The article title string is used as comparison key, and EM and F1 are standard that the
maximum is taken over all the ground truth articles. Table 2 demonstrates that ACS has the best
capability to provide consistent results with query
variation.
EM (N Q, KQ, k)
k
1 X1X
=
em(N Qkq (i), KQkq )
|Q|
k
q∈Q

(1)

i=1

Next, we evaluate the performance of PR and QA
with CovidQA dataset. CovidQA contains 27 questions and their answers from 124 question-article
pairs, which are selected by in-domain volunteers
as the most promising COVID-19 literature. Since
the annotation does not possess sufficient answer
coverage over the entire corpus, we leverage our
internal annotation resources to make PR and QA
judgement. More explicitly, we crawled the top 3
results characterized by the article, displayed passage and the highlighted text snippet on June 15,

2020 from ACS, Covidex and COVID-19 RE, respectively. After that, the annotators assess PR
and QA in terms of whether the displayed passage
contains relevant information and whether the highlighted text snippet answers the given question. To
avoid bias, the crawled results are combined and
shuffled randomly, therefore, the annotators do not
have access to the source engine and the rank position information during the judgement.
Table 3 presents precision of top PR and QA
results. We use P@k instead of EM and F1 to evaluate QA since ACS highlights the answer while
Covidex and COVID-19 RE highlight the sentence
that contains the answer. Instead of extracting the
answer, it is more reasonable to judge whether the
highlighted text answers the question and compute
precision. ACS achieves better accuracy on both
PR and QA on most metrics. Note that ACS highlights answer snippet for at most three passages
, while Covidex and COVID-19 RE highlight all
displayed passages. This explains why ACS P@3
underperforms COVID-19 RE. Explicitly with an
example, Figure 4 displays the topmost results of
querying “What is the incubation period of virus”
from the three systems. ACS highlights the answer
in the passage. In contrast, Covidex presents an
article published at 2004 with information of virus
which is irrelevant with coronavirus, and COVID19 RE does not answer the question at all.
As evident from the above results, ACS is one
of the top-performing systems that provides high
quality informative results over CORD-19 search.

ACS

Covidex

COVID-19 RE

What is the incubation period of the virus?
Importance of Social Distancing:
Modeling the spread of 2019nCoV using Susceptible-InfectedQuarantined-Recovered-t model

Deadly viral syndrome mimics

Prediction of the virus incubation period for COVID-19 and future outbreaks

“Studies on the nature of the virus
have suggested different incubation
periods of the virus, and reports have
suggested a median incubation period of 5-6 days and a very high symptom probability period of 14 days [5]
”

“...The incubation period is typically
3 to 14 days with the symptoms of
an acute nonspecific, flulike illness
developing suddenly...The incubation period is 7 to 10 days before
the onset of symptoms [37]. This
incubation period provides the potential for worldwide exposure because a person harboring the virus
can expose the world at large via air
travel...”

“. . . while minimizing the negative
consequences of the quarantine. 70 71
The length of the incubation period
varies both across and within virus
families 4 . To our knowledge, 72
genomic features (if any) that correlate with the incubation. . . ”

Figure 4: Top-1 result of article title and displayed passage by querying ’What is the incubation period of virus?’

4

Analysis

In this section we look into a number of sample
queries to shed light on how different components
of ACS help in improving search results. We begin
by observing how small semantic differences in the
query alter the results. The first sample in Figure 5
is specific to medications. While the top result does
not include the term medication the system highlights ribavirin and corticosteroids. The CORD-19
system understands that these terms represent medications with the help of CM NERe engine. In the
second example we change medications to measures and observe the top result discussing border
control, and quarantine. This clearly demonstrates
that Amazon Kendra has a deep comprehension of
token and query meanings.
Finally, we take a look at the effects of topic modeling when grouping and filtering results. The last
two examples in Figure 5 showcase the difference
this makes in the top result. Without specifying any
topic the resulting article discusses high level policy, specifically quarantine measures in Singapore.
When we filter by clinical treatment the top result
instead focuses on infections which is covered in
the clinical setting. Furthermore the extracted text
returned to the user still focuses on lessons learned
staying true to the query.

5

Limitations and Future Directions

AWS CORD-19 is an initial step towards helping medical researchers find relevant content in
a timely and meaningful way. In order to improve

the robustness, we see following areas as direction
for future research.
Feedback Loop - Since ACS is a search engine
the motivation would be to evaluate it as such; using well-established methodologies based on test
collections—comprising topics (information needs)
and human annotations. Since no designated evaluation data exist, our initial focus is to capture different interactions and feedback. Currently, ACS
lacks the feedback loop and federated learning approaches where the system would continuously
learn and improve the search. However, the system
captures feedback from the researchers in the form
of implicit and explicit reactions. Implicit feedback evaluation consists of topics of interests, their
clicks as well as the ranking of the results which
were selected by medical researchers. Explicit feedback evaluation is captured by providing up-down
rating associated with each search results. In the
future results can be personalized based on this
feedback. Now that we have a system in place, our
efforts have shifted to broader engagement with potential stakeholders to solicit additional guidance,
while trying to balance between the features and
ranking.
Q&A Curation - Curation and normalization of
questions have potential a use-case of presenting
trending questions asked by the medical research
community at a particular point. However, curation
would involve capturing the questions asked as well
as identifying similar questions that can be later
normalized. Currently, there is no mechanism to

Query

Article

Response

“What medications were most beneficial in the SARS outbreak?”

Development of chemical inhibitors
of the SARS coronavirus: Viral helicase as a potential target

“...spread of SARS, a number of broad-spectrum antiviral
medications were empirically administered to the SARS
patients during the SARS outbreak in 2003. These medications include ribavirin, HIV protease inhibitors, corticosteroids, and alpha-interferon (IFN-a). In a retrospective review of treatment...”

“What measures were most beneficial
in the SARS outbreak?”

Impact of quarantine on the 2003
SARS outbreak: A retrospective modeling study

“During the 2003 Severe Acute Respiratory Syndrome
(SARS) outbreak, traditional intervention measures
such as quarantine and border control were found to
be useful in containing the outbreak...”

“What did we learn from the SARS
outbreak?” (no topic)

Use of quarantine in the control of
SARS in Singapore

“The main lesson to learn from the SARS outbreak is the
capability of an emerging infection to cause a pandemic
in a short span of time and the paradigm shift needed to
respond to such a disease.”

“What did we learn from the SARS
outbreak?” (clinical-treatment)

Characteristics of COVID-19 infection in Beijing

“We compared the epidemic features between COVID-19
and 2003 SARS for learn lessons and control the outbreak.”

Figure 5: Sample queries demonstrating the semantic understanding of ACS, the use-fullness of Comprehend
Medical NERe, and the utility of topic modeling for filtering.

curate the questions asked by the researchers.

Acknowledgments

Summarization - Currently, ACS outputs the
relevant passage based on the query. It would be
beneficial to get the overall summary of the paper. A potential future direction would be to generates summaries (Raffel et al., 2019) from paper
abstracts and full body.

We acknowledge the broader collaboration with
AI2 team and White house as well as the broader
AWS CORD-19 team including Kendra Science
team, Tyler Stepke, Kingston Bosco, Victor Wang,
Vaibhav Chaddha, Miguel Calvo, Ninad Kulkani,
Kevin Longofer, Ray Chang, Adrian Bordone,
Tony Nguyen, and Kyle Johnson.

6

Conclusion

This paper describes our efforts in building AWS
CORD-19 Search with its capabilities consisting
of topic based, knowledge graph, and natural language search queries. This is further enhanced
with reading comprehension and FAQ matching
as well as document ranking providing a scalable
solution to COVID-19 researchers and policy makers in their search and discovery for answers to
high priority scientific questions. Our solution is
powered by Amazon Kendra, Comprehend Medical and Neptune which incorporate the latest neural
architectures to provide information access capabilities to the CORD-19 challenge. By comparing
with other public search engines that support similar functionalities over COVID-19 search, ACS
is demonstrated to be powerful on its document
ranking and question answering components. We
hope that our solution can prove useful in the fight
against this global pandemic, and that the capabilities we have developed can be applied to analyzing
the scientific literature more broadly.

References
David Andrzejewski and Xiaojin Zhu. 2009. Latent
dirichlet allocation with topic-in-set knowledge. In
Proceedings of the NAACL HLT 2009 Workshop
on Semi-Supervised Learning for Natural Language
Processing, pages 43–48. Association for Computational Linguistics.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: Pretrained language model for scientific text. In
EMNLP.
P. Bhatia, B. Celikkaya, M. Khalilia, and S. Senthivel.
2019. Comprehend medical: A named entity recognition and relationship extraction web service. In
2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), pages
1844–1851.
Parminder Bhatia, Busra Celikkaya, and Mohammed
Khalilia. 2018. Joint entity extraction and assertion detection for clinical text. arXiv preprint
arXiv:1812.05270.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993–1022.

Jeffrey Dalton, Laura Dietz, and James Allan. 2014.
Entity query feature expansion using knowledge
base links. In Proceedings of the 37th international
ACM SIGIR conference on Research & development
in information retrieval, pages 365–374.
Wei-jie Guan, Zheng-yi Ni, Yu Hu, Wen-hua Liang,
Chun-quan Ou, Jian-xing He, Lei Liu, Hong Shan,
Chun-liang Lei, David S.C. Hui, Bin Du, Lan-juan
Li, Guang Zeng, Kwok-Yung Yuen, Ru-chong Chen,
Chun-li Tang, Tao Wang, Ping-yan Chen, Jie Xiang,
Shi-yue Li, Jin-lin Wang, Zi-jing Liang, Yi-xiang
Peng, Li Wei, Yong Liu, Ya-hua Hu, Peng Peng,
Jian-ming Wang, Ji-yang Liu, Zhong Chen, Gang
Li, Zhi-jian Zheng, Shao-qin Qiu, Jie Luo, Changjiang Ye, Shao-yong Zhu, and Nan-shan Zhong.
2020. Clinical characteristics of coronavirus disease
2019 in china. New England Journal of Medicine,
382(18):1708–1720.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.
Jesse Read, Bernhard Pfahringer, Geoff Holmes, and
Eibe Frank. 2011. Classifier chains for multi-label
classification. Machine learning, 85(3):333.
Maya Rotmensch, Yoni Halpern, Abdulhakim Tlimat,
Steven Horng, and David Sontag. 2017. Learning
a health knowledge graph from electronic medical
records. Scientific reports, 7(1):1–11.
Gaurav Singh and Parminder Bhatia. 2019. Relation
extraction using explicit context conditioning. arXiv
preprint arXiv:1902.09271.
Raphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil
Gupta, Phuong Cam, Kyunghyun Cho, and Jimmy
Lin. 2020. Rapidly Bootstrapping a Question Answering Dataset for COVID-19. arXiv preprint
arXiv:2004.11339.
Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina
Demner-Fushman, William R Hersh, Kyle Lo, Kirk
Roberts, Ian Soborof, and Lucy Lu Wang. 2020.
TREC-COVID: Constructing a Pandemic Information Retrieval Test Collection.
arXiv preprint
arXiv:2005.04474.
Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,
Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn
Funk, Rodney Kinney, Ziyang Liu, William Merrill,
et al. 2020. Cord-19: The covid-19 open research
dataset. arXiv preprint arXiv:2004.10706.
Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.
2017. Knowledge graph embedding: A survey of
approaches and applications. IEEE Transactions
on Knowledge and Data Engineering, 29(12):2724–
2743.

Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang, and
George Karypis. 2020. Dgl-ke: Training knowledge graph embeddings at scale. arXiv preprint
arXiv:2004.08532.

