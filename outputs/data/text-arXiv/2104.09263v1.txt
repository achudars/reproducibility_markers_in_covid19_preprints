arXiv:2104.09263v1 [eess.SP] 19 Apr 2021

Fitbeat: COVID-19 Estimation based on Wristband
Heart Rate
Shuo Liua,∗, Jing Hana,b , Estela Laporta Puyalc,d , Spyridon Kontaxisc,d ,
Shaoxiong Sune , Patrick Locatellif , Judith Dineleya , Florian B. Pokornya,g ,
Gloria Dalla Costah , Letizia Leocanih , Ana Isabel Guerreroi , Carlos Nosi , Ana
Zabalzai , Per Soelberg Sørensenj , Mathias Buronj , Melinda Magyarij ,
Yatharth Ranjane , Zulqarnain Rashide , Pauline Condee , Callum Stewarte ,
Amos A Folarine,k , Richard JB Dobsone,k , Raquel Bailónc,d , Srinivasan
Vairavanl , Nicholas Cumminsa,e , Vaibhav A Narayanl , Matthew Hotopfm,n ,
Giancarlo Comio , Björn Schullera,p , RADAR-CNS Consortiumq
a EIHW

– Chair of Embedded Intelligence for Health Care and Wellbeing, University of
Augsburg, Augsburg, Germany
b Department of Computer Science and Technology, University of Cambridge, Cambridge,
United Kingdom
c BSICoS Group, Aragón Institute of Engineering Research (I3A), IIS Aragón, University of
Zaragoza, Zaragoza, Spain
d CIBER of Bioengineering, Biomaterials and Nanomedicine (CIBER-BNN), Madrid, Spain
e The Department of Biostatistics and Health informatics, Institute of Psychiatry,
Psychology and Neuroscience, King’s College London, London, UK
f Department of Engineering and Applied Science, University of Bergamo, Bergamo, Italy
g Division of Phoniatrics, Medical University of Graz, Graz, Austria
h Neurorehabilitation Unit and Institute of Experimental Neurology, University Vita Salute
San Raffaele, Istituto Di Ricovero e Cura a Carattere Scientifico Ospedale San Raffaele,
Milan, Italy
i Multiple Sclerosis Centre of Catalonia (Cemcat), Department of
NeurologyNeuroimmunology, Hospital Universitari Vall dH́ebron, Universitat Autònoma de
Barcelona, Barcelona, Spain
j Danish Multiple Sclerosis Centre, Department of Neurology, Copenhagen University
Hospital Rigshospitalet, Copenhagen, Denmark
k Institute of Health Informatics, University College London, London, United Kingdom
l Janssen Research and Development LLC, Titusville, NJ, USA
m The Department of Psychological Medicine, Institute of Psychiatry, Psychology and
Neuroscience, King’s College London, London, United Kingdom
n South London and Maudsley National Health Service Foundation Trust, London, United
Kingdom
o Institute of Experimental Neurology, Istituto Di Ricovero e Cura a Carattere Scientifico
Ospedale San Raffaele, Milan, Italy
p GLAM – Group on Language, Audio, & Music, Imperial College London, London, United
Kingdom
q The RADAR-CNS Consortium, London, United Kingdom. www.radar-cns.org

Abstract
∗ Corresponding

author
Email address: shuo.liu@informatik.uni-augsburg.com (Shuo Liu)

Preprint submitted to Elsevier

April 20, 2021

This study investigates the potential of deep learning methods to identify individuals with suspected COVID-19 infection using remotely collected heart-rate
data. The study utilises data from the ongoing EU IMI RADAR-CNS research
project that is investigating the feasibility of wearable devices and smartphones
to monitor individuals with multiple sclerosis (MS), depression or epilepsy. As
part of the project protocol, heart-rate data was collected from participants using a Fitbit wristband. The presence of COVID-19 in the cohort in this work
was either confirmed through a positive swab test, or inferred through the selfreporting of a combination of symptoms including fever, respiratory symptoms,
loss of smell or taste, tiredness and gastrointestinal symptoms. Experimental
results indicate that our proposed contrastive convolutional auto-encoder (contrastive CAE), i. e., a combined architecture of an auto-encoder and contrastive
loss, outperforms a conventional convolutional neural network (CNN), as well
as a convolutional auto-encoder (CAE) without using contrastive loss. Our final
contrastive CAE achieves 95.3% unweighted average recall, 86.4% precision, an
F1 measure of 88.2%, a sensitivity of 100% and a specificity of 90.6% on a test
set of 19 participants with MS who reported symptoms of COVID-19. Each of
these participants was paired with a participant with MS with no COVID-19
symptoms.
Keywords: COVID-19, respiratory tract infection, anomaly detection,
contrastive learning, convolutional auto-encoder, deep learning

1. Introduction
Remote, passive monitoring of physiological and behavioural characteristics
using smartphones and wearable devices can be used to rapidly collect a variety
of data in huge volumes. Such data has the potential to improve our understanding of the interplay between a variety of health conditions at both the
level of the individual and the population, if rigorously collected and validated
[1].
Passive data collection is typically implemented with a high temporal reso-

2

lution [2]. Wearable fitness trackers, for example, estimate parameters such as
heart rate up to every second, and can be worn 24 hours a day. Monitoring
individuals with a range of health states, lifestyles, and demographic variables
in combination with data artefacts and missing data leads to high variability.
From heart rate and general physical activity to GPS-based location information or the proximity to other individuals detected through Bluetooth, multiple
types of data can be monitored remotely. Therefore, studies using wearables
and smartphones in this way exhibit several ‘v’s of big data, namely velocity,
volume, variability and variety. As such, advanced analysis methodology such
as deep learning have the potential to make a significant contribution [3].
The volume and richness of remotely collected data has particular potential
in the context of infectious diseases, such as the novel corona virus (SARSCoV-2). Specific applications include individual screening and population-level
monitoring while minimising contact with infected individuals [4, 5, 6].
Among all remotely collectable physiological data, heart rate is a biomarker
of particular value in such applications. Patterns in heart rate fluctuations
over time in particular have been found to provide clinically relevant information about the integrity of the physiological system generating these dynamics.
Previous studies have not only revealed an altered heart rate variability in a
number of medical conditions, such as hypovolaemia [7], heart failure [8], or
angina [9], but also demonstrated that the degree of short-term heart rate alteration correlates with illness severity. Analysis of the autonomic regulation of
heart rate, such as heart rate variability, has also been discussed as a promising
approach for detecting infections earlier than conventional clinical methods and
prognosticating infections [10].
Wearables such as Fitbit fitness trackers1 can provide indirect measurements
of the heart rate through pulse rate estimates made using photoplethysmography. Radin and colleagues [4] analysed the resting heart rate collected in this
way, alongside with sleep duration data collected from over 47 000 individuals
1 https://www.fitbit.com/

[as of 12 November 2020]

3

to improve model predictions of influenza rates in five US states. In the ongoing
DETECT2 study [5], the same group is now applying this approach to monitor outbreaks of viral infections including COVID-19. In separate work, Fitbit
devices have also been used to identify individuals with COVID-19 using heart
rate measurements collected before symptom onset [11]. In addition to pre-print
studies [12, 13], other similar ongoing endeavours include the German project
Corona-Datenspende 3 , which has a cohort of over 500 000 volunteers, and the
TemPredict study in the US4 .
Applied to such data sets, deep learning methodology has the potential to
automatically identify individuals with COVID-19 purely on the basis of passive
data from wearable devices [5, 13, 11]. Natarajan and colleagues [13] used a
convolutional neural network (CNN) to predict illness on a given day using
Fitbit data from 1 181 individuals, reporting an area under the receiver operating
characteristics curve (AUC) of 0.77 ± 0.03. Similarly, this paper describes a
CNN-based deep learning approach for predicting the presence of COVID-like
symptoms using Fitbit heart rate data. Treating this task as analogous to
anomaly detection, we explore the suitability of using a convolutional autoencoder (CAE) with contrastive loss [14, 15]. The technique’s performance
is compared against a conventional auto-encoder and a convolutional neural
network.

2. Data Collection
The data used in this work was collected as part of the IMI2 RADAR-CNS
major programme5 . RADAR-CNS is an observational, longitudinal, prospective study aimed at developing new ways of measuring major depressive disorder, epilepsy, and multiple sclerosis (MS) using remote monitoring technologies
2 http://detectstudy.org/

[as of 12 November 2020]
[as of 12 Nov 2020]
4 http://osher.ucsf.edu/research/current-research-studies/tempredict [as of 12 November
3 http://corona-datenspende.de/science/en/

2020]
5 https://www.radar-cns.org/

4

(RMT), i. e., wearable devices and smartphone technology. The study is currently being conducted at multiple clinical sites spread across several European
countries. The data used in this work was collected in the MS arm of the project
with participants recruited from three sites: Ospedale San Raffaele (OSR) in
Milan, Italy, Vall d’Hebron Institut de Recerca (VHIR) in Barcelona, Spain, and
the University Hospital Copenhagen, Regshospitalet (RegionH) in Copenhagen,
Denmark. The MS work package started recruiting participants in June 2018.
As of March 1, 2020, 499 participants had been enrolled in the RADAR-MS
study and 403 (81 %) remained in the study. For further information on the MS
study and the inclusion criteria and assessment schemes the interested reader is
referred to [16].
RADAR-CNS collects both passive and active participant data. Passive
data is collected on a continuous 24/7 basis through a smartphone and wearable device in a manner that requires minimal participant effort. It includes
GPS-derived location, Bluetooth, phone usage data, activity, sleep, and heart
rate. Participants use their own Android smartphones where available, or were
provided with a Motorola G5, G6, or G7 if required. Fitbit Charge 2 or Charge
3 devices were provided to participants, who were asked to wear the device
on their non-dominant hand. The collection of active data requires direct input from the participant and includes surveys, questionnaires, and short tasks
such as speech recordings. Data collection and management were handled by
RADAR-BASE6 , a purpose built, open-source mobile Health (m-Health) technology platform [2]. To assess the impact of COVID-19 on the MS study, a
specially designed active questionnaire was distributed, via RADAR-base to all
active participants first on on March 25, 2020, and then again on April 8, 2020.
By April 15 2020, at least one of the questionnaires had been completed by
399 participants (99 %). At the time of these questionnaires, many confirmed
COVID-19 cases did not fulfil the criteria as defined by the World Health Organization (WHO) [17]. The RADAR-CNS MS workpackage used two alternative
6 https://radar-base.org/

5

definitions to determine the prevalence of COVID-19 in their participants [16].
The first case definition, herein referred to as CD1, was participants experiencing
fever or anosmia/ageusia in combination with any other COVID-19 symptoms
including respiratory symptoms, tiredness and gastrointestinal symptoms , or
respiratory symptoms plus two other COVID-19 symptoms, with the second case
definition, CD2, being participants experiencing fever + any other COVID-19
symptoms, or respiratory symptoms + anosmia/ageusia. Laboratory-confirmed
cases were included in both case definitions [16].
Table 1: Gender-, age-, and site-related distribution of participants per data subset

Positive participants Health Control

Genders

Pre-training

for testing

for testing

Female

14

5

5

Male

35

14

14

Italy

18

7

7

19

6

6

Denmark

12

6

6

≤ 30

1

2

2

30 - 39

10

3

4

40 - 49

12

6

5

50 - 59

19

6

6

60 - 69

6

1

1

≥ 70

1

−

−

Locations Spain

Ages

For this study, we considered Fitbit heart rate measurements, collected 24hours-a-day/7-days-a-week between 21 February and 20 May 2020, from 87
participants in Denmark, Italy and Spain, with an age range from 23 to 73
years (mean = 46.5 ± 10.5 standard deviation). Therein, 68 MS participants
(30 female, 38 male) reported to have symptoms characteristic of COVID-19.
However, in 49, symptoms did not meet CD1 or CD2 criteria. The heart rate
data of these 49 participants were used for model pre-training (pre-training set).
For testing, we applied leave one subject out (LOSO) cross-validation (CV) [18]

6

on the data of the 19 MS participants, whose symptoms were in line with CD1 or
CD2. To further increase robustness of our models, each of these 19 symptomatic
participants was paired with a COVID-like symptom-free control participant
with MS matched for site and gender and being at a similar age (cross-validation
set). Thus, in each of the 19 cross-validation rounds, the pre-trained models were
further trained on the data of 18 MS participants with COVID-like symptoms
and 18 MS participants without COVID-like symptoms, and subsequently tested
on the data of the left-out MS participants pair. Table 1 reveals the numbers of
participants per data subset as a function of the independent variables gender,
age, and recording site.
Heart rate data of the participants were assigned into temporal segments,
defining a 14-day interval extending from 7 days preceding symptom onset to 7
days following symptom onset in which we sought to identify infection-related
variations in heart rate. The interval mainly cover the duration of the COVID-19
incubation period [19, 20, 21], and minimises the anomalous effects of day-to-day
variations in activity, such as those observed between weekdays and weekends.
Fig. 1 demonstrates the segmentation and subsequent data pre-processing
procedure for the heart rate data of a participant with reported COVID-like
symptoms. A heart rate segment over 14 days centred at 00:00 at the day of
reported symptom onset, i. e., 7 consecutive days before the day of reported
symptom onset plus 7 consecutive days starting with the day of reported symptom onset (red box on top of Fig. 1) is referred to as symptomatic segment. In
contrast, an asymptomatic segment stands for any 14-days interval of consecutive heart rate data again starting at 0 o’clock that is at least 7 days distant
from a symptomatic segment (green box in top of Fig. 1).
Thus, asymptomatic segments were created by shifting a 14-days window
in full day steps over periods at least 7 days distant from the boundaries of a
symptomatic segment. With the chosen 7-days distance of asymptomatic segments from symptomatic segments we presumed, that (i) a participant might
not have already been infected 14 or more days prior to the onset of symptoms,
and (ii) participants might have recovered from illness 14 days after the onset of
7

Table 2: Available symptomatic and asymptomatic segments per data subset. Data completeness [%] in terms of each heart rate segments is given in parentheses (mean + std).

Positive Participants Health Control
# (%)

Pre-training

for testing

for testing

Symptomatic

49 (98.7 ± 0.3)

19 (97.6 ± 0.2)

−

Asymptomatic 1470 (98.1 ± 0.4)

570 (97.4 ± 0.2)

1140 (99.2 ± 0.5)

symptoms at the latest. From the 49 participants of the pre-training set, totally
49 symptomatic segments and 1 470 asymptomatic segments were extracted.
Since the number of available symptomatic and asymptomatic segments was
highly imbalanced, we replicated the symptomatic segments to the number of
asymptomatic segments to guide the detection model weighted in favour of the
minority class. For the LOSO CV procedure, 19 symptomatic and 570 asymptomatic segments were acquired from participants with reported symptoms, and
1 140 asymptomatic segments from the control participants also referred to as
control segments. An overview of available symptomatic and asymptomatic segments is given in Table 2.
Ideally, the instantaneous heart rate measurement for an individual was
captured every second, and was uploaded every five seconds along with the
recording time (blue curve in middle of Fig. 1). The mean of 5 minutes is taken
to smooth heart rate measurements while still tracking slow short-term changes
in the heart rate. Moreover, this approach alleviates the effect of different
sampling rates observed in actual heart rate recordings.
Missing data over full 5-minutes intervals is filled with the median value
of the overall 14-days segment. Finally, we have a single heart rate value per
every 5 minutes. The resulting smoothed heart rate trajectory is considered
appropriate to detect global heart rate patterns associated with COVID-like
symptoms (red curve in middle of Fig. 1).

8

9

hours, the 168 columns sum up to 14 days.

above illustrated symptomatic segment as 24 × 168 sized image of 5-minutes heart rate data related pixels. Each column represents an interval of 2

the red rectangle above, while the red curve indicates the heart rate trajectory averaged over 5-minutes intervals. Bottom: Representation of the

7 days distant from the symptomatic segment. Middle: Symptomatic segment. The blue curve represents the unprocessed heart rate trajectory of

the green rectangle gives an example of an asymptomatic segment, i. e., a segment in which no symptoms were present and which is located at least

symptom onset date. The red rectangle covers 7 days heart rate data before and after symptom onset and represents a symptomatic segment, while

24-hours-a-day/7-days-a-week from 21 February to 20 May 2020 (total 90 days). Onset (black vertical bar) indicates 0 o’clock at the reported

Figure 1: Segmentation and pre-processing of heart rate data of a participant with reported COVID-like symptoms. Top: Heart rate data recorded

We then transform the averaged heart rate data of each segment into a
feature map, i. e., an image of size 24 × 168 pixels (bottom of Fig. 1), in which
each pixel represents a 5-minutes heart rate sampling point. Thereby, each
column encodes a heart rate trajectory of 2 hours (24 × 5 minutes), resulting
in a covered interval of overall 14 days by 168 columns (168 × 2 hours). In our
experiments, we verified that this set-up of the feature map is effective as the
input of our deep learning models, leading to promising detection results.

3. Methodology
For detecting the presence of COVID-like symptoms in a given 14 days heart
rate segment, we investigate several different deep learning methods to represent
the problem by feature maps, including CNNs [22, 23] and a conventional CAE
[24, 25, 26, 27]. Further, we formulise the task as anomaly detection, and
propose a strategy for training a CAE, by means of fitting the reconstruction
error into the format of contrastive loss instead of conventional loss like root
mean square error (RMSE).
A CNN typically consists of a sequence of convolutional layers, with every convolutional layer containing its own learnable parameters, i. e., the filter
weights and biases, to extract features from its input feature map. The stacking of convolutional layers allows a hierarchical decomposition of the input, a
deeper CNN is expected to develop more complex representations of the the
feature maps [28, 29, 30]. Subsequently, fully-connected layers map the learnt
representations to the desired classes. Such CNN architectures have been successfully applied in many tasks, such as image and video recognition [31, 32],
sequential data processing [33, 34], and medical applications [35, 36, 37, 38]
including those recent works for Covid-19 diagnosis [39, 40, 41, 42].
The alternative approach to learn representations from an input image is
to use a CAE [43, 26, 27]. An auto-encoder contains two parts, i. e., an encoder and a decoder. The encoder learns latent attributes of the original input,
whereas the decoder aims at reconstructing the original input from the learnt

10

latent attribute. The dimensionality of the latent attributes is designed as a
bottleneck imposed in the architecture; it hence can be seen as a compressed
knowledge representation of the input. In order to guarantee the reproduction
of the original input at the output of the decoder, the reconstruction error is
minimised when optimising an auto-encoder network. In this work, we keep the
architecture of our CAE encoder as consistent with our CNNs, and the decoder
is arranged in a mirror-like fashion. Since the conventional CAE is an unsupervised learning approach, an additional multi-layer perception (MLP) [44] follows
to classify the learnt attributes. This strategy has been widely used for medical
and health related tasks [45, 46, 47], including tasks similar to ours, such as
anomaly detection [48, 49, 50, 51].
As the strategy for optimising a conventional CAE does not take class information into account, the learnt latent attributes may contain redundant representations that are of no sufficient importance for the final classification. To
incorporate the class information during training of the CAE, we apply contrastive loss [52] to the CAE reconstruction error in order to guide it to learn
sufficiently discriminative latent attributes for different classes. The three considered neural network architectures, i. e., CNN, conventional CAE, contrastive
CAE, used in this work are described in details in the following separate sections.
3.1. Convolutional Neural Network (CNN)
Our CNN architecture is comprised of a sequence of convolutional layers, an
example of 4 layers is illustrated in Fig. 2. Each convolutional layer contains
its own kernel size, stride, and number of channels as given in the specifications
that appear in Table 3. For one layer convolution, kernel size and stride together determine the receptive field, i. e., the perception scope on the original
input. Zero-padding is typically utilised to maintain a proper size of the feature
map. More kernel filters result in better representation diversity. In between
convoutional layers, batch normalisation can be used reduce the effect of internal covariate shift (ICS), an effect caused by different batches of training data
having slightly different distributions [53, 54]. Hence, the vanishing gradient
11

0
10
6
25

×
3

1

2

7
1×

21

6

×

256
conv4

1

fc2

fc1

avg.pool

conv3

64

12

12

32

6

×

42

42

128
128

1

×

×

84

84

64

3×

21

256

32

24

×

16

8

conv2

conv1

Figure 2: Convolutional neural network (CNN) architecture exemplified with 4 layers. Each
layer involves a sequence of convolution (conv) – batch normalisation – parametric
rectified linear unit (PReLU) activation – max pooling. The output feature maps are
averaged across all locations (avg. pool), and subsequently passed through two fully-connected
(fc) layers to finally discriminate between two classes.

issue can be alleviated. A parametric rectified linear unit (PReLU) [55] performs as the activation function for each layer. It carries on the merits of ReLU
activation [56] in stable training while avoiding dead neurons using a learnable
slope parameter.
A max-pooling layer processes the activations to reduce the spatial size of
the feature size, leading to less parameters in the following network layers and,
therefore, alleviating the potential over-fitting issues. The CNN is therefore a
sequential cascade of convolutional layer – batch normalisation – PReLU. Its
output is averaged across all locations, resulting in a representation vector. The
representation vector is then projected onto the two classes, symptomatic or
asymptomatic, through two fully-connected layers.
In our experiments, we tested CNNs with different number of layers, ranging
from 1 to 6, in order to optimise the performance.

12

Table 3: Specifications of the single convolutional (conv), pooling (pool), and fully-connected
(fc) layers of the implemented convolutional neural network (CNN) model in terms of kernel
size, stride size, padding size, and number of (#) channels. avg. = average; * = dimensionality
equals to the channel of the last convolutional layer.

Block Kernel Stride Padding # Channels
conv1

(5, 5)

(1, 1)

(2, 2)

32

pool1

(2, 2)

(2, 2)

32

conv2

(5, 5)

(1, 1)

−

(2, 2)

64

pool2

(2, 2)

(2, 2)

64

conv3

(5, 5)

(1, 1)

−

(2, 2)

128

pool3

(2, 2)

(2, 2)

128

conv4

(5, 5)

(1, 1)

−

(2, 2)

256

pool4

(3, 3)

(3, 3)

256

conv5

(3, 3)

(1, 1)

−

(1, 1)

512

conv6

(3, 3)

(1, 1)

(1, 1)

1024

# Hidden units
∗

avg. pool
fc1

100

fc2

2

3.2. Conventional Convolutional Auto-encoder (CAE)
The encoder part of our CAE keeps the identical structure as the CNN
introduced in Section 3.1, but contains only one FC layer before mapping into
latent attributes. Given the feature map of a heart rate segment xi , in which
i indicates segment index, the encoder f enc (·) processes the feature map and
produces the latent attributes
h = f enc (xi ).

(1)

The decoder presents an inverse processing of the encoder as seen in 3. For
each decoder layer, the input feature map mainly passes through transposed
convolution and transposed max-pooling, also known as de-convolution and depooling. Batch normalisation is employed in between, followed by PReLU as

13

42

21

128
⇥

92
17

1

3⇥

0
10
92
17

256

deconv4

128

64

6

7
1⇥

21
⇥
3

1

Rec.
Error

256

6

⇥

42

42

reshape

deconv3
84

flatten

⇥

64

32
8

deconv2

⇥

16

⇥

8

16

64

conv2

32

24

32

24

conv3

12

⇥
6

conv4

1
latent
attribute

12

12

32

128

256

⇥

⇥

84

84

64

3⇥

21

256
128

deconv1

conv1

Figure 3: The convolutional auto-encoder (CAE) architecture with 4 encoder layers and 4 decoder layers as an example. Each encoder layer shares the same architecture as for the CNN
in Fig. 2 and the output feature maps are flattened and projected onto the latent attribute.
In the decoder, the latent attribute is mapped back to the previous dimensionality. A decoder
layer is a sequence of transposed convolution – batch-normalisation – PReLU – transposed max-pooling. The output of decoder is seen as reconstructed image of the original
input, and hence, the distance between the original and reconstructed image represents the
reconstruction error.

the activation function. The decoding procedure can be represented as
x̂i = f dec (h),

(2)

where f dec (·) represent the operations in the decoder. Its output x̂i , seen as
the reconstructed feature map, is then compared to the original input to compute the reconstruction error. An auto-encoder is optimised by minimising the
reconstruction error, one typical such error is root mean squared error (RMSE)
[57]:
v
u
N
u1 X
RMSE = t
|xi − x̂i |2 ,
N i

(3)

where N stands for the number of samples.
The specifications of our CAE are given in Table 4. In experiments, we considered different number of convolutional layers in the CAE; the dimensionality
of the flatten layer is determined by the last encoder layer. Hence, for different
numbers of layers, we adjust the length of the fully-connected layer to optimise
the CAE performance.

14

Table 4: The specifications of our CAE models. Each convolution and pooling layer, as well
as de-convolution and de-pooling layer contains its own kernel size, stride, padding size, and
number of channels. *=dimensionality depends on the total layers, **= dimensionality of
latent attributes.

Blocks Kernel Stride Padding # Channels
conv1
(5, 5)
(1, 1)
(2, 2)
32
pool1
(2, 2)
(2, 2)
−
32
conv2
(5, 5)
(1, 1)
(2, 2)
64
pool2
(2, 2)
(2, 2)
−
64
CNN
conv3
(5, 5)
(1, 1)
(2, 2)
128
Encoder pool3
(2, 2)
(2, 2)
−
128
conv4
(5, 5)
(1, 1)
(2, 2)
256
pool4
(3, 3)
(3, 3)
−
256
conv5
(3, 3)
(1, 1)
(1, 1)
512
conv6
(3, 3)
(1, 1)
(1, 1)
1024
flatten
∗
fc
∗∗
fc
∗∗
deconv6 (3, 3)
(1, 1)
(1, 1)
512
deconv5 (3, 3)
(1, 1)
(1, 1)
256
deconv4 (3, 3)
(1, 1)
(1, 1)
128
unpool3 (3, 3)
(3, 3)
−
128
CNN
deconv4 (5, 5)
(1, 1)
(2, 2)
64
Decoder unpool4 (2, 2)
(2, 2)
−
64
deconv5 (5, 5)
(1, 1)
(2, 2)
32
unpool5 (2, 2)
(2, 2)
−
32
deconv6 (5, 5)
(1, 1)
(2, 2)
1
unpool6 (2, 2)
(2, 2)
−
1
3.3. Contrastive CAE
The difficulty in finding good latent attributes in the framework of an autoencoder lies in setting it to a proper dimensionality. Too long latent attributes
may contain redundancies for easier reconstructing the original input, but fall
short of concentrating on learning the saliently discriminative features for different classes. Meanwhile, shorter latent attributes can have less or limited
representation capability. The conventional optimisation strategy for an autoencoder considers no class information [26, 27], and hence the learnt latent
attributes is not well oriented to be discriminative for different classes. Specif15

ically, for our detection task, the auto-encoder may tend to learn the latent
attributes that can better reconstruct the original feature map, while ignoring
some salient attributes that indicate the difference between symptomatic and
asymptomatic segments.
To incorporate class information, symptomatic and asymptomatic, into the
optimisation of CAE, we fit the reconstruction error of the two classes into
contrastive loss [52]. As analogues to anomaly detection, we expect the CAE
to output a low reconstruction error for asymptomatic segments, and a high
reconstruction error for symptomatic segments. Therefore, the loss function for
our contrastive CAE can be seen as
v
u
N
u1 X
Contrastive Loss =t
|xpi − x̂pi |2 +
N i
v
u
N
u1 X
(m − t
|xni − x̂ni |2 ),
N i

(4)

where the superscripts p and n are used to distinguish positive (symptomatic)
and negative (asymptomatic) samples. Ideally, the reconstruction error for a
positive pair, original and reconstructed feature map for asymptomatic segments, is expected to be 0, indicating a successful reconstruction of the original
input at the decoder. In contrast, the reconstruction error for the negative input pair, original and reconstructed feature map for symptomatic segments, is
expected to be close to the margin value m. Therefore, the difference in classes
leads to different reconstruction errors from our CAE.
Contrastive loss has been successfully exploited in several image processing
tasks, especially those recently proposed for CT and X-ray diagnosis of COVID19 [58, 59, 60].
4. Experiments & Results
We conducted a series of experiments in order to test the models presented in
Section III for detecting COVID-like symptoms in the given 14-days heart rate
segments. The neural networks of different architectures are pre-trained with
16

the heart rate segments of 49 participants that reported COVID-like symptoms,
but had not met the CD1 or CD2 criterion. We apply leave-one-subject-out
cross-validation to the heart rate segments of the 19 individuals who meet CD1
or CD2, and further test the models on the corresponding symptom-free control
group.
The performance of our most effective method, i. e., the Contrastive CAE
(m is set to 5), is mainly compared to the conventional CNN and conventional
CAE that is optimised with an RMSE loss. Models of different layers are tested
using average Unweighted Average Recall (UAR, i. e., the recall of the classes
added divided by the number of classes), Precision, F1 measure, sensitivity, and
specificity as the evaluation metrics throughout the experiments.
To explore the advantage of using contrastive loss instead of RMSE loss when
training an CAE, we consider the dimensionality of latent attributes in different lengths, including 50, 100, 300, 500, and 1 000, ensuring fair comparisons.
A two-layers MLP is separately optimised for each model of different layers to
project the learnt latent attributes to classes, symptomatic or asymptomatic.
Furthermore, since the contrastive CAE is expected to produce a low reconstruction error for positive pairs (asymptomatic), and a high reconstruction error for
negative pairs (symptomatic), it provides us the possibility to directly perform
a classification based on the reconstruction error using classic machine learning
techniques, for instance, logistic regression.
The models’ parameters are optimised using an Adam optimiser. The learning rate decays from 0.03 to about 0.0001 with a decay factor of 0.33 after every
50 epochs. We keep using a batch size of 32 for all experiments. The settings
of the hyper-parameters are selected after careful fine-tuning the experiments,
to assure stable and fast convergence of our models.
In the following, we first compare our proposed contrastive CAE with a
CNN model, both of which use a different number of layers. As an additional
comparison method, we applied an MLP directly to the one-dimensional 5-min
average heart rate segment without formatting it into feature maps.

17

Table 5: Evaluation results for the binary COVID-19 yes/no (based on the symptom CD1/CD2
definitions above) classification [%] of the CNN and Contrastive CAE models with a different
number of layers. For the Contrastive CAE, classification is performed based on reconstruction error using logistic regression. Unweighted Average Recall (UAR, chance-level is 50 %),
Prescision, F1 measure, Sensitivity and Specificity are used as evaluation metrics.

#

r
ye
La

MLP

CNN

Contrastive
CAE

1
2
3
4
5
6
1
2
3
4
5
6

AR

io
cis
e
r

U
P
61.0
50.5
49.7
50.0
63.7
71.9
76.0
80.5
66.4
73.8
55.6
50.3
60.8
50.5
58.8
49.2
83.0
69.7
90.6
84.0
95.3
86.4
93.9
86.7
90.9
83.5

re

ty

su

n

s

F1

a
me

38.6
62.8
59.1
75.5
62.7
55.0
58.5
45.1
74.5
84.6
88.2
88.0
84.6

vi
iti
ns

Se

63.2
52.6
63.2
79.0
68.4
52.6
63.2
70.2
84.2
100.0
100.0
100.0
100.0

ity

Sp

ific
ec

58.8
46.8
64.3
73.1
64.3
58.5
58.5
47.4
81.9
81.3
90.6
87.7
81.9

4.1. Contrastive CAE vs MLP vs CNN
The MLP is found to be best be optimised with 4 layers (1000−250−50−20
hidden units of each layer), and its performance is shown in Table 5. In terms
of all evaluation metrics, the CNN model achieves its best performance with
3 convolutional layers, demonstrating significant improvements over the MLP
baseline (p < 0.05) in a one-tailed z-test.
Further significant improvement can be achieved by using the contrastive
CAE. We apply logistic regression to the reconstruction error of the test set,
and observe that the contrastive CAE with 4 encoder and 4 decoder layers
performs the best, achieving a UAR of 95.0%, precision of 86.4%, F1 measure of
88.2%, sensitivity of 100.0%, and specificity of 90.6%. Regarding the precision,
the contrastive CAE containing 5 encoder and 5 decoder layers can perform
slightly better. The performance results in significant improvements over the

18

CNN approaches (p < 0.05) in a one-tailed z-test.
4.2. Contrastive CAE vs Conventional CAE
We next investigate training a convolutional auto-encoder using contrastive
loss instead of RMSE for the binary COVID-19 (based on the symptoms described above) classification task. Different dimensionality of latent attribute
were tested, using a two-layers MLP classifier to project latent attributes to
classes. The parameters of the MLP classifier are separately tuned for different
dimensionalities in order to optimise each models’ detection performance. The
conventional CAE reaches its optimum with the latent attributes of the size of
50, achieving 60.1% UAR, 51.9% precision, 50.5% F1 measure, 50.5% sensitivity,
and 75.4% specificity as given in Table 6. This indicates that the CAE trained
with RMSE has a limited capability in learning discriminative lattent attributes
between symptomatic and asymptomatic segments, since the attributes learning
procedure considers no class information during optimisation, and, hence leaves
the classification difficulty to the final MLP classifiers. Besides, the optimised
performance of the conventional CAE is worse than that of the CNN models,
further stressing the need of involving the class information in training a more
efficient CAE for the detection task.
For the bianry classification task, the classes’ difference can be implicitly
modelled in the contrastive loss as in Eq. (4) for training the CAE, since the positive and negative reconstruction error are guided to produce a margin between
each other in a discriminative manner. Hence, the contrastive CAE is capable of
learning latent attributes that represent salient features to distinguish between
symptomatic and asymptomatic segments. Again, a 2-layer MLP is used to
classify the learnt latent attributes into classes, symptomatic or asymptomaic.
In our experiments, the contrastive CAE with an attribute dimensionality of 100
achieves its best result in terms of UAR, and when the dimensionality increases
to 300, the proposed contrastive CAE achieves a balanced results between the
UAR and precision measure, leading to its best F1 measure, i. e., 84.6%. In
addition, the model obtained the optimum sensitivity and a high specificity in
19

Table 6: Comparison of the results [%] between convolutional auto-encoders with 4 encoder
and 4 decoder layers trained with RMSE loss vs contrastive loss. Classification are performed
based on the latent latent attributes. Binary COVID-19 yes/no (based on symptom CD1/CD2
definitions above classification.) # Attr: dimensionality of latent attributes.

#

CAE

Contrastive
CAE

tr
At

50
100
300
500
1000
50
100
300
500
1000

ion
cis

R
e
UA Pr
66.6
51.9
58.5
51.2
63.4
51.0
65.8
51.3
55.3
50.5
92.0
64.3
92.2
67.2
90.9
84.0
90.9
64.4
71.9
53.3

re
su

F1

a
me

50.5
48.7
44.6
49.1
37.5
70.4
73.9
84.6
70.3
53.1

ty
ivi
t
i
s

n

Se

y
cit
fi
i
c
pe

S

57.9
47.4
63.2
68.4
47.4
100.0
100.0
100.0
94.7
68.4

75.4
69.5
63.7
63.2
63.2
83.9
84.3
81.9
87.1
75.4

the detection task, which considerably outperforms the conventional CAE that
is trained with RMSE loss.
Since our contrastive CAE was trained to create a sufficient margin between
the reconstruction errors of symptomatic and asymptomatic segments, applying classification directly on the reconstruction errors, rather than the learnt
latent attributes, is a more efficient way to use the contrastive CAE for our binary decision task. The decision threshold between the reconstruction errors of
symptomatic and asymptomatic classes is determined by the means of applying
logistic regression [61] on the training part for each cross-validation round. A
14-days heart rate segment is decided for as COVID-19 symptomatic (CD1/CD2
criterion) if the reconstruction error is above the decision boundary.
The best performance, shown in Table 7, is achieved with the attributes’
length equalling 100, achieving 95.0% UAR, 86.4% precision, 88.2% F1 measure, 100.0% sensitivity, and 90.6% specificity. Generally, the contrastive CAE
performs stable over different attributes’ dimensionality, reducing the difficulty
in setting its proper dimensionality. An extreme case is to combine the encoder

20

Table 7: The classification results [%] of contrastive convolutional auto-encoders with 4 encoder and 4 decoder layers based on the reconstruction error (rec. error) using logistic regression. Binary COVID-19 yes/no (based on symptom CD1/CD2 definitions above classification.
The last row indicates removing the latent attributes layer.

#

Contrastive
CAE
(rec. error)

tr.
At

50
100
300
500
1000
−

R
e
UA Pr
93.9
83.9
95.3
86.4
91.5
77.9
92.4
80.6
94.4
81.7
93.3
71.6

re

y
vit

su

on

i
cis

F1

a
me

87.5
88.2
84.7
85.4
88.1
76.8

i
sit

n

Se

100.0
100.0
100.0
100.0
100.0
100.0

ity

Sp

ific
ec

87.7
90.6
83.0
84.8
88.9
86.6

and decoder by removing the latent attributes layer. The performance, however,
maintains stable as given in the last row of Table 7.
4.3. Continuous Classification
We further explore the possibility to make binary COVID-19 yes/no (based
on the symptom CD1/CD2 definitions above) decisions earlier with our proposed contrastive CAE. To this end, we shift the window for sliding over the
symptomatic segments to earlier and later days, but still contain the onset date.
On the other hand, in order to test the model’s validality on later days, we shift
the sliding window for asymptomatic segments to later days that contain the
onset date. In our experiment, we keep the asymptomatic segments as in the
previous experiments, but replace the previous symptomatic segments with the
shifted symptomatic segments.
The experimental results, as seen in Table 8, reveal that the model works
well for the heart rate segments that are sliced one day earlier or three days later,
leading to no performance degradation. This is potentially due to the fact that
the model was trained with segments centred by onset date, and the shifted
symptomatic segments share similar internal structure as the original symptomatic segments. However, segments that further deviate from the original

21

Figure 4: Continuous binary COVID-19 yes/no (based on the symptom CD1/CD2 definitions
above) classification on each given 14-days heart rate windows of an examplary individual
(the same as in Figure 1, top).

symptomatic segments, i. e., shifting the sliding window to two more previous
days or four days later, results in decreased classification accuracy.
Table 8: Test results [%] for shifting the context window; binary COVID-19 yes/no (based on
symptom CD1/CD2 definitions above) classification.

#

Contrastive
CAE

ys
Da

−3
−2
−1
0
1
2
3
4
5

ion
cis

R
e
UA Pr
71.6
52.7
79.5
53.6
97.5
89.2
95.3
86.4
96.7
87.8
96.1
87.2
96.3
86.5
81.3
86.6
60.5
60.8

F1

re
su
a
e

m

52.7
54.2
91.3
88.2
89.8
89.1
88.4
81.8
60.3

y
vit

i
sit

n

Se

52.6
68.4
100.0
100.0
100.0
100.0
100.0
94.7
68.4

Sp

y
cit
fi
i
ec

62.2
61.0
91.2
90.6
90.8
90.3
89.9
80.2
54.6

Fig. 4 illustrates using our contrastive CAE to continuously classify COVID19 yes/no based on CD1/CD2 symptom presence based on the example given
in 1, top. This explicitly shows that the reconstruction error for the contrastive
CAE is low for the asymptomatic segments, but a decision can be made from the
14-days segments centred by date from one day before onset to three days after
onset. This indicates that the onset detection of the COVID-like symptoms can
be estimated in their earlier stage up to several days later.

22

5. Discussion
5.1. Shifting of Symptomatic Segments
Throughout all experiments, we kept assuming that the participant-reported
onset date is identical to the real symptom onset. As seen in Section 4.3,
the contrastive CAE performs effective on the symptomatic segments that are
centred relating to the reported symptom onset. In addition, we observe the
CAE to work robustly on the segments one day earlier and three days later, and
further shifting of the symptomatic segments leads to decrease in performance.
However, considering that the participants may have hesitated before reporting the appearance of their symptoms, the true symptom onset may happen
earlier than the reported onset date. As the experimental results given in Table 8, we see that the proposed model starts giving good results one day earlier,
although the model was optimised only with symptomatic segments and their
control segments, indicating that potentially some participants were conscious
of the symptoms and reported the symptoms on the second day. Furthermore,
shifting the segments up to a few days later (maximally three days in our experiments) leads to higher certainty that symptoms are indeed contained, and
hence, our model achieves stable performance in this case. Potentially, there exist some participants whose true symptom started on earlier days, but relieved
soon. In this case, the segments, that are shifted to a later point in time by
many days, may exclude the true symptomatic part, leading to a low detection
accuracy when assuming that the considered segments contain symptoms.
5.2. Features from Heart Rate Measurements
In this work, we base our study on 5-minutes mean of the heart rate measurements. In practice, we also considered the heart rate variance every 5 minutes
as input features when starting with standard CNNs, and did not see additional
improvement in terms of accuracy. Therefore, in further studies of conventional
CAEs and contrastive CAEs, we concentrated on the mean value rather than
variance. The mean heart rate values as features of our deep learning models
may be sufficient, since the global variance is retained in segmental mean values.
23

5.3. Difficulty in Model Optimisation
In some previous works, the class information can be applied to latent attribute layers, leading to the supervised auto-encoder introduced in [62]. Crossentropy losses are used to minimise the difference between predicted labels from
latent attributes and true labels. This approach provides a certain preservation
of the reconstructed feature map, taking the cross-entropy loss as a regularisation method. The reconstruction error and cross entropy loss are jointly
optimised. However, the optimisation of the joint loss requires a proper combination factor in order to balance the optimisation on reconstruction error and
prediction error. Since the two types of errors originate from different stages of
the auto-encoder model, leading to their different scale level, the difficulty lies
in seeking a good combination scale.
In our proposed method, the contrastive loss can be employed directly on
the reconstruction error for positive and negative input pairs. The training
procedure is simpler, since the reconstruction errors of the positive pairs and
negative pairs present in the same scale level at the beginning of the training
stage. The method also provides the chance to validate whether the model has
learnt discriminative latent attributes for different classes in the auto-encoder
framework.

6. Conclusion
In this work, we explored several deep learning models, including conventional CNNs, conventional CAEs, and contrastive CAEs, to make a machinelearning-based COVID-19 yes/no decision based on symptoms’ presence as definded
by two criteria (CD1/CD2) given 14-days heart rate measurements from a Fitbit wristband. The models were pre-trained based on the heart rate data of 49
participants with MS who reported to have COVID-like symptoms but did not
satisfy CD1 or CD2. However, the reported symptoms were highly related to
COVID-19 symptoms. The models were then tested on data of 19 MS participants whose reported symptoms met the criteria of CD1 or CD2, by means of

24

LOSO CV. In this process, each of the 19 symptomatic MS participants was
paired with a site-, gender-, and age-matched symptom-free MS participants.
Experimental results indicate that our proposed contrastive CAE approach, incorporating class information into optimising the CAE with contrastive loss,
achieved considerable improvements over the conventional CNN and CAE models in terms of performance, including UAR, precision, F1 measure, as well as
sensitivity, and specificity.
The effectiveness of contrastive learning as demonstrated in this work shall
motivate further research, including the investigation of different bio-signal measurements, features, and model architectures. Several recent works [5, 11] have
shown the potential of using more than one bio-signal measurement for Covid19 recognition. Such additional measurements can be audio and speech signal
[63, 64] which can also be achieved from wearable devices.
Since the set-up of our experiments was chosen to detect whether or not
the COVID-19-like symptoms appeared during a period of heart rate data, the
models show limitations in a causal set-up, i. e., when trying to predict potential
symptoms before they are present. To this end, future work shall try to answer
the question of how many days in advance we will already be able to reliably
predict the potential imminent onset of COVID-like symptoms. As the acquisition of data in the RADAR-CNS programme is still ongoing, the improvement
of our proposed binary COVID-19 yes/no (based on the symptom CD1/CD2
definitions above) classification model based on a broader data foundation is
expected. Further to that, other windows of time should be analysed. Overall,
we are optimistic that an applicable decision can be made as to COVID-19 presence based on the symptoms defined herein based on machine learning analysis
of consumer-type heart rate measurement.

25

Acknowledgement
This project has received funding from the Innovative Medicines Initiative7
2 Joint Undertaking under grant agreement No 115902. This Joint Undertaking
receives support from the European Union’s Horizon 2020 research and innovation programme and EFPIA. This communication reflects the views of the
RADAR-CNS consortium and neither IMI nor the European Union and EFPIA
are liable for any use that may be made of the information contained herein.
References
[1] L. Piwek, D. A. Ellis, S. Andrews, A. Joinson, The rise of consumer health
wearables: Promises and barriers, PLOS Medicine 13 (2) (2018) 9 pages.
[2] Y. Ranjan, Z. Rashid, C. Stewart, P. Conde, M. Begale, D. Verbeeck,
S. Boettcher, T. Hyve, R. Dobson, A. Folarin, T. R.-C. Consortium,
RADAR-base: Open source mobile health platform for collecting, monitoring, and analyzing data using sensors, wearables, and mobile devices,
JMIR Mhealth Uhealth 7 (8) (2019) 13 pages.
[3] M. Mohammadi, A. Al-Fuqaha, S. Sorour, M. Guizani, Deep Learning for
IoT big data and streaming analytics: A survey, IEEE Communications
Surveys & Tutorials 20 (4) (2018) 2923–2960.
[4] J. Radin, N. Wineinger, E. Topol, S. Steinhubl, Harnessing wearable device
data to improve state-level real-time surveillance of influenza-like illness in
the USA: A population-based study, Lancet Digital Health 2 (2) (2020)
85–93.
[5] G. Quer, J. Radin, M. Gadaleta, K. Baca-Motes, L. Ariniello, E. Ramos,
V. Kheterpal, E. Topol, S. Steinhubl, Wearable sensor data and selfreported symptoms for COVID-19 detection, Nature Medicine 2 (2020)
1–5.
7 https://www.imi.europa.eu/

26

[6] G. Zhu, J. Li, Z. Meng, Y. Yu, Y. Li, X. Tang, Y. Dong, G. Sun, R. Zhou,
H. Wang, K. Wang, W. Huang, Learning from large-scale wearable device
data for predicting epidemics trend of COVID-19, Discrete Dynamics in
Nature and Society 2020, 8 pages.
[7] J. K. Triedman, R. J. Cohen, J. P. Saul, Mild hypovolemic stress alters
autonomic modulation of heart rate, Hypertension 21 (2) (1993) 236–247.
[8] D. Bonaduce, M. Petretta, F. Marciano, M. L. Vicario, C. Apicella, M. A.
Rao, E. Nicolai, M. Volpe, Independent and incremental prognostic value
of heart rate variability in patients with chronic heart failure, American
Heart Journal 138 (2) (1999) 273–284.
[9] J. Huang, S. M. Sopher, E. Leatham, S. Redwood, A. J. Camm, J. C.
Kaski, Heart rate variability depression in patients with unstable angina,
American Heart Journal 130 (4) (1995) 772–779.
[10] S. Ahmad, A. Tejuja, K. D. Newman, R. Zarychanski, A. J. Seely, Clinical
review: A review and analysis of heart rate variability and the diagnosis
and prognosis of infection, Critical Care 13 (232) (2009) 1–7.
[11] T. Mishra, M. Wang, A. A. Metwally, G. Bogu, A. W. Brooks, A. Bahmani, A. Alavi, A. Celli, E. Higgs, O. Dagan-Rosenfeld, B. Fay, S. Kirkpatrick, R. Kellogg, M. Gibson, T. Wang, E. Hunting, P. Mamic, A. Gany,
B. Rolnik, A. B. Ganz, X. Li, M. P. Snyder, Pre-symptomatic detection of
COVID-19 from smartwatch data, Nature Biomedical Engineering 4 (12)
(2020) 1208–1220.
[12] N. Marinsek, A. Shapiro, I. Clay, B. Bradshaw, E. Ramirez, J. Min, A. Trister, Y. Wang, T. Althoff, L. Foschini, Measuring COVID-19 and influenza
in the real world via person-generated health data, medRxiv (2020) 23
pages.
[13] A. Natarajan, H.-W. Su, C. Heneghan, Assessment of physiological signs as-

27

sociated with COVID-19 measured using wearable devices, Digital Medicine
3 (156) (2020) 1–8.
[14] C. Aytekin, X. Ni, F. Cricri, E. Aksu, Clustering and unsupervised anomaly
detection with l2 normalized deep auto-encoder representations, in: Proceedings of International Joint Conference on Neural Networks, Rio, Brazil,
2018, pp. 1–6.
[15] Z. Chen, C. K. Yeo, B. S. Lee, C. T. Lau, Autoencoder-based network
anomaly detection, in: Proceedings of Wireless Telecommunications Symposium, Phoenix, AZ, 2018, pp. 1–5.
[16] G. Dalla Costa, L. Leocani, X. Montalban, A. I. Guerrero, P. S. Søorensen,
M. Magyari, R. Dobson, N. Cummins, V. Narayan, M. Hotopf, G. Comi,
The RADAR-CNS consortium, Real-time assessment of COVID-19 prevalence among multiple sclerosis patients: A multicenter European study,
Neurological Sciences 41 (7) (2020) 1647–1650.
[17] C. Huang, Y. Wang, X. Li, L. Ren, J. Zhao, Y. Hu, L. Zhang, G. Fan,
J. Xu, X. Gu, Z. Cheng, T. Yu, J. Xia, Y. Wei, W. Wu, X. Xie, W. Yin,
H. Li, M. Liu, Y. Xiao, H. Gao, L. Guo, J. Xie, G. Wang, R. Jiang, Z. Gao,
Q. Jin, J. Wang, B. Cao, Clinical features of patients infected with 2019
novel coronavirus in Wuhan, China, The Lancet 395 (10223) (2020) 497–
506.
[18] T.-T. Wong, Performance evaluation of classification algorithms by k-fold
and leave-one-out cross validation, Pattern Recognition 48 (9) (2015) 2839–
2846.
[19] J. Backer, D. Klinkenberg, J. Wallinga, Incubation period of 2019 novel
coronavirus (2019-nCoV) infections among travellers from Wuhan, China,
20–28 January 2020, Eurosurveillance 25 (5) (2020) 6 pages.
[20] S. Lauer, K. Grantz, Q. Bi, F. Jones, Q. Zheng, H. Meredith, A. Azman,
N. Reich, J. Lessler, The incubation period of coronavirus disease 2019
28

(COVID-19) from publicly reported confirmed cases: Estimation and application, Annals of Internal Medicine 172 (9) (2020) 577–582.
[21] R. P. Hirten, M. Danieletto, L. Tomalin, K. H. Choi, M. Zweig, E. Golden,
S. Kaur, D. Helmus, A. Biello, R. Pyzik, A. Charney, R. Miotto, B. S.
Glicksberg, M. Levin, I. Nabeel, J. Aberg, D. Reich, D. Charney, E. P.
Bottinger, L. Keefer, M. Suarez-Farinas, G. N. Nadkarni, Z. A. Fayad,
Use of physiological data from a wearable device to identify SARS-CoV-2
infection and symptoms and predict COVID-19 diagnosis: Observational
study, Journal of Medical Internet Research 23 (2) (2021) e26107.
[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
A. Karpathy, A. Khosla, M. Bernstein, A. Berg, L. Fei-Fei, Imagenet large
scale visual recognition challenge, International Journal of Computer Vision
115 (3) (2015) 211–252.
[23] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep
convolutional neural networks, Communications of the ACM 60 (6) (2017)
84–90.
[24] M. Makkie, H. Huang, Y. Zhao, A. V. Vasilakos, T. Liu, Fast and scalable
distributed deep convolutional autoencoder for fMRI big data analytics,
Neurocomputing 325 (2019) 20–30.
[25] Z. Cheng, H. Sun, M. Takeuchi, J. Katto, Deep convolutional autoencoderbased lossy image compression, in: Proceedings of Picture Coding Symposium, San Francisco, CA, 2018, pp. 253–257.
[26] S. Chen, H. Liu, X. Zeng, S. Qian, J. Yu, W. Guo, Image classification based
on convolutional denoising sparse autoencoder, Mathematical Problems in
Engineering 2017 (2017) 1–16.
[27] B. Du, W. Xiong, J. Wu, L. Zhang, L. Zhang, D. Tao, Stacked convolutional
denoising auto-encoders for feature representation, IEEE Transactions on
Cybernetics 47 (4) (2017) 1017–1027.
29

[28] K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional networks: Visualising image classification models and saliency maps, in: Proceedings of International Conference on Learning Representations, Banff,
Canada, 2014, 8 pages.
[29] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: Proceedings of European Conference on Computer Vision,
Zurich, Switzerland, 2014, pp. 818–833.
[30] S. Indolia, A. Goswami, S. Mishra, P. Asopa, Conceptual understanding of
convolutional neural network – a deep learning approach, Procedia Computer Science 132 (2018) 679–688.
[31] Z. Zhao, P. Zheng, S. Xu, X. Wu, Object detection with deep learning:
A review, IEEE Transactions on Neural Networks and Learning Systems
30 (11) (2019) 3212–3232.
[32] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, L. FeiFei, Large-scale video classification with convolutional neural networks, in:
Proceedings of Conference on Computer Vision and Pattern Recognition,
Columbus, OH, 2014, pp. 1725–1732.
[33] H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, P.-A. Muller, Deep
learning for time series classification: A review, Data Mining and Knowledge Discovery 33 (4) (2019) 917–963.
[34] A. Khamparia, D. Gupta, N. G. Nguyen, A. Khanna, B. Pandey, P. Tiwari,
Sound classification using convolutional neural network and tensor deep
stacking network, IEEE Access 7 (99) (2019) 7717–7727.
[35] K. Yasaka, H. Akai, O. Abe, S. Kiryu, Deep learning with convolutional
neural network for differentiation of liver masses at dynamic contrastenhanced CT: A preliminary study, Radiology 286 (3) (2017) 887–896.
[36] V. Gulshan, L. Peng, M. Coram, M. C. Stumpe, D. Wu, A. Narayanaswamy,
S. Venugopalan, K. Widner, T. Madams, J. Cuadros, R. Kim, R. Raman,
30

P. C. Nelson, J. L. Mega, D. R. Webster, Development and validation of
a deep learning algorithm for detection of diabetic retinopathy in retinal
fundus photographs, The Journal of the American Medical Association
316 (22) (2016) 2402–2410.
[37] B. Ehteshami Bejnordi, M. Veta, P. Johannes van Diest, B. van Ginneken, N. Karssemeijer, G. Litjens, J. A. W. M. van der Laak, the CAMELYON16 Consortium, Diagnostic assessment of deep learning algorithms
for detection of lymph node metastases in women with breast cancer, The
Journal of the American Medical Association 318 (22) (2017) 2199–2210.
[38] O. Ronneberger, P. Fischer, T. Brox, U-Net: Convolutional networks for
biomedical image segmentation, in: Proceedings of International Conference on Medical Image Computing & Computer Assisted Intervention, Munich, Germany, 2015, pp. 234–241.
[39] Z. Hu, J. Tang, Z. Wang, K. Zhang, L. Zhang, Q. Sun, Deep learning for
image-based cancer detection and diagnosis: A survey, Pattern Recognition
83 (2018) 134 – 149.
[40] F. Shi, J. Wang, J. Shi, Z. Wu, Q. Wang, Z. Tang, K. He, Y. Shi, D. Shen,
Review of artificial intelligence techniques in imaging data acquisition, segmentation and diagnosis for COVID-19, IEEE Reviews in Biomedical Engineering (2020) 13 pages.
[41] A. Oulefki, S. Agaian, T. Trongtirakul, A. K. Laouar, Automatic COVID19 lung infected region segmentation and measurement using CT-scans
images, Pattern Recognition (2020) 13 pages.
[42] X. Bai, C. Fang, Y. Zhou, S. Bai, Z. Liu, L. Xia, Q. Chen, Y. Xu,
T. Xia, S. Gong, X. Xudong, D. Song, R. Du, C. Zhou, C. Chen, D. Nie,
L. Qin, W. Chen, Predicting COVID-19 Malignant Progression with AI
Techniques, Medrxiv (2020) 29 pages.

31

[43] Y. Ding, X. Zhang, J. Tang, A noisy sparse convolution neural network
based on stacked auto-encoders, in: Proceedings of International Conference on Systems, Man and Cybernetics, Banff, Canada, 2017, pp. 3457–
3461.
[44] M. W. Gardner, S. Dorling, Artificial neural networks (the multilayer perceptron) – A review of applications in the atmospheric sciences, Atmospheric Environment 32 (14-15) (1998) 2627–2636.
[45] M. Chen, X. Shi, Y. Zhang, D. Wu, M. Guizani, Deep features learning
for medical image analysis with convolutional autoencoder neural network,
IEEE Transactions on Big Data (2017) 10 pages.
[46] A. Ruiz-Garcia, M. Elshaw, A. Altahhan, V. Palade, Stacked deep convolutional auto-encoders for emotion recognition from facial expressions,
in: Proceedings of International Joint Conference on Neural Networks, Anchorage, AK, 2017, pp. 1586–1593.
[47] Q. Hu, M. Feng, L. Lai, J. Pei, Prediction of drug-likeness using deep
autoencoder neural networks, Frontiers in Genetics 9 (585) (2018) 8 pages.
[48] C. Zhou, R. C. Paffenroth, Anomaly detection with robust deep autoencoders, in: Proceedings of Special Interest Group on Knowledge Discovery
and Data Mining, Nova Scotia, Canada, 2017, pp. 665–674.
[49] B. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu, D. Cho, H. Chen,
Deep autoencoding Gaussian mixture model for unsupervised anomaly detection, in: Proceedings of International Conference on Learning Representations, Vancouver, Canada, 2018, 19 pages.
[50] Y. Zhao, B. Deng, C. Shen, Y. Liu, H. Lu, X.-S. Hua, Spatio-temporal
autoencoder for video anomaly detection, in: Proceedings of ACM International Conference on Multimedia, New York, NY, 2017, pp. 1933–1941.
[51] C. Aytekin, X. Ni, F. Cricri, E. Aksu, Clustering and unsupervised anomaly
detection with l2 normalized deep auto-encoder representations, in: Pro32

ceedings of International Joint Conference on Neural Networks, Rio, Brazil,
2018, pp. 1–6.
[52] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot,
C. Liu, D. Krishnan, Supervised contrastive learning, in: Proceedings of
Conference on Neural Information Processing Systems, Vancouver, Canada,
2020, 13 pages.
[53] S. Ioffe, C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift, in: Proceedings of International
Conference on Machine Learning, Lille, France, 2015, pp. 448–456.
[54] J. Bjorck, C. P. Gomes, B. Selman, Understanding batch normalization,
in: Proceedings of Conference on Neural Information Processing Systems,
Montreal, Canada, 2018, 12 pages.
[55] K. He, X. Zhang, S. Ren, J. Sun, Delving deep into rectifiers: Surpassing human-Level performance on ImageNet classification, Proceedings of
International Conference on Computer Vision (2015) 1026–1034.
[56] W. Shang, K. Sohn, D. Almeida, H. Lee, Understanding and improving
convolutional neural networks via concatenated rectified linear units, in:
Proceedings of International Conference on Machine Learning, New York,
NY, 2016, pp. 2217–2225.
[57] J. F. Kenney, Mathematics of Statistics, D. Van Nostrand, 1939.
[58] M. Shorfuzzaman, M. S. Hossain, MetaCOVID: A Siamese neural network
framework with contrastive loss for n-shot diagnosis of COVID-19 patients,
Pattern Recognition 113 (2021) 107700, this issue.
[59] J. Li, G. Zhao, Y. Tao, P. Zhai, H. Chen, H. He, T. Cai, Multi-task contrastive learning for automatic CT and X-ray diagnosis of COVID-19, Pattern Recognition 114 (2021) 107848, this issue.

33

[60] X. Chen, L. Yao, T. Zhou, J. Dong, Y. Zhang, Momentum contrastive
learning for few-shot COVID-19 diagnosis from chest CT images, Pattern
Recognition 113 (2021) 107826, this issue.
[61] H. Park, An introduction to logistic regression: From basic concepts to interpretation with particular attention to nursing domain, Journal of Korean
Academy of Nursing 43 (2) (2013) 154–64.
[62] L. Le, A. Patterson, M. White, Supervised autoencoders: Improving generalization performance with unsupervised regularizers, in: Proceedings of
Conference on Neural Information Processing Systems, Montreal, Canada,
2018, pp. 107–117.
[63] B. Schuller, D. Schuller, K. Qian, J. Liu, H. Zheng, X. Li, COVID-19 and
computer audition: An overview on what speech & sound analysis could
contribute in the SARS-CoV-2 Corona crisis, in: arxiv, 2020, 6 pages.
[64] J. Han, K. Qian, M. Song, Z. Yang, Z. Ren, S. Liu, J. Liu, H. Zheng,
W. Ji, T. Koike, X. Li, Z. Zhang, Y. Yamamoto, B. Schuller, An early study
on intelligent analysis of speech under COVID-19: Severity, sleep quality,
fatigue, and anxiety, in: Proceedings of INTERSPEECH, Shanghai, China,
2020, pp. 4946–4950.

34

