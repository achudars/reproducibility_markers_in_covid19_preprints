arXiv:2010.01916v1 [cs.LG] 5 Oct 2020

Temporal Positive-unlabeled Learning for Biomedical
Hypothesis Generation via Risk Estimation
Uchenna Akujuobi1,2 Jun Chen1 Mohamed Elhoseiny1
Michael Spranger2 Xiangliang Zhang1
1
2
King Abdullah University of Science and Technology
Sony AI, Tokyo
{uchenna.akujuobi,jun.chen,mohamed.elhoseiny,xiangliang.zhang}@kaust.edu.sa
michael.spranger@gmail.com

Abstract
Understanding the relationships between biomedical terms like viruses, drugs, and
symptoms is essential in the fight against diseases. Many attempts have been made
to introduce the use of machine learning to the scientific process of hypothesis
generation (HG), which refers to the discovery of meaningful implicit connections
between biomedical terms. However, most existing methods fail to truly capture
the temporal dynamics of scientific term relations and also assume unobserved
connections to be irrelevant (i.e., in a positive-negative (PN) learning setting). To
break these limits, we formulate this HG problem as future connectivity prediction
task on a dynamic attributed graph via positive-unlabeled (PU) learning. Then,
the key is to capture the temporal evolution of node pair (term pair) relations
from just the positive and unlabeled data. We propose a variational inference
model to estimate the positive prior, and incorporate it in the learning of node
pair embeddings, which are then used for link prediction. Experiment results on
real-world biomedical term relationship datasets and case study analyses on a
COVID-19 dataset validate the effectiveness of the proposed model.

1

Introduction

Recently, the study of co-relationships between biomedical entities is increasingly gaining attention.
The ability to predict future relationships between biomedical entities like diseases, drugs, and genes
enhances the chances of early detection of disease outbreaks and reduces the time required to detect
probable disease characteristics. For instance, in 2020, the COVID-19 outbreak pushed the world to a
halt with scientists working tediously to study the disease characteristics for containment, cure, and
vaccine. An increasing number of articles encompassing new knowledge and discoveries from these
studies were being published daily [1]. However, with the accelerated growth rate of publications,
the manual process of reading to extract undiscovered knowledge increasingly becomes a tedious and
time-consuming task beyond the capability of individual researchers.
In an effort towards an advanced knowledge discovery process, computers have been introduced to
play an ever-greater role in the scientific process with automatic hypothesis generation (HG). The
study of automated HG has attracted considerable attention in recent years [40, 24, 44, 46]. Several
previous works proposed techniques based on association rules [24, 17, 46], clustering and topic
modeling [44, 43, 5], text mining [42, 41], and others [27, 48, 38]. However, these previous works
fail to truly utilize the crucial information encapsulated in the dynamic nature of scientific discoveries
and assume that the unobserved relationships denote a non-relevant relationship (negative).
To model the historical evolution of term pair relations, we formulate HG on a term relationship graph
G = {V, E}, which is decomposed into a sequence of attributed graphlets G = {G1 , G2 , ..., GT },
where the graphlet at time t is defined as,
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

Definition 1. Temporal graphlet: A temporal graphlet Gt = {V t , E t , xtv } is a temporal subgraph
at time step t, which consists of nodes (terms) V t satisfying V 1 ⊆ V 2 , ..., ⊆ V T and the observed
co-occurrence between these terms E t satisfying E 1 ⊆ E 2 , ..., ⊆ E T . And xtv is the node attribute.
Example of the node terms can be covid-19, fever, cough, Zinc, hepatitis B virus etc. When two terms
co-occurred at time t in scientific discovery, a link between them is added to E t , and the nodes are
added to V t if they haven’t been added.
Definition 2. Hypothesis Generation (HG): Given G = {G1 , G2 , ..., GT }, the target is to predict
which nodes unlinked in V T should be linked (a hypothesis is generated between these nodes).

We address the HG problem by modeling how E t was formed from t = 1 to T (on a dynamic graph),
rather than using only E T (on a static graph). In the design of learning model, it is clear to us the
observed edges are positive. However, we are in a dilemma whether the unobserved edges are positive
or negative. The prior work simply set them to be negative, learning in a positive-negative (PN)
setting) based on a closed world assumption that unobserved connections are irrelevant (negative)
[38, 27, 4]. We set the learning with a more realistic assumption that the unobserved connections are
a mixture of positive and negative term relations (unlabeled), a.k.a. Positive-unlabeled (PU) learning,
which is different from semi-supervised PN learning that assumes a known set of labeled negative
samples. For the observed positive samples in PU learning, they are assumed to be selected entirely
at random from the set of all positive examples [15]. This assumption facilitates and simplifies both
theoretical analysis and algorithmic design since the probability of observing the label of a positive
example is constant. However, estimating this probability value from the positive-unlabeled data is
nontrivial. We propose a variational inference model to estimate the positive prior and incorporate
it in the learning of node pair embeddings, which are then used for link prediction (hypothesis
generation).
We highlight the contributions of this work as follows.
1) Methodology: we propose a PU learning approach on temporal graphs. It differs from other
existing approaches that learn in a conventional PN setting on static graphs. In addition, we estimate
the positive prior via a variational inference model, rather than setting by prior knowledge.
2) Application: to the best of our knowledge, this is the first the application of PU learning on
the HG problem, and on dynamic graphs. We applied the proposed model on real-world graphs of
terms in scholarly publications published from 1945 to 2020. Each of the three graphs has around
30K nodes and 1-2 million edges. The model is trained end-to-end and shows superior performance
on HG. Case studies demonstrate our new and valid findings of the positive relationship between
medical terms, including newly observed terms that were not observed in training.

2

Related Work of PU Learning

In PU learning, since the negative samples are not available, a classifier is trained to minimize the
expected misclassification rate for both the positive and unlabeled samples. One group of study
[31, 30, 32, 21] proposed a two-step solution: 1) identifying reliable negative samples, and 2) learning
a classifier based on the labeled positives and reliable negatives using a (semi)-supervised technique.
Another group of studies [35, 29, 25, 16, 39] considered the unlabeled samples as negatives with label
noise. Hence, they place higher penalties on misclassified positive examples or tune a hyperparameter
based on suitable PU evaluation metrics. Such a proposed framework follows the SCAR (Selected
Completely at Random) assumption since the noise for negative samples is constant.
PU Learning via Risk Estimation Recently, the use of unbiased risk estimator has gained attention
[13, 10, 14, 47]. The goal is to minimize the expected classification risk to obtain an empirical risk
minimizer. Given an input representation h (in our case the node pair representation to be learned), let
f : Rd → R be an arbitrary decision function and l : R × {±1} → R be the loss function calculating
the incurred loss l(f (h), y) of predicting an output f (h) when the true value is y. Function l has
a variety of forms, and is determined by application needs [28]. In PN learning, the empirical risk
minimizer fˆP N is obtained by minimizing the PN risk R̂(f ) w.r.t. a class prior of πp :
−
R̂(f ) = πP R̂+
(1)
P (f ) + πN R̂N (f ),
P
P
n
n
+
−
P
N
1
N
where πN = 1 − πP , R̂P (f ) = n1P i=1 l(f (hP
i ), +1) and R̂N (f ) = nN
i=1 l(f (hi ), −1).
The variables nP and nN are the numbers of positive and negative samples, respectively.

2

PU learning has to exploit the fact that πN pN (h) = p(h) − πP pP (h), due to the absence of negative
samples. The second part of Eq. (1) can be reformulated as:
−
−
πN R̂−
N (f ) = R̂U − πP R̂P (f ),

(2)

−
where R−
U = Eh∼p(h) [l(f (h), −1)] and RP = Eh∼p(h|y=+1) [l(f (h), −1)]. Furthermore, the classification risk can then be approximated by:
−
−
R̂P U (f ) = πP R̂+
(3)
P (f ) + R̂U (f ) − πP R̂P (f ),
P
nU
−
−
1
1
P
U
where R̂P (f ) = nP i=1 l(f (hi ), −1) , R̂U (f ) = nU i=1 l(f (hi ), −1), and nU is the number
of unlabeled data sample. To obtain an empirical risk minimizer fˆP U for the PU learning framework,

PnP

R̂P U (f ) needs to be minimized. Kiryo et al. noted that the model tends to suffer from overfitting on
the training data when the model f is made too flexible [28]. To alleviate this problem, the authors
proposed the use of non-negative risk estimator for PU learning:
−
−
R̃P U (f ) = πP R̂+
(4)
P (f ) + max{0, R̂U − πP R̂P (f )}.
It works in fact by explicitly constraining the training risk of PU to be non-negative. The key challenge
in practical PU learning is the unknown of prior πP .

Prior Estimation The knowledge of the class prior πP is quintessential to estimating the classification risk. In PU learning for our node pairs, we represent a sample as {h, s, y}, where h is the node
pair representation (to be learned), s indicates if the pair relationship is observed (labeled, s = 1)
or unobserved (unlabeled, s = 0), and y denotes the true class (positive or negative). We have only
the positive samples labeled: p(y = 1|s = 1) = 1. If s = 0, the sample can belong to either the
positive or negative class. PU learning runs commonly with the Selected Completely at Random
(SCAR) assumption, which postulates that the labeled sample set is a random subset of the positive
sample set [15, 6, 8]. The probability of selecting a positive sample to observe can be denoted as:
p(s = 1|y = 1, h). The SCAR assumption means: p(s = 1|y = 1, h) = p(s = 1|y = 1). However,
it is hard to estimate πP = p(y = 1) with only a small set of observed samples (s = 1) and a large set
of unobserved samples (s = 0) [7]. Solutions have been tried by i) estimating from a validation set of
a fully labeled data set (all with s = 1 and knowing y = 1 or −1) [28, 11]; ii) estimating from the
background knowledge; and iii) estimating directly from the PU data [15, 6, 8, 26, 10]. In this paper,
we focus on estimating the prior directly from the PU data. Specifically, unlike the other methods, we
propose a scalable method based on deep variational inference to jointly estimate the prior and train
the classification model end-to-end. The proposed deep variational inference uses KL-divergence
to estimate the parameters of class mixture model distributions of the positive and negative class
in contrast to the method proposed in [10] which uses penalized L1 divergences to assign higher
penalties to class priors that scale the positive distribution as more than the total distribution.

3
3.1

PU learning on Temporal Attributed Networks
Model Design

The architecture of our Temporal Relationship Predictor (TRP) model is shown in Fig. 1. For a
given pair of nodes aij =< vi , vj > in any temporal graphlet Gt , the main steps used in the training
process of TRP for calculating the connectivity prediction score pt (aij ) are given in Algorithm 1.
The testing process also uses the same Algorithm 1 (with t=T ), calculating pT (aij ) for node pairs
that have not been connected in GT −1 . The connectivity prediction score is calculated in line 6 of
Algorithm 1 by pt (aij ) = fC (htaij ; θC ), where θC is the classification network parameter, and the
embedding vector htaij for the pair aij is iteratively updated in lines 1-5. These iterations of updating
htaij are shown as the recurrent structure in Fig. 1 (a), followed by the classifier fC (.; θC ).
−1 τ
The recurrent update function hτaij = fA (hτaij
, zvi , zvτj ; θA ), τ = 1...t, in line 4 is shown in Fig. 1
(b), and has a Gated recurrent unit (GRU) network at its core,
−1
P = σg (W z fm (zvτi , zvτj ) + U P hτaij
+ bP ),
−1
r = σg (W r fm (zvτi , zvτj ) + U r hτaij
+ br ),

−1
h̃τaij = σh0 (W fm (zvτi , zvτj ) + r ◦ U haτ ij
+ b),
−1
hτaij = P ◦ h̃τaij + (1 − P) ◦ hτaij
.

3

(5)

Recurrent
update
Block

Recurrent
update
Block

Recurrent
update
Block

Recurrent
update
Block

(a)

} Aggregators
[...]
[...]
[...]
[...]
[...]

[...]

[...]

[...]

[...]
[...]

[...]

m=1

[...]

[...]
m=2

(c)
(b)

Figure 1: The proposed TRP model. Block (a) shows the outer view of the model framework. The
inner structure of the recurrent update block and neighborhood aggregation method are shown in
block (b) and (c), respectively.
Algorithm 1: Calculate the future connection score for term pairs ai,j =< vi , vj >

1
2
3
4
5
6

Input: G = {G1 , G2 , . . . , GT } with node feature xtv , a node pair ai,j =< vi , vj > in Gt , and an
initialized pair embedding vector h0ai,j (e.g., by zeros)
Result: ptai,j , the connectivity prediction score for the node pair ai,j
for τ ← 1· · · t do
Obtain the current node feature xτv (v = vi , vj ) of both nodes (terms) vi , vj ; as well as xτN r(v)
(v = vi , vj ) for the node feature of sampled neighboring nodes for vi , vj ;
Aggregate the neighborhood information of node v = vi , vj , zvτ = fG (xτv , xτN r(v) ; θG );
−1
Update the embedding vector for the node pair hτai,j = fA (hτai,j
, zvτi , zvτj ; θA ) ;
end
Return ptai,j = fC (htai,j ; θC )

where ◦ denotes element-wise multiplication, σ is a nonlinear activation function, and fm (.) is an
aggregation function. In this study, we use a max pool aggregation. The variables {W, U } are the
−1
weights. The inputs to function fA include: hτaij
, the embedding vector in previous step; {zvt i , zvt j },
the representation of node vi and vj after aggregation their neighborhood, zvτ = fG (xτv , xτN r(v) ; θG ),
given in line 3. The aggregation function fG takes input the node feature xτv , and the neighboring
node feature xτN r(v) and goes through the aggregation block shown in Fig. 1 (c). The aggregation
network fG (; θG ) is implemented following GraphSAGE [20], which is one of the most popular
graph neural networks for aggregating node and its neighbors.
The loss function in our problem l(pt (aij ), y) evaluates the loss incurred by predicting a connectivity
pt (aij ) = fC (htaij ; θC ) when the ground truth is y. For constructing the training set for our PU
learning in the dynamic graph, we first clarify the label notations. For one pair aij from a graph
Gt , its label ytij = 1 (positive) if the two nodes have a link observed in Gt+1 (they have an edge
∈ E t+1 , observed in next time step), i.e., sij
t = 1. Otherwise when no link is observed between them
ij
t+1
ij
in G , a is unlabeled, i.e., st = 0, since ytij can be either 1 or -1. Since we consider insertion
4

only graphlets sequence, V 1 ⊆ V 2 , ..., ⊆ V T and E 1 ⊆ E 2 , ..., ⊆ E T , ytij = 1 maintains for all
future steps after t (once positive, always positive). At the final step t = T , all pairs with observed
connections already have yTij = 1, our objective is to predict the connectivity score for those pairs
with sij
T = 0. Our loss function is defined following the unbiased risk estimator in Eq. (3),
−
−
LR = πP R̂+
(6)
P (fC ) + R̂U (fC ) − πP R̂P (fC ),
P
P
−
1
1
t ij
where R̂+
P (fC ) = |HP |
aij ∈HP 1/(1 + exp(p (a ))), R̂U (fC ) = |HU |
aij ∈HU 1/(1 +
P
1
t ij
exp(−pt (aij ))), and R̂−
P (fC ) = |H |
aij ∈H 1/(1 + exp(−p (a ))) with the positive samples
P

P

HP and unlabeled samples HU , when taking l as sigmoid loss function. LR can be adjusted with the
−
−
non-negative constraint in Eq. (4), with the same definition of R̂+
P (f ), R̂U , and R̂P (f ).

3.2

Prior Estimation

The positive prior πP is a key factor in LR to be addressed. The samples we have from G are only
positive HP and unlabeled HU . Due to the absence of negative samples and of prior knowledge, we
present an estimate of the class prior from the distribution of h, which is the pair embedding from
fA . Without loss of generality, we assume that the learned h of all samples has a Gaussian mixture
distribution of two components, one is for the positive samples, while the other is for the negative
samples although they are unlabeled. The mixture distribution is parameterized by β, including
the mean, co-variance matrix and mixing coefficient of each component. We learn the mixture
distribution using stochastic variational inference [23] via the “Bayes by Backprop” technique [9].
The use of variational inference has been shown to have the ability to model salient properties of
the data generation mechanism and avoid singularities. The idea is to find variational distribution
variables θ∗ that minimizes the Kullback-Leibler (KL) divergence between the variational distribution
q(β|θ) and the true posterior distribution p(β|h):
θ∗ = arg minθ LE ,

(7)

E

where, L = KL(q(β|θ)||p(β|h)) = KL(q(β|θ)||p(β)) − Eq(β|θ) [log p(h|β)].
The resulting cost function LE on the right of Eq. (7) is known as the (negative) “evidence lower
bound” (ELBO). The second term in LE is the likelihood of h fitting to the mixture Gaussian with
parameter β: Eq(β|θ) [log p(h|β)], while the first term is referred to as the complexity cost [9]. We
optimize the ELBO using stochastic gradient descent. With θ∗ , the positive prior is then estimated as
πP = q(βiπ |θ∗ ), i = arg max |Ck |
k=1,2

(8)

where C1 = {h ∈ HP , p(h|β 1 ) > p(h|β 2 )} and C2 = {h ∈ HP , p(h|β 2 ) > p(h|β 1 )}.
3.3

Parameter Learning

To train the three networks fA (.; θA ), fG (.; θG ), fC (.; θC ) for connectivity score prediction, we
PT
E
R
jointly optimize L = t=1 LR
t + Lt , using Adam over the model parameters. Loss L is the PU
E
classification risk as described in section 3.1, and L is the loss of prior estimation as described in
section 3.2. Note that during training, yaTij = yaTij−1 since we do not observe GT in training. This is
to enforce prediction consistency.

4
4.1

Experimental Evaluation
Dataset and Experimental Setup

The graphs on which we apply our model are constructed from the title and abstract of papers
published in the biomedical fields from 1949 to 2020. The nodes are the biomedical terms, while the
edges linking two nodes indicate the co-occurrence of the two terms. Note that we focus only on the
co-occurrence relation and leave the polarity of the relationships for future study. To evaluate the
model’s adaptivity in different scientific domains, we construct three graphs from papers relevant to
COVID-19, Immunotherapy, and Virology. The graph statistics are shown in Table 1. To set up the
5

training and testing data for TRP model, we split the graph by year intervals (5 years for COVID-19
or 10 years for Virology and Immunotherapy). We use splits of {G1 , G2 , ..., GT −1 } for training, and
use connections newly added in the final split GT for testing. Since baseline models do not work
on dynamic data, hence we train on GT −1 and test on new observations made in GT . Therefore in
testing, the positive pairs are those linked in GT but not in GT −1 , i.e., E T \ E T −1 , which can be
new connections between nodes already existing in GT −1 , or between a new node in GT and another
node in GT −1 , or between two new nodes in GT . All other unlinked node pairs in GT are unlabeled.
At each t = 1, ..., T − 1, graph Gt is incrementally updated from Gt−1 by adding new nodes
(biomedical terms) and their links. For the node feature vector xtv , we extract its term description
and convert to a 300-dimensional feature vector by applying the latent semantic analysis (LSI). The
missing term and context attributes are filled with zero vectors. If this node already exists before
time t, the context features are updated with the new information about them in discoveries, and
publications. In the inference (testing) stage, the new nodes in GT are only presented with their
feature vectors xTv . The connections to these isolated nodes are predicted by our TRP model.
We implement TRP using the Tensorflow library. Each GPU based experiment was conducted
on an Nvidia 1080TI GPU. In all our experiments, we set the hidden dimensions to d = 128.
For each neural network based model, we performed a grid search over the learning rate lr =
{1e−2, 5e−3, 1e−3, 5e−2}, For the prior estimation, we adopted Gaussian, square-root inverse
Gamma, and Dirichlet distributions to model the mean, co-variance matrix and mixing coefficient
variational posteriors respectively.
4.2

Comparison Methods and Performance Matrices

We evaluate our proposed TRP model in several variants and by comparing with several competitors:
1) TRP variants: a) TRP-PN - the same framework but in PN setting (i.e., treating all unobserved
samples as negative, rather than unlabeled); b) TRP-NNPU - trained using the non-negative risk
estimator Eq. (4) or the equation defined in section 3.1 for our problem; and c) TRP-UPU - trained
using the unbiased PU risk estimator Eq. (3), the equation defined in section 3.1 for our problem.
The comparison of these variants will show the impact of different risk estimators.
2) SOTA PU learning: the state-of-the-art (SOTA) PU learning methods taking input h from
the SOTA node embedding models, which can be based on LSI [12], node2vec [19], DynAE
[18] and GraphSAGE [20]. Since node2vec learns only from the graph structure, we concatenate
the node2vec embeddings with the text (term and context) attributes to obtain an enriched node
representation. Unlike our TRP that learns h for one pair of nodes, these models learn embedding
vectors for individual nodes. Then, h of one pair from baselines is defined as the concatenation of the
embedding vector of two nodes. We observe from the results that a concatenation of node2vec and
LSI embeddings had the most competitive performance compared to others. Hence we only report
the results based on concatenated embeddings for all the baselines methods. The used SOTA PU
learning methods include [15] by reweighting all examples, and models with different estimation of
the class prior such as SAR-EM [8] (an EM-based SAR-PU method), SCAR-KM2 [37], SCAR-C
[8], SCAR-TIcE [6], and pen-L1 [10].
3) Supervised: weaker but simpler logistic regression applied also h.
We measure the performance using four different metrics. These metrics are the Macro-F1 score
(F1-M), F1 score of observed connections (F1-S), F1 score adapted to PU learning (F1-P) [7, 29],
and the label ranking average precision score (LRAP), where the goal is to give better rank to the
positive node pairs. In all metrics used, higher values are preferred.
4.3

Evaluation Results

Table 2 shows that TRP-UPU always has the superior performance over all other baselines across the
datasets due to its ability to capture and utilize temporal, structural, and textual information (learning
better h) and also the better class prior estimator. Among TRP variants, TRP-UPU has higher or equal
F1 values comparing to the other two, indicating the benefit of using the unbiased risk estimator. On
the LRAP scores, TRP-UPU and TRP-PN have the same performance on promoting the rank of the
positive samples. Note that the results in Table 2 are from the models trained with their best learning
rate, which is an important parameter that should be tuned in gradient-based optimizer, by either
exhaustive search or advanced auto-machine learning [34]. To further investigate the performance of
6

Table 1: Three graph dataset statistics, with their number of nodes and edges

COVID-19
Immunotherapy
Virology

Graphs until T
#nodes #edges
27,325 2,474,624
28,823
919,004
38,956 1,117,118

Node pairs in evaluation at T
T
T −1
T
T
T
#Positive E \E
#Unlabeled sampled from {V ×V }\E
655,649
1,019,458
303,516
1,075,659
446,574
1,382,856

Table 2: Evaluation results on the COVID-19, Immuniotherapy and Neurology datasets, respectively
Supervised
SCAR-C [8]
SCAR-KM2 [37]
SCAR-TIcE [6]
SAR-EM [6]
Elkan [15]
TRP-PN
penL1-NNPU
TRP-NNPU
penL1-UPU
TRP-UPU

F1-S
0.82
0.82
0.76
0.57
0.78
0.82
0.84
0.71
0.80
0.85
0.86

COVID-19
F1-M
F1-P
0.86
1.73
0.86
1.73
0.82
1.52
0.30
1.01
0.83
1.68
0.86
1.74
0.87
1.80
0.70
1.35
0.82
1.68
0.88
1.86
0.88
1.88

LRAP
0.77
0.77
0.73
0.39
0.76
0.81
0.91
0.45
0.89
0.89
0.91

Virology
F1-M
F1-P
0.73
1.42
0.72
1.40
0.61
1.21
0.22
1.02
0.75
1.52
0.73
1.47
0.83
2.31
0.71
1.83
0.82
2.38
0.83
2.45
0.83
2.38

F1-S
0.57
0.56
0.49
0.38
0.60
0.58
0.73
0.61
0.73
0.74
0.74

LRAP
0.43
0.43
0.33
0.23
0.60
0.58
0.81
0.63
0.83
0.81
0.81

F1-S
0.67
0.66
0.53
0.35
0.67
0.69
0.71
0.53
0.67
0.70
0.71

Immunotherapy
F1-M
F1-P
0.80
2.18
0.79
2.14
0.66
1.50
0.19
1.01
0.80
2.18
0.81
2.26
0.81
2.33
0.63
1.62
0.78
2.18
0.81
2.33
0.82
2.35

LRAP
0.56
0.56
0.36
0.22
0.62
0.65
0.77
0.73
0.76
0.72
0.77

TRP variants, we show in Figure 2 their F1-S at different learning rate in trained from 1 to 10 epochs.
We notice that TRP-UPU has a stable performance across different epochs and learning rates. This
advantage is attributed to the unbiased PU risk estimation, which learns from only positive samples
with no assumptions on the negative samples. We also found interesting that NNPU was worse than
UPU in our experimental results. However, it is not uncommon for UPU to outperform NNPU in
evaluation with real-world datasets. Similar observations were found in the results in [10, 16]. In
our case, we attribute this observation to the joint optimization of the loss from the classifier and
−
the prior estimation. Specifically, in the loss of UPU (Eq. (3)), πP affects both R̂+
P (f ) and R̂P (f ).
+
−
−
However, in the loss of NNPU (Eq. (4)), πP only weighted R̂P (f ) when R̂U − πP R̂P (f ) is negative.
In real-world applications, especially when the true prior is unknown, the loss selection affects the
estimation of πP , and thus the final classification results. TRP-PN is not as stable as TRP-UPU due
to the strict assumption of unobserved samples as negative.
TRP-PN (COVID-19)

TRP-PN (Virology)

TRP-PN (Immunotherapy)

0.9

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

0.3
1.E-03

5.E-03

1.E-02

0.3
5.E-02 1.E-03

TRP-NNPU (COVID-19)

5.E-03

1.E-02

0.3
5.E-02 1.E-03

TRP-NNPU (Virology)

0.9

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.3
1.E-03

5.E-03

1.E-02

5.E-03

1.E-02

0.3
5.E-02 1.E-03

TRP-UPU (Virology)
0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

Epoch 1

1.E-02
Epoch 2

0.3
5.E-02 1.E-03
Epoch 3

5.E-02

Epoch 4

5.E-03

1.E-02

Epoch 5

Epoch 6

5.E-03

1.E-02

5.E-02

TRP-UPU (Immunotherapy)

0.9

5.E-03

1.E-02

0.4

0.3
5.E-02 1.E-03

TRP-UPU (COVID-19)

0.3
1.E-03

5.E-03

TRP-NNPU (Immunotherapy)

0.3
5.E-02 1.E-03
Epoch 7

Epoch 8

5.E-03
Epoch 9

1.E-02

5.E-02

Epoch 10

Figure 2: Stability comparison of TRP-PN, TRP-NNPU and TRP-UPU, showing the F1-S performance of the models (Y-axis) with different learning rates (X-axis) on 10 epochs.

7

4.4

Incremental Prediction

In Figure 3, we compare the performance of the top-performing PU learning methods on different
year splits. We train TRP and other baseline methods on data until t − 1 and evaluate its performance
on predicting the testing pairs in t. It is expected to see performance gain over the incremental
training process, as more and more data are used. We show F1-P due to the similar pattern on other
metrics. We observe that the TRP models display an incremental learning curve across the three
datasets and outperformed all other models.
Virology

COVID-19
0.9
0.7

0.8

F1-Score

F1-Score

0.9
0.7
0.6
0.5

0.5
0.3
0.1

0.4
2001 - 2005
SAR-EM

SCAR-C

2006 - 2010 2011 - 2015
Evaluation year
Elkan

TRP-PN

TRP-NNPU

1980 - 1989

2016 - 2020

SAR-EM

TRP-UPU

Immunotherapy

1990 - 1999 2000 - 2009
Evaluation year

SCAR-C

Elkan

TRP-PN

TRP-NNPU

2010 - 2019
TRP-UPU

0.8

F1-Score

0.7
0.6
0.5
0.4
0.3
0.2
1980 - 1989
SAR-EM

SCAR-C

1990 - 1999 2000 - 2009
Evaluation year
Elkan

TRP-PN

TRP-NNPU

2010 - 2019
TRP-UPU

Figure 3: F1-P per year. The models are incrementally trained with data before the evaluation year.
4.5

Qualitative Analysis

We conduct qualitative analysis of the results obtained by TRP-UPU on the COVID-19 dataset. This
investigation is to qualitatively check the meaningfulness of the paired terms, e.g., can term covid-19
be paired meaningfully with other terms. We designed two evaluations. First, we set our training
data until 2015, i.e., excluding the new terms in 2016-2020 in the COVID-19 graph, such as covid-19,
sars-cov-2. The trained model then predicts the connectivity between covid-19 as a new term and
other terms, which can be also a new term or a term existing before 2015. Since new terms like
covid-19 were not in training graph, their term feature were initialized as defined in Section 4.1.
The top predicted terms predicted to be connected with covid-19 are shown in Table 3 top, with the
verification in COVID-19 graph of 2016-2020. We notice that the top terms are truly relevant to
covid-19, and we do observe their connection in the evaluation graph. For instance, Cough, Fever,
SARS, Hand (washing of hands) were known to be relevant to covid-19 at the time of writing this
paper.
In the second evaluation, we trained the model on the full COVID-19 data (≤ 2020) and then predict
to which terms covid-19 will be connected, but they haven’t been connect yet in the graph until 20201 .
We show the results in Table 3 bottom, and verified the top ranked terms by manually searching the
recent research articles online. We did find there exist discussions between covid-19 and some top
ranked terms, for example, [3] discusses how covid-19 affected the market of Chromium oxide and
[22] discusses about caring for people living with Hepatitis B virus during the covid-19 spread.
4.6

Pair Embedding Visualization

We further analyze the node pair embedding learned by TRP-UPU on the COVID-19 data by
visualizing them with t-SNE [33]. To have a clear visibility, we sample 800 pairs and visualize the
learned embeddings in Figure 4. We denote with colors the observed labels in comparison with
1

Dataset used in this analysis was downloaded in early March 2020 from https://www.semanticscholar.
org/cord19/download

8

Table 3: Top ranked terms predicted to be connected with term covid-19, trained until 2015 (the top
table) and until 2020 (the bottom table). Verification of existence (Ex) was conducted in the graph
in 2020 when trained until 2015, and by manual search otherwise. Sc is the predicted connectivity
score.
Terms
Leukocyte count
Fever
Hand
Terms
Antibodies
A549 cells
Hepatitis b virus
Mycoplasma

Sc
0.98
0.94
0.91
Sc
0.99
0.99
0.99
0.99

Ex
Yes
Yes
Yes
Ex
Yes
Yes
No
Yes

Terms
Air
Lung
SARs

Sc
0.85
0.81
0.70

Terms
Lymph
White matter
Alkaline phosphatase
Zinc

Ex
Yes
Yes
Yes
Sc
0.99
0.99
0.99
0.99

Terms
Infection control
Population
Public health
Ex
Yes
Yes
Yes
Yes

Terms
Tobacco
Serum albumin
Macrophages
Bacteroides

Sc
0.96
0.93
0.88
Sc
0.99
0.99
0.99
0.99

Ex
Yes
Yes
Yes
Ex
Yes
Yes
Yes
No

Terms
Serum
Ventilation
Cough
Terms
Adaptive immunity
Allopurinol
Liver function tests
Chromium dioxide

Sc
0.84
0.76
0.71
Sc
0.99
0.99
0.99
0.96

Ex
Yes
Yes
Yes
Ex
Yes
Yes
Yes
No

Figure 4: Pair embedding visualization. The blue color denotes the true positive samples, the red
points are unobserved negative, the green points are unobserved positive.
the predicted labels. We observe that the true positives (observed in GT and correctly predicted as
positives - blue) and unobserved negatives (not observed in GT and predicted as negatives - red) are
further apart. This clear separation indicates that the learned h appropriately grouped the positive
and negative (predicted) pairs in distinct clusters. We also observe that the unobserved positives
(not observed in GT but predicted as positives - green) and true positives are close. This supports
our motivation behind conducting PU learning: the unlabeled samples are a mixture of positive and
negative samples, rather than just negative samples. We observe that several unobserved positives are
relationships like between Tobacco and covid-19. Although they are not connected in the graph we
study, several articles have shown a link between terms [2, 45, 36].

5

Conclusion

In this paper, we propose TRP - a temporal risk estimation PU learning strategy for predicting the
relationship between biomedical terms found in texts. TRP is shown with advantages on capturing
the temporal evolution of the term-term relationship and minimizing the unbiased risk with a positive
prior estimator based on variational inference. The quantitative experiments and analyses show that
TRP outperforms several state-of-the-art PU learning methods. The qualitative analyses also show the
effectiveness and usefulness of the proposed method. For the future work, we see opportunities like
predicting the relationship strength between drugs and diseases (TRP for a regression task). We can
also substitute the experimental compatibility of terms for the term co-occurrence used in this study.

Acknowledgments and Disclosure of Funding
The research reported in this publication was supported by funding from the Computational Bioscience
Research Center (CBRC), King Abdullah University of Science and Technology (KAUST), under
award number URF/1/1976-31-01, and NSFC No 61828302.
9

6

Broader Impact

TRP can be adopted to a wide range of applications involving node pairs in a graph structure. For
instance, the prediction of relationships or similarities between two social beings, the prediction of
items that should be purchased together, the discovery of compatibility between drugs and diseases,
and many more. Our proposed model can be used to capture and analyze the temporal relationship of
node pairs in an incremental dynamic graph. Besides, it is especially useful when only samples of a
given class (e.g., positive) are available, but it is uncertain whether the unlabeled samples are positive
or negative. To be aligned with this fact, TRP treats the unlabeled data as a mixture of negative and
positive data samples, rather than all be negative. Thus TRP is a flexible classification model learned
from the positive and unlabeled data.
While there could be several applications of our proposed model, we focus on the automatic biomedical hypothesis generation (HG) task, which refers to the discovery of meaningful implicit connections
between biomedical terms. The use of HG systems has many benefits, such as a faster understanding
of relationships between biomedical terms like viruses, drugs, and symptoms, which is essential in
the fight against diseases. With the use of HG systems, new hypotheses with minimum uncertainty
about undiscovered knowledge can be made from already published scholarly literature. Scientific
research and discovery is a continuous process. Hence, our proposed model can be used to predict
pairwise relationships when it is not enough to know with whom the items are related, but also learn
how the connections have been formed (in a dynamic process).
However, there are some potential risks of hypothesis generation from biomedical papers. 1)
Publications might be faulty (with faulty/wrong results), which can result in a bad estimate of
future relationships. However, this is a challenging problem as even experts in the field might be
misled by the faulty results. 2) The access to full publication text (or even abstracts) is not readily
available, hence leading to a lack of enough data for a good understanding of the studied terms, and
then inaccurate h in generation performance. 3) It is hard to interpret and explain the learning process,
for example, the learned embedding vectors are relevant to which term features, the contribution of
neighboring terms in the dynamic evolution process. 4) For validating the future relationships, there
is often a need for background knowledge or a biologist to evaluate the prediction.
Scientific discovery is often to explore the new nontraditional paths. PU learning lifts the restriction
on undiscovered relations, keeping them under investigation for the probability of being positive,
rather than denying all the unobserved relations as negative. This is the key value of our work in this
paper.

References
[1] 19 primer. URL https://covid19primer.com/dashboard.
[2] Tobacco
and
waterpipe
use
increases
the
risk
of
suffering
from
covid-19.
URL
http://www.emro.who.int/tfi/know-the-truth/
tobacco-and-waterpipe-users-are-at-increased-risk-of-covid-19-infection.html.
[3] Global chromium oxide green market size, covid-19 impact analysis, key insights based on product type, end-use and regional demand till 2024, May 2020. URL https://tinyurl.com/
global-chromium-oxide-covid.
[4] U. Akujuobi, M. Spranger, S. K. Palaniappan, and X. Zhang. T-pair: Temporal node-pair embedding for
automatic biomedical hypothesis generation. IEEE Transactions on Knowledge and Data Engineering,
2020. doi: 10.1109/TKDE.2020.3017687.
[5] S. H. Baek, D. Lee, M. Kim, J. H. Lee, and M. Song. Enriching plausible new hypothesis generation in
pubmed. PloS one, 12(7):e0180539, 2017.
[6] J. Bekker and J. Davis. Estimating the class prior in positive and unlabeled data through decision tree
induction. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[7] J. Bekker and J. Davis.
arXiv:1811.04820, 2018.

Learning from positive and unlabeled data: A survey.

arXiv preprint

[8] J. Bekker, P. Robberechts, and J. Davis. Beyond the selected completely at random assumption for learning
from positive and unlabeled data. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 71–85. Springer, 2019.

10

[9] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks. arXiv
preprint arXiv:1505.05424, 2015.
[10] M. Christoffel, G. Niu, and M. Sugiyama. Class-prior estimation for learning from positive and unlabeled
data. In Asian Conference on Machine Learning, pages 221–236, 2016.
[11] F. De Comité, F. Denis, R. Gilleron, and F. Letouzey. Positive and unlabeled examples help learning. In
International Conference on Algorithmic Learning Theory, pages 219–230. Springer, 1999.
[12] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic
analysis. Journal of the American society for information science, 41(6):391–407, 1990.
[13] M. C. Du Plessis, G. Niu, and M. Sugiyama. Analysis of learning from positive and unlabeled data. In
Advances in neural information processing systems, pages 703–711, 2014.
[14] M. C. Du Plessis, G. Niu, and M. Sugiyama. Class-prior estimation for learning from positive and unlabeled
data. Machine Learning, 106(4):463, 2017.
[15] C. Elkan and K. Noto. Learning classifiers from only positive and unlabeled data. In Proceedings of the
14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 213–220,
2008.
[16] C. Gong, H. Shi, T. Liu, C. Zhang, J. Yang, and D. Tao. Loss decomposition and centroid estimation for
positive and unlabeled learning. IEEE transactions on pattern analysis and machine intelligence, 2019.
[17] V. Gopalakrishnan, K. Jha, A. Zhang, and W. Jin. Generating hypothesis: Using global and local features
in graph to discover new knowledge from medical literature. In Proceedings of the 8th International
Conference on Bioinformatics and Computational Biology, BICOB, pages 23–30, 2016.
[18] P. Goyal, S. R. Chhetri, and A. Canedo. dyngraph2vec: Capturing network dynamics using dynamic graph
representation learning. Knowledge-Based Systems, 187:104816, 2020.
[19] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864, 2016.
[20] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in
neural information processing systems, pages 1024–1034, 2017.
[21] F. He, T. Liu, G. I. Webb, and D. Tao. Instance-dependent pu learning by bayesian optimal relabeling.
arXiv preprint arXiv:1808.02180, 2018.
[22] Hepbtalk. Hep b and covid-19: Resources for individuals and healthcare workers, Apr 2020. URL https:
//www.hepb.org/blog/hep-b-covid-19-resources-individuals-healthcare-workers/.
[23] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. The Journal of
Machine Learning Research, 14(1):1303–1347, 2013.
[24] D. Hristovski, C. Friedman, T. C. Rindflesch, and B. Peterlin. Exploiting semantic relations for literaturebased discovery. In AMIA annual symposium proceedings, volume 2006, page 349, 2006.
[25] C.-J. Hsieh, N. Natarajan, and I. S. Dhillon. Pu learning for matrix completion. In ICML, pages 2445–2453,
2015.
[26] S. Jain, M. White, and P. Radivojac. Estimating the class prior and posterior from noisy positives and
unlabeled data. In Advances in neural information processing systems, pages 2693–2701, 2016.
[27] K. Jha, G. Xun, Y. Wang, and A. Zhang. Hypothesis generation from text based on co-evolution of
biomedical concepts. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pages 843–851. ACM, 2019.
[28] R. Kiryo, G. Niu, M. C. du Plessis, and M. Sugiyama. Positive-unlabeled learning with non-negative risk
estimator. In Advances in neural information processing systems, pages 1675–1685, 2017.
[29] W. S. Lee and B. Liu. Learning with positive and unlabeled examples using weighted logistic regression.
In ICML, volume 3, pages 448–455, 2003.
[30] W. Li, Q. Guo, and C. Elkan. A positive and unlabeled learning algorithm for one-class classification of
remote-sensing data. IEEE Transactions on Geoscience and Remote Sensing, 49(2):717–725, 2010.

11

[31] B. Liu, W. S. Lee, P. S. Yu, and X. Li. Partially supervised classification of text documents. In ICML,
volume 2, pages 387–394. Citeseer, 2002.
[32] L. Liu and T. Peng. Clustering-based method for positive and unlabeled text categorization enhanced by
improved tfidf. J. Inf. Sci. Eng., 30(5):1463–1481, 2014.
[33] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):
2579–2605, 2008.
[34] H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, M. Urban, M. Burkart, M. Dippel, M. Lindauer, and
F. Hutter. Towards automatically-tuned deep neural networks. In Automated Machine Learning, pages
135–149. Springer, 2019.
[35] F. Mordelet and J.-P. Vert. A bagging svm to learn from positive and unlabeled examples. Pattern
Recognition Letters, 37:201–209, 2014.
Powerful properties: how tobacco is be[36] D. Orzáez, D. Orzáez, and N. P. Coordinator.
ing used to fight covid-19, May 2020.
URL https://www.euronews.com/2020/05/25/
powerful-properties-how-tobacco-is-being-used-to-fight-covid-19.
[37] H. Ramaswamy, C. Scott, and A. Tewari. Mixture proportion estimation via kernel embeddings of
distributions. In International Conference on Machine Learning, pages 2052–2060, 2016.
[38] F. Shi, J. G. Foster, and J. A. Evans. Weaving the fabric of science: Dynamic network models of science’s
unfolding structure. Social Networks, 43:73–85, 2015.
[39] H. Shi, S. Pan, J. Yang, and C. Gong. Positive and unlabeled learning via loss decomposition and centroid
estimation. In IJCAI, pages 2689–2695, 2018.
[40] N. R. Smalheiser and D. R. Swanson. Using arrowsmith: a computer-assisted approach to formulating and
assessing scientific hypotheses. Computer methods and programs in biomedicine, 57(3):149–153, 1998.
[41] S. Spangler. Accelerating Discovery: Mining Unstructured Information for Hypothesis Generation.
Chapman and Hall/CRC, 2015.
[42] S. Spangler, A. D. Wilkins, B. J. Bachman, M. Nagarajan, T. Dayaram, P. Haas, S. Regenbogen, C. R.
Pickering, A. Comer, J. N. Myers, et al. Automated hypothesis generation based on mining scientific
literature. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 1877–1886. ACM, 2014.
[43] R. K. Srihari, L. Xu, and T. Saxena. Use of ranked cross document evidence trails for hypothesis generation.
In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 677–686. ACM, 2007.
[44] J. Sybrandt, M. Shtutman, and I. Safro. Moliere: Automatic biomedical hypothesis generation system.
In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 1633–1642, 2017.
[45] R. van ZylSmit, G. Richards, and F. Leone. Tobacco smoking and covid-19 infection. The Lancet
Respiratory Medicine, 2020.
[46] D. Weissenborn, M. Schroeder, and G. Tsatsaronis. Discovering relations between indirectly connected
biomedical concepts. Journal of biomedical semantics, 6(1):28, 2015.
[47] D. Xu and M. Denil. Positive-unlabeled reward learning. arXiv preprint arXiv:1911.00459, 2019.
[48] G. Xun, K. Jha, V. Gopalakrishnan, Y. Li, and A. Zhang. Generating medical hypotheses based on
evolutionary medical concepts. In 2017 IEEE International Conference on Data Mining (ICDM), pages
535–544. IEEE, 2017.

12

arXiv:2010.01916v1 [cs.LG] 5 Oct 2020

Supplementary Material: Temporal
Positive-unlabeled Learning for Automatic
Biomedical Hypothesis Generation

1
1.1

Data Preparation
Graph Construction

Given a dataset of scholarly publications (e.g., from PubMed), we extract and categorize the terms
in the documents defined on a set of UMLS [18] and MeSH [1] terms. In this study, we use data
from the pubmed database of March 2019 and Semantic scholar COVID-19 dataset [2] collected in
March 2020. Each UMLS term belongs to one of three categories, namely: 1) Genes, 2) Chemicals,
and 3) Diseases. We construct a network G = {V, E}, where V is the set of nodes corresponding
to the biomedical terms. The relationship E represents the close co-occurrence of the two terms
in literature. To be specific, an edge in E connects two nodes if the two corresponding terms are
mentioned together in the same title, abstract, or paragraph of a paper1 .
Next, we split the obtained network using year windows, thereby, obtaining a sequence of temporal
graphlets G = {G1 , G2 , ..., GT }. As defined in the Introduction Section 1 of the main paper, this
graphlet sequence encapsulates the temporal evolution of node pair relationships. Since the node
terms belong to several categories (e.g., drugs and diseases), the graph Gt = {V t , E t , xt } is, in fact,
a dynamic heterogeneous attributed graph, with incremental nodes V 1 ⊆ V 2 , ..., ⊆ V T and edges
E 1 ⊆ E 2 , ..., ⊆ E T . The node attribute xt is composed of the term description when available, and
the term contexts, which are the aggregation of sentences encompassing the mention of the terms in
the documents. We use the texts from the publication titles, abstracts, and full-text paragraphs when
available. The node attributes vary per time window due to the increase in the number of publications.
1.2

Positive Samples Construction

For each time step t, we construct only the node pairs of positive samples, since the negative pairs
are uncertain. The positive node pairs are identified based on the graph observed at the next time
step. Denote aij =< vi , vj > a node pair consisting of nodes vi and vj . As shown in Figure 1 of this
supplementary document, the node pair aij at time step t is assigned a positive class +1 if a connection
t+1
between node vi and vj is observed in graph Gt+1 (i.e., sij
).
t = +1 ⇐⇒ e(vi , vj ) ∈ E
ij
Otherwise, the node pair a remains as unlabeled.
Since we consider the insertion only graphlets sequence, the graph size of the graphlets grows
proportionally with the increase in time step. Therefore, the use of all possible pairs for training
becomes more computationally expensive and less feasible in application. In this study for large
graphs, the notion of a node pair set is defined as a sampled subset of all possible node pairs. This
sample is drawn uniformly for each time step t.
1
A mention can have a positive or negative connotation. We consider any kind of mention as a relationship,
regardless of positive or negative. We leave the study of edge polarity for future investigation.

34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.

6

6
2

4

1

4

1
2
3

2
5

3

5

3
3

2

𝐴

1

1
1 ? 1

1 ?

1 ?

1 1
2 1
3 ? 1
?

? ?

4 1 1 ?

1 ? 1

1 1 ?

1

1 1

? ? ?

?

? ? ?

2 1
1 1 1 -1 -1
3 -1 1
1 -1 -1 -1
4 1 1 1
-1 -1 -1

1

? ?
? ?

4

1 1 ?

? ?

5

1 ? ? ?

1 ?

1
2 1
3 ? 1
4 ? 1 ?

?

5 1 ? ? ?

5

6 ? ? ? ? ?
1 2 3 4 5

5 ? ? ? ?
1 2 3 4

𝐴

2
3

1 ?

?
6

3

4

𝐴

1

4
1

2

2
5

4

𝐺

𝐺

4

1

7

3

𝐺

1

𝐺

? 1

? ? ?
? ?

6 1 ? ? ?

?

7 ? ? ? ?

? ?

1 2

3 4 5

?
6 7

5

1 -1 1

1 1 1

1 1 -1 -1

-1 -1

6 1 -1 -1 -1 -1

1

7 1 -1 -1 -1 -1 1
1 2

3 4 5

6 7

Figure 1: An illustration of the graph sequence G = {G1 , G2 , ..., GT } with the sequential positiveunlabeled supervision in A = {A1 , A2 , ..., AT } (T=4 here). As the graph grows with more nodes
and more connections, At is constructed from the graph Gt+1 . One unit at i, j of At is filled by 1 (a
positive relation) if node i and j are connected in Gt+1 . The unit is filled by a 0 (an unknown relation)
if node i and j are NOT connected in Gt+1 . During training, since we do not observe the future, the
node pairs in AT are equivalent to those in AT −1 . The difference between them in the colored units
are only observed in testing. In the training process, Gt and At (t = 1...T ) form training samples
of node pairs with positive or unknown labels. When testing, we aim to predict the unobserved pair
relationships (colored tiles), in AT .

2

Neighborhood Aggregation

The neighborhood aggregation is handled by the aggregator network fG (.; θG ). This network uses
GraphSAGE at its core to aggregate each node’s information in a given node pair aij =< vi , vj > to
obtain a concise representation for them. For each node v in the pair, the aggregation network takes
as input the current node feature xtv as well as the neighborhood information, which includes the
node features of the sampled node neighbors xtN r(v) . Given a maximum neighboorhood layer M
to consider for information aggregation; at each aggregation step m, the representation vectors of
neighbors {Γm−1
, ∀u ∈ N r(v)} at iteration m − 1 are aggregated into a single vector Γm
u
v at iteration
m. Several aggregation techniques for the neighborhood aggregation are proposed in [14]. At the
initial aggregation step m = 0, node vector Γ0v is the input node attribute (i.e., Γ0v = xtv ). After the
neighborhood aggregation steps, the final representation zvt = ΓM
v . The performance of aggregators
often depends on the property of the applied graph [14]. We evaluate different aggregators and report
the best.
Neighborhood Definition. Following the principle of [14], to keep the computational footprint to
a minimum, we work on a fix-size sample set of node neighbors instead of the full neighborhood
nodes. Hence the notion of node neighbors N r(v) is defined as a fix-size sample of the full node
neighborhood {u ∈ V : (u, v) ∈ E}. This sample is drawn uniformly at each iteration, thereby
reducing the time and memory complexity. With the sampling
 strategy, the memory and time
QM
complexity per node aggregation step is fixed at O
S
m=1 m , where Sm is the neighborhood
sample size at layer m, and M is the maximum layer considered (i.e., up to M -hop neighbors).

3

Dataset

In this project, each dataset contains the title and abstract of papers published in the biomedical fields.
To evaluate the model’s adaptivity in different scientific domains, we construct three graphs from
papers on COVID-19, Immunotherapy, and Virology.
The graph statistics are shown in Table 1 of the main paper. To set up the training and testing data, we split the graph by a 10-year interval starting from 1949 (i.e., {≤ 1949}, {1950 −
1959}, . . . , {2010 − 2019}) for the virology and immunotherapy datasets. Due to the novelty of the COVID-19 virus, we split the graph by a 5-year interval starting from 1995 (i.e.,
{≤ 1995}, {1995 − 2000}, . . . , {2010 − 2015}). We use year splits of ≤ 2009 ({G1 , G2 , ..., G7 })
for training, and the final split 2010 − 2019 for testing on the virology and immunotherapy datasets.
2

We use year splits of ≤ 2015 ({G1 , G2 , ..., G5 }) for training, and the final split 2015 − 2020 for
testing on the COVID-19 datasets.
At each t, for a given node (a biomedical term), we extract its term description and context (sentences
encompassing the term in literature). The term description and contexts are respectively converted
to a 300-dimensional feature vector by applying the latent semantic analysis (LSI) method on the
document-term matrix features. The missing term and context attributes are completed with zero
vectors. At each time t, the context features are updated with the new information about them in
discoveries, and publications.

4
4.1

Experimental Setup
Baselines in Experiments

We use node2vec [13] to learn the graph structure feature and concatenate it with the text attributes to
obtain an enriched node representation. In our link prediction task, we concatenate the embeddings
of each node pair together as the final features.
When conducting the baseline experiments, we reweight the unlabeled examples following the
instructions from [11] for Elkan’s baseline. As for SAR-EM [5], SCAR-C [5], SCAR-KM2 [23],
SCAR-TIcE [4], we randomly select 30 features from the embedding features, which are used to
calculate propensity score. For other hyper-parameters, we follow the same setting as their paper.
However, due to training SAR-EM model is very time-consuming, and the result is very unstable, we
limit the expectation-maximization iteration to 10,30,300, and we finally select the best performance
among them.
4.2

TRP Model

In all our experiments, we treat the graph to be undirected and set the hidden dimensions to d = 128.
For each neural network-based model, we performed a grid search over the learning rate lr =
{1e−2 , 5e−3 , 1e−3 , 5e−2 }, on the Virology and Immunotherapy datasets from 1944 to 1999, and
from 1950 to 2010 for the COVID-19 dataset. The best parameters per model from the grid search are
then used in all experiments. The TRP models are trained with a parameter set (d = 128, S1 = 20, and
S2 = 10), where S1 , S2 are the neighborhood sample size for the one-hot and two-hop neighborhood
aggregation respectively. We implement TRP on Python, using the Tensorflow library. Each GPU
based experiment was conducted on an Nvidia 1080TI GPU. The code will be publicly available
upon the acceptance of the work.

5

Evidence for Supporting the Discovered Pairs

In this section, we provide evidences supporting the connectivity prediction between COVID-19 and
the terms in Table 3 of the main paper. Note that the cited reference papers as evidence here were not
present in our training and testing graphs.
Anti-bodies – COVID-19. The relationship between antibodies and the COVID-19 is well known,
and several articles have been published linking the two terms together. The relationship is mainly
seen in articles about the research and development of vaccines.
A549 cells – COVID-19. There are several very recent studies on the effects of COVID-19 on the
A549 cells. Specifically, the capacity of COVID-19 to infect and replicate in A549 cells [15, 6, 8].
Mycoplasma – COVID-19. Several articles studied the effects of coinfection of Mycoplasma and
COVID-19 and their correlation [19, 12].
White matter – COVID-19. White matter is the parts of the brain that connect brain cells to each
other. Brun et al. [7] studied and analyzed the effects of the COVID-19 virus on the neurological
functions of the brain.
Zinc – COVID-19. The effect of zinc on common colds, mostly caused by rhinoviruses, has been
studied. Although the novel coronavirus that causes COVID-19 is not the same type of coronavirus
that causes common colds, several studies [9, 16] have been made on the effect of zinc supplements
on COVID-19 virus.
3

Tobacco – COVID-19. Some researchers have studied and analyzed the effect of tobacco usage with
COVID-19 [3, 24]. Another research is the Cotiana Project [20], which studies the potential use of
tobacco for vaccine production.
Macrophages – COVID-19. Macrophages are a population of innate immune cells that sense
and respond to microbial threats by producing inflammatory molecules that eliminate pathogens
and promote tissue repair [17]. Several recent studies have shown the effects of Macrophages on
COVID-19 [22, 17].
Adaptive immunity – COVID-19. Adaptive immunity is an immunity that occurs after exposure to
an antigen either from a pathogen or a vaccination. Several works have studied the availability and
duration of adaptive immunity after a patient has been exposed to the COVID-19 virus [21, 10].

References
[1] Mesh browser. URL https://meshb.nlm.nih.gov/.
[2] Download covid-19. URL https://www.semanticscholar.org/cord19/download.
and
waterpipe
use
increases
the
risk
of
suffering
from
[3] Tobacco
covid-19.
URL
http://www.emro.who.int/tfi/know-the-truth/
tobacco-and-waterpipe-users-are-at-increased-risk-of-covid-19-infection.html.
[4] J. Bekker and J. Davis. Estimating the class prior in positive and unlabeled data through decision tree
induction. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[5] J. Bekker, P. Robberechts, and J. Davis. Beyond the selected completely at random assumption for learning
from positive and unlabeled data. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases, pages 71–85. Springer, 2019.
[6] D. Blanco-Melo, B. E. Nilsson-Payant, W.-C. Liu, S. Uhl, D. Hoagland, R. Møller, T. X. Jordan, K. Oishi,
M. Panis, D. Sachs, et al. Imbalanced host response to sars-cov-2 drives development of covid-19. Cell,
2020.
[7] G. Brun, J.-F. Hak, S. Coze, E. Kaphan, J. Carvelli, N. Girard, and J.-P. Stellmann. Covid-19—white matter
and globus pallidum lesions: Demyelination or small-vessel vasculitis? Neurology-Neuroimmunology
Neuroinflammation, 7(4), 2020.
[8] V. Cagno. Sars-cov-2 cellular tropism. The Lancet Microbe, 1(1):e2–e3, 2020.
[9] R. Derwand and M. Scholz. Does zinc supplementation enhance the clinical efficacy of chloroquine/hydroxychloroquine to win todays battle against covid-19? Medical Hypotheses, page 109815,
2020.
[10] S. Q. Du and W. Yuan. Mathematical modeling of interaction between innate and adaptive immune
responses in covid-19 and implications for viral pathogenesis. Journal of Medical Virology, 2020.
[11] C. Elkan and K. Noto. Learning classifiers from only positive and unlabeled data. In Proceedings of the
14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 213–220,
2008.
[12] B. E. Fan, K. G. E. Lim, V. C. L. Chong, S. S. W. Chan, K. H. Ong, and P. Kuperan. Covid-19 and
mycoplasma pneumoniae coinfection. American journal of hematology, 2020.
[13] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd
ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864, 2016.
[14] W. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on large graphs. In Advances in
neural information processing systems, pages 1024–1034, 2017.
[15] J. Harcourt, A. Tamin, X. Lu, S. Kamili, S. K. Sakthivel, L. Wang, J. Murray, K. Queen, B. Lynch,
B. Whitaker, et al. Isolation and characterization of sars-cov-2 from the first us covid-19 patient. BioRxiv,
2020.
[16] A. Kumar, Y. Kubota, M. Chernov, and H. Kasuya. Potential role of zinc supplementation in prophylaxis
and treatment of covid-19. Medical Hypotheses, page 109848, 2020.

4

[17] M. Merad and J. C. Martin. Pathological inflammation in patients with covid-19: a key role for monocytes
and macrophages. Nature Reviews Immunology, pages 1–8, 2020.
[18] NCBI. Ncbi resource coordinators. pubmed. URL https://www.ncbi.nlm.nih.gov/pubmed/.
[19] G. L. Nicolson, G. F. de Mattos, et al. COVID-19 Coronavirus: Is Infection along with Mycoplasma or
Other Bacteria Linked to Progression to a Lethal Outcome? International Journal of Clinical Medicine, 11
(05):282, 2020.
[20] D. Orzáez, D. Orzáez, and N. P. Coordinator.
Powerful properties: how tobacco is being used to fight covid-19, May 2020.
URL https://www.euronews.com/2020/05/25/
powerful-properties-how-tobacco-is-being-used-to-fight-covid-19.
[21] S. Pappas. After recovering from covid-19, are you immune?, May 2020.
livescience.com/covid-19-immunity.html.

URL https://www.

[22] M. D. Park. Macrophages: a trojan horse in covid-19?, 2020.
[23] H. Ramaswamy, C. Scott, and A. Tewari. Mixture proportion estimation via kernel embeddings of
distributions. In International Conference on Machine Learning, pages 2052–2060, 2016.
[24] R. van ZylSmit, G. Richards, and F. Leone. Tobacco smoking and covid-19 infection. The Lancet
Respiratory Medicine, 2020.

5

