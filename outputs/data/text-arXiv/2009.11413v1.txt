arXiv:2009.11413v1 [math.ST] 23 Sep 2020

An elementary approach for minimax
estimation of Bernoulli proportion in the
restricted parameter space
Heejune Sheen∗ and Yajun Mei†
H. Milton Stewart School of Industrial and Systems Engineering,
Georgia Institute of Technology, Atlanta, GA, USA
September 25, 2020

Abstract
We present an elementary mathematical method to find the minimax estimator of
the Bernoulli proportion θ under the squared error loss when θ belongs to the restricted
parameter space of the form Ω = [0, η] for some pre-specified constant 0 ≤ η ≤ 1. This
problem is inspired from the problem of estimating the rate of positive COVID-19 tests.
The presented results and applications would be useful materials for both instructors
and students when teaching point estimation in statistical or machine learning courses.

Keywords: Minimax estimation, Bernoulli distribution, squared error loss, restricted parameter space, convex function.

∗
†

(brianshn@gatech.edu)
(ymei@isye.gatech.edu)

1

1.

INTRODUCTION

Point estimation of model parameters have been an important topic in statistics, machine
learning, and data science. Besides the maximum likelihood estimator (MLE), the method
of moment (MOM), and Bayesian method, another important approach is the minimax estimator that minimizes the maximum risk. The applications of minimax estimators can be
found in many fields such as in statistics (Malinovsky and Albert 2015; Yaacoub, Moustakides and Mei 2018; Zinodiny, Strawderman and Parsian 2011), machine learning (BenHaim and Eldar 2007), physics (Ng and Englert 2012; Ng, Phuah and Englert 2012) and
finance (Chamberlain 2000).
One important question on the minimax estimator is to estimate the proportion θ of the
Bernoulli or binomial distributions. For instance, when X1 , · · · , Xn are independent and
identically distributed (i.i.d.) with Bernoulli(θ), i.e., Pθ (Xi = 1) = 1 − Pθ (Xi = 0) = θ, the
MLE estimator is X̄n = (X1 + · · · + Xn )/n, whereas the minimax estimator of θ under the
squared error loss over θ ∈ Ω = [0, 1] is
√
n
1
1
.
X̄n + √
δM = √
n+1
n+12

(1)

The proof of the minimax property of the estimator in (1) is based on the well-known
result that a Bayes procedure with constant risk is minimax, see (Lehmann and Casella
2006). There are also other approaches to prove minimax properties, such as information
inequality (Tsybakov 2008) or the invariance methods (Kiefer 1957). However, in general
it is challenging to derive the minimax estimators, and the standard statistical textbook
provides very limited examples of minimax procedures.
When we are teaching the minimax estimator, one example we used is to estimate the
positive rate θ of COVID-19 surveillance tests. As an illustrative example, suppose that an
organization tests n = 100 random employees/staff for COVID-19 surveillance tests during a
given period (say daily, weekly or monthly), and observes 5 positive cases. In this scenario,
the standard sample mean estimator is to estimate the positive rate is X̄ =
Meanwhile, the minimiax estimator in (1) yields the estimate δM =

√
100
√
0.05
1+ 100

5
100

+

= 0.05.

1
1
√
1+ 100 2

=

0.0939, which is much larger than the sample mean of 5%.
Meanwhile, the COVID-19 example also brings a new challenge. For instance, suppose
that we are pretty sure that the COVID-19 positive rate θ ∈ [0, η], say, η = 0.2, instead of
2

θ ∈ [0, 1]. While the estimator in (1) is minimax when the sample space Ω = {0 ≤ θ ≤ 1},
it is interesting to ask whether it is still minimax when the sample space Ω = {0 ≤ θ ≤ η}
for some 0 ≤ η ≤ 1.
The purpose of this note is to provide a complete solution of the minimax estimator
for the Bernoulli distribution when n = 1 and the sample space Ω = {0 ≤ θ ≤ η}. We
will show that the estimator in (1) is still minimax when η ≥ 3/4, but we will have a new
minimax estimator when 0 ≤ η < 3/4. Our proof is based on elementary mathematics tools,
as we focus on the n = 1 case. We hope that our note provides more examples of minimax
procedures that can be accessed to high school or undergraduate students, thereby enriching
the teaching materials on the minimax estimator and enhancing students’ understanding
and interests.
We should mention that our paper essentially deals with the minimax estimators in
restricted parameter spaces, and there are some related existing research for Bernoulli or
Binomial distribution in the literature. For instance, Moors (1985) or Berry (1989) derived
the minimax estimation when the parameter space Ω is a symmetric interval, i.e., Ω =
{ 12 − η ≤ θ ≤

1
2

+ η}. Marchand and MacGibbon (2000) considered the parameter space

Ω = {0 ≤ θ ≤ η} as in our note, but derive the minimax estimator for general n case under
the assumption that η is small. Here we assume that the 0 ≤ η ≤ 1 value can be arbitrary,
and we derive the minimax estimator for the general η values when n = 1.
The remainder of this note is as follows. In Section 2, we present the minimax estimation
problem of Bernoulli proportion in the restricted parameter spaces when the parameter
space Ω = {0 ≤ θ ≤ η}. In Section 3, we state our main result on the minimax estimator,
and present a rigorous elementary mathematical proof. Section 4 contains some conclusion
marks.
2.

PROBLEM FORMULATION

Suppose that X is a Bernoulli random variable with Pθ (X = 1) = 1−Pθ (X = 0) = θ, and we
want to estimate θ on the basis of X under the squared error loss function L(θ, d) = (θ − d)2 .
In this case, we have n = 1 observation, and the MLE estimator is to estimate θ as θ̂M LE = 0
if X = 0 and = 1 if X = 1. Meanwhile, the minimax estimator δM in (1) is to estimate θ as
1/4 if X = 0 and 3/4 if X = 1.
3

Here we assume further that the parameter θ belongs to a restricted space Ω = {0 ≤
θ ≤ η} for some pre-specified constant η, and we want to find the minimiax estimator of θ
under the squared loss function. That is, we want to find a procedure δ(X) that minimizes
the maximum risk
sup Eθ (θ − δ(X))2

(2)

0≤θ≤η

Since there is only n = 1 observations, the estimator δ(X) is completely determined by
δ(X = 0) = a and δ(X = 1) = b. Indeed, using the notation of a and b, we have
Eθ (θ − δ(X))2 = (θ − a)2 Pθ (X = 0) + (θ − b)2 Pθ (X = 1)
= (θ − a)2 (1 − θ) + (θ − b)2 θ

= (2a − 2b + 1)θ2 + (b2 − a2 − 2a)θ + a2 .

(3)

Thus, the minimax estimator problem in (2) can be written in the following elementary
mathematical form:
Find two real numbers, a and b, that minimize


sup (2a − 2b + 1)θ2 + (b2 − a2 − 2a)θ + a2 .

(4)

0≤θ≤η

for some 0 ≤ η ≤ 1.

3.

OUR MAIN RESULT

Our main results are summarized in the following theorem, whose proof will be presented in
a little later:
Theorem 1. When Ω = {0 ≤ θ ≤ η}, the minimax estimator of θ under (2), or the optimal
solution in (4), is given by

∗

δ(X = 0) = a =



√1 − η − (1 − η)

1
4

and

if 0 ≤ η ≤
if

3
4

<η≤1


 η, if η ≤ 3 ;
3
4
δ(X = 1) = b∗ = min{η, } =
 3 , if η > 3 .
4
4
4
4

3
4

In particular, Theorem 1 indicates that the estimator δM in (1) is still minimax for the
restricted parameter space Ω = {0 ≤ θ ≤ η} when
form of minimax estimators when 0 ≤ η ≤

3
.
4

3
4

≤ η ≤ 1, but we will have a new

For instance, assume that we are pretty

sure that the COVID-19 positive rate of an organization is θ ∈ [0, η], say, η = 0.2, and
assume that we randomly test a subject. Then the minimax estimate of the positive rate θ
√
is 1 − 0.2 − (1 − 0.2) ≈ 9.44% if the subject’s test is negative, and is 20% if the subject’s
test is positive.
Let us now provide a rigorous proof of Theorem 1.
Proof of Theorem 1: It suffices for us to consider 0 ≤ a ≤ η and 0 ≤ b ≤ η when solving
the optimization problem in (4) for 0 ≤ η ≤ 1. This is because it is evident from (3) that for

all 0 ≤ θ ≤ η, we have (θ − a)2 ≥ (θ − η)2 if a > η and (θ − a)2 ≥ (θ − 0)2 if a < 0. Thus,

the optimal solution in (4) must be in the interval [0, η], since otherwise we will improve the
objective function if we replace them by the endpoints 0 or η.
Moreover, note that the function


f (a, b, θ) = (2a − 2b + 1)θ2 + (b2 − a2 − 2a)θ + a2

(5)

is a quadratic function with respect to θ, the investigation of its maximum values depends
on the sign of the leading coefficient, and thus we need to split the region of (a, b) into two
sub-regions:
(A) When 2a − 2b + 1 ≥ 0, the maximum of f (a, b, θ) is attained at one of two endpoints,
θ = 0 or θ = η.
(B) When 2a − 2b + 1 < 0, f (a, b, θ) is a concave quadratic function of θ
Our main idea is to find the maximum value that can be minimized in each sub-region, which
will yield the global minimax solution.
Let us begin with case (A), or when a ≥ b − 21 , it suffices to compare two endpoints. In

this case, f (a, b, 0) = a2 and f (a, b, η) = (1 − 2b + 2a)η 2 + (b2 − 2a − a2 )η + a2 . It is evident

that f (a, b, 0) ≥ f (a, b, η) if and only if (1 − 2b + 2a)η + (b2 − 2a − a2 ) ≤ 0, or equivalently,

a2 + 2(1 − η)a ≥ b2 − 2ηb + η. This can be further simplified as
(a + (1 − η))2 − (b − η)2 ≥ 1 − η,
5

(6)

or a ≥

p

(η − b)2 + (1 − η) − (1 − η), since we are only interested in the case when a > 0.

A key observation is that the boundary of (6) defines a hyperbolic, which has a vertex
√
(a, b) = ( 1 − η − (1 − η), η) that has the smallest positive a value. Moreover, in case (A),
we have a ≥ b − 21 , and its boundary line a = b −

1
2

interests the hyperbolic boundary of (6)

at the unique point (a, b) = ( 14 , 43 ). This leads to two subcases, depending on whether η ≤

3
4

or not. This determines whether the unique intersection point is below or above the vertex
of the hyperbolic curve, or equivalently, whether the line a = b −

1
2

intersects on the upper

or lower part of the hyperbolic curve.
Meanwhile, in case (A), we will need to solve two sub-problems:
Problem(A1) : min a2

(7)

0≤a,b≤η

s.t. a ≥

p

(η − b)2 + (1 − η) − (1 − η)

1
a≥b− .
2



2
2
2
2
Problem(A2) : min (1 − 2b + 2a)η + (b − 2a − a )η + a
0≤a,b≤η

s.t. a ≤

p

(8)

(η − b)2 + (1 − η) − (1 − η)

1
a≥b− .
2

In other words, in case (A), we need to investigate two subproblems, (A1) and (A2), under
two subcases: one for 0 ≤ η ≤

3
4

and the other for

First, we claim that when 0 ≤ η ≤

3
,
4

3
4

≤ η ≤ 1.

both problems (A1) and (A2) have the same

optimal solution:
a∗ =

p

To see this, since 0 ≤ η ≤

1 − η − (1 − η)

3
4

b∗ = η.

and

and the line a = b −

1
2

(9)

intersects on the upper part of the

hyperbolic curve, the objective function a2 in problem (A1) (7) attains its minimum value of
√
( 1 − η − (1 − η))2 at the vertex. The proof for problem (A2) in (8) is a little complicated
but can follow similar ideas.
To be more specific, in problem (A2), set the objective value (1 − 2b + 2a)η 2 + (b2 − 2a −
6

a2 )η + a2 = γ. This is equivalent to the ellipse:
(a − η)2
(b − η)2
+
q 2
q 2 = 1,
γ
1−η

(10)

γ
η

with a center (η, η). Since the semi-major axis and semi-minor axis of the ellipse are proportional to γ, minimizing γ is equivalent to finding the smallest ellipse that intercepts with
a point (a, b) in the feasible region of (a, b) in (A2):
1
(η − b)2 + (1 − η) − (1 − η), a ≥ b − }.
2
p
The smallest ellipse is obtained when it intercepts with the curve a = (η − b)2 + (1 − η) −
√
(1 − η) at one point. For any 0 < η ≤ 1, by letting γ = ( 1 − η − (1 − η))2 , we have
p
an ellipse that intersects with the curve a = (η − b)2 + (1 − η) − (1 − η) at the vertex
√
√
( 1 − η − (1 − η), η). Hence, when 0 ≤ η ≤ 34 , the optimal solution is ( 1 − η − (1 − η), η).
{(a, b) : 0 ≤ a ≤ η, 0 ≤ b ≤ η, a ≤

Second, we claim that when

3
4

p

≤ η ≤ 1, both problems (A1) and (A2) have the same

optimal solution:
a∗ =
To prove this, note that when

3
4

1
4

and

3
b∗ = .
4

(11)

≤ η ≤ 1, the line a = b− 21 intersects on the lower part of the

hyperbolic curve, i.e., the vertex in (9) does not satisfy the constraint a ≥ b − 21 for problem

(A). In particular, for problem (A1), the point with the smallest a value is (a, b) = ( 14 , 43 ),
not the vertex, and thus (11) is the optimal solution to problem (A1). When

3
4

≤ η ≤ 1,

the argument to problem (A2) is similar to those in (A1), as the vertex does not belong to
the feasible region. In this case, from the shape of the hyperbola, we see that the optimal
solution is also given by (11), see Figure 1 and 2 for the illustration of the optimal solutions,
depending on the value of η.
Next, let us investigate case (B) when 2a − 2b + 1 < 0, i.e., when a +

1
2

< b. Our main

conclusion is that the global minimax solution cannot be obtained in this case. Recall that
in the feasible region, we have 0 ≤ a ≤ η and 0 ≤ b ≤ η. Thus the relation a +

hold if η ≤ 12 . Thus, it suffices to investigate case (B) when

1
2

< b cannot

< η ≤ 1. We claim that the

maximum value in case (B) will be larger those in case (A) when

7

1
2

1
2

< η ≤ 1. To prove this,

1

1
(η,η)
vertex

0.875

(1/4,3/4)
(η,η)
0.6

vertex

b

b

0

0
0

a

0.6

1

0

0.875

a

1

Figure 1: The gray area represents the feasible region of problem (A1). The left figure shows
√
that the optimal solution is obtained at the vertex ( 1 − η − (1 − η), η) when 0 < η ≤ 34 .

The right figure demonstrates that the optimal solution is ( 14 , 43 ) when
consider a specific value θ =

1
2

3
4

≤ η ≤ 1.

∈ [0, η]. Then we have the following inequalities:

1
1
1
f (a, b, ) = (1 − 2b + 2a) + (b2 − 2a − a2 ) + a2
2
4(
2

2 
2 )
1
1
1
=
a−
+ b−
2
2
2
(
)
2
1
1
a−
+ a2
>
2
2

2
1
1
= a−
+
4
16
1
≥ .
16
where the first inequality follows from the assumption of a +

1
2

< b for Case (B). On the

other hand, recall that the maximum values of case (A) attains the minimum value of
√
1
, respectively, depending on whether η < 34 or not. Both of these
( 1 − η − (1 − η))2 and 16
maximum values are less than or equal to

1
.
16

This implies that a minimax estimator for case

(B) cannot be a minimax estimator for problem (4). As a result, we exclude case (B) and
conclude that the minimax estimator in case (A) is the minimax estimator of the problem
(4). This proves the theorem.

8

1

1
(η,η)

vertex
0.875
(1/4,3/4)
(η,η)
0.6

vertex

b

b

0

0
0

a

0.6

1

0

a

0.875

1

Figure 2: The ellipse is centered at (η, η) and the gray area represents the feasible region of
problem (A2). The left figure is for the case when 0 < η ≤
case when

3
4

3
4

and the right figure is for the

≤ η ≤ 1.
4.

CONCLUSION

In the statistical decision theory course, the restricted minimax problem (4) would serve as
a useful advanced exercise for illustrating minimax estimation. Indeed, this problem have
been discussed in our advanced undergraduate level or first-year-graduate-level statistics
course. To find the minimax estimators for the problem (4), we have used the geometrical
interpretation of hyperbola, ellipse and convex functions. Since our method is simple and
different from the standard Bayesian approach, it can be illustrated to students with different
backgrounds. Moreover, our methodology motivates students to tackle statistical problems
with different and diverse perspectives.
REFERENCES
Ben-Haim, Z., and Eldar, Y. C. (2007), “Blind minimax estimation,” IEEE Transactions on
Information Theory, 53(9), 3145–3157.
Berry, C. J. (1989), “Bayes minimax estimation of a Bernoulli p in a restricted parameter
space,” Communications in Statistics-Theory and Methods, 18(12), 4607–4616.
Chamberlain, G. (2000), “Econometric applications of maxmin expected utility,” Journal of
Applied Econometrics, 15(6), 625–644.
9

Kiefer, J. (1957), “Invariance, minimax sequential estimation, and continuous time processes,” The Annals of Mathematical Statistics, 28(3), 573–601.
Lehmann, E. L., and Casella, G. (2006), Theory of point estimation, New York, NY: Springer
Science & Business Media.
Malinovsky, Y., and Albert, P. S. (2015), “A note on the minimax solution for the two-stage
group testing problem,” The American Statistician, 69(1), 45–52.
Marchand, É., and MacGibbon, B. (2000), “Minimax estimation of a constrained binomial
proportion,” Statistics & Risk Modeling, 18(2), 129–168.
Moors, J. (1985), Estimation in truncated parameter spaces, PhD thesis, Tilburg University.
Ng, H. K., and Englert, B.-G. (2012), “A simple minimax estimator for quantum states,”
International Journal of Quantum Information, 10(04), 1250038.
Ng, H. K., Phuah, K. T. B., and Englert, B.-G. (2012), “Minimax mean estimator for the
trine,” New Journal of Physics, 14(8), 085007.
Tsybakov, A. B. (2008), Introduction to Nonparametric Estimation, 1st edn, New York, NY:
Springer Publishing Company, Incorporated.
Yaacoub, T., Moustakides, G. V., and Mei, Y. (2018), “Optimal Stopping for Interval Estimation in Bernoulli Trials,” IEEE Transactions on Information Theory, 65(5), 3022–3033.
Zinodiny, S., Strawderman, W. E., and Parsian, A. (2011), “Bayes minimax estimation of the
multivariate normal mean vector for the case of common unknown variance,” Journal
of multivariate analysis, 102(9), 1256–1262.

10

